Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 88?97,Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational LinguisticsRegulating Dialogue with Gestures?Towards an Empirically GroundedSimulation with Conversational AgentsKirsten Bergmann1,2 Hannes Rieser1 Stefan Kopp1,21 Collaborative Research Center 673 ?Alignment in Communication?, Bielefeld University2 Center of Excellence ?Cognitive Interaction Technology?
(CITEC), Bielefeld University{kbergman,skopp}@TechFak.Uni-Bielefeld.DEhannes.rieser@Uni-Bielefeld.DEAbstractAlthough not very well investigated, a crucial as-pect of gesture use in dialogues is to regulate theorganisation of the interaction.
People use gesturesdecisively, for example to indicate that they wantsomeone to take the turn, to 'brush away' whatsomeone else said, or to acknowledge others' con-tributions.
We present first insights from a corpus-based investigation of how gestures are used toregulate dialogue, and we provide first results froman account to capture these phenomena in agent-based communication simulations.
By advancing amodel for autonomous gesture generation to alsocover gesture interpretation, this account enables afull gesture turn exchange cycle of generation, un-derstanding and acceptance/generation in virtualconversational agents.1 MotivationResearch on gestures must combine empirical,theoretical and simulation methods to investigateform, content and function of gestures in relationto speech.
Our work is based on a corpus of multi-modal data, the Bielefeld Speech and GestureAlignment corpus of route-description dialogues(SAGA corpus, L?cking et al 2010).
The point ofdeparture of our research has been work on iconicand deictic gestures over many years.
In this paperwe focus on a not very well investigated functionof gestures which we have repeatedly observed inthis corpus, namely, the regulation of dialogue.Most of current gesture research is oriented to-wards the semiotics of a Peircean tradition as canfor instance be seen from McNeill?s ?Kendon?scontinuum?
(McNeill 1992, p. 37).
As a conse-quence of this Peircian orientation, gestures havebeen viewed as single signs interfacing withspeech.
Going beyond the integration of in-put/output modalities in single speech-gesturecompositions (Johnston and Bangalore, 2005), lit-tle effort has been spent on the investigation ofsequences of gestures and speech-gesture composi-tion both within and across speakers (Hahn andRieser 2010, Rieser 2010).
Furthermore, researchof gesture meaning was restricted to the contribu-tion of gesture content to propositional content.
Anexception to this research line has been the work ofBavelas et al (1992, 1995).
It is characterised bytwo features, a functional perspective on gesture inopposition to purely classificatory and typologicalones and an interest to systematically investigatethe role of gesture in interaction.
In particular,Bavelas et al (1992) proposed a distinction be-tween ?topic gestures?
and ?interactive gestures?
:Topic gestures depict semantic information di-rectly related to the topic of discourse, while inter-active gestures refer to some aspect of the processof conversing with another person.
Interactive ges-tures include delivery gestures (e.g.
marking in-formation status as new, shared, digression), citinggestures (acknowledging others?
prior contribu-tions), seeking gestures (seeking agreement, orhelp in finding a word), and turn coordination ges-88tures (e.g.
taking or giving the turn).
Gill et al(1999) noted similar functions of gesture use, add-ing body movements to the repertoire of pragmaticacts used in dialogue act theory (e.g.
turn-taking,grounding, acknowledgements).We aim to find out how gestures are related to andhelp regulate the structure of dialogue.
We will callthese gestures `discourse gestures?.
Relevant re-search questions in this respect are the following:How can gesture support next speaker selection ifthis follows regular turn distribution mechanismssuch as current speaker selects next?
From the dia-logues in SAGA we know that averting nextspeaker?s self-selection is of similar importance ashanding over the floor to the next speaker.
So, howcan averting self-selection of other be accom-plished gesturally?
A still different problem is howgesture is utilised to establish an epistemicallytransparent, reliable common ground, say a tightworld of mutual belief.
A precondition for that ishow gesture can help to indicate a gesturer?s stanceto the information he provides.
Natural languagehas words to indicate degrees of confidence in in-formation such as probably, seemingly, approxi-mately, perhaps, believe, know, guess etc.
Can ges-tures acquire this function as well?All these issues can be synopsised as follows: Howcan gestures?apart from their manifest contribu-tion to propositional content?be used to push thedialogue machinery forward?
In our research, ges-ture simulation and theory of speech-gesture inte-gration are developed in tandem.
Up to now, bothhave been tied to occurrences of single gesturesand their embedding in dialogue acts.
In this paper,we present first steps along both methodologicalstrands to explore the use and function of gesturein dialogue.
We start with an empirical perspectiveon discourse gestures in section 2.
In section 3 webriefly describe our gesture simulation modelwhich so far simulates gesture use employing thevirtual agent MAX independent of discourse struc-tures.
Section 4 analyses a corpus example of aminimal discourse which is regulated mainly bygestures of the two interactants.
This provides thebasis for our proposed extension of the gesturegeneration approach to capture the discourse func-tion of gestures as described in section 5.
This ex-tension will encompass a novel approach to em-ploy the very generation model used for gestureproduction, and hence all the heuristic gestureknowledge it captures, also for gesture interpreta-tion in dialogue.
Section 6 discusses the differencebetween pure interactive gestures and discoursegestures and proposes further steps that need to betaken to elucidate how gestures are used as a vehi-cle for regulating dialogue.2 Empirical Work on Discourse GesturesIn looking for discourse gestures we started fromthe rated annotation of 6000 gestures in the SAGAcorpus.
We managed to annotate and rate about5000 of them according to traditional criteria usingpractices and fine-grained gesture morphology likehand-shape and wrist-movement.
About 1000 ges-tures could not be easily subsumed under the tradi-tional gesture types (iconics, deictics, metaphorics,beats).
Furthermore, they were observed to corre-late with discourse properties such as currentspeaker?s producing his contribution or non-regular interruption by other speaker.For purposes of the classification of the remaining1000 gestures we established the following func-tional working definition: `Discourse gestures?
aregestures tied up with properties or functions ofagents?
contributions in dialogue such as success-fully producing current turn, establishing coher-ence across different speakers?
turns by gesturalreference or indicating who will be next speaker.What did we use for dialogue structure?
Being fa-miliar with dialogue models such as SDRT (Asherand Lascarides, 2003), PTT (Poesio and Traum,1997), and KoS (Ginzburg, 2011) we soon foundthat these were too restricted to serve descriptivepurposes.
So we oriented our ?classification of dia-logue gesture enterprise?
on the well known turntaking organisation model of Sacks et al (1974)and Levinson?s (1983) discussion of it.
However, itsoon turned out that even these approaches weretoo normative for the SAGA data: This is due tothe fact that dialogue participants develop enor-mous creativity in establishing new rules of con-tent production and of addressing violations ofprima facie rules.Rules of turn-taking, for example, are not hard andfast rules, they can be skirted if the need arises,albeit there is a convention that this has to be ac-knowledged and negotiated.
A very clear exampleof an allowed interruption of an on-going produc-tion is a quickly inserted clarification request serv-ing the communicative goals of current speakerand the aims of the dialogue in general.
Another89problem with the Sacks et al model consists in thefollowing fact: Since its origination many dialogueregularities have been discovered which cannot beeasily founded on a phenomenological or observa-tional stratum which is essentially semantics-free.This can for example be seen from the develop-ment of the notion of grounding and commonground as originally discussed by Stalnaker (1978),Clark (1996) and others.
Nevertheless, grounding(roughly, coming to agree on the meaning of whathas been said (see e.g.
Traum, 1999; Roque andTraum, 2008;  Ginzburg 2011, ch.
4.2 for the op-tions available) generates verbal structure and ver-bal structure interfaces with gesture.
Other exam-ples in this class are acknowledgements or acceptsdiscussed in more detail below.How did we decide on which distinctions of ges-ture annotation have to be used for characterisingdiscourse gestures?
In other words, how did weconceive of the map between gestures of a certainsort and discourse structures?
First of all we ob-served that two types of discourse gestures emergefrom the SAGA data.
Some of them come withtheir own global shape and are close to emblems,(i.e.
conveyors of stable meaning like the victorysign).
This is true for example of the ?brush asideor brush away?
gesture shown in Figure 1 (left),indicating a gesturer?s assessment of the down-rated relevance of information, actions or situa-tions.
Discourse gestures of the second class ex-ploit the means of, for instance, referring gesturesor iconic gestures.
An example of an iconic gesturein this role will be discussed to some extent in sec-tion 4.
Its simulation will be described in sections 3and 5.Here we explain the phenomenon with respect toreferring pointing gestures which are easier to fig-ure out (see Figure 1 (right)).
Their usage as underfocus here is not tied to the information under dis-cussion but to objects in the immediate discoursesituation, preferably to the participants of the dia-logue.
These uses have a Gricean flavour in thefollowing way: Only considerations of relevanceand co-occurrence with a turn transition relevanceplace together indicate that prima facie not generalreference is at stake but indication of next speakerrole.
It wouldn?t make sense to point to the otherperson singling her or him out by indexing, be-cause her or his identity is clear and well estab-lished through the on-going interaction.
Thus wesee that a gestural device associated with estab-lished morphological features, pointing, acquires anew function, namely indicating the role of next-speaker.Figure 1: Examples of discourse gestures: the brush-awaygesture (left) and situated pointing to the upper part of theinterlocutor?s torso (right) used for next speaker selection ina ?Gricean?
sense (see text for explanation).Now both classes of gestures, ?brush away?
usedto indicate informational or other non-relevanceand pointing, indicating the role of being nextspeaker exploit the motor equipment of the hands.For this reason, annotation of discourse gesturescan safely be based on the classification schemaswe have developed for practices like indexing,shaping or modelling and for the fine-grained mo-tor behaviour of the hands as exhibited by palmorientation, back-of-hand trajectory etc.
In work byHahn & Rieser (2009-2011) the following broadclasses of discourse gestures were established.
Webriefly comment upon these classes of gesturesfound in the SAGA corpus relevant for dialoguestructure and interaction:?
Managing of own turn: A speaker may in-dicate how successful he is in editing out hiscurrent production.?
Mechanisms of next-speaker selection asproposed in classical CA research, for in-stance, pointing to the other?s torso is oftenused as a means to indicate next speaker.?
In grounding acts and feed-back especiallyiconic gestures are used to convey proposi-tional content.?
Clarification requests to work on contribu-tions: An addressee may indicate the needfor a quick interruption using a pointing todemand a clarification.
In contrast, a currentspeaker can ward off the addressee?s incipi-ent interruption using a palm-up gesture di-90rected against the intruder thus setting up a?fence?.?
Evidentials for establishing a confidenceleve: There are fairly characteristic gesturesindicating the confidence a speaker has inthe information he is able to convey.?
Handling of non-canonical moves by dis-course participants: Interaction sequencesconsisting of attempts by other speaker to in-terrupt and to thwart this intention by currentspeaker or to give way to it show how dis-course participants handle non-canonicalmoves.?
Assessment of relevance by discourse par-ticipants: Speakers provide an assessmentof which information is central and whichone they want to consider as subsidiary.?
An indication of topical information withrespect to time, place or objects is fre-quently given by pointing or by ?placing ob-jects?
into the gesture space.We know that this list is open and could, more-over, depend on the corpus.
In this paper the focuswill be on grounding acts and feedback (see sec-tions 3-5).
The reason is that this way we can pro-vide an extension of existing work on the simula-tion of gesture production in a fairly direct manner.3 Simulating Gesture Use: The Genera-tion PerspectiveOur starting point to simulate gestural behavior indialogue is a gesture generation system which isable to simulate speaker-specific use of iconic ges-tures given (1) a communicative intention, (2) dis-course contextual information, and (3) an imagisticrepresentation of the object to be described.
Ourapproach is based on empirical evidence thaticonic gesture production in humans is influencedby several factors.
Apparently, iconic gesturescommunicate through iconicity, that is their physi-cal form depicts object features such as shape orspatial properties.
Recent findings indicate that agesture?s form is also influenced by a number ofcontextual constraints such as information struc-ture (see for instance Cassell and Prevost, 1996), orthe use of more general gestural representationtechniques such as shaping or drawing is decisive.In addition, inter-subjective differences in gestur-ing are pertinent.
There is, for example, wide vari-ability in how much individuals gesture when theyspeak.
Similarly, inter-subjective differences arefound in preferences for particular representationtechniques or low-level morphological featuressuch as handshape or handedness (Bergmann &Figure 2: Schema of a gesture generation network in whichgesture production choices are considered eitherprobabilistically (chance nodes drawn as ovals) or rule-based(decision nodes drawn as rectangles).
Each choice isdepending on a number of contextual variables.
The links areeither learned from speaker-specific corpus data (dotted lines)or defined in a set of if-then rules (solid lines).Kopp, 2009).To meet the challenge of considering general andindividual patterns in gesture use, we have pro-posed GNetIc, a gesture net specialised for iconicgestures (Bergmann & Kopp, 2009a), in which wemodel the process of gesture formulation withBayesian decision networks (BDNs) that supple-ment standard Bayesian networks by decisionnodes.
This formalism provides a representation ofa finite sequential decision problem, combiningprobabilistic and rule-based decision-making.
Eachdecision to be made in the formation of an iconicgesture (e.g., whether or not to gesture at all orwhich representation technique to use) is repre-sented in the network either as a decision node(rule-based) or as a chance node with a specificprobability distribution.
Factors which contributeto these choices (e.g., visuo-spatial referent fea-tures) are taken as input to the model (see Figure 2)The structure of the network as well as local condi-tional probability tables are learned from theSAGA corpus by means of automated machine91learning techniques and supplemented with rule-based decision making.
Individual as well as gen-eral networks are learned from the SAGA corpusby means of automated machine learning tech-niques and supplemented with rule-based decisionmaking.
So far, three different factors have beenincorporated into this model: discourse context, thepreviously performed gesture, and features of thereferent.
The latter are extracted from a hierarchi-cal representation called Imagistic DescriptionTrees (IDT), which is designed to cover all deci-sive visuo-spatial features of objects one finds iniconic gestures (Sowa & Wachsmuth, 2009).
Eachnode in an IDT contains an imagistic descriptionwhich holds a schema representing the shape of anobject or object part.
Features extracted from thisrepresentation in order to capture the main charac-teristics of a gesture?s referent are whether an ob-ject can be decomposed into detailed subparts(whole-part relations), whether it has any symmet-rical axes, its main axis, its position in the VRstimulus, and its shape properties extracted on theare not only present in thely in terms of likeability, competence andcommunicative intentdescribe the landmark townhall with respect tocular speaker)sulting in a posterior distribution of probabilitiesnique is decided to be ?drawing?, to bebasis of so called multimodal concepts (see Berg-mann & Kopp, 2008).Analyzing the GNetIc modelling results enabled usto gain novel insights into the production processof iconic gestures: the resulting networks for indi-vidual speakers differ in their structure and in theirconditional probability distributions, revealing thatindividual differencesovert gestures, but also in the production processthey originate from.The GNetIc model has been extensively evaluated.First, in a prediction-based evaluation, the auto-matically generated gestures were comparedagainst their empirically observed counterparts,which yielded very promising results (Bergmann &Kopp, 2010).
Second, we evaluated the GNetIcmodels in a perception-based evaluation study withhuman addressees.
Results showed that GNetIc-generated gestures actually helped to increase theperceived quality of object descriptions given byMAX.
Moreover, gesturing behaviour generatedwith individual speaker networks was rated morepositivehuman-likeness (Bergmann, Kopp & Eyssel,2010).GNetIc gesture formulation has been embedded ina larger production architecture for speech and ges-ture production.
This architecture comprises mod-ules that carry out content planning, formulation,and realisation for speech and gesture separately,but in close and systematic coordination (Berg-mann & Kopp, 2009).
To illustrate gesture genera-tion on the basis of GNetIc models, consider thefollowing example starting upon the arrival of amessage which specifies thetoits characteristic properties:lmDescrProperty (townhall-1).Based on this communicative intention, the imag-istic description of the involved object gets acti-vated and the agent adopts a spatial perspectivetowards it from which the object is to be described(see Figure 3).
The representation is analyzed forreferent features required by the GNetIc model:position, main axis, symmetry, number of subparts,and shape properties.
Regarding the latter, a unifi-cation of the imagistic townhall-1 representationand a set of underspecified shape property repre-sentations (e.g.
for ?longish?, ?round?
etc.)
reveals?U-shaped?
as the most salient property to be de-picted.
All evidence available  (referent features,discourse context, previous gesture and linguisticcontext) is propagated through the network(learned from the data of  one partirefor the values in each chance node.Figure 3: The townhall in the virtual world (left) and sche-matic of the corresponding IDT content (right); activated partsare marked.This way, it is first decided to generate a gesture inthe current discourse situation at all, the represen-ation techtrealized with both hands and the pointing hand-shape ASL-G. Next, the model?s decision nodesare employed to decide on the palm and back ofhand (BoH) orientation as well as movement typeand direction: as typical in drawing gestures, thepalm is oriented downwards and the BoH awayfrom the speaker?s body.
These gesture features arecombined with a linear movement  consisting oftwo segments per hand (to the right and backwardswith the right hand; accordingly mirror-symmetrical with the left hand) to depict the shapeof the townhall.Accompanying speech is generated from selectedpropositional facts using an NLG engine.
Syn-92chrony between speech and gesture follows co-expressivity and is set to hold between the gesturewould approach the town-hall andinitialsequenttransitionore than a repetition of the wordRouter: Das ist dann das Rathaus [placing].This is then the townhall [placing].Das ist ein u-f?rmiges Geb?ude [drawing].That is a U-shaped building [drawing].Du blickst praktisch da rein [shaping].
Ystroke (depicting the U-shape property) and corre-sponding linguistic element.
These values are usedto fill the slots of a gesture feature matrix which istransformed into an XML representation to be real-ized with the virtual agent MAX (see Figure 4).Figure 4: Specification (left) and realization (right) of anautonomously generated drawing gesture which depicts the U-haped townhall.
s4 Example of a Minimal DiscourseTo start with the analysis of how gestures are notonly employed to carry referential content but alsoto regulate dialogue and discourse, we first presenta datum from the SAGA corpus showing how theFollower?s gesture aligns with the Router?s gestureto indicate acknowledgement or accept.
The situa-tion is as follows: the Router describes to the Fol-lower that hehow it looks to him.
A transcription of thedialogue passage by the Router and the subcrucial speech-gesture annotation, including theFollower, in ELAN looks as displayed in Figure 5(placing, drawing, and shaping are names of anno-tated gestural representation techniques).A short comment on the data might be in order:When introducing the townhall as a U-shapedbuilding, the Router draws the boundary of it,namely a ?U?.
He then goes on to describe how theon-looker apprehends the building.
This is accom-panied by a forward-oriented direction gesture withboth hands, mimicking into it.
In principle, all theinformation necessary to identify the townhallfrom a front perspective is given by then.
There isa short pause and we also have a turnrelevance place here.
However, there is no feed-back by the Follower at this point.
Therefore theRouter selects a typical pattern for self-repairs orcontinuations in German, a that is construction inthe guise of a propositional apposition.
Overlap-ping the production of kind, he produces a three-dimensional partial U-shaped object maintainingthe same perspective as in his first drawing of theU-shaped border.Observe that the Follower already gives feedbackafter front.
The most decisive contribution is theFollower?s acknowledgement, however.
She imi-tates the Router?s gesture but from her perspectiveas a potential observer.
Also, at the level of singleform features, she performs the gesture differently.
(different movement direction, different symmetry)The imitating gesture overlaps with her nod andher contribution OK.
It is important to see that hergesture provides mtownhall could possibly give.
It refers at the sametime to the town-hall (standing for a discourse ref-erent) and provides the information of a U-shapeindicating property, in other words, it expresses thepropositional information ?This building being U-shaped?
with this building acting as a definiteanaphora to the occurrence of a building in the firstpart of the Router?s contribution.
Hence, assessedfrom a dialogue perspective the following happens:The grounding process triggered by the Follower?sacknowledgement amounts to mutual belief amongRouter and Follower that the town hall is U-shapedand the approaching on-looker on the route per-ceives it from the open side of the U.olook practically there into it  [shaping].Das heisst, es hat vorne so zwei BuchtungenThat is, it has to the front kind of two bulges.und geht hinten zusammen dann.
and closes ithe rear then.Figure 5: Example showing the Router?s and the Fol-lower?s gestures and their crucial exchange in terms ofthe Router?s assertion and the Follower?s acknowl-edgement.93Figure 6: Overview of the production and understanding cycle in the simulation model.5 Extending the Simulation: The Under-standing-Acceptance/Generation CycleHow can we go beyond the simulation of isolatedspeaker-specific gestures towards the generation ofgestures in dialogues?
We build on our findings inthe corpus study, briefly taken up here again (seelist in section 2 and the respective comments):Gesture helps in structuring the dialogue support-nment of the current speaker?s (Router?s or Fol-re 5 (R1) and the sub-e fact that the BDN-ing next speaker selection or indicating non-regularco tributions of other speaker.
It enables assess-lower?s) communicative intentions by the ad-dressee, for example of whether the Router wantsto keep the turn but indicates current memory andrecapitulation problems thus appealing to the ad-dressee?s cooperation.
In addition, appraisal of thereliability of the information given by the Routercan be read off from some of the Router?s gestures.Finally, as shown in section 4, gestures comple-menting or even replacing verbal information isused in acknowledgements.Building on these observations, our goal is tosimulate such dialogic interaction with two virtualagents (Router and Follower), each of whom pro-vided with a speaker-specific GNetIc model.
In theminimal discourse example Router and Followeruse similar gestures which, notably, differ withrespect to some details (e.g.
speaker?s perspective).In the simulation we essentially capture theRouter?s contribution in Figusequent acknowledgement by the Follower (F1).
Inorder to vary the Router?s gesturing behavior weuse the representation technique of drawing insteadof shaping in the simulation.What we need to extend the model with is ananalysis of the Follower?s understanding of theRouter?s gesture.
Psychologically plausible butbeyond commonly specialised technical ap-proaches, we want to employ the same model of anagent?s ?gesture knowledge?
for both generatingand understanding gestures.
For an overview of theproduction and understanding cycle see Figure 6.Here we can make use of thformalism allows for two different types of infer-ence, causal inferences that follow the causal interactions from cause to effect, and diagnostic infer-ences that allow for introducing evidence for ef-fects and infer the most likely causes of these ef-fects.
This bi-directional use of BDNs could becomplementary to approaches of plan/intentionrecognition such as in Geib and Goldman (2003).To model a use of gestures for regulation as ob-served with the Follower F1, the Router agent?sgestural activity is set as evidence for the outputnodes of the Follower?s BDN.
A diagnostic infer-ence then yields the most likely causes, that is, themost likely referent properties and values of dis-course contextual variables.
In other words, weemploy the same speaker-specific GNetIc modelfor generation and for understanding.
That is, information about the physical appearance of the94Router?s gesture (as specified in Figure 4) is pro-vided as evidence for the Follower?s GNetIc modelrevealing?correctly?that the gesture?s representa-tion technique is ?drawing?
and the shape propertyis ?U-shaped?.Notably, just as the gesture generation process hasto make choices between similarly probable alter-natives, not all diagnostic inferences which aredrawn by employing the Follower agent?s GNetIcmodel are necessarily in line with the evidencefrom which the Router agent?s gesture was origi-nally generated.
For instance, the communicativegoal as inferred by the Follower agent is?lmDescrPosition?
(with a likelihood of .65) in-simulate such iconic ges-ntsgue structure such asext speaker selection or acknowledgement andouter?se?posed in classical CA researchbackparticipantsstead of ?lmDescrProperty?.
Nevertheless, the in-ferred knowledge reveals an underspecified repre-sentation of the referent (see Figure 7) as well asthe most likely specification of the discourse con-text.
That way, the Follower agent develops hisown hypothesis of the Router agent?s communica-tive goal and the content being depicted gesturally.This hypothesis is forwarded to the followeragent?s dialogue manager, which responds to suchdeclaratives by the Router with an acknowledge-ment grounding act.
Now the very same generationprocess as described in section 3 sets in.
The Fol-lower agent?s feedback is generated by employinghis GNetIc model for causal inference.
The result-ing gesture is, notably, different from the Routeragent?s gesture: it is a two-handed shaping gesturewith handshape ASL-C. Movement type andmovement features are the same as in the Routeragent?s drawing gesture.
Palm and BoH orientationare different due to representation technique spe-cific patterns which are implemented in the deci-sion nodes (see Figure 7).
This case of using iconicgesture for regulating dialogue has been success-fully implemented using GNetIc and the overallproduction architecture.6 Discussion and further research agendaIn this paper we addressed the dialogue-regulatingfunction of gestures.
Based on empirical observa-tions of interactional patterns from the SAGA cor-pus, the starting points for the simulation of thesegestures were non-interactional propositional onessuch as iconics used to describe routes or land-marks.
We achieved totures used in their function as acknowledgemeshown in section 3 which clearly transcends theirmere representational task.Figure 7: Imagistic representation of what the Follower un-derstood from the Router?s gestural depiction of the townhall(left) and the simulation of the Follower?s autonomously gen-erated shaping gesture used as an acknowledgement.We first note that we draw a distinction betweengestures relevant for dialonthose which focus on influencing the social climateamong the dialogue participants.
We did not havemany of the latter in SAGA but observed somewhich we classified as ?calming down?
and ?don?tbother?.
In certain communication cultures alsotouching the other?s body is accepted.As for a research agenda to elucidate further thefunctions of gestures in dialogue, we do not go toodeeply into matters of dialogue theory here.
Wealready have shown that gestures accompanyingbase-line information, being part of the Rreport or the Follower?s uptake can be modelled inPTT (Poesio and Rieser 2009, Rieser and Poesio2009), if one assumes a unified representation forverbal and gestural meaning.
Here we concentrateon how the simulation work can be pushed forwardbased on theoretical analyses of empirical data.Note that on the list of discourse gestures given insection 2 the following items are tied to Router?sbehaviour and can be generated in an autonomousfashion:?
managing of own turn?
evidentials for establishing a confidencelevel?
assessment of relevance by discourse par-ticipants?
indication of topicality with respect to time,place or objects.Observe, however, that these will also have an im-pact on the mental state of the Follower as is e.g.,obvious for evidentials or the ?brush away gestur(Figure 1).
Relevant for the sequencing of multi-modal contributions are clearly the following:?
mechanisms of next-speaker selection aspro?
grounding acts and feed?
handling of non-canonical moves by dis-course95?
clarification requests to work on contribu-tions.Thof adj ing a current and a nextcep-tanthis ki ation.AcThi rthees-ogue.
Personality andlletin, 21(4):394?405Kopp, S. (2009).
Increasing expres-siveness for virtual agents?Autonomous generationd gesture in spatial description tasks.
Inn-n-CaClaGeGilgyGinress).Ha 9-2011): Dialogue Struc-Ha ch-ing GestureLev 983).
Pragmatics.
Cambridge Uni-L?cM.
Kipp et al (Eds.
),McPoend et al (Eds.
), Proceedings of the 13th Work-Rieand Wachsmuth (Eds.
),Rie io, M. (2009).
Interactive Gesture inPoe ordi-, 1?89Rod-SackingStaSow 9).
A computationalguage and Dialogue, pages 132?146.
Oxford University Press.ese are intrinsically involved in the productionacency pairs, havcontribution and it is on these that simulation willfocus on in future work.
In combination with aninformation state-based multimodal discourse re-cord (Traum & Larsson, 2003), the implementatedcycle of generation, understanding and acce/generation provides the basis for modelingnd of gesture-based discourse regulknowledgmentsesearch is partially ss upported by the DFG inCRC 673 ?Alignment in Communication?
andthe Center of Excellence ?Cognitive InteractionTechnology?.ReferencesAsher, N. and Lascarides, A.
(2003).
The Logic of Con-versation.
Cambridge University PressBavelas, J., Chovil, N., Lawrie, D., and Wade, A.(1992).
Interactive gestures.
Discourse Processes,15(4):469?491.Bavelas, J., Chovil N., Coated, L., Roe, L. (1995).
Gtures Specialised for DialSocial Psychology BuBergmann, K., & Kopp, S. (2010).
Modelling the Pro-duction of Co-Verbal Iconic Gestures by LearningBayesian Decision Networks.
Applied Artificial In-telligence, 24(6):530?551.Bergmann, K. &of speech anProceedings of AAMAS 2009, pages 361?368.Bergmann, K. & Kopp, S. (2009a).
GNetIc?UsingBayesian Decision Networks for iconic gesture geeration.
In Proceedings of the 9th International Coference on Intelligent Virtual Agents, pages 76?89.rgmann, K., KoppBe , S., and Eyssel, F. (2010).
Indi-vidualized gesturing outperforms average gesturing?Evaluating gesture production in virtual humans.
InProceedings of IVA 2010, pages 104?117, Ber-lin/Heidelberg.
Springer.ssell, J. and S. Prevost (1996).
Distribution of Seman-tic Features Across Speech and Gesture by Humansand Computers.
Proceedings of the Workshop on theIntegration of Gesture in Language and Speech.rk, H.H.
(1996).
Using Language.
CUPib, C., Goldman, R.,(2003).
Recognizing Plan/GoalAbandonment.
In Proceedings of the InternationalJoint Conference on Artificial Intelligence (IJCAI),pp.
1515?1517.l, S. P., Kawamori, M., Katagiri, Y., and Shimojima,A.
(1999).
Pragmatics of body moves.
In Proceed-ings of the 3rd International Cognitive TechnoloConference, pages 345?358.zburg, J.
(2011).
The Interactive Stance.
Meaningfor Conversation.
Oxford University Press (in phn, F. and Rieser, H. (200ture Gestures and Interactive Gestures.
Manual, 1stversion.
CRC 673 Working Paper.
Bielefeld Univer-sityhn, F. and Rieser, H. (2010): Explaining SpeeGesture Alignment in MM Dialogue UsTypology.
In P. Lupowski and M. Purver (Eds.
), As-pects of Semantics and Pragmatics of Dialogue.SemDial 2010, pp.
99?111.inson, St. C. (1versity Press.king, A., Bergmann, K., Hahn, F., Kopp, S., & Rie-ser, H. (2010): The Bielefeld Speech and GestureAlignment Corpus (SaGA).
InLREC 2010 Workshop: Multimodal Corpora.Neill, D. (1992).
Hand and Mind.
Chicago Univer-sity Press.sio, M. & Rieser, H. (2009).
Anaphora and DirectReference: Empirical Evidence from Pointing.
In J.Edlushop on the Semantics and Pragmatics of Dialogue(DiaHolmia) (pp.
35?43).
Stockholm, Sweden.ser, H. (2010).
On Factoring out a Gesture Typologyfrom the Bielefeld Speech-And-Gesture-AlignmentCorpus (SAGA).
In KoppProceedings of GW 2009.
Springer, pp.
47?61.ser, H. & PoesDialogue: a PTT Model.
In P. Healey et al (Eds.
),Proceedings of the SIGDIAL 2009 Conference (pp.87?96).
London, UK: ACL.sio, M. and Rieser, H. (2010).
Completions, conation and alignment in dialogue.
Dialogue and Dis-course 1(1)Poesio, M. and Traum, D. (1997).
Conversational ac-tions and discourse situations.
Computational Intel-ligence, 13(3): 309?347que, A. and Traum, D. (2008).
Degrees of GroundingBased on Evidence of Understanding.
In Proceeings of the 9th SIGdial Workshop on Discourse andDialogue, pp.
54?63ks, H., Schegloff, E., Jefferson, G. (1974).
A sim-plest systematics for the organization of turn-tafor conversation.
Language, 50: 696?735lnaker, R. (1978): Assertion.
In Cole, P.
(Ed.)
Syntaxand Semantics 9: Pragmatics, pp.
315?322.a, T. and Wachsmuth, I.
(200model for the representation an processing of shapein coverbal iconic gestures.
In K. Coventry et al(Eds.
), Spatial Lan96Traum, D. (1999).
Computational models of groundinin collaborative systems.
In Working Notes of AAAIFall Symposium on Psycholgogical Models of Com-Tra eelt (Eds.
), Current and Newmunication, pp.
124?131.um, D., & Larsson, S. (2003).
The information statapproach to dialogue management.
In R.W.
Smithand J.C.J.
van KuppevDirections in Discourse & Dialogue (pp.
325?353).Kluwer Academic Publishers.97
