Transactions of the Association for Computational Linguistics, 1 (2013) 267?278.
Action Editor: Brian Roark.Submitted 3/2013; Published 7/2013.
c?2013 Association for Computational Linguistics.Efficient Parsing for Head-Split Dependency TreesGiorgio SattaDept.
of Information EngineeringUniversity of Padua, Italysatta@dei.unipd.itMarco KuhlmannDept.
of Linguistics and PhilologyUppsala University, Swedenmarco.kuhlmann@lingfil.uu.seAbstractHead splitting techniques have been success-fully exploited to improve the asymptoticruntime of parsing algorithms for project-ive dependency trees, under the arc-factoredmodel.
In this article we extend these tech-niques to a class of non-projective dependencytrees, called well-nested dependency trees withblock-degree at most 2, which has been previ-ously investigated in the literature.
We define astructural property that allows head splitting forthese trees, and present two algorithms that im-prove over the runtime of existing algorithmsat no significant loss in coverage.1 IntroductionMuch of the recent work on dependency parsing hasbeen aimed at finding a good balance between ac-curacy and efficiency.
For one end of the spectrum,Eisner (1997) showed that the highest-scoring pro-jective dependency tree under an arc-factored modelcan be computed in timeO.n3/, where n is the lengthof the input string.
Later work has focused on mak-ing projective parsing viable under more expressivemodels (Carreras, 2007; Koo and Collins, 2010).At the same time, it has been observed that formany standard data sets, the coverage of projectivetrees is far from complete (Kuhlmann and Nivre,2006), which has led to an interest in parsing al-gorithms for non-projective trees.
While non-project-ive parsing under an arc-factored model can be donein time O.n2/ (McDonald et al 2005), parsing withmore informed models is intractable (McDonald andSatta, 2007).
This has led several authors to investig-ate ?mildly non-projective?
classes of trees, with thegoal of achieving a balance between expressivenessand complexity (Kuhlmann and Nivre, 2006).In this article we focus on a class of mildly non-projective dependency structures called well-nesteddependency trees with block-degree at most 2.
Thisclass was first introduced by Bodirsky et al(2005),who showed that it corresponds, in a natural way, tothe class of derivation trees of lexicalized tree-adjoin-ing grammars (Joshi and Schabes, 1997).
While thereare linguistic arguments against the restriction to thisclass (Maier and Lichte, 2011; Chen-Main and Joshi,2010), Kuhlmann and Nivre (2006) found that it hasexcellent coverage on standard data sets.
Assum-ing an arc-factored model, well-nested dependencytrees with block-degree  2 can be parsed in timeO.n7/ using the algorithm of Go?mez-Rodr?
?guez etal.
(2011).
Recently, Pitler et al(2012) have shownthat if an additional restriction called 1-inherit is im-posed, parsing can be done in time O.n6/, withoutany additional loss in coverage on standard data sets.Standard context-free parsing methods, when adap-ted to the parsing of projective trees, provide O.n5/time complexity.
The O.n3/ time result reported byEisner (1997) has been obtained by exploiting moresophisticated dynamic programming techniques that?split?
dependency trees at the position of their heads,in order to save bookkeeping.
Splitting techniqueshave also been exploited to speed up parsing timefor other lexicalized formalisms, such as bilexicalcontext-free grammars and head automata (Eisnerand Satta, 1999).
However, to our knowledge no at-tempt has been made in the literature to extend thesetechniques to non-projective dependency parsing.In this article we leverage the central idea fromEisner?s algorithm and extend it to the class of well-nested dependency trees with block-degree at most 2.267We introduce a structural property, called head-split,that allows us to split these trees at the positions oftheir heads.
The property is restrictive, meaning thatit reduces the class of trees that can be generated.However, we show that the restriction to head-splittrees comes at no significant loss in coverage, and itallows parsing in timeO.n6/, an asymptotic improve-ment of one order of magnitude over the algorithmby Go?mez-Rodr?
?guez et al(2011) for the unrestric-ted class.
We also show that restricting the class ofhead-split trees by imposing the already mentioned1-inherit property does not cause any additional lossin coverage, and that parsing for the combined classis possible in time O.n5/, one order of magnitudefaster than the algorithm by Pitler et al(2012) forthe 1-inherit class without the head-split condition.The above results have consequences also for theparsing of other related formalisms, such as thealready mentioned lexicalized tree-adjoining gram-mars.
This will be discussed in the final section.2 Head SplittingTo introduce the basic idea of this article, we brieflydiscuss in this section two well-known algorithms forcomputing the set of all projective dependency treesfor a given input sentence: the na?
?ve, CKY-stylealgorithm, and the improved algorithm with headsplitting, in the version of Eisner and Satta (1999).1CKY parsing The CKY-style algorithm works ina pure bottom-up way, building dependency treesby combining subtrees.
Assuming an input stringw D a1    an, n  1, each subtree t is representedby means of a finite signature ?i; j; h?, called item,where i; j are the boundary positions of t ?s span overw and h is the position of t?s root.
This is the onlyinformation we need in order to combine subtreesunder the arc-factored model.
Note that the numberof possible signatures is O.n3/.The main step of the algorithm is displayed inFigure 1(a).
Here we introduce the graphical conven-tion, used throughout this article, of representing asubtree by a shaded area, with an horizontal line in-dicating the spanned fragment of the input string, andof marking the position of the head by a bullet.
Theillustrated step attaches a tree with signature ?k; j; d ?1Eisner (1997) describes a slightly different algorithm.
(a)ah adi k j)ah adi j(b)ah adk)ah ad(c)ah adj)ah adjFigure 1: Basic steps for (a) the CKY-style algorithmand (b, c) the head splitting algorithm.as a dependent of a tree with signature ?i; k; h?.
Therecan be O.n5/ instantiations of this step, and this isalso the running time of the algorithm.Eisner?s algorithm Eisner and Satta (1999) im-prove over the CKY algorithm by reducing the num-ber of position records in an item.
They do this by?splitting?
each tree into a left and a right fragment,so that the head is always placed at one of the twoboundary positions of a fragment, as opposed to be-ing placed at an internal position.
In this way itemsneed only two indices.
Left and right fragments canbe processed independently, and merged afterwards.Let us consider a right fragment t with head ah.Attachment at t of a right dependent tree with headad is now performed in two steps.
The first step at-taches a left fragment with head ad , as in Figure 1(b).This results in a new type of fragment/item that hasboth heads ah and ad placed at its boundaries.
Thesecond step attaches a right fragment with head ad ,as in Figure 1(c).
The number of possible instanti-ations of these steps, and the asymptotic runtime ofthe algorithm, is O.n3/.In this article we extend the splitting technique tothe class of well-nested dependency trees with block-degree at most 2.
This amounts to defining a fac-torization for these trees into fragments, each withits own head at one of its boundary positions, alongwith some unfolding of the attachment operation intointermediate steps.
While for projective trees headsplitting can be done without any loss in coverage,for the extended class head splitting turns out to bea proper restriction.
The empirical relevance of thiswill be discussed in ?7.2683 Head-Split TreesIn this section we introduce the class of well-nesteddependency trees with block-degree at most 2, anddefine the subclass of head-split dependency trees.3.1 PreliminariesFor non-negative integers i; j we write ?i; j ?
to de-note the set fi; iC1; : : : ; j g; when i > j , ?i; j ?
is theempty set.
For a string w D a1    an, where n  1and each ai is a lexical token, and for i; j 2 ?0; n?with i  j , we write wi;j to denote the substringaiC1    aj of w; wi;i is the empty string.A dependency tree t over w is a directed treewhose nodes are a subset of the tokens ai in w andwhose arcs encode a dependency relation betweentwo nodes.
We write ai !
aj to denote the arc.ai ; aj / in t ; here, the node ai is the head, and thenode aj is the dependent.
If each token ai , i 2 ?1; n?,is a node of t , then t is called complete.
Sometimeswe write tai to emphasize that tree t is rooted in nodeai .
If ai is a node of t , we also write t ?ai ?
to denotethe subtree of t composed by node ai as its root andall of its descendant nodes.The nodes of t uniquely identify a set of max-imal substrings of w, that is, substrings separatedby tokens not in t .
The sequence of such substrings,ordered from left to right, is the yield of t , writtenyd.t/.
Let ai be some node of t .
The block-degreeof ai in t , written bd.ai ; t /, is defined as the numberof string components of yd.t ?ai ?/.
The block-degreeof t , written bd.t/, is the maximal block-degree ofits nodes.
Tree t is non-projective if bd.t/ > 1.Tree t is well-nested if, for each node ai of t and forevery pair of outgoing dependencies ai !
ad1 andai !
ad2 , the string components of yd.t ?ad1 ?/ andyd.t ?ad2 ?/ do not ?interleave?
in w. More precisely,it is required that, if some component of yd.t ?adi ?/,i 2 ?1; 2?, occurs in w in between two componentss1; s2 of yd.t ?adj ?/, j 2 ?1; 2?
and j ?
i , then allcomponents of yd.t ?adi ?/ occur in between s1; s2.Throughout this article, whenever we consider adependency tree t we always implicitly assume thatt is over w, that t has block-degree at most 2, andthat t is well-nested.
Let tai be such a tree, withbd.ai ; tai / D 2.
We call the portion of w in betweenthe two substrings of yd.tai / the gap of tai , denotedby gap.tai /.ah ad4ad3ad2ad1m.tah/Figure 2: Example of a node ah with block-degree 2 in anon-projective, well-nested dependency tree tah .
Integerm.tah/, defined in ?3.2, is also marked.Example 1 Figure 2 schematically depicts a well-nested tree tah with block-degree 2; we have markedthe root node ah and its dependent nodes adi .
Foreach node adi , a shaded area highlights t ?adi ?.
Wehave bd.ah; tah/ D bd.ad1 ; tah/ D bd.ad4 ; tah/ D2 and bd.ad2 ; tah/ D bd.ad3 ; tah/ D 1.
3.2 The Head-Split PropertyWe say that a dependency tree t has the head-splitproperty if it satisfies the following condition.
Letah !
ad be any dependency in t with bd.ah; t / Dbd.ad ; t / D 2.
Whenever gap.t ?ad ?/ contains ah, itmust also contain gap.t ?ah?/.
Intuitively, this meansthat if yd.t ?ad ?/ ?crosses over?
the lexical token ah inw, then yd.t ?ad ?/ must also ?cross over?
gap.t ?ah?/.Example 2 Dependency ah !
ad1 in Figure 3 viol-ates the head-split condition, since yd.t ?ad1 ?/ crossesover the lexical token ah inw, but does not cross overgap.t ?ah?/.
The remaining outgoing dependencies ofah trivially satisfy the head-split condition, since thechild nodes have block-degree 1.
Let tah be a dependency tree satisfying the head-split property and with bd.ah; tah/ D 2.
We specifybelow a construction that ?splits?
tah with respect tothe position of the head ah in yd.tah/, resulting intwo dependency trees sharing the root ah and havingall of the remaining nodes forming two disjoint sets.Furthermore, the resulting trees have block-degree atmost 2.ahad1 ad2 ad3Figure 3: Arc ah !
ad1 violates the head-split condition.269(a)ah ad4ad3(b)ahad2ad1 m.tah/Figure 4: Lower tree (a) and upper tree (b) fragments forthe dependency tree in Figure 2.Let yd.tah/ D hwi;j ; wp;qi and assume that ahis placed within wi;j .
(A symmetric constructionshould be used in case ah is placed withinwp;q .)
Themirror image of ah with respect to gap.tah/, writtenm.tah/, is the largest integer in ?p; q?
such that thereare no dependencies linking nodes in wi;h 1 andnodes in wp;m.tah / and there are no dependencieslinking nodes in wh;j and nodes in wm.tah /;q .
It isnot hard to see that such an integer always exists,since tah is well-nested.We classify every dependent ad of ah as beingan ?upper?
dependent or a ?lower?
dependent ofah, according to the following conditions: (i) Ifd 2 ?i; h   1?
[ ?m.tah/C 1; q?, then ad is an upperdependent of ah.
(ii) If d 2 ?hC 1; j ?
[ ?p;m.tah/?,then ad is a lower dependent of ah.The upper tree of tah is the dependency treerooted in ah and composed of all dependenciesah !
ad in tah with ad an upper dependent ofah, along with all subtrees tah ?ad ?
rooted in thosedependents.
Similarly, the lower tree of tah is thedependency tree rooted in ah and composed of alldependencies ah !
ad in tah with ad a lower de-pendent of ah, along with all subtrees tah ?ad ?
rootedin those dependents.
As a general convention, in thisarticle we write tU;ah and tL;ah to denote the upperand the lower trees of tah , respectively.
Note that, insome degenerate cases, the set of lower or upper de-pendents may be empty; then tU;ah or tL;ah consistsof the root node ah only.Example 3 Consider the tree tah displayed in Fig-ure 2.
Integer m.tah/ denotes the boundary betweenthe right component of yd.tah ?ad4 ?/ and the rightcomponent of yd.tah ?ad1 ?/.
Nodes ad3 and ad4 arelower dependents, and nodes ad1 and ad2 are upperdependents.
Trees tL;ah and tU;ah are displayed inFigure 4 (a) and (b), respectively.
The importance of the head-split property can beinformally explained as follows.
Let ah !
ad be adependency in tah .
When we take apart the upper andthe lower trees of tah , the entire subtree tah ?ad ?
endsup in either of these two fragments.
This allows us torepresent upper and lower fragments for some headindependently of the other, and to freely recombinethem.
More formally, our algorithms will make useof the following three properties, stated here withoutany formal proof:P1 Trees tU;ah and tL;ah are well-nested, have block-degree  2, and satisfy the head-split property.P2 Trees tU;ah and tL;ah have their head ah alwaysplaced at one of the boundaries in their yields.P3 Let t 0U;ah and t 00L;ah be the upper and lower treesof distinct trees t 0ah and t 00ah , respectively.
If m.t 0ah/ Dm.t 00ah/, then there exists a tree tah such that tU;ah Dt 0U;ah and tL;ah D t 00L;ah .4 Parsing ItemsLet w D a1    an, n  1, be the input string.
Weneed to compactly represent trees that span substringsof w by recording only the information that is neededto combine these trees into larger trees during theparsing process.
We do this by associating eachtree with a signature, called item, which is a tuple?i; j ; p; q; h?X , where h 2 ?1; n?
identifies the tokenah, i; j with 0  i  j  n identify a substringwi;j ,and p; q with j < p  q  n identify a substringwp;q .
We also use the special setting p D q D  .The intended meaning is that each item repres-ents some tree tah .
If p; q ?
  then yd.tah/ Dhwi;j ; wp;qi.
If p; q D   thenyd.tah/ D8<?
:?hwi;j i if h 2 ?i C 1; j ?hwh;h; wi;j i if h < ihwi;j ; wh;hi if h > j C 1The two cases h < i and h > j C 1 above willbe used when the root node ah of tah has not yetcollected all of its dependents.Note that h 2 fi; j C 1g is not used in thedefinition of item.
This is meant to avoid differ-ent items representing the same dependency tree,270which is undesired for the specification of our al-gorithm.
As an example, items ?i; j ; ; ; i C 1?Xand ?i C 1; j ; ; ; i C 1?X both represent a depend-ency tree taiC1 with yd.taiC1/ D hwi;j i.
This andother similar cases are avoided by the ban againsth 2 fi; j C 1g, which amounts to imposing somenormal form for items.
In our example, only item?i; j ; ; ; i C 1?X is a valid signature.Finally, we distinguish among several item types,indicated by the value of subscript X .
These typesare specific to each parsing algorithm, and will bedefined in later sections.5 Parsing of Head-Split TreesWe present in this section our first tabular algorithmfor computing the set of all dependency trees for aninput sentence w that have the head-split property,under the arc-factored model.
Recall that tai denotesa tree with root ai , and tL;ai and tU;ai are the lowerand upper trees of tai .
The steps of the algorithmare specified by means of deduction rules over items,following the approach of Shieber et al(1995).5.1 Basic IdeaOur algorithm builds trees step by step, by attachinga tree tah0 as a dependent of a tree tah and creatingthe new dependency ah !
ah0 .
Computationally,the worst case for this operation is when both tahand tah0 have a gap; then, for each tree we need tokeep a record of the four boundaries, along with theposition of the head, as done by Go?mez-Rodr?
?guez etal.
(2011).
However, if we are interested in parsingtrees that satisfy the head-split property, we can avoidrepresenting a tree with a gap by means of a singleitem.
We instead follow the general idea of ?2 forprojective parsing, and use different items for theupper and the lower trees of the source tree.When we need to attach tah0 as an upper dependentof tah , defined as in ?3.2, we perform two consecutivesteps.
First, we attach tL;ah0 to tU;ah , resulting in anew intermediate tree t1.
As a second step, we attachtU;ah0 to t1, resulting in a new tree t2 which is tU;ahwith tah0 attached as an upper dependent, as desired.Both steps are depicted in Figure 5; here we introducethe convention of indicating tree grouping througha dashed line.
A symmetric procedure can be usedto attach tah0 as a lower dependent to tL;ah .
TheahtU;ahah0tL;ah0+t1(a)t1ah0 ahah0tU;ah0+t2(b)Figure 5: Two step attachment of tah0 at tU;ah : (a) attach-ment of tL;ah0 ; (b) attachment of tU;ah0 .correctness of the two step approach follows fromproperties P1 and P3 in ?3.2.By property P2 in ?3.2, in both steps above thelexical heads ah and ah0 can be read from the bound-aries of the involved trees.
Then these steps can beimplemented more efficiently than the na?
?ve methodof attaching tah0 to tah in a single step.
A more de-tailed computational analysis will be provided in ?5.7.To simplify the presentation, we restrict the use ofhead splitting to trees with a gap and parse trees withno gap with the na?
?ve method; this does not affectthe computational complexity.5.2 Item TypesWe distinguish five different types of items, indicatedby the subscriptX 2 f0;L;U; =L; =U g, as describedin what follows. If X D 0, we have p D q D   and yd.ah/ isspecified as in ?4. If X D L, we use the item to represent somelower tree.
We have therefore p; q ?
  andh 2 fi C 1; qg. If X D U , we use the item to represent someupper tree.
We have therefore p; q ?
  andh 2 fj; p C 1g. If X D =L or X D =U , we use the item torepresent some intermediate step in the parsingprocess, in which only the lower or upper tree ofsome dependent has been collected by the headah, and we are still missing the upper (=U ) orthe lower (=L) tree.271We further specialize symbol =U by writing =U<(=U>) to indicate that the missing upper tree shouldhave its head to the left (right) of its gap.
We also use=L< and =L> with a similar meaning.5.3 Item Normal FormIt could happen that our algorithm produces items oftype 0 that do not satisfy the normal form conditiondiscussed in ?4.
To avoid this problem, we assumethat every item of type 0 that is produced by thealgorithm is converted into an equivalent normal formitem, by means of the following rules:?i; j ; ; ; i ?0?i   1; j ; ; ; i ?0 (1)?i; j ; ; ; j C 1?0?i; j C 1; ; ; j C 1?0 (2)5.4 Items of Type 0We start with deduction rules that produce items oftype 0.
As already mentioned, we do not apply thehead splitting technique in this case.The next rule creates trees with a single node, rep-resenting the head, and no dependents.
The rule isactually an axiom (there is no antecedent) and thestatement i 2 ?1; n?
is a side condition.
?i   1; i ; ; ; i ?0?i 2 ?1; n?
(3)The next rule takes a tree headed in ah0 and makesit a dependent of a new head ah.
This rule imple-ments what has been called the ?hook trick?.
The firstside condition enforces that the tree headed in ah0has collected all of its dependents, as discussed in ?4.The second side condition enforces that no cycle iscreated.
We also write ah !
ah0 to indicate that anew dependency is created in the parse forest.
?i; j ; ; ; h0?0?i; j ; ; ; h?08<:h0 2 ?i C 1; j ?h 62 ?i C 1; j ?ah !
ah0(4)The next two rules combine gap-free dependentsof the same head ah.
?i; k; ; ; h?0 ?k; j ; ; ; h?0?i; j ; ; ; h?0 (5)?i; h; ; ; h?0 ?h   1; j ; ; ; h?0?i; j ; ; ; h?0 (6)We need the special case in (6) to deal with the con-catenation of two items that share the head ah at theconcatenation point.
Observe the apparent mismatchin step (6) between index h in the first antecedentand index h   1 in the second antecedent.
This isbecause in our normal form, both the first and thesecond antecedent have already incorporated a copyof the shared head ah.The next two rules collect a dependent of ah thatwraps around the dependents that have already beencollected.
As already discussed, this operation isperformed by two successive steps: We first collectthe lower tree and then the upper tree.
We presentthe case in which the shared head of the two trees isplaced at the left of the gap.
The case in which thehead is placed at the right of the gap is symmetric.
?i 0; j 0; ; ; h?0?i; i 0; j 0; j ; i C 1?L?i; j ; ; ; h?=U<h 62 ?i C 1; i 0?
[ ?j 0 C 1; j ?
(7)?i 0; j 0; ; ; h?=U<?i; i 0 C 1; j 0; j ; i 0 C 1?U?i; j ; ; ; h?08<:h 62 ?i C 1; i 0 C 1?
[ ?j 0 C 1; j ?ah !
ai 0C1(8)Again, there is an overlap in rule (8) between thetwo antecedents, due to the fact that both items havealready incorporated copies of the same head.5.5 Items of Type UWe now consider the deduction rules that are neededto process upper trees.
Throughout this subsectionwe assume that the head of the upper tree is placed atthe left of the gap.
The other case is symmetric.
Thenext rule creates an upper tree with a single node, rep-resenting its head, and no dependents.
We constructan item for all possible right gap boundaries j .
?i   1; i ; j; j ; i ?Ui 2 ?1; n?j 2 ?i C 1; n?
(9)The next rule adds to an upper tree a group of newdependents that do not have any gap.
We present thecase in which the new dependents are placed at theleft of the gap of the upper tree.
?i; i 0; ; ; j ?0 ?i 0; j ; p; q; j ?U?i; j ; p; q; j ?U (10)272The next two rules collect a new dependent thatwraps around the upper tree.
Again, this operation isperformed by two successive steps: We first collectthe lower tree, then the upper tree.
We present thecase in which the shared head of the two trees isplaced at the left of the gap.
?i 0; j ; p; q0; j ?U ?i; i 0; q0; q; i C 1?L?i; j ; p; q; j ?=U< (11)?i 0; j ; p; q0; j ?=U<?i; i 0 C 1; q0; q; i 0 C 1?U?i; j ; p; q; j ?U?aj !
ai 0C1 (12)5.6 Items of Type LSo far we have always expanded items (type 0 or U )at their external boundaries.
When dealing with lowertrees, we have to reverse this strategy and expanditems (type L) at their internal boundaries.
Apartfrom this difference, the deduction rules below areentirely symmetric to those in ?5.5.
Again, we as-sume that the head of the lower tree is placed atthe left of the gap, the other case being symmetric.Our first rule creates a lower tree with a single node,representing its head.
We blindly guess the rightboundary of the gap of such a tree.
?i   1; i ; j; j ; i ?Li 2 ?1; n?j 2 ?i C 1; n?
(13)The next rule adds to a lower tree a group of newdependents that do not have any gap.
We present thecase in which the new dependents are placed at theleft of the gap of the lower tree.
?j 0; j ; ; ; i C 1?0 ?i; j 0; p; q; i C 1?L?i; j ; p; q; i C 1?L (14)The next two rules collect a new dependent witha gap and embed it within the gap of our lower tree,creating a new dependency.
Again, this operation isperformed by two successive steps, and we presentthe case in which the common head of the lower andupper trees that are embedded is placed at the left ofthe gap, the other case being symmetric.
?i; j 0; p0; q; i C 1?L ?j 0; j ; p; p0; j ?U?i; j ; p; q; i C 1?=L< (15)?i; j 0; p0; q; i C 1?=L<?j 0   1; j ; p; p0; j 0?L?i; j ; p; q; i C 1?L?aiC1 !
aj 0 (16)ahad1 ad2 ad3 ad4 ad5tU;ah tLL;ah tLR;ahFigure 6: Node ah satisfies both the 1-inherit and head-split conditions.
Accordingly, tree tah can be split intothree fragments tU;ah , tLL;ah and tLR;ah .5.7 RuntimeThe algorithm runs in time O.n6/, where n is thelength of the input sentence.
The worst case is dueto deduction rules that combine two items, each ofwhich represents trees with one gap.
For instance,rule (11) involves six free indices ranging over ?1; n?,and thus could be instantiated O.n6/ many times.
Ifthe head-split property does not hold, attachment of adependent in one step results in time O.n7/, as seenfor instance in Go?mez-Rodr?
?guez et al(2011).6 Parsing of 1-Inherit Head-Split TreesIn this section we specialize the parsing algorithmof ?5 to a new, more efficient algorithm for a restric-ted class of trees.6.1 1-Inherit Head-Split TreesPitler et al(2012) introduce a restriction on well-nes-ted dependency trees with block-degree at most 2.A tree t satisfies the 1-inherit property if, for everynode ah in t with bd.ah; t / D 2, there is at mostone dependency ah !
ad such that gap.t ?ad ?/contains gap.t ?ah?/.
Informally, this means thatyd.t ?ad ?/ ?crosses over?
gap.t ?ah?/, and we say thatad ?inherits?
the gap of ah.
In this section we in-vestigate the parsing of head-split trees that also havethe 1-inherit property.Example 4 Figure 6 shows a head node ah alongwith dependents adi , satisfying the head-split condi-tion.
Only tad1 has its yield crossing over gap.tah/.Thus ah also satisfies the 1-inherit condition.
6.2 Basic IdeaLet tah be some tree satisfying both the head-splitproperty and the 1-inherit propery.
Assume that thedependent node ad which inherits the gap of tahis placed within tU;ah .
This means that, for every273dependency ah !
ad in tL;ah , yd.t ?ad ?/ does notcross over gap.tL;ah/.
Then we can further splittL;ah into two trees, both with root ah.
We call thesetwo trees the lower-left tree, written tLL;ah , and thelower-right tree, written tLR;ah ; see again Figure 6.The basic idea behind our algorithm is to split tahinto three dependency trees tU;ah , tLL;ah and tLR;ah ,all sharing the same root ah.
This means that tahcan be attached to an existing tree through three suc-cessive steps, each processing one of the three treesabove.
The correctness of this procedure followsfrom a straightforward extension of properties P1 andP3 from ?3.2, stating that the tree fragments tU;ah ,tLL;ah and tLR;ah can be represented and processedone independently of the others, and freely combinedif certain conditions are satisfied by their yields.In case ad is placed within tL;ah , we introducethe upper-left and the upper-right trees, writtentUL;ah and tUR;ah , and apply a similar idea.6.3 Item TypesWhen processing an attachment, the order in whichthe algorithm assembles the three tree fragments oftah defined in ?6.2 is not always the same.
Such anorder is chosen on the basis of where the head ahand the dependent ad inheriting the gap are placedwithin the involved trees.
As a consequence, in ouralgorithm we need to represent several intermediateparsing states.
Besides the item types from ?5.2, wetherefore need additional types.
The specificationof these new item types is rather technical, and istherefore delayed until we introduce the relevant de-duction rules.6.4 Items of Type 0We start with the deduction rules for parsing oftrees tLL;ah and tLR;ah ; trees tUL;ah and tUR;ah can betreated symmetrically.
The yields of tLL;ah and tLR;ahhave the form specified in ?4 for the case p D q D  .We can therefore use items of type 0 to parse thesetrees, adopting a strategy similar to the one in ?5.4.The main difference is that, when a tree tah0 with agap is attached as a dependent to the head ah, weuse three consecutive steps, each processing a singlefragment of tah0 .
We assume below that tah0 can besplit into trees tU;ah0 , tLL;ah0 and tLR;ah0 , the othercase can be treated in a similar way.We use rules (3), (4) and (5) from ?5.4.
Since inad ah????1????2????3???
?4tad tU;ad tLL;ad tLR;adFigure 7: Tree tU;ah is decomposed into tad and subtreescovering substrings i , i 2 ?1; 4?.
Tree tad is in turndecomposed into three fragments (trees tLL;ad , tLR;ad ,and tU;ad in this example).the trees tLL;ah and tLR;ah the head is never placed inthe middle of the yield, rule (6) is not needed nowand it can safely be discarded.
Rule (7), attachinga lower tree, needs to be replaced by two new rules,processing a lower-left and a lower-right tree.
Weassume here that the common head of these trees isplaced at the left boundary of the lower-left tree; weleave out the symmetric case.
?i; i 0; ; ; i C 1?0?i 0; j ; ; ; h?0?i; j ; ; ; h?=LR<?h 62 ?i C 1; i 0?
(17)?j 0; j ; ; ; i C 1?0?i; j 0; ; ; h?=LR<?i; j ; ; ; h?=U<?h 62 ?j 0 C 1; j ?
(18)The first antecedent in (17) encodes a lower-left treewith its head at the left boundary.
The consequentitem has then the new type =LR<, meaning that alower-right tree is missing that must have its headat the left.
The first antecedent in (18) provides themissing lower-right tree, having the same head asthe already incorporated lower-left tree.
After theserules are applied, rule (8) from ?5.4 can be appliedto the consequent item of (18).
This completes theattachment of a ?wrapping?
dependent of ah, with theincorporation of the missing upper tree and with theconstruction of the new dependency.6.5 Items of Type UWe now assume that node ad is realized withintU;ah , so that tah can be split into trees tU;ah , tLL;ahand tLR;ah .
We provide deduction rules to parse oftU;ah ; this is the most involved part of the algorithm.In case ad is realized within tL;ah , tah must besplit into tL;ah , tUL;ah and tUR;ah , and a symmetricalstrategy can be applied to parse tL;ah .274ad ah1 2 3 4tU;adtLL;ad tLR;adrule (19)rule (20)Figure 8: Decomposition of tU;ah as in Figure 7, withhighlighted application of rules (19) and (20).We start by observing that yd.tad / splitsyd.tU;ah/ into at most four substrings i ; see Fig-ure 7.2 Because of the well-nested property, withinthe tree tU;ah each dependent of ah other than adhas a yield that is entirely placed within one of thei ?s substrings.
This means that each substring ican be parsed independently of the other substrings.As a first step in the process of parsing tU;ah , weparse each substring i .
We do this following theparsing strategy specified in ?6.4.
As a second step,we assume that each of the three fragments resultingfrom the decomposition of tree tad has already beenparsed; see again Figure 7.
We then ?merge?
thesethree fragments and the trees for segments i ?s intoa complete parse tree representing tU;ah .
This isdescribed in detail in what follows.We assume that ah is placed at the left of the gapof tU;ah (the right case being symmetrical) and wedistinguish four cases, depending on the two ways inwhich tad can be split, and the two side positions ofthe head ad with respect to gap.tad /.Case 1 We assume that tad can be split into treestU;ad , tLL;ad , tLR;ad , and the head ad is placedat the left of gap.tad /; see again Figure 7.Rule (19) below combines tLL;ad with a parse forsegment 2, which has its head ah placed at its rightboundary; see Figure 8 for a graphical representationof rule (19).
The result is an item of the new type HH.This item is used to represent an intermediate treefragment with root of block-degree 1, where both theleft and the right boundaries are heads; a dependency2According to our definition of m.tah/ in ?3.2, 3 is alwaysthe empty string.
However, here we deal with the general formu-lation of the problem in order to claim in ?8 that our algorithmcan be directly adapted to parse some subclasses of lexicalizedtree-adjoining grammars.ahad1 2 3 4tU;adtLL;adtLR;adrule (22) rule (23)Figure 9: Decomposition of tU;ah as in Figure 7, withhighlighted application of rules (22) and (23).between these heads will be constructed later.
?i; i 0; ; ; i C 1?0 ?i 0; j ; ; ; j ?0?i; j ; ; ; j ?HH (19)Rule (20) combines tU;ad with a type 0 item rep-resenting tLR;ad ; see again Figure 8.
Note that thiscombination operation expands an upper tree at oneof its internal boundaries, something that was notpossible with the rules specified in ?5.5.
?i; j ; p0; q; j ?U ?p; p0; ; ; j ?0?i; j ; p; q; j ?U (20)Finally, we combine the consequents of (19)and (20), and process the dependency that was leftpending in the item of type HH.
?i; j 0; p; q; j 0?U?j 0   1; j ; ; ; j ?HH?i; j ; p; q; j ?U?aj !
aj 0 (21)After the above steps, parsing of tU;ah can be com-pleted by combining item ?i; j ; p; q; j ?U from (21)with items of type 0 representing parses for the sub-strings 1, 3 and 4.Case 2 We assume that tad can be split into treestU;ad , tLL;ad , tLR;ad , and the head ad is placedat the right of gap.tad /, as depicted in Figure 9.Rule (22) below, graphically represented in Fig-ure 9, combines tU;ad with a type 0 item represent-ing tLL;ad .
This can be viewed as the symmetricversion of rule (20) of Case 1, expanding an uppertree at one of its internal boundaries.
?i; j 0; p; q; p C 1?U ?j 0; j ; ; ; p C 1?0?i; j ; p; q; p C 1?U (22)275Arabic Czech Danish Dutch Portuguese SwedishNumber of trees 1,460 72,703 5,190 13,349 9,071 11,042WN2 O.n7/ 1,458 99.9% 72,321 99.5% 5,175 99.7% 12,896 96.6% 8,650 95.4% 10,955 99.2%Classes considered in this paperWN2 + HS O.n6/ 1,457 99.8% 72,182 99.3% 5,174 99.7% 12,774 95.7% 8,648 95.3% 10,951 99.2%WN2 + HS + 1I O.n5/ 1,457 99.8% 72,182 99.3% 5,174 99.7% 12,774 95.7% 8,648 95.3% 10,951 99.2%Classes considered by Pitler et al(2012)WN2 + 1I O.n6/ 1,458 99.9% 72,321 99.5% 5,175 99.7% 12,896 96.6% 8,650 95.4% 10,955 99.2%WN2 + 0I O.n5/ 1,394 95.5% 70,695 97.2% 4,985 96.1% 12,068 90.4% 8,481 93.5% 10,787 97.7%Projective O.n3/ 1,297 88.8% 55,872 76.8% 4,379 84.4% 8,484 63.6% 7,353 81.1% 9,963 90.2%Table 1: Coverage of various classes of dependency trees on the training sets used in the CoNLL-X shared task (WN2 =well-nested, block-degree  2; HS = head-split; 1I = 1-inherit; 0I = 0-inherit, ?gap-minding?
)Next, we combine the result of (22) with a parse forsubstring 2.
The result is an item of the new type=LR>.
This item is used to represent an intermediatetree fragment that is missing a lower-right tree withits head at the right.
In this fragment, two headsare left pending, and a dependency relation will beeventually established between them.
?i; j 0; p; q; p C 1?U ?j 0; j ; ; ; j ?0?i; j ; p; q; j ?=LR> (23)The next rule combines the consequent item of (23)with a tree tLR;ad having its head at the right bound-ary, and processes the dependency that was leftpending in the =LR> item.
?i; j ; p0; q; j ?=LR>?p; p0 C 1; ; ; p0 C 1?0?i; j ; p; q; j ?U?aj !
ap0C1 (24)After the above rules, parsing of tU;ah continues bycombining the consequent item ?i; j ; p; q; j ?U fromrule (24) with items representing parses for the sub-strings 1, 3 and 4.Cases 3 and 4 We informally discuss the cases inwhich tad can be split into trees tL;ad , tUL;ad ,tUR;ad , for both positions of the head ad with re-spect to gap.tad /.
In both cases we can adopt astrategy similar to the one of Case 2.We first expand tL;ad externally, at the side op-posite to the head ad , with a tree fragment tUL;ador tUR;ad , similarly to rule (22) of Case 2.
Thisresults in a new fragment t1.
Next, we merge t1with a parse for 2 containing the head ah, similarlyto rule (23) of Case 2.
This results in a new frag-ment t2 where a dependency relation involving theheads ad and ah is left pending.
Finally, we merget2 with a missing tree tUL;ad or tUR;ad , and pro-cess the pending dependency, similarly to rule (24).One should contrast this strategy with the alternativestrategy adopted in Case 1, where the fragment oftad having block-degree 2 cannot be merged with aparse for the segment containing the head ah (2 inCase 1), because of an intervening fragment of tadwith block-degree 1 (tLL;ad in Case 1).Finally, if there is no node ad in tU;ah that inheritsthe gap of ah, we can split tU;ah into two dependencytrees, as we have done for tL;ah in ?6.2, and parsethe two fragments using the strategy of ?6.4.6.6 RuntimeOur algorithm runs in time O.n5/, where n is thelength of the input sentence.
The reason of the im-provement with respect to the O.n6/ result of ?5 isthat we no longer have deduction rules where bothantecedents represent trees with a gap.
In the new al-gorithm, the worst case is due to rules where only oneantecedent has a gap.
This leads to rules involving amaximum of five indices, ranging over ?1; n?.
Theserules can be instantiated in O.n5/ ways.7 Empirical CoverageWe have seen that the restriction to head-split de-pendency trees enables us to parse these trees oneorder of magnitude faster than the class of well-nes-ted dependency trees with block-degree at most 2.276In connection with the 1-inherit property, this evenincreases to two orders of magnitude.
However, asalready stated in ?2, this improvement is paid for bya loss in coverage; for instance, trees of the formshown in Figure 3 cannot be parsed any longer.7.1 Quantitative EvaluationIn order to assess the empirical loss in coverage thatthe restriction to head-split trees incurs, we evaluatedthe coverage of several classes of dependency treeson standard data sets.
Following Pitler et al(2012),we report in Table 1 figures for the training sets ofsix languages used in the CoNLL-X shared task ondependency parsing (Buchholz and Marsi, 2006).
Aswe can see, the O.n6/ class of head-split trees hasonly slightly lower coverage on this data than thebaseline class of well-nested dependency trees withblock-degree at most 2.
The losses are up to 0.2percentage points on five of the six languages, and 0.9points on the Dutch data.
Our even more restrictedO.n5/ class of 1-inherit head-split trees has the samecoverage as ourO.n6/ class, which is expected giventhe results of Pitler et al(2012): Their O.n6/ classof 1-inherit trees has exactly the same coverage asthe baseline (and thereby more coverage than ourO.n6/ class).
Interestingly though, their O.n5/ classof ?gap-minding?
trees has a significantly smallercoverage than our O.n5/ class.
We conclude thatour class seems to strike a good balance betweenexpressiveness and parsing complexity.7.2 Qualitative EvaluationWhile the original motivation behind introducing thehead-split property was to improve parsing complex-ity, it is interesting to also discuss the linguistic relev-ance of this property.
A first inspection of the struc-tures that violate the head-split property revealed thatmany such violations disappear if one ignores gapscaused by punctuation.
Some decisions about whatnodes should function as the heads of punctuationsymbols lead to more gaps than others.
In order toquantify the implications of this, we recomputed thecoverage of the class of head-split trees on data setswhere we first removed all punctuation.
The resultsare given in Table 2.
We restrict ourselves to the fivenative dependency treebanks used in the CoNLL-Xshared task, ignoring treebanks that have been con-verted from phrase structure representations.Arabic Czech Danish Slovene Turkishwith 1 139 1 2 2without 1 46 0 0 2Table 2: Violations against the head-split property (relativeto the class of well-nested trees with block-degree  2)with and without punctuation.We see that when we remove punctuation fromthe sentences, the number of violations against thehead-split property at most decreases.
For Danishand Slovene, removing punctuation even has the ef-fect that all well-nested dependency trees with block-degree at most 2 become head-split.
Overall, theabsolute numbers of violations are extremely small?except for Czech, where we have 139 violations withand 46 without punctuation.
A closer inspection ofthe Czech sentences reveals that many of these fea-ture rather complex coordinations.
Indeed, out ofthe 46 violations in the punctuation-free data, only 9remain when one ignores those with coordination.For the remaining ones, we have not been able toidentify any clear patterns.8 Concluding RemarksIn this article we have extended head splitting tech-niques, originally developed for parsing of projectivedependency trees, to two subclasses of well-nesteddependency trees with block-degree at most 2.
Wehave improved over the asymptotic runtime of twoexisting algorithms, at no significant loss in coverage.With the same goal of improving parsing efficiencyfor subclasses of non-projective trees, in very recentwork Pitler et al(2013) have proposed an O.n4/time algorithm for a subclass of non-projective treesthat are not well-nested, using an approach that isorthogonal to the one we have explored here.Other than for dependency parsing, our resultshave also implications for mildly context-sensitivephrase structure formalisms.
In particular, the al-gorithm of ?5 can be adapted to parse a subclassof lexicalized tree-adjoining grammars, improvingthe result by Eisner and Satta (2000) from O.n7/ toO.n6/.
Similarly, the algorithm of ?6 can be adaptedto parse a lexicalized version of the tree-adjoininggrammars investigated by Satta and Schuler (1998),improving a na?
?ve O.n7/ algorithm to O.n5/.277ReferencesManuel Bodirsky, Marco Kuhlmann, and Mathias Mo?hl.2005.
Well-nested drawings as models of syntacticstructure.
In Proceedings of the 10th Conference onFormal Grammar (FG) and Ninth Meeting on Mathem-atics of Language (MOL), pages 195?203, Edinburgh,UK.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-Xshared task on multilingual dependency parsing.
InProceedings of the Tenth Conference on ComputationalNatural Language Learning (CoNLL), pages 149?164,New York, USA.Xavier Carreras.
2007.
Experiments with a higher-orderprojective dependency parser.
In Proceedings of theCoNLL Shared Task Session of EMNLP-CoNLL 2007,pages 957?961, Prague, Czech Republic.Joan Chen-Main and Aravind K. Joshi.
2010.
Unavoid-able ill-nestedness in natural language and the adequacyof tree local-MCTAG induced dependency structures.In Proceedings of the Tenth International Conferenceon Tree Adjoining Grammars and Related Formalisms(TAG+), New Haven, USA.Jason Eisner and Giorgio Satta.
1999.
Efficient parsingfor bilexical context-free grammars and Head Auto-maton Grammars.
In Proceedings of the 37th AnnualMeeting of the Association for Computational Linguist-ics (ACL), pages 457?464, College Park, MD, USA.Jason Eisner and Giorgio Satta.
2000.
A faster parsingalgorithm for lexicalized Tree-Adjoining Grammars.
InProceedings of the Fifth Workshop on Tree AdjoiningGrammars and Related Formalisms (TAG+), pages 14?19, Paris, France.Jason Eisner.
1997.
Bilexical grammars and a cubic-timeprobabilistic parser.
In Proceedings of the Fifth Inter-national Workshop on Parsing Technologies (IWPT),pages 54?65, Cambridge, MA, USA.Carlos Go?mez-Rodr?
?guez, John Carroll, and David J. Weir.2011.
Dependency parsing schemata and mildly non-projective dependency parsing.
Computational Lin-guistics, 37(3):541?586.Aravind K. Joshi and Yves Schabes.
1997.
Tree-Adjoining Grammars.
In Grzegorz Rozenberg andArto Salomaa, editors, Handbook of Formal Languages,volume 3, pages 69?123.
Springer.Terry Koo and Michael Collins.
2010.
Efficient third-order dependency parsers.
In Proceedings of the 48thAnnual Meeting of the Association for ComputationalLinguistics (ACL), pages 1?11, Uppsala, Sweden.Marco Kuhlmann and Joakim Nivre.
2006.
Mildly non-projective dependency structures.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics (COLING) and 44th Annual Meeting of theAssociation for Computational Linguistics (ACL) MainConference Poster Sessions, pages 507?514, Sydney,Australia.Wolfgang Maier and Timm Lichte.
2011.
Characteriz-ing discontinuity in constituent treebanks.
In Philippede Groote, Markus Egg, and Laura Kallmeyer, editors,Formal Grammar.
14th International Conference, FG2009, Bordeaux, France, July 25?26, 2009, RevisedSelected Papers, volume 5591 of Lecture Notes in Com-puter Science, pages 167?182.
Springer.Ryan McDonald and Giorgio Satta.
2007.
On the com-plexity of non-projective data-driven dependency pars-ing.
In Proceedings of the Tenth International Confer-ence on Parsing Technologies (IWPT), pages 121?132,Prague, Czech Republic.Ryan McDonald, Fernando Pereira, Kiril Ribarov, and JanHajic?.
2005.
Non-projective dependency parsing usingspanning tree algorithms.
In Human Language Techno-logy Conference (HLT) and Conference on EmpiricalMethods in Natural Language Processing (EMNLP),pages 523?530, Vancouver, Canada.Emily Pitler, Sampath Kannan, and Mitchell Marcus.2012.
Dynamic programming for higher order parsingof gap-minding trees.
In Proceedings of the 2012 JointConference on Empirical Methods in Natural LanguageProcessing (EMNLP) and Computational Natural Lan-guage Learning (CoNLL), pages 478?488, Jeju Island,Republic of Korea.Emily Pitler, Sampath Kannan, and Mitchell Marcus.2013.
Finding optimal 1-endpoint-crossing trees.Transactions of the Association for Computational Lin-guistics.Giorgio Satta and William Schuler.
1998.
Restrictions ontree adjoining languages.
In Proceedings of the 36thAnnual Meeting of the Association for ComputationalLinguistics (ACL) and 17th International Conferenceon Computational Linguistics (COLING), pages 1176?1182, Montre?al, Canada.Stuart M. Shieber, Yves Schabes, and Fernando Pereira.1995.
Principles and implementation of deductive pars-ing.
Journal of Logic Programming, 24(1?2):3?36.278
