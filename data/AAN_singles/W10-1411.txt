Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 94?102,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsOn the Role of Morphosyntactic Features in Hindi Dependency ParsingBharat Ram Ambati*, Samar Husain*, Joakim Nivre?
and Rajeev Sangal**Language Technologies Research Centre, IIIT-Hyderabad, India.
?Department of Linguistics and Philology, Uppsala University, Sweden.
{bharat,samar}@research.iiit.ac.in, joakim.nivre@lingfil.uu.se, san-gal@mail.iiit.ac.inAbstractThis paper analyzes the relative importance ofdifferent linguistic features for data-drivendependency parsing of Hindi, using a featurepool derived from two state-of-the-art parsers.The analysis shows that the greatest gain inaccuracy comes from the addition of morpho-syntactic features related to case, tense, aspectand modality.
Combining features from thetwo parsers, we achieve a labeled attachmentscore of 76.5%, which is 2 percentage pointsbetter than the previous state of the art.
We fi-nally provide a detailed error analysis andsuggest possible improvements to the parsingscheme.1 IntroductionThe dependency parsing community has since afew years shown considerable interest in parsingmorphologically rich languages with flexible wordorder.
This is partly due to the increasing availabil-ity of dependency treebanks for such languages,but it is also motivated by the observation that theperformance obtained for these languages has notbeen very high (Nivre et al, 2007a).
Attempts athandling various non-configurational aspects inthese languages have pointed towards shortcom-ings in traditional parsing methodologies (Tsarfatyand Sima'an, 2008; Eryigit et al, 2008; Seddah etal., 2009; Husain et al, 2009; Gadde et al, 2010).Among other things, it has been pointed out thatthe use of language specific features may play acrucial role in improving the overall parsing per-formance.
Different languages tend to encode syn-tactically relevant information in different ways,and it has been hypothesized that the integration ofmorphological and syntactic information could bea key to better accuracy.
However, it has also beennoted that incorporating these language specificfeatures in parsing is not always straightforwardand many intuitive features do not always work inexpected ways.In this paper, we are concerned with Hindi, anIndian language with moderately rich morphologyand relatively free word order.
There have beenseveral previous attempts at parsing Hindi as wellas other Indian languages (Bharati et al, 1995,Bharati et al, 2009b).
Many techniques were triedout recently at the ICON09 dependency parsingtools contest (Husain, 2009).
Both the best per-forming system (Ambati et al, 2009a) and the sys-tem in second place (Nivre, 2009b) used atransition-based approach to dependency parsing,as implemented in MaltParser (Nivre et al, 2007b).Other data driven parsing efforts for Indian lan-guages in the past have been Bharati et al (2008),Husain et al (2009), Mannem et al (2009b) andGadde et al (2010).In this paper, we continue to explore the transi-tion-based approach to Hindi dependency parsing,building on the state-of-the-art results of Ambati etal.
(2009a) and Nivre (2009b) and exploring thecommon pool of features used by those systems.Through a series of experiments we select featuresincrementally to arrive at the best parser features.The primary purpose of this investigation is tostudy the role of different morphosyntactic featuresin Hindi dependency parsing, but we also want toimprove the overall parsing accuracy.
Our finalresults are 76.5% labeled and 91.1% unlabeled at-tachment score, improving previous results by 2and 1 percent absolute, respectively.
In addition tothis, we also provide an error analysis, isolatingspecific linguistic phenomena and/or other factorsthat impede the overall parsing performance, andsuggest possible remedies for these problems.942 The Hindi Dependency TreebankHindi is a free word order language with SOV asthe default order.
This can be seen in (1), where(1a) shows the constituents in the default order,and the remaining examples show some of theword order variants of (1a).
(1) a. malaya  ne     sameer      ko     kitaba   dii.Malay   ERG  Sameer    DAT   book    gave?Malay gave the book to Sameer?
(S-IO-DO-V)1b.
malaya ne kitaba sameer ko dii.
(S-DO-IO-V)c. sameer ko malaya ne kitaba dii.
(IO-S-DO-V)d. sameer ko kitaba malaya ne dii.
(IO-DO-S-V)e. kitaba malaya ne sameer ko dii.
(DO-S-IO-V)f. kitaba sameer ko malaya ne dii.
(DO-IO-S-V)Hindi also has a rich case marking system, al-though case marking is not obligatory.
For exam-ple, in (1), while the subject and indirect object areexplicitly marked for the ergative (ERG) and da-tive (DAT) cases, the direct object is unmarked forthe accusative.The Hindi dependency treebank (Begum et al,2008) used for the experiment was released as partof the ICON09 dependency parsing tools contest(Husain, 2009).
The dependency framework (Bha-rati et al, 1995) used in the treebank is inspired byPanini?s grammar of Sanskrit.
The core labels,called karakas, are syntactico-semantic relationsthat identify the participant in the action denotedby the verb.
For example, in (1), ?Malay?
is theagent, ?book?
is the theme, and ?Sameer?
is the be-neficiary in the activity of ?give?.
In the treebank,these three labels are marked as k1, k2, and k4 re-spectively.
Note, however, that the notion of kara-ka does not capture the ?global?
semantics ofthematic roles; rather it captures the elements ofthe ?local semantics?
of a verb, while also takingcues from the surface level morpho-syntactic in-formation (Vaidya et al, 2009).
The syntactic re-lational cues (such as case markers) help identifymany of the karakas.
In general, the highest availa-ble karaka,2 if not case-marked, agrees with theverb in an active sentence.
In addition, the tense,1 S=Subject; IO=Indirect Object; DO=Direct Object;V=Verb; ERG=Ergative; DAT=Dative2 These are the karta karaka (k1) and karma karaka (k2).
k1and k2 can be roughly translated as ?agent?
and ?theme?
re-spectively.
For a complete description of the tagset and thedependency scheme, see Begum et al (2008) and Bharati et al(2009a).aspect and modality (TAM) marker can many atimes control the case markers that appear on k1.For example, in (1) ?Malay?
takes an ergative casebecause of the past perfective TAM marker (thatappears as a suffix in this case) of the main verb?gave?.
Many dependency relations other than ka-rakas are purely syntactic.
These include relationssuch as noun modifier (nmod), verb modifier(vmod), conjunct relation (ccof), etc.Each sentence is manually chunked and then an-notated for dependency relations.
A chunk is a mi-nimal, non-recursive structure consisting ofcorrelated groups of words (Bharati et al, 2006).
Anode in a dependency tree represents a chunk head.Each lexical item in a sentence is also annotatedwith its part-of-speech (POS).
For all the experi-ments described in this paper we use gold POS andchunk tags.
Together, a group of lexical items withsome POS tags within a chunk can be utilized toautomatically compute coarse grained morphosyn-tactic information.
For example, such informationcan represent the postposition/case-marking in thecase of noun chunks, or it may represent the TAMinformation in the case of verb chunks.
In the ex-periments conducted for this paper this local in-formation is automatically computed andincorporated as a feature of the head of a chunk.As we will see later, such information proves to beextremely crucial during dependency parsing.For all the experiments discussed in section 4,the training and development data size was 1500and 150 sentences respectively.
The training anddevelopment data consisted of ~22k and ~1.7kwords respectively.
The test data consisted of 150sentences (~1.6k words).
The average sentencelength is 19.85.3 Transition-Based Dependency ParsingA transition-based dependency parser is built oftwo essential components (Nivre, 2008):?
A transition system for mapping sentences todependency trees?
A classifier for predicting the next transition forevery possible system configuration95PTAG CTAG FORM LEMMA DEPREL CTAM OTHERSStack:        top 1 5 1 7  9Input:        next 1 5 1 7  9Input:        next+1 2 5 6 7Input:        next+2 2Input:        next+3 2Stack:       top-1 3String:      predecessor of top 3Tree:        head of top 4Tree:        leftmost dep of next 4 5 6Tree:        rightmost dep of top     8Tree:        left sibling of rightmost dep of top     8Merge:     PTAG of top and next       10Merge:     CTAM and DEPREL of top       10Table 1.
Feature pool based on selection from Ambati et al (2009a) and Nivre (2009b).Given these two components, dependency parsingcan be realized as deterministic search  through thetransition system, guided by the classifier.
Withthis technique, parsing can be performed in lineartime for projective dependency trees.
Like Ambatiet al (2009a) and Nivre (2009b), we use MaltPars-er, an open-source implementation of transition-based dependency parsing with a variety of transi-tion systems and customizable classifiers.33.1 Transition SystemPrevious work has shown that the arc-eager projec-tive transition system first described in Nivre(2003) works well for Hindi (Ambati et al, 2009a;Nivre, 2009b).
A parser configuration in this sys-tem contains a stack holding partially processedtokens, an input buffer containing the remainingtokens, and a set of arcs representing the partiallybuilt dependency tree.
There are four possible tran-sitions (where top is the token on top of the stackand next is the next token in the input buffer):?
Left-Arc(r): Add an arc labeled r from next totop; pop the stack.?
Right-Arc(r): Add an arc labeled r from top tonext; push next onto the stack.?
Reduce: Pop the stack.?
Shift: Push next onto the stack.Although this system can only derive projectivedependency trees, the fact that the trees are labeled3 MaltParser is available at http://maltparser.org.allows non-projective dependencies to be capturedusing the pseudo-projective parsing technique pro-posed in Nivre and Nilsson (2005).3.2 ClassifiersClassifiers can be induced from treebank data us-ing a wide variety of different machine learningmethods, but all experiments reported below usesupport vector machines with a polynomial kernel,as implemented in the LIBSVM package (Changand Lin, 2001) included in MaltParser.
The task ofthe classifier is to map a high-dimensional featurevector representation of a parser configuration tothe optimal transition out of that configuration.
Thefeatures used in our experiments represent the fol-lowing attributes of input tokens:?
PTAG: POS tag of chunk head.?
CTAG: Chunk tag.?
FORM: Word form of chunk head.?
LEMMA: Lemma of chunk head.?
DEPREL: Dependency relation of chunk.?
CTAM: Case and TAM markers of chunk.The PTAG corresponds to the POS tag associatedwith the head of the chunk, whereas the CTAGrepresent the chunk tag.
The FORM is the wordform of the chunk head, and the LEMMA is auto-matically computed with the help of a morphologi-cal analyzer.
CTAM gives the localmorphosyntactic features such as case markers(postpositions/suffixes) for nominals and TAMmarkers for verbs (cf.
Section 2).96The pool of features used in the experiments areshown in Table 1, where rows denote tokens in aparser configuration ?
defined relative to the stack,the input buffer, the partially built dependency treeand the input string ?
and columns correspond toattributes.
Each non-empty cell represents a fea-ture, and features are numbered for easy reference.4 Feature Selection ExperimentsStarting from the union of the feature sets used byAmbati et al (2009a and by Nivre (2009b), wefirst used 5-fold cross-validation on the combinedtraining and development sets from the ICON09tools contest to select the pool of features depictedin Table 1, keeping all features that had a positiveeffect on both labeled and unlabeled accuracy.
Wethen grouped the features into 10 groups (indicatedby numbers 1?10 in Table 1) and reran the cross-validation, incrementally adding different featuregroups in order to analyze their impact on parsingaccuracy.
The result is shown in Figure 1.3036424854606672788490Exp1Exp2Exp3Exp4Exp5Exp6Exp7Exp8Exp9Exp10UASLASFigure 1.
UAS and LAS of experiments 1-10; 5-foldcross-validation on training and development data of theICON09 tools contest.Experiment 1: Experiment 1 uses a baselinemodel with only four basic features: PTAG andFORM of top and next.
This results in a labeledattachment score (LAS) of 41.7% and an unlabeledattachment score (UAS) of 68.2%.Experiments 2?3: In experiments 2 and 3, thePTAG of contextual words of next and top areadded.
Of all the contextual words, next+1,next+2, next+3, top-1 and predecessor of top werefound to be useful.4 Adding these contextual fea-tures gave a modest improvement to 45.7% LASand 72.7% UAS.Experiment 4: In experiment 4, we used thePTAG information of nodes in the partially builttree, more specifically the syntactic head of topand the leftmost dependent of next.
Using thesefeatures gave a large jump in accuracy to 52%LAS and 76.8% UAS.
This is because partial in-formation is helpful in making future decisions.For example, a coordinating conjunction can havea node of any PTAG category as its child.
But allthe children should be of same category.
Knowingthe PTAG of one child therefore helps in identify-ing other children as well.Experiments 5?7: In experiments 5, 6 and 7,we explored the usefulness of CTAG, FORM, andLEMMA attributes.
These features gave small in-cremental improvements in accuracy; increasingLAS to 56.4% and UAS to 78.5%.
It is worth not-ing in particular that the addition of LEMMAattributes only had a marginal effect on accuracy,given that it is generally believed that this type ofinformation should be beneficial for richly in-flected languages.Experiment 8: In experiment 8, the DEPREL ofnodes in the partially formed tree is used.
Therightmost child and the left sibling of the rightmostchild of top were found to be useful.
This is be-cause, if we know the dependency label of one ofthe children, then the search space for other child-ren gets reduced.
For example, a verb cannot havemore than one k1 or k2.
If we know that the parserhas assigned k1 to one of its children, then itshould use different labels for the other children.The overall effect on parsing accuracy is neverthe-less very marginal, bringing LAS to 56.5% andUAS to 78.6%.Experiment 9: In experiment 9, the CTAMattribute of top and next is used.
This gave by farthe greatest improvement in accuracy with a hugejump of around 10% in LAS (to 66.3%) andslightly less in UAS (to 84.7%).
Recall that CTAMconsists of two important morphosyntactic fea-tures, namely, case markers (as suffixes or postpo-sitions) and TAM markers.
These feature helpbecause (a) case markers are important surface4 The predecessor of top is the word occurring immediatelybefore top in the input string, as opposed to top-1, which is theword immediately below top in the current stack.97Figure 2.
Precision and Recall of some important dependency labels.cues that help identify various dependency rela-tions, and (b) there exists a direct mapping be-tween many TAM labels and the nominal casemarkers because TAMs control the case markers ofsome nominals.
As expected, our experimentsshow that the parsing decisions are certainly moreaccurate after using these features.
In particular, (a)and (b) are incorporated easily in the parsingprocess.In a separate experiment we also added someother morphological features such as gender, num-ber and person for each node.
Through these fea-tures we expected to capture the agreement inHindi.
The verb agrees in gender, number and per-son with the highest available karaka.
However,incorporating these features did not improve pars-ing accuracy and hence these features were notused in the final setting.
We will have more to sayabout agreement in section 5.Experiment 10: In experiment 10, finally, weadded conjoined features, where the conjunction ofPOS of next and top and of CTAM and DEPRELof top gave slight improvements.
This is because achild-parent pair type can only take certain labels.For example, if the child is a noun and the parent isa verb, then all the dependency labels reflectingnoun, adverb and adjective modifications are notrelevant.
Similarly, as noted earlier, certain case-TAM combinations demand a particular set of la-bels only.
This can be captured by the combinationtried in this experiment.Experiment 10 gave the best results in the cross-validation experiments.
The settings from this ex-periment were used to get the final performance onthe test data.
Table 2 shows the final results alongwith the results of the first and second best per-forming systems in the ICON09 tools contest.
Wesee that our system achieved an improvement of 2percentage points in LAS and 1 percentage point inUAS over the previous state of the art reported inAmbati et al (2009a).System LAS UASAmbati et al (2009a) 74.5 90.1Nivre (2009b) 73.4 89.8Our system 76.5 91.1Table 2.
Final results on the test data from the ICON09tools contest.5 Error AnalysisIn this section we provide a detailed error analysison the test data and suggest possible remedies forproblems noted.
We note here that other than thereasons mentioned in this section, small treebanksize could be another reason for low accuracy ofthe parser.
The training data used for the experi-ments only had ~28.5k words.
With recent work onHindi Treebanking (Bhatt et al, 2009) we expectto get more annotated data in the near future.Figure 2 shows the precision and recall of someimportant dependency labels in the test data.
Thelabels in the treebank are syntacto-semantic in na-ture.
Morph-syntactic features such as case mark-ers and/or TAM labels help in identifying theselabels correctly.
But lack of nominal postpositionscan pose problems.
Recall that many case mark-ings in Hindi are optional.
Also recall that the verbagrees with the highest available karaka.
Sinceagreement features do not seem to help, if both k1and k2 lack case markers, k1-k2 disambiguationbecomes difficult (considering that word order in-formation cannot help in this disambiguation).
Inthe case of k1 and k2, error rates for instances thatlack post-position markers are 60.9% (14/23) and65.8% (25/38), respectively.98Correct Incorrectk1 k1s k2 pof k7p k7t k7 othersk1 184 5 3 8 3  1  3k1s 12 6  1 6    1k2 126 14  1 7 5   11pof 54 1 8 4k7p 54 3  7   1 2 3k7t 27 3  3 3  1  10k7 3 2   2 4Table 3.
Confusion matrix for important labels.
Thediagonal under ?Incorrect?
represents attachment errors.Table 3 shows the confusion matrix for someimportant labels in the test data.
As the presentinformation available for disambiguation is notsufficient, we can make use of some semantics toresolve these ambiguities.
Bharati et al (2008) andAmbati et al (2009b) have shown that this ambi-guity can be reduced using minimal semantics.They used six semantic features: human, non-human, in-animate, time, place and abstract.
Usingthese features they showed that k1-k2 and k7p-k7tambiguities can be resolved to a great extent.
Ofcourse, automatically extracting these semanticfeatures is in itself a challenging task, although?vrelid (2008) has shown that animacy featurescan be induced automatically from data.In section 4 we mentioned that a separate expe-riment explored the effectiveness of morphologicalfeatures like gender, number and person.
Counterto our intuitions, these features did not improve theoverall accuracy.
Accuracies on cross-validateddata while using these features were less than thebest results with 66.2% LAS and 84.6% UAS.Agreement patterns in Hindi are not straightfor-ward.
For example, the verb agrees with k2 if thek1 has a post-position; it may also sometimes takethe default features.
In a passive sentence, the verbagrees only with k2.
The agreement problem wor-sens when there is coordination or when there is acomplex verb.
It is understandable then that theparser is unable to learn the selective agreementpattern which needs to be followed.
Similar prob-lems with agreement features have also been notedby Goldberg and Elhadad (2009).In the following sections, we analyze the errorsdue to different constructions and suggest possibleremedies.5.1 Simple SentencesA simple sentence is one that has only one mainverb.
In these sentences, the root of the dependen-cy tree is the main verb, which is easily identifiedby the parser.
The main problem is the correctidentification of the argument structure.
Althoughthe attachments are mostly correct, the dependencylabels are error prone.
Unlike in English and othermore configurational languages, one of the maincues that help us identify the arguments is to befound in the nominal postpositions.
Also, as notedearlier these postpositions are many times con-trolled by the TAM labels that appear on the verb.There are four major reasons for label errors insimple sentences: (a) absence of postpositions, (b)ambiguous postpositions, (c) ambiguous TAMs,and (d) inability of the parser to exploit agreementfeatures.
For example in (2), raama and phala arearguments of the verb khaata.
Neither of them hasany explicit case marker.
This makes it difficult forthe parser to identify the correct label for thesenodes.
In (3a) and (3b) the case marker se is ambi-guous.
It signifies ?instrument?
in (3b) and ?agent?in (3a).
(2) raama    phala    khaata    hai?Ram?
?fruit?
?eat?
?is?
?Ram eats a fruit?
(3) a. raama   se     phala  khaayaa nahi    gaya?Ram?
INST ?fruit?
?eat?
?not?
?PAST?
?Ram could not eat the fruit?b.
raama  chamach   se     phala    khaata  hai?Ram?
?spoon?
INST ?fruit?
?eat?
?is?
?Ram eats fruit with spoon?5.2 Embedded ClausesTwo major types of embedded constructions in-volve participles and relative clause constructions.Participles in Hindi are identified through a set ofTAM markers.
In the case of participle embed-dings, a sentence will have more than one verb,i.e., at least one participle and the matrix verb.Both the matrix (finite) verb and the participle cantake their own arguments that can be identified viathe case-TAM mapping discussed earlier.
Howev-er, there are certain syntactic constraints that limitthe type of arguments a participle can take.
There99are two sources of errors here: (a) argument shar-ing, and (b) ambiguous attachment sites.Some arguments such as place/time nominalscan be shared.
Shared arguments are assigned toonly one verb in the dependency tree.
So the taskof identifying the shared arguments, if any, andattaching them to the correct parent is a complextask.
Note that the dependency labels can be identi-fied based on the morphosyntactic features.
Thetask becomes more complex if there is more thanone participle in a sentence.
12 out of 130 in-stances (9.23%) of shared arguments has an incor-rect attachment.Many participles are ambiguous and making thecorrect attachment choice is difficult.
Similar par-ticiples, depending on the context, can behave asadverbials and attach to a verb, or can behave asadjectives and attach to a noun.
Take (4) as a casein point.
(4) maine     daurte   hue        kutte  ko   dekhaa?I?-ERG  (while) running   dog   ACC ?saw?In (4) based on how one interprets ?daurte hue?,one gets either the reading that ?I saw a runningdog?
or that ?I saw a dog while running?.
In case ofthe adjectival participle construction (VJJ), 2 out of3 errors are due to wrong attachment.5.3 CoordinationCoordination poses problems as it often gives riseto long-distance dependencies.
Moreover, the tree-bank annotation treats the coordinating conjunctionas the head of the coordinated structure.
Therefore,a coordinating conjunction can potentially becomethe root of the entire dependency tree.
This is simi-lar to Prague style dependency annotation (Hajico-va, 1998).
Coordinating conjunctions poseadditional problems in such a scenario as they canappear as the child of different heads.
A coordinat-ing conjunction takes children of similar POS cat-egory, but the parent of the conjunction depends onthe type of the children.
(5) a. raama  aur  shyaama   ne     khaana khaayaa?Ram?
?and?
?Shyam?
?ERG?
?food?
?ate?
?Ram and Shyam ate the food.?b.
raama   ne    khaanaa  khaayaa  aur  paanii?Ram?
?ERG?
?food?
?ate?
?and?
?water?piyaa?drank?
?Ram ate food and drank water.
?In (5a), raama and shyaama are children of thecoordinating conjunction aur, which gets attachedto the main verb khaayaa with the label k1.
In ef-fect, syntactically aur becomes the argument of themain verb.
In (5b), however, the verbs khaayaaand piyaa are the children of aur.
In this case, aurbecomes the root of the sentence.
Identifying thenature of the conjunction and its children becomesa challenging task for the parser.
Note that thenumber of children that a coordinating conjunctioncan take is not fixed either.
The parser could iden-tify the correct head of the conjunctions with anaccuracy of 75.7% and the correct children with anaccuracy of 85.7%.The nature of the conjunction will also affect thedependency relation it has with its head.
For ex-ample, if the children are nouns, then the conjunc-tion behaves as a noun and can potentially be anargument of a verb.
By contrast, if the children arefinite verbs, then it behaves as a finite verb and canbecome the root of the dependency tree.
Unlikenouns and verbs, however, conjunctions do nothave morphological features.
So a child-to-headfeature percolation should help make a coordinat-ing node more transparent.
For example, in (5a) theErgative case ne is a strong cue for the dependencylabel k1.
If we copy this information from one ofits children (here shyaama) to the conjunct, thenthe parser can possibly make use of this informa-tion.5.4 Complex PredicatesComplex predicates are formed by combining anoun or an adjective with a verbalizer kar or ho.For instance, in taariif karanaa ?to praise?, taariif?praise?
is a noun and karanaa ?to do?
is a verb.Together they form the main verb.
Complex predi-cates are highly productive in Hindi.
Combinationof the light verb and the noun/adjective is depen-dent on not only syntax but also semantics andtherefore its automatic identification is not alwaysstraightforward (Butt, 1995).
A noun-verb com-plex predicate in the treebank is linked via the de-pendency label pof.
The parser makes mistakes in100identifying pof or misclassifies other labels as pof.In particular, the confusion is with k2 and k1swhich are object/theme and noun complements ofk1, respectively.
These labels share similar contex-tual features like the nominal element in the verbcomplex.
Table 3 includes the confusion matrix forpof errors.5.5 Non-ProjectivityAs noted earlier, MaltParser?s arc-eager parsingalgorithm can be combined with the pseudo-projective parsing techniques proposed in Nivreand Nilsson (2005), which potentially helps inidentifying non-projective arcs.
The Hindi treebankhas ~14% non-projective arcs (Mannem et al,2009a).
In the test set, there were a total of 11 non-projective arcs, but the parser did not find any ofthem.
This is consistent with earlier results show-ing that pseudo-projective parsing has high preci-sion but low recall, especially when the percentageof non-projective relations is small (Nilsson et al2007).Non-projectivity has proven to be one of the ma-jor problems in dependency parsing, especially forfree word-order languages.
In Hindi, the majorityof non-projective arcs are inter-clausal (Mannem etal., 2009a), involving conjunctions and relativeclauses.
There have been some attempts at han-dling inter-clausal non-projectivity in Hindi.
Hu-sain et al (2009) proposed a two-stage approachthat can handle some of the inter-clausal non-projective structures.5.6 Long-Distance DependenciesPrevious results on parsing other languages haveshown that MaltParser has lower accuracy on long-distance dependencies.
Our results confirm this.Errors in the case of relative clauses and coordina-tion can mainly be explained in this way.
For ex-ample, there are 8 instances of relative clauses inthe test data.
The system could identify only 2 ofthem correctly.
These two are at a distance of 1from its parent.
For the remaining 6 instances thedistance to the parent of the relative clause rangesfrom 4 to 12.Figure 3 shows how parser performance de-creases with increasing distance between the headand the dependent.
Recently, Husain et al (2009)have proposed a two-stage setup to parse inter-clausal and intra-clausal dependencies separately.They have shown that most long distance relationsare inter-clausal, and therefore, using such a clausemotivated parsing setup helps in maximizing bothshort distance and long distance dependency accu-racy.
In a similar spirit, Gadde et al (2010) showedthat using clausal features helps in identifying longdistance dependencies.
They have shown that pro-viding clause information in the form of clauseboundaries and clausal heads can help a parsermake better predictions about long distance depen-dencies.0204060801000 2 4 6 8 10 12Dependency LengthDependencyPrecision0204060801000 1 2 3 4 5 6 7 8 9 10 11 12Dependency LengthDependencyRecallFigure 3.
Dependency arc precision/recall relative todependency length, where the length of a dependencyfrom wi to wj is |i-j| and roots are assumed to have dis-tance 0 to their head.6 ConclusionIn this paper we have analyzed the importance ofdifferent linguistic features in data-driven parsingof Hindi and at the same time improved the state ofthe art.
Our main finding is that the combination ofcase markers on nominals with TAM markers onverbs is crucially important for syntactic disambig-uation, while the inclusion of features such as per-son, number gender that help in agreement has notyet resulted in any improvement.
We have alsopresented a detailed error analysis and discussedpossible techniques targeting different errorclasses.
We plan to use these techniques to im-prove our results in the near future.101ReferencesB.
R. Ambati, P. Gadde, and K. Jindal.
2009a.
Experi-ments in Indian Language Dependency Parsing.Proc.
of ICON09 NLP Tools Contest: Indian Lan-guage Dependency Parsing, 32-37.B.
R. Ambati, P. Gade, C. GSK and S. Husain.
2009b.Effect of Minimal Semantics on Dependency Pars-ing.
Proc.
of RANLP Student Research Workshop.R.
Begum, S. Husain, A. Dhwaj, D. Sharma, L. Bai, andR.
Sangal.
2008.
Dependency annotation scheme forIndian languages.
Proc.
of IJCNLP.A.
Bharati, V. Chaitanya and R. Sangal.
1995.
NaturalLanguage Processing: A Paninian Perspective, Pren-tice-Hall of India, New Delhi.A.
Bharati, S. Husain, B. Ambati, S. Jain, D. Sharma,and R. Sangal.
2008.
Two semantic features make allthe difference in parsing accuracy.
Proc.
of ICON.A.
Bharati, R. Sangal, D. M. Sharma and L. Bai.
2006.AnnCorra: Annotating Corpora Guidelines for POSand Chunk Annotation for Indian Languages.
Tech-nical Report (TR-LTRC-31), LTRC, IIIT-Hyderabad.A.
Bharati, D. M. Sharma, S. Husain, L. Bai, R. Begamand R. Sangal.
2009a.
AnnCorra: TreeBanks for In-dian Languages, Guidelines for Annotating HindiTreeBank.http://ltrc.iiit.ac.in/MachineTrans/research/tb/DS-guidelines/DS-guidelines-ver2-28-05-09.pdfA.
Bharati, S. Husain, D. M. Sharma and R. Sangal.2009b.
Two stage constraint based hybrid approachto free word order language dependency parsing.
InProc.
of IWPT.R.
Bhatt, B. Narasimhan, M. Palmer, O. Rambow, D.M.
Sharma and F. Xia.
2009.
Multi-Representationaland Multi-Layered Treebank for Hindi/Urdu.
Proc.of the Third LAW at ACL-IJCNLP, 186-189.M.
Butt.
1995.
The Structure of Complex Predicates inUrdu.
CSLI Publications.G.
Eryigit, J. Nivre, and K. Oflazer.
2008.
DependencyParsing of Turkish.
Computational Linguistics 34(3),357-389.P.
Gadde, K. Jindal, S. Husain, D. M. Sharma, and R.Sangal.
2010.
Improving Data Driven DependencyParsing using Clausal Information.
Proc.
of NAACL-HLT.Y.
Goldberg and M. Elhadad.
2009.
Hebrew Dependen-cy Parsing: Initial Results.
Proc.
of IWPT, 129-133.E.
Hajicova.
1998.
Prague Dependency Treebank: FromAnalytic to Tectogrammatical Annotation.
Proc.
ofTSD.J.
Hall, J. Nilsson, J. Nivre, G. Eryigit, B. Megyesi, M.Nilsson and M. Saers.
2007.
Single Malt or Blended?A Study in Multilingual Parser Optimization.
Proc.of EMNLP-CoNLL, 933-939.S.
Husain.
2009.
Dependency Parsers for Indian Lan-guages.
Proc.
of ICON09 NLP Tools Contest: IndianLanguage Dependency Parsing.S.
Husain, P. Gadde, B. Ambati, D. M. Sharma and R.Sangal.
2009.
A modular cascaded approach to com-plete parsing.
Proc.
of the COLIPS InternationalConference on Asian Language Processing.P.
Mannem, H. Chaudhry, and A. Bharati.
2009a.
In-sights into non-projectivity in Hindi.
Proc.
of ACL-IJCNLP Student Research Workshop.P.
Mannem, A. Abhilash and A. Bharati.
2009b.
LTAG-spinal Treebank and Parser for Hindi.
Proceedings ofInternational Conference on NLP, Hyderabad.
2009.R.
McDonald, K. Lerman, and F. Pereira.
2006.
Multi-lingual dependency analysis with a two-stage discri-minative parser.
Proc.
of CoNLL, 216-220.R.
McDonald and J. Nivre.
2007.
Characterizing theerrors of data-driven dependency parsing models.Proc.
of EMNLP-CoNLL, 122-131.I.
A. Mel'Cuk.
1988.
Dependency Syntax: Theory andPractice, State University Press of New York.J.
Nilsson, J. Nivre and J.
Hall.
2007.
Generalizing TreeTransformations for Inductive Dependency Parsing.Proc.
of ACL, 968-975.J.
Nivre.
2008.
Algorithms for Deterministic Incremen-tal Dependency Parsing.
Computational Linguistics34(4), 513-553.J.
Nivre and R. McDonald.
2008.
Integrating graph-based and transition-based dependency parsers.
InProceedings of ACL-HLT, pp.
950-958.J.
Nivre.
2009a.
Non-Projective Dependency Parsing inExpected Linear Time.
Proc.
of ACL-IJCNLP, 351-359.J.
Nivre.
2009b.
Parsing Indian Languages with Malt-Parser.
Proc.
of ICON09 NLP Tools Contest: IndianLanguage Dependency Parsing, 12-18.J.
Nivre, J.
Hall, S. Kubler, R. McDonald, J. Nilsson,  S.Riedel and D. Yuret.
2007a.
The CoNLL 2007Shared Task on Dependency Parsing.
Proc.
ofEMNLP/CoNLL, 915-932.J.
Nivre, J.
Hall, J. Nilsson, A. Chanev, G. Eryigit, S.K?bler, S. Marinov and E Marsi.
2007b.
MaltParser:A language-independent system for data-driven de-pendency parsing.
NLE, 13(2), 95-135.L.
?vrelid.
2008.
Argument Differentiation.
Soft con-straints and data-driven models.
PhD Thesis, Uni-versity of Gothenburg.D.
Seddah, M. Candito and B. Crabb?.
2009.
Crossparser evaluation: a French Treebanks study.
Proc.
ofIWPT, 150-161.R.
Tsarfaty and K. Sima'an.
2008.
Relational-Realizational Parsing.
Proc.
of CoLing, 889-896.A.
Vaidya, S. Husain, P. Mannem, and D. M. Sharma.2009.
A karaka-based dependency annotation schemefor English.
Proc.
of CICLing, 41-52.102
