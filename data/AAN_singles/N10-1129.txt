Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 867?875,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsImproved Models of Distortion Cost for Statistical Machine TranslationSpence Green, Michel Galley, and Christopher D. ManningComputer Science DepartmentStanford UniversityStanford, CA 94305{spenceg,mgalley,manning}@stanford.eduAbstractThe distortion cost function used in Moses-style machine translation systems has twoflaws.
First, it does not estimate the futurecost of known required moves, thus increas-ing search errors.
Second, all distortion ispenalized linearly, even when appropriate re-orderings are performed.
Because the costfunction does not effectively constrain search,translation quality decreases at higher dis-tortion limits, which are often needed whentranslating between languages of different ty-pologies such as Arabic and English.
To ad-dress these problems, we introduce a methodfor estimating future linear distortion cost, anda new discriminative distortion model that pre-dicts word movement during translation.
Incombination, these extensions give a statis-tically significant improvement over a base-line distortion parameterization.
When wetriple the distortion limit, our model achievesa +2.32 BLEU average gain over Moses.1 IntroductionIt is well-known that translation performance inMoses-style (Koehn et al, 2007) machine transla-tion (MT) systems deteriorates when high distortionis allowed.
The linear distortion cost model used inthese systems is partly at fault.
It includes no es-timate of future distortion cost, thereby increasingthe risk of search errors.
Linear distortion also pe-nalizes all re-orderings equally, even when appro-priate re-orderings are performed.
Because lineardistortion, which is a soft constraint, does not effec-tively constrain search, a distortion limit is imposedon the translation model.
But hard constraints areultimately undesirable since they prune the searchspace.
For languages with very different word or-Followers of all of the Christian and Islamic sectsengagedVerbNP-OBJPP !NP-SBJfor themin waitingFigure 1: The oracle translation for this Arabic VOS sen-tence would be pruned during search using typical dis-tortion parameters.
The Arabic phrases read right-to-left,but we have ordered the sentence from left-to-right in or-der to clearly illustrate the re-ordering problem.ders in which significant re-ordering is required, thedistortion limit can eliminate the oracle, or ?best,?translation prior to search, placing an artificial limiton translation performance (Auli et al, 2009).To illustrate this problem, consider the Arabic-English example in Figure 1.
Assuming that the En-glish translation is constructed left-to-right, the verb?CAJ shaaraka must be translated after the nounphrase (NP) subject.
If P phrases are used to trans-late the Arabic source s to the English target t, thenthe (unsigned) linear distortion is given byD(s, t) = p1first +P?i=2?
?pi?1last + 1?
pifirst??
(1)where pfirst and plast are the first and last sourceword indices, respectively, in phrase i.
By this for-mula, the cost of the step to translate the NP sub-ject before the verb is 9, which is high relative tothe monotone translation path.
Moreover, a con-ventional distortion limit (e.g., 5) would likely forcetranslation of the verb prior to the full subject un-less the exact subject phrase existed in the phrasetable.1Therefore, the correct re-ordering is eitherimprobable or impossible, depending on the choiceof distortion parameters.1Our constrained NIST MT09 Arabic-English system,which placed second, used a limit of 5 (Galley et al, 2009).867The objective of this work is to develop a dis-tortion cost model that allows the distortion limitto be raised significantly without a catastrophic de-crease in performance.
We first describe an admis-sible future cost heuristic for linear distortion thatrestores baseline performance at high distortion lim-its.
Then we add a feature-rich discriminative dis-tortion model that captures e.g.
the tendency of Ara-bic verbs to move right during translation to English.Model parameters are learned from automatic bitextalignments.
Together these two extensions allowus to triple the distortion limit in our NIST MT09Arabic-English system while maintaining a statisti-cally significant improvement over the low distor-tion baseline.
At the high distortion limit, we alsoshow a +2.32 BLEU average gain over Moses withan equivalent distortion parameterization.2 Background2.1 Search in Phrase-based MTGiven a J token source input string f ={fJi},we seek the most probable I token translation e ={eIi}.
The Moses phrase-based decoder models theposterior probability p?
(eI1|fJ1)directly accordingto a log-linear model (Och and Ney, 2004), whichgives the decision rulee?
= arg maxI,eI1{M?m=1?mhm(eI1, fJ1)}where hm(eI1, fJ1)areM arbitrary feature functionsover sentence pairs, and ?m are feature weights setusing a discriminative training method like MERT(Och, 2003).
This search is made tractable by theuse of beams (Koehn et al, 2003).
Hypotheses arepruned from the beams according the sum of the cur-rent model score and a future cost estimate for theuncovered source words.
Since the number of re-ordering possibilities for those words is very large?in theory it is exponential?an inadmissible heuris-tic is typically used to estimate future cost.
Thebaseline distortion cost model is a weighted featurein this framework and affects beam pruning onlythrough the current model score.When we say linear distortion, we refer to the?simple distortion model?
of Koehn et al (2003) thatis shown in Equation (1) and is converted to a costby multiplying by ?1.
When extended to phrases,the key property of this model is that monotone de-coding gives the least costly translation path.
Re-orderings internal to extracted phrases are not pe-nalized.
In practice, we commonly see n-best listsof hypotheses with linear distortion costs equal tozero.
More sophisticated local phrase re-orderingmodels have been proposed (Tillmann, 2004; Zensand Ney, 2006; Koehn et al, 2007; Galley and Man-ning, 2008), but these are typically used in additionto linear distortion.2.2 Arabic Linguistic EssentialsIn this paper we use Arabic-English as a case studysince we possess a strong experimental baseline.But we expect that the technique presented couldbe even more effective for high distortion languagepairs such as Chinese-English and Hindi-English.Since the analysis that follows is framed in terms ofArabic, we point out several linguistic features thatmotivate our approach.
From the perspective of thethree criteria used to specify basic word order typol-ogy (Greenberg, 1966), Arabic is somewhat unusualin its combination of features: it has prepositions(not postpositions), adjectives post-modify nouns,and the basic word order is VSO, but SVO and VOSconfigurations also appear.The implications for translation to English are:(1) prepositions remain in place, (2) NPs are in-verted, and most importantly, (3) basic syntac-tic constituents must often be identified and pre-cisely re-ordered.
The VOS configuration is espe-cially challenging for Arabic-English MT.
It usu-ally appears when the direct object is short?e.g.,pronominal?and the subject is long.
For example,translation of the VOS sentence in Figure 1 requiresboth a high distortion limit to accommodate the sub-ject movement and tight restrictions on the move-ment of the PP.
The particularity of these require-ments in Arabic and other languages, and the dif-ficulty of modeling them in phrase-based systems,has inspired significant work in source language pre-processing (Collins et al, 2005; Habash and Sadat,2006; Habash, 2007).Finally, we observe that target language modelscannot always select appropriate translations whenbasic word order transformation is required.
Bynot modeling source side features like agreement?which, in Arabic, appears between both verb and868 dlimit-4step k Fk ?cost D(s, t) D(s, t) + ?cost0 3 3 1 41 5 2 0 22 7 2 0 23 0 ?7 4 ?34 0 0 3 38 8Figure 2: Translation sequence in which the distortion limit is reached and the decoder is forced to cover the firstskipped word.
Future cost estimation penalizes the two monotone steps, yet total distortion cost remains unchanged.subject, and adjective and noun?baseline phrase-based systems rely on the language model to spec-ify an appropriate target word order (Avramidis andKoehn, 2008).
Returning to Figure 1, we could havean alternate hypothesis They waited for the followersof the Christian and Islamic sects, which is accept-able English and has low distortion, but is semanti-cally inconsistent with the Arabic.3 The Cost ModelIn this section we describe the new distortion costmodel, which has four independent components.3.1 Future Cost EstimationDespite its lack of sophistication, linear distortionis a surprisingly effective baseline cost model forphrase-based MT systems.
It can be computed inconstant time, gives non-decreasing values that aregood for search, and does not require an ancillaryfeature to adjust for the number of components inthe calculation (e.g., language model scores are ad-justed by the word penalty).
Moreover, when a largetraining bitext is used, many local re-orderings arecaptured in the phrase table, so the decoder can oftenrealize competitive performance by finding a best setof phrases with low distortion.
But linear distortionis not the only unlexicalized alternative: we can useany function of the jump width.
Table 1 shows de-velopment set (MT04) performance for polynomialsof degree 1.5 and degree 2.
The linear model is moreeffective than the higher order functions, especiallyat a higher distortion limit.Nevertheless, Table 1 shows an unacceptable de-crease in translation performance at the high distor-tion limit for all three polynomial models.
In Moses,the reason is due in part to a dramatic underestima-tion of future re-ordering cost.
Consider Figure 2in which a distortion limit of 4 is used.
The firstdlimit = 5 dlimit = 15LINEAR 51.65 49.35DEGREE 1.5 51.69 (+0.04) 48.73 (?0.62)DEGREE 2 51.55 (?0.10) 48.40 (?0.95)Table 1: BLEU-4 [%] dev set (MT04) scores (uncased)for several polynomial distortion models.
Higher degreepolynomial distortion models underperform at a high dis-tortion limit (15).word is skipped, and translation proceeds monoton-ically until the distortion limit forces the decoder tocover the first word.
At low distortion limits, sin-gle phrases often saturate the distortion window, sounderestimation is not problematic.
But at high dis-tortion limits, the decoder can skip many source po-sitions at low cost before the search is constrainedby the distortion limit.
Words and phrases sprinkledcarelessly throughout the hypotheses are evidence oferrant search directions that have not been appropri-ately penalized by the distortion cost model.To constrain search, we add an admissible futurecost estimate to the linear model.2By definition, themodel has a least cost translation path: monotone.Therefore, we can add to the baseline calculationD(s, t) the cost of skipping back to the first uncov-ered source word and then translating the remainingpositions monotonically.
It can be verified by induc-tion on |C| that this is an admissible heuristic.Formally, let j represent the first uncovered indexin the source coverage set C. Let Cj represent thesubset of C starting from position j.
Finally, let j?represent the leftmost position in phrase p applied attranslation step k. Then the future cost estimate Fk2Moore and Quirk (2007) propose an alternate future costformulation.
However, their model seems prone to the samedeterioration in performance shown in Table 1.
They observeddecreased translation quality above a distortion limit of 5.869isFk ={|Cj |+ (j?
+ |p|+ 1?
j) if j?
> j0 otherwiseFor k > 0, we add the difference between thecurrent future cost estimate and the previous costestimate ?cost = Fk ?
Fk?1 to the linear penaltyD(s, t).3 Table 2 shows that, as expected, the dif-ference between the baseline and augmented modelsis statistically insignificant at a low distortion limit.However, at a very high distortion limit, the futurecost estimate approximately restores baseline per-formance.
While we still need a distortion limit forcomputational efficiency, it is no longer required toimprove translation quality.3.2 A Discriminative Distortion ModelSo far, we have developed a search heuristic func-tion that gives us a greater ability to control searchat high distortion limits.
Now we need a cost modelthat is sensitive to the behavior of certain words dur-ing translation.
The model must accommodate apotentially large number of overlapping source-sidefeatures defined over the (possibly whole) transla-tion sequence.
Since we intend to train on auto-matic word alignments, data sparsity and noise arealso risks.
These requirements motivate two choices.First, we use a discriminative log-linear frameworkthat predicts one of the nine discretized distortionclasses in Figure 3.
Let dj,j?
indicate the class cor-responding to a jump from source word j to j?
com-puted as (j + 1 ?
j?).
The discriminative distortionclassifier is thenp?(dj,j?
|fJ1 , j, j?)
=exp[?Mm=1 ?mhm(fJ1 , j, j?, dj,j?
)]?dij,j?exp[?Mm=1 ?mhm(fJ1 , j, j?, dij,j?
)]where ?m are feature weights for thehm(fJ1 , j, j?, dij,j?)
arbitrary feature functions.This log conditional objective function is convexand can be optimized with e.g.
a gradient-basedprocedure.3One implementation choice is to estimate future cost toan artificial end-of-sentence token.
Here the decoder incurs apenalty for covering the last word prior to completing a hypoth-esis.
Although this implementation is inconsistent with Moseslinear distortion, we find that it gives a small improvement.dlimit = 5 dlimit = 15BASELINE 51.65 49.35FUTURECOST 51.73 51.65Table 2: BLEU-4 [%] dev set scores (uncased) for thelinear distortion with future cost estimation.  	    	 !"#$%Figure 3: Distortion in Arabic-English translation islargely monotonic, but with noticeable right movementas verbs move around arguments and nouns around mod-ifiers.
The ability to predict movement decreases with thejump size, hence the increasing bin boundaries.Second, we expect that many words will not beuseful for predicting translation order.4In a largetraining bitext, it can be extremely tedious to iden-tify informative words and word classes analytically.Our final decision is then to optimize the parame-ter weights ?m using L1 regularization (Andrew andGao, 2007), a technique that can learn good modelsin the presence of many irrelevant features.5TheL1 regularizer saves us from filtering the trainingdata (e.g., by discarding all words that appear lessthan an empirically-specified threshold), and pro-vides sparse feature vectors that can be analyzedseparately during feature engineering.We train two independent distortion models.
Fora transition from source word j to j?, we learn anoutbound model in which features are defined withrespect to word j.
We have a corresponding inbound4To train the models, we inverted and sorted the intersectionalignments in the bitext.
In our baseline system, we observedno decrease in performance between intersection and e.g.
grow-diag.
However we do expect that our method could be extendedto multi-word alignments.5We also add a Gaussian prior p (?)
v N (0, 1) to the ob-jective (Chen and Rosenfeld, 1999).
Using both L1 and L2 reg-ularization is mathematically odd, but often helps in practice.8701 2 3 4 5 678 9?6?5?4?3?2?10FirstQuintileOutbound Distortion Model1 2 3 4 5 678 9?6?5?4?3?2?10MiddleQuintile1 2 3 4 5 678 9?6?5?4?3?2?10LastQuintileTo leftDistortion ClassTo right(a) ?CAJ / VBD shaaraka (?he engaged?
)1 2 3 4 5 678 9?6?5?4?3?2?10FirstQuintileInbound Distortion Model1 2 3 4 5 678 9?6?5?4?3?2?10MiddleQuintile1 2 3 4 5 678 9?7?6?5?4?3?2?10LastQuintileFrom rightDistortion ClassFrom left(b) ?k?r?
?  / JJ al-aamriikii (?American?
)Figure 4: Selected discriminative cost curves (log scale) over three quintiles of the relative position feature.
Wecondition on the word, POS, and length features.
The classes correspond to those shown in Figure 3.
(4a) The VSObasic word order is evident: early in the sentence, there is a strong tendency towards right movement around argumentsafter covering the verb.
However, right movement is increasingly penalized at the end of the sentence.
(4b) Adjectivespost-modify nouns, so the model learns high inbound probabilities for jumps from positions earlier in the sentence.However, the curve is bi-modal reflecting right inbound moves from other adjectives in NPs with multiple modifiers.model trained on features with respect to j?.
Attraining time, we also add sentence beginning andending delimiters such that inbound probabilities arelearned for words that begin sentences (e.g., nouns)and outbound probabilities are available for tokensthat end sentences (e.g., punctuation).As a baseline, we use the following binaryfeatures: words, part-of-speech (POS) tags, rela-tive source sentence position, and source sentencelength.
Relative source sentence position is dis-cretized into five bins, one for each quintile of thesentence.
Source sentence length is divided into fourbins with bounds set empirically such that trainingexamples are distributed evenly.
To simplify the de-coder integration for this evaluation, we have cho-sen context-free features, but the framework permitsmany other promising possibilities such as agree-ment morphology and POS tag chains.Our models reveal principled cost curves for spe-cific words (Figure 4).
However, monotonic decod-ing no longer gives the least costly translation path,thus complicating future cost estimation.
We wouldneed to evaluate all possible re-orderings within thek-word distortion window.
For an input sentence oflength n, Zens (2008) shows that the number of re-ordering possibilities rn isrn ={kn?k ?
k!
n > kn!
n ?
kwhich has an asymptotic complexity ?(kn).
In-stead of using an inadmissible heuristic as is donein beam pruning, we take a shortcut: we includethe linear future cost model as a separate feature.Then we add the two discriminative distortion fea-tures, which calculate the inbound and outbound logprobabilities of the word alignments in a hypothe-sis.
Since hypotheses may have different numbersof alignments, we also include an alignment penaltythat adjusts the discriminative distortion scores forunaligned source words.
The implementation andbehavior of the alignment penalty is analogous tothat of the word penalty.
In total, the new distortioncost model has four independent MT features.4 MT Evaluation4.1 Experimental SetupOur MT system is Phrasal (Cer et al, 2010),which is a Java re-implementation of the Moses871dlimit = 5 MT03 MT05 MT06 MT08 AvgMOSESLINEAR 52.31 52.67 42.97 41.29COUNTS 52.05 52.32 42.28 40.56FUTURE 52.26 (?0.05) 52.53 (?0.14) 43.04 (+0.07) 41.01 (?0.28) ?0.09DISCRIM+FUTURE 52.68* (+0.37) 53.13* (+0.46) 43.75** (+0.78) 41.82** (+0.53) +0.59Table 3: BLEU-4 [%] scores (uncased) at the distortion limit (5) used in our baseline NIST MT09 Arabic-Englishsystem (Galley et al, 2009).
Avg is a weighted average of the performance deltas.
The stars for positive resultsindicate statistical significance compared to the MOSESLINEAR baseline (*: significance at p ?
0.05; **: significanceat p ?
0.01)dlimit = 15 MT03 MT05 MT06 MT08 AvgMOSESLINEAR 51.04 51.35 41.01 38.83COUNTS 49.92 49.73 39.44 37.65LEX 50.96 51.21 41.87 39.38FUTURE 52.28** (+1.24) 52.45** (+1.10) 42.78** (+1.77) 41.01** (+2.18) +1.66DISCRIM+FUTURE 52.36** (+1.32) 53.05** (+1.70) 43.65** (+2.64) 41.68** (+2.85) +2.32num.
sentences 663 1056 1797 1360 4876Table 4: BLEU-4 [%] scores (uncased) at a very high distortion limit (15).
DISCRIM+FUTURE also achieves astatistically significant gain over the MOSESLINEAR dlimit=5 baseline for MT05 (p ?
0.06), MT06 (p ?
0.01), andMT08 (p ?
0.01).decoder with the same standard features: fourtranslation features (phrase-based translation prob-abilities and lexically-weighted probabilities), wordpenalty, phrase penalty, linear distortion, and lan-guage model score.
We disable baseline linear dis-tortion when evaluating the other distortion costmodels.
To tune parameters, we run MERT with theDownhill Simplex algorithm on the MT04 dataset.For all models, we use 20 random starting points andgenerate 300-best lists.We use the NIST MT09 constrained track trainingdata, but remove the UN and comparable data.6Thereduced training bitext has 181k aligned sentenceswith 6.20M English and 5.73M Arabic tokens.
Wecreate word alignments using the Berkeley Aligner(Liang et al, 2006) and take the intersection of thealignments in both directions.
Phrase pairs with amaximum target or source length of 7 tokens are ex-tracted using the method of Och and Ney (2004).We build a 5-gram language model from theXinhua and AFP sections of the Gigaword corpus(LDC2007T40), in addition to all of the target sidetraining data permissible in the NIST MT09 con-strained competition.
We manually remove Giga-6Removal of the UN data does not affect the baseline ata distortion limit of 5, and lowers the higher distortion base-line by ?1.40 BLEU.
The NIST MT09 data is available athttp://www.itl.nist.gov/iad/mig/tests/mt/2009/.word documents that were released during periodsthat overlapped with the development and test sets.The language model is smoothed with the modifiedKneser-Ney algorithm, retaining only trigrams, 4-grams, and 5-grams that occurred two, three, andthree times, respectively, in the training data.We remove from the test sets source tokens notpresent in the phrase tables.
For the discriminativedistortion models, we tag the pre-processed input us-ing the log-linear POS tagger of Toutanova et al(2003).
After decoding, we strip any punctuationthat appears at the beginning of a translation.4.2 ResultsIn Table 3 we report uncased BLEU-4 (Papineni etal., 2001) scores at the distortion limit (5) of ourmost competitive baseline Arabic-English system.MOSESLINEAR uses the linear distortion modelpresent in Moses.
COUNTS is a separate baselinewith a discrete cost model that uses unlexicalizedmaximum likelihood estimates for the same classespresent in the discriminative model.
To show theeffect of the components in our combined distor-tion model, we give separate results for linear dis-tortion with future cost estimation (FUTURE) and forthe combined discriminative distortion model (DIS-CRIM+FUTURE) with all four features: linear distor-tion with future cost, inbound and outbound proba-872Ar Reference dutch national jaap de hoop scheffer today, monday, took up his responsibilities...MosesLinear-d5 over dutchman jaap de hoop today , monday , in the post of...MosesLinear-d15 dutch assumed his duties in the post of nato secretary general jaap de hoop today , monday...Discrim+Future the dutchman jaap de hoop today , monday , assumed his duties...Figure 5: Verb movement around both the subject and temporal NPs is impossible at a distortion limit of 5(MOSESLINEAR-d5).
The baseline system at a high distortion limit mangles the translation (MOSESLINEAR-d15).DISCRIM+FUTURE (dlimit=15) correctly guides the search.
The Arabic source is written right-to-left.bilities, and the alignment penalty.The main objective of this paper is to improveperformance at very high distortion limits.
Table 4shows performance at a distortion limit of 15.
Tothe set of baselines we add LEX, which is the lex-icalized re-ordering model of Galley and Manning(2008).
This model was shown to outperform otherlexicalized re-ordering models in common use.Statistical significance was computed with theapproximate randomization test of Riezler andMaxwell (2005), which is less sensitive to Type Ierrors than bootstrap re-sampling (Koehn, 2004).5 DiscussionThe new distortion cost model allows us to triple thedistortion limit while maintaining a statistically sig-nificant improvement over the MOSESLINEAR base-line at the lower distortion limit for three of thefour test sets.
More importantly, we can raise thedistortion limit in the DISCRIM+FUTURE configu-ration at minimal cost: a statistically insignificant?0.2 BLEU performance decrease on average.
Wealso see a considerable improvement over both theMOSESLINEAR and LEX baselines at the high dis-tortion limit (Figure 5).
As expected, future cost es-timation alone does not increase performance at thelower distortion limit.We also observe that the effect of conditioning onevidence is significant: the COUNTS model is cate-gorically worse than all other models.
To understandwhy, we randomly sampled 500 sentences from theexcluded UN data and computed the log-likelihoodsof the alignments according to the different models.7In this test, COUNTS is clearly better with a score of7We approximated linear distortion using a Laplacian dis-tribution with estimated parameters ??
= 0.51 and b?
= 1.76(Goodman, 2004).
?23388 versus, for example, the inbound model at?38244.
The explanation is due in part to optimiza-tion.
The two discriminative models often give verylow probabilities for the outermost classes.
Noisein the alignments along with the few cases of long-distance movement are penalized heavily.
For Ara-bic, this property works in our favor as we do notwant extreme movement (as we might with Chineseor German).
But COUNTS applies a uniform penaltyfor all movement that exceeds the outermost classboundaries, making it more prone to search errorsthan even linear distortion despite its favorable per-formance when tested in isolation.Finally, we note that previous attempts to improvere-ordering during search (particularly long-distancere-ordering (Chiang, 2007)) have delivered remark-able gains for languages like Chinese, but improve-ments for Arabic have been less exceptional.
Byrelaxing the distortion limit, we have left room formore sophisticated re-ordering models in conven-tional phrase-based decoders while maintaining asignificant performance advantage over hierarchicalsystems (Marton and Resnik, 2008).6 Prior WorkThere is an expansive literature on re-ordering instatistical MT.
We first review the development ofre-ordering constraints, then describe previous costmodels for those constraints in beam search de-coders.
Because we allow re-ordering during search,we omit discussion of the many different methodsfor preprocessing the source input prior to mono-tonic translation.
Likewise, we do not recite priorwork in re-ranking translations.Re-ordering constraints were first introduced byBerger et al (1996) in the context of the IBM trans-lation models.
The IBM constraints treat the source873word sequence as a coverage set C that is processedsequentially.
A source token is ?covered?
when it isaligned with a new target token.
For a fixed valueof k, we may leave up to k ?
1 positions uncov-ered and return to them later.
We can alter the con-straint slightly such that for the first uncovered posi-tion u /?
C we can cover position j whenj ?
u < k j /?
Cwhich is the definition of the distortion limit used inMoses.
Variations of the IBM constraints also ex-ist (Kanthak et al, 2005), as do entirely differentregimes like the hierarchical ITG constraints, whichrepresent the source as a sequence of blocks thatcan be iteratively merged and inverted (Wu, 1996).Zens and Ney (2003) exhaustively compare the IBMand ITG constraints, concluding that although theITG constraints permit more flexible re-orderings,the IBM constraints result in higher BLEU scores.Since our work falls under the IBM paradigm, weconsider cost models for those constraints.
We havesaid that linear distortion is the simplest cost model.The primary criticism of linear distortion is thatit is unlexicalized, thus penalizing all re-orderingsequally (Khalilov et al, 2009).
When extended tophrases as in Equation (1), linear distortion is alsoagnostic to internal phrase alignments.To remedy these deficiencies, Al-Onaizan andPapineni (2006) proposed a lexicalized, generativedistortion model.
Maximum likelihood estimatesfor inbound, outbound, and pairwise transitions arecomputed from automatic word alignments.
But noestimate of future cost is included, and their modelcannot easily accommodate features defined over theentire translation sequence.
As for experimental re-sults, they use a distortion limit that is half of whatwe report, and compare against a baseline that lacksa distortion model entirely.
Neither their model norours requires generation of lattices prior to search(Zhang et al, 2007; Niehues and Kolss, 2009).Lexicalized re-ordering models are the other sig-nificant approach to re-ordering.
These modelsmake local predictions about the next phrase to betranslated during decoding, typically assigning coststo one of three categories: monotone, swap, or dis-continuous.
Both generative (Tillmann, 2004; Ochand Ney, 2004; Koehn et al, 2007) and discrimina-tive training (Tillmann and Zhang, 2005; Zens andNey, 2006; Liang et al, 2006) algorithms have beenproposed.
Recently, Galley and Manning (2008) in-troduced a hierarchical model capable of analyzingalignments beyond adjacent phrases.
Our discrimi-native distortion framework is not designed as a re-placement for lexicalized re-ordering models, but asa substitute for linear distortion.Finally, we comment on differences between ourArabic-English results and the well-known high dis-tortion system of Zollmann et al (2008), who findoptimal baseline performance at a distortion limit of9.
First, they use approximately two orders of mag-nitude more training data, which allows them to ex-tract much longer phrases (12 tokens v. our maxi-mum of 7).
In this setting, many Arabic-English re-orderings can be captured in the phrase table.
Sec-ond, their ?Full?
system uses three language modelseach trained with significantly more data than oursingle model.
Finally, although they use a lexical-ized re-ordering model, no details are given aboutthe baseline distortion cost model.7 ConclusionWe have presented a discriminative cost frameworkthat both estimates future distortion cost and learnsprincipled cost curves.
The model delivers a statis-tically significant +2.32 BLEU improvement overMoses at a high distortion limit.
Unlike previousdiscriminative local orientation models (Zens andNey, 2006), our framework permits the definition ofglobal features.
The evaluation in this paper usedcontext-free features to simplify the decoder integra-tion, but we expect that context-dependent featurescould result in gains for other language pairs withmore complex re-ordering phenomena.AcknowledgementsWe thank the three anonymous reviewers and DanielCer for constructive comments, and Claude Re-ichard for editorial assistance.
The first author issupported by a National Defense Science and Engi-neering Graduate (NDSEG) fellowship.
This paperis based on work supported in part by the DefenseAdvanced Research Projects Agency through IBM.The content does not necessarily reflect the views ofthe U.S. Government, and no official endorsementshould be inferred.874ReferencesY Al-Onaizan and K Papineni.
2006.
Distortion modelsfor statistical machine translation.
In ACL.G Andrew and J Gao.
2007.
Scalable training of L1-regularized log-linear models.
In ICML.M Auli, A Lopez, H Hoang, and P Koehn.
2009.
Asystematic analysis of translation model search spaces.In WMT.E Avramidis and P Koehn.
2008.
Enriching morpholog-ically poor languages for statistical machine transla-tion.
In ACL.A Berger, P Brown, S Della Pietra, V Della Pietra,A Kehler, and R Mercer.
1996.
Language translationapparatus and method using context-based translationmodels.
US Patent 5,510,981.D Cer, M Galley, D Jurafsky, and C D Manning.
2010.Phrasal: A statistical machine translation toolkit forexploring new model features.
In NAACL, Demonstra-tion Session.S Chen and R Rosenfeld.
1999.
A Gaussian prior forsmoothing maximum entropy models.
Technical Re-port CMU-CS-99-10S, Carnegie Mellon University.D Chiang.
2007.
Hierarchical phrase-based translation.Computational Linguistics, 33(2):201?228.M Collins, P Koehn, and I Kucerova.
2005.
Clause re-structuring for statistical machine translation.
In ACL.M Galley and C D Manning.
2008.
A simple and effec-tive hierarchical phrase reordering model.
In EMNLP.M Galley, S Green, D Cer, P-C Chang, and C D Manning.2009.
Stanford University?s Arabic-to-English statisti-cal machine translation system for the 2009 NIST eval-uation.
Technical report, Stanford University.J Goodman.
2004.
Exponential priors for maximum en-tropy models.
In NAACL.JH Greenberg, 1966.
Some universals of grammar withparticular reference to the order of meaningful ele-ments, pages 73?113.
London: MIT Press.N Habash and F Sadat.
2006.
Arabic preprocessingschemes for statistical machine translation.
In NAACL.N Habash.
2007.
Syntactic preprocessing for statisticalmachine translation.
In MT Summit XI.S Kanthak, D Vilar, E Matusov, R Zens, and H Ney.2005.
Novel reordering approaches in phrase-basedstatistical machine translation.
In ACL Workshop onBuilding and Using Parallel Texts.M Khalilov, J A R Fonollosa, and M Dras.
2009.
Cou-pling hierarchical word reordering and decoding inphrase-based statistical machine translation.
In SSST.P Koehn, F J Och, and D Marcu.
2003.
Statistical phrase-based translation.
In NAACL.P Koehn, H Hoang, A Birch, C Callison-Burch, M Fed-erico, N Bertoldi, B Cowan, W Shen, C Moran,R Zens, C Dyer, O Bojar, A Constantin, and E Herbst.2007.
Moses: Open source toolkit for statistical ma-chine translation.
In ACL, Demonstration Session.P Koehn.
2004.
Statistical significance tests for machinetranslation evaluation.
In EMNLP.P Liang, B Taskar, and D Klein.
2006.
Alignment byagreement.
In NAACL.Y Marton and P Resnik.
2008.
Soft syntactic constraintsfor hierarchical phrased-based translation.
In ACL.R C Moore and C Quirk.
2007.
Faster beam-search de-coding for phrasal statistical machine translation.
InMT Summit XI.J Niehues and M Kolss.
2009.
A POS-based model forlong-range reorderings in SMT.
In WMT.F J Och and H Ney.
2004.
The alignment template ap-proach to statistical machine translation.
Computa-tional Linguistics, 30:417?449.F J Och.
2003.
Minimum error rate training for statisticalmachine translation.
In ACL.K Papineni, S Roukos, T Ward, and W-J Zhu.
2001.BLEU: a method for automatic evaluation of machinetranslation.
In ACL.S Riezler and J T Maxwell.
2005.
On some pitfalls in au-tomatic evaluation and significance testing in MT.
InACL Workshop on Intrinsic and Extrinsic EvaluationMeasures for Machine Translation and/or Summariza-tion (MTSE?05).C Tillmann and T Zhang.
2005.
A localized predictionmodel for statistical machine translation.
In ACL.C Tillmann.
2004.
A unigram orientation model for sta-tistical machine translation.
In NAACL.K Toutanova, D Klein, C D Manning, and Y Singer.2003.
Feature-rich part-of-speech tagging with acyclic dependency network.
In NAACL.D Wu.
1996.
A polynomial-time algorithm for statisticalmachine translation.
In ACL.R Zens and H Ney.
2003.
A comparative study on re-ordering constraints in statistical machine translation.In ACL.R Zens and H Ney.
2006.
Discriminative reorderingmodels for statistical machine translation.
In WMT.R Zens.
2008.
Phrase-based Statistical Machine Trans-lation: Models, Search, Training.
Ph.D. thesis, RWTHAachen University.Y Zhang, R Zens, and H Ney.
2007.
Chunk-level re-ordering of source language sentences with automati-cally learned rules for statistical machine translation.In SSST.A Zollmann, A Venugopal, F J Och, and J Ponte.
2008.A systematic comparison of phrase-based, hierarchicaland syntax-augmented statistical MT.
In COLING.875
