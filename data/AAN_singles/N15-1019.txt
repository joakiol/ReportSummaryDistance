Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 185?195,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsInferring latent attributes of Twitter users with label regularizationEhsan Mohammady Ardehaly and Aron CulottaDepartment of Computer ScienceIllinois Institute of TechnologyChicago, IL 60616emohamm1@hawk.iit.edu, aculotta@iit.eduAbstractInferring latent attributes of online users hasmany applications in public health, politics,and marketing.
Most existing approaches relyon supervised learning algorithms, which re-quire manual data annotation and therefore arecostly to develop and adapt over time.
Inthis paper, we propose a lightly supervisedapproach based on label regularization to in-fer the age, ethnicity, and political orientationof Twitter users.
Our approach learns froma heterogeneous collection of soft constraintsderived from Census demographics, trends inbaby names, and Twitter accounts that are em-blematic of class labels.
To counteract the im-precision of such constraints, we compare sev-eral constraint selection algorithms that opti-mize classification accuracy on a tuning set.We find that using no user-annotated data, ourapproach is within 2% of a fully supervisedbaseline for three of four tasks.
Using a smallset of labeled data for tuning further improvesaccuracy on all tasks.1 IntroductionData annotation is a key bottleneck in applyingsupervised machine learning to language process-ing problems.
This is especially problematic instreaming settings such as social media, where mod-els quickly become dated as new linguistic pat-terns emerge.
An attractive alternative is lightlysupervised learning (Schapire et al, 2002; Jin andLiu, 2005; Chang et al, 2007; Grac?a et al, 2007;Quadrianto et al, 2009; Mann and McCallum, 2010;Ganchev et al, 2010).
In this approach, classifiersare trained from a set of domain-specific soft con-straints, rather than individually labeled instances.For example, label regularization (Mann and Mc-Callum, 2007; Grac?a et al, 2007) uses prior knowl-edge of the expected label distribution to fit a modelfrom large pools of unlabeled instances.
Similarly,annotating features with their expected class fre-quency has proven to be an efficient way of boot-strapping from domain knowledge (Druck et al,2009; Melville et al, 2009; Settles, 2011).In this paper we use lightly supervised learning toinfer the age, ethnicity, and political orientation ofTwitter users.
Lightly supervised learning providesa natural method for incorporating the rich, declar-ative constraints available in social media.
Our ap-proach pairs unlabeled Twitter data with constraintsfrom county demographics, trends in first names,and exemplar Twitter accounts strongly associatedwith a class label.Prior applications of label regularization use asmall number of highly-accurate constraints; for ex-ample, Mann and McCallum (2007) use a singleconstraint that is the true label proportions of an un-labeled dataset, and Ganchev and Das (2013) usecross-lingual constraints from aligned text.
In con-trast, we use hundreds of constraints that are het-erogeneous, overlapping, and noisy.
For example,we constrain the predicted attributes of users froma county to match those collected by the Census,despite the known non-representativeness of Twit-ter users (Mislove et al, 2011).
Furthermore, usersfrom that county who list first names in their pro-file have additional constraints imposed upon them,which may conflict with the county constraints.185To deal with such noisy constraints, we exploreforward selection algorithms that choose from hun-dreds of soft constraints to optimize accuracy on atuning set.
We find that this approach is competi-tive with a fully supervised approach, with the addedadvantage of being less reliant on labeled data andtherefore easier to update over time.
Our primaryresearch questions and answers are as follows:RQ1.
What effect do noisy constraints have onlabel regularization?
We find that sim-ply using all constraints, ignoring noise andoverlap, results in surprisingly high accuracy,within 2% of a fully-supervised approach onthree of four tasks.
For age classification, theconstraint noise appears to substantially de-grade accuracy.RQ2.
How can we select the most useful con-straints?
Using a small tuning set, we findthat our forward selection algorithms im-prove label regularization accuracy while us-ing fewer than 10% of the available con-straints.
Constraint selection improves ageclassification accuracy by nearly 18% (abso-lute).RQ3.
Which constraints are most informative?We find that follower constraints result in thehighest accuracy in isolation, yet the con-straint types appear to be complementary.
Forthree of four tasks, combining all constrainttypes leads to the highest accuracy.In the following, we first review related work inlightly supervised learning and latent attribute infer-ence, then describe the Twitter data and constraints.Next, we formalize the label regularization problemand our constraint selection algorithms.
Finally, wepresent empirical results on four classification tasksand conclude with a discussion of future work.2 Related WorkInferring demographic attributes of users in socialmedia with supervised learning is a growing area ofinterest, with applications in public health (Dredze,2012), politics (O?Connor et al, 2010) and market-ing (Gopinath et al, 2014).
Attributes consideredinclude age (Nguyen et al, 2011; Al Zamal et al,2012), ethnicity (Pennacchiotti and Popescu, 2011;Rao et al, 2011), and political orientation (Conoveret al, 2011; Barber?a, 2013).The main of drawback supervised learning in so-cial media is that human annotation is expensive anderror-prone, and collecting pseudo-labeled data byself-identifying keywords is noisy and biased (e.g.,searching for profiles that mention political orien-tation).
For these reasons we investigate lightly-supervised learning, which takes advantage of theplentiful unlabeled data.Previous work in lightly-supervised learning hasdeveloped methods to train classifiers from priorknowledge of label proportions (Jin and Liu, 2005;Chang et al, 2007; Musicant et al, 2007; Mann andMcCallum, 2007; Quadrianto et al, 2009; Liang etal., 2009; Ganchev et al, 2010; Mann and McCal-lum, 2010; Chang et al, 2012; Wang et al, 2012;Zhu et al, 2014) or prior knowledge of features-label associations (Schapire et al, 2002; Haghighiand Klein, 2006; Druck et al, 2008; Melville et al,2009).
In addition to standard document categoriza-tion tasks, lightly supervised approaches have beenapplied to named-entity recognition (Mann and Mc-Callum, 2010; Ganchev and Das, 2013; Wang andManning, 2014), dependency parsing (Druck et al,2009; Ganchev et al, 2009), language identifica-tion (King and Abney, 2013), and sentiment anal-ysis (Melville et al, 2009).One similarly-motivated work is that of Chang etal.
(2010), who infer race/ethnicity of online usersusing name and ethnicity distributions provided bythe U.S. Census Bureau.
This external data is incor-porated into the model as a prior; however, no lin-guistic content is used in the model, limiting the cov-erage of the resulting approach.
Oktay et al (2014)extend the work of Chang et al (2010) to also in-clude statistics over first names.Other work has inferred population-level statisticsfrom social media; e.g., Eisenstein et al (2011) usegeolocated tweets to predict zip-code statistics of de-mographic attributes of users, and Schwartz et al(2013) predict county health statistics from Twitter.However, no user-level attributes are predicted.Patrini et al (2014) build a Learning with La-bel Proportions (LLP) model with the objective tolearn a supervised classifier when, instead of la-bels, only label proportions for bags of observations186are known.
Their empirical results demonstrate thattheir algorithms compete with or are just percents ofAUC away from the supervised learning approach.In preliminary work (Mohammady and Culotta,2014), we fit a regression model to predict the eth-nicity distribution of a county based on its Twitterusage, then applied the regression model to classifyindividual users.
In contrast, here we use label reg-ularization, which can more naturally be applied touser-level classification and can incorporate a widerrange of constraint types.3 DataIn this section we describe all data and constraintscollected for our experiments.3.1 Labeled Twitter DataFor validation (and for tuning some of the methods)we annotate Twitter users according to age, ethnic-ity, and political orientation.
We collects four dis-joint datasets for this purpose:Race/ethnicity: This data set comes from the re-search of Mohammady and Culotta (2014).
Theycategorized 770 Twitter profiles into one of four cat-egories (Asian, Black, Latino, White).
They usedthe Twitter Streaming API to obtain a random sam-ple of 1,000 users, filtered to the United States.These were manually categorized by analyzing theprofile, tweets, and profile image for each user, dis-carding those for which race could not be deter-mined (230/1,000; 23%).
The category frequency isAsian (22), Black (263), Latino (158), White (327).For each user, they collected the 200 most recenttweets using the Twitter API.
We refer to this datasetas the race dataset.Age: Annotating Twitter users by age can be dif-ficult, since it is rarely explicitly mentioned.
Sim-ilar to prior work (Rao et al, 2010; Al Zamal etal., 2012), we divide users into those below 25 andthose above above 25 years old.
Using the ideafrom Al Zamal et al (2012), we use the Twittersearch API to find tweets with phrases like ?happy30th birthday to me,?
and then we collect those usersand download their 200 most recent tweets using theTwitter API.
We collect 1,436 users (771 below 25and 665 above 25).
While this sampling procedureintroduces some selection bias, it provides a usefulform of validation in the absence of expedient alter-natives.
We refer to this dataset as the age dataset.Politician: Inspired by works of (Cohen andRuths, 2013), we select the official Twitter accountsof members of the U.S. Congress.
We select 189Democratic accounts and and 188 Republican ac-counts and download their most recent 200 tweets.We refer to this dataset as the politician dataset.Politician-follower: As the politician dataset isnot representative of typical users, we collect a sep-arate political datasets.
We first collect a list of fol-lowers of the official Twitter accounts for both par-ties (?thedemocrats?
and ?gop?).
We randomly se-lect 598 likely Democrats and 632 likely Republi-cans, and download the most recent 200 tweets foreach user.
While the labels for these data may con-tain moderate noise (since not everyone who follows?gop?
is Republican), a manual inspection did notreveal any mis-annotations.
We refer to this as thepolitician-follower dataset.1We split each of the datasets above into 40% tun-ing/training and 60% testing (though not all methodswill use the training set, as we describe below).3.2 Unlabeled Twitter DataLabel regularization depends on a pool of unlabeleddata, along with soft constraints over the label pro-portions in that data.
Since many of our constraintsinvolve location, we use the Twitter streaming APIto collect 1% of geolocated tweets, using a bound-ing box of the United States (48 contiguous statesplus Hawaii and Alaska).
In order to assign eachtweet to a county, we use the U.S. Census?
center ofpopulation data.2We use this data to map each ge-olocated Twitter user to a corresponding county.
Weuse the k-d tree algorithm (Maneewongvatana andMount, 2002) to find the nearest center of popula-tion for each tweet and use a threshold to discardtweets that are not within a specified distance of anycounty center.
In total, we collect 18 million geolo-cated tweets from 2.7 million unique users.1We were unfortunately unable to obtain the annotated po-litical data of Cohen and Ruths (2013) for direct comparison.2https://www.census.gov/geo/reference/centersofpop.html1873.3 ConstraintsFinally, we describe the soft constraints used by la-bel regularization.
Each constraint will apply to a(possibly overlapping) subset of users from the un-labeled Twitter data.
For all constraints below, weonly include the constraint for consideration if atleast 1,000 unlabeled Twitter users are matched.
Forexample, if we only have 500 users from a county,we will not use that county?s demographics as a con-straint.
This is to ensure that there is sufficient unla-beled data for learning.
We consider three classes ofconstraints:County constraints (cnt): The U.S. Census pro-duces annual estimates of the ethnicity and age de-mographics for each county.
We use the most recentdecennial census (2010) to compute the proportionof each county that is below and above 25 years old(to match the labels of the annotated data).
We addi-tionally use the 2012 updated estimates of ethnicityby county, restricting to Asian, Black, Latino, andWhite.
Each constraint, then, is applied to the usersassigned to that county in the unlabeled data.
Forexample, there are 46K unlabeled users from CookCounty, which the Census estimates as 45% White.We consider 3,000 total counties as constraints, ofwhich roughly 500 are retained for consideration af-ter filtering those that match fewer than 1,000 users.Name constraints (nam): Silver and McCanc(2014) recently demonstrated how a person?s firstname can often indicate their age.
The Social Secu-rity Administration reports the frequencies of namesgiven to children born in a given year,3and its ac-tuarial tables4estimate how many people born ina given year are still alive.
From these data, onecan estimate the age distribution of people with agiven name.
For example, the median age of some-one named ?Brittany?
is 23.
With this approach, wecan assign constraints indicating the fraction of peo-ple with a given name that are above and below 25years old.For each user in the unlabeled Twitter data, weparse the ?name?
field of the profile, assuming thatthe first token represents the first name.
Constraintsare assigned to users with matching names.
We3http://www.ssa.gov/oact/babynames/4http://www.ssa.gov/oact/NOTES/as120/LifeTables_Tbl_7.htmlconsider more than 50K total name constraints, ofwhich we retain 175 that match a sufficient numberof users.
For example, there are roughly 1,600 un-labeled users with the first name Katherine; the con-straint specifies that 86% of them are under 25.Follower constraints (fol): Our final type of con-straint uses Twitter accounts and hashtags stronglyassociated with a class label.
The constraint ap-plies to users that follow such exemplar accountsor use such hashtags.
We consider two sourcesof such constraints.
For age and race, we down-load demographic data for 1K websites from Quant-cast.com, an audience measurement company thattracks the demographics of visitors to millions ofwebsites (Kamerer, 2013).
We then identify theTwitter accounts for each website.
For example,one constraint indicates that 12% of Twitter userswho follow ?oprah?
are Latino.
For political con-straints, we manually identify 18 Twitter accountsor hashtags that are strongly associated with eitherDemocrats or Republicans.5The constraint speci-fies that 90% of users that follow one of these ac-counts (or use one of these hashtags) are affiliatedwith the corresponding party.
(We omit constraintsuse to construct the labeled data for the politician-follower data.
)4 Label RegularizationOur goal is to learn a classification model using theunlabeled Twitter data and the constraints describedabove.
The idea of label regularization is to de-fine an objective function that enforces that the pre-dicted label distribution for a set of unlabeled dataclosely matches the expected distribution accordingto a constraint.We select multinomial logistic regression as ourclassification model.
Given a feature vector x, aclass label y, and set of parameter vectors ?
={?y1.
.
.
?yk} (one vector per class), the conditionaldistribution of y given x is defined as follows:p?
(y|x) =exp(?y?
x)?y?exp(?y??
x)5For Democrats: thedemocrats, wegoted, dccc, col-legedems, dennis kucinich, sensanders, repjohnlewis, keithelli-son, #p2.
For Republicans: gop, nrsc, the rga, repronpaul, sen-randpaul, senmikelee, repjustinamash, gopleader, #tcot188Typically, ?
is set to maximize the likelihood of alabeled training set.
Instead, we will optimize theobjective defined in Mann and McCallum (2007),using only unlabeled data and constraints.Let U = {U1.
.
.
Uk} be a set of sets, where Ujconsists of unlabeled feature vectors x.
The ele-ments of U may be overlapping.
Let p?jbe the ex-pected label distribution of Uj.
E.g., p?j= {.9, .1}would indicate that 90% of examples in Ujare ex-pected to have class label 0.
The combination of(Uj, p?j) is called a constraint.Our goal, then, is to set ?
so that the predictedlabel distribution matches p?j, for all j.
Since usingthe predicted class counts results in an objective thatis non-differentiable, Mann and McCallum (2007)instead use the model?s posterior distribution:q?j(y) =?x?Ujp?
(y|x)p?j(y) =q?j(y)?y?q?j(y?
)where p?jis the normalized form of q?j.
Then, wewant to set ?
such that p?jand p?jare close.
Mannand McCallum (2007) use KL-divergence, whichis equivalent to augmenting the likelihood with aDirichlet prior over expectations where values forthe priors are proportional to p?j.
KL-divergence canbe factored into two parts:= ?
?yp?j(y) log p?j(y) +?yp?j(y) log p?j(y)= H(p?j, p?j)?H(p?j)where H(p?j) is constant for each j, and so we needto minimize H(p?j, p?j) in order to minimize KL-divergence, where H(p?j, p?j) is the cross-entropy ofthe hypothesized distribution and the expected dis-tribution for Uj.We additionally use L2 regularization, resulting inour final objective function:J(?)
=?jH(p?j, p?j) +1?
?y||?y||22In practice we find that ?
does not need tuning foreach data set.
We set it simply to:?
=C?j|Uj|We set C to 1.3e10 in our experiments.
Mann andMcCallum (2007) compute the gradient of cross-entropy as follows:??
?kH(p?j, p?j) = ??x?Uj?yp?(y|x)xk???p?j(y)p?j(y)??y?p?j(y)?
p?(y?|x)p?j(y)?
?The gradient for ?kis then a sum of the gradi-ents for each constraint j.
In order to minimize theobjective function, we use gradient descent with L-BFGS (Byrd et al, 1995).
(While the objective isnot guaranteed to be convex, this approximation hasworked well in prior work.)
To help reduce overfit-ting, we use early-stopping (10 iterations).Temperature: Mann and McCallum (2007) findthat sometimes label regularization returns a degen-erate solution.
For example, for a three class prob-lem with constraint p?j(y) = {.5, .35, .15}, it mayfind a solution such that p?
(y) = {.5, .35, .15} forevery instance and as a result all of the instancesare assigned the same label.
To avoid this behav-ior Mann and McCallum (2007) introduce a temper-ature parameter T into the classification function asfollows:p?
(y|x) =exp(?y?
x/T )?yexp(?y?
x/T )In practice we find that we can set T to two for bi-nary classification and ten for multi-class problems.While the approach described above closely fol-lows Mann and McCallum (2007), we note two im-portant distinctions: we use no labeled data in ourobjective, and we consider a set of hundreds ofnoisy, overlapping constraints (as opposed to onlya handful of precise constraints).4.1 Constraint SelectionAs described above, our proposed constraints are un-doubtedly inexact.
For example, it is generally ac-cepted that social media users are not a representa-tive sample of the population.
E.g., younger, urbanand minority populations tend to be overrepresentedon Twitter (Mislove et al, 2011; Lenhart and Fox,2009), and Latino users tend to be underrepresentedon Facebook (Watkins, 2009).
Thus, it is incorrect to189assume that the demographics of Twitter users froma county match those of all people from a county.While it may be possible to directly adjust for thesemismatches using techniques from survey reweight-ing (Gelman, 2007), it is difficult to precisely quan-tify the proper weights in this context.Instead, we propose a search-based approach in-spired by feature selection algorithms commonlyused in machine learning (Guyon and Elisseeff,2003).
The idea is to select the subset of constraintsthat result in the most accurate model.
We first as-sume the presence of a small set of labeled dataL = {(x1, y1) .
.
.
(xn, yn)}.
Given a set of con-straints C = {(U1, p?1) .
.
.
(Uk, p?k)}, the search ob-jective is to select a subset of constraints C??
C tominimize error on L:C??
argminC??CE(pC?
(y|x), L)where E(?)
is a classification error function, andpC?
(y|x) is the model fit by label regularization us-ing constraint set C?.In our experiments, |C| is in the hundreds, soexhaustive, exponential search is impractical.
In-stead, we consider the following greedy and pseudo-greedy forward-selection algorithms:?
Greedy (grdy): Standard greedy search.
Ateach iteration, we select the constraint thatleads to the greatest accuracy improvement onL.?
Semi-greedy (semi): Rather than selecting theconstraint that improves accuracy the most,we randomly select from the top three con-straints (Hart and Shogan, 1987).?
Improved-greedy (imp): The same as grdy,but after each iteration, optionally remove asingle constraint.
We consider each currentlyselected constraint, and compute the accuracyattained by removing this constraint from theset.
We remove the constraint that improves ac-curacy the most (if any exists).
This constraintis removed from consideration in future itera-tions.?
Grasp (grsp): Greedy Randomized AdaptiveSearch Procedures (Feo and Resende, 1995)combines semi and imp.We run each selection algorithm for 140 iterations(as we discuss below, accuracy plateaus well beforethen).
Then, we select the constraint set that resultsin the highest accuracy.
While this search proce-dure is computationally expensive, it is fortunatelyeasily parallelizable (by partitioning by constraint),which we take advantage of in our implementation.All constraint selection algorithms use the 40% ofthe labeled data reserved for training/tuning.
Afterwe finalized all models using the tuning data, wethen used them to classify the 60% of labeled datareserved for testing.5 BaselinesWe compare label regularization with standard lo-gistic regression (logistic) trained using the 40% oflabeled data reserved for training/tuning.
We alsoconsider several heuristic baselines:?
Name heuristic, race classification: We im-plement the method proposed by (Mohammadyand Culotta, 2014), using the top 1000 mostpopular last names with their race distribu-tion from the U.S. Census Bureau to inferrace/ethnicity of users based of most probablerace according last name.
If the last name is notamong the top 1000 most popular for a givenrace, we simply predict White (the most fre-quent class).?
Name heuristic, age classification: We use theheuristic described in Section 3.3 that estimatesa person?s age by their first name.
Given theage distribution of a first name, we classify theuser according to the more probable class.?
Follower heuristic, political classification:We reuse the exemplar accounts used in the fol-lower constraint in Section 3.3.
That is, ratherthan using the fact that a user follows ?den-nis kucinich?
as a soft constraint, we classifysuch a user as a Democrat.
If a user followsmore than one of the exemplar accounts, we se-lect the more frequent party.6In case of ties (orif the user does not follow any of the accounts),we classify at random.6For the politician-follower data the heuristic does not use?thedemocrats?
and ?gop,?
because these were used for the orig-inal annotation.190race age pol pol-f avgheuristic 43.7 56.0 89.4 65.4 63.6logistic 81.0 83.3 93.8 68.7 81.7all-constcnt 61.9 45.5 58.1 60.6 56.5fol 67.3 61.4 93.8 60.7 70.8nam 55.6cnt fol 79.4 45.5 79.3 67.9 68.0cnt nam 44.1fol nam 55.9cnt fol nam 44.0imp-greedycnt 80.1 76.6 65.6 58.9 70.3fol 76.6 66.1 86.8 69.1 74.7nam 68.3cnt fol 82.3 75.2 88.1 74.3 80.0cnt nam 79.2fol nam 68.1cnt fol nam 75.2Table 1: Accuracy on the testing set.
all-const doesno constraint selection; imp-greedy selects con-straints to maximize accuracy on the tuning set usingthe Improved-greedy algorithm.Features: For all models, we use a standard bag-of-words representation consisting of a binary termvector for the 200 tweets of each user, their descrip-tion field, and their name field.
We differentiate be-tween terms used in the description, tweet text, andname field, and also indicate hashtags.
Finally, weinclude additional features indicating the accountsfollowed by each user.6 ResultsTable 1 shows the classification accuracy on the testset for each of the four tasks (F1 results are simi-lar).
We begin by comparing heuristic and logisticto the all-const results, which is our proposed la-bel regularization approach using no constraint se-lection (i.e., no user-labeled data).
We can see thatfor three of the four tasks (race, pol, pol-f), labelregularization accuracy is either the same as logisticor within 2%.
That is, using no user-annotated data,we can obtain accuracy competitive with logistic re-gression.For age, however, label regularization does quiteall grdy semi imp grspRace 77.9 82.5 82.5 82.8 82.8Age 48.4 82.8 84.3 82.6 84.3Politician 84.0 98.7 96.0 99.3 96.7Politic-fol 61.8 79.1 77.0 79.5 77.0Average 68.0 85.7 85.0 86.0 85.2Table 2: Comparison of the accuracy of constraintselection algorithms on the tuning set.
all uses allpossible constraints.poorly; only using the fol constraints surpasses theheuristic baseline.
We suspect that this is in partdue to the greater noise in age constraints ?
Twit-ter users are particularly non-representative of theoverall population according to age.
To summarizeour answer to RQ1, label regularization appears toperform quite well under a moderate amount of con-straint noise, but can still fail under excessive noise.We next consider the effect of the constraint se-lection algorithms.
Table 2 compares the four dif-ferent constraint selection algorithms, along with themodel that selects all constraints.
We report the ac-curacy for each approach considering all constrainttypes (county, follow, and name, where applicable).Importantly, this accuracy is computed on the tun-ing set, not the test set.
The goal here is to deter-mine which search algorithm is able to find the bestapproximate solution.
By comparing with all, wecan see that constraint selection can significantly im-prove accuracy on the tuning set (by 18% absoluteon average).
The differences among the selectionalgorithms do not appear to be significant.Figure 1 plots the accuracy at each iteration ofconstraint select for three of the datasets.
The mainconclusion we draw from these figures is that highaccuracy can be achieved with only a small num-ber of constraints, provided they are carefully cho-sen. Each method is very close to convergence afterusing only 20 constraints (selected from hundreds).When examining which constraints are selected, wefind that those that apply to many users are oftenpreferred, presumably because there is more data toinform the final model.Returning to Table 1, we have also listed the ac-curacy of the imp-greedy selection method (whichperformed best on the tuning set), further strati-1910 20 40 60 80 100 120 140Iteration0.500.550.600.650.700.750.800.85Accuracygraspgreedyimp-greedysemi-greedyRace: county-follow(a)0 20 40 60 80 100 120 140Iteration0.550.600.650.700.750.800.85Accuracysemi-greedygreedyimp-greedygraspAge: county-follow-name(b)0 20 40 60 80 100 120 140Iteration0.550.600.650.700.750.80Accuracygraspgreedyimp-greedysemi-greedyPolitic-Followers: county-follow(c)Figure 1: Accuracy per iteration of constraint selection for three classification tasks.fied by constraint type.
Note that imp-greedy se-lects the constraints that perform best on the tun-ing set, fits the classification model, and then clas-sifies the testing set.
We can see that for threeof the four tasks (race, age, pol-f), imp-greedyresults in higher accuracy than using all the con-straints.
This is particularly pronounced for age: thebest result without constraint selection is 61.4, com-pared with 79.2 for imp-greedy.
Furthermore, imp-greedy outperforms logistic on two of four tasks,suggesting that using unlabeled data can improveaccuracy.
Note that both imp-greedy and logisticuse the same amount of labeled data, though in dif-ferent ways: logistic performs standard supervisedclassification; imp-greedy uses the labeled data toperform constraint selection for label regularization.Thus, to summarize our answer to RQ2, we find thatimp-greedy provides a robust method to select con-straints in the presence of noise.
While it comes atthe cost of a small amount of labeled data, it is lessreliant on this data than a traditional supervised ap-proach, and so may be more applicable in streamingsettings.To answer RQ3, we can compare the accuraciesprovided by each of the constraint types in Table 1.For all-const, the follower constraints (fol) outper-form the county constraints (cnt) for all tasks, whilethe name constraint (which only applies to age), fallsbetween the two.
Including both cnt and fol im-proves accuracy on two of the four tasks.
Thesetrends change somewhat for imp-greedy.
The cntconstraints are superior for two tasks, while fol aresuperior for the other two.
The nam constraintsagain fall between the two.
Unlike for all-const,using more constraint types improves accuracy onthree of four tasks.
These differences suggest thatthe constraint selection algorithms allow label regu-larization to be more robust to noisy and conflictingconstraints.
That is, using constraint selection, wecan view constraint engineering akin to feature engi-neering in discriminative, supervised learning meth-ods ?
developers can add many types of constraintsto the model without (much) fear of reducing accu-racy.
The usual caveat of overfitting applies here aswell; indeed, comparing the accuracies on the tuningset (Table 2) with those on the testing set (Table 1)suggests that some over-tuning has occurred, mostnotably on age and pol.We further examined the coefficients of the mod-els trained using each constraint type.
We find,for example, that county constraints result in mod-els with large coefficients for location-specific terms(e.g., college names for younger users, southerncities for Republican users), while follower con-straints tend to learn models dominated by followerfeatures (?thenation?
for Democrats, ?glennbeck?for Republicans).
Similarly, name constraints resultin models dominated by name features.
This anal-ysis helps explain how combining constraint typescan improve overall accuracy, since each type em-phasizes different subsets of features.This difference between constraint types is furthershown in Table 3, which lists the top features for thesemi-greedy constraint selection algorithm, fit usingdifferent subsets of constraints.
In this table, the ital-icized words are the words from the description fieldof the user?s profile, the underlined words are fol-lowed accounts, and the bold words are the words192age under 25 above 25Countyathens tech ugavirginia georgiaairportnashvillesceneat theonion andFollowaltpresscolourlovershotnewhiphopplanetminecraftmenewsobserverbaseballamericapeopleenespanolbreakingnewshogshavenNamekatherinediana me mythisdebra lorisandra janetNo Descpolitician Democratic RepublicanCountyoregon eugeneoregon nesnuniversitycolts beachtahoe indianajgfortwayneFollowkeithellisonrepjohnlewissensandersthinkprogresthenationgopleadersenmikeleesenrandpaul gopglennbeckTable 3: Top features learned by label regularizationfor the age and politician datasets using semi-greedyconstraint selection.
Models were fit separately foreach constraint type (county, follow, name).
Ital-icized words are from the description field, boldwords are from the name field, and underlined wordsare followed accounts.from the name field of the user profile.
In the firstrow, we display the top features for a model fit us-ing only county constraints.
College names appearas top features for younger users, and ?airport?
and@NashvilleScene (a newspaper) are for older users.The second row of Table 3 shows the top features forfollowing constraints; some news channels are ap-pear for younger (Alternative Press) and older (TheNews & Observer) users.
The third row shows thetop features for name constraints, and some namesare in the top features for younger (Katherine andDiana) and older (Debra, Lori, Sandra, and Janet).In addition, the absence of a profile description isindicative of older users.The bottom of Table 3 shows top features forthe politician dataset.
The first row shows thatsome colleges, a sports network in New England,and locations in the Pacific Northwest are indica-tive of Democrats.
Indiana-related terms are strongindicators of Republicans: indiana, the Indianapo-lis Colts (an American football team), and ?jgfort-wayne?
(The Journal Gazette, a newspaper in FortWayne, Indiana).
This aligns with the strong sup-port of the Republican party in Indiana.7The sec-ond row shows top-ranked following features.
Ac-counts ?keithellison?
and ?repjohnlewis?
are top fea-tures for Democratic Party; these belong to KeithEllison and John Robert Lewis, members of theDemocratic leadership of the House of Representa-tives.
On other hand, the ?gopleader?
(the officialaccount for the Republican?s majority leader in theHouse) and ?senmikelee?
(Republican Senator MikeLee from Utah) are the top features for Republicans.7 Conclusions and Future workWhile label regularization has been used on a num-ber of NLP tasks, we have presented evidence thatit is applicable to latent attribute inference even us-ing many noisy, heterogeneous constraints.
We havecompared a number of constraint selection algo-rithms and found they can make label regularizationmore robust to noisy constraints, allowing develop-ers to combine many rich constraint types withoutreducing accuracy.There are many avenues for future work.
Mostpressing is the need to directly address the samplingbias created when constraints derived from the over-all population are applied to online users.
We plan toexplore alternative optimization strategies to explic-itly address this issue.
Finally, additional researchshould quantify how responsive label regularizationapproaches are to the changing linguistic patternscommon in online data.ReferencesF Al Zamal, W Liu, and D Ruths.
2012.
Homophily andlatent attribute inference: Inferring latent attributes oftwitter users from neighbors.
In ICWSM.Pablo Barber?a.
2013.
Birds of the same feather tweettogether.
bayesian ideal point estimation using twitterdata.
Proceedings of the Social Media and PoliticalParticipation, Florence, Italy, pages 10?11.7http://en.wikipedia.org/wiki/Politics_of_Indiana193Richard H Byrd, Peihuang Lu, Jorge Nocedal, and CiyouZhu.
1995.
A limited memory algorithm for boundconstrained optimization.
SIAM Journal on ScientificComputing, 16(5):1190?1208.M.
Chang, L. Ratinov, and D. Roth.
2007.
Guiding semi-supervision with constraint-driven learning.
In ACL,pages 280?287, Prague, Czech Republic, 6.
Associa-tion for Computational Linguistics.Jonathan Chang, Itamar Rosenn, Lars Backstrom, andCameron Marlow.
2010. epluribus: Ethnicity on so-cial networks.
In ICWSM.Ming-Wei Chang, Lev Ratinov, and Dan Roth.
2012.Structured learning with constrained conditional mod-els.
Machine learning, 88(3):399?431.Raviv Cohen and Derek Ruths.
2013.
Classifying politi-cal orientation on twitter: It?s not easy!
In ICWSM.Michael D Conover, Bruno Gonc?alves, Jacob Ratkiewicz,Alessandro Flammini, and Filippo Menczer.
2011.Predicting the political alignment of twitter users.
InPrivacy, security, risk and trust (passat), 2011 ieeethird international conference on and 2011 ieee thirdinternational conference on social computing (social-com), pages 192?199.
IEEE.M.
Dredze.
2012.
How social media will change publichealth.
IEEE Intelligent Systems, 27(4):81?84.Gregory Druck, Gideon Mann, and Andrew McCallum.2008.
Learning from labeled features using gener-alized expectation criteria.
In Proceedings of the31st Annual International ACM SIGIR Conference onResearch and Development in Information Retrieval,pages 595?602.Gregory Druck, Gideon Mann, and Andrew McCal-lum.
2009.
Semi-supervised learning of dependencyparsers using generalized expectation criteria.
In ACL.Jacob Eisenstein, Noah A. Smith, and Eric P. Xing.2011.
Discovering sociolinguistic associations withstructured sparsity.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies - Volume 1, HLT?11, page 13651374, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Thomas A Feo and Mauricio GC Resende.
1995.
Greedyrandomized adaptive search procedures.
Journal ofglobal optimization, 6(2):109?133.Kuzman Ganchev and Dipanjan Das.
2013.
Cross-lingual discriminative learning of sequence modelswith posterior regularization.
In EMNLP, pages 1996?2006.Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar.2009.
Dependency grammar induction via bitext pro-jection constraints.
In Proceedings of the Joint Confer-ence of the 47th Annual Meeting of the ACL and the 4thInternational Joint Conference on Natural LanguageProcessing of the AFNLP: Volume 1-Volume 1, pages369?377.
Association for Computational Linguistics.Kuzman Ganchev, Joo Graca, Jennifer Gillenwater, andBen Taskar.
2010.
Posterior regularization for struc-tured latent variable models.
J. Mach.
Learn.
Res.,11:20012049, August.Andrew Gelman.
2007.
Struggles with survey weight-ing and regression modeling.
Statistical Science,22(2):153?164.Shyam Gopinath, Jacquelyn S Thomas, and LakshmanKrishnamurthi.
2014.
Investigating the relationshipbetween the content of online word of mouth, adver-tising, and brand performance.
Marketing Science,33(2):241?258.Joao Grac?a, Kuzman Ganchev, and Ben Taskar.
2007.Expectation maximization and posterior constraints.In NIPS, volume 20, pages 569?576.Isabelle Guyon and Andr?e Elisseeff.
2003.
An introduc-tion to variable and feature selection.
The Journal ofMachine Learning Research, 3:1157?1182.Aria Haghighi and Dan Klein.
2006.
Prototype-drivenlearning for sequence models.
In Proceedings ofthe main conference on Human Language Technol-ogy Conference of the North American Chapter of theAssociation of Computational Linguistics, pages 320?327.
Association for Computational Linguistics.J Pirie Hart and Andrew W Shogan.
1987.
Semi-greedyheuristics: An empirical study.
Operations ResearchLetters, 6(3):107?114.Rong Jin and Yi Liu.
2005.
A framework for incorporat-ing class priors into discriminative classification.
In InPAKDD.David Kamerer.
2013.
Estimating online audiences: Un-derstanding the limitations of competitive intelligenceservices.
First Monday, 18(5).Ben King and Steven Abney.
2013.
Labeling the lan-guages of words in mixed-language documents us-ing weakly supervised methods.
In Proceedings ofNAACL-HLT, pages 1110?1119.Amanda Lenhart and Susannah Fox.
2009.
Twitter andstatus updating.
pew internet & american life project.Percy Liang, Michael I. Jordan, and Dan Klein.
2009.Learning from measurements in exponential fami-lies.
In Proceedings of the 26th Annual InternationalConference on Machine Learning, ICML ?09, page641648, New York, NY, USA.
ACM.Songrit Maneewongvatana and David M Mount.
2002.Analysis of approximate nearest neighbor searchingwith clustered point sets.
Data Structures, NearNeighbor Searches, and Methodology, 59:105?123.Gideon S. Mann and Andrew McCallum.
2007.
Simple,robust, scalable semi-supervised learning via expecta-tion regularization.
In Proceedings of the 24th Inter-194national Conference on Machine Learning, ICML ?07,page 593600, New York, NY, USA.
ACM.Gideon S. Mann and Andrew McCallum.
2010.
Gener-alized expectation criteria for semi-supervised learn-ing with weakly labeled data.
J. Mach.
Learn.
Res.,11:955984, March.Prem Melville, Wojciech Gryc, and Richard D.Lawrence.
2009.
Sentiment analysis of blogs by com-bining lexical knowledge with text classification.
InProceedings of the 15th ACM SIGKDD InternationalConference on Knowledge Discovery and Data Min-ing, KDD ?09, page 12751284, New York, NY, USA.ACM.Alan Mislove, Sune Lehmann, Yong-Yeol Ahn, Jukka-Pekka Onnela, and J. Niels Rosenquist.
2011.
Un-derstanding the demographics of twitter users.
InProceedings of the Fifth International AAAI Con-ference on Weblogs and Social Media (ICWSM?11),Barcelona, Spain.Ehsan Mohammady and Aron Culotta.
2014.
Us-ing county demographics to infer attributes of twitterusers.
In ACL Joint Workshop on Social Dynamics andPersonal Attributes in Social Media.D.R.
Musicant, J.M.
Christensen, and J.F.
Olson.
2007.Supervised learning by training on aggregate outputs.In Seventh IEEE International Conference on DataMining, 2007.
ICDM 2007, pages 252?261.Dong Nguyen, Noah A. Smith, and Carolyn P. Ros.
2011.Author age prediction from text using linear regres-sion.
In Proceedings of the 5th ACL-HLT Workshopon Language Technology for Cultural Heritage, SocialSciences, and Humanities, LaTeCH ?11, page 115123,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Brendan O?Connor, Ramnath Balasubramanyan,Bryan R. Routledge, and Noah A. Smith.
2010.
FromTweets to polls: Linking text sentiment to public opin-ion time series.
In International AAAI Conference onWeblogs and Social Media, Washington, D.C.Huseyin Oktay, Aykut Firat, and Zeynep Ertem.
2014.Demographic breakdown of twitter users: An analysisbased on names.
In Academy of Science and Engineer-ing (ASE).Giorgio Patrini, Richard Nock, Tiberio Caetano, and PaulRivera.
2014.
(almost) no label no cry.
In Z. Ghahra-mani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q.Weinberger, editors, Advances in Neural InformationProcessing Systems 27, pages 190?198.
Curran Asso-ciates, Inc.Marco Pennacchiotti and Ana-Maria Popescu.
2011.
Amachine learning approach to twitter user classifica-tion.
In Lada A. Adamic, Ricardo A. Baeza-Yates, andScott Counts, editors, ICWSM.
The AAAI Press.Novi Quadrianto, Alex J. Smola, Tiberio S. Caetano, andQuoc V. Le.
2009.
Estimating labels from label pro-portions.
J. Mach.
Learn.
Res., 10:23492374, Decem-ber.Delip Rao, David Yarowsky, Abhishek Shreevats, andManaswi Gupta.
2010.
Classifying latent user at-tributes in twitter.
In Proceedings of the 2Nd In-ternational Workshop on Search and Mining User-generated Contents, SMUC ?10, page 3744, NewYork, NY, USA.
ACM.Delip Rao, Michael J. Paul, Clayton Fink, DavidYarowsky, Timothy Oates, and Glen Coppersmith.2011.
Hierarchical bayesian models for latent at-tribute detection in social media.
In Lada A. Adamic,Ricardo A. Baeza-Yates, and Scott Counts, editors,ICWSM.
The AAAI Press.Robert E. Schapire, Marie Rochery, Mazin G. Rahim, andNarendra K. Gupta.
2002.
Incorporating prior knowl-edge into boosting.
In Proceedings of the NineteenthInternational Conference, pages 538?545.H Andrew Schwartz, Johannes C Eichstaedt, Margaret LKern, Lukasz Dziurzynski, Stephanie M Ramones,Megha Agrawal, Achal Shah, Michal Kosinski, DavidStillwell, Martin E P Seligman, and Lyle H Ungar.2013.
Personality, gender, and age in the languageof social media: the open-vocabulary approach.
PloSone, 8(9):e73791.
PMID: 24086296.Burr Settles.
2011.
Closing the loop: Fast, interactivesemi-supervised annotation with queries on featuresand instances.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing,pages 1467?1478.
Association for Computational Lin-guistics.Nate Silver and Allison McCanc.
2014.
How to tellsomeone?s age when all you know is her name.
Re-trieved from http://fivethirtyeight.com/features/how-to-tell-someones-age-when-all-you-know-is-her-name/.Mengqiu Wang and Christopher D. Manning.
2014.Cross-lingual projected expectation regularization forweakly supervised learning.
TACL, 2:55?66.Zuoguan Wang, Siwei Lyu, Gerwin Schalk, and Qiang Ji.2012.
Learning with target prior.
In F. Pereira, C.J.C.Burges, L. Bottou, and K.Q.
Weinberger, editors, Ad-vances in Neural Information Processing Systems 25,pages 2231?2239.
Curran Associates, Inc.Samuel Craig Watkins.
2009.
The young and the digital:what the migration to social-network sites, games, andanytime, anywhere media means for our future.
Bea-con Press.Jun Zhu, Ning Chen, and Eric P Xing.
2014.
Bayesianinference with posterior regularization and applica-tions to infinite latent svms.
Journal of MachineLearning Research, 15:1799?1847.195
