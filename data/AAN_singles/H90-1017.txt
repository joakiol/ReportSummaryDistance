The Dragon Continuous Speech Recognition System:A Real-Time ImplementationPaul Bamberg, Yen-lu Chow, Laurence Gillick, Robert Roth and Dean SturtevantDragon Systems, Inc.90 Bridge StreetNewton, MA 02158AbstractWe present a 1000-word continuous speech recognition(CSR) system that operates in real time on a personalcomputer (PC).
The system, designed for large vocabularynatural language tasks, makes use of phonetic HiddenMarkov models (HMM) and incorporates acoustic, phonetic,and linguistic sources of knowledge to achieve highrecognition performance.
We describe the variouscomponents of this system.
We also present our strategyfor achieving real time recognition on the PC.
Using a 486-based PC with a 29K-based add-on board, the recognizer hasbeen timed at 1.1 times real time.1.
IntroductionThis paper describes the Dragon continuous peechrecognition system that runs in real time on the PC with a1000-word vocabulary.
To achieve the goal of real-timerecognition on a personal computer isa process that requiresanalysis of the computational requirements of therecognition algorithm along several dimensions, and theimproving the recognizer's performance along thosedimensions.We first present an overview of the Dragon CSR systemarchitecture, and describe its various components, includingsignal processing, recognition, rapid match, phoneticmodeling, and the application task.
We discuss our strategyfor achieving real-time continuous speech recognition, anddemonstrate how it is actually achieved by developingmultiple solutions and applying them in combination.2.
System DescriptionThe architecture of the continuous peech recognitionsystem is shown in Figure 1.
The various components ofthis system are described below.SYSTEM ARCHITECTURESpeechLI s,g.,/MatDchPer tcaWn;:~atee\[ MRa~PclhderRecognized SentenceFigure 1: Speech recognition system architecture2.1 Signal ProcessingAn TMS32010-based board that plugs into a slot on theAT-bus performs analog-to-digital conversion and digitalsignal processing of the input speech waveform, and extractsspectral features used in recognition.
Input speech issampled at 12 KHz and lowpass filtered at 6 KHz.
Eightspectral parameters are computed every 20 milliseconds, andare used as input to the HMM-based recognizer.782.2 RecognitionThe recognition search to find the most likely sentencehypothesis i  based on the time-synchronous decodingalgorithm that is used in almost all current CSR systems forthis vocabulary size.
In this algorithm, partial paths(representing incomplete sentential hypotheses) are extendedsynchronously using dynamic programming (DP), and allspan the same length of the input signal, so that their pathcost functions are directly comparable.
To reduce recognitionsearch, abeam pruning technique is applied to eliminate allpaths that score poorly relative to the best path, andtherefore would have very low probability of being theglobal best hypotheses that spans the entire utterance.
Wealso explored another family of speech decoding algorithms,the stack decoder\[l\], in our recognizer.
It is our conclusionat this time that at least for a task of this complexity, time-synchronous algorithms are considerably more efficient forfinding a single most likely answer.2.3 Rap id  MatcherAn important component ofthe recognition search is theRapid Matcher.
In the time-synchronous decoding scheme,the Rapid Matcher helps reduce the search space dramaticallyby proposing to the HMM DP matcher at any given frameonly a relative small number of word candidates that arelikely to start at that frame.
Only the words on this rapidmatch list (rather than the entire vocabulary) are consideredfor seeding aword for DP match.
Since the Rapid Matcheris designed to take up considerably ess computation thanthe DP Matcher, the combined rapid match/DP matchrecognition architecture sults in an order of magnitude ofsavings in computation, with minimal oss in recognitionaccuracy.
The rapid match algorithm is described in detail in\[2\].2.4 Training of  Acoustic ModelsThe research goal at Dragon is to build CSR systems forlarge vocabulary natural anguage tasks.
As such, it isdeemed impractical to use whole-word models to model thewords in the vocabulary for recognition since in such asystem, one must have training tokens (in different acousticcontexts) for every word in the vocabulary.
Our solutionthen is to make extensive use of phonetic modeling forrecognition.In general, the goal of acoustic modeling is to assurethat when a string of the acoustic units, whatever they maybe, are strung together according to the transcription of anutterance to generate a sequence of spectra, it would fairlyaccurately represent the actual sequence of speech spectra forthat utterance.
Towards this goal, we have chosen as thefundamental unit to be trained the "phoneme-in-context"(PIC), proposed in \[3\].
In the present implementation, aPIC is taken as completely specified by a phonemeaccompanied by a preceding phoneme (or silence), asucceeding phoneme (or silence), stress level, and a durationcode that indicates the degree of prepausal lengthening.
Torestrict he proliferation of PICs, syllable boundaries, evenword boundaries, are currently ignored.During training, tokens are phonemically labeled by asemi-automatic procedure using hidden Markov models inwhich each phoneme is modeled as a sequence of one to sixstates.
A model for a phoneme in a specific context isconstructed by interpolating models involving the desiredcontext and acoustically similar contexts.As each word in the vocabulary is spelled in terms of PICs,each PIC in turn is spelled in terms of allophonic acousticsegments, or clusters.
An acoustic luster consists of a meanvector and a variance vector.
The construction of theseclusters is done in a semi-supervised manner.
Currently, thetotal number of acoustic lusters required to construct allPICs is only slightly more than 2000.
As a result, the entireset of PICs can be adapted to a new user on the basis of acouple of thousand words of speech data.With this approach to acoustic modeling, we are able tomodel words reasonably well acoustically while maintainingto a large extent the desirable property of task-independence.By using different phonetic dictionaries (that make up wordsfor each task), we have constructed models for a 30,000-wordisolated word recognizer as well as for four differentcontinuous peech tasks.
Details of Dragon's acousticmodeling process can be found in \[4\].2.5 Task DescriptionThe Dragon application task consists of recognizingmammography reports.
All the training and test material forthis task have been extracted from a database of 1.3 millionwords of mammography text.
This text corpus forms partof a 38.2 million word database of radiology text.
Much ofthis text represents actual transcriptions ofspoken reports.All of the test material described here is performed withan 842-word subvocabulary.
Punctuation marks, digits, andletters of the alphabet were explicitly excluded.
Thisvocabulary covers about 75% of the full mammographydatabase, and 92% of the database without he excludedwords.
6000 sentences (or sentence fragments) containingonly these vocabulary words were extracted from themarnmography database.
Half of these sentences was usedfor training, and the other half was set aside as test.2.6 Recognition PerformanceUsing the system described above, we have obtainedpreliminary continuous speech recognition results for a 842-word mammography report ask, a subset of a full radiologyreport task.
A partial bigram language model wasconstructed from 40M words of radiology reports, 1M ofwhich was specific to mammography.
The bigram languagemodel consisted of unigrams together with common bigramsand uncommon bigrams of common words.
The perplexityof this task as measured on a set of 3000 sentences i 66.The result was measured on a single speaker, using 1000test utterances totaling 8571 words.
The total number ofword errors was 293 (3.4% word error rate), with 205substitutions, 62 insertions, and 26 deletions.
The sentenceerror ate was 19.5%.
The average number of words returnedfrom the Rapid Marcher (per frame) was 48.A sample of the test sentences and associatedrecognition errors made are shown below.1.
These too have increased very slightly=>These to have increased very slightly2o_-->There are no masses demonstrated on today'sexaminationThere are no mass is demonstrated on today'sexamination79,=.The patient returns for additional views forfurther evaluationThe patient returns for additional view is forfurther evaluationWe will be evaluating the system on several speakers.In addition, we are working on improving recognitionperformance, and we have very specific ideas about how thatcan be done.3.
Real-time ImplementationOur strategy in developing a prototype real-timecontinuous speech recognition system on the PC is to use amultitude of approaches to solve the computationalproblem.
Since one of our primary concerns is softwareportability, extensive r writes in assembly code is kept at aminimum.
Instead, we kept almost all of the system writtenin C, and rely mostly on both algorithm and hardwareimprovements to achieve real time performance.
Softwareoptimizations include the use of a rapid match algorithm toreduce recognition search space, C code optimization, andwriting assembly code for a few compute-intensive routines.With hardware, we are relying on the use of both fastermachines (e.g., 486-based PC) and more hardware (off-the-shelf boards) serving as compute ngines to the PC.3.1 A lgor i thms/Sof tware  Imp lementat ionsRapid matchThe single most important factor in achieving a realtime implementation is the use of rapid match to reducecomputation during recognition.
As described earlier, rapidmatch is used to compute a relatively short list of likelyword candidates that are likely to start at a given frame in thespeech input.
Thus instead of seeding the entire vocabulary(or close to it), only those words that are returned by theRapid Matcher are seeded.Profile and optimize in CAlternatively, we also invested in profiling therecognition program and getting a report of the amount oftime spent in each routine, sorted in decreasing order, so thatthe first routine on this profiling report is the most timeconsuming one.
Then, if possible, a rewrite of this routineor parts of it with efficiency as the objective is performed.This is done for the top few routines on the list (whichusually account for a significant percentage of the totalcomputation).
The entire procedure isthen repeated.Assembly language codeOnce in a while, as deemed necessary and appropriate, anentire C routine is rewritten in assembly code.
Currently,only a few routines have been rewritten this way, which areall routines of the Rapid Marcher.3.2 Hardware  Imp lementat ionsA second part of our strategy is to let advances in thetechnology of manufacturing PCs help in solving thecomputation problem in continuous peech recognition.Already, we have witnessed an order of magnitude increase inthe computation power of a personal computer within thelast decade (from AT running at 8 Mhz clock rate to 386 at33 Mhz).
Starting off this decade, the Intel 486-basedfamily of PC's that have just been introduced are a factor 2faster that its immediate predecessor (the 386-based)machines, given a fixed clock speed of 33 Mhz (see Table1).
This trend will be certain to continue, at least for thefo ib le  future.
Our recognizer sped up by almost afactorof two just by going from a 386/33 to a 486/33, withoutany modification tothe code (see Table 2).
In fact, since the486 instruction set is downward compatible, the exact sameexecutable code that ran on the 386 also ran on the 486.
Atthis rate, real-time very large vocabulary (> 10,000 words)continuous speech recognition on the PC is within reach nottoo far in the future.3.3 Paral le l  Arch i tectureWe also explored the use of a single (but expandable tomultiple) off-the-shelf board (29K-based coprocessor board)serving as compute ngine to the PC, and performing thecomputation i  parallel (a coarse grain 2-way parallelism).The board of our choice was an AMD 29000-based board(called the AT-Super made by YARC) that plugs directlyonto the AT-bus on the backplane of the PC.
The board isquoted at 17 MIPs, although our benchmark in running therecognizer on the board revealed a somewhat lower MIPnumber (see Table 1).
The board also came with somesoftware for development of programs to perform parallelcomputation.In analyzing the computation requirements of thevarious components of our algorithm, it was immediatelyapparent that a natural way to divide up the algorithm is tohave the DP Matcher and the Rapid Matcher un on separateprocessors, for the following reasons.
First, the twocomponents are functionally and logically separable, makingparallelization fairly straightforward.
Second, it makes ensefrom the point of the view of the the hardware benchmarks(the two processors give equivalent umber of MIPs) as thetwo recognition components ake up within a factor of tworelative to the other the number of CPU cycles.
Lastly, thecommunication bandwidth is low (on the order of 5kbytes/sec), so that little overhead is incurred.
In the nextsection, we present results using two alternate ways ofmapping the component algorithms to the two processors.Hardware  Benchmark386/33 PC 8 MIPs486/33 PC 15 MIPs29K Board 12 MIPsTable 1.
Hardware benchmark measured in MIPs.803.4 Recogn i t ion  BenchmarksTable 2 shows the recognition benchmarks (measured innumber of times real time) using the various hardwareplatforms.
As can be seen, using a baseline 386 PC, we areat 2.8 X real time.
Using a combined 386+29K architecture,and putting the Rapid Matcher on the host and DP Matcheron the 29K (RM/DM) gave us more than a factor of twoimprovement (to 1.3 X).Alternatively, going to a faster machine (486-based PC)immediately gave us almost a factor of two relative torunning on the 386.
However, using the combined 486+29Karchitecture, though putting us very close to real time (1.1X), did not provide a significant gain over the 386+29Kplatform.
This is due to the fact that the 29K board, inperforming DP match, has become the computationalbottleneck.
Also, going to the alternative softwarearchitecture of performing DP match on the host and rapidmatch on 29K board (DM/RM) resulted in worsecomputational performance.
This is largely explained by thefact that by performing the rapid match on the 29K, thecomputational gain that resulted from assembly coding (donefor the 386) of some rapid match routines was now lost.Hardware Arch.386/33486/33386+29K486+29K# times real time2.81.5RM/DM DM/RM1.3 1.8i1.1 1.5Table 2: Recognition benchmarks with various platforms (# times real time).3.5 D iscuss ionTable 3 demonstrates how real-time recognition on thePC was achieved.
As noted previously, the use of rapidmatch to reduce recognition search was the single mostimportant factor in achieving real time.
An order ofmagnitude r duction in computation was realized using thisalgorithm.
Rewriting of C code with runtime fficiency inmind and assembly language coding of some time-criticalrapid match routines resulted in factors of 2 and 1.5speedups, respectively.
Finally, making use of more MIPs(either with a 486-based PC or use of a single coprocessorboard) gave an additional factor of two to three, depending onthe exact hardware platform used.
In short, by combiningalgorithm improvements, oftware optimizations, andenhanced hardware capabilities, a 3-second long utterancethat initially required nearly three minutes to decode (60Xreal time) now can be decoded in real time.MethodRapid matchOl~timize C codeAssemblyHardwareSpeedup10.0 X2.0 X1.5 X2.0 - 3.0 XTable 3.
How real-time recognition was achieved.4.
ConclusionIn summary, we have presented a system for performing1000-word continuous speech recognition i  real time on thepersonal computer.
The system, designed for largevocabulary natural language tasks, is also largely task-independent in that given a new text corpus (used forlanguage modeling) for a new task, we are able to performrecognition on that task within a matter of days.We also presented our strategies for real-timeimplementation.
Use of advanced algorithms in combinationwith clever software optimizations, we have reducedcomputation requirements by a factor of 30, with minimalsacrifice in performance.
Using a 386-based PC, therecognizer has been clocked at 2.8 times real time; with a486-based PC, 1.5 times; and using a 29K-based add-onboard, at 1.1 times real time.REFERENCES\[1\] F. Jelinek, "A Fast Sequential Decoding AlgorithmUsing A Stack", IBM Journal of Research and Development,Vol.
13, pp.
675-685, November 1969.\[2\] L. Gillick and R. Roth, "A Rapid Match Algorithm forContinuous Speech Recognition", Proceedings of DARPASpeech and Natural Language Workshop, June 1990 HiddenValley, Pennsylvania.\[3\] R.M.
Schwartz, et al "Context-Dependent Modeling forAcoustic-Phonetic Recognition of Continuous Speech",IEEE Int.
Conf.
Acoust., Speech., Signal Processing,Tampa, FL, March 1985, pp.1205-1208, Paper 31.3\[4\] P. Bamberg and L. Gillick, "Phoneme-in-ContextModeling for Dragon's Continuous Speech Recognizer",Proceedings ofDARPA Speech and Natural LanguageWorkshop, June 1990 Hidden Valley, Pennsylvania.83_
