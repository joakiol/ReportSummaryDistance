Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 188?196,Beijing, August 2010A Twin-Candidate Based Approach for Event Pronoun Resolution us-ing Composite KernelChen Bin1 Su Jian2 Tan Chew Lim11National University of Singapore 2Institute for Inforcomm Research, A-STAR{chenbin,tancl}@comp.nus.edu.sg sujian@i2r.a-star.edu.sgAbstractEvent Anaphora Resolution is an importanttask for cascaded event template extractionand other NLP study.
In this paper, we providea first systematic study of resolving pronounsto their event verb antecedents for generalpurpose.
First, we explore various positional,lexical and syntactic features useful for theevent pronoun resolution.
We further exploretree kernel to model structural informationembedded in syntactic parses.
A compositekernel is then used to combine the above di-verse information.
In addition, we employed atwin-candidate based preferences learningmodel to capture the pair wise candidates?
pre-ference knowledge.
Besides we also look intothe incorporation of the negative training in-stances with anaphoric pronouns whose ante-cedents are not verbs.
Although these negativetraining instances are not used in previousstudy on anaphora resolution, our study showsthat they are very useful for the final resolu-tion through random sampling strategy.
Ourexperiments demonstrate that it?s meaningfulto keep certain training data as developmentdata to help SVM select a more accurate hyperplane which provides significant improvementover the default setting with all training data.1 IntroductionAnaphora resolution, the task of resolving a giv-en text expression to its referred expression inprior texts, is important for intelligent textprocessing systems.
Most previous works onanaphora resolution mainly aims at object ana-phora in which both the anaphor and its antece-dent are mentions of the same real world objectsIn contrast, an event anaphora as first definedin (Asher, 1993) is an anaphoric reference to anevent, fact, and proposition which is representa-tive of eventuality and abstract entity.
Considerthe following example:This was an all-white, all-Christian communitythat all the sudden was taken over -- not takenover, that's a very bad choice of words, but [in-vaded]1 by, perhaps different groups.
[It]2 began when a Hasidic Jewish family boughtone of the town's two meat-packing plants 13years ago.The anaphor [It]2 in the above example refersback to an event, ?all-white and all-Christian cityof Postville is diluted by different ethnic groups.
?Here, we take the main verb of the event, [in-vaded]1 as the representation of this event andthe antecedent for pronoun [It]2.According to (Asher, 1993), antecedents ofevent pronoun include both gerunds (e.g.
de-struction) and inflectional verbs (e.g.
destroying).In our study, we focus on the inflectional verbrepresentation, as the gerund representation isstudied in the conventional anaphora resolution.For the rest of this paper, ?event pronouns?
arepronouns whose antecedents are event verbswhile ?non-event anaphoric pronouns?
are thosewith antecedents other than event verbs.Entity anaphora resolution provides criticallinks for cascaded event template extraction.
Italso provides useful information for further infe-rence needed in other natural languageprocessing tasks such as discourse relation andentailment.
Event anaphora (both pronouns andnoun phrases) contributes a significant propor-tion in anaphora corpora, such as OntoNotes.19.97% of its total number of entity chains con-tains event verb mentions.In (Asher, 1993) chapter 6, a method to re-solve references to abstract entities using dis-course representation theory is discussed.
How-ever, no computation system was proposed forentity anaphora resolution.
(Byron, 2002) pro-posed semantic filtering as a complement to sa-lience calculations to resolve event pronoun tar-geted by us.
This knowledge deep approach only188works for much focused domain like trains spo-ken dialogue with handcraft knowledge of rele-vant events for only limited number of verbs in-volved.
Clearly, this approach is not suitable forgeneral event pronoun resolution say in newsarticles.
Besides, there?s also no specific perfor-mance report on event pronoun resolution, thusit?s not clear how effective their approach is.
(M?ller, 2007) proposed pronoun resolution sys-tem using a set of hand-crafted constraints suchas ?argumenthood?
and ?right-frontier condition?together with logistic regression model based oncorpus counts.
The event pronouns are resolvedtogether with object pronouns.
This explorativework produced an 11.94% F-score for event pro-noun resolution which demonstrated the difficul-ty of event anaphora resolution.
In (Pradhan,et.al, 2007), a general anaphora resolution sys-tem is applied to OntoNotes corpus.
However,their set of features is designed for object ana-phora resolution.
There is no specific perfor-mance reported on event anaphora.
We suspectthe event pronouns are not correctly resolved ingeneral as most of these features are irrelevant toevent pronoun resolution.In this paper, we provide the first systematicstudy on pronominal references to event antece-dents.
First, we explore various positional, lexi-cal and syntactic features useful for event pro-noun resolution, which turns out quite differentfrom conventional pronoun resolution exceptsentence distance information.
These have beenused together with syntactic structural informa-tion using a composite kernel.
Furthermore, wealso consider candidates?
preferences informa-tion using twin-candidate model.Besides we further look into the incorporationof negative instances from non-event anaphoricpronoun, although these instances are not used inprevious study on co-reference or anaphora reso-lution as they make training instances extremelyunbalanced.
Our study shows that they can bevery useful for the final resolution after randomsampling strategy.We further demonstrate that it?s meaningful tokeep certain training data as development data tohelp SVM select a more accurate hyper-planewhich provide significant improvement over thedefault setting with all training data.The rest of this paper is organized as follows.Section 2 introduces the framework for eventpronoun resolution, the considerations on train-ing instance, the various features useful for eventpronoun resolution and SVM classifier with ad-justment of hyper-plane.
Twin-candidate modelis further introduced to capture the preferencesamong candidates.
Section 3 presents in detailsthe structural syntactic feature and the kernelfunctions to incorporate such a feature in the res-olution.
Section 4 presents the experiment resultsand some discussion.
Section 5 concludes thepaper.2 The Resolution FrameworkOur event-anaphora resolution system adopts thecommon learning-based model for object ana-phora resolution, as employed by (Soon et al,2001) and (Ng and Cardie, 2002a).2.1 Training and Testing instanceIn the learning framework, training or testinginstance of the resolution system has a form ofwhere        is the ith candi-date of the antecedent of anaphor    .
An in-stance is labeled as positive if        is the ante-cedent of      , or negative if        is not theantecedent of     .
An instance is associatedwith a feature vector which records differentproperties and relations between     and       .The features used in our system will be discussedlater in this paper.During training, for each event pronoun, weconsider the preceding verbs in its current andprevious two sentences as its antecedent candi-dates.
A positive instance is formed by pairing ananaphor with its correct antecedent.
And a set ofnegative instances is formed by pairing an ana-phor with its candidates other than the correctantecedent.
In addition, more negative instancesare generated from non-event anaphoric pro-nouns.
Such an instance is created by pairing upa non-event anaphoric pronoun with each of theverbs within the pronoun?s sentence and previoustwo sentences.
This set of instances from non-event anaphoric pronouns is employed to provideextra power on ruling out non-event anaphoricpronouns during resolution.
This is inspired bythe fact that event pronouns are only 14.7% of allthe pronouns in the OntoNotes corpus.
Based onthese generated training instances, we can train abinary classifier using any discriminative learn-ing algorithm.189The natural distribution of textual data is of-ten imbalanced.
Classes with fewer examples areunder-represented and classifiers often performfar below satisfactory.
In our study, this becomesa significant issue as positive class (event ana-phoric) is the minority class in pronoun resolu-tion task.
Thus we utilize a random down sam-pling method to reduce majority class samples toan equivalent level with the minority class sam-ples which is described in (Kubat and Matwin,1997) and (Estabrooks et al 2004).
In (Ng andCardie, 2002b), they proposed a negative sampleselection scheme which included only negativeinstances found in between an anaphor and itsantecedent.
However, in our event pronoun reso-lution, we are distinguishing the event-anaphoricfrom non-event anaphoric which is differentfrom (Ng and Cardie, 2002b).2.2 Feature SpaceIn a conventional pronoun resolution, a set ofsyntactic and semantic knowledge has been re-ported as in (Strube and M?ller, 2003; Yang et al2004;2005a;2006).
These features include num-ber agreement, gender agreement and many oth-ers.
However, most of these features are not use-ful for our task, as our antecedents are inflection-al verbs instead of noun phrases.
Thus we haveconducted a study on effectiveness of potentialpositional, lexical and syntactic features.
Thelexical knowledge is mainly collected from cor-pus statistics.
The syntactic features are mainlyfrom intuitions.
These features are purposely en-gineered to be highly correlated with positiveinstances.
Therefore such kind of features willcontribute to a high precision classifier.?
Sentence DistanceThis feature measures the sentence distance be-tween an anaphor and its antecedent candidateunder the assumptions that a candidate in thecloser sentence to the anaphor is preferred to bethe antecedent.?
Word DistanceThis feature measures the word distance betweenan anaphor and its antecedent candidate.
It ismainly to distinguish verbs from the same sen-tence.?
Surrounding Words and POS TagsThe intuition behind this set of features is to findpotential surface words that occur most frequent-ly with the positive instances.
Since most ofverbs occurred in front of pronoun, we have builta frequency table from the preceding 5 words ofthe verb to succeeding 5 surface words of thepronoun.
After the frequency table is built, weselect those words with confidence1  > 70% asfeatures.
Similar to Surrounding Words, we havebuilt a frequency table to select indicative sur-rounding POS tags which occurs most frequentlywith positive instances.?
Co-occurrences of Surrounding WordsThe intuition behind this set of features is to cap-ture potential surface patterns such as ?Itcaused??
and ?It leads to?.
These patterns areassociated with strong indication that pronoun?it?
is an event pronoun.
The range for the co-occurrences is from preceding 5 words to suc-ceeding 5 words.
All possible combinations ofword positions are used for a co-occurrencewords pattern.
For example ?it leads to?
willgenerate a pattern as ?S1_S2_lead_to?
where S1and S2 mean succeeding position 1 and 2.
Simi-lar to previous surrounding words, we will con-duct corpus statistics analysis and select co-occurrence patterns with a confidence greaterthan 70%.
Following the same process, we haveexamined co-occurrence patterns for surroundingPOS tags.?
Subject/Object FeaturesThis set of features aims to capture the relativeposition of the pronoun in a sentence.
It denotesthe preference of pronoun?s position at the clauselevel.
There are 4 features in this category aslisted below.Subject of Main ClauseThis feature indicates whether a pronoun is at thesubject position of a main clause.Subject of Sub-clauseThis feature indicates whether a pronoun is at thesubject position of a sub-clause.Object of Main ClauseThis feature indicates whether a pronoun is at theobject position of a main clause.Object of Sub-clauseThis feature indicates whether a pronoun is at theobject position of a sub-clause.?
Verb of Main/Sub ClauseSimilar to the Subject/Object features of pro-noun, the following two features capture the rela-1190tive position of a verb in a sentence.
It encodesthe preference of verb position between mainverbs in main/sub clauses.Main Verb in Main ClauseThis feature indicates whether a verb is a mainverb in a main clause.Main Verb in Sub-clauseThis feature indicates whether a verb is a mainverb in a sub-clause.2.3 Support Vector MachineIn theory, any discriminative learning algorithmis applicable to learn a classifier for pronoun res-olution.
In our study, we use Support Vector Ma-chine (Vapnik, 1995) to allow the use of kernelsto incorporate the structure feature.
One advan-tage of SVM is that we can use tree kernel ap-proach to capture syntactic parse tree informationin a particular high-dimension space.Suppose a training set   consists of labeledvectors          , where    is the feature vectorof a training instance and    is its class label.
Theclassifier learned by SVM is:where    is the learned parameter for a supportvector   .
An instance   is classified as positiveif       .
Otherwise,   is negative.?
Adjust Hyper-plane with Development DataPrevious works on pronoun resolution such as(Yang et al 2006) used the default setting forhyper-plane which sets       .
And an in-stance is positive if        and negative oth-erwise.
In our study, we look into a method ofadjusting the hyper-plane?s position using devel-opment data to improve the classifier?s perfor-mance.Considering a default model setting for SVMas shown in Figure 2(for illustration purpose, weuse a 2-D example).Figure 2: 2-D SVM IllustrationThe objective of SVM learning process is to finda set of weight vector   which maximizes themargin (defined as) with constraints definedby support vectors.
The separating hyper-plane isgiven by         as bold line in the center.The margin is the region between the two dottedlines (bounded by         and).
The margin is a space without any in-formation from training instances.
The actualhyper-plane may fall in any place within themargin.
It does not necessarily occur in the.However, the hyper-plane is used to separatepositive and negative instances during classifica-tion process without consideration of the margin.Thus if an instance falls in the margin, SVM canonly decide class label from hyper-plane whichmay cause misclassification in the margin.Based on the previous discussion, we proposean adjustment of the hyper-plane using develop-ment data.
For simplicity, we adjust the hyper-plane function value instead of modeling thefunction itself.
The hyper-plane function valuewill be further referred as a threshold  .
The fol-lowing is a modified version of a learned SVMclassifier.where   is the threshold,    is the learned para-meter for a feature    and    is its class label.
Aset of development data is used to adjust the hy-per-plane function threshold   in order to max-imize the accuracy of the learned SVM classifieron development data.
The adjustment of hyper-plane is defined as:where        is an indicator function which out-put 1 if       is same sign as   and 0 otherwise.Thereafter, the learned threshold    is applied tothe testing set.3 Incorporating Structural Syntactic In-formationA parse tree that covers a pronoun and its ante-cedent candidate could provide us much syntac-tic information related to the pair which is expli-citly or implicitly represented in the tree.
There-fore, by comparing the common sub-structuresbetween two trees we can find out to what degreetwo trees contain similar syntactic information,which can be done using a convolution tree ker-nel.
The value returned from tree kernel reflectssimilarity between two instances in syntax.
Such191syntactic similarity can be further combined withother knowledge to compute overall similaritybetween two instances, through a composite ker-nel.
Normally, parsing is done at sentence level.However, in many cases a pronoun and its ante-cedent candidate do not occur in the same sen-tence.
To present their syntactic properties andrelations in a single tree structure, we construct asyntax tree for an entire text, by attaching theparse trees of all its sentences to an upper node.Having obtained the parse tree of a text, we shallconsider how to select the appropriate portion ofthe tree as the structured feature for a given in-stance.
As each instance is related to a pronounand a candidate, the structured feature at leastshould be able to cover both of these two expres-sions.3.1 Structural Syntactic FeatureGenerally, the more substructure of the tree isincluded, the more syntactic information wouldbe provided, but at the same time the more noisyinformation that comes from parsing errorswould likely be introduced.
In our study, we ex-amine three possible structured features that con-tain different substructures of the parse tree:?
Minimum Expansion TreeThis feature records the minimal structure cover-ing both pronoun and its candidate in parse tree.It only includes the nodes occurring in the short-est path connecting the pronoun and its candidate,via the nearest commonly commanding node.When the pronoun and candidate are from differ-ent sentences, we will find a path through pseudo?TOP?
node which links all the parse trees.
Con-sidering the example given in section 1,This was an all-white, all-Christian communitythat all the sudden was taken over -- not takenover, that's a very bad choice of words, but [in-vaded]1 by, perhaps different groups.
[It]2 began when a Hasidic Jewish family boughtone of the town's two meat-packing plants 13years ago.The minimum expansion structural feature of theinstance {invaded, it} is annotated with boldlines and shaded nodes in figure 1.?
Simple Expansion TreeMinimum-Expansion could, to some degree, de-scribe the syntactic relationships between thecandidate and pronoun.
However, it is incapableof capturing the syntactic properties of the can-didate or the pronoun, because the tree structuresurrounding the expression is not taken into con-sideration.
To incorporate such information, fea-ture Simple-Expansion not only contains all thenodes in Minimum-Expansion, but also includesthe first-level children of these nodes2 except thepunctuations.
The simple-expansion structuralfeature of instance {invaded, it} is annotated infigure 2.
In the left sentence?s tree, the node ?NP?for ?perhaps different groups?
is terminated toprovide a clue that we have a noun phrase at theobject position of the candidate verb.It began when a .PRP VBD WRB DT?....NPWHADVPSBARNPVPSVPS?...TOPS..groupsdifferentperhaps,invadedbutwasThis ?...DTNP VPVBD NNSJJRB,VBNCCNPSVPVP PP NPADVPbyINFigure 1: Minimum-Expansion TreeIt began when a .PRP VBD WRB DT?....NPWHADVPSBARNPVPSVPS?...TOPS..groupsdifferentperhaps,invadedbutwasThis ?...DTNP VPVBD NNSJJRB,VBNCCNPSVPVP PP NPADVPbyINFigure 2: Simple Expansion TreeIt began when a .PRP VBD WRB DT?....NPWHADVPSBARNPVPSVPS?...TOPS..groupsdifferentperhaps,invadedbutwasThis ?...DTNP VPVBD NNSJJRB,VBNCCNPSVPVP PP NPADVPbyINFigure 3: Full-Expansion Tree?
Full Expansion TreeThis feature focuses on the whole tree structurebetween the candidate and pronoun.
It not onlyincludes all the nodes in Simple-Expansion, butalso the nodes (beneath the nearest commandingparent) that cover the words between the candi-2 If the pronoun and the candidate are not in the same sen-tence, we will not include the nodes denoting the sentencesbefore the candidate or after the pronoun.192date and the pronoun3.
Such a feature keeps themost information related to the pronoun and can-didate pair.
Figure 3 shows the structure for fea-ture full-expansion for instance {invaded, it}.
Asillustrated, the ?NP?
node for ?perhaps differentgroups?
is further expanded to the POS level.
Allits child nodes are included in the full-expansiontree except the surface words.3.2 Convolution Parse Tree Kernel and Com-posite KernelTo calculate the similarity between two struc-tured features, we use the convolution tree kernelthat is defined by Collins and Duffy (2002) andMoschitti (2004).
Given two trees, the kernelwill enumerate all their sub-trees and use thenumber of common sub-trees as the measure ofsimilarity between two trees.
The above tree ker-nel only aims for the structured feature.
We alsoneed a composite kernel to combine the struc-tured feature and the flat features from section2.2.
In our study we define the composite kernelas follows:where       is the convolution tree kernel de-fined for the structured feature, and       is thekernel applied on the flat features.
Both kernelsare divided by their respective length4 for norma-lization.
The new composite kernel      , de-fined as the sum of normalized       and      ,will return a value close to 1 only if both thestructured features and the flat features have highsimilarity under their respective kernels.3.3 Twin-Candidate Framework using Rank-ing SVM ModelIn a ranking SVM kernel as described in (Mo-schitti et al 2006) for Semantic Role Labeling,two argument annotations (as argument trees) arepresented to the ranking SVM model to decidewhich one is better.
In our case, we present twosyntactic trees from two candidates to the rank-ing SVM model.
The idea is inspired by (Yang,et.al, 2005b;2008).
The intuition behind thetwin-candidate model is to capture the informa-tion of how much one candidate is more pre-3 We will not expand the nodes denoting the sentences otherthan where the pronoun and the candidate occur.4  The length of a kernel   is defined asferred than another.
The candidate wins most ofthe pair wise comparisons is selected as antece-dent.The feature vector for each training instancehas a form of                    .
An in-stance is positive if       is a better antecedentchoice than       .
Otherwise, it is a negativeinstance.
For each feature vector, both tree struc-tural features and flat features are used.
Thuseach feature vector has a form ofwhere    and    are trees of candi-date i and j respectively,    and    are flat featurevectors of candidate i and j respectively.In the training instances generation, we onlygenerate those instances with one candidate isthe correct antecedent.
This follows the samestrategy used in (Yang et al 2008) for objectanaphora resolution.In the resolution process, a list of m candi-dates is extracted from a three sentences window.A total ofinstances are generated by pairing-up the m candidates pair-wisely.
We used aRound-Robin scoring scheme for antecedent se-lection.
Suppose a SVM output for an instanceis 1, we will give a score1 for        and -1 for        and vice versa.
Atlast, the candidate with the highest score is se-lected as antecedent.
In order to handle a non-event anaphoric pronoun, we have set a thresholdto distinguish event anaphoric from non-eventanaphoric.
A pronoun is considered as eventanaphoric if its score is above the threshold.
Inour experiments, we kept a set of developmentdata to find out the threshold in an empirical way.4 Experiments and Discussions4.1 Experimental SetupOntoNotes Release 2.0 English corpus as in(Hovy et al 2006) is used in our study, whichcontains 300k words of English newswire data(from the Wall Street Journal) and 200k words ofEnglish broadcast news data (from ABC, CNN,NBC, Public Radio International and Voice ofAmerica).
Table 1 shows the distribution of var-ious entities.
We focused on the resolution of502 event pronouns encountered in the corpus.The resolution system has to handle both theevent pronoun identification and antecedent se-lection tasks.
To illustrate the difficulty of eventpronoun resolution, 14.7% of all pronoun men-tions are event anaphoric and only 31.5% of193event pronoun can be resolved using ?most re-cent verb?
heuristics.
Therefore a most-recent-verb baseline will yield an f-score 4.63%.To conduct event pronoun resolution, an inputraw text was preprocessed automatically by apipeline of NLP components.
The noun phraseidentification and the predicate-argument extrac-tion were done based on Stanford Parser (Kleinand Manning, 2003a;b) with F-score of 86.32%on Penn Treebank corpus.Non-Event Anaphora:        4952   80.03%EventAnaphora:123519.97%Event NP:        733   59.35%EventPronoun:502   40.65%It:       29.0%This:   16.9%That:  54.1%Table 1: The distribution of various types of 6187anaphora in OntoNotes 2.0For each pronoun encountered during resolu-tion, all the inflectional verbs within the currentand previous two sentences are taken as candi-dates.
For the current sentence, we take onlythose verbs in front of the pronoun.
On average,each event pronoun has 6.93 candidates.
Non-event anaphoric pronouns will generate 7.3 nega-tive instances on average.4.2 Experiment Results and DiscussionIn this section, we will present our experimentalresults with discussions.
The performance meas-ures we used are precision, recall and F-score.All the experiments are done with a 10-foldscross validation.
In each fold of experiments, thewhole corpus is divided into 10 equal sized por-tions.
One of them is selected as testing corpuswhile the remaining 9 are used for training.
Inexperiments with development data, 1 of the 9training portions is kept for development purpose.In case of statistical significance test for differ-ences is needed, a two-tailed, paired-sample Stu-dent?s t-Test is performed at 0.05 level of signi-ficance.In the first set of experiments, we are aimingto investigate the effectiveness of each singleknowledge source.
Table 2 reports the perfor-mance of each individual experiment.
The flatfeature set yields a baseline system with 40.6% f-score.
By using each tree structure along, we canonly achieve a performance of 44.4% f-scoreusing the minimum-expansion tree.
Therefore,we will further investigate the different ways ofcombining flat and syntactic structure knowledgeto improve resolution performances.Precision Recall F-scoreFlat 0.406 0.406 0.406Min-Exp 0.355 0.596 0.444Simple-Exp 0.347 0.512 0.414Full-Exp 0.323 0.476 0.385Table 2: Contribution from Single Knowledge SourceThe second set of experiments is conducted toverify the performances of various tree structurescombined with flat features.
The performancesare reported in table 3.
Each experiment is re-ported with two performances.
The upper one isdone with default hyper-plane setting.
The lowerone is done using the hyper-plane adjustment aswe discussed in section 2.3.Precision Recall F-scoreMin-Exp +Flat0.433 0.512 0.469(0.727) (0.446) (0.553)Simple-Exp+Flat0.423 0.534 0.472(0.652) (0.492) (0.561)Full-Exp +Flat0.416 0.526 0.465(0.638) (0.496) (0.558)Table 3: Comparison of Different Tree Structure +FlatAs table 3 shows, minimum-expansion giveshighest precision in both experiment settings.Minimum-expansion emphasizes syntactic struc-tures linking the anaphor and antecedent.
Al-though using only the syntactic path may lose thecontextual information, but it also prune out thepotential noise within the contextual structures.In contrast, the full-expansion gives the highestrecall.
This is probably due to the widest know-ledge coverage provides by the full-expansionsyntactic tree.
As a trade-off, the precision offull-expansion is the lowest in the experiments.One reason for this may be due to OntoNotescorpus is from broadcasting news domain.
Itstexts are less-formally structured.
Another typeof noise is that a narrator of news may read anabnormally long sentence.
It should appear asseveral separate sentences in a news article.However, in broadcasting news, these sentencesmaybe simply joined by conjunction word ?and?.Thus a very nasty and noisy structure is createdfrom it.
Comparing the three knowledge source,simple-expansion achieves moderate precisionand recall which results in the highest f-score.From this, we can draw a conclusion that simple-expansion achieves a balance between the indica-tive structural information and introduced noises.In the next set of experiments, we will com-pare different setting for training instances gen-eration.
A typical setting contains no negative194instances generated from non-event anaphoricpronoun.
This is not an issue for object pronounresolution as majority of pronouns in an article isanaphoric.
However in our case, the event pro-noun consists of only 14.7% of the total pro-nouns in OntoNotes.
Thus we incorporate theinstances from non-event pronouns to improvethe precision of the classifier.
However, if weinclude all the negative instances from non-eventanaphoric pronouns, the positive instances willbe overwhelmed by the negative instances.
Adown sampling is applied to the training in-stances to create a more balanced class distribu-tion.
Table 4 reports various training settingsusing simple-expansion tree structure.Simple-Exp Tree Precision Recall F-scoreWithout Non-event Negative0.423 0.534 0.472Incl.
All Negative 0.733 0.410 0.526Balanced Negative 0.599 0.506 0.549Development Data 0.652 0.492 0.561Table 4: Comparison of Training Setup, Simple-ExpIn table 4, the first line is experiment withoutany negative instances from non-event pronouns.The second line is the performance with all nega-tive instances from non-event pronouns.
Thirdline is performance using a balanced training setusing down sampling.
The last line is experimentusing hyper-plane adjustment.
The first linegives the highest recall measure because it has nodiscriminative knowledge on non-event anaphor-ic pronoun.
The second line yields the highestprecision which complies with our claim thatincluding negative instances from non-eventpronouns will improve precision of the classifierbecause more discriminative power is given bynon-event pronoun instances.
The balanced train-ing set achieves a better f-score comparing tomodels with no/all negative instances.
This isbecause balanced training set provides a betterweighted positive/negative instances which im-plies a balanced positive/negative knowledgerepresentation.
As a result of that, we achieve abetter balanced f-score.
In (Ng and Cardie,2002b), they concluded that only the negativeinstances in between the anaphor and antecedentare useful in the resolution.
It is same as ourstrategy without negative instances from non-event anaphoric pronouns.
However, our studyshowed an improvement by adding in negativeinstances from non-event anaphoric pronouns asshowed in table 4.
This is probably due to ourrandom sampling strategy over the negative in-stances near to the event anaphoric instances.
Itempowers the system with more discriminativepower.
The best performance is given by the hy-per-plane adaptation model.
Although the num-ber of training instances is further reduced fordevelopment data, we can have an adjustment ofthe hyper-plane which is more fit to dataset.In the last set of experiments, we will presentthe performance from the twin-candidates basedapproach in table 5.
The first line is the best per-formance from single candidate system with hy-per-plane adaptation.
The second line is perfor-mance using the twin-candidates approach.Simple-Exp Tree Precision Recall F-scoreSingle Candidate 0.652 0.492 0.561Twin-Candidates 0.626 0.540 0.579Table 5: Single vs. Twin Candidates, Simple-ExpComparing to the single candidate model, therecall is significantly improved with a smalltrade-off in precision.
The difference in results isstatistically significant using t-test at 5% level ofsignificance.
It reinforced our intuition that pre-ferences between two candidates are contributiveinformation sources in co-reference resolution.5 Conclusion and Future WorkThe purpose of this paper is to conduct a syste-matic study of the event pronoun resolution.
Wepropose a resolution system utilizing a set of flatpositional, lexical and syntactic feature andstructural syntactic feature.
The state-of-artsconvolution tree kernel is used to extract indica-tive structural syntactic knowledge.
A twin-candidates preference learning based approach isincorporated to reinforce the resolution systemwith candidates?
preferences knowledge.
Last butnot least, we also proposed a study of the variousincorporations of negative training instances,specially using random sampling to handle theimbalanced data.
Development data is also usedto select more accurate hyper-plane in SVM forbetter determination.To further our research work, we plan to em-ploy more semantic information into the systemsuch as semantic role labels and verb frames.AcknowledgmentWe would like to thank Professor Massimo Poesiofrom University of Trento for the initial discussion ofthis work.195ReferencesN.
Asher.
1993.
Reference to Abstract Objects in Dis-course.
Kluwer Academic Publisher.
1993.V.
Vapnik.
1995.
The Nature of Statistical LearningTheory.
Springer.1995.M.
Kubat and S. Matwin, 1997.
Addressing the curseof imbalanced data set: One sided sampling.
InProceedings of the Fourteenth International Con-ference on Machine Learning,1997.
pg179?186.T.
Joachims.
1999.
Making large-scale svm learningpractical.
In Advances in Kernel Methods - Sup-port Vector Learning.
MIT Press.1999.W.
Soon, H. Ng, and D. Lim.
2001.
A machine learn-ing approach to coreference resolution of nounphrases.
In Computational Linguistics, Vol:27(4),pg521?
544.D.
Byron.
2002.
Resolving Pronominal Reference toAbstract Entities, in Proceedings of the 40th An-nual Meeting of the Association for ComputationalLinguistics (ACL?02).
July 2002. , USAM.
Collins and N. Duffy.
2002.
New ranking algo-rithms for parsing and tagging: Kernels over dis-crete structures, and the voted perceptron.
In Pro-ceedings of the 40th Annual Meeting of the Associ-ation for Computational Linguistics (ACL?02).
July2002.
, USAV.
Ng and C. Cardie.
2002a.
Improving machinelearning approaches to coreference resolution.
InProceedings of the 40th Annual Meeting of the As-sociation for Computational Linguistics (ACL?02).July 2002. , USA.
pg104?111.V.
Ng, and C. Cardie.
2002b.
Identifying anaphoricand non-anaphoric noun phrases to improve core-ference resolution.
In Proceedings of the 19th In-ternational Conference on Computational Linguis-tics (COLING02).
(2002)M. Strube and C. M?ller.
2003.
A Machine LearningApproach to Pronoun Resolution in Spoken Dialo-gue.
.
In Proceedings of the 41st Annual Meeting ofthe Association for Computational Linguistics(ACL?03), 2003D.
Klein and C. Manning.
2003a.
Fast Exact Infe-rence with a Factored Model for Natural LanguageParsing.
In Advances in Neural InformationProcessing Systems 15 (NIPS 2002), Cambridge,MA: MIT Press, pp.
3-10.D.
Klein and C.Manning.
2003b.
Accurate Unlexica-lized Parsing.
In Proceedings of the 41st AnnualMeeting of the Association for Computational Lin-guistics (ACL?03), 2003.  pg423-430.X.
Yang, G. Zhou, J. Su, and C.Tan.
2003.
Corefe-rence Resolution Using Competition Learning Ap-proach.
In Proceedings of the 41st Annual Meetingof the Association for Computational Linguistics(ACL?03), 2003. pg176?183.A.
Moschitti.
2004.
A study on convolution kernelsfor shallow semantic parsing.
In Proceedings ofthe 42nd Annual Meeting of the Association forComputational Linguistics (ACL?04), pg335?342.A.
Estabrooks, T. Jo, and N. Japkowicz.
2004.
A mul-tiple resampling method for learning from imba-lanced data sets.
In Computational IntelligenceVol:20(1).
pg18?36.X.
Yang, J. Su, G. Zhou, and C. Tan.
2004.
Improvingpronoun resolution by incorporating coreferentialinformation of candidates.
In Proceedings of 42thAnnual Meeting of the Association for Computa-tional Linguistics, 2004. pg127?134.X.
Yang, J. Su and C.Tan.
2005a.
Improving PronounResolution Using Statistics-Based Semantic Com-patibility Information.
In Proceedings of Proceed-ings of the 43rd Annual Meeting of the Associationfor Computational Linguistics (ACL?05).
June2005.X.
Yang, J. Su and C.Tan.
2005b.
A Twin-CandidatesModel for Coreference Resolution with Non-Anaphoric Identification Capability.
In Proceed-ings of IJCNLP-2005.
Pp.
719-730, 2005E.
Hovy, M. Marcus, M. Palmer, L. Ramshaw, and R.Weischedel.
2006.
OntoNotes: The 90\% Solution.In Proceedings of the Human Language Technol-ogy Conference of the NAACL, 2006X.
Yang, J. Su and C.Tan.
2006.
Kernel-Based Pro-noun Resolution with Structured Syntactic Know-ledge.
In Proceedings of the 44th Annual Meetingof the Association for Computational Linguistics(ACL?06).
July 2006.
Australia.A.
Moschitti, Making tree kernels practical for naturallanguage learning.
In Proceedings EACL 2006,Trento, Italy, 2006.C.
M?ller.
2007.
Resolving it, this, and that in unre-stricted multi-party dialog.
In Proceedings of the45th Annual Meeting of the Association for Com-putational Linguistics (ACL?07).
2007.
Czech Re-public.
pg816?823.X.
Yang, J. Su and C.Tan.
2008.
A Twin-CandidatesModel for Learning-Based Coreference Resolution.In Computational Linguistics, Vol:34(3).
pg327-356.S.
Pradhan, L. Ramshaw, R. Weischedel, J. Mac-Bride, and L. Micciulla.
2007.
Unrestricted Corefe-rence: Identifying Entities and Events in Onto-Notes.
In Proceedings of the IEEE InternationalConference on Semantic Computing (ICSC), Sep.2007.196
