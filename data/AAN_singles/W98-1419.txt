TEXTUAL ECONOMY THROUGH CLOSE COUPLING OF SYNTAX AND SEMANTICS?
Matthew Stone Bonnie WebberDept.
of Computer & Information ScienceUniversity of Pennsylvania200 South 33rd StreetPhiladelphia PA 19104-6389 USAmatthewOlinc, cis.
upenn, edu, bonnie@central, cis.
upenn, eduAbstractWe focus on the production of efficient descriptions ofobjects, actions and events.
We define atype ofefficiency, textual economy, that exploits the hearer's recognition ofinferential links to material elsewherewithin a sentence.
Textual economy leads to efficient descriptions because the material that supports suchinferences has been included to satisfy independent communicative goals, and is therefore overloadedin the sense of Pollack \[18\].
We argue that achieving textual economy imposes trong requirementson the representation and reasoning used in generating sentences.
?
The representation must support thegenerator's simultaneous consideration f syntax and semantics.
Reasoningmust enable the generatorto assess quickly and reliably at any stage how the hearer will interpret the current sentence, with its-(inc0mplete)syntax,andsemantics.
We show that hese representational and reasoning requirements aremet in the SPUD system for sentence planning and realization.1 IntroductionTh e problem we address is that of producing efficient descriptions of objects, collections, acti0ns, events,etc.
(i.e., any generalized individual from a rich ontology for Natural Language such as those describedin \[2\] ?and advocated in \[9\]).
We are interested in a particular kind of efficiency that we call textualeconomy, which presupposes a view of sentence generation as goal-directed activity that has broad supportin Natural Language Generation (NLG) research \[1, 5, 15, 17\].
According to this view, a system has certaincommunicative intentions that it aims to fulfill in producing a description.
For example, the system mighthave the goal of identifying an individual or action o~ to the hearer, or ensuring that the hearer knows thathas property P. Such goals can be satisfied explicitly by assembling appropriate syntactic onstituents--forexample, ?
satisfying the goal of identifying an individual using a noun phrase that refers to it or identifyingan action using a verb phrase that specifies it.
Textual economy refers to satisfying such goals implicitly,by exploiting the hearer's (or reader's) recognition of inferential links to material elsewhere in the Sentencethat is there to satisfy independent communicative goals.
Such material is therefore overloaded in the senseof \[18\].
l While there are other ways of increasing the efficiency of descriptions (Section 5), our focus is?
on the efficiency to be gained by viewing a large part of generation i  terms of describing (generalized)individuals.Achieving this however places strong requirements on the representation a d reasoning used in gen-erating sentences.
The representation must support he generator's proceeding incrementally through thesyntax and semantics of the sentence as a whole.
The reasoning used must enable the generator to assessquickly and reliably at any stage how the hearer will interpret the current sentence, with its (incomplete)syntax and semantics.
Only by evaluating the status of such key questions as?
what (generalized) individuals could the sentence (or its parts) refer to?1Pollack used the term overloading torefer to cases where asingle intention toact is used to wholly or partially satisfy severalof an agent's goals imultaneously.i 78IIilIIilIIIIflIilFigure 1: "Remove the rabbit from the hat."?
what (generalized) individuals would the hearer take the sentence to refer to??
what would the sentence invite the hearer to conclude about hose individuals??
how can this sentence be modified or extended?can the generator recognize and exploit an opportunity for textual economy.These representational and reasoning requirements are met in the SPUD system for sentence planningand realization \[26, 27\].
SPUD draws on earlier work by Appelt \[1\] in building sentences using planningtechniques, PUD plans the syntax and semantics of a sentence by incorporating lexico-grammatical entriesinto a partial sentence one-by-one and incrementally assessing the answers to the questions given above.
Inthis paper, we describe the intermediate r presentations that allow SPUD to do so, since these representationshave been glossed over in earlier presentations \[26, 27\].
Reasoning in SPUD is performed using a fast modaltheorem prover \[24, 25\] to keep track both of what the sentence ntails and what the sentence requires incontext.
By reasoning about he predicated relationships withinclauses and the informational relationships\[16\] between clauses, sPUD is able to generate sentences that exhibit two forms of textual economy:referential interdependency among noun phrases within a single clause, and pragmatic overloading ofclauses in instructions \[7\].For an informal example of the textual economy to be gained by taking advantage of predicatedrelationships within clauses, consider the scene pictured in Figure 1 and the goal of getting the hearer to takethe rabbit currently in the hat out of the hat it's currently in.
Even though there are several rabbits, severalhats, and even a rabbit in a bathtub and a flower in a hat, it would be sufficient here to issue the command:(1) Remove the rabbit from the hat.It suffices because one of the semantic features of the verb remove--that itsobject (here, the rabbit) startsout in the source (here, the hat).
distinguishes the intended rabbit and hat in Figure 1 from the other ones.Pragmatic overloading \[7\] illustrates how an informational relation between clauses can support textualeconomy in the clauses that serve as its "arguments".
In \[7\], we focused on describing (complex) actions,showing how a clause interpreted asconveying the goal13 or termination condition r of an action a partiallyspecified in a related clause forms the basis of a constrained inference that provides additional informationabout a.
For example,(2a) Hold the cup under the spigot...(2b) ...to fill it with coffee.179Here, the two clauses (2a) and (2b) are related by purpose---specifically, enablement.
The action t~ describedin (2a) will enable the actor to achieve the goal/3 described in (2b).
While a itself does not specify theorientation of the cup under the spigot, its purpose fl can lead the hearer to an appropriate choice---4o fill acup with coffee, the cup must be held vertically, with its concavity pointing upwards.
As- noted in \[7\], thisconstraint depends crucially on the purpose for which a is performed.
The purpose specified i n (3b) doesnot constrain cup orientation i the same way:(3a) Hold the cup under the faucet...(3b) ...to wash it.Examples like (1) and?
(2) suggest that the natural ocality for sentence planning is in a descriptionof a generalized individual.
Even though such descriptions may play out over several clauses (or evensentences), the predications within clauses and the informational relations across clauses of a descriptiongive rise to similar textual economies, that merit a similar treatment.2 SPUD?An NLG system must satisfy at least three constraints in mapping the content planned for a sentence ontothe string of words that realize it \[4, 13, 20\].
Any fact to be communicated must be fit into an abstractgrammatical structure, including lexical items.
Any reference to a domain entity must be elaborated into adescription that distinguishes the entity from its distractors--the salient alternatives toit in context.
Finally,a surface form must be found for this conceptual material.In one architecture for NLG Systems that is becoming something of a standard \[22\], these tasks areperformed in Separate stages.
For example, to refer to a uniquely identifiable ntity x from the commonground, first a set of concepts is identified that together single out x from its distractors in context.
Onlylater is the syntactic structure that realizes those concepts derived.SPUD \[26, 27\] integrates these processes ingenerating a description--producing both syntax and seman-tics simultaneously, in stages, as illustrated in (4).?
(4)NP:Xb DET N:xI Ithe booki cNP 'xDET N:xIthe N N:XI Ituber book?
Each step adds to the representation a lexicalized entry encoded as an elementary tree in Feature-basedLexicalized Tree-Adjoining Grammar (LTAG) \[23\].
A tree may contain multiple lexical items (cf.
(4)b).Each such tree is paired with logical formulae that, by referring to a rich discourse model, characterize thesemantic and pragmatic contribution that it makes to the sentence.
We give a detailed example of SPUD'sprocessing in Section 3 and describe in Section 4 the reasoning methods we use to derive computationalrepresentations like the set of distractors shown in (4).
For now, a general understanding of SPUD suffices--this is provided by the summary in Figure 2.The procedure in Figure 2 is  sufficiently general so that SPUD?
can use similar steps to construct bothdefinite and indefinite referring forms.
The main difference lies how alternatives are evaluated.
When anindefinite referring form is used to refer tO a brand-new generalized individual \[19\] (an object, for example,180?
Start with a tree with one node (e.g., s, uP) and one or more referential or informational goals.?
While the current ree is incomplete, or its references are ambiguous to the hearer, or itsmeaning does not fully convey the informational goals (provided progress is being made):- consider the trees that extend the current one by the addition (using LTAG operations)of a true and appropriate l xicalized escriptor;- rank the results based on local factors (e.g., completeness of meaning, distractors forreference, unfilled substitution sites, specificity of licensing conditions);- make the highest ranking new tree the current ree.Figure 2: An outline of the SPUD algorithmor an action in an instruction), the object is marked as new and does not have to be distinguished from othersbecause the hearer creates afresh "file card" for it.
However, because the domain typically provides featuresneeded in an appropriate description for the object, SPUD continues its incremental ddition of content oconvey them, When an indefinite form is used to refer to an old object that cannot be distinguished fromother elements of a uniquely identifiable set (typically an inferrable ntity \[ 19\]), aprocess like that illustratedin (4) must build a description that identifies this set, based on the known common properties of its elements.Several advantages of using LTAG in such an integrated system are described in \[27\] (See also previouswork on using TAG in NLG such as \[11\] and \[29\]).
These advantages include:?
Syntactic onstraints can be handled early and naturally.
In the problem illustrated in (4), SPUDdirectly encodes the syntactic requirement that a description should have a head noun--missing fromthe concept-level account--using the NP substitution site.?
The order of  adding content is flexible.
Because an LTAG derivation allows modifiers to adjoin atany step (unlike a top-down CFG derivation), there is no tension between providing what the syntaxrequires and going beyond what the syntax requires.?
Grammatical knowledge is Stated once only.
All operations in constructing a sentence are guidedby LTAG's lexicalized grammar; by contrast, with separate processing, the lexicon is split into aninventory of concepts (used for organizing content orconstructing descriptions) and a further inventoryof concepts in correspondence With some syntax (for surface realization).This paper delineates a less obvious, but equally significant advantage that follows from the ability toconsider multiple goals in generating descriptions, using a representation a d a reasoning process in whichsyntax and semantics are more closely linked:?
It naturally supports textual economy.3 Achieving Textual EconomyTo see how SPUD supports textual economy, Consider first how SPUD might derive the instruction in Ex-ample (1).
For simplicity, this explanation assumes SPUD makes a nondeterministic choice from amongavailable lexical entries; this suffices to illustrate how SPUD can realize the textual economy of this example.A priori, SPUD has a general goal of describing a new action that the hearer is to perform, by makingsure the hearer can identify the key features that allow its performance.
For (1), then, SPUD is given two181features of the action to be described: it involves motion of an intended object by the agent, and its result isachieved when the object reaches aplace decisively away from its starting point.The first time through the loop of Figure 2, SPUD must expand an s node.
One of the applicable movesis to substitute a lexical entry for the verb remove.
Of the dements in the verb's LTAG tree family, the onethat fits the instructional context is the imperative tree of (5).NP: (REMOVER>E(5a) Syntax:S: (TIME, REMOVING>va: ( , MOVIN ,SOURCE)v <REMOVED>Iremovenucleus(PREP, REMOVING, RESULT) A in(PREP, start(TIME), REMOVED, SOURCE) A(5b) Semantics: caused-motion(REMOVING, REMOVER, REMOVED) Aaway(RESULT, end(TIME), REMOVED, SOURCE)The tree given in (5a) specifies that remove syntactically satisfies a requirement tOinclude an s, requires afurther NP to be included (describing what is removed), and allows the possibility of an explicit vv modifierthat describes what the latter has been removed from.
2 The semantics in (5b) consists of a set of features,formulated in an ontologically promiscuous semantics, as advocated in \[9\].
It follows \[14\] in viewing eventsas consisting of a preparatory phase, a transition, and a result state (what is called a nucleus in \[14\]).
Thesemantics in (5b) describes all parts of a remove event: In the preparatory phase, the object (REMOVED) isin/on SOURCE.
It undergoes motion caused by the agent (REMOVER), and ends up away from SOURCE in theresult state.Semantic features are used by SPUD in one of two ways.
Some make a semantic ontribution thatspecifies new information---these add to what new information the speaker can convey with the structure.Others simply impose a semantic requirement that a fact must be part of the conversational record--thesefigure in ruling out distractors.For this instruction, SPUD treats the CAUSED-MOTION and AWAY semantic features as semantic ontribu-tions."
It therefore determines that the use of this item communicates the needed features of the action.
Atthe same time, it treats the IN feature---because it refers to the shared initial state in which the instructionwill be executed--and the NUCLEUS feature---because it simply refers to our general ontology--as semanticrequirements.
SPUD therefore determines that the only (REMOVED,SOURCE) pairs that the hearer might hinkthe instruction could refer to are pairs where REMOVED starts out in/on SOURCE as the action begins.Thus, SPUD derives a triple effect from use of the word remove--increasing syntactic satisfaction,making semantic ontributions and satisfying semantic requirements--all of which contribute to SPUD'stask of completing an S syntactic onstituent that conveys needed content and refers successfully.
Suchmultiple ffects make it natural for SPUD to achieve textual economy.
Positive effects on any of the abovedimensions can suffice to merit inclusion of an item in a given sentence.
However, the effects of inclusionmay go beyond this: even if an item is chosen for its semantic ontribution, its semantic requirements can2Other possibilities are that SOURCE is not mentioned explicitlyl but is rather inferred from (1) the previous discourse or, as wewill discuss later, (2) either the predicated relationships within the clause or its informational relationship to another clause.182still be exploited in establishing whether the current lexico-syntactic description is sufficient to identify anentity, and its syntactic ontributions can still be exploited to add further content.Since the current ree is incomplete and referentially ambiguous, PUD repeats the loop of Figure 2,considering trees that extend it.
One option is to adjoin at the w the entry corresponding to from the hat.
Inthis compound entry, from matches the verb and the matches the context; hat carries semantics, requiringthat SOURCE be a hat.
After adjunction, the requirements reflect both remove and hat; reference, sPUDcomputes, has been narrowed to the hats that have something in/on them (the rabbit, the flower).Another option is to substitute the entry for the rabbit at the object NP; this imposes the requirement thatREMOVED be a rabbit.
Suppose sPUD discards this option in this iteration, making the other (perhaPS .
lessreferentially ambiguous) choice.
At the next iteration, the rabbit still remains an option.
Now combiningwith remove and hat, it derives asentence that SpUD recognizes to be complete and referentially unambiguous,and to satisfy the informational goals.
?Now we consider the derivation of (2), which shows how an informational relation between clauses cansupport extual economy in the clauses that serve as its "arguments".
SPUD starts with the goal of describingthe holding action in the main clause, and (if possible) also describing the filling action and indicating thepurpose relation (i.e., enablement) between them.
For the homing action, sPUD's goals include making surethat the sentence communicates where the cup will be held and how it will be held (i.e., UPWARD).
SPUD firstselects an appropriate l xico-syntactic tree for imperative hold; sPUD can choose to adjoin in the purposeclause next, and then to substitute in an appropriate l xico-syntactic ree forfill.
After this substitution, thesemantic ontributions of the sentence describe an action of holding an object which generates an actionof filling that object.
As shown ?in \[7\], these are the premises of an inference that the object is held uprightduring the filling.
When SPUD queries its goals at this stage, it thus finds that it has in fact conveyed howthe cup is to be held.
SPUD has no reason to describe the orientation of the cup with additional content.Additional examples of using SPUD to generate instructions can be found in \[3, 25\].4 Assessing interpretation in SPUDThis section describes in a bit more detail how SPUD computes the effects of incorporating a particularlexical item into the sentence being constructed.
For a more extensive discussion, see \[25\].?
sPUD's computations depend on its representation f overall contextual background, including the statusof propositions and entities in the discourse.
For the purpose of generating instructions toa single hearer, weassume that any proposition falls either within the private knowledge of the speaker or within the commonground that speaker and hearer share.
We implement this distinction by specifying facts in a modal ogicwith an explicit representation f knowledge: \[siP means that the speaker knows p; \[?\]p means that p ispart of the common ground.
Each entity, e, comes with a context set D(e) including it and its distractors.Linguistically, when we have a E D(b) but not b E D(a), then a is more salient han b.This conversational background serves as a resource for constructing and evaluating a three-part state-record for an incomplete sentence, consisting of:?
An instantiated tree describing the syntactic structure of the sentence under construction.
Its nodesare labeled by a sequence of variables v indicating the patterns of coreference in the tree; but the treealso records that the speaker intends v to refer to a particular sequence of generalized individuals e.?
The semantic requirements of the tree, represented by a formula R(v).
This formula must matchfacts in the common ground; in our modal specification, such a match corresponds to a proof whoseconclusion instantiates \[C\]R(v).
In particular, the speaker ensures that such a proof is available whenv is instantiated tothe entities e that the speaker means to refer to.
This determines what alternative183referents that the hearer may still consider: { a E D(e) \[ \[c\]R(a) }.
The semantic requirements ofthe tree result from conjoining the requirements Ri(vi) of the ?individual exical items from which thestate is derived.The semantic ontributions of the tree, represented by a formula N(v); again, this ~s the conjunctionof the contributions Ni(vi) of the individual items.
These contributions are added to the commonground, allowing both speaker and hearer to draw shared conclusions from them.
This has inspiredthe following test for whether a goal to communicate G has been indirectly achieved.
Considerthe content of the discourse as represented by \[C\], augmented by what this sentence will contribute(assuming we identify entities as needed for reference): N(e).
Then if G follows, the speaker has -conveyed what is needed.When SPUD considers extending a state by a lexical item, it must be able to update ach of these recordsquickly.
The heart of sPUD's approach is logic programming \[25\], which links complexity of computationand complexity of the domain in a predictable way.
For example, informational goals are assessed bythe  query \[c\](N(e) D G)I This leaves room for inference when necessary, without swamping sPUD; inpractice, G is often a primitive feature of the domain and the query reduces to a simple matching operation.Another source of tractability comes from combining logic programming with special-purpose r asoning.For example, in computing reference, { ai E D(ei) \[ \[c\]Ri(ai) } is found using logic programming but theoverall set of alternatives i maintained using arc-consistency onstraint-satisfaction, as in \[6, 8\].SPUD must also ?settle which semantic features are taken to constitute the semantic requirements of thelexical item and which are taken to constituteits semantic ontributions.
3 When SPUD partitions the semanticfeatures of the lexical item, as many features as possible are cast as requirements~that is, he item links asstrongly with the context as possible.
In some cases, the syntactic environment may further constrain thisassignment.
For example, we constrain items included in a definite NP to be semantic requirements, whilethe main verb in an indicative sentence is usually taken to make a semantic ontribution.
(Exceptions tosuch a policy are justified in \[28\].
)3These can vary with context: consider a variant on Figure 1, where the hearer isasked "What just happened?
".One possible response-- "I have removed the rabbit from the hat" m refers uccessfully, despite the many rabbits and hats, becausethere is still only one rabbit in this scene that could have been removed from a hat.
Here, where the scene is taken as shared, whatis taken as a semantic requirement of remove---thatthe rabbit ends up away from the hat--is used to identify aunique rabbit.
Thiscontrasts with the previous "rabbit" example where, taking the scene in Figure 1 as shared, the command "Remove the rabbit fromthe hat" takes as its semantic requirement that he rabbit be in the hat and uses it for unique identification.
Note that if the abovescene is not taken as shared, both are then taken as semantic contributions, and "I have removed a rabbit from a hat" becomes anacceptable answer.184III|IIIIIilIIIIII!IIItIIIIIIIIIFigure 3: "'The table with the apple and with the banana"5 Other Methods that Contribute to Eflicient DescriptionsThis section contrasts spoI>--and its close coupling of syntax and semantics--with prior work on generatingmore concise descriptions by considering the effects of broader goals, 4 starting with Appelt \[1\].
Appelt'splanning formalism includes plan-critics that can detect and collapse redundancies in sentence plans.However, his framework treats ubproblems in generation as independent bydefault; and writing tractableand general critics is hampered by the absence of abstractions like those used in SPUD to simultaneouslymodel the syntax and the interpretation f a whole sentence.\[6, 10, 12\], in contrast, use specialized mechanisms tocapture particular descriptive fficiencies.
Byusing syntax to work on inferential nd referential problems imultaneously, SPUD captures uch efficienciesin a uniform procedure.
For example, in \[12\], McDonald considers descriptions ofevents in domains whichimpose strong constraints on what information about events is semantically relevant.
He shows that suchmaterial should and can be omitted, if it is both syntactically optional and inferentially derivable:FAIRCHILD Corporation (Chantilly VA) Donald E Miller was named senior vice president andgeneral counsel, succeeding Dominic A Petito, who resigned in November, at this aerospacebusiness.
Mr. Miller, 43 years old, was previouslY principal attorney for Temkin & Miller Ltd.,Providence RI.Here, McDonald points out that one does not need to explicitly mention the position that Petito resignedfrom in specifying the resignation sub-event, since it must be the same as the one that Miller has beenappointed to.
This can be seen as a special case of pragmatic overloading.Meanwhile, Dale and Haddock \[6\] consider generating interacting references, building on Haddock'swork on reference resolution \[8\].
Their example NP, the rabbit in the hat, refers successfully in a contextwith many rabbits and many hats, so long as only one of the rabbits, 1"5 say, is actually in one of thehats, h3 say.
Like (1), the efficiency of this description comes from the uniqueness of this rabbit-hat pair.However, Dale and Haddock construct NP semantics in isolation and adopt a fixed, depth-first strategy foradding content.
Horacek \[10\], challenges this strategy with examples that show the need for modificationat multiple points in an NP.
For example, (6) refers with respect to the scene in Figure 3.
(6) the table with the apple and with the banana.
(6) identifies a unique table by exploiting its association with two objects it supports: the apple and thebanana that are on it.
(Note the other tables, apples and bananas in the figure--and even tables with applesand tables with bananas.)
Reference to one of these--the apple, say--is incorporated into the description4Other ways of making descriptions more concise, such as through the use of anaphoric and deictic pronouns (or even pointing,in multi-modal contexts), are parasitic On the heater's focus of attention, which can (in large part) be defined independently ofgoal-directed features of text.185first; then that (subordinate) ntity is identified by further describing the table (higher up).5 By consideringsentences rather than isolated noun phrases, SPUD extends uch descriptive capacities even further.6 Remarks and ConclusionIn this paper, we have shown how the semantics associated with predication within clauses and informationalrelations between clauses can be used to achieve textual economy in a system (SPUD) that closely couplessyntax and semantics.
In both cases, efficiency depends only on the informational consequences of currentlexico-syntactic choices in describing the generalized individual of interest;i there is no appeal to informationavailable in the discourse context, which is already well-known as a source of economy, licensing th e useof anaphoric and deictic forms, the use of ellipsis, etc.
Thus, we claim that this approach truly advancescurrent capabilities in NLG .
.
.
.
.
.Finally, we must make clear that we are talking about he possibility of producing a particular description(one in which a wider range of inferrable material is elided); we are not making claims about a particularalgorithm that exploits uch a capability.
Thus it is not relevant here to question computational complexity orlook for a comparison with algorithms previously proposed by Dale, Reiter, Horacek and others \[4, 10, 21\]that compute "minimal" descriptions of some form.
Currently, the control algorithm used in the SPUDgenerator is the simple greedy algorithm described in \[26, 27\] and summarized in Figure 2.
The importantpoint is that the process enables inferences to be performed that allow more economical texts: the nextstep is to address the complexity issues that these other authors have elaborated and show how SPUD'sdescription extension and verification process can be incorporated into a more efficient or more flexiblecontrol structure.7 AcknowledgmentsSupport for this work has come from an IRCS graduate fellowship, the Advanced Research ProjectAgency (ARPA) under grant N6600194C6-043 and the Army Research Organization (ARO) under grantDAAHO494GO426.
Thanks go to Mark Steedman and members of the SPUD group: Christy Doran, GannBierner, Tonia Bleam, Julie Bourne, Aravind Joshi, Martha Palmer, andAnoop Sarkar.References\[1\] Douglas Appelt.
Planning English Sentences.
Cambridge University Press, Cambridge England, 1985.\[2\] Emmon Barh.
Informal Lectures on Formal Semantics.
State University of New York Press, Albany, NY, 1989.\[3\] Juliet Bourne.
Generating effective instructions: Knowingwhen to stop.
PhD Thesis Proposal, Department ofComputer & Information Science, University of Pennsylvania, July 1998.\[4\] Robert Dale.
Generating Referring Expressions in a Domain of Objects and Processes.
PhD thesis, Centre forCognitive Science, University Of Edinburgh, 1989.\[5\] Robert Dale.
Generating Referring Expressions.
MIT Press, Cambridge MA, 1992.\[6\] Robert Dale and Nicholas Haddock.
Content determination in the generation of referring expressions.
Compu-tational Intelligence, 7(4):252-265, 1991.\[7\] Barbara Di Eugeni 0 and Bonnie Webber.
Pragmatic overloading in natural language instructions, lnternationlJournal of Expert Systems, 9(2):53-84, 1996.5Horacek also stresses that there is no need to describe these auxiliary objects eparately oridentify them uniquely.
For example,the table with two fruits provides apossible alternative to(6).
We see no principled obstacle to expressing the same insight in SPUD.However, because of SPUD's close coupling between syntax and semantics, this analysis must await development of a semanticanalysis of plurality in SPUD.186!II1I!II!IIIIIIII\[8\] Nicholas Haddock.
Incremental Semantics and Interactive Syntactic Processing.
PhD thesis, University ofEdinburgh, 1989.\[9\] Jerry R. Hobbs.
Ontological promiscuity.
In Proceedings of ACL, pages 61-69, 1985.\[10\] Helmut Horacek.
More on generating referring expressions.
In Proceedings of the Fifth Eurbpean Workshop onNatural Language Generation, pages 43-58, Leiden, 1995.\[11\] Aravind K. Joshi.
The relevance of tree adjoining rammar to generation.
In Gerard Kempen, editor, NaturalLanguage Generation, pages 233-252.
Martinus NijhoffPress, Dordrecht, The Netherlands, 1987.\[ 12\] David McDonald.
Type-driven suppression of redundancy in the generation of inference-rich reports.
In RobertDale, Eduard Hovy, Dietmar R0sner, and Oliviero Stock, editors, Aspects of Automated Natural Language Gen-eration: 6th International Workshop on Natural Language Generation, Lecture Notes in Artificial Intelligence587, pages 73-88.
Springer Verlag, Berlin, 1992.\[13\] Marie W. Meteer.
Bridging the generation gap between text planning and linguistic realization.
ComputationalIntelligence, 7(4):296-304, 1991.\[14\] Marc Moens and Mark Steedman.
Temporal ontology and temporal reference.
Computational Linguistics,14(2): 15-28, 1988.\[ 15\] Johanna Moore.
Participating inExplanatory Dialogues.
MIT Press, Cambridge MA, 1994.\[16\] Johanna Moore and Martha Pollack.
A problem for RST: The need for multi-levei discouse analysis.
Computa-tional Linguistics, 18(4):537-544, 1992.\[ 17\] Johanna D. Moore and Ctcile L. Paris.
Planning text for advisory dialogues: capturing intentional and rhetoricalinformation.
Computational Linguistics, 19(4):651-695, 1993.\[ 18\] Martha Pollack.
Overloading intentions for efficient practical reasoning.
Noas, 25:513-536, 1991.\[19\] Ellen Prince.
Toward a taxonomy of given-new information.
In P. Cole, editor, Radical Pragmatics.
AcademicPress, 1981.\[20\] Owen Rambow and Tanya Korelsky.
Applied text generation.
In ANLP, pages 40--47, 1992.\[21\] Ehud Reiter.
A new model of lexical choice for nouns.
Computationallntelligence, 7(4):240-251, 991.\[22\] Ehud Reiter and Robert Dale.
Building applied natural language generation systems.
Natural Language Engi-neering, 3:57-88, 1997.\[23\] Yves Schabes.
Mathematical nd Computational Aspects of Lexicalized Grammars.
PhD thesis, ComputerScience Department, University of Pennsylvania, 1990.\[24\] Matthew Stone.
Applying theories of communicative action in natural language generation using logic program-ming.
In AAAI Fall Symposium onCommunicative Action, 1997.
~\[25\] Matthew Stone.
Modality in Dialogue: Planning, Pragmatics and Computation.
PhD thesis, University ofPennsylvania, 1998.\[26\] Matthew Stone and Christine Doran.
Paying heed to collocations.
In InternationalNaturalLanguage GenerationWorkshop, ages 91-100, 1996.\[27\] Matthew Stone and Christine Doran.
Sentence planning as description using tree-adjoining grammar.
InProceedings of ACL, pages 198-205, 1997.\[28\] Lyn Walker.
Informational redundancy and resource bounds in dialogue.
PhD thesis, Department ofComputer& Information Science, University of Pennsylvania, 1993.
Institute for Research in Cognitive Science reportIRCS-93-45.\[29\] Gijoo Yang, Kathleen E McCoy, and K. Vijay-Shanker.
From functional specification to syntactic structures:systemic grammar and tree-adjoining grammar.
Computational Intelligence, 7(4):207-219, 1991.187
