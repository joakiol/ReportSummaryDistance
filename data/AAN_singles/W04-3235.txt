Error Measures and Bayes Decision Rules Revisitedwith Applications to POS TaggingHermann Ney, Maja Popovic?, David Su?ndermannLehrstuhl fu?r Informatik VI - Computer Science DepartmentRWTH Aachen UniversityAhornstrasse 5552056 Aachen, Germany{popovic,ney}@informatik.rwth-aachen.deAbstractStarting from first principles, we re-visit thestatistical approach and study two forms ofthe Bayes decision rule: the common rule forminimizing the number of string errors and anovel rule for minimizing the number of symbolserrors.
The Bayes decision rule for minimizingthe number of string errors is widely used, e.g.in speech recognition, POS tagging and machinetranslation, but its justification is rarely questioned.To minimize the number of symbol errors as ismore suitable for a task like POS tagging, we showthat another form of the Bayes decision rule canbe derived.
The major purpose of this paper is toshow that the form of the Bayes decision rule shouldnot be taken for granted (as it is done in virtuallyall statistical NLP work), but should be adaptedto the error measure being used.
We present firstexperimental results for POS tagging tasks.1 IntroductionMeanwhile, the statistical approach to naturallanguage processing (NLP) tasks like speechrecognition, POS tagging and machine translationhas found widespread use.
There are threeingredients to any statistical approach to NLP,namely the Bayes decision rule, the probabilitymodels (like trigram model, HMM, ...) and thetraining criterion (like maximum likelihood, mutualinformation, ...).The topic of this paper is to re-consider the formof the Bayes decision rule.
In virtually all NLPtasks, the specific form of the Bayes decision ruleis never questioned, and the decision rule is adaptedfrom speech recognition.
In speech recognition, thetypical decision rule is to maximize the sentenceprobability over all possible sentences.
However,this decision rule is optimal for the sentence errorrate and not for the word error rate.
This differenceis rarely studied in the literature.As a specific NLP task, we will consider part-of-speech (POS) tagging.
However, the problemaddressed comes up in any NLP task which istackled by the statistical approach and which makesuse of a Bayes decision rule.
Other prominentexamples are speech recognition and machinetranslation.
The advantage of the POS taggingtask is that it will be easier to handle from themathematical point of view and will result in closed-form solutions for the decision rules.
From thispoint-of-view, the POS tagging task serves as agood opportunity to illustrate the key concepts ofthe statistical approach to NLP.Related Work: For the task of POS tagging,statistical approaches were proposed already in the60?s and 70?s (Stolz et al, 1965; Bahl and Mercer,1976), before they started to find widespread usein the 80?s (Beale, 1985; DeRose, 1989; Church,1989).To the best of our knowledge, the ?standard?version of the Bayes decision rule, which minimizesthe number of string errors, is used in virtually allapproaches to POS tagging and other NLP tasks.There are only two research groups that do not takethis type of decision rule for granted:(Merialdo, 1994): In the context of POS tagging,the author introduces a method that he callsmaximum likelihood tagging.
The spirit of thismethod is similar to that of this work.
However, thismethod is mentioned as an aside and its implicationsfor the Bayes decision rule and the statisticalapproach are not addressed.
Part of this workgoes back to (Bahl et al, 1974) who considereda problem in coding theory.
(Goel and Byrne, 2003): The error measureconsidered by the authors is the word error rate inspeech recognition, i.e.
the edit distance.
Due tothe mathematical complexity of this error measure,the authors resort to numeric approximationsto compute the Bayes risk (see next section).Since this approach does not results in explicitclosed-form equations and involves many numericapproximations, it is not easy to draw conclusionsfrom this work.2 Bayes Decision Rule for Minimum ErrorRate2.1 The Bayes Posterior RiskKnowing that any task in NLP tasks is a difficultone, we want to keep the number of wrongdecisions as small as possible.
This point-of-viewhas been used already for more than 40 years inpattern classification as the starting point for manytechniques in pattern classification.
To classify anobservation vector y into one out of several classesc, we resort to the so-called statistical decisiontheory and try to minimize the average risk or lossin taking a decision.
The result is known as Bayesdecision rule (Chapter 2 in (Duda and Hart, 1973)):y ?
c?
= argminc{?c?Pr(c|y) ?
L[c, c?
]}where L[c, c?]
is the so-called loss function or errormeasure, i.e.
the loss we incur in making decision cwhen the true class is c?.In the following, we will consider two specificforms of the loss function or error measure L[c, c?
].The first will be the measure for string errors,which is the typical loss function used in virtuallyall statistical approaches.
The second is themeasure for symbol errors, which is the moreappropriate measure for POS tagging and alsospeech recognition with no insertion and deletionerrors (such as isolated word recognition).2.2 String ErrorFor POS tagging, the starting point is the observedsequence of words y = wN1 = w1...wN , i.e.
thesequence of words for which the POS tag sequencehas c = gN1 = g1...gN has to be determined.The first error measure we consider is the stringerror: the error is equal to zero only if the POSsymbols of the two strings are identical at eachposition.
In this case, the loss function is:L[gN1 , g?N1 ] = 1 ?N?n=1?
(gn, g?n)with the Kronecker delta ?
(c, c?).
In other words,the errors are counted at the string level and notat the level of single symbols.
Inserting this costfunction into the Bayes risk (see Section 2.1), weimmediately obtain the following form of Bayesdecision rule for minimum string error:wN1 ?
g?N1 = argmaxgN1{Pr(gN1 |wN1 )}= argmaxgN1{Pr(gN1 , wN1 )}This is the starting point for virtually all statisticalapproaches in NLP like speech recognition andmachine translation.
However, this decision rule isonly optimal when we consider string errors, e.g.sentence error rate in POS tagging and in speechrecognition.
In practice, however, the empiricalerrors are counted at the symbol level.
Apartfrom (Goel and Byrne, 2003), this inconsistency ofdecision rule and error measure is never addressedin the literature.2.3 Symbol ErrorInstead of the string error rate, we can also considerthe error rate of single POS tag symbols (Bahl etal., 1974; Merialdo, 1994).This error measure is defined by the loss function:L[gN1 , g?N1 ] =N?n=1[1 ?
?
(gn, g?n)]This loss function has to be inserted into the Bayesdecision rule in Section 2.1.
The computation of theexpected loss, i.e.
the averaging over all classes c?
=g?N1 , can be performed in a closed form.
We omitthe details of the straightforward calculations andstate only the result.
It turns out that we will needthe marginal (and posterior) probability distributionPrm(g|wN1 ) at positions m = 1, ..., N :Prm(g|wN1 ) :=?gN1 : gm=gPr(gN1 |wN1 )where the sum is carried out over all POS tag stringsgN1 with gm = g, i.e.
the tag gm at position m isfixed at gm = g. The question of how to performthis summation efficiently will be considered laterafter we have introduced the model distributions.Thus we have obtained the Bayes decision rulefor minimum symbol error at position m = 1, ..., N :(wN1 ,m) ?
g?m = argmaxg{Prm(g|wN1 )}= argmaxg{Prm(g,wN1 )}By construction this decision rule has the specialproperty that it does not put direct emphasis onlocal coherency of the POS tags produced.
In otherwords, this decision rule may produce a POS tagstring which is linguistically less likely.3 The Modelling Approaches to POSTaggingThe derivation of the Bayes decision rule assumesthat the probability distribution Pr(gN1 , wN1 ) (orPr(gN1 |wN1 )) is known.
Unfortunately, this is notthe case in practice.
Therefore, the usual approachis to approximate the true but unknown distributionby a model distribution p(gN1 , wN1 ) (or p(gN1 |wN1 )).We will review two popular modelling approaches,namely the generative model and the direct model,and consider the associated Bayes decision rules forboth minimum string error and minimum symbolerror.3.1 Generative Model: Trigram ModelWe replace the true but unknown joint distributionPr(gN1 , wN1 ) by a model-based probability distribu-tion p(gN1 , wN1 ):Pr(gN1 , wN1 ) ?
p(gN1 , wN1 ) = p(gN1 ) ?
p(wN1 |gN1 )We apply the so-called chain rule to factorize eachof the distributions p(gN1 ) and p(wN1 |gN1 ) into aproduct of conditional probabilities using specificdependence assumptions:p(gN1 , wN1 ) =N?n=1[p(gn|gn?1n?2) ?
p(wn|gn)]with suitable definitions for the case n = 1.Here, the specific dependence assumptions are thatthe conditional probabilities can be representedby a POS trigram model p(gn|gn?1n?2) and a wordmembership model p(wn|gn).
Thus we obtaina probability model whose structure fits intothe mathematical framework of so-called HiddenMarkov Model (HMM).
Therefore, this approach isoften also referred to as HMM-based POS tagging.However, this terminology is misleading: The POStag sequence is observable whereas in the HiddenMarkov Model the state sequence is always hiddenand cannot be observed.
In the experiments, we willuse a 7-gram POS model.
It is clear how to extendthe equations from the trigram case to the 7-gramcase.3.1.1 String ErrorUsing the above model distribution, we directlyobtain the decision rule for minimum string error:wN1 ?
g?N1 = argmaxgN1{p(gN1 , wN1 )}Since the model distribution is a basically a second-order model (or trigram model), there is an efficientalgorithm for finding the most probable POS tagstring.
This is achieved by a suitable dynamicprogramming algorithm, which is often referred toas Viterbi algorithm in the literature.3.1.2 Symbol ErrorTo apply the Bayes decision rule for minimumsymbol error rate, we first compute the marginalprobability pm(g,wN1 ):pm(g,wN1 ) =?gN1 : gm=gp(gN1 , wN1 )=?gN1 : gm=g?n[p(gn|gn?1n?2) ?
p(wn|gn)]Again, since the model is a second-order model,the sum over all possible POS tag strings gN1(with gm = g) can be computed efficientlyusing a suitable extension of the forward-backwardalgorithm (Bahl et al, 1974).Thus we obtain the decision rule for minimumsymbol error at positions m = 1, ..., N :(wN1 ,m) ?
g?m = argmaxg{pm(g,wN1 )}Here, after the the marginal probability pm(g,wN1 )has been computed, the task of finding the mostprobable POS tag at position m is computationallyeasy.
Instead, the lion?s share for the computationaleffort is required to compute the marginal probabil-ity pm(g,wN1 ).3.2 Direct Model: Maximum EntropyWe replace the true but unknown posterior distri-bution Pr(gN1 |wN1 ) by a model-based probabilitydistribution p(gN1 |wN1 ):Pr(gN1 |wN1 ) ?
p(gN1 |wN1 )and apply the chain rule:p(gN1 |wN1 ) =N?n=1p(gn|gn?11 , wN1 )=N?n=1p(gn|gn?1n?2 , wn+2n?2)As for the generative model, we have made specificassumptions: There is a second-order dependencefor the tags gn1 , and the dependence on the wordswN1 is limited to a window wn+2n?2 around positionn.
The resulting model is still rather complexand requires further specifications.
The typicalprocedure is to resort to log-linear modelling, whichis also referred to as maximum entropy modelling(Ratnaparkhi, 1996; Berger et al, 1996).3.2.1 String ErrorFor the minimum string error, we obtain thedecision rule:wN1 ?
g?N1 = argmaxgN1{p(gN1 |wN1 )}Since this is still a second-order model, we can usedynamic programming to compute the most likelyPOS string.3.2.2 Symbol ErrorFor the minimum symbol error, the marginal(and posterior) probability pm(g|wN1 ) has to becomputed:pm(g|wN1 ) =?gN1 : gm=gPr(gN1 |wN1 )=?gN1 : gm=g?np(gn|gn?1n?2 , wn+2n?2)which, due to the specific structure of the modelp(gn|gn?1n?2 , wn+2n?2), can be calculated efficientlyusing only a forward algorithm (without a?backward?
part).Thus we obtain the decision rule for minimumsymbol error at positions m = 1, ..., N :(wN1 ,m) ?
g?m = argmaxg{pm(g|wN1 )}As in the case of the generative model, thecomputational effort is to compute the posteriorprobability pm(g|wN1 ) rather than to find the mostprobable tag at position m.4 The Training ProcedureSo far, we have said nothing about how we trainthe free parameters of the model distributions.
Weuse fairly conventional training procedures that wemention only for the sake of completeness.4.1 Generative ModelWe consider the trigram-based model.
The freeparameters here are the entries of the POS trigramdistribution p(g|g?
?, g?)
and of the word membershipdistribution p(w|g).
These unknown parameters arecomputed from a labelled training corpus, i.e.
acollection of sentences where for each word theassociated POS tag is given.In principle, the free parameters of the modelsare estimated as relative frequencies.
For the testdata, we have to allow for both POS trigrams (or n-grams) and (single) words that were not seen in thetraining data.
This problem is tackled by applyingsmoothing methods that were originally designedfor language modelling in speech recognition (Neyet al, 1997).4.2 Direct ModelFor the maximum entropy model, the free param-eters are the so-called ?i or feature parameters(Berger et al, 1996; Ratnaparkhi, 1996).
Thetraining criterion is to optimize the logarithmof the model probabilities p(gn|gn?2n?1 , wn+2n?2) overall positions n in the training corpus.
Thecorresponding algorithm is referred to as GISalgorithm (Berger et al, 1996).
As usualwith maximum entropy models, the problem ofsmoothing does not seem to be critical and is notaddressed explicitly.5 Experimental ResultsOf course, there have already been many papersabout POS tagging using statistical methods.
Thegoal of the experiments is to compare the twodecision rules and to analyze the differences inperformance.
As the results for the WSJ corpus willshow, both the trigram method and the maximumentropy method have an tagging error rate of 3.0%to 3.5% and are thus comparable to the best resultsreported in the literature, e.g.
(Ratnaparkhi, 1996).5.1 Task and CorpusThe experiments are performed on the Wall StreetJournal (WSJ) English corpus and on the Mu?nsterTagging Project (MTP) German corpus.The POS tagging part of The WSJ corpus(Table 1) was compiled by the University ofPennsylvania and consists of about one millionEnglish words with manually annotated POS tags.Text POSTrain Sentences 43508Words+PMs 1061772Singletons 21522 0Word Vocabulary 46806 45PM Vocabulary 25 9Test Sentences 4478Words+PMs 111220OOVs 2879 0Table 1: WSJ corpus statistics.The MTP corpus (Table 2) was compiled at theUniversity of Mu?nster and contains tagged Germanwords from articles of the newspapers Die Zeitand Frankfurter Allgemeine Zeitung (Kinscher andSteiner, 1995).For the corpus statistics, it is helpful todistinguish between the true words and thepunctuation marks (see Table 1 and Table 2).
Thisdistinction is made for both the text and the POScorpus.
In addition, the tables show the vocabularysize (number of different tokens) for the words andfor the punctuation marks.Punctuation marks (PMs) are all tokens whichdo not contain letters or digits.
The total numberof running tokens is indicated as Words+PMs.Singletons are the tokens which occur only once inText POSTrain Sentences 19845Words+PMs 349699Singletons 32678 11Word Vocabulary 51491 68PM Vocabulary 27 5Test Sentences 2206Words+PMs 39052OOVs 3584 2Table 2: MTP corpus statistics.the training data.
Out-of-Vocabulary words (OOVs)are the words in the test data that did not not occurin the training corpus.5.2 POS Tagging ResultsThe tagging experiments were performed for bothtypes of models, each of them with both types ofthe decision rules.
The generative model is based onthe approach described in (Su?ndermann and Ney,2003).
Here the optimal value of the n-gram orderis determined from the corpus statistics and has amaximum of n = 7.
The experiments for the directmodel were performed using the maximum entropytagger described in (Ratnaparkhi, 1996).The tagging error rates are showed in Table 3 andTable 4.
In addition to the overall tagging error rate(Overall), the tables show the tagging error rates forthe Out-of-Vocabulary words (OOVs) and for thepunctuation marks (PMs).For the generative model, both decision rulesyield similar results.
For the direct model, theoverall tagging error rate increases on each of thetwo tasks (from 3.0 % to 3.3 % on WSJ and from5.4 % to 5.6 % on MTP) when we use the symboldecision rule instead of the string decision rule.
Inparticular, for OOVs, the error rate goes up clearly.Right now, we do not have a clear explanationfor this difference between the generative modeland the direct model.
It might be related to the?forward?
structure of the direct model as opposed tothe ?forward-backward?
structure of the generativemodel.
Anyway, the refined bootstrap method(Bisani and Ney, 2004) has shown that differencesin the overall tagging error rate are statistically notsignificant.5.3 ExamplesA detailed analysis of the tagging results showedthat for both models there are sentences where theone decision rule is more efficient and sentenceswhere the other decision rule is better.For the generative model, these differences seemto occur at random, but for the direct model, somedistinct tendencies can be observed.
For example,WSJ Task Decision Overall OOVs PMsRuleGenerative string 3.5 16.9 0Model symbol 3.5 16.7 0Direct string 3.0 15.4 0.08Model symbol 3.3 16.6 0.1Table 3: POS tagging error rates [%] for WSJ task.MTP Task Decision Overall OOVs PMsRuleGenerative string 5.4 13.4 3.6Model symbol 5.4 13.4 3.6Direct string 5.4 12.7 3.8Model symbol 5.6 13.4 3.7Table 4: POS tagging error rates [%] for MTP task.for the WSJ corpus, the string decision rule issignificantly better for the present and past tense ofverbs (VBP, VBN), and the symbol decision ruleis better for adverb (RB) and verb past participle(VBN).
Typical errors generated by the symboldecision rule are tagging present tense as infinitive(VB) and past tense as past participle (VBN), andfor string decision rule, adverbs are often tagged aspreposition (IN) or adjective (JJ) and past participleas past tense (VBD).For the German corpus, the string decisionrule better handles demonstrative determiners(Rr) and subordinate conjunctions (Cs) whereassymbol decision rule is better for definite articles(Db).
The symbol decision rule typically tagsthe demonstrative determiner as definite article(Db) and subordinate conjunctions as interrogativeadverbs (Bi), and the string decision rule tends toassign the demonstrative determiner tag to definitearticles.These typical errors for the symbol decision ruleare shown in Table 5, and for the string decision rulein Table 6.6 ConclusionSo far, the experimental tests have shown noimprovement when we use the Bayes decision rulefor minimizing the number of symbol errors ratherthan the number of string errors.
However, theimportant result is that the new approach results incomparable performance.
More work is needed tocontrast the two approaches.The main purpose of this paper has been to showthat, in addition to the widely used decision rule forminimizing the string errors, it is possible to derive adecision rule for minimizing the number of symbolerrors and to build up the associated mathematicalframework.There are a number of open questions for futurework:1) The error rates for the two decision rules arecomparable.
Is that an experimental coincidence?Are there situations for which we must expect asignificance difference between the two decisionrules?
We speculate that the two decision rulescould always have similar performance if the errorrates are small.2) Ideally, the training criterion should be closelyrelated to the error measure used in the decisionrule.
Right now, we have used the training criteriathat had been developed in the past and that hadbeen (more or less) designed for the string error rateas error measure.
Can we come up with a trainingcriterion tailored to the symbol error rate?3) In speech recognition and machine translation,more complicated error measures such as the editdistance and the BLEU measure are used.
Is itpossible to derive closed-form Bayes decision rules(or suitable analytic approximations) for these errormeasures?
What are the implications?ReferencesL.
Bahl, J. Cocke, F. Jelinek and J. Raviv.1974.
Optimal Decoding of Linear Codes forMinimizing Symbol Error Rate.
IEEE Trans.
onInformation Theory, No.
20, pages 284?287L.
Bahl and L. R. Mercer.
1976.
Part of SpeechAssignment by a Statistical Decision Algorithm.In IEEE Symposium on Information Theory,abstract, pages 88?89, Ronneby, Sweden.A.
D. Beale.
1985.
A Probabilistic Approachto Grammatical Analysis of Written English byComputer.
In 2nd Conf.
of the European Chapterof the ACL, pages 159?169, Geneva, Switzerland.A.
L. Berger, S. Della Pietra and V. Della Pietra.1996.
A Maximum Entropy Approach toNatural Language Processing.
ComputationalLinguistics, No.
22, Vol.
1, pages 39?71.M.
Bisani and H. Ney.
2004.
Bootstrap Estimatesfor Confidence Intervals in ASR PerformanceEvaluation.
In IEEE Int.
Conf.
on Acoustics,Speech and Signal Processing, pages 409?412,Montreal, Canada.K.
W. Church.
1989.
A Stochastic Parts ProgramNoun Phrase Parser for Unrestricted Text.
InIEEE Int.
Conf.
on Acoustics, Speech and SignalProcessing, pages 695?698, Glasgow, Scotland.S.
DeRose.
1989.
Grammatical Category Disam-biguation by Statistical Optimization.
Computa-tional Linguistics, No.
14, Vol.
1, pages 31?39R.
O. Duda and P. E. Hart.
1973.
PatternClassification and Scene Analysis.
John Wiley &Sons, New York.V.
Goel and W. Byrne.
2003.
Minimum Bayes-risk Automatic Speech Recognition.
In W. Chouand B. H. Juang (editors): Pattern Recognitionin Speech and Language Processing.
CRC Press,Boca Rota, Florida.J.
Kinscher and P. Steiner.
1995.
Mu?nster TaggingProject (MTP).
Handout for the 4th NorthernGerman Linguistic Colloquium, University ofMu?nster, Internal report.B.
Merialdo.
1994.
Tagging English Text with aProbabilistic Model.
Computational Linguistics,No.
20, Vol.
2, pages 155?168.H.
Ney, S. Martin and F. Wessel.
1997.Statistical Language Modelling by Leaving-One-Out.
In G. Bloothooft and S. Young (editors):Corpus-Based Methods in Speech and Language,pages 174?207.
Kluwer Academic Publishers,Dordrecht.A.
Ratnaparkhi.
1996.
A Maximum EntropyModel for Part-of-Speech Tagging.
In Conf.on Empirical Methods in Natural LanguageProcessing and Very Large Corpora , pages 133?142, Sommerset, NJ.W.
S. Stolz, P. H. Tannenbaum and F. V. Carstensen.1965.
Stochastic Approach to the GrammaticalCoding of English.
Communications of the ACM,No.
8, pages 399?405.D.
Su?ndermann and H. Ney.
2003.
SYNTHER- a New m-gram POS Tagger.
In Proc.
ofthe Int.
Conf.
on Natural Language Processingand Knowledge Engineering, pages 628?633,Beijing, China.VBP ?
VBreference ... investors/NNS already/RB have/VBP sharply/RB scaled/VBN ...string ... investors/NNS already/RB have/VBP sharply/RB scaled/VBN ...symbol ... investors/NNS already/RB have/VB sharply/RB scaled/VBN ...reference We/PRP basically/RB think/VBP that/IN ...string We/PRP basically/RB think/VBP that/IN ...symbol We/PRP basically/RB think/VB that/IN ...VBD ?
VBNreference ... plant-expansion/JJ program/NN started/VBD this/DT year/NN ...string ... plant-expansion/NN program/NN started/VBD this/DT year/NN ...symbol ... plant-expansion/NN program/NN started/VBN this/DT year/NN ...reference ... countries/NNS have/VBP in/IN recent/JJ years/NNS made/VBD agreements/NNS ...string ... countries/NNS have/VBP in/IN recent/JJ years/NNS made/VBD agreements/NNS ...symbol ... countries/NNS have/VBP in/IN recent/JJ years/NNS made/VBN agreements/NNS ...Rr ?
Dbreference Das/Db Sandma?nnchen/Ne ,/Fi das/Rr uns/Rp der/Db NDR/Ab pra?sentiert/Vf ...string Das/Db Sandma?nnchen/Ng ,/Fi das/Rr uns/Rp der/Db NDR/Ab pra?sentiert/Vf ...symbol Das/Db Sandma?nnchen/Ng ,/Fi das/Db uns/Rp der/Db NDR/Ab pra?sentiert/Vf ...reference ... fu?r/Po Leute/Ng ,/Fi die/Rr glauben/Vf ...string ... fu?r/Po Leute/Ng ,/Fi die/Rr glauben/Vf ...symbol ... fu?r/Po Leute/Ng ,/Fi die/Db glauben/Vf ...Cs ?
Bireference Denke/Vf ich/Rp nach/Qv ,/Fi warum/Cs mir/Rp die/Db Geschichte/Ng gefa?llt/Vf ...string Denke/Vf ich/Rp nach/Qv ,/Fi warum/Cs mir/Rp die/Db Geschichte/Ng gefa?llt/Vf ...symbol Denke/Vf ich/Rp nach/Qv ,/Fi warum/Bi mir/Rp die/Db Geschichte/Ng gefa?llt/Vf ...Table 5: Examples of tagging errors for the symbol decision rule (direct model)RB ?
IN, JJreference The/DT negotiations/NNS allocate/VBP about/RB 15/CD %/NN ...string The/DT negotiations/NNS allocate/VBP about/IN 15/CD %/NN ...symbol The/DT negotiations/NNS allocate/VBP about/RB 15/CD %/NN ...reference ... will/MD lead/VB to/TO a/DT much/RB stronger/JJR performance/NN ...string ... will/MD lead/VB to/TO a/DT much/JJ stronger/JJR performance/NN ...symbol ... will/MD lead/VB to/TO a/DT much/RB stronger/JJR performance/NN ...VBN ?
VBDreference ... by/IN a/DT police/NN officer/NN named/VBN John/NNP Klute/NNP ...string ... by/IN a/DT police/NN officer/NN named/VBD John/NNP Klute/NNP ...symbol ... by/IN a/DT police/NN officer/NN named/VBN John/NNP Klute/NNP ...Db ?
Rrreference er/Rp kam/Vf auf/Po die/Db Idee/Ng ,/Fi die/Db Emotionen/Ng zu/Qi kanalisieren/Vi ...string er/Rp kam/Vf auf/Po die/Db Idee/Ng ,/Fi die/Rr Emotionen/Ng zu/Qi kanalisieren/Vi ...symbol er/Rp kam/Vf auf/Po die/Db Idee/Ng ,/Fi die/Db Emotionen/Ng zu/Qi kanalisieren/Vi ...Table 6: Examples of tagging errors for the string decision rule (direct model)
