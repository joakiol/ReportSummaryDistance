Finite-state Multimodal Parsing and UnderstandingMichael JohnstonAT&T Labs - ResearchShannon Laboratory, 180 Park AveFIorham Park, NJ 07932, USAj ohnston@research ,  a t t .
tomSrinivas BangaloreAT&T Labs - ResearchShannon Laboratory, 180 Park AveFlorham Park, NJ 07932, USAs r in i@research ,  a r t .
tomAbstractMultimodal interfaces require effective parsing andnn(lerstanding of utterances whose content is dis-tributed across multiple input modes.
Johnston 1998presents an approach in which strategies lbr mul-timodal integration are stated declaratively using aunification-based grammar that is used by a mnlti-dilnensional chart parser to compose inputs.
Thisapproach is highly expressive and supports a broadclass of interfaces, but offers only limited potentialfor lnutual compensation among the input modes, issubject o signilicant concerns in terms o1' COml)uta-tional complexity, and complicates selection amongalternative multimodal interpretations of the input.In tiffs papeh we l)resent an alternative approaclain which multimodal lmrsing and understanding areachieved using a weighted finite-state device whichtakes speech and gesture streams as inputs and out-puts their joint interpretation.
This approach is sig-nificantly more efficienl, enables tight-coupling ofmultimodal understanding with speech recognition,and provides a general probabilistic fralnework formultimodal ambiguity resolution.1 IntroductionMultimodal interfaces are systems that allow inputand/or output o be conveyed over multiple differentchannels uch as speech, graphics, and gesture.
Theyenable more natural and effective interaction sincedifferent kinds of content can be conveyed in themodes to which they are best suited (Oviatt, 1997).Our specific concern here is with multimodal inter-faces supporting input by speech, pen, and touch, butthe approach we describe has far broader applicabil-ity.
These interfaces stand to play a critical role in theongoing migration of interaction fi'oln the desktopto wireless portable computing devices (PI)As, next-generation phones) that offer limited screen real es-tale, and other keyboard-less platforms uch as pub-lic information kiosks.To realize their full potential, multimodal inter-faces need to support not just input from multiplemodes, but synergistic multimodal utterances opti-mally distributed over the available modes (John-ston et al, 1997).
In order to achieve this, an e ffcctive method for integration of content fi'Oln dillferent modes is needed.
Johnston (1998b) showshow techniques from natural language processing(unification-based gramumrs and chart parsing) canbe adapted to support parsing and interpretation ofutterances distributed over multiple modes.
In thatapproach, speech and gesture recognition produce ~,-best lists of recognition results which are assignedtyped feature structure representations (Carpenter,1992) and passed to a luultidimensioual chart parsel ?that uses a lnultimodal unification-based granunar tocombine the representations assigned to the input el-ements.
Possible multimodal interpretations are thenranked and the optimal interpretation is passed onfor execution.
This approach overcomes many ofthe limitations of previous approaches tomultimodalintegration such as (Bolt, 1980; Neal and Shapiro,1991) (See (Johnston ct al., 1997)(1).
282)).
It sup-ports speech with multiple gestures, visual parsingof unimodal gestures, and its dechu'ative nature fa-cilitates rapid l)rototyping and iterative develol)meutof multimodal systems.
Also, the unification-basedapproach allows for mutual COlnpensatiou of recog-nition errors in the individual modalities (Oviatt,1999).However, the unification-based approach does notallow for tight-conpling of nmltimodal parsing withspeech and gesture recognition.
Compensation elLfects are dependent on the correct answer appear-ing in the ~;,-best list of interpretations a signed toeach mode.
Multimodal parsing cannot directly in-fluence the progress of speech or gesture recognition.The multidimensional parsing approach is also sub-ject to significant concerns in terms of computationalcomplexity.
In the worst case, the multidimensionalparsing algorithm (Johnston, 1998b) (p. 626) is ex-ponential with respect o the number of input ele-ments.
Also this approach does not provide a nat-ural fiamework for combining the probabilities ofspeech and gesture vents in order to select amongmultiple competing multimodal interpretations.
Wuet.al.
(1999) present a statistical approach for select-ing among multiple possible combinations of speech369and gesture.
However; it is not clear how the ap-proach will scale to more complex verbal languageand combinations of speech with multiple gestures.In this papm, we propose an alternative approachthat addresses these limitations: parsing, understand-ing, and integration of speech and gesture am pe>formed by a single finite-state device.
With certainsimplifying assumptions, multidimensional parsingand understanding with multimodal grammars canbe achieved using a weighted finite-state automa-ton (FSA) running on throe tapes which representspeech input (words), gesture input (gesture sym-bols and reference markers), and their combined in-terpretation.
We have implemented our approach inthe context of a multimodal messaging applicationin which users interact with a company directo Wusing synergistic ombinations of speech and peninput; a multimodal variant of VPQ (Buntschuh etal., 1998).
For example, the user might say emai lthis person  and this person and gesturewith the pen on pictures of two people on a user inter-face display.
In addition to the user interface client,the architecture contains peech and gesture recog-nition components which process incoming streamsof speech and electronic ink, and a multimodal lan-guage processing component (Figure 1 ).u, \[ASR ~ I ~esture Recognizer \[Multimodal Parser/Understander \]BackendFigure 1: Multimodal alvhitectureSection 2 provides background on finite-state lan-guage processing.
In Section 3, we define and exem-plify multimodal context-fiee grammars (MCFGS)and their approximation as multimodal FSAs.
Wedescribe our approach to finite-state representationof meaning and explain how the three-tape finitestate automaton can be factored out into a numberof finite-state transducers.
In Section 4, we explainhow these transducers can be used to enable tight-coupling of multimodal language processing withspeech and gesture recognition.2 Finite-state Language ProcessingFinite-state transducers (FST) are finite-state au-tomata (FSA) where each transition consists of aninput and an output symbol.
The transition is tra-versed if its input symbol matches the current sym-bol in the input and generates the output symbol as-sociated with the transition.
In other words, an FSTcan be regarded as a 2-tape FSA with an input tapefrom which the input symbols are read and an outputtape where the output symbols are written.Finite-state machines have been extensively ap-plied to many aspects of language processing in-cluding, speech recognition (Pereira nd Riley, 1997;Riccardi et al, 1996), phonology (Kaplan and Kay,1994), morphology (Koskenniemi, 1984), chunk-ing (Abney, 1991; Joshi and Hopely, 1997; Ban-galore, 1997), parsing (Roche, 1999), and machinetranslation (Bangalore and Riccardi, 2000).Finite-state models are attractive n~echanisms forlanguage processing since they are (a) efficientlylearnable fiom data (b) generally effective for decod-ing and (c) associated with a calculus for composingmachines which allows for straightforward integra-tion of constraints fl'om various levels of languageprocessing.
Furdmrmore, software implementingthe finite-state calculus is available for research pur-poses (Mohri eta\[., 1998).
Another motivation forour choice of finite-state models is that they enabletight integration of language processing with speechand gesture recognition.3 Finite-state MultimodalGrammarsMultimodal integration involves merging semanticcontent fi'om multiple streams to build a joint inter-pretation for a inultimodal utterance.
We use a finite-state device to parse multiple input strealns and tocombine their content into a single semantic repre-sentation.
For an interface with n inodes, a finite-state device operating over n+ 1 tapes is needed.
Thefirst n tapes represent the input streams and r~ + \] isan output stream representing their composition.
Inthe case of speech and pen input there are three tapes,one for speech, one for pen gesture, and a third fortheir combined meaning.As an example, in the messaging applicationdescribed above, users issue spoken commandssuch as emai l  this person and thatorganization and gestm'e on the appropriateperson and organization on the screen.
The struc-ture and interpretation of multimodal colnlnands ofthis kind can be captured eclaratively in a multi-modal context-free grammar.
We present a fi'agmentcapable of handling such commands in Figure 2.370S .~ V NP g:c:\]) NP -+ I)ET NCONJ --4 and:E:, NP --+ I)ET N CONJ NPV -+ cmail:g:cmail(\[ DET --+ |his:g:cV -+ page:c:page(\[ I)ET --+ lhat:?
:cN --:.
person:Gp:person( ENTP, YN -4 organization:Go:org( ENTRYN --+ dcpartment:Gd:dept( ENTRYENTRY -> C:el :el c:g:)ENTRY -> c:e2:e2 c:g:)ENTRY -4 c:ea:ea g:e:)ENTP, Y --+ ...Figure 2: Multimodal grammar fragmentThe non-terminals in the multimodal grammar areatomic symbols.
The multimodal aspects el' thegrammar become apparent in the terlninals.
Eachterminal contains three components W:G:M corre-sponding to the n q- 1 tapes, where W is for the spo-ken language stream, G is the gesture stream, andM is the combined meaning.
The epsilon symbol isused to indicate when oue of these is empty in a giventerminal.
The symbols in W are woMs from thespeech stream.
The symbols in G are of two types.Symbols like Go indicate the presence of a particularkind of gesturc in the gesture stream, while those likeet are used as references to entities referred to by thegesture (See Section 3.1).
Simple deictic pointinggestures are assigned semantic types based on tl~e n-tities they are references to.
Gp represents a gesturaltel'erence to a person on the display, Go to an orga-nization, and Gd lo a department.
Compared witha feature-based multimodal gralnlnar, these typesconstitute a set of atomic categories which makeltle relewmt dislinclions for gesture vents prcdicl-lug speech events and vice versa.
For example, ifthe gesture is G,, then phrases like thLs  personaud him arc preferred speech events and vice versa.These categories also play a role in constraining thesemantic representation when the speech is under-specified with respect o semantic type (e.g.
emai lth i s  one).
These gesture symbols can be orga-nized into a type hierarchy reflecting the ontologyof the entities in the application domain.
For exam-pie, there might be a general type G with subtypesGo and Gp, where G v has subtypes G,,,,~ and Gpf formale and female.A multimodal CFG (MCFG) can be defined fopreally as quadruple < N, 7', P, S >.
N is the set ofnonterminals.
1 ~ is the set of productions of the formA -+ (~whereA E Nand,~, C (NUT)* .
S i sthe start symbol for the grammar.
7' is the set ot' ter-minals of the l'orm (W U e) : (G U e) : M* whereW is the vocabulary of speech, G is the vocabularyof gesture=GestureSymbols U EventSymbols;GcsturcSymbols ={G v, Go, Gpj', G~.., ...} anda finite collections of \],gventSymbols ={c,,c~,.
.
.
,  c,,}.
M is the vocabulary to lel)rcsent meaningand includes event symbols (Evenl:Symbol.s C M).In general a context-free grammar can be approx-imated by an FSA (Pereira and Wright 1997, Neder-her 1997).
The transition symbols of the approx-imated USA are the terminals of the context-fieegrammar and in the case of multimodal CFG as de-tined above, these terminals contain three compo-nents, W, G and M. The multimodal CFG fi'ag-merit in Figurc 2 translates into the FSA in Figure 3,a three-tape finite state device capable of composingtwo input streams into a single output semantic rep-resentation stream.Our approach makes certain simplil'ying assump-tions with respect o ternporal constraints.
In multi-gesture utterances the primary flmction of tempo-ral constraints i to force an order on the gestures.If you say move th i s  here  and make two .ges-tures, the first corresponds toth i  s and the second tohere.
Our multimodal grammars encode order butdo not impose explicit temporal constraints, ltow-ever, general temporal constraints between speechand the first gesture can be enforced belbrc the FSAis applied.3.1 Finite-state Meaning RepresentationA novel aspect of our approach is that in additionto capturing the structure of language with a finitestate device, we also capture meaning.
Tiffs is veryimportant in nmltimodal language processing wherethe central goal is to capture how the multiple modescontribute to the combined interpretation.
Ottr ba-sic approach is to write symbols onto the third tape,which when concatenated together yield the seman-tic representation l'or the multimodal utterance.
Itsuits out" purposes here to use a simple logical repre-sentation with predicates pred(....) and lists la, b,...l.Many other kinds of semantic representation couldbe generated.
In the fl'agment in Figure 2, the wordema?l contributes email(\[ to the semantics tape,and the list and predicate arc closed when the ruleS --+ V NP e:z:\]) applies.
The word personwrites person( on the semantics tape.A signiiicant problem we face in adding mean-ing into the finite-state framework is how to reprc-sent all of the different possible specific values thatcan be contributed by a gesture.
For deictic refer-ences a unique identitier is needed for each object inthe interface that the user can gesture on.
For ex-alnple, il' the interface shows lists of people, thereneeds to be a unique ideutilier for each person.
Aspart of the composition process this identifier needs371departmcnl:Gd:dept( cps:cl :elor,mnization:Go:or-( tnat:eps:eps ~ z } ~ ~ .
\ [  3 ~ eps:eZ:e2 ~_/ / ~ e~q'e~ ~.\ " \]--.~cps:el~s:)+:?,,+.and:eps:,Figure 3: Multimodal three-tape FSAto be copied from the gesture stream into the seman-tic representation.
In the unification-based approachto multimodal integration, this is achieved by fea-ture sharing (Johnston, 1998b).
In the finite-state ap-proach, we would need to incorporate all of the dif-ferent possible IDs into the FSA.
For a person withid objid345 you need an arc e:objid345:objid345to transfer that piece of information fiom the ges-ture tape to the lneaning tape.
All of the arcs fordifferent IDs would have to be repeated everywherein the network where this transfer of information isneeded.
Furthermore, these arcs would have to beupdated as the underlying database was changed orupdated.
Matters are even worse for more complexpen-based ata such as drawing lines and areas in aninteractive map application (Cohen et al, 1998).
Inthis case, the coordinate set from the gesture needsto be incorporated into the senmntic representation.It might not be practical to incorporate the vast nuln-bet of different possible coordinate sequences into anFSA.Our solution to this problem is to store thesespecific values associated with incoming gesturesin a finite set of buffers labeled el,e,),ea .
.
.
.
andin place of the specific content write in the nalneof the appropriate buffer on the gesture tape.
In-stead of having the specific values in the FSA, wehave the transitions E:C I :C \ ] ,  C :C2:C2 ,  s:e3:e:3.., ineach location where content needs to be transferredfrom the gesture tape to the meaning tape (See Fig-ure 3).
These are generated fi'om the ENTRY pro-ductions in the multilnodal CFG in Figure 2.
Thegesture interpretation module empties the buffersand starts back at el after each multimodal com-mand, and so we am limited to a finite set of ges-ture events in a single utterance.
Returning tothe example email this person and thatorganization, assume the user gestures on en-tities objid367 and objid893.
These will be storedin buffers el and e2.
Figure 4 shows the speech andgesture streams and the resulting combined meaning.The elements on the meaning tape are concate-nated and the buffer references are replaced to yieldS: email this person and that organizationG: Gp cl 'Go e2M: email(\[ person(ct) , org(c2) \])Figure 4: Messaging domain exampleemail(~)er.son(objid367), or.q(objidS93)\]).
Asmore recursive semantic phenomena such as pos-sessives and other complex noun phrases are addedto the grammar the resulting machines becomelarger.
However, the computational consequencesof this can be lessened by lazy ewfluation tech-niques (Mohri, 1997) and we believe that this finite-state approach to constructing semantic representa-tions is viable for a broad range of sophisticated lan-guage interface tasks.
We have implemented a size-able multimodal CFG for VPQ (See Section 1): 417rules and a lexicon of 2388 words.3.2 Multimodal Finite-state TransducersWhile a three-tape finite-state automaton is feasi-ble in principle (Rosenberg, 1964), currently avail-able tools for finite-state language processing (Mohriet al, 1998) only support finite-state transducers(FSTs) (two tapes).
Furthermore, speech recogniz-ers typically do not support ile use of a three-tapeFSA as a language model.
In order to implement ourapproach, we convert he three-tape FSA (Figme 3)into an FST, by decomposing the transition symbolsinto an input component (G x W) and output compo-nent M, thus resulting in a function, T:(G x W) --+M.
This corresponds to a transducer in which ges-ture symbols and words are on the :input ape and themeaning is on the output tape (Figure 6).
The do-main of this function T can be further curried to re-sult in a transducer that maps 7~:G --> W (Figure 7).This transducer captures the constraints that gestureplaces on the speech stream and we use it as a Jan-guage model for constraining the speech recognizerbased on the recognized gesture string.
In the foplowing section, we explain how "F and 7% are used inconjunction with the speech recognition engine andgesture recognizer and interpreter to parse and inter-372pret nmltimodal input.4 Applying Multimodal TransducersThere arc number of different ways in which multi-modal finite-state transducers can be integrated withspeech and gesture recognition.
The best approachto take depends on the properties of the lmrticularinterface to be supported.
The approach we outlinehere involves recognizing esture ilrst then using theobserved gestures to modify the language model forspeech recognition.
This is a good choice if thereis limited ambiguity in gesture recognition, for ex-an@e, if lhe m~jority of gestures are unambiguousdeictic pointing gestures.The first step is for the geslure recognition andinterpretation module to process incoming pen ges-tures and construct a linite state machine GeslltVecorresponding tothe range of gesture interpretations.Ill our example case (Figure 4) tile gesture input isunambiguous and the Gestttre linite state machinewill be as in Figure 5.
\]f the gestural input involvesgesture recognition or is otherwise ambiguous it isrepresented as a lattice indicating all of the possi-ble recognitions and interpretations o1' tile gesturestream.
This allows speech to compensate for ges-ture errors and mutual compensation.Figure 5: (;eslttre linite-smte machineThis Ge,s'lure linite state machine is then com-posed with the transducer "R, which represents therelationship between speech and gesture (Figure 7).The result of this composition is a transducer Gesl-Lang (Figure 8).
This transducer represents the re-lationship between this particular sl.ream of gesturesand all of the possible word sequences tlmt could co-occur with those oes" , rares.
In order to use this in-lbnnation to guide the speech recognizer, we lhcntake a proiection on the output ape (speech) of Gesl-Lang to yield a finite-state machine which is usedas a hmguage model for speech recognition (Fig-ure 9).
Using this model enables the gestural in-formation to directly influence the speech recog-nizer's search.
Speech recognition yields a latticeof possible word sequences.
In our example case ityMds the wol~.t sequence mail this personand that organization (Figure 10).
Wenow need to reintegrale the geslure inl'ormation thatwc removed in the prqjection step before recog-nition.
This is achieved by composing Gest-Lang (Figure 8) with the result lattice from speechrecognition (Figure 10), yielding transducer Gesl~&)eechFST (Figure 11).
This transducer containsthe information both from the speech stream andfrom the gesture stream.
The next step is to gen-erate the Colnbined meaning representation.
Toachieve this Gest&)eechFST (G : W) is convertedinto an FSM GestSpeechFSM by combining out-put and input on one tape (G x W) (Figure 12).GestSk)eeckFSM is then composed with T (Fig-ure 6), which relates speech and gesture to mean-ing, yielding file result transducer Result (Figure 13).The meaning is lead from the output tape yield-ing cm,dl(\[perso,,,(ca), m'O(e2)\]).
We have imple-mented lifts approach and applied it in a multimodalinterface to VPQ on a wireless PDA.
In prelilni-nary speech recognition experiments, our approachyielded an average o1' 23% relative sentence-level er-ror reduction on a corpus of 1000 utterances (John-ston and Bangalore, 2000).5 ConclusionWe have presented here a novel approach to muI-timodal hmguage processing in which spoken lan-guage and gesture are parsed and integrated by asingle weighted lhfite-state device.
This device pro-vides language models for speech and gesture recog-nition alld colllposes content from speech and gcs-lure into a single semantic representalion.
Our ap-proach is novel not just in addressing multimodalhmguage but also in the encoding of semantics aswell as syntax in a finile-state device.Compared to previous al~proaches (Johnston el al.,1997; Jolmston, 1998a; Wu et al, 1999) which com-pose elements from 'n.-best lists of recognition re-sults, our approach provides an unprecedenled po-tential for mutual compensation among the inputmodes.
It enables gestural input to dynamicallyalter the hmguage model used tbr speech recogni-lion.
Furthermore, our approach avoids the com-putational complexity of multidimensional multi-modal parsing and our system of weighted finite-stale transducers provides a well understood prob-abilistic framcwork for combining the probabilitydistributions associated with speech and gesture in-put and selecting among multiple competing nmlti-modal interpretations.
Since the finite-state approachis more lightweight in coml)utational needs, it canmore readily be deployed on a broader ange of plat-forms.In ongoing research, we are collecting a corpus ofmultimodal data ill order to forlnally evahmte the ef-fectiveness of our approach and to train weights for1he multimodal inile-state transducers.
While wehave concentrated here on understanding, in princi-ple the same device could be applied to multimodal373Gd_dcpartnlcnt:dept( c I_cps:e 1.
~ Go or,,anization:or,,( ells tnat:eps ~ z j - ~ ~ ~ a b__...______cz ps:cz, ~  op~y,:om~< ~._ :pg_*>~_>/  -_______/ -- - " 'W' : ' ,ells_and:, ~ --ells:l) .
?Figure 6: Transducer elating gesture and speech to meaning (7-':(G x W) - -  M)Gd:departmcnt e 1 :eps/f~'~/'~ Go:organization cps:that ~ z } "~ -~ }-........_.cz:cl)s~ .
. "
/~"MQI~S? '
- - J~- - J " JNN.
.
e3:ep*-"-"'""~"s _.,,-((43)('7') ~p,:~ma,, ~ ~:y>.
/  ~p,:a,,d " - - - - - - - - - -~ '~-..,..j......__eps:pags_.......ac-..__J - __Figure 7: Transducer elating gesture and speech (TE:G ---+ W)eps:elllail eps:lhal Gp:personFigure 8: GestLang Transducer(}o:lu'ganizalionu page ~ this - -Figure 9: Projection of Output tape of GestLang Transducer@ email " @  this .
@  person .~@ and .
@  that =@Figure 10: Result from speech recognizerFigure 11: GestureSpeechFSTFigure 12: GestureSpeech FSMorganization ~ @organization _-Q ~,,se,l,a,l:Olll,i,~,~.q) ot,s_,,,is:e,,, >@ o,)-p~rso'l:,'e~go"~> G ~,?,,s:el >q)EllS_all(l:,Figure 13: Result Transducereps:) ~, Q~~i,.
:) >(~) ep,:\]) >(~)374generation which we are currently investigating.
Weare also exploring teclmiques to extend compilationfi'om feature structures gralnnlars to FSTs (Johnson,19!
)8) to nmltimodal unification-based grammars.ReferencesSteven Abney.
1991.
Parsing by chunks.
In RobertBerwick, Steven Abney, and Carol Tenny, editors,Principle-based palwing.
Kluwer Academic Pub-lishers.Srinivas Bangalore and Giuseppe Riccardi.
2000.Stochastic lhfite-state models for spoken languagemachine translation.
In Proceedings o/" the Work-shop on Embedded Machine Translation Systems.Srinivas Bangalore.
1997.
ComplexiO, of Lexic.alDescriptions and its Relevance to Partial Pmw-ing.
Ph.l).
tlaesis, University of Pennsylwmia,t~hiladelphia, PA, August.Robert A. Bolt.
1980.
"put-thal-there":voicc andgesture at the graphics interface.
ComputerGraphics, 14(3):262-270.Bruce Buntschuh, C. Kamm, G. DiFabbrizio,A.
Abella, M. Mohri, S. Narayanan, I. Zel.ikovic,R.D.
Sharp, J. Wright, S. Marcus, J. Shaffer,R.
I)uncan, and J.G.
Wilpon.
1998.
Vpq: Aspoken language interface to large scale directoryinformation.
In Proceedin,q,s o/' ICSLI', Sydney,Australia.Robert Carpenter.
1992.
The logic qf OT)ed./~'alurestructures.
Cambridge University Press, England.Philip R. Cohen, M. Johnston, 1).
McGee, S. L.Oviatt, J. Pittman, I. Smith, L. Chen, andJ.
Clew.
1998.
Multimodal interaction for dis-tributed interactive simulation.
In M. Mayburyand W. Wahlster, editors, Readings itz Intelligenthttelfiwes.
Morgan Kaul'mann Publishers.Mark Jollnson.
1998.
Finite-state approximationof constraint-based grammars using left-cornergrammar transforms.
In Proceedings q/'COLING-ACL, pages 619-623, Montreal, Canada.Michael Johnston and Srinivas Bangalore.
2000.Tight-coupling of multimodal language process-ing with speech recognition.
Technical report,AT&T Labs - Reseamh.Michael Johnston, ER.
Cohen, D. McGee, S.L.
Ovi-att, J.A.
Pittman, and 1.
Smidl.
1997.
Unilication-based multimodal integration.
In Proceedings o/lhe 35th ACL, pages 281-288, Madrid, Spain.Michael Johnston.
1998a.
Mullimodal languageprocessing.
In Proceedings q/" ICSLP, Sydney,Australia.Michael Johnston.
1998b.
Unification-based multi-modal parsing.
In Proceedings of COLING-ACL,pages 624-630, Montreal, Canada.Aravind Joshi and Philip Hopely.
1997.
A parserfiom antiquity.
Natural Language Engilzeering,2(4).Ronald M. Kaplan and M. Kay.
1994.
Regular mod-els of phonological rule systems.
ComputationalLinguislics, 20(3):331-378.K.
K. Koskenniemi.
1984.
7ire-level morphology: ageneral computation model,for wordzform recog-nition and production.
Ph.D. thesis, University ofHe\[sinki.Mehryar Mohri, Fernando C. N. Pereira, andMichael Riley.
1998.
A rational design for aweighted .finite-state transducer librao,.
Num-ber 1436 in Lecture notes in computer science.Springm; Berlin ; New York.Mehryar Mohri.
1997.
Finite-state transducers inlanguage and speech processing.
(7Oml~utationalLinguistics, 23(2):269-312.J.
G. Neal and S. C. Shapiro.
1991.
Intelligent multi-media interface technology.
In J. W. Sulliwm andS.
W. Tylm, editors, Intelligent User lnter\['aces,pages 45-68.
ACM Press, Addison Wesley, NewYork.Sharon L. Oviatt.
1997.
Multimodal interactivemaps: l)esigning l'or human performance.
InHmmut-Computer Interaction, pages 93-129.Sharon L. Ovialt.
1999.
Mutual disambiguation ofrecognition errors in a inultimodal architecture.
InCltl '99, pages 576-583.
ACM Press, New York.Fernando C.N.
Pereira and Michael I).
Riley.
1997.Speech recognition by composition of weighted fi-nite automata.
In E. Roche and Schabes Y., ed-itors, Finite State Devices for Nalttral LanguageProcessitlg, pages 431-456.
MIT Press, Cam-bridge, Massachusetts.Giuseppe Riccardi, R. Pieraccini, and E. Bocchieri.1996.
Stochastic Automata for Language Model-ing.
Computer Speech and Language, 10(4):265-293.Emmanuel Roche.
1999.
Finite state transducers:parsing free and fl'ozen sentences.
In Andrfis Ko-rnai, editol, Extended Finite State Models el'Lan-guage.
Cambridge University Press.A.L .
Rosenberg.
1964.
On n-tape finite state accep-ters.
FOCS, pages 76-81.Lizhong Wu, Sharon L. Oviatt, and Philip R. Cohen.1999.
Multilnodal integration - a statistical view.IEEE Transactions on Multimedia, I (4):334-34 l,I)ecember.375
