Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 256?263,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsFrustratingly Easy Domain AdaptationHal Daume?
IIISchool of ComputingUniversity of UtahSalt Lake City, Utah 84112me@hal3.nameAbstractWe describe an approach to domain adapta-tion that is appropriate exactly in the casewhen one has enough ?target?
data to doslightly better than just using only ?source?data.
Our approach is incredibly simple,easy to implement as a preprocessing step(10 lines of Perl!)
and outperforms state-of-the-art approaches on a range of datasets.Moreover, it is trivially extended to a multi-domain adaptation problem, where one hasdata from a variety of different domains.1 IntroductionThe task of domain adaptation is to develop learn-ing algorithms that can be easily ported from onedomain to another?say, from newswire to biomed-ical documents.
This problem is particularly inter-esting in NLP because we are often in the situationthat we have a large collection of labeled data in one?source?
domain (say, newswire) but truly desire amodel that performs well in a second ?target?
do-main.
The approach we present in this paper is basedon the idea of transforming the domain adaptationlearning problem into a standard supervised learn-ing problem to which any standard algorithm maybe applied (eg., maxent, SVMs, etc.).
Our transfor-mation is incredibly simple: we augment the featurespace of both the source and target data and use theresult as input to a standard learning algorithm.There are roughly two varieties of the domainadaptation problem that have been addressed in theliterature: the fully supervised case and the semi-supervised case.
The fully supervised case mod-els the following scenario.
We have access to alarge, annotated corpus of data from a source do-main.
In addition, we spend a little money to anno-tate a small corpus in the target domain.
We want toleverage both annotated datasets to obtain a modelthat performs well on the target domain.
The semi-supervised case is similar, but instead of having asmall annotated target corpus, we have a large butunannotated target corpus.
In this paper, we focusexclusively on the fully supervised case.One particularly nice property of our approachis that it is incredibly easy to implement: the Ap-pendix provides a 10 line, 194 character Perl scriptfor performing the complete transformation (avail-able at http://hal3.name/easyadapt.pl.gz).
Inaddition to this simplicity, our algorithm performs aswell as (or, in some cases, better than) current stateof the art techniques.2 Problem Formalization and Prior WorkTo facilitate discussion, we first introduce some no-tation.
Denote by X the input space (typically eithera real vector or a binary vector), and by Y the outputspace.
We will write Ds to denote the distributionover source examples and Dt to denote the distri-bution over target examples.
We assume access toa samples Ds ?
Ds of source examples from thesource domain, and samples Dt ?
Dt of target ex-amples from the target domain.
We will assume thatDs is a collection of N examples and Dt is a col-lection of M examples (where, typically, N ?
M ).Our goal is to learn a function h : X ?
Y withlow expected loss with respect to the target domain.256For the purposes of discussion, we will suppose thatX = RF and that Y = {?1,+1}.
However, mostof the techniques described in this section (as wellas our own technique) are more general.There are several ?obvious?
ways to attack thedomain adaptation problem without developing newalgorithms.
Many of these are presented and evalu-ated by Daume?
III and Marcu (2006).The SRCONLY baseline ignores the target data andtrains a single model, only on the source data.The TGTONLY baseline trains a single model onlyon the target data.The ALL baseline simply trains a standard learningalgorithm on the union of the two datasets.A potential problem with the ALL baseline is thatif N ?
M , then Ds may ?wash out?
any affectDt might have.
We will discuss this problem inmore detail later, but one potential solution isto re-weight examples from Ds.
For instance,if N = 10?M , we may weight each examplefrom the source domain by 0.1.
The next base-line, WEIGHTED, is exactly this approach, withthe weight chosen by cross-validation.The PRED baseline is based on the idea of usingthe output of the source classifier as a feature inthe target classifier.
Specifically, we first train aSRCONLY model.
Then we run the SRCONLYmodel on the target data (training, developmentand test).
We use the predictions made bythe SRCONLY model as additional features andtrain a second model on the target data, aug-mented with this new feature.In the LININT baseline, we linearly interpolatethe predictions of the SRCONLY and the TG-TONLY models.
The interpolation parameter isadjusted based on target development data.These baselines are actually surprisingly difficultto beat.
To date, there are two models that havesuccessfully defeated them on a handful of datasets.The first model, which we shall refer to as the PRIORmodel, was first introduced by Chelba and Acero(2004).
The idea of this model is to use the SR-CONLY model as a prior on the weights for a sec-ond model, trained on the target data.
Chelba andAcero (2004) describe this approach within the con-text of a maximum entropy classifier, but the ideais more general.
In particular, for many learningalgorithms (maxent, SVMs, averaged perceptron,naive Bayes, etc.
), one regularizes the weight vec-tor toward zero.
In other words, all of these algo-rithms contain a regularization term on the weightsw of the form ?
||w||22.
In the generalized PRIORmodel, we simply replace this regularization termwith ?
||w ?
ws||22, where ws is the weight vectorlearned in the SRCONLY model.1 In this way, themodel trained on the target data ?prefers?
to haveweights that are similar to the weights from the SR-CONLY model, unless the data demands otherwise.Daume?
III and Marcu (2006) provide empirical evi-dence on four datasets that the PRIOR model outper-forms the baseline approaches.More recently, Daume?
III and Marcu (2006) pre-sented an algorithm for domain adaptation for max-imum entropy classifiers.
The key idea of their ap-proach is to learn three separate models.
One modelcaptures ?source specific?
information, one captures?target specific?
information and one captures ?gen-eral?
information.
The distinction between thesethree sorts of information is made on a per-examplebasis.
In this way, each source example is consid-ered either source specific or general, while eachtarget example is considered either target specific orgeneral.
Daume?
III and Marcu (2006) present an EMalgorithm for training their model.
This model con-sistently outperformed all the baseline approachesas well as the PRIOR model.
Unfortunately, despitethe empirical success of this algorithm, it is quitecomplex to implement and is roughly 10 to 15 timesslower than training the PRIOR model.3 Adaptation by Feature AugmentationIn this section, we describe our approach to the do-main adaptation problem.
Essentially, all we are go-ing to do is take each feature in the original problemand make three versions of it: a general version, asource-specific version and a target-specific version.The augmented source data will contain only generaland source-specific versions.
The augmented target1For the maximum entropy, SVM and naive Bayes learn-ing algorithms, modifying the regularization term is simple be-cause it appears explicitly.
For the perceptron algorithm, onecan obtain an equivalent regularization by performing standardperceptron updates, but using (w + ws)?x for making predic-tions rather than simply w?x.257data contains general and target-specific versions.To state this more formally, first recall the nota-tion from Section 2: X and Y are the input andoutput spaces, respectively; Ds is the source do-main data set and Dt is the target domain data set.Suppose for simplicity that X = RF for someF > 0.
We will define our augmented input spaceby X?
= R3F .
Then, define mappings ?s,?t :X ?
X?
for mapping the source and target datarespectively.
These are defined by Eq (1), where0 = ?0, 0, .
.
.
, 0?
?
RF is the zero vector.
?s(x) = ?x,x,0?, ?t(x) = ?x,0,x?
(1)Before we proceed with a formal analysis of thistransformation, let us consider why it might be ex-pected to work.
Suppose our task is part of speechtagging, our source domain is the Wall Street Journaland our target domain is a collection of reviews ofcomputer hardware.
Here, a word like ?the?
shouldbe tagged as a determiner in both cases.
However,a word like ?monitor?
is more likely to be a verbin the WSJ and more likely to be a noun in the hard-ware corpus.
Consider a simple case whereX = R2,where x1 indicates if the word is ?the?
and x2 indi-cates if the word is ?monitor.?
Then, in X?
, x?1 and x?2will be ?general?
versions of the two indicator func-tions, x?3 and x?4 will be source-specific versions, andx?5 and x?6 will be target-specific versions.Now, consider what a learning algorithm could doto capture the fact that the appropriate tag for ?the?remains constant across the domains, and the tagfor ?monitor?
changes.
In this case, the model canset the ?determiner?
weight vector to something like?1, 0, 0, 0, 0, 0?.
This places high weight on the com-mon version of ?the?
and indicates that ?the?
is mostlikely a determiner, regardless of the domain.
Onthe other hand, the weight vector for ?noun?
mightlook something like ?0, 0, 0, 0, 0, 1?, indicating thatthe word ?monitor?
is a noun only in the target do-main.
Similar, the weight vector for ?verb?
mightlook like ?0, 0, 0, 1, 0, 0?, indicating the ?monitor?
isa verb only in the source domain.Note that this expansion is actually redundant.We could equally well use ?s(x) = ?x,x?
and?t(x) = ?x,0?.
However, it turns out that it is eas-ier to analyze the first case, so we will stick withthat.
Moreover, the first case has the nice propertythat it is straightforward to generalize it to the multi-domain adaptation problem: when there are morethan two domains.
In general, for K domains, theaugmented feature space will consist of K+1 copiesof the original feature space.3.1 A Kernelized VersionIt is straightforward to derive a kernelized version ofthe above approach.
We do not exploit this propertyin our experiments?all are conducted with a simplelinear kernel.
However, by deriving the kernelizedversion, we gain some insight into the method.
Forthis reason, we sketch the derivation here.Suppose that the data points x are drawn from areproducing kernel Hilbert space X with kernel K :X ?
X ?
R, with K positive semi-definite.
Then,K can be written as the dot product (in X ) of two(perhaps infinite-dimensional) vectors: K(x, x?)
=??(x),?(x?
)?X .
Define ?s and ?t in terms of ?, as:?s(x) = ??(x),?(x),0?
(2)?t(x) = ??(x),0,?
(x)?Now, we can compute the kernel product be-tween ?s and ?t in the expanded RKHS by mak-ing use of the original kernel K. We denote the ex-panded kernel by K?
(x, x?).
It is simplest to first de-scribe K?
(x, x?)
when x and x?
are from the samedomain, then analyze the case when the domaindiffers.
When the domain is the same, we get:K?
(x, x?)
= ??(x),?(x?
)?X + ??(x),?(x?
)?X =2K(x, x?).
When they are from different domains,we get: K?
(x, x?)
= ??(x),?(x?
)?X = K(x, x?
).Putting this together, we have:K?
(x, x?)
={2K(x, x?)
same domainK(x, x?)
diff.
domain (3)This is an intuitively pleasing result.
What itsays is that?considering the kernel as a measureof similarity?data points from the same domain are?by default?
twice as similar as those from differ-ent domains.
Loosely speaking, this means that datapoints from the target domain have twice as muchinfluence as source points when making predictionsabout test target data.2583.2 AnalysisWe first note an obvious property of the feature-augmentation approach.
Namely, it does not makelearning harder, in a minimum Bayes error sense.
Amore interesting statement would be that it makeslearning easier, along the lines of the result of (Ben-David et al, 2006) ?
note, however, that their re-sults are for the ?semi-supervised?
domain adapta-tion problem and so do not apply directly.
As yet,we do not know a proper formalism in which to an-alyze the fully supervised case.It turns out that the feature-augmentation methodis remarkably similar to the PRIOR model2.
Sup-pose we learn feature-augmented weights in a clas-sifier regularized by an ?2 norm (eg., SVMs, maxi-mum entropy).
We can denote by ws the sum of the?source?
and ?general?
components of the learnedweight vector, and by wt the sum of the ?target?
and?general?
components, so that ws and wt are the pre-dictive weights for each task.
Then, the regulariza-tion condition on the entire weight vector is approx-imately ||wg||2 + ||ws ?
wg||2 + ||wt ?
wg||2, withfree parameter wg which can be chosen to minimizethis sum.
This leads to a regularizer proportional to||ws ?
wt||2, akin to the PRIOR model.Given this similarity between the feature-augmentation method and the PRIOR model, onemight wonder why we expect our approach to dobetter.
Our belief is that this occurs because we op-timize ws and wt jointly, not sequentially.
First, thismeans that we do not need to cross-validate to es-timate good hyperparameters for each task (thoughin our experiments, we do not use any hyperparam-eters).
Second, and more importantly, this meansthat the single supervised learning algorithm thatis run is allowed to regulate the trade-off betweensource/target and general weights.
In the PRIORmodel, we are forced to use the prior variance onin the target learning scenario to do this ourselves.3.3 Multi-domain adaptationOur formulation is agnostic to the number of?source?
domains.
In particular, it may be the casethat the source data actually falls into a variety ofmore specific domains.
This is simple to accountfor in our model.
In the two-domain case, we ex-2Thanks an anonymous reviewer for pointing this out!panded the feature space from RF to R3F .
For aK-domain problem, we simply expand the featurespace to R(K+1)F in the obvious way (the ?+1?
cor-responds to the ?general domain?
while each of theother 1 .
.
.K correspond to a single task).4 ResultsIn this section we describe experimental results on awide variety of domains.
First we describe the tasks,then we present experimental results, and finally welook more closely at a few of the experiments.4.1 TasksAll tasks we consider are sequence labeling tasks(either named-entity recognition, shallow parsing orpart-of-speech tagging) on the following datasets:ACE-NER.
We use data from the 2005 AutomaticContent Extraction task, restricting ourselves tothe named-entity recognition task.
The 2005ACE data comes from 5 domains: Broad-cast News (bn), Broadcast Conversations (bc),Newswire (nw), Weblog (wl), Usenet (un) andConverstaional Telephone Speech (cts).CoNLL-NE.
Similar to ACE-NER, a named-entityrecognition task.
The difference is: we use the2006 ACE data as the source domain and theCoNLL 2003 NER data as the target domain.PubMed-POS.
A part-of-speech tagging problemon PubMed abstracts introduced by Blitzer etal.
(2006).
There are two domains: the sourcedomain is the WSJ portion of the Penn Tree-bank and the target domain is PubMed.CNN-Recap.
This is a recapitalization task intro-duced by Chelba and Acero (2004) and alsoused by Daume?
III and Marcu (2006).
Thesource domain is newswire and the target do-main is the output of an ASR system.Treebank-Chunk.
This is a shallow parsing taskbased on the data from the Penn Treebank.
Thisdata comes from a variety of domains: the stan-dard WSJ domain (we use the same data as forCoNLL 2000), the ATIS switchboard domain,and the Brown corpus (which is, itself, assem-bled from six subdomains).Treebank-Brown.
This is identical to the Treebank-Chunk task, except that we consider all of theBrown corpus to be a single domain.259Task Dom # Tr # De # Te # Ftbn 52,998 6,625 6,626 80kbc 38,073 4,759 4,761 109kACE- nw 44,364 5,546 5,547 113kNER wl 35,883 4,485 4,487 109kun 35,083 4,385 4,387 96kcts 39,677 4,960 4,961 54kCoNLL- src 256,145 - - 368kNER tgt 29,791 5,258 8,806 88kPubMed- src 950,028 - - 571kPOS tgt 11,264 1,987 14,554 39kCNN- src 2,000,000 - - 368kRecap tgt 39,684 7,003 8,075 88kwsj 191,209 29,455 38,440 94kswbd3 45,282 5,596 41,840 55kbr-cf 58,201 8,307 7,607 144kTree br-cg 67,429 9,444 6,897 149kbank- br-ck 51,379 6,061 9,451 121kChunk br-cl 47,382 5,101 5,880 95kbr-cm 11,696 1,324 1,594 51kbr-cn 56,057 6,751 7,847 115kbr-cp 55,318 7,477 5,977 112kbr-cr 16,742 2,522 2,712 65kTable 1: Task statistics; columns are task, domain,size of the training, development and test sets, andthe number of unique features in the training set.In all cases (except for CNN-Recap), we useroughly the same feature set, which has becomesomewhat standardized: lexical information (words,stems, capitalization, prefixes and suffixes), mem-bership on gazetteers, etc.
For the CNN-Recap task,we use identical feature to those used by both Chelbaand Acero (2004) and Daume?
III and Marcu (2006):the current, previous and next word, and 1-3 letterprefixes and suffixes.Statistics on the tasks and datasets are in Table 1.In all cases, we use the SEARN algorithm for solv-ing the sequence labeling problem (Daume?
III et al,2007) with an underlying averaged perceptron clas-sifier; implementation due to (Daume?
III, 2004).
Forstructural features, we make a second-order Markovassumption and only place a bias feature on the tran-sitions.
For simplicity, we optimize and report onlyon label accuracy (but require that our outputs beparsimonious: we do not allow ?I-NP?
to follow?B-PP,?
for instance).
We do this for three rea-sons.
First, our focus in this work is on buildingbetter learning algorithms and introducing a morecomplicated measure only serves to mask these ef-fects.
Second, it is arguable that a measure like F1 isinappropriate for chunking tasks (Manning, 2006).Third, we can easily compute statistical significanceover accuracies using McNemar?s test.4.2 Experimental ResultsThe full?somewhat daunting?table of results ispresented in Table 2.
The first two columns spec-ify the task and domain.
For the tasks with only asingle source and target, we simply report results onthe target.
For the multi-domain adaptation tasks,we report results for each setting of the target (whereall other data-sets are used as different ?source?
do-mains).
The next set of eight columns are the errorrates for the task, using one of the different tech-niques (?AUGMENT?
is our proposed technique).For each row, the error rate of the best performingtechnique is bolded (as are all techniques whose per-formance is not statistically significantly different atthe 95% level).
The ?T<S?
column is contains a ?+?whenever TGTONLY outperforms SRCONLY (thiswill become important shortly).
The final columnindicates when AUGMENT comes in first.3There are several trends to note in the results.
Ex-cluding for a moment the ?br-*?
domains on theTreebank-Chunk task, our technique always per-forms best.
Still excluding ?br-*?, the clear second-place contestant is the PRIOR model, a finding con-sistent with prior research.
When we repeat theTreebank-Chunk task, but lumping all of the ?br-*?data together into a single ?brown?
domain, the storyreverts to what we expected before: our algorithmperforms best, followed by the PRIOR method.Importantly, this simple story breaks down on theTreebank-Chunk task for the eight sections of theBrown corpus.
For these, our AUGMENT techniqueperforms rather poorly.
Moreover, there is no clearwinning approach on this task.
Our hypothesis isthat the common feature of these examples is thatthese are exactly the tasks for which SRCONLY out-performs TGTONLY (with one exception: CoNLL).This seems like a plausible explanation, since it im-plies that the source and target domains may not bethat different.
If the domains are so similar thata large amount of source data outperforms a smallamount of target data, then it is unlikely that blow-3One advantage of using the averaged perceptron for all ex-periments is that the only tunable hyperparameter is the numberof iterations.
In all cases, we run 20 iterations and choose theone with the lowest error on development data.260Task Dom SRCONLY TGTONLY ALL WEIGHT PRED LININT PRIOR AUGMENT T<S Winbn 4.98 2.37 2.29 2.23 2.11 2.21 2.06 1.98 + +bc 4.54 4.07 3.55 3.53 3.89 4.01 3.47 3.47 + +ACE- nw 4.78 3.71 3.86 3.65 3.56 3.79 3.68 3.39 + +NER wl 2.45 2.45 2.12 2.12 2.45 2.33 2.41 2.12 = +un 3.67 2.46 2.48 2.40 2.18 2.10 2.03 1.91 + +cts 2.08 0.46 0.40 0.40 0.46 0.44 0.34 0.32 + +CoNLL tgt 2.49 2.95 1.80 1.75 2.13 1.77 1.89 1.76 +PubMed tgt 12.02 4.15 5.43 4.15 4.14 3.95 3.99 3.61 + +CNN tgt 10.29 3.82 3.67 3.45 3.46 3.44 3.35 3.37 + +wsj 6.63 4.35 4.33 4.30 4.32 4.32 4.27 4.11 + +swbd3 15.90 4.15 4.50 4.10 4.13 4.09 3.60 3.51 + +br-cf 5.16 6.27 4.85 4.80 4.78 4.72 5.22 5.15Tree br-cg 4.32 5.36 4.16 4.15 4.27 4.30 4.25 4.90bank- br-ck 5.05 6.32 5.05 4.98 5.01 5.05 5.27 5.41Chunk br-cl 5.66 6.60 5.42 5.39 5.39 5.53 5.99 5.73br-cm 3.57 6.59 3.14 3.11 3.15 3.31 4.08 4.89br-cn 4.60 5.56 4.27 4.22 4.20 4.19 4.48 4.42br-cp 4.82 5.62 4.63 4.57 4.55 4.55 4.87 4.78br-cr 5.78 9.13 5.71 5.19 5.20 5.15 6.71 6.30Treebank-brown 6.35 5.75 4.80 4.75 4.81 4.72 4.72 4.65 + +Table 2: Task results.ing up the feature space will help.We additionally ran the MEGAM model (Daume?III and Marcu, 2006) on these data (though notin the multi-conditional case; for this, we consid-ered the single source as the union of all sources).The results are not displayed in Table 2 to savespace.
For the majority of results, MEGAM per-formed roughly comparably to the best of the sys-tems in the table.
In particular, it was not sta-tistically significantly different that AUGMENT on:ACE-NER, CoNLL, PubMed, Treebank-chunk-wsj,Treebank-chunk-swbd3, CNN and Treebank-brown.It did outperform AUGMENT on the Treebank-chunkon the Treebank-chunk-br-* data sets, but only out-performed the best other model on these data setsfor br-cg, br-cm and br-cp.
However, despite itsadvantages on these data sets, it was quite signifi-cantly slower to train: a single run required about tentimes longer than any of the other models (includingAUGMENT), and also required five-to-ten iterationsof cross-validation to tune its hyperparameters so asto achieve these results.4.3 Model IntrospectionOne explanation of our model?s improved perfor-mance is simply that by augmenting the featurespace, we are creating a more powerful model.While this may be a partial explanation, here weshow that what the model learns about the various* bn bc nw wl un ctsPERGPEORGLOCFigure 1: Hinton diagram for feature /Aa+/ at cur-rent position.domains actually makes some plausible sense.We perform this analysis only on the ACE-NERdata by looking specifically at the learned weights.That is, for any given feature f , there will be sevenversions of f : one corresponding to the ?cross-domain?
f and seven corresponding to each domain.We visualize these weights, using Hinton diagrams,to see how the weights vary across domains.For example, consider the feature ?current wordhas an initial capital letter and is then followed byone or more lower-case letters.?
This feature is pre-sumably useless for data that lacks capitalization in-formation, but potentially quite useful for other do-mains.
In Figure 1 we shown a Hinton diagram forthis figure.
Each column in this figure correspondto a domain (the top row is the ?general domain?
).261* bn bc nw wl un ctsPERGPEORGLOCFigure 2: Hinton diagram for feature /bush/ at cur-rent position.Each row corresponds to a class.4 Black boxes cor-respond to negative weights and white boxes corre-spond to positive weights.
The size of the box de-picts the absolute value of the weight.As we can see from Figure 1, the /Aa+/ featureis a very good indicator of entity-hood (it?s value isstrongly positive for all four entity classes), regard-less of domain (i.e., for the ?*?
domain).
The lackof boxes in the ?bn?
column means that, beyond thesettings in ?
*?, the broadcast news is agnostic withrespect to this feature.
This makes sense: there isno capitalization in broadcast news domain, so therewould be no sense is setting these weights to any-thing by zero.
The usenet column is filled with neg-ative weights.
While this may seem strange, it isdue to the fact that many email addresses and URLsmatch this pattern, but are not entities.Figure 2 depicts a similar figure for the feature?word is ?bush?
at the current position?
(this figure iscase sensitive).5 These weights are somewhat harderto interpret.
What is happening is that ?by default?the word ?bush?
is going to be a person?this is be-cause it rarely appears referring to a plant and soeven in the capitalized domains like broadcast con-versations, if it appears at all, it is a person.
Theexception is that in the conversations data, peopledo actually talk about bushes as plants, and so theweights are set accordingly.
The weights are high inthe usenet domain because people tend to talk aboutthe president without capitalizing his name.4Technically there are many more classes than are shownhere.
We do not depict the smallest classes, and have mergedthe ?Begin-*?
and ?In-*?
weights for each entity type.5The scale of weights across features is not comparable, sodo not try to compare Figure 1 with Figure 2.
* bn bc nw wl un ctsPERGPEORGLOCFigure 3: Hinton diagram for feature /the/ at currentposition.
* bn bc nw wl un ctsPERGPEORGLOCFigure 4: Hinton diagram for feature /the/ at previ-ous position.Figure 3 presents the Hinton diagram for the fea-ture ?word at the current position is ?the??
(again,case-sensitive).
In general, it appears, ?the?
is acommon word in entities in all domain except forbroadcast news and conversations.
The exceptionsare broadcast news and conversations.
These excep-tions crop up because of the capitalization issue.In Figure 4, we show the diagram for the feature?previous word is ?the?.?
The only domain for whichthis is a good feature of entity-hood is broadcastconversations (to a much lesser extent, newswire).This occurs because of four phrases very common inthe broadcast conversations and rare elsewhere: ?theIraqi people?
(?Iraqi?
is a GPE), ?the Pentagon?
(anORG), ?the Bush (cabinet|advisors|.
.
.
)?
(PER), and?the South?
(LOC).Finally, Figure 5 shows the Hinton diagram forthe feature ?the current word is on a list of com-mon names?
(this feature is case-insensitive).
Allaround, this is a good feature for picking out peopleand nothing else.
The two exceptions are: it is alsoa good feature for other entity types for broadcast262* bn bc nw wl un ctsPERGPEORGLOCFigure 5: Hinton diagram for membership on a listof names at current position.news and it is not quite so good for people in usenet.The first is easily explained: in broadcast news, itis very common to refer to countries and organiza-tions by the name of their respective leaders.
This isessentially a metonymy issue, but as the data is an-notated, these are marked by their true referent.
Forusenet, it is because the list of names comes fromnews data, but usenet names are more diverse.In general, the weights depicte for these featuresmake some intuitive sense (in as much as weightsfor any learned algorithm make intuitive sense).
Itis particularly interesting to note that while there aresome regularities to the patterns in the five diagrams,it is definitely not the case that there are, eg., twodomains that behave identically across all features.This supports the hypothesis that the reason our al-gorithm works so well on this data is because thedomains are actually quite well separated.5 DiscussionIn this paper we have described an incredibly sim-ple approach to domain adaptation that?under acommon and easy-to-verify condition?outperformsprevious approaches.
While it is somewhat frus-trating that something so simple does so well, itis perhaps not surprising.
By augmenting the fea-ture space, we are essentially forcing the learningalgorithm to do the adaptation for us.
Good super-vised learning algorithms have been developed overdecades, and so we are essentially just leveraging allthat previous work.
Our hope is that this approachis so simple that it can be used for many more real-world tasks than we have presented here with littleeffort.
Finally, it is very interesting to note that us-ing our method, shallow parsing error rate on theCoNLL section of the treebank improves from 5.35to 5.11.
While this improvement is small, it is real,and may carry over to full parsing.
The most impor-tant avenue of future work is to develop a formalframework under which we can analyze this (andother supervised domain adaptation models) theo-retically.
Currently our results only state that thisaugmentation procedure doesn?t make the learningharder ?
we would like to know that it actuallymakes it easier.
An additional future direction isto explore the kernelization interpretation further:why should we use 2 as the ?similarity?
betweendomains?we could introduce a hyperparamter ?that indicates the similarity between domains andcould be tuned via cross-validation.Acknowledgments.
We thank the three anony-mous reviewers, as well as Ryan McDonald andJohn Blitzer for very helpful comments and insights.ReferencesShai Ben-David, John Blitzer, Koby Crammer, and FernandoPereira.
2006.
Analysis of representations for domain adap-tation.
In Advances in Neural Information Processing Sys-tems (NIPS).John Blitzer, Ryan McDonald, and Fernando Pereira.
2006.Domain adaptation with structural correspondence learning.In Proceedings of the Conference on Empirical Methods inNatural Language Processing (EMNLP).Ciprian Chelba and Alex Acero.
2004.
Adaptation of max-imum entropy classifier: Little data can help a lot.
In Pro-ceedings of the Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), Barcelona, Spain.Hal Daume?
III and Daniel Marcu.
2006.
Domain adaptationfor statistical classifiers.
Journal of Artificial IntelligenceResearch, 26.Hal Daume?
III, John Langford, and Daniel Marcu.
2007.Search-based structured prediction.
Machine Learning Jour-nal (submitted).Hal Daume?
III.
2004.
Notes on CG and LM-BFGS opti-mization of logistic regression.
Paper available at http://pub.hal3.name/#daume04cg-bfgs, implemen-tation available at http://hal3.name/megam/, Au-gust.Christopher Manning.
2006.
Doing named entity recognition?Don?t optimize for F1.
Post on the NLPers Blog, 25August.
http://nlpers.blogspot.com/2006/08/doing-named-entity-recognition-dont.html.263
