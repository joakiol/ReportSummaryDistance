Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 174?182,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsLexical Patterns or Dependency Patterns:Which Is Better for Hypernym Extraction?Erik Tjong Kim SangAlfa-informaticaUniversity of Groningene.f.tjong.kim.sang@rug.nlKatja HofmannISLA, Informatics InstituteUniversity of Amsterdamkhofmann@science.uva.nlAbstractWe compare two different types of extractionpatterns for automatically deriving semanticinformation from text: lexical patterns, builtfrom words and word class information, anddependency patterns with syntactic informa-tion obtained from a full parser.
We are partic-ularly interested in whether the richer linguis-tic information provided by a parser allows fora better performance of subsequent informa-tion extraction work.
We evaluate automaticextraction of hypernym information from textand conclude that the application of depen-dency patterns does not lead to substantiallyhigher precision and recall scores than usinglexical patterns.1 IntroductionFor almost a decade, automatic sentence parsingsystems with a reasonable performance (90+% con-stituent precision/recall) have been available for En-glish (Charniak, 1999).
In recent years there hasbeen an increase in linguistic applications which useparsing as a preprocessing step, e.g.
Snow et al(2006) and Surdeanu et al (2008).
One of the boostsfor these new applications was the increasing powerof desktop computers, which allows for an easier ac-cess to the computing-intensive parsing results.
An-other is the increased popularity of dependency pars-ing of which the results can easily be incorporatedinto followup systems.Although there is a consensus about the fact thatthe richness of the dependency structures should, inprinciple, enable better performance than lexical in-formation or shallow parsing results, it is not clear ifthese better results can also be obtained in practice.A performance of 90% precision and recall at con-stituent level still leaves an average of one error ina medium-length sentence of ten words.
These er-rors could degrade the performance of any approachwhich relies heavily on parser output.The question of whether to include a full parser asa preprocessor for natural language processing task,has led to a heated discussion between the two au-thors of the paper.
One of us argues that full parsersare slow and make too many errors, and relies onshallow techniques like part-of-speech tagging forpreprocessing.
The other points at the decreasingcosts of computing and improvements in the reliabil-ity of parsers, and recommends dependency parsersas preprocessing tools.While no automatic text preprocessing method isfree of errors, it is indeed true that approaches otherthan full parsing, like for example shallow parsing,offer useful information at a considerably cheaperprocessing cost.
The choice between using a heavyfull parser or a light shallow language analyzer isone that developers of language processing systemsfrequently have to make.
The expected performanceboost of parsed data could be an important motiva-tion for choosing for full syntactic analysis.
How-ever, we do not know how big the difference be-tween the two methods will be.
In order to find thisout, we designed an experiment in which we com-pared the effects of preprocessing with and withoutusing information generated by a full parser.In this paper, we compare two text preprocess-ing approaches for a single language processingtask.
The first of the two methods is shallow lin-174guistic processing, a robust and fast text analysismethod which only uses information from words,like lemma information and part-of-speech classes.The second method is dependency parsing which in-cludes information about the syntactic relations be-tween words.
The natural language processing taskwhich we will use for assessing the usability of thetwo processing methods is automatic extraction ofhypernym information from text.
The language ofthe text documents is Dutch.
We expect that the find-ings of this study would have been similar if anyother Germanic language (including English) wasused.The contribution of this paper is a thorough andfair comparison of the involved preprocessing tech-niques.
There have been earlier studies of hyper-nym extraction with either lexical or dependency ex-traction patterns.
However, these studies applied thetechniques to a variety of different data sets and useddifferent evaluation techniques.
We will apply thetwo methods to the same data, evaluate the results ina consistent manner and examine the differences.After this introduction, we will describe the task,the preprocessing methods and the evaluation settingin more detail.
In the third section, we will showhow our experiments were set up and present the re-sults.
Section four contains a detailed discussion ofthe two methods and their effect on the extractiontask.
In the final section of the paper, we will presentsome concluding remarks.2 Task and methodsWe will apply two different preprocessing methodsto the task of extracting lexical information fromtext.
In the next sections we describe this task, dis-cuss different methods for preprocessing the dataand outline the method used for evaluating the re-sults.2.1 Extracting hypernym relationsWe will concentrate on extracting a single type oflexical relation: hypernymy.
Word A is a hypernymof word B if the meaning of A both covers the mean-ing of B and is broader.
For example, color is a hy-pernym of red which in turn is a hypernym of scar-let.
If A is a hypernym of B than B is a hyponym ofA.There has been quite a lot of work on extractinghypernymy pairs from text.
The pioneering workof Hearst (1992) applied fixed patterns like NP1 ,especially NP2 to derive that NP1 is a hypernymof NP2.
Lately there has been a lot of interest inacquiring such text patterns using a set of hyper-nymy examples, e.g.
Pantel et al (2004) and Snow(2006).
Application of such techniques has not beenrestricted to English but also involved other lan-guages such as Dutch (Tjong Kim Sang and Hof-mann, 2007).
Recent work has also examined ex-tracting hypernym information from structured data,like Wikipedia (Sumida and Torisawa, 2008).For our extraction work, we will closely followthe approach described in Snow et al (2006):1.
Collect from a text corpus phrases (consecutiveword sequences from a single sentence) thatcontain a pair of nouns2.
Mark each phrase as containing a hypernympair or a non-hypernym pair according to a lex-ical resource3.
Remove the noun pair from the phrases andregister how often each phrase is associated byhypernym pairs and by non-hypernym pairs4.
Use this information for training a machinelearning system to predict whether two nounsare a hypernym-hyponym pair based on thephrases in which they occur in a text corpusFor example, we find two phrases: colors such ascyan and colors such as birds, both of which containthe basic phrase such as.
We mark the first phraseas a hypernym phrase (color is a hypernym of cyan)while the second is marked as non-hypernym (coloris not a hypernym of bird).
Thus the pattern such aswill receive a positive point and a negative point.
Amachine learning algorithm can deduce from thesenumbers that two other nouns occurring in the samepattern will have an estimated probability of 50% ofbeing related according to hypernymy.
The learnercan use information from other patterns to obtain abetter estimation of this probability.2.2 Lexical patternsWe use two different text preprocessing methodswhich automatically assign linguistic information tosentences.
The first preprocessing method has the175advantage of offering a fast analysis of the data butits results are less elaborate than those of the secondmethod.
The first method consists of three steps:?
Tokenization: sentence boundaries are detectedand punctuation signs are separated from words?
Part-of-speech tagging: part-of-speech classeslike noun and verb are assigned to words?
Lemmatization: words are reduced to their ba-sic form (lemma)The analysis process would convert a phrase likeLarge cities in northern England such as Liverpoolare beyond revival.
to lemmas and their associatedpart-of-speech tags: large/JJ city/NN in/IN north/JJEngland/NNP such/DT as/IN Liverpool/NNP be/VBbeyond/IN revival/NN ./.Like in the work of Snow et al (2005), the tar-get phrases for hypernym extraction are two nounphrases, with a maximum of three tokens in be-tween and one or two optional extra tokens (a non-head token of the first noun phrase and/or one ofthe second noun phrase).
The lexical preprocessingmethod uses two basic regular expressions for find-ing noun phrases: Determiner?
Adjective* Noun+and ProperNoun+.
It assumes that the final tokenof the matched phrase is the head.
Here is one set offour patterns which can be derived from the examplesentence:1.
NP in NP2.
large NP in NP3.
NP in north NP4.
large NP in north NPThe patterns contain the lemmas rather than thewords of the sentence in order to allow for generalpatterns.
For the same reason, the noun phrases havebeen replaced by the token NP.
Each of the four pat-terns will be used as evidence for a possible hyper-nymy relation between the two noun phrase headscity and England.
As a novel extension to the workof Snow et al, we included two additional variantsof each pattern in which either the first NP or thesecond NP was replaced by its head:5. city in NP6.
NP in EnglandThis enabled us to identify among others appositionsas patterns: president NP.2.3 Dependency patternsA dependency analysis contains the same three stepsused for finding lexical patterns: tokenization, part-of-speech tagging and lemmatization.
Additionally,it includes a fourth step:?
Dependency parsing: find the syntactic depen-dency relations between the words in each sen-tenceThe syntactic analysis is head-based which meansthat for each word in the sentence it finds anotherword that dominates it.
Here is a possible analysisof the previous example sentence:large:JJ:MOD:NN:citycity:NN:SUBJ:VBD:bein:IN:MOD:NN:citynorth:JJ:MOD:NNP:EnglandEngland:NNP:OBJ1:IN:insuch:DT:MOD:IN:asas:IN:MOD:NN:cityLiverpool:NNP:OBJ1:IN:asbe:VB:?:?
:?beyond:IN:MOD:VB:berevival:NN::OBJ1:IN:beyondEach line contains a lemma, its part-of-speech tag,the relation between the word and its head, the part-of-speech tag of its head and the lemma of the headword.
Our work with dependency patterns closelyfollows the work of Snow et al (2005).
Patterns aredefined as dependency paths with at most three in-termediate nodes between the two focus nouns.
Ad-ditional satellite nodes can be present next to the twonouns.
The dependency patterns contain more infor-mation than the lexical patterns.
Here is one of thepatterns that can be derived for the two noun phraseslarge cities and northern England in the examplesentence:NP1:NN:SUBJ:VBD:in:IN:MOD:NN:NP1NP2:NNP:OBJ1:IN:inThe pattern defines a path from the head lemma cityvia in, to England.
Note that lemma informationlinking outside this pattern (be at the end of the firstline) has been removed and that lemma information176from the target noun phrases has been replaced bythe name of the noun phrase (NP1 at the end of thesecond line).
For each dependency pattern, we buildsix variants similar to the six variants of the lexicalpatterns: four with additional information from thetwo noun phrases and two more with head informa-tion of one of the two target NPs.Both preprocessing methods can identify phraseslike N such as N1 , N2 and N3 as well.
Such phrasesproduce evidence for each of the pairs (N,N1),(N,N2) and (N,N3).
These three noun pairs will beincluded in the data collected for the patterns thatcan be derived from the phrase.We expect that an important advantage of usingdependency patterns over lexical patterns will bethat the former offer a wider coverage.
In the ex-ample sentence, no lexical pattern will associate citywith Liverpool because there are too many words inbetween.
However, a dependency pattern will cre-ate a link between these two words, via the wordas.
This will enable the dependency patterns to findout that city is a hypernym of Liverpool, where thelexical patterns are not able to do this based on theavailable information.The two preprocessing methods generate a largenumber of noun pairs associated by patterns.
LikeSnow et al (2005), we keep only noun pairs whichare associated by at least five different patterns.
Thesame constraint is enforced on the extraction pat-terns: we keep only the patterns which are associ-ated by at least five different noun pairs.
The datais converted to binary feature vectors representingnoun pairs.
These are training data for a BayesianLogistic Regression system, BBRtrain (Genkin etal., 2004).
We use the default settings of the learn-ing system and test its prediction capability in a bi-nary classification task: whether two nouns are re-lated according to hypernymy or not.
Evaluation isperformed by 10-fold cross validation.2.4 EvaluationFor parameter optimization we need an automaticevaluation procedure, since repeated manual checksof results generated by different versions of thelearner require too much time.
We have adopted theevaluation method of Snow et al(2006): comparethe generated hypernyms with hypernyms present ina lexical resource, in our case the Dutch part of Eu-roWordNet (1998).This choice results in two restrictions.
First, wewill only consider pairs of known words (words thatare present in the lexical resource) for evaluation.We have no information about other words so wemake no assumptions about them.
Second, if twowords appear in the lexical resource but not in thehypernym relation of that same resource then wewill assume that they are unrelated.
In other words,we assume the hypernymy relation specified in thelexical resource as complete (like in the work ofSnow et al (2006)).We use standard evaluation scores.
We will com-pute precision and recall for the candidate hyper-nyms, as well as the related F?=1 rate, the harmonicmean between precision and recall.
Precision will becomputed against all chosen candidate hypernyms.However, recall will only be computed against thepositive noun pairs which occur in the phrases se-lected by the examined method.
The different pre-processing methods may cause different numbers ofpositive pairs to be selected.
Only these pairs willbe used for computing recall scores.
Others will beignored.
For this reason we will report the selectednumber of positive target pairs in the result tables aswell1.3 Experiments and resultsWe have applied the extraction techniques to twodifferent Dutch corpora.
The first is a collection oftexts from the news domain.
It consists of texts fromfive different Dutch news papers from the TwenteNews Corpus collection.
Two versions of this cor-pus exist.
We have worked with the version whichcontains the years 1997-2005 (26 million sentencesand 450 million tokens).
The second corpus is theDutch Wikipedia.
Here we used a version of Octo-ber 2006 (5 million sentences and 58 million words).Syntactic preprocessing of the material was donewith the Alpino parser, the best available parser forDutch with a labeled dependency accuracy of 89%(Van Noord, 2006).
Rather than performing theparsing task ourselves, we have relied on an avail-able parsed treebank which included the text corpora1In a seperate study we have shown that the observed differ-ences between the two methods remain the same when recall iscomputed over sets of similar sizes (Tjong Kim Sang, 2009).177that we wanted to use (Van Noord, 2009).The parser also performs part-of-speech taggingand lemmatization, tasks which are useful for thelexical preprocessing methods.
However, taking fu-ture real-time applications in mind, we did not wantthe lexical processing to be dependent on the parser.Therefore we have developed an in-house part-of-speech tagger and lemmatizer based on the materialcreated in the Corpus Spoken Dutch project (Eynde,2005).
The tagger achieved an accuracy of 96% ontest data from the same project while the lemmatizerachieved 98%.We used the Dutch part of EuroWordNet (Vossen,1998) as the gold standard lexical resource, both fortraining and testing.
In the lexicon, many nouns havedifferent senses.
This can cause problems for thepattern extraction process.
For example, if a nounN1 with sense X is related to another noun N2 thenthe appearance of N1 with sense Y with N2 in thetext may be completely accidental and say nothingabout the relation between the two words.
In thatcase it would be wrong to regard the context of thetwo words as an interesting extraction pattern.There are several ways to deal with this prob-lem.
One is to automatically assign senses to words.However we do not have a reliable sense tagger forDutch at our disposal.
Another method was pro-posed by Snow et al(2005): assume that every wordbears its most frequent sense.
But this is also in-formation which we lack for Dutch: our lexical re-source does not contain frequency information forword senses.
We have chosen the approach sug-gested by Hofmann and Tjong Kim Sang (2007):remove all nouns with multiple senses from the dataset and use only the monosemous words for find-ing good extraction patterns.
This restriction is onlyimposed in the training phase.
We consider bothmonosemous words and polysemous words in theevaluation process.We imposed two additional restrictions on the lex-ical resource.
First, we removed the top noun ofthe hypernymy hierarchy (iets) from the list of validhypernyms.
This word is a valid hypernym of anyother noun.
It is not an interesting suggestion forthe extraction procedure to put forward.
Second, werestricted the extraction procedure to propose onlyknown hypernyms as candidate hypernyms.
Nounsthat appeared in the lexical resources only as hy-lexical patternsData source Targ.
Prec.
Recall F?=1AD 620 55.8% 27.9% 37.2NRC 882 50.4% 23.8% 32.3Parool 462 51.8% 21.9% 30.8Trouw 607 54.1% 25.9% 35.0Volkskrant 970 49.7% 24.1% 32.5Newspapers 3307 43.1% 26.7% 33.0Wikipedia 1288 63.4% 44.3% 52.1dependency patternsData source Targ.
Prec.
Recall F?=1AD 706 42.9% 30.2% 35.4NRC 1224 26.2% 25.3% 25.7Parool 584 31.2% 23.8% 27.0Trouw 760 35.3% 29.0% 31.8Volkskrant 1204 29.2% 25.5% 27.2Newspapers 3806 20.7% 29.1% 24.2Wikipedia 1580 61.9% 47.0% 53.4Table 1: Hypernym extraction scores for the five news-papers in the Twente News Corpus (AD, NRC, Parool,Trouw and Volkskrant) and for the Dutch Wikipedia.The Targets column shows the number of unique posi-tive word pairs in each data set.
The Dutch Wikipediacontains about as much data as one of the newspaper sec-tions.ponyms (leaf nodes of the hypernymy tree) werenever proposed as candidate hypernyms.
This madesense for our evaluation procedure which is onlyaimed at finding known hypernym-hyponym pairs.We performed two hypernym extraction experi-ments, one which used lexical extraction patternsand one which used dependency patterns2.
The re-sults from the experiments can be found in Table1.
The newspaper F-scores obtained with lexicalpatterns are similar to those reported for English(Snow et al, 2005, 32.0) but the dependency pat-terns perform worse.
Both approaches perform wellon Wikipedia data, most likely because of the morerepeated sentence structures and the presence ofmany definition sentences.
For newspaper data, lex-ical patterns outperform dependency patterns bothfor precision and F?=1.
For Wikipedia data the dif-ferences are smaller and in fact the dependency pat-2The software used in these experiment has been made avail-able at http://www.let.rug.nl/erikt/cornetto/D08.zip178terns obtain the best F-score.
For all data sets, thedependency patterns suggest more related pairs thanthe lexical patterns (column Targets).
The differ-ences between the two pattern types are significant(p < 0.05) for all evaluation measures for Newspa-pers and for positive targets and recall for Wikipedia.4 Result analysisIn this section, we take a closer look at the results de-scribed in the previous section.
We start with look-ing for an explanation for the differences betweenthe scores obtained with lexical patterns and depen-dency patterns.
First we examine the results forWikipedia data and then the results for newspaperdata.
Finally, we perform an error analysis to findout the strengths and weaknesses of each of the twomethods.4.1 Wikipedia dataThe most important difference between the two pat-tern types for Wikipedia data is the number of posi-tive targets (Table 1).
Dependency patterns find 23%more related pairs in the Wikipedia data than lexi-cal patterns (1580 vs. 1288).
This effect can alsobe simulated by changing the size of the corpus.
Ifwe restrict the data set of the dependency patternsto 70% of its current size then the patterns retrieve asimilar number of positive targets as the lexical pat-terns, 1289, with comparable precision, recall andF?=1 scores (62.5%, 46.6% and 53.4).
So we expectthat the effect of applying the dependency patternsis the same as applying the lexical patterns to 43%more data.4.2 Newspaper dataPerformance-wise there seems to be only a smalldifference between the two preprocessing methodswhen applied to the Wikipedia data set.
However,when we examine the scores obtained on the news-paper data (Table 1) then we find larger differences.Dependency patterns remain finding more positivetargets and obtaining a larger recall score but theirprecision score is disappointing.
However, when weexamine the precision-recall plots of the two meth-ods (Figure 1, obtained by varying the acceptancethreshold of the machine learner), they are almostindistinguishable.
The performance line for lexicalpatterns extends further to the left than the one ofFigure 1: Performance of individual hypernym extractionpatterns applied to the combination of five newspapersand Wikipedia.
Each + in the graphs represent a differ-ent extraction pattern.
The precision-recall graphs for themachine learner (lines) are identical for each data sourceexcept for the extended part of the performance line forlexical patterns.179lexical patterns applied to NewspapersKey Phrase Targ.
Prec.
Recall F?=1N and other N 376 22.0% 11.4% 15.0N such as N 222 25.1% 6.7% 10.6N like N 579 7.6% 17.5% 10.6N , such as N 263 15.6% 8.0% 10.5N ( N 323 7.5% 9.8% 8.5dependency patterns applied to NewspapersKey Phrase Targ.
Prec.
Recall F?=1N and other N 420 21.1% 11.0% 14.5N be a N 451 8.2% 11.8% 9.7N like N 205 27.3% 5.4% 9.0N be N 766 5.7% 20.1% 8.8N such as N 199 22.4% 5.2% 8.5lexical patterns applied to WikipediaKey Phrase Targ.
Prec.
Recall F?=1N be a N 294 40.8% 22.8% 29.3N be N 418 22.9% 32.5% 26.9a N be N 185 53.3% 14.4% 22.6N such as N 161 57.5% 12.5% 20.5N ( N 188 21.2% 14.6% 17.3dependency patterns applied to WikipediaKey Phrase Targ.
Prec.
Recall F?=1N be N 609 33.6% 38.5% 35.9N be a N 452 44.3% 28.6% 34.8the N be N 258 34.0% 16.3% 22.1a N be N 184 44.7% 11.6% 18.5N N 234 16.6% 14.8% 15.6Table 2: Best performing extraction patterns according toF-scores.the dependency patterns but the remainder of the twographs overlap.
The measured performances in Ta-ble 1 are different because the machine learner putthe acceptance level for extracted pairs at differentpoints of the graph: the performance lines in bothnewspaper graphs contain (recall,precision) points(26.7%,43.1%) and (29.1%,20.7%).We are unable to find major differences in the re-sults of the two approaches.
We conclude that, apartfrom an effect which can be simulated with someextra data, there is no difference between prepro-cessing text with shallow methods and with a full56 ?
covered by other patterns12 48% required full parsing6 24% lemmatization errors3 12% omitted for lack of support3 12% pos tagging errors1 4% extraction pattern error81 100%45 ?
covered by other patterns38 64% parsing errors10 17% lemmatization errors7 12% extraction pattern errors3 5% omitted for lack of support1 2% pos tagging error104 100%Table 3: Primary causes of recall errors made by the lex-ical pattern N such as N (top) and the best performingcorresponding dependency pattern (bottom).dependency parser.4.3 Error analysisDespite the lack of performance differences betweenthe two preprocessing methods, there are still inter-nal differences which cause one method to generatedifferent related word pairs than the other.
We willnow examine in detail two extraction patterns andspecify their distinct effects on the output results.We hope that by carefully examining their output wecan learn about the strengths and weaknesses of thetwo approaches.We take a closer look at extraction pattern N suchas N for Newspaper data (second best for lexical pat-terns and fifth best for dependency patterns, see Ta-ble 2).
The lexical pattern found 222 related wordpairs while the dependency pattern discovered 199.118 of these pairs were found by both patterns whichmeans that the lexical pattern missed 81 of the pairswhile the dependency pattern missed 104.An overview of the cause of the recall errors canbe found in Table 3.
The two extraction patternsdo not overlap completely.
The dependency parserignored punctuation signs and therefore the depen-dency pattern covers both phrases with and withoutpunctuation.
However, these phrase variants resultin different lexical patterns.
This is the cause for56 hypernyms being missed by the lexical pattern.180Meanwhile there is a difference between a depen-dency pattern without the conjunction and and onewith the conjunction, while there is a unified lexi-cal pattern processing both phrases with and withoutconjunctions.
This caused the dependency pattern tomiss 45 hypernyms.
However, all of these ?missed?hypernyms are handled by other patterns.The main cause of the recall differences betweenthe two extraction patterns was the parser.
The de-pendency pattern found twelve hypernyms whichthe lexical pattern missed because they required ananalysis which was beyond part-of-speech taggingand the basic noun phrase identifier used by the lex-ical preprocessor.
Six hypernyms required extend-ing a noun phrase with a prepositional phrase, fiveneeded noun phrase extension with a relative clauseand one involved appositions.
An example of sucha phrase is illnesses caused by vitamin deficits, likescurvy and beriberi.However, the syntactic information that was avail-able to the dependency pattern did also have a neg-ative effect on its recall.
38 of the hypernyms de-tected by the lexical pattern were missed by the de-pendency pattern because there was a parsing errorin the relevant phrase.
In more than half of the cases,this involved attaching the phrase starting with suchas at an incorrect position.
We found that a phraselike N1 such as N2 , N3 and N4 could have been splitat any position.
We even found some cases of prepo-sitional phrases and relative clauses incorrectly be-ing moved from other positions in the sentence intothe target phrase.Other recall error causes appear less frequently.The two preprocessing methods used differentlemmatization algorithms which also made differenterrors.
The effects of this were visible in the errorsmade by the two patterns.
Some hypernyms thatwere found by both patterns but were not presentin both results because of insufficient support fromother patterns (candidate hypernyms should be sup-ported by at least five different patterns).
The ef-fect of errors in part-of-speech tags was small.
Ourdata analysis also revealed some inconsistencies inthe extraction patterns which should be examined.5 Concluding remarksWe have evaluated the effects of two different pre-processing methods for a natural language process-ing task: automatically identifying hypernymy in-formation.
The first method used lexical patternsand relied on shallow processing techniques likepart-of-speech tagging and lemmatization.
The sec-ond method used dependency patterns which re-lied on additional information obtained from depen-dency parsing.In earlier work, McCarthy et al (2007) foundthat for word sense disambiguation using the-sauri generated from dependency relations performonly slightly better than thesauri generated fromproximity-based relations.
Jijkoun et al (2004)showed that information obtained from dependencypatterns significantly improved the performance of aquestion answering system.
Li and Roth (2001) re-port that preprocessing by shallow parsing allows fora more accurate post-processing of ill-formed sen-tences than preprocessing with full parsing.Our study supports the findings of McCarthy etal.
(2007).
We found only minor differences in per-formances between the two preprocessing methods.The most important difference: about 20% extrapositive cases that were identified by the dependencypatterns applied to Wikipedia data, can be overcomeby increasing the data set of the lexical patterns byhalf.
We believe that obtaining more data may oftenbe easier than dealing with the extra computing timerequired for parsing the data.
For example, in thecourse of writing this paper, we had to refrain fromusing a recent version of Wikipedia because pars-ing the data would have taken 296 days on a singleprocessor machine compared with a single hour fortagging the data.ReferencesEugene Charniak.
1999.
A maximum-entropy inspiredparser.
Technical Report CS-99-12, Brown University.Frank Van Eynde.
2005.
Part of Speech Tagging en Lem-matisering van het Corpus Gesproken Nederlands.K.U.
Leuven.
(in Dutch).Alexander Genkin, David D. Lewis, and David Madigan.2004.
Large-Scale Bayesian Logistic Regression forText Categorization.
Technical report, Rutgers Uni-versity, New Jersey.181Marti A. Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proceedings ofACL-92.
Newark, Delaware, USA.Katja Hofmann and Erik Tjong Kim Sang.
2007.
Au-tomatic extension of non-english wordnets.
In Pro-ceedings of SIGIR?07.
Amsterdam, The Netherlands(poster).Valentin Jijkoun, Maarten de Rijke, and Jori Mur.
2004.Information extraction for question answering: Im-proving recall through syntactic patterns.
In Proceed-ings of Coling?04.
Geneva, Switzerland.Xin Li and Dan Roth.
2001.
Exploring evidence for shal-low parsing.Diana McCarthy, Rob Koeling, Julie Weeds, and JohnCaroll.
2007.
Unsupervised acquisition of predomi-nant word senses.
Computational Linguistics, 33(4).Patrick Pantel, Deepak Ravichandran, and Eduard Hovy.2004.
Towards terascale knowledge acquisition.In Proceedings of COLING 2004, pages 771?777.Geneva, Switzerland.Rion Snow, Daniel Jurafsky, and Andrew Y. Ng.
2005.Learning syntactic patterns for automatic hypernymdiscovery.
In NIPS 2005.
Vancouver, Canada.Rion Snow, Daniel Jurafsky, and Andrew Y. Ng.
2006.Semantic taxonomy induction from heterogenous evi-dence.
In Proceedings of COLING/ACL 2006.
Sydney,Australia.Asuka Sumida and Kentaro Torisawa.
2008.
Hackingwikipedia for hyponymy relation acquisition.
In Pro-ceedings of IJCNLP 2008.
Hyderabad, India.Mihai Surdeanu, Richard Johansson, Llu?
?s Ma`rquez,Adam Meyers, and Joakim Nivre.
2008.
The conll-2008 shared task on joint learning of syntactic and se-mantic dependencies.
In Proceedings of CoNLL-2008.Manchester, UK.Erik Tjong Kim Sang and Katja Hofmann.
2007.
Au-tomatic extraction of dutch hypernym-hyponym pairs.In Proceedings of CLIN-2006.
Leuven, Belgium.Erik Tjong Kim Sang.
2009.
To use a treebank or not ?which is better for hypernym extraction.
In Proceed-ings of the Seventh International Workshop on Tree-banks and Linguistic Theories (TLT 7).
Groningen,The Netherlands.Gertjan Van Noord.
2006.
At last parsing is now oper-ational.
In Piet Mertens, Cedrick Fairon, Anne Dis-ter, and Patrick Watrin, editors, TALN06.
Verbum ExMachina.
Actes de la 13e conference sur le traitementautomatique des langues naturelles.Gertjan Van Noord.
2009.
Huge parsed corpora in lassy.In Proceedings of TLT7.
LOT, Groningen, The Nether-lands.Piek Vossen.
1998.
EuroWordNet: A MultilingualDatabase with Lexical Semantic Networks.
KluwerAcademic Publisher.182
