Approximate Generationfrom Non-Hierarchical RepresentationsNicolas Nicolov,* Chris Mellish, Graeme RitchieDept of AI, Univ.
of Edinburgh80 South Bridge, Edinburgh EH1 1HN{nicolas, chrism, graeme}@aisb.ed.ac.ukAbst rac tThis paper presents a technique for sentence genera-tion.
We argue that the input to generators shouldhave a non-hierarchical nature.
This allows us to in-vestigate a more general version of the sentence gen-eration problem where one is not pre-committed to achoice of the syntactically prominent elements in theinitial semantics.
We also consider that a generatorcan happen to convey more (or less) information thanis originally specified in its semantic input.
In orderto constrain this approximate matching of the inputwe impose additional restrictions on the semantics ofthe generated sentence.
Our technique provides flex-ibility to address cases where the entire input cannotbe precisely expressed in a single sentence.
Thus thegenerator does not rely on the strategic componenthaving linguistic knowledge.
We show clearly howthe semantic structure is declaratively related to lin-guistically motivated syntactic representation.1 In t roduct ionNatural language generation is the process ofrealising communicative intentions as text (orspeech).
The generation task is standardlybroken down into the following processes: con-tent determination (what is the meaning tobe conveyed), sentence planning 1 (chunkingthe meaning into sentence sized units, choos-ing words), surface realisation (determining thesyntactic structure), morphology (inflection ofwords), synthesising speech or formatting thetext output.In this paper we address aspects of sentenceplanning (how content words are chosen but nothow the sem.untics i chunked in units realisable"Supported by Faculty of Science and EngineeringScholarship 343 EE06006 at the University of Edinburgh.1Note that this does not involve planning mechanisms!as sentences) and surface realisation (how syn-tactic structures are computed).
We thus dis-cuss what in the literature is sometimes referredto as tactical generation, that is "how to sayit"--as opposed to strategic generation--"whatto say".
We look at ways of realising a non-hierarchical semantic representation as a sen-tence, and explore the interactions between syn-tax and semantics.Before giving a more detailed descriptionof our proposals first we motivate the non-hierarchical nature of the input for sentencegenerators and review some approaches to gen-eration from non-hierarchical representations--semantic networks (Section 2).
We proceedwith some background about the grammaticalframework we will employ--D-Tree Grammars(Section 3) and after describing the knowledgesources available to the generator (Section 4)we present the generation algorithm (Section 5).This is followed by a step by step illustrationof the generation of one sentence (Section 6).We then discuss further semantic aspects of thegeneration (Section 7) and the implementation(Section 8).
We conclude with a discussion ofsome issues related to the proposed technique(Section 9).2 Generat ion  from Non-Hierarchical  Representat ionsThe input for generation systems varies radic-ally from system to system.
Many generatorsexpect heir input to be cast in a tree-like nota-tion which enables the actual systems to assumethat nodes higher in the semantic structure aremore prominent than lower nodes.
The semanticrepresentations u ed are variations of a predic-ate with its arguments.
The predicate is real-ised as the main verb of the sentence and the31arguments are realised as complements of themain verb--thus the control information is toa large extent encoded in the tree-like semanticstructure.
Unfortunately, such dominance rela-tionships between odes in the semantics oftenstem from language considerations and are notalways preserved across languages.
Moreover,if the semantic input comes from other applic-ations, it is hard for these applications to de-termine the most prominent concepts becauselinguistic knowledge is crucial for this task.
Thetree-like semantics assumption leads to simplific-ations which reduce the paraphrasing power ofthe generator (especially in the context of mul-tilingual generation).
2 In contrast, the use of anon-hierarchical representation for the underly-ing semantics allows the input to contain as fewlanguage commitments a possible and makes itpossible to address the generation strategy froman unbiased position.
We have chosen a partic-ular type of a non-hierarchical knowledge rep-resentation formalism, conceptual graphs \[24\],to represent the input to our generator.
Thishas the added advantage that the representa-tion has well defined deductive mechanisms.
Agraph is a set of concepts connected with rela-tions.
The types of the concepts and the rela-tions form generalisation lattices which also helpdefine a subsumption relation between graphs.Graphs can also be embedded within one an-other.
The counterpart of the unification op-eration for conceptual graphs is maximal join(which is non-deterministic).
Figure 1 shows asimple conceptual graph which does not havecycles.
The arrows of the conceptual relationsindicate the domain and range of the relationand do not impose a dominance relationship.Figure 1: A simple conceptual graphThe use of semantic networks in generationis not new \[21, 18\].
Two main approaches havebeen employed for generation from semantic net-works: utterance path traversal and incremental2The tree-like semantics imposes ome restrictionswhich the language may not support.consumption.
An utterance path is the sequenceof nodes and arcs that are traversed in the pro-cess of mapping a graph to a sentence.
Gener-ation is performed by finding a cyclic path inthe graph which visits each node at least once.If a node is visited more than once, grammarrules determine when and how much of its con-tent will be uttered \[23\].
Under the second ap-proach, that of incremental consumption, gen-eration is done by gradually relating (consum-ing) pieces of the input semantics to linguisticstructure \[3, 13\].
Such covering of the semanticstructure avoids some of the limitations of theutterance path approach and is also the generalmechanism we have adopted (we do not rely onthe directionality of the conceptual relations perse--the primitive operation that we use whenconsuming pieces of the input semantics i max-imal join which is akin to pattern matching).The borderline between the two paradigms isnot clear-cut.
Some researchers \[22\] are look-ing at finding an appropriate sequence of ex-pansions of concepts and reductions of subpartsof the semantic network until all concepts haverealisations in the language.
Others assume allconcepts are expressible and try to substitutesyntactic relations for conceptual relations \[2\].Other work addressing surface realisationfrom semantic networks includes: generation us-ing Meaning-Text Theory \[6\], generation usingthe SNePS representation formalism \[19\], gener-ation from conceptual dependency graphs \[26\].Among those that have looked at generationwith conceptual graphs are: generation usingLexical Conceptual Grammar \[15\], and gener-ating from CGs using categorial grammar in thedomain of technical documentation \[25\].This work improves on existing eneration ap-proaches in the following respects: (i) Unlikethe majority of generators this one takes a non-hierarchical (logically well defined) semantic rep-resentation as its input.
This allows us to lookat a more general version of the realisation prob-lem which in turn has direct ramifications forthe increased paraphrasing power and usabilityof the generator; (ii) Following Nogier & Zock\[14\], we take the view that lexical choice is es-sentially (pattern) matching, but unlike them weassume that the meaning representation may notbe entirely consumed at the end of the gener-ation process.
Our generator uses a notion ofapproximate matching and can happen to con-32vey more (or less) information than is origin-ally specified in its semantic input.
We have aprincipled way to constrain this.
We build thecorresponding semantics of the generated sen-tence and aim for it to be as close as possibleto the input semantics.
(i) and (ii) thus allowfor the input to come from a module that neednot have linguistic knowledge.
(iii) We showhow the semantics i systematically related tosyntactic structures in a declarative framework.Alternative processing strategies using the sameknowledge sources can therefore be envisaged.3 D-Tree GrammarsOur generator uses a particular syntactictheory--D-Tree Grammar (DIG) which webriefly introduce because the generation strategyis influenced by the linguistic structures and theoperations on them.D-Tree Grammar (DTG) [16] is a new gram-mar formalism which arises from work on Tree-Adjoining Grammars (TAG) [7].
In the contextof generation, TAGS have been used in a num-ber of systems MUMBLE [10], SPOKESMAN [11],Wm [27], the system reported in [9], the firstversion of PROTECTOR [12], and recently SPUD(by Stone & Doran).
In the area of grammardevelopment TAG has been the basis of one ofthe largest grammars developed for English [4].Unlike TAGs, DTGs provide a uniform treatmentof complementation a d modification at the syn-tactic level.
DTGs are seen as attractive for gen-eration because a close match between semanticand syntactic operations leads to simplificationsin the overall generation architecture.
DTGS tryto overcome the problems associated with TAGSwhile remaining faithful to what is seen as thekey advantages of TAGs [7]: the extended omainof locality over which syntactic dependencies arestated and function argument structure is cap-tured.DTG assumes the xistence of elementarystructures and uses two operations to form lar-ger structures from smaller ones.
The element-ary structures are tree descriptions 3 which aretrees in which nodes are linked with two types oflinks: domination links (d-links) and immediatedomination links (i-links) expressing (reflexive)domination and immediate domination relations3called d-trees hence the name of the formalism.between odes.
Graphically we will use a dashedline to indicate a d-link (see Figure 2).
D-treesallow us to view the operations for composingtrees as monotonic.
The two combination oper-ations that DTG uses are subsertion and sister-adjunetion.substitution nodeFigure 2: SubsertionSubser t ion .
When a d-tree a is subsertedinto another d-tree fl, a component 4 of a is sub-stituted at a frontier nonterminal node (a sub-stitution node) of j3 and all components of athat are above the substituted component areinserted into d-links above the substituted nodeor placed above the root node of ft.
It is pos-sible for components above the substituted nodeto drift arbitrarily far up the d-tree and distrib-ute themselves within domination links, or abovethe root, in any way that is compatible with thedomination relationships present in the substi-tuted d-tree.
In order to constrain the way inwhich the non-substituted components can beinterspersed DTG uses subsertion-insertion con-straints which explicitly specify what compon-ents from what trees can appear within a certaind-links.
Subsertion as it is defined as a non-deterministic operation.
Subsertion can modelboth adjunction and substitution in TAG .Figure 3: Sister-adjunctionS i s ter -ad junct ion .
When a d-tree a issister-adjoined at a node ~7 in a d-tree/3 the corn-4& subtree which contains only i-links.33posed d-tree 7 results from the addition to j3 ofv~ as a new leftmost or rightmost sub-d-tree be-low 7/.
Sister-adjunction i volves the addition ofexactly one new immediate domination link.
Inaddition several sister-adjunctions can occur atthe same node.
Sister-adjoining constraints as-sociated with nodes in the d-trees specify whichother d-trees can be sister-adjoined at this nodeand whether they will be right- or left-sister-adjoined.For more details on DTGS see \[16\].4 Knowledge SourcesThe generator assumes it is given as inputan input semantics (InputSem) and 'bound-ary' constraints for the semantics of the gen-erated sentence (BuiltSem which in generalis different from InputSemh).
The bound-ary constraints are two graphs (UpperSemand LowerSem) which convey the notion ofthe least and the most that should be ex-pressed.
So we want BuiltSem to satisfy:LowerSern < BuiltSem <_ UpperSern.
?
Ifthe generator happens to introduce more se-mantic information by choosing a particular ex-pression, LowerSem is the place where such ad-ditions can be checked for consistency.
Suchconstraints on BuiltSem are useful because ingeneral InputSem and BuiltSem can happento be incomparable (neither one subsumes theother).
In a practical scenario LowerSem can bethe knowledge base to which the generator hasaccess minus any contentious bits.
UpperSemcan be the minimum information that necessar-ily has to be conveyed in order for the generatorto achieve the initial communicative intentions.The goal of the generator is to produce a sen-tence whose corresponding semantics is as closeas possible to the input semantics, i.e., the real-isation adds as little as possible extra materialand misses as little as possible of the original in-put.
In generation similar constraints have beenused in the generation of referring expressionswhere the expressions should not be too general5This can come about from a mismatch between theinput and the semantic structures expressible by thegenerator.6The notation G1 <_ G2 means that G1 is subsumedby G2.
We consider UpperSem to be a generalisation ofBuiltSem and LowerSem a specialisation of BuiltSem(in terms of the conceptual graphs that represent them).so that discriminatory power is not lost and nottoo specific so that the referring expression is ina sense minimal.
Our model is a generalisationof the paradigm presented in \[17\] where issuesof mismatch in lexical choice are discussed.
Wereturn to how UpperSem and LowerSem areactually used in Section 7.4.1 Mapp ing  ru lesMapping rules state how the semantics is relatedto the syntactic representation.
We do not im-pose any intrinsic directionality on the mappingrules and view them as declarative statements.In our generator a mapping rule is representedas a d-tree in which certain nodes are annot-ated with semantic information.
Mapping rulesare a mixed syntactic-semantic representation.The nodes in the syntactic structure will be fea-ture structures and we use unification to com-bine two syntactic nodes.
The semantic annota-tions of the syntactic nodes are either conceptualgraphs or instructions indicating how to com-pute the semantics of the syntactic node fromthe semantics of the daughter syntactic nodes.Graphically we use dotted lines to show thecoreference between graphs (or concepts).
Eachgraph appearing in the rule has a single node("the semantic head") which acts as a root (in-dicated by an arrow in Figure 4).
This hierarch-ical structure is imposed by the rule, and is notpart of the semantic input.
Every mapping rulehas associated applicability semantics which isused to license its application.
The applicabil-ity semantics can be viewed as an evaluation ofthe semantic instruction associated with the topsyntactic node in the tree description.Figure 4 shows an example of a mapping rule.The applicability semantics of this mapping ruleis: I AN'MATE ACT,ONIf this structure matches part of the input se-mantics (we explain more precisely what wemean by matching later on) then this rule canbe triggered (if it is syntactically appropriate---see Section 5).
The internal generation goals(shaded areas) express the following: (1) gen-erate \[ACTION\[ as a verb and subsert (substi-tute,attach) the verb's syntactic structure at theVo node; (2) generate \[ANIMATE\] as a nounphrase and subsert the newly built structureat NPO; and (3) generate I EI~ITITY\[ aS anothernoun phrase and subsert the newly built struc-34Applicability semantics:Internal generation goalsFigure 4: A mapping rule for transitive constructionsture at NP1.
The newly built structures are alsomixed syntactic-semantic representations (an-notated d-trees) and they are incorporated inthe mixed structure corresponding to the cur-rent status of the generated sentence.5 Sentence  Generat ionIn this section we informally describe the gener-ation algorithm.
In Figure 5 and later in Fig-ure 8, which illustrate some semantic aspects ofthe processing, we use a diagrammatic notationto describe semantic structures which are actu-ally encoded using conceptual graphs.The input to the generator is InputSem,LowerSem, UpperSem and a mixed structure,Partial, which contains asyntactic part (usuallyjust one node but possibly something more com-plex) and a semantic part which takes the formof semantic annotations on the syntactic nodesin the syntactic part.
Initially Partial rep-resents the syntactic-semantic correspondenceswhich are imposed on the generator.
7 It has theformat of a mixed structure like the represent-ation used to express mapping rules (Figure 4).Later during the generation Partial is enrichedand at any stage of processing it represents hecurrent syntactic-semantic correspondences.We have augmented the DTG formalism so7In dialogue and question answering, for example,the syntactic form of the generated sentence may beconstrained.that the semantic structures associated withsyntactic nodes will be updated appropriatelyduring the subsertion and sister-adjunction p-erations.
The stages of generation are: (1) build-ing an initial skeletal structure; (2) attemptingto consume as much as possible of the semanticsuncovered in the previous tage; and (3) convert-ing the partial syntactic structure into a com-plete syntactic tree.5.1 Bu i ld ing  a ske leta l  s t ruc tureGeneration starts by first trying to find a map-ping rule whose semantic structure matches spart of the initial graph and whose syntacticstructure iscompatible with the goal syntax (thesyntactic part of Partial).
If the initial goalhas a more elaborate syntactic structure and re-quires parts of the semantics to be expressed ascertain syntactic structures this has to be re-spected by the mapping rule.
Such an initialmapping rule will have a syntactic structure thatwill provide the skeleton syntax for the sentence.If Lexicalised DTGiS used as the base syntacticformalism at this stage the mapping rule willintroduce the head of the sentence structurethe main verb.
If the rule has internal gener-ation goals then these are explored recursively(possibly via an agenda--we will ignore here theSvia the maximal join operation.
Also note thatthe arcs to/from the conceptual relations do not reflectany directionality of the processing--they can be 'tra-versed'/accessed from any of the nodes they connect.35LOWE"  .L.O N ........... ...............................................?
= .
.  "
A,N,NG .... j SEMANT, CSINITIAL ~ ".~'.~ EXTRAS .
~' .......... ~ "SEMANTICS.0F NEW MAPP,NG RULEFigure 5: Covering the remaining semantics with mapping rulesissue of the order in which internal generationgoals are executed).
Because of the minimalityof the mapping rule, the syntactic structure thatis produced by this initial stage is very basic--forex:mple only obligatory complements are con-sidered.
Any mapping rule can introduce addi-tional semantics and such additions are checkedagainst the lower semantic bound.
When ap-plying a mapping rule the generator keeps trackof how much of the initial semantic structurehas been covered/consumed.
Thus at the pointwhen all internal generation goals of the first(skeletal) mapping rule have been exhausted thegenerator knows how much of the initial graphremains to be expressed.5.2 Cover ing  the  remain ing  semant icsIn the second stage the generator aims to findmapping rules in order to cover most of the re-maining semantics (see Figure 5) .
The choiceof mapping rules is influenced by the followingcriteria:Connect iv i ty :  The semantics of the mappingrule has to match (cover) part of the coveredsemantics and part of the remaining se-mantics.In tegrat ion :  It should be possible to incor-porate the semantics of the mapping ruleinto the semantics of the current structurebeing built by the generator.Real isabi l i ty:  It should be possible to incor-porate the partial syntactic structure ofthe mapping rule into the current syntacticstructure being built by the generator.Note that the connectivity condition restrictsthe choice of mapping rules so that a rule thatmatches part of the remaining semantics andthe extra semantics added by previous mappingrules cannot be chosen (e.g., the "bad mapping"in Figure 5).
While in the stage of fleshing outthe skeleton sentence structure (Section 5.1) thesyntactic integration involves subsertion, in thestage of covering the remaining semantics it issister-adjunction that is used.
When incorporat-ing semantic structures the semantic head has tobe preserved--for example when sister-adjoiningthe d-tree for an adverbial construction the se-mantic head of the top syntactic node has tobe the same as the semantic head of the nodeat which sister-adjunction is done.
This explicitmarking of the semantic head concepts differsfrom \[20\] where the semantic head is a PROLOGterm with exactly the same structure as the in-put semantics.5.3 Complet ing  a der ivat ionIn the preceding stages of building the skeletalsentence structure and covering the remainingsemantics, the generator is mainly concernedwith consuming the initial semantic structure.In those processes, parts of the semantics aremapped onto partial syntactic structures whichare integrated and the result is still a partialsyntactic structure.
That is why a final stepof "closing off" the derivation is needed.
Thegenerator tries to convert the partial syntacticstructure into a complete syntactic tree.
A mor-phological post-processor reads the leaves of thefinal syntactic tree and inflects the words.6 ExampleIn this section we illustrate how the algorithmworks by means of a simple example.
Suppose36Inte al ge eratlon g v ~i i .
A P ~.
\I.IMPFigure 6: Mapping ruleswe start with an initial semantics as given inFigure 1.
This semantics can be expressed in anumber of ways: Fred limped quickly, Fred hur-ried with a limp, Fred's limping was quick, Thequickness of Fred's limping .
.
.
,  etc.
Here weshow how the first paraphrase is generated.In the stage of building the skeletal structurethe mapping rule (i) in Figure 6 is used.
Itsinternal generation goals are to realise the in-stantiation of \[ ACTION \] (which is \[ MOVEMENTas a verb and similarly\[ PERSON:FRED f as a nounphrase.
The generation of the subject nounphrase is not discussed here.
The main verbis generated using the terminal mapping rule 9(iii) in Figure 6. l?
The skeletal structure thusgenerated is Fred limp(ed).
(see (i) in Figure 7).An interesting point is that although the in-ternal generation goal for the verb referred onlyto the concept \[MOVEMENT\] in the initial se-mantics, all of the information suggested by theterminal mapping rule (iii) in Figure 6 is con-sumed.
We will say more about how this is donein Section 7.At this stage the only concept hat remainsto be consumed is \ [~K~.
This is done in thestage of covering the remaining semantics whenthe mapping rule (ii) is used.
This rule has aninternal generation goal to generate the instan-t ia t ion  Of\[MANNER\] as an adverb, which yieldsquickly.
The structure suggested by this rulehas to be integrated in the skeletal structure.
?Terminal mapping rules are mapping rules whichhave no internal generation goals and in which all ter-minal nodes of the syntactic structure are labelled withterminal symbols (lexemes).1?In Lexicalised DTGS the main verbs would be alreadypresent in the initial trees.On the syntactic side this is done using sister-adjunction.
The final mixed syntactic-semanticstructure is shown on the right in Figure 7.
Inthe syntactic part of this structure we have nodomination links.
Also all of the input semanticshas been consumed.
The semantic annotationsof the S and VP nodes are instructions abouthow the graphs/concepts of their daughters areto be combined.
If we evaluate in a bottom upfashion the semantics of the S node, we willget the same result as the input semantics inFigure 1.
After morphological post-processingthe result is Fred limped quickly.
An alternativeparaphrase like Fred hurried with a limp ll canbe generated using a lexical mapping rule forthe verb hurry which groups IMOVEMENTI and\ [ ~  together and a another mapping rule ex-pressing \[LIMPING\] as a PP.
To get both para-phrases would be hard for generators elying onhierarchical representations.7 Matching the applicabilitysemantics of mapping rulesMatching of the applicability semantics of map-ping rules against other semantic structures oc-curs in the following cases: when looking fora skeletal structure; when exploring an internalgeneration goal; and when looking for mappingrules in the phase of covering the remaining se-mantics.
During the exploration of internal gen-eration goals the applicability semantics of amapping rule is matched against he semanticsof an internal generation goal.
We assume that11 Our example is based on Iordanskaja e~ al.
's notion ofmaximal reductions of a semantic net (see \[6, page 300\]).It is also similar to the example in \[14\].37LIMPI PVo I ' I~V~T M~_~ U~P~ LIMPFigure 7: Skeletal structure and final structureIN LOWER SEM.
BOUNDSEMANTICS OF THE I ~ .~ -, ,,~.~'~.~GENERATION GOAL ~ ~"'~'~"'~ I X\APPLICABILITY SEMANTICSOF NEW MAPPING RULE INITIAL GRAFigure 8: Interactions involving the applicability semantics of a mapping rulethe following conditions hold:1.
The applicability semantics of the mapping rulecan be maximally joined with the goal se-mantics.2.
Any information introduced by the mappingrule that is more specialised than the goal se-mantics (additional concepts/relations, furthertype instantiation, etc.)
must be within thelower semantic bound (LowerSem).
If thisadditional information is within the input se-mantics, then information can propagate fromthe input semantics to the mapping rule (theshaded area 2 in Figure 8).
If the mapping rule'ssemantic additions are merely in LowerSem,then information cannot flow from LowerSemto the mapping rule (area 1 in Figure 8).Similar conditions hold when in the phase of cov-ering the remaining ~emantics the applicabilitysemantics of a mapping rule is matched againstthe initial semantics.
This way of matching al-lows the generator to convey only the informa-tion in the original semantics and what the lan-guage forces one to convey even though more in-formation might be known about the particularsituation.In the same spirit after the generator hasconsumed/expressed a concept in the input se-mantics the system checks that the lexical se-mantics of the generated word is more specificthan the corresponding concept (if there is one)in the upper semantic bound.8 ImplementationWe have developed a sentence generatorcalled PROTECTOR (approximate PROduct ion ofTExts from Conceptual graphs in a declaraT-ive framewORk).
PROTECTOR is implementedin LIFE \[1\].
The syntactic overage of the gener-ator is influenced by the XTAG system (the firstversion of PROTECTOR in fact used TAGS).
Byusing DTGs we can use most of the analysis ofXTAG while the generation algorithm is simpler.W~ are in a position to express subparts of theinput semantics as different syntactic categoriesas appropriate for the current generation goxl(e.g., VPs and nominalisations).
The syntactic38coverage of PROTECTOR includes: intransitive,transitive, and ditransitive verbs, topicalisation,verb particles, passive, sentential complements,control constructions, relative clauses, nominal-isations and a variety of idioms.
On backtrack-ing PROTECTOR returns all solutions.
We arealso looking at the advantages that our approachoffers for multilingual generation.9 D iscuss ionDuring generation it is necessary to find appro-priate mapping rules.
However, at each stagea number of rules might be applicable.
Due topossible interactions between some rules the gen-erator may have to explore different choices be-fore actually being able to produce a sentence.Thus, generation is in essence a search problem.In order to guide the search a number of heurist-ics can be used.
In \[14\] the number of matchingnodes has been used to rate different matches,which is similar to finding maximal reductions in\[6\].
Alternatively a notion of semantic distance\[5\] might be employed.
In PROTECTOR we willuse a much more sophisticated notion of whatit is for a conceptual graph to match better theinitial semantics than another graph.
This cap-tures the intuition that the generator should tryto express as much as possible from the inputwhile adding as little as possible extra material.We use instructions showing how the se-mantics of a mother syntactic node is computedbecause we want to be able to correctly up-date the semantics of nodes higher than theplace where substitution or adjunction has takenplacc i.e., we want to be able to propagatethe substitution or adjunction semantics up themixed structure whose backbone is the syntactictree.We also use a notion of headed conceptualgraphs, i.e., graphs that have a certain nodechosen as the semantic head.
The initial se-mantics need not be marked for its semantichead.
This allows the generator to choose anappropriate (for the natural anguage) perspect-ive.
The notion of semantic head and their con-nectivity is a way to introduce a hierarchicalview on the :emantic structure which is depend-ent on the language.
When matching two con-ceptual graphs we require that their heads be thesame.
This reduces the search space and speedsup the generation process.Our generator is not coherent or complete(i.e., it can produce sentences with moregeneral/specific semanticJ than the input se-mantics).
We try to generate sentences whosesemantics is as close as possible to the input inthe sense that they introduce little extra mater-ial and leave uncovered a small part of the inputsemantics.
We keep track of more structures asthe generation proceeds and are in a positionto make finer distinctions than was done in pre-vious research.
The generator never producessentences with semantics which is more specificthan the lower semantic bound which gives somedegree of coherence.
Our generation techniqueprovides flexibility to address cases where theentire input cannot be expressed in a single sen-tence by first generating a "best match" sentenceand allowing the remaining semantics to be gen-erated in a follow-up sentence.Our approach can be seen as a generalisa-tion of semantic head-driven generation \[20\]--we deal with a non-hierarchical input and non-concatenative grammars.
The use of LexicalisedDTG means that the algorithm in effect looksfirst for a syntactic head.
This aspect is similarto syntax-driven generation \[8\].The algorithm has to be checked against morelinguistic data and we intend to do more work onadditional control mechanisms and also using al-ternative generation strategies using knowledgesources free from control information.
To thisend we have explored aspects of a new semantic-indexed chart generation which also allows us torate intermediate results using syntactic as wellas semantic preferences.
Syntactic/stylistic pref-erences are helpful in cases where the semanticsof two paraphrases are the same.
One such in-stance of use of syntactic preferences is avoid-ing (giving lower rating to) heavy constituentsin split verb particle constructions.
Thus, thegenerator finds all possible solutions producingthe "best" first.10 Conc lus ionWe have presented a technique for sentencegeneration from conceptual graphs.
The useof a non-hierarchical representation for the se-mantics and approximate semantic matching in-creases the paraphrasing power of the generator39and enables the production of sentences withradically different syntactic structure due to al-ternative ways of grouping concepts into words.This is particularly useful for multilingual gen-eration and in practical generators which are fedinput from non linguistic applications.
The useof a syntactic theory (D-Tree Grammars) allowsfor the production of linguistically motivatedsyntactic structures which will pay off in termsof better coverage of the language and overallmaintainability of the generator.
The syntactictheory also affects the processing--we have aug-mented the syntactic operations to account forthe integration of the semantics.
The generationarchitecture makes explicit the decisions thathave to be taken and allows for experiments withdifferent generation strategies using the same de-clarative knowledge sources.References\[1\] H. A\[t-Kaci and A. Podelski.
Towards a meaning ofLIFE.
Journal of Logic Programming, 16(3&4):195-234, 1993.\[2\] F. Antonacci et al Analysis and Generation ofItalian Sentences.
In T. Nagle, J. Nagle, L. Ger-holz, and P. Eklund, editors, Conceptual Structures:Current research and Practice, pages 437-460.
EllisHorwood, 1992.\[3\] M. Boyer and G. Lapalme.
Generating paraphrasesfrom meaning-text semantic networks.
Computa-tional Intelligence, 1(1):103-117, 1985.\[4\] C. Doran et al XTAG--A Wide Coverage Grammarfor English.
In COLING'94, pages 922-928, 1994.\[5\] N. Foo et al Semantic distance in conceptualgraphs.
In J. Nagle and T. Nagle, editors, FourthAnnual Workshop on Conceptual Structures, 1989.\[6\] L. Iordanskaja et al Lexical Selection and Para-phrase in a Meaning-Text Generation Model.
InC.Paris, W.Swartout, and W.Mann, editors, NaturalLanguage Generation in Artificial Intelligence andComputational Linguistics, pages 293-312.
KluwerAcademic, 1991.\[7\] A. Joshi.
The Relevance of Tree Adjoining Gram-mar to Generation.
In G. Kempen, editor, NaturalLanguage Generation, pages 233-252.
Kluwer Aca-demic, 1987.\[8\] E. KSnig.
Syntactic head-driven generation.
InCOLING'94, 475-481, Kyoto, 1994.\[9\] K. F. McCoy, K. Vijay-Shanker, and G. Yang.
Afunctional approach to generation with tag.
In 30thAnnual Meeting of ACL, pages 48-55, 1992.\[10\] D. McDonald and J. Pustejovsky.
TAGs as a gram-matical formalism for generation.
In 23rd AnnualMeeting of the ACL, pages 94-103, 1985.\[11\] M. Meteer.
The "Generation Gap": The Problem ofExpressibility in Text Planning.
PhD thesis, Univ ofMassachusetts, 1990.
COINS TR 90-04.\[12\] N. Nicolov, C. Mellish, and ,3.
Ritchie.
SentenceGeneration from Conceptual Graphs.
In G.Ellis,R.Levinson, W.Rich, and J.Sowa, editors, Concep-tual Structures: Applications, Implementation andTheory, pages 74-88.
LNAI 954, Springer, 1995.3rd Int.
Conf.
on Conceptual Structures (ICCS'95),Santa Cruz, CA, USA.\[13\] J.-F. Nogier.
Gdndration automatique de langage etgraphs conceptuels.
Hermes, Paris, 1991.\[14\] J.-F. Nogier and M. Zock.
Lexical Choice as Pat-tern Matching.
In T. Nagle, J. Nagle, L. Gerholz,and P. Eklund, editors, Conceptual Structures: Cur-rent research and Practice, pages 413-436.
Ellis Hor-wood, 1992.\[15\] J. Oh et al NLP: Natural Language Parsers andGenerators.
In 1st Int.
Workshop on PEIRCE: AConceptual Graph Workbench, pages 48-55, 1992.\[16\] O. Rainbow, K. Vijay-Shanker, and D. Weir.
D-treegrammars.
In ACL, 1995.\[17\] E. Reiter.
A new model of lexical choice fornouns.
Computational Intelligence, 7(4):240-251,1991.
Special issue on Natural Language Genera-tion.\[18\] S. Shapiro.
Generalized augmented transition net-work grammars for generation from semantic net-works.
Computational Linguistics, 2(8):12-25, 1982.\[19\] S. Shapiro.
The CASSIE projects: An approach toNL Competence.
In 4th Portugese Conference on AI(EPIA-89).
LNAI 390: 362-380, Springer, 1989.\[20\] S. Shieber, G. van Noord, R. Moore, and F. Pereira.A semantic head-driven generation algorithm forunification-based formalisms.
Computational Lin-guistics, 16(1):30-42, 1990.\[21\] R. Simmons and J. Slocum.
Generating Eng-lish Discourse from Semantic Networks.
CACM,15(10):891-905, 1972.\[22\] M. Smith, R. Garigliano, and R. Morgan.
Gener-ation in the LOLITA system: an engineering ap-proach.
In 7th Int.
Workshop on Natural LanguageGeneration, pages 241-244, 1994.\[23\] J. Sown.
Conceptual Structures: Information Pro-cessing in Mind and Machine.
Addison-Wesley,1984.\[24\] J. F. Sowa.
Conceptual graphs summary.
In T. E.Nagle et al, editors, Conceptual Structures: CurrentResearch and Practice, pages 3-51.
Ellis Horwood,1992.\[25\] S. Svenberg.
Representing Conceptual and Lin-guistic Knowledge for Multilingual Generation in aTechnical Domain.
In 7th Int.
Workshop on NaturalLanguage Generation, pages 245-248, 1994.\[26\] A. van Rijn.
Natural Language Communicationbetween Man and Machine.
PhD thesis, TechnicalUniversity Delft, 1991.\[27\] W. Wahlster et al WIP: the coordinated genera-tion of multimodal presentations fro m a commonrepresentation.
RR 91-08, DFKI, 1991.40
