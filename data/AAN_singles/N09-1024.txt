Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 209?217,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsUnsupervised Morphological Segmentation with Log-Linear ModelsHoifung Poon?Dept.
of Computer Sci.
& Eng.University of WashingtonSeattle, WA 98195hoifung@cs.washington.eduColin CherryMicrosoft ResearchRedmond, WA 98052colinc@microsoft.comKristina ToutanovaMicrosoft ResearchRedmond, WA 98052kristout@microsoft.comAbstractMorphological segmentation breaks wordsinto morphemes (the basic semantic units).
Itis a key component for natural language pro-cessing systems.
Unsupervised morphologi-cal segmentation is attractive, because in ev-ery language there are virtually unlimited sup-plies of text, but very few labeled resources.However, most existing model-based systemsfor unsupervised morphological segmentationuse directed generative models, making it dif-ficult to leverage arbitrary overlapping fea-tures that are potentially helpful to learning.In this paper, we present the first log-linearmodel for unsupervised morphological seg-mentation.
Our model uses overlapping fea-tures such as morphemes and their contexts,and incorporates exponential priors inspiredby the minimum description length (MDL)principle.
We present efficient algorithmsfor learning and inference by combining con-trastive estimation with sampling.
Our sys-tem, based on monolingual features only, out-performs a state-of-the-art system by a largemargin, even when the latter uses bilingual in-formation such as phrasal alignment and pho-netic correspondence.
On the Arabic PennTreebank, our system reduces F1 error by 11%compared to Morfessor.1 IntroductionThe goal of morphological segmentation is to seg-ment words into morphemes, the basic syntac-tic/semantic units.
This is a key subtask in many?
This research was conducted during the author?s intern-ship at Microsoft Research.NLP applications, including machine translation,speech recognition and question answering.
Pastapproaches include rule-based morphological an-alyzers (Buckwalter, 2004) and supervised learn-ing (Habash and Rambow, 2005).
While successful,these require deep language expertise and a long andlaborious process in system building or labeling.Unsupervised approaches are attractive due to thethe availability of large quantities of unlabeled text,and unsupervised morphological segmentation hasbeen extensively studied for a number of languages(Brent et al, 1995; Goldsmith, 2001; Dasgupta andNg, 2007; Creutz and Lagus, 2007).
The lackof supervised labels makes it even more importantto leverage rich features and global dependencies.However, existing systems use directed generativemodels (Creutz and Lagus, 2007; Snyder and Barzi-lay, 2008b), making it difficult to extend them witharbitrary overlapping dependencies that are poten-tially helpful to segmentation.In this paper, we present the first log-linear modelfor unsupervised morphological segmentation.
Ourmodel incorporates simple priors inspired by theminimum description length (MDL) principle, aswell as overlapping features such as morphemes andtheir contexts (e.g., in Arabic, the string Al is likelya morpheme, as is any string between Al and a wordboundary).
We develop efficient learning and infer-ence algorithms using a novel combination of twoideas from previous work on unsupervised learn-ing with log-linear models: contrastive estimation(Smith and Eisner, 2005) and sampling (Poon andDomingos, 2008).We focus on inflectional morphology and test our209approach on datasets in Arabic and Hebrew.
Oursystem, using monolingual features only, outper-forms Snyder & Barzilay (2008b) by a large mar-gin, even when their system uses bilingual informa-tion such as phrasal alignment and phonetic corre-spondence.
On the Arabic Penn Treebank, our sys-tem reduces F1 error by 11% compared to Mor-fessor Categories-MAP (Creutz and Lagus, 2007).Our system can be readily applied to supervisedand semi-supervised learning.
Using a fraction ofthe labeled data, it already outperforms Snyder &Barzilay?s supervised results (2008a), which furtherdemonstrates the benefit of using a log-linear model.2 Related WorkThere is a large body of work on the unsupervisedlearning of morphology.
In addition to morpholog-ical segmentation, there has been work on unsuper-vised morpheme analysis, where one needs to deter-mine features of word forms (Kurimo et al, 2007)or identify words with the same lemma by model-ing stem changes (Schone and Jurafsky, 2001; Gold-smith, 2001).
However, we focus our review specif-ically on morphological segmentation.In the absence of labels, unsupervised learningmust incorporate a strong learning bias that reflectsprior knowledge about the task.
In morphologicalsegmentation, an often-used bias is the minimumdescription length (MDL) principle, which favorscompact representations of the lexicon and corpus(Brent et al, 1995; Goldsmith, 2001; Creutz and La-gus, 2007).
Other approaches use statistics on mor-pheme context, such as conditional entropy betweenadjacent n-grams, to identify morpheme candidates(Harris, 1955; Keshava and Pitler, 2006).
In this pa-per, we incorporate both intuitions into a simple yetpowerful model, and show that each contributes sig-nificantly to performance.Unsupervised morphological segmentation sys-tems also differ from the engineering perspective.Some adopt a pipeline approach (Schone and Ju-rafsky, 2001; Dasgupta and Ng, 2007; Demberg,2007), which works by first extracting candidateaffixes and stems, and then segmenting the wordsbased on the candidates.
Others model segmenta-tion using a joint probabilistic distribution (Goldwa-ter et al, 2006; Creutz and Lagus, 2007; Snyder andBarzilay, 2008b); they learn the model parametersfrom unlabeled data and produce the most proba-ble segmentation as the final output.
The latter ap-proach is arguably more appealing from the mod-eling standpoint and avoids error propagation alongthe pipeline.
However, most existing systems usedirected generative models; Creutz & Lagus (2007)used an HMM, while Goldwater et al (2006) andSnyder & Barzilay (2008b) used Bayesian modelsbased on Pitman-Yor or Dirichlet processes.
Thesemodels are difficult to extend with arbitrary overlap-ping features that can help improve accuracy.In this work we incorporate novel overlappingcontextual features and show that they greatly im-prove performance.
Non-overlapping contextualfeatures previously have been used in directed gen-erative models (in the form of Markov models) forunsupervised morphological segmentation (Creutzand Lagus, 2007) or word segmentation (Goldwateret al, 2007).
In terms of feature sets, our model ismost closely related to the constituent-context modelproposed by Klein and Manning (2001) for grammarinduction.
If we exclude the priors, our model canalso be seen as a semi-Markov conditional randomfield (CRF) model (Sarawagi and Cohen, 2004).Semi-Markov CRFs previously have been used forsupervised word segmentation (Andrew, 2006), butnot for unsupervised morphological segmentation.Unsupervised learning with log-linear models hasreceived little attention in the past.
Two notable ex-ceptions are Smith & Eisner (2005) for POS tagging,and Poon & Domingos (2008) for coreference res-olution.
Learning with log-linear models requirescomputing the normalization constant (a.k.a.
thepartition function) Z .
This is already challenging insupervised learning.
In unsupervised learning, thedifficulty is further compounded by the absence ofsupervised labels.
Smith & Eisner (2005) proposedcontrastive estimation, which uses a small neighbor-hood to compute Z .
The neighborhood is carefullydesigned so that it not only makes computation eas-ier but also offers sufficient contrastive informationto aid unsupervised learning.
Poon & Domingos(2008), on the other hand, used sampling to approx-imate Z .1 In this work, we benefit from both tech-niques: contrastive estimation creates a manageable,1Rosenfeld (1997) also did this for language modeling.210wvlAvwn(##__##)w(##__vl)vlAv(#w__wn)wn(Av__##)Figure 1: The morpheme and context (in parentheses)features for the segmented word w-vlAv-wn.informative Z , while sampling enables the use ofpowerful global features.3 Log-Linear Model for UnsupervisedMorphological SegmentationCentral to our approach is a log-linear model thatdefines the joint probability distribution for a cor-pus (i.e., the words) and a segmentation on the cor-pus.
The core of this model is a morpheme-contextmodel, with one feature for each morpheme,2 andone feature for each morpheme context.
We rep-resent contexts using the n-grams before and afterthe morpheme, for some constant n. To illustratethis, a segmented Arabic corpus is shown belowalong with its features, assuming we are tracking bi-gram contexts.
The segmentation is indicated withhyphens, while the hash symbol (#) represents theword boundary.Segmented Corpus hnAk w-vlAv-wn bn-wAl-ywm Al-jmAEpMorpheme Feature:Value hnAk:1 w:2 vlAv:1wn:1 bn:1 Al:2 ywm:1 jmAEp:1hnAk:1 wvlAvwn:1 bnw:1 Alywm:1 Alj-mAEp:1Bigram Context Feature:Value ## vl:1#w wn:1 Av ##:1 ## w#:1 bn ##:1## yw:1 Al ##:2 ## jm:1 ## ##:5Furthermore, the corresponding features for the seg-mented word w-vlAv-wn are shown in Figure 1.Each feature is associated with a weight, whichcorrelates with the likelihood that the correspond-ing morpheme or context marks a valid morpholog-ical segment.
Such overlapping features allow us tocapture rich segmentation regularities.
For example,given the Arabic word Alywm, to derive its correctsegmentation Al-ywm, it helps to know that Al andywm are likely morphemes whereas Aly or lyw are2The word as a whole is also treated as a morpheme in itself.not; it also helps to know that Al ## or ## yw arelikely morpheme contexts whereas ly ## or ## wmare not.
Ablation tests verify the importance of theseoverlapping features (see Section 7.2).Our morpheme-context model is inspired bythe constituent-context model (CCM) proposed byKlein and Manning (2001) for grammar induction.The morphological segmentation of a word can beviewed as a flat tree, where the root node corre-sponds to the word and the leaves correspond tomorphemes (see Figure 1).
The CCM uses uni-grams for context features.
For this task, however,we found that bigrams and trigrams lead to muchbetter accuracy.
We use trigrams in our full model.For learning, one can either view the corpus asa collection of word types (unique words) or tokens(word occurrences).
Some systems (e.g., Morfessor)use token frequency for parameter estimation.
Oursystem, however, performs much better using wordtypes.
This has also been observed for other mor-phological learners (Goldwater et al, 2006).
Thuswe use types in learning and inference, and effec-tively enforce the constraint that words can haveonly one segmentation per type.
Evaluation is stillbased on tokens to reflect the performance in realapplications.In addition to the features of the morpheme-context model, we incorporate two priors which cap-ture additional intuitions about morphological seg-mentations.
First, we observe that the number ofdistinct morphemes used to segment a corpus shouldbe small.
This is achieved when the same mor-phemes are re-used across many different words.Our model incorporates this intuition by imposinga lexicon prior: an exponential prior with nega-tive weight on the length of the morpheme lexi-con.
We define the lexicon to be the set of uniquemorphemes identified by a complete segmentationof the corpus, and the lexicon length to be the to-tal number of characters in the lexicon.
In thisway, we can simultaneously emphasize that a lexi-con should contain few unique morphemes, and thatthose morphemes should be short.
However, the lex-icon prior alone incorrectly favors the trivial seg-mentation that shatters each word into characters,which results in the smallest lexicon possible (sin-gle characters).
Therefore, we also impose a corpusprior: an exponential prior on the number of mor-211phemes used to segment each word in the corpus,which penalizes over-segmentation.
We notice thatlonger words tend to have more morphemes.
There-fore, each word?s contribution to this prior is nor-malized by the word?s length in characters (e.g., thesegmented word w-vlAv-wn contributes 3/7 to the to-tal corpus size).
Notice that it is straightforward toincorporate such a prior in a log-linear model, butmuch more challenging to do so in a directed gen-erative model.
These two priors are inspired by theminimum description length (MDL) length princi-ple; the lexicon prior favors fewer morpheme types,whereas the corpus prior favors fewer morpheme to-kens.
They are vital to the success of our model,providing it with the initial inductive bias.We also notice that often a word is decomposedinto a stem and some prefixes and suffixes.
This isparticularly true for languages with predominantlyinflectional morphology, such as Arabic, Hebrew,and English.
Thus our model uses separate lexiconsfor prefixes, stems, and suffixes.
This results in asmall but non-negligible accuracy gain in our exper-iments.
We require that a stem contain at least twocharacters and no fewer characters than any affixesin the same word.3 In a given word, when a mor-pheme is identified as the stem, any preceding mor-pheme is identified as a prefix, whereas any follow-ing morpheme as a suffix.
The sample segmentedcorpus mentioned earlier induces the following lex-icons:Prefix w AlStem hnAk vlAv bn ywm jmAEpSuffix wn wBefore presenting our formal model, we first in-troduce some notation.
Let W be a corpus (i.e., a setof words), and S be a segmentation that breaks eachword in W into prefixes, a stem, and suffixes.
Let ?be a string (character sequence).
Each occurrence of?
will be in the form of ?1?
?2, where ?1, ?2 are theadjacent character n-grams, and c = (?1, ?2) is thecontext of ?
in this occurrence.
Thus a segmentationcan be viewed as a set of morpheme strings and theircontexts.
For a string x, L(x) denotes the number ofcharacters in x; for a word w, MS(w) denotes the3In a segmentation where several morphemes have the max-imum length, any of them can be identified as the stem, eachresulting in a distinct segmentation.number of morphemes in w given the segmentationS; Pref(W,S), Stem(W,S), Suff(W,S) denotethe lexicons of prefixes, stems, and suffixes inducedby S for W .
Then, our model defines a joint proba-bility distribution over a restricted set of W and S:P?
(W,S) = 1Z ?
u?(W,S)whereu?
(W,S) = exp(????f?
(S) +?c?cfc(S)+ ?
?
???Pref(W,S)L(?
)+ ?
?
???Stem(W,S)L(?
)+ ?
?
???Suff(W,S)L(?
)+ ?
?
?w?WMS(w)/L(w) )Here, f?
(S) and fc(S) are respectively the occur-rence counts of morphemes and contexts under S,and ?
= (?
?, ?c : ?, c) are their feature weights.
?, ?
are the weights for the priors.
Z is the nor-malization constant, which sums over a set of cor-pora and segmentations.
In the next section, we willdefine this set for our model and show how to effi-ciently perform learning and inference.4 Unsupervised LearningAs mentioned in Smith & Eisner (2005), learningwith probabilistic models can be viewed as movingprobability mass to the observed data.
The questionis from where to take this mass.
For log-linear mod-els, the answer amounts to defining the set that Zsums over.
We use contrastive estimation and definethe set to be a neighborhood of the observed data.The instances in the neighborhood can be viewedas pseudo-negative examples, and learning seeks todiscriminate them from the observed instances.Formally, let W ?
be the observed corpus, and letN(?)
be a function that maps a string to a set ofstrings; let N(W ?)
denote the set of all corpora thatcan be derived from W ?
by replacing every wordw ?W ?
with one in N(w).
Then,Z = ?W?N(W ?
)?Su(W,S).212Unsupervised learning maximizes the log-likelihoodof observing W ?L?
(W ?)
= log?SP (W ?, S)We use gradient descent for this optimization; thepartial derivatives for feature weights are???iL?
(W ?)
= ES|W ?[fi]?
ES,W [fi]where i is either a string ?
or a context c. The firstexpected count ranges over all possible segmenta-tions while the words are fixed to those observed inW ?.
For the second expected count, the words alsorange over the neighborhood.Smith & Eisner (2005) considered various neigh-borhoods for unsupervised POS tagging, andshowed that the best neighborhoods are TRANS1(transposing any pair of adjacent words) andDELORTRANS1 (deleting any word or transposingany pair of adjacent words).
We can obtain theircounterparts for morphological segmentation bysimply replacing ?words?
with ?characters?.
Asmentioned earlier, the instances in the neighbor-hood serve as pseudo-negative examples from whichprobability mass can be taken away.
In this regard,DELORTRANS1 is suitable for POS tagging sincedeleting a word often results in an ungrammaticalsentence.
However, in morphology, a word less acharacter is often a legitimate word too.
For exam-ple, deleting l from the Hebrew word lyhwh (to thelord) results in yhwh (the lord).
Thus DELORTRANS1forces legal words to compete against each other forprobability mass, which seems like a misguided ob-jective.
Therefore, in our model we use TRANS1.
Itis suited for our task because transposing a pair ofadjacent characters usually results in a non-word.To combat overfitting in learning, we impose aGaussian prior (L2 regularization) on all weights.5 Supervised LearningOur learning algorithm can be readily applied to su-pervised or semi-supervised learning.
Suppose thatgold segmentation is available for some words, de-noted as S?.
If S?
contains gold segmentationsfor all words in W , we are doing supervised learn-ing; otherwise, learning is semi-supervised.
Train-ing now maximizes L?
(W ?, S?
); the partial deriva-tives become???iL?
(W ?, S?)
= ES|W ?,S?
[fi] ?
ES,W [fi]The only difference in comparison with unsuper-vised learning is that we fix the known segmenta-tion when computing the first expected counts.
InSection 7.3, we show that when labels are available,our model also learns much more effectively than adirected graphical model.6 InferenceIn Smith & Eisner (2005), the objects (sentences) areindependent from each other, and exact inference istractable.
In our model, however, the lexicon priorrenders all objects (words) interdependent in termsof segmentation decisions.
Consider the simple cor-pus with just two words: Alrb, lAlrb.
If lAlrb is seg-mented into l-Al-rb, Alrb can be segmented into Al-rb without paying the penalty imposed by the lexi-con prior.
If, however, lAlrb remains a single mor-pheme, and we still segment Alrb into Al-rb, thenwe introduce two new morphemes into the lexicons,and we will be penalized by the lexicon prior ac-cordingly.
As a result, we must segment the wholecorpus jointly, making exact inference intractable.Therefore, we resort to approximate inference.
Tocompute ES|W ?
[fi], we use Gibbs sampling.
To de-rive a sample, the procedure goes through each wordand samples the next segmentation conditioned onthe segmentation of all other words.
With m sam-ples S1, ?
?
?
, Sm, the expected count can be approx-imated asES|W ?
[fi] ?
1m?jfi(Sj)There are 2n?1 ways to segment a word of n char-acters.
To sample a new segmentation for a partic-ular word, we need to compute conditional proba-bility for each of these segmentations.
We currentlydo this by explicit enumeration.4 When n is large,4These segmentations could be enumerated implicitly us-ing the dynamic programming framework employed by semi-Markov CRFs (Sarawagi and Cohen, 2004).
However, in such asetting, our lexicon prior would likely need to be approximated.We intend to investigate this in future work.213this is very expensive.
However, we observe thatthe maximum number of morphemes that a wordcontains is usually a small constant for many lan-guages; in the Arabic Penn Treebank, the longestword contains 14 characters, but the maximum num-ber of morphemes in a word is only 5.
Therefore,we impose the constraint that a word can be seg-mented into no more than k morphemes, where kis a language-specific constant.
We can determinek from prior knowledge or use a development set.This constraint substantially reduces the number ofsegmentation candidates to consider; with k = 5, itreduces the number of segmentations to consider byalmost 90% for a word of 14 characters.ES,W [fi] can be computed by Gibbs sampling inthe same way, except that in each step we also sam-ple the next word from the neighborhood, in additionto the next segmentation.To compute the most probable segmentation, weuse deterministic annealing.
It works just like a sam-pling algorithm except that the weights are dividedby a temperature, which starts with a large value andgradually drops to a value close to zero.
To makeburn-in faster, when computing the expected counts,we initialize the sampler with the most probable seg-mentation output by annealing.7 ExperimentsWe evaluated our system on two datasets.
Our mainevaluation is on a multi-lingual dataset constructedby Snyder & Barzilay (2008a; 2008b).
It consists of6192 short parallel phrases in Hebrew, Arabic, Ara-maic (a dialect of Arabic), and English.
The paral-lel phrases were extracted from the Hebrew Bibleand its translations via word alignment and post-processing.
For Arabic, the gold segmentation wasobtained using a highly accurate Arabic morpholog-ical analyzer (Habash and Rambow, 2005); for He-brew, from a Bible edition distributed by Westmin-ster Hebrew Institute (Groves and Lowery, 2006).There is no gold segmentation for English and Ara-maic.
Like Snyder & Barzilay, we evaluate on theArabic and Hebrew portions only; unlike their ap-proach, our system does not use any bilingual in-formation.
We refer to this dataset as S&B .
Wealso report our results on the Arabic Penn Treebank(ATB), which provides gold segmentations for anArabic corpus with about 120,000 Arabic words.As in previous work, we report recall, precision,and F1 over segmentation points.
We used 500phrases from the S&B dataset for feature develop-ment, and also tuned our model hyperparametersthere.
The weights for the lexicon and corpus pri-ors were set to ?
= ?1, ?
= ?20.
The featureweights were initialized to zero and were penalizedby a Gaussian prior with ?2 = 100.
The learningrate was set to 0.02 for all experiments, except thefull Arabic Penn Treebank, for which it was set to0.005.5 We used 30 iterations for learning.
In eachiteration, 200 samples were collected to computeeach of the two expected counts.
The sampler wasinitialized by running annealing for 2000 samples,with the temperature dropping from 10 to 0.1 at 0.1decrements.
The most probable segmentation wasobtained by running annealing for 10000 samples,using the same temperature schedule.
We restrictedthe segmentation candidates to those with no greaterthan five segments in all experiments.7.1 Unsupervised Segmentation on S&BWe followed the experimental set-up of Snyder &Barzilay (2008b) to enable a direct comparison.
Thedataset is split into a training set with 4/5 of thephrases, and a test set with the remaining 1/5.
First,we carried out unsupervised learning on the trainingdata, and computed the most probable segmentationfor it.
Then we fixed the learned weights and the seg-mentation for training, and computed the most prob-able segmentation for the test set, on which we eval-uated.6 Snyder & Barzilay (2008b) compared sev-eral versions of their systems, differing in how muchbilingual information was used.
Using monolingualinformation only, their system (S&B-MONO) trailsthe state-of-the-art system Morfessor; however, theirbest system (S&B-BEST), which uses bilingual in-formation that includes phrasal alignment and pho-netic correspondence between Arabic and Hebrew,outperforms Morfessor and achieves the state-of-the-art results on this dataset.5The ATB set is more than an order of magnitude larger andrequires a smaller rate.6With unsupervised learning, we can use the entire datasetfor training since no labels are provided.
However, this set-up is necessary for S&B?s system because they used bilingualinformation in training, which is not available at test time.214ARABIC Prec.
Rec.
F1S&B-MONO 53.0 78.5 63.2S&B-BEST 67.8 77.3 72.2FULL 76.0 80.2 78.1HEBREW Prec.
Rec.
F1S&B-MONO 55.8 64.4 59.8S&B-BEST 64.9 62.9 63.9FULL 67.6 66.1 66.9Table 1: Comparison of segmentation results on the S&Bdataset.Table 1 compares our system with theirs.
Our sys-tem outperforms both S&B-MONO and S&B-BESTby a large margin.
For example, on Arabic, our sys-tem reduces F1 error by 21% compared to S&B-BEST, and by 40% compared to S&B-MONO.
Thissuggests that the use of monolingual morpheme con-text, enabled by our log-linear model, is more help-ful than their bilingual cues.7.2 Ablation TestsTo evaluate the contributions of the major compo-nents in our model, we conducted seven ablationtests on the S&B dataset, each using a model thatdiffered from our full model in one aspect.
The firstthree tests evaluate the effect of priors, whereas thenext three test the effect of context features.
Thelast evaluates the impact of using separate lexiconsfor affixes and stems.NO-PRIOR The priors are not used.NO-COR-PR The corpus prior is not used.NO-LEX-PR The lexicon prior is not used.NO-CONTEXT Context features are not used.UNIGRAM Unigrams are used in context.BIGRAM Bigrams are used in context.SG-LEXICON A single lexicon is used, rather thanthree distinct ones for the affixes and stems.Table 2 presents the ablation results in compari-son with the results of the full model.
When some orall priors are excluded, the F1 score drops substan-tially (over 10 points in all cases, and over 40 pointsin some).
In particular, excluding the corpus prior,as in NO-PRIOR and NO-COR-PR, results in over-segmentation, as is evident from the high recalls andlow precisions.
When the corpus prior is enactedbut not the lexicon priors (NO-LEX-PR), precisionARABIC Prec.
Rec.
F1FULL 76.0 80.2 78.1NO-PRIOR 24.6 89.3 38.6NO-COR-PR 23.7 87.4 37.2NO-LEX-PR 79.1 51.3 62.3NO-CONTEXT 71.2 62.1 66.3UNIGRAM 71.3 76.5 73.8BIGRAM 73.1 78.4 75.7SG-LEXICON 72.8 82.0 77.1HEBREW Prec.
Rec.
F1FULL 67.6 66.1 66.9NO-PRIOR 34.0 89.9 49.4NO-COR-PR 35.6 90.6 51.1NO-LEX-PR 65.9 49.2 56.4NO-CONTEXT 63.0 47.6 54.3UNIGRAM 63.0 63.7 63.3BIGRAM 69.5 66.1 67.8SG-LEXICON 67.4 65.7 66.6Table 2: Ablation test results on the S&B dataset.is much higher, but recall is low; the system now errson under-segmentation because recurring strings areoften not identified as morphemes.A large accuracy drop (over 10 points in F1score) also occurs when the context features areexcluded (NO-CONTEXT), which underscores theimportance of these overlapping features.
We alsonotice that the NO-CONTEXT model is compara-ble to the S&B-MONO model; they use the samefeature types, but different priors.
The accuracies ofthe two systems are comparable, which suggests thatwe did not sacrifice accuracy by trading the morecomplex and restrictive Dirichlet process prior forexponential priors.
A priori, it is unclear whether us-ing contexts larger than unigrams would help.
Whilepotentially beneficial, they also risk aggravating thedata sparsity and making our model more prone tooverfitting.
For this problem, however, enlarging thecontext (using higher n-grams up to trigrams) helpssubstantially.
For Arabic, the highest accuracy is at-tained by using trigrams, which reduces F1 error by16% compared to unigrams; for Hebrew, by usingbigrams, which reduces F1 error by 17%.
Finally, ithelps to use separate lexicons for affixes and stems,although the difference is small.215ARABIC %Lbl.
Prec.
Rec.
F1S&B-MONO-S 100 73.2 92.4 81.7S&B-BEST-S 200 77.8 92.3 84.4FULL-S 25 84.9 85.5 85.250 88.2 86.8 87.575 89.6 86.4 87.9100 91.7 88.5 90.0HEBREW %Lbl.
Prec.
Rec.
F1S&B-MONO-S 100 71.4 79.1 75.1S&B-BEST-S 200 76.8 79.2 78.0FULL-S 25 78.7 73.3 75.950 82.8 74.6 78.475 83.1 77.3 80.1100 83.0 78.9 80.9Table 3: Comparison of segmentation results with super-vised and semi-supervised learning on the S&B dataset.7.3 Supervised and Semi-Supervised LearningTo evaluate our system in the supervised and semi-supervised learning settings, we report the perfor-mance when various amounts of labeled data aremade available during learning, and compare themto the results of Snyder & Barzilay (2008a).
Theyreported results for supervised learning using mono-lingual features only (S&B-MONO-S), and for su-pervised bilingual learning with labels for both lan-guages (S&B-BEST-S).
On both languages, our sys-tem substantially outperforms both S&B-MONO-Sand S&B-BEST-S.
E.g., on Arabic, our system re-duces F1 errors by 46% compared to S&B-MONO-S, and by 36% compared to S&B-BEST-S. More-over, with only one-fourth of the labeled data, oursystem already outperforms S&B-MONO-S. Thisdemonstrates that our log-linear model is bettersuited to take advantage of supervised labels.7.4 Arabic Penn TreebankWe also evaluated our system on the Arabic PennTreebank (ATB).
As is common in unsupervisedlearning, we trained and evaluated on the entire set.We compare our system with Morfessor (Creutz andLagus, 2007).7 In addition, we compare with Mor-fessor Categories-MAP, which builds on Morfessorand conducts an additional greedy search specifi-cally tailored to segmentation.
We found that it per-7We cannot compare with Snyder & Barzilay?s system as itsstrongest results require bilingual data, which is not available.ATB-7000 Prec.
Rec.
F1MORFESSOR-1.0 70.6 34.3 46.1MORFESSOR-MAP 86.9 46.4 60.5FULL 83.4 77.3 80.2ATB Prec.
Rec.
F1MORFESSOR-1.0 80.7 20.4 32.6MORFESSOR-MAP 77.4 72.6 74.9FULL 88.5 69.2 77.7Table 4: Comparison of segmentation results on the Ara-bic Penn Treebank.forms much better than Morfessor on Arabic butworse on Hebrew.
To test each system in a low-data setting, we also ran experiments on the set con-taining the first 7,000 words in ATB with at leasttwo characters (ATB-7000).
Table 4 shows the re-sults.
Morfessor performs rather poorly on ATB-7000.
Morfessor Categories-MAP does much bet-ter, but its performance is dwarfed by our system,which further cuts F1 error by half.
On the full ATBdataset, Morfessor performs even worse, whereasMorfessor Categories-MAP benefits from the largerdataset and achieves an F1 of 74.9.
Still, our systemsubstantially outperforms it, further reducing F1 er-ror by 11%.88 ConclusionThis paper introduces the first log-linear model forunsupervised morphological segmentation.
It lever-ages overlapping features such as morphemes andtheir contexts, and enables easy extension to incor-porate additional features and linguistic knowledge.For Arabic and Hebrew, it outperforms the state-of-the-art systems by a large margin.
It can alsobe readily applied to supervised or semi-supervisedlearning when labeled data is available.
Future di-rections include applying our model to other in-flectional and agglutinative languages, modeling in-ternal variations of morphemes, leveraging paralleldata in multiple languages, and combining morpho-logical segmentation with other NLP tasks, such asmachine translation.8Note that the ATB and ATB-7000 experiments each mea-sure accuracy on their entire training set.
This difference intesting conditions explains why some full ATB results are lowerthan ATB-7000.216ReferencesGalen Andrew.
2006.
A hybrid markov/semi-markovconditional random field for sequence segmentation.In Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP).Michael R. Brent, Sreerama K. Murthy, and AndrewLundberg.
1995.
Discovering morphemic suffixes: Acase study in minimum description length induction.In Proceedings of the 15th Annual Conference of theCognitive Science Society.Tim Buckwalter.
2004.
Buckwalter Arabic morphologi-cal analyzer version 2.0.Mathias Creutz and Krista Lagus.
2007.
Unsupervisedmodels for morpheme segmentation and morphologylearning.
ACM Transactions on Speech and LanguageProcessing, 4(1).Sajib Dasgupta and Vincent Ng.
2007.
High-performance, language-independent morphologicalsegmentation.
In Proceedings of Human LanguageTechnology (NAACL).Vera Demberg.
2007.
A language-independent unsuper-vised model for morphological segmentation.
In Pro-ceedings of the 45th Annual Meeting of the Associationfor Computational Linguistics, Prague, Czech Repub-lic.John Goldsmith.
2001.
Unsupervised learning of themorphology of a natural language.
ComputationalLinguistics, 27(2):153?198.Sharon Goldwater, Thomas L. Griffiths, and Mark John-son.
2006.
Interpolating between types and tokens byestimating power-law generators.
In Advances in Neu-ral Information Processing Systems 18.Sharon Goldwater, Thomas L. Griffiths, and Mark John-son.
2007.
Distributional cues to word segmenta-tion: Context is important.
In Proceedings of the 31stBoston University Conference on Language Develop-ment.Alan Groves and Kirk Lowery, editors.
2006.
The West-minster Hebrew Bible Morphology Database.
West-minster Hebrew Institute, Philadelphia, PA, USA.Nizar Habash and Owen Rambow.
2005.
Arabic tok-enization, part-of-speech tagging and morphologicaldisambiguation in one fell swoop.
In Proceedings ofthe 43rd Annual Meeting of the Association for Com-putational Linguistics.Zellig S. Harris.
1955.
From phoneme to morpheme.Language, 31(2):190?222.Samarth Keshava and Emily Pitler.
2006.
A simple, intu-itive approach to morpheme induction.
In Proceedingsof 2nd Pascal Challenges Workshop, Venice, Italy.Dan Klein and Christopher D. Manning.
2001.
Natu-ral language grammar induction using a constituent-context model.
In Advances in Neural InformationProcessing Systems 14.Mikko Kurimo, Mathias Creutz, and Ville Turunen.2007.
Overview of Morpho Challenge in CLEF 2007.In Working Notes of the CLEF 2007 Workshop.Hoifung Poon and Pedro Domingos.
2008.
Joint un-supervised coreference resolution with markov logic.In Proceedings of the 2008 Conference on EmpiricalMethods in Natural Language Processing, pages 649?658, Honolulu, HI.
ACL.Ronald Rosenfeld.
1997.
A whole sentence maximumentropy language model.
In IEEE workshop on Auto-matic Speech Recognition and Understanding.Sunita Sarawagi and William Cohen.
2004.
Semimarkovconditional random fields for information extraction.In Proceedings of the Twenty First International Con-ference on Machine Learning.Patrick Schone and Daniel Jurafsky.
2001.
Knowlege-free induction of inflectional morphologies.
In Pro-ceedings of Human Language Technology (NAACL).Noah A. Smith and Jason Eisner.
2005.
Contrastive esti-mation: Training log-linear models on unlabeled data.In Proceedings of the 43rd Annual Meeting of the As-sociation for Computational Linguistics.Benjamin Snyder and Regina Barzilay.
2008a.
Cross-lingual propagation for morphological analysis.
InProceedings of the Twenty Third National Conferenceon Artificial Intelligence.Benjamin Snyder and Regina Barzilay.
2008b.
Unsuper-vised multilingual learning for morphological segmen-tation.
In Proceedings of the 46th Annual Meeting ofthe Association for Computational Linguistics.217
