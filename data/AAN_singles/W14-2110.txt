Proceedings of the First Workshop on Argumentation Mining, pages 69?78,Baltimore, Maryland USA, June 26, 2014. c?2014 Association for Computational LinguisticsApplying Argumentation Schemes for Essay Scoring    Yi Song   Michael Heilman   Beata Beigman Klebanov   Paul Deane Educational Testing Service Princeton, NJ, USA  {ysong, mheilman, bbeigmanklebanov, pdeane}@ets.org    AbstractUnder the framework of the argumentation scheme theory (Walton, 1996), we developed annotation protocols for an argumentative writing task to support identification and classification of the arguments being made in essays.
Each annotation protocol defined ar-gumentation schemes (i.e., reasoning pat-terns) in a given writing prompt and listed questions to help evaluate an argument based on these schemes, to make the argument structure in a text explicit and classifiable.
We report findings based on an annotation of 600 essays.
Most annotation categories were applied reliably by human annotators, and some categories significantly contributed to essay score.
An NLP system to identify sen-tences containing scheme-relevant critical questions was developed based on the human annotations.
1.
Introduction In this paper, we analyze the structure of argu-ments as a first step in analyzing their quality.
Argument structure plays a critical role in identi-fying relevant arguments based on their content, so it seems reasonable to focus first on identify-ing characteristic patterns of argumentation and the ways in which such arguments are typically developed when they are explicitly stated.
It is worthwhile to classify the arguments in a text and to identify their structure when they are ex-tended to include whole text segments (Walton, 1996; Walton, Reed, and Macagno, 2008), but it is not clear how far human annotation can go in analyzing argument structure.
An analysis of the effectiveness and full com-plexity of argument structure is different than the identification of generic elements that might compose an argument, such as claims (e.g., a thesis sentence), main reasons (e.g., supporting topic sentences), evidence (e.g., elaboratingsegments), and other components, such as the introduction and conclusion (Burstein, Kukich, Wolff, Lu, Chodorow, Braden-Harder, & Harris, 1998; Burstein, Marcu, and Knight, 2003; Pendar & Cotos, 2008).
In contrast, here we focus on analyzing specific types of arguments, what the literature terms argumentation schemes (Walton, 1996).
Argumentation schemes include schemat-ic content and take into account a pattern of pos-sible argumentation moves in a larger persuasive dialog.
Understanding these argumentation schemes is important for understanding the logic behind an argument.
Critical questions associat-ed with a particular argumentation scheme pro-vide a normative standard that can be used to evaluate the relevance of an argument?s justifica-tory structure (van Eemeren and Grootendorst, 1992; Walton, 1996; Walton et al., 2008).
We aimed to lay foundations for the automat-ed analysis of argumentation schemes, such as the identification and classification of the argu-ments in an essay.
Specifically, we developed annotation protocols for writing prompts in an argument analysis task from a graduate school admissions test.
The task was designed to assess how well a student analyzes someone else?s ar-gument, which is provided by the prompt.
The student must critically evaluate the logical soundness of the given argument.
The annotation categories were designed to map student re-sponses to the scheme-relevant critical questions.
We examined whether this approach provides a useful framework for describing argumentation and whether human annotators can apply it relia-bly and consistently.
Furthermore, we have be-gun work on automating the annotation process by developing a system to predict whether sen-tences contain scheme-relevant critical questions.
2.
Theoretical Framework As Nussbaum (2011) notes, there have been crit-ical advances in the study of informal argument,69which takes place within a social context involv-ing dialog among people with different beliefs, most notably the development of theories that provide relatively rich schemata for classifying informal arguments, such as Walton (1996).
An argumentation scheme is defined as ?a more or less conventionalized way of represent-ing the relation between what is stated in the ar-gument and what is stated in the standpoint?
(van Eemeren and Grootendorst, 1992, p. 96).
It is a strategic pattern of argumentation linking prem-ises to a conclusion and illustrating how the con-clusion is derived from the premises.
This ?in-ternal structure?
of argumentation reflects justifi-catory standards that can be used to help evaluate the reasonableness of an argument (van Eemeren and Grootendorst, 2004).
Argumentation schemes should be distinguished from the kinds of structures postulated in Mann and Thompson?s (1988) Rhetorical Structure Theory (RST) be-cause they focus on relations inherent in the meaning of the argument, regardless of whether they are explicitly realized in the discourse.
Consider, for instance, argument from conse-quences, which applies when the primary claim argues for or against a proposed policy (i.e., course of action) by citing positive or negative consequences that would follow if the policy were adopted (Walton, 1996).
Elaborations of an argument from consequences are designed to defend against possible objections.
For instance, an opponent could claim that the claimed conse-quences are not probable; or that they are not desirable; or that they are less important than other, undesirable consequences.
Thus a sophis-ticated writer, in elaborating an argument from consequences, may provide information to rein-force the idea that the argued consequences are probable, desirable, and more important than any possible undesired effects.
These moves corre-spond to what the literature calls critical ques-tions, which function as a standard for evaluating the reasonableness of an argument based on its argumentation schemes (Walton, 1996).
Walton and his colleagues (2008) analyzed over 60 argumentation schemes, and identified critical questions associated with certain schemes as the logical moves in argumentative discourse.
The range of possible moves is quite large, espe-cially when people use multiple schemes.
There have been several efforts to annotate corpora with argumentation scheme information to sup-port future machine learning efforts (Mochales and Ieven, 2009; Palau and Moens, 2009; Rienks, Heylen, and Van der Weijden, 2005;Verbree, Rienks, and Heylen, 2006), to support argument representation (Atkinson, Bench-Capon, and McBurney, 2006; Rahwan, Banihashemi, Reed, Walton, and Abdallah, 2010), and to teach argumentative writing (Fer-retti, Lewis, and Andrews-Weckerly, 2009; Nussbaum and Schraw, 2007; Nussbaum and Edwards, 2011; Song and Ferretti, 2013).
In ad-dition, Feng and Hirsh (2011) used the argumen-tation schemes to reconstruct the implicit parts (i.e., unstated assumptions) of the argument structure.
In many previous studies, the data sets on argumentation schemes were relatively small and the inter-rater agreement was not measured.
We are particularly interested in exploring the relationship between the use of scheme-relevant critical questions and essay quality, as measured by holistic essay scores.
The difference between an expert and a novice is that the expert knows which critical questions should be asked when the dynamic of the argument requires them, while the novice misses the essential moves to ask critical questions that help evaluate if the argument is valid or reasonable.
Often, students presume information and fail to ask questions that would reveal potential fallacies.
For exam-ple, they might use quotations from books, ar-guments from TV programs, or opinions posted online without evaluating whether the infor-mation is adequately supported by evidence.
Critically evaluating arguments is considered an important skill in college and graduate school.
For example, a widely accepted graduate admis-sions test has a task to assess students?
critical thinking and analytical writing skills.
In this ar-gument analysis task, students should demon-strate skills in critiquing other people?s argu-ments, such as identifying unwarranted assump-tions or discussing what specific evidence is need to support the argument.
They must com-municate their evaluation of the arguments clear-ly to the audience.
To accomplish this task suc-cessfully, students need to evaluate the argu-ments against appropriate criteria.
Therefore, their essays could be analyzed using an annota-tion approach based on the theory of argumenta-tion schemes and critical questions.
Our research questions were as follows:   1.
Can this scheme-based annotation approach be applied consistently by annotators to a corpus of argumentative essays?
2.
Do annotation categories based on the theo-ry of argumentation schemes contribute70significantly to the prediction of essay scores?
3.
Can we use NLP techniques to train an au-tomated classifier for distinguishing sen-tences that raise critical questions from sen-tences that contain no critical questions?
3  Development of Annotation Protocols Although Walton?s argumentation schemes pro-vided a good framework for analyzing argu-ments, it was challenging to apply them in some cases of argument essays because various inter-pretations could be made on some argument structures.
For instance, people were often con-fused with argument from consequences, argu-ment from correlation to cause, and argument from cause to effect because all these three types of arguments indicate a causal relationship.
While it is good that Walton tried to identify var-iations of a causal relationship, a side effect is that some schemes are not so distinguishable from each other, especially for someone who is not an expert in logic.
This ambiguity makes it difficult to apply his theory directly to annota-tion.
Thus, we modified Walton?s schemes and created new schemes when necessary to achieve exclusive annotation categories and capture the features in the argument analysis task.
In this paper, we illustrate our annotation pro-tocols on a policy argument because over half of the argument analysis prompts for the assess-ment we are working with deal with policy is-sues (i.e., issues involve the possibility of putting a practice into place).
Here, we use the ?Patriot Car?
prompt as an example.
The following appeared in a memo-randum from the new president of the Patriot car manufacturing company.
"In the past, the body styles of Patriot cars have been old-fashioned, and our cars have not sold as well as have our competitors' cars.
But now, since many regions in this country report rapid in-creases in the numbers of newly licensed drivers, we should be able to increase our share of the market by selling cars to this growing population.
Thus, we should discontinue our oldest models and con-centrate instead on manufacturing sporty cars.
We can also improve the success of our marketing campaigns by switching our advertising to the Youth Advertisingagency, which has successfully promoted the country's leading soft drink."
Test takers are asked to analyze the reasoning in the argument, consider any assumptions, and discuss how well any evidence that is mentioned supports the conclusion.
The prompt states that the new president of the Patriot car manufacturing company pointed out a problem that the body styles of Patriot cars have been old-fashioned and their cars have not sold as well as their competitors?
cars.
The president proposed a plan to discontinue their oldest mod-els and to concentrate on manufacturing sporty cars.
He believed that this plan will lead to an increase in their market share (i.e., the goal).
This is a policy issue because it involves whether the plan of discontinuing oldest car models and manufacturing sporty cars should be put into place.
This prompt shows a typical pattern of many argument analysis prompts about policy issues: (1) a problem is stated; (2) a plan is pro-posed; and (3) a desirable goal will be achieved if the plan is implemented.
Thus, we created a policy scheme that includes these three major components (i.e., problem, plan, and goal), and a causal relationship that bridges the plan to the goal in the policy scheme.
Therefore, a causal scheme appears in a policy argument to represent the causal relationship from the proposed plan to the goal.
This part is different from Walton?s analysis.
He uses the argument from conse-quences scheme for policy arguments, but it cre-ated confusions when applying it to annotation, especially when students unconsciously use the word ?cause?
to introduce a potential conse-quence that follows a policy.
In addition, our causal scheme combines the argument from cor-relation to cause scheme and the argument from cause to effect scheme specified by Walton.
Accordingly, we revised or re-arranged some of the critical questions in Walton?s theory.
For example, challenges to arguments that use a poli-cy scheme fall into the following six categories: (a) problem; (b) goal; (c) plan implementation; (d) plan definition; (e) side effect; and (f) alterna-tive plan.
When someone writes that the presi-dent should re-evaluate whether this is really a problem, it matches the question in the ?prob-lem?
category; when someone questions if there  is an alternative plan that could also help achieve the goal and is better than the plan proposed by the president, it should be categorized as a chal-lenge in ?alternative plan.?
We call these ?specif-ic questions?
because they are attached to a par-71ticular prompt.
In other words, specific questions are content dependent.
Each category also in-cludes one or more ?general questions?
that can be asked for any argument using the same argu-mentation scheme, and in this case, it is the poli-cy scheme.
We have developed annotation protocols for various argumentation schemes.
Table 1 includes part of the annotation protocols (i.e., scheme, category, and general critical questions) for three argumentation schemes: the policy argument scheme, the causal argument scheme, and the argument from a sample scheme.
This study fo-cuses on these three argumentation schemes and 16 associated categories.
4  Application of the Annotation Ap-proach This section focuses on applying the annotation approach and the following research question: Can this scheme-based annotation approach be applied consistently by raters to a corpus of ar-gumentative essays?
4.1  Annotation RulesThe first step of the annotation is reading the en-tire essay.
It is important to understand the writ-er?s major arguments and the organization of the essay.
Next, the annotator will identify and high-light any text segment (e.g., paragraph, sentence, or clause) that addresses a critical question.
Usu-ally, the minimal text segment is at the sentence-level, but it could be the case that the selection is at the phrase-level when a sentence includes multiple points that match more than one critical question.
Thirdly, for a highlighted unit, the an-notator will choose a topic, a category, and a se-cond topic, if applicable.
Only one category label can be assigned to each selected text unit.
?Generic?
information will not be selected or assigned an annotation label.
Generic infor-mation includes restatements of the text in the prompt, general statements that do not address any specific questions, rhetoric attacks, and irrel-evant information.
Note that this notion of gener-ic information is related to ?shell language,?
as described by Madnani et al (2012).
However, our definition here focuses more closely on sen-tences that do not raise critical questions.
Sur-face errors (e.g., grammar and spelling) can beScheme Category Critical QuestionPolicyProblem Is this really a problem?
Is the problem well-defined?
Goal How desirable is this goal?
Are there specific conflicting goals we do not wish to sacrifice?
Plan Implementation Is it practically possible to carry out this plan?
Plan Definition Is the plan well defined?
Side Effects Are there negative side effects that should be taken into account if we carry out our plan?
Alternative plan Are there better alternatives that could achieve the goal?CausalCausal Mechanism Is there really a correlation?
Is the correlation merely a coincidence (invalid causal relationship)?
Are there alternative causal factors?
Causal Efficacy Is the causal mechanism strong enough to produce the desired effects?
Applicability Does this causal mechanism apply?
Intervening Factors Are there intervening factors that could undermine the causal mechanism?SampleSignificance Are the patterns we see in the sample clear-cut enough (and in the right direction) to support the desired inference?
Representativeness Is there any reason to think that this sample might not be representative of the group about which we wish to make an inference?
Stability Is there any reason to think this pattern will be stable across all the circumstances about which we wish to make an inference?
Sample Size Is there any reason to think that the sample may not be large enough and reliable enough to support the inference we wish to draw?
Validity Is the sample measured in a way that will give valid information on the population attributes about which we wish to make inferences?
Alternatives Are there external considerations that could invalidate the claims?
Table 1: Annotation protocols for three types of argumentation schemes72ignored if they do not prevent people from un-derstanding the meaning of the essay.
Here is an example of annotated text.
As stated by the president, there is a rap-id increase in the number of newly li-censed drivers which would be a market-able target.
[However, there was no con-crete evidence that these newly licensed drivers favored sporty cars over other model types.
]Causal Applicability [On a similar note, there was no anecdotal evidence demonstrating that lack of sales was con-tributed to the old-fashion body styles of the Patriot cars.
]Causal Mechanism [There could be numerous other factors contrib-uting to their lack of sales:  prices are not competitive, safety ratings are not as high, features are not as appealing.
The best way to tackle this problem is to send out researches and surveys to get the opinions of consumers.
]Causal Mechanism 4.2  Annotation Tool The annotation interface includes the following elements: 1. the original writing prompt; 2. topics that the prompt addresses; 3. categories associated with critical questions relevant to that type of argument; 4. general critical questions that can be used across prompts that possess the same argu-mentation scheme; and 5. specific critical questions for this particular prompt.
The annotators highlight text segments to be an-notated and then clicked a button to choose a topic (e.g., body style versus advertising agency in the Patriot Car prompt) and a category to iden-tify which critical questions were addressed.
4.3  Data and Annotation Procedures In this section, we report our annotation on two selected argument analysis prompts in an as-sessment for graduate school admissions.
The actual prompts are not included here because they may be used in future tests.
Both prompts deal with policy issues and are involved in causal reasoning, but the second prompt also has a sam-ple scheme (see Table 1).
For each prompt, we randomly selected 300 essays to annotate.
These essays were written between 2008 and 2010.Four annotators with linguistics backgrounds who were not co-authors of the paper received training on the annotation approach.
Training focused on the application to specific prompts because each prompt had a specific annotation protocol that covers the argumentation schemes and how they relate to the prompt?s topics.
The first author delivered the training sessions, and helped resolve differences of opinion during practice annotation rounds.
After training and practice, the annotators annotated 20 pilot essays for a selected prompt to test their agreement.
This pilot stage gave us another chance to find and clarify any confusion about the annotation categories.
After that, the annotators worked on the sampled set of 300 essays, and these annota-tions were then used for analyses.
For each prompt, 40 essays were randomly selected, and all 4 annotators annotated these 40 essays to check the inter-annotator agreement.
For the experiments described later that involve the mul-tiply-annotated set, we used the annotations from the annotator who seemed most consistent.
4.4  Inter-Annotator Agreement To compute human-human agreement, we auto-matically split the essays into sentences.
For each sentence, we computed the annotations that overlapped with at least part of the tence.
Then, for each category, we computed human-human agreement across all sentences about whether that category should be marked or not.
We also created a ?Generic?
label, as dis-cussed in section 4.1, for sentences that were not marked by any of the other labels.
We computed two inter-annotator agreement statistics.
Our primary statistic is Cohen?s kappa between pairs of raters.
Four annotators generat-ed 6 pairs of kappa values, and in this report we only report the average kappa value for each an-notation category.
As an alternative statistic, we computed Krippendorff?s alpha, a chance-corrected statistic for calculating the inter-annotator agreement between multiple coders (four annotators in our case), which is similar to multi kappa (Krippendorff, 1980).
Table 2 shows the kappa and alpha values for each annotation category, excluding those that were rare.
To identify rare categories, we aver-aged the numbers of sentences annotated under a category among four annotators, which indicated how many sentences were annotated under this category in 40 essays.
If the number was lower than 10, which means that no more than one sen-tence was annotated in every four essays, then73the category was considered rare.
Most rare cate-gories had low inter-rater agreement, which is not surprising.
It is not realistic to require anno-tators to always agree about rare categories.
From Table 2, we can see that the kappa value and the alpha value on the same category were close.
The inter-annotator agreement on the ?ge-neric?
category varied little across the two prompts (kappa: 0.572-0.604; alpha: 0.571-0.603), which indicates that the annotators had a fairly good agreement on this category.
The an-notators had good agreements on most of the commonly used categories (kappa ranged from 0.549 to 0.848, and alpha ranged from 0.537 to 0.843) except the ?plan definition?
under the pol-icy scheme in prompt B (both kappa and alpha values were below 0.400).
The major reason for this disagreement is that one annotator marked a significantly higher number of sentences (more than double) for this category than others did.Table 2: Inter-annotator agreement 5  Essay Score and Annotation Features This section explores the second research ques-tion: Do annotation categories based on the theo-ry of argumentation schemes contribute signifi-cantly to the prediction of essay scores?
An-swering this question would tell us whether we capture an important construct of the argument analysis task by recognizing these argumentation features.
Specifically, we tested whether these features add predictive value to a model basedthe state-of-the-art e-rater essay scoring system (Burstein, Tetreault, and Madnani, 2013).
To explore the relationship between annota-tion categories and essay quality, we ran a multi-ple regression analysis for each prompt.
Essay quality was the dependent variable and was measured by a final human score, on a scale from 0 to 6.
The independent variables were nine high-level e-rater features and the annotation categories relevant to a prompt (Prompt A: 10 categories; Prompt B 16 categories).
The e-rater features were designed to measure different as-pects of writing (grammar, mechanics, style, us-age, word choice, word length, sentence variety, development, and organization).
We computed the percentage of sentences that were marked as belonging to each category (i.e., the number of sentences in a category divided by the total num-ber of sentences) to factor out essay length.
Note that the generic category was negatively correlated with the essay score in both prompts, since it included responses judged irrelevant to the scheme-relevant critical questions.
In other words, the generic responses are the parts of the text that do not present specific critical evalua-tions of the arguments in a given prompt.
For the purposes of our evaluation, we used the inverse feature labeled ?all critical questions?
: the pro-portion of the text that actually raises some criti-cal question (i.e., is not generic), regardless of scheme.
We believe this formulation more trans-parently expresses the underlying mechanism relating the feature to essay quality.
For each prompt, we split the 300 essays into two data sets: the training set and the testing set.
The testing set had the 40 essays that were anno-tated by all four annotators, and the training set had the remaining 260.
We trained three models with stepwise regression on the training set and evaluated them on the testing set:  1.
A model that included only the e-rater fea-tures to examine how well the e-rater mod-el works (?baseline?)
2.
A model with the baseline features and all the annotation category percentage varia-bles except for the "generic" category vari-able (?baseline + categories?)
3.
A model with the baseline features and a feature corresponding to the inverse of the "generic" category (?baseline + all critical questions?).
Table 3 presents the Pearson correlation coef-ficient r values for comparing model predictionsPrompt Category Kappa AlphaPrompt A     Generic 0.572 0.571  Policy : Problem 0.644 0.640  Policy : Side Effects 0.612 0.609  Policy : Alternative Plan 0.665 0.666  Causal : Causal Mechanism 0.680 0.676  Causal : Applicability 0.557 0.555 Prompt B     Generic 0.604 0.603  Policy : Problem 0.848 0.843  Policy : Plan Definition 0.346 0.327  Causal : Causal Mechanism 0.620 0.622  Causal : Applicability 0.767 0.769  Sample : Validity 0.549 0.53774to human scores for each of the models.
In prompt A, three annotation categories (causal mechanism, applicability, and alternative plan) were selected by the stepwise regression because they significantly contributed to the essay score above the nine e-rater features.
This model showed higher test set correlations than the base-line model (?
r = .014).
The model with the gen-eral argument feature (?all critical questions?)
showed a similar increase (?
r = .014).
Training Set r Testing Set r Testing Set ?
r Prompt A    baseline?
.838 .852 --- baseline + specific categories?
.852 .866 .014 baseline +  all critical questions?
.858 .866 .014  Prompt B    baseline?
.818 .761 --- baseline + specific categories?
.835 .817 .056 baseline +  all critical questions?
.845 .821 .060  Table 3: Performance of essay scoring models with and without argumentation features  Similar observations apply to prompt B.
The causal mechanism category added prediction significantly above e-rater with an increase (?
r = .056).
The model containing the general argu-ment feature (?all critical questions?)
performed slightly better (?
r = .060).
These results suggest that annotation catego-ries based on argumentation schemes contribute additional useful information about essay quality to a strong baseline essay scoring model.
In the next section, we report on preliminary experi-ments testing whether these annotations can be automated, which would almost certainly be nec-essary for practical applications.
6  Argumentation Schemes NLP System We developed an NLP system for automatically identifying the presence of scheme-relevant criti-cal questions in essays, and we evaluated this system with annotated data from the two selected argument prompts.
This addresses the third re-search question: Can we use NLP techniques to train an automated classifier for distinguishingsentences that raise critical questions from sen-tences that contain no critical questions?
6.1  Modeling In this initial development of the NLP system, we focused on the task of predicting whether a sentence raises any critical questions or none (i.e., generic vs. nongeneric).
As such, the task was binary classification at the level of the sen-tence.
The system we developed uses the SKLL tool1 to fit L2-penalized logistic regression mod-els with the following features:  ?
Word n-grams: Binary indicators for the presence of contiguous subsequences of n words in the sentence.
The value of n ranged from 1 to 3.
These features had value 1 if a particular n-gram was present in a sentence and 0 otherwise.
?
word n-grams of the previous and next sen-tences: These are analogous to the word n-gram features for the current sentence.
?
sentence length bins: Binary indicators for whether the sentence is longer than 2t word tokens, where t  ranges from 1 to 10. ?
sentence position: The sentence number di-vided by the number of sentences in text.
?
part of speech tags: Binary indicators for the presence of words with various parts of speech, as predicted by NLTK 2.0.4. ?
prompt overlap: Three features based on lex-ical overlap between the sentence and the prompt for the essay: a) the Jaccard similari-ty between the sets of word n-grams in the sentence and prompt (n = 1, 2, 3), b) the Jac-card similarity between the sets of word uni-grams (i.e., just n = 1) in the sentence and prompt, and c) the Jaccard similarity be-tween the sets of ?content?
word unigrams in the sentence and prompt (for this, content words were defined as word tokens that con-tained only numbers and letters and did not appear in NLTK?s English stopword list).
6.2  Experiments For these experiments, we used the training and testing sets described in Section 5.
We trained models on the training data for each prompt in-dividually and on the combination of the training data for both prompts.
To measure generalization across prompts, we tested these models on the testing data for each prompt and on the combina-                                                1 https://github.com/EducationalTestingService/skll75tion of the testing data for the two prompts.
We evaluated performance in terms of unweighted Cohen?s kappa.
The results are in Table 4.
Training Testing Kappa combined combined .438 Prompt A  .350 Prompt B  .346 combined Prompt A .379 Prompt A  .410 Prompt B  .217 combined Prompt B .498 Prompt A  .285 Prompt B  .478  Table 4: Performance of the NLP Model  The model trained on data from both prompts performed relatively well compared to the other models.
For the testing data for prompt B, the combined model outperformed the model trained on just data from prompt B.
However, the prompt-specific model for prompt A slightly outperformed the combined model on the testing data for prompt A.
Although the performance of models trained with data from one prompt and tested with data from another prompt did not perform as well, there is evidence of some generalization across prompts.
The model trained on data from prompt B and tested on data from prompt A had kappa = 0.217; the model trained on data from prompt A and tested on data from prompt B had kappa = 0.285.
Of course, these human-machine agree-ment values were somewhat lower than human-human agreement values (0.572 and 0.604, re-spectively), leaving substantial room for im-provement in future work.
We also examined the most strongly weighted features in the combined model.
We observed that multiple hedge words (e.g., ?perhaps?, ?may?)
had positive weights, which associated with the ?generic?
class.
We also observed that words related to argumentation (e.g., ?conclu-sions?, ?questions?)
had negative weights, which associated them with the nongeneric class, as one would expect.
One issue of concern is that some words related to the specific topics discussed in the prompts received high weights as well, which may limit generalizability.7  Conclusion Our research focused on identification and classi-fication of argumentation schemes in argumenta-tive text.
We developed annotation protocols that capture various argumentation schemes.
The an-notation categories corresponded to scheme-relevant critical questions, and for text segments that do not contain any critical questions, we as-signed a ?generic?
category.
In this paper, we reported the results based on an annotation of a large pool of student essays (both high-quality and low-quality essays).
Results showed that most of the common annotation categories (e.g.
causal mechanism, alternative plan) can be ap-plied reliably by the four annotators.
However, the annotation work is labor-intensive.
People need to receive sufficient train-ing to apply the approach consistently.
They must not only identify meaningful chunks of tex-tual information but also assign the right annota-tion category label for the selected text.
Despite these complexities, it is a worthwhile investiga-tion.
Developing a systematic classification of argument structures not only plays a critical role in this project, but also has a potential contribu-tion to other assessments on argumentation skills aligned with the Common Core State Standards.
This work would help improve the current auto-mated scoring techniques for argumentative es-says because this annotation approach takes into account the argument structure and its content.
We ran regression analyses and found that manual annotations grounded in the argumenta-tion schemes theory predict essay quality.
Our data showed that features based on manual ar-gument scheme annotations significantly con-tributed to models of essay scores for both prompts.
This is probably because our approach focused on the core of argumentation, rather than surface or word-level features (e.g., mechanics, grammar, usage, style, essay organization, and vocabulary) examined by the baseline model.
Furthermore, we have implemented an auto-mated system for predicting the human annota-tions.
This system focused only on predicting whether or not a sentence raises any critical questions (i.e., generic vs. nongeneric).
In the future, we plan to test whether features based on automated annotations make contributions to essay scoring models that are similar to the con-tributions of manual annotations.
We also plan to work on detecting specific critical questions and adding additional features, such as features from Feng and Hirst (2011).76Acknowledgements  We would like to thank Keelan Evanini, Jill Burstein, Aoife Cahill, and the anonymous re-viewers of this paper for their helpful comments.
We would also like to thank Michael Flor for helping set up the annotation interface, and Melissa Lopez, Matthew Mulholland, Patrick Houghton, and Laura Ridolfi for annotating the data.
References Katie Atkinson, Trevor Bench-Capon, and Peter McBurney.
2006.
Computational representation of practical argument.
Synthese, 152: 157-206.
Burstein, Jill, Karen Kukich, Susanne Wolff, Chi Lu, Martin Chodorow, Lisa Braden-Harder, and Mary Dee Harris.
1998.
"Automated scoring using a hybrid feature identification technique."
In Pro-ceedings of the 17th international conference on Computational linguistics-Volume 1, pp.
206-210.
Association for Computational Linguistics.
Jill Burstein, Daniel Marcu, and Kevin Knight.
2003.
Finding the WRITE stuff: Automatic identification of discourse structure in student essays.
IEEE Transactions on Intelligent Systems, 18(1): 32-39.
Jill Burstein, Joel Tetreault, and Nitin Madnani.
2013.
The e-rater automated essay scoring system.
In Sermis, M. D. and Burstein, J.
(eds.
), Handbook of Automated Essay Evaluation: Current Applications and New Directions (pp.
55-67).
New York: Routledge.
Vanessa W. Feng and Graeme Hirst.
2011.
Classify-ing arguments by scheme.
Proceedings of the 49th Annual Meeting of the Association for Computa-tional Linguistics, Portland, OR.
Ralph P. Ferretti, William E. Lewis, and Scott An-drews-Weckerly.
2009.
Do goals affect the struc-ture of students?
argumentative writing strategies?
Journal of Educational Psychology, 101: 577-589.
Klaus Krippendorff.
1980.
Content Analysis: An In-troduction to its Methodology.
Beverly Hills, CA : Sage Publications.Mann, William C., and Sandra A. Thompson.
1988.
"Rhetorical structure theory: Toward a functional theory of text organization."
Text 8(3): 243-281.
Nitin Madnani, Michael Heilman, Joel Tetreault, and Martin Chodorow.
2012.
Identifying High Level Organizational Elements in Argumentative Dis-course.
Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.
(pp.
20-28).
Association for Com-putational Linguistics.Raquel Mochales and Asgje Ieven.
2009.
Creating an argumentation corpus: do theories apply to real ar-guments?
: a case study on the legal argumentation of the ECHR.
In ICAIL ?09: Proceedings of the 12th International Conference on Artificial Intelli-gence and Law.
Michael Nussbaum.
2011.
Argumentation, dialogue theory, and probability modeling: alternative frameworks for argumentation research in educa-tion.
Educational Psychologist, 46: 84-106.
Nussbaum, E. M. and Edwards, O.V.
(2011).
Critical questions and argument stratagems: A framework for enhancing and analyzing students?
reasoning practices.
Journal of the Learning Sciences, 20, 443-488.
Palau, R.M.
and Moens, M. F. 2009.
Automatic ar-gument detection and its role in law and the seman-tic web.
In Proceedings of the 2009 conference on law, ontologies and the semantic web.
IOS Press, Amsterdam, The Netherlands.Pendar, Nick, and Elena Cotos.
2008.
"Automatic identification of discourse moves in scientific article introductions."
In Proceedings of the Third Workshop on Innova-tive Use of NLP for Building Educational Applica-tions, pp.
62-70.
Association for Computational Linguistics.
Rahwan, I., Banihashemi, B., Reed, C. Walton, D., and Abdallah, S. (2010).
Representing and classi-fying arguments on the semantic web.
The Knowledge Engineering Review.
Rienks, R., Heylen, D., and Van der Weijden, E. 2005.
Argument diagramming of meeting conver-sations.
In A. Vinciarelli, J. Odobez (Ed.
), Pro-ceedings of Multimodal Multiparty Meeting Pro-cessing, Workshop at the 7th International Confer-ence on Multimodal Interfaces (pp.
85?92).
Trento, Italy.
Yi Song and Ralph P. Ferretti.
2013.
Teaching critical questions about argumentation through the revising process: Effects of strategy instruction on college students?
argumentative essays.
Reading and Writ-ing: An Interdisciplinary Journal, 26(1): 67-90.
Stephen E. Toulmin.
1958.
The uses of argument.
Cambridge University Press, Cambridge, UK.
Frans H. van Eemeren and Rob Grootendorst.
1992.
Argumentation, communication, and fallacies: A pragma-dialectical perspective.
Mahwah, NJ: Erl-baum.
Frans H. van Eemeren and Rob Grootendorst.
2004.
A systematic theory of argumentation: A pragma-dialectical approach.
Cambridge, UK: Cambridge University Press.
Verbree, D., Rienks, H., and Heylen, D. (2006).
First Steps Towards the Automatic Construction of Ar-gument-Diagrams from Real Discussions.
In Pro-77ceedings of the 2006 conference on Computational Models of Argument: Proceedings of COMMA 2006.
IOS Press, Amsterdam, The Netherlands.
Douglas N. Walton.
1996.
Argumentation schemes for presumptive reasoning.
Mahwah, NJ: Lawrence Erlbaum.
Douglas N. Walton, Chris Reed, and Fabrizio Macagno.
2008.
Argumentation schemes.
New York, NY: Cambridge University Press.78
