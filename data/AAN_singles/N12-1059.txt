2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 528?532,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsTrait-Based Hypothesis Selection For Machine TranslationJacob Devlin and Spyros MatsoukasRaytheon BBN Technologies, 10 Moulton St, Cambridge, MA 02138, USA{jdevlin,smatsouk}@bbn.comAbstractIn the area of machine translation (MT) sys-tem combination, previous work on generat-ing input hypotheses has focused on varying acore aspect of the MT system, such as the de-coding algorithm or alignment algorithm.
Inthis paper, we propose a new method for gen-erating diverse hypotheses from a single MTsystem using traits.
These traits are simpleproperties of the MT output such as ?aver-age output length?
and ?average rule length.
?Our method is designed to select hypotheseswhich vary in trait value but do not signif-icantly degrade in BLEU score.
These hy-potheses can be combined using standard sys-tem combination techniques to produce a 1.2-1.5 BLEU gain on the Arabic-English NISTMT06/MT08 translation task.1 IntroductionIn Machine Translation (MT), the output from mul-tiple decoding systems can be used to create a newoutput which is better than any single input system,using a procedure known as system combination.Normally, the input systems are generated byvarying some important aspect of the MT system,such as the alignment algorithm (Xu and Rosti,This work was supported by DARPA/I2O Contract No.HR0011-12-C-0014 under the BOLT program (Approved forPublic Release, Distribution Unlimited).
The views, opinions,and/or findings contained in this article are those of the au-thor and should not be interpreted as representing the officialviews or policies, either expressed or implied, of the DefenseAdvanced Research Projects Agency or the Department of De-fense.2010) or tokenization algorithm (de Gispert et al,2009).
Unfortunately, creating novel algorithms toperform some important aspect of MT decoding isobviously quite challenging.
Thus, it is difficult toincrease the number of input systems in a meaning-ful way.In this paper, we show it is possible to creatediverse input hypotheses for combination withoutmaking any algorithmic changes.
Instead, we usetraits, which are very simple attributes of the MToutput, such as ?output length?
and ?average rulelength.?
Our basic procedure is to intelligently se-lect hypotheses from our decoding forest which varyin trait value, but have minimal BLEU degradationcompared to our baseline.
We then combine theseto produce a substantial gain.
Note that all of thehypotheses are generated from a single decode of asingle input system.Additionally, our method is completely compati-ble with multi-system combination, since our proce-dure can be applied to each input system, and thenthese systems can be combined as normal.Methods for automatically creating diverse hy-potheses from a single system have been exploredin speech recognition (Siohan et al, 2005), but weknow of no analogous work applied to machinetranslation.
Our procedure does share some surfacesimilarities with techniques such as variational de-coding (VD) (Li et al, 2009), but the goal in thosetechniques is to find output which is consistent withthe entire forest, rather than to select hypotheseswith particular attributes.
In fact, VD can be appliedin conjunction by running VD on the rescored forest528for each trait condition.12 Description of MT SystemOur machine translation system is a string-to-dependency hierarchical decoder based on (Shen etal., 2008) and (Chiang, 2007).
Bottom-up chartparsing is performed to produce a shared forest ofderivations.
The decoder uses a log-linear transla-tion model, so the score of derivation d is definedas:Sd(~w) =m?i=1wi?r?R(d)Fri (1)where R(d) is the set of translation rules that makeup derivation d, m is the number of features, Friis the score of the ith feature in rule r, and wi isthe weight of feature i.
This weight vector is opti-mized discriminatively to maximize BLEU score ona tuning set, using the Expected-BLEU optimizationprocedure (Rosti et al, 2010).Our decoder uses all of the standard statistical MTfeatures, such as the language model, rule probabil-ities, and lexical probabilities.
Additionally, we use50,000 sparse, binary-valued features such as ?Is thebi-gram ?united states?
present in the output?
?, basedon (Chiang et al, 2009).
We use a 3-gram LM fordecoding and a 5-gram LM for rescoring.3 Trait FeaturesAn MT trait represents a high-level property of theMT output.The traits used in this paper are:?
Null Source Words ?
The percentage of sourcecontent words which align to null, i.e., are nottranslated.?
Source Reorder ?
The percentage of sourceterminals/non-terminals which cross alignmentlinks inside their decoding rule.?
Ngram Frequency ?
The percentage of target 3-grams which are seen more than 10 times in themonolingual training.?
Rule Frequency ?
The percentage of ruleswhich are seen more than 3 times in the par-allel training.1We do not use VD here because we have not found it to bebeneficial to our system.?
Rule Length ?
The average number of targetwords per rule.?
Output Length ?
The ratio of the number of tar-get words in the MT output divided by the num-ber of source words in the input.?
High Lex Prob ?
The percentage of sourcewords which have a lexical translation proba-bility greater than 0.1.Each trait can be represented as the ratio of two lin-ear decoding features.
For example, for the OutputLength trait, the ?numerator?
feature is the numberof target words in the hypothesis, while the ?denom-inator?
feature is the number of source words in theinput sentence.
We can sum these feature scoresover a test set, and the resulting quotient is the Out-put Length for that set.Intuitively, each trait is associated with a par-ticular tradeoff, such as fluency/adequacy or preci-sion/recall.
For example, when MT performance ismaximized, shorter output tends to have higher pre-cision but lower recall than longer output.
For theNgram Frequency trait, a greater percentage of high-frequency n-grams tends to result in more fluent butless adequate output.
Similar intuitive justificationsshould be evident for the remaining traits.4 Hypothesis GenerationThe main goal of this work is to generate additionalhypotheses which vary in trait values, while mini-mizing degradation to the BLEU score.
So, imaginethat we have some baseline MT output.
Then, wewant to generate a second set of hypotheses whichhave maximal BLEU score, subject to the constraintthat the output must be 5% shorter.2The question then becomes how to figure outwhich 5% of words should be removed.
Rather thanattempting to do this with a new algorithm, we sim-ply let our existing MT models do it for us, usingour standard optimization procedure.
This is the es-sential purpose of the trait features ?
using the Out-put Length feature, the optimizer has a ?knob?
withwhich it can control the trait value independentlyof everything else.3 Thus, the new hypotheses that2Note that the trait value is always aggregated over the entireset, and not computed sentence-by-sentence.3A feature representing the number of words already existsin our baseline system, but no such feature exists for the other 6529we select are ?optimal?
in terms of our existing MTmodel probabilities, but have trait values which varyfrom the baseline in a precise way.4.1 Optimization FunctionOur normal optimization procedure uses n-best-based Expected-BLEU tuning (Rosti et al,2010), which is a differentiable approximation ofMaximum-BLEU tuning.
To ?target?
a particulartrait value, we add a second term equal to thesquared error between the current trait value andthe target trait value.
Our modified optimizationfunction which we seek to maximize is then:Obj(~w) = ExpBLEU(~w)?
?(N(~w)D(~w)?
??
)2where ~w is the MT feature weight vector, ?
is theweight of the trait term, ?
is the baseline value ofthe trait, and ?
is the ?target?
trait multiplier, N(~w)is the expected-value of the numerator feature, andD(~w) is the expected value of the denominator fea-ture.To give an example, imagine that for our baselinetune set the Output Length ratio is 1.2, and we wantto create a hypothesis set with 5% fewer words.
Inthat case, we would set ?
= 1.2 and ?
= 0.95, so thetarget trait value is 1.14.
We fix the free parameter?
to 10, which forces the optimized trait value to bevery close to the target.4The trait-value functions N(~w) and D(~w) arecomputed as standard expected value functions, e.g.
:N(~w) =?i?jpij(~w)Nijwhere pij(~w) is the posterior probability of the jthhypothesis of sentence i, and Nij is the value of thenumerator feature for hypothesis ij.54.2 Meta-OptimizationIt is somewhat problematic to use a fixed multiplier?
on all of the traits, since on some traits it maycause a larger degradation than others.
So, we takethe reverse approach ?
for some targeted BLEU losstraits.4Note that theExpBLEU(~w) is raw BLEU not BLEU per-centage, i.e., it?s 0.4528 not 45.285pij(~w) is computed the same way as in ExpBLEU(~w).See (Rosti et al, 2010) for details.
?, we find the maximum (or minimum) value of ?which causes a loss no greater than ?, as computedon a held-out portion of the tune set.6 Here, we findthe maximum and minimum trait value for ?
= 0.5and ?
= 2.0, resulting in 4 sets of weights per trait.We can find the optimal ?
for each ?
by perform-ing a binary search on ?
, where we run our optimiza-tion procedure and then compute the BLEU loss ateach iteration.4.3 Forest-Based OptimizationSince we have 7 traits, and we generate 4 sets ofweights per trait, we have 28 ?systems?
to combine.Obviously, running 28 full decodes on each new testsentence is highly undesirable.We resolve this issue by using our baseline deriva-tion forest for both optimization and hypothesis gen-eration.
We perform a single round of decoding togenerate a forest, and then perform iterative n-bestoptimization by rescoring the forest rather than re-decoding from scratch.
7 We constrain the 50,000sparse feature weights to be fixed at their baselinevalues, to prevent over-fitting.Once the weight sets are generated, the hypothe-ses for each trait condition can be generated byrescoring the forest inside of the decoder.
Therefore,all 28 trait hypotheses can be generated for almostno cost over a single decode.It should be noted we have found it beneficial torelax our MT pruning parameters in order to cre-ate a larger forest.
This results in decoding whichis roughly 2x-3x as slow as the baseline, and re-quires storing the larger forest in memory.
However,we have found that the procedure still works welleven with the standard pruning parameters.
Addi-tionally, we are investigating methods for diversify-ing the forest with less of a slowdown to decoding.5 CombinationOnce the different trait hypotheses have been gen-erated, system combination can be performed usingany method.Here, we use a confusion network decoder basedon (Rosti et al, 2010).
The basic procedure is to6For example if the held-out baseline BLEU is 40.0 and ?
=0.5, the BLEU after trait optimization can be no less than 39.5.7Forest-based optimization such as (Pauls et al, 2009) couldbe used instead.530select one hypothesis as the ?skeleton?
and then in-crementally align the remaining hypotheses to createa confusion network.
The confusion network is de-coded using an arc-level confidence score for eachinput system and a language model, the weights forwhich are estimated discriminatively to maximizeBLEU.6 ResultsWe present MT results in Table 1.
Our experimen-tal setup is compatible with the NIST MT08 con-strained track.
We trained our translation model on35 million words of parallel data and our languagemodel on 3.8 billion words of monolingual data.
Weuse a portion of MT02-05 for tuning the MT baselineand the trait systems, and another portion of MT02-05 for tuning system combination.We present results on Arabic-English MT06-newswire and MT08-eval.
The systems were tunedand evaluated using IBM-BLEU.
Our baseline sys-tem is 1.5 BLEU better than the best result from theNIST M08 evaluation.For the Trait Feats condition, we simply added thenumerator and denominator features for all 7 traits tothe baseline system and re-optimized.8 Somewhatsurprisingly, this produces an 0.5-0.7 BLEU gain onits own.
In this condition, although we do not targetany particular trait values, the optimizer will natu-rally fine-tune the trait values to whatever is optimalfor BLEU score.
For example, the MT08 baselinevalue of Source Reorder is 0.307, while for the TraitFeats it is 0.330, so the system determined it is ?op-timal?
to have 7.5% (0.330/0.307) more re-orderingthan the baseline.For the Trait Comb condition, we generated 28trait hypothesis sets using the decoding forest fromthe Trait Feats condition.
We combined these withthe Trait Feats output using consensus network de-coding.
This produces an additional 0.8 BLEU gain,resulting in a 1.2-1.5 BLEU gain over the baseline.We also present another condition, n-best Comb,where we perform confusion network combinationon the 28-best hypotheses from Trait Feats.
Thisrepresents the simplest and most trivial method ofhypothesis selection.
We observe no gain in BLEUon this condition.
Other simple methods of hy-8Including the 50k sparse features.potheses selection, such as optimizing systems to be?different?
from one another (i.e., have high inter-system TER), also produced no gain over the singlesystem.
We include these results simply to demon-strate that it is not trivial to select hypotheses from asingle system which produce a significant improve-ment in from system combination.MT06 nw MT08 evalBLEU Len BLEU LenBaseline 55.11 99.1% 46.75 96.1%Trait Feats 55.79?
99.3% 47.23?
96.0%+n-best Comb 55.65 99.3% 47.24 96.2%+Trait Comb 56.65??
99.3% 48.00??
96.2%Table 1: Results on Arabic-English MT.
?
= Significantimprovement at 95% confidence, as defined by (Koehn,2004).
??
= Significant improvement at 99.9% confi-dence.
BLEU = IBM-BLEU score.
Len = Hypothesis-to-reference length ratio.7 Conclusions and Future WorkWe demonstrated a method of intelligently select-ing hypotheses from a decoding forest which can becombined with the baseline hypotheses to producea significant gain in BLEU score.
In the future, weplan to explore more trait types and alternate meth-ods of system combination.One possible application of this work is in fieldedtranslation systems.
Because our method produceshigh-quality complementary hypotheses at a lowcomputational cost, the system could present theseto the user as alternate translations.
Going further,a user could prefer a particular output type, such asthe fluency-tuned condition, and set that to be theirdefault translation.The major open question is how our trait-basedcombination interacts with multi-system combina-tion.
Imagine there are three different types of de-coders which can be combined to produce some gainin the baseline condition.
If you independently im-prove all three using trait-based combination, willthe relative gain from multi-system combination bereduced?
Or can you jointly combine all of the traithypotheses and get an even greater relative gain?
Weplan to thoroughly explore this in the future.531ReferencesD.
Chiang, K. Knight, and W. Wang.
2009.
11,001 newfeatures for statistical machine translation.
In NAACL,pages 218?226.D.
Chiang.
2007.
Hierarchical phrase-based translation.Computational Linguistics, 33(2):201?228.A.
de Gispert, S. Virpioja, M. Kurimo, and W. Byrne.2009.
Minimum Bayes risk combination of translationhypotheses from alternative morphological decompo-sitions.
In NAACL, pages 73?76.P.
Koehn.
2004.
Pharaoh: A beam search decoderfor phrase-based statistical machine translation mod-els.
In AMTA, pages 115?124.Z.
Li, J. Eisner, and S. Khudanpur.
2009.
Variationaldecoding for statistical machine translation.
In ACL,pages 593?601.A.
Pauls, J. DeNero, and D. Klein.
2009.
Consensustraining for consensus decoding in machine transla-tion.
In EMNLP, pages 1418?1427.A.
Rosti, B. Zhang, S. Matsoukas, and R. Schwartz.2010.
BBN system description for WMT10 systemcombination task.
In WMT/MetricsMATR, pages 321?326.L.
Shen, J. Xu, and R. Weischedel.
2008.
A new string-to-dependency machine translation algorithm with atarget dependency language model.
In ACL-HLT,pages 577?585.O.
Siohan, B. Ramabhadran, and B. Kingsbury.
2005.Constructing ensembles of ASR systems using ran-domized decision trees.
In ICASSP.J.
Xu and A. Rosti.
2010.
Combining unsupervised andsupervised alignments for MT: An empirical study.
InEMNLP, pages 667?673.532
