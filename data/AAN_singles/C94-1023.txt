AUTOMATIC MODEI~ REFINEMENTwith an application to taggingYi-Chung Lin, Tung-lIui Chiang and Keh-Yih SuDepartment of Electrical En,@Teering, National 7X'ing ltua University,Hsinchu, 7?liwan 300, Republic of ChinaABSTRACTStatistical NLP models usually only considercoarse information and very restricted context omake the estimation of parameters feasible.
Toreduce the modeling error introduced by a sim-plified probabilistic model, the Classitication andRegression Tree (CART) method was adopted inthis paper to select more discriminative f aturesfor automatic model refinement.
Because thefeatures are adopted ependently during split-ting the classification tree in CART, the numberof training data in each terminal node is small,which makes the labeling process of terminalnodes not robust.
This over-tuning phenome-non cannot be completely removed by cross-validation process (i.e., pruning process).
Aprobabilistic lassification model based on theselected iscriminative f atures is thtls proposedto use the training data more efficiently.
In tag-ging the Brown Corpus, our probabilistic lassi-fication model reduces the error rate of the top10 error dominant words from 5.71% to 4.35%,which shows 23.82% improvement over the un-refined model.l.
INTRODUCTIONTo automatically acquire knowledge fromcorpora, statistical methods are widely used re-cently (Church, 1989; Chiang, Lin & Su, 1992;Su, Chang & Lin, 1992).
The perfonnanceof a probabilistic model is affected by the es-timation error due to insufficient raining dataand the modeling error due to lacking completeknowledge of the problem to be conquered.
Inthe literature, several smoothing methods (Good,1953; Katz, 1987) have been used to effectivelyreduce the estimation error.
On the contrary,the problem of reducing modeling error is lessstudied.Probabilistic models are usually simplifiedto make the estimation of parameters feasible.However, some important information may belost while simplifying a model.
For example,using the contextual words, instead of contextualparts of speech, enhances the prediction powerfor tagging parts of speech.
But, unfortunately,reducing the m(?teling error by increasing thedegree of model granularity is usually accompa-nied by a large estimation error if there is notenough training data.tIowever, if only the discriminative f aturesarc involved (i.e., only those important param-eters are used), modeling error could be sig-niIicantly reduced without using a large co>pus.
Those discriminative f atures usually varyfor different words, and it would be very time-consuming to induce such features from the cor-pus manually.
An algorithm for automaticallyextracting the discriminative f atures from a cor-pus is rims highly demanded.
In this paper,the Classification and Regression Tree (CARl')method (Breiman, Friedman, Olshen & Stone,1984) is first used to extract he discriminativefeatures, l lowever, CAP, T basically regards allselected features as jointly dependent.
Nodesin different branches are trained with differentsets of data, and the available training data of anode becomes less and less while CART asksmore and more questions.
"FhereR)re, CARTcan easily split and prune the classification treeto fit the training data and the cross-validationdata respectively.
The refinement model builtby CART tends to be over-tt, ned and its perfor-mance is consequently not robust.
A probabilis-tic classification m(,lel is, therefore, proposedto construct a more robust classification model.The experimental results how that this proposedmodel reduces the error rate of the top 10 errordominant words fi'om 5.71% to 4.35% (23.82%148error reduction rate) while CART only reducesthe error rate to 4.67% (18.21% error reductionrate).2.
PROBABILIS'FIC TA(\]GEl lSince part of speech tagging plays an im-portant role in the field of natural anguage pro-cessing (Ctmrch, 1989), it is used to evaluate theperformance of various approaches in tiffs paper.Tagging problem can be formulated (Church,1989;  IAn, Chiang & Su, 1992) as.7~ i= i  ('1 (1)where ~ is the category sequence selected bythe tagging model, wi is the i-th word, ci isthe possible corresponding category for the i-thword and c't ~ is tim Stiort-hand notation of tilecategory sequence Cl~ c2~ " ?
?
,  on.The Brown Corpus is used as the test bedfor tagging in this paper.
After prepr(xccssingthe Brown Corpus, a corpus of 1,050,(X)4 wordsin 50,(X)0 sentences i constructed.
It contains54,031 different words and 83 different ags (ig-noring the four designator tags "FW," " I l l , ""NC" and "TIJ' (Francis & KuSera, 1982)).
Totrain and test the model, the whole corpus is di-vided into the training set and the testing set.The v-foM cross-validation method (Breiman etal., 1984), where v is set to 10 in this paper,is adopted to reduce the error ira performanceevaluation.
The average number of words in thetraining sets and the testing sets are 945,004 (in45,000 sentences) and 105,000 (in 5,000 sen-tences) respectively.After applying back-off smoothing (Katz1987)  and robust learning (Lin et al, 1992)  onEquation (1) to reduce the estimation error, atagger with 1.87% error rate in the testing set isthen obtained.
Although the error rate of overalltesting set is small, many words are still withhigh error rates.
For instance, the error rate ofthe word "that" is 9.08% and the error ,ate ofthe word "out" is 21.09%.
To effectively im-prove accuracy over these words, it is suggestedin this paper that the tagging model should berefined.3.
MOI)EI, RE IqNEMENTFor not having enough training data, ttstt-ally only coarse infornmtion and rather limitedcontext are used in probabilistic models.
Somediscriminative f .
'm~res, therefore, may be sacri-riced to make the estimation of parameters lea-sine.
For example, compared to the tag-levelcontextual information used in a bigram or a tri-gt'am m(xlel, the word-level contextual inR)rma-tion provides more prediction power for taggingparts of speech, t lowever, even the simplestword-level contextual information (i.e., word bi-gram) requires a large number of parameters(about 3 billion in our task).
Esthnating sucha large ,mmber of parameters requires a vet',\]huge corpus and is far beyond the size of theP, rown Corpus.
Thus, the word-level contextualinformation is usually abandoned.To reduce the modeling error introduced bya simplified probabilistic model, one appealingapproach is to extract only the discriminativefeatures for those error dominant words.
In thisway, one can reduce the error rate without en-hu'ging the corpus size.
I)ifferent error dominantwords, however, might be associated with dif-ferent sets of discriminative features.
To inducethose discriminative f atures for each word froma corpus by hand is very tinae-consuming.
Auto-matically acquiring those features directly fl 'oma corpus is thus highly desirable.
In this section,the Classification and Regression Tree (CAP, T)method (P, reiman et al, 1984) is adopted to aulo-matically extract he discriminative fcatures ,'rodresolve the lexical ambigt, ity.CART, however, requires a la,'ge amount oftraining data and validation data, because it re-gards all those selected features as jointly de-pendent.
The characteristic of being jointly de-pendent comes from the splitting process, whichsplits those children nodes only based on thedata of their parent nodes.
As a result, CARTis easily tuned to fit tim training data and vali-dation data.
Its performance is thus not robust.A probabilistic lassification apl)roach is there-fore proposed to build robust retinement modelswith limited training data.149Table 1.
Some statistics of the top 10 error dominant words.Proportion toWord Frequency (%) Error rate (%) overall errors (%)that 0.895 9.08 4.33out 0.170 21.09 1.91to 2.259 1.43 1.72as 0.603 4.97 1.60than 0.160 17.17 1.46more 0.188 12.74 1.28about 0.147 15.24 1.20for 0.785 2.77 1.17one 0.252 8.26 1.11little 0.068 29.30 1.06TOTAL 5.526 5.71 16.843.1.
The error dominant wordsTo select those words which are worth formodel refinement, the top 10 error dominantwords ate ordered according to their contributionto overall errors, as listed in Table 1.
Thesecond column shows their relative fi'equenciesin the Brown Corpus.
The third column showsthe error rates of those words tagged by theprobabilistic tagger described in section 2.
Thelast column shows the contribution of the errorsof each word to the overall errors.
The lastrow indicates that the top 10 error dominantwords constitute 5.53% of the testing corpus andcontribute 16.84% of the errors in the testingcorpus.
Their averaged error rate is 5.71% (i.e.,the ratio of the total errors of these words to theirtotal occurrence times in the testing corpus).3.2.
Feature selection"lk~ reduce modeling error, more discrimina-tive infommtion should be incorporated in tag-ging.
In addition to the trigram context infor-mation of lexical category, the features in Table2 are considered to be potentially discriminativefor choosing the correct lexical category of agiven word.Since the size of the parameters will be hugeif all the features in Table 2 are jointly consid-ered, it is not suitable to incorporate all of them.Actually only some of the listed features are re-ally discriminative for a particuhtr word.
Forinstance, when we want to tag the word "out,"we do not care whether lhe word behind it (i.e.,the right-1 woM) is "book," "money" or "win-Table 2.
The potentially discriminative featu,'es.?
Tim left-2, left-l, right-1 and right-2 categories (denoted as L~t.g(2),Lcatg(1),/2cat.g(1) and \]l'catg(2))?
The left-1 and right-1 words (denoted as Lwo,.d(1) and \]{,,,,,.d(1))?
The?
The?
The?
The?
The?
Timdistance from the left period (gp,.,.iod)distance to the right period (\]~'t,,'riod)distance from the nearest left noun (?notm)distance tothe nearest right noun (/~,ot,,,)distance from the nearest left verb (Lwrb)distance to the nearest right verb (J?,.,,rb)150Table 3.
The improvement in the testing set after using CART as the refinedword model.
Value in parenthesis indicates lhc error rate of the validation set.Error rate of F.rror rate of ReductionWord the 1st stage (%) using CART (%) rate (%)that 9.08 8.69 (7.47) 4.3(/ (17.73)out 21.09 8.04 (7.13)  6t.88 (66.19)to 1.43 1.36 (1.12) 4.90 (21.68)as 4.97 3.33 (2.82) 33.00 (43.26)than 17.17 13.83 (11.24) 19.45 (34.54)more 12.74 10.96 (9.35) 13.97 (26.61)about 15.24 11.33 (9 .94)  25.66 (34.78)for 2.77 2.55 (2.28) 7.94 (17.69)one 8.26 6.48 (5.94) 21.55 (28.09)little 29.30 30.00 (25.16) -2.39 (14.13)7?7AL 5.71 4.67( 4.00) 18.21 (29.95)dow;" we only care whether the right-1 word is"of."
Thus, in this section, the CART (Breimanet al, 1984) method is used to extract the re-ally discriminative f atures fi'om the fcatu,e set.The error rate criterion is adopted to measurethe impurity of a node in the classification tree.For every error dominant word, its 4/5 trainingtokens are used to sp!it the classification tree;the remaining 1/5 training tokens (not the test-ing tokens) are used to prone that tree.
Then,all the questions asked by the pruned tree areconsidered to be the discriminative features.3.3.
CART classilicalion modelIn our task, it two-stage approach is adoptedto tag parts of speech.
The first stage is theprobabilistic tagger described in section 2, whichprovides the most likely category sequence ofthe input sentence.
The second stage consists ofthe refined word models of the error dominantwords.
In this stage, the p,'uned classificationtree is used to re-tag the part of speech.
Theresults in the testing set are shown in ~\[hble 3.In the table, the second column gives the errorrates of the error dominant words in the tirststage.
The third cohnnn gives the error ratesafter using CART to re-tag those words, and thelast column gives the corresponding ,eductionrates.
In parenthesis it gives the performancein the validation set.
The last row in "lable 3shows that the i'efined models built by CARTcan reduce the 18.21% o\[' error rate for the 10ClT/.
)I" dominant words.
Only the performance ofthe word "little" deteriorated.
This is due to therobusmess problem between the cross-validationdata and the testing data, which is induced hy therare occurrence of the discriminative features.3.4.
Prolmbillstic classilication model\]~ecause discriminative features are adopteddependently, CART can easily classify the train-ing data an(l usually introduce the problem ofover--tuning.
Besides, due to the wu'iation be-tween the validation data and the testing data, thepruning process cannot effectively diminish theproblem of over-tuning intr{?iuced while grow-ing the classification tree.
Thus, a probabilisticclassification model, which uses all the featuresselected by CART in an independent way, is pro-posed in this section to robustly re-tag the lexicalcategories of the error dominant words.151Table 4.
The 11 questions asked by theclassification tree for the word "than."?
QI,1 : L~atg(2) = "RB" ??
Q1,2 : Lc,~tg(2) = " IN"  ??
Q2,1 : Lcatg(1) = "AP" ??
Qa,1 : Rcatg(1) = "CD" ??
Qa,2 : Rcatg(1) = "JJ" ??
Q4,1 : I?~catg(2) = "JJ" ??
Os,1 : Lwo~d(1) = "rather" ??
Q6,~ : Rwo,.d(1) = "the" ??
Oa,.~ : R,vo,.a(1) = "with" ??
@7,1 : Lperiod ~ 2 ??
(28,1 : Lperiod ~ 6 ?To use the probabilistic htssification model,feature vectors are tirst constructed according tothe questions asked by the pruned classiticationtree.
Assume that the 11 questions in Table 4are asked by the classification tree for the word"than."
Every occurrence of "than" in the cor-pus is then accompanied by an 8-dimensionalfeature vector, F = \ [ f i , .
.
.
,  fs\].
The elementsof the feature vector are obtained by the follow-ing rule.f j, if Qi,j is true;k \ 0, otherwise.
(2)Notice that Ql,1 and Q1,2 are merged into thesame random variable because both of them askabout what the left-2 category is.After constructing the feature vectors, theproblem becomes to find a most probable cate-gory according to the given feature vector andit can be formulated as= a rgmax_P(cI r , , .
.
.
,  D,), (3)(2where c is a possible tag for the word to be re-tagged.
Assume thatP((21fl,", f,,)= I ' ( .1, , .
.
.
,  ./;, I().
r((2)s , ( f , , .
.
.
,  j;,), P((2),=,ie\[ s'(f~lc) s (.t.~777 ./..,,)(4)The probabilistic htssilication model (PCM) isthen defined as~ ;~.,<m~,~ i  P(f~I~)' _r'(~).
(5),,5: i:-1The estimation and learning processes of thePCM approach are generally more robust.
Asstated before, CAP, T regards all selected featuresIts jointly dependent.
The available training datafor a node become less its more questions areasked.
On the contrary, due to the conditionalindependent assumption for P ( f l , ' " , .
/ ; , \ [ c )  inEquation (4), every p,'lrameter of PCM can betrained by the whole training data, and therefore,the estimation and learning pr(xeesses are morerobust.Furthermore, every feature of PCM shouldbe weighted to retlect its discriminant powerbecause PCM regards all features of differentbranches in a tree its conditionally independent.Directly using these features without weightingcannot lead to good resuhs.
The weighting effectcan be implicitly achieved by adaptive learning.4.
RESUI;I'S AND I)ISCUSSIONAfter learning the model parameters (Amari,1967; Lin et al, I992), the results of usingthe probabilistic lassification model (PCM) arelisted in qable 5.
As shown in the last rows ofqables 3 and 5, the error rate of PCM is smallerthan that of CART in the testing set while theirerror rates in the wflidation set are almost thesame.
The last row of "lhble 5 shows that theerror rate of the 10 error dominant words is re-duced from 5.71% to 4.35% (23.82% reductionrate) by refining the woM models with the PCMapproach.In sumnaary, due to dividing the features intoindependent groups, PCM can use the whole"/52Table 5.
Improvement of using pmbabilistic lassification model (PCM) as the relinedword model.
Value in parenthesis ndicates the error rate of the valklatkm set.Error rate of Error rate {}f ReductionWord the 1st stage (%) using PCM (%) rate (%)that 9.08 7.98 (7.36) 12.11 (18.94)out 21.09 7.60 (7.43) 63.96 (64.77)to 1.43 1.33 (1.12) 6.99 (21.68)as 4.97 2.69 (2.49) 4.5.88 (49.90)than 17.17 13.49 (12.25) 21.43 (28.65)more 12.74 10.15 (9.16) 20.33 (28.10)about 15.24 10.60 (9.57) 30.45 (37.20)for 2.77 2.39 (2.34) 13.72 (15.52)one 8.26 6.19 (5.88) 25.06 (28.81)little 29.30 28.66 (27.63) 2.18 (5.70)TOTAL 5.71 4.35 ( 4.01) 23.82 (29.77)training data to train every feature and henceconstruct a more robust retinement model.
It isbelieved that this proposed probabilistic lassi-tication model (i.e., Equation (5)) can also beapplied to other problems attacked by CART,such as voiced/w)iceless top classilication andend-of-sentence d tection, etc.
(Riley 1989).5.
REFERENCEAmari, S. (1967).
A theory of adaptive pat-tern classitiers.
IEEE Transactions on ElectronicComputers, 16, 299-307.Breiman, L., Friedman, J.
II., Olshen, R. A.
&Stone, C. J.
(1984).
Classification and regres-sion trees.
Wadsworth Inc., Pacific Grove, Cal-ifornia, USA.Chiang, T.
It., Lin, Y. C. & Su, K..Y.
(1992).Syntactic ambiguity resolution using a discrim-ination and robustness oriented adaptive learn-ing algorithm.
In Proc.
of COLING-92, pp.352-358.
Aug 23-28 1992, Nantes, France.Church, K. W. (1989).
A stochastic parts pro-gram and noun phrase parser for unrestrictedtext.
In Proceedings of the IEEE 1989 Inter-national Conference on Acoustics, Speech, andSignal Processing, pp.
695-698.
May 23-261989, Glasgow, U.K..Francis, W. N. & Ku(:era, H. (1982).
Frequencyanalysis of English usage.
Itoughton MifflinCorn p q n y.Good, I. J.
(1953) The population frequencies ofspecies and tim estimation of population param-eters.
Biometrika, 40, 237-264.Katz, S. M. (1987) Estimation of probabilitiestrom sparse data for the language m(?tel compo-nent of a speech recognizer.
IEEE 7)'ansactionson Acoustics, Speech, and Signal Processing, 35,400--401.Lin, Y.-C., Chiang, T.-II.
& Su, K.-Y.
1992)Discrimination o,'icnted pmbabilistic tagging.
InProceedings oJ'ROCLING V, pp.
87-96.
Sep18-20 I992, Taipei, Taiwan, P,.O.C.Riley, M. D. (1989).
Some applications of tree-based modeling to speech and language.
In Pro-ceedings of the Speech anti Natural LanguageWorkshop, pp.
339-352.
Oct 15.-18 1989, CapeCod, Massachusetts, USA.Su, K. Y., Chang, J. S. & Lin, Y. C. (1992).A discriminative approach for ambiguity reso-lution based on a semantic score function.
InProc.
of 1992 International Conference on Spo-ken Language Processing, pp.
149-152.
OctI2-16 1992, Banff, Alberta, Canada.153
