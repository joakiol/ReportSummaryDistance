Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
630?639, Prague, June 2007. c?2007 Association for Computational LinguisticsTreebank Annotation Schemes and Parser Evaluation for GermanInes RehbeinNCLTSchool of Computing, DCU,Dublin, Irelandirehbein@computing.dcu.ieJosef van GenabithNCLT,School of Computing, DCU,Dublin, IrelandIBM Dublin Center for Advanced Studiesjosef@computing.dcu.ieAbstractRecent studies focussed on the questionwhether less-configurational languages likeGerman are harder to parse than English,or whether the lower parsing scores are anartefact of treebank encoding schemes anddata structures, as claimed by Ku?bler et al(2006).
This claim is based on the as-sumption that PARSEVAL metrics fully re-flect parse quality across treebank encodingschemes.
In this paper we present new ex-periments to test this claim.
We use thePARSEVAL metric, the Leaf-Ancestor met-ric as well as a dependency-based evalua-tion, and present novel approaches measur-ing the effect of controlled error insertionon treebank trees and parser output.
Wealso provide extensive past-parsing cross-treebank conversion.
The results of the ex-periments show that, contrary to Ku?bler etal.
(2006), the question whether or not Ger-man is harder to parse than English remainsundecided.1 IntroductionA long-standing and unresolved issue in the pars-ing literature is whether parsing less-configurationallanguages is harder than e.g.
parsing English.
Ger-man is a case in point.
Results from Dubey andKeller (2003) suggest that state-of-the-art parsingscores for German are generally lower than those ob-tained for English, while recent results from Ku?bleret al (2006) raise the possibility that this mightbe an artefact of particular encoding schemes anddata structures of treebanks, which serve as trainingresources for probabilistic parsers.
Ku?bler (2005)and Maier (2006) show that treebank annotationschemes have considerable influence on parsing re-sults.
A comparison of unlexicalised PCFG pars-ing (Ku?bler, 2005) trained and evaluated on the Ger-man NEGRA (Skut et al, 1997) and the Tu?Ba-D/Z (Telljohann et al, 2004) treebanks using LoPar(Schmid, 2000) shows a difference in parsing resultsof about 16%, using the PARSEVAL metric (Blacket al, 1991).
Ku?bler et al (2006) conclude that,contrary to what had been assumed, German is notactually harder to parse than English, but that theNEGRA annotation scheme does not support opti-mal PCFG parsing performance.Despite being the standard metric for measuringPCFG parser performance, PARSEVAL has beencriticised for not representing ?real?
parser quality(Carroll et al, 1998; Brisco et al, 2002; Sampsonand Babarbczy, 2003).
PARSEVAL checks label andwordspan identity in parser output compared to theoriginal treebank trees.
It neither weights results,differentiating between linguistically more or lesssevere errors, nor does it give credit to constituentswhere the syntactic categories have been recognisedcorrectly but the phrase boundary is slightly wrong.With this in mind, we question the assumptionthat the PARSEVAL results for NEGRA and Tu?Ba-D/Z reflect a real difference in quality between theparser output for parsers trained on the two differenttreebanks.
As a consequence we also question theconclusion that PARSEVAL results for German inthe same range as the parsing results for the English630Penn-II Treebank prove that German is not harderto parse than the more configurational English.
Toinvestigate this issue we present experiments on theGerman TIGER treebank (Dipper et al, 2001) andthe Tu?Ba-D/Z treebank.
TIGER is based on and ex-tends the NEGRA data and annotation scheme.
Ourerror insertion and past-parsing treebank-encodingexperiments experiments show that the differencesin parsing results for the two treebanks are notcaused by a higher number of errors in the outputof the parser trained on the TIGER treebank, but aredue to the bias of the PARSEVAL metric towards an-notation schemes (such as that of Tu?Ba-D/Z) with ahigher ratio of non-terminal/terminal nodes.
The ex-periments also show that compared to PARSEVALthe Leaf-Ancestor metric is somewhat less suscep-tible to non-terminal/terminal ratios and that con-trary to the PARSEVAL results, dependency-basedevaluations score TIGER trained parsers higher thanTu?Ba-D/Z trained parsers.This paper is structured as follows: Section 2gives an overview of the main features of the twotreebanks.
Section 3 describes our first experiment,where we systematically insert controlled errors intothe original treebank trees and compare the influenceof these modifications on the evaluation results inthe PARSEVAL metric and the Leaf-Ancestor met-ric against the original, unmodified trees for bothtreebanks.
In Section 4 we present the second ex-periment, where we extract an unlexicalised PCFGfrom each of the treebanks.
Then we convert the out-put of the PCFG parser trained on the Tu?Ba-D/Z intoa TIGER-style format and evaluate the convertedtrees.
In Section 5 we present a dependency-basedevaluation and compare the results to the results ofthe two other measures.
The last section concludes.2 The TIGER Treebank and the Tu?Ba-D/ZThe two German treebanks used in our experimentsare the TIGER Treebank (Release 2) and the Tu?ba-D/Z (Release 2).
The Tu?Ba-D/Z consists of approx-imately 22 000 sentences, while the TIGER Tree-bank is much larger with more than 50 000 sen-tences.
Both treebanks contain German newspapertext and are annotated with phrase structure and de-pendency (functional) information.
Both treebanksuse the Stuttgart Tu?bingen POS Tag Set (Schilleret al, 95).
TIGER uses 49 different grammaticalfunction labels, while the Tu?Ba-D/Z utilises only36 function labels.
For the encoding of phrasalnode categories the Tu?Ba-D/Z uses 30 different cat-egories, the TIGER Treebank uses a set of 27 cate-gory labels.Other major differences between the two tree-banks are: in the Tiger Treebank long distance de-pendencies are expressed through crossing branches(Figure 1), while in the Tu?Ba-D/Z the same phe-nomenon is expressed with the help of grammati-cal function labels (Figure 2), where the node labelV-MOD encodes the information that the PP mod-ifies the verb.
The annotation in the Tiger Tree-bank is rather flat and allows no unary branching,whereas the nodes in the Tu?Ba-D/Z do contain unarybranches and a more hierarchical structure, resultingin a much deeper tree structure than the trees in theTiger Treebank.
This results in an average highernumber of nodes per sentence for the Tu?Ba-D/Z.
Ta-ble 1 shows the differences in the ratio of nodes forthe Tiger treebank and the Tu?Ba-D/Z.phrasal phrasal wordsnodes/sent nodes/word /sentTIGER 8.29 0.47 17.60Tu?Ba-D/Z 20.69 1.20 17.27Table 1: Average number of phrasal nodes/words inTIGER and Tu?Ba-D/ZFigures 1 and 2 also illustrate the different annota-tion of PPs in both annotation schemes.
In the Tigertreebank the internal structure of the PP is flat andthe adjective and noun inside the PP are directly at-tached to the PP, while the Tu?Ba-D/Z is more hier-archical and inserts an additional NP node.Another major difference is the annotation oftopological fields in the style of Drach (1937) andHo?hle (1986) in the Tu?Ba-D/Z.
The model capturesGerman word order, which accepts three possiblesentence configurations (verb first, verb second andverb last), by providing fields like the initial field(VF), the middle field (MF) and the final field (NF).The fields are positioned relative to the verb, whichcan fill in the left (LK) or the right sentence bracket(VC).
The ordering of topological fields is deter-mined by syntactic constraints.631Auch mit staatlichen Auftr?agen sieht es schlecht aus.
?It also looks bad for public contracts.
?Figure 1: TIGER treebank treeIn Wales sieht es besser aus.
?Things seem better in Wales.
?Figure 2: Tu?Ba-D/Z treebank tree2.1 Differences between TIGER and NEGRATo date, most PCFG parsing for German hasbeen done using the NEGRA corpus as a train-ing resource.
The flat annotation scheme of theTIGER treebank is based on the NEGRA anno-tation scheme, but it also employs some impor-tant extensions, which include the annotation ofverb-subcategorisation, appositions and parenthe-ses, coordinations and the encoding of proper nouns(Brants et al, 2002).3 Treebank Preprocessing: ConvertingTIGER Graphs into CFG TreesThe sentences in the TIGER treebank are repre-sented as graphs with LDDs expressed throughcrossing branches.
Before being able to insert er-rors or extract a PCFG we had to resolve these cross-ing branches in the TIGER treebank.
This was doneby attaching the non-head child nodes higher up inthe tree, following Ku?bler (2006).
For the graphin Figure 1 this would mean that the modifying PP?Auch mit staatlichen Auftra?gen?
(also for publiccontracts) was attached directly to the S node, whilethe head of the adjectival phrase (AP) remained init?s original position.
As a side effect this leads to thecreation of some unary nodes in the TIGER trees.We also inserted a virtual root node and removedall functional labels from the TIGER and Tu?Ba-D/Ztrees.4 Experiment IExperiment I is designed to assess the impactof identical errors on the two treebank encodingschemes and the PARSEVAL1 and Leaf-Ancestorevaluation metrics.4.1 Experimental SetupThe TIGER treebank and the Tu?Ba-D/Z both con-tain newspaper text, but from different Germannewspapers.
To support a meaningful comparisonwe have to compare similar sentences from bothtreebanks.
In order to control for similarity we se-lected all sentences of length 10 ?
n ?
40 fromboth treebanks.
For all sentences with equal lengthwe computed the average number of prepositions,determiners, nouns (and related POS such as propernames and personal pronouns), interrogative pro-nouns, finite verbs, infinite verbs, past participlesand imperative verb forms.
For each sentence lengthwe selected all sentences from both treebanks whichshowed an average for each of the POS listed abovewhich did not deviate more than 0.8 from the av-erage for all sentences for this particular sentencelength.
From this set we randomly selected 1024sentences for each of the treebanks.
This results intwo test sets, comparable in word length, syntacticstructure and complexity.
Table 2 shows the ratio ofphrasal versus terminal nodes in the test sets.We then inserted different types of controlled er-rors automatically into the original treebank trees inour test sets and evaluated the modified trees against1In all our experiments we use the evalb metric (Sekineand Collins, 1997), the most commonly used implementationof the PARSEVAL metric.632phrasal phrasal nodes wordsnodes/sent nodes/word /sentTIGER 6.97 0.48 14.49Tu?Ba-D/Z 19.18 1.30 14.75Table 2: Average number of phrasal nodes/words inthe TIGER and Tu?Ba-D/Z test setthe original treebank trees, in order to assess the im-pact of similar (controlled for type and number) er-rors on the two encoding schemes.4.2 Error InsertionThe errors fall into three types: attachment, span andlabeling (Table 3).
We carried out the same numberof error insertions in both test sets.Error descriptionATTACH I Attach PPs inside an NP one levelhigher up in the treeATTACH II Change verb attachment to nounattachment for PPs on sentence level,inside a VP or in the MF (middle field)LABEL I Change labels of PPs to NPLABEL II Change labels of VPs to PPSPAN I Include adverb to the left of a PPinto the PPSPAN II Include NN to the left of a PPinto the PPSPAN III Combination of SPANI and SPANIITable 3: Description of inserted error types4.3 Results for Error Insertion for the OriginalTreebank TreesTable 4 shows the impact of the error insertion intothe original treebank trees on PARSEVAL results,evaluated against the gold trees.
PARSEVAL resultsin all experiments report labelled precision and re-call.
The first error (PP attachment I, 85 insertionsin each test set) leads to a decrease in f-score of 1.16for the TIGER test set, while for the Tu?Ba-D/Z testset the same error only caused a decrease of 0.43.The effect remains the same for all error types andis most pronounced for the category label errors, be-cause the frequency of the labels resulted in a largenumber of substitutions.
The last row lists the totalweighted average for all error types, weighted withrespect to their frequency of occurrence in the testsets.Table 4 clearly shows that the PARSEVALmeasure punishes the TIGER treebank annotationTIGER Tu?Ba # errorsPP attachment I 98.84 99.57 85PP attachment II 98.75 99.55 89Label I 80.02 92.73 1427Label II 93.00 97.45 500SPAN I 99.01 99.64 71SPAN II 97.47 99.08 181SPAN III 96.51 98.73 252total weighted ave. 87.09 95.30Table 4: f-score for PARSEVAL results for error in-sertion in the original treebank treesscheme to a greater extent, while the same num-ber and type of errors in the Tu?Ba-D/Z annotationscheme does not have an equally strong effect onPARSEVAL results for similar sentences.4.4 Discussion: PARSEVAL and LAExperiment I shows that the gap between the PAR-SEVAL results for the two annotation schemes doesnot reflect a difference in quality between the trees.Both test sets contain the same number of sentenceswith the same sentence length and are equivalent incomplexity and structure.
They contain the samenumber and type of errors.
This suggests that thedifference between the results for the TIGER andthe Tu?Ba-D/Z test set are due to the higher ratio ofnon-terminal/terminal nodes in the Tu?Ba-D/Z trees(Table 1).In order to obtain an alternative view on thequality of our annotation schemes we used theleaf-ancestor (LA) metric (Sampson and Babarbczy,2003), a parser evaluation metric which measuresthe similarity of the path from each terminal nodein the parse tree to the root node.
The path con-sists of the sequence of node labels between the ter-minal node and the root node, and the similarity oftwo paths is calculated by using the Levenshtein dis-tance (Levenshtein, 1966).
Table 5 shows the resultsfor the leaf-ancestor evaluation metric for our errorinsertion test sets.
Here the weighted average re-sults for the two test sets are much closer to eachother (94.98 vs. 97.18 as against 87.09 vs. 95.30).Only the label errors, due to the large numbers, showa significant difference between the two annotationschemes.
Tables 4 and 5 show that compared toPARSEVAL the LA metric is somewhat less sensi-tive to the nonterminal/terminal ratio.Figure 3 illustrates the different behaviour of the633TIGER Tu?Ba # errorsPP attachment I 99.62 99.70 85PP attachment II 99.66 99.78 89Label I 92.45 95.24 1427Label II 96.05 99.28 500SPAN I 99.82 99.84 71SPAN II 99.51 99.77 181SPAN III 99.34 99.62 252total weighted ave. 94.98 97.18Table 5: LA results for error insertion in the originaltreebank treestwo evaluation metrics with respect to an examplesentence.Sentence 9:Die Stadtverwaltung von Venedig hat erstmals streunendeKatzen gez?ahlt.
?For the first time the city council of Venice has counted stray-ing cats.?
(TOP(S(NP(ART Die [the] )(NN Stadtverwaltung [city counsil] )(PP(APPR von [of] )(NE Venedig [Venice] )))(VAFIN hat [has] )(VP(ADV erstmals [for the first time] )(NP(ADJA streunende [straying] )(NN Katzen [cats] ))(VVPP geza?hlt [counted] )))($.
.
))Figure 3: Sentence 9 from the TIGER Test SetTable 6 shows that all error types inserted intoSentence 9 in our test set result in the same eval-uation score for the PARSEVAL metric, while theLA metric provides a more discriminative treatmentof PP attachment errors, label errors and span errorsfor the same sentence (Table 6).
However, the dif-ferences in the LA results are only indirectly causedby the different error types.
They actually reflectthe number of terminal nodes affected by the errorinsertion.
For Label I and II the LA results varyconsiderably, because the substitution of the PP foran NP (Label I) in Figure 3 affects two terminalnodes only (PP von [of] Venedig [Venice]), whilethe change of the VP into a PP (Label II) altersthe paths of four terminal nodes (VP erstmals [forthe first time] streunende [straying] Katzen [cats]geza?hlt [counted]) and therefore has a much greaterimpact on the overall result for the sentence.ERROR PARSEVAL LAPP attachment I 83.33 96.30Label I 83.33 96.00Label II 83.33 91.00SPAN II 83.33 96.40Table 6: Evaluation results for Sentence 9The Tu?Ba-D/Z benefits from its overall higher ra-tio of nodes per sentence, resulting in a higher ratioof non-terminal/terminal nodes per phrase and theeffect, that the inserted label error affects a smallernumber of terminal nodes than in the TIGER test setfor LA testing.5 Experiment IIKu?bler (2005) and Maier (2006) assess the impact ofthe different treebank annotation schemes on PCFGparsing by conducting a number of modificationsconverting the Tu?Ba-D/Z into a format more sim-ilar to the NEGRA (and hence TIGER) treebank.After each modification they extract a PCFG fromthe modified treebank and measure the effect of thechanges on parsing results.
They show that witheach modification transforming the Tu?Ba-D/Z intoa more NEGRA-like format the parsing results alsobecome more similar to the results of the NEGRAtreebank, i.e.
the results get worse.
Maier takes thisas evidence that the Tu?Ba-D/Z is more adequate forPCFG parsing.
This assumption is based on the be-lief that PARSEVAL results fully reflect parse qual-ity across different treebank encoding schemes.
Thisis not always true, as shown in Experiment I.In our second experiment we crucially change theorder of events in the Ku?bler (2005), Maier (2006)and Ku?bler et al (2006) experiments: We first ex-tract an unlexicalised PCFG from each of the orig-inal treebanks.
We then transform the output ofthe parser trained on the Tu?Ba-D/Z into a formatmore similar to the TIGER Treebank.
In contrast toKu?bler (2005) and Maier (2006), who converted the634treebank before extracting the grammars in order tomeasure the impact of single features like topologi-cal fields or unary nodes on PCFG parsing, we con-vert the trees in the parser output of a parser trainedon the original unconverted treebank resources.
Thisallows us to preserve the basic syntactic structureand also the errors present in the output trees re-sulting from a potential bias in the original tree-bank training resources.
The results for the originalparser output evaluated against the unmodified goldtrees should not be crucially different from the re-sults for the modified parser output evaluated againstthe modified gold trees.5.1 Experimental SetupFor Experiment II we trained BitPar (Schmid, 2004),a parser for highly ambiguous PCFG grammars, onthe two treebanks.
The Tu?Ba-D/Z training data con-sists of the 21067 treebank trees not included in theTu?Ba-D/Z test set.
Because of the different size ofthe two treebanks we selected 21067 sentences fromthe TIGER treebank, starting from sentence 10000(and excluding the sentences in the TIGER test set).Before extracting the grammars we resolved thecrossing branches in the TIGER treebank as de-scribed in Section 3.
After this preprocessing stepwe extracted an unlexicalised PCFG from each ofour training sets.
Our TIGER grammar has a total of21163 rule types, while the grammar extracted fromthe Tu?Ba-D/Z treebank consists of 5021 rules only.We parsed the TIGER and Tu?Ba-D/Z test set withthe extracted grammars, using the gold POS tags forparser input.
We then automatically converted theTu?Ba-D/Z output to a TIGER-like format and com-pare the evaluation results for the unmodified treesagainst the gold trees with the results for the con-verted parser output against the converted gold trees.5.2 Converting the Tu?Ba-D/Z TreesThe automatic conversion of the Tu?Ba-D/Z-styletrees includes the removal of topological fields andunary nodes as well as the deletion of NPs insideof PPs, because the NP child nodes are directly at-tached to the PP in the TIGER annotation scheme.As a last step in the conversion process we adaptedthe Tu?Ba-D/Z node labels to the TIGER categories.5.2.1 The Conversion Process: An ExampleWe demonstrate the conversion process using anexample sentence from the Tu?Ba-D/Z test set (Fig-ure 4).
The converted tree is given in Figure 5:topological fields, here VF (initial field), MF (mid-dle field) and LK (left sentence bracket), as well asunary nodes have been removed.
The category la-bels have been changed to TIGER-style annotation.Erziehungsurlaub nehmen bisher nur zwei Prozent der M?anner.
?Until now only two percent of the men take parental leave.
?Figure 4: Original Tu?Ba-D/Z-style gold treeFigure 5: Converted TIGER-style gold treeFigure 6 shows the unmodified parser output fromthe Tu?Ba-D/Z trained grammar for the same string.The parser incorrectly included all adverbs inside anNP governed by the PP, while in the gold tree (Figure4) both adverbs are attached to the PP.
The modifiedparser output is shown in Figure 7.5.3 Results for Converted Parser OutputWe applied the conversion method described aboveto the original trees and the parser output for the sen-tences in the TIGER and the Tu?Ba-D/Z test sets.
Ta-ble 7 shows PARSEVAL and LA results for the mod-ified trees, evaluating the converted parser output635Figure 6: Parser output (Tu?Ba-D/Z grammar)Figure 7: Converted parser output (Tu?Ba-D/Z)for each treebank against the converted gold treesof the same treebank.
Due to the resolved crossingbranches in the TIGER treebank we also have someunary nodes in the TIGER test set.
Their removalsurprisingly improves both PARSEVAL and LA re-sults.
For the Tu?Ba-D/Z all conversions lead to adecrease in precision and recall for the PARSEVALmetric.
Converting the trees parsed by the Tu?Ba-D/Z grammar to a TIGER-like format produces an f-score which is slightly lower than that for the TIGERtrees.
The same is true for the LA metric, but not tothe same extent as for PARSEVAL.
The LA met-ric also gives slightly better results for the originalTIGER trees compared to the result for the unmodi-fied Tu?Ba-D/Z trees.The constant decrease in PARSEVAL results forthe modified trees is consistent with the results inKu?bler et al (2005), but our conclusions are slightlydifferent.
Our experiment shows that the Tu?Ba-D/Z annotation scheme does not generally producehigher quality parser output, but that the PARSE-VAL results are highly sensitive to the ratio of non-terminal/terminal nodes.
However, the parser outputfor the grammar trained on the Tu?Ba-D/Z yields aEVALB LAprec.
recall f-sco.
avg.TIGER 83.54 83.65 83.59 94.69no Unary 84.33 84.48 84.41 94.83Tu?Ba-D/Z 92.59 89.79 91.17 94.23Tu?Ba-D/Z?
TIGERno Top 92.38 88.76 90.53 93.93no Unary 89.96 85.67 87.76 93.59no Top + no U.
88.44 82.24 85.23 92.91no Top + no U.
87.15 79.52 83.16 92.47+ no NP in PPTable 7: The impact of the conversion process onPARSEVAL and LAhigher precision in the PARSEVAL metric againstthe Tu?Ba-D/Z gold trees than the parser output ofthe TIGER grammar against the TIGER gold trees.For PARSEVAL recall, the TIGER grammar givesbetter results.6 Experiment IIIIn Experiment I and II we showed that the tree-based PARSEVAL metric is not a reliable measurefor comparing the impact of different treebank an-notation schemes on the quality of parser output andthat the issue, whether German is harder to parsethan English, remains undecided.
In Experiment IIIwe report a dependency-based evaluation and com-pare the results to the results of the other metrics.6.1 Dependency-Based (DB) EvaluationThe dependency-based evaluation used in the exper-iments follows the method of Lin (1998) and Ku?blerand Telljohann (2002), converting the original tree-bank trees and the parser output into dependency re-lations of the form WORD POS HEAD.
Functionallabels have been omitted for parsing, therefore thedependencies do not comprise functional informa-tion.
Figure 8 shows the original TIGER Treebankrepresentation for the CFG tree in Figure 3.
Squareboxes denote grammatical functions.
Figure 9 showsthe dependency relations for the same tree, indicatedby labelled arrows.
Converted into a WORD POSHEAD triple format the dependency tree looks asfollows (Table 8).Following Lin (1998), our DB evaluation algo-rithm computes precision and recall:?
Precision: the percentage of dependency re-lationships in the parser output that are also636Figure 8: TIGER treebank representation for Figure 3SBNKPGNK NK OAMOOCthe  city counsil   of   Venice  has  for the    straying   cats  countedfirst timeDie    Stadtverwaltung    von    Venedig    hat     erstmals       streunende    Katzen    gez?hlt?For the first time the city counsil of Venice has counted straying cats.
?Figure 9: Dependency relations for Figure 8found in the gold triples?
Recall: the percentage of dependency relation-ships in the gold triples that are also found inthe parser output triples.WORD POS HEADDie [the] ART StadtverwaltungStadtverwaltung NN hat[city counsil]von [of] APPR StadtverwaltungVenedig [Venice] NE vonhat [has] VAFIN -erstmals ADV geza?hlt[for the first time]streunende [straying] ADJA KatzenKatzen [cats] NN geza?hltgeza?hlt [counted] VVPP hatTable 8: Dependency triples for Figure 9We assessed the quality of the automatic conver-sion methodology by converting the 1024 originaltrees from each of our test sets into dependency rela-tions, using the functional labels in the original treesto determine the dependencies.
Topological fieldsin the Tu?Ba-D/Z test set have been removed beforeextracting the dependency relationships.We then removed all functional information fromthe trees and converted the stripped trees into depen-dencies, using heuristics to find the head.
We eval-uated the dependencies for the stripped gold treesagainst the dependencies for the original gold treesincluding functional labels and obtained an f-scoreof 99.64% for TIGER and 99.13% for the Tu?Ba-D/Zdependencies.
This shows that the conversion is re-liable and not unduly biased to either the TIGER orTu?Ba-D/Z annotation schemes.6.2 Experimental SetupFor Experiment III we used the same PCFG gram-mars and test sets as in Experiment II.
Before ex-tracting the dependency relationships we removedthe topological fields in the Tu?Ba-D/Z parser output.As shown in Section 6.1, this does not penalise thedependency-based evaluation results for the Tu?Ba-D/Z.
In contrast to Experiment II we used raw textas parser input instead of the gold POS tags, allow-637ing a comparison with the gold tag results in Table 7.6.3 ResultsTable 9 shows the evaluation results for the threedifferent evaluation metrics.
For the DB evalua-tion the parser trained on the TIGER training setachieves about 7% higher results for precision andrecall than the parser trained on the Tu?Ba-D/Z.
Thisresult is clearly in contrast to the PARSEVAL scores,which show higher results for precision and recallfor the Tu?Ba-D/Z.
But contrary to the PARSEVALresults on gold POS tags as parser input (Table 7),the gap between the results for TIGER and Tu?Ba-D/Z is not as wide as before.
PARSEVAL givesa labelled bracketing f-score of 81.12% (TIGER)and 85.47% (Tu?Ba-D/Z) on raw text as parser in-put, while the results on gold POS tags are more dis-tinctive with an f-score of 83.59% for TIGER and91.17% for Tu?Ba-D/Z.
The LA results again givebetter scores to the TIGER parser output, this timethe difference is more pronounced than for Experi-ment II (Table 7).Dependencies PARSEVAL LAPrec Rec Prec Rec AvgTIGER 85.71 85.72 81.21 81.04 93.88Tu?Ba 76.64 76.63 87.24 83.77 92.58Table 9: Parsing results for three evaluation metricsThe considerable difference between the resultsfor the metrics raises the question which of the met-rics is the most adequate for judging parser outputquality across treebank encoding schemes.7 ConclusionsIn this paper we presented novel experiments assess-ing the validity of parsing results measured alongdifferent dimensions: the tree-based PARSEVALmetric, the string-based Leaf-Ancestor metric anda dependency-based evaluation.
By inserting con-trolled errors into gold treebank trees and measuringthe effects on parser evaluation results we gave newevidence for the downsides of PARSEVAL which,despite severe criticism, is still the standard mea-sure for parser evaluation.
We showed that PAR-SEVAL cannot be used to compare the output ofPCFG parsers trained on different treebank anno-tation schemes, because the results correlate withthe ratio of non-terminal/terminal nodes.
Compar-ing two different annotation schemes, PARSEVALconsistently favours the one with the higher node ra-tio.We examined the influence of treebank annotationschemes on unlexicalised PCFG parsing, and re-jected the claim that the German Tu?Ba-D/Z treebankis more appropriate for PCFG parsing than the Ger-man TIGER treebank and showed that convertingthe Tu?Ba-D/Z trained parser output to a TIGER-likeformat leads to PARSEVAL results slightly worsethan the ones for the TIGER treebank trained parser.Additional evidence comes from a dependency-based evaluation, showing that, for the output of theparser trained on the TIGER treebank, the mappingfrom the CFG trees to dependency relations yieldsbetter results than for the grammar trained on theTu?Ba-D/Z annotation scheme, even though PARSE-VAL scores suggest that the TIGER-based parseroutput trees are substantial worse than Tu?Ba-D/Z-based parser output trees.We have shown that different treebank annotationschemes have a strong impact on parsing results forsimilar input data with similar (simulated) parser er-rors.
Therefore the question whether a particularlanguage is harder to parse than another languageor not, can not be answered by comparing parsingresults for parsers trained on treebanks with differ-ent annotation schemes.
Comparing PARSEVAL-based parsing results for a parser trained on theTu?Ba-D/Z or TIGER to results achieved by a parsertrained on the English Penn-II treebank (Marcuset al, 1994) does not provide conclusive evidenceabout the parsability of a particular language, be-cause the results show a bias introduced by thecombined effect of annotation scheme and evalua-tion metric.
This means that the question whetherGerman is harder to parse than English, is stillundecided.
A possible way forward is perhaps adependency-based evaluation of TIGER/Tu?Ba-D/Zwith Penn-II trained grammars for ?similar?
test andtraining sets and cross-treebank and -language con-trolled error insertion experiments.
Even this is notentirely straightforward as it is not completely clearwhat constitutes ?similar?
test/training sets acrosslanguages.
We will attempt to pursue this in furtherresearch.638AcknowledgementsWe would like to thank the anomymous reviewersfor many helpful comments.
This research has beensupported by a Science Foundation Ireland grant04|IN|I527.ReferencesBlack, E., S. P. Abney, D. Flickinger, C. Gdaniec, R. Gr-ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,J.
Klavans, M. Liberman, M. P. Marcus, S. Roukos, B.Santorini, and T. Strzalkowski.
1991.
A procedure forquantitatively comparing the syntactic coverage of En-glish grammars.
In Proceedings DARPA Speech andNatural Language Workshop, Pacific Grove, CA, pp.306-311.Brants, Sabine, and Silvia Hansen.
2002.
Developmentsin the TIGER Annotation Scheme and their Realiza-tion in the Corpus.
In Proceedings of the Third Confer-ence on Language Resources and Evaluation (LREC2002), pp.
1643-1649 Las Palmas.Briscoe, E. J., J.
A. Carroll, and A. Copestake.
2002.Relational evaluation schemes.
In Proceedings Work-shop ?Beyond Parseval - towards improved evaluationmeasures for parsing systems?, 3rd International Con-ference on Language Resources and Evaluation, pp.4-38.
Las Palmas, Canary Islands.Carroll, J., E. Briscoe and A. Sanfilippo.
1998.
Parserevaluation: a survey and a new proposal.
In Proceed-ings of the 1st International Conference on LanguageResources and Evaluation, Granada, Spain.
447-454.Dipper, S., T. Brants, W. Lezius, O. Plaehn, and G. Smith.2001.
The TIGER Treebank.
In Third Workshop onLinguistically Interpreted Corpora LINC-2001, Leu-ven, Belgium.Drach, Erich.
1937.
Grundgedanken der Deutschen Sat-zlehre.
Frankfurt/M.Dubey, A., and F. Keller.
2003.
Probabilistic parsing forGerman using sisterhead dependencies.
In Proceed-ings of the 41st Annual Meeting of the Association forComputational Linguistics, Sapporo, Japan.Ho?hle, Tilman.
1998.
Der Begriff ?Mittelfeld?, An-merkungen u?ber die Theorie der topologischen Felder.In Akten des Siebten Internationalen Germansitenkon-gresses 1985, pages 329-340, Go?ttingen, Germany.Ku?bler, Sandra, and Heike Telljohann.
2002.
Towardsa Dependency-Oriented Evaluation for Partial Pars-ing.
In Proceedings of Beyond PARSEVAL ?
TowardsImproved Evaluation Measures for Parsing Systems(LREC 2002 Workshop), Las Palmas, Gran Canaria,June 2002.Lin, Dekang.
1998.
A dependency-based method forevaluating broad-coverage parsers.
Natural LanguageEngineering, 1998.Ku?bler, Sandra.
2005.
How Do Treebank AnnotationSchemes Influence Parsing Results?
Or How Not toCompare Apples And Oranges.
In Proceedings ofFANLP 2005), Borovets, Bulgaria, September 2005.Ku?bler, Sandra, Erhard Hinrichs, and Wolfgang Maier.2006.
Is it Really that Difficult to Parse German?In Proceedings of the 2006 Conference on Empiri-cal Methods in Natural Language Processing, EMNLP2006), Sydney, Australia, July 2006.Levenshtein, V. I.
1966.
Binary codes capable of correct-ing deletions, insertions, and reversals.
Soviet Physics- Doklady, 10.707-10 (translation of Russian originalpublished in 1965).Maier, Wolfgang.
2006.
Annotation Schemes andtheir Influence on Parsing Results.
In Proceedings ofthe COLING/ACL 2006 Student Research Workshop),Sydney, Australia, July 2006.Marcus, M., G. Kim, M. A. Marcinkiewicz, R. MacIn-tyre, M. Ferguson, K. Katz and B. Schasberger.
1994.The Penn Treebank: Annotating Predicate ArgumentStructure.
In Proceedings of the ARPA Human Lan-guage Technology Workshop, Princeton, NJ.Sampson, Geoffrey, and Anna Babarczy.
2003.
A testof the leaf-ancestor metric for parse accuracy.
NaturalLanguage Engineering, 9 (4):365-380.Schmid, Helmut.
2000.
LoPar: Design and Implemen-tation.
Arbeitspapiere des Sonderforschungsbereiches340, No.
149, IMS Stuttgart, July 2000.Schmid, Helmut.
2004.
Efficient Parsing of Highly Am-biguous Context-Free Grammars with Bit Vectors.
InProceedings of the 20th International Conference onComputational Linguistics (COLING 2004), Geneva,Switzerland.Sekine, S. and M. J. Collins.
1997.
The evalb software.http://nlp.cs.nyu.edu/evalb/Skut, Wojciech, Brigitte Krann, Thorsten Brants, andHans Uszkoreit.
1997.
An annotation scheme for freeword order languages.
In Proceedings of ANLP 1997,Washington, D.C.Telljohann, Heike, Erhard W. Hinrichs, Sandra Ku?bler,and Heike Zinsmeister.
2005.
Stylebook forthe Tu?bingen Treebank of Written German (Tu?Ba-D/Z).
Seminar fu?r Sprachwissenschaft, Universita?tTu?bingen, Germany.Schiller, Anne, Simone Teufel, and Christine Thielen.1995.
Guidelines fr das Tagging deutscher Textcor-pora mit STTS.
Technical Report, IMS-CL, UniversityStuttgart, 1995.639
