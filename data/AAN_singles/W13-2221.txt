Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 170?176,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsEdinburgh?s Syntax-Based Machine Translation SystemsMaria Nadejde, Philip Williams, and Philipp KoehnSchool of Informatics, University of Edinburgh, Scotland, United Kingdommaria.nadejde@gmail.com, P.J.Williams-2@sms.ed.ac.uk, pkoehn@inf.ed.ac.ukAbstractWe present the syntax-based string-to-tree statistical machine translation systemsbuilt for the WMT 2013 shared transla-tion task.
Systems were developed forfour language pairs.
We report on adaptingparameters, targeted reduction of the tun-ing set, and post-evaluation experimentson rule binarization and preventing drop-ping of verbs.1 OverviewSyntax-based machine translation models holdthe promise to overcome some of the fundamen-tal problems of the currently dominating phrase-based approach, most importantly handling re-ordering for syntactically divergent language pairsand grammatical coherence of the output.We are especially interested in string-to-treemodels that focus syntactic annotation on the tar-get side, especially for morphologically rich targetlanguages (Williams and Koehn, 2011).We have trained syntax-based systems for thelanguage pairs?
English-German,?
German-English,?
Czech-English, and?
Russian-English.We have also tried building systems for French-English and Spanish-English but the data sizeproved to be problematic given the time con-straints.
We give a brief description of the syntax-based model and its implementation within theMoses system.
Some of the available features aredescribed as well as some of the pre-processingsteps.
Several experiments are described and finalresults are presented for each language pair.2 System DescriptionThe syntax-based system used in all experimentsis the Moses string-to-tree toolkit implementingGHKM rule extraction and Scope-3 parsing previ-ously described in by Williams and Koehn (2012)2.1 GrammarOur translation grammar is a synchronous context-free grammar (SCFG) with phrase-structure labelson the target side and the generic non-terminal la-bel X on the source side.
In this paper, we writethese rules in the formLHS ?
RHSs | RHStwhere LHS is a target-side non-terminal label andRHSs and RHSt are strings of terminals and non-terminals for the source and target sides, respec-tively.
We use subscripted indices to indicate thecorrespondences between source and target non-terminals.For example, a translation rule to translate theGerman Haus into the English house isNN ?
Haus | houseIf our grammar also contains the translation ruleS ?
das ist ein X1 | this is a NN1then we can apply the two rules to an input das istein Haus to produce the output this is a house.2.2 Rule ExtractionThe GHKM rule extractor (Galley et al 2004,2006) learns translation rules from a word-alignedparallel corpora for which the target sentences aresyntactically annotated.
Given a string-tree pair,the set of minimally-sized translation rules is ex-tracted that can explain the example and is consis-tent with the alignment.
The resulting rules can becomposed in a non-overlapping fashion in order tocover the string-tree pair.Two or more minimal rules that are in a parent-child relationship can be composed together to ob-tain larger rules with more syntactic context.
Toavoid generating an exponential number of com-posed rules, several limitation have to be imposed.One such limitation is on the size of the com-posed rules, which is defined as the number ofnon-part-of-speech, non-leaf constituent labels inthe target tree (DeNeefe et al 2007).
The corre-sponding parameter in the Moses implementationis MaxRuleSize and its default value is 3.170Another limitation is on the depth of the rules?target subtree.
The rule depth is computed as themaximum distance from its root node to any of itschildren, not counting pre-terminal nodes (param-eter MaxRuleDepth, default 3).The third limitation considered is the number ofnodes in the composed rule, not counting targetwords (parameter MaxNodes, default 15).These parameters are language-dependent andshould be set to values that best represent the char-acteristics of the target trees on which the rule ex-tractor is trained on.
Therefore the style of thetreebanks used for training the syntactic parserswill also influence these numbers.
The defaultvalues have been set based on experiments onthe English-German language pair (Williams andKoehn, 2012).
It is worth noting that the Ger-man parse trees (Skut et al 1997) tend to bebroader and shallower than those for English.
InSection 3 we present some experiments where wechoose different settings of these parameters forthe German-English language pair.
We use thosesettings for all language pairs where the target lan-guage is English.2.3 Tree RestructuringThe coverage of the extracted grammar dependspartly on the structure of the target trees.
If thetarget trees have flat constructions such as longnoun phrases with many sibling nodes, the rulesextracted will not generalize well to unseen datasince there will be many constraints given by thetypes of different sibling nodes.In order to improve the grammar coverage togeneralize over such cases, the target tree can berestructured.
One restructuring strategy is treebinarization.
Wang et al(2010) give an exten-sive overview of different tree binarization strate-gies applied for the Chinese-English languagepair.
Moses currently supports left binarizationand right binarization.By left binarization all the left-most childrenof a parent node n except the right most childare grouped under a new node.
This node is in-serted as the left child of n and receives the la-bel n?.
Left binarization is then applied recursivelyon all newly inserted nodes until the leaves arereached.
Right binarization implies a similar pro-cedure but in this case the right-most children ofthe parent node are grouped together except theleft most child.Another binarization strategy that is not cur-rently integrated in Moses, but is worth investigat-ing for different language pairs, is parallel headbinarization.The result of parallel binarization of a parsetree is a binarization forest.
To generate a bina-rization forest node, both right binarization andleft binarization are applied recursively to a parentnode with more than two children.
Parallel headbinarization is a case of parallel binarization withthe additional constraint that the head constituentis part of all the new nodes inserted by either leftor right binarization steps.In Section 3 we give example of some initial ex-periments carried out for the German-English lan-guage pair.2.4 Pruning The GrammarDecoding for syntax-based model relies on abottom-up chart parsing algorithm.
Therefore de-coding efficiency is influenced by the followingcombinatorial problem: given an input sentenceof length n and a context-free grammar rule withs consecutive non-terminals, there are (n+1s) waysto choose subspans, or application contexts (Hop-kins and Langmead, 2010), that the rule can ap-plied to.
The asymptotic running time of chartparsing is linear in this number O(ns).Hopkins and Langmead (2010) maintain cubicdecoding time by pruning the grammar to removerules for which the number of potential applica-tion contexts is too large.
Their key observation isthat a rule can have any number of non-terminalsand terminals as long as the number of consecutivenon-terminal pairs is bounded.
Terminals act toanchor the rule, restricting the number of potentialapplication contexts.
An example is the rule X ?WyY Zz for which there are at most O(n2) appli-cation contexts, given that the terminals will havea fixed position and will play the role of anchorsin the sentence for the non-terminal spans.
Thenumber of consecutive non-terminal pairs plus thenumber of non-terminals at the edge of a rule isreferred to as the scope of the rule.
The scope of agrammar is the maximum scope of any of its rules.Moses implements scope-3 pruning and thereforethe resulting grammar can be parsed in cubic time.2.5 Feature FunctionsOur feature functions are unchanged from lastyear.
They include the n-gram language modelprobability of the derivation?s target yield, its word171count, and various scores for the synchronousderivation.
Our grammar rules are scored accord-ing to the following functions:?
p(RHSs|RHSt,LHS), the noisy-channeltranslation probability.?
p(LHS,RHSt|RHSs), the direct translationprobability.?
plex (RHSt|RHSs) and plex (RHSs|RHSt),the direct and indirect lexical weights (Koehnet al 2003).?
ppcfg(FRAGt), the monolingual PCFG prob-ability of the tree fragment from whichthe rule was extracted.
This is definedas ?ni=1 p(ri), where r1 .
.
.
rn are the con-stituent CFG rules of the fragment.
ThePCFG parameters are estimated from theparse of the target-side training data.
All lex-ical CFG rules are given the probability 1.This is similar to the pcfg feature proposedby Marcu et al(2006) and is intended to en-courage the production of syntactically well-formed derivations.?
exp(?1/count(r)), a rule rareness penalty.?
exp(1), a rule penalty.
The main grammarand glue grammars have distinct penalty fea-tures.3 ExperimentsThis section describes details for the syntax-basedsystems submitted by the University of Edinburgh.Additional post-evaluation experiments were car-ried out for the German-English language pair.3.1 DataWe made use of all available data for each lan-guage pair except for the Russian-English wherethe Commoncrawl corpus was not used.
Table 1shows the size of the parallel corpus used for eachlanguage pair.
The English side of the paral-lel corpus was parsed using the Berkeley parser(Petrov et al 2006) and the German side of theparallel corpus was parsed using the BitPar parser(Schmid, 2004).
For German-English, Germancompounds were split using the script providedwith Moses.
The parallel corpus was word-alignedusing MGIZA++ (Gao and Vogel, 2008).All available monolingual data was used fortraining the language models for each languageLang.
pair Sentences Grammar Sizeen-de 4,411,792 31,568,480de-en 4,434,060 55,310,162cs-en 14,425,564 209,841,388ru-en 1,140,359 7,946,502Table 1: Corpus statistics for parallel data.pair.
5-gram language models were trained us-ing SRILM toolkit (Stolcke, 2002) with modi-fied Kneser-Ney smoothing (Chen and Goodman,1998) and then interpolated using weights tunedon the newstest2011 development set.The feature weights for each system were tunedon development sets using the Moses implementa-tion of minimum error rate training (Och, 2003).The size of the tuning data varied for different lan-guages depending on the amount of available data.In the case of the the German-English pair a filter-ing criteria based on sentence level BLEU scorewas applied which is briefly described in Section3.5.
Table 2 shows the size of the tuning set foreach language pair.Lang.
pair Sentencesen-de 7,065de-en 2,400cs-en 10,068ru-en 1,501Table 2: Corpus statistics for tuning data.3.2 Pre-processingSome attention was given to pre-processing of theEnglish side of the corpus prior to parsing.
Thiswas done to avoid propagating parser errors to therule-extraction step.
These particular errors arisefrom a mismatch in punctuation and tokenizationbetween the corpus used to train the parser, thePennTree bank, and the corpus which is beingparsed and passed on to the rule extractor.
There-fore we changed the quotation marks, which ap-pear quite often in the parallel corpora, to openingand closing quotation marks.
We also added somePennTree bank style tokenization rules1.
Theserules split contractions such as I?ll, It?s, Don?t,Gonna, Commissioner?s in order to correctly sep-arate the verbs, negation and possessives that are1The PennTree bank tokenization rules considered weretaken from http://www.cis.upenn.edu/?treebank/tokenizer.sed.
Further examples of contractions wereadded.172Grammar Size BLEUParameters Full Filtered 2009-40 2010-40 2011-40 AverageDepth=3, Nodes=15, Size=3 2,572,222 751,355 18.57 20.43 18.51 19.17Depth=4, Nodes=20, Size=4 3,188,970 901,710 18.88 20.38 18.63 19.30Depth=5, Nodes=20, Size=5 3,668,205 980,057 19.04 20.47 18.75 19.42Depth=5, Nodes=30, Size=5 3,776,961 980,061 18.90 20.59 18.77 19.42Depth=5, Nodes=30, Size=6 4,340,716 1,006,174 18.98 20.52 18.80 19.43Table 3: Cased BLEU scores for various rule extraction parameter settings for German-English languagepair.
The parameters considered are MaxRuleDepth, MaxRuleSize, MaxNodes.
Grammar sizes are givenfor the full extracted grammar and after filtering for the newstest2008 dev set.newstest2012 newstest2013System Sentences BLEU Glue Rule Tree Depth BLEU Glue Rule Tree DepthBaseline 5,771 23.21 5.42 4.03 26.27 4.23 3.80Big tuning set 10,068 23.52 3.41 4.34 26.33 2.49 4.03Filtered tuning set 2,400 23.54 3.21 4.37 26.30 2.37 4.05Table 4: Cased BLEU scores for German-English systems tuned on different data.
Scores are emphasizedfor the system submitted to the shared translation task.parsed as separate constituents.For German?English, we carried out the usualcompound splitting (Koehn and Knight, 2003), butnot pre-reordering (Collins et al 2005).3.3 Rule ExtractionSome preliminary experiments were carried outfor the German-English language pair to deter-mine the parameters for the rule extraction step:MaxRuleDepth, MaxRuleSize, MaxNodes.
Table 3shows the BLEU score on different test sets forvarious parameter settings.
For efficiency rea-sons less training data was used, therefore thegrammar sizes, measured as the total number ofextracted rules, are smaller than the final sys-tems (Table 1).
The parameters on the third lineDepth=5, Nodes=20, Size=4 were chosen as theaverage BLEU score did not increase although thesize of the extracted grammar kept growing.
Com-paring the rate of growth of the full grammar andthe grammar after filtering for the dev set (thecolumns headed ?Full?
and ?Filtered?)
suggeststhat beyond this point not many more usable rulesare extracted, even while the total number of rulesstills increases.3.4 Decoder SettingsWe used the following non-default decoder param-eters:max-chart-span=25: This limits sub deriva-tions to a maximum span of 25 source words.
Gluerules are used to combine sub derivations allowingthe full sentence to be covered.ttable-limit=200: Moses prunes the translationgrammar on loading, removing low scoring rules.This option increases the number of translationrules that are retained for any given source sideRHSs.cube-pruning-pop-limit=1000: Number of hy-potheses created for each chart span.3.5 Tuning setsOne major limitation for the syntax-based systemsis that decoding becomes inefficient for long sen-tences.
Therefore using large tuning sets will slowdown considerably the development cycle.
Wecarried out some preliminary experiments to de-termine how the size of the tuning set affects thequality and speed of the system.Three tuning sets were considered.
The tun-ing set that was used for training the baseline sys-tem was built using the data from newstest2008-2010 filtering out sentences longer than 30 words.The second tuning set was built using all datafrom newstest2008-2011.
The final tuning setwas also built using the concatenation of the setsnewstest2008-2011.
All sentences in this set weredecoded with a baseline system and the output wasscored according to sentence-BLEU scores.
We se-173lected examples with high sentence-BLEU score ina way that penalizes excessively short examples2.Results of these experiments are shown in Table 4.Results show that there is some gain in BLEUscore when providing longer sentences during tun-ing.
Further experiments should consider tuningthe baseline with the newstest2008-2011 data, toeliminate variance caused by having different datasources.
Although the size of the third tuning set ismuch smaller than that of the other tuning sets, theBLEU score remains the same as when using thelargest tuning set.
The glue rule number, whichshows how many times the glue rule was applied,is lowest when tuning with the third data set.
Thetree depth number, which shows the depth of theresulting target parse tree, is higher for the thirdtuning set as compared to the baseline and similarto that resulted from using the largest tuning set.These numbers are all indicators of better utilisa-tion of the syntactic structure.Regarding efficiency, the baseline tuning set andthe filtered tuning set took about a third of the timeneeded to decode the larger tuning set.Therefore we could draw some initial conclu-sions that providing longer sentences is useful,but sentences for which some baseline system per-forms very poorly in terms of BLEU score can beeliminated from the tuning set.3.6 ResultsTable 5 summarizes the results for the systemssubmitted to the shared task.
The BLEU scores forthe phrase-based system submitted by the Univer-sity of Edinburgh are also shown for comparison.The syntax-based system had BLEU scores similarto those of the phrase-based system for German-English and English-German language pairs.
Forthe Czech-English and Russian-English languagepairs the syntax-based system was 2 BLEU pointsbehind the phrase-based system.However, in the manual evaluation, theGerman?English and English?German syntaxbased systems were ranked higher than the phrase-based systems.
For Czech?English, the syntaxsystems also came much closer than the BLEUscore would have indicated.The Russian-English system performed worsebecause we used much less of the available datafor training (leaving out Commoncrawl) and there-2Ongoing work by Eva Hasler.
Filtered data set was pro-vided in order to speed up experiment cycles.phrase-based syntax-basedBLEU manual BLEU manualen-de 20.1 0.571 19.4 0.614de-en 26.6 0.586 26.3 0.608cs-en 26.2 0.562 24.4 0.542ru-en 24.3 0.507 22.5 0.416Table 5: Cased BLEU scores and manual evalua-tion scores (?expected wins?)
on the newstest2013evaluation set for the phrase-based and syntax-based systems submitted by the University of Ed-inburgh.fore the extracted grammar is less reliable.
An-other reason was the mismatch in data format-ting for the Russian-English parallel corpus.
Allthe training data was lowercased which resulted inmore parsing errors.3.7 Post-Submission ExperimentsTable 6 shows results for some preliminary ex-periments carried out for the German-English lan-guage pair that were not included in the final sub-mission.
The baseline system is trained on allavailable parallel data and tuned on data fromnewstest2008-2010 filtered for sentences up to 30words.Tree restructuring ?
In one experiment theparse trees were restructured before training byleft binarization.
Tree restructuring is need to im-prove generalization power of rules extracted fromflat structures such as base noun phrases with sev-eral children.
The second raw in Table 6 showsthat the BLEU score did not improve and moreglue rules were applied when using left binariza-tion.
One reason for this result is that the rule ex-traction parameters MaxRuleDepth, MaxRuleSize,MaxNodes had the same values as in the baseline.Increasing this parameters should improve the ex-tracted grammar since binarizing the trees will in-crease these three dimensions.Verb dropping ?
A serious problem ofGerman?English machine translation is the ten-dency to drop verbs, which shatters sentence struc-ture.
One cause of this problem is the failure of theIBM Models to properly align the German verb toits English equivalent, since it is often dislocatedwith respect to English word order.
Further prob-lems appear when the main verb is not reordered inthe target sentence, which can result in lower lan-174newstest2012 newstest2013System Grammar size BLEU glue rule tree depth BLEU glue rule tree depthBaseline 55,310,162 23.21 5.42 4.03 26.27 4.23 3.80Left binarized 57,151,032 23.17 7.79 4.09 26.13 6.57 3.85Realigned vb 53,894,112 23.26 4.88 4.19 26.26 3.73 3.96Table 6: Cased BLEU scores for various German-English systems.System Vb droprulesVb Countnt2012Vb Countnt2013Baseline 1,038,597 9,216 8,418Realignedverbs 391,231 9,471 8,614Referencetranslation - 9,992 9,207Table 7: Statistics about verb dropping.guage model scores and BLEU scores.
Howeverthe syntax models handle the reordering of verbsbetter than phrase-based models.In an experiment we investigated how the num-ber of verbs dropped by the translation rules canbe reduced.
In order to reduce the number ofverb dropping rules we looked at unaligned verbsand realigned them before rule extraction.
An un-aligned verb in the source sentence was alignedto the verb in the target sentence for which IBMmodel 1 predicted the highest translation probabil-ity.
The third row in Table 6 shows the results ofthis experiment.
While there is no change in BLEUscore the number of glue rules applied is lower.Further analysis shows in Table 7 that the numberof verb dropping rules in the grammar is almostthree times lower and that there are more trans-lated verbs in the output when realigning verbs.4 ConclusionWe describe in detail the syntax-based machinetranslation systems that we developed for four Eu-ropean language pairs.
We achieved competitiveresults, especially for the language pairs involvingGerman.AcknowledgmentsThe research leading to these results has re-ceived funding from the European Union Sev-enth Framework Programme (FP7/2007-2013) un-der grant agreement 287658 (EU BRIDGE) andgrant agreement 288487 (MosesCore).ReferencesChen, S. F. and Goodman, J.
(1998).
An empiri-cal study of smoothing techniques for languagemodeling.
Technical report, Harvard University.Collins, M., Koehn, P., and Kucerova, I.
(2005).Clause restructuring for statistical machinetranslation.
In Proceedings of the 43rd AnnualMeeting of the Association for ComputationalLinguistics (ACL?05), pages 531?540, Ann Ar-bor, Michigan.
Association for ComputationalLinguistics.DeNeefe, S., Knight, K., Wang, W., and Marcu,D.
(2007).
What can syntax-based MT learnfrom phrase-based MT?
In Proceedings of the2007 Joint Conference on Empirical Methodsin Natural Language Processing and Compu-tational Natural Language Learning (EMNLP-CoNLL 2007).
June 28-30, 2007.
Prague, CzechRepublic.Galley, M., Graehl, J., Knight, K., Marcu, D., De-Neefe, S., Wang, W., and Thayer, I.
(2006).Scalable inference and training of context-richsyntactic translation models.
In ACL-44: Pro-ceedings of the 21st International Conferenceon Computational Linguistics and the 44th an-nual meeting of the Association for Computa-tional Linguistics, pages 961?968, Morristown,NJ, USA.
Association for Computational Lin-guistics.Galley, M., Hopkins, M., Knight, K., and Marcu,D.
(2004).
What?s in a translation rule?
In HLT-NAACL ?04.Gao, Q. and Vogel, S. (2008).
Parallel implemen-tations of word alignment tool.
In Software En-gineering, Testing, and Quality Assurance forNatural Language Processing, SETQA-NLP?08, pages 49?57, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Hopkins, M. and Langmead, G. (2010).
SCFGdecoding without binarization.
In Proceedingsof the 2010 Conference on Empirical Methods175in Natural Language Processing, pages 646?655, Cambridge, MA.
Association for Compu-tational Linguistics.Koehn, P. and Knight, K. (2003).
Empirical meth-ods for compound splitting.
In Proceedings ofMeeting of the European Chapter of the Associ-ation of Computational Linguistics (EACL).Koehn, P., Och, F. J., and Marcu, D. (2003).
Sta-tistical phrase-based translation.
In NAACL?03: Proceedings of the 2003 Conference ofthe North American Chapter of the Associationfor Computational Linguistics on Human Lan-guage Technology, pages 48?54, Morristown,NJ, USA.
Association for Computational Lin-guistics.Marcu, D., Wang, W., Echihabi, A., and Knight,K.
(2006).
SPMT: statistical machine transla-tion with syntactified target language phrases.In EMNLP ?06: Proceedings of the 2006 Con-ference on Empirical Methods in Natural Lan-guage Processing, pages 44?52, Morristown,NJ, USA.
Association for Computational Lin-guistics.Och, F. J.
(2003).
Minimum error rate trainingin statistical machine translation.
In Proceed-ings of the 41st Annual Meeting on Associationfor Computational Linguistics - Volume 1, ACL?03, pages 160?167, Morristown, NJ, USA.
As-sociation for Computational Linguistics.Petrov, S., Barrett, L., Thibaux, R., and Klein, D.(2006).
Learning accurate, compact, and in-terpretable tree annotation.
In Proceedings ofthe 21st International Conference on Computa-tional Linguistics and the 44th annual meetingof the Association for Computational Linguis-tics, ACL-44, pages 433?440, Stroudsburg, PA,USA.
Association for Computational Linguis-tics.Schmid, H. (2004).
Efficient parsing of highly am-biguous context-free grammars with bit vectors.In Proceedings of the 20th international con-ference on Computational Linguistics, COL-ING ?04, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Skut, W., Krenn, B., Brants, T., and Uszkoreit, H.(1997).
An annotation scheme for free word or-der languages.
In Proceedings of the Fifth Con-ference on Applied Natural Language Process-ing (ANLP-97).Stolcke, A.
(2002).
SRILM - an extensible lan-guage modeling toolkit.
In Intl.
Conf.
Spo-ken Language Processing, Denver, Colorado,September 2002.Wang, W., May, J., Knight, K., and Marcu, D.(2010).
Re-structuring, re-labeling, and re-aligning for syntax-based machine translation.Comput.
Linguist., 36(2):247?277.Williams, P. and Koehn, P. (2011).
Agreementconstraints for statistical machine translationinto german.
In Proceedings of the Sixth Work-shop on Statistical Machine Translation, pages217?226, Edinburgh, Scotland.
Association forComputational Linguistics.Williams, P. and Koehn, P. (2012).
Ghkm ruleextraction and scope-3 parsing in moses.
InProceedings of the Seventh Workshop on Sta-tistical Machine Translation, pages 388?394,Montre?al, Canada.
Association for Computa-tional Linguistics.176
