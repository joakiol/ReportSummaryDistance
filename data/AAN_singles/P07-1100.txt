Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 792?799,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsLearning to Compose Effective Strategies from a Library ofDialogue ComponentsMartijn Spitters?
Marco De Boni?
Jakub Zavrel?
Remko Bonnema??
Textkernel BV, Nieuwendammerkade 28/a17, 1022 AB Amsterdam, NL{spitters,zavrel,bonnema}@textkernel.nl?
Unilever Corporate Research, Colworth House, Sharnbrook, Bedford, UK MK44 1LQmarco.de-boni@unilever.comAbstractThis paper describes a method for automat-ically learning effective dialogue strategies,generated from a library of dialogue content,using reinforcement learning from user feed-back.
This library includes greetings, so-cial dialogue, chit-chat, jokes and relation-ship building, as well as the more usual clar-ification and verification components of dia-logue.
We tested the method through a mo-tivational dialogue system that encouragestake-up of exercise and show that it can beused to construct good dialogue strategieswith little effort.1 IntroductionInteractions between humans and machines have be-come quite common in our daily life.
Many ser-vices that used to be performed by humans havebeen automated by natural language dialogue sys-tems, including information seeking functions, asin timetable or banking applications, but also morecomplex areas such as tutoring, health coaching andsales where communication is much richer, embed-ding the provision and gathering of information ine.g.
social dialogue.
In the latter category of dia-logue systems, a high level of naturalness of interac-tion and the occurrence of longer periods of satisfac-tory engagement with the system are a prerequisitefor task completion and user satisfaction.Typically, such systems are based on a dialoguestrategy that is manually designed by an expertbased on knowledge of the system and the domain,and on continuous experimentation with test users.In this process, the expert has to make many de-sign choices which influence task completion anduser satisfaction in a manner which is hard to assess,because the effectiveness of a strategy depends onmany different factors, such as classification/ASRperformance, the dialogue domain and task, and,perhaps most importantly, personality characteris-tics and knowledge of the user.We believe that the key to maximum dialogue ef-fectiveness is to listen to the user.
This paper de-scribes the development of an adaptive dialogue sys-tem that uses the feedback of users to automaticallyimprove its strategy.
The system starts with a libraryof generic and task-/domain-specific dialogue com-ponents, including social dialogue, chit-chat, enter-taining parts, profiling questions, and informativeand diagnostic parts.
Given this variety of possi-ble dialogue actions, the system can follow manydifferent strategies within the dialogue state space.We conducted training sessions in which users inter-acted with a version of the system which randomlygenerates a possible dialogue strategy for each in-teraction (restricted by global dialogue constraints).After each interaction, the users were asked to re-ward different aspects of the conversation.
We ap-plied reinforcement learning to use this feedback tocompute the optimal dialogue policy.The following section provides a brief overviewof previous research related to this area and how ourwork differs from these studies.
We then proceedwith a concise description of the dialogue systemused for our experiments in section 3.
Section 4is about the training process and the reward model.Section 5 goes into detail about dialogue policy op-792timization with reinforcement learning.
In section 6we discuss our experimental results.2 Related WorkPrevious work has examined learning of effectivedialogue strategies for information seeking spo-ken dialogue systems, and in particular the use ofreinforcement learning methods to learn policiesfor action selection in dialogue management (seee.g.
Levin et al, 2000; Walker, 2000; Scheffler andYoung, 2002; Peek and Chickering, 2005; Framptonand Lemon, 2006), for selecting initiative and con-firmation strategies (Singh et al, 2002); for detect-ing speech recognition problem (Litman and Pan,2002); changing the dialogue according to the ex-pertise of the user (Maloor and Chai, 2000); adapt-ing responses according to previous interactionswith the users (Rudary et al, 2004); optimizingmixed initiative in collaborative dialogue (Englishand Heeman, 2005), and optimizing confirmations(Cuaya?huitl et al, 2006).
Other researchers havefocussed their attention on the learning aspect ofthe task, examining, for example hybrid reinforce-ment/supervised learning (Henderson et al, 2005).Previous work on learning dialogue managementstrategies has however generally been limited to welldefined areas of the dialogue, in particular dealingwith speech recognition and clarification problems,with small state spaces and a limited set of actionsto choose from (Henderson et al, 2005).
In a num-ber of contexts, however, dialogues need to have afar greater degree of complexity not just in the num-ber of states and possible actions but also in the va-riety of dialogue acts: for example in motivationaldialogue systems where the task is not limited toinformation gathering, slot-filling or querying of adatabase, and where dialogues must contain moresocial and relational elements to be successful (forthe usefulness of social dialogue see e.g.
Bickmore,2003; Liu and Picard, 2005).
Only little effort hasbeen directed to the question what dialogue compo-nents should make up the dialogue, involving deci-sions like how much and what type of social interac-tion should be used, different ways of forming a re-lationship with the user such as using chit-chat (forexample asking about a user?s hobbies or asking forthe user?s name), using humour, as well as the moreconventional tasks of clarifying user input, estab-lishing common ground and ensuring system repliesare appropriate.
Our work has focused on these as-pects of dialogue strategy construction, in order tocreate good dialogue strategies incorporating appro-priate levels of social interaction, humour, chit-chat,as well as successful information gathering and pro-vision.3 A Motivational Dialogue SystemThe domain of our system is physical exercise.
Thesystem is set up as an exercise advisor that asksthe user what is preventing him/her from exercis-ing more.
After the user has worded his/her exercise?barrier?, the system will give motivational advicefor how to overcome this barrier.
As an illustration,Table 1 shows an example dialogue, generated byour system.
Our dialogue system is text-based, soit does not involve speech recognition.
This sectiondescribes the system?s three main modules: the di-alogue state/action manager, the utterance classifierand the dialogue component library.Speaker Utterancesystem Good evening!system What?s your name?user my name is Brendasystem So Brenda, how are you today?user fine thank yousystem Tell me what is preventing you from exercisingmore.user I don?t have anyone to exercise with...system So your barrier is Loneliness (e.g.
You don?tlike exercising on your own ).user sort of, yeahsystem A solution for this barrier is: Try going on yourown and see whether it?s really that different togoing with a friend.
Try talking to other peoplewho are exercising, you?re likely to find theywelcome a little conversation!system Did you find this solution useful?user I guess...system Ok.
Try to work on this to overcome yourbarrier.system Bye!Table 1: Example dialogue.3.1 Dialogue state/action managementThe dialogue state is unique at every stage ofthe conversation and is represented as a vector offeature-values.
We use only a limited set of fea-tures because, as also noted in (Singh et al, 2002;Levin et al, 2000), it is important to keep the statespace as small as possible (but with enough distinc-793tive power to support learning) so we can constructa non-sparse Markov decision process (see section5) based on our limited training dialogues.
The statefeatures are listed in Table 2.Feature Values Descriptioncurnode c ?
N the current dialogue nodeactiontype utt, trans action typetrigger t ?
T utterance classifier categoryconfidence 1, 0 category confidenceproblem 1, 0 communication problem earlierTable 2: Dialogue state features.In each dialogue state, the dialogue manager willlook up the next action that should be taken.
In oursystem, an action is either a system utterance or atransition in the dialogue structure.
In the initialsystem, the dialogue structure was manually con-structed.
In many states, the next action requiresa choice to be made.
Dialogue states in which thesystem can choose among several possible actionsare called choice-states.
For example, in our sys-tem, immediately after greeting the user, the dia-logue structure allows for different directions: thesystem can first ask some personal questions, orit can immediately discuss the main topic withoutany digressions.
Utterance actions may also re-quire a choice (e.g.
directive versus open formula-tion of a question).
In training mode, the system willmake random choices in the choice-states.
This ap-proach will generate many different dialogue strate-gies, i.e.
paths through the dialogue structure.User replies are sent to an utterance classifier.
Thecategory assigned by this classifier is returned tothe dialogue manager and triggers a transition to thenext node in the dialogue structure.
The system alsoaccommodates a simple rule-based extraction mod-ule, which can be used to extract information fromuser utterances (e.g.
the user?s name, which is tem-plated in subsequent system prompts in order to per-sonalize the dialogue).3.2 Utterance classificationThe (memory-based) classifier uses a rich set of fea-tures for accurate classification, including words,phrases, regular expressions, domain-specific word-relations (using a taxonomy-plugin) and syntacti-cally motivated expressions.
For utterance pars-ing we used a memory-based shallow parser, calledMBSP (Daelemans et al, 1999).
This parser pro-vides part of speech labels, chunk brackets, subject-verb-object relations, and has been enriched with de-tection of negation scope and clause boundaries.The feature-matching mechanism in our classifi-cation system can match terms or phrases at speci-fied positions in the token stream of the utterance,also in combination with syntactic and semanticclass labels.
This allows us to define features that areparticularly useful for resolving confusing linguis-tic phenomena like ambiguity and negation.
A basefeature set was generated automatically, but quitea lot of features were manually tuned or added tocope with certain common dialogue situations.
Theoverall classification accuracy, measured on the dia-logues that were produced during the training phase,is 93.6%.
Average precision/recall is 98.6/97.3% forthe non-barrier categories (confirmation, negation,unwillingness, etc.
), and 99.1/83.4% for the barriercategories (injury, lack of motivation, etc.
).3.3 Dialogue Component LibraryThe dialogue component library contains genericas well as task-/domain-specific dialogue content,combining different aspects of dialogue (task/topicstructure, communication goals, etc.).
Table 3 listsall components in the library that was used for train-ing our dialogue system.
A dialogue component isbasically a coherent set of dialogue node represen-tations with a certain dialogue function.
The libraryis set up in a flexible, generic way: new componentscan easily be plugged in to test their usefulness indifferent dialogue contexts or for new domains.4 Training the Dialogue System4.1 Random strategy generationIn its training mode, the dialogue system uses ran-dom exploration: it generates different dialoguestrategies by choosing randomly among the allowedactions in the choice-states.
Note that dialogue gen-eration is constrained to contain certain fixed actionsthat are essential for task completion (e.g.
asking theexercise barrier, giving a solution, closing the ses-sion).
This excludes a vast number of useless strate-gies from exploration by the system.
Still, given allaction choices and possible user reactions, the totalnumber of unique dialogues that can be generated by794Component Description pa peStartSession Dialogue openings, including various greetings ?
?PersonalQuestionnaire Personal questions, e.g.
name; age; hobbies; interests, how are you today?
?ElizaChitChat Eliza-like chit-chat, e.g.
please go on...ExerciseChitChat Chit-chat about exercise, e.g.
have you been doing any exercise this week?
?Barrier Prompts concerning the barrier, e.g.
ask the barrier; barrier verification; ask a rephrase ?
?Solution Prompts concerning the solution, e.g.
give the solution; verify usefulness ?
?GiveBenefits Talk about the benefits of exercisingAskCommitment Ask user to commit his implementation of the given solution ?Encourage Encourage the user to work on the given solution ?
?GiveJoke The humor component: ask if the user wants to hear a joke; tell random jokes ?
?VerifyCloseSession Verification for closing the session (are you sure you want to close this session?)
?
?CloseSession Dialogue endings, including various farewells ?
?Table 3: Components in the dialogue component library.
The last two columns show which of the compo-nents was used in the learned policy (pa) and the expert policy (pe), discussed in section 6. ?
means thecomponent is always used, ?
means it is sometimes used, depending on the dialogue state.the system is approximately 345000 (many of whichare unlikely to ever occur).
During training, the sys-tem generated 490 different strategies.
There are 71choice-states that can actually occur in a dialogue.In our training dialogues, the opening state was ob-viously visited most frequently (572 times), almost60% of all states was visited at least 50 times, andonly 16 states were visited less than 10 times.4.2 The reward modelWhen the dialogue has reached its final state, a sur-vey is presented to the user for dialogue evaluation.The survey consists of five statements that can eachbe rated on a five-point scale (indicating the user?slevel of agreement).
The responses are mapped torewards of -2 to 2.
The statements we used are partlybased on the user survey that was used in (Singh etal., 2002).
We considered these statements to reflectthe most important aspects of conversation that arerelevant for learning a good dialogue policy.
Thefive statements we used are listed below.M1 Overall, this conversation went wellM2 The system understood what I saidM3 I knew what I could say at each point in the dialogueM4 I found this conversation engagingM5 The system provided useful advice4.3 Training set-upEight subjects carried out a total of 572 conversa-tions with the system.
Because of the variety of pos-sible exercise barriers known by the system (52 intotal) and the fact that some of these barriers aremore complex or harder to detect than others, thesystem?s classification accuracy depends largely onthe user?s barrier.
To prevent classification accuracydistorting the user evaluations, we asked the subjectsto act as if they had one of five predefined exercisebarriers (e.g.
Imagine that you don?t feel comfort-able exercising in public.
See what the advisor rec-ommends for this barrier to your exercise).5 Dialogue Policy Optimization withReinforcement LearningReinforcement learning refers to a class of machinelearning algorithms in which an agent explores anenvironment and takes actions based on its currentstate.
In certain states, the environment providesa reward.
Reinforcement learning algorithms at-tempt to find the optimal policy, i.e.
the policy thatmaximizes cumulative reward for the agent over thecourse of the problem.
In our case, a policy can beseen as a mapping from the dialogue states to thepossible actions in those states.
The environment istypically formulated as a Markov decision process(MDP).The idea of using reinforcement learning to au-tomate the design of strategies for dialogue systemswas first proposed by Levin et al (2000) and hassubsequently been applied in a.o.
(Walker, 2000;Singh et al, 2002; Frampton and Lemon, 2006;Williams et al, 2005).5.1 Markov decision processesWe follow past lines of research (such as Levin etal., 2000; Singh et al, 2002) by representing a dia-logue as a trajectory in the state space, determined795by the user responses and system actions: s1a1,r1????s2a2,r2????
.
.
.
snan,rn????
sn+1, in which siai,ri????
si+1means that the system performed action ai in statesi, received1 reward ri and changed to state si+1.In our system, a state is a dialogue context vectorof feature values.
This feature vector contains theavailable information about the dialogue so far thatis relevant for deciding what action to take next inthe current dialogue state.
We want the system tolearn the optimal decisions, i.e.
to choose the actionsthat maximize the expected reward.5.2 Q-value iterationThe field of reinforcement learning includes manyalgorithms for finding the optimal policy in an MDP(see Sutton and Barto, 1998).
We applied the algo-rithm of (Singh et al, 2002), as their experimentalset-up is similar to ours, constisting of: generationof (limited) exploratory dialogue data, using a train-ing system; creating an MDP from these data andthe rewards assigned by the training users; off-linepolicy learning based on this MDP.The Q-function for a certain action taken in a cer-tain state describes the total reward expected be-tween taking that action and the end of the dialogue.For each state-action pair (s, a), we calculated thisexpected cumulative reward Q(s, a) of taking actiona from state s, with the following equation (Suttonand Barto, 1998; Singh et al, 2002):Q(s, a) = R(s, a) + ?
?s?P (s?|s, a)maxa?Q(s?, a?
)(1)where: P (s?|s, a) is the probability of a transitionfrom state s to state s?
by taking action a, andR(s, a) is the expected reward obtained when tak-ing action a in state s. ?
is a weight (0 ?
?
?
1),that discounts rewards obtained later in time whenit is set to a value < 1.
In our system, ?
was setto 1.
Equation 1 is recursive: the Q-value of a cer-tain state is computed in terms of the Q-values ofits successor states.
The Q-values can be estimatedto within a desired threshold using Q-value iteration(Sutton and Barto, 1998).
Once the value iteration1In our experiments, we did not make use of immediate re-warding (e.g.
at every turn) during the conversation.
Rewardswere given after the final state of the dialogue had been reached.process is completed, by selecting the action withthe maximum Q-value (the maximum expected fu-ture reward) at each choice-state, we can obtain theoptimal dialogue policy pi.6 Results and Discussion6.1 Reward analysisFigure 1 shows a graph of the distribution of the fivedifferent evaluation measures in the training data(see section 4.2 for the statement wordings).
M1is probably the most important measure of success.The distribution of this reward is quite symmetri-cal, with a slightly higher peak in the positive area.The distribution of M2 shows that M1 and M2 arerelated.
From the distribution of M4 we can con-clude that the majority of dialogues during the train-ing phase was not very engaging.
Users obviouslyhad a good feeling about what they could say at eachpoint in the dialogue (M3), which implies good qual-ity of the system prompts.
The judgement about theusefulness of the provided advice is pretty average,tending a bit more to negative than to positive.
Wedo think that this measure might be distorted by thefact that we asked the subjects to imagine that theyhave the given exercise barriers.
Furthermore, theywere sometimes confronted with advice that had al-ready been presented to them in earlier conversa-tions.050100150200250-2 -1  0  1  2Number of dialoguesRewardReward distributionsM1M2M3M4M5Figure 1: Reward distributions in the training data.In our analysis of the users?
rewarding behavior,we found several significant correlations.
We foundthat longer dialogues (> 3 user turns) are appreci-ated more than short ones (< 4 user turns), whichseems rather logical, as dialogues in which the user796barely gets to say anything are neither natural norengaging.We also looked at the relationship between userinput verification and the given rewards.
Our intu-ition is that the choice of barrier verification is oneof the most important choices the system can makein the dialogue.
We found that it is much better tofirst verify the detected barrier than to immediatelygive advice.
The percentage of appropriate adviceprovided in dialogues with barrier verification is sig-nificantly higher than in dialogues without verifica-tion.In several states of the dialogue, we let the sys-tem choose from different wordings of the systemprompt.
One of these choices is whether to use anopen question to ask what the user?s barrier is (Howcan I help you?
), or a directive question (Tell mewhat is preventing you from exercising more.).
Themotivation behind the open question is that the usergets the initiative and is basically free to talk aboutanything he/she likes.
Naturally, the advantage ofdirective questions is that the chance of making clas-sification errors is much lower than with open ques-tions because the user will be better able to assesswhat kind of answer the system expects.
Dialoguesin which the key-question (asking the user?s barrier)was directive, were rewarded more positively thandialogues with the open question.6.2 Learned dialogue policiesWe learned a different policy for each evaluationmeasure separately (by only using the rewards givenfor that particular measure), and a policy based ona combination (sum) of the rewards for all evalu-ation measures.
We found that the learned policybased on the combination of all measures, and thepolicy based on measure M1 alone (Overall, thisconversation went well) were nearly identical.
Ta-ble 4 compares the most important decisions of thedifferent policies.
For convenience of comparison,we only listed the main, structural choices.
Table 3shows which of the dialogue components in the li-brary were used in the learned and the expert policy.Note that, for the sake of clarity, the state descrip-tions in Table 4 are basically summaries of a set ofmore specific states since a state is a specific repre-sentation of the dialogue context at a particular mo-ment (composed of the values of the features listedin Table 2).
For instance, in the pa policy, the deci-sion in the last row of the table (give a joke or not),depends on whether or not there has been a classifi-cation failure (i.e.
a communication problem earlierin the dialogue).
If there has been a classificationfailure, the policy prescribes the decision not to givea joke, as it was not appreciated by the training usersin that context.
Otherwise, if there were no commu-nication problems during the conversation, the usersdid appreciate a joke.6.3 EvaluationWe compared the learned dialogue policy with a pol-icy which was independently hand-designed by ex-perts2 for this system.
The decisions made in thelearned strategy were very similar to the ones madeby the experts, with only a few differences, indicat-ing that the automated method would indeed per-form as well as an expert.
The main differenceswere the inclusion of a personal questionnaire for re-lation building at the beginning of the dialogue anda commitment question at the end of the dialogue.Another difference was the more restricted use ofthe humour element, described in section 6.2 whichturns out to be intuitively better than the expert?s de-cision to simply always include a joke.
Of course,we can only draw conclusions with regard to the ef-fectiveness of these two policies if we empiricallycompare them with real test users.
Such evaluationsare planned as part of our future research.As some additional evidence against the possibil-ity that the learned policy was generated by chance,we performed a simple experiment in which we tookseveral random samples of 300 training dialoguesfrom the complete training set.
For each sample, welearned the optimal policy.
We mutually comparedthese policies and found that they were very similar:only in 15-20% of the states, the policies disagreedon which action to take next.
On closer inspectionwe found that this disagreement mainly concernedstates that were poorly visited (1-10 times) in thesesamples.
These results suggest that the learned pol-icy is unreliable at infrequently visited states.
Notehowever, that all main decisions listed in Table 4 are2The experts were a team made up of psychologists withexperience in the psychology of health behaviour change anda scientist with experience in the design of automated dialoguesystems.797State description Action choices p1 p2 p3 p4 p5 pa peAfter greeting the user - ask the exercise barrier ?
?
?- ask personal information ?
?
?
?- chit-chat about exerciseWhen asking the barrier - use a directive question ?
?
?
?
?
?
?- use an open questionUser gives exercise barrier - verify detected barrier ?
?
?
?
?
?
?- give solutionUser rephrased barrier - verify detected barrier ?
?
?
?
?
?- give solution ?Before presenting solution - ask if the user wants to see a solution for the barrier ?- give a solution ?
?
?
?
?
?After presenting solution - verify solution usefulness ?
?
?
?
?
?- encourage the user to work on the given solution ?- ask user to commit solution implementationUser found solution useful - encourage the user to work on the solution ?
?
?
?- ask user to commit solution implementation ?
?
?User found solution not useful - give another solution ?
?
?
?
?
?
?- ask the user wants to propose his own solutionAfter giving second solution - verify solution usefulness ?
?- encourage the user to work on the given solution ?
?
?
?- ask user to commit solution implementation ?End of dialogue - close the session ?
?
?- ask if the user wants to hear a joke ?
?
?
?Table 4: Comparison of the most important decisions made by the learned policies.
pn is the policy basedon evaluation measure n; pa is the policy based on all measures; pe contains the decisions made by expertsin the manually designed policy.made at frequently visited states.
The only disagree-ment in frequently visited states concerned system-prompt choices.
We might conclude that these par-ticular (often very subtle) system-prompt choices(e.g.
careful versus direct formulation of the exercisebarrier) are harder to learn than the more noticabledialogue structure-related choices.7 Conclusions and Future WorkWe have explored reinforcement learning for auto-matic dialogue policy optimization in a question-based motivational dialogue system.
Our system canautomatically compose a dialogue strategy from a li-brary of dialogue components, that is very similarto a manually designed expert strategy, by learningfrom user feedback.Thus, in order to build a new dialogue system,dialogue system engineers will have to set up arough dialogue template containing several ?multi-ple choice?-action nodes.
At these nodes, variousdialogue components or prompt wordings (e.g.
en-tertaining parts, clarification questions, social dia-logue, personal questions) from an existing or self-made library can be plugged in without knowing be-forehand which of them would be most effective.The automatically generated dialogue policy isvery similar (see Table 4) ?but arguably improved inmany details?
to the hand-designed policy for thissystem.
Automatically learning dialogue policiesalso allows us to test a number of interesting issuesin parallel, for example, we have learned that usersappreciated dialogues that were longer, starting withsome personal questions (e.g What is your name?,What are your hobbies?).
We think that altogether,this relation building component gave the dialoguea more natural and engaging character, although itwas left out in the expert strategy.We think that the methodology described in thispaper may be able to yield more effective dialoguepolicies than experts.
Especially in complicated di-alogue systems with large state spaces.
In our sys-tem, state representations are composed of multiplecontext feature values (e.g.
communication problemearlier in the dialogue, the confidence of the utter-ance classifier).
Our experiments showed that some-times different decisions were learned in dialoguecontexts where only one of these features was differ-ent (for example use humour only if the system hasbeen successful in recognising a user?s exercise bar-rier): all context features are implicitly used to learnthe optimal decisions and when hand-designing a di-798alogue policy, experts can impossibly take into ac-count all possible different dialogue contexts.With respect to future work, we plan to examinethe impact of different state representations.
We didnot yet empirically compare the effects of each fea-ture on policy learning or experiment with other fea-tures than the ones listed in Table 2.
As Tetreault andLitman (2006) show, incorporating more or differentinformation into the state representation might how-ever result in different policies.Furthermore, we will evaluate the actual generic-ity of our approach by applying it to different do-mains.
As part of that, we will look at automaticallymining libraries of dialogue components from ex-isting dialogue transcript data (e.g.
available scriptsor transcripts of films, tv series and interviews con-taining real-life examples of different types of dia-logue).
These components can then be plugged intoour current adaptive system in order to discover whatworks best in dialogue for new domains.
We shouldnote here that extending the system?s dialogue com-ponent library will automatically increase the statespace and thus policy generation and optimizationwill become more difficult and require more train-ing data.
It will therefore be very important to care-fully control the size of the state space and the globalstructure of the dialogue.AcknowledgementsThe authors would like to thank Piroska LendvaiRudenko, Walter Daelemans, and Bob Hurling fortheir contributions and helpful comments.
We alsothank the anonymous reviewers for their useful com-ments on the initial version of this paper.ReferencesTimothy W. Bickmore.
2003.
Relational Agents: Ef-fecting Change through Human-Computer Relationships.Ph.D.
Thesis, MIT, Cambridge, MA.Heriberto Cuaya?huitl, Steve Renals, Oliver Lemon, and HiroshiShimodaira.
2006.
Learning multi-goal dialogue strate-gies using reinforcement learning with reduced state-actionspaces.
Proceedings of Interspeech-ICSLP.Walter Daelemans, Sabine Buchholz, and Jorn Veenstra.
1999.Memory-Based Shallow Parsing.
Proceedings of CoNLL-99, Bergen, Norway.Michael S. English and Peter A. Heeman 2005.
Learn-ing Mixed Initiative Dialog Strategies By Using Reinforce-ment Learning On Both Conversants.
Proceedings ofHLT/NAACL.Matthew Frampton and Oliver Lemon.
2006.
Learning MoreEffective Dialogue Strategies Using Limited Dialogue MoveFeatures.
Proceedings of the Annual Meeting of the ACL.James Henderson, Oliver Lemon, and Kallirroi Georgila.
2005.Hybrid Reinforcement/Supervised Learning for DialoguePolicies from COMMUNICATOR Data.
IJCAI workshop onKnowledge and Reasoning in Practical Dialogue Systems.Esther Levin, Roberto Pieraccini, and Wieland Eckert.
2000.
AStochastic Model of Human-Machine Interaction for Learn-ing Dialog Strategies.
IEEE Trans.
on Speech and AudioProcessing, Vol.
8, No.
1, pp.
11-23.Diane J. Litman and Shimei Pan.
2002.
Designing and Eval-uating an Adaptive Spoken Dialogue System.
User Model-ing and User-Adapted Interaction, Volume 12, Issue 2-3, pp.111-137.Karen K. Liu and Rosalind W. Picard.
2005.
Embedded Em-pathy in Continuous, Interactive Health Assessment.
CHIWorkshop on HCI Challenges in Health Assessment, Port-land, Oregon.Preetam Maloor and Joyce Chai.
2000.
Dynamic User Leveland Utility Measurement for Adaptive Dialog in a Help-Desk System.
Proceedings of the 1st Sigdial Workshop.Tim Paek and David M. Chickering.
2005.
The Markov As-sumption in Spoken Dialogue Management.
Proceedings ofSIGDIAL 2005.Matthew Rudary, Satinder Singh, and Martha E. Pollack.2004.
Adaptive cognitive orthotics: Combining reinforce-ment learning and constraint-based temporal reasoning.
Pro-ceedings of the 21st International Conference on MachineLearning.Konrad Scheffler and Steve Young.
2002.
Automatic learningof dialogue strategy using dialogue simulation and reinforce-ment learning.
Proceedings of HLT-2002.Satinder Singh, Diane Litman, Michael Kearns, and MarilynWalker.
2002.
Optimizing Dialogue Management with Re-inforcement Learning: Experiments with the NJFun System.Journal of Artificial Intelligence Research (JAIR), Volume16, pages 105-133.Richard S. Sutton and Andrew G. Barto.
1998.
ReinforcementLearning.
MIT Press.Joel R. Tetreault and Diane J. Litman 2006.
Comparingthe Utility of State Features in Spoken Dialogue Using Re-inforcement Learning.
Proceedings of HLT/NAACL, NewYork.Marilyn A. Walker 2000.
An Application of ReinforcementLearning to Dialogue Strategy Selection in a Spoken Dia-logue System for Email.
Journal of Artificial IntelligenceResearch, Vol 12., pp.
387-416.Jason D. Williams, Pascal Poupart, and Steve Young.
2005.Partially Observable Markov Decision Processes with Con-tinuous Observations for Dialogue Management.
Proceed-ings of the 6th SigDial Workshop, September 2005, Lisbon.799
