Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 191?198,Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005Word Graphs for Statistical Machine TranslationRichard Zens and Hermann NeyChair of Computer Science VIRWTH Aachen University{zens,ney}@cs.rwth-aachen.deAbstractWord graphs have various applications inthe field of machine translation.
Thereforeit is important for machine translation sys-tems to produce compact word graphs ofhigh quality.
We will describe the gen-eration of word graphs for state of theart phrase-based statistical machine trans-lation.
We will use these word graphto provide an analysis of the search pro-cess.
We will evaluate the quality of theword graphs using the well-known graphword error rate.
Additionally, we intro-duce the two novel graph-to-string crite-ria: the position-independent graph worderror rate and the graph BLEU score.Experimental results are presented for twoChinese?English tasks: the small IWSLTtask and the NIST large data track task.For both tasks, we achieve significant re-ductions of the graph error rate alreadywith compact word graphs.1 IntroductionA statistical machine translation system usually pro-duces the single-best translation hypotheses for asource sentence.
For some applications, we are alsointerested in alternative translations.
The simplestway to represent these alternatives is a list with theN -best translation candidates.
These N -best listshave one major disadvantage: the high redundancy.The translation alternatives may differ only by a sin-gle word, but still both are listed completely.
Usu-ally, the size of the N -best list is in the range of a fewhundred up to a few thousand candidate translationsper source sentence.
If we want to use larger N -bestlists the processing time gets very soon infeasible.Word graphs are a much more compact represen-tation that avoid these redundancies as much as pos-sible.
The number of alternatives in a word graph isusually an order of magnitude larger than in an N -best list.
The graph representation avoids the com-binatorial explosion that make large N -best lists in-feasible.Word graphs are an important data structure withvarious applications:?
Word Filter.The word graph is used as a compact repre-sentation of a large number of sentences.
Thescore information is not contained.?
Rescoring.We can use word graphs for rescoring withmore sophisticated models, e.g.
higher-orderlanguage models.?
Discriminative Training.The training of the model scaling factors as de-scribed in (Och and Ney, 2002) was done onN -best lists.
Using word graphs instead couldfurther improve the results.
Also, the phrasetranslation probabilities could be trained dis-crimatively, rather than only the scaling factors.?
Confidence Measures.Word graphs can be used to derive confidencemeasures, such as the posterior probability(Ueffing and Ney, 2004).191?
Interactive Machine Translation.Some interactive machine translation systemsmake use of word graphs, e.g.
(Och et al,2003).State Of The Art.
Although there are these manyapplications, there are only few publications directlydevoted to word graphs.
The only publication, weare aware of, is (Ueffing et al, 2002).
The short-comings of (Ueffing et al, 2002) are:?
They use single-word based models only.
Cur-rent state of the art statistical machine transla-tion systems are phrase-based.?
Their graph pruning method is suboptimal as itconsiders only partial scores and not full pathscores.?
The N -best list extraction does not eliminateduplicates, i.e.
different paths that represent thesame translation candidate.?
The rest cost estimation is not efficient.
It hasan exponential worst-case time complexity.
Wewill describe an algorithm with linear worst-case complexity.Apart from (Ueffing et al, 2002), publications onweighted finite state transducer approaches to ma-chine translation, e.g.
(Bangalore and Riccardi,2001; Kumar and Byrne, 2003), deal with wordgraphs.
But to our knowledge, there are no publica-tions that give a detailed analysis and evaluation ofthe quality of word graphs for machine translation.We will fill this gap and give a systematic descrip-tion and an assessment of the quality of word graphsfor phrase-based machine translation.
We will showthat even for hard tasks with very large vocabularyand long sentences the graph error rate drops signif-icantly.The remaining part is structured as follows: firstwe will give a brief description of the translation sys-tem in Section 2.
In Section 3, we will give a def-inition of word graphs and describe the generation.We will also present efficient pruning and N -bestlist extraction techniques.
In Section 4, we will de-scribe evaluation criteria for word graphs.
We willuse the graph word error rate, which is well knownfrom speech recognition.
Additionally, we introducethe novel position-independent word graph error rateand the graph BLEU score.
These are generaliza-tions of the commonly used string-to-string evalua-tion criteria in machine translation.
We will presentexperimental results in Section 5 for two Chinese?English tasks: the first one, the IWSLT task, is in thedomain of basic travel expression found in phrase-books.
The vocabulary is limited and the sentencesare short.
The second task is the NIST Chinese?English large data track task.
Here, the domain isnews and therefore the vocabulary is very large andthe sentences are with an average of 30 words quitelong.2 Translation SystemIn this section, we give a brief description of thetranslation system.
We use a phrase-based transla-tion approach as described in (Zens and Ney, 2004).The posterior probability Pr(eI1|fJ1 ) is modeled di-rectly using a weighted log-linear combination ofa trigram language model and various translationmodels: a phrase translation model and a word-based lexicon model.
These translation models areused for both directions: p(f |e) and p(e|f).
Addi-tionally, we use a word penalty and a phrase penalty.With the exception of the language model, all mod-els can be considered as within-phrase models asthey depend only on a single phrase pair, but not onthe context outside of the phrase.
The model scalingfactors are optimized with respect to some evalua-tion criterion (Och, 2003).We extended the monotone search algorithm from(Zens and Ney, 2004) such that reorderings are pos-sible.
In our case, we assume that local reorder-ings are sufficient.
Within a certain window, allpossible permutations of the source positions are al-lowed.
These permutations are represented as a re-ordering graph, similar to (Zens et al, 2002).
Oncewe have this reordering graph, we perform a mono-tone phrase-based translation of this graph.
Moredetails of this reordering approach are described in(Kanthak et al, 2005).3 Word Graphs3.1 DefinitionA word graph is a directed acyclic graph G = (V, E)with one designated root node n0 ?
V .
The edgesare labeled with words and optionally with scores.We will use (n, n?, w) to denote an edge from node192n to node n?
with word label w. Each path throughthe word graph represents a translation candidate.
Ifthe word graph contains scores, we accumulate theedge scores along a path to get the sentence or stringscore.The score information the word graph has to con-tain depends on the application.If we want to use the word graph as a word fil-ter, we do not need any score information at all.
Ifwe want to extract the single- or N -best hypotheses,we have to retain the string or sentence score infor-mation.
The information about the hidden variablesof the search, e.g.
the phrase segmentation, is notneeded for this purpose.
For discriminative trainingof the phrase translation probabilities, we need allthe information, even about the hidden variables.3.2 GenerationIn this section, we analyze the search process in de-tail.
Later, in Section 5, we will show the (experi-mental) complexity of each step.
We start with thesource language sentence that is represented as a lin-ear graph.
Then, we introduce reorderings into thisgraph as described in (Kanthak et al, 2005).
Thetype of reordering should depend on the languagepair.
In our case, we assume that only local reorder-ings are required.
Within a certain window, all pos-sible reorderings of the source positions are allowed.These permutations are represented as a reorderinggraph, similar to (Knight and Al-Onaizan, 1998) and(Zens et al, 2002).Once we have this reordering graph, we performa monotone phrase-based translation of this graph.This translation process consists of the followingsteps that will be described afterward:1. segment into phrase2.
translate the individual phrases3.
split the phrases into words4.
apply the language modelNow, we will describe each step.
The first step isthe segmentation into phrases.
This can be imag-ined as introducing ?short-cuts?
into the graph.
Thephrase segmentation does not affect the number ofnodes, because only additional edges are added tothe graph.In the segmented graph, each edge represents asource phrase.
Now, we replace each edge with oneedge for each possible phrase translation.
The edgescores are the combination of the different transla-tion probabilities, namely the within-phrase modelsmentioned in Section 2.
Again, this step does notincrease the number of nodes, but only the numberof edges.So far, the edge labels of our graph are phrases.
Inthe final word graph, we want to have words as edgelabels.
Therefore, we replace each edge representinga multi-word target phrase with a sequence of edgesthat represent the target word sequence.
Obviously,edges representing a single-word phrase do not haveto be changed.As we will show in the results section, the wordgraphs up to this point are rather compact.
Thescore information in the word graph so far consistsof the reordering model scores and the phrase trans-lation model scores.
To obtain the sentence posteriorprobability p(eI1|fJ1 ), we apply the target languagemodel.
To do this, we have to separate paths accord-ing to the language model history.
This increases theword graph size by an order of magnitude.Finally, we have generated a word graph with fullsentence scores.
Note that the word graph may con-tain a word sequence multiple times with differenthidden variables.
For instance, two different seg-mentations into source phrases may result in thesame target sentence translation.The described steps can be implemented usingweighted finite state transducer, similar to (Kumarand Byrne, 2003).3.3 PruningTo adjust the size of the word graph to the desireddensity, we can reduce the word graph size usingforward-backward pruning, which is well-known inthe speech recognition community, e.g.
see (Manguet al, 2000).
This pruning method guarantees thatthe good strings (with respect to the model scores)remain in the word graph, whereas the bad ones areremoved.
The important point is that we comparethe full path scores and not only partial scores as, forinstance, in the beam pruning method in (Ueffing etal., 2002).The forward probabilities F (n) and backwardprobabilities B(n) of a node n are defined by the193following recursive equations:F (n) =?
(n?,n,w)?EF (n?)
?
p(n?, n, w)B(n) =?(n,n?,w)?EB(n?)
?
p(n, n?, w)The forward probability of the root node and thebackward probabilities of the final nodes are initial-ized with one.
Using a topological sorting of thenodes, the forward and backward probabilities canbe computed with linear time complexity.
The pos-terior probability q(n, n?, w) of an edge is definedas:q(n, n?, w) = F (n) ?
p(n, n?, w) ?
B(n?
)B(n0)The posterior probability of an edge is identical tothe sum over the probabilities of all full paths thatcontain this edge.
Note that the backward probabil-ity of the root node B(n0) is identical to the sumover all sentence probabilities in the word graph.Let q?
denoted the maximum posterior probabilityof all edges and let ?
be a pruning threshold, thenwe prune an edge (n, n?, w) if:q(n, n?, w) < q?
?
?3.4 N -Best List ExtractionIn this section, we describe the extraction of the N -best translation candidates from a word graph.
(Ueffing et al, 2002) and (Mohri and Riley, 2002)both present an algorithm based on the same idea:use a modified A* algorithm with an optimal restcost estimation.
As rest cost estimation, the negatedlogarithm of the backward probabilities is used.
Thealgorithm in (Ueffing et al, 2002) has two disadvan-tages: it does not care about duplicates and the restcost computation is suboptimal as the described al-gorithm has an exponential worst-case complexity.As mentioned in the previous section, the backwardprobabilities can be computed in linear time.In (Mohri and Riley, 2002) the word graph is rep-resented as a weighted finite state automaton.
Theword graph is first determinized, i.e.
the nondeter-ministic automaton is transformed in an equivalentdeterministic automaton.
This process removes theduplicates from the word graph.
Out of this deter-minized word graph, the N best candidates are ex-tracted.
In (Mohri and Riley, 2002), ?-transitions areignored, i.e.
transitions that do not produce a word.These ?-transitions usually occur in the backing-offcase of language models.
The ?-transitions have tobe removed before using the algorithm of (Mohriand Riley, 2002).
In the presence of ?-transitions,two path representing the same string are consideredequal only if the ?-transitions are identical as well.4 Evaluation Criteria4.1 String-To-String CriteriaTo evaluate the single-best translation hypotheses,we use the following string-to-string criteria: worderror rate (WER), position-independent word errorrate (PER) and the BLEU score.
More details onthese standard criteria can be found for instance in(Och, 2003).4.2 Graph-To-String CriteriaTo evaluate the quality of the word graphs, wegeneralize the string-to-string criteria to work onword graphs.
We will use the well-known graphword error rate (GWER), see also (Ueffing et al,2002).
Additionally, we introduce two novel graph-to-string criteria, namely the position-independentgraph word error rate (GPER) and the graph BLEUscore (GBLEU).
The idea of these graph-to-stringcriteria is to choose a sequence from the word graphand compute the corresponding string-to-string cri-terion for this specific sequence.
The choice of thesequence is such that the criterion is the optimumover all possible sequences in the word graph, i.e.the minimum for GWER/GPER and the maximumfor GBLEU.The GWER is a generalization of the word er-ror rate.
It is a lower bound for the WER.
It can becomputed using a dynamic programming algorithmwhich is quite similar to the usual edit distance com-putation.
Visiting the nodes of the word graph intopological order helps to avoid repeated computa-tions.The GPER is a generalization of the position-independent word error rate.
It is a lower bound forthe PER.
The computation is not as straightforwardas for the GWER.In (Ueffing and Ney, 2004), a method for com-puting the string-to-string PER is presented.
Thismethod cannot be generalized for the graph-to-stringcomputation in a straightforward way.
Therefore,194we will first describe an alternative computation forthe string-to-string PER and then use this idea forthe graph-to-string PER.Now, we want to compute the number of position-independent errors for two strings.
As the word or-der of the strings does not matter, we represent themas multisets1 A and B.
To do this, it is sufficient toknow how many words are in A but not in B, i.e.a := |A?B|, and how many words are in B but notin A, i.e.
b := |B?A|.
The number of substitutions,insertions and deletions are then:sub = min{a, b}ins = a ?
subdel = b ?
suberror = sub + ins + del= a + b ?
min{a, b}= max{a, b}It is obvious that there are either no insertions or nodeletions.
The PER is then computed as the num-ber of errors divided by the length of the referencestring.Now, back to the graph-to-string PER computa-tion.
The information we need at each node of theword graph are the following: the remaining multi-set of words of the reference string that are not yetproduced.
We denote this multiset C. The cardinal-ity of this multiset will become the value a in thepreceding notation.
In addition to this multiset, wealso need to count the number of words that we haveproduced on the way to this node but which are notin the reference string.
The identity of these words isnot important, we simply have to count them.
Thiscount will become the value b in the preceding nota-tion.If we make a transition to a successor node alongan edge labeled w, we remove that word w from theset of remaining reference words C or, if the wordw is not in this set, we increase the count of wordsthat are in the hypothesis but not in the reference.To compute the number of errors on a graph, weuse the auxiliary quantity Q(n, C), which is thecount of the produced words that are not in the refer-ence.
We use the following dynamic programmingrecursion equations:1A multiset is a set that may contain elements multipletimes.Q(n0, C0) = 0Q(n, C) = minn?,w:(n?,n,w)?E{Q(n?, C ?
{w}),Q(n?, C) + 1}Here, n0 denote the root node of the word graph,C0 denotes the multiset representation of the refer-ence string.
As already mentioned in Section 3.1,(n?, n, w) denotes an edge from node n?
to node nwith word label w.In the implementation, we use a bit vector to rep-resent the set C for efficiency reasons.
Note that inthe worst-case the size of the Q-table is exponen-tial in the length of the reference string.
However, inpractice we found that in most cases the computationis quite fast.The GBLEU score is a generalization of theBLEU score.
It is an upper bound for the BLEUscore.
The computation is similar to the GPER com-putation.
We traverse the word graph in topologi-cal order and store the following information: thecounts of the matching n-grams and the length of thehypothesis, i.e.
the depth in the word graph.
Addi-tionally, we need the multiset of reference n-gramsthat are not yet produced.To compute the BLEU score, the n-gram countsare collected over the whole test set.
This results ina combinatorial problem for the computation of theGBLEU score.
We process the test set sentence-wiseand accumulate the n-gram counts.
After each sen-tence, we take a greedy decision and choose the n-gram counts that, if combined with the accumulatedn-gram counts, result is the largest BLEU score.This gives a conservative approximation of the trueGBLEU score.4.3 Word Graph SizeTo measure the word graph size we use the wordgraph density, which we define as the number ofedges in the graph divided by the source sentencelength.5 Experimental Results5.1 TasksWe will show experimental results for two Chinese?English translation tasks.195Table 1: IWSLT Chinese?English Task: corpusstatistics of the bilingual training data.Chinese EnglishTrain Sentences 20 000Running Words 182 904 160 523Vocabulary 7 643 6 982Test Sentences 506Running Words 3 515 3 595avg.
SentLen 6.9 7.1Table 2: NIST Chinese English task: corpus statis-tics of the bilingual training data.Chinese EnglishTrain Sentences 3.2MRunning Words 51.4M 55.5MVocabulary 80 010 170 758Lexicon Entries 81 968Test Sentences 878Running Words 26 431 23 694avg.
SentLen 30.1 27.0IWSLT Chinese?English Task.
The first task isthe Chinese?English supplied data track task of theInternational Workshop on Spoken Language Trans-lation (IWSLT 2004) (Akiba et al, 2004).
The do-main is travel expressions from phrase-books.
Thisis a small task with a clean training and test corpus.The vocabulary is limited and the sentences are rel-atively short.
The corpus statistics are shown in Ta-ble 1.
The Chinese part of this corpus is alreadysegmented into words.NIST Chinese?English Task.
The second taskis the NIST Chinese?English large data track task.For this task, there are many bilingual corpora avail-able.
The domain is news, the vocabulary is verylarge and the sentences have an average length of 30words.
We train our statistical models on variouscorpora provided by LDC.
The Chinese part is seg-mented using the LDC segmentation tool.
After thepreprocessing, our training corpus consists of aboutthree million sentences with somewhat more than 50million running words.
The corpus statistics of thepreprocessed training corpus are shown in Table 2.We use the NIST 2002 evaluation data as test set.15202530354045500  200  400  600  800  1000  1200graphworderror rate[%]word graph densitywindow-size-1window-size-2window-size-3window-size-4window-size-5Figure 1: IWSLT Chinese?English: Graph error rateas a function of the word graph density for differentwindow sizes.5.2 Search Space AnalysisIn Table 3, we show the search space statistics of theIWSLT task for different reordering window sizes.Each line shows the resulting graph densities afterthe corresponding step in our search as described inSection 3.2.
Our search process starts with the re-ordering graph.
The segmentation into phrases in-creases the graph densities by a factor of two.
Doingthe phrase translation results in an increase of thedensities by a factor of twenty.
Unsegmenting thephrases, i.e.
replacing the phrase edges with a se-quence of word edges doubles the graph sizes.
Ap-plying the language model results in a significant in-crease of the word graphs.Another interesting aspect is that increasing thewindow size by one roughly doubles the searchspace.5.3 Word Graph Error RatesIn Figure 1, we show the graph word error rate forthe IWSLT task as a function of the word graph den-sity.
This is done for different window sizes forthe reordering.
We see that the curves start with asingle-best word error rate of about 50%.
For themonotone search, the graph word error rate goesdown to about 31%.
Using local reordering duringthe search, we can further decrease the graph worderror rate down to less than 17% for a window sizeof 5.
This is almost one third of the single-best worderror rate.
If we aim at halving the single-best worderror rate, word graphs with a density of less than196Table 3: IWSLT Chinese?English: Word graph densities for different window sizes and different stages ofthe search process.language level graph type window size1 2 3 4 5source word reordering 1.0 2.7 6.2 12.8 24.4phrase segmented 2.0 5.0 12.1 26.8 55.6target translated 40.8 99.3 229.0 479.9 932.8word TM scores 78.6 184.6 419.2 869.1 1 670.4+ LM scores 958.2 2874.2 7649.7 18 029.7 39 030.120253035404550556065700  200  400  600  800  1000  1200  1400graphworderror rate[%]word graph densitywindow-size-1window-size-2window-size-3window-size-4window-size-5Figure 2: NIST Chinese?English: Graph error rateas a function of the word graph density for differentwindow sizes.200 would already be sufficient.In Figure 2, we show the same curves for theNIST task.
Here, the curves start from a single-bestword error rate of about 64%.
Again, dependent onthe amount of reordering the graph word error rategoes down to about 36% for the monotone searchand even down to 23% for the search with a windowof size 5.
Again, the reduction of the graph word er-ror rate compare to the single-best error rate is dra-matic.
For comparison we produced an N -best listof size 10 000.
The N -best list error rate (or oracle-best WER) is still 50.8%.
A word graph with a den-sity of only 8 has about the same GWER.In Figure 3, we show the graph position-independent word error rate for the IWSLT task.
Asthis error criterion ignores the word order it is notaffected by reordering and we show only one curve.We see that already for small word graph densitiesthe GPER drops significantly from about 42% downto less than 14%.10152025303540450  50  100  150  200  250  300  350pos.-indep.graphworderror rate[%]word graph densityFigure 3: IWSLT Chinese?English: Graph position-independent word error rate as a function of theword graph density.In Figure 4, we show the graph BLEU scores forthe IWSLT task.
We observe that, similar to theGPER, the GBLEU score increases significantly al-ready for small word graph densities.
We attributethis to the fact that the BLEU score and especiallythe PER are less affected by errors of the word or-der than the WER.
This also indicates that produc-ing translations with correct word order, i.e.
syntac-tically well-formed sentences, is one of the majorproblems of current statistical machine translationsystems.6 ConclusionWe have described word graphs for statistical ma-chine translation.
The generation of word graphsduring the search process has been described in de-tail.
We have shown detailed statistics of the in-dividual steps of the translation process and havegiven insight in the experimental complexity of eachstep.
We have described an efficient and optimal1973035404550556065700  50  100  150  200  250graphBLEUscore[%]word graph densitywindow size = 1window size = 2window size = 3window size = 4window size = 5Figure 4: IWSLT Chinese?English: Graph BLEUscore as a function of the word graph density.pruning method for word graphs.
Using these tech-nique, we have generated compact word graphs fortwo Chinese?English tasks.
For the IWSLT task, thegraph error rate drops from about 50% for the single-best hypotheses to 17% of the word graph.
Even forthe NIST task, with its very large vocabulary andlong sentences, we were able to reduce the graph er-ror rate significantly from about 64% down to 23%.AcknowledgmentThis work was partly funded by the European Unionunder the integrated project TC-Star (Technologyand Corpora for Speech to Speech Translation, IST-2002-FP6-506738, http://www.tc-star.org).ReferencesY.
Akiba, M. Federico, N. Kando, H. Nakaiwa, M. Paul, andJ.
Tsujii.
2004.
Overview of the IWSLT04 evaluation cam-paign.
In Proc.
of the Int.
Workshop on Spoken LanguageTranslation (IWSLT), pages 1?12, Kyoto, Japan, Septem-ber/October.S.
Bangalore and G. Riccardi.
2001.
A finite-state approach tomachine translation.
In Proc.
Conf.
of the North AmericanAssociation of Computational Linguistics (NAACL), Pitts-burgh, May.S.
Kanthak, D. Vilar, E. Matusov, R. Zens, and H. Ney.
2005.Novel reordering approaches in phrase-based statistical ma-chine translation.
In 43rd Annual Meeting of the Assoc.
forComputational Linguistics: Proc.
Workshop on Building andUsing Parallel Texts: Data-Driven Machine Translation andBeyond, Ann Arbor, MI, June.K.
Knight and Y. Al-Onaizan.
1998.
Translation with finite-state devices.
In D. Farwell, L. Gerber, and E. H. Hovy,editors, AMTA, volume 1529 of Lecture Notes in ComputerScience, pages 421?437.
Springer Verlag.S.
Kumar and W. Byrne.
2003.
A weighted finite state trans-ducer implementation of the alignment template model forstatistical machine translation.
In Proc.
of the Human Lan-guage Technology Conf.
(HLT-NAACL), pages 63?70, Ed-monton, Canada, May/June.L.
Mangu, E. Brill, and A. Stolcke.
2000.
Finding consensusin speech recognition: Word error minimization and otherapplications of confusion networks.
Computer, Speech andLanguage, 14(4):373?400, October.M.
Mohri and M. Riley.
2002.
An efficient algorithm for the n-best-strings problem.
In Proc.
of the 7th Int.
Conf.
on SpokenLanguage Processing (ICSLP?02), pages 1313?1316, Den-ver, CO, September.F.
J. Och and H. Ney.
2002.
Discriminative training and max-imum entropy models for statistical machine translation.
InProc.
of the 40th Annual Meeting of the Association for Com-putational Linguistics (ACL), pages 295?302, Philadelphia,PA, July.F.
J. Och, R. Zens, and H. Ney.
2003.
Efficient search for in-teractive statistical machine translation.
In EACL03: 10thConf.
of the Europ.
Chapter of the Association for Com-putational Linguistics, pages 387?393, Budapest, Hungary,April.F.
J. Och.
2003.
Minimum error rate training in statistical ma-chine translation.
In Proc.
of the 41th Annual Meeting ofthe Association for Computational Linguistics (ACL), pages160?167, Sapporo, Japan, July.N.
Ueffing and H. Ney.
2004.
Bayes decision rule andconfidence measures for statistical machine translation.
InProc.
EsTAL - Espan?a for Natural Language Processing,pages 70?81, Alicante, Spain, October.N.
Ueffing, F. J. Och, and H. Ney.
2002.
Generation of wordgraphs in statistical machine translation.
In Proc.
of theConf.
on Empirical Methods for Natural Language Process-ing (EMNLP), pages 156?163, Philadelphia, PA, July.R.
Zens and H. Ney.
2004.
Improvements in phrase-basedstatistical machine translation.
In Proc.
of the HumanLanguage Technology Conf.
(HLT-NAACL), pages 257?264,Boston, MA, May.R.
Zens, F. J. Och, and H. Ney.
2002.
Phrase-based statisticalmachine translation.
In M. Jarke, J. Koehler, and G. Lake-meyer, editors, 25th German Conf.
on Artificial Intelligence(KI2002), volume 2479 of Lecture Notes in Artificial Intel-ligence (LNAI), pages 18?32, Aachen, Germany, September.Springer Verlag.198
