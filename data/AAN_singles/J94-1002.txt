A Hierarchical Stochastic Model forAutomatic Prediction of ProsodicBoundary LocationM.
Ostendor f*Boston UniversityN.
Veilleux*Boston UniversityProsodic phrase structure provides important information for the understanding and naturalnessof synthetic speech, and a good model of prosodic phrases has applications inboth speech synthesisand speech understanding.
This work describes a statistical model of an embedded hierarchy ofprosodic phrase structure, motivated by results in linguistic theory.
Each level of the hierarchy ismodeled as a sequence ofsubunits at the next level, with the lowest level of the hierarchy repre-senting factors uch as syntactic branching and prosodic onstituent length using a binary treeclassification.
A maximum likelihood solution for parameter stimation is presented, allowingautomatic training of different speaking styles.
For predicting prosodic phrase breaks from text,a dynamic programming algorithm is given for finding the maximum probability prosodic parse.Experimental results on a corpus of radio news demonstrate a high rate of success for predict-ing major and minor phrase boundaries from text without syntactic information (81% correctprediction with 4% false prediction).1.
IntroductionProsodic phrase structure plays a role in both naturalness and intelligibility of speech.For example, prosodic phrase boundaries break the flow of a sentence, dividing it intosmaller units for easier processing.
In addition, researchers have shown that prosodicphrase break placement is important in syntactic disambiguation (Lehiste 1973; Price,Ostendorf, Shattuck-Hufnagel, and Fong 1991).
For these reasons, computational mod-eling of prosodic phrases is important both for text-to-speech synthesis and speechunderstanding applications.
In this work, we present a computational model that rep-resents a hierarchy of prosodic onstituents using a stochastic formalism to capture thenatural variability allowable in prosodic phrasing.
The model is useful for both anal-ysis and synthesis applications; we focus on synthesis here, and present experimentalresults for predicting prosodic phrase structure from text.Prosodic phrase structure, or groupings of words in a sentence, can be equiva-lently represented by different phrase break markers.
The location and relative sizeof these breaks define the prosodic phrase structure, which we will refer to here asa prosodic parse.
Prosodic phrase breaks are discrete events that are associated withacoustic ues such as duration lengthening, pause insertion, and intonation markers.In this work, we are concerned only with the relationship between the abstract events?
ECS Department, 44Cummington Street, Boston, MA 02215(~) 1994 Association for Computational LinguisticsComputational Linguistics Volume 20, Number 1(different levels of phrase breaks) and text.
To be useful in synthesis or understandingapplications, the results presented here need to be integrated with a component thatmodels the acoustics associated with these abstract events (see, for example, Hiroseand Fujisaki 1982).Several observations about prosodic phrase breaks raise issues to be consideredin designing an algorithm to predict such breaks from text.
First, there is a signifi-cant body of literature in linguistics concerning various hierarchies that specify therelationship among prosodic onstituents, and the model should reflect his structure.Second, several different prosodic parses may all be acceptable for one sentence.
Thisvariability is particularly important to represent if the model is to be useful for anal-ysis as well as synthesis.
Third, prosodic phrase breaks do not always coincide withsyntactic phrase boundaries, and the relationship between prosody and syntax is notwell understood.
This means that prosodic phrases cannot simply be predicted fromsyntactic structure.
Finally, since most text-to-speech synthesis applications require alow cost implementation, there is the concern of computational complexity.
We shallexpand on these points separately below, to motivate the work described here.The various linguistic theories of prosodic phrase structure (e.g., Liberman andPrince 1977; Selkirk 1980, 1984; Beckman and Pierrehumbert 1986; Nespor and Vogel1983; Ladd 1986) differ in the specific levels that they represent, but all have a similarhierarchical structure.
Two levels of prosodic phrases are common to most propos-als: the intonational phrase and the intermediate phrase, using the terminology ofBeckman and Pierrehumbert.
A sentence is composed of a sequence of intonationalphrases, which in turn are composed of sequences of intermediate phrases.
An in-tonational phrase break is therefore perceived as stronger or more salient than anintermediate phrase break.
Intonational phrases are delimited by boundary tones, andintermediate phrases are theoretically marked with a phrase accent, where the pitchmarkers can be either high or low (Beckman and Pierrehumbert 1986).
(In other the-ories of intonation, for example, t'Hart, Collier, and Cohen \[1990\], pitch markers alsooccur at phrase boundaries, but are identified with movement and referred to as eitherrising or falling.)
Both types of constituents are also cued by segmental lengthening inthe phrase final syllable (Wightman, Shattuck-Hufnagel, Ostendorf, and Price 1992).Since intonational and intermediate phrases are generally accepted, the experimentshere will only address these two levels, referring to them as major and minor phrases,respectively.
However, other types of prosodic onstituents may be useful and, in fact,there is durational evidence for at least four levels (Wightman, Shattuck-Hufnagel, Os-tendorf, and Price 1991; Ladd and Campbell 1991).
We therefore propose a more gen-eral hierarchical model that can be extended to an arbitrary, but fixed, number of levels.In the examples given here, we will represent intonational phrases (I) using "\] I" tomark a major break and intermediate phrases (i) using "1" to mark a minor break.
Theexample below illustrates how phrase breaks are used to represent prosodic phrasestructure:Those on early release \] must check in with correction officials IIfifty times a week II according to Ash, IIwho says about half I the contacts for a select group IIwill now be made I by the computerized phone calls.
\[I((Those on early release)i (must check in with correction officials)i)i((fifty times a week)i)i ((according to Ash,)i)I((who says about half)i (the contacts for a select group)i)~((will now be made)i (by the computerized phone calls.)i)~28M.
Ostendorf and N. Vei l leux Hierarchical Stochastic Model for Automatic PredictionAnother important consideration i  modeling prosody (and evaluating the model)is that prosodic phrase structure is not deterministic.
Speakers can produce a sentencein several ways without altering the naturalness or the meaning.
Prosodic breaks candiffer in size and/or placement because of differences in style, competence, or simplynatural speaking variations.
For example, the following sentence was said three waysby five speakers:They're in jail I\] for such things \]l as bad checks or stealing.They're in jail \]for such things I as bad checks I or stealing.They're in jail I I for such things as bad checks I or stealing.Although deterministic rules can be used to predict phrase breaks for speech syn-thesis applications, uch a model will be limited in its usefulness in speech analysis.In addition, speech synthesis might be more natural if variability is included in themodel.
Here, a stochastic model is used to represent the natural variability in prosodicstructure by deriving probabilities of phrase breaks, rather than predicting locationsof phrase breaks by rule.The relationship between prosody and syntax is not fully understood, though itis generally accepted that there is such a relationship.
For example, relatively highersyntactic attachment usually corresponds torelatively larger prosodic breaks, but thereare many exceptions, as in:\[\[Mary\]np lwas amazed \[Ann Dewey was angry\]s'\]vp\]swhich was produced by four speakers asMary was amazed I IAnn Dewey was angry.In an analysis of the London-Lund corpus, Altenberg (1987) finds relative frequenciesthat describe the correspondence b tween prosodic constituents (tone units) and dif-ferent syntactic units.
This data supports the use of a probabilistic model, which alsohas an advantage in that it can be trained automatically, facilitating representation fa wide variety of speaking styles and allowing a means of discovering syntax-prosodyrelationships from a large corpus.
One reason that the mapping between syntax andprosody is not simple is because, in speech, the constraints of syntactic structure andphrase length are balanced to produce a regular, roughly equal, sequence of prosodicphrases (Gee and Grosjean 1983).
Consequently, we include constituent length as afactor in the model.The cost of obtaining afull and accurate syntactic parse can be high, which presentsdifficulties for text-to-speech synthesis ystems.
In addition, a full syntactic parse maynot be necessary for predicting prosodic phrases, since prosody is not directly related tosyntax.
Consequently, we investigate computation/performance trade-offs associatedwith using a skeletal syntactic parse vs. simple part-of-speech (POS) assignments.To summarize, the model proposed here addresses everal issues in modelingprosodic phrase structure.
The model is a general formalism for an embedded hier-archy, which we specifically apply to represent sentences, major phrases, and minorphrases.
In order to account for the allowable variability in prosodic parsing, the modelis probabilistic.
The structure of the model allows use of grammatical information suchas part-of-speech labels, syntactic structure and constituent length, but the specific pa-rameters are trained automatically.
Finally, computational complexity trade-offs areinvestigated by evaluating the algorithm with and without syntactic ues.29Computational Linguistics Volume 20, Number 1The remainder of the paper is organized as follows.
We begin, in Section 2, bydiscussing past work in predicting prosodic phrase breaks from text for speech syn-thesis.
In Section 3, we introduce the probabilistic formalism of the hierarchical modeland outline the implementation: text pre-processing, parameter stimation, and phrasebreak prediction using a dynamic programming algorithm to obtain the most likelyprosodic parse.
In Section 4, we present experimental results for prediction of majorand minor prosodic phrase breaks based on a corpus of FM radio news stories.
Finally,we conclude in Section 5 by discussing possible implications and extensions of theseresults.2.
Previous WorkInitial attempts to incorporate prosody in speech synthesis involved determining into-nation and duration patterns as a function of syntactic phrase structure (Allen, Hunni-cutt, Carlson, and Granstrom 1979; Allen, Hunnicutt, and Klatt 1987), which requiressyntactic parsing.
More recently, researchers have attempted to address the fact thatprosody and syntax are not directly related by explicitly predicting prosodic phraseboundaries rather than using syntactic lause boundaries.
An important difference be-tween these subsequent approaches i  in the amount of syntactic information used topredict prosodic boundaries.
The algorithms reflect different assumptions about therelationship between prosody and syntax, as well as different levels of computationalcomplexity.
Clearly, a greater use of syntactic information will require more computa-tion for finding a more detailed syntactic parse.One approach is based on the idea that a prosodic parse may not require a fullsyntactic parse and that detailed part-of-speech information (e.g., noun, verb, deter-miner) may not be necessary for generating a prosodic parse.
Sorin, Larreur, and Llorca(1987) proposed a simple prosodic parser for French based on content/function wordclassification to determine prosodic constituents referred to as prosodic groups.
Thelength and relative location of these prosodic groups is then used to determine phrasebreak locations that are marked with a pause.
Our earlier work drew on this schemefor predicting phrase boundaries in English: a Markov model was developed to pre-dict phrase breaks by representing the sequence of prosodic groups and breaks as aMarkov chain (Veilleux, Ostendorf, Price, and Shattuck-Hufnagel 1990).
An advantageof these approaches is that they only require asmall dictionary of function words to as-sign part-of-speech labels.
Motivated by similar principles and using only a 300-worddictionary, O'Shaughnessy (1989) proposes a somewhat more sophisticated parser forEnglish based on function word identification, umber agreement, and suffix identi-fication.
O'Shaughnessy's work differs from the other approaches in that his goal is asyntactic parse, though not complete, and he does not address the issue of differencesbetween prosody and syntax.At the other end of the spectrum are approaches based on the hypothesis thatprosodic phrase boundaries can be predicted by rule from a full syntactic parse.
Geeand Grosjean (1983) developed a rule-based system, called the Phi Algorithm, to pre-dict psycholinguistic "performance structures" that are represented by assigning aninteger number corresponding to boundary salience between each pair of words.
Con-stituent length information is incorporated primarily through the application of theirverb balancing rule, which splits the verb phrase and groups the verb with either theprevious or subsequent material, subject o syntactic onstraints.
Gee and Grosjeandeveloped their Phi Algorithm only to predict performance structures.
However, theirwork has been extended to prosodic phrase prediction for speech synthesis applica-tions by Bachenko and Fitzpatrick (1990), who explicitly find prosodic phrase breaks30M.
Ostendorf and N. Veil leux Hierarchical Stochastic Model for Automatic Predictionfrom derived boundary salience indices.
They relax many of the constraints on theuse of the verb rule and propose a Verb Adjacency Rule, so their algorithm requiresa fairly detailed parse, although not a complete one.
One of the relaxed constraintsobviates the need for clause information.
Altenberg (1987) has also proposed an algo-rithm for prediction of phrase boundary locations (specifically, tone unit boundariesfor British English) by rule from syntactic structure and semantic information.
How-ever, the detailed information required for the algorithm cannot currently be acquiredautomatically from text.Departing from these approaches, Wang and Hirschberg (1992) have recently usedbinary decision trees to predict he presence or absence of a prosodic break at eachword boundary in a sentence.
They consider a range of input variables, including text-derived information such as detailed POS labels and syntactic onstituent s ructure,and in some experiments, acoustic information.
POS labels were given by Church'stagger (Church 1988) and syntactic onstituents by Hindle's parser (Hindle 1987).
Theacoustic information (previous boundary location, pitch accent location, and phraseduration), which was based on hand-labeled prosodic markers, did not improve per-formance but resulted in a much smaller tree for prediction.All of these approaches have influenced the model proposed here.
For example, weinvestigate simple content/function word POS assignment, as in Sorin, Larreur, andLlorca (1987).
Like Wang and Hirschberg (1992), we use decision trees to automaticallydetermine the important factors influencing phrase break location.
In addition, all ofthe above works have influenced the choice of factors and questions incorporatedin the decision tree.
Two important differences in our approach include a stochasticmodel to capture variability and an explicit representation f a linguistically motivatedhierarchy.
Of course, whether it is effective and/or efficient for a computational modelto reflect a linguistic hierarchy is an empirical question.3.
Hierarchical Model of Prosodic PhrasesA prosodic parse of a sentence can be represented by a sequence of break indices, oneindex following each word, which code the level of bracketing or attachment in a tree.A prosodic parse S is therefore given byS = (bl,b2,...,bL),where bi is the break index after the ith word and L is the number of words in thesentence.
A break is a random variable that can take on one of a finite number ofvalues from "no break" (orthographic word boundary, but not a prosodic onstituentboundary) to "sentence boundary," where the values form an ordered set that corre-spond to the different levels of the hierarchy.
Below we consider a stochastic modelfor first a general hierarchical prosodic parse (any specified number of levels), andthen specifically for the three-level case that models a sentence as a sequence of majorphrases, which are in turn modeled as a sequence of minor phrases.
Although mostphonological theories do not recognize the "sentence" as a unit, it is useful for bothsynthesis and recognition applications to model sentences separately, as sentence-finalboundaries tend to be acoustically different from sentence-internal boundaries (e.g., alow boundary tone is much more likely).
11 We have chosen to use the term "sentence" rather than the more general term "utterance," since thealgorithm isdesigned topredict boundaries from text hat in our data nd in many applications31Computational Linguistics Volume 20, Number 1We will begin by presenting the mathematical structure, first generally and thenspecifically as a three-level embedded hierarchy.
Next, some pragmatic details of textprocessing are discussed, followed by a description of the parameter estimation andphrase break prediction algorithms.3.1 Stochastic ModelWe assume the relationship between units and subunits will hold at any level of thehierarchy.
Therefore, in describing the general case, we need only consider one level ofembedding and will use Ui and Uq when referring to units and subunits, respectively,at some unspecified level of the hierarchy.
Using this notation, the probability of a unitUi is parameterized in terms of the probability of the sequence of subunits uq (on thenext lower level) and the length ni in subunits of that sequence given the orthographictranscription of the sentence W:p(Ui\]W) = p(uit~...,Uini\]'lA2)= p(ua,...,Uini\[W, ni)p(ni\]?V)ni= p(nilW)p(uit \]W) H p(uij\[W, Uil,.. .
,  ui(j_l)).j=2The specific hierarchy considered here involves representing the prosodic parse ofa sentence S as an N-length sequence of major phrases Mi:S = (Mt,.. .
,MN).A major phrase Mi is composed of a ni-length sequence of minor phrases mij:Mi = (mil, .
?
?
, min i ) .Finally, a minor phrase mq is composed of a v/j-length sequence of breaks bt startingat time t(i,j) and ending at time t(i,j + 1) - 1,mq = (bt ( i , j ) , .
.
.
~ bt(i,j+l)-l),where t(i,j) is the time index of the first word of mij and t(i,j + 1) -1  is the time indexof the final word of mq, vii = t(i,j + 1) - t(i,j) is simply the number of words in theminor phrase, and the breaks bt take on values from the set {no break, minor break,major break, sentence break}.It might be useful to consider phonological words rather than orthographic wordsas possible sites for break indices.
This could be accomplished, without using de-terministic rules, by specifying the bottom of the hierarchy (e.g., break level 0) torepresent locations internal to a phonological word and the next level of the hierar-chy (e.g., break level 1) to represent phonological word boundaries.
However, it iscontroversial s to whether phonological words can be larger or smaller than ortho-graphic (lexical) words (Booij 1983; Nespor and Vogel 1983), so it is not clear how thelowest level should be defined relative to the orthographic words.
In this work, wehave chosen not to distinguish between these two levels, to reduce the complexityof implementation and performance evaluation.
For similar reasons, we have limitedcomprises yntactically well-formed sentences.
The phrase prediction model may also be useful inspeech recognition applications, in which case the term "utterance" would clearly be more appropriate.32M.
Ostendorf and N. Vei l leux Hierarchical Stochastic Model for Automatic PredictionTable 1Histograms of number of minor phrases in a major phrase and number of major phrases in asentence, as a function of quantized length of the unit.
The quantizer regions are indicated bythe length ranges.Number of minor phrases in major phrase Number of major phrases in sentencee(M) 1 2 3 4 5 ~(S) 1 2 3 4 5 61-5 522 69 2 1-14 31 39 11 2 16-7 106 78 9 15-21 4 23 28 17 9 58-14 61 107 43 1 22-31 6 6 20 24 1115-19 2 2 1 1 32-37 2 6 4this study to the more universally agreed-upon levels of major and minor prosodicphrases, although there is durational evidence that a more detailed hierarchy wouldbe useful (Ladd and Campbell 1991; Wightman, Shattuck-Hufnagel, Ostendorf, andPrice 1992).In the current work, we make several simplifying assumptions due to traininglimitations.
First, the probability of the number of subunits in a unit p(nilUi) is assumedto depend only on the number of words in the unit f(Ui).
As is not surprising, our dataindicate that units that span a larger number of words tend to comprise more subunits.Altenberg has noticed similar tendencies in the London-Lund corpus (Altenberg 1987,p.
81).
(Alternatively, it has been suggested that either phonological word count orstressed syllable count rather than orthographic word count may be a useful measureof phrase length on the lowest level \[Bachenko and Fitzpatrick 1990\].)
In addition,the probability distribution is approximated by conditioning on quantized lengthsQu(~(Ui)).
The quantizer varies as a function of the specific unit and is designedusing a regression tree (Breiman, Friedman, Olshen, and Stone 1984).
A regressiontree partitions the data along intervals of a continuous variable, in this case length ofthe unit, to decrease variance of the response variable, the number of subunits in theunit.
The resulting quantizer regions and the corresponding distribution of subunitsin a unit are given in Table 1 for major phrase and sentence units.Using these simplifying assumptions, the constituent length probability distribu-tions are then:p(N\]W) = qs(NlL) (1)p(niJW) -- qa(nilli) (2)p(~ij\]W) = qm(~jlAij) (3)where L = Qs(?
(S)), 1i = QM(f(Mi)) and Aij = Qm(f(mij)).Next, major phrases in a sequence are assumed to be Markov given the numberin the sequence:p(Mi\]W, M1,...~Mi_i) = p(MilW, Mi_l).Minor phrases are also assumed to be Markov, depending only on the previous minorphrase and the features of the major phrase it is contained in:p(mij\]Wi, rail,..., miO._l ),Mi-1) = p(rnijIWi, mi(j-1))p(mil\]Wi, Mi-1) -- p(mil\]Wi~m(i_l)ni_l)~where Wi is the sequence of feature vectors panning the ith major phrase.
For sim-33Computational Linguistics Volume 20, Number 1plicity, we will abbreviate the notation to:p(mij JVVi~ mil~ .
.
.
~ mi ( j -1 )~Mi -1 )  = p(mijlVVi~ mprev).The conditional probability of a sequence of words within a minor phrase is assumedto depend on a state determined by the (variable-length) sequence of past words andthe time of the last break, where the state is given by a decision tree as in Bahl, Brown,deSouza, and Mercer (1989):p(bk lWi~ bt(i,j)~ .
.
.
~ bk - l  ~ mprev ) = p(bk l f  (Wi~ mprev ) ).Note that within a minor phrase, probabilities for only two cases, that of no break orthat of any higher level break index, are used.Incorporating all of the above simplifying assumptions, the probability of a specificprosodic parse is given byNp(SIW ) = qs(NIn )Hp(MilW~Mi_i), (4)i=1nip(Mi\]W, Mi-1) = qM(nilli) I I  P(mijlWi~mprev)~ (5)j= lt ( i j )+vi j -1p(mijlWi,mprev) = qm(vijl&ij) I I  p(bklf(Wi~mp rev))" (6)k=t(i,j)W used in this model is not simply the orthographic word sequence.
Rather, it is asequence of feature vectors, one per word extracted from the word sequence.
Examplesof possible features include part-of-speech labels and syntactic information such asbracketing labels or labels of an associated node in a syntactic tree.
The decision treef(Wi~ mprev) used in determining the probabilities p(bklf(Wi, mpre~;)) includes questionsbased on these features, attributes of the previous minor phrase and the current majorphrase, and length in words of the sentence.
Details on our specific choice of featuresand questions is given in Section 4.Our use of decision trees is different from the phrase break detection algorithmof Wang and Hirschberg (1992), although the tree design algorithm and choice offeatures is similar.
The tree is not used to classify phrase breaks directly; instead it isused to determine the probability of the occurrence of a minor break at some location,conditioned on the decision tree structure.
This probability is used to represent thelowest level in the hierarchical model.Previously, we mentioned two important factors affecting the placement of phrasebreaks: (1) grammatical structure and (2) length constraints on the prosodic con-stituents uch as overall length and length relative to neighboring phrases.
Gram-matical information is incorporated in the tree f(Wi~mprev) through questions aboutthe feature sequence Wi.
Prosodic constituent length is modeled in two ways, throughthe constituent length probability distributions and through questions about he lengthof the previous phrase used in the tree f(Wi~ mpr~v).3.2 Text ProcessingIn the experiments reported here, the feature vectors include part-of-speech labels,punctuation and, optionally, information from a skeletal syntactic parse.
The featureextraction is described in more detail below, and an example is given in Figure 1.Two levels of detail are considered for part-of-speech (POS) labeling.
At the sim-plest level, a function word table look-up is used to categorize words either as one of34M.
Ostendorf and N. Veilleux Hierarchical Stochastic Model for Automatic Predictionword # in sent.word classpart-of-speechpunctuationleft dominatedright dominatedboth dominated# init.
constit.# term.
constit.\[VP is \[AP free \[PP on \[NP bail\]\] \[PP after \[NP \[VP facing ...5 6 7 8 9v c p c pv adj p noun pnone none none none nonesame same same PP sameAP PP NP PP NPVP AP PP AP PP1 1 1 1 20 0 0 2 010CverbnoneFigure 1Seven features are extracted for each word in a sentence to describe the boundary betweenthat word and the following word.
Syntactic information is based on a skeletal parse, asshown.
Part=of-speech assignment is based on a table look-up from lists of function words.six types of function words, as a proper name (P) if capitalized, or otherwise as a con-tent word (c).
Function words are divided into several classes: conjunctions (j) (such asand, but, if, because), auxiliary verbs and modals (v), determiners (d), prepositions (p),pronouns (n), and a general category (g), which includes the quantifiers and function-like adverbs uch as not, no, ever, now.
The POS labels given by the simple table look-upare referred to here as "word classes."
A more detailed part-of-speech classification isgiven by Penn Treebank POS tags (Marcus and Santorini 1993), which were obtainedautomatically using the BBN tagger (Meteer, Schwartz, and Weischedel 1991).
Whatwe refer to here as POS labels is actually a grouping of these classes that includesthe above function word categories, the proper name category (now determined bythe tagger rather than from capitalization), plus categories for particles (pa), nouns(noun), verbs (verb), adjectives (adj), adverbs (adv), and all other content words (def).Contractions are not decomposed into separate words, since it is not possible thata phrase break will occur within the word contraction.
The contraction is treated as asingle word in constituent length measures and feature extraction, and it is assignedthe POS label of its base word (left component).Punctuation following a word is incorporated as a feature for that word.
In ourdata, the only punctuation that appears are commas and periods.
Periods and othersentence-final punctuation deterministically assign a sentence break.
This implies sometext preprocessing that distinguishes periods used for abbreviation from sentence-finalperiods.
While commas often correspond to major breaks, there is a systematic excep-tion: a series of the same syntactic units such as a series of nouns (an apple, an orange,and a pear) or a series of adjectives (safe, cost-effective alternative ...) may or may not beassociated with a major prosodic break.
Therefore, we have chosen to use commas as afeature to determine the likelihood of a phrase break rather than as a deterministic cueto a prosodic break.
Although using commas deterministically to assign a major phraseboundary yields better performance on our test set than using commas as a feature,we felt that using commas as a feature was a more extensible approach, and have usedthis strategy in the results reported here.
Including commas as a feature does improveperformance relative to not using commas, as will be discussed in Section 4.
Commas(and other punctuation) can be very useful for prosodic boundary prediction whenthey are available, and they are used in other algorithms (e.g., Allen, Hunnicutt,  andKlatt 1987; O'Shaughnessy 1989; Bachenko and Fitzpatrick 1990).
However, commasare not reliably transcribed from spoken language and not consistently used in writtentext, so it is important hat the algorithm not depend too heavily on commas.35Computational Linguistics Volume 20, Number 1Syntactic features were extracted from skeletal parses provided through a prelim-inary version of the Penn Treebank Corpus.
Since these are hand-corrected parses, theresults are indicative of the performance possible using syntactic information, but donot reflect performance achievable with an existing parser.
Several researchers haveinvestigated the relationship between prosody and syntax (e.g., Selkirk 1984; Gee andGrosjean 1983; Cooper and Paccia-Cooper 1980; Altenberg 1987).
Our features havebeen motivated by some of these results, which suggest hat some syntactic con-stituents are more likely to be separated by a phrase break than others.
However,we have chosen to let the important constituents be determined automatically, similarto Wang and Hirschberg (1992), rather than by rule.
One feature is the highest syn-tactic constituent dominating the left word but not dominating the right word, whichdescribes potential locations for phrase breaks after a specific syntactic onstituent.
Wealso consider the similar case, the highest syntactic onstituent dominating the rightword but not the left, to allow for prosodic phrase breaks that may be associated withthe beginning of a syntactic onstituent.
The lowest syntactic onstituent that domi-nates both words is a feature that will provide information about which constituentsare not likely to be divided by a phrase break.
In addition, the number of terminat-ing constituents and the number of initiating constituents between the two wordswere included as features to investigate the influence of relative strength of syntac-tic attachment.
Eight categories of syntactic onstituent were used: sentence (S), nounphrase (NP), verb phrase (VP), prepositional phrase (PP), wh-noun phrase (WHNP),adjective or adverbial phrase (AP), any other constituent (O), and both words in thesame lowest level constituent (same).3.3 Parameter EstimationAn advantage of a stochastic model is that the parameters can be estimated automat-ically from a large corpus of data, which means that it is relatively straightforward toredesign the model to reflect a different speaking style.
Here we describe a maximumlikelihood approach to parameter estimation, where model parameters are chosen tomaximize the likelihood of the training data.We will assume that sentences are independent and identically distributed to sim-plify parameter estimation and prediction, although the independence assumptionprecludes capturing any speaker-dependent or discourse ffects.
In this case, the like-lihood of the prosodic parse of a corpus of sentences ($1,..., S T) given parameters 0is, from Equations (4)-(6),?
(o) = ~logp(StlW t)t= ~t \[logqs(NtlLt)+~i logp(MllWt,Ml_l) \]= t~\[logqs(NtlLt) q-\[~\[logqM(n~ll~)q-l~logp(mlj\[wit,m~rev)\]\] \]= logqs(Nt\[L t) + \[lOgqM(ni\]li)+\[ j~\[l?gqm(v~\[~lj)+~l?gp(b~lfO4"~,m~rev))l\]\]\]"36M.
Ostendorf and N. Vei l leux Hierarchical Stochastic Model for Automatic PredictionArranging terms we have?
(0) = ~logqs(NtlL t)t+ ~lOgqM(n~\] l f )t i+ ~~l?gqm(41Af j )t i j+ ~ ~ ~l?gp(b~If(W~'m~rev))"t i j k(7)Since there are no cross dependencies between parameters, the four terms in Equa-tion (7) can be maximized separately.
The resulting parameter estimates for qs, qM, andqm are then simply relative frequency estimates.
The last term is maximized jointly withthe design of the state function f (-) using standard classification tree design techniques,as described below.The tree is grown using a greedy algorithm, which iteratively extends branchesby choosing the parameters of a question, the question at a node and the node ina tree that together maximize some criterion for reducing the impurity of the classdistributions at the leaves of the tree.
The tree is used to find the probability of aminor phrase boundary, so there are only two classes: "break" and "no break."
Inthis work, we have used the Gini criterion, i.e., the node distribution impurity isgiven by i(t) = ~i#jp(ilt)p(jlt) (Breiman, Friedman, Olshen, and Stone 1984).
Sincethe relative frequency of the "break" class is so low (8% of all breaks), we includedifferent error costs in the design criterion.
Generally, the cost of classifying a "break"as "no break" is chosen to be three to four times higher than the opposite error,and the specific costs for each tree are chosen to control the false prediction rateon the training set.
Initially a tree is grown using two-thirds of the training data,and the remaining one-third of the data is used to determine a good complexity-performance trade-off point.
The complexity criterion determined at this point is thenused in pruning a second tree grown with the entire training set, in order to makebetter use of the available data.
Each leaf t of the tree is associated with a conditionalprobability distribution of "break" vs. "no break" (actually, the relative frequency es-timate).
This probability distribution p(blt ) is used in the hierarchical model for com-puting the probability of a minor phrase, Equation (6), by running test data throughthe tree and using the probability distribution associated with the final leaf node= f(Wi, mprev).3.4 Phrase Break Prediction AlgorithmThe stochastic model can be used to predict a prosodic parse for a sentence simplyby finding the most probable prosodic parse for that sequence of words, where theprobability of any given parse is determined by Equations (4)-(6).
In other words,we hypothesize all possible prosodic parses, compute the probability of each, andchoose the most probable.
The most likely prosodic parse can be found efficientlyusing a dynamic programming (DP) algorithm that is similar to algorithms used inspeech recognition, in particular that for the Stochastic Segment Model (Ostendorfand Roukos 1989), except hat the dynamic programming routine is called recursivelyfor successive levels in the hierarchy.
Defining pt(uil...Uinl~/Vi, U ) as the probabilityof the most likely sequence of n subunits in but not necessarily spanning Ui andending at location t, and Ui j (S  ~ t) as a subunit that spans boundaries {bs,...,bt}, the37Computational Linguistics Volume 20, Number 1dynamic programming algorithm can be expressed generally in the subroutine thatfollows.
This subroutine is called recursively for each level of the hierarchy, with thelowest level constituent probability being computed using probabilities given by thetree.Dynamic Programming Routine for Prosodic Parse PredictionFor each word t in unit Ui (t = 1~..., li):Compute log pt ( Uil (1, t ) \[Wi, Ui- l ).For each n-length sequence of subunits panning \[1, t\] (n = 2,...~ t):log Pt (Uil'"Uin \[\]/Vi~ Ui-1 ) = maxs<t log ps (Uil, .
.
.
, Ui,(n-1)\]~/Vi~ Ui-1 )+ logp(uin(S + 1, t)\[Wi, Ui(n-1))(Computing logp(uin(s + 1~ t)\]Wi, Ui(n-1)) with a recursive call to this routine.
)Save pointers to best previous break location s.To find the most likely sequence,p(Ui\[~/Vi, U i -1)  -~ maxn logPl,(Uil~ .
.
.
,Uin\[~/Vi, U i -1)  q- logq(n\[li)The final step is to decode the sequence of breaks once the value n* that maximizesthe above equation is determined.
Using the n* associated with any level unit, we cantrace back to find the optimal segmentation f subunits that comprise that unit.
Thecomplete parse is found by tracing back at the highest level units and successivelytracing back in each lower level.For the specific case of a three-level hierarchy, the most likely major phrase se-quence in a sentence p(S\[W) and the most likely minor phrase sequence in a majorphrase p(Mil W)  are found by a dynamic programming algorithm, called recursively.The lowest level unit considered here is the minor phrase, and the probability of theminor phrase is computed as given in Equation (6) using the decision tree.4.
Experiments4.1 CorpusFor our investigation of prosodic phrase structure, an FM radio news story corpuswas used.
The training data included ten stories from one announcer and another tenstories from a second announcer, both female, for a total of 312 sentences (6,157 words,or potential boundary locations).
The stories were studio recordings of actual radiobroadcasts, which were transcribed by a listener who did not have access to the originalscripts.
It is likely that transcription of punctuation did not exactly match the originalwritten text and may have been biased by the prosody of the utterance.
However,the radio announcers tended to annotate the transcribed text before reading the teststories, so we conjecture that commas were more often omitted than inserted in ourtranscriptions.
All of the training stories were used to estimate the probabilities of the38M.
Ostendorf and N. Veil leux Hierarchical Stochastic Model for Automatic Predictionnumber of subconstituents (Equations 1-3).
In the first pass of tree design, two-thirdsof the training data was used to grow the tree and one-third was used to determinethe performance complexity trade-off, but the final tree used was redesigned on theentire training set.For testing, we used five versions of a different story spoken by two female andtwo male announcers (one radio broadcast version and four radio-news-style labrecordings).
One of the female announcers (two spoken versions) was the same asthe speaker who provided roughly three-quarters of the training data.
Multiple testversions are used in order to allow for some acceptable differences in phrasing inthe context of the FM radio news style, and to investigate the possibility of speaker-dependent effects.
On average, there were 3.3 different prosodic parses among the fiveversions.
The test story contained 23 sentences (385 words) ranging in length from 3to 36 words.
For reference, the test sentences are included in an appendix with thephrase predictions of our best system.Prosodic phrase breaks were hand-labeled in the entire corpus; the training setlabels were used for estimating the parameters of the model and test set labels wereused for evaluating the performance ofthe model.
The prosodic phrase labeling systemused break indices marked between each pair of words, based on auditory perceptualjudgments (that is, the labelers did not have access to spectrogram or pitch displays).The break indices ranged on a scale of 0 to 6, chosen to map to a superset of theprosodic hierarchies proposed in the literature.
The labeling scheme is described inmore detail in Price, Ostendorf, Shattuck-Hufnagel, and Fong (1991).
Six of the storieswere labeled by polling two listeners who discussed any discrepancies.
The remainingstories were labeled by a third listener working independently.
Comparing the labelsof one story using both schemes howed that there was a high degree of consistencyacross labelers.
For the full seven-level labeling system, the correlation between thetwo sets of labels was 0.93, where correlation is computed as the maximum likelihoodestimate of the correlation coefficient based on the two sets of labels.
Only 1% of thelabels differed by 2, and these were at locations where the disagreement was actuallyover the location of the boundary rather than the relative strength of the boundary.In this work we considered only a three-level hierarchy and therefore mapped breaks0-2 to "no break," 3 to a "minor break" (I), 4 and 5 to a "major break" (ll) and 6 to a"sentence break.
"4.2 Evaluat ion MethodsThe goal of this algorithm is to predict placement of phrase breaks that sound naturalto listeners and that communicate he intended meaning of the sentence.
As men-tioned above, many renditions of a sentence can fulfill this criterion.
Therefore, wehave attempted toestimate system performance by comparing the predicted breaks toparses observed in five spoken versions of the sentence.
Although the ultimate test ofthe algorithm is in a speech synthesis ystem, a quantitative measure of system per-formance is useful in algorithm development and comparison.
We have consideredfour performance measures in this work.Since one incorrectly assigned break could make a whole sentence or clause un-acceptable, one measure of system performance is the number of sentences with apredicted parse that matches entirely a parse observed in any of the five spoken ver-sions.
When such a match occurs, we call the predicted parse "correct."
The five spokenversions do not represent an exhaustive set of acceptable parses, however.
Thereforein a separate valuation, the sentence is also judged subjectively to determine whetherit is an "acceptable" parse.
The number of sentences that fall into these two categories39Computational Linguistics Volume 20, Number 1are reported separately, and for the best case system are marked separately in theresults in the appendix.In order to better understand the system performance, we have chosen to com-pute additional error measures based on the prediction accuracy at individual breaklocations.
A predicted sentence is compared to each of the five spoken versions, andthe closest spoken version is used as the reference for that sentence.
(The closenessof parses is measured using a Euclidean distance with 0 for no break, 1 for minorbreak and 2 for major break.)
Then the correspondence b tween predicted and ob-served breaks is tabulated in a confusion matrix.
Sentence breaks are deterministicallyassigned at periods, but these are included in the performance r sults reported here(as major breaks) to be consistent with results reported elsewhere.
Also, note that con-fusion tables for different systems ometimes reflect different numbers of observedminor and major breaks because the predicted sentences may best match differentversions of the test sentence.It is also useful to have a simple measure for comparing systems.
One possibleperformance figure is the overall percent correct, but we have found this measure tobe difficult to interpret because the overall figure is dominated by the performanceon the much more frequent "no break" locations.
Instead, we compute the correctprediction and false prediction rate for breaks as a combined class (merging minorand major breaks).
Using terminology from detection theory, these are also referred toas correct detection (CD) and false detection (FD) in the following sections.
CD/FDresults must be interpreted with some caution, because there is a trade-off between thetwo error rates: higher break detection rates are associated with a higher rate of falsebreak insertion.
If the insertion rate is too high, there will be few good parses at thesentence l vel.
We have therefore tried to control the insertion rate as much as possiblefor the different systems evaluated.
Two types of CD/FD results are reported.
Onefigure is computed based on comparison to the nearest sentence of the five versions.In addition, since other research results have been reported based on comparison toonly one spoken version, we include correct prediction and false prediction rates thatcorrespond to the average rates over the five separate test versions.
In general, thecorrect prediction rates using the single version comparison are roughly 10% lowerthan using the comparison to five versions, so comparison to one version significantlyunderestimates performance of the algorithms.
The variation in error rate over thefive versions is relatively small, as shown later in the discussion of speaker-dependenteffects.4.3 Tree Questions and DesignsSeveral experiments using different sets of questions to train the embedded ecisiontrees were performed in order to compare the relative merits of different information ithe hierarchical model, as well as trade-offs associated with computational complexity.The entire set of questions is listed below.
All experiments included questions 1-8,which were based on features that were relatively straightforward to extract from text,using a table look-up to assign part-of-speech labels.
Experiments that made use ofsyntactic features also allowed questions 9-13.
The syntax experiments were basedon trees that were trained using only 14 of the 20 stories, since skeletal parses wereonly available for these stories.
Another set of experiments included question 14, whichtested the ratio of the current minor phrase length to the previous minor phrase length.Finally, experiments hat made use of the more detailed POS classifications includedquestion 15, and used the additional particle category in question 3.
All questionswere based on features derived from text information only.40M.
Ostendorf and N. Veil leux Hierarchical Stochastic Model for Automatic PredictionBelow we enumerate he questions used in the different ree design experiments,together with the motivation for each question.....5.....10.11.12.Is this a sentence or major phrase boundary?
Assuming major breaks occur atqualitatively different locations than minor breaks, we effectively removethe major breaks and sentences from our training corpus with thisquestion.Is the left word a content word and the right word a function word?
In thetraining data, 65% of the minor and major breaks combined occur atcontent word/function word (CW/FW) boundaries, and about half of theCW/FW boundaries are marked with breaks.
The CW/FW boundariesalso correspond to the prosodic group boundaries used deterministicallyin Sorin, Larreur, and Llorca (1987) and in Veilleux et al (1990).What is the function word type of the word to the right?
Previous work inprosodic parsing with a small dictionary (Sorin, Larreur, and Llorca1987) suggested that different types of function words may be more orless likely to signal a prosodic phrase break.Is either adjacent word a proper name (capitalized)?
Preliminary examinationof our data suggested there was some relationship between propernouns and phrase boundaries, probably related to the phrasing ofcomplex nominals.How many content words have occurred since the previous function word?Speakers eemed to insert phrase breaks when a string of content wordsbecame long, e.g., exceeded four or five words.Is there a comma t this location?
Usually, but not always, a major phrasebreak occurs at locations orthographically transcribed with commas.What is the relative location in the sentence (in eighths)?
Previous work (Geeand Grosjean 1983) has suggested that prosodic phrase boundaries tendto bisect a longer unit.
Therefore, one of the questions used to partitionthe training data is the ratio of the word number over the sentencelength, quantized to the nearest eighth.What is the relative location in the proposed major phrase (in eighths)?
Thisquestion is included following the same reasoning as the previousquestion.What is the largest syntactic unit that dominates the word preceding the potentialboundary location and not dominating the succeeding word?
Phrase breaks areknown to co-occur with certain syntactic onfigurations.
For example,phrase breaks often occur before subordinate clauses.What is the largest syntactic unit that dominates the word succeeding thepotential boundary location and not dominating the preceding word?
Therationale behind this question is similar to that of the previous question.What is the smallest syntactic unit that dominates both?
Some syntactic unitsmay be less likely to be broken up by a phrase break.How many syntactic units end between the two words?
This question providesinformation on the relative level of syntactic attachment between the twowords, capturing the effect of constituent endings.41Computational Linguistics Volume 20, Number 113.14.15.How many syntactic units begin between the two words?
This question issimilar to the previous one, except hat it captures effects associated withthe start of new constituents.How large is the ratio of the current minor phrase length over the previous minorphrase length?
This question incorporates the concept of balancing minorphrase lengths noted by other researchers (Gee and Grosjean 1983;Bachenko and Fitzpatrick 1990), and was found to be useful in phraseprediction trees investigated by Wang and Hirschberg (1992).
In thebeginning of a sentence where there is no previous minor phrase, theratio is treated as missing data and handled using a surrogate variable(Breiman, Friedman, Olshen, and Stone 1984).What is the label of the content word to the right?
to the left?
Wang andHirschberg found that part-of-speech information is useful in phrasebreak prediction (Wang and Hirschberg 1992).Questions 5, 12, 13, and 14 are based on numerical features, so the binary ques-tion asks whether the feature is greater than some threshold, where the threshold isdetermined automatically in tree design.
All other questions are based on categoricalvariables, and the best binary groupings of the possible values are determined auto-matically (Breiman, Friedman, Olshen, and Stone 1984).
Two of the questions (8, 14)require knowledge of major or minor phrase boundaries.
This information is availablein the training data or from a spoken utterance, but hypothesized locations of minorand major phrase breaks must be used in phrase prediction from text.
Therefore, thesefeatures are calculated ynamically in the prediction algorithm for each hypothesizedprosodic parse.The first tree designed used only the very simple information represented byquestions 1-8.
The resulting tree is shown in Figure 2, with the relative frequency of abreak in the training data included at each node.
The first split trivially locates the sen-tence and major break boundaries.
The second split utilized the content word/functionword boundary question that we had used deterministically in previous work (Veilleuxet al 1990).
The content/function word boundaries eem to be important in other al-gorithms as well: they correspond closely to the phi-phrase boundaries that would bepredicted by the Bachenko-Fitzpatrick algorithm, and they seem to be captured in theWang-Hirschberg text-only tree by a succession of questions about the part-of-speechlabels of the words adjacent o the break.
Of the boundaries that were preceded bya content word and followed by a function word, 30% were hand-labeled as minorbreaks, whereas only 4% of other locations were labeled as minor breaks and thesewere identified by the next question as coinciding with a comma.
The complete treewas relatively small (9 nodes), and used almost all questions provided.
On the trainingdata, the resulting tree classified 89% of the nonbreaks correctly and 59% of the minorbreaks correctly.
All sentence and major breaks were given in the tree design.The next stage was to incorporate syntactic information (questions 9-13) into thetree design algorithm to determine minor phrase probabilities.
Syntactic parses wereavailable for only 14 of the 20 training stories (217 sentences, 4 230 words), and the treewas designed using this subset.
A very simple five-node tree was designed, as shownin Figure 3.
Again the first nontrivial question was concerning the content/functionword boundaries, and the presence of a comma was again used to predict minorbreaks at other locations.
The two other questions in the tree were based on whichsyntactic unit dominated one or the other words at the boundary site.
The tree designalgorithm chose syntactic units that were less likely to contain a boundary as: words42M.
Ostendorf and N. Veil leux Hierarchical Stochastic Model for Automatic Predictionsentence / major wordboundary ?riOcontent / function word~boundary ?comma ?yesotherplace inmajor phrase ?other first or last,Quarter ?-la^e in ~ function word180/571 1-1 ~ 9 ~71/323 J t,,n e 9 sentence .. ~ ~v ?
'\eighth __Z_,~-----~-,xfunction word~- - - \ ]  ~-  ~ capital ?other X _\]g yes / \ noFigure 2Tree designed using only simple part-of-speech information, questions 1-8.
Relative frequencyof a "break" (in the training data) is indicated in each node for the subset of data associatedwith that node, and the left branch in a split is more likely to have a break.in the same constituent, words separated by a wh-noun phrase boundary, and wordsseparated by a verb phrase initial boundary.
(In their work on spontaneous speech,Wang and Hirschberg found that noun phrases in general tended to be less likelyto contain boundaries.)
The tree with syntactic information seemed to classify minorbreaks with slightly higher accuracy than the previous tree: 90% correct classificationof nonbreaks and 62% correct classification of minor breaks in the training data.A third tree, illustrated in Figure 4, was grown using the first 8 baseline questionsand question 14, which examines the ratio of current minor phrase length to previousminor phrase length.
The motivation behind this question is constituent balancing, asmentioned earlier.
The main difference between this tree and the first one is that theminor phrase length ratio test is chosen instead of the question about he position in amajor phrase.
These two questions erved similar roles, as evidenced by the fact thatthe surrogate variable for the ratio test was the location of the current word within43Computational Linguistics Volume 20, Number 1sentence / major phrasedary ?\] t '~ '~,~ content/function word752/752 \] ~ b ~ m d a r y  ?.
.
.
.
lef t  word  comma ?k,~_/?
~,Y dominated by ...- \ .right word I 1091212 ~ .
~ ~oghmti~a~er~ by ...
I 8/8 I I 10412760 I\s o,I J l  PFigure 3Tree designed using syntactic information but not the minor phrase length ratio test, questions1-13.
Relative frequency of a "break" (in the training data) is indicated in each node for thesubset of data associated with that node, and the left branch in a split is more likely to have abreak.the major phrase, in terms of the ratio of the number of words up to the currentposition over the total length of the major phrase.
Classification rates on the trainingdata for this tree were 87% for nonbreaks and 66% for minor breaks.
A fourth treewas designed using the first fourteen questions, and performance was similar to thatfor the third tree.The decision tree design algorithm's performance was not significantly changedby the introduction of additional features.
New features can supplant previously usedones, as also found by Wang and Hirschberg (1992), because of the redundancy ininformation between features.
For example, in the syntax trees, many of the baselinequestions were no longer chosen, but the overall classification performance was similar.4.4 Phrase Prediction ResultsThe trees were used in the hierarchical model, and the phrase break prediction algo-rithm was evaluated on the independent test set described in Section 4.1.
A summaryof the results is given in Table 2, and the corresponding confusion matrices are inTables 3 and 4.
The baseline system (questions 1-8) gave the best performance, witha correct prediction rate of 81% and a false prediction rate of 4%.
The results indicatethat syntactic information did not improve the performance of the algorithm, and infact gave poorer phrase predictions by every measure of performance on the test data.The difference in performance cannot be attributed to the smaller amount of trainingdata used in the experiments with syntax, because designing the model without syn-tax on this subset actually ielded slightly better performance on the test set than that44M.
Ostendorf and N. Veilleux Hierarchical Stochastic Model for Automatic Predictionentence/major ph ase boundary ?- - - .%__ .\[ 992/992 \[ (430/4599) content/function word I "'J ~ a r y ?~ comma ?
~comma ?111118/>!
~?~?erat io?
\[ 1 ~  \[ 13~/~664 \]45~ NN~4.5 %234/726 ~ function wordI I ~ e  ?~r lghrase  ~~i  ?rp~ <l~rat i ??
-9% I 2/62 I~aC:e inn  c>19% e? '
~0~S8% \[other / \ first, fifth or sixth/ \] N~ighth I20/49 I I 5/39 \[Figure 4Tree designed using the minor phrase length ratio test but no syntax questions, questions l-8and 14.
Relative frequency ofa "break" (in the training data) is indicated ineach node for thesubset of data associated with that node, and the left branch in a split is more likely to have abreak.designed on the full training set.
We conjecture that the poorer performance associ-ated with using syntax in our model may be due to the fact that syntax plays moreof a role in location of major breaks as opposed to the minor breaks predicted in thetree.
As we shall see later, syntactic ues were useful in our implementations of theBachenko-Fitzpatrick and Wang-Hirschberg al orithms.
We also found that the minorphrase length ratio test hurt performance, which is likely due to the fact that the ra-tios are based on hypothesized boundary locations in phrase prediction, as opposedto the known locations used in training.
In examining the confusion matrices, we seethe main effect of the additional syntactic and minor phrase length ratio questions ismore errors at minor phrase boundary locations.Examining the sentence l vel performance ofthe algorithms, we find that a phrasebreak was inserted between the verb and the particle in three of the six unacceptable45Computational Linguistics Volume 20, Number 1Table 2Performance of different break prediction algorithms, including variations of our hierarchicalmodel, variations of a tree-based classifier, and the Bachenko-Fitzpatrick (B-F) algorithmbased on a test set of 23 sentences (386 words).
"Questions Used" refers to those questionslisted in Section 4.3 for the two tree-based algorithms.
Although the B-F algorithm does notuse these specific questions, it does utilize syntactic information as well as relative constituentlength.
Correct Detection/False Detection (CD/FD) rates are for the merged category of minorand major breaks computing (a) error according to the closest utterance of five versions, and(b) the average rror in comparing to a single utterance.Phrase model Questions used Sentences CD/FD of merged breaksSyntax Minor ratio Correct Accept.
5 versions 1 versionHierarchical modelClassification treesB-FNo No 7 11 .81 /.04 .70/.05Yes No 4 6 .77/.04 .68/.06No Yes 5 10 .79/.03 .67/.05Yes Yes 3 11 .77/.04 .70/.05No No 7 6 .68/.03 .62/.03Yes No 7 5 .72/.01 .61 /.01Yes Yes 6 10 .88/,07 .84/.09Table 3Confusion matrices for predicted breaks using the hierarchical system with and withoutsyntax, and without the minor phrase length ratio test.No Syntax SyntaxActual ActualPredicted major minor no-break Predicted major minor no-breakmajor 49 6 5 major 46 14 11minor 5 12 6 minor 7 6 1no-break 7 10 286 no-break 12 10 279Table 4Confusion matrices for predicted breaks using the hierarchical system with and withoutsyntax, in both cases with the minor phrase length ratio test.No Syntax SyntaxActual ActualPredicted major minor no-break Predicted major minor no-breakmajor 51 15 9 major 49 13 9minor 5 1 1 minor 5 5 2no-break 9 9 286 no-break 9 12 282parses (e.g., tried I out, plugs I in, and check I in ).
This is not surprising since the simplePOS labeling scheme labels the particle as a preposition.
The trees using syntacticinformation were not able to overcome this effect because of the relative sparsityof particles in the training data (only 5% of the words labeled as preposit ions areparticles).
Other mistakes included a misplaced minor phrase and a deleted majorphrase where a comma occurs in the original text.
Most of the sentences that werecorrect (had an exact match with one of the spoken versions) were shorter in length.However,  there were several ong sentences judged to have acceptable parses.
Sincemany more variations in prosodic phrasing are allowable for longer sentences, it isnot surprising that the predicted version was not one of the five spoken versions.
The46M.
Ostendorf and N. Vei l leux Hierarchical Stochastic Model for Automatic PredictionTable 5Confusion matrices for predicted breaks using the simple classification trees (no hierarchicalmodel) with and without syntax, in both cases without he minor phrase length ratio test.No Syntax SyntaxActual ActualPredicted major minor no-break Predicted major minor no-breakmajor 57 4 8 major 57 3 4minor 0 0 0 minor 0 0 0no-break 13 15 288 no-break 5 18 298predicted breaks for the best system, the hierarchical model based on questions 1-8,are shown in the Appendix together with the closest spoken prosodic parse.In tree design, we chose to represent major breaks as a separate category that thetree was not explicitly designed to detect.
A consequence of this choice is that thereare fewer "break" data points for training the tree, since there are less than half asmany minor breaks as major breaks in the training data.
This choice is reasonable ifthe two breaks occur at qualitatively different locations, which we suspect.
In fact,results using trees that were trained by merging major and minor breaks into a singlecategory and then embedded in the hierarchical model had either lower predictionaccuracy or a higher false prediction rate.
Another consequence of using only minorbreaks to train the tree is that features that are associated with major breaks are notrepresented in the model, which may explain the poor performance of the model withsyntax.
However, this problem could be addressed with an extension of the currentmodel.In order to see if explicit modeling of a prosodic hierarchy was a useful aspectof the model, we conducted similar experiments using trees designed specifically forclassification.
A binary decision tree was trained using the baseline questions (1-8),and another tree was trained using syntax as well (1-13).
In both cases, the trees weretrained to predict hree classes: major break, minor break, and no break.
Including theminor phrase length ratio test using known break locations (from hand labels) did notimprove prediction performance, so we did not implement a dynamic version basedon hypothesized minor breaks.
Results for these two trees are included in Table 2, withconfusion matrices given in Table 5.
The costs of different errors were chosen to obtaina false detection rate similar to that for the hierarchical model.
Choosing good costsproved difficult, so the correct detection rate is lower than that for the other modelsprimarily because the false detection rate was so low.
The difference in false detectionrates makes comparison to the hierarchical model difficult.
However, experience withperformance of the models at different false detection rates suggests that the baselinehierarchical model outperforms the classification tree that does not use syntax, but thatthe classification tree that uses syntax is at least as good as the hierarchical model.Since the complexity associated with obtaining a syntactic parse is significantly greaterthan that associated with the simple three-level hierarchy that we have proposed, weconclude that explicit modeling of a hierarchy is a useful feature of the model.
Inaddition, the fact that syntactic information was useful for the classification tree butnot for the hierarchical model suggests that syntactic features are more important forpredicting major breaks than minor breaks, since major breaks are not represented asa class in tree design for the hierarchical model.For both the hierarchical model and the simple classification trees, we also inves-tigated the use of more detailed part-of-speech information both with and without47Computational Linguistics Volume 20, Number 1Table 6Confusion matrix for the Bachenko-Fitzpatrick algorithm, which uses syntax and rules toaccount for balancing constituent lengths.ActualPredicted major minor no-breakmajor 49 9 7minor 10 21 12no-break 4 8 265the syntactic features.
The more detailed part-of-speech did not improve performanceunder any of these conditions.
For the classification trees, correct detection improvedslightly, but there was a corresponding increase in false detection.
For the hierarchi-cal model, performance actually degraded.
These results suggest that the complexityassociated with more detailed part-of-speech tagging may not be necessary; however,further esearch is needed to answer this question.
It may be that other POS questions,such as testing a larger window of words around the break as in Wang and Hirschberg(1992), would yield better esults.Finally, we thought it would be interesting to compare the results of our predic-tion algorithm to that of Bachenko and Fitzpatrick's algorithm on our corpus.
Sincethe test set was relatively small, we were able to implement the algorithm by predict-ing the phrase boundaries from the rules by hand.
To assign node indices to prosodicbreaks (Bachenko and Fitzpatrick 1990), a critical value for separating major and mi-nor phrase breaks is calculated based on an average of the indices associated with theprosodic phrase nodes, where the prosodic phrase nodes are all those created by theBachenko-Fitzpatrick primary salience rules.
Boundaries with an index greater thanthe critical value are assigned a major break, indices below 5 have no prosodic break,and intermediate indices map to a minor prosodic break.
(Bachenko and Fitzpatrickinclude index 4 in the minor break category, but 5 was used here to obtain a lower in-sertion rate.)
For multiple verb phrases in sequence, the verb balancing rule is appliedleft-to-right until all verb phrases are grouped before applying the verb adjacency ruleor other processing.
The confusion matrix for these results is shown in Table 6, and theperformance summary is also included in Table 2.
Although the correct break detectionrate is significantly higher than that for the other algorithms, the false detection rate isalso higher, and so the sentence accuracy is similar to that for the baseline hierarchicalmodel.
Unlike the other algorithms, the Bachenko-Fitzpatrick algorithm did not makethe mistake of assigning a minor phrase break before a particle, but this relies onhaving a parser that can make that distinction.
An advantage ofboth the classificationtree and the hierarchical model over the Bachenko-Fitzpatrick model is that they canbe automatically trained, and thus can be tuned to handle particular tasks.Table 7 gives the correct detection and false detection rates calculated by compar-ing the predicted prosodic parses to each of the different spoken versions.
The perfor-mance of speaker f2b, whose speech made up roughly three-quarters of the trainingdata, had performance similar to the average for the five versions with slightly lowercorrect detection rates but also slightly lower false detection rates.
These results sug-gest that the automatic algorithms are not particularly speaker-dependent, though weexpect hat it is important to have similar styles for both training and test data.
Therewas no consistent difference in performance between male and female speakers, andthe difference in error rates for different speakers was relatively small.48M.
Ostendorf and N. Veilleux Hierarchical Stochastic Model for Automatic PredictionTable 7Correct detection/false d tection rates for predicted phrase breaks to each of the five differenttest versions.
With the exception of the Bachenko-Fitzpatrick algorithm, the systems includedhere did not use the minor ratio question.
Speaker codes begin with "f" or "m" for female andmale speakers, respectively.
Speaker code f2b is annotated with "r" for the original radiorecording and "1" for the subsequent lab recording.Phrase model Uses syntax?
CD/FD of merged breaksf2b(r) f2b(1) f3a mlb m3b averageHierarchical model No .69/.04 .66/.05 .73/.06 .72/.06 .70/.06 .70/.05Yes .66/.06 .69/.05 .68/.08 .71/.07 .69/.07 .68/.06Classification trees No .60/.02 .59/.02 .64/.04 .65/.04 .62/.04 .62/.03Yes .58/.01 .57/.01 .61/.02 .67/.01 .63/.01 .61/.01B-F Yes .85/.06 .82/.07 .81/.11 .85/.10 .85/.10 .84/.09Table 8Comparison of correct detection/false d tection rates computed from five test versions fordifferent break prediction algorithms that do and do not use comma information.Hierarchical Model Classification Treesbaseline all features baseline w/syntaxCommas .81/.04 .77/.04 .68/.03 .72/.01No commas .66/.05 .71/.04 .59/.04 .68/.02In all of the previous experiments, the presence of a comma was an important fea-ture for predicting phrase boundaries for all algorithms implemented.
While this is avalid feature in text-to-speech synthesis applications, it is not available in applicationsinvolving spoken speech.
(Although presence of a pause might be a useful alternativefeature.)
In addition, as we have mentioned earlier, commas are not reliably used evenin written text.
Therefore, it is interesting to determine the performance of the algo-rithm without the comma feature.
As expected, performance degrades ignificantly,both for the hierarchical model and for the classification trees.
In addition, fqr thehierarchical model, syntactic information and the minor phrase length ratio test nowprovide information that improves performance over the baseline system.
To illustrateperformance differences, some correct prediction/false prediction rates are given inTable 8.It is difficult to compare our performance figures with other reported resultsbecause of differences in corpora and speaking styles.
However, the average singlespeaker correct detection and false detection rates reported here for our implementa-tions of the Bachenko-Fitzpatrick and Wang-Hirschberg algorithms indicate the ro-bustness of these algorithms to different ypes of data.
Our results for the Bachenko-Fitzpatrick algorithm are somewhat higher than those that they report, .84/.09 vs.,78/.08.
2 Using only the features inferable from text, Wang and Hirschberg use classi-fication trees to predict prosodic boundaries in spontaneous speech, achieving phrasebreak prediction results of .66/.02.
3 (Again, note that these results are not directlycomparable because of the differences in false detection rates, and results for othertrees in Wang and Hirschberg \[1992\] suggest hat these two algorithms have similar2 This figure is calculated from the examples in the appendix in Bachenko and Fitzpatrick (1990),ignoring tertiary boundaries and including sentence-final boundaries as correct.
The sentence that didnot parse was not included in the calculation.3 This result is computed from Figure 6 of Wang and Hirschberg (1992), which illustrates classificationon training data.
Cross validation results may vary slightly.49Computational Linguistics Volume 20, Number 1performance.)
Our classification trees used somewhat different features, though alsobased on POS and syntactic information, and achieved results on radio news speechthat are surprisingly lower, i.e., .59/.02 for the tree that used syntax but did not usecommas.
Of course, the POS and syntactic information used here may not have beenas detailed and/or reliable as that used by Wang and Hirschberg.
(The comparisonshere are based on the average rror rates for single version comparisons.)5.
Discuss ionIn summary, the model proposed here addresses several issues in modeling prosodicphrase structure.
The model is a general formalism for an embedded hierarchy, whichrepresents a unit in terms of the probability of the sequence of subunits comprisingit.
The model is specifically applied to represent a hierarchy containing sentences,major phrases, and minor phrases.
The model captures grammatical and constituentlength factors through the use of a decision tree, as in Wang and Hirschberg (1992),but embedding the tree within a hierarchical structure yields better performance thanthat achieved by a decision tree alone.
The model is stochastic, which accounts forthe natural variability in prosodic parsing, and can be automatically trained to reflectdifferent speaking styles.
The automatic training algorithm described here, based onmaximum likelihood estimation, involves simple relative frequency estimates and de-cision tree design.
Automatic training on a large corpus to design the best predictors ofphrase breaks can also provide new insight into the relationship between prosody andsyntax.
Using the stochastic model, prosodic phrase break prediction involves choos-ing the most likely prosodic parse, which can be achieved using a recursive dynamicprogramming algorithm.
We have found that good phrase break prediction results canbe achieved without the use of syntactic information or detailed part-of-speech labels,resulting in a very low complexity prediction algorithm.
Without syntax, our algo-rithm predicted a good prosodic parse for 18 out of 23 sentences, which correspondsto a correct break prediction of 81% and a false prediction rate of 4%.There are many ways in which this work could be extended.
As we have pointedout, it would be useful to use features directly in determining the probability of a unit,rather than simply representing a unit in terms of the probabilities of the subunits.
Forexample, we conjecture that commas and certain syntactic structures might be goodpredictors of major phrase breaks, but not minor phrase breaks.
In addition, otherfeatures could be used in the model, including different syntactic features and differ-ent questions about the more detailed part-of-speech labels.
Automatically predictedprominence (or pitch accent) locations might also be useful in phrase boundary pre-diction, although it is arguable whether prominence prediction should come before orafter boundary placement.
Of course, it would also be interesting to consider a higherorder hierarchy, though we anticipate that a successful implementation would requirerepresentation f features at the different levels.
The results reported here were limitedto some extent by the amount of available data.
A larger training set would enablethe study of more factors, including possibly paragraph-level phenomena.
A largertest set would better establish the significance of the results.
Finally, the best test of aphrase break prediction algorithm is in perceptual judgments of synthesized speech,and we would like to evaluate our algorithm in this context.In this work we have focused on the synthesis application of prosodic phrase breakprediction.
However, one of the advantages of a stochastic model is that it may be use-ful for analysis of spoken speech.
Because there is some relationship between prosodyand syntax, prosodic phrase structure can be used to improve the performance and/orspeed of speech understanding systems.
For example, a score of the consistency be-50M.
Ostendorf and N. Veilleux Hierarchical Stochastic Model for Automatic Predictiontween a syntactic parse and a prosodic parse has been used to resolve ambiguitiesin sentence interpretation, where the score is computed by comparing automaticallydetected prosodic phrase breaks to phrase breaks predicted from the text of the dif-ferent interpretations (Ostendorf, Wightman, and Veilleux 1993).
Another approach tosyntactic disambiguation using prosodic information is described in Bear and Price(1990) and Ostendorf, Price, Bear, and Wightman (1990), where prosodic breaks areused to constrain grammar ules in a parser.
A stochastic phrase model could also beused to improve performance of this system simply by serving as a "language model"(as in speech recognition) to improve the performance of a detection algorithm usingacoustic information.AcknowledgmentsWe would like to thank Patti Price andStefanie Shattuck-Hufnagel fortheir usefulcomments and insights, an anonymousreviewer for many valuable comments, andGay Baldwin and Liz Shriberg forhand-labeling part of the corpus withprosodic labels.
The radio news corpus wasprovided by WBUR, a public radio station.The skeletal parses for the corpus are part ofa preliminary version of the Penn TreebankCorpus, copyright of the University ofPennsylvania.
This work was supported byNSF under grant number IRI-8805680.ReferencesAllen, J.; Hunnicutt, S.; Carlson, R.; andGranstrom, B.
(1979).
"MITalk-79: The1979 MIT text-to-speech system."
InSpeech Communications Papers Presented atthe 97th Meeting of the ASA, edited by Wolfand Klatt, 507-510.Allen, J.; Hunnicutt, M. S.; and Klatt, D.(1987).
From Text o Speech: The MITalkSystem.
Cambridge University Press.Altenberg, B.
(1987).
Prosodic Patterns inSpoken English.
Lund University Press.Bachenko, J., and Fitzpatrick, E. (1990).
"Acomputational grammar ofdiscourse-neutral prosodic phrasing inEnglish."
Computational Linguistics 16(3),155-170.Bahl, L. R.; Brown, P. E; deSouza, P. V.; andMercer, R. L. (1989).
"A tree-basedstatistical language model for naturallanguage speech recognition."
IEEETransactions on Acoustics, Speech, and SignalProcessing 37(7), 1001-1008.Bear, J., and Price, P. J.
(1990).
"Prosody,syntax and parsing."
In Proceedings, ACLConference, 17-22.Beckman, M., and Pierrehumbert, J.
(1986).
"Intonational structure in Japanese andEnglish."
In Phonology Yearbook 3, editedby J. Ohala, 255-309.Booij, G. (1983).
"Principles and parametersin prosodic phonology."
Linguistics 21,249-280.Breiman, L.; Friedman, J.; Olshen R.; andStone, C. (1984).
Classification andRegression Trees, The WadsworthStatistics/Probability Series, Wadsworthand Brooks.Church, K. W. (1988).
"A stochastic partsprogram and noun phrase parser forunrestricted text."
In Proceedings, SecondConference on Applied Natural LanguageProcessing, 136-143.Cooper, W., and Paccia-Cooper, J.
(1980).Syntax and Speech.
Harvard UniversityPress.Gee, J., and Grosjean, E (1983).
"Performance structures: Apsycholinguistic and linguistic appraisal.
"Cognitive Psychology 15, 411--458.Hindle, D. M. (1987).
"Acquiringdisambiguation rules from text."
InProceedings, Association for ComputationalLinguistics Meeting, 118-125.Hirose, K., and Fujisaki, H.
(1982).
"Analysis and synthesis of voicefundamental frequency contours ofspoken sentences."
In Proceedings,International Conference on Acoustics, Speech,and Signal Processing, 950-953.Ladd, D. R. (1986).
"Intonational phrasing:The case for recursive prosodic structure.
"In Phonology Yearbook 3, edited by J.-Ohala,311-340.Ladd, D. R., and Campbell, N.
(1991).
"Theories of prosodic structure: Evidencefrom syllable duration."
In Proceedings,XII International Congress of PhoneticSciences, 2, 290-293.Lehiste, I.
(1973).
"Phonetic disambiguationof syntactic ambiguity."
Glossa 7(2),107-121.Liberman, M. Y., and Prince, A. S.
(1977).
"On stress and linguistic rhythm.
"Linguistic Inquiry 8, 249-336.Marcus, M. P.; Santorini, B.; andMarcinkiewicz, M. (1993).
"Building avery large annotated corpus of English:51Computational Linguistics Volume 20, Number 1The Penn Treebank."
ComputationalLinguistics 19(2), 313-330.Meteer, M.; Schwartz, R.; and Weischedel, R.(1991).
"POST: Using probabilities inlanguage processing."
In Proceedings,International Joint Conference on ArtificialIntelligence, 960-965.Nespor, M., and Vogel, I.
(1983).
"Prosodicstructure above the word."
In Prosody:Models and Measurements, edited byA.
Cutler and D. R. Ladd, 123-140.Springer-Verlag.Nespor, M., and Vogel, I.
(1986).
ProsodicPhonology.
Foris.O'Shaughnessy, D. (1989).
"Parsing with asmall dictionary for applications such astext-to-speech."
Computational Linguistics15(2), 97-108.Ostendorf, M.; Price, P.; Bear, J.; andWightman, C. W. (1990).
"The use ofrelative duration in syntacticdisambiguation."
In Proceedings, ThirdDARPA Workshop on Speech and NaturalLanguage, 26-31.Ostendorf, M., and Roukos, S. (1989).
"Astochastic segment model forphoneme-based continuous speechrecognition."
IEEE Transactions onAcoustics, Speech, and Signal Processing37(12), 1857-1869.Ostendorf, M.; Wightman, C. W.; andVeilleux, N. M. (1993).
"Parse scoringwith prosodic information: Ananalysis/synthesis approach."
ComputerSpeech and Language, 193-210.Price, P.; Ostendorf, M.; Shattuck-Hufnagel,S.
; and Fong, C. (1991).
"The use ofprosody in syntactic disambiguation.
"Journal of the Acoustical Society of America90(6), 2956--2970.Selkirk, E. (1980).
"The role of prosodiccategories in English word stress.
"Linguistic Inquiry 11,563-605.Selkirk, E. (1984).
Phonology and Syntax: TheRelation between Sound and Structure.
MITPress.Sorin, C.; Larreur, D.; and Llorca, R.
(1987).
"A rhythm-based prosodic parser fortext-to-speech systems in French."
InProceedings, International Congress ofPhonetic Sciences 1, 125-128.t'Hart, J.; Collier, R.; and Cohen, A.
(1990).A Perceptual Study of Intonation.
CambridgeUniversity Press.Veilleux, N.; Ostendorf, M.; Price, P.; andShattuck-Hufnagel, S. (1990).
"Markovmodeling of prosodic phrase structure.
"In Proceedings, International Conference onAcoustics, Speech, and Signal Processing,777-780.Wang, M., and Hirschberg, J.
(1992).
"Automatic lassification of intonationalphrase boundaries."
Computer Speech andLanguage 6(2), 175-196.Wightman, C.; Shattuck-Hufnagel, S.;Ostendorf, M.; and Price, P.
(1992).
"Segmental durations in the vicinity ofprosodic phrase boundaries."
Journal of theAcoustical Society of America 91(3),1707-1717.52M.
Ostendorf and N. Veilleux Hierarchical Stochastic Model for Automatic PredictionAppendix: Predicted Breaks in Test CorpusThe sentences below illustrate the predicted minor (I) and major (1\[) phrase boundariesfor our best case algorithm, that which uses the basic questions 1-8 in the hierarchicalmodel.
The evaluations "correct" (C), "acceptable" (A), and "incorrect" (I) are indicatedafter each sentence.
For the cases where the predicted sentences were either acceptableor incorrect, we have included the spoken version that was closest to the predictedsentence in the sense of minimizing the Euclidean distance based on representing nobreak with 0, a minor break with 1, and a major break with 2.1.
Computerized phone calls, I I which do everything from selling magazinesubscriptions I to reminding people about meetings, \]1 have become thetelephone quivalent I I of junk mail.
I I (A)Computerized phone calls, I I which do everything I from sellingmagazine subscriptions \]1 to reminding people about meetings II havebecome the telephone quivalent I of junk mail.
I I2.
But a new application of the technology II is about to be tried I out inMassachusetts I I to ease crowded jail conditions.
II (I)But a new application of the technology I I is about to be tried out inMassachusetts \]lto ease crowded jail conditions.
II3.
Next week some inmates released early I from the Hampton County jailin Springfield \]1 will be wearing a wristband that hooks up with a specialjack I on their home phones.
I\] (A)Next week I I some inmates released early \[ from the Hampton Countyjail in Springfield I\[ will be wearing a wristband \[I that hooks up with aspecial jack \] on their home phones.
II4.
Whenever a computer andomly calls I\[ them from jail, I\[ the formerprisoner plugs I in to let corrections officials know I I they're in the rightplace I at the right time.
\]\[ (I)Whenever a computer andomly calls them from jail, I I the formerprisoner plugs in I to let corrections officials know I\] they're in the rightplace \] at the right time.
\]l5.
Margo Melnicove reports.
II (C)6.
The device is attached \] to a plastic wristband.
II (C)7.
It looks like a watch.
II (C)8.
It functions like an electronic probation officer.
II (A)It functions I like an electronic probation officer.
II9.
When a computerized call is made I to a former prisoner's home phone,II that person answers I by plugging in the device.
II (C)10.
The wristband can be removed II only by breaking its clasp, II and ifthat's done the inmate I is immediately returned to jail.
I\] (A)The wristband \] can be removed II only by breaking its clasp, I\] and ifthat's done II the inmate I is immediately returned to jail.
II11.
The description conjures up images \] of big brother watching.
II (C)12.
But Jay Ash, II deputy superintendent of the Hampton County jail I inSpringfield, I says the surveillance system \]l is not that sinister.
II (I)53Computational Linguistics Volume 20, Number 113.14.15.16.17.18.19.20.21.22.23.But Jay Ash, I I deputy superintendent of the Hampton County jail inSpringfield, II says the surveillance system I is not that sinister.
\[ISuch supervision, II according to Ash, I is a sensible, \[I cost effectivealternative to incarceration I that should not alarm civil libertarians.
II (A)Such supervision, II according to Ash, II is a sensible, L I cost effectivealternative to incarceration I\] that should not alarm II civil libertarians.
11Doctor Norman Rosenblatt, \] dean of the college I of criminal justice atNortheastern University, I\] agrees.
II (A)Doctor Norman Rosenblatt, II dean of the college I of criminal justice atNortheastern University, II agrees.
I\[Rosenblatt expects electronic surveillance I in parole situations to becomemore widespread, \]1 and he thinks eventually people II will get used tothe idea.
II (I)Rosenblatt expects electronic surveillance in parole situations to becomemore widespread, II and he thinks eventually II people will get used tothe idea.
IISpringfield jail deputy superintendent Ash says II although it will allowI\[ some prisoners to be released II a few months II before their sentencesare up, I L concerns that may raise about public safety II are not wellfounded.
I\] (A)Springfield jail deputy superintendent Ash \] says II although it will allowI some prisoners to be released I a few months before their sentences areup, II concerns that may raise I about public safety II are not wellfounded.
IIMost county jail inmates I\] did not commit violent crimes.
I1 (C)They're in jail for such things I as bad checks or stealing.
I\] (A)They're in jail for such things I as bad checks I or stealing.
I\]Those on early release must check I in with corrections officials fifty timesII a week according to Ash, II who says about half I the contacts for aselect group II will now be made I L by the computerized phone calls.
I\[ (I)Those on early release I must check in with corrections officials II fiftytimes a week \]1 according to Ash,\]l who says about half I the contacts fora select group \[I will now be made I by the computerized phone calls.
IIInitially the program will involve II only a handful  of inmates.
11 (A)Initially \] the program will involve I only a handful  of inmates.
IIAsh says the ultimate goal \[I is to use it to get I about forty out of jailearly.
II (A)Ash says I the ultimate goal II is to use it to get about forty II out of jailearly.
IIThe Springfield jail, I\[ built for 270 people, I now houses more than 500.\]L (A)The Springfield jail, \[I built for 270 people, II now houses more than 500.IIFor WBUR, II I 'm Margo Melnicove.
II (C)54
