INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 85?89,Utica, May 2012. c?2012 Association for Computational LinguisticsExtractive email thread summarization:Can we do better than He Said She Said?Pablo Ariel DuboueLes Laboratoires Foulab999 du CollegeMontreal, Quebe?cpablo.duboue@gmail.comAbstractHuman-written, good quality extractive sum-maries pay great attention to the text intermix-ing the extracts.
In this work, we focused onthe lexical choice for verbs introducing quotedtext.
We analyzed 4000+ high quality sum-maries for a high traffic mailing list and manu-ally assembled 39 quotation-introducing verbclasses that cover the majority of the verb oc-currences.
A significant amount of the data iscovered by on-going work on e-mail ?speechacts.?
However, we found that one third of the?tail?
is composed by ?risky?
verbs that mostlikely will be beyond the state of the art forlonger time.
We used this fact to highlight thetrade-offs of risk taking in NLG, where inter-esting prose might come at the cost of unset-tling some of the readers.1 IntroductionHigh traffic mailing lists pose a challenge to an ex-tended audience laterally interested on the subjectmatter but unable or unwilling to follow them oneveryday minutiae.
In this context, high-level sum-maries are of great help and in certain cases there arepeople or companies that step into the plate to pro-vide such service.
In recent years, there has beenan ever increasing interest (Muresan et al, 2001;Nenkova and Bagga, 2003; Newman and Blitzer,2003; Rambow et al, 2004; Wan and McKeown,2004; McKeown et al, 2007; Ulrich, 2008; Wang etal., 2009) in automating this task, with many worksfocusing on selectively extracting quotes from keye-mail exchanges.In this work, we focus on finding appropriate andvaried ways to cite selected quotes from the emailthreads.
A seemingly simple task, this problemtouches: speech act detection (Searle, 1975) (ques-tion vs. announcement vs. reply), opinion mining(Pang and Lee, 2008) (complained vs. thanked) andcitation polarity analysis (Teufel, 1999): (agreed vs.disagreed vs. added).At this stage, we will show training data we haveacquired for the task and a set of manually assem-bled verb clusters that show the richness of the prob-lem.
Moreover, we have used these clusters to high-light a trade-off of ?risk taking?
in NLG, where gen-erating interesting prose might lead to text that canupset some readers in the presence of errors.This paper is structured as follows: in the nextsection we discuss the data from where we obtainedthe raw verbs and then proceed to describe the man-ual analysis to cluster and identify ?risky?
verbs.
Wethen present the whole set of clusters and concludewith a discussion of risk taking in NLG.2 DataThis work is part of a larger effort to build automatictools to replace a key resource that the Linux Ker-nel development community enjoyed for five years:the Kernel Traffic summaries of the activities in theLinux Kernel mailing list (LKML).The LKML is of extremely high traffic (300 mailsa day on average).
For five years (since 1999), JackBrown hand-picked the most newsworthy threadsin a week time and published a summary for eachthread.
The summaries were made available (undera Free Software license) in a rich XML-based format85<p>Gregory Maxwell r e p l i e d , <quo t e who=?George Maxwell ?>Do yousee t h e ?
( s i c ) ?
Tha t u s u a l l y s t a n d s f o r ?
S p e l l i n g i s Co r r e c t ?
.</ quo t e></ p><p>Ol i v e r Xymoron r e j o i n e d :</ p><quo t e who=?
O l i v e r Xymoron?><p>I t h i n k what we have he r e i s an i r o n i c doub l e typo .
Themessage i s a c t u a l l y i n d i c a t i n g t h e d r i v e i s no t f e e l i n g ve rygood :</ p><p>+ { 0xb900 , ?
P l ay o p e r a t i o n a b o r t e d ( s i c k ) ? }
,</ p><p>Hope fu l l y t h i s ve ry impo r t a n t change w i l l make i t i n t o2 .
2 .
2 .</ p></ quo t e><p>Brendan Cu l l y k a f l o o g i t a t e d :</ p><quo t e who=?
Brendan Cu l l y ?><p>?
s i c ?
doesn ?
t s t a n d f o r ?
s p e l l i n g i s c o r r e c t ?
, o r even?
s t a t e d i n c o n t e x t ?
( yech ! )
.
< / p><p>In f a c t , i t s t a n d s f o r ?
yes , I know i t l o ok s funny , bu tt h a t ?
s how I want i t ?
.
But p eop l e go t t i r e d o f t y p i n gY, IKILF , BTHIWI so t h ey a b b r e v i a t e d i t t o SIC .</p>Figure 1: Kernel Traffic #6, Feb. 18th 1999 (excerpt).
(Figure 1) that included, among many other things,explicit marking of all quoted text, with attribution.These summaries were in general followed by amuch larger audience than the mailing list itself dueto a number of factors including the fact that theymake for quite an entertaining read.
Mr. Brown?sprose was high quality and quite consistent in style,1which highlights its potential as training materialfor NLG.
As Reiter and Sripada (2002) pointed out,learning tasks in NLG profit from training data of thehighest possible quality in terms of prose and con-sistency (as compared with training data for NLU,where robustness comes from exposing the systemto a variety of malformed texts).In our journey to approximate Mr. Brown?s workby automatic means, we decided to start on a rel-atively unstudied problem: introducing quoted ref-erences in a rich manner.
In the 4,253 hand writ-ten summaries by Mr. Brown (made available in 344newsletter issues) 95% contain a quote, with an av-erage of 3.28 quotes per summary.
Moreover, 72%of the total characters in the summaries are insidequotes (including markup).2.1 ProcessingWe employed a processing pipeline implemented inthe UIMA framework (Ferrucci and Lally, 2004)to extract the verbs immediately before a quota-tion.
We used annotators from the OpenNLP project(Apache, 2011) implementing Maximum Entropymodels for NLP (Ratnaparkhi, 1998).
For the sen-tence before a quotation we extracted the word1A quality of prose that continues with his editorial contri-butions to Linux Journal and Linux Magazine.marked with the POS tag ?VBD?
closer to the quota-tion.
Processing the 334 issues available for KernelTraffic resulted in 11,634 verb occurrences extractedfor 344 verbs (and verb-like errors).
These verbs arethe ones we employ for the analysis and inferencesdrawn in the next section.3 AnalysisFrom the grand-total of 344 verbs (including ty-pos and POS-tagger errors), we took all the verbsthat appeared at least a hundred times (the top 55verbs) and expanded them from the larger list (plusWordNet synsets (Miller, 1995)), grouping theminto classes.
The grouping captures synonyms forthe particular task of introducing quoted text in sum-maries.
The resulting 39 classes (Table 1) contain127 verbs accounting for 96% of the cases (the ta-ble contains an ?other?
class with the remaining 217verbs that account for 4% of the occurrences).
Theverbs included from WordNet do not appear in thecorpus and thus have a count of zero.
This large setof verbs highlights the many possibilities a systemthat chooses to go just with ?s/he said?
will be miss-ing.
Moreover, such a system can be immediatelyenriched with 17 different variations with associatedlikelihoods.We determined whether or not generation errorsfor a given verb class would be ?dangerous?
usingthe following criteria:If the automatic determination of whetherthe original quote fell into a particularverb class fails, would the original authortake issue with the summary upon readingthe misclassified verb?That is, if the system decides that Brendan Cully(from the example in the introduction) has indeedkafloogitated2 with his reply but such decision wasmade in error (and Mr. Cully was just remarkingor explaining), would Mr. Cully take issue with thesummary?
As with any automated system, the pos-sibility of automated mistakes should make its de-signers err on the side of making more conservativedecisions.
Under such desiderata, we think the 102That word has been invented by Mr. Brown and was usedonly once within the five years of Kernel Traffic.86classes highlighted in Table 1 are thus too ?danger-ous?
to be addressed currently by automated means.Initially, that might not appear such a big loss,as none of them account for more than 1% of thetotal occurrences.
However, as with many otherphenomena in NLP, a few cases account for mostoccurrences: the clusters for ?said,?
?asked,?
and?replied?
account for 2/3 of the total occurrencesand, overall, the top 9 classes account for 93%of the cases.
From the rich tail that encompassesMr.
Brown prose, the ?dangerous?
classes accountfor 35% of the cases from position 10 and onward.It is our opinion that such cases were the reasonMr.
Brown?s summaries were enjoyable to read andare only a small example of the humor and piquancybehind his prose.
Now, it might be the case suchquality will be beyond the state of the art of NLGfor quite some time.In that sense, we consider the prevalence of riskyclasses as a negative result that highlights a prob-lem for NLG well beyond the task at hand: we, ashumans, enjoy text that takes a stand, that arguesits points in an opinionated manner.3 Such is thedistinction between dull reports and flourish sum-maries.
Even in the highly technical domain of oper-ating system kernel discussions, Mr. Brown felt theneed to use words such as ?groused?
and ?chastised.
?The problem might as well be cultural, with opin-ionated prose paradigmatic to the Western world.
Itmight also be related to our culture as NLG prac-titioners, where we always thrive for perfect output.Our data shows that to go beyond ?He Said She Said?in a truly interesting manner we will have to be readyto make mistakes which could make some peopleunhappy, a trade-off that it would be interesting tosee explored more often in NLG.4 Related WorkSince the seminal work by Muresan et al (2001),email summarization and in particular email threadsummarization has spanned full dissertations (Ul-rich, 2008).
Existing resources for email summa-rization (Ulrich et al, 2008), however, do not em-phasize explicitly the type of quotes being used.Understandingly, most of the work has been de-voted to selecting the particular words, sentences or3Not unlike this discussion.paragraphs to extract from the original e-mails.
ei-ther by distilling terms or topics (Muresan et al,2001; Newman and Blitzer, 2003) or finding a repre-sentative example (Nenkova and Bagga, 2003; Ram-bow et al, 2004; Wang et al, 2009).The issue of choosing how to introduce the ex-tracted text has only been studied in the context ofspeech act detection (Cohen et al, 2004; Wan andMcKeown, 2004) within emails or within threadeddiscussions (Feng et al, 2006), which is limited toquestions, replies and the like (a very important casewhich covers 2/3 of our available data).
The prob-lem of detecting question / answer pairs in e-mailsis by far the one who has received the most attentionin the field (Bickel and Scheffer, 2004; Shrestha andMcKeown, 2004; McKeown et al, 2007).The verbs in each of the classes in Table 1 havea near-synonym relation:4 even though ?recom-mended?
and ?urged?
share most of their meaning,the differences in style, color and subtle meaningneed to be further elucidated for successful lexicalchoice.
This topic has started to be explored in de-tail recently (Edmonds and Hirst, 2002).Our work falls in the larger field of summarizationby using NLG means, a discipline that has receivedsignificant attention of late (Belz et al, 2009).5 ConclusionIn this paper, we have brought to the attention ofNLG practitioners the rich resource embodied in fiveyears of Kernel Traffic newsletters.
We had alsohighlighted the richness of the problem of lexicalchoice for verbs introducing quotations in extractiveemail summarization.Moreover, we contributed 39 clusters manuallyassembled from naturally occurring verbs extractedfrom 4000+ high quality summaries.
These clus-ters can enrich even the most straightforward exist-ing systems.
Finally, we argued that, while usefulsummaries might be around the corner, entertainingsummaries will be well beyond the state of the artuntil the field is willing to take the risk involved instanding behind automatically generated prose withintrinsic value-judgments.In our ongoing work, we are targeting the creation4Thanks to an anonymous reviewer for bringing this fact intoour attention.87Table 1: Quotation introducing verb classes, with counts.
The ?other?
class appears in row 7.
Lines in bold areconsidered ?dangerous.?
The last column is the author?s opinion about which type of technology is more relevant forchoosing that class (speech act detection (A), opinion mining (O) or citation link analysis (C)).
Verbs missing due tospace restrictions are in the appendix.# Top Verbs # verbs Total Counts Accum.
Type1 said (2726) remarked (361) posted (163) pointed out (148) 17 3531 (30.35%) 30.35% A2 replied (3476) responded (21) answered (11) 3 3508 (30.15%) 60.50% A3 added (1059) included (13) followed (10) 3 1082 (9.30%) 69.80% C4 announced (902) declared (1) 2 903 (7.76%) 77.56% A5 asked (509) inquired (0) 2 509 (4.37%) 81.94% A6 explained (427) 1 427 (3.67%) 85.61% A7 FELT (21) MADE (21) WANTED (8) BROKE (8) 217 403 (3.46%) 89.07% -8 reported (254) detailed (1) 2 255 (2.19%) 91.26% A9 suggested (188) proposed (35) 2 223 (1.91%) 93.18% O10 objected (90) protested (5) 2 95 (0.81%) 94.00% O11 concluded (48) ended (5) finished (4) closed (2) 5 59 (0.50%) 94.50% C12 offered (52) volunteered (6) 2 58 (0.49%) 95.00% O13 confirmed (44) supported (4) affirmed (3) reasserted (1) 7 52 (0.44%) 95.45% C14 summed up (21) summarized (18) 2 39 (0.33%) 95.78% A15 agreed (37) concurred (1) concorded (0) 3 38 (0.32%) 96.11% C16 described (33) 1 33 (0.28%) 96.39% A17 took issue (17) disagreed (11) dissented (2) differed (1) 4 31 (0.26%) 96.66% O18 complained (22) sounded off (2) kicked (1) groused (1) 7 29 (0.24%) 96.91% O19 argued (28) contended (0) debated (0) 3 28 (0.24%) 97.15% O20 listed (27) enumerated (0) 2 27 (0.23%) 97.38% A21 continued (25) kept (1) 2 26 (0.22%) 97.61% A22 clarified (25) elucidated (0) 2 25 (0.21%) 97.82% C23 recommended (17) urged (4) advised (2) advocated (1) 4 24 (0.20%) 98.03% C24 speculated (16) mused (2) guessed (2) supposed (1) 6 22 (0.18%) 98.22% O25 elaborated (11) expanded (7) expounded (2) 3 20 (0.17%) 98.39% C26 corrected (18) chastised (1) rectified (0) righted (0) 4 19 (0.16%) 98.55% O27 exclaimed (6) called out (5) cried out (4) shouted (2) 5 18 (0.15%) 98.71% O28 quoted (15) cited (2) 2 17 (0.14%) 98.85% C29 warned (8) cautioned (6) admonished (2) 3 16 (0.13%) 98.99% O30 interjected (11) sprung (1) interposed (1) 3 13 (0.11%) 99.10% O31 quipped (10) joked (1) chuckled (1) cracked (1) 4 13 (0.11%) 99.21% O32 requested (12) 1 12 (0.10%) 99.32% A33 tried (9) attempted (2) tested (1) 3 12 (0.10%) 99.42% O34 acknowledged (8) admitted (3) recognized (0) 3 11 (0.09%) 99.51% A35 countered (10) 1 10 (0.08%) 99.60% C36 found (7) discovered (2) launched (1) 3 10 (0.08%) 99.69% A37 reiterated (9) repeated (1) 2 10 (0.08%) 99.77% C38 started (9) began (1) 2 10 (0.08%) 99.86% A39 rejoined (6) retorted (2) returned (1) 3 9 (0.07%) 99.93% O40 chimed (7) 1 7 (0.06%) 100% O88of a systemic fragment for the quotation-introducingverbs, in the style of KPML (Bateman, 1995).AcknowledgmentsThe author would like to thank the anonymous re-viewers as well as Annie Ying for valuable feed-back and insights.
He will also like to thank theDebian NYC group for bringing the Kernel Trafficsummaries to his attention.AppendixThe verbs omitted for reasons of space in Table 1 are the fol-lowing: for the ?said?
cluster, mentioned (34), commented (25),wrote (20), noticed (17), spoke (9), expressed (6), showed (5),observed (5), stated (5), asserted (4), referred (1), noted (1),declared (1); for the ?concluded?
cluster, resolved (0); for the?confirmed?
cluster, corroborated (0), sustained (0), substanti-ated (0); for the ?complained?
cluster, hollered (1), ranted (1),kvetched (1); for the ?speculated?
cluster, theorized (1), conjec-tured (0); for the ?exclaimed?
cluster, sputtered (1).ReferencesApache.
2011.
OpenNLPhttp : //incubator.apache.org/opennlp.John A. Bateman.
1995.
KPML: The KOMET?Penmanmultilingual linguistic resource development environ-ment.
In Proc.
of EWNLG, pages 219?222.Anja Belz, Roger Evans, and Sebastian Varges, editors.2009.
Proc.
of the 2009 Workshop on LanguageGeneration and Summarisation (UCNLG+Sum 2009).ACL, Suntec, Singapore, August.Steffen Bickel and Tobias Scheffer.
2004.
Learningfrom message pairs for automatic email answering.
InECML, volume 3201 of LNCS, pages 87?98.
Springer.William W. Cohen, Vitor R. Carvalho, and Tom M.Mitchell.
2004.
Learning to classify email into?speech acts?.
In Proc.
of EMNLP, volume 4.Philip Edmonds and Graeme Hirst.
2002.
Near-synonymy and lexical choice.
Computational Linguis-tics, 28(2):105?144.Donghui Feng, Erin Shaw, Jihie Kim, and Eduard Hovy.2006.
Learning to detect conversation focus ofthreaded discussions.
In Proc.
HLT-NAACL, pages208?215.David Ferrucci and Adam Lally.
2004.
UIMA: an ar-chitectural approach to unstructured information pro-cessing in the corporate research environment.
Natu-ral Language Engineering, 10(3-4):327?348.Kathleen McKeown, Lokesh Shrestha, and Owen Ram-bow.
2007.
Using question-answer pairs in extractivesummarization of email conversations.
In CICLing,volume 4394 of LNCS, pages 542?550.
Springer.G.A.
Miller.
1995.
Wordnet: a lexical database for en-glish.
Communications of the ACM, 38(11):39?41.Smaranda Muresan, Evelyn Tzoukermann, and Judith L.Klavans.
2001.
Combining linguistic and machinelearning techniques for email summarization.
In Proc.of the 2001 workshop on Computational Natural Lan-guage Learning-Volume 7, page 19.
ACL.Ani Nenkova and Amit Bagga.
2003.
Facilitating emailthread access by extractive summary generation.
InRANLP, volume 260 of Current Issues in LinguisticTheory (CILT), pages 287?296.Paula S. Newman and John C. Blitzer.
2003.
Summariz-ing archived discussions: a beginning.
In IUI, pages273?276.
ACM.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and Trends in Infor-mation Retrieval, 2(1-2):1?135.Owen Rambow, L. Shrestha, J. Chen, and C. Lauridsen.2004.
Summarizing email threads.
In Proc.
of HLT-NAACL 2004: Short Papers, pages 105?108.
ACL.Adwait Ratnaparkhi.
1998.
Maximum Entropy Modelsfor Natural Language Ambiguity Resolution.
Ph.D.thesis, University of Pennsylvania, Philadelphia, PA.Ehud Reiter and S. Sripada.
2002.
Should corpora textsbe gold standards for NLG?
In Proceedings of SecondInternational Conference on Natural Language Gen-eration INLG-2002, pages 97?104, Arden House, NY.John R. Searle.
1975.
A taxonomy of illocutionary acts.In Language, Mind and Knowledge, pages 344?369.University of Minnesota Press.Lokesh Shrestha and Kathleen McKeown.
2004.
Detec-tion of question-answer pairs in email conversations.In Proc.
of ACL, page 889.
ACL.Simone Teufel.
1999.
Argumentative zoning: Informa-tion extraction from scientific text.
Ph.D. thesis, Uni-versity of Edinburgh, England.Jan Ulrich, Gabriel Murray, and Giuseppe Carenini.2008.
A publicly available annotated corpus for super-vised email summarization.
In AAAI08 EMAIL Work-shop, Chicago, USA.
AAAI.Jan Ulrich.
2008.
Supervised machine learning for emailthread summarization.
Master?s thesis, Computer Sci-ence.Stephen Wan and Kathleen McKeown.
2004.
Generatingoverview summaries of ongoing email thread discus-sions.
In Proc.
of ACL, page 549.
ACL.Baoxun Wang, Bingquan Liu, Chengjie Sun, XiaolongWang, and Bo Li.
2009.
Adaptive maximum marginalrelevance based multi-email summarization.
In AICI,volume 5855 of LNCS, pages 417?424.
Springer.89
