Explorations in Disambiguation Using XML Text RepresentationKenneth C. LitkowskiCL Research9208 Gue RoadDamascus, MD 20872ken@clres.comAbstractIn SENSEVAL-3, CL Research participated infour tasks: English all-words, English lexicalsample, disambiguation of WordNet glosses, andautomatic labeling of semantic roles.
Thisparticipation was performed within thedevelopment of CL Research?s KnowledgeManagement System, which massively tags textswith syntactic, semantic, and discoursecharacterizations and attributes.
This System isfully integrated with CL Research?s DIMAPdictionary maintenance software, which providesaccess to one or more dictionaries fordisambiguation and representation.
Our coredisambiguation functionality, unchanged sinceSENSEVAL-2, performed at a level comparableto our previous performance.
Our participationin the SENSEVAL-3 tasks was concernedprimarily with text processing and representationissues and did not advance our disambiguationcapabilities.IntroductionCL Research participated in four SENSEVAL-3tasks: English all-words, English lexical sample,disambiguation of WordNet glosses, and automaticlabeling of semantic roles.
We also ran the latter twotasks, but since their test sets were generated blindly,our results did not involve use of any priorinformation.Our participation in these tasks is a continuationand extension of our efforts to perform NLP taskswithin an integrated text processing system known asthe Knowledge Management System (KMS).
KMSparses and processes text into an XML representationtagged with syntactic, semantic, and discourseproperties.
This representation is then used for suchtasks as question answering and text summarization(Litkowski, 2004a; Litkowski, 2004b).The SENSEVAL-3 tasks were performed as partof CL Research?s efforts to extend and improve thesemantic characterizations in the KMS XMLrepresentations.
For each SENSEVAL-3 task, thecorresponding texts in the test sets were processedusing the general KMS functionality.
However, sincethe texts involved in the SENSEVAL tasks werequite small, the amount of processing was quiteminimal.
The descriptions below focus on theintegration of disambiguation technology in a largersystem and do not present any advancements in thistechnology.1 The SENSEVAL-3 All-Words TaskOur procedures for performing this task and ourresults were largely unchanged from SENSEVAL-2(Litkowski, 2001; Litkowski, 2002).
Our system isunsupervised, instead relying on information inwhatever dictionary is being used to disambiguate thewords.
In this case, as in SENSEVAL-2, WordNet1.7.1 was used.The main types of information used are defaultsense selection, idiomatic usage, syntactic andsemantic clues, subcategorization patterns, wordforms, syntactic usage, context, and topics or subjectfields.
As pointed out in Litkowski (2002), theamount of information available in WordNet isproblematic.
Additional information suitable fordisambiguation is available in WordNet 2.0, but wewere unable to test the effect of the changes, eventhough we could have easily switched our system touse this later version.In performing this task, we spent some timecleaning the text files, removing extraneous materialand creating a more natural text file (e.g., joiningcontractions).
Use of a preprocessed file is somewhatdifficult.
Since some tokens to be disambiguated wereunnatural (e.g., ?that?s?
broken into two tokens, withAssociation for Computational Linguisticsfor the Semantic Analysis of Text, Barcelona, Spain, July 2004SENSEVAL-3: Third International Workshop on the Evaluation of Systemsonly the ??s?
to be disambiguated), this affected thequality of our parse output.After removing extraneous material, KMS parsedand processed the XML source file, treating the textin its ordinary manner.
The first step of KMSinvolves splitting a text into sentences and thenparsing each sentence.
To customize KMS for thistask, we had to create a list of tokens, advancingthrough this list in concert with the parse output.
Thisprocess was different from the normal processing ofKMS where every word is disambiguated in anintegrated fashion.
Our results are shown in Table 1,broken down by part of speech as indicated in theanswer key.Table 1.
All-Words ResultsRun Items PrecisionNouns 895 0.523Verbs 731 0.361Adjectives 346 0.413Adverbs 13 0.077Hyphenated/U 56 0.179Total 2041 0.434These results are similar to our performance inSenseval-2, where our precision was 0.451.
Ourrecall is the same, since we attempted each item.As indicated, several factors degraded ourperformance, primarily the quality of the informationavailable in the dictionary used for disambiguation.We have not attempted to optimize our system forWordNet, but rather emphasize use oflexicographically-based dictionaries.
KMS can useseveral dictionaries at the same time, and theadditional effort to disambiguate against severalsense inventories at the same time is not demanding.Our system?s performance was also degraded bya difficulty in advancing through the token list, sothat we did not return a sense for 305 items (some ofwhich were due to our parser?s performance).
Wealso did not deal properly with the adverbs (most ofwhich were adverbial phrases) and hyphenated words(which we learned about only after downloading thetest set).As indicated in Table 1, our system?sperformance was lowest for verbs.
We believe, basedon our earlier studies, that this lower score is affectedby the WordNet verb sense inventory.2 The SENSEVAL-3 Lexical Sample TaskDisambiguation for the lexical sample task is quitesimilar to that used for the all-words task.
The effortis somewhat easier in preparation, since the text foreach instance is generally in a form that has not beenpreprocessed to an extensive degree.
Each instance inthe test set generally consisted of a paragraph whichcould be processed immediately within KMS.
It wasonly necessary to modify KMS in a minor way torecognize and keep track of the target word to bedisambiguated.The major difference in the SENSEVAL-3 taskfrom SENSEVAL-2 is the sense inventory.
WordNet1.7.1 was used for nouns and adjective, whileWordsymth provided the verb senses.
As indicatedabove, we were able to use WordNet immediately.For the Wordsmyth sense inventory, we had tocreate a new dictionary with CL Research?s DIMAPdictionary maintenance software.
The Wordsmythdefinitions were very uncomplicated, and we wereable to create this dictionary quickly afterdownloading the task training data.
On the otherhand, the Wordsmyth data is not as rich as would befound in ordinary dictionaries, particularly themachine-readable versions of these dictionaries.Nonetheless, we analyzed the dictionary data toextract nuggets of information about each sense.
Thisincluded creation of synsets (as in WordNet),identification of the definition proper, creation ofexamples where provided, identification of ?clues?
(e.g., ?followed by ?to??
), identification of typicalsubjects and objects, and identification of a sense?stopical area.
We also used the online version ofWordsmyth to identify the transitivity of each sense.We ran our system first on the trial data andobtained the results shown in Table 2, essentiallyusing the identical disambiguation routines developedfor SENSEVAL-2.
We intended to use the trainingdata, not for use as in supervised systems, but toanalyze our results using methods we had establishedfor identifying factors significant in disambiguation(Litkowski, 2002).
We also briefly investigated thevalue of using (1) the topical area characterization ofpreceding sentences, (2) WordNet relations amongwords in the sentences (including the target), and (3)prepositions following the target in examples.
Ourinvestigations indicated that only negligible changeswould occur from these possibilities.Table 2.
Lexical Sample Recall (Training)Run Items Fine CoarseAdjectives 314 0.382 0.516Nouns 3593 0.490 0.561Verbs 3961 0.409 0.525Total 7868 0.445 0.541We compared the results from the training datawith our performance in SENSEVAL-2 (Litkowski,2001).
In all categories, the recall was considerablyimproved, on average about 0.15.
This suggests thatthe lexical sample task for SENSEVAL-3 is mucheasier.
The improvement was relatively greater forverbs, suggesting that the sense inventory forWordsmyth is much closer to what might be found inordinary dictionaries.As a result of these preliminary investigations,we did not further modify our system for the test run.Our results for the test data are shown in Table 3.
Asis clear, the results are nearly identical with the testdata.
These patterns also hold for the individuallexical items (not shown), where there is much morevariation in performance.
The major reason for thevariations appears to lie primarily in the ordering ofthe senses in the dictionaries.
In other words, thesense inventories provide little discriminatinginformation, with the result that sense selection isprimarily to the default first sense.
This indicates thatthe sense inventories do not reflect the frequencies inthe training and test data.Table 3.
Lexical Sample Recall (Test)Run Items Fine CoarseAdjectives 159 0.409 0.503Nouns 1806 0.488 0.576Verbs 1977 0.419 0.540Total 3942 0.450 0.5553 Disambiguation of WordNet GlossesThe SENSEVAL-3 task to disambiguate contentwords in WordNet glosses was a slight modificationof the all-words task.
One main difference was thattokens to be disambiguated were not identified,requiring the systems to identify content words andphrases.
Content words were considered to be any ofthe four major parts of speech, i.e., words or phrasesthat could be found in WordNet.
Another majordifference was that minimal context was provided,i.e., only the gloss itself (although examples werealso available).
The WordNet synset was also given,providing some ?context?
within the WordNetnetwork of synsets.This task had no training data, but only test databased on the tagging of content words by theeXtended WordNet (XWN) project (Mihalcea andMoldovan, 2001).
The test data consisted of only andall those glosses from WordNet for which one ormore word forms (a single word or a multiword unit)had received a ?gold?
quality WordNet senseassignment.
Scoring for this task is based only on asystem?s performance in assigning a sense to theseword forms.
The test set consisted of 9257 glossescontaining 15179 ?gold?
assignments (out of 42491word forms in these glosses).To perform this task1, we used KMS to processeach gloss (treated by KMS as a ?text?).
Each glosswas parsed and processed and converted into anXML representation.
(No gloss was a sentence, soeach parse was ?degenerate?
in that only sentencefragments were identified.
)KMS has only recently been modified toincorporate ?all-words?
disambiguation in the XMLrepresentation.
At present, the disambiguation hasonly been partially implemented.
One aspect still indevelopment is a determination of exactly whichitems in the representation should be given adisambiguation and represented (e.g., exactly how totreat multiword units or verbs with particles).
Also,we have not yet integrated the full disambiguationmachinery (as used in the all-words and lexicalsample tasks) into KMS.
As a result, only the first(or default) sense of a word is selected.CL Research?s DIMAP dictionary softwareincludes considerable functionality to parse andanalyze dictionary definitions.
Part of the analysisfunctionality makes use of WordNet relations inorder to propagate information to features associatedwith a sense.
CL Research has previously parsedWordNet glosses as part of an investigation into1Note that, although CL Research ran this task, and wehad access to the test data beforehand, we did notactually work with the data until the date indicated forother participants to download and work with the dataprior to submission.
In any event, our participation inthis task was primarily to investigate the parsing andprocessing of sentence fragments in KMS.WordNet?s internal consistency.
However, we did notincorporate any of this experience in performing thistask.
We also did not incorporate any routines thatmake use of WordNet relations for disambiguation(as enabled by identification of the WordNet synsetidentifier).
Determining the extent to which thesefunctionalities are relevant for KMS is a matter forfuture investigation.Our performance for this task reflects oursomewhat limited implementation, as shown in Table4.
Among 10 participating runs, our precision wasthe second lowest and our recall was the third lowest.We were only able to identify 76.8 percent of the testitems with our current implementation.
However, incomparing our results with our performance in theall-words and lexical sample tasks, the results hereare not significantly different.
Moreover, these resultssuggest a minimum that might be obtained with adisambiguation system that relies only on picking thefirst sense.Table 4.
Disambiguation of WordNet GlossesItems Precision Recall?Gold?
words 15179 0.449 0.3454 Automatic Labeling of Semantic RolesThe SENSEVAL-3 task to label sentenceconstituents with semantic roles was designed toreplicate the tagging and identification of frameelements performed in the FrameNet project (Johnsonet al, 2003).
This task was modeled on the study ofautomatic labeling by Gildea & Jurafsky (2002), toallow other participants to investigate methods forassigning semantic roles.
That study was based onFrameNet 1.0, whereas this task used data fromFrameNet 1.1, which considerably expanded thenumber of frames and the corpus sentences that weretagged by FrameNet lexicographers.The test data for this task consisted of 200sentences that had been labeled with frame elementsfor 40 different frames.
Participants were providedwith the sentences, the target word (along with itsbeginning and ending positions in the sentence), andthe frame name (i.e., no attempt was made todetermine the applicable frame).
Specific trainingdata for the task consisted of all sentences not in thetest set for the individual frame (ranging from slightlyfewer than 200 sentences to as many as 1500sentences).
In addition, participants could use theremainder of the FrameNet corpus for trainingpurposes (another 447 frames and nearly 133,000sentences).
Participants could submit two types ofruns: unrestricted (in which frame elementboundaries, but not frame element names, could beused, i.e., essentially a classification task) andrestricted (in which these boundaries could not beused, i.e., the more difficult task of segmentingconstituents and identifying their semantic role).
CLResearch submitted only one run, for the restrictedtask.To perform this task2, we used KMS to parse andprocess the sentences (where each sentence wastreated as a ?text?).
We made a slight modification toour system to enable to identify the applicable frameand to keep track of the target word.
We also createda special dictionary for FrameNet frames.
Thisdictionary was put into an XML file and consistedonly of the frame name, the frame elements, the typeof frame element (a classification used by FrameNetas ?core?, ?peripheral?, or ?extra-thematic?
), and acharacterization or ?definition?
of the frame element.?Definitions?
of frame elements were written asspecifications for the type of syntactic constituentthat was expected to instantiate a frame element in asentence.
Thus, for frames usually associated withverbs, a specification for a frame element might be?subject?
or ?object?.
More generally, many frameelements specified ?prepositional phrases?
headed byone of a set of prepositions (such as ?about?
or?with?).
The basic structure of the FrameNetdictionary was created automatically.
Thespecifications for each frame element was createdmanually after inspecting the training set for only the40 frames in the task (which we had processed toshow what frame elements had been identified for2Note that, again, although CL Research ran this task,and we had access to the test data beforehand, we didnot actually work with the data until the date indicatedfor other participants to download and work with thedata prior to submission.
We used only the trainingdata for development of our system.
Our participationin this task was exploratory in nature, designed toexamine the feasibility and issues involved inintegrating frame semantics into KMS.
This involvesdevelopment of processing routines and examination ofmethods for including frame elements in our XMLrepresentation.each sentence).To process the test data and create answers, wefirst parsed and processed each sentence with KMSto create an XML representation using the full set oftags and attributes normally generated.
Then, weused the applicable FrameNet ?definition?
for theframe, the XML representation of the sentence, andthe identification of the target word.
We iteratedthrough the frame elements and if we had aspecification for that element, we used thisspecification to create an XPath expression used toquery the XML representation of the sentence todetermine if the sentence contained a constituent ofthe desired type.
If a frame element was labeled as a?core?
element for the frame, but no constituent wasidentified, KMS treated this a ?null?
instantiation(i.e., a situation where linguistic principles allowframe elements to be omitted within a sentence).
Eachframe element identified in the sentence wasappended to a growing list and the full list wasreturned as the set of labeled semantic roles for thesentence.Our results for this task are shown in Table 5.Precision and recall reflect standard measures of howwell we were able to identify frame elements.
Thelow recall is a reflection of the small percentage ofitems attempted.
The overlap indicates how well wewere able to identify the beginning and endingpositions of the constituents we identified.Table 5.
Automatic Labeling of Semantic RolesItems Precision Overlap Recall Attempted16279 0.583 0.480 0.111 19.0Our poor results stem in large part from only acursory development of our FrameNet dictionary.
Weonly created substantial entries for 16 of the 40frames, minimal entries for another 11, and nodetailed specifications at all for the remaining 13.The minimal entries were created on the basis offrame elements with the same name (such as time,manner, and duration), which appear in more thanone frame.
In addition, our method of specification isstill somewhat limiting.
For example, in framesassociated with both nouns and verbs, our methodonly permitted us to specify the subject or object fora verb and not also a prepositional phrase followinga noun.
Another deficiency of our system was seen incases where a long constituent (such as a noun phrasewith multiple attached prepositional phrases) wasrequired.
Notwithstanding, with only a limited timefor development, we able to obtain substantialresults, suggesting that simple methods may plausiblybe used for a large percentage of cases.It appears that most participants in this task usedstatistical methods in training their systems andachieved results better than those obtained by Gildea& Jurafsky.
It is possible that these improved resultsstem from the much larger corpus available inFrameNet 1.1.
These results suggest the possibilitythat it may be feasible and more appropriate toinclude statistical bases for identifying frameelements in KMS.ConclusionsIn participating in four tasks of SENSEVAL-3, weexamined several aspects of disambiguation withinthe framework of massive tagging of text withsyntactic, semantic, and discourse characterizationsand attributes.
We established basic mechanisms forintegrating disambiguation and representationalprocedures into a larger text processing and analysissystem.
Our results further demonstrated difficultiesin using the WordNet sense inventory, but havefurther illuminated a number of important issues indisambiguation and representation.
At the same time,we have identified a significant number ofshortcomings in our system, but with considerableopportunities for further refinement and development.ReferencesGildea, Daniel, and Daniel Jurafsky.
Automatic Labelingof Semantic Roles.
Computational Linguistics, 28 (3),245-288.Johnson, Christopher; Miriam Petruck, Collin Baker,Michael Ellsworth, Josef Ruppenhofer, and CharlesFillmore, (2003).
FrameNet: Theory and Practice.Berkeley, California.Litkowski, K. C. (2001, 5-6 July).
Use of Machine-Readable Dictionaries for Word-Sense Disambiguationin SENSEVAL-2.
Proceedings of SENSEVAL-2: 2ndInternational Workshop on Evaluating Word SenseDisambiguation Systems.
Toulouse, France, pp.
107-110.Litkowski, K. C. (2002, 11 July).
Sense Information forDisambiguation: Confluence of Supervised andUnsupervised Methods.
Word Sense Disambiguation:Recent Successes and Future Directions.
Philadelphia,PA, pp.
47-53.Litkowski, Kenneth.
C. (2004a).
Use of Metadata forQuestion Answering and Novelty Tasks.
In E. M.Voorhees & L. P. Buckland (eds.
), The Twelfth TextRetrieval Conference (TREC 2003).
(In press.
)Litkowski, Kenneth.
C. (2004b).
SummarizationExperiments in DUC 2004.
(In press.
)Mihalcea, Rada and Dan Moldovan.
(2001).
EXtendedWordNet: Progress Report.
In: WordNet and OtherLexical Resources: Applications, Extensions, andCustomizations.
NAACL 2001 SIGLEX Workshop.Pittsburgh, PA.: Association for ComputationalLinguistics.
