Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 36?44,Dublin, Ireland, August 23-24, 2014.SemEval-2014 Task 5: L2 Writing AssistantMaarten van Gompel, Iris Hendrickx,Antal van den BoschCentre for Language Studies,Radboud University Nijmegen,The Netherlandsproycon@anaproy.nl,i.hendrickx@let.ru.nl,a.vandenbosch@let.ru.nlEls Lefever and V?eronique HosteLT3,Language and Translation Technology Team,Ghent University,Belgiumels.lefever@ugent.be,veronique.hoste@ugent.beAbstractWe present a new cross-lingual task forSemEval concerning the translation ofL1 fragments in an L2 context.
Thetask is at the boundary of Cross-LingualWord Sense Disambiguation and MachineTranslation.
It finds its application in thefield of computer-assisted translation, par-ticularly in the context of second languagelearning.
Translating L1 fragments in anL2 context allows language learners whenwriting in a target language (L2) to fallback to their native language (L1) when-ever they are uncertain of the right wordor phrase.1 IntroductionWe present a new cross-lingual and application-oriented task for SemEval that is situated in thearea where Word Sense Disambiguation and Ma-chine Translation meet.
Finding the proper trans-lation of a word or phrase in a given context ismuch like the problem of disambiguating betweenmultiple senses.In this task participants are asked to build atranslation/writing assistance system that trans-lates specifically marked L1 fragments in an L2context to their proper L2 translation.
This typeof translation can be applied in writing assistancesystems for language learners in which users writein a target language, but are allowed to occasion-ally back off to their native L1 when they are un-certain of the proper lexical or grammatical formin L2.
The task concerns the NLP back-end ratherthan any user interface.Full-on machine translation typically concernsthe translation of complete sentences or texts fromThis work is licensed under a CreativeCommons Attribution 4.0 International Licence:http://creativecommons.org/licenses/by/4.0/L1 to L2.
This task, in contrast, focuses on smallerfragments, side-tracking the problem of full wordreordering.We focus on the following language combi-nations of L1 and L2 pairs: English-German,English-Spanish, French-English and Dutch-English.
Task participants could participate for alllanguage pairs or any subset thereof.2 Task DescriptionWe frame the task in the context of second lan-guage learning, yielding a specific practical appli-cation.Participants build a translation assistance sys-tem rather than a full machine translation system.The L1 expression, a word or phrase, is translatedby the system to L2, given the L2 context alreadypresent, including right-side context if available.The aim here, as in all translation, is to carry thesemantics of the L1 fragment over to L2 and findthe most suitable L2 expression given the alreadypresent L2 context.Other than a limit on length (6 words), we donot pose explicit constraints on the kinds of L1fragments allowed.
The number of L1 fragmentsis limited to one fragment per sentence.The task addresses both a core problem ofWSD, with cross-lingual context, and a sub-problem of Phrase-based Statistical MachineTranslation; that of finding the most suitable trans-lation of a word or phrase.
In MT this would bemodelled by the translation model.
In our taskthe full complexity of full-sentential translationis bypassed, putting the emphasis on the seman-tic aspect of translation.
Our task has specificpractical applications and a specific intended au-dience, namely intermediate and advanced secondlanguage learners, whom one generally wants toencourage to use their target language as much aspossible, but who may often feel the need to fallback to their native language.36Currently, language learners are forced to fallback to a bilingual dictionary when in doubt.
Suchdictionaries do not take the L2 context into ac-count and are generally more constrained to singlewords or short expressions.
The proposed applica-tion would allow more flexible context-dependentlookups as writing progresses.
The task tests howeffectively participating systems accomplish this.The following examples illustrate the task forthe four language pairs we offer:?
Input (L1=English,L2=Spanish): ?Todo ello,in accordance con los principios que siemprehemos apoyado.
?Desired output: ?Todo ello, de conformidadcon los principios que siempre hemos apoy-ado.??
Input (L1-English, L2=German): ?Das,was wir heute machen, is essentially ein?Argernis.
?Desired output: ?Das, was wir heute machen,ist im Grunde genommen ein?Argernis.??
Input (L1=French,L2=English): ?I rentre `ala maison because I am tired.
?Desired output: ?I return home because I amtired.??
Input (L1=Dutch, L2=English): ?Workersare facing a massive aanval op their employ-ment and social rights.
?Desired output: ?Workers are facing a mas-sive attack on their employment and socialrights.
?The task can be related to two tasks that wereoffered in previous years of SemEval: LexicalSubstitution (Mihalcea et al., 2010) and most no-tably Cross-lingual Word Sense Disambiguation(Lefever and Hoste, 2013).When comparing our task to the Cross-LingualWord-Sense Disambiguation task, one notable dif-ference is the fact that our task concerns not justwords, but also phrases.
Another essential differ-ence is the nature of the context; our context is inL2 instead of L1.
Unlike the Cross-Lingual WordSense Disambiguation task, we do not constrainthe L1 words or phrases that may be used for trans-lation, except for a maximum length which we setto 6 tokens, whereas Lefever and Hoste (2013)only tested a select number of nouns.
Our taskemphasizes a correct meaning-preserving choiceof words in which translations have to fit in theL2 context.
There is thus a clear morphosyntacticaspect to the task, although less prominent thanin full machine translation, as the remainder ofthe sentence, already in L2, does not need to bechanged.
In the Cross-Lingual Word Sense Dis-ambiguation tasks, the translations/senses werelemmatised.
We deliberately chose a different paththat allows for the envisioned application to func-tion directly as a translation assistance system.A pilot study was conducted to test the feasibil-ity of the proposed translation system (van Gom-pel and van den Bosch, 2014).
It shows that L2context information can be a useful cue in transla-tion of L1 fragments to L2, improving over a non-context-informed baseline.3 DataWe did not provide training data for this task, aswe did not want to bias participating systems byfavouring a particular sort of material and method-ology.
Moreover, it would be a prohibitively largetask to manually collect enough training data ofthe task itself.
Participants were therefore free touse any suitable training material such as parallelcorpora, wordnets, or bilingual lexica.Trial and test data has been collected for thetask, both delivered in a simple XML format thatexplicitly marks the fragments.
System output ofparticipants adheres to the same format.
The trialset, released early on in the task, was used by par-ticipants to develop and tune their systems on.
Thetest set corresponds to the final data released forthe evaluation period; the final evaluation was con-ducted on this data.The trial data was constructed in an automatedfashion in the way described in our pilot study(van Gompel and van den Bosch, 2014).
First aphrase-translation table is constructed from a par-allel corpus.
We used the Europarl parallel corpus(Koehn, 2005) and the Moses tools (Koehn et al.,2007), which in turn makes use of GIZA++ (Ochand Ney, 2000).
Only strong phrase pairs (ex-ceeding a set threshold) were retained and weakerones were pruned.
This phrase-translation tablewas then used to create input sentences in whichthe L2 fragments are swapped for their L1 coun-terparts, effectively mimicking a fall-back to L1 inan L2 context.
The full L2 sentence acts as refer-ence sentence.
Finally, to ensure all fragments arecorrect and sensible, a manual selection from this37automatically generated corpus constituted the fi-nal trial set.In our pilot study, such a data set, even with-out the manual selection stage, proved adequate todemonstrate the feasibility of translating L1 frag-ments in an L2 context (van Gompel and van denBosch, 2014).
One can, however, rightfully arguewhether such data is sufficiently representative forthe task and whether it would adequately cover in-stances where L2 language learners might experi-ence difficulties and be inclined to fall back to L1.We therefore created a more representative test setfor the task.The actual test set conforms to much morestringent constraints and was composed entirelyby hand from a wide variety of written sources.Amongst these sources are study books and gram-mar books for language learners, short bilingualon-line stories aimed at language learners, gap-exercises and cloze tests, and contemporary writ-ten resources such as newspapers, novels, andWikipedia.
We aimed for actual learner corpora,but finding suitable learner corpora with sufficientdata proved hard.
For German we could use thethe Merlin corpus (Abel et al., 2013).
In example(a) we see a real example of a fragment in a fall-back language in an L2 context from the Merlincorpus.
(a) Input: Das Klima hier ist Tropical und wir haben fastkeinen WinterReference: Das Klima hier ist tropisch und wir habenfast keinen Winter.For various sources bilingual data was avail-able.
For the ones that were monolingual (L2)we resorted to manual translation.
To ensure ourtranslations were correct, these were later indepen-dently verified, and where necessary corrected bynative speakers.A large portion of the test set comes from off-line resources because we wanted to make surethat a substantial portion of the test set could notbe found verbatim on-line.
This was done to pre-vent systems from solving the actual problem byjust attempting to just look up the sources throughthe available context information.Note that in general we aimed for the Europeanvarieties of the different languages.
However, forEnglish we did add the US spelling variants as al-ternatives.
A complete list of all sources used inestablishing the test set is available on our web-site1.We created a trial set and test set/gold standardof 500 sentence pairs per language pair.
Due tothe detection of some errors at a later stage, someof which were caused by the tokenisation pro-cess, we were forced to remove some sentencesfrom the test set and found ourselves slightly be-low our aim for some of the language pairs.
Thetest set was delivered in both tokenised2and unto-kenised form.
The trial set was delivered only intokenised form.
Evaluation was conducted againstthe tokenised version, but our evaluation scriptwas designed to be as lenient as possible regard-ing differences in tokenisation.
We explicitly tookcases into account where participant?s tokeniserssplit contractions (such as Spanish ?del?
to ?de?+ ?el?
), whereas our tokeniser did not.For a given input fragment, it may well be possi-ble that there are multiple correct translations pos-sible.
In establishing our test set, we therefore paidspecial attention to adding alternatives.
To ensureno alternatives were missed, all participant outputwas aggregated in one set, effectively anonymis-ing the systems, and valid but previously missedalternatives were added to the gold standard.4 EvaluationSeveral metrics are available for automatic eval-uation.
First, we measure the absolute accuracya = c/n, where c is the number of fragmenttranslations from the system output that preciselymatch the corresponding fragments in the refer-ence translation, and n is the total number of trans-latable fragments, including those for which notranslation was found.
We also introduce a word-based accuracy, which unlike the absolute accu-racy gives some credits to mismatches that showpartial overlap with the reference translation.
It as-signs a score according to the longest consecutivematching substring between output fragment andreference fragment and is computed as follows:wac =|longestsubmatch(output, reference)|max(|output|, |reference|)(1)The system with the highest word-based accu-racy wins the competition.
All matching is case-sensitive.1https://github.com/proycon/semeval2014task52Using ucto, available at https://github.com/proycon/ucto38Systems may decide not to translate fragmentsif they cannot find a suitable translation.
A recallmetric simply measures the number of fragmentsfor which the system generated a translation, re-gardless of whether that translation is correct ornot, as a proportion of the total number of frag-ments.In addition to these task-specific metrics, stan-dard MT metrics such as BLEU, NIST, METEORand error rates such as WER, PER and TER, areincluded in the evaluation script as well.
Scoressuch as BLEU will generally be high (> 0.95)when computed on the full sentence, as a largeportion of the sentence is already translated andonly a specific fragment remains to be evaluated.Nevertheless, these generic metrics are proven inour pilot study to follow the same trend as themore task-specific evaluation metrics, and will beomitted in the result section for brevity.It regularly occurs that multiple translations arepossible.
As stated, in the creation of the test setwe have taken this into account by explicitly en-coding valid alternatives.
A match with any alter-native in the reference counts as a valid match.
Forword accuracy, the highest word accuracy amongstall possible alternatives in the reference is taken.Likewise, participant system output may containmultiple alternatives as well, as we allowed twodifferent types of runs, following the example ofthe Cross-Lingual Lexical Substitution and Cross-Lingual Word Sense Disambiguation tasks:?
Best - The system may only output one, itsbest, translation;?
Out of Five - The system may output upto five alternatives, effectively allowing 5guesses.
Only the best match is counted.
Thismetric does not count how many of the fiveare valid.Participants could submit up to three runs perlanguage pair and evaluation type.5 ParticipantsSix teams submitted systems, three of which par-ticipated for all language pairs.
In alphabetic or-der, these are:1.
CNRC - Cyril Goutte, Michel Simard, Ma-rine Carpuat - National Research CouncilCanada ?
All language pairs2.
IUCL - Alex Rudnick, Liu Can, Levi King,Sandra K?ubler, Markus Dickinson - IndianaUniversity (US) ?
all language pairs3.
UEdin - Eva Hasler - University of Ed-inburgh (UK) ?
all language pairs exceptEnglish-German4.
UNAL - Sergio Jim?enez, Emilio Silva - Uni-versidad Nacional de Colombia ?
English-Spanish5.
Sensible - Liling Tan - Universit?at des Saar-landes (Germany) and Nanyang Technolog-ical University (Singapore) ?
all languagepairs6.
TeamZ - Anubhav Gupta - Universit?e deFranche-Comt?e (France) ?
English-Spanish,English-GermanParticipants implemented distinct methodolo-gies and implementations.
One obvious avenue oftackling the problem is through standard Statisti-cal Machine Translation (SMT).
The CNRC teamtakes a pure SMT approach with few modifica-tions.
They employ their own Portage decoder anddirectly send an L1 fragment in an L2 context, cor-responding to a partial translation hypothesis withonly one fragment left to decode, to their decoder(Goutte et al., 2014).
The UEdin team applies asimilar method using the Moses decoder, markingthe L2 context so that the decoder leaves this con-text as is.
In addition they add a context similarityfeature for every phrase pair in the phrase transla-tion table, which expresses topical similarity withthe test context.
In order to properly decode, thephrase table is filtered per test sentence (Hasler,2014).
The IUCL and UNAL teams do make useof the information from word alignments or phrasetranslation tables, but do not use a standard SMTdecoder.
The IUCL system combines various in-formation sources in a log-linear model: phrasetable, L2 Language Model, Multilingual Dictio-nary, and a dependency-based collocation model,although this latter source was not finished in timefor the system submission (Rudnick et al., 2014).The UNAL system extracts syntactic features as ameans to relate L1 fragments with L2 context totheir L2 fragment translations, and uses memory-based classifiers to achieve this (Silva-Schlenkeret al., 2014).
The two systems on the lower end ofthe result spectrum use different techniques alto-gether.
The Sensible team approaches the problem39by attempting to emulate the manual post-editingprocess human translators employ to correct MToutput (Tan et al., 2014), whereas TeamZ relies onWiktionary as the sole source (Gupta, 2014).6 ResultsThe results of the six participating teams can beviewed in consensed form in Table 1.
This tableshows the highest word accuracy achieved by theparticipants, in which multiple system runs havebeen aggregated.
A ranking can quickly be dis-tilled from this, as the best score is marked inbold.
The system by the University of Edinburghemerges as the clear winner of the task.
The fullresults of the various system runs by the six par-ticipants are shown in Tables 2 and 3, two pagesdown, all three aforementioned evaluation metricsare reported there and the systems are sorted byword accuracy per language pair and evaluationtype.Team en-es oof en-de oofCNRC 0.745 0.887 0.717 0.868IUCL 0.720 0.847 0.722 0.857UEdin 0.827 0.949 - -UNAL 0.809 0.880 - -Sensible 0.351 0.231 0.233 0.306TeamZ 0.333 0.386 0.293 0.385fr-en oof nl-en oofCNRC 0.694 0.839 0.610 0.723IUCL 0.682 0.800 0.679 0.753UEdin 0.824 0.939 0.692 0.811UNAL - - - -Sensible 0.116 0.14 0.152 0.171TeamZ - - - -Table 1: Highest word accuracy per team, per lan-guage pair, and per evaluation type (out-of-five isinclude in the ?oof?
column).
The best score ineach column is marked in bold.For the lowest-ranking participants, the score isnegatively impacted by the low recall; their sys-tems could not find translations for a large numberof fragments.Figures 1 (next page) and 2 (last page) show theresults for the best evaluation type for each sys-tem run.
Three bars are shown; from left to rightthese represent accuracy (blue), word-accuracy(green) and recall (red).
Graphs for out-of-fiveevaluation were omitted for brevity, but tend to fol-low the same trend with scores that are somewhathigher.
These scores can be viewed on the resultwebsite at http://github.com/proycon/semeval2014task5/.
The result website alsoholds the system output and evaluation scripts withwhich all graphs and tables can be reproduced.We observe that the best scoring team in thetask (UEdin), as well as the CNRC team, both em-ploy standard Statistical Machine Translation andachieve high results.
From this we can concludethat standard SMT techniques are suitable for thistask.
Teams IUCL and UNAL achieve similarlygood results, building on word and phrase align-ment data as does SMT, yet not using a traditionalSMT decoder.
TeamZ and Sensible, the two sys-tems ranked lowest do not rely on any techniquesfrom SMT.
To what extent the context-informedmeasures of the various participants are effectivecan not be judged from this comparison, but canonly be assessed in comparison to their own base-lines.
For this we refer to the system papers of theparticipants.7 DiscussionWe did not specify any training data for the task.The advantage of this is that participants were freeto build a wider variety of systems from varioussources, rather than introducing a bias towards forinstances statistical systems.
The disadvantage,however, is that a comparison of the various sys-tems does not yield conclusive results regardingthe merit of their methodologies.
Discrepanciesmight at least be partly due to differences in train-ing data, as it is generally well understood in MTthat more training data improves results.
The base-lines various participants describe in their systempapers provide more insight to the merit of theirapproaches than a comparison between them.In the creation of the test set, we aimed to mimicintermediate to high-level language learners.
Wealso aimed at a fair distribution of different part-of-speech categories and phrasal length.
The dif-ficulty of the task differs between language pairs,though not intentionally so.
We observe that theDutch-English set is the hardest and the Spanish-English is the easiest in the task.
One of the par-ticipants implicitly observes this through measure-ment of the number of Out-of-Vocabulary words(Goutte et al., 2014).
This implies that when com-paring system performance between different lan-guage pairs, one can not simply ascribe a lowerresult to a system having more difficulty with said40Figure 1: English to Spanish (top), English to German (middle) and French to English (bottom).
Thethree bars, left-to-right, represent Accuracy (blue), Word Accuracy (green) and Recall (red).41System Acc W.Acc.
RecallEnglish-Spanish (best)UEdin-run2 0.755 0.827 1.0UEdin-run1 0.753 0.827 1.0UEdin-run3 0.745 0.82 1.0UNAL-run2 0.733 0.809 0.994UNAL-run1 0.721 0.794 0.994CNRC-run1 0.667 0.745 1.0CNRC-run2 0.651 0.735 1.0IUCL-run1 0.633 0.72 1.0IUCL-run2 0.633 0.72 1.0Sensible-wtmxlingyu 0.239 0.351 0.819TeamZ-run1 0.223 0.333 0.751Sensible-wtm 0.145 0.175 0.470Sensible-wtmxling 0.141 0.171 0.470English-Spanish (out-of-five)UEdin-run3 0.928 0.949 1.0UEdin-run1 0.924 0.946 1.0UEdin-run2 0.92 0.944 1.0CNRC-run1 0.843 0.887 1.0CNRC-run2 0.837 0.884 1.0UNAL-run1 0.823 0.88 0.994IUCL-run1 0.781 0.847 1.0IUCL-run2 0.781 0.847 1.0Sensible-wtmxlingyu 0.263 0.416 0.819TeamZ-run1 0.277 0.386 0.751Sensible-wtm 0.173 0.231 0.470Sensible-wtmxling 0.169 0.228 0.470English-German (best)IUCL-run2 0.665 0.722 1.0CNRC-run1 0.657 0.717 1.0CNRC-run2 0.645 0.702 1.0TeamZ-run1 0.218 0.293 0.852IUCL-run1 0.198 0.252 1.0Sensible-wtmxlingyu 0.162 0.233 0.878Sensible-wtm 0.16 0.184 0.647Sensible-wtmxling 0.152 0.178 0.647English-German (out-of-five)CNRC-run1 0.834 0.868 1.0CNRC-run2 0.828 0.865 1.0IUCL-run2 0.806 0.857 1.0TeamZ-run1 0.307 0.385 0.852IUCL-run1 0.228 0.317 1.0Sensible-wtmxlingyu 0.18 0.306 0.878Sensible-wtm 0.182 0.256 0.647Sensible-wtmxling 0.174 0.25 0.647Table 2: Full results for English-Spanish andEnglish-German.language pair.
This could rather be an intrinsicproperty of the test set or the distance between thelanguages.Distance in syntactic structure between lan-guages also defines the limits of this task.
Dur-ing composition of the test set it became clear thatbacking off to L1 was not always possible whensyntax diverged to much.
An example of this isseparable verbs in Dutch and German.
Considerthe German sentence ?Er ruft seine Mutter an?
(translation: ?He calls his mother?).
ImagineSystem Acc W.Acc.
RecallFrench-English (best)UEdin-run1 0.733 0.824 1.0UEdin-run2 0.731 0.821 1.0UEdin-run3 0.723 0.816 1.0CNRC-run1 0.556 0.694 1.0CNRC-run2 0.533 0.686 1.0IUCL-run1 0.545 0.682 1.0IUCL-run2 0.545 0.682 1.0Sensible-wtmxlingyu 0.081 0.116 0.321Sensible-wtm 0.055 0.067 0.210Sensible-wtmxling 0.055 0.067 0.210French-English (out-of-five)UEdin-run2 0.909 0.939 1.0UEdin-run1 0.905 0.938 1.0UEdin-run3 0.907 0.937 1.0CNRC-run1 0.739 0.839 1.0CNRC-run2 0.731 0.834 1.0IUCL-run1 0.691 0.8 1.0IUCL-run2 0.691 0.8 1.0Sensible-wtmxlingyu 0.085 0.14 0.321Sensible-wtmxling 0.061 0.09 0.210Sensible-wtm 0.061 0.089 0.210Dutch-English (best)UEdin-run1 0.575 0.692 1.0UEdin-run2 0.567 0.688 1.0UEdin-run3 0.565 0.688 1.0IUCL-run1 0.544 0.679 1.0IUCL-run2 0.544 0.679 1.0CNRC-run1 0.45 0.61 1.0CNRC-run2 0.444 0.609 1.0Sensible-wtmxlingyu 0.115 0.152 0.335Sensible-wtm 0.092 0.099 0.214Sensible-wtmxling 0.088 0.095 0.214Dutch-English (out-of-five)UEdin-run1 0.733 0.811 1.0UEdin-run3 0.727 0.808 1.0UEdin-run2 0.725 0.808 1.0IUCL-run1 0.634 0.753 1.0IUCL-run2 0.634 0.753 1.0CNRC-run1 0.606 0.723 1.0CNRC-run2 0.602 0.721 1.0Sensible-wtmxlingyu 0.123 0.171 0.335Sensible-wtm 0.099 0.115 0.214Sensible-wtmxling 0.096 0.112 0.214Table 3: Full results for French-English andDutch-English.a German language learner wanting to composesuch a sentence but wanting to fall back to En-glish for the verb ?to call?, which would translateto German as ?anrufen?.
The possible input sen-tence may still be easy to construe: ?Er calls seineMutter?, but the solution to this problem wouldrequire insertion at two different points, whereasthe task currently only deals with a substitution ofa single fragment.
The reverse is arguably evenmore complex and may stray too far from whata language learner may do.
Consider an Englishlanguage learner wanting to fall back to her na-42tive German, struggling with the English transla-tion for ?anrufen?.
She may compose a sentencesuch as ?He ruft his mother an?, which wouldrequire translating two dependent fragments intoone.We already have interesting examples in thegold standard, such as example (b), showing syn-tactic word-order changes confined to a singlefragment.
(b) Input: I always wanted iemand te zijn , but now Irealize I should have been more specific.Reference: I always wanted to be somebody , butnow I realize I should have been more specific.Participant output (aggregated): to be a person; it tobe; someone to his; to be somebody; person to be;someone to; someone to be; to be anybody; to anyone;to be someone; a person to have any; to be someoneelseAnother question we can ask, but have not in-vestigated, is whether a language learner wouldinsert the proper morphosyntactic form of an L1word given the L2 context, or whether she maybe inclined to fall back to a normal form suchas an infinitive.
Especially in the above case ofseparable verbs someone may be more inclined tocircumvent the double fragments and provide theinput: ?He anrufen his mother?, but in simplercases the same issue arises as well.
Consider anEnglish learner falling back to her native Croatian,a Slavic language which heavily declines nouns.If she did not know the English word ?book?
andwanted to write ?He gave the book to him?, shecould use either the Croatian word ?knjigu?
in itsaccusative declension or fall back to the normalform ?knjiga?.
A proper writing assistant systemwould have to account for both options.We can analyse which of the sentences in thetest data participants struggled with most.
Firstwe look at the number of sentences that producean average word accuracy of zero, measured persentence over all systems and runs in the out-of-five metric.
This means no participant was closeto the correct output.
There were 6 such sentencesin English-Spanish, 17 in English-German, 6 inFrench-English, and 32 in Dutch-English.A particularly difficult context from the Span-ish set is when a subjunctive verb form was re-quired, but an indicative verb form was submit-ted by the systems, such as in the sentence: ?Es-pero que los frenos del coche funcionen bien.
?.Though this may be deduced from context (theword ?Espero?, expressing hope yet doubt, be-ing key here), it is often subtle and hard to cap-ture.
Another problematic case that recurs in theGerman and Dutch data sets is compound nouns.The English fragment ?work motivation?
shouldtranslate into the German compound ?Arbeitsmo-tivation?
or ?Arbeitsmoral?, yet participants werenot able to find the actual compound noun.
Besidecompound nouns, other less frequent multi-wordexpressions are also amongst the difficult cases.Sparsity or complete absence in training data ofthese expressions is why systems struggle here.Another point of discussion is the fact that weenriched the test set by adding previously unavail-able alternative translations from an aggregatedpool of system output.
This might draw criticismfor possibly introducing a bias, also consideringthe fact that the decision to include a particular al-ternative for a given context is not always straight-forward and at times subjective.
We, however,contend that this is the best way to ensure thatvalid system output is not discarded and reduce thenumber of false negatives.
The effect of this mea-sure has been an increase in (word) accuracy forall systems, without significant impact on ranking.8 ConclusionIn this SemEval task we showed that systems cantranslate L1 fragments in an L2 context, a taskthat finds application in computer-assisted trans-lation and computer-assisted language learning.The localised translation of a fragment in a cross-lingual context makes it a novel task in the field.Though the task has its limits, we argue for itspractical application in a language-learning set-ting: as a writing assistant and dictionary replace-ment.
Six contestants participated in the task,and used an ensemble of techniques from Statis-tical Machine Translation and Word Sense Disam-biguation.
Most of the task organizers?
time wentinto manually establishing a gold standard basedon a wide variety of sources, most aimed at lan-guage learners, for each of the four language pairsin the task.
We have been positively surprised bythe good results of the highest ranking systems.9 AcknowledgementsWe would like to thank Andreu van Hooft andSarah Schulz for their manual correction work,and Sean Banville, Geert Joris, Bernard De Clerck,Rogier Crijns, Adriane Boyd, Detmar Meurers,Guillermo Sanz Gallego and Nils Smeuninx forhelping us with the data collection.43Figure 2: Dutch to English.ReferencesAndrea Abel, Lionel Nicolas, Jirka Hana, Barbora?Stindlov?a, Katrin Wisniewski, Claudia Woldt, Det-mar Meurers, and Serhiy Bykh.
2013.
A trilinguallearner corpus illustrating european reference lev-els.
In Proceedings of the Learner Corpus ResearchConference, Bergen, Norway, 27-29 September.Cyril Goutte, Michel Simard, and Marine Carpuat.2014.
CNRC-TMT: Second language writing as-sistant system description.
In Proceedings of the8th International Workshop on Semantic Evaluation(SemEval-2014), Dublin, Ireland.Anubhav Gupta.
2014.
Team Z: Wiktionary as L2writing assistant.
In Proceedings of the 8th Interna-tional Workshop on Semantic Evaluation (SemEval-2014), Dublin, Ireland.Eva Hasler.
2014.
UEdin: Translating L1 phrases inL2 context using context-sensitive smt.
In Proceed-ings of the 8th International Workshop on SemanticEvaluation (SemEval-2014), Dublin, Ireland.Philipp Koehn, Hieu Hoang, Alexandra Birch,Chris Callison-Burch, Marcello Federico,Nicola Bertoldi, Brooke Cowan, Wade Shen,Christine Moran, Richard Zens, Chris Dyer,Ohttp://www.aclweb.org/anthology/P/P07/P072045ndrej Bojar, Alexandra Constantin, and EvanHerbst.
2007.
Moses: Open source toolkit forstatistical machine translation.
In Proceedings ofthe 45th Annual Meeting of the Association forComputational Linguistics Companion VolumeProceedings of the Demo and Poster Sessions,pages 177?180, Prague, Czech Republic, June.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In In Proceedingsof the Machine Translation Summit X ([MT]?05).,pages 79?86.Els Lefever and Veronique Hoste.
2013.
SemEval-2013 Task 10: Cross-Lingual Word Sense Disam-biguation.
In Proceedings of the 7th InternationalWorkshop on Semantic Evaluation (SemEval 2013),in conjunction with the Second Joint Conference onLexical and Computational Semantics.Rada Mihalcea, Ravi Sinha, and Diana McCarthy.2010.
Semeval 2010 task 2: Cross-lingual lex-ical substitution.
In Proceedings of the 5thInternational Workshop on Semantic Evaluations(SemEval-2010), Uppsala, Sweden.Franz Josef Och and Hermann Ney.
2000.
Giza++:Training of statistical translation models.
Technicalreport, RWTH Aachen, University of Technology.Alex Rudnick, Levi King, Can Liu, Markus Dickinson,and Sandra K?ubler.
2014.
IUCL: Combining infor-mation sources for semeval task 5.
In Proceedingsof the 8th International Workshop on Semantic Eval-uation (SemEval-2014), Dublin, Ireland.Emilio Silva-Schlenker, Sergio Jimenez, and Julia Ba-quero.
2014.
UNAL-NLP: Cross-lingual phrasesense disambiguation with syntactic dependencytrees.
In Proceedings of the 8th InternationalWorkshop on Semantic Evaluation (SemEval-2014),Dublin, Ireland.Liling Tan, Anne Schumann, Jos?e Martinez, and Fran-cis Bond.
2014.
Sensible: L2 translation assistanceby emulating the manual post-editing process.
InProceedings of the 8th International Workshop onSemantic Evaluation (SemEval-2014), Dublin, Ire-land.Maarten van Gompel and Antal van den Bosch.
2014.Translation assistance by translation of L1 frag-ments in an L2 context.
In To appear in Proceedingsof ACL 2014.44
