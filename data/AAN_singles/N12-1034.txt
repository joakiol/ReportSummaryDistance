2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 338?346,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsUsing paraphrases for improving first story detection in news and TwitterSas?a Petrovic?School of InformaticsUniversity of Edinburghsasa.petrovic@ed.ac.ukMiles OsborneSchool of InformaticsUniversity of Edinburghmiles@inf.ed.ac.ukVictor LavrenkoSchool of InformaticsUniversity of Edinburghvlavrenk@inf.ed.ac.ukAbstractFirst story detection (FSD) involves identify-ing first stories about events from a continuousstream of documents.
A major problem in thistask is the high degree of lexical variation indocuments which makes it very difficult to de-tect stories that talk about the same event butexpressed using different words.
We suggestusing paraphrases to alleviate this problem,making this the first work to use paraphrasesfor FSD.
We show a novel way of integrat-ing paraphrases with locality sensitive hashing(LSH) in order to obtain an efficient FSD sys-tem that can scale to very large datasets.
Oursystem achieves state-of-the-art results on thefirst story detection task, beating both the bestsupervised and unsupervised systems.
To testour approach on large data, we construct a cor-pus of events for Twitter, consisting of 50 mil-lion documents, and show that paraphrasing isalso beneficial in this domain.1 IntroductionFirst story detection (FSD), sometimes also callednew event detection (NED), is the task of detectingthe first story about a new event from a stream ofdocuments.
It began as one of the tasks in TopicDetection and Tracking (TDT) (Allan, 2002) wherethe overall goal of the project was to improve tech-nologies related to event-based information organi-zation tasks.
Of the five TDT tasks, first story de-tection is considered the most difficult one (Allanet al, 2000a).
A good FSD system would be veryuseful for business or intelligence analysts wheretimely discovery of events is crucial.
With the sig-nificant increase in the amount of information beingproduced and consumed every day, a crucial require-ment for a modern FSD system to be useful is effi-ciency.
This means that the system should be ableto work in a streaming setting where documents areconstantly coming in at a high rate, while still pro-ducing good results.
While previous work has ad-dressed the efficiency (Petrovic?
et al, 2010) aspect,there has been little work on improving FSD perfor-mance in the past few years.
A major obstacle is thehigh degree of lexical variation in documents thatcover the same event.
Here we address this problem,while keeping in mind the efficiency constraints.The problem of lexical variation plagues many IRand NLP tasks, and one way it has been addressedin the past is through the use of paraphrases.
Para-phrases are alternative ways of expressing the samemeaning in the same language.
For example, thephrase he got married can be paraphrased as he tiedthe knot.
Paraphrases were already shown to helpin a number of tasks: for machine translation totranslate unknown phrases by translating their para-phrases (Callison-Burch et al, 2006), for query ex-pansion in information retrieval (Spa?rck Jones andTait, 1984; Jones et al, 2006), or for improvingquestion answering (Riezler et al, 2007).
A muchmore detailed discussion on the use of paraphrasesand ways to extract them is given in (Madnani andDorr, 2010).
Here, we present the first work to useparaphrases for improving first story detection.
Us-ing paraphrases, we are able to detect that some doc-uments previously thought to be about new eventsare actually paraphrases of the documents already338seen.
Our approach is simple and we show a novelway of integrating paraphrases with locality sen-sitive hashing (LSH) (Indyk and Motwani, 1998).This way we obtain a very efficient FSD system withall the benefits of using paraphrases, while avoid-ing computationally expensive topic modeling ap-proaches such as Ahmed et al (2011).First story detection was introduced as a task be-fore the popularization of social media.
Event de-tection in social media, especially Twitter is a verygood fit: we cover a much larger set of events thanwould be possible by using newswire, and the sto-ries are reported in real time, often much soonerthan in news.
Of course, social media carries ad-ditional problems not found in traditional media: wehave to deal with huge amounts of data, the data isvery noisy (both due to spam and due to spellingand grammar errors), and in the case of Twitter, doc-uments are extremely short.
There has been littleeffort in solving these problems for FSD.
Arguablythe main reason for this is the lack of a TDT-stylecorpus for Twitter that researchers could use to testtheir approaches.
Here we build such a corpus anduse it to measure the performance of TDT systemson Twitter.Our main contributions are: i) we create a firstcorpus of events on Twitter, ii) we show how to useparaphrases in FSD, and how to combine it withLSH to handle high-volume streams, iii) our unsu-pervised system that uses paraphrases achieves thehighest reported results on the TDT5 corpus, beat-ing both the supervised and unsupervised state of theart, while still keeping a constant per-document timecomplexity, and iv) we show that paraphrases alsohelp in Twitter, although less than in TDT.2 Paraphrasing and FSD2.1 Current approaches to efficient FSDState-of-the-art FSD systems (Allan et al, 2000b)use a fairly simple approach.
Documents are repre-sented as TF-IDF weighted vectors, their distance ismeasured in terms of the cosine distance, and theyuse a k-nearest neighbors clustering algorithm, withk usually set to 1.
The novelty score for a documentis the cosine distance to the nearest neighbor:score(d) = 1?
maxd?
?Dtcos(d, d?).
(1)Dt is the set of all documents up to time t whendocument d arrived.Because the max in equation (1) takes O(|Dt|)time to compute in the worst case, Petrovic?
et al(2010) introduced a way of using locality sensitivehashing (LSH) to make this time O(1), while retain-ing the same accuracy level.
In particular, insteadof computing the max over the entire set Dt, likein (1), they compute it over a smaller set S of poten-tial nearest neighbors.
The set S is the set of docu-ments that collide with the current document undera certain type of hash function:S(x) = {y : hij(y) = hij(x), ?i ?
[1..L],?j ?
[1..k]},(2)where the hash functions hij are defined as:hij(x) = sgn(uTijx), (3)with the random vectors uij being drawn indepen-dently for each i and j.
The efficiency of this algo-rithm stems from the fact that it can be shown thatthe set S of potential nearest neighbors can be madeconstant in size, while still containing the nearestneighbor with high probability.2.2 ParaphrasesThere are several levels of paraphrasing ?
lexicalparaphrases, where the relationship is restricted toindividual lexical items, phrasal paraphrases, wherelonger phrases are considered, and sentential para-phrases, where entire sentences are in a paraphrasticrelationship.
Here we use the simplest form, lexi-cal paraphrases, but our approach, described in sec-tion 2.3, is general and it would be trivial to usephrasal paraphrases in the same way ?
we leave thisfor future work.We use three sources of paraphrases: Word-net (Fellbaum, 1998), a carefully curated lexicaldatabase of English containing synonym sets,Microsoft Research paraphrase tables (Quirk etal., 2004), a set of paraphrase pairs automaticallyextracted from news texts, and syntactically-constrained paraphrases from Callison-Burch(2008) which are extracted from parallel text.
Wealso considered using paraphrases from Cohn etal.
(2008), but using them provided only minorimprovement over the baseline model.
This is likelydue to the small size of that corpus (a total of 7339thousand pairs).
We do not show results for thisparaphrase corpus in our results section.Wordnet paraphrases contained 150 thousandword pairs extracted from Wordnet?s synsets, whereall the pairs of words within one synset were con-sidered to be paraphrases.
MSR paraphrases wereextracted from the phrase tables provided by MSR.Two words were considered paraphrases if they werealigned at least once in the most probable alignment,with the probability of both backward and forwardalignment of at least 0.2.
In our initial experimentswe varied this threshold and found it has little ef-fect on results.
Using this method, we extracted50 thousand paraphrase pairs.
Finally, we use themethod of Callison-Burch (2008) to extract syntac-tically constrained paraphrases from a parallel cor-pus.
This method requires that phrases and theirparaphrases be the same syntactic type, and has beenshown to substantially improve the quality of ex-tracted paraphrases (Callison-Burch, 2008).
We ex-tracted paraphrases for all the words that appearedin the MSR paraphrase corpus, and then kept all thepairs that had the paraphrase probability of at least0.2.
This way, we extracted 48 thousand pairs.
Allthree resources we use are very different: they comefrom different domains (news text, legal text, gen-eral English), and they have very little overlap (lessthan 5% of pairs are shared by any two resources).2.3 Efficient paraphrasing in FSDIn this section, we explain how to use paraphrasesin a first story detection system.
We account forparaphrases by changing how we compute the co-sine in equation (1).
Because the cosine measuredepends on the underlying inner product, we changethe way the inner product is computed.
We modelparaphrasing by using a binary word-to-word ma-trix of paraphrases Q.
An entry of 1 at row i andcolumn j in the matrix indicates that words i and jare paraphrases of each other.1 Note, however, thatour approach is not limited to using single words ?
ifthe document representation includes n-grams withn > 1, the matrix Q can contain phrases, and thuswe can capture non-compositional paraphrases like1This is of course a simplification ?
in general, one mightlike the entries in the matrix to be real numbers correspondingto the probability that the two words are paraphrases.
We leavethis for future work.he died ?
he kicked the bucket.
We use the matrixQ to define a new inner product space:2?x,y?Q = yTQx.
(4)This way of using paraphrases basically achieves ex-pansion of the terms in documents with their para-phrases.
Thus, if two documents have no terms incommon, but one has the term explosion and theother has the term blast, by knowing that the twoterms are paraphrases, their similarity will be differ-ent from zero, which would have been the case if noparaphrasing was used.
Alternatively, the new innerproduct in equation (4) can also be seen as introduc-ing a linear kernel.One problem with using Q as defined in (4) is thatit is not very suitable for use in an online setting.In particular, if documents come in one at a timeand we have to store each one, only for it to be re-trieved at some later point, simply storing them andcomputing the inner product as in (4) would lead tofrequent matrix-vector multiplications.
Even thoughQ is sparse, these multiplications become expen-sive when done often, as is the case in first storydetection.
We thus have to store a modified docu-ment vector x, call it x?, such that when we compute?x?,y??
we get ?x,y?Q.
Note that the inner productbetween x?
and y?
is computed in the original innerproduct space.
It is clear that by using:x?
= Q1/2x (5)we have achieved our goal: ?x?,y??
= y?Tx?
=(Q1/2y)T (Q1/2x) = (yTQ1/2T)(Q1/2x) =yTQx = ?x,y?Q.
Again, if we view equation (4)as defining a kernel, we can think of equation (5)as performing an explicit mapping into the featurespace defined by the kernel.
Because ours is a linearkernel, performing this mapping is fairly efficient.Unfortunately, the square root of the paraphrasingmatrix Q is in general dense (and can even containcomplex entries), which would make our approachinfeasible in practice because we would have to ex-pand every document with all (or a very large num-ber of) the words in the vocabulary.
Thus, we have to2Equation (4) does not define a proper inner product in thestrict technical sense because the positive definiteness propertydoes not hold.
However, because vectors x and y in practicealways have positive entries, equation (4) behaves like a properinner product for all practical purposes.340approximate Q1/2 with a sparse matrix, preferablyone that is as sparse as the original Q matrix.
To thisend, we introduce the following approximation:Q?1/2ij =Qij?
?k(Qik +Qkj)/2(6)To see how we arrive at this approximation, considerthe paraphrase matrix Q.
If there was no polysemyin the language, Q would be a block matrix, whereeach non-zero submatrix would correspond to a sin-gle meaning.
The square root of such a matrix wouldbe given exactly by (6).
While this approximationis somewhat simplistic, it has two major advantagesover the exact Q1/2: i) it is very easy to computeand, with proper implementation, takes O(n2) time,as opposed to O(n3) for Q1/2, making it scalable tovery large matrices, and ii) matrix Q?1/2 is guaran-teed to be as sparse as Q, whereas Q1/2 will in mostcases become dense, which would make it unusablein real applications.2.4 Locality-sensitive hashing withparaphrasingHere we explain how to integrate paraphrasing withefficient FSD, using LSH described in section 2.1.As we mentioned before, a single hash function hijin the original LSH scheme hashes the vector x to:h(x) = sgn(uTx), (7)where u is a (dense) random vector.
If we want touse paraphrases with LSH, we simply change thehash function toh1(x) = sgn(uT (Q?1/2x)).
(8)It is not difficult to show that by doing this, the LSHbounds for probability of collision hold in the newinner product space defined by the matrix Q. Weomit this proof due to space constraints.Space efficient LSH.
While LSH can significantlyreduce the running time, it is fairly expensivememory-wise.
This memory overhead is due to therandom vectors u being very large.
To solve thisproblem, (Van Durme and Lall, 2010) used a hash-ing trick for space-efficient storing of these vectors.They showed that it is possible to project the vectorsonto a much smaller random subspace, while still re-taining good properties of LSH.
They proposed thefollowing hash function for a vector x:h2(x) = sgn(uT (Ax)), (9)where A is a random binary matrix with exactly onenon-zero element in each column.
This approachguarantees a constant space use which is bounded bythe number of rows in the A matrix.
Here we showthat our paraphrasing approach can be easily usedtogether with this space-saving approach by defin-ing the following hash function for x:h3(x) = sgn(uT (AQ?1/2x)).
(10)This way we get the benefits of the hashing trick(the constant space use), while also being able to useparaphrases.
The hash function in (10) is the actualhash function we use in our system.
Together withthe heuristics from Petrovic?
et al (2010), it guaran-tees that our FSD system will use constant space andwill take constant time to process each document.3 Twitter Event Corpus3.1 Event detection on TwitterAs we mentioned before, research on event detec-tion in social media is hampered by the lack of acorpus that could be used to measure performance.The need for a standard corpus is evident from therelated work on event detection in Twitter.
For ex-ample, (Petrovic?
et al, 2010) address the scalingproblem in social media and present a system thatruns in constant time per document, but the evalua-tion of their system on Twitter data was limited tovery high-volume events.
The only attempt in creat-ing a corpus of events for Twitter that we are awareof was presented in Becker et al (2011).
Unfor-tunately, that corpus is not suitable for FSD eval-uation for two main reasons: i) the events werepicked from the highest-volume events identified bythe system (similar to what was done in Petrovic?
etal.
(2010)), introducing not only a bias towards high-volume events, but also a bias toward the kinds ofevents that their system can detect, and ii) the au-thors only considered tweets by users who set theirlocation to New York, which introduces a strong biastowards the type of events that can appear in the cor-pus.
While these problems were not relevant to the341work of (Becker et al, 2011) because the corpus wasonly used to compare different cluster representa-tion techniques, they would certainly pose a seriousproblem if we wanted to use the corpus to compareFSD systems.
In this paper we present a new corpusof tweets with labeled events by taking a very sim-ilar approach to that taken by NIST when creatingthe TDT corpora.3.2 Annotating the TweetsIn this section we describe the annotation process forour event corpus.
Note that due to Twitter?s terms ofservice, we distribute the corpus as a set of tweetIDs and the corresponding annotations ?
users willhave to crawl the tweets themselves, but this canbe easily done using any one of the freely availablecrawlers for Twitter.
This is the same method thatthe TREC microblog track3 used to distribute theirdata.
All our Twitter data was collected from thestreaming API4 and consists of tweets from begin-ning of July 2011 until mid-September 2011.
Afterremoving non-English tweets, our corpus consists of50 million tweets.In our annotation process, we have adopted theapproach used by the National Institute of Standardsand Technology (NIST) in labeling the data for TDTcompetitions.
First, we defined a set of events thatwe want to find in the data, thus avoiding the bias ofusing events that are the output of any particular sys-tem.
We choose the events from the set of importantevents for our time period according to Wikipedia.5Additionally, we used common knowledge of impor-tant events at that time to define more events.
Intotal, we define 27 events, with an average of 112on-topic tweets.
This is comparable to the first TDTcorpus which contained 25 events and average of 45on-topic documents.
However, in terms of the to-tal number of documents, our corpus is three ordersof magnitude larger than the first TDT corpus, andtwo orders of magnitude larger than the biggest TDTcorpus (TDT5).
Our corpus contains very differentevents, such as the death of Amy Winehouse, down-grading of US credit rating, increasing of US debtceiling, earthquake in Virginia, London riots, terror-ist attacks in Norway, Google announcing plans to3http://trec.nist.gov/data/tweets/4https://stream.twitter.com/5http://en.wikipedia.org/wiki/2011buy Motorola Mobility, etc.
The event with the moston-topic tweets had over 1,000 tweets (death of AmyWinehouse), and the smallest event had only 2 on-topic tweets (arrest of Goran Hadzic).We faced the same problems as NIST when label-ing the events ?
there were far too many stories to ac-tually read each one and decide which (if any) eventsit corresponds to.
In order to narrow down the setof candidates for each event, we use the same pro-cedure as used by NIST.
The annotator would firstread a description of the event, and from that de-scription compile a set of keywords to retrieve pos-sibly relevant tweets.
He would then read throughthis set, labeling each tweet as on- or off-topic, andalso adding new keywords for retrieving a new batchof tweets.
After labeling all the tweets in one batch,the newly added keywords were used to retrieve thenext batch, and this procedure was repeated until nonew keywords were added.
Unlike in TDT, however,when retrieving tweets matching a keyword, we donot search through the whole corpus, as this wouldreturn far too many candidates than is feasible to la-bel.
Instead, we limit the search to a time window ofone day around the time the event happened.Finally, the annotator guidelines contained someTwitter-specific instructions.
Links in tweets werenot taken into account (the annotator would not clickon links in the tweets), but retweets were (if theretweet was cut off because of the 140 characterlimit, the annotator would label the original tweet).Furthermore, hashtags were taken into account, sotweets like #Amywinehouseisdead were labeled asnormal sentences.
Also, to be labeled on-topic, thetweet would have to explicitly mention the event andthe annotator should be able to infer what happenedfrom the tweet alne, without any outside knowl-edge.
This means that tweets like Just heard aboutLokomotiv, this is a terrible summer for hockey!
areoff topic, even though they refer to the plane crashin which the Lokomotiv hockey team died.In total, our corpus contains over 50 milliontweets, of which 3035 tweets were labeled as be-ing on-topic for one of the 27 events.
While search-ing for first tweets (i.e., tweets that first mention anevent), fake first tweets were sometimes discovered.For example, in the case of the death of RichardBowes (victim of London riots), a Telegraph jour-nalist posted a tweet informing of the man?s death342more than 12 hours before he actually died.
Thistweet was later retracted by the journalist for beingincorrect, but the man then died a few hours later.Cases like this were labeled off-topic.4 Experiments4.1 EvaluationIn the official TDT evaluation, each FSD system isrequired to assign a score between 0 and 1 to ev-ery document upon its arrival.
Lower scores corre-spond to old stories, and vice versa.
Evaluation isthen carried out by first sorting all stories accord-ing to their scores and then performing a thresholdsweep.
For each value of the threshold, stories witha score above the threshold are considered new, andall others are considered old.
Therefore, for eachthreshold value, one can compute the probability ofa false alarm, i.e., probability of declaring a storynew when it is actually not, and the miss probability,i.e., probability of declaring a new story old (miss-ing a new story).
Using the false alarm and the missrate, the cost Cdet is defined as follows:Cdet = Cmiss?Pmiss?Ptarget+CFA?PFA?Pnon?target ,where Cmiss and CFA are costs of miss and falsealarm (0.02 and 0.98, respectively), Pmiss and PFAare the miss and false alarm rate, and Ptarget andPnon?target are the prior target and non-target prob-abilities.
Different FSD systems are compared onthe minimal cost Cmin , which is the minimal valueof Cdet over all threshold values.
This means that inFSD evaluation, a lower value of Cmin indicates abetter system.4.2 TDT resultsFor the TDT experiments, we use the English por-tion of TDT-5 dataset, consisting of 126 topics in278,108 documents.
Similar to (Petrovic?
et al,2010), we compare our approach to a state-of-the-art FSD system, namely the UMass system (Allan etal., 2000b).
This system always scored high in theTDT competitions and is known to perform at leastas well as other systems that also took part in thecompetition (Fiscus, 2001).
Our system is based onthe streaming FSD system of (Petrovic?
et al, 2010)which has a constant per-document time complex-ity.
We use stemming (Porter, 1980) and, the sameas (Petrovic?
et al, 2010), we use 13 bits per keyand 70 hash tables for LSH.
Additionally, we use thehashing trick described in section 2.4 with a pool ofsize 218.
Paraphrasing is implemented in this systemas described in section 2.4.While the UMass system was among the best sys-tems that took part in the TDT competitions, therehas been research in event detection since the com-petitions stopped.
Recent work on event detec-tion includes a hybrid clustering and topic modelwith rich features such as entities, time, and top-ics (Ahmed et al, 2011).
We do not compare oursystem to Ahmed et al (2011) because in terms ofthe numerical Cmin score, their approach does notoutperform the UMass system.
This is not surpris-ing as the primary goal in Ahmed et al (2011) wasnot to improve FSD performance, but rather to cre-ate storylines and support structured browsing.We compare our approach to the best reported re-sult in the literature on the TDT5 data.
To the best ofour knowledge, the highest reported results in FSDcome from a supervised system described in Ku-maran and Allan (2005).
This system uses an SVMclassifier with the features being FSD scores fromunsupervised systems (the authors used scores com-puted in the same way as is done in the UMass sys-tem) computed using i) full text, ii) only named en-tities in the document, and iii) only topic terms.
Theclassifier was trained on TDT3 and TDT4 corporaand tested on TDT5.Table 1 shows the results for TDT5 data.
UMass1000 is the run that was submitted as the officialrun in the TDT competition.6 We can see that us-ing paraphrases improves the results over the unsu-pervised state of the art, regardless of which sourceof paraphrasing is used.
However, it is clear thatnot all types of paraphrases are equally helpful.
Inparticular, the automatically extracted paraphrasesfrom Callison-Burch (2008) seem to be the mosthelpful, and by using them our unsupervised sys-tem is able to beat even the best known supervisedFSD system.
This is a very promising result becauseit indicates that we can use automatically extractedparaphrases and do not have to rely on hand-craftedresources like Wordnet as our source of paraphrases.6Our experiments, and experiments in Allan et al (2000b)showed that keeping full documents does not improve results,while increasing running time.343System CminUMass 100 0.721UMass 1000 0.706Best supervised system 0.661Wordnet 0.657MSR Paraphrases 0.642Syntactic paraphrases 0.575Table 1: TDT FSD results for different systems, lower isbetter.
The number next to UMass system indicates thenumber of features kept for each document (selected ac-cording to their TFIDF).
All paraphrasing systems workwith full documents.
Results for the best supervised sys-tem were taken from Kumaran and Allan (2005).The difference between our system and the UMasssystem is significant at p = 0.05 using a paired t-testover the individual topic costs.
We were not able totest significance against the supervised state-of-the-art because we did not have access to this system.
Interms of efficiency, our approach is still O(1), likethe approach in Petrovic?
et al (2010), but in practiceit is somewhat slower because hashing the expandeddocuments takes more time.
We measured the run-ning time of our system, and it is 3.5 times slowerthan the basic approach of Petrovic?
et al (2010), butalso 3.5 times faster than the UMass system, whileoutperforming both of these systems.How does quality of paraphrases affect results?We have shown that using automatically obtainedparaphrases to expand documents is beneficial infirst story detection.
Because there are differentways of extracting paraphrases, some of which aretargeted more towards recall, and some towards pre-cision, we want to know which techniques would bemore suitable to extract paraphrases for use in FSD.Here, precision is the ratio between extracted wordpairs that are actual paraphrases and all the wordpairs extracted, and recall is the ratio between ex-tracted word pairs that are actual paraphrases, andall the possible paraphrase pairs that could have beenextracted.
In this experiment we focus on the syn-tactic paraphrases which yielded the best results.
Tolower recall, we randomly remove paraphrase pairsfrom the corpus, and to lower precision, we add ran-dom paraphrase pairs to our table.
All the resultsare shown in Table 2.
Numbers next to precisionParaphrasing resource CminPrecision 0.1 0.603Precision 0.2 0.672Precision 0.3 0.565Precision 0.4 0.603Precision 0.5 0.626Recall 0.9 0.609Recall 0.8 0.606Recall 0.7 0.632Recall 0.6 0.610Recall 0.5 0.626Table 2: Effect of paraphrase precision and recall on FSDperformance.
Numbers next to recall and precision indi-cate the sampling rate and the proportion of added ran-dom pairs, respectively.and recall indicate the proportion of added randompairs and the proportion of removed pairs, respec-tively (e.g., recall 0.4 means that 40% of pairs wereremoved from the original resource).
We can seethat the results are much more stable with respect torecall ?
there is an initial drop in performance whenwe remove the first 10% of paraphrases, but afterthat removing more paraphrases does not affect per-formance very much.
On the other hand, changingthe precision has a bigger impact on the results.
Forexample, we can see that our system using a para-phrase corpus with 30% of pairs added at randomperforms even better than the system that uses theoriginal corpus.
On the other hand, adding 20% ofrandom pairs performs substantially worse than theoriginal corpus.
These results show that it is moreimportant for the paraphrases to have good precisionthan to have good recall.4.3 Twitter resultsBecause the Twitter event corpus that we use con-sists of over 50 million documents, we cannotuse the UMass system here due to its linear per-document time complexity.
Instead, our baselinesystem here is the FSD system of (Petrovic?
etal., 2010), without any paraphrasing.
This sys-tem uses the same approach as the UMass system,and (Petrovic?
et al, 2010) showed that it achievesvery similar results.
This means that our baseline, al-344though coming from a different system, is still state-of-the-art.
We make some Twitter-specific modifi-cation to the baseline system that slightly improvethe results.
Specifically, the baseline uses no stem-ming, ignores links, @-mentions, and treats hash-tags as normal words (i.e., removes the leading ?#?character).
While removing links and @-mentionswas also done in (Petrovic?
et al, 2010), our pre-liminary experiments showed that keeping hashtags,only without the hash sign improves the results.
Ad-ditionally, we limit the number of documents in abucket to at most 30% of the expected number ofcollisions for a single day (we assume one milliondocuments per day).Results for the different systems are shown in Ta-ble 3.
First, we can see that not using stemming ismuch better than using it, which is the opposite fromwhat is the case in TDT.
Second, we can see that theimprovements from using paraphrases that we had inTDT data are different here.
Syntactic paraphrasesand the MSR paraphrases do not help, whereas theparaphrases extracted from Wordnet did improve theresults, although the gains are not as large as in TDT.A paired t-test revealed that none of the differencesbetween the baseline system and the systems thatuse paraphrases were significant at p = 0.05.To gain more insight into why the results are dif-ferent here, we look at the proportion of words inthe documents that are being paraphrased, i.e., thecoverage of the paraphrasing resource.
We can seefrom Table 4 that the situation in TDT and Twit-ter is very different.
Coverage of MSR and syntac-tic paraphrases was lower in Twitter than in TDT,whereas Wordnet coverage was better on Twitter.While it seems that the benefits of using paraphrasesin Twitter are not as clear as in news, our efficientapproach enables us to answer questions like these,which could not be answered otherwise.To illustrate how paraphrases help detect oldtweets, consider the tweet According to Russian avi-ation officials, two passengers survived the crash,but are in critical condition.
Before paraphrasing,the closest tweet returned by our system was Shaz-aad Hussein has died in Birmingham after beingrun over, two others are in critical condition, whichis not very related.
After applying paraphrasing,in particular knowing that officials is a paraphraseof authorities, the closest tweet returned was SomeSystem CminBaseline system (stemming) 0.756Baseline system (no stemming) 0.694Wordnet 0.679MSR Paraphrases 0.739Syntactic paraphrases 0.729Table 3: Twitter FSD results for different systems, loweris better.
The baseline system is that of (Petrovic?
et al,2010).Paraphrases Coverage TDT (%) Coverage Twitter (%)Wordnet 52.5 56.1MSR 33.5 31.0Syntactic 35.6 31.7Table 4: Coverage of different resources.Russian authorities are reporting one survivor, oth-ers are saying there are three.
There were 37 totalon board, which is on the same event.
There are alsocases where paraphrases hurt.
For example, beforeparaphrasing the tweet Top News #debt #deal #ceil-ing #party had the nearest neighbor New debt ceilingdeal explained, whereas after paraphrasing, becausethe word roof is a paraphrase of ceiling, the nearestneighbor was The roof the roof the roof is on fire!.Cases like this could be fixed by looking at the con-text of the word, but we leave this for future work.5 ConclusionWe present a way of incorporating paraphrase infor-mation in a streaming first story detection system.To the best of our knowledge, this is the first workto use paraphrases in first story detection, and alsothe first work to combine paraphrases with locality-sensitive hashing to achieve fast retrieval of doc-uments that are written with different words, buttalk about the same thing.
We compare differentsources of paraphrases and show that our unsuper-vised FSD system that uses syntactically constrainedparaphrases achieves state-of-the-art results, beatingboth the best supervised and unsupervised systems.To test our approach on very large data, we constructa corpus of events for Twitter.
Our approach scaleswell on this data both in terms of time and mem-ory, and we show that paraphrases again help, but345this time the paraphrase sources yield different im-provements from TDT data.
We find that this differ-ence can be explained by the different coverage ofthe paraphrasing resources.AcknowledgmentsThe authors would like to thank Mirella Lapatafor her help with paraphrasing resources.
We alsoacknowledge financial support from EPSRC grantEP/J020664/1.ReferencesAmr Ahmed, Qirong Ho, Jacob Eisenstein, Eric Xing,Alex Smola, and Choon Hui Teo.
2011.
Unifiedanalysis of streaming news.
In Proceedings of WWW,pages 267?276.
ACM.James Allan, Victor Lavrenko, and Hubert Jin.
2000a.First story detection in tdt is hard.
In Proceedings ofthe CIKM, pages 374?381.
ACM.James Allan, Victor Lavrenko, Daniella Malin, and Rus-sell Swan.
2000b.
Detections, bounds, and timelines:Umass and tdt-3.
In Proceedings of Topic Detectionand Tracking Workshop, pages 167?174.James Allan.
2002.
Topic detection and tracking: event-based information organization.
Kluwer AcademicPublishers.Hila Becker, Mor Naaman, and Luis Gravano.
2011.
Se-lecting quality twitter content for events.
In Proceed-ings of ICWSM.Chris Callison-Burch, Philipp Koehn, and Miles Os-borne.
2006.
Improved statistical machine translationusing paraphrases.
In Proceedings of NAACL, pages17?24.
Association for Computational Linguistics.Chris Callison-Burch.
2008.
Syntactic constraints onparaphrases extracted from parallel corpora.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing, pages 196?205.
Asso-ciation for Computational Linguistics.Trevor Cohn, Chris Callison-Burch, and Mirella Lapata.2008.
Constructing corpora for the development andevaluation of paraphrase systems.
Computational Lin-guistics, 34(4):597?614.Christiane Fellbaum.
1998.
WordNet: An electronic lex-ical database.
The MIT press.Jonathan Fiscus.
2001.
Overview of results (nist).
InProceedings of the TDT 2001 Workshop.Piotr Indyk and Rajeev Motwani.
1998.
Approximatenearest neighbors: towards removing the curse of di-mensionality.
In STOC ?98: Proceedings of the thirti-eth annual ACM symposium on Theory of computing,pages 604?613, New York, NY, USA.
ACM.Rosie Jones, Benjamin Rey, Omid Madani, and WileyGreiner.
2006.
Generating query substitutions.
InProceedings of the 15th international conference onWorld Wide Web, pages 387?396.
ACM.Giridhar Kumaran and James Allan.
2005.
Using namesand topics for new event detection.
In Proceedingsof EMNLP, pages 121?128.
Association for Computa-tional Linguistics.Nitin Madnani and Bonnie Dorr.
2010.
Generat-ing phrasal and sentential paraphrases: A surveyof data-driven methods.
Computational Linguistics,36(3):341?387.Sas?a Petrovic?, Miles Osborne, and Victor Lavrenko.2010.
Streaming first story detection with applicationto twitter.
In Proceedings of the 11th annual confer-ence of the North American Chapter of the ACL, pages181?189.Martin F. Porter.
1980.
An algorithm for suffix stripping.Program, 14(3):130?137.Chris Quirk, Chris Brockett, and William Dolan.
2004.Monolingual machine translation for paraphrase gen-eration.
In In proceedings of EMNLP, pages 142?149.Stefan Riezler, Alexander Vasserman, Ioannis Tsochan-taridis, Vibhu Mittal, and Yi Liu.
2007.
Statisticalmachine translation for query expansion in answer re-trieval.
In Proceedings of ACL, volume 45, page 464.Karen Spa?rck Jones and John Tait.
1984.
Automaticsearch term variant generation.
Journal of Documen-tation, 40(1):50?66.Benjamin Van Durme and Ashwin Lall.
2010.
Onlinegeneration of locality sensitive hash signatures.
InProceedings of the 48th Annual Meeting of the Associ-ation for Computational Linguistics.Jinxi Xu and W. Bruce Croft.
1996.
Query expansionusing local and global document analysis.
In In Pro-ceedings of the 19th Annual International ACM SIGIRConference on Research and Development in Informa-tion Retrieval, pages 4?11.346
