Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1422?1426,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsElephant: Sequence Labeling for Word and Sentence SegmentationKilian Evang*, Valerio Basile*, Grzegorz Chrupa?a?
and Johan Bos**University of Groningen, Oude Kijk in ?t Jatstraat 26, 9712 EK Groningen, The Netherlands?Tilburg University, PO Box 90153, 5000 LE Tilburg, The Netherlands*{k.evang, v.basile, johan.bos}@rug.nl ?g.chrupala@uvt.nlAbstractTokenization is widely regarded as a solvedproblem due to the high accuracy that rule-based tokenizers achieve.
But rule-basedtokenizers are hard to maintain and theirrules language specific.
We show that high-accuracy word and sentence segmentation canbe achieved by using supervised sequence la-beling on the character level combined withunsupervised feature learning.
We evalu-ated our method on three languages and ob-tained error rates of 0.27 ?
(English), 0.35 ?
(Dutch) and 0.76 ?
(Italian) for our best mod-els.1 An Elephant in the RoomTokenization, the task of segmenting a text intowords and sentences, is often regarded as a solvedproblem in natural language processing (Dridan andOepen, 2012), probably because many corpora arealready in tokenized format.
But like an elephant inthe living room, it is a problem that is impossible tooverlook whenever new raw datasets need to be pro-cessed or when tokenization conventions are recon-sidered.
It is moreover an important problem, be-cause any errors occurring early in the NLP pipelineaffect further analysis negatively.
And even thoughcurrent tokenizers reach high performance, there arethree issues that we feel haven?t been addressed sat-isfactorily so far:?
Most tokenizers are rule-based and thereforehard to maintain and hard to adapt to new do-mains and new languages (Silla Jr. and Kaest-ner, 2004);?
Word and sentence segmentation are often seenas separate tasks, but they obviously informeach other and it could be advantageous to viewthem as a combined task;?
Most tokenization methods provide no align-ment between raw and tokenized text, whichmakes mapping the tokenized version backonto the actual source hard or impossible.In short, we believe that regarding tokenization,there is still room for improvement, in particular onthe methodological side of the task.
We are partic-ularly interested in the following questions: Can weuse supervised learning to avoid hand-crafting rules?Can we use unsupervised feature learning to reducefeature engineering effort and boost performance?Can we use the same method across languages?
Canwe combine word and sentence boundary detectioninto one task?2 Related WorkUsually the text segmentation task is split into wordtokenization and sentence boundary detection.
Rule-based systems for finding word and sentence bound-aries often are variations on matching hand-codedregular expressions (Grefenstette, 1999; Silla Jr. andKaestner, 2004; Jurafsky and Martin, 2008; Dridanand Oepen, 2012).Several unsupervised systems have been proposedfor sentence boundary detection.
Kiss and Strunk(2006) present a language-independent, unsuper-vised approach and note that abbreviations form amajor source of ambiguity in sentence boundarydetection and use collocation detection to build ahigh-accuracy abbreviation detector.
The resultingsystem reaches high accuracy, rivalling handcraftedrule-based and supervised systems.
A similar sys-tem was proposed earlier by Mikheev (2002).Existing supervised learning approaches for sen-tence boundary detection use as features tokens pre-ceding and following potential sentence boundary,part of speech, capitalization information and listsof abbreviations.
Learning methods employed in1422these approaches include maximum entropy models(Reynar and Ratnaparkhi, 1997) decision trees (Ri-ley, 1989), and neural networks (Palmer and Hearst,1997).Closest to our work are approaches that presenttoken and sentence splitters using conditional ran-dom fields (Tomanek et al 2007; Fares et al 2013).However, these previous approaches consider tokens(i.e.
character sequences) as basic units for labeling,whereas we consider single characters.
As a con-sequence, labeling is more resource-intensive, but italso gives us more expressive power.
In fact, our ap-proach kills two birds with one stone, as it allows usto integrate token and sentence boundaries detectioninto one task.3 Method3.1 IOB TokenizationIOB tagging is widely used in tasks identifyingchunks of tokens.
We use it to identify chunks ofcharacters.
Characters outside of tokens are labeledO, inside of tokens I.
For characters at the beginningof tokens, we use S at sentence boundaries, other-wise T (for token).
This scheme offers some nicefeatures, like allowing for discontinuous tokens (e.g.hyphenated words at line breaks) and starting a newtoken in the middle of a typographic word if the to-kenization scheme requires it, as e.g.
in did|n?t.
Anexample is given in Figure 1.It didn?t matter if the faces were male,SIOTIITIIOTIIIIIOTIOTIIOTIIIIOTIIIOTIIITOfemale or those of children.
Eighty-TIIIIIOTIOTIIIIOTIOTIIIIIIITOSIIIIIIOthree percent of people in the 30-to-34IIIIIOTIIIIIIOTIOTIIIIIOTIOTIIOTIIIIIIIOyear old age range gave correct responses.TIIIOTIIOTIIOTIIIIOTIIIOTIIIIIIOTIIIIIIIITFigure 1: Example of IOB-labeled characters3.2 DatasetsIn our experiments we use three datasets to compareour method for different languages and for differentdomains: manually checked English newswire textstaken from the Groningen Meaning Bank, GMB(Basile et al 2012), Dutch newswire texts, com-prising two days from January 2000 extracted fromthe Twente News Corpus, TwNC (Ordelman et al2007), and a random sample of Italian texts from thePAISA` corpus (Borghetti et al 2011).Table 1: Datasets characteristics.Name Language Domain Sentences TokensGMB English Newswire 2,886 64,443TNC Dutch Newswire 49,537 860,637PAI Italian Web/various 42,674 869,095The data was converted into IOB format by infer-ring an alignment between the raw text and the seg-mented text.3.3 Sequence labelingWe apply the Wapiti implementation (Lavergne etal., 2010) of Conditional Random Fields (Laffertyet al 2001), using as features the output label ofeach character, combined with 1) the character it-self, 2) the output label on the previous character, 3)characters and/or their Unicode categories from con-text windows of varying sizes.
For example, with acontext size of 3, in Figure 1, features for the E inEighty-three with the output label S would be E/S,O/S, /S, i/S, Space/S, Lowercase/S.
The intuitionis that the 31 existing Unicode categories can gen-eralize across similar characters whereas characterfeatures can identify specific contexts such as abbre-viations or contractions (e.g.
didn?t).
The contextwindow sizes we use are 0, 1, 3, 5, 7, 9, 11 and 13,centered around the focus character.3.4 Deep learning of featuresAutomatically learned word embeddings have beensuccessfully used in NLP to reduce reliance on man-ual feature engineering and boost performance.
Weadapt this approach to the character level, and thus,in addition to hand-crafted features we use textrepresentations induced in an unsupervised fashionfrom character strings.
A complete discussion ofour approach to learning text embeddings can befound in (Chrupa?a, 2013).
Here we provide a briefoverview.Our representations correspond to the activationof the hidden layer in a simple recurrent neural(SRN) network (Elman, 1990; Elman, 1991), imple-mented in a customized version of Mikolov (2010)?sRNNLM toolkit.
The network is sequentially pre-sented with a large amount of raw text and learns to1423predict the next character in the sequence.
It uses theunits in the hidden layer to store a generalized rep-resentation of the recent history.
After training thenetwork on large amounts on unlabeled text, we runit on the training and test data, and record the activa-tion of the hidden layer at each position in the stringas it tries to predict the next character.
The vector ofactivations of the hidden layer provides additionalfeatures used to train and run the CRF.
For each ofthe K = 10 most active units out of total J = 400hidden units, we create features (f(1) .
.
.
f(K)) de-fined as f(k) = 1 if sj(k) > 0.5 and f(k) = 0 oth-erwise, where sj(k) returns the activation of the kthmost active unit.
For training the SRN only raw textis necessary.
We trained on the entire GMB 2.0.0(2.5M characters), the portion of TwNC correspond-ing to January 2000 (43M characters) and a sampleof the PAISA` corpus (39M characters).4 Results and EvaluationIn order to evaluate the quality of the tokenizationproduced by our models we conducted several ex-periments with different combinations of featuresand context sizes.
For these tests, the models aretrained on an 80% portion of the data sets and testedon a 10% development set.
Final results are obtainedon a 10% test set.
We report both absolute numberof errors and error rates per thousand (?
).4.1 Feature setsWe experiment with two kinds of features at thecharacter level, namely Unicode categories (31 dif-ferent ones), Unicode character codes, and a combi-nation of them.
Unicode categories are less sparsethan the character codes (there are 88, 134, and 502unique characters for English, Dutch and Italian, re-spectively), so the combination provide some gener-alization over just character codes.Table 2: Error rates obtained with different feature sets.Cat stands for Unicode category, Code for Unicode char-acter code, and Cat-Code for a union of these features.Error rates per thousand (?
)Feature set English Dutch ItalianCat-9 45 (1.40) 1,403 (2.87) 1,548 (2.67)Code-9 6 (0.19) 782 (1.60) 692 (1.20)Cat-Code-9 8 (0.25) 774 (1.58) 657 (1.14)From these results we see that categories aloneperform worse than only codes.
For English there isno gain from the combination over using only char-acter codes.
For Dutch and Italian there is an im-provement, although it is only significant for Ital-ian (p = 0.480 and p = 0.005 respectively, bino-mial exact test).
We use this feature combination inthe experiments that follow.
Note that these modelsare trained using a symmetrical context of 9 charac-ters (four left and four right of the current character).In the next section we show performance of modelswith different window sizes.4.2 Context windowWe run an experiment to evaluate how the size of thecontext in the training phase impacts the classifica-tion.
In Table 4.2 we show the results for symmetri-cal windows ranging in size from 1 to 13.Table 3: Using different context window sizes.Error rates per thousand (?
)Feature set English Dutch ItalianCat-Code-1 273 (8.51) 4,924 (10.06) 9,108 (15.86)Cat-Code-3 118 (3.68) 3,525 (7.20) 2,013 (3.51)Cat-Code-5 20 (0.62) 930 (1.90) 788 (1.37)Cat-Code-7 10 (0.31) 778 (1.60) 667 (1.16)Cat-Code-9 8 (0.25) 774 (1.58) 657 (1.14)Cat-Code-11 9 (0.28) 761 (1.56) 692 (1.21)Cat-Code-13 8 (0.25) 751 (1.54) 670 (1.17)4.3 SRN featuresWe also tested the automatically learned features de-rived from the activation of the hidden layer of anSRN language model, as explained in Section 3.We combined these features with character code andUnicode category features in windows of differentsizes.
The results of this test are shown in Table 4.The first row shows the performance of SRN fea-tures on their own.
The following rows show thecombination of SRN features with the basic featuresets of varying window size.
It can be seen that aug-menting the feature sets with SRN features resultsin large reductions of error rates.
The Cat-Code-1-SRN setting has error rates comparable to Cat-Code-9.The addition of SRN features to the two bestprevious models, Cat-Code-9 and Cat-Code-13, re-duces the error rate by 83% resp.
81% for Dutch,1424and by 24% resp.
26% for Italian.
All these dif-ferences are statistically significant according to thebinomial test (p < 0.001).
For English, there are toofew errors to detect a statistically significant effectfor Cat-Code-9 (p = 0.07), but for Cat-Code-13 wefind p = 0.016.Table 4: Results obtained using different context windowsizes and addition of SRN features.Error rates per thousand (?
)Feature set English Dutch ItalianSRN 24 (0.75) 276 (0.56) 738 (1.28)Cat-Code-1-SRN 7 (0.21) 212 (0.43) 549 (0.96)Cat-Code-3-SRN 4 (0.13) 165 (0.34) 507 (0.88)Cat-Code-5-SRN 3 (0.10) 136 (0.28) 476 (0.83)Cat-Code-7-SRN 1 (0.03) 111 (0.23) 497 (0.86)Cat-Code-9-SRN 2 (0.06) 135 (0.28) 497 (0.86)Cat-Code-11-SRN 2 (0.06) 132 (0.27) 468 (0.81)Cat-Code-13-SRN 1 (0.03) 142 (0.29) 496 (0.86)In a final step, we selected the best models basedon the development sets (Cat-Code-7-SRN for En-glish and Dutch, Cat-Code-11-SRN for Italian), andchecked their performance on the final test set.
Thisresulted in 10 errors (0.27 ?)
for English (GMBcorpus), 199 errors (0.35 ?)
for Dutch (TwNC cor-pus), and 454 errors (0.76 ?)
for Italian (PAISA`corpus).5 DiscussionIt is interesting to examine what kind of errors theSRN features help avoid.
In the English and Dutchdatasets many errors are caused by failure to rec-ognize personal titles and initials or misparsing ofnumbers.
In the Italian data, a large fraction of er-rors is due to verbs with clitics, which are written asa single word, but treated as separate tokens.
Table 5shows examples of errors made by a simpler modelthat are fixed by adding SRN features.
Table 6 showsthe confusion matrices for the Cat-Code-7 and Cat-Code-7-SRN sets on the Dutch data.
The mistakemost improved by SRN features is T/I with 89% er-ror reduction (see also Table 5).
The is also the mostcommon remaining mistake.A comparison with other approaches is hard be-cause of the difference in datasets and task defini-tion (combined word/sentence segmentation).
Herewe just compare our results for sentence segmenta-tion (sentence F1 score) with Punkt, a state-of-the-Table 5: Positive impact of SRN features.Ms.
Hughes will joiCat-Code-7 SIIOSIIIIIOTIIIOTIICat-Code-7-SRN SIIOTIIIIIOTIIIOTII$ 3.9 trillion by tCat-Code-7 TOTTIOTIIIIIIIOTIOTCat-Code-7-SRN TOTIIOTIIIIIIIOTIOTbleek 0,4 procentCat-Code-11 OTIIIIOTTIOTIIIIIIOCat-Code-11-SRN OTIIIIOTIIOTIIIIIIOtoebedeeld: 6,2.
InCat-Code-11 TIIIIIIIIITOTTITOSICat-Code-11-SRN TIIIIIIIIITOTIITOSIprof.
Teulings hetCat-Code-11 TIIITOSIIIIIIIOTIIOCat-Code-11-SRN TIIIIOTIIIIIIIOTIIOper costringerlo alCat-Code-11 TIIOTIIIIIIIIIIIOTICat-Code-11-SRN TIIOTIIIIIIIIITIOTITable 6: Confusion matrix for Dutch development set.Predicted, Cat-Code-7 Predicted, Cat-Code-7-SRNGold I O S T I O S TI 328128 0 2 469 328546 0 0 53O 0 75234 0 0 0 75234 0 0S 4 0 4323 18 1 0 4332 12T 252 0 33 80828 35 0 10 81068art sentence boundary detection system (Kiss andStrunk, 2006).
With its standard distributed mod-els, Punkt achieves 98.51% on our English test set,98.87% on Dutch and 98.34% on Italian, comparedwith 100%, 99.54% and 99.51% for our system.
Oursystem benefits here from its ability to adapt to a newdomain with relatively little (but annotated) trainingdata.6 What Elephant?Word and sentence segmentation can be recast as acombined tagging task.
This way, tokenization iscast as a supervised learning task, causing a shift oflabor from writing rules to manually correcting la-bels.
Learning this task with CRF achieves high ac-curacy.1 Furthermore, our tagging method does notlose the connection between original text and tokens.In future work, we plan to broaden the scope ofthis work to other steps in document preparation,1All software needed to replicate our experiments isavailable at http://gmb.let.rug.nl/elephant/experiments.php1425such as normalization of punctuation, and their in-teraction with segmentation.
We further plan to testour method on a wider range of datasets, allowing amore direct comparison with other approaches.
Fi-nally, we plan to explore the possibility of a statis-tical universal segmentation model for mutliple lan-guages and domains.In a famous scene with a live elephant on stage,the comedian Jimmy Durante was asked about it bya policeman and surprisedly answered: ?What ele-phant??
We feel we can say the same now as far astokenization is concerned.ReferencesValerio Basile, Johan Bos, Kilian Evang, and NoortjeVenhuizen.
2012.
Developing a large semanticallyannotated corpus.
In Proceedings of the Eight In-ternational Conference on Language Resources andEvaluation (LREC 2012), pages 3196?3200, Istanbul,Turkey.Claudia Borghetti, Sara Castagnoli, and Marco Brunello.2011.
I testi del web: una proposta di classificazionesulla base del corpus PAISA`.
In M. Cerruti, E. Corino,and C. Onesti, editors, Formale e informale.
La vari-azione di registro nella comunicazione elettronica,pages 147?170.
Carocci, Roma.Grzegorz Chrupa?a.
2013.
Text segmentation withcharacter-level text embeddings.
In ICML Workshopon Deep Learning for Audio, Speech and LanguageProcessing, Atlanta, USA.Rebecca Dridan and Stephan Oepen.
2012.
Tokeniza-tion: Returning to a long solved problem ?
a survey,contrastive experiment, recommendations, and toolkit?.
In Proceedings of the 50th Annual Meeting of theAssociation for Computational Linguistics (Volume 2:Short Papers), pages 378?382, Jeju Island, Korea.
As-sociation for Computational Linguistics.Jeffrey L. Elman.
1990.
Finding structure in time.
Cog-nitive science, 14(2):179?211.Jeffrey L. Elman.
1991.
Distributed representations,simple recurrent networks, and grammatical structure.Machine learning, 7(2):195?225.Murhaf Fares, Stephan Oepen, and Zhang Yi.
2013.
Ma-chine learning for high-quality tokenization - replicat-ing variable tokenization schemes.
In A. Gelbukh, ed-itor, CICLING 2013, volume 7816 of Lecture Notes inComputer Science, pages 231?244, Berlin Heidelberg.Springer-Verlag.Gregory Grefenstette.
1999.
Tokenization.
In Hans vanHalteren, editor, Syntactic Wordclass Tagging, pages117?133.
Kluwer Academic Publishers, Dordrecht.Daniel Jurafsky and James H. Martin.
2008.
Speechand Language Processing.
An Introduction to NaturalLanguage Processing, Computational Linguistics, andSpeech Recognition.
Prentice Hall, 2nd edition.Tibor Kiss and Jan Strunk.
2006.
Unsupervised multi-lingual sentence boundary detection.
ComputationalLinguistics, 32(4):485?525.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic mod-els for segmenting and labeling sequence data.
In Pro-ceedings of ICML-01, pages 282?289.Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.2010.
Practical very large scale CRFs.
In Proceed-ings of the 48th Annual Meeting of the Association forComputational Linguistics, pages 504?513, Uppsala,Sweden, July.
Association for Computational Linguis-tics.Andrei Mikheev.
2002.
Periods, capitalized words, etc.Computational Linguistics, 28(3):289?318.Toma?s?
Mikolov, Martin Karafia?t, Luka?s?
Burget, JanC?ernocky?, and Sanjeev Khudanpur.
2010.
Recurrentneural network based language model.
In Interspeech.Roeland Ordelman, Franciska de Jong, Arjan van Hessen,and Hendri Hondorp.
2007.
TwNC: a multifacetedDutch news corpus.
ELRA Newsleter, 12(3/4):4?7.David D. Palmer and Marti A. Hearst.
1997.
Adap-tive multilingual sentence boundary disambiguation.Computational Linguistics, 23(2):241?267.Jeffrey C. Reynar and Adwait Ratnaparkhi.
1997.
Amaximum entropy approach to identifying sentenceboundaries.
In Proceedings of the Fifth Conferenceon Applied Natural Language Processing, pages 16?19, Washington, DC, USA.
Association for Computa-tional Linguistics.Michael D. Riley.
1989.
Some applications of tree-basedmodelling to speech and language.
In Proceedings ofthe workshop on Speech and Natural Language, HLT?89, pages 339?352, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Carlos N. Silla Jr. and Celso A.
A. Kaestner.
2004.
Ananalysis of sentence boundary detection systems forEnglish and Portuguese documents.
In Fifth Interna-tional Conference on Intelligent Text Processing andComputational Linguistics, volume 2945 of LectureNotes in Computer Science, pages 135?141.
Springer.Katrin Tomanek, Joachim Wermter, and Udo Hahn.2007.
Sentence and token splitting based on condi-tional random fields.
In Proceedings of the 10th Con-ference of the Pacific Association for ComputationalLinguistics, pages 49?57, Melbourne, Australia.1426
