Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 7?16,Baltimore, Maryland USA, 27 June 2014.c?2014 Association for Computational LinguisticsUsing county demographics to infer attributes of Twitter usersEhsan Mohammady and Aron CulottaDepartment of Computer ScienceIllinois Institute of TechnologyChicago, IL 60616emohamm1@hawk.iit.edu, culotta@cs.iit.eduAbstractSocial media are increasingly being usedto complement traditional survey methodsin health, politics, and marketing.
How-ever, little has been done to adjust for thesampling bias inherent in this approach.Inferring demographic attributes of socialmedia users is thus a critical step to im-proving the validity of such studies.
Whilethere have been a number of supervisedmachine learning approaches to this prob-lem, these rely on a training set of usersannotated with attributes, which can bedifficult to obtain.
We instead proposetraining a demographic attribute classi-fiers that uses county-level supervision.By pairing geolocated social media withcounty demographics, we build a regres-sion model mapping text to demographics.We then adopt this model to make predic-tions at the user level.
Our experimentsusing Twitter data show that this approachis surprisingly competitive with a fully su-pervised approach, estimating the race ofa user with 80% accuracy.1 IntroductionResearchers are increasingly using social mediaanalysis to complement traditional survey meth-ods in areas such as public health (Dredze, 2012),politics (O?Connor et al., 2010), and market-ing (Gopinath et al., 2014).
It is generally ac-cepted that social media users are not a representa-tive sample of the population (e.g., urban and mi-nority populations tend to be overrepresented onTwitter (Mislove et al., 2011)).
Nevertheless, fewresearchers have attempted to adjust for this bias.
(Gayo-Avello (2011) is an exception.)
This canin part be explained by the difficulty of obtainingdemographic information of social media users?
while gender can sometimes be inferred fromthe user?s name, other attributes such as age andrace/ethnicity are more difficult to deduce.
Thisproblem of user attribute prediction is thus crit-ical to such applications of social media analysis.A common approach to user attribute predictionis supervised classification ?
from a training setof annotated users, a model is fit to predict user at-tributes from the content of their writings and theirsocial connections (Argamon et al., 2005; Schleret al., 2006; Rao et al., 2010; Pennacchiotti andPopescu, 2011; Burger et al., 2011; Rao et al.,2011; Al Zamal et al., 2012).
Because collectinghuman annotations is costly and error-prone, la-beled data are often collected serendipitously; forexample, Al Zamal et al.
(2012) collect age anno-tations by searching for tweets with phrases suchas ?Happy 21st birthday to me?
; Pennacchiotti andPopescu (2011) collect race annotations by search-ing for profiles with explicit self identification(e.g., ?I am a black lawyer from Sacramento.?
).While convenient, such an approach likely sufferfrom selection bias (Liu and Ruths, 2013).In this paper, we propose fitting classificationmodels on population-level data, then applyingthem to predict user attributes.
Specifically, wefit regression models to predict the race distribu-tion of 100 U.S. counties (based on Census data)from geolocated Twitter messages.
We then ex-tend this learned model to predict user-level at-tributes.
This lightly supervised approach reducesthe need for human annotation, which is importantnot only because of the reduction of human effort,but also because many other attributes may be dif-ficult even for humans to annotate at the user-level(e.g., health status, political orientation).
We in-vestigate this new approach through the followingthree research questions:RQ1.
Can models trained on county statisticsbe used to infer user attributes?
We findthat a classifier trained on county statis-7tics can make accurate predictions at theuser level.
Accuracy is slightly lower (byless than 1%) than a fully supervised ap-proach using logistic regression trained onhundreds of labeled instances.RQ2.
How do models trained on county datadiffer from those using standard super-vised methods?
We analyze the highly-weighted features of competing models,and find that while both models discern lex-ical differences (e.g., slang, word choice),the county-based model also learns geo-graphical correlates of race (e.g., city, state).RQ3.
What bias does serendipitously labeleddata introduce?
By comparing trainingdatasets collected uniformly at random withthose collected by searching for certain key-words, we find that the search approach pro-duces a very biased class distribution.
Addi-tionally, the classifier trained on such biaseddata tends to overweight features matchingthe original search keywords.2 Related WorkPredicting attributes of social media users is agrowing area of interest, with recent work focus-ing on age (Schler et al., 2006; Rosenthal andMcKeown, 2011; Nguyen et al., 2011; Al Zamalet al., 2012), sex (Rao et al., 2010; Burger et al.,2011; Liu and Ruths, 2013), race/ethnicity (Pen-nacchiotti and Popescu, 2011; Rao et al., 2011),and personality (Argamon et al., 2005; Schwartzet al., 2013b).
Other work predicts demographicsfrom web browsing histories (Goel et al., 2012).The majority of these approaches rely on hand-annotated training data, require explicit self-identification by the user, or are limited to verycoarse attribute values (e.g., above or below 25-years-old).
Pennacchiotti and Popescu (2011)train a supervised classifier to predict whether aTwitter user is African-American or not basedon linguistic and social features.
To constructa labeled training set, they collect 6,000 Twitteraccounts in which the user description matchesphrases like ?I am a 20 year old African-American.?
In our experiments below, we demon-strate how such serendipitously labeled data canintroduce selection bias in the estimate of clas-sification accuracy.
Their final classifier obtainsa 65.5% F1 measure on this binary classificationtask (compared with the 76.5% F1 we report be-low for a different dataset labeled with four racecategories).A related lightly supervised approach includesChang et al.
(2010), who infer user-level eth-nicity using name/ethnicity distributions providedby the Census; however, that approach uses evi-dence from first and last names, which are oftennot available, and thus are more appropriate forpopulation-level estimates.
Rao et al.
(2011) ex-tend this approach to also include evidence fromother linguistic features to infer gender and ethnic-ity of Facebook users; they evaluate on the fine-grained ethnicity classes of Nigeria and use verylimited training data.Viewed as a way to make individual inferencesfrom aggregate data, our approach is related toecological inference (King, 1997); however, herewe have the advantage of user-level observations(linguistic data), which are typically absent in eco-logical inference settings.There have been several studies predictingpopulation-level statistics from social media.Eisenstein et al.
(2011) use geolocated tweets topredict zip-code statistics of race/ethnicity, in-come, and other variables using Census data;Schwartz et al.
(2013b) and Culotta (2014) simi-larly predict county health statistics from Twitter.However, none of this prior work attempts to pre-dict or evaluate at the user level.Schwartz et al.
(2013a) collect Facebook pro-files labeled with personality type, gender, and ageby administering a survey of users embedded in apersonality test application.
While this approachwas able to collect over 75K labeled profiles, itcan be difficult to reproduce, and is also challeng-ing to update over time without re-administeringthe survey.Compared to this related work, our core con-tribution is to propose and evaluate a classifiertrained only on county statistics to estimate therace of a Twitter user.
The resulting accuracyis competitive with a fully supervised baseline aswell as with prior work.
By avoiding the use of la-beled data, the method is simple to train and easierto update as linguistic patterns evolve over time.3 MethodsOur approach to user attribute prediction is as fol-lows: First, we collect population-level statistics,for example the racial makeup of a county.
Sec-8ond, we collect a sample of tweets from the samepopulation areas and distill them into one fea-ture vector per location.
Third, we fit a regres-sion model to predict the population-level statis-tics from the linguistic feature vector.
Finally, weadapt the regression coefficients to predict the at-tributes of individual Twitter user.
Below, we de-scribe the data, the regression and classificationmodels, and the experimental setup.3.1 DataWe collect three types of data: (1) Census data,listing the racial makeup of U.S.
Counties; (2)geolocated Twitter data from each county; (3) avalidation set of Twitter users manually annotatedwith race, for evaluation purposes.3.1.1 Census DataThe U.S. Census produces annual estimates ofthe race and Hispanic origin proportions for eachcounty in the United States.
These estimates arederived using the most recent decennial census andestimates of population changes (deaths, birth, mi-gration) since that census.
The census question-naire allows respondents to select one or more of 6racial categories: White, Black or African Ameri-can, American Indian and Alaska Native, Asian,Native Hawaiian and Other Pacific Islander, orOther.
Additionally, each respondent is askedwhether they consider themselves to be of His-panic, Latino, or Spanish origin (ethnicity).
Sincerespondents may select multiple races in additionto ethnicity, the Census reports many differentcombinations of results.While race/ethnicity is indeed a complex is-sue, for the purposes of this study we simplifyby considering only four categories: Asian, Black,Latino, White.
(For simplicity, we ignore the Cen-sus?
distinction between race and ethnicity; dueto small proportions, we also omit Other, Amer-ican Indian/Alaska Native, and Native Hawaiianand Other Pacific Islander.)
For the three cate-gories other than Latino, we collect the proportionof each county for that race, possibly in combina-tions with others.
For example, the percentage ofAsians in a county corresponds to the Census cat-egory: ?NHAAC: Not Hispanic, Asian alone or incombination.?
The Latino proportion correspondsto the ?H?
category, indicating the percentage ofa county identifying themselves as of Hispanic,Latino, or Spanish origin (our terminology againignores the distinction between the terms ?Latino?and ?Hispanic?).
We use the 2012 estimates forthis study.1We collect the proportion of residentsfrom each of these four categories for the 100 mostpopulous counties in the U.S.3.1.2 Twitter County DataFor each of the 100 most populous counties inthe U.S., we identify its geographical coordinates(from the U.S. Census), and construct a geograph-ical Twitter query (bounding box) consisting of a50 square mile area centered at the county coordi-nates.
This approximation introduces a very smallamount of noise ?
less than .02% of tweets comefrom areas of overlapping bounding boxes.2Wesubmit each of these 100 queries in turn from De-cember 5, 2012 to November 14, 2013.
Thesegeographical queries return tweets that carry ge-ographical coordinates, typically those sent frommobile devices with this preference enabled.3Thisresulted in 5.7M tweets from 839K unique users.3.1.3 Validation DataUniform Data: For validation purposes, we cate-gorized 770 Twitter profiles into one of four cate-gories (Asian, Black, Latino, White).
These werecollected as follows: First, we used the Twit-ter Streaming API to obtain a random sample ofusers, filtered to the United States (using timezone and the place country code from the pro-file).
From six days?
worth of data (December6-12, 2013), we sampled 1,000 profiles at ran-dom and categorized them by analyzing the pro-file, tweets, and profile image for each user.
Thosefor which race could not be determined were dis-carded (230/1,000; 23%).4The category fre-quency is Asian (22), Black (263), Latino (158),White (327).
To estimate inter-annotator agree-ment, a second annotator sampled and categorized120 users.
Among users for which both annota-tors selected one of the four categories, 74/76 la-bels agreed (97%).
There was some disagreementover when the category could be determined: for1http://www.census.gov/popest/data/counties/asrh/2012/files/CC-EST2012-ALLDATA.csv2The Census also publishes polygon data for each county,which could be used to remove this small source of noise.3Only considering geolocated tweets introduces somebias into the types of tweets observed.
However, we com-pared the unigram frequency vectors from geolocated tweetswith a sample of non-geolocated tweets and found a strongcorrelation (0.93).4This introduces some bias towards accounts with identi-fiable race; we leave an investigation of this for future work.921/120 labels (17.5%), one annotator indicated thecategory could not be determined, while the otherselected a category.
For each user, we collectedtheir 200 most recent tweets using the Twitter API.We refer to this as the Uniform dataset.Search Data: It is common in prior workto search for keywords indicating user attributes,rather than sampling uniformly at random and thenlabeling (Pennacchiotti and Popescu, 2011; Al Za-mal et al., 2012).
This is typically done for con-venience; a large number of annotations can becollected with little or no manual annotation.
Wehypothesize that this approach results in a biasedsample of users, since it is restricted to those witha predetermined set of keywords.
This bias mayaffect the estimate of the generalization accuracyof the resulting classifier.To investigate this, we used the Twitter SearchAPI to collect profiles containing a predefined setof keywords indicating race.
Examples includethe terms ?African?, ?Black?, ?Hispanic?, ?Latin?,?Latino?, ?Spanish?, ?Chinese?, ?Italian?, ?Irish.
?Profiles containing such words in the descriptionfield were collected.
These were further filteredin an attempt to remove businesses (e.g., Chineserestaurants) by excluding profiles with the key-words in the name field as well as those whosename fields did not contain terms on the Census?list of common first and last names.
Remainingprofiles were then manually reviewed for accu-racy.
This resulted in 2,000 annotated users withthe following distribution: Asian (377), Black(373), Latino (356), White (894).
For each user,we collected their 200 most recent tweets using theTwitter API.
We refer to this as the Search dataset.Table 1 compares the race distribution for eachof the two datasets.
It is apparent that the Searchdataset oversamples Asian users and undersam-ples Black users as compared to the Uniformdataset.
This may in part due to the greater num-ber of keywords used to identify Asian users (e.g.,Chinese, Japanese, Korean).
This highlights thedifficulty of obtaining a representative sample ofTwitter users with the search approach, since theinclusion of a single keyword can result in a verydifferent distribution of labels.3.2 Models3.2.1 County RegressionWe build a text regression model to predict theracial makeup of a county (from the Census data)Uniform SearchAsian 3% 19%Black 34% 19%Latino 21% 18%White 42% 44%Table 1: Percentage of users by race in the twovalidation datasets.based on the linguistic patterns in tweets from thatcounty.
For each county, we create a feature vectoras follows: for each unigram, we compute the pro-portion of users in the county who have used thatunigram.
We also distinguish between unigrams inthe text of a tweet and a unigram in the descriptionfield of the user?s profile.
Thus, two sample fea-ture values are (china, 0.1) and (desc china, 0.05),indicating that 10% of users in the county wrote atweet containing the unigram china, and 5% havethe word china in their profile description.
We ig-nore mentions and collapse URLs (replacing themwith the token ?http?
), but retain hashtags.We fit four separate ridge regression models,one per race.5For each model, the independentvariables are the unigram proportions from above;the dependent variable is the percentage of eachcounty of a particular race.
Ridge regression isan L2 regularized form of linear regression, where?
determines the regularization strength, yiis avector of dependent variables for category i, X isa matrix of independent variables, and ?
are themodel parameters:?
?i= argmin?||yi?X?i||22+ ?||?||22Thus, we have one parameter vector for each racecategory??
= {??A,??B,??L,??W}.
Related ap-proaches have been used in prior work to estimatecounty demographics and health statistics (Eisen-stein et al., 2011; Schwartz et al., 2013b; Culotta,2014).Our core hypothesis is that the??
coefficientslearned above can be used to categorize individ-ual users by race.
We propose a very simple ap-proach that simply treats??
as parameters of a lin-ear classifier.
For each user in the labeled dataset,we construct a binary feature vector x using thesame unigram vocabulary from the county regres-sion task.
Then, we classify each user according to5Subsequent experiments with lasso, elastic net, andmulti-output elastic net performed no better.10the dot product between this binary feature vectorx and the parameter vector for each category:y?
= argmaxi(x ??
?i)3.2.2 Baseline 1: Logistic RegressionFor comparison, we also train a logistic regres-sion classifier using the user-annotated data (eitherUniform or Search).
We perform 10-fold classifi-cation, using the same binary feature vectors de-scribed above (preliminary results using term fre-quency instead of binary vectors resulted in loweraccuracy).
We again use L2 regularization, con-trolled by tunable parameter ?.3.2.3 Baseline 2: Name HeuristicInspired by the approach of Chang et al.
(2010),we collect Census data containing the frequencyof racial categories by last name.
We use the top1000 most popular last names with their race dis-tribution from Census database.
If the last namein the user?s Twitter profile matches names onthis list, we categorize the user with the mostprobable race according to the Census data.
Forexample, the Census indicates that 91% of peo-ple with the last name Garcia identify themselvesas Latino/Hispanic.
We would thus label Twit-ter users with Garcia as a last name as Hispanic.Users whose last names are not matched are cate-gorized as White (the most common label).3.3 ExperimentsWe performed experiments to estimate the accu-racy of each approach, as well as how differenttraining sets affect performance.
The systems are:1.
County: The county regression approach ofSection 3.2.1, trained only using county-levelsupervision.2.
Uniform: A logistic regression classifiertrained on the Uniform dataset.3.
Search: A logistic regression classifiertrained on the Search dataset.4.
Name heuristic: The name heuristic of Sec-tion 3.2.3.We compare testing accuracy on both the Uni-form dataset and Search datasets.
For experimentsin which systems are trained and tested on thesame dataset, we report the average results of 10-fold cross-validation.Figure 1: Learning curve for the Uniform dataset.The solid black line is the cross-validation accu-racy of a logistic regression classifier trained usingincreasingly more labeled examples.Figure 2: Learning curve for the Search dataset.The solid black line is the cross-validation accu-racy of a logistic regression classifier trained usingincreasingly more labeled examples.We tune the ?
regularization parameter for bothridge and logistic regression, reporting the bestaccuracy for each approach.
Systems are imple-mented in Python using the scikit-learn li-brary (Pedregosa and others, 2011).4 ResultsFigure 1 plots cross-validation accuracy on theUniform dataset as the number of labeled exam-ples increases.
Surprisingly, the County model,which uses no user-labeled data, performs onlyslightly worse than the fully supervised approach(81.7% versus 82.2%).
This suggests that the lin-guistic patterns learned from the county data can11PPPPPPPPTrainTestSearch UniformSearch 0.7715 0.8000Uniform 0.5535 0.8221County 0.5490 0.8169Name heuristic 0.4955 0.4519Table 2: Accuracy of each system.PPPPPPPPTrainTestSearch UniformSearch 0.7650 0.8074Uniform 0.4721 0.8130County 0.4738 0.8050Name heuristic 0.3838 0.3178Table 3: F1 of each system.be transferred to make inferences at the user level.Figure 1 also shows slightly lower accuracyfrom training on the Search dataset and testing onthe Uniform dataset (80%).
This may in part bedue to the different label distributions between thedatasets, as well as the different characteristics ofthe linguistic patterns, discussed more below.The Name heuristic does poorly overall, mainlybecause few users provide their last names in theirprofiles, and only a fraction of those names are onthe Census?
name list.Figure 2 plots the learning curve for the Searchdataset.
Here, the County approach performs con-siderably worse than logistic regression trained onthe Search data.
However, the County approachagain performs comparable to the supervised Uni-form approach.
That is, training a supervised clas-sifier on the Uniform dataset is only slightly moreaccurate than training only using county supervi-sion (54.9% versus 55.3%).
By F1, county super-vision does slightly better than the Uniform ap-proach.
This again highlights the very differentcharacteristics of the Uniform and Search datasets.Importantly, if we remove features from the userdescription field, then the cross-validation accu-racy of the Search classifier is reduced from 77%to 67%.
Since a small set of keywords in the de-scription field were used to collect the Search data,the Search classifier simply recovers those key-words, thus inflating its performance.Tables 2-4 show the accuracy, F1, and precisionfor each method (averaged over each class label).The relative trends are the same for each metric.The primary difference is the high precision of thePPPPPPPPTrainTestSearch UniformSearch 0.7909 0.8250Uniform 0.6659 0.8155County 0.4781 0.7967Name heuristic 0.5897 0.6886Table 4: Precision of each system.PPPPPPPPTrainTestCountySearch 0.0190Uniform 0.0361County 0.0186Name heuristic 0.0154Table 5: Mean Squared Error of each systemon the task of predicting the racial makeup of acounty.
Values are averages over the four race cat-egories.Name heuristic ?
when users do provide a lastname on the Census list, this heuristic predicts thecorrect race 69% of the time on the Uniform data,and 59% of the time on the Search data.We additionally compute how well the differ-ent approaches predict the county demographics.For the County method, we perform 10-fold cross-validation, using the original county feature vec-tors as independent variables.
For the logistic re-gression methods, we train the classifier on one ofthe user datasets (Uniform or Search), then clas-sify each user in the county dataset.
These pre-dictions are aggregated to compute the proportionof each race per county.
For the name heuristic,we only consider users who match a name in theCensus list, and use the heuristic to compute theproportion of users of each race.Table 5 displays the mean squared error be-tween the predicted and true race proportions, av-eraged over all counties and races.
The nameheuristic outperforms all other systems on thistask, in contrast to the previous results showing thename heuristic is the least accurate predictor at theuser level.
This is most likely because the nameheuristic can ignore many users without penaltywhen predicting county proportions.
The Countymethod does better than the Search or Uniformmethods, which is to be expected, since it wastrained specifically for this task.
It is possible thatthe Search and Uniform error can be reduced byadjusting for quantification bias (Forman, 2008),12Black White Latino Asianblack white spanish asianafrican italian latin asianamerican irish hispanic filipinoblack british spanish koreanthe french latino chineseafrican german de koreanyoung girl en japanesesmh boy el philippinesto own que vietnamesemale italian latin japaneseyall russian es filipinoniggas pretty la asianswoman fucking por japanrip christmas latino chineseman buying hispanic manyTable 6: Top-weighted features for the classifiertrained on the Search dataset.
Terms from the de-scription field are in italics.though we do not investigate this here.4.1 Analysis of top featuresTables 6-8 show the top 15 features for each sys-tem, sorted by their corresponding model parame-ters.
In both our training and testing process, wedistinguish between words in the user descriptionfield and words in tweets.
We also include a fea-ture that indicates whether the user has any text atall in their profile description.
In addition, we ig-nore mentions but retain hashtags.
In these tables,words in description are shown in italics.Because the Search dataset is collected bymatching description keywords, in Table 6 manyof these keywords are top-weighted features (e.g.,?black?, ?white?, ?spanish?, ?asian?).
However inTable 7, there is no top feature word from the de-scription.
This observation shows how our searchdataset collection biases the resulting classifier.The top features for the Uniform method (Ta-ble 7) tend to represent lexical variations and slangcommon among these groups.
Interestingly, noterms from the profile description are stronglyweighted, most likely a result of the uniform sam-pling approach, which does not bias the data tousers with keywords in their profile.For the County approach, it is less revealingto simply report the features with the highestweights.
Since the regression models for each racewere fit independently, many of the top-weightedBlack White Latino Asianain makes pizza werelmao please 3rd sorrysomebody seriously drunk bittryna guys ti hahahabout whenever gets manigga snow el hurtsniggas pretty estoy keepblack literally self teamsmh thing lucky awtf isn special foodlil such everywhere sadbeen am sleep packedreal red la careeverybody glass chicken goodbyegon sucks tried foreverTable 7: Top-weighted features for the classifiertrained on the Uniform dataset.words are stop words (as opposed to the logisticregression approach, which treats this as a multi-class classification problem).
To report a moreuseful list of terms, we took the following steps:(1) we normalized the parameter vectors for eachclass by vector length; (2) from the parameter vec-tor of each class we subtracted the vectors of theother three classes (i.e., ?B?
?B?
(?A+ ?L+?W)).
The resulting vectors better reflect the fea-tures weighted more highly in one class than oth-ers.
We report the top 15 features per class.The top features for the County method (Ta-ble 8) reveal a mixture of lexical variations aswell as geographical indicators, which act as prox-ies for race.
There are many Spanish words forLatino-American users, for example ?de?, ?la?, and?que.?
In addition there are some state names(?texas?, ?hawaii?
), part of city names (?san?
), andabbreviations (?sfo?
is the code for the San Fran-cisco airport).
Texas is 37.6% Hispanic-American,and San Francisco is 34.2% Asian-American.
Ref-erences to the photo-sharing site Instagram arefound to be strongly indicative of Latino users.This is further supported by a survey conductedby the Pew Research Internet Project,6whichfound that while an equal percentage of Whiteand Latino online adults use Twitter (16%), onlineLatinos were almost twice as likely to use Insta-gram (23% versus 12%).
Additionally, the term6http://www.pewinternet.org/files/2013/12/PIP_Social-Networking-2013.pdf13Black White Latino Asianfollow you texas camy NoDesc lol sanbe and la hawaiigot so de hawaiiup you que hithis can el httpain re de californiauniversity have no hahabout is la franciscoget university tx #hawaiiall haha instagram canigga are tx beachon justin san igsmh to en comniggas would god sfoTable 8: Top-weighted features for the regressionmodel trained on the County dataset.
Terms fromthe description field are in italics.Truth Predicted Top Featureswhite latino de, la, que, no, la, el, san,en, amp, mewhite black this, on, be, got, up, in,shit, at, the, allblack white you, and, to, you, the, is,so, of, have, reTable 9: Misclassified by the County method.?justin?
in the user profile description is a strongindicator of White users ?
an inspection of theCounty dataset reveals that this is largely in ref-erence to the pop musician Justin Bieber.
(Recallthat users typically do not enter their own namesin the description field.
)We find some similarities with the results ofEisenstein et al.
(2011) ?
e.g., the term ?smh?
(?shaking my head?)
is a highly-ranked term forAfrican-Americans.4.2 Error AnalysisWe sample a number of users who were misclas-sified, then identify the highest weighted features(using the dot product of the feature vector and pa-rameter vector).
Table 9 displays the top featuresof a sample of users in the Uniform dataset thatwere correctly classified by the Uniform methodbut misclassified by the County method.
Similarly,Table 10 shows examples that were misclassifiedby the Uniform approach but correctly classifiedTruth Predicted Top Featuresblack white makes, guys, thing, isn,am, again, haha, every-one, remember, veryblack white please, guys, snow, pretty,literally, isn, am, again,happen, midnightblack white makes, snow, pretty, lit-erally, am, again, happen,yay, beer, amazingTable 10: Misclassified by the classifier trained onthe Uniform dataset.by the County approach.One common theme across all models is that be-cause White is the most common class label, manycommon terms are correlated with it (e.g., the, is,of).
Thus, for users that use only very commonterms, the models tend to select the White label.Indeed, examining the confusion matrix revealsthat the most common type of error is to misclas-sify a non-White user as White.5 Conclusions and Future WorkOur results suggest that models fit on aggregate,geolocated social media data can be used estimateindividual user attributes.
While further analysisis needed to test how this generalizes to other at-tributes, this approach may provide a low-cost wayof inferring user attributes.
This in turn will bene-fit growing attempts to use social media as a com-plement to traditional polling methods ?
by quan-tifying the bias in a sample of social media users,we can then adjust inferences using approachessuch as survey weighting (Gelman, 2007).There are clear ethical concerns with how sucha capability might be used, particularly if it is ex-tended to estimate more sensitive user attributes(e.g., health status).
Studies such as this may helpelucidate what we reveal about ourselves throughour language, intentionally or not.In future work, we will consider richer userrepresentations (e.g., social media activity, socialconnections), which have also been found to beindicative of user attributes.
Additionally, we willconsider combining labeled and unlabeled data us-ing semi-supervised learning from label propor-tions (Quadrianto et al., 2009; Ganchev et al.,2010; Mann and McCallum, 2010).14ReferencesF Al Zamal, W Liu, and D Ruths.
2012.
Homophilyand latent attribute inference: Inferring latent at-tributes of twitter users from neighbors.
In ICWSM.Shlomo Argamon, Sushant Dhawle, Moshe Koppel,and James W. Pennebaker.
2005.
Lexical predictorsof personality type.
In In proceedings of the JointAnnual Meeting of the Interface and the Classifica-tion Society of North America.John D. Burger, John Henderson, George Kim, andGuido Zarrella.
2011.
Discriminating gender ontwitter.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,EMNLP ?11, pages 1301?1309, Stroudsburg, PA,USA.
Association for Computational Linguistics.Jonathan Chang, Itamar Rosenn, Lars Backstrom, andCameron Marlow.
2010. ePluribus: ethnicity onsocial networks.
In Fourth International AAAI Con-ference on Weblogs and Social Media.Aron Culotta.
2014.
Estimating county health statis-tics with twitter.
In CHI.Mark Dredze.
2012.
How social media will changepublic health.
IEEE Intelligent Systems, 27(4):81?84.Jacob Eisenstein, Noah A. Smith, and Eric P. Xing.2011.
Discovering sociolinguistic associations withstructured sparsity.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies - Vol-ume 1, HLT ?11, pages 1365?1374, Stroudsburg, PA,USA.
Association for Computational Linguistics.George Forman.
2008.
Quantifying counts andcosts via classification.
Data Min.
Knowl.
Discov.,17(2):164?206, October.Kuzman Ganchev, Joo Graca, Jennifer Gillenwater, andBen Taskar.
2010.
Posterior regularization for struc-tured latent variable models.
J. Mach.
Learn.
Res.,11:2001?2049, August.Daniel Gayo-Avello.
2011.
Don?t turn social mediainto another ?Literary digest?
poll.
Commun.
ACM,54(10):121?128, October.Andrew Gelman.
2007.
Struggles with survey weight-ing and regression modeling.
Statistical Science,22(2):153?164.Sharad Goel, Jake M Hofman, and M Irmak Sirer.2012.
Who does what on the web: A large-scalestudy of browsing behavior.
In ICWSM.Shyam Gopinath, Jacquelyn S. Thomas, and LakshmanKrishnamurthi.
2014.
Investigating the relationshipbetween the content of online word of mouth, adver-tising, and brand performance.
Marketing Science.Published online in Articles in Advance 10 Jan 2014.Gary King.
1997.
A solution to the ecological infer-ence problem: Reconstructing individual behaviorfrom aggregate data.
Princeton University Press.Wendy Liu and Derek Ruths.
2013.
What?s in a name?using first names as features for gender inference intwitter.
In AAAI Spring Symposium on AnalyzingMicrotext.Gideon S. Mann and Andrew McCallum.
2010.Generalized expectation criteria for semi-supervisedlearning with weakly labeled data.
J. Mach.
Learn.Res., 11:955?984, March.Alan Mislove, Sune Lehmann, Yong-Yeol Ahn, Jukka-Pekka Onnela, and J. Niels Rosenquist.
2011.
Un-derstanding the demographics of twitter users.
InProceedings of the Fifth International AAAI Con-ference on Weblogs and Social Media (ICWSM?11),Barcelona, Spain.Dong Nguyen, Noah A. Smith, and Carolyn P. Ros.2011.
Author age prediction from text using lin-ear regression.
In Proceedings of the 5th ACL-HLTWorkshop on Language Technology for CulturalHeritage, Social Scie nces, and Humanities, LaT-eCH ?11, pages 115?123, Stroudsburg, PA, USA.Association for Computational Linguistics.Brendan O?Connor, Ramnath Balasubramanyan,Bryan R. Routledge, and Noah A. Smith.
2010.From Tweets to polls: Linking text sentimentto public opinion time series.
In InternationalAAAI Conference on Weblogs and Social Media,Washington, D.C.F.
Pedregosa et al.
2011.
Scikit-learn: Ma-chine learning in Python.
Machine Learning Re-search, 12:2825?2830.
http://dl.acm.org/citation.cfm?id=2078195.Marco Pennacchiotti and Ana-Maria Popescu.
2011.A machine learning approach to twitter user classifi-cation.
In Lada A. Adamic, Ricardo A. Baeza-Yates,and Scott Counts, editors, ICWSM.
The AAAI Press.Novi Quadrianto, Alex J. Smola, Tibrio S. Caetano, andQuoc V. Le.
2009.
Estimating labels from label pro-portions.
J. Mach.
Learn.
Res., 10:2349?2374, De-cember.Delip Rao, David Yarowsky, Abhishek Shreevats, andManaswi Gupta.
2010.
Classifying latent user at-tributes in twitter.
In Proceedings of the 2Nd In-ternational Workshop on Search and Mining User-generated Contents, SMUC ?10, pages 37?44, NewYork, NY, USA.
ACM.Delip Rao, Michael J. Paul, Clayton Fink, DavidYarowsky, Timothy Oates, and Glen Coppersmith.2011.
Hierarchical bayesian models for latent at-tribute detection in social media.
In ICWSM.Sara Rosenthal and Kathleen McKeown.
2011.
Ageprediction in blogs: A study of style, content, and15online behavior in pre- and post-social media gen-erations.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguis-tics: Human Language Technologies - Volume 1,HLT ?11, pages 763?772, Stroudsburg, PA, USA.Association for Computational Linguistics.Jonathan Schler, Moshe Koppel, Shlomo Argamon,and James W Pennebaker.
2006.
Effects of ageand gender on blogging.
In AAAI 2006 Spring Sym-posium on Computational Approaches to AnalysingWeblogs (AAAI-CAAW), pages 06?03.H Andrew Schwartz, Johannes C Eichstaedt, Mar-garet L Kern, Lukasz Dziurzynski, Stephanie MRamones, Megha Agrawal, Achal Shah, MichalKosinski, David Stillwell, Martin E P Seligman,and Lyle H Ungar.
2013a.
Personality, gen-der, and age in the language of social media: theopen-vocabulary approach.
PloS one, 8(9):e73791.PMID: 24086296.H Andrew Schwartz, Johannes C Eichstaedt, Mar-garet L Kern, Lukasz Dziurzynski, Stephanie M Ra-mones, Megha Agrawal, Achal Shah, Michal Kosin-ski, David Stillwell, Martin EP Seligman, et al.2013b.
Characterizing geographic variation in well-being using tweets.
In Seventh International AAAIConference on Weblogs and Social Media.16
