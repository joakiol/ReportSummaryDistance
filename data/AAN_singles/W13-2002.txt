Proceedings of the BioNLP Shared Task 2013 Workshop, pages 8?15,Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational LinguisticsThe Genia Event Extraction Shared Task, 2013 Edition - OverviewJin-Dong Kim and Yue Wang and Yamamoto YasunoriDatabase Center for Life Science (DBCLS)Research Organization of Information and Systems (ROIS){jdkim|wang|yy}@dbcls.rois.ac.jpAbstractThe Genia Event Extraction task is orga-nized for the third time, in BioNLP SharedTask 2013.
Toward knowledge based con-struction, the task is modified in a num-ber of points.
As the final results, it re-ceived 12 submissions, among which 2were withdrawn from the final report.
Thispaper presents the task setting, data sets,and the final results with discussion forpossible future directions.1 IntroductionAmong various resources of life science, litera-ture is regarded as one of the most important typesof knowledge base.
Nevertheless, lack of explicitstructure in natural language texts prevents com-puter systems from accessing fine-grained infor-mation written in literature.
BioNLP Shared Task(ST) series (Kim et al 2009; Kim et al 2011a)is one of the community-wide efforts to addressthe problem.
Since its initial organization in 2009,BioNLP-ST series has published a number of fine-grained information extraction (IE) tasks moti-vated for bioinformatics projects.
Having solicitedwide participation from the community of naturallanguage processing, machine learning, and bioin-formatics, it has contributed to the production ofrich resources for fine-grained BioIE, e.g., TEES1(Bjo?rne and Salakoski, 2011), SBEP2 (McCloskyet al 2011) and EVEX3 (Van Landeghem et al2011).The Genia Event Extraction (GE) task is a sem-inal task of BioNLP-ST.
It was first organized asthe sole task of the initial 2009 edition of BioNLP-ST.
The task was originally designed and imple-mented based on the Genia event corpus (Kim et1https://github.com/jbjorne/TEES/wiki2http://nlp.stanford.edu/software/eventparser.shtml3http://www.evexdb.org/al., 2008b) which represented domain knowledgearound NF?B proteins.
There were also some ef-forts to explore the possibility of literature miningfor pathway construction (Kim et al 2008a; Odaet al 2008).
The GE task was designed to makesuch an effort a community-driven one by sharingavailable resources, e.g., benchmark data sets, andevaluation tools, with the community.In its second edition (Kim et al 2011b) orga-nized in BioNLP-ST 2011 (Kim et al 2011a), thedata sets were extended to include full text articles.The data sets consisted of two collections.
The ab-stract collection, that had come from the first edi-tion, was used again to measure the progress of thecommunity between 2009 and 2011 editions, andthe full text collection, that was newly created, wasused to measure the generalization of the technol-ogy to full text papers.In its third edition this year, while succeedingthe fundamental characteristics from its previouseditions, the GE task tries to evolve with the goalto make it a more ?real?
task toward knowledgebase construction.
The first design choice to ad-dress the goal is to construct the data sets fullywith recent full papers, so that the extracted piecesof information can represent up-to-date knowl-edge of the domain.
The abstract collection, thathad been already used twice (in 2009 and 2011), isremoved from official evaluation this time4.
Sec-ond, GE task subsumes the coreference task whichhas long been considered critical for improvementof event extraction performance.
It is implementedby providing coreference annotation in integrationwith event annotation in the data sets.The paper explains the task setting and data sets,presents the final results of participating systems,and discusses notable observations with conclu-sions.4However, if necessary, the online evaluation for the pre-vious editions of GE task may be used, which is available athttp://bionlp-st.dbcls.jp/GE/.8Event Type Primary Argument Secondary ArgumentGene expression Theme(Protein)Transcription Theme(Protein)Localization Theme(Protein) Loc(Entity)?Protein catabolism Theme(Protein)Binding Theme(Protein)+ Site(Entity)*Protein modification Theme(Protein), Cause(Protein/Event)?
Site(Entity)?Phosphorylation Theme(Protein), Cause(Protein/Event)?
Site(Entity)?Ubiquitination Theme(Protein), Cause(Protein/Event)?
Site(Entity)?Acetylation Theme(Protein), Cause(Protein/Event)?
Site(Entity)?Deacetylation Theme(Protein), Cause(Protein/Event)?
Site(Entity)?Regulation Theme(Protein/Event), Cause(Protein/Event)?
Site(Entity)?, CSite(Entity)?Positive regulation Theme(Protein/Event), Cause(Protein/Event)?
Site(Entity)?, CSite(Entity)?Negative regulation Theme(Protein/Event), Cause(Protein/Event)?
Site(Entity)?, CSite(Entity)?Table 1: Event types and their arguments for Genia Event Extraction task.
The type of each filler entityis specified in parenthesis.
Arguments that may be filled more than once per event are marked with ?+?,and optional arguments are with ??
?.2 Task settingThis section explains the task setting of the 2013edition of the GE task with a focus on changes toprevious editions.
For comprehensive explanation,readers are referred to Kim et al(2009).The changes made to the task setting are three-folds, among which two are about event typesto be extracted.
Table 1 shows the event typesand their arguments targeted in the 2013 edition.First, four new event types are added to the targetof extraction; the Protein modificationtype and its three sub-types, Ubiquitination,Acetylation, Deacetylation.
Second,The Protein modification types are modi-fied to be directly linked to causal entities, whichwas only possible through Regulation eventsin previous editions.The modifications were made based on analy-sis on preliminary annotation during preparationof the data sets: in recent papers on NF?B, dis-cussions on protein modification were observedwith non-trivial frequency.
However, in the end,it turned out that the influence of the above modi-fications was trivial in terms of the number of an-notated instances in the final data sets, as shownin section 3, after filtering out events on non-individual proteins, e.g., protein families, proteincomplexes.Third change made to the task setting is additionof coreference and part-of annotations to the datasets.
It is to address the observation from 2009edition that coreference structures and entity rela-tions often hide the syntactic paths between eventtriggers and their arguments, restricting the perfor-mance of event extraction.
In 2011, the Proteincoreference task and Entity Relation were orga-nized as sub-tasks, to explicitly address the prob-lem, but this time, coreference and part-of anno-tations are integrated in the GE task, to encour-age an integrative use of them for event extrac-tion.
Figure 1 shows an example of annotationwith coreference and part-of annotations5.
Notethat the event representation in the figure is re-lation centric6, which is different from the eventcentric representation of the default BioNLP-STformat.
The two representations are interchange-able, and the GE task provides data sets in bothformats, together with an automatic converter be-tween them.
Below is the corresponding annota-tion in the BioNLP-ST format:T8 Protein 933 938 TRAF1T9 Protein 940 945 TRAF2T10 Protein 947 952 TRAF3T11 Protein 958 963 TRAF6T12 Protein 1038 1042 CD40T41 Anaphora 1058 1072 These proteinsT48 Binding 1112 1119 bindingT49 Entity 1127 1143 cytoplasmic tailT13 Protein 1147 1151 CD40R1 Coreference Subject:T41 Object:T8R2 Coreference Subject:T41 Object:T9R3 Coreference Subject:T41 Object:T10R4 Coreference Subject:T41 Object:T11E4 Binding:T48 Theme:T8 Theme2:T13 Site2:T49E5 Binding:T48 Theme:T9 Theme2:T13 Site2:T49E6 Binding:T48 Theme:T10 Theme2:T13 Site2:T49E7 Binding:T48 Theme:T11 Theme2:T13 Site2:T49In the example, the event trigger, binding, de-notes four binding events, in which the four pro-teins, TRAF1, TRAF2, TRAF3, and TRAF6, bindto the protein, CD40, respectively, through thesite, cytoplasmic tail.
The links between the four5The example is taken from the file, PMC-3148254-01-Introduction.6PubAnnotation (http://pubannotation.org) format.9Figure 1: Annotation example with coreferences and part-of relationshipproteins and the event trigger are however veryhard to find, without being bridged by the demon-strative noun phrase (NP), These proteins.
In thecase, if the link between the demonstrative NP,These proteins and its four antecedents, TRAF1,TRAF2, TRAF3, and TRAF6, can be somehow de-tected, the remaining link, between the demonstra-tive NP and the trigger, may be detected by theirsyntactic connection.
A key point here is the dif-ferent characteristics of the two step links: de-tecting the former is rather semantic or discour-sal while the latter may be a more syntactic prob-lem.
Then, solving them using different processeswould make a sense.
To encourage an explorationinto the hypothesis, the coreference annotation isprovided in the training and development data sets.Based on the definition of event types, the en-tire task is divided into three sub-tasks addressingevent extraction at different levels of specificity:Task 1.
Core event extraction addresses the ex-traction of typed events together with theirprimary arguments.Task 2.
Event enrichment addresses the extrac-tion of secondary arguments that furtherspecify the events extracted in Task 1.Task 3.
Negation/Speculation detectionaddresses the detection of negations andspeculations over the extracted events.For more detail of the subtasks, readers are re-ferred to Kim et al(2011b).Item Training Devel TestArticles 10 10 14Words 54938 57907 75144Proteins 3571 4138 4359Entities 121 314 327Events 2817 3199 3348Gene expression 729 591 619Transcription 122 98 101Localization 44 197 99Protein catabolism 23 30 14Binding 195 376 342Protein modification 8 1 1Phosphorylation 117 197 161Ubiquitination 4 2 30Acetylation 0 3 0Deacetylation 0 5 0Regulation 299 284 299Positive regulation 780 883 1144Negative regulation 496 532 538Coreferences 178 160 197to Protein 152 123 169to Entity 5 6 6to Event 18 27 13to Anaphora 3 4 9Table 2: Statistics of annotations in training, de-velopment, and test sets3 Data PreparationAs discussed in section 1, for the 2013 edition, thedata sets are constructed fully with full text pa-pers.
Table 2 shows statistics of three data sets fortraining, development and test.
The data sets con-sist of 34 full text papers from the Open Accesssubset of PubMed Central.
The papers were re-trieved using lexical variants of the term, ?NF?B?as primary keyword, and ?pathway?
and ?regula-tion?
as secondary keywords.
The retrieved paperswere given to the annotators with higher priority10Item TIAB Intro.
R/D/C Methods Caption allWords 10483 25543 125172 59612 29085 263133Proteins 816 1507 9060 1797 2169 16427(Density: P / W) (7.78%) (5.90%) (7.24%) (3.01%) (7.46%) (6.24%)Prot.
Coreferences 18 89 267 5 33 445(Density: C / P) (2.21%) (5.91%) (2.95%) (0.28%) (1.52%) (2.71%)Events 510 902 6391 311 892 9364(Density: E / W) (4.87%) (3.53%) (5.11%) (0.52%) (3.07%) (3.56%)(Density: E / P) (62.50%) (59.85%) (70.54%) (17.31%) (41.12%) (57.00%)Gene expression 101 152 1265 125 220 1939Transcription 10 18 209 36 47 321Localization 19 47 191 8 41 340Protein catabolism 0 3 49 0 8 67Binding 29 158 572 15 92 913Protein modification 1 1 7 0 0 10Phosphorylation 27 38 347 19 35 475Ubiquitination 0 2 8 0 10 36Acetylation 0 3 0 0 0 3Deacetylation 0 5 0 0 0 5Regulation 67 76 625 7 66 882Positive regulation 167 286 2045 19 203 2807Negative regulation 89 113 1073 69 170 1566Table 3: Statistics of annotations in different sections of text: the Abstract column is of the abstractioncollection (1210 titles and abstracts), and the following columns are of full paper collection (14 fullpapers).
TIAB = title and abstract, Intro.
= introduction and background, R/D/C = results, discussions,and conclusions, Methods = methods, materials, and experimental procedures.
Some minor sections,supporting information, supplementary material, and synopsis, are ignored.
Density = relative density ofannotation (P/W = Protein/Word, E/W = Event/Word, and E/P = Event/Protein).Figure 2: Event distribution in different sectionsto newer ones.
Note that among 34 papers, 14were from the full text collection of 2011 editiondata sets, and 20 were newly collected this time.The annotation to the all 34 papers were producedby the same annotators who also produced anno-tations for the previous editions of GE task.The annotated papers are divided into the train-ing, development, and test data sets; 10, 10, and14, respectively.
Note that the size of the trainingdata set is much smaller than previous editions,in terms of number of words and events, whilethe size of the development and test data sets arecomparable to previous editions.
It is the conse-quence of a design choice of the organizers withthe notion that (1) relevant resources are substan-tially accumulated through last two editions, andthat (2) therefore the importance of training dataset may be reduced while the importance of devel-opment and test data sets needs to be kept.
Instead,participants may utilize, for example, the abstractcollection of the 2011 edition, of which the anno-tation was produced by the same annotators withalmost same principles.
As another example, thedata sets of the EPI task (Ohta et al 2011) alsomay be utilized for the newly added protein modi-fication events.Table 3 shows the statistics of annotated eventtypes in different sections of the full papers in thedata sets.
For the analysis, the sections are classi-fied to five groups as follows:?
The TIAB group includes the titles andabstracts.
In the GE-2011 data sets,the corresponding files match the pattern,PMC-*TIAB*.txt.?
The Intro group includes sectionsfor introduction, and background.
Thecorresponding files match the pattern,PMC-*@(-|._)@(I|Back)*.txt.11Team ?09 ?11 Task ExpertiseEVEX UTurku 123 2C+2BI+1BTEES-2.1 UTurku 123 2BIBioSEM TM-SCS 1-- 1C+1BINCBI CCP-BTMG 1-- 3BIDlutNLP 1-- 3CHDS4NLP 1-- 3CNICTANLM CCP-BTMG 1-3 6CUSheff 1-- 2CUZH UZurich 1-- 6CHCMUS HCMUS 1-- 4CTable 4: Team profiles: The ?09 and ?11 columnsshow the predecessors in 2009 and 2011 editions.In Expertise column, C=Computer Scientist,BI=Bioinformatician, B=Biologist, L=Linguist?
The R/D/C group includes sectionson results, discussions, and conclu-sions.
The files match the pattern,PMC-*@(-|._)@(R|D|Conc)*.txt?
The Methods group includes sections onmethods, materials, and experimental pro-cedures.
The files match the pattern,PMC-*@(-|._)@(Met|Mat|MAT|E)*.txt?
The Caption group includes the captions oftables and figures.
The corresponding filesmath the pattern, PMC-*aption*.txt.Figure 2 illustrates the different distribution ofannotated event types in the five section groups.It shows that the Methods group has signifi-cantly different distribution of annotated events,confirming a similar observation reported in Kimet al(2011b).4 ParticipationThe GE task received final submissions from 12teams, among which 2 were withdrawn from finalreport.
Table 4 summarizes the teams.
Unfortu-nately, the subtasks 2 and 3 did not met a largeparticipation.Table 5 profiles the participating systems.
Thesystems are roughly grouped into SVM-basedpipeline (EVEX, TEES-2.1, and DlutNLP),rule-based pipeline (BioSEM and UZH), mixedpipeline (USheff and HCMUS), joint patternmatching (NCBI and NICTANLM), and joint SVM(HDS4NLP) systems.
In terms of use of ex-ternal resources, 5 teams (EVEX, TEES-2.1,NCBI, DlutNLP, and USheff) utilized data setsfrom 2011 edition, and two teams (HDS4NLP andNICTANLM) utilized independent resources, e.g.,UniProt (Bairoch et al 2005), IntAct (Kerrien etal., 2012), and CRAFT (Verspoor et al 2012).5 Results and DiscussionsTable 6 shows the final results of subtask 1.
Over-all EVEX, TEES-2.1, and BioSEM show thebest performance only with marginal differencebetween them.
In detail, the performance ofBioSEM is significantly different from EVEX andTEES-2.1: (1) while BioSEM show the best per-formance with Binding and Protein modificationevents, EVEX and TEES-2.1 show the best per-formance with Regulation events which takes thelargest portion of annotation in data sets; and (2)while the performance of EVEX and TEES-2.1is balanced over recall and precision, BioSEM isbiased for precision, which is a typical feature ofrule-based systems.
It is also notable that BioSEMhas achieved a near best performance using onlyshallow parsing.
Although it is not shown in thetable, NCBI is the only system which producedUbiquitination events, which is interpretedas a result of utilizing 2011-EPI data sets (Ohta etal., 2011) for the system development.Table 7 shows subtask 1 final results only withinTIAB sections.
It shows that the systems de-veloped utilizing previous resources, e.g., 2011data sets, and EVEX, perform better for titles andabstracts, which makes sense because those re-sources are title and abstract-centric.Tables 8 and 9 show evaluation results withinMethods and Captions section groups, respec-tively.
All the systems show their worst per-formance in the two section groups.
Especiallythe drop of performance with regulation events ishuge.
Note the two section groups also show sig-nificantly different event distribution compared toother section groups (see section 3).
It suggeststhat language expression in the two section groupsmay be quite different from other sections, and anextensive examination is required to get a reason-able performance in the sections.Table 10 and 11 show final results of Task 2(Event enrichment) and 3 (Negation/Speculationdetection), respectively, which unfortunately didnot meet a large participation.6 ConclusionsIn its third edition, the GE task is fully changedto a full text paper centric task, while the onlineevaluation service on the abstract-centric data sets12NLP Task Other resourcesTeam Lexical Proc.
Syntactic Proc.
Trig.
Arg.
group Dic.
OtherEVEX Porter McCCJ SVM SVM SVM S. cues EVEXTEES-2.1 Porter McCCJ SVM SVM SVM S. cuesBioSEM OpenNLP, LingPipe OpenNLP(shallow) dic rules rulesNCBI MedPost, BioLemm McCCJ Subgraph Isomorphism rules 2011 GE / EPIDlutNLP Porter, GTB-tok McCCJ SVM SVM rules 2011 GEHDS4NLP CNLP, Morpha McCCJ SVM SVM UniProt, IntActNICTANLM ClearParser Subgraph Isomorphism rules CRAFT, EVEXUSheff Porter, LingPipe Stanford dic SVM SVM, rules 2011 GEUZH Porter, Morpha, LingPipe LTT2, Pro3Gres dic.
MaxEnt rules rulesHCMUS SnowBall McCCJ dic, SVM rules, SVM rulesTable 5: System profiles: SnowBall=SnowBall Stemmer, CNLP=Stanford CoreNLP (tokenization),McCCJ=McClosky-Charniak-Johnson Parser, Stanford=Stanford Parser, S.=Speculation, N.=NegationTeam Simple Event Binding Prot-Mod.
Regulation AllEVEX 73.83 / 79.56 / 76.59 41.14 / 44.77 / 42.88 61.78 / 69.41 / 65.37 32.41 / 47.16 / 38.41 45.44 / 58.03 / 50.97TEES-2.1 74.19 / 79.64 / 76.82 42.34 / 44.34 / 43.32 63.87 / 69.32 / 66.49 33.08 / 44.78 / 38.05 46.17 / 56.32 / 50.74BioSEM 67.71 / 86.90 / 76.11 47.45 / 52.32 / 49.76 69.11 / 80.49 / 74.37 28.19 / 49.06 / 35.80 42.47 / 62.83 / 50.68NCBI 72.99 / 72.12 / 72.55 37.54 / 41.81 / 39.56 64.92 / 77.02 / 70.45 24.74 / 55.61 / 34.25 40.53 / 61.72 / 48.93DlutNLP 69.15 / 80.56 / 74.42 40.84 / 44.16 / 42.43 62.83 / 77.42 / 69.36 26.49 / 43.46 / 32.92 40.81 / 57.00 / 47.56HDS4NLP 75.27 / 83.27 / 79.07 41.74 / 33.74 / 37.32 70.68 / 75.84 / 73.17 16.67 / 30.86 / 21.64 37.11 / 51.19 / 43.03NICTANLM 73.59 / 57.67 / 64.66 32.13 / 31.10 / 31.61 42.41 / 72.97 / 53.64 21.60 / 47.14 / 29.63 36.99 / 50.68 / 42.77USheff 54.50 / 80.07 / 64.86 31.53 / 46.88 / 37.70 39.79 / 92.68 / 55.68 21.14 / 52.69 / 30.18 31.69 / 63.28 / 42.23UZH 60.26 / 77.47 / 67.79 22.22 / 28.03 / 24.79 62.30 / 70.83 / 66.30 11.06 / 31.02 / 16.31 27.57 / 51.33 / 35.87HCMUS 67.47 / 60.24 / 63.65 38.74 / 26.99 / 31.81 64.92 / 57.67 / 61.08 19.60 / 19.93 / 19.76 36.23 / 33.80 / 34.98Table 6: Evaluation results (recall / precision / f-score) of Task 1.
Some notable figures are emphasizedin bold.is kept maintained.
Unfortunately, the corefer-ence annotation, which has been integrated in theevent annotation in the data sets, was not exploitedby the participants, during the official shared taskperiod.
An analysis shows that the performanceof systems significantly drops in the Methods andCaptions sections, suggesting for an extensive ex-amination in the sections.As usual, after the official shared task period,the GE task is maintaining an online evaluationthat can be freely accessed by anyone but witha time limitation; once in 24 hours per a per-son.
With a few new features that are introducedin 2013 editions but are not fully exploited bythe participants, the organizers solicit participantsto continuously explore the task using the onlineevaluation.
The organizers are also planning toprovide more resources to the participants, basedon the understanding that interactive communica-tion between organizers and participants is impor-tant for progress of the participating systems andalso the task itself.ReferencesAmos Bairoch, Rolf Apweiler, Cathy H. Wu,Winona C. Barker, Brigitte Boeckmann, SerenellaFerro, Elisabeth Gasteiger, Hongzhan Huang, Ro-drigo Lopez, Michele Magrane, Maria J. Mar-tin, Darren A. Natale, Claire O?Donovan, NicoleRedaschi, and Lai-Su L. Yeh.
2005.
The universalprotein resource (uniprot).
Nucleic Acids Research,33(suppl 1):D154?D159.Jari Bjo?rne and Tapio Salakoski.
2011.
Generaliz-ing biomedical event extraction.
In Proceedings ofBioNLP Shared Task 2011 Workshop, pages 183?191, Portland, Oregon, USA, June.
Association forComputational Linguistics.Samuel Kerrien, Bruno Aranda, Lionel Breuza, AlanBridge, Fiona Broackes-Carter, Carol Chen, Mar-garet Duesbury, Marine Dumousseau, Marc Feuer-mann, Ursula Hinz, Christine Jandrasits, Rafael C.Jimenez, Jyoti Khadake, Usha Mahadevan, PatrickMasson, Ivo Pedruzzi, Eric Pfeiffenberger, PabloPorras, Arathi Raghunath, Bernd Roechert, SandraOrchard, and Henning Hermjakob.
2012.
The in-tact molecular interaction database in 2012.
NucleicAcids Research, 40(D1):D841?D846.Jin-Dong Kim, Tomoko Ohta, Kanae Oda, and Jun?ichiTsujii.
2008a.
From text to pathway: corpus annota-tion for knowledge acquisition from biomedical lit-erature.
In Proceedings of the 6th Asia Pacific Bioin-formatics Conference, Series on Advances in Bioin-13formatics and Computational Biology, pages 165?176.
Imperial College Press.Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii.2008b.
Corpus annotation for mining biomedicalevents from lterature.
BMC Bioinformatics, 9(1):10.Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-nobu Kano, and Jun?ichi Tsujii.
2009.
Overviewof BioNLP?09 Shared Task on Event Extraction.In Proceedings of Natural Language Processingin Biomedicine (BioNLP) NAACL 2009 Workshop,pages 1?9.Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, RobertBossy, and Jun?ichi Tsujii.
2011a.
Overview ofBioNLP Shared Task 2011.
In Proceedings ofthe BioNLP 2011 Workshop Companion Volume forShared Task, Portland, Oregon, June.
Associationfor Computational Linguistics.Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-nori Yonezawa.
2011b.
Overview of the GeniaEvent task in BioNLP Shared Task 2011.
In Pro-ceedings of the BioNLP 2011 Workshop CompanionVolume for Shared Task, Portland, Oregon, June.
As-sociation for Computational Linguistics.David McClosky, Mihai Surdeanu, and ChristopherManning.
2011.
Event extraction as dependencyparsing.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguis-tics: Human Language Technologies, pages 1626?1635, Portland, Oregon, USA, June.
Association forComputational Linguistics.Kanae Oda, Jin-Dong Kim, Tomoko Ohta, DaisukeOkanohara, Takuya Matsuzaki, Yuka Tateisi, andJun?ichi Tsujii.
2008.
New challenges for text min-ing: Mapping between text and manually curatedpathways.Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsu-jii.
2011.
Overview of the Epigenetics and Post-translational Modifications (EPI) task of BioNLPShared Task 2011.
In Proceedings of the BioNLP2011 Workshop Companion Volume for Shared Task,Portland, Oregon, June.
Association for Computa-tional Linguistics.Sofie Van Landeghem, Filip Ginter, Yves Van de Peer,and Tapio Salakoski.
2011.
Evex: A pubmed-scaleresource for homology-based generalization of textmining predictions.
In Proceedings of BioNLP 2011Workshop, pages 28?37, Portland, Oregon, USA,June.
Association for Computational Linguistics.Karin Verspoor, Kevin Cohen, Arrick Lanfranchi,Colin Warner, Helen Johnson, Christophe Roeder,Jinho Choi, Christopher Funk, Yuriy Malenkiy,Miriam Eckert, Nianwen Xue, William Baumgart-ner, Michael Bada, Martha Palmer, and LawrenceHunter.
2012.
A corpus of full-text journal articlesis a robust evaluation tool for revealing differencesin performance of biomedical natural language pro-cessing tools.
BMC Bioinformatics, 13(1):207.14Team Simple Event Binding Prot-Mod.
Regulation AllEVEX 91.67 / 88.00 / 89.80 55.56 / 62.50 / 58.82 85.71 / 75.00 / 80.00 51.18 / 59.09 / 54.85 62.83 / 68.18 / 65.40TEES-2.1 91.67 / 88.00 / 89.80 55.56 / 62.50 / 58.82 85.71 / 75.00 / 80.00 51.18 / 57.02 / 53.94 62.83 / 66.67 / 64.69NCBI 81.25 / 79.59 / 80.41 55.56 / 45.45 / 50.00 85.71 / 66.67 / 75.00 37.01 / 67.14 / 47.72 50.79 / 69.78 / 58.79BioSEM 83.33 / 88.89 / 86.02 66.67 / 66.67 / 66.67 85.71 / 75.00 / 80.00 35.43 / 54.22 / 42.86 50.79 / 66.90 / 57.74DlutNLP 87.50 / 93.33 / 90.32 44.44 / 50.00 / 47.06 85.71 / 85.71 / 85.71 37.01 / 51.09 / 42.92 51.83 / 65.13 / 57.73USheff 81.25 / 88.64 / 84.78 44.44 / 57.14 / 50.00 71.43 / 71.43 / 71.43 29.13 / 56.06 / 38.34 44.50 / 68.55 / 53.97NICTANLM 93.75 / 57.69 / 71.43 22.22 / 25.00 / 23.53 42.86 /100.00 / 60.00 29.92 / 49.35 / 37.25 46.07 / 53.01 / 49.30HDS4NLP 93.75 / 90.00 / 91.84 66.67 / 54.55 / 60.00 85.71 / 85.71 / 85.71 19.69 / 31.65 / 24.27 42.93 / 55.78 / 48.52HCMUS 93.75 / 69.23 / 79.65 33.33 / 27.27 / 30.00 71.43 / 41.67 / 52.63 27.56 / 25.36 / 26.42 46.07 / 38.94 / 42.21UZH 72.92 / 79.55 / 76.09 44.44 / 57.14 / 50.00 71.43 / 71.43 / 71.43 11.02 / 32.56 / 16.47 30.37 / 57.43 / 39.73Table 7: Evaluation results (recall / precision / f-score) of Task 1 in titles and abstracts.
Some notablefigures are emphasized in bold.Team Simple Event Binding Prot-Mod.
Regulation AllBioSEM 70.83 / 90.44 / 79.44 48.24 / 53.93 / 50.93 74.17 / 82.41 / 78.07 28.74 / 51.25 / 36.83 42.97 / 64.90 / 51.70EVEX 73.51 / 83.26 / 78.08 43.72 / 47.80 / 45.67 66.67 / 66.12 / 66.39 32.79 / 46.79 / 38.56 45.29 / 58.05 / 50.88TEES-2.1 74.09 / 83.37 / 78.46 43.72 / 47.80 / 45.67 66.67 / 65.04 / 65.84 33.24 / 44.48 / 38.04 45.70 / 56.34 / 50.46NCBI 74.28 / 75.59 / 74.93 38.19 / 45.24 / 41.42 67.50 / 81.82 / 73.97 24.69 / 55.46 / 34.17 40.01 / 63.56 / 49.11DlutNLP 70.06 / 84.49 / 76.60 39.20 / 44.32 / 41.60 67.50 / 74.31 / 70.74 27.78 / 43.23 / 33.83 41.01 / 56.70 / 47.60NICTANLM 75.24 / 57.14 / 64.95 35.68 / 41.76 / 38.48 52.50 / 76.83 / 62.38 22.33 / 46.83 / 30.24 37.73 / 52.30 / 43.84USheff 56.81 / 80.43 / 66.59 32.66 / 48.15 / 38.92 45.00 / 94.74 / 61.02 21.67 / 53.55 / 30.85 32.27 / 63.93 / 42.89HDS4NLP 76.20 / 84.65 / 80.20 41.21 / 38.14 / 39.61 75.83 / 75.21 / 75.52 16.58 / 30.16 / 21.40 36.19 / 51.26 / 42.42UZH 63.53 / 78.25 / 70.13 23.12 / 28.75 / 25.63 66.67 / 74.07 / 70.18 10.61 / 29.39 / 15.59 27.36 / 50.89 / 35.58HCMUS 67.18 / 62.84 / 64.94 38.19 / 28.15 / 32.41 67.50 / 61.83 / 64.54 19.45 / 20.11 / 19.78 35.09 / 33.95 / 34.51Table 8: Evaluation results (recall / precision / f-score) of Task 1 in Methods section group.
Some notablefigures are emphasized in bold.Team Simple Event Binding Prot-Mod.
Regulation AllTEES-2.1 76.67 / 67.65 / 71.88 53.19 / 46.30 / 49.50 60.61 / 76.92 / 67.80 22.68 / 39.29 / 28.76 43.41/53.74 / 48.02BioSEM 60.00 / 78.26 / 67.92 68.09 / 58.18 / 62.75 69.70 / 82.14 / 75.41 23.20 / 34.35 / 27.69 42.31/54.42 / 47.60EVEX 76.67 / 67.65 / 71.88 53.19 / 46.30 / 49.50 48.48 / 72.73 / 58.18 21.13 / 39.81 / 27.61 41.48/53.74 / 46.82DlutNLP 70.00 / 67.02 / 68.48 55.32 / 48.15 / 51.49 57.58 / 79.17 / 66.67 18.04 / 46.67 / 26.02 39.29/57.89 / 46.81NCBI 80.00 / 58.54 / 67.61 40.43 / 41.30 / 40.86 66.67 / 70.97 / 68.75 14.95 / 44.62 / 22.39 39.01/53.58 / 45.15HDS4NLP 78.89 / 78.02 / 78.45 48.94 / 29.49 / 36.80 66.67 / 68.75 / 67.69 06.19 / 14.63 / 08.70 35.16/45.23 / 39.57UZH 57.78 / 68.42 / 62.65 23.40 / 26.19 / 24.72 69.70 / 74.19 / 71.88 12.89 / 43.10 / 19.84 30.49/53.62 / 38.88USheff 47.78 / 74.14 / 58.11 36.17 / 45.95 / 40.48 30.30 /100.00 / 46.51 13.40 / 45.61 / 20.72 26.37/59.26 / 36.50NICTANLM 75.56 / 53.12 / 62.39 40.43 / 27.94 / 33.04 18.18 / 54.55 / 27.27 11.34 / 36.67 / 17.32 31.59/43.07 / 36.45HCMUS 73.33 / 52.80 / 61.40 53.19 / 25.51 / 34.48 63.64 / 53.85 / 58.33 15.46 / 17.96 / 16.62 39.01/33.10 / 35.81Table 9: Evaluation results (recall / precision / f-score) of Task 1 in Captions section group.
Some notablefigures are emphasized in bold.Team Site-Binding Site-Phosphorylation Loc-Localization TotalTEES-2.1 31.37 / 56.14 / 40.25 37.21 / 82.05 / 51.20 36.67 / 78.57 / 50.00 22.03 / 61.90 / 32.50EVEX 31.37 / 56.14 / 40.25 32.56 / 80.00 / 46.28 36.67 / 78.57 / 50.00 20.90 / 61.67 / 31.22Table 10: Evaluation results (recall / precision / f-score) of Task 2Team Negation Speculation TotalTEES-2.1 21.68 / 36.84 / 27.30 18.46 / 33.96 / 23.92 19.53 / 35.59 / 25.22EVEX 20.98 / 38.03 / 27.04 18.46 / 32.73 / 23.61 19.82 / 34.41 / 25.15NICTANLM 15.38 / 32.76 / 20.94 14.36 / 34.15 / 20.22 14.79 / 33.57 / 20.54Table 11: Evaluation results (recall / precision / f-score) of Task 315
