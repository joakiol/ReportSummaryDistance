Attaching Multiple Prepositional Phrases: GeneralizedBacked-off EstimationPaola Mer loIRCS-U.
of PennsylvaniaLATL-Univers i ty  of Geneva2 rue de Candol le1204 Geneva, Switzerlandmerlo~latl, unige, chMatthew W. CrockerCentre for Cognit ive ScienceUniversity of Edinburgh2 Buccleuch Place,Edinburgh,  UK EH8 9LWmwc?cogsci ,  ed.
ac .
ukCathy  Ber thouzozLATLUniversity of Geneva2 rue de Candol le1204 Geneva, Switzer landberthouzoz~lat i. unige, chAbst ractThere has recently been considerable inter-est in the use of lexically-based statisticaltechniques to resolve prepositional phraseattachments.
To our knowledge, however,these investigations have only consideredthe problem of attaching the first PP, i.e.,in a IV NP PP\] configuration.
In thispaper, we consider one technique whichhas been successfully applied to this prob-lem, backed-off estimation, and demon-strate how it can be extended to dealwith the problem of multiple PP attach-ment.
The multiple PP attachment intro-duces two related problems: sparser data(since multiple PPs are naturally rarer),and greater syntactic ambiguity (more at-tachment configurations which must be dis-tinguished).
We present and algorithmwhich solves this problem through re-useof the relatively rich data obtained fromfirst PP training, in resolving subsequentPP attachments.1 In t roduct ionAmbiguity is the most specific feature of natural an-guages, which sets them aside from programminglanguages, and which is at the root of the difficultyof the parsing enterprise, pervading languages at alllevels: lexical, morphological, syntactic, semanticand pragmatic.
Unless clever techniques are devel-oped to deal with ambiguity, the number of possibleparses for an average sentence (20 words) is simplyintractable.
In the case Of prepositional phrases, theexpansion of the number of possible analysis is theCatalan number series, thus the number of possibleanalyses grows with a function that is exponentialin the number of Prepositional Phrase (Church andPatil, 1982).
One of the most interesting topics ofdebate at the moment, is the use of frequency in-formation for automatic syntactic disambiguation.As argued in many pieces of work in the AI tra-dition (Marcus, 1980; Crain and Steedman, 1985;Altmann and Steedman, 1988; Hirst, 1987), the ex-act solution of the disambiguation problem requirescomplex reasoning and high level syntactic and se-mantic knowledge.
However, current work in part-of-speech tagging has succeeded in showing that itis possible to carve one particular subproblem andsolve it by approximation - -  using statistical tech-niques - -  independently of the other levels of com-putation.In this paper we consider the problem of prepo-sitional phrase (PP) ambiguity.
While there havebeen a number of recent studies concerning the useof statistical techniques for resolving single PP at-tachments, i.e.
in constructions of the form \[V NPPP\], we are unaware of published work which appliesthese techniques to the more general, and patho-logical, problem of multiple PPs, e.g.
IV NP PP1PP2 ...\].
In particular, the multiple PP attachmentproblem results in sparser data which must be usedto resolve greater ambiguity: a strong test for anyprobabilistic approach.We begin with an overview of techniques whichhave been used for PP attachment disambiguation,and then consider how one of the most successfulof these, the backed-off estimation technique, canbe applied to the general problem of multiple PPattachment.2 Ex is t ing  Mode ls  o f  A t tachmentAttempts to resolve the problem of PP attachmentin computational linguistics are numerous, but theproblem is hard and success rate typically dependson the domain of application.
Historically, the shiftfrom attempts to resolve the problem completely,by using heuristics developed using typical AI tech-niques (Jensen and Binot, 1987; Marcus, 1980; Crainand Steedman, 1985; Altmann and Steedman, 1988)has left the place for attempts to solve the problemby less expensive means, even if only approximately.As shown by many psycholinguistic and practical149studies (Ford et al, 1982; Taraban and McClelland,1988; Whittemore t al., 1990), lexical informationis one of the main cues to PP attachment disam-biguation.In one of the earliest attempts to resolve the prob-lem of PP attachment ambiguity using lexical mea-sures, Hindle and Pmoth (1993) show that a measureof mutual information limited to lexical associationcan correctly resolve 80% of the cases of PP attach-ment ambiguity, confirming the initial hypothesisthat lexical information, in particular co-occurrencefrequency, is central in determining the choice of at-tachment.The same conclusion is reached by Brill andResnik (1994).
They apply transformation-basedlearning (Brill, 1993) to the problem of learning dif-ferent patterns of PP attachment.
After acquiring471 patterns of PP attachment, he parser can cor-rectly resolve approximately 80% of the ambiguity.If word classes (Resnik, 1993) are taken into account,only 266 rules are needed to perform at 80% accu-racy.Magerman and Marcus (1991) report 54/55 cor-rect PP attachments for Pearl, a probabilistic hartparser, with Earley style prediction, that integrateslexical co-occurrence knowledge into a probabilisticcontext-free grammar.
The probabilities of the rulesare conditioned on the parent rule and on the tri-gram centered at the first input symbol that wouldbe covered by the rule.
Even if the parser has beentested only in the direction giving domain, where thebehaviour of prepositions i very consistent, it showsthat a mixture of lexical and structural informationis needed to solve the problem successfully.Collins and Brooks (1995) propose a4-gram modelfor PP disambiguation which exploits backed-off es-timation to smooth null events (see next section).Their model achieves 84.5% accuracy.
The authorspoint out that prepositions are the most informativeelement in the tuple, and that taking low frequencyevents into account improves performance by sev-eral percentage points.
In other words, in solvingthe PP attachment problem, backing-off is not ad-vantageous unless the tuple that is being tested isnot present in the training set (it has zero counts).Moreover, tuples that contain prepositions are themost informative.The second result is roughly confirmed by Brilland Resnik, (ignoring the importance of n2 when itis a temporal modifier, such as yesterday, today).
Intheir work, the top 20 transformations learned areprimarily based on specific prepositions.3 Back-o f f  Es t imat ionThe PP attachment model presented by Collins andBrooks (1995) determines the most likely attach-ment for a particular prepositional phrase by esti-mating the probability of the attachment.
We let Crepresent the attachment event, where C = 1 indi-cates that the PP attaches to the verb, and C = 2indicates attachment to the object NP.
The attach-ment is conditioned by the relevant head words, a4-gram, of the VP.
* Tuple format: (C, v, nl, p, n2)?
So:  John read \[\[the article\] \[about he budget\]\]?
Is encoded as: (2, read, article, about, budget)Using a simple maximal likelihood approach,the best attachment for a particular input tuple(v,nl,p,n2) can now be determined from the trainingdata via the following equation:argmaxi 15(C = ilv, nl,  p, n2) = f(i, v, nl, p, n2)f(v, nl,  p, n2)(1)Here f denotes the frequency with which a partic-ular tuple occurs.
Thus, we can estimate the proba-bility for each configuration 1 < i < 2, by countingthe number of times the four head words were ob-served in that configuration, and dividing it by thetotal number of times the 4-tuple appeared in thetraining set.While the above equation is perfectly valid in the-ory, sparse data means it is rather less useful in prac-tice.
That is, for a particular sentence containing aPP attachment ambiguity, it is very likely that wewill never have seen the precise (v,nl,p,n2) quadru-ple before in the training data, or that we will haveonly seen it rarely.
1 To address this problem, theyemploy backed-off estimation when zero counts oc-cur in the training data.
Thus if f(v, nl ,p, n2) iszero, they 'back-off' to an alternative stimation of/~ which relies on 3-tuples rather than 4-tuples:/Sa(C = ilv, nl ,p, n2) =f(i, v, nl, p) + f(i, v, p, ,72) + f(i, nl, p, n2)(2)f(v, nl, p) + f(v, p, n2) + f (n l ,  p, n2)Similarly, if no 3-tuples exist in the training data,they back-off urther:i52(C = i\[v, nl,p, n2)= (3)f( i ,v,p) + f(i, nl,p) + f( i ,p, n2)f(v, p) + f (n l ,  p) + f(p, n2)i51(C = ilv, nl, p, n2) - f(i, p) f(v) (4)The above equations incorporate the proposal byCollins and Brooks that only tuples including thepreposition should be  considered, following theirresults that the preposition is the most informa-tive lexical item.
Using this technique, Collins andBrooks achieve an overall accuracy of 84.5%.aThough as Collins and Brooks point out, this is lessof an issue since even low counts are still useful.1504 The  Mu l t ip le  PP  At tachmentP rob lemPrevious work has focussed on the problem of singlePP attachment, in configurations of the form IV NPPP\] where both the NP and the PP are assumedto be attached within the VP.
The algorithm pre-sented in the previous section, for example, simplydetermines the maximally likely attachment event(to NP or VP) based on the supervised trainingprovided by a parsed corpus.
The broader value ofthis approach, however, remains uspect until it canbe demonstrated to apply more generally.
We nowconsider how this approach - and the use of lexicalstatistics in general - might be naturally extendedto handle the more difficult problem of multiple PPattachment.
In particular, we investigate the PPattachment problem in cases containing two PPs,\[V NP PP1 PP2\], and three PPs, \[V NP PP1 PP2PP3\], with a view to determining whether n-grambased parse disambiguation models which use thebacked-off estimate can be usefully applied.
Mul-tiple PP attachment presents two challenges to theapproach:1.
For a single PP, the model must make a choicebetween two structures.
For multiple PPs, thespace of possible structural configurations in-creases dramatically, placing increased emandson the disambiguation technique.2.
Multiple PP structures are less frequent, andcontain more words, than single PP structures.This substantially increases the sparse dataproblems when compared with the single PPattachment case.4.1 Mater ia l s  and  MethodTo carry out the investigation, training and test datawere obtained from the Penn Tree-bank, using thetgrep  tools to extract uples for 1-PP, 2-PP, and 3-PP cases.
For the single PP study, VP attachmentwas coded as 1 and NP attachment was coded as2.
A database of quadruples of the form (configura-tion, v,n,p) was then created.
The table below showsthe two configurations and their frequencies in thecorpus.Configuration Structure Counts1 \[vpNP PP \].
77402 \[vP \[NPPP \]\] 12223The same procedure was used to create a databaseof 6-tuples (conflguratwn, v, nl,pl,n2,p2) for the at-tachment of 2 PPs.
The values for the configurationvaries over a range 1..5, corresponding to the 5 gram-matical structures possible for 2 PPs, shown and ex-emplified below with their counts in the corpus.
22We did not consider the left-recursive NP structurefor the 2 PP (or indeed 3 PP) cases.
Checking the fre-Config12345Structure Counts\[vpV NP PP PP\] 535\[veV \[NpNP PP\] PP\] 1160\[vpV \[NP\[PPP \[NpNP PP \]\]\]\] 1394\[vpV NP \[pp\[NpNP PP\]\]\] 1055\[vpV \[NpNP PP PP\]\] 5391.
The agency said it will keep the debt  underrev iew for possible further downgrade.2.
Penney decided to extend  its invo lvementw i th  the service for at least five years.3.
The bill was then sent back to the House toresolve the question of how to address  budgetl imits  on credit a l locat ions for  the FederalHousing Administration.4.
Sears officials insist they don't intend to aban-don the everyday pricing approach  in the faceof  the poor results.5.
Mr. Ridley hinted at this motive in answer-ing quest ions  f rom members of Par l iamentafter his announcementFinally, a database of 8-tuples (configura-tion, v, nl,pl, n2,p2,n3,p3) was created for 3 PPs.The value of the configuration varies over a range1..14, corresponding to the 14 structures possible for3 PPs, shown in Table 1 with their counts in the cor-pus.The above datasets were then split into trainingand test sets by automatically extracting stratifiedsamples.
For PP1, we extracted quadruples of about5% of the total (1014/19963).
We then created a testset for PP2 which is a subset of the PP1 test set, andapproximately 10% of the 2 PP tuples (464/4683).Similarly, the test set for PP3 is a subset of the PP2test set of approximately 10% (94/907).
It is impor-tant that the test sets are subsets to ensure that, e.g.,a PP2 test case doesn't appear in the PP1 trainingset, since the PP1 data is used by our algorithm toestimate PP2 attachment, and similarly for the PP3test set.4.2 Does D is tance  Mat ter?In exploring multiple PP attachment, it seems natu-ral to investigate the effects of the distance of the PPfrom the verb.
The following table reports accuracyof noun-attachment, when the attachment decisionis conditioned only on the preposition and on thedistance - in  other words, when estimating 15(lip, d)where 1 is the coding of the attachment to the noun,p is the preposition and d = {1,2, 3}.
3quency of their occurrences revealed that there were only2 occurrences of\[vP \[NP \[NP \[NPPP\] PP\]\]\] structures in thecorpus.3These figures are to be taken only as an indicationof a trend, as they represent the accuracy obtained by151Configuration1234567891011121314Structure\[vpV NP PP PP PP \]\[vvV\[vvV\[veV\[vvV\[vvV\[vvV\[vvV\[vpV\[vvV\[vvV\[vwV\[vvV\[vvVCounts15NP PP \[ppP \[NpPP \]\]\]\[NpNP PP\] PP PP \]\[NpNP PP\] \[ppp \[NPPP \]\]\]\[NP\[PPP \[NvNF PP \]\]\] FP\]\[NP\[PPP \[NpNP PP \]\]PP \]\]\[Np\[ppP \[NpNP PP PP \]\]\]\]\[NP\[PPP \[NpNP \[ppP \[NPPP \]\]\]\]\]\]NP \[pp\[NpNP PP\]\] PP \]NP \[pp\[NpNP PP PP \]\]\]NP \[pp\[NpNP \[ppP \[NpPP \]\]\]\]\]\[NpNP PF PP\] PP \]\[NpNF PP PP PP \]\]\[NpNP PP \[ppP \[NPPP \]\]\]\]8663168813147142473480202172Table 1: Corpus counts for the 14 structures possible for 3-PP sequences.1 PP 2PP  3PP  Total AllCount 20299 4711 939 25949 25949Correct 15173 3525 755 19453 19349% 74.7 74.8 80.4 75 74.5It can be seen from these figures that condition-mg the attachment according to both prepositionand distance results in only a minor improvementin performance, mostly because separating the bi-ases according to preposition distance increases thesparse data problem.
It must be noted, however,that counts show a steady increase in the proportionof low attachments for PP further from the verb, asshown in the table below.
The simplest explanationof this fact is that more (inherently) noun-attachingprepositions must be occurring in 2nd and 3rd posi-tions.
This predicts that the distribution of prepo-sition occurrences changes from PP1 to PP3, withan increase in the proportion of low attaching PPs.Globally, failure to use position results in 41.3% ofcorrect configurations, while use of position resultsin 45% correct attachments.1PP  2PP  3PP  TotalCount 20299 4711 939 25949Low 12223 3063 706 15992% Low 60.2 i 65.0 75.1 61.6Having established that the distance parameter isnot as influential a factor as we hypothesized, we ex-ploit the observation that attachment preferences donot significantly change depending on the distancetesting on the training data.
Moreover, we are only con-sidering 2 attachment possibilities for each preposition,either it attaches to the verb or it attaches to the lowestnOlln.of the PP from the verb.
In the following section,we discuss an extension of the back-off estimationmodel that capitalizes on this property.5 The  Genera l i zed  Backed-Of fAlgor i thmThe algorithm for attaching the first prepositionis almost identical to that of Collins and Brooks(1995), and we follow them in including only tupleswhich contain the preposition.
We do not, however,use the final noun (following the preposition) in anyof our tuples, thus basing our model of PP 1 on three,rather than four, head words.P rocedure  BI :The most likely configuration is:arg rnaxi pl(C2 ~- ilv, n,p), where 1 < i < 21.
IF f(v,n,p) > 0 THEN!
(i ..... p) th( i lv,  n ,p)  = J(.,~,v)2.
ELSEIF f(v, p) + f(n, p) > 0 THENlh( i lv,  n, p) = :(~,v,v)+.
:(i,,,v) f(v,p)+J(n,p)3.
ELSEIF f(p) > 0 THENh( i l v ,  ~,p)  = \](P)4.
ELSE l~l(llv, n ,p  ) = O,l)l(21v, n ,p  ) = 1In this case i denotes the attachment configura-tion: i = 1 is VP attachment, i = 2 is NP attach-ment.
The subscript on C~ is used simply to makeclear that C has 2 possible values.
In the subse-quent algorithms, C5 and C14 are used to indicatethe larger sets of configurations.The algorithm used to handle the cases contain-ing 2PPs is shown in Figure 1, where j ranges over152Procedure  B2The most likely configuration is:arg maxj/~2(C =j lv ,  nLp l ,n2 ,p2) ,  where 1 < j < 51.
IF f(v, n l ,p l ,n2 ,p2)  > 0 THENf(j,v ,n 1 ,pl,n2,p2)~2(j) = f(~,,~,pl,,2,p2)2.
ELSEIF f (n l ,p l ,  n2,p2) + f (v ,p l ,  n2,p2) + f(v, n l ,p l ,p2)  > 0 THENI52(j) = f(j,nl,pl,n2,p2)Tf(j,v,pl,n2,p2)+f(j,v,nl,pl,p2)f(nl,pl,n2,p2)+l(v,pl,n2,p2)+f(v,nl,pl,p2)3.
ELSEIF f (p l ,  n2, p2) + f (v ,p l ,p2)  + f (n l ,p l ,p2)  > 0 THEN132(j) = f(J'pl'n2'p2)+ f(J'v'pl'v2)+ f(j'nl'pl'v2)f(pl,n2,p2)+f(v,pl,p2)+I(nl,pl,p2)4.
ELSE Competit ive Backed-off EstimateFigure 1: Procedure B2the five possible attachment configurations outlinedabove.The first three steps use the standard backed-offestimation, again including only those tuples con-taining bolh prepositions.
However, after backing-offto three elements, we abandon the standard backed-off estimation technique.
The combination of sparsedata., and too few lexical heads, renders backed-offestimation ineffective.
Rather, we propose a tech-nique which makes use of the richer data availablefrom the PP1 training set.
Our hypothesis is thatthis information will be useful in determining theattachments of subsequent PPs as well.
This is mo-tivated by our observations, reported in the previ-ous section, that the distribution of high-low attach-ments for specific prepositions did not vary signifi-cantly for PPs further from the verb.
The Compet-itive Backed-Off Estimate procedure, presentedbelow, operates by initially fixing the configurationof the first preposition (to either the VP or the di-rect object NP), and then considers how the sec-ond preposition would be optimally attached intothe configuration.Procedure Competit ive Backed-off Estimate1.
C~ is the most likely configuration for PP1,arg maxi /)1(C~ = ilv, n l ,p l )2.
C~' is the preferred configuration for PP2 w.r.tn2,arg maxi /~I(C~' = ilv, n2,p2)3.
C~" is the preferred configuration for PP2 w.r.tnl ,max ^ I~ , , ,  iJv, nl ,p2) arg  i Pl/t-~2 :4.
Find Best ConfigurationFirst we determine C~,, on which depends the at-tachment of pl.
We then determine C~', which indi-cates the preference for p2 to attach to the VP or ton2, and C~", which is the preference for p2 to attachto the VP or to nl.
Given the preferred configura-tions C~, C~', and C~", we now must determine thebest of the five possible configurations, C5, for theentire VP.Procedure Find Best Configuration1.
I FC~=C5~12.
ELSEIFC5~43.
ELSEIFC~24.
ELSEIFC5~35.
ELSEIFC5~26.1 and C~ I = 1 THENC~ = 1 and G'~' = 2 THENC~ = 2 and C~ ' = 1 and C;" = 1 THENC~ = 2 and C~' = 2 and C~" = 1 THENC~ = 2 and C~' = 1 and C.~" = 2 THENELSEIF C~ = 2 and C~' = 2 and C~" = 2 THENtie-break(a) IF f(2, v, n2,p2) < f(2, v, nl ,p2) THENC5~5(b) ELSE C5 ~ 3The tests 1 to 5 simply use the attachment valuesC~, C~', and C~" to determine C%: the best config-uration.
In the final instance, step 6, where the C~'indicates a preference for n2 attachment, and C~" in-dicates a preference for nl attachment a tie-break isnecessary to determine which noun to attach to.
Asa first approximation, we use the frequency of occur-rence used in determining these preferences, ratherthan the probability for each preference.
That is,we favour the bias for which there is more evidence,153though whether this is optimal remains an empiricalquestion.
For example, if C~' is based on 4 observa-tions, and C~" is based on 7, then the C~" preferenceis considered stronger.Having constructed the algorithm to determinethe best configuration for 2 PPs, we can similarlygeneralize the algorithm to handle three.
In thiscase /k denotes one of fourteen possible attachmentconfigurations shown earlier.
The pseudo code forprocedure B3 is shown below, simplified for reasonsof space.Procedure B3The most likely configuration is:arg maxk p3(C14 ~- k\[v, na,pl, n2,p2, n3,p3), where1<k<141.
IF f(v, nl,pl,n2,p2, n3,p3) > 0) THEN\](k,v,nl@l,n2,p2,n3,p3)2.
ELSE Try backing-off to 6 or 5 items .. .3.
ELSE Competitive Backed-off Estimate:(a) Use Procedure  B2 to determine C~, theconfiguration of pl and p2(b) Compute C~', C~", C~'", the preferred at-tachment of p3 w.r.t nl, n2, n3 respectively(c) Determine the best configurationAgain, we back-off up to two times, always in-cluding tuples which contain the three prepositions.After this, backing-off becomes unstable, so we usethe Compet i t i ve  Backed-of f  Es t imate ,  as above,but scaled up to handle the three prepositions andfourteen possible configurations.5.1 Resu l tsTo evaluate the performance of our algorithm, wenmst first determine what the expected baseline, orlower-bound on, performance would be.
Given thevariation in the number of possible configurationsacross the three cases, the performance expected ueto chance would be 50% for 1 PP, 20% for 2 PPs,and 7% for 3 PPs.
A better baseline is the perfor-mance that would be expected by simply adoptingthe most likely configuration, without regard to lexi-cal items.
This is shown in the table below, with themost frequent configuration shown in parentheses.TotalMost FrequentPercent CorrectPPi(2)1996312223(2)61.2%PP2(5) PP3(14)4683 9071394(3) 168(4)29.8% 18.5%Table 2 presents the performance of the compet-itive backed-off estimation algorithm on the testdata.
As can be seen, the performance for PP1replicates the findings of Collins and Brooks, whoachieved 84.5% (using 4 lexical items, compared toour three).
For PP2 perfordlance is again high, re-calling that the algorithm is discriminating five pos-sible attachment configurations, and the baseline x-pectation was only 29.8%.
Similarly for PP3, ourperformance of43.6% accuracy (discriminating four-teen configurations) far out strips the baseline of18.5%.6 Conc lus ionsThe backed-off estimate has been demonstrated towork successfully for single PP attachment, but thesparse data problem renders it impractical for usein more complex constructions such as multiple PPattachment; here are too many configurations, toomany head words, too few training examples.
In thispaper we have demonstrated, however, that the rela-tively rich training data obtained for the first prepo-sition can be exploited in attaching subsequent PPs.The algorithm incrementally fixes each prepositioninto the configuration and the more informative PP1training data is exploited to settle the competitionfor possible attachments for each subsequent prepo-sition.
Performance is considerably better than bothchance and the naive baseline technique.
The gen-eralized backed-off estimation approach which wehave presented constitutes a practical solution tothe problem of multiple PP disambiguation.
Thisfurther suggests that backed-off estimation may besuccessfully integrated into more general syntacticdisambiguation systems.AcknowledgmentsWe gratefully acknowledge the support of the BritishCouncil and the Swiss National Science Foundationon grant 83BC044708 to the first two authors, andon grant 12-43283.95 and fellowship 8210-46569 fromthe Swiss NSF to the first author.
We thank theaudiences at Edinburgh and Pennsylvania for theiruseful comments.
All errors remain our responsibil-ity.ReferencesGerry Altmann and Mark Steedman.
1988.
Interac-tion with context during human sentence process-ing.
Cognition, 30(3):191-238.Eric Brill and Philip Resnik.
1994.
A rule-basedapproach to prepositional phrase attachment dis-ambiguation.
International Conference on Com-putational Linguistics (COLING).Eric Brill.
1993.
A Corpus-based Approach ~o Lan-guage Learning.
Ph.D. thesis, University of Penn-sylvania, Philadelphia, PA.154No back-offBack-off 1Back-off 2CompetitiveTotalPercentPP1Total Correct300 285614 510100 60NA NA1014 85584.3%PP2Total CorrectPP3Total Correct36 3561 54232 161135 73464 32369.6%1 11 13 389 3694 4143.6%Table 2: Performance of the competitive backed-off estimation algorithm on the test data.Ken Church and R. Patil.
1982.
Coping with syn-tactic ambiguity or how to put the block in thebox on the table.
American Journal of Computa-tional Linguistics, 8(3-4):139-149.Michael Collins and James Brooks.
1995.
Prepo-sitional phrase attachment through a backed-offmodel.
Third Workshop on Very Large Corpora.Stephen Crain and Mark Steedman.
1985.
On notbeing led up the garden path: The use of con-text by the psychological parser.
In David R.Dowty, Lauri Kartunnen, and Arnold M. Zwicky,editors, Natural Language Processing: Psychologi-cal, Computational, and Theoretical Perspectives,pages 320-358.
Cambridge University Press, Cam-bridge.Marylin Ford, Joan Bresnan, and Ron Kaptan.
1982.A competence-based theory of syntactic losure.In Joan Bresnan, editor, The Mental Represen-tations of Grammatical Relations, pages 727-796.MIT Press, Cambridge, MA.Donald Hindle and Mats Rooth.
1993.
Structuralambiguity and lexical relations.
ComputationalLinguistics, 19(1):103-120.Graheme Hirst.
1987.
Semantic Interpretation andthe Resolution of Ambiguity.
Cambridge Univer-sity Press.Karen Jensen and Jean-Louis Binot.
1987.
Disam-higuating prepositional phrase attachments byus-ing on-line dictionary definitions.
ComputationalLinguisticsl 13(3-4):251-260.David Magerman and Mitch Marcus.
1991.
Pearl:A probabilistic parser.
In Proceedings of the Eu-ropean Chapter off the Association for Computa-tional Linguistics, Berlin.Mitch Marcus.
1980.
A Theory of Syntactic Recog-nition for Natural Language.
MIT Press, Cam-bridge, MA.Philip Resnik.
1993.
Selection and Information:A Class-Based Approach to Lexical Relationships.Ph.D.
thesis, University of Pennsylvania.Roman Taraban and .\]ames L. McClelland.
1988.Constituent attachment and thematic role as-signment in sentence processing: Influences ofcontent-based xpectations.
Journal of Memoryand Language, 27:597-632.Greg Whittemore, Kathteen Ferrara, and HansBrunner.
1990.
Empirical study of predictivepower of simple attachment schenaes for post-modifiers prepositional phrases, pages 23-30,Pittsburgh, PA. Association for ComputationalLinguistics.155
