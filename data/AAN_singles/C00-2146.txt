St ructura l  d i sambiguat ion  of  morpho-syntact i c  categor ia l  pars ingfor Korean  *Jeongwon Cha and Geunbae LeeDepartment of Computer Science & EngineeringPohang University of Science & TechnologyPohang, Korea{himen, gblee}@postech.ac.krAbstractThe Korean Combinatory Categorial Grammar(KCCG) tbrmalism can unitbrmly handle wordorder variation among arguments and adjunctswithin a clause as well as in complex clausesand across clause boundaries, i.e., long distancescrambling.
Ill this paper, incremental pars-ing technique of a morpheme graph is devel-oped using the KCCG.
We present echniquesfor choosing the most plausible parse tree us-ing lexical information such as category mergeprobability, head-head co-occurrence heuristic,and the heuristic based on the coverage of sub-trees.
The performance r sults for various mod-els for choosing the most plausible parse tree arecompared.1 I n t roduct ionKorean is a non-configurational, t)ostpositional,agglutinative language.
Postpositions, uch asnoun-endings, verb-endings, and prefinal verb-endings, are morphemes that determine thefnnctional role of NPs (noun phrases) and VPs(verb phrases) in sentences and also transformVPs into NPs or APs (adjective phrases).
Sincea sequence of prefinal verb-endings, auxiliaryverbs and verb-endings can generate hundredsof different usages of the same verb, morpheme-based grammar modeling is considered as a nat-ural consequence for Korean.There have been various researches to dis-ambiguate the structural ambiguities in pars-ing.
Lexical and contextual information hasbeen shown to be most crucial for many pars-ing decisions, such as prepositional-phrase at-tachment (Hindle and Rooth, 1993).
(Charniak,1995; Collins, 1996) use the lexical intbrmation* This  research was part ial ly supported by KOSEF  spe-cial basic resem'ch 1)rogram (1997.9 ~ 2000.8).and (Magerman and Marcus, 1991; Magermanand Weir, 1992) use the contextual informationfor struct;nral disambiguation.
But, there havebeen few researches that used probability intbr-marion for reducing the spurious ambiguities inchoosing the most plausible parse tree of CCGformalism, especially for morpho-syntactic pars-ing of agglutinative language.In this paper, we describe the probabilisticnmthod (e.g., category merge probability, head-head co-occurrence, coverage heuristics) to re-duce the spurious atnbiguities and choose themost plausible parse tree for agglutinative lan-guages uch as Korean.2 Overv iew of  KCCGThis section briefly reviews the basic KCCG for-malism.Following (Steedman, 1985), order-preservingtype-raising rules are used to convert nouns ingrammar into the functors over a verb.
Thefollowing rules are obligatorily activated uringparsing when case-marking morphemes attachto  nora1 s tems.?
Type Raising Rules:np + case-markerfeature\])v/(v\np\[case-This rule indicates that a noun in the pres-ence of a case morpheme becomes a functorlooking for a verb on its right; this verb is alsoa flmctor looking for the original noun with theappropriate case on its left.
Alter tile nounfunctor combines with the appropriate verb, theresult is a flmctor, which is looking for the re-maining arguments of the verb.
'v' is a w~ri-able tbr a verb phrase at ally level, e.g., theverb of a matrix clause or the verb of an em-bedded clause.
And 'v' is matched to all of1002the "v\[X\]\Args" patterns of the verl, categories.Since all case-marked ilouns in Korean occur infront of the verb, we don't need to e, mploy thedirectional rules introduced by (Hoffman, 1995).We extend the combinatory rules ibr uncm'-ried flmctions as follows.
The sets indicated bybraces in these rules are order-free.?
Forward Application (A>):x/(args u {Y}) Y X/Args?
Backward Application (A<):Y X\(Args U {Y}) ==4- X\ArgsUsing these rules, a verb can apply to itsarguments in any order, or as in most cases,the casednarked noun phrases, which are type-raised flmctors, can apply to the, al)t)roi)riateverbs.Coordination constructions are moditied toallow two type-raised noml 1)hrases that arelooking tbr the saxne verb to combine together.Since noun phrases, or a noun phrase and ad-verb phrase, are fimctors, the following compo-sition rules combine two flmctions with a setvahle al'gulnents.?
Forward Composition (B>):X/(X\Ar.q.sx) Y/(Y\Arg.sy) ==~x /  (X\ ( A,.
:j<,: u ) ),Y = X\Arqsx?
Backward Comi)osition (B<):Y\Arg.sy X\(Ar.q.sx U {Y}) ===>X\(A'rgsx U Arosy)?
Coordination (~):X CONJX  ~ X3 Bas ic  morph-syntact i c  char tpars ingKorean chart parser has been developed basedon our KCCG modeling with a 10(},0()0 mor-pheme dictionary.
Each morpheme entry inthe dictionary has morphological category, mor-photactics connectivity and KCCG syntax (:at-egories tbr the morpheme.In the morphological analysis stage, a un-known word treatment nmthod based on a mor-pheme pattern dictionary and syllable bigramsis used after (Cha et al, 1998).
POS(part -ofspeech) tagger which is tightly coupled withthe morphological analyzer removes the irrele-wmt morpheme candidates from the lnorphemegraph.
The morpheme graph is a compactrepresentation method of Korean morphologi-cal structure.
KCCG parser analyzes the mor-pheme graph at once through the morphemegraph embedding technique (Lee et al, 1996).The KCCG parser incrementally analyzes thesentence, eojeol by eojeol :1 Whenever an eo-jeol is newly processed by the morphological n-alyzer, the morphenms resulted in a new mor-pheme graph are embedded in a chart and an-alyzed and combined with the previous parsingresults.4 Statistical structureddisambiguation for  KCCG parsingTh(' statistics which have been used in the ex-perinlents have been collected fronl the KCCGparsed corpora.
The data required for train-ing have been collected by parsing the stan-dard Korean sentence types 2, example sentencesof grammar book, and colloquial sentences intrade interview domain 3 and hotel reservationdomain 4.
We use about; 1500 sentences fortraining and 591 indq)endent sentences for eval-uation.The evaluation is based on parsewflmethod (Black el, a\]., 1991).
In the evalu-ation, "No-crossing" is 1;11o number of selltellceswhich have no crossing brackets between theresult and |;tie corresponding correct trees ofthe sentences.
"Ave. crossing" is the averagenumber of crossings per sentence.4.1 Bas ic  s ta t i s t i ca l  modelA basic method of choosing the nlost plausibleparse tree is to order the prot)abilities by the lex-ical preib, rences 5 and the syntactic merge prob-ability.
In general, a statistical parsing modeldefines the conditional probability, 1"(71S), foreach candidate tree r tbr a sentence S. A gener-ative model uses the observation that maximis-ing P(% S) is equivalent to maximising P(r IS)  6.1Eojeol is a spacing unit in Korean and is similar toan English word.2Sentences of length < 11.aSentences of length < 25.4Sentences ofhmgth _< 13.5The frequency with which a certain category is as-sociated with a morpheme tagged for part-of-speech.c'P(S) is constmlt.1003Thus, when S is a sentence consisted of a se-quence of morphemes tagged for part-of-speech,(w~, t~), (w2, t2), ..., (w,,, tu), where wi is a i thmorpheme, ti is the part-of-speech tag of themorpheme wi, and cij is a category with rela-tive position i, j, the basic statistical model willbe given by:r* = arg ,~x P(rl,S' ) (1)(2) = argn~x P(S),~ argmaxP(T,S ).
(3)TThe r* is the probabilities of the optimM parsetree.P(r, S) is then estimated by attaching proba-bilities to a bottom-up composition of the tree.P(r,S) = I I  P(cij) (4)c i j  ~T= H (P(eiilcik'ck+'J)c i j  ETxP(cik)P(cl~+lj)), (5)i<k<j ,i f  cij is a terminal,the,  P(c j) =andfrcquency(cij, ti, wi)frequency(ti, wi) ' (6)frequency(eli, cik, Ch+lj) (7)P(eijleik, C~+lj) ~ frequency(cik, ck+lj)The basic statistical model has been appliedto morpheme/part-of-speech/category 3-tuple.Due to the sparseness of the data, we haveused part-of-speech/category pairs 7 together,i.e., collected the frequencies of the categoriesassociated with the part-of-speeches assigned tothe morpheme.
Table 1 illustrates the sampleentries of the category probability database.
Intable, 'nal (fly)' has two categories with 0.6375mid 0.3625 probability respectively.
Table 2 il-lustrates the sample entries of the merge prob-ability database using equation 7.f requency  (old ,tl ) 7We def ine  th i s  as  P(c l j l t l )  ~ fvcq .
.
.
.
.
.
y ( tD  "Table 3:ModelResults fl'om the Basic StatisticalTotal sentencesNo-crossingAve.
crossingLabeled RecallLabeled Precision59174.62%1.0077.0279.15Figure 1: Sub-constituents for head-head co-occurrence heuristicsTable 3 summarizes the results on an opentest set of 591 sentences.4.2 Head-head co -occur rence  heur i s t i csIn the basic statistical model, lexicM depen-dencies between morphemes that take part inmerging process cannot be incorporated into themodel.
When there is a different morphemewith the same syntactic category, it can be amiss match on merging process.
This linfita-tion can be overcome through the co-occurrencebetween the head morphemes of left and rightsub-constituent.When B h is a head morphenm of left sub-constituent, r is a case relation, C h is a headmorpheme of right sub-constituent as shown infigure 1, head-head co-occurrence heuristics aredefined by:p(B,LI,.
,Ch ) ~ f requency(B h,r, C h)frequency(r, C h) " (8)Tile head-head co-occurrence heuristics havebeen augmented to equation 5 to model the lex-ical co-occurrence preference in category merg-ing process.
Table 4 illustrates the sample en-tries of the co-occurrence probability database.In Table 4, a morpheme 'sac (means 'bird')',which has a "MCK (common noun)" ms POStag, has been used a nominative of verb 'hal(means 'fly')' with 0.8925 probability.1004Table 1: Sample entries of the category probal)ility database ('DII' Ineans an '1' irregular verb.
)P()S, morpheme category probabilityDII, nal v\[D\]\ {np\[noln\]} 0.6375DI1, hal v\[D\]\{np\[noln\],nl)\[acc\]} 0.362,5DI1 v \[D\]  {rip \[nora\] } 0.3079DI1 v\[D\]\ {np\[llOm\],np\[acc\] } 0.2020Table 2: Sample entries of' syntactic merge probability databaseleft; category~, / ( ~ \,u,\[,,o,,l\])~,/(~, \ ,p lace\])right categoryv\[D\]\ {np\[noml,np\[acc\]}v\[D\]\ {,,p \[,lo,,,\],,u,\[acd }inerged categoryv\[D\]\{,,p\[acd}v\[D\]\ {ni)\[nonl\] }probability0.04730.6250nl, (v / (v \nont ) ) \n  t , v/(v\np\[nom\]) I).2197The modified model has been tested Oil thesame set of the open sentences as in the 1)asicmodel ext)eriment.
'l~fl)le 5 smnmarizes the re-sult of these expcwiments.?
Ezperimcnt: (linear combination af th, c ba-sic model and the head-h, cad co-occurrenceheuristics).P(% s)eij { r+/~p( \ ] / '  I,,., c*'))?
P(~,ik)~'(~,k+,;)), (9)i < k < j,i f  cij is a terminal,~J,.,;',~ p(c#i) = P(c.~:i I~g, td.Ta,bh; 5: Results from the Basic: StatisticalModel t)lns head-head co-occurrence heuristicsTotal sentences 591No-crossing 81.05%Ave.
crossing 0.70Labeled Recall 84.02Labeled Precision 85.304.3 The  coverage heur is t icsIf" there is a case relation or a modification re-lation in two constituents, coverage heuristicsdesignate it is easier to add the smaller tree tothe larger one ttlan to merge the two mediumsized trees.
On the contrary, in the coordinationrelation, it is easier to nmrge two medium sizedtrees.
We implemented these heuristics using/;tie tbllowing coverage score:Case relation, modification relation:COV_scorc =le f t  subtrec coverage + riqh, t sub/roe coverage.
(j_()~4 ?
~7~ ,~,,bt,.~,.,~ o,,,,',,e ;< ','i:jl,,i ~,,b>'.e eo,,~',',,.~,'.
"Coo~d'iuatio'n:COV_sco'rc ='e x x / le f t  .~,a,l.,'~c.
~o.,,,.~,.,,.:,..
x ,'#lht .~,O,l,,.,, ,:o.,,~,.,,,~ 1~leJ't subtree cove,.aqe + R~ ~b~r('.e ~; .~t  .A coverage heuristics are added to the basic:model to model the structural preferences.
Ta-ble 6 shows the results of the experinlents onthe same set of the open sentences.?
Ezpcriment: (the basic model to th, cCOV_scorc heuristics).
We have used (;tieCOV_.sco're as the exponent weight featurefor this experiment since the two nmnl)ersarc; in the different nature of statistics.P(7-, S) = H (P(ciJ\] cik, Ok+l J) l-COV-'sc?rceij CT?p(~k)p(c~+,j)), (1~,)i<k<j ,i f  Cij iS  a terminal,o,: , ,  P(~.j) = 1)(c~.jl~,~, ~d.1005Table 4: Sample entries of co-occurrence probability database.head-head co-occurrence probability(MCC <ganeungseong>,np\[nom\],HIl.< nob>) 0.8932(MCK<sae>,np\[nom\],DIl<nal>) 0.8925(MCK<galeuchim>,np\[acc\],DIeu<ddaleu>) 0.8743Table 6: Results from the Basic Statisticalmodel plus Coverage heuristicsTotal sentences 591No-crossing 80.13%Ave.
crossing 0.81Labeled Recall 82.59Labeled Precision 83.755 SummaryWe developed a morpho-syntactic categorialparser of Korean and devised a morpheme-based statistical structural disambiguations(;henles.Through the KCCG model, we successthllyhandled ifficult Korean modeling problems, in-chtding relative free-word ordering, coordina-tion, and case-marking, during the parsing.To extract he most plausible parse trees ti'omthe parse forest, we have presented basic statis-tical techniques using the lexical and contextualinformation such as morpheme-category p oba-bility and category merge probability.Two different nature of heuristics, head-headco-occurrence and coverage scores, are also de-veloped and tested to augment the basic statis-tical model.
Each of them demonstrates reason-able t)ertbrmance increase.The next step will be to devise more heuristicsand good combination strategies tbr the differ-ent nature of heuristics.ReferencesE.
Black, S. Abney, D. Flickenger, C. Gdaniec,R.
Grishman, P. Harrison, D. Hindle, R. In-gria, F. Jelinek, J. Klavans, M. Liberman,M.
Marcus, S. Roukos, B. Santorini, andT.
Strzalkowski.
1991.
A Procedm'e forQuantitatively Comparing the Syntactic Coy-erage of English Grammars.
In Prec.
ofFourth DARPA Speech and Natural Lan-guage Workshop.Jeongwon Cha, Gcunbae Lee, and Jong-HyeokLee.
1998.
Generalized unknown morphemeguessing for hybrid pos tagging of korean.In Pwceedings of Sixth Workshop on VeryLarge Corpora in Coling-ACL 98, Montreal,Canada.E.
Charniak.
1995.
Prsing with Context-FreeGrammars and Word Statistics.
TechnicalReport CS-95-28, Brown University.M.
Collins.
1996.
A New Statistical ParserBased on Bigram Lexical Dependencies.
InProceedings of th, e 3/tth Annual Meeting of theA CL, Santa Cruz.D.
Hindle and M. Rooth.
1993.
Structural am-biguity and lexical relations.
ComputationalLinguistics, 19(1):103-120.B.
Hoffman.
1995.
~7~,c Computational Analy-sis of the Syntax and Interpretation of 'if;roe"Word Order in Turkish.
Ph.D. thesis, Univer-sity of Pennsylwmia.
IRCS Report 95-17.Wonil Lee, Gennb:m Lee, and Jong-Hyeok Lee.1996.
Chart-driven connectionist categorialt)arsing of spoken korean.
Computer process-ing of oriental languages, Vol 10, No 2:147--159.D.
M. Magerman and M. P. Marcus.
1991.Parsing the voyager domain using t)earl.
InIn Prec.
Of the DARPA Speech and NaturalLanguage Workshop~ pages 231-236.D.
M. Magerman and C. Weir.
1992.
E fficiency, robustness and accuracy in pickychart parsing.
In In Prec.
Of the 30th An-nual Meeting of the Assoc.
For Computa-tional Linfluisties(ACL-92), pages 40 47.Mark Steedman.
1985.
Dependency and Coor-dination in the Grammar of Dutch and En-glish.
Language, 61:523 568.1006
