Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1346?1355,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsUsing Large Monolingual and Bilingual Corpora toImprove Coordination DisambiguationShane Bergsma, David Yarowsky, Kenneth ChurchDeptartment of Computer Science and Human Language Technology Center of ExcellenceJohns Hopkins Universitysbergsma@jhu.edu, yarowsky@cs.jhu.edu, kenneth.church@jhu.eduAbstractResolving coordination ambiguity is a clas-sic hard problem.
This paper looks at co-ordination disambiguation in complex nounphrases (NPs).
Parsers trained on the PennTreebank are reporting impressive numbersthese days, but they don?t do very well on thisproblem (79%).
We explore systems trainedusing three types of corpora: (1) annotated(e.g.
the Penn Treebank), (2) bitexts (e.g.
Eu-roparl), and (3) unannotated monolingual (e.g.Google N-grams).
Size matters: (1) is a mil-lion words, (2) is potentially billions of wordsand (3) is potentially trillions of words.
Theunannotated monolingual data is helpful whenthe ambiguity can be resolved through associ-ations among the lexical items.
The bilingualdata is helpful when the ambiguity can be re-solved by the order of words in the translation.We train separate classifiers with monolingualand bilingual features and iteratively improvethem via co-training.
The co-trained classifierachieves close to 96% accuracy on Treebankdata and makes 20% fewer errors than a su-pervised system trained with Treebank anno-tations.1 IntroductionDetermining which words are being linked by a co-ordinating conjunction is a classic hard problem.Consider the pair:+ellipsis rocket\w1 and mortar\w2 attacks\h?ellipsis asbestos\w1 and polyvinyl\w2 chloride\h+ellipsis is about both rocket attacks and mortar at-tacks, unlike ?ellipsis which is not about asbestoschloride.
We use h to refer to the head of the phrase,and w1 and w2 to refer to the other two lexical items.Natural Language Processing applications need torecognize NP ellipsis in order to make sense of newsentences.
For example, if an Internet search en-gine is given the phrase rocket attacks as a query, itshould rank documents containing rocket and mor-tar attacks highly, even though rocket and attacksare not contiguous in the document.
Furthermore,NPs with ellipsis often require a distinct type of re-ordering when translated into a foreign language.Since coordination is both complex and produc-tive, parsers and machine translation (MT) systemscannot simply memorize the analysis of coordinatephrases from training text.
We propose an approachto recognizing ellipsis that could benefit both MTand other NLP technology that relies on shallow ordeep syntactic analysis.While the general case of coordination is quitecomplicated, we focus on the special case of com-plex NPs.
Errors in NP coordination typically ac-count for the majority of parser coordination errors(Hogan, 2007).
The information needed to resolvecoordinate NP ambiguity cannot be derived fromhand-annotated data, and we follow previous workin looking for new information sources to applyto this problem (Resnik, 1999; Nakov and Hearst,2005; Rus et al, 2007; Pitler et al, 2010).We first resolve coordinate NP ambiguity in aword-aligned parallel corpus.
In bitexts, both mono-lingual and bilingual information can indicate NPstructure.
We create separate classifiers using mono-lingual and bilingual feature views.
We train thetwo classifiers using co-training, iteratively improv-ing the accuracy of one classifier by learning fromthe predictions of the other.
Starting from only two1346initial labeled examples, we are able to train a highlyaccurate classifier using only monolingual features.The monolingual classifier can then be used bothwithin and beyond the aligned bitext.
In particular,it achieves close to 96% accuracy on both bitext dataand on out-of-domain examples in the Treebank.2 Problem Definition and Related TasksOur system operates over a part-of-speech tagged in-put corpus.
We attempt to resolve the ambiguity inall tag sequences matching the expression:[DT|PRP$] (N.*|J.
*) and [DT|PRP$] (N.*|J.
*) N.*e.g.
[the] rocket\w1 and [the] mortar\w2 attacks\hEach example ends with a noun, h. Preceding hare a pair of possibly-conjoined words, w1 and w2,either nouns (rocket and mortar), adjectives, or amix of the two.
We allow determiners or possessivepronouns before w1 and/or w2.
This pattern is verycommon.
Depending on the domain, we find it inroughly one of every 10 to 20 sentences.
We mergeidentical matches in our corpus into a single exam-ple for labeling.
Roughly 38% of w1,w2 pairs areboth adjectives, 26% are nouns, and 36% are mixed.The task is to determine whether w1 and w2 areconjoined or not.
When they are not conjoined, thereare two cases: 1) w1 is actually conjoined with w2 has a whole (e.g.
asbestos and polyvinyl chloride),or 2) The conjunction links something higher up inthe parse tree, as in, ?farmers are getting older\w1and younger\w2 people\h are reluctant to take upfarming.?
Here, and links two separate clauses.Our task is both narrower and broader than pre-vious work.
It is broader than previous approachesthat have focused only on conjoined nouns (Resnik,1999; Nakov and Hearst, 2005).
Although pairsof adjectives are usually conjoined (and mixed tagsare usually not), this is not always true, as inolder/younger above.
For comparison, we also stateaccuracy on the noun-only examples (?
8).Our task is more narrow than the task tackledby full-sentence parsers, but most parsers do notbracket NP-internal structure at all, since such struc-ture is absent from the primary training corpus forstatistical parsers, the Penn Treebank (Marcus et al,1993).
We confirm that standard broad-coverageparsers perform poorly on our task (?
7).Vadas and Curran (2007a) manually annotated NPstructure in the Penn Treebank, and a few custom NPparsers have recently been developed using this data(Vadas and Curran, 2007b; Pitler et al, 2010).
Ourtask is more narrow than the task handled by theseparsers since we do not handle other, less-frequentand sometimes more complex constructions (e.g.robot arms and legs).
However, such constructionsare clearly amenable to our algorithm.
In addition,these parsers have only evaluated coordination res-olution within base NPs, simplifying the task andrendering the aforementioned older/younger prob-lem moot.
Finally, these custom parsers have onlyused simple count features; for example, they havenot used the paraphrases we describe below.3 Supervised Coordination ResolutionWe adopt a discriminative approach to resolving co-ordinate NP ambiguity.
For each unique coordinateNP in our corpus, we encode relevant informationin a feature vector, x?.
A classifier scores these vec-tors with a set of learned weights, w?.
We assume Nlabeled examples {(y1, x?1), ..., (yN , x?N )} are avail-able to train the classifier.
We use ?y = 1?
as theclass label for NPs with ellipsis and ?y = 0?
forNPs without.
Since our particular task requires a bi-nary decision, any standard learning algorithm canbe used to learn the feature weights on the train-ing data.
We use (regularized) logistic regression(a.k.a.
maximum entropy) since it has been shownto perform well on a range of NLP tasks, and alsobecause its probabilistic interpretation is useful forco-training (?
4).
In binary logistic regression, theprobability of a positive class takes the form of thelogistic function:Pr(y = 1) = exp(w?
?
x?
)1 + exp(w?
?
x?
)Ellipsis is predicted if Pr(y = 1) > 0.5 (equiva-lently, w?
?
x?
> 0), otherwise we predict no ellipsis.Supervised classifiers easily incorporate a rangeof interdependent information into a learned deci-sion function.
The cost for this flexibility is typicallythe need for labeled training data.
The more featureswe use, the more labeled data we need, since forlinear classifiers, the number of examples needed toreach optimum performance is at most linear in the1347Phrase Evidence Patterndairy and meat English: ... production of dairy and meat... h of w1 and w2production English: ... dairy production and meat production... w1 h and w2 h(ellipsis) English: ... meat and dairy production... w2 and w1 hSpanish: ... produccio?n la?ctea y ca?rnica... h w1 ... w2?
production dairy and meatFinnish: ... maidon- ja lihantuotantoon... w1- ... w2h?
dairy- and meatproductionFrench: ... production de produits laitiers et de viande... h ... w1 ... w2?
production of products dairy and of meatasbestos and English: ... polyvinyl chloride and asbestos... w2 h and w1polyvinyl English: ... asbestos , and polyvinyl chloride... w1 , and w2 hchloride English: ... asbestos and chloride... w1 and h(no ellipsis) Portuguese: ... o amianto e o cloreto de polivinilo... w1 ... h ... w2?
the asbestos and the chloride of polyvinylItalian: ... l?
asbesto e il polivinilcloruro... w1 ... w2h?
the asbestos and the polyvinylchlorideTable 1: Monolingual and bilingual evidence for ellipsis or lack-of-ellipsis in coordination of [w1 and w2 h] phrases.number of features (Vapnik, 1998).
In ?
4, we pro-pose a way to circumvent the need for labeled data.We now describe the particular monolingual andbilingual information we use for this problem.
Werefer to Table 1 for canonical examples of the twoclasses and also to provide intuition for the features.3.1 Monolingual FeaturesCount features These real-valued features encodethe frequency, in a large auxiliary corpus, of rel-evant word sequences.
Co-occurrence frequencieshave long been used to resolve linguistic ambigui-ties (Dagan and Itai, 1990; Hindle and Rooth, 1993;Lauer, 1995).
With the massive volumes of rawtext now available, we can look for very specificand indicative word sequences.
Consider the phrasedairy and meat production (Table 1).
A high countin raw text for the paraphrase ?production of dairyand meat?
implies ellipsis in the original example.In the third column of Table 1, we suggest a pat-tern that generalizes the particular piece of evidence.It is these patterns and other English paraphrasesthat we encode in our count features (Table 2).
Wealso use (but do not list) count features for the fourparaphrases proposed in Nakov and Hearst (2005,?
3.2.3).
Such specific paraphrases are more com-mon than one might think.
In our experiments, atleast 20% of examples have non-zero counts for a5-gram pattern, while over 70% of examples havecounts for a 4-gram pattern.Our features also include counts for subsequencesof the full phrase.
High counts for ?dairy produc-tion?
alone or just ?dairy and meat?
also indicate el-lipsis.
On the other hand, like Pitler et al (2010), wehave a feature for the count of ?dairy and produc-tion.?
Frequent conjoining of w1 and h is evidencethat there is no ellipsis, that w1 and h are compatibleand heads of two separate and conjoined NPs.Many of our patterns are novel in that they includecommas or determiners.
The presence of these of-ten indicate that there are two separate NPs.
E.g.seeing asbestos , and polyvinyl chloride or the as-bestos and the polyvinyl chloride suggests no ellip-sis.
We also propose patterns that include left-and-right context around the NP.
These aim to capturesalient information about the NP?s distribution as anentire unit.
Finally, patterns involving prepositionslook for explicit paraphrasing of the nominal rela-tions; the presence of ?h PREP w1 and w2?
in a cor-pus would suggest ellipsis in the original NP.In total, we have 48 separate count features, re-quiring counts for 315 distinct N-grams for each ex-ample.
We use log-counts as the feature value, anduse a separate binary feature to indicate if a partic-ular count is zero.
We efficiently acquire the countsusing custom tools for managing web-scale N-gram1348Real-valued count features.
C(p) ?
count of pC(w1) C(w2) C(h)C(w1 CC w2) C(w1 h) C(w2 h)C(w2 CC w1) C(w1 CC h) C(h CC w1)C(DT w1 CC w2) C(w1 , CC w2)C(DT w2 CC w1) C(w2 , CC w1)C(DT w1 CC h) C(w1 CC w2 ,)C(DT h CC w1) C(w2 CC w1 ,)C(DT w1 and DT w2) C(w1 CC DT w2)C(DT w2 and DT w1) C(w2 CC DT w1)C(DT h and DT w1) C(w1 CC DT h)C(DT h and DT w2) C(h CC DT w1)C(?L-CTXTi?
w1 and w2 h) C(w1 CC w2 h)C(w1 and w2 h ?R-CTXTi?)
C(h PREP w1)C(h PREP w1 CC w2) C(h PREP w2)Count feature filler setsDT = {the, a, an, its, his} CC = {and, or, ?,?
}PREP = {of, for, in, at, on, from, with, about}Binary features and feature templates ?
{0, 1}wrd1=?wrd(w1)?
tag1=?tag(w1)?wrd2=?wrd(w2)?
tag2=?tag(w2)?wrdh=?wrd(h)?
tagh=?tag(h)?wrd12=?wrd(w1),wrd(w2)?
wrd(w1)=wrd(w2)tag12=?tag(w1),tag(w2)?
tag(w1)=tag(w2)tag12h=?tag(w1),tag(w1),tag(h)?Table 2: Monolingual features.
For counts using thefiller sets CC, DT and PREP, counts are summed acrossall filler combinations.
In contrast, feature templates aredenoted with ??
?, where the feature label depends on the?bracketed argument?.
E.g., we have separate count fea-ture for each item in the L/R context sets, where{L-CTXT} = {with, and, as, including, on, is, are, &},{R-CTXT} = {and, have, of, on, said, to, were, &}data (?
5).
Previous approaches have used searchengine page counts as substitutes for co-occurrenceinformation (Nakov and Hearst, 2005; Rus et al,2007).
These approaches clearly cannot scale to usethe wide range of information used in our system.Binary features Table 2 gives the binary featuresand feature templates.
These are templates in thesense that every unique word or tag fills the tem-plate and corresponds to a unique feature.
We canthus learn if particular words or tags are associatedwith ellipsis.
We also include binary features to flagthe presence of any optional determiners before w1or w2.
We also have binary features for the contextwords that precede and follow the tag sequence inthe source corpus.
These context features are analo-gous to the L/R-CTXT features that were counted inthe auxiliary corpus.
Our classifier learns, for exam-Monolingual: x?m Bilingual: x?bC(w1):14.4 C(detl=h * w1 * w2),Dutch:1C(w2):15.4 C(detl=h * * w1 * * w2),Fr.
:1C(h):17.2 C(detl=h w1 h * w2),Greek:1C(w1 CC w2):9.0 C(detl=h w1 * w2),Spanish:1C(w1 h):9.8 C(detl=w1- * w2h),Swedish:1C(w2 h):10.2 C(simp=h w1 w2),Dutch:1C(w2 CC w1):10.5 C(simp=h w1 w2),French:1C(w1 CC h):3.5 C(simp=h w1 h w2),Greek:1C(h CC w1):6.8 C(simp=h w1 w2),Spanish:1C(DT w2 CC w1:7.8 C(simp=w1 w2h),Swedish:1C(w1 and w2 h and):2.4 C(span=5),Dutch:1C(h PREP w1 CC w2):2.6 C(span=7),French:1wrd1=dairy:1 C(span=5),Greek:1wrd2=meat:1 C(span=4),Spanish:1wrdh=production:1 C(span=3),Swedish:1tag1=NN:1 C(ord=h w1 w2),Dutch:1tag2=NN:1 C(ord=h w1 w2),French:1tagh=NN:1 C(ord=h w1 h w2),Greek:1wrd12=dairy,meat:1 C(ord=h w1 w2),Spanish:1tag12=NN,NN:1 C(ord=w1 w2 h),Swedish:1tag(w1)=tag(w2):1 C(ord=h w1 w2):4tag12h=NN,NN,NN:1 C(ord=w1 w2 h):1Table 3: Example of actual instantiated feature vectorsfor dairy and meat production (in label:value format).Monolingual feature vector, x?m, on the left (both countand binary features, see Table 2), Bilingual feature vec-tor, x?b, on the right (see Table 4).ple, that instances preceded by the words its and inare likely to have ellipsis: these words tend to pre-cede single NPs as opposed to conjoined NP pairs.Example Table 3 provides part of the actual in-stantiated monolingual feature vector for dairy andmeat production.
Note the count features have log-arithmic values, while only the non-zero binary fea-tures are included.A later stage of processing extracts a list of featurelabels from the training data.
This list is then usedto map feature labels to integers, yielding the stan-dard (sparse) format used by most machine learningsoftware (e.g., 1:14.4 2:15.4 3:17.2 ... 7149:1 24208:1).3.2 Bilingual FeaturesThe above features represent the best of the infor-mation available to a coordinate NP classifier whenoperating on an arbitrary text.
In some domains,however, we have additional information to informour decisions.
We consider the case where we seekto predict coordinate structure in parallel text: i.e.,English text with a corresponding translation in one1349or more target languages.
A variety of mature NLPtools exists in this domain, allowing us to robustlyalign the parallel text first at the sentence and thenat the word level.
Given a word-aligned parallel cor-pus, we can see how the different types of coordinateNPs are translated in the target languages.In Romance languages, examples with ellipsis,such as dairy and meat production (Table 1), tend tocorrespond to translations with the head in the firstposition, e.g.
?produccio?n la?ctea y ca?rnica?
in Span-ish (examples taken from Europarl (Koehn, 2005)).When there is no ellipsis, the head-first syntax leadsto the ?w1 and h w2?
ordering, e.g.
amianto e ocloreto de polivinilo in Portuguese.
Another cluefor ellipsis is the presence of a dangling hyphen, asin the Finnish maidon- ja lihantuotantoon.
We findsuch hyphens especially common in Germanic lan-guages like Dutch.
In addition to language-specificclues, a translation may resolve an ambiguity byparaphrasing the example in the same way it maybe paraphrased in English.
E.g., we see hard andsoft drugs translated into Spanish as drogas blandasy drogas duras with the head, drogas, repeated (akinto soft drugs and hard drugs in English).One could imagine manually defining the rela-tionship between English NP coordination and thepatterns in each language, but this would need to berepeated for each language pair, and would likelymiss many useful patterns.
In contrast, by represent-ing the translation patterns as features in a classifier,we can instead automatically learn the coordination-translation correspondences, in any language pair.For each occurrence of a coordinate NP in a word-aligned bitext, we inspect the alignments and de-termine the mapping of w1, w2 and h. Recall thateach of our examples represents all the occurrencesof a unique coordinate NP in a corpus.
We there-fore aggregate translation information over all theoccurrences.
Since the alignments in automatically-aligned parallel text are noisy, the more occurrenceswe have, the more translations we have, and themore likely we are to make a correct decision.
Forsome common instances in Europarl, like Agricul-ture and Rural Development, we have thousands oftranslations in several languages.Table 4 provides the bilingual feature templates.The notation indicates that, for a given coordi-nate NP, we count the frequency of each transla-C?detl(w1,w2,h)?,?LANG?C?simp(w1,w2,h)?,?LANG?C?span(w1,w2,h)?,?LANG?C?ord(w1,w2,h)?,?LANG?C?ord(w1,w2,h)?Table 4: Real-valued bilingual feature templates.
Theshorthand is detl=?detailed pattern,?
simp=?simple pat-tern,?
span=?span of pattern,?
ord=?order of words.?
Thenotation C?p?,?LANG?means the number of times we seethe pattern (or span) ?p?
as the aligned translation of thecoordinate NP in the target language ?LANG?.tion pattern in each target language, and generatereal-valued features for these counts.
The featurecounts are indexed to the particular pattern and lan-guage.
We also have one language-independent fea-ture, C?ord(w1,w2,h)?, which gives the frequency ofeach ordering across all languages.
The span is thenumber of tokens collectively spanned by the trans-lations of w1, w2 and h. The ?detailed pattern?
rep-resents the translation using wildcards for all otherforeign words, but maintains punctuation.
Letting?*?
stand for the wildcard, the detailed patterns forthe translations of dairy and meat production in Ta-ble 1 would be [h w1 * w2] (Spanish), [w1- * w2h](Finnish) and [h * * w1 * * w2] (French).
Fouror more consecutive wildcards are converted to ?...
?.For the ?simple pattern,?
we remove the wildcardsand punctuation.
Note that our aligner allows theEnglish word to map to multiple target words.
Thesimple pattern differs from the ordering in that it de-notes how many tokens each of w1, w2 and h span.Example Table 3 also provides part of the actualinstantiated bilingual feature vector for dairy andmeat production.4 Bilingual Co-trainingWe exploit the orthogonality of the monolingualand bilingual features using semi-supervised learn-ing.
These features are orthogonal in the sense thatthey look at different sources of information for eachexample.
If we had enough training data, a goodclassifier could be trained using either monolingualor bilingual features on their own.
With classifierstrained on even a little labeled data, it?s feasible thatfor a particular example, the monolingual classifiermight be confident when the bilingual classifier is1350Algorithm 1 The bilingual co-training algorithm: subscript m corresponds to monolingual, b to bilingualGiven: ?
a set L of labeled training examples in the bitext, {(x?i, yi)}?
a set U of unlabeled examples in the bitext, {x?j}?
hyperparams: k (num.
iterations), um and ub (size smaller unlabeled pools), nm and nb(num.
new labeled examples each iteration), C: regularization param.
for classifier trainingCreate Lm ?
LCreate Lb ?
LCreate a pool Um by choosing um examples randomly from U .Create a pool Ub by choosing ub examples randomly from U .for i = 0 to k doUse Lm to train a classifier hm using only x?m, the monolingual features of x?Use Lb to train a classifier hb using only x?b, the bilingual features of x?Use hm to label Um, move the nm most-confident examples to LbUse hb to label Ub, move the nb most-confident examples to LmReplenish Um and Ub randomly from U with nm and nb new examplesend foruncertain, and vice versa.
This suggests using aco-training approach (Yarowsky, 1995; Blum andMitchell, 1998).
We train separate classifiers on thelabeled data.
We use the predictions of one classi-fier to label new examples for training the orthogo-nal classifier.
We iterate this training and labeling.We outline how this procedure can be applied tobitext data in Algorithm 1 (above).
We follow priorwork in drawing predictions from smaller pools, Umand Ub, rather than from U itself, to ensure the la-beled examples ?are more representative of the un-derlying distribution?
(Blum and Mitchell, 1998).We use a logistic regression classifier for hm andhb.
Like Blum and Mitchell (1998), we also createa combined classifier by making predictions accord-ing to argmaxy=1,0 Pr(y|xm)Pr(y|xb).The hyperparameters of the algorithm are 1) k,the number of iterations, 2) um and ub, the size ofthe smaller unlabeled pools, 3) nm and nb, the num-ber of new labeled examples to include at each itera-tion, and 4) the regularization parameter of the logis-tic regression classifier.
All such parameters can betuned on a development set.
Like Blum and Mitchell(1998), we ensure that we maintain roughly the trueclass balance in the labeled examples added at eachiteration; we also estimate this balance using devel-opment data.There are some differences between our approachand the co-training algorithm presented in Blum andMitchell (1998, Table 1).
One of our key goals is toproduce an accurate classifier that uses only mono-lingual features, since only this classifier can be ap-plied to arbitrary monolingual text.
We thus breakthe symmetry in the original algorithm and allow hbto label more examples for hm than vice versa, sothat hm will improve faster.
This is desirable be-cause we don?t have unlimited unlabeled examplesto draw from, only those found in our parallel text.5 DataWeb-scale text data is used for monolingual featurecounts, parallel text is used for classifier co-training,and labeled data is used for training and evaluation.Web-scale N-gram Data We extract our countsfrom Google V2: a new N-gram corpus (withN-grams of length one-to-five) created from thesame one-trillion-word snapshot of the web as theGoogle 5-gram Corpus (Brants and Franz, 2006),but with enhanced filtering and processing of thesource text (Lin et al, 2010, Section 5).
We getcounts using the suffix array tools described in (Linet al, 2010).
We add one to all counts for smooth-ing.Parallel Data We use the Danish, German, Greek,Spanish, Finnish, French, Italian, Dutch, Por-tuguese, and Swedish portions of Europarl (Koehn,2005).
We also use the Czech, German, Span-ish and French news commentary data from WMT13512010.1 Word-aligned English-Foreign bitexts arecreated using the Berkeley aligner.2 We run 5 itera-tions of joint IBM Model 1 training, followed by 3-to-5 iterations of joint HMM training, and align withthe competitive-thresholding heuristic.
The Englishportions of all bitexts are part-of-speech tagged withCRFTagger (Phan, 2006).
94K unique coordinateNPs and their translations are then extracted.Labeled Data For experiments within the paral-lel text, we manually labeled 1320 of the 94K co-ordinate NP examples.
We use 605 examples to setdevelopment parameters, 607 examples as held-outtest data, and 2, 10 or 100 examples for training.For experiments on the WSJ portion of the PennTreebank, we merge the original Treebank annota-tions with the NP annotations provided by Vadas andCurran (2007a).
We collect all coordinate NP se-quences matching our pattern and collapse them intoa single example.
We label these instances by deter-mining whether the annotations have w1 and w2 con-joined.
In only one case did the same coordinate NPhave different labels in different occurrences; thiswas clearly an error and resolved accordingly.
Wecollected 1777 coordinate NPs in total, and dividedthem into 777 examples for training, 500 for devel-opment and 500 as a final held-out test set.6 Evaluation and SettingsWe evaluate using accuracy: the percentage of ex-amples classified correctly in held-out test data.We compare our systems to a baseline referred toas the Tag-Triple classifier.
This classifier has asingle feature: the tag(w1), tag(w2), tag(h) triple.Tag-Triple is therefore essentially a discriminative,unlexicalized parser for our coordinate NPs.All classifiers use L2-regularized logistic regres-sion training via LIBLINEAR (Fan et al, 2008).
Forco-training, we fix regularization at C = 0.1.
For allother classifiers, we optimize the C parameter on thedevelopment data.
At each iteration, i, classifier hmannotates 50 new examples for training hb, from apool of 750 examples, while hb annotates 50 ?
i newexamples for hm, from a pool of 750 ?
i examples.This ensures hm gets the majority of automatically-labeled examples.1www.statmt.org/wmt10/translation-task.html2nlp.cs.berkeley.edu/pages/wordaligner.html868890929496981000  10  20  30  40  50  60Accuracy(%)Co-training iterationBilingual ViewMonolingual ViewCombinedFigure 1: Accuracy on Bitext development data over thecourse of co-training (from 10 initial seed examples).We also set k, the number of co-training itera-tions.
The monolingual, bilingual, and combinedclassifiers reach their optimum levels of perfor-mance after different numbers of iterations (Fig-ure 1).
We therefore set k separately for each, stop-ping around 16 iterations for the combined, 51 forthe monolingual, and 57 for the bilingual classifier.7 Bitext ExperimentsWe evaluate our systems on our held-out bitext data.The majority class is ellipsis, in 55.8% of exam-ples.
For comparison, we ran two publicly-availablebroad-coverage parsers and analyzed whether theycorrectly predicted ellipsis.
The parsers were theC&C parser (Curran et al, 2007) and Minipar (Lin,1998).
They achieved 78.6% and 77.6%.3Table 5 shows that co-training results in muchmore accurate classifiers than supervised trainingalone, regardless of the features or amount of ini-tial training data.
The Tag-Triple system is theweakest system in all cases.
This shows that bettermonolingual features are very important, but semi-supervised training can also make a big difference.3We provided the parsers full sentences containing the NPs.
Wedirectly extracted the labels from the C&C bracketing, whilefor Minipar we checked whether w1 was the head of w2.
Ofcourse, the parsers performed very poorly on ellipsis involvingtwo nouns (partly because NP structure is absent from theirtraining corpora (see ?
2 and also Vadas and Curran (2008)),but neither exceeded 88% on adjective or mixed pairs either.1352# of ExamplesSystem 2 10 100Tag-Triple classifier 67.4 79.1 82.9Monolingual classifier 69.9 90.8 91.6Co-trained Mono.
classifier 96.4 95.9 96.0Relative error reduction via co-training 88% 62% 52%Bilingual classifier 76.8 85.5 92.1Co-trained Bili.
classifier 93.2 93.2 93.9Relative error reduction via co-training 71% 53% 23%Mono.+Bili.
classifier 69.9 91.4 94.9Co-trained Combo classifier 96.7 96.7 96.7Relative error reduction via co-training 89% 62% 35%Table 5: Co-training improves accuracy (%) over stan-dard supervised learning on Bitext test data for differentfeature types and number of training examples.System Accuracy ?Monolingual alone 91.6 -+ Bilingual 94.9 39%+ Co-training 96.0 54%+ Bilingual & Co-training 96.7 61%Table 6: Net benefits of bilingual features and co-trainingon Bitext data, 100-training-example setting.
?
= rela-tive error reduction over Monolingual alone.Table 6 shows the net benefit of our main contri-butions.
Bilingual features clearly help on this task,but not as much as co-training.
With bilingual fea-tures and co-training together, we achieve 96.7% ac-curacy.
This combined system could be used to veryaccurately resolve coordinate ambiguity in paralleldata prior to training an MT system.8 WSJ ExperimentsWhile we can now accurately resolve coordinate NPambiguity in parallel text, it would be even betterif this accuracy carried over to new domains, wherebilingual features are not available.
We test the ro-bustness of our co-trained monolingual classifier byevaluating it on our labeled WSJ data.The Penn Treebank and the annotations added byVadas and Curran (2007a) comprise a very specialcorpus; such data is clearly not available in everydomain.
We can take advantage of the plentiful la-beled examples to also test how our co-trained sys-tem compares to supervised systems trained with in-System Training WSJ Acc.Set # Nouns AllNakov & Hearst - - 79.2 84.8Tag-Triple WSJ 777 76.1 82.4Pitler et al WSJ 777 92.3 92.8MonoWSJ WSJ 777 92.3 94.4Co-trained Bitext 2 93.8 95.6Table 7: Coordinate resolution accuracy (%) on WSJ.domain labeled examples, and also other systems,like Nakov and Hearst (2005), which although un-supervised, are tuned on WSJ data.We reimplemented Nakov and Hearst (2005)4 andPitler et al (2010)5 and trained the latter on WSJ an-notations.
We compare these systems to Tag-Tripleand also to a supervised system trained on the WSJusing only our monolingual features (MonoWSJ).The (out-of-domain) bitext co-trained system is thebest system on the WSJ data, both on just the ex-amples where w1 and w2 are nouns (Nouns), and onall examples (All) (Table 7).6 It is statistically sig-nificantly better than the prior state-of-the-art Pitleret al system (McNemar?s test, p<0.05) and alsoexceeds the WSJ-trained system using monolingualfeatures (p<0.2).
This domain robustness is less sur-prising given its key features are derived from web-scale N-gram data; such features are known to gen-eralize well across domains (Bergsma et al, 2010).We tried co-training without the N-gram features,and performance was worse on the WSJ (85%) thansupervised training on WSJ data alone (87%).9 Related WorkBilingual data has been used to resolve a range ofambiguities, from PP-attachment (Schwartz et al,2003; Fossum and Knight, 2008), to distinguishinggrammatical roles (Schwarck et al, 2010), to fulldependency parsing (Huang et al, 2009).
Related4Nakov and Hearst (2005) use an unsupervised algorithm thatpredicts ellipsis on the basis of a majority vote over a numberof pattern counts and established heuristics.5Pitler et al (2010) uses a supervised classifier to predict brack-etings; their count and binary features are a strict subset of thefeatures used in our Monolingual classifier.6For co-training, we tuned k on the WSJ dev set but left otherparameters the same.
We start from 2 training instances; resultswere the same or slightly better with 10 or 100 instances.1353work has also focused on projecting syntactic an-notations from one language to another (Yarowskyand Ngai, 2001; Hwa et al, 2005), and jointly pars-ing the two sides of a bitext by leveraging the align-ments during training and testing (Smith and Smith,2004; Burkett and Klein, 2008) or just during train-ing (Snyder et al, 2009).
None of this work has fo-cused on coordination, nor has it combined bitextswith web-scale monolingual information.Most prior work has focused on leveraging thealignments between a single pair of languages.
Da-gan et al (1991) first articulated the need for ?a mul-tilingual corpora based system, which exploits thedifferences between languages to automatically ac-quire knowledge about word senses.?
Kuhn (2004)used alignments across several Europarl bitexts todevise rules for identifying parse distituents.
Ban-nard and Callison-Burch (2005) used multiple bi-texts as part of a system for extracting paraphrases.Our co-training algorithm is well suited to usingmultiple bitexts because it automatically learns thevalue of alignment information in each language.
Inaddition, our approach copes with noisy alignmentsboth by aggregating information across languages(and repeated occurrences within a language), andby only selecting the most confident examples ateach iteration.
Burkett et al (2010) also pro-posed exploiting monolingual-view and bilingual-view predictors.
In their work, the bilingual viewencodes the per-instance agreement between mono-lingual predictors in two languages, while our bilin-gual view encodes the alignment and target text to-gether, across multiple instances and languages.The other side of the coin is the use of syntax toperform better translation (Wu, 1997).
This is a richfield of research with its own annual workshop (Syn-tax and Structure in Translation).Our monolingual model is most similar to pre-vious work using counts from web-scale text, bothfor resolving coordination ambiguity (Nakov andHearst, 2005; Rus et al, 2007; Pitler et al, 2010),and for syntax and semantics in general (Lapataand Keller, 2005; Bergsma et al, 2010).
We donot currently use semantic similarity (either tax-onomic (Resnik, 1999) or distributional (Hogan,2007)) which has previously been found useful forcoordination.
Our model can easily include such in-formation as additional features.
Adding new fea-tures without adding new training data is often prob-lematic, but is promising in our framework, since thebitexts provide so much indirect supervision.10 ConclusionResolving coordination ambiguity is hard.
Parsersare reporting impressive numbers these days, butcoordination remains an area with room for im-provement.
We focused on a specific subcase, com-plex NPs, and introduced a new evaluation set.
Weachieved a huge performance improvement from79% for state-of-the-art parsers to 96%.7Size matters.
Most parsers are trained on a meremillion words of the Penn Treebank.
In this work,we show how to take advantage of billions of wordsof bitexts and trillions of words of unlabeled mono-lingual text.
Larger corpora make it possible touse associations among lexical items (compare dairyproduction vs. asbestos chloride) and precise para-phrases (production of dairy and meat).
Bitexts arehelpful when the ambiguity can be resolved by somefeature in another language (such as word order).The Treebank is convenient for supervised train-ing because it has annotations.
We show that evenwithout such annotations, high-quality supervisedmodels can be trained using co-training and featuresderived from huge volumes of unlabeled data.ReferencesColin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In Proc.
ACL,pages 597?604.Shane Bergsma, Emily Pitler, and Dekang Lin.
2010.Creating robust supervised classifiers via web-scale n-gram data.
In Proc.
ACL, pages 865?874.Avrim Blum and Tom Mitchell.
1998.
Combining la-beled and unlabeled data with co-training.
In Proc.COLT, pages 92?100.Thorsten Brants and Alex Franz.
2006.
The Google Web1T 5-gram Corpus Version 1.1.
LDC2006T13.David Burkett and Dan Klein.
2008.
Two languagesare better than one (for syntactic parsing).
In Proc.EMNLP, pages 877?886.David Burkett, Slav Petrov, John Blitzer, and Dan Klein.2010.
Learning better monolingual models with unan-notated bilingual text.
In Proc.
CoNLL, pages 46?53.7Evaluation scripts and data are available online:www.clsp.jhu.edu/?sbergsma/coordNP.ACL11.zip1354James Curran, Stephen Clark, and Johan Bos.
2007.
Lin-guistically motivated large-scale NLP with C&C andBoxer.
In Proc.
ACL Demo and Poster Sessions, pages33?36.Ido Dagan and Alan Itai.
1990.
Automatic processing oflarge corpora for the resolution of anaphora references.In Proc.
COLING, pages 330?332.Ido Dagan, Alon Itai, and Ulrike Schwall.
1991.
Twolanguages are more informative than one.
In Proc.ACL, pages 130?137.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
LIBLINEAR: A li-brary for large linear classification.
JMLR, 9:1871?1874.Victoria Fossum and Kevin Knight.
2008.
Using bilin-gual Chinese-English word alignments to resolve PP-attachment ambiguity in English.
In Proc.
AMTA Stu-dent Workshop, pages 48?53.Donald Hindle and Mats Rooth.
1993.
Structural ambi-guity and lexical relations.
Computational Linguistics,19(1):103?120.Deirdre Hogan.
2007.
Coordinate noun phrase disam-biguation in a generative parsing model.
In Proc.
ACL,pages 680?687.Liang Huang, Wenbin Jiang, and Qun Liu.
2009.Bilingually-constrained (monolingual) shift-reduceparsing.
In Proc.
EMNLP, pages 1222?1231.Rebecca Hwa, Philip Resnik, Amy Weinberg, ClaraCabezas, and Okan Kolak.
2005.
Bootstrappingparsers via syntactic projection across parallel texts.Natural Language Engineering, 11(3):311?325.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In Proc.
MT Summit X.Jonas Kuhn.
2004.
Experiments in parallel-text basedgrammar induction.
In Proc.
ACL, pages 470?477.Mirella Lapata and Frank Keller.
2005.
Web-basedmodels for natural language processing.
ACM Trans.Speech and Language Processing, 2(1):1?31.Mark Lauer.
1995.
Corpus statistics meet the noun com-pound: Some empirical results.
In Proc.
ACL, pages47?54.Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,David Yarowsky, Shane Bergsma, Kailash Patil, EmilyPitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani,and Sushant Narsale.
2010.
New tools for web-scaleN-grams.
In Proc.
LREC.Dekang Lin.
1998.
Dependency-based evaluation ofMINIPAR.
In Proc.
LREC Workshop on the Evalu-ation of Parsing Systems.Mitchell P. Marcus, Beatrice Santorini, and MaryMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19(2):313?330.Preslav Nakov and Marti Hearst.
2005.
Using the web asan implicit training set: application to structural ambi-guity resolution.
In Proc.
HLT-EMNLP, pages 17?24.Xuan-Hieu Phan.
2006.
CRFTagger: CRF English POSTagger.
crftagger.sourceforge.net.Emily Pitler, Shane Bergsma, Dekang Lin, and KennethChurch.
2010.
Using web-scale N-grams to improvebase NP parsing performance.
In In Proc.
COLING,pages 886?894.Philip Resnik.
1999.
Semantic similarity in a taxonomy:An information-based measure and its application toproblems of ambiguity in natural language.
Journal ofArtificial Intelligence Research, 11:95?130.Vasile Rus, Sireesha Ravi, Mihai C. Lintean, andPhilip M. McCarthy.
2007.
Unsupervised method forparsing coordinated base noun phrases.
In Proc.
CI-CLing, pages 229?240.Florian Schwarck, Alexander Fraser, and HinrichSchu?tze.
2010.
Bitext-based resolution of Germansubject-object ambiguities.
In Proc.
HLT-NAACL,pages 737?740.Lee Schwartz, Takako Aikawa, and Chris Quirk.
2003.Disambiguation of English PP attachment using mul-tilingual aligned data.
In Proc.
MT Summit IX, pages330?337.David A. Smith and Noah A. Smith.
2004.
Bilingualparsing with factored estimation: Using English toparse Korean.
In Proc.
EMNLP, pages 49?56.Benjamin Snyder, Tahira Naseem, and Regina Barzilay.2009.
Unsupervised multilingual grammar induction.In Proc.
ACL-IJCNLP, pages 1041?1050.David Vadas and James R. Curran.
2007a.
Adding nounphrase structure to the Penn Treebank.
In Proc.
ACL,pages 240?247.David Vadas and James R. Curran.
2007b.
Large-scalesupervised models for noun phrase bracketing.
In PA-CLING, pages 104?112.David Vadas and James R. Curran.
2008.
Parsing nounphrase structure with CCG.
In Proc.
ACL, pages 104?112.Vladimir N. Vapnik.
1998.
Statistical Learning Theory.John Wiley & Sons.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?403.David Yarowsky and Grace Ngai.
2001.
Inducing multi-lingual POS taggers and NP bracketers via robust pro-jection across aligned corpora.
In Proc.
NAACL, pages1?8.David Yarowsky.
1995.
Unsupervised word sense disam-biguation rivaling supervised methods.
In Proc.
ACL,pages 189?196.1355
