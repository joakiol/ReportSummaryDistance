COMPUTATIONAL COMPLEXITY  OF CURRENT GPSG THEORYEric Sven RistadMIT Artificial Intelligence Lab Thinking Machines Corporation545 Technology Square and 245 First StreetCambridge, MA 02139 Cambridge, MA 02142ABSTRACTAn important goal of computational linguistics has been to uselinguistic theory to guide the construction of computationallyefficient real-world natural language processing systems.
At firstglance, generalized phrase structure grammar (GPSG) appearsto be a blessing on two counts.
First, the precise formalisms ofGPSG might be a direct and fransparent guide for parser designand implementation.
Second, since GPSG has weak context-freegenerative power and context-free languages can be parsed inO(n ~) by a wide range of algorithms, GPSG parsers would ap-pear to run in polynomial time.
This widely-assumed GPSG"efficient parsability" result is misleading: here we prove thatthe universal recognition problem for current GPSG theory isexponential-polynomial t me hard, and  assuredly intractable.The paper pinpoints ources of complexity (e.g.
metarules andthe theory of syntactic features) in the current GPSG theoryand concludes with some linguistically and computationally mo-tivated restrictions on GPSG.1 IntroductionAn important goal of computational linguistics has been to uselinguistic theory to guide the construction of computationallyefficient real-world natural anguage processing systems.
Gen-eralized Phrase Structure Grammar (GPSG) linguistic theoryholds out considerable promise as an aid in this task.
The pre-cise formalisms of GPSG offer the prospect of a direct and trans-parent guide for parser design and implementation.
Further-more, and more importantly, GPSG's weak context-free gener-ative power suggests an efficiency advantage for GPSG-basedparsers.
Since context-free languages can be parsed in polyno-mial time, it seems plausible that GPSGs can also be parsed inpolynomial time.
This would in turn seem to provide "the be-ginnings of an explanation for the obvious, but largely ignored,fact thatlhumans process the utterances they hear very rapidly(Gazdar,198\] :155)."
1In this paper I argue that the expectations of the informalcomplexity argument from weak context-free generative powerare not in fact met.
I begin by examining the computationalcomplexity of metarules and the feature system of GPSG andshow that these systems can lead to computational intractabil-~See also Joshi, "Tree Adjoining Grammars ~ p.226, in Natural LanguageParsing (1985) ed.
by D. Dowty, L. Karttunen, and A. Zwicky, CambridgeUniversity Press: Cambridge, and aExceptlons tothe Rule, ~ Science News128: 314-315.ity.
Next I prove that the universal recognition problem for cur-rent GPSG theory is Exp-Poly hard, and assuredly intractable.
2That is, the problem of determining for an arbitrary GPSG Gand input string z whether x is in the language L(G) gener-ated by G, is exponential polynomial time hard.
This resultputs GPSG-Recognition i a complexity class occupied by fewnatural problems: GPSG-Recognition is harder than the trav-eling salesman problem, context-sensitive language recognition,or winning the game of Chess on an n x n board.
The complex-ity classification shows that the fastest recognition algorithm forGPSGs must take exponential time or worse.
One role of a com-putational analysis is to provide formal insights into linguistictheory.
To this end, this paper pinpoints ources of complexityin the current GPSG theory and concludes with some linguisti-cally and computationally motivated restrictions.2 Complexity of GPSG ComponentsA generalized phrase structure grammar contains five language-particular components - -  immediate dominance (ID) rules, meta-rules, linear precedence (LP) statements, feature co-occurrencerestrictions (FCRs), and feature specification defaults (FSDs)- -  and four universal components - - a theory of syntactic fea-tures, principles of universal feature instantiation, principles ofsemantic interpretation, and formal relationships among variouscomponents of the grammar, sSyntactic ategories are partial functions from features toatomic feature values and syntactic ategories.
They encodesubcategorization, agreement, unbounded dependency, and othersignificant syntactic information.
The set K of syntactic ate-gories is inductively specified by listing the set F of features, theset A of atomic feature values, the function po that defines therange of each atomic-valued feature, and a set R of restrictivepredicates on categories (FCRs).The set of ID rules obtained by taking the finite closure ofthe metarules on the ID rules is mapped into local phrase struc-ture trees, subject to principles of universal feature instantia-tion, FSDs, FCRs, and LP statements.
Finally, local trees are2We use the universal problem to more accurately explore the powerof a grammatical formalism (see section 3.1 below for support).
Ris-tad(1985) has previously proven that the universal recognition problem forthe GPSG's of Gazdar(1981) is NP-hard and likely to be intractable, venunder severe metarule restrictions.3This work is based on current GPSG theory as presented in Gazdar et.al.
(1985), hereafter GKPS.
The reader is urged to consult hat work for aformal presentation a d thorough exposition of current GPSG theory.30assembled to form phrase structure trees, which are terminatedby lexical elements.To identify sources of complexity in GPSG theory, we con-sider the isolated complexity of the finite metarule closure Ol>-station and the rule to tree mapping, using the finite closuremembership and category membership problems, respectively.Informally, the finite closure membership problem is to deter-mine if an ID rule is in the finite closure of a set of metarules Mon a set of ID rules R. The category membership problem is todetermine if a category or C or a legal extension of C is in theset K of all categories based the function p and the sets A, Fand R. Note that both problems must be solved by any GPSG-based parsing system when computing the ID rule to local treemapping.The major results are that finite closure membership is NP-hard and category membership s PSPACE-hard.
Barton(1985)has previously shown that the recognition problem for ID /LPgrammars is NP-hard.
The components of GPSG theory arecomputationally complex, as is the theory as a whole.Assumptions.
In the following problem definitions, we allowsyntactic categories to be based on arbitrary sets of featuresand feature values.
In actuality, GPSG syntactic categories arebased on fixed sets and a fixed function p. As such, the set K ofpermissible categories is finite, and a large table containing Kcould, in princip}e, be given.
4 We (uncontroversially) generalizeto arbitrary sets and an arbitrary function p to prevent such asolution while preserving GPSG's theory of syntactic features, sNo other modifications to the theory are made.An ambiguity in GKPS is how the FCRs actually apply toembedded categories.
6 Following Ivan Sag (personal communi-cation), I make the natural assumption here that FCRs applytop-level and to embedded categories equally.4This suggestion isof no practical significance, because the actual num-ber of GPSG syntactic ategories i extremely large.
The total number ofcategories, given the 25 atomic features and 4 category-valued features, is:J K = K '  I = 32s((1 +32s)C(1 +32s)((1 ?32~)(1 +32s)~)2)s)"~_ 32s(1 + 32~) s4 > 3 le2~ > 10 TMSee page 10 for details.
Many of these categories will be linguisticallymeaningless, but all GPSGs will generate all of them and then filter someout in consideration of FCRs, FSDs, universal feature instantiation, andthe other admissible local trees and lexical entries in the GPSG.
Whilethe FCRs in some grammars may reduce the number of categories, FCRsare a language-particular component of the grammar.
The vast number ofcategories cited above is inherent in the GPSG framework.SOur goal is to identify sources of complexity in GPSG theory.
The gen-eralization to arbitrary sets allows a fine-grained study of one componentof GPSG theory (the theory of syntactic features) with the tools of compu-tational complexity theory.
Similarly, the chess board is uncontroverslallygeneralized to size n ?
a in order to study the computational complexity ofchess.eA category C that is defined for a feature \], f E (F - Atom) n DON(C)(e.g.
f = SLASH ), contains an embedded category C~, where C(f) --- C~.GKPS does not explain whether FCR's must be true of C~ as well as C.2.1 Metaru lesThe complete set of ID rules in a GPSG is the maximal set thatcan be arrived at by taking each metarule and applying it tothe set of rules that have not themselves arisen as a result ofthe application of that metarule.
This maximal set is called thefinite closure (FC) of a set R of lexical ID rules under a set Mof metarules.The cleanest possible complexity proof for metarule finiteclosure would fix the GPSG (with the exception of metarules)for a given problem, and then construct metarules dependenton the problem instance that is being reduced.
Unfortunately,metarules cannot be cleanly removed from the GPSG system.Metarules take ID rules as input, and produce other ID rules astheir output.
If we were to separate metarules from their inputsand outputs, there would be nothing left to study.The best complexity proof for metarules, then, would fixthe GPSG modulo the metarules and their input.
We ensurethe input is not inadvertently performing some computation byrequiring the one ID rule R allowed in the reduction to be fullyspecified, with only one 0-1evel category on the left-hand sideand one unanalyzable terminal symbol on the right-hand side.Furthermore, no FCRs, FSDs, or principles of universal featureinstantiation are allowed to apply.
These are exceedingly severeconstraints.
The ID rules generated by this formal system willbe the finite closure of the lone ID rule R under the set M ofmetarules.The (strict, resp.)
finite closure membership roblem forGPSG metarules is: Given an ID rule r and sets of metarulesM and ID rules R, determine if 3r e such that r I ~ r (r I = r,resp.)
and r I ?
FC(M, R).Theorem 1: Finite Closure Membership is NP-hardProo f :  On input 3-CNF formula F of length n using the m vari-ables z l .
.
.
x,~, reduce 3-SAT, a known NP-complete problem,to Metarule-Membership in polynomial time.The set of ID rules consists of the one ID rule R, whosemother category represents the formula variables and clauses,and a set of metarules M s.t.
an extension of the ID rule A is inthe finite closure of M over R iff F is satisfiable.
The metarulesgenerate possible truth assignments for the formula variables,and then compute the truth value of F in the context of thosetruth assignments.Let w be the string of formula literals in F, and let wl denotethe i th symbol in the string w.1.
The ID rules R,A31R:A:where<satisfiable><satisfiability>F=F--*<satisfiabil ity>\[\[STAGE 3\]\]-~<satisfiable>is a terminal symbolis a terminal symbol{\[y, 0 \ ] : l< i<m}u {lc, o\]:I<i< ~}U {\[STAGE I \ ]}2.
Construct he metarules(a) m metarules to generate all possible assignments othe variablesVi, 1 < i < m{\[yi 0\],\[STAGE I\]} -* W (i){\[Yi I\],\[STAGE 1\]} ~ W(b) one metarule to stop the assignment generation pro-cess{\[STAGE 1\]) -~ W(2){\[STAGE 2\]} --* W(c) I w\[ metarules to verify assignmentsVi, j ,k  1<i<1~ j, l <_ j <_ m, O < k < 2,if wsi-k : xj, then construct he metarule{\[yi 1\],\[ei 0\],\[STAGE 2\]) --+ W(3){\[yj i\],\[ci 1\], \[STAGE 2\]} --' WVi, j ,k l< i<~ -1, l<_ j<_m,O<k<_2,if wsi-k = ~,  then construct he metarule{\[yj 0\], \[cl 0\], \[STAGE 2\]} -* W(4){\[yj O\],\[ci 1\],\[STAGE 2\]}---,W(d) Let the category C = {\[ci 1\]: 1 < i < l~J}.
Con-struct the metaruleC\[STAGE 2\] -~ W{\[STAGE 3\]} --* <satisfiable>(5)The reduction constructs O(I w l) metarules of size log(I w \[),and clearly may be performed in polynomial time: the reduc-tion time is essentially the number of symbols needed to writethe GPSG down.
Note that the strict finite closure membershipproblem is also NP-hard.
One need only add a polynomial num-ber of metarules to "change" the feature values of the mothernode C to some canonical value when C(STAGE ) = 3 - -  all 0,for example, with the exception of STAGE .
Let F = {\[Yi 0\] :l< i<m} U {\[c, O \ ] : l< i< ~}.
Then A would beA : F\[STAGE 3\] -~ <sat i s f iab le>Q.
?.PThe major source of intractability is the finite closure opera-tion itself.
Informally, each metarule can more than double thenumber of ID rules, hence by chaining metarules (i.e.
by apply-ing the output of a metarule to the input of the next metarule)finite closure can increase the number of ID rules exponentiallyff2.2 A Theory  o f  Syntact i c  FeaturesHere we show that the complex feature system employed byGPSG leads to computational intractability.
The underlyinginsight for the following complexity proof is the almost directequivalence between Alternating Turing Machines (ATMs) andsyntactic categories in GPSG.
The nodes of an ATM compu-tation correspond to 0-level syntactic ategories, and the ATMcomputation tree corresponds to a full, n-level syntactic ate-gory.
The finite feature closure restriction on categories, whichlimits the depth of category nesting, will limit the depth ofthe corresponding ATM computation tree.
Finite feature clo-sure constrains us to specifying (at most) a polynomially deep,polynomially branching tree in polynomial time.
This is ex-actly equivalent to a polynomial time ATM computation, andby Chandra and Stockmeyer(1976), also equivalent to a deter-ministic polynomial space-bounded 'luring Machine computa-tion.As a consequence of the above insight, one would expectthe GPSG Category-Membership problem to be PSPACE-hard.The actual proof is considerably simpler when framed as a re-duction from the Quantified Boolean Formula (QBF) problem,a known PSPACE-complete problem.Let a specification of K be the arbitrary sets of features F,atomic features Atom, atomic feature values A, and feature co-occurrence restrictions R and let p be an arbitrary function, allequivalent to those defined in chapter 2 of GKPS.The category membership problem is: Given a category Cand a specification of a set K of syntactic ategories, determinei f3C  Is.t.
C I~CandC IEK .The QBF problem is {QIF1Qzyz... Qmy,nF(yh YZ,..., y,n) IQi 6 {V, 3}, where the yi are boolean variables, F is a booleanformula of length n in conjunctive normal form with exactly~More precisely, the metarule finite closure operation can increase thesize of a GPSG G worse than exponentially: from I Gi to O(\] G \[2~).
Givena set of ID rules R of symbol size n, and a set M of m metarule, each ofsize p, the symbol size of FC(M,R) is O(n z~) = O(IGIZ~).
Each met~ulecan match the productions in R O(n) different ways, inducing O(n + p)new symbols per match: each metarule can therefore square the ID rulegrammar size.
There are m metarules, o finite closure can create an IDrule grammar with O(n 2~) symbols.32three variables per clause (3-CNF), and the quantified formulais true}.Theorem 2: GPSG Category-Membership is PSPACE-hardProof :  By reduction from QBF.
On input formulafl = Q ly lQ2y2 .
.
.
QmymF(y l ,  y2, .
.
.
, y,~)we construct an instance P of the Category-Membershipproblem in polynomial time, such that f~ E QBF if and onlyif P is true.Consider the QBF as a strictly balanced binary tree, wherethe i th quantifier Qi represents pairs of subtrees < Tt, T!
> suchthat (1) Tt and T!
each immediately dominate pairs of subtreesrepresenting the quantifiers Qi+l ... Qra, and (2) the i th variableyi is t rue in T~ and false in Tf.
All nodes at level i in the wholetree correspond to the quantifier Qi.
The leaves of the tree aredifferent instantiations of the formula F, corresponding to thequantifier-determined truth assignments o the m variables.
Aleaf node is labeled true if the instantiated formula F that itrepresents is true.
An internal node in the tree at level i islabeled t rue if1.
Qi = "3" and either daughter is labeled true, or2.
Qi  -= "V" and both daughters are labeled true.Otherwise, the node is labeled false.Similarly, categories can be_understood as trees, where thefeatures in the domain of a category constitute a node in thetree, and a category C immediately dominates all categories C ~such that Sf  e ( ( r  - Atom) A DON(C)) \ [C( f )  = C'\].In the QBF reduction, the atomic-valued features are usedto represent he m variables, the clauses of F, the quantifierthe category represents, and the truth label of the category.The category-valued features represent the quantifiers - -  twocategory-valued features qk,qtk represent he subtree pairs <Tt, T I > for the quantifier Qk.
FCRs maintain quantifier-imposedvariable truth assignments "down the tree" and calculate thetruth labeling of all leaves, according to F, and internal nodes,according to quantifier meaning.Detai ls .
Let w be the string of formula literals in F, and w~denote the i th symbol in the string w. We specify a set K ofpermissible categories based on A, F, p,.and the set of FCRs Rs.t.
the category \[\[LABEL 1\]\] or an extension of it is an elementof K iff ~ is true.First we define the set of possible 0-level categories, whichencode the formula F and truth assignments to the formulavariables.
The feature wi represents the formula literal wi in w,yj represents the variable yj in f2, and ci represents the truthvalue of the i th clause in F.Atom = {LEVEL ,LABEL }u {w,: 1 < i <lwl}u {y:- : 1 < j< m}u {c~:1<;< ~}F -Atom = {qk,q~ : l < k < m}p?
(LEVEL) = {k : l  <k< mA-1}po( f )  = {0,1} Vf E Atom-  {LEVEL }FCR's are included to constrain both the form and content ofthe guesses:1.
FCR's to create strictly balanced binary trees:Vk, l<k<m,\]LEVEL k\] = \[qk \[\[Yk 1\]\[LEVEL k + 1\]\]\]&\[ql \[\[Vk 0\]\[LEVEL k + 1\]\]\]2.
FCR's to ensure all 0-level categories are fully specified:Vi, 1 < i< m\[c,\] = \ [w3, -~\ ]&\ [~3~- l \ ]&\ [~3, \ ]\]LABEL \] -- = \[cl\]Vk,  1 <k<m,\[LABEL\] --= \[yk\]3.
FCR's to label internal nodes with truth values deter-mined by quantifier meaning:Vk, l<k<rn ,if Qk = "V", then include:\[LEVEL k\]&\[LABEL 1\] ------ \[qk \[\[LABEL ll\]\]&\[q~ \[\[LABEL 1\]\]1\[LEVEL k\]&\[LABEL O\] ----- \[qk \[\[LABEL 0\]\]\] V \[q~ \[\[LABEL 0\]\]1otherwise Qk = "3", and include:\[LEVEL k\]&\[LABEL 1\] -- \[qk \[\[LABEL 11\]\] Y \[q~ \[\[LABEL I\]\]\]\[LEVEL k\]&\[LABEL O\] -- \[qk \[\[LABEL 0\]\]\]&Iq ~ \[\[LABEL 0\]\]\]The category-valued features qk and q~ represent the quan-tifier Qk.
In the category value of qk, the formula vari-able yk = 1 everywhere, while in the category value of q~,Yk = 0 everywhere.4.
one FCR to guarantee that only satisfiable assignmentsare permitted:\[LEVEL 1\] ~ ILABEL 1\]5.
FCR's to ensure that quantifier assignments are preserved"down the tree":Vi, k l<_ i<k<m,\[Yi 1\] D \[qk \[\[Yi 1\]\]\]&\[q~ \[\[Yi 1\]\]\]\[~, O\] ~ \[q~ \[\[y~ o\]\]\]&\[q i \[\[y~ 0\]\]\]336.
FCR's to instantiate variable assignments into the formulaF:Vi, k l  < i< lw\ [  and 1< k<m,if wi = Yk, then include:\[Yk 11 D \[w, 11\[~ko\] D \ [~o\ ]else if wi = Y-~, then include:\[y,~ :\] D \[~, o\]\[~,~, o\] D N, 1\]7.
FCR's to verify the guessed variable assignments in leafnodes:Vi l< i<~,It, o\] _= \[~s,-2 o \ ]~\ [~,_ ,  o \ ]~\ [~,  o\]\[ci 1\] -- \[ws,-~ 1\]V\[ws,_I 1\]V\[ws, 1\]\ [LEVEL rn + l\]&\[c, 0\] D \ [LABEL 0\]\ [LEVEL m+ 1\]d~\[Cx 1\]&:\[c2 l\]&...&\[c~ol/31 \] D \[LABEL 11The reduction constructs O(1~1) features and O(m ~) FCRsof size O(log m) in a simple manner, and consequently may beseen to be polynomial time.
0.~.PThe primary source of intractability in the theory of syn-tactic features is the large number of possible syntactic ate-gories (arising from finite feature closure) in combination withthe computational power of feature co-occurrence r strictions,FCRs of the "disjunctive consequence" form \[f v\] D \[fl vl\] V. .
.
V \[fn vn\] compute the direct analogue of Satisfiability: whenused in conjunction with other FCRs, the GPSG effectivelymust try all n feature-value combinations.3 Complex i ty  o f  GPSG-Recogn i t ionTwo isolated membership problems for GPSG's component for-mal devices were considered above in an attempt to isolatesources of complexity in GPSG theory.
In this section the recog-nition problem (RP) for GPSG theory as a whole is considered.I begin by arguing that the linguistically and computationallyrelevant recognition problem is the universal recognition prob-lem, as opposed to the fixed language recognition problem.
Ithen show that the former problem is exponential-polynomial(Exp-Poly) time-hard.SFinite feature closure admits a surprisingly large number of possiblecategories.
Given a specification (F, Atom, A, R, p) of K, let a =lAteral andb = IF  - Atom I.
Assume that all atomic features are binary: a feature maybe +,-, or undefined and there are 3 a 0-1evel categories.
The b category-valued features may each assume O(3 ~) possible values in a 1-1evel category,so I/f' I= O(3=(3")b).
More generally,IK = K ' I -  O(3~'~C ~ or r~ - ,= ) = O(3  ~? '
'  ~C:oo ,~)  = O(~*" . )
= O(3  o.'
')where E~=o ~ converges toe ~ 2.7 very rapidly and a,b = O(IGI) ; a =25, b = 4 in GKPS.
The smallest category in K will be 1 symbol (nullset), and the largest, maximally-specified, category wilt be of symbol-slzelog I K I = oca.
b!
).3.1 Def in ing  the  Recogn i t ion  Prob lemThe universal recognition problem is: given a grammar G andinput string x, is z C L(G)?.
Alternately, the recognition prob-lem for a class of grammars may be defined as the family ofquestions in one unkown.
This fized language recognition prob-lem is: given an input string x, is z E L for some fixed languageL?.
For the fixed language RP, it does not matter which gram-mar is chosen to generate L - -  typically, the fastest grammar ispicked.It seems reasonable clear that the universal RP is of greaterlinguistic and engineering interest han the fixed language RP.The grammars licensed by linguistic theory assign structuraldescriptions to utterances, which are used to query and updatedatabases, be interpreted semantically, translated into other hu-man languages, and so on.
The universal recognition problem- -  unlike the fixed language problem - -  determines membershipwith respect o a grammar, and therefore more accurately mod-els the parsing problem, which must use a grammar to assignstructural descriptions.The universal RP also bears most directly on issues of nat-ural language acquisition.
The language learner evidently pos-sesses a mechanism for selecting grammmars from the class oflearnable natural anguage grammars/~a on the basis of linguis-tic inputs.
The more fundamental question for linguistic theory,then, is "what is the recognition complexity of the class /~c?
".If this problem should prove computationally intractable, thenthe (potential) tractability of the problem for each languagegenerated by a G in the class is only a partial answer to thelinguistic questions raised.Finally, complexity considerations favor the universal RP.The goal of a complexity analysis is to characterize the amountof computational resources (e.g.
time, space) needed to solve theproblem in terms of all computationally relevent inputs on somestandard machine model (typically, a multi-tape deterministicTuring machine).
We know that both input string length andgrammar size and structure affect the complexity of the recog-nition problem.
Hence, excluding either input from complexityconsideration would not advance our understanding.
9Linguistics and computer science are primarily interested inthe universal recognition problem because both disciplines areconcerned with the formal power of a family of grammars.
Lin-guistic competence and performance must be considered in thelarger context of efficient language acquisition, while computa-tional considerations demand that the recognition problem becharacterized in terms of both input string and grammar size.Excluding grammar size from complexity consideration i orderSThis ~consider all relevant inputs ~ methodology is universally assumedin the formal language and computational complexity literature.
For ex-ample, Hopcraft and Ullman(1979:139) define the context-free grammarrecognition problem as: "Given a CFG G = (V,T,P, $) and a string z inY', is x in L(G)?.".
Garey and Johnson(1979) is a standard reference workin the field of computational complexity.
All 10 automata nd languagerecognition problems covered in the book (pp.
265-271) are universal, i.e.of the form "Given an instance of a machine/grammar and an input, doesthe machine/grammar accept he input7 ~ The complexity of these recog-nition problems is alt#ays calculated in terms of grammar and input size.34to argue that the recognition problem for a family of grammarsis tractable is akin to fixing the size of the chess board in orderto argue that winning the game of chess is tractable: neitherclaim advances our scientific understanding of chess or naturallanguage.3.2 GPSG-Recogn i t ion  is Exp-Po ly  hardTheorem 3: GPSG-Recognition is Exp-Poly time-hardProo f  3: By direct simulation of a polynomial space boundedalternating Turing Machine M on input w.Let S(n) be a polynomial in n. Then, on input M, a S(n)space-bounded one tape alternating Turing Machine (ATM),and string w, we construct a GPSG G in polynomial time suchthat w E L(M) iff $0wllw22...w,~n$n?l E L(G).By Chandra and Stockmeyer(1976),ASPACE(S(n)) = U DTIM~ cs("))c:>0where ASPACE(S(n)) is the class of problems olvable inspace Sin ) on an ATM, and DTIME(F(n)) is the class of prob-lems solvable in time F(n) on a deterministic Turing Machine.As a consequence of this result and our following proof, we havethe immediate result that GPSG-Recognition is DTIME(cS(n)) -hard, for all constants c, or Exp-Poly time-hard.An alternating Turing Machine is like a nondeterministicTM, except that some subset of its states will be referred toas universal states, and the remainder as existential states.
Anondeterministic TM is an alternating TM with no universalstates.
10The nodes of the ATM computation tree are represented bysyntactic ategories in K ?
- -  one feature for every tape square,plus three features to encode the ATM tape head positions andthe current state.
The reduction is limited to specifying a poly-nomial number of features in polynomial time; since these fea-tures are used to encode the ATM tape, the reduction may onlyspecify polynomial space bounded ATM computations.The ID rules encode the ATM NextM() relation, i.e.
C ---*NextM(C)  for a universal configuration C. The reduction con-structs an ID rule for every combination of possible head po-sition, machine state, and symbol on the scanned tape square.Principles of universal feature instantiation transfer the rest ofthe instantaneous description (i.e.
contents of the tape) frommother to daughters in ID rules.1?Our ATM definition is taken from Chandra nd Stockmeyer(1976), withthe restriction that the work tapes are one-way infinite, instead of two-wayinfinite.
Without loss of generality, we use a 1-tape ATM, soC (Q x r)  ?
(Q ?
r k ?
(L,R} x (L,R))Let NextM(C ) ---- {C0,C l , .
.
.
,Ck}.
If C is a universal con-figuration, then we construct an ID rule of the formc ~ Co, C l , .
.
.
, ck  (6)Otherwise, C is an existential coi~figuration a d we constructthe k + 1 ID rulesc --, c~ vi, 0<i<k  (7)A universal ATM configuration is labeled accepting if andonly if it has halted and accepted, or if all of its daughters arelabeled accepting.
We reproduce this with the ID rules in 6(or 8), which will be admissible only if all subtrees rooted bythe RHS nodes are also admissible.An existential ATM configuration is labeled accepting if andonly if it has halted and accepted, or if one of its daughters islabeled accepting.
We reproduce this with the ID rules in 7(or 9), which will be admissible only if one subtree rooted by aRHS node is admissible.All features that represent tape squares are declared to bein the HEAD feature set, and all daughter categories in theconstructed ID rules are head daughters, thus ensuring that theHead Feature Convention (HFC) will transfer the tape contentsof the mother to the daughter(s), modulo the tape writing ac-tivity specified by the next move relation.Detai ls .Le__ttResult0M(i ,  a, d) =\[\[HEAD0 i+ l l , \ [ i  a\],\[A 1\]\] i fd=R\[\[HEAD0 i - 1\],\[i a\], \[A 1\]\] if d = LResu l t lM( j ,  c, p, d) =\[\[HEAD1 j+ l \ ] , \ [ r f  c\]\[STATE p\]\] if d= R\[\[HEAD1 j -  l \ ] , \ [ r i  c\]\[STATE pl\] if d= LTransM(q, a, b) = ((p, c, dl, d2): ((q, a, b), (p;c, dl, d2>) e B}wherea is the read-only (R/O) tape symbol currentlybeing scannedb is the read-write (R/W) tape symbol cur-rently being scanneddl is the R/O tape directiond2 is the R/W tape directionThe GPSG G contains:1.
Feature definitions35A category in K ?
represents a node of an ATM compu-tation tree, where the features in Atom encode the ATMconfiguration.
Labeling is performed by ID rules.
(a) definition of F, Atom, AF : Atom =A ={STATE ,HEADO ,HEAD1 ,A}u { i :O<i<\ [w l+ l  }u {r i :  1 _< j _< S( Iw l )}Q U E U r ; as defined earlier(b) definition of p0p?
(A) = {1,2,3}p?
(STATE ) = Q ; the ATM state setp?
(HEADO ) : { j :  1 < j <-I~1}p?
(HEAD1 ) = { i :  1 < i < S( I~ I )}vf  ?
{; :  o < ; <1~1 +1}po(f) = Z U {$} ; the ATM input alphabetVf ?
{ry : 1 < j < s( l~ l )}pO(f) = F ; the ATM tape alphabet(c) definition of HEAD feature setHEAD = { i :  0 _< ; -<M +l}u{r j .
: 1 _< j _< S(l~l)}(d) FCRs to ensure full specification of all categories ex-cept null ones.Vf .
f  e Atom, \[STATE \]D \[f\]2.
Grammatical rulesVi,j,q,a,b :1< / <lwl, 1 < J-< S(I~I), qcQ,  aeZ,  betif TransM(q, a, b) # @, construct he following ID rules.
(a) if q ?
U (universal state){\[HEADO i\], \[i a\], \[HEAD1 j\], Jr; b\], \[STATE q\], \[A I\]} --*{ResultOM(i, a, dlk) U Resul t  1M(j, ck, Pk, d2k) :(Pk, ck, dlk, d2k) e TransM(q, a, b)}(s)where all categories on the RHS are heads.
(b) otherwise q ?
Q - U (existential state)V(pk, ck, dlk, d2~) E TransM(q, a, b),{\[HEADO i\], \[i a\], \[HEAD1 j\], \[rj b\], \[STATE q\], \[A I\]} ---+ResultOM({ , a, dlk ) U Resul t  1M(\], ck,pk ,d2k )(9)where all categories on the RHS are heads.
(c) One ID rule to terminate accepting states, using null-transitions.
{\[STATE h\], \[1 Y\]} --* ~ (10)(d) Two ID rules to read input strings and begin theATM simulation.
The A feature is used to separatefunctionally distinct components of the grammar.
\[A1\] categories participate in the direct ATM simula-tion, \[A 2\] categories are involved in reading the in-put string, and the \[A 3\] category connects the readinput string with the ATM simulation start state.START---* {\[A 1\]},{\[A 21}(11){\[a 2\]}--~ {\[A 2\]},{\[A 2\]}where all daughters are head daughters, and whereSTART : {\[HEAD0 1\],\[HEAD1 I\],\[STATE s\],\[A 3\]}u {\[rj #1 : 1 _< j _< s(M)}(e) the lexical rules,Va, i acE ,  l< i< lw l ,< ~;,{\[A 2\],\[; ~\]} >(12)vi o _< i <lwl +1,< $i,{\[A 2\],\[i $\]} >The reduction plainly may be performed in polynomial timein the size of the simulated ATM, by inspection.No metarules or LP statements are needed, although recta-rules could have been used instead of the Head Feature Conven-tion.
Both devices are capable of transferring the contents of theATM tape from the mother to the daughter(s).
One metarulewould be needed for each tape square/tape symbol combinationin the ATM.GKPS Definition 5.14 of Admissibility guarantees that ad-missible trees must be terminated, n By the construction above- -  see especially the ID rule 10 - -  an \[A 1\] node can be termi-nated only if it is an accepting configuration (i.e.
it has haltedand printed Y on its first square).
This means the only admis-sible trees are accepting ones whose yield is the input stringfollowed by a very long empty string.
P.C.P**The admissibility ofnonlocal trees is defined as follows (GKPS, p.104):Definition: AdmissibilityLet R be a set of ID rules.
Then a tree t is admissible from Rif and only if1.
t is terminated, and2.
every local subtree in.
t is either terminated or locallyadmissible from some r 6 R.363.3  Sources  o f  In t rac tab i l i tyThe two sources Of intractability in GPSG theory spotlightedby this reduction are null-transitions in ID rules (see the IDrule 10 above), and universal feature instantiation (in this case,the Head Feature Convention).Grammars with unrestricted null-transitions can assign elab-orate phrase structure to the empty string, which is linguisti-cally undesirable and computationally costly.
The reductionmust construct a GPSG G and input string x in polynomialt ime such that x E L(G) iff w E L(M), where M is a PSPACE-bounded ATM with input w. The 'polynomial t ime' constraintprevents us from making either x or G too big.
Null-transitionsallow the grammar to simulate the PSPACE ATM computation(and an Exp-Poly TM computation indirectly) with an enor-mously long derivation string and then erase the string.
If theGPSG G were unable to erase the derivation string, G wouldonly accept strings which were exponentially larger than M andw, i.e.
too big to write down in polynomial time.The Head Feature Condition transfers HEAD feature val-ues from the mother to the head daughters just in case theydon't  conflict.
In the reduction we use HEAD'features to en-code the ATM tape, and thereby use the HFC to transfer thetape contents from one" ATM configuration C (represented bythe mother) to its immediate successors Co,... ,Cn (the headdaughters}.
The configurations C, C0, .
.
.
,Ca have identical tapes,with the critical exception of one tape square.
If the HFC en-forced absolute agreement between the HEAD features of themother and head daughters, we would be unable to simulate thePSPACE ATM computation in this manner.4 Interpreting the Result4.1  Generat ive  Power  and  Computat iona l  Com-p lex i tyAt first glance, a proof that GPSG-Recognit ion is Exp-Poly hardappears to contradict he fact that context-free languages canbe recognized in O(n s) t ime by a wide range of algorithms.
Tosee why there is no contradiction, we must first explicitly statethe argument from weak context-free generative power, whichwe dub the efficient parsability (EP) argument.The EP argument states that any GPSG can be convertedinto a weakly equivalent context-free grammar (CFG), and thatCFG-Recognition is polynomial time; therefore, GPSG-Recognit ionmust also be polynomial time.
The EP argument continues: ifthe conversion is fast, then GPSG-Recognit ion is fast, but evenif the conversion is slow, recognition using the "compiled" CFGwill still be fast, and we may justifiably lose interest in recogni-tion using the original, slow, GPSG.The EP argument is misleading because it ignores both theeffect conversion has on grammar size, and the effect grammarsize has on recognition speed.
Crucially, grammar size affectsrecognition time in all known algorithms, and the only gram-mars directly usable by context-free parsers, i.e.
with the samecomplexity as a CFG, are those composed of context-free pro-ductions with atomic nonterminal symbols.
For GPSG, this isthe set of admissible local trees, and this set is astronomical:o((3 m~','+') (Iz)in a GPSG G of size m. \]~Context-free parsers like the Earley algorithm run in timeO(I G' j2 .n3) where I G'I is the size of the CFG G' and n theinput string length, so a GPSG G of size m will be recognizedin timeO(3=.m!m=~+' .
~3) (14)The hyper-exponential term will dominate the Earley algo-r i thm complexity in the reduction above because m is a functionof the size of the ATM we are simulating.
Even if the GPSG isheld constant,  the stunning derived grammar size in formula 13turns up as an equally stunning 'constant '  multiplicative factorin 14, which in turn will dominate the real-world performance ofthe Earley algorithm for all expected inputs (i.e.
any that canbe written down in the universe), every time we use the derivedgrammar.iSPullum(1985) has suggested that "examination of a suitable'typical' GPSG description reveals a ratio of only 4 to I betweenexpanded and unexpanded grammar statements," strongly im-plying that GPSG is efficiently processable as a consequence.
14But this "expanded grammar" is not adequately expanded, i.e.it is not composed of context-free productions with unanalyz-12As we saw above, the metarule finite closure operation can increasethe ID rule grammar size from I R I = O(I G I) to O(m 2~) in a GPSGG of size m. We ignore the effects of ID/LP format on the number ofadmissible local trees here, and note that if we expanded out all admissiblelinear precedence possibilities in FC(M,R}, the resultant 'ordered' ID rulegrammar would be of size O(rn2'~7).
In the worst case, every symbol inFC(M,R) is underspecified, and every category in K extends every symbolin the FC(M,R} grammar.
Since there areo(s - - , ' )possible syntactic categories, and O(m TM) symbols in FU(M,R), the numberof admissible local trees (= atomic context-free productions} in G iso((3~.~,) ,,,, ') = o(s~, ,,,,~*' )i.e.
astronomical.
Ristad(1986) argues that the minimal set of admissiblelocal trees in GKPS' GPSG for English is considerably smaller, yet stillcontains more than 10 z?
local trees.laThe compiled grammar recognition problem is at least as intractableas the uncompiled one.
Even worse, Barton{1985) shows how the grammarexpansion increases both the space and time costs of recognltlon, whencompared to the cost of using the grammar directly.14Thls ubstantive argument issomewhat s range coming from a co-authorof a book which advocates the purely formal investigation of linguistics:"The universalism \[of natural anguage 1 is, ultimately, intended to be en-tirely embodied in the formal system, not expressed by statements made init.'GKPS(4).
It is difficult to respond precisely to the claims made in Pul-Ium(1985), since the abstract is (necessarily) brief and consists of assertionsunsupported by factual documentation or clarifying assumptions.37able nonterminal symbols.
15 These informal tractability argu-ments are a particular instance of the more general EP argumentand are equally misleading.The preceding discussion of how intractability arises whenconverting a GPSG into a weakly equivalent CFG does not inprinciple preclude the existence of an efficient compilation step.If the compiled grammar is truly fast and assigns the same struc-tural descriptions as the uncompiled GPSG, and it is possible tocompile the GPSG in practice, then the complexity of the uni-versal recognition problem would not accurately reflect the realcost of parsing.
16 But until such a suggestion is forthcoming,we must assume that it does not exist.
1~,1siS,Expanded grammar" appears to refer to the output of metarule finiteclosure (i.e.
ID rules), and this expanded grammar is tra,=table only ifthe grammar isdirectly usable by the Earley algorithm exactly as context-free productions are: all noaterminals in the context-free productions mustbe unanalyzable.
But the categories and ID rules of the metarule finiteclosure grammar do not have this property.
Nonterminals in GPSG aredecomposable into a complex set of feature specifications and cannot bemade atomicj in part because not all extensions of ID rule categories arelegal.
For example, the categories -OO01Vl~\[-tCF1g}~ PA$\] and VP\[+INV,VFOI~ FIN\] are not legal extensions ofVP in English, while VP \[?INV, +AUX.VFORI~ FINI is.
FCRs, FSDs, LP statements, and principles of universalfeature instantiation -- all of which contribute to GPSG's intractability - -must all still apply to the rules of this expanded grammar.Even if we ignore the significant computational complexity introduced bythe machinery mentioned in the previous paragraph (i.e.
theory of syntac-tic features, FCRs, FSDs, ID/LP format, null-transitions, and metarules),GPSG will still not obtain an e.fficient parsability result.
This is because theHead Feature Convention alone ensures that the universal recognition prob-lem for GPSGs will be NP-hard and likely to be intractable.
Ristad(1986)contains a proof.
This result should not be surprising, given that (1) prin-ciples of universal feature instant\]ation in current GPSG theory replace themetarules of earlier versions of GPSG theory, and (2) metarules are knownto cause intractability in GPSG.~6The xistence or nonexistence of efficient compilation functions doesnot affect either our scientific interest in the universal grammar recognitionproblem or the power and relevance of a complexity analysis.
If complexitytheory classifies a problem as intractable, we learn that something moremust be said to obtain tractability, and that any efficient compilation step,if it exists at all, must itself be costly.17Note that the GPSG we constructed in the preceding reduction willactually accept any input x of length less than or equal to Iwl if and onlyif the ATM M accepts it using S(\]wl) space.
We prepare an input string$ for the GPSG by converting it to the string $0xl lx22.., xn nSr~-1 e.g.shades is accepted by the ATM if and only if the string $Oalb2a3d4e5e657is accepted by the GPSG.
Trivial changes in the grammar allows us to per-mute and "spread" the characters of ?
across an infinite class of stringsin an unbounded number of ways, e.g.
$O'~x~i '~2.
.
.~z l l 'yb .
.
.
?~$a?lwhere each ~ is a string over an alphabet which is distinct from the ~ialphabet.
Although the flexibility of this construction results in a morecomplicated GPSG, it argues powerfully against he existence of any effi-cient compilation procedure for GPSGs.
Any efficient compilation proce-dure must perform more than an exponential polynomial amount of work(GPSG-Recognition takes at least Exp-Poly time) on at least an exponen-tial number of inputs (all inputs that fit in the t w t space of the ATM'sread-only tape).
More importantly, the required compilation procedure willconvert say exponential-polynomial t me bounded Turing Machine into apolynomial*time TM for the class inputs whose membership can be deter-mined within a arbitrary (fixed) exp-poly time bound.
Simply listing theaccepted inputs will not work because both the GPSG and TM may ac-cept an infinite class of inputs.
Such a compilation procedure would beextremely powerful.lSNote that compilation illegitimately assumes that the compilation step4.2  Complex i ty  and  Succ inc tnessThe major complexity result of this paper proves that the fastestalgorithm for GPSG-Recognition must take more than exponen-tial time.
The immediately preceding section demonstrates x-actly how a particular algorithm for GPSG-Recognition (the EPargument) comes to grief: weak context-free generative powerdoes not ensure fficient parsability because a GPSG G is weaklyequivalent to a very large CFG G ~, and CFG size affects recog-nition time.
The rebuttal does not suggest hat computationalcomplexity arises from representational succinctness, either hereor in general.Complexity results characterize the amount of resources neededto solve instances of a problem, while succinctness results mea-sure the space reduction gained by one representation ver an-other, equivalent, representation.There is no casual connection between computational com-plexity and representational succinctness, either in practice orprinciple.
In practice, converting one grammar into a more suc-cinct one can either increase or decrease the recognition cost.For example, converting an instance of context-free recognition(known to be polynomial time) into an instance of context-sensitive recognition (known to be PSPACE-complete and likelyto be intractable) can significantly speed the recognition prob-lem if the conversion decreases the size of the CFG logarithmi-cally or better.
Even more strangely, increasing ambiguity ina CFG can speed recognition time if the succinctness gain islarge enough, or slow it down otherwise - -  unambiguous CFGscan be recognized in linear time, while ambiguous ones requirecubic time.In principle, tractable problems may involv~ succinct rep-resentations.
For example, the iterating coordination schema(ICS) of GPSG is an unbeatably succinct encoding of an infi-nite set of context-free rules; from a computational complexityviewpoint, the ICS is utterly trivial using a slightly modifiedEarley algorithm.
19 Tractable problems may also be verboselyrepresented: consider a random finite language, which may berecognized in essentially constant ime on a typical computer(using a hash table), yet whose elements must be individuallylisted.
Similarly, intractable problems may be represented bothsuccinctly and nonsuccinctly.
As is well known, the Turing ma-chine for any arbitrary r.e.
set may be either extremely smallor monstrously big.
Winning the game of chess when played onan n x n board is likely to be computationMly intractable, yetthe chess board is not intended to be an encoding of anotherrepresentation, succinct or otherwise.is free.
There is one theory of primitive language l arning and use: conjec-ture a grammar and use it.
For this procedure to work, grammars shouldbe easy to test on small inputs.
The overall complexity of learning, testing,and speech must be considered.
Compilation speeds up the speech com-ponent at the expense of greater complexity in the other two components.For this linguistic reason the compilation argument is suspect.X~A more extreme xample of the unrelatedness of succinctness and com-plexity is the absolute succinctness with which the dense language ~" maybe represented -- whether by a regular expression, CFG, or even Taringmachine --  yet members of E ?
may be recognized in constant time (i.e.always accept).38Tractable problems may involve succinct or nonsuccinct rep-resentations, as may intractable problems.
The reductions inthis paper show that GPSGs are not merely succinct encod-ings of some context-free grammars; they are inherently com-plex grammars for some context-free languages.
The heart ofthe matter is that GPSG's formal devices are computationallycomplex and can encode provably intractable problems.4.3 Re levance  o f  the  Resu l tIn this paper, we argued that there is nothing in the GPSG for-mal framework that guarantees computational tractability: pro-ponents of GPSG must look elsewhere for an explanation ofefficient parsability, if one is to be given at all.
The crux ofthe matter is that the complex components of GPSG theoryinteract in intractable ways, and that weak context-free gener-ative power does not guarantee tractability when grammar sizeis taken into account.
A faithful implementation f the GPSGformalisms of GKPS will provably be intractable; expectationscomputational linguistics might have held in this regard are notfulfilled by current GPSG theory.This formal property of GPSGs is straightforwardly inter-esting to GPSG linguists.
As outlined by GKPS, "an importantgoal of the GPSG approach to linguistics \[is!
the constructionof theories of the structure of sentences under which significantproperties of grammars and languages fall out as theorems asopposed to being stipulated as axioms (p.4).
"The role of a computational nalysis of the sort providedhere is fundamentally positive: it can offer significant formalinsights into linguistic theory and human language, and sug-gest improvements in linguistic theory and real-world parsers.The insights gained may be used to revise the linguistic theoryso that it is both stronger linguistically and weaker formally.Work on revising GPSG is in progress.
Briefly, some proposedchanges uggested by the preceding reductions are: unit featureclosure, no FCRs or FSDs, no null-transitions in ID rules, meta-rule unit closure, and no problematic feature specifications inthe principles of universal feature instantiation.
Not only dothese restrictions alleviate most of GPSG's computational in-tractability, but they increase the theory's linguistic constraintand reduce the number of nonnatural language grammars li-censed by the theory.
Unfortunately, there is insufficient spaceto discuss these proposed revisions here - -  the reader is referredto Ristad(1986) for a complete discussion.Acknowledgments.
Robert Berwick, Jim Higginbotham, andRichard Larson greatly assisted the author in writing this paper.The author is also indebted to Sandiway Fong and David Waltzfor their help, and to the MIT  Artificial Intelligence Lab andThinking Machines Corporation for supporting this research.Barton, G.E.
(1985).
"On the Complexity of ID/LP Parsing,"Computational Linguistics, 11(4): 205-218.Chandra, A. and L. Stockmeyer (1976).
"Alternation," 17 thAnnual Symposium on Foundations of Computer Science,:98-108.Gazdar, G. (1981).
"Unbounded Dependencies and CoordinateStructure," Linguistic Inquiry 12: 155-184.Gazdar, G., E. Klein, G. Pullum, and I.
Sag (1985).
Gener-alized Phrase Structure Grammar.
Oxford, England: BasilBlackwell.Garey, M, and D. Johnson (1979).
Computers and Intractabil-ity.
San Francisco: W.H.
Freeman and Co.Hopcroft, J.E., and J.D.
Ullman (1979).
Introduction to Au-tomata Theory, Languages, and Computation.
Reading,MA: Addison-Wesley.Pullum, G.K. (1985).
"The Computational Tractability of GPSG,"Abstracts of the 60th Annual Meeting of the Linguistics So-ciety of America, Seattle, WA: 36.Ristad, E.S.
(1985).
"GPSG-Recognition is NP-hard," A.I.Memo No.
837, Cambridge, MA: M.I.T.
Artificial Intelli-gence Laboratory.Ristad, E.S.
(1986).
"Complexity of Linguistic Models: A Com-putational Analysis and Reconstruction ofGeneralized PhraseStructure Grammar," S.M.
Thesis, MIT Department of Elec-trical Engineering and Computer Science.
(In progress).5 Re ferences39
