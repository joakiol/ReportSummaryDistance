Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 389?398,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsEvent Discovery in Social Media FeedsEdward Benson, Aria Haghighi, and Regina BarzilayComputer Science and Artificial Intelligence LaboratoryMassachusetts Institute of Technology{eob, aria42, regina}@csail.mit.eduAbstractWe present a novel method for record extrac-tion from social streams such as Twitter.
Un-like typical extraction setups, these environ-ments are characterized by short, one sentencemessages with heavily colloquial speech.
Tofurther complicate matters, individual mes-sages may not express the full relation to beuncovered, as is often assumed in extractiontasks.
We develop a graphical model that ad-dresses these problems by learning a latent setof records and a record-message alignment si-multaneously; the output of our model is aset of canonical records, the values of whichare consistent with aligned messages.
Wedemonstrate that our approach is able to accu-rately induce event records from Twitter mes-sages, evaluated against events from a localcity guide.
Our method achieves significanterror reduction over baseline methods.11 IntroductionWe propose a method for discovering event recordsfrom social media feeds such as Twitter.
The taskof extracting event properties has been well studiedin the context of formal media (e.g., newswire), butdata sources such as Twitter pose new challenges.Social media messages are often short, make heavyuse of colloquial language, and require situationalcontext for interpretation (see examples in Figure 1).Not all properties of an event may be expressed ina single message, and the mapping between mes-sages and canonical event records is not obvious.1Data and code available at http://groups.csail.mit.edu/rbg/code/twitterCarnegie HallArtist VenueCraig FergusonDJ Pauly D Terminal 5Seated at @carnegiehall waiting for @CraigyFerg?s show to beginRT @leerader : getting REALLY stoked for #CraigyAtCarnegiesat night.
Craig, , want to join us for dinner at the pub across thestreet?
5pm, be there!
@DJPaulyD absolutely killed it at Terminal 5 last night.
@DJPaulyD : DJ Pauly D Terminal 5 NYC Insanity !
#ohyeah@keadour @kellaferr24Craig, nice seeing you at #noelnight this weekend @becksdavis!Twitter MessagesRecordsFigure 1: Examples of Twitter messages, along withautomatically extracted records.These properties of social media streams make exist-ing extraction techniques significantly less effective.Despite these challenges, this data exhibits an im-portant property that makes learning amenable: themultitude of messages referencing the same event.Our goal is to induce a comprehensive set of eventrecords given a seed set of example records, such asa city event calendar table.
While such resourcesare widely available online, they are typically highprecision, but low recall.
Social media is a naturalplace to discover new events missed by curation, butmentioned online by someone planning to attend.We formulate our approach as a structured graphi-cal model which simultaneously analyzes individualmessages, clusters them according to event, and in-duces a canonical value for each event property.
Atthe message level, the model relies on a conditionalrandom field component to extract field values such389as location of the event and artist name.
We bias lo-cal decisions made by the CRF to be consistent withcanonical record values, thereby facilitating consis-tency within an event cluster.
We employ a factor-graph model to capture the interaction between eachof these decisions.
Variational inference techniquesallow us to effectively and efficiently make predic-tions on a large body of messages.A seed set of example records constitutes our onlysource of supervision; we do not observe alignmentbetween these seed records and individual messages,nor any message-level field annotation.
The outputof our model consists of an event-based clustering ofmessages, where each cluster is represented by a sin-gle multi-field record with a canonical value chosenfor each field.We apply our technique to construct entertain-ment event records for the city calendar section ofNYC.com using a stream of Twitter messages.
Ourmethod yields up to a 63% recall against the citytable and up to 85% precision evaluated manually,significantly outperforming several baselines.2 Related WorkA large number of information extraction ap-proaches exploit redundancy in text collections toimprove their accuracy and reduce the need for man-ually annotated data (Agichtein and Gravano, 2000;Yangarber et al, 2000; Zhu et al, 2009; Mintzet al, 2009a; Yao et al, 2010b; Hasegawa et al,2004; Shinyama and Sekine, 2006).
Our work mostclosely relates to methods for multi-document infor-mation extraction which utilize redundancy in in-put data to increase the accuracy of the extractionprocess.
For instance, Mann and Yarowsky (2005)explore methods for fusing extracted informationacross multiple documents by performing extractionon each document independently and then merg-ing extracted relations by majority vote.
This ideaof consensus-based extraction is also central to ourmethod.
However, we incorporate this idea into ourmodel by simultaneously clustering output and la-beling documents rather than performing the twotasks in serial fashion.
Another important differenceis inherent in the input data we are processing: it isnot clear a priori which extraction decisions shouldagree with each other.
Identifying messages that re-fer to the same event is a large part of our challenge.Our work also relates to recent approaches for re-lation extraction with distant supervision (Mintz etal., 2009b; Bunescu and Mooney, 2007; Yao et al,2010a).
These approaches assume a database and acollection of documents that verbalize some of thedatabase relations.
In contrast to traditional super-vised IE approaches, these methods do not assumethat relation instantiations are annotated in the inputdocuments.
For instance, the method of Mintz et al(2009b) induces the mapping automatically by boot-strapping from sentences that directly match recordentries.
These mappings are used to learn a classi-fier for relation extraction.
Yao et al (2010a) furtherrefine this approach by constraining predicted rela-tions to be consistent with entity types assignment.To capture the complex dependencies among assign-ments, Yao et al (2010a) use a factor graph repre-sentation.
Despite the apparent similarity in modelstructure, the two approaches deal with various typesof uncertainties.
The key challenge for our methodis modeling message to record alignment which isnot an issue in the previous set up.Finally, our work fits into a broader area oftext processing methods designed for social-mediastreams.
Examples of such approaches includemethods for conversation structure analysis (Ritteret al, 2010) and exploration of geographic languagevariation (Eisenstein et al, 2010) from Twitter mes-sages.
To our knowledge no work has yet addressedrecord extraction from this growing corpus.3 Problem FormulationHere we describe the key latent and observed ran-dom variables of our problem.
A depiction of allrandom variables is given in Figure 2.Message (x): Each message x is a single posting toTwitter.
We use xj to represent the jth token of x,and we use x to denote the entire collection of mes-sages.
Messages are always observed during train-ing and testing.Record (R): A record is a representation of thecanonical properties of an event.
We use Ri to de-note the ith record and R`i to denote the value of the`th property of that record.
In our experiments, eachrecord Ri is a tuple ?R1i , R2i ?
which represents that390Mercury LoungeYonder MountainString BandCraig Ferguson Carnegie HallArtist Venue12k R?k R?+1k...Really     excited      for    #CraigyAtCarnegieSeeing     Yonder    Mountain        at             8@YonderMountain  rocking  Mercury  LoungeNone None None ArtistNone NoneArtist Artist NoneVenue VenueNoneArtistxiyixi?1yi?1xi+1yi+1Ai?1Ai+1AiFigure 2: The key variables of our model.
A collection ofK latent recordsRk, each consisting of a set ofL properties.In the figure above, R11 =?Craig Ferguson?
and R21 =?Carnegie Hall.?
Each tweet xi is associated with a labelingover tokens yi and is aligned to a record via the Ai variable.
See Section 3 for further details.record?s values for the schema ?ARTIST, VENUE?.Throughout, we assume a known fixed number Kof records R1, .
.
.
, RK , and we use R to denote thiscollection of records.
For tractability, we considera finite number of possibilities for each R`k whichare computed from the input x (see Section 5.1 fordetails).
Records are observed during training andlatent during testing.Message Labels (y): We assume that each messagehas a sequence labeling, where the labels consist ofthe record fields (e.g., ARTIST and VENUE) as wellas a NONE label denoting the token does not corre-spond to any domain field.
Each token xj in a mes-sage has an associated label yj .
Message labels arealways latent during training and testing.Message to Record Alignment (A): We assumethat each message is aligned to some record suchthat the event described in the message is the onerepresented by that record.
Each message xi is as-sociated with an alignment variable Ai that takes avalue in {1, .
.
.
,K}.
We use A to denote the set ofalignments across all xi.
Multiple messages can anddo align to the same record.
As discussed in Sec-tion 4, our model will encourage tokens associatedwith message labels to be ?similar?
to correspondingaligned record values.
Alignments are always latentduring training and testing.4 ModelOur model can be represented as a factor graphwhich takes the form,P (R,A, y|x) ?
(?i?SEQ(xi, yi))(Seq.
Labeling)(?`?UNQ(R`))(Rec.
Uniqueness)??
?i,`?POP (xi, yi, R`Ai)??
(Term Popularity)(?i?CON (xi, yi, RAi))(Rec.
Consistency)where R` denotes the sequence R`1, .
.
.
, R`K ofrecord values for a particular domain field `.
Eachof the potentials takes a standard log-linear form:?
(z) = ?T f(z)where ?
are potential-specific parameters and f(?
)is a potential-specific feature function.
We describeeach potential separately below.4.1 Sequence Labeling FactorThe sequence labeling factor is similar to a standardsequence CRF (Lafferty et al, 2001), where the po-tential over a message label sequence decomposes391XiYi?SEQR?kR?k+1R?k?1?UNQ ?th field(across records)?
?POPR?kAiYiXiAiYiXiR?k R?+1k?CONkkth recordFigure 3: Factor graph representation of our model.
Circles represent variables and squares represent factors.
Forreadability, we depict the graph broken out as a set of templates; the full graph is the combination of these factortemplates applied to each variable.
See Section 4 for further details.over pairwise cliques:?SEQ(x, y) = exp{?TSEQfSEQ(x, y)}=exp???
?TSEQ?jfSEQ(x, yj , yj+1)??
?This factor is meant to encode the typical messagecontexts in which fields are evoked (e.g.
going to seeX tonight).
Many of the features characterize howlikely a given token label, such as ARTIST, is for agiven position in the message sequence conditioningarbitrarily on message text context.The feature function fSEQ(x, y) for this compo-nent encodes each token?s identity; word shape2;whether that token matches a set of regular expres-sions encoding common emoticons, time references,and venue types; and whether the token matches abag of words observed in artist names (scraped fromWikipedia; 21,475 distinct tokens from 22,833 dis-tinct names) or a bag of words observed in NewYork City venue names (scraped from NYC.com;304 distinct tokens from 169 distinct names).3 Theonly edge feature is label-to-label.4.2 Record Uniqueness FactorOne challenge with Twitter is the so-called echochamber effect: when a topic becomes popular, or?trends,?
it quickly dominates the conversation on-line.
As a result some events may have only a fewreferent messages while other more popular eventsmay have thousands or more.
In such a circum-stance, the messages for a popular event may collectto form multiple identical record clusters.
Since we2e.g.
: xxx, XXX, Xxx, or other3These are just features, not a filter; we are free to extractany artist or venue regardless of their inclusion in this list.fix the number of records learned, such behavior in-hibits the discovery of less talked-about events.
In-stead, we would rather have just two records: onewith two aligned messages and another with thou-sands.
To encourage this outcome, we introduce apotential that rewards fields for being unique acrossrecords.The uniqueness potential ?UNQ(R`) encodes thepreference that each of the values R`, .
.
.
, R`K foreach field ` do not overlap textually.
This factor fac-torizes over pairs of records:?UNQ(R`) =?k 6=k?
?UNQ(R`k, R`k?
)where R`k and R`k?
are the values of field ` for tworecords Rk and Rk?
.
The potential over this pair ofvalues is given by:?UNQ(R`k, R`k?)
= exp{?
?TSIMfSIM (R`k, R`k?
)}where fSIM is computes the likeness of the two val-ues at the token level:fSIM (R`k, R`k?)
= |R`k ?R`k?
|max(|R`k|, |R`k?
|)This uniqueness potential does not encode anypreference for record values; it simply encourageseach field ` to be distinct across records.4.3 Term Popularity FactorThe term popularity factor ?POP is the first of twofactors that guide the clustering of messages.
Be-cause speech on Twitter is colloquial, we would likethese clusters to be amenable to many variations ofthe canonical record properties that are ultimatelylearned.
The ?POP factor accomplishes this by rep-resenting a lenient compatibility score between a392message x, its labels y, and some candidate valuev for a record field (e.g., Dave Matthews Band).This factor decomposes over tokens, and we aligneach token xj with the best matching token vk in v(e.g., Dave).
The token level sum is scaled by thelength of the record value being matched to avoid apreference for long field values.
?POP (x, y,R`A = v) =?jmaxk?POP (xj , yj , R`A = vk)|v|This token-level component may be thought of asa compatibility score between the labeled token xjand the record field assignment R`A = v. Given thattoken xj aligns with the token vk, the token-levelcomponent returns the sum of three parts, subject tothe constraint that yj = `:?
IDF (xj)I[xj = vk], an equality indicator be-tween tokens xj and vk, scaled by the inversedocument frequency of xj?
?IDF (xj) (I[xj?1 = vk?1] + I[xj+1 = vk+1]),a small bonus of ?
= 0.3 for matches on adja-cent tokens, scaled by the IDF of xj?
I[xj = vk and x contains v]/|v|, a bonus for acomplete string match, scaled by the size of thevalue.
This is equivalent to this token?s contri-bution to a complete-match bonus.4.4 Record Consistency FactorWhile the uniqueness factor discourages a flood ofmessages for a single event from clustering into mul-tiple event records, we also wish to discourage mes-sages from multiple events from clustering into thesame record.
When such a situation occurs, themodel may either resolve it by changing inconsis-tent token labelings to the NONE label or by reas-signing some of the messages to a new cluster.
Weencourage the latter solution with a record consis-tency factor ?CON .The record consistency factor is an indicator func-tion on the field values of a record being present andlabeled correctly in a message.
While the popular-ity factor encourages agreement on a per-label basis,this factor influences the joint behavior of messagelabels to agree with the aligned record.
For a givenrecord, message, and labeling, ?CON (x, y,RA) = 1if ?POP (x, y,R`A) > 0 for all `, and 0 otherwise.4.5 Parameter LearningThe weights of the CRF component of our model,?SEQ, are the only weights learned at training time,using a distant supervision process described in Sec-tion 6.
The weights of the remaining three factorswere hand-tuned4 using our training data set.5 InferenceOur goal is to predict a set of records R. Ideally wewould like to compute P (R|x), marginalizing outthe nuisance variables A and y.
We approximatethis posterior using variational inference.5 Con-cretely, we approximate the full posterior over latentvariables using a mean-field factorization:P (R,A,y|x) ?
Q(R,A,y)=(K?k=1?`q(R`k))(n?i=1q(Ai)q(yi))where each variational factor q(?)
represents an ap-proximation of that variable?s posterior given ob-served random variables.
The variational distribu-tion Q(?)
makes the (incorrect) assumption that theposteriors amongst factors are independent.
Thegoal of variational inference is to set factors q(?)
tooptimize the variational objective:minQ(?
)KL(Q(R,A,y)?P (R,A,y|x))We optimize this objective using coordinate descenton the q(?)
factors.
For instance, for the case of q(yi)the update takes the form:q(yi)?
EQ/q(yi) logP (R,A,y|x)where Q/q(yi) denotes the expectation under allvariables except yi.
When computing a mean fieldupdate, we only need to consider the potentials in-volving that variable.
The complete updates for eachof the kinds of variables (y, A, andR`) can be foundin Figure 4.
We briefly describe the computationsinvolved with each update.q(y) update: The q(y) update for a single mes-sage yields an implicit expression in terms of pair-wise cliques in y.
We can compute arbitrary4Their values are: ?UNQ = ?10, ?PhrasePOP = 5, ?TokenPOP = 10,?CON = 2e85See Liang and Klein (2007) for an overview of variationaltechniques.393Message labeling update:ln q(y) ?
{EQ/q(y) ln?SEQ(x, y) + ln[?POP (x, y,R`A)?CON (x, y,RA)]}= ln?SEQ(x, y) + EQ/q(y) ln[?POP (x, y,R`A)?CON (x, y,RA)]= ln?SEQ(x, y) +?z,v,`q(A = z)q(yj = `)q(R`z = v) ln[?POP (x, y,R`z = v)?CON (x, y,R`z = v)]Mention record alignment update:ln q(A = z) ?
EQ/q(A){ln?SEQ(x, y) + ln[?POP (x, y,R`A)?CON (x, y,RA)]}?
EQ/q(A){ln[?POP (x, y,R`A)?CON (x, y,RA)]}= ?z,v,`q(R`z = v){ln[?POP (x, y,R`z = v)?CON (x, y,R`z = v)]}= ?z,v,`q(R`z = v)q(yji = `) ln[?POP (x, y,R`z = v)?CON (x, y,R`z = v)]Record Field update:ln q(R`k = v) ?
EQ/q(R`k){?k?ln?UNQ(R`k?
, v) +?iln [?POP (xi, yi, v)?CON (xi, yi, v)]}= ?k?
6=k,v?(q(R`k?
= v?)
ln?UNQ(v, v?
)+?iq(Ai = k)?jq(yji = `) ln[?POP (x, y,R`z = v, j)?CON (x, y,R`z = v, j)])Figure 4: The variational mean-field updates used during inference (see Section 5).
Inference consists of performingupdates for each of the three kinds of latent variables: message labels (y), record alignments (A), and record fieldvalues (R`).
All are relatively cheap to compute except for the record field update q(R`k) which requires loopingpotentially over all messages.
Note that at inference time all parameters are fixed and so we only need to performupdates for latent variable factors.marginals for this distribution by using the forwards-backwards algorithm on the potentials defined inthe update.
Therefore computing the q(y) updateamounts to re-running forward backwards on themessage where there is an expected potential termwhich involves the belief over other variables.
Notethat the popularity and consensus potentials (?POPand ?CON ) decompose over individual message to-kens so this can be tractably computed.q(A) update: The update for individual recordalignment reduces to being log-proportional to theexpected popularity and consensus potentials.q(R`k) update: The update for the record fielddistribution is the most complex factor of the three.It requires computing expected similarity with otherrecord field values (the ?UNQ potential) and loopingover all messages to accumulate a contribution fromeach, weighted by the probability that it is aligned tothe target record.5.1 Initializing FactorsSince a uniform initialization of all factors is asaddle-point of the objective, we opt to initializethe q(y) factors with the marginals obtained usingjust the CRF parameters, accomplished by runningforwards-backwards on all messages using only the394?SEQ potentials.
The q(R) factors are initializedrandomly and then biased with the output of ourbaseline model.
The q(A) factor is initialized to uni-form plus a small amount of noise.To simplify inference, we pre-compute a finite setof values that each R`k is allowed to take, condi-tioned on the corpus.
To do so, we run the CRFcomponent of our model (?SEQ) over the corpus andextract, for each `, all spans that have a token-levelprobability of being labeled ` greater than ?
= 0.1.We further filter this set down to only values that oc-cur at least twice in the corpus.This simplification introduces sparsity that wetake advantage of during inference to speed perfor-mance.
Because each term in ?POP and ?CON in-cludes an indicator function based on a token matchbetween a field-value and a message, knowing thepossible values v of each R`k enables us to precom-pute the combinations of (x, `, v) for which nonzerofactor values are possible.
For each such tuple, wecan also precompute the best alignment position kfor each token xj .6 Evaluation SetupData We apply our approach to construct a databaseof concerts in New York City.
We used Twitter?spublic API to collect roughly 4.7 Million tweetsacross three weekends that we subsequently filterdown to 5,800 messages.
The messages have an av-erage length of 18 tokens, and the corpus vocabu-lary comprises 468,000 unique words6.
We obtainlabeled gold records using data scraped from theNYC.com music event guide; totaling 110 extractedrecords.
Each gold record had two fields of interest:ARTIST and VENUE.The first weekend of data (messages and events)was used for training and the second two weekendswere used for testing.Preprocessing Only a small fraction of Twitter mes-sages are relevant to the target extraction task.
Di-rectly processing the raw unfiltered stream wouldprohibitively increase computational costs and makelearning more difficult due to the noise inherent inthe data.
To focus our efforts on the promising por-tion of the stream, we perform two types of filter-6Only considering English tweets and not counting usernames (so-called -mentions.)ing.
First, we only retain tweets whose authors listsome variant of New York as their location in theirprofile.
Second, we employ a MIRA-based binaryclassifier (Ritter et al, 2010) to predict whether amessage mentions a concert event.
After training on2,000 hand-annotated tweets, this classifier achievesan F1 of 46.9 (precision of 35.0 and recall of 71.0)when tested on 300 messages.
While the two-stagefiltering does not fully eliminate noise in the inputstream, it greatly reduces the presence of irrelevantmessages to a manageable 5,800 messages withoutfiltering too many ?signal?
tweets.We also filter our gold record set to include onlyrecords in which each field value occurs at least oncesomewhere in the corpus, as these are the recordswhich are possible to learn given the input.
Thisyields 11 training and 31 testing records.Training The first weekend of data (2,184 messagesand 11 records after preprocessing) is used for train-ing.
As mentioned in Section 4, the only learned pa-rameters in our model are those associated with thesequence labeling factor ?SEQ.
While it is possi-ble to train these parameters via direct annotation ofmessages with label sequences, we opted instead touse a simple approach where message tokens fromthe training weekend are labeled via their intersec-tion with gold records, often called ?distant super-vision?
(Mintz et al, 2009b).
Concretely, we auto-matically label message tokens in the training cor-pus with either the ARTIST or VENUE label if theybelonged to a sequence that matched a gold recordfield, and with NONE otherwise.
This is the only usethat is made of the gold records throughout training.
?SEQ parameters are trained using this labeling witha standard conditional likelihood objective.Testing The two weekends of data used for test-ing totaled 3,662 tweets after preprocessing and 31gold records for evaluation.
The two weekends weretested separately and their results were aggregatedacross weekends.Our model assumes a fixed number of recordsK = 130.7 We rank these records according toa heuristic ranking function that favors the unique-ness of a record?s field values across the set and thenumber of messages in the testing corpus that have7Chosen based on the training set3950.2?0.25?0.3?0.35?0.4?0.45?0.5?0.55?0.6?0.65?0.7?1.00?
1.5?
2?
2.5?
3?
3.5?
4?
4.5?
5?Recall?against?Gold?Event?Records?k,?as?a?mul?ple?of?the?number?of?gold?records?Low?Thresh?
CRF?
List?
Our?Work?Figure 5: Recall against the gold records.
The horizontalaxis is the number of records kept from the ranked modeloutput, as a multiple of the number of golds.
The CRFlines terminate because of low record yield.token overlap with these values.
This ranking func-tion is intended to push garbage collection recordsto the bottom of the list.
Finally, we retain the top krecords, throwing away the rest.
Results in Section7 are reported as a function of this k.Baseline We compare our system against three base-lines that employ a voting methodology similar toMann and Yarowsky (2005).
The baselines labeleach message and then extract one record for eachcombination of labeled phrases.
Each extraction isconsidered a vote for that record?s existence, andthese votes are aggregated across all messages.Our List Baseline labels messages by findingstring overlaps against a list of musical artists andvenues scraped from web data (the same lists used asfeatures in our CRF component).
The CRF Baselineis most similar to Mann and Yarowsky (2005)?s CRFVoting method and uses the maximum likelihoodCRF labeling of each message.
The Low Thresh-old Baseline generates all possible records from la-belings with a token-level likelihood greater than?
= 0.1.
The output of these baselines is a set ofrecords ranked by the number of votes cast for each,and we perform our evaluation against the top k ofthese records.7 EvaluationThe evaluation of record construction is challeng-ing because many induced music events discussedin Twitter messages are not in our gold data set; ourgold records are precise but incomplete.
Becauseof this, we evaluate recall and precision separately.Both evaluations are performed using hard zero-oneloss at record level.
This is a harsh evaluation crite-rion, but it is realistic for real-world use.Recall We evaluate recall, shown in Figure 5,against the gold event records for each weekend.This shows how well our model could do at replac-ing the a city event guide, providing Twitter userschat about events taking place.We perform our evaluation by taking the topk records induced, performing a stable marriagematching against the gold records, and then evalu-ating the resulting matched pairs.
Stable marriagematching is a widely used approach that finds a bi-partite matching between two groups such that nopairing exists in which both participants would pre-fer some other pairing (Irving et al, 1987).
Withour hard loss function and no duplicate gold records,this amounts to the standard recall calculation.
Wechoose this bipartite matching technique because itgeneralizes nicely to allow for other forms of losscalculation (such as token-level loss).Precision To evaluate precision we assembled a listof the distinct records produced by all models andthen manually determined if each record was cor-rect.
This determination was made blind to whichmodel produced the record.
We then used this ag-gregate list of correct records to measure precisionfor each individual model, shown in Figure 6.By construction, our baselines incorporate a hardconstraint that each relation learned must be ex-pressed in entirety in at least one message.
Ourmodel only incorporates a soft version of this con-straint via the ?CON factor, but this constraintclearly has the ability to boost precision.
To showit?s effect, we additionally evaluate our model, la-beled Our Work + Con, with this constraint appliedin hard form as an output filter.The downward trend in precision that can be seenin Figure 6 is the effect of our ranking algorithm,which attempts to push garbage collection recordstowards the bottom of the record list.
As we incor-porate these records, precision drops.
These linestrend up for two of the baselines because the rank-3960.2?0.3?0.4?0.5?0.6?0.7?0.8?0.9?10?
20?
30?
40?
50?Precision?(Manual?Evelua?on)?Number?of?Records?Kept?Low?Thresh?
CRF?
List?
Our?Work?
Our?Work?+?Con?Figure 6: Precision, evaluated manually by cross-referencing model output with event mentions in the in-put data.
The CRF and hard-constrained consensus linesterminate because of low record yield.ing heuristic is not as effective for them.These graphs confirm our hypothesis that we gainsignificant benefit by intertwining constraints on ex-traction consistency in the learning process, ratherthan only using this constraint to filter output.7.1 AnalysisOne persistent problem is a popular phrase appear-ing in many records, such as the value ?New York?filling many ARTIST slots.
The uniqueness factor?UNQ helps control this behavior, but it is a rela-tively blunt instrument.
Ideally, our model wouldlearn, for each field `, the degree to which dupli-cate values are permitted.
It is also possible that bylearning, rather than hand-tuning, the ?CON , ?POP ,and ?UNQ parameters, our model could find a bal-ance that permits the proper level of duplication fora particular domain.Other errors can be explained by the lack of con-stituent features in our model, such as the selectionof VENUE values that do not correspond to nounphrases.
Further, semantic features could help avoidlearning syntactically plausible artists like ?Screwthe Rain?
because of the message:Screw the rainArtist!
Grab an umbrella and head down toWebster HallVenue for some American rock and roll.Our model?s soft string comparison-based clus-tering can be seen at work when our model uncov-ers records that would have been impossible withoutthis approach.
One such example is correcting themisspelling of venue names (e.g.
Terminal Five ?Terminal 5) even when no message about the eventspells the venue correctly.Still, the clustering can introduce errors by com-bining messages that provide orthogonal field con-tributions yet have overlapping tokens (thus escap-ing the penalty of the consistency factor).
An exam-ple of two messages participating in this scenario isshown below; the shared term ?holiday?
in the sec-ond message gets relabeled as ARTIST:Come check out the holiday cheerArtist parkside is bursting..Pls tune in to TV Guide NetworkVenue TONIGHT at 8 pmfor 25 Most Hilarious Holiday TV Moments...While our experiments utilized binary relations,we believe our general approach should be useful forn-ary relation recovery in the social media domain.Because short messages are unlikely to express higharity relations completely, tying extraction and clus-tering seems an intuitive solution.
In such a sce-nario, the record consistency constraints imposed byour model would have to be relaxed, perhaps exam-ining pairwise argument consistency instead.8 ConclusionWe presented a novel model for record extractionfrom social media streams such as Twitter.
Ourmodel operates on a noisy feed of data and extractscanonical records of events by aggregating informa-tion across multiple messages.
Despite the noiseof irrelevant messages and the relatively colloquialnature of message language, we are able to extractrecords with relatively high accuracy.
There is stillmuch room for improvement using a broader arrayof features on factors.9 AcknowledgementsThe authors gratefully acknowledge the support ofthe DARPA Machine Reading Program under AFRLprime contract no.
FA8750-09-C-0172.
Any opin-ions, findings, and conclusions expressed in this ma-terial are those of the author(s) and do not necessar-ily reflect the views of DARPA, AFRL, or the USgovernment.
Thanks also to Tal Wagner for his de-velopment assistance and the MIT NLP group fortheir helpful comments.397ReferencesEugene Agichtein and Luis Gravano.
2000.
Snowball:Extracting relations from large plain-text collections.In Proceedings of DL.Razvan C. Bunescu and Raymond J. Mooney.
2007.Learning to extract relations from the web using mini-mal supervision.
In Proceedings of the ACL.J Eisenstein, B O?Connor, and N Smith.
.
.
.
2010.
Alatent variable model for geographic lexical variation.Proceedings of the 2010 .
.
.
, Jan.Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.2004.
Discovering relations among named entitiesfrom large corpora.
In Proceedings of ACL.Robert W. Irving, Paul Leather, and Dan Gusfield.
1987.An efficient algorithm for the optimal stable marriage.J.
ACM, 34:532?543, July.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic mod-els for segmenting and labeling sequence data.
InProceedings of International Conference of MachineLearning (ICML), pages 282?289.P.
Liang and D. Klein.
2007.
Structured Bayesian non-parametric models with variational inference (tutorial).In Association for Computational Linguistics (ACL).Gideon S. Mann and David Yarowsky.
2005.
Multi-fieldinformation extraction and cross-document fusion.
InProceeding of the ACL.Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-sky.
2009a.
Distant supervision for relation extractionwithout labeled data.
In Proceedings of ACL/IJCNLP.Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-sky.
2009b.
Distant supervision for relation extrac-tion without labeled data.
In Proceedings of the ACL,pages 1003?1011.A Ritter, C Cherry, and B Dolan.
2010.
Unsupervisedmodeling of twitter conversations.
Human LanguageTechnologies: The 2010 Annual Conference of theNorth American Chapter of the Association for Com-putational Linguistics, pages 172?180.Yusuke Shinyama and Satoshi Sekine.
2006.
Preemp-tive information extraction using unrestricted relationdiscovery.
In Proceedings of HLT/NAACL.Roman Yangarber, Ralph Grishman, Pasi Tapanainen,and Silja Huttunen.
2000.
Automatic acquisition ofdomain knowledge for information extraction.
In Pro-ceedings of COLING.Limin Yao, Sebastian Riedel, and Andrew McCallum.2010a.
Collective cross-document relation extractionwithout labelled data.
In Proceedings of the EMNLP,pages 1013?1023.Limin Yao, Sebastian Riedel, and Andrew McCallum.2010b.
Cross-document relation extraction without la-belled data.
In Proceedings of EMNLP.Jun Zhu, Zaiqing Nie, Xiaojing Liu, Bo Zhang, and Ji-Rong Wen.
2009.
StatSnowball: a statistical approachto extracting entity relationships.
In Proceedings ofWWW.398
