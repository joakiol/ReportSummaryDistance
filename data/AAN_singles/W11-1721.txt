Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 161?167,24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational LinguisticsA Cross-corpus Study of Unsupervised Subjectivity Identificationbased on Calibrated EMDong Wang Yang LiuThe University of Texas at Dallas{dongwang,yangl}@hlt.utdallas.eduAbstractIn this study we investigate using an unsu-pervised generative learning method for sub-jectivity detection in text across different do-mains.
We create an initial training set usingsimple lexicon information, and then evaluatea calibrated EM (expectation-maximization)method to learn from unannotated data.
Weevaluate this unsupervised learning approachon three different domains: movie data, newsresource, and meeting dialogues.
We also per-form a thorough analysis to examine impact-ing factors on unsupervised learning, such asthe size and self-labeling accuracy of the ini-tial training set.
Our experiments and analysisshow inherent differences across domains andperformance gain from calibration in EM.1 IntroductionSubjectivity identification is to identify whether anexpression contains opinion or sentiment.
Auto-matic subjectivity identification can benefit manynatural language processing (NLP) tasks.
For ex-ample, information retrieval systems can provide af-fective or informative articles separately (Pang andLee, 2008).
Summarization systems may want tosummarize factual and opinionated content differ-ently (Murray and Carenini, 2008).
In this paper,we perform subjectivity detection at sentence level,which is more appropriate for some subsequent pro-cessing such as opinion summarization.Previous work has shown that when enough la-beled data is available, supervised classificationmethods can achieve high accuracy for subjectivitydetection in some domains.
However, it is often ex-pensive to create such training data.
On the otherhand, a lot of unannotated data is readily availablein various domains.
Therefore an interesting andimportant problem is to develop semi-supervised orunsupervised learning methods that can learn froman unannotated corpus.
In this study, we use an un-supervised learning approach where we first use aknowledge-based method to create an initial train-ing set, and then apply a calibrated EM methodto learn from an unannotated corpus.
Our experi-ments show significant differences among the threedomains: movie, news article, and meeting dialog.This can be explained by the inherent difference ofthe data, especially the task difficulty and classifier?sperformance for a domain.
We demonstrate that forsome domains (e.g., movie data) the unsupervisedlearning methods can rival the supervised approach.2 Related WorkIn the early age, knowledge-based methods werewidely used for subjectivity detection.
They useda lexicon or patterns and rules to predict whether atarget is subjective or not.
These methods tendedto yield a high precision and low recall, or lowprecision and high recall (Kim and Hovy, 2005).Recently, machine learning approaches have beenadopted more often (Ng et al, 2006).
There arelimitations in both methods.
In knowledge-basedapproaches, a predefined subjectivity lexicon maynot adapt well to different domains.
While in ma-chine learning approach, human labeling efforts arerequired to create a large training set.To overcome the above drawbacks, unsupervisedor semi-supervised methods have been explored insentiment analysis.
For polarity classification, someprevious work used spectral techniques (Dasguptaand Ng, 2009) or co-training (Li et al, 2010) tomine the reviews in a semi-supervised manner.
Forsubjectivity identification, Wiebe and Riloff (Wiebeand Riloff, 2005) applied a rule-based method tocreate a training set first and then used it to traina naive Bayes classifier.
Melville et al (Melvilleet al, 2009) used a pooling multinomial method tocombine lexicon derived probability and statisticalprobability.Our work is similar to the study in (Wiebe andRiloff, 2005) in that we both use a rule-basedmethod to create an initial training set and learn from161unannotated corpus.
However, there are two key dif-ferences.
First, unlike the self-training method theyused, we use a calibrated EM iterative learning ap-proach.
Second, we compare the results on three dif-ferent corpora in order to evaluate the domain/genreeffect of the unsupervised method.
Our cross-corpus study shows how the unsupervised learningapproach performs in different domains and helps usunderstand what are the factors impacting the learn-ing methods.3 DataWe use three data sets from different domains:movie, news resource, and meeting conversations.The first two are from written text domain and havebeen widely used in many previous studies for sen-timent analysis (Pang and Lee, 2004; Raaijmakersand Kraaij, 2008).
The third one is from speechtranscripts.
It has been used in a few recent stud-ies (Raaijmakers et al, 2008; Murray and Carenini,2009), but not as much as those text data.
The fol-lowing provides more details of the data.?
The first corpus is movie data (Pang and Lee,2004).
It contains 5,000 subjective sentencescollected from movie reviews and 5,000 objec-tive sentences collected from movie plot sum-maries.
The sentences in each collection arerandomly ordered.?
The second one is extracted from MPQA cor-pus (version 2.0) (Wilson and Wiebe, 2003),which is collected from news articles.
This datahas been annotated with subjective informationat phrase level.
We adopted the same rules as in(Riloff and Wiebe, 2003) to create the sentencelevel label: if a sentence has at least one pri-vate state of strength medium or higher, thenthe sentence is labeled SUBJECTIVE, other-wise it is labeled OBJECTIVE.
We randomlyextracted 5,000 subjective and 5,000 objectivesentences from this corpus to make it compara-ble with the movie data.?
The third data set is from AMI meeting cor-pus.
It has been annotated using the schemedescribed in (Wilson, 2008).
There are 3 maincategories of annotations regarding sentiments:subjective utterances, subjective questions, andobjective polar utterances.
We consider theunion of subjective utterance and subjectivequestion as subjective and the rest as objective.The subjectivity classification task is done atthe dialog act (DA) levels.
We label each DAusing the label of the utterance that has over-lap with it.
We create a balanced data set us-ing this corpus, containing 9,892 DAs in to-tal.
This number is slightly less than those formovie and MPQA data because of the availabledata size in this corpus.
The data is also ran-domly ordered without considering the role ofthe speaker and which meeting it belongs to.Table 1 summarizes statistics for the three datasets.
We can see that sentences in meeting dialogs(AMI data) are generally shorter than the other do-mains, and that sentences in news domain (MPQA)are longer, and also have a larger variance.
In ad-dition, the inter-annotator agreement on AMI datais quite low, which shows it is even difficult for hu-man to determine whether an utterance contains sen-timent in meeting conversations.Movie MPQA AMImin 3 1 3sent length max 100 246 67mean 20.37 22.38 8.78variance 75.26 147.18 34.26vocabulary size 15,847 13,414 3,337Inter-annotator agreement N/A 0.77 0.56Table 1: Statistics for the three data sets: movie, MPQA, andAMI data.
The inter-annotator agreement on movie data is notavailable because it is not annotated by human.4 Unsupervised Subjectivity DetectionIn this section, we describe our unsupervised learn-ing process that uses a knowledge-based method tocreate an initial training set, and then uses a cali-brated EM approach to incorporate unannotated datainto the learning process.
We use a naive Bayes clas-sifier as the base supervised classifier with a bag-of-words model.4.1 Create Initial Training SetA lexicon-based method is used to create an initialtraining set, since it can often achieve high precisionrate (though low recall) for subjectivity detection.We use a subjectivity lexicon (Wilson et al, 2005)to calculate the subjectivity score for each sentence.162This lexicon contains 8,221 entries that are catego-rized into strong and weak subjective clues.For each word w, we assign a subjectivity scoresub(w): 1 to strong subjective clues, 0.5 to weakclues, and 0 for any other word.
Then the subjec-tivity score of a sentence is the sum of the values ofall the words in the sentence, normalized by the sen-tence length.
We noticed that for sentences labeledas SUBJECTIVE in the three corpora, the subjectiveclues appear more frequently in movie data than theother two corpora.
Thus we perform different nor-malization for the three data sets to obtain the sub-jectivity score for each sentence, sub(s): Equation1 for the movie data, and Equation 2 for MPQA andAMI data.sub(s) =?w?ssub(w)/sent length (1)sub(s) =?w?ssub(w)/log(sent length) (2)We label the topm sentences with the highest sub-jective scores as SUBJECTIVE, and label m sen-tences with the lowest scores as OBJECTIVE.
These2m sentences form the initial training set for the it-erative learning methods.4.2 Calibrated EM Naive BayesExpectation-Maximization (EM) naive Bayesmethod is a semi-supervised algorithm proposed in(Nigam et al, 2000) for learning from both labeledand unlabeled data.
In the implementation of EM,we iterate the E-step and M-step until model param-eters converge or a predefined iteration number isreached.
In E-step, we use naive Bayes classifier toestimate the posterior probabilities of each sentencesi belonging to each class cj (SUBJECTIVE andOBJECTIVE), P (cj |si):P (cj |si) =P (cj)?|si|k=1 P (wk|cj)?cl?CP (cl)?|si|k=1 P (wk|cl)(3)The M-step uses the probabilistic results fromthe E-step to recalculate the parameters in the naiveBayes classifier, the probability of word wt in classcj and the prior probability of class cj :P (wt|cj) =0.1 +?si?SN(wt, si)P (cj |si)0.1?
|V |+?|V |k=1?si?SN(wk, si)P (cj |si)(4)P (cj) =0.1 +?si?SP (cj |si)0.1?
|C|+ |S|(5)S is the set of sentences.
N(wt, si) is the count ofword wt in a sentence si.
We use additive smooth-ing with ?
= 0.1 for probability parameter estima-tion.
|C| is the number of classes, which is 2 in ourcase, and |V| is the vocabulary size, obtained fromthe entire data set.In the first iteration, we assign P (cj |si) using thepseudo training data generated based on lexicon in-formation.
If a sentence is labeled SUBJECTIVE,then P (sub|si) is 1 and P (obj|si) is 0; for the sen-tences with OBJECTIVE labels, P (sub|si) is 0 andP (obj|si) is 1.In our work, we use a variant of standard EM:calibrated EM, introduced by (Tsuruoka and Tsujii,2003).
The basic idea of this approach is to shiftthe probability values of unlabeled data to the ex-tent such that the class distribution of unlabeled datais identical to the distribution in labeled data (bal-anced class in our case).
In our approach, beforemodel training (?M-step?)
in each iteration, we ad-just the posterior probability of each sentence in thefollowing steps:?
Transform the posterior probabilities throughthe inverse function of the sigmoid function.The outputs are real values.?
Sort them and use the median of all the valuesas the border value.
This is because our data isbalanced.?
Subtract this border value from the transformedvalues.?
Transform the new values back into probabilityvalues using a sigmoid function.Note that there is a caveat here.
We are assum-ing we know the class distribution, based on labeledtraining data or human knowledge.
This is often areasonable assumption.
In addition, we are assum-ing that this class distribution is the same for theunlabeled data.
If this is not true, then the distri-bution adjustment performed in calibrated EM mayhurt system performance.5 Empirical EvaluationIn this section, we evaluate our unsupervised learn-ing method and analyze various impacting factors.163In preprocessing, we removed the punctuation andnumbers from the data and performed word stem-ming.
To measure performance, we use classifica-tion accuracy.5.1 Unsupervised Learning ResultsIn experiments of unsupervised learning, we per-form 5-fold cross validation.
We divide the cor-pus into 5 parts with equal size (each with balancedclass distribution).
In each run we reserve one partas the test set.
From the remaining data, we usethe lexicon-based method to create the initial train-ing data, containing 1,000 SUBJECTIVE and 1,000OBJECTIVE sentences.
The rest is used as unla-beled data to perform iterative learning.
The finalmodel is then applied to the reserved test set.
Fig-ure 1 shows the learning curves of calibrated EM onmovie, MPQA and AMI data respectively.556065707580859095012345678910111213141516171819iterationaccuracy(%)556065707580859095movieMPQAAMIFigure 1: Calibrated EM results using unsupervised setting(2,000 self-labeled initial samples) on movie, MPQA, and AMIdata.On movie data, calibrated EM improves the per-formance significantly (p<0.005), compared to thatbased on the initial training set (iteration 0).
It takesonly a few iterations for the EM method to convergeand at the end of the iteration, it achieves 90.15%accuracy, which rivals the fully supervised learn-ing performance (91.31% when using all the 8,000labeled sentences for training).
On MPQA data,this method yields some improvement (p<0.1) com-pared to the initial point.
But there is a peak accu-racy in the first couple of iterations, and then perfor-mance starts dropping thereafter.
On AMI data, theperformance degrades after the first iteration.5.2 Analysis and Discussion5.2.1 Effect of initial setFor unsupervised learning, our first question ishow the accuracy and size of the initial training setaffect performance.
We calculate the self-labelingaccuracy for the initial set using the lexicon basedmethod.
Table 2 shows the labeling accuracy whenusing different initial size, measured for SUBJEC-TIVE and OBJECTIVE class separately.
In addi-tion, we present the classification performance onthe test set when using the naive Bayes classifiertrained from the initial set.
Each size in the tablerepresents the total number of sentences in the ini-tial set.Table 2 shows that when the size is 2,000 (as weused in previous experiments), the accuracy for bothclasses on MPQA are even better than on movies,even though we have seen that iterative learningmethods perform much better on movies, suggest-ing that the initial data set accuracy is not the reasonfor the worse performance on MPQA than movies.It also shows that on movie data, as the initial sizeincreases, the accuracy of the pseudo training set de-creases, which is as expected (the top ranked self-labeled samples are more confident and accurate).However, this is not the case on MPQA and AMIdata.
There is no obvious drop of accuracy, rather inmany cases accuracy even increases when the initialsize increases.
It shows that on these two corpora,our lexicon-based method does not perform verywell because the most highly ranked sentences ac-cording to the subjective lexicon are not those mostsubjective sentences.size 100 200 1000 2000 3000moviesub 95.20 92.20 82.48 79.24 77.13obj 82.20 82.00 80.88 79.04 77.31Acc Test 59.93 71.63 77.62 79.24 79.64MPQAsub 83.20 85.60 85.76 85.18 82.53obj 87.60 86.60 87.64 87.46 85.92Acc Test 60.45 63.83 66.98 68.75 70.05AMIsub 49.60 53.40 65.96 66.98 67.05obj 71.60 71.00 68.56 69.04 69.89Acc Test 50.51 53.81 60.53 60.39 60.46Table 2: Initial pseudo training accuracy for SUBJECTIVE(sub) and OBJECTIVE (obj) class, and performance on the testusing this initial training set (Acc Test).
Results (all in %) areshown for different initial data size.From the results on the test set, we find that when164the size is smaller, such as containing 100 or 200samples, the accuracy on test set is lower than usinga bigger initial set.
This is mainly because there isnot sufficient data for model training.
For AMI data,this is also due to the low accuracy in the training set.When the initial size is large enough, the improve-ment from a larger training set is not as substantial,for example, using 1,000, 2,000, or 3,000 sentences.On AMI data, there is almost no difference amongthe three sets.
There is a tradeoff between the twofactors, self-labeling accuracy and the data size.
Of-ten an improvement in one aspect causes degrada-tion of the other.
A reasonable starting point needsto be chosen considering both factors.
Overall, itshows that the performance on test set can benefitmore from using a larger initial training set, thoughit may be noisy.In order to further investigate the impact of self-labeled initial data set, we perform standard semi-supervised learning using reference labels in theinitial data set.
The learning curve of this semi-supervised setting is shown in Figure 2.63687378838893012345678910111213141516171819iterationa c c u r a c y ( % )63687378838893movieMPQAAMIFigure 2: Calibrated EM results using semi-supervised learn-ing (2,000 labeled seed) on movie, MPQA, and AMI data.On movie data, calibrated EM yields better per-formance over that based on the initial training data(iteration 0).
We can see that calibrated EM con-verges very fast and achieves very high performancein the first iteration.
On MPQA and AMI data, cali-brated EM increases the accuracy at the first iterationbut then degrades thereafter.
This shows that incor-porating unlabeled data in training is helpful, how-ever, more EM iterations do not yield further gain.We noticed that on AMI data, even when the ini-tial set has 100% accuracy (i.e., semi-supervised set-ting), it still fails to yield any performance gain onAMI data.
It shows that the low accuracy of initialtraining set does not explain the poor performanceof unsupervised learning method.
Therefore, weconducted another set of experiments which use thesame semi-supervised setting but start from differentinitial training sizes.
We observed that on MPQAand AMI data, calibrated EM is able to increase theaccuracy only when the initial training set is small(less than 100 instances) and the performance at thestart point is poor.
We believe this is related to thedata property and the assumptions used in EM.
Sim-ilar patterns have been found in some previous stud-ies (Chapelle et al, 2006).
They attribute this to theincorrect model assumption, i.e., when the modelingassumptions for a particular classifier do not matchthe characteristics of the distribution of the data, un-labeled data may degrade the performance of classi-fiers.5.2.2 Effect of calibrationFigure 3 compares calibrated EM with standardEM using unsupervised learning on the three do-mains.
We can see that calibrated EM outperformsstandard EM, with a larger improvement on MPQAand AMI data.
When using standard EM, we findthat there is a larger difference between the numberof instances in the two classes based on the model?sprediction on MPQA and AMI data than movie data.For example, in one run using EM, in the first iter-ation the ratio of the two classes is 2.21, 1.88, and1.23 for MPQA, AMI, and movie data respectively.Calibrated EM is more effective on the two domainsbecause it adjusts the posterior probability of eachsample according to the class distribution in the data,making it more accurate in training the model in thenext iteration.5.2.3 Error analysisThere are two points worth discussing based onour error analysis.A.
Domain difference.Much of the difference we have observed can beattributed to the genre difference.
In movie reviews,often a person expresses his/her favor (or not) of themovie explicitly, making the task relatively easy forautomatic subjectivity classification.
MPQA datais collected from news resource, where subjectiv-ity mostly means an attitude or a judgment.
Take16578808284868890927880828486889092movie_EMmovie_cali_EM6869707172a c c u r a c y ( % )6869707172MPQA_EMMPQA_cali_EM575859606162012345678910111213141516171819iteration56575859606162AMI_EMAMI_cali_EMFigure 3: Comparison of standard EM and calibrated EM.the following sentence as an example: ?The UnitedStates is prepared to fight terrorism alone?.
It is la-beled as SUBJECTIVE because it expresses a deter-mination.
However, it may also be interpreted as anobjective statement.The AMI corpus consists of meeting conversa-tions.
The free-style dialogues are very differentfrom the style in review and news articles.
There aremany incomplete sentences and disfluencies.
Moreimportantly, the meaning of a sentence is often con-text dependent.
In the examples shown below, thetwo sentences look very similar, however, the firstsentence is labeled as ?OBJECTIVE?, and the sec-ond one as ?SUBJECTIVE?.
This is because of thedifferent context and speaker information ?
the sec-ond sentence expresses agreement, but the first ex-ample is just a sequence of discourse marker words.?
Alright yeah okay?
Yeah okay, true, true.We notice that many of the classification errors inAMI occur in very short sentences, like in the ex-ample shown above.
These short sentences are veryambiguous for subjectivity classification.B.
Limitation of the bag-of-word model.Our analysis also showed that some sentences aredifficult to classify if simply using surface words.
Inthe following, we show some examples of systemerrors.False negatives: subjective sentences recognized asobjective?
Johnson has, in his first film, set himself a task he isnot nearly up to.
(movie data)?
The news from Israel is almost earth-shattering.(MPQA)?
We can stick with what we already get.
(AMI)False positives: objective sentences recognized assubjective?
Cathy (Julianne Moore) is the perfect 50s house-wife, living the perfect 50s life: healthy kids, suc-cessful husband, social prominence.
(movie data)?
The committee Wednesday opened a formal de-bate on human rights questions, including alterna-tive approaches for improving the effective enjoy-ment of human rights and fundamental freedoms.(MPQA)?
um uh you know apple been really successful withthis surgical white kind of business or this sleekkind of (AMI)In the first three examples, there are no explicitsubjective clues, resulting in false negative errors.The subjective word ?earth-shattering?
is not in-cluded in subjective lexicon and rarely used in thecorpus.
The last three examples contain several sub-jective words, and are therefore labeled as subjec-tive.
These are the problems with the current wordbased approaches.6 Conclusion and Future WorkThis paper investigates an unsupervised learningprocedure for subjectivity identification at sentencelevel.
We use a lexicon-based method to create ini-tial training data and then apply a calibrated EM toutilize unlabeled corpus.
We evaluate this methodacross three different data sets and observe signif-icant difference.
It yields good performance onmovie data but does not achieve much performancegain on MPQA corpus, while on AMI corpus it failsto yield improvement.
Our analysis showed that per-formance of the base classifier has a substantial im-pact on iterative learning methods.
In addition, wefound that calibrated EM outperforms the standardEM method when the class distribution based onclassifier?s hypotheses does not match the real one.Our iterative learning approach uses a naiveBayes classifier that may not have accurate posteriorprobabilities.
Therefore in our future work, we willevaluate using other base models.
Our cross-corpusanalysis shows poor performance of subjectivity de-tection in AMI data.
We plan to explore more in-formation from multiparty dialogs to help improveperformance for that domain.1667 AcknowledgmentThe authors thank Theresa Wilson for sharing annotationfor the AMI corpus and helping with data processing forthat data.
Part of this work is supported by an NSF awardCNS-1059226.ReferencesO.
Chapelle, B. Scho?lkopf, and A. Zien, editors.
2006.Semi-supervised learning.
MIT Press.Sajib Dasgupta and Vincent Ng.
2009.
Mine the easy,classify the hard: a semi-supervised approach to auto-matic sentiment classification.
In Proceedings of ACL-IJCNLP, pages 701?709.Soo-Min Kim and Eduard Hovy.
2005.
Automatic de-tection of opinion bearing words and sentences.
InProceedings of ACL.Shoushan Li, Chu-Ren Huang, Guodong Zhou, andSophia Yat Mei Lee.
2010.
Employing per-sonal/impersonal views in supervised and semi-supervised sentiment classification.
In Proceedings ofACL, pages 414?423.Prem Melville, Wojciech Gryc, and Richard D.Lawrence.
2009.
Sentiment analysis of blogs by com-bining lexical knowledge with text classification.
InProceedings of ACM SIGKDD, pages 1275?1284.Gabriel Murray and Giuseppe Carenini.
2008.
Summa-rizing spoken and written conversations.
In Proceed-ings of EMNLP, pages 773?782.Gabriel Murray and Giuseppe Carenini.
2009.
Detectingsubjectivity in multiparty speech.
In Proceedings ofInterspeech.Vincent Ng, Sajib Dasgupta, and S. M. Niaz Arifin.
2006.Examining the role of linguistic knowledge sources inthe automatic identification and classification of re-views.
In Proceedings of COLING/ACL, pages 611?618.Kamal Nigam, Andrew Kachites McCallum, SebastianThrun, and Tom Mitchell.
2000.
Text classificationfrom labeled and unlabeled documents using EM.
Ma-chine Learning, 39:103?134.Bo Pang and Lilian Lee.
2004.
A sentimental educa-tion: sentiment analysis using subjectivity summariza-tion based on minimum cuts.
In Proceedings of ACL,pages 271?278.Bo Pang and Lillian Lee.
2008.
Using very simple statis-tics for review search: An exploration.
In Proceedingsof COLING, pages 73?76.Stephan Raaijmakers and Wessel Kraaij.
2008.
A Shal-low approach to subjectivity classification.
In Pro-ceedings of ICWSM.Stephan Raaijmakers, Khiet Truong, and Theresa Wilson.2008.
Multimodal subjectivity analysis of multipartyconversation.
In Proceedings of EMNLP, pages 466?474.Ellen Riloff and Janyce Wiebe.
2003.
Learning extrac-tion patterns for subjective expressions.
In Proceed-ings of EMNLP, pages 105?112.Yoshimasa Tsuruoka and Jun?ichi Tsujii.
2003.
Train-ing a naive bayes classifier via the EM algorithmwith a class distribution constraint.
In Proceedingsof NAACL, pages 127?134.Janyce Wiebe and Ellen Riloff.
2005.
Creating subjec-tive and objective sentence classifiers from unanno-tated texts.
In Proceedings of CICLing, pages 486?497.Theresa Wilson and Janyce Wiebe.
2003.
Annotatingopinions in the world press.
In Proceedings of SIG-dial, pages 13?22.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-levelsentiment analysis.
In Proceedings of HLT-EMNLP,pages 347?354.Theresa Wilson.
2008.
Annotating subjective content inmeetings.
In Proceedings of LREC.167
