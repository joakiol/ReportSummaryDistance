Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing, pages 9?17,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsOn Proper Unit Selection in Active Learning:Co-Selection Effects for Named Entity RecognitionKatrin Tomanek1?
Florian Laws2?
Udo Hahn1 Hinrich Schu?tze21Jena University Language & Information Engineering (JULIE) LabFriedrich-Schiller-Universita?t Jena, Germany{katrin.tomanek|udo.hahn}@uni-jena.de2Institute for Natural Language Processing, Universita?t Stuttgart, Germany{fl|hs999}@ifnlp.orgAbstractActive learning is an effective method for cre-ating training sets cheaply, but it is a biasedsampling process and fails to explore largeregions of the instance space in many appli-cations.
This can result in a missed clustereffect, which signficantly lowers recall andslows down learning for infrequent classes.We show that missed clusters can be avoidedin sequence classification tasks by using sen-tences as natural multi-instance units for label-ing.
Co-selection of other tokens within sen-tences provides an implicit exploratory com-ponent since we found for the task of namedentity recognition on two corpora that en-tity classes co-occur with sufficient frequencywithin sentences.1 IntroductionActive learning (AL) has been shown to be an effec-tive approach to reduce the amount of data neededto train an accurate statistical classifier.
AL selectshighly informative examples from a pool of unla-beled data and prompts a human annotator for thelabels of these examples.
The newly labeled exam-ples are added to a training set used to build a statis-tical classifier.
This classifier is in turn used to assessthe informativeness of further examples.
Thus, aselect-label-retrain loop is formed that quickly se-lects hard to classify examples, honing in on the de-cision boundary (Cohn et al, 1996).A fundamental characteristic of AL is the fact thatit constitutes a biased sampling process.
This is so?
Both authors contributed equally to this work.by design, but the bias can have an undesirable con-sequence: partial coverage of the instance space.
Asa result, classes or clusters within classes may becompletely missed, resulting in low recall or slowlearning progress.
This has been called the missedcluster effect (Schu?tze et al, 2006).
While AL hasbeen studied for a range of NLP tasks, the missedcluster problem has hardly been addressed.This paper studies the missed class effect, a spe-cial case of the missed cluster effect where completeclasses are overlooked by an active learner.
Themissed class effect is the result of insufficient ex-ploration before or during a mainly exploitative ALprocess.
In AL approaches where exploration is onlyaddressed by an initial seed set, poor seed set con-struction gives rise to the missed class effect.We focus on the missed class effect in the con-text of a common NLP task: named entity recogni-tion (NER).
We show that for this task the missedclass effect is avoided by increasing the samplinggranularity from single-instance units (i.e., tokens)to multi-instance units (i.e., sentences).
For AL ap-proaches to NER, sentence selection recovers betterfrom unfavorable seed sets than token selection dueto what we call the co-selection effect.
Under thiseffect, a non-targeted entity class co-occurs in sen-tences that were originally selected because of un-certainty on tokens of a different entity class.The rest of the paper is structured as follows: Sec-tion 2 introduces the missed class effect in detail.Experiments which demonstrate the co-selection ef-fect achieved by sentence selection for NER are de-scribed in Section 3 and their results presented inSection 4.
We draw conclusions in Section 5.92 The Missed Class EffectThis section first describes the missed class ef-fect.
Then, we discuss several factors influencingthis effect, focusing on co-selection, a natural phe-nomenon in common NLP applications of AL.2.1 Sampling bias and misguided ALThe distribution of the labeled data points obtainedwith an active learner deviates from the true datadistribution.
While this sampling bias is intendedand accounts for the effectiveness of AL, it alsoposes challenges as it leads to classifiers that per-form poorly in some regions, or clusters, of the ex-ample space.
In the literature, this phenomenon hasbeen described as the missed cluster effect (Schu?tzeet al, 2006; Dasgupta and Hsu, 2008)In this context, we must distinguish between ex-ploration and exploitation.
By design, AL is ahighly exploitative strategy: regions around decisionboundaries are inspected thoroughly so that decisionboundaries are learned well, but regions far from anyof the initial decision boundaries remain unexplored.An exploitative sampling approach thus has to becombined with some kind of exploratory strategy tomake sure the example space is adequately covered.A common approach is to start an AL process withan initial seed set that accounts for the explorationstep.
However, a seed set which is not represen-tative of the example space may completely mis-guide AL ?
at least when no other explorative tech-niques are applied as a remedy.
While approachesto balancing exploration and exploitation (Baram etal., 2003; Dasgupta and Hsu, 2008; Cebron andBerthold, 2009) have been discussed, we here fo-cus on a ?pure?
AL scenario where exploration takesonly place in the beginning by a seed set.
In sum-mary, the missed clusters are the result of a sce-nario where poor exploration is combined with ex-clusively exploitative sampling.Why is AL an exploitative sampling strategy?
ALselects data points based on the confidence of the ac-tive learner.
Assume an initial seed set that does notcontain examples of a specific cluster.
This leads toan initial active learner that is mistakenly overconfi-dent about the class membership of instances in thismissed cluster.
Far away from the decision bound-ary, the active learner assumes a high confidence forABC(a)ABC(b)Figure 1: Illustration of the missed cluster effect in a 1-d scenario.
Shaded points are contained in the seed set,vertical lines are final decision boundaries, and dashedrectangles mark the explored regionsall instances in that cluster, even if they are in factmisclassified.
Consequently, the active learner willfail to select these instances for long until some re-direction impulse is received (if at all).To give an example, let us consider a simple 1-d toy scenario with examples from three clusters A,B, and C as shown in Figure 1.
In scenario (a), ALis started from a seed set including one example ofclusters A and B only.
In subsequent rounds, ALwill select examples in these clusters only (shown asthe dashed box in the figure).
Examples in clusterC are ignored as they are far from the initial deci-sion boundary.
Eventually, a decision boundary isfixed as shown by the vertical line which indicatesthat this AL process has completely overlooked ex-amples from cluster C.Assuming that the examples fall in two classesX1 = {A ?
C} and X2 = {B} the learned clas-sifier has low recall for class X1 and relatively lowprecision for class X2 as it erroneously assigns ex-amples of cluster C to class X2.
In a related sce-nario with three classes X1 = {A}, X2 = {B}, andX3 = {C} this would even mean that the classifieris not at all aware about the third class resulting inthe missed class problem.A more representative seed set circumvents thisproblem.
Given a seed set including one exampleof each cluster, AL might find a second decisionboundary1 between clusters B and C because it isnow aware of examples from C. Figure 1(b) showsa possible result of AL on this seed set.The missed cluster effect can be understood asthe generalized problem.
A special case of it is the1Assuming a classifier that can learn several boundaries.10missed class effect as shown in the previous exam-ple.
In general, it has the same causes (insufficientexploration and misguided exploitation), but is eas-ier to test.
Often we know (at least the number of) allclasses under scrutiny, while we usually cannot as-sume all clusters in the feature space to be known.
Inthis paper, we focus on the missed class effect, i.e.,scenarios where classes are overlooked by a mis-guided AL process resulting in a slow (active) learn-ing progress.2.2 Factors influcencing the missed class effectAL in a practical scenario is subject to several fac-tors which mitigate or intensify the missed class ef-fect described before.
In the following, we describethree such factors, with a special focus on the co-selection effect, which we claim to significantly mit-igate the missed class effect in a specific type of NLPtasks, sequence learning problems such as NER orPOS tagging.Class imbalance Many studies on AL for NLPtasks assume that AL is started from a randomlydrawn seed set.
Such a seed set can be problem-atic when the class distribution in the data is highlyskewed.
In this case, ?rare?
classes might not berepresented in the seed set, increasing the chance tocompletely miss out such a class using AL.
Whenclasses are relatively frequent, an active learner ?even when started from an unfavorable seed set ?might still mistake an example of one class for anuncertain example of a different class and conse-quently select it.
Thereby, it can acquire informationabout the former class ?by accident?
leading to sud-den and rapid discovery of the newly-found class.However, in the case of extreme class imbalance thisis very unlikely.
Severe class imbalance intensifiesthe missed cluster effect.Similarity of considered classes If, e.g., two ofthe classes to be learned, say Xi and Xj , are harderto discriminate than others, or if the data containslots of noise, an active learner is more likely to selectsome instances of Xi if at least its ?similar?
coun-terpart Xj was represented in the seed set.
Hence,it may mistake the instances of Xi and Xj before ithas acquired enough information to discriminate be-tween them.
So, under certain situations similarityof classes can mitigate the missed class effect.The co-selection effect Many NLP tasks are se-quence learning problems including, e.g., POS tag-ging, and named entity recognition.
Sequences areconsecutive text tokens constituting linguisticallyplausible chunks, e.g., sentences.
Algorithms for se-quence learning obviously work on sequence data,so respective AL approaches need to select completesequences instead of single text tokens (Settles andCraven, 2008).
Furthermore, sentence selection hasbeen preferred over token selection in other workswith the argument that the manual annotation of sin-gle, possibly isolated tokens is almost impossible orat least extremely time-consuming (Ringger et al,2007; Tomanek et al, 2007).Within such sequences, instances of differentclasses often co-occur.
Thus, an active learner thatselects uncertain examples of one class gets exam-ples of a second class as an unintended, yet pos-itive side effect.
We call this the co-selection ef-fect.
As a result, AL for sequence labeling is not?pure?
exploitative AL, but implicitly comprises anexploratory aspect which can substantially reducethe missed class problem.
In scenarios where wecannot hope for such a co-selection, we are muchmore likely to have decreased AL performance dueto missed clusters or classes.3 ExperimentsWe ran several experiments to investigate how thesampling granularity, i.e.
the size of the selectionunit, influences the missed class effect.
AL basedon token selection (T-AL) is compared to AL basedon sentence selection (S-AL).
Although our experi-ments are certainly also subject to the other factorsmitigating the missed class effect (e.g.
similarity ofclasses), the main focus of the experiments is on theco-selection effect that we expected to observe inS-AL.
Several scenarios of initial exploration weresimulated by seed sets of different characteristics.The experiments were run on synthetic and real datain the context of named entity recognition (NER).3.1 Classifiers and active learning setupThe active learning approach used for both S-ALand T-AL is based on uncertainty sampling (Lewisand Gale, 1994) with the margin metric (Schein andUngar, 2007) as uncertainty measure.
Let c and c?11be the two most likely classes predicted for tokenxj with p?c,xj and p?c?,xj being the associated classprobabilities.
The per-token margin is calculated asM = |p?c,xj ?
p?c?,xj |.For T-AL, the sampling granularity is the token,while in S-AL, complete sentences are selected.
ForS-AL, the margins of all tokens in a sentence areaveraged and the aggregate margin is used to selectsentences.
We chose this uncertainty measure for S-AL for better comparison with T-AL.
In either case,examples (tokens or sentences) with a small marginare preferred for selection.
In every iteration, a batchof examples is selected: 20 sentences for S-AL, 200tokens for T-AL.Bayesian logistic regression as implemented inthe BBR classification package (Genkin et al, 2007)with out-of-the-box parameter settings was used asbase learner for T-AL.
For S-AL, a linear-chainConditional Random Field (Lafferty et al, 2001) isemployed as implemented in MALLET (McCallum,2002).
Both base learners employ standard featuresfor NER including the lexical token itself, variousorthographic features such as capitalization, the oc-currence of special characters like hyphens, and con-text information in terms of features of neighboringtokens to the left and right of the current token.3.2 Data setsWe used three data sets in our experiments.
Two ofthem (ACE and PBIO) are standard data sets.
Thethird (SYN) is a synthetic set constructed to havespecific characteristics.
For simplicity, we consideronly scenarios with two entity classes, a majorityclass (MAJ) and a minority class (MIN).
We dis-carded all other entity annotations originally con-tained in the corpus assigning the OUTSIDE class.2The first data set (PBIO) is based on the annota-tions of the PENNBIOIE corpus for biomedical en-tity extraction (Kulick et al, 2004).
As PENNBIOIEmakes fine-grained and subtle distinctions betweenvarious subtypes of classes irrelevant for this study,we combined several of the original classes into twoentity classes: The majority class consists of thethree original classes ?gene-protein?, ?gene-generic?,and ?gene-rna?.
The minority class consists ofthe original and similar classes ?variation-type?
and2The OUTSIDE class marks that a token is not part of annamed entity.?variation-event?.
All other entity labels were re-placed by the OUTSIDE class.The second data set (ACE) is based on thenewswire section of the ACE 2005 MultilingualTraining Corpus (Walker et al, 2006).
We chosethe ?person?
class as majority class and the ?organi-zation?
class as the minority class.
Again, all otherclasses are mapped to OUTSIDE.The synthetic data set (SYN) was constructed bycombining the sentences from the original ACE andPENNBIOIE corpora.
The ?person?
class consti-tutes the minority class, the very similar classes?malignancy?
and ?malignancy-type?
were mergedto form the majority class.
All other class la-bels were set to OUTSIDE.
SYN?s constructionwas motivated by the following characteristics ofthe new data set which would make the appear-ance of the missed class effect very likely forinsufficient exploration scenarios:(i) absence of inner-sentence entity class correlationto ensure that sentences contain either mentions ofonly a single entity class or no mentions at all.
(ii) marked entity class imbalance between the ma-jority and minority classes(iii) dissimilar surface patterns of entity mentions ofthe two entity classes with the rationale that classsimilarity will be low.Table 1 summarizes characteristics of the datasets.
While SYN exhibits high imbalance (e.g., 1:9.4on the token level), PBIO and ACE are moderatelyskewed.
In PBIO, the number of sentences contain-ing any entity mention is relatively high comparedto ACE or SYN.
For our experiments, the corporawere randomly split in a pool for AL and a test setfor performance evaluation.Inner-sentence entity class co-occurrence Wehave described co-selection as a potential mitigat-ing factor for the missed class effect in Section 2.For this effect to occur, there must be some corre-lation between the occurrence of entity mentions ofthe MAJ class with those from MIN.Table 2 shows correlation statistics based on the?2 measure.
We found strong correlation in all threecorpora3: For ACE and PBIO, the correlation is pos-itive; for SYN it is negative so when a sentence inSYN contains a majority class entity mention, it is3All correlations are statistically significant (p < 0.01).12PBIO ACE SYNsentences (all) 11,164 2,642 13,804sentences (MAJ) 7,075 767 5,667sentences (MIN) 2,156 974 974MIN-MAJ ratio 1 : 3.3 1 : 1.3 1 : 5.8tokens (all) 277,053 66,752 343,773tokens (MAJ) 17,928 2,008 18,959tokens (MIN) 4,079 1,822 2,008MIN-MAJ ratio 1 : 4.4 1 : 1.1 1 : 9.4Table 1: Characteristics of the data sets; ?sentences(MAJ)?, e.g., specifies the number of sentences contain-ing mentions of the majority class.PBIO ACE SYN?2 132.34 6.07 727P (MIN |MAJ) 0.26 0.31 0.0Table 2: Co-occurrence of entity classes in sentenceshighly unlikely that it also contains a minority entity.In fact, it is impossible by construction of the dataset.
Further, this table shows the probability that asentence containing the majority class also containsthe minority class.
As expected, this is exactly 0 forSYN, but significantly above 0 for PBIO and ACE.3.3 Seed setsSelection of an appropriate seed set for the start of anAL process is important to the success of AL.
This isespecially relevant in the case of imbalanced classesbecause a typically small random sample will pos-sibly not contain any example of the rare class.
Weconstructed different types of seed sets (whose nam-ing intentionally reflects the use of the entity classesfrom Section 3.2) to simulate different scenarios ofill-managed initial exploration.
All seed sets havea size of 20 sentences.
The RANDOM set was ran-domly sampled, the MAJ set is made of sentencescontaining at least one majority class entity, but nominority class entity.
Accordingly, MIN is denselypopulated with minority entities.
Finally, OUTSIDEcontains only sentences without entity mentions.One could think of the OUTSIDE and MAJ seedsets of cases where a random seed set selection hasunluckily produced an especially bad seed set.
MINserves to demonstrate the opposite case.
For eachtype of seed set, we sampled ten independent ver-sions to calculate averages over several AL runs.3.4 Cost measureThe success of AL is usually measured as reduc-tion of annotation effort according to some cost mea-sure.
Traditionally, the most common cost measureconsiders a unit cost per annotated token, which fa-vors AL systems that select individual tokens.
Ina real annotation setting, however, it is unnatural,and therefore hard for humans to annotate single,possibly isolated tokens, leading to bad annotationquality (Hachey et al, 2005; Ringger et al, 2007).When providing context, the question arises whetherthe annotator can label several tokens present in thecontext (e.g., an entire multi-token entity or eventhe whole sentence) at little more cost than anno-tating a single token.
Thus, assigning a linear costof n to a sentence where n is the sentence?s lengthin tokens seems to unfairly disadvantage sentence-selection AL setups.However, more work is needed to find a more re-alistic cost measure.
At present there is no othergenerally accepted cost measure than unit cost pertoken, so we report costs using the token measure.4 ResultsThis section presents the results of our experimentson the missed class effect in two different ALscenarios, i.e., sentence selection (S-AL) and to-ken selection (T-AL).
The AL runs were stoppedwhen convergence on the minority class F-score wasachieved.
This was done because early AL iterationsbefore the convergence point are most important andrepresentative for a real-life scenario where the poolis extremely large, so that absolute convergence ofthe classifier?s performance will never be reached.The learning curves in Figures 2, 3, and 4 revealgeneral characteristics of S-AL compared to T-AL.For S-AL, the number of tokens on the x-axis is thetotal number of tokens in the sentences labeled sofar.
While S-AL generally yields higher F-scores, T-AL converges much earlier when counted in termsof tokens.
The reason for this is that T-AL can se-lect uncertain data more specifically.
In contrast, S-AL also selects tokens that the classifier can alreadyclassify reliably ?
these tokens are selected becausethey co-occur in a sentence that also contains an un-certain token.
Whether T-AL is really more efficientclearly depends on the cost-metric applied (cf.
Sec-130 5000 10000 150000.00.20.40.60.8tokensF?scoreMIN classMAJ class(a) T-AL learning curve, single runwith OUTSIDE seed0 5000 10000 150000.50.60.70.80.91.0tokensmeanmarginMIN classMAJ class(b) T-AL mean margin curve, singlerun with OUTSIDE seed0 5000 10000 150000.00.20.40.60.8tokensminorityclass F?scoreMIN seedMAJ seedOUTSIDE seedRANDOM seedrandom selection(c) T-AL learning curves, minorityclass, all seeds, 10 runs0 10000 30000 50000 700000.00.20.40.60.8tokensper class F?scoreMIN classMAJ class(d) S-AL learning curve, single runwith OUTSIDE seed0 10000 30000 50000 700000.00.20.40.60.81.0tokensmeanmarginMIN classMAJ class(e) S-AL mean margin curve, singlerun with OUTSIDE seed0 10000 30000 50000 700000.00.20.40.60.8tokensminorityclass F?scoreMIN seedMAJ seedOUTSIDE seedRANDOM seedrandom selection(f) S-AL learning curves, minorityclass, all seeds, 10 runsFigure 2: Results on SYN corpus for token selection (a,b,c) and sentence selection (d,e,f)tion 3.4).
Since the focus of this paper is on compar-ing the missed class effect in a sentence and a tokenselection AL setting (T-AL and S-AL) we apply thestraight-forward token measure.4.1 The pathological caseFigure 2 shows results on the SYN corpus for T-AL(upper row) and S-AL (lower row).
Figures 2(a)and 2(d) show the minority and majority class learn-ing curves for a single run starting from the OUT-SIDE seed set, which was particularly problematicon SYN.
(We show single runs to give a better pic-ture of what happens during the selection process.
)The figures show that for both AL scenarios, theOUTSIDE seed set caused the active learner to focusexclusively on the majority class and to completelyignore the minority class for many AL iterations (al-most 30,000 tokens for S-AL and over 4,000 tokensfor T-AL).
Had we stopped the AL process beforethis turning point, the classifier?s performance onthe majority entity class would have been reason-ably high while the minority class would not havebeen learned at all ?
which is precisely the defini-tion of an (initially) missed class.Figures 2(b) and 2(e) show the correspondingmean margin plots of these AL runs, indicating theconfidence of the classifier on each class.
The meanmargin is calculated as the average margin over to-kens in the remaining pool, separately for each trueclass label.4 As expected, the active learner is over-confident but wrong on instances of the minorityclass (assigning them to the OUTSIDE class, weassume).
Only after some time, margin scores onminority class tokens start decreasing.
This hap-pens because from time to time minority class ex-amples are mistakenly considered as majority classexamples with low confidence and thus selected byaccident.
Lowered minority class confidence thencauses the selection of further minority class exam-ples, resulting in a turning point with a steep slopeof the minority class learning curve.Consequences of seed set selection We comparethe minority class learning curves for all types of4Note that in a real, non-simulation active learning task, thetrue class labels would be unknown.140 5000 10000 15000 200000.00.20.40.60.8tokensF?scoreMIN classMAJ class(a) T-AL learning curve, single runwith MAJ seed0 5000 10000 15000 200000.50.60.70.80.91.0tokensmeanmarginMIN classMAJ class(b) T-AL mean margin curve, singlerun with MAJ seed0 5000 10000 15000 200000.00.20.40.60.8tokensminorityclass F?scoreMIN seedMAJ seedOUTSIDE seedRANDOM seedrandom selection(c) T-AL learning curves, minorityclass, all seeds, 10 runs0 10000 20000 30000 40000 500000.00.20.40.60.8tokensper class F?scoreMIN classMAJ class(d) S-AL learning curve, single runwith MAJ seed0 10000 20000 30000 40000 500000.00.20.40.60.81.0tokensmeanmarginMIN classMAJ class(e) S-AL mean margin curve, singlerun with MAJ seed0 10000 20000 30000 40000 500000.00.20.40.60.8tokensminorityclass F?scoreMIN seedMAJ seedOUTSIDE seedRANDOM seedrandom selection(f) S-AL learning curves, minorityclass, all seeds, 10 runsFigure 3: Results on PBIO corpus for token selection (a,b,c) and sentence selection (d,e,f)seed sets and for random selection (cf.
Figures 2(c)and 2(f)), now averaged over 10 runs.
On S-AL allbut the MIN seed set were inferior to random selec-tion.
Even the commonly used random seed set se-lection is problematic because the minority class isso rare that there are random seed sets without anyexample of the minority class.On T-AL, all seed sets are better than random se-lection.
This, however, is because random selec-tion is an extremely weak baseline for T-AL due tothe token distribution (cf.
Table 1).
Still, the RAN-DOM, MAJ, and OUTSIDE seed sets are signifi-cantly worse than a seed set which covers the minor-ity class well.
Note that the majority class learningcurves are relatively invariant against different seedsets.
The minority class seed set does have somenegative impact on initial learning progress on themajority class (not shown here), but the impact israther small.
Because of the higher frequency ofthe majority class, the classifier soon finds major-ity class examples to compensate for the seed set bychance or class similarity.4.2 Missed class effect mitigated by co-selectionResults on PBIO corpus On the PBIO corpus,where minority and majority class entity mentionsnaturally co-occur on the sentence level, we geta different picture.
Figure 3 shows the learning(3(a), 3(d)) and mean margin (3(b), 3(e)) curves forthe MAJ seed set.
T-AL still exhibits the missedclass effect on this seed set.
The minority classlearning curve again has a delayed slope and highmean margin scores of minority tokens at the be-ginning, resulting in insufficient selection and slowlearning.
S-AL, on the other hand, does not re-ally suffer from the missed class effect: minor-ity class entity mentions are co-selected in sen-tences which were chosen due to uncertainty onmajority class tokens.
Minority class mean mar-gin scores quickly fall, reinforcing selection for mi-nority class entities.
Learning curves for minorityand majority classes run approximately in parallel.Figure 3(f) shows that all seed sets perform quitesimilar for S-AL.
MIN unsurprisingly is a bit better.With the other seed sets, S-AL performance is com-150 2000 4000 6000 80000.00.20.40.60.8tokensminorityclass F?scoreMIN seedMAJ seedOUTSIDE seedRANDOM seedrandom selection(a) T-AL0 5000 15000 250000.00.20.40.60.8tokensminorityclass F?scoreMIN seedMAJ seedOUTSIDE seedRANDOM seedrandom selection(b) S-ALFigure 4: Minority class learning curves for all seeds onACE averaged over 10 runsparable to random selection.
On the PBIO corpus,random selection is a strong baseline as almost everysentence contains an entity mention ?
which is notthe case for SYN and ACE (cf.
Table 1).
As there isno co-selection effect for T-AL, the MAJ and OUT-SIDE seed sets also here are subject to the missedclass problem (Figure 3(c)), although not as severelyas on the SYN corpus.Results on ACE corpus Figure 4 shows learningcurves averaged over 10 runs on ACE.
Overall, themissed class effect is less pronounced on ACE com-pared to PBIO.
Still, co-selection avoids a good por-tion of the missed class effect on S-AL ?
all seedsets yield results much better than random selectionright from the beginning.On T-AL, the OUTSIDE seed set has a markednegative effect.
However, while different seedsets still have visible differences in learning perfor-mance, the magnitude of the effect is smaller thanon PBIO.
It is difficult to find the exact reasonsin a non-synthetic, natural language corpus where alot of different effects are intermingled.
One mightassume higher class similarity between the major-ity (?persons?)
and the minority (?organizations?
)classes on the ACE corpus than, e.g., on the PBIOcorpus.
Moreover, there is hardly any imbalancein frequency between the two entity classes on theACE corpus.
We briefly discussed such influencingfactors possibly mitigating the missed class effect inSection 2.2.4.3 DiscussionTo summarize, on a synthetic corpus (SYN) themissed class effect can be well studied in bothAL scenarios, i.e., S-AL and T-AL.
Moving froma relatively controlled, synthetic corpus (extremeclass imbalance, no inner-sentence co-occurrencebetween entity classes, quite different entity classes)to more realistic corpora, effects generally mix a bitdue to different degrees of class imbalance and prob-ably higher similarity between entity classes.Our experiments unveil that co-selection in S-ALeffectively helps avoid dysfunctional classifiers thatinsufficiently explore the instance space due to adisadvantageous seed set.
In contrast, AL basedon token-selection (T-AL) cannot recover from in-sufficient exploration as easy as AL with sentence-selection and is thus more sensitive to the missedclass effect.5 ConclusionWe have shown that insufficient exploration in theinitial stages of active learning gives rise to regionsof the sample space that contain missed classes thatare incorrectly classified.
This results in low clas-sification performance and slow learning progress.Comparing two sampling granularities, tokens vs.sentences, we found that the missed class effect ismore severe when isolated tokens instead of sen-tences are selected for labeling.The missed class problem in sequence classifica-tion tasks can be avoided using sentences as naturalmulti-instance units for selection and labeling.
Us-ing multi-instance units, co-selection of other tokenswithin sentences provides an implicit exploratorycomponent.
This solution is effective if classes co-occur sufficiently within sentences which is the casefor many real-life entity recognition tasks.While other work has proposed sentence selectionin AL for sequence labeling as a means to ease andspeed up annotation, we have gathered here addi-tional motivation from the perspective of robustnessof learning.
Future work will compare the beneficialeffect introduced by co-selection with other forms ofexploration-enabled active learning.AcknowledgementsThe first and the third author were funded by theGerman Ministry of Education and Research withinthe StemNet project (01DS001A-C) and by the ECwithin the BOOTStrep project (FP6-028099).16ReferencesYoram Baram, Ran El-Yaniv, and Kobi Luz.
2003.
On-line choice of active learning algorithms.
In ICML?03: Proceedings of the 20th International Conferenceon Machine Learning, pages 19?26.Nicolas Cebron and Michael R. Berthold.
2009.
Activelearning for object classification: From exploration toexploitation.
Data Mining and Knowledge Discovery,18(2):283?299.David A. Cohn, Zoubin Ghahramani, and Michael I. Jor-dan.
1996.
Active learning with statistical models.Journal of Artificial Intelligence Research, 4:129?145.Sanjoy Dasgupta and Daniel Hsu.
2008.
Hierarchicalsampling for active learning.
In ICML ?08: Proceed-ings of the 25th International Conference on MachineLearning, pages 208?215.Alexander Genkin, David D. Lewis, and David Madigan.2007.
Large-scale Bayesian logistic regression for textcategorization.
Technometrics, 49(3):291?304.Ben Hachey, Beatrice Alex, and Markus Becker.
2005.Investigating the effects of selective sampling on theannotation task.
In CoNLL ?05: Proceedings of the9th Conference on Computational Natural LanguageLearning, pages 144?151.Seth Kulick, Ann Bies, Mark Liberman, Mark Mandel,Ryan T. McDonald, Martha S. Palmer, and Andrew IanSchein.
2004.
Integrated annotation for biomedicalinformation extraction.
In Proceedings of the HLT-NAACL 2004 Workshop ?Linking Biological Litera-ture, Ontologies and Databases: Tools for Users?,pages 61?68.John D. Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Probabilis-tic models for segmenting and labeling sequence data.In ICML ?01: Proceedings of the 18th InternationalConference on Machine Learning, pages 282?289.David D. Lewis and William A. Gale.
1994.
A sequentialalgorithm for training text classifiers.
In Proceedingsof the 17th Annual International ACM SIGIR Confer-ence on Research and Development in Information Re-trieval, pages 3?12.Andrew McCallum.
2002.
MALLET: A machine learn-ing for language toolkit.
http://mallet.cs.umass.edu.Eric Ringger, Peter McClanahan, Robbie Haertel, GeorgeBusby, Marc Carmen, James Carroll, Kevin Seppi, andDeryle Lonsdale.
2007.
Active learning for part-of-speech tagging: Accelerating corpus annotation.
InProceedings of the Linguistic Annotation Workshop atACL-2007, pages 101?108.Andrew Schein and Lyle Ungar.
2007.
Active learn-ing for logistic regression: An evaluation.
MachineLearning, 68(3):235?265.Hinrich Schu?tze, Emre Velipasaoglu, and Jan Pedersen.2006.
Performance thresholding in practical text clas-sification.
In CIKM ?06: Proceedings of the 15th ACMInternational Conference on Information and Knowl-edge Management, pages 662?671.Burr Settles and Mark Craven.
2008.
An analysis of ac-tive learning strategies for sequence labeling tasks.
InEMNLP ?08: Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Processing,pages 1070?1079.Katrin Tomanek, Joachim Wermter, and Udo Hahn.2007.
An approach to text corpus construction whichcuts annotation costs and maintains reusability of an-notated data.
In Proceedings of the 2007 Joint Confer-ence on Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learn-ing, pages 486?495.Christopher Walker, Stephanie Strassel, Julie Medero,and Kazuaki Maeda.
2006.
ACE 2005 Multilin-gual Training Corpus.
Linguistic Data Consortium,Philadelphia.17
