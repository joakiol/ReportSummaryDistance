Identifying idiomatic expressions using automatic word-alignmentBegon?a Villada Moiro?n and Jo?rg TiedemannAlfa Informatica, University of GroningenOude Kijk in ?t Jatstraat 269712 EK Groningen, The Netherlands{M.B.Villada.Moiron,J.Tiedemann}@rug.nlAbstractFor NLP applications that require somesort of semantic interpretation it would behelpful to know what expressions exhibitan idiomatic meaning and what expres-sions exhibit a literal meaning.
We invest-igate whether automatic word-alignmentin existing parallel corpora facilitatesthe classification of candidate expressionsalong a continuum ranging from literal andtransparent expressions to idiomatic andopaque expressions.
Our method relies ontwo criteria: (i) meaning predictability thatis measured as semantic entropy and (ii),the overlap between the meaning of an ex-pression and the meaning of its compon-ent words.
We approximate the mentionedoverlap as the proportion of default align-ments.
We obtain a significant improve-ment over the baseline with both meas-ures.1 IntroductionKnowing whether an expression receives a lit-eral meaning or an idiomatic meaning is import-ant for natural language processing applicationsthat require some sort of semantic interpretation.Some applications that would benefit from know-ing this distinction are machine translation (Im-amura et al, 2003), finding paraphrases (Bannardand Callison-Burch, 2005), (multilingual) inform-ation retrieval (Melamed, 1997a), etc.The purpose of this paper is to explore to whatextent word-alignment in parallel corpora can beused to distinguish idiomatic multiword expres-sions from more transparent multiword expres-sions and fully productive expressions.In the remainder of this section, we present ourcharacterization of idiomatic expressions, the mo-tivation to use parallel corpora and related work.Section 2 describes the materials required to ap-ply our method.
Section 3 portraits the routine toextract a list of candidate expressions from auto-matically annotated data.
Experiments with differ-ent word alignment types and metrics are shownin section 4.
Our results are discussed in section 5.Finally, we draw some conclusions in section 6.1.1 What are idiomatic expressions?Idiomatic expressions constitute a subset of mul-tiword expressions (Sag et al, 2001).
We assumethat literal expressions can be distinguished fromidiomatic expressions provided we know how theirmeaning is derived.1 The meaning of linguisticexpressions can be described within a scale thatranges from fully transparent to opaque (in figur-ative expressions).
(1) Watwhatmoetenmustlidstatenmember statesondernemendoomtoaanathaarhereisendemandstetovoldoen?meet?
?What must EU member states do to meet herdemands??
(2) Dezethissituatiesituationbrengtbringsdethebestaandeexistingpolitiekepoliticalbarrie`resbarrierszeerveryduidelijkclearlyaaninhetthelicht.light?This situation brings the existing politicallimitations to light very clearly.
?1Here, we ignore morpho-syntactic and pragmatic factorsthat could help model the distinction.33(3) Wijwemogenmayonsushierherenietnotbijbyneerleggen,agree,maarbutmoetenmustdethesituatiesituationpubliekelijkpubliclyaanopdethekaakcheekstellen.state?We cannot agree but must denounce the situ-ation openly.
?Literal and transparent meaning is associatedwith high meaning predictability.
The meaning ofan expression is fully predictable if it results fromcombining the meaning of its individual wordswhen they occur in isolation (see (1)).
Whenthe expression undergoes a process of metaphor-ical interpretation its meaning is less predictable.Moon (1998) considers a continuum of transpar-ent, semi-transparent and opaque metaphors.
Themore transparent metaphors have a rather predict-able meaning (2); the more opaque have an un-predictable meaning (3).
In general, an unpredict-able meaning results from the fact that the mean-ing of the expression has been fossilized and con-ventionalized.
In an uninformative context, idio-matic expressions have an unpredictable meaning(3).
Put differently, the meaning of an idiomaticexpression cannot be derived from the cumulativemeaning of its constituent parts when they appearin isolation.1.2 Why checking translations?This paper addresses the task of distinguishing lit-eral (transparent) expressions from idiomatic ex-pressions.
Deciding what sort of meaning an ex-pression shows can be done in two ways:?
measuring how predictable the meaning ofthe expression is and?
assessing the link between (a) the meaning ofthe expression as a whole and (b) the cumu-lative literal meanings of the components.Fernando and Flavell (1981) observe that noconnection between (a) and (b) suggests the ex-istence of opaque idioms and, a clear link between(a) and (b) is observed in clearly perceived meta-phors and literal expressions.We believe we can approximate the meaningof an expression by looking up the expressions?translation in a foreign language.
Thus, we areinterested in exploring to what extent parallel cor-pora can help us to find out the type of meaning anexpression has.For our approach we make the following as-sumptions:?
regular words are translated (more or less)consistently, i.e.
there will be one or onlya few highly frequent translations whereastranslation alternatives will be infrequent;?
an expression has a (almost) literal meaningif its translation(s) into a foreign language isthe result of combining each word?s transla-tion(s) when they occur in isolation into a for-eign language;?
an expression has a non-compositional mean-ing if its translation(s) into a foreign languagedoes not result from a combination of the reg-ular translations of its component words.We also assume that an automatic word alignerwill get into trouble when trying to align non-decomposable idiomatic expressions word byword.
We expect the aligner to produce a largevariety of links for each component word in suchexpressions and that these links are different fromthe default alignments found in the corpus other-wise.Bearing these assumptions in mind, our ap-proach attempts to locate the translation of a MWEin a target language.
On the basis of all recon-structed translations of a (potential) MWE, it is de-cided whether the original expression (in sourcelanguage) is idiomatic or a more transparent one.1.3 Related workMelamed (1997b) measures the semantic entropyof words using bitexts.
Melamed computes thetranslational distribution T of a word s in a sourcelanguage and uses it to measure the translationalentropy of the word H(T|s); this entropy approx-imates the semantic entropy of the word that canbe interpreted either as (a) the semantic ambigu-ity or (b) the inverse of reliability.
Thus, a wordwith high semantic entropy is potentially very am-biguous and therefore, its translations are less re-liable (or highly context-dependent).
We alsouse entropy to approximate meaning predictabil-ity.
Melamed (1997a) investigates various tech-niques to identify non-compositional compoundsin parallel data.
Non-compositional compounds34are those sequences of 2 or more words (adja-cent or separate) that show a conventionalizedmeaning.
From English-French parallel corpora,Melamed?s method induces and compares pairs oftranslation models.
Models that take into accountnon-compositional compounds are highly accuratein the identification task.2 Data and resourcesWe base our investigations on the Europarl corpusconsisting of several years of proceedings from theEuropean Parliament (Koehn, 2003).
We focus onDutch expressions and their translations into Eng-lish, Spanish and German.2 Thus, we used the en-tire sections of Europarl in these three languages.The corpus has been tokenized and aligned at thesentence level (Tiedemann and Nygaard, 2004).The Dutch part contains about 29 million tokensin about 1.2 million sentences.
The English, Span-ish and German counterparts are of similar sizebetween 28 and 30 million words in roughly thesame number of sentences.Automatic word alignment has been done us-ing GIZA++ (Och, 2003).
We used standard set-tings of the system to produce Viterbi alignmentsof IBM model 4.
Alignments have been producedfor both translation directions (source to target andtarget to source) on tokenized plain text.3 We alsoused a well-known heuristics for combining thetwo directional alignments, the so-called refinedalignment (Och et al, 1999).
Word-to-word align-ments have been merged such that words are con-nected with each other if they are linked to thesame target.
In this way we obtained three differ-ent word alignment files: source to target (src2trg)with possible multi-word units in the source lan-guage, target to source (trg2src) with possiblemulti-word units in the target language, and re-fined with possible multi-word units in both lan-guages.
We also created bilingual word type linksfrom the different word-aligned corpora.
Theselists include alignment frequencies that we willuse later on for extracting default alignments forindividual words.
Henceforth, we will call themlink lexica.2This is only a restriction for our investigation but not forthe approach itself.3Manual corrections and evaluations of the tokenization,sentence and word alignment have not been done.
We relyentirely on the results of automatic processes.3 Extracting candidates from corporaThe Dutch section from the Europarl corpus wasautomatically parsed with Alpino, a Dutch wide-coverage parser.4 1.25% of the sentences couldnot be parsed by Alpino, given the fact that manysentences are rather lengthy.
We selected thosesentences in the Dutch Europarl section that con-tain at least one of a group of verbs that canfunction as main or support verbs.
Support verbsare prone to lexicalization or idiomatization alongwith their complementation (Butt, 2003).
The se-lected verbs are: doen, gaan, geven, hebben, ko-men, maken, nemen, brengen, houden, krijgen,stellen and zitten.5A fully parsed sentence is represented by the listof its dependency triples.
From the dependencytriples, each main verb is tallied with every de-pendent prepositional phrase (PP).
In this way, wecollected all the VERB PP tuples found in the selec-ted documents.
To avoid data sparseness, the NPinside the PP is reduced to the head noun?s lemmaand verbs are lemmatized, too.
Other potentialarguments under a verb phrase node are ignored.A sample of more than 191,000 candidates types(413,000 tokens) was collected.
To ensure statist-ical significance, the types that occur less than 50times were ignored.For each candidate triple, the log-likelihood(Dunning, 1993) and salience (Kilgarriff and Tug-well, 2001) scores were calculated.
These scoreshave been shown to perform reasonably well inidentifying collocations and other lexicalized ex-pressions (Villada Moiro?n, 2005).
In addition, thehead dependence between each PP in the candid-ates dataset and its selecting verbs was measured.Merlo and Leybold (2001) used the head depend-ence as a diagnostic to determine the argument(or adjunct) status of a PP.
The head dependenceis measured as the amount of entropy observedamong the co-occurring verbs for a given PP assuggested in (Merlo and Leybold, 2001; Bald-win, 2005).
Using the two association measuresand the head dependence heuristic, three differentrankings of the candidate triples were produced.The three different ranks assigned to each triplewere uniformly combined to form the final rank-ing.
From this list, we selected the top 200 triples4Available at http://www.let.rug.nl/?vannoord/alp/Alpino.5Butt (2003) maintains that the first 7 verbs are examplesof support verbs crosslinguistically.
The other 5 have beensuggested for Dutch by (Hollebrandse, 1993).35which we considered a manageable size to test ourmethod.4 MethodologyWe examine how expressions in the source lan-guage (Dutch) are conceptualized in a target lan-guage.
The translations in the target language en-code the meaning of the expression in the sourcelanguage.
Using the translation links in paral-lel corpora, we attempt to establish what type ofmeaning the expression in the source languagehas.
To accomplish this we make use of the threeword-aligned parallel corpora from Europarl asdescribed in section 2.Once the translation links of each expression inthe source language have been collected, the en-tropy observed among the translation links is com-puted per expression.
We also take into accounthow often the translation of an expression is madeout of the default alignment for each triple com-ponent.
The default ?translation?
is extracted fromthe corresponding bilingual link lexicon.4.1 Collecting alignmentsFor each triple in the source language (Dutch)we collect its corresponding (hypothetical) trans-lations in a target language.
Thus, we have a listof 200 VERB PP triples representing 200 potentialMWEs in Dutch.
We selected all occurrences ofeach triple in the source language and all alignedsentences containing their corresponding transla-tions into English, German and Spanish.
We re-stricted ourselves to instances found in 1:1 sen-tence alignments.
Other units contain many er-rors in word and sentence alignment and, there-fore, we discarded them.
Relying on automatedword-alignment, we collect all translation links foreach verb, preposition and noun occurrence withinthe triple context in the three target languages.To capture the meaning of a source expression(triple) S, we collect all the translation links of itscomponent words s in each target language.
Thus,for each triple, we gather three lists of transla-tion links Ts.
Let us see the example AAN LICHTBRENG representing the MWE iets aan het lichtbrengen ?reveal?.
Table 1 shows some of the linksfound for the triple AAN LICHT BRENG.
If a wordin the source language has no link in the target lan-guage (which is usually due to alignments to theempty word), NO LINK is assigned.Note that Dutch word order is more flexible thanTriple Links in Englishaan NO LINK, to, of, in, for, from, on, into, atlicht NO LINK, light, revealed, exposed, highlight,shown, shed light, clarifybreng NO LINK, brought, bring, highlighted,has, is, makesTable 1: Excerpt of the English links found for thetriple AAN LICHT BRENG ?bring to light?.English word order and that, the PP argument in acandidate expression may be separate from its se-lecting verb by any number of constituents.
Thisintroduces much noise during retrieving transla-tion links.
In addition, it is known that conceptsmay be lexicalized very differently in differentlanguages.
Because of this, words in the sourcelanguage may translate to nothing in a target lan-guage.
This introduces many mappings of a wordto NO LINK.4.2 Measuring translational entropyAccording to our intuition it is harder to alignwords in idiomatic expressions than other words.Thus, we expect a larger variety of links (includ-ing erroneous alignments) for words in such ex-pressions than for words taken from expressionswith a more literal meaning.
For the latter, weexpect fewer alignment candidates, possibly withonly one dominant default translation.
Entropyis a good measure for the unpredictability of anevent.
We like to use this measure for comparingthe alignment of our candidates and expect a highaverage entropy for idiomatic expressions.
In thisway we approximate a measure for meaning pre-dictability.For each word in a triple, we compute the en-tropy of the aligned target words as shown in equa-tion (1).H(Ts|s) = ?
?t?TsP (t|s)logP (t|s) (1)This measure is equivalent to translational en-tropy (Melamed, 1997b).
P (t|s) is estimated asthe proportion of alignment t among all align-ments of word s found in the corpus in the con-text of the given triple.6 Finally, the translationalentropy of a triple is the average translational en-tropy of its components.
It is unclear how to6Note that we also consider cases where s is part of analigned multi-word unit.36treat NO LINKS.
Thus, we experiment with threevariants of entropy: (1) leaving out NO LINKS,(2) counting NO LINKS as multiple types and (3)counting all NO LINKS as one unique type.4.3 Proportion of default alignments (pda)If an expression has a literal meaning, we expectthe default alignments to be accurate literal trans-lations.
If an expression has idiomatic meaning,the default alignments will be very different fromthe links observed in the translations.For each triple S, we count how often each ofits components s is linked to one of the defaultalignments Ds.
For the latter, we used the fourmost frequent alignment types extracted from thecorresponding link lexicon as described in section2.
A large proportion of default alignments7 sug-gests that the expression is very likely to have lit-eral meaning; a low percentage is suggestive ofnon-transparent meaning.
Formally, pda is calcu-lated in the following way:pda(S) =?s?S?d?Ds align freq(s, d)?s?S?t?Ts align freq(s, t)(2)where align freq(s, t) is the alignment fre-quency of word s to word t in the context of thetriple S.5 Discussion of experiments and resultsWe experimented with the three word-alignmenttypes (src2trg, trg2src and refined) and the twoscoring methods (entropy and pda).
The 200 can-didate MWEs have been assessed and classifiedinto idiomatic or literal expressions by a humanexpert.
For assessing performance, standard pre-cision and recall are not applicable in our case be-cause we do not want to define an artificial cut-off for our ranked list but evaluate the ranking it-self.
Instead, we measured the performance ofeach alignment type and scoring method by ob-taining another evaluation metric employed in in-formation retrieval, uninterpolated average preci-sion (uap), that aggregates precision points intoone evaluation figure.
At each point c where a truepositive Sc in the retrieved list is found, the pre-cision P (S1..Sc) is computed and, all precisionpoints are then averaged (Manning and Schu?tze,1999).7Note that we take NO LINKS into account when comput-ing the proportions.uap =?Sc P (S1..Sc)|Sc|(3)We used the initial ranking of our candidatesas baseline.
Our list of potential MWEs shows anoverall precision of 0.64 and an uap of 0.755.5.1 Comparing word alignment typesTable 2 summarizes the results of using the en-tropy measure (leaving out NO LINKS) with thethree alignment types for the NL-EN languagepair.8Alignment uapsrc2trg 0.864trg2src 0.785refined 0.765baseline 0.755Table 2: uap values of various alignments.Using word alignments improves the rankingof candidates in all three cases.
Among them,src2trg shows the best performance.
This issurprising because the quality of word-alignmentfrom English-to-Dutch (trg2src) in general ishigher due to differences in compounding in thetwo languages.
However, this is mainly an issuefor noun phrases which make up only one com-ponent in the triples.We assume that src2trg works better in our casebecause in this alignment model we explicitly linkeach word in the source language to exactly onetarget word (or the empty word) whereas in thetrg2src model we often get multiple words (in thetarget language) aligned to individual words in thetriple.
Many errors are introduced in such align-ment units.
Table 3 illustrates this with an examplewith links for the Dutch triple op prijs stel corres-ponding to the expression iets op prijs stellen ?toappreciate sth.
?src2trg trg2srcsource target target sourcegesteld appreciate NO LINK stellenprijs appreciate much appreciate indeed prijsop appreciate NO LINK opgesteld be keenly appreciate stellenprijs delighted fact prijsop NO LINK NO LINK opTable 3: Example src2trg and trg2src alignmentsfor the triple OP PRIJS STEL.8The performance of the three alignment types remainsuniform across all chosen language pairs.37src2trg alignment proposes appreciate as a linkto all three triple components.
This type of align-ment is not possible in trg2src.
Instead, trg2src in-cludes two NO LINKS in the first example in table3.
Furthermore, we get several multiword-units inthe target language linked to the triple compon-ents also because of alignment errors.
This way,we end up with many NO LINKS and many align-ment alternatives in trg2src that influence our en-tropy scores.
This can be observed for idiomaticexpressions as well as for literal expressions whichmakes translational entropy less reliable in trg2srcalignments for contrasting these two types of ex-pressions.The refined alignment model starts with the in-tersection of the two directional models and addsiteratively links if they meet some adjacency con-straints.
This results in many NO LINKS and alsoalignments with multiple words on both sides.This seems to have the same negative effect as inthe trg2src model.5.2 Comparing scoring metricsTable 4 offers a comparison of applying transla-tional entropy and the pda across the three lan-guage pairs.
To produce these results, src2trgalignment was used given that it reaches the bestperformance (refer to Table 2).Score NL-EN NL-ES NL-DEentropy- without NO LINKS 0.864 0.892 0.907- NO LINKS=many 0.858 0.890 0.883- NO LINKS=one 0.859 0.890 0.911pda 0.891 0.894 0.894baseline 0.755 0.755 0.755Table 4: Translational entropy and the pda acrossthree language pairs.
Alignment is src2trg.All scores produce better rankings than thebaseline.
In general, pda achieves a slightly betteraccuracy than entropy except for the NL-DE lan-guage pair.
Nevertheless, the difference betweenthe metrics is hardly significant.5.3 Further improvementsOne problem in our data is that we deal with word-form alignments and not with lemmatized ver-sions.
For Dutch, we know the lemma of eachword instance from our candidate set.
However,for the target languages, we only have access tosurface forms from the corpus.
Naturally, inflec-tional variations influence entropy scores (becauseof the larger variety of alignment types) and alsothe pda scores (where the exact wordforms have tobe matched with the default alignments instead oflemmas).
In order to test the effect of lemmatiz-ation on different language pairs, we used CELEX(Baayen et al, 1993) for English and German toreduce wordforms in the alignments and in the linklexicon to corresponding lemmas.
We assigned themost frequent lemma to ambiguous wordforms.Table 5 shows the scores obtained from applyinglemmatization for the src2trg alignment usingentropy (without NO LINKS) and pda.Setting NL-EN NL-ES NL-DEusing entropy scoreswith prepositionswordforms 0.864 0.892 0.907lemmas 0.873 ?
0.906without prepositionswordforms 0.906 0.923 0.932lemmas 0.910 ?
0.931using pda scoreswith prepositionswordforms 0.891 0.894 0.894lemmas 0.888 ?
0.903without prepositionswordforms 0.897 0.917 0.905lemmas 0.900 ?
0.910baseline 0.755 0.755 0.755Table 5: Translational entropy and pda fromsrc2trg alignments across languages pairs withdifferent settings.Surprisingly, lemmatization adds little or evendecreases the accuracy of the pda and entropyscores.
It is also surprising that lemmatizationdoes not affect the scores for morphologicallyricher languages such as German (compared toEnglish).
One possible reason for this is thatlemmatization discards morphological informa-tion that is crucial to identify idiomatic expres-sions.
In fact, nouns in idiomatic expressions aremore fixed than nouns in literal expressions.
Bycontrast, verbs in idiomatic expressions often al-low tense inflection.
By clustering wordforms intolemmas we lose this information.
In future work,we might lemmatize only the verb.Another issue is the reliability of the word align-ment that we base our investigation upon.
Wewant to make use of the fact that automatic wordalignment has problems with the alignment of in-dividual words that belong to larger lexical units.However, we believe that the alignment programin general has problems with highly ambiguouswords such as prepositions.
Therefore, preposi-38tions might blur the contrast between idiomatic ex-pressions and literal translations when measuredon the alignment of individual words.
Table 5includes scores for ranking our candidate expres-sions with and without prepositions.
We observethat there is a large improvement when leaving outthe alignments of prepositions.
This is consistentfor all language pairs and the scores we used forranking.rank pda entropy MWE triple1 9.80 8.3585 ok breng tot stand ?create?2 9.24 8.0923 ok breng naar voren ?bring up?3 16.40 7.8741 ok kom in aanmerking ?qualify?4 15.33 7.8426 ok kom tot stand ?come about?5 8.70 7.4973 ok stel aan orde ?bring under discussion?6 5.65 7.4661 ok ga te werk ?act unfairly?7 17.46 7.4057 ok kom aan bod ?get a chance?8 9.38 7.1762 ok ga van start ?proceed?9 14.15 7.1009 ok stel aan kaak ?expose?10 18.75 7.0321 ok breng op gang ?get going?11 13.00 6.9304 ok kom ten goede ?benefit?12 1.78 6.8715 ok neem voor rekening ?pay costs?13 20.99 6.7411 ok kom tot uiting ?manifest?14 1.41 6.7360 ok houd in stand ?preserve?15 0.81 6.6426 ok breng in kaart ?chart?16 16.71 6.5194 ok breng onder aandacht ?bring to attention?17 10.25 6.4893 ok neem onder loep ?scrutinize?18 7.83 6.4666 ok breng aan licht ?reveal?19 5.99 6.4049 ok roep in leven ?set up?20 15.89 6.3729 ok neem in aanmerking ?consider?...100 1.72 4.6940 ok leg aan band ?control?101 14.91 4.6884 ok houd voor gek ?pull s.o.
?s leg?102 23.56 4.6865 ok kom te weten ?find out?103 15.38 4.6713 ok neem in ontvangst ?receive?104 31.57 4.6556 * ga om waar ?go about where?105 35.95 4.6380 * houd met daar ?keep with there?106 34.86 4.6215 * ga om zaak ?go about issue?107 28.33 4.5846 ok kom tot overeenstemming ?come to terms?108 6.06 4.5715 ok breng in handel ?launch?109 35.62 4.5370 * ga om bedrag ?go about amount?110 22.58 4.5089 * blijk uit feit ?seems from fact?111 51.12 4.4063 ok ben van belang ?matter?112 49.69 4.3921 * ga om kwestie ?go about issue?113 23.61 4.3902 * voorzie in behoefte ?fill gap?114 16.18 4.3568 ok geef aan oproep ?make appeal?115 50.00 4.3254 * houd met aspect ?keep with aspect?116 40.91 4.3006 * houd aan regel ?adhere to rule?117 20.12 4.3002 * stel vast met voldoening ?settle with satisfaction?118 36.90 4.2931 ok kom tot akkoord ?reach agreement?119 36.49 4.2906 ok breng in stemming ?get in mood?120 14.06 4.2873 ok sta op schroeven ?be unsettled?...180 70.53 2.7395 * voldoe aan criterium ?satisfy criterion?181 52.33 2.7351 * beschik over informatie ?decide over information?182 74.71 2.6896 * stem voor amendement ?vote for amending?183 76.56 2.5883 * neem deel aan stemming ?participate in voting?184 30.26 2.4484 ok kan op aan ?be able to trust?185 68.89 2.3199 * zeg tegen heer ?tell a gentleman?186 45.00 2.1113 * verwijs terug naar commissie ?refer to comission?187 80.39 2.0992 * stem tegen amendement ?vote againsta amending?188 78.04 2.0924 * onthoud van stemming ?withhold one?s vote?189 77.63 1.9997 * feliciteer met werk ?congratulate with work?190 82.21 1.9020 * stem voor verslag ?vote for report?191 77.78 1.9016 * schep van werkgelegenheid ?set up of employment?192 86.36 1.8775 * stem voor resolutie ?vote for resolution ?193 73.33 1.8687 * bedank voor feit ?thank for fact?194 39.13 1.8497 * was wit van geld ?wash money?195 82.20 1.7944 * stem tegen verslag ?vote against report?196 80.49 1.6443 * schep van baan ?set up of job?197 86.17 1.4260 * stem tegen resolutie ?vote against resolution?198 85.56 1.1779 * dank voor antwoord ?thank for reply?199 90.55 1.0398 * ontvang overeenkomstig artikel ?receive similar article?200 87.88 1.0258 * recht van vrouw ?right of woman?Table 6: Rank (using entropy), entropy score, andpda score of 60 candidate MWEs.Table 6 provides an excerpt from the rankedlist of candidate triples.
The ranking has beendone using src2trg alignments from Dutch to Ger-man with the best setting (see table 5).
The scoreassigned by the pda metric is also shown.
Thecolumn labeled MWE states whether the expres-sion is idiomatic (?ok?)
or literal (?*?).
One issuethat emerges is whether we can find a thresholdvalue that splits candidate expressions into idio-matic and transparent ones.
One should choosesuch a threshold empirically however, it will de-pend on what level of precision is desirable andalso on the final application of the list.6 Conclusion and future workIn this paper we have shown that assessing auto-matic word alignment can help to identify idio-matic multi-word expressions.
We ranked candid-ates according to their link variability using trans-lational entropy and their link consistency withregards to default alignments.
For our experi-ments we used a set of 200 Dutch MWE candid-ates and word-aligned parallel corpora from Dutchto English, Spanish and German.
The MWE can-didates have been extracted using standard associ-ation measures and a head dependence heuristic.The word alignment has been done using standardmodels derived from statistical machine transla-tion.
Two measures were tested to re-rank the can-didates.
Translational entropy measures the pre-dictability of the translation of an expression bylooking at the links of its components to a targetlanguage.
Ranking our 200 MWE candidates us-ing entropy on Dutch to German word alignmentsimproved the baseline of 75.5% to 93.2% uninter-polated average precision (uap).
The proportion ofdefault alignments among the links found for MWEcomponents is another score we explored for rank-ing our MWE candidates.
Here, the accuracy israther similar giving us 91.7% while using the res-ults of a directional alignment model from Dutchto Spanish.
In general, we obtain slightly betterresults when using word alignment from Dutch toGerman and Spanish, compared to alignment fromDutch to English.There emerge several extensions of this workthat we wish to address in the future.
Alignmenttypes and scoring metrics need to be tested in lar-ger lists of randomly selected MWE candidates tosee if the results remain unaltered.
We also want toapply some weighting scheme by using the num-39ber of NO LINKS per expression.
Our assump-tion is that an expression with many NO LINKS isharder to translate compositionally, and probablyan idiomatic or ambiguous expression.
Altern-atively, an expression with no NO LINKS is verypredictable, thus a literal expression.
Finally, an-other possible improvement is combining severallanguage pairs.
There might be cases where idio-matic expressions are conceptualized in a similarway in two languages.
For example, a Dutch idio-matic expression with a cognate expression in Ger-man might be conceptualized in a different way inSpanish.
By combining the entropy or pda scoresfor NL-EN, NL-DE and NL-ES the accuracy mightimprove.AcknowledgmentsThis research was carried out as part of the re-search programs for IMIX, financed by NWO andthe IRME STEVIN project.
We would also liketo thank the three anonymous reviewers for theircomments on an earlier version of this paper.ReferencesR.H.
Baayen, R. Piepenbrock, and H. van Rijn.1993.
The CELEX lexical database (CD-ROM).
Linguistic Data Consortium, University ofPennsylvania,Philadelphia.Timothy Baldwin.
2005.
Looking for prepositionalverbs in corpus data.
In Proc.
of the 2nd ACL-SIGSEM Workshop on the Linguistic Dimensions ofPrepositions and their use in computational linguist-ics formalisms and applications, Colchester, UK.Colin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In Pro-ceedings of the 43th Annual Meeting of the ACL,pages 597?604, Ann Arbor.
University of Michigan.Miriam Butt.
2003.
The light verb jungle.http://ling.uni-konstanz.de/pages/home/butt/harvard-work.pdf.Ted Dunning.
1993.
Accurate methods for the stat-istics of surprise and coincidence.
Computationallinguistics, 19(1):61?74.Chitra Fernando and Roger Flavell.
1981.
On idiom.Critical views and perspectives, volume 5 of ExeterLinguistic Studies.
University of Exeter.Bart Hollebrandse.
1993.
Dutch light verb construc-tions.
Master?s thesis, Tilburg University, the Neth-erlands.K Imamura, E. Sumita, and Y. Matsumoto.
2003.Automatic construction of machine translationknowledge using translation literalness.
In Proceed-ings of the 10th EACL, pages 155?162, Budapest,Hungary.Adam Kilgarriff and David Tugwell.
2001.
Wordsketch: Extraction & display of significant colloc-ations for lexicography.
In Proceedings of the 39thACL & 10th EACL -workshop ?Collocation: Com-putational Extraction, Analysis and Explotation?,pages 32?38, Toulouse.Philipp Koehn.
2003.
Europarl: A multilin-gual corpus for evaluation of machine trans-lation.
unpublished draft, available fromhttp://people.csail.mit.edu/koehn/publications/europarl/.Christopher D. Manning and Hinrich Schu?tze.
1999.Foundations of Statistical Natural Language Pro-cessing.
The MIT Press, Cambridge, Massachu-setts.I.
Dan Melamed.
1997a.
Automatic discovery of non-compositional compounds in parallel data.
In 2ndConference on Empirical Methods in Natural Lan-guage Processing (EMNLP?97), Providence, RI.I.
Dan Melamed.
1997b.
Measuring semantic entropy.In ACL-SIGLEX Workshop Tagging Text with Lex-ical Semantics: Why, What and How, pages 41?46,Washington.Paola Merlo and Matthias Leybold.
2001.
Automaticdistinction of arguments and modifiers: the case ofprepositional phrases.
In Procs of the Fifth Com-putational Natural Language Learning Workshop(CoNLL?2001), pages 121?128, Toulouse.
France.Rosamund Moon.
1998.
Fixed expressions and Idiomsin English.
A corpus-based approach.
ClarendomPress, Oxford.Franz Josef Och, Christoph Tillmann, and HermannNey.
1999.
Improved alignment models for statist-ical machine translation.
In Proceedings of the JointSIGDAT Conference on Empirical Methods in Nat-ural Language Processing and Very Large Corpora(EMNLP/VLC), pages 20?28, University of Mary-land, MD, USA.Franz Josef Och.
2003.
GIZA++: Training ofstatistical translation models.
Available fromhttp://www.isi.edu/?och/GIZA++.html.Ivan Sag, T. Baldwin, F. Bond, A. Copestake, andD.
Flickinger.
2001.
Multiword expressions: a painin the neck for NLP.
LinGO Working Paper No.2001-03.Jo?rg Tiedemann and Lars Nygaard.
2004.
The OPUScorpus - parallel & free.
In Proceedings of theFourth International Conference on Language Re-sources and Evaluation (LREC?04), Lisbon, Por-tugal.Begon?a Villada Moiro?n.
2005.
Data-driven Identi-fication of fixed expressions and their modifiability.Ph.D.
thesis, University of Groningen.40
