Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 107?117,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsRationalizing Neural PredictionsTao Lei, Regina Barzilay and Tommi JaakkolaComputer Science and Artificial Intelligence LaboratoryMassachusetts Institute of Technology{taolei, regina, tommi}@csail.mit.eduAbstractPrediction without justification has limited ap-plicability.
As a remedy, we learn to extractpieces of input text as justifications ?
ratio-nales ?
that are tailored to be short and co-herent, yet sufficient for making the same pre-diction.
Our approach combines two modu-lar components, generator and encoder, whichare trained to operate well together.
The gen-erator specifies a distribution over text frag-ments as candidate rationales and these arepassed through the encoder for prediction.
Ra-tionales are never given during training.
In-stead, the model is regularized by desideratafor rationales.
We evaluate the approach onmulti-aspect sentiment analysis against manu-ally annotated test cases.
Our approach out-performs attention-based baseline by a signif-icant margin.
We also successfully illustratethe method on the question retrieval task.11 IntroductionMany recent advances in NLP problems have comefrom formulating and training expressive and elabo-rate neural models.
This includes models for senti-ment classification, parsing, and machine translationamong many others.
The gains in accuracy have,however, come at the cost of interpretability sincecomplex neural models offer little transparency con-cerning their inner workings.
In many applications,such as medicine, predictions are used to drive criti-cal decisions, including treatment options.
It is nec-essary in such cases to be able to verify and under-1Our code and data are available at https://github.com/taolei87/rcnn.the	beer	was	n?t	what	i	expected,	and	i?m	not	sure	it?s	?trueto	 style?,	 but	 i	 thought	 it	 was	 delicious.
a	 very	 pleasantruby	red-amber	color	with	a	rela9vely	brilliant	finish,	but	alimited	amount	of	carbona9on,	from	the	look	of	it.
aroma	iswhat	 i	 think	 an	 amber	 ale	 should	 be	 -	 a	 nice	 blend	 ofcaramel	and	happiness	bound	together.ReviewRatingsLook: 5 stars Smell: 4 starsFigure 1: An example of a review with ranking in two cate-gories.
The rationale for Look prediction is shown in bold.stand the underlying basis for the decisions.
Ide-ally, complex neural models would not only yieldimproved performance but would also offer inter-pretable justifications ?
rationales ?
for their predic-tions.In this paper, we propose a novel approach to in-corporating rationale generation as an integral partof the overall learning problem.
We limit ourselvesto extractive (as opposed to abstractive) rationales.From this perspective, our rationales are simply sub-sets of the words from the input text that satisfy twokey properties.
First, the selected words representshort and coherent pieces of text (e.g., phrases) and,second, the selected words must alone suffice forprediction as a substitute of the original text.
Moreconcretely, consider the task of multi-aspect senti-ment analysis.
Figure 1 illustrates a product reviewalong with user rating in terms of two categories oraspects.
If the model in this case predicts five starrating for color, it should also identify the phrase ?avery pleasant ruby red-amber color?
as the rationaleunderlying this decision.In most practical applications, rationale genera-107tion must be learned entirely in an unsupervisedmanner.
We therefore assume that our model withrationales is trained on the same data as the origi-nal neural models, without access to additional ra-tionale annotations.
In other words, target rationalesare never provided during training; the intermedi-ate step of rationale generation is guided only by thetwo desiderata discussed above.
Our model is com-posed of two modular components that we call thegenerator and the encoder.
Our generator specifies adistribution over possible rationales (extracted text)and the encoder maps any such text to task specifictarget values.
They are trained jointly to minimizea cost function that favors short, concise rationaleswhile enforcing that the rationales alone suffice foraccurate prediction.The notion of what counts as a rationale may beambiguous in some contexts and the task of select-ing rationales may therefore be challenging to eval-uate.
We focus on two domains where ambiguityis minimal (or can be minimized).
The first sce-nario concerns with multi-aspect sentiment analysisexemplified by the beer review corpus (McAuley etal., 2012).
A smaller test set in this corpus iden-tifies, for each aspect, the sentence(s) that relate tothis aspect.
We can therefore directly evaluate ourpredictions on the sentence level with the caveat thatour model makes selections on a finer level, in termsof words, not complete sentences.
The second sce-nario concerns with the problem of retrieving similarquestions.
The extracted rationales should capturethe main purpose of the questions.
We can thereforeevaluate the quality of rationales as a compressedproxy for the full text in terms of retrieval perfor-mance.
Our model achieves high performance onboth tasks.
For instance, on the sentiment predic-tion task, our model achieves extraction accuracy of96%, as compared to 38% and 81% obtained by thebigram SVM and a neural attention baseline.2 Related WorkDeveloping sparse interpretable models is of con-siderable interest to the broader research commu-nity(Letham et al, 2015; Kim et al, 2015).
The needfor interpretability is even more pronounced withrecent neural models.
Efforts in this area includeanalyzing and visualizing state activation (Hermansand Schrauwen, 2013; Karpathy et al, 2015; Li etal., 2016), learning sparse interpretable word vec-tors (Faruqui et al, 2015b), and linking word vectorsto semantic lexicons or word properties (Faruqui etal., 2015a; Herbelot and Vecchi, 2015).Beyond learning to understand or further con-strain the network to be directly interpretable, onecan estimate interpretable proxies that approximatethe network.
Examples include extracting ?if-then?rules (Thrun, 1995) and decision trees (Cravenand Shavlik, 1996) from trained networks.
Morerecently, Ribeiro et al (2016) propose a model-agnostic framework where the proxy model islearned only for the target sample (and its neighbor-hood) thus ensuring locally valid approximations.Our work differs from these both in terms of what ismeant by an explanation and how they are derived.In our case, an explanation consists of a concise yetsufficient portion of the text where the mechanismof selection is learned jointly with the predictor.Attention based models offer another means to ex-plicate the inner workings of neural models (Bah-danau et al, 2015; Cheng et al, 2016; Martinsand Astudillo, 2016; Chen et al, 2015; Xu andSaenko, 2015; Yang et al, 2015).
Such models havebeen successfully applied to many NLP problems,improving both prediction accuracy as well as vi-sualization and interpretability (Rush et al, 2015;Rockta?schel et al, 2016; Hermann et al, 2015).Xu et al (2015) introduced a stochastic attentionmechanism together with a more standard soft at-tention on image captioning task.
Our rationale ex-traction can be understood as a type of stochasticattention although architectures and objectives dif-fer.
Moreover, we compartmentalize rationale gen-eration from downstream encoding so as to exposeknobs to directly control types of rationales that areacceptable, and to facilitate broader modular use inother applications.Finally, we contrast our work with rationale-basedclassification (Zaidan et al, 2007; Marshall et al,2015; Zhang et al, 2016) which seek to improve pre-diction by relying on richer annotations in the formof human-provided rationales.
In our work, ratio-nales are never given during training.
The goal is tolearn to generate them.1083 Extractive Rationale GenerationWe formalize here the task of extractive rationalegeneration and illustrate it in the context of neuralmodels.
To this end, consider a typical NLP taskwhere we are provided with a sequence of wordsas input, namely x = {x1, ?
?
?
, xl}, where eachxt ?
Rd denotes the vector representation of the i-th word.
The learning problem is to map the inputsequence x to a target vector in Rm.
For example,in multi-aspect sentiment analysis each coordinateof the target vector represents the response or rat-ing pertaining to the associated aspect.
In text re-trieval, on the other hand, the target vectors are usedto induce similarity assessments between input se-quences.
Broadly speaking, we can solve the associ-ated learning problem by estimating a complex pa-rameterized mapping enc(x) from input sequencesto target vectors.
We call this mapping an encoder.The training signal for these vectors is obtained ei-ther directly (e.g., multi-sentiment analysis) or viasimilarities (e.g., text retrieval).
The challenge isthat a complex neural encoder enc(x) reveals lit-tle about its internal workings and thus offers littlein the way of justification for why a particular pre-diction was made.In extractive rationale generation, our goal is toselect a subset of the input sequence as a rationale.In order for the subset to qualify as a rationale itshould satisfy two criteria: 1) the selected wordsshould be interpretable and 2) they ought to sufficeto reach nearly the same prediction (target vector)as the original input.
In other words, a rationalemust be short and sufficient.
We will assume thata short selection is interpretable and focus on opti-mizing sufficiency under cardinality constraints.We encapsulate the selection of words as a ratio-nale generator which is another parameterized map-ping gen(x) from input sequences to shorter se-quences of words.
Thus gen(x) must include only afew words and enc(gen(x)) should result in nearlythe same target vector as the original input passedthrough the encoder or enc(x).
We can think of thegenerator as a tagging model where each word in theinput receives a binary tag pertaining to whether it isselected to be included in the rationale.
In our case,the generator is probabilistic and specifies a distri-bution over possible selections.The rationale generation task is entirely unsuper-vised in the sense that we assume no explicit anno-tations about which words should be included in therationale.
Put another way, the rationale is intro-duced as a latent variable, a constraint that guideshow to interpret the input sequence.
The encoderand generator are trained jointly, in an end-to-endfashion so as to function well together.4 Encoder and GeneratorWe use multi-aspect sentiment prediction as a guid-ing example to instantiate the two key components ?the encoder and the generator.
The framework itselfgeneralizes to other tasks.Encoder enc(?
): Given a training instance (x,y)where x = {xt}lt=1 is the input text sequence oflength l and y ?
[0, 1]m is the target m-dimensionalsentiment vector, the neural encoder predicts y?
=enc(x).
If trained on its own, the encoder wouldaim to minimize the discrepancy between the pre-dicted sentiment vector y?
and the gold target vectory.
We will use the squared error (i.e.
L2 distance)as the sentiment loss function,L(x,y) = ?y?
?
y?22 = ?enc(x)?
y?22The encoder could be realized in many ways suchas a recurrent neural network.
For example, letht = fe(xt,ht?1) denote a parameterized recurrentunit mapping input word xt and previous state ht?1to next state ht.
The target vector is then generatedon the basis of the final state reached by the recur-rent unit after processing all the words in the inputsequence.
Specifically,ht = fe(xt,ht?1), t = 1, .
.
.
, ly?
= ?e(Wehl + be)Generator gen(?
): The rationale generator ex-tracts a subset of text from the original input x tofunction as an interpretable summary.
Thus the ra-tionale for a given sequence x can be equivalentlydefined in terms of binary variables {z1, ?
?
?
, zl}where each zt ?
0, 1 indicates whether word xt isselected or not.
From here on, we will use z tospecify the binary selections and thus (z,x) is theactual rationale generated (selections, input).
Wewill use generator gen(x) as synonymous with a109probability distribution over binary selections, i.e.,z ?
gen(x) ?
p(z|x) where the length of z varieswith the input x.In a simple generator, the probability that the tthword is selected can be assumed to be conditionallyindependent from other selections given the input x.That is, the joint probability p(z|x) factors accord-ing top(z|x) =l?t=1p(zt|x) (independent selection)The component distributions p(zt|x) can be mod-eled using a shared bi-directional recurrent neuralnetwork.
Specifically, let ?
?f () and ?
?f () be the for-ward and backward recurrent unit, respectively, then?
?ht =?
?f (xt,???ht?1)?
?ht =?
?f (xt,??
?ht+1)p(zt|x) = ?z(Wz[??ht;?
?ht] + bz)Independent but context dependent selection ofwords is often sufficient.
However, the model is un-able to select phrases or refrain from selecting thesame word again if already chosen.
To this end, wealso introduce a dependent selection of words,p(z|x) =l?t=1p(zt|x, z1 ?
?
?
zt?1)which can be also expressed as a recurrent neuralnetwork.
To this end, we introduce another hiddenstate st whose role is to couple the selections.
Forexample,p(zt|x, z1,t?1) = ?z(Wz[??ht;?
?ht; st?1] + bz)st = fz([??ht;?
?ht; zt], st?1)Joint objective: A rationale in our definition cor-responds to the selected words, i.e., {xk|zk = 1}.We will use (z,x) as the shorthand for this rationaleand, thus, enc(z,x) refers to the target vector ob-tained by applying the encoder to the rationale as theinput.
Our goal here is to formalize how the ratio-nale can be made short and meaningful yet functionwell in conjunction with the encoder.
Our generatorand encoder are learned jointly to interact well butthey are treated as independent units for modularity.The generator is guided in two ways during learn-ing.
First, the rationale that it produces must sufficeas a replacement for the input text.
In other words,the target vector (sentiment) arising from the ratio-nale should be close to the gold sentiment.
The cor-responding loss function is given byL(z,x,y) = ?enc(z,x)?
y?22Note that the loss function depends directly (para-metrically) on the encoder but only indirectly on thegenerator via the sampled selection.Second, we must guide the generator to realizeshort and coherent rationales.
It should select only afew words and those selections should form phrases(consecutive words) rather than represent isolated,disconnected words.
We therefore introduce an ad-ditional regularizer over the selections?
(z) = ?1?z?+ ?2?t|zt ?
zt?1|where the first term penalizes the number of selec-tions while the second one discourages transitions(encourages continuity of selections).
Note that thisregularizer also depends on the generator only indi-rectly via the selected rationale.
This is because itis easier to assess the rationale once produced ratherthan directly guide how it is obtained.Our final cost function is the combination of thetwo, cost(z,x,y) = L(z,x,y) + ?(z).
Since theselections are not provided during training, we min-imize the expected cost:min?e,?g?
(x,y)?DEz?gen(x) [cost(z,x,y)]where ?e and ?g denote the set of parameters of theencoder and generator, respectively, and D is thecollection of training instances.
Our joint objectiveencourages the generator to compress the input textinto coherent summaries that work well with the as-sociated encoder it is trained with.Minimizing the expected cost is challenging sinceit involves summing over all the possible choicesof rationales z.
This summation could potentiallybe made feasible with additional restrictive assump-tions about the generator and encoder.
However, weassume only that it is possible to efficiently samplefrom the generator.110Doubly stochastic gradient We now derive asampled approximation to the gradient of the ex-pected cost objective.
This sampled approxima-tion is obtained separately for each input text x soas to work well with an overall stochastic gradientmethod.
Consider therefore a training pair (x,y).For the parameters of the generator ?g,?Ez?gen(x) [cost(z,x,y)]?
?g=?zcost(z,x,y) ?
?p(z|x)?
?g=?zcost(z,x,y) ?
?p(z|x)?
?g ?p(z|x)p(z|x)Using the fact (log f(?))?
= f ?(?)/f(?
), we get?zcost(z,x,y) ?
?p(z|x)?
?g ?p(z|x)p(z|x)=?zcost(z,x,y) ?
?
log p(z|x)?
?g ?
p(z|x)= Ez?gen(x)[cost(z,x,y)?
log p(z|x)?
?g]The last term is the expected gradient where the ex-pectation is taken with respect to the generator dis-tribution over rationales z.
Therefore, we can simplysample a few rationales z from the generator gen(x)and use the resulting average gradient in an overallstochastic gradient method.
A sampled approxima-tion to the gradient with respect to the encoder pa-rameters ?e can be derived similarly,?Ez?gen(x) [cost(z,x,y)]??e=?z?cost(z,x,y)??e?
p(z|x)= Ez?gen(x)[?cost(z,x,y)?
?e]Choice of recurrent unit We employ recurrentconvolution (RCNN), a refinement of local-ngrambased convolution.
RCNN attempts to learn n-gramfeatures that are not necessarily consecutive, andaverage features in a dynamic (recurrent) fashion.Specifically, for bigrams (filter width n = 2) RCNNcomputes ht = f(xt,ht?1) as followsNumber of reviews 1580kAvg length of review 144.9Avg correlation between aspects 63.5%Max correlation between two aspects 79.1%Number of annotated reviews 994Table 1: Statistics of the beer review dataset.
?t = ?
(W?xt + U?ht?1 + b?
)c(1)t = ?t  c(1)t?1 + (1?
?t) (W1xt)c(2)t = ?t  c(2)t?1 + (1?
?t) (c(1)t?1 + W2xt)ht = tanh(c(2)t + b)RCNN has been shown to work remarkably in clas-sification and retrieval applications (Lei et al, 2015;Lei et al, 2016) compared to other alternatives suchCNNs and LSTMs.
We use it for all the recurrentunits introduced in our model.5 ExperimentsWe evaluate the proposed joint model on two NLPapplications: (1) multi-aspect sentiment analysis onproduct reviews and (2) similar text retrieval onAskUbuntu question answering forum.5.1 Multi-aspect Sentiment AnalysisDataset We use the BeerAdvocate2 review datasetused in prior work (McAuley et al, 2012).3 Thisdataset contains 1.5 million reviews written by thewebsite users.
The reviews are naturally multi-aspect ?
each of them contains multiple sentencesdescribing the overall impression or one particu-lar aspect of a beer, including appearance, smell(aroma), palate and the taste.
In addition to the writ-ten text, the reviewer provides the ratings (on a scaleof 0 to 5 stars) for each aspect as well as an overallrating.
The ratings can be fractional (e.g.
3.5 stars),so we normalize the scores to [0, 1] and use them asthe (only) supervision for regression.McAuley et al (2012) also provided sentence-level annotations on around 1,000 reviews.
Eachsentence is annotated with one (or multiple) aspectlabel, indicating what aspect this sentence covers.2www.beeradvocate.com3http://snap.stanford.edu/data/web-BeerAdvocate.html111Method Appearance Smell Palate% precision % selected % precision % selected % precision % selectedSVM 38.3 13 21.6 7 24.9 7Attention model 80.6 13 88.4 7 65.3 7Generator (independent) 94.8 13 93.8 7 79.3 7Generator (recurrent) 96.3 14 95.1 7 80.2 7Table 2: Precision of selected rationales for the first three aspects.
The precision is evaluated based on whether the selected wordsare in the sentences describing the target aspect, based on the sentence-level annotations.
Best training epochs are selected basedon the objective value on the development set (no sentence annotation is used).D d l |?| MSESVM 260k - - 2.5M 0.0154SVM 1580k - - 7.3M 0.0100LSTM 260k 200 2 644k 0.0094RCNN 260k 200 2 323k 0.0087Table 3: Comparing neural encoders with bigram SVM model.MSE is the mean squared error on the test set.
D is the amountof data used for training and development.
d stands for the hid-den dimension, l denotes the depth of network and |?| denotesthe number of parameters (number of features for SVM).We use this set as our test set to evaluate the preci-sion of words in the extracted rationales.Table 1 shows several statistics of the beer reviewdataset.
The sentiment correlation between any pairof aspects (and the overall score) is quite high, get-ting 63.5% on average and a maximum of 79.1%(between the taste and overall score).
If directlytraining the model on this set, the model can be con-fused due to such strong correlation.
We thereforeperform a preprocessing step, picking ?less corre-lated?
examples from the dataset.4 This gives us ade-correlated subset for each aspect, each contain-ing about 80k to 90k reviews.
We use 10k as thedevelopment set.
We focus on three aspects sincethe fourth aspect taste still gets > 50% correlationwith the overall sentiment.Sentiment Prediction Before training the jointmodel, it is worth assessing the neural encoder sepa-rately to check how accurately the neural networkpredicts the sentiment.
To this end, we compareneural encoders with bigram SVM model, trainingmedium and large SVM models using 260k and all4Specifically, for each aspect we train a simple linear regres-sion model to predict the rating of this aspect given the ratingsof the other four aspects.
We then keep picking reviews withlargest prediction error until the sentiment correlation in the se-lected subset increases dramatically.0.0080.0100.0120.0140.0160% 25% 50% 75% 100%0.015SVM0.009EncoderFigure 2: Mean squared error of all aspects on the test set (y-axis) when various percentages of text are extracted as ratio-nales (x-axis).
220k training data is used.1580k reviews respectively.
As shown in Table 3,the recurrent neural network models outperform theSVM model for sentiment prediction and also re-quire less training data to achieve the performance.The LSTM and RCNN units obtain similar test er-ror, getting 0.0094 and 0.0087 mean squared errorrespectively.
The RCNN unit performs slightly bet-ter and uses less parameters.
Based on the results,we choose the RCNN encoder network with 2 stack-ing layers and 200 hidden states.To train the joint model, we also use RCNN unitwith 200 states as the forward and backward recur-rent unit for the generator gen().
The dependentgenerator has one additional recurrent layer.
For thislayer we use 30 states so the dependent version stillhas a number of parameters comparable to the inde-pendent version.
The two versions of the generatorhave 358k and 323k parameters respectively.Figure 2 shows the performance of our joint de-pendent model when trained to predict the sentimentof all aspects.
We vary the regularization ?1 and ?2to show various runs that extract different amount oftext as rationales.
Our joint model gets performanceclose to the best encoder run (with full text) whenfew words are extracted.112a	beer	that	is	not	sold	in	my	neck	of	the	woods	,	but	managed	to	get	while	on	a	roadtrip	.
poured	into	an	imperial	pint	glass	with	agenerous	head	that	sustained	life	throughout	.
nothing	out	of	the	ordinary	here	,	but	a	good	brew	s9ll	.
body	was	kind	of	heavy	,	butnot	thick	.
the	hop	smell	was	excellent	and	en9cing	.
very	drinkablevery	dark	beer	.
pours	a	nice	finger	and	a	half	of	creamy	foam	and	stays	throughout	the	beer	.
smells	of	coffee	and	roasted	malt	.
has	amajor	 coffee-like	 taste	with	hints	of	 chocolate	 .
if	 you	 like	black	 coffee	 ,	 you	will	 love	 this	 porter	 .
creamy	 smooth	mouthfeel	 anddefinitely	gets	smoother	on	the	palate	once	it	warms	.
it	's	an	ok	porter	but	i	feel	there	are	much	beAer	one	's	out	there	.poured	into	a	sniBer	.
produces	a	small	coffee	head	that	reduces	quickly	.
black	as	night	.
preAy	typical	imp	.
roasted	malts	hiton	 the	 nose	 .
a	 liAle	 sweet	 chocolate	 follows	 .
big	 toasty	 character	 on	 the	 taste	 .
in	 between	 i	 'm	 geDng	 plenty	 of	 darkchocolate	and	some	biAer	espresso	.
it	finishes	with	hop	biAerness	.
nice	smooth	mouthfeel	with	perfect	carbona9on	for	thestyle	.
overall	a	nice	stout	i	would	love	to	have	again	,	maybe	with	some	age	on	it	.i	really	did	not	like	this	.
it	just	seemed	extremely	watery	.
i	dont	'	think	this	had	any	carbona9on	whatsoever	.
maybe	it	was	flat	,	whoknows	?
but	even	if	i	got	a	bad	brew	i	do	n't	see	how	this	would	possibly	be	something	i	'd	get	9me	and	9me	again	.
i	could	taste	thehops	towards	the	middle	,	but	the	beer	got	preAy	nasty	towards	the	boAom	.
i	would	never	drink	this	again	,	unless	it	was	free	.
i	'mkind	of	upset	i	bought	this	.a	:	poured	a	nice	dark	brown	with	a	tan	colored	head	about	half	an	inch	thick	,	nice	red/garnet	accents	when	held	to	the	light	.
liAleclumps	of	lacing	all	around	the	glass	,	not	too	shabby	.
not	terribly	impressive	though	s	:	smells	like	a	more	guinness-y	guinness	really	,there	are	some	roasted	malts	there	,	signature	guinness	smells	,	less	burnt	though	,	a	liAle	bit	of	chocolate	?
?
m	:	rela9vely	thick	,	itis	n't	an	export	stout	or	imperial	stout	,	but	s9ll	is	preAy	heBy	in	the	mouth	,	very	smooth	,	not	much	carbona9on	.
not	too	shabby	d	:not	quite	as	drinkable	as	the	draught	,	but	s9ll	not	too	bad	.
i	could	easily	see	drinking	a	few	of	these	.Figure 3: Examples of extracted rationales indicating the sentiments of various aspects.
The extracted texts for appearance, smelland palate are shown in red, blue and green color respectively.
The last example is shortened for space.SVM Attention Gen (independent) Gen (recurrent)1 73.9 1 89.1 6 97.4 12 96.53 55.9 3 88.1 13 94.9 14 96.35 48.5 5 86.4 16 92.9 16 91.27 44.7 7 84.19 42.2 9 82.311 41.2 11 79.813 38.3 13 77.115 36.7 15 74.417 35.1 17 71.6304865831005 7 9 11 13 15 17SVMAttentionGen (independent)Gen (recurrent) 1Figure 4: Precision (y-axis) when various percentages of textare extracted as rationales (x-axis) for the appearance aspect.Rationale Selection To evaluate the supportingrationales for each aspect, we train the joint encoder-generator model on each de-correlated subset.
Weset the cardinality regularization ?1 between values{2e ?
4, 3e ?
4, 4e ?
4} so the extracted rationaletexts are neither too long nor too short.
For simplic-ity, we set ?2 = 2?1 to encourage local coherencyof the extraction.For comparison we use the bigram SVM modeland implement an attention-based neural networkmodel.
The SVM model successively extracts un-igram or bigram (from the test reviews) with thehighest feature.
The attention-based model learns anormalized attention vector of the input tokens (us-ing similarly the forward and backward RNNs), thenthe model averages over the encoder states accord-ingly to the attention, and feed the averaged vectorto the output layer.
Similar to the SVM model, theattention-based model can selects words based ontheir attention weights.0 50 1000.030.040.050.06Gen (recurrent)Gen (independent)0.20.40.60.81.0Figure 5: Learning curves of the optimized cost function on thedevelopment set and the precision of rationales on the test set.The smell (aroma) aspect is the target aspect.Table 2 presents the precision of the extracted ra-tionales calculated based on sentence-level aspectannotations.
The ?1 regularization hyper-parameteris tuned so the two versions of our model extractsimilar number of words as rationales.
The SVMand attention-based model are constrained similarlyfor comparison.
Figure 4 further shows the preci-sion when different amounts of text are extracted.Again, for our model this corresponds to changingthe ?1 regularization.
As shown in the table and thefigure, our encoder-generator networks extract textpieces describing the target aspect with high preci-sion, ranging from 80% to 96% across the three as-pects appearance, smell and palate.
The SVM base-line performs poorly, achieving around 30% accu-racy.
The attention-based model achieves reasonablebut worse performance than the rationale generator,suggesting the potential of directly modeling ratio-nales as explicit extraction.113Figure 5 shows the learning curves of our modelfor the smell aspect.
In the early training epochs,both the independent and (recurrent) dependent se-lection models fail to produce good rationales, get-ting low precision as a result.
After a few epochsof exploration however, the models start to achievehigh accuracy.
We observe that the dependent ver-sion learns more quickly in general, but both ver-sions obtain close results in the end.Finally we conduct a qualitative case study onthe extracted rationales.
Figure 3 presents severalreviews, with highlighted rationales predicted bythe model.
Our rationale generator identifies keyphrases or adjectives that indicate the sentiment ofa particular aspect.5.2 Similar Text Retrieval on QA ForumDataset For our second application, we usethe real-world AskUbuntu5 dataset used in recentwork (dos Santos et al, 2015; Lei et al, 2016).
Thisset contains a set of 167k unique questions (eachconsisting a question title and a body) and 16k user-identified similar question pairs.
Following previ-ous work, this data is used to train the neural en-coder that learns the vector representation of theinput question, optimizing the cosine distance (i.e.cosine similarity) between similar questions againstrandom non-similar ones.
We use the ?one-versus-all?
hinge loss (i.e.
positive versus other negatives)for the encoder, similar to (Lei et al, 2016).
Dur-ing development and testing, the model is used toscore 20 candidate questions given each query ques-tion, and a total of 400?20 query-candidate questionpairs are annotated for evaluation6.Task/Evaluation Setup The question descriptionsare often long and fraught with irrelevant details.
Inthis set-up, a fraction of the original question textshould be sufficient to represent its content, and beused for retrieving similar questions.
Therefore, wewill evaluate rationales based on the accuracy of thequestion retrieval task, assuming that better ratio-nales achieve higher performance.
To put this per-formance in context, we also report the accuracywhen full body of a question is used, as well as ti-tles alone.
The latter constitutes an upper bound on5askubuntu.com6https://github.com/taolei87/askubuntuMAP (dev) MAP (test) %wordsFull title 56.5 60.0 10.1Full body 54.2 53.0 89.9Independent 55.7 53.6 9.756.3 52.6 19.7Dependent 56.1 54.6 11.656.5 55.6 32.8Table 4: Comparison between rationale models (middle andbottom rows) and the baselines using full title or body (top row).Gen (independent) Gen (recurrent)0.052 47.08 0.063 50.540.058 52.36 0.067 49.480.059 46.02 0.07 51.960.062 49.76 0.078 51.540.064 47.94 0.086 52.550.068 48.93 0.095 53.590.07 49.5 0.108 53.150.081 52.18 0.112 51.480.081 51.84 0.116 54.620.094 51.24 0.121 52.120.094 52.21 0.137 530.097 53.61 0.163 53.20.098 54.11 0.179 54.130.122 49.03 0.193 52.110.133 54.19 0.262 52.320.135 50.21 0.277 50.870.136 48.22 0.328 53.210.145 50.96 0.328 55.610.155 52.91 0.347 510.173 52.74 0.378 54.930.197 52.645.047.850.553.356.05% 9% 13% 16% 20%Gen (independent)Gen (recurrent) 1Figure 6: Retrieval MAP on the test set when various percent-ages of the texts are chosen as rationales.
Data points corre-spond to models trained with different hyper-parameters.the model performance as in this dataset titles pro-vide short, informative summaries of the questioncontent.
We evaluate the rationales using the meanaverage precision (MAP) of retrieval.Results Table 4 presents the results of our ratio-nale model.
We explore a range of hyper-parametervalues7.
We include two runs for each version.
Thefirst one achieves the highest MAP on the develop-ment set, The second run is selected to compare themodels when they use roughly 10% of question text(7 words on average).
We also show the results ofdifferent runs in Figure 6.
The rationales achieve theMAP up to 56.5%, getting close to using the titles.The models also outperform the baseline of usingthe noisy question bodies, indicating the the models?capacity of extracting short but important fragments.Figure 7 shows the rationales for several questionsin the AskUbuntu domain, using the recurrent ver-sion with around 10% extraction.
Interestingly, themodel does not always select words from the ques-tion title.
The reasons are that the question bodycan contain the same or even complementary infor-mation useful for retrieval.
Indeed, some rationalefragments shown in the figure are error messages,7?1 ?
{.008, .01, .012, .015}, ?2 = {0, ?1, 2?1}, dropout?
{0.1, 0.2}114i	accidentally	removed	the	ubuntu	soBware	centre	,	when	i	was	actually	trying	to	remove	my	ubuntu	one	applica9ons	.
although	i	don't	remember	directly	uninstalling	the	centre	,	i	think	dele9ng		one	of	those	packages	might	have	triggered	it	.
i	can	not	look	at	historyof	applica9on	changes	,	as	the	soBware	centre	is	missing	.
please	advise	on	how	to	install	,	or	rather	reinstall	,	ubuntu	soBware	centreon	my	computer	.
how	do	i	install	ubuntu	soBware	centre	applica9on	?i	know	this	will	be	an	odd	ques9on	,	but	 i	was	wondering	 if	anyone	knew	how	to	 install	 the	ubuntu	 installer	package	 in	an	ubuntuinstalla9on	.
to	clarify	,	when	you	boot	up	to	an	ubuntu	livecd	,	it	's	got	the	installer	program	available	so	that	you	can	install	ubuntu	toa	drive	.
naturally	,	this	program	is	not	present	in	the	installed	ubuntu	.
is	there	,	though	,	a	way	to	download	and	install	it	like	otherpackages	?
invariably	,	someone	will	ask	what	i	'm	trying	to	do	,	and	the	answer	?
install	installer	package	on	an	installed	system	?what	is	the	easiest	way	to	install	all	the	media	codec	available	for	ubuntu	?
i	am	having	issues	with	mul9ple	applica9ons	promp9ngme	to	install	codecs	before	they	can	play	my	files	.
how	do	i		install	media	codecs	?what	should	i	do	when	i	see	<unk>	report	this	<unk>	?
an	unresolvable	problem	occurred	while	ini9alizing	the	package	informa9on	.please	report	this	bug	against	the	'update-manager	'	package	and	include	the	following	error	message	:	e	:	encountered	a	sec9on	withno	package	:	header	e	:	problem	with	mergelist	<unk>	e	:	the	package	lists	or	status	file	could	not	be	parsed	or	opened	.please	any	one	give	the	solu9on	for	this	whenever	i	try	to	convert	the	rpm	file	to	deb	file	i	always	get	this	problem	error	:	<unk>	:	notan	rpm	package	(	or	package	manifest	)	error	execu9ng	``	 	 lang=c	rpm	-qp	--	queryformat	%	{	name	}	<unk>	'	 ''	 :	at	<unk>	line	489thanks	conver9ng	rpm	file	to	debian	flehow	do	i	mount	a	hibernated	par99on	with	windows	8	in	ubuntu	?
i	ca	n't	mount	my	other	par99on	with	windows	8	,	i	have	ubuntu12.10	 amd64	 :	 error	 moun9ng	 /dev/sda1	 at	 <unk>	 :	 command-line	 `mount	 -t	 ``	 n[s	 ''	 -o	 ``	 uhelper=udisks2	 ,	 nodev	 ,	 nosuid	 ,uid=1000	 ,	 gid=1000	 ,	 dmask=0077	 ,	 fmask=0177	 ''	 ``	 /dev/sda1	 ''	 ``	 <unk>	 ''	 '	 exited	 with	 non-zero	 exit	 status	 14	 :	 windows	 ishibernated	,	refused	to	mount	.
failed	to	mount	'/dev/sda1	'	:	opera9on	not	permiAed	the	n[s	par99on	is	hibernated	.
please	resumeand	shutdown	windows	properly	,	or	mount	the	volume	read-only	with	the	'ro	'	mount	op9onFigure 7: Examples of extracted rationales of questions in the AskUbuntu domain.which are typically not in the titles but very usefulto identify similar questions.6 DiscussionWe proposed a novel modular neural frameworkto automatically generate concise yet sufficient textfragments to justify predictions made by neural net-works.
We demonstrated that our encoder-generatorframework, trained in an end-to-end manner, givesrise to quality rationales in the absence of any ex-plicit rationale annotations.
The approach could bemodified or ext nded in various ways to other appli-cations or types of data.Choices of enc(?)
and gen(?).
The encoder andgenerator can be realized in numerous ways with-out changing the broader algorithm.
For instance,we could use a convolutional network (Kim, 2014;Kalchbrenner et al, 2014), deep averaging net-work (Iyyer et al, 2015; Joulin et al, 2016) or aboosting classifier as the encoder.
When rationalescan be expected to conform to repeated stereotypi-cal patterns in the text, a simpler encoder consistentwith this bias can work better.
We emphasize that,in this paper, rationales are flexible explanations thatmay vary substantially from instance to another.
Onthe generator side, many additional constraints couldbe imposed to further guide acceptable rationales.Dealing with Search Space.
Our training methodemploys a REINFORCE-style algorithm (Williams,1992) where the gradient with respect to the param-eters is estimated by sampling possible rationales.Additional constraints on the generator output canbe helpful in alleviating problems of exploring po-tentially a large space of possible rationales in termsof their interaction with the encoder.
We could alsoapply variance reduction techniques to increase sta-bility of stochastic training (cf.
(Weaver and Tao,2001; Mni et al, 2014; Ba et al, 2015; Xu et al,2015)).7 AcknowledgmentsWe thank Prof. Julian McAuley for sharing the re-view dat s t and annotations.
We also th nk MITNLP group and the reviewers for their helpful com-ments.
The work is supported by the Arabic Lan-guage Technologies (ALT) group at Qatar Com-puting Research Institute (QCRI) within the IYASproject.
Any opinions, findings, conclusions, or rec-ommendations expressed in this paper are those ofthe authors, and do not necessarily reflect the viewsof the funding organizations.ReferencesJimmy Ba, Volodymyr Mnih, and Koray Kavukcuoglu.2015.
Multiple object recognition with visual atten-tion.
In Proceedings of the International Conferenceon Learning Representations (ICLR).Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2015.
Neural machine translation by jointlylearning to align and translate.
In International Con-ference on Learning Representations.Kan Chen, Jiang Wang, Liang-Chieh Chen, HaoyuanGao, Wei Xu, and Ram Nevatia.
2015.
Abc-115cnn: An attention based convolutional neural net-work for visual question answering.
arXiv preprintarXiv:1511.05960.Jianpeng Cheng, Li Dong, and Mirella Lapata.
2016.Long short-term memory-networks for machine read-ing.
arXiv preprint arXiv:1601.06733.Mark W Craven and Jude W Shavlik.
1996.
Extract-ing tree-structured representations of trained networks.In Advances in neural information processing systems(NIPS).Cicero dos Santos, Luciano Barbosa, Dasha Bogdanova,and Bianca Zadrozny.
2015.
Learning hybrid rep-resentations to retrieve semantically equivalent ques-tions.
In Proceedings of the 53rd Annual Meeting ofthe Association for Computational Linguistics and the7th International Joint Conference on Natural Lan-guage Processing (Volume 2: Short Papers), pages694?699, Beijing, China, July.
Association for Com-putational Linguistics.Manaal Faruqui, Jesse Dodge, Sujay K. Jauhar, ChrisDyer, Eduard Hovy, and Noah A. Smith.
2015a.Retrofitting word vectors to semantic lexicons.
In Pro-ceedings of NAACL.Manaal Faruqui, Yulia Tsvetkov, Dani Yogatama, ChrisDyer, and Noah A. Smith.
2015b.
Sparse overcom-plete word vector representations.
In Proceedings ofACL.Aure?lie Herbelot and Eva Maria Vecchi.
2015.
Build-ing a shared world: mapping distributional to model-theoretic semantic spaces.
In Proceedings of the 2015Conference on Empirical Methods in Natural Lan-guage Processing.
Association for Computational Lin-guistics.Karl Moritz Hermann, Tomas Kocisky, Edward Grefen-stette, Lasse Espeholt, Will Kay, Mustafa Suleyman,and Phil Blunsom.
2015.
Teaching machines to readand comprehend.
In Advances in Neural InformationProcessing Systems, pages 1684?1692.Michiel Hermans and Benjamin Schrauwen.
2013.Training and analysing deep recurrent neural net-works.
In Advances in Neural Information ProcessingSystems, pages 190?198.Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber,and Hal Daume?
III.
2015.
Deep unordered compo-sition rivals syntactic methods for text classification.In Proceedings of the 53rd Annual Meeting of the As-sociation for Computational Linguistics and the 7thInternational Joint Conference on Natural LanguageProcessing (Volume 1: Long Papers).Armand Joulin, Edouard Grave, Piotr Bojanowski, andTomas Mikolov.
2016.
Bag of tricks for efficient textclassification.
arXiv preprint arXiv:1607.01759.Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-som.
2014.
A convolutional neural network for mod-elling sentences.
In Proceedings of the 52th AnnualMeeting of the Association for Computational Linguis-tics.Andrej Karpathy, Justin Johnson, and Fei-Fei Li.
2015.Visualizing and understanding recurrent networks.arXiv preprint arXiv:1506.02078.B Kim, JA Shah, and F Doshi-Velez.
2015.
Mind thegap: A generative approach to interpretable feature se-lection and extraction.
In Advances in Neural Infor-mation Processing Systems.Yoon Kim.
2014.
Convolutional neural networks for sen-tence classification.
In Proceedings of the EmpiricialMethods in Natural Language Processing (EMNLP2014).Tao Lei, Regina Barzilay, and Tommi Jaakkola.
2015.Molding cnns for text: non-linear, non-consecutiveconvolutions.
In Proceedings of the 2015 Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP).Tao Lei, Hrishikesh Joshi, Regina Barzilay, TommiJaakkola, Katerina Tymoshenko, Alessandro Mos-chitti, and Llu?
?s Ma`rquez.
2016.
Semi-supervisedquestion retrieval with gated convolutions.
In Pro-ceedings of the 2016 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics: Human Language Technologies (NAACL).Benjamin Letham, Cynthia Rudin, Tyler H. McCormick,and David Madigan.
2015.
Interpretable classifiersusing rules and bayesian analysis: Building a betterstroke prediction model.
Annals of Applied Statistics,9(3):1350?1371.Jiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky.2016.
Visualizing and understanding neural models innlp.
In Proceedings of NAACL.Iain J Marshall, Joe?l Kuiper, and Byron C Wallace.
2015.Robotreviewer: evaluation of a system for automati-cally assessing bias in clinical trials.
Journal of theAmerican Medical Informatics Association.Andre?
F. T. Martins and Ramo?n Fernandez Astudillo.2016.
From softmax to sparsemax: A sparse modelof attention and multi-label classification.
CoRR,abs/1602.02068.Julian McAuley, Jure Leskovec, and Dan Jurafsky.
2012.Learning attitudes and attributes from multi-aspect re-views.
In Data Mining (ICDM), 2012 IEEE 12th In-ternational Conference on, pages 1020?1025.
IEEE.Volodymyr Mnih, Nicolas Heess, Alex Graves, et al2014.
Recurrent models of visual attention.
InAdvances in Neural Information Processing Systems(NIPS).116Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.2016.
?
why should i trust you??
: Explaining the pre-dictions of any classifier.
In ACM SIGKDD Interna-tional Conference on Knowledge Discovery and DataMining (KDD).Tim Rockta?schel, Edward Grefenstette, Karl Moritz Her-mann, Toma?s?
Koc?isky`, and Phil Blunsom.
2016.
Rea-soning about entailment with neural attention.
In In-ternational Conference on Learning Representations.Alexander M Rush, Sumit Chopra, and Jason Weston.2015.
A neural attention model for abstractive sen-tence summarization.
In Proceedings of the 2015 Con-ference on Empirical Methods in Natural LanguageProcessing.Sebastian Thrun.
1995.
Extracting rules from artifi-cial neural networks with distributed representations.In Advances in neural information processing systems(NIPS).Lex Weaver and Nigel Tao.
2001.
The optimal rewardbaseline for gradient-based reinforcement learning.
InProceedings of the Seventeenth conference on Uncer-tainty in artificial intelligence.Ronald J Williams.
1992.
Simple statistical gradient-following algorithms for connectionist reinforcementlearning.
Machine learning.Huijuan Xu and Kate Saenko.
2015.
Ask, attendand answer: Exploring question-guided spatial atten-tion for visual question answering.
arXiv preprintarXiv:1511.05234.Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,Aaron Courville, Ruslan Salakhudinov, Rich Zemel,and Yoshua Bengio.
2015.
Show, attend and tell: Neu-ral image caption generation with visual attention.
InProceedings of the 32nd International Conference onMachine Learning (ICML).Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng,and Alex Smola.
2015.
Stacked attention net-works for image question answering.
arXiv preprintarXiv:1511.02274.Omar Zaidan, Jason Eisner, and Christine D. Piatko.2007.
Using ?annotator rationales?
to improve ma-chine learning for text categorization.
In Proceedingsof Human Language Technology Conference of theNorth American Chapter of the Association of Com-putational Linguistics, pages 260?267.Ye Zhang, Iain James Marshall, and Byron C. Wallace.2016.
Rationale-augmented convolutional neural net-works for text classification.
CoRR, abs/1605.04469.117
