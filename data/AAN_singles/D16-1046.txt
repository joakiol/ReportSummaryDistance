Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 479?489,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsHow Transferable are Neural Networks in NLP Applications?Lili Mou,1 Zhao Meng,1 Rui Yan,2 Ge Li,1,?
Yan Xu,1,?
Lu Zhang,1 Zhi Jin1,?1Key Laboratory of High Confidence Software Technologies (Peking University), MoE, ChinaInstitute of Software, Peking University, China ?Corresponding authors2Insitute of Computer Science and Technology of Peking University, China{doublepower.mou,rui.yan.peking}@gmail.com,zhaomeng.pku@outlook.com{lige,xuyan14,zhanglu,zhijin}@sei.pku.edu.cnAbstractTransfer learning is aimed to make use ofvaluable knowledge in a source domain tohelp model performance in a target domain.It is particularly important to neural networks,which are very likely to be overfitting.
Insome fields like image processing, many stud-ies have shown the effectiveness of neuralnetwork-based transfer learning.
For neuralNLP, however, existing studies have only ca-sually applied transfer learning, and conclu-sions are inconsistent.
In this paper, we con-duct systematic case studies and provide anilluminating picture on the transferability ofneural networks in NLP.11 IntroductionTransfer learning, or sometimes known as domainadaptation,2 plays an important role in various nat-ural language processing (NLP) applications, espe-cially when we do not have large enough datasetsfor the task of interest (called the target task T ).
Insuch scenarios, we would like to transfer or adaptknowledge from other domains (called the sourcedomains/tasks S) so as to mitigate the problem ofoverfitting and to improve model performance inT .
For traditional feature-rich or kernel-based mod-els, researchers have developed a variety of ele-gant methods for domain adaptation; examples in-clude EasyAdapt (Daume?
III, 2007; Daume?
III et?Yan Xu is currently a research scientist at Inveno Co., Ltd.1Code released on https://sites.google.com/site/transfernlp/2In this paper, we do not distinguish the conceptual differ-ence between transfer learning and domain adaptation.
Do-main?in the sense we use throughout this paper?is defined bydatasets.al., 2010), instance weighting (Jiang and Zhai, 2007;Foster et al, 2010), and structural correspondencelearning (Blitzer et al, 2006; Prettenhofer and Stein,2010).Recently, deep neural networks are emerging asthe prevailing technical solution to almost everyfield in NLP.
Although capable of learning highlynonlinear features, deep neural networks are veryprone to overfitting, compared with traditional meth-ods.
Transfer learning therefore becomes even moreimportant.
Fortunately, neural networks can betrained in a transferable way by their incrementallearning nature: we can directly use trained (tuned)parameters from a source task to initialize the net-work in the target task; alternatively, we may alsotrain two tasks simultaneously with some parame-ters shared.
But their performance should be verifiedby empirical experiments.Existing studies have already shown some evi-dence of the transferability of neural features.
Forexample, in image processing, low-level neural lay-ers closely resemble Gabor filters or color blobs(Zeiler and Fergus, 2014; Krizhevsky et al, 2012);they can be transferred well to different tasks.
Don-ahue et al (2014) suggest that high-level layersare also transferable in general visual recognition;Yosinski et al (2014) further investigate the trans-ferability of neural layers in different levels of ab-straction.Although transfer learning is promising in imageprocessing, conclusions appear to be less clear inNLP applications.
Image pixels are low-level sig-nals, which are generally continuous and less relatedto semantics.
By contrast, natural language tokens479are discrete: each word well reflects the thoughtof humans, but neighboring words do not share asmuch information as pixels in images do.
Previ-ous neural NLP studies have casually applied trans-fer techniques, but their results are not consistent.Collobert and Weston (2008) apply multi-task learn-ing to SRL, NER, POS, and CHK,3 but obtain only0.04?0.21% error reduction4 (out of a base error rateof 16?18%).
Bowman et al (2015), on the contrary,improve a natural language inference task from anaccuracy of 71.3% to 80.8% by initializing parame-ters with an additional dataset of 550,000 samples.Therefore, more systematic studies are needed toshed light on transferring neural networks in the fieldof NLP.Our ContributionsIn this paper, we investigate the question ?Howtransferable are neural networks in NLP applica-tions?
?We distinguish two scenarios of transfer: (1)transferring knowledge to a semantically simi-lar/equivalent task but with a different dataset; (2)transferring knowledge to a task that is semanti-cally different but shares the same neural topol-ogy/architecture so that neural parameters can in-deed be transferred.
We further distinguish twotransfer methods: (1) using the parameters trainedon S to initialize T (INIT), and (2) multi-task learn-ing (MULT), i.e., training S and T simultaneously.
(Please see Sections 2 and 4).
Our study mainly fo-cuses on the following research questions:RQ1: How transferable are neural networks be-tween two tasks with similar or different se-mantics in NLP applications?RQ2: How transferable are different layers of NLPneural models?RQ3: How transferable are INIT and MULT, re-spectively?
What is the effect of combiningthese two methods?3The acronyms refer to semantic role labeling, named entityrecognition, part-of-speech tagging, and chunking, respectively.4Here, we quote the accuracies obtained by using unsuper-vised pretraining of word embeddings.
This is the highest per-formance in that paper; using pretrained word embeddings isalso a common practice in the literature.We conducted extensive experiments over sixdatasets on classifying sentences and sentence pairs.We leveraged the widely-used convolutional neu-ral network (CNN) and long short term memory(LSTM)-based recurrent neural network (RNN) asour models.Based on our experimental results, we have thefollowing main observations, some of which are un-expected.?
Whether a neural network is transferable inNLP depends largely on how semanticallysimilar the tasks are, which is different fromthe consensus in image processing.?
The output layer is mainly specific to thedataset and not transferable.
Word embed-dings are likely to be transferable to seman-tically different tasks.?
MULT and INIT appear to be generally com-parable to each other; combining these twomethods does not result in further gain in ourstudy.The rest of this paper is organized as follows.
Sec-tion 2 introduces the datasets that neural models aretransferred across; Section 3 details the neural archi-tectures and experimental settings.
We describe twoapproaches (INIT and MULT) to transfer learning inSection 4.
We present experimental results in Sec-tions 5?6 and have concluding remarks in Section 7.2 DatasetsIn our study, we conducted two series of experi-ments using six open datasets as follows.?
Experiment I: Sentence classification?
IMDB.
A large dataset for binary sentimentclassification (positive vs. negative).5?
MR. A small dataset for binary sentiment clas-sification.6?
QC.
A (small) dataset for 6-way questionclassification (e.g., location, time, andnumber).75https://drive.google.com/file/d/0B8yp1gOBCztyN0JaMDVoeXhHWm8/6https://www.cs.cornell.edu/people/pabo/movie-review-data/7http://cogcomp.cs.illinois.edu/Data/QA/QC/480Statistics (# of Samples)Experiment I Experiment IIIMDB MR QC SNLI SICK MSRP#Train 550,000 8,500 4,800 550,152 4,439 3,575#Val 50,000 1,100 600 10,000 495 501#Test 2,000 1,100 500 10,000 4,906 1,725Examples in Experiment ISentiment Analysis (IMDB and MR)An idealistic love story that brings out +the latent 15-year-old romantic in everyone.Its mysteries are transparently obvious, ?and its too slowly paced to be a thriller.Question Classification (QC)What is the temperature at the center of the earth?
numberWhat state did the Battle of Bighorn take place in?
locationExamples in Experiment IINatural Language Inference (SNLI and SICK)Premise Two men on bicycles competing in a race.People are riding bikes.
EHypothesis Men are riding bicycles on the streets.
CA few people are catching fish.
NParaphrase Detection (MSRP)The DVD-CCA then appealed to the stateParaphraseSupreme Court.The DVD CCA appealed that decisionto the U.S. Supreme Court.Earnings per share from recurring operationswill be 13 cents to 14 cents.
Non-That beat the company?s April earnings Paraphraseforecast of 8 to 9 cents a share.Table 1: Statistics and examples of the datasets.?
Experiment II: Sentence-pair classification?
SNLI.
A large dataset for sentence entail-ment recognition.
The classification objec-tives are entailment, contradiction,and neutral.8?
SICK.
A small dataset with exactly the sameclassification objective as SNLI.9?
MSRP.
A (small) dataset for paraphrase de-tection.
The objective is binary classification:judging whether two sentences have the samemeaning.10In each experiment, the large dataset serves as thesource domain and small ones are the target do-mains.
Table 1 presents statistics of the abovedatasets.We distinguish two scenarios of transfer regard-ing semantic similarity: (1) semantically equivalenttransfer (IMDB?MR, SNLI?SICK), that is, thetasks of S and T are defined by the same meaning,8http://nlp.stanford.edu/projects/snli/9http://http://alt.qcri.org/semeval2014/task1/10http://research.microsoft.com/en-us/downloads/and (2) semantically different transfer (IMDB?QC,SNLI?MSRP).
Examples are also illustrated in Ta-ble 1 to demonstrate semantic relatedness.It should be noticed that in image or speech pro-cessing (Yosinski et al, 2014; Wang and Zheng,2015), the input of neural networks pretty much con-sists of raw signals; hence, low-level feature detec-tors are almost always transferable, even if Yosinskiet al (2014) manually distinguish artificial objectsand natural ones in an image classification task.Distinguishing semantic relatedness?whichemerges from very low layers of either word em-beddings or the successive hidden layer?is specificto NLP and also a new insight of our paper.
Aswe shall see in Sections 5 and 6, the transferabilityof neural networks in NLP is more sensitive tosemantics than in image processing.3 Neural Models and SettingsIn each group, we used a single neural model tosolve three problems in a unified manner.
That isto say, the neural architecture is the same among thethree datasets, which makes it possible to investi-gate transfer learning regardless of whether the tasksare semantically equivalent.
Concretely, the neuralmodels are as follows.?
Experiment I: LSTM-RNN.
To classify asentence according to its sentiment or ques-tion type, we use a recurrent neural network(RNN, Figure 1a) with long short term mem-ory (LSTM) units (Hochreiter and Schmidhu-ber, 1997).
A softmax layer is added to the lastword?s hidden state for classification.?
Experiment II: CNN-pair.
In this group, weuse a ?Siamese?
architecture (Bromley et al,1993) to classify the relation of two sentences.We first apply a convolutional neural network(CNN, Figure 1b) with a window size of 5 tomodel local context, and a max pooling layergathers information to a fixed-size vector.
Thenthe sentence vectors are concatenated and fedto a hidden layer before the softmax output.In our experiments, embeddings were pretrainedby word2vec (Mikolov et al, 2013); all embed-dings and hidden layers were 100 dimensional.
We481twomenon.?.?.?peopleare.?.?.
?Convolution Max?poolingsoftmaxan????????idealistic???????
?EmbeddingOutput(a)(b)Hidden LSTM LSTM LSTMEmbedding????????????Hidden?layers????????????Outputsoftmax.?.?.
?00Figure 1: The models in our study.
(a) Experiment I: RNNswith LSTM units for sentence classification.
(b) Experiment II:CNN for sentence pair modeling.applied stochastic gradient descent with a mini-batch size of 50 for optimization.
In each setting, wetuned the hyperparameters as follows: learning ratefrom {3, 1, 0.3, 0.1, 0.03}, power decay of learningrate from {fast, moderate, low} (defined by howmuch, after one epoch, the learning rate residual is:0.1x, 0.3x, 0.9x, resp).
We regularized our networkby dropout with a rate from {0, 0.1, 0.2, 0.3}.
Notethat we might not run nonsensical settings, e.g., alarger dropout rate if the network has already beenunderfitting (i.e., accuracy has decreased when thedropout rate increases).
We report the test perfor-mance associated with the highest validation accu-racy.To setup a baseline, we trained our models with-out transfer 5 times by different random parameterinitializations (Table 2).
We have achieved reason-able performance that is comparable to similar mod-els reported in the literature with all six datasets.Therefore, our implementation is fair and suitablefor further study of transfer learning.It should be mentioned that the goal of this paperis not to outperform state-of-the-art results; instead,Dataset Avg acc.?std.
Related modelExp.I IMDB 87.0 89.3 (Non-NN, Dong+,2015)MR 75.1?
0.6 77.7 (RAE, Socher+, 2013)QC 90.8?
0.9 90.2 (RNN, Zhao+,2015)Exp.II SNLI 76.3 77.6 (RNN, Bowman+,2015)SICK 70.9?
1.3 71.3 (RNN, Bowman+,2015)MSRP 69.0?
0.5 69.6 (Arc-I CNN, Hu+,2014)Table 2: Accuracy (%) without transfer.
We also include re-lated models for comparison (Dong et al, 2015; Socher et al,2011; Zhao et al, 2015; Bowman et al, 2015; Hu et al, 2014),showing that we have achieved comparable results, and thus areready to investigate transfer learning.
The models were run oneonly once in source domains, because we could only transfer aparticular model instead of an average of several models.we would like to conduct a fair comparison of dif-ferent methods and settings for transfer learning inNLP.4 Transfer MethodsTransfer learning aims to use knowledge in a sourcedomain to aid the target domain.
As neural net-works are usually trained incrementally with gradi-ent descent (or variants), it is straightforward to usegradient information in both source and target do-mains for optimization so as to accomplish knowl-edge transfer.
Depending on how samples in sourceand target domains are scheduled, there are twomain approaches to neural network-based transferlearning:?
Parameter initialization (INIT).
The INIT ap-proach first trains the network on S, and then di-rectly uses the tuned parameters to initialize thenetwork for T .
After transfer, we may fix (?????
)the parameters in the target domain (Glorot et al,2011), i.e., no training is performed on T .
Butwhen labeled data are available in T , it would bebetter to fine-tune (1) the parameters.INIT is also related to unsupervised pretrainingsuch as word embedding learning (Mikolov et al,2013) and autoencoders (Bengio et al, 2006).
Inthese approaches, parameters that are (pre)trainedin an unsupervised way are transferred to initial-ize the model for a supervised task (Plank andMoschitti, 2013).
However, our paper focuses on?supervised pretraining,?
which means we trans-fer knowledge from a labeled source domain.482?
Multi-task learning (MULT).
MULT, on the otherhand, simultaneously trains samples in both do-mains (Collobert and Weston, 2008; Liu et al,2016).
The overall cost function is given byJ = ?JT + (1?
?
)JS (1)where JT and JS are the individual cost functionof each domain.
(Both JT and JS are normalizedby the number of training samples.)
?
?
(0, 1) isa hyperparameter balancing the two domains.It is nontrivial to optimize Equation 1 in practiceby gradient-based methods.
One may take the par-tial derivative of J and thus ?
goes to the learningrate (Liu et al, 2016), but the model is then vul-nerable because it is likely to blow up with largelearning rates (multiplied by ?
or 1 ?
?)
and bestuck in local optima with small ones.Collobert and Weston (2008) alternatively choosea data sample from either domain with a certainprobability (controlled by ?)
and take the deriva-tive for the particular data sample.
In this way, do-main transfer is independent of learning rates, butwe may not be able to fully use the entire datasetof S if ?
is large.
We adopted the latter approachin our experiment for simplicity.
(More in-depthanalysis may be needed in future work.)
Formally,our multi-task learning strategy is as follows.1 Switch to T with prob.
?, or to S withprob.
1?
?.2 Compute the gradient of the next data samplein the particular domain.Further, INIT and MULT can be combinedstraightforwardly, and we obtain the third setting:?
Combination (MULT+INIT).
We first pretrain onthe source domain S for parameter initialization,and then train S and T simultaneously.From a theoretical perspective, INIT and MULTwork in different ways.
In the MULT approach, thesource domain regularizes the model by ?aliasing?the error surface of the target domain; hence theneural network is less prone to overfitting.
In INIT,T ?s error surface remains intact.
Before training onthe target dataset, the parameters are initialized insuch a meaningful way that they contain additionalknowledge in the source domain.
However, in an ex-treme case where T ?s error surface is convex, INITis ineffective because the parameters can reach theglobal optimum regardless of their initialization.
Inpractice, deep neural networks usually have highlycomplicated, non-convex error surfaces.
By prop-erly initializing parameters with the knowledge ofS, we can reasonably expect that the parameters arein a better ?catchment basin,?
and that the INIT ap-proach can transfer knowledge from S to T .5 Results of Transferring by INITWe first analyze how INIT behaves in NLP-basedtransfer learning.
In addition to two different trans-fer scenarios regarding semantic relatedness as de-scribed in Section 2, we further evaluated two set-tings: (1) fine-tuning parameters 1, and (2) freez-ing parameters after transfer?????.
Existing evidenceshows that frozen parameters would generally hurtthe performance (Peng et al, 2015), but this settingprovides a more direct understanding on how trans-ferable the features are (because the factor of targetdomain optimization is ruled out).
Therefore, weincluded it in our experiments.
Moreover, we trans-ferred parameters layer by layer to answer our sec-ond research question.Through Subsections 5.1?5.3, we initialized theparameters of T with the ones corresponding tothe highest validation accuracy of S. In Subsec-tion 5.4, we further investigated when the parame-ters are ready to be transferred during the trainingon S.5.1 Overall PerformanceTable 3 shows the main results of INIT.
A quickobservation is that, in both groups, transfer learn-ing of semantically equivalent tasks (IMDB?MR,SNLI?SICK) appears to be successful with an im-provement of ?6%.
The results are not surprisingand also reported in Bowman et al (2015).For IMDB?QC and SNLI?MSRP, however,there is no improvement of transferring hidden lay-ers (embeddings excluded), namely LSTM-RNNunits and CNN feature maps.
The E1H1O2setting yields a slight degradation of 0.2?0.4%,?.5x std.
The incapability of transferring is alsoproved by locking embeddings and hidden layers483(E?????H?????O2).
We see in this setting, the test per-formance is very low in QC or even worse thanmajority-class guess in MSRP.
By further examin-ing its training accuracy, which is 48.2% and 65.5%,respectively, we conclude that extracted features byLSTM-RNN and CNN models in S are almost irrel-evant to the ultimate tasks T (QC and MSRP).Although in previous studies, researchers havemainly drawn positive conclusions about transferlearning, we find a negative result similar to oursupon careful examination of Collobert and We-ston (2008), and unfortunately, their results may besomewhat misinterpreted.
In that paper, the authorsreport transferring NER, POS, CHK, and pretrainedword embeddings improves the SRL task by 1.91?3.90% accuracy (out of 16.54?18.40% error rate),but their gain is mainly due to word embeddings.In the settings that use pretrained word embeddings(which is common in NLP), NER, POS, and CHKtogether improve the SRL accuracy by only 0.04?0.21%.The above results are rather frustrating, indicat-ing for RQ1 that neural networks may not be trans-ferable to NLP tasks of different semantics.
Trans-fer learning for NLP is more prone to semanticsthan the image processing domain, where even high-level feature detectors are almost always transfer-able (Donahue et al, 2014; Yosinski et al, 2014).5.2 Layer-by-Layer AnalysisTo answer RQ2, we next analyze the transferabil-ity of each layer.
First, we freeze both embeddingsand hidden layers (E?????H?????).
Even in semanticallyequivalent settings, if we further freeze the outputlayer (O?????
), the performance in both IMDB?MR andSNLI?SICK drops, but by randomly initializingthe output layer?s parameters (O2), we can obtain asimilar or higher result compared with the baseline(E4H2O2).
The finding suggests that the outputlayer is mainly specific to a dataset.
Transferring theoutput layer?s parameters yields little (if any) gain.Regarding embeddings and hidden layers (inthe settings E1H1O2/E1H2O2 vs. E4H2O2),the IMDB?MR experiment suggests both of em-beddings and the hidden layer play an importantrole, each improving the accuracy by 3%.
InSNLI?SICK, however, the main improvement liesin the hidden layer.
A plausible explanation is thatExperiment ISetting IMDB?MR IMDB?QCMajority 50.0 22.9E4 H2 O2 75.1 90.8E????
?H2 O2 78.2 93.2E?????H????
?O2 78.8 55.6E?????H?????O????
?73.6 ?E1 H2 O2 78.3 92.6E1 H1 O2 81.4 90.4E1 H1 O1 80.9 ?Experiment IISetting SNLI?SICK SNLI?MSRPMajority 56.9 66.5E4 H2 O2 70.9 69.0E????
?H2 O2 69.3 68.1E?????H????
?O2 70.0 66.4E?????H?????O????
?43.1 ?E1 H2 O2 71.0 69.9E1 H1 O2 76.3 68.8E1 H1 O1 77.6 ?Table 3: Main results of neural transfer learning by INIT.
Wereport test accuracies (%) in this table.
E: embedding layer;H: hidden layers; O: output layer.
4: Word embeddings arepretrained by word2vec; 2: Parameters are randomly initial-ized);?????
: Parameters are transferred but frozen; 1: Parame-ters are transferred and fine-tuned.
Notice that the E?????H?????O????
?and E1H1O1 settings are inapplicable to IMDB?QC andSNLI?MSRP, because the output targets do not share samemeanings and numbers of target classes.in sentiment classification tasks (IMDB and MR), in-formation emerges from raw input, i.e., sentimentlexicons and thus their embeddings, but natural lan-guage inference tasks (SNLI and SICK) addressmore on semantic compositionality and thus hiddenlayers are more important.Moreover, for semantically different tasks(IMDB?QC and SNLI?MSRP), the embeddingsare the only parameters that have been observed tobe transferable, slightly benefiting the target task by2.7x and 1.8x std, respectively.5.3 How does learning rate affect transfer?Bowman et al (2015) suggest that after transferring,a large learning rate may damage the knowledgestored in the parameters; in their paper, they transferthe learning rate information (AdaDelta) from S toT in addition to the parameters.484Experiment I20 60 100 140Epoch55657585Accuracy(%)(a)?
= 0.01?
= 0.03?
= 0.1?
= 0.3Experiment II20 60 100 140Epoch50607080Accuracy(%)(b)?
= 0.01?
= 0.03?
= 0.1?
= 0.3Figure 2: Learning curves of different learning rates (de-noted as ?).
(a) Experiment I: IMDB?MR; (b) Experiment II:SNLI?SICK.Although the rule of the thumb is to choose allhyperparameters?including the learning rate?byvalidation, we are curious whether the above con-jecture holds.
Estimating a rough range of sensiblehyperparameters can ease the burden of model selec-tion; it also provides evidence to better understandhow transfer learning actually works.We plot the learning curves of different learningrates ?
in Figure 2 (IMDB?MR and SNLI?SICK,E1H1O2).
(In the figure, no learning rate decay isapplied.)
As we see, with a large learning rate like?
= 0.3, the accuracy increases fast and peaks atearlier epochs.
Training with a small learning rate(e.g., ?
= 0.01) is slow, but its peak performance iscomparable to large learning rates when iterated by,say, 100 epochs.
The learning curves in Figure 2 aresimilar to classic speed/variance trade-off, and wehave the following additional discovery:In INIT, transferring learning rate informationis not necessarily useful.
A large learning ratedoes not damage the knowledge stored in thepretrained hyperparameters, but accelerates thetraining process to a large extent.
In all, we mayneed to perform validation to choose the learningrate if computational resources are available.Experiment I60708090Acc.
(%)(b)IMDB?MRIMDB?QC80Epoch: 5 10 15 20 2590IMDBAcc.
(%) (a)Learning curve of IMDBExperiment II50607080Acc.
(%)(d)79.0 76.3SNLI?SICKSNLI?MSRP70Epoch: 5 10 15 20 2580SNLIAcc.
(%) (c)Learning curve of SNLIFigure 3: (a) and (c): Learning curves of S. (b) and (d): Accu-racies of T when parameters are transferred at a certain epochduring the training of S. Dotted lines refer to non-transfer,which can be equivalently viewed as transferring before train-ing on S, i.e., epoch = 0.
Note that the x-axis shares acrossdifferent subplots.5.4 When is it ready to transfer?In the above experiments, we transfer the parame-ters when they achieve the highest validation perfor-mance on S .
This is a straightforward and intuitivepractice.However, we may imagine that the parameterswell-tuned to the source dataset may be too specificto it, i.e., the model overfits S and thus may underfitT .
Another advantage of early transfer lies in com-485putational concerns.
If we manage to transfer modelparameters after one or a few epochs on S, we cansave much time especially when S is large.We therefore made efforts in studying when theneural model is ready to be transferred.
Figures 3aand 3c plot the learning curves of the source tasks.The accuracy increases sharply from epochs 1?5;later, it reaches a plateau but is still growing slowly.We then transferred the parameters at differentstages (epochs) of training to target tasks (also withthe setting E1H1O2).
Their accuracies are plottedin Figures 3b and 3d.In IMDB?MR, the source performance and trans-ferring performance align well.
The SNLI?SICKexperiment, however, produces interesting yet unex-pected results.
Using the second epoch of SNLI?straining yields the highest transfer performance onSICK, i.e., 78.98%, when the SNLI performanceitself is comparatively low (72.65% vs. 76.26% atepoch 23).
Later, the transfer performance decreasesgradually by?2.7%.
The results in these two exper-iments are inconsistent and lack explanation.6 MULT, and its Combination with INITTo answer RQ3, we investigate how multi-tasklearning performs in transferring knowledge, as wellas the effect of the combination of MULT and INIT.In this section, we applied the setting: sharing em-beddings and hidden layers (denoted as E?H?O2),analogous to E1H1O2 in INIT.
When combiningMULT and INIT, we used the pretrained parametersof embeddings and hidden layers on S to initializethe multi-task training of S and T , visually repre-sented by E1?H1?O2.In both MULT and MULT+INIT, we had a hy-perparameter ?
?
(0, 1) balancing the source andtarget tasks (defined in Section 4).
?
was tuned witha granularity of 0.1.
As a friendly reminder, ?
= 1refers to using T only; ?
= 0 refers to using S only.After finding that a small ?
yields high performanceof MULT in the IMDB+MR and SNLI+SICK exper-iments (thick blue lines in Figures 4a and 4c), wefurther tuned the ?
from 0.01 to 0.09 with a fine-grained granularity of 0.02.The results are shown in Figure 4.
From the greencurves in the 2nd and 4th subplots, we see MULT(with or without INIT) does not improve the accu-Experiment I0.0 0.2 0.4 0.6 0.8 1.0?607080Accuracy(%)(a)81.3 81.4IMDB+MR, MULTIMDB+MR, MULT+INIT0.0 0.2 0.4 0.6 0.8 1.0?708090Accuracy(%)(b)IMDB+QC, MULTIMDB+QC, MULT+INITExperiment II0.0 0.2 0.4 0.6 0.8 1.0?607080Accuracy(%)(c)79.6 77.6SNLI+SICK, MULTSNLI+SICK, MULT+INIT0.0 0.2 0.4 0.6 0.8 1.0?607080Accuracy(%)(d)SNLI+MSRP, MULTSNLI+MSRP, MULT+INITFigure 4: Results of MULT and MULT+INIT, where we shareword embeddings and hidden layers.
Dotted lines are the non-transfer setting; dashed lines are the INIT setting E1H1O2,transferred at the peak performance of IMDB and SNLI.racy of target tasks (QC and MSRP); the inabilityto transfer is cross-checked by the INIT method inSection 5.
For MR and SICK, on the other hand,transferability of the neural model is also consis-tently positive (blue curves in Figures 4a and 4c),supporting our conclusion to RQ1 that neural trans-486fer learning in NLP depends largely on how similarin semantics the source and target datasets are.Moreover, we see that the peak performance ofMULT is slightly lower than INIT in Experiment I(Figure 4a), but higher in Experiment II (Figure 4c);they are in the same ballpark.In MULT+INIT (E1?H1?O2), the transferperformance of MULT+INIT remains high for dif-ferent values of ?.
Because the parameters givenby INIT have already conveyed sufficient informa-tion about the source task, MULT+INIT consis-tently outperforms non-transferring by a large mar-gin.
Its peak performance, however, is not higherthan MULT or INIT.
In summary, we answer ourRQ3 as follows: in our experiments, MULT andINIT are generally comparable; we do not obtainfurther gain by combining MULT and INIT.7 Concluding RemarksIn this paper, we addressed the problem of trans-fer learning in neural network-based NLP applica-tions.
We conducted two series of experiments onsix datasets, showing that the transferability of neu-ral NLP models depends largely on the semantic re-latedness of the source and target tasks, which isdifferent from other domains like image processing.We analyzed the behavior of different neural layers.We also experimented with two transfer methods:parameter initialization (INIT) and multi-task learn-ing (MULT).
Besides, we reported two additionalstudies in Sections 5.3 and 5.4 (not repeated here).Our paper provides insight on the transferability ofneural NLP models; the results also help to betterunderstand neural features in general.How transferable are the conclusions in thispaper?
We have to concede that empirical studiesare subject to a variety of factors (e.g., models, tasks,datasets), and that conclusions may vary in differentscenarios.
In our paper, we have tested all resultson two groups of experiments involving 6 datasetsand 2 neural models (CNN and LSTM-RNN).
Bothmodels and tasks are widely studied in the literature,and not chosen deliberately.
Results are mostly con-sistent (except Section 5.4).
Along with analyzingour own experimental data, we have also collectedrelated results in previous studies, serving as addi-tional evidence in answering our research questions.Therefore, we think the generality of this work isfair and that the conclusions can be generalized tosimilar scenarios.Future work.
Our work also points out some fu-ture directions of research.
For example, we wouldlike to analyze the effect of different MULT strate-gies.
More efforts are also needed in developing aneffective yet robust method for multi-task learning.AcknowledgmentsWe thank all reviewers for their constructive com-ments, Sam Bowman for helpful suggestion, andVicky Li for discussion on the manuscript.
Thisresearch is supported by the National Basic Re-search Program of China (the 973 Program) un-der Grant No.
2015CB352201 and the NationalNatural Science Foundation of China under GrantNos.
61232015, 91318301, 61421091, 61225007,and 61502014.ReferencesYoshua Bengio, Pascal Lamblin, Dan Popovici, andHugo Larochelle.
2006.
Greedy layer-wise train-ing of deep networks.
In Advances in Neural In-formation Processing Systems, pages 153?160.John Blitzer, Ryan McDonald, and FernandoPereira.
2006.
Domain adaptation with struc-tural correspondence learning.
In Proceedings ofthe Conference on Empirical Methods in NaturalLanguage Processing, pages 120?128.Samuel R. Bowman, Gabor Angeli, ChristopherPotts, and Christopher D. Manning.
2015.
Alarge annotated corpus for learning natural lan-guage inference.
In Proceedings of the Confer-ence on Empirical Methods in Natural LanguageProcessing, pages 632?642.Jane Bromley, James W Bentz, Le?on Bottou, Is-abelle Guyon, Yann LeCun, Cliff Moore, EduardSa?ckinger, and Roopak Shah.
1993.
Signatureverification using a ?Siamese?
time delay neuralnetwork.
International Journal of Pattern Recog-nition and Artificial Intelligence, 7(04):669?688.Ronan Collobert and Jason Weston.
2008.
A uni-fied architecture for natural language processing:487Deep neural networks with multitask learning.
InProceedings of the 25th International Conferenceon Machine Learning, pages 160?167.Hal Daume?
III, Abhishek Kumar, and Avishek Saha.2010.
Frustratingly easy semi-supervised domainadaptation.
In Proceedings of the Workshop onDomain Adaptation for Natural Language Pro-cessing, pages 53?59.Hal Daume?
III.
2007.
Frustratingly easy domainadaptation.
In Proceedings of the 45th AnnualMeeting of the Association of Computational Lin-guistics, pages 256?263.Jeff Donahue, Yangqing Jia, Oriol Vinyals, JudyHoffman, Ning Zhang, Eric Tzeng, and TrevorDarrell.
2014.
DeCAF: A deep convolutional ac-tivation feature for generic visual recognition.
InProceedings of the 31st International Conferenceon Machine Learning, pages 647?655.Li Dong, Furu Wei, Shujie Liu, Ming Zhou, andKe Xu.
2015.
A statistical parsing frameworkfor sentiment classification.
Computational Lin-guistics, 41(2):293?336.George Foster, Cyril Goutte, and Roland Kuhn.2010.
Discriminative instance weighting for do-main adaptation in statistical machine translation.In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, pages451?459.Xavier Glorot, Antoine Bordes, and Yoshua Bengio.2011.
Domain adaptation for large-scale senti-ment classification: A deep learning approach.
InProceedings of the 28th International Conferenceon Machine Learning, pages 513?520.Sepp Hochreiter and Ju?rgen Schmidhuber.
1997.Long short-term memory.
Neural Computation,9(8):1735?1780.Baotian Hu, Zhengdong Lu, Hang Li, and Qing-cai Chen.
2014.
Convolutional neural networkarchitectures for matching natural language sen-tences.
In Advances in Neural Information Pro-cessing Systems, pages 2042?2050.Jing Jiang and ChengXiang Zhai.
2007.
Instanceweighting for domain adaptation in NLP.
In Pro-ceedings of the 45th Annual Meeting of the Asso-ciation of Computational Linguistics, pages 264?271.Alex Krizhevsky, Ilya Sutskever, and Geoffrey EHinton.
2012.
ImageNet classification with deepconvolutional neural networks.
In Advances inNeural Information Processing Systems, pages1097?1105.Yang Liu, Sujian Li, Xiaodong Zhang, and ZhifangSui.
2016.
Implicit discourse relation classifica-tion via multi-task neural networks.
In Proceed-ings of the 30th AAAI Conference on Artificial In-telligence, pages 2750?2756.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg SCorrado, and Jeff Dean.
2013.
Distributed rep-resentations of words and phrases and their com-positionality.
In Advances in Neural InformationProcessing Systems, pages 3111?3119.Hao Peng, Lili Mou, Ge Li, Yunchuan Chen,Yangyang Lu, and Zhi Jin.
2015.
A comparativestudy on regularization strategies for embedding-based neural networks.
In Proceedings of theConference on Empirical Methods in NaturalLanguage Processing, pages 2106?2111.Barbara Plank and Alessandro Moschitti.
2013.Embedding semantic similarity in tree kernels fordomain adaptation of relation extraction.
In Pro-ceedings of the 51st Annual Meeting of the As-sociation for Computational Linguistics, pages1498?1507.Peter Prettenhofer and Benno Stein.
2010.
Cross-language text classification using structural corre-spondence learning.
In Proceedings of the 48thAnnual Meeting of the Association for Computa-tional Linguistics, pages 1118?1127.Richard Socher, Jeffrey Pennington, Eric H Huang,Andrew Y Ng, and Christopher D Manning.2011.
Semi-supervised recursive autoencodersfor predicting sentiment distributions.
In Pro-ceedings of the Conference on Empirical Methodsin Natural Language Processing, pages 151?161.488Dong Wang and Thomas Fang Zheng.
2015.
Trans-fer learning for speech and language processing.In Proceedings of the Asia-Pacific Signal and In-formation Processing Association Annual Summitand Conference, pages 1225?1237.Jason Yosinski, Jeff Clune, Yoshua Bengio, and HodLipson.
2014.
How transferable are features indeep neural networks?
In Advances in Neural In-formation Processing Systems, pages 3320?3328.Matthew D Zeiler and Rob Fergus.
2014.
Visual-izing and understanding convolutional networks.In Proceedings of 13th European Conference onComputer Vision, pages 818?833.Han Zhao, Zhengdong Lu, and Pascal Poupart.2015.
Self-adaptive hierarchical sentence model.In International Joint Conference on Artificial In-telligence, pages 4069?4076.489
