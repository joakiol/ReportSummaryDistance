Proceedings of the First Workshop on Multilingual Modeling, pages 1?10,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsImplementing a language-independent MT methodologySokratis Sofianopoulos Marina Vassiliou George TambouratzisILSP / Athena R.C.
ILSP / Athena R.C.
ILSP / Athena R.C.Artemidos 6 & Epidavrou Artemidos 6 & Epidavrou Artemidos 6 & EpidavrouAthens, Greece Athens, Greece Athens, Greeces_sofian @ilsp.gr mvas@ilsp.gr giorg_t@ilsp.grAbstractThe current paper presents a language-independent methodology, which facilitatesthe creation of machine translation (MT)systems for various language pairs.
Thismethodology is implemented in thePRESEMT hybrid MT system.
PRESEMThas the lowest possible requirements onspecialised resources and tools, given thatfor many languages (especially less widelyused ones) only limited linguistic resourcesare available.
In PRESEMT, the maintranslation process comprises two phases.The first one, Structure selection,determines the overall structure of a targetlanguage (TL) sentence, drawing onsyntactic information from a smallbilingual corpus.
The second phase,Translation equivalent selection, relies onmodels extracted solely from monolingualcorpora to implement translationdisambiguation, determine intra-phraseword order and handle functional words.This paper proposes extracting informationfor disambiguation from the monolingualcorpus.
Experimental results indicate thatsuch information substantially contributesin improving translation quality.1 IntroductionCurrently most language-independent MTapproaches are based on the statistical machinetranslation (SMT) paradigm (Koehn, 2010).
SMThas proved to be particularly amenable to newlanguage pairs, provided the necessary trainingdata are available.
The main SMT constraint is theneed for SL-TL bilingual corpora of a sufficientsize (at least several hundreds of thousands ofsentences) to allow the building of accuratetranslation models.
Such corpora are hard to find,particularly for less widely used languages.Furthermore, SMT translation accuracy largelydepends on the quality of the bilingual corpora aswell as their relevance to the domain of text to betranslated.
For instance, parliament proceedings(among the most widely available corpora) maynot suffice to train MT systems aimed towardstechnical manuals or news articles.Example-Based Machine Translation (EBMT) isanother MT paradigm, where a set of SL sentencesare provided together with their TL referencetranslations.
Translations are generated by analogy,where for an input sentence the most similar SLside from the sentence set is determined and thecorresponding TL side sentence is used to generatethe translation.
Hybrid MT systems combiningEBMT and SMT techniques have been proposed(cf.
Groves & Way, 2005 and Phillips, 2011).As an alternative to SMT, techniques forcreating MT systems using more limited but easilyobtainable resources have been proposed.
Even ifthese methods do not achieve an accuracy as highas that of SMT, their ability to develop MTsystems with very limited resources confers tothem an important advantage.
The present articlefocusses on the development of such amethodology.12 MT systems utilising low-cost resourcesA number of methods for the automatic inferenceof templates for the structural transfer from SL toTL have been proposed.
Notably, Caseli et al(2008) have proposed generating resources such asbilingual transfer rules and, more importantly,shallow transfer rules from parallel corpora.
In arelated set-up, Sanchez-Martinez et al (2009)suggest using small parallel corpora only to extracttransfer rules, assuming that a sufficient bilingualdictionary is already available.
Sanchez-Martinezet al (2009) report that the MT accuracy issubstantially higher for related languages, theproposed method exceeding even SMT systems(for which the parallel corpora used, averagingapproximately one million words each, are foundto be too small to allow effective linguisticmodelling).
Both aforementioned approaches havebeen combined with the Apertium1 MT system.Other MT systems have been proposed to caterfor the case of low resources.
Habash (2003) hasproposed the Matador system for translation fromSpanish to English, as a typical example ofGeneration-Heavy Machine Translation (GHMT),where resource poverty in the source language isaddressed by exploiting TL resources.
Carbonell etal.
(2006) propose an MT method that requires noparallel text, but relies on a translation modelutilising a full-form bilingual dictionary and adecoder using long-range context via large n-grams.Another family of systems using low-costresources encompasses METIS (Dologlou et al,2003) and METIS-II (Markantonatou et al, 2009;Carl et al, 2008).
These rely solely on extensivemonolingual corpora in order to translate SL texts.METIS and METIS-II employ pattern recognition-based algorithms to determine the translation.3 The PRESEMT system in briefThe architecture of PRESEMT has beenformulated on the basis of experience collectedwithin METIS and METIS-II.
However,PRESEMT has been substantially modified inorder to provide a measurable increase intranslation speed and accuracy.More specifically, in terms of resources,PRESEMT uses a bilingual dictionary providingSL ?
TL lexical correspondences.
It also uses, as1www.apertium.orgdoes METIS-II, an extensive TL monolingualcorpus, which is compiled automatically via webcrawling; a small bilingual corpus is yetadditionally employed, in order to (a) reduce thenumber of possible translations that need to beevaluated by the system and (b) define examples ofSL ?
TL structural modifications, thus improvingthe translation quality.
The bilingual corpus neednot cover a particular domain and only numbers afew hundred sentences (typically ~200) fordetermining structural equivalences between thesource and target languages.
Hence, in comparisonto SMT systems, the size of the parallel corpusrequired is reduced by at least three orders ofmagnitude.Both the bilingual and the monolingual corporaare annotated 2  with lemma and Part-of-Speech(PoS) information and, depending on the language,with additional morphological features (e.g.
case,number, tense etc.).
Furthermore, they aresegmented into non-recursive syntactic phrases(e.g.
noun phrase, verb phrase etc.).
The nextsection details the kind of information extracted.3.1 Exploiting the corporaThe processing of the bilingual corpus involves thecombined use of two modules, the Phrase alignermodule (PAM) and the Phrasing model generator(PMG).
Details on PAM and PMG are provided inTambouratzis et al (2011), though their operationis summarised here for reasons of completeness.Initially, the bilingual corpus is aligned at wordand phrase level by PAM.
PAM aims atcircumventing incompatibilities of differentannotation tools, based on a learning-by-exampleprinciple.
It identifies how the SL structure ismodified towards the TL one, allowing thededuction of a phrasing model for the sourcelanguage.
To operate, PAM assumes the existenceof a parser in TL, which provides chunkinginformation.
Based on lexical informationcombined with statistical data on PoS tagcorrespondences drawn from the bilingual lexicon,PAM transfers the parsing scheme from the TLside of the corpus (bearing lemma, tag and parsing2For the annotation task readily available tools are employed,including statistical taggers and (to some extent) chunkers thatprovide shallow parsing.
This alleviates the need fordeveloping new linguistic tools.2information3), to the SL side, which is only taggedand lemmatised.
In other words, the SL side issegmented into phrases in accordance to thephrasal segmentation provided for the TL side.PAM follows a three-step process, involving (a)lexicon-based correspondences, (b) alignmentbased on similarity of grammatical features andPoS tag correspondence and (c) alignment guidedby already aligned neighbouring words.
In eachconsecutive step, additional SL words are assignedto phrases, but with a reduced accuracy, the aimbeing for all words to be assigned to phrases.The SL side of the aligned corpus issubsequently processed by PMG, with a two-foldpurpose, namely to (i) deduce a phrasing modelbased on conditional random fields (CRF)(Lafferty et al, 2001) and (ii) employ this modelfor parsing any SL text submitted for translation.The TL monolingual corpus serves as the basisfor extracting two models, which are employedduring the translation process.
The first one is usedsolely for disambiguation purposes (cf.
subsection6.4).
The second model provides the micro-structural information on the translation output tosupport word reordering.
It derives from a phrase-based indexing of the TL monolingual corpus,which is performed offline during the pre-processing stage and is based on (i) phrase type,(ii) phrase head and (iii) phrase head PoS tag.To implement a fast retrieval, the TL phrases arethen organised in a hash map that allows thestorage of multiple values for each key, using as akey the three aforementioned criteria.
For eachphrase the number of occurrences within thecorpus is also retained.
Each hash map is serialisedand stored in a file with a unique name forimmediate access by the search algorithm.The number of files created as a result of thisprocess is large, yet each of the files is of smallsize and thus can be loaded quickly.
Furthermore,the existence of a given word in a phrase does notnecessarily mean that this phrase will be groupedwith other phrases containing the same word, sincethe model is based on the phrase head.For the experiments reported here, the TLmonolingual corpus is indexed based on thecriteria listed above.
However, a different indexingscheme may prove more effective, and thus3For the experiments reported here, TreeTagger (Schmid,1994) was used for the TL processing.experiments on the optimal indexing arecontinuing.
For instance, the environment of thephrase may also be stored (i.e.
the type of theprevious and next phrases) and in this case thephrase organisation may be modified.
Thesemodifications may yield a decrease incomputational load during translation, by reducingthe number of phrase comparisons.3.2 Main translation engineThe translation process is split into two phases,each of which makes use of only a single type ofcorpus.
Phase 1 (Structure selection) uses thebilingual corpus to determine, for a given input SLsentence, the appropriate TL structure in terms ofphrase type and order.
The output of the Structureselection phase is the SL sentence with a TLstructure, created by reordering the phrasesaccording to the parallel corpus, and all wordsreplaced by the TL lemmas and tag information asretrieved from the bilingual dictionary.Phase 2 (Translation equivalent selection) usesthe monolingual corpus to specify the most likelyword order within phrases, to handle functionalwords such as articles and prepositions and toresolve lexical ambiguities emerging from thepossible translations provided by the bilingualdictionary.
Finally, a token generator componentgenerates tokens out of lemmas.
Therefore, thefirst PRESEMT translation phase is closely relatedto EBMT, while the second phase is reliant uponstatistical information, resulting in a hybrid nature.4 Example of the PRESEMT translationprocessIn this section the translation process of thePRESEMT system is illustrated via a simpleexample.
Details on the algorithmic part areprovided in the subsequent sections.Input Sentence: ????????????
???????
??????????????
????
?????
???
?????????
(= ?Goodneighbourhood relations are established in theBalkan countries?
)Annotation at various levels [tagging &lemmatising; PMG-based segmentation to phrases;output of the lexicon look-up]Input sentence annotation after being input for translationPhrase Word Lemma Tag LexiconVC4 ????????????
????????
vbo3pl {consolidate;4VC: verb chunk, PC: prepositional chunk3Input sentence annotation after being input for translationPhrase Word Lemma Tag Lexiconestablish}PC??????????????????????????????????????
?nofeplnmajfesggenofesgge{relation;relationship}{nice; decent;good}{adjacency;neighbourhood}PC?????????????????????????????????????
?asfeplacnofeplacatneplgenoneplge{on; at; to; into;in; upon}{country}{the}{Balkan}1st translation phase: Establish the correctphrase order on the basis of TL.
Search thebilingual corpus for the most similar SL sentencein structural terms, find the corresponding TL oneand reorder the input sentence accordingly.Most similar SL sentence of the bilingual parallel corpusPhrase Word Lemma TagVC ????????????
????????
vb03plPC ????????????
???????????????????????????nofeplnmnomaplgePC??????????????????????????????
?asppsppnfe03placnofeplacatffesggeabbrCorresponding TL sentence of the bilingual parallel corpusPhrase Word Lemma TagPC student protestsstudentprotestnnnnsVC occur occur vvPCInotherEUcountriesinotherEUcountryinjjnpnnsOutput of 1st transl.
phase (expressed as list ofphrases and lemmas): [{relation; relationship};{nice; decent; good}; {adjacency; neighbourhood}PC] [{consolidate; establish} VC] [{on; at; to; into;in; upon}; {country}; {the}; {Balkan} PC]2nd translation phase: Identify the correct wordorder within each phrase.
Disambiguate thetranslations.
Generate tokens out of lemmasWord reordering results: [{nice; decent;good}; {adjacency; neighbourhood}; {relation;relationship} PC] [{consolidate; establish} VC] [{on; at;to; into; in; upon}; {the}; {Balkan}; {country} PC]Disambiguation: [{good}; {neighbourhood};{relation} PC] [{establish} VC] [{in}; {the};{Balkan}; {country} PC]Token generation: [{good}; {neighbourhood};{relations} PC] [{are established} VC] [{in}; {the};{Balkan}; {countries} PC]Final Translation: [Good neighbourhoodrelationsPC] [are established VC] [in the Balkancountries PC]5 Phase 1: Structure selectionThe task of Structure selection is to determine thetype of TL phrases to which the SL ones translateand to order them in the TL sentence.
To this end itconsults the patterns of SL ?
TL structuralmodifications to be found in the parallel corpus,thus resembling EBMT (Hutchins, 2005).Translation phase 1 receives as input an SLsentence (termed ISS ?
Input Source Sentence),bearing lexical translations from the dictionary,annotated with tag & lemma information andsegmented into phrases by PMG.
A dynamicprogramming algorithm then determines for eachISS the most similar, in terms of phrase structure,SL sentence found in the bilingual corpus (termedACS ?
Aligned Corpus Sentence)5.The similarity is determined by taking intoaccount structural information such as phrase type,phrase head PoS tag, phrase functional head infoand phrase head case.
The ISS phrases are thenreordered in accordance to the TL side of thechosen ACS by replicating the SL-TL phrasealignment mapping.
The data flow of the Structureselection is depicted in Figure 1.The dynamic programming algorithm isessentially a monolingual similarity algorithm.
Themost similar SL structure of the bilingual corpus,that determines the TL structure of the sentence tobe translated, is thus selected purely on SLproperties.
The implemented method is based onthe Smith-Waterman algorithm (Smith andWaterman, 1981), initially proposed for alignmentof DNA and RNA sequences.
This algorithm isguaranteed to find the optimal local alignmentbetween two input sequences.5If the most similar ACS retrieved from the parallel corpus isvery dissimilar, then ISS does not undergo any reordering.
It isnotable that in our experiments never did such an occasionappear, the similarity always reaching a high percentage(above 70%).
The fact that comparisons involve sentences ofthe same language (SL) ensures a high similarity score.45.1 Calculating structural similarityThe structural similarity between ISS and ACS isreflected on the similarity score, for the calculationof which a two-dimensional matrix is created withthe ISS along the top row and the ACS along theleft side.
A cell (i,j) represents the similarity of thesub-sequence of elements up to the mapping of theelements Ei of the ACS and E?j of the ISS, whereeach element corresponds to a phrase.
Thesimilarity for cell (i,j) is determined by examiningthe predecessor cells located directly to the left (i,j-1), directly above (i-1, j) and above-left (i-1, j-1),that contain values V1, V2 and V3 respectively,and is calculated iteratively as the maximum of thethree numbers {max(V1, V2,V3)+ElementSimilarity(Ei, E?j)}.
The similarity oftwo phrases (PhrSim) is calculated as the weightedsum of four criteria, namely the similarities of (a)the phrase type (PhrTypSim), (b) the phrase headPoS tag (PhrHPosSim), (c) the phrase head case(PhrHCasSim) and (d) the functional phrase headPoS tag (PhrfHPosSim):PhrSim(Ei,E?j) = WphraseType*PhrTypSim(Ei,E?j) +WheadPoS*PhrHPosSim(Ei,E?j) +WheadCase*PhrHCasSim(Ei,E?j) +WfheadPoS*PhrfHPosSim(Ei,E?j)For normalisation purposes, the sum of the fouraforementioned weights (whose experimentalvalues6  are 0.4, 0.1, 0.1 and 0.4 respectively) isequal to 1.
The similarity score ranges from 100 to0, these limits denoting exact match and totaldissimilarity between elements Ei and E?jrespectively.
In case of a zero similarity score, apenalty weight (-50) is employed, to furtherpenalise mapping of dissimilar items.When the algorithm has reached the jth elementof the ISS, the similarity score between the two SLsentences is calculated as the value of themaximum jth cell.
The ACS that achieves thehighest similarity score is the closest to the inputSL sentence in terms of phrase structure.After determining the similarity betweensentences, as the final similarity score, thecomparison matrix indicates the optimal phrasealignment between the two SL sentences.
Bycombining the SL sentence alignment from thealgorithm with the alignment information between6An optimisation module has been designed as part of thePRESEMT system for defining the optimal values of theseparameters (cf.
subsection 5.3 for more details).the ACS and the attached TL sentence, ISS phrasesare reordered according to the TL side structure.To illustrate this approach, an example isprovided with Greek as SL and English as TL.
Letus assume the ISS given in (1):(1) ??
???
???
????????
?????????????????????
??
???
????????pi???????
??????????
(?The term Machine Translation denotes anautomated procedure?
)The input sentence is segmented by PMG intothe structure depicted in (2a); the structureelements being exemplified in (2b):(2a) pc(as, no_ac) pc(-, no_ac) vp(-, vb) pc(as, no_ac)(2b) <Phrase type> (<Phrase fhead PoS tag>,<Phrase head PoS tag>_<Phrase head case>)An indicative ACS from the aligned corpus isgiven in (3):(3) ??
?????????
?????
???
????pi?????
???????????????
???
???????
?????????
??????.
(?Thehistorical roots of the European Union lie in theSecond World War?
)The corresponding structural information for (3)is: pc(-,no_nm) pc(-,no_ge) vc(vb) pc(as,no_ac).Input source sentence (ISS)pc (as,no_ac)pc (-,no_ac)vc(-, vb)pc (-,no_ac)0 0 0 0 0pc(-,no_nm) 0 60 80 -20 60pc(-,no_ge) 0 60 140 40 40vc(vb) 0 -50 10 240 140Alignedcorpussentence(ACS)pc(as,no_ac) 0 100 30 -40 340Table 1.
Matrix defining phrase correspondence ofsentences (1) and (3)Then, the matrix of Table 1 is created tocalculate the similarity scores between sentences(1) and (3) (cells forming the best alignedsubsequence are highlighted).
By choosing foreach element the maximum similarity, thetransformation cost is calculated (340 in this case).Based on this matrix, ISS is modified inaccordance to the attached TL structure.5Figure 1.
Data flow in Structure selection6 Phase 2: Translation equivalentselectionFollowing Phase 1, the issues to be resolved in thesecond phase include (i) word ordering withinphrases, (ii) handling of functional words and (iii)resolution of translation ambiguities.6.1 Searching for phrasal equivalentsThe monolingual TL corpus is searched todetermine the most similar phrase to each phrase inthe SL sentence, in order to establish the correctword order.
The similarity measure takes intoaccount the phrase type, and the words containedin the phrase in terms of lemma, PoS tag andmorphological features.
These factors enter thecomparison with different weights, whose relativemagnitudes are subject to an optimisation process.The main issue at this stage is to reorderappropriately any items within each phrase.
Thisentails that the words of a given phrase of the inputsentence (denoted as ISP ?
Input Sentence Phrase),and the words of a retrieved TL phrase (denoted asMCP ?
Monolingual Corpus (TL) Phrase), areclose to each other in terms of number and type.The data flow of the Translation equivalentselection is depicted in Figure 2.6.2 Establishing correct word orderWhen initiating Phase 2 of the translation process,the matching algorithm accesses the indexed TLphrase corpus to retrieve similar phrases and selectthe most similar one through a comparison process,which is viewed as an assignment problem.
Thisproblem can be solved via algorithms such as theGale-Shapley (Gale and Shapley, 1962; Mairson,1992) and Kuhn?Munkres ones (Kuhn, 1955;Munkres, 1957).
The Kuhn-Munkres approachcomputes an exact solution of the assignmentproblem to determine the optimal matchingbetween elements.
Experiments with METIS-IIhave shown that the solution of the assignmentproblem is computationally-intensive.On the contrary, the Gale-Shapley algorithmsolves the assignment problem in a reduced time.In this approach, the two sides are termed suitors(in PRESEMT, the SL side) and reviewers (the TLside).
The two groups have distinct roles, suitorsproclaiming their order of preference of beingassigned to a specific reviewer, via an ordered list.Each reviewer selects one of the suitors afterevaluating them based on the ordered preferencelist, in subsequent steps revising its selection sothat the resulting assignment is improved.
Thisprocess is suitor-optimal but possibly non-optimalfrom the reviewers?
viewpoint.
As its complexity issubstantially lower than that of Kuhn-Munkres, theGale-Shapley algorithm is adopted in PRESEMTto limit the computation time.For each SL phrase, it is necessary to establishthe correct word order for all possible TL phrasesthat can be produced by combining the lexicalequivalents of each word in the phrase.After the completion of this comparison process,the selected phrase from the monolingual corpusserves as a basis for resolving other issues such asthe handling of functional words (e.g.
insertion /deletion of articles).
In this process, the TLinformation prevails over the SL entries.6.3 Optimising the selection process ofphrasal equivalentsThe search for the most similar phrase depends ona set of parameters.
Within this set, different typesof weights are included, such as weights governingthe similarity of PoS tags, lemmas, phrase typesand morphological features.
The weights from both6translation phases are handled in a unified mannerby the Optimisation module.
Research in earlierMT systems has shown that the application ofGenetic Algorithms (GAs) and multi-objectiveevolutionary algorithms such as SPEA2 (ImprovedStrength Pareto Evolutionary Algorithm) for theoptimisation of parameters can considerablyimprove the translation quality (Sofianopoulos etal., 2010).For the experiments presented in the next section,manually-defined preliminary weights are used forthe parameters of both phases.
To further improvethe translation accuracy, an optimisation process isstudied.
This optimisation (which is beyond thescope of the present article) provides the prospectfor a substantial improvement in the accuracy viathe selection of appropriate parameter values.Figure 2.
Data flow in Translation equivalent selection6.4 Resolving translation ambiguitiesTranslation equivalent selection receives asinput the output of Structure selection, whichcontains sets of candidate translations for eachSL lemma.
One translation needs to be chosenfrom each set, thus disambiguating amongst thepossible translations.
The disambiguationprocess uses the semantic similarities betweenwords as evidenced by the monolingual corpus.Different approaches are evaluated for selectingthe most appropriate translation, includingVector Space Modelling (Marsi et al, 2010) andSelf-Organising Maps, following the work byTsimboukakis et al (2011).These disambiguation processes lie beyondthe scope of the present publication.
On thecontrary, a simpler, corpus-based approach isproposed here, which relies on the extraction ofstatistical information with only limited pre-processing.
This method reuses and enhances theindexed sets of the monolingual corpus phrases,by exploiting information on the frequency ofoccurrence of each TL phrase.
When searchingfor the best matching TL phrase for eachcombination of lexical alternatives, thefrequency of the TL phrase is taken into account.Notably, not all combinations are examined forlexical disambiguation; instead only the phrasemapped to the most frequent TL phrase isretained.7 Experimental ResultsThe evaluation results reported here concern theGreek ?
English language7 pair and were basedon the development datasets used in PRESEMTfor studying the system performance.
For eachSL, these datasets contain 1,000 sentences,collected via web-crawling.
Sentence lengthranges from 7 to 40 words.
From these datasets,200 sentences were randomly chosen, andmanually translated into each of the targetlanguages.
The correctness of these referencetranslations was checked independently bynative speakers.7PRESEMT handles 8 language pairs: SL {Czech, English,German, Greek, Norwegian} ?
TL {English, German}.7For the current evaluation phase fourautomatic evaluation metrics have beenemployed, i.e.
BLEU (Papineni et al, 2002),NIST (NIST 2002), Meteor (Denkowski andLavie, 2011) and TER (Snover et al, 2006).Table 2 summarises indicative scores obtained.Number of sentences 40 Source webReference translations 1 Language pair EL ?
ENMetrics MT systemBLEU NIST Meteor TERPRESEMT 1 0.1297 4.1568 0.2669 79.417PRESEMT 2 0.2004 4.9995 0.3294 72.678Metis-II 0.1222 3.1655 0.2698 82.878Google8 0.5472 7.1360 0.4713 29.963Systran9 0.3143 5.4615 0.3857 49.449WordLingo10 0.2908 5.1853 0.3728 49.632Table 2.
Evaluation resultsWhen using the base PRESEMT system withthe phrase-frequency disambiguation componentdeactivated (denoted as PRESEMT 1), a BLEUscore of 0.1297 and a Meteor score of 0.2669 areobtained.
When the disambiguation componentis activated (PRESEMT 2), these scores increasesubstantially, reaching a BLEU score of justover 0.20.
The BLEU improvement overPRESEMT 1 is 0.07 points (representing a 50%improvement), while NIST is increased by 0.85and Meteor by over 0.06.
TER is reduced by 7points, also marking an improvement.To put these scores into perspective, acomparison is made to MT systems available onthe Internet, both rule-based (SYSTRAN) andSMT ones (Google Translate).
In addition, theresults of METIS-II are quoted, to comparePRESEMT with a system based on monolingualcorpora.
As can be seen, web-based MT systemsproduce higher scores for all metrics, withGoogle Translate possessing the best values.Yet these scores are, especially in the case ofSystran and WordLingo, not far off the scoresobtained for PRESEMT with disambiguation.
Inparticular NIST scores are directly comparablewhilst the Meteor ones are not substantiallyhigher.
It can be reasonably assumed that due tothe language-independent methodology without8translate.google.com9www.systranet.com10www.worldlingo.comdirect provision of language-specificinformation, the scores obtained via PRESEMTwill be lower.
Still, it is expected that refinedversions of the PRESEMT algorithm will allowthe achievement of higher scores that render itsperformance directly comparable to that ofSystran and WordLingo, for the given languagepair.
In comparison to METIS-II, PRESEMToffers a substantial improvement for all metrics,with for instance BLEU and NIST scoresincreased by over 50%.
This illustrates theimprovements conferred by the new translationmethodology.
As noted, PRESEMT is still underdevelopment and it is anticipated that moreextensive experiments involving additionallanguage pairs will provide improvements in thetranslation quality.8 ConclusionsIn the present article the principles and theimplementation of a novel language-independentmethodology have been presented.
ThePRESEMT methodology draws on informationresiding in a large monolingual corpus and asmall bilingual one for creating MT systemsreadily portable to new language pairs.
Most ofthis information is extracted in an automatedmanner using pattern recognition techniques.First experimental results using objectiveevaluation metrics and comparisons toestablished systems have also been reported.These results are promising, especially takinginto account the fact that several PRESEMTmodules are still under development and thetranslation process is being refined, in particularwith respect to the handling of internal phrasalstructure.
These will be reported in futurearticles.ReferencesMichael Carl, Maite Melero, Toni Badia, VincentVandeghinste, Peter Dirix, Ineke Schuurman,Stella Markantonatou, Sokratis Sofianopoulos,Marina Vassiliou and Olga Yannoutsou.
2008.METIS-II: Low Resources Machine Translation:Background, Implementation, Results andPotentials.
Machine Translation, Vol.
22, No.
1-2,pp.
67-99.Jaime Carbonell, Steve Klein, David Miller, MichaelSteinbaum, Tomer Grassiany, and Jochen Frey.82006.
Context-Based Machine Translation.
InProceedings of the 7th Conference of theAssociation for Machine Translation in theAmericas, Cambridge, Massachusetts, USA, pp.19-28.Helena M. Caseli, Maria das Gracas V. Nunes, andMikel L. Forcada (2008) Automatic Induction ofBilingual resources from aligned parallel corpora:Application to shallow-transfer machinetranslation.
Machine Translation, Vol.
20, pp.
227-245.Michael Denkowski and Alon Lavie.
2011.
Meteor1.3: Automatic Metric for Reliable Optimizationand Evaluation of Machine Translation Systems.EMNLP 2011 Workshop on Statistical MachineTranslation, Edinburgh, Scotland, pp.
85-91.Ioannis Dologlou, Stella Markantonatou, GeorgeTambouratzis, Olga Yannoutsou, Athanasia Fourlaand Nikos Ioannou.
2003.
Using MonolingualCorpora for Statistical Machine Translation: TheMETIS System.
In Proceedings of the EAMT-CLAW?03 Workshop, Dublin, Ireland, pp.
61-68.David Gale and Lloyd S. Shapley.
1962.
CollegeAdmissions and the Stability of Marriage.American Mathematical Monthly, Vol.
69, pp.
9-14.Declan Groves & Andy Way, 2005.
Hybrid data-driven Models of Machine Translation.
MachineTranslation, Vol 19, pp.301-323.Nizar Habash.
2003.
Matador: A Large-ScaleSpanish-English GHMT System.
In Proceedingsof MT Summit IX, New Orleans, LA, pp.
149-156.John Hutchins.
2005.
Example-Based MachineTranslation: a Review and Commentary.
MachineTranslation, Vol.
19, pp.197-211.Philip Koehn.
2010.
Statistical Machine Translation.Cambridge University Press, Cambridge.Harold W. Kuhn.
1955.
The Hungarian method forthe assignment problem.
Naval Research LogisticsQuarterly, Vol.
2, pp.
83-97.John Lafferty, Andrew McCallum and FernandoPereira.
2001.
Conditional Random Fields:Probabilistic Models for Segmenting andLabelling Sequence Data.
28th InternationalConference on Machine Learning, ICML 2011,Bellevue, Washington, USA, pp.
282-289.Stella Markantonatou, Sokratis Sofianopoulos, OlgaGiannoutsou and Marina Vassiliou.
2009.
HybridMachine Translation for Low- and Middle-Density Languages.
Language Engineering forLesser-Studied Languages, S. Nirenburg (ed.
), IOSPress, pp.
243-274.Erwin Marsi, Andr?
Lynum, Lars Bungum, and Bj?rnGamb?ck.
2011.
Word TranslationDisambiguation without Parallel Texts.International Workshop on Using LinguisticInformation for Hybrid Machine Translation,Barcelona, Spain, pp.
66-74.Harry Mairson.
1992.
The Stable Marriage Problem.The Brandeis Review, 12:1.
Available at:www.cs.columbia.edu/~evs/intro/stable/writeup.htmlJames Munkres.
1957.
Algorithms for the assignmentand transportation problems.
Journal of the Societyfor Industrial and Applied Mathematics, Vol.
5,pp.
32-38.NIST 2002.
Automatic Evaluation of MachineTranslation Quality Using n-gram Co-occurrencesStatistics.Kishore Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2002.
BLEU: A Method forAutomatic Evaluation of Ma-chine Translation.40th Annual Meeting of the Association forComputational Linguistics, Philadelphia, USA, pp.311-318.Aaron Phillips.
2011.
CUNEI: Open-source MachineTranslation with Relevance-based models of eachtranslation instance.
Machine Translation, Vol.
25,pp.
161-177Felipe Sanchez-Martinez and Mikel L. Forcada.2009.
Inferring Shallow-transfer Machinetranslation Rules from Small Parallel Corpora.Journal of Artificial Intelligence Research, Vol.34, pp.
605-635.Helmut Schmid.
1994.
Probabilistic Part-of-SpeechTagging Using Decision Trees.
In Proceedings ofInternational Conference on New Methods inLanguage Processing, Manchester, UK, pp.
44-49.Temple F. Smith and Michael S. Waterman.
1981.Identification of Common MolecularSubsequences.
Journal of Molecular Biology, Vol.147, pp.
195-197.Matthew Snover, Bonnie Dorr, Richard Schwartz,Linnea Micciulla, and John Makhoul.
2006.
AStudy of Translation Edit Rate with TargetedHuman Annotation.
In Proceedings of the 7thConference of the Association for MachineTranslation in the Americas, Cambridge,Massachusetts, USA, pp.
223-231.Sokratis Sofianopoulos, and George Tambouratzis.2010.
Multiobjective Optimisation of real-valued9Parameters of a Hybrid MT System using GeneticAlgorithms.
Pattern Recognition Letters, Vol.
31,pp.1672-1682.George Tambouratzis, Fotini Simistira, SokratisSofianopoulos, Nikos Tsimboukakis, and MarinaVassiliou.
2011.
A resource-light phrase schemefor language-portable MT.
15th InternationalConference of the European Association forMachine Translation, Leuven, Belgium, pp.
185-192.Nikos Tsimboukakis, and George Tambouratzis.2011.
Word map systems for content-baseddocument classification.
IEEE Transactions onSystems, Man & Cybernetics ?
Part C, Vol.
41(5),pp.
662-673.10
