Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 61?64,Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLPA Combination of Active Learning and Semi-supervised LearningStarting with Positive and Unlabeled Examples for Word SenseDisambiguation: An Empirical Study on Japanese Web Search QueryMakoto Imamuraand Yasuhiro TakayamaInformation Technology R&D Center,Mitsubishi Electric Corporation5-1-1 Ofuna, Kamakura, Kanagawa, Japan{Imamura.Makoto@bx,Takayama.Yasuhiro@ea}.MitsubishiElectric.co.jpNobuhiro Kaji, Masashi Toyodaand Masaru KitsuregawaInstitute of Industrial Science,The University of Tokyo4-6-1 Komaba, Meguro-ku Tokyo, Japan{kaji,toyoda,kitsure}@tkl.iis.u-tokyo.ac.jpAbstractThis paper proposes to solve the bottle-neck of finding training data for wordsense disambiguation (WSD) in the do-main of web queries, where a complete setof ambiguous word senses are unknown.In this paper, we present a combination ofactive learning and semi-supervised learn-ing method to treat the case when positiveexamples, which have an expected wordsense in web search result, are only given.The novelty of our approach is to use?pseudo negative examples?
with reliableconfidence score estimated by a classifiertrained with positive and unlabeled exam-ples.
We show experimentally that ourproposed method achieves close enoughWSD accuracy to the method with themanually prepared negative examples inseveral Japanese Web search data.1 IntroductionIn Web mining for sentiment or reputationanalysis, it is important for reliable analysis toextract large amount of texts about certain prod-ucts, shops, or persons with high accuracy.
Whenretrieving texts from Web archive, we often suf-fer from word sense ambiguity and WSD systemis indispensable.
For instance, when we try toanalyze reputation of "Loft", a name of varietystore chain in Japan, we found that simple textsearch retrieved many unrelated texts which con-tain "Loft" with different senses such as an atticroom, an angle of golf club face, a movie title, aname of a club with live music and so on.
Thewords in Web search queries are often propernouns.
Then it is not trivial to discriminate thesesenses especially for the language like Japanesewhose proper nouns are not capitalized.To train WSD systems we need a largeamount of positive and negative examples.
In thereal Web mining application, how to acquiretraining data for a various target of analysis hasbecome a major hurdle to use supervised WSD.Fortunately, it is not so difficult to create posi-tive examples.
We can retrieve positive examplesfrom Web archive with high precision (but lowrecall) by manually augmenting queries with hy-pernyms or semantically related words (e.g.,"Loft AND shop" or "Loft AND stationary").On the other hand, it is often costly to createnegative examples.
In principle, we can createnegative examples in the same way as we did tocreate positive ones.
The problem is, however,that we are not sure of most of the senses of atarget word.
Because target words are oftenproper nouns, their word senses are rarely listedin hand-crafted lexicon.
In addition, since theWeb is huge and contains heterogeneous do-mains, we often find a large number of unex-pected senses.
For example, all the authors didnot know the music club meaning of Loft.
As theresult, we often had to spend much time to findsuch unexpected meaning of target words.This situation motivated us to study activelearning for WSD starting with only positive ex-amples.
The previous techniques (Chan and Ng,2007; Chen et al 2006) require balanced positiveand negative examples to estimate the score.
Inour problem setting, however, we have no nega-tive examples at the initial stage.
To tackle thisproblem, we propose a method of active learningfor WSD with pseudo negative examples, whichare selected from unlabeled data by a classifiertrained with positive and unlabeled examples.McCallum and Nigam (1998) combined activelearning and semi-supervised learning technique61by using EM with unlabeled data integrated intoactive learning, but it did not treat our problemsetting where only positive examples are given.The construction of this paper is as follows;Section 2 describes a proposed learning algo-rithm.
Section 3 shows the experimental results.2 Learning Starting with Positive andUnlabeled Examples for WSDWe treat WSD problem as binary classificationwhere desired texts are positive examples andother texts are negative examples.
This setting ispractical, because ambiguous senses other thanthe expected sense are difficult to know and areno concern in  most Web mining applications.2.1 ClassifierFor our experiment, we use naive Bayes classifi-ers as learning algorithm.
In performing WSD,the sense ?s?
is assigned to an example charac-terized with the probability of linguistic featuresf1,...,fn so as to maximize:?=njpp1)|(f)( ss j               (1)The sense s is positive when it is the targetmeaning in Web mining application, otherwise sis negative.
We use the following typical linguis-tic features for Japanese sentence analysis, (a)Word feature within sentences, (b) Precedingword feature within bunsetsu (Japanese basephrase), (c) Backward word feature within bun-setsu, (d) Modifier bunsetsu feature and (e)Modifiee bunsetsu feature.Using naive Bayes classifier, we can estimatethe confidence score c(d, s) that the sense of adata instance ?d?, whose features are f1, f2, ..., fn,is predicted sense ?s?.
?=+=njpp1)|(f log)( logs)c(d, ss j      (2)2.2 Proposed AlgorithmAt the beginning of our algorithm, the system isprovided with positive examples and unlabeledexamples.
The positive examples are collectedby full text queries with hypernyms or semanti-cally related words.First we select positive dataset P from initialdataset by manually augmenting full text query.At each iteration of active learning, we selectpseudo negative dataset Np (Figure 1 line 15).
Inselecting pseudo negative dataset, we predictword sense of each unlabeled example using thenaive Bayes classifier with all the unlabeled ex-amples as negative examples (Figure 2).
In detail,if the prediction score (equation(3)) is more than?, which means the example is very likely to benegative, it is considered as the pseudo negativeexample (Figure 2 line 10-12).pos)c(d,neg)c(d,psdNeg)c(d, ?=          (3)01    # Definition02   ?
(P, N): WSD system trained on P as Positive03                   examples, N as Negative examples.04   ?EM(P, N, U): WSD system trained on P as05   Positive examples, N as Negative examples,06   U as Unlabeled examples by using EM07   (Nigam et.
all 2000)08    # Input09    T ?
Initial unlabeled dataset which contain10            ambiguous words11    # Initialization12    P ?
positive training dataset by full text search on T13    N ?
?
(initial negative training dataset)14    repeat15      # selecting pseudo negative examples Np16          by   the score of  ?
(P, T-P)  (see figure 2)17      # building a classifier with  Np18      ?new ?
?EM (P,  N+Np, T-N-P)19      #  sampling data by using the score of ?new20      cmin   ?
?21      foreach d ?
(T ?
P ?
N )22         classify d by WSD system?new23         s(d) ?
word sense prediction for d using?new24         c(d, s(d)) ?
the confidence of  prediction of d25         if c(d, s(d))  ?
cmin   then26             cmin  ?
c(d),   d min ?
d27      end28    end29     provide correct sense s for d min  by human30     if s is positive then add d min   to P31                             else  add d min   to N32   until Training dataset reaches desirable size33   ?new  is the output classifierFigure 1: A combination of active learning andsemi-supervised learning starting with positiveand unlabeled examplesNext we use Nigam?s semi-supervised learningmethod using EM and a naive Bayes classifier(Nigam et.
all, 2000) with pseudo negative data-set Np  as negative training dataset to build therefined classifier ?EM (Figure 1 line 17).In building training dataset by active learning,we use uncertainty sampling like (Chan and Ng,2007) (Figure 1 line 30-31).
This step selects themost uncertain example that is predicted with thelowest confidence in the refined classifier ?EM.Then, the correct sense for the most uncertain62example is provided by human and added to thepositive dataset P or the negative dataset N ac-cording to the sense of d.The above steps are repeated until datasetreaches the predefined desirable size.01    foreach d ?
( T ?
P ?
N )02       classify d by WSD system?
(P, T-P)03       c(d, pos) ?
the confidence score that d is04           predicted as positive defined in equation (2)05       c(d, neg) ?
the confidence score that d is06           predicted as negative defined in equation (2)07       c(d, psdNeg) =  c(d, neg)  - c(d, pos)08                       (the confidence score that d is09                         predicted as pseudo negative)10        PN ?
d ?
( T ?
P ?
N ) |  s(d) = neg ?11                                                  c(d, psdNeg)  ??
}12                        (PN is pseudo negative dataset )13     endFigure 2: Selection of pseudo negative examples3 Experimental Results3.1 Data and Condition of ExperimentsWe select several example data sets from Japa-nese blog data crawled from Web.
Table 1 showsthe ambiguous words and each ambiguous senses.Word Positive sense Other ambiguous sensesWega product name(TV)Las Vegas, football teamname, nickname, star, horserace, Baccarat glass, atelier,wine, game, musicLoft store name attic room, angle of golfclub face, club with livemusic,  movieHonda personal name(football player)Personal names (actress,artists, other football play-ers, etc.)
hardware store, carcompany nameTsubaki product name(shampoo)flower name, kimono, horserace, camellia ingredient,shop nameTable 1: Selected examples for evaluationTable 2 shows the ambiguous words, the num-ber of its senses, the number of its data instances,the number of feature, and the percentage ofpositive sense instances for each data set.Assigning the correct labels of data instances isdone by one person and 48.5% of all the labelsare checked by another person.
The percentageof agreement between 2 persons for the assignedlabels is 99.0%.
The average time of assigninglabels is 35 minutes per 100 instances.Selected instances for evaluation are randomlydivided 10% test set and 90% training set.
Table3 shows the each full text search query and thenumber of initial positive examples and the per-centage of it in the training data set.word No.
ofsensesNo.
ofinstancesNo.
offeaturesPercentage ofpositive senseWega 11 5,372 164,617 31.1%Loft 5 1,582   38,491 39.4%Honda 25 2,100   65,687 21.2%Tsubaki 6 2,022   47,629 40.2%Table 2: Selected examples for evaluationword Full text query for initialpositive examplesNo.
of positiveexamples (percent-age in trainig set)Wega Wega  AND TV 316  (6.5%)Loft Loft AND (Grocery OR-Stationery)64  (4.5%)Honda Honda AND Keisuke 86 (4.6%)Tsubaki Tsubaki AND Shiseido 380 (20.9%)Table 3: Initial positive examplesThe threshold value?in figure 2 is set to em-pirically optimized value 50.
Dependency onthreshold value ?
will be discussed in 3.3.3.2 Comparison ResultsFigure 3 shows the average WSD accuracy ofthe following 6 approaches.Figure 3: Average active learning processB-clustering is a standard unsupervised WSD, aclustering using naive Bayes classifier learnedwith two cluster numbers via EM algorithm.
Thegiven number of the clusters are two, negativeand positive datasets.M-clustering is a variant of b-clustering wherethe given number of clusters are each number ofambiguous word senses in table 2.Human labeling, abbreviated as human, is anactive learning approach starting with humanlabeled negative examples.
The number of hu-5658606264666870720 10 20 30 40 50 60 70 80 90 100757779818385878991humanwith-EMwithout-EMrandomm-clusteringb-clustering63man labeled negative examples in initial trainingdata is the same as that of positive examples infigure 3.
Human labeling is considered to be theupper accuracy in the variants of selectingpseudo negative examples.Random sampling with EM, abbreviated aswith-EM, is the variant approach where dmin  inline 26 of figure 1 is randomly selected withoutusing confidence score.Uncertainty sampling without EM (Takayamaet al 2009), abbreviated as without-EM, is a vari-ant approach where ?EM (P,  N+Np, T-N-P) inline 18 of figure 1 is replaced by ?
(P, N+Np).Uncertainty Sampling with EM, abbreviated as un-certain, is a proposed method described in figure 1.The accuracy of the proposed approach with-EM is gradually increasing according to the per-centage of added hand labeled examples.The initial accuracy of with-EM, which meansthe accuracy with no hand labeled negative ex-amples, is the best score 81.4% except for that ofhuman.
The initial WSD accuracy of with-EM is23.4 and 4.2 percentage points higher than thoseof b-clustering (58.0%) and m-clustering(77.2%), respectively.
This result shows that theproposed selecting method of pseudo negativeexamples is effective.The initial WSD accuracy of with-EM is 1.3percentage points higher than that of without-EM(80.1%).
This result suggests semi-supervisedlearning using unlabeled examples is effective.The accuracies of with-EM, random and with-out-EM are gradually increasing according to thepercentage of added hand labeled examples andcatch up that of human and converge at 30 per-centage added points.
This result suggests thatour proposed approach can reduce the labor costof assigning correct labels.The curve with-EM are slightly upper than thecurve random at the initial stage of active learn-ing.
At 20 percentage added point, the accuracywith-EM is 87.0 %, 1.1 percentage points higherthan that of random (85.9%).
This result suggeststhat the effectiveness of proposed uncertaintysampling method is not remarkable depending onthe word distribution of target data.There is really not much difference between thecurve with-EM and without-EM.
As a classifiesto use the score for sampling examples in adapta-tion iterations, it is indifferent whether with-EMor without-EM.Larger evaluation is the future issue to confirmif the above results could be generalized beyondthe above four examples used as proper nouns.3.3 Dependency on Threshold Value ?Figure 4 shows the average WSD accuracies ofwith-EM at 0, 25, 50 and 75 as the values of ?.The each curve represents our proposed algorithmwith threshold value ?
in the parenthesis.
Theaccuracy in the case of ?
= 75 is higher than thatof?
= 50 over 20 percentage data added point.This result suggests that as the number of handlabeled negative examples increasing, ?
shouldbe gradually decreasing, that is, the number ofpseudo negative examples should be decreasing.Because, if sufficient number of hand labelednegative examples exist, a classifier does not needpseudo negative examples.
The control of?depending on the number of hand labeled examplesduring active learning iterations is a future issue.7678808284868890920 10 20 30 40 50 60 70 80 90 100?=   0.0?= 25.0?= 50.0?= 75.0Figure 4: Dependency of threshold value ?ReferencesChan, Y. S. and Ng, H. T. 2007.
Domain Adaptationwith Active Learning for Word Sense Disambigua-tion.
Proc.
of ACL 2007, 49-56.Chen, J., Schein, A., Ungar, L., and Palmer, M. 2006.An Empirical Study of the Behavior of ActiveLearning for Word Sense Disambiguation, Proc.
ofthe main conference on Human Language Tech-nology Conference of the North American Chapterof ACL, pp.
120-127.McCallum, A. and Nigam, K. 1998.
Employing EMand Pool-Based Active Learning for Text Classifi-cation.
Proceedings of the Fifteenth internationalConference on Machine Learning, 350-358.Nigam, K., McCallum, A., Thrun, S., and Mitchell, T.2000.
Text Classification from Labeled and Unla-beled Documents using EM, Machine Learning, 39,103-134.Takayama, Y., Imamura, M., Kaji N., Toyoda, M. andKitsuregawa, M. 2009.
Active Learning withPseudo Negative Examples for Word Sense Dis-ambiguation in Web Mining (in Japanese), Journalof IPSJ (in printing).64
