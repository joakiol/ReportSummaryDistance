Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language, pages 20?27,Gothenburg, Sweden, April 26, 2014.c?2014 Association for Computational LinguisticsAutomatic detection of causal relations in German multilogsTina B?ogel Annette Hautli-Janisz Sebastian Sulger Miriam ButtDepartment of LinguisticsUniversity of Konstanzfirstname.lastname@uni-konstanz.deAbstractThis paper introduces a linguistically-motivated, rule-based annotation systemfor causal discourse relations in transcriptsof spoken multilogs in German.
The over-all aim is an automatic means of determin-ing the degree of justification provided bya speaker in the delivery of an argumentin a multiparty discussion.
The systemcomprises of two parts: A disambiguationmodule which differentiates causal con-nectors from their other senses, and a dis-course relation annotation system whichmarks the spans of text that constitute thereason and the result/conclusion expressedby the causal relation.
The system is eval-uated against a gold standard of Germantranscribed spoken dialogue.
The resultsshow that our system performs reliablywell with respect to both tasks.1 IntroductionIn general, causality refers to the way of know-ing whether one state of affairs is causally relatedto another.1Within linguistics, causality has longbeen established as a central phenomenon for in-vestigation.
In this paper, we look at causalityfrom the perspective of a research question frompolitical science, where the notion is particularlyimportant when it comes to determining (a.o.)
thedeliberative quality of a discussion.
The notion ofdeliberation is originally due to Habermas (1981),who assumes that within a deliberative democ-racy, stakeholders participating in a multilog, i.e.a multi-party conversation, justify their positionstruthfully, rationally and respectfully and eventu-ally defer to the better argument.
Within polit-ical science, the question arises whether actual1This work is part of the BMBF funded eHumanitiesproject VisArgue, an interdisciplinary cooperation betweenpolitical science, computer science and linguistics.multilogs conducted in the process of a demo-cratic decision making indeed follow this idealand whether/how one can use automatic means toanalyze the degree of deliberativity of a multilog(Dryzek (1990; 2000), Bohman (1996), Gutmannand Thompson (1996), Holzinger and Landwehr(2010)).
The disambiguation of causal discoursemarkers and the determination of the relations theyentail is a crucial aspect of measuring the delibera-tive quality of a multilog.
In this paper, we developa system that is designed to perform this task.We describe a linguistically motivated, rule-based annotation system for German which disam-biguates the multiple usages of causal discourseconnectors in the language and reliably annotatesthe reason and result/conclusion relations that theconnectors introduce.
The paper proceeds as fol-lows: Section 2 briefly reviews related work on theautomatic extraction and annotation of causal rela-tions, followed by a set of examples that illustratesome of the linguistic patterns in German (Sec-tion 3).
We then introduce our rule-based anno-tation system (Section 4) and evaluate it against ahand-crafted gold standard in Section 5, where wealso present the results from the same annotationtask performed by a group of human annotators.In Section 6, we provide an in-depth system erroranalysis.
Section 7 concludes the paper.2 Related workThe automatic detection and annotation of causal-ity in language has been approached from variousangles, for example by providing gold-standard,(manually) annotated resources such as the PennDiscourse Treebank for English (Prasad et al.,2008), which was used, e.g., in the disambigua-tion of English connectives by Pitler and Nenkova(2009), the Potsdam Commentary Corpus for Ger-man (Stede, 2004) and the discourse annotationlayer of Tu?ba-D/Z, a corpus of written Germantext (Versley and Gastel, 2012).
Training auto-20matic systems that learn patterns of causality (Doet al., 2011; Mulkar-Mehta et al., 2011b, interalia) is a crucial factor in measuring discoursecoherence (Sanders, 2005), and is beneficial inapproaches to question-answering (Girju, 2003;Prasad and Joshi, 2008).With respect to automatically detecting causalrelations in German, Versley (2010) uses Englishtraining data from the Penn Discourse Treebank inorder to train an English annotation model.
TheseEnglish annotations can be projected to Germanin an English-German parallel corpus and on thebasis of this a classifier of German discourse rela-tions is trained.
However, as previous studies haveshown (Mulkar-Mehta et al., 2011a, inter alia), thereliability of detecting causal relations with auto-matic means differs highly between different gen-res.
Our data consist of transcriptions of originallyspoken multilogs and this type of data differs sub-stantially from newspaper or other written texts.Regarding the disambiguation of German con-nectives, Schneider and Stede (2012) carried outa corpus study of 42 German discourse connec-tives which are listed by Dipper and Stede (2006)as exhibiting a certain degree of ambiguity.
Theirresults indicate that for a majority of ambigu-ous connectives, plain POS tagging is not reliableenough, and even contextual POS patterns are notsufficient in all cases.
This is the same conclu-sion drawn by Dipper and Stede (2006), who alsostate that off-the-shelf POS taggers are too unre-liable for the task.
They instead suggest a map-ping approach for 9 out of the 42 connectivesand show that this assists considerably with dis-ambiguation.
As this also tallies with our experi-ments with POS taggers, we decided to implementa rule-based disambiguation module.
This mod-ule takes into account contextual patterns and fea-tures of spoken communication and reliably de-tects causal connectors as well as the reason andresult/conclusion discourse relations expressed inthe connected clauses.3 Linguistic phenomenonIn general, causality can hold between singleconcepts, e.g.
between ?smoke?
and ?fire?, or be-tween larger phrases.
The phrases can be put intoa causal relation via overt discourse connectorslike ?because?
or ?as?, whereas other phrases en-code causality implicitly by taking into accountworld knowledge about the connected events.
Inthis paper, we restrict ourselves to the analysis ofexplicit discourse markers; in particular we inves-tigate the eight most frequent German causal con-nectors, listed in Table 1.
The markers of reasonon the left head a subordinate clause that describesthe cause of an effect stated in the matrix clause(or in the previous sentence(s)).
The markers ofresult/conclusion on the other hand introduce aclause that describes the overall effect of a causecontained in the preceding clause/sentence(s).
Inthe genre of argumentation that we are workingwith, the ?results?
tend to be logical conclusionsthat the speaker sees as following irrevocably fromthe cause presented in the argument.Reason Result?because of?
?thus?da daherweil darumdenn deshalbzumal deswegenTable 1: German causal discourse connectorsThe sentences in (1) and (2) provide exam-ples of the phenomenon of explicit causal mark-ers in German in our multilogs.
Note that allof the causal markers in Table 1 connect a re-sult/conclusion with a cause/reason.
The differ-ence lies in which of these relations is expressedin the clause headed by the causal connector.The constructions in (1) and (2) exemplify this.2In (1), da ?since?
introduces the reason for the con-clusion in the matrix clause, i.e., the reason forthe travel times being irrelevant is that they are notcarried out as specified.
In (2), daher ?thus?
headsthe conclusion of the reason which is provided inthe matrix clause: Because the speaker has neverstated a fact, the accusation of the interlocutor isnot correct.There are several challenges in the automaticannotation of these relations.
First, some of theconnectors can be ambiguous.
In our case, fourout of the eight causal discourse connectors in Ta-ble 1 are ambiguous (da, denn, daher and darum)and have, in addition to their causal meaning, tem-poral, locational or other usages.
In example (3),denn is used as a particle signaling disbelief, whiledaher is used as a locational verb particle, having,together with the verb ?to come?, the interpretation2These examples are taken from the Stuttgart 21 arbitra-tion process, see section 5.1 for more information.21(1) Diese Fahrzeiten sind irrelevant, da sie so nicht gefahren werden.Art.Dem travel time.Pl be.3.Pl irrelevant because they like not drive.Perf.Part be.Fut.3.PlResult/Conclusion Reason?These travel times are irrelevant, because they are not executed as specified.?
(2) Das habe ich nicht gesagt, daher ist Ihr Vorwurf nicht richtigPron have.Pres.1.Sg I not say.Past.Part thus be.3.Sg you.Sg.Pol/Pl accusation not correctReason Result/Conclusion?I did not say that, therefore your accusation is not correct.?
(3) Wie kommen Sie denn daher?how come.Inf you.Sg.Pol then VPart?What is your problem anyway??
(lit.
?In what manner are you coming here??
)(4) Da bin ich mir nicht sicher.there be.Pres.1.Sg I I.Dat not sure?I?m not sure about that.?
(5) Das kommt daher, dass keiner etwas sagt.Pron come.Pres.3.Sg thus that nobody something say.Pres.3.SgResult/Conclusion Reason?This is because nobody says anything.
?of ?coming from somewhere to where the speakeris?
(literally and metaphorically).
In a second ex-ample in (4), da is used as the pronominal ?there?.Second, some of the causal connectors do notalways work the same way.
In (5), the re-sult/conclusion connector daher does not headan embedded clause, rather it is part of thematrix clause.
In this case, the embeddedclause expresses the reason rather than the re-sult/conclusion.
A third challenge is the span ofthe respective reason and result.
While there aresome indications as to how to define the stretchof these spans, there are some difficult challenges,further discussed in the error analysis in Section 6.In the following, we present the rule-based an-notation system, which deals with the identifica-tion of phrases expressing the result and reason,along the lines illustrated in (1) and (2), as well aswith the disambiguation of causal connectors.4 Rule-based annotation systemThe automatic annotation system that we intro-duce is based on a linguistically informed, hand-crafted set of rules that deals with the disambigua-tion of causal markers and the identification ofcausal relations in text.
As a first step, we divideall of the utterances into smaller units of text in or-der to be able to work with a more fine-grainedstructure of the discourse.
Following the liter-ature, we call these discourse units.
Althoughthere is no consensus in the literature on what ex-actly a discourse unit consists of, it is generallyassumed that each discourse unit describes a sin-gle event (Polanyi et al., 2004).
Following Marcu(2000), we term these elementary discourse units(EDUs) and approximate the assumption made byPolanyi et al.
(2004) by inserting a boundary atevery punctuation mark and every clausal con-nector (conjunctions, complementizers).
Sentenceboundaries are additionally marked.The annotation of discourse information is per-formed at the level of EDUs.
There are sometimesinstances in which a given relation such as ?rea-son?
spans multiple EDUs.
In these cases, each ofthe EDUs involved is marked/annotated individu-ally with the appropriate relation.In the following, we briefly lay out the two ele-ments of the annotation system, namely the disam-biguation module and the system for identifyingthe causal relations.224.1 DisambiguationAs shown in the examples above, markers likeda, denn, darum and daher ?because/thus?
have anumber of different senses.
The results presentedin Dipper and Stede (2006) indicate that POS tag-ging alone does not help in disambiguating thecausal usages from the other functions, particu-larly not for our data type, which includes muchnoise and exceptional constructions that are notpresent in written corpora.
As a consequence, wepropose a set of rules built on heuristics, whichtake into account a number of factors in the clausein order to disambiguate the connector.
To il-lustrate the underlying procedure, (6) schematizespart of the disambiguation rule for the Germancausal connector da ?since?.
(6) IF da is not followed directly by a verb ANDno other particle or connector precedes daANDda is not late in the EDU THENda is a causal connector.In total, the system comprises of 37 rules thatdisambiguate the causal connectors shown in Ta-ble 1.
The evaluation in Section 5 shows that thesystem performs well overall.34.2 Relation identificationAfter disambiguation, a second set of rules anno-tates discourse units as being part of the reason orthe result portion of a causal relation.
One aspectof deliberation is the assumption that participantsin a negotiation justify their positions.
Therefore,in this paper, we analyze causal relations within a3Two reviewers expressed interest in being able to accessour full set of rules.
Their reasons were two-fold.
For one,sharing our rules would benefit a larger community.
For an-other, the reviewers cited concerns with respect to replicabil-ity.
With respect to the first concern, we will naturally behappy to share our rule set with interested researchers.
Withrespect to the second concern, it is not clear to us that wehave understood it.
As far as we can tell, what seems to be atthe root of the comments is a very narrow notion of replica-bility, one which involves a freely available corpus in combi-nation with a freely available automatic processing tool (e.g.,a machine learning algorithm) that can then be used togetherwithout the need of specialist language knowledge.
We freelyadmit that our approach requires specialist linguistic training,but would like to note that linguistic analysis is routinely sub-ject to replicability in the sense that given a set of data, thelinguistic analysis arrived at should be consistent across dif-ferent sets of linguists.
In this sense, our work is immediatelyreplicable.
Moreover, given the publically available S21 dataset and the easily accessible and comprehensive descriptionsof German grammar, replication of our work is eminentlypossible.single utterance of a speaker, i.e., causal relationsthat are expressed in a sequence of clauses whicha speaker utters without interference from anotherspeaker.
As a consequence, the annotation systemdoes not take into account causal relations that aresplit up between utterances of one speaker or ut-terances of different speakers.Nevertheless, the reason and result portionof a causal relation can extend over multipleEDUs/sentences and this means that not only EDUswhich contain the connector itself are annotated,but preceding/following units that are part of thecausal relation also have to be marked.
This in-volves deep linguistic knowledge about the cuesthat delimit or license relations, information whichis encoded in a set of heuristics that feed the 20 dif-ferent annotation rules and mark the relevant units.An example for a (simplified) relation annotationis given in (7).
(7) IF result connector not in first EDU of sen-tence ANDresult connector not preceded by other con-nector within same sentence THENmark every EDU from sentence beginning tocurrent EDU with reason.ELSIF result connector in first EDU of sen-tence THENmark every EDU in previous sentence withreason UNLESSencountering another connector.5 EvaluationThe evaluation is split into two parts.
On the onehand, we evaluate the inter-annotator agreementbetween five, minimally trained annotators (?5.2).On the other hand, we evaluate the rule-basedannotation system against this hand-crafted gold-standard (?5.3).
Each evaluation is again split intotwo parts: One concerns the successful identifica-tion of the causal connectors.
The other concernsthe identification of the spans of multilog that in-dicate a result/conclusion vs. a reason.5.1 DataThe underlying data comprises of two data sets,the development and the test set.
The develop-ment set, on which the above-mentioned heuristicsfor disambiguation and relation identification arebased, consists of the transcribed protocols of theStuttgart 21 arbitration process (henceforth: S21).This public arbitration process took place in 201023and was concerned with a railway and urban de-velopment project in the German city of Stuttgart.The project remains highly controversial and hasgained international attention.
In total, the tran-scripts contain around 265.000 tokens in 1330 ut-terances of more than 70 participants.4The test set is based on different, but also tran-scribed natural speech data, namely on experi-ments simulating deliberative processes for estab-lishing a governmental form for a hypotheticalnew African country.5For testing, we randomlycollected utterances from two versions of the ex-periment.
Each utterance contained at least twocausal discourse connectors.
In total, we extracted60 utterances with an average length of 71 words.There are a total of 666 EDUs and 105 instancesof the markers in Table 1.
The composition of thetest set for each (possible) connector is in Table 2.Reason Result?because of?
?due to?da 23 daher 10weil 17 darum 11denn 17 deshalb 12zumal 4 deswegen 11Total: 61 44Table 2: Structure of the evaluation setFor the creation of a gold standard, the test setwas manually annotated by two linguistic experts.238 out of 666 EDUs were marked as being partof the reason of a causal relation, with the re-sult/conclusion contributed by 180 EDUs.
Out of105 connectors found in the test set, 87 have acausal usage.
In 18 cases, the markers have otherfunctions.5.2 Inter-annotator agreementThe task for the annotators comprised of two parts:First, five students (undergraduates in linguistics)had to decide wether an occurence of one of theelements in Table 1 was a causal marker or not.In a second step, they had to mark the bound-aries for the reason and result/conclusion parts ofthe causal relation, based on the boundaries of theautomatically generated EDUs.
Their annotationchoice was not restricted by, e.g., instructing them4The transcripts are publicly available for down-load under http://stuttgart21.wikiwam.de/Schlichtungsprotokolle5These have been produced by our collaborators in polit-ical science, Katharina Holzinger and Valentin Gold.to choose a ?wider?
or more ?narrow?
span whenin doubt.
These tasks served two purposes: Onthe one hand, we were able to evaluate how easilycausal markers can be disambiguated from theirother usages and how clearly they introduce eitherthe reason or the result/conclusion of a causal re-lation.
On the other hand, we gained insights intowhat span of discourse native speakers take to con-stitute a result/conclusion and cause/reason.For calculating the inter-annotator agreement(IAA), we used Fleiss?
kappa (Fleiss, 1971), whichmeasures the reliability of the agreement betweenmore than two annotators.
In the disambiguationtask, the annotators?
kappa is ?
= 0.96 (?almostperfect agreement?
), which shows that the annota-tors exhibit a high degree of confidence when dif-ferentiating between causal and other usages of themarkers.
When marking whether a connector an-notates the reason or the result/conclusion portionof a causal relation, the annotators have a kappaof ?
= 0.86.
This shows that not only are anno-tators capable of reliably disambiguating connec-tors, they are also reliably labeling each connectorwith the correct causal relation.In evaluating the IAA of the spans, we mea-sured three types of relations (reason, result andno causal relation) over the whole utterance, i.e.each EDU which is neither part of the result nor thereason relation was tagged as having no causal re-lation.
We calculated four different ?
values: onefor each relation type (vs. all other relation types),and one across all relation types.
The IAA fig-ures are summarized in Table 3: For the causalrelation types, ?Reason=0.86 and ?Result=0.90 in-dicate near-perfect agreement.
?
is significantlyhigher for causal EDUs than for non-causal (i.e.,unmarked) EDUs (?Non-causal=0.82); this is in factexpected since causal EDUs are the marked caseand are thus easier to identify for annotators in acoherent manner.IAA?Reason0.86?Result0.90?Non-causal0.82?All0.73Table 3: IAA of span annotationsAcross all relation types, ?All=0.73 indicates?substantial agreement?.
The drop in the agree-ment is anticipated and mirrors the problem that24is generally found in the literature when evalu-ating spans of discourse relations (Sporleder andLascarides, 2008).
First, measuring ?Allinvolvesthree categories, whereas the other measures in-volve two.
Second, a preliminary error analysisshows that there is substantial disagreement re-garding the extent of both reason and result spans.The examples in (8)?
(9) illustrate this.
While an-notator 1 marks the result span (indicated by the( S tag) as starting at the beginning of the sentence,annotator 2 excludes the first EDU from the resultspan.6In such cases, we thus register a mismatchin the annotation of the first EDU.Nevertheless, the numbers indicate a substantialagreement.
We thus conclude that the task we setthe annotators could be accomplished reliably.5.3 System performanceIn order to evaluate the automatic annotation sys-tem described in Section 4, we match the systemoutput against the manually-annotated gold stan-dard, calculating precision, recall and (balanced)f-score of the annotation.
For the disambiguationof the connectors in terms of causal versus otherusages, the system performs as shown in Table 4(the ?
indicates the average of both values).Precision Recall F-scoreCausal 1 0.94 0.97Non-causal 0.85 1 0.92?
0.93 0.97 0.95Table 4: Causal marker disambiguationThis result is very promising and shows thateven though the development data consists of datafrom a different source, the patterns in the de-velopment set are mirrored in the test set.
Thismeans that the genre of the spoken exchange ofarguments in a multilog does not exhibit the dif-ferences usually found when looking at data fromdifferent genres, as Mulkar-Mehta et al.
(2011a)report when comparing newspaper articles from fi-nance and sport.For evaluating the annotated spans of reasonand result, we base the calculation on whether anEDU is marked with a particular relation or not, i.e.if the system marks an EDU as belonging to thereason or result part of a particular causal markerand the gold standard encodes the same informa-tion, then the two discourse units match.
As a con-6We use the | sign to indicate EDU boundaries.sequence, spans which do not match perfectly, forexample in cases where their boundaries do notmatch, are not treated as non-matching instancesas a whole, but are considered to be made up ofsmaller units which match individually.
Table 5shows the results.Precision Recall F-scoreReason 0.88 0.75 0.81Result 0.81 0.94 0.87?
0.84 0.84 0.84Table 5: Results for relation identificationThese results are promising insofar as the de-tection of spans of causal relations is known to bea problem.
Again, this shows that developmentand test set seem to exhibit similar patterns, de-spite their different origins (actual political argu-mentation vs. an experimental set-up).
In the fol-lowing, we present a detailed error analysis andshow that we find recurrent patterns of mismatch,most of which can in principle be dealt with quitestraightforwardly.6 Error analysisFigure 1: Error analysis, in percent.Figure 1 shows a pie chart in which each prob-lem is identified and shown with its share inthe overall error occurrence.
In total, the sys-tem makes 26 annotation errors.
Starting fromthe top, empty connector position refers to struc-tures which an annotator can easily define as rea-son/result, but which do not contain an overt con-nector.
This causes the automatic annotation sys-25(8) Annotator 1:( S Ich mo?chte an dieser Stelle einwerfen, | dass die Frage, ob ...I would like.Pres.1.Sg at this point add.Inf that the question if ...?I?d like to add at this point that the question if...(9) Annotator 2:Ich mo?chte an dieser Stelle einwerfen, | ( S dass die Frage, ob ...I would like.Pres.1.Sg at this point add.Inf that the question if ...?I?d like to add at this point that the question if...tem to fail.
The group of other connectors refers tocases where a non-causal connector (e.g., the ad-versative conjunction aber ?but?)
signals the endof the result/conclusion or cause span for a humanannotator.
The presence of these other connectorsand their effect is not yet taken into account by theautomatic annotation system.
The error group iaarefers to the cases where we find a debatable dif-ference of opinion with respect to the length of aspan.
Speaker opinion refers to those cases wherea statement starts with expressions like ?I believe/ I think / in my opinion etc.?.
These are mostlyexcluded from a relation span by human anno-tators, but (again: as of yet) not by the system.Span over several sentences refers to those caseswhere the span includes several sentences.
Andlast, but not least, since the corpus consists of spo-ken data, an external transcriptor had to transcribethe speech signal into written text.
Some low-levelerrors in this category are missing sentence punc-tuation.
The human annotators were able to com-pensate for this, but not the automatic system.Roughly, three groups of errors can be distin-guished.
Some of the errors are relatively easyto solve, by, e.g., adding another class of con-nectors, by adding expressions or by correctingthe transcriptors script.
A second group (spanover several sentences and empty connector po-sition) needs a much more sophisticated system,including deep linguistic knowledge on semantics,pragmatics and notoriously difficult aspects of dis-course analysis like anaphora resolution.7 ConclusionIn conclusion, we have presented an automatic an-notation system which can reliably and preciselydetect German causal relations with respect toeight causal connectors in multilogs in which ar-guments are exchanged and each party is trying toconvince the other of the rightness of their stance.Our system is rule-based and takes into accountlinguistic knowledge at a similar level as that usedby human annotators.
Our work will directly ben-efit research in political science as it can flow intoproviding one measure for the deliberative qual-ity of a multilog, namely, do interlocutors supporttheir arguments with reasons or not?ReferencesJames Bohman.
1996.
Public Deliberation: Plural-ism, Complexity and Democracy.
The MIT Press,Cambridge, MA.Stefanie Dipper and Manfred Stede.
2006.
Disam-biguating potential connectives.
In Proceedings ofKONVENS (Conference on Natural Language Pro-cessing) 2006.Quang Xuan Do, Yee Seng Chan, and Dan Roth.
2011.Minimally Supervised Event Causality Identifica-tion.
In Proceedings of EMNLP?11, pages 294?303.John S. Dryzek.
1990.
Discursive Democracy: Poli-tics, Policy, and Political Science.
Cambridge Uni-versity Press, Cambridge, MA.John S. Dryzek.
2000.
Deliberative Democracy andBeyond: Liberals, Critics, Contestations.
OxfordUniversity Press, Oxford.Joseph L. Fleiss.
1971.
Measuring nominal scaleagreement among many raters.
Psychological Bul-letin, 76(5):378?382.Roxana Girju.
2003.
Automatic Detection of CausalRelations for Question-Answering.
In Proceedingsof the ACL Workshop on Multilingual summariza-tion and question-answering, pages 76?83.Amy Gutmann and Dennis Frank Thompson.
1996.Democracy and Disagreement.
Why moral conflictcannot be avoided in politics, and what should bedone about it.
Harvard University Press, Cam-bridge, MA.Ju?rgen Habermas.
1981.
Theorie des kommunikativenHandelns.
Suhrkamp, Frankfurt am Main.Katharina Holzinger and Claudia Landwehr.
2010.
In-stitutional determinants of deliberative interaction.European Political Science Review, 2:373?400.26Daniel Marcu.
2000.
The Theory and Practice ofDiscourse Parsing and Summarization.
MIT Press,Cambridge, Mass.Rutu Mulkar-Mehta, Andrew S. Gordon, Jerry Hobbs,and Eduard Hovy.
2011a.
Causal markers acrossdomains and genres of discourse.
In The 6th Inter-national Conference on Knowledge Capture.Rutu Mulkar-Mehta, Christopher Welty, Jerry R.Hoobs, and Eduard Hovy.
2011b.
Using granularityconcepts for discovering causal relations.
In Pro-ceedings of the FLAIRS conference.Emily Pitler and Ani Nenkova.
2009.
Using syntax todisambiguate explicit discourse connectives in text.In Proceedings of ACL-IJCNLP, pages 13?16.Livia Polanyi, Chris Culy, Martin van den Berg,Gian Lorenzo Thione, and David Ahn.
2004.
Sen-tential structure and discourse parsing.
In Proceed-ings of the 2004 ACL Workshop on Discourse Anno-tation, pages 80?87.Rashmi Prasad and Aravind Joshi.
2008.
A Discourse-based Approach to Generating Why-Questions fromTexts.
In Proceedings of the Workshop on the Ques-tion Generation Shared Task and Evaluation Chal-lenge.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind Joshi, and BonnieWebber.
2008.
The Penn Discourse Treebank 2.0.In Proceedings of LREC 2008, pages 2961?2968.Ted Sanders.
2005.
Coherence, Causality and Cog-nitive Complexity in Discourse.
In Proceedings ofSEM-05, First International Symposium on the Ex-ploratiaon and Modelling of Meaning, pages 105?114.Angela Schneider and Manfred Stede.
2012.
Ambi-guity in German Connectives: A Corpus Study.
InProceedings of KONVENS (Conference on NaturalLanguage Processing) 2012.Caroline Sporleder and Alex Lascarides.
2008.
Us-ing Automatically Labelled Examples to ClassifyRhetorical Relations: An Assessment.
Natural Lan-guage Engineering, 14(3):369?416.Manfred Stede.
2004.
The Potsdam Commentary Cor-pus.
In In Proceedings of the ACL?04 Workshop onDiscourse Annotation, pages 96?102.Yannick Versley and Anna Gastel.
2012.
LinguisticTests for Discourse Relations in the Tu?ba-D/Z Cor-pus of Written German.
Dialogue and Discourse,1(2):1?24.Yannick Versley.
2010.
Discovery of Ambiguous andUnambiguous Discourse Connectives via Annota-tion Projection.
In Workshop on the Annotation andExploitation of Parallel Corpora (AEPC).27
