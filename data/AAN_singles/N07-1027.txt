Proceedings of NAACL HLT 2007, pages 212?219,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsIs Question Answering Better Than Information Retrieval?Towards a Task-Based Evaluation Framework for Question SeriesJimmy LinCollege of Information StudiesDepartment of Computer ScienceInstitute for Advanced Computer StudiesUniversity of MarylandCollege Park, MD 20742, USAjimmylin@umd.eduAbstractThis paper introduces a novel evaluationframework for question series and em-ploys it to explore the effectiveness of QAand IR systems at addressing users?
infor-mation needs.
The framework is based onthe notion of recall curves, which char-acterize the amount of relevant informa-tion contained within a fixed-length textsegment.
Although it is widely assumedthat QA technology provides more effi-cient access to information than IR sys-tems, our experiments show that a simpleIR baseline is quite competitive.
These re-sults help us better understand the role ofNLP technology in QA systems and sug-gest directions for future research.1 IntroductionThe emergence of question answering (QA) hasbeen driven to a large extent by its intuitive appeal.Instead of ?hits?, QA technology promises to de-liver ?answers?, obviating the user from the tedioustask of sorting through lists of potentially-relevantdocuments.
The success of factoid QA systems,particularly in the NIST-sponsored TREC evalua-tions (Voorhees, 2003), has reinforced the percep-tion about the superiority of QA systems over tradi-tional IR engines.However, is QA really better than IR?
This workchallenges existing assumptions and critically exam-ines this question, starting with the development of anovel evaluation framework that better models usertasks and preferences.
The framework is then ap-plied to compare top TREC QA systems against anoff-the-shelf IR engine.
Surprisingly, experimentsshow that the IR baseline is quite competitive.
Theseresults help us better understand the added value ofNLP technology in QA systems, and are also usefulin guiding future research.2 Evolution of QA EvaluationAlthough most question answering systems rely oninformation retrieval technology, there has alwaysbeen the understanding that NLP provides signifi-cant added value beyond simple IR.
Even the earli-est open-domain factoid QA systems, which can betraced back to the late nineties (Voorhees and Tice,1999), demonstrated the importance and impact oflinguistic processing.
Today?s top systems deploya wide range of advanced NLP technology and cananswer over three quarters of factoid questions in anopen domain (Voorhees, 2003).
However, presentQA evaluation methodology does not take into ac-count two developments, discussed below.First, despite trends to the contrary in TREC eval-uations, users don?t actually like or want exact an-swers.
Most question answering systems are de-signed to pinpoint the exact named entity (person,date, organization, etc.)
that answers a particularquestion?and the development of such technologyhas been encouraged by the setup of the TREC QAtracks.
However, a study by Lin et al (2003) showsthat users actually prefer answers embedded withinsome sort of context, e.g., the sentence or the para-graph that the answer was found in.
Context pro-2123.
Hale Bopp comet1.
fact When was the comet discovered?2.
fact How often does it approach the earth?3.
list In what countries was the comet visi-ble on its last return?4.
other68.
Port Arthur Massacre1.
fact Where is Port Arthur?2.
fact When did the massacre occur?3.
fact What was the final death toll of themassacre?4.
fact Who was the killer?5.
fact What was the killer?s nationality?6.
list What were the names of the victims?7.
list What were the nationalities of the vic-tims?8.
otherTable 1: Sample question series.vides a means by which the user can establish thecredibility of system responses and also provides avehicle for ?serendipitous knowledge discovery?
?finding answers to related questions.
As the earlyTRECs have found (Voorhees and Tice, 1999), lo-cating a passage that contains an answer is consider-ably easier than pinpointing the exact answer.
Thus,real-world user preferences may erode the advantagethat QA has over IR techniques such as passage re-trieval, e.g., (Zobel et al, 1995; Tellex et al, 2003).Second, the focus of question answering researchhas shifted away from isolated factoid questions tomore complex information needs embedded withina broader context (e.g., a user scenario).
Since2004, the main task at the TREC QA tracks hasconsisted of question series organized around topics(called ?targets?
)?which can be people, organiza-tions, entities, or events (Voorhees, 2004; Voorhees,2005).
Questions in a series inquire about differ-ent facets of a target, but are themselves either fac-toid or list questions.
In addition, each series con-tains an explicit ?other?
question (always the lastone), which can be paraphrased as ?Tell me otherinteresting things about this target that I don?t knowenough to ask directly.?
See Table 1 for examplesof question series.
Separately, NIST has been ex-ploring other types of complex information needs,for example, the relationship task in TREC 2005and the ciQA (complex, interactive Question An-swering) task in TREC 2006 (Dang et al, 2006).One shared feature of these complex questions isthat they cannot be answered by simple named en-tities.
Answers usually span passages, which makesthe task very similar to the query-focused summa-rization task in DUC (Dang, 2005).
On these tasks,it is unclear whether QA systems actually outper-form baseline IR methods.
As one bit of evidence,in TREC 2003, a simple IR-based sentence rankeroutperformed all but the best system on definitionquestions, the precursor to current ?other?
ques-tions (Voorhees, 2003).We believe that QA evaluation methodology haslagged behind these developments and does not ade-quately characterize the performance of current sys-tems.
In the next section, we present an evaluationframework that takes into account users?
desire forcontext and the structure of more complex QA tasks.Focusing on question series, we compare the perfor-mance of top TREC systems to a baseline IR engineusing this evaluation framework.3 An Evaluation FrameworkQuestion series in TREC represent an attempt atmodeling information-seeking dialogues between auser and a system (Kato et al, 2004).
Primarilybecause dialogue systems are difficult to evaluate,NIST has adopted a setup in which individual ques-tions are evaluated in isolation?this implicitly mod-els a user who types in a question, receives an an-swer, and then moves on to the next question in theseries.
Component scores are aggregated using aweighted average, and no attempt is made to capturedependencies across different question types.Simultaneously acknowledging the challenges inevaluating dialogue systems and recognizing thesimilarities between complex QA and query-focusedsummarization, we propose an alternative frame-work for QA evaluation that considers the qualityof system responses as a whole.
Instead of gener-ating individual answers to each question, a systemmight alternatively produce a segment of text (i.e., asummary) that attempts to answer all the questions.This slightly different conception of QA brings itinto better alignment with recent trends in multi-213document summarization, which may yield previ-ously untapped synergies (see Section 7).To assess the quality of system responses,we adopt the nugget-based methodology usedpreviously for many types of complex ques-tions (Voorhees, 2003), which shares similaritieswith the pyramid evaluation scheme used in sum-marization (Nenkova and Passonneau, 2004).
Anugget can be described as an ?atomic fact?
that ad-dresses an aspect of an information need.
Instead ofthe standard nugget F-score, which hides importanttradeoffs between precision and recall, we proposeto measure nugget recall as a function of responselength.
The goal is to quantify the number of rel-evant facts that a user will have encountered afterreading a particular amount of text.
Intuitively, wewish to model how quickly a hypothetical user could?learn?
about a topic by reading system responses.Within this framework, we compared existingTREC QA systems against an IR baseline.
Pro-cessed outputs from the top-ranked, second-ranked,third-ranked, and median runs in TREC 2004 andTREC 2005 were compared to a baseline IR rungenerated by Lucene, an off-the-shelf open-sourceIR engine.
Our experiments focused on factoid and?other?
questions; as the details differ for these twotypes, we describe each separately and then return toa unified picture.4 Factoid SeriesOur first set of experiments focuses on the factoidquestions within a series.
In what follows, we de-scribe the data preparation process, the evaluationmethodology, and experimental results.4.1 Data PreparationWe began by preparing answer responses from thetop-ranked, second-ranked, third-ranked, and me-dian runs from TREC 2004 and TREC 2005.1 Con-sider the third-ranked run from TREC 2004 as a run-ning example; for the two factoid questions in tar-get 3 (Table 1), the system answers were ?July 22,1995?
and ?4,200 years?
(both correct).Since Lin et al (2003) suggest that users preferanswers situated within some sort of context, we1In cases where teams submitted multiple runs, we consid-ered only the best performing of each.projected these exact answers onto their source sen-tences.
This was accomplished by selecting the firstsentence in the source document (drawn from theAQUAINT corpus) that contains the answer string.2In our example, this procedure yielded the followingtext segment:The comet was named after its two observers?twoamateur astronomers in the United States who dis-covered it on July 22, 1995.
Its visit to the solarsystem?just once every 4,200 years, will give mil-lions of people a rare heavenly treat when it reachesits full brightness next year.Since projected sentences are simply concate-nated, the responses often exhibit readability prob-lems (although by chance this particular response isrelatively coherent).
Nevertheless, one might imag-ine that such output forms the basis for generatingcoherent query-focused summaries with sentence-rewrite techniques, e.g., (Barzilay et al, 1999).
Inthis work, we set aside problems with fluency sinceour evaluation framework is unable to measure this(desirable) characteristic.System responses were prepared for four runsfrom TREC 2004 and four runs from TREC 2005in the manner described above.
As a baseline, weemployed Lucene to retrieve the top 100 documentsfrom the AQUAINT corpus using the target as thequery (in our example, ?Hale Bopp comet?).
Fromthe result set, we retained all sentences that containat least a term from the target.
Sentence order withineach document and across the ranked list was pre-served.
Answer responses for this baseline condi-tion were limited to 10,000 characters.
FollowingTREC convention, all character counts include onlynon-whitespace characters.
Finally, since responsesprepared from TREC runs were significantly shorterthan this baseline condition, the baseline Lucene re-sponse was appended to the end of each TREC runto fill a quota of 10,000 characters.4.2 Evaluation MethodologyOur evaluation framework is designed to measurethe amount of useful information contained in a sys-tem response.
For factoid series, this can be quan-2As a backoff, if the exact answer string is not found in thetext, the sentence with the most terms in common with the an-swer string is selected.2140 0.1 0.2 0.3 0.4 0.5 0.6 0.70.8 0.910  100  1000  10000recall length of response (non-whitespace characters)Evaluation of TREC 2004 factoid questionstop-ranked run2nd-ranked run3rd-ranked runmedian runLucene0 0.1 0.2 0.3 0.4 0.5 0.6 0.70.8 0.910  100  1000  10000recall length of response (non-whitespace characters)Evaluation of TREC 2005 factoid questionstop-ranked run2nd-ranked run3rd-ranked runmedian runLuceneFigure 1: Factoid recall curves for runs from TREC 2004 (left) and TREC 2005 (right).Run 2004 2005top-ranked run 0.770 0.7132nd-ranked run 0.643 0.6663rd-ranked run 0.626 0.326median run 0.170 0.177Table 2: Official scores of selected TREC 2004 andTREC 2005 factoid runs.tified by recall?the fraction of questions withina series whose answers could be found within agiven passage.
By varying the passage length, wecan characterize systems in terms of recall curvesthat represent how quickly a hypothetical user can?learn?
about the target.
Below, we describe the im-plementation of such a metric.First, we need a method to automatically deter-mine if an answer string is contained within a seg-ment of text.
For this, regular expression answerpatterns distributed by NIST were employed?theyhave become a widely-accepted evaluation tool.Second, we must determine when a fact is ?ac-quired?
by our hypothetical user.
Since previousstudies suggest that context is needed to interpret ananswer, we assess system output on a sentence-by-sentence basis.
In our example, the lengths of thetwo sentences are 105 and 130 characters, respec-tively.
Thus, for this series, we obtain a recall of 0.5at 105 characters and 1.0 at 235 characters.Finally, we must devise a method for aggregatingacross different question series to factor out vari-ations.
We accomplish this through interpolation,much in the same way that precision?recall curvesare plotted in IR experiments.
First, all lengths areinterpolated to their nearest larger fifty character in-crement.
In our case, they are 150 and 250.
Oncethis is accomplished for each question series, we candirectly average across all question series at eachlength increment.
Plotting these points gives us arecall-by-length performance curve.4.3 ResultsResults of our evaluation are shown in Figure 1, forTREC 2004 (left) and TREC 2005 (right).
Theseplots have a simple interpretation?curves that risefaster and higher represent ?better?
systems.
The?knee?
in some of the curves indicate approximatelythe length of the original system output (recallthat the baseline Lucene run was appended to eachTREC run to produce responses of equal lengths).For reference, official factoid scores of the same runsare shown in Table 2.Results from TREC 2004 are striking: while thetop three systems appear to outperform the baselineIR run, it is unclear if the median system is betterthan Lucene, especially at longer response lengths.This suggests that if a user wanted to obtain answersto a series of factoid questions about a topic, usingthe median QA system isn?t any more efficient thansimply retrieving a few articles using an IR engineand reading them.
Turning to the 2005 results, themedian system fares better when compared to theIR baseline, although the separation between the topand median systems has narrowed.In the next two sections, we present additional ex-periments on question series.
A detailed analysis issaved for Section 7.2150 0.001 0.002 0.003 0.0040.005 0.00610  100  1000  10000POURPRE recall length of response (non-whitespace characters)Evaluation of TREC 2004 "other" questionstop-ranked run2nd-ranked run3rd-ranked runmedian runLucene0 0.001 0.002 0.003 0.0040.005 0.00610  100  1000  10000POURPRE recall length of response (non-whitespace characters)Evaluation of TREC 2005 "other" questionstop-ranked run2nd-ranked run3rd-ranked runmedian runLuceneFigure 2: POURPRE recall curves for ?other?
runs from TREC 2004 (left) and TREC 2005 (right).Run 2004 2005top-ranked run 0.460 0.2482nd-ranked run 0.404 0.2323rd-ranked run 0.367 0.228median run 0.197 0.152Table 3: Official scores of selected TREC 2004 andTREC 2005 ?other?
runs.5 ?Other?
QuestionsOur second set of experiments examine the perfor-mance of TREC systems on ?other?
questions.
Onceagain, we selected the top-ranked, second-ranked,third-ranked, and median runs from TREC 2004 andTREC 2005.
Since system submissions were al-ready passages, no additional processing was nec-essary.
The IR baseline was exactly the same as therun used in the previous experiment.
Below, we de-scribe the evaluation methodology and results.5.1 Evaluation MethodologyThe evaluation of ?other?
questions closely mir-rors the procedure developed for factoid series.
Weemployed POURPRE (Lin and Demner-Fushman,2005), a recently developed method for automati-cally evaluating answers to complex questions.
Themetric relies on n-gram overlap as a surrogate formanual nugget matching, and has been shown to cor-relate well with official human judgments.
We mod-ified the POURPRE scoring script to return only thenugget recall (of vital nuggets only).Formally, systems?
responses to ?other?
questionsconsist of unordered sets of answer strings.
We de-cided to break each system?s response into individ-ual answer strings and compute nugget recall on astring-by-string basis.
Since these answer stringsare for the most part sentences, results are compara-ble to the factoid series experiments.
Taking answerstrings as the basic response unit also makes sensebecause it respects segment boundaries that are pre-sumably meaningful to the original systems.Computing POURPRE recall at different responselengths yielded an uninterpolated data series foreach topic.
Results across topics were aggregatedin the same manner as the factoid series: first byinterpolating to the nearest larger fifty-character in-crement, and then averaging all topics across eachlength increment.35.2 ResultsResults of our experiment are shown in Figure 2.
Forreference, the official nugget F-scores of the TRECruns are shown in Table 3.
Most striking is the ob-servation that the baseline Lucene run is highly com-petitive with submitted TREC systems.
For TREC2004, it appears that the IR baseline outperforms allbut the top two systems at higher recall levels.
ForTREC 2005, differences between all the analyzedruns are difficult to distinguish.
Although scoresof submitted runs in TREC 2005 were more tightlyclustered, the strong baseline IR performance is sur-prising.
For ?other?
questions, it doesn?t appear thatQA is better than IR!We believe that relative differences in QA and IR3It is worth noting that this protocol treats the answer stringsas if they were ordered?but we do not believe this has an im-pact on the results or our conclusions.2160 0.001 0.002 0.003 0.004 0.005 0.0060.007 0.00810  100  1000  10000POURPRE recall length of response (non-whitespace characters)Evaluation of TREC 2004 factoid and "other" questionstop-ranked run2nd-ranked run3rd-ranked runmedian runLucene0 0.001 0.002 0.003 0.004 0.005 0.0060.007 0.00810  100  1000  10000POURPRE recall length of response (non-whitespace characters)Evaluation of TREC 2005 factoid and "other" questionstop-ranked run2nd-ranked run3rd-ranked runmedian runLuceneFigure 3: POURPRE recall curves for runs from TREC 2004 (left) and TREC 2005 (right), combining bothfactoid and ?other?
questions.performance between the 2004 and 2005 test setscan be attributed to the nature of the targets.
InTREC 2005, allowable semantic categories of tar-gets were expanded to include events such as ?MissUniverse 2000 crowned?, which by their very natureare narrower in scope.
This, combined with manyhighly-specific targets, meant that the corpus con-tained fewer topically-relevant documents for eachtarget to begin with.
As a result, an IR-based sen-tence extraction approach performs quite well?thisexplanation is consistent with the observations ofLin and Demner-Fushman (2006).6 Combining Question TypesIn the previous two sections, factoid and ?other?questions were examined in isolation, which ignorestheir complementary role in supplying informationabout a target.
To provide a more complete pic-ture of system performance, we devised a method bywhich both question types can be evaluated together.At the conceptual level, there is little differencebetween factoid and ?other?
questions.
The first typeasks for explicit facts, while the second type asksfor facts that the user didn?t know enough to askabout directly.
We can unify the evaluation of bothtypes by treating regular expression factoid patternsas if they were (vital) nuggets.
Many patterns don?tcontain any special symbols, and read quite likenugget descriptions already.
For others, we man-ually converted regular expressions into plain text,e.g., ?
(auto|car) crash?
becomes ?auto car crash?.To validate this method, we first evaluated fac-toid series using POURPRE, with nugget descrip-tions prepared from answer patterns in the mannerdescribed above.
For both TREC 2004 and TREC2005, we did not notice any qualitative differencesin the results, suggesting that factoid answers canindeed be treated like nuggets.We then proceeded to evaluate both factoid and?other?
questions together using the above proce-dure.
Runs were prepared by appending the 1st?other?
run to the 1st factoid run, the 2nd ?other?run to the 2nd factoid run, etc.4 The Lucene base-line run remained the same as before.Plots of POURPRE recall by answer length areshown in Table 3.
These graphs provide a more com-plete picture of QA performance on question series.The same trends observed in the two previous exper-iments are seen here also: it does not appear that themedian run in TREC 2004 performs any better thanthe IR baseline.
Considering the TREC 2005 runs,the IR baseline remains surprisingly competitive.Note that integration of list questions, the thirdcomponent of question series, remains a challenge.Whereas the answer to a factoid question can be nat-urally viewed as a vital nugget describing the target,the relative importance of a single answer instance toa list question cannot be easily quantified.
We leavethis issue for future work.7 DiscussionIt can be argued that quantitative evaluation is thesingle most important driver for advancing the state4Note that we?re mixing sections from different runs, sothese do not correspond to any actual TREC submissions.217of the art in language processing technology today.As a result, evaluation metrics and methodologiesneed to be carefully considered to insure that theyprovide proper guidance to researchers.
Along theselines, this paper makes two arguments: that recallcurves better capture aspects of complex QA tasksthan the existing TREC evaluation metrics; and thatthis novel evaluation framework allows us to explorethe relationship between QA and IR technology in amanner not possible before.7.1 Advantages of Recall CurvesWe see several advantages to the evaluation frame-work introduced here, beyond those already dis-cussed in Sections 2 and 3.Previously, QA and IR techniques were not di-rectly comparable since they returned different re-sponse units.
To make evaluation even more com-plex, different types of questions (e.g., factoid vs.?other?)
require different metrics?in TREC, theseincomparable values were then aggregated based onarbitrary weights to produce a final composite score.By noting similarities between factoid answers andnuggets, we were able to develop a unified evalu-ation framework for factoid and ?other?
questions.By emphasizing the similarities between complexQA and summarization, it becomes possible to com-pare QA and IR technology directly?this work pro-vides a point of reference much in the same way thatIR-based sentence extraction has served as a startingpoint for summarization research, e.g., (Goldstein etal., 1999).In addition, characterizing system performance interms of recall curves allows researchers to com-pare the effectiveness of systems under different taskmodels.
Measuring recall at short response lengthsmight reflect time-constrained scenarios, e.g., pro-ducing an action-oriented report with a 30-minutedeadline.
Measuring recall at longer responselengths might correspond to in-depth research, e.g.,writing a summary article due by the end of the day.Recall curves are able to capture potential systemtradeoffs that might otherwise be hidden in single-point metrics.7.2 Understanding QA and IRBeyond answering a straightforward question, theresults of our experiments yield insights about therelationship between QA and IR technology.Most question answering systems today employ atwo-stage architecture: IR techniques are first usedto select a candidate set of documents (or alter-natively, passages, sentences, etc.
), which is thenanalyzed by more sophisticated NLP techniques.For factoids, analysis usually involves named-entityrecognition using some sort of answer type ontol-ogy; for ?other?
questions, analysis typically in-cludes filtering for definitions based on surface pat-terns and other features.
The evaluation frameworkdescribed in this paper is able to isolate the per-formance contribution of this second NLP stage?which corresponds to the difference between thebaseline IR and QA recall curves.For factoid questions, NLP technology providesa lot of added value: the set of techniques devel-oped for pinpointing exact answers allows users toacquire information more quickly than they other-wise could with an IR system (shown by Figure 1).The added value of NLP techniques for answering?other?
questions is less clear?in many instances,those techniques do not appear to be contributingmuch (shown by Figure 2).
Whereas factoid QAtechnology is relatively mature, researchers havemade less progress in developing general techniquesfor answering complex questions.Our experiments also illuminate when exactly QAworks.
For short responses, there is little differ-ence between QA and IR, or between all QA sys-tems for that matter, since it is difficult to crammuch information into a short response with cur-rent (extractive) technology.
For extremely long re-sponses, the advantages provided by the best QAsystems are relatively small, since there?s an upperlimit to their accuracy (and researchers have yet todevelop a good backoff strategy).
In the middlerange of response lengths is where QA technologyreally shines?where a user can much more effec-tively gather knowledge using a QA system.7.3 Implications for Future ResearchBased on the results presented here, we suggest twofuture directions for the field of question answering.First, we believe there is a need to focus on an-swer generation.
High-precision answer extractionalone isn?t sufficient to address users?
complex in-formation needs?information nuggets must be syn-218thesized and presented for efficient human consump-tion.
The coherence and fluency of system responsesshould be factored into the evaluation methodologyas well.
In this regard, QA researchers have muchto learn from the summarization community, whichhas already grappled with these issues.Second, more effort is required to developed task-based QA evaluations.
The ?goodness?
of answerscan only be quantified with respect to a task?examples range from winning a game show (Clarkeet al, 2001) to intelligence gathering (Small et al,2004).
It is impossible to assess the real-worldimpact of QA technology without considering howsuch systems will be used to solve human problems.Our work takes a small step in this direction.8 ConclusionIs QA better than IR?
The short answer, somewhat toour relief, is yes.
But this work provides more thana simple affirmation.
We believe that our contribu-tions are two-fold: a novel framework for evaluatingQA systems that more realistically models user tasksand preferences, and an exploration of QA and IRperformance within this framework that yields newinsights about these two technologies.
We hope thatthese results are useful in guiding the developmentof future question answering systems.9 AcknowledgmentsThis work has been supported in part by DARPAcontract HR0011-06-2-0001 (GALE), and has bene-fited from discussions with Ellen Voorhees and HoaDang.
I would also like to thank Esther and Kiri fortheir loving support.ReferencesRegina Barzilay, Kathleen R. McKeown, and MichaelElhadad.
1999.
Information fusion in the contextof multi-document summarization.
In Proc.
of ACL1999.Charles Clarke, Gordon Cormack, and Thomas Lynam.2001.
Exploiting redundancy in question answering.In Proc.
of SIGIR 2001.Hoa Trang Dang, Jimmy Lin, and Diane Kelly.
2006.Overview of the TREC 2006 question answering track.In Proc.
of TREC 2006.Hoa Dang.
2005.
Overview of DUC 2005.
In Proc.
ofDUC 2005.Jade Goldstein, Mark Kantrowitz, Vibhu Mittal, andJaime Carbonell.
1999.
Summarizing text documents:Sentence selection and evaluation metrics.
In Proc.
ofSIGIR 1999.Tsuneaki Kato, Jun?ichi Fukumoto, Fumito Masui, andNoriko Kando.
2004.
Handling information accessdialogue through QA technologies?a novel challengefor open-domain question answering.
In Proc.
of theHLT-NAACL 2004 Workshop on Pragmatics of Ques-tion Answering.Jimmy Lin and Dina Demner-Fushman.
2005.
Automat-ically evaluating answers to definition questions.
InProc.
of HLT/EMNLP 2005.Jimmy Lin and Dina Demner-Fushman.
2006.
Willpyramids built of nuggets topple over?
In Proc.
ofHLT/NAACL 2006.Jimmy Lin, Dennis Quan, Vineet Sinha, Karun Bakshi,David Huynh, Boris Katz, and David R. Karger.
2003.What makes a good answer?
The role of context inquestion answering.
In Proc.
of INTERACT 2003.Ani Nenkova and Rebecca Passonneau.
2004.
Evaluat-ing content selection in summarization: The pyramidmethod.
In Proc.
of HLT/NAACL 2004.Sharon Small, Tomek Strzalkowski, Ting Liu, SeanRyan, Robert Salkin, Nobuyuki Shimizu, Paul Kan-tor, Diane Kelly, Robert Rittman, and Nina Wacholder.2004.
HITIQA: towards analytical question answer-ing.
In Proc.
of COLING 2004.Stefanie Tellex, Boris Katz, Jimmy Lin, Gregory Marton,and Aaron Fernandes.
2003.
Quantitative evaluationof passage retrieval algorithms for question answering.In Proc.
of SIGIR 2003.Ellen M. Voorhees and DawnM.
Tice.
1999.
The TREC-8 question answering track evaluation.
In Proc.
ofTREC-8.Ellen M. Voorhees.
2003.
Overview of the TREC 2003question answering track.
In Proc.
of TREC 2003.Ellen M. Voorhees.
2004.
Overview of the TREC 2004question answering track.
In Proc.
of TREC 2004.Ellen M. Voorhees.
2005.
Using question series to evalu-ate question answering system effectiveness.
In Proc.of HLT/EMNLP 2005.Justin Zobel, Alistair Moffat, and Ross Wilkinson RonSacks-Davis.
1995.
Efficient retrieval of partial docu-ments.
IPM, 31(3):361?377.219
