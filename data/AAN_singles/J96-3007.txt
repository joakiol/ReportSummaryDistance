A Chart Re-estimation Algorithm for aProbabilistic Recursive TransitionNetworkYoung S. HawUniversity of SuwonKey-Sun Choi*Korea Advanced Institute of Science andTechnologyA Probabilistic Recursive Transition Network is an elevated version of a Recursive TransitionNetwork used to model and process context-free languages in stochastic parameters.
We presenta re-estimation algorithm for training probabilistic parameters, and show how efficiently it canbe implemented using charts.
The complexity of the Outside algorithm we present is O(N4G 3)where N is the input size and G is the number of states.
This complexity can be significantlyovercome when the redundant computations are avoided.
Experiments on the Penn tree corpusshow that re-estimation can be done more efficiently with charts.1.
IntroductionThough hidden Markov models have been successful in some applications uch ascorpus tagging, they are limited to the problems of regular languages.
There havebeen attempts to associate probabilities with context-free grammar formalisms.
Re-cently Briscoe and Carroll (1993) have reported work on generalized probabilistic LRparsing, and others have tried different formalisms uch as LTAG (Schabes, Roth, andOsborne 1993) and Link grammar (Lafferty, Sleator, and Temperley 1992).
Kupiec ex-tended a SCFG that worked on CNF to a general CFG (Kupiec 1991).
The re-estimationalgorithm presented in this paper may be seen as another version for general CFG.One significant problem of most probabilistic approaches i  the computationalburden of estimating the parameters (Lari and Young 1990).
In this paper, we considera probabilistic recursive transition network (PRTN) as an underlying rammar ep-resentation, and present an algorithm for training the probabilistic parameters, thensuggest an improved version that works with reduced redundant computations.
Thekey point is to save intermediate results and avoid the same computation later on.Moreover, the computation of Outside probabilities can be made only on the validparse space once a chart is prepared.2.
A Probabilistic Recursive Transition NetworkA PRTN denoted by A is a 6-tuple.= (A ,u ,s , ; : , r ,5 ) .
* Computer Science Department, University of Suwon, Suwon P.O.
Box 77-78, Suwon, 440-600, Korea.E-mail: yshan@world.kaist.ac.krComputer Science Department, Korea Advanced Institute of Science and Technology, Taejon, 305-701,Korea.
E-mail: kschoi@csking.kaist.ac.kr(~ 1996 Association for Computational LinguisticsComputational Linguistics Volume 22, Number 3CFG NP ~ ar t  AP  nounNP , AP nounNP ~ nounAP .
ad j  APAP  = adjPRTNCa l li States noun 0.2~.o o.8 ~p a i v ~ v o.~,,~.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
J : .....Figure 1Illustration of PRTN.
A parse is composed of dark-headed transitions.i Returni States~@A is a transition matrix containing transition probabilities, and B is a word matrixcontaining the probability distribution of the words observable at each terminal tran-sition.
I?
specifies the types of transitions, and ~ represents a stack.
S and .~ denotestart and final states, respectively.Stack operations are associated with transitions; transitions are classified into threetypes, according to the stack operation.
The first type is nonterminal transition, inwhich state identification is pushed into the stack.
The second type is pop transition, inwhich transition is determined by the content of the stack.
The third type is transitionsnot committed to stack operation; these are terminal and empty transitions.
In general,the grammar expressed in PRTN consists of layers.
A layer is a fragment of networkthat corresponds to a nonterminal.
A table of the probability distribution of words isdefined at each terminal transition.
Pop transitions represent the returning of a layerto one of its (possibly multiple) higher layers.In this paper, parses are assumed to be sequences of dark-headed transitions (seeFigure 1).
States at which pop transitions are defined are called pop states.
Othernotations are listed below.first(1) returns the first state of layer I.last(l) returns the last state of layer I.layer(s) returns the layer state s belongs to.bout(1) returns the states from which layer 1 branches out.bin(1) returns the states to which layer 1 returns.terminal(1) returns a set of terminal edges in layer 1.nonterminal(1) returns a set of nonterminal edges in layer I.denotes the edge between states i and j.\[i,j\] denotes the network segment between states i and j.Wa~b is a word sequence covering the a th to  b th word.422Han and Choi A Chart Re-estimation Algorithm3.
Re-estimation AlgorithmThe task of a re-estimation algorithm is to assign probabilities to transitions and theword symbols defined at each terminal transition.
The Inside-Outside algorithm pro-vides a formal basis for estimating parameters of context free languages so that theprobabilities of the word sequences (sample sentences) may be maximized.
The re-estimation algorithm for PRTN uses a variation of the Inside-Outside algorithm cus-tomized for PRTN.Let a word sequence of length N be denoted by:W = Wl Wa--.WN.Now define the Inside probability.Definition 1The Inside probability denoted by Pi(i)s~t of state i is the probability that layer(i)generates the string positioned from s to t starting at state i given a model ,~.That is:Pi(i)s~t = P(\[i,e\] --+ Ws~tl,~)where e = last(layer(i)).
And by definition:tP,(i)s~t = ~ aikb(T~, Ws)P,(k)s+l~t +~ Y2 aijauvP,(j)s~rP,(v)r+l~t.k j r=s(1)- - -+  ___+where ik E terminal(layer(i)), ij E nonterminal(layer(i) ), u = last(layer(j)), v E bin(layer(j)),and layer(i) = layer(v).After the last word is generated, the last state of layer(i) should be reached.1 if i = last(layer(i)),Pl(i)t+l~t = 0 otherwise.Figure 2 is the pictorial view of the Inside probability.
A valid sequence can be-gin only at state $, thus to be strict, P1(8) has an additional product, P($).
Whenthe immediate transition ~ is of terminal type, the transition probability aq and theprobability of the S th word at the transition b(~j, Ws) are multiplied together with theInside probability of the rest of the sequence, Ws+l~t.Now define the Outside probability.Definition 2The Outside probability denoted by Po (i,j)s~t is the probability that partial sequences,Wl~s_l and Wt+l~N, are  generated, provided that the partial sequence, Ws~t, is gen-erated by \[i,j\] given a model ~.And by definition:Po(i,j)s~t = P(\[,S',i\] --~ Wl~s_ l ,  ~',..~-\] -~- Wt+l~ N I A)N= ~ ~ ~ axfaeYPr(f'i)a~s-lPl(j)t+l~bPo(x,y)a~b ?x a=l  b=t(2)423Computational Linguistics Volume 22, Number 3T~layer(j) , .
, ,  = .
.
.
.W I~ t IFigure 2Illustration of Inside probability.,aye~Ix> _- ?
@layer(i) ~ .
.
.
.
~\[)_~.... =~_~.
.
.
.
_~w I'  s, I' '1'*' NIFigure 3Illustration of Outside probability.where x E bout(layer(i)), y E bin(layer(i)), f =first(layer(i)), e = last(layer(i)), layer(i) =layer(\]'), and layer(x) = layer(y).The summation on x is defined only when a ~ 1 or b ~ N (i.e., there are wordsleft to be generated).
Nonterminal and its corresponding pop transitions are definedto be 1 when a = 1 and b = N.For a boundary case of the Outside probability where f is the first state of a layerin the above equation:1 i f f  = $,Po(i, j) l~S = 0 otherwise.Figure 3 shows the network configuration in computing the Outside probability.
Inequation 2, P~(f, i)~s-1 is the probability that sequence, Wa~- l ,  is generated by layer(i)left to state i, and Pl(j)t+l~b is the probability that sequence Wt+l~b is generated bylayer(i) right to state j.The computation of P~ 0 c, i)s~t--a slight variation of the Inside probability in whichthe P~(f)a~b'S in equation 1 are replaced by P~0 c, i)a~b--is done as follows:P~(f,i)s~t{ pl0C)s~t if s __q t,= 1 if s > t andf  = i,0 if s > t andf?
i.It is basically the same as the Inside probability except hat it carries an i that indicatesa stop state.Now we can derive the re-estimation algorithm for .g and/3 using the Inside andOutside probabilities.
As the result of constrained maximization of Baum's auxiliary424Han and Choi A Chart Re-estimation Algorithmfunction, we have the following form of re-estimation for each transition (Rabiner1989).expected number of transitions from state i to state jexpected number of transitions from state iThe expectation of each transition type is computed as follows: For a terminal transi-tion:Y'~ffr=l aijb(~, W~)Po(i,j)~~~ Et( ij ) = P(W I A)For a nonterminal transition:N ~s=l Y~fft-s aijP,(j)~ta~Po( i, v)s~tEnt(~ ) = P(wlwhere u = last(layer(j)), v ?
bin(layer(j)), layer(i) = layer(v), layer(j) = layer(u), and uvis a pop transition.
For a pop transition:N N ~-~s=l ~-~t=s &,vPl(v)s~taijPo(u, J)sNt Go,( ij ) =P(W I ,k)where u E bout(layer(i)), j c bin(layer(i)), v = first(layer(i)), layer(u) = layer(j), layer(v) =layer(i), and uv is a nonterminal transition.Since transitions of terminal and nonterminal types can occur together at a state,terminal transitions are estimated as follows:- - -+a-ij Gk Et(ik) + Gk Ent(~) (3)For nonterminal transitions:E,,,( q)(4)And for pop transitions, notice that only pop transitions are possible at a pop state:L aq _ Go , (q )Y~k Epop( ik )(5)For a terminal transition ~ and a word symbol w:Ct  ~.,.
w,=~,, aijb(~, Wt)Po(i,j)t~t b( ij,w) =Y'~fft=~ aqb(~, Wt)Po(i,j)t~tThe re-estimation continues until the probability of the word sequences reaches acertain stability.425Computational Linguistics Volume 22, Number 3sentence WTCompute Inside probability 1ofW \[Inside TableSelect valid Insides by \ [ ~running Inside algorithmtopdownofvalid InsidesRun Outside algorithm \]IFigure 4Outside computation with chart.
Inside computation builds a table of computed Insides.4.
Chart Re-estimation AlgorithmIt can be shown that the complexity of the Inside algorithm is O(N3G 3) and that of theOutside algorithm is O(N4G 3) where N is the input size and G is the number of states.The complexity is too much for current workstations when either N or G becomesbigger than a few 10s.
A basic implementation f the algorithm is to use a chart andavoid doing the same computations more than once.
For instance, the table for storingInside computations takes O(N2G2C) store, where C is the number of terminal andnonterminal categories.
A chart item is a function of five parameters, and returns anInside probability.I(i,j, s, t, c) = Pi(i,j)s~t.A chart item is associated with categories implying that the item is valid on thespecified categories that begin the net fragment of the item.
Suppose a net fragment\[i,j\] begins with NP and ADJP, then given a sentence fragment Ws~t, ADJP may notparticipate in generating Ws~t, while NP may.
The information of valid categories iuseful when the chart is used in computing Outside probabilities.An Outside probability is the result of computing many Inside probabilities.
Com-puting an Inside probability even in an application of moderate size can be impractical.A naive implementation f Outside computation takes numerous Inside computations,so estimating even a parameter will not be realistic in a serial workstation (Lari andYoung 1990).The proposed estimation algorithm aims at reducing the redundant Inside com-putations in computing an Outside probability.
The idea is to identify the Inside prob-abilities used in generating an input sentence and to compute an Outside probabilityusing mainly those Insides.
This is done first by computing an Inside probability ofthe input sentence, which can return a table of Insides used in the computation.
Notethat the Insides in the deepest depth are produced first, as the recursion is released,thus there can be many Insides that are not relevant to the given sentence.
The Insidesthat participate in generating the input sentence can be identified by running the In-side algorithm one more time, top-down.
Figure 4 illustrates the steps of the revisedOutside computation.The identified Insides, however, do not cover all the Insides needed in computingan Outside probability.
This is because the Inside algorithm works on a network from426Han and Choi A Chart Re-estimation Algorithmleft to right and one transition at a time.
Many Insides that are missed in the table arecompositions of smaller Insides.Once charts of selected Insides are prepared, an Outside probability is computedas follows:Po(i,j)s~, = ~ ~ axfaevlOe, i,a,s - 1)I(j,e,t + 1, b)Po(x,y)a~b .
{xiI(x,y,a,b,c)>O} (a,b)ca(f,e,s,t)where x E bout(layer(i)), y c bin(layer(i)), f = first(layer(i)), e = last(layer(i)), c c{nonterminal}, ayer(i) = layer(j), and layer(x) = layer(y).The function cr0C, e,s, t) returns a set of (a, b) pairs where there are Inside itemsI0 c, e, a, b) defined at the chart such that a G s and b > t. In short, the items forstate f indicate the possible combinations of sentence segments inclusive of the givenfragment Ws~t because the chart contains items of all the valid sentence segments hatwere generated through the layer ~c, e\].
When the current layer ~c, e\] is completed withthe two Insides computed, the computation extends to the Outside.Useless advancements into high layers that do not lead to the successful comple-tion of a given sentence can be avoided by making sure that Ix, y\] generates Wa~b andthe category of current layer c is defined, which can be checked by consulting thechart items for state x.5.
ExperimentsThe goal of our experiments is to see how much saving the new estimation algorithmachieves in computational cost.
Out of 14,132 Wall Street Journal trees of the Penn treecorpus, 1,543 trees corresponding to sentences with 10 words or less were chosen, andthe programs written in C language were run at a Sparcl0 workstation.The basic implementation f an Inside-Outside algorithm assumes tables for In-sides and Outsides o that identical Insides and Outsides need not be recomputed.A chart Outside or re-estimation algorithm assumes a refined table of Insides thatcontains only valid Insides used in generating the input sentence as discussed earlier,and Outside computation is done based on the refined table.The improvement from the chart re-estimation algorithm is measured in the num-ber of actual Inside and Outside computations done to estimate the parameters.
Fig-ure 5 shows the average counts of Insides used in estimating 50 trees randomly selectedfrom 1,543 samples.
Before the re-estimation algorithm is applied, an RTN that faith-fully encodes the input trees without any overgeneration is constructed from the 50trees.
The gain in Insides from the chart re-estimation algorithm is very clear, and inthe case of Outsides the gain is even more conspicuous ( ee Figure 6).
The number ofInsides counted in chart version also includes the Insides computed in preparing thechart.6.
ConclusionWe have presented an efficient re-estimation algorithm for a PRTN that made useof only valid Insides.
The method requires the preparation of a chart by runningInside computation twice over a whole sentence.
The suggested method focuses mainlyon reducing the computational overhead of Outside computation, which is a majorportion of a re-estimation.
The computation of an Inside probability may be improvedfurther using a similar technique introduced in this paper.427Computational Linguistics Volume 22, Number 32000 I I I Iwithout chart o + J1500 wit r I ,Insides 10005OO00 2 4 6 8 10Sentence sizeFigure gGain in Insides using chart re-estimation, showing 50 randomly-chosen sentences out of 1,543samples.400 I I J I I I I |,1,350 without chart owith ch3O025OOutsides 20015010050o _ .
_ .
__ .7 -~ o .~ t I I I I I2 3 4 5 6 7 8 9 10Sentence sizeFigure 6Gain in Outsides using chart re-estimation with the same 50 sentences as in Figure 5.428Han and Choi A Chart Re-estimation AlgorithmReferencesgriscoe, Ted, and John Carroll.
1993.Generalized probabilistic LR parsing ofnatural anguage (Corpora) withunification-based grammars.
"Computational Linguistics 19(1): 25-57.Kupiec, Julian.
1991.
A trellis-basedalgorithm for estimating the parametersof a hidden stochastic ontext-freegrammar.
In Proceedings ofthe Speech andNatural Language Workshop, ages 241-246,DARPA, Pacific Grove.Lafferty, John, Daniel Sleator, and DavyTemperley.
1992.
Grammatical trigrams: Aprobabilistic model of link grammar.AAAI Fall Symposium Series: ProbabilisticApproaches toNatural Language, pages89-97, Cambridge.Lari, K. and S. J.
Young.
1990.
Theestimation of stochastic ontext-freegrammars using the Inside-Outsidealgorithm.
Computer Speech and Language4:35-56.Rabiner, Lawrence R. 1989.
A tutorial onhidden Markov models and selectedapplications in speech recognition.
InProceedings ofthe IEEE 77, Volume 2.Schabes, Yves, Michael Roth, and RandyOsborne.
1993.
Parsing the Wall StreetJournal with the inside-outside algorithm.Sixth Conference ofthe European Chapter ofthe ACL, '93, Utrecht, the Netherlands,April.429
