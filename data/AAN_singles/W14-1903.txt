Proceedings of the 5th Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 13?19,Baltimore, Maryland USA, August 26 2014. c?2014 Association for Computational LinguisticsDialogue Strategy Learning in Healthcare: A Systematic Approach forLearning Dialogue Models from DataHamid R. ChinaeiHamidreza.Chinaei.1@ulaval.caBrahim Chaib-draaBrahim.Chaib-Draa@ift.ulaval.caAbstractWe aim to build dialogue agents that op-timize the dialogue strategy, specificallythrough learning the dialogue model com-ponents from dialogue data.
In this paper,we describe our current research on au-tomatically learning dialogue strategies inthe healthcare domain.
We go through oursystematic approach of learning dialoguemodel components from data, specificallyuser intents and the user model, as wellas the agent reward function.
We demon-strate our experiments on healthcare datafrom which we learned the dialogue modelcomponents.
We conclude by describ-ing our current research for automaticallylearning dialogue features that can be usedin representing dialogue states and learn-ing the reward function.1 IntroductionCognitive assistive technologies provide supportsystems for the elderly, possibly with cognitive orphysical disabilities, for instance people with de-mentia (such as Alzheimer?s disease) (Boger et al.,2005; Pineau et al., 2011; Rudzicz et al., 2012).Such support systems can significantly reduce thecosts of performing several tasks, currently doneby family members or employed caregivers.
Inthis context, (Rudzicz et al., 2012) are workingon a computerized caregiver that assist individualswith Alzheimer?s disease (AD) to complete dailytasks (e.g., preparing meals) using verbal commu-nication.
Thus, an important component of suchtechnologies is the dialogue agent.Table 1 (left) shows sample dialogues collectedby SmartWheeler, an intelligent wheelchair forpersons with disabilities (Pineau et al., 2011).
Inparticular, SmartWheeler aims to minimize thephysical and cognitive load required in steering it.SmartWheeler is equipped with a dialogue agent,thus the users can give their commands throughthe spoken language besides a joystick.The first line denoted by u1 shows the true userutterance, which is the one that has been extractedmanually from user audio recordings.
The follow-ing line denoted by u?1 is the recognized user utter-ances by automatic speech recognition (ASR).
Fi-nally, the line denoted by a1 shows the performedaction in response to the ASR output at the timeof collecting the dialogues.
First, the users maysay a command in different ways.
For instance forturning right, the user may say turn right a littleplease, turn right, right a little, or right.Furthermore, challenges can occur when ASRoutput is imperfect.
That is, for each dialogueutterance recognized by ASR, the dialogue agentmust estimate the human user intent and then toperform the best action that satisfies the user in-tent.
The recognized utterance by ASR is not re-liable for decision making.
For instance, the firstutterance, u1: ?turn right a little?, shows the trueuser utterance.
The ASR output for this utteranceis, u?1: [10 writer little].
As such, the action per-formed by SmartWheeler at this dialogue turn is,the general query action u1: PLEASE REPEATYOUR COMMAND.We are working on automatically learning thedialogue strategy by learning the dialogue modelcomponents such as user intents, dialogue fea-tures, and the agent?s reward function from data.Our data includes dialogues between human usersand expert agents (such as caregivers).2 Theoretical approachSequential Decision Making: Figure 1 showsthe cycle of interaction between an agent (such asa dialogue agent or a caregiver) and its environ-ment (such as a human user).
The agent performsan action and receives an observation in return.The observation can be used by the agent, for in-13u1 : turn rightu?1 : [turn right]a1 : TURN RIGHTu2 : turn right a littleu?2 : [10 writer little]a2 : PLEASE REPEAT YOUR COMMANDu3 : turn right a littleu?3 : [turn right to lead a]a3 : TURN RIGHT A LITTLEu4 : stopu?4 : [stop]a4 : STOPu1 : turn rightu?1 : [turn right]s1 : turn-right-little, 60%u2 : turn right a littleu?2 : [10 writer little]s2 : turn-right-little, 90%u3 : turn right a littleu?3 : [turn right to lead a]s3 : turn-right-little, 90%u4 : stopu?4 : [stop]s4 : stop, 90%Table 1: Left: A sample from the SmartWheeler dialogues (Pineau et al., 2011).
Right: results of learninghuman user intents from patients?
noisy dialogues.stance to update its state and reward.
The rewardworks as reinforcement from the environment thatshows how well the agent performed.
In sequen-tial decision making, the agent is required to makedecision for sequence of states rather than makinga one-shot decision.
Then, the sequential decisionmaking is performed with the objective of maxi-mizing the long term rewards.
The sequence ofactions is called a strategy, and the major questionin sequential decision making is how to find a nearoptimal strategy.Reinforcement learning (RL): RL in (partiallyobservable) Markov decision processes, so calledthe (PO)MDPs, is a learning approach in sequen-tial decision making.
In particular, (PO)MDPshave been successfully applied in dialogue agents(Roy et al., 2000; Zhang et al., 2001; Williams,2006; Thomson and Young, 2010; Gas?ic?, 2011).The (PO)MDP framework is a formal frameworkto represent uncertainty explicitly while support-ing automated strategy solving.
Specifically, itis an optimization framework that supports au-tomated strategy solving by maximizing a ?re-ward function?.3 ObjectiveSDS (Spoken dialogue system) researchers haveaddressed several practical challenges of apply-ing (PO)MDPs to SDS (Williams, 2006; Paekand Pieraccini, 2008).
Specifically, estimating theuser model and the reward function is a signifi-cant challenge since these model components havea direct impact on the optimized dialogue strat-egy.
Furthermore, the reward function is perhapsthe most hand-crafted aspect of the optimizationframeworks such as (PO)MDPs (Paek and Pierac-cini, 2008).
Using inverse reinforcement learning(IRL) techniques, a reward function can be deter-mined from expert actions (such as caregiver ac-tions) (Ng and Russell, 2000).
Fortunately, learn-ing the reward function using IRL methods havealready been proposed for the general (PO)MDPframework (Ng and Russell, 2000; Kim et al.,2011), paving the way for investigating its use fordialogue (PO)MDPs.
In this context, the IRL algo-rithms require dialogue features (for instance ASRrecognitions with their confidence scores) for rep-resenting the reward function.
Extracting relevantdialogue features is important since the dialoguefeatures and their representation highly affect thelearned reward function and finally the optimizedstrategy.Thus, our goals include building (PO)MDP-based dialogue technologies that optimizes the di-alogue strategy through learning user intents andthe user model, and reward function from dialoguedata, as follows:1.
Learning user intents and the user modelfrom collected dialogues, i.e., ASR recogni-tions, or directly from acoustic data.2.
Learning the reward function.
(a) Learning useful dialogue features.
(b) Representing features in IRL for learn-ing the reward function.Recall Figure 1 that shows the cycle of interac-tion between an agent (such as a dialogue agent ora caregiver) and its environment (such as a humanuser).
In this figure, circles represent the learnedmodels.
The model denoted by (PO)MDP in-cludes the (PO)MDP model components, without14(PO)MDPRIRL (PO)MDPsolverEnvironment Agenta/otrajectories learningactingFigure 1: The cycle of acting/learning between the agent and environment.
The circles represent themodels.
The model denoted by (PO)MDP includes the (PO)MDP model components, without a rewardfunction, learned from step 1 in the objective section.
The learned (PO)MDP model together with expertaction/observation trajectories are used in IRL to learn the reward function denoted by R, in step 2 inthe objective section.
The learned (PO)MDP and reward function are used in the (PO)MDP solver tolearn/update the strategy.a reward function, which have been learned fromstep 1 above.
The learned (PO)MDP together withaction/observation trajectories are used in IRL tolearn the reward function, denoted by R. Then,the learned (PO)MDP and the reward function areused in a (PO)MDP solver to learn/update the op-timal strategy.4 SmartWheeler dataThe SmartWheeler project aims to build an in-telligent wheelchair for persons with disabil-ities (Pineau et al., 2011).
In particular,SmartWheeler aims to minimize the physical andcognitive load required in steering it.
This projecthas been initiated in 2006, and a first prototype,shown in Figure 2, was built in-house at McGill?sCenter for Intelligent Machines.We used the dialogues collected bySmartWheeler to develop dialogue (PO)MDPs,learned primarily from data.
The data includeseight dialogues with healthy users and nine dia-logues with target users of SmartWheeler (Pineauet al., 2011).
The dialogues with target users,who are the elderly, are somehow more noisy thanthe ones with healthy users.
More specifically,the average word error rate (WER) equals 13.9%Figure 2: The SmartWheeler robot plat-form (Pineau et al., 2011).for the healthy user dialogues and 18.5% for thetarget user dialogues.
In order to perform ourexperiments on a larger amount of data, we usedall the healthy and target user dialogues.
In total,there are 2853 user utterances and 422 distinctwords in the SmartWheeler dialogues.5 Learning user intents from dataWe learned the (PO)MDP states by learning theuser intents occurred in the dialogue set usinga topic modeling approach, i.e., Hidden Topic15Markov Model (HTMM) (Gruber et al., 2007).HTMM is a variation of Latent Dirichlet Alloca-tion (LDA) which learns topics from text based onco-occurrence of words and using Dirichlet dis-tribution for generating the topics of text docu-ments (Blei et al., 2003).
HTMM adds Markovianassumption to the LDA model in order to exploitthe Markovian property between sentences in thedocuments.
Thus, HTMM can be seen both as avariation of Hidden Markov Model (HMM) and avariation of LDA.Our experimental results showed that HTMMlearns proper user intents that can be used as dia-logue states, and is able to exploit the Markovianproperty between dialogue utterances, adequately.The learned states, using our proposed methods,from SmartWheeler data are as follows: s1 :move-forward-little, s2 : move-backward-little,s3 : turn-right-little, s4 : turn-left-little, s5 :follow-left-wall, s6 : follow-right-wall, s7 :turn-degree-right, s8 : go-door, s9 : set-speed,s10 : follow-person, s11 : stop.
Table 3 shows thelearned user intents, five of them, with their top-four words, i.e., the intent keywords.Table 1 (right) shows results of HTMM appli-cation on SmartWheeler for the example shownin Table 1 (left).
For instance, the second ut-terance shows that the user actually uttered turnright a little, but it is recognized as 10 writer lit-tle by ASR.
The most probable intent returned byHTMM for this utterance is s3 : turn-right-littlewith 90% probability.
This is because HTMMconsiders Markovian property for deriving intents.As a result, in the second turn it estimates correctlythe true user intent based on the user intent in thefirst turn.The list of all SmartWheeler actions are shownin Table 2.
Each action is the right action ofone state (the user intent for a specific com-mand).
So, ideally, there should be 24 statesfor SmartWheeler dialogues (There are 24 actionsother than the general query action: REPEAT).However, we only learned 11 of the states, mainlybecause of the number of dialogues.
That is, notall of the states appeared in the data frequentlyenough.
There are also states that do not appearin dialogues at all.6 Learning reward functions from dataIn this section, we experiment our implementationof the trajectory-based MDP-IRL algorithm pro-a1 DRIVE FORWARD A LITTLEa2 DRIVE BACKWARD A LITTLEa3 TURN RIGHT A LITTLEa4 TURN LEFT A LITTLEa5 FOLLOW THE LEFT WALLa6 FOLLOW THE RIGHT WALLa7 TURN RIGHT DEGREEa8 GO THROUGH THE DOORa9 SET SPEED TO MEDIUMa10 FOLLOW THE WALLa11 STOPa12 TURN LEFTa13 DRIVE FORWARDa14 APPROACH THE DOORa15 DRIVE BACKWARDa16 SET SPEED TO SLOWa17 MOVE ON SLOPEa18 TURN AROUNDa19 PARK TO THE RIGHTa20 TURN RIGHTa21 DRIVE FORWARD METERa22 PARK TO THE LEFTa23 TURN LEFT DEGREEa24 PLEASE REPEAT YOUR COMMANDTable 2: The list of the possible actions, performedby SmartWheeler.posed by (Ng and Russell, 2000).
The IRL ex-periments are designed to verify if the introducedIRL methods are able to learn a reward functionfor the expert strategy, where the expert strategy isrepresented as a (PO)MDP strategy.
That is, theexpert strategy is the strategy that the underlying(PO)MDP framework optimizes.
The MDP expertstrategy for each of the (PO)MDP state is repre-sented in Table 4.
This strategy suggests perform-ing the right action of each state.6.1 MDP-IRL learned rewardsWe applied the MDP-IRL algorithm onSmartWheeler dialogue MDP described aboveusing the introduced keyword features in Table 5.The algorithm was able to learn a reward functionin which the strategy equals the expert strategy forall states, (the expert strategy shown in Table 4).Table 6 shows the learned reward function.
Notethat, for instance for state s3: turn-right-little, thereward of performing both actions a3: TURNRIGHT A LITTLE and a4: FOLLOW THERIGHT WALL is close to 1.
Nevertheless,the optimized strategy for this reward functionsuggest the correct action, i.e., TURN RIGHT ALITTLE for this state (turn-right-little).16intent 1forward 18.0%move 16.1%little 11.4%drive 08.1%.
.
.
.
.
.intent 2backward 38.0%drive 33.3%little 10.9%top 01.7%.
.
.
.
.
.intent 3right 20.9%turn 17.1%little 13.1%bit 07.4%.
.
.
.
.
.intent 4left 18.9%turn 17.1%little 13.8%right 09.0%.
.
.
.
.
.. .
.. .
.
.
.
.intent 11stop 94.2%stopp 02.2%scott 00.7%but 00.2%.
.
.
.
.
.Table 3: The learned user intents from the SmartWheeler dialogues and their top words.
Each percentageshows the probability of each word given the intent.state state description expert action expert action descriptions1 move-forward-little a1 DRIVE FORWARD A LITTLEs2 move-backward-little a2 DRIVE BACKWARD A LITTLEs3 turn-right-little a3 TURN RIGHT A LITTLEs4 turn-left-little a4 TURN LEFT A LITTLEs5 follow-left-wall a5 FOLLOW THE LEFT WALLs6 follow-right-wall a6 FOLLOW THE RIGHT WALLs7 turn-degree-right a7 TURN RIGHT DEGREESs8 go-door a8 GO THROUGH THE DOORs9 set-speed a9 SET SPEED TO MEDIUMs10 follow-wall a10 FOLLOW THE WALLs11 stop a11 STOPTable 4: The learned strategy using the learned dialogue MDP from SmartWheeler dialogues.forward backward right left turn go for top stops1 1 0 0 0 0 0 0 0 0s2 0 1 0 0 0 0 0 0 0s3 0 0 1 0 0 0 0 0 0s4 0 0 0 1 0 0 0 0 0s5 0 0 0 1 0 0 0 0 0s6 0 0 1 0 0 0 0 0 0s7 0 0 0 0 1 0 0 0 0s8 0 0 0 0 0 1 0 0 0s9 0 0 0 0 0 0 1 0 0s10 0 0 0 0 0 0 0 1 0s11 0 0 0 0 0 0 0 0 1Table 5: Keyword features for the SmartWheeler dialogues.a1 a2 a3 a4 a5 a6 a7 a8 a9 a10 a11 a12 ... REPEATs1 1.0 0 0 0 0 0 0 0 0 0 0 0 .
.
.
0s2 0 1.0 0 0 0 0 0 0 0 0 0 0 .
.
.
0s3 0 0 1.0 0 0 1.0 0 0 0 0 0 0 .
.
.
0s4 0 0 0 1.0 1.0 0 0 0 0 0 0 0 .
.
.
0s5 0 0 0 1.0 1.0 0 0 0 0 0 0 0 .
.
.
0s6 0 0 1.0 0 0 1.0 0 0 0 0 0 0 .
.
.
0s7 0 0 0 0 0 0 1.0 0 0 0 0 0 .
.
.
0s8 0 0 0 0 0 0 0 1.0 0 0 0 0 .
.
.
0s9 0 0 0 0 0 0 0 0 1.0 0 0 0 .
.
.
0s10 0 0 0 0 0 0 0 0 0 1.0 0 0 .
.
.
0s11 0 0 0 0 0 0 0 0 0 0 1.0 0 .
.
.
0Table 6: The learned reward function for the learned dialogue MDP from SmartWheeler dialogues usingkeyword features.
176.2 Choice of featuresIRL needs features to represent the reward func-tion.
We propose keyword features for applyingIRL on the learned dialogue MDP/POMDP fromSmartWheeler.
The keyword features are automat-ically learned as the top-one words for each userintent (see Table 3).
There are nine learned key-words:forward, backward, right, left, turn, go, for,top, stop.The keyword features for each state ofSmartWheeler dialogue POMDP are representedin a vector, as shown in Table 5.
The fig-ure shows that states s3, (turn-right-little) ands6 (follow-right-wall) share the same features,i.e., right.
Moreover, states s4 (turn-left-little)and s5 (follow-left-wall) share the same feature,i.e., left.
In our experiments, we used keyword-action-wise feature representation.
Such featuresinclude an indicator function for each pair of state-keyword and action.
Thus, the feature size forSmartWheeler equals 216 = 9 ?
24 (9 keywordsand 24 actions).Note that the choice of features is applicationdependent.
The reason for using keywords as statefeatures is that in the intent-based dialogue appli-cations the states are the dialogue intents, whereeach intent is described as a vector of k-top wordsfrom the domain dialogues.
Therefore, the key-word features are relevant features for the states.7 ConclusionIn this paper, we described our our systematicapproach for learning dialogue (PO)MDP modelcomponents from unannotated dialogues.
Inour approach, we start by learning the dialogue(PO)MDP states, i.e., the learned user intents fromdata.
The learned states were then used for learn-ing the user model.
Building off these model com-ponents, we learned the agent?s reward function byimplementing a model-based IRL algorithm.
Wedemonstrated our experiments on data collected ina healthcare domain to learn the dialogue modelcomponents solely from data.8 Ongoing workWe are working on a variation of MDP-IRL algo-rithm, that is a model-free trajectory-based MDP-IRL algorithm.
In the model-free MDPs, thestates are usually presented using features (andthus there is no defined/learned transition model).Then, model-free MDP algorithms are used forestimating the optimal strategy of such MDPs.Model-free MDPs can be used in the place ofPOMDPs where state features are analogous to ob-servations.In this context, data analysis for feature selec-tion is highly important.
Dialogue features canbe used to represent dialogue situations (as wellas the observations in the dialogue POMDPs).Moreover, the IRL algorithms require (dialogue)features for representing the reward function.As mentioned earlier, the reward function of(PO)MDPs highly affects the optimized strategy.A relevant reward function to the dialogue agentand users can only be learned by studying andextracting relevant features from the dialogue do-main.
We would like to learn the relevant andproper features that are suitable for both state fea-tures as well as the reward representation.
In par-ticular, we are going to use the experts?
(care-givers?)
strategies in the place of a (PO)MDP strat-egy in order to learn a reward function that ac-counts for caregivers?
strategies.9 AcknowledgmentThe authors thank Jason D. Williams and SuhridBalakrishnan for helpful discussions in the earlydevelopment of this work.
The authors alsothank Joelle Pineau for providing them withthe Smartwheeler data.
The dataset has beencollected with contributions from researchersat McGill University, E?cole Polytechnique deMontre?al, Universite?
de Montre?al, and the Cen-tre de re?adaptation Lucie-Bruneau and Constance-Lethbridge.
The authors thank Ethan Selfridgefor his help in proofreading.
Last but not least,many thank to FQRNT (Fonds Que?be?cois de larecherche sur la nature et les technologies) for par-tial financial support of this work.ReferencesDavid M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet allocation.
Journal of Ma-chine Learning Research, 3:993?1022.Jennifer Boger, Pascal Poupart, Jesse Hoey, CraigBoutilier, Geoff Fernie, and Alex Mihailidis.
2005.A decision-theoretic approach to task assistance forpersons with dementia.
In Proceedings of the 19thInternational Joint Conference on Artificial Intelli-gence (IJCAI?05), pages 1293?1299, San Francisco,CA, USA.
Morgan Kaufmann Publishers Inc.18Milica Gas?ic?.
2011.
Statistical Dialogue Modelling.Ph.D.
thesis, Department of Engineering, Universityof Cambridge.Amit Gruber, Michal Rosen-Zvi, and Yair Weiss.2007.
Hidden topic Markov models.
In ArtificialIntelligence and Statistics (AISTATS?07), San Juan,Puerto Rico, USA.D.
Kim, J. Kim, and K.E.
Kim.
2011.
Robust per-formance evaluation of POMDP-based dialogue sys-tems.
IEEE Transactions on Audio, Speech, andLanguage Processing, 19(4):1029?1040.Andrew Y. Ng and Stuart J. Russell.
2000.
Algorithmsfor inverse reinforcement learning.
In Proceedingsof the 17th International Conference on MachineLearning (ICML?00), Stanford, CA, USA.T.
Paek and R. Pieraccini.
2008.
Automating spokendialogue management design using machine learn-ing: An industry perspective.
Speech Communica-tion, 50(8):716?729.Joelle Pineau, Robert West, Amin Atrash, Julien Ville-mure, and Francois Routhier.
2011.
On the fea-sibility of using a standardized test for evaluat-ing a speech-controlled smart wheelchair.
Interna-tional Journal of Intelligent Control and Systems,16(2):124?131.Nicholas Roy, Joelle Pineau, and Sebastian Thrun.2000.
Spoken dialogue management using proba-bilistic reasoning.
In Proceedings of the 38th An-nual Meeting on Association for Computational Lin-guistics (ACL?00), Hong Kong.Frank Rudzicz, Rozanne Wilson, Alex Mihailidis, Eliz-abeth Rochon, and Carol Leonard.
2012.
Commu-nication strategies for a computerized caregiver forindividuals with alzheimer?s disease.
In Proceed-ings of the Third Workshop on Speech and LanguageProcessing for Assistive Technologies, (SLPAT?12),pages 47?55, Montreal, Quebec, Canada.
Associa-tion for Computational Linguistics.Blaise Thomson and Steve Young.
2010.
Bayesianupdate of dialogue state: A POMDP framework forspoken dialogue systems.
Computer Speech andLanguage, 24(4):562?588.Jason D. Williams.
2006.
Partially ObservableMarkov Decision Processes for Spoken DialogueManagement.
Ph.D. thesis, Department of Engi-neering, University of Cambridge.Bo Zhang, Qingsheng Cai, Jianfeng Mao, and Bain-ing Guo.
2001.
Planning and acting under uncer-tainty: A new model for spoken dialogue system.
InProceedings of the 17th Conference in Uncertaintyin Artificial Intelligence (UAI?01), Seattle, Washing-ton, USA, August.19
