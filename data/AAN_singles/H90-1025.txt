Session 6: ATIS Site Reports and General DiscussionDavid S. Pallett, ChairNational Institute of Standards and TechnologyBldg.
225, Rm A216Gaithersburg, MD 20899Many of the evening session papers served to describelocal implementations of ATIS domain systems and toprovide local "glass-box" diagnostic evaluations to comple-ment he NIST "black box" scoring.
In some cases, thesehad the appearance of "nit-picking".
However, since ourcollective intent was to develop and implement an evalua-tion procedure, the criticisms were generally constructive,and each site has much to contribute toward improved im-plementations of SLS performance evaluation procedures.Many of these observations underscore the complexity ofdeveloping standard test procedures ina new and challeng-ing arena: spoken language systems.In the first paper in the evening session, Sean Batesreported on the BBN ATIS System \[1\].
Results were citedfor two different systems, one of which was the commer-cial "Parlance" system.
The other, the "Delphi" system, isintended to comprise the research NL component of BBN'sHARC (Hear And Recognize Continuous peech) system.BBN's diagnostic evaluation indicated that the productionof incorrect answers was not a significant problem for theDelphi System.
However, there were 38 (out of the 90"official" queries in the test set) for which theNO_ANSWER response had been given.
BBN's analysisoutlined the major causes of these NO_ANSWERresponses: (1) word senses not previously encountered and(2) lack of inference.
Boisen concluded by noting that: (1)"This evaluation methodology works!
", (2) more trainingdata and more time to use it is needed, (3) more careful"definition of answer criteria is needed", and (4) that (par-ticularly in view of the low intra-speaker variability in lin-guistic structure noted in the ATIS Pilot Corpus) the ATISlanguage is not "varied enough", so that it "would increasethe validity of tests of SLS systems if more than onedomain were used".The CMU system to understand spontaneous speech asbeen termed "Phoenix", and was introduced by WayneWard, in the second evening presentation \[2\].
For textinput, a frame-based parser is used to process iU-formedtext.
In Ward's "diagnostic evaluation", he noted that "asignificant number of utterances that parsed were scored asincorrect.
Most of these were of two types that resultedfrom a misunderstanding on \[Ward's\] part as to what wasto be generated."
These involved ates and abbreviations.The CMU implementation was unique among those forwhich results were reported at this meeting in that inputspeech (waveform) files were processed in addition to thetext (SNOR format) files.
In this case, output from theSphinx speech recognition system was passed to the parser,using a word-pair grammar with a cited perplexity of 85.In questions following Ward's presentation, and innoting the "misunderstanding on \[Ward's\] part as to whatwas to be generated" that led to answers "scored as incor-rect", Patti Price asked "What is the moral of this story?
".To everyone's amusement, particularly those who had beeninvolved in the disputes about he proposed test protocolsprior !o the test, Ward responded that now "he'll read thenet mail!
"Preliminary ATIS development work at MIT wasdescribed by Stephanie Seneff \[3\].
In the M1T ATIS sys-tem, "low level functions typically fill slots in an eventframe with ... semantic information.
Once the entire sen-tence has been processed and the history frames have beenmerged, an IDIL query is then constructed from the com-pletely specified query."
\[In this case, the IDIL querymakes use of the Intelligent Database Interface (IDI) "as anintermediary to SQL", provided by researchers at Unisys.\]There had been four releases of incremental portions of theATIS Pilot Corpus, and the MIT group monitored progressin handling the utterances in each successive release, bothin terms of parser coverage and agreement of the back-endresponses with the canonical answers.
These studies ledSeneff to express concern "that rules created to deal withutterances in one release don't seem to generalize well tonew releases", a finding that may be related to other obser-vations about he "very high inter-speakers variability thataccompanies low intra-speaker variability in linguisticstructure" (see, for example \[1\]).
While noting that aninordinate amount of time had been required to work withthe back end and the need to generate SQL queries, Seneffremarked that "the idea of a common task involving book-ing flights is a good one", and that hey "look forward to ...integrating the natural anguage component with a recog-nizer".In a refreshing contrast o the other papers in theevening presentation (which focussed largely on diagnosticevaluations), Patti Price reported on studies at SRI (involv-ing the ATIS relational database) which assessed the ef-fects of changes in the simulations on the speech and lan-guage of the experimental subjects \[4\].
The stated goal ofthese studies is to "design an appropriate human-machineinterface".
Price also noted that "the greatest source ofvariability in the system is that across subjects".
Five ex-pedrnents were described for several data collection con-123ditions.
The SRI studies uggest that "the goal of designingan appropriate spoken language system can conflict withthe goal of collecting data for evaluation of spokendatabase queries", but that they "believe that it is possibleto find some ways to coordinate the two endeavors.
"In the last of the formal presentations in the session,Lew Norton described the Unisys ATIS domain system \[5\].The Unisys approach combines a number of elements: (1)the MIT SUMMIT speech recognition system, (2) theUnisys PUNDIT language understanding system, (3) use ofa module termed QTIP (Query Translation and InterfaceProgram, and (4) the Intelligent Database Server (IDS), a"general knowledge/database int rface" to mediate accessto the database, (5)INGRES to access the ATIS relationaldatabase, and (6) a Dialogue Manager to integrate overalluser-system interaction.
\[Note that the MIT systemdescribed by Seneff also made use of elements of the IDScomponent (i.e., the IDI portion).\] In the Unisys "diag-nostic evaluation" as reported by Norton, errors were noteddue to several causes: (1) words not being in the lexicon,(2) problems in parsing, (3) problems in obtaining an ap-propriate semantic/pragmatic nalysis, and (4) failure of theQTIP module to generate an appropriate call to the rela-tional database.
Like other sites, the Unisys researchersnoted great inter-subject variability - -  with their systems's"success rate" for the different subjects in the test set rang-ing from "30% to 88%".
Norton further noted that theimplications of this finding suggest that there "are a largenumber of different ways to ask essentially the same ques-tions", and that "a natural anguage understanding systemwill have to be trained on much larger volumes of data.
"This observation was further supported by data document-ing the rate of incremental growth of the grammar in theATIS domain, which appears to be much slower for ATISthan for the MIT Voyager domain.Following the presentations from BBN, CMU, MIT,SRI, and Unisys, some time was devoted to general discus-sion of issues raised in the afternoon and evening ATISSessions.Bob Moore underscored what a number of individualshad noted: that there had been insufficient time between therelease(s) of the training data and the test data.
lit is im-portant o note that the relational ATIS database used inthese studies had not been "frozen" until mid-April, andincremental releases of the training data took place during asix-week period during May and just prior to the release ofthe test data on June 15th.\]Lynnette Hirschman oted that substantially more damshould be made available for this domain m of the order often times more than to date.Victor Zue noted that proposals to extend the testmethodology to accommodate context (such as those out-lined by Bates and Hirschman) seemed attractive, but thatall evaluations are, to some degree, subjective and that weneed to plan on developing procedures for formal subjec-tive evaluations.
Victor likened the present approach toATIS implementations to a "shotgun" approach, and ex-pressed a preference for more focused or constrainedscenarios and local implementations that might be regardedas "rifle" approaches.
Victor also underscored what othershad suggested: that a pooling of data from several sitesmay be the only practical way to gather the amount of datathat appear to be needed.John Makhoul noted that the focus of the studiesreported at this session should be seen as a vehicle forNL/SLS evaluation, not so much as an effort to developreal air travel information systems.Patti Price noted "TI's Heroic Role" in developing theATIS relational database used for these studies, collectingspontaneous peech data and providing "canonicalanswers".
Charles Hemphill and his colleagues at TIworked very hard to provide data for the Pilot Corpus forboth training and test purposes, and the ATIS studies at theseveral sites would not have been possible without he TIgroup's efforts.Charles Wayne closed the discussion by thanking theparticipants for the significant Spoken Language Systemsprogress in the ATIS task domain.REFERENCES\[1\] Bates, M. et al, BBN ATIS System Progress Report -June 1990 (in this Proceedings).\[2\] Ward, W., The CMU Air Travel Information Service\[sic\]: Understanding Spontaneous Speech (in thisProceedings).\[3\] Zue, V. et al, Preliminary ATIS Development a MIT(in this Proceedings).\[4\] Bly, B. et al, Designing the Human Machine Interfacein the ATIS Domain (in this Proceedings).\[5\] Norton, L. M. et al, Management and Evaluation ofInteractive Dialogue in the Air Travel Domain (in thisProceedings).124
