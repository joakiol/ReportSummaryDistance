The Domain Dependence of ParsingSatoshi  SekineNew York Un ivers i tyComputer  Sc ience Depar tment715 Broadway,  Room 709New York,  NY  10003, USAsekine@cs, nyu.
eduhttp ://cs .nyu.
edu/cs/proj ect s/prot eus/sekineAbst ractA major concern in corpus based ap-proaches is that the applicability of the ac-quired knowledge may be limited by somefeature of the corpus, in particular, the no-tion of text 'domain'.
In order to examinethe domain dependence of parsing, in thispaper, we report 1) Comparison of struc-ture distributions across domains; 2) Ex-amples of domain specific structures; and3) Parsing experiment using some domaindependent grammars.
The observationsusing the Brown corpus demonstrate do-main dependence and idiosyncrasy of syn-tactic structure.
The parsing results showthat the best accuracy is obtained usingthe grammar acquired from the same do-main or the same class (fiction or non-fiction).
We will also discuss the relation-ship between parsing accuracy and the sizeof training corpus.1 In t roduct ionA major concern in corpus based approaches i thatthe applicability of the acquired knowledge may belimited by some feature of the corpus.
In particular,the notion of text 'domain' has been seen as a ma-jor constraint on the applicability of the knowledge.This is a crucial issue for most application systems,since most systems operate within a specific domainand we are generally limited in the corpora availablein that domain.There has been considerable research in this area(Kittredge and Hirschman, 1983) (Grishman andKittredge, 1986).
For example, the domain depen-dence of lexical semantics is widely known.
It iseasy to observe that usage of the word 'bank' is dif-ferent between the 'economic document' domain andthe 'geographic' domain.
Also, there are surveys ofdomain dependencies concerning syntax or syntax-related features (Slocum, 1986)(niber, 1993)(Karl-gren, 1994).
It is intuitively conceivable that thereare syntactic differences between 'telegraphic mes-sages' and 'press report', or between 'weather fore-cast sentences' and 'romance and love story'.
But,how about the difference between 'press report' and'romance and love story'?
Is there a general and sim-ple method to compare domains?
More importantly,shall we prepare different knowledge for these twodomain sets?In this paper, we describe two observations andan experiment which suggest an answer to the ques-tions.
Among the several types of linguistic knowl-edge, we are interested in parsing, the essential com-ponent of many NLP systems, and hence domain de-pendencies of syntactic knowledge.
The observationsand an experiment are the following:?
Comparison of structure distributions acrossdomains?
Examples of domain specific structures?
Parsing experiment using some domain depen-dent grammars2 Data  and  Too lsThe definition of domain will dominate the perfor-mance of our experiments, o it is very important tochoose a proper corpus.
However, for practical rea-sons (availability and time constraint), we decidedto use an existing multi-domain corpus which hasnaturally acceptable domain definition.
In order toacquire grammar ules in our experiment, we need asyntactically tagged corpus consisting of different do-mains, and the tagging has to be uniform throughoutthe corpus.
To meet these requirements, the BrownCorpus (Francis and Kucera, 1964) on the distribu-tion of PennTreeBank version 1 (Marcus et.al., 1995)is used in our experiments.
The corpus consists of 1596domains as shown in Appendix A; in the rest of thepaper, we use the letters from the list to representthe domains.
Each sample consists of about the samesize of text in terms of the number of words (2000words), although a part of the data is discarded be-cause of erroneous data format.For the parsing experiment, we use 'Apple PieParser' (Sekine, 1995) (Sekine, 1996).
It is aprobabilistic, bottom-up, best-first search, chartparser and its grammar can be obtained from asyntactically-tagged corpus.
We acquire two-non-terminal grammars from corpus.
Here, 'two-non-terminal grammar'  means a grammar which usesonly 'S' (sentence) and 'NP'  (noun phrase) as ac-tual non-terminals in the grammar and other gram-matical nodes, like 'VP'  or 'PP' ,  are embedded into arule.
In other words, all rules can only have either 'S'or 'NP'  as their left hand-side symbol.
This strat-egy is useful to produce better accuracy corrlparedto all non-terminal grammar.
See (Sekine, 1995) fordetails.In this experiment, grammars are acquired fromthe corpus of a single domain, or from some combina-tion of domains.
In order to avoid the unknown wordproblem, we used a general dictionary to supplementthe dictionary acquired from corpus.
Then, we ap-ply each of the grammars to some texts of differentdomains.
We use only 8 domains (A,B,E,J,K,L,Nand P) for this experiment, because we want to fixthe corpus size for each domain, and we want tohave the same number of domains for the non-fictionand the fiction domains.
The main objective is toobserve the parsing performance based on the gram-mar acquired from the same domain compared withthe performance based on grammars of different do-mains, or combined omains.
Also, the issue of thesize of training corpus will be discussed.3 Domain  Dependence  o f  S t ructuresFirst, we investigate the syntactic structure of eachdomain of the Brown corpus and compare these fordifferent domains.
In order to represent the syntacticstructure of each domain, the distribution of partialtrees of syntactic structure is used.
A partial tree isa part of syntactic tree with depth of one, and it cor-responds to a production rule.
Note that this partialtree definition is not the same as the structure defini-tion used in the parsing experiments described later.We accumulate these partial trees for each domainand compute the distribution of partial trees basedon their frequency divided by the total number ofpartial trees in the domain.
For example, Figure1 shows the five most frequent partial trees (in theformat of production rule) in domain A (Press: Re-domain A domain PPP -> IN NP 8.40~ NP -> PRP 9.52XNP -> NNPX 5.42Z PP -> IN NP 5.79~S -> S 5.06~ S -> NP VP 5.77XS -> NP VP 4.28~ S -> S 5.37~NP -> DT NNX 3.81~ NP -> DT NNX 3.90~Figure 1: Partial TreesT\M A B E J K LA 5 .13  5 .35  5 ,41  5 .45  5 .51  5 .52B 5 .47  5 .19  5 .50  5 .51  5 .55  5 .58E 5.50 5.48 5.20 5.48 5.58 5.59J 5.39  5 .37  5 ,35  5 .15  5 .52  5 .57K 5 .32  5 .25  5 .31  5 .41  4 .95  5 .14L 5 .32  5 .26  5 ,32  5 .45  5 .12  4 .91N 5 .29  5 .25  5 .28  5 .43  5 .10  5 .06P 5 .43  5 .36  5 .40  5 .55  5 .23  5 .21N P5 .53  5 .555 .60  5 .605 .58  5 .615 .58  5 .595 .15  5 .17,5.09 5 .134 .89  5 .125 .21  5 .00Figure 2: Cross Entropy of grammar across domainsportage) and domain P (Romance and love story).For each domain, we compute the probabilities ofpartial trees like this.
Then, for each pair of domains,cross entropy is computed using the probability data.Figure 2 shows a part of the cross entropy data.
Forexample, 5.41 in column A, row E shows the crossentropy of modeling by domain E and testing on do-main A.
From the matrix, we can tell that some pairsof domains have lower cross entropy than others.
Itmeans that there are difference in similarity amongdomains.
In particular, the differences among fictiondomains are relatively small.In order to make the observation easier, we clus-tered the domains based on the cross entropy data.The distance between two domains is calculated asthe average of the two cross-entropies in both direc-tions.
We use non-overlapping and average-distanceclustering.
Figure 3 shows the clustering result basedon grammar cross entropy data.
From the results,we can clearly see that fiction domains, in particulardomains K, L, and N are close which is intuitivelyunderstandable.4 Domain  Spec i f i c  S t ructuresSecondly, in contrast o the global analysis reportedin the previous ection, we investigate the structuralidiosyncrasies of each domain in the Brown corpus.For each domain, the list of partial trees which arerelatively frequent in that domain is created.
Weselect the partial trees which satisfy the following975.085.135.175.265.275.305.305.335.335.345.375.385.425.50ADFGKLNPRM JEB  CHI I I II - - I I- -  I IIFigure 3: Clustering resulttwo conditions:1.
Frequency of the partial tree in a domain shouldbe 5 times greater than that in the entire corpus2.
It occurs more than 5 times in the domainThe second condition is used to delete noise, becauselow frequency partial trees which satisfy the first con-dition have very low frequency in the entire corpus.The list is too large to show in this paper; a partof the list is shown in Appendix B.
It obviouslydemonstrates that each domain has many idiosyn-cratic structures.
Many of them are interesting tosee and can be easily explained by our linguistic in-tuition.
(Some examples are listed under the cor-responding partial tree) This supports the idea ofdomain dependent grammar, because these idiosyn-cratic structures are useful only in that domain.5 Pars ing  Resu l tsIn this section, the parsing experiments are de-scribed.
There are two subsections.
The first is theindividual experiment, where texts from 8 domainsare parsed with 4 different ypes of grammars.
Theseare grammars acquired from the same size corpus ofthe same domain, all domains, non-fiction domainsand fiction domains.The other parsing experiment is the intensive x-periment, where we try to find the best suitablegrammar for some particular domain of text and tosee the relationship of the size of the training corpus.We use the domains of 'Press Reportage' and 'Ro-mance and Love Story' in this intensive xperiment.TextA 66.62/64.14B 87.65/62.55E 64.o5/6o.79J 67.s0/65.50K 70.99168.54L 67so/6so2N 73.o9/71.38P 66.44/65.51Same domain All non-fiction fiction64.39/61.4564.67/61.7865.25/61.5165.87/63.9071.00/68.0468.08/66.2272.97/70.2764.52/63.9565.57/62.4065.73/62.696~.26/62.1865.57/64.5870.04/66.6457.32/64.3170.51/67.9062.37/61.5562.23/59.3263.03/60.3662.87/59.0463.04/60.7771.79/68.9568.89/66.5574.29/72.286469/645oFigure 4: Parsing accuracy for individual sectionIn order to measure the accuracy of parsing, recalland precision measures are used (Black et.al., 1991).5.1 Ind iv idua l  Exper imentFigure 4 shows the parsing performance for domainA, B, E, J, K, L, N and P with four types of gram-mars.
In the table, results are shown in the form of'recall/precision'.
Each grammar is acquired fromroughly the same size (24 samples except L with 21samples) of corpus.
For example, the grammar of alldomains is created using corpus of 3 samples eachfrom the 8 domains.
The grammar of non-fiction andfiction domains are created from corpus of 6 sampleseach from 4 domains.
Then text of each domain isparsed by the four types of grammar.
There is nooverlap between training corpus and test corpus.We can see that the result is always the best whenthe grammar acquired from either the same domainor the same class (fiction or non-fiction) is used.
Wewill call the division into fiction and non-fiction as'class'.
It is interesting to see that the grammar ac-quired from all domains is not the best grammar inany tests.
In other words, if the size of the trainingcorpus is the same, using a training corpus drawnfrom a wide variety of domains does not help toachieve better parsing performance.For non-fiction domain texts (A, B, E and J),the performance of the fiction grammar is notablyworse than that of the same domain grammar or thesame class grammar.
In contrast, the performanceon some fiction domain texts (K and L) with thenon-fiction grammar is not so different from that ofthe same domain.
Here, we can find a relationshipbetween these results and the cross entropy obser-vations.
The cross entropies where any of the fic-tion domains are models and any of the non-fictiondomains are test are the highest figures in the ta-ble.
This means that the fiction domains are notsuitable for modeling the syntactic structure of thenon-fiction domains.
On the other hand, the crossentropies where any of the non-fiction domains are98models and any of the non-fiction domains (exceptP) are test have some lower figures.
Except for thecase of N with the non-fiction grammar, these ob-servations explains the result of parsing very nicely.The higher the cross entropy, the worse the parsingperformance.It is not easy to argue why, for some domains, theresult is better with the grammar of the same classrather than the same domain.
One rationale we canthink of is based on the comparison observation de-scribed in section 3.
For example, in the cross com-parison experiment, we have seen that domains K, Land N are very close.
So it may be plausible to saythat the grammar of the fiction domains is mainlyrepresenting K, L and N and, because it covers widesyntactic structure, it gives better performance foreach of these domains.
This could be the explana-tion that the grammar of fiction domains are superiorto the own grammar for the three domains.
In otherwords, it is a small sampling problem, which can beseen in the next experiment, too.
Because only 24samples are used, a single domain grammar tends tocovers relatively small part of the language phenom-ena.
On the other hands, a corpus of similar domainscould provide wider coverage for the grammar.
Theassumption that the fiction domain grammar epre-sents domains of K, L and M may explain that theparsing result of domain P strongly favors the gram-mar of the same domain compared to that of thefiction class domains.5.2 Intens ive  Exper imentsIn this section, the parsing experiments on texts oftwo domains are reported.
The texts of the two do-mains are parsed with several grammars, e.g.
gram-mars acquired from different domains or classes, anddifferent sizes of the training corpus.
The size of thetraining corpus is an interesting and important issue.We can easily imagine that the smaller the trainingcorpus, the poorer the parsing performance.
How-ever, we don't know which of the following two typesof grammar produce better performance: a grammartrained on a smaller corpus of the same domain, ora grammar trained on a larger corpus including dif-ferent domains.Figure 5 and Figure 6 shows recall and precision ofthe parsing result for the Press Reportage text.
Thesame text is parsed with 5 different types of gram-mars of several variations of training corpus size.
Be-cause of corpus availability, we can not make singledomain grammars of large size training corpus, asyou can find it in the figures.Figure 7 and Figure 8 shows recall and precisionof the parsing result for the Romance and Love Story99recall706O55 o ALL?
fiction50 ?
non-fictiono press reportq9 romance/love45 i0 2'0 40 6'0 8'0 100Number of SamplesFigure 5: Size and Recall (Press Report)precision706560555045o ALL?
fiction?
non-fictiono press report~9 romance/love2'0 4'0 6.0 8.0 100Number of SamplesFigure 6: Size and Precision (Press Report)recall70 -,655550450: fic~?~ctm no press reportq) romance/lovei i i i J2O 40 60 8O 100Number of SamplesFigure 7: Size and Recall (Romance/Love)precision706055 o ALL* fiction50 ?
non-fictiono press reportq~ romance/love450 20 4'0 6'0 8'0 100Number of SamplesFigure 8: Size and Precision (Romance/Love)text.
This text is also parsed with 5 different ypesof grammars.The graph between the size of training corpus andaccuracy is generally an increasing curve with theslope graduMly flattening as the size of the corpus in-creases.
Note that the small declines of some graphsat large number of samples are mainly due to thememory limitation for parsing.
Parsing is carriedout with the same memory size, but when the train-ing corpus grows and the grammar becomes large,some long sentences can't be parsed because of dataarea limitation.
When the data area is exhaustedduring the parsing, a fitted parsing technique is usedto build the most plausible parse tree from the par-tially parsed trees.
These are generally worse thanthe trees completely parsed.It is very interesting to see that the saturationpoint of any graph is about 10 to 30 samples.
Thatis about 20,000 to 60,000 words, or about 1,000 to3,000 sentences.
In the romance and love story do-main, the precision of the grammar acquired from 8samples of the same domain is only about 2% lowerthan the precision of the grammar trained on 26 sam-ples of the same domain.
We believe that the reasonwhy the performance in this domain saturates withsuch a small corpus is that there is relatively littlevariety in the syntactical structure of this domain.The order of the performance is generally the fol-lowing: the same domain (best), the same class,all domMns, the other class and the other domain(worst).
The performance of the last two grammarsare very close in many cases.
In the romance andlove story domain, the grammar acquired from thesame domain made the solo best performance.
Thedifference of the accuracy of the grammars of thesame domain and the other domain is quite large.The results for the press reportage is not so obvious,but the same tendencies can be observed.In terms of the relationship between the size oftraining corpus and domain dependency, we willcompare the performance of the grammar acquiredfrom 24 samples of the same domain (we will callit 'baseline grammar') ,  and that of the other gram-mars.
In the press reportage domain, one needs athree to four times bigger corpus of all domains ornon-fiction domains to catch up to the performanceof the baseline grammar.
It should be noticed thata quarter of the non-fiction domain corpus and oneeighth of the all domain corpus consists of the pressreport domain corpus.
In other words, the fact thatthe performance of the baseline grammar is aboutthe same as that of 92 samples of the non-fiction do-mains means that in the latter grammar, the rest ofthe corpus does not improve or is not harmful forthe parsing performance.
In the romance and lovestory domain, the wide variety grammar, in particu-lar the fiction domain grammar quickly catch up tothe performance of the baseline grammar.
It needsonly less than twice size of fiction domain corpus toachieve the performance of the baseline grammar.These two results and the evidence that fiction do-mains are close in terms of structure indicate that ifyou have a corpus consisting of similar domains, it isworthwhile to include the corpus in grammar acqui-sition, otherwise not so useful.
We need to furtherquantify these trade-offs in terms of the syntactic di-versity of individual domains and the difference be-tween domains.We also find the small sampling problem in thisexperiment.
In the press reportage xperiment, hegrammar acquired from the same domain does notmake the best performance when the size of the train-ing corpus is small.
We observed the same phenom-ena in the previous experiment.6 DiscussionOne of our basic claims is the following.
Whenwe try to parse a text in a particular domain, weshould prepare a grammar which suits that domain.This idea naturally contrasts to the idea of robustbroad-coverage parsing (Carroll and Briscoe, 1996),in which a single grammar should be prepared forparsing of any kind of text.
Obviously, the latteridea has a great advantage that you do not have tocreate a number of grammars for different domainsand also do not need to consider which grammarshould be used for a given text.
On the other hand,it is plausible that a domain specific grammar canproduce better results than a domain independentgrammar.
Practically, the increasing availability of100corpora provides the possibilities of creating domaindependent grammars.
Also, it should be noted thatwe don't need a very large corpus to achieve a rela-tively good quality of parsing.To summarize our observations and experiments:?
There are domain dependencies on syntacticstructure distribution.?
Fiction domains in the Brown corpus are verysimilar in terms of syntactic structure.?
We found many idiosyncratic structures fromeach domain by a simple method.For 8 different domains, domain dependentgrammar or the grammar of the same class pro-vide the best performance, if the size of thetraining corpus is the same.The parsing performance is saturated at verysmall size of training corpus.
This is the case,in particular, for the romance and love story do-main.The order of the parsing performance is gener-ally the following; the same domain (best), thesame class, all domain, the other class and theother domain (worst).?
Sometime, training corpus in similar domains isuseful for grammar acquisition.It may not be so useful to use different domaincorpus even if the size of the corpus is relativelylarge.Undoubtedly these conclusions depend on theparser, the corpus and the evaluation methods.
Alsoour experiments don't cover all domains and possi-ble combinations.
However, the observations and theexperiment suggest he significance of the notion ofdomain in parsing.
The results would be useful fordeciding what strategy should be taken in developinga grammar on a 'domain dependent' NLP applicationsystems.7 AcknowledgmentsThe work reported here was supported under con-tract 95-F145800-000 from the Office of Researchand Development.
We would like to thank ourcolleagues, in particular Prof.Ralph Grishman andMs.Sarah Taylor for valuable discussions and sug-gestions.ReferencesDouglas Biber: 1993.
Using Register-DiversifiedCorpora for General Language Studies.
Journalof Computer Linguistics Vol.19, Num 2, pp219-241.Ezra Black, et.ah 1991.
A procedure for Quanti-tatively Comparing the Syntactic Coverage of En-glish Grammars.
Proc.
of Fourth DARPA Speechand Natural Language WorkshopJohn Carroll and Ted Briscoe: 1996.
Apportioningdevelopment effort in a probabilistic LR parsingsystem through evaluation.
Proceedings of Confer-ence on Empirical Methods in Natural LanguageProcessing.W.
Nelson Francis and Henry Kucera: 1964/1979.Manual of information to accompany A StandardCorpus of Present-Day Edited American English.Brown University, Department of LinguisticsRalph Grishman and Richard Kittredge: 1986.
An-alyzing Language in Restricted Domains: Sublan-guage Description and Processing.
Lawrence Erl-baum Associates, PublishersJussi Karlgren and Douglass Cutting: 1994.
Rec-ognizing Text Genres with Simple Metrics Us-ing Discriminant Analysis.
The 15th Interna-tional Conference on Computational Linguistics,pp1071-1075.Richard Kittredge, Lynette Hirschman: 1983.
Sub-language: Studies of Language in Restricted Se-mantic domains.
Series of Foundations of Com-munications, Walter de Gruyter, BerlinMitchell P. Marcus, Beatrice Santorini and Mary AMarcinkiewicz: 1993.
Building a Large Anno-tated Corpus of English: The Penn TreeBank.Computational Linguistics, 19.1, pp313-330.Satoshi Sekine: 1996.
Apple Pie Parser homepage.http://cs, nyu.
edu/cs/projects/proteus/appSatoshi Sekine and Ralph Grishman: 1995.
ACorpus-based Probabilistic Grammar with OnlyTwo Non-terminals.
International Workshop onParsing Technologies, pp216-223.Johathan Slocum: 1986.
How One Might Automat-ically Identify and Adapt to a Sublanguage: AnInitial Exploration.
Analyzing Language in Re-stricted Domains, pp195-210.101APPENDIXA Categories in Brown corpusI.
Informative Prose (374 samples)A.
Press: Reportage (44)B.
Press: Editorial (27)C. Press: Reviews (17)D. Religion (17)E. Skills and Hobbies (36)F. Popular Lore (48)G. Letters,Bibl iography,Memories, (75)H. Miscellaneous (30)J.
Learned (80)I I .
Imaginative Prose (126 Samples)K. General F ic t ion  (29)L. Mystery and Detect ive F ic t ion  (24)M. Science Fiction (6 )N. Adventure and Western Fiction (29)P. Romance and Love Story (29)R. Humor (9 )B Sample of Relatively FrequentPartial TreesSYM.
DOMAIN (num.of type;total freq.
ofqualif ied partial trees)ra t io  frequency ru le  (Example)(domain/corpus)A.
Press: Reportage (30;507)9.40 11 / 14 NP -> NNPX NNX NP9.30 7 / 9 NP -> NP POS JJ NNPX8.70 8 / ii S -> NP VBX VP NP PP8.44 12 / 17 NP -> DT $ CD NNX"The $40,000,000 budget""a 12,500 payment"8.30 77 / 111 NP -> NNPX NP"Vice President L.B.
Johnson""First Lady Jacqueline Kennedy"B.
Press: Editorial (20;255)18.57 34 / 34 S -> PP :"To the editor:""To the editor of New York Times:"11.14 6 / 10 NP -> DT "" ADJP "" NNX"an "'autistic'' child""a "'stair-step'" plan"C. Press: Reviews (19;267)26.27 8 / 9 WHADVP -> NNPX25.33 12 / 14 NP -> NP POS "" NNPX ""D. Religion (8;87)26.83 26 / 28 S -> NP -RRB- S25.28 14 / 16 NP -> NNPX CD : CD"St. Peter 1:4""St. John 3:8"E. Skills and Hobbies (17;219)10.58 22 / 22 NP -> CD NNX ""10.21 27 / 28 S -> SBAR :"How to feed :""What it does :"F. Popular Lore (12;86)10.58 8 / 8 NP -> DT NP POS NNPX10.58 6 / 6 NP -> NNX DT NNX PPG.
Letters,Bibl iography,Memories,etc (12;125)6.59 8 / 8 WHPP -> TO SBAR"to what they mean by the concept""to what may happen next"6.04 22 / 24 WHPP -> ~OF SBAR"of what it is all about""of what he had to show his country"H. Miscel laneous (69;2607)16.82 70 / 70 S -> NP .
S16.82 17 / 17 S -> -LRB- VP .
-RRB-J.
Learned (22;295)6.51 28 / 28 NP -> CD : CD6.51 20 / 20 NP -> NNX :6.22 44 / 46 S -> S -LRB- NP -RRB-Sentence and name and year in bracketSentence and figure name in bracketK.
General Fiction (14;148)11.58 7 / I0 NP -> PRP S11.03 6 / 9 S -> ADVP S : : S10.75 13 / 20 S -> PP S , CC SL.
Mystery and Detective Fiction (19;229)14.28 8 / 11 Sq -> S , SqTag questionsM.
Science Fiction (6;57)17.89 7 / 32 S -> S , SINV.... Forgive me, Sandalphon'', said Hal""''sentence'', remarked Helva"10.22 8 / 64 S -> SBARQ ""N. Adventure and Western Fict ion (24;422)14.59 45 / 50 VP -> VBX RB12.97 8 / 10 VP -> VBX RB PPP.
Romance and Love Story (31;556)15.99 7 / 7 S -> CC SBARq15.99 6 / 6 S -> "" NP VP , NP ""12.23 13 / 17 S -> SQ S11.99 6 / 8 S -> "" VP , NP ""R. Humor (3;20)6.92 6 / 47 NP -> DT ADJP NP6.78 7 / 56 NP -> PRP ?DLQ5.67 7 / 67 PP -> IN "" NP """as "'off-Broadway''"102
