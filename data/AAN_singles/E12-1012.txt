Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 109?119,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsAdaptation of Statistical Machine Translation Model for Cross-LingualInformation Retrieval in a Service ContextVassilina NikoulinaXerox Research Center Europevassilina.nikoulina@xrce.xerox.comBogomil KovachevInformatics InstituteUniversity of AmsterdamB.K.Kovachev@uva.nlNikolaos LagosXerox Research Center Europenikolaos.lagos@xrce.xerox.comChristof MonzInformatics InstituteUniversity of AmsterdamC.Monz@uva.nlAbstractThis work proposes to adapt an existinggeneral SMT model for the task of translat-ing queries that are subsequently going tobe used to retrieve information from a tar-get language collection.
In the scenario thatwe focus on access to the document collec-tion itself is not available and changes tothe IR model are not possible.
We proposetwo ways to achieve the adaptation effectand both of them are aimed at tuning pa-rameter weights on a set of parallel queries.The first approach is via a standard tuningprocedure optimizing for BLEU score andthe second one is via a reranking approachoptimizing for MAP score.
We also extendthe second approach by using syntax-basedfeatures.
Our experiments show improve-ments of 1-2.5 in terms of MAP score overthe retrieval with the non-adapted transla-tion.
We show that these improvements aredue both to the integration of the adapta-tion and syntax-features for the query trans-lation task.1 IntroductionCross Lingual Information Retrieval (CLIR) is animportant feature for any digital content providerin today?s multilingual environment.
However,many of the content providers are not willing tochange existing well-established document index-ing and search tools, nor to provide access totheir document collection by a third-party exter-nal service.
The work presented in this paper as-sumes such a context of use, where a query trans-lation service allows translating queries posed tothe search engine of a content provider into sev-eral target languages, without requiring changesto the undelying IR system used and without ac-cessing, at translation time, the content provider?sdocument set.
Keeping in mind these constraints,we present two approaches on query translationoptimisation.One of the important observations done dur-ing the CLEF 2009 campaign (Ferro and Peters,2009) related to CLIR was that the usage of Sta-tistical Machine Translation (SMT) systems (eg.Google Translate) for query translation led toimportant improvements in the cross-lingual re-trieval performance (the best CLIR performanceincreased from ?55% of the monolingual baselinein 2008 to more than 90% in 2009 for Frenchand German target languages).
However, general-purpose SMT systems are not necessarily adaptedfor query translation.
That is because SMT sys-tems trained on a corpus of standard parallelphrases take into account the phrase structure im-plicitly.
The structure of queries is very differ-ent from the standard phrase structure: queries arevery short and the word order might be differentthan the typical full phrase one.
This problem canbe seen as a problem of genre adaptation for SMT,where the genre is ?query?.To our knowledge, no suitable corpora of par-allel queries is available to train an adapted SMTsystem.
Small corpora of parallel queries1 how-ever can be obtained (eg.
CLEF tracks) or man-ually created.
We suggest to use such corporain order to adapt the SMT model parameters forquery translation.
In our approach the parametersof the SMT models are optimized on the basis ofthe parallel queries set.
This is achieved either di-rectly in the SMT system using the MERT (Mini-mum Error Rate Training) algorithm and optimiz-1Insufficient for a full SMT system training (?500 entries)109ing according to the BLEU2(Papineni et al 2001)score, or via reranking the Nbest translation can-didates generated by a baseline system based onnew parameters (and possibly new features) thataim to optimize a retrieval metric.It is important to note that both of the pro-posed approaches allow keeping the MT systemindependent of the document collection and in-dexing, and thus suitable for a query translationservice.
These two approaches can also be com-bined by using the model produced with the firstapproach as a baseline that produces the Nbest listof translations that is then given to the rerankingapproach.The remainder of this paper is organized as fol-lows.
We first present related work addressing theproblem of query translation.
We then describetwo approaches towards adapting an SMT systemto the query-genre: tuning the SMT system on aparallel set of queries (Section 3.1) and adaptingmachine translation via the reranking framework(Section 3.2).
We then present our experimentalsettings and results (Section 4) and conclude insection 5.2 Related workWe may distinguish two main groups of ap-proaches to CLIR: document translation andquery translation.
We concentrate on the secondgroup which is more relevant to our settings.
Thestandard query translation methods use differenttranslation resources such as bilingual dictionar-ies, parallel corpora and/or machine translation.The aspect of disambiguation is important for thefirst two techniques.Different methods were proposed to deal withdisambiguation issues, often relying on the docu-ment collection or embedding the translation stepdirectly into the retrieval model (Hiemstra andJong, 1999; Berger et al 1999; Kraaij et al2003).
Other methods rely on external resourceslike query logs (Gao et al 2010), Wikipedia (Ja-didinejad and Mahmoudi, 2009) or the web (Nieand Chen, 2002; Hu et al 2008).
(Gao et al2006) proposes syntax-based translation modelsto deal with the disambiguation issues (NP-based,dependency-based).
The candidate translationsproposed by these models are then reranked withthe model learned to minimize the translation er-2Standard MT evaluation metricror on the training data.To our knowledge, existing work that use MT-based techniques for query translation use an out-of-the-box MT system, without adapting it forquery translation in particular (Jones et al 1999;Wu et al 2008) (although some query expan-sion techniques might be applied to the producedtranslation afterwards (Wu and He, 2010)).There is a number of works done for do-main adaptation in Statistical Machine Transla-tion.
However, we want to distinguish betweengenre and domain adaptation in this work.
Gen-erally, genre can be seen as a sub-problem of do-main.
Thus, we consider genre to be the generalstyle of the text e.g.
conversation, news, blog,query (responsible mostly for the text structure)while the domain reflects more what the text isabout ?
eg.
social science, healthcare, history, sodomain adaptation involves lexical disambigua-tion and extra lexical coverage problems.
To ourknowledge, there is not much work addressing ex-plicitly the problem of genre adaptation for SMT.Some work done on domain adaptation could beapplied to genre adaptation, such as incorporatingavailable in-domain corpora in the SMT model:either monolingual (Bertoldi and Federico, 2009;Wu et al 2008; Zhao et al 2004; Koehn andSchroeder, 2007), or small parallel data used fortuning the SMT parameters (Zheng et al 2010;Pecina et al 2011).3 Our approachThis work is based on the hypothesis that thegeneral-purpose SMT system needs to be adaptedfor query translation.
Although in (Ferro andPeters, 2009) it has been mentioned that usingGoogle translate (general-purpose MT) for querytranslation allowed to CLEF participants to obtainthe best CLIR performance, there is still 10% gapbetween monolingual and cross-lingual IR.
Webelieve that, as in (Clinchant and Renders, 2007),more adapted query translation, possibly furthercombined with query expansion techniques, canlead to improved retrieval.The problem of the SMT adaptation for query-genre translation has different quality aspects.On the one hand, we want our model to pro-duce a ?good?
translation (well-formed and trans-mitting the information contained in the sourcequery) of an input query.
On the other hand, wewant to obtain good retrieval performance using110the proposed translation.
These two aspects arenot necessarily correlated: a bag-of-word transla-tion can lead to good retrieval performance, eventhough it won?t be syntactically well-formed; atthe same time a well-formed translation can leadto worse retrieval if the wrong lexical choice isdone.
Moreover, often the retrieval demands somelinguistic preprocessing (eg.
lemmatisation, PoStagging) which in interaction with badly-formedtranslations might bring some noise.A couple of works studied the correlation be-tween the standard MT evaluation metrics andthe retrieval precision.
Thus, (Fujii et al 2009)showed a good correlation of the BLEU scoreswith the MAP scores for Cross-Lingual PatentRetrieval.
However, the topics in patent search(long and well structured) are very different fromstandard queries.
(Kettunen, 2009) also found apretty high correlation ( 0.8 ?
0.9) between stan-dard MT evaluation metrics (METEOR(Banerjeeand Lavie, 2005), BLEU, NIST(Doddington,2002)) and retrieval precision for long queries.However, the same work shows that the correla-tion decreases ( 0.6?
0.7) for short queries.In this paper we propose two approaches toSMT adaptation for queries.
The first one op-timizes BLEU, while the second one optimizesMean Average Precision (MAP), a standard met-ric in information retrieval.
We?ll address the is-sue of the correlation between BLEU and MAP inSection 4.Both of the proposed approaches rely on thephrase-based SMT (PBMT) model (Koehn et al2003) implemented in the Open Source SMTtoolkit MOSES (Koehn et al 2007).3.1 Tuning for genre adaptationFirst, we propose to adapt the PBMT model bytuning the model?s weights on a parallel set ofqueries.
This approach addresses the first as-pect of the problem, which is producing a ?good?translation.
The PBMT model combines differ-ent types of features via a log-linear model.
Thestandard features include (Koehn, 2010, Chapter5): language model, word penalty, distortion, dif-ferent translation models, etc.
The weights ofthese features are learned during the tuning stepwith the MERT (Och, 2003) algorithm.
Roughlythe MERT algorithm tunes feature weights one byone and optimizes them according to the BLEUscore obtained.Our hypothesis is that the impact of differentfeatures should be different depending on whetherwe translate a full sentence, or a query-genre en-try.
Thus, one would expect that in the caseof query-genre the language model or the distor-tion features should get less importance than inthe case of the full-sentence translation.
MERTtuning on a genre-adapted parallel corpus shouldleverage this information from the data, adaptingthe SMT model to the query-genre.
We wouldalso like to note that the tuning approach (pro-posed for domain adaptation by (Zheng et al2010)) seems to be more appropriate for genreadaptation than for domain adaptation where theproblem of lexical ambiguity is encoded in thetranslation model and re-weighting the main fea-tures might not be sufficient.We use the MERT implementation providedwith the Moses toolkit with default settings.
Ourassumption is that this procedure although not ex-plicitly aimed at improving retrieval performancewill nevertheless lead to ?better?
query transla-tions when compared to the baseline.
The resultsof this apporach allow us also to observe whetherand to what extent changes in BLEU scores arecorrelated to changes in MAP scores.3.2 Reranking framework for querytranslationThe second approach addresses the retrieval qual-ity problem.
An SMT system is usually trained tooptimize the quality of the translation (eg.
BLEUscore for SMT), which is not necessarily corre-lated with the retrieval quality (especially for theshort queries).
Thus, for example, the word or-der which is crucial for translation quality (and istaken into account by most MT evaluation met-rics) is often ignored by IR models.
Our secondapproach follows (Nie, 2010, pp.106) argumentthat ?the translation problem is an integral partof the whole CLIR problem, and unified CLIRmodels integrating translation should be defined?.We propose integrating the IR metric (MAP) intothe translation model optimisation step via thereranking framework.Previous attempts to apply the reranking ap-proach to SMT did not show significant improve-ments in terms of MT evaluation metrics (Ochet al 2003; Nikoulina and Dymetman, 2008).One of the reasons being the poor diversity of theNbest list of the translations.
However, we be-111lieve that this approach has more potential in thecontext of query translation.First of all the average query length is ?5 words,which means that the Nbest list of the translationsis more diverse than in the case of general phrasetranslation (average length 25-30 words).Moreover, the retrieval precision is more natu-rally integrated into the reranking framework thanstandard MT evaluation metrics such as BLEU.The main reason is that the notion of Average Re-trieval Precision is well defined for a single querytranslation, while BLEU is defined on the corpuslevel and correlates poorly with human qualityjudgements for the individual translations (Speciaet al 2009; Callison-Burch et al 2009).Finally, the reranking framework allows a lotof flexibility.
Thus, it allows enriching the base-line translation model with new complex featureswhich might be difficult to introduce into thetranslation model directly.Other works applied the reranking frameworkto different NLP tasks such as Named EntitiesExtraction (Collins, 2001), parsing (Collins andRoark, 2004), and language modelling (Roark etal., 2004).
Most of these works used the rerankingframework to combine generative and discrimina-tive methods when both approaches aim at solv-ing the same problem: the generative model pro-duces a set of hypotheses, and the best hypoth-esis is chosen afterwards via the discriminativereranking model, which allows to enrich the base-line model with the new complex and heteroge-neous features.
We suggest using the rerankingframework to combine two different tasks: Ma-chine Translation and Cross-lingual InformationRetrieval.
In this context the reranking frameworkdoesn?t only allow enriching the baseline transla-tion model but also performing training using amore appropriate evaluation metric.3.2.1 Reranking trainingGenerally, the reranking framework can be re-sumed in the following steps :1.
The baseline (generic-purpose) MT systemgenerates a list of candidate translationsGEN(q) for each query q;2.
A vector of features F (t) is assigned to eachtranslation t ?
GEN(q);3.
The best translation t?
is chosen as the onemaximizing the translation score, which isdefined as a weighted linear combination offeatures: t?(?)
= argmaxt?GEN(q) ?
?F (t)As shown above the best translation is selected ac-cording to features?
weights ?.
In order to learnthe weights ?
maximizing the retrieval perfor-mance, an appropriate annotated training set hasto be created.
We use the CLEF tracks to createthe training set.
The retrieval scores annotationsare based on the document relevance annotationsperformed by human annotators during the CLEFcampaign.The annotated training set is created out ofqueries {q1, ..., qK} with an Nbest list of trans-lations GEN(qi) of each query qi, i ?
{1..K} asfollows:?
A list of N (we take N = 1000) translations(GEN(qi)) is produced by the baseline MTmodel for each query qi, i = 1..K.?
Each translation t ?
GEN(qi) is usedto perform a retrieval from a target docu-ment collection, and an Average Precisionscore (AP (t)) is computed for each t ?GEN(qi) by comparing its retrieval to therelevance annotations done during the CLEFcampaign.The weights ?
are learned with the objective ofmaximizing MAP for all the queries of the train-ing set, and, therefore, are optimized for retrievalquality.The weights optimization is done withthe Margin Infused Relaxed Algorithm(MIRA)(Crammer and Singer, 2003), whichwas applied to SMT by (Watanabe et al 2007;Chiang et al 2008).
MIRA is an online learningalgorithm where each weights update is done tokeep the new weights as close as possible to theold weights (first term), and score oracle trans-lation (the translation giving the best retrievalscore : t?i = argmaxtAP (t)) higher than eachnon-oracle translation (tij) by a margin at least aswide as the loss lij (second term):?
= min??12????
?
?2 +C?Ki=1 maxj=1..N(lij ?
???
(F (t?i )?
F (tij))The loss lij is defined as the difference in the re-trieval average precision between the oracle andnon-oracle translations: lij = AP (t?i )?AP (tij).C is the regularization parameter which is chosenvia 5-fold cross-validation.1123.2.2 FeaturesOne of the advantages of the reranking frame-work is that new complex features can be easilyintegrated.
We suggest to enrich the rerankingmodel with different syntax-based features, suchas:?
features relying on dependency structures:called therein coupling features (proposed by(Nikoulina and Dymetman, 2008));?
features relying on Part of Speech Tagging:called therein PoS mapping features.By integrating the syntax-based features wehave a double goal: showing the potential ofthe reranking framework with more complex fea-tures, and examining whether the integration ofsyntactic information could be useful for querytranslation.Coupling features.
The goal of the couplingfeatures is to measure the similarity betweensource and target dependency structures.
The ini-tial hypothesis is that a better translation shouldhave a dependency structure closer to the one ofthe source query.In this work we experiment with two dif-ferent coupling variants proposed in (Nikoulinaand Dymetman, 2008), namely, Lexicalised andLabel-specific coupling features.The generic coupling features are based onthe notion of ?rectangles?
that are of the follow-ing type : ((s1, ds12, s2), (t1, dt12, t2)), whereds12 is an edge between source words s1 and s2,dt12 is an edge between target words t1 and t2,s1 is aligned with t1 and s2 is aligned with t2.Lexicalised features take into account the qual-ity of lexical alignment, by weighting each rect-angle (s1, s2, t1, t2) by a probability of align-ing s1 to t1 and s2 to t2 (eg.
p(s1|t1)p(s2|t2) orp(t1|s1)p(t2|s2)).The Label-Specific features take into accountthe nature of the aligned dependencies.
Thus, therectangles of the form ((s1, subj, s2), (t1, subj,t2)) will get more weight than a rectangle ((s1,subj, s2), (t1, nmod, t2)).
The importance ofeach ?rectangle?
is learned on the parallel anno-tated corpus by introducing a collection of Label-Specific coupling features, each for a specific pairof source label and target label.PoS mapping features.
The goal of the PoSmapping features is to control the correspondenceof Part Of Speech Tags between an input queryand its translation.
As the coupling features, thePoS mapping features rely on the word align-ments between the source sentence and its trans-lation3.
A vector of sparse features is introducedwhere each component corresponds to a pair ofPoS tags aligned in the training data.
We intro-duce a generic PoS map variant, which counts anumber of occurrences of a specific pair of PoStags, and lexical PoS map variant, which weightsdown these pairs by a lexical alignment score(p(s|t) or p(t|s)).4 Experiments4.1 Experimental basis4.1.1 DataTo simulate parallel query data we used trans-lation equivalent CLEF topics.
The data set usedfor the first approach consists of the CLEF topicdata from the following years and tasks: AdHoc-main track from 2000 to 2008; CLEF AdHoc-TEL track 2008; Domain Specific tracks from2000 to 2008; CLEF robust tracks 2007 and 2008;GeoCLEf tracks 2005-2007.
To avoid the issue ofoverlapping topics we removed duplicates.
Thecreated parallel queries set contained 500 ?
700parallel entries (depending on the language pair,Table 1) and was used for Moses parameters tun-ing.In order to create the training set for the rerank-ing approach, we need to have access to the rele-vance judgements.
We didn?t have access to allrelevance judgements of the previously desribedtracks.
Thus we used only a subset of the previ-ously extracted parallel set, which includes CLEF2000-2008 topics from the AdHoc-main, AdHoc-TEL and GeoCLEF tracks.The number of queries obtained altogether isshown in (Table 1).4.1.2 BaselineWe tested our approaches on the CLEF AdHoc-TEL 2009 task (50 topics).
This task dealtwith monolingual and cross-lingual search in alibrary catalog.
The monolingual retrieval is3This alignment can be either produced by a toolkit likeGIZA++(Och and Ney, 2003) or obtained directly by a sys-tem that produced the Nbest list of the translations (Moses).113Language pair Number of queriesTotal queriesEn - Fr, Fr - En 470En - De, De - En 714Annotated queriesEn - Fr, Fr - En 400En - De, De - En 350Table 1: Top: total number of parallel queries gatheredfrom all the CLEF tasks (size of the tuning set).
Bot-tom: number of queries extracted from the tasks forwhich the human relevance judgements were availble(size of the reranking training set).performed with the lemur4 toolkit (Ogilvie andCallan, 2001).
The preprocessing includes lem-matisation (with the Xerox Incremental Parser-XIP (A?
?t-Mokhtar et al 2002)) and filtering outthe function words (based on XIP PoS tagging).Table 2 shows the performance of the monolin-gual retrieval model for each collection.
Themonolingual retrieval results are comparable tothe CLEF AdHoc-TEL 2009 participants (Ferroand Peters, 2009).
Let us note here that it is notthe case for our CLIR results since we didn?t ex-ploit the fact that each of the collections could ac-tually contain the entries in a language other thanthe official language of the collection.The cross-lingual retrieval is performed as fol-lows :?
the input query (eg.
in English) is first trans-lated into the language of the collection (eg.German);?
this translation is used to search the targetcollection (eg.
Austrian National Library forGerman ) .The baseline translation is produced withMoses trained on Europarl.
Table 2 reports thebaseline performance both in terms of MT evalu-ation metrics (BLEU) and Information Retrievalevaluation metric MAP (Mean Average Preci-sion).The 1best MAP score corresponds to the casewhen the single translation is proposed for theretrieval by the query translation model.
5bestMAP score corresponds to the case when the 5top translations proposed by the translation ser-vice are concatenated and used for the retrieval.4http://www.lemurproject.org/The 5best retrieval can be seen as a sort of queryexpansion, without accessing the document col-lection or any external resources.Given that the query length is shorter than for astandard sentence, the 4-gramm BLEU (used forstandart MT evaluation) might not be able to cap-ture the difference between the translations (eg.English-German 4-gramm BLEU is equal to 0 forour task).
For that reason we report both 3- and4-gramm BLEU scores.Note, that the French-English baseline retrievalquality is much better than the German-English.This is probably due to the fact that our German-English translation system doesn?t use any de-coumpounding, which results into many non-translated words.4.2 ResultsWe performed the query-genre adaptation ex-periments for English-French, French-English,German-English and English-German languagepairs.Ideally, we would have liked to combine thetwo approaches we proposed: use the query-genre-tuned model to produce the Nbest listwhich is then reranked to optimize the MAPscore.
However, it was not possible in our exper-imental settings due to the small amount of train-ing data available.
We thus simply compare thesetwo approaches to a baseline approach and com-ment on their respective performance.4.2.1 Query-genre tuning approachFor the CLEF-tuning experiments we used thesame translation model and language model as forthe baseline (Europarl-based).
The weights werethen tuned on the CLEF topics described in sec-tion 4.1.1.
We then tested the system obtained on50 parallel queries from the CLEF AdHoc-TEL2009 task.Table 3 describes the results of the evalua-tion.
We observe consistent 1-best MAP improve-ments, but unstable BLEU (3-gramm) (improve-ments for English-German, and degradation forother language pairs), although one would haveexpected BLEU to be improved in this experi-mental setting given that BLEU was the objectivefunction for MERT.
These results, on one side,confirm the remark of (Kettunen, 2009) that thereis a correlation (although low) between BLEUand MAP scores.
The unstable BLEU scores114MAPMAP MAP BLEU BLEU1-best 5-best 4-gramm 3-grammMonolingual IR Bilingual IREnglish 0.3159French-English 0.1828 0.2186 0.1199 0.1568German-English 0.0941 0.0942 0.2351 0.2923French 0.2386 English-French 0.1504 0.1543 0.2863 0.3423German 0.2162 English-German 0.1009 0.1157 0.0000 0.1218Table 2: Baseline MAP scores for monolingual and bilingual CLEF AdHoc TEL 2009 task.MAP MAP BLEU BLEU1-best 5-best 4-gramm 3-grammFr-En 0.1954 0.2229 0.1062 0.1489De-En 0.1018 0.1078 0.2240 0.2486En-Fr 0.1611 0.1516 0.2072 0.2908En-De 0.1062 0.1132 0.0000 0.1924Table 3: BLEU and MAP performance on CLEF AdHoc TEL 2009 task for the genre-tuned model.might also be explained by the small size of thetest set (compared to a standard test set of 1000full-sentences).Secondly, we looked at the weights of the fea-tures both in the baseline model (Europarl-tuned)and in the adapted model (CLEF-tuned), shown inTable 4.
We are unsure how suitable the sizes ofthe CLEF tuning sets are, especially for the pairsinvolving English and French.
Nevertheless wedo observe and comment on some patterns.For the pairs involving English and Germanthe distortion weight is much higher when tuningwith CLEF data compared to tuning with Europarldata.
The picture is reversed when looking at thetwo pairs involving English and French.
This isto be expected if we interpret a high distortionweight as follows: ?it is not encouraged to placesource words that are near to each other far awayfrom each other in the translation?.
Indeed, the lo-cal reorderings are much more frequent betweenEnglish and French (e.g.
white house = maisonblanche), while the long-distance reorderings aremore typcal between English and German.The word penalty is consistenly higher over allpairs when tuning with CLEF data compared totuning with Europarl data.
We could see an ex-planation for this pattern in the smaller size ofthe CLEF sentences if we interpret higher wordpenalty as a preference for shorter translations.This can be explained both with the smaller aver-age size of the queries and with the specific querystructure: mostly content words and fewer func-tion words when compared to the full sentence.The language model weight is consistentlythough not drastically smaller when tuning withCLEF data.
We suppose that this is due to thefact that a Europarl-base language model is notthe best choice for translating query data.4.2.2 Reranking approachThe reranking experiments include differentfeatures combinations.
First, we experiment withthe Moses features only in order to make this ap-proach comparable with the first one.
Secondly,we compare different syntax-based features com-binations, as described in section 3.2.2.
Thus, wecompare the following reranking models (definedby the feature set): moses, lex (lexical coupling+ moses features), lab (label-specific coupling +moses features), posmaplex (lexical PoS mapping+ moses features ), lab-lex (label-specific cou-pling + lexical coupling + moses features), lab-lex-posmap (label-specific coupling + lexical cou-pling features + generic PoS mapping).
To reducethe size of feature-functions vectors we take onlythe 20 most frequent features in the training datafor Label-specific coupling and PoS mapping fea-tures.
The computation of the syntax features isbased on the rule-based XIP parser, where someheuristics specific to query processing have beenintegrated into English and French (but not Ger-man) grammars (Brun et al 2012).The results of these experiments are illustrated115Lng pair Tune set DW LM ?
(f |e) lex(f |e) ?
(e|f) lex(e|f) PP WPFr-EnEuroparl 0.0801 0.1397 0.0431 0.0625 0.1463 0.0638 -0.0670 -0.3975CLEF 0.0015 0.0795 -0.0046 0.0348 0.1977 0.0208 -0.2904 0.3707De-EnEuroparl 0.0588 0.1341 0.0380 0.0181 0.1382 0.0398 -0.0904 -0.4822CLEF 0.3568 0.1151 0.1168 0.0549 0.0932 0.0805 0.0391 -0.1434En-FrEuroparl 0.0789 0.1373 0.0002 0.0766 0.1798 0.0293 -0.0978 -0.4002CLEF 0.0322 0.1251 0.0350 0.1023 0.0534 0.0365 -0.3182 -0.2972En-DeEuroparl 0.0584 0.1396 0.0092 0.0821 0.1823 0.0437 -0.1613 -0.3233CLEF 0.3451 0.1001 0.0248 0.0872 0.2629 0.0153 -0.0431 0.1214Table 4: Feature weights for the query-genre tuned model.
Abbreviations: DW - distortion weight, LM - languagemodel weight, PP - phrase penalty, WP - word penalty, ?-phrase translation probability, lex-lexical weighting.Query Example MAP bleu1Src1 Weibliche Ma?rtyrerRef Female MartyrsT1 female martyrs 0.07 1T2 Women martyr 0.4 0Src 2 Genmanipulation amMenschenRef Human Gene Manipula-tionT1 On the genetic manipula-tion of people0.044 0.167T2 genetic manipulation ofthe human being0.069 0.286Src 3 Arbeitsrecht in der Eu-ropa?ischen UnionRef European Union LabourLawsT1 Labour law in the Euro-pean Union0.015 0.5T2 labour legislation in theEuropean Union0.036 0.5Table 5: Some examples of queries translations (T1:baseline, T2: after reranking with lab-lex), MAP and1-gramm BLEU scores for German-English.in Figure 1.
To keep the figure more readable,we report only on 3-gramm BLEU scores.
Whencomputing the 5best MAP score, the order in theNbest list is defined by a corresponding rerankingmodel.
Each reranking model is illustrated by asingle horizontal red bar.
We compare the rerank-ing results to the baseline model (vertical line) andalso to the results of the first approach (yellow barlabelled MERT:moses) on the same figure.First, we remark that the adapted models(query-genre tuning and reranking) outperformthe baseline in terms of MAP (1best and 5 best)for French-English and German-English transla-tions for most of the models.
The only exceptionis posmaplex model (based on PoS tagging) forGerman which can be explained by the fact thatthe German grammar used for query processingwas not adapted for queries as opposed to Englishand French grammars.
However, we do not ob-serve the same tendency for BLEU score, whereonly a few of the adapted models outperform thebaseline, which confirms the hypothesis of thelow correlation between BLEU and MAP scoresin these settings.
Table 5 gives some examples ofthe queries translations before (T1) and after (T2)reranking.
These examples also illustrate differ-ent types of disagreement between MAP and 1-gramm BLEU5 score.The results for English-German and English-French look more confusing.
This can be partlydue to the more rich morphology of the target lan-guages which may create more noise in the syn-tax structure.
Reranking however improves overthe 1-best MAP baseline for English-German, and5-best MAP is also improved excluding the mod-els involving PoS tagging for German (posmap,posmaplex, lab-lex-posmap).
The results forEnglish-French are more difficult to interpret.
Tofind out the reason of such a behavior, we lookedat the translations.
We observed the following to-kenization problem for French: the apostrophe issystematically separated, e.g.
?d ?
aujourd ?
hui?.This leads to both noisy pre-retrieval preprocess-ing (eg.
d is tagged as a NOUN) and noisy syntax-based feature values, which might explain the un-stable results.Finally, we can see that the syntax-based fea-tures can be beneficial for the final retrieval qual-ity: the models with syntax features can outper-form the model basd on the moses features only.The syntax-based features leading to the most sta-5The higher order BLEU scores are equal to 0 for mostof the individual translations.116Figure 1: Reranking results.
The vertical line corresponds to the baseline scores.
The lowest bar (MERT:moses,in yellow): the results of the tuning approach, other bars(in red): the results of the reranking approach.ble results seem to be lab-lex (combination of lex-ical and label-specific coupling): it leads to thebest gains over 1-best and 5-best MAP for all lan-guage pairs excluding English-French.
This is asurprising result given the fact that the underlyingIR model doesn?t take syntax into account in anyway.
In our opinion, this is probably due to theinteraction between the pre-retrieval preprocess-ing (lemmatisation, PoS tagging) done with thelinguistic tools which might produce noisy resultswhen applied to the SMT outputs.
The rerank-ing with syntax-based features allows to choosea better-formed query for which the PoS taggingand lemmatisation tools produce less noise whichleads to a better retrieval.5 ConclusionIn this work we proposed two methods for query-genre adaptation of an SMT model: the firstmethod addressing the translation quality aspectand the second one the retrieval precision aspect.We have shown that CLIR performance in termsof MAP is improved between 1-2.5 points.
Webelieve that the combination of these two meth-ods would be the most beneficial setting, althoughwe were not able to prove this experimentally(due to the lack of training data).
None of thesemethods require access to the document collec-tion at test time, and can be used in the contextof a query translation service.
The combinationof our adapted SMT model with other state-of-theart CLIR techniques (eg.
query expansion withPRF) will be explored in future work.AcknowledgementsThis research was supported by the EuropeanUnion?s ICT Policy Support Programme as part ofthe Competitiveness and Innovation FrameworkProgramme, CIP ICT-PSP under grant agreementnr 250430 (Project GALATEAS).ReferencesSalah A?
?t-Mokhtar, Jean-Pierre Chanod, and ClaudeRoux.
2002.
Robustness beyond shallowness: in-117cremental deep parsing.
Natural Language Engi-neering, 8:121?144, June.Satanjeev Banerjee and Alon Lavie.
2005.
METEOR:an automatic metric for MT evaluation with im-proved correlation with human judgments.
In Pro-ceedings of the ACL Workshop on Intrinsic and Ex-trinsic Evaluation Measures for Machine Transla-tion and/or Summarization, pages 65?72, Ann Ar-bor, Michigan, June.
Association for ComputationalLinguistics.Adam Berger, John Lafferty, and John La Erty.
1999.The weaver system for document retrieval.
In InProceedings of the Eighth Text REtrieval Confer-ence (TREC-8, pages 163?174.Nicola Bertoldi and Marcello Federico.
2009.
Do-main adaptation for statistical machine translationwith monolingual resources.
In Proceedings ofthe Fourth Workshop on Statistical Machine Trans-lation, pages 182?189.
Association for Computa-tional Linguistics.Caroline Brun, Vassilina Nikoulina, and Nikolaos La-gos.
2012.
Linguistically-adapted structural queryannotation for digital libraries in the social sciences.In Proceedings of the 6th EACL Workshop on Lan-guage Technology for Cultural Heritage, Social Sci-ences, and Humanities, Avignon, France, April.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Josh Schroeder.
2009.
Findings of the 2009Workshop on Statistical Machine Translation.
InProceedings of the Fourth Workshop on StatisticalMachine Translation, pages 1?28, Athens, Greece,March.
Association for Computational Linguistics.David Chiang, Yuval Marton, and Philip Resnik.2008.
Online large-margin training of syntactic andstructural translation features.
In Proceedings of the2008 Conference on Empirical Methods in NaturalLanguage Processing, pages 224?233.
Associationfor Computational Linguistics.Ste?phane Clinchant and Jean-Michel Renders.
2007.Query translation through dictionary adaptation.
InCLEF?07, pages 182?187.Michael Collins and Brian Roark.
2004.
Incrementalparsing with the perceptron algorithm.
In ACL ?04:Proceedings of the 42nd Annual Meeting on Asso-ciation for Computational Linguistics.Michael Collins.
2001.
Ranking algorithms fornamed-entity extraction: boosting and the votedperceptron.
In ACL?02: Proceedings of the 40thAnnual Meeting on Association for ComputationalLinguistics, pages 489?496, Philadelphia, Pennsyl-vania.
Association for Computational Linguistics.Koby Crammer and Yoram Singer.
2003.
Ultracon-servative online algorithms for multiclass problems.Journal of Machine Learning Research, 3:951?991.George Doddington.
2002.
Automatic evaluationof Machine Translation quality using n-gram co-occurrence statistics.
In Proceedings of the sec-ond international conference on Human LanguageTechnology Research, pages 138?145, San Diego,California.
Morgan Kaufmann Publishers Inc.Nicola Ferro and Carol Peters.
2009.
CLEF 2009ad hoc track overview: TEL and persian tasks.In Working Notes for the CLEF 2009 Workshop,Corfu, Greece.Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, andTakehito Utsuro.
2009.
Evaluating effects of ma-chine translation accuracy on cross-lingual patentretrieval.
In Proceedings of the 32nd internationalACM SIGIR conference on Research and develop-ment in information retrieval, SIGIR ?09, pages674?675.Jianfeng Gao, Jian-Yun Nie, and Ming Zhou.
2006.Statistical query translation models for cross-language information retrieval.
5:323?359, Decem-ber.Wei Gao, Cheng Niu, Jian-Yun Nie, Ming Zhou, Kam-Fai Wong, and Hsiao-Wuen Hon.
2010.
Exploit-ing query logs for cross-lingual query suggestions.ACM Trans.
Inf.
Syst., 28(2).Djoerd Hiemstra and Franciska de Jong.
1999.
Dis-ambiguation strategies for cross-language informa-tion retrieval.
In Proceedings of the Third EuropeanConference on Research and Advanced Technologyfor Digital Libraries, pages 274?293.Rong Hu, Weizhu Chen, Peng Bai, Yansheng Lu,Zheng Chen, and Qiang Yang.
2008.
Web querytranslation via web log mining.
In Proceedings ofthe 31st annual international ACM SIGIR confer-ence on Research and development in informationretrieval, SIGIR ?08, pages 749?750.
ACM.Amir Hossein Jadidinejad and Fariborz Mahmoudi.2009.
Cross-language information retrieval us-ing meta-language index construction and structuralqueries.
In Proceedings of the 10th cross-languageevaluation forum conference on Multilingual in-formation access evaluation: text retrieval experi-ments, CLEF?09, pages 70?77, Berlin, Heidelberg.Springer-Verlag.Gareth Jones, Sakai Tetsuya, Nigel Collier, Akira Ku-mano, and Kazuo Sumita.
1999.
Exploring theuse of machine translation resources for english-japanese cross-language information retrieval.
In InProceedings of MT Summit VII Workshop on Ma-chine Translation for Cross Language InformationRetrieval, pages 181?188.Kimmo Kettunen.
2009.
Choosing the best mt pro-grams for clir purposes ?
can mt metrics be help-ful?
In Proceedings of the 31th European Confer-ence on IR Research on Advances in InformationRetrieval, ECIR ?09, pages 706?712, Berlin, Hei-delberg.
Springer-Verlag.Philipp Koehn and Josh Schroeder.
2007.
Experi-ments in domain adaptation for statistical machinetranslation.
In Proceedings of the Second Work-shop on Statistical Machine Translation, StatMT118?07, pages 224?227.
Association for ComputationalLinguistics.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
InNAACL ?03: Proceedings of the 2003 Conferenceof the North American Chapter of the Associationfor Computational Linguistics on Human LanguageTechnology, pages 48?54, Morristown, NJ, USA.Association for Computational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch,Chris Callison-Burch, Marcello Federico, NicolaBertoldi, Brooke Cowan, Wade Shen, ChristineMoran, Richard Zens, Chris Dyer, Ondr?ej Bojar,Alexandra Constantin, and Evan Herbst.
2007.Moses: open source toolkit for statistical machinetranslation.
In ACL ?07: Proceedings of the 45thAnnual Meeting of the ACL on Interactive Posterand Demonstration Sessions, pages 177?180.
As-sociation for Computational Linguistics.Philip Koehn.
2010.
Statistical Machine Translation.Cambridge University Press.Wessel Kraaij, Jian-Yun Nie, and Michel Simard.2003.
Embedding web-based statistical trans-lation models in cross-language information re-trieval.
Computational Linguistiques, 29:381?419,September.Jian-yun Nie and Jiang Chen.
2002.
Exploiting theweb as parallel corpora for cross-language informa-tion retrieval.
Web Intelligence, pages 218?239.Jian-Yun Nie.
2010.
Cross-Language Information Re-trieval.
Morgan & Claypool Publishers.Vassilina Nikoulina and Marc Dymetman.
2008.
Ex-periments in discriminating phrase-based transla-tions on the basis of syntactic coupling features.
InProceedings of the ACL-08: HLT Second Workshopon Syntax and Structure in Statistical Translation(SSST-2), pages 55?60.
Association for Computa-tional Linguistics, June.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,Anoop Sarkar, Kenji Yamada, Alex Fraser, ShankarKumar, Libin Shen, David Smith, Katherine Eng,Viren Jain, Zhen Jin, and Dragomir Radev.
2003.Syntax for Statistical Machine Translation: Finalreport of John Hopkins 2003 Summer Workshop.Technical report, John Hopkins University.Franz Josef Och.
2003.
Minimum error rate train-ing in statistical machine translation.
In ACL ?03:Proceedings of the 41st Annual Meeting on Asso-ciation for Computational Linguistics, pages 160?167, Morristown, NJ, USA.
Association for Com-putational Linguistics.Paul Ogilvie and James P. Callan.
2001.
Experimentsusing the lemur toolkit.
In TREC.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2001.Bleu: a method for automatic evaluation of machinetranslation.Pavel Pecina, Antonio Toral, Andy Way, Vassilis Pa-pavassiliou, Prokopis Prokopidis, and Maria Gi-agkou.
2011.
Towards using web-crawled data fordomain adaptation in statistical machine translation.In Proceedings of the 15th Annual Conference ofthe European Associtation for Machine Translation,pages 297?304, Leuven, Belgium.
European Asso-ciation for Machine Translation.Brian Roark, Murat Saraclar, Michael Collins, andMark Johnson.
2004.
Discriminative languagemodeling with conditional random fields and theperceptron algorithm.
In Proceedings of the 42ndAnnual Meeting of the Association for Computa-tional Linguistics (ACL?04), July.Lucia Specia, Marco Turchi, Nicola Cancedda, MarcDymetman, and Nello Cristianini.
2009.
Estimat-ing the sentence-level quality of machine translationsystems.
In Proceedings of the 13th Annual Confer-ence of the EAMT, page 28?35, Barcelona, Spain.Taro Watanabe, Jun Suzuki, Hajime Tsukada, andHideki Isozaki.
2007.
Online large-margin train-ing for statistical machine translation.
In Proceed-ings of the 2007 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL), pages 764?773, Prague, Czech Republic.Association for Computational Linguistics.Dan Wu and Daqing He.
2010.
A study of querytranslation using google machine translation sys-tem.
Computational Intelligence and Software En-gineering (CiSE).Hua Wu, Haifeng Wang, and Chengqing Zong.
2008.Domain adaptation for statistical machine transla-tion with domain dictionary and monolingual cor-pora.
In Proceedings of the 22nd InternationalConference on Computational Linguistics (Col-ing2008), pages 993?100.Bing Zhao, Matthias Eck, and Stephan Vogel.
2004.Language model adaptation for statistical machinetranslation with structured query models.
In Pro-ceedings of the 20th international conference onComputational Linguistics, COLING ?04.
Associ-ation for Computational Linguistics.Zhongguang Zheng, Zhongjun He, Yao Meng, andHao Yu.
2010.
Domain adaptation for statisti-cal machine translation in development corpus se-lection.
In Universal Communication Symposium(IUCS), 2010 4th International, pages 2?7.
IEEE.119
