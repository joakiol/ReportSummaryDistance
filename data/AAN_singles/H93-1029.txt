VALIDATION OF TERMINOLOGICAL INFERENCEIN AN INFORMATION EXTRACTION TASKMarc VilainThe MITRE CorporationBurlington Rd.Bedford, MA 01730mbv@linus.mitre.orgABSTRACTThis paper is concerned with an inferential approach toinformation extraction, reporting in particular on the results of anempirical study that was performed to validate the approach.
Thestudy brings together two lines of research: (1) the RHO frame-work for tractable terminological knowledge representation, and(2) the Alembic message understanding system.
There arecorrespondingly two principal aspects of interest o this work.From the knowledge representation perspective, the present studyserves to validate xperimentally a normal form hypothesis thatguarantees tractability of inference in the RrtO framework.
Fromthe message processing perspective, this study substantiates theutility of limited inference to the information extraction task.1.
SOME BACKGROUNDAlembic is a natural language-based information extractionsystem that has been under development for about oneyear.
As with many such systems, the information ex-traction process in Alembic occurs through patternmatching against he semantic representation f sentences.These representations are themselves derived from parsingthe input text, in our case with a highly lexicalized neo-categorial grammar \[1\].Experience has shown that this kind of approach can yieldimpressive performance l vels in the data extraction task(see \[18\]).
We have found--as have others--thatmeaningful results can be obtained espite only havingsketchy sentence semantics (as can happen when there arewidespread gaps in the lexicon's emantic assignments).In addition, because the parsing process normalizes thesentence semantics to a significant degree, the number ofextraction patterns can be relatively small, especiallycompared to approaches that use only rudimentary parsing.Strict semantic pattern-matching is unattractive, however,in cases that presume some degree of inference.
Considerthe following example of an East-West joint venture:\[...\] Samsung signed an agreement with Soyuz, theexternal-trade organization of the Soviet Union, toswap Korean TV's and VCR's for pig iron from theSoviet UnionWhat makes this sentence an example of the given jointventure concept is an accumulation of small inferences:that Soyuz is a Soviet entity, that signing an agreementdesignates agreement between the signing parties, and thatthe resulting agreement holds between a Soviet and non-Soviet entity.
Such examples suggest that it is farpreferable to approach the extraction problem through a setof small inferences, rather than through some monolithicextraction pattern.
This notion has been embodied in anumber of earfier approaches, e.g.
\[11\] or \[17\].The inferential approach we were interested in bringing tobear on this problem is the RHO framework.
RHO is aterminological classification framework that ultimatelydescends from KL-ONE.
Unlike most recent such systems,however, RHO focuses on terminological inference (ratherthan subsumption).
And whereas most KL-ONE descen-dants sacrifice completeness for computational tractability,inference in RHO is complete in polynomial time if termi-nological axioms meet a normal form criterion.Nevertheless, before embarking on a significant develop-ment effort to implement he RHO framework underAlembic, we wanted to verify that the framework was up tothe data extraction task.
In particular, we were keen toensure that the theoretical criterion that guaranteespolynomial time completeness for RHO was actually met inpractice.
Towards this end, my colleagues and I undertookan extensive mpirical study whose goal was, amongothers, to validate this criterion.The present paper is a summary of our findings, with aspecial focus on RHO itself and on the validation task.
Weprovide some suggestive interpretations of these findings,and touch on current and ongoing work towards bringingRHO to bear on the extraction task in Alembic.2.
THE RHO FRAMEWORKThe RHO framework, as noted above, arose in reaction tostandard approaches to terminological reasoning, as em-bodied in most descendants of KL-ONE, e.g., CLASSIC \[4\],BACK \[13\], LOOM \[12\], and many others.
This line of workhas come to place a major emphasis on computing concept150subsumption, i.e., the determination f whether a represen-tational description (a concept) necessarily entails anotherdescription.
In our view, this emphasis mistaken.Indeed, this emphasis ignores the way in which practicalapplications have successfully exploited the terminologicalframework.
These systems primarily rely on the operationof classification, especially instance classification.Although subsumption helps to provide a semantic modelof classification, it does not necessarily follow that itshould provide its computational underpinnings.In addition, the emphasis on complete subsumption algo-rithms has led to restricted languages that are representa-tionally weak.
As is well-known, these languages havebeen the subject of increasingly pessimistic theoreticalresults, from intractability of subsumption \[5\], toundecidability of subsumption \[15, 16\], to intractability ofthe fundamental normalization f a terminological KB \[14\].Against this background, RHO was targeted to supportinstance classification, and thus departs in significant waysfrom traditional terminological reasoners.
The mostdraconian departure is in separating the normal termino-logical notion of necessary and sufficient definitions intoseparate sufficiency axioms and necessity axioms.
Thethrust of the former is to provide the kind of antecedentinference that is the hallmark of classification, e.g.,western-corp (x) ~ corporation (x) & hq-ln (x, y) (1)& western-nation (y)The role of necessity conditions is to provide consequentinference such as that typically associated with inheritanceand sort restrictions on predicates, e.g.,organization (x) ~-- corporation (x) (2)corporation (x) ~ western-corp (x) (3)organization (x) ~ agreement (x, y, z) (4)Although both classes of axioms are expressed in the samesyntactic garb, namely function-free Horn clauses, theydiffer with respect to their inferential import.
If one thinksof predicates as being organized according to sometaxonomy (see Fig.
1), then necessity axioms encode infe-rence that proceeds up the hierarchy (i.e., inheritance),while sufficiency axioms encode inference that proceedsdown the hierarchy (i.e., classification).The most interesting consequence of RHO's uniformlanguage for necessity and sufficiency is that it facilitatesthe formulation of a cfitedon under which classification isguaranteed to be tractable.
For a knowledge base to beguaranteed tractable, the criterion requires that there be atree shape to the implicit dependencies between thevariables in any given axiom in the knowledge base.For the sample axioms above, Fig.
2 informally illustratesthis notion of variable dependencies.
Axiom (1), fororganizationcorporationwestern-corpFigure 1: A predicate taxonomyX agree~ X ~ ~hq-iny y zFigure 2: Dependency trees for variables in axioms (1),on the left, and 0), on the fight.example, mentions two variables, x and y.
A dependencybetween these variables is introduced by the predicativeterm hq-in(x,y): the term makes the two variables depen-dent by virtue of mentioning them as arguments of thesame predicate.
As the axiom mentions no other variables,its dependency graph is the simple tree on the left of Fig.
1.Similarly, in axiom (4) the agreement predicate makesboth y and z dependent on x, also yielding a tree.
Finally,axioms (2) and (3) lead to degenerate rees containing onlyx.
Since all the dependency relations between thesevariables are tree-shaped, the knowledge base formed outof their respective axioms is tractable under the cfitedon.A formal proof that tractability follows from the cfitedonappears in an appendix below, as well as in \[19\].3.
VAL IDAT ING RHOThis formal tractability result is appealing, especially inlight of the overwhelming number of intractability claimsthat are usually associated with terminological reasoning.Its correctness, however, is crucially dependent on anormal form assumption, and as with all such normal formcntefia, it remains of little more than theoretical interestunless it is validated in practice.
As we mentioned above,we strove to achieve such a validation by determiningthrough a paper study whether the RHO framework couldbe put to use in the data extraction phase of Alembic.Towards this end, my colleagues and I assembled a set ofunbiased texts on Soviet economics.
The validation taskthen consisted of deriving a set of terminological rules thatwould allow RHO to perform the inferential patternmatching necessary toextract from these texts all instancesof a pre-determined class of target concepts.
Thehypothesis that RHO's tractability criterion can be met inpractice would thus be considered validated just in casethis set of inference rules was tractable under the criterion.151/ - -3.1.
Some assumptionsAt the time that we undertook the study, however, theAlembic implementation was still in its infancy.
We thushad to make a number of assumptions about what could beexpected out of Alembic's parsing and semantic ompo-sition components.
In so doing, we took great pain not torequire superhuman performance on the part of the parser,and restricted our expected syntactic overage to pheno-mena that we felt were well within the state of the art.In particular, we rejected the need to derive S. As withmany similar systems, Alembic uses a fragment parser thatproduces partial syntactic analyses when its grammar isinsufficient to derive S. In addition, we exploitedAlembic's hierarchy of syntactic ategories, and postulateda number of relatively fine-grained categories that were notcurrently in the system.
This allowed us for example toassume we could obtain the intended parse of "Irish-Sovietairline" on the basis of the pre-modifiers being bothadjectives of geographic origin (and hence co-ordinable).We also exploited the fact that the Alembic grammar ishighly lexicalized (being based on the combinatorialcategofial framework).
This allowed us to postulate somefairly detailed subcategorization frames for verbs and theirnominalizations.
As is cu~ently the case with our system,we assumed that verbs and their nominalizations arecanonicalized toidentical semantic representations.Elsewhere at the semantic level, we assumed basiccompetence at argument-passing, a characteristic already inplace in the system.
This allowed us, for example, toassume congruent semantics for the phrases "Samsung wasannounced to have X'd" and "Samsung has X'd."3.2.
The validation corpusWith these assumptions in mind, we assembled a corpus ofdata extraction inference problems in the area of Sovieteconomics.
The corpus consisted of text passages that hadbeen previously identified for an evaluation of informationretrieval techniques in this subject area.
The texts weredrawn from over 6200 Wall Street Journal articles from1989 that were released through the ACL-DCI.
Thesearticles were filtered (by extensive use of GREP) tO a subsetof 300-odd articles mentioning the then-extant SovietUnion.
These articles were read in detail to locate allpassages on a set of three pre-determined conomic topics:1.
East-West joint ventures, these being anybusiness arrangements between Soviet andnon-Soviet agents.2.
Hard currency, being any discussion ofattempts to introduce a convertible unit ofmonetary value in the former USSR.3.
Private cooperatives, i.e., employee-ownedenterprises within the USSR.We found 85 such passages in 74 separate articles (1.2% ofthe initial set of articles under consideration).Among these, 47 passages were eliminated from consi-deration because they were just textual mentions of thetarget concepts (e.g.
the string "joint venture") or of somesimple variant.
These passages could easily be identifiedby Boolean keyword techniques, and as such were nottaken to provide a particularly insightful validation of acomplex NL-based information-extraction process!Unfortunately, this eliminated all instances of privatecooperatives from the corpus, because in these texts, theword "cooperative" is a perfect predictor of the concept.An additional four passages were also removed uring across-rater reliability verification.
These were all amplifi-cations of an earlier instance of one of the target concepts,e.g., "U.S. and Soviet officials hailed the joint project.
"These passages were eliminated because the corpuscollectors had differing intuitions as to whether they weresufficient indications in and of themselves of the targetconcepts, or were somehow pragmatically "parasitic" uponearlier instances of the target concept.
The remaining 34passages required some degree of terminological inference,and formed the corpus for this study.4.
INFERENTIAL  DATA EXTRACTIONWe then set about writing a collection of terminologicalaxioms to handle this corpus.
As these axioms arepropositional in nature, and the semantic representationsproduced by Alembic are not strictly propositional, thisrequired specifying a mapping from the language ofinterpretations to that of the inference axioms.4.1.
Semantic representation i  AlembicAlembic produces emantic representations at the increa-singly popular interpretation level \[2, 10\].
That is, insteadof generating fully scoped and disambiguated logicalforms, Alembic produces representations that are ambi-guous with respect to quantifier scoping.
For example, thenoun phrase "a gold-based ruble" maps into somethingakin to the following interpretation:\[ \[head ruble\]\[quant :exists\]\[args NIL\]\[proxy P117\]\[roods { \[ \[head basis-of~\[args { Pl17 \[ \[head gold\]\[quant :kind\]\] }\]\]}\]\]Semantic heads of phrases are mapped to the head slot ofthe interpretation, arguments are mapped to the args slot,152modifiers to the mods slot, and generalized quantifiers tothe quant slot.
The proxy slot contains a unique variabledesignating the individuals that satisfy the interpretation.If this interpretation were to be fully mapped to a sortedfirst-order logical form, it would result in the followingsentence, where gold is treated as a kind individual:3 Pl17 : ruble basis-of(P117, gold)Details of this semantic framework can be found in \[3\].4.2 Conversion to propositional formAxioms in RHO are strictly function-free Horn clauses, andas such are intended to match neither interpretations orfirst-order logical forms.
As a result, we needed to specifya mapping from interpretations to some propositionalencoding that can be exploited by RHO'S terminologicalaxioms.
In brief, this mapping hyper-Skolemizes theproxy variables in the interpretation a d then recursivelyflattens the interpretation's modifiers.
1For example, the interpretation for "a gold-based ruble" ismapped to the following propositions:ruble(Pll7)basis-of(P117, gold)The interpretation has been flattened by pulling itsmodifier to the same level as the head proposition (yieldingan implicit overall conjunction).
In addition, the proxyvariable has been interpreted as a Skolem constant, in thiscase the "gensymed" individual ~17.This interpretation of proxies as Skolem constants isactually hyper-Skolemization, because we perform it onuniversally quantified proxies as well as on existentiallyquantified ones.
Ignoring issues of negation and disjunc-tion, this unorthodox Skolemization process has a curiousmodel-theoretic justification (which is beyond our presentscope).
Intuitively, however, one can think of these hyper-Skolemized variables as designating the individuals thatwould satisfy the interpretation, once it has been assignedsome unambiguously scoped logical form.To see this, say we had the following inference rule:m-loves-w(x,y) ~-- loves (x, y) & man (x) & woman(y)Now say this rule were to be applied against he semanticsof the infamously ambiguous case of "every man loves awoman."
In propositionalized form, this would be:man(P118)woman(P119)Ioves(P118,PI19)1This glosses over issues of event reference, which weaddress through apartly Davidsonian framework, as in \[9\].target occurrences, sufficiency rule density,n rules, r r/njoint venture 12 17 1.4hard curr.
22 13 .59Table 1: Summary of experimental findings.From this, the rule will infer m-loves-w(Pl18,P119).
If wethink of Pl18 and Pl19 as designating the individuals thatsatisfy the logical form of "every man loves a woman" insome model, then we can see that indeed the m-loves-wrelation necessarily must hold between them.
This is trueregardless of whether the model itself satisfies the standardV-3 scoping of the sentence or the notorious 3-V scoping.This demonstrates a crucial property of this approach,namely that it enables inferential extraction over ambi-guously scoped text, without requiring resolution of thescope ambiguity (and without expensive theorem proving).5.
F INDINGSReturning to our validation study, we took this propo-sifionalized representation as the basis for writing the set ofaxioms necessary to cover our corpus of data extractionproblems.
In complete honesty, we expected that theresulting axioms would not all end up meeting the trac-tability criterion.
Natural anguage is notoriously complex,and even such classic simple KL-ONE concepts asBrachman's arch \[6\] do not meet he criterion.What we found took us by surprise.
We came across manyexamples that were challenging at various levels: complexsyntactic phenomena, nightmares of reference resolution,and the ilk.
However, once the corpus passages weremapped to their corresponding interpretations, the termi-nological axioms necessary to perform data extractionfrom these interpretations all met the criterion.Table 1, above, summarizes these findings.
To cover ourcorpus of 34 passages, we required between two and threedozen sufficiency rules, depending upon how one encodedcertain economic concepts, and depending on whatassumptions one made about argument-passing i  syntax.We settled on a working set of thirty such rules.Note that this inventory does not include any necessityrules.
We ignored necessity rules for the present purposesin part because they only encode inheritance r lationships.The size of their inventory thus only reflects the degree towhich one chooses to model intermediate l vels of thedomain hierarchy.
For this study, we could arguably haveused none.
In addition, necessity rules are guaranteed tomeet the tractability criterion, and were consequently ofonly secondary interest to our present objectives.1535.1.
Considerations for data extractionFrom a data extraction perspective, these results are clearlypreliminary.
Looking at the positive side, we are encou-raged that the rules for our hard currency examples wereshared over multiple passages, as follows from their frac-tional rule density of .59 (see Table 1).
The joint venturerules fared less well, mainly because the concept hey en-code is fairly complex, and can be described in many ways.Given our restricted ata set, however, it is not possible toconclude how well either set of rules will generalize ifpresented with a larger corpus.
What is clearly needed is alarger corpus of examples.
This would allow us toestimate generalizability of the rules by considenng theasymptotic growth of the rule set as it is extended to covermore examples.
Unfortunately, constructing such a corpusis a laborious task, since the examples we are interested inare precisely those that escape simple automated searchtechniques such as Boolean keyword patterns.
The timeand expense that were incurred in constructing the MUC3/4and TIPSTER corpora ttest to this difficulty.We soon hope to know more about this question of rulegeneralizability.
We are currently in the process ofimplementing a version of RHO in the context of theAlembic system, which is now considerably more maturethan when we undertook the present study.
We intend toexploit this framework for our participation i  MUCS, aswell as retool our system for the MUC4 task.
As theTIPSTER and MUC4 data sets contain a considerably greaternumber of training examples than our Soviet economicscorpus, we expect o gain much better insights into theways in which our rule sets grow and generalize.5.2.
Considerations for R.OFrom the perspective of our terminological inferenceframework, however, these preliminary results are quiteencouraging indeed.
We started with a very simpletractable inference framework, and studied how it could beapplied to a very difficult problem in natural anguageprocessing.
And it appears to work.Once again, one should refrain from reaching overlygeneral conclusions based on a small test sample.
Andadmittedly RHO gets a lot of help from other parts ofAlembic, especially the parser and a rudimentary inheri-tance taxonomy.
Further analyses, however, reveal someadditional findings that suggest hat RHO's tractabilitycfitedon may be of general validity to this kind of naturallanguage inference.Most interestingly, the tractability result can be understoodin the context of some basic characteristics of naturallanguage sentence structure.
In particular, axioms thatviolate the tractability criterion can only be satisfied bysentences that display anaphora or definite reference.
Forexample, an axiom with the following fight hand side:own(x, z) & scorn(x, y) & dislike(y, z)matches the sentences "the man who owns a Ferrari scornsanyone who dislikes it/his car/that car/the car."
It isimpossible, however, to satisfy this kind of circular axiomwithout invoking one of these referential mechanisms (atleast in English).
This observation, which was made inanother context in \[8\], suggests a curious alignmentbetween tractable cases of terminological natural languageinference and non-anaphofic cases of language use.It is particularly tantalizing that the cases where theseterminological inferences are predicted to become compu-tationally expensive are just those for which heuristic inter-pretation methods eem to play a large role (e.g., discoursestructure and other reference resolution strategies).Though one must avoid the temptation to draw too strong aconclusion form such coincidences, one is still left thinkingof Alice's ineffable words, "Curiouser and curiouser.
"?,~ Acknowledgments ?,~Much gratitude is owed John Aberdeen for preparing ourcorpus through tireless perusal of the Wall Street Journal.Many thanks also to those who served as technicalinspiration or as sounding boards: Bill Woods, RemkoScha, Steve Minton, Dennis Connolly, and John Burger.REFERENCES\[11\[2\]\[31\[4\]151\[6\]\[71181Aberdeen, J., Burger, J., Connolly, D., Roberts, S., &Vilain, M. (1992).
"Mitre-Bedford: Description of theAlembic system as used for MUC-4".
In \[18\].Alshawi, H.cS-Van Eijck, J.
(1989).
"Logical forms in thecore language ngine".
In Prcdgs.
Of ACL89.
Vancouver,ac, 25-32.Bayer, S. L. ~Vilain, M. B.
(1991).
"The relation-basedknowledge representation f King Kong".
Sigart Bulletin2(3), 15-21.Brachman, R. J., Borgida, A., McGuiness, D. L., drPatel-Schneider, P. F. (1991).
"Living with CLASSIC".
InSowa, J., ed., Principles of Semantic Networks.
SanMateo, CA: Morgan-Kaufmann.Brachman, R. J.
~'Levesque, H. (1984).
"The tractabilityof subsumption i frame-based description languages".In Prcdgs.
of AAAtS4.
Austin, Texas, 34-37.Brachman, R. J.
~ Schmolze, J. K. (1985).
"An over-view of the KL-ONE knowledge representation system".Cognitive Science 9(2), 171-216.Garey, M. R. ~Johnson, D. S. (1979).
Computers andIntractability.
New York: W. H. Freeman.Haddock, N. J., (1992).
"Semantic evaluation asconstraint network consistency".
In Prcdgs.
of AAAt92.San Jose, CA, 415-420.154\[9\] Hobbs, J. R. (1985).
"Ontological promiscuity".
InPrcdgs.
Of ACLSS.
Chicago, IL, 119-124.\[10\] Hobbs, J. R. drShieber, S. M. (1987).
"An algorithm forgenerating quantifier scopings".
ComputationalLinguistics 13(1-2), 47-63.\[11\] Jacobs, P. S. (1988).
"Concretion: Assumption-basedunderstanding".
In Prcdgs.
of the 1988 Intl.
Conf.
onComput.
Linguistics (COLING88).
Budapest, 270-274.\[12\] MacGregor, R. (1991).
"Inside the LOOM descriptionclassifier".
Sigart Bulletin 2(3), 88-92.\[13\] Nebel, B.
(1988).
"Computational complexity ofterminological reasoning in BACK".
Artificial Intelligence34(3), 371-383.\[14\] Nebel, B.
(1990).
"Terminological reasoning in inhe-rently intractable".
Artificial Intelligence 43, 235-249.\[15\] Patel-Schneider, P. F. (1989).
Undecidability of sub-sumption in NIKL.
Artificial Intelligence 39: 263-272.\[16\] Schmidt-SchauB, M. (1989).
"Subsumption i  KL-ONE isundecidable".
InPrcdgs.
ofgRsg.
Toronto, ON.\[ 17\] Stallard, D. G. (1986).
"A terminological simplificationtransformation for natural language question-answeringsystems".
In Prcdgs.
Of ACL86.
New York, NY, 241-246.\[181 Sundheim, B.,ed.
(1992).
Prcdgs.
of the Fourth MessageUnderstanding Conf.
(MUC-4), McLean, VA, 215-222.\[19\] Vilain, M. (1991).
"Deduction as parsing: Tractableclassification i  the KL-ONE framework".
In Prcdgs.
ofAAAI91.
Anaheim, CA, 464-470APPENDIX:  PROOF OF  TRACTABIL ITYTo demonstrate he validity of the tractability criterion, weonly need consider the computational cost of finding allinstantiations of the fight-hand side of an axiom.
In gene-ral, finding a single such instantiafion is NP-complete, byreduction to the conjunctive Boolean query problem (see\[7\]).
Intuitively, this is because general function-flee Hornclauses can have arbitrary interactions between thevariables on the right-hand side, i.e., their dependencygraphs are fully cross-connected, as in:R(vl,v2) & R(vl,v3) & R(v2,v.5) & R(vl,v4) & R(v2,v4)...Intuitively again, verifying the instantiation of a givenvariable in a rule may require (in the worst case) checkingall instantiations of all other variables in the rule.
Underthe usual assumptions of NP-completeness, no knownalgorithm exists that performs better in the worst case thanenumerating all these instantiafions.
As each variable maytake on as many as ~c instanfiations, where t?
is the numberof constants present in the knowledge base, the overall costof finding a single globally consistent instantiation isO(x~), where ~ is the number of variables in the rule.
Theresulting complexity is thus exponential in ~, which itselfvaries in the worst case with the length of the rule.Consider now an axiom that satisfies the tractabilitycriterion, yielding a graph such as that in Fig.
3.
Byleft-hand side vars: 11 .....
I a/ \ f Vl v 4 right-handsidevars: / ~ /v 2 v 3 v 5 v 6Figure 3: A dependency graph.definition, the root of the graph corresponds to all thevariables on the left-hand side, and all other nodescorrespond to some variable introduced on the fight-handside.
The cost of finding all the instantiations of the rootvariables is bounded by t?
a, where a is the maximalpredicate valence for all the predicates appearing in thedatabase.
The cost of instantiating each non-root variable vis in turn bounded by ax  a, corresponding to the cost ofenumerating all possible instantiations of any predicaterelating v to its single parent in the graph.The topological restriction of the criterion leads dkectly tothe fact that the exponent of these terms is a low-magnitude constant, a, rather than a parameter, ~, that canbe allowed to grow arbitrarily with the complexity ofinference rules.
The topological restriction also leads tothe fact that these terms contribute additively to the overallcost of finding all instantiations ofa rule.
This overall costis thus bounded by t?a + atca+.. .+atca,  or O(~ara).Finally, we note that with the appropriate indexing scheme,finding all consequents of all rules only adds a multipli-cative cost of p, where p is the total number of rules,yielding a final overall cost of O(p~axa).
It is oftenassumed that predicates in natural languages have no morethan three arguments, so this formula approximatelyreduces to O(~).This is of course a worst-case stimate.
We are lookingforward to measuring run-time performance figures on theMUC5 task, and are of course hoping to find actual per-formance to lie well below this cubic upper bound.155
