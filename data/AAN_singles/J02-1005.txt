Squibs and DiscussionsThe DOP Estimation Method Is Biased andInconsistentMark JohnsonBrown UniversityA data-oriented parsing or DOP model for statistical parsing associates fragments of linguisticrepresentations with numerical weights, where these weights are estimated by normalizing theempirical frequency of each fragment in a training corpus (see Bod [1998] and references citedtherein).
This note observes that this estimation method is biased and inconsistent; that is, theestimated distribution does not in general converge on the true distribution as the size of thetraining corpus increases.1.
IntroductionThe data-oriented parsing or DOP approach to statistical natural language analy-sis has attracted considerable attention recently and has been used to produce sta-tistical language models based on various kinds of linguistic representation, asdescribed in Bod (1998).
These models are based on the intuition that statistical gen-eralizations about natural languages should be stated in terms of ?chunks?
or ?frag-ments?
of linguistic representations.
Linguistic representations are produced bycombining these fragments, but unlike in stochastic models such as Probabilistic Con-text-Free Grammars, a single linguistic representation may be generated by severaldifferent combinations of fragments.
These fragments may be large, permitting DOPmodels to describe nonlocal dependencies.
Usually the fragments used in aDOP model are themselves obtained from a training corpus of linguistic represen-tations.
For example, in DOP1 or Tree-DOP the fragments are typically all theconnected multinode trees that appear as subgraphs of any tree in the trainingcorpus.This note shows that the estimation procedure standardly used to set the parame-ters or fragment weights of a DOP model (see, for example, Bod [1998]) is biased andinconsistent.
This means that as sample size increases, the corresponding sequence ofprobability distributions estimated by this procedure does not converge to the truedistribution that generated the training data.
Consistency is usually regarded as theminimal requirement any estimation method must satisfy (Breiman 1973; Shao 1999),and the inconsistency of the standard DOP estimation method suggests it may beworth looking for other estimation methods.
Note that while the bulk of DOP re-search uses the estimation procedure studied here, recently there has been researchthat has used other estimators for DOP models (Bonnema, Buying, and Scha 1999;Bod 2000), and it would be interesting to investigate the statistical properties of theseestimators as well. Cognitive and Linguistic Sciences, Brown University, Providence, RI 02912.
E-mail:Mark Johnson@Brown.edu.c?
2002 Association for Computational LinguisticsComputational Linguistics Volume 28, Number 1ZZZZSNP VPVlikesNPNPpizzaNPAlex""ZZZZVPV NPNPpizza"likesV"SVP"NP"NPAlexZZZZSNP VPNPNPpizzaNPAlex""likesVV"Figure 1Depictions of three different derivations of the same tree representation of Alex likes pizza, witharrows indicating the sites of tree fragment substitutions.2.
DOP1 ModelsFor simplicity, this note focuses on DOP1 or Tree-DOP models, in which linguisticrepresentations are phrase structure trees, but the results carry over to more complexmodels that use attribute-value feature structure representations such as LFG-DOP.The fragments used in DOP1 are multinode trees whose leaves may be labeled withnonterminals as well as terminals.
A derivation starts with a fragment whose rootis labeled with the start symbol, and it proceeds by substituting a fragment for theleftmost nonterminal leaf under the constraint that the fragment?s root node and theleaf node have the same label.
The derivation terminates when there are no nonter-minal leaves.
Figure 1 depicts three different derivations that yield the same tree.
Thefragments used in these derivations could have been obtained from a training corpusof trees that contains trees for examples such as Sasha likes motorcycles, Alex eats pizza,and so on.In a DOP model, each fragment is associated with a real-valued weight, and theweight of a derivation is the product of the weights of the tree fragments involved.The weight of a representation is the sum of the weights of its derivations, and aprobability distribution over linguistic representations is obtained by normalizing therepresentations?
weights.1 Given a combinatory operation and a fixed set of fragments,a DOP model is a parametric model where the fragment weights are the parameters.In DOP1 and DOP models based on it, the weight associated with a fragment isestimated as follows (Bod 1998).
For each tree fragment f , let n(f ) be the number oftimes it appears in the training corpus, and let F be the set of all tree fragments withthe same root as f .
Then the weight w(f ) associated with f isw(f ) =n(f )Pf 02F n(f0):This relative-frequency estimation method has the advantage of simplicity, but asshown in the following sections, it is biased and inconsistent.1 In DOP1 and similar models, it is not necessary to normalize the representations?
weights if thefragments?
weights are themselves appropriately normalized.72Johnson DOP Is Biased and Inconsistent3.
Bias and InconsistencyBias and inconsistency are usually defined for parametric estimation procedures interms that are not quite appropriate for evaluating the DOP estimation procedure,but their standard definitions (see Shao [1999] for a textbook exposition) will serveas the basis for the definitions adopted below.
Let  be a vector space of real-valuedparameters, so that P,  2  is a probability distribution.
In the DOP1 case,  wouldbe the space of all possible weight assignments to fragments.
An estimator  is afunction from a vector x of n samples to a parameter value (x) 2 , and an estimationprocedure specifies an estimator n for each sample size n.Let X be a vector of n independent random variables distributed according toP?
for some ?
2 .
Then (X) is also a random variable, ranging over parametervectors , with an expected value E?
((X)).
The bias of the estimator  at ?
is thedifference E?
((X))?
?
between its expected value and the ?true?
parameter value?
that determines the distribution X.
A biased estimator is one with nonzero bias forsome value of ?.A loss function L is a function from pairs of parameter vectors to the nonnegativereals.
Given a sample x drawn from the distribution ?, L(?,(x)) measures the ?cost?or the ?loss?
incurred by the error in the estimate (x) of ?.
For example, a standardloss function is the Euclidean distance metric L(?,(x)) = k(X)?
?k2 (note that theresults below do not depend on this choice of loss function).
The risk of an estimatorat ?
is its expected loss E?
(L(?,(X)).
An estimation procedure is consistent ifand only if the limit of the risk of n is 0 as n !
1 for all ?.
(There are variousdifferent notions of consistency depending on how convergence is defined; however,the DOP1 estimator is not consistent with respect to any of the standard definitionsof consistency.
)Strictly speaking, the standard definitions of bias and loss function are not appli-cable to DOP estimation because there can be two distinct parameter vectors 1, 2 forwhich P1 = P2 even though 1 6= 2 (such a case is presented in the next section).Thus it is more natural to define bias and loss in terms of the probability distributionsthat the parameters specify, rather than in terms of the parameters themselves.
In thispaper, an estimator is unbiased iff PE?
((X)) = P?
for all ?
; that is, its expectedparameter estimate specifies the same distribution as the true parameters.
Similarly,the loss function is the mean squared difference between the ?true?
and estimateddistributions; that is, if ?
is the event space (in DOP1, the space of all phrase structuretrees), thenL(?,(x)) =X!2?P?(!)(P?(!)?
P(x)(!
))2:As before, the risk of an estimator is its expected loss, and an estimation procedure isconsistent iff the limit of the expected loss is 0 as n!1.4.
A DOP1 ExampleThis section presents a simple DOP1 model that only generates two trees with prob-ability p and 1 ?
p, respectively.
The DOP relative frequency estimator is applied toa random sample of size n drawn from this population to estimate the tree weightparameters for the model.
The bias and inconsistency of the estimator follow fromthe fact that these estimated parameters generate the trees with probabilities differ-ent from p and 1 ?
p. The trees used and their DOP1 fragments are shown in Fig-ure 2.73Computational Linguistics Volume 28, Number 1Figure 2The trees t1, t2 and their associated fragments f1, : : : , f7 in the DOP1 model.Suppose the ?true?
weights for the fragments f1, : : : , f7 are 0 except for the follow-ing fragments:w?
(f4) = p,w?
(f6) = 1?
p,w?
(f7) = 1:Then Pw?
(t1) = p and Pw?
(t2) = 1 ?
p. (Note that exactly the same tree distributioncould be obtained by setting w?
(f1) = p and w?
(f5) = 1?
p and all other weights to 0;thus the tree weights are not identifiable.)
Then in a sample of size n drawn from thedistribution Pw?
the expected number of occurrences of tree t1 is np and the expectednumber of occurrences of tree t2 is n(1?
p).
Thus the expected number of occurrencesof the fragments in a sample of size n isE(n(fi)) = np for i = 1, : : : , 4;E(n(fi)) = n(1?
p) for i = 5, 6;E(n(f7)) = n + np:Thus after normalizing, the expected estimated weights for the fragments using theDOP estimator areE(w^(fi)) =p2 + 2pfor i = 1, : : : , 4;E(w^(fi)) =1?
p2 + 2pfor i = 5, 6;E(w^(f7)) = 1:Further calculation shows thatPE(w^)(t1) =2p1 + p,74Johnson DOP Is Biased and InconsistentFigure 3The value of PE(w^)(t1) as a function of Pw?
(t1) = p. The identity function p is also plotted forcomparison.PE(w^)(t2) =1?
p1 + p:Figure 3 shows how PE(w^)(t1) varies as a function of Pw?
(t1) = p. The differencePE(w^)(t1) ?
p reaches a maximum value of approximately 0:17 at p =p2 ?
1.
Thusexcept for p = 0 and p = 1, PE(w^) 6= Pw?
; that is, the DOP1 estimator is biased.Further, note that the estimated distribution PE(w^) does not approach Pw?
as thesample size increases, so the expected loss does not converge to 0 as the sample sizen increases.
Thus the DOP1 estimator is also inconsistent.5.
ConclusionThe previous section showed that the relative frequency estimation procedure usedin DOP1 and related DOP models is biased and inconsistent.
Bias is not necessarily adefect in an estimator, and Geman, Bienenstock, and Doursat (1992) argue that it maybe desirable to trade variance for bias.
However, inconsistency is usually viewed as afatal flaw of an estimator.
Nevertheless, excellent empirical results have been claimedfor the DOP1 model, so perhaps there are some circumstances in which inconsistentestimators perform well.
Undoubtedly there are other estimation procedures for DOPmodels that are unbiased and consistent.
For example, maximum likelihood estimatorsare unbiased and consistent across a wide class of models, including, it would seem, allreasonable DOP models (Shao 1999).
Bod (2000) describes a procedure for maximumlikelihood estimation of DOP models based on an Expectation Maximization?like al-gorithm.
In addition, Rens Bod (personal communication) points out that because theset of fragments in a DOP1 model includes all of the trees in the training corpus,the maximum likelihood estimator will assign the training corpus trees their empiri-cal frequencies, and assign 0 weight to all other trees.
However, this seems to be anoverlearning problem rather than a problem with maximum likelihood estimation perse, and standard methods, such as cross-validation or regularization, would seem inprinciple to be ways to avoid such overlearning.
Obviously, empirical investigationwould be useful here.75Computational Linguistics Volume 28, Number 1AcknowledgmentsI would like to thank Rens Bod, MichaelCollins, Eugene Charniak, DavidMcAllester, and the anonymous reviewersfor their excellent advice.ReferencesBod, Rens.
1998.
Beyond Grammar: AnExperience-Based Theory of Language.
CSLIPublications, Stanford, CA.Bod, Rens.
2000.
Combining semantic andsyntactic structure for languagemodelling.
In Proceedings of the 8thInternational Conference on Spoken LanguageProcessing (ICSLP 2000), Beijing.Bonnema, Remko, Paul Buying, and RemkoScha.
1999.
A new probability model fordata oriented parsing.
In Proceedings of the12th Amsterdam Colloquium, Amsterdam.Breiman, Leo.
1973.
Statistics with a Viewtoward Applications.
Houghton Mifflin,Boston.Geman, Stuart, Elie Bienenstock, and Rene?Doursat.
1992.
Neural networks and thebias/variance dilemma.
NeuralComputation, 4:1?58.Shao, Jun.
1999.
Mathematical Statistics.Springer-Verlag, New York.76
