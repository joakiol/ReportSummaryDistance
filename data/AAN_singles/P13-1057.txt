Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 583?592,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsReal-World Semi-Supervised Learningof POS-Taggers for Low-Resource LanguagesDan Garrette1 Jason Mielens21Department of Computer Science 2Department of LinguisticsThe University of Texas at Austin The University of Texas at Austindhg@cs.utexas.edu {jmielens,jbaldrid}@utexas.eduJason Baldridge2AbstractDeveloping natural language processingtools for low-resource languages often re-quires creating resources from scratch.While a variety of semi-supervised meth-ods exist for training from incompletedata, there are open questions regardingwhat types of training data should be usedand how much is necessary.
We dis-cuss a series of experiments designed toshed light on such questions in the con-text of part-of-speech tagging.
We obtaintimed annotations from linguists for thelow-resource languages Kinyarwanda andMalagasy (as well as English) and eval-uate how the amounts of various kindsof data affect performance of a trainedPOS-tagger.
Our results show that an-notation of word types is the most im-portant, provided a sufficiently capablesemi-supervised learning infrastructure isin place to project type information ontoa raw corpus.
We also show that finite-state morphological analyzers are effectivesources of type information when few la-beled examples are available.1 IntroductionLow-resource languages present a particularly dif-ficult challenge for natural language processingtasks.
For example, supervised learning meth-ods can provide high accuracy for part-of-speech(POS) tagging (Manning, 2011), but they per-form poorly when little supervision is avail-able.
Good results in weakly-supervised tagginghave been obtained by training sequence modelssuch as hidden Markov models (HMM) using theExpectation-Maximization algorithm (EM), how-ever most work in this area has still relied on rel-atively large amounts of data, both annotated andunannotated, as well as an assumption that the an-notations are very clean (Kupiec, 1992; Merialdo,1994).The ability to learn taggers using very little datais enticing: only a tiny fraction of the world?s lan-guages have enough data for standard supervisedmodels to work well.
The collection or develop-ment of resources is a time-consuming and expen-sive process, creating a significant barrier for anunder-studied language where there are few ex-perts and little funding.
It is thus important todevelop approaches that achieve good accuracybased on the amount of data that can be reasonablyobtained, for example, in just a few hours by a lin-guist doing fieldwork on a non-native language.Previous work explored learning taggers fromweak information, but the type, amount, quality,and sources of data raise questions about the appli-cability of those results to real-world low-resourcescenarios (Toutanova and Johnson, 2008; Ravi andKnight, 2009; Hasan and Ng, 2009; Garrette andBaldridge, 2012).
Most research simulated weaksupervision with tag dictionaries extracted fromexisting large, expertly-annotated corpora.
Theseresources have been developed over long periodsof time by trained annotators who collaborate toproduce high-quality analyses.
They are also bi-ased towards including only the most likely tagfor each word type, resulting in a cleaner dictio-nary than one would find in a real scenario.
Assuch, these experiments do not reflect real-worldconstraints.One exception to this work is Goldberg et al(2008): they use a manually-constructed lexiconfor Hebrew in order to learn an HMM tagger.
How-ever, this lexicon was constructed by trained lexi-cographers over a long period of time and achievesvery high coverage of the language with very goodquality, much better than could be achieved byour non-expert linguistics graduate student anno-tators in just a few hours.
Cucerzan and Yarowsky583(2002) learn a POS-tagger from existing linguis-tic resources, namely a dictionary and a refer-ence grammar, but these resources are not avail-able, much less digitized, for most under-studiedlanguages.
Haghighi and Klein (2006) develop amodel in which a POS-tagger is learned from a listof POS tags and just three ?prototype?
word typesfor each tag, but their approach requires a vectorspace to compute the distributional similarity be-tween prototypes and other word types in the cor-pus.
Such distributional models are not feasiblefor low-resource languages because they requireimmense amounts of raw text, much more than isavailable in these settings (Abney and Bird, 2010).Further, they extracted their prototype lists directlyfrom a labeled corpus, something we are specif-ically avoiding.
Ta?ckstro?m et al (2013) evalu-ate the use of mixed type and token constraintsgenerated by projecting information from a high-resource language to a low-resource language viaa parallel corpus.
However, large parallel corporaare not available for most low-resource languages.These are also expensive resources to create andwould take considerably more effort to producethan the monolingual resources that our annotatorswere able to generate in a four-hour timeframe.Of course, if they are available, such parallel textlinks could be incorporated into our approach.In our previous work, we developed a differ-ent strategy based on generalizing linguistic inputwith a computational model: linguists annotatedeither types or tokens for two hours, these anno-tations are projected onto a corpus of unlabeledtokens using label propagation and HMMs, anda final POS-tagger is trained on this larger auto-labeled corpus (Garrette and Baldridge, 2013).That approach uses much more realistic typesand quantities of resources than previous work;nonetheless, it leaves many open questions regard-ing the effectiveness of incrementally more anno-tation, the role of unannotated data, and whetherthere is a good balance to be found using a combi-nation of type- and token-supervision.
We also didnot consider morphological analyzers as a formof type supervision, as suggested by Merialdo(1994).This paper addresses these questions via a se-ries of experiments designed to quantify the ef-fect on performance given by the amount of timespent finding or annotating training materials.
Wespecifically look at the impact of four types of datacollection:1.
Time annotating sentences (token supervision)2.
Time creating tag dictionary (type supervision)3.
Time constructing a finite state transducer(FST) to analyze word-type morphology4.
Amount of raw data available for trainingWe explore these strategies in the context of POS-tagging for Kinyarwanda and Malagasy.
We alsoinclude experiments for English, pretending asthough it is a low-resource language.
The over-whelming take away from our results is that typesupervision?when backed by an effective semi-supervised learning approach?is the most impor-tant source of linguistic information.
Also, mor-phological analyzers help for morphologically richlanguages when there are few labeled types or to-kens (and, it never hurts to use them).
Finally, per-formance improves with more raw data, though wesee diminishing returns past 400,000 tokens.
Withjust four hours of type annotation, our system ob-tains good accuracy across the three languages:89.8% on English, 81.9% on Kinyarwanda, and81.2% on Malagasy.Our results compare favorably with previouswork despite using considerably less supervisionand a more difficult set of tags.
For example, Li etal.
(2012) use the entirety of English Wiktionarydirectly as a tag dictionary to obtain 87.1% accu-racy on English, below our result.
Ta?ckstro?m et al(2013) average 88.8% across 8 major languages,but for Turkish, a morphologically rich language,they achieve only 65.2%, significantly below our81.9% for morphologically-rich Kinyarwanda.2 DataKinyarwanda (KIN) and Malagasy (MLG) are low-resource, KIN is morphologically rich, and English(ENG) is used for comparison.
For each language,sentences were divided into four sets: training datato be labeled by annotators, raw training data, de-velopment data, and test data.Data sources The KIN texts are transcripts oftestimonies by survivors of the Rwandan geno-cide provided by the Kigali Genocide MemorialCenter.
The MLG texts are articles from the web-sites1 Lakroa and La Gazette and Malagasy GlobalVoices.2 Texts in both KIN and MLG were tok-1www.lakroa.mg and www.lagazette-dgi.com2mg.globalvoicesonline.org/584KIN MLG ENG - Experienced ENG - Novicetime type token type token type token type token1:00 801 559 (1093) 660 422 (899) 910 522 (1124) 210 308 (599)2:00 1814 948 (2093) 1363 785 (1923) 2660 1036 (2375) 631 646 (1429)3:00 2539 1324 (3176) 2043 1082 (3064) 4561 1314 (3222) 1350 953 (2178)4:00 3682 1651 (4119) 2773 1378 (4227) 6598 1697 (4376) 2185 1220 (2933)Table 1: Annotations for each language and annotator as time increases.
Shows the number of tagdictionary entries from type annotation vs. token.
(The count of labeled tokens is shown in parentheses).For brevity, the table only shows hourly progress.enized and labeled with POS tags by two linguis-tics graduate students, each of which was studyingone of the languages.
The KIN and MLG data have12 and 23 distinct POS tags, respectively.The Penn Treebank (PTB) (Marcus et al, 1993)is used as ENG data.
Section 01 was used fortoken-supervised annotation, sections 02-14 wereused as raw data, 15-18 for development of theFST, 19-21 as a dev set and 22-24 as a test set.The PTB uses 45 distinct POS tags.Collecting annotations Linguists with non-native knowledge of KIN and MLG produced anno-tations for four hours (in 30-minute intervals) fortwo tasks.
In the first task, type-supervision, theannotator was given a list of the words in the tar-get language (ranked from most to least frequent),and they annotated each word type with its poten-tial POS tags.
The word types and frequencies usedfor this task were taken from the raw training dataand did not include the test sets.
In the secondtask, token-supervision, full sentences were anno-tated with POS tags.
The 30-minute intervals allowus to investigate the incremental benefit of addi-tional annotation of each type as well as how bothannotation types might be combined within a fixedannotation budget.Baldridge and Palmer (2009) found that anno-tator expertise greatly influences effectiveness ofactive learning for morpheme glossing, a relatedtask.
To see how differences in annotator speedand quality impact our task, we obtained ENG datafrom an experienced annotator and a novice one.Ngai and Yarowsky (2000) investigated the ef-fectiveness of rule-writing versus annotation (us-ing active learning) for chunking, and found thelatter to be far more effective.
While we do notexplore a rule-writing approach to POS-tagging,we do consider the impact of rule-based morpho-logical analyzers as a component in our semi-supervised POS-tagging system.ENG - Exp.
ENG - Nov.time type tok type tok1:00 0.05 0.03 0.01 0.022:00 0.15 0.05 0.03 0.033:00 0.24 0.06 0.07 0.054:00 0.32 0.08 0.11 0.06Table 2: Tag dictionary recall against the test setfor ENG annotators on type and token annotations.Annotations Table 1 gives statistics for all lan-guages and annotators showing progress duringthe 4-hour tasks.
With token-annotation, tagdictionary growth slows because high-frequencywords are repeatedly annotated, producing onlyadditional frequency and sequence information.In contrast, every type-annotation label is a newtag dictionary entry.
For types, growth increasesover time, reflecting the fact that high-frequencywords (which are addressed first) tend to be moreambiguous and thus require more careful thoughtthan later words.
For ENG, we can compare thetagging speed of the experienced annotator withthe novice: 50% more tokens and 3 times as manytypes.
The token-tagging speed stayed fairly con-stant for the experienced annotator, but the noviceincreased his rate, showing the result of practice.Checking the annotators?
output against thegold tags in the PTB shows that both had goodtagging accuracy on tokens: 94-95%.
Comparingthe tag dictionary entries versus the test data, pre-cision starts in the high 80%s and falls to to themid-70%s in all cases.
However, the differencesin recall, shown in Table 2, are more interesting.On types, the experienced annotator maxed out at32%, but the novice only reaches 11%.
More-over, the maximum for token annotations is muchlower due to high repeat-annotation.
The discrep-ancies between experienced and novice, and be-tween type and token recall explain a great deal ofthe performance disparity seen in the experiments.5853 Morphological TransducersFinite-state transducers (FSTs) accept regular lan-guages and can be constructed easily using regu-lar expressions, which makes them quite useful forphonology, morphology and limited areas of syn-tax (Karttunen, 2001).
Past work has used FSTsfor direct POS-tagging (Roche and Schabes, 1995),but this requires tight coupling between the FSTand target tagset.
We use FSTs for morphologi-cal analysis: the FST accepts a word type and pro-duces a set of morphological features.
If there aremultiple possible analyses for a given word type,the FST returns them all.
For instance the Kin-yarwanda verb sibatarazuka ?he is not yet resur-rected?
is analyzed in several ways:?
+NEG+CL2+1PL+V+arazuk+IMP?
+NEG+CL2+NOT.YET+PRES+zuk+IMP?
+NEG+CL2+NOT.YET+razuk+IMPFSTs are particularly valuable for their abilityto analyze out-of-vocabulary items.
By lookingfor known affixes, FSTs can guess the stem ofa word and produce an analysis despite not hav-ing knowledge of that stem.
For morphologicallycomplex languages like KIN, this ability is espe-cially useful.
Other factors, such as a large num-ber of morphologically-conditioned phonologicalchanges (seen in MLG) make out-of-vocabularyguessing more challenging because of the largenumber of potential stems (high ambiguity).Development of the FSTs for all three languageswas done by iteratively adding rules and lexicalitems with the goal of increasing coverage on araw dataset.
To accomplish this on a fixed timebudget, the most frequently occurring unanalyzedtokens were examined, and their stems plus anyobservable morphological or phonological pat-terns were added to the transducer.
Addition-ally, developers searched for known morpholog-ical alternations to locate instances of phonolog-ical change for inclusion.
Coverage was checkedagainst a raw dataset which did not include the testdata used for the POS experiments.The KIN and MLG FSTs were created byEnglish-speaking linguists who were familiar withtheir respective language.
They also used dictio-naries and grammars.
Each FST was developedin 10 hours.
To evaluate the benefits of more de-velopment time, a version of the English FST wassaved every 30 minutes, as shown in Table 3.elapsedtimetokens typescount pct count pct2:00 130k 61% 2.1k 12%4:00 159k 75% 4.1k 24%6:00 170k 80% 6.7k 39%8:00 182k 86% 7.7k 44%10:00 192k 91% 10.7k 62%Table 3: Coverage of the English morphologicalFST during development.
For brevity, showing 2-hour increments instead of 30-minute segments.tokens typescov.
ambig.
cov.
ambig.KIN 86% 2.62 82% 5.31MLG 78% 2.98 37% 1.13ENG 91% 1.19 62% 1.97Table 4: Coverage and ambiguity of the final FSTfor each language.4 ApproachLearning under low-resource conditions is moredifficult than scenarios in most previous POS workbecause the vast majority of the word types in thetraining and test data are not covered by the an-notations.
When most words are unknown, learn-ing algorithms such as EM struggle (Garrette andBaldridge, 2012).
Recall that most work on learn-ing POS-taggers from tag dictionaries used tag dic-tionaries culled from test sets (even when consid-ering incomplete dictionaries).
We thus build onour previous approach, which exploits extremelysparse, human-generated annotations that are pro-duced without knowledge of which words appearin the test set (Garrette and Baldridge, 2013).This approach generalizes a small initial tag dic-tionary to include unannotated word types appear-ing in raw data.
It estimates word/tag pair andtag-transition frequency information using model-minimization, which also reduces noise intro-duced by automatic tag dictionary expansion.
Theapproach exploits type annotations effectively tolearn parameters for out-of-vocabulary words andinfer missing frequency and sequence informa-tion.
This pipeline is described in detail in theprevious work, so we give only a brief overviewand describe our additions.The purpose of tag dictionary expansion is to es-timate label distributions for tokens in a raw cor-586pus, including words missing in the annotations.For this, a graph connecting annotated words tounannotated words via features is constructed andPOS labels are pushed between these items usinglabel propagation (LP) (Talukdar and Crammer,2009).
LP has been used successfully for extend-ing POS labels from high-resource languages tolow via parallel corpora (Das and Petrov, 2011;Ta?ckstro?m et al, 2013; Ding, 2011) or high- tolow-resource domains (Subramanya et al, 2010),among other tasks.
These works have typicallyused n-gram features (capturing basic syntax) andcharacter affixes (basic morphology).The character n-gram affix-as-morphology ap-proach produces many features, but only a fractionof them represent actual morphemes.
Incorrectfeatures end up pushing noise around the graph,so affixes can lead to more false labels that drownout the true labels.
While affixes may be suffi-cient for languages with limited morphology, theireffectiveness diminishes for morphology-rich lan-guages, which have much higher type-to-token ra-tios.
More types means sparser word frequencystatistics and more out-of-vocabulary items, andthus problems for EM.
Here, we modify the LPgraph by supplementing or replacing generic af-fix features with a focused set of morphologicalfeatures produced by an FST.
These targeted mor-phological features are effective during LP becausewords that share them are much more likely to ac-tually share POS tags.FSTs produce multiple analyses, which is actu-ally advantageous for LP.
Ambiguities need not beresolved since we just take the union of all mor-phological features for all analyses and use themas features in the graph.
Note that each FST pro-duces its own POS-tags as features, but these donot correspond to the target POS tagset used by thetagger.
This is important because it decouples FSTdevelopment and the final POS task.
Thus, any FSTfor the language, regardless of its provenance, canbe used with any target POS tagset.Since the LP graph contains a node for each cor-pus token, and each node is labeled with a distri-bution over POS tags, the graph provides a corpusof sentences labeled with noisy tag distributionsalong with an expanded tag dictionary.
This out-put is useful as input to EM because it containslabels for all seen word types as well as sequenceand frequency information.
There is a high degreeof noise in the LP output, so we employ the modelminimization strategy of Ravi et al (2010), whichfinds a minimal set of tag bigrams needed to ex-plain the sentences in the raw corpus.
It outputsa corpus of tagged sentences, which are used asa good starting point for EM training of an HMM.The expanded tag dictionary constrains the EMsearch space by providing a limited tagset for eachword type, steering EM towards a desirable result.Because the HMM trained by EM will con-tain zero-probabilities for words that did not ap-pear in the training corpus, we use the ?auto-supervision?
step from our previous work: a Max-imum Entropy Markov Model tagger is trainedon a corpus that is noisily labeled by the HMM(Garrette and Baldridge, 2012).
While trainingan HMM before the MEMM is not strictly neces-sary, our tests have shown that this generative-then-discriminative combination generally resultsin around 3% accuracy improvement.5 Experiments3To better understand the effect that each type ofsupervision has on tagger accuracy, we perform aseries of experiments, with KIN and MLG as truelow-resource languages.
English experiments, forwhich we had both experienced and novice an-notators, allow for further exploration into issuesconcerning data collection and preparation.The overall best accuracies achieved by lan-guage are 81.9% for KIN using all types, 81.2% forMLG using half types and half tokens, and 89.8%for ENG using all types and the maximal amountof raw data.
All of these best values were achievedusing both FST and affix LP features.All results described in this section are averagedover five folds of raw data.5.1 Types versus tokensOur primary question was the relationship be-tween annotation type and time.
Annotation mustbe done by someone familiar with the target lan-guage, linguistics, and the target POS tagset.
Formany low-resource languages, such people, andthe time they have to spend, are likely to be inshort supply.
To make the best use of their time,we need to know which annotations are most use-3Code and all MLG data available at github.com/dhgarrette/low-resource-pos-tagging-2013We are unable to provide the KIN or ENG data for down-load due to licensing restrictions.
However, ENG data maybe shared with those holding a license for the Penn Treebankand KIN data may be shared on a case-by-case basis.587(a) KIN type annotations ?
Elapsed Annotation TimeAccuracy0:30 1:00 1:30 2:00 2:30 3:00 3:30 4:0050556065707580No LPAffixes onlyFST onlyAffixes+FST(b) KIN token annotations ?
Elapsed Annotation TimeAccuracy0:30 1:00 1:30 2:00 2:30 3:00 3:30 4:0050556065707580No LPAffixes onlyFST onlyAffixes+FST(c) MLG type annotations ?
Elapsed Annotation TimeAccuracy0:30 1:00 1:30 2:00 2:30 3:00 3:30 4:0065707580No LPAffixes onlyFST onlyAffixes+FST(d) MLG token annotations ?
Elapsed Annotation TimeAccuracy0:30 1:00 1:30 2:00 2:30 3:00 3:30 4:0065707580No LPAffixes onlyFST onlyAffixes+FSTFigure 1: Annotation time vs. tagger accuracy for type-only and token-only annotations.Elapsed Annotation TimeAccuracy0:30 1:00 1:30 2:00 2:30 3:00 3:30 4:006570758085Experienced annotator ?
TypesExperienced annotator ?
TokensNovice annotator ?
TypesNovice annotator ?
TokensFigure 2: Annotation time vs. tagger accuracy forENG type-only and token-only annotations withaffix and FST LP features.ful so that efforts can be concentrated there.
Ad-ditionally, it is useful to identify when returns onannotation effort diminish so that annotators donot spend time doing work that is unlikely to addmuch value.The annotators produced four hours each oftype and token annotations, each in 30-minute in-crements.
To assess the effects of annotation time,we trained taggers cumulatively on each incrementand determine the value of each additional half-hour of effort.
Results are shown for KIN and MLGin Figure 1 and ENG in Figure 2.
In all scenarios,the use of LP (and model minimization) delivershuge performance gains.
Additionally, the use ofFST features, usually along with affixes, yieldedbetter results than without.
This indicates the LPprocedure makes effective use of the morpholog-ical features produced by the FST and that the af-fix features are able to capture missing informationwithout adding too much noise to the LP graph.Furthermore, performance is considerably bet-ter when type annotations are used than only to-kens.
Type annotations plateau much faster, soa shorter amount of time must be spent annotat-ing types than if token annotations are used.
ForKIN it takes approximately 1.5 hours to reach near-maximum accuracy for types, but 2.5 hours for to-kens.
This difference is due to the fact that the typeannotations started with the most frequent wordswhereas the token annotations were on randomsentences.
Thus, type annotations quickly cover asignificant portion of the language?s tokens.
Withannotations directly on tokens, some of the highest588(a) KIN ?
Type/Token Annotation MixtureAccuracyt0/s8 t1/s7 t2/s6 t3/s5 t4/s4 t5/s3 t6/s2 t7/s1 t8/s06065707580No LPAffixes onlyFST onlyAffixes+FST(b) MLG ?
Type/Token Annotation MixtureAccuracyt0/s8 t1/s7 t2/s6 t3/s5 t4/s4 t5/s3 t6/s2 t7/s1 t8/s0707274767880No LPAffixes onlyFST onlyAffixes+FSTFigure 3: Annotation mixture vs. tagger accuracy.
X-axis labels give annotation proportions, e.g.
?t2/s6?indicates 2/8 of the time (1 hour) was spent annotating types and 6/8 (3 hours), full sentences.Type/Token Annotation MixtureAccuracyt0/s8 t1/s7 t2/s6 t3/s5 t4/s4 t5/s3 t6/s2 t7/s1 t8/s070758085Exp.
?
With LPNov.
?
With LPExp.
?
No LPNov.
?
No LPFigure 4: Annotation mixture vs. tagger accuracyon ENG using affix and FST LP features for experi-enced (Exp.)
and novice (Nov.) annotators.frequency types are covered, but annotation timeis also ineffectively used on low-frequency typesthat happen to appear in those sentences.Finally, the use of FST features yields the largestgains for KIN, but only when small amounts ofannotation are available.
This makes sense: KINis a morphologically rich language, so sparsity isgreater and crude affixes capture less actual mor-phology.
With little annotated data, LP relies heav-ily on morphological features to make clean linksbetween words.
But, with more annotations, thegains of the FST over affix features alone dimin-ishes: the affix features eventually capture enoughof the morphology to make up the difference.Figure 2 shows the dramatic differences be-tween the experienced and novice ENG annota-tors.4 For the former, results using types and to-4The ENG graph omits ?No LP?
results since they fol-lowed patterns similar to KIN and MLG.
Additionally, theresults without FST features are not shown because they werenearly identical (though slightly lower) than with the FST.kens were similar after 30 minutes, but type an-notations proved much more useful beyond that.In contrast, the novice annotated types much moreslowly, so early on there were not enough anno-tated types for the training to be as effective.
Evenso, after three hours of annotation, type annota-tions still win with the novice, and even beat theexperienced annotator labeling tokens.5.2 Mixing type and token annotationsBecause type and token annotations are each bet-ter at providing different information ?
a tag dic-tionary of high-frequency words vs. sequence andfrequency information ?
it is reasonable to ex-pect that a combination of the two might yieldhigher performance by each contributing differ-ent but complementary information during train-ing.
This matters in low-resource settings becausetype or token annotations will likely be producedby the same people, so there is a tradeoff betweenspending resources on one form of annotation overthe other.
Understanding the best mixture of an-notations can inform us on how to maximize thebenefit of a set annotation budget.
To this end, weran experiments fixing the annotation time to fourhours while varying the mix of type and token an-notations.
Results are shown for KIN and MLG inFigure 3 and ENG in Figure 4.For KIN and ENG, tagger accuracy increases asthe proportion of type annotations increases for allLP feature configurations.
For MLG, however, asthe reliance on the FST increases, the optimal mix-ture shifts toward higher type proportions.
Whenonly affix features are used, the optimal mixture is1 hour of types and 3 hours of tokens.
When FSTand affix features are used, the optimum is 2 hours589each of types and tokens.
When only FST featuresare used, it is best to use 3.5 hours of types andonly 30 minutes of tokens.
Because the FST op-erates on word types, it is effective at exploitingtype annotations.
Thus, when the LP focuses moreon FST features, it becomes more desirable to havelarger amounts of type annotations.Types clearly win for ENG.
The experienced an-notator was much faster at annotating types andthe speed difference was less pronounced for to-kens, so accuracy is most similar when only tokenannotations are used.
The performance disparitygrows with increasing the type proportion.Ta?ckstro?m et al (2013) explore the use ofmixed type and token annotations in which a tag-ger is learned by projecting information via par-allel text.
In their experiments, they?like us?found that type information is more valuable thantoken information.
However, they were able to seegains through the complementary effects of mix-ing type and token annotations.
It is likely that thisdifference in our results is due to the amount of an-notated data used.
It seems that the amount of typeinformation collected in four hours is not sufficientto saturate the system, meaning that switching toannotating tokens tends to hurt performance.5.3 FST developmentThe third set of experiments evaluate how theamount of time spent developing an FST affectsthe performance of trained tagger.
To do this,we had our ENG FST developer save progress af-ter each hour (for ten hours).
The results showthat, for ENG, the FST provided no value, regard-less of how much time was spent on its develop-ment.
Moreover, since large gains in accuracy canbe achieved by spending a small amount of timejust annotating word types with POS tags, we areled to conclude that time should be spent annotat-ing types or tokens instead of developing an FST.While it is likely that FST development time wouldhave a greater impact for morphologically richlanguages, we suspect that greater gains can stillbe obtained by instead annotating types.
Nonethe-less, FSTs never seems to hurt performance, so ifone is readily available, it should be used.5.4 The effect of more raw dataIn addition to annotations, semi-supervised taggertraining requires a corpus of raw text.
Raw datacan be easier to acquire since it does not needthe attention of a linguist.
Even so, for manyNumber of Raw Data TokensAccuracy100k 200k 300k 400k 500k 600k8082848688904hr types, FST, With LP4hr types, FST, No LP1hr types, No FST, With LPFigure 5: Amount of raw data vs. tagger accuracyfor ENG using high vs. low amounts of annotationand using LP vs. no LP., for experienced annotator(novice results were similar).low-resource languages, the amount of digitizedtext, such as transcripts or websites, is very lim-ited and may, in fact, require substantial effortto accumulate, even with assistance from compu-tational tools (Bird, 2011).
Therefore, the col-lection of raw data can be considered anothertime-sensitive task for which the tradeoffs withpreviously-discussed annotation efforts must con-tend.It could be the case that more raw data for train-ing could make up for additional annotation andFST development effort or make the LP proce-dure unnecessary.
Figure 5 shows that that in-creased raw data does provide increasing gains,but they diminish after 200k tokens.
The best per-formance is achieved by using more annotationand LP.
Most importantly, however, removing ei-ther annotations or LP results in a significant de-cline in accuracy, such that even with 600k train-ing tokens, we are unable to achieve the results ofhigh annotation and LP using only 100k tokens.5.5 Correcting existing annotationsFor all of the ENG experiments, we also ran ?or-acle?
experiments using gold tags for the samesentences or a tag dictionary containing the samenumber of type/tag entries as the annotator pro-duced, but containing only the most frequententries as determined by the gold-labeled cor-pus.
Using this simulated ?perfect annotator?
datashows we lose accuracy due to annotator mistakes:for our experienced annotator and maximal FST,using 4 hours of types the oracle accuracy is 90.5vs.
88.5 while using only tokens we see 83.9 vs.59081.5.
This indicates that there are gains to be madeby correcting mistakes in the annotations.
Thisis true even after the point of diminishing returnson the learning curve, meaning that even whenadding more annotations no longer improves per-formance, progress can still be made by correctingerrors, so it may be reasonable to ask annotators toattempt to correct errors in their past annotations.Automated techniques for facilitating error identi-fication can be employed for this (Dickinson andMeurers, 2003).6 Conclusions and Future WorkCare must be taken when drawing conclusionsfrom small-scale annotation studies such as thosepresented in this paper.
Nonetheless, we haveexplored realistic annotation scenarios for POS-tagging for low-resource languages and found sev-eral consistent patterns.
Most importantly, it isclear that type annotations are the most useful in-put one can obtain from a linguist?provided asemi-supervised algorithm for projecting that in-formation reliably onto raw tokens is available.
Ina sense, this result validates the research trajectoryof efforts of the past two decades put into learningtaggers from tag dictionaries: papers have succes-sively removed layers of unrealistic assumptions,and in doing so have produced pipelines for type-supervision that easily beat token-supervision pre-pared in comparable amounts of time.The result of most immediate practical value isthat we show it is possible to train effective POS-taggers on actual low-resource languages givenonly a relatively small amount of unlabeled textand a few hours of annotation by a non-nativelinguist.
Instead of having annotators label fullsentences as one might expect the natural choicewould be, it is much more effective to simplyextract a list of the most frequent word types inthe language and concentrate efforts on annotat-ing these types with their potential parts of speech.Furthermore, for languages with rich morphology,a morphological transducer can yield significantperformance gains when large amounts of otherannotated resources are unavailable.
(And it neverhurts performance.
)Finally, additional raw text does improve per-formance.
However, using substantial amounts ofraw text is unlikely to produce gains larger thanonly a few hours spent annotating types.
Thus,when deciding whether to spend time locatinglarger volumes of digitized text or to spend timeannotating types, choose types.Despite the consistent superiority of type anno-tations in our experiments, it of course may be thecase that techniques such as active learning maybetter select sentences for token annotation, so thisshould be explored in future work.AcknowledgementsWe thank Kyle Jerro, Vijay John, Jim Evans, YoavGoldberg, Slav Petrov, and the reviewers for theirassistance and feedback.
This work was sup-ported by the U.S. Department of Defense throughthe U.S. Army Research Office (grant numberW911NF-10-1-0533) and through a National De-fense Science and Engineering Graduate Fellow-ship for the first author.
Experiments were runon the UTCS Mastodon Cluster, provided by NSFgrant EIA-0303609.ReferencesSteven Abney and Steven Bird.
2010.
The human lan-guage project: Building a universal corpus of theworlds languages.
In Proceedings of ACL.Jason Baldridge and Alexis Palmer.
2009.
How welldoes active learning actually work?
Time-basedevaluation of cost-reduction strategies for languagedocumentation.
In Proceedings of EMNLP, Singa-pore.Steven Bird.
2011.
Bootstrapping the languagearchive: New prospects for natural language pro-cessing in preserving linguistic heritage.
LinguisticIssues in Language Technology, 6.Silviu Cucerzan and David Yarowsky.
2002.
Boot-strapping a multilingual part-of-speech tagger in oneperson-day.
In Proceedings of CoNLL, Taipei, Tai-wan.Dipanjan Das and Slav Petrov.
2011.
Unsupervisedpart-of-speech tagging with bilingual graph-basedprojections.
In Proceedings of ACL-HLT, Portland,Oregon, USA.Markus Dickinson and W. Detmar Meurers.
2003.
De-tecting errors in part-of-speech annotation.
In Pro-ceedings of EACL.Weiwei Ding.
2011.
Weakly supervised part-of-speech tagging for Chinese using label propagation.Master?s thesis, University of Texas at Austin.Dan Garrette and Jason Baldridge.
2012.
Type-supervised hidden Markov models for part-of-speech tagging with incomplete tag dictionaries.
InProceedings of EMNLP, Jeju, Korea.591Dan Garrette and Jason Baldridge.
2013.
Learning apart-of-speech tagger from two hours of annotation.In Proceedings of NAACL, Atlanta, Georgia.Yoav Goldberg, Meni Adler, and Michael Elhadad.2008.
EM can find pretty good HMM POS-taggers(when given a good start).
In Proceedings ACL.Aria Haghighi and Dan Klein.
2006.
Prototype-driven learning for sequence models.
In Proceed-ings NAACL.Kazi Saidul Hasan and Vincent Ng.
2009.Weakly supervised part-of-speech tagging formorphologically-rich, resource-scarce languages.
InProceedings of EACL, Athens, Greece.Lauri Karttunen.
2001.
Applications of finite-statetransducers in natural language processing.
LectureNotes in Computer Science, 2088.Julian Kupiec.
1992.
Robust part-of-speech taggingusing a hidden Markov model.
Computer Speech &Language, 6(3).Shen Li, Joa?o Grac?a, and Ben Taskar.
2012.
Wiki-lysupervised part-of-speech tagging.
In Proceedingsof EMNLP, Jeju Island, Korea.Christopher D. Manning.
2011.
Part-of-speech tag-ging from 97% to 100%: Is it time for some linguis-tics?
In Proceedings of CICLing.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: The Penn Treebank.
Computa-tional Linguistics, 19(2).Bernard Merialdo.
1994.
Tagging English text witha probabilistic model.
Computational Linguistics,20(2).Grace Ngai and David Yarowsky.
2000.
Rule writingor annotation: Cost-efficient resource usage for basenoun phrase chunking.
In Proceedings ACL.Sujith Ravi and Kevin Knight.
2009.
Minimized mod-els for unsupervised part-of-speech tagging.
In Pro-ceedings of ACL-AFNLP.Sujith Ravi, Ashish Vaswani, Kevin Knight, and DavidChiang.
2010.
Fast, greedy model minimization forunsupervised tagging.
In Proceedings of COLING.Emmanuel Roche and Yves Schabes.
1995.
Determin-istic part-of-speech tagging with finite-state trans-ducers.
Computational Linguistics, 21(2).Amarnag Subramanya, Slav Petrov, and FernandoPereira.
2010.
Efficient graph-based semi-supervised learning of structured tagging models.
InProceedings EMNLP, Cambridge, MA.Oscar Ta?ckstro?m, Dipanjan Das, Slav Petrov, RyanMcDonald, and Joakim Nivre.
2013.
Token andtype constraints for cross-lingual part-of-speech tag-ging.
In Transactions of the ACL.
Association forComputational Linguistics.Partha Pratim Talukdar and Koby Crammer.
2009.New regularized algorithms for transductive learn-ing.
In Proceedings of ECML-PKDD, Bled, Slove-nia.Kristina Toutanova and Mark Johnson.
2008.
ABayesian LDA-based model for semi-supervisedpart-of-speech tagging.
In Proceedings of NIPS.592
