Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1523?1533,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsParsing as ReductionDaniel Fern?andez-Gonz?alez?
?Andr?e F. T. Martins?#?Departamento de Inform?atica, Universidade de Vigo, Campus As Lagoas, 32004 Ourense, Spain?Instituto de Telecomunicac?
?oes, Instituto Superior T?ecnico, 1049-001 Lisboa, Portugal#Priberam Labs, Alameda D. Afonso Henriques, 41, 2o, 1000-123 Lisboa, Portugaldanifg@uvigo.es, atm@priberam.ptAbstractWe reduce phrase-based parsing to depen-dency parsing.
Our reduction is groundedon a new intermediate representation,?head-ordered dependency trees,?
shownto be isomorphic to constituent trees.
Byencoding order information in the depen-dency labels, we show that any off-the-shelf, trainable dependency parser can beused to produce constituents.
When thisparser is non-projective, we can performdiscontinuous parsing in a very naturalmanner.
Despite the simplicity of our ap-proach, experiments show that the result-ing parsers are on par with strong base-lines, such as the Berkeley parser for En-glish and the best non-reranking systemin the SPMRL-2014 shared task.
Resultsare particularly striking for discontinuousparsing of German, where we surpass thecurrent state of the art by a wide margin.1 IntroductionConstituent parsing is a central problem inNLP?one at which statistical models trained ontreebanks have excelled (Charniak, 1996; Kleinand Manning, 2003; Petrov and Klein, 2007).However, most existing parsers are slow, sincethey need to deal with a heavy grammar con-stant.
Dependency parsers are generally faster, butless informative, since they do not produce con-stituents, which are often required by downstreamapplications (Johansson and Nugues, 2008; Wu etal., 2009; Berg-Kirkpatrick et al, 2011; Elming etal., 2013).
How to get the best of both worlds?Coarse-to-fine decoding (Charniak and John-son, 2005) and shift-reduce parsing (Sagae andLavie, 2005; Zhu et al, 2013) were a step forward?This research was carried out during an internship atPriberam Labs.to accelerate constituent parsing, but typical run-times still lag those of dependency parsers.
Thisis only made worse if discontinuous constituentsare allowed?such discontinuities are convenientto represent wh-movement, scrambling, extrapo-sition, and other linguistic phenomena common infree word order languages.
While non-projectivedependency parsers, which are able to model suchphenomena, have been widely developed in thelast decade (Nivre et al, 2007; McDonald et al,2006; Martins et al, 2013), discontinuous con-stituent parsing is still taking its first steps (Maierand S?gaard, 2008; Kallmeyer and Maier, 2013).In this paper, we show that an off-the-shelf,trainable, dependency parser is enough to builda highly-competitive constituent parser.
This (sur-prising) result is based on a reduction1of con-stituent to dependency parsing, followed by a sim-ple post-processing procedure to recover unaries.Unlike other constituent parsers, ours does notrequire estimating a grammar, nor binarizing thetreebank.
Moreover, when the dependency parseris non-projective, our method can perform discon-tinuous constituent parsing in a very natural way.Key to our approach is the notion of head-ordered dependency trees (shown in Figure 1):by endowing dependency trees with this additionallayer of structure, we show that they become iso-morphic to constituent trees.
We encode this struc-ture as part of the dependency labels, enablinga dependency-to-constituent conversion.
A re-lated conversion was attempted by Hall and Nivre(2008) to parse German, but their complex encod-ing scheme blows up the number of arc labels, af-fecting the final parser?s quality.
By contrast, ourlight encoding achieves a 10-fold decrease in thelabel alphabet, leading to more accurate parsing.While simple, our reduction-based parsers areon par with the Berkeley parser for English (Petrov1The title of this paper is inspired by the seminal paper ofPereira and Warren (1983) ?Parsing as Deduction.
?1523and Klein, 2007), and with the best single systemin the recent SPMRL shared task (Seddah et al,2014), for eight morphologically rich languages.For discontinuous parsing, we surpass the currentstate of the art by a wide margin on two Germandatasets (TIGER and NEGRA), while achieving fastparsing speeds.
We provide a free distribution ofour parsers along with this paper, as part of theTurboParser toolkit.22 BackgroundWe start by reviewing constituent and dependencyrepresentations, and setting up the notation.
Fol-lowing Kong and Smith (2014), we use c-/d- pre-fixes for convenience (e.g., we write c-parser forconstituent parser and d-tree for dependency tree).2.1 Constituent TreesConstituent-based representations are commonlyseen as derivations according to a context-freegrammar (CFG).
Here, we focus on propertiesof the c-trees, rather than of the grammars usedto generate them.
We consider a broad scenariothat permits c-trees with discontinuities, such asthe ones derived with linear context-free rewrit-ing systems (LCFRS; Vijay-Shanker et al (1987)).We also assume that the c-trees are lexicalized.Formally, let w1w2.
.
.
wLbe a sentence, wherewidenotes the word in the ith position.
A c-tree is a rooted tree whose leaves are the words{wi}Li=1, and whose internal nodes (constituents)are represented as a tuple ?Z, h, I?, where Zis a non-terminal symbol, h ?
{1, .
.
.
, L} in-dicates the lexical head, and I ?
{1, .
.
.
, L}is the node?s yield.
Each word?s parent is apre-terminal unary node of the form ?pi, i, {i}?,where pidenotes the word?s part-of-speech (POS)tag.
The yields and lexical heads are defined sothat for every constituent ?Z, h, I?
with children{?Xk,mk,Jk?
}Kk=1, (i) we have I =?Kk=1Jk;and (ii) there is a unique k such that h = mk.
Thiskth node (called the head-child node) is commonlychosen applying a handwritten set of head rules(Collins, 1999; Yamada and Matsumoto, 2003).A c-tree is continuous if all nodes ?Z, h, I?have a contiguous yield I, and discontinuous oth-erwise.
Trees derived by a CFG are always con-tinuous; those derived by a LCFRS may have dis-continuities, the yield of a node being a union ofspans, possibly with gaps in the middle.
Figure 12http://www.ark.cs.cmu.edu/TurboParsershows an example of a continuous and a discontin-uous c-tree.
Discontinuous c-trees have crossingbranches, if the leaves are drawn in left-to-rightsurface order.
An internal node which is not a pre-terminal is called a proper node.
A node is calledunary if it has exactly one child.
A c-tree with-out unary proper nodes is called unaryless.
If allproper nodes have exactly two children then it iscalled a binary c-tree.
Continuous binary treesmay be regarded as having been generated by aCFG in Chomsky normal form.Prior work.
There has been a long string ofwork in statistical c-parsing, shifting from sim-ple models (Charniak, 1996) to more sophisticatedones using structural annotation (Johnson, 1998;Klein and Manning, 2003), latent grammars (Mat-suzaki et al, 2005; Petrov and Klein, 2007), andlexicalization (Eisner, 1996; Collins, 1999).
Anorthogonal line of work uses ensemble or rerank-ing strategies to further improve accuracy (Char-niak and Johnson, 2005; Huang, 2008; Bj?orkelundet al, 2014).
Discontinuous c-parsing is con-sidered a much harder problem, involving mildlycontext-sensitive formalisms such as LCFRS orrange concatenation grammars, with treebank-derived c-parsers exhibiting near-exponential run-time (Kallmeyer and Maier, 2013, Figure 27).To speed up decoding, prior work has consid-ered restrictons, such as bounding the fan-out(Maier et al, 2012) and requiring well-nestedness(Kuhlmann and Nivre, 2006; G?omez-Rodr?
?guez etal., 2010).
Other approaches eliminate the dis-continuities via tree transformations (Boyd, 2007;K?ubler et al, 2008), sometimes as a pruning stepin a coarse-to-fine parsing approach (van Cranen-burgh and Bod, 2013).
However, reported run-times are still superior to 10 seconds per sentence,which is not practical.
Recently, Versley (2014a)proposed an easy-first approach that leads to con-siderable speed-ups, but is less accurate.
In thispaper, we design fast discontinuous c-parsers thatoutperform all the ones above by a wide margin,with similar runtimes as Versley (2014a).2.2 Dependency TreesIn this paper, we use d-parsers as a black box toparse constituents.
Given a sentence w1.
.
.
wL,a d-tree is a directed tree spanning all the wordsin the sentence.3Each arc in this tree is a tuple3We assume throughout that dependency trees have a sin-gle root among {w1, .
.
.
, wL}.
Therefore, there is no need to1524S..VPADJPJJcautiousADVPRBstillVBZisNPNNpublicDTTheThe public is still cautious .DT NN VBZ RB JJ .NP#1 S#2 VP#1VP#1S#2Es kam nichts Interessantes .PPER VVFIN PIAT NN $.NP#2NP#1S#1VROOT#1Figure 1: Top: a continuous (left) and a discontinuous (right) c-tree, taken from English PTB ?22 and German NEGRA,respectively.
Head-child nodes are in bold.
Bottom: corresponding head-ordered d-trees.
The indices #1, #2, etc.
denote theorder of attachment events for each head.
Note that the English unary nodes ADVP and ADJP are dropped in the conversion.really needs cautionRB VBZ NNVP VPVPNNcautionVBZneedsRBreallyVPVPNNcautionVBZneedsRBreallyVPNNcautionVPVBZneedsRBreallyFigure 2: Three different c-structures for the VP ?really needscaution.?
All are consistent with the d-structure at the top left.
?h,m, `?, expressing a typed dependency relation` between the head word whand the modifier wm.A d-tree is projective if for every arc ?h,m, `?there is a directed path from h to all words that liebetween h and m in the surface string (Kahane etal., 1998).
Projective d-trees can be obtained fromcontinuous c-trees by reading off the lexical headsand dropping the internal nodes (Gaifman, 1965).However, this relation is many-to-one: as shownin Figure 2, several c-trees may project onto thesame d-tree, differing on their flatness and on leftor right-branching decisions.
In the next section,we introduce the concept of head-ordered d-treesand express one-to-one mappings between thesetwo representations.Prior work.
There has been a considerableamount of work developing rich-feature d-parsers.While projective d-parsers can use dynamic pro-gramming (Eisner and Satta, 1999; Koo andconsider an extra root symbol, as often done in the literature.Collins, 2010), non-projective d-parsers typicallyrely on approximate decoders, since the underly-ing problem is NP-hard beyond arc-factored mod-els (McDonald and Satta, 2007).
An alternativeare transition-based d-parsers (Nivre et al, 2006;Zhang and Nivre, 2011), which achieve observedlinear time.
Since d-parsing algorithms do nothave a grammar constant, typical implementationsare significantly faster than c-parsers (Rush andPetrov, 2012; Martins et al, 2013).
The key con-tribution of this paper is to reduce c-parsing to d-parsing, allowing to bring these runtimes closer.3 Head-Ordered Dependency TreesWe next endow d-trees with another layer of struc-ture, namely order information.
In this frame-work, not all modifiers of a head are ?born equal.
?Instead, their attachment to the head occurs asa sequence of ?events,?
which reflect the head?spreference for attaching some modifiers beforeothers.
As we will see, this additional structurewill undo the ambiguity expressed in Figure 2.3.1 Strictly Ordered Dependency TreesLet us start with the simpler case where the attach-ment order is strict.
For each head word h withmodifiers Mh= {m1, .
.
.
,mK}, we endow Mhwith a strict order relation ?h, so we can or-ganize all the modifiers of h as a chain, mi1?hmi2?h.
.
.
?hmiK.
We regard this chain asreflecting the order by which words are attached(i.e., if mi?hmjthis means that ?miis attached1525Figure 3: Transformation of a strictly-ordered d-tree into abinary c-tree.
Each node is split into a linked list forming aspine, to which modifiers are attached in order.Figure 4: Two discontinuous constructions caused by a non-nested order (top) and a non-projective d-tree (bottom).
Inboth cases node A has a non-contiguous yield.to h before mj?).
We represent this graphicallyby decorating d-arcs with indices (#1,#2, .
.
.)
todenote the order of events, as we do in Figure 1.A d-tree endowed with a strict order for eachhead is called a strictly ordered d-tree.
We es-tablish below a correspondence between strictlyordered d-trees and binary c-trees.
Before doingso, we need a few more definitions about c-trees.For each word position h ?
{1, .
.
.
, L}, we define?
(h) as the node higher in the c-tree whose lexi-cal head is h. We call the path from ?
(h) down tothe pre-terminal phthe spine of h. We may regarda c-tree as a set of L spines, one per word, whichattach to each other to form a tree (Carreras et al,2008).
We then have the followingProposition 1.
Binary c-trees and strictly-orderedd-trees are isomorphic, i.e., there is a one-to-onecorrespondence between the two sets, where thenumber of symbols is preserved.Proof.
We use the construction in Figure 3.
A for-mal proof is given as supplementary material.3.2 Weakly Ordered Dependency TreesNext, we relax the strict order assumption, restrict-ing the modifier sets Mh= {m1, .
.
.
,mK} to beonly weakly ordered.
This means that we can par-tition the K modifiers into J equivalence classes,Mh=?Jj=1?Mjh, and define a strict order ?honthe quotient set:?M1h?h.
.
.
?h?MJh.
Intuitively,there is still a sequence of events (1 to J), but nowat each event j it may happen that multiple mod-ifiers (the ones in the equivalence set?Mjh) are si-Algorithm 1 Conversion from c-tree to d-treeInput: c-tree C.Output: head-ordered d-tree D.1: Nodes := GETPOSTORDERTRAVERSAL(C).2: Set j(h) := 1 for every h = 1, .
.
.
, L.3: for v := ?Z, h, I?
?
Nodes do4: for every u := ?X,m,J ?
which is a child of v do5: ifm 6= h then6: Add toD an arc ?h,m,Z?, and put it in?Mj(h)h.7: end if8: end for9: Set j(h) := j(h) + 1.10: end formultaneously attached to h. A weakly orderedd-tree is a d-tree endowed with a weak order foreach head and such that any pairm,m?in the sameequivalence class (written m ?hm?)
receive thesame dependency label `.We now show that Proposition 1 can be gener-alized to weakly ordered d-trees.Proposition 2.
Unaryless c-trees and weakly-ordered d-trees are isomorphic.Proof.
This is a simple extension of Proposition 1.The construction is the same as in Figure 3, butnow we can collapse some of the nodes in thelinked list, originating multiple modifiers attach-ing to the same position of the spine?this is onlypossible for sibling arcs with the same index andarc label.
Note, however, that if we start with ac-tree with unary nodes and apply the inverse pro-cedure to obtain a d-tree, the unary nodes will belost, since they do not involve attachment of mod-ifiers.
In a chain of unary nodes, only the last nodeis recovered in the inverse transformation.We emphasize that Propositions 1?2 hold with-out blowing up the number of symbols.
That is,the dependency label alphabet is exactly the sameas the set of phrasal symbols in the constituentrepresentations.
Algorithms 1?2 convert back andforth between the two formalisms, performing theconstruction of Figure 3.
Both algorithms run inlinear time with respect to the size of the sentence.3.3 Continuous and Projective TreesWhat about the more restricted class of projectived-trees?
Can we find an equivalence relation withcontinuous c-trees?
In this section, we give a pre-cise answer to this question.
It turns out that weneed an additional property, illustrated in Figure 4.We say that ?hhas the nesting property iffcloser words in the same direction are always at-tached first, i.e., iff h < mi< mjor h > mi>1526Algorithm 2 Conversion from d-tree to c-treeInput: head-ordered d-tree D.Output: c-tree C.1: Nodes := GETPOSTORDERTRAVERSAL(D).2: for h ?
Nodes do3: Create v := ?ph, h, {h}?
and set ?
(h) := v.4: Sort Mh(D), yielding?M1h?h?M2h?h.
.
.
?h?MJh.5: for j = 1, .
.
.
, J do6: Let Z be the label in {?h,m,Z?
| m ?
?Mjh}.7: Obtain c-nodes ?
(h) = ?X,h, I?
and ?
(m) =?Ym,m,Jm?
for all m ?
?Mjh.8: Add c-node v := ?Z, h, I ??m??MjhJm?
to C.9: Set ?
(h) and {?
(m) |m ?
?Mjh} as children of v.10: Set ?
(h) := v.11: end for12: end formjimplies that either mi?hmjor mi?hmj.A weakly-ordered d-tree which is projective andwhose orders ?hhave the nesting property for ev-ery h is called a nested-weakly ordered projec-tive d-tree.
We then have the following result.Proposition 3.
Continuous unaryless c-trees andnested-weakly ordered projective d-trees are iso-morphic.Proof.
See the supplementary material.Together, Propositions 1?3 have as corollarythat nested-strictly ordered projective d-trees arein a one-to-one correspondence with binary con-tinuous c-trees.
The intuition is simple: if ?hhasthe nesting property, then, at each point in time, allone needs to decide about the next event is whetherto attach the closest available modifier on the leftor on the right.
This corresponds to choosingbetween left-branching or right-branching in a c-tree.
While this is potentially interesting for mostcontinuous c-parsers, which work with binarizedc-trees when running the CKY algorithm, our c-parsers (to be described in ?4) do not require anybinarization since they work with weakly-orderedd-trees, using Proposition 2.4 Reduction-Based Constituent ParsersWe now invoke the equivalence results establishedin ?3 to build c-parsers when only a trainable d-parser is available.
Given a c-treebank provided asinput, our procedure is outlined as follows:1.
Convert the c-treebank to dependencies (Algo-rithm 1).2.
Train a labeled d-parser on this treebank.3.
For each test sentence, run the labeled d-parserand convert the predicted d-tree into a c-treewithout unary nodes (Algorithm 2).4.
Do post-processing to recover unaries.The next subsections describe each of these stepsin detail.
Along the way, we illustrate with exper-iments using the English Penn Treebank (Marcuset al, 1993), which we lexicalized by applying thehead rules of Collins (1999).44.1 Dependency EncodingThe first step is to convert the c-treebank to head-ordered dependencies, which we do using Algo-rithm 1.
If the original treebank has discontinu-ous c-trees, we end up with non-projective d-treesor with violations of the nested property, as estab-lished in Proposition 3.
We handle this gracefullyby training a non-projective d-parser in the sub-sequent stage (see ?4.2).
Note also that this con-version drops the unary nodes (a consequence ofProposition 2).
These nodes will be recovered inthe last stage, as described in ?4.4.Since in this paper we are assuming that onlyan off-the-shelf d-parser is available, we need toconvert head-ordered d-trees to plain d-trees.
Wedo so by encoding the order information in the de-pendency labels.
We tried two different strategies.The first one, direct encoding, just appends suf-fixes #1, #2, etc., as in Figure 1.
A disadvantage isthat the number of labels grows unbounded withthe treebank size, as we may encounter complexsubstructures where the event sequences are long.The second strategy is a delta-encoding schemewhere, rather than writing the absolute indices inthe dependency label, we write the differences be-tween consecutive ones.5We used this strategyfor the continuous treebanks only, whose d-treesare guaranteed to satisfy the nested property.For comparison, we also implemented a repli-cation of the encoding proposed by Hall and Nivre(2008), which we call H&N-encoding.
This strat-egy concatenates all the c-nodes?
symbols in themodifier?s spine with the attachment position inthe head?s spine (e.g., in Figure 3, if the modi-fier m2has a spine with nodes X1, X2, X3, thegenerated d-label would be X1|X2|X3#2; our directencoding scheme generates Z2#2 instead).
Sincetheir strategy encodes the entire spines into com-4We train on ?02?21, use ?22 for validation, and test on?23.
We predict automatic POS tags with TurboTagger (Mar-tins et al, 2013), with 10-fold jackknifing on the training set.5For example, if #1,#3,#4 and #2,#3,#3,#5 arerespectively the sequence of indices from the head to the leftand to the right, we encode these sequences as #1,#2,#1and #2,#1,#0,#2 (using 3 distinct indices instead of 5).1527plex arc labels, many such labels will be gener-ated, leading to slower runtimes and poorer gener-alization, as we will see.For the training portion of the English PTB,which has 27 non-terminal symbols, the direct en-coding strategy yields 75 labels, while delta en-coding yields 69 labels (2.6 indices per symbol).By contrast, the H&N-encoding procedure yields731 labels, more than 10 times as many.
We latershow (in Tables 1?2) that delta-encoding leads to aslightly higher c-parsing accuracy than direct en-coding, and that both strategies are considerablymore accurate than H&N-encoding.4.2 Training the Labeled Dependency ParserThe next step is to train a labeled d-parser on theconverted treebank.
If we are doing continuous c-parsing, we train a projective d-parser; otherwisewe train a non-projective one.In our experiments, we found it advantageous toperform labeled d-parsing in two stages, as doneby McDonald et al (2006): first, train an unla-beled d-parser; then, train a dependency labeler.6Table 1 compares this approach against a one-shot strategy, experimenting with various off-the-shelf d-parsers: MaltParser (Nivre et al, 2007),MSTParser (McDonald et al, 2005), ZPar (Zhangand Nivre, 2011), and TurboParser (Martins etal., 2013), all with the default settings.
For Tur-boParser, we used basic, standard and full models.Our separate d-labeler receives as input a back-bone d-structure and predicts a label for each arc.For each head h, we predict the modifiers?
labelsusing a simple sequence model, with features ofthe form ?
(h,m, `) and ?
(h,m,m?, `, `?
), wherem and m?are two consecutive modifiers (possi-bly on opposite sides of the head) and ` and `?aretheir labels.
We use the same arc label features?
(h,m, `) as TurboParser.
For ?
(h,m,m?, `, `?
),we use the POS triplet ?ph, pm, pm?
?, plus unilex-ical features where each of the three POS is re-placed by the word form.
Both features are con-joined with the label pair ` and `?.
Decoding un-der this model can be done by running the Viterbialgorithm independently for each head.
The run-time is almost negligible compared with the timeto parse: it took 2.1 seconds to process PTB ?22,6The reason why a two-stage approach is preferable isthat one-shot d-parsers, for efficiency reasons, use label fea-tures parsimoniously.
However, for our reduction approach,d-labels are crucial and strongly interdependent, since theyjointly encode the c-structure.Dependency Parser UAS LAS F1# toks/s.MaltParser 90.93 88.95 86.87 5,392MSTParser 92.17 89.86 87.93 363ZPar 92.93 91.28 89.50 1,022TP-Basic 92.13 90.23 87.63 2,585TP-Standard 93.55 91.58 90.41 1,658TP-Full 93.70 91.70 90.53 959TP-Full + Lab., H&N enc.
93.80 87.86 89.39 871TP-Full + Lab, direct enc.
93.80 91.99 90.89 912TP-Full + Lab., delta enc.
93.80 92.00 90.94 912Table 1: Results on English PTB ?22 achieved by various d-parsers and encoding strategies.
For dependencies, we reportunlabeled/labeled attachment scores (UAS/LAS), excludingpunctuation.
For constituents, we show F1-scores (withoutpunctuation and root nodes), as provided by EVALB (Blacket al, 1992).
We report total parsing speeds in tokens per sec-ond (including time spent on pruning, decoding, and featureevaluation), measured on a Intel Xeon processor @2.30GHz.direct enc.
delta enc.# labels F1# labels F1Basque 26 85.04 17 85.17French 61 79.93 56 80.05German 66 83.44 59 83.39Hebrew 62 83.26 43 83.29Hungarian 24 86.54 15 86.67Korean 44 79.79 16 79.97Polish 47 92.39 34 92.64Swedish 29 77.02 25 77.19Table 2: Impact of direct and delta encodings on the dev setsof the SPMRL14 shared task.
Reported are the number oflabels and the F1-scores yielded by each encoding technique.a fraction of about 5% of the total runtime.4.3 Decoding into Unaryless ConstituentsAfter training the labeled d-parser, we can run iton the test data.
Then, we need to convert the pre-dicted d-tree into a c-tree without unaries.To accomplish this step, we first need to recover,for each head h, the weak order of its modifiersMh.
We do this by looking at the predicted depen-dency labels, extracting the event indices j, andusing them to build and sort the equivalent classes{?Mjh}Jj=1.
If two modifiers have the same indexj, we force them to have consistent labels (by al-ways choosing the label of the modifier which isthe closest to the head).
For continuous c-parsing,we also decrease the index j of the modifier closerto the head as much as necessary to make sure thatthe nesting property holds.
In PTB ?22, these cor-rections were necessary only for 0.6% of the to-kens.
Having done this, we use Algorithm 2 toobtain a predicted c-tree without unary nodes.15284.4 Recovery of Unary NodesThe last stage is to recover the unary nodes.
Givena unaryless c-tree as input, we predict unaries byrunning independent classifiers at each node in thetree (a simple unstructured task).
Each class iseither NULL (in which case no unary node is ap-pended to the current node) or a concatenation ofunary node labels (e.g., S->ADJP for a node JJ).We obtained 64 classes by processing the trainingsections of the PTB, the fraction of unary nodesbeing about 11% of the total number of nodes.
Toreduce complexity, for each node symbol we onlyconsider classes that have been observed with thatsymbol in the training data.
In PTB ?22, this yieldsan average of 9.9 candidates per node occurrence.The classifiers are trained on the original c-treebank, stripping off unary nodes and trained torecover those nodes.
We used the following fea-tures (conjoined with the class and with a flag in-dicating if the node is a pre-terminal):?
The production rules above and beneath thenode (e.g., S->NP VP and NP->DT NN);?
The node?s label, alone and conjoined with theparent?s label or the left/right sibling?s label;?
The leftmost and rightmost word/lemma/POStag/morpho-syntactic tags in the node?s yield;?
If the left/right node is a pre-terminal, theword/lemma/morpho-syntactic tags beneath.This is a relatively easy task: when gold unarylessc-trees are provided as input, we obtain an EVALBF1-score of 99.43%.
This large figure is due to thesmall amount of unary nodes, making this mod-ule have less impact on the final parser than thed-parser.
Being a lightweight unstructured task,this step took only 0.7 seconds to run on PTB ?22,a tiny fraction (less than 2%) of the total runtime.Table 1 shows the accuracies obtained with thed-parser followed by the unary predictor.
Sincetwo-stage TP-Full with delta-encoding is the beststrategy, we use this configuration in the sequel.To further explore the impact of delta encoding,we report in Table 2 the scores obtained by directand delta encodings on eight other treebanks (see?5.2 for details on these datasets).
With the ex-ception of German, in all cases the delta encodingyielded better EVALB F1-scores with fewer labels.5 ExperimentsTo evaluate the performance of our reduction-based parsers, we conduct experiments in a varietyParser LR LP F1 #Toks/s.Charniak (2000) 89.5 89.9 89.5 ?Klein and Manning (2003) 85.3 86.5 85.9 143Petrov and Klein (2007) 90.0 90.3 90.1 169Carreras et al (2008) 90.7 91.4 91.1 ?Zhu et al (2013) 90.3 90.6 90.4 1,290Stanford Shift-Reduce (2014) 89.1 89.1 89.1 655Hall et al (2014) 88.4 88.8 88.6 12This work 89.9 90.4 90.2 957Charniak and Johnson (2005)?91.2 91.8 91.5 84Socher et al (2013)?89.1 89.7 89.4 70Zhu et al (2013)?91.1 91.5 91.3 ?Table 3: Results on the English PTB ?23.
All systems report-ing runtimes were run on the same machine.
Marked as?arereranking and semi-supervised c-parsers.of treebanks, both continuous and discontinuous.5.1 Results on the English PTBTable 3 shows the accuracies and speeds achievedby our system on the English PTB ?23, in compar-ison to state-of-the-art c-parsers.
We can see thatour simple reduction-based c-parser surpasses thethree Stanford parsers (Klein and Manning, 2003;Socher et al, 2013, and Stanford Shift-Reduce),and is on par with the Berkeley parser (Petrov andKlein, 2007), while being more than 5 times faster.The best supervised competitor is the recentshift-reduce parser of Zhu et al (2013), whichachieves similar, but slightly better, accuracy andspeed.
Our technique has the advantage of beingflexible: since the time for d-parsing is the domi-nating factor (see ?4.4), plugging a faster d-parserautomatically yields a faster c-parser.
Whilereranking and semi-supervised systems achievehigher accuracies, this aspect is orthogonal, sincethe same techniques can be applied to our parser.5.2 Results on the SPMRL DatasetsWe experimented with datasets for eight lan-guages, from the SPMRL14 shared task (Seddahet al, 2014).
We used the official training, de-velopment and test sets with the provided pre-dicted POS tags.
For French and German, weused the lexicalization rules detailed in Dybro-Johansen (2004) and Rehbein (2009), respectively.For Basque, Hungarian and Korean, we alwaystook the rightmost modifier as head-child node.For Hebrew and Polish we used the leftmost mod-ifier instead.
For Swedish we induced head rulesfrom the provided dependency treebank, as de-scribed in Versley (2014b).
These choices werebased on dev-set experiments.Table 4 shows the results.
For all languages ex-1529cept French, our system outperforms the Berke-ley parser (Petrov and Klein, 2007), with or with-out prescribed POS tags.
Our average F1-scoresare superior to the best non-reranking system par-ticipating in the shared task (Crabb?e and Seddah,2014) and to the c-parser of Hall et al (2014),achieving the best results for 4 out of 8 languages.5.3 Results on the Discontinuous TreebanksFinally, we experimented on two widely-used dis-continuous German treebanks: TIGER (Brants etal., 2002) and NEGRA (Skut et al, 1997).
Forthe former, we used two different splits: TIGER-SPMRL, provided in the SPMRL14 shared task;and TIGER-H&N, used by Hall and Nivre (2008).For NEGRA, we used the standard splits.
In theseexperiments, we skipped the unary recovery stage,since very few unary nodes exist in the data.7Weran TurboTagger to predict POS tags for TIGER-H&N and NEGRA, while in TIGER-SPMRL we usedthe predicted POS tags provided in the shared task.All treebanks were lexicalized using the head-rulesets of Rehbein (2009).
For comparison to relatedwork, sentence length cut-offs of 30, 40 and 70were applied during the evaluation.Table 5 shows the results.
We observe thatour approach outperforms all the competitors con-siderably, achieving state-of-the-art accuracies forboth datasets.
The best competitor, van Cranen-burgh and Bod (2013), is more than 3 points be-hind, both in TIGER-H&N and in NEGRA.
Ourreduction-based parsers are also much faster: vanCranenburgh and Bod (2013) report 3 hours toparse NEGRA with L ?
40.
Our system parsesall NEGRA sentences (regardless of length) in 27.1seconds in a single core, which corresponds to arate of 618 tokens per second.
This approaches thespeed of the easy-first system of Versley (2014a),who reports runtimes in the range 670?920 tokensper second, but is much less accurate.6 Related WorkConversions between constituents and dependen-cies have been considered by De Marneffe et al(2006) in one direction, and by Collins et al(1999) and Xia and Palmer (2001) in the other, to-ward multi-representational treebanks (Xia et al,2008).
This prior work aimed at linguisticallysound conversions, involving grammar-specific7NEGRA has no unaries; for the TIGER-SPMRL and H&Ndev-sets, the fraction of unaries is 1.45% and 1.01%.TIGER-SPMRL L ?
70 allV14b, gold 76.46 / 41.05 76.11 / 40.94Ours, gold 80.98 / 43.44 80.62 / 43.32V14b, pred 73.90 / 37.00 ?
/ ?Ours, pred 77.72 / 38.75 77.32 / 38.64TIGER-H&N L ?
40 allHN08, gold 79.93 / 37.78 ?
/ ?V14a, gold 74.23 / 37.32 ?
/ ?Ours, gold 85.53 / 51.21 84.22 / 49.63HN08, pred 75.33 / 32.63 ?
/ ?CB13, pred 78.8?
/ 40.8?
?
/ ?Ours, pred 82.57 / 45.93 81.12 / 44.48NEGRA L ?
30 L ?
40 allM12, gold 74.5?
/ ?
?
/ ?
?
/ ?C12, gold ?
/ ?
72.33 / 33.16 71.08 / 32.10KM13, gold 75.75 / ?
?
/ ?
?
/ ?CB13, gold ?
/ ?
76.8?
/ 40.5?
?
/ ?Ours, gold 82.56 / 52.13 81.08 / 48.04 80.52 / 46.70CB13, pred ?
/ ?
74.8?
/ 38.7?
?
/ ?Ours, pred 79.63 / 48.43 77.93 / 44.83 76.95 / 43.50Table 5: F1/ exact match scores on TIGER and NEGRA testsets, with gold and predicted POS tags.
These scores are com-puted by the DISCO-DOP evaluator ignoring root nodes and,for TIGER-H&N and NEGRA, punctuation tokens.
The base-lines are published results by Hall and Nivre 2008 (HN08),Maier et al 2012 (M12), van Cranenburgh 2012 (C12),Kallmeyer and Maier 2013 (KM13), van Cranenburgh andBod 2013 (CB13), and Versley 2014a, 2014b (V14a, V14b).transformation rules to handle the kind of ambigu-ities expressed in Figure 2.
Our work differs in thatwe are not concerned about the linguistic plausi-bility of our conversions, but only with the formalaspects that underlie the two representations.The work most related to ours is Hall and Nivre(2008), who also convert dependencies to con-stituents to prototype a c-parser for German.
Theirencoding strategy is compared to ours in ?4.1: theyencode the entire spines into the dependency la-bels, which become rather complex and numer-ous.
A similar strategy has been used by Vers-ley (2014a) for discontinuous c-parsing.
Both arelargely outperformed by our system, as shown in?5.3.
The crucial difference is that we encode onlythe top node?s label and its position in the spine?besides being a much lighter representation, ourshas an interpretation as a weak ordering, leading tothe isomorphisms expressed in Propositions 1?3.Joint constituent and dependency parsing havebeen tackled by Carreras et al (2008) and Rushet al (2010), but the resulting parsers, while ac-curate, are more expensive than a single c-parser.Very recently, Kong et al (2015) proposed a muchcheaper pipeline in which d-parsing is performedfirst, followed by a c-parser constrained to be con-1530Parser Basque French German Hebrew Hungar.
Korean Polish Swedish Avg.Berkeley 70.50 80.38 78.30 86.96 81.62 71.42 79.23 79.19 78.45Berkeley Tagged 74.74 79.76 78.28 85.42 85.22 78.56 86.75 80.64 81.17Hall et al (2014) 83.39 79.70 78.43 87.18 88.25 80.18 90.66 82.00 83.72Crabb?e and Seddah (2014) 85.35 79.68 77.15 86.19 87.51 79.35 91.60 82.72 83.69This work 85.90 78.75 78.66 88.97 88.16 79.28 91.20 82.80 84.22Bj?orkelund et al (2014) 88.24 82.53 81.66 89.80 91.72 83.81 90.50 85.50 86.72Table 4: F1-scores on eight treebanks of the SPMRL14 shared task, computed with the provided EVALB SPMRL tool, whichtakes into account all tokens except root nodes.
Berkeley Tagged is a version of Petrov and Klein (2007) using the predicted POStags provided by the organizers.
Crabb?e and Seddah (2014) is the best non-reranking system in the shared task, and Bj?orkelundet al (2014) the ensemble and reranking-based system which won the official task.
We report their published scores.sistent with the predicted d-structure.
Our workdiffers in which we do not need to run a c-parserin the second stage?instead, the d-parser alreadystores constituent information in the arc labels,and the only necessary post-processing is to re-cover unary nodes.
Another advantage of ourmethod is that it can be readily used for discon-tinuous parsing, while their constrained CKY al-gorithm can only produce continuous parses.7 ConclusionWe proposed a reduction technique that allowsto implement a c-parser when only a d-parser isgiven.
The technique is applicable to any d-parser,regardless of its nature or kind.
This reduction wasaccomplished by endowing d-trees with a weak or-der relation, and showing that the resulting class ofhead-ordered d-trees is isomorphic to constituenttrees.
We showed empirically that the our re-duction leads to highly-competitive c-parsers forEnglish and for eight morphologically rich lan-guages; and that it outperforms the current stateof the art in discontinuous parsing of German.AcknowledgmentsWe would like to thank the three reviewersfor their insightful comments, and Slav Petrov,Djam?e Seddah, Yannick Versley, David Hall,Muhua Zhu, Lingpeng Kong, Carlos G?omez-Rodr?
?guez, and Andreas van Cranenburgh forvaluable feedback and help in preparing dataand running software code.
This research hasbeen partially funded by the Spanish Ministryof Economy and Competitiveness and FEDER(project TIN2010-18552-C03-01), Ministry ofEducation (FPU Grant Program) and Xunta deGalicia (projects R2014/029 and R2014/034).A.
M. was supported by the EU/FEDER pro-gramme, QREN/POR Lisboa (Portugal), underthe Intelligo project (contract 2012/24803), andby the FCT grants UID/EEA/50008/2013 andPTDC/EEI-SII/2312/2012.ReferencesTaylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.2011.
Jointly learning to extract and compress.
InProc.
of Annual Meeting of the Association for Com-putational Linguistics.Anders Bj?orkelund,?Ozlem C?etino?glu, AgnieszkaFale?nska, Rich?ard Farkas, Thomas Mueller, Wolf-gang Seeker, and Zsolt Sz?ant?o.
2014.
Introducingthe ims-wroc?aw-szeged-cis entry at the spmrl 2014shared task: Reranking and morpho-syntax meet un-labeled data.
In Proc.
of the First Joint Workshopon Statistical Parsing of Morphologically Rich Lan-guages and Syntactic Analysis of Non-CanonicalLanguages.Ezra Black, John Lafferty, and Salim Roukos.
1992.Development and evaluation of a broad-coverageprobabilistic grammar of english-language computermanuals.
In Proc.
of Annual Meeting on Associationfor Computational Linguistics.Adriane Boyd.
2007.
Discontinuity revisited: Animproved conversion to context-free representations.In Proc.
of Linguistic Annotation Workshop.Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-gang Lezius, and George Smith.
2002.
The TIGERtreebank.
In Proc.
of the workshop on treebanks andlinguistic theories.Xavier Carreras, Michael Collins, and Terry Koo.2008.
TAG, Dynamic Programming, and the Per-ceptron for Efficient, Feature-rich Parsing.
In Proc.of the International Conference on Natural Lan-guage Learning.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminativereranking.
In Proc.
of Annual Meeting of the As-sociation for Computational Linguistics.Eugene Charniak.
1996.
Tree-bank grammars.
InProc.
of the National Conference on Artificial Intel-ligence.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proc.
of the North AmericanChapter of the Association for Computational Lin-guistics Conference.1531Michael Collins, Lance Ramshaw, Jan Haji?c, andChristoph Tillmann.
1999.
A Statistical Parser forCzech.
In Proc.
of the Annual Meeting of the Asso-ciation for Computational Linguistics on Computa-tional Linguistics.Michael Collins.
1999.
Head-driven statistical modelsfor natural language parsing.
Ph.D. thesis, Univer-sity of Pennsylvania.Benoit Crabb?e and Djam?e Seddah.
2014.
Multilingualdiscriminative shift reduce phrase structure parsingfor the SPMRL 2014 shared task.
In Proc.
of theFirst Joint Workshop on Statistical Parsing of Mor-phologically Rich Languages and Syntactic Analysisof Non-Canonical Languages.Marie-Catherine De Marneffe, Bill MacCartney,Christopher D Manning, et al 2006.
Generat-ing typed dependency parses from phrase structureparses.
In Proc.
of the Meeting of the Language Re-sources and Evaluation Conference.Ane Dybro-Johansen.
2004.
Extraction automatiquede Grammaires d?Arbres Adjoints `a partir d?un cor-pus arbor?e du franc?ais.
Master?s thesis, Univer-sit?e Paris 7.Jason Eisner and Giorgio Satta.
1999.
Efficient pars-ing for bilexical context-free grammars and head au-tomaton grammars.
In Proc.
of Annual Meeting ofthe Association for Computational Linguistics.Jason Eisner.
1996.
Three new probabilistic modelsfor dependency parsing: An exploration.
In Proc.of International Conference on Computational Lin-guistics.Jakob Elming, Anders Johannsen, Sigrid Klerke,Emanuele Lapponi, Hector Martinez Alonso, andAnders S?gaard.
2013.
Down-stream effects oftree-to-dependency conversions.
In Proc.
of the An-nual Conference of the Human Language Technolo-gies - North American Chapter of the Associationfor Computational Linguistics.Haim Gaifman.
1965.
Dependency systems andphrase-structure systems.
Information and control.Carlos G?omez-Rodr?
?guez, Marco Kuhlmann, and Gior-gio Satta.
2010.
Efficient parsing of well-nested lin-ear context-free rewriting systems.
In Proc.
of theAnnual Conference of the North American Chapterof the Association for Computational Linguistics.Johan Hall and Joakim Nivre.
2008.
A dependency-driven parser for german dependency and con-stituency representations.
In Proc.
of the Workshopon Parsing German.David Hall, Greg Durrett, and Dan Klein.
2014.
Lessgrammar, more features.
In Proc.
of the AnnualMeeting of the Association for Computational Lin-guistics.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In Proc.
of the An-nual Meeting of the Association for ComputationalLinguistics.Richard Johansson and Pierre Nugues.
2008.Dependency-based Semantic Role Labeling of Prop-Bank.
In Empirical Methods for Natural LanguageProcessing.Mark Johnson.
1998.
PCFG models of linguistic treerepresentations.
Computational Linguistics.Sylvain Kahane, Alexis Nasr, and Owen Rambow.1998.
Pseudo-projectivity: a polynomially parsablenon-projective dependency grammar.
In Proc.
ofthe International Conference on Computational Lin-guistics.Laura Kallmeyer and Wolfgang Maier.
2013.
Data-driven parsing using probabilistic linear context-freerewriting systems.
Computational Linguistics.Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
In Proc.
of Annual Meet-ing on Association for Computational Linguistics.Lingpeng Kong and Noah A Smith.
2014.
An em-pirical comparison of parsing methods for stanforddependencies.
arXiv preprint arXiv:1404.4314.Lingpeng Kong, Alexander M. Rush, and Noah A.Smith.
2015.
Transforming dependencies intophrase structures.
In Proc.
of the Conference ofthe North American Chapter of the Association forComputational Linguistics.Terry Koo and Michael Collins.
2010.
Efficient third-order dependency parsers.
In Proc.
of Annual Meet-ing of the Association for Computational Linguis-tics.Sandra K?ubler, Wolfgang Maier, Ines Rehbein, andYannick Versley.
2008.
How to compare treebanks.In Proc.
of the Meeting of the Language Resourcesand Evaluation Conference.Marco Kuhlmann and Joakim Nivre.
2006.
Mildlynon-projective dependency structures.
In Proc.
ofthe joint conference of the International Committeeon Computational Linguistics and the Associationfor Computational Linguistics.Wolfgang Maier and Anders S?gaard.
2008.
Tree-banks and mild context-sensitivity.
In Proc.
of For-mal Grammar.Wolfgang Maier, Miriam Kaeshammer, and LauraKallmeyer.
2012.
Data-driven plcfrs parsing revis-ited: Restricting the fan-out to two.
In Proc.
of theEleventh International Conference on Tree Adjoin-ing Grammars and Related Formalisms.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of english: The penn treebank.
Computa-tional Linguistics.Andr?e F. T. Martins, Miguel B. Almeida, and Noah A.Smith.
2013.
Turning on the turbo: Fast third-ordernon-projective turbo parsers.
In Proc.
of the AnnualMeeting of the Association for Computational Lin-guistics.Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.2005.
Probabilistic CFG with latent annotations.
InProc.
of the Annual Meeting of the Association forComputational Linguistics.Ryan McDonald and Giorgio Satta.
2007.
On the com-plexity of non-projective data-driven dependencyparsing.
In Proc.
of International Conference onParsing Technologies.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic.
2005.
Non-projective dependency pars-1532ing using spanning tree algorithms.
In Proc.
of Em-pirical Methods for Natural Language Processing.Ryan McDonald, Kevin Lerman, and Fernando Pereira.2006.
Multilingual dependency analysis with a two-stage discriminative parser.
In Proc.
of InternationalConference on Natural Language Learning.Joakim Nivre, Johan Hall, Jens Nilsson, G?ulsenEryi?git, and Svetoslav Marinov.
2006.
Labeledpseudo-projective dependency parsing with supportvector machines.
In Proc.
of International Confer-ence on Natural Language Learning.Joakim Nivre, Johan Hall, Jens Nilsson, AtanasChanev, G?ulsen Eryi?git, Sandra K?ubler, SvetoslavMarinov, and Erwin Marsi.
2007.
Maltparser: Alanguage-independent system for data-driven depen-dency parsing.
Natural Language Engineering.Fernando C. N. Pereira and David H. D. Warren.
1983.Parsing as Deduction.
In Proc.
of the Annual Meet-ing of the Association for Computational Linguis-tics.Slav Petrov and Dan Klein.
2007.
Improved infer-ence for unlexicalized parsing.
In Proc.
of the NorthAmerican Chapter of the Association for Computa-tional Linguistics.Ines Rehbein.
2009.
Treebank-Based Grammar Acqui-sition for German.
Ph.D. thesis, School of Comput-ing, Dublin City University.Alexander M Rush and Slav Petrov.
2012.
Vine prun-ing for efficient multi-pass dependency parsing.
InProc.
of the North American Chapter of the Associ-ation for Computational Linguistics.Alexander Rush, David Sontag, Michael Collins, andTommi Jaakkola.
2010.
On dual decomposition andlinear programming relaxations for natural languageprocessing.
In Proc.
of Empirical Methods for Nat-ural Language Processing.Kenji Sagae and Alon Lavie.
2005.
A classifier-basedparser with linear run-time complexity.
In Proc.
ofthe Ninth International Workshop on Parsing Tech-nology.Djam?e Seddah, Sandra K?ubler, and Reut Tsarfaty.2014.
Introducing the spmrl 2014 shared task onparsing morphologically-rich languages.
In Proc.of the First Joint Workshop on Statistical Parsingof Morphologically Rich Languages and SyntacticAnalysis of Non-Canonical Languages, August.Wojciech Skut, Brigitte Krenn, Thorsten Brants, andHans Uszkoreit.
1997.
An annotation scheme forfree word order languages.
In Proc.
of the FifthConference on Applied Natural Language Process-ing ANLP-97.Richard Socher, John Bauer, Christopher D Manning,and Andrew Y Ng.
2013.
Parsing with composi-tional vector grammars.
In Proc.
of Annual Meetingof the Association for Computational Linguistics.Andreas van Cranenburgh and Rens Bod.
2013.
Dis-continuous parsing with an efficient and accuratedop model.
Proc.
of International Conference onParsing Technologies.Andreas van Cranenburgh.
2012.
Efficient parsingwith linear context-free rewriting systems.
In Proc.of the Conference of the European Chapter of theAssociation for Computational Linguistics.Yannick Versley.
2014a.
Experiments with easy-firstnonprojective constituent parsing.
In Proc.
of theFirst Joint Workshop on Statistical Parsing of Mor-phologically Rich Languages and Syntactic Analysisof Non-Canonical Languages.Yannick Versley.
2014b.
Incorporating semi-supervised features into discontinuous easy-firstconstituent parsing.
CoRR, abs/1409.3813.Krishnamurti Vijay-Shanker, David J Weir, and Ar-avind K Joshi.
1987.
Characterizing structuraldescriptions produced by various grammatical for-malisms.
In Proc.
of the Annual Meeting on Associ-ation for Computational Linguistics.Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.2009.
Phrase dependency parsing for opinion min-ing.
In Proc.
of Empirical Methods for Natural Lan-guage Processing.Fei Xia and Martha Palmer.
2001.
Converting depen-dency structures to phrase structures.
In Proc.
of theFirst International Conference on Human LanguageTechnology Research.Fei Xia, Owen Rambow, Rajesh Bhatt, Martha Palmer,and Dipti Misra Sharma.
2008.
Towards a multi-representational treebank.
LOT Occasional Series.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statis-tical dependency analysis with support vector ma-chines.
In Proc.
of International Conference onParsing Technologies.Yue Zhang and Joakim Nivre.
2011.
Transition-baseddependency parsing with rich non-local features.
InProc.
of the Annual Meeting of the Association forComputational Linguistics.Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang,and Jingbo Zhu.
2013.
Fast and accurate shift-reduce constituent parsing.
In Proc.
of Annual Meet-ing of the Association for Computational Linguis-tics.1533
