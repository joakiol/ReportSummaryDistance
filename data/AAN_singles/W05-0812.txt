Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 83?86,Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005Improved HMM Alignment Models for Languages with Scarce ResourcesAdam LopezInstitute for Advanced Computer StudiesDepartment of Computer ScienceUniversity of MarylandCollege Park, MD 20742alopez@cs.umd.eduPhilip ResnikInstitute for Advanced Computer StudiesDepartment of LinguisticsUniversity of MarylandCollege Park, MD 20742resnik@umiacs.umd.eduAbstractWe introduce improvements to statistical wordalignment based on the Hidden MarkovModel.
One improvement incorporates syntac-tic knowledge.
Results on the workshop datashow that alignment performance exceeds thatof a state-of-the art system based on more com-plex models, resulting in over a 5.5% absolutereduction in error on Romanian-English.1 IntroductionThe most widely used alignment model is IBM Model 4(Brown et al, 1993).
In empirical evaluations it has out-performed the other IBM Models and a Hidden MarkovModel (HMM) (Och and Ney, 2003).
It was the basisfor a system that performed very well in a comparisonof several alignment systems (Dejean et al, 2003; Mihal-cea and Pedersen, 2003).
Implementations are also freelyavailable (Al-Onaizan et al, 1999; Och and Ney, 2003).The IBM Model 4 search space cannot be efficientlyenumerated; therefore it cannot be trained directly usingExpectation Maximization (EM).
In practice, a sequenceof simpler models such as IBM Model 1 and an HMMModel are used to generate initial parameter estimatesand to enumerate a partial search space which can be ex-panded using hill-climbing heuristics.
IBM Model 4 pa-rameters are then estimated over this partial search spaceas an approximation to EM (Brown et al, 1993; Och andNey, 2003).
This approach yields good results, but it hasbeen observed that the IBM Model 4 performance is onlyslightly better than that of the underlying HMM Modelused in this bootstrapping process (Och and Ney, 2003).This is illustrated in Figure 1.Based on this observation, we hypothesize that imple-mentations of IBM Model 4 derive most of their per-formance benefits from the underlying HMM Model.Furthermore, owing to the simplicity of HMM Models,we believe that they are more conducive to study andimprovement than more complex models such as IBMModel 4.
We illustrate this point by introducing modifi-cations to the HMM model which improve performance..3.35.4.45.5.55.6.65.7Model 1 HMM Model 4AERTraining Iterations???
?
??
?
?
?
?
?
?
?
?
?
??????
?
?
?
?
?
?
?
?
?Figure 1: The improvement in Alignment Error Rate(AER) is shown for both P(f|e) and P(e|f) alignments onthe Romanian-English development set over several iter-ations of the IBM Model 1 ?
HMM ?
IBM Model 4training sequence.2 HMMs and Word AlignmentThe objective of word alignment is to discover the word-to-word translational correspondences in a bilingual cor-pus of S sentence pairs, which we denote {(f(s),e(s)) : s ?[1,S]}.
Each sentence pair (f,e) = ( f M1 ,eN1 ) consists ofa sentence f in one language and its translation e in theother, with lengths M and N, respectively.
By conventionwe refer to e as the English sentence and f as the Frenchsentence.
Correspondences in a sentence are representedby a set of links between words.
A link ( f j ,ei) denotes acorrespondence between the ith word ei of e and the jthword f j of f.Many alignment models arise from the conditional dis-tribution P(f|e).
We can decompose this by introducingthe hidden alignment variable a = aM1 .
Each element ofa takes on a value in the range [1,N].
The value of aidetermines a link between the ith French word fi andthe aith English word eai .
This representation introduces83an asymmetry into the model because it constrains eachFrench word to correspond to exactly one English word,while each English word is permitted to correspond to anarbitrary number of French words.
Although the result-ing set of links may still be relatively accurate, we cansymmetrize by combining it with the set produced by ap-plying the complementary model P(e|f) to the same data(Och and Ney, 2000b).
Making a few independence as-sumptions we arrive at the decomposition in Equation 1.
1P(f,a|e) =M?i=1d(ai|ai?1) ?
t( fi|eai) (1)We refer to d(ai|ai?1) as the distortion model and t( fi|eai)as the translation model.
Conveniently, Equation 1 is inthe form of an HMM, so we can apply standard algo-rithms for HMM parameter estimation and maximization.This approach was proposed in Vogel et al (1996) andsubsequently improved (Och and Ney, 2000a; Toutanovaet al, 2002).2.1 The Tree Distortion ModelEquation 1 is adequate in practice, but we can improveit.
Numerous parameterizations have been proposed forthe distortion model.
In our surface distortion model, itdepends only on the distance ai ?
ai?1 and an automati-cally determined word class C(eai?1) as shown in Equa-tion 2.
It is similar to (Och and Ney, 2000a).
The wordclass C(eai?1) is assigned using an unsupervised approach(Och, 1999).d(ai|ai?1) = p(ai|ai ?ai?1,C(eai?1)) (2)The surface distortion model can capture local move-ment but it cannot capture movement of structures or thebehavior of long-distance dependencies across transla-tions.
The intuitive appeal of capturing richer informa-tion has inspired numerous alignment models (Wu, 1995;Yamada and Knight, 2001; Cherry and Lin, 2003).
How-ever, we would like to retain the simplicity and good per-formance of the HMM Model.We introduce a distortion model which depends on thetree distance ?
(ei,ek) = (w,x,y) between each pair of En-glish words ei and ek.
Given a dependency parse of eM1 ,w and x represent the respective number of dependencylinks separating ei and ek from their closest common an-cestor node in the parse tree.
2 The final element y = {11We ignore the sentence length probability p(M|N), whichis not relevant to word alignment.
We also omit discussionof HMM start and stop probabilities, and normalization oft( fi|eai), although we find in practice that attention to these de-tails can be beneficial.2The tree distance could easily be adapted to work withphrase-structure parses or tree-adjoining parses instead of de-pendency parses.I1 very2 much3 doubt4 that5?
(I1,very2) = (1,2,0)?
(very2, I1) = (2,1,1)?
(I1,doubt4) = (1,0,0)?
(that5, I1) = (1,1,1)Figure 2: Example of tree distances in a sentence fromthe Romanian-English development set.if i > k; 0 otherwise} is simply a binary indicator of thelinear relationship of the words within the surface string.Tree distance is illustrated in Figure 2.In our tree distortion model, we condition on the treedistance and the part of speech T (ei?1), giving us Equa-tion 3.d(ai|ai?1) = p(ai, |?
(eai ,eai?1),T (eai?1)) (3)Since both the surface distortion and tree distortionmodels represent p(ai|ai?1), we can combine them usinglinear interpolation as in Equation 4.d(ai|ai?1) =?C(eai?1),T (eai?1 )p(ai|?
(eai ,eai?1),T (eai?1)) +(1?
?C(eai?1),T (eai?1 ))p(ai|ai ?ai?1,C(eai?1))(4)The ?C,T parameters can be initialized from a uniformdistribution and trained with the other parameters usingEM.
In principle, any number of alternative distortionmodels could be combined with this framework.2.2 Improving InitializationOur HMM produces reasonable results if we draw ourinitial parameter estimates from a uniform distribution.However, we can do better.
We estimate the initialtranslation probability t( f j |ei) from the smoothed log-likelihood ratio LLR(ei, f j)?1 computed over sentencecooccurrences.
Since this method works well, we applyLLR(ei, f j) in a single reestimation step shown in Equa-tion 5.t( f |e) = LLR( f |e)?2 +n?e?
LLR( f |e?
)?2 +n ?
|V |(5)In reestimation LLR( f |e) is computed from the expectedcounts of f and e produced by the EM algorithm.
This issimilar to Moore (2004); as in that work, |V | = 100,000,and ?1, ?2, and n are estimated on development data.We can also use an improved initial estimate for distor-tion.
Consider a simple distortion model p(ai|ai ?ai?1).We expect this distribution to have a maximum nearP(ai|0) because we know that words tend to retain theirlocality across translation.
Rather than wait for this tooccur, we use an initial estimate for the distortion modelgiven in Equation 6.84corpus n ?1 ?2 ?
symmetrization n?1 ?
?11 ?
?12 ?
?1English-Inuktitut 1?4 1.0 1.75 -1.5 ?
5?4 1.0 1.75 -1.5Romanian-English 5?4 1.5 1.0 -2.5 refined (Och and Ney, 2000b) 5?4 1.5 1.0 -2.5English-Hindi 1?4 1.5 3.0 -2.5 ?
1?2 1.0 1.0 -1.0Table 1: Training parameters for the workshop data (see Section 2.2).
Parameters n, ?1, ?2, and ?
were used in theinitialization of P(f|e) model, while n?1, ?
?11 , ?
?12 , and ?
?1 were used in the initialization of the P(e|f) model.corpus type HMM limited (Eq.
2) HMM unlimited (Eq.
4) IBM Model 4P R AER P R AER P R AEREnglish-InuktitutP(f|e) .4962 .6894 .4513 ?
?
?
.4211 .6519 .5162P(e|f) .5789 .8635 .3856 ?
?
?
.5971 .8089 .3749?
.8916 .6280 .2251 ?
?
?
.8682 .5700 .2801English-HindiP(f|e) .5079 .4769 .5081 .5057 .4748 .5102 .5219 .4223 .5332P(e|f) .5566 .4429 .5067 .5566 .4429 .5067 .5652 .3939 .5358?
.4408 .5649 .5084 .4365 .5614 .5088 .4543 .5401 .5065Romanian-EnglishP(f|e) .6876 .6233 .3461 .6876 .6233 .3461 .6828 .5414 .3961P(e|f) .7168 .6217 .3341 .7155 .6205 .3354 .7520 .5496 .3649refined .7377 .6169 .3281 .7241 .6215 .3311 .7620 .5134 .3865Table 2: Results on the workshop data.
The systems highlighted in bold are the ones that were used in the shared task.For each corpus, the last row shown represents the results that were actually submitted.
Note that for English-Hindi,our self-reported results in the unlimited task are slightly lower than the original results submitted for the workshop,which contained an error.d(ai|ai?1) ={|ai ?ai?1|?/Z,?
< 0 if ai 6= ai?1.1/Z if ai = ai?1.
(6)We choose Z to normalize the distribution.
We mustoptimize ?
on a development set.
This distribution hasa maximum when |ai ?
ai?1| ?
{?1,0,1}.
Although wecould reasonably choose any of these three values as themaximum for the initial estimate, we found in develop-ment that the maximum of the surface distortion distribu-tion varied with C(eai?1), although it was always in therange [?1,2].2.3 Does NULL Matter in Asymmetric Alignment?Och and Ney (2000a) introduce a NULL-alignment ca-pability to the HMM alignment model.
This allows anyword f j to link to a special NULL word ?
by conven-tion denoted e0 ?
instead of one of the words eN1 .
A link( f j ,e0) indicates that f j does not correspond to any wordin e. This improved alignment performance in the ab-sence of symmetrization, presumably because it allowsthe model to be conservative when evidence for an align-ment is lacking.We hypothesize that NULL alignment is unnecessaryfor asymmetric alignment models when we symmetrizeusing intersection-based methods (Och and Ney, 2000b).The intuition is simple: if we don?t permit NULL align-ments, then we expect to produce a high-recall, low-precision alignment; the intersection of two such align-ments should mainly improve precision, resulting in ahigh-recall, high-precision alignment.
If we allow NULLalignments, we may be able produce a high-precision,low-recall asymmetric alignment, but symmetrization byintersection will not improve recall.3 Results with the Workshop DataIn our experiments, the dependency parse and parts ofspeech are produced by minipar (Lin, 1998).
This parserhas been used in a much different alignment model(Cherry and Lin, 2003).
Since we only had parses forEnglish, we did not use tree distortion in the applicationof P(e|f), needed for symmetrization.The parameter settings that we used in aligning theworkshop data are presented in Table 1.
Although ourprior work with English and French indicated that in-tersection was the best method for symmetrization, wefound in development that this varied depending on thecharacteristics of the corpus and the type of annotation(in particular, whether the annotation set included proba-ble alignments).
The results are summarized in Table 2.It shows results with our HMM model using both Equa-tions 2 and 4 as our distortion model, which represent85the unlimited and limited resource tracks, respectively.It also includes a comparison with IBM Model 4, forwhich we use a training sequence of IBM Model 1 (5iterations), HMM (6 iterations), and IBM Model 4 (5 it-erations).
This sequence performed well in an evaluationof the IBM Models (Och and Ney, 2003).For comparative purposes, we show results of apply-ing both P(f|e) and P(e|f) prior to symmetrization, alongwith results of symmetrization.
Comparison of the asym-metric and symmetric results largely supports the hypoth-esis presented in Section 2.3, as our system generally pro-duces much better recall than IBM Model 4, while of-fering a competitive precision.
Our symmetrized resultsusually produced higher recall and precision, and loweralignment error rate.We found that the largest gain in performance camefrom the improved initialization.
The combined distor-tion model (Equation 4), which provided a small benefitover the surface distortion model (Equation 2) on the de-velopment set, performed slightly worse on the test set.We found that the dependencies on C(eai?1) andT (eai?1) were harmful to the P(f|e) alignment for Inukti-tut, and did not submit results for the unlimited resourcesconfiguration.
However, we found that alignment wasgenerally difficult for all models on this particular task,perhaps due to the agglutinative nature of Inuktitut.4 ConclusionsWe have proposed improvements to the largely over-looked HMM word alignment model.
Our improvementsyield good results on the workshop data.
We have addi-tionally shown that syntactic information can be incorpo-rated into such a model; although the results are not su-perior, they are competitive with surface distortion.
In fu-ture work we expect to explore additional parameteriza-tions of the HMM model, and to perform extrinsic evalu-ations of the resulting alignments by using them in the pa-rameter estimation of a phrase-based translation model.AcknowledgementsThis research was supported in part by ONR MURI Con-tract FCPO.810548265.
The authors would like to thankBill Byrne, David Chiang, Okan Kolak, and the anony-mous reviewers for their helpful comments.ReferencesYaser Al-Onaizan, Jan Curin, Michael Jahr, KevinKnight, John Lafferty, Dan Melamed, Franz Josef Och,David Purdy, Noah A. Smith, and David Yarowsky.1999.
Statistical machine translation: Final report.
InJohns Hopkins University 1999 Summer Workshop onLanguage Engineering.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathematicsof statistical machine translation: Parameter estima-tion.
Computational Linguistics, 19(2):263?311, Jun.Colin Cherry and Dekang Lin.
2003.
A probabilitymodel to improve word alignment.
In ACL Proceed-ings, Jul.Herve Dejean, Eric Gaussier, Cyril Goutte, and KenjiYamada.
2003.
Reducing parameter space for wordalignment.
In Proceedings of the Workshop on Build-ing and Using Parallel Texts: Data Driven MachineTranslation and Beyond, pages 23?26, May.Dekang Lin.
1998.
Dependency-based evaluation ofminipar.
In Proceedings of the Workshop on the Eval-uation of Parsing Systems, May.Rada Mihalcea and Ted Pedersen.
2003.
An evaluationexercise for word alignment.
In Proceedings of theWorkshop on Building and Using Parallel Texts: DataDriven Machine Translation and Beyond, pages 1?10,May.Robert C. Moore.
2004.
Improving IBM word-alignment model 1.
In ACL Proceedings, pages 519?526, Jul.Franz Josef Och and Hermann Ney.
2000a.
A compari-son of alignment models for statistical machine trans-lation.
In COLING Proceedings, pages 1086?1090,Jul.Franz Josef Och and Hermann Ney.
2000b.
Improvedstatistical alignment models.
In ACL Proceedings,pages 440?447, Oct.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison on various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Franz Josef Och.
1999.
An efficient method for deter-mining bilingual word classes.
In EACL Proceedings,pages 71?76, Jun.Kristina Toutanova, H. Tolga Ilhan, and Christopher D.Manning.
2002.
Extensions to hmm-based statisticalword alignment models.
In EMNLP, pages 87?94, Jul.Stephan Vogel, Hermann Ney, and Christoph Tillman.1996.
Hmm-based word alignment in statistical ma-chine translation.
In COLING Proceedings, pages836?841, Aug.Dekai Wu.
1995.
Stochastic inversion transductiongrammars, with application to segmentation, bracket-ing, and alignment of parallel corpora.
In Proceedingsof the 14th International Joint Conference on ArtificialIntelligence, pages 1328?1335, Aug.Kenji Yamada and Kevin Knight.
2001.
A syntax-basedstatistical translation model.
In ACL Proceedings.86
