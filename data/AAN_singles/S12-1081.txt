First Joint Conference on Lexical and Computational Semantics (*SEM), pages 557?564,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsDSS: Text Similarity Using Lexical Alignments of Form, DistributionalSemantics and Grammatical RelationsDiana McCarthySaarland University?diana@dianamccarthy.co.ukSpandana GellaUniversity of Maltaspandanagella@gmail.comSiva ReddyLexical Computing Ltd.siva@sivareddy.inAbstractIn this paper we present our systems for theSTS task.
Our systems are all based on asimple process of identifying the componentsthat correspond between two sentences.
Cur-rently we use words (that is word forms), lem-mas, distributional similar words and gram-matical relations identified with a dependencyparser.
We submitted three systems.
All sys-tems only use open class words.
Our first sys-tem (alignheuristic) tries to obtain a map-ping between every open class token using allthe above sources of information.
Our secondsystem (wordsim) uses a different algorithmand unlike alignheuristic, it does not usethe dependency information.
The third sys-tem (average) simply takes the average ofthe scores for each item from the other twosystems to take advantage of the merits ofboth systems.
For this reason we only pro-vide a brief description of that.
The resultsare promising, with Pearson?s coefficients oneach individual dataset ranging from .3765to .7761 for our relatively simple heuristicsbased systems that do not require training ondifferent datasets.
We provide some analy-sis of the results and also provide results forour data using Spearman?s, which as a non-parametric measure which we argue is betterable to reflect the merits of the different sys-tems (average is ranked between the others).1 IntroductionOur motivation for the systems entered in the STStask (Agirre et al, 2012) was to model the contribu-?
The first author is a visiting scholar on the ErasmusMundus Masters Program in ?Language and CommunicationTechnologies?
(LCT, 2007?0060).tion of each linguistic component of both sentencesto the similarity of the texts by finding an align-ment.
Ultimately such a system could be exploitedfor ranking candidate paraphrases of a chunk of textof any length.
We envisage a system as outlined inthe future work section.
The systems reported aresimple baselines to such a system.
We have twomain systems (alignheuristic and wordsim) andalso a system which simply uses the average scorefor each item from the two main systems (average).In our systems we:?
only deal with open class words as to-kens i.e.
nouns, verbs, adjectives, adverbs.alignheuristic and average also use num-bers?
assume that tokens have a 1:1 mapping?
match:?
word forms?
lemmas?
distributionally similar lemmas?
(alignheuristic and average only) ar-gument or head in a matched grammaticalrelation with a word that already has a lex-ical mapping?
score the sentence pair based on the size of theoverlap.
Different formulations of the score areused by our methodsThe paper is structured as follows.
In the nextsection we make a brief mention of related workthough of course there will be more pertinent relatedwork presented and published at SemEval 2012.
Insection 3 we give a detailed account of the systems557and in section 4 we provide the results obtained onthe training data on developing our systems.
In sec-tion 5 we present the results on the test data, alongwith a little analysis using the gold standard data.
Insection 6 we conclude our findings and discuss ourideas for future work.2 Related WorkSemantic textual similarity relates to textual entail-ment (Dagan et al, 2005), lexical substitution (Mc-Carthy and Navigli, 2009) and paraphrasing (Hirst,2003).
The key issue for semantic textual similarityis that the task is to determine similarity, where sim-ilarity is cast as meaning equivalence.
1 In textualentailment the relation under question is the morespecific relation of entailment, where the meaningof one sentence is entailed by another and a sys-tem needs to determine the direction of the entail-ment.
Lexical substitution relates to semantic tex-tual similarity though the task involves a lemma inthe context of a sentence, candidate substitutes arenot provided, and the relation at question in the taskis one of substitutability.
2 Paraphrase recognitionis a highly related task, for example using compa-rable corpora (Barzilay and Elhadad, 2003), and itis likely that semantic textual similarity measuresmight be useful for ranking candidates in paraphraseacquisition.In addition to various works related to textualentailment, lexical substitution and paraphrasing,there has been some prior work explicitly on se-mantic text similarity.
Semantic textual similarityhas been explored in various works.
Mihalcea et al(2006) extend earlier work on word similarity us-ing various WordNet similarity measures (Patward-han et al, 2003) and a couple of corpus-based dis-tributional measures: PMI-IR (Turney, 2002) andLSA (Berry, 1992).
They use a measure whichtakes a summation over all tokens in both sen-tences.
For each token they find the maximum sim-ilarity (WordNet or distributional) weighted by theinverse document frequency of that word.
The dis-1See the guidelines given to the annotators athttp://www.cs.columbia.edu/?weiwei/workshop/instructions.pdf2This is more or less semantic equivalence since the an-notators were instructed to focus on meaning http://www.dianamccarthy.co.uk/files/instructions.pdf.tributional similarity measures perform at a simi-lar level to the knowledge-based measures that useWordNet.
Mohler and Mihalcea (2009) adapt thiswork for automatic short answer grading, that ismatching a candidate answer to one supplied bythe tutor.
Mohler et al (2011) take this applica-tion forward, combining lexical semantic similaritymeasures with a graph-alignment which considersdependency graphs using the Stanford dependencyparser (de Marneffe et al, 2006) in terms of lexical,semantic and syntactic features.
A score is then pro-vided for each node in the graph.
The features arecombined using machine learning.The systems we propose likewise use lexical sim-ilarity and dependency relations, but in a simpleheuristic formulation without a man-made thesaurussuch as WordNet and without machine learning.3 SystemsWe lemmatize and part-of-speech tag the data usingTreeTagger (Schmid, 1994).
We process the taggeddata with default settings of the Malt Parser (Nivreet al, 2007) to dependency parse the data.
All sys-tems make use of a distributional thesaurus whichlists distributionally similar lemmas (?neighbours?
)for a given lemma.
This is a thesaurus constructedusing log-dice (Rychly?, 2008) and UkWaC (Fer-raresi et al, 2008).
3 Note that we use only thetop 20 neighbours for any word in all the methodsdescribed below.
We have not experimented withvarying this threshold.In the following descriptions, we refer to our sen-tences as s1 and s2 and these open classed tokenswithin those sentences as ti ?
s1 and t j ?
s2 whereeach token in either sentence is represented by aword (w), lemma (l), part-of-speech (p) and gram-matical relation (gr), identified by the Malt parser,to its dependency head at a given position (hp) inthe same sentence.3.1 alignheuristicThis method uses nouns, verbs, adjectives, adverbsand numbers.
The algorithm aligns words (w), orlemmas (l) from left to right from s1 to s2 and vice3This is the ukWaC distributional thesaurus avail-able in Sketch Engine (Kilgarriff et al, 2004) athttp://the.sketchengine.co.uk/bonito/run.cgi/first\_form?corpname=preloaded/ukwac2558versa (wmtch).
If there is no alignment for words orlemmas then it does the same matching process (s1given s2 and vice versa) for distributionally similarneighbours using the distributional thesaurus men-tioned above (tmtch) and also another matching pro-cess looking for a corresponding grammatical rela-tion identified with the Malt parser in the other sen-tence where the head (or argument) already has amatch in both sentences (rmtch).A fuller and more formal description of the algo-rithm follows:1. retain nouns, verbs (not be), adjectives, adverbsand numbers in both sentences s1 and s2.2.
wmtch:(a) look for word matches?
wi ?
s1 to w j ?
s2, left to right i.e.
thefirst matching w j ?
s2 is selected as amatch for wi.?
w j ?
s2 to wi ?
s1, left to right i.e.
thefirst matching wi ?
s1 is selected as amatch for w j(b) and then lemma matches for any ti ?
s1and t j ?
s1 not matched in steps 2a?
li ?
s1 to l j ?
s2 , left to right i.e.
thefirst matching l j ?
s2 is selected as amatch for li.?
l j ?
s2 to li ?
s1 , left to right i.e.
thefirst matching li ?
s1 is selected as amatch for l j3.
using only ti ?
s1 and t j ?
s2 not matched inthe above steps:?
tmtch: match lemma and PoS (l + p) withthe distributional thesaurus against the top20 most similar lemma-pos entries.
Thatis:(a) For l + pi ?
s1, if not already matchedat step 2 above, find the most similarwords in the thesaurus, and match ifone of these is in l + p j ?
s2, left toright i.e.
the first matching l + p j ?
s2to any of the most similar words tol + pi according to the thesaurus is se-lected as a match for l + pi ?
s1.
(b) For l + p j ?
s2, if not already matchedat step 2 above, find the most similarwords in the thesaurus, and match ifone of these is in l + pi ?
s1, left toright?
rmtch: match the tokens, if not alreadymatched at step 2 above, by looking fora head or argument relation with a tokenthat has been matched at step 2 to a tokenwith the inverse relation.
That is:i For ti ?
s1, if not already matched atstep 2 above, if hpi ?
s1 (the pointerto the head, i.e.
parent, of ti) refers toa token tx ?
s1 which has wmtch at tkin s2, and there exists a tq ?
s2 withgrq = gri and hpq = tk, then match tiwith tqii For ti ?
s1 , if not already matchedat step 2 or the preceding step (rmtch3i) and if there exists another tx ?
s1with a hpx which refers to ti (i.e.
ti isthe parent, or head, of tx) with a matchbetween tx and tk ?
s2 from step 2, 4and where tk has grk = grx with hpkwhich refers to tq in s2, then match tiwith tq 5iii we do likewise in reverse for s2 to s1and then check all matches are recip-rocated with the same 1:1 mappingFinally, we calculate the score sim(s1, s2):|wmtch| + (wt ?
|tmtch + rmtch|)|s1| + |s2|?
5 (1)where wt is a weight of 0.5 or less (see below).The sim score gives a score of 5 where twosentences have the same open class tokens, sincematches in both directions are included and the de-nominator is the number of open class tokens in bothsentences.
The score is 0 if there are no matches.The thesaurus and grammatical relation matches arecounted equally and are considered less important4In the example illustrated in figure 1 and discussed below,ti could be rose in the upper sentence (s1) and Nasdaq would betx and tk.5So in our example, from figure 1, ti (rose) is matched with tq(climb) as climb is the counterpart head to rose for the matchedarguments (Nasdaq).559NasdaqThe tech?loaded composite rose 20.96 points to 1595.91, ending at its highest level for 12 months.thesaurusmaltmaltpoints, or 1.2 percent, to 1,615.02.The technology?laced climbed 19.11 Index <.IXIC>CompositeNasdaqFigure 1: Example of matching with alignheuristicfor the score as the exact matches.
We set wt to 0.4for the official run, though we could improve perfor-mance by perhaps setting a bit lower as shown belowin section 4.1.Figure 1 shows an example pair of sentences fromthe training data in MSRpar.
The solid lines showalignments between words.
Composite and compos-ite are not matched because the lemmatizer assumesthat the former is a proper noun and does not decap-italise; we could decapitalise all proper nouns.
Thedotted arcs show parallel dependency relations in thesentences where the argument (Nasdaq) is matchedby wmtch.
The rmtch process therefore assumes thecorresponding heads (rise and climb) align.
In addi-tion, tmtch finds a match from climb to rise as rise isin the top 20 most similar words (neighbours) in thedistributional thesaurus.
climb is not however in thetop 20 for rise and so a link is not found in the otherdirection.
We have not yet experimented with val-idating the thesaurus and grammatical relation pro-cesses together, though that would be worthwhile infuture.3.2 wordsimIn this method, we first choose the shortest sentencebased on the number of open words.
Let s1 and s2be the shortest and longest sentences respectively.For every lemma li ?
s1, we find its best matchinglemma l j ?
s2 using the following heuristics andassigning an alignment score as follows:1. if li=l j, then the alignment score of li(algnscr(li)) is one.2.
else li ?
s1 is matched with a lemma l j ?
s2with which it has the highest distributional sim-ilarity.
6 The alignment score, algnscr(li) isthe distributional similarity between li and l j(which is always less than one).The final sentence similarity score between thepair s1 and s2 is computed assim(s1, s2) =?li?s1 algnscr(li)|s1|(2)3.3 averageThis system simple uses the average score for eachitem from alignheuristic and wordsim.
This issimply so we can make a compromise between themerits of the two systems.4 Experiments on the Training DataTable 1 displays the results on training data for thesystem settings as they were for the final test run.
Weconducted further analysis for the alignheuristicsystem and that is reported in the following subsec-tion.
We can see that while the alignheuristicis better on the MSRpar and SMT-eur datasets, thewordsim outperforms it on the MSRvid dataset,which contains shorter, simpler sentences.
One rea-son might be that the wordsim credits alignmentsin one direction only and this works well when sen-tences are of a similar length but can loose out on thelonger paraphrase and SMT data.
This behaviour is6Provided this is within the top 20 most similar words in thethesaurus.560MSRpar MSRvid SMT-euralignheuristic 0.6015 0.6994 0.5222wordsim 0.4134 0.7658 0.4479average 0.5337 0.7535 0.5061Table 1: Results on training dataconfirmed by the results on the test data reported be-low in section 5, though we cannot rule out that otherfactors play a part.4.1 alignheuristicWe developed the system on the training data for thepurpose of finding bugs, and setting the weight inequation 1.
During development we found the opti-mal weight for wt to be 0.4.
7 Unfortunately we didnot leave ourselves sufficient time to set the weightsafter resolving the bugs.
In table 1 we report theresults on the training data for the system that weuploaded, however in table 2 we report more recentresults for the final system but with different valuesof wt.
From table 2 it seems that results may havebeen improved if we had determined the final valueof wt after debugging our system fully, however thisdepends on the type of data as 0.4 was optimal forthe datasets with more complex sentences (MSRparand SMT-eur).In table 3, we report results for alignheuristicwith and without the distributional similaritythesaurus (tmtch) and the dependency relations(rmtch).
In table 4 we show the total number of to-ken alignments made by the different matching pro-cesses on the training data.
We see, from table 4that the MSRvid data relies on the thesaurus and de-pendency relations to a far greater extent than theother datasets, presumably because of the shortersentences where many have a few contrasting wordsin similar syntactic relations, for example s1 Some-one is drawing.
s2 Someone is dancing.
8 We seefrom table 3 that the use of these matching processesis less accurate for MSRvid and that while tmtchimproves performance, rmtch seems to degrade per-formance.
From table 2 it would seem that on thistype of data we would get the best results by reduc-7We have not yet attempted setting the weight on alignmentby relation and alignment by distributional similarity separately.8Note that the alignheuristic algorithm is symmetricalwith respect to s1 and s2 so it does not matter which is which.wt MSRpar MSRvid SMT-eur0.5 0.5998 0.6518 0.52900.4 0.6015 0.6994 0.52220.3 0.6020 0.7352 0.51460.2 0.6016 0.7577 0.50590.1 0.6003 0.7673 0.49640 0.5981 0.7661 0.4863Table 2: Results for the alignheuristic algorithm onthe training data: varying wtMSR MSR SMTpar vid -eur-tmtch+rmtch 0.6008 0.7245 0.5129+tmtch-rmtch 0.5989 0.7699 0.4959-tmtch-rmtch 0.5981 0.7661 0.4863+tmtch+rmtch 0.6015 0.6994 0.5222Table 3: Results for the alignheuristic algorithm onthe training data: with and without tmtch and rmtching wt to a minimum, and perhaps it would makesense to drop rmtch.
Meanwhile, on the longer morecomplex MSRpar and SMT-eur data, the less precisermtch and tmtch are used less frequently (relative tothe wmtch) but can be seen from table 3 to improveperformance on both training datasets.
Moreover, aswe mention above, from table 2 the parameter set-ting of 0.4 used for our final test run was optimal forthese datasets.MSRpar MSRvid SMT-eurwmtch 10960 2349 12155tmtch 378 1221 964rmtch 1008 965 1755Table 4: Number of token alignments for the differentmatching processes561run ALL MSRpar MSRvid SMT-eur On-WN SMT-newsalignheuristic .5253 (60) .5735 (24) .7123 (53) .4781 (25) .6984 (7) .4177 (38)average .5490 (58) .5020 (48) .7645 (41) .4875 (16) .6677(14) .4324 (31)wordsim .5130 (61) .3765 (75) .7761 (34) .4161 (58) .5728 (59) .3964 (48)Table 5: Official results: Rank (out of 89) is shown in bracketsrun ALL MSRpar MSRvid SMT-eur On-WN SMT-news average ?alignheuristic 0.5216 0.5539 0.7125 0.5404 0.6928 0.3655 0.5645average 0.5087 0.4818 0.7653 0.5415 0.6302 0.3835 0.5518wordsim 0.4279 0.3608 0.7799 0.4487 0.4976 0.3388 0.4756Table 7: Spearman?s ?
for the 5 datasets, ?all?
and the average coefficient across the datasetsrun mean Allnrmalignheuristic 0.6030 (21) 0.7962 (42)average 0.5943 (26) 0.8047 (35)wordsim 0.5287 (55) 0.7895 (49)Table 6: Official results: Further metrics suggested in dis-cussion.
Rank (out of 89) is shown in brackets5 ResultsTable 5 provides the official results for our submittedsystems, along with the rank on each dataset.
The re-sults in the ?all?
column which combine all datasetstogether are at odds with our intuitions.
Our sys-tems were ranked higher in every individual datasetcompared to the ?all?
ranking, with the exception ofwordsim and the MSRpar dataset.
This ?all?
met-ric is anticipated to impact systems that have dif-ferent settings for different types of data howeverwe did not train our systems to run differently ondifferent data.
We used exactly the same parame-ter settings for each system on every dataset.
Webelieve Pearson?s measure has a significant impacton results because it is a parametric measure andas such the shape of the distribution (the distribu-tion of scores) is assumed to be normal.
We presentthe results in table 6 from new metrics proposed byparticipants during the post-results discussion: All-nrm (normalised) and mean (this score is weightedby the number of sentence pairs in each dataset).
9The Allnrm score, proposed by a participant duringthe discussion phase to try and combat issues with9Post-results discussion is archived at http://groups.google.com/group/sts-semeval/topicsthe ?all?
score, also does not accord with our intu-ition given the ranks of our systems on the individ-ual datasets.
The mean score, proposed by anotherparticipant, however does reflect performance on theindividual datasets.
Our average system is rankedbetween alignheuristic and wordsim which isin line with our expectations given results on thetraining data and individual datasets.As mentioned above, an issue with the use ofPearson?s correlation coefficient is that it is para-metric and assumes that the scores are normally dis-tributed.
We calculated Spearman?s ?
which is thenon-parametric equivalent of Pearson?s and uses theranks of the scores, rather than the actual scores.
10The results are presented in table 7.
We cannot cal-culate the results for other systems, and therefore theranks for our system, since we do not have the othersystem?s outputs but we do see that the relative per-formance of our system on ?all?
is more in line withour expectations: average, which simply uses theaverage of the other two systems for each item, isusually ranked between the other two systems, de-pending on the dataset.
Spearman?s ?all?
gives a sim-ilar ranking of our three systems as the mean score.We also show average ?.
This is a macro averageof the Spearman?s value for the 5 datasets withoutweighting by the number of sentence pairs.
1110Note that Spearman?s ?
is often a little lower than Pear-son?s (Mitchell and Lapata, 2008)11We do recognise the difficulty in determining metrics on anew pilot study.
The task organisers are making every effort tomake it clear that this enterprise is a pilot, not a competition andthat they welcome feedback.5626 ConclusionsThe systems were developed in less than a weekincluding the time with the test data.
There aremany trivial fixes that may improve the basic algo-rithm, such as decapitalising proper nouns.
Thereare many things we would like to try, such as val-idating the dependency matching process with thethesaurus matching.
We would like to match largerunits rather than tokens, with preferences towardsthe longer matching blocks.
In parallel to the devel-opment of alignheuristic, we developed a sys-tem which measures the similarity between a nodein the dependency tree of s1 and a node in the de-pendency tree of s2 as the sum of the lexical sim-ilarity of the lemmas at the nodes and the simi-larity of its children nodes.
We did not submit arun for the system as it did not perform as well asalignheuristic, probably because the score fo-cused on structure too much.
We hope to spend timedeveloping this system in future.Ultimately, we envisage a system that:?
can have non 1:1 mappings between tokens, i.e.a phrase may be paraphrased as a word for ex-ample blow up may be paraphrased as explode?
can map between sequences of non-contiguouswords for example the words in the phrase blowup may be separated but mapped to the wordexplode as in the bomb exploded ?
They blewit up?
has categories (such as equivalence, entailment,negation, omission .
.
. )
associated with eachmapping.
Speculation, modality and sentimentshould be indicated on any relevant chunk sothat differences can be detected between candi-date and referent?
scores the candidate using a function of thescores of the mapped units (as in the systemsdescribed above) but alters the score to reflectthe category as well as the source of the map-ping, for example entailment without equiva-lence should reduce the similarity score, in con-trast to equivalence, and negation should re-duce this still furtherCrucially we would welcome a task where anno-tators would also provide a score on sub chunks ofthe sentences (or arbitrary blocks of text) that alignalong with a category for the mapping (equivalence,entailment, negation etc..).
This would allow us tolook under the hood at the text similarity task and de-termine the reason behind the similarity judgments.7 AcknowledgementsWe thank the task organisers for their efforts in or-ganising the task and their readiness to take on boarddiscussions on this as a pilot.
We also thank theSemEval-2012 co-ordinators.ReferencesAgirre, E., Cer, D., Diab, M., and Gonzalez-Agirre,A.
(2012).
Semeval-2012 task 6: A pilot on se-mantic textual similarity.
In Proceedings of the6th International Workshop on Semantic Evalua-tion (SemEval 2012), in conjunction with the FirstJoint Conference on Lexical and ComputationalSemantics (*SEM 2012).Barzilay, R. and Elhadad, N. (2003).
Sentence align-ment for monolingual comparable corpora.
InCollins, M. and Steedman, M., editors, Proceed-ings of the 2003 Conference on Empirical Meth-ods in Natural Language Processing, pages 25?32.Berry, M. (1992).
Large scale singular value compu-tations.
International Journal of SupercomputerApplications, 6(1):13?49.Dagan, I., Glickman, O., and Magnini, B.
(2005).The pascal recognising textual entailment chal-lenge.
In Proceedings of the PASCAL First Chal-lenge Workshop, pages 1?8, Southampton, UK.de Marneffe, M.-C., MacCartney, B., and Man-ning, C. D. (2006).
Generating typed dependencyparses from phrase structure parses.
In To appearat LREC-06.Ferraresi, A., Zanchetta, E., Baroni, M., and Bernar-dini, S. (2008).
Introducing and evaluatingukwac, a very large web-derived corpus of en-glish.
In Proceedings of the Sixth InternationalConference on Language Resources and Evalua-tion (LREC 2008), Marrakech, Morocco.Hirst, G. (2003).
Paraphrasing paraphrased.
In-vited talk at the Second International Workshop563on Paraphrasing, 41st Annual Meeting of the As-sociation for Computational Linguistics.Kilgarriff, A., Rychly?, P., Smrz, P., and Tugwell, D.(2004).
The sketch engine.
In Proceedings of Eu-ralex, pages 105?116, Lorient, France.
Reprintedin Patrick Hanks (ed.).
2007.
Lexicology: Criticalconcepts in Linguistics.
London: Routledge.McCarthy, D. and Navigli, R. (2009).
The Englishlexical substitution task.
Language Resources andEvaluation Special Issue on Computational Se-mantic Analysis of Language: SemEval-2007 andBeyond, 43(2):139?159.Mihalcea, R., Corley, C., and Strapparava, C.(2006).
Corpus-based and knowledge-based mea-sures of text semantic similarity.
In Proceedingsof the American Association for Artificial Intelli-gence (AAAI 2006), Boston, MA.Mitchell, J. and Lapata, M. (2008).
Vector-basedmodels of semantic composition.
In Proceed-ings of ACL-08: HLT, pages 236?244, Columbus,Ohio.
Association for Computational Linguistics.Mohler, M., Bunescu, R., and Mihalcea, R. (2011).Learning to grade short answer questions us-ing semantic similarity measures and dependencygraph alignments.
In Proceedings of the 49thAnnual Meeting of the Association for Compu-tational Linguistics: Human Language Technolo-gies, pages 752?762, Portland, Oregon, USA.
As-sociation for Computational Linguistics.Mohler, M. and Mihalcea, R. (2009).
Text-to-text se-mantic similarity for automatic short answer grad-ing.
In Proceedings of the 12th Conference ofthe European Chapter of the ACL (EACL 2009),pages 567?575, Athens, Greece.
Association forComputational Linguistics.Nivre, J., Hall, J., Nilsson, J., Chanev, A., Eryigit,G., Ku?bler, S., Marinov, S., and Marsi, E. (2007).Maltparser: A language-independent system fordata-driven dependency parsing.
Natural Lan-guage Engineering, 13(2):95?135.Patwardhan, S., Banerjee, S., and Pedersen, T.(2003).
Using measures of semantic relatednessfor word sense disambiguation.
In Proceedingsof the Fourth International Conference on Intelli-gent Text Processing and Computational Linguis-tics (CICLing 2003), Mexico City.Rychly?, P. (2008).
A lexicographer-friendly associ-ation score.
In Proceedings of 2nd Workshop onRecent Advances in Slavonic Natural LanguagesProcessing, RASLAN 2008, Brno.Schmid, H. (1994).
Probabilistic Part-of-SpeechTagging Using Decision Trees.
In Proceedings ofthe International Conference on New Methods inLanguage Processing, pages 44?49, Manchester,UK.Turney, P. D. (2002).
Mining the web for synonyms:Pmi-ir versus lsa on toefl.
CoRR, cs.LG/0212033.564
