Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),pages 237?240, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsThe Integration of Syntactic Parsing and Semantic Role LabelingSzu-ting YiUniversity of Pennsylvania3330 Walnut StreetPhiladelphia, PA 19104 USAszuting@linc.cis.upenn.eduMartha PalmerUniversity of Pennsylvania3330 Walnut StreetPhiladelphia, PA 19104 USAmpalmer@linc.cis.upenn.eduAbstractThis paper describes a system for theCoNLL-2005 Shared Task on SemanticRole Labeling.
We trained two parserswith the training corpus in which the se-mantic argument information is attachedto the constituent labels, we then used theresulting parse trees as the input of thepipelined SRL system.
We present our re-sults of combining the output of variousSRL systems using different parsers.1 IntroductionSemantic parsing, identifying and classifying the se-mantic entities in context and the relations betweenthem, potentially has great impact on its downstreamapplications, such as text summarization, questionanswering, and machine translation.
As a result, se-mantic parsing could be an important intermediatestep for natural language comprehension.
In this pa-per, we investigate the task of Semantic Role Label-ing (SRL): Given a verb in a sentence, the goal is tolocate the constituents which are arguments of theverb, and assign them appropriate semantic roles,such as, Agent, Patient, and Theme.Previous SRL systems have explored the effectsof using different lexical features, and experimentedon different machine learning algorithms.
(Gildeaand Palmer, 2002; Pradhan et al, 2005; Punyakanoket al, 2004) However, these SRL systems generallyextract features from sentences processed by a syn-tactic parser or other shallow parsing components,such as a chunker and a clause identifier.
As a result,the performance of the SRL systems relies heavilyon those syntax-analysis tools.In order to improve the fundamental performanceof an SRL system, we trained parsers with trainingdata containing not only syntactic constituent infor-mation but also semantic argument information.
Thenew parsers generate more correct constituents thanthat trained on pure syntactic information.
Becausethe new parser generate different constituents than apure syntactic parser, we also explore the possibilityof combining the output of several parsers with thehelp of a voting post-processing component.This paper is organized as follows: Section 2demonstrates the components of our SRL system.We elaborate the importance of training a new parserand outline our approach in Section 3 and Section 4.Finally, Section 5 reports and discusses the results.2 Semantic Role Labeling: theArchitectureOur SRL system has 5 phases: Parsing, Pruning, Ar-gument Identification, Argument Classification, andPost Processing.
The Argument Identification andClassification components are trained with Sec 02-21 of the Penn Treebank corpus.2.1 ParsingPrevious SRL systems usually use a pure syntacticparser, such as (Charniak, 2000; Collins, 1999), toretrieve possible constituents.
Once the boundary ofa constituent is defined, there is no way to changeit in later phases.
Therefore the quality of the syn-tactic parser has a major impact on the final per-237formance of an SRL system, and the percentage ofcorrect constituents that is generated by the syntac-tic parser also defines the recall upper bound of anSRL system.
In order to attack this problem, in addi-tion to Charniak?s parser (Charniak, 2000), our sys-tem combine two parser which are trained on bothsyntactic constituent information and semantic argu-ment information.
(See Section 3)2.2 PruningGiven a parse tree, a pruning component filters outthe constituents which are unlikely to be semanticarguments in order to facilitate the training of the Ar-gument Identification component.
Our system usesthe heuristic rules introduced by (Xue and Palmer,2004).
The heuristics first spot the verb and then ex-tract all the sister nodes along the verb spine of theparse tree.
We expand the coverage by also extract-ing all the immediate children of an S, ADVP, PPand NP node.
This stage generally prunes off about80% of the constituents given by a parser.
For ournewly trained parsers, we also extract constituentswhich have a secondary constituent label indicatingthe constituent in question is an argument.2.3 Argument Identification and ClassificationWe have as our Argument Identification componenta binary maximum-entropy classifier to determinewhether a constituent is an argument or not.
Ifa constituent is tagged as an argument, the Argu-ment Classification component, which is a multi-class maximum-entropy classifier, would assign ita semantic role.
The implementation of both theArgument Identification and Classification compo-nents makes use of the Mallet package1.The lexical features we use to train these twocomponents are taken from (Xue and Palmer, 2004).We trained the Argument Identification compo-nent with the following single features: the pathfrom the constituent to the verb, the head word ofthe constituent and its POS tag, and the distancebetween the verb and the constituent, and featurecombinations: the verb and the phrasal type of theconstituent, the verb and the head word of the con-stituent.
If the parent node of the constituent is a PPnode, then we also include the head word of the PP1http://mallet.cs.umass.edunode and the feature combination of the verb and thehead word of the PP node.In addition to the features listed above, the Ar-gument Classification component also contains thefollowing features: the verb, the first and the lastcontent word of the constituent, the phrasal typeof the left sibling and the parent node, voice (pas-sive or active), position of the constituent relative tothe verb, the subcategorization frame, and the syn-tactic frame which describes the sequential patternof the noun phrases and the verb in the sentence.2.4 Post ProcessingThe post processing component merges adjacent dis-continuous arguments and marks the R-argumentsbased on the content word and phrase type of the ar-gument.
Also it filters out arguments according tothe following constraints:1.
There are no overlapping arguments.2.
There are no repeating core arguments.In order to combine the different systems, we alsoinclude a voting scheme.
The algorithm is straight-forward: Suppose there are N participating systems,we pick arguments with N votes, N-1 votes ..., andfinally 1 vote.
The way to break a tie is based onthe confidence level of the argument given by thesystem.
Whenever we pick an argument, we needto check whether this argument conflicts with pre-viously selected arguments based on the constraintsdescribed above.3 Training a Parser with SemanticArgument InformationA good start is always important, especially for asuccessful SRL system.
Instead of passively accept-ing candidate constituents from the upstream syn-tactic parser, an SRL system needs to interact withthe parser in order to obtain improved performance.This motivated our first attempt which is to integratesyntactic parsing and semantic parsing as a singlestep, and hopefully as a result we would be able todiscard the SRL pipeline.
The idea is to augmentthe Penn Treebank (Marcus et al, 1994) constituentlabels with the semantic role labels from the Prop-Bank (Palmer et al, 2005), and generate a rich train-ing corpus.
For example, if an NP is also an ar-238gument ARG0 of a verb in the given sentence, wechange the constituent label NP into NP-ARG0.
Aparser therefore is trained on this new corpus andshould be able to serve as an SRL system at the sametime as predicting a parse.However, this ideal approach is not feasible.Given the fact that there are many different semanticrole labels and the same constituent can be differentarguments of different verbs in the same sentence,the number of constituent labels will soon grow outof control and make the parser training computation-ally infeasible.
Not to mention that anchor verb in-formation has not yet been added to the constituentlabel, and general data sparseness.
As a compro-mise, we decided to integrate only Argument Iden-tification with syntactic parsing.
We generated thetraining corpus by simply marking the constituentswhich are also semantic arguments.4 Parsing ExperimentsWe trained a maximum-entropy parser basedon (Ratnaparkhi, 1999) using the OpenNLP pack-age 2.
We started our experiments with this specificparsing implementation because of its excellent flex-ibility that allows us to test different features.
Be-sides, this parser contains four clear parse tree build-ing stages: TAG, CHUNK, BUILD, and CHECK.This parsing structure offers us an isolated workingenvironment for each stage that helps us confine nec-essary implementation modifications and trace downimplementation errors.4.1 Data PreparationFollowing standard practice, we use Sec 02-21 ofthe Penn Treebank and the PropBank as our trainingcorpus.
The constituent labels defined in the PennTreebank consist of a primary label and several sec-ondary labels.
A primary label represents the majorsyntactic function carried by the constituent, for in-stance, NP indicates a noun phrase and PP indicatesa prepositional phrase.
A secondary label, startingwith ?-?, represents either a grammatical function ofa constituent or a semantic function of an adjunct.For example, NP-SBJ means the noun phrase is asurface subject of the sentence; PP-LOC means theprepositional phrase is a location.
Although the sec-2http://sourceforge.net/projects/opennlp/ondary labels give us much to encourage informa-tion, because of data sparseness problem and train-ing efficiency, we stripped off all the secondary la-bels from the Penn Treebank.After stripping off the secondary labels from thePenn Treebank, we augment the constituent labelswith the semantic argument information from thePropBank.
We adopted four different labels, -AN,-ANC, -AM, and -AMC.
If the constituent in thePenn Treebank is a core argument, which meansthe constituent has one of the labels of ARG0-5 andARGA in the PropBank, we attach -AN to the con-stituent label.
The label -ANC means the constituentis a discontinuous core argument.
Similarly, -AMindicates an adjunct-like argument, ARGM, and -AMC indicates a discontinuous ARM.For example, the sentence from Sec 02, [ARG0The luxury auto maker] [ARGM-TMP last year]sold [ARG1 1,214 cars] [ARGM-LOC in the U.S.],would appear in the following format in our train-ing corpus: (S (NP-AN (DT The) (NN luxury) (NNauto) (NN maker) ) (NP-AM (JJ last) (NN year) )(VP (VBD sold) (NP-AN (CD 1,214) (NNS cars) )(PP -AM (IN in) (NP (DT the) (NNP U.S.) ) ) ) )4.2 The 2 Different ParsersSince the core arguments and the ARGMs in thePropBank loosely correspond to the complementsand adjuncts in the linguistics literature, we are in-terested in investigating their individual effect onparsing performance.
We trained two parsers.
AnAN-parser was trained on the Penn Treebank cor-pus augmented with two semantic argument labels:-AN, and -ANC.
Another AM-parser was trained onlabels -AM, and -AMC.5 Results and DiscussionTable 1 shows the results after combining variousSRL systems using different parsers.
In order to ex-plore the effects of combining, we include the over-all performance on the development dataset of indi-vidual SRL systems in Table 2.The performance of Semantic Role Labeling(SRL) is determined by the quality of the syntacticinformation provided to the system.
In this paper,we investigate that for the SRL task whether it ismore suitable to use a parser trained with data con-239Precision Recall F  Development 75.70% 69.99% 72.73Test WSJ 77.51% 72.97% 75.17Test Brown 67.88% 59.03% 63.14Test WSJ+Brown 76.31% 71.10% 73.61Test WSJ Precision Recall F  Overall 77.51% 72.97% 75.17A0 85.14% 77.32% 81.04A1 77.61% 75.16% 76.37A2 68.18% 62.16% 65.03A3 66.91% 52.60% 58.90A4 77.08% 72.55% 74.75A5 100.00% 40.00% 57.14AM-ADV 59.73% 51.58% 55.36AM-CAU 67.86% 52.05% 58.91AM-DIR 65.67% 51.76% 57.89AM-DIS 80.39% 76.88% 78.59AM-EXT 78.95% 46.88% 58.82AM-LOC 57.43% 55.37% 56.38AM-MNR 54.37% 56.10% 55.22AM-MOD 96.64% 94.01% 95.31AM-NEG 96.88% 94.35% 95.59AM-PNC 41.38% 41.74% 41.56AM-PRD 50.00% 20.00% 28.57AM-REC 0.00% 0.00% 0.00AM-TMP 77.13% 74.15% 75.61R-A0 86.82% 85.27% 86.04R-A1 67.72% 82.05% 74.20R-A2 46.15% 37.50% 41.38R-A3 0.00% 0.00% 0.00R-A4 0.00% 0.00% 0.00R-AM-ADV 0.00% 0.00% 0.00R-AM-CAU 0.00% 0.00% 0.00R-AM-EXT 0.00% 0.00% 0.00R-AM-LOC 100.00% 42.86% 60.00R-AM-MNR 33.33% 33.33% 33.33R-AM-TMP 78.57% 63.46% 70.21R-C-A1 0.00% 0.00% 0.00V 97.35% 95.54% 96.44Table 1: Overall results (top) and detailed results onthe WSJ test (bottom).taining both syntactic bracketing and semantic ar-gument boundary information than a pure syntacticone.The results of the SRL systems using the AM-or AN- parsers are not significantly better than thatusing the Charniak?s parser.
This might due to thesimple training mechanism of the base parsing al-gorithm which the AM- and AN- parsers exploit.
Italso suggests our future work to apply the approachto more sophisticated parsing frameworks.
By then,We show that we can boost the final performanceby combining different SRL systems using differentparsers, given that the combination algorithm is ca-Precision Recall F  AN-parser 71.31% 63.68% 67.28AM-parser 74.09% 65.11% 69.31Charniak 76.31% 64.62% 69.98All 3 combined 75.70% 69.99% 72.73Table 2: Overall results on the development set ofindividual SRL systems.pable of maintaining the quality of the final argu-ments.6 AcknowledgmentsWe thank Tom Morton for providing detailed expla-nation for any of our parsing related inquiries.ReferencesEugene Charniak.
2000.
A Maximum-Entropy-InspiredParser.
In Proceedings of NAACL-2000.Michael Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
PhD Dissertation, Uni-versity of Pennsylvania.Daniel Gildea and Martha Palmer.
2002.
The Neces-sity of Parsing for Predicate Argument Recognition.In Proceedings of ACL 2002, Philadelphia, USA.Mitchell Marcus, Grace Kim, Mary AnnMarcinkiewicz,et al 1994.
The Penn Treebank: Annotating PredicateArgument Structure.
In Proceedings of ARPA Speechand Natural Language Workshop.Martha Palmer, Dan Gildea, and Paul Kingsbury.
2005.The Proposition Bank: An Annotated Corpus of Se-mantic Roles.
Computational Linguistics, 31(1).Pradhan, S., Hacioglu, K., Krugler, V., Ward, W., Martin,J., and Jurafsky, D. 2005.
Support Vector Learning forSemantic Argument Classification.
To appear in Ma-chine Learning journal, Special issue on Speech andNatural Language Processing.V.
Punyakanok, D. Roth, W. Yih, and D. Zimak.
2004.Semantic Role Labeling via Integer Linear Program-ming Inference.
In Proceedings of COLING.Adwait Ratnaparkhi.
1999.
Learning to Parse NaturalLanguage with Maximum Entropy Models.
MachineLearning, 34, 151?175.Nianwen Xue and Martha Palmer.
2004.
CalibratingFeatures for Semantic Role Labeling.
In Proceedingsof EMNLP.240
