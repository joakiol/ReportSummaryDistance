Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1045?1053,Beijing, China, July 26-31, 2015. c?2015 Association for Computational LinguisticsSentence-level Emotion Classification with Label and Context Dependence?Shoushan Li?
?,   Lei Huang?,    Rong Wang?,   Guodong Zhou?
*?Natural Language Processing Lab, Soochow University, China?
Collaborative Innovation Center of Novel Software Technology and Industrialization{shoushan.li, lei.huang2013, wangrong2022}@gmail.com,gdzhou@suda.edu.cnAbstractPredicting emotion categories, such as anger,joy, and anxiety, expressed by a sentence ischallenging due to its inherent multi-labelclassification difficulty and data sparseness.In this paper, we address above two chal-lenges by incorporating the label dependenceamong the emotion labels and the context de-pendence among the contextual instances intoa factor graph model.
Specifically, we recastsentence-level emotion classification as a fac-tor graph inferring problem in which the labeland context dependence are modeled as vari-ous factor functions.
Empirical evaluationdemonstrates the great potential and effective-ness of our proposed approach to sentence-level emotion classification.
11 IntroductionPredicting emotion categories, such as anger, joy,and anxiety, expressed by a piece of text encom-passes a variety of applications, such as onlinechatting (Galik et al, 2012), news classification(Liu et al, 2013) and stock marketing (Bollen etal., 2011).
Over the past decade, there has been asubstantial body of research on emotion classifi-cation, where a considerable amount of work hasfocused on document-level emotion classification.Recently, the research community has becomeincreasingly aware of the need on sentence-levelemotion classification due to its wide potential ap-plications, e.g.
the massively growing importanceof analyzing short text in social media (Ki-ritchenko et al, 2014; Wen and Wan, 2014).
Ingeneral, sentence-level emotion classification ex-hibits two challenges.1 *  Corresponding author??<S1>????????????????????????????????????????????????
?</S1> <S2>?????????
?</S2> <S3>??????????????????????????????????????</S3>??
(English: ?
?<S1> The girls fall to sleep, so I make my waynoiselessly onto the bed, wishing I could get achance to give a kiss to Yan, suddenly she turnover to me and her little soft hand fall onto myface.</S1> <S2>Praise the Lord, that is all Iwant.</S2> <S3>Feeling the warm of her handand the attachment she hold to me, I couldn?t af-ford to move even a little, fearing I may lost herhand.</S3>)??)
-------------------------------------------------------------------Sentence-level Emotion Classification?
Input:      S1, S2, S3?
Output:     S1 :   joy, loveS2:   joyS3:  joy, love, anxietyFigure 1: An example of a paragraph and thesentences therein with their emotion categoriesfrom the corpus collected by Quan and Ren(2009)On one hand, like document-level emotionclassification, sentence-level emotion classifica-tion is naturally a multi-label classification prob-lem.
That is, each sentence might involve morethan one emotion category.
For example, asshown in Figure 1, in one paragraph, two sen-tences, i.e., S1 and S3, have two and three emotioncategories respectively.
Automatically classifyinginstances with multiple possible categories is1045sometimes much more difficult than classifyinginstances with a single label.On the other hand, unlike document-level emo-tion classification, sentence-level emotion classi-fication is prone to the data sparseness problembecause a sentence normally contains much lesscontent.
Given the short text of a sentence, it isoften difficult to predict its emotion due to thelimited information therein.
For example, in S2,only one phrase ?????
(that is all I want)?
ex-presses the joy emotion.
Once this phrase fails toappear in the training data, it will be hard for theclassifier to give a correct prediction according tothe limited content in this sentence.In this paper, we address above two challengesin sentence-level emotion classification by mod-eling both the label and context dependence.
Here,the label dependence indicates that multiple emo-tion labels of an instance are highly correlated toeach other.
For instance, the two positive emo-tions, joy and love, are more likely to appear at thesame time than the two counterpart emotions, joyand hate.
The context dependence indicates thattwo neighboring sentences or two sentences in thesame paragraph (or document) might share thesame emotion categories.
For instance, in Figure1, S1, S2, and S3, from the same paragraph, allshare the emotion category joy.Specifically, we propose a factor graph, namelyDependence Factor Graph (DFG), to model the la-bel and context dependence in sentence-levelemotion classification.
In our DFG approach, boththe label and context dependence are modeled asvarious factor functions and the learning task aimsto maximize the joint probability of all these fac-tor functions.
Empirical evaluation demonstratesthe effectiveness of our DFG approach to captur-ing the inherent label and context dependence.
Tothe best of our knowledge, this work is the firstattempt to incorporate both the label and contextdependence of sentence-level emotion classifica-tion into a unified framework.The remainder of this paper is organized as fol-lows.
Section 2 overviews related work on emo-tion analysis.
Section 3 presents our observationson label and context dependence in the corpus.Section 4 proposes our DFG approach to sen-tence-level emotion classification.
Section 5 eval-uates the proposed approach.
Finally, Section 6gives the conclusion and future work.2 Related WorkOver the last decade, there has been an explosionof work exploring various aspects of emotionanalysis, such as emotion resource creation(Wiebe et al, 2005; Quan and Ren, 2009; Xu etal., 2010), writer?s emotion vs. reader?s emotionanalysis (Lin et al, 2008; Liu et al, 2013), emo-tion cause event analysis (Chen et al, 2010), doc-ument-level emotion classification (Alm et al,2005; Li et al, 2014) and sentence-level or shorttext-level emotion classification (Tokushisa et al,2008; Bhowmick et al, 2009; Xu et al, 2012).This work focuses on sentence-level emotion clas-sification.Among the studies on sentence-level emotionclassification, Tokushisa et al (2008) propose adata-oriented method for inferring the emotion ofan utterance sentence in a dialog system.
Theyleverage a huge collection of emotion-provokingevent instances from the Web to deal with the datasparseness problem in sentence-level emotionclassification.
Bhowmick et al (2009) andBhowmick et al (2010) apply KNN-based classi-fication algorithms to classify news sentences intomultiple reader emotion categories.
Although themulti-label classification difficulty has been no-ticed in their study, the label dependence is notexploited.
More recently, Xu et al (2012) pro-poses a coarse-to-fine strategy for sentence-levelemotion classification.
They deal with the datasparseness problem by incorporating the transferprobabilities from the neighboring sentences torefine the emotion categories.
To some extent, thiscan be seen a specific kind of context information.However, they ignore the label dependence by di-rectly applying Binary Relevance to overcome themulti-label classification difficulty.Unlike all above studies, this paper emphasizesthe importance of the label dependence and ex-ploits it in sentence-level emotion classificationvia a factor graph model.
Moreover, besides thelabel dependence, our factor graph-based ap-proach incorporates the context dependence in aunified framework to further improve the perfor-mance of sentence-level emotion classification.3 ObservationsTo better illustrate our motivation of modeling thelabel and context dependence, we systematicallyinvestigate both dependence phenomena in ourevaluation corpus.1046Figure 2: Probability distribution of most and least frequently-occurred pairs of emotion categories,with left four most frequently-occurred and right four least frequently-occurred, among all 28 pairsThe corpus contains 100 documents, randomlyselected from Quan and Ren (2009).
There are to-tally 2751 sentences and each of them is manuallyannotated with one or more emotion labels.Table 1: The numbers of the sentences in eachemotion categoryEmotion #Sentence  Emotion #Sentencejoy 691  anxiety 567hate 532  surprise 180love 1025  anger 287sorrow 611  expect 603Table 2: The numbers of the sentencesgrouped by the emotion labels they contain#SentenceNo Label 180One Label 1096Two Labels 1081Three Labels 346Four or more labels 48ALL 2751Table 1 shows the sentence distribution of theeight emotion categories.
Obviously, the distribu-tion is a bit imbalanced.
While about to one quar-ter of sentences express the emotion category love,only ~6% and ~10% express surprise and angerrespectively, with the remaining 5 emotion cate-gories distributed rather evenly from ~20% to~25%.
Table 2 shows the numbers of the sen-tences grouped by the emotion labels they contain.From this table, we can see that more than halfsentences have two or more emotion labels.
Thisindicates the popularity of the multi-label issue insentence-level emotion classification.To investigate the phenomenon of label de-pendence, we first assume that dX R?
denotesan input domain of instances and1 2{ , ,..., }mY l l l?be a finite domain of possible emotion labels.Each instance is associated with a subset of Y andthis subset is described as an m-dimensional vec-tor 1 2{ , ,..., }my y y y?
where =1iy  only if in-stance x has label .il  and =0iy  otherwise.
Then,we can calculate the probability that an instancetakes both emotion labelsil and jl, denoted as( , )i jp l l. Figure 2 shows the probability distribu-tion of most and least frequently-occurred pairs ofemotion categories, with left four most fre-quently-occurred and right four least frequently-occurred, among all 28 pairs.
From this figure, wecan see that some pairs, e.g., joy and love, aremuch more likely to be taken by one sentence thansome other pairs, e.g.
joy and anger.Finally, we investigate the phenomenon of thecontext dependence by calculating the probabili-ties that two instanceskx and lx have at least oneidentical emotion label, i.e., )k lp y y ???
indifferent settings.0.1830.0940.078 0.0770.005 0.003 0.002 0.000300.020.040.060.080.10.120.140.160.180.21047Figure 4: An example of DFG when two instances are involved: sentence-1 with the label vector [1, 0,1] and sentence-2 with the label vector [1, 1, 0]Note: each multi-label instance is transformed into three pseudo samples, represented as  ( 1,2,3)kiX k ?
.
( )f ?
represents a factor function for modeling textual features.
( )g ?
represents a factor function for mod-eling the label dependence between two pseudo samples.
( )h ?
represents a factor function for modelingthe context dependence between two instances in the same context.Figure 3: Probabilities that two instances havean identical emotion label in different settingsFigure 3 shows the probabilities that two in-stances have at least one identical emotion label indifferent settings, where neighbor, paragraph,document and random mean two neighboring in-stances, two instances from the same paragraph,two instances from the same document, and twoinstances from a random selection, respectively.From this figure, we can see that two instancesfrom the same context are much more likely totake an identical emotion label than two randominstances.From above statistics, we come to two basic ob-servations:1) Label dependency: One sentence is morelikely to take some pair of emotion labels, e.g.,hate and angry than some other pair of emo-tion labels, e.g., hate and happy.2) Context dependency: Two instances from thesame context are more likely to share the sameemotion label than those from a random selec-tion.4 Dependence Factor Graph ModelIn this section, we propose a dependence factorgraph (DFG) model for learning emotion labels ofsentences with both label and context dependence.4.1 PreliminaryFactor GraphA factor graph consists of two layers of nodes, i.e.,variable nodes and factor nodes, with links be-tween them.
The joint distribution over the wholeset of variables can be factorized as a product ofall factors.
Figure 4 gives an example of our de-pendence factor graph (DFG) when two instances,i.e., sentence-1 and sentence-2 are involved.Binary RelevanceA popular solution to multi-label classification iscalled binary relevance which constructs a binaryclassifier for each label, resulting a set of inde-0.68 0.690.50.2200.20.40.60.8Neighbor Paragraph Document Random21yf (1 11 1,X y)31y11y22y12y11 1,X l21 2,X l22 2,X l12 1,X lf (1 12 2,X y)31 3,X l32 3,X l32yg(1 31 1,y y)g(1 32 2,y y)3l2l1l3l1l2lSentence-1Sentence-2DFG modelh(2 21 2,y y)h(1 11 2,y y)1048pendent binary classification problems (Tsouma-kas and Katakis, 2007; Tsoumakas et al, 2009).In our approach, binary relevance is utilized as apreliminary step so that each original instance istransformed into K pseudo samples, where K isthe number of categories.
For example, in Figure4, 11X , 21X , and 31X  represent the three pseudosamples, generated from the same original in-stance sentence-1.4.2 Model DefinitionFormally, let ?
?, ,G V E X?
represent an instancenetwork, where V denotes a set of sentence in-stances.
E V V?
?
is a set of relationships be-tween sentences.
Two kinds of relationship existin our instance network: One represents the labeldependence between each two pseudo instancesgenerated from the same original instance, whilethe other represents the context dependence whenthe two instances are from the same context, e.g.,the same paragraph.
X  is the textual feature vec-tor associated with a sentence.We model the above network with a factorgraph and our objective is to infer the emotion cat-egories of instances by learning the followingjoint distribution:?
??
?
?
??
?
?
??
?, , ,k k k k k ki i i i i ik iP Y Gf X y g y G y h y H y???
(1)where three kinds of factor functions are used.1) Textual feature factor function: ?
?,k ki if X ydenotes the traditional textual feature factorfunctions associated with each text kiX .
Thetextual feature factor function is instantiated asfollows:?
?
?
?11, exp ,k k k ki i kj ij ijf X y x yZ ??
??
??
??
??
(2)Where ?
?,k kij ix y?
is a feature function and kijxrepresents a textual feature, i.e., a word featurein this study.2) Label dependence factor function:?
??
?,k ki ig y G y  denotes the additional label de-pendence relationship among the pseudo in-stances, where ?
?kiG y  is the label set of theinstances connected to kiy .
?
?kiG y and kiy  arelabels of the pseudo instances generated fromthe same original instance.
The label depend-ence factor function is instantiated as follows:?
?
?
?2( )21, ( ) expl ki ik k k li i ikl i iy G yg y G y y yZ ???
??
??
??
??
??
??(3)Whereikl?
is the weight of the function, rep-resenting the influence degree of the two in-stances kiy and liy .3) Context dependence factor function:?
??
?,k ki ih y H y  denotes the additional contextdependence relationship among the instances,where ?
?kiH y  is the set of the instances con-nected to kiy .
?
?kiH y  and kiy  are the labels ofthe pseudo instances from the same context butgenerated from different original instances.The context dependence factor function is in-stantiated as follows:?
?
?
?2( )31, ( ) expk kj ik k k ki i ijk i jy H yh y H y y yZ ???
??
??
??
??
??
??
(4)Whereijk?is the weight of the function, repre-senting the influence degree of the two in-stances kiy and kjy.4.3 Model LearningLearning the DFG model is to estimate the bestparameter configuration ({ },{ },{ })?
?
?
??
tomaximize the log-likelihood objective function?
?
?
?logL P Y G??
?
, i.e.,?
?
* argmax L?
??
(5)In this study, we employ the gradient decentmethod to optimize the objective function.
For ex-ample, we can write the gradient of eachkj?
withregard to the objective function:?
?
?
?
?
?
?
?|, ,kjk kij i ij iP Y GkjL E x y E x y????
?
?
?
??
?
?
??
?
?
??
(6)Where ?
?, kij iE x y?
???
?
is the expectation of featurefunction ?
?, kij ix y?
given the data distribution.?
?
?
?| ,kj kij iP Y GE x y?
?
???
?is the expectation of featurefunction ?
?, kij ix y?
under the distribution?
?kjP Y G?
given by the estimated model.
Figure 5illustrates the detailed algorithm for learning theparameter ?
.
Note that LBP denotes the Loopy1049Belief Propagation (LBP) algorithm which is ap-plied to approximately infer the marginal distribu-tion in a factor graph (Frey and MacKay, 1998).A similar gradient can be derived for the other pa-rameters.Input: Learning rate ?Output: Estimated parameters ?Initialize 0?
?Repeat1) Calculate ?
?, kij iE x y?
???
?
using LBP2) Calculate?
?
?
?| ,kj kij iP Y GE x y?
?
???
?using LBP3) Calculate the gradient of ?
according toEq.
(6)4) Update parameter ?
with the learningrate ??
?new oldL ??
?
?
??
?Until ConvergenceFigure 5: The learning algorithm for DGP model4.4 Model PredictionWith the learned parameter configuration ?
, theprediction task is to find a *UY  which optimizesthe objective function, i.e.,?
?
* argmax , ,U U LY P Y Y G ??
(7)Where *UY  are the labels of the instances in thetesting data.Again, we utilize LBP to calculate the marginalprobability of each instance ?
?, ,k LiP y Y G ?
andpredict the label with the largest marginal proba-bility.
As all instances in the test data are con-cerned, above prediction is performed in an itera-tion process until the results converge.5 ExperimentationWe have systematically evaluated our DFG ap-proach to sentence-level emotion classification.5.1 Experimental SettingCorpusThe corpus contains 100 documents (2751 sen-tences) from the Ren-CECps corpus (Quan andRen, 2009).
In our experiments, we use 80 docu-ments as the training data and the remaining 20documents as the test data.FeaturesEach instance is treated as a bag-of-words andtransformed into a binary vector encoding thepresence or absence of word unigrams.Evaluation MetricsIn our study, we employ three evaluation metricsto measure the performances of different ap-proaches to sentence-level emotion classification.These metrics have been popularly used in somemulti-label classification problems (Godbole andSarawagi, 2004; Schapire and Singer, 2000).1) Hamming loss: It evaluates how many timesan instance-label pair is misclassified consid-ering the predicted set of labels and theground truth set of labels, i.e.,'1 111 1 j ji iq my yi jhloss mq ??
??
?
??
(8)where q is the number of all test instances andm is the number of all emotion labels.
'jiy isthe estimated label while jiy is the true label.2) Accuracy: It gives an average degree of thesimilarity between the predicted and theground truth label sets of all test examples, i.e.,''11 q i ii i iy yAccuracy q y y???
??
(9)3) F1-measure: It is the harmonic mean betweenprecision and recall.
It can be calculated fromtrue positives, true negatives, false positiveand false negatives based on the predictionsand the corresponding actual values, i.e.,''11 q i ii i iy yF1 q y y?????
(10)Note that smaller Hamming loss corresponds tobetter classification quality, while larger accuracyand F-measure corresponds to better classifica-tion quality.1050Figure 6: Performance comparison of different approaches to sentence-level emotion classificationwith the label dependence onlyFigure 7: Performance comparison of different approaches to sentence-level emotion classificationwith the context dependence only5.2 Experimental Results with Label De-pendenceIn this section, we compare following approacheswhich only consider the label dependence amongpseudo instances:?
Baseline: As a baseline, this approach appliesa maximum entropy (ME) classifier with onlytextual features, ignoring both the label andcontext dependence.?
LabelD: As the state-of-the-art approach tohandling multi-label classification, this ap-proach incorporates label dependence, as de-scribed in (Wang et al, 2014).
Specifically,this approach first utilizes a Bayesian networkto infer the relationship among the labels andthen employ them in the classifier.?
DFG-label: Our DFG approach with the labeldependence.Figure 6 compares the performance of differentapproaches to sentence-level emotion classifica-tion with the label dependence.
From this figure,we can see that our DFG approach improves thebaseline approach with an impressive improve-ment in all three kinds of evaluation metrics, i.e.,23.5% reduction in Hloss, 25.6% increase in Ac-curacy, and 11.8% increase in F1.
This result ver-ifies the effectiveness of incorporating the labeldependence in sentence-level emotion classifica-tion.
Compared to the state-of-the-art LabelD ap-proach, our DFG approach is much superior.
Sig-nificant test show that our DFG approach signifi-cantly outperforms both the baseline approach andLabelD (p-value<0.01).
One reason that LabelDperforms worse than our approach is possibly dueto their separating learning on textual features andlabel relationships.
Also, different from ours, theirapproach could not capture the information be-tween two conflict emotion labels, such as ?happy?and ?sad?
(they are not possibly appearing to-gether).5.3 Experimental Results with Context De-pendenceIn this section, we compare following approacheswhich only consider the context dependenceamong pseudo instances:0.4770.3780.2610.4610.3910.2690.2420.6340.3790.10.20.30.40.50.60.7Hloss Accuracy F1Baseline LebelD(Wang et al,2014) DFG-label(Our approach)0.4770.3780.2610.4720.3820.2640.4160.4430.2920.450.4070.2750.5690.2950.2150.20.30.40.50.6Hloss Accuracy F1Baseline  Tansfer(Xu et al,2012) DFG-context(Neighbor)DFG-context(Paragraph) DFG-context(Document)1051?
Baseline: same as the one in Section 5.2,which applies a maximum entropy (ME) clas-sifier with only textual features, ignoring boththe label and context dependence.?
Transfer: As the state-of-the-art approach toincorporating contextual information in sen-tence-level emotion classification (Xu et al,2012), this approach utilizes the label transfor-mation probability to refine the classificationresults.?
DFG-label (Neighbor): Our DFG approachwith the context dependence only.
Specifically,the neighboring instances are considered ascontext.?
DFG-label (Paragraph): Our DFG approachwith the context dependence only.
Specifically,the instances in the same paragraph are consid-ered as context.?
DFG-label (Document): Our DFG approachwith the context dependence only.
Specifically,the instances in the same document are consid-ered as context.Figure 7 compares the performance of differentapproaches to sentence-level emotion classifica-tion with the context dependence only.
From thisfigure, we can see that our DFG approach consist-ently improves the state-of-the-art in all threekinds of evaluation metrics, i.e., 6.1% reduction inHloss, 6.5% increase in Accuracy, and 3.1% in-crease in F1 when the neighboring instances areconsidered as context.
Among the three kinds ofcontext, the neighboring setting performs best.We also find that using the whole document as thecontext is not helpful and it performs even worsethan the baseline approach.
Compared to the state-of-the-art Transfer approach, our DFG approachwith the neighboring context dependence is muchsuperior.
Significant test show that our DFG ap-proach with the neighboring context dependencesignificantly outperforms the baseline approachand the state-of-the-art LabelD approach (p-value<0.01).5.4 Experimental Results with Both Labeland Context DependenceTable 3 shows the performance of our DFG ap-proach with both label and context dependence,denoted as DGF-both.
From this table, we can seethat using both label and context dependence fur-ther improves the performance.Figure 8 shows the performance of our DGF-both approach when different sizes of trainingdata are used to train the model.
From this figure,we can see that incorporating both the label andcontext dependence consistently improves theperformance with a large margin, irrespective ofthe amount of training data available.Table 3: Performance of our DFG approachwith both label and context dependenceHloss Accuracy F1Baseline  0.447 0.378 0.261DFG-label 0.254 0.621 0.372DFG-context 0.416 0.443 0.292DFG-both 0.242 0.634 0.379Figure 8: Performance of our DGF-both ap-proach when different sizes of training data areused6 ConclusionIn this paper, we propose a novel approach to sen-tence-level emotion classification by incorporat-ing both the label dependence among the emotionlabels and the context dependence among the con-textual instances into a factor graph, where the la-bel and context dependence is modeled as variousfactor functions.
Empirical evaluation shows that0.20.30.40.50.620% 40% 60% 80%Hloss0.20.30.40.50.60.720% 40% 60% 80%Accuracy0.20.250.30.350.420% 40% 60% 80%F1Baseline DFG-both1052our DFG approach performs significantly betterthan the state-of-the-art.In the future work, we would like to explore bet-ter ways of modeling the label and context de-pendence and apply our DFG approach in moreapplications, e.g.
micro-blogging emotion classi-fication.AcknowledgmentsThis research work has been partially supportedby three NSFC grants, No.61273320,No.61375073, No.61331011, and CollaborativeInnovation Center of Novel Software Technologyand Industrialization.ReferencesAlm C., D. Roth and R. Sproat.
2005.
Emotions fromText: Machine Learning for Text-based EmotionPrediction.
In Proceedings of EMNLP-05, pp.579-586.Bhowmick P., A. Basu, P. Mitra, and A. Prasad.
2009.Multi-label Text Classification Approach for Sen-tence Level News Emotion Analysis.
PatternRecognition and Machine Intelligence.
LectureNotes in Computer Science,  Volume 5909,  pp 261-266.Bhowmick P., A. Basu, P. Mitra, and A. Prasad.
2010.Sentence Level News Emotion Analysis in FuzzyMulti-label Classification Framework.
Research inComputing Science.
Special Issue: Natural Lan-guage Processing and its Applications, pp.143-154.Bollen J., H. Mao, and X.-J.
Zeng.
2011.
Twitter MoodPredicts the Stock Market.
Journal of Computa-tional Science, 2(1):1?8, 2011.Chen Y., S. Lee, S. Li and C. Huang.
2010.
EmotionCause Detection with Linguistic Constructions.
InProceedings of COLING-10, pp.179-187.Frey B. and D. MacKay.
1998.
A Revolution: BeliefPropagation in Graphs with Cycles.
In Proceedingsof NIPS-98, pp.479?485.Galik M. and S. Rank.
2012.
Modelling Emotional Tra-jectories of Individuals in an Online Chat.
In Pro-ceedings of Springer-Verlag Berlin Heidelberg-12,pp.96-105.Godbole S. and S. Sarawagi.
2004.
DiscriminativeMethods for Multi-labeled Classification.
In Ad-vances in knowledge discovery and data mining.
pp.22-30.Kiritchenko S., X. Zhu, and S. Mohammad.
2014.
Sen-timent Analysis of Short Informal Texts.
Journal ofArtificial Intelligence Research, 50(2014), pp.723-762.Li C., H. Wu, and Q. Jin.
2014.
Emotion Classificationof Chinese Miroblog Text via Fusion of BoW andeVector Feature Representations.
In Proceedings ofNLP&CC-14, pp.217-228.Lin K., C. Yang, and H. Chen.
2008.
Emotion Classi-fication of Online News Articles from the Reader?sPerspective.
In Proceedings of the InternationalConference on Web Intelligence and IntelligentAgent Technology-08, pp.220-226.Liu H., S. Li, G. Zhou, C. Huang, and P. Li.
2013.
JointModeling of News Reader?s and Comment Writer?sEmotions.
In Proceedings of ACL-13, short paper,pp.511-515.Quan C. and F. Ren.
2009.
Construction of a BlogEmotion Corpus for Chinese Emotional ExpressionAnalysis.
In Proceedings of EMNLP-09, pp.1446-1454.Schapire R. E and Y.
Singer.
2000.
A Boosting-basedSystem for Text Categorization.
Machine learning,pp.
135-168TOKUHISA R., K. Inui, and Y. Matsumoto.
2008.Emotion Classification Using Massive ExamplesExtracted from the Web.
In Proceedings of COL-ING-2008, pp.881-888.Tsoumakas G. and I. Katakis.
2007.
Multi-label Clas-sification: An Overview.
In Proceedings of Interna-tional Journal of Data Warehousing and Mining,3(3), pp.1-13.Tsoumakas G., I. Katakis, and I. Vlahavas.
2009.
Min-ing Multi-label Data.
Data Mining and KnowledgeDiscovery Handbook, pages 1?19.Wen S. and X. Wan.
2014.
Emotion Classification inMicroblog Texts Using Class Sequential Rules.
InProceedings of AAAI-14, 187-193.Wang S., J. Wang, Z. Wang, and Q. Ji.
2014.
Enhanc-ing Multi-label Classification by Modeling Depend-encies among Labels.
Pattern Recognition.
Vol.
47.Issue 10: 3405-3413, 2014.Wiebe J., T. Wilson, and C. Cardie.
2005.
AnnotatingExpressions of Opinions and Emotions in Language.Language Resources and Evaluation, 39, 65-210.Xu G., X. Meng and H. Wang.
2010.
Build ChineseEmotion Lexicons Using A Graph-based Algorithmand Multiple Resources.
In Proceedings of COL-ING-10, pp.1209-1217.Xu J., R. Xu, Q. Lu, and X. Wang.
2012.
Coarse-to-fine Sentence-level Emotion Classification based onthe Intra-sentence Features and Sentential Context.In Proceedings of CIKM-12, poster, pp.2455-2458.1053
