Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 889?896Manchester, August 2008Relational-Realizational ParsingReut Tsarfaty and Khalil Sima?anInstitute for Logic, Language and Computation, University of AmsterdamPlantage Muidergracht 24, 1018TV, Amsterdam, The Netherlands{rtsarfat,simaan}@science.uva.nlAbstractState-of-the-art statistical parsing modelsapplied to free word-order languages tendto underperform compared to, e.g., pars-ing English.
Constituency-based mod-els often fail to capture generalizationsthat cannot be stated in structural terms,and dependency-based models employ a?single-head?
assumption that often breaksin the face of multiple exponence.
In thispaper we suggest that the position of a con-stituent is a form manifestation of its gram-matical function, one among various pos-sible means of realization.
We develop theRelational-Realizational approach to pars-ing in which we untangle the projectionof grammatical functions and their meansof realization to allow for phrase-structurevariability and morphological-syntactic in-teraction.
We empirically demonstratethe application of our approach to pars-ing Modern Hebrew, obtaining 7% errorreduction from previously reported results.1 IntroductionMany broad-coverage statistical parsers to date areconstituency-based with a Probabilistic Context-Free Grammar (PCFG) or a Stochastic Tree Sub-stitution Grammar (STSG) at their backbone.
Themajority of such models belong to a Head-Drivenparadigm, in which a head constituent is gen-erated first, providing a positional anchor forsubsequent (e.g., Markovian) sisters?
generation.c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.Constituency-based models, lexicalized and un-lexicalized alike, demonstrate state-of-the-art per-formance for parsing English (Charniak, 1997;Collins, 2003; Klein and Manning, 2003; Bod,2003), yet a direct application of such models toparsing less configurational languages often failsto yield comparable results.
The parameters ofsuch parsers capture generalizations that are eas-ily stated in structural terms (e.g., subjects linearlyprecede predicates, VPs dominate objects, etc.
)which may not be adequate for parsing languageswith less configurational character.A different vein of research explores data-drivendependency-based parsing methods (e.g., (Mc-Donald et al, 2005)) which seem to be intuitivelymore adequate for the task.
It turns out, how-ever, that even such models fail to provide thedesired remedy.
Recent reports by (Nivre, 2007)delineated a class of richly-inflected languageswith relatively free word-order (including Greek,Basque, and Modern Standard Arabic) for whichthe parsers performed poorly, regardless of theparsing method used.
The need for parsing meth-ods that can effectively cope with such phenomenadoesn?t seem to have been eliminated by depen-dency parsing ?
perhaps quite the contrary.The essential argument we promote here is thatin order to deal with the kind of variation thatis empirically observed cross-linguistically an al-ternative view of the generation process is re-quired.
Our Relational-Realizational parsing pro-posal, strongly inspired by Relational Grammar(Perlmutter, 1982), takes grammatical relationssuch as ?Subject?
and ?Predicate?
as central, primi-tive notions of the syntactic representation, and re-tains a distinction between the projection of suchrelations and the means by which they are real-ized.
The grammar we develop here, formally889represented as a PCFG, articulates two alternatinggeneration phases: a Relational phase, in which aclause-level category projects a monostratal Rela-tional Network (RN) representation for the clause,and a Realizational phase, in which the projectedrelations are realized in a certain surface config-uration.
Paradigmatic morphosyntactic represen-tations are constructed for all non-terminal nodes,allowing for morphosyntactic interaction at vari-ous levels of the syntactic parse tree.We illustrate the application of our theoreticalreconstruction to the representation of clause-levelcategories in Modern Hebrew (MH) and their in-teraction with a handful of morphological features.The treebank grammar resulting from our applica-tion yields 13% error reduction relative to a tree-bank PCFG which uses the same information inthe form of state-splits, and our best result showsa 7% error reduction over the best parsing resultsfor MH so far.
Through a quantitative and quali-tative analysis we illustrate the advantages of theRelational-Realizational approach and its poten-tial promise for parsing other ?exotic?
languages.2 BackgroundRecent decades have seen a surge of interest in sta-tistical models using a body of annotated text forlearning the distributions of grammatically mean-ingful structures, in order to assign the most likelyones to unseen sentences.
Probabilistic ContextFree Grammars (PCFGs) have become popular inthe articulation of such models, and unlexicalizedtreebank grammars (or representational variationsthereof) were shown to perform reasonably well onEnglish benchmark corpora (Johnson, 1998; Kleinand Manning, 2003).A major leap in the performance of PCFG-basedstatistical parsers has been introduced by the movetowards a Head-Driven paradigm (Collins, 2003;Charniak, 1997), in which syntactic categories areenriched with head information percolated up thetree.
The head-driven generation process allowsone to model the relation between the informationcontent of a constituent and the information con-tent of its head-marked sister.
At the same time,such models introduce a bias with respect to thepositioning of a non-head constituent relative to itshead-marked sister.
The vast improvement in pars-ing results came about not without modeling costs,e.g., additional ad-hoc modifications for capturingcomplex structures such as conjunction.An inherent difficulty with the application ofconstituency-based parsing models is the implicitassumption that the relation between the posi-tion of a constituent and its grammatical func-tion is fully predictable.
For languages withrelatively free word-order, this assumption oftenbreaks down.
Distinguishing, e.g., ?left?
and?right?
distributions for constituents of the same?sort?
implicitly takes the position of a constituentto be a primitive syntactic notion, and their gram-matical function to be a secondary, derived one.Theoretical accounts show that this may be insuf-ficient (Perlmutter, 1982).
A subsequent difficultywith the head-driven paradigm, also shared bydependency-based parsing methods, is the stipu-lation that all grammatically relevant properties ofa phrase are recovered from a single head.
In fact,it is typologically established that grammaticallymeaningful properties of a constituent may jointlyemerge from different surface forms dominated byit (co-heads or multiple exponence (Zwicky, 1993;Blevins, 2008)).1The task we undertake here is to suggest a sta-tistical generative parsing method which is linguis-tically plausible as well as technologically viablefor parsing languages with relatively free word-order and variable means of realization.
In whatfollows we remain within the computationally ef-ficient framework of PCFGs, and propose a varia-tion that draws on insights from syntactic and mor-phological theories that have been explored cross-linguistically.3 Approach3.1 Relational Grammars (RGs)Relational Grammars (RGs) were introduced in theearly 80?s when attempts to find a universal def-inition for notions such as a ?Subject?
in termsof various ?behavioral properties?
seemed to havefailed (Perlmutter, 1982).
The unsuccessful at-tempts to recover an adequate definition of gram-matical functions in structural terms led to a revivalof a view in which grammatical relations such as?Subject?
and ?Object?
are primitive notions by1We refrain here from referring to the increasingly popularapproach of discriminative parsing, firstly, because we are in-terested in a generative parsing model that assigns a probabil-ity distribution to all sentence-structure pairs in the language,essentially allowing it to be used as a language model (e.g.,in SR or SMT applications).
Secondly, so far the features thathave been explored in these frameworks are mainly those eas-ily stated in structural terms, with not much effort towardsmodeling morphosyntactic interactions systematically.890which syntactic structures are defined (Postal andPerlmutter, 1977).
This view proved useful for de-scriptive purposes, influencing the design of for-malisms such as Arc-Pair grammars (Postal, 1982)and LFG (Bresnan, 1979).The two main primitive elements used in RGsare (a) a set of nodes representing linguistic ele-ments (which we refer to using upper case letters),and (b) a set of names of grammatical relations(which we refer to as gr1...grn).
RGs representthe fact that a linguistic element bears a certain re-lation to another element using a structure calledan ?Arc?, represented as [gri(A,B)].
Arcs are rep-resented as arrows, with A the head of the Arc andB its tail, and a Relational Network (RN) is definedto be a set of Arcs that share a single head.2Now, a few theoretical observations are due.Firstly, the essential difference between RGs anddependency-based grammars is that RNs take thelinguistic element at the head of a network to be aclause-level category, not a particular surface form.The corresponding tails are then the various nom-inals bearing the different grammatical relationsto the clause (including a ?Predicate?, a ?Subject?,an ?Object?, etc.).
In addition, RNs abstract awayfrom elements such as auxiliary verbs and particleswhich do not have their own arc representation.RGs also differ from phrase-structure grammarsin that their RNs are unordered.
Therefore, linearprecedence need not play a role in stating general-izations.
RGs differ from both constituency- anddependency-based formalisms in that they do notweigh heavily the ?single-head?
assumption ?
RNsmay delineate a whole chunk as bearing a certaingrammatical relation to the clause.The set-theoretic notion of RNs in RGs abstractsaway from surface phenomena crucial for generat-ing phrase-structure trees.
Thus, we next turn tomodeling how grammatical relations are realized.3.2 Form, Function and SeparationMorphological phenomena such as suprasegmen-tation, interdigitation, reduplication, subtractivemorphology, templatic morphology, and methathe-sis demonstrate that it is sometimes impossible tofind a direct correspondence between a certain part2RGs also define the notion of a stratum, a single levelof syntactic representation, and for the current discussion weassume a monostratal representation.
We do not claim thatour framework is capable of dealing with the full range ofphenomena multistratal RNs were shown to account for, yetthere is nothing in our proposal that excludes extending therepresentation into a multistratal framework that does so.of a word (a ?morpheme?)
and the function it hasin altering the word?s meaning (Anderson, 1992).Attempts to model such morphological phenom-ena brought forward the hypothesis that ?form?
and?function?
need not stand in one-to-one correspon-dence, and that one is not necessarily immediatelypredicted by the other.
This hypothesis is known asthe ?Separation Hypothesis?
(Beard, 1988).
Theproblem of modeling certain surface phenomenathen boils down to modeling form and functioncorrelations, bearing in mind that these may bequite complex.Bringing this general notion of separation intothe syntactic derivation, we propose to view theposition of a constituent in a phrase as its form andthe articulated grammatical relation as its function.The task of learning the position of different con-stituents realizing the grammatical relations in anRN is now delegated to a statistical component.
Aset of parameters which we refer to as ?configu-ration?
determines the syntactic position in whicheach of the grammatical relations is to be realized.3.3 Morphosyntactic RepresentationsIn order to connect the abstract RN representationwith the constituents that syntactic parse trees are?made of?
we propose to view the internal nodesof a tree as Morphosyntactic Paradigms.
Ourmorphosyntactic representation for constituents,loosely inspired by (Anderson, 1992), is a struc-tured representation of morphological and syntac-tic properties for an internal node in the parse tree.In our model, the morphological features asso-ciated with a syntactic constituent are percolatedfrom its dominated surface forms, and we allowthe specification of head (PoS tag) information andstructural features such as vertical markovization.Given the grammatical relation an element bearsto a clause, it is statistically feasible to learn themorphosyntactic paradigm by which it is realized.4 The Model4.1 The Generative ModelLet Sp?
?Sc1-gr1.
.
.
Scn-grn?
be a context-freerule where Spis the morphosyntactic representa-tion of a parent constituent, gr1...grnare the gram-matical relations forming its RN, and ?Sc1.
.
.
Scn?are ordered morphosyntactic representations of thechild constituents bearing the respective relationsto the parent.
Our grammar then conceptualizesthe generation of such a rule in three phases:891?
Projection:Sp?
{gri}ni=1@Sp?
Configuration:{gri}ni=1@Sp?
?gr1@Sp.
.
.
grn@Sp??
Realization:{gri@Sp?
Sci}ni=1In the projection stage we generate the set of gram-matical relations in the RN of a constituent.
Inthe configuration stage we order these grammat-ical relations, and in realization we generate themorphosyntactic representation of each child con-stituent given the relation to its parent.
Figure (1)shows the application of this process to two clausesbearing identical RNs that are in turn realized indifferent possible configurations.This three-step process does not generate func-tional elements (such as auxiliary verbs andspecial-purpose particles) that are outside of con-stituents?
RNs.
We thus let the configuration stageplace obligatory or optional ?realizational slots?between the ordered elements (marked gri: grj),signalling periphrastic adpositions and/or modifi-cation.
Note that modification may introduce morethan one constituent, to be generated in realization.?
Projection:Sp?
{gri}ni=1@Sp?
Configuration:{gri}ni=1@Sp?
?gr0: gr1@Spgr1@Sp.
.
.
grn: grn+1@Sp??
Realization:{gri@Sp?
Sci}ni=1{gri: gri+1@Sp?
?Sci1...Scimi?
}ni=0In figure (2), the configuration stage reserves a slotfor an obligatory punctuation mark at the end of anaffirmative sentence.
It further reserves a slot foran optional adverbial modifier at a position com-monly employed in MH for interjections.In the current framework, grammatical rela-tions may be realized in a certain surface positionvia configuration, or using explicit morphologicalmarking per grammatical relation independently oflinear context.
Figure (3) demonstrates how therealization phase models the correlation betweengrammatical relations and morphological informa-tion percolated from dominated surface forms.
Inparticular, our model can capture the interactionbetween marked features, e.g., the ?exclusive or?relation between definiteness and accusativity inmarking direct objects in MH (Danon, 2001).Finally, a conjunction structure in our modelis simply an RN representation of multiple mor-phosyntactically equivalent conjuncts, as illus-trated in figure (4).
This modeling choice avoidsthe need to stipulate a single head for such struc-tures (cf.
head-driven processes) and allows thedifferent conjuncts to share a realization distribu-tion ?
essentially implying homogeneity in theassignment of heads and morphosyntactic featuresacross conjuncts.4.2 The Probabilistic ModelOur probablistic model is a PCFG, where CFGrules capture the three stages of generation.
Ev-ery time we apply our projection-configuration-realization cycle we replace the rule probabilitywith the probabilities of the three stages, multi-plied (n +?ni=0midaugthers , gr0=grn+1=null).P (?Sci1, .., ScimiSci-gri, Sci+11, .., Sci+1mi+1?ni=0|Sp) =P ({gri}ni=1|Sp)?P (?gr0: gr1, g1, .
.
.
?|{gri}ni=1, Sp)?
?ni=1P (Sci|gri, Sp)?P (?Sc01, ..., Sc0m0?|gr0: gr1, Sp)?
?ni=1P (?Sci1, ..., Scimi?|gri: gri+1, Sp)The multiplication implements the independenceassumption between form and function underlyingthe Separation Hypothesis, and the conditioningwe articulate captures one possible way to model asystematic many-to-many correspondence.4.3 The GrammarWe use a probabilistic treebank grammar in whichthe different parameters and their probability dis-tributions are read off and estimated from thetreebank trees.
Clause-level (or clause-like) con-stituents such as S, SQ, FRAG, FRAGQ, inter-nally complex VPs and a small number NPs canhead RNs.
For the rest we use flat CFG rules.We use a limited set of grammatical relations,namely, ?Predicate?, ?Subject?, ?Object?
and ?Com-plement?
?
making the distinction between anominal complement and a verbal (infinitival) one.Our linguistic elements are morphosyntactic rep-resentations of labeled non-terminal constituents,where the morphosyntactic representations of con-stituents incorporate morphological informationpercolated from surface forms and syntactic infor-mation about the constituent?s environment.892(a) SNP-SBJ VP-PRD NP-OBJS{PRD,OBJ,SBJ}@SSBJ@SNPPRD@SVPOBJ@SNP(b) SVP-PRD NP-SBJ NP-OBJS{PRD,OBJ,SBJ}@SPRD@SVPSBJ@SNPOBJ@SNPFigure 1: Generating Canonical and Non-Canonical Configurations: The CF depictions of the S level constituents at theLHS of (a) and (b) are distinct, whereas the RR-CFG representations at the RHS of (a) and (b) share the projection of GRs anddiffers in their configuration ?
while (a) generates an SVO order, (b) generates a so-called Verb-Initial (VI) construction.
(c) SVP-PRD ADVP NP-SBJ NP-OBJ DOTS{PRD,OBJ,SBJ}@SPRD@SVPPRD:SBJ@SADVPSBJ@SNPOBJ@SNPOBJ:@SDOTFigure 2: Generating Adjunction and Periphrastic Configurations: The CF depiction of S at the LHS of (c) generatescomplements, adjuncts, and punctuation in one go, whereas the RR-CFG representation at the RHS generates first the projec-tion of core grammatical elements and then the configuration of a modified affirmative sentence in which they are realized.
(Similarly, realising a question configuration using inversion in, e.g., English, naturally follows).
(d) SVP-PRDVBNP[Def+,Acc+]-OBJAT[Acc+]NP[Def+]NNT NN[Def+]NP[Def+]-SBJNN[Def+]S{PRD,OBJ,SBJ}@SPRD@SVPVBOBJ@SNP[Def+,Acc+]AT[Acc+]NP[Def+]NNT NN[Def+]SBJ@SNP[Def+]NN[Def+]Figure 3: Realizing Grammatical Relations with bounded and unbounded Morphemes: The CF depiction of the S levelconstituent at the LHS of (d) shows a strong dependence between the position of syntactic constituents and the morphologicallyrealized features percolated from lower surface forms.
In the RR-CFG representation at the RHS the feature distribution amongsub constituents is dependent on grammatical relations, independently of their positioning.
The realization stage generates amorphosyntactic paradigm in one go, allowing to capture meaningful collocations and idiosyncrasies, e.g., the Xor relation ofthe Acc+ and def+ features when marking direct objects in MH (Danon, 2001).S-CNJS S CC S DOTS{SCNJ,SCNJ,SCNJ}@SSCNJ@SSSCNJ@SSSCNJ:SCNJ@SCCSCNJ@SSSCNJ:@SDOTFigure 4: Generating a Conjunction Structure: The conjunction structure in the LHS of (e) is generated by the RR-CFGon the RHS in three stages.
First, a relational network of finite number of conjuncts is generated, then a configuration for theconjuncts and conjunction markers (in MH, a CC before the last conjunct) is proposed, and finally the different conjuncts aregenerated conditioned on the same grammatical relation and the same parent.
(Note that the possibility of different means forrealizing conjunction, e.g., using morphemes, punctuation or multiple adpositions, falls out naturally from this setup.
)8935 ExperimentsData The data we use is taken from the ModernHebrew Treebank (MHTB) (Sima?an et al, 2001)which consists of 6501 sentences from the newspa-per ?haaretz?
annotated with phrase-structure treesand decorated with various morphological andfunctional features.
We use version 2.0 of the tree-bank3 which we processed and head-annotated asin (Tsarfaty and Sima?an, 2007).
We experimentedwith sentences 1?500 (development set) and sen-tences 501?6001 (training set), and used sentences6001-6501 (test set) for confirming our best result.Models Our Plain models use the coarse-levelMHTB category-labels enriched with various mor-phological features.
Our morphological represen-tation Base varies with respect to the use of the per-colated features definiteness Def and accusativityAcc.
Constituents?
morphosyntactic representa-tions enriched with their head PoS tag are referredto as Head and grand-parent encodings as Parent.For each combination of morphological and syn-tactic features we experimented with a state-splitPCFG and with our RR-PCFG implementation.Procedure We read off our models?
parame-ters from the decorated phrase-structure trees inthe MHTB, and use relative frequency estimationto instantiate their probability distributions.
Wesmooth lexical rules using a PoS-tags distributionwe learn for rare-words, where the ?rare?
thresholdis set to 1.
We then use BitPar, a general purposechart parser,4 to find the most likely structures,and we extract the corresponding coarse-grainedtree-skeletons for the purpose of evaluation.5 Weuse PARSEVAL measures to quantitatively evalu-ate our models and perform a qualitative analysisof the resulting parse trees.Results Table 1 shows the average F-Measurevalue for all sentences of length ?40 in ourdevelopment set with/without punctuation.
Thena?
?ve baseline implementation for our experi-ments, the BasePlain PCFG, performs at the levelof 67.61/68.67 (comparable to the baseline re-ported in (Tsarfaty and Sima?an, 2007)).
For all3http://www.mila.cs.technion.ac.il/english/resources/corpora/treebank/ver2.0/index.html4:http://www.ims.uni-stuttgart.de/tcl/SOFTWARE/BitPar.html5Our setup is comparable to English, which means that oursurface forms are segmented per PoS tag without specifyingtheir respective PoS tags and morphological features.Syntax Plain Head Parent ParentHeadMorphologyBase (PCFG) 67.61/68.77 71.01/72.48 73.56/73.79 73.44/73.61(RR-PCFG) 65.86/66.86 71.84/72.76 74.06/74.28 75.13/75.29BaseDef (PCFG) 67.68/68.86 71.17/72.47 74.13/74.39 72.54/72.79(RR-PCFG) 66.65/67.86 73.09/74.13 74.59/74.59 76.05/76.34BaseDefAcc (PCFG) 68.11/69.30 71.50/72.75 74.16/ 74.41 72.77/73.01(RR-PCFG) 67.13/68.01 73.63/74.69 74.65/74.79 76.15/ 76.43Table 1: Parsing Results for Sentences of Length < 40in the Development Set: Averaged F-Measure With/WithoutPunctuation.
Base refers to coarse syntactic categories, Defindicates percolating definiteness values, Acc indicated per-colating accusativity marking.
The underlined results repli-cate previously reported results in similar settings.models in the Plain column the simple PCFG out-performs the RR-variety.
Yet, the contribution ofpercolated morphological features is higher withthe RR-PCFG than with the simple PCFG.Moving to the Head column, we see that all RR-models already outperform their enriched PCFGcounterparts.
Again, morphological informationcontributes more to the RR-variety.
The best resultfor this column, achieved by the BaseDefAccHeadRR-model (63.73/64.69), outperforms its PCFGcounterpart as well as all two-dimensional modelsreported by (Tsarfaty and Sima?an, 2007).
In theParent column), our RR-variety continues to out-perform the PCFG albeit in an insignificant rate.
(Both results are at the same level as the best modelof (Tsarfaty and Sima?an, 2007).
)Finally, for all models in the ParentHead col-umn the RR-models outperform their PCFG coun-terparts to a significant degree.
Similarly to theHead column, the more morphological informa-tion is added, the greater the improvement is.
Ourbest RR-model, BaseDefAccParentHead, scoresalmost 10pt (25% error reduction) more than thePlain PCFG, it is about 3.5pt better (13% error re-duction) than a state-split PCFG using the sameinformation, and almost 2pt (7% error reduction)more than the best results reported for MH so far.We confirmed the results of our best model onour test set, for which our baseline (BasePlain)obtained 69.63/70.31.
The enriched PCFGof DaseDefAccHeadParent yields 73.66/73.86whereas the RR-PCFG yields 75.83/75.89.
Theoverall performance for PCFGs is higher on thisset, yet the RR-model shows a notable improve-ment (about 9% error reduction).6 Analysis and DiscussionThe trends in our quantitative analysis suggest thatthe RR-models are more powerful in exploitingdifferent sorts of information encoded in parse894(a) SNPCDTEFRWTtens-ofNPNNANFIMpeopleVPVBMGIEIMarrivePPINMfromNPNNPTAILNDThailandPPINLtoNPNNPIFRALIsrael...(b) SNPCDTEFRWTtens-ofNPNNANFIMpeopleVPVBMGIEIMarrivePPINMfromNPNPNNPTAILNDThailandPPINLtoNPNNPIFRALIsrael....Figure 5: Qualitative Analysis of Sentence (Fragment)#1: (a) is the gold tree fragment, correctly predicted by ourbest RR-PCFG model.
(b) is the tree fragment predicted bythe PCFG corresponding to previously reported results.trees, be it morphological information comingfrom dominated surface forms or functional infor-mation on top of syntactic categories.We have shown that head information, whichhas very little contribution to parsing accuracy asa mere state-split, turns out to have crucial ef-fects within the RR-models.
For state-splits basedPCFGs, adding head information brings abouta category fragmentation and decreasing perfor-mance.
The separation between form and functionwe articulate in the RR-approach allows us to cap-ture generalizations concerning the distribution ofsyntactic constituents under heads based on theirgrammatical function, and use fine-grained fea-tures to predict their morphosyntactic behaviour.We have further shown that morphological in-formation contributes a substantial improvementwhen adopting the RR-approach, which is inlinewith the linguistic insight that there is a correlationbetween morphological marking on top of surfaceforms and the grammatical function their domi-nating constituents realize.
Morphological infor-mation is particularly useful in the presence ofheads.
Taken together, head and percolated fea-tures implement a rather complete conceptualiza-tion of multiple exponence.To wrap up the discussion, we leave numbersaside and concentrate on the kind of structures pre-dicted by our best model in comparison to theones suggested by previously reported unlexical-ized PCFGs ((Tsarfaty and Sima?an, 2007), un-derlined in our table).
Due to lack of space we(a) SPPMCD FNIon theother handVPVBMTIRallowsNPNNPMSRD HEBWDH WHRWWXHthe ministry of...VPVBLHESIKto-employNPEWBDIM ZRIMforeignworkersPPB..in..(b) SPPINMfromNPNNTCDsideNPCDTFNItwoNPNNTMTIRallowsNPNNPMSRD HEBWDH WHRWWXHthe ministry of...VPLHESIKto-employNPNPEWBDIMworkersADJPZRIMforeignersPPB..in..Figure 6: Qualitative Analysis of Sentence (Fragment)#4: (a) is the gold tree fragment, correctly predicted by ourbest RR-PCFG model.
(b) is the tree fragment predicted bythe PCFG corresponding to previously reported results.only discuss errors found within the first 10 parsedsentence, yet we note that the qualitative trendwe describe here persists throughout our develop-ment set.
Figures (5) and (6) show a gold tree(a fragment of sentence #1) correctly predictedby our best RR-model (a) in comparison with theone predicted by the respective PCFG (b).
Thetree fragment in figure (5) shows that the RR-grammar bracketed and attached correctly all theconstituents that bear grammatical relations to theS clause (5a).
The corresponding PCFG conflatedthe ?to?
and ?from?
phrases to a rather meaning-less prepositional phrase (5b).
For (a fragment of)sentence #4 in our set (figure 6) the RR-model re-covered all grammatically meaningful constituentsunder the S clause and under the internal VP (6a).Notably, the PCFG in (6b) recovered none of them.Both grammars make attachment mistakes internalto complex NPs, but the RR-model is better at iden-tifying higher level constituents that correlate withmeaningful grammatical functions.Our qualitative analysis suggests that our modelis even more powerful than our quantitative analy-sis indicates, yet we leave the discussion of betterways to quantify this for future research.A Note on Related Work Studies on parsingMH to date concentrate mostly on spelling outthe integration of a PCFG parser with a mor-phological disambiguation component (e.g., (Tsar-faty, 2006; Goldberg and Tsarfaty, 2008)).
On asetup identical to ours (gold segmentation, no PoS)the latter obtained 70pt.
(Tsarfaty and Sima?an,8952007) examined the contribution of horizontaland vertical conditioning to an unlexicalized MHparser and concluded that head-driven Markoviza-tion performs below the level of vertical condi-tioning enriched with percolated features.
We donot know of existing dependency-parsers appliedto parsing MH or mildly-context-sensitive broad-coverage parsers applied to parsing a Semitic lan-guage.6 To the best of our knowledge, this is thefirst fully generative probabilistic framework thatmodels explicitly morpho-syntactic interaction toenhance parsing for non-configrational languages.7 ConclusionProjection and Realization are two sides of thesame coin.
Projection determines which gram-matical relations appear in the syntactic represen-tation, and Realization determines how such rela-tions are realized.
We suggest that the Relational-Realizational (RR) approach is adequate for pars-ing languages characteristically different from En-glish, and we illustrate it with an application toparsing MH.
We show that our approach to mod-eling the interaction between syntactic categoriesand a handful of percolated features already yieldsa notable improvement in parsing accuracy andsubstantially improves the quality of suggestedparses.
Incorporating additional functional andmorphological information, we expect, will helpbridging the gap in performance between Hebrewand configurational languages such as English.Acknowledgements We thank Remko Scha,Jelle Zuidema, Yoav Goldberg, and three anony-mous reviewers for comments on earlier drafts.The first author wishes to thank Jim Blevins, JuliaHockenmeir, Mark Johnson, Kevin Knight, ChrisManning, Joakim Nivre and Gerald Penn for stim-ulating discussion.
Errors are our own.
The workof the first author is funded by the Dutch ScienceFoundation (NWO), grant number 017.001.271.ReferencesAnderson, S. R. 1992.
A-Morphus Morphology.
Cam-bridge University Press.Beard, R. 1988.
The Separation of Derivation andAffixation: Toward a Lexeme-Morpheme Base Mor-phology.
Quarderni di semantica, pages 277?287.6Parsing MSA has been explored with a treebank threetimes as large as ours using a head-driven lexicalized parserobtaining around 78% accuracy (http://papers.ldc.upenn.edu/).
The input setup assumes gold segmentationas well as PoS tags information and some diacritization.Blevins, J. P. 2008.
Periphrasis as Syntactic Ex-ponence.
In Ackerman, F., J.P. Blevins, and G.S.Stump, editors, Patterns in Paradigms.
CSLI.Bod, R. 2003.
An Efficient Implementation of a NewDop Model.
In Proceedings of EACL.Bresnan, Joan.
1979.
A Theory of Grammatical Repre-sentation.
Duplicated Lecture Notes.
Department ofLinguistics and Philosophy, MIT.Charniak, E. 1997.
Statistical Parsing with a Context-Free Grammar and Word Statistics.
In AAAI/IAAI.Collins, M. 2003.
Head-Driven Statistical Models forNatural Language Parsing.
Comp.
Linguistics.Danon, G. 2001.
Syntactic Definiteness in the Gram-mar of Modern Hebrew.
Linguistics, 6(39).Goldberg, Y. and R. Tsarfaty.
2008.
A Single Gener-ative Framework for Joint Morphological Segmenta-tion and Syntactic Parsing.
In Proceedings of ACL.Johnson, M. 1998.
PCFG Models of Linguistic TreeRepresentations.
Computational Linguistics, 24(4).Klein, D. and C. Manning.
2003.
Accurate Unlexical-ized Parsing.
In Proceedings of ACL.McDonald, R., F. Pereira, K. Ribarov, and J. Hajic?.2005.
Non-Projective Dependency Parsing usingSpanning Tree Algorithms.
In Proceedings of HLT.Nivre, J.
2007.
Data-driven Dependency ParsingAcross Languages and Domains; Perspectives fromthe CoNLL-2007 Shared Task.
In Proceedings ofIWPT.Perlmutter, D. M. 1982.
Syntactic Representation,Syntactic levels, and the Notion of Subject.
In Ja-cobson, Pauline and Geoffrey Pullum, editors, TheNature of Syntactic Representation.
Springer.Postal, P. M. and D. M. Perlmutter.
1977.
Towarda Universal Characterization of Passivization.
InBLS3.Postal, P. M. 1982.
Some Arc-Pair Grammar Decrip-tions.
In Jacobson, P. and G. K. Pullum, editors, TheNature of Syntactic Representation.
Dordrecht.Sima?an, K., A. Itai, Y.
Winter, A. Altman, and N. Na-tiv.
2001.
Building a Tree-Bank for Modern HebrewText.
In Traitement Automatique des Langues.Tsarfaty, R. and K. Sima?an.
2007.
Three-DimensionalParametrization for Parsing Morphologically RichLanguages.
In Proceedings of IWPT.Tsarfaty, R. 2006.
Integrated Morphological and Syn-tactic Disambiguation for Modern Hebrew.
In Pro-ceeding of ACL-SRW.Zwicky, A. M. 1993.
Heads, Bases, and Functors.
InCorbett, G.G., N. Fraser, and S. McGlashan, editors,Heads in Grammatical Theory.
Cambridge.896
