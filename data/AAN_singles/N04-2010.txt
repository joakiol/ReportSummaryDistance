Speaker Recognition with Mixtures of Gaussians with Sparse RegressionMatricesConstantinos BoulisUniversity of Washington,Electrical Engineering dept., SSLI Labboulis@ee.washington.eduAbstractWhen estimating a mixture of Gaussians thereare usually two choices for the covariance typeof each Gaussian component.
Either diag-onal or full covariance.
Imposing a struc-ture though may be restrictive and lead to de-graded performance and/or increased compu-tations.
In this work, several criteria to esti-mate the structure of regression matrices of amixture of Gaussians are introduced and eval-uated.
Most of the criteria attempt to estimatea discriminative structure, which is suited forclassification tasks.
Results are reported onthe 1996 NIST speaker recognition task andperformance is compared with structural EM,a well-known, non-discriminative, structure-finding algorithm.1 IntroductionMost state-of-the-art systems in speech and speakerrecognition use mixtures of Gaussians when fitting aprobability distribution to data.
Reasons for this choiceare the easily implementable estimation formulas and themodeling power of mixtures of Gaussians.
For example,a mixture of diagonal Gaussians can still model depen-dencies on the global level.
An established practice whenapplying mixtures of Gaussians is to use either full or di-agonal covariances.
However, imposing a structure maynot be optimum and a more general methodology shouldallow for joint estimation of both the structure and pa-rameter values.1.The first question we have to answer is what type ofstructure we want to estimate.
For mixtures of Gaussiansthere are three choices.
Covariances, inverse covariancesor regression matrices.
For all cases, we can see as se-lecting a structure by introducing zeros in the respectivematrix.
The three structures are distinctively different andzeros in one matrix do not, in general, map to zeros in an-other matrix.
For example, we can have sparse covariance1Here, we describe the Maximum Likelihood estimationmethodology for both structure and parameters.
One alterna-tive is Bayesian estimation.but full inverse covariance or sparse inverse covarianceand full regression matrix.There are no clear theoretical reasons why one choiceof structure is more suitable than others.
However, in-troducing zeros in the inverse covariance can be seen asdeleting arcs in an Undirected Graphical Model (UGM)where each node represents each dimension of a singleGaussian (Bilmes, 2000).
Similarly, introducing zeros inthe regression matrix can be seen as deleting arcs in a Di-rected Graphical Model (DGM).
There is a rich body ofwork on structure learning for UGM and DGM and there-fore the view of a mixture of Gaussians as a mixture ofDGM or UGM may be advantageous.
Under the DGMframework, the problem of Gaussian parameter estima-tion can be cast as a problem of estimating linear regres-sion coefficients.
Since the specific problem of selectingfeatures for linear regression has been encountered in dif-ferent fields in the past, we adopt the view of a mixtureof Gaussians as a mixture of DGM.In (Bilmes, 2000), the problem of introducing zeros inregression matrices of a mixture of Gaussians was pre-sented.
The approach taken was to set to zero the pairswith the lowest mutual information, i.e.
bmi,j = 0 ?
?I(Xi, Xj) ?
0, wherem is the Gaussian index and bi,j isthe (i, j) element of regression matrix B.
The approachwas tested for the task of speech recognition in a lim-ited vocabulary corpus and was shown to offer the sameperformance with the mixture of full-covariance Gaus-sians with 30% less parameters.
full covariances.
Oneissue with the work in (Bilmes, 2000) is that the structure-estimation criterion that was used was not discriminative.For classification tasks, like speaker or speech recog-nition, discriminative parameter estimation approachesachieve better performance than generative ones, but arein general hard to estimate especially for a high num-ber of classes.
In this work, a number of discrimina-tive structure-estimation criteria tailored for the task ofspeaker recognition are introduced.
We avoid the com-plexities of discriminative parameter estimation by esti-mating a discriminative structure and then applying gen-erative parameter estimation techniques.
Thus, overallthe models attempt to model the discriminability betweenclasses without the numerical and implementation diffi-X X X X1 2 3 4Figure 1: A Directed Graphical Modelculties that such techniques have.
A comparison of thenew discriminative structure-estimation criteria with thestructural EM algorithm is also presented.This paper is structured as follows.
In section 2, theview of a Gaussian as a directed graphical model ispresented.
In section 3, discriminative and generativestructure-estimation criteria for the task of speaker recog-nition are detailed, along with a description of the struc-tural EM algorithm.
In section 4, the application task isdescribed and the experiments are presented.
Finally, insection 5, a summary and possible connections of thiswork with the speaker adaptation problem are discussed.2 Gaussians as Directed Graphical ModelsSuppose that we have a mixture of M Gaussians:p(x) =M?mp(z = m)N(x;?m,?m) (1)It is known from linear algebra that any square ma-trix A can be decomposed as A = LDU , where L is alower triangular matrix, D is a diagonal matrix and U isan upper triangular matrix.
In the special case where Ais also symmetric and positive definite the decompositionbecomes A = UTDU where U is an upper triangularmatrix with ones in the main diagonal.
Therefore we canwrite U = I ?B with bij = 0 if i >= j.The exponent of the Gaussian function can now bewritten as (Bilmes, 2000):(x??Bx?)TD(x??Bx?)
(2)where x?
= x ?
?.
The i-th element of (x?
?
Bx?)
can bewritten as:x?i ?V?k=i+1bi,kx?k (3)with V being the dimensionality of each vector.
Equation3 shows that the problem of Gaussian parameter estima-tion can be casted as a linear regression problem.
Regres-sion schemes can be represented as Directed GraphicalModels.
In fact, the multivariate Gaussian can be rep-resented as a DGM as shown in Figure 1.
Absent arcsrepresent zeros in the regression matrix.
For example theB matrix in Figure 1 would have b1,4 = b2,3 = 0.We can use the EM algorithm to estimate the param-eters of a mixture of Gaussian ?
= [?mBmDm].
Thisformulation offers a number of advantages over the tra-ditional formulation with means and covariances.
First,it avoids inversion of matrices and instead solves V + 1linear systems of d equations each where d = 1 : V + 1.If the number of components and dimensionality of inputvectors are high, as it is usually the case in speech recog-nition applications, the amount of computations savedcan be important.
Second, the number of computationsscale down with the number of regression coefficientsset to zero.
This is not true in the traditional formu-lation because introducing zeros in the covariance ma-trix may result in a non-positive definite matrix and it-erative techniques should be used to guarantee consis-tency (Dempster, 1972).
Third, for the traditional for-mulation, adapting a mixture of non-diagonal Gaussianswith linear transformations leads to objective functionsthat cannot be maximized analytically.
Instead, iterativemaximization techniques, such as gradient descent, areused.
With the new formulation even with arbitrary Gaus-sians, closed-form update equations are possible.
Finally,the new formulation offers flexibility in tying mecha-nisms.
Regression matrices Bm and variances Dm canbe tied in different ways, for example all the componentscan share the same regression matrix but estimate a differ-ent variance diagonal matrix for each component.
Simi-lar schemes were found to be succesful for speech recog-nition (Gales, 1999) and this formulation can provide amodel that can extend such tying schemes.
The advan-tages of the new formulation are summarized in Table 1.3 Structure LearningIn general, structure learning in DGM is an NP-hardproblem even when all the variables are observed (Chick-ering et al, 1994).
Our case is further complicated bythe fact that we have a hidden variable (the Gaussian in-dex).
Optimum structure-finding algorithms, like the onein (Meila and Jordan, 2000) assume a mixture of treesand therefore are making limiting assumptions about thespace of possible structures.
In this paper, no prior as-sumptions about the space of possible structures are madebut this leads to absence of guarantee for an optimumstructure.
Two approaches for structure-learning are in-troduced.The first approach is to learn a discriminative struc-ture, i.e.
a structure that can discriminate between classeseven though the parameters are estimated in an ML fash-ion.
The algorithm starts from the fully connected modeland deletes arcs, i.e.
sets bi,jm = 0 ?m = 1 : M (M is thenumber of Gaussian components in a mixture).
After set-ting regression coefficients to zero, maximum likelihoodparameter estimation of the sparse mixture is employed.A number of different structure-estimation criteria weretested for the speaker recognition task (at the right of eachequation a shorthand for each criterion is defined):?
= [?m, Bm, Dm] ?
= [?m,?m]Computations?
Each component m=1:M re-quires solution of V+1 linearsystems of d equations each,d=1:V+1.?
Computations scale down withthe number of regression coef-ficients set to zero.?
Each component m=1:M re-quires an inversion of a rank Vmatrix.?
Iterative techniques must beemployed for the sparse case.Adaptation Easy EM equations for estimating alinear transformation of a mixture ofarbitrary Gaussians.Gradient descent techniques for theM-step of EM algorithm for estimat-ing a linear transformation of a mix-ture of arbitrary Gaussians.Tying More flexible tying mechanismsacross components, i.e.
componentscan share B but estimate Dm.Components can share the entire ?.Table 1: Comparison between viewing a mixture of Gaussians as a mixture of DGMs and the traditional representationMI = I(Xi;Xj |target speaker s) (4)DMIimp = I(Xi;Xj |target speaker s)?
(1/N)?nI(Xi;Xj |impostor n) (5)MIimp =?nI(Xi;Xj |impostor n) (6)DMIconf = I(Xi;Xj |target speaker s)?I(Xi;Xj |target speaker k) (7)where I(Xi;Xj) is the mutual information between el-ements Xi and Xj of input vector X .
The mutual in-formations are estimated by first fitting a mixture of 30diagonal Gaussians and then applying the methods de-scribed in (Bilmes, 1999).
All but MI and MIimp arediscriminative criteria and all are based on finding thepairs (i, j) with the lowest values and zeroing the respec-tive regression coefficients, for every component of themixture.
MIimp assigns the same speaker-independentstructure for all speakers.
For DMIconf target speakerk is the most confusable for target speaker s in terms ofhits, i.e.
when the truth is s, speaker k fires more thanany other target speaker.
We can see that different criteriaaim at different goals.
MI attempts to avoid the overfit-ting problem by zeroing regression coefficients betweenleast marginally dependent feature elements.
DMIimpattempts to discriminate against impostors, MIimp at-tempts to build a speaker-independent structure whichwill be more robustly estimated since there are more datato estimate the mutual informations and DMIconf at-tempts to discriminate against the most confusable targetspeaker.
The most confusable target speaker k for a giventarget speaker s should be determined from an indepen-dent held-out set.There are three main drawbacks that are shared by allof the above criteria.
First, they are limited by the factthat all Gaussians will have the same structure.
Second,since we are estimating sparse regression matrices, it isknown that the absence of an arc is equivalent to con-ditional independencies, yet the above criteria can onlytest for marginal independencies.
Third, we introduceanother free parameter (the number of regression coef-ficients to be set to zero) which can be determined froma held-out set but will require time consuming trial anderror techniques.
Nevertheless, they may lead to betterdiscrimination between speakers.The second approach we followed was one based onan ML fashion which may not be optimum for classi-fication tasks, but can assign a different structure foreach component.
We used the structural EM (Friedman,1997), (Thiesson et al, 1998) and adopt it for the case ofmixtures of Gaussians.
Structural EM is an algorithm thatgeneralizes on the EM algorithm by searching in the com-bined space of structure and parameters.
One approachto the problem of structure finding would be to start fromthe full model, evaluate every possible combination ofarc removals in every Gaussian, and pick the ones withthe least decrease in likelihood.
Unfortunately, this ap-proach can be very expensive since every time we removean arc on one of the Gaussians we have to re-estimate allthe parameters, so the EM algorithm must be used foreach combination.
Therefore, this approach alternatesparameter search with structure search and can be veryexpensive even if we follow greedy approaches.
On theother hand, structural EM interleaves parameter searchwith structure search.
Instead of following the sequenceEstep ?Mstep ?
structure search, structural EM fol-lows Estep ?
structure search ?
Mstep.
By treatingexpected data as observed data, the scoring of likelihooddecomposes and therefore local changes do not influencethe likelihood on other parameters.
In essence, structuralEM has the same core idea as standard EM.
If M is thestructure, ?
are the parameters and n is the iteration in-dex, then the naive approach would be to do:{Mn,?n} ?
{Mn+1,?n+1} (8)On the other hand, structural EM follows the sequence:{Mn,?n} ?
{Mn+1,?n} ?
{Mn+1,?n+1} (9)If we replace M with H , i.e.
the hidden variables orsufficient statistics, we will recognize the sequence ofsteps as the standard EM algorithm.
For a more thor-ough discussion of structural EM, the reader is referredto (Friedman, 1997).
The paper in (Friedman, 1997) hasa general discussion on the structural EM algorithm for anarbitrary graphical model.
In this paper, we introduced agreedy pruning algorithm with step size K for mixturesof Gaussians.
The algorithm is summarized in Table 2.One thing to note about the scoring criterion is that it islocal, i.e.
zeroing regression coefficient m, i, j will notinvolve computations on other parameters.4 ExperimentsWe evaluated our approach in the male subset of the 1996NIST speaker recognition task (Przybocki and Martin,1998).
The problem can be described as following.
Given21 target speakers, perform 21 binary classifications (onefor each target speaker) for each one of the test sentences.Each one of the binary classifications is a YES if the sen-tence belongs to the target speaker and NO otherwise.Under this setting, one sentence may be decided to havebeen generated by more than one speaker, in which casethere will be at least one false alarm.
Also, some of thetest sentences were spoken by non-target speakers (im-postors) in which case the correct answer would be 21NO.
All speakers are male and the data are from theSwitchboard database (Godfrey et al, 1992).
There areapproximately 2 minutes of training data for each targetspeaker.
All the training data for a speaker come fromthe same session and the testing data come from differentsessions, but from the same handset type and phone num-ber (matched conditions).
The algorithms were evaluatedon sentence sizes of three and thirty seconds.
The fea-tures are 20-dimensional MFCC vectors, cepstrum meannormalized and with all silences and pauses removed.
Inthe test data there are impostors who don?t appear in thetraining data and may be of different gender than the tar-get speakers.A mixture of Gaussians is trained on each one of thetarget speakers.
For impostor modeling, a separate modelis estimated for each gender.
There are 43 impostors foreach gender, each impostor with 2 minutes of speech.Same-gender speakers are pooled together and a mixtureof 100 diagonal Gaussians is estimated on each pool.
Im-postor models remained fixed for all the experiments re-ported in this work.
During testing and because someof the impostors are of different gender than the targetspeakers, each test sentence is evaluated against both im-postor models and the one with the highest log-likelihoodis chosen.
For each test sentence the log-likelihood ofeach target speaker?s model is subtracted from the log-likelihood of the best impostor model.
A decision forYES is made if the difference of the log-likelihoods isabove a threshold.
Although in real operation of the sys-tem the thresholds are parameters that need to be esti-mated from the training data, in this evaluation the thresh-olds are optimized for the current test set.
Therefore theresults reported should be viewed as a best case scenario,but are nevertheless useful for comparing different ap-proaches.The metric used in all experiments was Equal ErrorRate (EER).
EER is defined as the point where the proba-bility of false alarms is equal to the probability of misseddetections.
Standard NIST software tools were used forthe evalution of the algorithms (Martin et al, 1997).It should be noted that the number of components perGaussian is kept the same for all speakers.
A scheme thatallowed for different number of Gaussians per speakerdid not show any gains.
Also, the number of componentsis optimized on the test set which will not be the case inthe real operation of the system.
However, since there areonly a few discrete values for the number of componentsand EER was not particularly sensitive to that parameter,we do not view this as a major problem.Table 3 shows the EER obtained for different base-line systems.
Each cell contains two EER numbers, theleft is for 30-second test utterances and the right for 3-second.
For the Diagonal case 35 components wereused, while for the full case 12 components were used.The Random case corresponds to randomly zeroing10% of the regression coefficients of a mixture of 16 com-ponents.
This particular combination of number of pa-rameters pruned and number of components was shownto provide the best results for a subset of the test set.All structure-finding experiments are with the same num-ber of components and percent of regression coefficientspruned.Table 4 shows the EER obtained for different baselineAlgorithm: Finding both structure and parameter values using structural EMStart with the full model for a given number of Gaussianswhile (number of pruned regression coefficients < T )E?
step: Collect sufficient statistics for given structure, i.e, ?m(n) = p(zn = m|xn,Mold)StructureSearch: Remove one arc from a Gaussian at a time, i.e.
set bmi,j = 0.The score associated with zeroing a single regression coefficient is.Scorem,i,j = 2Dimbmi,j?Nn ?m(n)x?jn,m(x?in,m ?Bimx?n,m) +Dim(bmi,j)2?Nn ?m(n)x?jn,mOrder coefficients in ascending order of score.
P is the set of the first K coefficients.Set the new structure Mnew as Mnew = Mold\{P}.M?
step: Calculate the new parameters given Mnew.This step can be followed by a number of EM iterations to obtain better parameter values.endTable 2: The Structural EM algorithm for a mixture of GaussiansFull Diagonal Random6.3/10.3 5.6/9.0 6.3/10.3Table 3: Baseline EER, left number is for 30-second testutterances and right number for 3-secondsparse structures.
SEM is structural EM.
The first col-umn is zeroing the pairs with the minimum values of thecorresponding criterion and the second column is zeroingthe pairs with the maximum values.
The second columnis more of a consistency check.
If the min entry of crite-rion A is lower than the min entry of criterion B then themax entry of criterion A should be higher than the maxentry of criterion B.
For the structural EM, pruning stepsizes of 50 and 100 were tested and no difference wasobserved.min maxMI 6.3/10.0 6.6/10.6DMIimp 5.9/9.0 6.6/10.3MIimp 6.3/10.3 6.3/10.3DMIconf 5.9/9.3 6.6/10.3SEM 6.3/9.6 6.6/10.3Table 4: EER for different sparse structures, left numberis for 30 second test utterances and right number for 3-second.From Table 4 we can see improved results from thefull-covariance case but results are not better than thediagonal-covariance case.
All criteria appear to performsimilarly.
Table 4 also shows that zeroing the regressioncoefficients with the maximum of each criterion func-tion does not lead to systems with much different perfor-mance.
Also from Table 3 we can see that randomly ze-roing regression coefficients performs approximately thesame as taking the minimum or maximum.
These num-bers, seem to suggest that the structure of a mixture ofGaussians is not a critical issue for speaker recognition,at least with the current structure-estimation criteria used.5 Summary-Future workIn this work the problem of estimating sparse regressionmatrices of mixtures of Gaussians was addressed.
Dif-ferent structure-estimation criteria were evaluated, bothdiscriminative and generative.
The general problem offinding the optimum structure of a mixture of Gaussianshas direct applications in speaker identification as well asspeech recognition.Interesting connections can be drawn with MaximumLikelihood Linear Regression (MLLR) speaker adapta-tion (Leggetter and Woodland, 1995).
Not surprisingly,the estimation equations for the regression matrix bareresemblance with the MLLR equations.
However, re-searchers have thus far barely looked into the problem ofstructure-finding for speaker adaptation, focusing mostlyon parameter adaptation.
An interesting new topic forspeaker adaptation could be joint structure and parameteradaptation.ReferencesJ.
Bilmes.
1999.
Natural Statistical Models for Auto-matic Speech Recognition.
Ph.D. thesis, U.C.
Berke-ley, Dept.
of EECS.J.
Bilmes.
2000.
Factored sparse inverse covariance ma-trices.
In Proceedings of International Conference onAcoustics, Speech and Signal Processing (ICASSP).D.M.
Chickering, D. Geiger, and D.E.
Heckerman.
1994.Learning bayesian networks is NP-hard.
Technical Re-port MSR-TR-94-17, Microsoft Research.A.
P. Dempster.
1972.
Covariance selection.
Journal ofBiometrics, 28:157?75.N.
Friedman.
1997.
Learning belief networks in the pres-ence of missing values and hidden variables.
In Proc.14th International Conference on Machine Learning(ICML), pages 125?133.M.
Gales.
1999.
Semi-tied covariance matrices for hid-den markov models.
IEEE Transactions on Speech andAudio Processing, 7:272?281.J.
Godfrey, E. Holliman, and J. McDaniel.
1992.
Switch-board: Telephone speech corpus for research develop-ment.
In Proceedings of ICASSP, pages 517?520.C.
J. Leggetter and P. C. Woodland.
1995.
Maximumlikelihood linear regression for speaker adaptation ofcontinuous density hidden markov models.
ComputerSpeech and Language, 9:171?185.A.
Martin, G. Doddington, T. Kamm, M. Ordowski, andM.
Przybocki.
1997.
The DET curve in assessmentof detection task performance.
In Proceedings of Eu-rospeech ?97, pages 1895?1898.M.
Meila and M. I. Jordan.
2000.
Learning with mixturesof trees.
Journal of Machine Learning Research, 1:1?48.M.
Przybocki and A. Martin.
1998.
NIST speaker recog-nition evaluations:1996-2001.
In Proceedings of theInternational Conference on Language Resources andEvaluation (LREC), pages 331?335.B.
Thiesson, C. Meek, D. Chickering, and D. Heckerman.1998.
Learning mixtures of dag models.
TechnicalReport MSR-TR-97-30, Microsoft Research.
