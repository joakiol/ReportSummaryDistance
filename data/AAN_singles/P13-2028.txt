Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 153?158,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsPost-Retrieval Clustering Using Third-Order Similarity MeasuresJose?
G. MorenoNormandie UniversityUNICAEN, GREYC CNRSF-14032 Caen, Francejose.moreno@unicaen.frGae?l DiasNormandie UniversityUNICAEN, GREYC CNRSF-14032 Caen, Francegael.dias@unicaen.frGuillaume CleuziouUniversity of Orle?ansLIFOF-45067 Orle?ans, Francecleuziou@univ-orleans.frAbstractPost-retrieval clustering is the task of clus-tering Web search results.
Within thiscontext, we propose a new methodologythat adapts the classical K-means algo-rithm to a third-order similarity measureinitially developed for NLP tasks.
Resultsobtained with the definition of a new stop-ping criterion over the ODP-239 and theMORESQUE gold standard datasets evi-dence that our proposal outperforms all re-ported text-based approaches.1 IntroductionPost-retrieval clustering (PRC), also known assearch results clustering or ephemeral clustering,is the task of clustering Web search results.
Fora given query, the retrieved Web snippets are au-tomatically clustered and presented to the userwith meaningful labels in order to minimize theinformation search process.
This technique canbe particularly useful for polysemous queries butit is hard to implement efficiently and effectively(Carpineto et al, 2009).
Indeed, as opposed toclassical text clustering, PRC must deal with smallcollections of short text fragments (Web snippets)and be processed in run-time.As a consequence, most of the successfulmethodologies follow a monothetic approach (Za-mir and Etzioni, 1998; Ferragina and Gulli, 2008;Carpineto and Romano, 2010; Navigli and Crisa-fulli, 2010; Scaiella et al, 2012).
The underlyingidea is to discover the most discriminant topicalwords in the collection and group together Websnippets containing these relevant terms.
On theother hand, the polythetic approach which mainidea is to represent Web snippets as word featurevectors has received less attention, the only rele-vant work being (Osinski and Weiss, 2005).
Themain reasons for this situation are that (1) wordfeature vectors are hard to define in small collec-tions of short text fragments (Timonen, 2013), (2)existing second-order similarity measures such asthe cosine are unadapted to capture the seman-tic similarity between small texts, (3) Latent Se-mantic Analysis has evidenced inconclusive re-sults (Osinski and Weiss, 2005) and (4) the la-beling process is a surprisingly hard extra task(Carpineto et al, 2009).This paper is motivated by the fact that the poly-thetic approach should lead to improved results ifcorrectly applied to small collections of short textfragments.
For that purpose, we propose a newmethodology that adapts the classical K-meansalgorithm to a third-order similarity measure ini-tially developed for Topic Segmentation (Dias etal., 2007).
Moreover, the adapted K-means algo-rithm allows to label each cluster directly from itscentroids thus avoiding the abovementioned extratask.
Finally, the evolution of the objective func-tion of the adapted K-means is modeled to auto-matically define the ?best?
number of clusters.Finally, we propose different experiments overthe ODP-239 (Carpineto and Romano, 2010)and MORESQUE (Navigli and Crisafulli, 2010)datasets against the most competitive text-basedPRC algorithms: STC (Zamir and Etzioni, 1998),LINGO (Osinski and Weiss, 2005), OPTIMSRC(Carpineto and Romano, 2010) and the classicalbisecting incremental K-means (which may beseen as a baseline for the polythetic paradigm)1.A new evaluation measure called the b-cubed F -measure (Fb3) and defined in (Amigo?
et al, 2009)is then calculated to evaluate both cluster homo-geneity and completeness.
Results evidence thatour proposal outperforms all state-of-the-art ap-proaches with a maximum Fb3 = 0.452 for ODP-239 and Fb3 = 0.490 for MORESQUE.1The TOPICAL algorithm proposed by (Scaiella etal., 2012) is a knowledge-driven methodology based onWikipedia.1532 Polythetic Post-Retrieval ClusteringThe K-means is a geometric clustering algorithm(Lloyd, 1982).
Given a set of n data points, thealgorithm uses a local search approach to partitionthe points into K clusters.
A set of K initial clus-ter centers is chosen.
Each point is then assignedto the center closest to it and the centers are recom-puted as centers of mass of their assigned points.The process is repeated until convergence.
To as-sure convergence, an objective function Q is de-fined which decreases at each processing step.
Theclassical objective function is defined in Equation(1) where pik is a cluster labeled k, xi ?
pik isan object in the cluster, mpik is the centroid of thecluster pik and E(., .)
is the Euclidean distance.Q =K?k=1?xi?pikE(xi,mpik )2.
(1)Within the context of PRC, the K-means algo-rithm needs to be adapted to integrate third-ordersimilarity measures (Mihalcea et al, 2006; Diaset al, 2007).
Third-order similarity measures,also called weighted second-order similarity mea-sures, do not rely on exact matches of word fea-tures as classical second-order similarity measures(e.g.
the cosine metric), but rather evaluate simi-larity based on related matches.
In this paper, wepropose to use the third-order similarity measurecalled InfoSimba introduced in (Dias et al, 2007)for Topic Segmentation and implement its simpli-fied version S3s in Equation 2.S3s (Xi, Xj) =1p2p?k=1p?l=1Xik ?Xjl ?
S(Wik,Wjl).
(2)Given two Web snippets Xi and Xj , their sim-ilarity is evaluated by the similarity of its con-stituents based on any symmetric similarity mea-sure S(., .)
where Wik (resp.
Wjl) corresponds tothe word at the kth (resp.
lth) position in the vectorXi (resp.
Xj) and Xik (resp.
Xjl) is the weight ofword Wik (resp.
Wjl) in the set of retrieved Websnippets.
A direct consequence of the change insimilarity measure is the definition of a new ob-jective function QS3s to ensure convergence.
Thisfunction is defined in Equation (3) and must bemaximized2.2A maximization process can easily be transformed into aminimization oneQS3s =K?k=1?xi?pikS3s (xi,mpik ).
(3)A cluster centroid mpik is defined by a vector ofp words (wpik1 , .
.
.
, wpikp ).
As a consequence, eachcluster centroid must be instantiated in such a waythat QS3s increases at each step of the clusteringprocess.
The choice of the best p words repre-senting each cluster is a way of assuring conver-gence.
For that purpose, we define a procedurewhich consists in selecting the best p words fromthe global vocabulary V in such a way that QS3sincreases.
The global vocabulary is the set of allwords which appear in any context vector.So, for each word w ?
V and any symmet-ric similarity measure S(., .
), its interestingness?k(w) is computed as regards to cluster pik.
Thisoperation is defined in Equation (4) where si ?
pikis any Web snippet from cluster pik.
Finally, the pwords with higher ?k(w) are selected to constructthe cluster centroid.
In such a way, we can easilyprove that QS3s is maximized.
Note that a wordwhich is not part of cluster pik may be part of thecentroid mpik .
?k(w) = 1p?si?pik?wiq?siS(wiq, w).
(4)Finally, we propose to rely on a modified ver-sion of the K-means algorithm called Global K-means (Likasa et al, 2003), which has proved tolead to improved results.
To solve a clusteringproblem with M clusters, all intermediate prob-lems with 1, 2, ...,M ?
1 clusters are sequentiallysolved.
The underlying idea is that an optimal so-lution for a clustering problem with M clusterscan be obtained using a series of local searches us-ing the K-means algorithm.
At each local search,the M ?
1 cluster centers are always initiallyplaced at their optimal positions corresponding tothe clustering problem with M ?
1 clusters.
Theremaining M th cluster center is initially placed atseveral positions within the data space.
In addi-tion to effectiveness, the method is deterministicand does not depend on any initial conditions orempirically adjustable parameters.
Moreover, itsadaptation to PRC is straightforward.3 Stopping CriterionOnce clustering has been processed, selecting thebest number of clusters still remains to be decided.154For that purpose, numerous procedures have beenproposed (Milligan and Cooper, 1985).
However,none of the listed methods were effective or adapt-able to our specific problem.
So, we proposeda procedure based on the definition of a ratio-nal function which models the quality criterionQS3s .
To better understand the behaviour of QS3sat each step of the adapted GK-means algorithm,we present its values for K = 10 in Figure 1.Figure 1: QS3s and its modelisation.QS3s can be modelled as in Equation (5) whichconverges to a limit ?
whenK increases and startsfrom Q1S3s (i.e.
QS3s at K = 1).
The underlyingidea is that the best number of clusters is given bythe ?
value which maximizes the difference withthe average ?mean.
So, ?, ?
and ?
need to beexpressed independently of unknown variables.
?K, f(K) = ??
?K?
.
(5)As ?
can theoretically or operationally be de-fined and it can easily be proved that ?
= ?
?Q1S3s ,?
needs to be defined based on ?
or ?.
This canalso be easily proved and the given result is ex-pressed in Equation (6).?
=log(?
?Q1S3s )?
log(?
?QKS3s)log(K) .
(6)Now, the value of ?
which best approximatesthe limit of the rational function must be defined.For that purpose, we computed its maximum theo-retical and experimental values as well as its ap-proximated maximum experimental value basedon the ?2-Aitken (Aitken, 1926) procedure to ac-celerate convergence as explained in (Kuroda etal., 2008).
Best results were obtained with themaximum experimental value which is defined asbuilding the cluster centroid mpik for each Websnippet individually.
Finally, the best number ofclusters is defined as in Algorithm (1) and eachone receives its label based on the p words withgreater interestingness of its centroid mpik .Algorithm 1 The best K selection procedure.1.
Calculate ?K for each K2.
Evaluate the mean of all ?K i.e.
?mean3.
Select ?K which maximizes ?K ?
?mean4.
Return K as the best number of partitionsThis situation is illustrated in Figure (1) wherethe red line corresponds to the rational functionalfor ?mean and the blue line models the best ?value (i.e.
the one which maximizes the differencewith ?mean).
In this case, the best number wouldcorrespond to ?6 and as a consequence, the bestnumber of clusters would be 6.
In order to illus-trate the soundness of the procedure, we presentthe different values for ?
at each K iteration andthe differences between consecutive values of ?
ateach iteration in Figure 2.
We clearly see that thehighest inclination of the curve is between clus-ter 5 and 6 which also corresponds to the highestdifference between two consecutive values of ?.Figure 2: Values of ?
(on the left) and differencesbetween consecutive values of ?
(on the right).4 EvaluationEvaluating PRC systems is a difficult task as statedin (Carpineto et al, 2009).
Indeed, a successfulPRC system must evidence high quality level clus-tering.
Ideally, each query subtopic should be rep-resented by a unique cluster containing all the rel-evant Web pages inside.
However, this task is farfrom being achievable.
As such, this constraintis reformulated as follows: the task of PRC sys-tems is to provide complete topical cluster cov-erage of a given query, while avoiding excessive155Fb3 K Stop Criterion2 3 4 5 6 7 8 9 10 Fb3 Avg.
KSCP p2 0.387 0.396 0.398 0.396 0.391 0.386 0.382 0.378 0.374 0.395 4.7993 0.400 0.411 0.412 0.409 0.406 0.400 0.397 0.391 0.388 0.411 4.6904 0.405 0.416 0.423 0.425 0.423 0.420 0.416 0.414 0.411 0.441 4.7665 0.408 0.422 0.431 0.431 0.429 0.429 0.423 0.422 0.421 0.452 4.778PMI p2 0.391 0.399 0.397 0.393 0.388 0.383 0.377 0.373 0.366 0.393 4.7783 0.408 0.418 0.422 0.418 0.414 0.410 0.405 0.398 0.392 0.416 4.8794 0.420 0.434 0.439 0.439 0.435 0.430 0.425 0.420 0.412 0.436 4.8745 0.423 0.444 0.451 0.451 0.451 0.445 0.441 0.434 0.429 0.450 4.778Table 1: Fb3 for SCP and PMI for the global search and the stopping criterion for the ODP-239 dataset.Adapated GK-meansSTC LINGO BIK OPTIMSRCSCP PMIODP-239p p2 3 4 5 2 3 4 5F1 0.312 0.341 0.352 0.366 0.332 0.358 0.378 0.390 0.324 0.273 0.200 0.313F2 0.363 0.393 0.404 0.416 0.363 0.395 0.421 0.435 0.319 0.167 0.173 0.341F5 0.411 0.441 0.453 0.462 0.390 0.430 0.459 0.476 0.322 0.153 0.165 0.380Fb3 0.395 0.411 0.441 0.452 0.393 0.416 0.436 0,450 0.403 0.346 0.307 N/AMORESQUEF1 0.627 0.649 0.665 0.664 0.615 0.551 0.543 0.571 0.455 0.326 0.317 N/AF2 0.685 0.733 0.767 0.770 0.644 0.548 0.521 0.551 0.392 0.260 0.269 N/AF5 0.747 0.817 0.865 0.872 0.679 0.563 0.519 0.553 0.370 0.237 0.255 N/AFb3 0.482 0.482 0.473 0.464 0.490 0.465 0.462 0.485 0.460 0.399 0.315 N/ATable 2: PRC comparative results for F?
and Fb3 over the ODP-239 and MORESQUE datasets.redundancy of the subtopics in the result list ofclusters.
So, in order to evaluate our methodol-ogy, we propose two different evaluations.
First,we want to evidence the quality of the stoppingcriterion when compared to an exhaustive searchover all tunable parameters.
Second, we propose acomparative evaluation with existing state-of-the-art algorithms over gold standard datasets and re-cent clustering evaluation metrics.4.1 Text ProcessingBefore the clustering process takes place, Websnippets are represented as word feature vectors.In order to define the set of word features, theWeb service proposed in (Machado et al, 2009) isused3.
In particular, it assigns a relevance score toany token present in the set of retrieved Web snip-pets based on the analysis of left and right tokencontexts.
A specific threshold is then applied towithdraw irrelevant tokens and the remaining onesform the vocabulary V .
Then, each Web snippet isrepresented by the set of its p most relevant to-kens in the sense of the W (.)
value proposed in(Machado et al, 2009).
Note that within the pro-posed Web service, multiword units are also iden-tified.
They are exclusively composed of relevantindividual tokens and their weight is given by thearithmetic mean of their constituents scores.3Access to this Web service is available upon request.4.2 Intrinsic EvaluationThe first set of experiments focuses on understand-ing the behaviour of our methodology within agreedy search strategy for different tunable param-eters defined as a tuple < p,K, S(Wik,Wjl) >.In particular, p is the size of the word feature vec-tors representing both Web snippets and centroids(p = 2..5), K is the number of clusters to befound (K = 2..10) and S(Wik,Wjl) is the col-location measure integrated in the InfoSimba sim-ilarity measure.
In these experiments, two asso-ciation measures which are known to have dif-ferent behaviours (Pecina and Schlesinger, 2006)are tested.
We implement the Symmetric Condi-tional Probability (Silva et al, 1999) in Equation(7) which tends to give more credits to frequent as-sociations and the Pointwise Mutual Information(Church and Hanks, 1990) in Equation (8) whichover-estimates infrequent associations.
Then, best< p,K, S(Wik,Wjl) > configurations are com-pared to our stopping criterion.SCP (Wik,Wjl) =P (Wik,Wjl)2P (Wik)?
P (Wjl).
(7)PMI(Wik,Wjl) = log2P (Wik,Wjl)P (Wik)?
P (Wjl).
(8)In order to perform this task, we evaluate per-formance based on the Fb3 measure defined in(Amigo?
et al, 2009) over the ODP-239 gold stan-dard dataset proposed in (Carpineto and Romano,1562010).
In particular, (Amigo?
et al, 2009) indi-cate that common metrics such as the F?-measureare good to assign higher scores to clusters withhigh homogeneity, but fail to evaluate cluster com-pleteness.
First results are provided in Table 1 andevidence that the best configurations for different< p,K, S(Wik,Wjl) > tuples are obtained forhigh values of p, K ranging from 4 to 6 clustersand PMI steadily improving over SCP .
How-ever, such a fuzzy configuration is not satisfac-tory.
As such, we proposed a new stopping cri-terion which evidences coherent results as it (1)does not depend on the used association measure(FSCPb3 = 0.452 and FPMIb3 = 0.450), (2) discov-ers similar numbers of clusters independently ofthe length of the p-context vector and (3) increasesperformance with high values of p.4.3 Comparative EvaluationThe second evaluation aims to compare ourmethodology to current state-of-the-art text-basedPRC algorithms.
We propose comparative exper-iments over two gold standard datasets (ODP-239(Carpineto and Romano, 2010) and MORESQUE(Di Marco and Navigli, 2013)) for STC (Za-mir and Etzioni, 1998), LINGO (Osinski andWeiss, 2005), OPTIMSRC (Carpineto and Ro-mano, 2010) and the Bisecting Incremental K-means (BIK) which may be seen as a baseline forthe polythetic paradigm.
A brief description ofeach PRC algorithm is given as follows.STC: (Zamir and Etzioni, 1998) defined theSuffix Tree Clustering algorithm which is still adifficult standard to beat in the field.
In partic-ular, they propose a monothetic clustering tech-nique which merges base clusters with high stringoverlap.
Indeed, instead of using the classical Vec-tor Space Model (VSM) representation, they pro-pose to represent Web snippets as compact tries.LINGO: (Osinski and Weiss, 2005) proposed apolythetic solution called LINGO which takes intoaccount the string representation proposed by (Za-mir and Etzioni, 1998).
They first extract frequentphrases based on suffix-arrays.
Then, they reducethe term-document matrix (defined as a VSM) us-ing Single Value Decomposition to discover latentstructures.
Finally, they match group descriptionswith the extracted topics and assign relevant doc-uments to them.OPTIMSRC: (Carpineto and Romano, 2010)showed that the characteristics of the outputs re-turned by PRC algorithms suggest the adoption ofa meta clustering approach.
As such, they intro-duce a novel criterion to measure the concordanceof two partitions of objects into different clustersbased on the information content associated to theseries of decisions made by the partitions on singlepairs of objects.
Then, the meta clustering phaseis casted to an optimization problem of the concor-dance between the clustering combination and thegiven set of clusterings.With respect to implementation, we used theCarrot2 APIs4 which are freely available for STC,LINGO and the classical BIK.
It is worth notic-ing that all implementations in Carrot2 are tunedto extract exactly 10 clusters.
For OPTIMSRC,we reproduced the results presented in the paperof (Carpineto and Romano, 2010) as no imple-mentation is freely available.
The results are il-lustrated in Table 2 including both F?-measureand Fb3 .
They evidence clear improvements ofour methodology when compared to state-of-the-art text-based PRC algorithms, over both datasetsand all evaluation metrics.
But more important,even when the p-context vector is small (p = 3),the adapted GK-means outperforms all other ex-isting text-based PRC which is particularly impor-tant as they need to perform in real-time.5 ConclusionsIn this paper, we proposed a new PRC ap-proach which (1) is based on the adaptation ofthe K-means algorithm to third-order similar-ity measures and (2) proposes a coherent stop-ping criterion.
Results evidenced clear improve-ments over the evaluated state-of-the-art text-based approaches for two gold standard datasets.Moreover, our best F1-measure over ODP-239(0.390) approximates the highest ever-reached F1-measure (0.413) by the TOPICAL knowledge-driven algorithm proposed in (Scaiella et al,2012)5.
These results are promising and in futureworks, we propose to define new knowledge-basedthird-order similarity measures based on studies inentity-linking (Ferragina and Scaiella, 2010).4http://search.carrot2.org/stable/search [Last access:15/05/2013].5Notice that the authors only propose the F1-measure al-though different results can be obtained for different F?-measures and Fb3 as evidenced in Table 2.157ReferencesA.C.
Aitken.
1926.
On bernoulli?s numerical solutionof algebraic equations.
Research Society Edinburgh,46:289?305.E.
Amigo?, J. Gonzalo, J. Artiles, and F. Verdejo.
2009.A comparison of extrinsic clustering evaluation met-rics based on formal constraints.
Information Re-trieval, 12(4):461?486.C.
Carpineto and G. Romano.
2010.
Optimal metasearch results clustering.
In 33rd International ACMSIGIR Conference on Research and Development inInformation Retrieval (SIGIR), pages 170?177.C.
Carpineto, S. Osinski, G. Romano, and D. Weiss.2009.
A survey of web clustering engines.
ACMComputer Survey, 41(3):1?38.K.
Church and P. Hanks.
1990.
Word associationnorms mutual information and lexicography.
Com-putational Linguistics, 16(1):23?29.A.
Di Marco and R. Navigli.
2013.
Clustering anddiversifying web search results with graph-basedword sense induction.
Computational Linguistics,39(4):1?43.G.
Dias, E. Alves, and J.G.P.
Lopes.
2007.
Topicsegmentation algorithms for text summarization andpassage retrieval: An exhaustive evaluation.
In Pro-ceedings of 22nd Conference on Artificial Intelli-gence (AAAI), pages 1334?1339.P.
Ferragina and A. Gulli.
2008.
A personalized searchengine based on web-snippet hierarchical clustering.Software: Practice and Experience, 38(2):189?225.P.
Ferragina and U. Scaiella.
2010.
Tagme: On-the-fly annotation of short text fragments (by wikipediaentities).
In Proceedings of the 19th ACM Inter-national Conference on Information and KnowledgeManagement (CIKM), pages 1625?1628.M.
Kuroda, M. Sakakihara, and Z. Geng.
2008.
Ac-celeration of the em and ecm algorithms using theaitken ?2 method for log-linear models with par-tially classified data.
Statistics & Probability Let-ters, 78(15):2332?2338.A.
Likasa, Vlassis.
N., and J. Verbeek.
2003.The global k-means clustering algorithm.
PatternRecognition, 36:451?461.S.P.
Lloyd.
1982.
Least squares quantization inpcm.
IEEE Transactions on Information Theory,28(2):129?137.D.
Machado, T. Barbosa, S. Pais, B. Martins, andG.
Dias.
2009.
Universal mobile information re-trieval.
In Proceedings of the 5th International Con-ference on Universal Access in Human-ComputerInteraction (HCI), pages 345?354.R.
Mihalcea, C. Corley, and C. Strapparava.
2006.Corpus-based and knowledge-based measures oftext semantic similarity.
In Proceedings of the21st National Conference on Artificial Intelligence(AAAI), pages 775?780.G.W.
Milligan and M.C.
Cooper.
1985.
An exami-nation of procedures for determining the number ofclusters in a data set.
Psychometrika, 50(2):159?179.R.
Navigli and G. Crisafulli.
2010.
Inducing wordsenses to improve web search result clustering.In Proceedings of the 2010 Conference on Em-pirical Methods in Natural Language Processing(EMNLP), pages 116?126.S.
Osinski and D. Weiss.
2005.
A concept-driven algo-rithm for clustering search results.
IEEE IntelligentSystems, 20(3):48?54.P.
Pecina and P. Schlesinger.
2006.
Combining as-sociation measures for collocation extraction.
InProceedings of the Joint Conference of the Inter-national Committee on Computational Linguisticsand the Association for Computational Linguistics(COLING/ACL), pages 651?658.U.
Scaiella, P. Ferragina, A. Marino, and M. Ciaramita.2012.
Topical clustering of search results.
In Pro-ceedings of the 5th ACM International Conferenceon Web Search and Data Mining (WSDM), pages223?232.J.
Silva, G. Dias, S.
Guillore?, and J.G.P.
Lopes.
1999.Using localmaxs algorithm for the extraction of con-tiguous and non-contiguous multiword lexical units.In Proceedings of 9th Portuguese Conference in Ar-tificial Intelligence (EPIA), pages 113?132.M.
Timonen.
2013.
Term Weighting in Short Docu-ments for Document Categorization, Keyword Ex-traction and Query Expansion.
Ph.D. thesis, Uni-versity of Helsinki, Finland.O.
Zamir and O. Etzioni.
1998.
Web document clus-tering: A feasibility demonstration.
In 21st AnnualInternational ACM SIGIR Conference on Researchand Development in Information Retrieval (SIGIR),pages 46?54.158
