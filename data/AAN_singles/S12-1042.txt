First Joint Conference on Lexical and Computational Semantics (*SEM), pages 319?327,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsUiO2: Sequence-Labeling Negation Using Dependency FeaturesEmanuele Lapponi Erik Velldal Lilja ?vrelid Jonathon ReadUniversity of Oslo, Department of Informatics{emanuel,erikve,liljao,jread}@ifi.uio.noAbstractThis paper describes the second of two sys-tems submitted from the University of Oslo(UiO) to the 2012 *SEM Shared Task on re-solving negation.
The system combines SVMcue classification with CRF sequence labelingof events and scopes.
Models for scopes andevents are created using lexical and syntacticfeatures, together with a fine-grained set of la-bels that capture the scopal behavior of certaintokens.
Following labeling, negated tokens areassigned to their respective cues using simplepost-processing heuristics.
The system wasranked first in the open track and third in theclosed track, and was one of the top perform-ers in the scope resolution sub-task overall.1 IntroductionNegation Resolution (NR) is the task of determin-ing, for a given sentence, which tokens are affectedby a negation cue.
The data set most prominentlyused for the development of systems for automaticNR is the BioScope Corpus (Vincze et al, 2008), acollection of clinical reports and papers in the bio-medical domain annotated with negation and specu-lation cues and their scopes.
The data sets releasedin conjunction with the 2012 shared task on NRhosted by The First Joint Conference on Lexical andComputational Semantics (*SEM 2012) are com-prised of the following negation annotated stories ofConan Doyle (CD): a training set of 3644 sentencesdrawn from The Hound of the Baskervilles (CDT), adevelopment set of 787 sentences taken from Wis-teria Lodge (CDD; we will refer to the combina-tion of CDT and CDD as CDTD), and a held-outtest set of 1089 sentences from The Cardboard Boxand The Red Circle (CDE).
In these sets, the con-cept of negation scope extends on the one adoptedin the BioScope corpus in several aspects: Nega-tion cues are not part of the scope, morphological(affixal) cues are annotated and scopes can be dis-continuous.
Moreover, in-scope states or events aremarked as negated if they are factual and presentedas events that did not happen (Morante and Daele-mans, 2012).
Examples (1) and (2) below are exam-ples of affixal negation and discontinuous scope re-spectively: The cues are bold, the tokens containedwithin their scopes are underlined and the negatedevent is italicized.
(1) Since we have been so unfortunate as to miss him [.
.
.
](2) If he was in the hospital and yet not on the staff he couldonly have been a house-surgeon or a house-physician: lit-tle more than a senior student.Example (2) has no negated events because the sen-tence is non-factual.The *SEM shared task thus comprises three sub-tasks: cue identification, scope resolution and eventdetection.
It is furthermore divided into two separatetracks: one closed track, where only the data sup-plied by the organizers (word form, lemma, PoS-tagand syntactic constituent for each token) may be em-ployed, and an open track, where participants mayemploy any additional tools or resources.Pragmatically speaking, a token can be either outof scope or assigned to one or more of the three re-maining classes: negation cue, in scope and negatedevent.
Additionally, in-scope tokens and negatedevents are paired to the cues they are negated by.319Our system achieves this by remodeling the task as asequence labeling task.
With annotations convertedto sequences of labels, we train a Conditional Ran-dom Field (CRF) classifier with a range of differentfeature types, including features defined over depen-dency graphs.
This article presents two submissionsfor the *SEM shared task, differing only with re-spect to how these dependency graphs were derived.For our open track submission, the dependency rep-resentations are produced by a state-of-the-art de-pendency parser, whereas the closed track submis-sion employs dependencies derived from the con-stituent analyses supplied with the shared task datasets through a process of constituent-to-dependencyconversion.
In both systems, labeling of test data isperformed in two stages.
First, cues are detected us-ing a token classifier,1 and secondly, scope and eventresolution is achieved by post-processing the outputof the sequence labeler.The two systems described in this paper have beendeveloped using CDT for training and CDD for test-ing, and differ only with regard to the source of syn-tactic information.
All reported scores are generatedusing an evaluation script provided by the task or-ganizers.
In addition to providing a full end-to-endevaluation, the script breaks down results with re-spect to identification of cues, events, scope tokens,and two variants of scope-level exact match; one re-quiring exact match also of cues and another onlypartial cue match.
For our system these two scope-level scores are identical and so are not duplicatedin our reporting.
Additionally we chose not to opti-mize for the scope tokens measure, and hence this isalso not reported as a development result.Note also that the official evaluation actually in-cludes two different variants of the metrics men-tioned above; a set of primary measures with pre-cision computed as P=TP/(TP+FP) and a set of Bmeasures where precision is rather computed asP=TP/SYS, where SYS is the total number of pre-dictions made by the system.
The reason why SYS isnot identical with TP+FP is that partial matches are1Note that the cue classifier applied in the current paper isthe same as that used in the other shared task submission fromthe University of Oslo (Read et al, 2012), and the two systemdescriptions will therefore have much overlap on this particularpoint.
For all other components the architectures of the twosystem are completely different, however.only counted as FNs (and not FPs) in order to avoiddouble penalties.
We do not report the B measuresfor development testing as they were introduced forthe final evaluation and hence were not consideredin our system optimization.
We note though, that therelative-ranking of participating systems for the pri-mary and B measures is identical, and that the cor-relation between the paired lists of scores is nearlyperfect (r=0.997).The rest of the paper is structured as follows.First, the cue classifier, its features and results aredescribed in Section 2.
Section 3 presents the sys-tem for scope and event resolution and details differ-ent features, the model-internal representation usedfor sequence-labeling, as well as the post-processingcomponent.
Error analyses for the cue, scope andevent components are provided in the respective sec-tions.
Section 4 and 5 provide developmental andheld-out results, respectively.
Finally, we provideconclusions and some reflections regarding futurework in Section 6.2 Cue detectionIdentification of negation cues is based on the light-weight classification scheme presented by Velldal etal.
(2012).
By treating the set of cue words as aclosed class, Velldal et al (2012) showed that onecould greatly reduce the number of examples pre-sented to the learner, and correspondingly the num-ber of features, while at the same time improvingperformance.
This means that the classifier only at-tempts to ?disambiguate?
known cue words whileignoring any words not observed as cues in the train-ing data.The classifier applied in the current submissionis extended to also handle affixal negation cues,such as the prefix cue in impatience, the infix incarelessness, and the suffix of colourless.
The typesof negation affixes observed in CDTD are; the pre-fixes un, dis, ir, im, and in; the infix less (we inter-nally treat this as the suffixes lessly and lessness);and the suffix less.
Of the total number of 1157 cuesin the training and development set, 192 are affixal.There are, however, a total of 1127 tokens matchingone of the affix patterns above, and while we main-tain the closed class assumption also for the affixes,the classifier will need to consider its status as a cue320or non-cue when attaching to any such token, likefor instance image, recklessness, and bless.2.1 FeaturesIn the initial formulation of Velldal (2011), an SVMclassifier was trained using simple n-gram featuresover words, both full forms and lemmas, to the leftand right of the candidate cues.
In addition to thesetoken-level features, the classifier we apply here in-cludes some features specifically targeting morpho-logical or affixal cues.
The first such feature recordscharacter n-grams from both the beginning and endof the base that an affix attaches to (up to five po-sitions).
For a context like impossible we wouldrecord n-grams such {possi, poss, pos, .
.
.}
and{sible, ible, ble, .
.
.
}, and combine this with infor-mation about the affix itself (im) and the token part-of-speech (?JJ?
).For the second feature type targeting affix cueswe try to emulate the effect of a lexicon look-upof the remaining substring that an affix attaches to,checking its status as an independent base form andits part-of-speech.
In order to take advantage ofsuch information while staying within the confinesof the closed track, we automatically generate a lex-icon from the training data, counting the instancesof each PoS tagged lemma in addition to n-gramsof word-initial characters (again recording up to fivepositions).
For a given match of an affix pattern, afeature will then record the counts from this lexiconfor the substring it attaches to.
The rationale for thisfeature is that the occurrence of a substring such asun in a token such as underlying should be consid-ered more unlikely to be a cue given that the firstpart of the remaining string (e.g., derly) would be anunlikely way to begin a word.Note that, it is also possible for a negation cueto span multiple tokens, such as the (discontinuous)pair neither / nor or fixed expressions like on thecontrary.
There are, however, only 16 instances ofsuch multiword cues (MWCs) in the entire CDTD.Rather than letting the classifier be sensitive to thesecorner cases, we cover such MWC patterns usinga small set of simple post-processing heuristics.
Asmall stop-list is used for filtering out the relevantwords from the examples presented to the classifier(on, the, etc.
).Data set Model Prec Prec F1CDDBaseline 90.68 84.39 87.42Classifier 93.75 95.38 94.56CDEBaseline 87.10 92.05 89.51Classifier 89.17 93.56 91.31Table 1: Cue classification results for the final classifierand the majority-usage baseline, showing test scores forthe development set (training on CDT) and the final held-out set (training on CDTD).2.2 ResultsTable 1 presents results for the cue classifier.
Whilethe classifier configuration was optimized againstCDD, the model used for the final held-out testingis trained on the entire CDTD, which (given ourclosed-class treatment of cues) provides a total of1162 positive and 1100 negative training examples.As an informed baseline, we also tried classifyingeach word based on its most frequent use as cueor non-cue in the training data.
(Affixal cue oc-currences are counted by looking at both the affix-pattern and the base it attaches to, basically treatingthe entire token as a cue.
Tokens that end up be-ing classified as cues are then matched against theaffix patterns observed during training in order tocorrectly delimit the annotation of the cue.)
Thissimple majority-usage approach actually provides afairly strong baseline, yielding an F1 of 87.42 onCDD (P=90.68, R=84.39).
Compare this to the F1 of94.56 obtained by the classifier on the same data set(P=93.75, R=95.38).
However, when applying themodels to the held-out set, with models estimatedover the entire CDTD, the baseline seems to ableto make good use of the additional data and provesto be even more competitive: While our final cueclassifier achieves F1=91.31, the baseline achievesF1=89.51, almost two percentage points higher thanits score on the development data, and even outper-forms four of the ten cue detection systems submit-ted for the shared task (three of the 12 shared tasksubmissions use the same classifier).When inspecting the predictions of our final cueclassifier on CDD, comprising a total of 173 goldannotated cues, we find that our system mislabels11 false positives (FPs) and 7 false negatives (FNs).321Of the FPs, we find five so-called false negation cues(Morante et al, 2011), including three instances ofnone in the fixed expression none the less.
Theothers are affixal cues, of which two are clearlywrong (underworked, universal) while others mightarguably be due to annotation errors (insuperable,unhappily, endless, listlessly).
Among the FNs, twoare due to MWCs not covered by our heuristics (e.g.,no more), while the remaining errors concern af-fixes, including one in an interesting context of dou-ble negation; not dissatisfied.3 Scope and event resolutionIn this work, we model negation scope resolutionas a special instance of the classical IOB (Inside,Outside, Begin) sequence labeling problem, wherenegation cues are labeled to be sequence starters andscopes and events as two different kinds of chunks.CRFs allow the computation of p(X|Y), whereX isa sequence of labels andY is a sequence of observa-tions, and have already been shown to be efficient insimilar, albeit less involved, tasks of negation scoperesolution (Morante and Daelemans, 2009; Councillet al, 2010).
We employ the CRF implementation inthe Wapiti toolkit, using default settings (Lavergneet al, 2010).
A number of features were used tocreate the models.
In addition to the informationprovided for each token in the CD corpus (lemma,part of speech and constituent), we extracted bothleft and right token distance to the closest negationcue.
Features were expanded to include forward andbackward bigrams and trigrams on both token andPoS level, as well as lexicalized PoS unigrams andbigrams2.
Table 2 presents a complete list of fea-tures.
The more intricate, dependency-based fea-tures are presented in Section 3.1, while the labelingof both scopes and events is detailed in Section 3.2.3.1 Dependency-based featuresFor the system submitted to the closed track, the syn-tactic representations were converted to dependencyrepresentations using the Stanford dependency con-verter, which comes with the Stanford parser (deMarneffe et al, 2006).3 These dependency represen-2By lexicalized PoS we mean an instance of a PoS-Tag inconjunction with the sentence token.3Note that the converter was applied directly to the phrase-structure trees supplied with the negation data sets, and theGeneral featuresTokenLemmaPoS unigramForward token bigram and trigramBackward token bigram and trigramForward PoS trigramBackward PoS trigramLexicalized PoSForward Lexicalized PoS bigramBackward Lexicalized PoS bigramConstituentDependency relationFirst order head PoSSecond order head PoSLexicalized dependency relationPoS-disambiguated dependency relationCue-dependent featuresToken distanceDirected dependency distanceBidirectional dependency distanceDependency pathLexicalized dependency pathTable 2: List of features used to train the CRF models.tations result from a conversion of Penn Treebank-style phrase structure trees, combining ?classic?
headfinding rules with rules that target specific linguisticconstructions, such as passives or attributive adjec-tives.
The so-called basic format provides a depen-dency graph which is a directed tree, see Figure 1for an example.For the open track submission we used Maltparser(Nivre et al, 2006) with its pre-trained parse modelfor English.4 The parse model has been trained on aconversion of sections 2-21 of the Wall Street Jour-nal section of the Penn Treebank to Stanford depen-dencies, augmented with data from Question Bank.The parser was applied to the negation data, usingthe word tokens and supplied parts-of-speech as in-put to the parser.The features extracted via the dependency graphsaim at modeling the syntactic relationship betweeneach token and the closest negation cue.
Token dis-tance was therefore complemented with two variantsof dependency distance from each token to the lexi-Stanford parser was not used to parse the data.4The pre-trained model is available from maltparser.org322we   have  never  gone  out  without  keeping  a  sharp  watch  ,  and  no  one  could  have  escaped  our  notice  .
"nsubjauxnegconjccpunctpreppartpcompdobjdetamoddepnsubjauxauxpunctpunctdobjpossrootann.
1:ann.
2:ann.
3:cuecuecuelabels: CUE CUE CUEN N E EN NN N E N N N NS O S ONFigure 1: A sentence from the CD corpus showing a dependency graph and the annotation-to-label conversion.cally closest cue, Directed Distance (DD) and Bidi-rectional Distance (BD).
DD is extracted by follow-ing the reversed, directed edges from token X to thecue.
If there is no such path, the value of the featureis -1.
BD uses the Dijkstra shortest path algorithmon an undirected representation of the graph.
Thelatter feature proved to be more effective than theformer when not used together; using them in con-junction seemed to confuse the model, thus the fi-nal model utilizes only BD.
We furthermore use theDependency Graph Path (DGP) as a feature.
Thisfeature was inspired by the Parse Tree Path featurepresented in Gildea and Jurafsky (2002) in the con-text of Semantic Role Labeling.
It represents thepath traversed from each token to the cue, encod-ing both the dependency relations and the directionof the arc that is traversed: for instance, the rela-tion between our and no in Figure 1 is described as poss  dobj  nsubj  det.
Like Councill etal.
(2010), we also encode the PoS of the first andsecond order syntactic head of each token.
For thetoken no in Figure 1, for instance, we record the PoSof one and escaped, respectively.3.2 Model-internal representationThe token-wise annotations in the CD corpus con-tain multiple layers of information.
Tokens may ormay not be negation cues and they can be either inor out of scope; in-scope tokens may or may notbe negated events, and are associated with each ofthe cues they are negated by.
Moreover, scopes maybe (partially) overlapping, as in Figure 1, where thePoS # S PoS # MCUE PoS # CUEpunctuation 1492 JJ 268 RB 1026CC 52 RB 28 DT 296IN + TO 46 NN 16 NN 146RB 38 NN 4 UH 118PRP 32 IN 2 IN 64rest 118 rest ?
rest 38Table 3: Frequency distribution of parts of speech overthe S, MCUE and CUE labels in CDTD.scope of without is contained within the scope ofnever.
We convert this representation internally byassigning one of six labels to each token: O, CUE,MCUE, N, E and S, for out-of-scope, cue, mor-phological (affixal) cue, in-scope, event and nega-tion stop respectively.
The CUE, O, N and E la-bels parallel the IOB chunking paradigm and areeventually translated in the final annotations by ourpost-processing component.
MCUE and S extendthe label set to account for the specific behavior ofthe tokens they are associated with.
The rationalebehind the separation of cues in two classes is thepronounced differences between the PoS frequencydistributions of standard versus morphological cues.Table 3 presents the frequency distribution of PoS-tags over the different cue types in CDTD and showsthat, unsurprisingly, the majority class for morpho-logical cues is adjectives, which typically generatedifferent scope patterns compared to the majorityclass for standard cues.
The S label, a special in-stance of an out-of-scope token, is defined as the323first non-cue, out-of-scope token to the right of onelabeled with N, and targets mostly punctuation.After some experimentation with joint labeling ofscopes and events, we opted for separation of thetwo models, hence training separate models for thetwo tasks of scope resolution and event detection.In the model for scopes, all E labels are switchedto N; conversely, Ns become Os in the event model.Given the nature of the annotations, the predictionsprovided by the model for events serve a double pur-pose: finding the negated token in a sentence anddeciding whether a sentence is factual or not.
Theoutputs of the two classifiers are merged during post-processing.3.3 Post-processingA simple, heuristics-based algorithm was appliedto the output of the labelers in order to pair eachin-scope token to its negation cue(s) and determineoverlaps.
Our algorithm works by first determiningthe overlaps among negation cues.
Cue A negatescue B if the following conditions are met:?
B is to the right of A.?
There are no tokens labeled with S between Aand B.?
Token distance between A and B does not ex-ceed 10.In the example in Figure 1, the overlapping condi-tion holds for never and without but not for withoutand no, because of the punctuation between them.The token distance threshold of 10 was determinedempirically on CDT.
In order to assign in-scope to-kens to their respective cue, tokens labeled with Nare treated as follows:?
Assign each token T to the closest negation cueA with no S-labeled tokens or punctuation sep-arating it from T.?
If A was found to be negated by cue B, assignT to B as well.?
If T is labeled with E by the event classifier,mark it as an event.F1Configuration Closed Open(A) O, N, CUE, MCUE, E, S 64.85 66.41Dependency Features(B) O, N, CUE, MCUE, E, S 59.35 59.35No Dependency Features(C) O, N, CUE, E 62.69 63.24Dependency Features(D) O, N, CUE, E 56.44 56.44No Dependency FeaturesTable 4: Full negation results on CDD with gold cues.This algorithm yields the correct annotations forthe example in Figure 1; when applied to label se-quences originating from the gold scopes in CDD,the reported F1 is 95%.
We note that this loss of in-formation could have been avoided by presenting theCRF with a version of a sentence for each negationcue.
Then, when labeling new sentences, the modelcould be applied repeatedly (based on the number ofcues provided by the cue detection system).
How-ever, training with multiple instances of the samesentence could result in a dilution of the evidenceneeded for scope labeling; this remains to be inves-tigated in future work.4 Development resultsTo investigate the effects of the augmented set of la-bels and that of dependency features comparatively,we present four different configurations of our sys-tem in Table 4, using F1 for the stricter score thatcounts perfect-match negation resolution for eachnegation cue.
Comparing (B) and (D), we observethat explicitly encoding significant tokens with extralabels does improve the performance of the classi-fier.
Comparing (A) to (B) and (C) to (B) shows theeffect of the dependency features with and withoutthe augmented set of labels.
With (A) being our topperforming system and (D) a kind of internal base-line, we observe that the combined effects of the la-bels and dependency features is beneficial, with amargin of about 8 and 10 percentage points for ourclosed and open track systems respectively.Table 5 presents the results for scope resolution onCDD with gold cues.
Interestingly, the constituent324Closed OpenPrec Rec F1 Prec Rec F1Scopes 100.00 70.24 82.52 100.00 66.67 80.00Scope Tokens 94.69 82.16 87.98 90.64 81.36 85.75Negated 82.47 72.07 76.92 83.65 77.68 80.55Full negation 100.00 47.98 64.85 100.00 49.71 66.41Table 5: Results for scope resolution on CDD with gold cues.trees converted to Stanford dependencies used in theclosed track outperform the open system employingMaltparser on scopes, while for negated events thelatter is over 5 percentage points better than the for-mer, as shown in Table 5.4.1 Error analysisWe performed a manual error analysis of the scoperesolution on the development data using gold cueinformation.
Since our system does not deal specifi-cally with discontinuous scopes, and seeing that weare employing a sequence classifier with a fairly lo-cal window, we are not surprised to find that a sub-stantial portion of the errors are caused by discon-tinuous scopes.
In fact, in our closed track system,these errors amount to 34% of the total number oferrors.
Discontinuous scopes, as in (3) below, ac-count for 9.3% of all scopes in CDD and the closedtask system does not analyze any of these correctly,whereas the open system correctly analyzes one dis-continuous scope.
(3) I therefore spent the day at my club and did notreturn to Baker Street until evening.A similar analysis with respect to event detectionon gold scope information indicated that errors aremostly due to either predicting an event for a non-factual context (false positive) or not predicting anevent for a factual context (false negative), i.e., thereare relatively few instances of predicting the wrongtoken for a factual context (which result in both afalse negative and a false positive).
This suggeststhat the CRF has learned what tokens should be la-beled as an event for a negation, but has not learnedso well how to determine whether the negation isfactual or non-factual.
In this respect it may be thatincorporating information from a separate and dedi-cated component for factuality detection ?
as in thesystem of Read et al (2012) ?
could yield improve-ments for the CRF event model.5 Held-out evaluationFinal results on held-out data for both closed andopen track submissions are reported in Table 6.
Forthe final run, we trained our systems on CDTD.
Weobserve a similar relative performance to our devel-opment results, with the open track system outper-forming the closed track one, albeit by a smallermargin than what we saw in development.
We arealso surprised to see that despite not addressing dis-continuous scopes directly, our system obtained thebest score on scope resolution (according to the met-ric dubbed ?Scopes (cue match)?
).6 Conclusions and future workThis paper has provided an overview of our systemsubmissions for the *SEM 2012 shared task on re-solving negation.
This involves the subtasks of iden-tifying negations cues, identifying the in-sentencescope of these cues, as well as identifying negated(and factual) events.
While a simple SVM tokenclassifier is applied for the cue detection task, we ap-ply CRF sequence classifiers for predicting scopesand events.
For the CRF models we experimentedwith a fine-grained set of labels and a wide range offeature types, drawing heavily on information fromdependency structures.
We have detailed two dif-ferent system configurations ?
one submitted forthe open track and another for the closed track ?and the two configurations only differ with respectto the source used for the dependency parses: Forthe closed track submission we simply convertedthe constituent structures provided in the shared taskdata to Stanford dependencies, while for the opentrack we apply the Maltparser.
For the end-to-endevaluation, our submission was ranked first in theopen track and third in the closed track.
The systemalso had the best performance for each individualsub-task in the open track, as well as being among325Closed OpenPrec Rec F1 Prec Rec F1Cues 89.17 93.56 91.31 89.17 93.56 91.31Scopes 85.71 62.65 72.39 85.71 62.65 72.39Scope Tokens 86.03 81.55 83.73 82.25 82.16 82.20Negated 68.18 52.63 59.40 66.90 57.40 61.79Full negation 78.26 40.91 53.73 78.72 42.05 54.82Cues B 86.97 93.56 90.14 86.97 93.56 90.14Scopes B 59.32 62.65 60.94 59.54 62.65 61.06Negated B 67.16 52.63 59.01 63.82 57.40 60.44Full negation B 38.03 40.91 39.42 39.08 42.05 40.51Table 6: End-to-end results on the held-out data.the top-performers on the scope resolution sub-taskacross both tracks.Due to time constraints we were not able to di-rectly address discontinuous scopes in our system.For future work we plan on looking for ways totackle this problem by taking advantage of syntac-tic information, both in the classification and in thepost-processing steps.
We are also interested in de-veloping the CRF-internal label-set to include moreinformative labels.
We also want to test the sys-tem design developed for this task on other corporaannotated for negation (or other related phenom-ena such as speculation), as well as perform extrin-sic evaluation of our system as a sub-component toother NLP tasks such as sentiment analysis or opin-ion mining.
Lastly, we would like to try trainingseparate classifiers for affixal and token-level cues,given that largely separate sets of features are effec-tive for the two cases.AcknowledgementsWe thank colleagues at the University of Oslo, andin particular Johan Benum Evensberget and ArneSkj?rholt for fruitful discussions and suggestions.We also thank the anonymous reviewers for theirhelpful feedback.ReferencesIsaac G. Councill, Ryan McDonald, and Leonid Ve-likovich.
2010.
What?s great and what?s not: Learn-ing to classify the scope of negation for improved sen-timent analysis.
In Proceedings of the Workshop OnNegation and Speculation in Natural Language Pro-cessing, pages 51?59.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typed de-pendency parses from phrase structure parses.
In Pro-ceedings of the 5th International Conference on Lan-guage Resources and Evaluation.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic la-beling of semantic roles.
Computational Linguistic,28(3):245?288.Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.2010.
Practical very large scale CRFs.
In Proceed-ings the 48th Annual Meeting of the Association forComputational Linguistics (ACL), pages 504?513.
As-sociation for Computational Linguistics.Roser Morante and Walter Daelemans.
2009.
A met-alearning approach to processing the scope of nega-tion.
In CoNLL ?09: Proceedings of the Thir-teenth Conference on Computational Natural Lan-guage Learning, pages 21?29.
Association for Com-putational Linguistics.Roser Morante and Walter Daelemans.
2012.ConanDoyle-neg: Annotation of negation in ConanDoyle stories.
In Proceedings of the Eighth Interna-tional Conference on Language Resources and Evalu-ation, Istanbul.Roser Morante, Sarah Schrauwen, and Walter Daele-mans.
2011.
Annotation of negation cues and theirscope: Guidelines v1.0.
Technical report, Univer-sity of Antwerp.
CLIPS: Computational Linguistics& Psycholinguistics technical report series.Joakim Nivre, Johan Hall, and Jens Nilsson.
2006.
Malt-Parser: A data-driven parser-generator for dependencyparsing.
In Proceedings of the Fifth InternationalConference on Language Resources and Evaluation,pages 2216?2219.Jonathon Read, Erik Velldal, Lilja ?vrelid, and StephanOepen.
2012.
UiO1: Constituent-based discrimina-tive ranking for negation resolution.
In Proceedingsof the First Joint Conference on Lexical and Computa-tional Semantics, Montreal.
Submission under review.326Erik Velldal, Lilja ?vrelid, Jonathon Read, and StephanOepen.
2012.
Speculation and negation: Rules,rankers and the role of syntax.
Computational Lin-guistics, 38(2).Erik Velldal.
2011.
Predicting speculation: A simple dis-ambiguation approach to hedge detection in biomedi-cal literature.
Journal of Biomedical Semantics, 2(5).Veronika Vincze, Gyo?rgy Szarvas, Richa?rd Farkas,Gyo?rgy Mo?ra, and Ja?nos Csirik.
2008.
The Bio-Scope corpus: Biomedical texts annotated for uncer-tainty, negation and their scopes.
BMC Bioinformat-ics, 9 (Suppl.
11).327
