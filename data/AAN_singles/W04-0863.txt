Joining forces to resolve lexical ambiguity:East meets West in BarcelonaRichard WICENTOWSKI*, Grace NGAI?1 , Dekai WU?2Marine CARPUAT?
, Emily THOMFORDE*, Adrian PACKEL**Swarthmore CollegeSwarthmore, PAUSA?
Dept.
of Computing?
HK Polytechnic UniversityHong Kong?
HKUST, Dept of Computer ScienceHuman Language Technology CenterHong Kongrichardw@cs.swarthmore.edu, csgngai@polyu.edu.hk, dekai@cs.ust.hkmarine@cs.ust.hk, ethomfo1@cs.swarthmore.edu, packel@cs.swarthmore.eduAbstractThis paper describes the component models andcombination model built as a joint effort be-tween Swarthmore College, Hong Kong PolyU, andHKUST.
Though other models described elsewherecontributed to the final combination model, this pa-per focuses solely on the joint contributions to the?Swat-HK?
effort.1 IntroductionThis paper describes the two joint component mod-els of the Swat-HK systems entered into four ofthe word sense disambiguation lexical sample tasksin Senseval-3: Basque, Catalan, Italian and Roma-nian, as well as a combination model for each lan-guage.
The feature engineering (and construction ofthree other component models which are describedin (Wicentowski et al, 2004)) was performed atSwarthmore College, while the Hong Kong teamconstructed two component models based on well-known machine learning algorithms.
The combina-tion model, which was constructed at Swarthmore,uses voting to combine all five models.2 Experimental FeaturesA full description of the experimental features forall four tasks can be found in the report submittedby the Swarthmore College Senseval team (Wicen-towski et al, 2004).
Briefly, the systems used lexi-cal and syntactic features in the context of the targetword:?
The ?bag of words (and lemmas)?
in the con-text of the ambiguous word.?
Bigrams and trigrams of words (and lemmas,1The author would like to thank the Hong Kong PolytechnicUniversity for supporting this research in part through researchgrants A-PE37 and 4-Z03S.2The author would like to thank the Hong Kong ResearchGrants Council (RGC) for supporting this research in partthrough research grants RGC6083/99E, RGC6256/00E, andDAG03/04.EG09.part-of-speech tags, and, for Basque, case in-formation) surrounding the ambiguous word.?
The topic (or code) of the document containingthe current instance of the word was extracted.
(Basque and Catalan only.
)These features have been shown to be effectivein previous WSD research.
Since our systems wereall supervised, all the data used was provided by theSenseval organizers; no additional (unlabeled) datawas included.3 MethodologyThe systems that were constructed by this team in-cluded two component models: a boosting modeland a maximum entropy model as well as a com-bination system.
The component models were alsoused in other Senseval-3 tasks: Semantic Role La-beling (Ngai et al, 2004) and the lexical sampletasks for Chinese and English, as well as the Multi-lingual task (Carpuat et al, 2004).To perform parameter tuning for the two compo-nent models, 20% of the samples from the trainingset were held out into a validation set.
Since wedid not expect the senses of different words to shareany information, the training data was partitioned bythe ambiguous word in question.
A model was thentrained for each ambiguous word type.
In total, wehad 40 models for Basque, 27 models for Catalan,45 models for Italian and 39 models for Romanian.3.1 BoostingBoosting is a powerful machine learning algorithmwhich has been shown to achieve good results ona variety of NLP problems.
One known propertyof boosting is its ability to handle large numbers offeatures.
For this reason, we felt that it would bewell suited to the WSD task, which is known to behighly lexicalized with a large number of possibleword types.Our system was constructed around the Boostex-ter software (Schapire and Singer, 2000), which im-plements boosting on top of decision stumps (deci-Association for Computational Linguisticsfor the Semantic Analysis of Text, Barcelona, Spain, July 2004SENSEVAL-3: Third International Workshop on the Evaluation of Systemssion trees of one level), and was originally designedfor text classification.Tuning a boosting system mainly lies in modify-ing the number of iterations, or the number of basemodels it would learn.
Larger number of iterationscontribute to the boosting model?s power.
However,they also make it more prone to overfitting and in-crease the training time.
The latter, a simple dis-advantage in another problem, becomes a real issuefor Senseval, since large numbers of models (one foreach word type) need to be trained in a short periodof time.Since the available features differed from lan-guage to language, the optimal number of iterationsalso varied.
Table 1 shows the performance of themodel on the validation set with respect to the num-ber of iterations per language.AccuracyNumber of iterationsLanguage 500 1000 2000Basque 66.12% 67.07% 67.08%Catalan 84.77% 84.89% 85.02%Italian 51.11% 50.93%Romanian 64.68% 64.52%Table 1: Boosting models on the validation sets.The final systems for the languages used 2000 it-erations for Basque and Catalan and 500 iterationsfor Italian and Romanian.
The test set results areshown in Table 43.2 Maximum EntropyThe other individual system was based on the maxi-mum entropy model, another machine learning al-gorithm which has been successfully applied tomany NLP problems.
Our system was implementedon top of the YASMET package (Och, 2002).Due to lack of time, we did not manage to fine-tune the maximum entropy model.
The YASMETpackage does provide a number of easily variableparameters, but we were only able to try varying thefeature selection count threshold and the smoothingparameter, and only on the Basque data.Experimentally, however, smoothing did notseem to make a difference.
The only change in per-formance was caused by varying the feature selec-tion count threshold, which controls the number oftimes a feature has to be seen in the training set inorder to be considered.
Table 2 shows the perfor-mances of the system on the Basque validation set,with count thresholds of 0, 1 and 2.Since word sense disambiguation is known to beThreshold0 1 2Accuracy 55.62% 66.13% 65.68%Table 2: Maximum Entropy Models on Basque val-idation set.a highly lexicalized task involving many feature val-ues and sparse data, it is not too surprising that set-ting a low threshold of 1 proves to be the most effec-tive.
The final system kept this threshold, smooth-ing was not done and the GIS iterations allowed toproceed until it converged on its own.
These param-eters were used for all four languages.The maximum entropy model was not enteredinto the competition as an official contestant; how-ever, it did participate in the combined system.3.3 Combined SystemEnsemble methods have been widely studied inNLP research, and it is well-known that a set ofsystems will often combine to produce better re-sults than those achieved by the best individual sys-tem alone.
The final system contributed by theSwarthmore-Hong Kong team was such an ensem-ble.
In addition to the boosting and maximum en-tropy models mentioned earlier, three other modelswere included: a nearest-neighbor clustering model,a decision list, and a Na?
?ve Bayes model.
The fivemodels were then combined by a simple weightedmajority vote, with an ad-hoc weight of 1.1 givento the boosting and decision lists systems, and 1.0otherwise, with ties broken arbitrarily.Due to an unfortunate error with the input data ofthe voting algorithm (Wicentowski et al, 2004), theofficial submitted results for the combined systemwere poorer than they should have been.
Table 3compares the official (submitted) results to the cor-rected results on the test set.
The decrease in per-formance caused by the error ranged from 0.9% to3.3%.Language official corrected net gainBasque 67.0% 67.9% 0.9%Catalan 79.5% 80.4% 0.9%Italian 51.4% 54.7% 3.3%Romanian 72.4% 73.3% 0.9%Table 3: Ensemble system results on the test set.Both official and corrected results are included.SystemDescription Name Acc.
(%)BasqueBoosting basque-swat hk-bo 71.1Combined swat-hk-basque 67.0 (67.9)NNC 66.0DL 64.6Maxent 62.1NB 60.4Baseline 55.8CatalanBoosting catalan-swat hk-bo 79.6DL 80.6Combined swat-hk-catalan 79.5 (80.4)NNC 77.5NB 71.3Maxent 70.9Baseline 66.4ItalianCombined swat-hk-italian 51.4 (54.7)DL 50.3Boosting italian-swat hk 48.3Maxent 46.9NNC 44.9NB 42.1Baseline 23.7RomanianBoosting romanian-swat hk-bo 72.7Combined swat-hk-romanian 72.4 (73.3)DL 70.9NNC 67.9Maxent 66.5NB 62.8Baseline 58.4Table 4: Test set results on 4 languages.
Offi-cial contestants are in bold; corrected voting resultsare in parentheses.
Key: NB: Na?
?ve Bayes, NNC:Nearest-Neighbor Clustering, DL: Decision List4 Test Set ResultsFinal results from all the systems are shown in Ta-ble 4.
As a reference, the results of a simple base-line system which assigns the most frequent senseas seen in the training set is also provided.Due to the error in the voting system, the offi-cial results for the combination system were lowerthan they should have been ?
as a result, boostingwas officially the top ranked system for 3 of the 4languages.
With the corrected results, however, thecombined system outperforms the individual mod-els, as expected.
The only exception is Basque,where the booster had an exceptionally strong per-formance.
This is probably due to the fact thatBasque has a much richer feature set than the otherlanguages, which boosting was better able to takeadvantage of.The poor performance of the maximum entropymodel was also unexpected at first; however, it isperhaps not too surprising, given the lack of timespent on fine-tuning the model.
As a result, most ofthe parameters were left at their default values.One thing worth noting is the fact that the sys-tems were combined as ?closed systems?
?
i.e.
allthat was known about them was the output result,and nothing else.
The result was that no confidencemeasures from the boosting and maximum entropycould be used in the combined system.
It is likelythat the performance could have been further im-proved if more information had been available.5 Conclusions and DiscussionThis paper describes the ?Swat-HK?
systems whichwere the result of collaborative work betweenSwarthmore College, Hong Kong Polytechnic Uni-versity and HKUST.
Several base systems were con-structed on the same feature set, and a weighed ma-jority voting system was used to combine the re-sults.
The individual systems all achieve good re-sults, easily beating the baseline.
As expected, thecombined system outperforms the best individualsystem for the majority of the tasks.ReferencesMarine Carpuat, Weifeng Su, and Dekai Wu.
2004.Augmenting Ensemble Classification for WordSense Disambiguation with a Kernel PCA Model.In Proceedings of Senseval-3, Barcelona.Grace Ngai, Dekai Wu, Marine Carpuat, Chi-ShingWang, and Chi-Yung Wang.
2004.
SemanticRole Labeling with Boosting, SVMs, MaximumEntropy, SNOW, and Decision Lists.
In Proceed-ings of Senseval-3, Barcelona.Franz Josef Och.
2002.
Yet Another SmallMaxent Toolkit: Yasmet.
http://www-i6.informatik.rwth-aachen.de/Colleagues/och.Robert E. Schapire and Yoram Singer.
2000.
Boos-texter: A boosting-based system for text catego-rization.
Machine Learning, 39(2/3):135?168.Richard Wicentowski, Emily Thomforde, andAdrian Packel.
2004.
The Swarthmore CollegeSenseval-3 system.
In Proceedings of Senseval-3, Barcelona.
