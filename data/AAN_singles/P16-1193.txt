Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2052?2062,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsA Vector Space for Distributional Semantics for Entailment?James Henderson and Diana Nicoleta PopaXerox Research Centre Europejames.henderson@xrce.xerox.com and diana.popa@xrce.xerox.comAbstractDistributional semantics creates vector-space representations that capture manyforms of semantic similarity, but their re-lation to semantic entailment has been lessclear.
We propose a vector-space modelwhich provides a formal foundation for adistributional semantics of entailment.
Us-ing a mean-field approximation, we de-velop approximate inference proceduresand entailment operators over vectors ofprobabilities of features being known (ver-sus unknown).
We use this frameworkto reinterpret an existing distributional-semantic model (Word2Vec) as approxi-mating an entailment-based model of thedistributions of words in contexts, therebypredicting lexical entailment relations.
Inboth unsupervised and semi-supervisedexperiments on hyponymy detection, weget substantial improvements over previ-ous results.1 IntroductionModelling entailment is a fundamental issue incomputational semantics.
It is also important formany applications, for example to produce ab-stract summaries or to answer questions from text,where we need to ensure that the input text entailsthe output text.
There has been a lot of interest inmodelling entailment in a vector-space, but mostof this work takes an empirical, often ad-hoc, ap-proach to this problem, and achieving good resultshas been difficult (Levy et al, 2015).
In this work,we propose a new framework for modelling entail-ment in a vector-space, and illustrate its effective-?This work was partially supported by French ANR grantCIFRE N 1324/2014.?
unk f g ?funk 1 0 0 0f 1 1 0 0g 1 0 1 0?f 1 0 0 1Table 1: Pattern of logical entailment betweennothing known (unk), two different features f andg known, and the complement of f (?f ) known.ness with a distributional-semantic model of hy-ponymy detection.Unlike previous vector-space models of entail-ment, the proposed framework explicitly modelswhat information is unknown.
This is a crucialproperty, because entailment reflects what infor-mation is and is not known; a representation y en-tails a representation x if and only if everythingthat is known given x is also known given y. Thus,we model entailment in a vector space where eachdimension represents something we might know.As illustrated in Table 1, knowing that a feature fis true always entails knowing that same feature,but never entails knowing that a different feature gis true.
Also, knowing that a feature is true alwaysentails not knowing anything (unk), since strictlyless information is still entailment, but the reverseis never true.
Table 1 also illustrates that knowingthat a feature f is false (?f ) patterns exactly thesame way as knowing that an unrelated feature gis true.
This illustrates that the relevant dichotomyfor entailment is known versus unknown, and nottrue versus false.Previous vector-space models have been verysuccessful at modelling semantic similarity, in par-ticular using distributional semantic models (e.g.
(Deerwester et al, 1990; Sch?utze, 1993; Mikolovet al, 2013a)).
Distributional semantics uses thedistributions of words in contexts to induce vector-space embeddings of words, which have been2052shown to be useful for a wide variety of tasks.Two words are predicted to be similar if the dotproduct between their vectors is high.
But thedot product is an anti-symmetric operator, whichmakes it more natural to interpret these vectorsas representing whether features are true or false,whereas the dichotomy known versus unknown isasymmetric.
We surmise that this is why distribu-tional semantic models have had difficulty mod-elling lexical entailment (Levy et al, 2015).To develop a vector-space model of whetherfeatures are known or unknown, we start with dis-crete binary vectors, where 1 means known and 0means unknown.
Entailment between these dis-crete binary vectors can be calculated by indepen-dently checking each dimension.
But as soon aswe try to do calculations with distributions overthese vectors, we need to deal with the case wherethe features are not independent.
For example, iffeature f has a 50% chance of being true and a50% chance of being false, we can?t assume thatthere is a 25% chance that both f and ?f areknown.
This simple case of mutual exclusion isjust one example of a wide range of constraintsbetween features which we need to handle in se-mantic models.
These constraints mean that thedifferent dimensions of our vector space are notindependent, and therefore exact models are notfactorised.
Because the models are not factorised,exact calculations of entailment and exact infer-ence of vectors are intractable.Mean-field approximations are a popular ap-proach to efficient inference for intractable mod-els.
In a mean-field approximation, distributionsover binary vectors are represented using a sin-gle probability for each dimension.
These vectorsof real values are the basis of our proposed vectorspace for entailment.In this work, we propose a vector-space modelwhich provides a formal foundation for a distri-butional semantics of entailment.
This frameworkis derived from a mean-field approximation to en-tailment between binary vectors, and includes op-erators for measuring entailment between vectors,and procedures for inferring vectors in an entail-ment graph.
We validate this framework by us-ing it to reinterpret existing Word2Vec (Mikolovet al, 2013a) word embedding vectors as approxi-mating an entailment-based model of the distribu-tion of words in contexts.
This reinterpretation al-lows us to use existing word embeddings as an un-supervised model of lexical entailment, success-fully predicting hyponymy relations using the pro-posed entailment operators in both unsupervisedand semi-supervised experiments.2 Modelling Entailment in aVector SpaceTo develop a model of entailment in a vectorspace, we start with the logical definition of en-tailment in terms of vectors of discrete known fea-tures: y entails x if and only if all the known fea-tures in x are also included in y.
We formalise thisrelation with binary vectors x, y where 1 meansknown and 0 means unknown, so this discrete en-tailment relation (y?x) can be defined with thebinary formula:P ((y?x) | x, y) =?k(1?
(1?yk)xk)Given prior probability distributions P (x), P (y)over these vectors, the exact joint and marginalprobabilities for an entailment relation are:P (x, y, (y?x)) = P (x) P (y)?k(1?
(1?yk)xk)P ((y?x)) = EP (x)EP (y)?k(1?
(1?yk)xk) (1)We cannot assume that the priors P (x) andP (y) are factorised, because there are many im-portant correlations between features and there-fore we cannot assume that the features are in-dependent.
As discussed in Section 1, even justrepresenting both a feature f and its negation ?frequires two different dimensions k and k?in thevector space, because 0 represents unknown andnot false.
Given valid feature vectors, calculatingentailment can consider these two dimensions sep-arately, but to reason with distributions over vec-tors we need the prior P (x) to enforce the con-straint that xkand xk?are mutually exclusive.
Ingeneral, such correlations and anti-correlations ex-ist between many semantic features, which makesinference and calculating the probability of entail-ment intractable.To allow for efficient inference in such a model,we propose a mean-field approximation.
This ineffect assumes that the posterior distribution overvectors is factorised, but in practice this is a muchweaker assumption than assuming the prior is fac-torised.
The posterior distribution has less un-certainty and therefore is influenced less by non-factorised prior constraints.
By assuming a fac-torised posterior, we can then represent distribu-tions over feature vectors with simple vectors of2053probabilities of individual features (or as below,with their log-odds).
These real-valued vectors arethe basis of the proposed vector-space model ofentailment.In the next two subsections, we derive a mean-field approximation for inference of real-valuedvectors in entailment graphs.
This derivationleads to three proposed vector-space operators forapproximating the log-probability of entailment,summarised in Table 2.
These operators will beused in the evaluation in Section 5.
This inferenceframework will also be used in Section 3 to modelhow existing word embeddings can be mapped tovectors to which the entailment operators can beapplied.2.1 A Mean-Field ApproximationA mean-field approximation approximates theposterior P using a factorised distribution Q. Firstof all, this gives us a concise description of theposterior P (x| .
.
.)
as a vector of continuous val-ues Q(x=1), where Q(x=1)k= Q(xk=1) ?EP (x|...)xk= P (xk=1| .
.
.)
(i.e.
the marginalprobabilities of each bit).
Secondly, as is shownbelow, this gives us efficient methods for doing ap-proximate inference of vectors in a model.First we consider the simple case wherewe want to approximate the posterior distribu-tion P (x, y|y?x).
In a mean-field approxi-mation, we want to find a factorised distribu-tion Q(x, y) which minimises the KL-divergenceDKL(Q(x, y)||P (x, y|y?x)) with the true distri-bution P (x, y|y?x).L = DKL(Q(x, y)||P (x, y|(y?x)))?
?xQ(x) logQ(x, y)P (x, y, (y?x))=?kEQ(xk)logQ(xk) +?kEQ(yk)logQ(yk)?
EQ(x)logP (x)?
EQ(y)logP (y)??kEQ(xk)EQ(yk)log(1?
(1?yk)xk)In the final equation, the first two terms are thenegative entropy of Q, ?H(Q), which acts as amaximum entropy regulariser, the final term en-forces the entailment constraint, and the middletwo terms represent the prior for x and y.
One ap-proach (generalised further in the next subsection)to the prior terms ?EQ(x)logP (x) is to boundthem by assuming P (x) is a function in the ex-ponential family, giving us:EQ(x)logP (x) ?
EQ(x)logexp(?k?xkxk)Z?=?kEQ(xk)?xkxk?
logZ?where the logZ?is not relevant in any of our in-ference problems and thus will be dropped below.As typically in mean-field approximations, in-ference ofQ(x) andQ(y) can?t be done efficientlywith this exact objective L, because of the non-linear interdependence between xkand ykin thelast term.
Thus, we introduce two approximationsto L, one for use in inferring Q(x) given Q(y)(forward inference), and one for the reverse in-ference problem (backward inference).
In bothcases, the approximation is done with an appli-cation of Jensen?s inequality to the log function,which gives us an upper bound on L, as is stan-dard practice in mean-field approximations.
Forforward inference:L ??H(Q)?Q(xk=1)?xk?
EQ(yk)?ykyk(2)?Q(xk=1) logQ(yk=1) )which we can optimise for Q(xk=1):Q(xk=1) = ?
( ?xk+ logQ(yk=1) ) (3)where ?
() is the sigmoid function.
The sig-moid function arises from the entropy regulariser,making this a specific form of maximum entropymodel.
And for backward inference:L ??H(Q)?
EQ(xk)?xkxk?Q(yk=1)?yk(4)?
(1?Q(yk=1)) log(1?Q(xk=1)) )which we can optimise for Q(yk=1):Q(yk=1) = ?
( ?yk?
log(1?Q(xk=1)) ) (5)Note that in equations (2) and (4) thefinal terms, Q(xk=1) logQ(yk=1) and(1?Q(yk=1)) log(1?Q(xk=1)) respectively,are approximations to the log-probability of theentailment.
We define two vector-space operators,<?and>?, to be these same approximations.logQ(y?x)??kEQ(xk)log(EQ(yk)(1?
(1?yk)xk))= Q(x=1) ?
logQ(y=1) ?
X<?YlogQ(y?x)??kEQ(yk)log(EQ(xk)(1?
(1?yk)xk))= (1?Q(y=1)) ?
log(1?Q(x=1)) ?
Y>?X2054X<?Y ?
?
(X) ?
log ?
(Y )Y>?X ?
?
(?Y ) ?
log ?
(?X)Y ?
?X ??klog(1?
?(?Yk)?
(Xk))Table 2: The proposed entailment operators, ap-proximating logP (y?x).We parametrise these operators with the vectorsX,Y of log-odds of Q(x), Q(y), namely X =logQ(x=1)Q(x=0)= ?-1(Q(x=1)).
The resulting opera-tor definitions are summarised in Table 2.Also note that the probability of entailmentgiven in equation (1) becomes factorised when wereplace P with Q.
We define a third vector-spaceoperator, ?
?, to be this factorised approximation,also shown in Table 2.2.2 Inference in Entailment GraphsIn general, doing inference for one entailment isnot enough; we want to do inference in a graph ofentailments between variables.
In this section wegeneralise the above mean-field approximation toentailment graphs.To represent information about variables thatcomes from outside the entailment graph, we as-sume we are given a prior P (x) over all variablesxiin the graph.
As above, we do not assume thatthis prior is factorised.
Instead we assume that theprior P (x) is itself a graphical model which can beapproximated with a mean-field approximation.Given a set of variables xieach represent-ing vectors of binary variables xik, a set of en-tailment relations r = {(i, j)|(xi?xj)}, anda set of negated entailment relations r?
={(i, j)|(xi/?xj)}, we can write the joint posteriorprobability as:P (x, r, r?)
=1ZP (x)?i((?j:r(i,j)?kP (xik?xjk|xik, xjk))(?j:r?(i,j)(1?
?kP (xik?xjk|xik, xjk))))We want to find a factorised distribution Q thatminimises L = DKL(Q(x)||P (x|r, r?)).
Asabove, we bound this loss for each elementXik=?-1(Q(xik=1)) of each vector we want toinfer, using analogous Jensen?s inequalities for theterms involving nodes i and j such that r(i, j) orr(j, i).
For completeness, we also propose similarinequalities for nodes i and j such that r?
(i, j) orr?
(j, i), and bound them using the constantsCijk??k?6=k(1??(?Xik?)?(Xjk?
)).To represent the prior P (x), we use the terms?ik(X?ik) ?
logEQ(x?ik)P (x?ik, xik=1)1?
EQ(x?ik)P (x?ik, xik=1)where x?ikis the set of all xi?k?such that either i?6=ior k?6=k.
These terms can be thought of as the log-odds terms that would be contributed to the lossfunction by including the prior?s graphical modelin the mean-field approximation.Now we can infer the optimal Xikas:Xik= ?ik(X?ik) +?j:r(i,j)?
log ?
(?Xjk) (6)+?j:r(j,i)log ?
(Xjk) +?j:r?(j,i)log1?Cijk?(Xjk)1?Cijk+?j:r?(i,j)?
log1?Cijk?
(?Xjk)1?CijkIn summary, the proposed mean-field approx-imation does inference in entailment graphs byiteratively re-estimating each Xias the sum of:the prior log-odds, ?
log ?
(?Xj) for each en-tailed variable j, and log ?
(Xj) for each entailingvariable j.1This inference optimises Xi<?Xjforeach entailing j plus Xi>?Xjfor each entailed j,plus a maximum entropy regulariser on Xi.
Neg-ative entailment relations, if they exist, can alsobe incorporated with some additional approxima-tions.
Complex priors can also be incorporatedthrough their log-odds, simulating the inclusion ofthe prior within the mean-field approximation.Given its dependence on mean-field approxima-tions, it is an empirical question to what extent weshould view this model as computing real entail-ment probabilities and to what extent we shouldview it as a well-motivated non-linear mappingfor which we simply optimise the input-output be-haviour (as for neural networks (Henderson andTitov, 2010)).
In Sections 3 and 5 we argue for theformer (stronger) view.3 Interpreting Word2Vec VectorsTo evaluate how well the proposed framework pro-vides a formal foundation for the distributional se-mantics of entailment, we use it to re-interpret an1It is interesting to note that ?
log ?
(?Xj) is a non-negative transform of Xj, similar to the ReLU nonlinear-ity which is popular in deep neural networks (Glorot et al,2011).
log ?
(Xj) is the analogous non-positive transform.2055existing model of distributional semantics in termsof semantic entailment.
There has been a lot ofwork on how to use the distribution of contexts inwhich a word occurs to induce a vector represen-tation of the semantics of words.
In this paper,we leverage this previous work on distributionalsemantics by re-interpreting a previous distribu-tional semantic model and using this understand-ing to map its vector-space word embeddings tovectors in the proposed framework.
We then usethe proposed operators to predict entailment be-tween words using these vectors.
In Section 5 be-low, we evaluate these predictions on the task ofhyponymy detection.
In this section we motivatethree different ways to interpret the Word2Vec(Mikolov et al, 2013a; Mikolov et al, 2013b) dis-tributional semantic model as an approximation toan entailment-based model of the semantic rela-tionship between a word and its context.Distributional semantics learns the semantics ofwords by looking at the distribution of contextsin which they occur.
To model this relationship,we assume that the semantic features of a wordare (statistically speaking) redundant with those ofits context words, and consistent with those of itscontext words.
We model these properties usinga hidden vector which is the consistent unificationof the features of the middle word and the context.In other words, there must exist a hidden vectorwhich entails both of these vectors, and is consis-tent with prior constraints on vectors.
We split thisinto two steps, inference of the hidden vector Yfrom the middle vector Xm, context vectors Xcand prior, and computing the log-probability (7)that this hidden vector entails the middle and con-text vectors:maxY(logP (y, y?xm, y?xc)) (7)We interpret Word2Vec?s Skip-Gram model aslearning its context and middle word vectors sothat the log-probability of this entailment is highfor the observed context words and low for other(sampled) context words.
The word embeddingsproduced by Word2Vec are only related to the vec-tors Xmassigned to the middle words; contextvectors are computed but not output.
We modelthe context vectors X?cas combining (as in equa-tion (5)) information about a context word itselfwith information which can be inferred from thisword given the prior, X?c= ?c?
log ?
(?Xc).The numbers in the vectors output by Word2Vecare real numbers between negative infinity and in-finity, so the simplest interpretation of them is asthe log-odds of a feature being known.
In this casewe can treat these vectors directly as theXmin themodel.
The inferred hidden vector Y can then becalculated using the model of backward inferencefrom the previous section.Y = ?c?
log ?(?Xc)?
log ?
(?Xm)= X?c?
log ?
(?Xm)Since the unification Y of context and middleword features is computed using backward infer-ence, we use the backward-inference operator>?tocalculate how successful that unification was.
Thisgives us the final score:logP (y, y?xm, y?xc)?
Y>?Xm+ Y>?Xc+??
(?Y )?
?c= Y>?Xm+??
(?Y )?X?cThis is a natural interpretation, but it ignores theequivalence in Word2Vec between pairs of posi-tive values and pairs of negative values, due to itsuse of the dot product.
As a more accurate in-terpretation, we interpret each Word2Vec dimen-sion as specifying whether its feature is knownto be true or known to be false.
Translating thisWord2Vec vector into a vector in our entailmentvector space, we get one copy Y+of the vectorrepresenting known-to-be-true features and a sec-ond negated duplicate Y?of the vector represent-ing known-to-be-false features, which we concate-nate to get our representation Y .Y+= X?c?
log ?
(?Xm)Y?= ?X?c?
log ?
(Xm)logP (y, y?xm, y?xc)?
Y+>?Xm+??
(?Y+)?X?c+ Y?>?
(?Xm) +??(?Y?)?
(?X?c)As a third alternative, we modify this latter in-terpretation with some probability mass reservedfor unknown in the vicinity of zero.
By subtract-ing 1 from both the original and negated copies ofeach dimension, we get a probability of unknownof 1??(Xm?1)?
?(?Xm?1).
This gives us:Y+= X?c?
log ?(?
(Xm?1))Y?= ?X?c?
log ?(?
(?Xm?1))logP (y, y?xm, y?xc)?
Y+>?
(Xm?1) +??
(?Y+)?X?c+ Y?>?
(?Xm?1)) +??(?Y?)?
(?X?c)2056Figure 1: The learning gradients for Word2Vec,the log-odds>?, and the unk dup>?interpretationof its vectors.To understand better the relative accuracy ofthese three interpretations, we compared the train-ing gradient which Word2Vec uses to train itsmiddle-word vectors to the training gradient foreach of these interpretations.
We plotted these gra-dients for the range of values typically found inWord2Vec vectors for both the middle vector andthe context vector.
Figure 1 shows three of theseplots.
As expected, the second interpretation ismore accurate than the first because its plot is anti-symmetric around the diagonal, like the Word2Vecgradient.
In the third alternative, the constant 1was chosen to optimise this match, producing aclose match to the Word2Vec training gradient, asshown in Figure 1 (Word2Vec versus Unk dup).Thus, Word2Vec can be seen as a good ap-proximation to the third model, and a progres-sively worse approximation to the second and firstmodels.
Therefore, if the entailment-based distri-butional semantic model we propose is accurate,then we would expect the best accuracy in hy-ponymy detection using the third interpretation ofWord2Vec vectors, and progressively worse accu-racy for the other two interpretations.
As we willsee in Section 5, this prediction holds.4 Related WorkThere has been a significant amount of work on us-ing distributional-semantic vectors for hyponymydetection, using supervised, semi-supervised orunsupervised methods (e.g.
(Yu et al, 2015; Nec-sulescu et al, 2015; Vylomova et al, 2015; Weedset al, 2014; Fu et al, 2015; Rei and Briscoe,2014)).
Because our main concern is modellingentailment within a vector space, we do not do athorough comparison to models which use mea-sures computed outside the vector space (e.g.symmetric measures (LIN (Lin, 1998)), asym-metric measures (WeedsPrec (Weeds and Weir,2003; Weeds et al, 2004), balAPinc (Kotlermanet al, 2010), invCL (Lenci and Benotto, 2012))and entropy-based measures (SLQS (Santus et al,2014))), nor to models which encode hyponymy inthe parameters of a vector-space operator or clas-sifier (Fu et al, 2015; Roller et al, 2014; Baroniet al, 2012)).
We also limit our evaluation of lex-ical entailment to hyponymy, not including otherrelated lexical relations (cf.
(Weeds et al, 2014;Vylomova et al, 2015; Turney and Mohammad,2014; Levy et al, 2014)), leaving more complexcases to future work on compositional semantics.We are also not concerned with models or evalua-tions which require supervised learning about in-dividual words, instead limiting ourselves to semi-supervised learning where the words in the train-ing and test sets are disjoint.For these reasons, in our evaluations we repli-cate the experimental setup of Weeds et al (2014),for both unsupervised and semi-supervised mod-els.
Within this setup, we compare to the results ofthe models evaluated by Weeds et al (2014) and topreviously proposed vector-space operators.
Thisincludes one vector space operator for hyponymywhich doesn?t have trained parameters, proposedby Rei and Briscoe (2014), called weighted cosine.The dimensions of the dot product (normalisedto make it a cosine measure) are weighted to putmore weight on the larger values in the entailed(hypernym) vector.We base this evaluation on the Word2Vec(Mikolov et al, 2013a; Mikolov et al, 2013b) dis-tributional semantic model and its publicly avail-able word embeddings.
We choose it because itis popular, simple, fast, and its embeddings havebeen derived from a very large corpus.
Levy andGoldberg (2014) showed that it is closely relatedto the previous PMI-based distributional semanticmodels (e.g.
(Turney and Pantel, 2010)).The most similar previous work, in terms of mo-tivation and aims, is that of Vilnis and McCallum(2015).
They also model entailment directly usinga vector space, without training a classifier.
Butinstead of representing words as a point in a vec-tor space (as in this work), they represent wordsas a Gaussian distribution over points in a vectorspace.
This allows them to represent the extent towhich a feature is known versus unknown as the2057amount of variance in the distribution for that fea-ture?s dimension.
While nicely motivated theoret-ically, the model appears to be more computation-ally expensive than the one proposed here, particu-larly for inferring vectors.
They do make unsuper-vised predictions of hyponymy relations with theirlearned vector distributions, using KL-divergencebetween the distributions for the two words.
Theyevaluate their models on the hyponymy data from(Baroni et al, 2012).
As discussed further in sec-tion 5.2, our best models achieve non-significantlybetter average precision than their best models.The semi-supervised model of Kruszewski et al(2015) also models entailment in a vector space,but they use a discrete vector space.
They traina mapping from distributional semantic vectorsto Boolean vectors such that feature inclusion re-spects a training set of entailment relations.
Theythen use feature inclusion to predict hyponymy,and other lexical entailment relations.
This ap-proach is similar to the one used in our semi-supervised experiments, except that their discreteentailment prediction operator is very differentfrom our proposed entailment operators.5 EvaluationTo evaluate whether the proposed framework is aneffective model of entailment in vector spaces, weapply the interpretations from Section 3 to pub-licly available word embeddings and use them topredict the hyponymy relations in a benchmarkdataset.
This framework predicts that the more ac-curate interpretations of Word2Vec result in moreaccurate unsupervised models of hyponymy.
Weevaluate on detecting hyponymy relations betweenwords because hyponymy is the canonical type oflexical entailment; most of the semantic featuresof a hypernym (e.g.
?animal?)
must be included inthe semantic features of the hyponym (e.g.
?dog?
).We evaluate in both a fully unsupervised setup anda semi-supervised setup.5.1 Hyponymy with Word2Vec VectorsFor our evaluation on hyponymy detection, wereplicate the experimental setup of Weeds et al(2014), using their selection of word pairs2fromthe BLESS dataset (Baroni and Lenci, 2011).32https://github.com/SussexCompSem/learninghypernyms3Of the 1667 word pairs in this data, 24 were removedbecause we do not have an embedding for one of the words.These noun-noun word pairs include positive hy-ponymy pairs, plus negative pairs consisting ofsome other hyponymy pairs reversed, some pairsin other semantic relations, and some randompairs.
Their selection is balanced between positiveand negative examples, so that accuracy can beused as the performance measure.
For their semi-supervised experiments, ten-fold cross validationis used, where for each test set, items are removedfrom the associated training set if they contain anyword from the test set.
Thus, the vocabulary ofthe training and testing sets are always disjoint,thereby requiring that the models learn about thevector space and not about the words themselves.We had to perform our own 10-fold split, but applythe same procedure to filter the training set.We could not replicate the word embeddingsused in Weeds et al (2014), so instead we use pub-licly available word embeddings.4These vectorswere trained with the Word2Vec software appliedto about 100 billion words of the Google-Newsdataset, and have 300 dimensions.The hyponymy detection results are given in Ta-ble 3, including both unsupervised (upper box)and semi-supervised (lower box) experiments.
Wereport two measures of performance, hyponymydetection accuracy (50% Acc) and direction clas-sification accuracy (Dir Acc).
Since all the opera-tors only determine a score, we need to choose athreshold to get detection accuracies.
Given thatthe proportion of positive examples in the datasethas been artificially set at 50%, we threshold eachmodel?s score at the point where the proportion ofpositive examples output is 50%, which we call?50% Acc?.
Thus the threshold is set after seeingthe testing inputs but not their target labels.Direction classification accuracy (Dir Acc) in-dicates how well the method distinguishes the rel-ative abstractness of two nouns.
Given a pair ofnouns which are in a hyponymy relation, it classi-fies which word is the hypernym and which is thehyponym.
This measure only considers positiveexamples and chooses one of two directions, so itis inherently a balanced binary classification task.Classification is performed by simply comparingthe scores in both directions.
If both directionsproduce the same score, the expected random ac-curacy (50%) is used.As representative of previous work, we report4https://code.google.com/archive/p/word2vec/2058operator supervision 50% Acc Dir AccWeeds et.al.
None 58% ?log-odds<?None 54.0% 55.9%weighted cos None 55.5% 57.9%dot None 56.3% 50%dif None 56.9% 59.6%log-odds ??
None 57.0% 59.4%log-odds>?None 60.1%* 62.2%dup>?None 61.7% 68.8%unk dup ??
None 63.4%* 68.8%unk dup>?None 64.5% 68.8%Weeds et.al.
SVM 75% ?mapped dif cross ent 64.3% 72.3%mapped<?cross ent 74.5% 91.0%mapped ??
cross ent 77.5% 92.3%mapped>?cross ent 80.1% 90.0%Table 3: Accuracies on the BLESS data fromWeeds et al (2014), for hyponymy detection (50%Acc) and hyponymy direction classification (DirAcc), in the unsupervised (upper box) and semi-supervised (lower box) experiments.
For unsuper-vised accuracies, * marks a significant differencewith the previous row.the best results from Weeds et al (2014), whotry a number of unsupervised and semi-supervisedmodels, and use the same testing methodologyand hyponymy data.
However, note that theirword embeddings are different.
For the semi-supervised models, Weeds et al (2014) trains clas-sifiers, which are potentially more powerful thanour linear vector mappings.
We also compare theproposed operators to the dot product (dot),5vec-tor differences (dif ), and the weighted cosine ofRei and Briscoe (2014) (weighted cos), all com-puted with the same word embeddings as for theproposed operators.In Section 3 we argued for three progressivelymore accurate interpretations of Word2Vec vec-tors in the proposed framework, the log-odds inter-pretation (log-odds>?
), the negated duplicate inter-pretation (dup>?
), and the negated duplicate inter-pretation with unknown around zero (unk dup>?
).We also evaluate using the factorised calculationof entailment (log-odds ?
?, unk dup ??
), and thebackward-inference entailment operator (log-odds<?
), neither of which match the proposed interpre-5We also tested the cosine measure, but results were veryslightly worse than dot.tations.
For the semi-supervised case, we traina linear vector-space mapping into a new vectorspace, in which we apply the operators (mappedoperators).
All these results are discussed in thenext two subsections.5.2 Unsupervised Hyponymy DetectionThe first set of experiments evaluate the vector-space operators in unsupervised models of hy-ponymy detection.
The proposed models are com-pared to the dot product, because this is the stan-dard vector-space operator and has been shown tocapture semantic similarity very well.
However,because the dot product is a symmetric operator,it always performs at chance for direction clas-sification.
Another vector-space operator whichhas received much attention recently is vector dif-ferences.
This is used (with vector sum) to per-form semantic transforms, such as ?king - male+ female = queen?, and has previously been usedfor modelling hyponymy (Vylomova et al, 2015;Weeds et al, 2014).
For our purposes, we sum thepairwise differences to get a score which we usefor hyponymy detection.For the unsupervised results in the upper box oftable 3, the best unsupervised model of Weeds etal.
(2014), and the operators dot, dif and weightedcos all perform similarly on accuracy, as does thelog-odds factorised entailment calculation (log-odds ??).
The forward-inference entailment op-erator (log-odds<?)
performs above chance but notwell, as expected given the backward-inference-based interpretation of Word2Vec vectors.
By def-inition, dot is at chance for direction classification,but the other models all perform better, indicat-ing that all these operators are able to measurerelative abstractness.
As predicted, the>?opera-tor performs significantly better than all these re-sults on accuracy, as well as on direction classifi-cation, even assuming the log-odds interpretationof Word2Vec vectors.When we move to the more accurate interpreta-tion of Word2Vec vectors as specifying both orig-inal and negated features (dup>?
), we improve(non-significantly) on the log-odds interpretation.Finally, the third and most accurate interpretation,where values around zero can be unknown (unkdup>?
), achieves the best results in unsupervisedhyponymy detection, as well as for direction clas-sification.
Changing to the factorised entailmentoperator (unk dup ??)
is worse but also signifi-2059cantly better than the other accuracies.To allow a direct comparison to the modelof Vilnis and McCallum (2015), we also evalu-ated the unsupervised models on the hyponymydata from (Baroni et al, 2012).
Our best modelachieved 81% average precision on this dataset,non-significantly better than the 80% achieved bythe best model of Vilnis and McCallum (2015).5.3 Semi-supervised Hyponymy DetectionSince the unsupervised learning of word embed-dings may reflect many context-word correlationswhich have nothing to do with hyponymy, wealso consider a semi-supervised setting.
Addingsome supervision helps distinguish features thatcapture semantic properties from other featureswhich are not relevant to hyponymy detection.
Buteven with supervision, we still want the resultingmodel to be captured in a vector space, and notin a parametrised scoring function.
Thus, we trainmappings from the Word2Vec word vectors to newword vectors, and then apply the entailment opera-tors in this new vector space to predict hyponymy.Because the words in the testing set are always dis-joint from the words in the training set, this experi-ment measures how well the original unsupervisedvector space captures features that generalise en-tailment across words, and not how well the map-ping can learn about individual words.Our objective is to learn a mapping to a newvector space in which an operator can be appliedto predict hyponymy.
We train linear mappingsfor the>?operator (mapped>?)
and for vector dif-ferences (mapped dif ), since these were the bestperforming proposed operator and baseline opera-tor, respectively, in the unsupervised experiments.We do not use the duplicated interpretations be-cause these transforms are subsumed by the abilityto learn a linear mapping.6Previous work on usingvector differences for semi-supervised hyponymydetection has used a linear SVM (Vylomova et al,2015; Weeds et al, 2014), which is mathemati-cally equivalent to our vector-differences model,except that we use cross entropy loss and they usea large-margin loss and SVM training.The semi-supervised results in the bottom boxof table 3 show a similar pattern to the unsuper-vised results.7The>?operator achieves the best6Empirical results confirm that this is in practice the case,so we do not include these results in the table.7It is not clear how to measure significance for cross-validation results, so we do not attempt to do so.generalisation from training word vectors to test-ing word vectors.
The mapped>?model has thebest accuracy, followed by the factorised entail-ment operator mapped ??
and Weeds et al (2014).Direction accuracies of all the proposed operators(mapped>?, mapped ?
?, mapped<?)
reach into the90?s.
The dif operator performs particularly poorlyin this mapped setting, perhaps because both themapping and the operator are linear.
These semi-supervised results again support our distributional-semantic interpretations of Word2Vec vectors andtheir associated entailment operator>?.6 ConclusionIn this work, we propose a vector-space modelwhich provides a formal foundation for a distri-butional semantics of entailment.
We developeda mean-field approximation to probabilistic entail-ment between vectors which represent known ver-sus unknown features.
And we used this frame-work to derive vector operators for entailment andvector inference equations for entailment graphs.This framework allows us to reinterpret Word2Vecas approximating an entailment-based distribu-tional semantic model of words in context, andshow that more accurate interpretations result inmore accurate unsupervised models of lexical en-tailment, achieving better accuracies than previ-ous models.
Semi-supervised evaluations confirmthese results.A crucial distinction between the semi-supervised models here and much previous workis that they learn a mapping into a vector spacewhich represents entailment, rather than learninga parametrised entailment classifier.
Within thisnew vector space, the entailment operators andinference equations apply, thereby generalisingnaturally from these lexical representations to thecompositional semantics of multi-word expres-sions and sentences.
Further work is needed toexplore the full power of these abilities to extractinformation about entailment from both unla-belled text and labelled entailment data, encode itall in a single vector space, and efficiently performcomplex inferences about vectors and entailments.This future work on compositional distributionalsemantics should further demonstrate the fullpower of the proposed framework for modellingentailment in a vector space.2060ReferencesMarco Baroni and Alessandro Lenci.
2011.
Howwe blessed distributional semantic evaluation.
InProceedings of the GEMS 2011 Workshop on GE-ometrical Models of Natural Language Semantics,GEMS ?11, pages 1?10.
Association for Computa-tional Linguistics.Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do,and Chung-chieh Shan.
2012.
Entailment above theword level in distributional semantics.
In Proceed-ings of the 13th Conference of the European Chap-ter of the Association for Computational Linguistics(EACL), pages 23?32, Avignon, France.
Associationfor Computational Linguistics.Scott Deerwester, Susan T. Dumais, George W. Fur-nas, Thomas K. Landauer, and Richard Harshman.1990.
Indexing by latent semantic analysis.
Jour-nal of the American Society for Information Science,41(6):391?407.Ruiji Fu, Jiang Guo, Bing Qin, Wanxiang Che, HaifengWang, and Ting Liu.
2015.
Learning semantic hi-erarchies: A continuous vector space approach.
Au-dio, Speech, and Language Processing, IEEE/ACMTransactions on, 23(3):461?471.Xavier Glorot, Antoine Bordes, and Yoshua Bengio.2011.
Deep sparse rectifier neural networks.
In In-ternational Conference on Artificial Intelligence andStatistics, pages 315?323.James Henderson and Ivan Titov.
2010.
Incre-mental sigmoid belief networks for grammar learn-ing.
Journal of Machine Learning Research,11(Dec):3541?3570.Lili Kotlerman, Ido Dagan, Idan Szpektor, and MaayanZhitomirsky-geffet.
2010.
Directional distributionalsimilarity for lexical inference.
Natural LanguageEngineering, 16(4):359?389.Germn Kruszewski, Denis Paperno, and Marco Baroni.2015.
Deriving boolean structures from distribu-tional vectors.
Transactions of the Association forComputational Linguistics, 3:375?388.Alessandro Lenci and Giulia Benotto.
2012.
Identify-ing hypernyms in distributional semantic spaces.
InProceedings of the First Joint Conference on Lexicaland Computational Semantics, SemEval ?12, pages75?79.
Association for Computational Linguistics.Omer Levy and Yoav Goldberg.
2014.
Neuralword embedding as implicit matrix factorization.In Z. Ghahramani, M. Welling, C. Cortes, N. D.Lawrence, and K. Q. Weinberger, editors, Advancesin Neural Information Processing Systems 27, pages2177?2185.
Curran Associates, Inc.Omer Levy, Ido Dagan, and Jacob Goldberger.
2014.Focused entailment graphs for open ie propositions.In Proceedings of the Eighteenth Conference onComputational Natural Language Learning, pages87?97, Ann Arbor, Michigan.
Association for Com-putational Linguistics.Omer Levy, Steffen Remus, Chris Biemann, and IdoDagan.
2015.
Do supervised distributional meth-ods really learn lexical inference relations?
In Pro-ceedings of the 2015 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages970?976, Denver, Colorado, May?June.
Associationfor Computational Linguistics.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of the 17th Inter-national Conference on Computational Linguistics -Volume 2, COLING ?98, pages 768?774.
Associa-tion for Computational Linguistics.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013a.
Efficient estimation of word represen-tations in vector space.
CoRR, abs/1301.3781.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013b.
Distributed repre-sentations of words and phrases and their compo-sitionality.
In C.J.C.
Burges, L. Bottou, M. Welling,Z.
Ghahramani, and K.Q.
Weinberger, editors, Ad-vances in Neural Information Processing Systems26, pages 3111?3119.
Curran Associates, Inc.Silvia Necsulescu, Sara Mendes, David Jurgens, N?uriaBel, and Roberto Navigli.
2015.
Reading betweenthe lines: Overcoming data sparsity for accurateclassification of lexical relationships.
In Proceed-ings of the Fourth Joint Conference on Lexical andComputational Semantics, pages 182?192, Denver,Colorado.
Association for Computational Linguis-tics.Marek Rei and Ted Briscoe.
2014.
Looking for hy-ponyms in vector space.
In Proceedings of the Eigh-teenth Conference on Computational Natural Lan-guage Learning, pages 68?77, Ann Arbor, Michi-gan.
Association for Computational Linguistics.Stephen Roller, Katrin Erk, and Gemma Boleda.
2014.Inclusive yet selective: Supervised distributional hy-pernymy detection.
In Proceedings of COLING2014, the 25th International Conference on Compu-tational Linguistics: Technical Papers, pages 1025?1036, Dublin, Ireland.
Dublin City University andAssociation for Computational Linguistics.Enrico Santus, Alessandro Lenci, Qin Lu, andSabine Schulte im Walde.
2014.
Chasing hyper-nyms in vector spaces with entropy.
In Proceed-ings of the 14th Conference of the European Chap-ter of the Association for Computational Linguistics,EACL 2014, April 26-30, 2014, Gothenburg, Swe-den, pages 38?42.Hinrich Sch?utze.
1993.
Word space.
In Advancesin Neural Information Processing Systems 5, pages895?902.
Morgan Kaufmann.2061Peter D. Turney and Saif M. Mohammad.
2014.
Ex-periments with three approaches to recognizing lex-ical entailment.
CoRR, abs/1401.8269.Peter D. Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of seman-tics.
J. Artif.
Int.
Res., 37(1):141?188.Luke Vilnis and Andrew McCallum.
2015.
Word rep-resentations via Gaussian embedding.
In Proceed-ings of the International Conference on LearningRepresentations 2015 (ICLR).Ekaterina Vylomova, Laura Rimell, Trevor Cohn, andTimothy Baldwin.
2015.
Take and took, gaggle andgoose, book and read: Evaluating the utility of vec-tor differences for lexical relation learning.
In CoRR2015.Julie Weeds and David Weir.
2003.
A general frame-work for distributional similarity.
In Proceedings ofthe 2003 Conference on Empirical Methods in Nat-ural Language Processing, EMNLP ?03, pages 81?88.Julie Weeds, David Weir, and Diana McCarthy.
2004.Characterising measures of lexical distributionalsimilarity.
In Proceedings of the 20th InternationalConference on Computational Linguistics, COLING?04, pages 1015?1021.
Association for Computa-tional Linguistics.Julie Weeds, Daoud Clarke, Jeremy Reffin, David Weir,and Bill Keller.
2014.
Learning to distinguish hy-pernyms and co-hyponyms.
In Proceedings of COL-ING 2014, the 25th International Conference onComputational Linguistics: Technical Papers, pages2249?2259, Dublin, Ireland.
Dublin City Universityand Association for Computational Linguistics.Zheng Yu, Haixun Wang, Xuemin Lin, and Min Wang.2015.
Learning term embeddings for hypernymyidentification.
In Proceedings of the Twenty-FourthInternational Joint Conference on Artificial Intelli-gence, IJCAI 2015.
AAAI Press / International JointConferences on Artificial Intelligence.2062
