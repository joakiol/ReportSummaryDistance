Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 12?22,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsSelf-training with Products of Latent Variable GrammarsZhongqiang Huang?
?UMIACSUniversity of MarylandCollege Park, MDzqhuang@umd.eduMary Harper??
?HLT Center of ExcellenceJohns Hopkins UniversityBaltimore, MDmharper@umd.eduSlav Petrov?
?Google Research76 Ninth AvenueNew York, NYslav@google.comAbstractWe study self-training with products of latentvariable grammars in this paper.
We showthat increasing the quality of the automaticallyparsed data used for self-training gives higheraccuracy self-trained grammars.
Our genera-tive self-trained grammars reach F scores of91.6 on the WSJ test set and surpass evendiscriminative reranking systems without self-training.
Additionally, we show that multi-ple self-trained grammars can be combined ina product model to achieve even higher ac-curacy.
The product model is most effectivewhen the individual underlying grammars aremost diverse.
Combining multiple grammarsthat were self-trained on disjoint sets of un-labeled data results in a final test accuracy of92.5% on the WSJ test set and 89.6% on ourBroadcast News test set.1 IntroductionThe latent variable approach of Petrov et al (2006)is capable of learning high accuracy context-freegrammars directly from a raw treebank.
It startsfrom a coarse treebank grammar (Charniak, 1997),and uses latent variables to refine the context-freeassumptions encoded in the grammar.
A hierarchi-cal split-and-merge algorithm introduces grammarcomplexity gradually, iteratively splitting (and po-tentially merging back) each observed treebank cat-egory into a number of increasingly refined latentsubcategories.
The Expectation Maximization (EM)algorithm is used to train the model, guaranteeingthat each EM iteration will increase the training like-lihood.
However, because the latent variable gram-mars are not explicitly regularized, EM keeps fit-ting the training data and eventually begins over-fitting (Liang et al, 2007).
Moreover, EM is a lo-cal method, making no promises regarding the finalpoint of convergence when initialized from differentrandom seeds.
Recently, Petrov (2010) showed thatsubstantial differences between the learned gram-mars remain, even if the hierarchical splitting re-duces the variance across independent runs of EM.In order to counteract the overfitting behavior,Petrov et al (2006) introduced a linear smoothingprocedure that allows training grammars for 6 split-merge (SM) rounds without overfitting.
The in-creased expressiveness of the model, combined withthe more robust parameter estimates provided by thesmoothing, results in a nice increase in parsing ac-curacy on a held-out set.
However, as reported byPetrov (2009) and Huang and Harper (2009), an ad-ditional 7th SM round actually hurts performance.Huang and Harper (2009) addressed the issue ofdata sparsity and overfitting from a different angle.They showed that self-training latent variable gram-mars on their own output can mitigate data spar-sity issues and improve parsing accuracy.
Becausethe capacity of the model can grow with the sizeof the training data, latent variable grammars areable to benefit from the additional training data, eventhough it is not perfectly labeled.
Consequently,they also found that a 7th round of SM training wasbeneficial in the presence of large amounts of train-ing data.
However, variation still remains in theirself-trained grammars and they had to use a held-outset for model selection.The observation of variation is not surprising;EM?s tendency to get stuck in local maxima has beenstudied extensively in the literature, resulting in vari-ous proposals for model selection methods (e.g., see12Burnham and Anderson (2002)).
What is perhapsmore surprising is that the different latent variablegrammars seem to capture complementary aspectsof the data.
Petrov (2010) showed that a simple ran-domization scheme produces widely varying gram-mars.
Quite serendipitously, these grammars canbe combined into an unweighted product model thatsubstantially outperforms the individual grammars.In this paper, we combine the ideas of self-training and product models and show that bothtechniques provide complementary effects.
We hy-pothesize that the main factors contributing to thefinal accuracy of the product model of self-trainedgrammars are (i) the accuracy of the grammar usedto parse the unlabeled data for retraining (singlegrammar versus product of grammars) and (ii) thediversity of the grammars that are being combined(self-trained grammars trained using the same auto-matically labeled subset or different subsets).
Weconduct a series of analyses to develop an under-standing of these factors, and conclude that both di-mensions are important for obtaining significant im-provements over the standard product models.2 Experimental Setup2.1 DataWe conducted experiments in two genres: newswiretext and broadcast news transcripts.
For thenewswire studies, we used the standard setup (sec-tions 02-21 for training, 22 for development, and 23for final test) of the WSJ Penn Treebank (Marcus etal., 1999) for supervised training.
The BLLIP cor-pus (Charniak et al, 2000) was used as a source ofunlabeled data for self-training the WSJ grammars.We ignored the parse trees contained in the BLLIPcorpus and retained only the sentences, which arealready segmented and tokenized for parsing (e.g.,contractions are split into two tokens and punctua-tion is separated from the words).
We partitionedthe 1,769,055 BLLIP sentences into 10 equally sizedsubsets1.For broadcast news (BN), we utilized the Broad-1We corrected some of the most egregious sentence segmen-tation problems in this corpus, and so the number of sentences isdifferent than if one simply pulled the fringe of the trees.
It wasnot uncommon for a sentence split to occur on abbreviations,such as Adm.cast News treebank from Ontonotes (Weischedel etal., 2008) together with the WSJ Penn Treebank forsupervised training, because their combination re-sults in better parser models compared to using thelimited-sized BN corpus alone (86.7 F vs. 85.2 F).The files in the Broadcast News treebank representnews stories collected during different time periodswith a diversity of topics.
In order to obtain a rep-resentative split of train-test-development sets, wedivided them into blocks of 10 files sorted by alpha-betical filename order.
We used the first file in eachblock for development, the second for test, and theremaining files for training.
This training set wasthen combined with the entire WSJ treebank.
Wealso used 10 equally sized subsets from the Hub4CSR 1996 utterances (Garofolo et al, 1996) for self-training.
The Hub 4 transcripts are markedly noisierthan the BLLIP corpus is, in part because it is harderto sentence segment, but also because it was pro-duced by human transcription of spoken language.The treebanks were pre-processed differently forthe two genres.
For newswire, we used a slightlymodified version of the WSJ treebank: emptynodes and function labels were deleted and auxiliaryverbs were replaced with AUXB, AUXG, AUXZ,AUXD, or AUXN to represent infinitive, progres-sive, present, past, or past participle auxiliaries2.The targeted use of the broadcast models is for pars-ing broadcast news transcripts for language mod-els in speech recognition systems.
Therefore, inaddition to applying the transformations used fornewswire, we also replaced symbolic expressionswith verbal forms (e.g., $5 was replaced with fivedollars) and removed punctuation and case.
TheHub4 data was segmented into utterances, punctua-tion was removed, words were down-cased, and con-tractions were tokenized for parsing.
Table 1 sum-marizes the data set sizes used in our experiments,together with average sentence length and standarddeviation.2.2 ScoringParses from all models are compared with respectivegold standard parses using SParseval bracket scor-ing (Roark et al, 2006).
This scoring tool pro-2Parsing accuracy is marginally affected.
The average over10 SM6 grammars with the transformation is 90.5 compared to90.4 F without it, a 0.1% average improvement.13Genre Statistics Train Dev Test UnlabeledNewswire# sentences 39.8k 1.7k 2.4k 1,769.1k# words 950.0k 40.1k 56.7k 43,057.0klength Avg./Std.
28.9/11.2 25.1/11.8 25.1/12.0 24.3/10.9Broadcast News# sentences 59.0k 1.0k 1.1k 4,386.5k# words 1,281.1k 17.1k 19.4k 77,687.9klength Avg./Std.
17.3/11.3 17.4/11.3 17.7/11.4 17.7/12.8Table 1: The number of words and sentences, together with average (Avg.)
sentence length and its standard deviation(Std.
), for the data sets used in our experiments.duces scores that are identical to those producedby EVALB for WSJ.
For Broadcast News, SParse-val applies Charniak and Johnson?s (Charniak andJohnson, 2001) scoring method for EDITED nodes3.Using this method, BN scores were slightly (.05-.1)lower than if EDITED constituents were treated likeany other, as in EVALB.2.3 Latent Variable GrammarsWe use the latent variable grammar (Matsuzaki etal., 2005; Petrov et al, 2006) implementation ofHuang and Harper (2009) in this work.
Latent vari-able grammars augment the observed parse trees inthe treebank with a latent variable at each tree node.This effectively splits each observed category intoa set of latent subcategories.
An EM-algorithm isused to fit the model by maximizing the joint like-lihood of parse trees and sentences.
To allocate thegrammar complexity only where needed, a simplesplit-and-merge procedure is applied.
In every split-merge (SM) round, each latent variable is first splitin two and the model is re-estimated.
A likelihoodcriterion is used to merge back the least useful splits(50% merge rate for these experiments).
This itera-tive refinement proceeds for 7 rounds, at which pointparsing performance on a held-out set levels off andtraining becomes prohibitively slow.Since EM is a local method, different initial-izations will result in different grammars.
Infact, Petrov (2010) recently showed that this EM-algorithm is very unstable and converges to widelyvarying local maxima.
These local maxima corre-3Non-terminal subconstituents of EDITED nodes are re-moved so that the terminal constituents become immediate chil-dren of a single EDITED node, adjacent EDITED nodes aremerged, and they are ignored for span calculations of the otherconstituents.spond to different high quality latent variable gram-mars that have captured different types of patterns inthe data.
Because the individual models?
mistakesare independent to some extent, multiple grammarscan be effectively combined into an unweightedproduct model of much higher accuracy.
We buildupon this line of work and investigate methods toexploit products of latent variable grammars in thecontext of self-training.3 Self-training MethodologyDifferent types of parser self-training have been pro-posed in the literature over the years.
All of theminvolve parsing a set of unlabeled sentences with abaseline parser and then estimating a new parser bycombining this automatically parsed data with theoriginal training data.
McClosky et al (2006) pre-sented a very effective method for self-training atwo-stage parsing system consisting of a first-stagegenerative lexicalized parser and a second-stage dis-criminative reranker.
In their approach, a largeamount of unlabeled text is parsed by the two-stagesystem and the parameters of the first-stage lexical-ized parser are then re-estimated taking the countsfrom the automatically parsed data into considera-tion.More recently Huang and Harper (2009) pre-sented a self-training procedure based on an EM-algorithm.
They showed that the EM-algorithm thatis typically used to fit a latent variable grammar(Matsuzaki et al, 2005; Petrov et al, 2006) to a tree-bank can also be used for self-training on automati-cally parsed sentences.
In this paper, we investigateself-training with products of latent variable gram-mars.
We consider three training scenarios:ST-Reg Training Use the best single grammar to14Regular Best Average ProductSM6 90.8 90.5 92.0SM7 90.4 90.1 92.2Table 2: Performance of the regular grammars and theirproducts on the WSJ development set.parse a single subset of the unlabeled data andtrain 10 self-trained grammars using this singleset.ST-Prod Training Use the product model to parsea single subset of the unlabeled data and train10 self-trained grammars using this single set.ST-Prod-Mult Training Use the product model toparse all 10 subsets of the unlabeled data andtrain 10 self-trained grammars, each using adifferent subset.The resulting grammars can be either used individu-ally or combined in a product model.These three conditions provide different insights.The first experiment allows us to investigate theeffectiveness of product models for standard self-trained grammars.
The second experiment enablesus to quantify how important the accuracy of thebaseline parser is for self-training.
Finally, the thirdexperiment investigates a method for injecting someadditional diversity into the individual grammars todetermine whether a product model is most success-ful when there is more variance among the individ-ual models.Our initial experiments and analysis will focus onthe development set of WSJ.
We will then followup with an analysis of broadcast news (BN) to de-termine whether the findings generalize to a second,less structured type of data.
It is important to con-struct grammars capable of parsing this type of dataaccurately and consistently in order to support struc-tured language modeling (e.g., (Wang and Harper,2002; Filimonov and Harper, 2009)).4 Newswire ExperimentsIn this section, we compare single grammars andtheir products that are trained in the standard waywith gold WSJ training data, as well as the threeself-training scenarios discussed in Section 3.
WeST-Reg Best Average ProductSM6 91.5 91.2 92.0SM7 91.6 91.5 92.4Table 3: Performance of the ST-Reg grammars and theirproducts on the WSJ development set.report the F scores of both SM6 and SM7 grammarson the development set in order to evaluate the ef-fect of model complexity on the performance of theself-trained and product models.
Note that we use6th round grammars to produce the automatic parsetrees for the self-training experiments.
Parsing withthe product of the 7th round grammars is slow andrequires a large amount of memory (32GB).
Sincewe had limited access to such machines, it was in-feasible for us to parse all of the unlabeled data withthe SM7 product grammars.4.1 Regular TrainingWe begin by training ten latent variable models ini-tialized with different random seeds using the goldWSJ training set.
Results are presented in Table 2.The best F score attained by the individual SM6grammars on the development set is 90.8, with anaverage score of 90.5.
The product of grammarsachieves a significantly improved accuracy at 92.04.Notice that the individual SM7 grammars performworse on average (90.1 vs. 90.5) due to overfitting,but their product achieves higher accuracy than theproduct of the SM6 grammars (92.2 vs. 92.0).
Wewill further investigate the causes for this effect inSection 5.4.2 ST-Reg TrainingGiven the ten SM6 grammars from the previous sub-section, we can investigate the three self-trainingmethods.
In the first regime (ST-Reg), we use thebest single grammar (90.8 F) to parse a single subsetof the BLLIP data.
We then train ten grammars fromdifferent random seeds, using an equally weightedcombination of the WSJ training set with this sin-gle set.
These self-trained grammars are then com-bined into a product model.
As reported in Table 3,4We use Dan Bikel?s randomized parsing evaluation com-parator to determine the significance (p < 0.05) of the differ-ence between two parsers?
outputs.15ST-Prod Best Average ProductSM6 91.7 91.4 92.2SM7 91.9 91.7 92.4Table 4: Performance of the ST-Prod grammars and theirproducts on the WSJ development set.thanks to the use of additional automatically labeledtraining data, the individual SM6 ST-Reg grammarsperform significantly better than the individual SM6grammars (91.2 vs. 90.5 on average), and the indi-vidual SM7 ST-Reg grammars perform even better,achieving a high F score of 91.5 on average.The product of ST-Reg grammars achieves signif-icantly better performance over the individual gram-mars, however, the improvement is much smallerthan that obtained by the product of regular gram-mars.
In fact, the product of ST-Reg grammars per-forms quite similarly to the product of regular gram-mars despite the higher average accuracy of the in-dividual grammars.
This may be caused by the factthat self-training on the same data tends to reducethe variation among the self-trained grammars.
Wewill show in Section 5 that the diversity among theindividual grammars is as important as average ac-curacy for the performance attained by the productmodel.4.3 ST-Prod TrainingSince products of latent variable grammars performsignificantly better than individual latent variablegrammars, it is natural to try using the productmodel for parsing the unlabeled data.
To investi-gate whether the higher accuracy of the automati-cally labeled data translates into a higher accuracyof the self-trained grammars, we used the product of6th round grammars to parse the same subset of theunlabeled data as in the previous experiment.
Wethen trained ten self-trained grammars, which wecall ST-Prod grammars.
As can be seen in Table 4,using the product of the regular grammars for label-ing the self-training data results in improved individ-ual ST-Prod grammars when compared with the ST-Reg grammars, with 0.2 and 0.3 improvements forthe best SM6 and SM7 grammars, respectively.
In-terestingly, the best individual SM7 ST-Prod gram-mar (91.9 F) performs comparably to the product ofST-Prod-Mult Best Average ProductSM6 91.7 91.4 92.5SM7 91.8 91.7 92.8Table 5: Performance of the ST-Prod-Mult grammars andtheir products on the WSJ development set.the regular grammars (92.0 F) that was used to labelthe BLLIP subset used for self-training.
This is veryuseful for practical reasons because a single gram-mar is faster to parse with and requires less memorythan the product model.The product of the SM6 ST-Prod grammars alsoachieves a 0.2 higher F score compared to the prod-uct of the SM6 ST-Reg grammars, but the productof the SM7 ST-Prod grammars has the same perfor-mance as the product of the SM7 ST-Reg grammars.This could be due to the fact that the ST-Prod gram-mars are no more diverse than the ST-Reg grammars,as we will show in Section 5.4.4 ST-Prod-Mult TrainingWhen creating a product model of regular gram-mars, Petrov (2010) used a different random seed foreach model and conjectured that the effectiveness ofthe product grammars stems from the resulting di-versity of the individual grammars.
Two ways tosystematically introduce bias into individual mod-els are to either modify the feature sets (Baldridgeand Osborne, 2008; Smith and Osborne, 2007) orto change the training distributions of the individualmodels (Breiman, 1996).
Petrov (2010) attempted touse the second method to train individual grammarson either disjoint or overlapping subsets of the tree-bank, but observed a performance drop in individ-ual grammars resulting from training on less data,as well as in the performance of the product model.Rather than reducing the amount of gold trainingdata (or having treebank experts annotate more datato support the diversity), we employ the self-trainingparadigm to train models using a combination of thesame gold training data with different sets of theself-labeled training data.
This approach also allowsus to utilize a much larger amount of low-cost self-labeled data than can be used to train one model bypartitioning the data into ten subsets and then train-ing ten models with a different subset.
Hence, in16-3-2-10123Total VP QP NP SBAR PP ADVP_PRT S WHNP ADJPDifferenceinFG0G1G2G3G4G5G6G7G8G9(a) Difference in F score between the product and the individual SM6 regular grammars.-3-2-10123Total VP QP NP SBAR PP ADVP_PRT S WHNP ADJPDifferenceinFG0G1G2G3G4G5G6G7G8G9(b) Difference in F score between the product of SM6 regular grammars and the individual SM7 ST-Prod-Multgrammars.Figure 1: Difference in F scores between various individual grammars and representative product grammars.the third self-training experiment, we use the prod-uct of the regular grammars to parse all ten subsetsof the unlabeled data and train ten grammars, whichwe call ST-Prod-Mult grammars, each using a dif-ferent subset.As shown in Table 5, the individual ST-Prod-Multgrammars perform similarly to the individual ST-Prod grammars.
However, the product of the ST-Prod-Mult grammars achieves significantly higheraccuracies than the product of the ST-Prod gram-mars, with 0.3 and 0.4 improvements for SM6 andSM7 grammars, respectively, suggesting that the useof multiple self-training subsets plays an importantrole in model combination.5 AnalysisWe conducted a series of analyses to develop an un-derstanding of the factors affecting the effectivenessof combining self-training with product models.5.1 What Has Improved?Figure 1(a) depicts the difference between the prod-uct and the individual SM6 regular grammars onoverall F score, as well as individual constituent Fscores.
As can be observed, there are significantvariations among the individual grammars, and theproduct of the regular grammars improves almost allcategories, with a few exceptions (some individualgrammars do better on QP and WHNP constituents).Figure 1(b) shows the difference between theproduct of the SM6 regular grammars and the indi-vidual SM7 ST-Prod-Mult grammars.
Self-trainingdramatically improves the quality of single gram-mars.
In most of the categories, some individ-ual ST-Prod-Mult grammars perform comparably orslightly better than the product of SM6 regular gram-mars used to automatically label the unlabeled train-ing set.5.2 Overfitting vs. SmoothingFigure 2(a) and 2(b) depict the learning curves ofthe regular and the ST-Prod-Mult grammars.
Asmore latent variables are introduced through the iter-ative SM training algorithm, the modeling capacityof the grammars increases, leading to improved per-formance.
However, the performance of the regulargrammars drops after 6 SM rounds, as also previ-ously observed in (Huang and Harper, 2009; Petrov,2009), suggesting that the regular SM7 grammarshave overfit the relatively small-sized gold training17data.
In contrast, the performance of the self-trainedgrammars continues to improve in the 7th SM round.Huang and Harper (2009) argued that the additionalself-labeled training data adds a smoothing effect tothe grammars, supporting an increase in model com-plexity without overfitting.Although the performance of the individual gram-mars, both regular and self-trained, varies signif-icantly and the product model consistently helps,there is a non-negligible difference between the im-provement achieved by the two product models overtheir component grammars.
The regular productmodel improves upon its individual grammars morethan the ST-Prod-Mult product does in the later SMrounds, as illustrated by the relative error reductioncurves in figures 2(a) and (b).
In particular, the prod-uct of the SM7 regular grammars gains a remarkable2.1% absolute improvement over the average perfor-mance of the individual regular SM7 grammars and0.2% absolute over the product of the regular SM6grammars, despite the fact that the individual regularSM7 grammars perform worse than the SM6 gram-mars.
This suggests that the product model is ableto effectively exploit less smooth, overfit grammars.We will examine this issue further in the next sub-section.5.3 DiversityFrom the perspective of Products of Experts (Hin-ton, 1999) or Logarithmic Opinion Pools (Smith etal., 2005), each individual expert learns complemen-tary aspects of the training data and the veto powerof product models enforces that the joint predictionof their product has to be licensed by all individualexperts.
One possible explanation of the observa-tion in the previous subsection is that with the ad-dition of more latent variables, the individual gram-mars become more deeply specialized on certain as-pects of the training data.
This specialization leadsto greater diversity in their prediction preferences,especially in the presence of a small training set.On the other hand, the self-labeled training set sizeis much larger, and so the specialization process istherefore slowed down.Petrov (2010) showed that the individuallylearned grammars are indeed very diverse by look-ing at the distribution of latent annotations across thetreebank categories, as well as the variation in over-all and individual category F scores (see Figure 1).However, these measures do not directly relate to thediversity of the prediction preferences of the gram-mars, as we observed similar patterns in the regularand self-trained models.Given a sentence s and a set of grammars G ={G1, ?
?
?
, Gn}, recall that the decoding algorithm ofthe product model (Petrov, 2010) searches for thebest tree T such that the following objective functionis maximized:?r?T?G?Glog p(r|s,G)where log p(r|s,G) is the log posterior probabilityof rule r given sentence s and grammar G. Thepower of the product model comes directly from thediversity in log p(r|s,G) among individual gram-mars.
If there is little diversity, the individualgrammars would make similar predictions and therewould be little or no benefit from using a productmodel.
We use the average empirical variance ofthe log posterior probabilities of the rules among thelearned grammars over a held-out set S as a proxyof the diversity among the grammars:?s?S?G?G?r?R(G,s)p(r|s,G)VAR(log(p(r|s,G)))?s?S?G?G?r?R(G,s)p(r|s,G)where R(G, s) represents the set of rules extractedfrom the chart when parsing sentence s with gram-mar G, and VAR(log(p(r|s,G))) is the variance oflog(p(r|s,G)) among all grammars G ?
G.Note that the average empirical variance is onlyan approximation of the diversity among grammars.In particular, this measure tends to be biased to pro-duce larger numbers when the posterior probabili-ties of rules tend to be small, because small differ-ences in probability produce large changes in the logscale.
This happens for coarser grammars producedin early SM stages when there is more uncertaintyabout what rules to apply, with the rules remainingin the parsing chart having low probabilities overall.As shown in Figure 2(c), the average variancesall start at a high value and then drop, probably dueto the aforementioned bias.
However, as the SMiteration continues, the average variances increasedespite the bias.
More interestingly, the variance188385878991932 3 4 5 6 75%9%13%17%21%25%Regular GrammarsF(a) SM RoundsRelativeErrorReduction8385878991932 3 4 5 6 75%9%13%17%21%25%ST-Prod-Mult GrammarsF(b) SM RoundsRelativeErrorReductionProduct Mean Error Reduction0.10.20.30.40.52 3 4 5 6 7TestAverageVariance(c) SM RoundsRegularST-Prod-MultST-ProdST-RegFigure 2: Learning curves of the individual regular (a) and ST-Prod-Mult (b) grammars (average performance, withminimum and maximum values indicated by bars) and their products before and after self-training on the WSJ de-velopment set.
The relative error reductions of the products are also reported.
(c) The measured average empiricalvariance among the grammars trained on WSJ.among the regular grammars grows at a much fasterspeed and is consistently greater when compared tothe self-trained grammars.
This suggests that thereis more diversity among the regular grammars thanamong the self-trained grammars, and explains thegreater improvement obtained by the regular productmodel.
It is also important to note that there is morevariance among the ST-Prod-Mult grammars, whichwere trained on disjoint self-labeled training data,and a greater improvement in their product modelrelative to the ST-Reg and ST-Prod grammars, fur-ther supporting the diversity hypothesis.
Last but notthe least, the trend seems to indicate that the vari-ance of the self-trained grammars would continueincreasing if EM training was extended by a fewmore SM rounds, potentially resulting in even bet-ter product models.
It is currently impractical to testthis due to the dramatic increase in computationalrequirements for an SM8 product model, and so weleave it for future work.5.4 Generalization to Broadcast NewsWe conducted the same set of experiments on thebroadcast news data set.
While the development setresults in Table 6 show similar trends to the WSJresults, the benefits from the combination of self-training and product models appear even more pro-nounced here.
The best single ST-Prod-Mult gram-mar (89.2 F) alone is able to outperform the productof SM7 regular grammars (88.9 F), and their prod-uct achieves another 0.7 absolute improvement, re-sulting in a significantly better accuracy at 89.9 F.Model Rounds Best ProductRegularSM6 87.1 88.6SM7 87.1 88.9ST-ProdSM6 88.5 89.0SM7 89.0 89.6ST-Prod-MultSM6 88.8 89.5SM7 89.2 89.9Table 6: F-score for various models on the BN develop-ment set.Figure 3 shows again that the benefits of self-training and product models are complementary andcan be stacked.
As can be observed, the self-trained grammars have increasing F scores as thesplit-merge rounds increase, while the regular gram-mars have a slight decrease in F score after round 6.In contrast to the newswire models, it appears thatthe individual ST-Prod-Mult grammars trained onbroadcast news always perform comparably to theproduct of the regular grammars at all SM rounds,including the product of SM7 regular grammars.This is noteworthy, given that the ST-Prod-Multgrammars are trained on the output of the worse per-forming product of the SM6 regular grammars.
One19798183858789912 3 4 5 6 73%6%9%12%15%Regular GramamrsF(a) SM RoundsRelativeErrorReduction798183858789912 3 4 5 6 73%6%9%12%15%ST-Prod-Mult GramamrsF(b) SM RoundsRelativeErrorReductionProduct Mean Error Reduction0.10.20.30.40.52 3 4 5 6 7TestAverageVariance(c) SM RoundsRegularST-Prod-MultST-ProdFigure 3: Learning curves of the individual regular (a) and ST-Prod-Mult (b) grammars (average performance, withminimum and maximum values indicated by bars) and their products before and after self-training on the BN develop-ment set.
The relative error reductions of the products are also reported.
(c) The measured average empirical varianceamong the grammars trained on BN.possible explanation is that we used more unlabeleddata for self-training the broadcast news grammarsthan for the newswire grammars.
The product of theST-Prod-Mult grammars provides further and signif-icant improvement in F score.6 Final ResultsWe evaluated the best single self-trained gram-mar (SM7 ST-Prod), as well as the product ofthe SM7 ST-Prod-Mult grammars on the WSJ testset.
Table 7 compares these two grammars toa large body of related work grouped into sin-gle parsers (SINGLE), discriminative reranking ap-proaches (RE), self-training (SELF), and systemcombinations (COMBO).Our best single grammar achieves an accuracythat is only slightly worse (91.6 vs. 91.8 in F score)than the product model in Petrov (2010).
This ismade possible by self-training on the output of ahigh quality product model.
The higher quality ofthe automatically parsed data results in a 0.3 pointhigher final F score (91.6 vs. 91.3) over the self-training results in Huang and Harper (2009), whichused a single grammar for parsing the unlabeleddata.
The product of the self-trained ST-Prod-Multgrammars achieves significantly higher accuracieswith an F score of 92.5, a 0.7 improvement over theproduct model in Petrov (2010).8Our ST-Reg grammars are trained in the same way as inType Parser LP LR EXSINGLE Charniak (2000) 89.9 89.5 37.2Petrov and Klein (2007) 90.2 90.1 36.7Carreras et al (2008) 91.4 90.7 -RE Charniak and Johnson (2005) 91.8 91.2 44.8Huang (2008) 92.2 91.2 43.5SELF Huang and Harper (2009)8 91.6 91.1 40.4McClosky et al (2006) 92.5 92.1 45.3COMBO Petrov (2010) 92.0 91.7 41.9Sagae and Lavie (2006) 93.2 91.0 -Fossum and Knight (2009) 93.2 91.7 -Zhang et al (2009) 93.3 92.0 -This PaperBest Single 91.8 91.4 40.3Best Product 92.7 92.2 43.1Table 7: Final test set accuracies on WSJ.Although our models are based on purely gen-erative PCFG grammars, our best product modelperforms competitively to the self-trained two-stepdiscriminative reranking parser of McClosky et al(2006), which makes use of many non-local rerank-ing features.
Our parser also performs comparablyto other system combination approaches (Sagae andLavie, 2006; Fossum and Knight, 2009; Zhang etal., 2009) with higher recall and lower precision,Huang and Harper (2009) except that we keep all unary rules.The reported numbers are from the best single ST-Reg grammarin this work.20but again without using a discriminative rerankingstep.
We expect that replacing the first-step genera-tive parsing model in McClosky et al (2006) with aproduct of latent variable grammars would give evenhigher parsing accuracies.On the Broadcast News test set, our best perform-ing single and product grammars (bolded in Table 6)obtained F scores of 88.7 and 89.6, respectively.While there is no prior work using our setup, we ex-pect these numbers to set a high baseline.7 Conclusions and Future WorkWe evaluated methods for self-training high accu-racy products of latent variable grammars with largeamounts of genre-matched data.
We demonstratedempirically on newswire and broadcast news genresthat very high accuracies can be achieved by traininggrammars on disjoint sets of automatically labeleddata.
Two primary factors appear to be determin-ing the efficacy of our self-training approach.
First,the accuracy of the model used for parsing the unla-beled data is important for the accuracy of the result-ing single self-trained grammars.
Second, the diver-sity of the individual grammars controls the gainsthat can be obtained by combining multiple gram-mars into a product model.
Our most accurate sin-gle grammar achieves an F score of 91.6 on the WSJtest set, rivaling discriminative reranking approaches(Charniak and Johnson, 2005) and products of latentvariable grammars (Petrov, 2010), despite being asingle generative PCFG.
Our most accurate productmodel achieves an F score of 92.5 without the use ofdiscriminative reranking and comes close to the bestknown numbers on this test set (Zhang et al, 2009).In future work, we plan to investigate additionalmethods for increasing the diversity of our self-trained models.
One possibility would be to utilizemore unlabeled data or to identify additional ways tobias the models.
It would also be interesting to deter-mine whether further increasing the accuracy of themodel used for automatically labeling the unlabeleddata can enhance performance even more.
A simplebut computationally expensive way to do this wouldbe to parse the data with an SM7 product model.Finally, for this work, we always used productsof 10 grammars, but we sometimes observed thatsubsets of these grammars produce even better re-sults on the development set.
Finding a way to se-lect grammars from a grammar pool to achieve highperformance products is an interesting area of futurestudy.8 AcknowledgmentsThis research was supported in part by NSF IIS-0703859.
Opinions, findings, and recommendationsexpressed in this paper are those of the authors anddo not necessarily reflect the views of the fundingagency or the institutions where the work was com-pleted.ReferencesJason Baldridge and Miles Osborne.
2008.
Active learn-ing and logarithmic opinion pools for HPSG parse se-lection.
Natural Language Engineering.Leo Breiman.
1996.
Bagging predictors.
MachineLearning.Kenneth P. Burnham and David R. Anderson.
2002.Model Selection and Multimodel Inference: A Prac-tical Information-Theoretic Approach.
New York:Springer-Verlag.Xavier Carreras, Michael Collins, and Terry Koo.
2008.Tag, dynamic programming, and the perceptron for ef-ficient, feature-rich parsing.
In CoNLL, pages 9?16.Eugene Charniak and Mark Johnson.
2001.
Edit detec-tion and parsing for transcribed speech.
In NAACL.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative rerank-ing.
In ACL.Eugene Charniak, Don Blaheta, Niyu Ge, Keith Hall,John Hale, and Mark Johnson, 2000.
BLLIP 1987-89WSJ Corpus Release 1.
Linguistic Data Consortium,Philadelphia.Eugene Charniak.
1997.
Statistical parsing with acontext-free grammar and word statistics.
In ICAI.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In ACL.Denis Filimonov and Mary Harper.
2009.
A jointlanguage model with fine-grain syntactic tags.
InEMNLP, pages 1114?1123, Singapore, August.Victoria Fossum and Kevin Knight.
2009.
Combiningconstituent parsers.
In NAACL, pages 253?256.John Garofolo, Jonathan Fiscus, William Fisher, andDavid Pallett, 1996.
CSR-IV HUB4.
Linguistic DataConsortium, Philadelphia.Geoffrey E. Hinton.
1999.
Products of experts.
InICANN.21Zhongqiang Huang and Mary Harper.
2009.
Self-training PCFG grammars with latent annotationsacross languages.
In EMNLP.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In ACL.Percy Liang, Slav Petrov, Michael I. Jordan, and DanKlein.
2007.
The infinite PCFG using hierarchicalDirichlet processes.
In EMNLP.Mitchell P. Marcus, Beatrice Santorini, Mary AnnMarcinkiewicz, and Ann Taylor, 1999.
Treebank-3.Linguistic Data Consortium, Philadelphia.Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.2005.
Probabilistic CFG with latent annotations.
InACL.David McClosky, Eugene Charniak, and Mark Johnson.2006.
Effective self-training for parsing.
In HLT-NAACL.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In HLT-NAACL.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In ACL.Slav Petrov.
2009.
Coarse-to-Fine Natural LanguageProcessing.
Ph.D. thesis, University of California atBekeley.Slav Petrov.
2010.
Products of random latent variablegrammars.
In HLT-NAACL.Brian Roark, Mary Harper, Yang Liu, Robin Stewart,Matthew Lease, Matthew Snover, Izhak Shafran, Bon-nie J. Dorr, John Hale, Anna Krasnyanskaya, and LisaYung.
2006.
SParseval: Evaluation metrics for pars-ing speech.
In LREC.Kenji Sagae and Alon Lavie.
2006.
Parser combinationby reparsing.
In NAACL, pages 129?132.Andrew Smith and Miles Osborne.
2007.
Diversityin logarithmic opinion pools.
Lingvisticae Investiga-tiones.Andrew Smith, Trevor Cohn, and Miles Osborne.
2005.Logarithmic opinion pools for conditional randomfields.
In ACL.Wen Wang and Mary P. Harper.
2002.
The superarv lan-guage model: Investigating the effectiveness of tightlyintegrating multiple knowledge sources.
In EMNLP,pages 238?247, Philadelphia, July.Ralph Weischedel, Sameer Pradhan, Lance Ramshaw,Martha Palmer, Nianwen Xue, Mitchell Marcus, AnnTaylor, Craig Greenberg, Eduard Hovy, Robert Belvin,and Ann Houston, 2008.
OntoNotes Release 2.0.
Lin-guistic Data Consortium, Philadelphia.Hui Zhang, Min Zhang, Chew Lim Tan, and HaizhouLi.
2009.
K-best combination of syntactic parsers.In EMNLP, pages 1552?1560.22
