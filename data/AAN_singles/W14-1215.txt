Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 131?140,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsAn evaluation of syntactic simplification rules for people with autismRichard Evans, Constantin Or?asan and Iustin DornescuResearch Institute in Information and Language ProcessingUniversity of WolverhamptonUnited Kingdom{R.J.Evans, C.Orasan, I.Dornescu2}@wlv.ac.ukAbstractSyntactically complex sentences consti-tute an obstacle for some people withAutistic Spectrum Disorders.
This pa-per evaluates a set of simplification rulesspecifically designed for tackling complexand compound sentences.
In total, 127 dif-ferent rules were developed for the rewrit-ing of complex sentences and 56 for therewriting of compound sentences.
Theevaluation assessed the accuracy of theserules individually and revealed that fullyautomatic conversion of these sentencesinto a more accessible form is not very re-liable.1 IntroductionPeople with Autistic Spectrum Disorders (ASD)show a diverse range of reading abilities: onthe one hand, 5%-10% of users have the capac-ity to read words from an early age without theneed for formal learning (hyperlexia), on the otherhand many users demonstrate weak comprehen-sion of what has been read (Volkmar and Wiesner,2009).
They may have difficulty inferring contex-tual information or may have trouble understand-ing mental verbs or emotional language, as wellas long sentences with complex syntactic structure(Tager-Flusberg, 1981; Kover et al., 2012).
To ad-dress these difficulties, the FIRST project1is de-veloping a tool which makes texts more accessiblefor people with ASD.
In order to get a better un-derstanding of the needs of these readers, a thor-ough analysis was carried out to derive a list ofhigh priority obstacles to reading comprehension.Some of these obstacles are related to syntacticcomplexity and constitute the focus of this paper.Even though the research in the FIRST project fo-cuses on people with ASD, many of the obstacles1http://first-asd.euidentified in the project can pose difficulties for awide range of readers such as language learnersand people with other language disorders.This paper presents and evaluates a set of rulesused for simplifying English complex and com-pound sentences.
These rules were developed aspart of a syntactic simplification system which wasinitially developed for users with ASD, but whichcan also be used for other tasks that require syn-tactic simplification of sentences.
In our research,we consider that syntactic complexity is usuallyindicated by the occurrence of certain markers orsigns of syntactic complexity, referred to hereafteras signs, such as punctuation ([,] and [;]), con-junctions ([and], [but], and [or]), complementis-ers ([that]) or wh-words ([what], [when], [where],[which], [while], [who]).
These signs may havea range of syntactic linking and bounding func-tions which need to be automatically identified,and which we analysed in more detail in (Evansand Orasan, 2013).Our syntactic simplification process operates intwo steps.
In the first, signs of syntactic complex-ity are automatically classified and in the second,manually crafted rules are applied to simplify therelevant sentences.
Section 3 presents more detailsabout the method.
Evaluation of automatic simpli-fication is a difficult issue.
Given that the purposeof this paper is to gain a better understanding ofthe performance of the rules used for simplifyingcompound sentences and complex sentences, Sec-tion 4 presents the methodology developed for thisevaluation and discusses the results obtained.
Thepaper finishes with conclusions.2 Background informationDespite some findings to the contrary (Arya et al.,2011), automatic syntactic simplification has beenmotivated by numerous neurolinguistic and psy-cholinguistic studies.
Brain imaging studies indi-cate that processing syntactically complex struc-131tures requires more neurological activity than pro-cessing simple structures (Just et al., 1996).
Astudy undertaken by Levy et al.
(2012) showedthat people with aphasia are better able to un-derstand syntactically simple reversible sentencesthan syntactically complex ones.Further motivation is brought by research inNLP, which demonstrates that performance levelsin information extraction (Agarwal and Boggess,1992; Rindflesch et al., 2000; Evans, 2011),syntactic parsing (Tomita, 1985; McDonald andNivre, 2011), and, to some extent, machine trans-lation (Gerber and Hovy, 1998) are somewhat de-termined by the length and syntactic complexity ofthe sentences being processed.Numerous rule-based methods for syntacticsimplification have been developed (Siddharthan,2006) and used to facilitate NLP tasks such asbiomedical information extraction (Agarwal andBoggess, 1992; Rindflesch et al., 2000; Evans,2011).
In these approaches, rules are triggeredby pattern-matching applied to the output of textanalysis tool such as partial parsers and POS tag-gers.
Chandrasekar and Srinivas (1997) presentedan automatic method to learn syntactic simplifi-cation rules for use in such systems.
Unfortu-nately, that approach is only capable of learninga restricted range of rules and requires access toexpensive annotated resources.With regard to applications improving text ac-cessibility for human readers, Max (2000) de-scribed the use of syntactic simplification foraphasic readers.
In work on the PSET project,Canning (2002) implemented a system which ex-ploits a syntactic parser in order to rewrite com-pound sentences as sequences of simple sentencesand to convert passive sentences into active onesfor readers with aphasia.
The success of these sys-tems is tied to the performance levels of the syn-tactic parsers that they employ.More recently, the availability of resources suchas Simple Wikipedia has enabled text simplifi-cation to be included in the paradigm of statis-tical machine translation (Yatskar et al., 2010;Coster and Kauchak, 2011).
In this context,translation models are learned by aligning sen-tences in Wikipedia with their corresponding ver-sions in Simple Wikipedia.
Manifesting Basic En-glish (Ogden, 1932), the extent to which SimpleWikipedia is accessible to people with autism hasnot yet been fully assessed.The field of text summarisation includes numer-ous approaches that can be regarded as examplesof syntactic simplification.
For example, Cohn andLapata (2009) present a tree-to-tree transductionmethod that is used to filter non-essential infor-mation from syntactically parsed sentences.
Thiscompression process often reduces the syntacticcomplexity of those sentences.
An advantage ofthis approach is that it can identify elements fordeletion even when such elements are not indi-cated by explicit signs of syntactic complexity.The difficulty is that they rely on high levels of ac-curacy and granularity of automatic syntactic anal-ysis.
As noted earlier, it has been observed that theaccuracy of parsers is inversely proportional to thelength and complexity of the sentences being anal-ysed (Tomita, 1985; McDonald and Nivre, 2011).The approach to syntactic simplification de-scribed in the current paper is a two step pro-cess involving detection and tagging of the bound-ing and linking functions of various signs of syn-tactic complexity followed by a rule-based sen-tence rewriting step.
Relevant to the first step, VanDelden and Gomez (2002) developed a machinelearning method to determine the syntactic rolesof commas.
Meier et al.
(2012) describe Germanlanguage resources in which the linking functionsof commas and semicolons are annotated.
The an-notated resources exploited by the machine learn-ing method presented in Section 3.2.1 of the cur-rent paper are presented in (Evans and Orasan,2013).
From a linguistic perspective, Nunberg etal.
(2002) provide a grammatical analysis of punc-tuation in English.The work described in this paper was under-taken in a project aiming to improve the accessibil-ity of text for people with autism.
It was motivatedat least in part by the work of O?Connor and Klein(2004), which describes strategies to facilitate thereading comprehension of people with ASD.The proposed method is intended to reducecomplexity caused by both complex and com-pound sentences and differs from those describedearlier in this section.
Sentence compressionmethods are not suitable for the types of rewrit-ing required in simplifying compound sentences.Parsers are more likely to have lower accuracywhen processing these sentences, and therefore theproposed method does not use information aboutthe syntactic structure of sentences in the process.Our method is presented in the next section.1323 The syntactic simplifierIn our research, we regard coordination and sub-ordination as key elements of syntactic complex-ity.
A thorough study of the potential obstacles tothe reading comprehension of people with autismhighlighted particular types of syntactic complex-ity, many of which are linked to coordinationand subordination.
Section 3.1 briefly presentsthe main obstacles linked to syntactic complexityidentified by the study.
It should be mentioned thatmost of the obstacles are problematic not only forautistic people and other types of reader can alsobenefit from their removal.
The obstacles identi-fied constituted the basis for developing the sim-plification approach briefly described in Section3.2.3.1 User requirementsConsultations with 94 subjects meeting the strictDSM-IV criteria for ASD and with IQ > 70 led tothe derivation of user preferences and high priorityuser requirements related to structural processing.A comprehensive explanation of the findings canbe found in (Martos et al., 2013).
This section dis-cusses briefly the two types of information of rel-evance to the processing of sentence complexityobtained in our study.First, in terms of the demand for access to textsof particular genres/domains, it was found thatyoung people (aged 12-16) seek access to doc-uments in informative (arts/leisure) domains andthey have less interest in periodicals and newspa-pers or imaginative texts.
Adults (aged 16+) seekaccess to informative and scientific texts (includ-ing newspapers), imaginative text, and the lan-guage of social networking and communication.In an attempt to accommodate the interests of bothyoung people and adults, we developed a cor-pus which contains newspaper articles, texts abouthealth, and literary texts.Second, the specific morpho-syntactic phenom-ena that pose obstacles to reading comprehensionthat are relevant to this paper are:1.
Compound sentences, which should be splitinto sentences containing a single clause.2.
Complex sentences: in which relative clausesshould either be:(a) converted into adjectival pre-modifiersor(b) deleted from complex sentences andused to generate copular constructionslinking the NP in the matrix clause withthe predication of the relative clauseIn addition, the analysis revealed other typesof obstacles such as explicative clauses, whichshould be deleted, and uncommon conjunctions(including conjuncts) which should be replacedby more common ones.
Conditional clauses thatfollow the main clause and non-initial adverbialclauses should be pre-posed, and passive sen-tences should be converted in the active form.
Var-ious formatting issues such as page breaks that oc-cur within paragraphs and end-of-line hyphenationare also problematic and should be avoided.Section 3.2 describes the method developed toaddress the obstacles caused by compound andcomplex sentences.3.2 The approachProcessing of obstacles to reading comprehensionin this research has focused on detection and re-duction of syntactic complexity caused by the oc-currence in text of compound sentences (1) andcomplex sentences (2).
(1) Elaine Trego never bonded with 16-month-oldJacob [and] he was often seen with bruises, amurder trial was told.
(2) The two other patients, who are far morefragile than me, would have been killed bythe move.In (1), the underlined phrases are the conjoinsof a coordinate constituent.
In (2), the underlinedphrase is a subordinate constituent of the larger,superordinate phrase the two other patients, whoare far more fragile than me.The overall syntactic simplification pipelineconsists of the following steps:Step 1.
Tagging of signs of syntactic complexitywith information about their syntactic linkingor bounding functionsStep 2.
The complexity of sentences tagged instep 1 is assessed and used to trigger the ap-plication of two iterative simplification pro-cesses, which are applied exhaustively andsequentially to each input sentence:133a.
Decomposition of compound sentences(the simplification function converts oneinput string into two output strings)b. Decomposition of complex sentences(the simplification function converts oneinput string into two output strings)Step 3.
Personalised transformation of sentencesaccording to user preference profiles whichlist obstacles to be tackled and the thresholdcomplexity levels that specify whether sim-plification is necessary.Steps 1 and 2 are applied iteratively ensuringthat an input sentence can be exhaustively simpli-fied by decomposition of the input string into pairsof progressively simpler sentences.
No furthersimplification is applied to a sentence when thesystem is unable to detect any signs of syntacticcomplexity within it.
This paper reports on steps 1and 2.
The personalisation step, which takes intoconsideration the needs of individual users, is notdiscussed.3.2.1 Identification of signs of complexitySigns of syntactic complexity typically indicateconstituent boundaries, e.g.
punctuation marks,conjunctions, and complementisers.
To facilitateinformation extraction, a rule-based approach tosimplify coordinated conjoins was proposed byEvans (2011), which relies on classifying signsbased on their linking functions.In more recent work, an extended annotationscheme was proposed in (Evans and Orasan, 2013)which enables the encoding of links and bound-aries between a wider range of syntactic con-stituents and covers more syntactic phenomena.A corpus covering three text categories (news ar-ticles, literature, and patient healthcare informa-tion leaflets), was annotated using this extendedscheme.2Most sign labels contain three types of infor-mation: boundary type, syntactic projection level,and grammatical category of the constituent(s).Some labels cover signs which bound interjec-tions, tag questions, and reported speech and aclass denoting false signs of syntactic complex-ity, such as use of the word that as a specifier oranaphor.
The class labels are a combination of thefollowing acronyms:2http://clg.wlv.ac.uk/resources/SignsOfSyntacticComplexity/1.
{C|SS|ES}, the generic function as a coor-dinator (C), the left boundary of a subordi-nate constituent (SS), or the right boundaryof a subordinate constituent (ES).2.
{P |L|I|M |E}, the syntactic projection levelof the constituent(s): prefix (P), lexical (L),intermediate (I), maximal (M), or extended/-clausal (E).3.
{A|Adv|N |P |Q|V }, the grammatical cate-gory of the constituent(s): adjectival (A), ad-verbial (Adv), nominal (N), prepositional (P),quantificational (Q), and verbal (V).4.
{1|2}, used to further differentiate sub-classes on the basis of some other label-specific criterion.The scheme uses a total of 42 labels to distin-guish between different syntactic functions of thebounded constituents.
Although signs are markedby a small set of tokens (words and punctuation),the high number of labels and their skewed dis-tribution make signs highly ambiguous.
In addi-tion, each sign is only assigned exactly one label,i.e.
that of the dominant constituent in the case ofnesting, further increasing ambiguity.
These char-acteristics make automatic classification of signschallenging.The automatic classification of signs of syntac-tic complexity is achieved using a machine learn-ing approach described in more detail in Dornescuet al.
(2013).
After experimenting with severalmethods of representing the training data and withseveral classifiers, the best results were obtainedby using the BIO model to train a CRF tagger.
Thefeatures used were the signs?
surrounding con-text (a window of 10 tokens and their POS tags)together with information about the distance toother signs signs in the same sentence and theirtypes.
The method achieved an overall accuracyof 82.50% (using 10 fold cross-validation) on themanually annotated corpus.3.2.2 Rule-based approach to simplificationof compound sentences and complexsentencesThe simplification method exploits two iterativeprocesses that are applied in sequence to inputtext that has been tokenised with respect to sen-tences, words, punctuation, and signs of syntac-tic complexity.
The word tokens in the input text134Rule ID CEV-12Sentence type Compound (coordination)Match pattern A that [B] signCEV[C] .Transform pattern A that [B].
A that [C].Ex: input [Investigations showed]Athat [the glass came from a car?s side window]BandCEV[thousands of batches had been tampered with on five separate weekends]C.Ex: output [Investigations showed]Athat [the glass came from a car?s side window]B.
[Investigations showed]Athat [thousands of batches had been tampered with on fiveseparate weekends]C.Rule ID CEV-26Sentence type Compound (coordination)Match pattern A vCCB: ?
[C] signCEV[D]?.Transform pattern A v B: ?[C]?.
A v B: ?
[D]?.Ex: input [He]Aadded[]B: ?
[If I were with Devon and Cornwall police I?d be very interested inthe result of this case]CandCEV[I certainly expect them to renew their interest]D.?Ex: output [He]Aadded[]B: ?
[If I were with Devon and Cornwall police I?d be very interested inthe result of this case]C.?
[He]Aadded[]B: ?
[I certainly expect them to renew their interest]D.?Table 1: Patterns used to identify conjoined clauses.have also been labelled with their parts of speechand the signs have been labelled with their gram-matical linking and bounding functions.
The pat-terns rely mainly on nine sign labels which delimitclauses (*EV)3, noun phrases (*MN) and adjecti-val phrases (*MA).
These sign labels can signaleither coordinated conjoins (C*) or the start (SS*)or end (ES*) of a constituent.The first iterative process exploits patterns in-tended to identify the conjoins of compound sen-tences.
The elements common to these patternsare signs tagged as linking clauses in coordination(label CEV).
The second process exploits patternsintended to identify relative clauses in complexsentences.
The elements common to these patternsare signs tagged as being left boundaries of subor-dinate clauses (label SSEV).The identification of conjoint clauses dependson accurate tagging of words with informationabout their parts of speech and signs with informa-tion about their general roles in indicating the leftor right boundaries of subordinate constituents.The identification of subordinate clauses requiresmore detailed information.
In addition to the in-formation required to identify clause conjoins, in-formation about the specific functions of signs isrequired.
The simplification process is thus highlydependent on the performance of the automaticsign tagger.Table 1 displays two patterns for identifyingconjoined clauses and Table 2 displays two pat-terns for identifying subordinate clauses.
In the3In these example the * character is used to indicate anysequence of characters, representing the bounding or linkingfunction of the sign.tables, upper case letters denote contiguous se-quences of text,4the underbar denotes signs ofclass CEV (in row Compound) and SSEV (in rowComplex).
Verbs with clause complements aredenoted by vCC, while words of part of speechX are denoted by wX.
The symbol s is usedto denote additional signs of syntactic complex-ity while v denotes words with verbal POS tags.Words explicitly appearing in the input text areitalicised.
Elements of the patterns representingclause conjoins and subordinate clauses appear insquare brackets.Each pattern is associated with a sentencerewriting rule.
A rule is applied on each itera-tion of the algorithm.
Sentences containing signswhich correspond to conjoint clauses are con-verted into two strings which are identical to theoriginal save that, in one, the conjoint clause isreplaced by a single conjoin identified in the con-joint while in the other, the identified conjoin isomitted.
Sentences containing signs which indi-cate subordinate clauses are converted into twonew strings.
One is identical to the original savethat the relative clause is deleted.
The second isautomatically generated, and consists of the NP inthe matrix clause modified by the relative clause, aconjugated copula, and the predication of the rela-tive clause.
Tables 1 and 2 give examples of trans-formation rules for the given patterns.
In total,127 different rules were developed for the rewrit-ing of complex sentences and 56 for the rewritingof compound sentences.4Note that these sequences of text may contain additionalsigns tagged CEV or SSEV.135Rule ID SSEV-61Sentence type Complex (subordination)Match pattern A s B [signSSEVC v D].Transform pattern A s B.
That C v D.Ex: input [During the two-week trial, the jury heard how Thomas became a frequent visitor toRoberts?s shop in the summer of 1997]A, [after meeting him through a friend]B[who[lived near the shop,]C[described as a ?child magnet?
by one officer]D.Ex: output [During the two-week trial, the jury heard how Thomas became a frequent visitor toRoberts?s shop in the summer of 1997]A, [after meeting him through a friend]B.That friend [lived near the shop,]C[described as a ?child magnet?
by one officer]D.Rule ID SSEV-72Sentence type Complex (subordination)Match pattern [A wINwDT* n {n|of}* signSSEV] wV BDB {.|?|!
}Transform pattern N/APattern SSEV-72 is used to prevent rewriting of complex sentences when the subordinateclause is the argument of a clause complement verb.
The result of this rule is to strip thetag from the triggering sign of syntactic complexityEx: input [Eamon Reidy, 32,]Afled [across fields in Windsor Great Park after the crash[, the courtheard.
]Table 2: Patterns used to identify subordinate clauses.4 EvaluationThe detection and classification of signs of syntac-tic complexity can be evaluated via standard meth-ods in LT based on comparing classifications madeby the system with classifications made by linguis-tic experts.
This evaluation is reported in (Dor-nescu et al., 2013).
Unfortunately, the evaluationof the actual simplification process is difficult, asthere are no well established methods for measur-ing its accuracy.
Potential methodologies for eval-uation include comparison of system output withhuman simplification of a given text, analysis ofthe post-editing effort required to convert an au-tomatically simplified text into a suitable form forend users, comparisons using experimental meth-ods such as eye tracking and extrinsic evaluationvia NLP applications such as information extrac-tion, all of which have weaknesses in terms of ad-equacy and expense.Due to the challenges posed by these previouslyestablished methods, we decided that before weemploy them and evaluate the output of the sys-tem as a whole, we focus first on the evaluationof the accuracy of the two rule sets employed bythe syntactic processor.
The evaluation method isbased on comparing sets of simplified sentencesderived from an original sentence by linguistic ex-perts with sets derived by the method described inSection 3.4.1 The gold standardTwo gold standards were developed to supportevaluation of the two rule sets.
Texts from the gen-res of health, literature, and news were processedby different versions of the syntactic simplifier.
Inone case, the only rules activated in the syntac-tic simplifier were those concerned with rewritingcompound sentences.
In the second case, the onlyrules activated were those concerned with rewrit-ing complex sentences.
The output of the two ver-sions was corrected by a linguistic expert to ensurethat each generated sentence was grammaticallywell-formed and consistent in meaning with theoriginal sentence.
Sentences for which even man-ual rewriting led to the generation of grammati-cally well-formed sentences that were not consis-tent in meaning with the originals were removedfrom the test data.
After filtering, the test datacontained nearly 1,500 sentences for use in eval-uating rules to simplify of compound sentences,and nearly 1,100 sentences in the set used in eval-uating rules to simplify complex sentences.
Thebreak down per genre/domain is given in Tables3a and 3b.The subset of sentences included in the goldstandard contained manually annotated informa-tion about the signs of syntactic complexity.
Thiswas done to enable reporting of the evaluation re-sults in two modes: one in which the system con-sults an oracle for classification of signs of syntac-tic complexity and one in which the system con-sults the output of the automatic sign tagger.4.2 Evaluation resultsEvaluation results are reported in terms of accu-racy of the simplification process and the changein readability of the generated sentences.
Com-putation of accuracy is based on the mean Leven-136Text categoryNews Health Literature#Compound sentences 698 325 418Accuracy Oracle 0.758 0.612 0.246Classifier 0.314 0.443 0.115?Flesch Oracle 11.1 8.2 15.3Classifier 9.9 10.2 13.6?Avg.
Oracle -12.58 -9.86 -16.69Sent.
Len.
Classifier -13.08 -12.30 -16.79(a) Evaluation of simplification of compound sentencesText categoryNews Health Literature#Complex sentences 369 335 379Accuracy Oracle 0.452 0.292 0.475Classifier 0.433 0.227 0.259?Flesch Oracle 2.5 0.8 2.3Classifier 2.3 0.9 2.3?Avg.
Oracle -2.96 -0.90 -2.80Sent.
Len.
Classifier -2.80 -0.99 -2.11(b) Evaluation of simplification of complex sentencesTable 3: Evaluation results for the two syntactic phenomena on three text genresshtein similarity5between the sentences generatedby the system and the most similar simplified sen-tences verified by the linguistic expert.
Once themost similar sentence in the key has been found,that element is no longer considered for the rest ofthe simplified sentences in the system?s responseto the original.
In this evaluation, sentences areconsidered to be converted correctly if their LS >0.95.
The reason for setting such a high thresholdfor the Levenshtein ratio is because the evaluationmethod should only reward system responses thatmatch the gold standard almost perfectly save for afew characters which could be caused by typos orvariations in the use of punctuation and spaces.
Asentence is considered successfully simplified, andimplicitly all the rules used in the process are con-sidered correctly applied, when all the sentencesproduced by the system are converted correctly ac-cording to the gold standard.
This evaluation ap-proach may be considered too inflexible as it doesnot take into consideration the fact that a sentencecan be simplified in several ways.
However, thepurpose here is to evaluate the way in which sen-tences are simplified using specific rules.In order to calculate the readability of the gen-erated sentences we initially used the Flesch score(Flesch, 1949).
However, our system changes thetext only by rewriting sentences into sequences ofsimpler sentences and does not make any changesat the lexical level.
For this reason, any changesobserved in the Flesch score are due to changesin the average sentence length.
Therefore, for ourexperiments we report both ?Flesch score and?average sentence length.The evaluation results are reported separatelyfor the three domains.
In addition, the results arecalculated when the classes of the signs are de-5Defined as 1 minus the ratio of Levenshtein distance be-tween the two sentences to the length in characters of thelongest of the two sentences being compared.rived from the manually annotated data (Oracle)and from use of the automatic classifier (Classi-fier).Table 3a presents the accuracy of the rules im-plemented to convert compound sentences into amore accessible form.
The row #Compound sen-tences displays the number of sentences in the testdata that contain signs of conjoint clauses (signsof class CEV).
The results obtained are not unex-pected.
In all cases the accuracy of the simplifi-cation rules is higher when the labels of signs areassigned by the oracle.
With the exception of thehealth domain, the same pattern is observed when?Flesch is considered.
The highest accuracy isobtained on the news texts, then the health do-main, and finally the literature domain.
However,despite significantly lower accuracy on the litera-ture domain, the readability of the sentences fromthe literature domain benefits most from the auto-matic simplification.
This can be noticed both inthe improved Flesch scores and reduced sentencelength.Table 3b presents the accuracy of the ruleswhich simplify complex sentences.
In this table,#Complex sentences denotes the number of sen-tences in the test data that contain relative clauses.The rest of the measures are calculated in the sameway as in Table 3a.
Inspection of the table showsthat, for the news and health domains, the accu-racy of these simplification rules is significantlylower than the simplification rules used for com-pound sentences.
Surprisingly, the rules work bet-ter for the literature domain than for the others.The improvement in the readability of texts fromthe health domain is negligible, which can be ex-plained by the poor performance of the simplifica-tion rules on this domain.1374.3 Error analysisIn order to have a better understanding of the per-formance of the system, the performance of theindividual rules was also recorded.
Tables 4 and 5contain the most error prone trigger patterns forconjoined and subordinate clauses respectively.The statistics were derived from rules applied totexts of all three categories of texts and the signsof syntactic complexity were classified using anoracle, in order to isolate the influence of the rulesin the system output.
In this context, the accu-racy with which the syntactic processor convertssentences containing conjoint clauses into a moreaccessible form is 0.577.
The accuracy of this taskwith regard to subordinate clauses is 0.411.The most error-prone trigger patterns for con-joined clauses are listed in Table 4, together withinformation on the conjoin that they are intendedto detect (left or right), their error rate, and thenumber of number of errors made.
The same in-formation is presented for the rules converting sen-tences containing subordinate clauses in Table 5,but in this case the patterns capture the subordina-tion relations.
In the patterns, words with partic-ular parts of speech are denoted by the symbol wwith the relevant Penn Treebank tag appended as asubscript.
Verbs with clause complements are de-noted vCC.
Signs of syntactic complexity are de-noted by the symbol s with the abbreviation of thefunctional class appended as a subscript.
Specificwords are printed in italics.
In the patterns, theclause coordinator is denoted ?
?
and upper caseletters are used to denote stretches of contiguoustext.Rules CEV-25a and SSEV-78a are applied whenthe input sentence triggers none of the other imple-mented patterns.
Errors of this type quantify thenumber of sentences containing conjoint or subor-dinate clauses that cannot be converted into a moreaccessible form by rules included in the structuralcomplexity processor.
Both rules have quite higherror rates, but these errors can only be addressedvia the addition of new rules or the adjustment ofalready implemented rules.SSEV-36a is a pattern used to prevent process-ing of sentences that contain verbs with clausecomplements.
This pattern was introduced be-cause using the sentence rewriting algorithm pro-posed here to process sentences containing thesesubordinate clauses would generate ungrammati-cal output.Table 5 contains only 4 items because for therest of the patterns the number of errors was lessthan 3.
A large number of these rules had an errorrate of 1 which motivated their deactivation.
Un-fortunately this did not lead to improved accuracyof the overall conversion process.5 Conclusions and future workError analysis revealed that fully automatic con-version compound and complex sentences into amore accessible form is quite unreliable, partic-ularly for texts of the literature category.
It wasnoted that conversion of complex sentences into amore accessible form is more difficult than con-version of compound sentences.
However, sub-ordinate clauses are significantly more prevalentthan conjoint clauses in the training and testingdata collected so far.The evaluation of the rule sets used in the con-version of compound and complex sentences intoa more accessible form motivates further specificdevelopment of the rule sets.
This process in-cludes deletion of rules that do not meet particu-lar thresholds for accuracy and the development ofnew rules to address cases where input sentencesfail to trigger any conversion rules (signalled byactivation of redundant rules CEV-25a and SSEV-78a).The results are disappointing given that thesyntactic simplification module presented in thispaper is expected to be integrated in a systemthat makes texts more accessible for people withautism.
However, this simplification module willbe included in a post-editing environment for peo-ple with ASD.
In this setting, it may still proveuseful, despite its low accuracy.AcknowledgmentsThe research described in this paper was par-tially funded by the European Commission un-der the Seventh (FP7-2007-2013) Framework Pro-gramme for Research and Technological Devel-opment (FP7- ICT-2011.5.5 FIRST 287607).
Wegratefully acknowledge the contributions of all themembers of the FIRST consortium for their feed-back and comments during the development of themethods, and to Laura Hasler for her help with theevaluation.138ID Conjoin Trigger pattern Error rate #ErrorsCEV-24b B A B 0.131 59CEV-24a A A B 0.119 54CEV-12b A that C A that B C 0.595 25CEV-25a NA NA 0.956 22CEV-26a A vCCVB : ?C?
A vCCB : ?C D?
0.213 16CEV-26b A vCCVB : ?D?
A vCCB : ?C D?
0.203 14Table 4: Error rates for rules converting sentences with conjoint clausesID Matrix clause / subordinate clause Trigger pattern Error rate #ErrorsSSEV-78a NA NA 0.517 45SSEV-72a A , C w{verb}D A s B C w{verb}D 0.333 4SSEV-36a NA A told w{noun|PRP|DT|IN}* B 0.117 4SSEV-13b wVBNwIN(w{DT|PRP$|noun|CD}|-|,)* w{noun}BA wVBNwIN{w{DT|PRP$|noun|CD}|-|,}* w{noun}B1 3Table 5: Error rates for rules converting sentences with subordinate clausesReferencesRajeev Agarwal and Lois Boggess.
1992.
A simple butuseful approach to conjunct identification.
In Pro-ceedings of the 30th annual meeting for Computa-tional Linguistics, pages 15?21, Newark, Delaware.Association for Computational Linguistics.D.
J. Arya, Elfrieda H. Hiebert, and P. D. Pearson.2011.
The effects of syntactic and lexical com-plexity on the comprehension of elementary sciencetexts.
International Electronic Journal of Elemen-tary Education, 4 (1):107?125.Y.
Canning.
2002.
Syntactic Simplification of Text.Ph.d.
thesis, University of Sunderland.R Chandrasekar and B Srinivas.
1997.
Automatic in-duction of rules for text simplification.
Knowledge-Based Systems, 10:183?190.T.
Cohn and M. Lapata.
2009.
Sentence Compressionas Tree Transduction.
Journal of Artificial Intelli-gence Research, 20(34):637?74.W.
Coster and D. Kauchak.
2011.
Simple englishwikipedia: A new text simplification task.
In Pro-ceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL-2011),pages 665?669, Portland, Oregon, June.
Associationof Computational Linguistics.Iustin Dornescu, Richard Evans, and ConstantinOr?asan.
2013.
A Tagging Approach to IdentifyComplex Constituents for Text Simplification.
InProceedings of Recent Advances in Natural Lan-guage Processing, pages 221 ?
229, Hissar, Bul-garia.Richard Evans and Constantin Orasan.
2013.
Annotat-ing signs of syntactic complexity to support sentencesimplification.
In I. Habernal and V. Matousek, edi-tors, Text, Speech and Dialogue.
Proceedings of the16th International Conference TSD 2013, pages 92?104.
Springer, Plzen, Czech Republic.R.
Evans.
2011.
Comparing methods for the syn-tactic simplification of sentences in information ex-traction.
Literary and Linguistic Computing, 26(4):371?388.R.
Flesch.
1949.
The art of readable writing.
Harper,New York.Laurie Gerber and Eduard H. Hovy.
1998.
Improvingtranslation quality by manipulating sentence length.In David Farwell, Laurie Gerber, and Eduard H.Hovy, editors, AMTA, volume 1529 of Lecture Notesin Computer Science, pages 448?460.
Springer.M.
A.
Just, P. A. Carpenter, and K. R. Thulborn.
1996.Brain activation modulated by sentence comprehen-sion.
Science, 274:114?116.S.
T. Kover, E. Haebig, A. Oakes, A. McDuffie, R. J.Hagerman, and L. Abbeduto.
2012.
Syntactic com-prehension in boys with autism spectrum disorders:Evidence from specific constructions.
In Proceed-ings of the 2012 International Meeting for AutismResearch, Athens, Greece.
International Society forAutism Research.J.
Levy, E. Hoover, G. Waters, S. Kiran, D. Caplan,A.
Berardino, and C. Sandberg.
2012.
Effects ofsyntactic complexity, semantic reversibility, and ex-plicitness on discourse comprehension in personswith aphasia and in healthy controls.
AmericanJournal of Speech?Language Pathology, 21(2):154?
165.Wolfgang Maier, Sandra K?ubler, Erhard Hinrichs, andJulia Kriwanek.
2012.
Annotating coordination inthe penn treebank.
In Proceedings of the Sixth Lin-guistic Annotation Workshop, pages 166?174, Jeju,Republic of Korea, July.
Association for Computa-tional Linguistics.Juan Martos, Sandra Freire, Ana Gonzlez, David Gil,Richard Evans, Vesna Jordanova, Arlinda Cerga,Antoneta Shishkova, and Constantin Orasan.
2013.User preferences: Updated report.
Technical report,139The FIRST Consortium, Available at http://first-asd.eu/D2.2.A.
Max.
2000.
Syntactic simplification - an applica-tion to text for aphasic readers.
Mphil in computerspeech and language processing, University of Cam-bridge, Wolfson College.Ryan T. McDonald and Joakim Nivre.
2011.
Analyz-ing and integrating dependency parsers.
Computa-tional Linguistics, 37(1):197?230.Geoffrey Nunberg, Ted Briscoe, and Rodney Huddle-ston.
2002.
Punctuation.
chapter 20 In Huddleston,Rodney and Geoffrey K. Pullum (eds) The Cam-bridge Grammar of the English Language, pages1724?1764.
Cambridge University Press.I.
M. O?Connor and P. D. Klein.
2004.
Explorationof strategies for facilitating the reading comprehen-sion of high-functioning students with autism spec-trum disorders.
Journal of Autism and Developmen-tal Disorders, 34:2:115?127.C.
K. Ogden.
1932.
Basic English: a general intro-duction with rules and grammar.
K. Paul, Trench,Trubner & Co., Ltd., London.Thomas C. Rindflesch, Jayant V. Rajan, and LawrenceHunter.
2000.
Extracting molecular binding rela-tionships from biomedical text.
In Proceedings ofthe sixth conference on Applied natural languageprocessing, pages 188?195, Seattle, Washington.Association of Computational Linguistics.A.
Siddharthan.
2006.
Syntactic simplification andtext cohesion.
Research on Language and Compu-tation, 4:1:77?109.Helen Tager-Flusberg.
1981.
Sentence comprehen-sion in autistic children.
Applied Psycholinguistics,2:1:5?24.Masaru Tomita.
1985.
Efficient Parsing for NaturalLanguage: A Fast Algorithm for Practical Systems.Kluwer Academic Publishers, Norwell, MA, USA.Sebastian van Delden and Fernando Gomez.
2002.Combining finite state automata and a greedy learn-ing algorithm to determine the syntactic roles ofcommas.
In Proceedings of the 14th IEEE Inter-national Conference on Tools with Artificial Intel-ligence, ICTAI ?02, pages 293?, Washington, DC,USA.
IEEE Computer Society.F.R.
Volkmar and L. Wiesner.
2009.
A Practical Guideto Autism.
Wiley, Hoboken, NJ.M.
Yatskar, B. Pang, C. Danescu-Niculescu-Mizil, andL.
Lee.
2010.
For the sake of simplicity: Unsu-pervised extraction of lexical simplifications fromwikipedia.
In Proceedings of Human LanguageTechnologies: The 2010 Annual Conference of theNorth American Chapter of the ACL, pages 365?368, Los Angeles, California, June.
Association ofComputational Linguistics.140
