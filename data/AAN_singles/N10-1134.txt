Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 912?920,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsMulti-document Summarization viaBudgeted Maximization of Submodular FunctionsHui LinDept.
of Electrical EngineeringUniversity of WashingtonSeattle, WA 98195, USAhlin@ee.washington.eduJeff BilmesDept.
of Electrical EngineeringUniversity of WashingtonSeattle, WA 98195, USAbilmes@ee.washington.eduAbstractWe treat the text summarization problem asmaximizing a submodular function under abudget constraint.
We show, both theoreticallyand empirically, a modified greedy algorithmcan efficiently solve the budgeted submodu-lar maximization problem near-optimally, andwe derive new approximation bounds in do-ing so.
Experiments on DUC?04 task showthat our approach is superior to the best-performing method from the DUC?04 evalu-ation on ROUGE-1 scores.1 IntroductionAutomatically generating summaries from large textcorpora has long been studied in both informationretrieval and natural language processing.
Thereare several types of text summarization tasks.
Forexample, if an input query is given, the generatedsummary can be query-specific, and otherwise it isgeneric.
Also, the number of documents to be sum-marized can vary from one to many.
The constituentsentences of a summary, moreover, might be formedin a variety of different ways ?
summarization canbe conducted using either extraction or abstraction,the former selects only sentences from the origi-nal document set, whereas the latter involves natu-ral language generation.
In this paper, we addressthe problem of generic extractive summaries fromclusters of related documents, commonly known asmulti-document summarization.In extractive text summarization, textual units(e.g., sentences) from a document set are extractedto form a summary, where grammaticality is as-sured at the local level.
Finding the optimal sum-mary can be viewed as a combinatorial optimiza-tion problem which is NP-hard to solve (McDon-ald, 2007).
One of the standard methods forthis problem is called Maximum Marginal Rele-vance (MMR) (Dang, 2005)(Carbonell and Gold-stein, 1998), where a greedy algorithm selects themost relevant sentences, and at the same time avoidsredundancy by removing sentences that are too sim-ilar to already selected ones.
One major problemof MMR is that it is non-optimal because the deci-sion is made based on the scores at the current it-eration.
McDonald (2007) proposed to replace thegreedy search of MMR with a globally optimal for-mulation, where the basic MMR framework can beexpressed as a knapsack packing problem, and aninteger linear program (ILP) solver can be used tomaximize the resulting objective function.
ILP Al-gorithms, however, can sometimes either be expen-sive for large scale problems or themselves mightonly be heuristic without associated theoretical ap-proximation guarantees.In this paper, we study graph-based approachesfor multi-document summarization.
Indeed, severalgraph-based methods have been proposed for extrac-tive summarization in the past.
Erkan and Radev(2004) introduced a stochastic graph-based method,LexRank, for computing the relative importance oftextual units for multi-document summarization.
InLexRank the importance of sentences is computedbased on the concept of eigenvector centrality inthe graph representation of sentences.
Mihalcea andTarau also proposed an eigenvector centrality algo-rithm on weighted graphs for document summariza-tion (Mihalcea and Tarau, 2004).
Mihalcea et allater applied Google?s PageRank (Brin and Page,1998) to natural language processing tasks ranging912from automatic keyphrase extraction and word sensedisambiguation, to extractive summarization (Mi-halcea et al, 2004; Mihalcea, 2004).
Recent workin (Lin et al, 2009) presents a graph-based approachwhere an undirected weighted graph is built for thedocument to be summarized, and vertices representthe candidate sentences and edge weights representthe similarity between sentences.
The summary ex-traction procedure is done by maximizing a submod-ular set function under a cardinality constraint.Inspired by (Lin et al, 2009), we perform summa-rization by maximizing submodular functions undera budget constraint.
A budget constraint is naturalin summarization task as the length of the summaryis often restricted.
The length (byte budget) limita-tion represents the real world scenario where sum-maries are displayed using only limited computerscreen real estate.
In practice, the candidate tex-tual/linguistic units might not have identical costs(e.g., sentence lengths vary).
Since a cardinalityconstraint is a special case (a budget constraint withunity costs), our approach is more general than (Linet al, 2009).
Moreover, we propose a modifiedgreedy algorithm (Section 4) and both theoretically(Section 4.1) and empirically (Section 5.1) show thatthe algorithm solves the problem near-optimally,thanks to submodularity.
Regarding summarizationperformance, experiments on DUC?04 task showthat our approach is superior to the best-performingmethod in DUC?04 evaluation on ROUGE-1 scores(Section 5).2 Background on SubmodularityConsider a set function f : 2V ?
R, which mapssubsets S ?
V of a finite ground set V to real num-bers.
f(?)
is called normalized if f(?)
= 0, andis monotone if f(S) ?
f(T ) whenever S ?
T .f(?)
is called submodular (Lovasz, 1983) if for anyS, T ?
V , we havef(S ?
T ) + f(S ?
T ) ?
f(S) + f(T ).
(1)An equivalent definition of submodularity is theproperty of diminishing returns, well-known in thefield of economics.
That is, f(?)
is submodular if forany R ?
S ?
V and s ?
V \ S,f(S ?
{s})?
f(S) ?
f(R ?
{s})?
f(R).
(2)Eqn.
2 states that the ?value?
of s never increasesin the contexts of ever larger sets, exactly the prop-erty of diminishing returns.
This phenomenon arisesnaturally in many other contexts as well.
For ex-ample, the Shannon entropy function is submodu-lar in the set of random variables.
Submodular-ity, moreover, is a discrete analog of convexity (Lo-vasz, 1983).
As convexity makes continuous func-tions more amenable to optimization, submodular-ity plays an essential role in combinatorial optimiza-tion.Many combinatorial optimization problems canbe solved optimally or near-optimally in polynomialtime only when the underlying function is submod-ular.
It has been shown that any submodular func-tion can be minimized in polynomial time (Schri-jver, 2000)(Iwata et al, 2001).
Maximization of sub-modular functions, however, is an NP-complete op-timization problem but fortunately, some submod-ular maximization problems can be solved near-optimally.
A famous result is that the maximizationof a monotone submodular function under a cardi-nality constraint can be solved using a greedy al-gorithm (Nemhauser et al, 1978) within a constantfactor (0.63) of being optimal.
A constant-factor ap-proximation algorithm has also been obtained formaximizing monotone submodular function with aknapsack constraint (see Section 4.2).
Feige et.al.
(2007) studied unconstrained maximization of a ar-bitrary submodular functions (not necessarily mono-tone).
Kawahara et.al.
(2009) proposed a cutting-plane method for optimally maximizing a submod-ular set function under a cardinality constraint, andLee et.al.
(2009) studied non-monotone submodu-lar maximization under matroid and knapsack con-straints.3 Problem SetupIn this paper, we study the problem of maximizing asubmodular function under budget constraint, statedformally below:maxS?V{f(S) :?i?Sci ?
B}(3)where V is the ground set of all linguistic units (e.g.,sentences) in the document, S is the extracted sum-mary (a subset of V ), ci is the non-negative cost of913selecting unit i and B is our budget, and submodularfunction f(?)
scores the summary quality.The budgeted constraint arises naturally since of-ten the summary must be length limited as men-tioned above.
In particular, the budget B could bethe maximum number of words allowed in any sum-mary, or alternatively the maximum number of bytesof any summary, where ci would then be either num-ber of words or the number of bytes in sentence i.To benefit from submodular optimization, theobjective function measuring the summary qualitymust be submodular.
In general, there are two waysto apply submodular optimization to any applicationdomain.
One way is to force submodularity on anapplication, leading to an artificial and poorly per-forming objective function even if it can be opti-mized well.
The alternative is to address applica-tions where submodularity naturally applies.
We arefortunate in that, like convexity in the continuous do-main, submodularity seems to arise naturally in a va-riety of discrete domains, and as we will see below,extractive summarization is one of them.
As men-tioned in Section 1, our approach is graph-based,not only because a graph is a natural representationof the relationships and interactions between textualunits, but also because many submodular functionsare well defined on a graph and can naturally be usedin measuring the summary quality.Suppose certain pairs (i, j) with i, j ?
V are sim-ilar and the similarity of i and j is measured by anon-negative value wi,j .
We can represent the en-tire document with a weighted graph (V,E), withnon-negative weights wi,j associated with each edgeei,j , e ?
E. One well-known graph-based submod-ular function that measures the similarity of S to theremainder V \ S is the graph-cut function:fcut(S) =?i?V \S?j?Swi,j .
(4)In multi-document summarization, redundancy is aparticularly important issue since textual units fromdifferent documents might convey the same infor-mation.
A high quality (small and meaningful) sum-mary should not only be informative about the re-mainder but also be compact (non-redundant).
Typ-ically, this goal is expressed as a combination ofmaximizing the information coverage and minimiz-ing the redundancy (as used in MMR (Carbonell andGoldstein, 1998)).
Inspired by this, we use the fol-lowing objective by combining a ?-weighted penaltyterm with the graph cut function:fMMR(S) =?i?V \S?j?Swi,j??
?i,j?S:i?=jwi,j , ?
?
0.Luckily, this function is still submodular as both thegraph cut function and the redundancy term are sub-modular.
Neither objective, however, is monotone,something we address in Theorem 3.
Although sim-ilar to the MMR objective function, our approach isdifferent since 1) ours is graph-based and 2) we for-malize the problem as submodular function maxi-mization under the budget constraint where a simplegreedy algorithm can solve the problem guaranteednear-optimally.4 AlgorithmsAlgorithm 1 Modified greedy algorithm1: G?
?2: U ?
V3: while U ?= ?
do4: k ?
argmax?
?U f(G?{?})?f(G)(c?
)r5: G ?
G ?
{k} if?i?G ci + ck ?
B andf(G ?
{k})?
f(G) ?
06: U ?
U \ {k}7: end while8: v?
?
argmaxv?V,cv?B f({v})9: return Gf = argmaxS?{{v?
},G} f(S)Inspired by (Khuller et al, 1999), we proposeAlgorithm 1 to solve Eqn.
(3).
The algorithm se-quentially finds unit k with the largest ratio of ob-jective function gain to scaled cost, i.e., (f(G ?{?})?
f(G))/cr?
, where r > 0 is the scaling factor.If adding k increases the objective function valuewhile not violating the budget constraint, it is thenselected and otherwise bypassed.
After the sequen-tial selection, setG is compared to the within-budgetsingleton with the largest objective value, and thelarger of the two becomes the final output.The essential aspect of a greedy algorithm isthe design of the greedy heuristic.
As discussedin (Khuller et al, 1999), a heuristic that greedily se-lects the k that maximizes (f(G?{k})?
f(G))/ckhas an unbounded approximation factor.
For ex-ample, let V = {a, b}, f({a}) = 1, f({b}) = p,914ca = 1, cb = p + 1, and B = p + 1.
The solutionobtained by the greedy heuristic is {a} with objec-tive function value 1, while the true optimal objec-tive function value is p. The approximation factorfor this example is then p and therefore unbounded.We address this issue by the following two mod-ifications to the naive greedy algorithms.
The firstone is the final step (line 8 and 9) in Algorithm 1where set G and singletons are compared.
This stepensures that we could obtain a constant approxima-tion factor for r = 1 (see the proof in the Appendix).The second modification is that we introduce ascaling factor r to adjust the scale of the cost.
Sup-pose, in the above example, we scale the cost asca = 1r, cb = (p+1)r, then selecting a or b dependsalso on the scale r, and we might get the optimal so-lution using a appropriate r. Indeed, the objectivefunction values and the costs might be uncalibratedsince they might measure different units.
E.g., it ishard to say if selecting a sentence of 15 words withan objective function gain of 2 is better than select-ing sentence of 10 words with gain of 1.
Scalingcan potentially alleviate this mismatch (i.e., we canadjust r on development set).
Interestingly, our the-oretical analysis of the performance guarantee of thealgorithm also gives us guidance about how to scalethe cost for a particular problem (see Section 4.1).4.1 Analysis of performance guaranteeAlthough Algorithm 1 is essentially a simple greedystrategy, we show that it solves Eqn.
(3) globally andnear-optimally, by exploiting the structure of sub-modularity.
As far as we know, this is a new resultfor submodular optimization, not previously statedor published before.Theorem 1.
For normalized monotone submodularfunction f(?
), Algorithm 1 with r = 1 has a constantapproximation factor as follows:f(Gf ) ?(1?
e?12)f(S?
), (5)where S?
is an optimal solution.Proof.
See Appendix.Note that an ?-approximation algorithm for anoptimization problem is a polynomial-time algo-rithm that for all instances of the problem producesa solution whose value is within a factor of ?
of thevalue of the an optimal solution.
So Theorem 1 ba-sically states that the solution found by Algorithm 1can be at least as good as (1 ?
1/?e)f(S?)
?0.39f(S?)
even in the worst case.
A constant ap-proximation bound is good since it is true for all in-stances of the problem, and we always know howgood the algorithm is guaranteed to be without anyextra computation.
For r ?= 1, we resort to instance-dependent bound where the approximation can beeasily computed per problem instance.Theorem 2.
With normalized monotone submodu-lar f(?
), for i = 1, .
.
.
, |G|, let vi be the ith unitadded intoG andGi is the set after adding vi.
When0 ?
r ?
1,f(Gi) ?(1?i?k=1(1?crvkBr|S?|1?r))f(S?)(6)?
(1?i?k=1(1?crvkBr|V |1?r))f(S?)
(7)and when r ?
1,f(Gi) ?(1?i?k=1(1?(cvkB)r))f(S?).
(8)Proof.
See Appendix.Theorem 2 gives bounds for a specific instance ofthe problem.
Eqn.
(6) requires the size |S?|, whichis unknown, requiring us to estimate an upper boundof the cardinality of the optimal set S?.
Obviously,|S?| ?
|V |, giving us Eqn.
(7).
A tighter upperbound is obtained, however, by sorting the costs.That is, let c[1], c[2], .
.
.
, c[|V |] be the sorted sequenceof costs in nondecreasing order, giving |S?| < mwhere?m?1k=1 c[i] ?
B and?mk=1 c[i] > B.
In thiscase, the computation cost for the bound estimationis O(|V | log |V |), which is quite feasible.Note that both Theorem 1 and 2 are for mono-tone submodular functions while our practical ob-jective function, i.e.
fMMR, is not guaranteed every-where monotone.
However, our theoretical resultsstill holds for fMMR with high probability in prac-tice.
Intuitively, in summarization tasks, the sum-mary is usually small compared to the ground setsize (|S| ?
|V |).
When |S| is small, fMMR is915monotone and our theoretical results still hold.
Pre-cisely, assume that all edge weights are bounded:wi,j ?
[0, 1] (which is the case for cosine simi-larity between non-negative vectors).
Also assumethat edges weights are independently identically dis-tributed with mean ?, i.e.
E(wi,j) = ?.
Given abudget B, assume the maximum possible size of asolution is K. Let ?
= 2?
+ 1, and ?
= 2K ?
1.Notice that ?
?
|V | for our summarization task.
Wehave the following theorem:Theorem 3.
Algorithm 1 solves the summarizationproblem near-optimally (i.e.
Theorem 1 and Theo-rem 2 hold) with high probability of at least1?
exp{?2(|V | ?
(?
+ 1)?
)2?2|V |+ (?2 ?
1)?+ lnK}Proof.
Omitted due to space limitation.4.2 Related workAlgorithms for maximizing submodular functionunder budget constraint (Eqn.
(3)) have been stud-ied before.
Krause (2005) generalized the work byKhuller et al(1999) on budgeted maximum coverproblem to the submodular framework, and showeda 12(1 ?
1/e)-approximation algorithm.
The algo-rithm in (Krause and Guestrin, 2005) and (Khulleret al, 1999) is actually a special case of Algorithm 1when r = 1, and Theorem 1 gives a better bound(i.e., (1?
1/?e) > 12(1?
1/e)) in this case.
Thereis also a greedy algorithm with partial enumerations(Sviridenko, 2004; Krause and Guestrin, 2005) fac-tor (1?
1/e).
This algorithm, however, is too com-putationally expensive and thus not practical for realworld applications (the computation cost is O(|V |5)in general).
When each unit has identical cost, thebudget constraint reduces to cardinality constraintwhere a greedy algorithm is known to be a (1?1/e)-approximation algorithm (Nemhauser et al, 1978)which is the best that can be achieved in polyno-mial time (Feige, 1998) if P ?= NP.
Recent work(Takamura and Okumura, 2009) applied the maxi-mum coverage problem to text summarization (with-out apparently being aware that their objective issubmodular) and studied a similar algorithm to ourswhen r = 1 and for the non-penalized graph-cutfunction.
This problem, however, is a special caseof constrained submodular function maximization.5 ExperimentsWe evaluated our approach on the data set ofDUC?04 (2004) with the setting of task 2, whichis a multi-document summarization task on Englishnews articles.
In this task, 50 document clustersare given, each of which consists of 10 documents.For each document cluster, a short multi-documentsummary is to be generated.
The summary shouldnot be longer than 665 bytes including spaces andpunctuation, as required in the DUC?04 evaluation.We used DUC?03 as our development set.
All docu-ments were segmented into sentences using a scriptdistributed by DUC.
ROUGE version 1.5.5 (Lin,2004), which is widely used in the study of summa-rization, was used to evaluate summarization perfor-mance in our experiments 1.
We focus on ROUGE-1 (unigram) F-measure scores since it has demon-strated strong correlation with human annotation(Lin, 2004).The basic textual/linguistic units we consider inour experiments are sentences.
For each documentcluster, sentences in all the documents of this clusterforms the ground set V .
We built semantic graphsfor each document cluster based on cosine similar-ity, where cosine similarity is computed based onthe TF-IDF (term frequency, inverse document fre-quency) vectors for the words in the sentences.
Thecosine similarity measures the similarity betweensentences, i.e., wi,j .Here the IDF values were calculated using all thedocument clusters.
The weighted graph was builtby connecting vertices (corresponding to sentences)with weight wi,j > 0.
Any unconnected vertex wasremoved from the graph, which is equivalent to pre-excluding certain sentences from the summary.5.1 Comparison with exact solutionIn this section, we empirically show that Algo-rithm 1 works near-optimally in practice.
To deter-mine how much accuracy is lost due to approxima-tions, we compared our approximation algorithmswith an exact solution.
The exact solutions were ob-tained by Integer Linear Programming (ILP).
Solv-ing arbitrary ILP is an NP-hard problem.
If the sizeof the problem is not too large, we can sometimesfind the exact solution within a manageable time1Options used: -a -c 95 -b 665 -m -n 4 -w 1.2916using a branch-and-bound method.
In our experi-ments, MOSEK was used as our ILP solver.We formalize Eqn.
(3) as an ILP by introducingindicator (binary) variables xi,j , yi,j , i ?= j and zifor i, j ?
V .
In particular, zi = 1 indicates thatunit i is selected, i.e., i ?
S, xi,j = 1 indicates thati ?
S but j /?
S, and yi,j = 1 indicates both i andj are selected.
Adding constraints to ensure a validsolution, we have the following ILP formulation forEqn.
(3) with objective function fMMR(S):max?i?=j,i,j?Vwi,jxi,j ?
?
?i?=j,i,j?Vwi,jyi,jsubject to:?i?Vcizi ?
B,xi,j ?
zi ?
0, xi,j + zj ?
1, zi ?
zj ?
xi,j ?
0,yi,j ?
zi ?
0, yi,j ?
zj ?
0, zi + zj ?
yi,j ?
1,xi,j , yi,j , zi ?
{0, 1},?i ?= j, i, j ?
VNote that the number of variables in the ILP for-mulation is O(|V |2).
For a document cluster withhundreds of candidate textual units, the scale of theproblem easily grows involving tens of thousandsof variables, making the problem very expensive tosolve.
For instance, solving the ILP exactly on adocument cluster with 182 sentences (as used in Fig-ure 1) took about 17 hours while our Algorithm 1finished in less than 0.01 seconds.We tested both approximate and exact algorithmson DUC?03 data where 60 document clusters wereused (30 TDT document clusters and 30 TREC doc-ument clusters), each of which contains 10 docu-ments on average.
The true approximation factorwas computed by dividing the objective functionvalue found by Algorithm 1 over the optimal ob-jective function value (found by ILP).
The averageapproximation factors over the 58 document clus-ters (ILP on 2 of the 60 document clusters failed tofinish) are shown in Table 1, along with other statis-tics.
On average Algorithm 1 finds a solution that isover 90% as good as the optimal solution for manydifferent r values, which backs up our claim thatthe modified greedy algorithm solves the problemnear-optimally, even occasionally optimally (Figure1 shows one such example).The higher objective function value does not al-ways indicate higher ROUGE-1 score.
Indeed,0204060801001201400 2 4 6 8 10 12op malr=0r=0.5r=1r=1.5number of sentences in the summaryObjective functionvalueexact solutionFigure 1: Application of Algorithm 1 when summariz-ing document cluster d30001t in the DUC?04 dataset withsummary size limited to 665 bytes.
The objective func-tion was fMMR with ?
= 2.
The plots show the achievedobjective function as the number of selected sentencesgrows.
The plots stop when in each case adding moresentences violates the budget.
Algorithm 1 with r = 1found the optimal solution exactly.rather than directly optimizing ROUGE, we opti-mize a surrogate submodular function that indicatesthe quality of a summary.
Optimality in the submod-ular function does not necessary indicate optimalityin ROUGE score.
Nevertheless, we will show thatour approach outperforms several other approachesin terms of ROUGE.
We note that ROUGE is itselfa surrogate for true human-judged summary quality,it might possibly be that fMMR is a still better surro-gate ?
we do not consider this possibility further inthis work, however.5.2 Summarization ResultsWe used DUC?03 (as above) for our developmentset to investigate how r and ?
relate to the ROUGE-1 score.
From Figure 2, the best performance isachieved with r = 0.3, ?
= 4.
Using these settings,we applied our approach to the DUC?04 task.
Theresults, along with the results of other approaches,are shown in Table 2.
All the results in Table 2 arepresented as ROUGE-1 F-measure scores.
2We compared our approach to two other well-2When the evaluation was done in 2004, ROUGEwas still inrevision 1.2.1, so we re-evaluated the DUC?04 submissions us-ing ROUGE v1.5.5 and the numbers are slightly different fromthe those reported officially.917Table 1: Comparison of Algorithm 1 to exact algorithmson DUC?03 dataset.
All the numbers shown in the ta-ble are the average statistics (mean/std).
The ?true?
ap-proximation factor is the ratio of objective function valuefound by Algorithm 1 over the ILP-derived true-optimalobjective value, and the approximation bounds were esti-mated using Theorem 2.Approx.
factor ROUGE-1true bound (%)exact 1.00 - 33.60/5.05r = 0.0 0.65/0.15 ?0.19/0.08 33.50/5.94r = 0.1 0.71/0.15 ?0.24/0.08 33.68/6.03r = 0.3 0.88/0.11 ?0.37/0.06 34.77/5.49r = 0.5 0.96/0.04 ?0.48/0.05 34.33/5.94r = 0.7 0.98/0.02 ?0.56/0.05 34.08/5.41r = 1.0 0.98/0.02 ?0.65/0.04 33.32/5.14r = 1.2 0.97/0.02 ?0.48/0.05 32.54/4.6932.0%32.5%33.0%33.5%34.0%34.5%35.0%0 5 10 15r=0r=0.3r=0.5r=0.7r=1ROUGE-1F-measureFigure 2: Different combinations of r and ?
for fMMRrelated to ROUGE-1 score on DUC?03 task 1.known graph-based approaches, LexRank andPageRank.
LexRank was one of the participat-ing system in DUC?04, with peer code 104.
ForPageRank, we implemented the recursive graph-based ranking algorithm ourselves.
The importanceof sentences was estimated in an iterative way asin (Brin and Page, 1998)(Mihalcea et al, 2004).Sentences were then selected based on their impor-tance rankings until the budget constraint was vi-olated.
The graphs used for PageRank were ex-actly the graphs in our submodular approaches (i.e.,an undirected graph).
In both cases, submodu-lar summarization achieves better ROUGE-1 scores.The improvement is statistically significant by theWilcoxon signed rank test at level p < 0.05.
Ourapproach also outperforms the best system (Conroyet al, 2004), peer code 65 in the DUC?04 evalua-tion although not as significant (p < 0.08).
The rea-son might be that DUC?03 is a poor representationof DUC?04 ?
indeed, by varying r and ?
over theranges 0 ?
r ?
0.2 and 5 ?
?
?
9 respectively, theDUC?04 ROUGE-1 scores were all > 38.8% withthe best DUC?04 score being 39.3%.Table 2: ROUGE-1 F-measure results (%)Method ROUGE-1 scorepeer65 (best system in DUC04) 37.94peer104 (LexRank) 37.12PageRank 35.37Submodular (r = 0.3, ?
= 4) 38.396 AppendixWe analyze the performance guarantee of Algorithm 1.We use the following notation: S?
is the optimal solu-tion; Gf is the final solution obtained by Algorithm 1;G is the solution obtained by the greedy heuristic (line1 to 7 in Algorithm 1); vi is the ith unit added to G,i = 1, .
.
.
, |G|;Gi is the set obtained by greedy algorithmafter adding vi (i.e., Gi = ?ik=1{vk}, for i = 1, .
.
.
, |G|,with G0 = ?
and G|G| = G); f(?)
: 2V ?
R is amonotone submodular function; and ?k(S) is the gain ofadding k to S, i.e., f(S ?
{k})?
f(S).Lemma 1.
?X,Y ?
V ,f(X) ?
f(Y ) +?k?X\Y?k(Y ).
(9)Proof.
See (Nemhauser et al, 1978)Lemma 2.
For i = 1, .
.
.
, |G|, when 0 ?
r ?
1,f(S?)?
f(Gi?1) ?Br|S?|1?rcrvi(f(Gi)?
f(Gi?1)),(10)and when r ?
1,f(S?)?
f(Gi?1) ?(Bcvi)r(f(Gi)?
f(Gi?1))(11)Proof.
Based on line 4 of Algorithm 1, we have?u ?
S?
\Gi?1,?u(Gi?1)cru?
?vi(Gi?1)crvi.918Thus when 0 ?
r ?
1,?u?S?\Gi?1?u(Gi?1) ??vi(Gi?1)crvi?u?S?\Gi?1cru?
?vi(Gi?1)crvi|S?
\Gi?1|(?u?S?\Gi?1 cu|S?
\Gi?1|)r?
?vi(Gi?1)crvi|S?|1?r???u?S?\Gi?1cu??r?
?vi(Gi?1)crvi|S?|1?rBr,where the second inequality is due to the concavity ofg(x) = xr, x > 0, 0 ?
r ?
1.
The last inequality usesthe fact that?u?S?
cu ?
B.
Similarly, when r ?
1,?u?S?\Gi?1?u(Gi?1) ??vi(Gi?1)crvi?u?S?\Gi?1cru?
?vi(Gi?1)crvi???u?S?\Gi?1cu??r?
?vi(Gi?1)crviBr.Applying Lemma 1, i.e., let X = S?
and Y = Gi?1, thelemma immediately follows.The following is a proof of Theorem 2.Proof.
Obviously, the theorem is true when i = 1 byapplying Lemma 2.Assume that the theorem is true for i?1, 2 ?
i ?
|G|,we show that it also holds for i.
When 0 ?
r ?
1,f(Gi) = f(Gi?1) + (f(Gi)?
f(Gi?1))?
f(Gi?1) +crviBr|S?|1?r(f(S?)?
f(Gi?1))=(1?crviBr|S?|1?r)f(Gi?1) +crviBr|S?|1?rf(S?)?(1?crviBr|S?|1?r)(1?i?1?k=1(1?crvkBr|S?|1?r))f(S?)
+crviBr|S?|1?rf(S?)=(1?i?k=1(1?crvkBr|S?|1?r))f(S?
).The case when r ?
1 can be proven similarly.Now we are ready to prove Theorem 1.Proof.
Consider the following two cases:Case 1: ?v ?
V such that f({v}) > 12f(S?).
Then itis guaranteed that f(Gf ) ?
f({v})) > 12f(S?)
due line9 of Algorithm 1.Case 2: ?v ?
V , we have f({v}) ?
12f(S?).
Weconsider the following two sub-cases, namely Case 2.1and Case 2.2:Case 2.1: If?v?G cv ?12B, then we know that?v /?
G, cv > 12B since otherwise we can add a v /?
Ginto G to increase the objective function value withoutviolating the budget constraint.
This implies that there isat most one unit in S?
\ G since otherwise we will have?v?S?
cv > B.
By assumption, we have f(S?
\ G) ?12f(S?).
Submodularity of f(?)
gives us:f(S?
\G) + f(S?
?G) ?
f(S?
),which implies f(S?
?G) ?
12f(S?).
Thus we havef(Gf ) ?
f(G) ?
f(S?
?G) ?12f(S?
),where the second inequality follows from monotonicity.Case 2.2: If?v?G cv >12B, for 0 ?
r ?
1, usingTheorem 2, we havef(G) ???1?|G|?k=1(1?crvkBr|S?|1?r)??
f(S?)???1?|G|?k=1?
?1?crvk |S?|r?12r(?|G|k=1 cvk)r????
f(S?)?(1?(1?
|S?|r?12r|G|r)|G|)f(S?)?(1?
e?12?|S?|2|G|?r?1)f(S?
)where the third inequality uses the fact (provable usingLagrange multipliers) that for a1, .
.
.
, an ?
R+ such that?ni=1 ai = ?, function1?n?i=1(1?
?ari?r)achieves its minimum of 1 ?
(1 ?
?/nr)n when a1 =?
?
?
= an = ?/n for?, ?
> 0.
The last inequality followsfrom e?x ?
1?
x.In all cases, we havef(Gf ) ?
min{12, 1?
e?12?|S?|2|G|?r?1}f(S?
)In particular, when r = 1, we obtain the constant approx-imation factor, i.e.f(Gf ) ?(1?
e?
12)f(S?
)919AcknowledgmentsThis work is supported by an ONR MURI grant(No.
N000140510388), the Companions project(IST programme under EC grant IST-FP6-034434),and the National Science Foundation under grantIIS-0535100.
We also wish to thank the anonymousreviewers for their comments.ReferencesS.
Brin and L. Page.
1998.
The anatomy of a large-scalehypertextual Web search engine.
Computer networksand ISDN systems, 30(1-7):107?117.Jaime Carbonell and Jade Goldstein.
1998.
The use ofMMR, diversity-based reranking for reordering docu-ments and producing summaries.
In Proc.
of SIGIR.J.M.
Conroy, J.D.
Schlesinger, J. Goldstein, and D.P.O?leary.
2004.
Left-brain/right-brain multi-documentsummarization.
In Proceedings of the Document Un-derstanding Conference (DUC 2004).H.T.
Dang.
2005.
Overview of DUC 2005.
In Proceed-ings of the Document Understanding Conference.2004.
Document understanding conferences (DUC).http://www-nlpir.nist.gov/projects/duc/index.html.G.
Erkan and D.R.
Radev.
2004.
LexRank: Graph-based Lexical Centrality as Salience in Text Summa-rization.
Journal of Artificial Intelligence Research,22:457?479.U.
Feige, V. Mirrokni, and J. Vondrak.
2007.
Maximiz-ing non-monotone submodular functions.
In Proceed-ings of 48th Annual IEEE Symposium on Foundationsof Computer Science (FOCS).U.
Feige.
1998.
A threshold of ln n for approximating setcover.
Journal of the ACM (JACM), 45(4):634?652.G.
Goel, , C. Karande, P. Tripathi, and L. Wang.2009.
Approximability of Combinatorial Problemswith Multi-agent Submodular Cost Functions.
FOCS.S.
Iwata, L. Fleischer, and S. Fujishige.
2001.
Acombinatorial strongly polynomial algorithm for min-imizing submodular functions.
Journal of the ACM,48(4):761?777.Yoshinobu Kawahara, Kiyohito Nagano, Koji Tsuda, andJeff Bilmes.
2009.
Submodularity cuts and appli-cations.
In Neural Information Processing Society(NIPS), Vancouver, Canada, December.S.
Khuller, A. Moss, and J. Naor.
1999.
The budgetedmaximum coverage problem.
Information ProcessingLetters, 70(1):39?45.A.
Krause and C. Guestrin.
2005.
A note on the bud-geted maximization of submodular functions.
Techni-cal Rep. No.
CMU-CALD-05, 103.J.
Lee, V.S.
Mirrokni, V. Nagarajan, and M. Sviridenko.2009.
Non-monotone submodular maximization un-der matroid and knapsack constraints.
In Proceedingsof the 41st annual ACM symposium on Symposium ontheory of computing, pages 323?332.
ACM New York,NY, USA.Hui Lin, Jeff Bilmes, and Shasha Xie.
2009.
Graph-based submodular selection for extractive summariza-tion.
In Proc.
IEEE Automatic Speech Recognitionand Understanding (ASRU), Merano, Italy, December.Chin-Yew Lin.
2004.
ROUGE: A package for auto-matic evaluation of summaries.
In Text SummarizationBranches Out: Proceedings of the ACL-04 Workshop.L.
Lovasz.
1983.
Submodular functions and convexity.Mathematical programming-The state of the art,(eds.A.
Bachem, M. Grotschel and B. Korte) Springer,pages 235?257.R.
McDonald.
2007.
A study of global inference al-gorithms in multi-document summarization.
LectureNotes in Computer Science, 4425:557.R.
Mihalcea and P. Tarau.
2004.
TextRank: bringing or-der into texts.
In Proceedings of EMNLP, Barcelona,Spain.R.
Mihalcea, P. Tarau, and E. Figa.
2004.
PageRank onsemantic networks, with application to word sense dis-ambiguation.
In Proceedings of the 20th InternationalConference on Computational Linguistics (COLING-04).R.
Mihalcea.
2004.
Graph-based ranking algorithms forsentence extraction, applied to text summarization.
InProceedings of the ACL 2004 (companion volume).2006.
Mosek.G.L.
Nemhauser, L.A. Wolsey, and M.L.
Fisher.
1978.An analysis of approximations for maximizing sub-modular set functions I.
Mathematical Programming,14(1):265?294.A.
Schrijver.
2000.
A combinatorial algorithm mini-mizing submodular functions in strongly polynomialtime.
Journal of Combinatorial Theory, Series B,80(2):346?355.M.
Sviridenko.
2004.
A note on maximizing a submod-ular set function subject to a knapsack constraint.
Op-erations Research Letters, 32(1):41?43.H.
Takamura and M. Okumura.
2009.
Text summariza-tion model based on maximum coverage problem andits variant.
In Proceedings of the 12th Conference ofthe European Chapter of the Association for Compu-tational Linguistics, pages 781?789.
Association forComputational Linguistics.920
