Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 563?570,Sydney, July 2006. c?2006 Association for Computational LinguisticsSegmented and unsegmented dialogue-act annotation with statisticaldialogue models?Carlos D.
Mart?
?nez Hinarejos, Ramo?n Granell, Jose?
Miguel Bened?
?Departamento de Sistemas Informa?ticos y Computacio?nUniversidad Polite?cnica de ValenciaCamino de Vera, s/n, 46022, Valencia{cmartine,rgranell,jbenedi}@dsic.upv.esAbstractDialogue systems are one of the most chal-lenging applications of Natural LanguageProcessing.
In recent years, some statis-tical dialogue models have been proposedto cope with the dialogue problem.
Theevaluation of these models is usually per-formed by using them as annotation mod-els.
Many of the works on annotationuse information such as the complete se-quence of dialogue turns or the correctsegmentation of the dialogue.
This in-formation is not usually available for dia-logue systems.
In this work, we propose astatistical model that uses only the infor-mation that is usually available and per-forms the segmentation and annotation atthe same time.
The results of this modelreveal the great influence that the availabil-ity of a correct segmentation has in ob-taining an accurate annotation of the dia-logues.1 IntroductionIn the Natural Language Processing (NLP) field,one of the most challenging applications is dia-logue systems (Kuppevelt and Smith, 2003).
Adialogue system is usually defined as a com-puter system that can interact with a human be-ing through dialogue in order to complete a spe-cific task (e.g., ticket reservation, timetable con-sultation, bank operations,.
.
. )
(Aust et al, 1995;Hardy et al, 2002).
Most dialogue system have acharacteristic behaviour with respect to dialogue?
Work partially supported by the Spanish projectTIC2003-08681-C02-02 and by Spanish Ministry of Cultureunder FPI grants.management, which is known as dialogue strat-egy.
It defines what the dialogue system must doat each point of the dialogue.Most of these strategies are rule-based, i.e., thedialogue strategy is defined by rules that are usu-ally defined by a human expert (Gorin et al, 1997;Hardy et al, 2003).
This approach is usually diffi-cult to adapt or extend to new domains where thedialogue structure could be completely different,and it requires the definition of new rules.Similar to other NLP problems (like speechrecognition and understanding, or statistical ma-chine translation), an alternative data-based ap-proach has been developed in the last decade (Stol-cke et al, 2000; Young, 2000).
This approach re-lies on statistical models that can be automaticallyestimated from annotated data, which in this case,are dialogues from the task.Statistical modelling learns the appropriate pa-rameters of the models from the annotated dia-logues.
As a simplification, it could be consideredthat each label is associated to a situation in the di-alogue, and the models learn how to identify andreact to the different situations by estimating theassociations between the labels and the dialogueevents (words, the speaker, previous turns, etc.
).An appropriate annotation scheme should be de-fined to capture the elements that are really impor-tant for the dialogue, eliminating the informationthat is irrelevant to the dialogue process.
Severalannotation schemes have been proposed in the lastfew years (Core and Allen, 1997; Dybkjaer andBernsen, 2000).One of the most popular annotation schemes atthe dialogue level is based on Dialogue Acts (DA).A DA is a label that defines the function of the an-notated utterance with respect to the dialogue pro-cess.
In other words, every turn in the dialogue563is supposed to be composed of one or more ut-terances.
In this context, from the dialogue man-agement viewpoint an utterance is a relevant sub-sequence .
Several DA annotation schemes havebeen proposed in recent years (DAMSL (Core andAllen, 1997), VerbMobil (Alexandersson et al,1998), Dihana (Alca?cer et al, 2005)).In all these studies, it is necessary to annotatea large amount of dialogues to estimate the pa-rameters of the statistical models.
Manual anno-tation is the usual solution, although is very time-consuming and there is a tendency for error (theannotation instructions are not usually easy to in-terpret and apply, and human annotators can com-mit errors) (Jurafsky et al, 1997).Therefore, the possibility of applying statisticalmodels to the annotation problem is really inter-esting.
Moreover, it gives the possibility of evalu-ating the statistical models.
The evaluation of theperformance of dialogue strategies models is a dif-ficult task.
Although many proposals have beenmade (Walker et al, 1997; Fraser, 1997; Stolckeet al, 2000), there is no real agreement in the NLPcommunity about the evaluation technique to ap-ply.Our main aim is the evaluation of strategy mod-els, which provide the reaction of the system givena user input and a dialogue history.
Using thesemodels as annotation models gives us a possibleevaluation: the correct recognition of the labelsimplies the correct recognition of the dialogue sit-uation; consequently this information can help thesystem to react appropriately.
Many recent workshave attempted this approach (Stolcke et al, 2000;Webb et al, 2005).However, many of these works are based on thehypothesis of the availability of the segmentationinto utterances of the turns of the dialogue.
This isan important drawback in order to evaluate thesemodels as strategy models, where segmentation isusually not available.
Other works rely on a de-coupled scheme of segmentation and DA classifi-cation (Ang et al, 2005).In this paper, we present a new statistical modelthat computes the segmentation and the annota-tion of the turns at the same time, using a statis-tical framework that is simpler than the modelsthat have been proposed to solve both problemsat the same time (Warnke et al, 1997).
The resultsdemonstrate that segmentation accuracy is reallyimportant in obtaining an accurate annotation ofthe dialogue, and consequently in obtaining qual-ity strategy models.
Therefore, more accurate seg-mentation models are needed to perform this pro-cess efficiently.This paper is organised as follows: Section 2,presents the annotation models (for both the un-segmented and segmented versions); Section 3,describes the dialogue corpora used in the ex-periments; Section 4 establishes the experimentalframework and presents a summary of the results;Section 5, presents our conclusions and future re-search directions.2 Annotation modelsThe statistical annotation model that we used ini-tially was inspired by the one presented in (Stol-cke et al, 2000).
Under a maximum likeli-hood framework, they developed a formulationthat assigns DAs depending on the conversationevidence (transcribed words, recognised wordsfrom a speech recogniser, phonetic and prosodicfeatures,.
.
.
).
Stolcke?s model uses simple andpopular statistical models: N-grams and HiddenMarkov Models.
The N-grams are used to modelthe probability of the DA sequence, while theHMM are used to model the evidence likelihoodgiven the DA.
The results presented in (Stolcke etal., 2000) are very promising.However, the model makes some unrealistic as-sumptions when they are evaluated to be used asstrategy models.
One of them is that there is acomplete dialogue available to perform the DAassignation.
In a real dialogue system, the onlyavailable information is the information that isprior to the current user input.
Although this al-ternative is proposed in (Stolcke et al, 2000), noexperimental results are given.Another unrealistic assumption corresponds tothe availability of the segmentation of the turnsinto utterances.
An utterance is defined as adialogue-relevant subsequence of words in the cur-rent turn (Stolcke et al, 2000).
It is clear that theonly information given in a turn is the usual in-formation: transcribed words (for text systems),recognised words, and phonetic/prosodic features(for speech systems).
Therefore, it is necessary todevelop a model to cope with both the segmenta-tion and the assignation problem.Let Ud1 = U1U2 ?
?
?Ud be the sequence of DAassigned until the current turn, corresponding tothe first d segments of the current dialogue.
Let564W = w1w2 .
.
.
wl be the sequence of the wordsof the current turn, where subsequences W ji =wiwi+1 .
.
.
wj can be defined (1 ?
i ?
j ?
l).For the sequence of words W , a segmentationis defined as sr1 = s0s1 .
.
.
sr, where s0 = 0 andW = W s1s0+1Ws2s1+1 .
.
.Wsrsr?1+1.
Therefore, theoptimal sequence of DA for the current turn willbe given by:U?
= argmaxUPr(U |W l1, Ud1 ) =argmaxUd+rd+1?
(sr1,r)Pr(Ud+rd+1 |Wl1, Ud1 )After developing this formula and making sev-eral assumptions and simplifications, the finalmodel, called unsegmented model, is:U?
= argmaxUd+rd+1max(sr1,r)d+r?k=d+1Pr(Uk|Uk?1k?n?1) Pr(Wsk?dsk?
(d+1)+1|Uk)This model can be easily implemented usingsimple statistical models (N-grams and HiddenMarkov Models).
The decoding (segmentationand DA assignation) was implemented using theViterbi algorithm.
A Word Insertion Penalty(WIP) factor, similar to the one used in speechrecognition, can be incorporated into the model tocontrol the number of utterances and avoid exces-sive segmentation.When the segmentation into utterances is pro-vided, the model can be simplified into the seg-mented model, which is:U?
= argmaxUd+rd+1d+r?k=d+1Pr(Uk|Uk?1k?n?1) Pr(Wsk?dsk?
(d+1)+1|Uk)All the presented models only take into accountword transcriptions and dialogue acts, althoughthey could be extended to deal with other features(like prosody, sintactical and semantic informa-tion, etc.
).3 Experimental dataTwo corpora with very different features wereused in the experiment with the models proposedin Section 2.
The SwitchBoard corpus is com-posed of human-human, non task-oriented dia-logues with a large vocabulary.
The Dihana corpusis composed of human-computer, task-oriented di-alogues with a small vocabulary.Although two corpora are not enough to let usdraw general conclusions, they give us more reli-able results than using only one corpus.
Moreover,the very different nature of both corpora makesour conclusions more independent from the cor-pus type, the annotation scheme, the vocabularysize, etc.3.1 The SwitchBoard corpusThe first corpus used in the experiments was thewell-known SwitchBoard corpus (Godfrey et al,1992).
The SwitchBoard database consists ofhuman-human conversations by telephone with nodirected tasks.
Both speakers discuss about gen-eral interest topics, but without a clear task to ac-complish.The corpus is formed by 1,155 conversations,which comprise 126,754 different turns of spon-taneous and sometimes overlapped speech, usinga vocabulary of 21,797 different words.
The cor-pus was segmented into utterances, each of whichwas annotated with a DA following the simpli-fied DAMSL annotation scheme (Jurafsky et al,1997).
The set of labels of the simplified DAMSLscheme is composed of 42 different labels, whichdefine categories such as statement, backchannel,opinion, etc.
An example of annotation is pre-sented in Figure 1.3.2 The Dihana corpusThe second corpus used was a task-oriented cor-pus called Dihana (Bened??
et al, 2004).
It is com-posed of computer-to-human dialogues, and themain aim of the task is to answer telephone queriesabout train timetables, fares, and services for long-distance trains in Spanish.
A total of 900 dialogueswere acquired by using the Wizard of Oz tech-nique and semicontrolled scenarios.
Therefore,the voluntary caller was always free to expresshim/herself (there were no syntactic or vocabu-lary restrictions); however, in some dialogues, s/hehad to achieve some goals using a set of restric-tions that had been given previously (e.g.
depar-ture/arrival times, origin/destination, travelling ona train with some services, etc.
).These 900 dialogues comprise 6,280 user turnsand 9,133 system turns.
Obviously, as a task-565Utterance LabelYEAH, TO GET REFERENCES AND THAT, SO, BUT, UH, I DON?T FEEL COMFORTABLE ABOUT LEAVING MY KIDS IN A BIGDAY CARE CENTER, SIMPLY BECAUSE THERE?S SO MANY KIDS AND SO MANY <SNIFFING> <THROAT CLEARING>Yeah, aato get references and that, sdso, but, uh, %I don?t feel comfortable about leaving my kids in a big day care center, simply because there?s somany kids and so many <sniffing> <throat clearing> sdI THINK SHE HAS PROBLEMS WITH THAT, TOO.I think she has problems with that, too.
sdFigure 1: An example of annotated turns in the SwitchBoard corpus.oriented and medium size corpus, the total numberof different words in the vocabulary, 812, is not aslarge as the Switchboard database.The turns were segmented into utterances.
Itwas possible for more than one utterance (withtheir respective labels) to appear in a turn (on av-erage, there were 1.5 utterances per user/systemturn).
A three-level annotation scheme of the ut-terances was defined (Alca?cer et al, 2005).
Theselabels represent the general purpose of the utter-ance (first level), as well as more specific semanticinformation (second and third level): the secondlevel represents the data focus in the utterance andthe third level represents the specific data presentin the utterance.
An example of three-level anno-tated user turns is given in Figure 2.
The corpuswas annotated by means of a semiautomatic pro-cedure, and all the dialogues were manually cor-rected by human experts using a very specific setof defined rules.After this process, there were 248 different la-bels (153 for user turns, 95 for system turns) usingthe three-level scheme.
When the detail level wasreduced to the first and second levels, there were72 labels (45 for user turns, 27 for system turns).When the detail level was limited to the first level,there were only 16 labels (7 for user turns, 9 forsystem turns).
The differences in the number oflabels and in the number of examples for each la-bel with the SwitchBoard corpus are significant.4 Experiments and resultsThe SwitchBoard database was processed to re-move certain particularities.
The main adaptationsperformed were:?
The interrupted utterances (which were la-belled with ?+?)
were joined to the correctprevious utterance, thereby avoiding inter-ruptions (i.e., all the words of the interruptedutterance were annotated with the same DA).Table 1: SwitchBoard database statistics (mean forthe ten cross-validation partitions)Training TestDialogues 1,136 19Turns 113,370 1,885Utterances 201,474 3,718Running words 1,837,222 33,162Vocabulary 21,248 2,579?
All the words were transcribed in lowercase.?
Puntuaction marks were separated fromwords.The experiments were performed using a cross-validation approach to avoid the statistical biasthat can be introduced by the election of fixedtraining and test partitions.
This cross-validationapproach has also been adopted in other recentworks on this corpus (Webb et al, 2005).
In ourcase, we performed 10 different experiments.
Ineach experiment, the training partition was com-posed of 1,136 dialogues, and the test partitionwas composed of 19 dialogues.
This proportionwas adopted so that our results could be comparedwith the results in (Stolcke et al, 2000), wheresimilar training and test sizes were used.
Themean figures for the training and test partitions areshown in Table 1.With respect to the Dihana database, the prepro-cessing included the following points:?
A categorisation process was performed forcategories such as town names, the time,dates, train types, etc.?
All the words were transcribed in lowercase.?
Puntuaction marks were separated fromwords.?
All the words were preceded by the speakeridentification (U for user, M for system).566Utterance 1st level 2nd level 3rd levelYES, TIMES AND FARES.Yes, Acceptance Dep Hour Niltimes and fares Question Dep Hour,Fare NilYES, I WANT TIMES AND FARES OF TRAINS THAT ARRIVE BEFORE SEVEN.Yes, I want times and fares of trains that arrive before seven.
Question Dep Hour,Fare Arr HourON THURSDAY IN THE AFTERNOON.On thursday Answer Day Dayin the afternoon Answer Time TimeFigure 2: An example of annotated turns in the Dihana corpus.
Original turns were in Spanish.Table 2: Dihana database statistics (mean for thefive cross-validation partitions)Training TestDialogues 720 180Turns 12,330 3,083User turns 5,024 1,256System turns 7,206 1,827Utterances 18,837 4,171User utterances 7,773 1,406System utterances 11,064 2,765Running words 162,613 40,765User running words 42,806 10,815System running words 119,807 29,950Vocabulary 832 485User vocabulary 762 417System vocabulary 208 174A cross-validation approach was adopted in Di-hana as well.
In this case, only 5 different parti-tions were used.
Each of them had 720 dialoguesfor training and 180 for testing.
The statistics onthe Dihana corpus are presented in Table 2.For both corpora, different N-gram models,with N = 2, 3, 4, and HMM of one state weretrained from the training database.
In the case ofthe SwitchBoard database, all the turns in the testset were used to compute the labelling accuracy.However, for the Dihana database, only the userturns were taken into account (because systemturns follow a regular, template-based scheme,which presents artificially high labelling accura-cies).
Furthermore, in order to use a really sig-nificant set of labels in the Dihana corpus, weperformed the experiments using only two-levellabels instead of the complete three-level labels.This restriction allowed us to be more independentfrom the understanding issues, which are stronglyrelated to the third level.
It also allowed us to con-centrate on the dialogue issues, which relate moreTable 3: SwitchBoard results for the segmentedmodelN-gram Utt.
accuracy Turn accuracy2-gram 68.19% 59.33%3-gram 68.50% 59.75%4-gram 67.90% 59.14%to the first and second levels.The results in the case of the segmented ap-proach described in Section 2 for SwitchBoard arepresented in Table 3.
Two different definitions ofaccuracy were used to assess the results:?
Utterance accuracy: computes the proportionof well-labelled utterances.?
Turn accuracy: computes the proportion oftotally well-labelled turns (i.e.
: if the la-belling has the same labels in the same or-der as in the reference, it is taken as a well-labelled turn).As expected, the utterance accuracy results area bit worse than those presented in (Stolcke et al,2000).
This may be due to the use of only thepast history and possibly to the cross-validationapproach used in the experiments.
The turn accu-racy was calculated to compare the segmented andthe unsegmented models.
This was necessary be-cause the utterance accuracy does not make sensefor the unsegmented model.The results for the unsegmented approach forSwitchBoard are presented in Table 4.
In this case,three different definitions of accuracy were used toassess the results:?
Accuracy at DA level: the edit distance be-tween the reference and the labelling of theturn was computed; then, the number of cor-rect substitutions (c), wrong substitutions (s),deletions (d) and insertions (i) was com-567Table 4: SwitchBoard results for the unsegmentedmodel (WIP=50)N-gram DA acc.
Turn acc.
Segm.
acc.2-gram 38.19% 39.47% 38.92%3-gram 38.58% 39.61% 39.06%4-gram 38.49% 39.52% 38.96%puted, and the accuracy was calculated as100 ?
c(c+s+i+d) .?
Accuracy at turn level: this provides the pro-portion of well-labelled turns, without takinginto account the segmentation (i.e., if the la-belling has the same labels in the same or-der as in the reference, it is taken as a well-labelled turn).?
Accuracy at segmentation level: this pro-vides the proportion of well-labelled and seg-mented turns (i.e., the labels are the same asin the reference and they affect the same ut-terances).The WIP parameter used in Table 4 was 50,which is the one that offered the best results.
Thesegmentation accuracy in Table 4 must be com-pared with the turn accuracy in Table 3.
As Table 4shows, the accuracy of the labelling decreased dra-matically.
This reveals the strong influence of theavailability of the real segmentation of the turns.To confirm this hypothesis, similar experimentswere performed with the Dihana database.
Ta-ble 5 presents the results with the segmented cor-pus, and Table 6 presents the results with the un-segmented corpus (with WIP=50, which gave thebest results).
In this case, only user turns weretaken into account to compute the accuracy, al-though the model was applied to all the turns (bothuser and system turns).
For the Dihana corpus,the degradation of the results of the unsegmentedapproach with respect to the segmented approachwas not as high as in the SwitchBoard corpus, dueto the smaller vocabulary and complexity of thedialogues.These results led us to the same conclusion,even for such a different corpus (much more la-bels, task-oriented, etc.).
In any case, these ac-curacy figures must be taken as a lower bound onthe model performance because sometimes an in-correct recognition of segment boundaries or dia-logue acts does not cause an inappropriate reactionof the dialogue strategy.Table 5: Dihana results for the segmented model(only two-level labelling for user turns)N-gram Utt.
accuracy Turn accuracy2-gram 75.70% 74.46%3-gram 76.28% 74.93%4-gram 76.39% 75.10%Table 6: Dihana results for the unsegmentedmodel (WIP=50, only two-level labelling for userturns)N-gram DA acc.
Turn acc.
Segm.
acc.2-gram 60.36% 62.86% 58.15%3-gram 60.05% 62.49% 57.87%4-gram 59.81% 62.44% 57.88%An illustrative example of annotation errors inthe SwitchBoard database, is presented in Figure 3for the same turns as in Figure 1.
An error anal-ysis of the segmented model was performed.
Theresults reveals that, in the case of most of the er-rors were produced by the confusion of the ?sv?and ?sd?
classes (about 50% of the times ?sv?
wasbadly labelled, the wrong label was ?sd?)
The sec-ond turn in Figure 3 is an example of this type oferror.
The confusions between the ?aa?
and ?b?classes were also significant (about 27% of thetimes ?aa?
was badly labelled, the wrong label was?b?).
This was reasonable due to the similar defini-tions of these classes (which makes the annotationdifficult, even for human experts).
These errorswere similar for all the N-grams used.
In the caseof the unsegmented model, most of the errors wereproduced by deletions of the ?sd?
and ?sv?
classes,as in the first turn in Figure 3 (about 50% of theerrors).
This can be explained by the presence ofvery short and very long utterances in both classes(i.e., utterances for ?sd?
and ?sv?
did not present aregular length).Some examples of errors in the Dihana corpusare shown in Figure 4 (in this case, for the sameturns as those presented in Figure 2).
In the seg-mented model, most of the errors were substitu-tions between labels with the same first level (es-pecially questions and answers) where the secondlevel was difficult to recognise.
The first and thirdturn in Figure 4 are examples of this type of er-ror.
This was because sometimes the expressionsonly differed with each other by one word, or568Utt Label1 % Yeah, to get references and that, so, but, uh, I don?t2 sdfeel comfortable about leaving my kids in a big day care center, simply becausethere?s so many kids and so many <sniffing> <throat clearing>Utt Label1 sv I think she has problems with that, too.Figure 3: An example of errors produced by the model in the SwitchBoard corpusthe previous segment influence (i.e., the languagemodel weight) was not enough to get the appro-priate label.
This was true for all the N-gramstested.
In the case of the unsegmented model, mostof the errors were caused by similar misrecogni-tions in the second level (which are more frequentdue to the absence of utterance boundaries); how-ever, deletion and insertion errors were also sig-nificant.
The deletion errors corresponded to ac-ceptance utterances, which were too short (mostof them were ?Yes?).
The insertion errors corre-sponded to ?Yes?
words that were placed after anew-consult system utterance, which is the caseof the second turn presented in Figure 4.
Thesewords should not have been labelled as a separateutterance.
In both cases, these errors were verydependant on the WIP factor, and we had to getan adequate WIP value which did not increase theinsertions and did not cause too many deletions.5 Conclusions and future workIn this work, we proposed a method for simultane-ous segmentation and annotation of dialogue ut-terances.
In contrast to previous models for thistask, our model does not assume manual utterancesegmentation.
Instead of treating utterance seg-mentation as a separate task, the proposed methodselects utterance boundaries to optimize the accu-racy of the generated labels.
We performed ex-periments to determine the effect of the availabil-ity of the correct segmentation of dialogue turnsin utterances in the statistical DA labelling frame-work.
Our results reveal that, as shown in previ-ous work (Warnke et al, 1999), having the correctsegmentation is very important in obtaining accu-rate results in the labelling task.
This conclusionis supported by the results obtained in very differ-ent dialogue corpora: different amounts of trainingand test data, different natures (general and task-oriented), different sets of labels, etc.Future work on this task will be carried outin several directions.
As segmentation appearsto be an important step in these tasks, it wouldbe interesting to obtain an automatic and accu-rate segmentation model that can be easily inte-grated in our statistical model.
The application ofour statistical models to other tasks (like VerbMo-bil (Alexandersson et al, 1998)) would allow us toconfirm our conclusions and compare results withother works.The error analysis we performed shows the needfor incorporating new and more reliable informa-tion resources to the presented model.
Therefore,the use of alternative models in both corpora, suchas the N-gram-based model presented in (Webb etal., 2005) or an evolution of the presented statis-tical model with other information sources wouldbe useful.
The combination of these two modelsmight be a good way to improve results.Finally, it must be pointed out that the main taskof the dialogue models is to allow the most correctreaction of a dialogue system given the user in-put.
Therefore, the correct evaluation techniquemust be based on the system behaviour as wellas on the accurate assignation of DA to the userinput.
Therefore, future evaluation results shouldtake this fact into account.AcknowledgementsThe authors wish to thank Nick Webb, Mark Hep-ple and Yorick Wilks for their comments andsuggestions and for providing the preprocessedSwitchBoard corpus.
We also want to thank theanonymous reviewers for their criticism and sug-gestions.ReferencesN.
Alca?cer, J. M.
Bened?
?, F. Blat, R. Granell, C.
D.Mart?
?nez, and F. Torres.
2005.
Acquisition andlabelling of a spontaneous speech dialogue corpus.In Proceedings of SPECOM, pages 583?586, Patras,Greece.Jan Alexandersson, Bianka Buschbeck-Wolf, Tsu-tomu Fujinami, Michael Kipp, Stephan Koch, Elis-569Utterance 1st level 2nd levelYes, times Acceptance Dep Hour,Fareand fares Question Dep Hour,FareYes, I want Acceptance Dep Hour,Faretimes and fares of trains that arrive before seven.
Question Dep Hour,FareOn thursday in the afternoon Answer TimeFigure 4: An example of errors produced by the model in the Dihana corpusabeth Maier, Norbert Reithinger, Birte Schmitz,and Melanie Siegel.
1998.
Dialogue acts inVERBMOBIL-2 (second edition).
Technical Report226, DFKI GmbH, Saarbru?cken, Germany, July.J.
Ang, Y. Liu, and E. Shriberg.
2005.
Automatic dia-log act segmentation and classification in multipartymeetings.
In Proceedings of the International Con-ference of Acoustics, Speech, and Signal Process-ings, volume 1, pages 1061?1064, Philadelphia.H.
Aust, M. Oerder, F. Seide, and V. Steinbiss.
1995.The philips automatic train timetable informationsystem.
Speech Communication, 17:249?263.J.
M.
Bened?
?, A. Varona, and E. Lleida.
2004.
Dihana:Dialogue system for information access using spon-taneous speech in several environments tic2002-04103-c03.
In Reports for Jornadas de Seguimiento- Programa Nacional de Tecnolog?
?as Informa?ticas,Ma?laga, Spain.Mark G. Core and James F. Allen.
1997.
Coding di-alogs with the damsl annotation scheme.
In Work-ing Notes of AAAI Fall Symposium on Communica-tive Action in Humans and Machines, Boston, MA,November.Layla Dybkjaer and Niels Ole Bernsen.
2000.
Themate workbench.N.
Fraser, 1997.
Assessment of interactive systems,pages 564?614.
Mouton de Gruyter.J.
Godfrey, E. Holliman, and J. McDaniel.
1992.Switchboard: Telephone speech corpus for researchand development.
In Proc.
ICASSP-92, pages 517?520.A.
Gorin, G. Riccardi, and J. Wright.
1997.
How mayi help you?
Speech Communication, 23:113?127.Hilda Hardy, Kirk Baker, Laurence Devillers, LoriLamel, Sophie Rosset, Tomek Strzalkowski, Cris-tian Ursu, and Nick Webb.
2002.
Multi-layer di-alogue annotation for automated multilingual cus-tomer service.
In Proceedings of the ISLE Workshopon Dialogue Tagging for Multi-Modal Human Com-puter Interaction, Edinburgh, Scotland, December.Hilda Hardy, Tomek Strzalkowski, and Min Wu.
2003.Dialogue management for an automated multilin-gual call center.
In Proceedings of HLT-NAACL2003 Workshop: Research Directions in DialogueProcessing, pages 10?12, Edmonton, Canada, June.D.
Jurafsky, E. Shriberg, and D. Biasca.
1997.
Switch-board swbd-damsl shallow- discourse-function an-notation coders manual - draft 13.
Technical Report97-01, University of Colorado Institute of CognitiveScience.J.
Van Kuppevelt and R. W. Smith.
2003.
Currentand New Directions in Discourse and Dialogue, vol-ume 22 of Text, Speech and Language Technology.Springer.A.
Stolcke, N. Coccaro, R. Bates, P. Taylor, C. van Ess-Dykema, K. Ries, E. Shriberg, D. Jurafsky, R. Mar-tin, and M. Meteer.
2000.
Dialogue act modellingfor automatic tagging and recognition of conversa-tional speech.
Computational Linguistics, 26(3):1?34.Marilyn A. Walker, Diane Litman J., Candace A.Kamm, and Alicia Abella.
1997.
PARADISE: Aframework for evaluating spoken dialogue agents.In Philip R. Cohen and Wolfgang Wahlster, edi-tors, Proceedings of the Thirty-Fifth Annual Meet-ing of the Association for Computational Linguis-tics and Eighth Conference of the European Chap-ter of the Association for Computational Linguistics,pages 271?280, Somerset, New Jersey.
Associationfor Computational Linguistics.V.Warnke, R. Kompe, H. Niemann, and E. No?th.
1997.Integrated Dialog Act Segmentation and Classifica-tion using Prosodic Features and Language Models.In Proc.
European Conf.
on Speech Communicationand Technology, volume 1, pages 207?210, Rhodes.V.
Warnke, S. Harbeck, E. No?th, H. Niemann, andM.
Levit.
1999.
Discriminative Estimation of Inter-polation Parameters for LanguageModel Classifiers.In Proceedings of the IEEE Conference on Acous-tics, Speech, and Signal Processing, volume 1, pages525?528, Phoenix, AZ, March.N.
Webb, M. Hepple, and Y. Wilks.
2005.
Dialogueact classification using intra-utterance features.
InProceedings of the AAAI Workshop on Spoken Lan-guage Understanding, Pittsburgh.S.
Young.
2000.
Probabilistic methods in spoken di-alogue systems.
Philosophical Trans Royal Society(Series A), 358(1769):1389?1402.570
