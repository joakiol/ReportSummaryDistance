Analyzing the Semantics of Patient Data to Rank Records ofLiterature RetrievalEneida A. Mendon?aDepartment of MedicalInformaticsColumbia Universityem264@columbia.eduStephen B. JohnsonDepartment of MedicalInformaticsColumbia Universitysbj2@columbia.eduYoon-Ho SeolDepartment of MedicalInformaticsColumbia Universityseol@dmi.columbia.eduJames J. CiminoDepartment of MedicalInformaticsColumbia Universityjjc7@columbia.eduAbstractWe describe the use of clinical datapresent in the medical record todetermine the relevance of researchevidence from literature databases.We studied the effect of usingautomated knowledge approaches ascompared to physician?s selection ofarticles, when using a traditionalinformation retrieval system.
Threemethods were evaluated.
The firstmethod identified terms and theirsemantics and relationships in thepatient?s record to build a map of therecord, which was represented inconceptual graph notation.
Thisapproach was applied to data in anindividual?s medical record and usedto score citations retrieved using agraph matching algorithm.
Thesecond method identified associationsbetween terms in the medical record,assigning them semantic types andweights based on the co-occurrence ofthese associations in citations ofbiomedical literature.
The method wasapplied to data in an individual?smedical record and used to scorecitations.
The last method combinedthe first two.
The results showed thatphysicians agreed better with eachother than with the automatedmethods.
However, we found asignificant positive relation betweenphysicians?
selection of abstracts andtwo of the methods.
We believe theresults encourage the use of clinicaldata to determine the relevance ofmedical literature to the care ofindividual patients.1 IntroductionThe practice of evidence-based medicine, whichgained popularity in the last decade, hasencouraged clinicians to understand and utilizecritically appraised published research evidence.The tremendous increase of biomedicalknowledge resources in electronic form,particularly on the World Wide Web, hasgenerated a great deal of interest.
The increasedavailability of information does not make it easyfor clinicians to filter large amounts ofinformation and incorporate evidence to clinicalpractice.
Although the number of clinicians andmedical students who routinely perform theirown searches has increased, they still havedifficulty keeping-up-to-date with advances inmedical science.
(Gorman and Helfand, 1995)Decision support tools designed to providerelevant and current evidence to clinicianspromise to substantially improve health carequality (Haynes, Hayward, and Lomas, 1995;Rodrigues, 2000 ;Sim, et al, 2001) andpotentially reduce medical errors.
(Bates, et al,2001) Such tools include those that facilitate theaccess to, extraction of, and summarization ofevidence.
The Evidence and Decision Supporttrack of the 2000 AMIA Spring Symposiumexamined the challenges in the development andadoption of clinical decision support systems forevidence-based practice.
(Sim, et al, 2001) Thespeakers for the Evidence and Decision Supporttrack described five central areas of activity asessential for the adoption of those systems.
Twoof the areas were a) the capture of bothliterature-based and practice based researchevidence into machine-interpretable form, andb) the establishment of a technical andmethodological foundation for applying researchevidence to individual patients at the point ofcare.Association for Computational Linguistics.the Biomedical Domain, Philadelphia, July 2002, pp.
69-76.Proceedings of the Workshop on Natural Language Processing inOur goal is to improve the way retrievedmedical literature is presented by identifyingcritical information in the individual medicalrecord that is useful for determining therelevance of literature data, also called researchevidence.
We describe an automated knowledgebased approach that uses case-specific evidencepresent in patient?s medical record to rankresearch evidence from literature databases.22.1BackgroundThe integration of information with clinicalapplications may facilitate the access toscientific evidence, clinical guidelines, and otherdecision tools, in a way that informationretrieved from these sources is personalizedbased on the context of individualneeds.
(Cimino, 1996) One of many challengesin building such systems is to understand whatinformation in the individual medical record isimportant to the user and therefore potentiallyuseful in search, retrieval, summarization, andpresentation processes.
Identifying the importantterms, their semantic types, and commonrelationships maybe an interesting solution tothe problem.
The approach we describe here isbased on previous research on automatedmethods to extract information from medicalliterature, and the use of natural languageprocessing techniques to analyze free textclinical reports.
Natural language processingtechniques have been used to analyze free textreports in order to provide data for applications,such as automated encoding, decision support,patient management, quality assurance,outcomes analysis, and clinical research.
(Baud,et al, 1995 ;Fiszman, et al, 2000 ;Friedman, etal., 1994 ;Friedman, et al, 1999 ;Gundersen, etal., 1996 ;Sager, et al, 1995) Data mining andknowledge discovery techniques have been usedto interpret data from natural languageprocessing output of narrative reports.
(Wilcoxand Hripcsak, 2000)Automated extraction from medicalliteratureResearch studies have introduced approaches tofacilitate knowledge extraction from MEDLINE(Cimino and Barnett, 1993 ;Mendon?a andCimino, 2000) and the Unified MedicalLanguage System (UMLS).
(Zeng and Cimino,1998) MEDLINE is the National Library ofMedicine (NLM) premier bibliographic databasecovering the fields of medicine, nursing,dentistry, veterinarian medicine, the health caresystem, and the preclinial sciences.
MEDLINEcontains bibliographic citations and authorabstracts from more than 4,600 biomedicaljournals published in the United States and 70other countries.
MEDLINE citations are indexedwith Medical Subject Headings (MeSH) terms.MeSH (1999) is the NLM?s controlledvocabulary used specifically for medicalbibliographic indexing.
Terms from MeSH aremanually assigned to each document.
TheUMLS project was initiated in the mid-1980s bythe National Library of Medicine.
(Humphreysand Lindberg, 1993) The main goal was toprovide a mechanism for linking diversemedical vocabularies as well as sources ofinformation.
There are currently threecomponents of the UMLS Knowledge Sources:the Metathesaurus, Semantic Network, andSPECIALIST Lexicon.We based our method on the approach describedby Mendon?a and Cimino.
The researchersdescribed an automated knowledge extractionmethod from MEDLINE citations, based on theideas introduced by Zeng and Cimino (Zeng andCimino, 1998), using the search strategies byHaynes and colleagues.
(Haynes, et al, 1994)The approach involved the use of hierarchicaland semantic links in the Medical EntitiesDictionary (MED)(Cimino, et al, 1994) toidentify additional terms which could be used tobuild specific patient-oriented queries.
TheMED uses a frame-based semantic network thatincludes a classification hierarchy to representmedical concepts and the relationship amongthem.
The authors identified semanticassociations in literature citations of four basicclinical tasks: etiology, prognosis, diagnosis,and therapy.
These associations were based onthe co-occurrence of MeSH terms in 4,000MEDLINE citations.The results of the study showed that only 7 to8% of the semantic pairs generated in each taskgroup differ significantly from random chance.A pilot study to assess the clinical validity of theassociations showed a relative good specificityand sensitivity for their intended purpose,information retrieval, except in onegroup(prognosis).
Performance was especiallygood in the therapy group.Figure 1.
Conceptual representation of a culture and sensitivity test344.1Research QuestionThe work we describe here focused on theclinical data present in patients' medical records,and the use of these data to determine therelevance of research evidence.
The mainresearch question was ?What is the effect ofusing the automated knowledge based approachcompared to a physician?s selection of articleswhen using a traditional information retrievalsystem?
?MethodsWe evaluated the application of semanticalgorithms to data in an electronic medicalrecord for sorting abstracts of articles (citations)retrieved from medical literature databases.Semantic ApproachesData from an individual?s medical record wasretrieved from the clinical repository using thelatest entry of each laboratory test and narrativereports, if within one month from the retrievaldata, to create a ?map?
or summary of themedical record.
Discharge summaries were anexception to this rule.
The latest dischargesummary was always retrieved independently ofthe time constraints.The selected narrative reports were parsed byAQUA - A QUery Analyzer,(Johnson, et al,1993) a natural language parser that translatestext into a standard notation: conceptual graphs.
(Sowa, 1984) AQUA?s lexicon is based on theUMLS Metathesaurus.
The UMLS SemanticNet recommends which concepts and relationscan be sensibly combined.Coded data (e.g., laboratory tests) were alsorepresented as conceptual graphs.
We used theMED to infer knowledge when appropriate.
Forinstance, when a glucose measure of 150 mg/dlwas retrieved, the information in the MEDallowed us to infer that the result could also beinterpreted as hyperglycemia.
The MED wasalso used to map concepts in the electronicmedical record to UMLS concepts in order toobtain their semantic types.
Figure 1 shows anexample of a test result extracted from themedical record and its conceptual graphrepresentation.Three semantic algorithms are used.
The firstalgorithm is based on graph matchingtechniques.
The second method identifiesassociations between terms in the medicalrecord, assigning them semantic types andweights based on the co-occurrence of theseassociations in citations of biomedical literature.The method is applied to data in an individual?smedical record, and scored citations accordingto this information.
The last method combinesthe first two.The graph matching algorithm is based onassumption that the similarity of tworepresentations is a function of the amount ofinformation they share.
(Maher, 1993 ;Poole andCampbell, 1995) It worked as follows:1. graphs on both sides (clinical data andcitations) are broken into subgraphs;2. subgraphs of clinical data are thencompared to subgraphs of the citations;3. if a perfect match is found (semantictype and relationship) a score of 1 isgiven.
If not, points are reduced for eachtype of relation that did not match.Points are reduced based on the UMLSsemantic types and relationshiphierarchy (UMLS Semantic Net);4. indirect matches are searched;5. the score is then normalized based onthe number of subgraphs generated byeach graph, and the number of graphs inthe document.Figure 2 shows how the similarity betweentwo graphs is computed.Figure 2.
Simplified graph matchingrepresentationThe second method studied is based on thesemantic associations between concepts in themedical record.
A knowledge base containingthe statistically significant semantic typeassociations found in MEDLINE by Mendon?aand Cimino was built.
In addition to thesemantic types, the knowledge base also storesthe number of times the association occurred inthe citations, the MeSH terms that originated theassociation, and the P values generated by thesignificance test.
The knowledge base containsthree groups of associations: therapy, etiologyand diagnosis.
The associations are groupedbased on the type of questions the citations wereretrieved to answer.
In this method, we identifyall possible associations between semantic typesin the medical record.
Semantic relationships arenot taken in consideration.
If the sameassociations are found in the citations retrieved,we consider it a match.
Only the associationspresent in the knowledge base are weighted.
Theweights for each citation depend on the type ofquestion that originated the citation.The algorithm may be best understoodthrough an example.
Assume a clinician seesMr.
Ocean, and has a question about how totreat Mr.
Ocean?s migraine.
The cliniciansearches the literature and finds two citations,one published in the Annals of InternalMedicine and the second, in the New EnglandJournal of Medicine.
In the semantic approachdescribed, if a pair of semantic types is found inMr.
Ocean?s medical record (e.g., Disease orSyndrome ?
Pharmaceutical Substance) and alsoin the citations retrieved, and the association ispresent in the knowledge base for questions ontherapy, then that association receives a certainweight.
The association weights are based on theco-occurrence of these associations in citationsof biomedical literature.
Two values are used inthe scoring process:  a) number of associationsthat are present in the medical record andcitation, b) the logarithm of the sum of theinverse of P values of each association found.The third semantic algorithm combinesfeatures from the previous two.
For eachassociation that matches the medical record, 0.1point is added to the graph matching score forthat citation.4.2 Evaluation studiesWe performed a study in order to assess theeffect of using the automated knowledgeapproach compared to a physicians?
selection ofarticles when using traditional informationretrieval systems.Three patients consented to the use ofanonymized versions of the data stored in theirelectronic medical records.
We randomlyselected one admission of each patient to buildthe clinical cases.
Data from these individuals?medical records were retrieved from the clinicalrepository as previously described.
Narrativereports were parsed differently depending on thealgorithm in evaluation.
The ?maps?
of the threemedical records were created.
For each case,four clinical questions were selected from adatabase of generic questions based on the workof Ely and collaborators.
(Ely, et al, 2000)Nonclinical questions (e.g., What are theadministrative rules/considerations in <situationy?>) were eliminated from the database beforethe selection.
Each question selected was alsoeliminated before the next random selection, sothat we had a total of 12 unique questions.
Ahealth science librarian generated the searchstrategy for each question based on the casedescription.
Two information retrieval systemswere searched: PubMED (clinical queries usingresearch methodology filters based largely onthe work of Haynes and colleagues) (Haynes, etal., 1994) and OVID (Evidence-Based MedicineReviews)1.
All search strategies were keywordbased with Boolean connectors.
The search wastime limited (last 3 years).
In the cases where nocitation was retrieved, the time limit wasremoved.
The time limit was imposed becausethe time required by an expert to analyze allcitations retrieved without this limitation wouldhave been a disincentive to their participation inthe study.Subjects were recruited as follows.
Threeboard-certified internists, one board-certifiedfamily physician, and one research physicianwere selected as experts.
Four of the fivephysicians actively practice medicine in theirfields.
Participants were given instructions andreceived the following materials: a) cases?description, b) clinical questions selected foreach case, and c) citations retrieved to answereach question.
Case descriptions were based onthe admission note (chief complaint, history ofpresent illness, past medical and surgery history,and current medications), and the results oflaboratory tests performed during the admission.Subjects were asked to score each citationaccording to the relevance of the article(citation) to the question asked and to the patientthe case referred to.
We asked each to define arelevant citation as providing information thatcould be used in the care of that particularpatient.51 EBM Reviews includes the followingdatabases : ACP Journal Club (ACP), CochraneDatabase of Systematic Reviews (COCH), andDatabase of Abstracts of Reviews of Effectiveness(DARE)The score used by the physicians was:1 ?
completely nonrelevant2 ?
almost entirely nonrelevant3 ?
somewhat relevant4 ?
very relevant5 ?
completely relevantEach participant analyzed all questions.The automated methods also scored eachcitation.
The scores were based on how well theabstract and title in the citation matched thecase?s summary.
The computer scores aredescribed in the previous section.
We used theinverse chronological order in which thecitations were provided by their respectiveprograms as an additional method forcomparison (control).The main outcome measure in my study wasthe distance of averaged correlation coefficientsbetween subjects and the average of the raters.For each physician, we calculated the averagedistance from the average of the other 4physicians, and for each automated method, wecalculated the average distance from the averageof all 5 physicians.
The null hypotheses were:a) that each subject was no more distant fromthe average of the physicians than the physicianswere from each other and b) that there was nocorrelation between the average of thephysicians?
scores and the average of thesubjects?
scores.
We used bootstrapping toestimate variance directly from the data.We used Pearson?s product-momentcorrelation to calculate the strength of theassociation between subjects and the average ofthe raters.
In order to accommodate the fact thatquestions had a different number of citationsassociated with them, we calculated a weightedaverage r_ of correlation coefficients ri givenweights wi as follows:r_ = ri * (wi / ?
(wi))wi = (ni - 1) ?where n is the number of citations retrievedin question i.ResultsThe 3 clinical cases and 12 questions generateda set of 219 citations: 111 from PubMED and108 from EBM reviews.
The number of citationsper question varied from 1 to 28.
The fourquestions that retrieved only one citation wereremoved from the statistical analysis.
Thus, thetotal number of citations analyzed was 215.The correlation coefficient between subjectsand the average of raters varied from -0.07 to0.52.
The weighted correlation coefficient foreach subject is listed in Table 1.
A significantpositive correlation was found between theaverage of physicians?
scores and the scoresgiven by the graph matching and the combinedalgorithms.The main outcome measure, the differencebetween subject correlations minus averagephysician correlations, is shown in Table 2.Positive numbers imply worse performance(more unlike the average physician).
Nophysicians differed significantly from otherphysicians.
The automated methods did differfrom physicians with significant P values.Table 1.
Correlation coefficients and significance ofthe correlationSubject Correlation P ValuePhysician 1 0.46 < 0.0001Physician 2 0.44 < 0.0001Physician 3 0.52 < 0.0001Physician 4 0.52 < 0.0001Physician 5 0.48 < 0.0001Graph matching 0.19 0.0098Graph matching +associations0.15 0.046Number of associations -0.07 > 0.05Associations value -0.03 > 0.05Inverse chronologicalorder0.04 > 0.05Table 2.
Average subject correlations minus averagephysician correlationsSubject Difference (95% CI) P ValuePhysician 1 -0.03 (-0.08 to 0.14) 0.60Physician 2 -0.05 (-.08 to 0.18) 0.43Physician 3 0.04 (-0.06 to 0.14) 0.41Physician 4 0.05 (-0.07 to 0.17) 0.40Physician 5 -0.01 (-0.11 to 0.13) 0.86Graph matching 0.29 (0.24 to 0.54)  0.0002Graph matching+ associations0.33 (0.29 to 0.58) < 0.0002Inversechronologicalorder0.44 (0.32 to 0.56) < 0.00026 DiscussionOur main goal in this project was to assess theeffect of the use of clinical data to improvepresentation of medical literature.
We evaluatedthree semantic methods.The level of association between pairs ofsubjects ranged from -0.07 to 0.52.
The level ofassociation associations among physiciansseemed to be similar to levels of agreementbetween 2 independent raters reported in theliterature.
(Wilczynski, McKibbon, and Haynes,2001) No single physician stood out assignificantly different from the others.The graph matching algorithm highlycorrelated with physicians?
average, although itdid not perform as well as individual physicians.This finding encourages the use of clinical datato determine the relevance of medical literatureto the care of individual patients.
In anintegrated system (medical record withinformation resources) this positive correlationsuggests that our method can facilitatepresentation of online biomedical literature.
Forinstance, if the electronic medical record isintegrated to an existent information retrieval,findings from an individual medical record canbe used to rearrange the way retrievedinformation is presented; in a way that literaturematching that individual?s medical record willbe presented first, rather than the usualpresentation in reverse chronological order.The combined method also correlatedsignificantly with physicians?
average, althoughits performance was not as good as of the simplegraph matching.
This result may be due to anegative effect of the associations in theknowledge over the matching.
There was nocorrelation between the methods that use the co-occurrence of semantic types in medicalliterature citations and the average of physicians.The automated method based on thechronological order of articles did not correlatewith physicians?
average.The poor results of the method which usedthe knowledge base of semantic co-occurrencesin Medline citations may be due to severalaspects.
The terms used for indexing medicalcitations may not correspond well to datausually found in medical records.
Approachesusing the UMLS Semantic Net may be alsosomewhat limited by the fact approximately onefourth of the Metathesaurus concepts areassigned several semantic types, which makes itdifficult to get a precise understanding of the co-occurrences.
(Burgun and Bodenreider, 2001)We believe enhancements can still be made.The graph matching algorithm is highlydependent on the output of the natural languageprocessor.
The general language processor usedto parse both clinical data and citations wasnever validated for this use.
AQUA wasdesigned to translate user?s natural languagequeries into a conceptual graph representation.
Itwas developed on a corpus of clinical queries.Prior to this study, the parser was trained withonly a few sentences from the medical literature.The complexity of the clinical data and medicalliterature involved in the study generated asignificant number of ?broken?
graphs.
Thesimilarity found between the graphs was usuallyat the level of single nodes.
It was also observedthat the parser had difficult with very longsentences and sentences in the results section ofthe abstract.
An example of a sentence partiallyparsed is ?Furthermore, patients treated withaprotinin had significantly less totalpostoperative blood loss (718 +/- 340 ml vs 920+/- 387 ml, p =0.04)?.
With enhancements to thenatural language processor, we believe we couldobtain a better representation of the data, andconsequently more accurate results.The use of UMLS Semantic Net may havealso contributed to the elevated incidence of?broken?
graphs.
Mendon?a and Cimino(Mendon?a and Cimino, 2001) found that only22.99% of the associations of semantic typesbased on MeSH terms retrieved from themedical literature had a direct semanticrelationship in the UMLS Semantic Net.
Acareful appreciation of the missing relationshipsmay help us to understand whether the additionof new semantic relationships can contribute to abetter representation of clinical and literaturedata.Whether improvements in the parser to allowit to handle medical literature and complexclinical data would improve the performance ofthe automated methods is unclear; furtherstudies are needed.
The use of this method inassociation with other information retrievaltechniques is being investigated by the authors.7   ConclusionThe goal of the study is to support the use ofclinical data to facilitate the informationretrieval of biomedical literature.
The results ofthis study support this goal.
The use ofconceptual graph representation and graphmatching techniques correlated significantlywith the average of physicians when judging therelevance of citations to the care of an individualpatient.
Additional studies are needed in order tounderstand if this performance is acceptable in aclinical environment.
A careful evaluation of theparsed reports and careful appreciation of themissing relationships may help us to understandthe results and enhance the performance of thealgorithms.ReferencesMedical Subject Headings - AnnotatedAlphabetical List.
Bethesda, MD:  1999.
(National Library of Medicine).Bates DW, Cohen M, Leape LL, OverhageJM, Shabot MM, Sheridan T. Reducing thefrequency of errors in medicine usinginformation technology.
Journal of theAmerican Medical Informatics Association2001; 8(4):299-308.Baud RH, Rassinoux AM, Wagner JC et alRepresenting clinical narratives usingconceptual graphs.
Methods of Information inMedicine 1995; 34(1-2):176-86.Burgun A, Bodenreider O.
Methods forExploring the Semantics of the Relationshipsbetween Co- occurring UMLS Concepts.Medinfo 2001; 10(Pt 1):171-5.Cimino JJ.
Linking patient informationsystems to bibliographic resources.
Methods ofInformation in Medicine 1996; 35(2):122-6.Cimino JJ, Barnett GO.
Automaticknowledge acquisition from MEDLINE.Methods of Information in Medicine 1993;32(2):120-30.Cimino JJ, Clayton PD, Hripcsak G, JohnsonSB.
Knowledge-based approaches to themaintenance of a large controlled medicalterminology.
Journal of the American MedicalInformatics Association 1994; 1(1):35-50.Ely JW, Osheroff JA, Gorman PN et al Ataxonomy of generic clinical questions:classification study.
British Medical Journal2000; 321(7258):429-32.Fiszman M, Chapman WW, Aronsky D,Evans RS, Haug PJ.
Automatic detection ofacute bacterial pneumonia from chest X-rayreports.
Journal of the American MedicalInformatics Association 2000; 7(6):593-604.Friedman C, Alderson PO, Austin JH,Cimino JJ, Johnson SB.
A general natural-language text processor for clinical radiology.Journal of the American Medical InformaticsAssociation 1994; 1(2):161-74.Friedman C, Knirsch C, Shagina L, HripcsakG.
Automating a severity score guideline forcommunity-acquired pneumonia employingmedical language processing of dischargesummaries.
Proceedings of the AMIA FallSymposium 1999; 256-60.Gorman PN, Helfand M. Information seekingin primary care: how physicians choose whichclinical questions to pursue and which to leaveunanswered.
Medical Decision Making 1995;15(2):113-9.Gundersen ML, Haug PJ, Pryor  TA et alDevelopment and evaluation of a computerizedadmission diagnoses encoding system.Computers and Biomedical Research 1996;29(5):351-72.Haynes RB, Hayward RS, Lomas J. Bridgesbetween health care research evidence andclinical practice.
Journal of the AmericanMedical Informatics Association 1995;2(6):342-50.Haynes RB, Wilczynski N, McKibbon KA,Walker CJ, Sinclair JC.
Developing optimalsearch strategies for detecting clinically soundstudies in MEDLINE.
Journal of the AmericanMedical Association 1994; 1(6):447-58.Humphreys BL, Lindberg DAB.
The UMLSproject: making the conceptual connectionbetween users and the information they need.Bulletin of the Medical Library Association1993; 81(2):170-7.Johnson SB, Aguirre A, Peng P, Cimino J.Interpreting natural language queries using theUMLS.
Proceedings of the Annual Symposiumon Computer Applications in Medical Care1993; 294-8.Maher PE.
A similarity measure forconceptual graphs.
International Journal ofIntelligent Systems 1993; 8:819-37.Mendon?a EA, Cimino JJ.
Automatedknowledge extraction from MEDLINE citations.Proceedings of the AMIA Fall Symposium2000; (20 Suppl):575-9.Mendon?a EA, Cimino JJ.
Contentevaluation of a knowledge base.
2001; 974.Poole J, Campbell JA.
A novel algorithm formatching conceptual graphs and related graphs.Ellis G, Levinson R , Rich W, Sowa JF, edts.Conceptual Structures: Applications,Implementation and Theory, Third InternationalConference on Conceptual Structures, ICCS'95.Springer, 1995: 293-307.Rodrigues RJ.
Information systems: the keyto evidence-based health practice.
Bulletin of theWorld Health Organization 2000; 78(11):1344-51.Sager N, Lyman M, Nhan NT, Tick LJ.Medical language processing: applications topatient data representation and automaticencoding.
Methods of Information in Medicine1995; 34(1-2):140-6.Sim I, Gorman P, Greenes RA et al Clinicaldecision support systems for the practice ofevidence-based medicine.
Journal of theAmerican Medical Informatics Association2001; 8(6):527-34.Sowa JF.
Conceptual structures: informationprocessing in mind and machine.
Reading, MA:Addison-Wesley, 1984.Wilcox A, Hripcsak G. Medical textrepresentations for inductive learning.Proceedings of the AMIA Fall Symposium2000; 923-7.Wilczynski NL, McKibbon KA, Haynes RB.Enhancing retrieval of best evidence for healthcare from bibliographic databases: calibration ofthe hand search of the literature.
Medinfo 2001;10(Pt 1):390-3.Zeng Q, Cimino JJ.
Automated knowledgeextraction from the UMLS.
Chute CG.Proceedings of the AMIA Fall Symposium.Philadelphia: Hanley & Belfus Inc., 1998: 568-72.
