Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 437?442,Dublin, Ireland, August 23-24, 2014.NRC-Canada-2014: Detecting Aspects and Sentimentin Customer ReviewsSvetlana Kiritchenko, Xiaodan Zhu, Colin Cherry, and Saif M. MohammadNational Research Council Canada1200 Montreal Rd., Ottawa, ON, Canada{Svetlana.Kiritchenko, Xiaodan.Zhu, Colin.Cherry, Saif.Mohammad}@nrc-cnrc.gc.caAbstractReviews depict sentiments of customerstowards various aspects of a product orservice.
Some of these aspects can begrouped into coarser aspect categories.SemEval-2014 had a shared task (Task 4)on aspect-level sentiment analysis, withover 30 teams participated.
In this pa-per, we describe our submissions, whichstood first in detecting aspect categories,first in detecting sentiment towards aspectcategories, third in detecting aspect terms,and first and second in detecting senti-ment towards aspect terms in the laptopand restaurant domains, respectively.1 IntroductionAutomatically identifying sentiment expressed intext has a number of applications, including track-ing sentiment towards products, movies, politi-cians, etc.
; improving customer relation models;and detecting happiness and well-being.
In manyapplications, it is important to associate sentimentwith a particular entity or an aspect of an entity.For example, in reviews, customers might expressdifferent sentiment towards various aspects of aproduct or service they have availed.
Consider:The lasagna was great, but the servicewas a bit slow.The review is for a restaurant, and we can gatherfrom it that the customer has a positive sentimenttowards the lasagna they serve, but a negative sen-timent towards the service.The SemEval-2014 Task 4 (Aspect Based Sen-timent Analysis) is a shared task where given acustomer review, automatic systems are to deter-mine aspect terms, aspect categories, and senti-ment towards these aspect terms and categories.An aspect term is defined to be an explicit men-tion of a feature or component of the target prod-uct or service.
The example sentence above hasRestaurants LaptopsTerm T-Sent.
Cat.
C-Sent.
Term T-Sent.3 2 1 1 3 1Table 1: Rank obtained by NRC-Canada in vari-ous subtasks of SemEval-2014 Task 4.the aspect term lasagna.
Similar aspect terms canbe grouped into aspect categories.
For example,lasagna and other food items can be grouped intothe aspect category of ?food?.
In Task 4, customerreviews are provided for two domains: restaurantsand laptops.
A fixed set of five aspect categoriesis defined for the restaurant domain: food, ser-vice, price, ambiance, and anecdotes.
Automaticsystems are to determine if any of those aspectcategories are described in a review.
The exam-ple sentence above describes the aspect categoriesof food (positive sentiment) and service (negativesentiment).
For the laptop reviews, there is no as-pect category detection subtask.
Further details ofthe task and data can be found in the task descrip-tion paper (Pontiki et al., 2014).We present an in-house sequence tagger to de-tect aspect terms and supervised classifiers to de-tect aspect categories, sentiment towards aspectterms, and sentiment towards aspect categories.
Asummary of the ranks obtained by our submissionsto the shared task is provided in Table 1.2 Lexical Resources2.1 Unlabeled Reviews CorporaApart from the training data provided for Task 4,we compiled large corpora of reviews for restau-rants and laptops that were not labeled for aspectterms, aspect categories, or sentiment.
We gen-erated lexicons from these corpora and used themas a source of additional features in our machinelearning systems.437Yelp restaurant reviews corpus: The YelpPhoenix Academic Dataset1contains customer re-views posted on the Yelp website.
The businessesfor which the reviews are posted are classified intoover 500 categories.
Further, many of the busi-nesses are assigned multiple business categories.We identified all food-related business categories(58 categories) that were grouped along with thecategory ?restaurant?
and extracted all customerreviews for these categories.
We will refer to thiscorpus of 183,935 reviews as the Yelp restaurantreviews corpus.Amazon laptop reviews corpus: McAuley andLeskovec (2013) collected reviews posted onAmazon.com from June 1995 to March 2013.
Asubset of this corpus is marked as reviews for elec-tronic products.
We extracted from this subset allreviews that mention either laptop or notebook.We will refer to this collection of 124,712 reviewsas the Amazon laptop reviews corpus.Both the Yelp and the Amazon reviews have oneto five star ratings associated with each review.
Wetreated the one- and two-star reviews as negativereviews, and the four- and five-star reviews as pos-itive reviews.2.2 LexiconsSentiment Lexicons: From the Yelp restaurantreviews corpus, we automatically created an in-domain sentiment lexicon for restaurants.
Follow-ing Turney and Littman (2003) and Mohammadet al.
(2013), we calculated a sentiment score foreach term w in the corpus:score (w) = PMI (w , pos)?PMI (w ,neg) (1)where pos denotes positive reviews and neg de-notes negative reviews.
PMI stands for pointwisemutual information:PMI (w , pos) = log2freq (w , pos) ?Nfreq (w) ?
freq (pos)(2)where freq (w, pos) is the number of times a termw occurs in positive reviews, freq (w) is the to-tal frequency of term w in the corpus, freq (pos)is the total number of tokens in positive reviews,and N is the total number of tokens in the cor-pus.
PMI (w ,neg) was calculated in a similarway.
Since PMI is known to be a poor estima-tor of association for low-frequency events, we ig-nored terms that occurred less than five times ineach (positive and negative) groups of reviews.1http://www.yelp.com/dataset_challengeA positive sentiment score indicates a greateroverall association with positive sentiment,whereas a negative score indicates a greater asso-ciation with negative sentiment.
The magnitude isindicative of the degree of association.Negation words (e.g., not, never) can signifi-cantly affect the sentiment of an expression (Zhuet al., 2014).
Therefore, when generating the sen-timent lexicons we distinguished terms appearingin negated contexts (defined as text spans betweena negation word and a punctuation mark) and af-firmative (non-negated) contexts.
The sentimentscores were then calculated separately for the twotypes of contexts.
For example, the term good inaffirmative contexts has a sentiment score of 1.2whereas the same term in negated contexts has ascore of -1.4.
We built two lexicons, Yelp Restau-rant Sentiment AffLex and Yelp Restaurant Senti-ment NegLex, as described in (Kiritchenko et al.,2014).Similarly, we generated in-domain sentimentlexicons from the Amazon laptop reviews corpus.In addition, we employed existing out-of-domain sentiment lexicons: (1) large-coverage au-tomatic tweet sentiment lexicons, Hashtag Sen-timent lexicons and Sentiment140 lexicons (Kir-itchenko et al., 2014), and (2) three manually cre-ated sentiment lexicons, NRC Emotion Lexicon(Mohammad and Turney, 2010), Bing Liu?s Lex-icon (Hu and Liu, 2004), and the MPQA Subjec-tivity Lexicon (Wilson et al., 2005).Yelp Restaurant Word?Aspect AssociationLexicon: The Yelp restaurant reviews corpus wasalso used to generate a lexicon of terms associatedwith the aspect categories of food, price, service,ambiance, and anecdotes.
Each sentence of thecorpus was labeled with zero, one, or more of thefive aspect categories by our aspect category clas-sification system (described in Section 5).
Then,for each term w and each category c an associa-tion score was calculated as follows:score (w , c) = PMI (w , c)?
PMI (w ,?c) (3)2.3 Word ClustersWord clusters can provide an alternative represen-tation of text, significantly reducing the sparsityof the token space.
Using Brown clustering algo-rithm (Brown et al., 1992), we generated 1,000word clusters from the Yelp restaurant reviewscorpus.
Additionally, we used publicly available438word clusters generated from 56 million English-language tweets (Owoputi et al., 2013).3 Subtask 1: Aspect Term ExtractionThe objective of this subtask is to detect aspectterms in sentences.
We approached this problemusing in-house entity-recognition software, verysimilar to the system used by de Bruijn et al.
(2011) to detect medical concepts.
First, sen-tences were tokenized to split away punctuation,and then the token sequence was tagged using asemi-Markov tagger (Sarawagi and Cohen, 2004).The tagger had two possible tags: O for outside,and T for aspect term, where an aspect term couldtag a phrase of up to 5 consecutive tokens.
Thetagger was trained using the structured Passive-Aggressive (PA) algorithm with a maximum step-size of C = 1 (Crammer et al., 2006).Our features can be divided into two categories:emission and transition features.
Emission fea-tures couple the tag sequence y to the input w.Most of these work on the token level, and con-join features of each token with the tag coveringthat token.
If a token is the first or last token cov-ered by a tag, then we produce a second copy ofeach of its features to indicate its special position.Let wibe the token being tagged; its token fea-ture templates are: token-identity within a win-dow (wi?2.
.
.
wi+2), lower-cased token-identitywithin a window (lc(wi?2) .
.
.
lc(wi+2)), and pre-fixes and suffixes of wi(up to 3 characters inlength).
There are only two phrase-level emissionfeature templates: the cased and uncased identityof the entire phrase covered by a tag, which al-low the system to memorize complete terms suchas, ?getting a table?
or ?fish and chips.?
Transi-tion features couple tags with tags.
Let the cur-rent tag be yj.
Its transition feature templates areshort n-grams of tag identities: yj; yj, yj?1; andyj, yj?1, yj?2.During development, we experimented with thetraining algorithm, trying both PA and the simplerstructured perceptron (Collins, 2002).
We alsoadded the lowercased back-off features.
In Ta-ble 2, we re-test these design decisions on the testset, revealing that lower-cased back-off featuresmade a strong contribution, while PA training wasperhaps not as important.
Our complete systemachieved an F1-score of 80.19 on the restaurantdomain and 68.57 on the laptop domain, rankingthird among 24 teams in both.RestaurantsSystem P R F1NRC-Canada (All) 84.41 76.37 80.19All ?
lower-casing 83.68 75.49 79.37All ?
PA + percep 83.37 76.45 79.76LaptopsSystem P R F1NRC-Canada (All) 78.77 60.70 68.57All ?
lower-casing 78.11 60.55 68.22All ?
PA + percep 77.76 61.47 68.66Table 2: Test set ablation experiments for Sub-task 1: Aspect Term Detection.4 Subtask 2: Aspect Term PolarityIn this subtask, the goal is to detect sentiment ex-pressed towards a given aspect term.
For example,in sentence ?The asian salad is barely eatable.?
theaspect term asian salad is referred to with negativesentiment.
There were defined four categories ofsentiment: positive, negative, neutral, or conflict.The conflict category is assigned to cases wherean aspect term is mentioned with both positive andnegative sentiment.To address this multi-class classification prob-lem, we trained a linear SVM classifier usingthe LibSVM software (Chang and Lin, 2011).Sentences were first tokenized and parsed withthe Stanford CoreNLP toolkits2to obtain part-of-speech (POS) tags and (collapsed) typed depen-dency parse trees (de Marneffe et al., 2006).
Then,features were extracted from (1) the target term it-self; (2) its surface context, i.e., a window of nwords surrounding the term; (3) the parse context,i.e., the nodes in the parse tree that are connectedto the target term by at most three edges.Surface features: (1) unigrams (single words)and bigrams (2-word sequences) extracted from aterm and its surface context; (2) context-target bi-grams (i.e., bigrams formed by a word from thesurface context and a word from the term itself).Lexicon features: (1) the number of posi-tive/negative tokens; (2) the sum of the tokens?sentiment scores; (3) the maximal sentiment score.The lexicon features were calculated for eachmanually and automatically created sentiment lex-icons described in Section 2.2.Parse features: (1) word- and POS-ngrams in2http://nlp.stanford.edu/software/corenlp.shtml439Laptops Rest.System Acc.
Acc.NRC-Canada (All) 70.49 80.16All ?
sentiment lexicons 63.61 77.13All ?
Yelp lexicons 68.65 77.85All ?
Amazon lex.
68.13 80.11All ?
manual lexicons 67.43 78.66All ?
tweet lexicons 69.11 78.57All ?
parse features 69.42 78.40Table 3: Test set ablation experiments for Sub-task 2: Aspect Term Polarity.the parse context; (2) context-target bigrams, i.e.,bigrams composed of a word from the parse con-text and a word from the term; (3) all paths thatstart or end with the root of the target terms.
Theidea behind the use of the parse features is thatsometimes an aspect term is separated from itsmodifying sentiment phrase and the surface con-text is insufficient or even misleading for detect-ing sentiment expressed towards the aspect.
Forexample, in sentence ?The food, though differentfrom what we had last time, is actually great?
theword great is much closer to the word food in theparse tree than in the surface form.
Furthermore,the features derived from the parse context canhelp resolve local syntactic ambiguity (e.g., theword bad in the phrase ?a bad sushi lover?
modi-fies lover and not sushi).Table 3 presents the results of our official sub-mission on the test sets for the laptop and restau-rant domains.
On the laptop dataset, our systemachieved the accuracy of 70.49 and was rankedfirst among 32 submissions from 29 teams.
Fromthe ablation experiments we see that the most sig-nificant gains come from the use of the sentimentlexicons; without the lexicon features the perfor-mance of the system drops by 6.88 percentagepoints.
Observe that the features derived fromthe out-of-domain Yelp Restaurant Sentiment lex-icon are very helpful on the laptop domain.
Theparse features proved to be useful as well; theycontribute 1.07 percentage points to the final per-formance.
On the restaurant data, our system ob-tained the accuracy of 80.16 and was ranked sec-ond among 36 submissions from 29 teams.5 Subtask 3: Aspect Category DetectionThe objective of this subtask is to detect aspectcategories discussed in a given sentence.
ThereRestaurantsSystem P R F1NRC-Canada (All) 91.04 86.24 88.58All ?
lex.
resources 86.53 78.34 82.23All ?W?A lexicon 88.47 80.10 84.08All ?
word clusters 90.84 86.15 88.43All ?
post-processing 91.47 84.78 88.00Table 4: Test set ablation experiments for Sub-task 3: Aspect Category Detection.
?W?A lexicon?stands for Yelp Restaurant Word?Aspect Associa-tion Lexicon.are 5 pre-defined categories for the restaurant do-main: food, price, service, ambience, and anec-dotes/miscellaneous.
Each sentence can be la-beled with one or more categories from the pre-defined set.
No aspect categories were defined forthe laptop domain.We addressed the subtask as a multi-class multi-label text classification problem.
Five binary one-vs-all Support Vector Machine (SVM) classifierswere built, one for each category.
The parameter Cwas optimized through cross-validation separatelyfor each classifier.
Sentences were tokenizedand stemmed with Porter stemmer (Porter, 1980).Then, the following sets of features were gener-ated for each sentence: ngrams, stemmed ngrams,character ngrams, non-contiguous ngrams, wordcluster ngrams, and lexicon features.
For the lex-icon features, we used the Yelp Restaurant Word?Aspect Association Lexicon and calculated the cu-mulative scores of all terms appeared in the sen-tence for each aspect category.
Separate scoreswere calculated for unigram and bigram entries.Sentences with no category assigned by any of thefive classifiers went through the post-processingstep.
For each such sentence, a category c with themaximal posterior probability P (c|d) was identi-fied and the sentence was labeled with the categoryc if P (c|d) ?
0.4.Table 4 presents the results on the restaurant testset.
Our system obtained the F1-score of 88.58and was ranked first among 21 submissions from18 teams.
Among the lexical resources (lexiconsand word clusters) employed in the system, theWord?Aspect Association Lexicon provided themost gains: an increase in F1-score of 4.5 points.The post-processing step also proved to be benefi-cial: the recall improved by 1.46 points increasingthe overall F1-score by 0.58 points.4406 Subtask 4: Aspect Category PolarityIn the Aspect Category Polarity subtask, the goalis to detect the sentiment expressed towards agiven aspect category in a given sentence.
Foreach input pair (sentence, aspect category), theoutput is a single sentiment label: positive, neg-ative, neutral, or conflict.We trained one multi-class SVM classifier(Crammer and Singer, 2002) for all aspect cate-gories.
The feature set was extended to incorpo-rate the information about a given aspect categoryc using a domain adaptation technique (Daum?e III,2007) as follows: each feature f had two copies,f general (for all the aspect categories) and f c(for the specific category of the instance).
For ex-ample, for the input pair (?The bread is top notchas well.
?, ?food?)
two copies of the unigram topwould be used: top general and top food .
Withthis setup the classifier can take advantage of thewhole training dataset to learn common sentimentfeatures (e.g., the word good is associated withpositive sentiment for all aspect categories).
At thesame time, aspect-specific sentiment features canbe learned from the training instances pertainingto a specific aspect category (e.g., the word deli-cious is associated with positive sentiment for thecategory ?food?
).Sentences were tokenized and part-of-speechtagged with CMU Twitter NLP tool (Gimpel et al.,2011).
Then, each sentence was represented as afeature vector with the following groups of fea-tures: ngrams, character ngrams, non-contiguousngrams, POS tags, cluster ngrams, and lexiconfeatures.
The lexicon features were calculated asdescribed in Section 4.A sentence can refer to more than one aspectcategory with different sentiment.
For example,in the sentence ?The pizza was delicious, but thewaiter was rude.
?, food is described with posi-tive sentiment while service with negative.
If thewords delicious and rude occur in the training set,the classifier can learn that delicious usually refersto food (with positive sentiment) and rude to ser-vice (with negative sentiment).
If these terms donot appear in the training set, their polarities canstill be inferred from sentiment lexicons.
How-ever, sentiment lexicons do not distinguish amongaspect categories and would treat both words, de-licious and rude, as equally applicable to both cat-egories, ?food?
and ?service?.
To (partially) over-come this problem, we applied the Yelp Restau-RestaurantsSystem AccuracyNRC-Canada (All) 82.93All ?
lexical resources 74.15All ?
lexicons 75.32All ?
Yelp lexicons 79.22All ?
manual lexicons 82.44All ?
tweet lexicons 84.10All ?
word clusters 82.93All ?
aspect term features 82.54Table 5: Test set ablation experiments for Sub-task 4: Aspect Category Polarity.rant Word?Aspect Association Lexicon to collectall the terms having a high or moderate associ-ation with the given aspect category (e.g., pizza,delicious for the category ?food?
and waiter, rudefor the category ?service?).
Then, the feature setdescribed above was augmented with the samegroups of features generated just for the terms as-sociated with the given category.
We call thesefeatures aspect term features.Table 5 presents the results on the test set forthe restaurant domain.
Our system achieved theaccuracy of 82.93 and was ranked first among 23submissions from 20 teams.
The ablation exper-iments demonstrate the significant impact of thelexical resources employed in the system: 8.78percentage point gain in accuracy.
The major ad-vantage comes from the sentiment lexicons, andspecifically from the in-domain Yelp RestaurantSentiment lexicons.
The out-of-domain tweet sen-timent lexicons did not prove useful on this sub-task.
Also, word clusters did not offer additionalbenefits on top of those provided by the lexicons.The use of aspect term features resulted in gainsof 0.39.7 ConclusionThe paper describes supervised machine-learningapproaches to detect aspect terms and aspect cat-egories and to detect sentiment expressed towardsaspect terms and aspect categories in customer re-views.
Apart from common surface-form featuressuch as ngrams, our approaches benefit from theuse of existing and newly created lexical resourcessuch as word?aspect association lexicons and sen-timent lexicons.
Our submissions stood first on 3out of 4 subtasks, and within the top 3 best resultson all 6 task-domain evaluations.441ReferencesPeter F Brown, Peter V Desouza, Robert L Mercer,Vincent J Della Pietra, and Jenifer C Lai.
1992.Class-based n-gram models of natural language.Computational linguistics, 18(4):467?479.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIB-SVM: A library for support vector machines.
ACMTransactions on Intelligent Systems and Technology,2(3):27:1?27:27.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing.Koby Crammer and Yoram Singer.
2002.
On the algo-rithmic implementation of multiclass kernel-basedvector machines.
The Journal of Machine LearningResearch, 2:265?292.Koby Crammer, Ofer Dekel, Joseph Keshet, ShaiShalev-Shwartz, and Yoram Singer.
2006.
Onlinepassive-aggressive algorithms.
Journal of MachineLearning Research, 7:551?585.Hal Daum?e III.
2007.
Frustratingly easy domain adap-tation.
In Proceedings of the Annual Meeting ofthe Association for Computational Linguistics, ACL?07, pages 256 ?
263.Berry de Bruijn, Colin Cherry, Svetlana Kiritchenko,Joel Martin, and Xiaodan Zhu.
2011.
Machine-learned solutions for three stages of clinical infor-mation extraction: the state of the art at i2b2 2010.Journal of the American Medical Informatics Asso-ciation, 18(5):557?562.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.In Proceedings of the International Conference onLanguage Resources and Evaluation, LREC ?06.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein,Michael Heilman, Dani Yogatama, Jeffrey Flanigan,and Noah A. Smith.
2011.
Part-of-speech taggingfor Twitter: Annotation, features, and experiments.In Proceedings of the Annual Meeting of the Associ-ation for Computational Linguistics, ACL ?11.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the 10thACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining, KDD ?04, pages168?177, New York, NY, USA.
ACM.Svetlana Kiritchenko, Xiaodan Zhu, and Saif M. Mo-hammad.
2014.
Sentiment analysis of short infor-mal texts.
Journal of Artificial Intelligence Research(to appear).Julian McAuley and Jure Leskovec.
2013.
Hidden fac-tors and hidden topics: understanding rating dimen-sions with review text.
In Proceedings of the 7thACM conference on Recommender systems, pages165?172.
ACM.Saif M. Mohammad and Peter D. Turney.
2010.
Emo-tions evoked by common words and phrases: UsingMechanical Turk to create an emotion lexicon.
InProceedings of the NAACL-HLT Workshop on Com-putational Approaches to Analysis and Generationof Emotion in Text, LA, California.Saif M. Mohammad, Svetlana Kiritchenko, and Xiao-dan Zhu.
2013.
NRC-Canada: Building the state-of-the-art in sentiment analysis of tweets.
In Pro-ceedings of the International Workshop on SemanticEvaluation, SemEval ?13, Atlanta, Georgia, USA,June.Olutobi Owoputi, Brendan OConnor, Chris Dyer,Kevin Gimpel, Nathan Schneider, and Noah ASmith.
2013.
Improved part-of-speech tagging foronline conversational text with word clusters.
InProceedings of NAACL-HLT, pages 380?390.Maria Pontiki, Dimitrios Galanis, John Pavlopou-los, Harris Papageorgiou, Ion Androutsopoulos, andSuresh Manandhar.
2014.
SemEval-2014 Task 4:Aspect based sentiment analysis.
In Proceedings ofthe International Workshop on Semantic Evaluation,SemEval ?14, Dublin, Ireland, August.M.F.
Porter.
1980.
An algorithm for suffix stripping.Program, 3:130?137.Sunita Sarawagi and William W Cohen.
2004.
Semi-markov conditional random fields for informationextraction.
In Advances in Neural Information Pro-cessing Systems, volume 17, pages 1185?1192.Peter Turney and Michael L Littman.
2003.
Measuringpraise and criticism: Inference of semantic orienta-tion from association.
ACM Transactions on Infor-mation Systems, 21(4).Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of the Con-ference on Human Language Technology and Em-pirical Methods in Natural Language Processing,HLT ?05, pages 347?354, Stroudsburg, PA, USA.Xiaodan Zhu, Hongyu Guo, Saif M. Mohammad, andSvetlana Kiritchenko.
2014.
An empirical study onthe effect of negation words on sentiment.
In Pro-ceedings of the Annual Meeting of the Associationfor Computational Linguistics, ACL ?14.442
