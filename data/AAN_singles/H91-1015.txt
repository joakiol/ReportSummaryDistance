SPEECH RECOGNITION IN SRI'S RESOURCE MANAGEMENTAND ATIS SYSTEMSHy Murveit, John Butzberger, Mitch WeintraubSRI  Internat ional ,  Men lo  Park,  CA  94025OVERVIEWThis paper describes improvements to DECIPHER, the speech recog-nition component in SKI's Air Travel Information Systems (ATIS) andResource Management systems.
DECIPHER is a speaker-independent co -tinuous peech recognition system based on hidden Markov model (HMM)technology.
We show significant performance improvements in DECIPHERdue to (I) the addition of tied-mixture I-IMM modeling (2) rejection of out-of-vocabulary speech and background noise while continuing to recognizespeech (3) adapting to the current speaker (4) the implementation of N-gramstatistical grammars with DECIPHER.
Finally we describe our performancein the February 1991 DARPA Resource Management evaluation (4.8 per-cent word error) and in the February 1991 DARPA-ATIS speech and SLSevaluations (95 sentences correct, 15 wrong of 140).
We show that, for theATIS evaluation, a well-conceived system integration can be relativelyrobust to speech recognition errors and to linguistic variability and errors.IntroductionThe DARPA ATIS Spoken Language System (SLS) taskrepresents ignificant new challenges for speech and naturallanguage technologies.
For speech recognition, the S IS  task ismore difficult than our previous task, DARPA ResourceManagement, along several dimensions: it is recorded in a noisierenvironment, the vocabulary is not fixed, and, most important, it isspontaneous speech, which differs significantly from read speech.Spontaneous peech is a significant challenge to speechrecognition, since it contains false starts, and non-words, andbecause it tends to be more casual than read speech.
It is also amajor challenge to natural language technologies because thestructure of spontaneous language differs dramatically from thestructure of written language, and almost all natural languageresearch as been focused on written language.SLS  Arch i tec tureSRI has developed a spoken language system (SLS) forDARPA's ATIS benchmark task \[1\].
This system can be broken upinto two distinct components, the speech recognition and naturallanguage components.
DECIPHER, the speech recognitioncomponent, accepts the speech waveform as input and produces aword list.
The word list is processed by the natural language (NL)component, which generates a data base query (or no response).This simple serial integration of speech and natural languageprocessing works well because the speech recognition system usesa statistical language model to improve recognition performance,and because the natural language processing uses a templatematching approach that makes it somewhat insensitive torecognition errors.
SRI's SLS achieves relatively high performancebecause the SLS-level system integration acknowledges theimperfect performance of the speech and natural languagetechnologies.
Our natural language component is described inanother paper in this volume \[2\].
This paper focuses on the speechrecognition system and the evaluation of the speech recognition andoverall ATIS SLS systems.Resource Management ArchitectureSRI has also evaluated DECIPHER using DARPA's ResourceManagement task \[3,4\].
The system architecture for this task issimply the speech recognition system with no NL postprocessing.There are two language models used in the evaluation: a perplexity60 word-pair grammar, and a perplexity 1000 all-word grammar.The output is simply an attempted transcription of the input speech.DECIPHERThis section reviews the structure of the DECIPHER system\[5\].
The following sections describe changes to DECIPHER.F ront  End  Ana lys i sDECIPHER uses an FFT-based Mel-cepstra front end.
Twenty-five FFT-Mel filters spanning 100 to 6400 Hz are used to derive 12Mel-cepslxa coefficients every 10-ms frame.
Four features arederived every frame from this cepstra sequence.
They are?
Energy-normalized Mel-cepstra?
Smoothed 40-ms time derivatives of the Mel-cepstra?
Energy?
Smoothed 40-ms energy differences.We use 256-word speaker-independent codebooks to vector-quantize the Mel-cepstra nd the Mel-cepstral differences.
Theresulting four-feature-per-frame vector is used as input to theDECIPHER HMM-based speech recognition system.P ronunc ia t ion  Mode lsDECIPHER uses pronunciation models generated by applyinga phonological rule set to word baseforms.
The techniques used togenerate the rules are described in \[6\] and \[5\], These generateapproximately 40 pronunciations per word as measured on theDARPA Resource Management vocabulary and 75 per word on theATIS vocabulary.
Speaker-independent pronunciation probabilitiesare then estimated using these bushy word networks and theforward-backward algorithm in DECIPHER.
The networks are thenpruned so that only the likely pronunciations remain--typicallyabout 4 per word for the resource management task and 2.6 perword on the ATIS task.
This modeling of pronunciation is one ofthe ways that DECIPHER is distinguished from other HMM-basedsystems.
We have shown in \[6\] that this modeling reduces errorrate.94Acoust i c  Mode l ingDECIPHER builds and trains word models by using context-dependent phone models arranged according to the pronunciationnetworks for the word being modeled.
Models used inelode unique-phone-in-word, phone-in-word, triphone, biphone, and generalizedbiphones and Wiphones, as well as context-independent models.Similar contexts are automatically smoothed together, if they do notadequately model the training data, according to a deleted-estimation interpolation algorithm similar to \[7\].
The acousticmodels reflect both inter-word and across-word eoarticulatoryeffects.
Training proceeds as follows:?
Initially, context-independent boot models are estimatedfrom hand-labels in the TIMIT training database.?
The boot models are used as input for a two-iteration con-text-independent model training run, where context-inde-pendent models are refined and pronunciation probabilitiesare calculated using the full word networks.
These largenetworks are then pruned by eliminating low probabilitypronunciations.?
Context-dependent models are then estimated from a see-ond two-iteration forward-backward run, which uses thecontext-independent models and the pruned networks fromthe previous iterations as input.ACOUSTIC  MODEL ING IMPROVEMENTSTied MixturesWe have implemented tied-mixture HMMs (TM-HMMs) in theDECIPHER system.
Tied mixtures were first described byHuang\[9\] and more recently in by Bellegarda nd Nahamoo\[8\].TM-HMMs use Gaussian mixtures as HMM output probabilities.The mixture weights are unique to each phonetic model used, butthe set of Gaussians i shared among the states.
The tied Ganssianscould be viewed as forming a Gaussian-based VQ codebook that isreestimated bythe HMM forward -backward algorithm.Our implementation of TM-HMMs has the followingcharacteristics:?
We used 12-dimensional diagonal-eovariance Gaussians.The variances were estimated and then smoothed withgrand variances.?
Computation can be significantly reduced in TM-HMMs bypruning either the mixture weights or the Gaussians them-selves.
We found that shortfall threshold Gaussian pruning---discarding all Gaussians whose probability density ofinput at a frame is less than a constant times the best proba-bility density for that flame--works as well for us as stan-dard top-N pruning (keeping the N best Gaussians) andrequires less computation.?
We use two separate sets of Gaussian mixtures for our TM-HMMs; one for Mel cepstra nd one for Mel-cepstral deriv-atives.
We retained our discrete distribution models for ourenergy features.Corrective training \[5,10,11\] was used to update the mixtureweights for the TM-HMMs.
The algorithm is identical to thatused for discrete HMMs.
That is, the mixture weights areupdated as ff they were discrete output probabilities.
No mix-ture means or variances were corrected.We evaluated TM-HMMs on the RM task using the perplexity 60word-pair grammar.
Our training corpus was the standard 3990sentence training set.
We used the combined DARPA 1988, February1989, and October 1989 test sets for our development set.
Thiscontains 900 sentences from 32 speakers.
We achieved a 6.8 percentword error rate using our discrete HMM system on this test set.
TheTM-HMM approach achieved an error rate of 5.5 percent.
Thus, theTM-HMMs improved word recognition error rate by 20 percentcompared to discrete HMMs.Word  Er rorSystem Type (percent)Discrete DECIPHER 6.8Discrete+sex separation 6.3TM-HMM for recognition only 6.4TM-HMM 5.5TM-HMM + sex separation 4.9TM-HMM + corrective training 4.7TM-HMM +sex +corrective 4.5TABLE 1.
Error rate improvements with TM-HMMs with our900-sentence RM development setMale -Female  Separat ionIn the June 1990 DARPA Speech and Natural Language meeting\[5\], we reported a 20 percent reduction in RM word-error rate bytraining separate male and female recognizers, decoding usingrecognizers from both sexes, and then choosing the sex according tothe recognizer with the highest probability hypothesis.
Thisimprovement was achieved using a recognizer trained on 11,190sentences.
We did not achieve a significant improvement using male-female separation on the smaller 3990 sentence training set.
We setout to see, as has been claimed in \[8\], whether TM-HMMs can takeadvantage of male-female separation with smaller (3990 sentence)training sets.
Our results were mixed.
Although performance didimprove from 5.5 percent word error with combined models, to 4.9percent word error with separate male-female models (a 10 percentimprovement) we note that 2/3 of the overall improvement was due tothe dramatic improvement for speaker HXS.
Aside from this onespeaker, the performance gain was not significant.
Based on our laststudy, however, we are confident hat male-female separation doesimprove performance with sufficient raining data.
The table belowshows performance for tied-mixture HMMs using combined and sex-separated models.95Standard Models Male-FemaleModelsName Errs  Wds %Err  Errs  Wds %ErrESG 2 241 0.83 4 241 1.66TAB 4 178 2.25 3 178 1.69CEW 11 241 4.56 5 241 2.07AJC 10 253 3.95 6 253 2.37HXS 36 222 16.22 6 222 2.70DMS 6 179 3.35 5 179 2.79GMB 3 246 1.22 7 246 2.85HLM 11 296 3.72 9 296 3.04BEF 5 226 2.21 7 226 3.10TJS 9 265 3.40 9 265 3.40DAS 14 203 6.90 7 203 3.45JDH 12 246 4.88 9 246 3.66EWM 12 272 4.41 10 272 3.68KLS 8 244 3.28 9 244 3.69DTD 10 233 4.29 10 233 4.29tLEO 9 229 3.93 10 229 4.37DML 18 272 6.62 12 272 4.41PGH 13 204 6.37 9 204 4.41ERS 11 212 5.19 10 212 4.72GAW 15 244 6.15 12 244 4.92AEM 8 302 2.65 17 302 5.63DTB 7 227 3.08 13 227 5.73CTW 17 253 6.72 15 253 5.93CMH 18 230 7.83 15 230 6.52CRZ 23 302 7.62 20 302 6.62DWA 19 270 7.04 19 270 7.04CMR 19 231 8.23 17 231 7.36.IDM 16 271 5.90 21 271 7.75LNS 21 272 7.72 22 272 8.09GAG 22 296 7.43 24 296 8.11JWS 16 222 7.21 21 222 9.46RKM 22 209 10.53 21 209 10.05AVG 427 7791 5.48 384 7791 4.93TABLE 2.
Performance with and without sex-separationThere was no significant additional gain from using correctivetraining in addition to male-female separation.
Performanceimproved from 4.9 percent error (male-female only) or 4.7 percenterror (corrective training only) to 4.5 percent error (both methods).This lack of further improvement is due to the reduction i  trainingdata.Speaker  AdaptationWe have begun experiments into speaker-adaptation,converting speaker-independent models into speaker-dependentones.
Our experiment involved using VQ codebook adaptation viatied-mixture HMMs as proposed by Rtischev \[13\].
That is, weadjusted VQ codeword locations based on forward-backwardalignments of adaptation sentences.
However, since we are using atied-mixture cognition system, we adapted the Gaussian meansinstead of the codebook.We selected 21 of the speakers in our development test set foruse in an adaptation experiment.
We had either 25 or 30 ResourceManagement sentences recorded for each of these speakers.
Wechose to use their first 20 sentences for adaptation, and the other 5or 10 sentences for adaptation testing.Using our original TM-HMM models, we achieved an errorrate of 7.4 percent (114 errors in 1541 reference words) on thisadaptation test set.
After adjusting means for each speaker using the20 adaptation sentences, we achieved an error rate of 6.1 percent(94 errors in 1541 reference words) on the adaptation testsentences.This improvement with adaptation leads to performance that isstill quite short of speaker-dependent accuracy (the ultimate goal ofadaptation).
Thus, it does not seem worth the added inconvenience ofobtaining 20 known sentences from a potential system user, though itis promising for on-line adaptation.
We plan to look into several areasfor further improvement.
For example:1.
Rtischev et al \[14\] have shown that adapting mixture weights is atleast as important as adapting means.2.
Kubala \[15\] et al have shown that adapting speaker-dependentmodels can be superior to adapting from speaker-independentmodels.3.
It is possible that he adaptation sentences need not be supervisedgiven the relatively good (7.4 percent error) initial performance.Re jec t ion  of  Out -o f -Vocabu lary  InputWe implemented a version of DECIPHER that rejects false inputas well as recognizing legal input (our standard recognizer attempts toclassify all the inpu0.
In addition to standard word models, it uses anout-of-vocabulary word model to recognize the extraneous input.
Theword model has the following pronunciation network similar to \[17\].Q All contextindependentphonesah._  ,d lhAll context All contextindependent independentphones phonesFIGURE 1.
Out-of-vocabulary word modelThere are 67 phonetic models on each of the arcs in the aboveword network.
All phonetic transition probabilities in this wordnetwork are equal, and are scaled by a parameter that adjusts theamount of false rejection vs. false acceptance.Thus far, we have performed a pilot study that shows this methodto be promising.
We gathered a database of58 sentences total from sixpeople.
About half of the sentences are digit strings and the other halfare digits mixed with other things.
There are a total of 426 digits in thedatabase, and 176 additional non-digit words.
Example sentences areoutlined in Table 3.We considered correct recognition for these sentences to be thedigits in the string without he rest of the words (i.e.
2138767287,3876541104, 33589170429 are the correct answers for the top threesentences inTable 3).We trained a digit recognizer with rejection from the ResourceManagement training set and achieved a word error ate of 5.3 percentfor the 27 sentences that contained only digits (13 errors = 1 insert 3delete 9 subs in 243 reference words), which is within one error of thesystem without rejection.
Thus, in this pilot study, using rejectiondidn't hurt performance for "clean" input.
The overall error rate was11.7 percent (26 inserts 15 deletes 9 subs in 426 reference words).That is, 402 of 426 digits were detected, and at least 141 of the 176extraneous words were rejected.96my parents number is 2 1 3 urn 8 7 6 ok 7 2 8 7if you have questions please dial extension 3 8 7 6 at5 4 1 1 oh 4please call3 3 5 89 1 urn 7oh4 2 9hmm let's see what's this 1 2 3 4 5 uh that's not right 2 3 4 51 2 3 oh no that's wrong 2 4 5 8 9 yeah i think that's itthis is a test I 2 3 4 5 8 7 this was only a test<grunt> 1 2 <cough> 34 5 <sneeze> 8 7 <mic-noise>4 1 dollars and 3 1 8 centswhat's this oh 4 1 0 8well let's see 3 1 4 7 8 okTABLE 3.
Sample sentences for the rejection studyLANGUAGE MODEL INGB igram Language ModelingWe used a bigram language model to constrain the speechrecognition system for the ATIS evaluation.
A back-off estimationalgorithm \[16\] was used for estimation of the bigram parameters.The training data for the grammar consisted of 5,050 sentences ofspontaneous speech from various sites--l,606 from MIT's ATISdata collection project, 774 from NIST CD-ROM releases, 538from SRI's ATIS data collection project, and 2,132 from variousother sites.Robust estimates for many of the bigram probabilities cannotbe achieved since the vast majority of them are seen veryinfrequently (because of the lack of sufficient training data).Furthermore, frequencies of words such as months and cities werebiased by the data collection scenarios and the time of year the datawas collected.
To reduce these effects, words with effectivelysimilar usage were assigned to groups, and instead of collectingcounts for the individual words, counts were collected for thegroups.
After estimation of the bigram probabilities, theprobabilities of transitioning toindividual words were assigned thegroup probability divided by the number of words in the group.This scheme not only reduced some of the problems due to thesparse training data, but also allowed some unseen words (othercity names, restriction codes, etc.)
to be easily added to thegrammar.
The table below contains the groups of words tiedtogether.months, days, digits, teens, decades, date-ordinals, cities, airports,states, airlines, class-codes, restriction-codes, fare-codes, airline-codes, aircraft-codes, airport-codes, other-codesTABLE 4.
Tied GroupsUsing our back-off bigram on our ATIS development set (mostof the June 1990 DARPA-ATIS test set), we achieved a 14.1percent word error ate with a test-set perplexity of 19 (not counting6 words not covered by the grammar).
When we applied thisgrammar to the February 1991 ATIS evaluation test set (200sentences) the perplexity was 43, excluding 26 instances of wordsnot covered in our vocabulary.
For the 148 Class A sentences, therecognition word error ate was 17.8 percent.We also explored various class-grammar implementations.These grammars were generated by interpolating word-based bigramswith class-based bigrams.
We were able to vary the grammars andtheir perplexities by varying the interpolation coefficients.
However,recognition performance never improved over that for the back-offbigram.
In fact, accuracy remained relatively constant throughout alarge range of perplexities.Table 5 illustrates recognition accuracy using bigrams withdifferent perplexities on our ATIS development test set.
A preliminaryset of models was used for recognition (with 442 words in thevocabulary) and the grammars were estimated using 2,909 sentences.Backed-off BigramInterpolated BigramsWord  ErrorPerplexity (percent)19 14.120 14.524 15.371 14.989 14.791 14.5113 14.9442 29.2TABLE 5.
Perplexity vs. word error on the ATISdevelopment setThese tables also illustrate that recognition performance did notdepend strongly on the test-set perplexity.
Clearly, other factors aredominating performance.
We believe that one of our most pressingneeds in this research is to understand what this bottleneck is, and todevelop ways that express itbetter than perplexity.Multi-Word Lexieal UnitsMany words occur with sufficient frequency and with significantcross-word coarticulation that a better acoustic model might be madeby training these word combinations as a single word model.
Thesewords include "what-are-the," "give-me," etc., which can have avariety of pronunciations best modeled with a network of phonesrepresenting the phonetic and phonological variation of the wholesequence ("what're-the," "gimme," etc.)
instead of each wordseparately.Also, when considering class grammars, multiple word sequencesallow classes which could not be constructed by considering everyword separately.
For instance, having distinct models of all therestriction codes (e.g.
"v-u-slash-one") might be more appropriatethan modeling alpha->alpha->slash->number in the bigram.
Thelatter form would allow all the alphabet letters to transition to all thealphabet letters, with probabilities as prescribed by the bigram, andwould incorrectly increase the probability for invalid restrictioncodes.This multi-word technique allows all the probabilities of all therestriction codes to be tied together, so that all are equally covered atthe appropriate place in the grammar, instead of depending completelyon the individual words' statistics estimated from sparse training data.The multi-word approach resulted in only a slight performanceimprovement compared to a system where non-coarticulatory multi-words were left separated.
That is, for the "separate words" system,words like "a p slash eighty" were separate words, but coarticulatoryword models like "what-are-the" and "list-the" were retained.
On a97ll9-sentence subset of the June 90 evaluation set, the results wereas shown in Table 6.Development Set PerformancePerplexity Word Er ror(percent)Multi-Word 26 9.6Separate Words 20 10.7February  1991 Class-A Evaluat ion PerformanceWord Er rorPerplexity (percent)Multi-Word 43 17.8Separate Words 34 18.3TABLE 6.
Effectiveness of multi-word modelingNote that the higher perplexity of the multi-word system isdeceiving since high probability grammar transitions are nowhidden within the multi-word models, and are not seen by thegrammar.
Tables 7 and 8 list the various multi-word units.flights-from, what-is-the, show-me-the, show-me-all, show-me,how-many, one-way, what-are-the, give-me, what-is, i-would-like,i'd-like-to, what-doesTABLE 7.
Coarticulatory Multi-WordsCITIES:AIRLINES:AIRCRAFT:AIRPORTS:CLASS CODES:RESTRICT CODES:COLUMN HEADS:san-francisco, washington-de ....a-l, c-o, t-w-a, u-s-air, ...d-e-ten, seven-forty-seven ....a-t-l, b-o-s, s-f-o, d-f-w, ...q-x, f-y-b-m-q, k-y, y-n ....a-p-eighty, a-p-slash-eighty,...d-u-r-a, e-q-p, r-t-n-max ....TABLE 8.
Semantic Multi-WordsEVALUATIONRM Eva luat ionSRI evaluated the DECIPHER system on DARPA's February1991 speaker-independent tes set.
The characteristics of theevaluated system were:?
Speaker-independent r cognition?
3990 sentence DARPA-RM training?
3 state, left-to-right, context-dependent hidden Markermodel using deleted-interpolation estimation ofparameters?
Input features were 12 Mel-cepstra nd delta-Mel-cepstraand scalar quantized energy and delta-energy?
Tied-mixture modeling for Mel cepstra nd delta-Mel-cep-stra?
256 diagonal covariance Gaussians for each?
Independent discrete density HMM models for energy anddelta energy?
Multiple pronunciation trained phonological modeling, about4 pronunciations per word on average?
Cross-word acoustic and phonological modeling?
Sex-consistent modeling?
Corrective training on mixture weights?
Resource Management all-word and word-pair grammarsused with 992-word Resource Management vocabulary.We achieved the performance shown in Table 9.Speaker P=60 P=1000ALK03 9.7 20.8CALl5 2.5 11.9CAU07 2.6 14.7EAC02 10.2 22.0JLS04 1.6 11.1JWG05 7.5 19.5MEB03 2.9 17.6SAS05 2.2 10.4STK01 4.1 21.2TBR01 5.2 27.8Average 4.8 17.6TABLE 9.
DARPA-RM February 1991 speaker-independentevaluationOur performance is severely limited by training data\[S\], and manyfurther improvements for the RM task may only be ways to workaround RM's artificial limit on training data.
Thus, we expect odevelop and evaluate our system in the future with the ATIS taskwhich both has more training data available and uses more realistic(spontaneous) speech.SLS Eva luat ionWe evaluated on DARPA's February 1991 ATIS test set using asystem similar to the one described above xcept:?
The system was trained on 17,042 sentences (3990 RM-SI,4200 TIM1T, 7932 read ATIS, 920 spontaneous ATIS).?
1,139 word vocabulary (the test set vocabulary was notrevealed in advance) using multi-word units.?
Discrete distribution HMM modeling was used for all fea-tures.?
A back-off bigram language model \[16\] with tied word-groups was used, with a test set perplexity of 43 (not counting26 words out of vocabulary).?
A template-matcher natural language component \[2\] was usedto generate ATIS database queries based on the speech recog-nition output.We achieved the performance shown in Table 10.98SPKR Corr Sub Del Ins Err Sent ErrCL 93.6 5.1 1.3 1.7 8.1 42.3CJ 92.0 6.9 1.0 0.7 8.7 46.2CO 92.0 3.7 4.3 1.2 9.3 56.2CP 90.7 7.5 1.8 2.5 11.8 59.3CK 83.3 8.8 7.8 1.0 17.6 58.3CH 84.2 5.3 10.5 5.3 21.1 100.0CE 81.5 12.0 6.5 3.2 21.8 70.0CI 73.1 24.0 2.9 5.8 32.7 90.0CM 75.0 23.5 1.5 26.5 51.5 100.0Average 86.5 10.3 3.1 4.3 17.8 60.1All-word (Perplexity 1139)Average 86.5 23.9 3.7 8.0 35.5speech TABLE 10.
DARPA-ATIS February 1991148 Class A Sentences91.2evaluationSPKR Cor r  Sub Del Ins Er r  Sent ErrCJ 91.9 6.5 1.6 0.8 8.9 54.5CP 91.7 6.6 1.7 1.7 10.0 55.2CL 91.4 6.7 1.9 1.9 10.4 44.8CK 85.0 8.7 6.3 0.5 15.5 64.0CE 83.0 11.8 5.2 2.6 19.6 73.9CO 79.4 13.7 6.9 1.4 22.0 75.9CH 78.6 13.1 8.3 3.6 25.0 100.0CI 67.1 27.3 5.6 5.6 38.6 92.9CM 72.5 25.2 2.3 23.9 51.4 100.0Average 83.5 12.6 3.9 4.2 20.7 66.5DARPA-ATIS FebruaryAll sentencesTABLE 11.
1991 speech evaluationAs can be seen, speakers CI and CM contributed significantlyto the overall error rate.
Furthermore, many of the errors occurreddespite their relatively small bigram probabilities, indicating thatthe grammar is still not completely effective in overriding pooracoustic matches.Table 12performance.SystemNL OnlySLSdescribes overall spoken language systemRight Wrong NA 1 WErr  2 Score 3109 9 27 31.0 69.096 11 38 41.4 58.6TABLE 12.
DARPA-ATIS February 1991 SLS evaluation148 Class A sentencesDiscuss ionThe most interesting result of this evaluation (see the paper byPaUett in this proceedings) was that, though SRI along with BBNachieved the best speech recognition accuracy, and SRI along withCMU had the best natural-language-only performance, theaccuracy of SRI's combined speech and natural language systems1.
NA is no answer2.
WErr or weighted error is percent no answer plus two times thepercent wrong.3.
Score = 100 - Werrwas far better than that for the other sites.
We attribute this to the errortolerant nature of our speech/natural-language interface.
For instance,note that performance using spoken language is not much worse thanthe performance of the NL component given transcribed input (i.e.given a perfect speech recognition component) even though the SLSspeech recognition component had a 60 percent sentence error rate (atleast one word was wrong in 60 percent of the sentences).The above results indicate to us that steady progress in the speechrecognition and natural anguage technologies, together with error-tolerant speech/natural-language int rfaces can lead to practicalspoken language systems in the near future.REFERENCES1 Price, P., "The ATIS Common Task: Selection and Overview,"Proceedings DARPA Speech and Natural Language Workshop,June 1990.2 Jackson, E., D. Appelt, J.
Bear, R. Moore, and A. Podlozny, "ATemplate Matcher for Robust NL Interpretation," ProceedingsDARPA Speech and Natural Language Workshop, June 1991.3 Pallet, D., "Benchmark Tests for DARPA Resource ManagementDatabase Performance Evaluations," Proceedings ICASSP-89.4 Price, P., W.M.
Fisher, J. Bemstein, and D.S.
Pallet, "The DARPA1000-Word Resource Management Database for ContinuousSpeech Recognition," Proceedings ICASSP-88.5 Murveit, H., M. Weintraub, M. Cohen, "Training Set Issues inSRI's DECIPHER Speech Recognition System," ProceedingsDARPA Speech and Natural Language Workshop, June 1990.6 Cohen, M., H. Murveit, J. Bernstein, P. Price, M. Weintraub, "TheDECIPHER Speech Recognition System," Proceedings ICASSP,April 1990.7 Jelinek, F. and R. Mercer, "Interpolated Estimation of MarkovSource Parameters from Sparse Data," pp.
381-397 in E.S.Gelsima nd L.N.
Kanal (editors), Pattern Recognition i  Prac-t/ce, North Holland Publishing Company, Amsterdam, The Neth-erlands.8 BeUegarda, J. D. Nahamoo, "Tied Mixture Continuous ParameterModeling for Speech Recognition," IEEE Trans.
ASSP, December1990.9 Huang, X.D.,"Semi-continuous hidden Markov models for speechrecognition," Computer Speech and Language, 3 pp.
239-251(1989)10 Bahl, L., P. Brown, P. De Souza, and R. Mercer, "A New Algo-rithm for the Estimation of Hidden Markov Model Parameters,"Proceedings ICASSP-g8.11 Lee, K-F., and S. Mahajan, Corrective and Reinforcement Learn-ing for Speaker-Independent Co inuous Speech Recognition,Technical Report CMU-CS-89-100, Carnegie Mellon University,January 1989.12 Huang, X., F. Alleva, S. Hayamizu, H.-W. Hon, M.-Y.
Hwang, andK.-E Lee, "Improved Hidden Markov Modeling for Speaker-Inde-pendent Continuous Speech Recognition," Proceedings DARPASpeech and Natural Language Workshop, June 1990.9913 Rtischev, Dimitry, Speaker Adaptation i a Large-VocabularySpeech Recognition System, Master's Thesis, Dept.
of ElectricalEngineering and Computer Science, Massachusetts In titute ofTechnology.14 Rtischev, D., D.. Nahamoo, and M. Picheny, "Speaker Adapta-tion via VQ Prototype Modification," submitted to IEEE Trans.Signal Processing.15 Kubala, Francis, Richard Schwartz, and Chris Barry, "SpeakerAdaptation from a Speaker Independent Training Corpus," Pro-ceedings ICASSP-90.16 Katz, S., "Estimation of Probabilities from Sparse Data for theLanguage Model Component ofa Speech Recognizer," IEEETrans.
ASSP, March 1987.17 Asadi, A., R. Schwartz, and J. Makhoul, "Automatic Detectionof New Words in a Large Vocabulary Continuous Speech Rec-ognition System," Proceedings DARPA Speech and NaturalLanguage Workshop, OcL 1989.i 00
