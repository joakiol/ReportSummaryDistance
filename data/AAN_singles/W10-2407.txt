Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 53?56,Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational LinguisticsTransliteration Mining with Phonetic Conflation and Iterative TrainingKareem DarwishCairo Microsoft Innovation CenterCairo, Egyptkareemd@microsoft.comAbstractThis paper presents transliteration mining on theACL 2010 NEWS workshop shared translitera-tion mining task data.
Transliteration miningwas done using a generative transliteration modelapplied on the source language and whose outputwas constrained on the words in the target lan-guage.
A total of 30 runs were performed on 5language pairs, with 6 runs for each languagepair.
In the presence of limited resources, theruns explored the use of phonetic conflation anditerative training of the transliteration model toimprove recall.
Using letter conflation improvedrecall by as much as 48%, with improvements inrecall dwarfing drops in precision.
Using itera-tive training improved recall, but often at the costof significant drops in precision.
The best runstypically used both letter conflation and iterativelearning.1 IntroductionTransliteration Mining (TM) is the process of find-ing transliterated word pairs in parallel or compa-rable corpora.
TM has many potential applicationssuch as building training data for training translit-erators and improving lexical coverage for machinetranslation and cross language search via transla-tion resource expansion.
TM has been gainingsome attention of late with a shared task in theACL 2010 NEWS workshop1.
In this paper, TMwas performed using a transliterator that was usedto generate possible transliterations of a word whileconstraining the output to tokens that exist in a tar-get language word sequence.
The paper presentsthe use of phonetic letter conflation and iterativetransliterator training to improve TM when onlylimited transliteration training data is available.For phonetic letter conflation, a variant ofSOUNDEX (Russell, 1918) was used to improvethe coverage of existing training data.
As for itera-tive transliterator training, an initial transliterator,which was trained on initial set of transliterationpairs, was used to mine transliterations in paralleltext.
Then, the automatically found transliterationspairs were considered correct and were used to re-train the transliterator.1 http://translit.i2r.a-star.edu.sg/news2010/The proposed improvements in TM were testedusing the ACL 2010 NEWS workshop data for Ar-abic, English-Chinese, English-Hindi, English-Russian, and English-Tamil.
For language pair, abase set of 1,000 transliteration pairs were availablefor training.The rest of the paper is organized as follows:  Sec-tion 2 surveys prior work on transliteration mining;Section 3 describes the TM approach and the pro-posed improvements; Section 4 describes the ex-perimental setup including the evaluation sets; Sec-tion 5 reports on experimental results; and Section6 concludes the paper.2 BackgroundMuch work has been done on TM for different lan-guage pairs such as English-Chinese (Kuo et al,2006; Kuo et al, 2007; Kuo et al, 2008; Jin et al2008;), English-Tamil (Saravanan and Kumaran,2008; Udupa and Khapra, 2010), English-Korean(Oh and Isahara, 2006; Oh and Choi, 2006), Eng-lish-Japanese (Brill et al, 2001; Oh and Isahara,2006), English-Hindi (Fei et al, 2003; Mahesh andSinha, 2009), and English-Russian (Klementievand Roth, 2006).
The most common approach fordetermining letter sequence mapping between twolanguages is using automatic letter alignment of atraining set of transliteration pairs.
Automaticalignment can be performed using different algo-rithms such as the EM algorithm (Kuo et al, 2008;Lee and Chang, 2003) or using an HMM aligner(Udupa et al, 2009a; Udupa et al, 2009b).
Anoth-er method is to use automatic speech recognitionconfusion tables to extract phonetically equivalentcharacter sequences to discover monolingual andcross lingual pronunciation variations (Kuo andYang, 2005).
Alternatively, letters can be mappedinto a common character set.
One example of thatis to use a predefined transliteration scheme totransliterate a word in one character set into anothercharacter set (Oh and Choi, 2006).
Different meth-ods were proposed to ascertain if two words can betransliterations of each other.
One such way is touse a generative model that attempts to generatepossible transliterations given the character map-pings between two character sets (Fei et al, 2003;Lee and Chang, 2003, Udupa et al, 2009a).
A sim-ilar alternative is to use back-transliteration to de-53termine if one sequence could have been generatedby successively mapping character sequences fromone language into another (Brill et al, 2001; Bilacand Tanaka, 2005; Oh and Isahara, 2006).
Anothermapping method is to map candidate translitera-tions into a common representation space (Udupaet al, 2010).
When using a predefined translitera-tion scheme, edit distance was used to determine ifcandidate transliterations were indeed translitera-tions (Oh and Choi, 2006).
Also letter conflationwas used to find transliterations (Mahesh and Sin-ha, 2009).
Different methods were proposed toimprove the recall of mining.
For example, Oh andChoi (2006) used a SOUNDEX like scheme tominimize the effect of vowels and differentschemes of phonetically coding names.SOUNDEX is used to convert English words into asimplified phonetic representation, in which vowelsare removed and phonetically similar characters areconflated.
Another method involved expandingcharacter sequence maps by automatically miningtransliteration pairs and then aligning these pairs togenerate an expanded set of character sequencemaps (Fei et al, 2003).3 Transliteration MiningTM proposed in this paper uses a generative trans-literation model, which is trained on a set of trans-literation pairs.
The training involved automatical-ly aligning character sequences.
SOUNDEX likeletter conflation and iterative transliterator trainingwas used to improve recall.
Akin to phrasal align-ment in machine translation, character sequencealignment was treated as a word alignment problembetween parallel sentences, where transliterationswere treated as if they were sentences and the char-acters from which they were composed were treat-ed as if they were words.
The alignment was per-formed using a Bayesian learner that trained onword dependent transition models for HMM basedword alignment (He, 2007).
Alignment produced amapping of source character sequence to a targetcharacter sequence along with the probability ofsource given target.For all the work reported herein, given an English-foreign language transliteration candidate pair,English was treated as the target language and theforeign language as the source.
Given a foreignsource language word sequenceand an Englishtarget word sequence,is a potentialtransliteration of.
Given Fi, composed ofthe character sequence f1 ?
fo, and Ej, composed ofthe character sequence e1 ?
ep, P(Fi|Ej) is calculat-ed using the trained model, as follows:(  |  )  ?The non-overlapping segments fx ?
fy are generatedby finding all possible 2n-1segmentations of theword Fi.
For example, given ?man?
then all pos-sible segmentations are (m,a,n), (ma,n), (m,an), and(man).
The segmentation producing the highestprobability is chosen.
All segment sequences e?k ...e?l known to produce fx ?
fy for each of the possiblesegmentations are produced.
If a set of non-overlapping sequences of e?k ... e?l generates thesequence e1 ?
ep (word), then Ej is con-sidered a transliteration of Fi.
If multiple targetwords have P(Fi|Ej) > 0, then Ej that maximizesP(Fi|Ej) is taken as the proper transliteration.
Asuffix tree containingwas used to constraingeneration, improving efficiency.
No smoothingwas used.To improve recall, a variant of SOUNDEX wasused on the English targets.
The originalSOUNDEX scheme applies the following rules:1.
Retain the first letter in a word2.
Remove all vowels, H, and W3.
Perform the following mappings:B, F, P, V ?
1 C, G, J, K, Q, S, X, Z ?
2D,T ?
3 L ?
4M,N ?
5 R ?
64.
Trim all result sequences to 4 characters5.
Pad short sequences with zeros to have exactly4 characters.SOUNDEX was modified as follows:1.
The first letter in a word was not retained andwas changed according the mapping in step 3of SOUNDEX.2.
Resultant sequences longer than 4 characterswere not trimmed.3.
Short resultant strings were not padded withzeros.SOUNDEX after the aforementioned modificationsis referred at S-mod.
Alignment was performedbetween transliteration pairs where English wordswere replaced with their S-mod representation.Case folding was always applied to English.Iterative transliterator training involved training atransliterator using an initial seed of transliterationpairs, which was used to automatically mine trans-literations from a large set of parallel words se-quences.
Automatically mined transliteration pairswere assumed to be correct and were used to retrainthe transliterator.
S-mod and iterative training wereused in isolation or in combination as is shown inthe next section.Russian and Arabic were preprocessed as follows:?
Russian: characters were case-folded?
Arabic: the different forms of alef (alef, alefmaad, alef with hamza on top, and alef withhamza below it) were normalized to alef, yaand alef maqsoura were normalized to ya, andta marbouta was mapped to ha.54No preprocessing was performed for the other lan-guages.
Since Wikipedia English entries often hadnon-English characters, the following letter confla-tions were performed:?, ?
?
z                ?, ?, ?, ?, ?, ?, ?, ?
?
a?, ?, ?
?
e ?, ?, ?
?
c?
?
l ?, ?, ?, ?
?
i?, ?, ?, ?
?
o ?, ?, ?
?
n?, ?, ?, ?
?
s ?
?
r?
?
y ?, ?, ?, ?
?
uLanguage Pair # of Parallel SequencesEnglish-Arabic 90,926English-Chinese 196,047English-Hindi 16,963English-Russian 345,969English-Tamil 13,883Table 1: Language pairs and no.
of parallel sequencesRun Precision Recall F-score1 0.900 0.796 0.8452  0.966 0.587 0.7303 0.952 0.588 0.7274  0.886 0.817 0.8505 0.895 0.678 0.7716  0.818 0.827 0.822Table 2: English-Arabic mining resultsRun Precision Recall F-score1 1.000 0.024 0.0472  1.000 0.016 0.0323 1.000 0.016 0.0324  1.000 0.026 0.0505 1.000 0.022 0.0446  1.000 0.030 0.059Table 3: English-Chinese mining resultsRun Precision Recall F-score1 0.959 0.786 0.8642  0.987 0.559 0.7143 0.984 0.569 0.7214  0.951 0.812 0.8765 0.981 0.687 0.8086  0.953 0.855 0.902Table 4: English-Hindi mining resultsRun Precision Recall F-score1 0.813 0.839 0.8262  0.868 0.748 0.8043 0.843 0.747 0.7924  0.716 0.868 0.7855 0.771 0.794 0.7826  0.673 0.881 0.763Table 5: English-Russian mining resultsRun Precision Recall F-score1 0.963 0.604 0.7432  0.976 0.407 0.5753 0.975 0.446 0.6124  0.952 0.668 0.7855 0.968 0.567 0.7156  0.939 0.741 0.828Table 6: English-Tamil mining resultsFor each foreign language (F) and English (E) pair,a set of 6 runs were performed.
The first two runsinvolved training a transliterator using the 1,000transliteration pairs and using it for TM as in sec-tion 3.
The runs were:Run 1:  align F with S-mod(E)Run 2:  align F with EThe four other runs involved iterative training inwhich all automatically mined transliterations fromRuns 1 and 2 were considered correct, and wereused to retrain the transliterator.
The runs were:Run 3:  Use Run 2 output, align F with ERun 4:  Use Run 2 output, align F with S-mod(E)Run 5:  Use Run 1 output, align F with ERun 6:  Use Run 1 output, align F with S-mod(E)For evaluation, the system would mine translitera-tions and a set of 1,000 parallel sequences werechosen randomly for evaluation.
The figures ofmerit are precision, recall, F1 measure.4 Experimental SetupThe experiments were done on the ACL-2010NEWS Workshop TM shared task datasets.
Thedatasets cover 5 language pairs.
For each pair, adataset includes a list of 1,000 transliterated wordsto train a transliterator, and list of parallel wordsequences between both languages.
The parallelsequences were extracted parallel Wikipedia articletitles for which cross language links exist betweenboth languages.
Table 1 lists the language pairsand the number of the parallel word sequences.5 Experimental ResultsTables 2, 3, 4, 5, and 6 report results for Arabic,Chinese, Hindi, Russian and Tamil respectively.As shown in Table 3, the recall for English-ChineseTM was dismal and suggests problems in experi-mental setup.
This would require further investiga-tion.
For the other 4 languages, the results showthat not using S-mod and not using iterative train-ing, as in Run 2, led to the highest precision.
Usingboth S-mod and iterative training, as in Run 6, ledto the highest recall.In comparing Runs 1 and 2, where 1 uses S-modand 2 does not, using S-mod led to 35.6%, 40.6%,12.2%, and 48.4% improvement in recall and to6.8%, 2.8%, 6.3%, and 1.3% decline in precisionfor Arabic, Chinese, Russian, and Tamil respective-ly.
Except for Russian, the improvements in recalldwarf decline in precision, leading to overall im-provements in F-measure for all 4 languages.In comparing runs 2 and 3 where iterative trainingis used, iterative training had marginal impact onprecision and recall.
When using S-mod, compar-ing run 6 where iterative training was performedover the output from run 1, recall increased by553.9%, 8.8%, 5.0%, and 22.7% for Arabic, Chinese,Russian, and Tamil respectively.
The drop in pre-cision was 9.1% and 17.2% for Arabic and Russianrespectively and marginal for Hindi and Tamil.Except for Russian, the best runs for all languagesincluded the use of S-mod and iterative training.The best runs were 4 for Arabic and Hindi and 6for Tamil.
For Russian, the best runs involved us-ing S-mod only without iterative training.
Thedrop in Russian could be attributed to the relativelylarge size of training data compared to the otherlanguages (345,969 parallel word sequences).6 ConclusionThis paper presented two methods for improvingtransliteration mining, namely phonetic conflationof letters and iterative training of a transliterationmodel.
The methods were tested using on the ACL2010 NEWS workshop shared transliteration min-ing task data.
Phonetic conflation of letters in-volved using a SOUNDEX like conflation schemefor English.
This led to much improved recall andgeneral improvements in F-measure.
The iterativetraining of the transliteration model led to im-proved recall, but recall improvements were oftenoffset by decreases in precision.
However, the bestexperimental setups typically involved the use ofboth improvements.The success of phonetic conflation for Englishmay indicate that similar success may be attained ifphonetic conflation is applied to other languages.Further, the use of smoothing of the transliterationmodel may help improve recall.
The recall fortransliteration mining between English and Chinesewere dismal and require further investigation.ReferencesSlaven Bilac, Hozumi Tanaka.
Extracting transliterationpairs from comparable corpora.
NLP-2005, 2005.Eric Brill, Gary Kacmarcik, Chris Brockett.
Automati-cally harvesting Katakana-English term pairs fromsearch engine query logs.
NLPRS 2001, pages 393?399, 2001.Huang Fei, Stephan Vogel, and Alex Waibel.
2003.
Ex-tracting Named Entity Translingual Equivalence withLimited Resources.
TALIP, 2(2):124?129.Xiaodong He, 2007.
Using Word-Dependent TransitionModels in HMM based Word Alignment for Statisti-cal Machine Translation.
ACL-07 2ndSMT workshop.Chengguo Jin, Dong-Il Kim, Seung-Hoon Na, Jong-Hyeok Lee.
2008.
Automatic Extraction of English-Chinese Transliteration Pairs using Dynamic Windowand Tokenizer.
Sixth SIGHAN Workshop on Chi-nese Language Processing, 2008.Alexandre Klementiev and Dan Roth.
2006.
Named En-tity Transliteration and Discovery from MultilingualComparable Corpora.
HLT Conf.
of the North Ameri-can Chapter of the ACL, pages 82?88.Jin-Shea Kuo, Haizhou Li, Ying-Kuei Yang.
2006.Learning Transliteration Lexicons from the Web.COLING-ACL2006, Sydney, Australia, 1129 ?
1136.Jin-shea Kuo, Haizhou Li, Ying-kuei Yang.
A phoneticsimilarity model for automatic extraction of translit-eration pairs.
TALIP, 2007Jin-Shea Kuo, Haizhou Li, Chih-Lung Lin.
2008.
Min-ing Transliterations from Web Query Results: An In-cremental Approach.
Sixth SIGHAN Workshop onChinese Language Processing, 2008.Jin-shea Kuo, Ying-kuei Yang.
2005.
IncorporatingPronunciation Variation into Extraction of Translit-erated-term Pairs from Web Corpora.
Journal of Chi-nese Language and Computing, 15 (1): (33-44).Chun-Jen Lee, Jason S. Chang.
Acquisition of English-Chinese transliterated word pairs from parallel-aligned texts using a statistical machine transliterationmodel.
Workshop on Building and Using ParallelTexts, HLT-NAACL-2003, 2003.R.
Mahesh, K. Sinha.
2009.
Automated Mining OfNames Using Parallel Hindi-English Corpus.
7thWorkshop on Asian Language Resources, ACL-IJCNLP 2009, pages 48?54, Suntec, Singapore, 2009.Jong-Hoon Oh, Key-Sun Choi.
2006.
Recognizingtransliteration equivalents for enriching domain-specific thesauri.
3rd Intl.
WordNet Conf.
(GWC-06),pages 231?237, 2006.Jong-Hoon Oh, Hitoshi Isahara.
2006.
Mining the Webfor Transliteration Lexicons: Joint-Validation Ap-proach.
pp.254-261, 2006 IEEE/WIC/ACM Intl.Conf.
on Web Intelligence (WI'06), 2006.Raghavendra Udupa, K. Saravanan, Anton Bakalov, andAbhijit Bhole.
2009a.
"They Are Out There, If YouKnow Where to Look": Mining Transliterations ofOOV Query Terms for Cross-Language InformationRetrieval.
ECIR-2009, Toulouse, France, 2009.Raghavendra Udupa, K. Saravanan, A. Kumaran, andJagadeesh Jagarlamudi.
2009b.
MINT: A Methodfor Effective and Scalable Mining of Named EntityTransliterations from Large Comparable Corpora.EACL 2009.Raghavendra Udupa and Mitesh Khapra.
2010.
Trans-literation Equivalence using Canonical CorrelationAnalysis.
ECIR-2010, 2010.Robert Russell.
1918.
Specifications of Letters.
USpatent number 1,261,167.K Saravanan, A Kumaran.
2008.
Some Experiments inMining Named Entity Transliteration Pairs fromComparable Corpora.
The 2ndIntl.
Workshop onCross Lingual Information Access addressing theneed of multilingual societies, 2008.56
