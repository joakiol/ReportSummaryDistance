Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 11?15,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsCollaborative Machine Translation Service for Scientific textsPatrik LambertUniversity of Le Manspatrik.lambert@lium.univ-lemans.frJean SenellartSystran SAsenellart@systran.frLaurent RomaryHumboldt Universita?t Berlin /INRIA Saclay - Ile de Francelaurent.romary@inria.frHolger SchwenkUniversity of Le Mansholger.schwenk@lium.univ-lemans.frFlorian ZipserHumboldt Universita?t Berlinf.zipser@gmx.dePatrice LopezHumboldt Universita?t Berlin /INRIA Saclay - Ile de Francepatrice.lopez@inria.frFre?de?ric BlainSystran SA /University of Le Mansfrederic.blain@lium.univ-lemans.frAbstractFrench researchers are required to fre-quently translate into French the descrip-tion of their work published in English.
Atthe same time, the need for French peopleto access articles in English, or to interna-tional researchers to access theses or pa-pers in French, is incorrectly resolved viathe use of generic translation tools.
Wepropose the demonstration of an end-to-endtool integrated in the HAL open archive forenabling efficient translation for scientifictexts.
This tool can give translation sugges-tions adapted to the scientific domain, im-proving by more than 10 points the BLEUscore of a generic system.
It also providesa post-edition service which captures userpost-editing data that can be used to incre-mentally improve the translations engines.Thus it is helpful for users which need totranslate or to access scientific texts.1 IntroductionDue to the globalisation of research, the Englishlanguage is today the universal language of sci-entific communication.
In France, regulations re-quire the use of the French language in progressreports, academic dissertations, manuscripts, andFrench is the official educational language of thecountry.
This situation forces researchers to fre-quently translate their own articles, lectures, pre-sentations, reports, and abstracts between Englishand French.
In addition, students and the generalpublic are also challenged by language, when itcomes to find published articles in English or tounderstand these articles.
Finally, internationalscientists not even consider to look for Frenchpublications (for instance PhD theses) becausethey are not available in their native languages.This problem, incorrectly resolved through theuse of generic translation tools, actually revealsan interesting generic problem where a commu-nity of specialists are regularly performing trans-lations tasks on a very limited domain.
At thesame time, other communities of users seek trans-lations for the same type of documents.
Withoutappropriate tools, the expertise and time spent fortranslation activity by the first community is lostand do not benefit to translation requests of theother communities.We propose the demonstration of an end-to-endtool for enabling efficient translation for scientifictexts.
This system, developed for the COSMATANR project,1 is closely integrated into the HALopen archive,2 a multidisciplinary open-accessarchive which was created in 2006 to archive pub-lications from all the French scientific commu-nity.
The tool deals with handling of source doc-ument format, generally a pdf file, specialisedtranslation of the content, and user-friendly user-interface allowing to post-edit the output.
Behind1http://www.cosmat.fr/2http://hal.archives-ouvertes.fr/?langue=en11the scene, the post-editing tool captures user post-editing data which are used to incrementally im-prove the translations engines.
The only equip-ment required by this demonstration is a computerwith an Internet browser installed and an Internetconnection.In this paper, we first describe the completework-flow from data acquisition to final post-editing.
Then we focus on the text extraction pro-cedure.
In Section 4, we give details about thetranslation system.
Then in section 5, we presentthe translation and post-editing interface.
We fi-nally give some concluding remarks.The system will be demonstrated at EACL inhis tight integration with the HAL paper depositsystem.
If the organizers agree, we would like tooffer the use of our system during the EACL con-ference.
It would automatically translate all theabstracts of the accepted papers and also offersthe possibility to correct the outputs.
This result-ing data would be made freely available.2 Complete Processing Work-flowThe entry point for the system are ?ready to pub-lish?
scientific papers.
The goal of our systemwas to extract content keeping as many meta-information as possible from the document, totranslate the content, to allow the user to performpost-editing, and to render the result in a format asclose as possible to the source format.
To train oursystem, we collected from the HAL archive morethan 40 000 documents in physics and computerscience, including articles, PhD theses or researchreports (see Section 4).
This material was used totrain the translation engines and to extract domainbilingual terminology.The user scenario is the following:?
A user uploads an article in PDF format3 onthe system.?
The document is processed by the open-source Grobid tool (see section 3) to extract3The commonly used publishing format is PDF fileswhile authoring format is principally a mix of MicrosoftWord file and LaTeX documents using a variety of styles.The originality of our approach is to work on the PDF fileand not on these source formats.
The rationale being that 1/the source format is almost never available, 2/ even if we hadaccess to the source format, we would need to implement afilter specific to each individual template required by such orsuch conference for a good quality content extractionthe content.
The extracted paper is structuredin the TEI format where title, authors, refer-ences, footnotes, figure captions are identi-fied with a very high accuracy.?
An entity recognition process is performedfor markup of domain entities such as:chemical compounds for chemical papers,mathematical formulas, pseudo-code and ob-ject references in computer science papers,but also miscellaneous acronyms commonlyused in scientific communication.?
Specialised terminology is then recognisedusing the Termsciences4 reference termi-nology database, completed with terminol-ogy automatically extracted from the train-ing corpus.
The actual translation of the pa-per is performed using adapted translation asdescribed in Section 4.?
The translation process generates a bilingualTEI format preserving the source structureand integrating the entity annotation, multi-ple terminology choices when available, andthe token alignment between source and tar-get sentences.?
The translation is proposed to the user forpost-editing through a rich interactive inter-face described in Section 5.?
The final version of the document is thenarchived in TEI format and available for dis-play in HTML using dedicated XSLT stylesheets.3 The Grobid SystemBased on state-of-the-art machine learning tech-niques, Grobid (Lopez, 2009) performs reliablebibliographic data extraction from scholar articlescombined with multi-level term extraction.
Thesetwo types of extraction present synergies and cor-respond to complementary descriptions of an arti-cle.This tool parses and converts scientific arti-cles in PDF format into a structured TEI docu-ment5 compliant with the good practices devel-oped within the European PEER project (Bretel etal., 2010).
Grobid is trained on a set of annotated4http://www.termsciences.fr5http://www.tei-c.org12scientific article and can be re-trained to fit tem-plates used for a specific conference or to extractadditional fields.4 Translation of Scientific TextsThe translation system used is a Hybrid MachineTranslation (HMT) system from French to En-glish and from English to French, adapted totranslate scientific texts in several domains (sofar physics and computer science).
This sys-tem is composed of a statistical engine, cou-pled with rule-based modules to translate spe-cial parts of the text such as mathematical for-mulas, chemical compounds, pseudo-code, andenriched with domain bilingual terminology (seeSection 2).
Large amounts of monolingual andparallel data are available to train a SMT systembetween French and English, but not in the scien-tific domain.
In order to improve the performanceof our translation system in this task, we extractedin-domain monolingual and parallel data from theHAL archive.
All the PDF files deposited in HALin computer science and physics were made avail-able to us.
These files were then converted toplain text using the Grobid tool, as described inthe previous section.
We extracted text from allthe documents from HAL that were made avail-able to us to train our language model.
We builta small parallel corpus from the abstracts of thePhD theses from French universities, which mustinclude both an abstract in French and in English.Table 1 presents statistics of these in-domain data.The data extracted from HAL were used toadapt a generic system to the scientific litera-ture domain.
The generic system was mostlytrained on data provided for the shared task ofSixth Workshop on Statistical Machine Transla-tion6 (WMT 2011), described in Table 2.Table 3 presents results showing, in theEnglish?French direction, the impact on the sta-tistical engine of introducing the resources ex-tracted from HAL, as well as the impact of do-main adaptation techniques.
The baseline statis-tical engine is a standard PBSMT system basedon Moses (Koehn et al 2007) and the SRILMtookit (Stolcke, 2002).
Is was trained and tunedonly on WMT11 data (out-of-domain).
Incorpo-rating the HAL data into the language model andtuning the system on the HAL development set,6http://www.statmt.org/wmt11/translation-task.htmlSet Domain Lg Sent.
Words Vocab.Parallel dataTrain cs+phys En 55.9 k 1.41 M 43.3 kFr 55.9 k 1.63 M 47.9 kDev cs En 1100 25.8 k 4.6 kFr 1100 28.7 k 5.1 kphys En 1000 26.1 k 5.1 kFr 1000 29.1 k 5.6 kTest cs En 1100 26.1 k 4.6 kFr 1100 29.2 k 5.2 kphys En 1000 25.9 k 5.1 kFr 1000 28.8 k 5.5 kMonolingual dataTrain cs En 2.5 M 54 M 457 kFr 761 k 19 M 274 kphys En 2.1 M 50 M 646 kFr 662 k 17 M 292 kTable 1: Statistics for the parallel training, develop-ment, and test data sets extracted from thesis abstractscontained in HAL, as well as monolingual data ex-tracted from all documents in HAL, in computer sci-ence (cs) and physics (phys).
The following statisticsare given for the English (En) and French (Fr) sides(Lg) of the corpus: the number of sentences, the num-ber of running words (after tokenisation) and the num-ber of words in the vocabulary (M and k stand for mil-lions and thousands, respectively).yielded a gain of more than 7 BLEU points, inboth domains (computer science and physics).
In-cluding the theses abstracts in the parallel trainingcorpus, a further gain of 2.3 BLEU points is ob-served for computer science, and 3.1 points forphysics.
The last experiment performed aims atincreasing the amount of in-domain parallel textsby translating automatically in-domain monolin-gual data, as suggested by Schwenk (2008).
Thesynthesised bitext does not bring new words intothe system, but increases the probability of in-domain bilingual phrases.
By adding a syntheticbitext of 12 million words to the parallel trainingdata, we observed a gain of 0.5 BLEU point forcomputer science, and 0.7 points for physics.Although not shown here, similar results wereobtained in the French?English direction.
TheFrench?English system is actually slightly bet-ter than the English?French one as it is an easiertranslation direction.13Translation Model Language Model Tuning Domain CS PHYSwords (M) Bleu words (M) Bleuwmt11 wmt11 wmt11 371 27.3 371 27.1wmt11 wmt11+hal hal 371 36.0 371 36.2wmt11+hal wmt11+hal hal 287 38.3 287 39.3wmt11+hal+adapted wmt11+hal hal 299 38.8 307 40.0Table 3: Results (BLEU score) for the English?French systems.
The type of parallel data used to train thetranslation model or language model are indicated, as well as the set (in-domain or out-of-domain) used to tunethe models.
Finally, the number of words in the parallel corpus and the BLEU score on the in-domain test set areindicated for each domain: computer science and physics.Figure 1: Translation and post-editing interface.Corpus English FrenchBitexts:Europarl 50.5M 54.4MNews Commentary 2.9M 3.3MCrawled (109 bitexts) 667M 794MDevelopment data:newstest2009 65k 73knewstest2010 62k 71kMonolingual data:LDC Gigaword 4.1G 920MCrawled news 2.6G 612MTable 2: Out-of-domain development and training dataused (number of words after tokenisation).5 Post-editing InterfaceThe collaborative aspect of the demonstrated ma-chine translation service is based on a post-editingtool, whose interface is shown in Figure 1.
Thistool provides the following features:?
WYSIWYG display of the source and targettexts (Zones 1+2)?
Alignment at the sentence level (Zone 3)?
Zone to review the translation with align-ment of source and target terms (Zone 4) andterminology reference (Zone 5)?
Alternative translations (Zone 6)The tool allows the user to perform sentencelevel post-editing and records details of post-editing activity, such as keystrokes, terminologyselection, actual edits and time log for the com-plete action.6 Conclusions and PerspectivesWe proposed the demonstration of an end-to-endtool integrated into the HAL archive and enabling14efficient translation for scientific texts.
This toolconsists of a high-accuracy PDF extractor, a hy-brid machine translation engine adapted to the sci-entific domain and a post-edition tool.
Thanks toin-domain data collected from HAL, the statisti-cal engine was improved by more than 10 BLEUpoints with respect to a generic system trained onWMT11 data.Our system was deployed for a physic confer-ence organised in Paris in Sept 2011.
All acceptedabstracts were translated into author?s native lan-guages (around 70% of them) and proposed forpost-editing.
The experience was promoted bythe organisation committee and 50 scientists vol-unteered (34 finally performed their post-editing).The same experience will be proposed for authorsof the LREC conference.
We would like to offera complete demonstration of the system at EACL.The goal of these experiences is to collect and dis-tribute detailed ?post-editing?
data for enablingresearch on this activity.AcknowledgementsThis work has been partially funded by the FrenchGovernment under the project COSMAT (ANRANR-09-CORD-004).ReferencesFoudil Bretel, Patrice Lopez, Maud Medves, AlainMonteil, and Laurent Romary.
2010.
Back tomeaning ?
information structuring in the PEERproject.
In TEI Conference, Zadar, Croatie.Philipp Koehn, Hieu Hoang, Alexandra Birch,Chris Callison-Burch, Marcello Federico, NicolaBertoldi, Brooke Cowan, Wade Shen, ChristineMoran, Richard Zens, Chris Dyer, Ondrej Bojar,Alexandra Constantin, and Evan Herbst.
2007.Moses: Open source toolkit for statistical ma-chine translation.
In Proc.
of the 45th AnnualMeeting of the Association for Computational Lin-guistics (Demo and Poster Sessions), pages 177?180, Prague, Czech Republic, June.
Association forComputational Linguistics.Patrice Lopez.
2009.
GROBID: Combining auto-matic bibliographic data recognition and term ex-traction for scholarship publications.
In Proceed-ings of ECDL 2009, 13th European Conference onDigital Library, Corfu, Greece.Holger Schwenk.
2008.
Investigations on large-scalelightly-supervised training for statistical machinetranslation.
In IWSLT, pages 182?189.A.
Stolcke.
2002.
SRILM: an extensible languagemodeling toolkit.
In Proc.
of the Int.
Conf.
on Spo-ken Language Processing, pages 901?904, Denver,CO.15
