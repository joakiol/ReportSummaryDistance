Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 161?169,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPReview Sentiment Scoring via a Parse-and-Paraphrase ParadigmJingjing Liu, Stephanie SeneffMIT Computer Science & Artificial Intelligence Laboratory32 Vassar Street, Cambridge, MA 02139{jingl, seneff}@csail.mit.eduAbstractThis paper presents a parse-and-paraphrase pa-radigm to assess the degrees of sentiment forproduct reviews.
Sentiment identification hasbeen well studied; however, most previouswork provides binary polarities only (positiveand negative), and the polarity of sentiment issimply reversed when a negation is detected.The extraction of lexical features such as uni-gram/bigram also complicates the sentimentclassification task, as linguistic structure suchas implicit long-distance dependency is oftendisregarded.
In this paper, we propose an ap-proach to extracting adverb-adjective-nounphrases based on clause structure obtained byparsing sentences into a hierarchical represen-tation.
We also propose a robust general solu-tion for modeling the contribution of adver-bials and negation to the score for degree ofsentiment.
In an application involving extract-ing aspect-based pros and cons from restaurantreviews, we obtained a 45% relative improve-ment in recall through the use of parsing me-thods, while also improving precision.1 IntroductionOnline product reviews have provided an exten-sive collection of free-style texts as well as prod-uct ratings prepared by general users, which inreturn provide grassroots contributions to usersinterested in a particular product or service asassistance.
Yet, valuable as they are, free-stylereviews contain much noisy data and are tediousto read through in order to reach an overall con-clusion.
Thus, we conducted this study to auto-matically process and evaluate product reviewsin order to generate both numerical evaluationand textual summaries of users?
opinions, withthe ultimate goal of adding value to real systemssuch as a restaurant-guide dialogue system.Sentiment summarization has been well stu-died in the past decade (Turney, 2002; Pang et al,2002; Dave et al, 2003; Hu and Liu, 2004a,2004b; Carenini et al, 2006; Liu et al, 2007).The polarity of users?
sentiments in each seg-ment of review texts is extracted, and the polari-ties of individual sentiments are aggregatedamong all the sentences/segments of texts to givea numerical scaling on sentiment orientation.Most of the work done for sentiment analysisso far has employed shallow parsing featuressuch as part-of-speech tagging.
Frequent adjec-tives and nouns/noun phrases are extracted asopinion words and representative product fea-tures.
However, the linguistic structure of thesentence is usually not taken into consideration.High level linguistic features, if well utilized andaccurately extracted, can provide much insightinto the semantic meaning of user opinions andcontribute to the task of sentiment identification.Furthermore, in addition to adjectives andnouns, adverbials and negation also play an im-portant role in determining the degree of theorientation level.
For example, ?very good?
and?good?
certainly express different degrees ofpositive sentiment.
Also, in previous studies,when negative expressions are identified, thepolarity of sentiment in the associated segmentof text is simply reversed.
However, semanticexpressions are quite different from the absoluteopposite values in mathematics.
For example,?not bad?
does not express the opposite meaningof ?bad?, which would be highly positive.
Simp-ly reversing the polarity of sentiment on the ap-pearance of negations may result in inaccurateinterpretation of sentiment expressions.
Thus, asystem which attempts to quantify sentimentwhile ignoring adverbials is missing a significantcomponent of the sentiment score, especially ifthe adverbial is a negative word.161Another challenging aspect of negation isproper scoping of the negative reference over theright constituent, which we argue, can be han-dled quite well with careful linguistic analysis.Take the sentence ?I don?t think the place is veryclean?
as example.
A linguistic approach asso-ciating long-distance elements with semanticrelations can identify that the negation ?not?scopes over the complement clause, thus extract-ing ?not very clean?
instead of ?very clean?.Our goal in modeling adverbials is to investi-gate whether a simple linear correction modelcan capture the polarity contribution of all ad-verbials.
Furthermore, is it also appropriate toadjust for multiple adverbs, including negation,via a linear additive model?
I.e., can ?not verygood?
be modeled as not(very(good))?
The factthat ?not very good?
seems to be less negativethan ?not good?
suggests that such an algorithmmight work well.
From these derivations we havedeveloped a model which treats negations in theexact same way as modifying adverbs, via anaccumulative linear offset model.
This yields avery generic and straightforward solution tomodeling the strength of sentiment expression.In this paper we utilize a parse-and-paraphraseparadigm to identify semantically related phrasesin review texts, taking quantifiers (e.g., modify-ing adverbs) and qualifiers (e.g., negations) intospecial consideration.
The approach makes useof a lexicalized probabilistic syntactic grammarto identify and extract sets of adverb-adjective-noun phrases that match review-related patterns.Such patterns are constructed based on well-formed linguistic structure; thus, relevant phrasescan be extracted reliably.We also propose a cumulative linear offsetmodel to calculate the degree of sentiment forjoint adjectives and quantifiers/qualifiers.
Theproposed sentiment prediction model takes mod-ifying adverbs and negations as universal scaleson strength of sentiment, and conducts cumula-tive calculation on the degree of sentiment forthe associated adjective.
With this model, we canprovide not only qualitative textual summariza-tion such as ?good food?
and ?bad service?, butalso a numerical scoring of sentiment, i.e., ?howgood the food is?
and ?how bad the service is.
?2 Related WorkThere have been many studies on sentimentclassification and opinion summarization (Pangand Lee, 2004, 2005; Gamon et al, 2005; Popes-cu and Etzioni, 2005; Liu et al, 2005; Zhuang etal., 2006; Kim and Hovy, 2006).
Specifically,aspect rating as an interesting topic has also beenwidely studied (Titov and McDonald, 2008a;Snyder and Barzilay, 2007; Goldberg and Zhu,2006).
Recently, Baccianella et.
al.
(2009)conducted a study on multi-facet rating ofproduct reviews with special emphasis on how togenerate vectorial representations of the text bymeans of POS tagging, sentiment analysis, andfeature selection for ordinal regression learning.Titov and McDonald (2008b) proposed a jointmodel of text and aspect ratings which utilizes amodified LDA topic model to build topics thatare representative of ratable aspects, and builds aset of sentiment predictors.
Branavan et al (2008)proposed a method for leveraging unstructuredannotations in product reviews to infer semanticdocument properties, by clustering userannotations into semantic properties and tyingthe induced clusters to hidden topics in the text.3 System OverviewOur review summarization task is to extract setsof descriptor-topic pairs (e.g., ?excellent service?
)from a set of reviews (e.g., for a particular res-taurant), and to cluster the extracted phrases intorepresentative aspects on a set of dimensions(e.g., ?food?, ?service?
and ?atmosphere?).
Dri-ven by this motivation, we propose a three-stagesystem that automatically processes reviews.
Ablock diagram is given in Figure 1.Figure 1.
Framework of review processing.The first stage is sentence-level data filtering.Review data published by general users is oftenin free-style, and a large fraction of the data iseither ill-formed or not relevant to the task.
Weclassify these as out of domain sentences.
To fil-ter out such noisy data, we collect unigram statis-tics on all the relevant words in the corpus, andselect high frequency adjectives and nouns.
Anysentence that contains none of the high-frequency nouns or adjectives is rejected fromfurther analysis.
The remaining in-domain sen-tences are subjected to the second stage, parse162analysis and semantic understanding, for topicextraction.From the parsable sentences we extract de-scriptor-topic phrase patterns based on a careful-ly-designed generation grammar.
We then applyLM (language model) based topic clustering togroup the extracted phrases into representativeaspects.
The third stage scores the degree of sen-timent for adjectives, as well as the strength ofsentiment for modifying adverbs and negations,which further refine the degree of sentiment ofthe associated adjectives.
We then run a linearadditive model to assign a combined sentimentscore for each extracted phrase.The rest of the paper is structured as follows:In Section 4, we explain the linguistic analysis.In Section 5, we describe the cumulative modelfor assessing the degree of sentiment.
Section 6provides a systematic evaluation, conducted onreal data in the restaurant review domain har-vested from the Web.
Section 7 provides a dis-cussion analyzing the results.
Section 8 summa-rizes the paper as well as pointing to future work.4 Linguistic Analysis4.1 Parse-and-ParaphraseOur linguistic analysis is based on a parse-and-paraphrase paradigm.
Instead of the flat structureof a surface string, the parser provides a hierar-chical representation, which we call a linguisticframe (Xu et al, 2008).
It preserves linguisticstructure by encoding different layers of seman-tic dependencies.
The grammar captures syntac-tic structure through a set of carefully con-structed context free grammar rules, and employsa feature-passing mechanism to enforce long dis-tance constraints.
The grammar is lexicalized,and uses a statistical model to rank order compet-ing hypotheses.
It knows explicitly about 9,000words, with all unknown words being interpretedas nouns.
The grammar probability model wastrained automatically on the corpus of reviewsentences.To produce the phrases, a set of generationrules is carefully constructed to only extract setsof related adverbs, adjectives and nouns.
Theadjective-noun relationships are captured fromthe following linguistic patterns: (1) all adjec-tives attached directly to a noun in a noun phrase,(2) adjectives embedded in a relative clausemodifying a noun, and (3) adjectives related tonouns in a subject-predicate relationship in aclause.
These patterns are compatible, i.e., if aclause contains both a modifying adjective and apredicate adjective related to the same noun, twoadjective-noun pairs are generated by differentpatterns.
As in, ?The efficient waitress was none-theless very courteous.?
It is a ?parse-and-paraphrase-like?
paradigm: the paraphrase triesto preserve the original words intact, while reor-dering them and/or duplicating them into mul-tiple NP units.
Since they are based on syntacticstructure, the generation rules can also be appliedin any other domain involving opinion mining.An example linguistic frame is shown in Fig-ure 2, which encodes the sentence ?The caesarwith salmon or chicken is really quite good.?
Inthis example, for the adjective ?good?, the near-by noun ?chicken?
would be associated with it ifonly proximity is considered.
From the linguisticframe, however, we can easily associate ?caesar?with ?good?
by extracting the head of the topicsub-frame and the head of the predicate sub-frame, which are encoded in the same layer (rootlayer) of the linguistic frame.
Also, we can tellfrom the predicate sub-frame that there is an ad-verb ?quite?
modifying the head word ?good?.The linguistic frame also encodes an adverb ?re-ally?
in the upstairs layer.
A well-constructedgeneration grammar can create customized ad-verb-adjective-noun phrases such as ?quite goodcaesar?
or ?really quite good caesar?.
{c cstatement:topic {q caesar:quantifier "def":pred {p with :topic {q salmon:pred {p conjunction:or {q chicken  }}}}:adv "really":pred {p adj_complement:pred {p adjective:adv "quite":pred {p quality :topic "good" }}}}Figure 2.
Linguistic frame for ?The caesar withsalmon or chicken is really quite good.
?Interpreting negation in English is notstraightforward, and it is often impossible to docorrectly without a deep linguistic analysis.
Xu-ehui Wu (2005) wrote: ?The scope of negation isa complex linguistic phenomenon.
It is easy toperceive but hard to be defined from a syntacticpoint of view.
Misunderstanding or ambiguitymay occur when the negative scope is not un-derstood clearly and correctly.?
The majorityrule for negation is that it scopes over the re-mainder of its containing clause, and this workswell for most cases.
For example, Figure 3 shows163the linguistic frame for the sentence ?Their menuwas a good one that didn?t try to do too much.?
{c cstatement:topic {q menu   :poss "their" } }:complement {q pronoun   :name ?one?
:adj_clause {c cstatement:conjn "that":negate "not":pred {p try :to_clause  {p do:topic {q object:adv "too":quant "much" }}}}:pred {p adjective:pred {p quality :topic "good" }}}Figure 3.
Linguistic frame for ?Their menu was agood one that didn?t try to do too much.
?Traditional approaches which do not considerthe linguistic structure would treat the appear-ance of ?not?
as a negation and simply reversethe sentiment of the sentence to negative polarity,which is wrong as the sentence actually ex-presses positive opinion for the topic ?menu?.
Inour approach, the negation ?not?
is identified asunder the sub-frame of the complement clause,instead of in the same or higher layer of the ad-jective sub-frame; thus it is considered as unre-lated to the adjective ?good?.
In this way we cansuccessfully predict the scope of the reference ofthe negation over the correct constituent of a sen-tence and create proper association between ne-gation and its modified words.4.2 LM-based Topic ClusteringTo categorize the extracted phrases into repre-sentative aspects, we automatically group theidentified topics into a set of clusters based onLM probabilities.
The LM-based algorithm as-sumes that topics which are semantically relatedhave high probability of co-occurring with simi-lar descriptive words.
For example, ?delicious?might co-occur frequently with both ?pizza?
and?dessert?.
By examining the distribution of bi-gram probability of these topics with correspond-ing descriptive words, we can group ?pizza?
and?dessert?
into the same cluster of ?food?.We select a small set of the most common top-ics, i.e., topics with the highest frequency counts,and put them into an initial set I.
Then, for eachcandidate topic  outside set I, we calculate itsprobability given each topic  within the initialset I, given by:| ?
|?
|?
,?
,??
, ?
,         (1)where A represents the set of all the adjectives inthe corpus.
For each candidate topic  , wechoose the cluster of the initial topic   withwhich it has the highest probability score.There are also cases where a meaningful ad-jective occurs in the absence of an associatedtopic, e.g., ?It is quite expensive.?
We call suchcases the ?widow-adjective?
case.
Without hard-coded ontology matching, it is difficult to identi-fy ?expensive?
as a price-related expression.
Todiscover such cases and associate them with re-lated topics, we propose a ?surrogate topic?matching approach based on bigram probability.As aforementioned, the linguistic frame orga-nizes all adjectives into separate clauses.
Thus,we create a ?surrogate topic?
category in the lin-guistic frames for widow-adjective cases, whichmakes it easy to detect descriptors that are affi-liated with uninformative topics like the pronoun?it?.
We then have it generate phrases such as?expensive surrogate_topic?
and use bigramprobability statistics to automatically map eachsufficiently strongly associated adjective to itsmost common topic among our major classes,e.g., mapping ?expensive?
with its surrogate top-ic ?price?.
Therefore, we can generate sets ofadditional phrases in which the topic is ?halluci-nated?
from the widow-adjective.5 Assessment of Sentiment Strength5.1 Problem FormulationGiven the sets of adverb-adjective-noun phrasesextracted by linguistic analysis, our goal is toassign a score for the degree of sentiment to eachphrase and calculate an average rating for eachaspect.
An example summary is given in Table 1.Table 1.
Example of review summary.Aspect Extracted phrases RatingAtmosphere very nice ambiance,outdoor patio 4.8Food not bad meal, quite authentic food 4.1Place not great place,very smoky restaurant 2.8Price so high bill, high cost,not cheap price 2.2To calculate the numerical degree of sentiment,there are three major problems to solve: 1) howto associate numerical scores with textual senti-ment; 2) whether to calculate sentiment scoresfor adjectives and adverbs jointly or separately; 3)164whether to treat negations as special cases or inthe same way as modifying adverbs.There have been studies on building sentimentlexicons to define the strength of sentiment ofwords.
Esuli and Sebastiani (2006) constructed alexical resource, SentiWordNet, a WordNet-likelexicon emphasizing sentiment orientation ofwords and providing numerical scores of howobjective, positive and negative these words are.However, lexicon-based methods can be tediousand inefficient and may not be accurate due tothe complex cross-relations in dictionaries likeWordNet.
Instead, our primary approach to sen-timent scoring is to make use of collective datasuch as user ratings.
In product reviews collectedfrom online forums, the format of a review entryoften consists of three parts: pros/cons, free-styletext and user rating.
We assume that user ratingis normally consistent with the tone of the reviewtext published by the same user.
By associatinguser ratings with each phrase extracted from re-view texts, we can easily associate numericalscores with textual sentiment.A simple strategy of rating assignment is totake each extracted adverb-adjective pair as acomposite unit.
However, this method is likely tolead to a large number of rare combinations, thussuffering from sparse data problems.
Therefore,an interesting question to ask is whether it isfeasible to assign to each adverb a perturbationscore, which adjusts the score of the associatedadjective up or down by a fixed scalar value.This approach thus hypothesizes that ?very ex-pensive?
is as much worse than ?expensive?
as?very romantic?
is better than ?romantic?.
Thisallows us to pool all instances of a given adverbregardless of which adjective it is associated with,in order to compute the absolute value of the per-turbation score for that adverb.
Therefore, weconsider adverbs and adjectives separately whencalculating the sentiment score, treating eachmodifying adverb as a universal quantifier whichconsistently scales up/down the strength of sen-timent for the adjectives it modifies.Furthermore, instead of treating negation as aspecial case, the universal model works for alladverbials.
The model hypothesizes that ?not bad?is as much better than ?bad?
as ?not good?
isworse than ?good?, i.e., negations push posi-tive/negative adjectives to the other side of sen-timent polarity by a universal scale.
This again,allows us to pool all instances of a given nega-tion and compute the absolute value of the per-turbation score for that negation, in the same wayas dealing with modifying adverbs.5.2 Linear Additive ModelFor each adjective, we average all its ratings giv-en by:??
!"?
 (2)where  represents the set of appearances of ad-jective, 	 represents the associated user rat-ing in each appearance of, # represents thenumber of entities (e.g., restaurants) in the entiredata set, and $!  represents the number of entitieswith rating 	.
The score is averaged over all theappearances, weighted by the frequency count ofeach category of rating to remove bias towardsany category.As for adverbs, using a slightly modified ver-sion of equation (2), we can get a rating table forall adverb-adjective pairs.
For each adverb adv,we get a list of all its possible combinations withadjectives.
Then, for each adj in the list, we cal-culate the distance between the rating of adv-adjand the rating of the adj alone.
We then aggre-gate the distances among all the pairs of adv-adjand adj in the list, weighted by the frequencycount of each adv-adj pair:%?
&'()*,)+,?
&'(-)*,)+./.0 ?1?
%,2 (3)where 3$%,represents the count ofthe combination% 2, 4 represents the setof adjectives that co-occur with% ,%,represents the sentiment rating ofthe combination% 2 , and represents the sentiment rating of the adjective  alone.
1represents the polarity of, assigned as 1 if is positive, and -1 ifnegative.Specifically, negations are well handled by thesame scoring strategy, treated exactly the sameas modifying adverbs, except that they get suchstrong negative scores that the sentiment of theassociated adjectives is pushed to the other sideof the polarity scale.After obtaining the strength rating for adverbsand the sentiment rating for adjectives, the nextstep is to assign the strength of sentiment to eachphrase (negation-adverb-adjective-noun) ex-tracted by linguistic analysis, as given by: 5$6-%/7  81?
%8 1?
$6(4)165where represents the rating of adjective, %represents the rating of adverb%,and $6represents the rating of negation $6.1represents the polarity of, assignedas 1 ifis positive, and -1 if negative.
Thus, ifis positive, we assign a combined rating8 %to this phrase.
If it is negative,we assign 2 %.
Specifically, if it isa negation case, we further assign a linear offset$6ifis positive or 2$6ifisnegative.
For example, given the ratings <good:4.5>, <bad: 1.5>, <very: 0.5> and <not: -3.0>,we would assign ?5.0?
to ?very good?
(score(very(good))=4.5+0.5), ?1.0?
to ?very bad?
(score(very(bad))=1.5-0.5), and ?2.0?
to ?notvery good?
(score(not(very(good)))= 4.5+0.5-3.0).
The corresponding sequence of differentdegrees of sentiment is: ?very good: 5.0?
>?good: 4.5?
> ?not very good: 2.0?
> ?bad: 1.5?
>?very bad: 1.0?.6 ExperimentsIn this section we present a systematic evaluationof the proposed approaches conducted on realdata.
We crawled a data collection of 137,569reviews on 24,043 restaurants in 9 cities in theU.S.
from an online restaurant evaluation web-site1.
Most of the reviews have both pros/consand free-style text.
For the purpose of evaluation,we take those reviews containing pros/cons asthe experimental set, which is 72.7% (99,147 re-views) of the original set.6.1 Topic ExtractionBased on the experimental set, we first filteredout-of-domain sentences based on frequencycount, leaving a set of 857,466 in-domain sen-tences (67.5%).
This set was then subjected toparse analysis; 78.6% of them are parsable.Given the parsing results in the format of lin-guistic frame, we used a set of language genera-tion rules to extract relevant adverb-adjective-noun phrases.
We then selected the most frequent6 topics that represented appropriate dimensionsfor the restaurant domain (?place?, ?food?, ?ser-vice?, ?price?, ?atmosphere?
and ?portion?)
asthe initial set, and clustered the extracted topicmentions into different aspect categories bycreating a set of topic mappings with the LM-based clustering method.
Phrases not belongingto any category are filtered out.1http://www.citysearch.comTo evaluate the performance of the proposedapproach (LING) to topic extraction, we com-pare it with a baseline method similar to (Hu andLiu, 2004a, 2004b; Liu et al, 2005).
We per-formed part-of-speech tagging on both parsableand unparsable sentences, extracted each pair ofnoun and adjective that has the smallest proximi-ty, and filtered out those with low frequencycounts.
Adverbs and negation words that are ad-jacent to the identified adjectives were also ex-tracted along with the adjective-noun pairs.
Wecall this the ?neighbor baseline?
(NB).The proposed method is unable to make use ofthe non-parsable sentences, which make up over20% of the data.
Hence, it seems plausible toutilize a back-off mechanism for these sentencesvia a combined system (COMB) incorporatingNB only for the sentences that fail to parse.In considering how to construct the ?groundtruth?
set of pros and cons for particular aspects,our goal was to minimize error as much as possi-ble without requiring exorbitant amounts of ma-nual labeling.
We also wanted to assure that themethods were equally fair to both systems(LING and NB).
To these ends, we decided topool together all of the topic mappings and sur-rogate topic hallucinations obtained automatical-ly from both systems, and then to manually editthe resulting list to eliminate any that weredeemed unreasonable.
We then applied theseedited mappings in an automatic procedure to theadjective-noun pairs in the user-provided prosand cons of all the restaurant reviews.
The result-ing aspect-categorized phrase lists are taken asthe ground truth.
Each system then used its own(unedited) set of mappings in processing the as-sociated review texts.We also needed an algorithm to decide on aparticular set of reviews for consideration, again,with the goal of omitting bias towards either ofthe two systems.
We decided to retain as theevaluation set al reviews which obtained at leastone topic extraction from both systems.
Thus thetwo systems processed exactly the same datawith exactly the same definitions of ?groundtruth?.
Performance was evaluated on this set of62,588 reviews in terms of recall (percentage oftopics in the ground truth that are also identifiedfrom the review body) and precision (percentageof extracted topics that are also in the groundtruth).
These measures are computed separatelyfor each review, and then averaged over all re-views.As shown in Table 2, without clustering, theLING approach gets 4.6% higher recall than the166NB baseline.
And the recall from the COMB ap-proach is 3.9% higher than that from the LINGapproach and 8.5% higher than that from the NBbaseline.
With topic clustering, the COMB ap-proach also gets the highest recall, with a 4.9%and 17.5% increase from the LING approach andthe NB baseline respectively.
The precision isquite close among the different approaches,around 60%.
Table 2 also shows that the topicclustering approach increases the recall by 4.8%for the NB baseline, 12.8% for the LING ap-proach, and 13.8% for the COMB approach.Table 2.
Experimental results of topic extraction bythe NB baseline, the proposed LING approach anda combined system (COMB).No ClusteringNB LING COMBRecall 39.6% 44.2% 48.1%Precision 60.2% 60.0% 59.8%With ClusteringNB LING COMBRecall 44.4% 57.0% 61.9%Precision 56.8% 61.1% 60.8%6.2 Sentiment ScoringTo score the degree of sentiment for each ex-tracted phrase, we built a table of sentiment score(<adjective: score>) for adjectives and a table ofstrength score (<adverb: score>) for adverbs.The pros/cons often contain short and well-structured phrases, and have better parsing quali-ty than the long and complex sentences in free-style texts; pros/cons also have clear sentimentorientations.
Thus, we use pros/cons to score thesentiment of adjectives, which requires strongpolarity association.
To obtain reliable ratings,we associate the adjectives in the ?pros?
of re-view entries which have a user rating 4 or 5, andassociate the adjectives in the ?cons?
of reviewentries with user ratings 1 or 2 (the scale of userrating is 1 to 5).
Reviews with rating 3 are on theboundary of sentiment, so we associate bothsides with the overall rating.
On the other hand,the frequencies of adverbs in free-style texts aremuch higher than those in pros/cons, aspros/cons mostly contain adjective-noun patterns.Thus, we use free-style texts instead of pros/consto score the strength of adverbs.Partial results of the sentiment scoring areshown in Tables 3 and 4.
As shown in Table 3,the polarity of sentiment as well as the degree ofpolarity of an adjective can be distinguished byits score.
The higher the sentiment score is, themore positive the adjective is.Table 3.
Sentiment scoring for selected adjectives.Adjective Rating Adjective RatingExcellent  5.0 Awesome  4.8Easy  4.1 Great  4.4Good  3.9 Limited  3.4Inattentive  2.75 Overpriced  2.3Rude  1.69 Horrible  1.3Table 4 gives the scores of strength for mostcommon adverbs.
The higher the strength scoreis, the more the adverb scales up/down the de-gree of sentiment of the adjective it modifies.While ?not?
gets a strong negative score, someadverbs such as ?a little?
(-0.65) and ?a bit?
(-0.83) also get negative scores, indicating slightlyless sentiment for the associated adjectives.Table 4.
Strength scoring for selected adverbs.Adverb Rating Adverb RatingSuper  0.58 Fairly  0.13Extremely  0.54 Pretty 0.07Incredibly  0.49 A little  -0.65Very 0.44 A bit -0.83Really  0.39 Not -3.10To evaluate the performance of sentimentscoring, we randomly selected a subset of 1,000adjective-noun phrases and asked two annotatorsto independently rate the sentiment of eachphrase on a scale of 1 to 5.
We compared thesentiment scoring between our system and theannotations in a measurement of mean distance:9$   |:|?
|	;;<: 2 ;|      (5)where   represents the set of phrases, =represents each phrase in the set , 	; representsthe rating on phrase = from our sentiment scor-ing system, and ; represents the annotated rat-ing on phrase =.
As shown in Table 5, the ob-tained mean distance between the scoring fromour approach and that from each annotation set is0.46 and 0.43 respectively, based on the absoluterating scale from 1 to 5.
This shows that the scor-ing of sentiment from our system is quite close tohuman annotation.
The kappa agreement be-tween the two annotation sets is 0.68, indicatinghigh consistency between the annotators.
Thereliability of these results gives us sufficient con-fidence to make use of the scores of sentimentsfor summarization.To examine the prediction of sentiment polari-ty, for each annotation set, we pooled the phraseswith rating 4/5 into ?positive?, rating 1/2 into?negative?, and rating 3 into ?neutral?.
Then werounded up the sentiment scores from our systemto integers and pooled the scores into three polar-167ity sets (?positive?, ?negative?
and ?neutral?)
inthe same way.
As shown in Table 5, the obtainedkappa agreement between the result from oursystem and that from each annotation set is 0.55and 0.60 respectively.
This shows reasonablyhigh agreement on the polarity of sentiment be-tween our system and human evaluation.Table 5.
Comparison of sentiment scoring betweenthe proposed approach and two annotation sets.Annotation 1 Annotation 2Mean distance 0.46 0.43Kappa agreement 0.55 0.60Table 6.
Experimental results of topic extractionbased on sentiment polarity matching.No ClusteringNB LING COMBRecall 34.5% 38.9% 42.2%Precision 53.8% 54.0% 53.3%With ClusteringNB LING COMBRecall 37.4% 49.7% 54.1%Precision 48.5% 52.9% 51.4%To evaluate the combination of topic extrac-tion and sentiment identification, we repeated thetopic extraction experiments presented in Table 2,but this time requiring as well a correct polarityassignment to obtain a match with the pros/consground truth.
As shown in Table 6, the COMBapproach gets the highest recall both with andwithout topic clustering, and the recall from theLING approach is higher than that from the NBbaseline in both cases as well, indicating the su-periority of the proposed approach.
The precisionis stable among the different approaches, consis-tent with the case without the consideration ofsentiment polarity.7 DiscussionIt is surprising that the parse-and-paraphrase me-thod performs so well, despite the fact that it uti-lizes less than 80% of the data (parsable set).
Inthis section, we will discuss two experiments thatwere done to tease apart the contributions of dif-ferent variables.
In both experiments, we com-pared the change in relative improvement in re-call between NB and LING, relative to the valuesin Table 6, in the with-clustering condition.
Inthe table, LING obtains a score of 49.7% for re-call, which is a 33% relative increase from thescore for NB (37.4%).
Three distinct factorscould play a role in the improvement: the widow-adjective topic hallucinations, the topic mappingfor clustering, and the extracted phrases them-selves.
An experiment involving omitting topichallucinations from widow adjectives determinedthat these account for 12% of the relative in-crease.
To evaluate the contribution of clustering,we replaced the mapping tables used by both sys-tems with the edited one used by the ground truthcomputation.
Thus, both systems made use of thesame mapping table, removing this variable fromconsideration.
This improved the performance ofboth systems (NB and LING), but resulted in adecrease of LING?s relative improvement by17%.
This implies that LING?s mapping table issuperior.
Since both systems use the same senti-ment scores for adjectives and adverbs, the re-mainder of the difference (71%) must be duesimply to higher quality extracted phrases.We suspected that over-generated phrases (the40% of phrases that find no mappings in thepros/cons) might not really be a problem.
To testthis hypothesis, we selected 100 reviews for theirhigh density of extracted phrases, and manuallyevaluated all the over-generated phrases.
Wefound that over 80% were well formed, correct,and informative.
Therefore, a lower precisionhere does not necessarily mean poor performance,but instead shows that the pros/cons provided byusers are often incomplete.
By extracting sum-maries from review texts we can recover addi-tional valuable information.8 Conclusions & Future WorkThis paper presents a parse-and-paraphrase ap-proach to assessing the degree of sentiment forproduct reviews.
A general purpose context freegrammar is employed to parse review sentences,and semantic understanding methods are devel-oped to extract representative negation-adverb-adjective-noun phrases based on well-definedsemantic rules.
A language modeling-based me-thod is proposed to cluster topics into respectivecategories.
We also introduced in this paper acumulative linear offset model for supporting theassessment of the strength of sentiment in adjec-tives and quantifiers/qualifiers (including nega-tions) on a numerical scale.
We demonstratedthat the parse-and-paraphrase method can per-form substantially better than a neighbor baselineon topic extraction from reviews even with lessdata.
The future work focuses in two directions:(1) building a relational database from the sum-maries and ratings and using it to enhance users?experiences in a multimodal spoken dialoguesystem; and (2) applying our techniques to otherdomains to demonstrate generality.168ReferencesStefano Baccianella, Andrea Esuli, and Fabrizio Se-bastiani.
2009.
Multi-facet Rating of Product Re-views.
In Proceedings of European Conference onInformation Retrieval.S.R.K.
Branavan, Harr Chen, Jacob Eisenstein, andRegina Barzilay.
2008.
Learning document-levelsemantic properties from free-text annotations.
InProceedings of the Annual Conference of the As-sociation for Computational Linguistics.Giuseppe Carenini, Raymond Ng, and Adam Pauls2006.
Multi-Document Summarization of Evalua-tive Text.
In Proceedings of the Conference of theEuropean Chapter of the Association for Computa-tional Linguistics.Kushal Dave, Steve Lawrence, and David M. Pen-nock.
2003.
Mining the peanut gallery: opinion ex-traction and semantic classification of product re-views.
In Proceedings of the International Confe-rence on World Wide Web.Andrea Esuli and Fabrizio Sebastiani.
2006.
SENTI-WORDNET: A Publicly Available Lexical Re-source for Opinion Mining.
In Proceedings of the5th Conference on Language Resources and Evalu-ation.Michael Gamon, Anthony Aue, Simon Corston-Oliver,and Eric Ringger .
2005.
Pulse: Mining customeropinions from free text.
In Proceedings of the 6thInternational Symposium on Intelligent Data Anal-ysis.Andrew Goldberg and Xiaojin Zhu.
2006.
Seeingstars when there aren't many stars: Graph-basedsemi-supervised learning for sentiment categoriza-tion.
In HLT-NAACL 2006 Workshop on Text-graphs: Graph-based Algorithms for Natural Lan-guage Processing.Minqing Hu and Bing Liu.
2004a.
Mining and sum-marizing customer reviews.
In Proceedings of the2004 ACM SIGKDD international conference onKnowledge Discovery and Data mining.Minqing Hu and Bing Liu.
2004b.
Mining OpinionFeatures in Customer Reviews.
In Proceedings ofNineteenth National Conference on Artificial Intel-ligence.Soo-Min Kim and Eduard Hovy.
2006.
Automaticidentification of pro and con reasons in online re-views.
In Proceedings of the COLING/ACL 2006Main Conference Poster Sessions, pages 483?490.Bing Liu, Minqing Hu, and Junsheng Cheng.
2005.Opinion observer: analyzing and comparing opi-nions on the web.
In Proceedings of InternationalConference on World Wide Web.Jingjing Liu, Yunbo Cao, Chin-Yew Lin, YalouHuang, and Ming Zhou.
2007.
Low-Quality Prod-uct Review Detection in Opinion Summarization.In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing.Bo Pang and Lillian Lee.
2004.
A sentimental educa-tion: Sentiment analysis using subjectivity summa-rization based on minimum cuts.
In Proceedings ofthe Annual Conference of the Association forComputational Linguistics.Bo Pang and Lillian Lee.
2005.
Seeing stars: Exploit-ing class relationships for sentiment categorizationwith respect to rating scales.
In Proceedings of theAnnual Conference of the Association for Compu-tational Linguistics.Bo Pang, Lillian Lee, and S. Vaithyanathan.
2002.Thumbs up?
Sentiment classification using ma-chine learning techniques.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing.A.M.
Popescu and O. Etzioni.
2005.
Extracting prod-uct features and opinions from reviews.
In Pro-ceedings of the Conference on Empirical Methodsin Natural Language Processing.Benjamin Snyder and Regina Barzilay.
2007.
Mul-tiple Aspect Ranking using the Good Grief Algo-rithm.
In Proceedings of the Joint Conference ofthe North American Chapter of the Association forComputational Linguistics and Human LanguageTechnologies.Ivan Titov and Ryan McDonald.
2008a.
Modelingonline reviews with multi-grain topic models.
InProceedings of the 17h International Conference onWorld Wide Web.Ivan Titov and Ryan McDonald.
2008b.
A Joint Mod-el of Text and Aspect Ratings for Sentiment Sum-marization.
In Proceedings of the Annual Confe-rence of the Association for Computational Lin-guistics.Peter D. Turney.
2002.
Thumbs up or thumbs down?Sentiment orientation applied to unsupervised clas-sification of reviews.
In Proceedings of the AnnualConference of the Association for ComputationalLinguistics.Xuehui Wu, 2005.
On the Scope of Negation in Eng-lish, Sino-US English Teaching, Vol.
2, No.
9, Sep.2005.
pp.
53-56.Yushi Xu, Jingjing Liu, Stephanie Seneff.
2008.Mandarin Language Understanding in DialogueContext.
In Proceedings of International Sympo-sium on Chinese Spoken Language Processing.Li Zhuang, Feng Jing, and Xiao-Yan Zhu.
2006.Movie review mining and summarization.
In Pro-ceedings of the 15th ACM international conferenceon Information and knowledge management.169
