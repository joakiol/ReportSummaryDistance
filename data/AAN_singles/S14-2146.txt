Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 823?827,Dublin, Ireland, August 23-24, 2014.UWM: Applying an Existing Trainable Semantic Parser to Parse RoboticSpatial CommandsRohit J. KateUniversity of Wisconsin-MilwaukeeMilwaukee, WIkaterj@uwm.eduAbstractThis paper describes Team UWM?s sys-tem for the Task 6 of SemEval 2014for doing supervised semantic parsing ofrobotic spatial commands.
An existingsemantic parser, KRISP, was trained us-ing the provided training data of naturallanguage robotic spatial commands pairedwith their meaning representations in theformal robot command language.
The en-tire process required very little manual ef-fort.
Without using the additional annota-tions of word-aligned semantic trees, thetrained parser was able to exactly parsenew commands into their meaning repre-sentations with 51.18% best F-measure at72.67% precision and 39.49% recall.
Re-sults show that the parser was particularlyaccurate for short sentences.1 IntroductionSemantic parsing is the task of converting natu-ral language utterances into their complete formalmeaning representations which are executable forsome application.
Example applications of seman-tic parsing include giving natural language com-mands to robots and querying databases in natu-ral language.
Some old semantic parsers were de-veloped manually to work for specific applications(Woods, 1977; Warren and Pereira, 1982).
How-ever, such semantic parsers were generally brittleand building them required a lot of manual effort.In addition, these parsers could not be ported toany other application without again putting signif-icant manual effort.More recently, several semantic parsers havebeen developed using machine learning (Zelle andThis work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/Mooney, 1996; Ge and Mooney, 2005; Zettle-moyer and Collins, 2005; Wong and Mooney,2006; Kate and Mooney, 2006; Lu et al., 2008;Kwiatkowski et al., 2011).
In this approach, train-ing data is first created for the domain of inter-est.
Then using one of the many machine learn-ing methods and semantic parsing frameworks, asemantic parser is automatically learned from thetraining data (Mooney, 2007).
The trained seman-tic parser is then capable of parsing new natu-ral language utterances into their meaning repre-sentations.
Semantic parsers built using machinelearning tend to be more robust and can be easilyported to other application domains with appropri-ate domain-specific training data.The Task 6 of SemEval 2014 provided a new ap-plication domain for semantic parsing along withtraining and test data.
The domain involved giv-ing natural language commands to a robotic armwhich would then move blocks on a board (Dukes,2013).
The domain was inspired from the classicAI system SHRDLU (Winograd, 1972).
The train-ing data contained 2500 examples of sentencespaired with their meaning representations in theRobot Command Language (RCL) which was de-signed for this domain (Dukes, 2013).
The testdata contained 909 such example pairs.We trained an existing and freely available1se-mantic parser KRISP (Kate and Mooney, 2006)using the training data for this domain.
Besideschanging the format of the data for running KRISPand writing a context-free grammar for the mean-ing representation language RCL, the entire pro-cess required minimal manual effort.
The authorspent less than a week?s time for participating inthe Task 6, and most of it was spent in runningthe experiments.
This demonstrates that train-able semantic parsers like KRISP can be rapidlyadopted to new domains.
In the Results sectionwe show different precisions and recalls it ob-1http://www.cs.utexas.edu/users/ml/krisp/823tained at different confidence levels in the form ofa precision-recall curve.
The results also show thatthe parser was particularly accurate on shorter sen-tences.
Two major reasons that prevented KRISPfrom performing better on this domain were - itshigh computational demand for memory whichprevented it from being trained beyond 1500 train-ing examples, and some variability in the mean-ing representation language RCL that negativelyaffected training as well as evaluation.2 Background: KRISP Semantic ParserKRISP (Kernel-based Robust Interpretation for Se-mantic Parsing) is a trainable semantic parser(Kate and Mooney, 2006) that uses Support VectorMachines (SVMs) (Cristianini and Shawe-Taylor,2000) as the machine learning method with string-subsequence kernel (Lodhi et al., 2002).
It takesnatural language utterances and their correspond-ing formal meaning representation as the trainingdata along with the context-free grammar of themeaning representation language (MRL).
The keyidea in KRISP is that every production of the MRLis treated as a semantic concept.
For every MRLproduction, an SVM classifier is trained so that itcan give for any input natural language substringof words the probability that it expresses the corre-sponding semantic concept.
Once these classifiersare trained, parsing a sentence reduces to findingthe most probable semantic derivation of the sen-tence in which different productions cover differ-ent parts of the sentence and together form a com-plete meaning representation.
Figure 1 shows anexample semantic derivation of a robotic spatialcommand.
Productions of RCL grammar (Table 1)are shown at tree nodes depicting different parts ofthe sentence they cover.Since the training data is not in the form of suchsemantic derivations, an EM-like iterative algo-rithm is used to collect appropriate positive andnegative examples in order to train the classifiers(Kate and Mooney, 2006).
Positive examples arecollected from correct semantic derivations de-rived by the parser learned in the previous itera-tion, and negative examples are collected from theincorrect semantic derivations.KRISP was shown to work well on the US geog-raphy database query domain (Tang and Mooney,2001) as well as on the RoboCup Coach Lan-guage (CLang) domain (Kate et al., 2005).
It wasalso shown to be particularly robust to noise inFigure 1: Semantic derivation of the robotic spatial com-mand ?pick up the turquoise pyramid?
obtained by KRISPduring testing which gives the correct RCL representation(event: (action: take) (entity: (color: cyan) (type: prism))).the natural language utterances (Kate and Mooney,2006).
KRISP was later extended to do semi-supervised semantic parsing (Kate and Mooney,2007b), to learn from ambiguous supervision inwhich multiple sentences could be paired with asingle meaning representation in the training data(Kate and Mooney, 2007a), and to transform theMRL grammar to improve semantic parsing (Kate,2008).3 MethodsIn order to apply KRISP to the Task 6 of SemEval2014, the format of the provided data was firstchanged to the XML-type format that KRISP ac-cepts.
The data contained several instances ofco-references which was also part of RCL, butKRISP was not designed to handle co-referencesand expects them to be pre-resolved.
We ob-served that almost all co-references in the mean-ing representations, indicated by ?reference-id?token, resolved to the first occurrence of an ?en-tity?
element in the meaning representation.
Thiswas found to be true for more than 99% of thecases.
We used this observation to resolve co-references during semantic parsing in the follow-ing way.
As a pre-processing step, we first removefrom the meaning representations all the ?id:?
to-kens (these resolve the references) but keep the?reference-id:?
tokens (these encode presence ofco-references).
The natural language sentencesare not modified in any way and the parser learnsfrom the training data to relate words like ?it?and ?one?
to the RCL token ?reference-id?.
AfterKRISP generates a meaning representation duringtesting, as a post-processing step, ?id: 1?
is addedto the first ?entity?
element in the meaning repre-sentation if it contains the ?reference-id:?
token.The context-free grammar for RCL was not pro-vided by the Task organizers.
There are multi-824ple ways to write a context-free grammar for ameaning representation language and those thatconform better to natural language work betterfor semantic parsing (Kate, 2008).
We manu-ally wrote grammar for RCL which mostly fol-lowed the structure of the meaning representa-tions as they already conformed highly to naturallanguage commands and hence writing the gram-mar was straightforward.
KRISP runs faster ifthere are fewer non-terminals on the right-hand-side (RHS) of the grammar because that makesthe search for the most probable semantic deriva-tion faster.
Hence we kept non-terminals on RHSas few as possible while writing the grammar.Table 1 shows the entire grammar for RCL thatwe wrote which was given to KRISP.
The non-terminals are indicated with a ?*?
in their front.We point out that KRISP needs grammar only forthe meaning representation language (an applica-tion will need it anyway if the statements are to beexecuted) and not for the natural language.KRISP?s training algorithm could be aided byproviding it with information about which natu-ral language words are usually used to express theconcept of a production.
For example, word ?red?usually expresses ?
*color: ?
( color: red )?.
Thedata provided with the Task 6 came with the word-aligned semantic trees which indicated which nat-ural language words corresponded to which mean-ing representation components.
This informationcould have been used to aid KRISP, however, wefound many inconsistencies and errors in the pro-vided word-aligned semantic trees and chose notto use them.
In addition, KRISP seemed to learnmost of that information on its own anyway.The Task 6 also included integrating semanticparsing with spatial planning.
This meant that ifthe semantic parser generates an RCL representa-tion that does not make sense for the given blockconfiguration on the board, then it could be dis-missed and the next best RCL representation couldbe considered.
Besides generating the best mean-ing representation for a natural language utterance,KRISP is also capable of generating multiple pos-sible meaning representations sorted by their prob-abilities.
We could have used this capability tooutput only the best RCL representation that isvalid for the given board configuration.
Unfortu-nately, unfamiliarity with the provided Java APIfor the spatial planner and lack of time preventedus from doing this.
*action: ?
( action: move )*action: ?
( action: drop )*action: ?
( action: take )*cardinal: ?
( cardinal: 1 )*cardinal: ?
( cardinal: 2 )*cardinal: ?
( cardinal: 3 )*cardinal: ?
( cardinal: 4 )*color: ?
( color: magenta )*color: ?
( color: red )*color: ?
( color: white )*color: ?
( color: cyan )*color: ?
( color: green )*color: ?
( color: yellow )*color: ?
( color: blue )*color: ?
( color: gray )*indicator: ?
( indicator: rightmost )*indicator: ?
( indicator: back )*indicator: ?
( indicator: center )*indicator: ?
( indicator: right )*indicator: ?
( indicator: leftmost )*indicator: ?
( indicator: individual )*indicator: ?
( indicator: nearest )*indicator: ?
( indicator: front )*indicator: ?
( indicator: left )*reference-id: ?
( reference-id: 1 )*relation: ?
( relation: right )*relation: ?
( relation: forward )*relation: ?
( relation: within )*relation: ?
( relation: above )*relation: ?
( relation: nearest )*relation: ?
( relation: adjacent )*relation: ?
( relation: front )*relation: ?
( relation: left )*relation: ?
( relation: backward )*type: ?
( type: type-reference-group )*type: ?
( type: board )*type: ?
( type: prism )*type: ?
( type: cube )*type: ?
( type: type-reference )*type: ?
( type: cube-group )*type: ?
( type: corner )*type: ?
( type: robot )*type: ?
( type: stack )*type: ?
( type: edge )*type: ?
( type: region )*type: ?
( type: tile )*type: ?
( type: reference )*indicator: ?
*indicator: *indicator:*color: ?
*color: *color:*ct: ?
*color: *type:*ict: ?
*indicator: *ct:*ctr: ?
*ct: *reference-id:*cct: ?
*cardinal: *ct:*ed: ?
*entity: ( destination: *spatial-relation: )*entity: ?
( entity: *type: )*entity: ?
( entity: *type: *reference-id: )*entity: ?
( entity: *type: *spatial-relation: )*entity: ?
( entity: *ct: )*entity: ?
( entity: *indicator: *type: )*entity: ?
( entity: *ict: )*entity: ?
( entity: *ict: *spatial-relation: )*entity: ?
( entity: *cardinal: *type: )*entity: ?
( entity: *cct: )*entity: ?
( entity: *cct: *spatial-relation: )*entity: ?
( entity: *ctr: )*entity: ?
( entity: *ct: *spatial-relation: )*entity: ?
( entity: *ctr: *spatial-relation: )*measure: ?
( measure: *entity: )*mr: ?
*measure: *relation:*spatial-relation: ?
( spatial-relation: *relation: *entity: )*spatial-relation: ?
( spatial-relation: *mr: )*spatial-relation: ?
( spatial-relation: *mr: *entity: )*S?
( sequence: *S *S )*S?
( event: *action: *ed: )*S?
( event: *action: *entity: )Table 1: Grammar for the Robot Command Lan-guage (RCL) given to KRISP for semantic parsing.The non-terminals are indicated with a ?*?
in theirfront.
The start symbol is *S.82501020304050607080901000  5  10  15  20  25  30  35  40  45  50Precision(%)Recall (%)Figure 2: Precision-recall curve for the semanticparsing output on test sentences.4 ResultsWe found that KRISP could not be trained beyond1500 examples in this domain because the num-ber of negative examples that are generated duringthe training process would become too large forthe available memory size.
This is something thatcould be fixed in the future by suitably samplingnegative examples.
Using the first 1500 train-ing examples, we evaluated KRISP?s performanceon the provided 909 test examples.
A generatedRCL representation is considered correct only ifit exactly matches the correct answer; no partialcredit is given.
In order to avoid generating incor-rect meaning representations when it is not confi-dent, KRISP uses a threshold and if the confidence(probability) of the best semantic derivation is be-low this threshold, it does not generate any mean-ing representation.
This threshold was set to 0.05as was previously done for other domains.Performance was measured in terms of preci-sion (the percentage of generated meaning repre-sentations that were correct) and recall (the per-centage of all sentences for which correct meaningrepresentations were obtained).
Given that KRISPalso gives confidences with its output meaningrepresentations, we can compute precisions andrecalls at various confidence levels.
Figure 2shows the entire precision-recall curve thus ob-tained.
The best F-measure (harmonic mean ofprecision and recall) on this curve is 51.18% pdfat 72.67% precision and 39.49% recall.
The pre-cision at the highest recall was 45.98% which wehad reported as our official evaluation result forthe SemEval Task 6.We further analyzed the results according to thelengths of the sentences and found that KRISP wasSentence length Accuracy (Correct/Total)1-3 100.00% (15/15)4-7 71.20% (136/191)8-11 51.76% (147/284)12-15 41.80% (79/189)16-19 22.22% (28/126)20-23 15.71% (11/70)24-27 3.23% (1/31)28-31 33.33% (1/3)All 45.98% (418/909)Table 2: Accuracy of semantic parsing across testsentences of varying lengths.very accurate with shorter sentences and becameprogressively less accurate as the lengths of thesentences increase.
Table 2 shows these results.This could be simply because the longer the sen-tence, the more the likelihood of making an error,and since no partial credit is given, the entire out-put meaning representation is deemed incorrect.On further error analysis we observed that therewas some variability in the meaning representa-tions.
The ?move?
and ?drop?
actions seemedto mean the same thing and were used alterna-tively.
For example in the training data, the ut-terance ?place the red block on single blue block?had ?
(action: drop)?
in the corresponding mean-ing representation, while ?place red cube on greycube?
had ?
(action: move)?, but there is no ap-parent difference between the two cases.
Therewere many such instances.
This was confusingKRISP?s training algorithm because it would col-lect the same phrase sometimes as a positive ex-ample and sometimes as a negative example.
Thisalso affected the evaluation, because KRISP wouldgenerate ?move?
which won?t match ?drop?, orvice-versa, and the evaluator will call it an error.5 ConclusionsWe participated in the SemEval 2014 Task 6 of su-pervised semantic parsing of robotic spatial com-mands.
We used an existing semantic parserlearner, KRISP, and trained it on this domainwhich required minimum time and effort from ourside.
The trained parser was able to map natu-ral language robotic spatial commands into theirformal robotic command language representationswith good accuracy, particularly for shorter sen-tences.826ReferencesNello Cristianini and John Shawe-Taylor.
2000.
AnIntroduction to Support Vector Machines and OtherKernel-based Learning Methods.
Cambridge Uni-versity Press.Kais Dukes.
2013.
Semantic annotation of roboticspatial commands.
In Proceedings of the Languageand Technology Conference (LTC-2013), Poznan,Poland.Ruifang Ge and Raymond J. Mooney.
2005.
A sta-tistical semantic parser that integrates syntax andsemantics.
In Proceedings of the Ninth Confer-ence on Computational Natural Language Learning(CoNLL-2005), pages 9?16, Ann Arbor, MI, July.Rohit J. Kate and Raymond J. Mooney.
2006.
Us-ing string-kernels for learning semantic parsers.
InProceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguistics(COLING/ACL-06), pages 913?920, Sydney, Aus-tralia, July.Rohit J. Kate and Raymond J. Mooney.
2007a.
Learn-ing language semantics from ambiguous supervi-sion.
In Proceedings of the Twenty-Second Con-ference on Artificial Intelligence (AAAI-07), pages895?900, Vancouver, Canada, July.Rohit J. Kate and Raymond J. Mooney.
2007b.Semi-supervised learning for semantic parsing us-ing support vector machines.
In Proceedings ofHuman Language Technologies: The Conference ofthe North American Chapter of the Association forComputational Linguistics (NAACL-HLT-07), pages81?84, Rochester, NY, April.Rohit J. Kate, Yuk Wah Wong, and Raymond J.Mooney.
2005.
Learning to transform natural to for-mal languages.
In Proceedings of the Twentieth Na-tional Conference on Artificial Intelligence (AAAI-05), pages 1062?1068, Pittsburgh, PA, July.Rohit J. Kate.
2008.
Transforming meaning represen-tation grammars to improve semantic parsing.
InProceedings of the Twelfth Conference on Computa-tional Natural Language Learning (CoNLL-2008),pages 33?40.
Association for Computational Lin-guistics.Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-ter, and Mark Steedman.
2011.
Lexical generaliza-tion in CCG grammar induction for semantic pars-ing.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing (EMNLP-2011), pages 1512?1523.
Association for Computa-tional Linguistics.Huma Lodhi, Craig Saunders, John Shawe-Taylor,Nello Cristianini, and Chris Watkins.
2002.
Textclassification using string kernels.
2:419?444.Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S.Zettlemoyer.
2008.
A generative model for pars-ing natural language to meaning representations.
InProceedings of the 2008 Conference on EmpiricalMethods in Natural Language Processing (EMNLP-08), Honolulu, HI, October.Raymond J. Mooney.
2007.
Learning for semanticparsing.
In A. Gelbukh, editor, Computational Lin-guistics and Intelligent Text Processing: Proceed-ings of the 8th International Conference (CICLing-2007), Mexico City, pages 311?324.
Springer Ver-lag, Berlin.Lappoon R. Tang and Raymond J. Mooney.
2001.
Us-ing multiple clause constructors in inductive logicprogramming for semantic parsing.
In Proceedingsof the 12th European Conference on Machine Learn-ing (ECML-2001), pages 466?477, Freiburg, Ger-many.David H. D. Warren and Fernando C. N. Pereira.
1982.An efficient easily adaptable system for interpret-ing natural language queries.
American Journal ofComputational Linguistics, 8(3-4):110?122.Terry Winograd.
1972.
Understanding Natural Lan-guage.
Academic Press, Orlando, FL.Yuk Wah Wong and Raymond J. Mooney.
2006.Learning for semantic parsing with statistical ma-chine translation.
In Proceedings of Human Lan-guage Technology Conference / North AmericanChapter of the Association for Computational Lin-guistics Annual Meeting (HLT-NAACL-06), pages439?446, New York City, NY.William A.
Woods.
1977.
Lunar rocks in naturalEnglish: Explorations in natural language questionanswering.
In Antonio Zampoli, editor, LinguisticStructures Processing.
Elsevier North-Holland, NewYork.John M. Zelle and Raymond J. Mooney.
1996.
Learn-ing to parse database queries using inductive logicprogramming.
In Proceedings of the Thirteenth Na-tional Conference on Artificial Intelligence (AAAI-96), pages 1050?1055, Portland, OR, August.Luke S. Zettlemoyer and Michael Collins.
2005.Learning to map sentences to logical form: Struc-tured classification with probabilistic categorialgrammars.
In Proceedings of 21st Conference onUncertainty in Artificial Intelligence (UAI-2005),Edinburgh, Scotland, July.827
