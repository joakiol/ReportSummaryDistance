Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 333?337,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsFine Granular Aspect Analysis using Latent Structural ModelsLei Fang1 and Minlie Huang2State Key Laboratory of Intelligent Technology and Systems,Tsinghua National Laboratory for Information Science and Technology,Department of Computer Science and Technology,Tsinghua University, Beijing 100084, PR China.1fang-l10@mails.tsinghua.edu.cn2aihuang@tsinghua.edu.cnAbstractIn this paper, we present a structural learningmodel for joint sentiment classification and as-pect analysis of text at various levels of gran-ularity.
Our model aims to identify highly in-formative sentences that are aspect-specific inonline custom reviews.
The primary advan-tages of our model are two-fold: first, it per-forms document-level and sentence-level sen-timent polarity classification jointly; second,it is able to find informative sentences that areclosely related to some respects in a review,which may be helpful for aspect-level senti-ment analysis such as aspect-oriented sum-marization.
The proposed method was eval-uated with 9,000 Chinese restaurant reviews.Preliminary experiments demonstrate that ourmodel obtains promising performance.1 IntroductionOnline reviews have been a major resource fromwhich users may find opinions or comments on theproducts or services they want to consume.
How-ever, users sometimes might be overwhelmed, andnot be able to read reviews one by one when facinga considerably large number of reviews, and theymay be not be satisfied by only being served withdocument-level reviews statistics (that is, the num-ber of reviews with 1-star, 2-star, .
.
.
, respectively).Aspect-level review analysis may be alternative foraddressing this issue as aspect-specific opinions maymore clearly, explicitly, and completely describe thequality of a product from different properties.Our goal is to discover informative sentences thatare consistent with the overall rating of a review, andsimultaneously, to perform sentiment analysis at as-pect level.
Notice, that a review with a high rating(say, 4/5 stars) may contain both negative and posi-tive opinions, and the same to a review with a verylow rating (say, 1/2 star).
From our point of view,each review has a set of sentences that are informa-tive and coherent to its overall rating.
To performfine granular sentiment analysis, the first step is todiscover such coherent content.Many information needs require the systems toperform fine granular sentiment analysis.
Aspect-level sentiment analysis may be more useful forusers to have a global picture of opinions on theproduct?s properties.
Furthermore, different usersmay have different preferences on different aspectsof a product.
Taking the reviews on mobile phonesas an example, female users may focus more on theappearance while male users may lay more emphasison the hardware configuration; younger users preferto the app or game resources while older users mayjust pay attention to the basic function of calling ormessaging.In recent years, there has been much work focusedmultilevel sentiment classification using structurallearning models.
Yi (2007) extends the standardconditional random fields to model the local senti-ment flow.
Ryan (2007) proposed structured modelsfor fine-to-coarse sentiment analysis.
Oscar (2011)proposed to discover fine-grained sentiment withhidden-state CRF(Quattoni et al, 2007).
Yessenali-na (2010) deployed the framework of latent struc-tural SVMs(Yu and Joachims., 2009) for multilevelsentiment classification.
As for aspect level rating,ranking, or summarization, Benjamin(2007) em-333ployed the good grief algorithm for multiple aspectranking and the extensions of the generative topicmodels were also widely studied, such as (Titov andMcDonald., 2008; Brody and Elhadad., 2010; Wanget al, 2010; Li et al, 2011; Lu et al, 2011; Jo andOh., 2011; Lin and He, 2009).In this paper, we build a general structural learn-ing model for joint sentiment classification and as-pect analysis using a latent discriminate method.Our model is able to predict the sentiment polari-ty of document as well as to identify aspect-specificsentences and predict the polarity of such sentences.The proposed method was evaluated with 9,000 Chi-nese restaurant reviews.
Preliminary experimentsdemonstrate that our model obtains promising per-formance.2 Model2.1 Document StructureWe assume that the polarity of document is closelyrelated to some aspects for the reason that people arewriting reviews to praise or criticize certain aspect-s.
Therefore, each informative sentence of the doc-ument characterizes one aspect, expressing aspec-t specific polarity or subjective features.
Similar toprevious work on aspect analysis (Wang et al, 2010)and multi-level sentiment classification (Yessenali-na et al, 2010), we define the aspect as a collectionof synonyms.
For instance, the word set {?value?,?price?, ?cost?, ?worth?, ?quality?}
is a synonymset corresponding to the aspect ?price?.
For eachdocument, an aspect is described by one or severalsentences expressing aspect specific polarity or sub-jective information.Let document be denoted by x, and y ?
{+1,?1}represents the positive or negative polarity of thedocument, s is the set of informative sentences, inwhich each sentence is attached with certain aspectai ?
A = {a1, ..., ak}.
Yessenalina (2010) choosesa sentence set that best explains the sentiment of thewhole document while the s here retain this proper-ty.
Let ?
(x, y, s) denote the joint feature map thatoutputs the features describing the quality of predict-ing sentiment y using the sentence set s.Let xj denote the j-th sentence of documen-t x, and aj is the attached aspect of xj .
In spiritto (Yessenalina et al, 2010), we propose the follow-ing formulation of the discriminate functionw?T?
(x, y, s) =1N(x)?j?s(y ?
w?Tpolaj?pol(xj) + w?Tsubjaj?subj(xj))where N(x) is the normalizing factor, ?pol(xj) and?subj(xj) represents the polarity and subjectivityfeatures of sentence xj respectively.
w?pol and w?subjdenote the weight for polarity and subjectivity fea-tures.
To be specific for each aspect, we have w?polaand w?subja representing the vector of feature weightfor aspect a to calculate the polarity and subjectivityscore.w?Tpol =???w?Tpola0...w?Tpolak??
?, w?Tsubj =???w?Tsubja0...w?Tsubjak??
?To make prediction, we have the document-levelsentiment classifier ash(x; w?)
= argmaxy=?1maxs?S(x)w?T?
(x, y, s)where S(x) = {s ?
1, .
.
.
, |x| : |s| ?
f(|x|)},f(|x|) is a function that depends only on the numberof sentences in x, as illustrated in (Yessenalina et al,2010).
Therefore, for each sentence xj , we computethe joint subjectivity and polarity score with respectto aspect a and label y asscore(xj , a, y) = y?w?Tpola?pol(xj)+w?Tsubja?subj(xj)we then assign aspect aj to sentence xj ifaj = argmaxa?Ascore(xj , a, y)After sorting score(xj , aj , y) in decreasing orderand taking summation by selecting the top f(|x|) (orfewer, if there are fewer than f(|x|) that have posi-tive joint score) sentences as the total score for eachy?
{+1,?1} , we then predict y with the higher jointscore as the sentiment of the whole document.
Thisformulation of w?T?
(x, y, s) and classifier explainsthat for each sentence, the assigned aspect has thehighest score over other aspects.3342.2 Feature SpaceIn our model, we use bag-of-words features.
In or-der to obtain a model that is jointly trained, and sat-isfy the condition that the overall polarity of docu-ment should influence the sentiment of extracted in-formative sentences.
We denote the weight vectormodeling the polarity of entire document as w?doc, asfollows:w?T?
(x, y, s) =yN(x)??
?j?s(w?Tpolaj?pol(xj) + w?Tdoc?pol(xj))?
?+ 1N(x)???j?sw?Tsubjaj?subj(xj)?
?+y ?w?Tdoc?pol(x)2.3 TrainingWe trained our model using the latent structuralSVMs (Yu and Joachims., 2009).OP1:minw?,?
?012 ||w||2 + CNN?i=1?is.t.
?i :maxs?S(xi)w?T?
(xi, yi, s) ?maxs??S(xi)w?T?
(xi,?yi, s?)
+?
(yi,?yi, s?)?
?iWe define ?
(yi,?yi, s?)
= 1, that is, we viewdocument level sentiment classification loss as theloss function.
It should be noticed that OP1 is non-convex.
To circumvent the optimization difficul-ty, we employ the framework of structural SVM-s (Tsochantaridis et al, 2004) with latent variablesproposed by Yu (2009) using the CCCP algorith-m (Yuille and Rangarajan., 2003).
In terms of theformulation here, since the true informative sentenceset is never observed, it is a hidden or latent variable.Thus, we keep si fixed to compute the upper boundfor the concave part of each constraint, and rewritethe constraints as?i ?
maxs??S(xi)w?T?
(xi,?yi, s?)?
w?T?
(xi, yi, si)+1After that, we have yi completed with the laten-t variable si as if it is observed.
For each trainingexample, starting with an initialization sentence setin which each sentence is with an aspect label, thetraining procedure alternates between solving an in-stance of the structural SVM using the si and pre-dicting a new sentence until the learned weight vec-tor w?
converges.
In our work, we use the perfor-mance on a validation set to choose the halting iter-ation, as is similar to Yessenalina (2010).2.4 Model InitializationTo initialize the informative sentence set, followingthe experiment result of Yessenalina (2010), we setf(|x|) = 0.3 ?
|x|, that is, we only select the top30% of the total sentences as the set of informativepart of the document.
The normalizing factor is setas N(x) =?f |x|, as Yessenalina (2010) demon-strates that square root normalization can be useful.To analyze the aspect of each sentence, we need togive an initial guess of the aspect and sentiment foreach sentence.Sentence level sentiment initialization : To ini-tialize the sentence level sentiment, we employ arule based method incorporating positive and neg-ative sentiment terms, with adversative relation con-sidered.Sentence aspect assignment initialization : Obvi-ously, if a synonym of aspect a occurs in sentencexl, we assign aspect a to xl, and add xl to an aspectspecific sentence set Pa.For sentence xl without anyaspect term, we set a as the aspect label ifa = argmax similarity(xl, Pa?)a?
?AWe select the sentences whose sentiment is consis-tent with the overall rating of a review as the initialguess of the latent variable.3 ExperimentsIn this section, we evaluate our model in terms ofdocument and sentence level sentiment classifica-tion, we also analyze the performance of aspect as-signment for each sentence.
The model is evaluatedon the Chinese restaurant reviews crawled from Di-anping1.
Each of the reviews has an overall ratingranging from one to five stars.
To be specific, weconsider a review as positive if its rating is greater1http://www.dianping.com/335than or equal to 4 stars, or negative if less than orequal to 2 stars.
The corpus has 4500 positive and4500 negative reviews.
Data and an implementationof our model are publicly available2.We train 5 different models by splitting these re-views into 9 folds.
Two folds are left out as the test-ing set, and each model takes 5 folds as training set,2 folds as validation set, and the performance is aver-aged.
Besides, we also manually label 100 reviews,in which each sentence is labeled as positive or neg-ative corresponding to certain aspect or with no as-pect description.
On average, each review has 9.66sentences.
However, only 21.5% of the total sen-tences can be assigned to aspect by directly match-ing with aspect terms, which explains that keywordsbased aspect sentiment analysis may fail.
For restau-rant reviews, we pre-defined 11 aspects, and for eachaspect, we select about 5 frequently used terms todescribe that aspect.
Table 1 shows some examplesof the aspect synonym set used in this paper:Aspect Synonym SetTaste ???taste?,??
?flavor?Price ???price?
,??
?cost?Dishes ???dishes?,??
?cuisine?Ingredients ???food?
,??
?ingredients?Facility ???facility?,??
?seat?Location ??
?location?,Environment ???environment?,??
?decoration?Service ???service?
,????waiter???
?attitude?Table 1: Samples of Aspect Synonym.Document level sentiment classification We com-pare our method with previous work on sentimen-t classification using standard SVM(Pang et al,2002).
Our model yields an accuracy of 94.15%while the standard SVM classifier yields an accu-racy of 90.35%.
Clearly, our model outperforms thebaseline on document level sentiment classification.Sentence level sentiment classification Ourmethod can extract a set of informative sentencethat are coherent to the overall rating of a re-view.
The evaluation of sentence-level sentimentclassification is based on manual annotation.
We2http://www.qanswers.net/faculty/hml/sample 100 reviews, and present the extracted 300sentences to annotators who have been asked toassign positive/negative/non-related labels.
Amongthe sentences, 251 correctly classified as positiveor negative while 49 are misclassified.
And, 38sentences of the 49 sentences have mix opinions orare non-subjective sentences.Aspect Assignment To evaluate the accuracy of as-pect assignment, we compare the predicted aspec-t labels with the ground truth (manual annotation).As some of sentences have explicit aspect terms andcan be easily identified, we only consider those sen-tences without aspect words.
In the extracted 300sentences, 78 sentences have aspect terms, and forthe rest, our model assigns correct aspect labels to44 sentences while random guess only maps 21 sen-tences with right labels.4 Conclusion and Future WorkIn this paper, we address the task of multilevel sen-timent classification of online custom reviews forfine granular aspect analysis.
We present a struc-tural learning model based on struct-SVM with la-tent variables.
The informative sentence set is re-garded as latent variable, in which each sentence isattached with certain aspect label.
The training pro-cedure alternates between solving an instance of thestandard structural SVM optimization and predict-ing a new sentence set until the halting condition issatisfied.
In addition, our model is a enough gen-eral model which can be easily extended to otherdomains.
Preliminary experiments demonstrate thatour model obtains promising performance.There are several possibilities to improve ourmodel.
For future work, we propose to incorpo-rate prior knowledge of latent variables to the mod-el.
One possible way is to reformulate the loss func-tion by taking the predicted aspect of the extract-ed sentences into consideration.
Another is to in-troduce confidence score to the extracted sentences,such that the learned support vectors that are labeledwith higher confidence shall assert more force on thedecision plane.AcknowledgmentsThis paper was supported by Chinese 973 projectunder No.2012CB316301 and National Chinese Sci-336ence Foundation projects with No.60803075 andNo.60973104.ReferencesSamuel Brody and Noemie Elhadad.
2010.
An unsuper-vised aspect-sentiment model for online reviews.
InProceedings of Annual Conference of the North Amer-ican Chapter of the ACL, (NAACL).Yohan Jo and Alice Oh.
2011.
Aspect and sentiment uni-fication model for online review analysis.
In Proceed-ings of Conference on Web Search and Data Mining(WSDM).Peng Li, Yinglin Wang, Wei Gao, and Jing Jiang.
2011.Generating aspect-oriented multi-document summa-rization with event-aspect model.
In Proceedings ofConference on Empirical Methods in Natural Lan-guage Processing, (EMNLP).Chenghua Lin and Yulan He.
2009.
Joint sentimen-t/topic model for sentiment analysis.
In Proceedingsof the conference on Information and knowledge man-agement(CIKM).Bin Lu, Myle Ott, Claire Cardie, and Benjamin Tsou.2011.
Multi-aspect sentiment analysis with topic mod-els.
In The ICDM?2011 Workshop on Sentiment Elic-itation from Natural Text for Information Retrieval andExtraction.Yi Mao and Guy Lebanon.
2007.
Isotonic conditionalrandom fields and local sentiment flow.
In Proceed-ings of Advances in Neural Information ProcessingSystems (NIPS).Ryan McDonald, Kerry Hannan, Tyler Neylon, MikeWells, and Jeff Reynar.
2007.
Structured models forfine-to-coarse sentiment analysis.
In Proceedings ofAnnual Meeting of the Association for ComputationalLinguistics, (ACL).Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
sentiment classification using ma-chine learning techniques.
In Proceedings of Confer-ence on Empirical Methods in Natural Language Pro-cessing (EMNLP).A.
Quattoni, S. Wang, L.-P Morency, M. Collins, andT.
Darrell.
2007.
Hidden-state conditional randomfields.
IEEE Transactions on Pattern Analysis andMachine Intelligence.Benjamin Snyder and Regina Barzilay.
2007.
Multipleaspect ranking using the good grief algorithm.
In Pro-ceedings of Annual Conference of the North AmericanChapter of the ACL, (NAACL).Oscar Ta?ckstro?m and Ryan McDonald.
2011.
Discov-ering fine-grained sentiment with latent variable struc-tured prediction models.
In Proceedings of Annual Eu-ropean Conference on Information Retrieval , (ECIR).Ivan Titov and Ryan McDonald.
2008.
A joint model oftext and aspect ratings for sentiment summarization.In Proceedings of Annual Meeting of the Associationfor Computational Linguistics, (ACL).Ioannis Tsochantaridis, Thomas Hofmann, ThorstenJoachims, and Yasemin Altun.
2004.
Support vec-tor machine learning for interdependent and structuredoutput spaces.
In Proceedings of the InternationalConference on Machine Learning, (ICML).Hongning Wang, Yue Lu, and Chengxiang Zhai.
2010.Latent aspect rating analysis on review text data: Arating regression approach.
In Proceedings of the In-ternational Conference on Knowledge Discovery andData Mining (KDD).Ainur Yessenalina, Yisong Yue, and Claire Cardie.
2010.Multi-level structured models for document-level sen-timent classification.
In Proceedings of Conference onEmpirical Methods in Natural Language Processing(EMNLP).Chun-Nam John Yu and Thorsten Joachims.
2009.Learning structural svms with latent variables.
In Pro-ceedings of the International Conference on MachineLearning, (ICML).A.
L. Yuille and Anand Rangarajan.
2003.
Theconcave-convex procedure (cccp).
Neural Computa-tion, 15:915?936.337
