Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1004?1013,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsUsing Supervised Bigram-based ILP for Extractive SummarizationChen Li, Xian Qian, and Yang LiuThe University of Texas at DallasComputer Science Departmentchenli,qx,yangl@hlt.utdallas.eduAbstractIn this paper, we propose a bigram basedsupervised method for extractive docu-ment summarization in the integer linearprogramming (ILP) framework.
For eachbigram, a regression model is used to es-timate its frequency in the reference sum-mary.
The regression model uses a vari-ety of indicative features and is trained dis-criminatively to minimize the distance be-tween the estimated and the ground truthbigram frequency in the reference sum-mary.
During testing, the sentence selec-tion problem is formulated as an ILP prob-lem to maximize the bigram gains.
Wedemonstrate that our system consistentlyoutperforms the previous ILP method ondifferent TAC data sets, and performscompetitively compared to the best resultsin the TAC evaluations.
We also con-ducted various analysis to show the im-pact of bigram selection, weight estima-tion, and ILP setup.1 IntroductionExtractive summarization is a sentence selectionproblem: identifying important summary sen-tences from one or multiple documents.
Manymethods have been developed for this problem, in-cluding supervised approaches that use classifiersto predict summary sentences, graph based ap-proaches to rank the sentences, and recent globaloptimization methods such as integer linear pro-gramming (ILP) and submodular methods.
Theseglobal optimization methods have been shown tobe quite powerful for extractive summarization,because they try to select important sentences andremove redundancy at the same time under thelength constraint.Gillick and Favre (Gillick and Favre, 2009) in-troduced the concept-based ILP for summariza-tion.
Their system achieved the best result in theTAC 09 summarization task based on the ROUGEevaluation metric.
In this approach the goal isto maximize the sum of the weights of the lan-guage concepts that appear in the summary.
Theyused bigrams as such language concepts.
The as-sociation between the language concepts and sen-tences serves as the constraints.
This ILP methodis formally represented as below (see (Gillick andFavre, 2009) for more details):max ?i wici (1)s.t.
sjOccij ?
ci (2)?j sjOccij ?
ci (3)?j ljsj ?
L (4)ci ?
{0, 1} ?i (5)sj ?
{0, 1} ?j (6)ci and sj are binary variables (shown in (5) and(6)) that indicate the presence of a concept anda sentence respectively.
wi is a concept?s weightand Occij means the occurrence of concept i insentence j. Inequalities (2)(3) associate the sen-tences and concepts.
They ensure that selecting asentence leads to the selection of all the conceptsit contains, and selecting a concept only happenswhen it is present in at least one of the selectedsentences.There are two important components in thisconcept-based ILP: one is how to select the con-cepts (ci); the second is how to set up their weights(wi).
Gillick and Favre (Gillick and Favre, 2009)used bigrams as concepts, which are selected froma subset of the sentences, and their document fre-quency as the weight in the objective function.In this paper, we propose to find a candidatesummary such that the language concepts (e.g., bi-grams) in this candidate summary and the refer-ence summary can have the same frequency.
Weexpect this restriction is more consistent with the1004ROUGE evaluation metric used for summarization(Lin, 2004).
In addition, in the previous concept-based ILP method, the constraints are with respectto the appearance of language concepts, hence itcannot distinguish the importance of different lan-guage concepts in the reference summary.
Ourmethod can decide not only which language con-cepts to use in ILP, but also the frequency of theselanguage concepts in the candidate summary.
Toestimate the bigram frequency in the summary,we propose to use a supervised regression modelthat is discriminatively trained using a variety offeatures.
Our experiments on several TAC sum-marization data sets demonstrate this proposedmethod outperforms the previous ILP system andoften the best performing TAC system.2 Proposed Method2.1 Bigram Gain Maximization by ILPWe choose bigrams as the language concepts inour proposed method since they have been suc-cessfully used in previous work.
In addition, weexpect that the bigram oriented ILP is consistentwith the ROUGE-2 measure widely used for sum-marization evaluation.We start the description of our approach for thescenario where a human abstractive summary isprovided, and the task is to select sentences toform an extractive summary.
Then Our goal isto make the bigram frequency in this system sum-mary as close as possible to that in the reference.For each bigram b, we define its gain:Gain(b, sum) = min{nb,ref , nb,sum} (7)where nb,ref is the frequency of b in the referencesummary, and nb,sum is the frequency of b in theautomatic summary.
The gain of a bigram is nomore than its frequency in the reference summary,hence adding redundant bigrams will not increasethe gain.The total gain of an extractive summary is de-fined as the sum of every bigram gain in the sum-mary:Gain(sum) =?bGain(b, sum)=?bmin{nb,ref ,?sz(s) ?
nb,s} (8)where s is a sentence in the document, nb,s isthe frequency of b in sentence s, z(s) is a binaryvariable, indicating whether s is selected in thesummary.
The goal is to find z that maximizesGain(sum) (formula (8)) under the length con-straint L.This problem can be casted as an ILP problem.First, using the fact thatmin{a, x} = 0.5(?|x ?
a| + x + a), x, a ?
0we have?bmin{nb,ref ,?sz(s) ?
nb,s} =?b0.5 ?
(?|nb,ref ?
?sz(s) ?
nb,s|+nb,ref +?sz(s) ?
nb,s)Now the problem is equivalent to:maxz?b(?|nb,ref ?
?sz(s) ?
nb,s| +nb,ref +?sz(s) ?
nb,s)s.t.
?sz(s) ?
|S| ?
L; z(s) ?
{0, 1}This is equivalent to the ILP:max?b(?sz(s) ?
nb,s ?Cb) (9)s.t.
?sz(s) ?
|S| ?
L (10)z(s) ?
{0, 1} (11)?Cb ?
nb,ref ?
?sz(s) ?
nb,s ?
Cb(12)where Cb is an auxiliary variable we introduce thatis equal to |nb,ref ?
?s z(s) ?
nb,s|, and nb,ref isa constant that can be dropped from the objectivefunction.2.2 Regression Model for Bigram FrequencyEstimationIn the previous section, we assume that nb,ref isat hand (reference abstractive summary is given)and propose a bigram-based optimization frame-work for extractive summarization.
However, forthe summarization task, the bigram frequency isunknown, and thus our first goal is to estimate suchfrequency.
We propose to use a regression modelfor this.Since a bigram?s frequency depends on the sum-mary length (L), we use a normalized frequency1005in our method.
Let nb,ref = Nb,ref ?
L, whereNb,ref = n(b,ref)?b n(b,ref)is the normalized frequencyin the summary.
Now the problem is to automati-cally estimate Nb,ref .Since the normalized frequency Nb,ref is a realnumber, we choose to use a logistic regressionmodel to predict it:Nb,ref =exp{w?f(b)}?j exp{w?f(bj)}(13)where f(bj) is the feature vector of bigram bj andw?
is the corresponding feature weight.
Since evenfor identical bigrams bi = bj , their feature vectorsmay be different (f(bi) 6= f(bj)) due to their dif-ferent contexts, we sum up frequencies for identi-cal bigrams {bi|bi = b}:Nb,ref =?i,bi=bNbi,ref=?i,bi=b exp{w?f(bi)}?j exp{w?f(bj)}(14)To train this regression model using the givenreference abstractive summaries, rather than tryingto minimize the squared error as typically done,we propose a new objective function.
Since thenormalized frequency satisfies the probability con-straint?b Nb,ref = 1, we propose to use KL di-vergence to measure the distance between the es-timated frequencies and the ground truth values.The objective function for training is thus to mini-mize the KL distance:min?bN?b,ref logN?b,refNb,ref(15)where N?b,ref is the true normalized frequency ofbigram b in reference summaries.Finally, we replace Nb,ref in Formula (15) withEq (14) and get the objective function below:max?bN?b,ref log?i,bi=b exp{w?f(bi)}?j exp{w?f(bj)}(16)This shares the same form as the contrastive es-timation proposed by (Smith and Eisner, 2005).We use gradient decent method for parameter esti-mation, initial w is set with zero.2.3 FeaturesEach bigram is represented using a set of featuresin the above regression model.
We use two typesof features: word level and sentence level features.Some of these features have been used in previouswork (Aker and Gaizauskas, 2009; Brandow et al,1995; Edmundson, 1969; Radev, 2001):?
Word Level:?
1.
Term frequency1: The frequency ofthis bigram in the given topic.?
2.
Term frequency2: The frequency ofthis bigram in the selected sentences1 .?
3.
Stop word ratio: Ratio of stop wordsin this bigram.
The value can be {0, 0.5,1}.?
4.
Similarity with topic title: Thenumber of common tokens in these twostrings, divided by the length of thelonger string.?
5.
Similarity with description of thetopic: Similarity of the bigram withtopic description (see next data sectionabout the given topics in the summariza-tion task).?
Sentence Level: (information of sentencecontaining the bigram)?
6.
Sentence ratio: Number of sentencesthat include this bigram, divided by thetotal number of the selected sentences.?
7.
Sentence similarity: Sentence sim-ilarity with topic?s query, which is theconcatenation of topic title and descrip-tion.?
8.
Sentence position: Sentence posi-tion in the document.?
9.
Sentence length: The number ofwords in the sentence.?
10.
Paragraph starter: Binary featureindicating whether this sentence is thebeginning of a paragraph.3 Experiments3.1 DataWe evaluate our method using several recent TACdata sets, from 2008 to 2011.
The TAC summa-rization task is to generate at most 100 words sum-maries from 10 documents for a given topic query(with a title and more detailed description).
Formodel training, we also included two years?
DUCdata (2006 and 2007).
When evaluating on oneTAC data set, we use the other years of the TACdata plus the two DUC data sets as the trainingdata.1See next section about the sentence selection step10063.2 Summarization SystemWe use the same system pipeline described in(Gillick et al, 2008; McDonald, 2007).
The keymodules in the ICSI ILP system (Gillick et al,2008) are briefly described below.?
Step 1: Clean documents, split text into sen-tences.?
Step 2: Extract bigrams from all the sen-tences, then select those bigrams with doc-ument frequency equal to more than 3.
Wecall this subset as initial bigram set in the fol-lowing.?
Step 3: Select relevant sentences that containat least one bigram from the initial bigramset.?
Step 4: Feed the ILP with sentences and thebigram set to get the result.?
Step 5: Order sentences identified by ILP asthe final result of summary.The difference between the ICSI and our systemis in the 4th step.
In our method, we first extract allthe bigrams from the selected sentences and thenestimate each bigram?s Nb,ref using the regressionmodel.
Then we use the top-n bigrams with theirNb,ref and all the selected sentences in our pro-posed ILP module for summary sentence selec-tion.
When training our bigram regression model,we use each of the 4 reference summaries sepa-rately, i.e., the bigram frequency is obtained fromone reference summary.
The same pre-selection ofsentences described above is also applied in train-ing, that is, the bigram instances used in trainingare from these selected sentences and the referencesummary.4 Experiment and Analysis4.1 Experimental ResultsTable 1 shows the ROUGE-2 results of our pro-posed system, the ICSI system, and also the bestperforming system in the NIST TAC evaluation.We can see that our proposed system consistentlyoutperforms ICSI ILP system (the gain is statis-tically significant based on ROUGE?s 95% confi-dence internal results).
Compared to the best re-ported TAC result, our method has better perfor-mance on three data sets, except 2011 data.
Notethat the best performing system for the 2009 datais the ICSI ILP system, with an additional com-pression step.
Our ILP method is purely extrac-tive.
Even without using compression, our ap-proach performs better than the full ICSI system.The best performing system for the 2011 data alsohas some compression module.
We expect that af-ter applying sentence compression and merging,we will have even better performance, however,our focus in this paper is on the bigram-based ex-tractive summarization.ICSI Proposed TAC Rank1ILP System System2008 0.1023 0.1076 0.10382009 0.1160 0.1246 0.12162010 0.1003 0.1067 0.09572011 0.1271 0.1327 0.1344Table 1: ROUGE-2 summarization results.There are several differences between the ICSIsystem and our proposed method.
First is thebigrams (concepts) used.
We use the top 100bigrams from our bigram estimation module;whereas the ICSI system just used the initial bi-gram set described in Section 3.2.
Second, theweights for those bigrams differ.
We used the es-timated value from the regression model; the ICSIsystem just uses the bigram?s document frequencyin the original text as weight.
Finally, two systemsuse different ILP setups.
To analyze which fac-tors (or all of them) explain the performance dif-ference, we conducted various controlled experi-ments for these three factors (bigrams, weights,ILP).
All of the following experiments use theTAC 2009 data as the test set.4.2 Effect of Bigram WeightsIn this experiment, we vary the weighting methodsfor the two systems: our proposed method and theICSI system.
We use three weighting setups: theestimated bigram frequency value in our method,document frequency, or term frequency from theoriginal text.
Table 2 and 3 show the results usingthe top 100 bigrams from our system and the ini-tial bigram set from the ICSI system respectively.We also evaluate using the two different ILP con-figurations in these experiments.First of all, we can see that for both ILP sys-tems, our estimated bigram weights outperformthe other frequency-based weights.
For the ICSIILP system, using bigram document frequencyachieves better performance than term frequency(which verified why document frequency is usedin their system).
In contrast, for our ILP method,1007# Weight ILP ROUGE-21 Estimated value Proposed 0.12462 ICSI 0.11783 Document freq Proposed 0.11094 ICSI 0.11325 Term freq Proposed 0.11166 ICSI 0.1080Table 2: Results using different weighting meth-ods on the top 100 bigrams generated from ourproposed system.# Weight ILP ROUGE-21 Estimated value Proposed 0.11572 ICSI 0.11613 Document freq Proposed 0.11014 ICSI 0.11605 Term freq Proposed 0.11096 ICSI 0.1072Table 3: Results using different weighting meth-ods based on the initial bigram sets.
The averagenumber of bigrams is around 80 for each topic.the bigram?s term frequency is slightly more use-ful than its document frequency.
This indicatesthat our estimated value is more related to bi-gram?s term frequency in the original text.
Whenthe weight is document frequency, the ICSI?s re-sult is better than our proposed ILP; whereas whenusing term frequency as the weights, our ILP hasbetter results, again suggesting term frequency fitsour ILP system better.
When the weight is esti-mated value, the results depend on the bigram setused.
The ICSI?s ILP performs slightly better thanours when it is equipped with the initial bigram,but our proposed ILP has much better results us-ing our selected top100 bigrams.
This shows thatthe size and quality of the bigrams has an impacton the ILP modules.4.3 The Effect of Bigram Set?s sizeIn our proposed system, we use 100 top bigrams.There are about 80 bigrams used in the ICSI ILPsystem.
A natural question to ask is the impactof the number of bigrams and their quality on thesummarization system.
Table 4 shows some statis-tics of the bigrams.
We can see that about onethird of bigrams in the reference summary are inthe original text (127.3 out of 321.93), verifyingthat people do use different words/bigram whenwriting abstractive summaries.
We mentioned thatwe only use the top-N (n is 100 in previous ex-periments) bigrams in our summarization system.On one hand, this is to save computational cost forthe ILP module.
On the other hand, we see fromthe table that only 127 of these more than 2K bi-grams are in the reference summary and are thusexpected to help the summary responsiveness.
In-cluding all the bigrams would lead to huge noise.# bigrams in ref summary 321.93# bigrams in text and ref summary 127.3# bigrams used in our regression model 2140.7(i.e., in selected sentences)Table 4: Bigram statistics.
The numbers are theaverage ones for each topic.Fig 1 shows the bigram coverage (number of bi-grams used in the system that are also in referencesummaries) when we vary N selected bigrams.
Asexpected, we can see that as n increases, thereare more reference summary bigrams included inthe system.
There are 25 summary bigrams in thetop-50 bigrams and about 38 in top-100 bigrams.Compared with the ICSI system that has around 80bigrams in the initial bigram set and 29 in the ref-erence summary, our estimation module has bettercoverage.010203040506070809010011012013050 500 950 1400 1850 2300 2750 3200Number of Selected BigramNumberofBigrambothinSelectedandReferenceFigure 1: Coverage of bigrams (number of bi-grams in reference summary) when varying thenumber of bigrams used in the ILP systems.Increasing the number of bigrams used in thesystem will lead to better coverage, however, theincorrect bigrams also increase and have a nega-tive impact on the system performance.
To exam-ine the best tradeoff, we conduct the experimentsby choosing the different top-N bigram set for thetwo ILP systems, as shown in Fig 2.
For both theILP systems, we used the estimated weight valuefor the bigrams.1008We can see that the ICSI ILP system performsbetter when the input bigrams have less noise(those bigrams that are not in summary).
However,our proposed method is slightly more robust to thiskind of noise, possibly because of the weights weuse in our system ?
the noisy bigrams have lowerweights and thus less impact on the final systemperformance.
Overall the two systems have sim-ilar trends: performance increases at the begin-ning when using more bigrams, and after certainpoints starts degrading with too many bigrams.The optimal number of bigrams differs for the twosystems, with a larger number of bigrams in ourmethod.
We also notice that the ICSI ILP systemachieved a ROUGE-2 of 0.1218 when using top60 bigrams, which is better than using the initialbigram set in their method (0.1160).0.1090.1110.1130.1150.1170.1190.1210.1230.12540 50 60 70 80 90 100 110 120 130Number of selected bigramRouge-2Proposed ILPICSIFigure 2: Summarization performance when vary-ing the number of bigrams for two systems.4.4 Oracle ExperimentsBased on the above analysis, we can see the impactof the bigram set and their weights.
The followingexperiments are designed to demonstrate the bestsystem performance we can achieve if we have ac-cess to good quality bigrams and weights.
Here weuse the information from the reference summary.The first is an oracle experiment, where we useall the bigrams from the reference summaries thatare also in the original text.
In the ICSI ILPsystem, the weights are the document frequencyfrom the multiple reference summaries.
In our ILPmodule, we use the term frequency of the bigram.The oracle results are shown in Table 5.
We cansee these are significantly better than the automaticsystems.From Table 5, we notice that ICSI?s ILP per-forms marginally better than our proposed ILP.
Wehypothesize that one reason may be that many bi-grams in the summary reference only appear once.Table 6 shows the frequency of the bigrams in thesummary.
Indeed 85% of bigram only appear onceILP System ROUGE-2Our ILP 0.2124ICSI ILP 0.2128Table 5: Oracle experiment: using bigrams andtheir frequencies in the reference summary asweights.and no bigrams appear more than 9 times.
For themajority of the bigrams, our method and the ICSIILP are the same.
For the others, our system hasslight disadvantage when using the reference termfrequency.
We expect the high term frequencymay need to be properly smoothed/normalized.Freq 1 2 3 4 5 6 7 8 9Ave# 277 32 7.5 3.2 1.1 0.3 0.1 0.1 0.04Table 6: Average number of bigrams for each termfrequency in one topic?s reference summary.We also treat the oracle results as the gold stan-dard for extractive summarization and comparedhow the two automatic summarization systemsdiffer at the sentence level.
This is different fromthe results in Table 1, which are the ROUGE re-sults comparing to human written abstractive sum-maries at the n-gram level.
We found that amongthe 188 sentences in this gold standard, our systemhits 31 and ICSI only has 23.
This again showsthat our system has better performance, not justat the word level based on ROUGE measures, butalso at the sentence level.
There are on average3 different sentences per topic between these tworesults.In the second experiment, after we obtain theestimated Nb,ref for every bigram in the selectedsentences from our regression model, we onlykeep those bigrams that are in the reference sum-mary, and use the estimated weights for both ILPmodules.
Table 7 shows the results.
We canconsider these as the upper bound the systemcan achieve if we use the automatically estimatedweights for the correct bigrams.
In this experi-ment ICSI ILP?s performance still performs betterthan ours.
This might be attributed to the fact thereis less noise (all the bigrams are the correct ones)and thus the ICSI ILP system performs well.
Wecan see that these results are worse than the pre-vious oracle experiments, but are better than usingthe automatically generated bigrams, again show-ing the bigram and weight estimation is critical for1009summarization.# Weight ILP ROUGE-21 Estimated value Proposed 0.18882 ICSI 0.1942Table 7: Summarization results when using the es-timated weights and only keeping the bigrams thatare in the reference summary.4.5 Effect of Training SetSince our method uses supervised learning, weconduct the experiment to show the impact oftraining size.
In TAC?s data, each topic has twosets of documents.
For set A, the task is a standardsummarization, and there are 4 reference sum-maries, each 100 words long; for set B, it is an up-date summarization task ?
the summary includesinformation not mentioned in the summary fromset A.
There are also 4 reference summaries, with400 words in total.
Table 8 shows the results on2009 data when using the data from different yearsand different sets for training.
We notice that whenthe training data only contains set A, the perfor-mance is always better than using set B or the com-bined set A and B.
This is not surprising becauseof the different task definition.
Therefore, for therest of the study on data size impact, we only usedata set A from the TAC data and the DUC data asthe training set.
In total there are about 233 topicsfrom the two years?
DUC data (06, 07) and threeyears?
TAC data (08, 10, 11).
We incrementallyadd 20 topics every time (from DUC06 to TAC11)and plot the learning curve, as shown in Fig 3.
Asexpected, more training data results in better per-formance.Training Set # Topics ROUGE-208 Corpus (A) 48 0.119208 Corpus( B) 48 0.117808 Corpus (A+B) 96 0.118810 Corpus (A) 46 0.117410 Corpus (B) 46 0.116710 Corpus (A+B) 92 0.117011 Corpus (A) 44 0.115711 Corpus (B) 44 0.113011 Corpus (A+B) 88 0.1140Table 8: Summarization performance when usingdifferent training corpora.0.1120.1130.1140.1150.1160.1170.1180.1190.120.1210.1220.1230.1240.12520 40 60 80 100 120 140 160 180 200 220 240Number of trainning topicsRouge-2Figure 3: Learning curve4.6 Summary of AnalysisThe previous experiments have shown the impactof the three factors: the quality of the bigramsthemselves, the weights used for these bigrams,and the ILP module.
We found that the bigramsand their weights are critical for both the ILP se-tups.
However, there is negligible difference be-tween the two ILP methods.An important part of our system is the super-vised method for bigram and weight estimation.We have already seen for the previous ILP method,when using our bigrams together with the weights,better performance can be achieved.
Therefore weask the question whether this is simply becausewe use supervised learning, or whether our pro-posed regression model is the key.
To answer this,we trained a simple supervised binary classifierfor bigram prediction (positive means that a bi-gram appears in the summary) using the same setof features as used in our bigram weight estima-tion module, and then used their document fre-quency in the ICSI ILP system.
The result forthis method is 0.1128 on the TAC 2009 data.
Thisis much lower than our result.
We originally ex-pected that using the supervised method may out-perform the unsupervised bigram selection whichonly uses term frequency information.
Further ex-periments are needed to investigate this.
From thiswe can see that it is not just the supervised meth-ods or using annotated data that yields the over-all improved system performance, but rather ourproposed regression setup for bigrams is the mainreason.5 Related WorkWe briefly describe some prior work on summa-rization in this section.
Unsupervised methodshave been widely used.
In particular, recently sev-eral optimization approaches have demonstrated1010competitive performance for extractive summa-rization task.
Maximum marginal relevance(MMR) (Carbonell and Goldstein, 1998) uses agreedy algorithm to find summary sentences.
(Mc-Donald, 2007) improved the MMR algorithm todynamic programming.
They used a modified ob-jective function in order to consider whether theselected sentence is globally optimal.
Sentence-level ILP was also first introduced in (McDon-ald, 2007), but (Gillick and Favre, 2009) revisedit to concept-based ILP.
(Woodsend and Lapata,2012) utilized ILP to jointly optimize different as-pects including content selection, surface realiza-tion, and rewrite rules in summarization.
(Gala-nis et al, 2012) uses ILP to jointly maximize theimportance of the sentences and their diversityin the summary.
(Berg-Kirkpatrick et al, 2011)applied a similar idea to conduct the sentencecompression and extraction for multiple documentsummarization.
(Jin et al, 2010) made a com-parative study on sentence/concept selection andpairwise and list ranking algorithms, and con-cluded ILP performed better than MMR and thediversity penalty strategy in sentence/concept se-lection.
Other global optimization methods in-clude submodularity (Lin and Bilmes, 2010) andgraph-based approaches (Erkan and Radev, 2004;Leskovec et al, 2005; Mihalcea and Tarau, 2004).Various unsupervised probabilistic topic modelshave also been investigated for summarization andshown promising.
For example, (Celikyilmaz andHakkani-Tu?r, 2011) used it to model the hiddenabstract concepts across documents as well as thecorrelation between these concepts to generatetopically coherent and non-redundant summaries.
(Darling and Song, 2011) applied it to separatethe semantically important words from the low-content function words.In contrast to these unsupervised approaches,there are also various efforts on supervised learn-ing for summarization where a model is trained topredict whether a sentence is in the summary ornot.
Different features and classifiers have beenexplored for this task, such as Bayesian method(Kupiec et al, 1995), maximum entropy (Osborne,2002), CRF (Galley, 2006), and recently reinforce-ment learning (Ryang and Abekawa, 2012).
(Akeret al, 2010) used discriminative reranking on mul-tiple candidates generated by A* search.
Recently,research has also been performed to address someissues in the supervised setup, such as the classdata imbalance problem (Xie and Liu, 2010).In this paper, we propose to incorporate thesupervised method into the concept-based ILPframework.
Unlike previous work using sentence-based supervised learning, we use a regressionmodel to estimate the bigrams and their weights,and use these to guide sentence selection.
Com-pared to the direct sentence-based classification orregression methods mentioned above, our methodhas an advantage.
When abstractive summariesare given, one needs to use that information to au-tomatically generate reference labels (a sentenceis in the summary or not) for extractive summa-rization.
Most researchers have used the similaritybetween a sentence in the document and the ab-stractive summary for labeling.
This is not a per-fect process.
In our method, we do not need togenerate this extra label for model training sinceours is based on bigrams ?
it is straightforward toobtain the reference frequency for bigrams by sim-ply looking at the reference summary.
We expectour approach also paves an easy way for future au-tomatic abstractive summarization.
One previousstudy that is most related to ours is (Conroy et al,2011), which utilized a Naive Bayes classifier topredict the probability of a bigram, and appliedILP for the final sentence selection.
They usedmore features than ours, whereas we use a discrim-inatively trained regression model and a modifiedILP framework.
Our proposed method performsbetter than their reported results in TAC 2011 data.Another study closely related to ours is (Davis etal., 2012), which leveraged Latent Semantic Anal-ysis (LSA) to produce term weights and selectedsummary sentences by computing an approximatesolution to the Budgeted Maximal Coverage prob-lem.6 Conclusion and Future WorkIn this paper, we leverage the ILP method as a corecomponent in our summarization system.
Dif-ferent from the previous ILP summarization ap-proach, we propose a supervised learning method(a discriminatively trained regression model) todetermine the importance of the bigrams fed tothe ILP module.
In addition, we revise the ILP tomaximize the bigram gain (which is expected tobe highly correlated with ROUGE-2 scores) ratherthan the concept/bigram coverage.
Our proposedmethod yielded better results than the previousstate-of-the-art ILP system on different TAC data1011sets.
From a series of experiments, we found thatthere is little difference between the two ILP mod-ules, and that the improved system performance isattributed to the fact that our proposed supervisedbigram estimation module can successfully gatherthe important bigram and assign them appropriateweights.
There are several directions that warrantfurther research.
We plan to consider the contextof bigrams to better predict whether a bigram is inthe reference summary.
We will also investigatethe relationship between concepts and sentences,which may help move towards abstractive summa-rization.AcknowledgmentsThis work is partly supported by DARPA underContract No.
HR0011-12-C-0016 and FA8750-13-2-0041, and NSF IIS-0845484.
Any opinionsexpressed in this material are those of the authorsand do not necessarily reflect the views of DARPAor NSF.ReferencesAhmet Aker and Robert Gaizauskas.
2009.
Summarygeneration for toponym-referenced images using ob-ject type language models.
In Proceedings of theInternational Conference RANLP.Ahmet Aker, Trevor Cohn, and Robert Gaizauskas.2010.
Multi-document summarization using a*search and discriminative training.
In Proceedingsof the EMNLP.Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.2011.
Jointly learning to extract and compress.
InProceedings of the ACL.Ronald Brandow, Karl Mitze, and Lisa F. Rau.
1995.Automatic condensation of electronic publicationsby sentence selection.
Inf.
Process.
Manage.Jaime Carbonell and Jade Goldstein.
1998.
The use ofmmr, diversity-based reranking for reordering docu-ments and producing summaries.
In Proceedings ofthe SIGIR.Asli Celikyilmaz and Dilek Hakkani-Tu?r.
2011.
Dis-covery of topically coherent sentences for extractivesummarization.
In Proceedings of the ACL.John M. Conroy, Judith D. Schlesinger, Jeff Kubina,Peter A. Rankel, and Dianne P. O?Leary.
2011.Classy 2011 at tac: Guided and multi-lingual sum-maries and evaluation metrics.
In Proceedings of theTAC.William M. Darling and Fei Song.
2011.
Probabilisticdocument modeling for syntax removal in text sum-marization.
In Proceedings of the ACL.Sashka T. Davis, John M. Conroy, and Judith D.Schlesinger.
2012.
Occams - an optimal combinato-rial covering algorithm for multi-document summa-rization.
In Proceedings of the ICDM.H.
P. Edmundson.
1969.
New methods in automaticextracting.
J. ACM.Gu?nes Erkan and Dragomir R. Radev.
2004.
Lexrank:graph-based lexical centrality as salience in textsummarization.
J. Artif.
Int.
Res.Dimitrios Galanis, Gerasimos Lampouras, and Ion An-droutsopoulos.
2012.
Extractive multi-documentsummarization with integer linear programming andsupport vector regression.
In Proceedings of theCOLING.Michel Galley.
2006.
A skip-chain conditional randomfield for ranking meeting utterances by importance.In Proceedings of the EMNLP.Dan Gillick and Benoit Favre.
2009.
A scalable globalmodel for summarization.
In Proceedings of theWorkshop on Integer Linear Programming for Natu-ral Langauge Processing on NAACL.Dan Gillick, Benoit Favre, and Dilek Hakkani-Tu?r.2008.
In The ICSI Summarization System at TAC2008.Feng Jin, Minlie Huang, and Xiaoyan Zhu.
2010.
Acomparative study on ranking and selection strate-gies for multi-document summarization.
In Pro-ceedings of the COLING.Julian Kupiec, Jan Pedersen, and Francine Chen.
1995.A trainable document summarizer.
In Proceedingsof the SIGIR.Jure Leskovec, Natasa Milic-Frayling, and Marko Gro-belnik.
2005.
Impact of linguistic analysis on thesemantic graph coverage and learning of documentextracts.
In Proceedings of the AAAI.Hui Lin and Jeff Bilmes.
2010.
Multi-document sum-marization via budgeted maximization of submodu-lar functions.
In Proceedings of the NAACL.Chin-Yew Lin.
2004.
Rouge: a package for auto-matic evaluation of summaries.
In Proceedings ofthe ACL.Ryan McDonald.
2007.
A study of global inference al-gorithms in multi-document summarization.
In Pro-ceedings of the European conference on IR research.Rada Mihalcea and Paul Tarau.
2004.
Textrank:Bringing order into text.
In Proceedings of theEMNLP.Miles Osborne.
2002.
Using maximum entropy forsentence extraction.
In Proceedings of the ACL-02Workshop on Automatic Summarization.1012Dragomir R. Radev.
2001.
Experiments in single andmultidocument summarization using mead.
In InFirst Document Understanding Conference.Seonggi Ryang and Takeshi Abekawa.
2012.
Frame-work of automatic text summarization using rein-forcement learning.
In Proceedings of the EMNLP.Noah A. Smith and Jason Eisner.
2005.
Contrastiveestimation: training log-linear models on unlabeleddata.
In Proceedings of the ACL.Kristian Woodsend and Mirella Lapata.
2012.
Mul-tiple aspect summarization using integer linear pro-gramming.
In Proceedings of the EMNLP.Shasha Xie and Yang Liu.
2010.
Improving supervisedlearning for meeting summarization using samplingand regression.
Comput.
Speech Lang.1013
