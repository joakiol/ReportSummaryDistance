Significant Sentence Extraction by EuclideanDistance Based on Singular ValueDecompositionChangbeom Lee1, Hyukro Park2, and Cheolyoung Ock11 School of Computer Engineering & Information Technology, University of Ulsan,Ulsan 680-749, South Korea{chblee1225, okcy}@mail.ulsan.ac.kr2 Department of Computer Science, Chonnam National University, 300,Youngbong-dong, Buk-gu, Kwangju 500-757, South Koreahyukro@chonnam.ac.krAbstract.
This paper describes an automatic summarization approachthat constructs a summary by extracting the significant sentences.
Theapproach takes advantage of the cooccurrence relationships betweenterms only in the document.
The techniques used are principal compo-nent analysis (PCA) to extract the significant terms and singular valuedecompostion (SVD) to find out the significant sentences.
The PCA canquantify both the term frequency and term-term relationship in the doc-ument by the eigenvalue-eigenvector pairs.
And the sentence-term matrixcan be decomposed into the proper dimensional sentence-concentratedand term-concentrated marices which are used for the Euclidean dis-tances between the sentence and term vectors and also removed the noiseof variability in term usage by the SVD.
Experimental results on Koreannewspaper articles show that the proposed method is to be preferredover random selection of sentences or only PCA when summarization isthe goal.keywords: Text summarization; Principal component analysis; Singularvalue decomposition.1 IntroductionAutomatic text summarization is the process of reducing the length of textdocuments, while retaining the essential qualities of the orginal.
Many searchengines have tried to solve the problem of information overflowing by showingeither the title and beginning of of a document.
However, such the title andbeginning are insufficient to decide the relevance of the documents which userwants to search, and this is the reason that the text summarization is requiredto resolve this problem.The process of text summarization could consist of two phases: a documentinterpretation phase and a summary generation phase.
The primary goal of adocument interpretation phase is to find the main theme of a document and itsR.
Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
636?645, 2005.c?
Springer-Verlag Berlin Heidelberg 2005Significant Sentence Extraction by Euclidean Distance 637corresponding significant words.
Since the significant words collectively repre-sent the main theme of a document, it is important to find them more reason-ably.
For the purpose of this doing , the word frequency of a document mightbe utilized[4,8].
But this approach is limited in that cooccurrence relationshipsamong words are not considered at all.
In contrast to word frequency, the othermethod by using WordNet or a thesaurus[2] makes good use of word relationshipssuch as NT(narrow term), RT(related term), and so on.
However such resourcesrequire a large cost to compile, and often represent too general relationships tofit a specific domain.In this paper, we propose a new summarization approach by both princi-pal component analysis (PCA) and singular value decomposition (SVD) thatare called quantification methods or statistical analysis methods.
PCA is uti-lized to find significant words or terms in the document by term-relationships.Since the necessary term-relationships can be acquired only from the given doc-ument by linear transformation of PCA, the proposed method need not exploitthe additional information such as WordNet or a thesaurus.
And the SVD isused to extract the significant sentences.
After performing SVD, a sentence-term matrix is decomposed into three matrices; that is, a sentence-concentratedmatrix, a term-concentrated matrix, and a singular value matrix.
The distancesbetween significant term vectors and sentence vectors can be calculated by usinga sentence-concentrated matrix and a term-concentrated matrix.
The shorterthe distance is, the more important the sentence is.
In a word, to produce thesummary of a document, we first identify significant terms by the term-termrelationships of being generated by PCA, and second extract the significantsentences by the distances betweeen significant term vectors and all sentencevectors.This paper is organized as follows.
Section 2 and 3 describe the way to identifythe significant terms by PCA, and extract the significant sentences by SVD,respectively.
Section 4 reports experimental results.
A brief conclusion is givenin Section 5.
And this paper enlarges [7] whose main content is to find out thesignificant terms by PCA.2 Significant Term Extraction by PCA2.1 PCA Overview and Its ApplicationIn this subsection, we will outline PCA which is adapted from [6] and which isused to extract the significant terms in the document.PCA is concerned with explaining the variance?covariance structure througha few linear combinations of the original variables.
Its general objective is datareduction and interpretation.
Algebraically, principal components are particularlinear combinations of the p random variables X1, X2, ?
?
?
, Xp.
Geometrically,these linear combinations represent the selection of a new coordinate systemobtained by rotating the orginal system with X1, X2, ?
?
?
, Xp as the coordinateaxes.638 C. Lee, H. Park, and C. OckPCA uses the covariance matrix (i.e., term-term correlation or cooccurrencematrix) instead of the obsevation-variable matrix (i.e., sentence-term matrix)such as Table 2.
Let ?
be the covariance matrix associated with the random vec-tor XT = [X1, X2, ?
?
?
, Xp].
Let ?
have the eigenvalue-eigenvector pairs (?1, e1),(?2, e2), .
.
.
, (?p, ep) where ?1 ?
?2 ?
.
.
.
?p ?
0 .
The ith principal compo-nent(PC) is given byYi = eTi X = e1iX1 + e2iX2 + ?
?
?
+ epiXp, i = 1, 2, .
.
.
, p (1)The first PC is the linear combination with maximum variance, and has thewidest spread in a new coordinate system geometrically.
Consequently, we cansay that it can cover the distribution of term frequency of a document as wideas possible, and also say that it has the power of explanation of the distributionas large as possible (but, not considering the meaning).The PCs are uncorrelated and have variances equal to the eigenvalues of ?,and the proportion of total population variance due to the ith PC, ?i, is?i =?i?1 + ?2 + .
.
.
+ ?p, i = 1, 2, .
.
.
, p (2)If most (80 ?
90%) of the total population variance, for large p, can be attributedto the first one, two, or three components, then these components can ?replace?the original p variables without much loss of information.
The first i PCs havealso maximal mutual information with respect to the inputs among projectionsonto all possible i directions, and mutual information is given by I(X, Y ) =H(X) ?
H(X |Y ) [5].As we expected, all terms of the document are not necessary to represent thecontent (i.e., term frequency distribution) of the document by using a few first iPCs, because they have most of the total population variance and maximal mutualinformation as noted earlier.
In addition, since PCA exploits a covariance matrix,we can use the term-term relationships by eigenvalues and eigenvectors withoutadditional information resources.
In the next subsection, we will describe how toextract the significant terms by eigenvalue-eigenvector pairs of a few first i PCs.2.2 Extracting Significant Terms by Eigenvalue-Eigenvector PairsWe assume that the candidates for the significant terms are confined only tonouns occurred more than 2 times in a document.
We also regard the sentences asobservations, the extracted nouns (terms) as variables, and the value of variablesas the number of occurrence of terms in each sentence (cf.
cumulative frequencyof the document in [7] ) .Table 1 shows the term list extracted from one of the Korean newspaper arti-cles composed of 12 sentences, and all of these terms have the occurrences morethan twice in the document.
Since the terms occurred just once are not reason-able to be representative nouns, we do not consider such terms.
In our samplearticle, 9 terms are extracted for the cadidates for the significant ones as shown inTable 1.
The sample article has 12 sentences (observations) originally, but thereSignificant Sentence Extraction by Euclidean Distance 639Table 1.
Variable (term) listvariable notationdae-tong-ryeong (president) X1mun-je (problem) X2guk-ga (nation) X3sa-ram (person) X4bu-jeong-bu-pae (illegality and corruption) X5gyeong-je (economy) X6guk-min (people) X7bang-beop (method) X8dan-che-jang (administrator) X9Table 2.
Observation-variable (sentence-term) matrixobs.
\ var.
X1 X2 X3 X4 X5 X6 X7 X8 X91 1 0 0 0 0 0 0 0 02 2 1 0 0 0 0 0 0 03 0 0 1 1 0 0 0 0 04 0 0 1 1 1 0 0 0 05 0 0 0 0 1 2 0 0 06 0 0 1 0 0 0 1 0 07 0 1 0 0 0 0 1 1 08 1 1 0 0 0 0 0 0 09 0 0 0 0 0 0 0 1 2are 9 ones in sentence-term matrix as shown in Table 2.
The only reason for thisdifference was that the sentences, which did not include the 9 extracted terms atall, were omitted.
In Table 2, the column under the head X1 shows the freqeuncyof X1, that is, X1 occurred once in first sentence, twice in second sentence, once ineighth sentence.
In Table 3, the 9 PCs are obtained after performning PCA withthe 9 variables.
The column under the head PC1 shows the eigenvector of thefirst PC, for instance,??
?PC1 = (?0.713, ?0.417, ?
?
?
, 0.025, 0.111).
Its eigenvalueis 0.871, and its proportion of total population is 32.34% computed by Eq.
(2).There are two steps to extract the significant terms by eigenvales and eigen-vectors as shown in Table 3.
First we need to decide how many PCs are selected,and second to find out how to express the each selected PC.
In order to select afew most salient PCs, we can make good use of cumulative ratio of eigenvalues.For example, the first four PCs can justify more than 90% (91.28%) of the totalsample variance, we can choose them without much loss information.
In otherwords, sample variance is summarized very well by these four PCs and the datafrom 9 observations on 9 variables can be reasonably reduced to 9 observationson 4 PCs.
Until now, we could not know what the selected PCs represent ex-actly, but we could describe them by their coefficients approximately.
A PC canbe represented by linear combination of variables multiplied by their respectivecoefficient.
For instance,640 C. Lee, H. Park, and C. OckTable 3.
Eigenvector and corresponding eigenvalue of each PCvar.
\ PC PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9X1 -0.713 0.220 0.068 -0.334 0.131 0.300 0.035 -0.467 0.000X2 -0.417 -0.003 0.010 0.337 -0.596 0.044 0.518 0.295 0.000X3 0.303 0.151 -0.490 -0.083 0.144 0.289 0.529 -0.139 -0.485X4 0.234 0.149 -0.333 -0.292 -0.498 -0.248 0.074 -0.421 0.485X5 0.278 0.268 0.212 -0.096 -0.396 0.726 -0.309 0.134 0.000X6 0.281 0.337 0.710 0.135 0.076 -0.151 0.402 -0.310 0.000X7 0.038 -0.111 -0.181 0.653 0.258 0.375 0.038 -0.288 0.485X8 0.025 -0.464 0.092 0.236 -0.358 -0.028 -0.236 -0.548 -0.485X9 0.111 -0.703 0.234 -0.416 0.050 0.267 0.362 0.043 0.243eigenvalue(?i) 0.871 0.676 0.563 0.349 0.134 0.049 0.035 0.017 0.000cumulative ratio(%) 32.34 57.42 78.31 91.28 96.25 98.07 99.38 100.00 100.00PC1 = ?0.713 ?
X1 ?
0.417 ?
X2 + 0.303 ?
X3 + ?
?
?
+ 0.111 ?
X9 (3)Since the coefficients represent the degree of relationship between variables anda PC, the variables with coefficient higher than 0.5 can be reasonably used toexpress the PC.
When the PC has not such coefficient higher than 0.5, thevariable with the highest coefficient can be also used to represent the PC.
Forexample, in table 3, the correlation coefficient between PC1 and X3 is 0.303,so X3 can be selected for the description of PC1.
As the most part variance ofa document justified by some of the PCs, we selected variables which have astrong correlation (?
0.5 or highest) with one of these PCs as significant terms.In our example, the extracted significant terms from PC1 to PC4 are X3, X6and X7.3 Significant Sentence Extraction by SVD3.1 SVD Overview and Its ApplicationWe will give an outline of SVD adapted from [3,9] and how to make good use ofextracting the significant sentences.Let A be any rectangular matrix, for instance an S ?
T matrix of sentencesand terms, such as Table 2.
The matrix A can be written as the product of anS?R column-orthogonal matrix U , an R?R daigonal matrix W with positive orzero elements (i.e., the singular values), and the transpose of a T ?R orthogonalmatrix V .
Here, R is the rank of the matrix A (R ?
min(S, T )).
The SVDdecompositon is shown in Eq.
(4).A = U ?
W ?
V T (4)where UT U = I, V T V = I, and W is the diagonal matrix of singualr val-ues.
In contrast to our usage of SVD, [3] used term-document matrix: ourSignificant Sentence Extraction by Euclidean Distance 641sentence-term matrix can be regarded as the transpose of term-document ma-trix, since the documents can be thought of the sentences in the summarizationfields.In this regard, the matrix A can be regarded as the sentence-term matrixlike Table 2, U as the sentence-concentrated matrix whose number of rows isequal to the number of rows of the matrix A, and V as the term-concentratedmatrix whose number of rows is equal to the number of columns of the matrixA.
Then, the sentence vector si is defined as si = (ui1, ui2, ?
?
?
, uiR) where R isthe rank of the matrix A.
As before, the vector for a term tj is represented bytj = (vj1, vj2, ?
?
?
, vjR).
Consequently, both the sentence and term vectors canbe used to calculate the distances between them.Actually the reduced dimensionality can be used instead of the full rank, R,by the cumulative ratio of the singular values.
The cumulative ratio, ?k, canbe calculated by Eq.
(5).
When the ?k is more than 90%, k can be selectedfor the reduced dimensionality.
And this is large enough to capture most of theimportant underlying structure in association of sentences and terms, and alsosmall enough to remove the noise of variability in term usage.
?k =?ki=1 wiw1 + w2 + .
.
.
+ wR, k = 1, 2, .
.
.
, R (5)To extract the significant sentences, in the first step, the Euclidean distancescan be computed between all the sentence vectors and the significant term vec-tors (not all the term vectors).
In this regard, the shorter the distance is, themore important the sentence is, since the significant terms can be described asrepresentative words of a document.
In the second step, the sentences are ex-tracted by means of these Euclidean distances, and then these are included inthe summary in the order of their sequences.
And the number of the includedsentences is depend on the compression rate of user?s need.3.2 Extracting Significant Sentences by the Decomposed MatricesIn this subsection, we will illustrate how to extract the significant sentences byexamples.
In [7], the importance of each sentence is computed by repeatedlysumming 1 for each occurrence of significant terms in the sentence.
However,the proposed method can be regarded as more formal or reasonable, since theEuclidean distance between vectors is used to calculate the degree of importanceof each sentence.Computing the SVD of the sentence-term matrix as shown in Table 2 re-sults in the following three matrices for U ?, W ?, V ?.
The matrices are reducedby the cumulative ratio of the singular values computed by Eq.
(5).
Since thefirst six singular values can justify 92.31% of the total, the 9-dimension canbe reduced to 6-dimension.
Thus, the sentence-concentrated matrix, U ?, andthe term-concentrated matrix, V ?, can be represented only by 6-dimensionalvectors.
The U ?
and V ?
are the vectors for the 9 sentences and 9 terms re-spectively.
The diagonal matrix W ?
shows the first six values (originally, ninevalues).642 C. Lee, H. Park, and C. OckU?
=??????????????
?0.289 0.032 ?0.086 0.022 ?0.195 0.293?0.771 0.063 ?0.153 0.033 ?0.182 0.094?0.015 ?0.367 ?0.019 ?0.441 ?0.209 ?0.077?0.017 ?0.577 ?0.067 ?0.366 ?0.260 ?0.344?0.006 ?0.657 ?0.189 0.704 0.118 0.085?0.052 ?0.269 0.070 ?0.363 0.354 0.767?0.280 ?0.101 0.320 ?0.094 0.750 ?0.371?0.481 0.031 ?0.067 0.011 0.013 ?0.198?0.094 ?0.115 0.904 0.181 ?0.340 0.092??????????????W?
=(2.826 2.424 2.314 2.117 1.672 0.984)V?
=??????????????
?0.818 0.078 ?0.199 0.047 ?0.326 0.288?0.542 ?0.003 0.043 ?0.024 0.348 ?0.483?0.030 ?0.500 ?0.007 ?0.553 ?0.069 0.352?0.011 ?0.389 ?0.037 ?0.381 ?0.281 ?0.427?0.008 ?0.509 ?0.111 0.160 ?0.085 ?0.263?0.004 ?0.542 ?0.163 0.666 0.141 0.173?0.118 ?0.153 0.168 ?0.216 0.660 0.402?0.132 ?0.089 0.529 0.041 0.245 ?0.284?0.066 ?0.095 0.781 0.171 ?0.407 0.187?????????????
?The Euclidean distances between the significant term vectors and the sen-tence vectors can be computed by above two matrices, V ?
and U ?, to extractthe significant sentences.
The significant term vectors are the third, sixth andseventh rows in the V ?.
The significant sentence by X3, for instance, is the thirdsentence of the document, since the distance between them is the shortest.
Allthe distances from X3, X6 and X7 vectors are shown in Table 4.
Consequently,the three significant sentences (third, fifth and sixth) can be included in thesummary of our sample article.
When the number of the selected sentences areless than that of user?s need, the summary can be supplemented with the othersentences by their distances.4 ExperimentsWe compared the proposed method with both only PCA[7] and random selectionof sentences.The [7] selected the significant sentences by the appearance of the signifi-cant terms.
The [7] also exploited from one to three consecutive sentences forthe observation of PCA; however, the performance of extracting the significantsentences was similar.
In this paper, we used each sentence within a documentfor the observation of PCA.To extract sentences randomly, first, random numbers amounting to 30% ofthe total number of sentences in a document were created, and then the sentenceswere extracted by these random numbers.We tried out the proposed method with two ways.
First, the sentences wereextracted by the distances of each significant term as described in subsectionSignificant Sentence Extraction by Euclidean Distance 643Table 4.
Euclidean distances between all the sentence vectors and the significant termvectors (X3, X6 and X7)distancenum.
of the sentence X3 X6 X71 0.841 0.979 0.9042 1.144 1.210 1.2013 0.484 1.209 1.0624 0.752 1.226 1.2935 1.321 0.154 1.2796 0.668 1.260 0.5267 1.317 1.322 0.8218 1.057 1.071 1.0269 1.289 1.342 1.341Table 5.
Evaluation resultMethodPCA & SVDMeasure Random PCA All EachAverage Precision 0.256 0.386 0.395 0.407Average Recall 0.413 0.451 0.486 0.500F-Measure 0.316 0.416 0.436 0.4493.2.
Second, the sentences were selected by all the distances of all the signif-icant terms.
In Table 5, ?All?
and ?Each?
denote the latter and the former,respectively.We used 127 documents of Korean newspaper articles for the evaluation,which were compiled by KISTI(Korea Institute of Science & Technology In-formation).
Each document consists of orginal article and manual summaryamounting to 30% of the source text.
We regarded the sentences within thismanual summary as correct ones.We use three measures to evaluate the methods: precision, recall, and F-measure.
Let?
Count(SystemCorrect) denote the number of correct sentences that the sys-tem extracts.?
Count(SystemExtract) denote the number of sentences that the system ex-tracts.?
Count(Correct) denote the number of correct sentences provided in the testcollection.The measures are defined respectively as follows.Precision =Count(SystemCorrect)Count(SystemExtract)644 C. Lee, H. Park, and C. OckRecall =Count(SystemCorrect)Count(Correct)F ?
Measure = 2 ?
Precision ?
RecallPrecision + RecallTable 5 shows that, by means of F-measure, the proposed method has im-proved the performance by about 2% ?
3.3% over only PCA, and by about 12%?
13.3% over random selection.
Table 5 also shows that the ?Each?
method issuperior to ?All?.
Furthermore, the performance of using PCA is better thanthat of using term frequency or a thesaurus [7].5 ConclusionIn this paper, we have proposed a summarization approach that constructs asummary by extracting sentences in a single document.
The particular tech-niques used are PCA and SVD for extracting the significant terms and sentencesrespectively.PCA can quantify the information on both the term frequency and the term-term cooccurrence in the document by the eigenvalue-eigenvector pairs.
Thesepairs were used to find out the significant terms among the nouns in the docu-ment.
In addition, these terms can be regarded as those extracted by relation-ships between terms in the document, since PCA exploits the variance-covariancestructure.In contrast to PCA, SVD has the inforamtion on the sentences and terms aftercomputing it of sentence-term matrix.
In this regard, we can use the decomposedmatrices to calculate the distances between the sentence and term vectors, andto make an effective removal of the noise of variability in term usage.Experimental results on Korean newspaper articles show that the proposedmethod is superior to the methods of both random selection and only usingPCA, and that extracting sentences by the distances per each term is betterperformance than extracting by all the distances of all the terms.In conclusion, the information on the cooccurrence relationships betweenthe terms by the PCA and the vector expressions of the sentences and termsby the SVD can be helpful for the text summarization.
Furthermore, the pro-posed methods only exploited the pattern of the statistical occurrences withina document without the additional resources like a thesaurus to find out therelationships between the terms, and the proper dimension of the vectors.AcknowledgementsThis research was supported by the MIC(Ministry of Information and Com-munication), Korea, under the ITRC(Information Technology Research Center)support program supervised by the IITA(Institute of Information TechnologyAssessment)Significant Sentence Extraction by Euclidean Distance 645References1.
Baeza-Yates, R., Ribeiro-Neto, B.: Modern Information Retrieval.
New York: ACMPress (1999)2.
Barzilay, R., Elhadad, M. : Using Lexical chains for Text Summarization.
In: I. Mani,& M. T. Maybury (eds.
): Advances in automatic text summarization.
Cambridge,MA: The MIT Press (1999) 111?121.3.
Deerwester, S., Dumais, S. T., Harshman, R.: Indexing by latent semantic analysis.Journal of the American Society for Information Science, 41 (6).
(1990) 381?4074.
Edmundson, H. P.: New Methods in Automatic Extracting.
In: I. Mani, & M. T.Maybury (eds.
): Advances in automatic text summarization.
Cambridge, MA: TheMIT Press (1999) 23?42.5.
Haykin, S. S.: Neural networks: A comprehensive foundation.
2nd edn.
Paramus,NJ: Prentice Hall PTR (1998)6.
Johnson, R. A., Wichern, D. W.: Applied Multivariate Statistical Analysis.
3rd edn.NJ: Prentice Hall (1992)7.
Lee, C., Kim, M., Park, H.: Automatic Summarization Based on Principal Compo-nent Analysis.
In: Pires, F.M., Abreu, S.
(eds.
): Progress in Artificial Intelligence.Lecture Notes in Artificial Intelligence, Vol.
2902.
Springer-Verlag, Berlin HeidelbergNew York (2003) 409?4138.
Luhn, H. P.: The Automatic Creation of Literature Abstracts.
In: I. Mani, & M. T.Maybury (eds.
): Advances in automatic text summarization.
Cambridge, MA: TheMIT Press (1999) 15?21.9.
Press, W. H., Teukolsky, S. A., Vetterling, W. T., Flannery, B. P.: Numerical recipesin C++.
2nd edn.
New York: Cambridge University Press (1992)
