Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006, pages 21?28,Sydney, July 2006. c?2006 Association for Computational LinguisticsProbing the space of grammatical variation:induction of cross-lingual grammatical constraints from treebanksFelice Dell?OrlettaUniversit?
di Pisa, Dipartimento diInformatica - Largo B. Pontecorvo 3ILC-CNR - via G. Moruzzi 156100 Pisa, Italyfelice.dellorletta@ilc.cnr.itAlessandro LenciUniversit?
di Pisa, Dipartimento diLinguistica - Via Santa Maria 3656100 Pisa, Italyalessandro.lenci@ilc.cnr.itSimonetta MontemagniILC-CNR - via G. Moruzzi 156100 Pisa, Italysimonetta.montemagni@ilc.cnr.itVito PirrelliILC-CNR - via G. Moruzzi 156100 Pisa, Italyvito.pirrelli@ilc.cnr.itAbstractThe paper reports on a detailedquantitative analysis of distributionallanguage data of both Italian and Czech,highlighting the relative contribution of anumber of distributed grammaticalfactors to sentence-based identification ofsubjects and direct objects.
The workuses a Maximum Entropy model ofstochastic resolution of conflictinggrammatical constraints and isdemonstrably capable of puttingexplanatory theoretical accounts to thetest of usage-based empirical verification.1 IntroductionThe paper illustrates the application of aMaximum Entropy (henceforth MaxEnt) model(Ratnaparkhi 1998) to the processing of subjectsand direct objects in Italian and Czech.
Themodel makes use of richly annotated Treebanksto determine the types of linguistic factorsinvolved in the task and weigh up their relativesalience.
In doing so,  we set ourselves a two-fold goal.
On the one hand, we intend to discussthe use of Treebanks to discover typologicallyrelevant and linguistically motivated factors andassess the relative contribution of the latter tocross-linguistic parsing issues.
On the otherhand, we are interested in testing the empiricalplausibility of constraint-resolution models oflanguage processing (see infra) when confrontedwith real language data.Current research in natural language learningand processing supports the view thatgrammatical competence consists in masteringand integrating multiple, parallel constraints(Seidenberg and MacDonald 1999, MacWhinney2004).
Moreover, there is growing consensus ontwo major properties of grammatical constraints:i.)
they are probabilistic ?soft constraints?
(Bresnan et al 2001), and ii.)
they have aninherently functional nature, involving differenttypes of linguistic (and non linguistic)information (syntactic, semantic, etc.).
Thesefeatures emerge particularly clearly in dealingwith one of the core aspects of grammarlearning: the ability to identify syntactic relationsin text.
Psycholinguistic evidence shows thatspeakers learn to identify sentence subjects anddirect objects by combining various types ofprobabilistic, functional cues, such as wordorder, noun animacy, definiteness, agreement,etc.
An important observation is that the relativeprominence of each such cue can considerablyvary cross-linguistically.
Bates et al (1984), forexample, argue that while, in English, word orderis the most effective cue for Subject-ObjectIdentification (henceforth SOI) both in syntacticprocessing and during the child?s syntacticdevelopment, the same cue plays second fiddle inrelatively free phrase-order languages such asItalian or German.If grammatical constraints are inherentlyprobabilistic (Manning 2003), the path throughwhich adult grammar competence is acquired canbe viewed as the process of building a stochasticmodel out of the linguistic input.
Incomputational linguistics, MaxEnt models have21proven to be robust statistical learning algorithmsthat perform well in a number of processingtasks.
Being supervised learning models, theyrequire richly annotated data as training input.Before we turn to the use of Treebanks fortraining a MaxEnt model for SOI, we firstanalyse the range of linguistic factors that aretaken to play a significant role in the task.2 Subjects and objects in Czech andItalianGrammatical relations - such as subject (S) anddirect object (O) - are variously encoded inlanguages, the two most widespread strategiesbeing: i) structural encoding through word order,and ii) morpho-syntactic marking.
In turn,morpho-syntactic marking can apply either onthe noun head only, in the form of caseinflections, or on both the noun and the verb, inthe form of agreement marking (Croft 2003).Besides formal coding, the distribution ofsubjects and object is also governed by semanticand pragmatic factors, such as noun animacy,definiteness, topicality, etc.
As a result, thereexists a variety of linguistic clues jointly co-operating in making a particular noun phrase thesubject or direct object of a sentence.
Cruciallyfor our present purposes, cross-linguisticvariation does not only concern the particularstrategy used to encode S and O, but also therelative strength that each factor plays in a givenlanguage.
For instance, while English word orderis by and large the dominant clue to identify Sand O, in other languages the presence of a richmorphological system allows word order to havea much looser connection with the coding ofgrammatical relations, thus playing a secondaryrole in their identification.
Moreover, there arelanguages where semantic and pragmaticconstraints such as animacy and/or definitenessplay a predominant role in the processing ofgrammatical relations.
A large spectrum ofvariations exists, ranging from languages whereS must have a higher degree of animacy and/ordefiniteness relative to O, to languages wherethis constraint only takes the form of a softerstatistical preference (cf.
Bresnan et al 2001).The goal of this paper is to explore the area ofthis complex space of grammar variation throughcareful assessment of the distribution of S and Otokens in Italian and Czech.
For our presentanalysis, we have used a MaxEnt statisticalmodel trained on data extracted from twosyntactically annotated corpora: the PragueDependency Treebank (PDT, Bohmova et al2003) for Czech, and the Italian SyntacticSemantic Treebank (ISST, Montemagni et al2003) for Italian.
These corpora have beenchosen not only because they are the largestsyntactically annotated resources for the twolanguages, but also because of their high degreeof comparability, since they both adopt adependency-based annotation scheme.Czech and Italian provide an interestingvantage point for the cross-lingual analysis ofgrammatical variation.
They are both Indo-European languages, but they do not belong tothe same family: Czech is a West Slavoniclanguage, while Italian is a Romance language.For our present concerns, they appear to sharetwo crucial features: i) the free order ofgrammatical relations with respect to the verb; ii)the possible absence of an overt subject.Nevertheless, they also greatly differ due to: thevirtual non-existence of case marking in Italian(with the only marginal exception of personalpronouns), and the degree of phrase-orderfreedom in the two languages.
Empiricalevidence supporting the latter claim is providedin Table 1, which reports data extracted fromPDT and ISST.
Notice that although in bothlanguages S and O can occur either pre-verballyor post-verbally, Czech and Italian greatly differin their propensity to depart from the (unmarked)SVO order.
While in Italian preverbal O ishighly infrequent (1.90%), in Czech more than30% of O tokens occur before the verb.
Thesituation is similar but somewhat more balancedin the case of S, which occurs post-verbally in22.21% of the Italian cases, and  in  40% ofCzech ones.
For sure, one can argue that, inspoken Italian, the number of pre-verbal objectsis actually higher, because of the greater numberof left dislocations and topicalizations occurringin informal speech.
However reasonable, theobservation does not explain away thedistributional differences in the two corpora,since both PDT and ISST contain writtenlanguage only.
We thus suggest that there is clearempirical evidence in favour of a systematic,higher phrase-order freedom in Czech, arguablyrelated to the well-known correlation of Czechconstituent placement with sentence informationstructure, with the element carrying newinformation showing a tendency to occursentence-finally (Stone 1990).
For our presentconcerns, however, aspects of informationstructure, albeit central in Czech grammar, werenot taken into  account, as they  happen not to  be22Czech ItalianSubj Obj Subj ObjPre 59.82% 30.27% 77.79% 1.90%Post 40.18% 69.73% 22.21% 98.10% PosAll 100.00% 100.00% 100.00% 100.00%Agr 98.50% 56.54% 97.73% 58.33%NoAgr 1.50% 43.46% 2.27% 41.67% AgrAll 100.00% 100.00% 100.00% 100.00%Anim 34.10% 15.42% 50.18% 10.67%NoAnim 65.90% 84.58% 49.82% 89.33% AnimAll 100.00% 100.00% 100.00% 100.00%Table 1 ?Distribution of Czech and Italian S and O wrt word order,agreement and noun animacyCzechSubj ObjNominative 53.83% 0.65%Accusative 0.15% 28.30%Dative 0.16% 9.54%Genitive 0.22% 2.03%Instrumental 0.01% 3.40%Ambiguous 45.63% 56.08%All 100.00% 100.00%Table 2 - Distribution of Czech S and Owrt casemarked-up in the Italian corpus.According to the data reported in Table 1,Czech and Italian show similar correlationpatterns between animacy and grammaticalrelations.
S and O in ISST were automaticallyannotated for animacy using the SIMPLE Italiancomputational lexicon (Lenci et al 2000) as abackground semantic resource.
The annotationwas then checked manually.
Czech S and O wereannotated for animacy using Czech WordNet(Pala and Smrz 2004); it is worth remarking thatin Czech animacy annotation was done onlyautomatically, without any manual revision.Italian shows a prominent asymmetry in thedistribution of animate nouns in subject andobject roles: over 50% of ISST subjects areanimate, while only 10% of the objects areanimate.
Such a trend is also confirmed in Czech?
although to a lesser extent - with 34.10% ofanimate subjects vs. 15.42% of objects.1 Such anoverwhelming preference for animate subjects incorpus data suggests that animacy may play avery important role for S and O identification inboth languages.Corpus data also provide interesting evidenceconcerning the actual role of morpho-syntacticconstraints in the distribution of grammaticalrelations.
Prima facie, agreement and case arethe strongest and most directly accessible cluesfor S/O processing, as they are marked bothovertly and locally.
This is also confirmed bypsycholinguistic evidence, showing that subjectstend to rely on these clues to identify S/O.However, it should be observed that agreementcan be relied upon conclusively in S/Oprocessing only when a nominal constituent and1 In fact, the considerable difference in animacy distributionbetween the two languages might only be an artefact of theway we annotated Czech nouns semantically, on the basis oftheir context-free classification in the Czech WordNet.a verb do not agree in number and/or person (asin leggono il libro ?
(they) read the book?
).Conversely, when N and V share the sameperson and number, no conclusion can be drawn,as trivially shown by a sentence like il bambinolegge il libro ?the child reads the book?.
In ISST,more than 58% of O tokens agree with theirgoverning V, thus being formallyindistinguishable from S on the basis ofagreement features.
PDT also exhibits a similarratio, with 56% of O tokens agreeing with theirverb head.
Analogous considerations apply tocase marking, whose perceptual reliability isundermined by morphological syncretism,whereby different  cases are realized through thesame marker.
Czech data reveal the massiveextent of this phenomenon and its impact on SOI.As reported in Table 2, more than 56% of Otokens extracted from PDT are formallyindistinguishable from S in case ending.Similarly, 45% of S tokens are formallyindistinguishable from O uses on the sameground.
All in all, this means that in 50% of thecases a Czech noun can not be understood as theS/O of a sentence by relying on overt casemarking only.To sum up, corpus data lend support to theidea that in both Italian and in Czech SOI isgoverned by a complex interplay of probabilisticconstraints of a different nature (morpho-syntactic, semantic, word order, etc.)
as the latterare neither singly necessary nor jointly sufficientto attack the processing task at hand.
It istempting to hypothesize that the joint distributionof these data can provide a statistically reliablebasis upon which relevant probabilisticconstraints are bootstrapped and combinedconsistently.
This should be possible due to i) thedifferent degrees of clue salience in the twolanguages and ii) the functional need to minimize23processing ambiguity in ordinary communicativeexchanges.
With reference to the latter point, forexample, we may surmise that a speaker will bemore inclined to violate one constraint on S/Odistribution (e.g.
word order) when another clueis available (e.g.
animacy) that strongly supportsthe intended interpretation only.
The followingsection illustrates how a MaxEnt model can beused to model these intuitions by bootstrappingconstraints and their interaction from languagedata.3 Maximum Entropy modellingThe MaxEnt framework offers a mathematicallysound way to build a probabilistic model for SOI,which combines different linguistic cues.
Givena linguistic context c and an outcome a?A thatdepends on c, in the MaxEnt framework theconditional probability distribution p(a|c) isestimated on the basis of the assumption that noa priori constraints must be met other than thoserelated to a set of features fj(a,c) of c, whosedistribution is derived from the training data.
Itcan be proven that the probability distribution psatisfying the above assumption is the one withthe highest entropy, is unique and has thefollowing exponential form (Berger et al 1996):(1) ?==kjcafjjcZcap 1),()(1)|( awhere Z(c) is a normalization factor, fj(a,c) arethe values of k features of the pair (a,c) andcorrespond to the linguistic cues of c that arerelevant to predict the outcome a.
Features areextracted from the training data and define theconstraints that the probabilistic model p mustsatisfy.
The parameters of the distribution ?1, ?,?k correspond to weights associated with thefeatures, and determine the relevance of eachfeature in the overall model.
In the experimentsreported below feature weights have beenestimated with the Generative Iterative Scaling(GIS) algorithm implemented in the AMISsoftware (Miyao and Tsujii 2002).We model SOI as the task of predicting thecorrect syntactic function ?
?
{subject, object}of a noun occurring in a given syntactic context?.
This is equivalent to building the conditionalprobability distribution p(?|?)
of having asyntactic function ?
in a syntactic context ?.Adopting the MaxEnt approach, the distributionp can be rewritten in the parametric form of (1),with features corresponding to the linguisticcontextual cues relevant to SOI.
The context ?
isa pair <v?, n?>, where v?
is the verbal head and n?its nominal dependent in ?.
This notion of ?departs from more traditional ways of describingan SOI context as a triple of one verb and twonouns in a certain syntactic configuration (e.g,SOV or VOS, etc.).
In fact, we assume that SOIcan be stated in terms of the more local task ofestablishing the grammatical function of a nounn observed in a verb-noun pair.
This simplifyingassumption is consistent with the claim inMacWhinney et al (1984) that SVO word orderis actually derivative from SV and VO localpatterns and downplays the role of the transitivecomplex construction in sentence processing.Evidence in favour of this hypothesis also comesfrom corpus data: for instance, in ISST completesubject-verb-object configurations represent only26% of the cases, a small percentage if comparedto the 74% of verb tokens appearing with either asubject or an object only; a similar situation canbe observed in PDT where complete subject-verb-object configurations occur in only 20% ofthe cases.
Due to the comparative sparseness ofcanonical SVO constructions in Czech andItalian, it seems more reasonable to assume thatchildren should pay a great deal of attention toboth SV and VO units as cues in sentenceperception (Matthews et al in press).Reconstruction of the whole lexical SVO patterncan accordingly be seen as the end point of anacquisition process whereby smaller units are re-analyzed as being part of more comprehensiveconstructions.
This hypothesis is more in linewith a distributed view of canonicalconstructions as derivative of more basic localpositional patterns, working together to yieldmore complex and abstract constructions.
Lastbut not least, assuming verb-noun pairs as therelevant context for SOI allows us tosimultaneously model the interaction of wordorder variation with pro-drop.4 Feature selectionThe most important part of any MaxEnt model isthe selection of the context features whoseweights are to be estimated from datadistributions.
Our feature selection strategy isgrounded on the main assumption that featuresshould correspond to theoretically andtypologically well-motivated contextual cues.This allows us to evaluate the probabilisticmodel also with respect to its consistency withcurrent linguistic generalizations.
In turn, themodel can be used as a probe into thecorrespondence between theoretically motivated24generalizations and usage-based empiricalevidence.Features are binary functions fki,?
(?,?
), whichtest whether a certain cue ki for the feature ?occurs in the context ?.
For our MaxEnt model,we have selected different features types that testmorpho-syntactic, syntactic, and semantic keydimensions in determining the distribution of Sand O.Morpho-syntactic features.
These include N-Vagreement, for Italian and Czech, and case, onlyfor Czech.
The combined use of such featuresallow us not only to test the impact of morpho-syntactic information on SOI, but also to analyzepatterns of cross-lingual variation stemmingfrom language specific morphologicaldifferences, e.g.
lack of case marking in Italian.Word order.
This feature essentially test theposition of the noun wrt the verb, for instance:(2)???
== otherwisepostposnounifsubjf subjpost 0.1),(, ssAnimacy.
This is the main semantic feature,which tests whether the noun in ?
is animate orinanimate (cf.
section 2).
The centrality of thiscue for grammatical relation assignment iswidely supported by typological evidence (cf.Aissen 2003, Croft 2003).
The AnimacyMarkedness Hierarchy - representing the relativemarkedness of the associations betweengrammatical functions and animacy degrees ?
isactually assigned the role of a functionaluniversal principle in grammar.
The hierarchy isreported below, with each item in these scalesbeen less marked than the elements to its right:Animacy Markedness HierarchySubj/Human > Subj/Animate > Subj/InanimateObj/Inanimate > Obj/Animate > Obj/HumanMarkedness hierarchies have also beeninterpreted as probabilistic constraints estimatedfrom corpus data (Bresnan et al 2001).
In ourMaxEnt model we have used a reduced versionof the animacy markedness hierarchy in whichhuman and animate nouns have been bothsubsumed under the general class animate.Definiteness tests the degree of ?referentiality?
ofthe noun in a context pair ?.
Like for animacy,definiteness has been claimed to be associatedwith grammatical functions, giving rise to thefollowing universal markedness hierarchy Aissen(2003):Definiteness Markedness HierarchySubj/Pro > Subj/Name > Subj/Def > Subj/IndefObj/Indef > Obj/Def > Obj/Name > Obj/ProAccording to this hierarchy, subjects with a lowdegree of definiteness are more marked thansubjects with a high degree of definiteness (forobjects the reverse pattern holds).
Given theimportance assigned to the definitenessmarkedness hierarchy in current linguisticresearch, we have included the definiteness cuein the MaxEnt model.
In our experiments, forItalian we have used a compact version of thedefiniteness scale: the definiteness cue testswhether the noun in the context pair i) is a nameor a pronoun ii) has a definite article iii), has anindefinite article or iv) is a bare noun (i.e.
withno article).
It is worth saying that bare nouns areusually placed at the bottom end of thedefiniteness scale.
Since in Czech there is noarticle, we only make a distinction betweenproper names and common nouns.5 Testing the modelThe Italian MaxEnt model was trained on 14,643verb-subject/object pairs extracted from ISST.For Czech, we used a training corpus of 37,947verb-subject/object pairs extracted from PDT.
Inboth cases, the training set was obtained byextracting all verb-subject and verb-objectdependencies headed by an active verb, with theexclusion of all cases where the position of thenominal constituent was grammaticallydetermined (e.g.
clitic objects, relative clauses).It is interesting to note that in both training setsthe proportion of subjects and objects relations isnearly the same: 63.06%-65.93% verb-subjectpairs and 36.94%-34.07% verb-object pairs forItalian and Czech respectively.The test corpus consists of a set of verb-nounpairs randomly extracted from the referenceTreebanks: 1,000 pairs for Italian and 1,373 forCzech.
For Italian, 559 pairs contained a subjectand 441 contained an object; for Czech, 905pairs contained a subject and 468 an object.Evaluation was carried out by calculating thepercentage of  correctly  assigned  relations  overthe total number of test pairs (accuracy).
As ourmodel always assigns one syntactic relation toeach test pair, accuracy equals both standardprecision and recall.25Czech ItalianSubj Obj Subj ObjPreverb 1.99% 19.40% 0.00% 6.90%Postverb 71.14% 7.46% 71.55% 21.55%Anim 0.50% 3.98% 6.90% 21.55%Inanim 72.64% 22.89% 64.66% 6.90%Nomin 0.00% 1.00%Genitive 0.50% 0.00%Dative 1.99% 0.00%Accus 0.00% 0.00%Instrum 0.00% 0.00%Ambig 70.65% 25.87%NaAgr 70.15% 25.87% 61.21% 12.07%NoAgr 2.99% 0.50% 7.76% 1.72%NAAgr 0.00% 0.50% 2.59% 14.66%Table 3 ?
Types of errors for Czech and ItalianCzech ItalianSubj Obj Subj ObjPreverb 1.24E+00 5.40E-01 1.31E+00 2.11E-02Postverb 8.77E-01 1.17E+00 5.39E-01 1.38E+00Anim 1.16E+00 6.63E-01 1.28E+00 3.17E-01Inanim 1.03E+00 9.63E-01 8.16E-01 1.23E+00PronName 1.13E+00 7.72E-01 1.13E+00 8.05E-01DefArt 1.01E+00 1.02E+00IndefArt 6.82E-01 1.26E+00NoArticle1.05E+00 9.31E-019.91E-01 1.02E+00Nomin 1.23E+00 2.22E-02Genitive 2.94E-01 1.51E+00Dative 2.85E-02 1.49E+00Accus 8.06E-03 1.39E+00Instrum 3.80E-03 1.39E+00NaAgr 1.18E+00 6.67E-01 1.28E+00 4.67E-01NoAgr 7.71E-02 1.50E+00 1.52E-01 1.58E+00NAAgr 3.75E-01 1.53E+00 2.61E-01 1.84E+00Table 4 - Feature value weights in NLC for Czech andItalianWe have assumed a baseline score of 56% forItalian and of 66% for Czech, corresponding tothe result yielded by a naive model   assigningto  each   test   pair  the   most frequent relationin the training corpus, i.e.
subject.
Experimentswere carried out with the general featuresillustrated in section 4: verb agreement, case (forCzech only), word order, noun animacy andnoun definiteness.Accuracy on the test corpus is 88.4% forItalian and 85.4% for Czech.
A detailed erroranalysis for the two languages is reported inTable 3, showing that in both languages subjectidentification appears to be particularlyproblematic.
In Czech, it appears that theprototypically mistaken subjects are post-verbal(71.14%), inanimate (72.64%), ambiguouslycase-marked (70.65%) and agreeing with theverb (70.15%), where reported percentages referto the whole error set.
Likewise, Italian mistakensubjects can be described thus: they typicallyoccur in post-verbal position (71.55%), aremostly inanimate (64.66%) and agree with theverb (61.21%).
Interestingly, in both languages,the highest number of errors occurs when a) Nhas the least prototypical syntactic and semanticproperties for O or S (relative to word order andnoun animacy) and b) morpho-syntactic featuressuch as agreement and case are neutralised.
Thisshows that MaxEnt is able to home in on the corelinguistic properties that govern the distributionof S and O in Italian and Czech, while remaininguncertain in the face of somewhat peripheral andoccasional cases.A further way to evaluate the goodness of fitof our model is by inspecting the weightsassociated with feature values for the twolanguages.
They are reported in Table 4, wheregrey cells highlight the preference of eachfeature value for either subject or objectidentification.
In both languages agreement withthe verb strongly relates to the subject relation.For Czech, nominative case is stronglyassociated with subjects while the other caseswith objects.
Moreover, in both languagespreverbal subjects are strongly preferred overpreverbal objects; animate subjects are preferredover animate objects; pronouns and propernames are typically subjects.Let us now try to relate these feature values tothe Markedness Hierarchies reported in section4.
Interestingly enough, if we rank the ItalianAnim and Inanim values for subjects and objects,we observe that they distribute consistently withthe Animacy Markedness Hierarchy: Subj/Anim> Subj/Inanim and Obj/Inanim > Obj/Anim.
Thisis confirmed by the Czech results.
Similarly, byranking the Italian values for the definitenessfeatures in the Subj column by decreasing weightvalues we obtain the following ordering:PronName > DefArt > IndefArt > NoArt, whichnicely fits in with the Definiteness MarkednessHierarchy in section 4.
The so-called?markedness reversal?
is replicated with a gooddegree of approximation, if we focus on thevalues for the same features in the Obj column:the PronName feature represents the mostmarked option, followed by IndefArt, DefArt andNoArt (the latter two showing the same featurevalue).
The exception here is represented by therelative ordering of IndefArt and DefArt whichhowever show very close values.
The same26seems to hold for Czech, where the featureordering for Subj is PronName >DefArt/IndefArt/NoArt and the reverse isobserved for Obj.5.1 Evaluating comparative feature salienceThe relative salience of the different constraintsacting on SOI can be inferred by comparing theweights associated with individual featurevalues.
For instance, Goldwater and Johnson(2003) show that MaxEnt can successfully beapplied to learn constraint rankings in OptimalityTheory, by assuming the parameter weights <?1,?, ?k> as the ranking values of the constraints.Table 5 illustrates the constraint ranking forthe two languages, ordered by decreasing weightvalues for both S and O.
Note that, although notall constraints are applicable in both languages,the weights associated with applicableconstraints exhibit the same relative salience inCzech and Italian.
This seems to suggest theexistence of a rather dominant (if not universal)salience scale of S and O processing constraints,in spite of the considerable difference in themarking strategies adopted by the two languages.As the relative weight of each constraintcrucially depends on its overall interaction withother constraints on a given processing task,absolute weight values can considerably varyfrom language to language, with a resultingimpact on the distribution of S and Oconstructions.
For example, the possibility ofovertly and unambiguously marking a directobject with case inflection makes wider room forpreverbal use of objects in Czech.
Conversely,lack of case marking in Italian considerablylimits the preverbal distribution of direct objects.This evidence, however, appears to be anepiphenomenon of the interaction of fairly stableand invariant preferences, reflecting commonfunctional tendencies in language processing.
Asshown in Table 5, if constraint ranking largelyconfirms the interplay between animacy andword order in Italian, Czech does not contradictit but rather re-modulate it somewhat, due to the?perturbation?
factors introduced by its richerbattery of case markers.6 ConclusionsProbabilistic language models, machine languagelearning algorithms and linguistic theorizing allappear to support a view of language processingas a process of dynamic, on-line resolution ofconflicting grammatical constraints.
We begin togain considerable insights into the complexprocess of bootstrapping nature and behaviour ofthese constraints upon observing their actualdistribution in perceptually salient contexts.
Inour view of things, this trend outlines apromising framework providing fresh support tousage-based models of language acquisitionthrough mathematical and computationalsimulations.
Moreover, it allows scholars toinvestigate patterns of cross-linguistictypological variation that crucially depend on theappropriate setting of model parameters.
Finally,it promises to solve, on a principled basis,traditional performance-oriented cruces ofgrammar theorizing such as degrees of humanacceptability of ill-formed grammaticalconstructions (Hayes 2000) and the inherentlygraded compositionality of linguisticconstructions such as morpheme-based wordsand word-based phrases (Bybee 2002, Hay andBaayen 2005).We argue that the current availability ofcomparable, richly annotated corpora and ofmathematical tools and models for corpusexploration make time ripe for probing the spaceof grammatical variation, both intra- and inter-linguistically, on unprecedented levels ofsophistication and granularity.
All in all, weanticipate that such a convergence is likely tohave a twofold impact: it is bound to shed lighton the integration of performance andcompetence factors in language study; it willmake mathematical models of languageincreasingly able to accommodate richer andricher language evidence, thus puttingexplanatory theoretical accounts to the test of ausage-based empirical verification.In the near future, we intend to pursue twoparallel lines of development.
First we wouldlike to increase the context-sensitiveness of ourprocessing task by integrating binarygrammatical constraints into the broader contextof multiply conflicting grammar relations.
Thisway, we will be in a position to capture theconstraint that a (transitive) verb has at most onesubject and one object, thus avoiding multipleassignment of subject (object) relations in thesame context.
Suppose, for example, that bothnouns in a noun-noun-verb triple are amenable toa subject interpretation, but that one of them is amore likely subject than the other.
Then, it isreasonable to expect the model to process theless likely subject candidate as the object of theverb in the triple.
Another promising line ofdevelopment is based on the observation that the27order in which verb arguments appear in contextis also lexically governed: in Italian, forexample, report verbs show a strong tendency toselect subjects post-verbally.
Dell?Orletta et al(2005) report a substantial improvement on themodel performance on Italian SOI when lexicalinformation is taken into account, as a lexicalizedMaxEnt model appears to integrate generalconstructional and semantic biases withlexically-specific preferences.
In a cross-lingualperspective, comparable evidence of lexicalconstraints on word order would allow us todiscover language-wide invariants in the lexicon-grammar interplay.ReferencesBates E., MacWhinney B., Caselli C., Devescovi A.,Natale F., Venza V. 1984.
A crosslinguistic studyof the development of sentence interpretationstrategies.
Child Development, 55: 341-354.Bohmova A., Hajic J., Hajicova E., Hladka B.
2003.The Prague Dependency Treebank: Three-LevelAnnotation Scenario, in A. Abeille (ed.
)Treebanks: Building and Using SyntacticallyAnnotated Corpora, Kluwer Academic Publishers,pp.
103-128.Bybee J.
2002.
Sequentiality as the basis ofconstituent structure.
in T. Giv?n and B.
Malle(eds.)
The Evolution of Language out of Pre-Language, Amsterdam: John Benjamins.
107-132.Croft W. 2003.
Typology and Universals.
SecondEdition, Cambridge University Press, Cambridge.Bresnan J., Dingare D., Manning C. D. 2001.
Softconstraints mirror hard constraints: voice andperson in English and Lummi.
Proceedings of theLFG01 Conference, Hong Kong: 13-32.Dell?Orletta F., Lenci A., Montemagni S., Pirrelli V.2005.
Climbing the path to grammar: a maximumentropy model of subject/object learning.Proceedings of the ACL-2005 Workshop?Psychocomputational Models of HumanLanguage Acquisition?, University of Michigan,Ann Arbour (USA), 29-30 June 2005.Hay J., Baayen R.H. 2005.
Shifting paradigms:gradient structure in morphology, Trends inCognitive Sciences, 9(7): 342-348.Hayes B.
2000.
Gradient Well-Formedness inOptimality Theory, in Joost Dekkers, Frank vander Leeuw and Jeroen van de Weijer (eds.
)Optimality Theory: Phonology, Syntax, andAcquisition, Oxford University Press, pp.
88-120.Lenci A. et al 2000.
SIMPLE: A General Frameworkfor the Development of Multilingual Lexicons.International Journal of Lexicography, 13 (4):249-263.MacWhinney B.
2004.
A unified model of languageacquisition.
In J. Kroll & A.
De Groot (eds.
),Handbook of bilingualism: Psycholinguisticapproaches, Oxford University Press, Oxford.Manning C. D. 2003.
Probabilistic syntax.
In R. Bod,J.
Hay, S. Jannedy (eds), Probabilistic Linguistics,MIT Press, Cambridge MA: 289-341.Miyao Y., Tsujii J.
2002.
Maximum entropyestimation for feature forests.
Proc.
HLT2002.Montemagni S. et al 2003.
Building the Italiansyntactic-semantic treebank.
In Abeill?
A.
(ed.)Treebanks.
Building and Using Parsed Corpora,Kluwer, Dordrecht: 189-210.Ratnaparkhi A.
1998.
Maximum Entropy Models forNatural Language Ambiguity Resolution.
Ph.D.Dissertation, University of Pennsylvania.Constraints for S  Constraints for OFeature Italian Czech  Feature Italian CzechPreverbal 1.31E+00 1.24E+00  Genitive na 1.51E+00Nomin na 1.23E+00  NoAgr 1.58E+00 1.50E+00Agr 1.28E+00 1.18E+00  Dative na 1.49E+00Anim 1.28E+00 1.16E+00  Accus na 1.39E+00Inanim 8.16E-01 1.03E+00  Instrum na 1.39E+00Postverbal 5.39E-01 8.77E-01  Postverbal 1.38E+00 1.17E+00Genitive na 2.94E-01  Inanim 1.23E+00 9.63E-01NoAgr 1.52E-01 7.71E-02  Agr 4.67E-01 6.67E-01Dative na 2.85E-02  Anim 3.17E-01 6.63E-01Accus na 8.06E-03  Preverbal 2.11E-02 5.40E-01Instrum na 3.80E-03  Nomin na 2.22E-02Table 5 ?
Ranked constraints for S and O in Czech and Italian28
