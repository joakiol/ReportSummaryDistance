EVALUATION OF MACHINE TRANSLATIONJohn S. White, Theresa A. O'ConnellPRC Inc.McLean, VA 22102andLynn M. CarlsonDoDABSTRACTThis paper eports results of the 1992 Evaluation of machinetranslation (MT) systems in the DARPA MT initiative andresults of a Pre-test o the 1993 Evaluation.
The DARPAinitiative is unique in that the evaluated systems differradically in languages translated, theoretical approach tosystem design, and intended end-user application.
In the 1992suite, a Comprehension Test compared the accuracy andinterpretability of system and control outputs; a Quality Panelfor each language pair judged the fidelity of translations fromeach source version.
The 1993 suite evaluated adequacy andfluency and investigated three scoring methods.1.
INTRODUCTIONDespite the long history of machine translation projects,and the well-known effects that evaluations such as theALPAC Report (Pierce et al, 1966) have had on thathistory, optimal MT evaluation methodologies remainelusive.
This is perhaps due in part to the subjectivityinherent in judging the quality of any translation output(human or machine).
The difficulty also lies in theheterogeneity of MT language pairs, computationalapproaches, and intended end-use.The DARPA machine translation i itiative is faced with allof these issues in evaluation, and so requires a suite ofevaluation methodologies which minimize subjectivity andtranscend the heterogeneity problems.
At the same time,the initiative seeks to formulate this suite in such a waythat it is economical to administer and portable to otherMT development initiatives.
This paper describes anevaluation of three research MT systems along withbenchmark haman and external MT outputs.
Two sets ofevaluations were performed, one using a relatively complexsuite of methodologies, and the other using a simpler seton the same data.
The test procedure is described, alongThe authors would like to express their gratitude toMichael Naber for his assistance in compiling, expressingand interpreting data.with a comparison of the results of the differentmethodologies.2.
SYSTEMSIn a test conducted in July, 1992, three DARPA-sponsoredresearch systems were evaluated in comparison with eachother, with external MT systems, and with human-onlytranslations.
Each system translated 12 common MasterPassages and six unique Original Passages, retrieved fromcommercial databases in the domain of business mergersand acquisitions.
Master Passages were Wall StreetJournal articles, translated into French, Spanish andJapanese for cross-comparison among the MT systems andlanguages.
Original Passages were retrieved in French,Spanish, and Japanese, for translation i to English.The 1992 Evaluation tested three research MT systems:CANDIDE (IBM, French - English) uses a statisticallanguage modeling technique based on speech recognitionalgorithms (see Brown et al, 1990).
It employsalignments generated between French strings and Englishstrings by training on a very large corpus of Canadianparliamentary proceedings represented in parallel French andEnglish.
The CANDIDE system was tested in both FullyAutomatic (FAMT) and Human-assisted (HAMT) modes.PANGLOSS (Carnegie Mellon University, New MexicoState University, and University of Southern California)uses lexical, syntactic, semantic, and knowledge-basedtechniques for analysis and generation (Nirenburg, et al1991).
The Spanish-English system is essentially an"interlingua" type.
Pangloss operates in human-assistedmode, with system-initiated interactions with the user fordisambiguation during the MT process.LINGSTAT (Dragon Systems Inc.) is a computer-aidedtranslation environment in which a knowledgeable non-expert can compose English translations of Japanese byusing a variety of contextual cues with word parsing andcharacter interpretation aids (Bamberg 1992).206Three organizations external to the DARPA initiativeprovided benchmark output.
These systems ran all the testinput that was submitted to the research systems.
Whilethese systems are not all at the same state of commercialrobustness, they nevertheless provided external perspectiveon the state of FAMT outside the DARPA initiative.The Pan American Health Organization provided outputfrom the SPANAM Spanish-English system, a productionsystem used daily by the organization.SYSTRAN Translation Systems Inc. provided output froma French - English production system and a Spanish -English pilot prototype.The Foreign Broadcast Information Service provided outputfrom a Japanese-English SYSTRAN system.
Though it isused operationally, SYSTRAN Japanese-English is nottrained for the test domain.3.
MT  EVALUATIONMETHODOLOGIESThe 1992 Evaluation introduced two methods to meet hechallenge of developing a black-box evaluation that wouldminimize judgment subjectivity while allowing a measureof comparison among three disparate systems.
AComprehension Test measured the adequacy orintelligibility of translated outputs, while a Quality Panelwas established tomeasure translation fidelity.The 1992 Evaluation provided meaningful measures ofperformance and progress of the research systems, whileproviding quantitative measures of comparability ofdiversesystems.
By these measures, the methodologies servedtheir purpose.
However, developing and evaluatingmaterials was difficult and labor-intensive, involvingspecial personnel categories.In order to assess whether alternative metrics could providecomparable or better evaluation results at reduced costs, aPre-test o the 1993 Evaluation was conducted.
The Pre-test was also divided into two parts: an evaluation ofadequacy according to a methodology suggested by TomCrystal of DARPA; and an evaluation of fluency.
The newmethodologies were applied to the 1992 MT test output ocompare translations of a small number of OriginalPassages by the DARPA and benchmark systems againsthaman-alone translations produced by human translators.These persons were nonprofessional level 2 translators asdefined by the Interagency Language Roundtable andadopted government-wide by the Office of PersonnelManagement in 1985.In the second suite, three numerical scoring scales wereinvestigated: yes/no, 1-3 and 1-5.
Two determinationsarise from the comparison: whether the new methodologyis in fact better in terms of cost, sensitivity (howaccurately the variation between systems i  represented) andportability, and which sconng variant of the evaluation isthe best by the same terms.The methodologies u ed in the 1992 Evaluation and 1993Prc-test are described briefly below.3.1.
Comprehension Test MethodologyIn the 1992 Evaluation, a set of Master Passage versionsformed the basis of a multiple-choice Comprehension Test,similar to the comprehension section of the verbalScholastic Aptitude Test (SAT).
These versions consistedof the "master passages" originally in English,professionally translated into the test source languages, andtranslated back into English by the systems, benchmarksand human translators.Twelve test takers unfamiliar with the source languagesanswered the same multiple choice questions over differenttranslation versions of the passages.
They each read thesame 12 passages, but rendered variously into the 12outputs represented in the test (CANDIDE FAMT,CANDIDE HAMT, PANGLOSS HAMT, LINGSTATHAMT, SPANAM FAMT, SYSTRAN FAMT for allthree language pairs, human-only for all three pairs, and theMaster Passages themselves.)
The passages were orderedso that no person saw any passage, nor any output versiontwice.3.2.
Quality Panel MethodologyIn the second part of the 1992 Evaluation, for each sourcelanguage, a Quality Panel of three professional translatorsassigned numerical scores rating the fidelity of translatedversions of six Original and six Master Passages againstsources or back-translations.
Within a given version of apassage, sentences were judged for syntactic, lexical,stylistic and orthographic errors.3.3.
Pre-test Adequacy MethodologyAs part of the 1993 Pre-test, nine monolinguals judged theextent o which the semantic ontent of six baseline textsfrom each source language was present in translationsproduced for the 1992 Evaluation by the test systems andthe benchmark systems.
The 1992 Evaluation level 2translations were used as baselines.
In the 18 baselines,scorable units were bracketed fragments that correspondedto a variety of grammatical constituents.
Each monolingualsaw 16 machine or human-assisted translations.
Eachevaluator saw two passages from each system.
Thepassages were ordered so that no person saw the samepassage twice.2073.4.
Pre-test Fluency MethodologyIn Part Two of the Pre-test, the nine monolingualsevaluated the fluency (well-formedness) of each sentence inthe same distribution of the same 16 versions that they hadseen in Part One.
In Part Two, these sentences appeared inparagraph form, without brackets.4.
RESULTSIn both the 1992 Evaluation and the 1993 Pre-test, thequality of output and time taken to produce that outputwere compared across:human-alone translationsoutput from benchmark MT systems?
output from the research systems in FAMT and/orHAMT modes.The results of the Comprehension Test (in which allsystems used what were originally the same passages) aresimilar to the results of the Quality Panel, with someminor exceptions ( ee White, 1992).
Thus for the purposeof the discussion that follows, we compare the results ofthe second, adequacy-fluency suite against he comparablesubset of the Quality Panel test from the first suite.The Pre-test evaluation results are arrayed in a manner thatemphasizes both the adequacy or fluency of the human-assisted and machine translations and the human effortinvolved to produce translations, expressed in (normalized)time.
For each part of the Pre-test, scores were tabulated,entered into a spreadsheet table according to scoring methodand relevant unit, and represented in two dimensionalarrays.
The relevant unit for Part 1 is the adequacy scorefor each fragment in each version evaluated.
For Part 2,the relevant unit is the score for fluency of each sentence ineach version evaluated.Performance for each of the systems cored was computedby averaging the fragment (or sentence) score over allfragments (or sentences), passages, and test subjects.
Themethod for normalizing these average scores was to dividethem by the maximum score per fragment (or sentence); forexample, 5 for the 1-5 tests.
Thus, a perfect averagednormalized system score is 1, regardless of the test.Three evaluators each saw two passages per system; thusthere was a total of six normalized average scores persystem.
The mean for each system is based on the sixscores for that system.
The eight system means were usedto calculate the global variance.
The F-raatio was calculatedby dividing the global variance, i.e.
the variance of themean per system, by the local variance, i.e.
the meanvariance of each system.
The F-ratio is used as a measureof sensitivity.The Quality Panel scores were arrayed in a like manner.The quality score per passage was divided by the number ofsentences in that passage.
The six Original Passages wereeach evaluated by 3 translators producing a total of 18scores per system.
Adding the 18 scores per systemtogether and dividing by 18 produced the mean of thenormalized quality score per system.
The means, variancesand F-ratios were calculated as described above for adequacyand fluency.4.1.
Quality Panel Evaluation ResultsFigure 1 is a representation of the Quality Panelevaluation, from the first evaluation suite, using thecomparable subset of the 1992 data (i.e., the originalpassages).
The quality scores range from .570 for CandideHAMT to .100 for Systran Japanese FAMT.
The scoresfor time in HAMT mode, represented asthe ratio of HAMTtime to Human-Only translation time, range from .689 forCandide HAMT to 1.499 for Pangloss Spanish HAMT.The normalized time for FAMT systems is set at 0.4.2.
Adequacy Test ResultsFigure 2 represents he results of the adequacy evaluationfrom the second suite.
Using the 1-5 variation of theevaluation, the adequacy (vertical axis) scores range from.863 for Candide HAMT to .250 for Systran JapaneseFAMT.
The time axis reflects the same ratio as isindicated in Figure 1.4.3.
Fluency Test ResultsFigure 3 represents the results of the fluency evaluationfrom the second suite.
Using the 1-5 variant, fluencyscores range from .853 for Candide HAMT to .214 forSystran Japanese FAMT.
The time axis reflects the sameratio as is indicated in Figure 1.5.
COMPARISON OFMETHODOLOGIESThe measures of adequacy and fluency used in the secondsuite are equated with the measure of quality used by the1992 Evaluation Quality Panel.
The methodologies werecompared on the bases of sensitivity, efficiency, andexpenditures of human time and effort involved inconstructing, administering and performing the evaluation.Cursory comparison of MT system performance in thethree results shown in Figures 1 through 3 showssimilarity in behavior.
All three methodologiesdemonstrate higher adequacy, fluency and quality scores for208I 0o00?o00 eooo700 4*OeO0 ti O.S~o~ "I'0.2~ ~0000 1 IO.OOO 0 2o0OUALII"Y P~EL  ?
~(IIOU&ILI~P S~DAE (XOl~ly  14UM $El1411~1\[ I ~ PA.IPdlAOI)/111 V'lI~qlINJEED111E FOR XOPI ~ ?
- ' -  - * ' -  IIiiiCI~4D J.IMIrIiJ, IO6T ~4~id)"I I I I I l I0400 0 1100 0 10?
I 000 1.2c?
+ 400 i mooIIOAV,I4.11mO TIMI lldllq:t4,1Lllff i * Imq~ml.le M Ylmo l~ 14Nqm A+lllfFigure 1: Quality Panel Results14 ?14C1~ ' l l~ff  P~l141~OI, JIACY Oil Plltl l ' l~llr P~I IAO!
IiCOIg\[ll VII IlO$11JM.I~O 11'111 POllOAIOII4AL IM, IOA~10000.II000tO0070001100 IiC+II,C F+~e, IT0,800O4OO0~0,2OO01000000 I +O.OOO 0.~00I.IIimew40*~411~m ~q,.lqOST/,.twrI I I I I I l0+400 0.400 O+llO0 1.000 I .
|00  1.400 1.600llOlil.ll~lO 'lIMlt (llll4kllll0T IUl PI*~II l I ,  el Inlm Iel Itlmm alelFigure 2: Adequacy Evaluation ResultsI|i!1 !It1 !b !1,4  O41OICl 'Irl\[llI' PO l l  IPl, J J l l l lg l t  O l i  pp l l~ l l r  pA , I IAOI  I iCQIW.i i  V l l  U , I I , i i i E  F4011l I IA , I I~1.0000g000.1000.P00O.lOQ '0 " ~0 .~0.000 0.~00II~ClJ.VllI"IIJIe~TJ~I1P\[ b4w40*e lmla  i1~ tl,*km IIPJV4~ll...I.Ikl, ffI I I I I I I0.400 0.1100 0.1100 I.OOO 1.200 1.400 1.4100I l lU J J l lO  1ri l l  l l l l~ la l l l '  JlJ i~ml ,  l , l l  el lrlmm iw mFigure 3: Fluency Evaluation Results209HAMT than FAMT.
Candide HAMT receives the highestscores for adequacy, fluency and quality; Systran JapaneseFAMT receives the lowest.
Bounds are consistent, butoccasionally Lingstat and Pangloss trade places on the yaxis as do SpanAm FAMT and Systran French FAMT.Given a similarity in performance, the comparison ofevaluation suite 1 to evaluation suite 2 should depend uponthe sensitivity of the measurements, a  well as the facilityof implementation f the evaluation.To determine sensitivity, an F-ratio calculation wasperformed.
For the suite 1 ( Quality Panel) and suite 2, aswell as for the variants that were performed on the suite 2set (yes/no, 1-3, 1-5).
The F-ratio statistic indicates thatthe second suite is indeed more sensitive than the suite 1tests.
(The Quality Panel test shows an F-ratio of 2.153.
)The 1-3 and 1-5 versions both have certain sensitivityadvantages: the 1-3 scale is central for adequacy (1.329.
),but proves most sensitive for fluency (3.583).
The 1-5scale is by far the most sensitive for adequacy (4.136) andcentral for fluency (3.301).
The 1-5 test for adequacyappears to be the most sensitive methodology overall.The suite 2 methodologies r quire less time/effort than theQuality Panel.
For all three scoring variants used in thesecond suite, less time was required of evaluators thanQuality Panefists.
The overall average time per passage forthe Quality Panel was 26 minutes per passage, whileaverage times for the Pre- tests were 11 minutes perpassage for the 1-5 variant of adequacy and four minutes perpassage for the 1-5 variant of fluency.The level of expertise required of evaluators i reduced inthe second suite; monolinguals perform the Pre-testevaluation, whereas Quality Panelists must be nativespeakers of English who are expert in French, Japanese orSpanish.
The second suite eliminates a considerableamount of time and effort involved in preparation of textsin French, Spanish and Japanese for the test booklets.6.
NEED FOR ADDIT IONAL TEST INGHuman effort, expertise, and test sensitivity seem toindicate that the suite 2 evaluations are preferred over thesuite 1 sets.
However, the variance within a particularsystem result remains quite high.
The standard eviations(represented in the figures as standard eviation of pooledvariance) are large, due perhaps to the sample size, but alsodue to the fact that the baseline English used in this Suite2 Pre-test evaluation were produced by level 2 translators,and not by professional translators.
Accordingly, we intendto re-apply the evaluation of the 1992 output, usingprofessional translations of the texts as the adequacybasefine.
Results will again be compared with the resultsof the 1992 Quality Panel.
This will help us furtherdetermine the usefulness, portability, and sensitivity of theevaluation methodologies.The Pre-test methodologies measure the well-formedness ofa translation and the degree to which a translation expressesthe content of the source document.
While results of the1992 Evaluation showed that results of the Quality Paneland the Comprehension Test were comparable, a test of thecomprehensibility of the translation provides uniqueinsight into the performance of an MT system.
Therefore,the 1993 Evaluation will include a Comprehension Test onversions of Original Passages to evaluate the intelligibilityof those versions.7.
CONCLUSIONSThe DARPA MT evaluation methodology strives tominimize the inherent subjectivity of judging translations,while optimizing the portability and replicability of testresults and accommodating the variety of approaches,Languages, and end-user applications.The two evaluation suites described in this paper sought toaccomplish these goals.
The comparison among themaccordingly is based upon the fidelity of the measurement,the efficiency of administration, and ultimately theportability of the test to other environments.
We find,subject o further testing underway, that he second suite isadvantageous in all these respects.R E F E R E N C E S.....Bamberg, Paul.
1992.
"The LINGSTAT Japanese-English MAT System" Status Report presented atthe 1992 DARPA MT Workshop, Newton, MAAugust, 1992.Brown, P. F., J. Cocke, S. A. DellaPietra, V. J.DellaPietra, F. Jelinek, J. D. Lafferty, R. L.Mercer, and P.S.
Roossin.
1990.
"A StatisticalApproach to Machine Translation."
ComputationalLinguistics, vol.
16, pp.
79-85.Nirenburg, S., J. Carbonell, M. Tomita, and K.Goodman.
1991.
Machine Translation: AKnowledge-Based Approach.
New York: MorganKaufmann.Pierce, J., J. Caroll, E. Hamp, D. Hays, C.Hockett, A. Oettinger, and A. Perlis.
1966.
"Language and Machines: Computers inTranslation and Linguistics."
National Academy ofSciences Publication 416.White, J.S.
"The DARPA Machine TranslationEvaluation: Implications for MethodologicalExtensibility."
Presented at the November 1992Meeting of the Association for Machine Translationof the Americas.
San Diego.210
