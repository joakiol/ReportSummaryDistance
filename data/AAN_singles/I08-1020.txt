Experiments on Semantic-based Clustering for Cross-document CoreferenceHoracio SaggionDepartment of Computer ScienceUniversity of Sheffield211 Portobello Street - Sheffield, England, UK, S1 4DPTel: +44-114-222-1947Fax: +44-114-222-1810saggion@dcs.shef.ac.ukAbstractWe describe clustering experiments forcross-document coreference for the firstWeb People Search Evaluation.
In our ex-periments we apply agglomerative cluster-ing to group together documents potentiallyreferring to the same individual.
The algo-rithm is informed by the results of two dif-ferent summarization strategies and an off-the-shelf named entity recognition compo-nent.
We present different configurations ofthe system and show the potential of the ap-plied techniques.
We also present an analy-sis of the impact that semantic informationand text summarization have in the cluster-ing process.1 IntroductionFinding information about people on huge text col-lections or on-line repositories on the Web is a com-mon activity.
In ad-hoc Internet retrieval, a requestfor documents/pages referring to a person name mayreturn thousand of pages which although containingthe name, do not refer to the same individual.
Cross-document coreference is the task of deciding if twoentity mentions in two sources refer to the same indi-vidual.
Because person names are highly ambiguous(i.e., names are shared by many individuals), decid-ing if two documents returned by a search enginesuch as Google or Yahoo!
refer to the same individ-ual is a difficult problem.Automatic techniques for solving this problem arerequired not only for better access to informationbut also in natural language processing applicationssuch as multidocument summarization, question an-swering, and information extraction.
Here, we con-centrate on the Web People Search Task (Artiles etal., 2007) as defined in the SemEval 2007 Work-shop: a search engine user types in a person name asa query.
Instead of ranking web pages, an ideal sys-tem should organise search results in as many clus-ters as there are different people sharing the samename in the documents returned by the search en-gine.
The input is, therefore, the results given bya web search engine using a person name as query.The output is a number of sets, each containing doc-uments referring to the same individual.
The task isrelated to the coreference resolution problem disre-garding however the linking of mentions of the tar-get entity inside each single document.Similarly to (Bagga and Baldwin, 1998; Phan etal., 2006), we have addressed the task as a documentclustering problem.
We have implemented our ownclustering algorithms but rely on available extractionand summarization technology to produce documentrepresentations used as input for the clustering pro-cedure.
We will shown that our techniques producenot only very good results but are also very compet-itive when compared with SemEval 2007 systems.We will also show that carefully selection of docu-ment representation is of paramount importance toachieve good performance.
Our system has a sim-ilar level of performance as the best system in therecent SemEval 2007 evaluation framework.
Thispaper extends our previous work on this task (Sag-gion, 2007).1492 Evaluation FrameworkThe SemEval evaluation has prepared two sets ofdata to investigate the cross-document coreferenceproblem: one for development and one for testing.The data consists of around 100 Web files per per-son name, which have been frozen and so, can beused as an static corpus.
Each file in the corpus isassociated with an integer number which indicatesthe rank at which the particular page was retrievedby the search engine.
In addition to the files them-selves, the following information was available: thepage title, the url, and the snippet.
In addition to thedata itself, human assessments are provided whichare used for evaluating the output of the automaticsystems.
The assessment for each person name isa file which contains a number of sets where eachset is assumed to contain all (and only those) pagesthat refer to one individual.
The development data isa selection of person names from different sourcessuch as participants of the European Conference onDigital Libraries (ECDL) 2006 and the on-line en-cyclop?dia Wikipedia.The test data to be used by the systems consistedof 30 person names from different sources: (i) 10names were selected from Wikipedia; (ii) 10 nameswere selected from participants in the ACL 2006conference; and finally, (iii) 10 further names wereselected from the US Census.
One hundred doc-uments were retrieved using the person name as aquery using the search engine Yahoo!.Metrics used to measure the performance ofautomatic systems against the human output wereborrowed from the clustering literature (Hotho etal., 2003) and they are defined as follows:Precision(A,B) = |A ?B||A|Purity(C,L) =n?i=1|Ci|nmaxjPrecision(Ci, Lj)Inverse Purity(C,L) =n?i=1|Li|nmaxjPrecision(Li, Cj)F-Score?
(C,L) =Purity(C,L) ?
Inverse Purity(C,L)?Purity(C,L) + (1?
?
)Inverse Purity(C,L)where C is the set of clusters to be evaluated andL is the set of clusters produced by the human.
Notethat purity is a kind of precision metric which re-wards a partition which has less noise.
Inverse pu-rity is a kind of recall metric.
?
was set to 0.5 inthe SemEval 2007 evaluation.
Two simple baselinesystems were defined in order to measure if the tech-niques used by participants were able to improveover them.
The all-in-one baseline produces one sin-gle cluster ?
all documents belonging to that cluster.The one-in-one baseline produces n cluster with onedifferent document in each cluster.3 Agglomerative Clustering AlgorithmClustering is an important technique used in areassuch as information retrieval, text mining, and datamining (Cutting et al, 1992).
Clustering algorithmscombine data points into groups such that: (i) datapoints in the same group are similar to each other;and (ii) data points in one group are ?different?
fromdata points in a different group or cluster.
In infor-mation retrieval it is assumed that documents thatare similar to each other are likely to be relevantfor the same query, and therefore having the doc-ument collection organised in clusters can provideimproved document access (van Rijsbergen, 1979).Different clustering techniques exist (Willett, 1988)the simplest one being the one-pass clustering al-gorithm (Rasmussen and Willett, 1987).
We haveimplemented an agglomerative clustering algorithmwhich is relatively simple, has reasonable complex-ity, and gave us rather good results.
Our algorithmoperates in an exclusive way, meaning that a doc-ument belongs to one and only one cluster ?
whilethis is our working hypothesis, it might not be validin some cases.The input to the algorithm is a set of documentrepresentations implemented as vectors of terms andweights.
Initially, there are as many clusters asinput documents; as the algorithm proceeds clus-ters are merged until a certain termination condi-tion is reached.
The algorithm computes the similar-ity between vector representations in order to decidewhether or not to merge two clusters.The similarity metric we use is the cosine of theangle between two vectors.
This metric gives valueone for identical vectors and zero for vectors whichare orthogonal (non related).
Various options havebeen implemented in order to measure how close150two clusters are, but for the experiments reportedhere we have used the following approach: the sim-ilarity between two clusters (simC) is equivalent tothe ?document?
similarity (simD) between the twomore similar documents in the two clusters ?
this isknown as single linkage in the clustering literature;the following formula is used:simC (C1,C2) =maxdi?C1;dj?C2simD(di,dj)Where Ck are clusters, dl are document represen-tations (e.g., vectors), and simD is the cosine metricgiven by the following formula:cosine(d1, d2) =?ni=1 wi,d1 ?
wi,d2?
?ni=1(wi,d1)2 ??
?ni=1(wi,d2)2where wi,d is the weight of term i in document dand n is the numbers of terms.If this similarity is greater than a threshold ?
ex-perimentally obtained ?
the two clusters are mergedtogether.
At each iteration the most similar pair ofclusters is merged.
If this similarity is less than acertain threshold the algorithm stops.
Merging twoclusters consist of a simple step of set union, so thereis no re-computation involved ?
such as computinga cluster centroid.We estimated the threshold for the clustering al-gorithm using the ECDL subset of the training dataprovided by SemEval.
We applied the clustering al-gorithm where the threshold was set to zero.
Foreach document set, purity, inverse purity, and F-score were computed at each iteration of the algo-rithm, recording the similarity value of each newlycreated cluster.
The similarity values for the bestclustering results (best F-score) were recorded, andthe maximum and minimum values discarded.
Therest of the values were averaged to obtain an esti-mate of the optimal threshold.
The thresholds usedfor the experiments reported here are as follows:0.10 for word vectors and 0.12 for named entity vec-tors (see Section 5 for vector representations).4 Natural Language ProcessingTechnologyWe rely on available extraction and summarizationtechnology in order to linguistically process the doc-uments for creating document representations forclustering.
Although the SemEval corpus containsinformation other than the retrieved pages them-selves, we have made no attempt to analyse or usecontextual information given with the input docu-ment.Two tools are used: the GATE system (Cunning-ham et al, 2002) and a summarization toolkit (Sag-gion, 2002; Saggion and Gaizauskas, 2004) whichis compatible with GATE.
The input for analysis isa set of documents and a person name (first nameand last name).
The documents are analysed by thedefault GATE1 ANNIE system which creates differ-ent types of named entity annotations.
No adap-tation of the system was carried out because wewanted to verify how far we could go using availabletools.
Summarization technology was used fromsingle document summarization modules from oursummarization toolkit.The core of the toolkit is a set of summariza-tion modules which compute numeric features foreach sentence in the input document, the value ofthe feature indicates how relevant the informationin the sentence is for the feature.
The computedvalues, which are normalised yielding numbers inthe interval [0..1] ?
are combined in a linear for-mula to obtain a score for each sentence which isused as the basis for sentence selection.
Sentencesare ranked based on their score and top ranked sen-tences selected to produce an extract.
Many fea-tures implemented in this tool have been suggestedin past research as valuable for the task of identify-ing sentences for creating summaries.
In this work,summaries are created following two different ap-proaches as described below.The text and linguistic processors used in our sys-tem are: document tokenisation to identify differentkinds of words; sentence splitting to segment the textinto units used by the summariser; parts-of-speechtagging used for named entity recognition; namedentity recognition using a gazetteer lookup moduleand regular expressions grammars; and named entitycoreference module using a rule-based orthographicname matcher to identify name mentions consideredequivalent (e.g., ?John Smith?
and ?Mr.
Smith?
).Named entities of type Person, Organization, Ad-dress, Date, and Location are considered relevant1http://gate.ac.uk151document terms and stored in a special named en-tity called Mention as an annotation.
The perfor-mance of the named entity recogniser on Web data(business news from the Web) is around 0.90 F-score(Maynard et al, 2003).Coreference chains are created and analysed andif they contain an entity matching the target person?ssurname, all elements of the chain are marked as afeature of the annotation.We have tested two summarization conditions inthis work: In one set of experiments a sentence be-longs to a summary if it contains a mention whichis coreferent with the target entity.
In a second setof experiments a sentence belongs to a summary ifit contains a ?biographical pattern?.
We rely on anumber of patterns that have been proposed in thepast to identify descriptive phrases in text collec-tions (Joho and Sanderson, 2000).
The patterns usedin the experiments described here are shown in Ta-ble 1.
In the patterns, dp is a descriptive phrase thatin (Joho and Sanderson, 2000) is taken as a nounphrase.
These patterns are likely to capture infor-mation which is relevant to create person profiles, asused in DUC 2004 and in TREC QA ?
to answerdefinitional questions.These patterns are implemented as regular expres-sions using the JAPE language (Cunningham et al,2002).
Our implementation of the patterns make useof coreference information so that target is any namein text which is coreferent with sought person.
In or-der to implement the dp element in the patterns weuse the information provided by a noun phrase chun-ker.
The following is one of the JAPE rules for iden-tifying key phrases as implemented in our system:({TargetPerson}({ Token.string == "is" } |{Token.string == "was" }){NounChunk}):annotate --> :annotate.KeyPhrase = {}where TargetPerson is the sought entity, andNounChunk is a noun chunk.
The rule states thatwhen the pattern is found, a KeyPhrase should becreated.Some examples of these patterns in text are shownin Table 4.
A profile-based summarization systemwhich uses these patterns to create person profiles isreported in (Saggion and Gaizauskas, 2005).Patternstarget (is | was |...) (a | an | the) dptarget, (who | whose | ...)target, (a | the | one ...) dptarget, dptarget?starget and othersTable 1: Set of patterns for identifying profile infor-mation.Dickson?s invention, the Kinetoscope, was simple:a strip of several images was passed in front of anilluminated lens and behind a spinning wheel.James Hamilton, 1st earl of ArranJames Davidson, MD, Sports Medicine Orthope-dic Surgeon, Phoenix ArizonaAs adjutant general, Davidson was chief of theState Police, qv which he organized quickly.Table 2: Descriptive phrases in test documents fordifferent target names.4.1 Frequency InformationUsing language resources creation modules from thesummarization tool, two frequency tables are cre-ated for each document set (or person) on-the-fly: (i)an inverted document frequency table for words (nonormalisation is applied); and (ii) an inverted fre-quency table for Mentions (the full entity string isused, no normalisation is applied).Statistics (term frequencies (tf(Term)) and in-verted document frequencies (idf(Term))) are com-puted over tokens and Mentions using tools from thesummarization toolkit (see examples in Table 3).word frequencies Mention frequenciesof (92) Jerry Hobbs (80)Hobbs (92) Hobbs (56)Jerry (90) Krystal Tobias (38)to (89) Texas (37)in (87) Jerry (36)and (86) Laura Hobbs (35)the (85) Monday (34)a (85) 1990 (31)Table 3: Examples of top frequent terms (words andnamed entities) and their frequencies in the JerryHobbs set.Using these tables vector representations are cre-ated for each document (same as in (Bagga and152Baldwin, 1998)).
We use the following formula tocompute term weight (N is the number of documentsin the input set):weight(Term) = tf(Term) ?
log2( Nidf(Term) )These vectors are also stored in the GATE doc-uments.
Two types of representations were con-sidered for these experiments: (i) full document orsummary (terms in the summary are considered forvector creation); and (ii) words are used as terms orMentions are used as terms.5 Cross-document Coreference SystemsIn this section we present results of six different con-figurations of the clustering algorithm.
The config-urations are composed of two parts one which indi-cates where the terms are extracted from and the sec-ond part indicates what type of terms were used.
Thetext conditions are as follows: Full Document (FD)condition means that the whole document was usedfor extracting terms for vector creation; Person Sum-mary (PS) means that sentences containing the targetperson name were used to extract terms for vectorcreation; Descriptive Phrase (DP) means that sen-tences containing a descriptive patterns were used toextract terms for vector creation.
The term condi-tions are: Words (W) words were used as terms andMentions (M) named entities were used as terms.Local inverted term frequencies were used to weightthe terms.6 SemEval 2007 Web People SearchResultsThe best system in SemEval 2007 obtained an F-score of 0.78, the average F-score of all 16 partic-ipant systems is 0.60.
Baseline one-in-one has anF-score of 0.61 and baseline all-in-one an F-score of0.40.
Results for our system configurations are pre-sented in Table 4.
Our best configuration (FD+W)obtains an F-score of 0.74 (or a fourth position in theSemEval ranking).
All our configurations obtainedF-scores greater than the average of 0.60 of all par-ticipant systems.
They also perform better than thetwo baselines.Our optimal configurations (FD+W and PS+W)both perform similarly with respect to F-score.While the full document condition favours ?inversepurity?, summary condition favours ?purity?.
Asone may expect, the use of descriptive phrases tocreate summaries has the effect of increasing purityto one extreme, these expressions are far too restric-tive to capture all necessary information for disam-biguation.Configuration Purity Inv.Purity F-ScoreFD+W 0.68 0.85 0.74FD+M 0.62 0.85 0.68PS+W 0.84 0.70 0.74PS+M 0.65 0.75 0.64DP+W 0.90 0.62 0.71DP+M 0.97 0.53 0.66Table 4: Results for different clustering configura-tions.
These results are those obtained on the wholeset of 30 person names.7 Semantic-based ExperimentsWhile these results are rather encouraging, theywere not optimal.
In particular, we were surprisedthat semantic information performed worst than asimple word-based approach.
We decided to inves-tigate whether some types of semantic informationmight be more helpful than others in the cluster-ing process.
We therefore created one vector foreach type of information: Organization, Person,Location, Date, Address in each document and re-clustered all test data using one type at a time, with-out modifying any of the system parameters (e.g.,without re-training).
The results were very encour-aging.7.1 ResultsResults of semantic-based clustering per informa-tion type are presented in Tables 5 and 6.
Each rowSemantic Type Purity Inv.Purity F-Score +/-Organization 0.90 0.72 0.78 +0.10Person 0.81 0.72 0.75 +0.07Address 0.82 0.64 0.69 +0.01Date 0.58 0.85 0.67 -0.01Location 0.55 0.85 0.64 -0.04Table 5: Results for full document condition anddifferent semantic information types.
Improvementsover FD+M are reported.153Semantic Type Purity Inv.Purity F-Score +/-Person 0.85 0.64 0.70 +0.06Organization 0.97 0.57 0.69 +0.05Date 0.87 0.60 0.68 +0.04Location 0.82 0.63 0.67 +0.03Address 0.93 0.54 0.65 +0.01Table 6: Results for summary condition and differ-ent semantic information types.
Improvements overPS+M are reported.in the tables reports results for clustering using onetype of information alone.
Table 5 reports results forsemantic information with full text condition and itis therefore compared to our configuration FD+Mwhich also uses full text condition together with se-mantic information.
The last column in the tableshows improvements over that configuration.
UsingOrganization type of information in full text condi-tion, not only outperforms the previous system byten points, also exceeds by a fraction of a point thebest system in SemEval 2007 (one point if we con-sider macro averaged F-score).
Statistical tests (t-test) show that improvement over FD+M is statisti-cally significant.
Other semantic types of informa-tion also have improved performance, not all of themhowever.
Location and Date in the full documentsare probably too ambiguous to help disambiguatingthe target named entity.Table 6 reports results for semantic informationwith summary text condition (only personal sum-maries were tried, experiments using descriptivephrases are underway) and it is therefore comparedto our configuration PS+M which also uses sum-mary condition together with semantic information.The last column in the table shows improvementsover that configuration.
Here all semantic types ofinformation taken individually outperform a systemwhich uses the combination of all types.
This isprobably because all types of information in a per-sonal summary are somehow related to the targetperson.7.2 Results per Person SetFollowing (Popescu and Magnini, 2007), we presentpurity, inverse purity, and F-score results for allour configurations per category (ACL, US Census,Wikipedia) in the test set.In Tables 7, 8, and 9, results are reported for fullConfiguration Set Purity I.Purity F-ScoreFD+Address ACL 0.86 0.48 0.57FD+Address US C. 0.81 0.71 0.75FD+Address Wikip.
0.78 0.70 0.73PS+Address ACL 0.96 0.38 0.50PS+Address US C. 0.94 0.61 0.72PS+Address Wikip.
0.88 0.62 0.71FD+Date ACL 0.63 0.82 0.69FD+Date US C. 0.52 0.87 0.64FD+Date Wikip.
0.59 0.85 0.68PS+Date ACL 0.88 0.49 0.59PS+Date US C. 0.88 0.64 0.72PS+Date Wikip.
0.84 0.67 0.72FD+Location ACL 0.63 0.78 0.65FD+Location US C. 0.52 0.86 0.64FD+Location Wikip.
0.49 0.91 0.62PS+Location ACL 0.87 0.47 0.54PS+Location US C. 0.85 0.66 0.73PS+Location Wikip.
0.74 0.75 0.72Table 7: Results for clustering configurations perperson type set (ACL, US Census, and Wikipedia)- Part I.Configuration Set Purity I.Purity F-ScoreFD+Org.
ACL 0.92 0.57 0.69FD+Org.
US C. 0.87 0.78 0.82FD+Org.
Wikip.
0.88 0.79 0.83PS+Org.
ACL 0.98 0.42 0.54PS+Org.
US C. 0.95 0.63 0.74PS+Org.
Wikip.
0.96 0.65 0.77FD+Person ACL 0.82 0.66 0.72FD+Person US C. 0.81 0.74 0.76FD+Person Wikip.
0.77 0.75 0.75PS+Person ACL 0.86 0.53 0.63PS+Person US C. 0.85 0.6721 0.73PS+Person Wikip.
0.82 0.70 0.73Table 8: Results for clustering configurations perperson type set (ACL, US Census, and Wikipedia)- Part II.document condition(FD), summary condition (PS),word-based representation (W), mention representa-tion (M) ?
i.e.
all types of named entities, and fivedifferent mention types: Person, Location, Organi-zation, Date, and Address.While the Organization type of entity worked bet-ter overall, it is not optimal across different cat-egories of people.
Note for example that verygood results are obtained for the Wikipedia and USCensus sets, but rather poor results for the ACLset, where a technique which relies on using fulldocuments and words for document representationsworks better.
These results show that more work is154Configuration Set Purity I.Purity F-ScoreFD+W ACL 0.73 0.84 0.77FD+W US C. 0.54 0.91 0.67FD+W Wikip.
0.57 0.91 0.68FD+M ACL 0.73 0.76 0.70FD+M US C. 0.68 0.82 0.71FD+M Wikip.
0.60 0.86 0.68PS+W ACL 0.84 0.59 0.65PS+W US C. 0.80 0.74 0.75PS+W Wikip.
0.70 0.81 0.73PS+M ACL 0.75 0.62 0.60PS+M US C. 0.71 0.74 0.69PS+M Wikip.
0.58 0.83 0.66Table 9: Results for clustering configurations perperson type set (ACL, US Census, and Wikipedia)- Part III.needed before reaching any conclusions on the bestdocument representation for our algorithm in thistask.8 Related WorkThe problem of cross-document coreference hasbeen studied for a number of years now.
Baggaand Baldwin (Bagga and Baldwin, 1998) used thevector space model together with summarizationtechniques to tackle the cross-document coreferenceproblem.
Their approach uses vector representa-tions following a bag-of-words approach.
Terms forvector representation are obtained from sentenceswhere the target person appears.
They have not pre-sented an analysis of the impact of full documentversus summary condition and their clustering algo-rithm is rather under-specified.
Here we have pre-sented a clearer picture of the influence of summaryvs full document condition in the clustering process.Mann and Yarowsky (Mann and Yarowsky, 2003)used semantic information extracted from docu-ments referring to the target person in an hierarchicalagglomerative clustering algorithm.
Semantic infor-mation here refers to factual information about a per-son such as the date of birth, professional career oreducation.
Information is extracted using patternssome of them manually developed and others in-duced from examples.
We differ from this approachin that our semantic information is more general andis not particularly related - although it might be - tothe target person.Phan el al.
(Phan et al, 2006) follow Mann andYarowsky in their use of a kind of biographical in-formation about a person.
They use a machine learn-ing algorithm to classify sentences according to par-ticular information types in order to automaticallyconstruct a person profile.
Instead of comparingbiographical information in the person profile alto-gether as in (Mann and Yarowsky, 2003), they com-pare each type of information independently of eachother, combining them only to make the final deci-sion.Finally, the best SemEval 2007 Web PeopleSearch system (Chen and Martin, 2007) used tech-niques similar to ours: named entity recognition us-ing off-the-shelf systems.
However in addition tosemantic information and full document conditionthey also explore the use of contextual informationsuch as the url where the document comes from.They show that this information is of little help.
Ourimproved system obtained a slightly higher macro-averaged f-score over their system.9 Conclusions and Future WorkWe have presented experiments on cross-documentcoreference of person names in the context of thefirst SemEval 2007 Web People Search task.
Wehave designed and implemented a solution whichuses an in-house clustering algorithm and availableextraction and summarization techniques to producerepresentations needed by the clustering algorithm.We have presented different approaches and com-pared them with SemEval evaluation?s results.
Wehave also shown that one system which uses onespecific type of semantic information achieves state-of-the-art performance.
However, more work isneeded, in order to understand variation in perfor-mance from one data set to another.Many avenues of improvement are expected.Where extraction technology is concerned, we haveused an off-the-shelf system which is probably notthe most appropriate for the type of data we are deal-ing with, and so adaptation is needed here.
With re-spect to the clustering algorithm we plan to carry outfurther experiments to test the effect of different sim-ilarity metrics, different merging criteria includingcreation of cluster centroids, and cluster distances;with respect to the summarization techniques we in-tend to investigate how the extraction of sentences155containing pronouns referring to the target entity af-fects performance, our current version only exploitsname coreference.
Our future work will also explorehow (and if) the use of contextual information avail-able on the web can lead to better performance.AcknowledgementsWe are indebted to the three anonymous reviewersfor their extensive suggestions that helped improvethis work.
This work was partially supported by theEU-funded MUSING project (IST-2004-027097).ReferencesJ.
Artiles, J. Gonzalo, and S. Sekine.
2007.
TheSemEval-2007 WePS Evaluation: Establishing abenchmark for Web People Search Task.
In Proceed-ings of Semeval 2007, Association for ComputationalLinguistics.A.
Bagga and B. Baldwin.
1998.
Entity-Based Cross-Document Coreferencing Using the Vector SpaceModel.
In Proceedings of the 36th Annual Meetingof the Association for Computational Linguistics andthe 17th International Conference on ComputationalLinguistics (COLING-ACL?98), pages 79?85.Y.
Chen and J.H.
Martin.
2007.
Cu-comsem: Explor-ing rich features for unsupervised web personal nameddisambiguation.
In Proceedings of SemEval 2007, As-socciation for Computational Linguistics, pages 125?128.H.
Cunningham, D. Maynard, K. Bontcheva, andV.
Tablan.
2002.
GATE: A Framework and GraphicalDevelopment Environment for Robust NLP Tools andApplications.
In Proceedings of the 40th AnniversaryMeeting of the Association for Computational Linguis-tics (ACL?02).Douglass R. Cutting, Jan O. Pedersen, David Karger, andJohn W. Tukey.
1992.
Scatter/gather: A cluster-based approach to browsing large document collec-tions.
In Proceedings of the Fifteenth Annual Interna-tional ACM SIGIR Conference on Research and De-velopment in Information Retrieval, pages 318?329.A.
Hotho, S. Staab, and G. Stumme.
2003.
WordNet im-proves text document clustering.
In Proc.
of the SIGIR2003 Semantic Web Workshop.H.
Joho and M. Sanderson.
2000.
Retrieving Descrip-tive Phrases from Large Amounts of Free Text.
InProceedings of Conference on Information and Know-eldge Management (CIKM), pages 180?186.
ACM.G.
S. Mann and D. Yarowsky.
2003.
Unsupervised per-sonal name disambiguation.
In W. Daelemans andM.
Osborne, editors, Proceedings of the 7th Confer-ence on Natural Language Learning (CoNLL-2003),pages 33?40.
Edmonton, Canada, May.D.
Maynard, K. Bontcheva, and H. Cunningham.2003.
Towards a semantic extraction of namedentities.
In G. Angelova, K. Bontcheva, R. Mitkov,N.
Nicolov, and N. Nikolov, editors, Proceedings ofRecent Advances in Natural Language Processing(RANLP?03), pages 255?261, Borovets, Bulgaria, Sep.http://gate.ac.uk/sale/ranlp03/ranlp03.pdf.X.-H. Phan, L.-M. Nguyen, and S. Horiguchi.
2006.Personal name resolution crossover documents by asemantics-based approach.
IEICE Trans.
Inf.
& Syst.,Feb 2006.Octavian Popescu and Bernardo Magnini.
2007.
Irst-bp:Web people search using name entities.
In Proceed-ings of the Fourth International Workshop on SemanticEvaluations (SemEval-2007), pages 195?198, Prague,Czech Republic, June.
Association for ComputationalLinguistics.E.
Rasmussen and P. Willett.
1987.
Non-hierarchicaldocument clustering using the icl distribution ar-ray processor.
In SIGIR ?87: Proceedings of the10th annual international ACM SIGIR conference onResearch and development in information retrieval,pages 132?139, New York, NY, USA.
ACM Press.H.
Saggion and R. Gaizauskas.
2004.
Multi-documentsummarization by cluster/profile relevance and redun-dancy removal.
In Proceedings of the Document Un-derstanding Conference 2004.
NIST.H.
Saggion and R. Gaizauskas.
2005.
Experiments onstatistical and pattern-based biographical summariza-tion.
In Proceedings of EPIA 2005, pages 611?621.H.
Saggion.
2002.
Shallow-based Robust Summariza-tion.
In Automatic Summarization: Solutions and Per-spectives, ATALA, December, 14.H.
Saggion.
2007.
Shef: Semantic tagging and summa-rization techniques applied to cross-document corefer-ence.
In Proceedings of SemEval 2007, Assocciationfor Computational Linguistics, pages 292?295.C.J.
van Rijsbergen.
1979.
Information Retrieval.
But-terworths, London.P.
Willett.
1988.
Recent trends in hierarchic documentclustering: A critical review.
Information Processing& Management, 24(5):577?597.156
