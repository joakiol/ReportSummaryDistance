IIIIII1General Word Sense Disambiguation MethodBased on a Full Sentential ContextJiri Stet ina Sadao Kurohashi Makoto NagaoGraduate  School of Infomatics, Kyoto UniversityYoshida-honmachi,  Sakyo, Kyoto,  606-8501, Japan{ st et ?na, kuro, nagao } ~kuee.
kyot o-u.
ac.
j pAbst ractThis paper presents a new general supervised wordsense disambiguation method based on a relativelysmall syntactically parsed and semantically taggedtraining corpus.
The method exploits a full senten-tial context and all the explicit semantic relations ina sentence to identify the senses of all of that sen-tence's content words.
In spite of a very small train-ing corpus, we report an overall accuracy of 80.3%(85.7, 63.9, 83.6 and 86.5%, for nouns, verbs, adjec-tives and adverbs, respectively), which exceeds theaccuracy of a statistical sense-frequency based se-mantic tagging, the only really applicable generaldisambiguating technique.1 In t roduct ionIdentification of the right sense of a word in a sen-tence is crucial to any successful Natural LanguageProcessing system.
The same word can have dif-ferent meanings in different contexts.
The task ofWord Sense Disambiguation is to determine the cor-rect sense of a word in a given context.In most cases the correct word sense can be iden-tified using only the words co-occurring in the samesentence.
However, very often we also need to usethe context of words that appear outside the givensentence.
For this reason we distinguish two typesof contexts: the sentential context and the discoursecontext.
The sentential context is given by the wordswhich co-occur with the word in a sentence and bytheir relations to this word, while the discourse con-text is given by the words outside the sentence andtheir relations to the word.
The problem that ariseshere is that most of the co-occurring words are alsopolysemous, and unless disambiguated they cannotfully contribute to the process of disambiguation.The senses of these words, however, also depend onthe sense of the disambiguated word and thereforethere is a reciprocal dependency which we will tryto resolve by the algorithm described in this paper.Table I: Percentage of nouns, verbs, adjectives andadverbs and average number of sensesCategoryNOUNSVERBSADJ ECTIVESADVERBSTOTALNumber48,53426,67419,743I 1,804106,755% Average #of senses45.5 5.425.0 10.518.5 5.511.0 3.?100.0 5.82 The  Task  Spec i f i ca t ionFor our work, we used the word sense definitionsas given in WordNet (Miller, 1990), which is com-parable to a good printed dictionary in its cover-age and distinction of senses.
Since WordNet onlyprovides definitions for content words (nouns, verbs,adjectives and adverbs), we are only concerned withidentifying the correct senses of the content words.Both for the training and for the testing of ouralgorithm, we used the syntactically analysed sen-tences of the Brown Corpus (Marcus, 1993), whichhave been manually semantically tagged (Miller etal., 1993) into semantic oncordance files (SemCor).These files combine 103 passages of the Brown Cor-pus with the WordNet lexical database insuch a waythat every content word in the text carries both asyntactic tag and a semantic tag pointing to the ap-propriate sense of that word in WordNet.
Passagesin the Brown Corpus are approximately 2,000 wordslong, and each contains approximately 1,000 contentwords.The percentages of the nouns, verbs, adjectivesand adverbs in the semantically tagged corpus, to-gether with their average number of Word Net senses,are given in Table I.
Although most of the wordsin a dictionary are monosemous, it is the polyse-mous words that occur most frequently in speechand text.
For example, over 80% of words in Word-Net are monosemous, but almost 78% of the contentwords in the tested corpus had more than one sense,as shown in Table 2.I!I!IIInI!IIIIilTable 2: Percentage of polysemous word in the cor-~USCategoryNOUNSVERBSADJECT\[VESADVERBSTOTALNumber Polysemous48,534 38,27926,674 24,84519,743 13,315II,804 6,715106,755 83,154%78.993.167.456.977.9Assigning the most frequent sense (as defined byWordNet) to every content word in the used corpuswould result in an accuracy of 75.2 %.
Our aim isto create a word sense disambiguation system foridentifying the correct senses of all content wordsin a gwen sentence, with an accuracy higher thanwould be achieved solely by a use of the most fre-quent sense.3 General Word SenseDisambiguat ionThe aim of the system described here is to take anysyntactically analysed sentence on the input and as-sign each of its content words a pointer to an ap-propriate sense in WordNet.
Because the words ina sentence are bound by their syntactic relations,all the word's senses are determined by their mostprobable combination in all the syntactic relationsderived from the parse structure of the given sen-tence.
It is assumed here that each phrase has onecentral constituent (head), and all other constituentsin the phrase modify the head (modifiers).
It isalso assumed that there is no relation between themodifiers.
The relations are explicitly present in theparse tree, where head words propagate up throughthe tree, each parent receiving its head word fromits head-child.
Every syntactic relation can be alsoviewed as a semantic relationship between the con-cepts represented by the participating words.
Con-sider, for example, the sentence (1) whose syntacticstructure is given in Figure 1.
(1) The Fulton County Grand Jury saidFriday an investigation of Atlanta's recentprimary election produced no evidence thatany irregularities took place.Each word in the above sentence is bound by anumber of syntactic relations which determine thecorrect sense of the word.
For example, the senseof the verb produced is constrained by the subject-verb relation with the noun investigation, by theverb-object relation with the noun evidence and bythe subordinate clause relation with the verb said.Similarly, the verb said is constrained by its rela-tions with the words Jury, Friday and produced; thesense of the noun investigation is constrained by therelation with the head of its prepositional phrase -election, and by the subject-verb relation with theverb produced, and so on.The key to extraction of the relations is that anyphrase can be substituted by the corresponding treehead-word (links marked bold in Figure 1).
To de-termine the tree head-word we used a set of rulessimilar to that described by (Magerman, 1995)(Je-linek et al, 1994) and also used by (Collins, 1996),which we modified in the following way:?
The head of a prepositional phrase (PP - -  INNP) was substituted by a function the name ofwhich corresponds to the preposition, and itssole argument corresponds to the head of thenoun phrase NP.?
The head of a subordinate clause was changedto a function named after the head of the firstelement in the subordinate clause (usually 'that'or a 'NULL' element) and its sole argument cor-responds to the head of its second element (usu-ally head of a sentence).Because we assumed that the relations within thesame phrase are independent, all the relations arebetween the modifier constituents and the head ofa phrase only.
This is not necessarily true in somesituations, but for the sake of simplicity we took theliberty to assume so.
A complete list of applicablerelations for sentence (I) is given in (2).
(2) NP(NN P(County), NN P(Jury))N P(NNP(Grand),NN P(Jury))NP(NP(Atlanta),NP(election))N P( J J(recent),N P(election))N P(J J(primary),N (election))N P(N N(in vestigation), P P(of(election)))S(N P(irregularities),VP(took))V P(VB D(took),NP(place))N P(NN(evidence),SBAR(that(took))S( N P(investigation),VP(produced))V P(VB D(produced),N P(evidence))VPIVBD(said),N P(Friday))V P(VBD(said),S BA R(0(produced)))S(NP(Jury),VP(said))Each of the extracted syntactic relations has a cer-tain probability for each combination of the sensesof its arguments.
This probability is derived fromthe probability of the semantic relation of each com-bination of the sense candidates of the related con-tent words.
Therefore, the approach described hereconsists of two phases: 1. learning the semantic re-lations, and 2. disambiguation through the proba-bility evaluation of relations.4 Learn ingAt first, every content word in every sentence in thetraining set was tagged by an appropriate pointer toa sense in WordNet.Secondly, using the parse trees of all the corpussentences, all the syntactic relations present in theVP (said)DT NP NPNNP NNP NNP NNPI I I IThe Ful ton County  Grand  Jury.
.
.
.
.
.
.
?o??
?ooo  .
.
.
.
.
??
.
.
.
.
.
.
.
.
.
.
.
.
.
."
"  SBAR (d~u(wok) )IN(d~) $ (~onk)T T NNany i r regul~i t ies  t p~eL .
.
.
.
.
.
.
.
?
?oo???
.
.
.
?
.
?
?o .
.
.
.
.
o .
.
.
.
.VBD NP ,~~roduced) )NNP NONEI?IT I I o~ I~N "*'SBAR an i nve~igat ion  IN NP (el.)
p~edIof  NP POS NP (election) noI /NNP / JJ NP (election)I I Atlanm's recent  Imrna:y  e lect ionI !?~o.~o...
oo?.i t - - " iIIIIIIIIIIIiFigure 1: Example parse treetraining corpus were extracted and converted intothe following form:(4) reI(PNT, MNT, HNT, MS, HS, RP).where PNT is the phrase parent non-terminal, MNTthe modifier non-terminal, HNT the head non-terminal, MS the semantic ontent (see below) ofthe modifier constituent, \['IS the semantic contentof the head constituent and RP the relative posi-tion of the modifier and the head (RP=I  indicatesthat the modifier precedes the head, while for RP=2the head precedes the modifier).
Relations involvingnon-content modifiers were ignored.
Synsets of thewords not present in WordNet were substituted bythe words themselves.The semantic ontent was either a WordNet senseidentificator (synset) or, in the case of prepositionaland subordinate phrases, a function of the preposi-tion (or a null element) and the sense identificatorof the second phrase constituent.5 Disambiguation AlgorithmAs mentioned above, we assumed that all the con-tent words in a sentence are bound by a number ofsyntactic relations.
Every content word can haveseveral meanings, but each of these meanings has adifferent probability, which is given by the set of se-mantic relations in which the word participates.
Be-cause every relation has two arguments (head and itsmodifier), the probability of each sense also dependson the probability of the sense of the other partic-ipant in the relation.
The task is to select such acombination of senses for all the content words, thatthe overall relational probability is maximal.
If, forany given sentence, we had extracted N syntactic re-lations PA, the overall relational probability for thecombination of senses X would be:N(5) ORP(X) = I-\[ p(&IX)i= lwhere p(Ri IX) is the probability of the i-th relationgiven the combination of senses X.
If we consider,that an average word sense ambiguity in the usedcorpus is 5.8 senses, a sentence with 10 content wordswould have 5.8 t?
possible sense combinations, lead-ing to a combinatorial explosion of over 43,080,420overall probability combinations, which is not feasi-ble.
Also, with a very small training corpus, it isnot possible to estimate the sense probabilities veryaccurately.
Therefore, we have opted for a hierar-chical disambiguation approach based on similaritymeasures between the tested and the training rela-tions, which we will describe in Section 5.2.
At first,however, we will describe the part of the probabilis-tic model which assigns probability estimates to theindividual sense combinations based on the semanticrelations acquired in the learning phase.5.1 Relat ional  P robab i l i ty  Es t imateConsider, for example, the syntactic relation be-tween a head noun and its adjectival modifier de-rived from NP~ JJ NN.
Let us assume that thenumber of senses in WordNet is k for the adjectiveand 1 for the noun.
The number of possible sensecombinations i therefore m = k ?
1.
The probabilityestimate of a sense combination (i,j) in the relationR, where i is the sense of the modifier (adjective inthis example) and j is the sense of the head (nounin this example), is calculated as follows:fR( i , j )i (6)pR(i,j) = ~ to=lp=tI \[~j) is a sco !e ~f co-occurrence',x with a had  word sense.mantic rel.
Lti >ns R extract.dlase.
Pleas~ n ~te, that beta as~I ~ but rather a 5core of co-oc~ urv), pR(i~j) is not a real plobrather its approximation.
Because tilei count is replaced by a similarity score, the sparsedata problem of a small training corpus is substan-tially reduced.
The score of co-occurrences i  de-fined as a sum of hits of similar pairs, where a hit isI a multiplication of the similarity measures, im(i,x)and sim(j,y), between both participants, i.e.
:rI (7) fR( i , j )= ~= sim(i,z), sim(j,y)where x, yER;  r is the number of rela-tions of the same type (for the above exampleI R=reI(NP,ADJ,NOUN,x,y,1)) found in the trainingcorpus.
To emphasise the sense-restricting contri-bution of each example found, every pair (x,y) isi restricted to contributing to only one sense combina-tion (id): every example pair (x,y) contributes onlyto such a combination for which sim(i, x) * sim(j, y)is maximal.I fR0, j )  represents a sum of all hits in the train-ing corpus for the sense combination (ij).
Becausethe similarity measure (see below) has a value be-tween 0 and 1 and each hit is a multiplication ofI two similarities, its value is also between 0 and 1.The reason why we used a multiplication of simi-larities was to eliminate the contributions of exam-i pies in which one participant belonged to a com-pletely different semantic lass.
For example, thetraining pair new airport, makes no contribution tothe probability estimate of any sense combination ofi a new management, because none of the two senses ofII the noun management (group or human activity) be- longs to the same semantic lass as airport (entity).On the other hand, new airport would contribute toI the probability estimate of the sense combination ofmodern building because one sense of the adjectivemodern is synonymous toone sense of the adjectivei new, and one sense of the noun building belongsto the same conceptual class (entity) as the nounairport.
The situation is analogous for all other re-lations.
The reason why we used a count modifiedI by the semantic distances, rather than a count ofexact matches only, was to avoid situations whereno match would be found due to the sparse data, aproblem of many small training corpora.I Every semantic relation can be represented by arelational matrix, which is a matrix whose firstcoordinate represents he sense of the modifier, theIilwhere fl~(id) i re of s of a mod-ifier sensex ead y, amongthe same semantic atio s e  uring thelearning ph s .
e ote, c use fR( i j )  isnot a count score currences (de-fined below), i , j) r ability buth occurrence4second coordinate represents the sense of the headand the value at the coordinate position (i j) is theestimate of the probability of the sense combination(id) computed by (6).
An example of a relationalmatrix for an adjective-noun relation modern build-ing based on two training examples (new airport andclassical music) is given in Figure 3.
Naturally, themore the examples, the more fields of the matrix getfilled.
The training examples have an accumulativeeffect on the matrix, because the sense probabilitiesin the matrix are calculated as a sum of 'similaritybased frequency scores' of all examples (7) dividedby the sum of all matrix entries, (6).
The most likelysense combination scores the highest value in thematrix.
Each semantic relation has its own matrix.The way all the relations are combined is describedin Section 5.2.5.1.1 Semantic Simi lar i tyWe base the definition of the semantic similaritybetween two concepts (concepts are defined by theirWordNet synsets a,b) on their semantic distance, asfollows:(8) sire(a, b) "-- 1 - sd(a, b) ~-,The semantic distance sd(a,b) is squared in theabove formula in order to give a bigger weight tocloser matches.The semantic distance is calculated as follows.Semant ic  Distance for Nouns and  VerbsI D I  - D D2-  D)sd(a,b) = { .
( ~ + -D-2where DI is the depth of synset a, D2 is the depthof synset D2, and D is the depth of their nearestcommon ancestor in the WordNet hierarchy.
If aand b have no common ancestor, sd(a,b) = 1.If any of the participants in the semantic distancecalculation is a function (derived from a preposi-tional phrase or subordinate clause), the distance isequal to the distance of the function arguments forthe same functor, or equals 1 for different functors.For example, sd(of(sensel), of(sense2)) = sd(sense 1,sense2), while sd(of(senset), about(sense2)) = t, nomatter what sensel and sense2 are.Semant ic  Distance for Adject ivessd(a,b) = 0 for the same adjectival synsets(inci.synonymy),sd(a,b) = 0 for the synsets in antonymy relations,i.e.
for ant(a,b),sd(a,b) = 0.5 for the synsets in the same similaritycluster,sd(a,b) = 0.5 if a belongs to the same similaritycluster as c and b is the antonymy of c (indirectantonymy),sd(a,b) = I for all other synsets.IIIII!IiIIIIIIIItIIENTITYIPHYSICAL OBJECTIARTIFACTFA~IUTY STRUCTUREIAIRFIELD BUILDING( I )I 102207842AIRPORT102055456' ?NRAW MATERIALS 100313161I BULDING(3)BULDING(2) to611462100506493Example to disambiguate: MODERN(X) BUILDING(Y): reI(NP,ADJ,NOUN,X,Y,1)Example training set: NEW(9) AIRPORT: reI(NP,ADJ,NOUN,3006112602,102055456,1)CLASSICAL(I) MUSIC(3): reI(NP,ADJ,NOUN,300306289,100313161,1)sd(AIRPORT, BUILDING(1)) = 1/2(3/6+2/5) = 0.45sd(AIRPORT,BUILDING(2)) = 1.0sd(AIRPORT, RUILDING(3)) = 1.0sd(BUILDING(2),BUILDING(3)) = 1/2(4/5+4/5) = 0.8sd(BUILDING(2),MUSIC(3)) = 1/2(3/5 + 2/4) = 0.55sire(AIRPORT, BUILDING(I)) = 1-0.452 = 0.8Relational matrix:BUILDING(I) 0.0 0.0 0.0 0.0 0.6~ IBUILDING(2) 0.0 0.0 0.4 0.0 0.0 ~,1 !
BUILDING(3) 0.0 0.0 .,: 0.0 0.0 0.0 .
j  .
.
.
.
.
.
.
.
.
.
o"sim(MUSIC(3),BUILDING(2)) = 1-0.552= 0.7 ,-??'-?a?
:~O ~ z~: -~?
z?sim(CLASSICAL(1)'MODERN(3)) = 1-0"52= 0"75 ~' :* i ~ O  ~ ZO O~z~O ' <m O0 zO -- :~0 mofR(5, I) = sim(NEW(9),MODERN(5))" sire(AIRPORT, BUILDING(1)) = ~ >'n ~=1.0"0.8=0.8 zz  -~z z :fR(3,2) = sire(CLASSICAL( 1),MOO ERN(3)) * sim(MUSIC(3),BUILDING(2)) = ."
~ c~ ~ m~:'~ ~'~ ::= 0.75 "0.7 = 0.53 : -r me" ~ :pR(3,2) = fR(3,2)/sum(fR(i,j)) =0.53/(0.8+0.53) = 0.4 .. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.  "
~ ..... ,,-"pR(5,1) fR(5,l)/sum(fR(i,j)) = 0.8/(0.8+0.53) = 0.6 :1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
* .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
? "
?Figure 2: Relational matrix based on two training examplesSemant ic  Distance for Adverbssd(a,b) = 0 for the same synsets (incl.synonymy),sd(a,b) = 0 for the synsets in antonymy relationant(a,b),sd(a,b) = I for all other synsets.5.2 Hierarchical D isambJguat ionThis section describes the main part of the algo-rithm, i.e.
the disambiguation process based onthe overall probability estimate of sententia\] rela-tions.
As we have outlined above, for computationalreasons, it is not feasible to evaluate overall proba-bilities for all the sense combinations.
Instead, wetake advantage of the hierarchical structure of eachsentence and arrive at the optimum combination ofits word senses, in a process which has two parts:\[.
bottom-up ropagation of the head word sensescores and 2. top-down disambiguation.5.2.1 Bot tom-up head word  sense scorepropagation\[n compliance with our assumption that all thesemantic relations are only between a head wordand its modifiers at any syntactic level, the modi-fiers do not participate in any relation with an ele-ment outside their parent phrase.
As depicted in theexample in Figure l, it is only the head word con-cepts that propagate through the parse tree and thatparticipate in semantic relations with concepts onother levels of the parse tree.
The modifiers (whichare heads themselves at lower tree levels), however,play an important role in constraining the head-wordsenses.
The number of relations derived at each levelof the tree depends on the number of concepts thatmodify the head.
Each of these relations contributesto the score of each sense of the head word.
We de-fine the sense score vector  of a word w as a vectorof scores of each WordNet sense of the word w. Theinitial sense score vector  of the word w is givenby its contextually independent sense distributionin the whole training corpus.
Because the trainingcorpus is relatively small, and because it always ex-cludes the tested file, an appropriate sense of theword w may not be present in it at all.
Therefore,each sense i of the word w is always given a non-zeroinitial score Pi(W) (ga):5IIiII(9a)pi(w) = , count(w), + 1(cou.t(w)i + l)j=twhere count(w), is the number of occurrences of thesense i of the word w in the entire training corpus,and n is the number of different WordNet senses ofthe word w.The sense score vectors of head words propagateup the tree.
At each level, they are modified byall the semantic relations with their modifiers whichoccur at that level.
Also, the sense score vectors ofhead words are used to calculate the matrices of thesense score vectors of the modifiers.
This is done asfollows:Let H -- Jill, h2 .... , hit\] be the sense score vectorof the head word h. Let T = \[R1, R2, ...Rn\] be aset of relations between the head word h and itsmodifiers.1.
For each semantic relation R,  E T between thehead word h and a modifier mi with sense scorevector Mi = loll, oi2 .... oil\], do:1.1 Using (6), calculate the relational matrixRi(m,h) of the relation Ri1.2 For each ol E Mi multiply all the elementsof the R i (m,h)  for which m=oi  by oi,yielding Qi - the sense  score  matr ix  ofthe modifier mi2.
The new sense score vector of the head word his now G-" \[gl ,g2, ...,gk\], whereLj(lo)g i = 2-- ,  h~L j /L  represents the score of the head wordsense j based on the matrices Q calculated inthe step 1., i.e.
:( l l )  Lj = ~ maz(zi(j, u))i= Iwhere x i ( j ,u)E Qi and max(x i ( j ,u ) )  is thehighest score in the line of the matrix Qi whichcorresponds to the head word sense j. n is thenumber of modifiers of the head word h at thecurrent ree level, andki Lj = j~l Ljwhere k is the number of senses of the headword h.The reason why gj (I0) is calculated as a sum ofthe best scores (ll), rather than by using the tradi-tional maximum likelihood estimate (Berger et al,1996)(Gah eta\[., 1993), is to minimise the effect ofthe sparse data problem.
Imagine, for example, thephrase VP-- VB NP PP, where the head verb VBis in the object relation with the head of the nounphrase NP and also in the modifying relation withthe head of the prepositional phr~e PP.
Let us alsoassume that the correct sense of the verb VB is a.Even if the verb-object relation provided a strongselectional support for the sense a, if there was noexample in the training set for the second relation(between VB and PP) which would score a hit for thesense a, multiplying the scores of that sense derivedfrom the first and from the second relation respec-tively, would gain a zero probability for this senseand thus prevent its correct assignment.The newly created head word sense score vector Gpropagates upwards in the parse tree and the sameprocess repeats at the next syntactic level.
Notethat at the higher level, depending on the head ex-traction rules described in section 3, the roles maybe changed and the former head word may become amodifier of a new head (and participate in the abovecalculation as a modifier).
The process repeats itselfuntil the root of the tree is reached.
The word sensescore vector which has reached the root, represents avector of scores of the senses of the main head wordof the sentence (verb said in the example in Figure1), which is based on the whole syntactic structureof that sentence.
The sense with the highest score isselected and the sentence head disambiguated.5.2 .2  Top-down Disambiguat ionHaving ascertained the sense of the sentence head,the process of top-down disambiguation begins.
Thetop-down disambiguation algorithm, which startswith the sentence head, can be described recursivelyas follows:Let 1 be the sense of the head word h on the in-put.
Let M- \ [ml ,m2, .
.
.
,mx\ ]  be the set of themodifiers of the head word h. For every modifiermi E M,  do:l. In the sense score matrix Qi of the modifier mi(calculated in step 1.2 of the bottom-up hase)find all the elements x(ki, l) ,  where I is the senseof the head h2.
Assign the modifier mi such a sense k - -k '  forwhich the value x(ki, l)  is maximum.
In thecase of a draw, choose the sense which is listedas more frequent in WordNet.3.
If the modifier mi has descendants in the parsetree, call the same algorithm again with ml be-ing the head and k being its sense, else end.The disambiguation of the modifiers (which becomeheads at lower levels of the parse tree), is basedsolely on those lines of their sense score matriceswhich correspond to the sense of the head they arein relation with.
This is possible because of our as-sumption that the modifiers are related only to theirhead words, and that there is no relation among themodifiers themselves.
To what extent this assump-IIIIiIIIIIIIII!IiIITable 3: Number of words with the same and different sense as its previous occurrence in the same discourse(shortened)Has predecessor with the same senseDistanceanywhereNOUNS\[' 15,373VERBS6,923<10 9,474 3,697<5 6,892 2,4265,9644,7973,039<3<22,0651,578986ADJs ADVs5,523 38122,733 16721,834 10001,566 8411,219 614733 348NOUNS VERBS2,057 5,227649 2,521355 1,561290 1,269208 929103 555ADJs ADVs933 830258 214104 135104 8283 5542 27tion holds in real life sentences, however, has yet tobe investigated.6 Discourse Context(Yarowsky, 1995) pointed out that the sense of a tar-get word is highly consistent within any given doc-ument (one sense per discourse).
Because our al-gorithm does not consider the context given by thepreceding sentences, we have conducted the follow-ing experiment to see to what extent the discoursecontext could improve the performance of the word-sense disambiguation:Using the semantic concordance files (Miller et al,1993), we have counted the occurrences of contentwords which previously appear in the same discoursefile.
The experiment indicated that the "one senseper discourse" hypothesis works fairly well for nouns,however, the evidence is much weaker for verbs, ad-verbs and adjectives.
Table 3 shows the numbers ofcontent words which appear previously in the samediscourse with the same meaning (same synset), andthose which appear previously with a different mean-ing.
The experiment also confirmed our expectationthat the ratio of words with the same sense to thosewith a different sense, depends on the distance ofsentences in which the same words appear (distanceI indicates that the same word appeared in the pre-vious sentence, distance 2 that the same word waspresent 2 sentences before, etc.
).We have modified the disambiguation algorithmto make use of the information gained by the aboveexperiment in the following way: All the disam-biguated words and their senses are stored.
Thewords of all the input sentences are first comparedwith the set of these stored word-sense pairs.
If thesame word is found in the set, the initial sense scoreassigned to it by (ga) is modified using Table 3, sothat the sense, which has been previously assignedto the word, gets higher priority.
The calculation ofthe initial sense score (9a) is thus replaced by (9b):(9b)pi(w) = nc?unt(w)i + 1 *e(POS, SN)E(count(w)j + l)J=!Table 4: Result Accuracy \[%\]CONTEXT NOUNS VERBS ADJs ADVs TOTALFirst sense 77.8 61.7 81.9 84.5 75.2Sentence 84.2 63.6 82.9 86.3 79.4+Discourse 85.7 63.9 83.6 86.5 80.3where e(POS,SN) is the probability that the wordwith syntactic ategory POS which already occurredSN sentences before, has the same sense as its previ-ous occurrence.
If, for example, the same noun hasoccurred in the previous entence (SN=I)  where itwas assigned sense n, the probability of sense n ofthe same noun in the current sentence is multipliedby e(NOUN,I)=3,039/(3,039+I03)=0.967, while allthe probabilities of its remaining senses are multi-plied by I-0.967=0.033.
Ifno match is found, i.e.
theword has not previously occurred in the discourse,e(POS,SN) is set to 1 for all senses.7 EvaluationTo evaluate the algorithm, we randomly selected 15files (with a total of 18,413 content words tagged inSemCor) from the set of 103 files of the sense taggedsection of the Brown Corpus.
Each tested file wasremoved from the set and the remaining 102 fileswere used for learning (Section 4).
Every sense as-signed by the hierarchical disambiguation algorithm(Section 5) was compared with the sense from thecorresponding semantic oncordance file.
Table 4shows the achieved accuracy compared with the ac-curacy which would be achieved by a simple use ofthe most frequent sense.As the above table shows, the accuracy of the wordsense disambiguation achieved by our method wasbetter than using the first sense for all lexicai cate-gories.
In spite of a very small training corpus, theoverall word sense accuracy exceeds 80%.8 Re la ted  WorkTo our knowledge, there is no current method whichattempts to identify the senses of all words in whole7!
!sentences, o we cannot make a practical compari-son.Similarly to our work, (Resnik, 1995)(Agirre andRigau, 1996) challenge the fine-grainedness of Word-Net, but their work is limited to nouns only.
(Agirreand Rigau, 1996) report coverage 86.2%, precision71.2% and recall 61.4% for nouns in four randomlyselected semantic oncordance files.
From amongthe methods based on semantic distance, (Reanik,1993)(Sussna, 1993) use a similar semantic distancemeasure for two concepts in WordNet, but they alsofocus on selected group of nouns only.
(Karov andEdelman, 1996) use an interesting iterative algo-rithm and attempt o solve the sparse data bottle-neck by using a graded measure of contextual sim-ilarity.
They achieve 90.5, 92.5, 94.8 and 92.3 per-cent accuracy in distinguishing between two sensesof the noun drug, sentence, suit and player, re-spectively.
(Yarowsky, 1995), whose training corpusfor the noun drug was 9 times bigger than that ofKarov and Edelman, reports 91.4% correct perfor-mance improved to impressive 93.9% when using the"one sense per discourse" constraint.
These meth-ods, however, focus on only two senses of a verylimited number of nouns and therefore are not com-parable with our approach.9 ConclusionThis paper presents a new general approach to wordsense disambiguation.
Unlike most of the existingmethods, it identifies the senses of all content wordsin a sentence based on an estimation of the overallprobability of all semantic relations in that sentence.By using the semantic distance measure, our methodreduces the sparse data problem since the trainingexamples and their contexts do not have to matchthe disambiguated words exactly.
All the semanticrelations in a sentence are combined according to thesyntactic structure of the sentence, which makes themethod particularly suitable for integration with astatistical parser into a powerful Natural LanguageProcessing system.
The method is designed to workwith any type of common text and is capable of dis-tinguishing among many word senses.
It has a verywide scope of applicability and is not limited to onlyone part-of-speech.M.
Collins.
1996.
A new statistical parser based onbigram lexical dependencies.
In Proc.
of the .
?4thAnnual Meeting of the ACL, pages 184-191.W.
Gale, K. Church, and D. Yarowsky.
1993.
Amethod for disambiguating word senses in a largecorpus.
Computers and Humanities, (26):415-4397.F.
Jelinek, J. Lafferty, D. Magerman, R. Mercer,A.
Rathnaparkhi, and S. Roukos.
1994.
Decisiontree parsing using a hidden derivation model.
InProc.
of the ARPA Human Language TechnologyWorkshop, pages 272-277.Y.
Karov and S. Edelman, 1996.
Learningsimilarity-based word sense disambiguation fromsparse data.
In Proc.
of the 3rd Workshop on VeryLarge Corpora, pages 42-55.D.
Magerman.
1995.
Statistical decision-tree mod-els for parsing.
In Proc.
of the 33rd Annual Meet-in 9 of ACL, pages 276-283.M.
Marcus.
1993.
Building a large annotated corpusof english: The penn treebank.
ComputationalLinguistics, 2( 19):313-330.G.
Miller, C. Leacock, and R. Tengi.
1993.
A seman-tic concordance.
In Proc.
of the ARPA HumanLanguage Technology Workshop, ages 303-308.G Miller.
1990.
Wordnet: An on-line lexicaldatabase.
International Journal of Lexicography,3(4):235-312.P.
Resnik.
1993.
Semantic lassses and syntacticambiguity.
In Proc.
of the APRA Human Lan-guage Technology Workshop, pages 278-283.P.
Resnik.
1995.
Disambiguating noun groupingswith respect to wordnet senses.
In Proc.
of the3rd Workshop on Very Large Corpora.M.
Sussna.
1993.
Word sense disambiguation forfree-text indexing using a massive semantic net-work.
In Proc.
of Second International Confer-ence on Information and Knowledge Management,pages 67-74.D.
Yarowsky.
1995.
Unsupervised word sense disam-biguation rivaling supervised methods.
In Proc.
o/the 32nd Annual Meeting of the ACL.!References!!!!E.
Agirre and G. Rigau.
1996.
Word sense disam-biguation using conceptual density.
In Proc.
ofCOLLING, pages 16-22.A.
Berger, V. Pietra, and S. Pietra.
1996.
A max-imum entropy approach to natural anguage pro-cessing.
Computatzoaal Linguistics, 1(22):39-72.
