Error Annotation for Corpus of Japanese Learner EnglishEmi Izumi                    Kiyotaka Uchimoto                 Hitoshi IsaharaNational Institute of Information and Communications Technology (NICT),Computational Linguistics Group3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan{emi,uchimoto,isahara}@nict.go.jpAbstractIn this paper, we discuss how errorannotation for learner corpora shouldbe done by explaining the state of theart of error tagging schemes in learnercorpus research.
Several learnercorpora, including the NICT JLE(Japanese Learner English) Corpus thatwe have compiled are annotated witherror tagsets designed by categorizing?likely?
errors implied from theexisting canonical grammar rules orPOS (part-of-speech) system inadvance.
Such error tagging can help tosuccessfully assess to what extentlearners can command the basiclanguage system, especially grammar,but is insufficient for describinglearners?
communicative competence.To overcome this limitation, we re-examined learner language in the NICTJLE Corpus by focusing on?intelligibility?
and ?naturalness?, anddetermined how the current error tagsetshould be revised.1 IntroductionThe growth of corpus research in recent years isevidenced not only by the growing number ofnew corpora but also by their wider variety.Various ?specialized corpora?
have recentlybeen created.
One of them is the ?learnercorpus?, which is a collection of the languagespoken or written by non-native speakers.
Theprimary purpose of learner corpora is to offerSecond Language Acquisition (SLA)researchers and language teaching professionalsresources for their research.
In order to developa curriculum or pedagogy of language teaching,it would be beneficial to have interlanguage dataso that researchers can scientifically describe thecharacteristics of each developmental stage oftheir interlanguage.
One of the most effectiveways of doing this is to analyze learner errors.Some of the existing learner corpora areannotated for errors, and our learner corpuscalled the ?NICT JLE (Japanese LearnerEnglish) Corpus?
is one of them.
This is a two-million-word speech corpus of Japanese learnerEnglish.
The source of the corpus data is 1,281audio-recorded speech samples of an Englishoral proficiency interview test ACTFL-ALCStandard Speaking Test (SST).
The advantage ofusing the SST data as a source is that eachspeaker?s data includes his or her proficiencylevel based on the SST scoring method, whichmakes it possible to easily analyze and comparethe characteristics of interlanguage of eachdevelopmental stage.
This is one of theadvantages of the NICT JLE corpus that israrely found in other learner corpora.Although there are a lot of advantages oferror-annotated learner corpora, we found somedifficulties in designing an error tagset thatcovers important features of learner errors.
Thecurrent version of our error tagset targetsmorphological, grammatical, and lexical errors,and we found it can help to successfully assessto what extent learners can command the basiclanguage system, especially grammar.
However,we also found that it is not sufficient to measurelearners?
communicative skills.
In order todetermine how the current error tagset should beextended to cover more communicative aspects71of learner language, we re-examined the learnerdata in the NICT JLE Corpus by focusing on?intelligibility?
and ?naturalness?.In this paper, we discuss how errorannotation for learner corpora should bedesigned and actually performed.
The remainderof this paper is organized as follows.
Section 2outlines the influence that Error Analysis (EA)in SLA research in the 1970s had on errorannotation for learner corpora to enable us torethink the concept of error annotation.
Section3 provides some examples of learner corpusprojects in which error tagging is performed.Section 4 describes the current error taggingscheme for the NICT JLE Corpus.
Section 5examines how we can expand it to make it moreuseful for measuring learners?
communicativeskills.
Finally, section 6 draws some generalconclusions.2 Error Tagging and EAThe idea of trying to tag errors in learnercorpora might come from the notion of EA inSLA research in the 1970s.
In order to design anerror tagset and to actually perform tagging, wewould like to reconfirm the concept of thetraditional EA by considering about thedefinition of learner errors, the importance ofanalyzing learner errors for describing learnerlanguage, the actual EA procedures, andproblems, and limitations of EA.2.1 Definition of Learner ErrorsErrors in a second language (L2) are oftencompared with errors in the first language (L1).According to Ellis (1994), before EA wasintroduced, L2 errors were often considered as?undesirable forms?.
On the other hand, errorsmade by young L1 learners were regarded as the?transitional phase?
in L1 acquisition, whileerrors made by adult L1 speakers were seen justas slips of the tongue.
L2 errors were oftenshoved back into the closet as a negative aspectof learner language.In EA, errors are treated as evidence thatplays an important role in describing learnerlanguage.
Corder (1981) asserts that talkingabout learner errors only with terms like?deviant?
or ?ill-formed?
is inappropriatebecause it leads to learner errors being treatedjust as superficial deviations.
Even if learnersproduce outputs whose surface structures arewell-formed, this is not enough to prove thatthey have acquired the same language system asthat L1 speakers have.In EA, learner errors are treated as somethingthat proves that learners are in the transitionalphase in L2 acquisition in a similar way totreating the language of L1 children.
However, itis problematic to assume these two are exactlythe same language.
In L1 and L2 acquisition,there are certain processes in common, but theyhave not been scientifically confirmed.
Manydifferences are found between these two.
Thelearner language including errors can be definedas ?interlanguage?, which lies between L1 andL2 (Selinker, 1972).
According to Corder (1981),learner errors are evidence of learning strategiesin which learners are ?investigating?
the systemof the new language (L2) by examining to whatextent L1 and L2 are similar and how differentthey are.2.2 Importance of EAAnalyzing learner errors is important forteachers, researchers, and learners themselves inthe following way (Corder, 1981).
First, forteachers, errors can give them hints about theextent to which the learners have acquired thelanguage system by that time and what they stillhave to learn.
For SLA researchers, errors canreveal the process by which L2 is acquired andthe kinds of strategies or methodology thelearners use in that process.
Finally, for learnersthemselves, as stated in 2.1, making errors is oneof the most important learning strategies fortesting the interlanguage hypothesis that learnershave established about L2.
In other words,knowing what kinds of errors were made bythemselves or by other learners can be ?negativeevidence (or feedback)?
given directly orindirectly to learners that an interlanguagehypothesis is incorrect (Ellis, 1997).2.3 EA ProcedureIn general, the EA procedure can be divided intofour stages as shown in Figure 1 (Ellis, 1994).72Identifying ErrorsDescribing ErrorsExplaining ErrorsEvaluating ErrorsFigure1.
EA Procedure.In the first stage, identifying errors, it isnecessary to localize errors by pointing outwhich letters, words, and phrases, or howsentence structures or word order, are incorrect.In the second stage, identified errors should bedescribed by being linguistically categorizeddepending on, for example, their POS (part-of-speech), linguistic level (morpheme, syntax,lexis, or discourse), or how they deviate fromthe correct usage on the surface structure(redundancy, omission, or replacement).
Thirdly,?explaining errors?
means identifying why thoseerrors occurred.
This is a very important task inorder to figure out the learners?
cognitive stage.Some causes of learner errors have beenrecognized in common such as errors caused bylanguage transfer, learning and communicationstrategy-based errors, and the transfer of trainingand induced errors.
Finally, errors are evaluated.This can be done by estimating intelligibility ornear-nativeness of erroneous outputs.
In otherwords, ?error gravity?
is estimated by examininghow each error interferes with the intelligibilityof the entire outputs.2.4 Problems and Limitations ofTraditional EAAlthough it is widely recognized that EAcontributes to describing learner language andthe improving second language pedagogy,several problems and limitations have beenpointed out mainly because a concretemethodology of EA has not been established yet.Most importantly, EA cannot be successfulwithout robust error typology, which is oftenvery difficult to obtain.
Since it used to bedifficult to collect or access large databases oflearner language, a robust error typology thatcovers almost all error types was not establishedin traditional EA.Another criticism against EA is that errorsreflect only one side of learner language.
A lotof people point out that if a researcher analyzesonly errors and neglects what learners can docorrectly, he/she will fail to capture the entirepicture of learner language.
It is time-consumingto count both correct and incorrect usages inlearner data, and this must have been quitedifficult to do in the past before computingtechnology was developed.Furthermore, the real significance of EAcannot be identified without using diachronicdata in order to describe learners?
developmentalstages.
The types and frequencies of errorschange with each acquisition phase.
Withoutlongitudinal data of learner language, it isdifficult to obtain a reliable result by EA.2.5 From EA to Error?coded LearnerCorporaThe problems and limitations of traditional EAare mainly due to the deficiency of computingtechnology and the lack of large databases inearly times.
However, now that computingtechnology has advanced, and a lot of learnerdata is available, it might be possible to performEA more effectively mainly by annotating errors.Although the basic motivations for errorannotation are the same as those of traditionalEA, such as describing learner language andimproving second-language pedagogy, severalnew applications of EA might become possiblesuch as the development of a new computer-aided language learning (CALL) environmentthat can process learners?
erroneous input andgive feedback automatically.Degneaux, et al (1998) call EA based onlearner corpora ?computer-aided error analysis(CEA)?, and expect that the rapid progress ofcomputing technology and learner corpora willbe able to solve the problems and overcome thelimitations of traditional EA.
Surely, thanks tothe quantitative database of learner language, wewill become able to cover a wider range oflearner errors.
Advances in computingtechnology make it possible to performstatistical analysis with quantitative data moreeasily.
However, it must be noted that humanresearchers still have a lot of work to do in thesame manner as in traditional EA, such asestablishing an error typology for error tagging73or examining results obtained from CEAcarefully.3 Related WorkThere are a few learner corpus projects thatimplement CEA.
For example, in theInternational Corpus of Learner English (ICLE)project, which was launched by ProfessorSylviane Granger at the University of Louvain,Belgium, and has been a ?pioneer?
in learnercorpus research since the early 1990s, theyperformed error tagging with a custom-designederror tagset (Degneaux, et al, 1996).
Thegrammatical, lexical and pragmatic errors aredealt with in their error tagset and the correctedform is also indicated for each error.
We guessthat error categorization has been done mainlyby translating the basic English grammar orlexical rules into an error ontology to try tocover as many types of errors as possible.
TheICLE team currently comprises 17 partnersinternationally, and the corpus encloses 17subcorpora of learners of the different mothertongues (Bulgarian, Czech, Dutch, Finnish,French, German, Italian, Polish, Russian,Spanish, Swedish, and so on).
A comparison ofthis data will make possible ?contrastiveinterlanguage analysis (CIA)?
proposed byGranger (2002).
CIA involves both NN/NNSand NNS/NNS comparisons (NS: nativespeakers; NNS: non-native speakers), as shownin Figure 2.
NS/NNS comparisons might revealhow and why learner language is non-nativelike.NNS/NNS comparisons help researchers todistinguish features shared by several learnerpopulations, which are more likely to bedevelopmental from ones peculiar to a certainNNS group, which may be L1-dependent.CIANS NNS NNS NNSvs vsFigure 2.
Contrastive Interlanguage Analaysis(Granger, 2002).Another corpus thathas been error tagged isthe ?Japanese EFL Learner (JEFLL) Corpus?.This corpus, which was created by ProfessorYukio Tono at Meikai University in Japan, hasthree parts: i) the L2 learner corpora whichinclude written (composition) and spoken(picture description) data of Japanese learnerEnglish, ii) the L1 corpora consisting ofJapanese written texts for the same tasks asthose in the first part and Japanese newspaperarticles, and iii) the EFL textbook corpus, whichis the collection of EFL textbooks used officiallyat every junior high school in Japan (Tono,2002).
Compared with the ICLE, which hasbeen annotated with the generic error tagset, theerror tagging for the JEFFL Corpus focuses onspecific types of errors, especially majorgrammatical morphemes such as articles, pluralforms of nouns, and third person singularpresent forms of verbs, and so on.
We assumethat those items were selected due to the corpusdeveloper?s research interests.
Althoughcompleteness for covering all errors would bedecreased by focusing on a limited number oferror types, annotators will be able to performtagging more stably without being confusedamong various different types of errors.The Cambridge Learners?
Corpus (CLC),which has been compiled by CambridgeUniversity Press and Cambridge ESOL (Englishfor Speakers of Other Languages), is also anerror-coded learner corpus.
It forms part of theCambridge International Corpus (CIC) and is alarge collection of essay writing from learners ofEnglish all over the world.
This corpus has beenutilized for the development of publications byauthors and writers in Cambridge UniversityPress and by members of staff at CambridgeESOL.
Over eight million words of the CLChave been error-coded with a Learner ErrorCoding System devised by CambridgeUniversity Press.
In order to make the taggeddata as consistent as possible, tagging has beendone by only one annotator since it started in1993.
Their error tagset covers 80 types of errors.The annotator chooses an appropriate tag foreach error mainly by identifying which POS theerror involves and how it deviates from thecorrect usage (redundancy, omission, orreplacement).
A corrected form is also indicatedfor each error.4 Error Tags in the NICT JLE CorpusIn this section, we introduce the error annotationscheme we used for the NICT JLE Corpus.We are aware that it is quite difficult todesign a consistent error tagset as the learner74errors extend across various linguistic areas,including grammar, lexis, and phoneme, and soon.
We designed the original error tagset onlyfor morphological, grammatical, and lexicalerrors, which are relatively easy to categorizecompared with other error types, such asdiscourse errors and other types of errors relatedto more communicative aspects of learners?language.
As shown in Figure 3, our error tagscontain three pieces of information: POS,morphological/grammatical/lexical rules, and acorrected form.
For errors that cannot becategorized as they do not belong to any wordclass, such as the misordering of words, weprepared special tags.
The error tagset currentlyconsists of 46 tags (Table 1).POS(i.e.
n =noun)Grammatical system(i.e.
num =number)Erroneous partCorrected form<n_num crr=?X?>?</n_num>example) I belong to two baseball <n_num crr=?teams?>team</n_num>.Figure 3.
Structure of an Error Tag and anExample of an Error-tagged SentenceThe tags are based on XML (extensiblemarkup language) syntax.
One advantage ofusing XML is that it can clearly identify thestructure of the text and it is also very beneficialwhen corpus data is utilized for web-basedpedagogical tools or databases as a hypertext.The error tagset was designed based on theconcept of the ICLE?s error tagging, that is, todeal with as many morphological, grammatical,and lexical errors as possible to have a genericerror tagset.
However, there are severaldifferences between these two tagsets.
Forexample, in the ICLE, only replacement-typeerrors are linguistically categorized, andredundant- and omission-type errors are notcategorized any more and just called as ?wordredundant?
or ?word missing?, while in ourerror tagset, all these three types of errors arelinguistically categorized.Although our error tagset covers majorgrammatical and lexical errors, annotators oftenhave difficulties to select the most appropriateone for each error in actual tagging process.
Forexample, one erroneous part can often beeninterpreted as more than one error type, orsometimes multiple errors are overlapping in thesame position.To solve these problems, tagging was doneunder a few basic principles as follows.1) Because of the limitation of XML syntax(i.e.
Crossing of different tags is notallowed.
), each sentence should becorrected in a small unit (word or phrase)and avoid to change a sentence structureunnecessarily.2) If one phenomenon can be interpreted asmore than one error type, select an errortype with which an erroneous sentencecan be reconstructed into a correct onewithout changing the sentence structuredrastically.
In this manner, errors shouldbe annotated as locally as possible, butthere is only one exception forprefabricated phrases.
For example, if asentence ?There are lot of books.?
shouldbe corrected into ?There are a lot ofbooks.
?, two ways of tagging arepossible as shown in a) and b).a) There are <at crr= ?a?></at> lot ofbooks.b) There are <o_lxc crr= ?a lot of?>lotof</o_lxc> books.In a), just an article ?a?
is added before?lot of?, while in b), ?lot of?
is correctedinto ?a lot of?
as a prefabricated phrase.In this case, b) is preferred.3) If multiple errors overlap in the same orpartly-same position, choose error tagswith which an erroneous sentence can bereconstructed into a correct one step bystep in order to figure out as many errorsas possible.
For example, in the case thata sentence ?They are looking monkeys.
?should be corrected into a sentence?They are watching monkeys.
?, two waysof tagging are possible as shown in c)and d).c) They are <v_lxc crr= ?watching?>looking</v_lxc> monkeys.d) They are <v_lxc crr= ?watching?>looking<prp_lxc2 crr= ?at?></prp_lxc2></v_lxc> monkeys.In c), ?looking?
is replaced with?watching?
in one step, while in d),missing of a preposition ?at?
is pointedout first, then, ?looking at?
is replaced75with ?watching?.
In our error taggingscheme, d) is more preferred.Tag Error category<n_inf>?</n_inf> Noun inflection<n_num>?</n_num> Noun number<n_cs>?</n_cs> Noun case<n_cnt>?</n_cnt> Countability of noun<n_cmp>?</n_cmp> Complement of noun<n_lxc>?</n_lxc> Lexis<v_inf>?</v_inf> Verb inflection<v_agr>?</v_agr> Subject-verb disagreement<v_fml>?</v_fml> Verb form<v_tns>?</v_tns> Verb tense<v_asp>?</v_asp> Verb aspect<v_vo>?</v_vo> Verb voice<v_fin>?</v_fin> Usage of finite/infinite verb<v_ng>?</v_ng> Verb negation<v_qst>?</v_qst> Question<v_cmp>?</v_cmp> Complement of verb<v_lxc>?</v_lxc> Lexis<mo_lxc>?</mo_lxc> Lexis<aj_inf>?</aj_inf> Adjective inflection<aj_us>?</aj_us> Usage of positive/comparative/superlative of adjective<aj_num>?</aj_num> Adjective number<aj_agr>?</aj_agr> Number disagreement of adjective<aj_qnt>?</aj_qnt> Quantitative adjective<aj_cmp>?</aj_cmp> Complement of adjective<aj_lxc>?</aj_lxc> Lexis<av_inf>?</av_inf> Adverb inflection<av_us>?</av_us> Usage of positive/comparative/superlative of adverb<av_pst>?
</av_pst> Adverb position<av_lxc>?</av_lxc> Lexis<prp_cmp>?</prp_cmp> Complement of preposition<prp_lxc1>?</prp_lxc1> Normal preposition<prp_lxc2>?</prp_lxc2> Dependent preposition<at>?</at> Article<pn_inf>?</pn_inf> Pronoun inflection<pn_agr>?</pn_agr> Number/sex disagreement of pronoun<pn_cs>?</pn_cs> Pronoun case<pn_lxc>?</pn_lxc> Lexis<con_lxc>?</con_lxc> Lexis<rel_cs>?</rel_cs> Case of relative pronoun<rel_lxc>?</rel_lxc> Lexis<itr_lxc>?</itr_lxc> Lexis<o_je>?</o_je> Japanese English<o_lxc>?</o_lxc> Collocation<o_odr>?</o_odr> Misordering of words<o_uk>?</o_uk> Unknown type errors<o_uit>?</o_uit> Unintelligible utteranceRELATIVE PRONOUNINTERROGATIVEOTHERSPREPOSITIONARTICLEPRONOUNCONJUNCTIONNOUNVERBMODAL VERBADVERBADJECTIVETable 1.
Error Tags for the NICT JLE Corpus.4.1 Advantages of Current Error TagsetError tagging for learner corpora including theNICT JLE Corpus and the other corpora listed inSection 3 is carried out mainly by categorizing?likely?
errors implied from the existingcanonical grammar rules or POS system inadvance.
In this sub-section, we examine theadvantages of this type of error tagging throughresearch and development done by using thesecorpora.Tono (2002) tried to determine the order inwhich Japanese learners acquire the majorEnglish grammatical morphemes using the errortag information in the JEFFL Corpus.
Izumi andIsahara (2004) did the same investigation basedon the NICT JLE Corpus and found that therewas a significant correlation between theirsequence and Tono?s except for a fewdifferences that we assume arose from thedifference in the languga e production medium(written or spoken).
Granger (1999) found thatFrench learners of English tended to make verberrors in the simple present and past tensesbased on the French component of the ICLE.Izumi et al (2004) also developed a frameworkfor automated error detection based on machinelearning in which the error-tagged data of theNICT JLE Corpus was used as training data.
Inthe experiment, they obtained 50% recall and76% precision.Error tagging based on the existing canonicalgrammar rules or POS system can help tosuccessfully assess to what extent learners cancommand the basic language system, especiallygrammar.
This can assist people such as teacherswho want to improve their grammar teachingmethod, researchers who want to construct amodel of learners?
grammatical competence, andlearners who are studying for exams withparticular emphasis on grammatical accuracy.5 Future ImprovementFinally, let us explain our plans for futureimproving and extending error tagging for theNICT JLE Corpus.5.1 Problems of Current Error TagsetAlthough the current error tagging scheme isbeneficial in the ways mentioned in 4.1, itcannot be denied that much could be improvedto make it useful for teachers and researcherswho want to know learners?
communicativeskills rather than grammatical competence.
Thesame can be said for learners themselves.
In thepast, English education in Japan mainly focusedon developing grammatical competence in thepast.
However, in recent years, because of therecognition of English as an importantcommunication tool among peoples withdifferent languages or cultures, acquiringcommunicative competence, especially76production skills, has become the main goal forlearners.
One of the most important things foracquiring communicative skills might beproducing outputs that can be understoodproperly by others.
In other words, for manylearners, conveying their messages clearly isoften more important than just producinggrammatically-correct sentences.It is necessary to make the current errortagset more useful for measuring learners?communicative competence.
To do this, firstlywe need to know what kind of learners?
outputscan be understood by native speakers and inwhat cases they fail to convey their messagesproperly.
By doing this, it should becomepossible to differentiate fatal errors that preventthe entire output from being understood fromsmall errors that do not interfere withunderstanding.Another goal of studying English for learners,especially at the advanced level, is to speak likea native speaker.
Some learners mind whethertheir English sounds natural or not to nativespeakers.
In the current error tagging, bothobvious errors and expressions that are noterrors but are unnatural are treated at the samelevel.
It would be better to differentiate them inthe new error annotation scheme.5.2 Survey for Extending Current ErrorTagsetTo solve the problems of our current errortagging system discussed in 5.1, we decided todo a survey to:1) Identify fatal errors and small ones byexamining ?learners?
outputs that canbe understood properly by nativespeakers?
and ?those that do not makesense to native speakers?.2) Identify unnatural and non-nativelikeexpressions and examine why theysound unnatural.We will do this mainly by examining thelearner data corrected by a native speaker.Correction by NSWe asked a native speaker of English to correctraw learner data (15 interviews, 17,068 words,1,657 sentences) from the NICT JLE Corpusand add one of the following three comments(Table 2) to each part.Comment 1 It is obviously an error, but doesnot interfere with understanding.Comment 2 The meaning of the utterance doesnot make sense at all.Comment 3 It is not an error, and theutterance makes sense, but itsounds unnatural.Table 2.
Comments added to each errorThe person who did the corrections is amiddle-aged British man who has lived in Japanfor 14 years.
He does not have experience as anEnglish teacher, but used to teach JapaneseLinguistics at a British University.
Although heis familiar with English spoken by Japanesepeople because of his long residence in Japanand the knowledge of the Japanese language, weasked him to apply the corrections objectivelywith considering whether or not each utterancewas generally intelligible to native speakers.Corrected PartsA total of 959 errors were corrected and 724 ofthese were labeled with Comment 1, 57 withComment 2, and 178 with Comment 3,respectively (Table 3).Comment 1 724Comment 2 57Comment 3 178Total 959Table 3.
Number of Errors Labeled with EachComment.In order to examine what kind of differencescan be found among errors labeled with thesecomments, we categorized them into four types(morpheme, grammar, lexis, and discourse)depending on which linguistic level each ofthem belongs to based on corrected forms andadditional comments made by the labeler (Table4).Comment1 Comment2 Comment3 TotalMorpheme 6 0 0 6Grammar 429 0 52 481Lexis 286 43 78 407Discourse 3 14 48 65Total 724 57 178 959Table 4.
Linguistic Level Involved in Each Error.As a whole, the most common type wasgrammar (481), but most of the grammaticalerrors (or cases of unnaturalness) were labeledwith Comment 1, which implies that in mostcases, the grammatical errors do not have a fatalinfluence making the entire output unintelligible.The second-most common type was lexicalerrors (or cases of unnaturalness) (407).
Half ofthem were labeled with Comment 1, but 23errors got Comment 2.
This means that some77errors can interfere with understanding.Discourse errors accounted for a fraction of apercent of all errors (65).
However, comparedwith other types of errors, the percentage ofComment 2 was the highest (14 out of 65),which means that discourse errors can greatlyinterfere with the intelligibility of the entireoutput.
The main difference between thediscourse errors labeled with Comment 2 andthose labeled with Comment 3 was that most ofthe latter related to collocational expressions,while the former involved non-collocationalphrases where learners need to construct aphrase or sentence by combining single words.In the following sections, we examine thecharacteristics of each type of error (or cases ofunnaturalness) in detail.Comment 1Half of the Comment 1 errors were grammaticalones.
Most of them were local errors such assubject-verb disagreement or article errors.There were 286 lexical errors, but in most cases,they were not very serious, for example lexicalconfusions among semantically similarvocabulary items.Comment 2Most of the Comment 2 errors had something todo with lexis or discourse.1) Too abrupt literary style (discourse error)ex) I?ve been to the restaurant is first.
Itook lunch.
The curry the restaurantserves is very much, so I was surprisedand I?m now a little sleepy.2) Unclear context (discourse error)3) Unclear anaphora (pronouns anddemonstratives) (discourse error)4) Mis-selection of vocabulary (lexicalerror)5) Omission of an important word (subject,predicate or object) (lexical or syntaxerror)ex) She didn?t (*) so much about fashion.ex) Last year, I enjoyed living alone, butnowadays, it?s a little bit troublesomebecause I have to (*) all of the things bymyself.6) Japanese English/Direct translation(lexical error)ex) bed town (as ?bedroom suburbs?
)ex) claim (as ?complaint?
)Comment 3There were grammatical, lexical and discourseproblems with the parts labeled with Comment1) Verbose expressions (discourse-levelunnaturalness)ex) T: Can I call you Hanako?L: Yes, please call me Hanako.better  Yes, please do.ex) Three couples are there and they?re havingdinner.better  Three couples are having dinner.ex) I told my friends about this, and my friendsagreed with me.better  I told my friends about this, and theyagreed with me.2) Socio-linguistically inappropriate expressions(discourse/pragmatic-level unnaturalness)ex) What?better  I beg your pardon?ex) Good.better  I?m fine.3) Abrupt expressions (discourse/pragmatic-level unnaturalness)ex) T: Have you been busy lately?L: No.better  No, not really.4) Overstatement (discourse/pragmatic-levelunnaturalness)(In a normal context)ex) T: How are you?L: I?m very fine.better  I?m fine.5) There are more appropriate words orexpressions.
(discourse/pragmatic-level ofunnaturalness)ex) To go to high school in the mainland, I wentout of the island.better  ...
I left the island.5.3 Limitation of Current ErrorAnnotation SchemeIt is obvious that discourse and some types oflexical errors can often impede theunderstanding of the entire utterance.Although our current error tagset does notcover discourse errors, it is still possible to78?just?
assign any one of error tags to theerroneous parts shown in 5.2.
There are tworeasons for this.
One is that, in the current errortagging principle, it is possible to replace, add ordelete all POS in order to make it possible to?reconstruct?
an erroneous sentence into acorrect one.
The other reason is that sincediscourse structure is liked to grammatical andlexical selections, it is possible to translate termsfor describing discourse into terms fordescribing grammar or lexis.However, annotating discourse errors withtags named with grammatical or lexical termscannot represents the nature of discourse errors.Since discourse errors are often related tointelligibility of learners?
outputs, describingthose errors with appropriate terms is quiteimportant for making the current error tagsetsomething helpful for measuring learners?communicative competence.
We will need toknow what kind of discourse errors are made bylearners, and classify them to build in the errortagset.
Some parts labeled with Comment 3were also related to discourse-level problems.
Itwould be beneficial to provide learners withfeedback such as ?Your English soundsunnatural because it?s socio-linguisticallyinappropriate?.
Therefore, it is also necessary toclassify discourse-level unnaturalness in learnerslanguage.5.4 Works for Expansion to New ErrorTagsetWe decided the basic principles for revising thecurrent error tagset as following.1) Classify second language discourseerrors and building them into a newerror tagset.2) Differentiate unnatural expressionsfrom errors.
Information on why itsounds unnatural will also be added.3) Add information on linguistic level(morpheme, grammar, lexis, anddiscourse) to each tag.4) Do a further survey on how we candifferentiate errors that interfere withunderstanding and those that do not,and add information on error gravity toeach tag.Classifying discourse errors will be the mostimportant task in the tagset revision.
In severalstudies, second language discourse has alreadybeen discussed (James, 1998), but there is nocommonly recognized discourse error typology.Although grammatical and lexical errors can beclassified based on the existing canonicalgrammar rules or POS system, in order toconstruct the discourse error typology, we willneed to do more investigation into ?real?samples of learners?
discourse errors.Adding the information on linguistic level(morpheme, grammar, lexis and discourse) toeach tag is also important.
From the survey, wefound that the linguistic level of errors isstrongly related to the intelligibility of the entireoutput.
If linguistic level information is added toeach error tag, this might help to measure theintelligibility of learners?
utterances, that is,learners?
communicative competence.6 ConclusionIn this paper, we discussed how the errorannotation scheme for learner corpora should bedesigned mainly by explaining the current errortagging scheme for the NICT JLE Corpus andits future expansion.
Through learner datacorrected by a native speaker, we decided tointroduce discourse errors into the errorannotation in order to cover learners?communicative competence, which cannot bemeasured with the current error tagging scheme.ReferencesCorder, P. (1981).
Error Analysis and Interlanguage.Oxford: Oxford University Press.Degneaux, E., Denness, S., Granger, S., & Meunier,F.
(1996).
Error Tagging Manual Version 1.1.Centre for English Corpus Linguistics, UniversiteCatholique de Louvain.Degneaux, E., Denness, S., & Ganger, S. (1998).Computer-aided error analysis, System, 26, 163-174.Ellis, R. (1994).
The Study of Second LanguageAcquisition.
Oxford: Oxford University Press.Ellis, R. (1997).
Second Language Acquisition.Oxford: Oxford University Press.
pp.
47, 67.Granger, S. (1999).
Use of tenses by advanced EFLlearners: evidence from an error-tagged computercorpus.
In Hasselgard, H., & Oksefjell, S. (Eds).Out of Corpora.
(pp.
191-202).
Amsterdam:Rodopi.79Granger, S. (2002) A bird?s-eye view of learnercorpus research.
In Granger, S., Hung, J., andTyson, P.S.
(Eds.).
(2002).
Computer LearnerCorpora, Second Language Acquisition andForeign Language Teaching, Amsterdam: JohnBenjamins Publishing Company.Izumi, E., Uchimoto, K., & Isahara, H. (2004).
Theoverview of the SST speech corpus of Japaneselearner English and evaluation through theexperiment on automatic detection of learners'errors.
In Proceedings of Language Resource andEvaluation Conference (LREC) 2004, Portugal,1435-1438.Izumi, E., & Isahara, H. (2004).
Investigation intolanguage learners' acquisition order based on theerror analysis of the learner corpus.
InProceedings of Pacific-Asia Conference onLanguage, Information and Computation(PACLIC) 18 Satellite Workshop on E-Learning.Tokyo, Japan.James, C. (1998).
Errors in Language Learning andUse: exploring error analysis.
Essex: Longman.Selinker, L. (1972).
Interlanguage.
In Robinett, B. W.,& Schachter, J.
(Eds.).
(1983).
Second LanguageLearning: Contrastive analysis, error analysis,and related aspects.
(pp.
173-196).
Michigan: TheUniversity of Michigan Press.Tono, Y.
(2002).
The Role of Learner Corpora inSLA Research and Foreign Language Teaching:The Multiple Comparison Approach.
UnpublishedPh.D.
Thesis.
Lancaster University, UK.CLC (Cambridge Learners Corpus)?s Website:http://uk.cambridge.org/elt/corpus/clc.htm80
