The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 86?94,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsUsing an Ontology for Improved Automated Content Scoring of Spontaneous Non-Native Speech   Miao Chen Klaus Zechner School of Information Studies Educational Testing Service Syracuse University 660 Rosedale Road Syracuse, NY 13244, USA Princeton, NJ 08541, USA mchen14@syr.edu kzechner@ets.org   AbstractThis paper presents an exploration into auto-mated content scoring of non-native sponta-neous speech using ontology-based information to enhance a vector space ap-proach.
We use content vector analysis as a baseline and evaluate the correlations between human rater proficiency scores and two co-sine-similarity-based features, previously used in the context of automated essay scoring.
We use two ontology-facilitated approaches to improve feature correlations by exploiting the semantic knowledge encoded in WordNet: (1) extending word vectors with semantic con-cepts from the WordNet ontology (synsets); and (2) using a reasoning approach for esti-mating the concept weights of concepts not present in the set of training responses by ex-ploiting the hierarchical structure of WordNet.
Furthermore, we compare features computed from human transcriptions of spoken respons-es with features based on output from an au-tomatic speech recognizer.
We find that (1) for one of the two features, both ontologically based approaches improve average feature correlations with human scores, and that (2) the correlations for both features decrease on-ly marginally when moving from human speech transcriptions to speech recognizer output.
1 Introduction Currently, automated speech scoring systems mainly utilize features related to the acoustic as-pects of a spoken response of a test taker, for ex-ample, fluency, pronunciation, and prosody features (Cucchiarini et al, 2000, 2002; Franco et al, 2010; Zechner et al, 2009).
In terms of thecontent aspect of speech, for highly predictable speech, such as reading a passage aloud, scoring of content reduces to measuring the reading accuracy of the read passage which is typically achieved by computing the string edit distance between the tar-get passage and the actual text read by the test tak-er, using the speech recognizer hypothesis as a proxy (Alwan et al, 2007; Balogh et al, 2007).
For high entropy speech whose content is difficult to predict such as spontaneous speech in this study, on the other hand, content scoring has not been investigated much so far, mostly due to the diffi-culty of obtaining accurate word hypotheses for spontaneous non-native speech by Automated Speech Recognition (ASR) systems.
In this paper, we use spoken responses from an English language spoken proficiency test where candidates, all non-native speakers of English, re-spond to four different prompts1 with a speaking time of one minute per response.
For this study, we decide to use a baseline ap-proach for content scoring of spontaneous speech that was previously employed for a similar task in the context of automated essay scoring (Attali & Burstein, 2006), namely Content Vector Analysis (CVA) where every document is represented as a vector of word weights, based on their frequencies in a document or document collection.
However, there are two issues with the CVA vector of words representation that we want to address with this study: (1) Similar words are treated in isolation and not grouped together.
Words with similar meaning should be treated in the same way in an automated scoring system, so grouping word synonyms into semantic concepts can help with this issue.
(2) The vector of word representation is based on an exist-                                                            1 Prompts are test tasks assigned to test takers to elicit spoken responses.86ing corpus of training documents.
When encoun-tering a word or concept in a test document that is not contained in the training set, it is difficult to decide the relevance of that word or concept.
We propose to use ontology-facilitated ap-proaches as solutions to these two issues, aiming at enriching speech content representations to im-prove speech content scoring.
Specifically, to ad-dress issue (1), we represent speech content by concept-level vectors, using the synsets (lists of synonymous words) of the WordNet ontology (Fellbaum, 1998; WordNet 3.0, 2010).
As for issue (2), we expand the vector representation by infer-ring the importance (weight) of concepts not pre-sent in the training vectors based on their path distance to known concepts or words in the hierar-chical structure of the WordNet ontology.
Since we only look at the content aspect of speech without considering the acoustic features in this study, we work on speech transcripts exclu-sively, both from human transcribers as well as from a state-of-the-art automated speech recogni-tion system, and compare results between the ideal human transcripts and the imperfect transcripts generated by the speech recognizer.
For the pur-pose of simplified illustration, speech transcripts are often referred to as ?documents?
in the paper as they are a special type of textual documents.
The remainder of this paper is organized as fol-lows: in Section 2, we review related research in content scoring of texts, particularly student es-says; Section 3 describes the data set we use for this study and the ASR system; and Section 4 pre-sents the ontologically-facilitated methods we are using in detail.
In Section 5, we present our exper-iments along with their results, followed by a dis-cussion in Section 6, and we conclude the paper with a summary and outlook in Section 7.
2 Related Work  There have been some effective approaches for test takers?
written responses in language tests, namely in the area of Automated Essay Scoring (AES).
AES has employed content vector analysis, i.e., vectors of words to represent text, for example, the e-rater system (Burstein, 2003; Attali & Burstein, 2006) and the experimental system in Larkey and Croft (2003).
Representations in the BETSY sys-tem (Bayesian Essay Test Scoring System) also involve words, such as the frequency of contentwords, and also include specific phrases as well (Dikli, 2006).
AES has also used latent concepts for text representation, such as the Intelligent Es-say Assessor system (Landauer et al, 2003).
The latent concepts are generated by a statistical ap-proach called Latent Semantic Analysis (LSA), which constructs a semantic vector space and pro-jects essays to the new space.
Representing texts by vectors of words has also been a common practice in many research areas beyond AES, including information retrieval (Sal-ton et al, 1975; Croft et al, 2010).
One of its weaknesses, however, is its difficulty in addressing issues such as synonyms and related terms.
Differ-ent words, such as lawyer, attorney, counsel etc.
can share similar meaning, while in a word vector representation they are treated as different dimen-sions; however, because they are conceptually sim-ilar, it makes more sense to group them into the same vector dimension.
Ontologies are in a good position to resolve this issue because they organize words and terms under structured concepts, group terms with similar meaning together and also maintain various semantic relations between con-cepts.
Therefore, text can be represented on a con-cept level by using ontology concepts as features.
Recognizing concepts in documents can further reveal semantic relations between documents (Hotho et al, 2003a), thus can facilitate further text-related tasks such as clustering, information retrieval, as well as our speech scoring task.
This type of representation has been tried in several studies (e.g., Hotho et al, 2003a; Hotho et al, 2003b; Bloehdorn & Hotho, 2004).
Hotho et al (2003a; 2003b) use ontology con-cepts to represent text and use the representation for document clustering.
The studies employ the WordNet ontology, a general domain ontology.
The experiments test three parameters of using an ontology for text representation: (1) whether con-cept features should be used alone or replace word features or be used together with word features; (2) word sense disambiguation strategies when using concepts; and (3) investigating the optimal level of word generalization in terms of the hierarchical structure of the ontology, i.e., how general the con-cepts should be.
Some options of the first two pa-rameters will be implemented and tested in our experiment design below.
The vector representation approach of text doc-uments, either using words or concepts, can be87used to measure the content similarity between essays.
E-rater, for example, measures the similari-ty between test essays and training essays by com-puting the cosine similarity of their word vectors and by generating two content features based on this similarity metric.
It uses multiple regression as its final scoring model, using both content features, as well as features related to other aspects of the essay, such as grammar and vocabulary usage (Burstein, 2003; Attali & Burstein, 2006).
Intelli-gent Essay Assessor also employs cosine similarity between to-be-scored essays and training essays as basis of one content feature, and models the scor-ing process by normalization and regression analy-sis (Landauer et al, 2003).
The IntelliMetric system uses a nonlinear and multidimensional modeling approach to reflect the complexity of the writing process as opposed to the general linear model (Dikli, 2006).
Larkey and Croft (2003) em-ploy Bayesian classifiers for modeling, which is a type of text categorization technique.
It treats essay scoring as a text categorization task, the purpose of which is to classify essays into score categories based on content features (i.e., if the scores range from 1-4, then there are four score categories).
Zechner and Xi (2008) report on experiments related to scoring of spontaneous speech responses where content vector analysis was used as one of several features in scoring models for two different item types.
They found that while these content features performed reasonably well by themselves, they were not able to increase the overall scoring model performance over a baseline that did not use content features.
This paper will use CVA as a baseline for our experiment and investigate two ontology-based approaches to enhance the content representation and improve content feature performance.
3 Data  We use data from a test for English proficiency for non-native speakers of English.
Candidates are asked to provide spontaneous speech responses to four prompts, with each of the responses being one minute in length.
The four prompts are all integrat-ed prompts, meaning candidates are first given some materials to read or listen and then are asked to respond with their opinions or arguments to-wards the materials.
The responses are scored ho-listically by human raters on a scale of 1 to 4, 4being the highest score.
For holistic scoring, the human raters use a speech scoring rubric as the guideline of expected performance on aspects such as fluency, pronunciation, and content for each score level.
Our data set contains 1243 speech samples in to-tal as responses to four different prompts, obtained from 327 speakers (note that not all speakers re-sponded to all prompts).
Each response is verbatim transcribed by a human transcriber.
The responses are grouped by their prompts since our experi-ments are prompt-specific.
For responses of each prompt, we randomly split the responses into a training set (44%) and a test set (56%), making sure that response scores are distributed in a simi-lar proportion in both training and test sets.
Each response is considered as a single document here.
Table 1 shows the size of the two data sets.
Prompt Training Set Test Set Total A 143 176 319 (4/79/158/78) B 140  168 308 (7/86/146/69) C 139  172 311 (4/74/154/79) D 137  168 305 (8/75/141/81) Table 1.
Size of training and test data sets.
The numbers in parentheses are the number of documents on score levels 1-4.
The training set is used for generating repre-sentative vectors of a prompt on different score levels, which are to be compared with test docu-ments.
The test set is primarily used to compute content features for test documents and examine performance of approaches under different exper-iment setups.
Besides human transcriptions of the speech files, we also obtained ASR output of the files, in order to examine performance of the proposed approach-es on imperfect output, in a fully automated opera-tional scenario where no human transcribers would be in the loop.
Since the training set is used for deriving representative vectors for the four differ-ent prompts and we would like to generate accurate vectors based on human transcriptions, we do not use a separate training set for ASR data.
Thus, we only obtain corresponding ASR output for the test set of each prompt.
The ASR system we use for our experiments in this paper is a state-of-the-art gender-independent continuous density Hidden Markov Model speech recognizer, trained on about 30 hours of non-native88spontaneous speech.
Its word error rate on the test set used here is about 12.8%.
4 Method  We employ one baseline approach for word-level features and two experimental approaches for con-cept-level features to examine the effect of the WordNet ontology and concept-level features on content feature correlations.
4.1 Baseline Approach: Content Vector Analysis (CVA) We decide to use the two content features used by e-rater based on CVA analysis, called ?max.cos?
and ?cos.w4?
here (Attali & Burstein, 2006).
The assumption behind this approach is that essays with similar human scores contain similar words; thus, they should share similar vector representa-tions in CVA.
For our data, this assumption is held for the spoken test documents in the same way.
Moreover, we conjecture this assumption is mostly true for high score responses as opposed to low score responses, because we expect high vocabu-lary uniformity in high score responses and more irrelevant and more diverse vocabulary in low score responses.
Before feature computation, some preprocessing is conducted on the speech transcripts.
For each prompt, we group its training set into four groups according to their score levels (?score-level docu-ments?).
Then we use the score-level documents of each prompt to generate a super vector as a repre-sentation for documents on this score level of this specific prompt.
As a result, we have four score-level vectors under each prompt, generated from their training sets.
While the score-level training vectors are produced using multiple documents of the same score level, vectors of test documents are generated on an individual document level.
Given a test document that needs to be scored, we first convert it into the vector representation.
Then we are ready to compute the two content features.
Equation 1 provides the exact formula for the co-sine similarity measure used in all of our methods.(1)?
?where n is the number of words and/or concepts in the score-level vector (from the training set docu-ments),  ?w ?
,?
are the word or concept weights of a score-level vector and w?,?
are the word or concept weights of a test document (response transcrip-tion).
??,?
are computed by term frequency and ??
,?
are computed in the same way after concatenating documents of the same score level as one large document.
The max.cos feature.
This feature measures which score level of documents the test document is most similar to in vector space by computing the cosine similarity with each score-level vector and then selecting the score level which has the largest cosine similarity to the test vector as feature value.
Thus, this feature assumes integer values from 1 to 4 only.
The cos.w4 feature2.
This feature measures con-tent similarity between the test document and the best quality documents in vector space.
Since score 4 is the highest level in our data set of spoken re-sponses, we compute the cosine similarity between the test vector and the score level 4 vector as an indicator of how similar the test document is to the speech content of the test takers with highest profi-ciency.
The two features are evaluated based on their Pearson r correlation to human assigned scores.
We evaluate the features in all experiments, as a way to observe how the two features?
predictive-ness varies among different experiment setups.
Note that since the max.cos feature assumes inte-ger values but the cos.w4 feature is real valued, we expect correlations to be higher for cos.w4 due to this difference, all other things being equal.
4.2 Ontology-facilitated Approaches We use two ontology-facilitated document repre-sentation approaches, which represent documents based on the WordNet ontology.
The first approach matches words in a document to concepts and rep-resents documents by vectors of concepts, whereas the second one addresses the unknown word issue by inferring their weight based on the structure of the WordNet ontology.2 The feature is referred to as ?cos.w/6?
in Attali and Burstein (2006) because there are usually 6 score levels, while here our data has 4 score levels therefore it is written as ?cos.w4?.
???
== = ni islni itni islit ww ww 1 2 ,1 2,1 ,,**894.2.1 Ontology-facilitated representation ap-proach This representation uses concepts instead of the words as elements in the document vectors.
Given a document, we map words in the document to concepts, using the synsets in WordNet.
For exam-ple, chance and opportunity are different words, however they belong to the same WordNet synset (?opportunity.n.01?).
This concept-level representa-tion groups words of similar meaning in the same vector dimension, thus making the vector space more succinct and semantically meaningful.
The weighting scheme of concepts follows the one in the CVA approach.
In this study, we focus on sin-gle words and match them to WordNet synsets; in future work, we consider matching multi-word ex-pressions to ontologies like Wikipedia (Wikipedia, 2011).
Experiments show that including words and their corresponding WordNet synsets as vector dimensions has better performance than only in-cluding WordNet synsets for text clustering tasks (Hotho et al, 2003a) and the same result also oc-curs in our preliminary experiments.
Therefore, we include both WordNet synsets and words in the vector representation.
4.2.2 Ontology-facilitated reasoning approach This approach is based on the ontology-facilitated representation and goes further to resolve the un-known word issue, i.e., handling words in test doc-uments that have not been seen in the training documents.
First, test documents are converted to vectors of concepts plus words.
If a concept in the test vector does not appear in the score level vector, its weight therefore is unknown, as well.
We then estimate its weight based on structural information contained in the WordNet ontology.
More specifically, given an unknown concept in the test document, we find the N most similar concepts to that unknown con-cept from the set of all concepts contained in the score level vector.
We use a WordNet-based simi-larity estimate to measure similarity between con-cepts, namely the edge-based Path Similarity, which measures the length of a path from one con-cept to another concept in WordNet by computing the inverse of the shortest path between the two concepts (Pedersen et al, 2004).
We submit that the estimated weight of the unknown concept inthe test document vector should be close to the weights of its most similar concepts in the score level vector derived from the training documents.
From this assumption, we propose estimating the unknown concept?s weight by averaging the weights of the N most similar concepts: (2) ???
?
( ??)/?????
with N denoting the number of similar concepts in a score level vector,   ?w?
denoting the weights of these similar concepts, and w ??
standing for the resulting concept weight for the unknown concept in a test document.
For example, a test document may be ?so radio also create a great impact on this uh people com-munication?.
The words are matched to WordNet concepts, and we find that the concept synset ?im-pact.n.01?
is an unknown concept to the score level 4 vector.
From the dimensions of the score level 4 vector we find these three most similar concepts to the unknown concept: ?happening.n.01?, ?event.n.01?, and ?change.n.01?.
We now can aver-age the weights of these three concepts in the score-level vector to use it as a weight estimate for the unknown concept ?impact.n.01?.
We want to note that while this approach can es-timate weights for test document words or con-cepts contained in WordNet (but not in the training vectors), it cannot handle words that are not in-cluded in WordNet at all, such as many proper names, foreign words, etc.
To address the latter as well, we would have to use a much larger and more comprehensive ontology, e.g., the online en-cyclopedia Wikipedia.
5 Experiments and Results We design experiments according to the above ap-proaches.
The first experiment group is the base-line system using two features employed by e-rater, max.cos and cos.w4.
The second and third experiment groups implement the two ontology-facilitated approaches, respectively.
We first run CVA and compare several different parameter set-ups to optimize them for further experiments.
5.1 Parameter Optimization in CVA Experi-ments For the CVA method, we need to decide (1) which term weighting scheme to use, and (2) whether or not to use a list of stopwords to exclude common90non-content words such as determiners or preposi-tions from consideration.
We compare five com-monly used term weighting schemes, each one with or without using a stoplist, based on averaged correlations with human scores across all four prompts.
The best results are obtained for the weighting scheme (TF/EDL)*IDF, where TF is the frequency of a term in a document, EDL is the Eu-clidean document length3, and IDF is the inverse document frequency of a term based on a collec-tion of documents.
For this scheme, as for most others, there is almost no difference between using vs. not using a stoplist and we decide to use a stoplist for our experiments based on the tradition in the field.
The selected term weighting scheme is applied in the same way for both the score-level vectors as well as the test document vectors.
5.2 Experiment Groups 5.2.1 Group 1: CVA As described above, we first convert the training sets to score level vectors and the test documents into test vectors with the TF/EDL*IDF weighting, and compute the max.cos and cos.w4 features for each test document.
5.2.2 Group 2: Ontology-facilitated Representa-tion We first match words in documents to WordNet concepts.
There are several ways to achieve this (Hotho et al, 2003a).
Given a word, it may corre-spond to multiple concepts in WordNet, in which each possibility is called a ?sense?
in WordNet, and we need to decide which sense to use.
WordNet-Sense-1.
In this study we employ a simple word sense disambiguation method by us-ing the first sense returned by WordNet.
We send a word to WordNet synset search function, which returns all synstes of the word, and we select to use the first result because it is also the most frequently used sense for the word.
After obtaining the senses and concepts for the words, the training sets and test documents are                                                             3 Given a vector of raw term frequencies (rtf?, rtf?,?
, rtf?
), its Euclidean length is computed in this way:  ???
?????
?converted to vectors of WordNet concepts plus words, using TF/EDL*IDF weighting, the same one used by the CVA approach.
We compute the max.cos and cos.w4 features in the same way as for the baseline CVA method.
5.2.3 Group 3: Ontology-facilitated Reasoning This approach, called here ?WordNet-Reasoning?, also extracts vectors of WordNet concepts plus words with the same term weighting scheme as before.
For matching words to concepts, we still employ the WordNet?Sense-1 sense selection method.
For unknown concepts, which appear in a test vector but not in any score level vectors, we infer their weights by using the reasoning approach proposed in section 4.2.2 with N=5 as the number of most similar concepts to the unknown concept4, located in the WordNet hierarchy.
The score level vectors are expanded by the inferred unknown concepts.
When we obtain the expanded score lev-el vectors, we compute the two content features from the vectors in the same way as before, and finally calculate feature correlations with human scores.
5.3 Results We run the three experiment groups on human and ASR transcriptions respectively and obtain the max.cos and cos.w4 feature values of test docu-ments in the experiments.
As stated in 4.1, we compute the correlations between the two features and the human assigned scores for evaluating the approaches.
Tables 2 and 3 (next page) list correlations of the two content features with human scores under different experiment setups.
Significant differences on individual prompts between correlations of the two WordNet-based methods WordNet-Sense-1 and WordNet-Reasoning and the CVA baseline are denoted with * (p<0.05) and ** (p<0.01).4 We manually inspected some of the similar concepts of the unknown concepts and found the first 5 similar concepts were relevant to the unknown concepts, and thus made the decision of N=5.91Prompt Hum, CVA Hum, WordNet-Sense-1 Hum, Word-Net-Reasoning ASR, CVA ASR, Word-Net-Sense-1 ASR, Word-Net-Reasoning A 0.320 0.333 0.038** 0.293 0.286 0.014** B 0.348 0.352 0.350 0.308 0.338 0.339 C 0.366 0.373 0.074** 0.396 0.386 0.106** D 0.343 0.323 0.265 0.309 0.309 0.265 Average 0.344 0.345 0.182 0.327 0.330 0.181 Table 2.
Correlations between the max.cos feature and human scores (Hum=using human transcriptions; ASR=using ASR hypotheses).
Prompt Hum, CVA Hum, WordNet-Sense-1 Hum, Word-Net-Reasoning ASR, CVA ASR, Word-Net-Sense-1 ASR, Word-Net-Reasoning A 0.427 0.429 0.434 0.409 0.416 0.411 B 0.295 0.303 0.327* 0.259 0.278 0.292* C 0.352 0.385* 0.402** 0.338 0.366 0.380** D 0.368 0.385 0.389 0.360 0.379 0.374 Average 0.360 0.376 0.388 0.342 0.360 0.364 Table 3.
Correlations between the cos.w4 feature and human scores (Hum=using human transcriptions; ASR=using ASR hypotheses) 6 Discussion 6.1 Results on Human Transcriptions On human transcriptions, Table 2 shows that the max.cos feature correlations increase, albeit not significantly, when using the method WordNet?Sense-1 on all prompts except for prompt D but decrease sometimes significantly when using the WordNet-Reasoning approach.
The cos.w4 feature correlations, on the other hand, exhibit constant increases on all four prompts when using WordNet-Sense-1 and the increase on prompt C is significant.
The average correlations further increase for all prompts when using WordNet-Reasoning and the increase is sig-nificant on prompts B and C (Table 3).
6.2 Results on ASR Output On the ASR output, for the max.cos feature, the average correlation barely changes when using the WordNet-Sense-1 method but decreases when us-ing WordNet-Reasoning with significant decrease on prompts A and C (Table 2).
For the cos.w4 feature, however, WordNet-Sense-1 improves correlations on all four prompts with 0.018 correlation increase on average but in-creases are not statistically significant on a prompt level.
WordNet-Reasoning does not further im-prove correlations much beyond the correlations of WordNet-Sense-1, with a further 0.004 increase inaverage correlation.
Compared to CVA, though, correlations for WordNet-Reasoning are signifi-cantly higher on prompts B and C (Table 3).
6.3 Overall Discussion Based on these observations, we find that for cos.w4, the WordNet-Sense-1 approach can im-prove average correlations compared to the CVA baseline on both ASR and human transcriptions.
Hence, the extension of the document vectors by WordNet synsets has a positive impact on the ac-curacy of content scoring of the spoken responses by non-native speakers.
Again looking at the cos.w4 feature, while the WordNet Reasoning approach works well on hu-man transcriptions to further improve correlations compared to WordNet-Sense-1, it does not consist-ently improve correlations on ASR output.
This may indicate that WordNet-Reasoning is more sen-sitive to ASR errors than WordNet-Sense-1.
For the max.cos feature, the correlation of WordNet-Reasoning decreases significantly from WordNet-Sense-1 on prompts A and C for both human and ASR transcriptions; moreover, in the WordNet-Reasoning approach the max.cos correla-tions vary greatly on the four prompts (Table 2).
We conjecture that one reason for this finding may lie in the rather small sample size of the data set, as this is an exploratory study, and the differences across prompts may be smaller when using a sub-stantially larger data set.92Comparing the average reduction in correlation between human and ASR transcriptions, we find an absolute drop in correlations of 0.017 between the CVA baseline for the max.cos and of 0.019 for the cos.w4 feature.
Looking at the WordNet-Sense-1 approach for the cos.w4 feature, the average corre-lation of 0.376 for human transcriptions is reduced by 0.016 to 0.360 for ASR hypotheses.
Hence, we observe that the imperfect speech recognition out-put does not cause a major degradation for this content feature; the degradations observed are all in the range of 5% relative (the ASR word error rate on the test set is about 13%.)
Overall, the ontology-facilitated approaches are effective for the cos.w4 feature and seem to be less appropriate for the max.cos feature.
We conjecture that the characteristics of the max.cos feature may be the reason for the poor performance of the on-tology-facilitated approaches on this feature.
To compute this feature, we need to compare a test vector with vectors for each score level, and it is assumed that these vectors are representative vec-tors for documents at these score levels.
In reality though, while the score level 4 vector is quite a good representative for the prompt topic (highest proficiency speakers), score level vectors of less proficient speakers are less uniform and more di-verse.
The reason is that there are only a few ways to appropriately represent the correct topic in a good quality spoken response but there can be many different ways of generating responses that are not on topic.
For example, the score level 1 vector contains vectors generated from score 1 documents, whose words are considered mostly irrelevant for the prompt.
Then, given a test docu-ment, which also contains irrelevant words for the prompt but with little overlap to the level 1 score vector, the similarity between them would be very small.
Thus, any ontological approach has to face this heterogeneous distribution of words in the score level vectors for responses with lower scores; any semantic generalizations are inherently more difficult compared to those on higher scoring re-sponses.
For the cos.w4 feature, in contrast, only score level 4 vectors are used, and this problem does not surface here.
Finally, we observe that average correlations of both features based on ASR hypotheses (except for WordNet-Reasoning for the max.cos feature) fall in the range of 0.32-0.37.
This range is well in line with our better performing features in other dimen-sions of spontaneous speech responses, e.g., fluen-cy, pronunciation, and prosody.
7 Conclusion and Future Work In this paper, we propose using ontology-facilitated approaches for content scoring of non-native spontaneous speech due to specific merits of ontologies.
Two ontology-facilitated approaches are proposed and evaluated, and their results are compared against a CVA baseline.
The results in-dicate that the ontology approaches can improve content feature correlations in some circumstances.
As a summary, concept-level features and reason-ing-based approaches work well on the cos.w4 content feature where test documents are compared against a vector representing all training set docu-ments with the highest human score.
For future work, we plan to investigate more so-phisticated reasoning approaches.
For this study, we use a simple averaging method to infer the con-cept importance based on hierarchy-inferred simi-larity metrics.
As a next step, we plan to infer weights according to different similarity metrics and differential weighting of the N closest terms.
Another avenue for future research is to employ different ontologies, for example, Wikipedia, which contains more concepts and entities than WordNet and has a structure that has grown more organically and less from first principles.
Wikipe-dia also has a larger pool of multi-word expres-sions and we would like to explore how representations based on the Wikipedia ontology affects automated speech scoring performance.
References  Alwan, A., Bai, Y., Black, M., Casey, L., Gerosa, M., Heritage, M., & Wang, S. (2007).
A system for tech-nology based assessment of language and literacy in young children: The role of multiple information sources.
Proceedings of the IEEE International Workshop on Multimedia signal Processing, Greece.
Attali, Y., & Burstein, J.
(2006).
Automated essay scor-ing with e-rater?
V. 2.
The Journal of Technology, Learning and Assessment, 4(3).
Balogh, J., Bernstein, J., Cheng, J., & Townshend, B.
(2007).
Automatic evaluation of reading accuracy: Assessing machine scores.
Proceedings of the ISCA-SLaTE-2007 Workshop, Farmington, PA, October.
Bloehdorn, S., & Hotho, A.
(2004).
Boosting for text classification with semantic features.
Workshop on mining for and from the semantic web at the 10th93ACM SIGKDD conference on knowledge discovery and data mining (KDD 2004).
Burstein, J.
(2003).
The E-rater?
scoring engine: Au-tomated essay scoring with natural language pro-cessing.
In M. D. Shermis, Burstein, J.C.
(Ed.
), Automated essay scoring: A cross-disciplinary per-spective (pp.
113-121).
Mahwah, NJ: Lawrence Erl-baum Associates, Inc. Croft, W. B., Metzler, D., & Strohman, T. (2010).
Search engines: Information retrieval in practice.
Boston, MA: Addison-Wesley.
Cucchiarini, C., Strik, H., & Boves, L. (2000).
Quantita-tive assessment of second language learners?
fluency by means of automatic speech recognition technolo-gy.
Journal of the Acoustical Society of America, 107(2), 989-999.
Cucchiarini, C., Strik, H., & Boves, L. (2002).
Quantita-tive assessment of second language learners' fluen-cy: Comparisons between read and spontaneous speech.
Journal of the Acoustical Society of Ameri-ca, 111(6), 2862-2873.
Dikli, S. (2006).
An overview of automated scoring of essays.
The Journal of Technology, Learning and As-sessment, 5(1), 1-35.
Fellbaum, C.
(Ed.).
(1998).
WordNet: An electronic lexical database.
Cambridge, MA: The MIT press.
Franco, H., Bratt, H., Rossier, R., Gadde, V. R., Shriberg, E., Abrash, V., & Precoda, K. (2010).
EduSpeak: A speech recognition and pronunciation scoring toolkit for computer-aided language  learn-ing applications.
Language Testing, 27(3), 401-418.
Hotho, A., Staab, S., & Stumme, G. (2003a).
Ontologies improve text document clustering.
Proceedings of the Third IEEE International Conference on Data Min-ing (ICDM?03).
Hotho, A., Staab, S., & Stumme, G. (2003b).
Text clus-tering based on background knowledge (Technical report, no.425.
): Institute of Applied Informatics and Formal Description Methods AIFB, University of Karlsruche.
Landauer, T. K., Laham, D., & Foltz, P. W. (2003).
Au-tomated scoring and annotation of essays with the In-telligent Essay Assessor.
In M. D. Shermis, Burstein, J.C.
(Ed.
), Automated essay scoring: A cross-disciplinary perspective (pp.
87?112).
Mahwah, NJ: Lawrence Erlbaum Associates, Inc. Larkey, L. S., & Croft, W. B.
(2003).
A Text Categori-zation Approach to Automated Essay Grading.
In M. D. Shermis & J. C. Burstein (Eds.
), Automated Essay Scoring: A Cross-discipline Perspective: Mahwah, NJ, Lawrence Erlbaum.
Pedersen, T., Patwardhan, S., & Michelizzi, J.
(2004).
WordNet:: Similarity: measuring the relatedness of concepts.
Proceedings of the Fifth Annual Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL-04).Salton, G., Wong, A., & Yang, C. S. (1975).
A vector space model for automatic indexing.
Communica-tions of the ACM, 18(11), 613-620.
Wikipedia: The free encyclopedia (2011).
FL: Wiki-media Foundation, Inc. Retrieved Apr 26, 2012, from http://www.wikipedia.org WordNet 3.0 Reference Manual.
(2010).
Retrieved Apr 26, 2012 from http://wordnet.princeton.edu/wordnet/documentation/ Zechner, K., Higgins, D., Xi, X, & D. M. Williamson (2009).
Automatic scoring of non-native spontaneous speech in tests of spoken English.
Speech Communi-cation, 51(10), 883-895.
Zechner, K., & X. Xi (2008).
Towards Automatic Scor-ing of a Test of Spoken Language with Heterogene-ous Task Types.
Proceedings of the ACL Workshop on Innovative Use of NLP for Building Educational Applications, Columbus, OH, June.94
