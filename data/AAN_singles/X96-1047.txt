DESIGN OF THE MUC-6 EVALUATIONRalph GrishmanDept.
of Computer  ScienceNew York Univers i ty715 Broadway, 7th FloorNew York, NY 10003, USAgr i shman?cs ,  nyu.
eduBeth SundheimNaval Command,  Control  and Ocean Survei l lance CenterResearch,  Development,  Test and Evaluat ion Division (NRaD)Code 4420853140 Gatchell  RoadSan Diego, Cal i fornia 92152-7420sundheimOpoj ke.
nosc .
milAbstractThe sixth in a series of "Message Understanding Con-ferences", which are designed to promote and evalu-ate research in information extraction, was held lastfall.
MUC-6 introduced several innovations over priorMUCs, most notably in the range of different asksfor which evaluations were conducted.
We describethe development of the "message understanding" taskover the course of the prior MUCs, some of the mo-tivations for the new format, and the steps which ledup to the formal evaluation.1THE MUC EVALUATIONSLast fall we completed the sixth in a series of Mes-sage Understanding Conferences, which have been or-ganized by NRAD, the RDT&E division of the NavalCommand, Control and Ocean Surveillance Center(formerly NOSC, the Naval Ocean Systems Center)with the support of DARPA, the Defense AdvancedResearch Projects Agency.
This paper looks brieflyat the history of these Conferences and then exam-ines the considerations which led to the structure ofMUC-6.
21 Port ions of this article are taken from the paper "MessageUnderstanding Conference-6: A Brief History", in COLING-96, Proc.
of the Int'l Conf.
on Computational Linguistics.2 The full proceedings of the conference are to be distr ibutedby Morgan Kaufmann Publ ishers, San Mateo, California; ear-lier MUC proceedings, for MUC-3, 4, and 5, are also availableThe Message Understanding Conferences were ini-tiated by NOSC to assess and to foster esearch on theautomated analysis of military messages containingtextual information.
Although called "conferences",the distinguishing characteristic of the MUCs are notthe conferences themselves, but the evaluations towhich participants must submit in order to be per-mitted to attend the conference.
For each MUC, par-ticipating groups have been given sample messagesand instructions on the type of information to be ex-tracted, and have developed a system to process uchmessages.
Then, shortly before the conference, par-ticipants are given a set of test messages to be runthrough their system (without making any changes tothe system); the output of each participant's systemis then evaluated against a manually-prepared answerkey.The MUCs have helped to define a program of re-search and development.
DARPA has a number ofinformation science and technology programs whichare driven in large part by regular evaluations.
TheMUCs are notable, however, in that they have sub-stantially shaped the research program in informationextraction and brought it to its current state.
3from Morgan Kaufmann.3There were, however, a number  of individual research ef-forts in information extract ion underway before the first MUC,including the work on information formatt ing of medical nar-rative by Sager at New York University \[3\]; the formatt ing ofnaval equipment failure reports at the Naval Research Labora-tory \[1\]; and the DBG work by Montgomery  for RADC (now413PRIOR MUCSMUC-1 (1987) was basically exploratory; each groupdesigned its own format for recording the informationin the document, and there was no formal evaluation.By MUC-2 (1989), the task had crystalized as oneof template filling.
One receives a description of aclass of events to be identified in the text; for each ofthese events one must fill a template with informationabout the event.
The template has slots for informa-tion about the event, such as the type of event, theagent, the time and place, the effect, etc.
For MUC-2,the template had 10 slots.
Both MUC-1 and MUC-2involved sanitized forms of military messages aboutnaval sightings and engagements.The second MUC also worked out the details of theprimary evaluation measures, recall and precision.
Topresent it in simplest erms, suppose the answer keyhas Nkey filled slots; and that a system fills Ncorreetslots correctly and Nincorrect incorrectly (with someother slots possibly left unfilled).
Then~eorrectreca l l  -NkeyYcorreetprec is ion  =gcorreet @ gincorrectFor MUC-3 (1991), the task shifted to reports of ter-rorist events in Central and South America, as re-ported in articles provided by the Foreign BroadcastInformation Service, and the template became some-what more complex (18 slots).
A sample MUC-3 mes-sage and template is shown in Figure 1.
This sametask was used for MUC-4 (1992), with a further smallincrease in template complexity (24 slots).
For MUC-1 through 4, all the text was in upper case.MUC-5 (1993), which was conducted as part of theTipster program, represented a substantial furtherjump in task complexity.
Two tasks were involved,international joint ventures and electronic ircuit fab-rication, in two languages, English and Japanese.
Inplace of a single template, the joint venture task em-ployed 11 object types with a total of 47 slots forthe output - -  double the number of slots defined forMUC-4 - -  and the task documentation also doubledin size to over 40 pages in length.
A sample articleand corresponding template for the MUC-5 Englishjoint venture task are shown in Figures 2 and 3.
Thetext shown is all upper case, but (for the first time)the test materials contained mixed-case text as well.One innovation of MUC-5 was the use of a nestedstructure of objects.
In earlier MUCs, each eventhad been represented as a single template - in effect,Rome Labs) \[2\].a single record in a data base, with a large numberof attributes.
This format proved awkward when anevent had several participants (e.g., several victims ofa terrorist attack) and one wanted to record a set offacts about each participant.
This sort of informationcould be much more easily recorded in the hierarchicalstructure introduced for MUC-5, in which there wasa single object for an event, which pointed to a listof objects, one for each participant in the event.The sample template in Figure 3 illustrates everalof the other features which added to the complexityof the MUC-5 task.
The TIE_UP_RELATIONSHIPobject points to the ACTIVITY object, which in turnpoints to the INDUSTRY object, which describeswhat the joint venture actually did.
Within the IN-DUSTRY object, the PRODUCT/SERVICE slot hasto list not just the specific product or service ofthe joint venture, but also a two-digit code for thisproduct or service, based on the top-level classifica-tion of the Standard Industrial Classification.
TheTIE_UP_RELATIONSHIP also pointed to an OWN-ERSHIP object, which specified the total capitaliza-tion using standard codes for different currencies, andthe percentage ownership of the various participantsin the joint venture (which may involve some calcu-lation, as in the example shown here).
While eachindividual feature of the template structure adds tothe value of the extracted information, the net effectwas a substantial investment by each participant inimplementing the many details of the task.MUC-6: INITIAL GOALSDARPA convened a meeting of Tipster participantsand government representatives in December 1993 todefine goals and tasks for MUC-6.
4 Among the goalswhich were identified were?
demonstrating domain-independent componenttechnologies of information extraction whichwould be immediately useful?
encouraging work to make information extractionsystems more portable* encouraging work on "deeper understanding"Each of these can been seen in part as a reaction tothe trends in the prior MUCs.
The MUC-5 tasks, in4The representatives of the research community were JimCowie, Ralph Grishman (committee chair), Jerry Hobbs, PaulJacobs, Len Schubert, Carl Weir, and Ralph Weischedel.
Thegovernment people attending were George Doddington, DonnaHarman, Boyan Onyshkevyeh, John Prange, Bill Schultheis,and Beth Sundheim.414TST1-MUC3-0080BOGOTA, 3 APR 90 (INRAVISION TELEVISION CADENA 1) - \[REPORT\] \[JORGE ALONSO SIERRAVALENCIA\] \[TEXT\] LIBERAL SENATOR FEDERICO ESTRADA VELEZ WAS KIDNAPPED ON 3APRIL AT THE CORNER OF 60TH AND 48TH STREETS IN WESTERN M.EDELLIN, ONLY 100 ME-TERS FROM A METROPOLITAN POLICE CAI \[IMMEDIATE ATTENTION CENTER\].
THE ANTIO-QUIA DEPARTMENT LIBERAL PARTY LEADER HAD LEFT HIS HOUSE WITHOUT ANY BODY-GUARDS ONLY MINUTES EARLIER.
AS HE WAITED FOR THE TRAFFIC LIGHT TO CHANGE,THREE HEAVILY ARMED MEN FORCED HIM TO GET OUT OF HIS CAR AND INTO A BLUERENAULT.HOURS LATER, THROUGH ANONYMOUS TELEPHONE CALLS TO THE METROPOLITAN POLICEAND TO THE MEDIA, THE EXTRADITABLES CLAIMED RESPONSIBILITY FOR THE KIDNAP-PING.
IN THE CALLS, THEY ANNOUNCED THAT THEY WILL RELEASE THE SENATOR WITH ANEW MESSAGE FOR THE NATIONAL GOVERNMENT.LAST WEEK, FEDERICO ESTRADA VELEZ HAD REJECTED TALKS BETWEEN THE GOVERN-MENT AND THE DRUG TRAFFICKERS.O.
MESSAGE IDi.
TEMPLATE ID2.
DATE OF INCIDENT3.
TYPE OF INCIDENT4.
CATEGORY OF INCIDENT5.
PERPETRATOR: ID OF INDIV(S)6.
PERPETRATOR: ID OF ORG(S)7.
PERPETRATOR: CONFIDENCE8.
PHYSICAL TARGET: ID(S)9.
PHYSICAL TARGET: TOTAL MUMi0.
PHYSICAL TARGET: TYPE(S)ii.
HUMAN TARGET: ID(S)12.
HUMAN TARGET: TOTAL MUM13.
HUMAN TARGET: TYPE(S)14.
TARGET: FOREIGN NATION(S)15.
INSTRUMENT: TYPE(S)16.
LOCATION OF INCIDENT17.
EFFECT ON PHYSICAL TARGET(S)18.
EFFECT ON HUMAN TARGET(S)TSTI-MUC3-O080103 APR 90KIDNAPPINGTERRORIST ACT"THREE HEAVILY ARMED MEN""THE EXTRADITABLES"CLAIMED OR ADMITTED: "THE EXTRADITABLES"$"FEDERICO ESTRADA VELEZ" ("LIBERAL SENATOR")1GOVERNMENT OFFICIAL: "FEDERICO ESTRADA VELEZ"$COLOMBIA: MEDELLIN (CITY)Figure h A sample message and associated filled template from MUC-3 (terrorist domain).
Slots which arenot applicable to this type of incident (a kidnapping) are marked with an "*".
For several of these slots,there are alternative "correct" answers; only one of these answers is shown here.415<DOCNO> 0592 </DOCNO><DD> NOVEMBER 24, 1989, FRIDAY </DD><SO> Copyright (c) 1989 Jiji Press Ltd.; </SO><TXT>BRIDGESTONE SPORTS CO. SAID FRIDAY IT HAS SET UP A JOINT VENTURE IN TAIWAN WITHA LOCAL CONCERN AND A JAPANESE TRADING HOUSE TO PRODUCE GOLF CLUBS TO BESHIPPED TO JAPAN.THE JOINT VENTURE, BRIDGESTONE SPORTS TAIWAN CO., CAPITALIZED AT 20 MILLIONNEW TAIWAN DOLLARS, WILL START PRODUCTION IN JANUARY 1990 WITH PRODUCTIONOF 20,000 IRON AND "METAL WOOD" CLUBS A MONTH.
THE MONTHLY OUTPUT WILL BELATER RAISED TO 50,000 UNITS, BRIDGESTON SPORTS OFFICIALS SAID.THE NEW COMPANY, BASED IN KAOHSIUNG, SOUTHERN TAIWAN, IS OWNED 75 PCT BYBRIDGESTONE SPORTS, 15 PCT BY UNION PRECISION CASTING CO. OF TAIWAN AND THEREMAINDER BY TAGA CO., A COMPANY ACTIVE IN TRADING WITH TAIWAN, THE OFFICIALSSAID.BRIDGESTONE SPORTS HAS SO FAR BEEN ENTRUSTING PRODUCTION OF GOLF CLUB PARTSWITH UNION PRECISION CASTING AND OTHER TAIWAN COMPANIES.WITH THE ESTABLISHMENT OF THE TAIWAN UNIT, THE JAPANESE SPORTS GOODS MAKERPLANS TO INCREASE PRODUCTION OF LUXURY CLUBS IN JAPAN.</TXT></DOC>Figure 2: A sample article from the MUC-5 English joint ventures task.particular, had been quite complex and a great efforthad been invested by the government in preparingthe training and test data and by the participants inadapting their systems for these tasks.
Most partic-ipants worked on the tasks for 6 months; a few (theTipster contractors) had been at work on the tasks forconsiderably longer.
While the performance of somesystems was quite impressive (the best got 57% re-call, 64% precision overall, with 73% recall and 74%precision on the 4 "core" object types), the questionnaturally arose as to whether there were many ap-plications for which an investment of one or severaldevelopers over half-a-year (or more) could be justi-fied.Furthermore, while so much effort had been ex-pended, a large portion was specific to the particulartasks.
It wasn't clear whether much progress was be-ing made on the underlying technologies which wouldbe needed for better understanding.SHORT-TERM SUBTASKSThe first goal was to identify, from the componenttechnologies being developed for information extrac-tion, functions which would be of practical use, wouldbe largely domain independent, and could in the nearterm be performed automatically with high accu-racy.
To meet this goal the committee developed the"named entity" task, which basically involves identi-fying the names of all the people, organizations, andgeographic locations in a text.The final task specification, which also involvedtime, currency, and percentage xpressions, usedSGML markup to identify the names in a text.
Fig-ure 4 shows a sample sentence with named entityannotations.
The tag ENAMEX ("entity name expres-sion") is used for both people and organization names;the tag NUMEX ("numeric expression") is used for cur-rency and percentages.PORTABIL ITYTo address these goals, the meeting formulated anambitious menu of tasks for MUC-6, with the ideathat individual participants could choose a subset ofthese tasks.
We consider the three goals in the threesections below, and describe the tasks which were de-veloped to address each goal.The second goal was to focus on portability in theinformation extraction task - -  the ability to rapidlyretarget a system to extract information about a dif-ferent class of events.
The committee felt that it wasimportant to demonstrate that useful extraction sys-tems could be created in a few weeks.
To meet thisgoal, we decided that the information extraction task416<TEMPLATE-0592-1> :=DOC NR: 0592DOC DATE: 241189DOCUMENT SOURCE: "Jiji Press Ltd."CONTENT: <TIE_UP_RELATIONSHIP-0592-1><TIE_UP_RELATIONSHIP-O592-1> :=TIE-UP STATUS: EXISTINGENTITY: <ENTITY-0592-1><ENTITY-0592-2><ENTITY-O592-3>JOINT VENTURE CO: <ENTITY-O592-4>OWNERSHIP: <OWNERSMIP-O592-1>ACTIVITY: <ACTIVITY-0592-1><ENTITY-O592-1> :=NAME: BRIDGESTONE SPORTS COALIASES: "BRIDGESTONE SPORTS""BRIDGESTON SPORTS"NATIONALITY: Japan (COUNTRY)TYPE: COMPANYENTITY RELATIONSHIP: <ENTITY_RELATIONSHIP-0592-1><ENTITY-0592-2> :=NAME: UNION PRECISION CASTING COALIASES: "UNION PRECISION CASTING"LOCATION: Taiwan (COUNTRY)NATIONALITY: Taiwan (COUNTRY)TYPE: COMPANYENTITY RELATIONSHIP: <ENTITY_RELATIONSHIP-0592-1><ENTITY-O592-3> :=NAME: TAGA CONATIONALITY: Japan (COUNTRY)TYPE: COMPANYENTITY RELATIONSHIP: <ENTITY_RELATIONSHIP-0592-1><ENTITY-0592-4> :=NAME: BRIDGESTONE SPORTS TAIWAN COLOCATION: "KAOHSIUNG" (UNKNOWN) Taiwan (COUNTRY)TYPE: COMPANYENTITY RELATIONSHIP: <ENTITY_RELATIONSHIP-0592-1><INDUSTRY-O592-1> :=INDUSTRY-TYPE: PRODUCTIONPRODUCT/SERVICE: (39 "20,000 IRON AND "METAL WOOD" [CLUBS]")<ENTITY_RELATIONSHIP-0592-1> :=ENTITY1: <ENTITY-0592-1><ENTITY-O592-2><ENTITY-0592-3>ENTITY2: <ENTITY-0592-4>REL OF ENTITY2 TO ENTITY1: CHILDSTATUS: CURRENT<ACTIVITY-0592-1> :=INDUSTRY: <INDUSTRY-OS92-1>ACTIVITY-SITE: (Taiwan (COUNTRY) <ENTITY-0592-4>)START TIME: <TIME-0592-1><TIME-0592-1> :=DURING: 0190<OWNERSHIP-0592-1> :=OWNED: <ENTITY-O592-4>TOTAL-CAPITALIZATION: 20000000 TWDOWNERSHIP-E: (<ENTITY-0592-3> 10)(<ENTITY-0592-2> 15)(<ENTITY-0592-1> 75)Figure 3: A sample filled template from the MUC-5 English joint ventures task.417Mr.
<ENAMEX TYPE="PERSON">Dooner</ENAMEX> met with <ENAMEX TYPE="PERSON">MartinPuris</ENAMEX>, president and chief executive officer of <ENAMEXTYPE="ORGANIZATION">Ammirati ~ Puris</ENAMEX>, about <ENAMEXTYPE="ORGANIZATION">McCann</ENAMEX>'s acquiring the agency with billings of <NUMEXTYPE="MONEY">$400 million</NUMEX>, but nothing has materialized.Figure 4: Sample named entity annotation.for MUC-6 would have to involve a relatively simpletemplate, more like MUC-2 than MUC-5; this wasdubbed "mini-MUC".
In keeping with the hierarchi-cal object structure introduced in MUC-5, it was envi-sioned that the mini-MUC would have an event-levelobject pointing to objects representing the partici-pants in the event (people, organizations, products,etc.
), mediated perhaps by a "relational" level object.To further increase portability, a proposal wasmade to standardize the lowest-level objects (for peo-ple, organizations, etc.
), since these basic classes areinvolved in a wide variety of actions.
In this way,MUC participants could develop code for these low-level objects once, and then use them with many dif-ferent types of events.
These low-level objects werenamed "template lements".As the specification finally developed, the templateelement for organizations had six slots, for the max-imal organization ame, any aliases, the type, a de-scriptive noun phrase, the locale (most specific loca-tion), and country.
Slots are filled only if informationis explicitly given in the text (or, in the case of thecountry, can be inferred from an explicit locale).
ThetextWe are striving to have a strong renewedcreative partnership with Coca-Cola," Mr.Dooner says.
However, odds of that hap-pening are slim since word from Coke head-quarters in Atlanta is that...would yield an organization template lement withfive of the six slots filled:<ORGANIZATION-9402240133-5> :=ORG_NAME: "Coca-Cola"ORG_ALIAS: "Coke"ORG_TYPE: COMPANYORG_LOCALE: Atlanta CITYORG_COUNTRY: United States(the first line identifies this as organization object 5from article 9402240133).Ever on the lookout for additional evaluation mea-sures, the committee decided to make the creation oftemplate lements for all the people and organizationsin a text a separate MUC task.
Like the named entitytask, this was also seen as a potential demonstrationof the ability of systems to perform a useful, relativelydomain independent task with near-term extractiontechnology (although it was recognized as being moredifficult than named entity, since it required merg-ing information from several places in the text).
Theold-style MUC information extraction task, based ona description of a particular class of events (a "sce-nario") was called the "scenario template" task.
Asample scenario template is shown in the appendix.MEASURES OFDERSTANDINGDEEP UN-Another concern which was noted about the MUCsis that the systems were tending towards relativelyshallow understanding techniques (based primarily onlocal pattern matching), and that not enough workwas being done to build up the mechanisms neededfor deeper understanding.
Therefore, the committee,with strong encouragement from DARPA, includedthree MUC tasks which were intended to measureaspects of the internal processing of an informationextraction or language understanding system.
Thesethree tasks, which were collectively called SemEval("Semantic Evaluation") were:?
Coreference:  the system would have to markcoreferential noun phrases (the initial specifica-tion envisioned marking set-subset, part-whole,and other relations, in addition to identity rela-tions)?
Word  sense d isambiguat ion:  for each openclass word (noun, verb, adjective, adverb) inthe text, the system would have to determineits sense using the Wordnet classification (its"synset", in Wordnet erminology)?
Pred lcate -argument  s t ructure :  the systemwould have to create a tree interrelating the con-stituents of the sentence, using some set of gram-matical functional relationsThe committee recognized that, in selecting such in-ternal measures, it was making some presumptions418regarding the structures and decisions which an ana-lyzer should make in understanding a document.
Noteveryone would share these presumptions, but par-ticipants in the next MUC would be free to enter theinformation extraction evaluation and skip some or allof these internal evaluations.
Language understand-ing technology might develop in ways very differentfrom those imagined by the committee, and these in-ternal evaluations might turn out to be irrelevant dis-tractions.
However, from the current perspective ofmost of the committee, these seemed fairly basic as-pects of understanding, and so an experiment in eval-uating them (and encouraging improvement in them)would be worthwhile.PREPARATION PROCESSRound 1: Reso lu t ion  of  SemEva lThe committee had proposed a very ambitious pro-gram of evaluations.
We now had to reduce these pro-posals to detailed specifications.
The first step was todo some manual text annotation for the four tasks - -named entity and the SemEval triad - -  which werequite different from what had been tried before.
Briefspecifications were prepared for each task, and in thespring of 1994 a group of volunteers (mostly veteransof earlier MUCs) annotated a short newspaper articleusing each set of specifications.Problems arose with each of the SemEval tasks.?
For coreference, there were problems identifyingpart-whole and set-subset relations, and distin-guishing the two (a proposal to tag more generalcoreference relations had been dropped earlier);a decision was later made to limit ourselves toidentity relations.?
For sense tagging, the annotators found that insome cases Wordnet made very fine distinctionsand that making these distinctions consistentlyin tagging was very difficult.?
For predicate-argument structure, practically ev-ery new construct beyond simple clauses andnoun phrases raised new issues which had to becollectively resolved.Beyond these individual problems, it was felt thatthe menu was simply too ambitious, and that wewould do better by concentrating on one element ofthe SemEval triad for MUC-6; at a meeting held inJune 1994, a decision was made to go with corefer-ence.
In part, this reflected a feeling that the prob-lems with the coreference specification were the mostamenable to solution.
It also reflected a convictionthat coreference identification had been, and wouldremain, critical to success in information extraction,and so it was important o encourage advances incoreference.
In contrast, most extraction systemsdid not build full predicate-argument structures, andword-sense disambiguation played a relatively smallrole in extraction (particularly since extraction sys-tems operated in a narrow domain).The coreference task, like the named entity task,was annotated using SGML notation.
A COREF taghas an ID attribute which identifies the tagged nounphrase or pronoun.
It may also have an attributeof the form REF=n, which indicates that this phraseis coreferential with the phrase with ID n. Fig-ure 5 shows an excerpt from an article, annotatedfor coreference.
5Round 2: annotat ionThe next step was the preparation of a substantialtraining corpus for the two novel tasks which re-mained (named entity and coreferenee).
For anno-tation purposes, we wanted to use texts which couldbe redistributed to other sites with minimal encum-brances.
We therefore selected Wall Street Journaltexts from 1987, 1988, and 1989 which had alreadybeen distributed as part of the "ACL/DCI" CD-ROMand which were available at nominal cost from theLinguistic Data Consortium.SRA Corporation kindly provided tools whichaided in the annotation process.
Again a stalwartgroup of volunteer annotators was assembled; 6 eachwas provided with 25 articles from the Wall StreetJournal.
There was some overlap between the articlesassigned, so that we could measure the consistency ofannotation between sites.
This annotation was donein the winter of 1994-95.A major role of the annotation process was to iden-tify and resolve problems with the task specifications.For named entities, this was relatively straightfor-ward.
For coreference, it proved remarkably difficultto formulate guidelines which were reasonably preciseand consistent.
75The TYPE and MIN attributes which appear in the actualannotation have been omitted here for the sake of readability.6The annotation groups were from BBN, Brandeis Univ.,the Univ.
of Durham, Lockheed-Martin, New Mexico StateUniv., NRaD, New York Univ., PRC, the Univ.
of Pennsyl-vania, SAIC (San Diego), SRA, SRI, the Univ.
of Sheffield,Southern Methodist Univ., and Unisys.7As experienced computational linguists, we probablyshould have known better than to think this was an easy task.419Maybe <COREF ID="136" REF="I34">he</COREF>'Ii  even leave  something from <COREF ID="138"REF="i39"><COREF ID="137 ' REF="i36">his</COREF> office</COREF> for <COREF ID="i40"REF="91">Mr.
Dooner</COREF>.
Perhaps <COREF ID="144">a framed page from the New YorkTimes, dated Dec. 8, 1987, showing a year-end chart of the stock market crash earlierthat year</COREF>.
<COREF ID="i41" REF="i37">Mr.
James</COREF> says <COREF ID="142 'REF="i41">he</COREF> framed <CDREF ID="143" REF="i44 ' STATUS="DPT">it</COREF> and kept<COREF ID="145" REF="i44">it</COREF> by <COREF ID="146" REF="i42">his</COREF> desk as a"personal reminder.
It can all be gone like that.
"Figure 5: Sample coreference annotation.Round 3: d ry  runOnce the task specifications seemed reasonably sta-ble, NRaD organized a "dry run" - a full-scale re-hearsal for MUC-6, but with all results reportedanonymously.
The dry run took place in April 1995,with a scenario involving labor union contract nego-tiations, and texts which were again drawn from the1987-89 Wall Street Journal.
Of the sites which wereinvolved in the annotation process, ten participated inthe dry run.
Results of the dry run were reported atthe Tipster Phase II 12-month meeting in May 1995.An algorithm developed by the MITRE Corpora-tion for MUC-6 was implemented by SAIC and usedfor scoring the coreference task \[4\].
The algorithmcompares the equivalence classes defined by the coref-erence links in the manually-generated answer key(the "key") and in the system-generated output (the"response").
The equivalence classes are the modelsof the identity equivalence coreference relation.
Us-ing a simple counting scheme, the algorithm obtainsrecall and precision scores by determining the min-imal perturbations required to align the equivalenceclasses in the key and response.THE FORMAL EVALUATIONA call for participation in the MUC-6 formal evalu-ation was issued in June 1995; the formal evaluationwas held in September 1995.
The scenario definitionwas distributed at the beginning of September; thetest data was distributed four weeks later, with re-sults due by the end of the week.
The scenario in-volved changes in corporate executive managementpersonnel.The texts used for the formal evaluation weredrawn from the 1993 and 1994 Wall Street Jour-nal, and were provided through the Linguistic DataConsortium.
This data had been much less exposedthan the earlier Wall Street Journal data, and sowas deemed suitable for the evaluation (participantswere required to promise not to look at Wall StreetJournal data from this period during the evaluation).There had originally been consideration given to us-ing a more varied test corpus, drawn from severalnews sources.
It was decided, however, that multi-pie sources, with different formats and text mark-up,would be yet another complication for the participantsat a time when they were already dealing with multi-ple tasks.There were evaluations for four tasks: named en-tity, coreference, template lement, and scenario tem-plate.
There were 16 participants; 15 participated inthe named entity task, 7 in coreference, 11 in templateelement, and 9 in scenario template.
The participants,and the tasks they participated in, are listed in Fig-ure 6.The results of the MUC-6 evaluations are de-scribed in detail in a companion paper in this vol-ume, "Overview of Results of the MUC-6 Evalua-tion".
Overall, the evaluation met many, though notall, of the goals which had been set by the initial plan-ning conference in December of 1993.The named ent i ty  task exceeded our expectationin producing systems which could perform a relativelysimple task at levels good enough for immediate use.The nearly half the sites had recall and precision over90%; the highest-scoring system had a recall of 96%and a precision of 97%.The template  e lement  task was harder and thescores correspondingly lower than for named entity(ranging across most systems from 65 to 75% in re-call, and from 75% to 85% in precision).
Thereseemed general agreement, however, that having pre-pared code for template lements in advance did makeit easier to port a system to a new scenario in a fewweeks.
The goal for scenar io  templates  - -  mini-MUC - -  was to demonstrate hat effective informationextraction systems could be created in a few weeks.Although it is difficult to meaningfully compare re-sults on different scenarios, the scores obtained bymost systems after a few weeks (40% to 50% recall,60% to 70% precision) were comparable to the bestscores obtained in prior MUCs.Pushing improvements in the underlying technol-ogy was one of the goals of SemEval and its current420Tasksite named entity coreference template element scenario templateBBN Systems and Technology ?
?
?Univ.
of Durham (UK) ?
?
?
?Knight-Ridder InformationLockheed-Martin ?
?
?Univ.
of Manitoba ?
?
?
?Univ.
of Massachusetts, Amherst ?
?
?
?M ITRE ?
?New Mexico State Univ., Las CrucesNew York Univ.
?
?
?
?Univ.
of PennsylvaniaSAICUniv.
of Sheffield (UK) ?
?
?
?SRA ?
?
,SKI ?
?
?
?Sterling Software ?
.Wayne State Univ.Figure 6: The participants in MUC-6.survivor, core ference .
Much of the energy for thecurrent round, however, went into honing the def-inition of the task.
We may hope that, once thetask specification settles down, further evaluations,coupled with the availability of coreference-annotatedcorpora, will encourage more work in this area.Appendix: Sample Scenario Tem-plateShown below is a sample filled template for the MUC-6 scenario template task.
The scenario involvedchanges in corporate executive management person-nel.
For the textMcCann has initiated a new so-called globalcollaborative system, composed of world-wide account directors paired with creativepartners.
In addition, Peter Kim was hiredfrom WPP Group's J. Walter Thompsonlast September as vice chairman, chief strat-egy officer, world-wide.the following objects were to be generated:<SUCCESSION_EVENT-9402240133-3> : =SUCCESS I ON_ORG:<ORGANIZATI 0N-9402240133- i>POST: "vice chairman, chief strategyofficer, world-wide"IN_AND_OUT : <IN_AND_OUT-9402240133-5>VACANCY_REASON: OTH_UNK<IN_AND_OUT-9402240133-5> :=IO_PERSON: <PERSON-9402240133-5>NEW_STATUS: INON_THE_JOB: YESOTHER_ORG: <ORGANIZATION-9402240133-8>REL_OTHER_ORG: OUTSIDE_ORG<ORGANIZATION-9402240133-1> :=ORG_NAME: "McCann"ORG_TYPE: COMPANY<ORGANIZATION-9402240i33-8> :=ORG_NAME: "J. Walter Thompson"ORG_TYPE: COMPANY<PERSON-9402240133-5> :=PER_NAME: "Peter Kim"Although we cannot explain all the details of thetemplate here, a few highlights should be noted.For each executive post, one generates a SUCCES-SION_EVENT object, which contains references tothe ORGANIZATION object for the organization in-volved, and the IN_AND_OUT object for the ac-tivity involving that post (if an article describesa person leaving and a person starting the samejob, there will be two IN_AND_OUT objects).
TheIN_AND_OUT object contains references to the ob-jects for the PERSON and for the ORGANIZAT IONfrom which the person came (if he/she is starting anew job).
The PERSON and ORGANIZAT ION ob-jects are the "template lement" objects, which areinvariant across scenarios.421References[1] Marsh, E. General Semantic Patterns in Differ-ent Sublanguages.
In Analyzing Language in Re-stricted Domains: Sublanguage Description andProcessing, R. Grishman and R. Kittredge, eds.,Lawrence Erlbaum Assoc., Hillsdale, N J, 1986.
[2] Montgomery, C. Distinguishing Fact from Opin-ion and Events from Meta-Events.
Proc.
Conf.Applied Natural Language Processing, 1983.
[3] Sager, N., Friedman, C., and Lyman, M. etal.
Medical Language Processing: ComputerManagement of Narrative Data.
Addison-Wesley,Reading, MA, 1987.
[4] Vilain, M. et al, A Model-Theoretic Corefer-ence Scoring Scheme.
Proc.
Sixth Message Un-derstanding Conference (MUC-6), Morgan Kauf-mann, San Francisco, 1996.422
