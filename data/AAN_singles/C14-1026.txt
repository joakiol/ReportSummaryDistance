Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 257?268, Dublin, Ireland, August 23-29 2014.Multi-view Chinese TreebankingLikun Qiu1,2,3, Yue Zhang1, Peng Jin4and Houfeng Wang21Singapore University of Technology and Design, Singapore2Institute of Computational Linguistics, Peking University, China3School of Chinese Language and Literature, Ludong University, China4Lab of Intelligent Information Processing and Application, Leshan Normal University, China{qiulikun,jandp,wanghf}@pku.edu.cn, yue zhang@sutd.edu.sgAbstractWe present a multi-view annotation framework for Chinese treebanking, which uses dependen-cy structures as the base view and supports conversion into phrase structures with minimal lossof information.
A multi-view Chinese treebank was built under the proposed framework, andthe first release (PMT 1.0) containing 14,463 sentences is be made freely available.
To verifythe effectiveness of the multi-view framework, we implemented an arc-standard transition-baseddependency parser and added phrase structure features produced by the phrase structure view.Experimental results show the effectiveness of additional features for dependency parsing.
Fur-ther, experiments on dependency-to-string machine translation show that our treebank and parsercould achieve similar results compared to the Stanford Parser trained on CTB 7.0.1 IntroductionPhrase structures (PS) and dependency structures (DS) are two of the most popular grammar formalismsfor statistical parsing (Collins, 2003; Charniak, 2000; McDonald et al., 2005; Nivre, 2006; Petrov andKlein, 2007; Zhang and Clark, 2008).
While DS trees emphasize the grammatical relation between headsand dependents, PS trees stress the hierarchical constituent structures of sentences.
Several researchershave explored DS and PS simultaneously to enhance the quality of syntactic parsing (Wang and Zong,2010; Farkas and Bohnet, 2012; Sun and Wan, 2013) and tree-to-string machine translation (Meng et al.,2013), showing that the two types of information complement each other for NLP tasks.Most existing Chinese and English treebanks fall into the phrase structure category, and much workhas been done to convert PS into DS (Magerman, 1994; Collins et al., 1999; Collins, 2003; Sun andJurafsky, 2004; Johansson and Nugues, 2007; Duan et al., 2007; Zhang and Clark, 2008).
Research onstatistical dependency parsing has frequently used dependency treebanks converted from phrase structuretreebanks, such as the Penn Treebank (PTB) (Marcus et al., 1993) and Penn Chinese Treebank (CTB)(Xue et al., 2000).
However, previous research shows that dependency categories in converted tree-banks are simplified (Johansson and Nugues, 2007), and the widely used head-table PS to DS conversionapproach encounters ambiguities and uncertainty, especially for complex coordination structures (Xue,2007).
The main reason is that the PS treebanks were designed without consideration of DS conversion,leading to inherent ambiguities in the mapping, and loss of information in the resulting DS treebanks.
Tominimize information loss during treebank conversions, a treebank could be designed by considering PSand DS information simultaneously; such treebanks have been proposed as multi-view treebanks (Xia etal., 2009).
We develop a multi-view treebank for Chinese, which treats PS and DS as different views ofthe same internal structures of a sentence.We choose the DS view as the base view, from which PS would be derived.
Our choice is based on theeffectiveness of information transfer rather than convenience of annotation (Rambow, 2010; Bhatt andXia, 2012).
Research on Chinese syntax (Zhu, 1982; Chen, 1999; Chen, 2009) shows that the phrasalcategory of a constituent can be derived from the phrasal categories of its immediate subconstituents andThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/257PKU POS Our POSAg, a, ad, ia, ja, la a (adjective)Bg,b, ib, jb, jm, lb b (distinguishing words)Dg, d, dc, df, id, jd, ld d (adverb)m, mq m(number)n, an, in, jn, ln, Ng, vn, nr, kn n (noun)Qg,q, qb, qc, qd, qe, qj, ql, qr, qt, qv, qz q (measure word)Rg,r, rr, ry, ryw, rz, rzw r (pronoun)Tg, t, tt t (temporal noun)u, ud, ue, ui, ul, uo, us, uz, Ug u (auxiliary word)v, iv, im, jv, lv, Vg, vd, vi, vl, vq,vu, vx, vt,kv v (verb)w, wd, wf, wj, wk, wky, wkz, wm,wp, ws, wt, wu, ww, wy, wyy, wyz w (punctuation)Table 1: Mapping from PKU POS to our POS.the dependency categories between them (for terminal words, parts-of-speech can be used as phrasal cat-egories).
Consequently, in Chinese, the canonical PS, containing information of constituent hierarchiesand phrasal categories, can be derived naturally from the canonical DS.
As Xia et al.
(2009) stated, a richset of dependency categories should be designed to ensure lossless conversion from DS to PS.
When theinformation of PS has been represented in DS explicitly or implicitly, we can convert DS to PS withoutambiguity (Rambow et al., 2002).Given our framework, a multi-view Chinese treebank, containing 14,463 sentences and 336K words,is constructed.
This main corpus is based on the Peking University People?s Daily Corpus.
We name ourtreebank the Peking University Multi-view Chinese Treebank (PMT) release 1.0.
To verify the useful-ness of the treebank for statistical NLP, a transition-based dependency parser is implemented to includePS features produced in the derivation process of phrasal categories.
We perform a set of empiricalevaluations, with experimental results on both dependency parsing and dependency-to-string machinetranslation showing the effectiveness of the proposed annotation framework and treebank.
We make thetreebank, the DS to PS conversion script and the parser freely available.2 Annotation Framework2.1 Part-of-speech TagsetOur part-of-speech (POS) tagset is based on the Peking University (PKU) People?s Daily corpus, whichconsists of over 100 tags (Yu et al., 2003).
We simplify the PKU tagset by syntactic distribution.
Thesimplified tagset contains 33 POS tags.
The mapping from the original PKU POS to our simplified POS isshown in Table 1.
For instance, Ag (adjective morpheme), ad (adjective acting as an adverb), ia (adjectiveidioms), ja (adjective abbreviation) and la (temporary phrase acting as an adjective) are all mapped to onetag a (adjective).
A set of basic PKU POS tags, including c (conjunction), e (interjection), f (localizer),g (morpheme), h (prefix), i (idiom), j (abbreviation), k (suffix), l (temporary phrase), nr (personal name),nrf (family name), nrg (surname), ns (toponym), nt (organization name), nx (non-Chinese noun), nz(other proper noun), o (onomonopeia), p (preposition), q (measure word), r (pronoun), s (locative), x(other non-Chinese word), y (sentence final particle), z (state adjective), are left unchanged.2.2 Dependency Category TagsetIn a DS, the modifier is tagged with a dependency category, which denotes the role the modifier playswith regard to its head.
The root word of a sentence is dependent on a virtual root node R and taggedwith the dependency category HED.
Table 2 lists the 32 dependency categories used in our annotationguideline.
These categories are designed in consideration of PS conversion with minimal ambiguities,and can be classified according to the following criteria:(1) whether the head dominates a compound clause (i.e.
has an IC modifier) in the PS view.
Accord-ing to this, dependency categories can be cross-clause or in-clause.
For instance, in Figure 1, the lastpunctuation (") is labeled with the cross-clause tag PUS, and its head dominates an IC modifier.
(2) therelative position of the modifier to the head.
According to this, dependency categories can be left, rightor free.
For instance, the LAD, SBV, ADV, COS, DE and ATT labels in Figure 1 are all left.
The VOB label258Tag Description Tag DescriptionACT action object LAD left additiveADV adverbial MT modality and timeAPP appositive element NUM numberATT attribute POB propositional objectCMP complement PUN punctuationCOO other coordination element PUS cross-clause punctuationCOS share-right-child coordination element QUC post-positional quantityDE de (modifier of(special function word)) QUCC non-shared post-positional quantityDEI dei (modifier of(special function word)) QUN quantityDI di (modifier of/(special function word)) RAD right additiveFOC focus RADC non-shared right additiveHED root of a sentence RED reduplicate elementIC independent clause SBV subjectIOB indirect object TPC topicIS independent structure VOB direct objectISC non-shared independent structure VV serial verb constructionTable 2: Proposed dependency category set.is right, while the PUS, PUN, IC labels are free and can lie on both sides.
(3) whether the modifiers of ahead follows the right-to-left order when combined with the head for deriving the PS structure.
Accord-ing to this, dependency categories can be special (not following the right-to-left order) or common.
Forinstance, in Figure 1, the word ?
*	 (observe)?
was labeled with the special left tag COS, because it iscombined with its head ?Nb (consider)?
before ?Nb (consider)?
?s VOB modifier on the right.Combining the three perspectives, the 32 dependency categories can be classified into 8 classes.
Cate-gories in different classes have different priorities when attached to the head word during PS conversion.
(1) Special left (2 labels): COS and RED.
If there is a word tagged with the special left category, allthe words between this word and its head word should be taken as special left.
(2) Common left (13 labels): ADV, APP, ATT, DE, DI, FOC, NUM, QUN, SBV, TPC, VV, PUN andIS.
For instance, ???
(must)?
in Figure 1 is labeled with the common left tag ADV and follows theright-to-left order, being combined with its head ?
?u (be good at)?
after ?Nb (consider)?.
(3) Common left cross-clause (5 labels): ADV, SBV, LAD, TPC and IS.
A common left cross-clausemodifier can also act like common left in-clause, but not vice versa.
(4) Common right (7 labels): ACT, CMP, DEI, IOB, MT, POB and VOB.
For instance, the word ?%n (psychology)?
in Figure 1 is labeled with VOB and follows the right-to-left order.
(5) Special right (4 labels): QUC, RAD, PUN, IS.
In particular, PUN and IS are common categorieswhen appearing on the left side but special categories on the right side of the head.
(6) Special right (attached before COO) (3 labels): QUCC, RADC and ISC.
These categories differfrom those in the previous class in that they would be combined to the head before COO modifiers.
(7) Free cross-clause (2 labels): IC, PUS.
IC is a clausal category and so can be used to connect twoclauses.
PUS denotes cross-clause punctuations.
(8) Common left coordination (2 labels): COO and LAD.2.3 Rules for Annotating PunctuationsTo resolve the ambiguity of finding the head of a punctuation, we make the following rules.
(1) Coupled punctuations (e.g.
brackets and quotation marks) take the head word of the phrase betweenthe two punctuations as their head.
(2) Full stops, question marks, exclamatory marks and semicolons take the topmost head word (with-out violating projectivity) on their left as their heads.
(3) Commas take the nearest word on the right with HED or IC, or the topmost head words on the right(if there is no right node tagged with HED or IC), or the nearest words on their left tagged with HED orIC as their heads, all under the condition of not breaking projectivity.
(4) Colons take the topmost head word (without violating projectivity) on their right as their heads.
(5) Slight-pause marks (!)
take the head of the COO or COS constituent on their left as their heads.2593 Automatic Derivation of Phrase Function and HierarchyIn our treebank, DS is represented explicitly and PS implicitly.
The conversion from DS to PS consistsof two steps.
First, a binary PS hierarchy is generated bottom-up according to the DS.
Second, eachnon-terminal node in the hierarchy is tagged with a phrasal tag (e.g.
NP, VP) based on manual rules.
Weadopt the PS tagset of the CTB (Xue et al., 2000) for our treebank.3.1 Derivation of Phrase Hierarchy3.1.1 Derivation AlgorithmThe PS trees in our grammar are binary-branching, making the derivation of hierarchical PS from DSrelatively straightforward.
With leaf nodes being pre-terminals, a PS is derived bottom-up by recursivecombinations of neighbouring spans according to the dependency links in a sentence.
In this process,a head word is always combined with the nearest modifier that is currently not in the constituents itdominates.
The only ambiguities lie in the orders in which neighbouring PS are combined to form alarger PS, which can be denoted as (A (B C)) versus ((A B) C), with A, B, and C being three neighbouringspans.
For the above ambiguity to exist, the head word for each span must bare the dependency links(Ax B y C), with the head word of B being the head of those of A and C.In most cases, the (A (B C)) structure is chosen.
An intuitive example is that a verb is first combinedwith the object (VOB, a common right category) to form a VP, before being combined with the subject(SBV, a common left category) to form an IP.
One example of ((A B) C) structures is the coordinationstructure shown in Figure 1, where the spans headed by ?
*	 (observe)?
and ?%n (psychology)?
arecombined after those by ?
*	 (observe)?
and ?Nb (consider)?, due to the fact that ?%n (psycholo-gy)?
is a shared object to the coordinated verbs, linked by a COS (a special left category) arc.
In general,the modifiers of a given head are attached according the following priorities:(1) the special left category > (2) the common right category > (3) the common left category > (4) thespecial right category before COO > (5) the common left coordination category > (6) the other specialright category > (7) the free cross-clause clausal category (IC) > (8) the common left cross-clausecategory > (9) the free cross-clause punctuations (PUS).3.1.2 A Case Study: Generating the Hierarchy of Coordination StructureWe take coordination structures as an example to illustrate the PS hierarchy generation process.
Typ-ically, researchers treat the rightmost conjunct as the head of a coordinate structure.
However, doingso introduces modifier scope ambiguities when modifiers are also attached to the rightmost head.
Viceversa, treating the leftmost conjunct as the head will lead to ambiguities when modifiers attached to theleft head (Che et al., 2012).
Another choice is treating the conjunction as the head (Huang et al., 2000;Xue, 2007).
However, this is usually not preferred since it makes parsing more difficult and a choice stillhas to be made between the left and right elements when there is no conjunction in a coordinate structure(Xue, 2007).
Our strategy is as follows: (1) Choose the rightmost conjunct as the head to eliminate theambiguities when the modifiers are attached to the left; (2) Classify coordinate structures into commoncoordinate structures (COO) and sharing-right-child coordinate structures (COS).
COO words are takenas common left nodes (as shown in Figure 2), while COS words are special left nodes (as shown in Figure1).
Doing so avoids the aforementioned scope ambiguities for modifiers.3.2 Derivation of Phrasal CategorySeveral Chinese linguists discuss the issue of deriving phrasal categories from the syntactic categoriesof the PS and DS context.
Both Zhu (1982) and Chen (1999) state that if two phrases have constituentswith the same phrasal categories and the dependency types between them are also the same, the phrasalcategories of their combinations must be the same.
Consequently, it is natural to derive the category ofa phrase from the phrasal categories of the immediate constituents and the dependency type between theconstituents.
We make a set of rules for the derivation, each being a DS pattern/phrasal type pair.
The DSpattern is a modifier-head link with associated information such as the dependency category (DepCate)260Figure 1: An instance of DS-PS conversion ((moreover)??
(crew) ??
(must) ?u (be goodat) *	 (observe) Nb (consider)??
(passenger) (?s) %n (psychology) ???
(timely) J?(provide)`?
(quality)??
(service)).
?!?
denotes the head constituent.Figure 2: A second instance of DS-PS conversion (# (new) (de, an auxiliary word)?
(one)c (year)?
(will) ?
(be)??
(be full of) )?
(vitality) !
??
(be full of) F" (hope) (de, an auxiliaryword)?
(one)c (year) ).
?!?
denotes the head constituent.and the phrasal categories (POS tags for terminial nodes) of the subphrases that the modifier and headdominates.
Some high-frequency rules are listed in Table 3.For instance, the phrasal category of??
(be full of) )?
(vitality) in Figure 2 is VP using the rule(v-NP-VOB, VP).
Executing the derivation algorithms in Section 3.1 and derivation rules in Section 3.2,a DS in the proposed framework can be converted into corresponding PS, as shown in Figure 1 and 2.4 The Annotation Process of PMTAccording to the proposed schema, we constructed the multi-view Chinese treebank (PMT), version1.0, which contains about 14,463 sentences and 336K words, and supports both the PS view and DSview.
Our treebanking is based on the work of Yu et al.
(2003), who built a segmented and POS-taggedChinese corpus (the PFR Corpus), and released a sub-corpus containing about 1.1M words for free1.
Wechoose the previous 14,463 sentences from the corpus, follow the original word segmentation standardbut simplify the POS tagset according to the mapping rules described in Section 2.1.
Then each sentenceis annotated into a projective dependency tree according to the annotation framework described in thispaper.To speed up the annotation, a statistical dependency parser is used to give automatic parse trees andannotators are required to check each tree on a visualized annotation platform, which supports detecting1http://klcl.pku.edu.cn/ShowNews.aspx?id=110261HCate MCate DepCate PCate HCate MCate DepCate PCatev NP VOB VP VP NP SBV IPIP w PUS IP NP w PUN NPn NP ATT NP VP IP IC IPp NP POB PP n CP ATT NPn DNP ATT NP NP CP ATT NPu VP DE CP NP DNP ATT NPNP n COO NP v IP VOB VPVP n SBV IP VP v ADV VPu IP DE CP NP NP ATT NPp LCP POB PP VP c LAD VPVP d ADV VP VP PP ADV VPIP IP IC IP VP NP VOB VPu NP DE DNP VP r SBV IPNP NP COO NP VP r SBV IPNP n ATT NP VP VP IC IPTable 3: Some rules for generating phrasal categories.
HCate, MCate and PCate denote the phrasalcategory of the head subphrase, the modifier subphrase and the combined phrase, respectively.invalid derivation from DS to PS.For quality control, a detailed annotation guideline is provided with abundant instances for differenttypes of syntactic structures in Mandarin Chinese.
More information of the guideline can be found in anextended version of this paper.
In addition, we adopt the annotation strategy for the construction of thePenn Chinese Treebank (Xue et al., 2000) ?
one annotator examines an automatic parse tree first, and asecond annotator verifies the annotation of the first annotator.5 A Transition-Based Parser for Multi-view TreebankIn order to demonstrate the usefulness of our treebank in comparison with existing Chinese treebanks,we perform empirical analysis to the treebank, by the statistical dependency parsing and dependency-to-string machine translation tasks.
Several researchers explored joint DS and PS information to enhancethe quality of syntactic parsing (Wang and Zong, 2010; Farkas and Bohnet, 2012; Sun and Wan, 2013).Most tried to combine the outputs of constituent and dependency parsers by stacking or bagging.
Sinceour treebank is multi-view, it is possible to combine DS features and PS features directly in the decodingprocess.We implemented an arc-standard transition-based dependency parser (Nivre, 2008) based on the arc-eager parser of Zhang and Nivre (2011), which is a state-of-the-art transition-based dependency parser(Zhang and Nivre, 2012).
It is more reasonable to derive the phrasal category of a phrase after thecomplete subtree (phrase) rather than partial subtree headed by a word has been built.
The arc-standardparser differs from the arc-eager parser in that it postpones the attachment of right-modifiers until thecomplete subtrees headed by the modifiers themselves have been built.
Because of this, we add PSfeatures into an arc-standard parser rather than an arc-eager one.The parser processes a sentence from left to right, using a stack to maintain partially built derivationsand a queue to hold next incoming words.
Three transition actions (LEFT, RIGHT and SHIFT) are definedto consume input words from the queue and construct arcs using the stack (Nivre, 2008):LEFT pops the second top item off the stack, and adds it as a modifier to the top of the stack;RIGHT pops the top item off the stack, and adds it as a modifier to the second top of the stack;SHIFT removes the front of the queue and pushes it onto the top of the stack.Table 4 show the feature templates of our parser, most of which are based on those of Zhang andNivre (2011).
The contextual information consists of the top four nodes of the stack (S3, S2, S1and S0),the next three input words (N0, N1and N2), the left and right children (ld, rd) of these nodes, and thedistance between S0and S1.
Word and POS information from the context are manually combined.Due to the multi-view nature of our treebank, the DS parser can be extended naturally to incorporate PSinformation.
Further, because our PS is binary branching, each constituent corresponds to a dependencylink.
In the decoding process, we derive the phrasal category c of a subtree whenever a dependency link262features of stack top S0wt;S0w;S0t;S1wt;S1w;S1t;S2wt;S2w;S2t;S3wt;S3w;S3t;N0wt;features of next input N0w;N0t;N1wt;N1w;N1t;N2wt;N2w;N2t;bigram features S0wS1w;S0wS1t;S0tS1w;S0tS1t;S0wN0w;S0wN0t;S0tN0w;S0tN0t;children features of S0S0ldw;S0ldt;S0ldwt;S0ldd;S0rdw;S0rdt;S0rdwt;S0rdd;children features of S1S1ldw;S1ldt;S1ldwt;S1ldd;S1rdw;S1rdt;S1rdwt;S1rdd;distance features S0wDistance(S0, S1);S0tDistance(S0, S1);S1wDistance(S0, S1);S1tDistance(S0, S1);PS features S0c;S1c;S0cS1c;S0wS1c;S0tS1c;S0wS1dS1c;S1wS0c;S1tS0c;S1wS0dS0c;S0cS1cS0S1cTable 4: Transition-based feature templates for the arc-standard dependency parser.
w=word; t=POS tag.d=dependency category.
c=phrasal category.is established, using the derivation rules in Table 3.
Using c and its combination with other features, wecan produce several PS features, as shown in Table 4.
By this simple extension of features, we arrive atan efficient linear-time joint DS and PS parser.6 Experiments6.1 Syntactic ParsingPMT 1.0 contains all the articles of People?s Daily from January 1st to January 10th, 1998.
Sentences12001-13000 and 13001-14463 are used as the development and test set, respectively.
The remainingsentences are used as training data.Several state-of-the-art statistical parsers, including Mate-tools (Bohnet, 2010)2, BerkeleyParser(Petrov and Klein, 2007)3, ZPar-dep (Zhang and Nivre, 2011) and ZPar-con (Zhang and Clark, 2009;Zhu et al., 2013)4are used for comparison.
We used the gold segmentation, and the Stanford POS tagger(Toutanova et al., 2003) (version 3.3.1) to provide automatic POS tags for all the experiments.
The POStagger was trained on the PKU corpus (Yu et al., 2003) containing articles of People?s Daily from January2000 to June 2000.
It achieved a 95.78% precision on the PMT.
In the baseline parser (Ours-standard),the feature templates in Table 4 except the PS features are used.
We refer to the parser after adding PSfeatures as Ours-PS.
The results of dependency (ZPar-eager, Ours-standard, Ours-PS and Mate-tools)and constituent parsers (BerkeleyParser and ZPar-con) are measured by the unlabeled accuracy score(UAS), labeled accuracy score (LAS) and bracketing f-measure (BF), respectively.We display the parsing results in Table 5.
Our dependency parser (Ours-PS) outperforms the baselineparser (Ours-standard) with a 0.47% increase in UAS.
For additional evaluation, we also converted theDS trees parsed by the dependency parsers to PS using the conversion procedure in Section 3, in orderto compare the results of dependency parsers and constituent parsers.
The three ZPar-based dependencyparsers gave higher accuracies than the two state-of-the-art constituent parsers.
In particular, the DS2PSoutputs of Ours-PS parser outperforms the PS outputs of Berkeley Parer with 0.62% higher BF.Both Zhang and Clark (2011) and Petrov and McDonald (2012) show that DS trees converted fromthe outputs of PS parsers outperform those produced directly by DS parsers trained on DS conversions ofthe CTB.
Interestingly, our evaluation on the PMT gave results in the opposite direction: parsers trainedon the DS treebank outperforms parsers trained on the PS conversion.
One possible reason is that parsererrors can be hidden in the conversion process.
Take the sentence in Figure 3 for example.
Figure 3(a)shows the correct PS while Figure 3(b) shows an incorrect parser output.
In particular, ?i?
(dawn)?is put under the incorrect constituent.
When converted into DS, both lead to the correct link, with ?i?
(dawn)?
being the SBV modifier of ?
? (come)?
(Figure 3(c)).
As a result, the PS parser error iserased in the conversion into DS.
The same can happen in DS to PS conversion.6.2 Dependency-to-string Machine TranslationWe compare the effects of our treebank and the Stanford dependencies converted from CTB on machinetranslation, using the dependency-to-string system of Xie et al.
(2011).
Our training corpus consists of2https://code.google.com/p/mate-tools/3http://code.google.com/p/berkeley-parser-analyser/4http://sourceforge.net/projects/zpar/263Figure 3: An instance where PS parser error is erased in the PS to DS conversion (?D (bright) (de,an auxiliary word)i?
(dawn)q (again)?
(one)g(time)? (come)3 (in)??
(the Pearl River)?
(estuary)).
?!?
denotes the head constituent.Dependency Parsing Constituent Parsing Constituent Parsing(DS2PS)Parsers UAS LAS len<=40 words Unlimited len<=40 words UnlimitedMate-tools 82.98 79.37 / / 84.77 83.43ZPar-dep 82.73 80.20 / / 85.47 84.33Ours-standard 82.81 80.04 / / 85.53 84.47Ours-PS 83.28 80.50 / / 85.92 84.84Berkeley Parser / / 85.25 84.22 / /ZPar-con / / 85.02 84.12 / /Table 5: Parsing results on our treebank using automatic POS-tags.31K Chinese-English sentence pairs from the Xinhua Corpus (Liu et al., 2006), and we used NIST MTEvaluation 2006 test set as the development set, and the NIST 2003 (MT03), 2004 (MT04) and 2005(MT05) test sets as the test sets.
For Stanford dependency trees, we parsed the source sentences withthe Stanford Parser (Chang et al., 2009) (version 3.3.1), which was trained on CTB 7.0.
For the PMTtreebank, we used the Ours-PS parser, trained with 14000 sentences (the last 463 sentences are used asdevelopment data for the parser).
All the MT configurations are the same as Xie et al.
(2011).The results are shown in Table 6.
The Chinese-English translation outputs using our parser and tree-bank are slightly lower but comparable to those using the Stanford Parser.
Note that our treebank contains336K words on People?s Daily, while the CTB 7.0 contains about 1.19M words, most on Xinhua, thesource of the MT training and test data.
This result to some degree demonstrates the usefulness of ourtreebank for NLP applications, in comparison with a well-established treebank.7 Related WorkPS Treebanks and DS Conversion PTB (Marcus et al., 1993) and CTB (Xue et al., 2000) are the mostwidely used treebanks for English and Chinese in the literature.
Both are in PS.
For conversion from PSto DS, a head-table approach (Magerman, 1994; Collins, 2003; Yamada and Matsumoto, 2003; Sun andJurafsky, 2004; Nivre, 2006; Johansson and Nugues, 2007; Duan et al., 2007; Zhang and Clark, 2008)is widely used.
However, the reliability of head tables has been questioned (Xue, 2007).
Xue (2007)proposed a novel approach that better exploits the structural information in the CTB and pointed out thatthe results of the approach and the widely used Penn2Malt tools5agree only 60.6% in terms of unlabeleddependency.
The coordination structures, in particular, are not properly converted by Penn2Malt.DS Treebanks and PS Conversion An existing DS treebank for Chinese is the Chinese DependencyTreebank (Che et al., 2012), which is not designed as a multi-view treebank.
For conversion from DSto PS, Xia and Palmer (2001) compare three algorithms.
These algorithms do not use a rich set of5http://stp.lingfil.uu.se/ nivre/research/Penn2Malt.html264Parsers Treebank MT03(BLEU4) MT04(BLEU4) MT05(BLEU4)Stanford Parser CTB 7.0 28.23 29.00 25.72Ours-PS PMT 1.0 27.73 28.71 25.20Table 6: Results of dependency-to-string machine translation.dependency categories, only distinguishing arguments and modifiers.
Xia et al.
(2009) propose a DS-to-PS algorithm, which assumes that a given DS is identical to a flattened version of the desired PS, andthen introduce a set of conversion rules.
Their error analysis show that coordination and punctuationamount to about 32.1% of conversion errors, while other errors fall into missing content in DS andinconsistency in the target treebank (PTB).
This analysis demonstrates that coordination and punctuationshould be tackled carefully for the conversion between PS and DS, which we do in the design of ourtreebank.
Bhatt et al.
(2011) presented three scenarios arising in the conversion of DS into PS.
Bhattand Xia (2012) further described 7 phenomena of incompatibility in the conversion from DS to PS,mainly involving the annotation of empty categories, yet coordination structure and punctuation werenot discussed.Multi-view Treebanks The Tiger (Brants et al., 2002) and T?uBa-D/Z (Telljohann et al., 2003) tree-banks for German seek to explicitly represent both PS and DS by labeling both nodes and edges in thesyntactic tree.
For these treebanks, both dependency categories and phrasal categories have been an-notated explicitly.
The English side of the Czech-English parallel corpus is annotated and linked alsoas both PS (original PTB annotation) and DS (Hajic et al., 2012), while the DS is a conversion of theoriginal PS.
Our multi-view treebank is different in that dependency categories and phrasal categoriesderive from each other.
The Hindi/Urdu treebank (Xia et al., 2009; Palmer et al., 2009; Bhatt et al., 2009)can be taken as a multi-view treebank.
Its PS view is derived automatically from the DS.
However, theconverted PS is not a PS with a full hierarchy but a flattened one (Xia et al., 2009).8 ConclusionWe presented an DS-based multi-view annotation framework, and built a Chinese treebank accordingto the framework and an arc-standard transition-based dependency parser that exploits the multi-viewnature of the treebank.
We used SMT as an example to demonstrate the usefulness of our treebank forNLP applications.
Experiments showed that the proposed treebank and parser can give similar resultsto the Stanford Parser trained on CTB 7.0.
We make our treebank (PMT 1.0) (http://klcl.pku.edu.cn/ResourceList.aspx), the DS to PS conversion script and the proposed parser (http://sourceforge.net/projects/zpar/) freely available.AcknowledgmentsWe gratefully acknowledge the invaluable assistance of Jun Xie in helping us make experiments ondependency-to-string machine translation.
We thank Ji Ma for helpful suggestions and comments, andYijia Liu for providing a generic arc-standard dependency parser.
We also thank all the students fromLudong University and Leshan Normal University for annotating the large-scale treebank.
Special thanksto Xiaoyan Wang, Na Wei and Huarong Ni from Ludong University.
Finally, we thank the anonymousreviewers for their constructive comments, and gratefully acknowledge the support of the SingaporeMinistry of Education (MOE) AcRF Tier 2 grant T2MOE201301, the National Natural Science Foun-dation of China (No.
61103089, No.
61373056), National High Technology Research and DevelopmentProgram of China (863 Program) (No.
2012AA011101), Major National Social Science Fund of China(No.
12&ZD227), Scientific Research Foundation of Shandong Province Outstanding Young ScientistAward (No.
BS2013DX020) and Humanities and Social Science Projects of Ludong University (No.WY2013003).265ReferencesRajesh Bhatt and Fei Xia.
2012.
Challenges in converting between treebanks: a case study from the hutb.
In Pro-ceedings of META-RESEARCH Workshop on Advanced Treebanking, in conjunction with LREC-2012, Istanbul,Turkey.Rajesh Bhatt, Bhuvana Narasimhan, Martha Palmer, Owen Rambow, Dipti Misra Sharma, and Fei Xia.
2009.A multi-representational and multi-layered treebank for hindi/urdu.
In Proceedings of the Third LinguisticAnnotation Workshop, pages 186?189.Rajesh Bhatt, Owen Rambow, and Fei Xia.
2011.
Linguistic phenomena, analyses, and representations: Under-standing conversion between treebanks.
In Proceedings of IJCNLP 2011, pages 1234?1242.Bernd Bohnet.
2010.
Very high accuracy and fast dependency parsing is not a contradiction.
In Proceedings ofCOLING 2010, pages 89?97.Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang Lezius, and George Smith.
2002.
The TIGER treebank.In Proceedings of the workshop on treebanks and linguistic theories, volume 168.Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and Christopher D Manning.
2009.
Discriminative reorderingwith chinese grammatical relations features.
In Proceedings of the Third Workshop on Syntax and Structure inStatistical Translation, pages 51?59.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings of the NAACL 2000, pages 132?139.Wanxiang Che, Li Zhenghua, and Liu Ting.
2012.
Chinese Dependency Treebank 1.0.
Linguistic Data Consor-tium.Baoya Chen.
1999.
Chinese Linguistic Methodology in the 21th Century, 1898-1998.
Shandong EducationPublishing House.Baoya Chen.
2009.
Contemporary Linguistics.
Higher Education Press.Michael Collins, Lance Ramshaw, Jan Haji?c, and Christoph Tillmann.
1999.
A statistical parser for czech.
InProceedings of the 37th annual meeting of the Association for Computational Linguistics on ComputationalLinguistics, pages 505?512.
Association for Computational Linguistics.Michael Collins.
2003.
Head-driven statistical models for natural language parsing.
Computational linguistics,29(4):589?637.Xiangyu Duan, Jun Zhao, and Bo Xu.
2007.
Probabilistic parsing action models for multi-lingual dependencyparsing.
In Proceedings of EMNLP-CoNLL 2007, pages 940?946.Rich?ard Farkas and Bernd Bohnet.
2012.
Stacking of dependency and phrase structure parsers.
In Proceedings ofCOLING 2012, pages 849?866.Jan Hajic, Eva Hajicov?a, Jarmila Panevov?a, Petr Sgall, Ondrej Bojar, Silvie Cinkov?a, Eva Fuc?
?kov?a, MarieMikulov?a, Petr Pajas, Jan Popelka, et al.
2012.
Announcing prague czech-english dependency treebank 2.0.
InLREC, pages 3153?3160.Chu-Ren Huang, Feng-Yi Chen, Keh-Jiann Chen, Zhao-ming Gao, and Kuang-Yu Chen.
2000.
Sinica treebank:design criteria, annotation guidelines, and on-line interface.
In Proceedings of the second workshop on Chineselanguage processing: held in conjunction with the 38th ACL-Volume 12, pages 29?37.Richard Johansson and Pierre Nugues.
2007.
Extended constituent-to-dependency conversion for English.
In 16thNordic Conference of Computational Linguistics, pages 105?112.
University of Tartu.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machine translation.In Proceedings of COLING-ACL 2006, pages 609?616.
Association for Computational Linguistics.David M Magerman.
1994.
Natural language parsing as statistical pattern recognition.
arXiv preprint cmp-lg/9405009.Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini.
1993.
Building a large annotated corpus ofEnglish: The Penn Treebank.
Computational linguistics, 19(2):313?330.266Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Haji?c.
2005.
Non-projective dependency parsing usingspanning tree algorithms.
In Proceedings of HLT-EMNLP 2005, pages 523?530.Fandong Meng, Jun Xie, Linfeng Song, Yajuan L?u, and Qun Liu.
2013.
Translation with source constituency anddependency trees.
In Proceedings of EMNLP 2010, pages 1066?1076.Joakim Nivre.
2006.
Inductive dependency parsing.
Springer.Joakim Nivre.
2008.
Algorithms for deterministic incremental dependency parsing.
Computational Linguistics,34(4):513?553.Martha Palmer, Rajesh Bhatt, Bhuvana Narasimhan, Owen Rambow, Dipti Misra Sharma, and Fei Xia.
2009.Hindi syntax: Annotating dependency, lexical predicate-argument structure, and phrase structure.
In The 7thInternational Conference on Natural Language Processing, pages 14?17.Slav Petrov and Dan Klein.
2007.
Improved inference for unlexicalized parsing.
In Proceedings of HLT-NAACL2007, pages 404?411.Slav Petrov and Ryan McDonald.
2012.
Overview of the 2012 shared task on parsing the web.
In Notes of theFirst Workshop on Syntactic Analysis of Non-Canonical Language (SANCL), volume 59.Owen Rambow, Cassandre Creswell, Rachel Szekely, Harriet Taber, and Marilyn A Walker.
2002.
A dependencytreebank for english.
In Proceedings of LREC 2002.Owen Rambow.
2010.
The simple truth about dependency and phrase structure representations: An opinion piece.In Proceedings of HLT-NAACL 2010, pages 337?340.Honglin Sun and Daniel Jurafsky.
2004.
Shallow semantc parsing of Chinese.
In Proceedings of HLT-NAACL2004, pages 249?256.Weiwei Sun and Xiaojun Wan.
2013.
Data-driven, PCFG-based and pseudo-PCFG-based models for Chinesedependency parsing.
Transactions of the Association for Computational Linguistics, 1(1):301?314.Heike Telljohann, Erhard W Hinrichs, Sandra K?ubler, Heike Zinsmeister, and Kathrin Beck.
2003.
Stylebook forthe t?ubingen treebank of written german (t?uba-d/z).
In Seminar f?ur Sprachwissenschaft, Universit?at T?ubingen,Germany.Kristina Toutanova, Dan Klein, Christopher D Manning, and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Proceedings of NAACL 2003-Volume 1, pages 173?180.Zhiguo Wang and Chengqing Zong.
2010.
Phrase structure parsing with dependency structure.
In Proceedings ofColing 2010: Posters, pages 1292?1300.Fei Xia and Martha Palmer.
2001.
Converting dependency structures to phrase structures.
In Proceedings of HLT2001, pages 1?5.
Association for Computational Linguistics.Fei Xia, Owen Rambow, Rajesh Bhatt, Martha Palmer, and Dipti Misra Sharma.
2009.
Towards a multi-representational treebank.
In The 7th International Workshop on Treebanks and Linguistic Theories.Jun Xie, Haitao Mi, and Qun Liu.
2011.
A novel dependency-to-string model for statistical machine translation.In Proceedings of EMNLP 2011, pages 216?226.Nianwen Xue, Fei Xia, Shizhe Huang, and Anthony Kroch.
2000.
The bracketing guidelines for the penn chinesetreebank (3.0).Nianwen Xue.
2007.
Tapping the implicit information for the PS to DS conversion of the chinese treebank.
InProceedings of the Sixth International Workshop on Treebanks and Linguistics Theories.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statistical dependency analysis with support vector machines.
InProceedings of IWPT 2003, volume 3.Shiwen Yu, Huiming Duan, Xuefeng Zhu, Bin Swen, and Baobao Chang.
2003.
Specification for corpus process-ing at peking university: Word segmentation, pos tagging and phonetic notation.
Journal of Chinese Languageand Computing, 13(2):121?158.Yue Zhang and Stephen Clark.
2008.
A tale of two parsers: investigating and combining graph-based andtransition-based dependency parsing using beam-search.
In Proceedings of EMNLP 2008, pages 562?571.267Yue Zhang and Stephen Clark.
2009.
Transition-based parsing of the Chinese treebank using a global discrimina-tive model.
In Proceedings of IWPT 2009, pages 162?171.Yue Zhang and Stephen Clark.
2011.
Syntactic processing using the generalized perceptron and beam search.Computational Linguistics, 37(1):105?151.Yue Zhang and Joakim Nivre.
2011.
Transition-based dependency parsing with rich non-local features.
In Pro-ceedings of ACL-HLT 2011: short papers-Volume 2, pages 188?193.Yue Zhang and Joakim Nivre.
2012.
Analyzing the effect of global learning and beam-search on transition-baseddependency parsing.
In COLING (Posters), pages 1391?1400.Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu.
2013.
Fast and accurate shift-reduceconstituent parsing.
In Proceedings of ACL 2013, pages 434?443.Dexi Zhu.
1982.
Grammar Finder.
Commercial Press.268
