Proceedings of the 12th Conference of the European Chapter of the ACL, pages 505?513,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsUser Simulations for context-sensitive speech recognition in SpokenDialogue SystemsOliver LemonEdinburgh Universityolemon@inf.ed.ac.ukIoannis KonstasUniversity of Glasgowkonstas@dcs.gla.ac.ukAbstractWe use a machine learner trained on acombination of acoustic and contextualfeatures to predict the accuracy of incom-ing n-best automatic speech recognition(ASR) hypotheses to a spoken dialoguesystem (SDS).
Our novel approach is touse a simple statistical User Simulation(US) for this task, which measures thelikelihood that the user would say eachhypothesis in the current context.
SuchUS models are now common in machinelearning approaches to SDS, are trained onreal dialogue data, and are related to the-ories of ?alignment?
in psycholinguistics.We use a US to predict the user?s next dia-logue move and thereby re-rank n-best hy-potheses of a speech recognizer for a cor-pus of 2564 user utterances.
The methodachieved a significant relative reduction ofWord Error Rate (WER) of 5% (this is44% of the possible WER improvementon this data), and 62% of the possible se-mantic improvement (Dialogue Move Ac-curacy), compared to the baseline policyof selecting the topmost ASR hypothesis.The majority of the improvement is at-tributable to the User Simulation feature,as shown by Information Gain analysis.1 IntroductionA crucial problem in the design of spoken dia-logue systems (SDS) is to decide for incomingrecognition hypotheses whether a system shouldaccept (consider correctly recognized), reject (as-sume misrecognition), or ignore (classify as noiseor speech not directed to the system) them.Obviously, incorrect decisions at this point canhave serious negative effects on system usabilityand user satisfaction.
On the one hand, accept-ing misrecognized hypotheses leads to misunder-standings and unintended system behaviors whichare usually difficult to recover from.
On the otherhand, users might get frustrated with a system thatbehaves too cautiously and rejects or ignores toomany utterances.
Thus an important feature in di-alogue system engineering is the tradeoff betweenavoiding task failure (due to misrecognitions) andpromoting overall dialogue efficiency, flow, andnaturalness.In this paper, we investigate the use of machinelearning trained on a combination of acoustic fea-tures and features computed from dialogue contextto predict the quality of incoming n-best recogni-tion hypotheses to a SDS.
These predictions arethen used to select a ?best?
hypothesis and to de-cide on appropriate system reactions.
We evalu-ate this approach in comparison with a baselinesystem that works in the standard way: alwayschoosing the topmost hypothesis in the n-best list.In such systems, complex repair strategies are re-quired when the top hypothesis is incorrect.The main novelty of this work is that we ex-plore the use of predictions from simple statisti-cal User Simulations to re-rank n-best lists of ASRhypotheses.
These User Simulations are now com-monly used in statistical learning approaches to di-alogue management (Williams and Young, 2003;Schatzmann et al, 2006; Young, 2006; Young etal., 2007; Schatzmann et al, 2007), but they havenot been used for context-sensitive ASR before.In our model, the system?s ?belief?
b(h) in arecognition hypothesis h is factored in two parts:the observation probability P (o|h) (approximatedby the ASR confidence score) and the User Simu-lation probability P (h|us,C) of the hypothesis:b(h) = P (o|h).P (h|us,C) (1)where us is the state of the User Simulation incontext C. The context is simply a window of di-505alogue acts in the dialogue history, that the US issensitive to (see section 3).The paper is organized as follows.
After a shortrelation to previous work, we describe the data(Section 5) and derive baseline results (Section6).
Section 3 describes the User Simulations thatwe use for re-ranking hypotheses.
Section 7 de-scribes our learning experiments for classifyingand selecting from n-best recognition hypothesesand Section 9 reports our results.2 Relation to Previous WorkIn psycholinguistics, the idea that human dialogueparticipants simulate each other to some extent isgaining currency.
(Pickering and Garrod, 2007)write:?if B overtly imitates A, then A?s com-prehension of B?s utterance is facilitatedby A?s memory for A?s previous utter-ance.
?We explore aspects of this idea in a computa-tional manner.
Similar work in the area of spokendialogue systems is described below.
(Litman et al, 2000) use acoustic-prosodic in-formation extracted from speech waveforms, to-gether with information derived from their speechrecognizer, to automatically predict misrecog-nized turns in a corpus of train-timetable informa-tion dialogues.
In our experiments, we also userecognizer confidence scores and a limited num-ber of acoustic-prosodic features (e.g.
amplitudein the speech signal) for hypothesis classification,but we also use User Simulation predictions.
(Walker et al, 2000) use a combination of fea-tures from the speech recognizer, natural languageunderstanding, and dialogue manager/discoursehistory to classify hypotheses as correct, partiallycorrect, or misrecognized.
Our work is related tothese experiments in that we also combine con-fidence scores and higher-level features for clas-sification.
However, both (Litman et al, 2000)and (Walker et al, 2000) consider only single-bestrecognition results and thus use their classifiers as?filters?
to decide whether the best recognition hy-pothesis for a user utterance is correct or not.
Wego a step further in that we classify n-best hypothe-ses and then select among the alternatives.
We alsoexplore the use of more dialogue and task-orientedfeatures (e.g.
the dialogue move type of a recogni-tion hypothesis) for classification.
(Gabsdil and Lemon, 2004) similarly performreordering of n-best lists by combining acousticand pragmatic features.
Their study shows that di-alogue features such as the previous system ques-tion and whether a hypothesis is the correct answerto a particular question contributed more to classi-fication accuracy than the other attributes.
(Jonson, 2006) classifies recognition hypothe-ses with labels denoting acceptance, clarification,confirmation and rejection.
These labels werelearned in a similar way to (Gabsdil and Lemon,2004) and correspond to varying levels of con-fidence, being essentially potential directives tothe dialogue manager.
Apart from standard fea-tures Jonson includes attributes that account forthe whole n-best list, i.e.
standard deviation ofconfidence scores.As well as the use of a User Simulation, themain difference between our approach and workon hypothesis reordering (e.g.
(Chotimongkol andRudnicky, 2001)) is that we make a decision re-garding whether a dialogue system should accept,clarify, reject, or ignore a user utterance.
Like(Gabsdil and Lemon, 2004; Jonson, 2006), ourapproach is more generally applicable than pre-ceding research, since we frame our methodologyin the Information State Update (ISU) approachto dialogue management (Traum et al, 1999) andtherefore expect it to be applicable to a range ofrelated multimodal dialogue systems.3 User SimulationsWhat makes this study different from the previouswork in the area of post-processing of the ASR hy-potheses is the incorporation of a User Simulationoutput as an additional feature.
The history of a di-alogue between a user and a dialogue system playsan important role as to what the user might be ex-pected to say next.
As a result, most of the stud-ies mentioned in the previous section make vari-ous efforts to capture history by including relevantfeatures directly in their classifiers.Various statistical User Simulations have beentrained on corpora of dialogue data in order tosimulate real user behaviour (Schatzmann et al,2006; Young, 2006; Georgila et al, 2006; Younget al, 2007; Schatzmann et al, 2007).
We devel-oped a simple n-gram User Simulation, using n-grams of dialogue moves.
It treats a dialogue asa sequence of lists of consecutive user and systemturns in a high level semantic representation, i.e.506< SpeechAct >,< Task > pairs, for example< provide info >,< music genre(punk) >.It takes as input the n ?
1 most recent lists of< SpeechAct >,< Task > pairs in the dialoguehistory, and uses the statistics in the training setto compute a distribution over the possible nextuser actions.
If no n-grams match the current his-tory, the model can back-off to n-grams of lowerorder.
We use this model to assess the likelihoodof each candidate ASR hypothesis.
Intuitively, thisis the likelihood that the user really would say thehypothesis in the current dialogue situation.
Thebenefit of using n-gram models is that they are fastand simple to train even on large corpora.The main hypothesis that we investigate is thatby using the User Simulation model to predict thenext user utterance, we can effectively increase theperformance of the speech recogniser module.4 Evaluation metricsTo evaluate performance we use Dialogue MoveAccuracy (DMA), a strict variant of Concept Er-ror Rate (CER) as defined by (Boros et al, 1996),which takes into account the semantic aspects ofthe difference between the classified utterance andthe true transcription.
CER is similar to WER,since it takes into account deletions, insertionsand substitutions on the semantic (rather than theword) level of the utterance.
DMA is stricter thanCER in the sense that it does not allow for par-tial matches in the semantic representation.
Inother words, if the classified utterance correspondsto the same semantic representation as the tran-scribed then we have 100% DMA, otherwise 0%.Sentence Accuracy (SA) is the alignment of asingle hypothesis in the n-best list with the truetranscription.
Similarly to DMA, it accounts forperfect alignment between the hypothesis and thetranscription, i.e.
if they match perfectly we have100% SA, otherwise 0%.5 Data CollectionFor our experiments, we use data collected in auser study with the Town-Info spoken dialoguesystem, using the HTK speech recognizer (Young,2007).
In this study 18 subjects had to solve 10search/browsing tasks with the system, resulting in180 complete dialogues and 2564 utterances (av-erage 14.24 user utterances per dialogue).For each utterance we have a series of files of60-best lists produced by the speech recogniser,namely the transcription hypotheses on a sentencelevel along with the acoustic model score and theequivalent transcriptions on a word level, with in-formation such as the duration of each recognisedframe and the confidence score of the acoustic andlanguage model of each word.5.1 LabelingWe transcribed all user utterances and parsed thetranscriptions offline using a natural language un-derstanding component (a robust Keyword Parser)in order to get a gold-standard labeling of the data.We devised four labels with decreasing order ofconfidence: ?opt?
(optimal), ?pos?
(positive), ?neg?
(negative), ?ign?
(ignore).
These are automaticallygenerated using two different modules: a key-word parser that computes the < SpeechAct ><Task > pair as described in the previous sec-tion and a Levenshtein Distance calculator, for thecomputation of the DMA and WER of each hy-pothesis respectively.
The reason for opting for amore abstract level, namely the semantics of thehypotheses rather than individual word recogni-tion, is that in SDS it is usually sufficient to relyon the meaning of message that is being conveyedby the user rather than the precise words that theyused.Similar to (Gabsdil and Lemon, 2004; Jonson,2006) we ascribe to each utterance either of the?opt?, ?pos?, ?neg?, ?ign?
labels according to thefollowing schema:?
opt: The hypothesis is perfectly aligned andsemantically identical to the transcription?
pos: The hypothesis is not entirely aligned(WER < 50) but is semantically identical tothe transcription?
neg: The hypothesis is semantically identicalto the transcription but does not align well(WER > 50) or is semantically different tothe transcription?
ign: The hypothesis was not addressed tothe system (crosstalk), or the user laughed,coughed, etc.The 50% value for the WER as a threshold forthe distinction between the ?pos?
and ?neg?
cate-gory is adopted from (Gabsdil, 2003), based onthe fact that WER is affected by concept accuracy(Boros et al, 1996).
In other words, if a hypothe-sis is erroneous as far as its transcript is concerned507Transcript: I?d like to find a bar pleaseI WOULD LIKE TO FIND A BAR PLEASE posI LIKE TO FIND A FOUR PLEASE negI?D LIKE TO FIND A BAR PLEASE optWOULD LIKE TO FIND THE OR PLEASE ignTable 1: Example hypothesis labellingthen it is highly likely that it does not convey thecorrect message from a semantic point of view.We always label conceptually equivalent hypothe-ses to a particular transcription as potential candi-date dialogue strategy moves, and total misrecog-nitions as rejections.
In table 5.1 we show exam-ples of the four labels.
Note that in the case ofsilence, we give an ?opt?
to the empty hypothesis.6 The Baseline and Oracle SystemsThe baseline for our experiments is the behaviorof the Town-Info spoken dialogue system that wasused to collect the experimental data.
We evaluatethe performance of the baseline system by analyz-ing the dialogue logs from the user study.As an oracle for the system we defined thechoice of either the first ?opt?
in the n-best list,or if this does not exist the first ?pos?
in the list.In this way it is guaranteed that we always get asoutput a perfect match to the true transcript as faras its Dialogue Move is concerned, provided thereexists a perfect match somewhere in the list.6.1 Baseline and Oracle ResultsTable 2 summarizes the evaluation of the baselineand oracle systems.
We note that the Baseline sys-tem already performs quite well on this data, whenwe consider that in about 20% of n-best lists thereis no semantically correct hypothesis.Baseline OracleWER 47.72% 42.16%DMA 75.05% 80.20%SA 40.48% 45.27%Table 2: Baseline and Oracle results (statisticallysignificant at p < 0.001)7 Classifying and Selecting N-bestRecognition HypothesesWe use a threshold (50%) on a hypothesis?
WERas an indicator for whether hypotheses should beclarified or rejected.
This is adopted from (Gabs-dil, 2003), based on the fact that WER correlateswith concept accuracy (CA, (Boros et al, 1996)).7.1 Classification: Feature GroupsWe represent recognition hypotheses as 13-dimensional feature vectors for automatic classi-fication.
The feature vectors combine recognizerconfidence scores, low-level acoustic information,and information from the User Simulation.All the features used by the system are extractedby the dialogue logs, the n-best lists per utteranceand per word and the audio files.
The majorityof the features chosen are based on their successin previous systems as described in the literature(see section 2).
The novel feature here is the UserSimulation score which may make redundant mostof the dialogue features used in other studies.In order to measure the usefulness of each can-didate feature and thus choose the most importantwe use the metrics of Information Gain and GainRatio (see table 3 in section 8.1) on the wholetraining set, i.e.
93240 hypotheses.In total 13 attributes were extracted, that can begrouped into 4 main categories; those that concernthe current hypothesis to be classified, those thatconcern low-level statistics of the audio files, thosethat concern the whole n-best list, and finally theUser Simulation feature.?
Current Hypothesis Features (CHF) (6):acoustic score, overall model confidencescore, minimum word confidence score,grammar parsability, hypothesis length andhypothesis duration.?
Acoustic Features (AF) (3): minimum, max-imum and RMS amplitude?
List Features (LF) (3): n-best rank, deviationof confidence scores in the list, match withmost frequent Dialogue Move?
User Simulation (US) (1): User Simulationconfidence scoreThe Current Hypothesis features (CHF) wereextracted from the n-best list files that containedthe hypotheses?
transcription along with overallacoustic score per utterance and from the equiv-alent files that contained the transcription of eachword along with the start of frame, end of frameand confidence score:508Acoustic score is the negative log likelihood as-cribed by the speech recogniser to the whole hy-pothesis, being the sum of the individual wordacoustic scores.
Intuitively this is considered tobe helpful since it depicts the confidence of thestatistical model only for each word and is alsoadopted in previous studies.
Incorrect alignmentsshall tend to adapt less well to the model and thushave low log likelihood.Overall model confidence score is the averageof the individual word confidence scores.Minimum word confidence score is also com-puted by the individual word transcriptions and ac-counts for the confidence score of the word whichthe speech recogniser is least certain of.
It is ex-pected to help our classifier distinguish betweenpoor overall hypothesis recognitions since a highoverall confidence score can sometimes be mis-leading.Grammar Parsability is the negative loglikelihood of the transcript for the current hy-pothesis as produced by the Stanford Parser, awide-coverage Probabilistic Context-Free Gram-mar (PCFG) (Klein and Manning, 2003) 1.
Thisfeature seems helpful since we expect that a highlyungrammatical hypothesis is likely not to matchwith the true transcription semantically.Hypothesis duration is the length of the hy-pothesis in milliseconds as extracted from the n-best list files with transcriptions per word that in-clude the start and the end time of the recognisedframe.
The reason for the inclusion of this fea-ture is that it can help distinguish between shortutterances such as yes/no answers, medium-sizedutterances of normal answers and long utterancescaused by crosstalk.Hypothesis length is the number of words in ahypothesis and is considered to help in a similarway as the above feature.The Acoustic Features (AF) were extracted di-rectly from the wave files using SoX: Minimum,maximum and RMS amplitude are straightforwardfeatures common in the previous studies men-tioned in section 2.The List Features (LF) were calculated basedon the n-best list files with transcriptions per utter-ance and per word and take into account the wholelist:N-best rank is the position of the hypothesis inthe list and could be useful in the sense that ?opt?1http://nlp.stanford.edu/software/lex-parser.shtmland ?pos?
may be found in the upper part of the listrather than the bottom.Deviation of confidence scores in the list isthe deviation of the overall model confidence scoreof the hypothesis from the mean confidence scorein the list.
This feature is extracted in the hopethat it will indicate potential clusters of confidencescores in particular positions in the list, i.e.
grouphypotheses that deviate in a specific fashion fromthe mean and thus indicating them being classifiedwith the same label.Match with most frequent Dialogue Move isthe only boolean feature and indicates whether theDialogue Move of the current hypothesis, i.e.
thepair of < SpeechAct >< Task > coincides withthe most frequent one.
The trend in n-best listsis to have a majority of utterances that belong toone or two labels and only one hypothesis belong-ing to the ?opt?
category and/or a few to the ?pos?category.
As a result, the idea behind this featureis to extract such potential outliers which are thedesired goal for the re-ranker.Finally, the User Simulation score is given asan output from the User Simulation model andadapted for the purposes of this study (see section3 for more details).
The model is operating with 5-grams.
Its input is given by two different sources:the history of the dialogue, namely the 4 previousDialogue Moves, is taken from the dialogue logand the current hypothesis?
semantic parse whichis generated on the fly by the same keyword parserused in the automatic labelling.User Simulation score is the probability thatthe current hypothesis?
Dialogue Move has reallybeen said by the user given the 4 previous Dia-logue Moves.
The potential advantages of this fea-ture have been discussed in section 3.7.2 Learner and Selection ProcedureWe use the memory based learner TiMBL (Daele-mans et al, 2002) to predict the class of each ofthe 60-best recognition hypotheses for a given ut-terance.TiMBL was trained using different parametercombinations mainly choosing between number ofk-nearest neighbours (1 to 5) and distance metrics(Weighted Overlap andModified Value DifferenceMetric).
In a second step, we decide which (if any)of the classified hypotheses we actually want topick as the best result and how the user utteranceshould be classified as a whole.5091.
Scan the list of classified n-best recognitionhypotheses top-down.
Return the first resultthat is classified as ?opt?.2.
If 1. fails, scan the list of classified n-bestrecognition hypotheses top-down.
Return thefirst result that is classified as ?pos?.3.
If 2. fails, count the number of negs and ignsin the classified recognition hypotheses.
Ifthe number of negs is larger or equal than thenumber of igns then return the first ?neg?.4.
Else return the first ?ign?
utterance.8 ExperimentsExperiments were conducted in two layers: thefirst layer concerns only the classifier, i.e.
the abil-ity of the system to correctly classify each hypoth-esis to either of the four labels ?opt?, ?pos?, ?neg?,?ign?
and the second layer the re-ranker, i.e.
theability of the system to boost the speech recog-niser?s accuracy.All results are drawn from the TiMBL classi-fier trained with the Weighted Overlap metric andk = 1 nearest neighbours settings.
Both layersare trained on 75% of the same Town-Info Corpusof 126 dialogues containing 60-best lists for 1554user utterances or a total of 93240 hypotheses.
Thefirst layer was tested against a separate Town-InfoCorpus of 58 dialogues containing 510 user utter-ances or a total of 30600 hypotheses, while thesecond was tested on the whole training set with10-fold cross-validation.Using this corpus, a series of experiments wascarried out using different sets of features in orderto both determine and illustrate the increasing per-formance of the classifier.
These sets were deter-mined not only by the literature but also by the In-formation Gain measures that were calculated onthe training set using WEKA, as shown in table 3.8.1 Information GainQuite surprisingly, we note that the rank given bythe Information Gain measure coincides perfectlywith the logical grouping of the attributes that wasinitially performed (see table 3).As a result, we chose to use this grouping forthe final 4 feature sets on which the classifierexperiments were performed, in the followingorder:Experiment 1: List Features (LF)InfoGain Attribute1.0324 userSimulationScore0.9038 rmsAmp0.8280 minAmp0.8087 maxAmp0.4861 parsability0.3975 acousScore0.3773 hypothesisDuration0.2545 hypothesisLength0.1627 avgConfScore0.1085 minWordConfidence0.0511 nBestRank0.0447 standardDeviation0.0408 matchesFrequentDMTable 3: Information GainExperiment 2: List Features + Current Hypothe-sis Features (LF+CHF)Experiment 3: List Features + Current Hypothe-sis Features + Acoustic Features (LF+CHF+AF)Experiment 4: List Features + Current Hy-pothesis Features + Acoustic Features + UserSimulation (LF+CHF+AF+US)Note that the User Simulation score is a verystrong feature, scoring first in the InformationGain rank, validating our central hypothesis.The testing of the classifier using each of theabove feature sets was performed on the remain-ing 25% of the Town-Info corpus comprising of 58dialogues, consisting of 510 utterances and takingthe 60-best lists resulting in a total of 30600 vec-tors.
In each experiment we measured Precision,Recall, F-measure per class and total Accuracy ofthe classifier .For the second layer, we used a trained instanceof the TiMBL classifier on the 4th feature set (ListFeatures + Current Hypothesis Features + Acous-tic Features + User Simulation) and performed re-ranking using the algorithm presented in section7.2 on the same training set used in the first layerusing 10-fold cross validation.9 Results and EvaluationWe performed two series of experiments in twolayers: the first corresponds to the training of theclassifier alone and the second to the system as awhole measuring the re-ranker?s output.510Feature set (opt) Precision Recall F1LF 42.5% 58.4% 49.2%LF+CHF 62.4% 65.7% 64.0%LF+CHF+AF 55.6% 61.6% 58.4%LF+CHF+AF+US 70.5% 73.7% 72.1%Table 4: Results for the ?opt?
categoryFeature set (pos) Precision Recall F1LF 25.2% 1.7% 3.2%LF+CHF 51.2% 57.4% 54.1%LF+CHF+AF 51.5% 54.6% 53.0%LF+CHF+AF+US 64.8% 61.8% 63.3%Table 5: Results for the ?pos?
category9.1 First Layer: Classifier ExperimentsIn these series of experiments we measure preci-sion, recall and F1-measure for each of the fourlabels and overall F1-measure and accuracy of theclassifier.
In order to have a better view of theclassifier?s performance we have also included theconfusion matrix for the final experiment with all13 attributes.
Tables 4 -7 show per class and perattribute set measures, while Table 8 shows a col-lective view of the results for the four sets of at-tributes and the baseline being the majority classlabel ?neg?.
Table 9 shows the confusion matrixfor the final experiment.In tables 4 - 8 we generally notice an increasein precision, recall and F1-measure as we pro-gressively add more attributes to the system withthe exception of the addition of the Acoustic Fea-tures which seem to impair the classifier?s perfor-mance.
We also make note of the fact that in thecase of the 4th attribute set the classifier can dis-tinguish very well the ?neg?
and ?ign?
categorieswith 86.3% and 99.9% F1-measure respectively.Most importantly, we observe a remarkable boostin F1-measure and accuracy with the addition ofthe User Simulation score.
We find a 37.36% rel-ative increase in F1-measure and 34.02% increaseFeature set (neg) Precision Recall F1LF 54.2% 96.4% 69.4%LF+CHF 70.7% 75.0% 72.8%LF+CHF+AF 69.5% 73.4% 71.4%LF+CHF+AF+US 85.6% 87.0% 86.3%Table 6: Results for the ?neg?
categoryFeature set (ign) Precision Recall F1LF 19.6% 1.3% 2.5%LF+CHF 63.5% 48.7% 55.2%LF+CHF+AF 59.3% 48.9% 53.6%LF+CHF+AF+US 99.9% 99.9% 99.9%Table 7: Results for the ?ign?
categoryFeature set F1 AccuracyBaseline - 51.1%LF 37.3% 53.1%LF+CHF 64.1% 64.8%LF+CHF+AF 62.6% 63.4%LF+CHF+AF+US 86.0% 84.9%Table 8: F1-Measure and Accuracy for the fourattribute setsin the accuracy compared to the 3rd experiment,which contains all but the User Simulation scoreattribute and a 66.20% relative increase of the ac-curacy compared to the Baseline.
In table 7 wemake note of a rather low recall measure for the?ign?
category in the case of the LF experiment,suggesting that the list features do not add extravalue to the classifier, partially validating the In-formation Gain measure (Table 3).Taking a closer look at the 4th experiment withall 13 features we notice in table 9 that most er-rors occur between the ?pos?
and ?neg?
category.In fact, for the ?neg?
category the False PositiveRate (FPR) is 18.17% and for the ?pos?
8.9%, allin all a lot larger than for the other categories.9.2 Second Layer: Re-ranker ExperimentsIn these experiments we measure WER, DMAand SA for the system as a whole.
In order tomake sure that the improvement noted was re-ally attributed to the classifier we computed thep-values for each of these measures using theWilcoxon signed rank test for WER andMcNemarchi-square test for the DMA and SA measures.In table 10 we note that the classifier scoresopt pos neg ignopt 232 37 46 0pos 47 4405 2682 8neg 45 2045 13498 0ign 5 0 0 7550Table 9: Confusion Matrix for LF+CHF+AF+US511Baseline Classifier OracleWER 47.72% 45.27% ** 42.16%***DMA 75.05% 78.22% * 80.20% ***SA 40.48% 42.26% 45.27%***Table 10: Baseline, Classifier, and Oracle results(*** = p < 0.001, ** = p < 0.01, * = p < 0.05)Label Precision Recall F1opt 74.0% 64.1% 68.7%pos 76.3% 46.2% 57.6%neg 81.9% 94.4% 87.7%ign 99.9% 99.9% 99.9%Table 11: Precision, Recall and F1: high-level fea-tures45.27% WER making a notable relative reductionof 5.13% compared to the baseline and 78.22%DMA incurring a relative improvement of 4.22%.The classifier scored 42.26% on SA but it wasnot considered significant compared to the base-line (0.05 < p < 0.10).
Comparing the classifier?sperformance with the Oracle it achieves a 44.06%of the possible WER improvement on this data,61.55% for the DMA measure and 37.16% for theSA measure.Finally, we also notice that the Oracle has a80.20% for the DMA, which means that 19.80%of the n-best lists did not include at all a hypothe-sis that matched semantically to the true transcript.10 Experiment with high-level featuresWe trained a Memory Based Classifier based onlyon the higher level features of merely the UserSimulation score and the Grammar Parsability(US + GP).
The idea behind this choice is to tryand find a combination of features that ignores lowlevel characteristics of the user?s utterances as wellas features that heavily rely on the speech recog-niser and thus by default are not considered to bevery trustworthy.Quite surprisingly, the results taken from an ex-periment with just the User Simulation score andthe Grammar Parsability are very promising andcomparable with those acquired from the 4th ex-periment with all 13 attributes.
Table 11 showsthe precision, recall and F1-measure per label andtable 12 illustrates the classifier?s performance incomparison with the 4th experiment.Table 12 shows that there is a somewhat consid-Feature set F1 Accuracy TiesLF+CHF+AF+US 86.0% 84.9% 4993US+GP 85.7% 85.6% 115Table 12: F1, Accuracy and number of ties cor-rectly resolved for LF+CHF+AF+US and US+GPfeature setserable decrease in the recall and a correspondingincrease in the precision of the ?pos?
and ?opt?
cat-egories compared to the LF + CHF + AF + US at-tribute set, which account for lower F1-measures.However, all in all the US + GP set manages toclassify correctly 207 more vectors and quite in-terestingly commits far fewer ties and manages toresolve more compared to the full 13 attribute set.11 ConclusionWe used a combination of acoustic features andfeatures computed from dialogue context to pre-dict the quality of incoming recognition hypothe-ses to an SDS.
In particular we use a score com-puted from a simple statistical User Simulation,which measures the likelihood that the user re-ally said each hypothesis.
The approach is novelin combining User Simulations, machine learning,and n-best processing for spoken dialogue sys-tems.
We employed a User Simulation model,trained on real dialogue data, to predict the user?snext dialogue move.
This prediction was used tore-rank n-best hypotheses of a speech recognizerfor a corpus of 2564 user utterances.
The results,obtained using TiMBL and an n-gram User Sim-ulation, show a significant relative reduction ofWord Error Rate of 5% (this is 44% of the pos-sible WER improvement on this data), and 62%of the possible Dialogue Move Accuracy improve-ment, compared to the baseline policy of selectingthe topmost ASR hypothesis.
The majority of theimprovement is attributable to the User Simulationfeature.
Clearly, this improvement would result inbetter dialogue system performance overall.AcknowledgmentsWe thank Helen Hastie and Kallirroi Georgila.The research leading to these results has re-ceived funding from the EPSRC (project no.EP/E019501/1) and from the European Commu-nity?s Seventh Framework Programme (FP7/2007-2013) under grant agreement no.
216594 (CLAS-SiC project www.classic-project.org)512ReferencesM.
Boros, W. Eckert, F. Gallwitz, G. Go?rz, G. Han-rieder, and H. Niemann.
1996.
Towards understand-ing spontaneous speech: Word accuracy vs. conceptaccuracy.
In Proceedings ICSLP ?96, volume 2,pages 1009?1012, Philadelphia, PA.Ananlada Chotimongkol and Alexander I. Rudnicky.2001.
N-best Speech Hypotheses Reordering UsingLinear Regression.
In Proceedings of EuroSpeech2001, pages 1829?1832.Walter Daelemans, Jakub Zavrel, Ko van der Sloot, andAntal van den Bosch.
2002.
TIMBL: Tilburg Mem-ory Based Learner, version 4.2, Reference Guide.
InILK Technical Report 02-01.Malte Gabsdil and Oliver Lemon.
2004.
Combiningacoustic and pragmatic features to predict recogni-tion performance in spoken dialogue systems.
InProceedings of ACL-04, pages 344?351.Malte Gabsdil.
2003.
Classifying Recognition Resultsfor Spoken Dialogue Systems.
In Proceedings of theStudent Research Workshop at ACL-03.Kallirroi Georgila, James Henderson, and OliverLemon.
2006.
User simulation for spoken dialoguesystems: Learning and evaluation.
In Proceedingsof Interspeech/ICSLP, pages 1065?1068.R.
Jonson.
2006.
Dialogue Context-Based Re-rankingof ASR Hypotheses.
In Proceedings IEEE 2006Workshop on Spoken Language Technology.D.
Klein and C. Manning.
2003.
Fast exact inferencewith a factored model for natural language parsing.Journal of Advances in Neural Information Process-ing Systems, 15(2).Diane J. Litman, Julia Hirschberg, and Marc Swerts.2000.
Predicting Automatic Speech RecognitionPerformance Using Prosodic Cues.
In Proceedingsof NAACL.M.
Pickering and S. Garrod.
2007.
Do people use lan-guage production to make predictions during com-prehension?
Journal of Trends in Cognitive Sci-ences, 11(3).J Schatzmann, K Weilhammer, M N Stuttle, and S JYoung.
2006.
A Survey of Statistical User Sim-ulation Techniques for Reinforcement-Learning ofDialogue Management Strategies.
Knowledge En-gineering Review, 21:97?126.J.
Schatzmann, B. Thomson, K. Weilhammer, H. Ye,and S. Young.
2007.
Agenda-based User Simula-tion for Bootstrapping a POMDP Dialogue System.In Proceedings of HLT/NAACL.David Traum, Johan Bos, Robin Cooper, Staffan Lars-son, Ian Lewin, Colin Matheson, and Massimo Poe-sio.
1999.
A Model of Dialogue Moves and In-formation State Revision.
Technical Report D2.1,Trindi Project.Marilyn Walker, Jerry Wright, and Irene Langkilde.2000.
Using Natural Language Processing and Dis-course Features to Identify Understanding Errorsin a Spoken Dialogue System.
In Proceedings ofICML-2000.Jason Williams and Steve Young.
2003.
Using wizard-of-oz simulations to bootstrap reinforcement-learning-based dialog management systems.
InProc.
4th SIGdial workshop.SJ Young, J Schatzmann, K Weilhammer, and H Ye.2007.
The Hidden Information State Approach toDialog Management.
In ICASSP 2007.SJ Young.
2006.
Using POMDPs for Dialog Manage-ment.
In IEEE/ACL Workshop on Spoken LanguageTechnology (SLT 2006), Aruba.Steve Young.
2007.
ATK: An Application Toolkitfor HTK, Version 1.6.
Technical report, CambridgeUniversity Engineering Department.513
