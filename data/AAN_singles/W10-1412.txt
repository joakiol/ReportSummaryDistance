Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 103?107,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsEasy First Dependency Parsing of Modern HebrewYoav Goldberg?
and Michael ElhadadBen Gurion University of the NegevDepartment of Computer SciencePOB 653 Be?er Sheva, 84105, Israel{yoavg|elhadad}@cs.bgu.ac.ilAbstractWe investigate the performance of an easy-first, non-directional dependency parser on theHebrew Dependency treebank.
We show thatwith a basic feature set the greedy parser?s ac-curacy is on a par with that of a first-orderglobally optimized MST parser.
The additionof morphological-agreement feature improvesthe parsing accuracy, making it on-par with asecond-order globally optimized MST parser.The improvement due to the morphologicalagreement information is persistent both whengold-standard and automatically-induced mor-phological information is used.1 IntroductionData-driven Dependency Parsing algorithms arebroadly categorized into two approaches (Ku?bler etal., 2009).
Transition based parsers traverse thesentence from left to right1 using greedy, local in-ference.
Graph based parsers use global inferenceand seek a tree structure maximizing some scoringfunction defined over trees.
This scoring functionis usually decomposed over tree edges, or pairs ofsuch edges.
In recent work (Goldberg and Elhadad,2010), we proposed another dependency parsing ap-proach: Easy First, Non-Directional dependency?Supported by the Lynn and William Frankel Center forComputer Sciences, Ben Gurion University1Strictly speaking, the traversal order is from start to end.This distinction is important when discussing Hebrew parsing,as the Hebrew language is written from right-to-left.
We keepthe left-to-right terminology throughout this paper, as this is thecommon terminology.
However, ?left?
and ?right?
should beinterpreted as ?start?
and ?end?
respectively.
Similarly, ?a tokento the left?
should be interpreted as ?the previous token?.parsing.
Like transition based methods, the easy-first method adopts a local, greedy policy.
How-ever, it abandons the strict left-to-right processingorder, replacing it with an alternative order, whichattempts to make easier attachments decisions priorto harder ones.
The model was applied to Englishdependency parsing.
It was shown to be more accu-rate than MALTPARSER, a state-of-the-art transitionbased parser (Nivre et al, 2006), and near the perfor-mance of the first-order MSTPARSER, a graph basedparser which decomposes its score over tree edges(McDonald et al, 2005), while being more efficient.The easy-first parser works by making easier de-cisions before harder ones.
Each decision can beconditioned by structures created by previous deci-sions, allowing harder decisions to be based on rel-atively rich syntactic structure.
This is in contrast tothe globally optimized parsers, which cannot utilizesuch rich syntactic structures.
It was hypothesizedin (Goldberg and Elhadad, 2010) that this rich con-ditioning can be especially beneficial in situationswhere informative structural information is avail-able, such as in morphologically rich languages.In this paper, we investigate the non-directionaleasy-first parser performance on Modern Hebrew, asemitic language with rich morphology, relativelyfree constituent order, and a small treebank com-pared to English.
We are interested in two mainquestions: (a) how well does the non-directionalparser perform on Hebrew data?
and (b) can theparser make effective use of morphological features,such as agreement?In (Goldberg and Elhadad, 2009), we describea newly created Hebrew dependency treebank, and103report results on parsing this corpus with bothMALTPARSER and first- and second- order vari-ants of MSTPARSER.
We find that the second-order MSTPARSER outperforms the first order MST-PARSER, which in turn outperforms the transitionbased MALTPARSER.
In addition, adding mor-phological information to the default configurationsof these parsers does not improve parsing accu-racy.
Interestingly, when using automatically in-duced (rather than gold-standard) morphological in-formation, the transition based MALTPARSER?s ac-curacy improves with the addition of the morpho-logical information, while the scores of both glob-ally optimized parsers drop with the addition of themorphological information.Our experiments in this paper show that the ac-curacy of the non-directional parser on the samedataset outperforms the first-order MSTPARSER.With the addition of morphological agreement fea-tures, the parser accuracy improves even further, andis on-par with the performance of the second-orderMSTPARSER.
The improvement due to the morpho-logical information persists also when automaticallyinduced morphological information is used.2 Modern HebrewSome aspects that make Hebrew challenging from alanguage-processing perspective are:Affixation Common prepositions, conjunctionsand articles are prefixed to the following word, andpronominal elements often appear as suffixes.
Thesegmentation of prefixes and suffixes is often am-biguous and must be determined in a specific contextonly.
In term of dependency parsing, this means thatthe dependency relations occur not between space-delimited tokens, but instead between sub-token el-ements which we?ll refer to as segments.
Further-more, mistakes in the underlying token segmenta-tions are sure to be reflected in the parsing accuracy.Relatively free constituent order The ordering ofconstituents inside a phrase is relatively free.
Thisis most notably apparent in the verbal phrases andsentential levels.
In particular, while most sentencesfollow an SVO order, OVS and VSO configurationsare also possible.
Verbal arguments can appear be-fore or after the verb, and in many ordering.
Forexample, the message ?went from Israel to Thai-land?
can be expressed as ?went to Thailand fromIsrael?, ?to Thailand went from Israel?, ?from Israelwent to Thailand?, ?from Israel to Thailand went?and ?to Thailand from Israel went?.
This results inlong and flat VP and S structures and a fair amountof sparsity, which suggests that a dependency repre-sentations might be more suitable to Hebrew than aconstituency one.NP Structure and Construct State While con-stituents order may vary, NP internal structure isrigid.
A special morphological marker (ConstructState) is used to mark noun compounds as well assimilar phenomena.
This marker, while ambiguous,is essential for analyzing NP internal structure.Case Marking definite direct objects are marked.The case marker in this case is the function word z`appearing before the direct object.2Rich templatic morphology Hebrew has a veryproductive morphological structure, which is basedon a root+template system.
The productive mor-phology results in many distinct word forms anda high out-of-vocabulary rate which makes it hardto reliably estimate lexical parameters from anno-tated corpora.
The root+template system (combinedwith the unvocalized writing system) makes it hardto guess the morphological analyses of an unknownword based on its prefix and suffix, as usually donein other languages.Unvocalized writing system Most vowels are notmarked in everyday Hebrew text, which results in avery high level of lexical and morphological ambi-guity.
Some tokens can admit as many as 15 distinctreadings, and the average number of possible mor-phological analyses per token in Hebrew text is 2.7,compared to 1.4 in English (Adler, 2007).Agreement Hebrew grammar forces morpholog-ical agreement between Adjectives and Nouns(which should agree in Gender and Number and def-initeness), and between Subjects and Verbs (whichshould agree in Gender and Number).2The orthographic form z` is ambiguous.
It can also standfor the noun ?shovel?
and the pronoun ?you?
(2nd,fem,sing).1043 Easy First Non Directional ParsingEasy-First Non Directional parsing is a greedysearch procedure.
It works with a list of partialstructures, pi, .
.
.
, pk, which is initialized with then words of the sentence.
Each structure is a headtoken which is not yet assigned a parent, but mayhave dependants attached to it.
At each stage of theparsing algorithm, two neighbouring partial struc-tures, (pi, pi+1) are chosen, and one of them be-comes the parent of the other.
The new dependant isthen removed from the list of partial structures.
Pars-ing proceeds until only one partial structure, corre-sponding to the root of the sentence, is left.
Thechoice of which neighbouring structures to attach isbased on a scoring function.
This scoring functionis learned from data, and attempts to attach moreconfident attachments before less confident ones.The scoring function makes use of features.
Thesefeatures can be extracted from any pre-built struc-tures.
In practice, the features are defined on pre-built structures which are around the proposed at-tachment point.
For complete details about training,features and implementation, refer to (Goldberg andElhadad, 2010).4 ExperimentsWe follow the setup of (Goldberg and Elhadad,2009).Data We use the Hebrew dependency treebank de-scribed in (Goldberg and Elhadad, 2009).
We useSections 2-12 (sentences 484-5724) as our trainingset, and report results on parsing the developmentset, Section 1 (sentences 0-483).
As in (Goldbergand Elhadad, 2009), we do not evaluate on the testset in this work.The data in the treebank is segmented and POS-tagged.
Both the parsing models were trained onthe gold-standard segmented and tagged data.
Whenevaluating the parsing models, we perform two setsof evaluations.
The first one is an oracle experi-ment, assuming gold segmentation and tagging isavailable.
The second one is a real-world experi-ment, in which we segment and POS-tag the test-set sentences using the morphological disambigua-tor described in (Adler, 2007; Goldberg et al, 2008)prior to parsing.Parsers and parsing models We use our freelyavailable implementation3 of the non-directionalparser.Evaluation Measure We evaluate the resultingparses in terms of unlabeled accuracy ?
the percentof correctly identified (child,parent) pairs4.
To beprecise, we calculate:number of correctly identified pairsnumber of pairs in gold parseFor the oracle case in which the gold-standard to-ken segmentation is available for the parser, this isthe same as the traditional unlabeled-accuracy eval-uation metric.
However, in the real-word setting inwhich the token segmentation is done automatically,the yields of the gold-standard and the automaticparse may differ, and one needs to decide how tohandle the cases in which one or more elements inthe identified (child,parent) pair are not present inthe gold-standard parse.
Our evaluation metric pe-nalizes these cases by regarding them as mistakes.5 ResultsBase Feature Set On the first set of experiments,we used the English feature set which was used in(Goldberg and Elhadad, 2010).
Our only modifica-tion to the feature set for Hebrew was not to lex-icalize prepositions (we found it to work somewhatbetter due to the smaller treebank size, and Hebrew?srather productive preposition system).Results of parsing the development set are sum-marized in Table 1.
For comparison, we list the per-formance of the MALT and MST parsers on the samedata, as reported in (Goldberg and Elhadad, 2009).The case marker z`, as well as the morpholog-ically marked construct nouns, are covered by allfeature models.
z` is a distinct lexical element in apredictable position, and all four parsers utilize suchfunction word information.
Construct nouns are dif-ferentiated from non-construct nouns already at thePOS tagset level.All models suffer from the absence of goldPOS/morphological information.
The easy-firstnon-directional parser with the basic feature set3http://www.cs.bgu.ac.il/?yoavg/software/nondirparser/4All the results are macro averaged.105(NONDIR) outperforms the transition based MALT-PARSER in all cases.
It also outperforms the first or-der MST1 model when gold POS/morphology infor-mation is available, and has nearly identical perfor-mance to MST1 when automatically induced POS/-morphology information is used.Additional Morphology Features Error inspec-tion reveals that most errors are semantic in nature,and involve coordination, PP-attachment or main-verb hierarchy.
However, some small class of er-rors reflected morphological disagreement betweennouns and adjectives.
These errors were either in-side a simple NP, or, in some cases, could affect rel-ative clause attachments.
We were thus motivated toadd specific features encoding morphological agree-ment to try and avoid this class of errors.Our features are targeted specifically at capturingnoun-adjective morphological agreement.5 Whenattempting to score the attachment of two neigh-bouring structures in the list, pi and pi+1, we in-spect the pairs (pi, pi+1), (pi, pi+2), (pi?1, pi+1),(pi?2, pi), (pi+1, pi+2).
For each such pair, in caseit is made of a noun and an adjective, we add twofeatures: a binary feature indicating presence or ab-sence of gender agreement, and another binary fea-ture for number agreement.The last row in Table 1 (NONDIR+MORPH)presents the parser accuracy with the addition ofthese agreement features.
Agreement contributesto the accuracy of the parser, making it as accu-rate as the second-order MST2.
Interestingly, thenon-directional model benefits from the agreementfeatures also when automatically induced POS/mor-phology information is used (going from 75.5% to76.2%).
This is in contrast to the MST parsers,where the morphological features hurt the parserwhen non-gold morphology is used (75.6 to 73.9for MST1 and 76.4 to 74.6 for MST2).
This canbe attributed to either the agreement specific na-ture of the morphological features added to the non-directional parser, or to the easy-first order of thenon-directional parser, and to the fact the morpho-logical features are defined only over structurallyclose heads at each stage of the parsing process.5This is in contrast to the morphological features used inout-of-the-box MST and MALT parsers, which are much moregeneral.Gold Morph/POS Auto Morph/POSMALT 80.3 72.9MALT+MORPH 80.7 73.4MST1 83.6 75.6MST1+MORPH 83.6 73.9MST2 84.3 76.4MST2+MORPH 84.4 74.6NONDIR 83.8 75.5NONDIR+MORPH 84.2 76.2Table 1: Unlabeled dependency accuracy of the variousparsing models.6 DiscussionWe have verified that easy-first, non-directional de-pendency parsing methodology of (Goldberg and El-hadad, 2010) is successful for parsing Hebrew, asemitic language with rich morphology and a smalltreebank.
We further verified that the model canmake effective use of morphological agreement fea-tures, both when gold-standard and automatically in-duced morphological information is provided.
Withthe addition of the morphological agreement fea-tures, the non-directional model is as effective as asecond-order globally optimized MST model, whilebeing much more efficient, and easier to extend withadditional structural features.While we get adequate parsing results for Hebrewwhen gold-standard POS/morphology/segmentationinformation is used, the parsing performance inthe realistic setting, in which gold POS/morpholo-gy/segmentation information is not available, is stilllow.
We strongly believe that parsing and morpho-logical disambiguation should be done jointly, orat least interact with each other.
This is the mainfuture direction for dependency parsing of ModernHebrew.ReferencesMeni Adler.
2007.
Hebrew Morphological Disambigua-tion: An Unsupervised Stochastic Word-based Ap-proach.
Ph.D. thesis, Ben-Gurion University of theNegev, Beer-Sheva, Israel.Yoav Goldberg and Michael Elhadad.
2009.
Hebrew De-pendency Parsing: Initial Results.
In Proc.
of IWPT.Yoav Goldberg and Michael Elhadad.
2010.
An effi-cient algorithm for easy-first non-directional depen-dency parsing.
In Proc.
of NAACL.Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008.106EM Can find pretty good HMM POS-Taggers (whengiven a good start).
In Proc.
of ACL.Sandra Ku?bler, Ryan T. McDonald, and Joakim Nivre.2009.
Dependency Parsing.
Synthesis Lectures onHuman Language Technologies.
Morgan & ClaypoolPublishers.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005.
Online large-margin training of dependencyparsers.
In Proc of ACL.Joakim Nivre, Johan Hall, and Jens Nillson.
2006.
Malt-Parser: A data-driven parser-generator for dependencyparsing.
In Proc.
of LREC.107
