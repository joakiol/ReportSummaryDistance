Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1365?1374,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsDiscovering Sociolinguistic Associations with Structured SparsityJacob Eisenstein Noah A. Smith Eric P. XingSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USA{jacobeis,nasmith,epxing}@cs.cmu.eduAbstractWe present a method to discover robust andinterpretable sociolinguistic associations fromraw geotagged text data.
Using aggregate de-mographic statistics about the authors?
geo-graphic communities, we solve a multi-outputregression problem between demographicsand lexical frequencies.
By imposing a com-posite `1,?
regularizer, we obtain structuredsparsity, driving entire rows of coefficientsto zero.
We perform two regression studies.First, we use term frequencies to predict de-mographic attributes; our method identifies acompact set of words that are strongly asso-ciated with author demographics.
Next, weconjoin demographic attributes into features,which we use to predict term frequencies.
Thecomposite regularizer identifies a small num-ber of features, which correspond to com-munities of authors united by shared demo-graphic and linguistic properties.1 IntroductionHow is language influenced by the speaker?s so-ciocultural identity?
Quantitative sociolinguisticsusually addresses this question through carefullycrafted studies that correlate individual demographicattributes and linguistic variables?for example, theinteraction between income and the ?dropped r?
fea-ture of the New York accent (Labov, 1966).
Butsuch studies require the knowledge to select the?dropped r?
and the speaker?s income, from thou-sands of other possibilities.
In this paper, we presenta method to acquire such patterns from raw data.
Us-ing multi-output regression with structured sparsity,our method identifies a small subset of lexical itemsthat are most influenced by demographics, and dis-covers conjunctions of demographic attributes thatare especially salient for lexical variation.Sociolinguistic associations are difficult to model,because the space of potentially relevant interactionsis large and complex.
On the linguistic side thereare thousands of possible variables, even if we limitourselves to unigram lexical features.
On the demo-graphic side, the interaction between demographicattributes is often non-linear: for example, gendermay negate or amplify class-based language differ-ences (Zhang, 2005).
Thus, additive models whichassume that each demographic attribute makes a lin-ear contribution are inadequate.In this paper, we explore the large space of po-tential sociolinguistic associations using structuredsparsity.
We treat the relationship between languageand demographics as a set of multi-input, multi-output regression problems.
The regression coeffi-cients are arranged in a matrix, with rows indicatingpredictors and columns indicating outputs.
We ap-ply a composite regularizer that drives entire rowsof the coefficient matrix to zero, yielding compact,interpretable models that reuse features across dif-ferent outputs.
If we treat the lexical frequenciesas inputs and the author?s demographics as outputs,the induced sparsity pattern reveals the set of lexi-cal items that is most closely tied to demographics.If we treat the demographic attributes as inputs andbuild a model to predict the text, we can incremen-tally construct a conjunctive feature space of demo-graphic attributes, capturing key non-linear interac-tions.1365The primary purpose of this research is ex-ploratory data analysis to identify both the mostlinguistic-salient demographic features, and themost demographically-salient words.
However, thismodel also enables predictions about demographicfeatures by analyzing raw text, potentially support-ing applications in targeted information extractionor advertising.
On the task of predicting demo-graphics from text, we find that our sparse modelyields performance that is statistically indistinguish-able from the full vocabulary, even with a reductionin the model complexity an order of magnitude.
Onthe task of predicting text from author demograph-ics, we find that our incrementally constructed fea-ture set obtains significantly better perplexity than alinear model of demographic attributes.2 DataOur dataset is derived from prior work in whichwe gathered the text and geographical locations of9,250 microbloggers on the website twitter.com (Eisenstein et al, 2010).
Bloggers were se-lected from a pool of frequent posters whose mes-sages include metadata indicating a geographical lo-cation within a bounding box around the continen-tal United States.
We limit the vocabulary to the5,418 terms which are used by at least 40 authors; nostoplists are applied, as the use of standard or non-standard orthography for stopwords (e.g., to vs. 2)may convey important information about the author.The dataset includes messages during the first weekof March 2010.O?Connor et al (2010) obtained aggregate demo-graphic statistics for these data by mapping geoloca-tions to publicly-available data from the U. S. Cen-sus ZIP Code Tabulation Areas (ZCTA).1 Thereare 33,178 such areas in the USA (the 9,250 mi-crobloggers in our dataset occupy 3,458 unique ZC-TAs), and they are designed to contain roughlyequal numbers of inhabitants and demographically-homogeneous populations.
The demographic at-tributes that we consider in this paper are shownin Table 1.
All attributes are based on self-reports.The race and ethnicity attributes are not mutuallyexclusive?individuals can indicate any number ofraces or ethnicities.
The ?other language?
attribute1http://www.census.gov/support/cen2000.htmlmean std.
dev.race & ethnicity% white 52.1 29.0% African American 32.2 29.1% Hispanic 15.7 18.3language% English speakers 73.7 18.4% Spanish speakers 14.6 15.6% other language speakers 11.7 9.2socioeconomic% urban 95.1 14.3% with family 64.1 14.4% renters 48.9 23.4median income ($) 42,500 18,100Table 1: The demographic attributes used in this research.aggregates all languages besides English and Span-ish.
?Urban areas?
refer to sets of census tracts orcensus blocks which contain at least 2,500 residents;our ?% urban?
attribute is the percentage of individ-uals in each ZCTA who are listed as living in an ur-ban area.
We also consider the percentage of indi-viduals who live with their families, the percentagewho live in rented housing, and the median reportedincome in each ZCTA.While geographical aggregate statistics are fre-quently used to proxy for individual socioeconomicstatus in research areas such as public health (e.g.,Rushton, 2008), it is clear that interpretation mustproceed with caution.
Consider an author from a ZIPcode in which 60% of the residents are Hispanic:2we do not know the likelihood that the author is His-panic, because the set of Twitter users is not a rep-resentative sample of the overall population.
Pollingresearch suggests that users of both Twitter (Smithand Rainie, 2010) and geolocation services (Zick-uhr and Smith, 2010) are much more diverse withrespect to age, gender, race and ethnicity than thegeneral population of Internet users.
Nonetheless,at present we can only use aggregate statistics tomake inferences about the geographic communitiesin which our authors live, and not the authors them-selves.2In the U.S. Census, the official ethnonym is Hispanic orLatino; for brevity we will use Hispanic in the rest of this paper.13663 ModelsThe selection of both words and demographic fea-tures can be framed in terms of multi-output regres-sion with structured sparsity.
To select the lexicalindicators that best predict demographics, we con-struct a regression problem in which term frequen-cies are the predictors and demographic attributesare the outputs; to select the demographic featuresthat predict word use, this arrangement is reversed.Through structured sparsity, we learn models inwhich entire sets of coefficients are driven to zero;this tells us which words and demographic featurescan safely be ignored.This section describes the model and implemen-tation for output-regression with structured sparsity;in Section 4 and 5 we give the details of its applica-tion to select terms and demographic features.
For-mally, we consider the linear equationY = XB+,where,?
Y is the dependent variable matrix, with di-mensions N ?
T , where N is the number ofsamples and T is the number of output dimen-sions (or tasks);?
X is the independent variable matrix, with di-mensions N ?
P , where P is the number ofinput dimensions (or predictors);?
B is the matrix of regression coefficients, withdimensions P ?
T ;?
 is a N ?
T matrix in which each element isnoise from a zero-mean Gaussian distribution.We would like to solve the unconstrained opti-mization problem,minimizeB ||Y ?XB||2F + ?R(B), (1)where ||A||2F indicates the squared Frobenius norm?i?j a2ij , and the function R(B) defines a normon the regression coefficients B. Ridge regres-sion applies the `2 norm R(B) =?Tt=1?
?Pp b2pt,and lasso regression applies the `1 norm R(B) =?Tt=1?Pp |bpt|; in both cases, it is possible to de-compose the multi-output regression problem, treat-ing each output dimension separately.
However, ourworking hypothesis is that there will be substantialcorrelations across both the vocabulary and the de-mographic features?for example, a demographicfeature such as the percentage of Spanish speakerswill predict a large set of words.
Our goal is to selecta small set of predictors yielding good performanceacross all output dimensions.
Thus, we desire struc-tured sparsity, in which entire rows of the coefficientmatrix B are driven to zero.Structured sparsity is not achieved by the lasso?s`1 norm.
The lasso gives element-wise sparsity, inwhich many entries ofB are driven to zero, but eachpredictor may have a non-zero value for some outputdimension.
To drive entire rows of B to zero, we re-quire a composite regularizer.
We consider the `1,?norm, which is the sum of `?
norms across outputdimensions: R(B) =?Tt maxp bpt (Turlach et al,2005).
This norm, which corresponds to a multi-output lasso regression, has the desired property ofdriving entire rows of B to zero.3.1 OptimizationThere are several techniques for solving the `1,?normalized regression, including interior pointmethods (Turlach et al, 2005) and projected gradi-ent (Duchi et al, 2008; Quattoni et al, 2009).
Wechoose the blockwise coordinate descent approachof Liu et al (2009) because it is easy to implementand efficient: the time complexity of each iterationis independent of the number of samples.3Due to space limitations, we defer to Liu et al(2009) for a complete description of the algorithm.However, we note two aspects of our implementa-tion which are important for natural language pro-cessing applications.
The algorithm?s efficiency isaccomplished by precomputing the matrices C =X?TY?
and D = X?TX?, where X?
and Y?
are the stan-dardized versions ofX andY, obtained by subtract-ing the mean and scaling by the variance.
Explicitmean correction would destroy the sparse term fre-quency data representation and render us unable tostore the data in memory; however, we can achievethe same effect by computing C = XTY ?N x?Ty?,where x?
and y?
are row vectors indicating the means3Our implementation is available at http://sailing.cs.cmu.edu/sociolinguistic.html.1367ofX andY respectively.4 We can similarly computeD = XTX?N x?Tx?.If the number of predictors is too large, it maynot be possible to store the dense matrix D in mem-ory.
We have found that approximation based on thetruncated singular value decomposition provides aneffective trade-off of time for space.
Specifically, wecompute XTX ?USVT(USVT)T= U(SVTVSTUT)= UM.Lower truncation levels are less accurate, but arefaster and require less space: for K singular val-ues, the storage cost is O(KP ), instead of O(P 2);the time cost increases by a factor of K. This ap-proximation was not necessary in the experimentspresented here, although we have found that it per-forms well as long as the regularizer is not too closeto zero.3.2 RegularizationThe regularization constant ?
can be computed us-ing cross-validation.
As ?
increases, we reuse theprevious solution of B for initialization; this ?warmstart?
trick can greatly accelerate the computationof the overall regularization path (Friedman et al,2010).
At each ?i, we solve the sparse multi-outputregression; the solution Bi defines a sparse set ofpredictors for all tasks.We then use this limited set of predictors to con-struct a new input matrix X?i, which serves as theinput in a standard ridge regression, thus refittingthe model.
The tuning set performance of this re-gression is the score for ?i.
Such post hoc refittingis often used in tandem with the lasso and relatedsparse methods; the effectiveness of this procedurehas been demonstrated in both theory (Wassermanand Roeder, 2009) and practice (Wu et al, 2010).The regularization parameter of the ridge regressionis determined by internal cross-validation.4 Predicting Demographics from TextSparse multi-output regression can be used to selecta subset of vocabulary items that are especially in-dicative of demographic and geographic differences.4Assume without loss of generality that X and Y are scaledto have variance 1, because this scaling does not affect the spar-sity pattern.Starting from the regression problem (1), the predic-tors X are set to the term frequencies, with one col-umn for each word type and one row for each authorin the dataset.
The outputsY are set to the ten demo-graphic attributes described in Table 1 (we considermuch larger demographic feature spaces in the nextsection) The `1,?
regularizer will drive entire rowsof the coefficient matrix B to zero, eliminating alldemographic effects for many words.4.1 Quantitative EvaluationWe evaluate the ability of lexical features to predictthe demographic attributes of their authors (as prox-ied by the census data from the author?s geograph-ical area).
The purpose of this evaluation is to as-sess the predictive ability of the compact subset oflexical items identified by the multi-output lasso, ascompared with the full vocabulary.
In addition, thisevaluation establishes a baseline for performance onthe demographic prediction task.We perform five-fold cross-validation, using themulti-output lasso to identify a sparse feature setin the training data.
We compare against severalother dimensionality reduction techniques, match-ing the number of features obtained by the multi-output lasso at each fold.
First, we compare againsta truncated singular value decomposition, with thetruncation level set to the number of terms selectedby the multi-output lasso; this is similar in spirit tovector-based lexical semantic techniques (Schu?tzeand Pedersen, 1993).
We also compare against sim-ply selecting the N most frequent terms, and the Nterms with the greatest variance in frequency acrossauthors.
Finally, we compare against the completeset of all 5,418 terms.
As before, we perform posthoc refitting on the training data using a standardridge regression.
The regularization constant for theridge regression is identified using nested five-foldcross validation within the training set.We evaluate on the refit models on the heldouttest folds.
The scoring metric is Pearson?s correla-tion coefficient between the predicted and true de-mographics: ?
(y, y?)
= cov(y,y?)?y?y?
, with cov(y, y?)
in-dicating the covariance and ?y indicating the stan-dard deviation.
On this metric, a perfect predictorwill score 1 and a random predictor will score 0.
Wereport the average correlation across all ten demo-1368102 1030.160.180.20.220.240.260.28number of featuresaveragecorrelationmulti?output lassoSVDhighest variancemost frequentFigure 1: Average correlation plotted against the numberof active features (on a logarithmic scale).graphic attributes, as well as the individual correla-tions.Results Table 2 shows the correlations obtainedby regressions performed on a range of different vo-cabularies, averaged across all five folds.
Linguisticfeatures are best at predicting race, ethnicity, lan-guage, and the proportion of renters; the other de-mographic attributes are more difficult to predict.Among feature sets, the highest average correlationis obtained by the full vocabulary, but the multi-output lasso obtains nearly identical performanceusing a feature set that is an order of magnitudesmaller.
Applying the Fischer transformation, wefind that all correlations are statistically significantat p < .001.The Fischer transformation can also be used toestimate 95% confidence intervals around the cor-relations.
The extent of the confidence intervalsvaries slightly across attributes, but all are tighterthan ?0.02.
We find that the multi-output lasso andthe full vocabulary regression are not significantlydifferent on any of the attributes.
Thus, the multi-output lasso achieves a 93% compression of the fea-ture set without a significant decrease in predictiveperformance.
The multi-output lasso yields highercorrelations than the other dimensionality reductiontechniques on all of the attributes; these differencesare statistically significant in many?but not all?cases.
The correlations for each attribute are clearlynot independent, so we do not compare the averageacross attributes.Recall that the regularization coefficient was cho-sen by nested cross-validation within the trainingset; the average number of features selected is394.6.
Figure 1 shows the performance of eachdimensionality-reduction technique across the reg-ularization path for the first of five cross-validationfolds.
Computing the truncated SVD of a sparse ma-trix at very large truncation levels is computationallyexpensive, so we cannot draw the complete perfor-mance curve for this method.
The multi-output lassodominates the alternatives, obtaining a particularlystrong advantage with very small feature sets.
Thisdemonstrates its utility for identifying interpretablemodels which permit qualitative analysis.4.2 Qualitative AnalysisFor a qualitative analysis, we retrain the model onthe full dataset, and tune the regularization to iden-tify a compact set of 69 features.
For each identifiedterm, we apply a significance test on the relationshipbetween the presence of each term and the demo-graphic indicators shown in the columns of the ta-ble.
Specifically, we apply the Wald test for compar-ing the means of independent samples, while mak-ing the Bonferroni correction for multiple compar-isons (Wasserman, 2003).
The use of sparse multi-output regression for variable selection increases thepower of post hoc significance testing, because theBonferroni correction bases the threshold for sta-tistical significance on the total number of compar-isons.
We find 275 associations at the p < .05 level;at the higher threshold required by a Bonferroni cor-rection for comparisons among all terms in the vo-cabulary, 69 of these associations would have beenmissed.Table 3 shows the terms identified by our modelwhich have a significant correlation with at least oneof the demographic indicators.
We divide words inthe list into categories, which order alphabeticallyby the first word in each category: emoticons; stan-dard English, defined as words with Wordnet entries;proper names; abbreviations; non-English words;non-standard words used with English.
The cate-gorization was based on the most frequent sense inan informal analysis of our data.
A glossary of non-standard terms is given in Table 4.Some patterns emerge from Table 3.
StandardEnglish words tend to appear in areas with more1369vocabulary # features averagewhiteAfr.Am.Hisp.Eng.lang.Span.lang.otherlang.urbanfamilyrentermed.inc.full 5418 0.260 0.337 0.318 0.296 0.384 0.296 0.256 0.155 0.113 0.295 0.152multi-output lasso394.60.260 0.326 0.308 0.304 0.383 0.303 0.249 0.153 0.113 0.302 0.156SVD 0.237 0.321 0.299 0.269 0.352 0.272 0.226 0.138 0.081 0.278 0.136highest variance 0.220 0.309 0.287 0.245 0.315 0.248 0.199 0.132 0.085 0.250 0.135most frequent 0.204 0.294 0.264 0.222 0.293 0.229 0.178 0.129 0.073 0.228 0.126Table 2: Correlations between predicted and observed demographic attributes, averaged across cross validation folds.English speakers; predictably, Spanish words tendto appear in areas with Spanish speakers and His-panics.
Emoticons tend to be used in areas withmany Hispanics and few African Americans.
Ab-breviations (e.g., lmaoo) have a nearly uniformdemographic profile, displaying negative correla-tions with whites and English speakers, and posi-tive correlations with African Americans, Hispanics,renters, Spanish speakers, and areas classified as ur-ban.Many non-standard English words (e.g., dats)appear in areas with high proportions of renters,African Americans, and non-English speakers,though a subset (haha, hahaha, and yep) displaythe opposite demographic pattern.
Many of thesenon-standard words are phonetic transcriptions ofstandard words or phrases: that?s?dats, what?sup?wassup, I?m going to?ima.
The relationshipbetween these transcriptions and the phonologicalcharacteristics of dialects such as African-AmericanVernacular English is a topic for future work.5 Conjunctive Demographic FeaturesNext, we demonstrate how to select conjunctions ofdemographic features that predict text.
Again, weapply multi-output regression, but now we reversethe direction of inference: the predictors are demo-graphic features, and the outputs are term frequen-cies.
The sparsity-inducing `1,?
norm will select asubset of demographic features that explain the termfrequencies.We create an initial feature set f (0)(X) by bin-ning each demographic attribute, using five equal-frequency bins.
We then constructive conjunctivefeatures by applying a procedure inspired by relatedwork in computational biology, called ?Screen andClean?
(Wu et al, 2010).
On iteration i:?
Solve the sparse multi-output regression prob-lem Y = f (i)(X)B(i) + .?
Select a subset of features S(i) such that m ?S(i) iff maxj |b(i)m,j | > 0.
These are the rowindices of the predictors with non-zero coeffi-cients.?
Create a new feature set f (i+1)(X), includingthe conjunction of each feature (and its nega-tion) in S(i) with each feature in the initial setf (0)(X).We iterate this process to create features that con-join as many as three attributes.
In addition to thebinned versions of the demographic attributes de-scribed in Table 1, we include geographical infor-mation.
We built Gaussian mixture models over thelocations, with 3, 5, 8, 12, 17, and 23 components.For each author we include the most likely clusterassignment in each of the six mixture models.
Forefficiency, the outputs Y are not set to the raw termfrequencies; instead we compute a truncated sin-gular value decomposition of the term frequenciesW ?
UVDT, and use the basis U.
We set the trun-cation level to 100.5.1 Quantitative EvaluationThe ability of the induced demographic features topredict text is evaluated using a traditional perplex-ity metric.
The same test and training split is usedfrom the vocabulary experiments.
We construct alanguage model from the induced demographic fea-tures by training a multi-output ridge regression,which gives a matrix B?
that maps from demographicfeatures to term frequencies across the entire vocab-ulary.
For each document in the test set, the ?raw?predicted language model is y?d = f(xd)B, whichis then normalized.
The probability mass assigned1370whiteAfr.Am.Hisp.Eng.lang.Span.lang.otherlang.urbanfamilyrentermed.inc.- - - + - + + +;) - + - +:( -:) -:d + - + - +as - + -awesome + - - - +break - + - -campus - + - -dead - + - + + +hell - + - -shit - +train - + +will - + -would + -atlanta - + - -famu + - + - - -harlem - +bbm - + - + + +lls + - + - -lmaoo - + + - + + + +lmaooo - + + - + + + +lmaoooo - + + - + + +lmfaoo - + - + + +lmfaooo - + - + + +lml - + + - + + + + -odee - + - + + +omw - + + - + + + +smfh - + + - + + + +smh - + + +w| - + - + + + +con + - + +la - + - +si - + - +dats - + - + -deadass - + + - + + + +haha + - -hahah + -hahaha + - - +ima - + - + +madd - - + +nah - + - + + +ova - + - +sis - + +skool - + - + + + -wassup - + + - + + + + -wat - + + - + + + + -ya - + +yall - +yep - + - - - -yoo - + + - + + + +yooo - + - + +Table 3: Demographically-indicative terms discovered bymulti-output sparse regression.
Statistically significant(p < .05) associations are marked with a + or -.term definitionbbm Blackberry Messengerdats that?sdead(ass) veryfamu Florida Agriculturaland Mechanical Univ.ima I?m going tolls laughing like shitlm(f)ao+ laughing my (fucking)ass offlml love my lifemadd very, lotsnah noodee veryterm definitionomw on my wayova oversis sisterskool schoolsm(f)h shake my (fuck-ing) headw| withwassup what?s upwat whatya your, youyall you pluralyep yesyoo+ youTable 4: A glossary of non-standard terms from Ta-ble 3.
Definitions are obtained by manually inspectingthe context in which the terms appear, and by consultingwww.urbandictionary.com.model perplexityinduced demographic features 333.9raw demographic attributes 335.4baseline (no demographics) 337.1Table 5: Word perplexity on test documents, usinglanguage models estimated from induced demographicfeatures, raw demographic attributes, and a relative-frequency baseline.
Lower scores are better.to unseen words is determined through nested cross-validation.
We compare against a baseline languagemodel obtained from the training set, again usingnested cross-validation to set the probability of un-seen terms.Results are shown in Table 5.
The language mod-els induced from demographic data yield small butstatistically significant improvements over the base-line (Wilcoxon signed-rank test, p < .001).
More-over, the model based on conjunctive features signif-icantly outperforms the model constructed from rawattributes (p < .001).5.2 Features DiscoveredOur approach discovers 37 conjunctive features,yielding the results shown in Table 5.
We sort allfeatures by frequency, and manually select a sub-set to display in Table 6.
Alongside each feature,we show the words with the highest and lowest log-odds ratios with respect to the feature.
Many of theseterms are non-standard; while space does not permita complete glossary, some are defined in Table 4 orin our earlier work (Eisenstein et al, 2010).1371feature positive terms negative terms1 geo: Northeast m2 brib mangoville soho odeee fasho #ilovefamu foo coo fina2 geo: NYC mangoville lolss m2 brib wordd bahaha fasho goofy #ilovefamutacos4 geo: South+Midwest renter?
0.615 white?
0.823 hme muthafucka bae charlotte tx odeee m2 lolss diner mangoville7 Afr.
Am.> 0.101 renter> 0.615 Span.
lang.> 0.063 dhat brib odeee lolss wassupp bahaha charlotte california ikr en-ter8 Afr.
Am.?
0.207 Hispanic> 0.119 Span.
lang.> 0.063 les ahah para san donde bmore ohio #lowkey #twitterjailnahhh9 geo: NYC Span.
lang.?
0.213 mangoville thatt odeee lolssbuzzinlanded rodney jawn wiz golf12 Afr.
Am.> 0.442 geo: South+Midwest white?
0.823 #ilovefamu panama midtermswillies #lowkey knoe esta pero odeee hii15 geo: West Coast other lang.> 0.110 ahah fasho san koo diego granted pride adore phat pressure17 Afr.
Am.> 0.442 geo: NYC other lang.?
0.110 lolss iim buzzin qonna qood foo tender celebs pages pandora20 Afr.
Am.?
0.207 Span.
lang.> 0.063 white> 0.823 del bby cuando estoy muscle knicks becoming uncomfortablelarge granted23 Afr.
Am.?
0.050 geo: West Span.
lang.?
0.106 leno it?d 15th hacked government knicks liquor uu hunn homee33 Afr.
Am.> 0.101 geo: SF Bay Span.
lang.> 0.063 hella aha california bay o.o aj everywhere phones shift re-gardless36 Afr.
Am.?
0.050 geo: DC/Philadelphia Span.
lang.?
0.106 deh opens stuffed yaa bmore hmmmmm dyin tea cousin hellaTable 6: Conjunctive features discovered by our method with a strong sparsity-inducing prior, ordered by frequency.We also show the words with high log-odds for each feature (postive terms) and its negation (negative terms).In general, geography was a strong predictor, ap-pearing in 25 of the 37 conjunctions.
Features 1and 2 (F1 and F2) are purely geographical, captur-ing the northeastern United States and the New YorkCity area.
The geographical area of F2 is completelycontained by F1; the associated terms are thus verysimilar, but by having both features, the model candistinguish terms which are used in northeastern ar-eas outside New York City, as well as terms whichare especially likely in New York.5Several features conjoin geography with demo-graphic attributes.
For example, F9 further refinesthe New York City area by focusing on communitiesthat have relatively low numbers of Spanish speak-ers; F17 emphasizes New York neighborhoods thathave very high numbers of African Americans andfew speakers of languages other than English andSpanish.
The regression model can use these fea-tures in combination to make fine-grained distinc-tions about the differences between such neighbor-hoods.
Outside New York, we see that F4 combinesa broad geographic area with attributes that select atleast moderate levels of minorities and fewer renters(a proxy for areas that are less urban), while F15identifies West Coast communities with large num-5Mangoville and M2 are clubs in New York; fasho and coowere previously found to be strongly associated with the WestCoast (Eisenstein et al, 2010).bers of speakers of languages other than English andSpanish.Race and ethnicity appear in 28 of the 37 con-junctions.
The attribute indicating the proportion ofAfrican Americans appeared in 22 of these features,strongly suggesting that African American Vernac-ular English (Rickford, 1999) plays an importantrole in social media text.
Many of these featuresconjoined the proportion of African Americans withgeographical features, identifying local linguisticstyles used predominantly in either African Amer-ican or white communities.
Among features whichfocus on minority communities, F17 emphasizes theNew York area, F33 focuses on the San FranciscoBay area, and F12 selects a broad area in the Mid-west and South.
Conversely, F23 selects areas withvery few African Americans and Spanish-speakersin the western part of the United States, and F36 se-lects for similar demographics in the area of Wash-ington and Philadelphia.Other features conjoined the proportion ofAfrican Americans with the proportion of Hispan-ics and/or Spanish speakers.
In some cases, featuresselected for high proportions of both African Amer-icans and Hispanics; for example, F7 seems to iden-tify a general ?urban minority?
group, emphasizingrenters, African Americans, and Spanish speakers.Other features differentiate between African Ameri-1372cans and Hispanics: F8 identifies regions with manySpanish speakers and Hispanics, but few AfricanAmericans; F20 identifies regions with both Span-ish speakers and whites, but few African Americans.F8 and F20 tend to emphasize more Spanish wordsthan features which select for both African Ameri-cans and Hispanics.While race, geography, and language predom-inate, the socioeconomic attributes appear in farfewer features.
The most prevalent attribute is theproportion of renters, which appears in F4 and F7,and in three other features not shown here.
This at-tribute may be a better indicator of the urban/ruraldivide than the ?% urban?
attribute, which has avery low threshold for what counts as urban (seeTable 1).
It may also be a better proxy for wealththan median income, which appears in only one ofthe thirty-seven selected features.
Overall, the se-lected features tend to include attributes that are easyto predict from text (compare with Table 2).6 Related WorkSociolinguistics has a long tradition of quantitativeand computational research.
Logistic regression hasbeen used to identify relationships between demo-graphic features and linguistic variables since the1970s (Cedergren and Sankoff, 1974).
More re-cent developments include the use of mixed factormodels to account for idiosyncrasies of individualspeakers (Johnson, 2009), as well as clustering andmultidimensional scaling (Nerbonne, 2009) to en-able aggregate inference across multiple linguisticvariables.
However, all of these approaches assumethat both the linguistic indicators and demographicattributes have already been identified by the re-searcher.
In contrast, our approach focuses on iden-tifying these indicators automatically from data.
Weview our approach as an exploratory complement tomore traditional analysis.There is relatively little computational work onidentifying speaker demographics.
Chang et al(2010) use U.S. Census statistics about the ethnicdistribution of last names as an anchor in a latent-variable model that infers the ethnicity of Facebookusers; however, their paper analyzes social behav-ior rather than language use.
In unpublished work,David Bamman uses geotagged Twitter text and U.S.Census statistics to estimate the age, gender, andracial distributions of various lexical items.6 Eisen-stein et al (2010) infer geographic clusters that arecoherent with respect to both location and lexicaldistributions; follow-up work by O?Connor et al(2010) applies a similar generative model to demo-graphic data.
The model presented here differs intwo key ways: first, we use sparsity-inducing regu-larization to perform variable selection; second, weeschew high-dimensional mixture models in favor ofa bottom-up approach of building conjunctions ofdemographic and geographic attributes.
In a mix-ture model, each component must define a distribu-tion over all demographic variables, which may bedifficult to estimate in a high-dimensional setting.Early examples of the use of sparsity in natu-ral language processing include maximum entropyclassification (Kazama and Tsujii, 2003), languagemodeling (Goodman, 2004), and incremental pars-ing (Riezler and Vasserman, 2004).
These papers allapply the standard lasso, obtaining sparsity for a sin-gle output dimension.
Structured sparsity has rarelybeen applied to language tasks, but Duh et al (2010)reformulated the problem of reranking N -best listsas multi-task learning with structured sparsity.7 ConclusionThis paper demonstrates how regression with struc-tured sparsity can be applied to select words andconjunctive demographic features that reveal soci-olinguistic associations.
The resulting models arecompact and interpretable, with little cost in accu-racy.
In the future we hope to consider richer lin-guistic models capable of identifying multi-word ex-pressions and syntactic variation.Acknowledgments We received helpful feedbackfrom Moira Burke, Scott Kiesling, Seyoung Kim, Andre?Martins, Kriti Puniyani, and the anonymous reviewers.Brendan O?Connor provided the data for this research,and Seunghak Lee shared a Matlab implementation ofthe multi-output lasso, which was the basis for our Cimplementation.
This research was enabled by AFOSRFA9550010247, ONR N0001140910758, NSF CAREERDBI-0546594, NSF CAREER IIS-1054319, NSF IIS-0713379, an Alfred P. Sloan Fellowship, and Google?ssupport of the Worldly Knowledge project at CMU.6http://www.lexicalist.com1373ReferencesHenrietta J. Cedergren and David Sankoff.
1974.
Vari-able rules: Performance as a statistical reflection ofcompetence.
Language, 50(2):333?355.Jonathan Chang, Itamar Rosenn, Lars Backstrom, andCameron Marlow.
2010. ePluribus: Ethnicity on so-cial networks.
In Proceedings of ICWSM.John Duchi, Shai Shalev-Shwartz, Yoram Singer, andTushar Chandra.
2008.
Efficient projections onto the`1-ball for learning in high dimensions.
In Proceed-ings of ICML.Kevin Duh, Katsuhito Sudoh, Hajime Tsukada, HidekiIsozaki, and Masaaki Nagata.
2010. n-best rerank-ing by multitask learning.
In Proceedings of the JointFifth Workshop on Statistical Machine Translation andMetrics.Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,and Eric P. Xing.
2010.
A latent variable model of ge-ographic lexical variation.
In Proceedings of EMNLP.Jerome Friedman, Trevor Hastie, and Rob Tibshirani.2010.
Regularization paths for generalized linearmodels via coordinate descent.
Journal of StatisticalSoftware, 33(1):1?22.Joshua Goodman.
2004.
Exponential priors for maxi-mum entropy models.
In Proceedings of NAACL-HLT.Daniel E. Johnson.
2009.
Getting off the GoldVarbstandard: Introducing Rbrul for mixed-effects variablerule analysis.
Language and Linguistics Compass,3(1):359?383.Jun?ichi Kazama and Jun?ichi Tsujii.
2003.
Evaluationand extension of maximum entropy models with in-equality constraints.
In Proceedings of EMNLP.William Labov.
1966.
The Social Stratification of En-glish in New York City.
Center for Applied Linguis-tics.Han Liu, Mark Palatucci, and Jian Zhang.
2009.
Block-wise coordinate descent procedures for the multi-tasklasso, with applications to neural semantic basis dis-covery.
In Proceedings of ICML.John Nerbonne.
2009.
Data-driven dialectology.
Lan-guage and Linguistics Compass, 3(1):175?198.Brendan O?Connor, Jacob Eisenstein, Eric P. Xing, andNoah A. Smith.
2010.
A mixture model of de-mographic lexical variation.
In Proceedings of NIPSWorkshop on Machine Learning in Computational So-cial Science.Ariadna Quattoni, Xavier Carreras, Michael Collins, andTrevor Darrell.
2009.
An efficient projection for `1,?regularization.
In Proceedings of ICML.John R. Rickford.
1999.
African American VernacularEnglish.
Blackwell.Stefan Riezler and Alexander Vasserman.
2004.
Incre-mental feature selection and `1 regularization for re-laxed maximum-entropy modeling.
In Proceedings ofEMNLP.Gerard Rushton, Marc P. Armstrong, Josephine Gittler,Barry R. Greene, Claire E. Pavlik, Michele M. West,and Dale L. Zimmerman, editors.
2008.
GeocodingHealth Data: The Use of Geographic Codes in CancerPrevention and Control, Research, and Practice.
CRCPress.Hinrich Schu?tze and Jan Pedersen.
1993.
A vector modelfor syntagmatic and paradigmatic relatedness.
In Pro-ceedings of the 9th Annual Conference of the UW Cen-tre for the New OED and Text Research.Aaron Smith and Lee Rainie.
2010. Who tweets?
Tech-nical report, Pew Research Center, December.Berwin A. Turlach, William N. Venables, and Stephen J.Wright.
2005.
Simultaneous variable selection.
Tech-nometrics, 47(3):349?363.Larry Wasserman and Kathryn Roeder.
2009.
High-dimensional variable selection.
Annals of Statistics,37(5A):2178?2201.Larry Wasserman.
2003.
All of Statistics: A ConciseCourse in Statistical Inference.
Springer.Jing Wu, Bernie Devlin, Steven Ringquist, MassimoTrucco, and Kathryn Roeder.
2010.
Screen and clean:A tool for identifying interactions in genome-wide as-sociation studies.
Genetic Epidemiology, 34(3):275?285.Qing Zhang.
2005.
A Chinese yuppie in Beijing: Phono-logical variation and the construction of a new profes-sional identity.
Language in Society, 34:431?466.Kathryn Zickuhr and Aaron Smith.
2010.
4% of onlineAmericans use location-based services.
Technical re-port, Pew Research Center, November.1374
