Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 50?54,October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational LinguisticsA Process for Predicting MOOC AttritionMike SharkeyPresidentBlue CanaryChandler, AZ  USAmike@bluecanarydata.comRobert SandersSr.
Software EngineerClairvoyant, LLCChandler, AZ  USArobert.sanders@clairvoyantsoft.comAbstractThe goal of this shared task was to predictattrition in a MOOC through use of thedata and logs generated by the course.Our approach to the task reinforces theidea that the process of gathering andstructuring the data is more important (andmore time consuming) than the predictivemodel itself.
The result of the analysiswas that a subset of 15 different data fea-tures did a sufficiently good job at predict-ing whether or not a student would exhibitany activity in the following week.1 IntroductionBlue Canary is a higher education analytics com-pany located in Chandler, Arizona USA.
Thecompany has extensive experience in dealing withacademic course/enrollment/retention data and isproud to collaborate with other researchers on theEMNLP 2014 shared task.
The goal of the task isto use data from one MOOC, create a model topredict course attrition, and then apply that modelto five other MOOCs in order to observe the effi-cacy of the model across courses.
The goal of thispaper is to document the process that Blue Canarywent through in order to generate the model.2 Understanding the ProblemIn order to successfully complete a task such asthis, the team needed the right context to the prob-lem.
The context for this particular challenge (us-ing MOOC data to predict attrition) was very fa-miliar to the Blue Canary team.
First, the teamhas developed retention-oriented predictive mod-els for a number of institutions in the past.
Thisexperience was vital.
Second, the team hasworked with data at scale.
The MOOC course had20,000 enrolled students with a log file that gen-erated 1.6 million rows of data.
The Blue Canaryteam has experience working with a large onlineuniversity that had over 300,000 students generat-ing millions of rows of data on a daily basis.Lastly, all of the team members have participatedin at least one MOOC, so the processes and inter-actions associated with such a course are known.The combination of all of these factors gave theBlue Canary team the necessary context to tacklethe attrition problem from the ground up.3 Approach to the ProblemAs with other such data initiatives, the process isa stepwise iterative one.
Each step and iterationprovides more insight, allowing the team to refinethe prediction.3.1 Step 1: Feature ExtractionFeature extraction is the process of defining theindependent variables (or inputs) for the predic-tive model.
This is arguably the most importantstep in the process of developing a predictivemodel.
It requires a deep understanding of thesource data from a technical side as well as a con-textual understanding of how the data relate to thefront-end user experience.Blue Canary used two techniques for feature ex-traction.
The first was experience.
Having lookedat course activity data and developed predictivemodels for other courses, we knew the kinds offeatures that would likely have an impact on theprediction.
This experience gave us simplisticfeatures like ?number of videos watched?
and ?to-tal minutes spent in class?
to more nuanced fea-tures like ?attempted quiz without referring toother materials?.The second technique was using visualizationsto explore data relationships.
The team used theTableau visualization tool to ingest course activitydata and map it across users & weeks.
Looking atthese relationships visually helped to determine ifwe should include the features in the modeling ornot.503.2 Step 2: Define Outcome/PredictionOnce the list of features have been developed,next step is to define exactly what it is we are pre-dicting.
At a high level, it sounds easy ?
will thestudent retain in the class?
From a data perspec-tive, though, we need to define what it means toretain.
Does it mean that the student submitted theassignment for the week?
Watched a video?Simply logged in?
Zeroing in on a reliable defi-nition of retention is a part of the process.3.3 Step 3: Run the Predictive ModelWith the input and output data in place, the teamneeds to run a model to derive a prediction.
BlueCanary has consistently used machine learningtechniques (as opposed to statistical modeling).As Bogard (2011) alludes to in a blog post com-paring the two approaches, Blue Canary?s tech-nical expertise combined with an unknown under-lying relationship make machine learning our pre-ferred method of analysis.
For this analysis, BlueCanary implemented a random forest method us-ing the SciKit python toolset (http://scikit-learn.org/).3.4 Step 4: Observe/Validate/IterateThe last step in the process is to observe the out-comes of the modeling, validate the results (bothquantitatively and qualitatively) and iterate to im-prove.
When looking at the modeling results, wefocused on accuracy.
More specifically, we fo-cused on the true positive rate (recall) and the truenegative rate individually.
The combination ofthese components equal the accuracy of themodel, but we thought it was important to look atboth since the application of any such solutionwould involve treatments for both parties.Value DefinitionTrue Positive # predicted to retain /# actually retainedTrue Negative # predicted to attrite /# actual attritionAccuracy (True positive + Truenegative) / populationTable 1: Definition of model accuracy values3.5 Acknowledging Prior ResearchIt should be noted that Blue Canary has stood onthe shoulders of others who have tackled similarproblems in the past.
Our choice for analyticalmethods and features has been inspired by earlierpredictive projects like Purdue?s Course Signals(Arnold and Pistilli, 2012) and research done atAmerican Public University (Boston et.
al., 2011).We also referenced contemporary MOOC re-search that explored the descriptive (Breslow et.al., 2013), predictive (Taylor et.
al., 2014), and so-cial (Ros?
et.
al., 2014) contributors to attrition.4 Predicting Attrition for PSY-001The course in question was from a 2013 GeorgiaTech/Coursera MOOC called ?Introduction toPsychology as a Science?.
Blue Canary executedseven iterative steps as explained in the previoussection.
At the end we came up with a model thatused 15 features to predict retention and attritionat an 88% accuracy rate.4.1 Iteration 1: Feature ExtractionThe first iteration didn?t result in any prediction.The goal was to explore the data and extract aninitial set of features for processing.
We also cre-ated our training, testing, and hold back data usinga 70/15/15 split.
Table 2 lists the features we ini-tially extracted from the activity data.?
id?
user_id?
username?
week_id?
week_num?
week_start_date?
week_end_date?
session_count?
url_wiki_edit_count?
url_wiki_view_count?
url_quiz_count?
url_lecture_count?
url_forum_count?
is_english?
ip_count?
most_common_browser?
most_common_browser_date?
browser_count?
unique_quizzes_attempted?
total_quiz_attempts?
average_attempts_per_quiz?
videos_accessed_count?
average_video_per_session?
did_peer_review?
actually_attendedTable 2: Initial list of features51These features were very basic.
We didn?t spendmuch time on more advanced features.
The goalof this first was simply to lay the foundation forour data analysis pipeline.4.2 Iteration 2: Test Analytical API?sWith a bulk of the features in place, our next goalwas to connect the machine learning toolset to thepipeline.
We used Weka (http://www.cs.wai-kato.ac.nz/ml/weka/) since the team had some ex-perience with the tool.
Since our approach was toconstruct the pipeline as a smooth-running appli-cation, we utilized the Weka API?s to feed data inand get results out.Unfortunately, we ran into technical problemswith the API?s and got out of memory exceptionerrors.
We were unable to troubleshoot and de-cided to move on to another toolset.
In addition,though, we added more features, mainly fromparsing the URL strings in the access log files (Ta-ble 3).?
event_count?
total_minutes_spent?
url_quiz_submits_count?
url_quiz_actual_submits_count?
url_quiz_percent_of_actual_submits?
url_quiz_at-tempt_in_more_than_one_session?
url_quiz_retry?
url_quiz_attempt_but_no_submit?
url_quiz_submit_no_help?
url_human_grading_count?
url_forum_search_count?
url_class_preferences_count?
url_signature_countTable 3: URL features added4.3 Iteration 3: Too Good to be TrueWe switched to SciKit as our analytical tool ofchoice, but we still used the Random Forestmethod.
We ran our first analysis and got the cor-responding accuracy rates.
As explained in sec-tion 3.4, we produce accuracy rates for ?False?
(correctly predicting that the student won?t attendnext week), ?True?
(correctly predicting that thestudent will attend next week) and ?Average?
(ac-curacy ?
the weighted average of False and True).The results for our first run were as follows:Measure RateFalse 99%True 87%Accuracy 96%The team was skeptical about such high accu-racy rates, especially given that it was our firstrun.
We suspected that there was some sort ofleakage ?
information about the prediction fieldmay have leaked into one of the features.
Thatsuspicion was confirmed when we dug deeper intothe model.The predominant feature was ?is_english?.
Welooked at the user agent data in the activity logsand parsed the language parameter to determine ifthe web browser language was set to English ornot.
It turns out that when there was no activityfor the week, we populated this field with null val-ues.
Since the majority of the students had Eng-lish as their language, the model was seeing?is_english?
= TRUE when there was activity and?is_english?
= FALSE when there wasn?t activity.This was a great example of the kinds of errorsone finds early on in the analysis.4.4 Iteration 4: First Real ModelFor the next iteration, we fixed the ?is_english?field and ran the model again.
This run was ourfirst valid predictive model for the dataset and theresults were:Measure RateFalse 92%True 55%Accuracy 89%Note that we are doing a very good job at predict-ing students who won?t attend next week.
This isdue to the fact that there are a large number of stu-dents don?t attend.
We estimated that about20,000 students signed up for the class, 11,000 ofthem showed any activity at all, and less than3,000 completed the course.4.5 Iteration 5: Defining the OutcomeFor experimentation purposes, we wanted to seeif changing the definition of ?attending?
wouldhave any effect on the modeling.
Our original def-inition of attending was that there were ANY useractions in the data (viewing a page, posting a dis-cussion item, taking a quiz, etc.).
We decided toadd variations to that definition such as ?viewingat least one lecture?, ?submitting at least onequiz?, or ?will never attend again?
(as opposed to52just not attending next week).
The table below isa sampling of some of the results we generated:Measure Out_i Out_a Out_b Out_cFalse 92% 94% 97% 87%True 55% 45% 47% 90%Accuracy 89% 91% 95% 89%This exercise showed some interesting results.Specifically, we saw how we would improve ourability to predict students who wouldn?t attend(False) but decrease the True accuracy.
We didsee significant improvement in the case where theoutcome was ?will never attend again?.
However,we decided to stay with our base definition of at-tendance as ?no activity in the following week?.Validating these alternate definitions of attend-ance is a task that would be worthwhile for addi-tional research.4.6 Iteration 6: Team CollaborationBlue Canary prides itself on collaboration notonly amongst researchers in the learning analyticsfield, but also collaboration inside of our owncompany.
We made sure to share informationabout this shared task with others in the company,and that collaboration allowed us to positively ex-pand our feature set.
One employee had comeacross MOOC research that had found good pre-dictive results when using an aggregate engage-ment/activity score (Poellhuber, 2014).
We de-cided to utilize a similar feature where the numberof sessions, pages, days, and hours of activity in agiven week were combined into an engagementscore.4.7 Iteration 7: Winnowing the FieldAs a final step, we wanted to reduce the numberof features used in the modeling process so as toimprove cycle times.
We knew that the majorityof the fields had little to no predictive value, so weran models where we just used the top 10, 15, or20 features.
In the end, all permutations gave sim-ilar accuracy scores and we decided to use the top15 features.
Those features resulted in accuracyrates of:Measure RateFalse 92%True 54%Accuracy 88%The accuracy rates are similar to the rates wehad been getting in the past two iterations of themodeling.
This led us to conclude that we were atthe point of diminishing returns and we decided tofinalize the model with the 15 features and theircorresponding importance level as illustrated inTable 4 (below).Feature Import.total_minutes_spent_previous_wk 0.336initial_activity_score_previous_wk 0.072final_activity_score_previous_wk 0.071final_activity_score_up_to_wk 0.070event_count_up_to_wk 0.068most_com-mon_browser_count_up_to_wk 0.059initial_activity_score_up_to_wk 0.049url_wiki_view_count_up_to_wk 0.041session_count_up_to_wk 0.038url_quiz_count_up_to_wk 0.037total_minutes_spent_up_to_wk 0.037url_lecture_count_up_to_wk 0.037browser_count_up_to_wk 0.031ip_count_up_to_wk 0.031session_count_previous_wk 0.023Table 4: Features and Importance5 ConclusionsThe overarching conclusion from this researchcan be summarized in two points:1.
Machine learning models can do an aboveaverage job at predicting retention/attri-tion in MOOC?s2.
The predictive factors are not surprising ?they are variants of measures of the stu-dent?s engagement and activity in thecourse5.1 FeaturesLooking at the features in Table 4, one can see thatalmost all of the important features are measuresof activity.
Minutes, events, views and even theaggregated activity feature are all measuring sim-ilar characteristics.
The takeaway here is thatthere shouldn?t be an expectation of some uniquemarker that predicts retention.
There?s no secretin the secret sauce.6 AcknowledgementsThe authors would like to thank Mohammed An-sari, Andy Allen, Satish Divakarla, David Mor-gan, and the entire Blue Canary and Clairvoyantteam for their support in this shared task.53ReferencesMatt Bogard.
(2011, January 29) Culture War: Classi-cal Statistics vs. Machine Learning.
Retrieved fromhttp://econometricsense.blog-spot.com/2011/01/classical-statistics-vs-ma-chine.htmlArnold, K. E., & Pistilli, M. D. (2012, April).
CourseSignals at Purdue: Using learning analytics to in-crease student success.
In Proceedings of the 2nd In-ternational Conference on Learning Analytics andKnowledge (pp.
267-270).
ACM.Boston, W. E., Ice, P., & Gibson, A. M. (2011).
Com-prehensive assessment of student retention in onlinelearning environments.
Online Journal of DistanceLearning Administration, 14(4).Breslow, L., Pritchard, D. E., DeBoer, J., Stump, G. S.,Ho, A. D., & Seaton, D. T. (2013).
Studying learn-ing in the worldwide classroom: Research intoedX?s first MOOC.
Research & Practice in Assess-ment, 8, 13-25.Taylor, C., Veeramachaneni, K., & O'Reilly, U. M.(2014).
Likely to stop?
Predicting Stopout in Mas-sive Open Online Courses.
arXiv preprintarXiv:1408.3382.Ros?, C. P., Carlson, R., Yang, D., Wen, M., Resnick,L., Goldman, P., & Sherer, J.
(2014, March).
Socialfactors that contribute to attrition in moocs.
In Pro-ceedings of the first ACM conference on Learning@scale conference (pp.
197-198).
ACM.Poellhuber, B., Roy, N., Bouchoucha, I., Anderson, T.(2014, April).
The Relationship Between the Moti-vational Profiles, Engagement Profiles and Persis-tence of MOOC Participants.
Retrieved fromhttp://www.moocresearch.com/wp-content/up-loads/2014/06/MOOC-Research-InitiativePoelhu-ber9187v4a.pdf, September 1, 2014.54
