Proceedings of the 14th European Workshop on Natural Language Generation, pages 210?211,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsUIC-CSC: The Content Selection Challenge entryfrom the University of Illinois at ChicagoHareen VenigallaComputer ScienceUniversity of Illinois at ChicagoChicago, IL, USAhareen058@gmail.comBarbara Di EugenioComputer ScienceUniversity of Illinois at ChicagoChicago, IL, USAbdieugen@uic.eduAbstractThis paper described UIC-CSC, the en-try we submitted for the Content SelectionChallenge 2013.
Our model consists ofheuristic rules based on co-occurrences ofpredicates in the training data.1 IntroductionThe core of the Content Selection Challenge taskis formulated as Build a system which, given a setof RDF triples containing facts about a celebrityand a target text (for instance, a Wikipedia - stylearticle about that person), selects those triples thatare reflected in the target text.
The organizers pro-vided training data consisting of 62618 pairs oftexts and triple sets.
The text is the introductorytext tfC of the Wikipedia article corresponding tocelebrity C; the set of triples trC concerning C wasgrepped from the Freebase official weekly RDFdump.
It is important to note that we do not knowwhich specific triples from trC are rendered in tfC .A sample triple in the file is as follows:ns:m.04wqrns:award.award winner.awards wonns:m.07ynmx5In the above triple, ns:m.04wqr isthe subject id, of Marilyn Monroe inthis case (ns denotes namespace);ns:award.award winner.awards won isthe predicate and ns:m.07ynmx5 is the objectid of the award.
Since this format is not readable,the organizers provided a script to transform theturtle file into a human readable form, where theobject id is replaced by its actual value:/award/award winner/awards won ?
?GoldenGlobe Awards for Actress - Musical orComedy Film - 17th Golden Globe Awards- Some Like It Hot - 1960 - earliye -Award Honor??
/m/07ynmx5In the following, we will refer to the first elementof these expressions as the predicate.
Our ap-proach relies on heuristics derived from clusteringpredicates directly, or clustering them based onthe co-occurrence of the argument of predicate piin a text tf and in turtle files tr that contain bothpi and another predicate pj .2 Deriving heuristic rulesWe observed that in total there are 613 distinctpredicates.
Out of these 613 predicate, only 11are present in over 40 percent of the files and only19 predicates are present in over 10 percent of thefiles.
This means that a large number of predi-cates are present only in a few files.
This makes itharder to decide whether we have to include thesepredicates or not.
Conversely, nearly 40 percent oftext files only contain one or two sentences, whichcompounds the sparsity problem.Predicate Clustering.
In the first method, wegenerate predicate clusters by simply removingthe leaf from each predicate expression.
For exam-ple, /people/person/place of birth,and /people/person/education belong tothe same cluster, labelled /people/person asthey have the same parent /people/person.We found 35 such clusters.
We then ana-lyzed the frequency of each predicate pi onits own, and conditional on other predicates inthe same cluster: for example, how frequent/people/person/education is, andhow often it occurs in those triple files, where/people/person/place of birth is alsopresent.Intersection on Arguments.
For each predicatepi, we compute the set of its intersection sets ISi,j .Each set isi,j comprises all the turtle files tri,jwhere pi co-occurs with a second predicate pj .
Foreach tri,j , we retrieve the corresponding text filetf (recall that each turtle file is associated withone text file) and check whether the argument of210pi occurs in tf ?
this is indirect evidence that thetext does include the information provided by pi(of course this inference may be wrong, if this ar-gument occurs in a context different from what piconveys).
If the argument of pi does occur in tf ,we keep tri,j , otherwise we discard it.
As above,we then proceed to compute the frequencies of theoccurrences of pi on its own, and of pi when pjis also present, over all the turtle files tri,j ?
isi,jthat have not been filtered out as just discussed.Given these two methods, we derive rules suchas the following:IF /baseball/baseball player/position ?
trkAND/baseball/baseball player/batting stats?
trkTHENselect/baseball/baseball player/positionThe set of rules is then filtered as follows.
Ona small development set, we manually annotatedwhich triples are included in the correspondingtext files.
We keep a rule if the F-measure concern-ing predicate pi (i.e., concerning the triples whosepredicate is pi) improves when using the rule, asopposed to including pi if it belongs to a set offrequent predicates.We also need to deal with multiple occurrencesof pi in one single turtle file.
Predicates such as/music/artist/track can have multiple in-stances, up to 30, in a certain trk, with differentarguments; however, those predicates may occurfar fewer times in the corresponding text files ?
be-cause say trMM on Marilyn Monroe includes onetriple for each of her movies, but the correspond-ing tfMM only mentions a few of those movies.Hence, we impose an upper limit of 5 on the num-ber of occurrences in the same turtle file, for a cer-tain predicate to be included, for example:IF /music/artist/trackAND its count ?
5THEN select /music/artist/track3 EvaluationApart from our participation in the Challenge, weevaluated our system on a small test set composedof 96 pairs of text and turtle files, randomly se-lected from the data released by the organizers.This resulted in a total of 153 unique predicates(hence, about 14 of the total number of distinctpredicates).
We manually annotated the predicatesin the turtle files as present/absent in the corre-sponding text file.We consider four domains:1.
Basic facts: general, very frequent informa-tion, such as people/person/profession,people/person/nationality.2.
Books: predicates whose root is book,like book/author/works written,book/book subject/works.3.
Sports: predicates whose root is a sport, likebaseball/baseball player/position s,ice hockey/hockey player/former team.4.
Film and Music: predicateswhose root is film or music,like /film/director/film,/music/artist/track.5.
Television: predicates whose root is tv, like/tv/tv director/episodes directed.As apparent from Table 1, the performance ofour system varies considerably according to thedomain of the predicates.
Specifically, we be-lieve that the exceedingly low precision for pred-icates of type book, film & music, tv isdue to the sparseness of the data.
As we notedabove, 40% of the text files only include one ortwo sentences.
Hence, our system selects manymore predicates than are actually present in thecorresponding text file.Table 1: Performance on in-house test setDomain P R F-scoreBasic Facts 79.83 51.25 62.40Sports 79.84 49.22 60.90Books 12.80 66.30 21.47Film & Music 5.77 55.19 10.45TV 5.46 43.36 9.704 Future EnhancementsUIC-CSC could be improved by more closely an-alyzing the features of the text files, especially theshortest ones: when they include only few sen-tences, which kinds of predicates (and arguments)do they include?
For example, if only two moviesare mentioned as far as Monroe is concerned, whatelse can we infer from the Monroe turtle file trMMabout those two movies?211
