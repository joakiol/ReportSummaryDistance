Learn ing  and App l i ca t ion  of  Di f ferent ia l  GrammarsDavid M. W. PowersArtificial Intelligence LaboratoryDepartment of Computer ScienceThe Flinders University of South Australiapowers~acm, orgAbstractWe examine the Differential Grammar,a representation designed to discriminatewhich of a set of eonfusable alternatives imost likely in the context it occurs in.
Thisapproach is useful whereever uncertaintymay exist about the identity of a tokenor sequence of tokens, including in speechrecognition, optical character ecognitionand machine translation.
In this paper ourapplication is word processing: we discussmultiple models of confusion which maybe used in the identification of confusedwords, we show how significant contextsmay be identified and condensed into Dif-ferential Grammars,  and we contrast theperformance of our implementation withthat of two commercial grammar checkerswhich purport  to handle the confused wordproblem.1 IntroductionIn this paper, we explore the concept of a DifferentialGrammar  and apply it to the problem of grammarchecking - in the sense that it is used on the box yourWYSIWYG word processor came in!
A DifferentialGrammar  is not a grammar in the tradit ional rule-oriented sense, and although it is lexically-focussedit doesn't really have a concept of a rule at all, butis somewhat more specialized, and extremely sim-ple.
We introduce a simple statistical preclassifica-tion, which we use to define a modified Ngram envi-ronment for each member of a confusion class, andthen adjust the size of our environment until a pre-determined significance level or diameter is reached.We also demonstrate several approaches to the au-tomated generation of the confusion classes.The structure of the paper is as follows: we in-troduce Differential Grammars and motivate themin terms of grammar checking, we discuss the acqui-sition, efficiency and significance issues, we presentan improved user interface for grammar checkers,and we demonstrate the improvement achieved byDifferential Grammar based checkers compared withcommercial products.
Finally, we discuss other ex-periments relating to automated acquisition of dif-ferential grammars.2 Commonly Confused WordsA word processor's grammar checker typical ly checksgrammar in two senses, a pre-/proscript ive sensewhich should be characterized by what are moreproperly identified as style rules, and a spell-checking like sense which is characterized as the useof grammar rules to identify typos, substitutions,omissions, duplications which occur at the word orletter level.The errors we focus on are those ' typos'  which re-sult in one word being substituted by another word "- both of which are words which would be acceptedby the spell-checker.
This is closely related to theproblems of commonly confused words, homophonesand near homophones, but the most important  dif-ference between the different types of substitutionis in fact whether readers, proofreading their ownwork, or looking at a detected error, will recognizethat it is an error or not.This is not a new problem, either from the pointof view of commercial word processors (Johnson,1992) or Computat ional  Linguistics (Ill, 1983), andtwo aspects to the problem must be differentiated:we want to find the typos, but we don't  want tobe overwhelmed with false errors.
It is this latterconcern which leads to people deciding not to usethe available grammar checkers - and those who dotend to turn off the style rules, which they regard aspure noise, making the whole question of the valueof grammar checkers rather controversial (Wampler,1995) .Powers 88 Differential GrammarsDavid M. W. Powers (1997) Learning and Appl icat ion  of Differential  Grammars .
In T.M.
Ellison (ed.
)CoNLL97: Computational Natural Language Learning, ACL pp 88-96.
(~) 1997 Association for Computational LinguisticsBy targeting commonly confused words, ratherthe general problem, we have simply to search forcontexts which differentiate he words well.
Further-more, we only wish to distinguish words on gram-matical grounds - probabilistic methods which relyon semantic associations are simply too sensitive tochanges in genre or topic to use as the primary dis-criminant.3 Differential GrammarsA differential grammar is basically a small set of en-vironments that allows us to differentiates betweena pair of confused words in all contexts(Kernick,1996).
However, we want to emphasize that theapproach need not be limited to word confusion orgrammar checking, and that there is no need tolimit it to pairs of targets.
Conversely, we also wantto strengthen the definition slightly, as we want tosyntactic errors.
We therefore define a DifferentialGrammar as:Definit ion: Differential GrammarA minimal set of syntactically significantenvironments hat differentiate amongst aset of possible targets.However, we do not wish to have to specify thedifferential grammars or the syntactic environments,but rather wish to learn them.
For the commonlyconfused word problem (in the more general and in-clusive sense that encompasses verything from com-mon typos to near homophones), we thus have threeaspects to the problem where we wish to do somekind of learning:1. identifying pairs of commonly confusedwords;2. selecting appropriate syntactically signif-icant environments;3. deciding when an error has occurred.This involves learning in three different senses,and the programs we present here have performedeach kind of learning to varying degrees.
First, wewant to learn to select appropriate data for applyingour grammar building methodology to - we want toavoid having to provide positive and negative xam-ples.
Second, we want to learn what is syntacticallyappropriate - we want to avoid having to providetags, bracketing or parses.
Third, we want to learnand dynamically adjust to what the user wants andthe target text requires - we want to avoid usershaving to set parameters.In addition we have some further goals relating tooptimization:4. minimizing the size of the differentialgrammar;5. ensuring the significance of the contextsstored;6. facilitating the users' interactions withthe system.4 Discovery of Confused Word pairsWe consider here the first of our six goals.
We as-sume that certain substitutions are more likely thanothers, and we do not aim to deal with the generalcase which includes arbitrary unlikely substitutions.We distinguish six different ypes of reasons for sub-stituted word errors:a. typos: keyboard proximity (knowledgeof keyboard used);b. phonos: phonological proximity (phono-logical features used);c. grammos: grammatical proximity(grammatical features used);d. frequens: frequency disparity (frequencyinformation used);e. foreignish: interlinguistic disparity (nottargeted at present);f. idiosyncratic: unknown reason (somesystem or user confuses the pair).Note that we do not include semantic or style er-rors - the latter tend to be a result of prescriptive lin-guists proscribing certain constructs, or maintainingtraditional distinctions which are falling out of com-mon use: e.g.
the distinction between 'due to' (asmeaning 'caused by' but not 'because of') and 'ow-ing to' (as meaning 'because of' but not 'caused by');rules about prepositions at the end of sentences; plitinfinitives; deprecated passives; etc.In our first set of experiments (a) has been mod-eled by simple adjacency on the keyboard, testing inprinciple all pairs in decreasing order of frequencyof the more frequent member.
Errors are modeledprimarily as systematic displacements of the handon the keyboard, or substitutions of adjacent char-acters in the order 1 case.
Deletions are handledby treating the empty string " as being adjacent oall characters, and insertions are handled inverselyas deletions.
Transpositions and interspersions canbe ranked on the number of moved characters, anddisplacements by the number of substituted charac-ters, but in our experiment we limited both to one.The grammatical errors (c) are somewhat trickier tocharacterize, but a brute-force first approximationwould simply list all morphological derivations fromeach root, ideally working at the morphological level.Powers 89 Diffferential GrammarsIn relation to (f) we note that for any confusionpair identified by the commercial grammar checkerson our test texts (true error or false error), we haveforced generation of Differential Grammars.
We aretrying to target the exact same class of substitutionerrors.
This has two effects: it increases the possi-bility of false errors and decreases the possibility ofmissed errors.The frequens (d) are an interesting class (Kernick,1996) in that we tend to make disproportionatelymore errors in which one word of the confused pairis very frequent and the other less so.
The veryfrequent words seem to be more easily activated thantheir near homonyms, and we have tendency to typethe frequens automatically even when it is the lessfrequent partner that was intended.
This is handledin our experiments by our use of the higher of thetwo frequencies ranking for grammar generation andevaluation.
In addition, we could (but don't) allowmore latitude in the search for pairings of frequentwords in classes (a), (b) and (?
), e.g.
by increasingthe number of characters or features that might beout of place, substituted or inserted.
Instead, wehave manually included pairs involving some of themost common words.Two common errors (which I made in typing thelast paragraph) are 'out' ~ 'our' (a) and 'our' --+'are' (b/d), and the single most common error is' its/it 's' - but often the confusion classes are notsimply pairs of words, e.g: 'yaw/your/yore/you're'and 'there/their/they're'.We tested around 100 pairs of words generated au-tomatically on the basis of keyboard proximity (a),as well as those proposed manually under (b) to (f).76 were used in our system and 55 were rejected forlack of either significance or discrimination.5 Bu i ld ing  an  e f f i c ient  D i f fe rent ia lGrammarWe now refine the concept of a differential grammarand present he specific form we employ.
The firstthing we need to do is to define what we mean by'syntactically significant environments'.
Basicallywe collect Ngram statistics for each of the targetwords, but we reduce the amount of statistics bya form of syntactic preclassification, then we startwith a minimal diameter and progressively increaseit until a desired degree of certainty and significanceis reached.We define an environment and its diameterstraightforwardly (again generalizing (Kernick,1996)):Definition: EnvironmentA sequence of contiguous units which in-eludes the target unit.Definition: DiameterThe number of units other than the targetwhich constitute the specified environmentof the target unit.We note that we have generalized the definitionfrom the specific focus on words we have here: read'word' for 'unit' throughout.
Also we highlight thefact that there is not a unique environment for eachtarget word, but rather that there are, in general,multiple environments for each possible diameter.Diameter 0 refers to the target word alone.
It's fre-quency relative to the combined frequency of theother members of a confused word set provides a0th order likelihood of the word being correct.
How-ever, because of the phenomenon of frequens, thisis a highly unreliable stimate and we therefore s-chew it no matter how great the order 0 likelihoodof error may be.
However, some commercial word-processors seem to use an order 0 model and flag alloccurrences of certain words (Kernick, 1996).We therefore look for significant environments oflarger diameter and estimate the probability of eachalternative in terms of their relative frequency inthat environment.
An environment of diameter N-1corresponds to an N-gram and environment statis-tics are therefore derived from N-gram statistics.
Inpractice we limit ourselves to near symmetric envi-ronments with the target word in the centre.
Thisgives us a unique environment for each even diame-ter, but a pair of environments designated +N (moreleft context) and -N (more right context) for eachodd diameter N.Our algorithm stores statistics only for significantenvironments and increases the diameter progres-sively, up to a predefined maximum diameter or un-til a specified certainty threshold is reached.
Notethat we overload our term 'environment' o meannot only a specific environment of a target word inthe current text, but the set of environments of thetarget word in the training corpus and having thespecified diameter, as summarized in the collectedstatistics.
Our usage will be clear from context, andwill vary according to whether we are talking abouttesting/text or training/corpus (respectively).This approach to the storing of Differential Gram-mars helps to keep the requirements for a given con-fused word set small, and thus contributes to ourgoal (4) of minimizing the size of the grammar, with-out significantly affecting reliability (precision anddetection rate).Powers 90 Differential Grammarsi6 Significance ~ likehood estimatesWe now discuss the different models of significancewe have experimented with, and the issue of com-bining information from multiple environments.
Wewill illustrate this with examples relating to the com-mon substitution 'from' ~ 'form', and we will as-sume that a desired minimal level of significance shas been specified (0.95 in the examples).
Note thatmodels which are statistically significant are alsolikely to be skewed so as to provide good discrim-ination, but the reverse is not iri general true.
Themore data we have, the more statistically significanta particular likelihood ratio is - so we also want toensure 'significance' in a more application-orientedsense and only store information which is both sig-nificant and discriminative.Since we are training on a large corpus, and allow-ing for potentially many confusion sets from a hugeset of possibilities, we want to eliminate as quicklyas possible those pairs which can't possibly be dis-criminated reliably.
For this purpose we introduceda first order test based on the binary Laplacian esti-mator, and require that the number of instances of aspecific environment Na for the confusion set satis-fies Na > 1 / (1 -  s ) -  2 (where d is its diameter).
Forour modest 95% significance level, only 18 examplesare required.
Note that, as previously discussed, wedo not ever directly use the 0-diameter environmentalone to determine likelihoods, but rather we insistthat environments must be more significant han the~0-environment in terms of the Laplacian test.Next we want to ensure that the each environ-ment is not only significant in its own right but issignificantly different from the next smaller environ-ment.
The example of 1105 instances of ' from' and5 of ' form' being reduced to 47:1 (Kernick, 1996) issignificant according to our Laplacian estimate, butthe 1 could just have easily been a 0 or a 2 andthus doesn't improve on the smaller environment.The larger environment is not significantly differentaccording to a likelihood derivative, which consid-ers the rate of change of the ratio (Kernick, 1996),and the difference between the two environments ialso not significant by Fisher's exact test (Winston,1993) (or the closely related G 2 (Kilgarriff, 1996))which assesses the probability that the distributionis due to chance.Note that we make no attempt to correct orsmooth data, since 0% and 100% likelihoods are notundesirable, and the 'corrections' are as likely to dis-tort as to improve the data (Church and Gale, 1991),rather we discard data which does not reach signif-icance.
Our current model also discounts any caseshandled by a larger environment (see section 9).This leaves two further issues to discuss.
The firstis how we determine likelihoods.
Normally, it isvery simple, we take the biggest environment thatmatches and simply use the relative probabil ity ofthe target word in the sampled environment.
In thecase of odd diameters, both a left and a right envi-ronment may exist and these may agree or disagree.Here we use the rain operator to combine them -the minimum fits both the case where they tend toagree, and is conservative, or when they tend to dif-fer, when if one side think it is wrong it is probablynot correct.
However, this is not always the case.An actual example from our experiment on Usenettext is: "I don't know where you're coming from onthis" where the diameter one environments stronglysuggest "coming from" and "form on" respectively.With a larger corpus a significant diameter two envi-ronment for ' from' would form to handle this idiom.The second question is how to allow for the biasingeffect of frequens, where we tend to make dispropor-tionately more errors in the direction of the more fre-quent word, since this is also the direction our statis-tics are pointing.
This effect however can have itsimpact reduced by taking larger environments andprovides support for the intuitively and empiricallydetermined threshold function of (Kernick, 1996),which converts the user supplied precision (?)
to adiscrimination threshold value (/9) which increasesas the diameter (d) of the environment reduces, thusgiving more credence to the larger diameter environ-ments: 0 = ?
?
(1 - ?
)/2d.7 Focussing on syntactic ontextsLimiting our environments to a small diameter al-ready biases toward correlations which express yn-tactic rather than semantic relationships (Finch,1993), which very phenomenon is responsible for thesuccess of Ngram models as an alternative to a gram-mar (Charniak, 1993).
Semantic associations arenormally found in larger segments in which the fre-quency of semantically primed words is higher thanexpected on the basis of relative frequency in the cor-pus or the language (this locally increased frequencyhas recently been dubbed 'clumping' (Church andGale, 1995) and various measures have been pro-posed to compensate for it (Kilgarriff, 1996)).Another statistical attribute which is associatedwith the syntax/semantics di tinction is the raw fre-quency of a word.
The most frequent words tend tobe syntactic in nature, and will often function wordsor closed class words.
The less frequent words tendto be more content words or open class words, likenouns and verbs (Kilgarriff, 1996).
Due to the ex-Powers 91 Differential Grammarstreme skewing of the frequency distribution, varyinginversely with rank according to Zipf's law, the first150 words of a corpus with a lexicon of 250,000 wordscover over half of the corpus (Kernick, 1996).Collecting statistics based on these 150 'eigen-words', almost all of which are function words, givesus our a syntactic bias and we used the standardUnix l i s t /us r / l ib /e ign  in our initial experiments.Furthermore since the function words tend to actat fairly close quarters, these words are appropri-ate for smallish environments.
However, we don'twant our statistics to be restricted to environmentscomposed solely of the 150 eigenwords, and we donot want to have to resort to just bigram statisticscollected at various displacements (Finch, 1993).
In-deed for function words, for anything but the small-est displacements, our experiments show that suchbigram statistics quickly approach corpus/authornorms (Kernick, 1996).
The obvious step is to intro-duce an 'open class', denoted by 'O', as a placeholderfor the rest of the lexicon.
But we can to do betterthan this by finding other useful classes which areeasy to discover using our collected statistics.
Wetherefore move our search for syntactic ues to themorphological level.
Again, rather than seeking todevelop a formal morphology and associate gram-matical information with the morphemes, we simplykeep additional statistics for words classified by themost common affixes (we use 12 suffixes for English).Note that our residue class now represents the nullmorph, '0'.After we include numbers and punctuation we endup with a nominal 172 'eigenunits', but irregular orproblematic forms could usefully be added to re-duce the noise in these blindly recognized classes,e.g, classes may have multiple syntactic functions ('-s' and the null morph '-0' can indicate a noun or averb) and/or fortuitous mismatches ('-ed' and '-ing'will accept 'red' and 'ring').Fortunately, such mismatches have a good chanceof already being one of the 150 eigenwords (betterthan 50%) and a low probability of occurrence in anyparticular slot (a fraction of 1%).
Those occurrenceswhich are not systematic simply contribute to theoverall noise in the method, whilst those which aresystematic actually contribute to the success of thetechnique!In addition, a broad definition of affix as a word-initial or -final sequence can give us affixes whichmay deviate from the morphological.
In our first ex-periment we use only 12 hand-chosen suffixes, but insubsequent experiments we also split each of theseclasses according to whether they had a vowel or aconsonant prefix, which permits us to ensure thatwe can deal with the 'a/an' distinction.
Later weinvestigate how affixes may be discovered automati-cally.
Note that (Entwisle and Groves, 1994) use es-sentially the same crude affix information to achievecomplete parsing/validation f English sentences us-ing a (computationally expensive) constraint parser.This now allows us to complete the definition ofour syntactic environment: The Ngram informationis reduced to eigenunit environments by simply re-placing each word other than the target by the firstmatching eigenunit (eigenwords are checked beforeaffix classes, shortest first).8 In ter faceIn addition to the learning of Differential Grammarsusing the pure syntactic methods defined above, andtesting on 'known good' and 'expected bad' text, wehave also paid some attention to the user interface.Two interfaces are available, an emacs interface - itworks just like the spell checker - and a frame-basedweb-interface.
We used the likelihoods to colour thewords so that the words which are more likely tobe wrong are highlighted more strongly.
Also theenvironment used to make the decision is highlightedcontrastingly.
This is useful both for the user, andfor the developer, in evaluating and enhancing theperformance of the system.We also allow the user to change the threshold atwhich notification of potential errors occurs.
Nor-mally this is set at a relative probability 0.75 for thetarget word relative to other members of the confu-sion class, a precision setting of 75% (our results arepresented for this setting).
If this threshold is ex-ceeded the most likely replacement is automaticallyproposed.9 Resu l tsTable 1 presents results from an experiment using100Meg of training text (TIPSTER, 1994) and threetest texts of similar size but different character, inwhich Differential Grammars are trained and used togrammar check the test texts, which are also checkedby two commercial systems.
Our methodology issummarized generally in Fig.
1.We trained Differential Grammars for 78 confu-sion pairs using 161 eigentokens and a 95% signifi-cance level and tested the grammar checker at thedefault 75% likelihood threshold.
Performance wascomparable with that of the two commercial sys-tems, but all three systems howed individual cov-erage characteristics.
The confusion pair ' its/it 's'was responsible for our poorer performance on thenewsgroup corpus (SFB), but we demonstrated thatPowers 92 Differential GrammarsGRAMMAR CHECKER THC-f THC-t SFK-f SFK-t SFB-f SFB-t SFB-itsMicrosoft  Nord 282 0 328 17 225 40 13Word Per fect  75 0 116 21 77 49 9Differentia175~ 158 6 170 33 165 19 19Table 1: Initial results (Kernick, 1996) comparing three systems on three 12000 line texts of approximately 100000words each.
The -f columns represent false errors (lower ~f better) and the -t columns show true errors (higher isbetter).
Our system could not resolve 'its/~t's' which was the most common error in SFB so the final -its columnshows the results with these errors discounted.
The three corpora were chosen to be as similar as possible, includingone published computer-related work (THC), science fiction genre text written by a member of our team (SFK), andtext of the same genre taken from a newgroup (SFB).Identify/model potent ia l  confusion pa i rsBuild signif icantlDGs fo r  themEnsure su f f i c ient  instances of pa i rCol lect legal  eigenunit environmentsAnalyze contexts  of s i ze  one to l imiti f  not s ign i f i cant  data aborti f  use fu l  s tore  and cont inueScan and correct  ?ext sampleFor each potent ia l  confused wordCo l lec t  maximal e igencontextsReport b iggest  context  above thresho ldFigure 1: Summary of DG methodology used.the statistics for this pair invalidated our assump-tion of ergodicity across the three different 12000 linetest corpora used.
Also our initial prototype couldnot distinguish 'a /an '  correctly.
Conversely, on sup-posedly correct published text, we found six errorswhich had been missed by human proofreaders andcommercial systems alike.
For the record, these er-rors consisted of four 'was/were', one 'affect/effect',and one 'are/our'  substitution.
The first two errorsare clear syntactic errors where the semantics is es-sentiMly the same.
The second is a very commonphono-frequens where, as different parts of speech,resolution is again straightforward.
Note that a 95%precision setting should have been sufficient o findthem, but would have eliminated around 80% of thefalse errors.
The most difficult of these errors to re-solve is the 'was/were' error because of the higherlikelihood of a parenthetic intervention, which alsocontributes to the problem with ' its/ it 's ' .An example from (THC) demonstrating an un-likely usage of 'its', which requires a context of morethan ten words to resolve, illustrates the problem ofparenthetic intervention:I t s  spec ia l ty  magaz ines ,  suchas *Te lephony ,*  *AT&T Techn ica lJ ourna l , *  *Te lephone  Eng ineer  andManagement,*  are  decades  o ld ;  theymake computer pub l i ca t ions  l i ke*Macworld* and *PC Week* look  l i keamateur  j ohnny-come- la te l ies .Another factor which causes evere problems with' i ts/ i t 's '  is the extreme sensitivity of its differentialgrammar to contexts.
Even the raw counts illus-trate this quite clearly, and a far more representativetraining corpus will be needed to resolve the questionof whether an adequate differential grammar can bebuilt for this case: see Table 2.The method we used to cope with the 'a /an '  pairis simple and effective, but increases the number ofadditional affix classes from 13 to 26 as each is splitaccording to whether it starts with a vowel or not.This increase the size of the eigenset o 174, but inaddition we added 20 h-words and 2 y-words whichtake 'an', giving a total of 196 eigenunits.
We il-lustrate what the eigenset now looks like in Table3, where we present he top 15 eigenunits and theiroccurrence counts.The affix information in Table 3 is equivalentto the cross-product of 26 prefixes with 13 suffixes(counting the 0-morph) and would have tripled thenumber of classes required if we hadn't  made thepreclassification i to consonant and vowel.
This isrelevant as we go on to consider how our affix infor-mation could be derived automatically.One of stated our aims was to seek to learn thesyntactic information we use, but, in fact, we haveused a set of 12 hand-chosen syntactically significantsuffixes in the grammar checking discussed above,along with 150 words chosen on the basis of fre-quency, to which we have now added a pair of phono-logically motivated features.
We have therefore x-perimented with the automated iscovery of an ap-propriate set of words and affixes.For this purpose we sought o derive a set of maxi-mal Ngrams which were significant but were not partof any larger significant Ngrams.
Allowing Ngramsof different sizes means we are double counting somestrings, and it is thus usual to deduct from a givenNgram prefix the frequencies of all N+l -grams whichit prefixes, and similarly for suffixes.
Using these assignificance measures, however, tends to lead to uspicking up not only frequent words and affixes, butfrequent phrases and all proper substrings of eachof these.
Furthermore the last character of a suf-fix may well be involved in many other words andsuffixes and thus tends to appear more significant.Powers 93 Differential GrammarsNORD RSV RSV-i THC THC-i SFJ SFJ-iits 1344 1370 179 186 210 215i t ' s  0 0 49 113 1138 1644TOTAL: 757523 757523 106433 106433 414114 414114Table 2: Corpus sensitivity of ' its/ it 's'  shown with both case sensitive and insensitive (-i} counts taken respectivelyfrom the corpora RSV, THC and SFJ.PERCENT COUNT NGRAN PERCENT COUNT NGRAN PERCENT COUNT NGRAN29 223225 C- 3 26854 C-s 1 9577 you8 61033 the  2 21046 to  1 9140 fo r5 40812 and 1 13398 C-ed 1 9079 a4 37855 V- 1 11762 in 1 8009 i4 31491 of 1 9874 he 1 7837 hisTable 3: Most significant 15 eigenunits with frequencies.
'V-~C-' are respectively all words that a vowel/consonantthat are not matched as words or with specific suffixes like 'C-s C-ed'.
The corpus (RSV) was selected to be topicallyfocussed and of convenient size (757523 words).We therefore used a related heuristic in which werequired that a unit be significant in both contextsin order to be treated as significant, and achievedthis by double discounting - subtracting counts forboth prefixes and suffixes.
Although this methodwas intended as only a rough ranking for examiningthe results, it did indeed provide more useful infor-mation than either of the more principled discountsor their maximum or sum, for which again frequentwords were represented multiply.
With our doublediscount, words which are almost always used as partof a bigger significant string will end up heavily neg-atively weighted, and thus the heuristic is likely toprefer to embed it in a larger string - see Table 4.N RIGHT LEFT BI COUNT NGRAM6 2241 2241 2241 2241 #th is#6 2050 2050 2050 2050 #the#b6 1907 1907 1907 1907 e#and#5 1886 1886 1886 1886 #of#a4 1840 1840 1840 1840 #me#5 1809 2377 1809 2377 #had#1 2233 2699 1801 3131 -3 1767 2201 1767 2201 #ba5 1761 2285 1761 2285 #one#7 2228 3257 1734 3751 #the i r#7 2074 1624 1624 2074 e,#and#4 1597 1597 1597 1597 #we#9 1591 2175 1591 2175 s#of#the#3 1588 1588 1588 1588 's#10 1579 1579 1579 1579 #from#the#Table 4: '# '  represents pace.
These are the 15 mostsignificant maximal strings from an experiment whichsought to discover the eigenunits of RSV as discountedNgrams.
Significance threshold was set at the 99.99%level, and contexts were discounted by the frequency ofany significant contexts which extended them to the right,to the left, or either.While our eigenwords and hand-selected suffixestended to be proposed relatively quickly, it willbe observed that many actually occurred as partof strings which crossed word boundaries.
More-over, without some segmentation i formation thetechnique is sensitive to the significance thresh-old, which has a direct influence on the length ofthe Ngrams proposed.
Limiting to maximal space-bounded 'words' is however easonable in this appli-cation, but since we need to include punctuation andnumbers in our eigenclass, we do not to filter theseout.
The top 75 candidates then consist almost en-tirely of Unix eigenwords, plus corpus eigenwords'god' and 'lord', some punctuation, some standardaffixes, some combinations of punctuation and af-fixes, and some unexpected candidate affixes.
In factsome of these candidates, '-e -es', are not at all un-reasonable: '-es' is a variant of '-s' and both can fitin the same slot as '-ed'.
But others, 'bo- ba- ne-',are harder to make sense of.
The next 75 strings aresimilar with a higher proportion of affixes, both syn-tactic (6/12 now covered) and non-syntactic (20), aswell as two unclassifiable sequences ('rai ob').Thus, it is clearly easy to obtain a fair approxima-tion to our list of eigenunits, and the fact that 10 or20% of them may not satisfy our syntactic expecta-tions does not preclude them from being useful andwill not necessarily worsen the results.
For example,we note that our 24 prefixes handle resp.
33% and20% of the 'a/an' cases covered by our 'V*' and 'C*'classes.
As long as we are not overwhelmed by poorcandidates, our eigenset will still be able to meet itsgoal.An automatically generated eigenset, of the samesize as our original 172 eigenunit version, included80 of our original eigenwords which covered 54% (theUnix 150 covers 60%) of the corpus, and included 7of our original suffixes covering an additional 12%(our handpicked 12 cover 13%).
On the other hand,it proved that one of our hand-selected suffixes wasnot very significant in the corpus ('-ic') and occurredonly 92 times (the .0001 threshold sets significanceat 75 occurences).
The last of the other suffixes ('-ble') to be proposed had rank 503, again because ofPowers 94 Differential Grammarslarger significant contexts '-able ble-', which causedit to be discounted as a suffix in its own right.Forcing a word-boundary between words andpunctuation increases the rate at which eigenunitsare found, as combinations ofletters and punctationsconstitute the majority of the dross.
Word-internalapostrophe (but not hyphen) is treated as a letterfor this purpose.10 ConclusionsDifferential Grammars allow high-order Ngramstatistics to be focussed on the problem of decid-ing between the correct and one or more incorrecttokens, reducing Ngram contexts to environmentsbased on high frequency eigentokens: words, num-bers, punctuation and affixes.
Using the 150 Unixeigenwords gives us a 50% likelihood that we willhave a hit in any slot, while our 12 non-zero suffixesincreases the coverage to 25%, ensuring that goodsyntactic relevance is obtained.In further smaller experiments, we demonstratedthat the 'a/an'  distinction could be handled by split-ting our suffix and open eigenunits into vowel andconsonant subclasses.
We further demonstrated thatsimilarly appropriate igenunits could be automat-ically derived on a discounted frequency basis, us-ing a crude heuristic to order the potential eige-nunits, while restricting them to the form of lex-ical, space-bounded, words.
Experiments involvingtraining with an automatically derived eigenset haveyet to be performed, and will focus on deciding theoptimum size of eigenset and development of an im-proved heuristic.The eigenset has two functions: to allow us toreduce the size of the tree for a given performancelevel, and to allow us to reduce the role of genre andsemantic related fluctuations in word frequencies byconcentrating on features of relatively high syntacticsignificance.
Increasing the size of the eigenset isexpected to decrease performance due to increasednoise after a certain point.
Similarly, increasing thesize of the eigenset may eventually tend to increasethe size of the stored differential grammars withoutsignificant gain in precision.The use of a significance factor in the trainingstage allowed the size of the trees generated by thedifferential grammar generator to be limited to whatwas necessary to achieve that level of precision on thetraining corpus, whilst the likelihood values storedin the tree allowed the user to be informed of thelikelihood of an error (using colour or upon query),and to control the threshold for which errors wouldbe reported.Maximum diameter is another parameter of thetraining stage, and experiments on optimal size andthe role of diameter in relation to syntactic and se-mantic words were undertaken early on in setting 10as the size beyond which environments were unlikelyto reach significance.
If generation of the grammarwas stopped due to lack of significance, the problemwas often lack of data.
If the search was terminatedat maximum diameter it was an indication that thewords were functionally similar, and most likely thesame part of speech.The differential grammar approach as proven tobe a successful way of applying statistical, Ngram-like, techniques for practical grammar-checking in amodest computing environment, with useful gram-mar trees requiring of the order of 100 to 1000 bytesof storage per confused word pair in most cases.
Thisreport has concentrated on presenting empirical re-sults for a single system, rather than on optimiza-tion of the system, and there remains considerablescope for investigation of the role of the system pa-rameters and optimizing the eigenset, for which onlythe primary considerations have been outlined.
Theprimary deficiency of the system is its inability tocope with arbitrarily long parentheses or subclauseswhich separate syntactically bound elements, but itis also rather sensitive the genre and representative-ness of the training corpus.AcknowledgementsThis work was undertaken jointly with Philip Kernickwho, in particular, implemented the user interfaces andcarried out the exlferiments whose results are reportedin Table 1 as part of an Honours project (Kernick, 1996).ReferencesEugene Charniak.
1993.
Statistical Language Learn-ing.
MIT Press.Kenneth W. Church and William A. Gale.
1991.A comparison of the enhanced Good-Turingand deleted estimation methods for estimat-ing probabilities of English bigrams.
ComputerSpeech and Language, 5:19-54.Kenneth W. Church and William A. Gale.
1995.Poisson mixtures.
Journal of Natural Lan-guage Engineering, 2:163-190.Jim Entwisle and M. Groves.
1994.
A method ofparsing English based on sentence form.
NeM-LaP.Steven P. Finch.
1993.
Finding Structure in Lan-guage.
Ph.D. Thesis, University of Edinburgh.Powers 95 Differential GrammarsIll editors.
1983.
Special issue on Word-Sense Disam-biguation.
Computational Linguistics 9#3-4.E.
Johnson.
1992 The Ideal Grammar and StyleChecker Text Technology 2#4Philip Kernick.
1996.
A Statistical GrammarChecker.
Honours Thesis, Flinders Universityof South AustraliaAdam Kilgarriff 1996 Which words are particularlycharacteristic ofa text?
A survey of statisticalapproaches.
ITRI Technical Report, Univer-sity of BrightonDonald E. Knuth.
1973.
The Art of Computer Pro-gramming, Vol.3.
Addison Wesley.RSV: United Bible Society.
The Bible.
Revised Stan-dard Version.SFB: News Contributors.
1996.
Contributions tousenet group aus.sf.babylon5.SFJ: J. M. Straczynski.
1997.
Contributions tousenet group alt.tv.babylon-5, Jul 93 to Feb97.SFK: P. Kernick.
1996.
Science Fiction writingsTHC: Bruce Sterling.
1992.
The Hacker Crackdown.Bantam Books.
Electronic version 1994.TIPSTER Information Retrieval.
1994.
Text Re-search Collection Vol.2.
University of Penn-sylvaniaB.
E. Wampler.
1995.
Risks of grammar checkers.The Risks Digest 17#54, Dec 1995.Patrick Winston.
1993.
Artificial Intelligence.
3rdEdition.
Addison Wesley.Powers 96 Differential Grammars
