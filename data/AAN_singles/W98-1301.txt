The Proper Treatment of Optimalityin Computational PhonologyLauri KarttunenXerox Research Centre Europe6, chemin de Maupertuis38240 Meylan, FranceAbstract.
This paper presents a novel formalization of optimality theory.
Unlike pre-vious treatments of optimality in computational linguistics, starting with EUison (1994),the new approach does not require any explicit marking and counting of constraint vi-olations.
It is based on the notion of "lenient composition", defined as the combinationof ordinary composition and priority union.
If an underlying form has outputs that canmeet a given constraint, lenient composition enforces the constraint; if none of the outputcandidates meets the constraint, lenient composition allows all of them.
For the sake ofgreater efficiency, we may "leniently compose" the GEN relation and all the constraintsinto a single finite-state transducer that maps each underlying form directly into its op-timal surface realizations, and vice versa.
Seen f~om this perspective, optimality theolTis surprisingly similar to the two older strains of finite-state phonology: classical rewritesystems and two-level models.
In particular, the ranking of optimality constraints corre-sponds to the ordering of rewrite rules.1 IntroductionIt has been recognized for some time that Optimality Theory (OT), introduced by Princeand Smolensky \[24\], is from a computational point of view closely related to classi-cal phonological rewrite systems (Chomsky and Halle \[11) and to two-level descriptions(Kosksnniemi \[21\]).Ellison \[61 observes that the ~EN function of OT can be regarded as a regular relationand that OT constraints seem to be regular.
Thus each constraint can be modeled as atransducer that maps a string to a sequence of marks indicating the presence or absenceof a violation.
The most optimal solution can then be found by sorting and comparingthe marks.
Frank and Satta \[7\] give a formal proof that OT models can be construedas regtdar relations provided that the number of violations is bounded.
Eisner \[3, 4, 5\]develops a typology of OT constraints that corresponds to two types of rules in two-leveldescriptions: restrictions and prohibitions.The practice of marking and counting constraint violations is closely related to the tableaumethod introduced in Prince and Smolensky for selecting the most optimal output can-didate.
Much of the current work in optimality theory consists of constructing tableauxthat demonstrate the need for particular constraints and rankings that allow the favoredcandidate to emerge with the best score.From a computational viewpoint, this evaluation method is suboptimal.
Although thework of ~EN and the assignment of violation marks can be carried out by finite-statetransducers, the sorting and counting of the marks envisioned by Ellison and subsequentwork (Walther \[26\]) is an off-line activity that is not a finite-state process.
This kindof optimality computation cannot be straightforwardly integrated with other types oflinguistic processing (morphological analysis, text-to-speech generation etc.)
that arecommonly performed by means of finite-state transduction.This paper demonstrates that the computation of the most optimal surface realizationsof any input string can be carried out entirely within a finite-state calculus, subject tothe limitation (Frank and Satta \[7\]) that the maximal number of violations that need tobe considered is bounded, we  will show that optimality constraints can be treated com-putationally in a similar m~-ner to two-level constraints and rewrite rules.
For example,optimality constraints can be merged with one another, respecting their rAniclug, just asit is possible to merge rewrite rules and two-level constraints.
A system of optimalityconstraints can be imposed on a finite-state l xicon creating a transducer that maps eachmember of a possibly infinite set of lexicai forms into its most optimal surface realization,and vice versa.For the sake of conciseness, we limit the discussion to optimality theory as originallypresented in Prince and Smolensky \[24\].
The techniques described below can also beapplied to the correspondence v rsion of the theory (McCarthy and Prince \[22\]) thatexpands the model to encompass output/output constraints between reduplicant andbase forms.To set the stage for discussing the application and merging of optimality constraints itis useful to look first at the corresponding operations in the context of rewrite rules andtwo-level constraints.
Thus we can see both the similarities and the differences amongthe three approaches.2 Background: rewrite rules and two-level constraintsAs is well-known, phonological rewrite rules and two-level constraints can be implementedas finite-state transducers (Johnson \[9\], Karttunen, Koskenniemi and Kaplan \[14\], Kaplanand Kay \[10\]).The application of a system of rewrite rules to an input string can be modeled as a cascadeof transductions, that is, a sequence of compositions that yields a relation mapping theinput string to one or more surface realizations.
The application of a set of two-levelconstraints is a combination of intersection and composition (Karttunen \[18\]).To illustrate the idea of rule application as composition, let us take a concrete example,the well-known vowel alternations in Yokuts (Kisseberth \[20\], Cole and Kisseberth \[2\],McCarthy \[23\]).
Yokuts vowels are subject to three types of alternations:- Underspecified sui~ voweis are rounded in the presence of a stem vowel of the sameheight: dub+bin ~ dubhun, bok'+Al ~ bok'oL- Long high vowels are lowered: fu:t+It -~ fo:tut, mi:k+lt -4 me:t~it.- Vowels are shortened in closed syllables: sa:p --+ sap, go:b?hln -~ gobhin.Because of examples such as fu:t?hln -~ .~othun, the rules must be applied in the givenorder.
Rounding must precede lowering because the suir~ vowel in ?u:t+hln emerges asu.
Shortening must follow lowering because the stem vowel in fu:t+hln would otherwiseremain high giving futhun rather than fothun as the final output.These three rewrite rules can be formalized straightforwardly asregular eplace xpres-sions (Karttunen \[19\]) and compiled into finite-state ransducers.
The derivation 7u:t?hlnfothun can thus be modeled as a cascade of three compositions that yield a transducerthat relates the input directly to the final output.The first step, the composition of the initial network (an identity transducer on thestring fu:t?hln) with the rounding transducer, produces the network that maps between?a:t+hln and fu:t?hun.
The symbol, o. in Figure 1 denotes the composition operation.It is important to realize that the result of each rule application in Figure 1 is not anoutput string but a relation.
The first application produces a mapping from ?u:t+hlnto ?u:t+hun.
In essence, it is the original Rounding transducer restricted to the specificinput.
The resulting network represents a relation between two languages (= sets ofstrings).
In this case both languages contain just one string; but if the Rounding rulewere optional, the output language would contain two strings: one with, the other withoutrounding.
?u : t+h In?u : t+h In?0?Rounding+?u : t+hun?u : t+h In'}u : t+h In.o.Rounding.o.Lowering+?o : t+hunFigure 1.
Cascade of rewrite rule applications.
?u : t+h In?u : t+ l~In.O,Rounding.o.Lowering.o.Shortening+?ot+hunAt the next'step in Figure 1, the intermediate output created by the Rounding transduceris eliminated as a result of the composition with the Lowering transducer.
The final stageis a transducer that maps directly between the input string and its surface realizationwithout any intermediate stages.We could achieve this same result in a different way: by first composing the three rulesto produce a transducer that maps any underlying form directly to its Yokuts surfacerealization (Figure 2) and then applying the resulting single transducer to the particularinput.+Rounding.0.Lowering.o.ShorteningFigure 2.
Yokuts vowel alternations.mmmmmuThe small network (21 states) pictured in Figure 2 merges the three rules and thusrepresents he complexity of ?okuts vowel alternations without any "serialism', that is,without any intermediate r presentations.In the context of the two-level model, the Yokuts vowel alternations can be describedquite simply.
The two-level version of the rounding rule controls rounding by the lexicalcontext.
It ignores the surface realization of the trigger, the underlyingly high stem vowel.The joint effect of the lowering and shortening constraints i  that a lexical u: in .~u:t-l-hInis realized as o.
Thus a two-level description of the Yokuts alternations consists of threerule transducers operating in parallel (Figure 3).IRoondi"gl ILowo"n l (S"o onin IFigure 3.
Parallel two-level constraints.3mmmThe application of a two-level system to an input can be formaiized as intersecting com-position (Karttunen \[18\]).
It involves constructing a partial intersection of the constraintnetworks and composing it with the input.
We can of course carry out the intersectionof the rules independently of any particular input.
This merging operation results inthe very same 21-state transducer as the composition of the corresponding rewrite rulespictured in Figure 2.Thus the two descriptions of Yolmts sketched above are completely equivalent in thatthey yield the same mapping between underlying and surface forms.
They decompose thesame complex vowel alternation relation in different ways into a set of simpler elationsthat are easily understood and manipulated, r As we will see shortly, optimality theorycan be characterized as yet another way of achieving this kind of decomposition.The fundamental computational operation for rewrite rules is composition, as it is in-volved both in the application of rules to strings and in merging the rules themselves.For two-level rules, the corresponding operations are intersecting composition and inter-section.Turning now to optimality theory, our main interest will be in finding what the corre-sponding computations are in this new paradigm.
Wh.
at does applying a constraint meanin the context of optimality theory?
Can optimality constraints be merged while takinginto account heir ranking?-mmmmmmmmm3 Optimality theoryOptimality theory (Prince and Smolensky \[24\]) abandons rewrite rules.
Rules are replacedby two new concepts: (1) a universal function called GEN and (2) a set of ranked uni-versal constraints.
GEN provides each input form with a (possibly infinite) set of outputcandidates.
The constraintseliminate all but the best output candidate.
Because manyconstraints are in conflict, it may be impossible for any candidate to satisfy all of them.The winner is determined by taking into consideration the language-specific ranking ofthe constraints.
The winning candidate is the one with the least serious violations.In order to explore the computational ~pects of the theory it is useful to focus on aconcrete xample, even simpler than the Yolmts vowel alternation we just discussed.
2 Wewill take the familiar case of syllabification constraints discussed by Prince and Smolensky\[24\] and many subsequent authors (Ellison \[6\], Tesar \[25\], Hammond \[8\]).3.1 GEN for  sy l lab i f i ca t ionWe assume that the input to OEN consists of strings of vowels V and consonants C. GENallows each segment o play a role in the syllable or to remain "unparsed".
A syllablecontains at least a nucleus and possibly an onset and a coda.Let us assume that GEN marks these roles by inserting labeled brackets around eachinput element.
An input consonant such as b will have three outputs 0\[b\] (onset), D\[b\](coda), and X\[b\] (~mparsed).
Each vowel such as a will have two outputs, N\[a\] (nucleus)and x \[a\] (unparsed), In addition, GEN "overparses", that is, it freely inserts empty onset0 \[ \], nucleus N \[ \], and coda D I" \] brackets.For the sake of concreteness, we give here an explicit definition of QEN using the notationof the Xerox regular expression calculus (Karttunen.
et al\[15\]).
We define GEN as the com-position of four simple components, Input, Parse, 0verParse~ and $yl lableStructure.The definitions of the first three components are shown in Figure 4.1 For more discussion of these issues, see Karttunen \[17\].2 The Yokuts case is problematic for Optimality theory (Cole and Kisseberth \[2\], McCarthy \[23\])because rounding depends on the height of the stem vowel in the underlying representation.
Cole andKisseberth offer a baroque version of the two-level solution.
McCarthy strives mightily to distinguishhis "sympathy" candidates from the intermediate r presentations postulated by the rewrite approach.mmmmmmmmmmmnmmmdefine Input \[C J V\]*define Parse C -> \["0\[" I "D\[" J "X\["\] .
.
.
"\]".0 .v-> \["NC" I "x\["\] .
.
.
"\]"define OverParse \ [ .
.
\ ]  (->) \["O\["I"N\["I"D\["\] "\]" ;F igure 4.
Input, Parse, and OverParseA replace expression of the type A -> B .
.
.
C in the Xerox calculus denotes a relationthat wraps the prefix strings in B and the sutF~ strings in C around every string in A.Thus Parse is a transducer that inserts appropriate bracket pairs around input segments.Consonants can be onsets, codas, or be ignored.
Vowels can be nuclei or be ignored.0verParse inserts optionally unfilled onsets, codas, and nuclei.
The dotted brackets \[.?
\] specify that only a single instance of a given bracket pair is inserted at any position.The role of the third GEN component, Syl lableStructure, is to constrain the output ofParse and 0verParse.
A syllable needs a nucleus, onsets and codas are optional; theymust be ~ the right order; unparsed elements may occur freely.
For the sake of clarity,we define Syl lableStructure with the help of four auxiliary terms (Figure 5).define Onset "0\[" (C) "\]" ;def ine Nucleus "N\[" (V) "\]" ;def ine Coda "D\[" (C) "\]" ;define Unparsed "X\[" \[ClV\] "\]" ;define Syl lableStructure \[\[(Onset) Nucleus (coda)\]/Unparsed\]* ;Figure 5.
SyllableStructureRound parentheses in the Xerox regular expression otation indicate optionality.
Thus(C) in the definition of Onset indicates that onsets may be empty or filled with a con-sonant.
Similarly, (Onset) in the definition of SyllableStructture means that a syllablemay have or not have an onset.
The effect of the / operator is to allow unparsed conso-nants and vowels to occur freely within a syllable.
The disjunction \[CJ V\] in the definitionof Unparsed allows consonants and vowels to remain unparsed.With these preliminaries we can now define GEN as a simple composition of the fourcomponents (Figure 6).def ine GEN Input.o.OverParse.o.Parse.0 .Syl lableStructnre ;F igure 6.
GEN for syllabificationWith the appropriate definitions for C (consonants) and V (vowels), the expression inFigure 6 yields a transducer with 22 states and 229 arcs.It is not necessary to include Input in the definition of GEN but it has technically abeneficial effect.
The constraints have less work to do when it is made explicit that theauxih'ary bracket alhabet is not included in the input.Because QEN over- and underparses with wild abandon, it produces a large number ofoutput candidates even for very short inputs.
For example, the string a composed withtEN yields a relation with 14 strings on the output side (Figure 7).SialN\[a\]10N\[a\]DD~Qsra\]NON\[a\]NQ~ON\[a\]D0NOXta\]N\[~X\[a\]NC\]NDX\[a\]DOOON\[a3OON\[a\]NDOOS\[aJVOOOX\[a)SOX\[a\]NOFigure 7.
GEN applied to aThe number of output candidates for abracadabra is nearly 1.7 million, although thenetwork representing the mapping has only 193 states, It is evident hat working withfinite-state tools has a significant advantage over manual tableau methods.3.2 Syl labif icat ion const ra intsThe syllabification constraints of Prince and Smoleusky \[24\] can easily be expressed asregular expressions in the Xerox calculus.
Figure 8 lists the five constraints with theirtranslations.Syllables must have onsets.Syllables must not have codas.Input segments must be parsed.A nucleus position must be filled.An onset position must be filled.define HaveOns N\[" ,~> "0\[" (C) "\]" .
;def ine NoCoda "$"D\[" ;def ine Parse  "$"X\[" ;def ine Fi l lNuc "$\[ "N\[" "\]" \] ;def ine Fi l lOns "$ \[ "0 \[ .
.
.
.
\]" \] ;Figure 8.
Syllabification constraintsThe definition of the llave0ns constraint uses the restriction operator =>.
It requires thatany occurrence of the nucleus bracket, IN, must be immediately preceded by a filled 0\[C\]or unfilled 0\[ \] onset.
The definitions of the other four constraints are composed of thenegation" and the contains operator $.
For example, the NoCoda constraint, "$"D\[", canbe read as "does not contain D~.
The FillNu?
and Fi l l0ns constraints forbid emptynucleus S\[ \] and onset 0\[ \] brackets.These constraints compile into very small networks, the largest one, Have0ns, containsfour states.
Each constraint network encodes an infinite regular language.
For example,the ilaveOns language includes all strings of any length that contain no instances of N\[at all and all strings of any length in which every instance of N \[ is immediately precededby an onset.The identity relations on these constraint languages can be thought of as filters.
Forexample, the identity relation on ilave0nz maps all llave0ns trings into themselves andblocks on all other strings.
In the following section, we will in fact consistently treat heconstraint networks as representing identity relations.3.3 Const ra in t  app l i ca t ionHaving defined GEN and the five syllabification constraints we are now in a position toaddress the main issue: houl are optimality constraints applied ~.Given that Q~.N denotes a relation and that the constraints can be thought of as identityrelations on sets, the simplest idea is to proceed in the same way as with the rewriterules in Figure 2.
We could compose GEN with the constraints to yield a transducer thatmaps each input to its most optimal realization letting the ordering of the constraints inthe cascade implement their ranking (Figure 9).GENoO.HaveOns.O,NoCoda.0,FillNuco0?Parse,O.FillOnsFigure 9.
Merciless cascade.But it is immediately obvious that composition does not work here as intended.
The6-state transducer illustrated in Figure 9 works fine on inputs such as panama yielding0\[p\]N\[a\]0\[~S\[a\]0\[m\]N\[a\] but it fails to produce any output on inputs like americathat fall on some constraint.
Only strings that have a perfect output candidate survivethis merciless cascade.
We need to replace composition with some new operation to makethis schema work correctly.4 Lenient compositionThe necessary operation, let us call it lenient composition, is not di~cuLt to construct,but to our knowledge it has not previously been defined.
Frank and Satta \[7\] come veryclose but do not take the final step to encapsulate he notion.
Hammond \[8\] has the ideabut lacks the means to spell it out in formal terms.As the first step toward defining lenient composition, let us review an old notion calledpriority union (Kaplan \[12\]).
This term was originally defined as an operation for unifyingtwo feature structures in a way that eliminates any risk of failure by stipulating that oneof the two has priority in case of a conflict.
3 A finite-state version of this notion hasproved very useful in the management of transducer lexicons (Kaplan and Newman \[11\]).Let us consider the relations q and R depicted in Figure 10.
The Q relation maps a to zand b to y.
The It relation maps b to z and c to z,.
The priority union of Q and It, denotedQ .P.
R, maps a to z, b to y, and c to w. That is, it includes all the pairs from Q andevery pair from R that has as its upper element a string that does not occur as the upperstring of any pair in Q.
If some string occurs as the upper element of some pair in bothQ and R, the priority union of Q and R only includes the pair in Q. Consequently Q .P.
Itin Figure 10 maps b to y instead of z.3 The DPATR system at SRI (Karttunen \[16\]) had the same operation with a less respectable title, itwas called "clobber".
{a b}  {b  c}Q= I R= I ?x ~y z 9 w ?a b c }Q.P.R= I I I ?x 9y9  wFigure 10.
Example of priority union.
?The priority union operator .P.
can be defined in terms of other regular expressionoperators in the Xerox calculus.
A straightforward definition is given in Figure 11.Q .p.
R ffi Q I \['CQ.~J .o.
R\]Figure 11.
Definition of priority unionThe .u operator in Figure 11 extracts the '~pper" language from a regular elation.
Thusthe expression "\[Q. u\] denotes the set of strings that do not occur on the upper side ofthe Q relation.
The effect of the composition i  Figure 11 is to restrict R to mappings thatconcern strings that are not mapped to anything in Q.
Only this subset of R is unionedwith Q.We define the desired operation, lenient composition, denoted .0., as a combination ofordinary composition and priority union (Figure 12).R .0 .
C = \[R .o .
C\] .P .
ItFigure 12.
Definition of lenient compositionTo better visualize the effect of the operation defined in Figure 12 one may think ofthe relation R as a set of mappings induced by GEN and the relation C as oneof theconstraints defined in Figure 8.
The left side of the priority union, \[It .
o. C\] restricts ttto mappings that satisfy the constraint.
That is, any pair whose lower side string is not inC will be eliminated.
If some string in the upper language of R has no counterpart on thelower side that meets the constraint, hen it is not present in \[1l .o.
C\] .u but, for thatvery reason, it will be "rescued" by the priority union.
In other words, if an underlyingform has some output hat can meet he given constraint, lenient composition enforces theconstraint.
If an underlying form has no output candidates that meet the constraint, henthe underlying form and all its outputs are retained.
The definition of lenient compositionentails that the upper language of It is preserved in R .
0.
C.Many people, including Hammond \[8\] and Frank and Satta \[7\], have independently had asimilar idea without conceiving it as a finite-state operation.
4 If one already knows aboutpriority union, lenient composition is an obvious idea.Let us illustrate the effect of lenient composition starting with the example in Figure ?The composition of the input a with GSl~ yields a relation that maps a to the 14 outputsin Figure 7.
We will leniently compose this relation with each of the constraints in theorder of their ranking, starting with the ltave0ns constraint (Figure 13).
The lower-caseoperator, o. stands for ordinary composition, the upper case.
0. for lenient composition.As Figure 13 illustrates, applying ltave0ns by lenient composition removes most of the14 output candidates produced by OEN.
The resulting relation maps a to two outputs0\[ \]N\[a\] and 0\[ \]N\[a\]D\[ \].
The next highest-ranking constraint, NoCoda, removes thelatter alternative.
The twelve candidates that were eliminated by the first lenient com-position are no longer under consideration.4 Hammond implements a pruning operation that removes uutput candidates under the condition that"pruning cannot reduce the candidate set to null" (p 13).
Frank and Satta (p.
?)
describe a processof "conditional intersection" that enforces a constraint if it can be met and does nothing otherwise.aa.0?GEN.0.HaveOnsaao0.GEN.0.HaveOns.0.NoCodaO\[ \]N\[a\], 0\[ \]N\[a\]D\[ \]O\[ IN\[a\]Figure 13.
Cascade of constraint applications.aaoO.GEN.O.HaveOns.0.NoCoda.0.FillNu?.0.Parse.0.FiliOnso\[ IN\[a\]The next two constraints in the sequence, FillNuc and Parse, obviously do not changethe relation because the one remaining output candidate, 0 \[ IN\[a\], satisfies them.
Up tothis point, the distinction between lenient and ordinary composition does not make anydifference because we have not exhausted the set of output candidates.
However, whenwe bring in the last constraint, FillOns, the fight half of the definition in Figure 12 hasto come to the rescue; otherwise there would be no output for a.This example demonstrates that the application of optimality constraints can be thoughtof as a cascade of lenient compositions that carry down an ever decreasing number ofoutput candidates without allowing the set to become mpty.
Instead of intermediaterepresentations (c.f.
Figure 1) there are intermediate candidate populations correspondingto the columns in the left-to-right ordering of the constraint tableau.Instead of applying the constraints one by one to the output provided by GEN for a par-ticular input, we may also leniently compose the GEN relation itself with the constraints.Thus the suggestion made in Figure 9 is (nearly) correct after all, provided that wereplace ordinary composition with lenient composition (Figure 14).GEN.O.Ha.vs?nsNoCoda.0.Parse.0.FillOnsFigure 14.
Lenient cascadeThe composite single transducer shown in Figure 14 maps a and any other input directlyinto its viable outputs without ever producing any failing candidates.5 Mu l t ip le  v io la t ionsIIHowever, we have not yet addressed one very important issue.
It is not sufficient toobey the ranking of the constraints.
If two or more output candidates "violate the sameconstraint multiple times we should prefer the candidate or candidates with the smallestnumber of violations.
This does not come for free.
The system that we have sketchedso far does not make that distinction.
If the input form has no perfect outputs, we mayget a set of outputs that di~er with respect o the number of constraint violations.
Forexample, the transducer in Figure 14 gives three outputs for the string bebop (Figure 15).0 \[b\]N\[e\]X fb\]X\[o\] X\[p\]0 \[b\] ~\[e\] 0 \[b\]X~\[o\] X \[p\]XEb\]X\[e\]0Cb\]N\[o\]X\[p\]Figure 15.
Two many outputsBecause bebop has no output hat meets the Parse constraint, lenient composition allowsall outputs that contain a Parse violation regardless of the number of violations.
Herethe second alternative with just one violation should win but it does not.Instead of viewing Parse as a single constraint, we need to reconstruct i as a series ofever more relaxed parse constraints.
The ">n operator in Figure 16 means "more than niterations".define Parse "$ \["X \ [ " \ ]  ;def ine Parse1 "\[\[$?
'XE"\]'>I\] ;define Parse2 "CC$"XC"\]'>2\] ;ooodefine ParseN "\[\[$"I\["\] '>N\] ;Figure 16.
A family of Parse constraintsOur original Parse constraint is violated by a single unparsed element.
Parse1 allows oneunparsed element.
Parse2 allows up to two violations, and Parseg up to N violations.The single Parse line in Figure 14 must be replaced by the sequence of lenient composi-tions in Figure 17 up to some chosen N.ParsQ?0oParse1.0oParse2.O?ParseSFiKure 17.
Gradient Parse constraintIf an input string has at least one output form that meets the Parse constraint (noviolations), all the competing output forms with Parse violations are eliminated.
Failingthat, if the input string has at least one output form with just one violation, all theoutputs with more violations are eliminated.
And so on.The particular order in which the individual parse constraints apply actually has no effecthere on the final outcome because the constraint languages are in a strict subset relation:Parse C Parsel C Parse2 C ... ParseN.
5 For example, if the best candidate incurs two5 Thanks to Jason Eisner (p.c.)
for this observation.10violations, it is in Parse2 and in all the weaker constraints.
The ranking in Figure 17determines only the order in which the losing candidates are eliminated.
If we start withthe strictest constraint, all the losers are eliminated at once when Parse2 is applied; ifwe start with a weaker constraint, some output candidates will be eliminated earlier thanothers but the winner remains the same.As the number of constraints goes up, so does the size of the combined constraint networkin Figure 14, from 66 states (no Parse violations) to 248 (at most five violations).
It mapsbebop to 0\[bJSCe\]0\[b\]NCoJX\[p\] and abracadabra to 0DN\[edX\[bJ0CrJNCa\]0\[c\]N\[a\]-0 \[d\]N \[aJ X \[b\] 0 It\] N \[a\] correctly and instantaneously.It is immediately evident hat while we can construct a cascade of constraints hat prefern violations to n+I violations up to any given n, there is no way in a finite-Rate systemto express the general idea that fewer violations is better than more violations.
As Frankand Satta \[7\] point out, finite-state constraints cannot make infinitely many distinctionsof well-formedness.
It is not likely that this limitation is a serious obstacle to practicaloptimality computations with finite-state systems as the number of constraint violationsthat need to be taken into account is generally small.It is curious that violation counting should emerge as the crucial issue that potentiallypushes optimality theory out of the finite-state domain thus making it formally morepowerful than rewrite systems and two-level models.
It has never been presented as anargument against he older models that they do not allow unlimited counting.
It is notclear whether the additional power constitutes an asset or an embarrassment for OT.6 ConclusionThis novel formalization of optimality theory has several technical advantages over theprevious computational treatments:- No marking, sorting, or counting of constraint violations.- Application of optimality constraints is done within the finite-state calculus.- A system of optimality constraints can be merged into a single constraint network.This approach shows clearly that optimality theory is very similar to the two older strainsof finite-state phonology: classical rewrite systems and two-level models.
In optimalitytheory, lenient composition plays the same role as ordinary composition in rewrite sys-tems.
The top-down sorialism of rule ordering is replaced by the left-to-right serialism ofthe constraint tableau.The new lenient composition operator has other uses beyond phonology.
In the area ofsyntax, Constraint Grammar (Karlsson et el.
\[13\]) is from a formal point of view verysimilar to optimality theory.
Although constraint grammars so far have not been imple-mented as pure finlte-state systems, it is evident that the lenient composition operatormakes it possible.References1.
Noam Chomsky and Morris Halle.
1968.
The Sound Pattern of English.
Harper andRow, New York.2.
Jennifer S. Cole and Charles W. Kisseberth.
1995.
Restricting multi-level constraintevaluation: Opaque rule interaction i  Yawelmani vowel harmony.
(ROA-98-0000).3.
Jason Eisner.
1997a.
Decomposing FootForm: Primitive constraints in OT.
In SCILVIII.
(ROA-205-0797).4.
Jason Eisner.
1997"o.
Efficient generation i primitive optimality theory.
In ACL'97,Madrid, Spain.
(ROA-206-0797).5.
Jason Eisner.
1997c.
What constraints should OT allow?
Handout (20p) for talk atthe LSA Annual Meeting, Chicago, 1/4/97.
(ROA-204-0797).116.
Mark T. Ellison.
1994.
Phonological derivation i  optimality theory, In COLING'g ~Vol//, pages 1007-1013, Kyoto, Japan.
(ROA-75-0000), (cmp-lg/9505031).7.
Robert Frank and Giorgio Satta.
1998.
Optimality theory and the generative com-plexity of constraint violability.
Computational Linguistics (forthcoming).
(ROA-228-1197).8.
Michael Hammond.
1997.
Parsing syllables: Modeling OT computationally.
(ROA-222-1097).9.
C. Douglas Johnson.
1972.
Formal Aspects of Phonological Description.
Mouton,The Hague.10.
Ronald M. Kaplan and Martin Kay.
1994.
Regular models of phonological rulesystems.
Computational Linguistics, 20(3):331-378.11.
Ronald M. Kaplan and Panla S. Newman.
1997.
Lexical resource reconciliation inthe Xerox Linguistic Environment.
In ACL/EACL'g8 Workshop on ComputationalEnvironments for Grammar Development and Linguistic Engineering, pages 54-61,Madrid, Spain, July 12.12.
Ronald M. Kaplan.
1987.
Three seductions of computational psycholinguistics.
InP.
Whitelock, M. M. Wood, H. L. Somers, R. Johnson, and P. Bennett, editors, Lin-guistic Theory and Computer Applications, pages 149-181.
Academic Press, New York.Reprinted in Formal Issues in Lexical-Functional Grammar, ed.
M. Dalrymple, R. M.Kaplan, J. T. Maxwell III, and A. Zaenen.
University of Chicago Press, 1996.13.
Fred Karlsson, Atro Voutilainen, Juha Heikkila, and Arto Anttila.
1995.
ConstraintGrammar: A Language-Independent Framework for Parsing Unrestricted Te~.
Moutonde Gruyter, Berlin/New York.14.
Lauri Karttunen, Kimmo Koskenniemi, and Ronald M. Kaplan.
1987.
A compilerfor two-level phonological rules.
Technical report, Center for the Study of Languageand Information, Stanford University, June 25.15.
Lauri Karttunen, Jean-Pierre Chanod, Gregory Grefenstette, and Anne Schiller.1996.
Regular expressions for language ngineering.
Journal of Natural LanguageEngineering, 2(4):305-328.16.
Lauri Karttunen.
1986.
D-PATR: A development environment for unification-basedgrammars.
In COLING'86, pages 74--80.17.
Lauri Karttunen.
1993.
Finite-state constraints.
In John Goldsmith, editor, TheLast Phonological Rule, pages 173-194.
Chicago University Press, Chicago.18.
Lauri Karttunen.
1994.
Constructing lexical transducers.
In COLING'9~, Kyoto,?
Japan.19.
Lauri Karttunen.
1995.
The replace operator.
In Proceedings of the 33rd AnnualMeeting of the ACL, Cambridge, MA.
(emp-lg/9504032).20.
Charles Kisseberth.
1969.
On the abstractness of phonology.
Papers in Linguistics,1:248-282.21.
Kimmo Koskenniemi.
1983.
Two-level morphology: A general computational modelfor word-form recognition and production.
Publication 11, University of Helsinki, De-partment of General Linguistics, Helsinki.22.
John McCarthy and Alan Prince.
1998.
Faithfulness and identity in prosodic mor-phology.
In R. Kager, H. van der Hulst, and W. Zonneveld, editors, The prosody-morphology interface.
Cambridge University Press, Cambridge, UK.
(ROA-216-0997).23.
John J. McCarthy.
1998.
Sympathy & phonological opacity.
(ROA-252-0398).24.
Alan Prince and Paul Smolensky.
1993.
Optimality Theory: Constraint Interactionin Generative Grammar.
Technical Report TR-2, 'Rutgers University Cognitive ScienceCenter, New Brunswick, NJ.
To appear, MIT Press.25.
Bruce Tesar.
1995.
Computational Optimality Theory.
Ph.D. thesis, University ofColorado, Boulder, CO.26.
Markus Walther.
1996.
OT SIMPLE - A construction-kit approach to OptimalityTheory implementation.
(ROA-152-1090).B\[\]\[\]12
