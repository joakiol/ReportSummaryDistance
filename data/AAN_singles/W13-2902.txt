Proceedings of the 2nd Workshop on Predicting and Improving Text Readability for Target Reader Populations, pages 11?19,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsBuilding a German/Simple German Parallel Corpusfor Automatic Text SimplificationDavid Klaper Sarah Ebling Martin VolkInstitute of Computational Linguistics, University of ZurichBinzm?hlestrasse 14, 8050 Zurich, Switzerlanddavid.klaper@uzh.ch, {ebling|volk}@cl.uzh.chAbstractIn this paper we report our experimentsin creating a parallel corpus using Ger-man/Simple German documents from theweb.
We require parallel data to build astatistical machine translation (SMT) sys-tem that translates from German into Sim-ple German.
Parallel data for SMT sys-tems needs to be aligned at the sentencelevel.
We applied an existing monolingualsentence alignment algorithm.
We showthe limits of the algorithm with respect tothe language and domain of our data andsuggest ways of circumventing them.1 IntroductionSimple language (or, ?plain language?, ?easy-to-read language?)
is language with low lexical andsyntactic complexity.
It provides access to infor-mation to people with cognitive disabilities (e.g.,aphasia, dyslexia), foreign language learners, Deafpeople,1 and children.
Text in simple languageis obtained through simplification.
Simplificationis a text-to-text generation task involving multipleoperations, such as deletion, rephrasing, reorder-ing, sentence splitting, and even insertion (Costerand Kauchak, 2011a).
By contrast, paraphrasingand compression, two other text-to-text generationtasks, involve merely rephrasing and reordering(paraphrasing) and deletion (compression).
Textsimplification also shares common ground withgrammar and style checking as well as with con-trolled natural language generation.Text simplification approaches exist for vari-ous languages, including English, French, Span-ish, and Swedish.
As Matausch and Nietzio (2012)write, ?plain language is still underrepresented in1It is an often neglected fact that Deaf people tend to ex-hibit low literacy skills (Gutjahr, 2006).the German speaking area and needs further devel-opment?.
Our goal is to build a statistical machinetranslation (SMT) system that translates from Ger-man into Simple German.SMT systems require two corpora aligned at thesentence level as their training, development, andtest data.
The two corpora together can form abilingual or a monolingual corpus.
A bilingualcorpus involves two different languages, while amonolingual corpus consists of data in a singlelanguage.
Since text simplification is a text-to-text generation task operating within the same lan-guage, it produces monolingual corpora.Monolingual corpora, like bilingual corpora,can be either parallel or comparable.
A parallelcorpus is a set of two corpora in which ?a no-ticeable number of sentences can be recognized asmutual translations?
(Tom?s et al 2008).
Paral-lel corpora are often compiled from the publica-tions of multinational institutions, such as the UNor the EU, or of governments of multilingual coun-tries, such as Canada (Koehn, 2005).
In contrast, acomparable corpus consists of two corpora createdindependently of each other from distinct sources.Examples of comparable documents are news ar-ticles written on the same topic by different newsagencies.In this paper we report our experiments in cre-ating a monolingual parallel corpus using Ger-man/Simple German documents from the web.
Werequire parallel data to build an SMT system thattranslates from German into Simple German.
Par-allel data for SMT systems needs to be aligned atthe sentence level.
We applied an existing mono-lingual sentence alignment algorithm.
We showthe limits of the algorithm with respect to the lan-guage and domain of our data and suggest ways ofcircumventing them.The remainder of this paper is organized as fol-lows: In Section 2 we discuss the methodologiespursued and the data used in previous work deal-11ing with automatic text simplification.
In Section 3we describe our own approach to building a Ger-man/Simple German parallel corpus.
In particu-lar, we introduce the data obtained from the web(Section 3.1), describe the sentence alignment al-gorithm we used (Section 3.2), present the resultsof the sentence alignment task (Section 3.3), anddiscuss them (Section 3.4).
In Section 4 we givean overview of the issues we tackled and offer anoutlook on future work.2 Approaches to Text SimplificationThe task of simplifying text automatically can beperformed by means of rule-based, corpus-based,or hybrid approaches.
In a rule-based approach,the operations carried out typically include replac-ing words by simpler synonyms or rephrasing rel-ative clauses, embedded sentences, passive con-structions, etc.
Moreover, definitions of difficultterms or concepts are often added, e.g., the termweb crawler is defined as ?a computer programthat searches the Web automatically?.
Gasperin etal.
(2010) pursued a rule-based approach to textsimplification for Brazilian Portuguese within thePorSimples project,2 as did Brouwers et al(2012)for French.As part of the corpus-based approach, machinetranslation (MT) has been employed.
Yatskar et al(2010) pointed out that simplification is ?a form ofMT in which the two ?languages?
in question arehighly related?.As far as we can see, Zhu et al(2010) were thefirst to use English/Simple English Wikipedia datafor automatic simplification via machine transla-tion.3 They assembled a monolingual compara-ble corpus4 of 108,016 sentence pairs based onthe interlanguage links in Wikipedia and the sen-tence alignment algorithm of Nelken and Shieber(2006) (cf.
Section 3.2).
Their system applies a?tree-based simplification model?
including ma-chine translation techniques.
The system learnsprobabilities for simplification operations (substi-tution, reordering, splitting, deletion) offline from2http://www2.nilc.icmc.usp.br/wiki/index.php/English3English Wikipedia: http://en.wikipedia.org/; Simple English Wikipedia: http://simple.wikipedia.org/.4We consider this corpus to be comparable rather thanparallel because not every Simple English Wikipedia articleis necessarily a translation of an English Wikipedia article.Rather, Simple English articles can be added independentlyof any English counterpart.the comparable Wikipedia data.
At runtime, an in-put sentence is parsed and zero or more simplifica-tion operations are carried out based on the modelprobabilities.Specia (2010) used the SMT system Moses(Koehn et al 2007) to translate from BrazilianPortuguese into a simpler version of this language.Her work is part of the PorSimples project men-tioned above.
As training data she used 4483 sen-tences extracted from news texts that had beenmanually translated into Simple Brazilian Por-tuguese.5 The results, evaluated automaticallywith BLEU (Papineni et al 2002) and NIST(Doddington, 2002) as well as manually, show thatthe system performed lexical simplification andsentence splitting well, while it exhibited prob-lems in reordering phrases and producing subject?verb?object (SVO) order.
To further improve hersystem Specia (2010) suggested including syntac-tic information through hierarchical SMT (Chi-ang, 2005) and part-of-speech tags through fac-tored SMT (Hoang, 2007).Coster and Kauchak (2011a; 2011b) trans-lated from English into Simple English using En-glish/Simple English Wikipedia data.
Like Spe-cia (2010), they applied Moses as their MT sys-tem but in addition to the default configuration al-lowed for phrases to be empty.
This was moti-vated by their observation that 47% of all SimpleEnglish Wikipedia sentences were missing at leastone phrase compared to their English Wikipediacounterparts.
Coster and Kauchak (2011a; 2011b)used four baselines to evaluate their system: in-put=output,6 two text compression systems, andvanilla Moses.
Their system, Moses-Del, achievedhigher automatic MT evaluation scores (BLEU)than all of the baselines.
In particular, it outper-formed vanilla Moses (lacking the phrase deletionoption).Wubben et al(2012) also worked with En-glish/Simple English Wikipedia data and Moses.They added a post-hoc reranking step: Follow-ing their conviction that the output of a simplifi-cation system has to be a modified version of theinput,7 they rearranged the 10-best sentences out-put by Moses such that those differing from the5Hence, the corpus as a whole is a monolingual parallelcorpus.6The underlying assumption here was that not every sen-tence needs simplification.7Note that this runs contrary to the assumption Coster andKauchak (2011a; 2011b) made.12input sentences were given preference over thosethat were identical.
Difference was calculated onthe basis of the Levenshtein score (edit distance).Wubben et al(2012) found their system to workbetter than that of Zhu et al(2010) when evalu-ated with BLEU, but not when evaluated with theFlesch-Kincaid grade level, a common readabilitymetric.Bott and Saggion (2011) presented a monolin-gual sentence alignment algorithm, which uses aHidden Markov Model for alignment.
In contrastto other monolingual alignment algorithms, Bottand Saggion (2011) introduced a monotonicity re-striction, i.e., they assumed the order of sentencesto be the same for the original and simplified texts.Apart from purely rule-based and purelycorpus-based approaches to text simplification,hybrid approaches exist.
For example, Bott et al(2012) in their Simplext project for Spanish8 let astatistical classifier decide for each sentence of atext whether it should be simplified (corpus-basedapproach).
The actual simplification was then per-formed by means of a rule-based approach.As has been shown, many MT approaches totext simplification have used English/Simple En-glish Wikipedia as their data.
The only excep-tion we know of is Specia (2010), who togetherwith her colleagues in the PorSimples project builther own parallel corpus.
This is presumably be-cause there exists no Simple Brazilian PortugueseWikipedia.
The same is true for German: To date,no Simple German Wikipedia has been created.Therefore, we looked for data available elsewherefor our machine translation system designated totranslate from German to Simple German.
We dis-covered that German/Simple German parallel datais slowly becoming available on the web.
In whatfollows, we describe the data we harvested and re-port our experience in creating a monolingual par-allel corpus from this data.3 Building a German/Simple GermanParallel Corpus from the Web3.1 DataAs mentioned in Section 1, statistical machinetranslation (SMT) systems require parallel data.A common approach to obtain such material isto look for it on the web.9 The use of already8http://www.simplext.es/9Resnik (1999) was the first to discuss the possibility ofcollecting parallel corpora from the web.available data offers cost and time advantages.Many websites, including that of the German gov-ernment,10 contain documents in Simple German.However, these documents are often not linked to asingle corresponding German document; instead,they are high-level summaries of multiple Germandocuments.A handful of websites exist that offer articlesin two versions: a German version, often calledAlltagssprache (AS, ?everyday language?
), anda Simple German version, referred to as LeichteSprache (LS, ?simple language?).
Table 1 lists thewebsites we used to compile our corpus.
The num-bers indicate how many parallel articles were ex-tracted.
The websites are mainly of organizationsthat support people with disabilities.
We crawledthe articles with customized Python scripts that lo-cated AS articles and followed the links to their LScorrespondents.
A sample sentence pair from ourdata is shown in Example 1.
(1) German:Wir freuen uns ?ber Ihr Interesse an unsererArbeit mit und f?r Menschen mitBehinderung.
(?We appreciate your interest in our workwith and for people with disabilities.?
)Simple German:Sch?n, dass Sie sich f?r unsere Arbeitinteressieren.Wir arbeiten mit und f?r Menschen mitBehinderung.
(?Great that you are interested in our work.We work with and for people withdisabilities.?
)The extracted data needed to be cleaned fromHTML tags.
For our purpose, we considered textand paragraph structure markers as important in-formation; therefore, we retained them.
We subse-quently tokenized the articles.
The resulting cor-pus consisted of 7755 sentences, which amountedto 82,842 tokens.
However, caution is advisedwhen looking at these numbers: Firstly, the tok-enization module overgenerated tokens.
Secondly,some of the LS articles were identical, either be-cause they summarized multiple AS articles or be-cause they were generic placeholders.
Hence, the10http://www.bundesregierung.de/Webs/Breg/DE/LeichteSprache/leichteSprache_node.html (last accessed 15th April 2013)13Short name URL No.
of parallel art.ET www.einfach-teilhaben.de 51GWW www.gww-netz.de 65HHO www.os-hho.de 34LMT www.lebenshilfe-main-taunus.de 47OWB www.owb.de 59Table 1: Websites and number of articles extractedactual numbers were closer to 7000 sentences and70,000 tokens.SMT systems usually require large amount oftraining data.
Therefore, this small experimen-tal corpus is certainly not suitable for large-scaleSMT experiments.
However, it can serve as proofof concept for German sentence simplification.Over time more resources will become available.SMT systems rely on data aligned at the sen-tence level.
Since the data we extracted from theweb was aligned at the article level only, we hadto perform sentence alignment.
For this we splitour corpus into a training set (70% of the texts),development set (10%), and test set (20%).
Wemanually annotated sentence alignments for all ofthe data.
Example 2 shows an aligned AS/LS sen-tence pair.
(2) German:In den Osnabr?cker Werkst?tten (OW) undOSNA-Techniken sind rund 2.000 Menschenmit einer Behinderung besch?ftigt.
(?In the Osnabr?ck factories andOSNA-Techniken, about 2.000 people withdisability are employed.?
)Simple German:In den Osnabr?cker Werkst?tten und denOsna-Techniken arbeiten zweitausendMenschen mit Behinderung.
(?Two thousand people with disability workin the Osnabr?ck factories andOsna-Techniken.?
)To measure the amount of parallel sentencesin our data, we calculated the alignment di-versity measure (ADM) of Nelken and Shieber(2006).
ADM measures how many sentences arealigned.
It is calculated as 2?matches(T1,T2)|T1|+|T2| , wherematches is the number of alignments between thetwo texts T1 and T2.
ADM is 1.0 in a perfectlyparallel corpus, where every sentence from onetext is aligned to exactly one sentence in anothertext.ADM for our corpus was 0.786, which meansthat approximately 78% of the sentences werealigned.
This is a rather high number compared tothe values reported by Nelken and Shieber (2006):Their texts (consisting of encyclopedia articles andgospels) resulted in an ADM of around 0.3.
A pos-sible explanation for the large difference in ADMis the fact that most simplified texts in our corpusare solely based on the original texts, whereas thesimple versions of the encyclopedia articles mighthave been created by drawing on external informa-tion in addition.3.2 Sentence Alignment AlgorithmSentence alignment algorithms differ according towhether they have been developed for bilingual ormonolingual corpora.
For bilingual parallel cor-pora many?typically length-based?algorithmsexist.
However, our data was monolingual.
Whilethe length of a regular/simple language sentencepair might be different, an overlap in vocabularycan be expected.
Hence, monolingual sentencealignment algorithms typically exploit lexical sim-ilarity.We applied the monolingual sentence alignmentalgorithm of Barzilay and Elhadad (2003).
The al-gorithm has two main features: Firstly, it uses ahierarchical approach by assigning paragraphs toclusters and learning mapping rules.
Secondly,it aligns sentences despite low lexical similarityif the context suggests an alignment.
This isachieved through local sequence alignment, a dy-namic programming algorithm.The overall algorithm has two phases, a train-ing and a testing phase.
The training phase in turnconsists of two steps: Firstly, all paragraphs of thetexts of one side of the parallel corpus (henceforthreferred to as ?AS texts?)
are clustered indepen-dently of all paragraphs of the texts of the other14side of the parallel corpus (henceforth termed ?LStexts?
), and vice versa.
Secondly, mappings be-tween the two sets of clusters are calculated, giventhe reference alignments.As a preprocessing step to the clustering pro-cess, we removed stopwords, lowercased allwords, and replaced dates, numbers, and namesby generic tags.
Barzilay and Elhadad (2003) ad-ditionally considered every word starting with acapital letter inside a sentence to be a proper name.In German, all nouns (i.e., regular nouns as well asproper names) are capitalized; thus, this approachdoes not work.
We used a list of 61,228 first namesto remove at least part of the proper names.We performed clustering with scipy (Jones etal., 2001).
We adapted the hierarchical complete-link clustering method of Barzilay and Elhadad(2003): While the authors claimed to have set aspecific number of clusters, we believe this is notgenerally possible in hierarchical agglomerativeclustering.
Therefore, we used the largest num-ber of clusters in which all paragraph pairs had acosine similarity strictly greater than zero.Following the formation of the clusters, lex-ical similarity between all paragraphs of corre-sponding AS and LS texts was computed to es-tablish probable mappings between the two setsof clusters.
Barzilay and Elhadad (2003) usedthe boosting tool Boostexter (Schapire and Singer,2000).
All possible cross-combinations of para-graphs from the parallel training data served astraining instances.
An instance consisted of thecosine similarity of the two paragraphs and a stringcombining the two cluster IDs.
The classifica-tion result was extracted from the manual align-ments.
In order for an AS and an LS paragraphto be aligned, at least one sentence from the LSparagraph had to be aligned to one sentence in theAS paragraph.
Like Barzilay and Elhadad (2003),we performed 200 iterations in Boostexter.
Afterlearning the mapping rules, the training phase wascomplete.The testing phase consisted of two additionalsteps.
Firstly, each paragraph of each text in thetest set was assigned to the cluster it was clos-est to.
This was done by calculating the cosinesimilarity of the word frequencies in the clusters.Then, every AS paragraph was combined with allLS paragraphs of the parallel text, and Boostexterwas used in classification mode to predict whetherthe two paragraphs were to be mapped.Secondly, within each pair of paragraphsmapped by Boostexter, sentences with very highlexical similarity were aligned.
In our case, thethreshold for an alignment was a similarity of 0.5.For the remaining sentences, proximity to otheraligned or similar sentences was used as an indi-cator.
This was implemented by local sequencealignment.
We set the mismatch penalty to 0.02,as a higher mismatch penalty would have reducedrecall.
We set the skip penalty to 0.001 conform-ing to the value of Barzilay and Elhadad (2003).The resulting alignments were written to files.
Ex-ample 3 shows a successful sentence alignment.
(3) German:Die GWW ist in den Landkreisen B?blingenund Calw aktiv und bietet an den folgendenStandorten Wohnm?glichkeiten f?rMenschen mit Behinderung an ?
ganz inIhrer N?he!
(?The GWW is active in the counties ofB?blingen and Calw and offers housingoptions for people with disabilities at thefollowing locations ?
very close to you!?
)Simple German:Die GWW gibt es in den Landkreisen Calwund B?blingen.Wir haben an den folgenden OrtenWohn-M?glichkeiten f?r Sie.
(?The GWW exists in the counties of Calwand B?blingen.
We have housing options foryou in the following locations.?
)The algorithm described has been modified invarious ways.
Nelken and Shieber (2006) usedTF/IDF instead of raw term frequency, logistic re-gression on the cosine similarity instead of cluster-ing, and an extended version of the local alignmentrecurrence.
Both Nelken and Shieber (2006) andQuirk et al(2004) found that the first sentenceof each document is likely to be aligned.
We ob-served the same for our corpus.
Therefore, in ouralgorithm we adopted the strategy of uncondition-ally aligning the first sentence of each document.3.3 ResultsTable 2 shows the results of evaluating the algo-rithm described in the previous section with re-spect to precision, recall, and F1 measure.
We in-troduced two baselines:15Method Precision Recall F1Adapted algorithm of Barzilay and Elhadad (2003) 27.7% 5.0% 8.5%Baseline I: First sentence 88.1% 4.8% 9.3%Baseline II: Word in common 2.2% 8.2% 3.5%Table 2: Alignment results on test set1.
Aligning only the first sentence of each text(?First sentence?)2.
Aligning every sentence with a cosine simi-larity greater than zero (?Word in common?
)As can be seen from Table 2, by applying thesentence alignment algorithm of Barzilay and El-hadad (2003) we were able to extract only 5%of all reference alignments, while precision wasbelow 30%.
The rule of aligning the first sen-tences performed well with a precision of 88%.Aligning all sentences with a word in commonclearly showed the worst performance; this is be-cause many sentences have a word in common.Nonetheless, recall was only slightly higher thanwith the other methods.In conclusion, none of the three approaches(adapted algorithm of Barzilay and Elhadad(2003), two baselines ?First sentence?
and ?Wordin common?)
performed well on our test set.
Weanalyzed the characteristics of our data that ham-pered high-quality automatic alignment.3.4 DiscussionCompared with the results of Barzilay and El-hadad (2003), who achieved 77% precision at55.8% recall for their data, our alignment scoreswere considerably lower (27.7% precision, 5% re-call).
We found two reasons for this: languagechallenges and domain challenges.
In what fol-lows, we discuss each reason in more detail.While Barzilay and Elhadad (2003) aligned En-glish/Simple English texts, we dealt with Ger-man/Simple German data.
As mentioned in Sec-tion 3.2, in German nouns (regular nouns as wellas proper names) are capitalized.
This makesnamed entity recognition, a preprocessing step toclustering, more difficult.
Moreover, German isan example of a morphologically rich language:Its noun phrases are marked with case, leadingto different inflectional forms for articles, pro-nouns, adjectives, and nouns.
English morphol-ogy is poorer; hence, there is a greater likelihoodof lexical overlap.
Similarly, compounds are pro-ductive in German; an example from our corpusis Seniorenwohnanlagen (?housing complexes forthe elderly?).
In contrast, English compounds aremultiword units, where each word can be accessedseparately by a clustering algorithm.
Therefore,cosine similarity is more effective for English thanit is for German.
One way to alleviate this problemwould be to use extensive morphological decom-position and lemmatization.In terms of domain, Barzilay and Elhadad(2003) used city descriptions from an encyclope-dia for their experiments.
For these descriptionsclustering worked well because all articles had thesame structure (paragraphs about culture, sports,etc.).
The domain of our corpus was broader:It included information about housing, work, andevents for people with disabilities as well as infor-mation about the organizations behind the respec-tive websites.Apart from language and domain challenges weobserved heavy transformations from AS to LS inour data (Figure 1 shows a sample article in ASand LS).
As a result, LS paragraphs were typi-cally very short and the clustering process returnedmany singleton clusters.
Example 4 shows anAS/LS sentence pair that could not be aligned be-cause of this.
(4) German:Der Beauftragte informiert ?ber dieGesetzeslage, regt Rechts?nderungen an,gibt Praxistipps und zeigt M?glichkeiten derEingliederung behinderter Menschen inGesellschaft und Beruf auf.
(?The delegate informs about the legalsituation, encourages revisions of laws, givespractical advice and points out possibilitiesof including people with disabilities insociety and at work.?
)Simple German:Er gibt ihnen Tipps und Infos.16Figure 1: Comparison of AS and LS article from http://www.einfach-teilhaben.de(?He provides them with advice andinformation.?
)Figure 2 shows the dendrogram of the cluster-ing of the AS texts.
A dendrogram shows the re-sults of a hierarchical agglomerative clustering.
Atthe bottom of the dendrogram every paragraph ismarked by an individual line.
At the points wheretwo vertical paths join, the corresponding clustersare merged to a new larger cluster.
The Y-axis isthe dissimilarity value of the two clusters.
In ourexperiment the resulting clusters are the clustersat dissimilarity 1 ?
1?10.
Geometrically this is ahorizontal cut just below dissimilarity 1.0.
As canbe seen from Figure 2, many of the paragraphsin the left half of the picture are never mergedto a slightly larger cluster but are directly con-nected to the universal cluster that merges every-thing.
This is because they contain only stopwordsor only words that do not appear in all paragraphsof another cluster.
Such an unbalanced clustering,where many paragraphs are clustered to one clus-ter and many other paragraphs remain singletonclusters, reduces the precision of the hierarchicalapproach.4 Conclusion and OutlookIn this paper we have reported our experiments increating a monolingual parallel corpus using Ger-man/Simple German documents from the web.
Wehave shown that little work has been done on au-tomatic simplification of German so far.
We havedescribed our plan to build a statistical machinetranslation (SMT) system that translates form Ger-man into Simple German.
SMT systems requireparallel corpora.
The process of creating a parallelcorpus for use in machine translation involves sen-tence alignment.
Sentence alignment algorithmsfor bilingual corpora differ from those for mono-lingual corpora.
Since all of our data was fromthe same language, we applied the monolingualsentence alignment approach of Barzilay and El-hadad (2003).
We have shown the limits of the al-gorithm with respect to the language and domainof our data.
For example, named entity recogni-tion, a preprocessing step to clustering, is harderfor German than for English, the language Barzi-lay and Elhadad (2003) worked with.
Moreover,German features richer morphology than English,which leads to less lexical overlap when workingon the word form level.17Figure 2: Dendrogram of AS clustersThe domain of our corpus was also broader thanthat of Barzilay and Elhadad (2003), who used citydescriptions from an encyclopedia for their exper-iments.
This made it harder to identify commonarticle structures that could be exploited in clus-tering.As a next step, we will experiment with othermonolingual sentence alignment algorithms.
Inaddition, we will build a second parallel corpus forGerman/Simple German: A person familiar withthe task of text simplification will produce simpleversions of German texts.
We will use the result-ing parallel corpus as data for our experiments inautomatically translating from German to SimpleGerman.
The parallel corpus we compiled as partof the work described in this paper can be madeavailable to interested parties upon request.ReferencesRegina Barzilay and Noemie Elhadad.
2003.
Sentencealignment for monolingual comparable corpora.
InProceedings of EMNLP.Stefan Bott and Horacio Saggion.
2011.
An un-supervised alignment algorithm for text simplifica-tion corpus construction.
In Proceedings of theWorkshop on Monolingual Text-To-Text Generation,MTTG ?11, pages 20?26, Stroudsburg, PA, USA.Stefan Bott, Horacio Saggion, and David Figueroa.2012.
A Hybrid System for Spanish Text Simpli-fication.
In Proceedings of the Third Workshop onSpeech and Language Processing for Assistive Tech-nologies, pages 75?84, Montr?al, Canada, June.Laetitia Brouwers, Delphine Bernhard, Anne-LaureLigozat, and Thomas Fran?ois.
2012.
Simplifica-tion syntaxique de phrases pour le fran?ais.
In Actesde la conf?rence conjointe JEP-TALN-RECITAL2012, volume 2: TALN, pages 211?224.David Chiang.
2005.
A Hierarchical Phrase-basedModel for Statistical Machine Translation.
In ACL-05: 43rd Annual Meeting of the Association forComputational Linguistics, pages 263?270, Univer-sity of Michigan, Ann Arbor, Michigan, USA.William Coster and David Kauchak.
2011a.
Learn-ing to simplify sentences using Wikipedia.
In Pro-ceedings of the Workshop on Monolingual Text-To-Text Generation, MTTG ?11, pages 1?9, Strouds-burg, PA, USA.William Coster and David Kauchak.
2011b.
SimpleEnglish Wikipedia: a new text simplification task.In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies: short papers - Volume 2,HLT ?11, pages 665?669, Stroudsburg, PA, USA.George Doddington.
2002.
Automatic Evaluationof Machine Translation Quality Using N-gram Co-occurrence Statistics.
In HLT 2002: Human Lan-guage Technology Conference, Proceedings of theSecond International Conference on Human Lan-guage Technology Research, pages 138?145, SanDiego, California.18Caroline Gasperin, Erick Maziero, and Sandra M.Aluisio.
2010.
Challenging choices for text sim-plification.
In Computational Processing of the Por-tuguese Language.
Proceedings of the 9th Interna-tional Conference, PROPOR 2010, volume 6001of Lecture Notes in Artificial Intelligence (LNAI),pages 40?50, Porto Alegre, RS, Brazil.
Springer.A.
Gutjahr.
2006.
Lesekompetenz Geh?rloser: EinForschungs?berblick.
Universit?t Hamburg.Hieu Hoang.
2007.
Factored Translation Models.In EMNLP-CoNLL 2007: Proceedings of the 2007Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational NaturalLanguage Learning, pages 868?876, Prague, CzechRepublic.Eric Jones, Travis Oliphant, Pearu Peterson, et al2001.
SciPy: Open Source Scientific Tools forPython.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondr?ej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: OpenSource Toolkit for Statistical Machine Translation.In ACL 2007, Proceedings of the 45th Annual Meet-ing of the Association for Computational Linguis-tics, pages 177?180, Prague, Czech Republic.Kerstin Matausch and Annika Nietzio.
2012.
Easy-to-read and plain language: Defining criteria and re-fining rules.
http://www.w3.org/WAI/RD/2012/easy-to-read/paper11/.Rani Nelken and Stuart M. Shieber.
2006.
Towards ro-bust context-sensitive sentence alignment for mono-lingual corpora.
In Proceedings of 11th Conferenceof the European Chapter of the Association for Com-putational Linguistics, pages 161?168.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a Method for AutomaticEvaluation of Machine Translation.
In 40th AnnualMeeting of the Association for Computational Lin-guistics, Proceedings of the Conference, pages 311?318, Philadelphia, PA, USA.Chris Quirk, Chris Brocket, and William Dolan.
2004.Monolingual machine translation for paraphrasegeneration.
In Proceedings Empirical Methods inNatural Language Processing.Philip Resnik.
1999.
Mining the Web for BilingualText.
In 37th Annual Meeting of the Association forComputational Linguistics, Proceedings of the Con-ference, pages 527?534, University of Maryland,College Park, Maryland, USA.Robert E. Schapire and Yoram Singer.
2000.
BoosTex-ter: A boosting-based system for text categorization.Machine Learning, 39(2?3):135?168.Lucia Specia.
2010.
Translating from complex tosimplified sentences.
In Computational Process-ing of the Portuguese Language.
Proceedings of the9th International Conference, PROPOR 2010, vol-ume 6001 of Lecture Notes in Artificial Intelligence(LNAI), pages 30?39, Porto Alegre, RS, Brazil.Springer.Sander Wubben, Antal van den Bosch, and EmielKrahmer.
2012.
Sentence simplification by mono-lingual machine translation.
In Proceedings of the50th Annual Meeting of the Association for Compu-tational Linguistics: Long Papers - Volume 1, ACL?12, pages 1015?1024, Jeju Island, Korea.Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-Mizil, and Lillian Lee.
2010.
For the sake of sim-plicity: Unsupervised extraction of lexical simplifi-cations from Wikipedia.
In Proceedings of the An-nual Meeting of the North American Chapter of theAssociation for Computational Linguistics, pages365?368.Z.
Zhu, D. Bernhard, and I. Gurevych.
2010.
Amonolingual tree-based translation model for sen-tence simplification.
In Proceedings of the Inter-national Conference on Computational Linguistics,pages 1353?1361.19
