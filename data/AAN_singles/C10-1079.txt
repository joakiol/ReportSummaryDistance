Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 698?706,Beijing, August 2010Semantic Role Labeling for News Tweets1,2Xiaohua Liu, 3Kuan Li*, 4Bo Han*, 2Ming Zhou,2Long Jiang, 3Zhongyang Xiong and 2Changning Huang1School of Computer Science and TechnologyHarbin Institute of Technology2Microsoft Research Asia3College of Computer ScienceChongqing University4School of SoftwareDalian University of Technology{xiaoliu, v-kuli, v-bohan, mingzhou, longj}@microsoft.comzyxiong@cqu.edu.cnv-cnh@microsoft.comAbstractNews tweets that report what is happen-ing have become an important real-timeinformation source.
We raise the prob-lem of Semantic Role Labeling (SRL)for news tweets, which is meaningful forfine grained information extraction andretrieval.
We present a self-supervisedlearning approach to train a domain spe-cific SRL system to resolve the problem.A large volume of training data is auto-matically labeled, by leveraging the ex-isting SRL system on news domain andcontent similarity between news andnews tweets.
On a human annotated testset, our system achieves  state-of-the-artperformance, outperforming the SRLsystem trained on news.1 IntroductionTweets are text messages up to 140 characters.Every day, more than 50 million tweets are gen-erated by millions of Twitter users.
According tothe investigation by Pear Analytics (2009), about4% tweets are related to news1.
* This work has been done while the author was visitingMicrosoft Research Asia.1 http://blog.twitter.com/2010/02/measuring-tweets.htmlWe divide news related tweets into two cate-gories: those excerpted from news articles andthose not.
The former kind of tweets, hereaftercalled news excerpt, is formally written whilethe latter, hereafter called news tweet, varies instyle and often is not grammatically correct.
Tounderstand the proportion of news tweets, werandomly selected 1000 tweets related to news,and got 865 news tweets.
Following is an exam-ple of anews tweet, containing oh, yea, whichusually appear in spoken language, and :-(, anemoticon.oh yea and Chile earthquake the earth off it'saxis according to NASA and shorten the dayby a wee second :-(                                     (S1)News tweets arean important informationsource because they keep reporting what is hap-pening in real time.
For example, the earthquakenear Los Angeles that happened on Tuesday,July 29, 2008 was first reported through newstweets only seconds later than the outbreak ofthe quake.
Official news did not emerge aboutthis event until four minutes later.
By then,"Earthquake" was trending on Twitter Searchwith thousands of updates2.However, it is a daunting task for people tofind out information they are interested in fromsuch a huge number of news tweets, thus moti-vating us to conduct some kind of information2 http://blog.twitter.com/2008/07/twitter-as-news-wire.html698extraction such as event mining, where SRLplays a crucial  role (Surdeanu et al, 2003).Considering Sentence 1, suppose the agentearthquake and the patient day for the predicateshorten are identified.
Then it is straightforwardto output the event Chile earthquake shorten theday, which captures the essential informationencoded in this tweet.Following M?rquez (2009), we define SRLfor news tweets as the task of identifying thearguments of a given verb as predicate in a newstweet and assigning them semantic labels de-scribing the roles they play for the predicate.
Tomake our method applicable to general infor-mation extraction tasks,  rather than only tosome special scenarios such as arresting eventextraction, we adopt general semantic roles, i.e.,Agent(A0), Patient(A1), Location(AM-LOC),Temporal(AM-TMP),etc., instead of situation-specific roles (Fillmore et al, 2004) such asSuspect, Authorities, and Offense in an arrestframe.Our first attempt is to directly apply the state-of-art SRL system (Meza-Ruiz and Riedel, 2009)that trained on the CoNLL 08 shared task da-taset(Surdeanu et al, 2008), hereafter calledSRL-BS, to news tweets.
Not surprisingly, weobserve its F1 score drops sharply from 75.5%on news corpus to 43.3% on our human annotat-ed news tweets, owing much to the informalwritten style of news tweets.Therefore, we have to build a domain specificSRL system for news tweets.
Given the diversi-fied styles of news tweets, building such a sys-tem requires a larger number of annotated newstweets, which are not available, and are not af-fordable for human labeling.
We propose a novelmethod to automatically annotate news tweets,which leverages the existing resources of SRLfor news domain, and content similarity betweennews and news tweets.
We argue that the sameevent is likely to be reported by both news andnews tweets, which results in  content similaritybetween the news and news tweet.
Further, weargue that the news and news tweets reportingthe same event tend to have similar predicate-argument structures.
We tested our assumptionson the event Chile earthquake that happened onMatch 2nd, 2010.
We got 261 news and 722 newstweets published on the same day that describedthis event.
Sentence 2 and 3 are two examplesof the news excerpts and Sentence 1 is one ex-ample of news tweets for this event.Chile Earthquake Shortened Earth Day    (S2)Chile Earthquake Shortened Day              (S3)Obviously Sentence 1, 2 and 3 all have predi-FDWH ?shortened?
with the same A0 and A1 ar-guments.
Our manually checking showed that inaverage each news tweet in those 993 sampleshad 2.4 news excerpts that had the same predi-cate-argument structures.Our news tweet annotation approach consistsof four steps.
First, we submit hot queries toTwitter and for each query we obtain a list oftweets.
Second, for each list of tweets, we singleout news excerpts using heuristic rules and re-move them from the list, conduct SRL on newsexcerpts using SRL-BS, and cluster them interms of the similarity in content and predicate-argument structures.
Third, for each list oftweets, we try to merge every remaining tweetinto one news excerpt cluster according to itscontent similarity to the cluster.
Those that canbe put into one news group are regarded as newstweet.
Finally, semantic structures of news ex-cerpts are passed to the news tweet in the samegroup through word alignment.Our domain specific SRL system is thentrained on automatically constructed trainingdata using the Conditional Random Field (CRF:Lafferty et al, 2001) learning framework.
Oursystem is evaluated on a human labeled dataset,and achieves state-of-the-art performance, out-performing the baseline SRL-BS.Our contributions can be summarized as fol-lows:1) We propose to conduct SRL for newstweets for fine grained information ex-traction and retrieval;2) We present a semi-supervised learningapproach to train a domain specific SRLsystem for news tweets, which outper-forms SRL-BS and achieves the state-of-the-art performance on a human labeleddataset.The rest of this paper is organized as follows:In the next section, we review related work.
InSection 3 we detail key components of our ap-proach.
In Section 4, we setup experiments andevaluate the effectiveness of our method.
Final-699ly, Section 5 concludes and presents the futurework.2 Related WorkOur related work falls into two categories: SRLon news and domain adaption.As for SRL on news, most researchers usedthe pipelined approach, i.e., dividing the taskinto several phases such as argument identifica-tion, argument classification, global inference,etc.,  and conquering them individually (Xue andPalmer, 2004; Koomen et al, 2005; Cohn andBlunsom, 2005; Punyakanok et al, 2008;Toutanova et al, 2005; Toutanova et al, 2008).Exceptions to the pipelined approach exist.M?rquez et al (2005) sequentially labeled thewords according to their positions relative to anargument (i.e., inside, outside or at the beginningof it).
Carreras et al (2004) and Surdeanu et al(2007) jointly labeled all the predicates.
Vickreyand Koller(2008) simplified the input sentenceby hand-written and machine learnt rules beforeconducting SRL.
Some other approaches simul-taneously resolved all the sub-tasks by integrat-ing syntactic parsing and SRL into a single mod-el (Musillo and Merlo, 2006; Merlo and Musillo,2008), or by using Markov Logic Networks(MLN, Richardson and Domingos, 2005) as thelearning framework (Riedel and Meza-Ruiz,2008; Meza-Ruiz and Riedel, 2009).All the above approaches focus on sentencesfrom news articles or other formal documents,and depend on human annotated corpus fortraining.
To our knowledge, little study has beencarried out on SRL for news tweets.As for domain adaption, some researchers re-garded the out-of-GRPDLQ GDWD DV ?SULRUNQRZOHGJH?DQGestimated the model parametersby maximizing the posterior under this prior dis-tribution, and successfully applied their ap-proach to language modeling (Bacchiani andRoark, 2003) and parsing (Roark and Bacchiani,2003).
Daum?
III and Marcu (2006) presented aQRYHO IUDPHZRUN E\ GHILQLQJ D ?JHQHUDO Go-PDLQ?EHWZHHQWKH?WUXO\LQ-GRPDLQ?DQG?WUXO\out-of-GRPDLQ?Unlike existing domain adaption approaches,our method is about adapting SRL system onnews domain to the news tweets domain, twodomains that differ in writing style but are linkedthrough content similarity.3 Our MethodOur method of SRL for news tweets is to train adomain specific SRL on automatically annotatedtraining data as briefed in Section 1.In this section we present details of the fivecrucial components of our method, i.e., newsexcerpt identification, news excerpt clustering,news tweets identification, semantic structuremapping, and the domain specific SRL systemconstructing.3.1 News Excerpt IdentificationWe use one heuristic rule to decide whether ornot a tweet is news excerpt:  if a tweet has a linkto a news article and its text content is includedby the news article, it is news excerpt, otherwisenot.Given a tweet, to apply this rule, we first ex-tract the content link and expand it, if any, intothe full link with the unshorten service3.
Thisstep is necessary because content link in tweet isusually shortened to reduce the total amount ofcharacters.
Next, we check if the full link pointsto any of the pre-defined news sites, which, inour experiments, are 57 English news websites.If yes, we download the web page and check if itexactly contains the text content of the inputtweet.
Figure 1 illustrates this process.Figure 1.
An illustration of news excerpt identi-fication.To test the precision of this approach, whilepreparing for the training data for the experi-ments, we checked 100 tweets that were identi-fied as news excerpt by this rule to find out theyall are excerpted from news.3 http://unshort.me7003.2 News Excerpt ClusteringGiven as input a list of news excerpts concerningthe same query and published in the same timescope, this component uses the hierarchical ag-glomerative clustering algorithm (Manning etal., 2008) to divide news excerpts into groups interms of the similarity in content and predicate-argument structures.Before clustering, for every news excerpt, weremove the content link and other metadata suchas author, retweet marks (starting with RT @),reply marks (starting with @ immediately afterthe author), hash tags (starting with #), etc., andkeep only the text content; then it is furtherparsed into tokens, POS tags, chunks and syntac-tic tree using the OpenNLP toolkit4.
After that,SRL is conducted with SRL-BS to get predicate-argument structures.
Finally, every news excerptis represented as frequency a vector of terms,including tokens, POS tagger, chunks, predicate-argument structures, etc.
A news cluster is re-garded as a ?macro?
news excerpt and is alsorepresented as a term frequency vector, i.e., thesum of all the term vectors in the cluster.
Noisyterms, such as numbers and predefined stopwords are excluded from the frequency vector.To reduce data sparseness, words are stemmedby Porter stemmer (Martin F. Porter, 1980).The cosine similarity is used to measure therelevance between two clusters, as defined inFormula 1.,'''CVCVCVCVCCCSu?
(1)Where C, &?
denote two clusters, CV, CV?
de-note  the term frequency vectors of C and  &?respectively, and CS(C, &?)
stands for the  co-sine similarity between C and  &?.Initially, one news excerpt forms one cluster.Then the clustering process repeats merging thetwo most similar clusters into one till the simi-larity between any pair of clusters is below athreshold, which is experimentally set to 0.7 inour experiments.During the training data preparation process,we randomly selected 100 clusters, each with 3.2pieces of news in average.
For every pair ofnews excerpts in the same cluster, we checked if4 http://opennlp.sourceforge.net/they shared similar contents and semantic struc-tures, and found out that 91.1% were the cases.3.3 News Tweets IdentificationAfter news excerpts are identified and removedfrom the list, every remaining tweet is checked ifit is a news tweet.
Here we group news excerptsand news tweets together in two steps because 1)news excerpts count for only a small proportionof all the tweets in the list, making our two-stepclustering algorithm more efficient; and 2) one-step clustering tends to output meaningless clus-ters that include no news tweets.Intuitively, news tweet, more often than not,have news counterparts that report similar con-tents.
Thus we use the following rule to identifynews tweets: if the content similarity betweenthe tweet and any news excerpt cluster is greaterthan a threshold, which is experimentally set to0.7 in our experiments, the tweet is a news tweet,otherwise it is not.
Furthermore, each newstweet is merged into the cluster with most simi-lar content.
Finally, we re-label any news tweetas news excerpt, which is then process by SRL-BS, if its content similarity to the cluster exceedsa threshold, which is experimentally set to 0.9 inour experiments.Again, the cosine similarity is used to meas-ure the content similarity between tweet andnews excerpt cluster.
Each tweet is repressed asa term frequency vector.
Before extracting termsfrom tweet, tweet metadata is removed and arule-based normalization process is conducted torestore abnormal strLQJVVD\?
DSRV?LQWRWKHLUKXPDQ IULHQGO\ IRUP VD\ ?
? ? 1H[W VWHPPLQJtools and OpenNLP are applied to get lemmas,POS tags, chunks, etc., and noisy terms are fil-tered.We evaluated the performance of this ap-proach when preparing for the training data.
Werandomly sampled 500 tweets that were identi-fied as news tweets, to find that 93.8% were truenews tweets.3.4 Semantic Structure MappingSemantic structure mapping is formed as thetask of word alignment from news excerpt tonews tweet.
A HMM alignment model is trainedwith GIZA++ (Franz and Hermann, 2000) on all(news excerpt, news tweet) pairs in the samecluster.
After word alignment is done, semantic701information attached to a word in a news excerptis passed to the corresponding word in the newstweet as illustrated in Figure 2.Chile Earthquake Shortened Earth DayA0 predicate A1NASA and shorten the day by a wee second :-(oh yea and Chile earthquake the earth off it's axis according toFigure 2.
An example of mapping semanticstructures from news excerpts to news tweets.In Figure 2, shorten, earthquake and day intwo sentences are aligned, respectively; and twopredicate-argument structures in the first sen-tence, i.e., (shortened, earthquake, A0), (short-ened, day, A1), are passed to the second.News tweets may receive no semantic infor-mation from related news excerpts after mapping,because of word alignment errors or no newsexcerpt in the cluster with similar semanticstructures.
Such tweets are dropped.Mapping may also introduce cases that violatethe following two structural constraints in SRL(Meza-Ruiz and Riedel, 2009): 1) one (predi-cate, argument) pair has only one role label inone sentence; and 2) for each predicate, each ofthe proper arguments (A0~A5) can occur at mostonce.
Those conflicts are largely owing to thenoisy outputs of SRL trained on news and to thealignment errors.
While preparing for the train-ing data for our experiments, we found 38.9% ofnews tweets had such conflicts.A majority voting schema and the structuralconstrains are used to resolve the conflicts asdescribed below.1) Step 1, for every cluster, each (predicate,argument, role) is weighted according toits frequency in the cluster;2) Step 2, for every cluster, detect conflictsusing the structural constrains; if no con-flicts exist, stop; otherwise go to Step 3;3) Step 3, for every cluster, keep the onewith higher weight in each conflicting(predicate, argument, role) pair; if theweights are equal,  drop both;Here is an example to show the conflictingresolution process.
Consider the cluster includ-ing Sentence 1, 2 and 3, where (shorten, earth-quake, A0), (shorten, earthquake, A1), (shorten,axis, A0), and (shorten, day, A1) occur 6, 4, 1and 3 times, respectively.
This cluster includesthree conflicting pairs:1) (shorten, earthquake, A0) vs. (shorten,earthquake, A1);2) (shorten, earthquake, A1) vs. (shorten,day, A1);3) (shorten, earthquake, A0) vs. (shorten, ax-is, A0);The first pair is first resolved, causing (short-en, earthquake, A0) to be kept and (shorten,earthquake, A1) removed, which leads to thesecond pair being resolved as well; then we pro-cess the third pair resulting in (shorten, earth-quake, A0) being kept and (shorten, axis, A0)dropped; finally (shorten, earthquake, A0) and(shorten, day, A1) stay in the cluster.The conflicting resolution algorithm is sensi-tive to the order of conflict resolution in Step 3.Still consider the three conflicting pairs listedabove.
If the second pair is first processed, only(shorten, earthquake, A0) will be left.
Our strat-egy is to first handle the conflict resolving whichleads to most conflicts resolved.We tested the performance of this semanticstructure mapping strategy while preparing forthe training data.
We randomly selected 56 newstweets with conflicts and manually annotatedthem with SRL.
After the conflict resolutionmethod was done, we observed that 38 newstweets were resolved correctly, 9 resolved butincorrectly, and 9 remain unresolved, suggestingthe high precision of this method, which fits ourtask.
We leave it to our future work to studymore advanced approach for semantic structuremapping.3.5 SRL System for News TweetsFollowing M?rquez et al (2005), we regard SRLfor tweets as a sequential labeling task, becauseof its joint inference ability and its openness tosupport other languages.We adopt conventional features for each tokendefined in M?rquez et al(2005),  such as thelemma/POS tag of the current/previous/next to-ken, the lemma of predicate and its combinationwith the lemma/POS tag of the current token, thevoice of the predicate (active/passive), the dis-tance between the current token and the predi-cate, the relative position of the current token to702the predicate, and so on.
We do not use featuresrelated to syntactic parsing trees, to allow oursystem not to rely on any syntactic parser, whoseperformance depends on style and language oftext, which limits the generality of our system.Before extracting features, we perform a pre-processing step to remove tweet metadata andnormalize tweet text content, as described inSection 3.3.
The OpenNLP toolkit is used forfeature extraction, and the CRF++ toolkit 5  isused to train the model.4 ExperimentsIn this section, we evaluate our SRL system on agold-standard dataset consisting of 1,110 humanannotated news tweets and show that our systemachieves the state-of-the-art performance com-pared with SRL-BS that is trained on news.
Fur-thermore, we study the contribution of automati-cally generated training data.4.1 Evaluation MetricWe adopt the widely used precision (Pre.
), recall(Rec.)
and F-score (F., the harmonic mean ofprecision and recall) as evaluation metrics.4.2 Baseline SystemWe use SRL-BS as our baseline because of itsstate-of-art performance on news domain, and itsreadiness to use as well.4.3 Data PreparationWe restrict to English news tweets to test ourmethod.
Our method can label news tweets ofother languages, given that the related tools suchas the SRL system on news domain, the wordalignment tool, OpenNLP, etc., can support oth-er languages.We build two corpora for our experiments:one is the training dataset of 10,000 news tweetswith semantic roles automatically labeled; theother is the gold-standard dataset of 1,110 newstweets with semantic roles manually labeled.Training DatasetWe randomly sample 80 queries from 300English queries extracted from the top stories ofBing news, Google news and Twitter trendingtopics from March 1, 2010 to March 4, 2010.5 http://crfpp.sourceforge.net/Submitting the 80 queries to Twitter search,we retrieve and download 512,000 tweets, fromwhich we got 4,785 news excerpts and 11,427news tweets, which were automatically annotat-ed using the method described in Section 3.Furthermore, 10,000 tweets are randomly se-lected from the automatically annotated newstweets, forming the training dataset, while theother 1,427 news tweets are used to construct thegold-standard dataset.Gold-standard DatasetWe ask two people to annotate the 1,427 newstweets, following the Annotation guidelines forPropBank6 with one exception: for phrasal ar-guments, only the head word is labeled as theargument, because our system and SRL-BS con-duct word level SRL.317 news tweets are dropped because of in-consistent annotation, and the remaining 1,110news tweets form the gold-standard dataset.Quality of Training datasetSince the news tweets in the gold-standard da-taset are randomly sampled from the automati-cally labeled corpus and are labeled by both hu-man and machine, we use them to estimate thequality of training data, i.e., to which degree theautomatically generated results are similar tohumans?.We find that our method achieves 75.6% F1score, much higher than the baseline, suggestingthe relatively high quality of the training data.4.4 Result and AnalysisTable 1 reports the experimental results of oursystem (SRL-TS) and the baseline on the gold-standard dataset.Precision Recall F-ScoreSRL-BS 36.0 % 54.5% 43.3%SRL-TS 78.0% 57.1% 66.0%Table 1.
Performances of our system and thebaseline on the gold-standard dataset.As shown in Table 1, our system performsmuch better than the baseline on the gold-standard dataset in terms of all metrics.
We ob-serve two types of errors that are often made by6 http://verbs.colorado.edu/~mpalmer/projects/ace/PBguidelines.pdf703SRL-BS but not so often by our system, whichlargely explains the difference in performance.The first type of errors, which accounts for25.3% of the total errors made by SRL-BS, iscaused by the informal written style, such as el-lipsis, of news tweets.
For instance, for the ex-ample Sentence 1 listed in Section 1, the SRL-BS incorrectly identify earth as the A0 argumentof the predicate shorten.
The other type of errors,which accounts for 10.2% of the total errorsmade by SRL-BS, is related to the discretionarycombination of news snippets.
For example,consider the following news tweet:The Chile earthquake shifted the earth's axis,"shortened the length of an Earth day by 1.26miliseconds".
(S4)We analyze the errors made by our systemand find that 12.5% errors are attributed to thecomplex syntactic structures, suggesting thatcombining our system with systems on newsdomain is a promising direction.
For example,our system cannot identify the A0 argument ofthe predicate shortened, because of its blindnessof attributive clause; in contrast, SRL-BS workson this case.wow..the earthquake that caused the 2004 In-dian Ocean tsunami shortened the day by al-most 3 microseconds..what does that evenmean?!
HOW?
(S5)We also find that 32.3% of the errors made byour system are more or less related to the train-ing data, which has noise and cannot fully repre-sent the knowledge of SRL on news tweets.
Forinstance, our system fails to label the followingsentence, partially because the predicate strikedoes not occur in the training set.8.8-Magnitude-Earthquake-Strikes-Chile (S6)We further study how the size of automatical-O\ODEHOHGWUDLQLQJGDWDDIIHFWVRXUV\VWHP?VSHr-formance, as illustrated in Figure 3.
We conducttwo sets of experiments: in the first set, the train-ing data is automatically labeled and the testingdata is the gold-standard dataset; in the secondset, half of the news tweets from the gold-standard dataset are added to the training data,the remaining half forms the testing dataset.Curve 1 and 2 represent the experimental resultsof set 1 and 2, respectively.From Curve 1, we see that RXUV\VWHP?VSHr-formance increases sharply when the trainingdata size varies from 5,000 to 6,000; then in-creases relatively slowly with more training data;and finally reaches the highest when all trainingdata is used.
Curve 2 reveals a similar trend.Figure 3.
Performance on training data of vary-ing size.This phenomenon is largely due to the com-peting between two forces: the noise in the train-ing data, and the knowledge of SRL encoded inthe training data.Interestingly, from Figure 3, we observe thatthe contribution of human labeled data is nolonger significant after 6,000 automatically la-beled training data is used, reaffirming the effec-tiveness of the training data.5 Conclusions and Future WorkWe propose to conduct SRL on news tweets forfine grained information extraction and retrieval.We present a self-supervised learning approachto train a domain specific SRL system for newstweets.
Leveraging the SRL system on newsdomain and content similarity between news andnews tweets, our approach automatically labels alarge volume of training data by mapping SRL-BS generated results of news excerpts to newstweets.
Experimental results show that our sys-tem outperforms the baseline and achieves thestate-of-the-art performance.In the future, we plan to enlarge training datasize and test our system on a larger dataset; wealso plan to further boost the performance of oursystem by incorporating tweets specific featuressuch as hash tags, reply/re-tweet marks into our704CRF model, and by combining our system withSRL systems trained on news.ReferencesBacchiani, Michiel and Brian Roark.
2003.
Unsuper-vised language model adaptation.
Proceedings ofthe 2003 International Conference on Acoustics,Speech and Signal Processing, volume 1, pages:224-227Carreras, Xavier, Llu?s M?rquez, and Grzegorz&KUXSD?D+LHUDUFKLFDOUHFRJQLWLRQRISURSo-sitional arguments with Perceptrons.
Proceedingsof the Eighth Conference on Computational Natu-ral Language Learning, pages: 106-109.Cohn, Trevor and Philip Blunsom.
2005.
Semanticrole labeling with tree conditional random fields.Proceedings of the Ninth Conference on Computa-tional Natural Language Learning, pages: 169-172.Daum?, Hal III and Daniel Marcu.
2006.
Domainadaptation for statistical classifiers.
Journal of Ar-tificial Intelligence Research, 26(1), 101-126.Fillmore, Charles J., Josef Ruppenhofer, Collin F.Baker.
2004.
FrameNet and Representing the Linkbetween Semantic and Syntactic Relations.
Com-putational Linguistics and Beyond, Institute ofLinguistics, Academia Sinica.Kelly, Ryan, ed.
2009.
Twitter Study Reveals Inter-esting Results About Usage.
San Antonio, Texas:Pear Analytics.Koomen, Peter, Vasin Punyakanok, Dan Roth, andWen-tau Yih.
2005.
Generalized inference withmultiple semantic role labeling systems.
Proceed-ings of the Ninth Conference on ComputationalNatural Language Learning, pages: 181-184.Lafferty, John D., Andrew McCallum, Fernando C.N.
Pereira.
2001.
Conditional Random Fields:Probabilistic Models for Segmenting and LabelingSequence Data.
Proceedings of the Eighteenth In-ternational Conference on Machine Learning,pages: 282-289.Manning, Christopher D., Prabhakar Raghavan andHinrich Schtze.
2008.
Introduction to InformationRetrieval.
Cambridge University Press, Cam-bridge, UK.M?rquez, Llu?s, Jesus Gim?nez Pere Comas andNeus Catal?.
2005.
Semantic Role Labeling as Se-quential Tagging.
Proceedings of the Ninth Con-ference on Computational Natural LanguageLearning, pages: 193-196.M?rquez, Llu?s.
2009.
Semantic Role Labeling Past,Present and Future, Tutorial of ACL-IJCNLP2009.Merlo, Paola and Gabriele Musillo.
2008.
Semanticparsing for high-precision semantic role labelling.Proceedings of the Twelfth Conference on Compu-tational Natural Language Learning, pages: 1-8.Meza-Ruiz, Ivan and Sebastian Riedel.
2009.
JointlyIdentifying Predicates, Arguments and Senses us-ing Markov Logic.
Human Language Technolo-gies: The 2009 Annual Conference of the NorthAmerican Chapter of the ACL, pages: 155-163.Musillo, Gabriele and Paola Merlo.
2006.
AccurateParsing of the proposition bank.
Proceedings ofthe Human Language Technology Conference ofthe NAACL, pages: 101-104.Och, Franz Josef, Hermann Ney.
Improved StatisticalAlignment Models.
Proceedings of the 38th Annu-al Meeting of the Association for ComputationalLinguistics, pages: 440-447.Porter, Martin F. 1980.
An algorithm for suffix strip-ping.
Program, 14(3), 130-137.Punyakanok, Vasin, Dan Roth and Wen-tau Yih.2008.
The importance of syntactic parsing and in-ference in semantic role labeling.
Journal of Com-putational Linguistics, 34(2), 257-287.Richardson, Matthew and Pedro Domingos.
2005.Markov logic networks.
Technical Report, Univer-sity of Washington, 2005.Riedel, Sebastian and Ivan Meza-Ruiz.
2008.
Collec-tive semantic role labelling with Markov Logic.Proceedings of the Twelfth Conference on Compu-tational Natural Language Learning, pages: 193-197.Roark, Brian and Michiel Bacchiani.
2003.
Super-vised and unsupervised PCFG adaptation to noveldomains.
Proceedings of the 2003 Conference ofthe North American Chapter of the Association forComputational Linguistics on Human LanguageTechnology, volume 1, pages: 126-133.Surdeanu, Mihai, Sanda Harabagiu, JohnWilliamsand Paul Aarseth.
2003.
Using predicate-argumentstructures for information extraction.
Proceedingsof the 41st Annual Meeting on Association forComputational Linguistics, volume 1, pages: 8-15.Surdeanu, Mihai, Llu?s M?rquez, Xavier Carreras andPere R. Comas.
2007.
Combination strategies forsemantic role labeling.
Journal of Artificial Intelli-gence Research, 29(1), 105-151.705Surdeanu, Mihai, Richard Johansson, Adam Meyers,Llu?s M?rquez, and Joakim Nivre.
2008.
The conll2008 shared task on joint parsing of syntactic andsemantic dependencies.
Proceedings of the TwelfthConference on Computational Natural LanguageLearning, pages: 159-177.Toutanova, Kristina, Aria Haghighi and ChristopherD.
Manning.
2005.
Joint learning improves seman-tic role labeling.
Proceedings of the 43rd AnnualMeeting of the Association for Computational Lin-guistics, pages: 589-596.Toutanova, Kristina, Aria Haghighi and ChristopherD.
Manning.
2008.
A global joint model for se-mantic role labeling.
Journal of ComputationalLinguistics, 34(2), 161-191.Vickrey, David and Daphne Koller.
2008.
Applyingsentence simplification to the conll-2008 sharedtask.
Proceedings of the Twelfth Conference onComputational Natural Language Learning, pag-es: 268-272Xue, Nianwen and Martha Palmer.
2004.
Calibratingfeatures for semantic role labeling.
Proceedings ofthe Conference on Empirical Methods in NaturalLanguage Processing, pages: 88-94.706
