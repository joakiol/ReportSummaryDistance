Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 622?630,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsNon-Monotonic Sentence Alignment via Semisupervised LearningXiaojun Quan, Chunyu Kit and Yan SongDepartment of Chinese, Translation and LinguisticsCity University of Hong Kong, HKSAR, China{xiaoquan,ctckit,[yansong]}@[student.
]cityu.edu.hkAbstractThis paper studies the problem of non-monotonic sentence alignment, motivatedby the observation that coupled sentencesin real bitexts do not necessarily occurmonotonically, and proposes a semisuper-vised learning approach based on two as-sumptions: (1) sentences with high affinityin one language tend to have their counter-parts with similar relatedness in the other;and (2) initial alignment is readily avail-able with existing alignment techniques.They are incorporated as two constraintsinto a semisupervised learning frameworkfor optimization to produce a globally op-timal solution.
The evaluation with real-world legal data from a comprehensivelegislation corpus shows that while exist-ing alignment algorithms suffer severelyfrom non-monotonicity, this approach canwork effectively on both monotonic andnon-monotonic data.1 IntroductionBilingual sentence alignment is a fundamentaltask to undertake for the purpose of facilitatingmany important natural language processing ap-plications such as statistical machine translation(Brown et al, 1993), bilingual lexicography (Kla-vans et al, 1990), and cross-language informa-tion retrieval (Nie et al, 1999).
Its objective is toidentify correspondences between bilingual sen-tences in given bitexts.
As summarized by Wu(2010), existing sentence alignment techniquesrely mainly on sentence length and bilingual lex-ical resource.
Approaches based on the formerperform effectively on cognate languages but noton the others.
For instance, the statistical cor-relation of sentence length between English andChinese is not as high as that between two Indo-European languages (Wu, 1994).
Lexicon-basedapproaches resort to word correspondences in abilingual lexicon to match bilingual sentences.
Afew sentence alignment methods and tools havealso been explored to combine the two.
Moore(2002) proposes a multi-pass search procedure us-ing both sentence length and an automatically-derived bilingual lexicon.
Hunalign (Varga et al,2005) is another sentence aligner that combinessentence length and a lexicon.
Without a lexicon,it backs off to a length-based algorithm and thenautomatically derives a lexicon from the align-ment result.
Soon after, Ma (2006) develops thelexicon-based aligner Champollion, assuming thatdifferent words have different importance in align-ing two sentences.Nevertheless, most existing approaches to sen-tence alignment follow the monotonicity assump-tion that coupled sentences in bitexts appear ina similar sequential order in two languages andcrossings are not entertained in general (Langlaiset al, 1998; Wu, 2010).
Consequently the task ofsentence alignment becomes handily solvable bymeans of such basic techniques as dynamic pro-gramming.
In many scenarios, however, this pre-requisite monotonicity cannot be guaranteed.
Forexample, bilingual clauses in legal bitexts are of-ten coordinated in a way not to keep the sameclause order, demanding fully or partially crossingpairings.
Figure 1 shows a real excerpt from a leg-islation corpus.
Such monotonicity seriously im-pairs the existing alignment approaches foundedon the monotonicity assumption.This paper is intended to explore the problem ofnon-monotonic alignment within the frameworkof semisupervised learning.
Our approach is mo-tivated by the above observation and based onthe following two assumptions.
First, monolin-gual sentences with high affinity are likely to havetheir translations with similar relatedness.
Follow-ing this assumption, we propose the conceptionof monolingual consistency which, to the best of622British Overseas citizen" (??????)
means a person who has thestatus of a British Overseas citizen under the British Nationality Act1981 (1981 c. 61 U.K.)British protected person" (???????)
means a person who hasthe status of a British protected person under the British Nationality Act1981 (1981 c. 61 U.K.)...1.
Interpretation of words and expressionsBritish citizen" (???? )
means a person who has the status of aBritish citizen under the British Nationality Act 1981 (1981 c. 61 U.K.)British Dependent Territories citizen" (??????)
means a personwho has or had the status of a British Dependent Territories citizenunder the British Nationality Act 1981 (1981 c. 61 U.K.)British enactment" and "imperial enactment" (??????)
Mean-(a) any Act of Parliament; (b) any Order in Council; and (c) any rule,regulation, proclamation, order, notice, rule of court, by-law or otherinstrument made under or by virtue of any such Act or Order in Council???????
(British Overseas citizen)????1981????????
(1981 c. 61 U.K.)???????????????????
(British Dependent Territories citizen)????1981????????
(1981 c. 61 U.K.)????????????????1.??????????????????????????????????????????????????????????????????????????????????????????????
(British citizen)????1981????????(1981c.
61 U.K.)?????????????????
(British enactment, imperial enactment)??(a)??????????(b)?????????(c)??????????????????????????????????????????????????..."""""????
?Figure 1: A real example of non-monotonic sentence alignment from BLIS corpus.our knowledge, has not been taken into account inany previous work of alignment.
Second, initialalignment of certain quality can be obtained bymeans of existing alignment techniques.
Our ap-proach attempts to incorporate both monolingualconsistency of sentences and bilingual consistencyof initial alignment into a semisupervised learningframework to produce an optimal solution.
Ex-tensive evaluations are performed using real-worldlegislation bitexts from BLIS, a comprehensivelegislation database maintained by the Depart-ment of Justice, HKSAR.
Our experimental resultsshow that the proposed method can work effec-tively while two representatives of existing align-ers suffer severely from the non-monotonicity.2 Methodology2.1 The ProblemAn alignment algorithm accepts as input a bi-text consisting of a set of source-language sen-tences, S = {s1, s2, .
.
.
, sm}, and a set of target-language sentences, T = {t1, t2, .
.
.
, tn}.
Dif-ferent from previous works relying on the mono-tonicity assumption, our algorithm is generalizedto allow the pairings of sentences in S and Tto cross arbitrarily.
Figure 2(a) illustrates mono-tonic alignment with no crossing correspondencesin a bipartite graph and 2(b) non-monotonic align-ment with scrambled pairings.
Note that it is rela-tively straightforward to identify the type of many-to-many alignment in monotonic alignment usingtechniques such as dynamic programming if thereis no scrambled pairing or the scrambled pairingsare local, limited to a short distance.
However,the situation of non-monotonic alignment is muchmore complicated.
Sentences to be merged into abundle for matching against another bundle in theother language may occur consecutively or discon-tinuously.
For the sake of simplicity, we will notconsider non-monotonic alignment with many-to-many pairings but rather assume that each sen-tence may align to only one or zero sentence inthe other language.Let F represent the correspondence relation be-tween S and T , and therefore F ?
S ?
T .
Letmatrix F denote a specific alignment solution ofF , where Fij is a real score to measure the likeli-hood of matching the i-th sentence si in S againstthe j-th sentence tj in T .
We then define an align-ment function A : F ?
A to produce the finalalignment, where A is the alignment matrix for Sand T , with Aij = 1 for a correspondence be-tween si and tj and Aij = 0 otherwise.2.2 Semisupervised LearningA semisupervised learning framework is intro-duced to incorporate the monolingual and bilin-gual consistency into alignment scoringQ(F ) = Qm(F ) + ?Qb(F ), (1)where Qm(F ) is the term for monolingual con-straint to control the consistency of sentences withhigh affinities, Qb(F ) for the constraint of initialalignment obtained with existing techniques, and?
is the weight between them.
Then, the optimalalignment solution is to be derived by minimizingthe cost function Q(F ), i.e.,F ?
= argminFQ(F ).
(2)623s1s2s3s4s5s6t1t2t3t4t5t6(a)s1s2s3s4s5s6t1t2t3t4t5t6(b)Figure 2: Illustration of monotonic (a) and non-monotonic alignment (b), with a line representing thecorrespondence of two bilingual sentences.In this paper, Qm(F ) is defined as14m?i,j=1Wijn?k,l=1Vkl(Fik?DiiEkk?
Fjl?DjjEll)2, (3)whereW and V are the symmetric matrices to rep-resent the monolingual sentence affinity matricesin S and T , respectively, and D and E are the di-agonal matrices with entries Dii = ?jWij andEii =?j Vij .
The idea behind (3) is that to min-imize the cost function, the translations of thosemonolingual sentences with close relatedness re-flected inW and V should also keep similar close-ness.
The bilingual constraint term Qb(F ) is de-fined asQb(F ) =m?i=1n?j=1(Fij ?
A?ij)2, (4)where A?
is the initial alignment matrix obtainedby A : F?
?
A?.
Note that F?
is the initial relationmatrix between S and T .The monolingual constraint term Qm(F ) de-fined above corresponds to the smoothness con-straint in the previous semisupervised learningwork by Zhou et al (2004) that assigns higherlikelihood to objects with larger similarity to sharethe same label.
On the other hand, Qb(F ) corre-sponds to their fitting constraint, which requiresthe final alignment to maintain the maximum con-sistency with the initial alignment.Taking the derivative of Q(F ) with respect toF , we have?Q(F )?F = 2F ?
2SFT + 2?F ?
2?A?, (5)where S and T are the normalized matrices of Wand V , calculated by S = D?1/2WD?1/2 andT = E?1/2V E?1/2.
Then, the optimal F ?
is tobe found by solving the equation(1 + ?
)F ?
?
SF ?T = ?A?, (6)which is equivalent to ?F ?
?
F ??
= ?
with?
= (1 + ?
)S?1, ?
= T and ?
= ?S?1A?.This is in fact a Sylvester equation (Barlow et al,1992), whose numerical solution can be found bymany classical algorithms.
In this research, it issolved using LAPACK,1 a software library for nu-merical linear algebra.
Non-positive entries in F ?indicate unrealistic correspondences of sentencesand are thus set to zero before applying the align-ment function.2.3 Alignment FunctionOnce the optimal F ?
is acquired, the remainingtask is to design an alignment function A to con-vert it into an alignment solution.
An intuitive ap-proach is to use a heuristic search for local op-timization (Kit et al, 2004), which produces analignment with respect to the largest scores ineach row and each column.
However, this does notguarantee a globally optimal solution.
Figure 3 il-lustrates a mapping relation matrix onto an align-ment matrix, which also shows that the optimalalignment cannot be achieved by heuristic search.Banding is another approach frequently used toconvert a relation matrix to alignment (Kay andRo?scheisen, 1993).
It is founded on the observa-tion that true monotonic alignment paths usuallylie close to the diagonal of a relation matrix.
How-ever, it is not applicable to our task due to the non-monotonicity involved.
We opt for converting arelation matrix into specific alignment by solving1http://www.netlib.org/lapack/624alignmentmatrixrelationmatrix21 2 43 5 6 71345600.4 0 0.5 0 000.30 0 0.6 0 0000 0 0 0 000.40 0 0 0.2 000.50 0 0 0 00.600.1 0 0 0 00.821 2 43 5 6 71345601 0 0 0 0000 0 1 0 0000 0 0 0 0000 0 0 1 0010 0 0 0 0000 0 0 0 01Figure 3: Illustration of sentence alignment from relation matrix to alignment matrix.
The scores markedwith arrows are the best in each row/column to be used by the heuristic search.
The right matrix repre-sents the corresponding alignment matrix by our algorithm.the following optimizationA =argmaxXm?i=1n?j=1XijFij (7)s.t.m?i=1Xij ?
1,n?j=1Xij ?
1, Xij ?
{0, 1}This turns sentence alignment into a problem tobe resolved by binary linear programming (BIP),which has been successfully applied to word align-ment (Taskar et al, 2005).
Given a scoring matrix,it guarantees an optimal solution.2.4 Alignment InitializationOnce the above alignment function is available,the initial alignment matrix A?
can be derived froman initial relation matrix F?
obtained by an avail-able alignment method.
This work resorts to an-other approach to initializing the relation matrix.In many genres of bitexts, such as governmenttranscripts or legal documents, there are a certainnumber of common strings on the two sides of bi-texts.
In legal documents, for example, transla-tions of many key terms are usually accompaniedwith their source terms.
Also, common number-ings can be found in enumerated lists in bitexts.These kinds of anchor strings provide quite reli-able information to link bilingual sentences intopairs, and thus can serve as useful cues for sen-tence alignment.
In fact, they can be treated as aspecial type of highly reliable ?bilexicon?.The anchor strings used in this work are derivedby searching the bitexts using word-level invertedindexing, a basic technique widely used in infor-mation retrieval (Baeza-Yates and Ribeiro-Neto,2011).
For each index term, a list of postings iscreated.
Each posting includes a sentence identi-fier, the in-sentence frequency and positions of thisterm.
The positions of terms are intersected to findcommon anchor strings.
The anchor strings, oncefound, are used to calculate the initial affinity F?ijof two sentences using Dice?s coefficientF?ij =2|C1i ?
C2j ||C1i|+ |C2j |(8)where C1i and C2j are the anchor sets in si and tj ,respectively, and | ?
| is the cardinality of a set.Apart from using anchor strings, other avenuesfor the initialization are studied in the evaluationsection below, i.e., using another aligner and anexisting lexicon.2.5 Monolingual AffinityAlthough various kinds of information from amonolingual corpus have been exploited to booststatistical machine translation models (Liu et al,2010; Su et al, 2012), we have not yet beenexposed to any attempt to leverage monolingualsentence affinity for sentence alignment.
In ourframework, an attempt to this can be made throughthe computation of W and V .
Let us take W as anexample, where the entry Wij represents the affin-ity of sentence si and sentence sj , and it is set to0 for i = j in order to avoid self-reinforcementduring optimization (Zhou et al, 2004).When two sentences in S or T are not too short,or their content is not divergent in meaning, theirsemantic similarity can be estimated in terms ofcommon words.
Motivated by this, we define Wij(for i 6= j) based on the Gaussian kernel asWij = exp(?
12?2(1?
vTi vj?vi?
?vj?
)2)(9)625where ?
is the standard deviation parameter, viand vj are vectors of si and sj with each com-ponent corresponding to the tf-idf value of a par-ticular term in S (or T ), and ???
is the norm ofa vector.
The underlying assumption here is thatwords appearing frequently in a small number ofsentences but rarely in the others are more signifi-cant in measuring sentence affinity.Although semantic similarity estimation is astraightforward approach to deriving the two affin-ity matrices, other approaches are also feasible.
Analternative approach can be based on sentencelength under the assumption that two sentenceswith close lengths in one language tend to havetheir translations also with close lengths.2.6 DiscussionThe proposed semisupervised framework for non-monotonic alignment is in fact generalized be-yond, and can also be applied to, monotonic align-ment.
Towards this, we need to make use of sen-tence sequence information.
One way to do it isto incorporate sentence positions into Equation (1)by introducing a position constraint Qp(F ) to en-force that bilingual sentences in closer positionsshould have a higher chance to match one another.For example, the new constraint can be defined asQp(F ) =m?i=1n?j=1|pi ?
qj |F 2ij ,where pi and qj are the absolute (or relative) posi-tions of two bilingual sentences in their respectivesequences.
Another way follows the banding as-sumption that the actual couplings only appear ina narrow band along the main diagonal of relationmatrix.
Accordingly, all entries of F ?
outside thisband are set to zero before the alignment functionis applied.
Kay and Ro?scheisen (1993) illustratethat this can be done by modeling the maximumdeviation of true couplings from the diagonal asO(?n).3 Evaluation3.1 Data SetOur data set is acquired from the BilingualLaws Information System (BLIS),2 an electronicdatabase of Hong Kong legislation maintainedby the Department of Justice, HKSAR.
BLIS2http://www.legislation.gov.hkprovides Chinese-English bilingual texts of ordi-nances and subsidiary legislation in effect on or af-ter 30 June 1997.
It organizes the legal texts into ahierarchy of chapters, sections, subsections, para-graphs and subparagraphs, and displays the con-tent of a such hierarchical construct (usually a sec-tion) on a single web page.By web crawling, we have collected in total31,516 English and 31,405 Chinese web pages,forming a bilingual corpus of 31,401 bitexts afterfiltering out null pages.
A text contains several totwo hundred sentences.
Many bitexts exhibit par-tially non-monotonic order of sentences.
Amongthem, 175 bitexts are randomly selected for man-ual alignment.
Sentences are identified based onpunctuations.
OpenNLP Tokenizer3 is applied tosegment English sentences into tokens.
For Chi-nese, since there is no reliable segmenter for thisgenre of text, we have to treat each Chinese char-acter as a single token.
In addition, to calculate themonolingual sentence affinity, stemming of En-glish words is performed with the Porter Stemmer(Porter, 1980) after anchor string mining.The manual alignment of the evaluation data setis performed upon the initial alignment by Hu-nalign (Varga et al, 2005), an effective sentencealigner that uses both sentence length and a bilex-icon (if available).
For this work, Hunalign re-lies solely on sentence length.
Its output is thendouble-checked and corrected by two experts inbilingual studies, resulting in a data set of 17471-1 and 70 1-0 or 0-1 sentence pairs.The standard deviation ?
in (9) is an importantparameter for the Gaussian kernel that has to bedetermined empirically (Zhu et al, 2003; Zhou etal., 2004).
In addition, theQ function also involvesanother parameter ?
to adjust the weight of thebilingual constraint.
This work seeks an approachto deriving the optimal parameters without any ex-ternal training data beyond the initial alignment.
Athree-fold cross-validation is thus performed onthe initial 1-1 alignment and the parameters thatgive the best average performance are chosen.3.2 Monolingual ConsistencyTo demonstrate the validity of the monolingualconsistency, the semantic similarity defined byvTi vj?vi??vj?
is evaluated as follows.
500 pairs of En-glish sentences with the highest similarities are se-lected, excluding null pairings (1-0 or 0-1 type).3http://opennlp.apache.org/6260.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 10.60.650.70.750.80.850.90.951Similarity of English sentence pairSimilarityof Chinese sentencepairFigure 4: Demonstration of monolingual consis-tency.
The horizontal axis is the similarity of En-glish sentence pairs and the vertical is the similar-ity of the corresponding pairs in Chinese.Type Total initAlign NonmoAlignPred Corr Pred Corr1-0 70 662 66 70 501-1 1747 1451 1354 1747 1533Table 1: Performance of the initial alignment andour aligner, where the Pred and Corr columns arethe numbers of predicted and correct pairings.All of these high-affinity pairs have a similarityscore higher than 0.72.
A number of duplicatesentences (e.g., date) with exceptionally high sim-ilarity 1.0 are dropped.
Also, the similarity of thecorresponding translations of each selected pairis calculated.
These two sets of similarity scoresare then plotted in a scatter plot, as in Figure 4.If the monolingual consistency assumption holds,the plotted points would appear nearby the diag-onal.
Figure 4 confirms this, indicating that sen-tence pairs with high affinity in one language dohave their counterparts with similarly high affinityin the other language.3.3 Impact of Initial AlignmentThe 1-1 initial alignment plays the role of labeledinstances for the semisupervised learning.
It isof critical importance to the learning performance.As shown in Table 1, our alignment function pre-dicts 1451 1-1 pairings by virtue of anchor strings,among which 1354 pairings are correct, yieldinga relatively high precision in the non-monotoniccircumstance.
It also predicts null alignment formany sentences that contain no anchor.
This ex-plains why it outputs 662 1-0 pairings when there20 % 30 % 40 % 50 % 60 % 70 % 80 % 90 % 100%02004006008001000120014001600Percentage of initial 1?1 alignmentCorrectlydetected1?1pairingsNonmoAligninitAlignFigure 5: Performance of non-monotonic align-ment along the percentage of initial 1-1 alignment.are only 70 1-0 true ones.
Starting from this initialalignment, our aligner (let us call it NonmoAlign)discovers 179 more 1-1 pairings.A question here is concerned with how the scaleof initial alignment affects the final alignment.
Toexamine this, we randomly select 20%, 40%, 60%and 80% of the 1451 1-1 detected pairings as theinitial alignments for a series of experiments.
Therandom selection for each proportion is performedten times and their average alignment performanceis taken as the final result and plotted in Figure 5.An observation from this figure is that the alignerconsistently discovers significantly more 1-1 pair-ings on top of an initial 1-1 alignment, which hasto be accounted for by the monolingual consis-tency.
Another observation is that the alignmentperformance goes up along the increase of thepercentage of initial alignment while performancegain slows down gradually.
When the percentageis very low, the aligner still works quite effectively.3.4 Non-Monotonic AlignmentTo test our aligner with non-monotonic sequencesof sentences, we have them randomly scrambledin our experimental data.
This undoubtedly in-creases the difficulty of sentence alignment, espe-cially for the traditional approaches critically rely-ing on monotonicity.The baseline methods used for comparison areMoore?s aligner (Moore, 2002) and Hunalign(Varga et al, 2005).
Hunalign is configured withthe option [-realign], which triggers a three-stepprocedure: after an initial alignment, Hunalignheuristically enriches its dictionary using word co-occurrences in identified sentence pairs; then, itre-runs the alignment process using the updated627Type Moore Hunalign NonmoAlignP R F1 P R F1 P R F11-1 0.104 0.104 0.104 0.407 0.229 0.293 0.878 0.878 0.8781-0 0.288 0.243 0.264 0.033 0.671 0.062 0.714 0.714 0.714Micro 0.110 0.110 0.110 0.184 0.246 0.210 0.871 0.871 0.871Table 2: Performance comparison with the baseline methods.dictionary.
According to Varga et al(2005), thissetting gives a higher alignment quality than oth-erwise.
In addition, Hunalign can use an externalbilexicon.
For a fair comparison, the identified an-chor set is fed to Hunalign as a special bilexicon.The performance of alignment is measured by pre-cision (P), recall (R) and F-measure (F1).
Micro-averaged performance scores of precision, recalland F-measure are also computed to measure theoverall performance on 1-1 and 1-0 alignment.The final results are presented in Table 2, show-ing that both Moore?s aligner and Hunalign under-perform ours on non-monotonic alignment.
Theparticularly poor performance of Moore?s alignerhas to be accounted for by its requirement of morethan thousands of sentences in bitext input for re-liable estimation of its parameters.
Unfortunately,our available data has not reached that scale yet.3.5 Partially Non-Monotonic AlignmentFull non-monotonic bitexts are rare in practice.But partial non-monotonic ones are not.
Unliketraditional alignment approaches, ours does notfound its performance on the degree of monotonic-ity.
To test this, we construct five new versions ofthe data set for a series of experiments by ran-domly choosing and scrambling 0%, 10%, 20%,40%, 60% and 80% sentence parings.
In the-ory, partial non-monotonicity of various degreesshould have no impact on the performance of ouraligner.
It is thus not surprised that it achievesthe same result as reported in last subsection.NonmoAlign initialized with Hunalign (markedas NonmoAlign Hun) is also tested.
The experi-mental results are presented in Figure 6.
It showsthat both Moore?s aligner and Hunalign work rel-atively well on bitexts with a low degree of non-monotonicity, but their performance drops dra-matically when the non-monotonicity is increased.Despite the improvement at low non-monotonicityby seeding our aligner with Hunalign, its per-formance decreases likewise when the degree ofnon-monotonicity increases, due to the quality de-0 % 10% 20% 30% 40% 50% 60% 70% 80%0.20.30.40.50.60.70.80.911.1Non?monotonic ratioMicro?F1NonmoAlignHunalignMooreNonmoAlign_HunFigure 6: Performance of alignment approaches atdifferent degrees of non-monotonicity.crease of the initial alignment by Hunalign.3.6 Monotonic AlignmentThe proposed alignment approach is also expectedto work well on monotonic sentence alignment.An evaluation is conducted for this using a mono-tonic data set constructed from our data set bydiscarding all its 126 crossed pairings.
Of thetwo strategies discussed above, banding is usedto help our aligner incorporate the sequence in-formation.
The initial relation matrix is built withthe aid of a dictionary automatically derived byHunalign.
Entries of the matrix are derived byemploying a similar strategy as in Varga et al(2005).
The evaluation results are presented in Ta-ble 3, which shows that NonmoAlign still achievesvery competitive performance on monotonic sen-tence alignment.4 Related WorkThe research of sentence alignment originates inthe early 1990s.
Gale and Church (1991) andBrown (1991) report the early works using lengthstatistics of bilingual sentences.
The general ideais that the closer two sentences are in length, themore likely they are to align.
A notable differenceof their methods is that the former uses sentence628Type Moore Hunalign NonmoAlignP R F1 P R F1 P R F11-1 0.827 0.828 0.827 0.999 0.972 0.986 0.987 0.987 0.9871-0 0.359 0.329 0.343 0.330 0.457 0.383 0.729 0.729 0.729Micro 0.809 0.807 0.808 0.961 0.951 0.956 0.976 0.976 0.976Table 3: Performance of monotonic alignment in comparison with the baseline methods.length in number of characters while the latter innumber of tokens.
Both use dynamic program-ming to search for the best alignment.
As shown inChen (1993) and Wu (1994), however, sentence-length based methods suffer when the texts to bealigned contain small passages, or the languagesinvolved share few cognates.
The subsequent stageof sentence alignment research is accompanied bythe advent of a handful of well-designed alignmenttools.
Moore (2002) proposes a three-pass proce-dure to find final alignment.
Its bitext input is ini-tially aligned based on sentence length.
This stepgenerates a set of strictly-selected sentence pairsfor use to train an IBM translation model 1 (Brownet al, 1993).
Its final step realigns the bitext usingboth sentence length and the discovered word cor-respondences.
Hunalign (Varga et al, 2005), orig-inally proposed as an ingredient for building paral-lel corpora, has demonstrated an outstanding per-formance on sentence alignment.
Like many otheraligners, it employs a similar strategy of combin-ing sentence length and lexical data.
In the ab-sence of a lexicon, it first performs an initial align-ment wholly relying on sentence length and thenautomatically builds a lexicon based on this align-ment.
Using an available lexicon, it produces arough translation of the source text by convertingeach token to the one of its possible counterpartsthat has the highest frequency in the target corpus.Then, the relation matrix of a bitext is built of sim-ilarity scores for the rough translation and the ac-tual translation at sentence level.
The similarity oftwo sentences is calculated in terms of their com-mon pairs and length ratio.To deal with noisy input, Ma (2006) proposesa lexicon-based sentence aligner - Champollion.Its distinctive feature is that it assigns differentweights to words in terms of their tf-idf scores,assuming that words with low sentence frequen-cies in a text but high occurrences in some localsentences are more indicative of alignment.
Un-der this assumption, the similarity of any two sen-tences is calculated accordingly and then a dy-namic programming algorithm is applied to pro-duce final alignment.
Following this work, Li etal.
(2010) propose a revised version of Champol-lion, attempting to improve its speed without per-formance loss.
For this purpose, the input bitextsare first divided into smaller aligned fragments be-fore applying Champollion to derive finer-grainedsentence pairs.
In another related work by Deng etal.
(2007), a generative model is proposed, accom-panied by two specific alignment strategies, i.e.,dynamic programming and divisive clustering.
Al-though a non-monotonic search process that toler-ates two successive chunks in reverse order is in-volved, their work is essentially targeted at mono-tonic alignment.5 ConclusionIn this paper we have proposed and testeda semisupervised learning approach to non-monotonic sentence alignment by incorporatingboth monolingual and bilingual consistency.
Theutility of monolingual consistency in maintain-ing the consonance of high-affinity monolingualsentences with their translations has been demon-strated.
This work also exhibits that bilingual con-sistency of initial alignment of certain quality isuseful to boost alignment performance.
Our eval-uation using real-world data from a legislationcorpus shows that the proposed approach outper-forms the baseline methods significantly when thebitext input is composed of non-monotonic sen-tences.
Working on partially non-monotonic data,this approach also demonstrates a superior per-formance.
Although initially proposed for non-monotonic alignment, it works well on monotonicalignment by incorporating the constraint of sen-tence sequence.AcknowledgmentsThe research described in this paper was substan-tially supported by the Research Grants Council(RGC) of Hong Kong SAR, China, through theGRF grant 9041597 (CityU 144410).629ReferencesRicardo Baeza-Yates and Berthier Ribeiro-Neto.
2011.Modern Information Retrieval: The Conceptsand Technology Behind Search, 2nd ed., Harlow:Addison-Wesley.Jewel B. Barlow, Moghen M. Monahemi, and Dianne P.O?Leary.
1992.
Constrained matrix Sylvester equa-tions.
In SIAM Journal on Matrix Analysis and Ap-plications, 13(1):1-9.Peter F. Brown, Jennifer C. Lai, Robert L. Mercer.1991.
Aligning sentences in parallel corpora.
InProceedings of ACL?91, pages 169-176.Peter F. Brown, Vincent J. Della Pietra, Stephen A.Della Pietra and Robert L. Mercer.
1993.
The math-ematics of statistical machine translation: Parameterestimation.
Computational Linguistics, 19(2):263-311.Stanley F. Chen.
1993.
Aligning sentences in bilingualcorpora using lexical information.
In Proceedings ofACL?93, pages 9-16.Yonggang Deng, Shankar Kumar, and William Byrne.2007.
Segmentation and alignment of parallel textfor statistical machine translation.
Natural Lan-guage Engineering, 13(3): 235-260.William A. Gale, Kenneth Ward Church.
1991.
A Pro-gram for aligning sentences in bilingual corpora.
InProceedings of ACL?91, pages 177-184.Martin Kay and Martin Ro?scheisen.
1993.
Text-translation alignment.
Computational Linguistics,19(1):121-142.Chunyu Kit, Jonathan J. Webster, King Kui Sin, HaihuaPan, and Heng Li.
2004.
Clause alignment for bilin-gual HK legal texts: A lexical-based approach.
In-ternational Journal of Corpus Linguistics, 9(1):29-51.Chunyu Kit, Xiaoyue Liu, King Kui Sin, and JonathanJ.
Webster.
2005.
Harvesting the bitexts of the lawsof Hong Kong from the Web.
In The 5th Workshopon Asian Language Resources, pages 71-78.Judith L. Klavans and Evelyne Tzoukermann.
1990.The bicord system: Combining lexical informationfrom bilingual corpora and machine readable dictio-naries.
In Proceedings of COLING?90, pages 174-179.Philippe Langlais, Michel Simard, and Jean Ve?ronis.1998.
Methods and practical issues in evaluatingalignment techniques.
In Proceedings of COLING-ACL?98, pages 711-717.Zhanyi Liu, Haifeng Wang, Hua Wu, and Sheng Li.2010.
Improving statistical machine translation withmonolingual collocation.
In Proceedings of ACL2010, pages 825-833.Xiaoyi Ma.
2006.
Champollion: A robust parallel textsentence aligner.
In LREC 2006, pages 489-492.Peng Li, Maosong Sun, Ping Xue.
2010.
Fast-Champollion: a fast and robust sentence alignmentalgorithm.
In Proceedings of ACL 2010: Posters,pages 710-718.Robert C. Moore.
2002.
Fast and accurate sentencealignment of bilingual corpora.
In Proceedings ofAMTA 2002, page 135-144.Jian-Yun Nie, Michel Simard, Pierre Isabelle andRichard Durand.
1999.
Cross-language informationretrieval based on parallel texts and automatic min-ing of parallel texts from the Web.
In Proceedingsof SIGIR?99, pages 74-81.Martin F. Porter.
1980.
An algorithm for suffix strip-ping.
Program, 14(3): 130-137.Jinsong Su, Hua Wu, Haifeng Wang, Yidong Chen, Xi-aodong Shi, Huailin Dong, Qun Liu.
2012.
Transla-tion model adaptation for statistical machine trans-lation with monolingual topic information.
In Pro-ceedings of ACL 2012, Vol.
1, pages 459-468.Ben Taskar, Simon Lacoste-Julien and Dan Klein.2005.
A discriminative matching approach to wordalignment.
In Proceedings of HLT/EMNLP 2005,pages 73-80.Da?niel Varga, Pe?ter Hala?csy, Andra?s Kornai, ViktorNagy, La?szlo?
Ne?meth, Viktor Tro?n.
2005.
Parallelcorpora for medium density languages.
In Proceed-ings of RANLP 2005, pages 590-596.Dekai Wu.
1994.
Aligning a parallel English-Chinesecorpus statistically with lexical criteria.
In Proceed-ings of ACL?94, pages 80-87.Dekai Wu.
2010.
Alignment.
Handbook of NaturalLanguage Processing, 2nd ed., CRC Press.Dengyong Zhou, Olivier Bousquet, Thomas N. Lal, Ja-son Weston, Bernhard Schlkopf.
2004.
Learningwith local and global consistency.
Advances in Neu-ral Information Processing Systems, 16:321-328.Xiaojin Zhu, Zoubin Ghahramani and John Lafferty.2003.
Semi-supervised learning using Gaussianfields and harmonic functions.
In Proceedings ofICML 2003, pages 912-919.630
