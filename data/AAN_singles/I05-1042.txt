Empirical Study of Utilizing Morph-SyntacticInformation in SMTYoung-Sook Hwang, Taro Watanabe, and Yutaka SasakiATR SLT Research Labs, 2-2-2 Hikaridai Seika-cho,Soraku-gun Kyoto, 619-0288, Japan{youngsook.hwang, taro.watanabe, yutaka.sasaki}@atr.jpAbstract.
In this paper, we present an empirical study that utilizesmorph-syntactical information to improve translation quality.
With threekinds of language pairs matched according to morph-syntactical similar-ity or difference, we investigate the effects of various morpho-syntacticalinformation, such as base form, part-of-speech, and the relative positionalinformation of a word in a statistical machine translation framework.We learn not only translation models but also word-based/class-basedlanguage models by manipulating morphological and relative positionalinformation.
And we integrate the models into a log-linear model.
Ex-periments on multilingual translations showed that such morphologicalinformation as part-of-speech and base form are effective for improvingperformance in morphologically rich language pairs and that the relativepositional features in a word group are useful for reordering the localword orders.
Moreover, the use of a class-based n-gram language modelimproves performance by alleviating the data sparseness problem in aword-based language model.1 IntroductionFor decades, many research efforts have contributed to the advance of statisti-cal machine translation.
Such an approach to machine translation has provensuccessful in various comparative evaluations.
Recently, various works have im-proved the quality of statistical machine translation systems by using phrasetranslation [1,2,3,4] or using morpho-syntactic information [6,8].
But most sta-tistical machine translation systems still consider surface forms and rarely uselinguistic knowledge about the structure of the languages involved[8].
In thispaper, we address the question of the effectiveness of morpho-syntactic featuressuch as parts-of-speech, base forms, and relative positions in a chunk or an ag-glutinated word for improving the quality of statistical machine translations.Basically, we take a statistical machine translation model based on an IBMmodel that consists of a language model and a separate translation model [5]:eI1 = argmaxeI1Pr(fJ1 |eI1)Pr(eI1) (1)The translation model links the source language sentence to the target languagesentence.
The target language model describes the well-formedness of the targetlanguage sentence.R.
Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
474?485, 2005.c?
Springer-Verlag Berlin Heidelberg 2005Empirical Study of Utilizing Morph-Syntactic Information in SMT 475One of the main problems in statistical machine translation is to learn theless ambiguous correspondences between the words in the source and targetlanguages from the bilingual training data.
When translating one source lan-guage(which may be inflectional or non-inflectional) into the morphologicallyrich language such like Japanese or Korean, the bilingual training data can beexploited better by explicitly taking into account the interdependencies of re-lated inflected or agglutinated forms.
In this study, we represent a word withits morphological features in both sides of the source and the target languageto learn less ambiguous correspondences between the source and the target lan-guage words or phrases.
In addition, we utilize the relative positional informationof a word in its word group to consider the word order in an agglutinated wordor a chunk.Another problem is to produce a correct target sentence.
To produce morecorrect target sentence, we should consider the following problems: word re-ordering in a language pair with different word order, production of correctinflected and agglutinated words in an inflectional or agglutinative target lan-guage.
In this study, we tackle the problem with language models.
For learninglanguage model that can treat morphological and word-order problem, we rep-resent a word with its morphological and positional information.
However, aword-based language model with enriched word is likely to suffer from a severedata sparseness problem.
To alleviate the problem, we interpolate the word-basedlanguage model with a class-based n-gram model.In the next section, we briefly discuss related works.
Then, we describe themethod that utilizes morpho-syntactic information under consideration for im-proving the quality of translations.
Then we report the experimental results withsome analysis and conclude our study.2 Related WorkFew papers deal with the integration of linguistic information into the process ofstatistical machine translation.
[8] introduced hierarchical lexicon models includ-ing base-form and POS information for translation from German into English.Irrelevant information contained in the German entries for the generation of theEnglish translation were omitted.
They trained the lexicon model using maxi-mum entropy.
[6] enriched English with knowledge to help select the correct full-form from morphologically richer languages such as Spanish and Catalan.
In otherwords, they introduced a splicing operation that merged the pronouns/modalsand verbs for treating differences in verbal expressions.
To treat the unknown en-tries in the lexicon resulting from the splicing operation, they trained the lexiconmodel using maximum entropy and used linguistic knowledge just in the sourcelanguage part and not in the target language.
They don?t use any linguistic knowl-edge in the target language and use full-form words during training.In addition, [6] and [8] proposed re-ordering operations to make similar wordorders in the source and target language sentences.
In other words, for the in-terrogative phrases with different word order from the declarative sentences,476 Y.-S. Hwang, T. Watanabe, and Y. Sasakithey introduced techniques of question inversion and removed unnecessary aux-iliary verbs.
But, such inversion techniques require additional preprocessing withheuristics.Unlike them, we investigate methods for utilizing linguistic knowledge inboth of the source and the target language at the morpheme level.
To generatea correct full-form word in a target language, we consider not only both thesurface and base form of a morpheme but also the relative positional informa-tion in a full-form word.
We strongly utilize the combined features in languagemodeling.
By training alignments and language models with morphological andpositional features at the morpheme-level, the severe data sparseness problemcan be alleviated with the combined linguistic features.
And the correspondenceambiguities between the source and target words can be decreased.3 Utilization of Morpho-Syntactic Information in SMTGenerally, the probabilistic lexicon resulting from training a translation modelcontains all word forms occurring in the training corpus as separate entries,not taking into account whether they are inflected forms.
A language modelis also composed of the words in the training corpus.
However, the use of afull-form word itself may cause severe data sparseness problem, especially rel-evant for more inflectional/agglutinative languages like Japanese and Korean.One alternative is to utilize the results of morphological analysis such as baseform, part-of-speech and other information at the morpheme level.
We addressthe usefulness of morphological information to improve the quality of statisticalmachine translation.3.1 Available Morpho-Syntactic InformationA prerequisite for methods that improve the quality of statistical machine trans-lation is the availability of various kinds of morphological and syntactic infor-mation.
In this section, we examine the morpho-syntactic information availablefrom the morphological analyzers of Korean, Japanese, English and Chinese anddescribe a method of utilizing the information.Japanese and Korean are highly inflectional and agglutinative languages, andin English inflection has only a marginal role; whereas Chinese usually is regardedas an isolating language since it has almost no inflectional morphology.
As thesyntactic role of each word within Japanese and Korean sentences are oftenmarked, word order in a sentence plays a relatively small role in characterizingthe syntactic function of each word than in English or Chinese sentences.
Thus,Korean and Japanese sentences have a relatively free word order; whereas wordswithin Chinese and English sentences adhere to a rigid order.
The treatmentof inflection, and not word order, plays the most important role in processingJapanese and Korean, while word order has a central role in Chinese and English.Figure 1 shows some examples of morphological information by Chinese,Japanese, English and Korean morphological analyzers and Figure 2 the corre-spondences among the words.
Note that Korean and Japanese are very similar:Empirical Study of Utilizing Morph-Syntactic Information in SMT 477Fig.
1.
Examples of linguistic information from Chinese, Japanese, English, and Koreanmorphological analyzersFig.
2.
Correspondences among the words in parallel sentenceshighly inflected and agglutinated.
One difference in Korean from Japanese isthat a Korean sentence consists of spacing units, eojeols,1 while there are nospace in a Japanese sentence.
Especially, a spacing unit(i.e., eojeol) in Koreanoften becomes a base phrase that contains such syntactic information as subject,object, and the mood/tense of a verb in a given sentence.
The treatment of sucha Korean spacing unit may contribute to the improvement of translation qualitybecause a morpheme can be represented with its relative positional informationwithin an eojeol.
The relative positional information is obtained by calculatingthe distance between the beginning syllable of a given eojeol and the beginningof each morpheme within the eojeol.
The relative positional information is rep-resented with indexes of the beginning and the ending syllables (See Figure 1).3.2 Word RepresentationA word(i.e.
morpheme) is represented by the combination of the information pro-vided by a morphological analyzer including the surface form, base form, part-of-speech or other information such as relative position within an eojeol.
The word1 An eojeol is composed of no less than one morpheme by agglutination principle.478 Y.-S. Hwang, T. Watanabe, and Y. SasakiTable 1.
Word Representation According to Morpho-Syntactic Characteristics (S: sur-face form, B:base form, P:part-of-speech, L:RelativePosition)Chinese English Japanese KoreanMorph-Syntactic no inflection Inflectional Inflectional, InflectionalCharacteristics Agglutinative AgglutinativeSpacing Unit(Word-Order) Rigid Rigid Partial Free Partial FreeWord Representation S?P S?B?P S?B?P S?B?P?LS?B, S?P S?B, S?P S?B?P, S?B?L, S?P?LS?B, S?P, S?Lenriched by the combination of morph-syntactic information must alway includethe surface form of a given word for the direct generation of target sentencewithout any post-processing.
Other different morphological information is com-bined according to representation models such as surface plus base form (SB),surface plus part-of-speech (SP), surface plus relative position (SL), and so on.Table 1 shows the word representation of each language with every possiblemorphological information.
Yet, we are not limited to only this word represen-tation, but we have many possibilities of word representation by removing somemorphological information or inserting additional morpho-syntactic informationas mentioned previously.
In order to develop the best translation systems, weselect the best word representation models of the source and the target languagethrough empirical experiments.The inherent in the original word forms is augmented by a morphologicalanalyzer.
Of course, this results in an enlarged vocabulary while it may provideuseful disambiguation clues.
However, since we regard a morpheme as a wordin a corpus(henceforth, we call a morpheme a word), the enlarged vocabularydoes not make more severe data sparseness problem than using the inflected oragglutinated word.
By taking the approch of morpheme-level alignment, we mayobtain more accurate correspondences among words as illustrated in Figure 2.Moreover, by learning the language model with rich morph-syntactic informa-tion, we can generate more syntactically fluent and correct sentence.3.3 Log-Linear Model for Statistical Machine TranslationIn order to improve translation quality, we evaluate the translation candidatesby using the relevant features in a log-linear model framework[11].
The log-linearmodel used in our statistical translation process, Pr(eI1|fJ1 ), is:Pr(eI1|f I1 ) =exp(?m ?mhm(eI1, fJ1 , aJ1 ))?e?I1 ,fI1 ,aI1exp(?m ?mhm(e?I1 , fJ1 , aJ1 ))(2)where hm(eI1, fJ1 , aJ1 ) is the logarithm value of the m-th feature; ?m is the weightof the m-th feature.
Integrating different features in the equation results in dif-ferent models.Empirical Study of Utilizing Morph-Syntactic Information in SMT 479The statistical machine translation process in IBM models is as follows; agiven source string fJ1 = f1 ?
?
?
fJ is to be translated into eI1 = e1 ?
?
?
eI .
Accord-ing to the Bayes?
decision rule, we choose the optimal translation for given stringfJ1 that maximizes the product of target language model Pr(eI1) and translationmodel Pr(fJ1 |eI1)eI1 = argmaxeI1Pr(fJ1 |eI1)Pr(eI1) (3)In IBM model 4, translation model P (fJ1 |eI1) is further decomposed into foursubmodels:?
Lexicon Model, t(f |e): probability of word f in the source language beingtranslated into word e in the target language.?
Fertility model, n(?|e): probability of target language word e generating ?words.?
Distortion model d: probability of distortion, which is decomposed into thedistortion probabilities of head words and non-head words.?
NULL translation model p1: a fixed probability of inserting a NULL wordafter determining each target word.In addition to the five features (Pr(eI1), t(f |e), n(?|e), d, p1) from IBM model4, we incorporate the following features into the log-linear translation model:?
Class-based n-gram model Pr(eI1) =?i Pr(ei|ci)Pr(ci|ci?11 ): Grouping ofwords into C classes is done according to the statistical similarity of theirsurroundings.
Target word ei is mapped into its class, ci, which is one of Cclasses[13].?
Length model Pr(l|eI1, fJi ): l is the length (number of words) of a translatedtarget sentence.?
Example matching score: The translated target sentence is matched withphrase translation examples.
A score is derived based on the number ofmatches [10].
To extract phrase translation examples, we compute the inter-section of word alignment of both directions and derive the union.
Then wegrab the phrase translation pairs that contain at least one intersected wordalignment and some unioned word alignments[1].Under the framework of log-linear models, we investigate the effects of morpho-syntactic information with word representation.
The overall training and testingprocess with morphological and positional information is depicted in Figure 3.
Inthe training step, we train the word- and class-based language models with var-ious word representation methods[12].
Also, we make word alignments throughthe learning of IBM models by using GIZA++ toolkit[3]: we learn the translationmodel toward IBM model 4, initiating translation iterations from IBM model1 with intermediate HMM model iterations.
Then, we extract example phrasesand translation model features from the alignment results.Then in the test step, we perform morphological anlysis of a given sentence forword representation corresponding to training corpus representation.
We decodethe best translation of a given test sentence by generating word graphs andsearching for the best hypothesis in a log-linear model[7].480 Y.-S. Hwang, T. Watanabe, and Y. SasakiFig.
3.
Overview of training and test of statistical machine translation system withlinguistic information4 Experiments4.1 Experimental EnvironmentsThe corpus for the experiment was extracted from the Basic Travel ExpressionCorpus (BTEC), a collection of conversational travel phrases for Chinese, En-glish, Japanese and Korean[15].
The entire corpus was split into three parts:152,169 sentences in parallel for training, 10,150 sentences for testing and theremaining 10,148 sentences for parameter tuning, such as termination criteriafor training iteration and parameter tuning for decoders.
For the reconstructionof each corpus with morphological information, we used in-house morphologicalTable 2.
Statistics of Basic Travel Expression CorpusChinese English Japanese Korean# of sentences 167,163# of words(morph) 1,006,838 1,128,151 1,226,774 1,313,407Vocabulary size(S) 17,472 11,737 19,485 17,600Vocabulary size(B) 17,472 9172 15,939 15,410Vocabulary size(SB) 17,472 13,385 20,197 18,259Vocabulary size(SP) 18,505 13,467 20,118 20,249Vocabulary size(SBP(L)) 18,505 14,408 20,444 20,369(26,668)# of singletons(S) 7,137 4,046 8,107 7,045# of singletons(B) 7,137 3,025 6,497 6,303# of singletons(SB) 7,137 4,802 9,453 7,262# of singletons(SP) 7,601 4,693 8,343 7,921# of singletons(SBP(L)) 7,601 5,140 8,525 7,983(11,319)Empirical Study of Utilizing Morph-Syntactic Information in SMT 481Table 3.
Perplexities of tri-gram language model trained on the training corporawith S, SB, SP SBP, SBL, and SBPL morpho-syntactic representation: word-based3-gram/class-based 5-gramS SB SP SBP SBL SBPLChinese 31.57/24.09 N/S 35.83/26.28 N/A N/A N/AEnglish 22.35/18.82 22.19/18.54 22.24/18.12 22.08/18.03 N/A N/AJapanese 17.89/ 13.44 17.92/13.29 17.82/13.13 17.83/13.06 N/A N/AKorean 15.54/12.42 15.41/12.09 16.04/11.89 16.03/11.88 16.48/12.24 17.13/11.99analyzers for four languages: Chinese morphological analyzer with 31 parts-of-speech tags, English morphological analyzer with 34 tags, Japanese morphologi-cal analyzer with 34 tags, and Korean morphological analyzer with 49 tags.
Theaccuracies of Chinese, English, Japanese and Korean morphological analyzers in-cluding segmentation and POS tagging are 95.82% , 99.25%, 98.95%, and 98.5%respectively.
Table 2 summarizes the morph-syntactic statistics of the Chinese,English, Japanese, and Korean.For the four languages, word-based and class-based n-gram language modelswere trained on the training set by using SRILM toolkit[12].
The perplexity ofeach language model is shown in Table 3.For the four languages, we chose three kinds of language pairs according tothe linguistic characteristics of morphology and word order, Chinese-Korean,Japanese-Korean, and English-Korean.
42 translation models based on wordrepresentation methods(S, SB, SP, SBP, SBL, SPL,SBPL) were trained by usingGIZA++[3].4.2 EvaluationTranslation evaluations were carried out on 510 sentences selected randomlyfrom the test set.
The metrics for the evaluations are as follows:mWER(multi-reference Word Error Rate), which is based on the minimumedit distance between the target sentence and the sentences in the referenceset [9].BLEU, which is the ratio of the n-gram for the translation results found in thereference translations with a penalty for too short sentences [14].NIST which is a weighted n-gram precision in combination with a penalty fortoo short sentences.For this evaluation, we made 16 multiple references available.
We computed allof the above criteria with respect to these multiple references.Table 4, 5 and 6 show the evaluation results on three kinds of language pairs.The effects of morpho-syntactic information and class-based n-gram languagemodels on multi-lingual machine translation are shown: The combined morpho-logical information was useful for improving the translation quality in the NIST,BLEU and mWER evaluations.
Moreover, the class-based n-gram language mod-els were effective in the BLEU and the mWER scores.482 Y.-S. Hwang, T. Watanabe, and Y. SasakiTable 4.
Evaluation results of Japanese to Korean and Korean to Japaneses transla-tions(with class-based n-gram/word-based n-gram language model)J to K K to JNIST BLEU WER NIST BLEU WERS 8.46/8.64 0.694/0.682 26.33/26.73 8.21/8.39 0.666/0.649 25.00/25.81SB 8.05/8.32 0.705/0.695 26.82/26.97 7.67/8.17 0.690/0.672 23.77/24.68SP 9.15/9.25 0.755/0.747 21.71/22.22 9.02/9.13 0.720/0.703 21.94/23.50SL 8.37/8.47 0.699/0.667 25.49/27.76 8.48/8.74 0.671/0.629 25.14/27.88SBL 8.92/9.12 0.748/0.730 22.66/23.36 8.85/8.92 0.712/0.691 21.88/23.37SBP 8.19/8.57 0.713/0.696 26.17/27.09 8.21/8.39 0.698/0.669 22.94/24.88SBPL 8.41/8.85 0.772/0.757 22.30/21.74 7.77/7.83 0.626/0.619 25.19/25.57Table 5.
Evaluation results of English to Korean and Korean to English transla-tions(with class-based n-gram/word-based n-gram language model)E to K K to ENIST BLEU WER NIST BLEU WERS 5.12/5.79 0.353/0.301 51.12/58.52 5.76/6.05 0.300/0.255 52.54/61.23SB 6.71/6.87 0.533/0.474 39.10/47.18 7.72/8.15 0.482/0.446 37.86/42.71SP 6.88/7.19 0.552/0.502 37.63/42.34 8.01/8.46 0.512/0.460 35.13/40.91SL 6.66/6.96 0.546/0.516 38.20/40.67 7.71/8.02 0.484/0.436 36.79/42.88SPL 6.16/7.01 0.542/0.519 38.21/39.85 7.83/8.22 0.482/0.443 37.52/41.63SBL 6.52/6.93 0.547/0.504 37.76/42.23 7.64/8.08 0.479/0.439 37.10/42.30SBP 7.42/7.60 0.612/0.573 32.17/35.96 8.86/9.05 0.551/0.523 33.13/37.07SBPL 6.29/6.59 0.580/0.561 36.73/38.36 8.08/8.36 0.528/0.515 36.46/38.21Table 6.
Evaluation results of Chinese to Korean and Korean to Chinese transla-tions(with class-based n-gram/word-based n-gram language model)C to K K to CNIST BLEU WER NIST BLEU WERS 7.62/7.82 0.640/0.606 30.01/32.79 7.85/7.69 0.380/0.365 53.65/58.46SB 7.73/7.98 0.643/0.632 29.26/30.08 7.68/7.50 0.366/0.349 54.48/60.49SP 7.71/7.98 0.651/0.643 28.26/28.60 8.00/7.77 0.383/0.362 54.15/58.30SL 7.64/7.97 0.656/0.635 28.94/30.33 7.84/7.65 0.373/0.350 54.53/58.38SPL 7.69/7.93 0.665/0.659 28.43/28.88 7.78/7.62 0.373/0.351 56.14/59.54SBL 7.65/7.94 0.659/0.635 28.76/30.87 7.85/7.64 0.377/0.354 55.01/58.39SBP 7.81/7.98 0.660/0.643 28.85/29.61 7.94/7.68 0.386/0.360 53.99/58.94SBPL 7.64/7.90 0.652/0.634 29.54/30.46 7.82/7.66 0.376/0.358 55.64/58.79In detail, Table 4 shows the effects of the morphological and relative posi-tional information on Japanese-to-Korean and Korean-to-Japanese translation.In almost of the evaluation metrics, the SP model in which a word is repre-sented by a combination of its surface form and part-of-speech showed the bestperformance.
The SBL model utilizing the base form and relative positionalinformation only in Korean showed the second best performance.
In Korean-Empirical Study of Utilizing Morph-Syntactic Information in SMT 483to-Japanese translation, the SBPL model showed the best score in BLEU andmWER.
In this language pair of highly inflectional and agglutinative languages,the part-of-speech information combined with surface form was the most ef-fective in improving the performance.
The base form and relative positionalinformation were less effective than part-of-speech.
It could be explained in sev-eral points: Japanese and Korean are very similar languages in the word orderof SOVs and the ambiguities of translation correspondences in both directionswere converged into 1.0 by combining the distinctive morphological informationwith the surface form.
When refering to the vocabulay size of SP model in Table2, it makes it more clear.
The Japanese-to-Korean translation outperforms theKorean-to-Japanese.
It might be closely related to the language model: the per-plexity of the Korean language model is lower than Japanese according to ourcorpus statistics.Table 5 shows the performance of the English-to-Korean and Korean-to-English translation: a pair of highly inflectional and agglutinative language withpartially free word-order and an inflectional language with rigid word order.
Inthis language pair, the combined word representation models improved the trans-lation performance into significantly higher BLEU and mWER scores in bothdirections.
The part-of-speech and the base form information were distinctivefeatures.
When comparing the performance of SP, SB and SL models, part-of-speech might be more effective than base form or relative positional information,and the relative positional information in Korean might play a role not only incontrolling word order in the language models but also in discriminating wordcorrespondences during alignment.When the target language was Korean, we had higher BLEU scores in all themorpho-syntactic models but lower NIST scores.
In other words, we took advan-tage of generating more accurate full-form eojeol with positional information,i.e.
local word ordering.Table 6 shows the performance of the Chinese-to-Korean and Korean-to-Chinese translation: a pair of a highly inflectional and agglutinative languagewith partially free word order and a non-inflectional language with rigid wordorder.
This language pair is a quite morpho-syntactically different.
When a non-inflectional language is a target language(i.e.
Korean-to-Chinese translation), theperformance was the worst compared with other language pairs and directions inBLEU and mWER.
On the other hand, the performance of Chinese-to-Koreanwas much better than Korean-to-Chinese, meaning that it is easier to generateKorean sentence from Chinese the same as in Japanese-to-Korean and English-to-Korean.
In this language pair, we had gradual improvements according tothe use of combined morpho-syntactic information, but there was no significantdifference from the use of only the surface form.
There was scant contribution ofChinese morphological information such as part-of-speech.
On the other hand,we could get some advantageous Korean morpho-syntactic information in theChinese-to-Korean translation, i.e., the advantage of language and translationmodels using morpho-syntactic information.484 Y.-S. Hwang, T. Watanabe, and Y. Sasaki5 Conclusion and Future WorksIn this paper, we described an empirical study of utilizing morpho-syntacticinformation in a statistical machine translation framework.
We empirically in-vestigated the effects of morphological information with several language pairs:Japanese and Korean with the same word order and high inflection/agglutination, English and Korean, a pair of a highly inflecting and agglutinat-ing language with partial free word order and an inflecting language with rigidword order, and Chinese-Korean, a pair of a highly inflecting and agglutinatinglanguage with partially free word order and a non-inflectional language withrigid word order.
As the results of experiments, we found that combined mor-phological information is useful for improving the translation quality in BLEUand mWER evaluations.
According to the language pair and the direction, wehad different combinations of morpho-syntactic information that are the bestfor improving the translation quality: SP(surface form and part-of-speech) fortranslating J-to-K or K-to-J, SBP(surface form, base form and part-of-speech)for E-to-K or K-to-E, SPL(surface form, part-of-speech and relative position) forC-to-K.
The utilization of morpho-syntactic information in the target languagewas the most effective.
Language models based on morpho-syntactic informa-tion were very effective for performance improvement.
The class-based n-grammodels improved the performance with smoothing effects in the statistical lan-guage model.
However, when translating an inflectional language, Korean intoa non-inflectional language, Chinese with quite different word order, we foundvery few advantages using morphological information.
One of the main reasonsmight be the relatively low performance of the Chinese morphological analyzer.The other might come from the linguistic difference.
For the latter case, we needto adopt approaches to reflect the structural characteristics such like using achunker/parser, context-dependent translation modeling.AcknowledgmentsThe research reported here was supported in part by a contract with the NationalInstitute of Information and Communications Technology entitled ?A study ofspeech dialogue translation technology based on a large corpus?.References1.
Koehn P., Och F.J., and Marcu D.: Statistical Phrase-Based Translation, Proc.
ofthe Human Language Technology Conference(HLT/NAACL) (2003)2.
Och F. J., Tillmann C., Ney H.: Improved alignment models for statistical machinetranslation, Proc.
of EMNLP/WVLC (1999).3.
Och F.J. and Ney H. Improved Statistical Alignment Models, Proc.
of the 38thAnnual Meeting of the Association for Computational Linguistics (2000) pp.
440-447.4.
Zens R. and Ney H.: Improvements in Phrase-Based Statistical Machine Transla-tion, Proc.
of the Human Language Technology Conference (HLT-NAACL) (2004)pp.
257-264Empirical Study of Utilizing Morph-Syntactic Information in SMT 4855.
Brown P. F., Della Pietra S. A., Della Pietra V. J., and Mercer R. L.: The math-ematics of statistical machine translation: Parameter estimation, ComputationalLinguistics, (1993) 19(2):263-3116.
Ueffing N., Ney H.: Using POS Information for Statistical Machine Translationinto Morphologically Rich Languages, In Proc.
10th Conference of the EuropeanChapter of the Association for Computational Linguistics (EACL), (2003) pp.
347-3547.
Ueffing N., Och F.J., Ney H.: Generation of Word Graphs in Statistical MachineTranslation In Proc.
Conference on Empirical Methods for Natural Language Pro-cessing, (2002) pp.
156-1638.
Niesen S., Ney H.: Statistical Machine Translation with Scarce Resources usingMorpho-syntactic Information, Computational Linguistics, (2004) 30(2):181-2049.
Niesen S., Och F.J., Leusch G., Ney H: An Evaluation Tool for Machine Transla-tion: Fast Evaluation for MT Research, Proc.
of the 2nd International Conferenceon Language Resources and Evaluation, (2000) pp.
39-4510.
Watanabe T. and Sumita E.: Example-based Decoding for Statistical MachineTranslation, Proc.
of MT Summit IX (2003) pp.
410?41711.
Och F. J. Och and Ney H.: Discriminative Training and Maximum Entropy Modelsfor Statistical Machine Translation, Proc.
of ACL (2002)12.
Stolcke, A.: SRILM - an extensible language modeling toolkit.
In Proc.
Intl.
Conf.Spoken Language Processing, (2002) Denver.13.
Brown P. F., Della Pietra V. J. and deSouza P. V. and Lai J. C. and MercerR.L.
: Class-Based n-gram Models of Natural Language, Computational Linguistics(1992) 18(4) pp.
467-47914.
Papineni K., Roukos S., Ward T., and Zhu W.-J.
: Bleu: a method for automaticevaluation of machine translation, IBM Research Report,(2001) RC22176.15.
Takezawa T., Sumita E., Sugaya F., Yamamoto H., and Yamamoto S.: Toward abroad-coverage bilingual corpus for speech translation of travel conversations inthe real world, Proc.
of LREC (2002), pp.
147-152.
