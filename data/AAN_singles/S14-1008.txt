Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 63?68,Dublin, Ireland, August 23-24 2014.Learning the Peculiar Value of ActionsDaniel DahlmeierResearch & Innovation, SAP Asia, Singapored.dahlmeier@sap.comAbstractWe consider the task of automatically es-timating the value of human actions.
Wecast the problem as a supervised learning-to-rank problem between pairs of actiondescriptions.
We present a large, noveldata set for this task which consists ofchallenges from the I Will If You WillEarth Hour challenge.
We show that anSVM ranking model with simple linguisticfeatures can accurately predict the relativevalue of actions.1 IntroductionThe question on how humans conceptualize valueis of great interest to researchers in various fields,including linguistics (Jackendoff, 2006).
The linkbetween value and language arises from the factthat we cannot directly observe value due to its ab-stract nature and instead often study language ex-pressions that describe actions which have somevalue attached to them.
This creates an interestinglink between the semantics of the words that de-scribe the actions and the underlying moral valueof the actions.Jackendoff (2006) describes value as an ?inter-nal accounting system?
for ethical decision pro-cesses that exhibits both valence (good or bad)and magnitude (better or worse).
Most interest-ingly, value is governed by a ?peculiar logic?
thatprovides constraints on which actions are deemedmorally acceptable and which are not.
In par-ticular, the principal of reciprocity states that thevalence and magnitude of reciprocal actions (ac-tions that are done ?in return?
for something else)should match, i.e., positive valued actions shouldThis work is licenced under a Creative Commons Attribution4.0 International License.
Page numbers and proceedingsfooter are added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/match with positive valued reciprocal actions (re-actions) of similar magnitude, and conversely neg-atively valued actions should match with nega-tive valued reciprocal actions (reactions) of similarmagnitude.In this paper, we consider the task of automati-cally estimating the value of actions.
We present asimple and effective method for learning the valueof actions from ranked pairs of textual action de-scriptions based on a statistical learning-to-rankapproach.
Our experiments are based on a noveldata set that we create from challenges submit-ted to the I Will if You Will Earth Hour challengewhere participants pledge to do something daringor challenging if other people commit to sustain-able actions for the planet.
Our method achievesa surprisingly high accuracy of up to 94.72% in a10-fold cross-validation experiment.
The resultsshow that the value of actions can accurately beestimated by machine learning methods based onlexical descriptions of the actions.The main contribution of this paper is that weshow how the semantics of value in language canaccurately be learned from empirical data using alearning-to-rank approach.
Our work shows an in-teresting link between empirical research on se-mantics in natural language processing and theconcept of value.2 The Logic of ValueOur approach is based on the concept of valueas presented by Jackendoff (2006) who describesvalue as an abstract property that is attributed toobjects, persons, and actions.
He further describeslogical inference rules that humans use to deter-mine which actions are deemed morally accept-able and which are not.
The most important in-ference rule for our work is the principal of recip-rocation, things that are done ?in return?
for someother action (Fiengo and Lasnik, 1973).
In En-glish, this relation is often expressed by the prepo-63sition for, as shown by the following example sen-tences (Jackendoff, 2006).1.
Susan praised Sam for behaving nicely.2.
Fred cooked Lois dinner for fixing his com-puter.3.
Susan insulted Sam for behaving badly.4.
Lois slashed Fred?s tires for insulting her.The first two examples describe actions with pos-itive value, while the last two examples describeactions with negative value.
We expect that thevalence values of reciprocal actions match: posi-tively valued actions demand a positively valuedaction in return, while negatively valued actionstrigger negatively valued responses.
If we switchthe example sentences and match positive actionswith negative actions, we get sentences that soundcounter-intuitive or perhaps comical (we prefixcounter-intuitive sentences with a hash character?#?).1.
#Susan insulted Sam for behaving nicely.2.
#Lois slashed Fred?s tires for fixing her com-puter.Similarly, we expect that the magnitudes of valuebetween reciprocal actions match.
Sentenceswhere the magnitude of the value of the responseaction does not match the magnitude of the initialaction seem odd or socially inappropriate (over-acting/underacting).1.
#Fred cooked Lois dinner for saying hello tohim.2.
#Fred cooked Lois dinner for rescuing all hisrelatives from certain death.3.
#Fred slashed Lois?s tires for eating too littleat dinner.4.
#Fred slashed Lois?s tires for murdering hisentire family.We observe that reciprocal actions typically matcheach other in valence and magnitude.
Comingback to our initial goal of learning the value ofactions, this gives us a method for comparing thevalue of actions that were done in return to thesame initial action.3 I Will If You Will challengeThe I Will If You Will (IWIYW) challenge1is partof the World Wildlife Fund?s Earth Hour campaign1www.earthhour.org/i-will-if-you-willI will quit smoking if you will start recycling.
(500 people)I will adopt a panda if you will start recycling.
(1000 people)I will dance gangnam style if you will planta tree.
(100 people)I will dye my hair red if you will upload anIWIYW challenge.
(500 people)I will learn Java if you will upload an IWIYWchallenge.
(10,000 people)Table 1: Examples of I Will If You Will chal-lenges.which has the goal to increase awareness of sus-tainability issues.
In this challenge, participantsmake a pledge to do something daring or challeng-ing if a certain number of people commit to sus-tainable actions for the planet.
The challenges arecreated by ordinary people on the Earth Hour cam-paign website.
Each challenge takes the form of asimple school yard dare: I will do X, if you will doY, where X is typically some daring or challengingtask that the challenge creator commits to do if asufficient number of people commit to do actionY which is some sustainable action for the planet.Together with the textual description, each chal-lenge includes the number of people that need tocommit to doing Y in order for the challenge cre-ator to perform X.
Examples of the challenges areshown in Table 1.It is important to note that during the challengecreation on the IWIYW website, the X challengeis a free text input field that allows the author tocome up with creative and interesting challenges.The sustainable actions Y and the number of peo-ple that need to commit to it are usually chosenfrom a fixed list of choices.
As a result, there isa large number of different X actions and a com-parably smaller number of Y actions.
The col-lected challenges provide a unique data set that al-lows us to quantitatively measure the value of eachpromised task by the number of people that needto fulfill the sustainable action.4 MethodIn this section, we present our approach for esti-mating the value of actions.
Our approach caststhe problem as a supervised learning-to-rank prob-lem between pairs of actions.
Given, a textual de-scription of an action a, we want to estimate its64value magnitude v. We represent the action a via aset of features that are extracted from the descrip-tion of the action.
We use a linear model that com-bines the features into a single scalar value for thevalue vv = wTxa, (1)where xais the feature vector for action descrip-tion a and w is a learned weight vector.
The goalis to learn a suitable weight vector w that approxi-mates the true relationship between textual expres-sions of actions and their magnitude of value.Instead of estimating the value directly, we takean alternative approach and consider the task oflearning the relative ranking of pairs of actions.We follow the pairwise approach to ranking (Her-brich et al., 1999; Cao et al., 2007) that reducesranking to a binary classification problem.
Rank-ing the values v1and v2of two actions a1and a2isequivalent to determining the sign of the dot prod-uct between the weight vector w and the differencebetween the feature vectors xa1and xa2.v1> v2?
wTxa1> wTxa2?
wTxa1?
wTxa2> 0?
wT(xa1?
xa2) > 0 (2)For each ranking pair of actions, we create twocomplimentary classification instances: (xa1?xa2, l1) and (xa2?
xa1, l2), where the labels arel1= +1, l2= ?1 if the first challenge has highervalue than the second challenge and l1= ?1, l2=+1 otherwise.
We can train a standard linear clas-sifier on the generated training instances to learnthe weight vector w.In the case of the IWIYW data, there is no ex-plicit ranking between actions.
However, we areable to create ranking pairs for the IWIYW datain the following way.
As we have seen, there isonly a small set of different You Will challengesthat are reciprocal actions for a diverse set of IWill challenges.
Thus, many I Will challenges willend up having the same You Will challenge.
Wecan use the You Will challenges as a pivot to ef-fectively ?join?
the I Will challenges.
The numberof required people to perform Y induces a natu-ral ordering between the values of the I Will ac-tions where a higher number of required partici-pants means that the I Will task has higher value.For example, for the challenges displayed in Ta-ble 1, we can use the common You Will challengesto create the following ranked challenge pairs.I will quit smoking < I will adopt a pandaI will dye my hair red < I will learn Java (3)According to the examples, adopting a panda hashigher value than quitting smoking and learningJava has higher value than dying ones hair red.The third challenge does not share a common YouWill challenge with any other challenge and there-fore no ranking pairs can be formed with it.As the IWIYW challenges are created online ina non-controlled environment, we have to expectthat there is some noise in the automatically cre-ated ranked challenges.
However, a robust learn-ing algorithm has to be able to handle a certainamount of noise.
We note that our method is notlimited to the IWIYW data set but can be appliedto any data set of actions where relative rankingsare provided or can be induced.4.1 FeaturesThe choice of appropriate feature representationsis crucial to the success of any machine learningmethod.
We start by parsing each I Will If YouWill challenge with a constituency parser.
Be-cause each challenge has the same I Will If YouWill structure, it is easy to identify the subtrees thatcorrespond to the I Will and You Will parts of thechallenge.
An example parse tree of a challengeis shown in Figure 1.
The yield of the You Willsubtree serves as a pivot to join different I Willchallenges.
To represent the I Will action a as afeature vector xa, we extract the following lexicaland syntax features from the I Will subtree of thesentence.?
Verb: We extract the verb of the I Will clauseas a feature.
To identify the verb, we pickthe left-most verb of the I Will subtree basedon its part-of-speech (POS) tag.
We extractthe lowercased word token as a feature.
Forexample, for the sentence in Figure 1, theverb feature is verb=quit.
If the verb isnegated (the left sibling of the I Will sub-tree spans exactly the word not), we add thepostfix NOT to the verb feature, for exampleverb=quit NOT.?
Object: We take the right sibling of the Iwill verb as the object of the action.
If theright sibling is a particle with constituent la-bel PRT, e.g., travel around the UK on bike,65SNPPRPIVPMDwillVPI willVBquitNPsmokingSBARY ou WillINifSyou will commit to recycling.Figure 1: Parse tree of a I Will If You Will challenge.
The subtrees governing the I Will and You Will partof the sentence are marked.we skip the particle and take the next sib-ling as the object.
If the object is a prepo-sitional phrase with constituent tag PP, e.g.,go without electricity for a month, we takethe second child of the prepositional phraseas the object phrase.
We then extract two fea-tures to represent the object.
First, we extractthe lowercased head word of the object as afeature.
Second, we extract the concatena-tion of all the words in the yield of the objectnode as a single feature to capture the com-plete argument for longer objects.
In our ex-ample sentence, the object head feature andthe complete object feature are identical: ob-ject head=smoking and object=smoking.?
Unigram: We take all lowercased words thatare not stopwords in the I Will part of thesentence as binary features.
In our examplesentence, the unigram features unigr quit andunigr smoking would be active.?
Bigram: We take all lowercased bigrams inthe I Will part of the sentence as binary fea-tures.
We do not remove stopwords for bi-gram features.
In our example sentence, thebigram features bigr quit smoking would beactive.We note that our method is not restricted to thesefeature templates.
More sophisticated features,like tree kernels (Collins and Duffy, 2002) or se-mantic role labeling (Palmer et al., 2010), can beimagined.5 ExperimentsWe evaluate our approach using standard 10-foldcross-validation and report macro-average accu-racy scores for each of the feature sets.
The classi-fier in all our experiments is a linear SVM imple-mented in SVM-light (Joachims, 2006).5.1 DataWe obtained a snapshot of 18,290 challenges cre-ated during the 2013 IWIYW challenge.
The snap-shot was taken in mid May 2013, just 1.5 weeksbefore the 2013 Earth Hour event day.
We per-form the following pre-processing.
We normal-ize the text to proper UTF-8 encoding and removechallenges where the complete sentence containedless than 7 tokens.
These challenges were usuallyempty or incomplete.
We filter the challenges us-ing the langid.py tool (Lui and Baldwin, 2012)and only keep English challenges.
We normal-ized the casing of the sentences by first lower-casing all texts and then re-casing each sentencewith a simple re-casing model that replaces a wordwith its most frequent casing form.
The re-casingmodel is trained on the Brown corpus (Ku andFrancis, 1967).
We tokenize the sentences withthe Penn Treebank tokenizer.
We parse the sen-tences with the Stanford parser (Klein and Man-ning, 2003a; Klein and Manning, 2003b) to ob-66Features Accuracyrandom 0.5000verb 0.6241unigrams 0.8481unigrams + verb 0.8573object 0.8904verb + object 0.9115bigrams 0.9251unigrams + bigrams 0.9343unigrams + bigrams + verb 0.9361unigrams + bigrams + verb + object 0.9472Table 2: Results of 10-fold cross-validation exper-iments.tain a constituency parse tree for each challenge.After pre-processing, we are left with 5,499 chal-lenges (4,982 unique), with 4,474 unique I Willchallenges and 70 unique You Will challenges.We create binary classifications examples be-tween pairs of actions as described in Section 4.As we create all possible combinations between IWill challenges with common You Will challenges,the number of ranking pairs for training is large.In our case, we ended up with over 840,000 classi-fication instances.
We note that not every I Will ac-tion is guaranteed to be included in the final set ofranking pairs as challenges with a unique You Willpart that is not found in any other challenge cannotbe joined and are effectively ignored.
However,this is not a problem for our experiments.
The bi-nary classification instances are used to train andtest a ranking model for estimating the value ofactions as described in the last section.5.2 ResultsThe results of our cross-validation experiments areshown in Table 2.The random baseline for all experiments is 50%.Just using the verb of the I Will action as a fea-ture improves over the random baseline to 62.41%.Using a unigram bag-of-words representation ofthe actions achieves a very respectable score of84.81%.
When we combine unigrams with theverb feature, we achieve 85.73%.
One of the mostsurprising results of our experiments is that theobject of the action alone is a very effective fea-ture, achieving 89.04%.
When combined with theverb feature, the object feature achieves 91.15%which shows that the verb and object carry mostof the relevant information that the model requiresto gauge the value of actions.
Using bigrams asfeatures, seems to catch this information just as ac-curately, achieving 92.51% accuracy.
The score isfurther improved by combining the different fea-ture sets.
The best result of 94.72% is obtainedby combining all the features: unigrams, bigrams,verb, and object.
In summary, these results showthat our method is able to accurately predict therelative value of actions using simple linguisticfeatures, which is the main contribution of thiswork.6 Related WorkThe concept of value and reciprocity has beenextensively studied in the social sciences (Ger-gen and Greenberg, 1980), anthropology (Sahlins,1972), economics (Fehr and G?achter, 2000), andphilosophy (Becker, 1990).
In linguistics, valuehas been studied by Jackendoff (2006).
His workforms the starting point of our approach.In natural language processing, there has beenvery little work on the concept of value.
Paul et al.
(2009) and Girju and Paul (2011) address the prob-lem of semi-automatically mining patterns that en-code reciprocal relationships using pronoun tem-plates.
Their work focuses on mining patterns ofreciprocity while our work uses expressions of re-ciprocal actions to learn the value of actions.None of the above works tries to estimate thevalue of actions, as we do in this work.
In fact, weare not aware of any other work that tries to esti-mate the value of actions from lexical expressionsof value.7 ConclusionWe have presented a simple and effective methodfor learning the value of actions from reciprocalsentences.
We show that our SVM-based rankingmodel with simple linguistic features is able to ac-curately rank pairs of actions from the I Will IfYou Will Earth Hour challenge, achieving an ac-curacy of up to 94.72%.AcknowledgementWe thank Sid Das from Earth Hour for shar-ing the IWIYW data with us.
We thank MarekKowalkiewicz for helpful discussions.
The re-search is partially funded by the Economic Devel-opment Board and the National Research Founda-tion of Singapore.67ReferencesLawrence C Becker, editor.
1990.
Reciprocity.
Uni-versity of Chicago Press.Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, andHang Li.
2007.
Learning to rank: from pairwiseapproach to listwise approach.
In Proceedings of the24th International Conference on Machine Learning(ICML), pages 129?136.Michael Collins and Nigel Duffy.
2002.
Convolutionkernels for natural language.
In Advances in Neu-ral Information Processing Systems 14 (NIPS 2001),pages 625?632.Ernst Fehr and Simon G?achter.
2000.
Cooperationand punishment in public goods experiments.
pages980?994.Robert Fiengo and Howard Lasnik.
1973.
The logicalstructure of reciprocal sentences in English.
Foun-dations of language, pages 447?468.Kenneth J. Gergen and Willis Richard H. Greenberg,Martin S., editors.
1980.
Social exchange: Ad-vances in theory and research.
Plenum Press.Roxana Girju and Michael J Paul.
2011.
Modelingreciprocity in social interactions with probabilisticlatent space models.
Natural Language Engineer-ing, 17(1):1?36.Ralf Herbrich, Thore Graepel, and Klaus Obermayer.1999.
Support vector learning for ordinal regres-sion.
In In Proceedings of the 1999 InternationalConference on Articial Neural Networks, pages 97?102.Ray Jackendoff.
2006.
The peculiar logic of value.Journal of Cognition and Culture, 6(3-4):375?407.Thorsten Joachims.
2006.
Training linear SVMs in lin-ear time.
In Proceedings of the 12th ACM SIGKDDinternational conference on Knowledge discoveryand data mining, pages 217?226.Dan Klein and Christopher D. Manning.
2003a.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting of the Association for Compu-tational Linguistics (ACL 2003), pages 423?430.Dan Klein and Christopher D. Manning.
2003b.
Fastexact inference with a factored model for naturallanguage parsing.
Advances in Neural InformationProcessing Systems 15 (NIPS 2002), pages 423?430.Henry Ku and W. Nelson Francis.
1967.
Computa-tional Analysis of Present-Day American English.Brown University Press.Marco Lui and Timothy Baldwin.
2012.
An off-the-shelf language identification tool.
In Proceedingsof the 50th Annual Meeting of the Association forComputational Linguistics (ACL 2012), pages 25?30.Martha Palmer, Daniel Gildea, and Nianwen Xue.2010.
Semantic role labeling.
Synthesis Lectureson Human Language Technologies, 3(1):1?103.Michael Paul, Roxana Girju, and Chen Li.
2009.
Min-ing the web for reciprocal relationships.
In Proceed-ings of the 13th Conference on Computational Nat-ural Language Learning (CoNLL), pages 75?83.Marshall D. Sahlins.
1972.
Stone age economics.Transaction Publishers.68
