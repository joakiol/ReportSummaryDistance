Error-driven HMM-based Chunk Taggerwith Context-dependent LexiconGuoDong ZHOUKent Ridge Digital Labs21 Heng Hui Keng TerraceSingapore 119613zhougd@krdl.org.sgJian SUKent Ridge Digital Labs21 Heng Hui Keng TerraceSingapore 119613sujian@krdl.org.sgAbstractThis paper proposes a new error-driven HMM-based text chunk tagger with context-dependentlexicon.
Compared with standard HMM-basedtagger, this tagger uses a new Hidden MarkovModelling approach which incorporates morecontextual information into a lexical entry.Moreover, an error-driven learning approach isadopted to decrease the memory requirement bykeeping only positive lexical entries and makesit possible to further incorporate more context-dependent lexical entries.
Experiments howthat this technique achieves overall precisionand recall rates of 93.40% and 93.95% for allchunk types, 93.60% and 94.64% for nounphrases, and 94.64% and 94.75% for verbphrases when trained on PENN WSJ TreeBanksection 00-19 and tested on section 20-24, while25-fold validation experiments of PENN WSJTreeBank show overall precision and recallrates of 96.40% and 96.47% for all chunk types,96.49% and 96.99% for noun phrases, and97.13% and 97.36% for verb phrases.IntroductionText chunking is to divide sentences into non-overlapping segments on the basis of fairlysuperficial analysis.
Abney(1991) proposed thisas a useful and relatively tractable precursor tofull parsing, since it provides a foundation forfurther levels of analysis, while still allowingmore complex attachment decisions to bepostponed toa later phase.Text chunking typically relies on fairlysimple and efficient processing algorithms.Recently, many researchers have looked at textchunking in two different ways: Someresearchers have applied rule-based methods,combining lexical data with finite state or otherrule constraints, while others have worked oninducing statistical models either directly fromthe words and/or from automatically assignedpart-of-speech classes.
On the statistics-basedapproaches, Skut and Brants(1998) proposed aHMM-based approach to recognise the syntacticstructures of limited length.
Buchholz, Veenstraand Daelemans(1999), and Veenstra(1999)explored memory-based learning method to fredlabelled chunks.
Ratnaparkhi(1998) usedmaximum entropy to recognise arbitrary chunkas part of a tagging task.
On the rule-basedapproaches, Bourigaut(1992) used someheuristics and a grammar to extract"terminology noun phrases" from French text.Voutilainen(1993) used similar method to detectEnglish noun phrases.
Kupiec(1993) applied.finite state transducer in his noun phrasesrecogniser for both English and French.Ramshaw and Marcus(1995) usedtransformation-based l arning, an error-drivenlearning technique introduced by EricBn11(1993), to locate chunks in the taggedcorpus.
Grefenstette(1996) applied finite statetransducers to fred noun phrases and verbphrases.In this paper, we will focus on statistics-based methods.
The structure of this paper is asfollows: In section 1, we will briefly describethe new error-driven HMM-based chunk taggerwith context-dependent lexicon in principle.
Insection 2, a baseline system which only includesthe current part-of-speech in the lexicon isgiven.
In section 3, several extended systemswith different context-dependent lexicons aredescribed.
In section 4, an error=driven learningmethod is used to decrease memory requirementof the lexicon by keeping only positive lexical71entries and make it possible to further improvethe accuracy by merging different context-dependent lexicons into one after automaticanalysis of the chunking errors.
Finally, theconclusion is given.The data used for all our experiments iextracted from the PENN" WSJ Treebank(Marcus et al 1993) by the program providedby Sabine Buchholz from Tilbug University.We use sections 00-19 as the training data and20-24 as test data.
Therefore, the performance ison large scale task instead of small scale task onCoNLL-2000 with the same evaluationprogram.For evaluation of our results, we use theprecision and recall measures.
Precision is thepercentage of predicted chunks that are actuallycorrect while the recall is the percentage ofcorrect chunks that are actually found.
Forconvenient comparisons of only one value, wealso list the F~= I value(Rijsbergen 1979):(/32 + 1).
precision, recall, with/3 = 1./3 2. precision + recall1 HMM-based Chunk TaggerThe idea of using statistics for chunking goesback to Church(1988), who used corpusfrequencies to determine the boundaries ofsimple non-recursive noun phrases.
Skut andBrants(1998) modified Church's approach in away permitting efficient and reliable recognitionof structures of limited depth and encoded thestructure in such a way that it can be recognisedby a Viterbi tagger.
This makes the process runin time linear to the length of the input string.Our approach follows Skut and Brants' wayby employing HMM-based tagging method tomodel the chunking process.Given a token sequence G~ = g~g2 ""g, ,the goal is to fred a stochastic optimal tagsequence Tin = tlt2...t n which maximizeslog P(T~" I Of ) :e(:q",G?
)log P(Ti n \[ G? )
= log P(Ti n ) + log P(Ti n )" P(G?
)The second item in the above equation is themutual information between the tag sequenceTin and the given token sequence G~.
Byassuming that the mutual information betweenG~ and T1 ~ is equal to the summation of mutualinformation between G~ and the individual tagti(l_<i_<n ) :n log P(TI"' G?)
= ~ log P(t,, G~)e(Tln ).
P(G~) i=1 P(t,).
P(G?
)ornn MI(T~ ~ , G~ ) = ~ MI(t, ,  G? )
,i= lwe have:log P(T~ n I G~)= log P(T1 n ) + ~, log P(ti' G?
)_P(t i).
P(G?
)rl n= log P(T1 ~ ) - Z log P(t, ) + ~ log P(t, \[ G?
)i=1  i=1The first item of above equation can besolved by using chain rules.
Normally, each tagis assumed to be probabilistic dependent on theN-1 previous tags.
Here, backoff bigram(N=2)model is used.
The second item is thesummation of log probabilities of all the tags.Both the first item and second item correspondto the language model component of the taggerwhile the third item corresponds to the lexiconcomponent of the tagger.
Ideally the third itemcan be estimated by using the forward-backwardalgorithm(Rabiner 1989) recursively for thefirst-order(Rabiner 1989) or second-orderHMMs(Watson and Chunk 1992).
However,several approximations on it will be attemptedlater in this paper instead.
The stochasticoptimal tag sequence can be found bymaxmizing the above equation over all thepossible tag sequences.
This is implemented bythe Viterbi algorithm.The main difference between our tagger andother standard taggers lies in our tagger has acontext-dependent lexicon while others use acontext-independent l xicon.For chunk tagger, we haveg 1= piwi whereW~ n = w~w2---w n is the word-sequence andP~ = PiP2 "" P~ is the part-of-speech72sequence.
Here, we use structural tags torepresenting chunking(bracketing and labelling)structure.
The basic idea of representing thestructural tags is similar to Skut andBrants(1998) and the structural tag consists ofthree parts:1) Structural relation.
The basic idea is simple:structures of limited depth are encoded using afinite number of flags.
Given a sequence ofinput tokens(here, the word and part-of-speechpairs), we consider the structural relationbetween the previous input token and thecurrent one.
For the recognition of chunks, it issufficient to distinguish the following fourdifferent structural relations which uniquelyidentify the sub-structures of depth l(Skut andBrants used seven different structural relationsto identify the sub-structures of depth 2).00 the current input token and the previous onehave the same parent90 one ancestor of the current input token andthe previous input oken have the same parent09 the current input token and one ancestor ofthe previous input oken have the same parent99 one ancestor of the current input token andone ancestor of the previous input token havethe same parentFor example, in the following chunk taggedsentence(NULL represents the beginning andend of the sentence):NULL \[NP He/PRP\] \[VP reckons/VBZ\] [NPthe/DT current/JJ account/NN deficit/NN\] \[VPwill/MD narrow/VB\] \[PP to/TO\] \[NP only/RB#/# 1.8/CD billion/CD\] \[PP in/IN\] \[NPSeptember/NNP\] \[O./.\] NULLthe corresponding structural relations betweentwo adjacent input okens are:90(NULL He/PRP)99(He/PRP reckons/VBZ)99(reckons/VBZ the/DT)00(the/DT current/JJ)00(current/JJ account/NN)00(account/NN deficit/NN)99(deficit/NN will/MD)00(will/MD narrow/VB)99(narrow/VB to/TO)99(to/TO only/RB)O0(only/RB #/#)00(#/# 1.8/CD)00(1.8/CD billion/CD)99(billion/CD in/IN)99(in/IN september/NNP)99(september/NNP ./.)09(./.
NULL)Compared with the B-Chunk and I-Chunkused in Ramshaw and Marcus(1995), structuralrelations 99 and 90 correspond to B-Chunkwhich represents the first word of the chunk,and structural relations 00 and 09 correspond toI-Chunk which represnts each other in the chunkwhile 90 also means the beginning of thesentence and 09 means the end of the sentence.2)Phrase category.
This is used to identify thephrase categories of input tokens.3)Part-of-speech.
Because of the limitednumber of structural relations and phrasecategories, the part-of-speech is added into thestructural tag to represent more accurate models.For the above chunk tagged sentence, thestructural tags for all the corresponding inputtokens are:90 PRt~NP(He/PRP)99_VB Z_VP(reckons/VBZ)99 DT NP(the/DT)O0 JJ NP(currentJJJ)00_N/'~NP(account/NN)00 N1NNP(deficiffNN)99_MDSVP(will/MD)00 VB_VP(narrow/VB)99_TO PP(to/TO)99_RB~,IP(only/RB)oo_# NP(#/#)00 CD_NP(1.8/CD)0(~CD~qP(billion/CD)99_IN PP(in/IN)99~lNP~,lP(september/NNP)99_._0(./.
)2 The Baseline SystemAs the baseline system, we assumeP(t i I G?
)= P(t i I pi ).
That is to say, only thecurrent part-of-speech is used as a lexical entryto determine the current structural chunk tag.Here, we define:?
?
is the list of lexical entries in thechunking lexicon,73?
\[ @ \[ is the number of lexical entries(the sizeof the chunking lexicon)?
C is the training data.For the baseline system, we have :?
@={pi,p~3C}, where Pi is a part-of-speech existing in the tra\]Lning data C?
\]@ \[=48 (the number of part-of-speech tagsin the training data).Table 1 gives an overview of the results ofthe chunking experiments.
For convenience,precision, recall and F#_ 1 values are givenseperately for the chunk types NP, VP, ADJP,ADVP and PP.Type Precision Recall Fa__~Overall 87.01 89.68 88.32NP 90.02 90.50 90.26VP 89.86 93.14 91.47ADJP 70.94 63.84 67.20ADVP 57.98 80.33 I 67.35PP 85.95 96.62 90.97Table 1 : Results of chunking experiments withthe lexical entry list : ~ = { pi, p~3C}3 Context-dependent LexiconsIn the last section, we only use current part-of-speech as a lexical entry.
In this section, we willattempt o add more contextual information toapproximate P(t i/G~).
This can be done byadding lexical entries with more contextualinformation into the lexicon ~.
In thefollowing, we will discuss five context-dependent lexicons which consider differentcontextual information.3.1 Context of current part-of-speech andcurrent wordHere, we assume:e(t i I G~) = I P(ti I p~wi)\[ P(tl I Pi)wherepiwi ~ dpPiWi ~ dp~={piwi,piwi3C}+{pi,pi3C } and piwi is apart-of-speech and word pair existing in thetraining data C.In this case, the current part-of-speech andword pair is also used as a lexical entry todetermine the current structural chunk tag andwe have a total of about 49563 lexicalentries(\[ ?
\]=49563).
Actually, the lexicon usedhere can be regarded as context-independent.The reason we discuss it in this section is todistinguish it from the context-independentlexicon used in the baseline system.
Table 2give an overview of the results of the chunkingexperiments on the test data.Type \[PrecisionOverall 90.32NP 90.75VP 90.88ADJP 76.01ADVP 72.67PP 94.96Table 2 : Results of chunking experimentsthe lexical entry= {piwi ,Piwi3C} "1" {Pi" Pi 3C}Recall Fa~.l92.18 9i.2492.14 91.4492.78 91.8270.00 72.8888.33 79.7496.48 95.71withlist :Table 2 shows that incorporation of currentword information improves the overall F~=~value by 2.9%(especially for the ADJP, ADVPand PP chunks), compared with Table 1 of thebaseline system which only uses current part-of-speech information.
This result suggests thatcurrent word information plays a very importantrole in determining the current chunk tag.3.2 Context of previous part-of-speech andcurrent part-of-speechHere, we assume :P(t i / G~)I P(ti / pi-lPi ) Pi-lPi E= \[ P(ti I Pi) Pi-!
Pi ~ ~where= {Pi-l Pi, P~-1Pi 3C} + { Pi, pi3C} and Pi-lPiis a pair of previous part-of-speech and currentpart-of-speech existing in the training data C.In this case, the previous part-of-speech andcurrent part-of-speech pair is also used as alexical entry to determine the current structuralchunk tag and we have a total of about 1411lexical entries(l~\]=1411).
Table 3 give anoverview of the results of the chunkingexperiments.74TypeOverallPrecision88.63NP 90.77VP 92.46ADJP 74.93 60.13 66.72ADVP 71.65 73.21 72.42PP 87.28 91.80 89.49Table 3: Results of chunking experiments withthe lexical entry list : ?
={Pi-lPi, Pi-lPi 3C} + {Pi, Pi 3C}Recall F#= I89.00 88.8291.18 90.9792.98 92.72Compared with Table 1 of the baselinesystem, Table 3 shows that additional contextualinformation of previous part-of-speech improvesthe overall F/~_~ value by 0.5%.
Especially,F/3_ ~ value for VP improves by 1.25%, whichindicates that previous part-of-speechinformation has a important role in determiningthe chunk type VP.
Table 3 also shows that therecall rate for chunk type ADJP decrease by3.7%.
It indicates that additional previous part-of-speech information makes ADJP chunkseasier to merge with neibghbouring chunks.3.3 Context of previous part-of-speech,previous word and current part-of-speechHere, we assume :P(t, / G~)IP(ti / pi_lwi_lpi) pi_lwi_lpl ~ dpI\[ P(ti \[ Pi ) Pi-lWi-I Pi ~ ~where= { Pi-i wi-l Pi, Pi-l wi-I Pi3 C} + { Pi, Pi 3 C },where pi_lwi_lp~ is a triple pattern existing inthe training corpus.In this case, the previous part-of-speech,previous word and current part-of-speech tripleis also used as a lexical entry to determine thecurrent structural chunk tag and } ?
1=136164.Table 4 gives the results of the chunkingexperiments.
Compared with Table 1 of thebaseline system, Table 4 shows that additional136116 new lexical entries of formatPi-lw~-lPi improves the overall F#= l value by3.3%.
Compared with Table 3 of the extendedsystem 2.2 which uses previous part-of-speechand current part-of-speech as a lexical entry,Table 4 shows that additional contextualinformation of previous word improves theoverall Fa= 1 value by 2.8%.Type Precision Recall F~=lOverall 91.23 92.03 91.63NP 92.89 93.85 93.37VP 94.10 94.23 94.16ADJP 79.83 69.01 74.03ADVP 76.91 80.53 78.68PP 90.41 94.77 92.53Table 4 : Results of chunking experiments withthe lexical entry list :={p,_lw~_~ p,,p,_~ w,_ip,3C } + {Pi , p~3C}3.4 Context of previous part-of-speech, currentpart-of-speech and current wordHere, we assume :P(t i I G~ )IP(tt I Pi-i PiWi) Pi-I piwi E dp\[ P(ti / Pi ) Pi-I Pi Wi ~ 1I)where= {Pi-lPiWi, Pi-lP~W~ 3C} + {Pi, Pi3C},where pi_lpiw~ is a triple pattern existing inthe training and \] ?
\[=131416.Table 5 gives the results of the chunkingexperiments.Type Precision Recall F/3= 1Overall 92.67 93.43 93.05NP 93.35 94.10 93.73VP 93.05 94.30 93.67ADJP 80.65 72.27 76.23ADVP 78.92 84.48 81.60PP 95.30 96.67 95.98Table 5: Results of chunking experiments withthe lexical entry list :={Pi-lPiWi, P,-iP, w,3C} + {pi , Pi 3C}Compared with Table 2 of the extendedsystem which uses current part-of-speech andcurrent word as a lexical entry, Table 5 showsthat additional contextual information ofprevious part-of-speech improves the overallFa= 1 value by 1.8%.3.5 Context of previous part-of-speech,previous word, current part-of-speech andcurrent wordHere, the context of previous part-of-speech,current part-of-speech and current word is usedas a lexical entry to determine the current75structural chunk tag and qb ={Pi-l wi-lPiWi, Pi-lwi-~piwi 36'} + {Pi, P i3C} ,where p~_lWi_~P~W~ is a pattern existing in thetraining corpus.
Due to memory limitation, onlylexical entries which occurs :more than 1 timesare kept.
Out of 364365 possible lexical entriesexisting in the training data, 98489 are kept(1~ 1=98489).= I P(ti/Pi-\]wi-,PiWli)\[ P(t, lp,) pi_lwi_lpiwi ~Table 6 gives the results of the chunkingexperiments.TypeOverallNPVPADJPADVPPPPrecision92.2893.5092.6281.3975.0994.12Recall93.0493.5394.0772.1786.2397.12F~=l92.6693.5293.3576.5080.2795.59Table 6: Results of chunking experiments withthe lexical entry list : ?
={Pi-l wi-\]PiWi, Pi-lwi-lpiwi3C} + {Pi, p~3C}Compared with Table 2 of the extendedsystem which uses current part-of-speech andcurrent word as a lexical entry, Table 6 showsthat additional contextual information ofprevious part-of-speech improves the overallFt3=l value by 1.8%.3.6 ConclusionAbove experiments shows that adding morecontextual information i to lexicon significantlyimproves the chunking accuracy.
However, thisimprovement is gained at the expense of a verylarge lexicon and we fred it difficult o merge allthe above context-dependent l xicons in a singlelexicon to further improve the chunkingaccurracy because of memory limitation.
Inorder to reduce the size of lexicon effectively,an error-driven learning approach is adopted toexamine the effectiveness of lexical entries andmake it possible to further improve thechunking accuracy by merging all the abovecontext-dependent l xicons in a single lexicon.This will be discussed in the next section.4 Error-driven LearningIn section 2, we implement a basefine systemwhich only considers current part-of-speech as alexical entry to dete, ufine the current chunk tagwhile in section 3, we implement severalextended systems which take more contextualinformation i to consideration.Here, we will examine the effectiveness oflexical entries to reduce the size of lexicon andmake it possible to further improve thechunking accuracy by merging several context-dependent lexicons in a single lexicon.For a new lexical entry e i, the effectivenessF~ (e i) is measured by the reduction in errorwhich results from adding the lexical entry to- -  ~ Er ro r  (e,).
the lexicon : F~ (e i ) = F :  rr?r (e i ) - o+AoHere, F,~ r~?r (el) is the chunking error numberof the lexical entry e i for the old lexiconr~ Er ror  / x and r~,+~ te i) is the chunking error number ofthe lexical entry e i for the new lexicon+ AO where e~ e A~ (A~ is the list ofnew lexical entries added to the old lexicon ~ ).If F o (e i ) > 0, we define the lexical entry ei aspositive for lexicon ~.
Otherwise, the lexicalentry e i is negative for lexicon ~.Tables 7 and 8 give an overview of theeffectiveness distributions for different lexiconsapplied in the extended systems, compared withthe lexicon appfied in the baseline system, onthe test data and the training data, respectively.Tables 7 and 8 show that only a minority oflexical entries are positive.
This indicates thatdiscarding non-positive lexical entries willlargely decrease the lexicon memoryrequirement while keeping the chunkingaccurracy.Context Positive1800209Negative314136Total4083 I 155Table 7 : The effectiveness of lexicalthe test data .....4951513632876 229 1361162895 193 13136898441entries on76Context Positive i Negative6724lType i Precision Recall Fa=lOverall 91.02 92.21 91.61NP 92.36 93.69 93.02VP 93.68 94.94 94.30ADJP 78.28 71.46 74.71ADVP 76.77 81.79 79.20PP 90.67 95.37 92.96Totalvos,w, 719 49515eos,_,Pos, 357 196 1363POS,.~w,.,eos,, 13205 582 136116POS,_,eos,w, 14186 325 131368POS,.,w,_leos,,w, 15516 144 98441Table 8 : The effectiveness of lexical entries onthe training dataTables 9-13 give the performances of thefive error-driven systems which discard all thenon-positive l xical enrties on the training data.Here, ~'  is the lexicon used in the baselinesystem, dP'={pi,pi3C } and A~=~-~' .
Itis found that Ffl_~ values of error drivensystems for context of current part-of-speechand word pak and for context of previous part-of-speech and current part-of-speech increase by1.2% and 0.6%.
Although F~= 1values for otherthree cases slightly decrease by 0.02%, 0.02%and 0.19%, the sizes of lexicons have beengreatly reduced by 85% to 97%.Type Precision Recall F#=lOverall 91.69 93.28 92.48NP 92.64 93.48 93.06VP 92.16 93.66 92.90ADJP 78.39 71.69 74.89ADVP 73.66 87.80 80.11PP 95.18 97.38 96.27Table 9 : Results of chunking experiments witherror-driven lexicon : dp={ p~w~, p,w,3C & F~,.
(p~w i ) > O} + { p~, p~3C}Type Precision Recall F~=lOverall 88.68 90.28 89.47NP 90.61 91.57 91.08VP 91.80 94.08 92.90ADJP 72.20 62.72 67.13ADVP 70.53 78.90 74.48PP 86.55 96.34 91.19Table 10: Results of chunking experimentswith error-driven lexicon : ?
={ P,-~ Pi, Pi-1 Pi ~C & F~.
(p,_~ p, ) > 0}+ { Pi, Pi 3C}Table 11: Results of chunking experimentswith error-driven lexicon : ?
={ pi_l Wi_lPi , pi_l wi_lpi3C & V~,(Pi_l Wi_iPi ) > O}+{pi ,P i~C}TypeOverall 92.84NPVPPrecision93.3593.9779.4995.19Recall93.2193.6594.6772.94Ffl=l93.0393.5094.3276.07 ADJPADVP 79.47 85.91 82.57PP7796.29 95.74Table 12: Results of chunking experimentswith error-driven lexicon : ?
={ Pi-I P~W~, p~_~ Piw,3C & F.. (pi_~ p,w i) > 0}+{pi ,P i3C}Type Precision Recall F~_ 1Overall 91.99 92.95 92.47NP 93.35 93.39 93.37VP 92.89 94.36 93.62ADJP 80.01 71.70 75.63ADVP 73.40 87.32 79.76PP 93.42 97.33 95.33Table 13: Results of chunking experimentswith error-driven lexicon : ?
={Pi-l Wi-lPiWi' Pi-lWi-lpiWi3C+{pi ' Pi3C}& F?.
(p~_~w~_~p~w~) > O}After discussing the five context-dependentlexicons separately, now we explore themerging of context-dependent lexicons byassuming :CI~ .~ { Pi-lWi-I PiWi, Pi-lWi-I PiwigC& Fa,.
(pi-lwi-t piwi ) > 0}+ { Pi-I PiW~, Pi-l piwi ~C & Fa" (Pi-l piwi ) > O}+ { Pi-lWi-I Pi" Pi-lWi-1Pi 3C & F~.
(pi_lWi_l Pi ) > 0}+ { Pi-1 Pi, Pi-I Pii ~C & F~, (Pi-l Pi )> O}+ { piw~, Piw~3C & F~,.
(PiWi) > 0} + { Pi, p~3C}and P(t i /G~) is approximatl~ by the followingorder :1. if Pi_lWi_iPiWi E fI~,P(ti /G~)=P( t  i / p i _ lw i _ lP iWi )2. if p~_lp~wi E cb,P(ti /G~)=P( t  i /p i _ lw i _ lP iWi )3. if Pi-twi-lPi E ~,P(t i/G~) = P(t i / pi_l wi_l: pi )4. if PiWi E ~,  P(t i / G~ ) = P(t i / piwi )5. if Pi-I Pi E ~,  P(t i / G~ ) = P(t i / Pi-1Pi)6.
P(t i lG : )=P( t  i lpi_lpi)Table 14 gives an overview of the chunkingexperiments using the above assumption.
Itshows that the F:=i value for the mergedcontext-dependent lexicon inreases to 93.68%.For a comparison, the F/~=i value is 93.30%when all the possible lexical entries are includedin ~ (Due to memory limitation, only the top150000 mostly occurred lexical entries areincluded).Type Precision Recall F#=iOverall 93.40 93.95 93.68NP 93.60 94.64 94.12VP 94.64 94.75 94.70ADJP 77.12 74.55 75.81ADVP 82.39 83.80 83.09PP 96.61 96.63 96.62Table 14: Results of chunking experimentswith the merged context-dependent l xiconFor the relationship between the trainingcorpus size and error driven learningperformance, Table 15 shows that theperformance of error-driven learning improvesstably when the training corpus ize increases.Training Sections I ~ I Accuracy i FB 10-10-30-50-70-90-110-130-i50-170-1914384 94.78% 91.9524507 95.19% i 92.5132316 95.28%1 92.7738286 95.41% 93.0039876 95.53% i 93.1243372 95.65% 93.3146029 95.62% 93.2947901 95.66% 93.3448813 95.74% i 93.4149988 95.92% 93.68Table 15: The performance oferror-drivenlearning with different training corpus izeFor comparison with other chunk taggers,we also evaluate our chunk tagger with themerged context-dependent lexicon by cross-validation on all 25 partitions of the PENN WSJTreeBank.
Table 16 gives an overview of suchchunking experiments.Type Precision Recall Fa=lOverall 96.40 96.47 96.44NP 96.49 96.99 96.74VP 97.13 97.36 97.25ADJP 89.92 88.15 89.03ADVP 91.52 87.57 89.5097.13 97.36 PP 97.25Table 16: Results of 25-fold cross-validationchunking experiments with the mergedcontext-dependent l xiconTables 14 and 16 shows that our new chunktagger greatly outperforms other eported chunktaggers on the same training data and test databy 2%-3%.
(Buchholz S., Veenstra J. andDaelmans W.(1999), Ramshaw L.A. andMarcus M.P.
(1995), Daelemans W., BuchholzS.
and Veenstra J.
(1999), and VeenstraJ.
(1999)).ConclusionThis paper proposes a new error-driven HMM-based chunk tagger with context-dependentlexicon.
Compared with standard HMM-basedtagger, this new tagger uses a new HiddenMarkov Modelling approach which incorporatesmore contextual information into a lexical entrynby assuming MI(Tqn,G~)= 2Ml ( t , ,G f ) .i=1Moreover, an error-driven learning approach isadopted to drease the memeory requirement andfurther improve the accuracy by including morecontext-dependent information i to lexicon.It is found that our new chunk taggersingnificantly outperforms other eported chunktaggers on the same training data and test data.For future work, we will explore theeffectivessness of considering even morecontextual information on approximation ofP(T~"IG ~) by using the forward-backwardalgodthm(Rabiner 1989) while currently weonly consider the contextual information ofcurrent location and previous location.78AcknowledgementWe wish to thank Sabine Buchholzfrom Tilbug University for kindly providing usher program which is also used toextact data for Conll-2000 share task.ReferencesAbney S. "Parsing by chunks ".
Principle-BasedParsing edited by Berwick, Abney and Tenny.Kluwer Academic Publishers.Argamon S., Dagan I. and Krymolowski Y.
"Amemory-based approach to learning shallownatural language patterns."
COL1NG/ACL-1998.
Pp.67-73.
Montreal, Canada.
1998.Bod R. "A computational model of languageperformance: Data-oriented parsing."COLING-1992.
Pp.855-859.
Nantes, France.1992.Boungault D. "Surface grammatical nalysis forthe extraction of terminological nounphrases".
COLING-92.
Pp.977-981.
1992.Bdll Eric.
"A corpus-based approach tolanguage learning".
Ph.D thesis.
Univ.
ofPenn.
1993Buchholz S., Veenstra J. and Daelmans W."Cascaded grammatical relation assignment.
"Proceeding of EMNLP/VLC-99, at ACL'99.1999Cardie C. "A case-based approach to knowledgeacquisition for domain-specific sentenceanalysis."
Proceeding of the 11 'h NationalConference on Artificial Intelligence.
Pp.798-803.
Menlo Park, CA, USA.
AAAI Press.1993.Church K.W.
"A stochastic parts program andnoun phrase parser for unrestricted Text.
"Proceeding of Second Conference on AppliedNatural Language Processing.
Pp.136-143.Austin, Texas, USA.
1988.Daelemans W., Buchholz S. and Veenstra J.
"Memory-based shallow parsing."
CoNLL-1999.
Pp.53-60.
Bergen, Norway.
1999.Daelemans W., Zavrel J., Berck P. and Gillis S."MBT: A memory-based part-of-speechtagger generator."
Proceeding of the FourthWorkshop on Large Scale Corpora.
Pp.
14-27.ACL SIGDAT.
1996.Grefenstette G. "Light parsing as finite-statefiltering".
Workshop on Extended Finite StateModels of Language at ECAI'96.
Budapest,Hungary.
1996.Kupiec J. "
An algorithm for finding nounphrase correspondences in bilingual corpora".ACL'93.
Pp17-22.
1993.Marcus M., Santodni B. and Marcinkiewicz?
M.A.
"Buliding a large annotated corpus ofEnglish: The Penn Treebank".
ComputationalLinguistics.
19(2):313-330.
1993.Rabiner L. "A tutorial on Hidden MarkovModels and selected applications in speechrecognition".
IEEE 77(2), pp.257-285.
1989.Ramshaw L.A. and Marcus M.P.
"Transformation-based Learning".Proceeding of 3th ACL Workshop on VeryLarge Corpora at ACL'95.
1995.Rijsbergen C.J.van.
Information Retrieval.Buttersworth, London.
1979.Skut W. and Brants T. "Chunk tagger: statisticalrecognition of noun phrases."
ESSLLI-1998Workshop on Automated Acquisition of Syntaxand Parsing.
Saarbruucken, Germany.
1998.Veenstra J.
"Memory-based text chunking".Workshop on machine learning in humanlanguage technology at A CAI'99.
1999.Voutilainen A.
"Nptool: a detector of Englishphrases".
Proceeding of the Workshop onVery Large Corpora.
Pp48-57.
ACL' 93.1993Watson B. and Chunk Tsoi A.
"Second orderHidden Markov Models for speechrecognition".
Proceeding of 4 ~ AustralianInternational Conference on Speech Scienceand Technology.
Pp.
146-151.1992.79
