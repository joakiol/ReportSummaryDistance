Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 46?55,Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational LinguisticsThe Role of Information Extraction in the Design of a Document TriageApplication for BiocurationSandeep PokkunuriSchool of ComputingUniversity of UtahSalt Lake City, UTsandeepp@cs.utah.eduCartic RamakrishnanInformation Sciences InstituteUniv.
of Southern CaliforniaMarina del Rey, CAcartic@isi.eduEllen RiloffSchool of ComputingUniversity of UtahSalt Lake City, UTriloff@cs.utah.eduEduard HovyInformation Sciences InstituteUniv.
of Southern CaliforniaMarina del Rey, CAhovy@isi.eduGully APC BurnsInformation Sciences InstituteUniv.
of Southern CaliforniaMarina del Rey, CAburns@isi.eduAbstractTraditionally, automated triage of papers isperformed using lexical (unigram, bigram,and sometimes trigram) features.
This pa-per explores the use of information extrac-tion (IE) techniques to create richer linguisticfeatures than traditional bag-of-words models.Our classifier includes lexico-syntactic pat-terns and more-complex features that repre-sent a pattern coupled with its extracted noun,represented both as a lexical term and as asemantic category.
Our experimental resultsshow that the IE-based features can improveperformance over unigram and bigram fea-tures alone.
We present intrinsic evaluationresults of full-text document classification ex-periments to determine automatically whethera paper should be considered of interest tobiologists at the Mouse Genome Informatics(MGI) system at the Jackson Laboratories.
Wealso further discuss issues relating to designand deployment of our classifiers as an ap-plication to support scientific knowledge cu-ration at MGI.1 IntroductionA long-standing promise of Biomedical NaturalLanguage Processing is to accelerate the process ofliterature-based ?biocuration?, where published in-formation must be carefully and appropriately trans-lated into the knowledge architecture of a biomed-ical database.
Typically, biocuration is a manualactivity, performed by specialists with expertise inboth biomedicine and the computational represen-tation of the target database.
It is widely acknowl-edged as a vital lynch-pin of biomedical informatics(Bourne and McEntyre, 2006).A key step in biocuration is the initial triage ofdocuments in order to direct to specialists only thedocuments appropriate for them.
This classifica-tion (Cohen and Hersh, 2006)(Hersh W, 2005) canbe followed by a step in which desired informationis extracted and appropriately standardized and for-malized for entry into the database.
Both these stepscan be enhanced by suitably powerful Natural Lan-guage Processing (NLP) technology.
In this paper,we address text mining as a step within the broadercontext of developing both infrastructure and toolsfor biocuration support within the Mouse GenomeInformatics (MGI) system at the Jackson Labora-tories.
We previously identified ?document triage?as a crucial bottleneck (Ramakrishnan et al, 2010)within MGI?s biocuration workflow.Our research explores the use of information ex-traction (IE) techniques to create richer linguis-tic features than traditional bag-of-words models.These features are employed by a classifier to per-form the triage step.
The features include lexico-syntactic patterns as well as more-complex features,such as a pattern coupled with its extracted noun,where the noun is represented both as a lexical termand by its semantic category.
Our experimental re-sults show that the IE-based enhanced features canimprove performance over unigram and bigram fea-tures alone.46Evaluating the performance of BioNLP tools isnot trivial.
So-called intrinsic metrics measure theperformance of a tool against some gold standard ofperformance, while extrinsic ones (Alex et al, 2008)measure how much the overall biocuration processis benefited.
Such metrics necessarily involve thedeployment of the software in-house for testingby biocurators, and require a large-scale software-engineering infrastructure effort.
In this paper, wepresent intrinsic evaluation results of full-text doc-ument classification experiments to determine auto-matically whether a paper should be considered ofinterest to MGI curators.
We plan in-house deploy-ment and extrinsic evaluation in near-term work.Our work should be considered as the first step ofa broader process within which (a) the features usedin this particular classification approach will be re-engineered so that they may be dynamically recre-ated in any new domain by a reusable component,(b) this component is deployed into reusable infras-tructure that also includes document-, annotation-and feature-storage capabilities that support scalingand reuse, and (c) the overall functionality can thenbe delivered as a software application to biocuratorsthemselves for extrinsic evaluation in any domainthey choose.
Within the ?SciKnowMine?
project, weare constructing such a framework (Ramakrishnan etal., 2010), and this work reported here forms a pro-totype component that we plan to incorporate intoa live application.
We describe the underlying NLPresearch here, and provide context for the work bydescribing the overall design and implementation ofthe SciKnowMine infrastructure.1.1 MotivationMGI?s biocurators use very specific guidelines fortriage that continuously evolve.
These guidelinesare tailored to specific subcategories within MGI?striage task (phenotype, Gene Ontology1 (GO) term,gene expression, tumor biology and chromosomallocation mapping).
They help biocurators decidewhether a paper is relevant to one or more subcat-egories.
As an example, consider the guideline forthe phenotype category shown in Table 1.This example makes clear that it is not sufficientto match on relevant words like ?transgene?
alone.1http://www.geneontology.org/?Select paperIf: it is about transgenes where a gene from anyspecies is inserted in mice and this results ina phenotype.Except: if the paper uses transgenes toexamine promoter function?.Table 1: Sample triage guideline used by MGI biocura-torsTo identify a paper as being ?within-scope?
or ?out-of-scope?
requires that a biocurator understand thecontext of the experiment described in the paper.To check this we examined two sample papers; onethat matches the precondition of the above rule andanother that matches its exception.
The first paper(Sjo?gren et al, 2009) is about a transgene inser-tion causing a pheotype and is a positive exampleof the category phenotype, while the second paper(Bouatia-Naji et al, 2010) is about the use of trans-genes to study promoter function and is a negativeexample for the same category.Inspection of the negative-example paper illus-trates the following issues concerning the languageused: (1) This paper is about transgene-use in study-ing promoter function.
Understanding this requiresthe following background knowledge: (a) the twogenes mentioned in the title are transgenes; (b) thephrase ?elevation of fasting glucose levels?
in the ti-tle represents an up-regulation phenotype event.
(2)Note that the word ?transgene?
never occurs in theentire negative-example paper.
This suggests thatrecognizing that a paper involves the use of trans-genes requires annotation of domain-specific enti-ties and a richer representation than that offered bya simple bag-of-words model.Similar inspection of the positive-example paperreveals that (3) the paper contains experimental ev-idence showing the phenotype resulting from thetransgene insertion.
(4) The ?Materials and Meth-ods?
section of the positive-example paper clearlyidentifies the construction of the transgene and the?Results?
section describes the development of thetransgenic mouse model used in the study.
(3)and (4) above suggest that domain knowledge aboutcomplex biological phenomena (events) such asphenotype and experimental protocol may be help-ful for the triage task.47Together, points (1)?
(4) suggest that differentsections of a paper contain additional importantcontext-specific clues.
The example highlights thecomplex nature of the triage task facing the MGIbiocurators.
At present, this level of nuanced ?un-derstanding?
of content semantics is extremely hardfor machines to replicate.
Nonetheless, merely treat-ing the papers as a bag-of-words is unlikely to makenuanced distinctions between positive and negativeexamples with the level of precision and recall re-quired in MGI?s triage task.In this paper we therefore describe: (1) the designand performance of a classifier that is enriched withthree types of features, all derived from informa-tion extraction: (a) lexico-syntactic patterns, (b) pat-terns coupled with lexical extractions, and (c) pat-terns coupled with semantic extractions.
We com-pare the enriched classifier against classifiers thatuse only unigram and bigram features; (2) the de-sign of a biocuration application for MGI along withthe first prototype system where we emphasize theinfrastructure necessary to support the engineeringof domain-specific features of the kind describedin the examples above.
Our application is basedon Unstructured Information Management Architec-ture (UIMA) (Ferrucci and Lally, 2004), which isa pipeline-based framework for the development ofsoftware systems that analyze large volumes of un-structured information.2 Information Extraction for TriageClassificationIn this section, we present the information extractiontechniques that we used as the basis for our IE-basedfeatures, and we describe the three types of IE fea-tures that we incorporated into the triage classifier.2.1 Information Extraction TechniquesInformation extraction (IE) includes a variety oftechniques for extracting factual information fromtext.
We focus on pattern-based IE methodsthat were originally designed for event extrac-tion.
Event extraction systems identify the rolefillers associated with events.
For example, con-sider the task of extracting information from dis-ease outbreak reports, such as ProMed-mail arti-cles (http://www.promedmail.org/).
In contrast to anamed entity recognizer, which should identify allmentions of diseases and people, an event extractionsystem should only extract the diseases involved inan outbreak incident and the people who were thevictims.
Other mentions of diseases (e.g., in histori-cal discussions) or people (e.g., doctors or scientists)should be discarded.We utilized the Sundance/AutoSlog softwarepackage (Riloff and Phillips, 2004), which is freelyavailable for research.
Sundance is an informationextraction engine that applies lexico-syntactic pat-terns to extract noun phrases from specific linguisticcontexts.
Sundance performs its own syntactic anal-ysis, which includes morphological analysis, shal-low parsing, clause segmentation, and syntactic roleassignment (i.e., identifying subjects and direct ob-jects of verb phrases).
Sundance labels verb phraseswith respect to active/passive voice, which is im-portant for event role labelling.
For example, ?TomSmith was diagnosed with bird flu?
means that TomSmith is a victim, but ?Tom Smith diagnosed the el-derly man with bird flu?
means that the elderly manis the victim.Sundance?s information extraction engine can ap-ply lexico-syntactic patterns to extract noun phrasesthat participate in syntactic relations.
Each pat-tern represents a linguistic expression, and extractsa noun phrase (NP) argument from one of three syn-tactic positions: Subject, Direct Object, or Prepo-sitional Phrase.
Patterns may be defined manu-ally, or they can be generated by the AutoSlog pat-tern generator (Riloff, 1993), which automaticallygenerates patterns from a domain-specific text cor-pus.
AutoSlog uses 17 syntactic ?templates?
that arematched against the text.
Lexico-syntactic patternsare generated by instantiating the matching words inthe text with the syntactic template.
For example,five of AutoSlog?s syntactic templates are shown inTable 2:(a) <SUBJ> PassVP(b) PassVP Prep <NP>(c) <SUBJ> ActVP(d) ActVP Prep <NP>(e) Subject PassVP Prep <NP>Table 2: Five example syntactic templates (PassVPmeans passive voice verb phrase, ActVP means activevoice verb phrase)48Pattern (a) matches any verb phrase (VP) in a pas-sive voice construction and extracts the Subject ofthe VP.
Pattern (b) matches passive voice VPs thatare followed by a prepositional phrase.
The NPin the prepositional phrase is extracted.
Pattern (c)matches any active voice VP and extracts its Subject,while Pattern (d) matches active voice VPs followedby a prepositional phrase.
Pattern (e) is a more com-plex pattern that requires a specific Subject2, passivevoice VP, and a prepositional phrase.
We applied theAutoSlog pattern generator to our corpus (describedin Section 3.1) to exhaustively generate every pat-tern that occurs in the corpus.As an example, consider the following sentence,taken from an article in PLoS Genetics:USP14 is endogenously expressed inHEK293 cells and in kidney tissue derivedfrom wt mice.<SUBJ> PassVP(expressed)<SUBJ> ActVP(derived)PassVP(expressed) Prep(in) <NP>ActVP(derived) Prep(from) <NP>Subject(USP14) PassVP(expressed) Prep(in) <NP>Table 3: Lexico-syntactic patterns for the PLoS Geneticssentence shown above.AutoSlog generates five patterns from this sen-tence, which are shown in Table 3:The first pattern matches passive voice instancesof the verb ?expressed?, and the second patternmatches active voice instances of the verb ?de-rived?.3 These patterns rely on syntactic analysis,so they will match any syntactically appropriate con-struction.
For example, the first pattern would match?was expressed?, ?were expressed?, ?have been ex-pressed?
and ?was very clearly expressed?.
The thirdand fourth patterns represent the same two VPs butalso require the presence of a specific prepositionalphrase.
The prepositional phrase does not need tobe adjacent to the VP, so long as it is attached tothe VP syntactically.
The last pattern is very spe-cific and will only match passive voice instances of2Only the head nouns must match.3Actually, the second clause is in reduced passive voice (i.e.,tissue that was derived from mice), but the parser misidentifiesit as an active voice construction.?expressed?
that also have a Subject with a particularhead noun (?USP14?)
and an attached prepositionalphrase with the preposition ?in?.The example sentence contains four noun phrases,which are underlined.
When the patterns generatedby AutoSlog are applied to the sentence, they pro-duce the following NP extractions (shown in bold-face in Table 4):<USP14> PassVP(expressed)<kidney tissue> ActVP(derived)PassVP(expressed) Prep(in) <HEK293 cells>ActVP(derived) Prep(from) <wt mice>Subject(USP14) PassVP(expressed) Prep(in) <HEK293cells>Table 4: Noun phrase extractions produced by Sundancefor the sample sentence.In the next section, we explain how we use the in-formation extraction system to produce rich linguis-tic features for our triage classifier.2.2 IE Pattern FeaturesFor the triage classification task, we experimentedwith four types of IE-based features: Patterns, Lexi-cal Extractions, and Semantic Extractions.The Pattern features are the lexico-syntactic IEpatterns.
Intuitively, each pattern represents a phraseor expression that could potentially capture contextsassociated with mouse genomics better than isolatedwords (unigrams).
We ran the AutoSlog pattern gen-erator over the training set to exhaustively generateevery pattern that appeared in the corpus.
We thendefined one feature for each pattern and gave it abinary feature value (i.e., 1 if the pattern occurredanywhere in the document, 0 otherwise).We also created features that capture not just thepattern expression, but also its argument.
The Lex-ical Extraction features represent a pattern pairedwith the head noun of its extracted noun phrase.Table 5 shows the Lexical Extraction features thatwould be generated for the sample sentence shownearlier.
Our hypothesis was that these features couldhelp to distinguish between contexts where an activ-ity is relevant (or irrelevant) to MGI because of thecombination of an activity and its argument.The Lexical Extraction features are very specific,requiring the presence of multiple terms.
So we49PassVP(expressed), USP14ActVP(derived), tissuePassVP(expressed) Prep(in), cellsActVP(derived) Prep(from), miceSubject(USP14) PassVP(expressed) Prep(in), cellsTable 5: Lexical Extraction featuresalso experimented with generalizing the extractednouns by replacing them with a semantic category.To generate a semantic dictionary for the mouse ge-nomics domain, we used the Basilisk bootstrappingalgorithm (Thelen and Riloff, 2002).
Basilisk hasbeen used previously to create semantic lexicons forterrorist events (Thelen and Riloff, 2002) and senti-ment analysis (Riloff et al, 2003), and recent workhas shown good results for bioNLP domains usingsimilar bootstrapping algorithms (McIntosh, 2010;McIntosh and Curran, 2009).As input, Basilisk requires a domain-specific textcorpus (unannotated) and a handful of seed nounsfor each semantic category to be learned.
A boot-strapping algorithm then iteratively hypothesizes ad-ditional words that belong to each semantic cat-egory based on their association with the seedwords in pattern contexts.
The output is a lexiconof nouns paired with their corresponding semanticclass.
(e.g., liver : BODY PART).We used Basilisk to create a lexicon for eight se-mantic categories associated with mouse genomics:BIOLOGICAL PROCESS, BODY PART, CELL TYPE,CELLULAR LOCATION, BIOLOGICAL SUBSTANCE,EXPERIMENTAL REAGENT, RESEARCH SUBJECT,TUMOR.
To choose the seed nouns, we parsedthe training corpus, ranked all of the nouns by fre-quency4, and selected the 10 most frequent, unam-biguous nouns belonging to each semantic category.The seed words that we used for each semantic cat-egory are shown in Table 6.Finally, we defined Semantic Extraction featuresas a pair consisting of a pattern coupled with thesemantic category of the noun that it extracted.
Ifthe noun was not present in the semantic lexicons,then no feature was created.
The Basilisk-generatedlexicons are not perfect, so some entries will be in-correct.
But our hope was that replacing the lexicalterms with semantic categories might help the clas-4We only used nouns that occurred as the head of a NP.BIOLOGICAL PROCESS: expression, ac-tivity, activation, development, function,production, differentiation, regulation, re-duction, proliferationBODY PART: brain, muscle, thymus, cor-tex, retina, skin, spleen, heart, lung, pan-creasCELL TYPE: neurons, macrophages, thy-mocytes, splenocytes, fibroblasts, lym-phocytes, oocytes, monocytes, hepato-cytes, spermatocytesCELLULAR LOCATION: receptor, nu-clei, axons, chromosome, membrane, nu-cleus, chromatin, peroxisome, mitochon-dria, ciliaBIOLOGICAL SUBSTANCE: antibody,lysates, kinase, cytokines, peptide, anti-gen, insulin, ligands, peptides, enzymeEXPERIMENTAL REAGENT: buffer,primers, glucose, acid, nacl, water, saline,ethanol, reagents, paraffinRESEARCH SUBJECT: mice, embryos,animals, mouse, mutants, patients, litter-mates, females, males, individualsTUMOR: tumors, tumor, lymphomas,tumours, carcinomas, malignancies,melanoma, adenocarcinomas, gliomas,sarcomaTable 6: Seed words given to Basilisksifier learn more general associations.
For exam-ple, ?PassVP(expressed) Prep(in), CELLULAR LO-CATION?
will apply much more broadly than thecorresponding lexical extraction with just one spe-cific cellular location (e.g., ?mitochondria?
).Information extraction patterns and their argu-ments have been used for text classification in pre-vious work (Riloff and Lehnert, 1994; Riloff andLorenzen, 1999), but the patterns and argumentswere represented separately and the semantic fea-tures came from a hand-crafted dictionary.
In con-trast, our work couples each pattern with its ex-tracted argument as a single feature, uses an auto-matically generated semantic lexicon, and is the firstapplication of these techniques to the biocurationtriage task.503 Results3.1 Data SetFor our experiments in this paper we use articleswithin the PubMed Central (PMC) Open AccessSubset5.
From this subset we select all articles thatare published in journals of interest to biocuratorsat MGI.
This results in a total of 14,827 documentsout of which 981 have been selected manually byMGI biocurators as relevant (referred to as IN docu-ments).
This leaves 13,846 that are presumably outof scope (referred to as OUT documents), althoughit was not guaranteed that all of them had been man-ually reviewed so some relevant documents could beincluded as well.
(We plan eventually to present tothe biocurators those papers not included by thembut nonetheless selected by our tools as IN withhigh confidence, for possible reclassification.
Suchchanges will improve the system?s evaluated score.
)As preprocessing for the NLP tools, we splitthe input text into sentences using the Lin-gua::EN::Sentence perl package.
We trimmed non-alpha-numerics attached before and after words.We also removed stop words using the Lin-gua::EN::StopWords package.3.2 ClassifierWe used SVM Light6(Joachims, 1999) for all of ourexperiments.
We used a linear kernel and a tol-erance value of 0.1 for QP solver termination.
Inpreliminary experiments, we observed that the costfactor (C value) made a big difference in perfor-mance.
In SVMs, the cost factor represents theimportance of penalizing errors on the training in-stances in comparison to the complexity (general-ization) of the model.
We observed that higher val-ues of C produced increased recall, though at the ex-pense of some precision.
We used a tuning set toexperiment with different values of C, trying a widerange of powers of 2.
We found that C=1024 gen-erally produced the best balance of recall and preci-sion, so we used that value throughout our experi-ments.5http://www.ncbi.nlm.nih.gov/pmc/about/openftlist.html6http://svmlight.joachims.org/3.3 ExperimentsWe randomly partitioned our text corpus into 5 sub-sets of 2,965 documents each.7 We used the first 4subsets as the training set, and reserved the fifth sub-set as a blind test set.In preliminary experiments, we found that theclassifiers consistently benefitted from feature se-lection when we discarded low-frequency features.This helps to keep the classifier from overfitting tothe training data.
For each type of feature, we seta frequency threshold ?
and discarded any featuresthat occurred fewer than ?
times in the training set.We chose these ?
values empirically by performing4-fold cross-validation on the training set.
We eval-uated ?
values ranging from 1 to 50, and chose thevalue that produced the highest F score.
The ?
val-ues that were selected are: 7 for unigrams, 50 forbigrams, 35 for patterns, 50 for lexical extractions,and 5 for semantic extractions.Finally, we trained an SVM classifier on the en-tire training set and evaluated the classifier on thetest set.
We computed Precision (P), Recall (R), andthe F score, which is the harmonic mean of preci-sion and recall.
Precision and recall were equallyweighted, so this is sometimes called an F1 score.Table 7 shows the results obtained by using eachof the features in isolation.
The lexical extractionfeatures are shown as ?lexExts?
and the semantic ex-traction features are shown as ?semExts?.
We alsoexperimented with using a hybrid extraction fea-ture, ?hybridExts?, which replaced a lexical extrac-tion noun with its semantic category when one wasavailable but left the noun as the extraction termwhen no semantic category was known.Table 7 shows that the bigram features producedthe best Recall (65.87%) and F-Score (74.05%),while the hybrid extraction features produced thebest Precision (85.52%) but could not match the bi-grams in terms of recall.
This is not surprising be-cause the extraction features on their own are quitespecific, often requiring 3-4 words to match.Next, we experimented with adding the IE-basedfeatures to the bigram features to allow the classifierto choose among both feature sets and get the bestof both worlds.
Combining bigrams with IE-based7Our 5-way random split left 2 documents aside, which weignored for our experiments.51Feature P R Funigrams 79.75 60.58 68.85bigrams 84.57 65.87 74.05patterns 78.98 59.62 67.95lexExts 76.54 59.62 67.03semExts 72.39 46.63 56.73hybridExts 85.52 59.62 70.25bigrams + patterns 84.87 62.02 71.67bigrams + lexExts 85.28 66.83 74.93bigrams + semExts 85.43 62.02 71.87bigrams + hybridExts 87.10 64.90 74.38Table 7: Triage classifier performance using different setsof features.features did in fact yield the best results.
Using bi-grams and lexical extraction features achieved boththe highest recall (66.83%) and the highest F score(74.93%).
In terms of overall F score, we see a rela-tively modest gain of about 1% by adding the lexicalextraction features to the bigram features, which isprimarily due to the 1% gain in recall.However, precision is of paramount importancefor many applications because users don?t want towade through incorrect predictions.
So it is worthnoting that adding the hybrid extraction features tothe bigram features produced a 2.5% increase in pre-cision (84.57% ?
87.10%) with just a 1% drop inrecall.
This recall/precision trade-off is likely to beworthwhile for many real-world application settings,including biocuration.4 Biocuration Application for MGIDeveloping an application that supports MGI biocu-rators necessitates an application design that mini-mally alters existing curation workflows while main-taining high classification F-scores (intrinsic mea-sures) and speeding up the curation process (extrin-sic measures).
We seek improvements with respectto intrinsic measures by engineering context-specificfeatures and seek extrinsic evaluations by instru-menting the deployed triage application to record us-age statistics that serve as input to extrinsic evalua-tion measures.4.1 Software ArchitectureAs stated earlier, one of our major goals is to build,deploy, and extrinsically evaluate an NLP-assistedcuration application (Alex et al, 2008) for triage atMGI.
By definition, an extrinsic evaluation of ourtriage application requires its deployment and sub-sequent tuning to obtain optimal performance withrespect to extrinsic evaluation criteria.
We antici-pate that features, learning parameters, and trainingdata distributions may all need to be adjusted duringa tuning process.
Cognizant of these future needs,we have designed the SciKnowMine system so asto integrate the various components and algorithmsusing the UIMA infrastructure.
Figure 1 shows aschematic of SciKnowMine?s overall architecture.4.1.1 Building configurable & reusable UIMApipelinesThe experiments we have presented in this paperhave been conducted using third party implementa-tions of a variety of algorithms implemented on awide variety of platforms.
We use SVMLight totrain a triage classifier on features that were pro-duced by AutoSlog and Sundance on sentences iden-tified by the perl package Lingua::EN::Sentence.Each of these types of components has either beenreimplemented or wrapped as a component reusablein UIMA pipelines within the SciKnowMine in-frastructure.
We hope that building such a li-brary of reusable components will help galvanize theBioNLP community towards standardization of aninteroperable and open-access set of NLP compo-nents.
Such a standardization effort is likely to lowerthe barrier-of-entry for NLP researchers interested inapplying their algorithms to knowledge engineeringproblems in Biology (such as biocuration).4.1.2 Storage infrastructure for annotations &featuresAs we develop richer section-specific andcontext-specific features we anticipate the need forprovenance pertaining to classification decisions fora given paper.
We have therefore built an AnnotationStore and a Feature Store collectively referred to asthe Classification Metadata Store8 in Figure 1.
Fig-ure 1 also shows parallel pre-processing populatingthe annotation store.
We are working on develop-ing parallel UIMA pipelines that extract expensive(resource & time intensive) features (such as depen-8Our classification metadata store has been implemented us-ing Solr http://lucene.apache.org/solr/52dency parses).The annotation store holds featuresproduced by pre-processing pipelines.
The annota-tion store has been designed to support query-basedcomposition of feature sets specific to a classifica-tion run.
These feature sets can be asserted to thefeature store and reused later by any pipeline.
Thisdesign provides us with the flexibility necessary toexperiment with a wide variety of features and tuneour classifiers in response to feedback from biocura-tors.5 Discussions & ConclusionsIn this paper we have argued the need for richer se-mantic features for the MGI biocuration task.
Ourresults show that simple lexical and semantic fea-tures used to augment bigram features can yieldhigher classification performance with respect to in-trinsic metrics (such as F-Score).
It is noteworthythat using a hybrid of lexical and semantic featuresresults in the highest precision of 87%.In our motivating example, we have proposedthe need for sectional-zoning of articles and havedemonstrated that certain zones like the ?Materi-als and Methods?
section can contain contextualfeatures that might increase classification perfor-mance.
It is clear from the samples of MGI man-ual classification guidelines that biocurators do, infact, use zone-specific features in triage.
It there-fore seems likely that section specific feature ex-traction might result in better classification perfor-mance in the triage task.
Our preliminary analysis ofthe MGI biocuration guidelines suggests that exper-imental procedures described in the ?Materials andMethods?
seem to be a good source of triage clues.We therefore propose to investigate zone and contextspecific features and the explicit use of domain mod-els of experimental procedure as features for docu-ment triage.We have also identified infrastructure needs aris-ing within the construction of a biocuration applica-tion.
In response we have constructed preliminaryversions of metadata stores and UIMA pipelines tosupport MGI?s biocuration.
Our next step is to de-ploy a prototype assisted-curation application thatuses a classifier trained on the best performing fea-tures discussed in this paper.
This application willbe instrumented to record usage statistics for use inextrinsic evaluations (Alex et al, 2008).
We hopethat construction on such an application will alsoengender the creation of an open environment forNLP scientists to apply their algorithms to biomedi-cal corpora in addressing biomedical knowledge en-gineering challenges.6 AcknowledgementsThis research is funded by the U.S. National Sci-ence Foundation under grant #0849977 for theSciKnowMine project (http://sciknowmine.isi.edu/).
We wish to acknowledge Kevin Co-hen for helping us collect the seed terms for Basiliskand Karin Verspoor for discussions regarding featureengineering.References[Alex et al2008] Beatrice Alex, Claire Grover, BarryHaddow, Mijail Kabadjov, Ewan Klein, MichaelMatthews, Stuart Roebuck, Richard Tobin, and Xin-glong Wang.
2008.
Assisted curation: does text min-ing really help?
Pacific Symposium On Biocomputing,567:556?567.
[Bouatia-Naji et al2010] Nabila Bouatia-Naji, Ame?lieBonnefond, Devin A Baerenwald, Marion Marchand,Marco Bugliani, Piero Marchetti, Franc?ois Pattou,Richard L Printz, Brian P Flemming, Obi C Umu-nakwe, Nicholas L Conley, Martine Vaxillaire, OlivierLantieri, Beverley Balkau, Michel Marre, Claire Le?vy-Marchal, Paul Elliott, Marjo-Riitta Jarvelin, DavidMeyre, Christian Dina, James K Oeser, PhilippeFroguel, and Richard M O?Brien.
2010.
Genetic andfunctional assessment of the role of the rs13431652-A and rs573225-A alleles in the G6PC2 promoter thatare strongly associated with elevated fasting glucoselevels.
Diabetes, 59(10):2662?2671.
[Bourne and McEntyre2006] Philip E Bourne and Jo-hanna McEntyre.
2006.
Biocurators: Contributors tothe World of Science.
PLoS Computational Biology,2(10):1.
[Cohen and Hersh2006] Aaron M Cohen and William RHersh.
2006.
The TREC 2004 genomics track cate-gorization task: classifying full text biomedical docu-ments.
Journal of Biomedical Discovery and Collab-oration, 1:4.
[Ferrucci and Lally2004] D Ferrucci and A Lally.
2004.Building an example application with the UnstructuredInformation Management Architecture.
IBM SystemsJournal, 43(3):455?475.53CitationStoreFeatureStore(token trigrams,stemmed bigrams)DocumentStoreMGItrainingCorpusDigital LibraryParallelpre-processing pipelinesClassification Metadata StoreAnnotation Store(token, Sundancepatterns, parse trees,)Classifier trainingpipelinetrained triageclassificationmodelRanked Triage ResultsDigital LibraryMGI Biocuration ApplicationNewlypublishedpapersFigure 1: Design schematic of the MGI biocuration application.
The components of the application are: (A) DigitalLibrary composed of a citation store and document store.
(B) Pre-processing UIMA pipelines which are a mecha-nism to pre-extract standard features such as parse trees, tokenizations etc.
(C) Classification Metadata Store whichis composed of an Annotation Store for the pre-extracted standard features from (B), and a Feature Store to hold de-rived features constructed from the standard ones in the Annotation Store.
(D) Classifier training pipeline.
(E) MGIBiocuration Application.
[Hersh W2005] Yang J Bhupatiraju RT Roberts P M.Hearst M Hersh W, Cohen AM.
2005.
TREC 2005genomics track overview.
In The Fourteenth Text Re-trieval Conference.
[Joachims1999] Thorsten Joachims.
1999.
MakingLarge-Scale SVM Learning Practical.
Advances inKernel Methods Support Vector Learning, pages 169?184.
[McIntosh and Curran2009] T. McIntosh and J. Curran.2009.
Reducing Semantic Drift with Bagging andDistributional Similarity.
In Proceedings of the 47thAnnual Meeting of the Association for ComputationalLinguistics.
[McIntosh2010] Tara McIntosh.
2010.
Unsupervised dis-covery of negative categories in lexicon bootstrapping.In Proceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, number Oc-tober, pages 356?365.
Association for ComputationalLinguistics.
[Ramakrishnan et al2010] Cartic Ramakrishnan, WilliamA Baumgartner Jr, Judith A Blake, Gully A P CBurns, K Bretonnel Cohen, Harold Drabkin, JananEppig, Eduard Hovy, Chun-Nan Hsu, Lawrence EHunter, Tommy Ingulfsen, Hiroaki Rocky Onda,Sandeep Pokkunuri, Ellen Riloff, and Karin Verspoor.2010.
Building the Scientific Knowledge Mine ( Sci-KnowMine 1 ): a community- driven framework fortext mining tools in direct service to biocuration.
Inproceeding of Workshop ?New Challenges for NLPFrameworks?
collocated with The seventh interna-tional conference on Language Resources and Eval-uation (LREC) 2010.
[Riloff and Lehnert1994] E. Riloff and W. Lehnert.
1994.Information Extraction as a Basis for High-PrecisionText Classification.
ACM Transactions on Information54Systems, 12(3):296?333, July.
[Riloff and Lorenzen1999] E. Riloff and J. Lorenzen.1999.
Extraction-based text categorization: Generat-ing domain-specific role relationships automatically.In Tomek Strzalkowski, editor, Natural Language In-formation Retrieval.
Kluwer Academic Publishers.
[Riloff and Phillips2004] E. Riloff and W. Phillips.
2004.An Introduction to the Sundance and AutoSlog Sys-tems.
Technical Report UUCS-04-015, School ofComputing, University of Utah.
[Riloff et al2003] E. Riloff, J. Wiebe, and T. Wilson.2003.
Learning Subjective Nouns using ExtractionPattern Bootstrapping.
In Proceedings of the SeventhConference on Natural Language Learning (CoNLL-2003), pages 25?32.
[Riloff1993] E. Riloff.
1993.
Automatically Construct-ing a Dictionary for Information Extraction Tasks.
InProceedings of the 11th National Conference on Arti-ficial Intelligence.
[Sjo?gren et al2009] Klara Sjo?gren, Marie Lagerquist,Sofia Moverare-Skrtic, Niklas Andersson, Sara HWindahl, Charlotte Swanson, Subburaman Mohan,Matti Poutanen, and Claes Ohlsson.
2009.
Elevatedaromatase expression in osteoblasts leads to increasedbone mass without systemic adverse effects.
Journalof bone and mineral research the official journal ofthe American Society for Bone and Mineral Research,24(7):1263?1270.
[Thelen and Riloff2002] M. Thelen and E. Riloff.
2002.A Bootstrapping Method for Learning Semantic Lexi-cons Using Extraction Pa ttern Contexts.
In Proceed-ings of the 2002 Conference on Empirical Methods inNatural Language Processing, pages 214?221.55
