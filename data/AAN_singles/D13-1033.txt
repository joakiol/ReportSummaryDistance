Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 333?344,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsThe Effects of Syntactic Features in Automatic Prediction of MorphologyWolfgang Seeker and Jonas KuhnInstitute for Natural Language ProcessingUniversity of Stuttgart{seeker,jonas}@ims.uni-stuttgart.deAbstractMorphology and syntax interact considerablyin many languages and language processingshould pay attention to these interdependen-cies.
We analyze the effect of syntactic fea-tures when used in automatic morphology pre-diction on four typologically different lan-guages.
We show that predicting morphologyfor languages with highly ambiguous wordforms profits from taking the syntactic contextof words into account and results in state-of-the-art models.1 IntroductionIn this paper, we investigate the interplay betweensyntax and morphology with respect to the task ofassigning morphological descriptions (or tags) toeach token of a sentence.
Specifically, we examinethe effect of syntactic information when it is inte-grated into the feature model of a morphological tag-ger.
We test the effect of syntactic features on fourlanguages ?
Czech, German, Hungarian, and Span-ish ?
and find that syntactic features improve our tag-ger considerably for Czech and German, but not forHungarian and Spanish.
Our analysis of construc-tions that show morpho-syntactic agreement sug-gests that syntactic features are important if the lan-guage shows frequent word form syncretisms1 thatcan be disambiguated by the syntactic context.The meaning of a sentence is structurally encoded1Syncretism describes the situation where a word form isambiguous between several different morphological descrip-tions within its inflection paradigm.by morphological and syntactic means.2 Differentlanguages, however, use them to a different extent.Languages like English encode grammatical infor-mation (like the subject vs object status of an argu-ment) via word order, whereas languages like Czechor Hungarian use different word forms.
Automaticanalysis of languages with rich morphology needsto pay attention to the interaction between morphol-ogy and syntax in order to arrive at suitable com-putational models.
Linguistic theory (e. g., Bresnan(2001), Melc?uk (2009)) suggests many interactionsbetween morphology and syntax.
For example, lan-guages with a case system use different forms of thesame word to mark different syntactic (or seman-tic) relations (Blake, 2001).
In many languages, twowords that participate in a syntactic relation showcovariance in some or all of their morphological fea-tures (so-called agreement, Corbett (2006)).3Automatic annotation of morphology assignsmorphological descriptions (e. g., nominative-singular-masculine) to word forms.
It is usuallymodeled as a sequence model, often in combinationwith part-of-speech tagging and lemmatization(Collins, 2002; Hajic?, 2004; Smith et al 2005;Chrupa?a et al 2008, and others).
Sequence modelsachieve high accuracy and coverage but since theyonly use linear context they only approximate someof the underlying hierarchical relationships.
Asan example for these hierarchical relationships,2And also by prosodic means, which we will not discusssince text-based tools rarely have access to this information.3For example, in English, the subject of a sentence and thefinite verb agree with respect to their number and person fea-ture.333die wirtschaftlich am weitesten entwickelten , modernen und zum Teil katholisch gepra?gten Regionennom/acc.pl.fem nom/acc.pl.femthe economic - most developed , modern and to part catholic influenced regionsNKMOPM MONKCJCDMONK MOCJ?the regions that are economically most developed, modern, and partly catholic?Figure 1: Example of a German noun phrase.
First and last word agree in number, gender, and case value.Figure 1 shows a German noun phrase taken fromthe German TiGer corpus (Brants et al 2002).The two bold-faced words are the determiner andthe head noun of the phrase, and they agree intheir gender, number, and case values.
The wordRegionen (regions) is four-way ambiguous for itscase value, which is reduced to a two-way ambi-guity between nominative and accusative by thedeterminer.
Further disambiguation would requireinformation about the syntactic role of the nounphrase in a sentence.
There are 11 tokens betweenthese two words, which would require a contextwindow of at least 13 to capture the agreementrelation within a sequence model.
Syntactically,however, as indicated by the dependency tree,the determiner and the head are linked directly.The interdependency between morphology andsyntax in the example thus manifests itself in themorphological disambiguation of a highly syncreticword form because of its government or agreementrelation to its respective syntactic head/dependents.Of course, the sequence model is most of thetime a reasonable approximation, because the ma-jority of noun phrases in the TiGer corpus are notas long as the example in Figure 1.4 Furthermore,not all languages show this kind of relationship be-tween morphological forms and syntactic relation asdemonstrated for German.
But taking advantage ofthe morphosyntactic dependencies in a language cangive us better models that may even be capable ofhandling the more difficult or rare cases.
We there-fore advocate that models for predicting morphologyshould be designed with the typological characteris-tics of a language and its morphosyntactic propertiesin mind, and should, where appropriate, integrate4We find 57,551 noun phrases with less than three tokensbetween determiner and noun and 4,670 with three or more.syntactic information in order to better model themorphosyntactic interdependencies of the language.In the remainder of the paper, we show empiri-cally that taking syntactic information into accountproduces state-of-the-art models for languages witha high interdependency between morphology andsyntax.
We use a simple setup, where we combinea morphological tagger and a dependency parser ina bootstrapping architecture in order to analyze theeffect of syntactic information on the performanceof the morphological tagger (Section 2).
Using syn-tactic features in morphology prediction requires asyntactically annotated corpus for training a statisti-cal parser, which may not be available for languageswith few resources.
We show in Section 3 that onlyvery little syntactically annotated data is required toachieve the improvements.
We furthermore expectthat the improved morphological information alsoimproves parsing performance and present a prelim-inary experiment in Section 4.2 ExperimentsIn this section, we present a series of experimentsthat investigate the effect of syntactic information onthe prediction of morphological features.
We startby describing our data sets and the system that weused for the experiments.2.1 Languages and Data SetsWe test our hypotheses on four different languages:Czech, German, Hungarian, and Spanish.Spanish, a Romance language, and German, aGermanic language, constitute inflecting languagesthat show verbal and nominal morphology, but notas sophisticated as Czech and Hungarian.
As wewill see in the experiments, it is relatively easy to334predict the morphological information annotated inthe Spanish data set.Czech and Hungarian represent languages withvery rich morphological systems both in verbal andnominal morphological paradigms.
They differ sig-nificantly in the way in which morphological infor-mation is encoded in word forms.
Czech, a Slaviclanguage, is an inflecting language, where one suf-fix may signal several different morphological cate-gories simultaneously (e. g., number, gender, case).In contrast, Hungarian, a Finno-Ugric language, isof the agglutinating type, where each morphologicalcategory is marked by its own morpheme.Both German and Czech show various form syn-cretisms in their inflection paradigms.
Form syn-cretisms emerge when the same word form is am-biguous between several different morphological de-scriptions, and they are a major challenge to auto-matic morphological analysis.
Spanish shows syn-cretism in the verbal inflection paradigms.
In Hun-garian, form syncretisms are much less frequent.The case paradigm of Hungarian only shows oneform syncretism between dative and genitive case(out of about 18 case suffixes).All languages show agreement between subjectand verb, and within the noun phrase.
The word or-der in Czech and Hungarian is very variable whereasit is more restrictive in Spanish and German.As our data, we use the CoNLL 2009 SharedTask data sets (Hajic?
et al 2009) for Czech andSpanish.
For German, we use the dependencyconversion of the TiGer treebank by Seeker andKuhn (2012), splitting it into 40k/5k/5k sentencesfor training/development/test.
For Hungarian, weuse the Szeged Dependency Treebank (Vincze et al2010), with the split of Farkas et al(2012).2.2 System DescriptionTo test our hypotheses, we implemented a taggerthat assigns full morphological descriptions to eachtoken in a sentence.
The system was inspired by themorphological tagger included in mate-tools.5 Likethe tagger provided with mate-tools, it is a classifierthat tags each token using the surrounding tokens in5A collection of language independent, data-driven analysistools for lemmatization, pos-tagging, morphological analysis,and dependency parsing: http://code.google.com/p/mate-toolsits feature model.
Models are trained using passive-aggressive online training (Crammer et al 2003).The system makes two passes over each sentence:The first pass provides predicted tags that are usedas features during the second pass.
We also adoptedthe idea of a tag filter, which deterministically as-signs tags for words that always occur with the sametag in the training data.For all matters of syntactic annotation in this pa-per, we use the graph-based dependency parser byBohnet (2010), also included in mate-tools.
All datasets are annotated with gold syntactic information,which is used to train the parsing models.For our experiments, we use a bootstrapping ap-proach: the parser uses the output of the morphologyin its feature set, and the morphological tagger wewant to analyze uses the output of the parser as syn-tactic features.
Since it is best to keep the trainingsetting as similar as possible to the test setting, weuse 10-fold jackknifing to annotate our training datawith predicted morphology or syntax respectively.Jackknifing differs from cross-validation only inits purpose.
Cross-validation is used for evaluatingdata, jackknifing is used to annotate data.
The dataset is split into n parts, and n-1 parts are used to traina model for annotating the nth part.
This is thenrotated n times such that each part is annotated bythe automatic tool without training it on its own testdata.
Jackknifing is important for creating a realis-tic training scenario that provides automatic prepro-cessing.
For annotating development and test sets,models are trained on the jackknifed training set.2.3 The Effects of Syntactic FeaturesIn the first experiment, we use the system describedin Section 2.2 to predict morphological informationon all four languages.
We start with describing thegeneral setup and the feature set, and continue witha discussion of the results.The experimental setup is as follows: the Germanand Spanish data sets are annotated with lemma andpart-of-speech information using 10-fold jackknif-ing.
The annotation is done with mate-tools?
lem-matizer and pos-tagger.
For Czech and Hungarian,we keep the annotation provided with the data sets.Note that our experimental setup does not includelemmas or part-of-speech tags as part of the predic-tion of the morphology but annotates them in a pre-335processing step.
It is not necessary to separate part-of-speech and lemma from the prediction of mor-phology and, in fact, many systems perform thesesteps simultaneously (e. g. Spoustova?
et al(2009)).Doing morphology prediction as a separate step al-lows us to use lemma and part-of-speech informa-tion in the feature set.6static featuresform form1b form2bform3b form1a lemma2apos1b pos2b pos1aform+pos pos+s1 pos+s2pos+s3 pos+s4 lemma+p2lemma+p3 pos+number form+form1bpos+pos1a pos+pos1b+pos2b s1+s1 1bs1+s1 1a s2+s2 1a last-verb-lemmalast-verb-pos next-verb-lemma next-verb-posdynamic featurestag1b+tag2b tag2b+tag3b tag1atag1a+tag1b tag1a+tag2a tag2a+tag3apos1b+case1b last-verb-tag next-verb-tagpos1b+case1b+pos2b+case2bHungarian only featurespos+uppercaseCzech only featurespos+p2Spanish only featuress5 p1 p4p5 s2 1a s3 1as4 1aTable 1: Baseline feature set.
form means word form,lemma is lemma, pos is part-of-speech, s1/p1 stand forsuffix and prefix of length 1 (characters), tag is the mor-phological tag predicted by the system, 1b/1a means 1token before/after the current token, and + marks featureconjunctions.
number marks if the form contains a digit.After preprocessing the data, our baseline systemis trained using the feature set shown in Table 1.
Thebaseline system does not make use of any syntacticinformation but predicts morphological informationbased solely on tokens and their linear context.
Thefeatures are divided into static features, which can becomputed on the input, and dynamic features, whichare computed also on previous output of the system(cf.
two passes in Section 2.2).6Lemma and part-of-speech prediction may also profit fromsyntactic information, see e.g.
Prins (2004) or Bohnet and Nivre(2012).The feature sets in Table 1 were developed specif-ically for our experiments and are the result of anautomatic forward/backward feature selection pro-cess.
The purpose of the feature selection was to ar-rive at a baseline system that performs well withoutany syntactic information.
With such an optimizedbaseline system, we can measure the contribution ofsyntactic features more reliably.The last-verb/next-verb and pos+case features arevariants of the features proposed in Votrubec (2006).They extract information about the first verb withinthe last 10/the next 30 tokens in the sentence.
Thecase feature extracts the case value from previouslyassigned morphological tags.
Note that the verbfeatures are approximating syntactic information bymaking the assumption that the closest verbs arelikely to be syntactic heads for many words.static featuresh lemma h s2 h s3 pos+h pos s1+h s1h dir h dir+h posld s1 ld s2 ld p1 ld p4dynamic featuresh tag ld tagTable 2: Syntactic features.
h and ld mark features fromthe head and the left-most daughter, dir is a binary fea-ture marking the direction of the head with respect to thecurrent token.After training the baseline models, we use them toannotate the whole data set with morphological in-formation (using 10-fold jackknifing for the trainingportions).
We then use 10-fold jackknifing again toannotate the data sets with the dependency parser.At this point, all our data sets are annotated withpredicted morphology from our baseline system andwith syntactic information from the parser, whichuses the morphological information from our base-line system in its feature set.
We can now retrain ourmorphological tagger using features that are derivedfrom the dependency trees provided by the parser.Note that this is not a stacking architecture, sincethe second system does not use the predicted mor-phology output from the baseline system.
The loopsimply ensures that we get the best possible syntac-tic features.We extract two kinds of syntactic features: fea-tures of the syntactic head of the current token, and336dev set test setall oov all oovCzechmorfette 90.37 68.66 90.01 67.25our baseline 92.51 73.12 92.29 72.58pred syntax *93.18 74.04 *92.82 73.11gold syntax *93.64 75.20 *93.30 74.96Germanmorfette 86.78 66.37 84.58 61.05our baseline 90.92 72.52 89.11 69.67pred syntax *92.07 75.06 *90.10 71.18gold syntax *92.70 *76.29 *90.87 *73.20Hungarianmorfette *96.19 *85.82 95.99 *85.43our baseline 96.08 84.49 95.94 83.76pred syntax 96.18 84.70 96.11 83.85gold syntax *96.46 85.30 *96.35 84.50Spanishmorfette 97.83 89.67 97.76 91.00our baseline 97.83 89.05 97.59 90.88pred syntax 97.84 89.08 97.67 90.91gold syntax 98.11 90.34 97.88 91.61Table 3: The effect of syntactic features when predictingmorphological information.
* mark statistically signifi-cantly better models compared to our baseline (sentence-based t-test with ?
= 0.05).features of the left-most daughter of the current to-ken.
We also experimented with other types, e. g.the right-most daughter, but these features did notimprove the model.
This is likely due to the waythese languages encode morphological informationand may be different for other languages.
From thehead and the left-most daughter, we construct fea-tures about form, lemma, affixes, and tags.
Table 2lists the syntactic features that we use in the model.With the syntactic features available due to theparsing step, we train new models with the full sys-tem.
For each language, we run four experiments.The first two are baseline experiments, where weuse the off-the-shelf morphological tagger morfette(Chrupa?a et al 2008) and our own baseline sys-tem, both of which do not use any syntactic features.In the third experiment, we evaluate our full systemusing the syntactic features provided by the depen-dency parser.
As an oracle experiment, we also re-port results on the full system when using the goldstandard syntax from the treebank.
Table 3 presentsall results in terms of accuracy on all tokens (all)dev set test setall oov all oovCzechfeaturama 94.75 84.12 94.78 84.23our baseline 93.80 80.47 93.57 80.53pred syntax *94.40 81.51 *94.24 81.61gold syntax *94.80 82.45 *94.64 82.80GermanRFTagger 90.63 72.11 89.04 70.80our baseline 92.59 80.73 91.48 78.83pred syntax *93.70 82.71 *92.51 80.20gold syntax *94.28 *84.12 *93.32 *82.35Hungarianour baseline 97.27 92.61 97.03 91.28pred syntax 97.38 92.39 97.19 91.50gold syntax *97.63 92.79 *97.45 91.92Spanishour baseline 98.23 92.46 98.02 93.15pred syntax 98.24 92.30 98.07 93.03gold syntax 98.40 92.82 *98.22 93.64Table 4: The effect of syntactic features when predictingmorphology using lexicons.
* mark statistically signifi-cantly better models compared to our baseline (sentence-based t-test with ?
= 0.05).and out-of-vocabulary tokens only (oov).
Out-of-vocabulary tokens do not occur in the training data.We find trends along several axes: Generally, thesyntactic features work well for Czech and Ger-man, whereas for Hungarian and Spanish, they donot yield any significant improvement.
The im-provements for German and Czech are between 0.5(Czech) and 1.0 (German) percentage points abso-lute in token accuracy, and between 0.2 (Czech testset) and 2.5 (German dev set) percentage points ab-solute in accuracy of unknown words.
There are noobvious differences between the development andthe test set in any of the languages.Compared to the morfette baseline, we find oursystems to be either superior or equal to morfette interms of token accuracy.
Regarding accuracy on un-known words, morfette outperforms our systems forHungarian, but is outperformed on Czech and Ger-man.
For Spanish, all systems yield similar results.Looking at the oracle experiment, we see that forall languages, the system can learn something fromsyntax.
For Czech and German, this is clearly the337case, for Hungarian and Spanish, the differences aresmall but visible.
There are pronounced differencesbetween the predicted and the gold syntax experi-ments in Czech and German.
Clearly, the parsermakes mistakes that propagate through to the pre-diction of the morphology.2.4 Syntax vs LexiconThe current state-of-the-art in predicting morpho-logical features makes use of morphological lexi-cons (e.g.
Hajic?
(2000), Hakkani-Tu?r et al(2002),Hajic?
(2004)).
Lexicons define the possible morpho-logical descriptions of a word and a statistical modelselects the most probable one among them.
In thefollowing experiment, we test whether the contribu-tion of syntactic features is similar or different to thecontribution of morphological lexicons.Lexicons encode important knowledge that is dif-ficult to pick up in a purely statistical system, e. g.the gender of nouns, which often cannot be deducedfrom the word form (Corbett, 1991).7We extend our system from the previous experi-ment to include information from a morphologicaldictionaries.
For Czech, we use the morphologi-cal analyzer distributed with the Prague DependencyTreebank 2 (Hajic?
et al 2006).
For German, weuse DMor (Schiller, 1994).
For Hungarian, we use(Tro?n et al 2006), and for Spanish, we use the mor-phological analyzer included in Freeling (Carreras etal., 2004).
The output of the analyzers is given to thesystem as features that simply record the presence ofa particular morphological analysis for the currentword.
The system can thus use the output of anytool regardless of its annotation scheme, especiallyif the annotation scheme of the treebank is differentfrom the one of the morphological analyzer.Table 4 presents the results of experiments wherewe add the output of the morphological analyzersto our system.
Again, we run experiments with andwithout syntactic features.
For Czech, we also showresults from featurama8 with the feature set devel-oped by Votrubec (2006).
For German, we show re-sults for RFTagger (Schmid and Laws, 2008).As expected, the information from the morpho-logical lexicon improves the overall performance7Lexicons are also often used to speed up processing con-siderably by restricting the search space of the statistical model.8http://sourceforge.net/projects/featurama/considerably compared to the results in Table 3, es-pecially on unknown tokens.
This shows that evenwith the considerable amounts of training data avail-able nowadays, rule-based morphological analyzersare important resources for morphological descrip-tion (cf.
Hajic?
(2000)).
The contribution of syn-tactic features in German and Czech is almost thesame as in the previous experiment, indicating thatthe syntactic features contribute information that isorthogonal to that of the morphological lexicon.
Thelexicon provides lexical knowledge about a wordform, while the syntactic features provide the syn-tactic context that is needed in German and Czechto decide on the right morphological tag.2.5 Language DifferencesFrom the previous experiments, we conclude thatsyntactic features help in the prediction of morphol-ogy for Czech and German, but not for Hungarianand Spanish.
To further investigate the differencebetween Czech and German on the one hand, andHungarian and Spanish on the other, we take a closerlook at the output of the tagger.We find an interesting difference between thetwo pairs of languages, namely the performancewith respect to agreement.
Agreement is a phe-nomenon where morphology and syntax strongly in-teract.
Morphological features co-vary between twoitems in the sentence, but the relation between theseitems can occur at various linguistic levels (Corbett,2006).
If the syntactic information helps with pre-dicting morphological information, we expect thisto be particularly helpful with getting agreementright.
All languages show agreement to some ex-tent.
Specifically, all languages show agreement innumber (and person) between the subject and theverb of a clause.
Czech, German, and Spanish showagreement in number, gender, and case (not Span-ish) within a noun phrase.
Hungarian shows caseagreement within the noun phrase only rarely, e.g.for attributively used demonstrative pronouns.In order to test the effect on agreement, we mea-sure the accuracy on tokens that are in an agreementrelation with their syntactic head.
We counted sub-ject verb agreement as well as agreement with re-spect to number, gender, and case (where applicable)between a noun and its dependent adjective and de-terminer.
Table 5 displays the counts from the devel-338opment sets of each language.
We compare the base-line system that does not use any syntactic informa-tion with the output of the morphological tagger thatuses the gold syntax.
We use the gold syntax ratherthan the predicted one in order to eliminate any in-fluence from parsing errors.
As can be seen from theresults, the level of agreement relations in Czech andGerman improves when using syntactic information,whereas in Spanish and Hungarian, only very tinychanges occur.agreement baseline gold syntaxCzechsbj-verb 3199/4044 = 79.10 3264/4044 = 80.71NP case 8719/9132 = 95.48 8821/9132 = 96.59NP num 8933/9132 = 97.82 9016/9132 = 98.73NP gen 8493/9132 = 93.00 8768/9132 = 96.01Germansbj-verb 4412/4696 = 93.95 4562/4696 = 97.15NP case 13340/13951 = 95.62 13510/13951 = 96.84NP num 13631/13951 = 97.71 13788/13951 = 98.83NP gen 13253/13951 = 95.00 13528/13951 = 96.97Hungariansbj-verb 8653/10219 = 84.68 8655/10219 = 84.70NP case 402/891 = 45.12 412/891 = 46.24Spanishsbj-verb 1930/2004 = 96.31 1932/2004 = 96.41NP num 8810/8849 = 99.56 8816/8849 = 99.63NP gen 8810/8849 = 99.56 8821/8849 = 99.68Table 5: Agreement counts in morphological annotationcompared between the baseline system and the oraclesystem using gold syntax.For Czech and German, these results sugguestthat syntactic information helps with agreement.
Webelieve that the reasons why it does not help forHungarian and Spanish are the following: for Span-ish, we see that also the baseline model achievesvery high accuracies (cf.
Table 3) and also high ratesof correct agreement.
It seems that for Spanish, syn-tactic context is simply not necessary to make thecorrect prediction.
For Hungarian, the reason lieswithin the inflectional paradigms of the language,which do not show any form syncretism, mean-ing that word forms in Hungarian are usually notambiguous within one morphological category (e.g.case).
Making a morphological tag prediction, how-ever, is difficult only if the word form itself is am-biguous between several morphological tags.
In thiscase, using the agreement relation between the wordand its syntactic head can help the system makingthe proper prediction.
This is the situation that wefind in Czech and German, where form syncretismis pervasive in the inflectional paradigms.2.6 Syntactic Features in CzechIn Section 2.4 we compared the performance of oursystem on Czech to another system, featurama (seeTable 4).
Featurama outperforms our baseline sys-tem by a percentage point in token accuracy (andeven more for unknown tokens).
Syntactic informa-tion closes that gap to a large extent but only usinggold syntax gets our system on a par with featurama.The question then arises whether the syntacticfeatures actually contribute something new to thetask, or whether the same effect could also beachieved with linear context features alone as in fea-turama.
In order to test this we run an additionalexperiment, where we add some of the syntax fea-tures to the feature set of featurama.
Specifically,we add the static features from Table 2 that do notuse lemma or part-of-speech information.
Due to theway featurama works, we cannot use features fromthe morphological tags (the dynamic features).The results in Table 6 show that also featuramaprofits from syntactic features, which corroboratesthe findings from the previous experiments.
We alsonote again that better syntax would improve resultseven more.dev set test setall oov all oovfeaturama 94.75 84.12 94.78 84.23pred syntax 95.18 84.65 95.09 84.52gold syntax *95.39 84.62 *95.34 85.03Table 6: Syntactic features for featurama (Czech).
* markstatistically significantly better models compared to feat-urama (sentence-based t-test with ?
= 0.05).3 How Much Syntax is Needed?Syntactic features require syntactically annotatedcorpora.
Without a treebank to train the parser, themorphology cannot profit from syntactic features.9This may be problematic for languages for whichthere is no treebank, because creating a treebank isexpensive.
Fortunately, it turns out that very smallamounts of syntactically annotated data are enough9Which is of course only a problem for statistical parsers.339German Czech888990919293940  5000  10000  15000  20000  25000  30000  35000  40000accuracyof morphology# of sentences in training data of syntactic parserdevtest888990919293940  5000  10000  15000  20000  25000  30000  35000  40000accuracyof morphology# of sentences in training data of syntactic parserdevtestFigure 2: Dependency between amount of training data for syntactic parser and quality of morphological prediction.to provide a parsing quality that is sufficient for themorphological tagger.In order to test what amount of training data isneeded, we train several parsing models on increas-ing amounts of syntactically annotated data.
For ex-ample, the first experiment uses the first 1,000 sen-tences of the treebank.
We perform 5-fold jackknif-ing with the parser on these sentences to annotatethem with syntax.
Then we train one parsing modelon these 1,000 sentences and use it to annotate therest of the training data as well as the developmentand the test set.
This gives us the full data set an-notated with syntax that was learned from the first1,000 sentences of the treebank.
The morphologi-cal tagger is then trained on the full training set andapplied to development and test set.Figure 2 shows the dependency between theamount of training data given to the parser and thequality of the morphological tagger using syntac-tic features provided by this parser.
The left-mostpoint corresponds to a model that does not use syn-tactic information.
For both languages, Germanand Czech, we find that already 1,000 sentences areenough training data for the parser to provide usefulsyntactic information to the morphological tagger.After 5,000 sentences, both curves flatten out andstay on the same level.
We conclude that using syn-tactic features for morphological prediction is viableeven if there is only small amounts of syntactic dataavailable to train the parser.As a related experiment, we also test if we can getthe same effect with a very simple and thus muchfaster parser.
We use the brute-force algorithm de-scribed in Covington (2001), which selects for eachtoken in the sentence another token as the head.
Itdoes not have any tree requirements, so it is not evenguaranteed to yield a cycle-free tree structure.
In Ta-ble 7, we compare the simple parser with the mate-parser, both trained on the first 5,000 sentences ofthe treebank.
Evaluation is done in terms of labeled(LAS) and unlabeled attachment score (UAS).10dev set test setLAS UAS LAS UASCzechsimple parser (5k) 71.57 78.96 69.09 77.23full parser (5k) 76.77 84.38 74.70 83.00Germansimple parser (5k) 83.06 85.23 78.56 81.18full parser (5k) 87.56 90.08 83.69 86.58Table 7: Simple parser vs full parser ?
syntactic quality.Trained on first 5,000 sentences of the training set.As expected, the simple parser performs muchworse in terms of syntactic quality.
Table 8 showsthe performance of the morphological tagger whenusing the output of both parsers as syntactic fea-tures.
For Czech, both parsers seem to supply sim-ilar information to the morphological tagger, whilefor German, using the full parser is clearly better.In both cases, the morphological tagger outperformsthe models that do not use syntactic information (cf.Table 3).
The performance on unknown words ishowever much worse for both languages.
We con-clude that even with a simple parser and little train-ing data, the morphology can make use of syntacticinformation to some extent.10LAS: correct edges with correct labelsall edges , UAS:correct edgesall edges340dev set test setall oov all oovCzechno syntax 92.51 73.12 92.29 72.58simple syntax 92.96 73.45 92.53 72.66full syntax 93.08 73.64 92.69 73.39Germanno syntax 90.92 72.52 89.11 69.67simple syntax 91.52 73.34 89.66 70.52full syntax 91.92 83.46 89.91 80.50Table 8: Simple parser vs full parser ?
morphologicalquality.
The parsing models were trained on the first5,000 sentences of the training data, the morphologicaltagger was trained on the full training set.4 Does Better Morphology lead to BetterParses?In the previous sections, we show that syntactic in-formation improves a model for predicting morphol-ogy for Czech and German, where syntax and mor-phology interact considerably.
A natural questionthen is whether the improvement also occurs in theother direction, namely whether the improved mor-phology also leads to better parsing models.In the previous experiments, we run a 10-foldjackknifing process to annotate the training data withmorphological information using no syntactic fea-tures and afterwards use jackknifing with the parserto annotate syntax.
The syntax is subsequently usedas features for our predicted-syntax experiments.We can apply the same process once more with themorphology prediction in order to annotate the train-ing data with morphological information that is pre-dicted using the syntactic features.
A parser trainedon this data will then use the improved morphologyas features.
If the improved morphology has an im-pact on the parser, the quality of the second parsingmodel should then be superior to the first parsingmodel, which uses the morphology predicted with-out syntactic information.
Note that for the follow-ing experiments, neither morphology model uses themorphological lexicon.Table 9 presents the evaluation of the two pars-ing models (one using morphology without syntacticfeatures, the other one using the improved morphol-ogy).
The results show no improvement in parsingperformance when using the improved morphology.Looking closer at the output, we find differences be-dev set test setLAS UAS LAS UASCzechbaseline morph 81.73 88.45 81.02 87.77morph w/ syntax 81.63 88.37 80.83 87.61Germanbaseline morph 91.16 92.97 88.06 90.24morph w/ syntax 91.20 92.97 88.15 90.34Table 9: Impact of the improved morphology on the qual-ity of the dependency parser for Czech and German.tween the two parsing models with respect to gram-matical functions that are morphologically marked.For example, in German, performance on subjectsand accusative objects improves while performancefor dative objects and genitives decreases.
This sug-gests different strengths in the two parsing models.However, the question how to make use of the im-proved morphology in parsing clearly needs moreresearch in the future.
A promising avenue may bethe approach by Hohensee and Bender (2012).5 Related WorkMorphological taggers have been developed formany languages.
The most common approach is thecombination of a morphological lexicon with a sta-tistical disambiguation model (Hakkani-Tu?r et al2002; Hajic?, 2004; Smith et al 2005; Spoustova?et al 2009; Zsibrita et al 2013).Our work has been inspired by Versley et al(2010), who annotate a treebank with morphologi-cal information after the syntax had been annotatedalready.
The system used a finite-state morphologyto propose a set of candidate tags for each word,which is then further restricted using hand-craftedrules over the already available syntax tree.Lee et al(2011) pursue the idea of jointly predict-ing syntax and morphology, out of the motivationthat joint models should model the problem morefaithfully.
They demonstrate that both sides can useinformation from each other.
However, their modelis computationally quite demanding and its overallperformance falls far behind the standard pipelineapproach where both tasks are done in sequence.The problem of modeling the interaction betweenmorphology and syntax has recently attracted someattention in the SPMRL workshops (Tsarfaty et al3412010).
Modeling morphosyntactic relations explic-itly has been shown to improve statistical parsingmodels (Tsarfaty and Sima?an, 2010; Goldberg andElhadad, 2010; Seeker and Kuhn, 2013), but the co-dependency between morphology and syntax makesit a difficult problem, and linguistic intuition is oftencontradicted by the empirical findings.
For example,Marton et al(2013) show that case information isthe most helpful morphological feature for parsingArabic, but only if it is given as gold information,whereas using case information from an automaticsystem may even harm the performance.Morphologically rich languages pose differentchallenges for automatic systems.
In this paper, wework with European languages, where the problemof predicting morphology can be reduced to a tag-ging problem.
In languages like Arabic, Hebrew,or Turkish, widespread ambiguity in segmentationof single words into meaningful morphemes adds anadditional complexity.
Given a good segmentationtool that takes care of this, our approach is appli-cable to these languages as well.
For Hebrew, thisproblem has also been addressed by jointly mod-eling segmentation, morphological prediction, andsyntax (Cohen and Smith, 2007; Goldberg and Tsar-faty, 2008; Goldberg and Elhadad, 2013).6 ConclusionIn this paper, we have demonstrated that using syn-tactic information for predicting morphological in-formation is helpful if the language shows form syn-cretism in combination with morphosyntactic phe-nomena like agreement.
A model that uses syntacticinformation is superior to a sequence model becauseit leverages the syntactic dependencies that may holdbetween morphologically dependent words as sug-gested by linguistic theory.
We also showed thatonly small amounts of training data for a statisticalparser would be needed to improve the morphologi-cal tagger.
Making use of the improved morphologyin the dependency parser is not straight-forward andrequires more investigation in the future.Modeling the interaction between morphologyand syntax is important for building successful pars-ing pipelines for languages with free word order andrich morphology.
Moreover, our experiments showthat paying attention to the individual properties of alanguage can help us explain and predict the behav-ior of automatic tools.
Thus, the term ?morpholog-ically rich language?
should be viewed as a broadterm that covers many different languages, whosedifferences among each other may be as important asthe difference with languages with a less rich mor-phology.AcknowledgmentsWe would like to thank Jan Hajic?
and Jan S?te?pa?nekfor their kind help with the Czech morphology andfeaturama.
We would also like to thank ThomasMu?ller for sharing resources and thoughts with us,and Anders Bjo?rkelund for commenting on earlierversions of this paper.
This work was funded bythe Deutsche Forschungsgemeinschaft (DFG) viaSFB 732 ?Incremental Specification in Context?,project D8.ReferencesBarry J. Blake.
2001.
Case.
Cambridge UniversityPress, Cambridge, New York, 2nd edition.Bernd Bohnet and Joakim Nivre.
2012.
A Transition-Based System for Joint Part-of-Speech Tagging andLabeled Non-Projective Dependency Parsing.
In Pro-ceedings of the 2012 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning, pages 1455?1465, Jeju, South Korea.
Association for Computa-tional Linguistics.Bernd Bohnet.
2010.
Very high accuracy and fast depen-dency parsing is not a contradiction.
In Proceedings ofthe 23rd International Conference on ComputationalLinguistics, pages 89?97, Beijing, China.
InternationalCommittee on Computational Linguistics.Sabine Brants, Stefanie Dipper, Silvia Hansen-Shirra,Wolfgang Lezius, and George Smith.
2002.
TheTIGER treebank.
In Proceedings of the 1st Workshopon Treebanks and Linguistic Theories, pages 24?41,Sozopol, Bulgaria.Joan Bresnan.
2001.
Lexical-Functional Syntax.
Black-well Publishers.Xavier Carreras, Isaac Chao, Llus Padr, and Muntsa Padr.2004.
Freeling: An open-source suite of languageanalyzers.
In Proceedings of the 4th InternationalConference on Language Resources and Evaluation(LREC?04), pages 239?242.
European Language Re-sources Association (ELRA).342Grzegorz Chrupa?a, Georgiana Dinu, and Josef vanGenabith.
2008.
Learning morphology with mor-fette.
In Proceedings of the Sixth InternationalConference on Language Resources and Evaluation(LREC?08), pages 2362?2367, Marrakech, Morocco.European Language Resources Association (ELRA).http://www.lrec-conf.org/proceedings/lrec2008/.Shay B. Cohen and Noah A. Smith.
2007.
Joint morpho-logical and syntactic disambiguation.
In Proceedingsof the 2007 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, pages 208?217, Prague,Czech Republic.
Association for Computational Lin-guistics.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and experi-ments with perceptron algorithms.
In Proceedings ofthe 2002 Conference on Empirical Methods in Natu-ral Language Processing, pages 1?8.
Association forComputational Linguistics, July.Greville G. Corbett.
1991.
Gender.
Cambridge Text-books in Linguistics.
Cambridge University Press.Greville G. Corbett.
2006.
Agreement.
Cambridge Text-books in Linguistics.
Cambridge University Press.Michael A. Covington.
2001.
A fundamental algorithmfor dependency parsing (with corrections).
In Pro-ceedings of the 39th Annual ACM Southeast Confer-ence, Athens, Gorgia.
ACM.Koby Crammer, Ofer Dekel, Shai Shalev-Shwartz, andYoram Singer.
2003.
Online passive-aggressive algo-rithms.
In Proceedings of the 16th Annual Conferenceon Neural Information Processing Systems, volume 7,pages 1217?1224, Cambridge, Massachusetts, USA.MIT Press.Richa?rd Farkas, Veronika Vincze, and Helmut Schmid.2012.
Dependency parsing of hungarian: Baseline re-sults and challenges.
In Proceedings of the 13th Con-ference of the European Chapter of the Associationfor Computational Linguistics, pages 55?65, Avignon,France.
Association for Computational Linguistics.Yoav Goldberg and Michael Elhadad.
2010.
Easy firstdependency parsing of modern Hebrew.
In Proceed-ings of the NAACL HLT 2010 First Workshop on Sta-tistical Parsing of Morphologically-Rich Languages,pages 103?107, Los Angeles, California, USA.
Asso-ciation for Computational Linguistics.Yoav Goldberg and Michael Elhadad.
2013.
Word seg-mentation, unknown-word resolution, and morpholog-ical agreement in a hebrew parsing system.
Computa-tional Linguistics, 39(1):121?160.Yoav Goldberg and Reut Tsarfaty.
2008.
A single gener-ative model for joint morphological segmentation andsyntactic parsing.
In Proceedings of the 46th AnnualMeeting of the Association for Computational Linguis-tics, pages 371?379, Columbus, Ohio.
Association forComputational Linguistics.Jan Hajic?.
2000.
Morphological Tagging: Data vs. Dic-tionaries.
In Proceedings of the 6th ANLP Conference/ 1st NAACL Meeting, pages 94?101, Seattle, Wash-ington.
Association for Computational Linguistics.Jan Hajic?.
2004.
Disambiguation of Rich Inflection(Computational Morphology of Czech).
Nakladatel-stv??
Karolinum, Prague, Czech Republic.Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr Sgall,Petr Pajas, Jan S?te?pa?nek, Ji??
Havelka, and MarieMikulova?.
2006.
Prague Dependency Treebank 2.0.Jan Hajic?, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Anto`nia Mart?
?, Llu?
?sMa`rquez, Adam Meyers, Joakim Nivre, SebastianPado?, Jan Stepa?nek, Pavel Strana?k, Mihai Surdeanu,Nianwen Xue, and Yi Zhang.
2009.
The CoNLL-2009 shared task: Syntactic and Semantic dependen-cies in multiple languages.
In Proceedings of the13th Conference on Computational Natural LanguageLearning: Shared Task, pages 1?18, Boulder, Col-orado, USA.
Association for Computational Linguis-tics.Dilek Z. Hakkani-Tu?r, Kemal Oflazer, and Go?khan Tu?r.2002.
Statistical morphological disambiguation foragglutinative languages.
Computers and the Humani-ties, 36(4):381?410.Matt Hohensee and Emily M. Bender.
2012.
Gettingmore from morphology in multilingual dependencyparsing.
In Proceedings of the 2012 Conference of theNorth American Chapter of the Association for Com-putational Linguistics: Human Language Technolo-gies, pages 315?326, Montre?al, Canada.
Associationfor Computational Linguistics.John Lee, Jason Naradowsky, and David A. Smith.
2011.A discriminative model for joint morphological disam-biguation and dependency parsing.
In Proceedings ofthe 49th annual meeting of the Association for Compu-tational Linguistics, pages 885?894, Portland, USA.Association for Computational Linguistics.Yuval Marton, Nizar Habash, and Owen Rambow.
2013.Dependency parsing of modern standard arabic withlexical and inflectional features.
Computational Lin-guistics, 39(1):161?194.Igor Melc?uk.
2009.
Dependency in linguistic descrip-tion.Robbert Prins.
2004.
Beyond N in N-gram tagging.
InLeonoor Van Der Beek, Dmitriy Genzel, and DanielMidgley, editors, Proceedings of the ACL 2004 StudentResearch Workshop, pages 61?66, Barcelona, Spain.Association for Computational Linguistics.Anne Schiller.
1994.
Dmor - user?s guide.
Technicalreport, University of Stuttgart.343Helmut Schmid and Florian Laws.
2008.
Estimationof conditional probabilities with decision trees and anapplication to fine-grained POS tagging.
In Proceed-ings of the 22nd International Conference on Compu-tational Linguistics, pages 777?784, Morristown, NJ,USA.
Association for Computational Linguistics.Wolfgang Seeker and Jonas Kuhn.
2012.
Making El-lipses Explicit in Dependency Conversion for a Ger-man Treebank.
In Proceedings of the 8th Interna-tional Conference on Language Resources and Eval-uation, pages 3132?3139, Istanbul, Turkey.
EuropeanLanguage Resources Association (ELRA).Wolfgang Seeker and Jonas Kuhn.
2013.
Morphologi-cal and syntactic case in statistical dependency pars-ing.
Computational Linguistics, 39(1):23?55.Noah A. Smith, David A. Smith, and Roy W. Tromble.2005.
Context-based morphological disambiguationwith random fields.
In Proceedings of Human Lan-guage Technology Conference and Conference onEmpirical Methods in Natural Language Processing,pages 475?482, Vancouver, British Columbia, Canada,October.
Association for Computational Linguistics.Drahom?
?ra ?Johanka?
Spoustova?, Jan Hajic?, Jan Raab,and Miroslav Spousta.
2009.
Semi-supervised train-ing for the averaged perceptron POS tagger.
In Pro-ceedings of the 12th Conference of the EuropeanChapter of the Association for Computational Linguis-tics, pages 763?771, Athens, Greece.
Association forComputational Linguistics.Viktor Tro?n, Pe?ter Hala?csy, Pe?ter Rebrus, Andra?s Rung,Pe?ter Vajda, and Eszter Simon.
2006.
Morphdb.hu:Hungarian lexical database and morphological gram-mar.
In Proceedings of the 5th International Confer-ence on Language Resources and Evaluation, pages1670?1673, Genoa, Italy.Reut Tsarfaty and Khalil Sima?an.
2010.
Modeling mor-phosyntactic agreement in constituency-based parsingof Modern Hebrew.
In Proceedings of the NAACLHLT 2010 First Workshop on Statistical Parsing ofMorphologically-Rich Languages, pages 40?48, LosAngeles, California, USA.
Association for Computa-tional Linguistics.Reut Tsarfaty, Djame?
Seddah, Yoav Goldberg, SandraKu?bler, Marie Candito, Jennifer Foster, Yannick Vers-ley, Ines Rehbein, and Lamia Tounsi.
2010.
Statisticalparsing of morphologically rich languages (SPMRL):what, how and whither.
In Proceedings of the NAACLHLT 2010 First Workshop on Statistical Parsing ofMorphologically-Rich Languages, pages 1?12, LosAngeles, California, USA.
Association for Computa-tional Linguistics.Yannick Versley, Kathrin Beck, Erhard Hinrichs, andHeike Telljohann.
2010.
A syntax-first approach tohigh-quality morphological analysis and lemma dis-ambiguation for the tba-d/z treebank.
In 9th Confer-ence on Treebanks and Linguistic Theories (TLT9),pages 233?244.Veronika Vincze, Do?ra Szauter, Attila Alma?si, Gyo?rgyMo?ra, Zolta?n Alexin, and Ja?nos Csirik.
2010.
Hungar-ian Dependency Treebank.
In Proceedings of the 7thConference on International Language Resources andEvaluation, pages 1855?1862, Valletta, Malta.
Euro-pean Language Resources Association (ELRA).Jan Votrubec.
2006.
Morphological tagging based onaveraged perceptron.
In WDS?06 Proceedings of Con-tributed Papers, pages 191?195, Praha, Czechia.
Mat-fyzpress, Charles University.Ja?nos Zsibrita, Veronika Vincze, and Richa?rd Farkas.2013.
magyarlanc 2.0: szintaktikai elemze?s e?s felgy-ors?
?tott szo?faji egye?rtelms??te?s.
In Attila Tana?cs andVeronika Vincze, editors, IX.
Magyar Sza?m?
?to?ge?pesNyelve?szeti Konferencia, pages 368?374, Szeged,Hungary.344
