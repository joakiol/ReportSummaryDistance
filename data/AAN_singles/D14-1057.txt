Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 511?522,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsA Comparison of Selectional Preference Models for Automatic VerbClassificationWill Roberts and Markus EggInstitut f?r Anglistik und Amerikanistik, Humboldt University10099 Berlin, Germany{will.roberts,markus.egg}@anglistik.hu-berlin.deAbstractWe present a comparison of different selec-tional preference models and evaluate themon an automatic verb classification task inGerman.
We find that all the models wecompare are effective for verb clustering;the best-performing model uses syntacticinformation to induce nouns classes fromunlabelled data in an unsupervised man-ner.
A very simple model based on lexicalpreferences is also found to perform well.1 IntroductionSelectional preferences (Katz and Fodor, 1963;Wilks, 1975; Resnik, 1993) are the tendency fora word to semantically select or constrain whichother words may appear in a direct syntactic re-lation with it.
Selectional preferences (SPs) havebeen a perennial knowledge source for NLP taskssuch as word sense disambiguation (Resnik, 1997;Stevenson and Wilks, 2001; McCarthy and Car-roll, 2003) and semantic role labelling (Erk, 2007);and recognising selectional violations is thoughtto play a role in identifying and interpreting meta-phor (Wilks, 1978; Shutova et al., 2013).
We focuson the SPs of verbs, since determining which argu-ments are typical of a given verb sheds light on thesemantics of that verb.In this study, we present the first empirical com-parison of different SP models from the perspectiveof automatic verb classification (Schulte im Walde,2009; Sun, 2012), the task of grouping verbs to-gether based on shared syntactic and semantic prop-erties.We cluster German verbs using features captur-ing their valency or subcategorisation, followingprior work (Schulte im Walde, 2000; Esteve Ferrer,2004; Schulte im Walde, 2006; Sun et al., 2008;Korhonen et al., 2008; Li and Brew, 2008), andinvestigate the effect of adding information aboutverb argument preferences.
SPs are representedby features capturing lexical information about theheads of arguments to the verbs; we restrict ourfocus here to nouns.We operationalise a selectional preference modelas a function which maps such an argument headto a concept label.
We submit that the primarycharacteristic of such a model is its granularity.
Inour baseline condition, all nouns are mapped to thesame label; this effectively captures no informationabout a verb?s SPs (i.e., we cluster verbs using sub-categorisation information only).
On the other ex-treme, each noun is its own concept label; we termthis condition lexical preferences (LP).
Betweenthe baseline and LP lie a spectrum of models, inwhich multiple concepts are distinguished, andeach concept label can represent multiple nouns.Our main hypothesis is that verb clustering willwork best using a model of such intermediate gran-ularity.
This follows the intuition that verbs wouldseem to select for classes of nouns; for instance,we suppose that essen ?eat?
would tend to prefer asa direct object a noun from the abstract concept Es-sen (?food?).
We assume that these concepts can beexpressed independently of particular predicates;that is, there exist selectional preference modelsthat will work for all verbs (and all grammaticalrelations).
Further benefits of grouping nouns intoclasses include combating data sparsity, as wellas deriving models which can generalise to nounsunseen in training data.Another parameter of a selectional preferencemodel is the methodology used to induce the con-ceptual classes; put another way, the success ofan SP model hinges on how it represents concepts.In this paper, we investigate the choice of nouncategorisation method through an empirical com-parison of selectional preference models previouslyused in the literature.We set out to investigate the following questions:1.
What classes of nouns are effective descriptors511of selectional preference concepts?
For ex-ample, do they correspond to features such asANIMATE?2.
What is the appropriate granularity of selec-tional preference concepts?3.
Which methods of classifying nouns into con-cepts are most effective at capturing selec-tional preferences for verb clustering?This paper is structured as follows: In Section 2,we introduce our baseline method of clusteringverbs using subcategorisation information and de-scribe evaluation; Section 3 lists the models of se-lectional preferences that we compare in this work;Section 4 presents results and discussion; Section 5summarises related work; and Section 6 concludeswith directions for future research.2 Automatic verb classificationVerb classifications such as VerbNet (Kipper-Schuler, 2005) allow generalisations about the syn-tax and semantics of verbs and have proven usefulfor a range of NLP tasks; however, creation of theseresources is expensive and time-consuming.
Auto-matic verb classification seeks to learn verb classesautomatically from corpus data in a cheaper andfaster way.
This endeavour is possible due to thelink between a verb?s semantics and its syntactic be-haviour (Levin, 1993).
Recent research has foundthat even automatically-acquired classifications canbe useful for NLP applications (Shutova et al., 2010;Guo et al., 2011).
In this section, we introduce theverb classification method used by our baselinemodel, which clusters verbs based on subcategor-isation information.
Following this, Section 2.2 ex-plains the gold standard verb clustering and clusterpurity metric which we use for evaluation.2.1 Baseline modelIn this work, we take subcategorisation to meanthe requirement of a verb for particular types ofargument or concomitant.
For example, the Englishverb put subcategorises for subject, direct object,and a prepositional phrase (PP) like on the shelf :(1) [NPAl] put [NPthe book] [PPon the shelf].A subcategorisation frame (SCF) describes acombination of arguments required by a specificverb; a description of the set of SCFs which a verbmay take is called its subcategorisation preference.We acquire descriptions of verbal SCF preferenceson the basis of unannotated corpus data.Our experiments use the SdeWaC corpus (Faa?and Eckart, 2013), containing 880 million wordsin 45 million sentences; this is a subset of deWaC(Baroni et al., 2009), a corpus of 109words extrac-ted from Web search results.
SdeWaC is filteredto include only those sentences which are max-imally parsable1.
We parsed SdeWaC with themate-tools dependency parser (Bohnet et al.,2013)2, which performs joint POS and morpholo-gical tagging, as well as lemmatisation.
Our sub-categorisation analyses are delivered by the rule-based SCF tagger described by Roberts et al.
(2014),which operates using the dependency parses and as-signs each finite verb an SCF type.
The SCF tags aretaken from the SCF inventory proposed by Schulteim Walde (2002), which indicates combinationsof nominal and verbal complement types, such asnap:f?r.Acc (transitive verb, with a PP headedby f?r ?for?).
Examples of complements are n fornominative subject, and a for accusative direct ob-ject; in SCFs which include PPs (p), the SCF tagspecifies the head of the PP and the case of the pre-positional argument (Acc in our example indicatesthe accusative case of the prepositional argument).The SCF tagger undoes passivisation and analysesverbs embedded in modal and tense constructions.We record 673 SCF types in SdeWaC.From SdeWaC, we extracted the first 3,000,000verb instances assigned an SCF tag by the SCF tag-ger, where the verb lemma is one of the 168 listedin our gold standard clustering (this requires ap-proximately 270 million words of parsed text, or25% of SdeWaC).
We refer to this as our test set.In this set, each verb is seen on average 17,857times; the most common is geben (?give?, 328,952instances), and the least is grinsen (?grin?, 50).We represent verbs as vectors, where each di-mension represents a different SCF type.
Vectorentries are initialised with SCF code counts overthe test set, and each vector is then normalised tosum to 1, so that a vector represents a discrete prob-ability distribution over the SCF inventory.
We usethe Jensen-Shannon divergence as a dissimilaritymeasure between pairs of verb vectors.
The Jensen-Shannon divergence (Lin, 1991) is an information-theoretic, symmetric measure (Equation (2)) re-1The filtering used a rule-based dependency parser to es-timate a per-token parse error rate for each sentence, andremoved those sentences with very high error rates.2https://code.google.com/p/mate-tools/512lated to the Kullback-Leibler divergence (Equa-tion (3)).JS(p, q) = D(p||p+ q2) +D(q||p+ q2) (2)D(p||q) =?ipilogpiqi(3)With this dissimilarity measure, we use hier-archical clustering with Ward?s criterion (Ward,Jr, 1963) to partition the verbs into K disjoint sets(i.e., hard clustering), where we match K to thenumber of classes in our gold standard (describedbelow).2.2 Evaluation paradigmWe evaluate the automatically induced verb cluster-ings against a manually-constructed gold standard,published by Schulte im Walde (2006, page 162ff.
).This Levin-style classification groups 168 high-and low-frequency verbs into 43 semantic classes;examples include Aspect (e.g., anfangen ?begin?
),Propositional Attitude (e.g., denken ?think?
), andWeather (e.g., regnen ?rain?).
Some of the classesare further sub-classified; for the purposes of ourevaluation, we ignore the hierarchical structure ofthe classification and consider each class or sub-class to be a separate entity.
In this way, we obtainclasses of fairly comparable size and sufficient se-mantic consistency.3We evaluate a given verb clustering againstthe gold standard using the pairwise F -score(Hatzivassiloglou and McKeown, 1993).
To calcu-late this statistic, we construct a contingency tableover the(n2)pairs of verbs, the idea being that thegold standard provides binary judgements aboutwhether two verbs should be clustered together ornot.
If a clustering agrees with the gold standard asto whether a pair of verbs belong together or not,this is a ?correct?
answer.
Using the contingencytable, the standard information retrieval measuresof precision (P ) and recall (R) can be computed;the F -score is then the harmonic mean of these:F = 2PR/(P +R).
The random baseline is 2.08(calculated as the average score of 50 random parti-tions), and the optimal score is 95.81, calculated byevaluating the gold standard against itself.
As thegold standard includes polysemous verbs, which3In contrast, a top-level class like ?Transfer of Possession(Obtaining)?, not only covers 25% of the gold standard, it alsocomprises the semantically very diverse subclasses ?Transferof Possession (Giving)?, ?Manner of Motion?, and ?Emotion?.belong to more than one cluster, the optimal score iscalculated by randomly picking one of their senses;the average is then taken over 50 such trials.The pairwise F -score is known to be somewhatnonlinear (Schulte im Walde, 2006), penalisingearly clustering ?mistakes?
more than later ones,but it has the advantage that we can easily determ-ine statistical significance using the contingencytable and McNemar?s test.We use only one clustering algorithm and onepurity metric, because our prior work shows thatthe most important choices for verb clustering arethe distance measure used, and how verbs are rep-resented.
These factors set, we expect similar per-formance trends from different algorithms, withpredictable variation (e.g., spectral tends to outper-form hierarchical clustering, which in turn outper-forms k-means).
Combining Ward?s criterion andF -score is a trade-off at this point; the criterion isdeterministic, giving reproducible results withoutcomputational complexity, but disallows estimatesof density over our evaluation metric and is greedy(see discussion in Section 4.3).3 Selectional preference modelsIn this section, we introduce the various SP modelsthat we compare in this paper.
In all cases, wehold the verb clustering procedure described in theprevious section unchanged, with the exceptionthat SCF tags for verbs are parameterised forselectional preferences.
As an example, a verbinstance observed in a simple transitive frame witha nominal subject and accusative object wouldreceive the SCF tag na.
Assuming that a given SPmodel places the subject noun in the SP conceptanimate and the object noun in the conceptconcrete, the parameterised SCF tag would bena*subj-{animate}*obj-{concrete}.This process captures argument co-occurrenceinformation about verb instances, and has the effectof multiplying the SCF inventory size, making theverb vectors described in Section 2.1 both longerand sparser.We evaluate various types of SP models: thesimple lexical preferences model; three modelswhich perform automatic unsupervised inductionof noun concepts from unlabelled data; and onewhich uses a manually-built lexical resource.
Asfar as we are aware, two of these, the word spaceand LDA models, have never been applied to verbclassification before.513N Coverage of test set100 12.08%200 17.18%500 26.11%1,000 32.70%5,000 45.31%10,000 49.09%50,000 55.69%100,000 57.67%Table 1: Fraction of verb instances in the test setparameterised by LP as a function of the number ofnouns N included in the LP model.3.1 Lexical preferencesThe LP model is the simplest in our study after thebaseline condition; it simply maps a noun to its ownlemma.
We include as a parameter of the LP modela maximum number of nouns N to admit as LPtags.
In this way, the LP model parameterises SCFsusing only the N most frequent nouns in SdeWaC;nouns beyond rank N are treated as if they wereunseen.
Table 1 indicates what fraction of the 3million verb instances receive SCF tags specifyingone or more LPs as a function of this parameter.Note that the coverage approaches an asymptoteof around 60%.
This is due to the fact that nounarguments are not observed for every verb instance;many verbs?
arguments are pronominal or verbaland are not treated by our SP models.
Setting Nallows a simple way of tuning the LP model: Withincreasing N , the LP model should capture moredata about verb instances, but after a point thisbenefit should be cancelled out by the increasingsparsity in the verb vectors.3.2 Sun and Korhonen modelThe SP model described in this section (SUN) wasfirst used by Sun and Korhonen (2009) to de-liver state-of-the-art verb classification perform-ance for English; more recently, the technique wasapplied to successfully identify metaphor in freetext (Shutova et al., 2010; Shutova et al., 2013).It uses co-occurrence counts that describe whichnouns are found with which verbs in which gram-matical relations; this information is used to sort thenouns into classes in a procedure almost identicalto our verb clustering method described in Sec-tion 2.1.We extract all verb instances in SdeWaC whichare analysed by the SCF tagger, and count all(verb, grammatical relation, nominal argumenthead) triples, where the grammatical relation issubject, direct (accusative) object, indirect (dative)object, or prepositional object4, and is listed in theverb instance?s SCF tag; we undo passivisation, re-move instances of auxiliary and modal verbs, andfilter out those triples seen less than 10 times in thecorpus.These observations cover 60,870 noun types and33,748,390 tokens, co-occurring with 6,705 verbtypes (11,426 verb-grammatical-relation types); anexample is (sprechen, obj, Wort) (?speak?
with dir-ect object ?word?, occurring 1,585 times)5.
We rep-resent each noun by a vector whose 11,426 dimen-sions are the different verb-grammatical-relationpairs; coordinates in the vector indicate the ob-served corpus counts.
The vectors are then norm-alised to sum to 1, such that each represents someparticular noun?s discrete probability distributionover the set of verb-grammatical-relation pairs.
Thedistance between two noun vectors is defined tobe the Jensen-Shannon divergence between theirprobability distributions, and we partition the setof nouns into M groups using hierarchical Ward?sclustering.The SP model then maps a noun to an arbitrarylabel indicating which of the M disjoint sets thatnoun is to be found in (i.e., all nouns in the firstnoun class map to the concept label concept1);we employ the parameter M to model SP conceptgranularity.
As with the LP model, we use theparameter N to indicate how many nouns are in-cluded in the SUN model; we search the parametervalues N = {300, 500, 1000, 5000, 10000} andNM= {5, 10, 15, 20, 30, 50}.3.3 Word space modelWord space models (WSMs, (Sahlgren, 2006;Turney and Pantel, 2010)) use word co-occurrencecounts to represent the distributional semantics of aword.
This strategy makes possible a clustering ofnouns that does not depend on verbal dependenciesin the first place.4We have also experimented with adding features for eachnoun showing nominal modification features (e.g., (schwarz,nmod, Haar), ?hair?
modified by ?black?
), but these seem tohurt performance.5Triples representing prepositional object relations are dis-tinguished by preposition (e.g., the triple (geben, prep-in,Auftrag), ?give?
with PP headed by ?in?
with argument head?contract?, an idiomatic expression meaning ?to commission?something).514Dagan et al.
(1999) address the problem of datasparseness for the automatic determination of wordco-occurrence probabilities, which includes selec-tional preferences.
They introduce the idea of es-timating the probability of hitherto unseen wordcombinations using available information on wordsthat are closest w.r.t.
distributional word similar-ity.
Following this idea, Erk (2007) and Pad?
et al.
(2007) describe a memory-based SP model, using aWSM similarity measure to generalise the model tounseen data.We build a WSM of German nouns and use it topartition nouns into disjoint sets, which we thenemploy as with the SUN model.
We compute wordco-occurrence counts across the whole SdeWaCcorpus, using as features the 50,000 most commonwords in SdeWaC, skipping the first 50 most com-mon words (i.e., we use words 50 through 50,050),with sentences as windows.
We lemmatise the cor-pus and remove all punctuation; no other normalisa-tion is performed.
Co-occurrence counts betweena word wiand a feature cjare weighted using thet-test scheme:ttest(wi, cj) =p(wi, cj)?
p(wi)p(cj)?p(wi)p(cj)We use a recent technique called context selec-tion (Polajnar and Clark, 2014) to improve the wordspace model, whereby only the C most highlyweighted features are kept for each word vector.We set C by optimising the correlation between theword space model?s cosine similarity and a dataset of human semantic relatedness judgements for65 word pairs (Gurevych and Niederlich, 2005); atC = 380, we obtain Spearman ?
= 0.813 and Pear-son r = 0.707 (human inter-annotator agreementfor this data set is given as r = 0.810).After this, we build a similarity matrix betweenall pairs of nouns using the cosine similarity, andthen partition the set of N nouns into M disjointclasses using spectral clustering with the MNCutalgorithm (Meil?a and Shi, 2001).
As with the SUNmodel, this SP model assigns labels to nouns indic-ating which noun class they belong to.
We searchthe same parameter space for N and M as for theSUN model.3.4 GermaNetStatistical models of SPs have often used WordNetas a convenient and well-motivated inventory ofconcepts (e.g., Resnik (1997), Li and Abe (1998),Clark and Weir (2002)).
Typically, such modelsmake use of probabilistic treatments to determinean appropriate concept granularity separately foreach predicate; we opt here for a simple model thatallows more direct control over concept granularity.We take the set of concepts relevant to describingselectional preferences to be a target set of synsetsin GermaNet (Hamp and Feldweg, 1997), and rep-resent the target set as the set of synsets which areat some depth d or less in the GermaNet noun hier-archy: {s | depth(s) ?
d} where depth(s) countsthe number of hypernym links separating s fromthe root of the hierarchy.
We model concept gran-ularity by varying d = 1 .
.
.
6; at d = 1, the targetset is of size 5, and at d = 6, it is of size 17,125.Nouns are attributed to concepts as follows: Givena noun belonging to a synset s, either s is in thetarget set, or we take s?s lowest hypernym in thetarget set.
For polysemous nouns, each synset list-ing a sense of the noun votes for a member of thetarget set; the noun observation is then spread overthe target set using the votes as weights.This procedure makes our GermaNet SP model asoft clustering over nouns (i.e., a noun can belongto more than one SP concept); a consequence ofthis is that a single verb occurrence in the corpuscan contribute fractional counts to multiple SCFtypes.3.5 LDALatent Dirichlet allocation (Blei et al., 2003) is agenerative model that discovers similarities in datausing latent variables; it is frequently used for topicmodelling.
LDA models of SPs have been proposedby ?
S?aghdha (2010) and Ritter et al.
(2010);previous to this, Rooth et al.
(1999) also describeda latent variable model of SPs.We implement the LDA model of selectional pref-erences described by ?
S?aghdha (2010).
Gener-atively, the model produces nominal arguments toverbs as follows: For a given (verb, grammatical re-lation) pair (v, r), (1) Sample a noun class z from afrom a multinomial distribution ?v,rwith a Dirich-let prior parameterised by ?
; (2) Sample a noun nfrom a multinomial distribution ?zwith a Dirichletprior parameterised by ?.
Like ?
S?aghdha, we usean asymmetric Dirichlet prior for ?v,r(i.e., ?
candiffer for each noun class) and a symmetric priorfor ?z(?
is the same for each ?z).
We estimatethe LDA model using the MALLET software (Mc-Callum, 2002) using the same (verb, grammatical515relation, argument head) co-occurrence statisticsused for the SUN model.
We train for 1,000 it-erations using the software?s default parameters,allowing the LDA hyperparameters ?
and ?
to bere-estimated every 10 iterations.
We build mod-els with 50 or 100 topics as a proxy to conceptgranularity; models include number of nouns N of{500, 1000, 5000, 10000, 50000, 100000}.As with the GermaNet-based model, the LDAmodel creates a soft clustering of nouns; the abil-ity of a noun to have degrees of membership inmultiple concepts might be a good way to modelpolysemy.
We also experiment with a hard cluster-ing version of the LDA model; to do this, we assigneach noun n its most likely class label z using themodel?s estimate for P (z|n).4 ResultsWe experimented with applying the SP models todifferent combinations of grammatical relations(e.g., only subject, only object, subject+object,etc.
), but generally obtained better results by para-meterising SCF tags for all grammatical relations.Table 2 summarises the evaluation scores and para-meter settings for the best-performing SP models,applied to verb arguments in all four grammaticalrelations (subject, direct, indirect and prepositionalobject)6.
The table also indicates the number ofSCF types constructed by each SP model (i.e., thenumber of dimensions of the vectors representingverbs).All the SP models we compare help with auto-matic verb clustering.
Using McNemar?s test onthe contingency tables underlying the F -scores, allmodels score better than the baseline at at least thep < 0.01 level.
LDA-hard is better than the Ger-maNet, LDA-soft, WSM and LP models at at leastthe p < 0.05 level; SUN is better (p ?
0.05) thanall models except LDA-hard.
All other performancedifferences are not statistically significant7.We can also demonstrate the effectiveness of theSP models with a regression analysis on the models?coverage of the test set.
By varying the number ofnouns N included in the SP models which use thisparameter (LP, SUN, WSM, LDA), or by paramet-erising SCF tags with SP information only for par-6Due to space constraints, we do not present here a de-tailed per-model study of performance as a function of para-meter settings; we feel a summary to be adequate, since therelative performances of the models reflect trends across arange of parameter settings.7Using a significance criterion of p < 0.05.ticular combinations of grammatical relations, dif-ferent numbers of the verb instances in the test datawill end up with SP information in their SCF tags(this is the ?coverage?
statistic in Table 1); withthe exception of the GermaNet model, all of the SPmodels we examine here show positive correlationbetween the number of verb instances tagged forSP information and verb clustering performance.This effect is independent of parameter settings,indicating the performance benefit conferred by theSP models is robust.4.1 Comparison of SP modelsThe GermaNet model is the least successful in ourstudy.
It achieves its best performance with a depthof 5; after this, verb clustering performance dropsoff again.
Verb clustering using the GermaNetSP model is only slightly better than the baselinecondition.Against our expectations, the hard clusteringLDA models perform better than the soft cluster-ing ones, achieving the second highest score in ourevaluation; also, in contrast to the other SP mod-els studied in this paper, LDA performs best withfewer, coarser-grained topics.
We observe that thesoft clustering models produce verb vectors morethan an order of magnitude longer than the hardclustering models, and suggest that simple soft clus-tering may be causing problems with data sparsitythat interfere with verb clustering.
We have alsoobserved that the topics found by LDA do not rep-resent polysemy as we had hoped.
While someof the topics discovered by the LDA models canbe easily assigned labels (e.g., body parts, people,quantities, emotions, places, buildings, tools, etc.
),others are less cohesive.
We found that frequentwords (e.g., time, person) are generated with highprobability by multiple topics in ways that do notappear to reflect multiple word senses, and that the100-topic models exhibit this property to a greaterextent.
For instance, Zeit ?time?
is highly predict-ive of three topics in the 50-topic models, of whichonly the highest-weighted topic groups time ex-pressions together; in the 100-topic models, Zeitis found in six topics.
Again, of these six, onlythe topic with the highest ?
consists of time expres-sions.
In the 50-topic models, we find 11 topics thatwe cannot assign a coherent label; in the 100-topicmodels, there are 38 of these mismatched topics.In our work to date, we have not found that LDAmodels with greater numbers of topics find more516SP model Parameters Granularity F -score Number of SCF typesSUN 10,000 nouns 1,000 noun classes 39.76 248,665LDA (hard) 10,000 nouns 50 topics 39.10 78,409LP 5,000 nouns 38.02 388,691WSM 10,000 nouns 500 noun classes 36.95 149,797LDA (soft) 10,000 nouns 50 topics 35.91 1,524,338GermaNet depth = 5 8,196 synsets 34.41 851,265Baseline 33.47 673Table 2: Evaluation of the best SP models.100101102103104105N3132333435363738PairF0.00.10.20.30.40.50.6CoverageFigure 1: Verb clustering performance (black) andtest set coverage (grey) of the LP model as a func-tion of the number of nouns N included in themodel.specific concepts; it is possible that this problemmight be alleviated by careful filtering of the (verb,grammatical relation, noun) triples, but we leavethis question to future research.The LP model is very effective, which is surpris-ing given its simplicity.
As expected, with increas-ing N , we do observe sparsity effects which hurtverb clustering performance (see Figure 1).Our best performing model is SUN.
Our best res-ult is obtained with 10,000 nouns (the maximumvalue of N that we tried) in 1,000 classes, givingrelatively fine-grained classes (on average 10 nounsper class).
Table 3 shows some example nounclasses learned by the SUN model.
These include:groups with synonyms or near synonyms, often in-cluding alternate spellings of the same word (suchas in the truck grouping); and groups of closely-related co-hyponyms, such as the body part group-ing and the clothing grouping.
In the latter, bill,joint responsibility, complicity and inscription arealso included as things which can be borne, thisis due to the fact that the SUN noun clustering isbased on triples of verbs, grammatical relations,and nouns.LKW (truck), Lkw (truck), Lastwagen (truck),Castor (container for highly radioactive mater-ial), Laster (truck), Krankenwagen (ambulance),Transporter (van), Traktor (tractor)Hand (hand), Kopf (head), Fu?
(foot), Haar(hair), Bein (leg), Arm (arm), Zahn (tooth), Fell(fur)Leiche (corpse), Leichnam (body), Sch?del(skull), Skelett (skeleton), Wrack (wreck), Mu-mie (mummy), Tr?mmer (debris)Sauna (sauna), Badezimmer (bathroom),Schwimmbad (swimming pool), Nachbildung(replica), Kamin (fireplace), Aufenthaltsraum(common room), Mensa (cafeteria)Rechnung (bill), Kopftuch (headscarf), Uniform(uniform), Anzug (suit), Helm (helmet), Gewand(garment), Handschuh (glove), Mitverantwor-tung (joint responsibility), Bart (beard), R?s-tung (armour), Mitschuld (complicity), Socke(sock), Jeans (jeans), Sonnenbrille (sunglasses),Aufschrift (inscription), Pullover (sweater),Weste (vest), Handschellen (handcuffs), H?rner(horns), Kennzeichen (marking), Tracht (tradi-tional costume), Korsett (corset), Schuhwerk(footwear), Kopfbedeckung (headgear), Pelz(fur), Maulkorb (muzzle)Missionar (missionary), Weihnachtsmann(Santa Claus), Selbstmordattent?ter (sui-cide bomber), Bote (messenger), Nikolaus(Nicholas), Killer (killer), Bomber (bomber),Osterhase (Easter bunny)Table 3: Example noun clusters in the SUN SPmodel.517Furthermore, there are thematically relatedgroups (corpse, body, etc., and sauna, bathroom,etc.).
All months are placed together in one 12-word group.Some classes can be easily subdivided into sep-arate groups, and sometimes the source for this canbe guessed: For example, sports (football, golf, ten-nis) are lumped together with musical instruments(guitar, piano, violin) and film roles (starring role,supporting role), these all being things that canbe played.
Many groups of personal roles (suchas various kinds of government ministers) are dis-tinguished, as are diseases and medications; othergroupings contain proper names or geographicallocations, sometimes of surprising specificity (e.g.,authors, Biblical names, philosophers, NGOs, East-ern European countries, foreign currencies, Ger-man male first names, newspapers, television chan-nels).
The last group in Table 3 shows a groupingwhich appears to combine two of these semantic-ally narrow categories, in which Santa Claus andthe Easter bunny are united with killers and suicidebombers.4.2 Noun classes as SP conceptsThe WSM SP model is not as successful as SUN, but,due to the methodological similarity between thesetwo (SP concepts modelled as hard partitions ofnouns), it affords us an opportunity to investigatethe question of what properties might make for aneffective noun partition.The WSM model partitions nouns based onparadigmatic information (which sentence con-texts a noun appears in), rather than SUN?s useof syntagmatic information (which grammaticalcontexts a noun appears in).
Therefore, it is per-haps not surprising that the noun classes derivedby the WSM are organised thematically, and thesynonym/co-hyponym structure observed in theSUN noun classes is in many cases absent (e.g.,{Pferd (horse), Reiter (rider), Stall (stable), Sattel(saddle), Stute (mare)}; these classes can easilyconflate semantic roles (e.g., Agent for rider andLocation for stable), which is presumably unhelp-ful for representing selectional preferences.The distribution of noun classes also differsbetween SUN and WSM.
The largest noun classin the WSM model contains 1,076 high-frequencynouns which are semantically unrelated (day, ques-tion, case, part, reason, kind, form, week, person,month, .
.
.
).
We suppose that these nouns are them-105106107108Number of verb instances15202530354045PairFBaselineLPWSMSUNLDA-hardFigure 2: Verb clustering performance of SP mod-els as a function of number of verb instances.atically ?neutral?
and are classed together by virtueof their usage in a wide variety of sentences.
Thisone noun class by itself subsumes 13.6% of allnoun tokens in SdeWaC.
WSM also includes 56singleton noun classes; the variance in noun classsize is 2800.
For comparison, in SUN, the largestnoun class has 73 words, and the smallest, 2 (thereare 12 of these two-word classes); noun class sizevariance is 37.
The 73-word class in SUN does in-deed appear to be a grab bag (including gas, taboo,pioneer, mustard, spy, mafia, and skinhead), butthese are uncommon words and account for only0.1% of noun tokens in SdeWaC.
The next twomost common classes (with some 40 nouns each)are lists of names (politicians?
surnames, and malefirst names).
The noun class in the SUN model con-taining the largest number of high-frequency nouns(28 nouns: human, child, woman, man, people, Mr.,mother, father, .
.
. )
only covers 3.6% of noun us-ages in SdeWaC and is both semantically cohesiveand intuitively useful as a SP concept.These issues raise the question of why the WSMmodel is effective at all for verb classification.We think that the larger less-related noun classesneither help nor hurt verb clustering, and we findthat some of the thematic classes represent abstrac-tions that should be useful for describing SPs.
Ex-amples include lists of body parts, countries (separ-ate classes for Europe, Africa, Asia, etc.
), diseases,human names, articles of clothing, and the group{fruit, apple, banana, pear, strawberry}.4.3 Effects of test set sizeWe were curious if the success of the LP modelmight be due to the size of the test set preventing518sparsity from becoming a problem.
To pursue thisquestion, we take the four best performing SP mod-els and run the verb clustering evaluation with thenumber of verb instances in the test set varyingbetween 10,000 and the full SdeWaC corpus (11million).
The results are displayed in Figure 2.
Thisgraph indicates that below 3?
105verb instances,sparsity seems to become a problem for all mod-els on this task, and the baseline delivers the bestperformance.
Above this threshold, it seems thatsparsity is not a major issue: LP performs fairly con-sistently, and is competitive with the SUN model.We attribute this to our use of the Jensen-Shannondivergence as a verb dissimilarity measure, whichseems relatively robust to data sparsity.
The LDA-hard model with its fewer topics seems to do quitewell with fewer data; as the test set size increases, itdrops off in the rankings.
At the maximum numberof verb instances, the best-performing models areSUN, WSM and the lexical preferences.
The figurealso shows that our evaluation metric is not smooth(note, e.g., the fluctuations in the baseline score).We believe that this reflects a degree of instabilityin the Ward?s hierarchical clustering algorithm; thisclustering method is greedy, and clustering errorscan be expected to propagate, which might explainthe jaggedness of the plot.4.4 ConclusionsTo conclude, we summarise the results of our ana-lysis, using the questions formulated in the Intro-duction as guidelines.First, we wanted to compare the efficiency ofdifferent classes of nouns as descriptors of selec-tional preference concepts.
Our findings suggestthat noun classes are most effective when they aresemantically highly consistent, representing groupsof strongly related nouns.
It seems reasonable thatSP concepts representing collections of synonymswould be useful for generalising observations, andshould represent arguments better than simple LP.A classification of proper names (e.g., as human,corporation, country, medication) is also useful.This implies that we can expect features such asANIMATE to be shared by all members of a nouncluster.Second, we were interested in the appropriategranularity of selectional preference concepts.
Inour evaluation, we have observed a tendency forsmaller, more specific noun classes to be superior;this holds because data sparsity is not a problemin our experiment.
Beyond this finding, we wouldhave liked to present a direct juxtaposition of differ-ent models on ?granularity?
but this is difficult: Wehave not yet identified a strong abstraction of gran-ularity from the proxies we use (e.g., GermaNetdepth, or SUN?s N/M ).Finally, which methods of classifying nouns intoconcepts are most effective at capturing selectionalpreferences for verb clustering?
In our experiments,the SUN and LDA-hard models proved to be moreeffective than lexical preferences, supporting ourprimary hypothesis that some level of SP conceptgranularity above the lexical level is desirable forverb clustering.
On the other hand, the LP model isonly slightly worse than SUN and LDA-hard, mak-ing it attractive because it is so simple.
As we haveshown, the potential data sparsity issues with LPcan be alleviated by judiciously choosing the valueof the N parameter that controls the number ofnouns included in the model.
In addition, compar-ing the SUN and WSM models, and observing theperformance of the LDA-hard method, we concludethat inducing noun classes using syntagmatic in-formation is more effective than using paradigmaticrelations.5 Related workIn this study, we have looked at the utility of selec-tional preferences for automatic verb classification.Some previous research has followed this line ofinquiry, though prior studies have not comparedalternative methods of modelling SPs.
Schulte imWalde (2006) presented a detailed examination ofparameters for k-means-based verb clustering inGerman, using the same gold standard that we em-ploy here.
She reports on the effects of adding SPinformation to a SCF-based verb clustering using15 high-level GermaNet synsets as SP concepts; SPinformation for some combinations of grammaticalrelations improves clustering performance slightly,but neither are the effects consistent, nor is theimprovement delivered by the SP model over theSCF-based baseline statistically significant.
Schulteim Walde et al.
(2008) used expectation maximisa-tion to induce latent verb clusters from the BritishNational Corpus while simultaneously building atree cut model of SPs on the WordNet hierarchyusing a minimum description length method; theirevaluation focuses on the induced soft verb clusters,reporting the model?s estimated perplexity of (verb,grammatical relation, argument head) triples.
The519SPs are described qualitatively by presenting twoexample cases.
Sun and Korhonen (2009) studythe effect of adding selectional preferences to asubcategorisation-based verb clustering in Eng-lish using the SUN model (see Section 3.2).
Theydemonstrate that adding SPs to the SCF preferencedata leads to the best results on their two clusteringevaluations; overall, their best results come fromusing SP information only for the subject gram-matical relation.
They employ coarse SP concepts(20 or 30 noun clusters) which capture general se-mantic categories (Human, Building, Idea, etc.
).Selectional preferences are usually evaluatedeither from a word sense disambiguation stand-point using pseudo-words (Chambers and Juraf-sky, 2010), or in terms of how acceptable an ar-gument is with a verb, via regression against hu-man plausibility judgements.
Several studies havecompared SP methodologies from the latter per-spective.
These include Brockmann and Lapata(2003), who compared three GermaNet-based mod-els of SP, showing that different models were mosteffective for describing different grammatical re-lations; ?
S?aghdha (2010), who compared dif-ferent LDA-based models of SP, showing these tobe effective for a variety of grammatical relations;and ?
S?aghdha and Korhonen (2012), who showthat WordNet tree cut models, LDA, and a hybridLDA-WordNet model are effective for describingverb-object relations.6 Future workOur GermaNet model delivered disappointing per-formance in this study; we would be interested inseeing whether a more sophisticated implementa-tion such as the tree cut model of Li and Abe (1998)would be more competitive.
We also would like toexplore alternative noun clustering methods suchas CBC (Pantel and Lin, 2002) and Brown clusters(Brown et al., 1992), which were not covered inthis work; these would fit easily into our SP eval-uation paradigm.
More challenging would be averb classification-based evaluation of the SP mod-els of (Rooth et al., 1999) and (Schulte im Waldeet al., 2008), which use expectation maximisationto simultaneously cluster verbs into verb classesand nominal arguments into noun classes; these ap-proaches are not compatible with the evaluationframework we have used here.
Finally, the SPmodel of Bergsma et al.
(2008) has also achievedimpressive results on a number of tasks, but has notbeen investigated for use in verb classification.Our verb clustering evaluation in this workhas matched K, the number of clusters found byWard?s method, to the number of classes in thegold standard.
Since the number of clusters has aninfluence on the quality of the ensuing semanticclassification (Schulte im Walde, 2006, page 180f.
),we will also be running our experiments with dif-ferent settings of K to explore whether this alsoinfluences the overall results of our evaluation.ReferencesMarco Baroni, Silvia Bernardini, Adriano Ferraresi,and Eros Zanchetta.
2009.
The WaCky wide web: Acollection of very large linguistically processed Web-crawled corpora.
Language Resources and Evalu-ation, 43(3):209?226.Shane Bergsma, Dekang Lin, and Randy Goebel.
2008.Discriminative learning of selectional preferencefrom unlabeled text.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, pages 59?68.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet allocation.
Journal of Ma-chine Learning Research, 3:993?1022.Bernd Bohnet, Joakim Nivre, Igor Boguslavsky,Rich?rd Farkas, Filip Ginter, and Jan Haji?c.
2013.Joint morphological and syntactic analysis for richlyinflected languages.
Transactions of the Associationfor Computational Linguistics, 1:415?428.Carsten Brockmann and Mirella Lapata.
2003.
Evalu-ating and combining approaches to selectional pref-erence acquisition.
In Proceedings of the Tenth Con-ference on European Chapter of the Association forComputational Linguistics, pages 27?34.Peter F. Brown, Peter V. Desouza, Robert L. Mer-cer, Vincent J. Della Pietra, and Jenifer C. Lai.1992.
Class-based n-gram models of natural lan-guage.
Computational Linguistics, 18(4):467?479.Nathanael Chambers and Daniel Jurafsky.
2010.
Im-proving the use of pseudo-words for evaluating se-lectional preferences.
In Proceedings of the 48th An-nual Meeting of the Association for ComputationalLinguistics, pages 445?453.Stephen Clark and David Weir.
2002.
Class-basedprobability estimation using a semantic hierarchy.Computational Linguistics, 28(2):187?206.Ido Dagan, Lillian Lee, and Fernando C.N.
Pereira.1999.
Similarity-based models of word cooccur-rence probabilities.
Machine Learning, 34(1?3):43?69.520Katrin Erk.
2007.
A simple, similarity-based modelfor selectional preferences.
In Proceedings of the45th Annual Meeting of the Association for Compu-tational Linguistics, pages 216?223.Eva Esteve Ferrer.
2004.
Towards a semantic classi-fication of Spanish verbs based on subcategorisationinformation.
In Proceedings of the Student ResearchWorkshop at the Annual Meeting of the Associationfor Computational Linguistics, pages 37?42.Gertrud Faa?
and Kerstin Eckart.
2013.
SdeWaC - Acorpus of parsable sentences from the Web.
In Lan-guage processing and knowledge in the Web, pages61?68.
Springer, Berlin, Heidelberg.Yufan Guo, Anna Korhonen, and Thierry Poibeau.2011.
A weakly-supervised approach to argumentat-ive zoning of scientific documents.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing, pages 273?283.Iryna Gurevych and Hendrik Niederlich.
2005.
Com-puting semantic relatedness in German with revisedinformation content metrics.
In Proceedings of "On-toLex 2005 - Ontologies and Lexical Resources"IJCNLP?05 Workshop, pages 28?33.Birgit Hamp and Helmut Feldweg.
1997.
GermaNet:A lexical-semantic net for German.
In Proceedingsof ACL Workshop on Automatic Information Extrac-tion and Building of Lexical Semantic Resources forNLP Applications, pages 9?15.Vasileios Hatzivassiloglou and Kathleen R. McKeown.1993.
Towards the automatic identification of ad-jectival scales: Clustering adjectives according tomeaning.
In Proceedings of the 31st Annual Meet-ing on Association for Computational Linguistics,pages 172?182.Jerrold J. Katz and Jerry A. Fodor.
1963.
The structureof a semantic theory.
Language, 39:170?210.Karin Kipper-Schuler.
2005.
VerbNet: A broad-coverage, comprehensive verb lexicon.
Ph.D. thesis,University of Pennsylvania.Anna Korhonen, Yuval Krymolowski, and Nigel Col-lier.
2008.
The choice of features for classifica-tion of verbs in biomedical texts.
In Proceedingsof the 22nd International Conference on Computa-tional Linguistics, pages 449?456.Beth Levin.
1993.
English verb classes and altern-ations: A preliminary investigation.
University ofChicago Press, Chicago.Hang Li and Naoki Abe.
1998.
Generalizing caseframes using a thesaurus and the MDL principle.Computational Linguistics, 24(2):217?244.Jianguo Li and Chris Brew.
2008.
Which are the bestfeatures for automatic verb classification.
In Pro-ceedings of ACL-08: HLT, pages 434?442.Jianhua Lin.
1991.
Divergence measures based on theShannon entropy.
IEEE Transactions on Informa-tion Theory, 37(1):145?151.Andrew McCallum.
2002.
MALLET: A machinelearning for language toolkit.Diana McCarthy and John Carroll.
2003.
Disambig-uating nouns, verbs, and adjectives using automat-ically acquired selectional preferences.
Computa-tional Linguistics, 29(4):639?654.Marina Meil?a and Jianbo Shi.
2001.
A random walksview of spectral segmentation.
In Proceedings of theInternational Conference on Artificial Intelligenceand Statistics (AISTATS).Diarmuid ?
S?aghdha and Anna Korhonen.
2012.Modelling selectional preferences in a lexical hier-archy.
In Proceedings of the 1st Joint Conference onLexical and Computational Semantics, pages 170?179.Diarmuid ?
S?aghdha.
2010.
Latent variable mod-els of selectional preference.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics, pages 435?444.Sebastian Pad?, Ulrike Pad?, and Katrin Erk.
2007.Flexible, corpus-based modelling of human plausib-ility judgements.
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), pages 400?409.Patrick Pantel and Dekang Lin.
2002.
Discoveringword senses from text.
In Proceedings of the EighthACM SIGKDD International Conference on Know-ledge Discovery and Data Mining, pages 613?619.Tamara Polajnar and Stephen Clark.
2014.
Improv-ing distributional semantic vectors through contextselection and normalisation.
In Proceedings of the14th Conference of the European Chapter of the As-sociation for Computational Linguistics, pages 230?238.Philip Resnik.
1993.
Selection and information: Aclass-based approach to lexical relationships.
Ph.D.thesis, University of Pennsylvania.Philip Resnik.
1997.
Selectional preference and sensedisambiguation.
In Proceedings of the ACL SIGLEXWorkshop on Tagging Text with Lexical Semantics:Why, What, and How, pages 52?57.Alan Ritter, Mausam, and Oren Etzioni.
2010.
A lat-ent Dirichlet allocation method for selectional pref-erences.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguistics,pages 424?434.Will Roberts, Markus Egg, and Valia Kordoni.
2014.Subcategorisation acquisition from raw text for afree word-order language.
In Proceedings of the52114th Conference of the European Chapter of the As-sociation for Computational Linguistics, pages 298?307.Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Car-roll, and Franz Beil.
1999.
Inducing a semantic-ally annotated lexicon via EM-based clustering.
InProceedings of the 37th Annual Meeting of the Asso-ciation for Computational Linguistics on Computa-tional Linguistics, pages 104?111.Magnus Sahlgren.
2006.
The word-space model: Us-ing distributional analysis to represent syntagmaticand paradigmatic relations between words in high-dimensional vector spaces.
Ph.D. thesis, StockholmUniversity.Sabine Schulte im Walde, Christian Hying, ChristianScheible, and Helmut Schmid.
2008.
CombiningEM training and the MDL principle for an automaticverb classification incorporating selectional prefer-ences.
In Proceedings of the 46th Annual Meet-ing of the Association for Computational Linguistics,pages 496?504.Sabine Schulte im Walde.
2000.
Clustering verbs se-mantically according to their alternation behaviour.In Proceedings of the 18th Conference on Computa-tional Linguistics, pages 747?753.Sabine Schulte im Walde.
2002.
A subcategorisationlexicon for German verbs induced from a lexicalisedPCFG.
In Proceedings of the 3rd Conference onLanguage Resources and Evaluation (LREC), pages1351?1357.Sabine Schulte im Walde.
2006.
Experiments onthe automatic induction of German semantic verbclasses.
Computational Linguistics, 32(2):159?194.Sabine Schulte im Walde.
2009.
The induction ofverb frames and verb classes from corpora.
InAnke L?deling and Merja Kyt?, editors, Corpuslinguistics: An international handbook, volume 2,chapter 44, pages 952?971.
Mouton de Gruyter, Ber-lin.Ekaterina Shutova, Lin Sun, and Anna Korhonen.2010.
Metaphor identification using verb and nounclustering.
In Proceedings of the 23rd InternationalConference on Computational Linguistics, pages1002?1010.Ekaterina Shutova, Simone Teufel, and AnnaKorhonen.
2013.
Statistical metaphor processing.Computational Linguistics, 39(2):301?353.Mark Stevenson and Yorick Wilks.
2001.
The interac-tion of knowledge sources in word sense disambigu-ation.
Computational Linguistics, 27(3):321?349.Lin Sun and Anna Korhonen.
2009.
Improving verbclustering with automatically acquired selectionalpreferences.
In Proceedings of the 2009 Confer-ence on Empirical Methods in Natural LanguageProcessing, pages 638?647.Lin Sun, Anna Korhonen, and Yuval Krymolowski.2008.
Verb class discovery from rich syntactic data.In Proceedings of the Ninth International Confer-ence on Intelligent Text Processing and Computa-tional Linguistics, pages 16?27.Lin Sun.
2012.
Automatic induction of verb classesusing clustering.
Ph.D. thesis, University of Cam-bridge, Cambridge.Peter D. Turney and Patrick Pantel.
2010.
Fromfrequency to meaning: Vector space models of se-mantics.
Journal of Artificial Intelligence Research,37(1):141?188.Joe H. Ward, Jr. 1963.
Hierarchical grouping to optim-ize an objective function.
Journal of the AmericanStatistical Association, 58(301):236?244.Yorick Wilks.
1975.
An intelligent analyzer and un-derstander of English.
Communications of the ACM,18(5):264?274.Yorick Wilks.
1978.
Making preferences more active.Artificial Intelligence, 11(3):197?223.522
