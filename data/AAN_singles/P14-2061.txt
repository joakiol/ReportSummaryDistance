Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 370?376,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsUnsupervised Feature Learning for Visual Sign Language IdentificationBinyam Gebrekidan Gebre1, Onno Crasborn2, Peter Wittenburg1,Sebastian Drude1, Tom Heskes21Max Planck Institute for Psycholinguistics,2Radboud University Nijmegenbingeb@mpi.nl,o.crasborn@let.ru.nl,peter.wittenburg@mpi.nl,sebastian.drude@mpi.nl,t.heskes@science.ru.nlAbstractPrior research on language identification fo-cused primarily on text and speech.
In thispaper, we focus on the visual modality andpresent a method for identifying sign lan-guages solely from short video samples.
Themethod is trained on unlabelled video data (un-supervised feature learning) and using thesefeatures, it is trained to discriminate betweensix sign languages (supervised learning).
Weran experiments on short video samples in-volving 30 signers (about 6 hours in total).
Us-ing leave-one-signer-out cross-validation, ourevaluation shows an average best accuracy of84%.
Given that sign languages are under-resourced, unsupervised feature learning tech-niques are the right tools and our results indi-cate that this is realistic for sign language iden-tification.1 IntroductionThe task of automatic language identification isto quickly identify the identity of the languagegiven utterances.
Performing this task is key inapplications involving multiple languages such asmachine translation and information retrieval (e.g.metadata creation for large audiovisual archives).Prior research on language identification isheavily biased towards written and spoken lan-guages (Dunning, 1994; Zissman, 1996; Li et al,2007; Singer et al, 2012).
While language iden-tification in signed languages is yet to be studied,significant progress has been recorded for writtenand spoken languages.Written languages can be identified to about99% accuracy using Markov models (Dunning,1994).
This accuracy is so high that currentresearch has shifted to related more challeng-ing problems: language variety identification(Zampieri and Gebre, 2012), native language iden-tification (Tetreault et al, 2013) and identificationat the extremes of scales; many more languages,smaller training data, shorter document lengths(Baldwin and Lui, 2010).Spoken languages can be identified to accura-cies that range from 79-98% using different mod-els (Zissman, 1996; Singer et al, 2003).
Themethods used in spoken language identificationhave also been extended to a related class of prob-lems: native accent identification (Chen et al,2001; Choueiter et al, 2008; Wu et al, 2010) andforeign accent identification (Teixeira et al, 1996).While some work exists on sign languagerecognition1(Starner and Pentland, 1997; Starneret al, 1998; Gavrila, 1999; Cooper et al, 2012),very little research exists on sign language iden-tification except for the work by (Gebre et al,2013), where it is shown that sign language identi-fication can be done using linguistically motivatedfeatures.
Accuracies of 78% and 95% are reportedon signer independent and signer dependent iden-tification of two sign languages.This paper has two goals.
First, to present amethod to identify sign languages using featureslearned by unsupervised techniques (Hinton andSalakhutdinov, 2006; Coates et al, 2011).
Sec-ond, to evaluate the method on six sign languagesunder different conditions.Our contributions: a) show that unsupervisedfeature learning techniques, currently popular inmany pattern recognition problems, also work forvisual sign languages.
More specifically, we showhow K-means and sparse autoencoder can be usedto learn features for sign language identification.b) demonstrate the impact on performance of vary-ing the number of features (aka, feature maps orfilter sizes), the patch dimensions (from 2D to 3D)and the number of frames (video length).1There is a difference between sign language recognitionand identification.
Sign language recognition is the recogni-tion of the meaning of the signs in a given known sign lan-guage, whereas sign language identification is the recognitionof the sign language itself from given signs.3702 The challenges in sign languageidentificationThe challenges in sign language identificationarise from three sources as described below.2.1 Iconicity in sign languagesThe relationship between forms and meanings arenot totally arbitrary (Perniss et al, 2010).
Bothsigned and spoken languages manifest iconicity,that is forms of words or signs are somehow mo-tivated by the meaning of the word or sign.
Whilesign languages show a lot of iconicity in the lex-icon (Taub, 2001), this has not led to a universalsign language.
The same concept can be iconi-cally realised by the manual articulators in a waythat conforms to the phonological regularities ofthe languages, but still lead to different sign forms.Iconicity is also used in the morphosyntax anddiscourse structure of all sign languages, however,and there we see many similarities between signlanguages.
Both real-world and imaginary objectsand locations are visualised in the space in frontof the signer, and can have an impact on the artic-ulation of signs in various ways.
Also, the use ofconstructed action appears to be used in many signlanguages in similar ways.
The same holds for therich use of non-manual articulators in sentencesand the limited role of facial expressions in thelexicon: these too make sign languages across theworld very similar in appearance, even though themeaning of specific articulations may differ (Cras-born, 2006).2.2 Differences between signersJust as speakers have different voices unique toeach individual, signers have also different sign-ing styles that are likely unique to each individual.Signers?
uniqueness results from how they articu-late the shapes and movements that are specifiedby the linguistic structure of the language.
Thevariability between signers either in terms of phys-ical properties (hand sizes, colors, etc) or in termsof articulation (movements) is such that it does notaffect the understanding of the sign language byhumans, but that it may be difficult for machinesto generalize over multiple individuals.
At presentwe do not know whether the differences betweensigners using the same language are of a similar ordifferent nature than the differences between dif-ferent languages.
At the level of phonology, thereare few differences between sign languages, butthe differences in the phonetic realization of words(their articulation) may be much larger.2.3 Diverse environmentsThe visual ?activity?
of signing comes in a contextof a specific environment.
This environment caninclude the visual background and camera noises.The background objects of the video may also in-clude dynamic objects ?
increasing the ambiguityof signing activity.
The properties and configu-rations of the camera induce variations of scale,translation, rotation, view, occlusion, etc.
Thesevariations coupled with lighting conditions mayintroduce noise.
These challenges are by no meansspecific to sign interaction, and are found in manyother computer vision tasks.3 MethodOur method performs two important tasks.
First,it learns a feature representation from patches ofunlabelled raw video data (Hinton and Salakhut-dinov, 2006; Coates et al, 2011).
Second, it looksfor activations of the learned representation (byconvolution) and uses these activations to learn aclassifier to discriminate between sign languages.3.1 Unsupervised feature learningGiven samples of sign language videos (unknownsign language with one signer per video), our sys-tem performs the following steps to learn a featurerepresentation (note that these video samples areseparate from the video samples that are later usedfor classifier learning or testing):1.
Extract patches.
Extract small videos (here-after called patches) randomly from any-where in the video samples.
We fix thesize of the patches such that they all have rrows, c columns and f frames and we ex-tract patches m times.
This gives us X ={x(1), x(1), .
.
.
, x(m)}, where x(i)?
RNandN = r?c?f (the size of a patch).
For our ex-periments, we extract 100,000 patches of size15 ?
15 ?
1 (2D) and 15 ?
15 ?
2 (3D).2.
Normalize the patches.
There is evidencethat normalization and whitening (Hyv?arinenand Oja, 2000) improve performance in un-supervised feature learning (Coates et al,2011).
We therefore normalize every patchx(i)by subtracting the mean and dividing by371Figure 1: Illustration of feature extraction: convolution and pooling.the standard deviation of its elements.
For vi-sual data, normalization corresponds to localbrightness and contrast normalization.3.
Learn a feature-mapping.
Our unsuper-vised algorithm takes in the normalized andwhitened datasetX = {x(1), x(1), .
.
.
, x(m)}and maps each input vector x(i)to a new fea-ture vector of K features (f : RN?
RK).We use two unsupervised learning algorithmsa) K-means b) sparse autoencoders.
(a) K-means clustering: we train K-meansto learns K c(k)centroids that mini-mize the distance between data pointsand their nearest centroids (Coates andNg, 2012).
Given the learned centroidsc(k), we measure the distance of eachdata point (patch) to the centroids.
Natu-rally, the data points are at different dis-tances to each centroid, we keep the dis-tances that are below the average of thedistances and we set the other to zero:fk(x) = max{0, ?(z)?
zk} (1)where zk= ||x?
c(k)||2and ?
(z) is themean of the elements of z.
(b) Sparse autoencoder: we train a sin-gle layer autoencoder with K hid-den nodes using backpropagation tominimize squared reconstruction error.At the hidden layer, the features aremapped using a rectified linear (ReL)function (Maas et al, 2013) as follows:f(x) = g(Wx+ b) (2)where g(z) = max(z, 0).
Note that ReLnodes have advantages over sigmoid ortanh functions; they create sparse repre-sentations and are suitable for naturallysparse data (Glorot et al, 2011).From K-means, we get K RNcentroids and fromthe sparse autoencoder, we get W ?
RKxNandb ?
RKfilters.
We call both the centroids andfilters as the learned features.3.2 Classifier learningGiven the learned features, the feature mappingfunctions and a set of labeled training videos, weextract features as follows:1.
Convolutional extraction: Extract featuresfrom equally spaced sub-patches covering thevideo sample.2.
Pooling: Pool features together over fournon-overlapping regions of the input video toreduce the number of features.
We performmax pooling for K-means and mean poolingfor the sparse autoencoder over 2D regions(per frame) and over 3D regions (per all se-quence of frames).3.
Learning: Learn a linear classifier to predictthe labels given the feature vectors.
We uselogistic regression classifier and support vec-tor machines (Pedregosa et al, 2011).The extraction of classifier features throughconvolution and pooling is illustrated in figure 1.3724 Experiments4.1 DatasetsOur experimental data consist of videos of 30signers equally divided between six sign lan-guages: British sign language (BSL), Danish(DSL), French Belgian (FBSL), Flemish (FSL),Greek (GSL), and Dutch (NGT).
The data for theunsupervised feature learning comes from half ofthe BSL and GSL videos in the Dicta-Sign cor-pus2.
Part of the other half, involving 5 signers, isused along with the other sign language videos forlearning and testing classifiers.For the unsupervised feature learning, two typesof patches are created: 2D dimensions (15 ?
15)and 3D (15 ?
15 ?
2).
Each type consists of ran-domly selected 100,000 patches and involves 16different signers.
For the supervised learning, 200videos (consisting of 1 through 4 frames taken at astep of 2) are randomly sampled per sign languageper signer (for a total of 6,000 samples).4.2 Data preprocessingThe data preprocessing stage has two goals.First, to remove any non-signing signals that re-main constant within videos of a single sign lan-guage but that are different across sign languages.For example, if the background of the videos isdifferent across sign languages, then classifyingthe sign languages could be done with perfectionby using signals from the background.
To avoidthis problem, we removed the background by us-ing background subtraction techniques and manu-ally selected thresholds.The second reason for data preprocessing is tomake the input size smaller and uniform.
Thevideos are colored and their resolutions vary from320 ?
180 to 720 ?
576.
We converted the videosto grayscale and resized their heights to 144 andcropped out the central 144 ?
144 patches.4.3 EvaluationWe evaluate our system in terms of average accu-racies.
We train and test our system in leave-one-signer-out cross-validation, where videos fromfour signers are used for training and videos of theremaining signer are used for testing.
Classifica-tion algorithms are used with their default settingsand the classification strategy is one-vs.-rest.2http://www.dictasign.eu/5 Results and DiscussionOur best average accuracy (84.03%) is obtainedusing 500 K-means features which are extractedover four frames (taken at a step of 2).
This ac-curacy obtained for six languages is much higherthan the 78% accuracy obtained for two sign lan-guages (Gebre et al, 2013).
The latter uses lin-guistically motivated features that are extractedover video lengths of at least 10 seconds.
Our sys-tem uses learned features that are extracted overmuch smaller video lengths (about half a second).All classification accuracies are presented in ta-ble 5 for 2D and table 5 for 3D.
Classification con-fusions are shown in table 5.
Figure 2 shows fea-tures learned by K-means and sparse autoencoder.
(a) K-means features (b) SAE featuresFigure 2: All 100 features learned from 100,000patches of size 15?15.
K-means learned relativelymore curving edges than the sparse auto encoder.K-means Sparse AutoencoderK LR-L1 LR-L2 SVM LR-L1 LR-L2 SVM# of frames = 1100 69.23 70.60 67.42 73.85 74.53 71.8300 76.08 77.37 74.80 72.27 70.67 68.90500 83.03 79.88 77.92 67.50 69.38 66.20# of frames = 2100 71.15 72.07 67.42 72.78 74.62 72.08300 77.33 78.27 76.60 71.85 71.07 68.27500 83.58 79.50 79.90 67.73 70.15 66.45# of frames = 3100 71.42 73.10 67.82 65.70 67.52 63.68300 78.40 78.57 76.50 72.53 71.68 68.18500 83.48 80.05 80.57 67.85 70.85 66.77# of frames = 4100 71.88 73.05 68.70 64.93 67.48 63.80300 79.32 78.65 76.42 72.27 72.18 68.35500 84.03 80.38 80.50 68.25 71.57 67.27K = # of features, SVM = SVM with linear kernelLR-L?
= Logistic Regression with L1 and L2 penaltyTable 1: 2D filters (15?15): Leave-one-signer-outcross-validation average accuracies.3731 2 3 4 5 6 7 8 9 1012345678910BSL1 2 3 4 5 6 7 8 9 1012345678910DSL1 2 3 4 5 6 7 8 9 1012345678910FBSL1 2 3 4 5 6 7 8 9 1012345678910FSL1 2 3 4 5 6 7 8 9 1012345678910GSL1 2 3 4 5 6 7 8 9 1012345678910NGT0.00.10.20.30.40.50.60.70.80.91.0Figure 3: Visualization of coefficients of Lasso (logistic regression with L1 penalty) for each sign lan-guage with respect to each of the 100 filters of the sparse autoencoder.
The 100 filters are shown in figure2(b).
Each grid cell represents a frame and each filter is activated in 4 non-overlapping pooling regions.K-means Sparse AutoencoderK LR-L1 LR-L2 SVM LR-L1 LR-L2 SVM# of frames = 2100 70.63 69.62 68.87 67.40 66.53 65.73300 73.73 74.05 73.03 72.83 73.48 70.52500 75.30 76.53 75.40 72.28 74.65 68.72# of frames = 3100 72.48 73.30 70.33 68.68 67.40 68.33300 74.78 74.95 74.77 74.20 74.72 70.85500 77.27 77.50 76.17 72.40 75.45 69.42# of frames = 4100 74.85 73.97 69.23 68.68 67.80 68.80300 76.23 76.58 74.08 74.43 75.20 70.65500 79.08 78.63 76.63 73.50 76.23 70.53Table 2: 3D filters (15?15?2): Leave-one-signer-out cross-validation average accuracies.BSL DSL FBSL FSL GSL NGTBSL 56.11 2.98 1.79 3.38 24.11 11.63DSL 2.87 92.37 0.95 0.46 3.16 0.18FBSL 1.48 1.96 79.04 4.69 6.62 6.21FSL 6.96 2.96 2.06 60.81 18.15 9.07GSL 5.50 2.55 1.67 2.57 86.05 1.65NGT 9.08 1.33 3.98 18.76 4.41 62.44Table 3: Confusion matrix ?
confusions averagedover all settings for K-means and sparse autoen-coder with 2D and 3D filters (i.e.
for all # offrames, all filter sizes and all classifiers).Tables 5 and 5 indicate that K-means performsbetter with 2D filters and that sparse autoencoderperforms better with 3D filters.
Note that featuresfrom 2D filters are pooled over each frame andconcatenated whereas, features from 3D filters arepooled over all frames.Which filters are active for which language?Figure 3 shows visualization of the strength of fil-ter activation for each sign language.
The figureshows what Lasso looks for when it identifies anyof the six sign languages.6 Conclusions and Future WorkGiven that sign languages are under-resourced,unsupervised feature learning techniques are theright tools and our results show that this is realis-tic for sign language identification.Future work can extend this work in two direc-tions: 1) by increasing the number of sign lan-guages and signers to check the stability of thelearned feature activations and to relate these toiconicity and signer differences 2) by comparingour method with deep learning techniques.
In ourexperiments, we used a single hidden layer of fea-tures, but it is worth researching into deeper layersto improve performance and gain more insight intothe hierarchical composition of features.Other questions for future work.
How good arehuman beings at identifying sign languages?
Cana machine be used to evaluate the quality of signlanguage interpreters by comparing them to a na-tive language model?
The latter question is partic-ularly important given what happened at the Nel-son Mandela?s memorial service3.3http://www.youtube.com/watch?v=X-DxGoIVUWo374ReferencesTimothy Baldwin and Marco Lui.
2010.
Languageidentification: The long and the short of the mat-ter.
In Human Language Technologies: The 2010Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 229?237.
Association for Computational Lin-guistics.Tao Chen, Chao Huang, E. Chang, and Jingchun Wang.2001.
Automatic accent identification using gaus-sian mixture models.
In Automatic Speech Recog-nition and Understanding, 2001.
ASRU ?01.
IEEEWorkshop on, pages 343?346.Ghinwa Choueiter, Geoffrey Zweig, and PatrickNguyen.
2008.
An empirical study of automatic ac-cent classification.
In Acoustics, Speech and SignalProcessing, 2008.
ICASSP 2008.
IEEE InternationalConference on, pages 4265?4268.
IEEE.Adam Coates and Andrew Y Ng.
2012.
Learn-ing feature representations with k-means.
In Neu-ral Networks: Tricks of the Trade, pages 561?580.Springer.Adam Coates, Andrew Y Ng, and Honglak Lee.
2011.An analysis of single-layer networks in unsuper-vised feature learning.
In International Conferenceon Artificial Intelligence and Statistics, pages 215?223.H.
Cooper, E.J.
Ong, N. Pugeault, and R. Bowden.2012.
Sign language recognition using sub-units.Journal of Machine Learning Research, 13:2205?2231.Onno Crasborn, 2006.
Nonmanual structures in signlanguages, volume 8, pages 668?672.
Elsevier, Ox-ford.T.
Dunning.
1994.
Statistical identification of lan-guage.
Computing Research Laboratory, New Mex-ico State University.Dariu M Gavrila.
1999.
The visual analysis of humanmovement: A survey.
Computer vision and imageunderstanding, 73(1):82?98.Binyam Gebrekidan Gebre, Peter Wittenburg, and TomHeskes.
2013.
Automatic sign language identifica-tion.
In Proceedings of ICIP 2013.Xavier Glorot, Antoine Bordes, and Yoshua Bengio.2011.
Deep sparse rectifier networks.
In Proceed-ings of the 14th International Conference on Arti-ficial Intelligence and Statistics.
JMLR W&CP Vol-ume, volume 15, pages 315?323.Geoffrey E Hinton and Ruslan R Salakhutdinov.
2006.Reducing the dimensionality of data with neural net-works.
Science, 313(5786):504?507.Aapo Hyv?arinen and Erkki Oja.
2000.
Independentcomponent analysis: algorithms and applications.Neural networks, 13(4):411?430.Haizhou Li, Bin Ma, and Chin-Hui Lee.
2007.
Avector space modeling approach to spoken languageidentification.
Audio, Speech, and Language Pro-cessing, IEEE Transactions on, 15(1):271?284.Andrew L Maas, Awni Y Hannun, and Andrew Y Ng.2013.
Rectifier nonlinearities improve neural net-work acoustic models.
In Proceedings of the ICML.Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-fort, Vincent Michel, Bertrand Thirion, OlivierGrisel, Mathieu Blondel, Peter Prettenhofer, RonWeiss, Vincent Dubourg, et al 2011.
Scikit-learn:Machine learning in python.
The Journal of Ma-chine Learning Research, 12:2825?2830.Pamela Perniss, Robin L Thompson, and GabriellaVigliocco.
2010.
Iconicity as a general propertyof language: evidence from spoken and signed lan-guages.
Frontiers in psychology, 1.E.
Singer, PA Torres-Carrasquillo, TP Gleason,WM Campbell, and D.A.
Reynolds.
2003.
Acous-tic, phonetic, and discriminative approaches to auto-matic language identification.
In Proc.
Eurospeech,volume 9.E.
Singer, P. Torres-Carrasquillo, D. Reynolds, A. Mc-Cree, F. Richardson, N. Dehak, and D. Sturim.2012.
The mitll nist lre 2011 language recogni-tion system.
In Odyssey 2012-The Speaker and Lan-guage Recognition Workshop.Thad Starner and Alex Pentland.
1997.
Real-timeamerican sign language recognition from video us-ing hidden markov models.
In Motion-Based Recog-nition, pages 227?243.
Springer.Thad Starner, Joshua Weaver, and Alex Pentland.1998.
Real-time american sign language recogni-tion using desk and wearable computer based video.IEEE Transactions on Pattern Analysis and MachineIntelligence, 20(12):1371?1375.Sarah Taub.
2001.
Language from the body: iconicityand metaphor in American Sign Language.
Cam-bridge University Press, Cambridge.C.
Teixeira, I. Trancoso, and A. Serralheiro.
1996.
Ac-cent identification.
In Spoken Language, 1996.
IC-SLP 96.
Proceedings., Fourth International Confer-ence on, volume 3, pages 1784?1787 vol.3.Joel Tetreault, Daniel Blanchard, and Aoife Cahill.2013.
A report on the first native language identi-fication shared task.
NAACL/HLT 2013, page 48.Tingyao Wu, Jacques Duchateau, Jean-Pierre Martens,and Dirk Van Compernolle.
2010.
Feature subsetselection for improved native accent identification.Speech Communication, 52(2):83?98.Marcos Zampieri and Binyam Gebrekidan Gebre.2012.
Automatic identification of language vari-eties: The case of portuguese.
In Proceedings ofKONVENS, pages 233?237.375M.A.
Zissman.
1996.
Comparison of four approachesto automatic language identification of telephonespeech.
IEEE Transactions on Speech and AudioProcessing, 4(1):31?44.376
