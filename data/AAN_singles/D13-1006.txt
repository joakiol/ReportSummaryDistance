Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 55?60,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsAnimacy Detection with Voting ModelsJoshua L. Moore?Dept.
Computer ScienceCornell UniversityIthaca, NY 14853jlmo@cs.cornell.eduChristopher J.C. Burges Erin Renshaw Wen-tau YihMicrosoft ResearchOne Microsoft WayRedmond, WA 98052{cburges, erinren, scottyih}@microsoft.comAbstractAnimacy detection is a problem whose solu-tion has been shown to be beneficial for anumber of syntactic and semantic tasks.
Wepresent a state-of-the-art system for this taskwhich uses a number of simple classifierswith heterogeneous data sources in a votingscheme.
We show how this framework cangive us direct insight into the behavior of thesystem, allowing us to more easily diagnosesources of error.1 IntroductionAnimacy detection has proven useful for a va-riety of syntactic and semantic tasks, such asanaphora and coreference resolution (Ora?san andEvans, 2007; Lee et al 2013), verb argument dis-ambiguation (Dell?Orletta et al 2005) and depen-dency parsing (?vrelid and Nivre, 2007).
Existingapproaches for animacy detection typically rely ontwo types of information: linguistic databases, andsyntactic cues observed from the corpus.
They usu-ally combine two types of approaches: rule basedsystems, and machine learning techniques.
In thispaper we explore a slightly different angle: we wishto design an animacy detector whose decisions areinterpretable and correctable, so that downstreamsemantic modeling systems can revisit those deci-sions as needed.
Thus here, we avoid defininga large number of features and then using a ma-chine learning method such as boosted trees, sincesuch methods, although powerful, result in hard-to-interpret systems.
Instead, we explore combininginterpretable voting models using machine learning?
Work performed while visiting Microsoft Research.only to reweight their votes.
We show that suchan approach can indeed result in a high perform-ing system, with animacy detection accuracies in themid 90% range, which compares well with other re-ported rates.
Ensemble methods are well known (seefor example, Dietterich (2000)) but our focus here ison using them for interpretability while still main-taining accuracy.2 Previous Work2.1 Definitions of AnimacyPrevious work uses several different definitions ofanimacy.
Ora?san and Evans (2007) define animacyin the service of anaphora resolution: an NP is con-sidered animate ?if its referent can also be referredto using one of the pronouns he, she, him, her, his,hers, himself, herself, or a combination of such pro-nouns (e.g.
his/her )?.
Although useful for the taskat hand, this has counterintuitive consequences: forexample, baby may be considered animate or inan-imate, and ant is considered inanimate (Ibid., Fig-ure 1).
Others have argued that animacy should becaptured by a hierarchy or by categories (Aissen,2003; Silverstein, 1986).
For instance, Zaenen etal.
(2004) propose three levels of animacy (human,other animate and inanimate), which cover ten cat-egories of noun phrases, with categories like ORG(organization), ANIM (animal) and MAC (intelli-gent machines such as robots) categorised as otheranimate.
Bowman and Chopra (2012) report resultsfor animacy defined both this way and with the cat-egories collapsed to a binary (animate, inanimate)definition.552.2 Methods for Animacy DetectionEvans and Ora?san (2000) propose a rule-based sys-tem based on the WordNet taxonomy (Fellbaum,1998).
Each synset is ascribed a binary animacylabel based on its unique beginner.
A given nounis then associated with the fraction of its animatesynsets (where all synsets are taken to be animateor inanimate) and one minus that fraction, similarlyfor a given verb.
Animacy is then ascribed by ap-plying a series of rules imposing thresholds on thosefractions, together with rules (and a gazetteer) to de-tect names and acronyms, and a rule triggered by theoccurrence of who, or reflexives, in the NP.
In laterwork, Ora?san and Evans (2007) extend the algorithmby propagating animacy labels in the WordNet graphusing a chi-squared test, and then apply a k-nearestneighbor classifier based on four lexical features.
Intheir work, the only context used was the animacy ofthe verb in the NP, for heads of subject NPs (e.g., thesubject of eat is typically animate).
?vrelid (2009)and Bowman and Chopra (2012) extend this idea byusing dependency relations to generate features fortheir classifier, enabled by corpora created by Zae-nen et al(2004).
In another approach, Ji and Lin(2009) apply a simple ?relative-pronoun?
pattern tothe Google n-gram corpus (Brants and Franz, 2006)to assign animacy (see the List model in Section 5for details).
Although the animacy decision is againcontext-independent, such a list provides a strongbaseline and thus benefit applications like anaphoraresolution (Lee et al 2013).3 The TaskWe adopt a definition of animacy closest to the bi-nary version in Bowman and Chopra (2012): wedefine an entity to be animate if it is alive and hasthe ability to move under its own will.
We adoptthis simple definition because it fits well with thecommon meaning and is therefore less error prone,both in terms of incorporation into higher level mod-els, and for labeling (Ora?san and Evans (2007) re-port that the labeling of animacy tuned for anaphoraproved challenging for the judges).
We also ap-ply the label to single noun tokens where possible:the only exceptions are compound names (?SarahJones?)
which are treated as single units.
Thus,for example, ?puppy food?
is treated as two words,with puppy animate and food inanimate.
A morecomplete definition would extend this to all nounphrases, so that puppy food as a unit would be inan-imate, a notion we plan to revisit in future work.Note that even this simple definition presents chal-lenges, so that a binary label must be applied de-pending on the predominant meaning.
In ?A plateof chicken,?
chicken is treated as inanimate since itrefers to food.
In ?Caruso (1873-1921) is consid-ered one of the world?s best opera singers.
He...,?although at the time of writing clearly Caruso wasnot alive, the token is still treated as animate herebecause the subsequent writing refers to a live per-son.4 The DataWe used the MC160 dataset, which is a subset of theMCTest dataset and which is composed of 160 gradelevel reading comprehension stories generated usingcrowd sourcing (Richardson et al 2013).
Workerswere asked to write a short story (typically less than300 words) with a target audience of 5 to 7 yearolds.
The available vocabulary was limited to ap-proximately 8000 words, to model the reading abil-ity of a first or second grader.
We labeled this datafor animacy using the definition given above.
Thefirst 100 of the 160 stories were used as the trainingset, and the remaining 60 were used for the test set.These animacy labels will be made available on theweb site for MCTest (Richardson et al 2013).5 The ModelsSince one of our key goals is interpretability wechose to use an ensemble of simple voting models.Each model is able to vote for the categories Ani-mal, Person, Inanimate, or to abstain.
The distinc-tion between Animal and Person is only used whenwe combine votes, where Animal and Person votesappear as distinct inputs for the final voting combi-nation model.
Some voters do not distinguish be-tween Person and Animal, and vote for Animate orInanimate.
Our models are:List: The n-gram list method from (Ji and Lin,2009).
Here, the frequencies with which the rela-tive pronouns who, where, when, and which occurare considered.
Any noun followed most frequentlyby who is classified as Animate, and any other noun56in the list is classified as Inanimate.
This voter ab-stains when the noun is not present in the list.Anaphora Design: The WordNet-based approachof Evans and Ora?san (2000).WordNet: A simple approach using WordNet.This voter chooses Animal or Person if the uniquebeginner of the first synset of the noun is either ofthese, and Inanimate otherwise.WordSim: This voter uses the contextual vectorspace model of Yih and Qazvinian (2012) computedusing Wikipedia and LA Times data.
It uses shortlists of hand-chosen signal words for the categoriesAnimal, Person, and Inanimate to produce a ?re-sponse?
of the word to each category.
This responseis equal to the maximum cosine similarity in the vec-tor space of the query word to any signal word in thecategory.
The final vote goes to the category withthe highest response.Name: We used an in-house named entity tagger.This voter can recognize some inanimate entitiessuch as cities, but does not distinguish between peo-ple and animals, and so can only vote Animate, Inan-imate or Abstain.Dictionaries: We use three different dictionarysources (Simple English Wiktionary, Full EnglishWiktionary, and the definitions found in Word-Net) with a recursive dictionary crawling algorithm.First, we fetch the first definition of the query anduse a dependency tree and simple heuristics to findthe head noun of the definition, ignoring qualifica-tion NPs like ?piece?
or ?member.?
If this nounbelongs to a list of per-category signal words, thevoter stops and votes for that category.
Otherwise,the voter recursively runs on the found head noun.To prevent cycling, if no prediction is made after 10recursive lookups, the voter abstains.Transfer: For each story, we first process eachsentence and detect instances of the patterns xam/is/was/are/were y and y named x.
In each ofthese cases, we use majority vote of the remainingvoters to predict the animacy of y and transferits vote to x, applying this label (as a vote) to allinstances of x in the text.The WordSim and Dictionaries voters share listsof signal words, which were chosen early in the ex-perimental process using the training set.
The sig-nal words for the Animal category were animal andmammal1.
Person contains person and people.
Fi-nally, Inanimate uses thing, object, space, place,symbol, food, structure, sound, measure, and unit.We considered two methods for combining vot-ers: majority voting (where the reliable Name voteroverrides the others if it does not abstain) and a lin-ear reweighting of votes.
In the reweighting method,a feature vector is formed from the votes.
Exceptfor WordSim, this vector is an indicator vector ofthe vote ?
either Animal, Person, Animate (if thevoter doesn?t distinguish between animals and peo-ple), Inanimate, or Abstain.For Dictionaries, the vector?s non-zero compo-nent is multiplied by the number of remaining al-lowed recursive calls that can be performed, plus one(so that a success on the final lookup gives a 1).
Forexample, if the third lookup finds a signal word andchooses Animal, then the component correspondingto Animal will have a value of 9.For WordSim, instead of an indicator vector, theresponses to each category are used, or an indica-tor for abstain if the model does not contain theword.
If the word is in the model, a second vec-tor is appended containing the ratio of the maximumresponse to the second-largest response in the com-ponent for the maximum response category.
Theseper-voter feature vectors are concatenated to form a35 dimensional vector, and a linear SVM is trainedto obtain the weights for combining the votes.6 ResultsWe used the POS tagger in MSR SPLAT (Quirk etal., 2012) to extract nouns from the stories in theMC160 dataset and used these as labeled examplesfor the SVM.
This resulted in 5,120 extracted nounsin the 100 training stories and 3,009 in the 60 teststories.
We use five-fold cross-validation on thetraining set to select the SVM parameters.
57.2%of the training examples were inanimate, as were58.1% of the test examples.Table 1 gives the test accuracy of each voter.
List1This was found to work well given typical dictionary defi-nitions despite the fact that people are also mammals.57List Anaphora WNet WSim Dict Name84.6 77.1 78.8 57.6 74.3 16.0Table 1: Accuracy of various individual voters on the testset.
Abstentions are counted as errors.
Note that Transferdepends on a secondary source for classification, and istherefore not listed here.Majority SVMN+WN+D+WS+AD+L 87.7 95.0N+WN+WS 80.1 95.0N+WN+D+WS+AD+L+T 87.4 95.0N+WN+D+WS 86.4 94.8N+WN+WS+AD+L 86.5 94.7N+WN+D+WS+T 86.8 94.0N+WN+D 86.1 93.7N+WN 89.3 93.0N+D 82.6 93.0N+AD 87.6 89.4N+L 85.4 88.9Table 2: Accuracy of various combinations of votersamong Name (N), Anaphora Design (AD), List (L),WordNet (WN), WordSim (WS), Dictionary (D), andTransfer (T) under majority voting and SVM schemes.Bold indicates a statistically significant difference overthe next lower bolded entry with p < 0.01, for the SVM.comes out on top when taken alone, but we see inlater results that it is less critical when used withother voters.
Name performs poorly on its own, butlater we will see that it is a very accurate voter whichfrequently abstains.Table 2 gives the test performance of various com-binations of voters, both under majority vote andreweighting.
Statistical significance was tested us-ing a paired t-test, and bold indicates a methodwas significant over the next lower bold line withp value p < 0.01.
We see a very large gain fromthe SVM reweighting: 14.9 points in the case ofName+WordNet+WordSim.In Table 3, we show the results of ablation exper-iments on the voters.
We see that the most valuablesources of information are WordSim and Dictionar-ies.Finally, in Table 4, we show a breakdown ofwhich voters cause the most errors, for the majorityvote system.
In this table, we considered only ?fi-nal errors,?
i.e.
errors that the entire system makes.Over all such errors, we counted the number of timesMajority SVMWordSim 87.6 93.7SimpleWikt (dict) 87.3 94.1FullWikt (dict) 86.4 94.3Dict 87.4 94.5Name 86.6 94.7List 86.4 94.8WordNet (dict) 88.7 94.8WordNet 87.5 94.9Anaphora Design 88.6 94.9Transfer 87.7 95.0Table 3: Test accuracy when leaving out various voters,using both majority vote and and reweighting.
Bold indi-cates statistical significance over the next lower bold linewith p < 0.01.each voter chose incorrectly, giving a count of howmany times each voter contributed to a final error.We see that the Anaphora Design system has thelargest number of errors on both train and test sets.After this, WordNet, List, and WordNet (dict) are alsolarge sources of error.
On the other hand, Name andWordSim have very few errors, indicating high re-liability.
The table also gives the number of criti-cal errors, where the voter selected the wrong cate-gory and was a deciding vote (that is, when chang-ing its vote would have resulted in a correct overallclassification).
We see a similar pattern here, withAnaphora Design causing the most errors and Word-Sim and Name among the most reliable.
We includedAnaphora Design even though it uses a different def-inition of animacy, to determine if its vote was nev-ertheless valuable.Error tables such as these show how voting mod-els are more interpretable and therefore correctablecompared to more complex learned models.
The ta-bles indicate the largest sources of error and sug-gest changes that could be made to increase accu-racy.
For example, we could make significant gainsby improving WordNet, WordNet (dictionary), orList, whereas there is relatively little reason to ad-just WordSim or Name.7 ConclusionsWe have shown that linear combinations of votingmodels can give animacy detection rates in the mid90% range.
This is well above the accuracy found58Errors CriticalTrain Test Train TestAnaphora Design 555 266 117 76WordNet 480 228 50 45List 435 195 94 45Transfer 410 237 54 58WordNet (dict) 385 194 84 65SimpleWikt (dict) 175 111 39 16FullWikt (dict) 158 67 1 5WordSim 107 89 11 19Name 71 55 27 19Table 4: Errors column: number of errors on train andtest where a source voted incorrectly, and was thus atleast in part responsible for an error of the overall sys-tem.
Critical column: number of errors on train and testwhere a source voted incorrectly, and in addition cast adeciding vote.
Results are for majority vote.by using the n-gram method of (Ji and Lin, 2009),which is used as an animacy detection componentin other systems.
In this sense the work presentedhere improves upon the state of the art, but there arecaveats, since other workers define animacy differ-ently and so a direct comparison with their work isnot possible.
Our method has the added advantageof interpretability, which we believe will be usefulwhen using it as a component in a larger system.AcknowledgmentsWe wish to thank Andrzej Pastusiak for his help withthe labeling tool.ReferencesJudith Aissen.
2003.
Differential object marking:Iconicity vs. economy.
Natural Language & Linguis-tic Theory, 21(3):435?483.Samuel Bowman and Harshit Chopra.
2012.
Automaticanimacy classification.
In Proceedings of the NAACL-HLT 2012 Student Research Workshop.Thorsten Brants and Alex Franz.
2006.
Web 1T 5-gramVersion 1.
Linguistic Data Consortium.Felice Dell?Orletta, Alessandro Lenci, Simonetta Monte-magni, and Vito Pirrelli.
2005.
Climbing the path togrammar: a maximum entropy model of subject/objectlearning.
In Proceedings of the Workshop on Psy-chocomputational Models of Human Language Acqui-sition, PMHLA ?05, pages 72?81, Stroudsburg, PA,USA.
Association for Computational Linguistics.Thomas G. Dietterich.
2000.
Ensemble methods in ma-chine learning.
In Multiple Classifier Systems, pages1?15.Richard Evans and Constantin Ora?san.
2000.
Improv-ing anaphora resolution by identifying animate entitiesin texts.
In Proceedings of the Discourse Anaphoraand Reference Resolution Conference (DAARC2000),pages 154?162.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
The MIT Press.Heng Ji and Dekang Lin.
2009.
Gender and animacyknowledge discovery from Web-scale n-grams for un-supervised person mention detection.
In Proceedingsof the 23rd Pacific Asia Conference on Language, In-formation and Computation, pages 220?229, HongKong, December.
City University of Hong Kong.Heeyoung Lee, Angel Chang, Yves Peirsman, NathanaelChambers, Mihai Surdeanu, and Dan Jurafsky.
2013.Deterministic coreference resolution based on entity-centric, precision-ranked rules.
Computational Lin-guistics, 39(4).Constantin Ora?san and Richard J. Evans.
2007.
NP an-imacy identification for anaphora resolution.
Journalof Artificial Intelligence Research (JAIR), 29:79?103.Lilja ?vrelid and Joakim Nivre.
2007.
When word or-der and part-of-speech tags are not enough ?
Swedishdependency parsing with rich linguistic features.
InProceedings of the International Conference on RecentAdvances in Natural Language Processing (RANLP),pages 447?451.Lilja ?vrelid.
2009.
Empirical evaluations of animacyannotation.
In Proceedings of the 12th Conference ofthe European Chapter of the Association for Compu-tational Linguistics (EACL).Chris Quirk, Pallavi Choudhury, Jianfeng Gao, HisamiSuzuki, Kristina Toutanova, Michael Gamon, Wen-tau Yih, Colin Cherry, and Lucy Vanderwende.
2012.MSR SPLAT, a language analysis toolkit.
In Proceed-ings of the Demonstration Session at the Conferenceof the North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 21?24, Montre?al, Canada, June.
As-sociation for Computational Linguistics.Matthew Richardson, Chris Burges, and Erin Renshaw.2013.
MCTest: A challenge dataset for the open-domain machine comprehension of text.
In Confer-ence on Empirical Methods in Natural Language Pro-cessing (EMNLP).Michael Silverstein.
1986.
Hierarchy of features andergativity.
In P. Muysken and H. van Riemsdijk, ed-itors, Features and Projections, pages 163?232.
ForisPublications Holland.Wen-tau Yih and Vahed Qazvinian.
2012.
Measur-ing word relatedness using heterogeneous vector space59models.
In Proceedings of NAACL-HLT, pages 616?620, Montre?al, Canada, June.Annie Zaenen, Jean Carletta, Gregory Garretson, JoanBresnan, Andrew Koontz-Garboden, Tatiana Nikitina,M.
Catherine O?Connor, and Tom Wasow.
2004.
An-imacy encoding in English: Why and how.
In Bon-nie Webber and Donna K. Byron, editors, ACL 2004Workshop on Discourse Annotation, pages 118?125,Barcelona, Spain, July.
Association for ComputationalLinguistics.60
