Proceedings of the 14th European Workshop on Natural Language Generation, pages 40?50,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsGenerating Elliptic CoordinationClaire GardentCNRS, LORIA, UMR 7503Vandoeuvre-le`s-Nancy, F-54500, Franceclaire.gardent@loria.frShashi NarayanUniversite?
de Lorraine, LORIA, UMR 7503Villers-le`s-Nancy, F-54600, Franceshashi.narayan@loria.frAbstractIn this paper, we focus on the task of gen-erating elliptic sentences.
We extract fromthe data provided by the Surface Realisa-tion (SR) Task (Belz et al 2011) 2398 in-put whose corresponding output sentencecontain an ellipsis.
We show that 9% of thedata contains an ellipsis and that both cov-erage and BLEU score markedly decreasefor elliptic input (from 82.3% coverage fornon-elliptic sentences to 65.3% for ellip-tic sentences and from 0.60 BLEU scoreto 0.47).
We argue that elided materialshould be represented using phoneticallyempty nodes and we introduce a set ofrewrite rules which permits adding theseempty categories to the SR data.
Finally,we evaluate an existing surface realiser onthe resulting dataset.
We show that, afterrewriting, the generator achieves a cover-age of 76% and a BLEU score of 0.74 onthe elliptical data.1 IntroductionTo a large extent, previous work on generating el-lipsis has assumed a semantically fully specifiedinput (Shaw, 1998; Harbusch and Kempen, 2009;Theune et al 2006).
Given such input, ellipticsentences are then generated by first producingfull sentences and second, deleting from these sen-tences substrings that were identified to obey dele-tion constraints.In contrast, recent work on generation often as-sumes input where repeated material has alreadybeen elided.
This includes work on sentence com-pression which regenerates sentences from surfacedependency trees derived from parsing the initialtext (Filippova and Strube, 2008); Surface realisa-tion approaches which have produced results forregenerating from the Penn Treebank (Langkilde-Geary, 2002; Callaway, 2003; Zhong and Stent,2005; Cahill and Van Genabith, 2006; White andRajkumar, 2009); and more recently, the SurfaceRealisation (SR) Task (Belz et al 2011) whichhas proposed dependency trees and graphs de-rived from the Penn Treebank (PTB) as a com-mon ground input representation for testing andcomparing existing surface realisers.
In all theseapproaches, repeated material is omitted from therepresentation that is input to surface realisation.As shown in the literature, modelling the inter-face between the empty phonology and the syn-tactic structure of ellipses is a difficult task.
Forparsing, Sarkar and Joshi (1996), Banik (2004)and Seddah (2008) propose either to modify thederivation process of Tree Adjoining Grammaror to introduce elementary trees anchored withempty category in a synchronous TAG to accom-modate elliptic coordinations.
In HPSG (Head-Driven Phrase Structure Grammar), Levy and Pol-lard (2001) introduce a neutralisation mechanismto account for unlike constituent coordination ;in LFG (Lexical Functional Grammar), Dalrym-ple and Kaplan (2000) employ set values to modelcoordination; in CCG (Combinatory CategorialGrammar, (Steedman, 1996)), it is the non stan-dard notion of constituency assumed by the ap-proach which permits accounting for coordinatedstructures; finally, in TLCG (Type-Logical Cat-egorial Grammar), gapping is treated as like-category constituent coordinations (Kubota andLevine, 2012).In this paper, we focus on how surface reali-sation handles elliptical sentences given an inputwhere repeated material is omitted.
We extractfrom the SR data 2398 input whose correspond-ing output sentence contain an ellipsis.
Based onprevious work on how to annotate and to representellipsis, we argue that elided material should berepresented using phonetically empty nodes (Sec-tion 3) and we introduce a set of rewrite ruleswhich permits adding these empty categories to40the SR data (Section 4).
We then evaluate our sur-face realiser (Narayan and Gardent, 2012b) on theresulting dataset (Section 5) and we show that, onthis data, the generator achieves a coverage of 76%and a BLEU score, for the generated sentences, of0.74.
Section 6 discusses related work on generat-ing elliptic coordination.
Section 7 concludes.2 Elliptic SentencesElliptic coordination involves a wide range of phe-nomena including in particular non-constituentcoordination (1, NCC) i.e., cases where sequencesof constituents are coordinated; gapping (2, G)i.e., cases where the verb and possibly someadditional material is elided; shared subjects (3,SS) and right node raising (4, RNR) i.e., caseswhere a right most constituent is shared by two ormore clauses1.
(1) [It rose]i 4.8 % in June 1998 and i 4.7% inJune 1999.
NCC(2) Sumitomo bank [donated]i $500, 000, Tokyoprefecture i $15, 000 and the city of Osaka i$10, 000 .
Gapping(3) [the state agency ?s figures]i i confirm pre-vious estimates and i leave the index at 178.9 .Shared Subject(4) He commissions i and splendidly interpretsi [fearsome contemporary scores]i .
RNRWe refer to the non elliptic clause as the sourceand to the elliptic clause as the target.
In thesource, the brackets indicate the element sharedwith the target while in the target, the i sign in-dicate the elided material with co-indexing indi-cating the antecedent/ellipsis relation.
In gappingclauses, we refer to the constituents in the gappedclause, as remnants.3 Representing and Annotating ElidedMaterialWe now briefly review how elided material is rep-resented in the literature.Linguistic Approaches.
While Sag (1976),Williams (1977), Kehler (2002), Merchant (2001)1Other types of elliptic coordination include sluicing andVerb-Phrase ellipsis.
These will not be discussed here be-cause they can be handled by the generator by having theappropriate categories in the grammar and the lexicon e.g.,in a Tree Adjoining Grammar, an auxiliary anchoring a verbphrase for VP ellipsis and question words anchoring a sen-tence for sluicing.and van Craenenbroeck (2010) have argued fora structural approach i.e., one which posits syn-tactic structure for the elided material, Keenan(1971), Hardt (1993), Dalrymple et al(1991),Ginzburg and Sag (2000) and Culicover and Jack-endoff (2005) all defend a non structural approach.Although no consensus has yet been reached onthese questions, many of these approaches do pos-tulate an abstract syntax for ellipsis.
That is theyposit that elided material licenses the introductionof phonetically empty categories in the syntax orat some more abstract level (e.g., the logical formof generative linguistics).Treebanks.
Similarly, in computational linguis-tics, the treebanks used to train and evaluateparsers propose different means of representing el-lipsis.For phrase structure syntax, the Penn TreebankBracketing Guidelines extensively describe how toannotate coordination and missing material in En-glish (Bies et al 1995).
For shared complements(e.g., shared subject and right node raising con-structions), these guidelines state that the elidedmaterial licenses the introduction of an empty*RNR* category co-indexed with the shared com-plement (cf.
Figure 1) while gapping construc-tions are handled by labelling the gapping rem-nants (i.e., the constituents present in the gappingclause) with the index of their parallel element inthe source (cf.
Figure 2).
(S(VP (VB Do)(VP (VB avoid)(S (VP (VPG puncturing(NP *RNR*-5))(CC or)(VP (VBG cutting)(PP (IN into)(NP *RNR*-5)))(NP-5 meats)))))Figure 1: Penn Treebank annotation for RightNode Raising ?Do avoid puncturing i or cutting into i[meats]i.?
(S(S (NP-SBJ-10 Mary)(VP (VBZ likes) (NP-11 potatoes)))(CC and)(S (NP-SBJ=10 Bill)(, ,) (NP=11 ostriches)))Figure 2: Penn Treebank annotation for gapping?Mary [likes]i potatoes and Bill i ostriches.
?In dependency treebanks, headless elliptic con-structs such as gapping additionally raise the is-41sue of how to represent the daughters of an emptyhead.
Three main types of approaches have beenproposed.
In dependency treebanks for German(Daum et al 2004; Hajic?
et al 2009) and inthe Czech treebank ( ?Cmejrek et al 2004; Hajic?et al 2009), one of the dependents of the head-less phrase is declared to be the head.
This isa rather undesirable solution because it hides thefact that there the clause lacks a head.
In contrast,the Hungarian dependency treebank (Vincze et al2010) explicitly represents the elided elements inthe trees by introducing phonetically empty ele-ments that serve as attachment points to other to-kens.
This is the cleanest solution from a linguisticpoint of view.
Similarly, Seeker and Kuhn (2012)present a conversion of the German Tiger treebankwhich introduces empty nodes for verb ellipses ifa phrase normally headed by a verb is lacking ahead.
They compare the performance of two sta-tistical dependency parsers on the canonical ver-sion and the CoNLL 2009 Shared Task data andshow that the converted dependency treebank theypropose yields better parsing results than the tree-bank not containing empty heads.In sum, while some linguists have argued for anapproach where ellipsis has no syntactic represen-tation, many have provided strong empirical evi-dence for positing empty nodes as place-holdersfor elliptic material.
Similarly, in devising tree-banks, computational linguists have oscillated be-tween representations with and without empty cat-egories.
In the following section, we present theway in which elided material is represented in theSR data; we show that it underspecifies the sen-tences to be generated; and we propose to mod-ify the SR representations by making the relation-ship between ellipsis and antecedent explicit usingphonetically empty categories and co-indexing.4 Rewriting the SR DataThe SR Task 2011 made available two types ofdata for surface realisers to be tested on: shallowdependency trees and deep dependency graphs.Here we focus on the shallow dependency treesi.e., on syntactic structures.The input data provided by the SR Task wereobtained from the Penn Treebank.
They werederived indirectly from the LTH Constituent-to-Dependency Conversion Tool for Penn-style Tree-banks (Pennconverter, (Johansson and Nugues,2007)) by post-processing the CoNLL data to re-d o n a t eSumitomo bankSBJa n dCOORD$ 5 0 0 , 0 0 0OBJTokyo prefectureGAP-SBJ$ 1 5 , 0 0 0GAP-OBJFigure 3: Gapping in the SR data.
?Sumitomo bank[donated]i $500, 000 and Tokyo prefecture i $15, 000.?move word order, inflections etc.
It consists ofa set of unordered labelled syntactic dependencytrees whose nodes are labelled with word forms,part of speech categories, partial morphosyntac-tic information such as tense and number and, insome cases, a sense tag identifier.
The edges arelabelled with the syntactic labels provided by thePennconverter.
All words (including punctuation)of the original sentence are represented by a nodein the tree.
Figures 3, 4, 5 and 6 show (simplified)input trees from the SR data.In the SR data, the representation of ellipsisadopted in the Penn Treebank is preserved modulosome important differences regarding co-indexing.Gapping is represented as in the PTB by la-belling the remnants with a marker indicating thesource element parallel to each remnant.
Howeverwhile in the PTB, this parallelism is made explicitby co-indexing (the source element is marked withan index i and its parallel target element with themarker = i), in the SR data this parallelism is ap-proximated using functions.
For instance, if theremnant is parallel to the source subject, it will belabelled GAP-SBJ (cf.
Figure 3).commissionHeSBJa n dCOORDfearsome contemporary  scoreOBJsplendidly interpretCONJFigure 4: Subject Sharing and RNR in the SRdata.
?
[He]j j commissions i and j splendidly interpretsi [fearsome contemporary scores]i .
?For right-node raising and shared subjects, thecoindexation present in the PTB is dropped in theSR data.
As a result, the SR representation under-42b eTheySBJs h o wVCn o tADVt a k eOPRDJames MadisonOBJorCOORDa puffOBJlight upCONJFigure 5: Non shared Object ?They aren?t showingJames Madison taking a puff or lighting up?riseItSBJa n dCOORD4.8  %EXTin June 1998TMP4.7  %GAP-EXTin June 1999GAP-TMPFigure 6: NCC in the SR data.
?It rose 4.8 % in June1998 and 4.7% in June 1999.?V VFUNC COORD FUNC COORDX CONJ W X CONJ WGAP-FUNC ?
CONJY VFUNCY WFigure 7: Gapping and Non Constituent Coordina-tion structures rewriting (V: a verb, CONJ: a con-junctive coordination, X, Y and W three sets of de-pendents).
The antecedent verb (V) and the sourcematerial without counterpart in the gapping clause(W) are copied over to the gapping clause andmarked as phonetically empty.specifies the relation between the object and thecoordinated verbs in RNR constructions: the ob-ject could be shared as in He commissions i and splen-didly interprets i [fearsome contemporary scores]i .
(Fig-ure 4) or not as in They aren?t showing James Madisontaking a puff or lighting up (Figure 5).
In both cases,the representation is the same i.e., the shared ob-ject (fearsome contemporary scores) and the unsharedobject (a puff ) are both attached to the first verb.Finally, NCC structures are handled in the sameway as gapping by having the gapping remnantslabelled with a GAP prefixed function (e.g., GAP-SBJ) indicating which element in the source thegapping remnant is parallel to (cf.
Figure 6).Summing up, the SR representation schema un-derspecifies ellipsis in two ways.
For gapping andnon-constituent coordination, it describes paral-lelism between source and target elements ratherthan specifying the syntax of the elided material.For subject sharing and right node raising, it failsto explicitly specify argument sharing.V1 V1SUBJ COORD SUBJ COORDX CONJ Y1 X CONJ Y1CONJ ?
CONJV2 V2SUBJY2 X Y2Figure 8: Subject sharing: the subject dependentis copied over to the target clause and marked asphonetically empty.To resolve this underspecification, we rewritethe SR data using tree rewrite rules as follows.In Gapping and NCC structures, we copy thesource material that has no (GAP- marked) coun-terpart in the target clause to the target clausemarking it to indicate a phonetically empty cate-gory (cf.
Figure 7).For Subject sharing, we copy the shared subjectof the source clause in the target clause and mark itto be a phonetically empty category (cf.
Figure 8).For Right-Node-Raising, we unfold the am-biguity producing structures where argumentspresent in the source but not in the target are op-tionally copied over to the target (cf.
Figure 9).These rewrite rules are implemented efficiently43V1 V1 V1COORD OBJ COORD OBJ COORD OBJX1 CONJ Y X1 CONJ Y X1 CONJ YCONJ ?
{CONJ , CONJ}V2 V2 V2OBJX2 X2 X2 YFigure 9: Right-Node-Raising: the object dependent is optionally copied over to the target clause andmarked as phonetically empty in the source clause.using GrGen, an efficient graph rewriting sys-tem (Gei?et al 2006).5 Generating Elliptic Coordination5.1 The Surface RealiserTo generate sentences from the SR data, weuse our surface realiser (Narayan and Gardent,2012b), a grammar-based generator based on aFeature-Based Lexicalised Tree Adjoining Gram-mar (FB-LTAG) for English.
This generator firstselects the elementary FB-LTAG trees associatedin the lexicon with the lemmas and part of speechtags associated with each node in the input de-pendency tree.
It then attempts to combine theselected trees bottom-up taking into account thestructure of the input tree (only trees that are se-lected by nodes belonging to the same local inputtree are tried for combination).
A language modelis used to implement a beam search letting throughonly the n most likely phrases at each bottom upcombination step.
In this experiment, we set n to5.
The generator thus outputs at most 5 sentencesfor each input.Figure 10: Gapping after rewriting ?Sumitomo bank[donated]i $500, 000 and Tokyo prefecture i $15, 000.?As mentioned in the introduction, most compu-tational grammars have difficulty accounting forellipses and FB-LTAG is no exception.The difficulty stems from the fact that in ellip-tical sentences, there is meaning without sound.As a result, the usual form/meaning mappings thatin non-elliptic sentences allow us to map soundsonto their corresponding meanings, break down.For instance, in the sentence John eats applesand Mary pear, the Subject-Verb-Object structurewhich can be used in English to express a binaryrelation is present in the source clause but not inthe elided one.
In practice, the syntax of ellipti-cal sentences often leads to a duplication of thegrammatical system, one system allowing for nonelliptical sentences and the other for their elidedcounterpart.For parsing with TAG, two main methods havebeen proposed for processing elliptical sentences.
(Sarkar and Joshi, 1996) introduces an additionaloperation for combining TAG trees which yieldsderivation graphs rather than trees.
(Seddah, 2008)uses Multi-Component TAG and proposes to asso-ciate each elementary verb tree with an elliptic treewith different pairs representing different types ofellipses.We could use either of these approaches forgeneration.
The first approach however has thedrawback that it leads to a non standard notion ofderivation (the derivation trees become derivationgraphs).
The second on the other hand, induces aproliferation of trees in the grammar and impactsefficiency.Instead, we show that, given an input enrichedwith empty categories as proposed in the previoussection, neither the grammar nor the tree combi-nation operation need changing.
Indeed, our FB-LTAG surface realiser directly supports the gen-eration of elliptic sentences.
It suffices to as-sume that an FB-LTAG elementary tree may be an-chored by the empty string.
Given an input nodemarked as phonetically empty, the generator will44then select all FB-LTAG rules that are compatiblewith the lexical and the morpho-syntactic featureslabelling that node.
Generation will then proceedas usual by composing the trees selected on the ba-sis of the input using substitution and adjunction;and by retrieving from the generation forest thosesentences whose phrase structure tree covers theinput.For instance, given the rewritten input shown inFigure 10, the TAG trees associated in the lexi-con with donate will be selected; anchored withthe empty string and combined with the TAG treesbuilt for Tokyo Prefecture and $15,000 thus yield-ing the derivation shown in Figure 11.NPTokyo prefectureSNP?
VPV NP?NP$15,000Figure 11: Derivation for ?Tokyo prefecture  $15,000?5.2 The DataWe use both the SR test data (2398 sentences) andthe SR training data (26604 sentences) to evaluatethe performance of the surface realiser on ellipticcoordination.
Since the realiser we are using isnot trained on this data (the grammar was writtenmanually), this does not bias evaluation.
Usingthe training data allows us to gather a larger set ofelliptic sentences for evaluation while evaluatingalso on the test data allows comparison with otherrealisers.To focus on ellipses, we retrieve those sentenceswhich were identified by our rewrite rules as po-tentially containing an elliptic coordination.
Inessence, these rewrite rules will identify all casesof non-constituent coordination and gapping (be-cause these involve GAP-X dependencies with ?X?a dependency relation and are therefore easily de-tected) and of shared-subjects (because the treepatterns used to detect are unambiguous i.e., onlyapply if there is indeed a shared subject).
ForRNR, as discussed in the previous section, the SRformat is ambiguous and consequently, the rewriterules might identify as object sharing cases wherein fact the object is not shared.
As noted by oneof our reviewers, the false interpretation could beElliptic Coordination DataElliptic Coordination Pass BLEU ScoresCOV ALLRNR (384)Before 66% 0.68 0.45After 81% 0.70 0.57Delta +15 +0.02 +0.12SS (1462)Before 70% 0.74 0.52After 75% 0.75 0.56Delta +5 +0.01 +0.04SS + RNR(456)Before 61% 0.71 0.43After 74% 0.73 0.54Delta +13 +0.02 +0.11Gapping (36)Before 3% 0.53 0.01After 67% 0.74 0.49Delta +64 +0.21 +0.48NCC (60)Before 5% 0.68 0.03After 73% 0.74 0.54Delta +68 +0.06 +0.51Total (2398)Before 65% 0.72 0.47After 76% 0.74 0.56Delta +11 +0.02 +0.09Table 1: Generation results on elliptical data be-fore and after input rewriting (SS: Shared Subject,NCC: Non Constituent Coordination, RNR: RightNode Raising).
The number in brackets in the firstcolumn is the number of cases.
Pass stands forthe coverage of the generator.
COV and ALL inBLEU scores column stand for BLEU scores forthe covered and the total input data.dropped out by consulting the Penn Treebank2.The approach would not generalise to other datahowever.In total, we retrieve 2398 sentences3 potentiallycontaining an elliptic coordination from the SRtraining data.
The number and distribution of thesesentences in terms of ellipsis types are given in Ta-ble 1.
From the test data, we retrieve an additional182 elliptic sentences.5.3 EvaluationWe ran the surface realiser on the SR input databoth before and after rewriting elliptic coordina-tions; on the sentences estimated to contain ellip-sis; on sentences devoid of ellipsis; and on all sen-tences.
The results are shown in Table 2.
Theyindicate coverage and BLEU score before and af-ter rewriting.
BLEU score is given both with re-spect to covered sentences (COV) i.e., the set ofinput for which generation succeeds; and for allsentences (ALL).
We evaluate both with respect tothe SR test data and with respect to the SR training2The Penn Treebank makes the RNR interpretations ex-plicit (refer to Figure 1).3It is just a coincidence that the size of the SR test dataand the number of extracted elliptic sentences are the same.45SR Data Pass BLEU ScoresCOV ALLTest+E (182)Before 58% 0.59 0.34After 67% 0.59 0.40Delta +9 +0.00 +0.06-E (2216)Before 80% 0.59 0.47After 80% 0.59 0.48Delta +0 +0.00 +0.01T (2398)Before 78% 0.58 0.46After 79% 0.59 0.47Delta +1 +0.01 +0.01Training+E (2398)(Table 1)Before 65% 0.72 0.47After 76% 0.74 0.56Delta +11 +0.02 +0.09-E (24206)Before 82% 0.73 0.60After 82% 0.73 0.60Delta +0 +0.00 +0.00T (26604)Before 81% 0.72 0.58After 82% 0.73 0.60Delta +1 +0.01 +0.02Table 2: Generation results on SR test and SRtraining data before and after input rewriting (+Estands for elliptical data, -E for non elliptical dataand T for total.)data.
We use the SR Task scripts for the computa-tion of the BLEU score.The impact of ellipsis on coverage and preci-sion.
Previous work on parsing showed that co-ordination was a main source of parsing failure(Collins, 1999).
Similarly, ellipses is an importantsource of failure for the TAG generator.
Ellipsesare relatively frequent with 9% of the sentencesin the training data containing an elliptic struc-ture and performance markedly decreases in thepresence of ellipsis.
Thus, before rewriting, cov-erage decreases from 82.3% for non-elliptic sen-tences to 80.75% on all sentences (elliptic and nonelliptic sentences) and to 65.3% on the set of el-liptic sentences.
Similarly, BLEU score decreasesfrom 0.60 for non elliptical sentences to 0.58 forall sentences and to 0.47 for elliptic sentences.
Insum, both coverage and BLEU score decrease asthe number of elliptic input increases.The impact of the input representation on cov-erage and precision.
Recent work on treebankannotation has shown that the annotation schemaadopted for coordination impacts parsing.
In par-ticular, Maier et al(2012) propose revised annota-tion guidelines for coordinations in the Penn Tree-bank whose aim is to facilitate the detection of co-ordinations.
And Dukes and Habash (2011) showthat treebank annotations which include phoneti-cally empty material for representing elided mate-rial allows for better parsing results.Similarly, Table 2 shows that the way in whichellipsis is represented in the input data has a strongimpact on generation.
Thus rewriting the inputdata markedly extends coverage with an overallimprovement of 11 points (from 65% to 76%) forelliptic sentences and of almost 1 point for all sen-tences.As detailed in Table 1 though, there are impor-tant differences between the different types of el-liptic constructs: coverage increases by 68 pointsfor NCC and 64 points for gapping against only15, 13 and 5 points for RNR, mixed RNR-SharedSubject and Shared Subject respectively.
The rea-son for this is that sentences are generated formany input containing the latter types of con-structions (RNR and Shared Subject) even with-out rewriting.
In fact, generation succeeds on thenon rewritten input for a majority of RNR (66%PASS), Shared Subject (70% PASS) and mixedRNR-Shared Subject (61% PASS) constructionswhereas it fails for almost all cases of gapping(3% PASS) and of NCC (5% PASS).
The reasonfor this difference is that, while the grammar can-not cope with headless constructions such as gap-ping and NCC constructions, it can often providea derivation for shared subject sentences by usingthe finite verb form in the source sentence and thecorresponding infinitival form in the target.
Sincethe infinitival does not require a subject, the tar-get sentence is generated.
Similarly, RNR con-structions can be generated when the verb in thesource clause has both a transitive and an intran-sitive form: the transitive form is used to gener-ate the source clause and the intransitive for thetarget clause.
In short, many sentences contain-ing a RNR or a shared subject construction can begenerated without rewriting because the grammarovergenerates i.e., it produces sentences which arevalid sentences of English but whose phrase struc-ture tree is incorrect.Nevertheless, as the results show, rewritingconsistently helps increasing coverage even forRNR (+15 points), Shared Subject (+5 points) andmixed RNR-Shared Subject (+13 points) construc-tions because (i) not all verbs have both a transi-tive and an intransitive verb form and (ii) the inputfor the elliptic clause may require a finite form forthe target verb (e.g., in sentences such as ?
[they]iweren?t fired but instead i were neglected?
where the tar-get clause includes an auxiliary requiring a past46participial which in this context requires a sub-ject).Precision is measured using the BLEU score.For each input, we take the best score obtainedwithin the 5 derivations4 produced by the gener-ator.
Since the BLEU score reflects the degree towhich a sentence generated by the system matchesthe corresponding Penn Treebank sentence, it isimpacted not just by elliptic coordination but alsoby all linguistic constructions present in the sen-tence.
Nonetheless, the results show that rewrit-ing consistently improves the BLEU score with anoverall increase of 0.09 points on the set of ellip-tic sentences.
Moreover, the consistent improve-ment in terms of BLEU score for generated sen-tences (COV column) shows that rewriting simul-taneously improves both coverage and precisionthat is, that for those sentences that are generated,rewriting consistently improves precision.Analysing the remaining failure cases.
To bet-ter assess the extent to which rewriting and the FB-LTAG generation system succeed in generating el-liptic coordinations, we performed error miningon the elliptic data using our error miner describedin (Narayan and Gardent, 2012a).
This methodpermits highlighting the most likely sources of er-ror given two datasets: a set of successful casesand a set of failure cases.
In this case, the suc-cessful cases is the subset of rewritten input datafor elliptic coordination cases for which genera-tion succeeds .
The failure cases is the subset forwhich generation fails.
If elliptic coordination wasstill a major source of errors, input nodes or edgeslabelled with labels related to elliptic coordination(e.g., the COORD and the GAP-X dependency rela-tions or the CONJ part of speech tag) would sur-face as most suspicious forms.
In practice how-ever, we found that the 5 top sources of errorshighlighted by error mining all include the DEP re-lation, an unknown dependency relation used bythe Pennconverter when it fails to assign a labelto a dependency edge.
In other words, most of theremaining elliptic cases for which generation fails,fails for reasons unrelated to ellipsis.Comparison with other surface realisersThere is no data available on the performanceof surface realisers on elliptic input.
However,the performance of the surface realiser can be4The language model used in the generator allows only 5likely derivations (refer to section 5.1).compared with those participating in the shallowtrack of the SR challenge.
On the SR trainingdata, the TAG surface realiser has an averagerun time of 2.78 seconds per sentence (with anaverage of 20 words per sentence), a coverageof 82% and BLEU scores of 0.73 for coveredand 0.60 for all.
On the SR test data, the realiserachieves a coverage of 79% and BLEU scores of0.59 for covered and 0.47 for all.
In comparison,the statistical systems in the SR Tasks achieved0.88, 0.85 and 0.67 BLEU score on the SR testset and the best symbolic system 0.25 (Belz et al2011).6 Related workPrevious work on generating elliptic sentences hasmostly focused on identifying material that couldbe elided and on defining procedures capable ofproducing input structures for surface realisationthat support the generation of elliptic sentences.Shaw (1998) developed a sentence plannerwhich generates elliptic sentences in 3 steps.
First,input data are grouped according to their simi-larities.
Second, repeated elements are marked.Third, constraints are used to determine which oc-currences of a marked element should be deleted.The approach is integrated in the PLANDoc sys-tem (McKeown et al 1994) and shown to gen-erate a wide range of elliptic constructs includ-ing RNR, VPE and NCC using FUF/SURGE (El-hadad, 1993), a realisation component based onFunctional Unification Grammar.Theune et al(2006) describe how elliptic sen-tences are generated in a story generation system.The approach covers conjunction reduction, rightnode raising, gapping and stripping and uses de-pendency trees connected by rhetorical relationsas input.
Before these trees are mapped to sen-tences, repeated elements are deleted and their an-tecedent (thesource element) is related by a SUB-ORROWED relation to their governor in the ellip-tic clause and a SUIDENTICAL relation to theirgovernor in the antecedent clause.
This is then in-terpreted by the surface realiser to mean that therepeated element should be realised in the sourceclause, elided in the target clause and that it li-censes the same syntactic structure in both clauses.Harbusch and Kempen (2009) have proposed amodule called Elleipo which takes as input unre-duced, non-elliptic, syntactic structures annotatedwith lexical identity and coreference relationships47between words and word groups in the conjuncts;and returns as output structures annotated withelision marks indicating which elements can beelided and how (i.e., using which type of ellip-sis).
The focus is on developing a language in-dependent module which can mediate between theunreduced input syntactic structures produced by agenerator and syntactic structures that are enrichedwith elision marks rich enough to determine therange of possible elliptic and non elliptic outputsentences.In CCG, grammar rules (type-raising and com-position) permit combining non constituents into afunctor category which takes the shared element asargument; and gapping remnants into a clause tak-ing as argument its left-hand coordinated sourceclause.
White (2006) describes a chart based algo-rithm for generating with CCG and shows that itcan efficiently realise NCC and gapping construc-tions.Our proposal differs from these approaches inthat it focuses on the surface realisation stage (as-suming that the repeated elements have alreadybeen identified) and is tested on a large corpusof newspaper sentences rather than on hand-madedocument plans and relatively short sentences.7 ConclusionIn this paper, we showed that elliptic structuresare frequent and can impact the performance ofa surface realiser.
In line with linguistic theoryand with some recent results on treebank annota-tion, we argued that the representation of ellipsisshould involve empty categories and we provideda set of tree rewrite rules to modify the SR data ac-cordingly.
We then evaluated the performance of aTAG based surface realiser on 2398 elliptic inputderived by the SR task from the Penn Treebankand showed that it achieved a coverage of 76% anda BLEU score of 0.74 on generated sentences.
Ourapproach relies both on the fact that the grammaris lexicalised (each rule is associated with a wordfrom the input) and on TAG extended domain oflocality (which permits using a rule anchored withthe empty string to reconstruct the missing syntaxin the elided clause thereby making it grammati-cal).We will release the 2398 input representationswe gathered for evaluating the generation of el-liptic coordination so as to make it possible forother surface realisers to be evaluated on their abil-ity to generate ellipsis.
In particular, its wouldbe interesting to examine how other grammarbased generators perform on this dataset suchas White?s CCG based generator (2006) (whicheschews empty categories by adopting a moreflexible notion of constituency) and Carroll andOepen?s HPSG based generator (2005) (whose do-main of locality differs from that of TAG).AcknowledgmentsWe would like to thank Anja Belz and Mike Whitefor providing us with the evaluation data and theevaluation scripts.
The research presented in thispaper was partially supported by the EuropeanFund for Regional Development within the frame-work of the INTERREG IV A Allegro Project.ReferencesEva Banik.
2004.
Semantics of VP coordinationin LTAG.
In Proceedings of the 7th InternationalWorkshop on Tree Adjoining Grammars and Re-lated Formalisms (TAG+), volume 7, pages 118?125, Vancouver, Canada.Anja Belz, Michael White, Dominic Espinosa, EricKow, Deirdre Hogan, and Amanda Stent.
2011.
Thefirst surface realisation shared task: Overview andevaluation results.
In Proceedings of the 13th Eu-ropean Workshop on Natural Language Generation(ENLG), Nancy, France.Ann Bies, Mark Ferguson, Katz Katz, Robert MacIn-tyre, Victoria Tredinnick, Grace Kim, Marry AnnMarcinkiewicz, and Britta Schasberger.
1995.Bracketing guidelines for treebank II style penn tree-bank project.
University of Pennsylvania.Aoife Cahill and Josef Van Genabith.
2006.
Robustpcfg-based generation using automatically acquiredlfg approximations.
In Proceedings of the 21st Inter-national Conference on Computational Linguistics(COLING) and the 44th annual meeting of the Asso-ciation for Computational Linguistics (ACL), pages1033?1040, Sydney, Australia.Charles B Callaway.
2003.
Evaluating coverage forlarge symbolic nlg grammars.
In Proceedings of the18th International joint conference on Artificial In-telligence (IJCAI), volume 18, pages 811?816, SanFrancisco, CA, USA.
Morgan Kaufmann PublishersInc.John Carroll and Stephan Oepen.
2005.
High ef-ficiency realization for a wide-coverage unificationgrammar.
In Proceedings of the 2nd InternationalJoint Conference on Natural Language Process-ing (IJCNLP), pages 165?176, Jeju Island, Korea.Springer.48M.
?Cmejrek, J.
Hajic?, and V. Kubon?.
2004.
Pragueczech-english dependency treebank: Syntacticallyannotated resources for machine translation.
InProceedings of the 4th International Conference onLanguage Resources and Evaluation (LREC), Lis-bon, Portugal.Michael Collins.
1999.
Head-Driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania.Peter W. Culicover and Ray Jackendoff.
2005.
SimplerSyntax.
Oxford University Press.Mary Dalrymple and Ronald M. Kaplan.
2000.
Fea-ture indeterminacy and feature resolution.
Lan-guage, pages 759?798.Mary Dalrymple, Stuart M. Sheiber, and FernandoC.
N. Pereira.
1991.
Ellipsis and higher-order unifi-cation.
Linguistics and Philosophy.Michael Daum, Kilian Foth, and Wolfgang Menzel.2004.
Automatic transformation of phrase treebanksto dependency trees.
In Proceedings of the 4th In-ternational Conference on Language Resources andEvaluation (LREC), Lisbon, Portugal.Kais Dukes and Nizar Habash.
2011.
One-step sta-tistical parsing of hybrid dependency-constituencysyntactic representations.
In Proceedings of the12th International Conference on Parsing Technolo-gies, pages 92?103, Dublin, Ireland.
Association forComputational Linguistics.Michael Elhadad.
1993.
Using argumentation to con-trol lexical choice: a functional unification imple-mentation.
Ph.D. thesis, Columbia University.Katja Filippova and Michael Strube.
2008.
Depen-dency tree based sentence compression.
In Proceed-ings of the Fifth International Natural LanguageGeneration Conference (INLG), pages 25?32, SaltFork, Ohio, USA.
Association for ComputationalLinguistics.Rubino Gei?, Gernot Veit Batz, Daniel Grund, Sebas-tian Hack, and Adam M. Szalkowski.
2006.
Grgen:A fast spo-based graph rewriting tool.
In Proceed-ings of the 3rd International Conference on GraphTransformation, pages 383?397.
Springer.
Natal,Brasil.Jonathan Ginzburg and Ivan Sag.
2000.
Interrogativeinvestigations.
CSLI Publications.J.
Hajic?, M. Ciaramita, R. Johansson, D. Kawahara,M.A.
Mart?
?, L. Ma`rquez, A. Meyers, J. Nivre,S.
Pado?, J.
?Ste?pa?nek, et al2009.
The conll-2009shared task: Syntactic and semantic dependencies inmultiple languages.
In Proceedings of the 13th Con-ference on Computational Natural Language Learn-ing: Shared Task, pages 1?18.Karin Harbusch and Gerard Kempen.
2009.
Gener-ating clausal coordinate ellipsis multilingually: Auniform approach based on postediting.
In Proceed-ings of the 12th European Workshop on Natural Lan-guage Generation, pages 138?145, Athens, Greece.Association for Computational Linguistics.Daniel Hardt.
1993.
Verb phrase ellipsis: Form,meaning and processing.
Ph.D. thesis, Universityof Pennsylvania.Richert Johansson and Pierre Nugues.
2007.
Ex-tended constituent-to-dependency conversion for en-glish.
In Proceedings of the 16th Nordic Conferenceof Computational Linguistics (NODALIDA), pages105?112, Tartu, Estonia.Edward Keenan.
1971.
Names, quantifiers, and thesloppy identity problem.
Papers in Linguistics,4:211?232.Andrew Kehler.
2002.
Coherence in discourse.
CSLIPublications.Yusuke Kubota and Robert Levine.
2012.
Gappingas like-category coordination.
In Proceedings of the7th international conference on Logical Aspects ofComputational Linguistics (LACL), pages 135?150,Nantes, France.
Springer-Verlag.Irene Langkilde-Geary.
2002.
An empirical verifi-cation of coverage and correctness for a general-purpose sentence generator.
In Proceedings of the12th International Natural Language GenerationWorkshop, pages 17?24.Roger Levy and Carl Pollard.
2001.
Coordination andneutralization in HPSG.
Technology, 3:5.Wolfgang Maier, Erhard Hinrichs, Julia Krivanek, andSandra Ku?bler.
2012.
Annotating coordinationin the Penn Treebank.
In Proceedings of the 6thLinguistic Annotation Workshop (LAW), pages 166?174, Jeju, Republic of Korea.
Association for Com-putational Linguistics.Kathleen McKeown, Karen Kukich, and James Shaw.1994.
Practical issues in automatic documentationgeneration.
In Proceedings of the fourth conferenceon Applied natural language processing (ANLC),pages 7?14, Stuttgart, Germany.
Association forComputational Linguistics.Jason Merchant.
2001.
The syntax of silence: Sluicing,islands, and the theory of ellipsis.
Oxford Univer-sity Press.Shashi Narayan and Claire Gardent.
2012a.
Error min-ing with suspicion trees: Seeing the forest for thetrees.
In Proceedings of the 24th International Con-ference on Computational Linguistics (COLING),Mumbai, India.Shashi Narayan and Claire Gardent.
2012b.
Structure-driven lexicalist generation.
In Proceedings of the24th International Conference on ComputationalLinguistics (COLING), Mumbai, India.49Ivan Sag.
1976.
Deletion and logical form.
Ph.D.thesis, Massachusetts Institute of Technology, Cam-bridge, Massachusetts.Anoop Sarkar and Arvind Joshi.
1996.
Coordinationin tree adjoining grammars: Formalization and im-plementation.
In Proceedings of the 16th conferenceon Computational linguistics-Volume 2, pages 610?615, Copenhagen, Denmark.
Association for Com-putational Linguistics.Djame?
Seddah.
2008.
The use of mctag to processelliptic coordination.
In Proceedings of The NinthInternational Workshop on Tree Adjoining Gram-mars and Related Formalisms (TAG+ 9), volume 1,page 2, Tu?bingen, Germany.Wolfgang Seeker and Jonas Kuhn.
2012.
Making el-lipses explicit in dependency conversion for a ger-man treebank.
In Proceedings of the 8th Interna-tional Conference on Language Resources and Eval-uation (LREC), Istanbul, Turkey.James Shaw.
1998.
Segregatory coordination andellipsis in text generation.
In Proceedings of the36th Annual Meeting of the Association for Com-putational Linguistics and 17th International Con-ference on Computational Linguistics, pages 1220?1226, Montreal, Quebec, Canada.Mark Steedman.
1996.
Surface Structure and Inter-pretation, volume 30.
MIT press Cambridge, MA.Marie?t Theune, Feikje Hielkema, and Petra Hendriks.2006.
Performing aggregation and ellipsis using dis-course structures.
Research on Language & Compu-tation, 4(4):353?375.Jeoren van Craenenbroeck.
2010.
The syntax of ellip-sis: Evidence from Dutch dialects.
Oxford Univer-sity Press.V.
Vincze, D. Szauter, A.
Alma?si, G. Mo?ra, Z. Alexin,and J. Csirik.
2010.
Hungarian dependency tree-bank.
In Proceedings of the 7th InternationalConference on Language Resources and Evaluation(LREC), Valletta, Malta.Michael White and Rajakrishnan Rajkumar.
2009.Perceptron reranking for ccg realization.
In Pro-ceedings of the 2009 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP),pages 410?419, Singapore.
Association for Compu-tational Linguistics.Michael White.
2006.
Efficient realization of coordi-nate structures in combinatory categorial grammar.Research on Language & Computation, 4(1):39?75.Edwin Williams.
1977.
Discourse and logical form.Linguistic Inquiry.Huayan Zhong and Amanda Stent.
2005.
Buildingsurface realizers automatically from corpora.
InProceedings of the Workshop on Using Corpora forNatural Language Generation (UCNLG), volume 5,pages 49?54.50
