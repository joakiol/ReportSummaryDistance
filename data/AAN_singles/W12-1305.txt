JEP-TALN-RECITAL 2012, Atelier TALAf 2012: Traitement Automatique des Langues Africaines, pages 53?62,Grenoble, 4 au 8 juin 2012. c?2012 ATALA & AFCPAnalyse des performances de mod?les de langage sub-lexicalepour des langues peu-dot?es ?
morphologie richeHadrien Gelas1, 2 Solomon Teferra Abate2Laurent Besacier2 Fran?ois Pellegrino1(1) Laboratoire Dynamique Du Langage, CNRS - Universit?
de Lyon, France(2) Laboratoire Informatique de Grenoble, CNRS - Universit?
Joseph Fourier Grenoble 1, France{hadrien.gelas, francois.pellegrino}@univ-lyon2.fr{solomon.abate, laurent.besacier@imag.fr}R?SUM?Ce papier ?tudie l?impact de l?utilisation d?unit?s sous-lexicales sur les performances d?un syst?mede RAP pour deux langues africaines peu-dot?es et morphologiquement riches (l?amharique et leswahili).
Deux types de sous-unit?s sous-lexicales sont consider?s : la syllabe et le morph?me,ce dernier ?tant obtenu de mani?re supervis?e ou non-supervis?e.
La reconstruction en mots?
partir de sorties de RAP en syllabes ou morph?mes est aussi prise en compte.
Pour les deuxlangues, les meilleurs r?sultats sont obtenus avec les morph?mes non-supervis?s.
Le tauxd?erreur de mots est grandement r?duit pour la reconnaissance de l?amharique dont les donn?esd?entrainement du LM sont tr?s faibles (2,3M de mots).
Les scores pour la RAP du swahili sontaussi am?lior?s (28M de mots pour l?entrainement).
Il est ausi pr?sent?
une analyse d?taill?e dela reconstruction des mots hors vocabulaires, un pourcentage important de ceux-ci (jusqu??
75%pour l?amharique) sont retrouv?s ?
l?aide de mod?les de langage ?
base de morph?mes et lam?thode de reconstruction appropri?e.ABSTRACTPerformance analysis of sub-word language modeling for under-resourced languages withrich morphology : case study on Swahili and AmharicThis paper investigates the impact on ASR performance of sub-word units for two under-resourced african languages with rich morphology (Amharic and Swahili).
Two subwordunits are considered : syllable and morpheme, the latter being obtained in a supervised orunsupervised way.
The important issue of word reconstruction from the syllable (or morpheme)ASR output is also discussed.
For both languages, best results are reached with morphemesgot from unsupervised approach.
It leads to very significant WER reduction for Amharic ASRfor which LM training data is very small (2.3M words) and it also slightly reduces WER overa Word-LM baseline for Swahili ASR (28M words for LM training).
A detailed analysis of theOOV word reconstruction is also presented ; it is shown that a high percentage (up to 75% forAmharic) of OOV words can be recovered with morph-based language model and appropriatereconstruction method.MOTS-CL?S : Mod?le de langage, Morph?me, Hors vocabulaire, Langues peu-dot?es.KEYWORDS: Language model, Morpheme, Out-of-Vocabulary , Under-resourced languages.531 IntroductionDue to world?s globalisation and answering the necessity of bridging the numerical gap withthe developing world, speech technology for under-resourced languages is a challenging issue.Applications and usability of such tools in developing countries are proved to be numerousand are highlighted for information access in Sub-Saharan Africa (Barnard et al, 2010a,b),agricultural information in rural India (Patel et al, 2010), or health information access bycommunity health workers in Pakistan (Kumar et al, 2011).In order to provide a totally unsupervised and language independent methodology to develop anautomatic speech recognition (ASR) system, some particular language characteristics should betaken into account.
Such specific features as tones ((Lei et al, 2006) on Mandarin Chinese) orwriting systems without explicit word boundaries ((Seng et al, 2008) on Khmer) need a specificmethodology adaptation.
This is especially true when dealing with under-resourced languages,where only few data are available.During recent years, many studies tried to deal with morphologically rich languages (whetherthey are agglutinative, inflecting and compounding languages) in NLP (Sarikaya et al, 2009).Such a morphology results in data sparsity and in a degraded lexical coverage with a similarlexicon size than state-of-the-art speech recognition setup (as one for English).
It yields highOut-of-Vocabulary (OOV) rates and degrades Word-Error rate (WER) as each OOV words will notbe recognized but can also affect their surrounding words and strongly increase WER.When the corpus size is limited, a common approach to overcome the limited lexical coverageis to segment words in sub-word units (morphemes or syllables).
Segmentation in morphemescan be obtained in a supervised or unsupervised manner.
Supervised approaches were mainlyused through morphological analysers built on carefully annotated corpora requiring impor-tant language-specific knowledge (as in (Ar?soy et al, 2009)).
Unsupervised approaches arelanguage-independent and do not require any linguistic-knowledge.
In (Kurimo et al, 2006),several unsupervised algorithms have been compared, including their own public method calledMorfessor ((Creutz et Lagus, 2005)) for two ASR tasks in Turkish and Finnish (see also (Hirsimakiet al, 2009) for a recent review of morh-based approaches).
The other sub-word type that is alsoutilized for reducing high OOV rate is the syllable.
Segmentation is mainly rule-based and wasused in (Shaik et al, 2011b) and (Shaik et al, 2011a), even if outperformed in WER by ASRmorpheme-based recognition for Polish and German.In this work, we investigate those different methodologies and see how to apply them for twodifferent speech recognition tasks : read speech ASR in Amharic and broadcast speech trans-cription in Swahili.
These tasks represents two different profiles of under-resourced languagescases.
Amharic with an acoustic model (AM) trained on 20h of read-speech but limited text data(2.3M) and on the opposite, Swahili with a weaker acoustic model (12h of broadcast news frominternet mixing genre and quality) but a more robust LM (28M words of web-mining news, stillwithout any adaptation to spoken broadcast news).
If such study on sub-unit has already beenconducted on Amharic (Pellegrini et Lamel, 2009), no prior work are known to us for Swahili.But, the main goal of this study is to better understand what does really impact performanceof ASR using sub-word unit through a comparison of different methodologies.
Both supervisedand unsupervised segmentation strategies are explored as well as different approaches to tagsegmentation.54The next section describes the target languages and the available corpora.
Then, we introduceseveral segmentation approaches in section 3.
Section 4 presents the analysis of experimentalresults for Swahili and Amharic while section 5 concludes this work.2 Experiment description2.1 LanguagesAmharic is a Ethio-Semitic language from the Semitic branch of the Afroasiatic super family.
Itis related to Hebrew, Arabic, and Syrian.
According to the 1998 census, it is spoken by over17 million people as a first language and by over 5 million as a second language throughoutEthiopia.
Amharic is also spoken in other countries such as Egypt, Israel and the United States.It has its own writing system which is syllabary.
It exhibits non-concatenative, inflectional andderivational morphology.
Like other Semitic languages such as Arabic, Amharic exhibits a root-pattern morphological phenomenon.
Case, number, definiteness, and gender-marking affixesinflect nouns.
Some adverbs can be derived from adjectives but adverbs are not inflected.
Nounsare derived from other basic nouns, adjectives, stems, roots, and the infinitive form of a verb isobtained by affixation and intercalation.Swahili is a Bantu language often used as a vehicular language in a wide area of East Africa.
Itis not only the national language of Kenya and Tanzania but it is also spoken in different partsof Democratic Republic of Congo, Mozambique, Somalia, Uganda, Rwanda and Burundi.
Mostestimations give over 50 million speakers (with only less than 5 million native speakers).
It hasmany typical Bantu features, such as noun class and agreement systems and complex verbalmorphology.
Structurally, it is often considered as an agglutinative language (Marten, 2006).2.2 Speech corpora descriptionBoth Amharic and a small part of Swahili training audio corpora were collected following thesame protocol.
Texts were extracted from news websites and segmented by sentence.
Nativespeakers were recorded using a self-paced reading interface (with possible rerecordings).
TheAmharic speech corpus (Abate et al, 2005) consists of 20 hours of training speech collectedfrom 100 speakers who read a total of 10,850 sentences.
Swahili corpus corresponds to 2 hoursand a half read by 5 speakers (3 male and 2 female) along with almost 10 hours of web-miningbroadcast news representing various types of recording quality (noisy speech, telephone speech,studio speech) and speakers.
They were transcribed using a collaborative transcription processbased on the use of automatic pre-transcriptions to increase productivity gains (See details in(Gelas et al, 2012)).
Test corpora are made of 1.5 hours (758 sentences) of read speech forAmharic and 2 hours (1,997 sentences) of broadcast news for Swahili.552.3 Text corpora descriptionWe built all statistical N-gram language model (LM) using the SRI 1 language model toolkit.Swahili text corpus is made of data collected from 12 news websites (over 28M words).
Togenerate a pronunciation dictionary, we extracted the 65k most frequent words from the textcorpus and automatically created pronunciations taking benefit of the regularity of the graphemeto phoneme conversion in Swahili.
The same methodology and options have been applied to allsub-words LM.
For Amharic, we have used the data (2.3M words text) described in (Tachbelieet al, 2010).3 Segmenting text data3.1 Unsupervised morphemic segmentationFor the unsupervised word segmentation, we used a publicly available tool called Morfessor 2.Its data-driven approach learns a sub-word lexicon from a training corpus of words by using aMinimum Description Length (MDL) algorithm (Creutz et Lagus, 2005).
It has been used withdefault options and without any adaptation.3.2 Supervised morphemic and syllabic segmentationFor Amharic, we used the manually-segmented text described in (Tachbelie et al, 2011a) to trainan FSM-based segmenter (a composition of morpheme transducer and 12gram consonant vowelsyllable-based language model) using the AT&T FSM Library (FiniteState Machine Library) andGRM Library (Grammar Library)(Mohri et al, 1998).
The trained segmenter with the languagemodel is applied to segment the whole text mentioned in (Tachbelie et al, 2010).The supervised decomposition for Swahili is performed with the public Part-Of-Speech taggernamed TreeTagger 3.
It is using the parameters available for Swahili to extract sub-word units.As for as syllable segmentation is concerned, we designed rule-based algorithms followingstructural and phonological restrictions of the respective languages.3.3 Segmentation tagging and vocabulary sizeWhile working on sub-word unit, one should think on how to incorporate the segmentationinformation.
Morphological information can be included within factored LM as in (Tachbelieet al, 2011b) or directly as a full unit in the n-gram LM itself.
By choosing the latter, the ASRdecoder output is a sequence of sub-word units and an additional step is needed to recover1.
www.speech.sri.com/projects/srilm/2.
The unit obtained with Morfessor is referred here as morpheme even if it do not automatically corresponds to thelinguistic definition of morpheme (the smallest semantically meaningful unit)3. www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/DecisionTreeTagger.html56words from sub-units.
In (Diehl et al, 2011), a n-gram SMT-based morpheme-to-word conversionapproach is proposed.In this work, we evaluate how the recognition performance is affected by different ways oftagging the segmentation information straightly in the training text.
In (Ar?soy et al, 2009), it isnoticed that this aspect need to be considered as it impacts WER.
In (Guijarrubia et al, 2009),a similar methodology is applied without reading any conclusion since a too small and easyrecognition task was performed.Three distinct types of tagging are evaluated here :?
UNIT_AFX : A morpheme boundary (MB) is added on left and/or right side of segmentationleaving the (so-called with Morfessor) root alone.
To rebuild up to words, we reconnect everyunits containing MB to the one next to it.(ex.
kiMB tabu?
kitabu)?
UNIT_ALL : A MB tag is added on each side of segmentation, in other words, we add to thelexicon the information to distinguish roots from their context (we can get up to four differententries for a same root : ROOT, MBROOT, ROOTMB, MBROOTMB).
To rebuild, we reconnectevery time two MB appearing consecutively.(ex.
kiMB MBtabu?
kitabu)?
UNIT_POS : For syllables, we add to the unit the position of the syllable in the word.(ex.
1ki 2ta 3bu?
kitabu)In table 1, it is shown that each choice has an influence on the size of the full text vocabularyand thus on the lexical coverage of the 65k lexicon.
As expected from a language with richmorphology, the word baseline 65k lexicon shows a dramatically low lexical coverage (13.95%).For the same text information, syllables logically reduce the size of vocabulary and got a fulltheoretical lexical coverage without reaching the 65k limits, but with the cost of really shortlength unit.
Concerning both morpheme segmentation types, as expected the supervised approachleads to a larger number of units than the unsupervised statistical approach, the latter leads to abetter theoretical lexical coverage.
The average token length do not reduce much compared toword unit as most frequent words are already short mono-morphemic grammatical words.
Theinfluence of different tagging techniques is also shown on the same table.
Detailed comments onWER will be given in 4.2.FullVoc 65k Cov.
Token WERLM (%) (%) length (%)Word 100 13.95 5.5 35.7Syl_Pos (V=27k) 5.79 100 2.0 51.7Treetag_All 79.38 17.57 4.4 44.7Treetag_Afx 78.61 17.74 4.4 43.3Morf_All 45.24 30.83 5.3 34.8Morf_Afx 38.07 36.64 5.3 35.4TABLE 1 ?
Swahili - Size of full text corpus vocabulary in comparison with a word level baseline(FullVoc) ; lexical coverage of a 65k lexicon on the full vocabulary (65k Cov.)
; average token lengthin character for the text corpus ; word error rate depending on the choice of unit and segmentationtag (WER), all systems using 3gram LM and 65k lexicon except when specified574 Results4.1 ASR system descriptionWe used SphinxTrain 4 toolkit from Sphinx project for building Hidden Markov Models (HMM)based acoustic models (AMs) for Swahili.
With the speech training database described in 2.2,we trained a context-dependent model with 3,000 tied states.
The acoustic models have 36 and40 phones for Swahili and Amharic, respectively.
We used the HDecode decoder of the HTK forAmharic.
The Amharic acoustic model is more precisely described in (Tachbelie et al, 2010).4.2 Analysis of Sub-word units performance for SwahiliComparing all results for Swahili broadcast speech transcription task (table 1), Morfessor basedsegmentation ASR system is the only one, with 34.8% WER, performing significantly better thanthe 35.7% word baseline.
As in (Ar?soy et al, 2009) and (Hirsimaki et al, 2006), segmentationbased on a morphological analyser reaches lower results (43.3% WER) than words and unsu-pervised based segmentation.
Finally, rule-based syllabic system have the worst performancewith 51.7% WER.
Those scores in table 1 gives a good indication on how to choose the mostperforming unit.
It seems that one need to balance and optimise two distinct criteria : n-gramlength coverage and lexical coverage.The importance of n-gram length coverage can be seen with poor performance of too shortunits, like syllables in this work.
A syllable trigram (average 6.0 character-long) is approximatelyequivalent to a word unigram in Swahili (average 5.5 character-long), thus such a short trigramlength is directly impacting ASR system performance even if lexical coverage is maximized(100%).
The importance to use higher order n-gram LM when dealing with short units is alsoshown in (Hirsimaki et al, 2009).
However, if a lattice rescoring framework is often used, it isdifficult to recover enough information if the first trigram pass do not perform well enough.
It isthen recommended to directly implement the higher order n-gram LM in the decoder.In the same time, a larger lexical coverage (lex.cov.
), allows better performance if not usedwith too short units as shows the difference of performance between word-based LM (13.95%lex.cov.
and 35.7% WER) and Morfessor-based LM (30.83% lex.cov.
and 34.8% WER), bothhaving similar average token lengths.Concerning the different tagging techniques, they have an impact on WER.
The better choiceseems to be influenced by the lexical coverage.
When lexical coverage is good enough (Morfessor-based system), one can get advantage of having more different and precise contexts (tag on allunits, separating roots alone and roots with affixes in the vocabulary and on n-gram estimations),whereas for low lexical coverage (TreeTagger-based system), having more various words is better(tag only on affixes, regrouping all same roots together allowing more distinct units in thelexicon).4. cmusphinx.sourceforge.net/584.3 Sub-word units performance for AmharicFor the read speech recognition task for Amharic, only the best performing systems are presentedin table 2.
Similar trend is found concerning the tagging techniques (better systems are taggedALL for Morfessor and tagged AFX for FSM) and by the fact that Morfessor system outperformsthe others.
Even if the unit length in Morfessor is 40% shorter than average word length, itgets important benefits from a 100% lexical coverage of the training corpus.
However, for thistask, the supervised segmentation (FSM) has better results than word baseline system.
It can beexplained by a slightly increased lexical coverage and still a reasonable token length.
Throughthis task, we also considered several vocabulary sizes.
Results show that WER greatly benefitsfrom sub-units in smaller lexicon tasks.
Finally, as for Amharic sub-word units being notablyshorter than word units, we rescored output lattices from the trigram LM system with a 5gramLM.
It leads to an absolute WER decrease of 2.0% for Morfessor.65k Cov.
Token Word Error Rate (%)LM (%) length 5K 20K 65KWord_3g 30.79 8.3 52.4 29.6 15.9FSM_Afx_3g 45.13 6.3 39.3 20.8 12.2FSM_Afx_5g 45.13 6.3 39.1 20.3 11.4Morf_All-3g 100 4.9 36.7 14.8 9.9Morf_All-5g 100 4.9 34.9 12.6 7.9TABLE 2 ?
Amharic - Lexical coverage of a 65k lexicon on the full vocabulary (65k Cov.)
; averagetoken length in the whole text corpus ; word error rate depending on the choice of unit, segmentationtag and vocabulary size4.4 OOV benefits of using sub-word unitsMaking good use of sub-word units for ASR has been proved efficient in many research torecognize OOV words over baseline word LMs (as in (Shaik et al, 2011a)).
Table 3 presentsthe different OOV rates considering both token and type for each LM (OOV morphemes forMorfessor-based LM).
We also present the proportion of correctly recognized words (COOV)which were OOVs in the word baseline LM.
Results show important OOV rate reduction andcorrectly recognised OOV rate for both languages (Morfessor-based outputs).
For Amharic, thedifference of COOV rate between each lexicon is correlated with the possible OOVs each systemcan recognized.Swahili obtain less benefits for COOV.
It can be explained by the specificity of the broadcast newstask, leading to important OOV entity names or proper names (the 65k Morfessor-based lexiconis still having 11.36% of OOV types).
But if we consider only the OOVs that can possibly berecognized (i.e.
only those which are not also OOVs in the Morfessor-based lexicon), 36.04% ofthem are rebuilt.
Due to decoder limitations we restrained this study to a 65k lexicon, but for aSwahili 200k word vocabulary we get a type OOV rate of 12.46% and still 10.28% with a fullvocab (400k).
Those numbers are really close to those obtained with the 65k Morfessor lexiconand could only be reached with the cost of more computational power and less robust LM.
In the59OOV (%) OOV (%) COOV (%)LM Token TypeAmharicWord-5k 35.21 57.14 -Word-20k 19.48 32.18 -Word-65k 9.06 14.99 -Morf_All-5k 13.67 40.58 33.76Morf_All-20k 2.50 7.88 66.95Morf_All-65k 0.12 2.81 75.30SwahiliWord-65k 5.73 19.17 -Morf_All-65k 3.67 11.36 8.77TABLE 3 ?
Amharic and Swahili - Token and type OOV rate in test reference transcriptions dependingon LM (OOV morphemes for Morfessor-based LM) ; correctly recognised baseline OOV words rate inASR outputs (COOV)same time, growing Morfessor lexicon to 200k would be more advantageous as it reduces thetype OOV rate to 1.61%.While using sub-word system outputs rebuilt to word level reduces OOV words, in contrary, it canalso generate non words by ungrammatical or non-sense concatenation.
We checked the 5029words generated by the best Amharic Morfessor output to see if they exist in the full trainingtext vocabulary.
It appears that only 37 are non-words (33 after manual validation).
Amongthose 33, there were 26 isolated affixes and 7 illegal concatenations, all due to poor acousticestimation from the system.
Considering this small amount of non-words and with no possibilityto retrieve good ones in lattices, we did not process to constraint illegal concatenation as in(Ar?soy et Sara?lar, 2009).5 ConclusionWe investigated the use of sub-word units in n-gram language modeling through different metho-dologies.
The best results are obtained using unsupervised segmentation with Morfessor.
Thistool outperforms supervised methodologies (TreeTagger, FSM or rule-based syllables) becausethe choice of sub-word units optimise two essential criteria which are n-gram length coverageand lexical coverage.
In the same time, it appears that the way one implements the segmentationinformation affects the speech recognition performance.
As expected, using sub-word units bringsmajor benefits to the OOV problem.
It shows to be effective in two very different tasks for twounder-resourced African languages with rich morphology (one being highly inflectional, Amharicand the other being agglutinative, Swahili).
The Amharic read speech recognition task, get themore advantages of it, since the word baseline LM suffers from data sparsity.
But results are alsoimproved for a broadcast speech transcription task for Swahili.60R?f?rencesABATE, S., MENZEL, W. et TAFILA, B.
(2005).
An Amharic speech corpus for large vocabularycontinuous speech recognition.
In Interspeech, pages 67?76.ARISOY, E., CAN, D., PARLAK, S., SAK, H. et SARA?LAR, M. (2009).
Turkish broadcast newstranscription and retrieval.
Audio, Speech, and Language Processing, IEEE Transactions on,17(5):874?883.ARISOY, E. et SARA?LAR, M. (2009).
Lattice extension and vocabulary adaptation for TurkishLVCSR.
Audio, Speech, and Language Processing, IEEE Transactions on, 17(1):163?173.BARNARD, E., DAVEL, M. et van HUYSSTEEN, G. (2010a).
Speech technology for informationaccess : a South African case study.
In AAAI Symposium on Artificial Intelligence, pages 22?24.BARNARD, E., SCHALKWYK, J., van HEERDEN, C. et MORENO, P. (2010b).
Voice search for develop-ment.
In Interspeech.CREUTZ, M. et LAGUS, K. (2005).
Unsupervised morpheme segmentation and morphologyinduction from text corpora using morfessor 1.0.
Rapport technique, Computer and InformationScience, Report A81, Helsinki University of Technology.DIEHL, F., GALES, M., TOMALIN, M. et WOODLAND, P. (2011).
Morphological decomposition inArabic ASR systems.
Computer Speech & Language.GELAS, H., BESACIER, L. et PELLEGRINO, F. (2012).
Developments of swahili resources for anautomatic speech recognition system.
In SLTU.GUIJARRUBIA, V., TORRES, M. et JUSTO, R. (2009).
Morpheme-based automatic speech recognitionof basque.
Pattern Recognition and Image Analysis, pages 386?393.HIRSIMAKI, T., CREUTZ, M., SIIVOLA, V., KURIMO, M., VIRPIOJA, S. et PYLKKONEN, J.
(2006).
Unlimi-ted vocabulary speech recognition with morph language models applied to Finnish.
ComputerSpeech & Language.HIRSIMAKI, T., PYLKKONEN, J. et KURIMO, M. (2009).
Importance of high-order n-gram models inmorph-based speech recognition.
Audio, Speech, and Language Processing, IEEE Transactions on,17(4):724?732.KUMAR, A., TEWARI, A., HORRIGAN, S., KAM, M., METZE, F. et CANNY, J.
(2011).
Rethinking speechrecognition on mobile devices.
In IUI4DR.
ACM.KURIMO, M., CREUTZ, M., VARJOKALLIO, M., ARISOY, E. et SARACLAR, M. (2006).
Unsupervisedsegmentation of words into morphemes?morpho challenge 2005, application to automaticspeech recognition.
In Interspeech.LEI, X., SIU, M., HWANG, M., OSTENDORF, M. et LEE, T. (2006).
Improved tone modeling forMandarin broadcast news speech recognition.
In Interspeech.MARTEN, L. (2006).
Swahili.
In BROWN, K., ?diteur : The Encyclopedia of Languages and Linguistics,2nd ed., volume 12, pages 304?308.
Oxford : Elsevier.MOHRI, M., PEREIRA, F. et RILEY, M. (1998).
A rational design for a weighted finite-statetransducer library.
In Lecture Notes in Computer Science, pages 144?158.
Springer.PATEL, N., CHITTAMURU, D., JAIN, A., DAVE, P. et PARIKH, T. (2010).
Avaaj otalo : a field study ofan interactive voice forum for small farmers in rural India.
In CHI, pages 733?742.
ACM.61PELLEGRINI, T. et LAMEL, L. (2009).
Automatic word decompounding for ASR in a morphologicallyrich language : Application to Amharic.
Audio, Speech, and Language Processing, IEEE Transactionson, 17(5):863?873.SARIKAYA, R., KIRCHHOFF, K., SCHULTZ, T. et HAKKANI-TUR, D. (2009).
Introduction to the specialissue on processing morphologically rich languages.
Audio, Speech, and Language Processing,IEEE Transactions on, 17(5).SENG, S., SAM, S., BESACIER, L., BIGI, B. et CASTELLI, E. (2008).
First broadcast news transcriptionsystem for Khmer language.
In LREC.SHAIK, M., MOUSA, A., SCHLUTER, R. et NEY, H. (2011a).
Hybrid language models using mixedtypes of sub-lexical units for open vocabulary German LVCSR.
In Interspeech.SHAIK, M., MOUSA, A., SCHLUTER, R. et NEY, H. (2011b).
Using morpheme and syllable basedsub-words for Polish LVCSR.
In ICASSP.TACHBELIE, M., ABATE, S. et BESACIER, L. (2011a).
Part-of-speech tagging for under-resourcedand morphologically rich languages - the case of Amharic.
In HLTD.TACHBELIE, M., ABATE, S. et MENZEL, W. (2010).
Morpheme-based automatic speech recognitionfor a morphologically rich language - amharic.
In SLTU.TACHBELIE, M., ABATE, S. et MENZEL, W. (2011b).
Morpheme-based and factored languagemodeling for Amharic speech recognition.
In VETULANI, Z., ?diteur : Human Language Technology.Challenges for Computer Science and Linguistics, volume 6562 de Lecture Notes in ComputerScience, pages 82?93.
Springer.62
