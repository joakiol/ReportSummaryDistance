Memory-Based Learning: Using Similarity for SmoothingJ akub  Zavre l  and  Wal ter  Dae lemansComputat iona l  L inguist icsT i lburg  Univers i tyPO Box 901535000 LE  T i lburgThe  Nether lands{zavrel, walt er}@kub, nlAbst rac tThis paper analyses the relation betweenthe use of similarity in Memory-BasedLearning and the notion of backed-offsmoothing in statistical anguage model-ing.
We show that the two approaches areclosely related, and we argue that featureweighting methods in the Memory-Basedparadigm can offer the advantage of au-tomatically specifying a suitable domain-specific hierarchy between most specificand most general conditioning informationwithout the need for a large number of pa-rameters.
We report two applications ofthis approach: PP-attachment and POS-tagging.
Our method achieves tate-of-the-art performance in both domains, and al-lows the easy integration of diverse infor-mation sources, such as rich lexical repre-sentations.1 In t roduct ionStatistical approaches to disambiguation offer theadvantage of making the most likely decision on thebasis of available vidence.
For this purpose a largenumber of probabilities has to be estimated from atraining corpus.
However, many possible condition-ing events are not present in the training data, yield-ing zero Maximum Likelihood (ML) estimates.
Thismotivates the need for smoothing methods, which re-estimate the probabilities of low-count events frommore reliable estimates.Inductive generalization from observed to newdata lies at the heart of machine-learning approachesto disambiguation.
In Memory-Based Learning 1(MBL) induction is based on the use of similarity(Stanfill & Waltz, 1986; Aha et al, 1991; Cardie,1994; Daelemans, 1995).
In this paper we describehow the use of similarity between patterns embod-ies a solution to the sparse data problem, how it1The Approach is also referred to as Case-based,Instance-based or Exemplar-based.relates to backed-off smoothing methods and whatadvantages it offers when combining diverse and richinformation sources.We illustrate the analysis by applying MBL totwo tasks where combination of information sourcespromises to bring improved performance: PP-attachment disambiguation and Part of Speech tag-ging.2 Memory-Based  LanguageProcess ingThe basic idea in Memory-Based language process-ing is that processing and learning are fundamen-tally interwoven.
Each language xperience l aves amemory trace which can be used to guide later pro-cessing.
When a new instance of a task is processed,a set of relevant instances is selected from memory,and the output is produced by analogy to that set.The techniques that are used are variants andextensions of the classic k-nearest neighbor (k-NN) classifier algorithm.
The instances of a taskare stored in a table as patterns of feature-valuepairs, together with the associated "correct" out-put.
When a new pattern is processed, the k nearestneighbors of the pattern are retrieved from memoryusing some similarity metric.
The output is then de-termined by extrapolation from the k nearest neigh-bors, i.e.
the output is chosen that has the highestrelative frequency among the nearest neighbors.Note that no abstractions, such as grammaticalrules, stochastic automata, or decision trees are ex-tracted from the examples.
Rule-like behavior e-sults from the linguistic regularities that are presentin the patterns of usage in memory in combinationwith the use of an appropriate similarity metric.It is our experience that even limited forms of ab-straction can harm performance on linguistic tasks,which often contain many subregularities and excep-tions (Daelemans, 1996).2.1 Similarity metricsThe most basic metric for patterns with symbolicfeatures is the Over lap  metr i c  given in equations 1436and 2; where A(X, Y) is the distance between pat-terns X and Y, represented by n features, wi is aweight for feature i, and 5 is the distance per fea-ture.
The k-NN algorithm with this metric, andequal weighting for all features is called IB1 (Ahaet al, 1991).
Usually k is set to 1.where:A(X, Y) = ~ wi 6(xi, yi) (1)i= ltf(xi,yi) = 0 i f  xi = yi, else 1 (2)This metric simply counts the number of(mis)matching feature values in both patterns.
Ifwe do not have information about the importanceof features, this is a reasonable choice.
But if wedo have some information about feature relevanceone possibility would be to add linguistic bias toweight or select different features (Cardie, 1996).
Analternative--more empiricist--approach, is to lookat the behavior of features in the set of examplesused for training.
We can compute statistics aboutthe relevance of features by looking at which fea-tures are good predictors of the class labels.
Infor-mation Theory gives us a useful tool for measuringfeature relevance in this way (Quinlan, 1986; Quin-lan, 1993).In fo rmat ion  Ga in  (IG) weighting looks at eachfeature in isolation, and measures how much infor-mation it contributes to our knowledge of the cor-rect class label.
The Information Gain of feature fis measured by computing the difference in uncer-tainty (i.e.
entropy) between the situations with-out and with knowledge of the value of that feature(Equation 3).w\] = H(C) - ~-\]~ev, P(v) x H(Clv )si(f) (3)si(f) = - Z P(v)log 2 P(v) (4)vEVsWhere C is the set of class labels, V f isthe set of values for feature f ,  and H(C) =- ~cec  P(c) log 2 P(e) is the entropy of the class la-bels.
The probabilities are estimated from relativefrequencies in the training set.
The normalizing fac-tor si(f) (split info) is included to avoid a bias infavor of features with more values.
It represents theamount of information eeded to represent all val-ues of the feature (Equation 4).
The resulting IGvalues can then be used as weights in equation 1.The k-NN algorithm with this metric is called ml -IG (Daelemans & Van den Bosch, 1992).The possibility of automatically determining therelevance of features implies that many different andpossibly irrelevant features can be added to the fea-ture set.
This is a very convenient methodology iftheory does not constrain the choice enough before-hand, or if we wish to measure the importance ofvarious information sources experimentally.Finally, it should be mentioned that MB-classifiers, despite their description as table-lookupalgorithms here, can be implemented to workfast, using e.g.
tree-based indexing into the case-base (Daelemans et al, 1997).3 Smoothing of EstimatesThe commonly used method for probabilistic las-sification (the Bayesian classifier) chooses a classfor a pattern X by picking the class that has themaximum conditional probability P(classlX ).
Thisprobability is estimated from the data set by lookingat the relative joint frequency of occurrence of theclasses and pattern X.
If pattern X is described bya number of feature-values X l , .
.
.
,  xn, we can writethe conditional probability as P(classlxl , .
.
.
, xn).
Ifa particular pattern x~, .
.
.
,  x" is not literally presentamong the examples, all classes have zero ML prob-ability estimates.
Smoothing methods are needed toavoid zeroes on events that could occur in the testmaterial.There are two main approaches to smoothing:count re-estimation smoothing such as the Add-Oneor Good-Turing methods (Church & Gale, 1991),and Back-off type methods (Bahl et al, 1983; Katz,1987; Chen & Goodman, 1996; Samuelsson, 1996).We will focus here on a comparison with Back-offtype methods, because an experimental comparisonin Chen & Goodman (1996) shows the superiorityof Back-off based methods over count re-estimationsmoothing methods.
With the Back-off method theprobabilities of complex conditioning events are ap-proximated by (a linear interpolation of) the proba-bilities of more general events:/5(ctasslX) = ~x/3(clas~lX) + ~x,/3(d~sslX')+.. .
+ ),x,~(dasslX n) (5)Where/5 stands for the smoothed estimate,/3 forthe relative frequency estimate, A are interpolationweights, ~-'\]i~0"kx' = 1, and X -< X i for all i,where -< is a (partial) ordering from most specificto most general feature-sets 2 (e.g the probabilitiesof trigrams (X) can be approximated by bigrams(X') and unigrams (X")).
The weights of the lin-ear interpolation are estimated by maximizing theprobability of held-out data (deleted interpolation)with the forward-backward algorithm.
An alterna-tive method to determine the interpolation weightswithout iterative training on held-out data is givenin Samuelsson (1996).2X -< X' can be read as X is more specific than X'.437We can assume for simplicity's ake that the Ax,do not depend on the value of X i, but only on i. Inthis case, if F is the number of features, there are2 F - 1 more general terms, and we need to estimateA~'s for all of these.
In most applications the inter-polation method is used for tasks with clear order-ings of feature-sets (e.g.
n-gram language modeling)so that many of the 2 F --  1 terms can be omittedbeforehand.
More recently, the integration of infor-mation sources, and the modeling of more complexlanguage processing tasks in the statistical frame-work has increased the interest in smoothing meth-ods (Collins ~z Brooks, 1995; Ratnaparkhi, 1996;Magerman, 1994; Ng & Lee, 1996; Collins, 1996).For such applications with a diverse set of featuresit is not necessarily the case that terms can be ex-cluded beforehand.If we let the Axe depend on the value of X ~, thenumber of parameters explodes even faster.
A prac-tical solution for this is to make a smaller numberof buckets for the X i, e.g.
by clustering (see e.g.Magerman (1994)):Note that linear interpolation (equation 5) actu-ally performs two functions.
In the first place, if themost specific terms have non-zero frequency, it stillinterpolates them with the more general terms.
Be-cause the more general terms should never overrulethe more specific ones, the Ax e for the more generalterms should be quite small.
Therefore the inter-polation effect is usually small or negligible.
Thesecond function is the pure back-off unction: if themore specific terms have zero frequency, the proba-bilities of the more general terms are used instead.Only if terms are of a similar specificity, the A's trulyserve to weight relevance of the interpolation terms.If we isolate the pure back-off unction of the in-terpolation equation we get an algorithm similar tothe one used in Collins & Brooks (1995).
It is givenin a schematic form in Table 1.
Each step consistsof a back-off to a lower level of specificity.
Thereare as many steps as features, and there are a totalof  2 F terms, divided over all the steps.
Because allfeatures are considered of equal importance, we callthis the Naive Back-off algorithm.Usually, not all features x are equally important,so that not all back-off terms are equally relevantfor the re-estimation.
Hence, the problem of fittingthe Axe parameters i  replaced by a term selectiontask.
To optimize the term selection, an evaluationof the up to 2 F terms on held-out data is still neces-sary.
In summary, the Back-off method oes not pro-vide a principled and practical domain-independentmethod to adapt to the structure of a particular do-main by determining a suitable ordering -< betweenevents.
In the next section, we will argue that a for-mal operationalization f similarity between events,as provided by MBL, can be used for this purpose.In MBL the similarity metric and feature weightingscheme automatically determine the implicit back-If f(xl , .
.
.
,  xn) > 0:#(clzl ...,xn) = f(c,~l ..... ~. )'
f (~ l  .....
~.
)Else if f (x l ,  ...,Xn-1, *) "4- ... A- f(*,x2, ...,Xn) > O:~(c lz l , .
.
.
, zn )  = f (c ,~l  ..... ~ , -1 , , )+ .
.
.+f (c , * ,~2 ..... ~)  f(zl,...,z.-1,*)+...+/(*,z2 .....z.
)Else if ... :~(c lz l ,  ..., z,~) =Else if f (x l ,  *, ..., *) + ... + f(*, ..., *, x,~) > O:~(c lz l , .
.
.
, x~)  = f ( c '~ l ' *  .. .
.
.
.
)++f (c ' * '  .
.
.
.
.
.
.
.  )
f(zl,*,...,*)+...+/(*,...,*,z~)Table 1: The Naive Back-off smoothing algorithm.f (X )  stands for the frequency of pattern X in thetraining set.
An asterix (*) stands for a wildcard ina pattern.
The terms at a higher level in the back-offsequence are more specific (-<) than the lower levels.off ordering using a domain independent heuristic,with only a few parameters, in which there is noneed for held-out data.4 A Compar i sonIf we classify pattern X by looking at its nearestneighbors, we are in fact estimating the probabil-ity P(classlX), by looking at the relative frequencyof the class in the set defined by simk(X),  whereslink(X) is a function from X to the set of most sim-ilar patterns present in the training data 3.
Althoughthe name "k-nearest neighbor" might mislead us bysuggesting that classification is based on exactly ktraining patterns, the sima(X) fimction given by theOverlap metric groups varying numbers of patternsinto buckets of equal similarity.
A bucket is definedby a particular number of mismatches with respectto pattern X.
Each bucket can further be decom-posed into a number of schemata characterized bythe position of a wildcard (i.e.
a mismatch).
Thussimk(X) specifies a ~ ordering in a Collins 8z Brooksstyle back-off sequence, where each bucket is a stepin the sequence, and each schema is a term in theestimation formula at that step.
In fact, the un-weighted overlap metric specifies exactly the sameordering as the Naive Back-off algorithm (table 1).In Figure 1 this is shown for a four-featured pat-tern.
The most specific schema is the schema withzero mismatches, which corresponds to the retrievalof an identical pattern from memory, the most gen-eral schema (not shown in the Figure) has a mis-match on every feature, which corresponds to the3Note that MBL is not limited to choosing the bestclass.
It can also return the conditional distribution ofall the classes.438Overlapexact matchI I I I IOverlap IGI I i t l1 mismatch 2 mismatches 3 mismatchesXXXXxx  XXXXX ?
XXx ~ X ??
X ?xx?
Xx XIX\] I I I 1><\]><3 I I ~I 1><3 I I IX\] I I><:\]I I I IX\] I IXI IX\]Figure 1: An analysis of nearest neighbor sets into buckets (from left to right) and schemata (stacked).
IGweights reorder the schemata.
The grey schemata re not used if the third feature has a very high weight(see section 5.1).entire memory being best neighbor.If Information Gain weights are used in combina-tion with the Overlap metric, individual schematainstead of buckets become the steps of the back-offsequence 4.
The -~ ordering becomes lightly morecomplicated now, as it depends on the number ofwildcards and on the magnitude of the weights at-tached to those wildcards.
Let S be the most specific(zero mismatches) chema.
We can then define theordering between schemata in the following equa-tion, where A(X,Y)  is the distance as defined inequation 1.s' -< s" ~ ~,(s, s') < a(s, s") (6)Note that this approach represents a type of im-plicit parallelism.
The importance of the 2~back-offterms is specified using only F parameters--the IGweights-, where F is the number of features.
Thisadvantage is not restricted to the use of IG weights;many other weighting schemes exist in the machinelearning literature (see Wettschereck et aL (1997)for an overview).Using the IG weights causes the algorithm to relyon the most specific schema only.
Although in mostapplications this leads to a higher accuracy, becauseit rejects schemata which do not match the mostimportant features, sometimes this constraint needs4Unless two schemata re exactly tied in their IGvalues.to be weakened.
This is desirable when: (i) thereare a number of schemata which are almost equallyrelevant, (ii) the top ranked schema selects too fewcases to make a reliable estimate, or (iii) the chancethat the few items instantiating the schema re mis-labeled in the training material is high.
In suchcases we wish to include some of the lower-rankedschemata.
For case (i) this can be done by discretiz-ing the IG weights into bins, so that minor differ-ences will lose their significance, in effect mergingsome schemata back into buckets.
For (ii) and (iii),and for continuous metrics (Stanfill & Waltz, 1986;Cost & Salzberg, 1993) which extrapolate from ex-actly k neighbors 5, it might be necessary to choose ak parameter larger than 1.
This introduces one addi-tional parameter, which has to be tuned on held-outdata.
We can then use the distance between a pat-tern and a schema to weight its vote in the nearestneighbor extrapolation.
This results in a back-offsequence in which the terms at each step in the se-quence are weighted with respect o each other, butwithout the introduction of any additional weight-ing parameters.
A weighted voting function thatwas found to work well is due to Dudani (1976): thenearest neighbor schema receives a weight of 1.0, thefurthest schema weight of 0.0, and the other neigh-bors are scaled linearly to the line between these twopoints.5Note that the schema nalysis does not apply tothese metrics.439MethodIB1 (=Naive Back-off)IBI-IGLexSpace IGBack-off model (Collins & Brooks)C4.5 (Ratnaparkhi et al)Max Entropy (Ratnaparkhi et al)Brill's rules (Collins 8z Brooks)% Accuracy83.7 %84.1%84.4 %84.1%79.9 %81.6 %81.9 %Table 2: Accuracy on the PP-attachment test set.5 App l i ca t ions5.1 PP -a t tachmentIn this section we describe experiments with MBLon a data-set of Prepositional Phrase (PP) attach-ment disambiguation cases.
The problem in thisdata-set is to disambiguate whether a PP attachesto the verb (as in I ate pizza with a fork) or to thenoun (as in I ate pizza with cheese).
This is a dif-ficult and important problem, because the semanticknowledge needed to solve the problem is very diffi-cult to model, and the ambiguity can lead to a verylarge number of interpretations for sentences.We used a data-set extracted from the PennTreebank WSJ corpus by Ratnaparkhi et al (1994).It consists of sentences containing the possiblyambiguous equence verb noun-phrase PP.
Caseswere constructed from these sentences by record-ing the features: verb, head noun of the first nounphrase, preposition, and head noun of the nounphrase contained in the PP.
The cases were la-beled with the attachment decision as made bythe parse annotator of the corpus.
So, for thetwo example sentences given above we would getthe feature vectors ate,pizza,with,fork,V, andate ,p i zza ,  with,  cheese, N. The data-set contains20801 training cases and 3097 separate test cases,and was also used in Collins & Brooks (1995).The IG weights for the four features (V,N,P,N)were respectively 0.03, 0.03, 0.10, 0.03.
This identi-fies the preposition as the most important feature:its weight is higher than the sum of the other threeweights.
The composition of the back-off sequencefollowing from this can be seen in the lower partof Figure 1.
The grey-colored schemata were effec-tively left out, because they include a mismatch onthe preposition.Table 2 shows a comparison of accuracy on thetest-set of 3097 cases.
We can see that Is l ,  whichimplicitly uses the same specificity ordering as theNaive Back-off algorithm already performs quite wellin relation to other methods used in the literature.Collins & Brooks' (1995) Back-off model is more so-phisticated than the naive model, because they per-formed a number of validation experiments on held-out data to determine which terms to include and,more importantly, which to exclude from the back-off sequence.
They excluded all terms which didnot match in the preposition!
Not surprisingly, the84.1% accuracy they achieve is matched by the per-formance of IBI-IG.
The two methods exactly mimiceach others behavior, in spite of their huge differ-ence in design.
It should however be noted that thecomputation of IG-weights is many orders of mag-nitude faster than the laborious evaluation of termson held-out data.We also experimented with rich lexical represen-tations obtained in an unsupervised way from wordco-occurrences in raw WSJ text (Zavrel & Veenstra,1995; Schiitze, 1994).
We call these representationsLexical Space vectors.
Each word has a numeric 25dimensional vector representation.
Using these vec-tors, in combination with the IG weights mentionedabove and a cosine metric, we got even slightly bet-ter results.
Because the cosine metric fails to groupthe patterns into discrete schemata, it is necessaryto use a larger number of neighbors (k = 50).
Theresult in Table 2 is obtained using Dudani's weightedvoting method.Note that to devise a back-off scheme on the basisof these high-dimensional representations (each pat-tern has 4 x 25 features) one would need to considerup to 2 l??
smoothing terms.
The MBL frameworkis a convenient way to further experiment with evenmore complex conditioning events, e.g.
with seman-tic labels added as features.5.2 POS- tagg ingAnother NLP problem where combination of differ-ent sources of statistical information is an impor-tant issue, is POS-tagging, especially for the guess-ing of the POS-tag of words not present in the lex-icon.
Relevant information for guessing the tag ofan unknown word includes contextual information(the words and tags in the context of the word), andword form information (prefixes and suffixes, firstand last letters of the word as an approximation ofaffix information, presence or absence of capitaliza-tion, numbers, special characters etc.).
There is alarge number of potentially informative features thatcould play a role in correctly predicting the tag ofan unknown word (Ratnaparkhi, 1996; Weischedelet al, 1993; Daelemans et al, 1996).
A priori, itis not clear what the relative importance is of thesefeatures.We compared Naive Back-off estimation and MBLwith two sets of features:?
PDASS:  the first letter of the unknown word (p),the tag of the word to the left of the unknownword (d), a tag representing the set of possiblelexical categories of the word to the right of theunknown word (a), and the two last letters (s).The first letter provides information about cap-italisation and the prefix, the two last letters440about suffixes.?
PDDDAAASSS: more left and right context fea-tures, and more suffix information.The data set consisted of 100,000 feature valuepatterns taken from the Wall Street Journal corpus.Only open-class words were used during construc-tion of the training set.
For both IBI-IG and NaiveBack-off, a 10-fold cross-validation experiment wasrun using both PDASS and PDDDAAASSS patterns.The results are in Table 3.
The IG values for thefeatures are given in Figure 2.The results how that for Naive Back-off (and ml )the addition of more, possibly irrelevant, featuresquickly becomes detrimental (decrease from 88.5 to85.9), even if these added features do make a gener-alisation performance increase possible (witness theincrease with IBI-IG from 88.3 to 89.8).
Notice thatwe did not actually compute the 21?
terms of NaiveBack-off in the PDDDAAASSS condition, as IB1 isguaranteed to provide statistically the same results.Contrary to Naive Back-off and IB1, memory-basedlearning with feature weighting (ml-IG) managesto integrate diverse information sources by differ-entially assigning relevance to the different features.Since noisy features will receive low IG weights, thisalso implies that it is much more noise-tolerant.0.30  -0 .25  -0.20  -Fo~ "3~_ 0 .15-0 .10  ,0 .05  -0 .0 -p d d d a a a s s sfeatureFigure 2: IG values for features used in predictingthe tag of unknown words.IB1, Naive Back-off IBI-IGPDASS 88.5 (0.4) 88.3 (0.4)PDDDAAASSS 85.9 (0.4) 89.8 (0.4)Table 3: Comparison of generalization accuracy ofBack-off and Memory-Based Learning on predictionof category of unknown words.
All differences arestatistically significant (two-tailed paired t-test, p <0.05).
Standard deviations on the 10 experimentsare between brackets.6 Conc lus ionWe have analysed the relationship between Back-off smoothing and Memory-Based Learning and es-tablished a close correspondence b tween these twoframeworks which were hitherto mostly seen as un-related.
An exception is the use of similarity for al-leviating the sparse data problem in language mod-eling (Essen & Steinbiss, 1992; Brown et al, 1992;Dagan et al, 1994).
However, these works differ intheir focus from our analysis in that the emphasisis put on similarity between values of a feature (e.g.words), instead of similarity between patterns thatare a (possibly complex) combination of many fea-tures.The comparison of MBL and Back-off shows thatthe two approaches perform smoothing in a very sim-ilar way, i.e.
by using estimates from more generalpatterns if specific patterns are absent in the train-ing data.
The analysis hows that MBL and Back-offuse exactly the same type of data and counts, andthis implies that MBL can safely be incorporatedinto a system that is explicitly probabilistic.
Sincethe underlying k-NN classifier is a method that doesnot necessitate any of the common independence ordistribution assumptions, this promises to be a fruit-ful approach.A serious advantage of the described approach,is that in MBL the back-off sequence is specifiedby the used similarity metric, without manual in-tervention or the estimation of smoothing parame-ters on held-out data, and requires only one param-eter for each feature instead of an exponential num-ber of parameters.
With a feature-weighting met-ric such as Information Gain, MBL is particularlyat an advantage for NLP tasks where conditioningevents are complex, where they consist of the fusionof different information sources, or when the data isnoisy.
This was illustrated by the experiments onPP-attachment and POS-tagging data-sets.441AcknowledgementsThis research was done in the context of the "Induc-tion of Linguistic Knowledge" research programme,partially supported by the Foundation for Lan-guage Speech and Logic (TSL), which is funded bythe Netherlands Organization for Scientific Research(NWO).
We would like to thank Peter Berck andAnders Green for their help with software for theexperiments.Re ferencesD.
Aha, D. Kibler, and M. Albert.
1991.
Instance-based Learning Algorithms.
Machine Learning,Vol.
6, pp.
37-66.L.R.
Bahl, F. Jelinek and R.L.
Mercer.
1983.A Maximum Likelihood Approach to Continu-ous Speech Recognition.
IEEE Transactions onPattern Analysis and Machine Intelligence, Vol.PAMI-5 (2), pp.
179-190.Peter F. Brown, Vincent J. Della Pietra, PeterV.
deSouza, Jennifer C. Lai, and Robert L. Mer-cer.
1992.
Class-based N-gram Models of NaturalLanguage.
Computational Linguistics, Vol.
18(4),pp.
467-479.Claire Cardie.
1994.
Domain Specific Knowl-edge Acquisition for Conceptual Sentence Anal-ysis, PhD Thesis, University of Massachusets,Amherst, MA.Claire Cardie.
1996.
Automatic Feature Set Selec-tion for Case-Based Learning of Linguistic Knowl-edge.
In Proc.
of the Conference on EmpiricalMethods in Natural Language Processing, May 17-18, 1996, University of Pennsylvania.Stanley F.Chen and Joshua Goodman.
1996.
AnEmpirical Study of Smoothing Techniques forLanguage Modelling.
In Proc.
of the 34th AnnualMeeting of the ACL, June 1996, Santa Cruz, CA,ACL.Kenneth W. Church and William A. Gale.
1991.A comparison of the enhanced Good-Turing anddeleted estimation methods for estimating proba-bilities of English bigrams.
Computer Speech andLanguage, Vol 19(5), pp.
19-54.M.
Collins.
1996.
A New Statistical Parser Based onBigram Lexical Dependencies.
In Proc.
of the 34thAnnual Meeting of the ACL, June 1996, SantaCruz, CA, ACL.M.
Collins and J. Brooks.
1995.
PrepositionalPhrase Attachment through a Backed-Off Model.In Proceedings of the Third Workshop on VeryLarge Corpora, Cambridge, MA.S.
Cost and S. Salzberg.
1993.
A weighted near-est neighbour algorithm for learning with symbolicfeatures.
Machine Learning, Vol.
10, pp.
57-78.Walter Daelemans and Antal van den Bosch.1992.
Generalisation Performance of Backprop-agation Learning on a Syllabification Task.
InM.
F. J. Drossaers & A. Nijholt (eds.
), TWLT3:Connectionism and Natural Language Processing.Enschede: Twente University.
pp.
27-37.Walter Daelemans.
1995.
Memory-based lexicalacquisition and processing.
In P. Steffens (ed.
),Machine Translation and the Lexicon.
SpringerLecture Notes in Artificial Intelligence, no.
898.Berlin: Springer Verlag.
pp.
85-98Walter Da~lemans.
1996.
Abstraction ConsideredHarmful: Lazy Learning of Language Process-ing.
In J. van den Herik and T. Weijters (eds.),Benelearn-96.
Proceedings of the 6th Belgian-Dutch Conference on Machine Learning.
MA-TRIKS: Maastricht, The Netherlands, pp.
3-12.Walter Daelemans, Jakub Zavrel, Peter Berck, andSteven Gillis.
1996.
MBT: A Memory-Based Partof Speech Tagger Generator.
In E. Ejerhed andI.
Dagan (eds.)
Proc.
of the Fourth Workshop onVery Large Corpora, Copenhagen: ACL SIGDAT,pp.
14-27.Walter Daelemans, Antal van den Bosch, and TonWeijters.
1997.
IGTree: Using Trees for Com-pression and Classification in Lazy Learning Al-gorithms.
In D. Aha (ed.)
Artificial IntelligenceReview, special issue on Lazy Learning, Vol.
11(1-5).Ido Dagan, Fernando Pereira, and Lillian Lee.
1994.Similarity-Based Estimation of Word Cooccur-fence Probabilities.
In Proc.
of the 32nd AnnualMeeting of the ACL, June 1994, Las Cruces, NewMexico, ACL.S.A.
Dudani.
1981 The Distance-Weighted k-Nearest Neighbor Rule IEEE Transactions onSystems, Man, and Cybernetics, Vol.
SMC-6, pp.325-327.Ute Essen, and Volker Steinbiss.
1992.
CoocurrenceSmoothing for Stochastic Language Modeling.
InProc.
ofICASSP, Vol.
1, pp.
161-164, IEEE.Slava M. Katz.
1987.
Estimation of Probabilitiesfrom Sparse Data for the Language Model Com-ponent of a Speech Recognizer IEEE Transac-tions on Acoustics, Speech and Signal Processing,Vol.
ASSP-35, pp.
400-401, March 1987.David M. Magerman.
1994.
Natural Language Pars-ing as Statistical Pattern Recognition, PhD The-sis, Department of Computer Science, StanfordUniversity.Hwee Tou Ng and Hian Beng Lee.
1996.
Integrat-ing Multiple Knowledge Sources to DisambiguateWord Sense: An Exemplar-Based Approach.
InProc.
of the 3~th Annual Meeting of the ACL,June 1996, Santa Cruz, CA, ACL.442J .
.R.
Quinlan.
1986.
Induction of Decision Trees.Machine Learning, Vol.
1, pp.
81-206.J.
.R.
Quinlan.
1993.
?4.5: Programs for MachineLearning.
San Mateo, CA: Morgan Kaufmann.Adwait Ratnaparkhi.
1996.
A Maximum EntropyPart-Of-Speech Tagger.
In Proc.
of the Confer-ence on Empirical Methods in Natural LanguageProcessing, May 17-18, 1996, University of Penn-sylvania.A.
Ratnaparkhi, J. Reynar and S. Roukos.
1994.
Amaximum entropy model for Prepositional PhraseAttachment.
In ARPA ~,Vorkshop n Human Lan-guage Technology, Plainsboro, NJ.Christer Samuelsson.
1996.
Handling Sparse Databy Successive Abstraction In Proc.
of the Interna-tional Confercncc on Computational Linguistics(COLING'96), August 1996, Copenhagen, Den-mark.Hinrich Sch/itze.
1994.
Distributional Part-of-Speech Tagging.
In Proc.
of the 7th Conference ofthe European Chapter of the Association for Com-putational Linguistics (EACL'95), Dublin, Ire-land.C.
Stanfill and D. Waltz.
1986.
Toward memory-based reasoning.
Communications of the ACM,Vol.
29, pp.
1213-1228.Ralph Weischedel, Marie Meteer, Richard Schwartz,Lance Ramshaw, and Jeff Palmucci.
1993.
Cop-ing with Ambiguity and Unknown Words throughProbabilistic Models.
Computational Linguistics,Vol.
19(2).
pp.
359-382.D.
Wettschereck, D. W. Aha, and T. Mohri.1997.
A Review and Comparative Evaluation ofFeature-Weighting Methods for Lazy Learning Al-gorithms.
In D. Aha (ed.)
Artificial IntelligenceReview, special issue on Lazy Learning, Vol.
11(1-5).Jakub Zavrel and Jorn B. Veenstra.
1995.
The Lan-guage Environment and Syntactic Word-Class Ac-quisition.
In C.Koster and F.Wijnen (eds.)
Proc.of the Groningen Assembly on Language Acquisi-tion (GALA95).
Center for Language and Cogni-tion, Groningen, pp.
365-374.443
