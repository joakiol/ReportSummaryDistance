Automatic Acquisition of Subcategorization Framesfrom Tagged TextMichael R. Brent and Robert C. BerwickMIT  Artif icial Intell igence Lab545 Technology SquareCambridge,  MA 02139ABSTRACTThis paper describes an implemented program that takes a taggedtext corpus and generates a partial ist of the subcategorizationframes in wtfich each verb occurs.
The completeness of the out-put list increases monotonically with the total occurrences ofeachverb in the training corpus.
False positive rates are one to threepercent.
Five subeategorization frames are currently detectedand we foresee no impediment to detecting many more.
Ulti-mately, we expect to provide a large subcategorization dictio-nary to the NLP community and to train dictionaries for specificcorpora.INTRODUCTIONAccurate parsing requires knowing the subcategoriza-tion frames of verbs, as shown by (1).
(1) a. I expected \[NP the man who smoked NP\] to eatice-creamb.
I doubted \[NP the man who liked to eat ice-creamNP\]Current high-coverage parsers tend to use either custom,hand-generated lists of subcategorization frames (e.g., \[7\]),or published, hand-generated lists like the Ox\[ord AdvancedLearner's Dictionary of Contemporary English, \[9\] (e.g., \[5\]).In either case, such lists are expensive to build and to main-tain in the face of evolving usage.
In addition, they tendnot to include rare usages or specialized vocabularies likefinancial or military jargon.
Further, they are often incom-plete in arbitrary ways.
For example, Webster's Ninth NewCollegiate Dictionary lists the sense of strike meaning "tooccur to", as in "it struck him that .
.
. "
,  but it does notlist that same sense of hit.
(Our program discovered both.
)To address these problems we have implemented a programthat takes a tagged text corpus and generates a partial listof the subcategorization frames in which each verb occurs.The program uses only a small, finite-state grammar for afragment of English.
The completeness of the output list in-creases monotonically with the total number of occurrencesof each verb in the training corpus.Automatically learning subcategorization frames (SFs)is impeded by a bootstrapping problem - -  you can't parsewithout knowing SFs and you can't learn from exampleswithout parsing them.
For instance, the obvious approachto identifying verbs that take infinitival complements wouldbe to look for a verb followed by an infinitive.
Unfortu-nately, as shown by (1), finding such a case does not licenseany definite conclusions.
Our system bootstraps by rec-ognizing those sentences that it can parse without alreadyknowing the SFs - -  mainly sentences involving pronouns orproper names rather than full noun-phrases in certain argu-ment positions.
It simply ignores other sentences.
The dis-tributional constraints on pronouns and full noun-phrasesare almost identical, so lessons learned in the easy-to-parsecases apply to all cases.The remainder of this paper consists of a section de-scribing and quantifying our results, a section describingthe methods used to obtain them, and a section discussingrelated work.RESULTSSo far, we have concentrated on the five subcategoriza-tion frames shown in Table 1.
Table 2 shows the resultsSF Description Good Example Bad Exampledirect objectdirect object &clausedirect object &infinitiveclauseinfinitivehit themtell him he's a foolwant him to attendknow I'll attendhope to attend* arrive them* slap him he's a fool* hope him to attend* want I'll attend* hit to attendTable 1: The five subcategorization frames detected so farobtained on a 2.6 million-word Wall Street Journal corpus342provided by Penn Treebank project (\[2\]).
aSF tokensfoundDO I 8,606% false positives & source of error1.0% Subj of comp clause taken for DO0.5% Adv taken for DODO & 381clauseDO & inf 3,597clause 14,144inf comp 11,8801.0% Rel.
clause taken as comp.
clause0.5% Fronted adjunct taken as mainclause0.5% Comp belonged to a higher verb1.5% Purposive adjuncts taken for inf.Demonstrative "that" taken for comp.2.0% Purposive adjuncts taken for inf.1.0% Adjective comp like "hard to take"Quantity of text processed = 2,644,618 words of WSJTotal time = 192.5 seconds(tagged, SPARC 2)Throughput Rate = 13738.3 words/secondsTable 2: Top: Lexicographic results, error rates, and sources oferror.
Bottom: speed and volume.METHODOLOGYOur program uses a finite-state grammar for recogniz-ing the auxiliary, and determining subcategorization frames.The English auxiliary system is known to be finite state andour treatment of it is standard, so the first subsection dis-cusses the determination f subcategorization frames.
Thesecond subsection describes a planned statistical approachto the one to three percent error rates described above.Complement  GrammarThe obvious approach to finding an SF like "V NP toV" is to look for occurrences of just that pattern in thetraining corpus, but the obvious approach fads to addressthe bootstrapping problem, as shown by (1) above.
Oursolution is based on the following insights:?
Some examples are clear and unambiguous.?
Observations made in clear cases generalize to all cases.?
It is possible to distinguish the clear cases from theambiguous ones with reasonable accuracy.?
With enough examples, it pays to wait for the clearcases.1 Error rates computed by hand verification of 200 examples foreach SF using the tagged mode.
Error rates for verb detection areestimated separately below.Rather than take the obvious approach of looking for "VNP to V", we look for clear cases like "V PRONOUN toV ' .
The advantages can be seen by contrasting (2) with (1)(page 1).
(1) a. oK I expected him to eat ice-creamb.
* I doubted him to eat ice-creamMore generally, our system recognizes linguistic structureusing a small finite-state grammar that describes only thatfragment of English that is most useful for recognizing SFs.The grammar relies exclusively on closed-class lexlcal itemssuch as pronouns, prepositions, determiners, and auxiliaryverbs.The complement grammar needs to distinguish threetypes of complements: direct objects, infinitives, and clauses.Figure 1 shows a substantial part of the grammar responsi-ble for detecting these complements.
Any verb followed im-<clause><subj-pron><subj-obj -pron><DO><DO><obj-pron><in f in i t i ve>:= that?
(<subj-pron> I <subj-obj -pron> Ih i s  I <proper-name>)<tensed-verb>:= I \] he I she I we I they:= you, i t ,  yours,  hers ,  ours,  the i rs:= <obj-pron>:ffi (<subj-obj -pron> I <proper-name>):<tensed-verb>:ffi me I him \[ us \[ them:ffi to <uninf lected-verb>Figure 1: A non-recursive (finite-state) grammar for detect-ing certain verbal complements.
"T' indicates an optionalelement.
<DO> is specified in context-sensitive notation,for convenience.
Any verb followed immediately expressionsmatching <DO>, <clause>, <infinitive>, <DO> <clause>, or<DO> <infinitive> is assigned the corresponding SF.mediately by matches for <DO>, <clause>, <infinitive>,<DO><clause>, or <DO><inf> is assigned the correspond-ing SF.
Adverbs are ignored for purposes of adjacency.
Thenotation "?"
follows optional expressions, and D0 is speci-fied in context-sensitive notation for convenience.Robust  Classif icationOur system, like any other, occasionally makes mis-takes.
Error rates of one to three percent are a substantialaccomplishment, but if a word occurs enough times in acorpus it is bound to show up eventually in some construe-tion that fools the system.
For that reason any learning343system that gets only positive examples and makes a per-manent judgment on a single example will always degradeas the number of occurrences increases.
In fact, makinga judgment based on any fixed number of examples withany finite error rate will always lead to degradation withcorpus-size.
A better approach is to require a fixed per-centage of the total occurrences of any given verb to appearwith a given SF before concluding that random error is notresponsible for these observations.
Unfortunately, the cutoffpercentage is arbitrary and sampling error makes classifica-tion unstable for verbs with few occurrences in the input.The sampling error can be dealt with (\[1\]) but the arbitrarycutoffpercentage can't, z Rather than using fixed cutoffs, weare developing an approach that will automatically gener-ate statistical models of the sources of noise using standardregression techniques.
For example, purposive adjuncts like"Jon quit to pursue a career in finance" are quite rare, ac-counting for only two percent of the apparent infinitivalcomplements.
Furthermore, they are distributed across amuch larger set of matrix verbs than the true infinitivalcomplements, so any given verb occurs very rarely indeedwith purposive adjuncts.
In a histogram sorting verbs bytheir apparent frequency of occurrence with infinitival com-plements, those that in fact have appeared with purposiveadjuncts and not true infinitival complements will be clus-tered at the low frequencies.
The distributions of such clus-ters can be modeled automatically and the models used foridentifying false positives.RELATED WORKInterest in extracting lexical and especially colloca-tional information from text has risen dramatically in thelast two years, as sufficiently large corpora and sufficientlycheap computation have become available.
Three recent pa-pers in this area are \[3\], \[8\], and \[12\].
The latter two are con-cerned exclusively with collocation relations between open-class words and not with grammatical properties.
Churchis also interested primarily in open-class collocations, buthe does discuss verbs that tend to be followed by infinitiveswithin his mutual information framework.Mutual information, as applied by Church, is a measureof the tendency of two items to appear near one-another - -their observed frequency in nearby positions is divided bythe expectation of that frequency if their positions were ran-dom and independent.
As Church points out, having suchstatistics for word-pairs is useful for the predictive modelsused in optical character-recognit ion and speech recogni-tion as well as for syntactic disambiguation.
To measuresthe tendency of verbs to be followed within a few words by2Note that this is not an arbitrary confidence level, which would beless unsavory, but an actual percentage of verb occurrences.
That  is,there is a fact of the matter -- a natural clustering, but no systematiccharacterization of it is available, so an eyeball estimate must  be usedinstead.infinitives, Church uses his statistical disambiguator (\[4\]) todistinguish between to as an infinitive marker and to as apreposition.
Then he measures the mutual information be-tween occurrences of the verb and occurrences of infinitivesfollowing within a certain number of words.
Unlike our sys-tem, Church's approach does not aim to decide whether ornot a verb occurs with an infinitival complement - - exam-ple (1) showed that being followed by an infinitive is not thesame as taking an infinitival complement.
It might be in-teresting to try building a verb categorization scheme basedon Church's mutual information measure, but to the bestof our knowledge no such work has been reported.CONCLUSIONSThe initial results reported above are only the begin-ning of what promises to be a be large and rewarding en-deavor.
In a forthcoming paper Brent reports on acquisitionof subeategorization frames using raw, untagged text.
Run-ning on raw text, the program starts with only the grammarand a lexicon of some 200 closed-class words.
This opensup the possibility of learning from literally hundreds of mil-lions of words of text without worrying the possible majorcategories of all the words or their relative frequencies.Along with implementing detection schemes for moreSFs, our next major goal will be noise-reduction.
If thatis successful we hope to release to the community a sub-stantial dictionary of verbs and subcategorization frames.We also hope to use the SF information for semantic at-egorization \[6\] using lex ica l -syntax/ lex ica l -semant ics  con-straints \[10, 11\].
A particularly clear example of how thiscan be done is provided by the verbs taking DO&clausewith a non-pleonastic subject: all such verbs can describecommunication \[13\].
The complete list of DO&clause verbsour program program found more than once, running inraw text mode on 2.6 million words of Wall Street Journal,supports Zwicky's observation (3).
(1) advise, assure, convince, inform, reassure, remind,tell, warnACKNOWLEDGMENTSThanks to Don Hindle, Leila Gleitman, and Jane Grimshawfor useful and encouraging conversations.
Thanks also toMark Liberman and the Penn Treebank project at the Uni-versity of Pennsylvania for supplying tagged text.
Thiswork was supported in part by National Science Foundationgrant DCR-85552543 under a Presidential Young Investiga-tor Award to Professor Robert C. Berwick.REFERENCES1.
M. Brent.
Semantic Classification of Verbs from their Syn-tactic Contexts: An Implemented Case Study of Stativity.344In Proceedings of the 5th European A CL Conference.
Asso-ciation for Computational Linguistics, 1991.2.
E. Bril], D. Magerman, M. Marcus, B. Santorinl.
DeducingLinguistic Structure from the Statistics of Large Corpora.Proceedings of the 3rd DARPA Speech and Nat~tral Lan-guage Workshop, 1990.3.
K. Church and P. Hanks.
Word association orms, mutualinformation, and lexicography.
Comp.
Ling., 16, 1990.4.
K. Church.
A Stochastic Parts Program and Noun PhraseParser for Unrestricted Text.
In Proceedings of the ~nd A CLConference on Applied NLP.
ACL, 1988.5.
C. DeMarcken.
Parsing the LOB Corpus.
In Proceedings ofthe A CL.
Association for Comp.
Ling., 1990.6.
L. Gleitman.
The structural sources of verb meanings.
Lan-g~tage Acquisition, 1(1):3-56, 1990.7.
D. Hindle.
User Manual for Fiddltch, a DeterministicParser.
Technical Report 7590-142, Naval Research Lab-oratory, 1983.8.
D. Hindle.
Noun classification from predicate argumentstructures.
In Proceedings of the ~8th Annual Meeting ofthe A CL, pages 268-275.
ACL, 1990.9.
A. Hornby and A.
Covey.
Oxford Advanced Learner's Dic-tionary of Contemporary English.
Oxford University Press,Oxford, 1973.10.
B. Levin.
English Verbal Diathesis.
Lexicon Project work-ing Papers no.
32, MIT Center for Cognitive Science, MIT,Cambridge, MA., 1989.11.
S. Pinker.
LearnabiliQi and Cognition: The Acquisition ofArgument Structure.
MIT Press, Cambridge, MA, 1989.12.
F. Smadja and K. McKeown.
Automatically extracting andrepresenting collocations for language generation.
In ~SthAnn~tal Meeting oj r the Association .for Comp.
Ling., pages252-259.
ACL, 1990.13.
A. Zwicky.
In a Manner of Speaking.
Linguistic Inquiry,2:223-23.3, 1970.345
