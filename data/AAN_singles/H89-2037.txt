TOWARDS SPEECH RECOGNITIONWITHOUT VOCABULARY-SPECIFIC TRAININGHsiao-Wuen Hon, Kai-Fu Lee, Robert WeideSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213AbstractWith the emergence ofhigh-performance speaker-independent sys-tems, a great barrier to man-machine interface has been overcome.This work describes our next step to improve the usability of speechrecognizers---the use of vocabulary-independent (VI) models.
Ifsuccessful, VI models are trained once and for all.
They willcompletely eliminate task-specific training, and will enable rapidconfiguration of speech recognizers for new vocabularies.
Our initialresults using generalized triphones a VIi models how that with moretraining data and more detailed modeling, the error rate of VI modelscan be reduced substantially.
For example, the error rates for VImodels with 5,000, 10,000 and 15,000 training sentences are 23.9%,15.2% and 13.3% respectively.
Moreover, if task-specific trainingdata were available, we can interpolate them with VI models.
Ourprelimenary results show that this interpolation can lead to an 18%error rate reduction over task-specific models.1.
IntroductionOne of the most exciting and promising areas of speechresearch is large-vocabulary continuous peech recognition.A myriad of applications await a good speech recognizer.However, while many reasonable recognizers exist today,they are impractical and inflexible due to the tedious processof configuring a recognizer.
This tedium is typically em-bodied in one of the following forms:?
Speaker-specific training: each speaker mustspeak for about an hour to train the system.?
Vocabulary-specific training: with each newvocabulary comes the dilemma of tedious retrain-ing for optimal performance, or tolerating sub-stantially higher error rate.?
Training time: with each new speaker orvocabulary, many hours are needed to process thespeech and train the system.Recent work at Carnegie Mellon \[1, 2\] and several otherlaboratories has shown that highly accurate speaker-independent speech recognition is possible, thus alleviatingthe need for speaker-dependent training.
However, thesespeaker-independent sys ems till need vocabulary-dependenttraining on a large population of speakers for eachvocabulary, which requires a very large amount of time fordata collection (weeks to months), dictionary generation (daysto weeks), and processing (hours to days).As speech recognition flourishes and new applicationsemerge, the demand for vocabulary-specific training will be-come the bottleneck in building speech recognizers.
In thispaper, we will discuss our initial work to alleviate the tediousvocabulary-specific training process.Our work thus far has involved collecting and processing alarge general English database, and evaluating thegeneralized triphone \[3, 2\] as a vocabulary independent unit.We collected and trained generalized triphone models on upto 15,000 training sentences, and compared our results to thatfrom vocabulary-dependent models.
We found that as weincreased VI training data, VI generalized triphones improvedfrom 109% more errors than vocabulary-dependent training toonly 16% more errors.
In another vocabulary-adaptation ex-periment, we found that interpolating vocabulary-dependentmodels with vocabulary-independent models reduces the errorrate by 18%.Based on the enouraging results of this preliminary study,we conjecture that generalized triphones are a reasonablestarting point in our search for a more vocabulary-independent subword unit.
In the future, we hope to furtherincrease our training database.
With increased training datawill come the ability to train more detailed subword models.We expect that this combination will enable us to furtherimprove our results.In this paper, we will first discuss the issue of VI models.Next, we will briefly describe generalized triphones.
Then,we will describe our databases and experimental results.Finally, we will close with some concluding remarks aboutthis and future work.2.
Vocabulary-Independent Subword ModelingSubword modeling has become an increasingly more im-portant issue because as the vocabulary capacity of recog-nizers increases, it becomes difficult, if not impossible, totrain whole-word models.
Many subword modeling tech-niques have been proposed (see \[4\] for a survey on thesetechniques).
However, most subword models were evaluatedusing the same vocabulary for training and testing.
An impor-tant question that has often been ignored is: how well willthese subword models perform under vocabulary-independentconditions?
In other words, if we train on one vocabulary andtest on another, will the performance degrade considerably?If so, it will then be necessary to retrain for each newvocabulary, which is time-consuming, tedious, and costly.Why should performance degrade across vocabularies?There are two main causes: the lack of coverage and theinconsistency of the models.
The coverage problem is caused271by the fact that the phonetic events in the testing vocabularyare not covered by the training vocabulary.
This lack ofcoverage makes it impossible to train the models needed forthe testing vocabulary.
Instead, we must improvise with amore general model.
For example, if the phone/ t /  in thetriphone context o f / s  t r /  occurs in testing but not intraining, it will be necessary to use a more general model,such as / t /  in the context o f / s  t / ,  o r / t  r / ,  or even acontext-independent/t/.The problem of improvising with general models is thatthey may become inconsistent.
That is, the same model maygenerate many different realizations.
For example, if context-independent phone models are used, the same model fo r / t /must capture various events, such as flapping, unreleasedstop, and realizations in / t s /and/ t  r / .
Then, i f / t  s /is the only context in which/ t /occurs  in the training, while/ t  r / i s  the only context in the testing, the model used willbe highly inappropriate.To ensure that the models are consistent and that newcontexts are covered, it is necessary to account for all causesof phonetic variability.
However, the enumeration f all thecauses* will lead to an astronomical number of models.
Thismakes the models untrainable, which renders them powerless.In view of the above analysis, we believe that a successfulapproach to vecabulary-independent subword modeling mustuse models that are consistent, rainable, and generalizable.Consistency means the variabilities within a model should beminimized; trainability means there should be sufficient train-ing data for each model; and generalizability means reason-able models for the testing vocabulary can be used in spite ofthe lack of precise coverage in the training.3.
Generalized TriphonesIn this section, we describe the basis of our current workgeneralized triphone models, which are based on triphonemodels \[5\].
Triphones account for the left and right phoneticcontexts by creating a different model for each possible con-text pair.
Since the left and right phonetic ontexts are themost important factors that affect the realization of a phone,triphone models are a powerful technique and have led togood results.
However, there are a very large number oftriphones, which can only be sparsely trained.
Moreover, theydo not take into account the similarity of certain phones intheir effect on other phones (such as /b /  and /p /  onvowels).In view of this, we introduce generalized triphone models\[3\].
Generalized triphones are created from triphone modelsusing a clustering procedure that combines triphone HMMsaccording to a maximum likelihood criterion.
In other words,we want to cluster triphones into a set of generalized*A paxtial ist might include: ph(metic ontexts, articulator position, stress,semantics, prosody, intonation, dialect, accent, loudness, peaking-rate, speakeranat(mly, ete.triphones that will have as high as possible a probability ofgenerating the training data.
This is consistent with themaximum-likelihood criterion used in the forward-backwardalgorithm.Context generalization provides the ideal means for findingthe equilibrium between trainability and consistency.
Given afixed amount of training data, it is possible to find the largestnumber of trainable models that are consistent.
Moreover, itis easy to incorporate other causes of variability such asstress, syllable position, and word position.One flaw with bottom-up clustering approaches to contextgeneralization is that there is no easy way of generalizing tocontexts that have not been observed before.
Indeed, in apilot experiment, we found that generalized triphones trainedon the resource management task performed poorly on a newvoice calculator vocabulary.
We believe this was mainly dueto the fact that 36.3% of the triphones in the testingvocabulary were not covered, and context-independentphones had to be used.In order to overcome these problems, we need a muchlarger database that has a better coverage of the triphones thatare more vocabulary-independent.
To that end, we are cur-rently collecting ageneral English database.
Our first step isto use this database to train triphone and generalized triphonemodels, and then evaluate them on new vocabularies.
As thisdatabase grows, more triphone-based models can be ade-quately trained.
Eventually, we will be able to model otheracoustic-phonetic detail such as stress, syllable position,between-word phenomena, and units larger than triphones.4.
DatabasesTraining : The General English DatabaseIn order to train VI models, we need a very large trainingdatabase that covers all English phonetic variations.
For-tunately, because our focus is speaker-independent r cog-nition, this database can be collected incrementally withoutcreating an unreasonable burden on any speaker.
Initially,this database is a combination of four sub-databases, whichwe will describe below.
Two of the databases were recordedat Texas Instruments in a soundproof booth, and the other twowere collected at Carnegie Mellon in an office environment.The same microphone and processing were used for all foursub-databases.
The ratio of male to female speakers is abouttwo to one in all four sub-databases.The first database is the 991-word resource managementdatabase \[6\], which was designed for inquiry of navalresources.
For this study, we used a total of 4690 sentencesfrom the 80 training and the 40 development test speakers.The TIMIT database \[7\] consists of 630 speakers, eachsaying 10 sentences.
We used a subset of this database,including a total of 420 speakers and 3300 sentences.
Thereare total of 4900 different words.The Harvard database consists of 108 speakers each say-272ing 20 sentences for a total of 2160 sentences.
There are 1900different words.The General English database consists of 250 speakerseach saying 20 sentences for a total 5000 sentences.
It coversabout 10000 different words.Testing: The Voice Calculator DatabaseArt independent task and vocabulary was created to test heVI models.
This task deals with the operation of a calculatorby voice.
There are 122 words, including the alphabet andnumbers, which are highly confusable.
We used 1000 sen-tences from 100 speakers to train vocabulary-dependentmodels and 90 sentences from 10 speakers to test varioussystems under a word-pair grammar with perplexity 53.5.
Experiments and Resu l tsWe used a version of SPHINX for the experiments on our VImodels.
Since SPHINX is described elsewhere in theseproceedings \[8\], we will not be repetitive here.
We note,however, that between-word triphone models \[9\] and correc-tive training \[10\] were not used in this study.
More detaileddescriptions ofSPHINX can be found in \[ 1, 2\].We used 90 sentences from 10 speakers from the voicecalculator task for evaluation.
The following training setswere used:VI-5000 Approximately 5000 sentences from resourcemanagement.
The triphone coverage on the voicecalculator task is 63.7%, and word coverage is44.3%.HARV-TIMITApproximately 5000 sentences from Harvard andTIMIT database.
Triphone coverage is 91.9% andword coverage is53.3%GENENG Approximately 5000 sentences from generalEnglish database.
Triphone coverage is96.9% andword coverage is65.6%VI-10000 Approximately 10,000 sentences from resourcemanagement, TIM1T, and Harvard.
Triphonecoverage is 95.3%, and word coverage is 63.9%.VI-15000 Approximately 15,000 sentences from resourcemanagement, TIM1T, Harvard, and generalEnglish.
Triphone coverage is 99.2%, and wordcoverage is70.5%.VD-1000 Approximately 1000 sentences from voice cal-culator training.
Triphone coverage is 100%, andword coverage is 100%.Our first experiment used 48 phonetic models, trained fromeach of the above four training sets, and tested them on thevoice calculator task.
Table 1 shows the accuracy of phonemodels.
Although phones are well-covered in each of thethree VI databases, the VD results are still much better thanthe VI results.
This is due to the fact that he voice calculatorhas a small vocabulary, and the VD phone models were ableto model the few contexts in this vocabulary well.TrainingSetVI-5000HARV -TIM1TGENENGVI-10000VI-15000VD-1000Table 1: RecognitionRecognitionAccuracy31.1%25.4%22.9%22.8%21.5%16.4%results using phonetic(VI) and models, with vocabulary-independentvocabulary-dependent (VD) training.Next, we trained generalized triphone models on the fourtraining databases.
For each VI training set, we chose anappropriate number of generalized triphones to train from thetraining corpus.
Then, for each phone in the voice calculatortask, if the triphone context was covered, we mapped it to ageneralized triphone.
Otherwise, we used the correspondingcontext-independent phone.
For vocabulary-dependent train-ing, we felt that sufficient raining was available for alltriphones, so no generalization was performed, and we usedVD triphone models.
In all four cases, the trained modelparameters were interpolated with context-independent phonemodels to avoid insufficient training.
The results of thesemodels are shown in Table 2.
Also shown in Table 2 are thetriphone and word coverage statistics using the above fourtraining databases.
Note that as training data is increased,triphone coverage improves more rapidly than word coverage.With 15,000 training sentences, almost all triphones arecovered and the result is close to that from VD training with1000 training sentences.
Moreover, the result of GENENGwhich only conatins 5000 sentences is almost he same as thatof VI-10000 which contains 10000 sentences, because thetriphone coverage of GENENG is better.
Therefore, to coveras many as triphone contexts is crucial for Vocabulary-Independent training.Training Word Trlphone RecognitionSet Coverage Coverage AccuracyVI-5000 44.3% 63.7% 23.9%HARV-TIMIT 53.3% 91.9% 16.3%GENENG 65.6% 96.6% 15.1%VI-10000 63.9% 95.3% 15.2%VI-15000 70.5% 99.2% 13.3%VD-1000 l 100% 100% 11.4%Table2: Recognition results using generalizedtriphones with vocabulary-independent (VI) andvocabulary-dependent (VD) training.The final experiment involves the combination of the VI273and the VD models.
Assuming that we have a set of VImodels trained from a large training database, and avocabulary-dependent training set, we use the following algo-rithm to utilize both training sets:1.
Initialization - Use the VI models to initializeVD training.
As before, for each phone in thevoice calculator task, if the triphone is covered,then it is used to initialize that triphone.
Other-wise, the corresponding context-independentphone is used.2.
Training - Run the forward-backward algo-rithm on the VD sentences to train a set of VDmodels.3.
Interpolation - Use deleted interpolation \[11, 1\]to combine the appropriate task-specific VDmodels with the robust task-independent VImodels.Table 3 shows results using the above interpolation algo-rithm.
We found that the combination of the VI models from15,000 sentences and the VD models from 1000 sentences canreduce the error rate by 18% over VD training alone.
Thisalgorithm can be used to improve any task-dependent recog-nizer given a set of VI models.
Also, these results how thatvocabulary-adaptation is promising.Training Set Recognit ion ErrorAccuracy ReductionVD-1000 11.4% .....VI-5000 & VD-1000 10.3% 9.7%VI-10000 & VD-1000 9.5% 16.7%VI-15000 & VD-1000 9.3% 18.4%Table 3: Recognition results of vocabulary-dependent models interpolated with vocabulary-independent models.We have begun to experiment without grammar; however,at the time of this writing, the results with VI models are notas good relative to the VD models.6.
Conclusion and Future WorkThis paper addressed the issue of vocabulary-independentsubword modeling.
Vocabulary independence is importantbecause the overhead of vocabulary-dependent training isvery high.
Yet, vocabulary-independent subword modelsmust be consistent, rainable, and generalizable.
We believethis requires a large training database and a set of flexiblesubword units.
To this end, we have collected a large multi-speaker database, from which we trained generalized triphonemodels.
We found that with sufficient raining, over 99%triphone coverage of the testing vocabulary can be attained.We report a vocabulary-dependent word accuracy of 88.6%,while the best vocabulary-independent models led to 86.7%.In another experiment, we found that it is possible to reducethe vocabulary-dependent error rate by 18% (to 90.7%) byinterpolating the vocabulary-dependent models with thevocabulary-independent ones.These results are very encouraging.
In the future, we hopeto further enlarge our multi-speaker database.
As thisdatabase grows, we hope to model other acoustic-phoneticdetail such as stress, syllable position, between-wordphenomena, nd units larger than triphones.
To reduce thelarge number of resultant models, we will first use phoneticknowledge to identify the relevant ones, and then apply theclustering technique used in generalized triphones to reducethese detailed phonetic units into a set of generalizedallophones.
We will also experiment with top-down cluster-ing of allophones.
While the top-down approach may lead toless "optimal" clusters, ithas more potential for generalizationin spite of poor coverage.The choice of speaker-independence giv s us the luxury ofplentiful training.
We believe that the combination ofknowledge and subword clustering will lead to subwordmodels that are consistent, rainable, and generalizable.
Wehope that plentiful training, careful selection of contexts, andautomatic clustering can compensate for the lack ofvocabulary-specific training.AcknowledgmentsThe authors wish to thank the members of the CarnegieMellon Speech Group for their contributions.
We would alsolike to acknowledge US West and DARPA for their support.References1.
Lee, K.F., Automatic Speech Recognition: TheDevelopment of the SPHINX System, KluwerAcademic Publishers, Boston, 1989.2.
Lee, K.F., Hon.
H.W., Reddy, R., "An Overview ofthe SPHINX Speech Recognition System", IEEETransactions on Acoustics, Speech, and SignalProcessing, January 1990.3.
Lee.
K.F., Hon, H.W., Hwang, M.Y., Mahajan, S.,Reddy, R., "The SPHINX Speech RecognitionSystem", IEEE International Conference on Acous-tics, Speech, and Signal Processing, April 1989.4.
Lee, K.F., "Context-Dependent Phonetic HiddenMarkov Models for Continuous Speech Recognition",IEEE Transactions on Acoustics, Speech, and SignalProcessing, April 1990.5.
Schwartz, R., Chow, Y., Kimball, O., Roucos, S.,Krasner, M., Makhoul, J., "Context-DependentModeling for Acoustic-Phonetic Recognition of Con-tinuous Speech", IEEE International Conference onAcoustics, Speech, and Signal Processing, April 1985.6.
Price, P.J., Fisher, W., Bernstein, J., Pallett, D., "ADatabase for Continuous Speech Recognition in a1000-Word Domain", IEEE International Conferenceon Acoustics, Speech, and Signal Processing, April1988.2747.8.9.10.11.Fisher, W.M., Zue, V., Bernstein, J., Pallett, D., "AnAcoustic-Phonetic Data Base", 113th Meeting of theAcoustical Society of America, May 1987.Lee, K.F., "Hidden Markov Models : Past, Present,and Future", Proceedings ofEurospeech, September1989.Hwang, M.Y., Hon, H.W., Lee, K.F., "ModelingBetween-Word Coarticulafion in Continuous SpeechRecognition", Proceedings ofEurospeech, September1989.Lee, K.F., Mahajan, S., "Corrective and Reinforce-ment Learning for Speaker-Independent Continuous-Speech Recognition", Proceedings of Eurospeech,September 1989.Jelinek, F., Mercer, R.L., "Interpolated Estimation ofMarkov Source Parameters from Sparse Data", inPattern Recognition i Practice, E.S.
Gelsema ndL.N.
Kanal, ed., North-Holland Publishing Company,Amsterdam, the Netherlands, 1980, pp.
381-397.275
