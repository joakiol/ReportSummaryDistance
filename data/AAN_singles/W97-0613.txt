The "Casual Cashmere Diaper Bag":Constraining Speech Recognition Using ExamplesPau l  Mar t inSun Microsystems LaboratoriesTwo El izabeth DriveChelmsford,  MA 01824-4195 USApaul.
mart in?east ,  sun.
comAbst rac tWe describe a new technology for usingsmall collections of example sentences toautomatically restrict a speech recognitiongrammar to allow only the more plausi-ble subset of the sentences it would other-wise admit.
This technology is unusual be-cause it bridges the gap between hand-builtgrammars (used with no training data) andstatistical approaches (which require signif-icant data).1 Mot ivat ionWhenever the utterances to be recognized o notcorrespond irectly to a large body of text, tradi-tional statistical modeling cannot be used; there isnot enough data to "train" a statistical model.This new technique is relevant when the speechto be recognized contains patterns that can bestraightforwardly abstracted from the semantics ofthe words appearing in them, but where writing sim-ple rules to do so is not practical due to the size ofthe sentence set to be recognized.
Using this technol-ogy, a small representative s t of utterances (on theorder of hundreds of sentences or phrases) is com-bined with an overly-permissive general grammar toautomatically create a much tighter grammar that isspecific to the particular domain.
The grammar pro-duced is a context free grammar in whatever BNFthe speech recognizer of choice requires.The technique works by using a grammar compilerthat accepts grammars composed of rules written us-ing both patterns and restrictions (which can be syn-tactic or semantic); the Unified Grammar compiler(Martin and Kehler, 1994) provides this foundation.Our approach requires the developer to write gram-mars with rules which enforce semantic restrictionsthat test whether the class markings in the lexiconon a phrase head correspond to similar markingson the lexical entries for the possible modifiers ofthat head.
If the grammar writer took the troubleto mark the lexicon appropriately, then these tests,processed by the grammar compiler, would sufficeto build the restricted grammar.
Instead, with thenew technique, only the semantic markings neces-sary to understand the "meaning" of the resultantutterance need be marked by the grammar devel-oper; processing of a set of example sentences servesto automatically provide the additional lexical mark-ings needed by the modifier words, and a subsequentgrammar compilation reflects these restrictions inthe pure BNF grammar used by the speech recog-nizer.2 BackgroundCurrently, the best speaker-independent continu-ous speech recognition (SR) is orders of magnitudeweaker than a human native speaker in recogniz-ing arbitrary sequences of words.
That is, humansdo pretty well on clearly spoken sequences of wordschosen randomly from a pool of tens of thousandsof words, while unconstrained SR systems only doas well when the vocabulary is much smaller, in therange of hundreds of words.
When the recognition isto be done over the telephone, the reduced signal-to-noise ratio of the speech data makes this weaknesseven more dramatic.2.1 Language ModelsIn order to achieve useful recognition rates, currentSR systems impose constraints beyond just a lim-ited vocabulary, either by specifying an exact gram-mar of the sequences which are allowed or by pro-viding statistical likelihoods for word sequences (n-gram statistics).
The grammars are built by hand ascontext-free formalisms determining allowable wordsequences.
The statistical models use tables of the"raw" probabilities of each word (unigram) usuallyaugmented with additional tables of the likelihoodof each word given each possible preceding word (bi-gram) or each possible two preceding words (tri-gram).
These statistical systems have been exper-imentally extended to include n-grams where n isexceeds three, but even for higher n they generallyexpress only the probability of a word based on theadjacent preceding words.612.1.1 Sentence  GrammarsHand-built grammars can provide exquisitely finecontrol over the word sequences recognized, but theirconstruction is difficult and painstaking, even forthose who are practiced in the art.2.1.2 Stat i s t i ca l  Mode lConversely, statistical "grammars" can be builtautomatically by running an analysis program overan appropriate collection of the kinds of sentencesthat one wishes to recognize.
A prime example ofthis technology is the ARPA-initiated Wall StreetJournal (WSJ) dictation project, where recogniz-ers trained on the text of previously-printed articlesfrom the WSJ are tested by having them recognizetext read from a later edition of the WSJ.
Unfortu-nately, the database of WSJ text used in these exper-iments contained approximately 40 million words,and researchers using this database have indicatedthat their speech systems work better when theywere able to double the size of their training set(Schwartz et al, 1994).While the recognition achieved on the WSJ withthis technique is impressive, the information embod-ied in the statistical model is so specific there is notmuch "transfer" to recognizing text that varies instyle, even when content and vocabulary are shared.\[cite example of NYTimes financial stories and theads in WSJ not working well\]2.2 Command and Cont ro lIn the domain of command and control of computerprograms, the utterances to be recognized o notcorrespond irectly to any existing body of text thatcould be used analogously to the WSJ text's role intraining the dictation recognizers.
Traditional sta-tistical modeling requires a relatively huge databaseof example utterances and the models do not in-clude any abstraction of the words, so the actual co-occurence of words is necessary to count the relativefrequency of each.
For many applications of speechrecognition there simply is not enough training datato support using statistical models.2.2.1 Automat ing  the  Lands '  End  cata logWe discovered the need for some new method torestrict a speech recognizer when we attempted toimplement an automated customer service agent tointeract with users wanting to browse and orderitems from an online catalog.
Lands' End DirectMerchants provided a collection of "video assets"from one of their catalogs for this experiment.
Atypical "page" illustrated and described an item ora collection of related items, and might have as-sociated with it additional information such as avideo clip, color and size pages, and indications ofthe pages that are specializations of this page.
Weprototyped a speech-controlled application which al-lows a user to interact with the automated agent us-ing speech through the telephone while viewing thevideo on a televison 1.
Allowing a free conversationaldialogue and supporting a large subset of the myriadways an untrained caller might describe the catalogitems overwhelmed our speech recognizer.3 How Rest r i c t ive  is a grammar?Writing a grammar to allow a user to make queriesabout the contents of this computerized catalog wasthe concrete xample that drove our new approach.3.1 What  do users  say?We collected examples of what users said to an ex-pert human service representative in a "Wizard ofOz" experiment (Yankelovich, forthcoming).
Be-sides the action words and phrases ("can you showme <itemPhrase>?"
or "what <itemPhrase> doyou carry?")
in a shopping query, the user com-monly supplied a phrase that names or describes theitem of interest.D: I'd like a soft-sided attache.<displays luggage page>D: The canvas line.C: How about kids?B: Can I see the squall jacket?C: Could I see the men's clothes?<displays menswear page>C: Dress shirts.S: Could we switch to children's clothing.L: Let's look at some casual dresses.M: I'd like to see the sweaters please.S: I 'm looking for things from bed and bath.B: Let's go back to sweaters.B: Can I go back to the main screen.L: I'll go back to the womens.A: I 'm looking for a blazer and slacks and skirts togo with it.C: I need a flat sheet and a fitted sheet in queen.Example queries users said to "Wizard" system.3.1.1 A grammar  to col lect  semant icsWe implemented the prototype Lands' End sys-tem using our SpeechActs (Martin et al, 1996) sys-tem, collecting the relevant semantics from utter-ances with a simple grammar specifying the allow-able phrases.One over-simplified grammar of such "item speci-fication" phrases would allow any basic item (such astin a real installation, the televison would be con-nected to a pay-per-view channel or a cable system suchas in a hotel62"pants") to be modified by any combination ofmeta-style, pattern style, color, size, gender, wearer's age,fabric type, fabric style, and maker's name.
A par-ticular sweater could be referred to as "the petitewomen's medium dusty sage jewel-neck cashmerefine-knit 'drifter' sweater".
While no one would everspontaneously utter this monster, we cannot predictwhich portion of these options will be used in anygiven utterance.
Such an accepting rammar worksjust fine for extracting the meaning from a writtenform of the item description, and in fact, is usedin the Lands' End system to identify what items aredisplayed on each "page" of the video-accessible cat-alog.
( leNounPhrase/nosize : = \[determiners\]\[style/styl\] \[preModifiers\] \[style/sty2\]\[sem=style-name\] \[sem=fabric-style\]\[sem=material\] \[style/sty3\]leNoun \[postModPhrase\] ;head leNoun;fabric := material.root;fabric "= postModPhrase.material;index := postModPhrase.index;fabric-style := fabric-style.root ;fabric-style "= postModPhrase.fabric-style;genderCat  : = preModifiers, genderCat ;genderCat  "= postModPhrase.genderCat  ;....?~Example UG rule allowing many possible modifers3.2 Semant ic  g rammar  is too  loose for  SRUnfortunately, the perplexity of the grammar pro-duced by the cross product of all these choices is solarge that the word accuracy of the speech recog-nition becomes uselessly low.
Phrases that no userwould ever utter are "heard" by the Sl:t engine; the"casual cashmere diaper bag" mentioned in the titleof this paper refers to one of the more outrageouscombinations that pass the muster of this weakly-constraining rammar.3.2.1 Mark ing  semant ic  "agreement"If the lexical entry for every modifier were markedwith a feature containing the set of things it couldrealistically modify (or, better yet, the set of classesof things), then the grammar could be written to al-low only the "reasonable" combinations and to ruleout the ridiculous ones that should be omitted toreduce the perplexity.
With a grammar compilerthat accepts such restrictions based on features inthe lexicon, such a markup appears to be a possi-ble solution.
The grammar writer could create andrecord classes of basic items, noting that "chinos"and "jeans" were "tough clothing" and then only al-lowing them to be associated with fabrics appropri-ate for "tough" clothes.
This strategy would blockcombinations such as "lace chinos" but allow "silkblouses" and "denim jeans.
"The biggest disadvantage of requiring a grammarwriter to figure out and record the features that de-termine allowable modifiers is the large amount ofdetailed work required to make such annotations.
Ifthese markings could be derived automatically fromsome pre-existing or easily-created data, then thetask would be much reduced, and the cost of addingnew items to the catalog would be much smaller.
(Inthe particular ease of modeling a catalog, the effortrequired to accomodate each subsequent revision ofthe items carried is a primary concern 2)genderCat = ' womensfabric = 'cottonmeta-sty le = 'casualcatalogtype = 'pantsstyle = 'chinoExample indexing of an item page describedas "women's chino slacks" and as "casual cottonpants.
"3.2.2 Us ing example  sentencesIn the Lands' End example, we already have itemdescriptions which are part of their standard cata-log database.
We use these descriptive phrases bothto navigate to the item or item collection (such as"men's jackets") the user has requested and to verifythat the semantic grammar and lexicon will acceptthe phrases used by the catalog designers.
Any newversion of the catalog will necessarily already havethese phrases created for it; using them addition-ally for grammar estriction almost automates theupdate chore for new editions of the catalog.If the grammar were written incorporating teststo require the lexical markings indicating allow-able modifiers, then it would reject any phrase thatlacked the needed marks.
If such a grammar wereused with a "bare" lexicon (one lacking these modi-fier markings), it would not support parsing the pagedescriptors, and would compile into a speech gram-mar allowing only bare item names, devoid of anymodifiers.
We addressed this problem by adding theability to switch the restrictions on or off, and thenturning them off when parsing the (written) pagedescriptors.
(See the example of switched tests in agrammar ule.
)3.2.3 Automat ing  the  markupIndexing and then processing the results of allthe page descriptor parses provides the informationcontent needed to automatically mark up the lexi-con with the compatibility results derived from thepage descriptors.
Once the lexicon has been en-hanced with this information, the restrictions canbe turned on while the unified grammar is used bythe speech recognizer.
In our system, we compilethe unified grammar to produce BNF reflecting the2We don't mind working hard once in a while, but wedo not want a new career updating this catalog.63restrictions, but logically these restrictions could beapplied "on the fly" by a speech recognizer or usedin post-processing to choose among the n-best alter-natives from a less restricted SR.
Regardless of howit is implemented, the resultant grammar will not al-low "lace jeans" simply because no page descriptionphrase mentions any such thing.~lel|ounPhrase/nosize := \[determiners\]\[style/styli \[preModifiers\] \[style/sty2\]\[sem=style-name\] \[sem=fabric-style\]\[sem=mat erial\] \[style/sty3\]leIIoun \[postModPhrase\] ;head leNoun;le~oun, cat-type *= material.cat-type-set;fabric := material.root;fabric "= postModPhrase.material;index := postModPhrase.index;fabric-style := fabric-style.root;fabric-style "= postModPhrase.fabric-style;genderCat := preModifiers.genderCat;genderCat "= postModPhrase.genderCat;The *= operator is the switched test operator in thisexample grammar ule.4 Log ic  versus  rea l i ty4.1 A myster ious  deafnessOne final problem must be addressed to make thisscheme actually useful; there are sure to be some"reasonable" combinations of modifiers and basicitems that the catalog makers just do not includein their catalog.
If there were "canvas jackets" and"denim jeans" in the catalog but no "denim jack-ets," then unless jeans and jackets shared a com-mon "kind of thing" property on which to base thegrammar estrictions, the restricted grammar couldnot hear the phrase "denim jacket".
Presented withthose sounds, it would probably produce somethinglike the "d'women jacket" pronunciation of "thewomen\['s\] jacket", but it could not "hear" what theuser actually said.
This would be baffling to a naiveuser of the system, especially since rephrasing hisrequest o include % jacket made of denim" wouldalso fail.4.2 F i l l ing i n  the  gapsTo fix this shortcoming, the examples that generatethe automatic marking of the lexicon must be aug-mented to include the logical extensions of the ac-tual database of "real" items.
When proposing thisapproach, Nicole Yankelovich loosely described it as"listing all the things that aren't in the catalog".
Ofcourse, taking this literally would be an unboundedtask and would defeat the whole goal of restrictingthe grammar; such a list would include the infamouscashmere diaper bag!
What we really needed was alisting of the things that one might logically expectto find but which do not exist in this particular cat-alog.
In our Lands' End example, we created pagesof "missing" items and associated these explicitlymissing pages as phantom pages under their logicalparent pages in the catalog.
These phantom pagesserve to attach the information we give the customerwhen we report the omission.
With this addition tothe scheme, the user can be "heard" asking for adenim jacket, and will be told something helpful in3 response5 System Deta i l sThe restrictions computed by this scheme must beapplied to the speech recognizer if any reduction inperplexity is to be achieved.
Testing restrictions dur-ing SR or selecting "semantically" among the n-bestare both possible implementations.
Neither workswith currently available SRs; these SRs use BNFgarmmars and do not deliver semantically distinctalternatives for n-best (Hemphill, 1993; Smith andBates, 1993).5.1 Compi l ing  res t r i c t ions  in the  SRgrammarThe tool we use to impose these restrictions i a com-piler capable of converting a grammar composed ofpatterns and calculated "semantic" restrictions intotwo compiled grammars: one for use in a speechrecognizer and one to parse the recognized wordsand produce a structure representing the relevantsemantics of the sentence.
The Unified Grammarand its associated tools fill this requirement, pro-viding a generally adequate approximation to thisideal compiler.
The ideal compiler would turn thepatterns and restrictions into just patterns, and doso without expanding the compact notation of theoriginal grammar into some "rolled-out" form thatis too large for the SR to use; this compactness re-quirement rules out any approach which ennumer-ates the acceptable sentences of the grammar.
TheUnified Grammar compiler produces a patterns-onlygrammar that also reflects the restrictions by pre-computing these tests, when possible, to create morespecific patterns reflecting the constraints.
It omitsrestrictions that are too complex for it to effect, thusallowing all the good utterances and possibly somebad ones as well.5.2 P roees ing  stepsTo implement he example-based restrictions, theUnified Grammar language was extended to includeaWe attach explicit helpful messages to some phan-tom pages ("Sorry, but the jackets do not come in denim,only Polartec, Thinsulate, and wool") and otherwise gen-erate a message indicating the query was heard, but nosuch item is in this catalog.64tests that could be disabled or enabled with a globalswitch, and then the following processing was used:1.
Disable the feature restrictions and compile theUnified Grammar to produce a semantic gram-mar.2.
Parse the (written) page descriptors with the"relaxed" semantic parser, building an index ofall the parses which can be used later to locatethe related pages of the catalog.3.
Reprocess this index to extract he informationabout existing modifier types and use this in-formation to add the implied markings to thelexicon.4.
Turn the global switch to enable the lexicalrestrictions and compile the Unified Grammaragain to produce the speech recognition gram-mar.
The compiler will use the enhanced lexi-con while applying the restrictions now enabled,and this will produce a "tight" speech recogni-tion grammar.5.
Use the restricted grammars for both speechrecognition and semantics extraction when run-ning the catalog with users, so that the systemcan "hear" and process "canvas diaper bag" butnot "cashmere diaper bag".6 App l i cab i l i ty  and  app l i ca t ion  o fth i s  approach6.1 Where  does this approach work?It is important to note that this approach dependson there being some simple way to indicate in agrammar what sort of "agreement" is required be-tween the parts of a phrase, and that a relativelyrich example set illustrating the "good" agreementsalso must be available.
General language procesinglacks one or both of these requirements, so this ap-proach must be understood as having relevance onlywhere the ratio of example data is high relative tothe variability that must be supported in the spokenlanguage being processed.Our example case used a "binary" decisionparadigm, completely ruling out combinations whichdid not match up with criteria from the example set;by using likelihood weighting instead of rigid exclu-sion, a more flexible system could be built.6.2 Semant ic  leverageClearly some categorization f the items used in thistechnique improves both the simplicity and the gen-erality of the restrictions that can be generated.
I.e.,if we use the know!edge that "diaper bag" and "bookbag" are types of luggage, we can write the restric-tion rules to record and test the markings on their"type" rather than their "species", and thus get in-formation about the appropriate modifiers for "duf-65fel bag" without having ever "seen" sentences aboutduffel bags.7 Conc lus ionsWe have presented an implemented scheme whichsignificantly reduces the perplexity of the speechrecognition task in cases where the perplexity arisesfrom allowing semantically irrelevant grammaticalconstructions.
This method is applicable wherethere is a modest collection of relevant sample sen-tences to support building the restrictions by ex-ample.
This method is applicable only in certainclasses of speech, but in those cases it can automatethe otherwise quite tedious task of manually mark-ing semantic restrictions for a grammar.AcknowledgmentsThis work is part of a larger effort within Sun Mi-crosystems Labs prototyping tools to make the useof computer speech more practical.
Thanks to myco-Principal Investigator Nicole Yankelovich and toStuart Adams, Eric Baatz, and Andrew Kehler fortheir contributions.Re ferencesCharles Hemphill.
1993.
Dagger: Directed acyclicgraphs of grammars for enhanced recognition,user's guide and reference manual.
Technical re-port, Texas Instruments, May.Paul Martin and Andrew Kehler.
1994.
Speechacts:A testbed for continuous speech applications.
InProceedings of the AAAI Workshop on Integrationof Natural Language and Speech Processing, pages65-71, Cambridge,MA, August.
MIT Press.Paul Martin, Fredrick Crabbe, Stuart Adams, ErieBaatz, and Nieole Yankelovich.
1996.
Speechacts:A spoken language framework.
IEEB Computer,29(7):33-40.R.
Schwartz, L. Nguyen, F. Kubala, G. Chou,G.
Zavaliagkos, and J. Makhoul.
1994.
On us-ing written language training data for spoken lan-guage modeling.
In Proceedings of the Human lan.guage Technology Workshop, pages 94-98, Plains-boro,NJ, March.
Morgan Kaufmann.Graeme W. Smith and Madeleine Bates.
1993.
Voiceactivated automated telephone call routing.
InProceedin#s ofthe Ninth IEEE Conference on Ar-tificial Intelligence for Applications, Orlando,FL,March.Nicole Yankelovich.
forthcoming.
Using natural di-alogs as the basis for speech interface dsign.
InSusann Luperfoy, editor, Automated Spoken Dia-logue Systems.
MIT Press.
