BioNLP 2007: Biological, translational, and clinical language processing, pages 145?152,Prague, June 2007. c?2007 Association for Computational LinguisticsThe Extraction of Enriched Protein-Protein Interactions from BiomedicalTextBarry Haddow and Michael MatthewsSchool of InformaticsUniversity of Edinburgh2 Buccleuch PlaceEdinburgh, Scotland, EH8 9LW{bhaddow,mmatsews}@inf.ed.ac.ukAbstractThere has been much recent interest in theextraction of PPIs (protein-protein interac-tions) from biomedical texts, but in orderto assist with curation efforts, the PPIs mustbe enriched with further information of bi-ological interest.
This paper describes theimplementation of a system to extract andenrich PPIs, developed and tested using anannotated corpus of biomedical texts, andemploying both machine-learning and rule-based techniques.1 IntroductionThe huge volume of literature generated in thebiomedical field is such that researchers are unableto read all the papers that interest them.
Instead theymust rely on curated databases, containing informa-tion extracted from the literature about, for example,which proteins interact.These curated databases are expensive to produceas they rely on qualified biologists to select the pa-pers, read them to extract the relevant information,enter this information into the database, and cross-check the information for quality control, a proce-dure which can be very time-consuming.
If NLPtechniques could be used to aid curators in their taskthen the costs of producing curated databases couldbe substantially reduced.In the context of biomedical information extrac-tion, there has been much recent interest in theautomated extraction of PPIs (protein-protein in-teractions) from biomedical literature.
The recentBioCreAtIvE Challenge highlights the desire to uti-lize these extraction techniques to automatically orsemi-automatically populate curated PPI databases.However, just identifying the interactions is not nec-essarily sufficient, as curators typically require ad-ditional information about the interactions, such asthe experimental method used to detect the interac-tion, and the names of any drugs used to influencethe behaviour of the proteins.
Furthermore, curatorsmay only be interested in interactions which are ex-perimentally proven within the paper, or where theproteins physically touch during the interaction.This paper describes the implementation of asystem designed to extract mentions of PPIs frombiomedical text, and to enrich those PPIs with ad-ditional information of biological interest.
The en-riched information consists of properties (name-value pairs associated with a PPI, for example a di-rectness property could indicate whether the inter-action is direct or not direct) and attributes (rela-tions between the PPI relation or its participatingentities and other entities, such as the experimentalmethod used to detect the PPI).
This system for ex-tracting and enriching PPIs was developed as part ofthe TXM programme, which aims to develop tools tohelp with the curation of biomedical papers.After reviewing related work in the following sec-tion, a detailed description of how the annotated cor-pus was created and its descriptive statistics is pro-vided in section 3.
The methods used to extract theproperties and attributes are explained in section 4,and then evaluated and discussed in section 5.
Someconclusions and suggestions for further work are of-fered in section 6.1452 Related WorkThere has been much recent interest in extractingPPIs from abstracts and full text papers (Bunescuand Mooney, 2006; Giuliano et al, 2006; Plake etal., 2005; Blaschke and Valencia, 2002; Donaldsonet al, 2003).
In these systems however, the focus hasbeen on extracting just the PPIs without attempts toenrich the PPIs with further information.
EnrichedPPIs can be seen as a type of biological event ex-traction (Alphonse et al, 2004; Wattarujeekrit et al,2004), a technique for mapping entities found in textto roles in predefined templates which was madepopular in the MUC tasks (Marsh and Perzanowski,1998).
There has also been work to enrich sentenceswith semantic categories (Shah and Bork, 2006) andqualitative dimensions such as polarity (Wilbur etal., 2006).Using NLP to aid in curation was addressed inthe KDD 2002 Cup (Yeh et al, 2002), where par-ticipants attempted to extract records curatable withrespect to the FlyBase database, and has been furtherstudied by many groups (Xu et al, 2006; Karamaniset al, 2007; Ursing et al, 2001).The Protein-Protein Interaction task of the recentBioCreAtIvE challenge (Krallinger et al, 2007) wasconcerned with selecting papers and extracting in-formation suitable for curation.
The PPI detectionsubtask (IPS) required participants not simply to de-tect PPI mentions, but to detect curatable PPI men-tions, in other words to enrich the PPI mentions withextra information.
Furthermore, another of the sub-tasks (IMS) required participants to add informationabout experimental methods to the curatable PPIs.3 Data Collection and Corpus3.1 Annotation of the CorpusA total of 217 papers were selected for annotationfrom PubMed and PubMedCentral as having exper-imentally proven protein-protein interactions (PPIs).The papers were annotated by a team of nine anno-tators, all qualified in biology to at least PhD level,over a period of approximately five months.The XML versions of the papers were used wher-ever possible, otherwise the HTML versions wereused and converted to XML using an in-house tool.The full-text of each paper, including figure cap-tions, was annotated, although the materials andmethods sections were not included in the annota-tion.From the 217 annotated papers, a total of 65were selected randomly for double annotation and27 for triple annotation.
These multiply-annotatedpapers were used to measure inter-annotator agree-ment (IAA), by taking each pair of annotations onthe same paper, and scoring one annotation againstthe other using the same algorithm as for scoring thesystem against the annotated data (see Section 5).Each doubly annotated paper contributed one pair ofannotations, whilst the triply annotated papers con-tributed three pairs of annotations.
The overall IAAscore is the micro-average of the F1 scores on eachpair of corresponding annotations, where it shouldbe emphasised that the F1 does not depend on theorder in which the annotated papers were combined.The multiply annotated papers were not reconciledto produce a single gold version, rather the multipleversions were left in the corpus.The papers were annotated for entities and rela-tions, and the relations were enriched with proper-ties and attributes.
The entities chosen for anno-tation were those involved in PPIs (Protein, Com-plex, Fusion, Mutant and Fragment) and thosewhich could be attributes of PPIs (CellLine, Drug-Compound, ExperimentalMethod and Modification-Type).
A description of the properties and attributes,as well as counts and IAA scores are shown in Ta-bles 1 and 2.Once annotated, the corpus was split randomlyinto three sections, TRAIN (66%), DEVTEST (17%)and TEST (17%).
TRAIN and DEVTEST were to beused during the development of the system, for fea-ture exploration, parameter tuning etc., whilst TESTwas reserved for scoring the final system.
The splitswere organised so that multiply annotated versionsof the same paper were placed into the same section.3.2 Descriptive Statistics of CorpusThe total number of distinct PPIs annotated in the336 papers was 11523, and the PPI IAA, measuredusing F1, was 64.77.
The following are examples ofenriched PPIs, with the entities in bold face:(1) Tat may also increase initiation of HIV-1 transcription by enhancing phosphoryla-tion of SP1, a transcription factor involvedin the basal HIV-1 transcription [14].146Name Explanation Values Counts Pct IAAIsPositive The polarity of the statement about the PPI.
Positive 10718 93.01 99.57Negative 836 7.26 90.12IsDirect Whether the PPI is direct or not.
Direct 7599 65.95 86.59NotDirect 3977 34.51 61.38IsProven Whether the PPI is proven in the paper or not.Proven 7562 65.63 87.75Referenced 2894 25.11 88.61Unspecified 1096 9.51 34.38Table 1: The properties that were attached to PPIs, their possible values, counts and IAAName Entity type Explanation Count IAAInteractionDetectionMethod ExperimentalMethod Method used to detect thePPI.2085 59.96ParticipantIdentificationMethod ExperimentalMethod Method used to detect theparticipant.1250 36.83ModificationBefore Modification Modification of partici-pant before interaction.240 68.13ModificationAfter Modification Modification of partici-pant after interaction.1198 86.47DrugTreatment DrugCompound Treatment applied to par-ticipant.844 49.00CellLine CellLine Cell-line from which par-ticipant was drawn.2000 64.38Table 2: The attributes that could be attached to the PPIs, with their entity type, counts and IAA(2) To confirm that LIS1 and Tat interact invivo, we used yeast two-hybrid system, inwhich Tat was expressed as a bait and LIS1as a prey.
Again, we found that LIS1 andTat interacted in this system.In Example 1, the properties attached to the PPI be-tween ?Tat?
and ?SP1?
are Referenced, Direct andPositive, and ?phosphorylated?
is attached as a Mod-ificationAfter attribute.
Example 2 shows a PPI be-tween ?Tat?
and ?LIS1?
(in the second sentence)which is given the properties Proven, Direct andPositive, and has the InteractionDetectionMethod at-tribute ?yeast two-hybrid system?.
This second ex-ample indicates that attributes do not have to occurin the same sentence.Statistics on the occurrence of properties areshown in Table 1.
For most of the property val-ues, there are significant numbers of PPIs, exceptfor Unspecified and Negative, which are used in lessthan 10% of cases.
Note that annotators were per-mitted to mark more than one PPI between a givenpair of entities if, for example, they wished to markboth Positive and Negative PPIs because the authoris making a statement that proteins interact underone condition and not under another condition.
Forthe purposes of data analysis and to make modellingeasier, such PPIs have been collapsed to give a singlePPI which may have multiple values for each prop-erty and attribute.Table 2 shows occurrence statistics for attributes,where, as for properties, there can be multiple val-ues for the same attribute.
A notable feature of theattribute attachment counts is that certain attributes(ModificationBefore and DrugTreatment especially)are quite rarely attached, making it difficult to usestatistical techniques.Also shown in Tables 1 and 2 are the IAA figuresfor all properties and attributes.
The IAA for proper-ties is generally high, excepted for the Unspecifiedvalue of the IsProven property.
This being some-thing of a ?none of the above?
category means thatthe annotators probably have different standards re-147garding the uncertainty required before the PPI isplaced in this class.
The IAA for attributes is, onthe whole, lower, with some attributes showing par-ticularly low IAA (ParticipantIdentificationMethod).A closer investigation shows that the bulk of the dis-agreement is about when to attach, in other words ifboth annotators decide to attach an attribute to a par-ticular PPI, they generally agree about which one,scoring a micro-averaged overall F1 of 95.10 in thiscase.4 Methods4.1 Pipeline ProcessingThe property and attribute assignment modules wereimplemented as part of an NLP pipeline based onthe LT-XML2 architecture1 .
The pipeline consists oftokenisation, lemmatisation, part-of-speech tagging,species word identification, abbreviation detectionand chunking, named entiry recognition (NER) andrelation extraction.
The part-of-speech tagging usesthe Curran and Clark POS tagger (Curran and Clark,2003) trained on MedPost data (Smith et al, 2004),whilst the other preprocessing stages are all rulebased.
Tokenisation, species word identification andchunking were implemented in-house using the LT-XML2 tools (Grover and Tobin, 2006), whilst ab-breviation extraction used the Schwartz and Hearstabbreviation extractor (Schwartz and Hearst, 2003)and lemmatisation used morpha (Minnen et al,2000).The NER module uses the Curran and Clark NERtagger (Curran and Clark, 2003), augmented withextra features tailored to the biomedical domain.
Fi-nally, a relation extractor based on a maximum en-tropy model and a set of shallow linguistic featuresis employed, as described in (Nielsen, 2006).4.2 PropertiesTo assign properties to each PPI extracted by therelation extraction component, a machine learningbased property tagger was trained on a set of featuresextracted from the context of the PPI.
The propertytagger used a separate classifier for each property,but with the same feature set, and both MaximumEntropy (implemented using Zhang Le?s maxent2)and Support Vector Machines (implemented using1http://www.ltg.ed.ac.uk/software/xml/2http://homepages.inf.ed.ac.uk/s0450736/maxent_toolkit.htmlsvmlight3) were tested.
To choose an optimal fea-ture set, an iterative greedy optimisation procedurewas employed.
A set of potential features were im-plemented, with options to turn parts of the featureset on or off.
The full feature set was then tested onthe DEVTEST data with each of the feature optionsknocked out in turn.
After examining the scores onall possible feature knockouts, the one which offeredthe largest gain in performance was selected and re-moved permanently.
The whole procedure was thenrepeated until knockouts produced no further gainsin performance.
The resulting optimised feature setcontains the following features:ngram Both unigrams and bigrams were imple-mented, although, after optimisation, unigramswere switched off.
The ngram feature uses vlwbackoff, which means that words are replacedby their verb stems, backed off to lemmas andthen to the word itself if not available.
Further-more, all digits in the words are replaced with?0?.
Ngrams are extracted from the sentencescontaining the participants in the PPI, and allsentences in between.
Ngrams occurring be-fore, between and after the participants of thePPI are treated as separate features.entity The entity feature includes the text and typeof the entities in the PPI.headword This feature is essentially constructed inthe same way as the ngram feature, except thatonly head verbs of chunks in the context areincluded, and the vlw backoff is not used.entity-context In the entity context feature, the vlwbackoffs of the two words on either side of eachof the entities in the PPI are included, with theirpositions marked.4.3 AttributesFor attribute assignment, experiments were per-formed with both rule-based and machine-learningapproaches.
The following sections summarise themethods used for each approach.4.3.1 Rule-basedIn the rule-based approach, hand-written ruleswere written for each attribute, using part-of-speechtags, lemmas, chunk tags, head words and the NERtags.
In all, 20 rules were written.
Each rule is3http://svmlight.joachims.org/148Rule Protein Prec CountP1 ATT P2 P2 100 13P1 is ATT by P2 P1 100 1ATT of P2 P2 86.1 112ATT of P1 P1 74.5 80P1 * ATT site P1 72.2 13P1 * ATT by * P2 P2 70.0 100P1 * (ATT pass) * P1 P2 64.0 16P1 * ATT * P2 P2 67.5 187P2 ATT P2 75.0 100P2 - any-word ATT P1 73.7 14Table 3: The rules used to assign ModificationAfterattributes.
The protein column indicates whether theattribute attaches to the 1st or 2nd protein, the precfield indicates the precision of the rule on the train-ing set and the count indicates the number of timesthe rule applied correctly in training.
In the rules,P1 refers to the first protein, P2 refers to the sec-ond protein, ATT refers to the attribute, * refers toany number of words, any-word refers to any singleword, and pass refers to the passive voice.
For exam-ple, the rule ?P2 - any-word ATT?
applied to the sen-tence ?protein 1 is regulated by protein 2-dependentphosphorylation?
would result in the attribute phos-phorylation being assigned as the ModificationAfterattribute to protein 1.ranked according to its precision as determined onthe TRAIN set, and the rules are applied in orderof their precision.
This is particularly importantwith modification attributes which are constrainedso that a given modification entity can only be at-tached once per interaction.
Table 3 lists the rulesused to assign the ModificationAfter attribute.4.3.2 Machine LearningFor this approach, attributes are modelled as rela-tions between PPIs and other entities.
For each PPIin a document, a set of candidate relations is cre-ated between each of the entities in the PPI and eachof the attribute entities contained in the same sen-tence(s) as the PPI4.
If there are no entities of theappropriate type for a given attribute in the samesentence as the PPI, the sentences before and af-ter the PPI are also scanned for candidate entities.Each of the candidate relations that correspond to4PPIs spanning more than 2 sentences were ignoredattributes annotated in the gold standard are consid-ered positive examples, whilst those that were notannotated are considered negative examples.
For ex-ample, given the following sentence:Protein A phosphorylates protein B[Protein] [Modification] [Protein]If the gold standard indicates a PPI between Pro-tein A and Protein B with phosphorylates assignedas a ModificationAfter attribute to Protein B, fourcandidate relations will be created as shown in Ta-ble 4Type Entity 1 Entity 2 LabelMod Before Prot A phosphorylates negMod Before Prot B phosphorylates negMod After Prot A phosphorylates negMod After Prot B phosphorylates posTable 4: Candidate Attribute Relations for Protein Aphosphorylates Protein BA set of features is extracted for each of the exam-ples and a maximum entropy (ME) model is trainedusing Zhang Le?s maxent toolkit.
The features usedare listed below:entity The text and part-of-speech of the attribute,as used for properties.entity-context The entity context feature used forproperties, except that the context size was in-creased to 4, and parts-of-speech of the contextwords were also included.ngram This is the same as the ngram featureused for properties, except that unigrams wereswitched on.entities-between The entities that appear betweenthe two entities involved in the candidate rela-tion.parent-relation-feature Indicates the position ofthe attribute entity with respect to parent PPI(i.e.
before, after, or in between).
For attributesthat are in between the two entities involved inthe PPI, also indicates if the sentence is activeor passive.5 Evaluation5.1 PropertiesTo score the property tagger, precision, recall andF1 are calculated for each of the seven possible149Name Value Baseline Maximum Entropy SVMGold Predicted Gold Predicted Gold PredictedIsPositive Positive 96.87 97.33 97.10 98.22 97.08 98.27Negative 0.00 0.00 38.46 48.39 45.45 57.53IsDirect Direct 78.66 81.90 82.05 85.54 81.94 86.87NotDirect 0.00 0.00 58.92 54.33 60.80 63.44IsProven Proven 78.21 78.85 87.86 82.73 88.08 88.51Referenced 0.00 0.00 81.46 69.65 82.83 81.97Unspecified 0.00 0.00 25.74 29.41 22.77 28.00Overall 74.20 76.24 83.87 83.33 84.09 86.79Table 5: The performance of the property tagger, measured by training on TRAIN and DEVTEST combined,then testing on TEST.
The two scores given for each system are for testing on gold PPIs, and testing onpredicted PPIs.
An F1 score is shown for each property value, as well as a microaveraged overall score.property values and then the F1 scores are micro-averaged to give an overall score.
As mentioned inSection 3.1, all versions of the annotation for eachmultiply-annotated document were included in thetraining and test sets, taking care that all versions ofthe same document were included in the same set.This has the disadvantage that the system can neverachieve 100% in cases where the annotators differ,but the advantage of giving partial credit where thereis genuine ambiguity and the system agrees with oneof the options chosen by the annotators.The scores for all property values, tested on TEST,are shown in Table 5, both using the model (withMaximum Entropy and SVM) and using a base-line where the most popular value is assigned.
Twoscores are shown, the performance as measuredwhen the test set has the gold PPIs, and the per-formance when the test set has the predicted PPIs,scored only on those PPIs where both system andgold agree.
The relation extractor used to predictthe PPIs is trained on the same documents as wereused to train the property tagger.To see which features were most effective, aknockout (lesion) test was conducted in which fea-tures were knocked out one by one and performancewas measured on the DEVTEST set.
In each featureknockout, one of the features from the list in Sec-tion 4.2 was removed.
Table 6 shows how the overallperformance is affected by the different knockouts.From the knockout experiment it is clear that thengram (actually bigram) feature is by far the mosteffective, with the other features only contributingmarginally to the results.Feature Knockout score Differencevanilla 86.08 0.00ngram 81.86 -4.22entity 85.30 -0.77headword 84.38 -0.50entity-context 85.54 -0.54Table 6: The effect of knocking out features on theproperty score.
Tests are conducted by training onTRAIN and testing on DEVTEST, on predicted PPIs.?vanilla?
refers to the case where the optimal fea-tures set is employed.5.2 AttributesThe attributes are scored in the same manner as theproperties.
Table 7 summarises the results for boththe rule-based and machine learning attribute sys-tems.
These are compared to a baseline system thatsimply attaches the nearest entity of the appropriatetype for each attribute.5.3 DiscussionThe results for the more common property values aregenerally close to human performance (as measuredby IAA), however performance on both IsNegativeand Unspecified is fairly low.
In the case of Un-specified, the IAA is also low, making it likely thatthe training and test data is inconsistent, compound-ing the problem of the low occurrence rate of thisvalue.
The Negative value also suffers from a lowoccurrence rate, leading to an imbalance betweenNegative and Positive which makes life hard for the150Attribute Baseline Rule-based Machine LearningGold Predicted Gold Predicted Gold PredictedInteractionDetectionMethod 36.02 39.71 39.22 41.38 37.02 46.81ParticipantIdentificationMethod 08.68 09.27 12.32 12.87 03.37 05.97ModificationBefore 13.10 16.00 42.22 43.84 04.88 08.33ModificationAfter 43.37 46.00 64.93 73.04 62.32 69.64DrugTreatment 49.57 51.11 51.29 53.33 13.90 24.52CellLine 50.19 45.90 54.47 50.47 45.13 42.28Overall 29.68 30.32 45.26 48.32 32.08 43.11Table 7: The performance of the attribute tagger, on TEST.
The two scores given for each system are fortesting on gold PPIs, and testing on predicted PPIs.
Performance on each attribute value is measured usingF1, and then microaveraged to give an overall figure.machine learners.
However it is also possible thatthe shallow linguistic features used in these experi-ments are not sufficient to make the sometimes sub-tle distinction between a negative statement aboutan interaction and a positive one, and that modelsbased on a deeper linguistic analysis (e.g.
parse treesas in (Moschitti, 2004)) would be more successful.Note also that the feature set was optimised for max-imum performance across all property values, withall given equal weight, but if some values are moreimportant than others then this could be taken intoaccount in the optimisation, with possibly differentfeature sets used for different property names.The results for the attributes using the rule-basedsystem are approximately 75% of human perfor-mance and are higher than results for the machinelearning system.
However, for the Modification-After, CellLine, and InteractionDetectionMethod at-tributes, which occur more frequently than the otherattributes and have higher IAA, the machine learningsystem is competitive and even slightly outperformsin the case of the InteractionDetectionMethod.
Thescores are directly correlated with the IAA and boththe scores and the IAA are higher for the attributesthat tend to occur in the same sentence as the PPI.
Ona practical level, this suggests that those who hope tocreate similar systems would be advised to start withlocal attributes and pay particular attention to IAA onnon-local attributes.5.4 Further workAs regards properties, good results were obtainedusing shallow linguistic features, but it would beinteresting to learn whether machine learning tech-niques based on a deeper linguistic analysis wouldbe more effective.
Also, properties were treated asadditional information added on to the PPIs after therelation extractor had run, but perhaps it would bemore effective to combine relation extraction andproperty tagging to, for example, consider positiveand negative PPIs as different types of relations.For attributes, it would be interesting to combinethe rule-based and machine learning systems.
Thishas the advantage of having a system that can bothlearn from annotated data when it exists, but canbe potentially improved by rules when necessary orwhen annotated data is not available.
Another issuemay be that some attributes might not be representedexplicitly by a single entity in a document.
For ex-ample, an experimental method may be describedrather than explicitly stated.
Attributes that are notlocal to the PPI caused difficulty for both the anno-tators and the system.
It would be interesting to seeif it is easier to attach attributes to a single PPI thathas been derived from the text, rather than attempt-ing to assign attributes to each specific mention of aPPI within the text.
This could be accomplished byattempting to merge the information gathered fromeach relation along the lines described in (Hobbs,2002)Since the main motivation for developing the sys-tem to extract enriched PPIs was to develop a tool toaid curators, it would be useful to know how effec-tive the system is in this task.
Aside from (Karama-nis et al, 2007), there has been little work publishedto date on the effect that NLP could have on the cu-ration process.
In the most recent BioCreAtIvE eval-uation, the PPI subtasks were concerned with au-151tomating information extraction tasks typically per-formed by curators such as distinguishing betweencuratable and non-curatable PPI mentions and spec-ifying the details of how the PPI was detected.6 ConclusionsA system was implemented for enriching protein-protein interactions (PPIs) with properties and at-tributes providing additional information useful tobiologists.
It was found that a machine learningapproach to property tagging, using simple contex-tual features, was very effective in most cases, butless effective for values that occurred rarely, or forwhich annotators found difficulty in assigning val-ues.
For the attributes, sparsity of data meant thatrule-based approaches worked best, using fairly sim-ple rules that could be quickly developed, althoughmachine learning approaches could be competitivewhen there was sufficient data.7 AcknowledgementsThe authors are very grateful to the annotationteam, and to Cognia (http://www.cognia.com) for their collaboration on the TXM project.This work is supported by the Text Mining Pro-gramme of ITI Life Sciences Scotland (http://www.itilifesciences.com).ReferencesErick Alphonse, Sophie Aubin, Philippe Bessieres, GillesBisson, Thierry Hamon, Sandrine Lagarrigue, AdelineNazarenko, Alain-Pierre Manine, Claire Nedellec, Mo-hamed Ould Abdel Vetah, Thierry Poibeau, and Davy Weis-senbacher.
2004.
Event-based information extraction for thebiomedical domain: the Caderige project.C.
Blaschke and A. Valencia.
2002.
The frame-based moduleof the suiseki information extraction system.
IEEE Intelli-gent Systems, (17):14?20.Razvan Bunescu and Raymond Mooney.
2006.
Subsequencekernels for relation extraction.
In Y. Weiss, B. Schlkopf, andJ.
Platt, editors, Advances in Neural Information ProcessingSystems 18.
Cambridge, MA.James R. Curran and Stephen Clark.
2003.
Language indepen-dent NER using a maximum entropy tagger.
In Proceedingsof CoNLL-2003.Ian Donaldson, Joel Martin, Berry de Bruijn, Cheryl Wolt-ing, Vicki Lay, Brigitte Tuekam, Shudong Zhang, BerivanBaskin, Gary D. Bader, Katerina Michalickova, Tony Paw-son, and Christopher W. V. Hogue.
2003.
PreBIND and Tex-tomy - mining the biomedical literature for protein-proteininteractions using a support vector machine.
BMC Bioinfor-matics, 4:11.Claudio Giuliano, Alberto Lavelli, and Lorenza Romano.
2006.Exploiting shallow linguistic information for relation extrac-tion from biomedical literature.
In Proceedings of the EACL.Claire Grover and Richard Tobin.
2006.
Rule-Based Chunkingand Reusability.
In Proceedings of LREC 2006.Jerry R. Hobbs.
2002.
Information extraction from biomedicaltext.
Journal of Biomedical Informatics, 35(4):260?264.N.
Karamanis, I. Lewin, R. Seal, R. Drysdale, and E. J. Briscoe.2007.
Integrating natural language processing with flybasecuration.
In Proceedings of PSB 2007.Martin Krallinger, Florian Leitner, and Alfonso Valencia.
2007.Assessment of the Second BioCreative PPI Task: AutomaticExtraction of Protein-Protein Interactions.
In Proceedings ofthe Second BioCreative Challenge Evaluation Workshop.E.
Marsh and D. Perzanowski.
1998.
MUC-7 evaluation of IEtechnology: Overview of results.
In Proceedings of MUC-7.Guido Minnen, John Carroll, and Darren Pearce.
2000.
Robust,applied morphological generation.
In Proceedings of INLG2000.Alessandro Moschitti.
2004.
A study on convolution kernelsfor shallow semantic parsing.
In Proceedings of the ACL.Leif Arda Nielsen.
2006.
Extracting protein-protein interac-tions using simple contextual features.
In Proceedings of theBioNLP 2006 at HLT/NAACL 2006.Conrad Plake, Jo?rg Hakenberg, and Ulf Leser.
2005.
Op-timizing syntax-patterns for discovering protein-protein-interactions.
In Proc ACM Symposium on Applied Comput-ing, SAC, Bioinformatics Track, volume 1, March.A.S.
Schwartz and M.A.
Hearst.
2003.
Identifying abbreviationdefinitions in biomedical text.
In Proceedings of PSB 2003.Parantu K. Shah and Peer Bork.
2006.
Lsat: learning about al-ternative transcripts in medline.
Bioinformatics, 22(7):857?865.L.
Smith, T. Rindflesch, and W. J. Wilbur.
2004.
MedPost: apart-of-speech tagger for biomedical text.
Bioinformatics,20(14):2320?2321.Bjo?rn M. Ursing, Frank H. J. van Enckevort, Jack A. M. Leu-nissen, and Roland J. Siezen.
2001.
Exprot - a database forexperimentally verified protein functions.
In Silico Biology,2:1.Tuangthong Wattarujeekrit, Parantu K. Shah, and Nigel Collier.2004.
PASBio: predicate-argument structures for event ex-traction in molecular biology.
BMC Bioinformatics, 5:155.John W. Wilbur, Andrey Rzhetsky, and Hagit Shatkay.
2006.New directions in biomedical text annotation: definitions,guidelines and corpus construction.
BMC Bioinformatics,7:356+, July.H.
Xu, D. Krupke, J. Blake, and C. Friedman.
2006.
A naturallanguage processing (nlp) tool to assist in the curation of thelaboratory mouse tumor biology database.
AMIA Annu SympProc.Alexander Yeh, Lynette Hirschman, and Alexander Morgan.2002.
Background and overview for KDD cup 2002 task 1:information extraction from biomedical articles.
SIGKDDExplor.
Newsl., 4(2):87?89.152
