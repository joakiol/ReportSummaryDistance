Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 161?164,Suntec, Singapore, 4 August 2009.c?2009 ACL and AFNLPAutomatic Satire Detection: Are You Having a Laugh?Clint BurfootCSSEUniversity of MelbourneVIC 3010 Australiacburfoot@csse.unimelb.edu.auTimothy BaldwinCSSEUniversity of MelbourneVIC 3010 Australiatim@csse.unimelb.edu.auAbstractWe introduce the novel task of determin-ing whether a newswire article is ?true?or satirical.
We experiment with SVMs,feature scaling, and a number of lexicaland semantic feature types, and achievepromising results over the task.1 IntroductionThis paper describes a method for filtering satiricalnews articles from true newswire documents.
Wedefine a satirical article as one which deliberatelyexposes real-world individuals, organisations andevents to ridicule.Satirical news articles tend to mimic truenewswire articles, incorporating irony and non se-quitur in an attempt to provide humorous insight.An example excerpt is:Bank Of England Governor Mervyn King is aQueen, Says Fed Chairman Ben BernankeDuring last night?s appearance on the Amer-ican David Letterman Show, Fed ChairmanBen Bernanke let slip that Bank of England(BOE) Governor, Mervyn King, enjoys wearingwomen?s clothing.Contrast this with a snippet of a true newswire ar-ticle:Delegates prepare for Cairo conference amidtight securityDelegates from 156 countries began preparatorytalks here Saturday ahead of the official openingof the UN World Population Conference amidtight security.The basis for our claim that the first document issatirical is surprisingly subtle in nature, and relatesto the absurdity of the suggestion that a prominentfigure would expose another prominent figure asa cross dresser, the implausibility of this story ap-pearing in a reputable news source, and the pun onthe name (King being a Queen).Satire classification is a novel task to compu-tational linguistics.
It is somewhat similar to themore widely-researched text classification tasks ofspam filtering (Androutsopoulos et al, 2000) andsentiment classification (Pang and Lee, 2008), inthat: (a) it is a binary classification task, and (b)it is an intrinsically semantic task, i.e.
satire newsarticles are recognisable as such through interpre-tation and cross-comparison to world knowledgeabout the entities involved.
Similarly to spam fil-tering and sentiment classification, a key ques-tion asked in this research is whether it is possi-ble to perform the task on the basis of simple lex-ical features of various types.
That is, is it pos-sible to automatically detect satire without accessto the complex inferencing and real-world knowl-edge that humans make use of.The primary contributions of this research are asfollows: (1) we introduce a novel task to the arenaof computational linguistics and machine learning,and make available a standardised dataset for re-search on satire detection; and (2) we develop amethod which is adept at identifying satire basedon simple bag-of-words features, and further ex-tend it to include richer features.2 CorpusOur satire corpus consists of a total of 4000newswire documents and 233 satire news articles,split into fixed training and test sets as detailed inTable 1.
The newswire documents were randomlysampled from the English Gigaword Corpus.
Thesatire documents were selected to relate closelyto at least one of the newswire documents by:(1) randomly selecting a newswire document; (2)hand-picking a key individual, institution or eventfrom the selected document, and using it to for-mulate a phrasal query (e.g.
Bill Clinton); (3) us-ing the query to issue a site-restricted query to the161Training Test TotalTRUE 2505 1495 4000SATIRE 133 100 233Table 1: Corpus statisticsGoogle search engine;1and (4) manually filteringout ?non-newsy?, irrelevant and overly-offensivedocuments from the top-10 returned documents(i.e.
documents not containing satire news articles,or containing satire articles which were not rel-evant to the original query).
All newswire andsatire documents were then converted to plain textof consistent format using lynx, and all contentother than the title and body of the article wasmanually removed (including web page menus,and header and footer data).
Finally, all documentswere manually post-edited to remove references tothe source (e.g.
AP or Onion), formatting quirksspecific to a particular source (e.g.
all caps in thetitle), and any textual metadata which was indica-tive of the document source (e.g.
editorial notes,dates and locations).
This was all in an effort toprevent classifiers from accessing superficial fea-tures which are reliable indicators of the documentsource and hence trivialise the satire detection pro-cess.It is important to note that the number of satiri-cal news articles in the corpus is significantly lessthan the number of true newswire articles.
Thisreflects an impressionistic view of the web: thereis far more true news content than satirical newscontent.The corpus is novel to this research,and is publicly available for download athttp://www.csse.unimelb.edu.au/research/lt/resources/satire/.3 Method3.1 Standard text classification approachWe take our starting point from topic-based textclassification (Dumais et al, 1998; Joachims,1998) and sentiment classification (Turney, 2002;Pang and Lee, 2008).
State-of-the-art results inboth fields have been achieved using support vec-1The sites queried were satirewire.com,theonion.com, newsgroper.com, thespoof.com, brokennewz.com, thetoque.com,bbspot.com, neowhig.org, humorfeed.com,satiricalmuslim.com, yunews.com,newsbiscuit.com.tor machines (SVMs) and bag-of-words features.We supplement the bag-of-words model with fea-ture weighting, using the two methods describedbelow.Binary feature weights: Under this schemeall features are given the same weight, regard-less of how many times they appear in each arti-cle.
The topic and sentiment classification exam-ples cited found binary features gave better perfor-mance than other alternatives.Bi-normal separation feature scaling: BNS(Forman, 2008) has been shown to outperformother established feature representation schemeson a wide range of text classification tasks.
Thissuperiority is especially pronounced for collec-tions with a low proportion of positive class in-stances.
Under BNS, features are allocated aweight according to the formula:|F?1(tpr)?
F?1(fpr)|where F?1is the inverse normal cumulative dis-tribution function, tpr is the true positive rate(P(feature|positive class)) and fpr is the false pos-itive rate (P(feature|negative class)).BNS produces the highest weights for featuresthat are strongly correlated with either the nega-tive or positive class.
Features that occur evenlyacross the training instances are given the lowestweight.
This behaviour is particularly helpful forfeatures that correlate with the negative class ina negatively-skewed classification task, so in ourcase BNS should assist the classifier in making useof features that identify true articles.SVM classification is performed with SVMlight(Joachims, 1999) using a linear kernel and the de-fault parameter settings.
Tokens are case folded;currency amounts (e.g.
$2.50), abbreviations (e.g.U.S.A.
), and punctuation sequences (e.g.
acomma, or a closing quote mark followed by a pe-riod) are treated as separate features.3.2 Targeted lexical featuresThis section describe three types of features in-tended to embody characteristics of satire newsdocuments.Headline features: Most of the articles in thecorpus have a headline as their first line.
To a hu-man reader, the vast majority of the satire docu-ments in our corpus are immediately recognisableas such from the headline alone, suggesting thatour classifiers may get something out of having the162headline contents explicitly identified in the fea-ture vector.
To this end, we add an additional fea-ture for each unigram appearing on the first lineof an article.
In this way the heading tokens arerepresented twice: once in the overall set of uni-grams in the article, and once in the set of headingunigrams.Profanity: true news articles very occasionallyinclude a verbal quote which contains offensivelanguage, but in practically all other cases it is in-cumbent on journalists and editors to keep theirlanguage ?clean?.
A review of the corpus showsthat this is not the case with satirical news, whichoccasionally uses profanity as a humorous device.Let P be a binary feature indicating whetheror not an article contains profanity, as determinedby the Regexp::Common::profanity Perlmodule.2Slang: As with profanity, it is intuitively truethat true news articles tend to avoid slang.
An im-pressionistic review of the corpus suggests that in-formal language is much more common to satiricalarticles.
We measure the informality of an articleas:idef=1|T |?t?Ts(t)where T is the set of unigram tokens in the articleand s is a function taking the value 1 if the tokenhas a dictionary definition marked as slang and 0if it does not.It is important to note that this measure of ?in-formality?
is approximate at best.
We do not at-tempt, e.g., to disambiguate the sense of individ-ual word terms to tell whether the slang sense ofa word is the one intended.
Rather, we simplycheck to see if each word has a slang usage inWik-tionary.3A continuous feature is set to the value of i foreach article.
Discrete features highi and lowi areset as:highidef={1 v >?i + 2?
;0lowidef={1 v <?i?
2?
;0where?i and ?
are, respectively, the mean and stan-dard deviation of i across all articles.2http://search.cpan.org/perldoc?Regexp::Common::profanity3http://www.wiktionary.org3.3 Semantic validityLexical approaches are clearly inadequate if weassume that good satirical news articles tend toemulate real news in tone, style, and content.What is needed is an approach that captures thedocument semantics.One common device in satire news articles isabsurdity, in terms of describing well-known indi-viduals in unfamiliar settings which parody theirviewpoints or public profile.
We attempt to cap-ture this via validity, in the form of the relative fre-quency of the particular combination of key partic-ipants reported in the story.
Our method identifiesthe named entities in a given document and queriesthe web for the conjunction of those entities.
Ourexpectation is that true news stories will have beenreported in various forums, and hence the numberof web documents which include the same com-bination of entities will be higher than with satiredocuments.To implement this method, we first use theStanford Named Entity Recognizer4(Finkel et al,2005) to identify the set of person and organisationentities, E, from each article in the corpus.From this, we estimate the validity of the com-bination of entities in the article as:v(E)def= |g(E)|where g is the set of matching documents returnedby Google using a conjunctive query.
We antici-pate that v will have two potentially useful prop-erties: (1) it will be relatively lower when E in-cludes made-up entity names such as Hitler Com-memoration Institute, found in one satirical corpusarticle; and (2) it will be relatively lower when Econtains unusual combinations of entities such as,for example, those in the satirical article beginningMissing Brazilian balloonist Padre spotted strad-dling Pink Floyd flying pig.We include both a continuous representation ofv for each article, in the form of log(v(E)), anddiscrete variants of the feature, based on the samemethodology as for highi and lowi.4 ResultsThe results for our classifiers over the satire cor-pus are shown in Table 2.
The baseline is a naiveclassifier that assigns all instances to the positive4http://nlp.stanford.edu/software/CRF-NER.shtml163(?article?SATIRE??)
P R Fall-positive baseline 0.063 1.000 0.118BIN 0.943 0.500 0.654BIN+lex 0.945 0.520 0.671BIN+val 0.943 0.500 0.654BIN+all 0.945 0.520 0.671BNS 0.944 0.670 0.784BNS+lex 0.957 0.660 0.781BNS+val 0.945 0.690 0.798BNS+all 0.958 0.680 0.795Table 2: Results for satire detection (P = preci-sion, R = recall, and F = F-score) for binary un-igram features (BIN) and BNS unigram features(BNS), optionally using lexical (lex), validity (val)or combined lexical and validity (all) featuresclass (i.e.
SATIRE).
An SVM classifier with simplebinary unigram word features provides a standardtext classification benchmark.All of the classifiers easily outperform the base-line.
This is to be expected given the low pro-portion of positive instances in the corpus.
Thebenchmark classifier has very good precision, butrecall of only 0.500.
Adding the heading, slang,and profanity features provides a small improve-ment in both precision and recall.Moving to BNS feature scaling keeps the veryhigh precision and increases the recall to 0.670.Adding in the heading, slang and profanity lexicalfeatures (?+lex?)
actually decreases the F-scoreslightly, but adding the validity features (?+val?
)provides a near 2 point F-score increase, resultingin the best overall F-score of 0.798.All of the BNS scores achieve statisticallysignificant improvements over the benchmark interms of F-score (using approximate randomisa-tion, p < 0.05).
The 1-2% gains given by addingin the various feature types are not statistically sig-nificant due to the small number of satire instancesconcerned.All of the classifiers achieve very high precisionand considerably lower recall.
Error analysis sug-gests that the reason for the lower recall is subtlersatire articles, which require detailed knowledgeof the individuals to be fully appreciated as satire.While they are not perfect, however, the classi-fiers achieve remarkably high performance giventhe superficiality of the features used.5 Conclusions and future workThis paper has introduced a novel task to computa-tional linguistics and machine learning: determin-ing whether a newswire article is ?true?
or satiri-cal.
We found that the combination of SVMs withBNS feature scaling achieves high precision andlower recall, and that the inclusion of the notion of?validity?
achieves the best overall F-score.ReferencesIon Androutsopoulos, John Koutsias, Konstantinos V.Chandrinos, George Paliouras, and Constantine D.Spyropoulos.
2000.
An evaluation of NaiveBayesian anti-spam filtering.
In Proceedings of the11th European Conference on Machine Learning,pages 9?17, Barcelona, Spain.Susan Dumais, John Platt, David Heckerman, andMehran Sahami.
1998.
Inductive learning algo-rithms and representations for text categorization.In Proceedings of the Seventh International Confer-ence on Information and Knowledge Management,pages 148?155, New York, USA.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by Gibbssampling.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguistics(ACL?05), pages 363?370, Ann Arbor, USA.George Forman.
2008.
BNS scaling: An improvedrepresentation over TF-IDF for SVM text classifi-cation.
In Proceedings of the 17th InternationalConference on Information and Knowledge Man-agement, pages 263?270, Napa Valley, USA.Thorsten Joachims.
1998.
Text categorization withsupport vector machines: learning with many rele-vant features.
In Proceedings of the 10th EuropeanConference on Machine Learning, pages 137?142,Chemnitz, Germany.Thorsten Joachims.
1999.
Making large-scale sup-port vector machine learning practical.
In BernhardSch?olkopf, Christopher J. C. Burges, and Alexan-der J. Smola, editors, Advances in Kernel Meth-ods: Support Vector Learning, pages 169?184.
MITPress, Cambridge, USA.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and Trends in In-formation Retrieval, 2(1?2):1?135.Peter Turney.
2002.
Thumbs up or thumbs down?
se-mantic orientation applied to unsupervised classifi-cation of reviews.
In Proceedings of 40th AnnualMeeting of the Association for Computational Lin-guistics, pages 417?424, Philadelphia, USA.164
