Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 712?721,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsLearning Sub-Word Units for Open Vocabulary Speech RecognitionCarolina Parada1, Mark Dredze1, Abhinav Sethy2, and Ariya Rastrow11Human Language Technology Center of Excellence, Johns Hopkins University3400 N Charles Street, Baltimore, MD, USAcarolinap@jhu.edu, mdredze@cs.jhu.edu, ariya@jhu.edu2IBM T.J. Watson Research Center, Yorktown Heights, NY, USAasethy@us.ibm.comAbstractLarge vocabulary speech recognition systemsfail to recognize words beyond their vocab-ulary, many of which are information richterms, like named entities or foreign words.Hybrid word/sub-word systems solve thisproblem by adding sub-word units to large vo-cabulary word based systems; new words canthen be represented by combinations of sub-word units.
Previous work heuristically cre-ated the sub-word lexicon from phonetic rep-resentations of text using simple statistics toselect common phone sequences.
We pro-pose a probabilistic model to learn the sub-word lexicon optimized for a given task.
Weconsider the task of out of vocabulary (OOV)word detection, which relies on output froma hybrid model.
A hybrid model with ourlearned sub-word lexicon reduces error by6.3% and 7.6% (absolute) at a 5% false alarmrate on an English Broadcast News and MITLectures task respectively.1 IntroductionMost automatic speech recognition systems operatewith a large but limited vocabulary, finding the mostlikely words in the vocabulary for the given acousticsignal.
While large vocabulary continuous speechrecognition (LVCSR) systems produce high qualitytranscripts, they fail to recognize out of vocabulary(OOV) words.
Unfortunately, OOVs are often infor-mation rich nouns, such as named entities and for-eign words, and mis-recognizing them can have adisproportionate impact on transcript coherence.Hybrid word/sub-word recognizers can produce asequence of sub-word units in place of OOV words.Ideally, the recognizer outputs a complete word forin-vocabulary (IV) utterances, and sub-word unitsfor OOVs.
Consider the word ?Slobodan?, the givenname of the former president of Serbia.
As an un-common English word, it is unlikely to be in the vo-cabulary of an English recognizer.
While a LVCSRsystem would output the closest known words (e.x.
?slow it dawn?
), a hybrid system could output asequence of multi-phoneme units: s l ow, b ax,d ae n. The latter is more useful for automaticallyrecovering the word?s orthographic form, identify-ing that an OOV was spoken, or improving perfor-mance of a spoken term detection system with OOVqueries.
In fact, hybrid systems have improved OOVspoken term detection (Mamou et al, 2007; Paradaet al, 2009), achieved better phone error rates, espe-cially in OOV regions (Rastrow et al, 2009b), andobtained state-of-the-art performance for OOV de-tection (Parada et al, 2010).Hybrid recognizers vary in a number of ways:sub-word unit type: variable-length phonemeunits (Rastrow et al, 2009a; Bazzi and Glass, 2001)or joint letter sound sub-words (Bisani and Ney,2005); unit creation: data-driven or linguisticallymotivated (Choueiter, 2009); and how they are in-corporated in LVCSR systems: hierarchical (Bazzi,2002) or flat models (Bisani and Ney, 2005).In this work, we consider how to optimally cre-ate sub-word units for a hybrid system.
These unitsare variable-length phoneme sequences, although inprinciple our work can be use for other unit types.Previous methods for creating the sub-word lexi-712con have relied on simple statistics computed fromthe phonetic representation of text (Rastrow et al,2009a).
These units typically represent the most fre-quent phoneme sequences in English words.
How-ever, it isn?t clear why these units would produce thebest hybrid output.
Instead, we introduce a prob-abilistic model for learning the optimal units for agiven task.
Our model learns a segmentation of atext corpus given some side information: a mappingbetween the vocabulary and a label set; learned unitsare predictive of class labels.In this paper, we learn sub-word units optimizedfor OOV detection.
OOV detection aims to identifyregions in the LVCSR output where OOVs were ut-tered.
Towards this goal, we are interested in select-ing units such that the recognizer outputs them onlyfor OOV regions while prefering to output a com-plete word for in-vocabulary regions.
Our approachyields improvements over state-of-the-art results.We begin by presenting our log-linear model forlearning sub-word units with a simple but effectiveinference procedure.
After reviewing existing OOVdetection approaches, we detail how the learnedunits are integrated into a hybrid speech recognitionsystem.
We show improvements in OOV detection,and evaluate impact on phone error rates.2 Learning Sub-Word UnitsGiven raw text, our objective is to produce a lexiconof sub-word units that can be used by a hybrid sys-tem for open vocabulary speech recognition.
Ratherthan relying on the text alone, we also utilize sideinformation: a mapping of words to classes so wecan optimize learning for a specific task.The provided mapping assigns labels Y to the cor-pus.
We maximize the probability of the observedlabeling sequence Y given the text W : P (Y |W ).We assume there is a latent segmentation S of thiscorpus which impacts Y .
The complete data likeli-hood becomes: P (Y |W ) =?S P (Y, S|W ) duringtraining.
Since we are maximizing the observed Y ,segmentation S must discriminate between differentpossible labels.We learn variable-length multi-phone units bysegmenting the phonetic representation of each wordin the corpus.
Resulting segments form the sub-word lexicon.1 Learning input includes a list ofwords to segment taken from raw text, a mappingbetween words and classes (side information indi-cating whether token is IV or OOV), a pronuncia-tion dictionaryD, and a letter to sound model (L2S),such as the one described in Chen (2003).
The cor-pus W is the list of types (unique words) in the rawtext input.
This forces each word to have a uniquesegmentation, shared by all common tokens.
Wordsare converted into phonetic representations accord-ing to their most likely dictionary pronunciation;non-dictionary words use the L2S model.22.1 ModelInspired by the morphological segmentation modelof Poon et al (2009), we assume P (Y, S|W ) is alog-linear model parameterized by ?:P?
(Y, S|W ) =1Z(W )u?
(Y, S,W ) (1)where u?
(Y, S,W ) defines the score of the pro-posed segmentation S for words W and labels Yaccording to model parameters ?.
Sub-word units?
compose S, where each ?
is a phone sequence, in-cluding the full pronunciation for vocabulary words;the collection of ?s form the lexicon.
Each unit?
is present in a segmentation with some contextc = (?l, ?r) of the form ?l??r.
Features based onthe context and the unit itself parameterize u?.In addition to scoring a segmentation based onfeatures, we include two priors inspired by the Min-imum Description Length (MDL) principle sug-gested by Poon et al (2009).
The lexicon priorfavors smaller lexicons by placing an exponentialprior with negative weight on the length of the lex-icon??
|?|, where |?| is the length of the unit ?in number of phones.
Minimizing the lexicon priorfavors a trivial lexicon of only the phones.
Thecorpus prior counters this effect, an exponentialprior with negative weight on the number of unitsin each word?s segmentation, where |si| is the seg-mentation length and |wi| is the length of the wordin phones.
Learning strikes a balance between thetwo priors.
Using these definitions, the segmenta-tion score u?
(Y, S,W ) is given as:1Since sub-word units can expand full-words, we refer toboth words and sub-words simply as units.2The model can also take multiple pronunciations (?3.1).713s l ow b ax d ae ns l ow(#,#, , b, ax)b ax(l,ow, , d, ae)d ae n(b,ax, , #, #)Figure 1: Units and bigram phone context (in parenthesis)for an example segmentation of the word ?slobodan?.u?
(Y, S,W ) = exp(??,y?
?,yf?,y(S, Y )+?c,y?c,yfc,y(S, Y )+ ?
???
?S|?|+ ?
?
?i?W|si|/|wi|)(2)f?,y(S, Y ) are the co-occurrence counts of the pair(?, y) where ?
is a unit under segmentation S and yis the label.
fc,y(S, Y ) are the co-occurrence countsfor the context c and label y under S. The modelparameters are ?
= {?
?,y, ?c,y : ?
?, c, y}.
The neg-ative weights for the lexicon (?)
and corpus priors(?)
are tuned on development data.
The normalizerZ sums over all possible segmentations and labels:Z(W ) =?S?
?Y ?u?
(Y?, S?,W ) (3)Consider the example segmentation for the word?slobodan?
with pronunciation s,l,ow,b,ax,d,ae,n(Figure 1).
The bigram phone context as a four-tupleappears below each unit; the first two entries corre-spond to the left context, and last two the right con-text.
The example corpus (Figure 2) demonstrateshow unit features f?,y and context features fc,y arecomputed.3 Model TrainingLearning maximizes the log likelihood of the ob-served labels Y ?
given the words W :`(Y ?|W ) = log?S1Z(W )u?
(Y?, S,W ) (4)We use the Expectation-Maximization algorithm,where the expectation step predicts segmentations SLabeled corpus: president/y = 0 milosevic/y = 1Segmented corpus: p r eh z ih d ih n t/0 m ih/1 l aa/1s ax/1 v ih ch/1Unit-feature:Value p r eh z ih d ih n t/0:1 m ih/1:1l aa/1:1 s ax/1:1 v ih ch/1:1Context-feature:Value(#/0,#/0, ,l/1,aa/1):1,(m/1,ih/1, ,s/1,ax/1):1,(l/1,aa/1, ,v/1,ih/1):1,(s/1,ax/1, ,#/0,#/0):1,(#/0,#/0, ,#/0,#/0):1Figure 2: A small example corpus with segmentationsand corresponding features.
The notation m ih/1:1represents unit/label:feature-value.
Overlapping contextfeatures capture rich segmentation regularities associatedwith each class.given the model?s current parameters ?
(?3.1), andthe maximization step updates these parameters us-ing gradient ascent.
The partial derivatives of theobjective (4) with respect to each parameter ?i are:?`(Y ?|W )?
?i= ES|Y ?,W [fi]?
ES,Y |W [fi] (5)The gradient takes the usual form, where we en-courage the expected segmentation from the currentmodel given the correct labels to equal the expectedsegmentation and expected labels.
The next sectiondiscusses computing these expectations.3.1 InferenceInference is challenging since the lexicon prior ren-ders all word segmentations interdependent.
Con-sider a simple two word corpus: cesar (s,iy,z,er),and cesium (s,iy,z,iy,ax,m).
Numerous segmen-tations are possible; each word has 2N?1 possiblesegmentations, where N is the number of phones inits pronunciation (i.e., 23 ?
25 = 256).
However,if we decide to segment the first word as: {s iy,z er}, then the segmentation for ?cesium?
:{s iy,z iy ax m} will incur a lexicon prior penalty forincluding the new segment z iy ax m. If insteadwe segment ?cesar?
as {s iy z, er}, the segmen-tation {s iy, z iy ax m} incurs double penaltyfor the lexicon prior (since we are including two newunits in the lexicon: s iy and z iy ax m).
Thisdependency requires joint segmentation of the entirecorpus, which is intractable.
Hence, we resort to ap-proximations of the expectations in Eq.
(5).One approach is to use Gibbs Sampling: it-erating through each word, sampling a new seg-714mentation conditioned on the segmentation of allother words.
The sampling distribution requiresenumerating all possible segmentations for eachword (2N?1) and computing the conditional prob-abilities for each segmentation: P (S|Y ?,W ) =P (Y ?, S|W )/P (Y ?|W ) (the features are extractedfrom the remaining words in the corpus).
Using Msampled segmentations S1, S2, .
.
.
Sm we computeES|Y ?,W [fi] as follows:ES|Y ?,W [fi] ?1M?jfi[Sj ]Similarly, to compute ES,Y |W we sample a seg-mentation and a label for each word.
We com-pute the joint probability of P (Y, S|W ) for eachsegmentation-label pair using Eq.
(1).
A sampledsegmentation can introduce new units, which mayhave higher probability than existing ones.Using these approximations in Eq.
(5), we updatethe parameters using gradient ascent:?
?new = ?
?old + ??`??
(Y?|W )where ?
> 0 is the learning rate.To obtain the best segmentation, we use determin-istic annealing.
Sampling operates as usual, exceptthat the parameters are divided by a value, whichstarts large and gradually drops to zero.
To makeburn in faster for sampling, the sampler is initializedwith the most likely segmentation from the previousiteration.
To initialize the sampler the first time, weset al the parameters to zero (only the priors havenon-zero values) and run deterministic annealing toobtain the first segmentation of the corpus.3.2 Efficient SamplingSampling a segmentation for the corpus requirescomputing the normalization constant (3), whichcontains a summation over all possible corpus seg-mentations.
Instead, we approximate this constantby sampling words independently, keeping fixed allother segmentations.
Still, even sampling a singleword?s segmentation requires enumerating probabil-ities for all possible segmentations.We sample a segmentation efficiently using dy-namic programming.
We can represent all possiblesegmentations for a word as a finite state machine(FSM) (Figure 3), where arcs weights arise fromscoring the segmentation?s features.
This weight isthe negative log probability of the resulting modelafter adding the corresponding features and priors.However, the lexicon prior poses a problem forthis construction since the penalty incurred by a newunit in the segmentation depends on whether thatunit is present elsewhere in that segmentation.
Forexample, consider the segmentation for the wordANJANI: AA N, JH, AA N, IY.
If none of these unitsare in the lexicon, this segmentation yields the low-est prior penalty since it repeats the unit AA N. 3 Thisglobal dependency means paths must encode the fullunit history, making computing forward-backwardprobabilities inefficient.Our solution is to use the Metropolis-Hastings al-gorithm, which samples from the true distributionP (Y, S|W ) by first sampling a new label and seg-mentation (y?, s?)
from a simpler proposal distribu-tion Q(Y, S|W ).
The new assignment (y?, s?)
is ac-cepted with probability:?
(Y ?, S?|Y, S,W )=min?1,P (Y ?, S?|W )Q(Y, S|Y ?, S?,W )P (Y, S|W )Q(Y ?, S?|Y, S,W )?We choose the proposal distribution Q(Y, S|W )as Eq.
(1) omitting the lexicon prior, removing thechallenge for efficient computation.
The probabilityof accepting a sample becomes:?
(Y ?, S?|Y, S,W )=min?1,P??S?
|?|P?
?S |?|?
(6)We sample a path from the FSM by running theforward-backward algorithm, where the backwardcomputations are carried out explicitly, and the for-ward pass is done through sampling, i.e.
we traversethe machine only computing forward probabilitiesfor arcs leaving the sampled state.4 Once we samplea segmentation (and label) we accept it according toEq.
(6) or keep the previous segmentation if rejected.Alg.
1 shows our full sub-word learning proce-dure, where sampleSL (Alg.
2) samples a segmen-tation and label sequence for the entire corpus fromP (Y, S|W ), and sampleS samples a segmentationfrom P (S|Y ?,W ).3Splitting at phone boundaries yields the same lexicon priorbut a higher corpus prior.4We use OpenFst?s RandGen operation with a costumed arc-selector (http://www.openfst.org/).7150 1AA5AA_N_JH_AA_N4AA_N_JH_AA3AA_N_JH 2AA_NN_JH_AA_NN_JH_AAN_JHN6N_JH_AA_N_IYIYNAA_NAAAA_N_IYJH_AA_NJH_AAJHJH_AA_N_IYFigure 3: FSM representing all segmentations for the word ANJANI with pronunciation: AA,N,JH,AA,N,IYAlgorithm 1 TrainingInput: Lexicon L from training text W , Dictionary D,Mapping M , L2S pronunciations, Annealing temp T .Initialization:Assign label y?m = M [wm].
?
?0 = 0?S0 = random segmentation for each word in L.for i = 1 to K do/* E-Step */Si = bestSegmentation(T, ?i?1, Si?1).for k = 1 to NumSamples do(S?k, Y?k) = sampleSL(P (Y, Si|W ),Q(Y, Si|W ))S?k = sampleS(P (Si|Y ?,W ),Q(Si|Y ?,W ))end for/* M-Step */ES,Y |W [fi] =1NumSamples?k f?,l[S?k, Y?k]ES|Y ?,W [f?,l] =1NumSamples?k f?,l[S?k, Y?]?
?i = ?
?i?1 + ??L??
(Y?|W )end forS = bestSegmentation(T, ?K , S0)Output: Lexicon Lo from S4 OOV Detection Using Hybrid ModelsTo evaluate our model for learning sub-word units,we consider the task of out-of-vocabulary (OOV)word detection.
OOV detection for ASR output canbe categorized into two broad groups: 1) hybrid(filler) models: which explicitly model OOVs us-ing either filler, sub-words, or generic word mod-els (Bazzi, 2002; Schaaf, 2001; Bisani and Ney,2005; Klakow et al, 1999; Wang, 2009); and2) confidence-based approaches: which label un-reliable regions as OOVs based on different con-fidence scores, such as acoustic scores, languagemodels, and lattice scores (Lin et al, 2007; Burgetet al, 2008; Sun et al, 2001; Wessel et al, 2001).In the next section we detail the OOV detectionapproach we employ, which combines hybrid andAlgorithm 2 sampleSL(P (S, Y |W ), Q(S, Y |W ))for m = 1 to M (NumWords) do(s?m, y?m) = Sample segmentation/label pair forword wm according to Q(S, Y |W )Y ?
= {y1 .
.
.
ym?1y?mym+1 .
.
.
yM}S?
= {s1 .
.
.
sm?1s?msm+1 .
.
.
sM}?=min(1,P??S?
|?|P?
?S |?|)with prob ?
: ym,k = y?m, sm,k = s?mwith prob (1?
?)
: ym,k = ym, sm,k = smend forreturn (S?k, Y?k) = [(s1,k, y1,k) .
.
.
(sM,k, yM,k)]confidence-based models, achieving state-of-the artperformance for this task.4.1 OOV Detection ApproachWe use the state-of-the-art OOV detection model ofParada et al (2010), a second order CRF with fea-tures based on the output of a hybrid recognizer.This detector processes hybrid recognizer output, sowe can evaluate different sub-word unit lexicons forthe hybrid recognizer and measure the change inOOV detection accuracy.Our model (?2.1) can be applied to this task byusing a dictionary D to label words as IV (yi = 0 ifwi ?
D) and OOV (yi = 1 if wi /?
D).
This resultsin a labeled corpus, where the labeling sequence Yindicates the presence of out-of-vocabulary words(OOVs).
For comparison we evaluate a baselinemethod (Rastrow et al, 2009b) for selecting units.Given a sub-word lexicon, the word and sub-words are combined to form a hybrid languagemodel (LM) to be used by the LVCSR system.
Thishybrid LM captures dependencies between word andsub-words.
In the LM training data, all OOVs arerepresented by the smallest number of sub-wordswhich corresponds to their pronunciation.
Pronun-ciations for all OOVs are obtained using grapheme716to phone models (Chen, 2003).Since sub-words represent OOVs while buildingthe hybrid LM, the existence of sub-words in ASRoutput indicate an OOV region.
A simple solution tothe OOV detection problem would then be reducedto a search for the sub-words in the output of theASR system.
The search can be on the one-besttranscripts, lattices or confusion networks.
Whilelattices contain more information, they are harderto process; confusion networks offer a trade-off be-tween richness (posterior probabilities are alreadycomputed) and compactness (Mangu et al, 1999).Two effective indications of OOVs are the exis-tence of sub-words (Eq.
7) and high entropy in anetwork region (Eq.
8), both of which are used asfeatures in the model of Parada et al (2010).Sub-word Posterior =??
?tjp(?|tj) (7)Word-Entropy =?
?w?tjp(w|tj) log p(w|tj) (8)tj is the current bin in the confusion network and?
is a sub-word in the hybrid dictionary.
Improvingthe sub-word unit lexicon, improves the quality ofthe confusion networks for OOV detection.5 Experimental SetupWe used the data set constructed by Can et al(2009) (OOVCORP) for the evaluation of SpokenTerm Detection of OOVs since it focuses on theOOV problem.
The corpus contains 100 hours oftranscribed Broadcast News English speech.
Thereare 1290 unique OOVs in the corpus, which wereselected with a minimum of 5 acoustic instances perword and short OOVs inappropriate for STD (lessthan 4 phones) were explicitly excluded.
ExampleOOVs include: NATALIE, PUTIN, QAEDA,HOLLOWAY, COROLLARIES, HYPERLINKED,etc.
This resulted in roughly 24K (2%) OOV tokens.For LVCSR, we used the IBM Speech Recogni-tion Toolkit (Soltau et al, 2005)5 to obtain a tran-script of the audio.
Acoustic models were trainedon 300 hours of HUB4 data (Fiscus et al, 1998)and utterances containing OOV words as marked inOOVCORP were excluded.
The language model wastrained on 400M words from various text sources5The IBM system used speaker adaptive training based onmaximum likelihood with no discriminative training.with a 83K word vocabulary.
The LVCSR system?sWER on the standard RT04 BN test set was 19.4%.Excluded utterances amount to 100hrs.
These weredivided into 5 hours of training for the OOV detec-tor and 95 hours of test.
Note that the OOV detectortraining set is different from the LVCSR training set.We also use a hybrid LVCSR system, combin-ing word and sub-word units obtained from ei-ther our approach or a state-of-the-art baseline ap-proach (Rastrow et al, 2009a) (?5.2).
Our hybridsystem?s lexicon has 83K words and 5K or 10Ksub-words.
Note that the word vocabulary is com-mon to both systems and only the sub-words are se-lected using either approach.
The word vocabularyused is close to most modern LVCSR system vo-cabularies for English Broadcast News; the result-ing OOVs are more challenging but more realistic(i.e.
mostly named entities and technical terms).
The1290 words are OOVs to both the word and hybridsystems.In addition we report OOV detection results on aMIT lectures data set (Glass et al, 2010) consistingof 3 Hrs from two speakers with a 1.5% OOV rate.These were divided into 1 Hr for training the OOVdetector and 2 Hrs for testing.
Note that the LVCSRsystem is trained on Broadcast News data.
This out-of-domain test-set help us evaluate the cross-domainperformance of the proposed and baseline hybridsystems.
OOVs in this data set correspond mainly totechnical terms in computer science and math.
e.g.ALGORITHM, DEBUG, COMPILER, LISP.5.1 Learning parametersFor learning the sub-words we randomly selectedfrom training 5,000 words which belong to the 83Kvocabulary and 5,000 OOVs6.
For development weselected an additional 1,000 IV and 1,000 OOVs.This was used to tune our model hyper parameters(set to ?
= ?1, ?
= ?20).
There is no overlapof OOVs in training, development and test sets.
Allfeature weights were initialized to zero and had aGaussian prior with variance ?
= 100.
Each of thewords in training and development was converted totheir most-likely pronunciation using the dictionary6This was used to obtain the 5K hybrid system.
To learn sub-words for the 10K hybrid system we used 10K in-vocabularywords and 10K OOVs.
All words were randomly selected fromthe LM training text.717for IV words or the L2S model for OOVs.7The learning rate was ?k =?(k+1+A)?
, where k isthe iteration,A is the stability constant (set to 0.1K),?
= 0.4, and ?
= 0.6.
We used K = 40 itera-tions for learning and 200 samples to compute theexpectations in Eq.
5.
The sampler was initializedby sampling for 500 iterations with deterministic an-nealing for a temperature varying from 10 to 0 at 0.1intervals.
Final segmentations were obtained using10, 000 samples and the same temperature schedule.We limit segmentations to those including units of atmost 5 phones to speed sampling with no significantdegradation in performance.
We observed improvedperformance by dis-allowing whole word units.5.2 Baseline Unit SelectionWe used Rastrow et al (2009a) as our baselineunit selection method, a data driven approach wherethe language model training text is converted intophones using the dictionary (or a letter-to-soundmodel for OOVs), and a N-gram phone LM is es-timated on this data and pruned using a relative en-tropy based method.
The hybrid lexicon includesresulting sub-words ?
ranging from unigrams to 5-gram phones, and the 83K word lexicon.5.3 EvaluationWe obtain confusion networks from both the wordand hybrid LVCSR systems.
We align the LVCSRtranscripts with the reference transcripts and tageach confusion region as either IV or OOV.
TheOOV detector classifies each region in the confusionnetwork as IV/OOV.
We report OOV detection accu-racy using standard detection error tradeoff (DET)curves (Martin et al, 1997).
DET curves measuretradeoffs between false alarms (x-axis) and misses(y-axis), and are useful for determining the optimaloperating point for an application; lower curves arebetter.
Following Parada et al (2010) we separatelyevaluate unobserved OOVs.87In this work we ignore pronunciation variability and sim-ply consider the most likely pronunciation for each word.
Itis straightforward to extend to multiple pronunciations by firstsampling a pronunciation for each word and then sampling asegmentation for that pronunciation.8Once an OOV word has been observed in the OOV detectortraining data, even if it was not in the LVCSR training data, it isno longer truly OOV.6 ResultsWe compare the performance of a hybrid sys-tem with baseline units9 (?5.2) and one with unitslearned by our model on OOV detection and phoneerror rate.
We present results using a hybrid systemwith 5k and 10k sub-words.We evaluate the CRF OOV detector with two dif-ferent feature sets.
The first uses only Word En-tropy and Sub-word Posterior (Eqs.
7 and 8) (Fig-ure 4)10.
The second (context) uses the extendedcontext features of Parada et al (2010) (Figure 5).Specifically, we include all trigrams obtained fromthe best hypothesis of the recognizer (a window of 5words around current confusion bin).
Predictions atdifferent FA rates are obtained by varying a proba-bility threshold.At a 5% FA rate, our system (This Paper 5k) re-duces the miss OOV rate by 6.3% absolute over thebaseline (Baseline 5k) when evaluating all OOVs.For unobserved OOVs, it achieves 3.6% absoluteimprovement.
A larger lexicon (Baseline 10k andThis Paper 10k ) shows similar relative improve-ments.
Note that the features used so far do not nec-essarily provide an advantage for unobserved ver-sus observed OOVs, since they ignore the decodedword/sub-word sequence.
In fact, the performanceon un-observed OOVs is better.OOV detection improvements can be attributed toincreased coverage of OOV regions by the learnedsub-words compared to the baseline.
Table 1 showsthe percent of Hits: sub-word units predicted inOOV regions, and False Alarms: sub-word unitspredicted for in-vocabulary words.
We can seethat the proposed system increases the Hits by over8% absolute, while increasing the False Alarms by0.3%.
Interestingly, the average sub-word lengthfor the proposed units exceeded that of the baselineunits by 0.3 phones (Baseline 5K average lengthwas 2.92, while that of This Paper 5K was 3.2).9Our baseline results differ from Parada et al (2010).
Whenimplementing the lexicon baseline, we discovered that their hy-brid units were mistakenly derived from text containing testOOVs.
Once excluded, the relative improvements of previouswork remain, but the absolute error rates are higher.10All real-valued features were normalized and quantized us-ing the uniform-occupancy partitioning described in White etal.
(2007).
We used 50 partitions with a minimum of 100 train-ing values per partition.7180 5 10 15 20%FA303540455055606570%MissesBaseline (5k) This Paper (5k) Baseline (10k) This Paper (10k)(a)0 5 10 15 20%FA303540455055606570%MissesBaseline (5k) This Paper (5k) Baseline (10k) This Paper (10k)(b)Figure 4: DET curves for OOV detection using baseline hybrid systems for different lexicon size and proposed dis-criminative hybrid system on OOVCORP data set.
Evaluation on un-observed OOVs (a) and all OOVs (b).0 5 10 15 20%FA303540455055606570%MissesBaseline (10k) Baseline (10k) + context-features This Paper (10k) This Paper (10k) + context-features(a)0 5 10 15 20%FA1020304050607080%MissesBaseline (10k) Baseline (10k) + context-features This Paper (10k) This Paper (10k) + context-features(b)Figure 5: Effect of adding context features to baseline and discriminative hybrid systems on OOVCORP data set.Evaluation on un-observed OOVs (a) and all OOVs (b).Consistent with previously published results, in-cluding context achieves large improvement in per-formance.
The proposed hybrid system (This Pa-per 10k + context-features) still improves over thebaseline (Baseline 10k + context-features), howeverthe relative gain is reduced.
In this case, we ob-tain larger gains for un-observed OOVs which ben-efit less from the context clues learned in training.Lastly, we report OOV detection performance onMIT Lectures.
Both the sub-word lexicon and theLVCSR models were trained on Broadcast Newsdata, helping us evaluate the robustness of learnedsub-words across domains.
Note that the OOVsin these domains are quite different: MIT Lec-tures?
OOVs correspond to technical computer sci-Hybrid System Hits FAsBaseline (5k) 18.25 1.49This Paper (5k) 26.78 1.78Baseline (10k) 24.26 1.82This Paper (10k) 28.96 1.92Table 1: Coverage of OOV regions by baseline and pro-posed sub-words in OOVCORP.ence and math terms, while in Broadcast News theyare mainly named-entities.Figure 6 and 7 show the OOV detection results inthe MIT Lectures data set.
For un-observed OOVs,the proposed system (This Paper 10k) reduces themiss OOV rate by 7.6% with respect to the base-line (Baseline 10k) at a 5% FA rate.
Similar toBroadcast News results, we found that the learnedsub-words provide larger coverage of OOV regionsin MIT Lectures domain.
These results suggest thatthe proposed sub-words are not simply modeling thetraining OOVs (named-entities) better than the base-line sub-words, but also describe better novel unex-pected words.
Furthermore, including context fea-tures does not seem as helpful.
We conjecture thatthis is due to the higher WER11 and the less struc-tured nature of the domain: i.e.
ungrammatical sen-tences, disfluencies, incomplete sentences, makingit more difficult to predict OOVs based on context.11WER = 32.7% since the LVCSR system was trained onBroadcast News data as described in Section 5.7190 5 10 15 20%FA30405060708090%MissesBaseline (5k) This Paper (5k) Baseline (10k) This Paper (10k)(a)0 5 10 15 20%FA30405060708090%MissesBaseline (5k) This Paper (5k) Baseline (10k) This Paper (10k)(b)Figure 6: DET curves for OOV detection using baseline hybrid systems for different lexicon size and proposed dis-criminative hybrid system on MIT Lectures data set.
Evaluation on un-observed OOVs (a) and all OOVs (b).0 5 10 15 20%FA30405060708090%MissesBaseline (10k) Baseline (10k) + context-features This Paper (10k) This Paper (10k) + context-features(a)0 5 10 15 20%FA30405060708090%MissesBaseline (10k) Baseline (10k) + context-features This Paper (10k) This Paper (10k) + context-features(b)Figure 7: Effect of adding context features to baseline and discriminative hybrid systems on MIT Lectures data set.Evaluation on un-observed OOVs (a) and all OOVs (b).6.1 Improved Phonetic TranscriptionWe consider the hybrid lexicon?s impact on PhoneError Rate (PER) with respect to the reference tran-scription.
The reference phone sequence is obtainedby doing forced alignment of the audio stream to thereference transcripts using acoustic models.
Thisprovides an alignment of the pronunciation variantof each word in the reference and the recognizer?sone-best output.
The aligned words are converted tothe phonetic representation using the dictionary.Table 2 presents PERs for the word and differ-ent hybrid systems.
As previously reported (Ras-trow et al, 2009b), the hybrid systems achieve bet-ter PER, specially in OOV regions since they pre-dict sub-word units for OOVs.
Our method achievesmodest improvements in PER compared to the hy-brid baseline.
No statistically significant improve-ments in PER were observed on MIT Lectures.7 ConclusionsOur probabilistic model learns sub-word units forhybrid speech recognizers by segmenting a text cor-pus while exploiting side information.
Applying ourSystem OOV IV AllWord 1.62 6.42 8.04Hybrid: Baseline (5k) 1.56 6.44 8.01Hybrid: Baseline (10k) 1.51 6.41 7.92Hybrid: This Paper (5k) 1.52 6.42 7.94Hybrid: This Paper (10k) 1.45 6.39 7.85Table 2: Phone Error Rate for OOVCORP.method to the task of OOV detection, we obtain anabsolute error reduction of 6.3% and 7.6% at a 5%false alarm rate on an English Broadcast News andMIT Lectures task respectively, when compared to abaseline system.
Furthermore, we have confirmedprevious work that hybrid systems achieve betterphone accuracy, and our model makes modest im-provements over a baseline with a similarly sizedsub-word lexicon.
We plan to further explore ournew lexicon?s performance for other languages andtasks, such as OOV spoken term detection.AcknowledgmentsWe gratefully acknowledge Bhuvaha Ramabhadranfor many insightful discussions and the anonymousreviewers for their helpful comments.
This workwas funded by a Google PhD Fellowship.720ReferencesIssam Bazzi and James Glass.
2001.
Learning unitsfor domain-independent out-of-vocabulary word mod-eling.
In EuroSpeech.Issam Bazzi.
2002.
Modelling out-of-vocabulary wordsfor robust speech recognition.
Ph.D. thesis, Mas-sachusetts Institute of Technology.M.
Bisani and H. Ney.
2005.
Open vocabulary speechrecognition with flat hybrid models.
In INTER-SPEECH.L.
Burget, P. Schwarz, P. Matejka, M. Hannemann,A.
Rastrow, C. White, S. Khudanpur, H. Hermansky,and J. Cernocky.
2008.
Combination of strongly andweakly constrained recognizers for reliable detectionof OOVS.
In ICASSP.D.
Can, E. Cooper, A. Sethy, M. Saraclar, and C. White.2009.
Effect of pronounciations on OOV queries inspoken term detection.
Proceedings of ICASSP.Stanley F. Chen.
2003.
Conditional and joint modelsfor grapheme-to-phoneme conversion.
In Eurospeech,pages 2033?2036.G.
Choueiter.
2009.
Linguistically-motivated sub-word modeling with applications to speech recogni-tion.
Ph.D. thesis, Massachusetts Institute of Technol-ogy.Jonathan Fiscus, John Garofolo, Mark Przybocki,William Fisher, and David Pallett, 1998.
1997 En-glish Broadcast News Speech (HUB4).
LinguisticData Consortium, Philadelphia.James Glass, Timothy Hazen, Lee Hetherington, andChao Wang.
2010.
Analysis and processing of lec-ture audio data: Preliminary investigations.
In NorthAmerican Chapter of the Association for Computa-tional Linguistics (NAACL).Dietrich Klakow, Georg Rose, and Xavier Aubert.
1999.OOV-detection in large vocabulary system using au-tomatically defined word-fragments as fillers.
In Eu-rospeech.Hui Lin, J. Bilmes, D. Vergyri, and K. Kirchhoff.
2007.OOV detection by joint word/phone lattice alignment.In ASRU, pages 478?483, Dec.Jonathan Mamou, Bhuvana Ramabhadran, and OlivierSiohan.
2007.
Vocabulary independent spoken termdetection.
In Proceedings of SIGIR.L.
Mangu, E. Brill, and A. Stolcke.
1999.
Finding con-sensus among words.
In Eurospeech.A.
Martin, G. Doddington, T. Kamm, M. Ordowski, andM.
Przybocky.
1997.
The det curve in assessment ofdetection task performance.
In Eurospeech.Carolina Parada, Abhinav Sethy, and Bhuvana Ramab-hadran.
2009.
Query-by-example spoken term detec-tion for oov terms.
In ASRU.Carolina Parada, Mark Dredze, Denis Filimonov, andFred Jelinek.
2010.
Contextual information improvesoov detection in speech.
In North American Chap-ter of the Association for Computational Linguistics(NAACL).H.
Poon, C. Cherry, and K. Toutanova.
2009.
Unsu-pervised morphological segmentation with log-linearmodels.
In ACL.Ariya Rastrow, Abhinav Sethy, and Bhuvana Ramab-hadran.
2009a.
A new method for OOV detectionusing hybrid word/fragment system.
Proceedings ofICASSP.Ariya Rastrow, Abhinav Sethy, Bhuvana Ramabhadran,and Fred Jelinek.
2009b.
Towards using hybrid,word, and fragment units for vocabulary independentLVCSR systems.
INTERSPEECH.T.
Schaaf.
2001.
Detection of OOV words using gen-eralized word models and a semantic class languagemodel.
In Eurospeech.H.
Soltau, B. Kingsbury, L. Mangu, D. Povey, G. Saon,and G. Zweig.
2005.
The ibm 2004 conversationaltelephony system for rich transcription.
In ICASSP.H.
Sun, G. Zhang, f. Zheng, and M. Xu.
2001.
Usingword confidence measure for OOV words detection ina spontaneous spoken dialog system.
In Eurospeech.Stanley Wang.
2009.
Using graphone models in au-tomatic speech recognition.
Master?s thesis, Mas-sachusetts Institute of Technology.F.
Wessel, R. Schluter, K. Macherey, and H. Ney.
2001.Confidence measures for large vocabulary continuousspeech recognition.
IEEE Transactions on Speech andAudio Processing, 9(3).Christopher White, Jasha Droppo, Alex Acero, and Ju-lian Odell.
2007.
Maximum entropy confidence esti-mation for speech recognition.
In ICASSP.721
