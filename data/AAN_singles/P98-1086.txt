Eliminative Parsing with Graded ConstraintsJohannes Heinecke and J i i rgen Kunze(heinecke I kunze@compling.hu-berlin.de )Lehrstuhl Computerlinguistik, Humboldt-Universit~t zu BerlinSchiitzenstraf~e 21, 10099 Berlin, GermanyWol fgang Menze l  and Ingo  Schr t ider(menzel I ingo.schroeder@informatik.uni-hamburg.de )Fachbereich Informatik,  Universit~t HamburgVogt-Kbl ln-Stra~e 30, 22527 Hamburg,  GermanyAbstract Resource adaptlvity" Because the sets of struc-Natural language parsing is conceived to be a pro-cedure of disambiguation, which successively re-duces an initially totally ambiguous structural rep-resentation towards a single interpretation.
Gradedconstraints are used as means to express well-formedness conditions of different strength and todecide which partial structures are locally least pre-ferred and, hence, can be deleted.
This approachfacilitates a higher degree of robustness of the ana-lysis, allows to introduce resource adaptivity into theparsing procedure, and exhibits a high potential forparallelization of the computation.1 In t roduct ionUsually parsing is understood as a constructive pro-cess, which builds structural descriptions out of ele-mentary building blocks.
Alternatively, parsing canbe considered a procedure of disambiguation whichstarts from a totally ambiguous tructural repre-sentation containing all possible interpretations ofa given input utterance.
A combinatorial explosionis avoided by keeping ambiguity strictly local.
Al-though particular eadings can be extracted fromthis structure at every time point during disam-biguation they are not maintained explicitly, and arenot immediately available.Ambiguity is reduced successively towards asingleinterpretation bydeleting locally least preferred par-tial structural descriptions from the set of solutions.This reductionistic behavior coins the term elimina-tire parsing.
The criteria which the deletion deci-sions are based on are formulated as compatibilityconstraints, thus parsing is considered a constraintsatisfaction problem (CSP).Eliminative parsing by itself shows some interest-ing advantages:Fail soft behavior: A rudimentary obustness canbe achieved by using procedures that leave thelast local possibility untouched.
More elabo-rated procedures taken from the field of partialconstraint satisfaction (PCSP) allow for evengreater obustness (cf.
Section 3).tural possibilities are maintained explicitly, theamount of disambiguation already done and theamount of the remaining effort are immediatelyavailable.
Therefore, eliminative approacheslend themselves tothe active control of the pro-cedures in order to fulfill external resource lim-itations.Parallelization: Eliminative parsing holds a highpotential for parallelization because ambiguityis represented locally and all decisions are basedon local information.Unfortunately even for sublanguages of fairlymodest size in many cases no complete disambigua-tion can be achieved (Harper et al, 1995).
This ismainly due to the crisp nature of classical constraintsthat do not allow to express the different strength ofgrammatical conditions: A constraint can only al-low or forbid a given structural configuration andall constraints are of equal importance.To overcome this disadvantage gradings can beadded to the constraints.
Grades indicate how seri-ous one considers a specific constraint violation andallow to express a range of different types of condi-tions including preferences, defaults, and strict re-strictions.
Parsing, then, is modelled as a partialconstraint satisfaction problem with scores (Tsang,1993) which can almost always be disambiguated to-wards a single solution if only the grammar providesenough evidence, which means that the CSP is over-constrained in the classical sense because at leastpreferential constraints are violated by the solution.We will give a more detailed introduction to con-straint parsing in Section 2 and to the extension tograded constraints in Section 3.
Section 4 presentsalgorithms for the solution of the previously definedparsing problem and the linguistic modeling for con-straint parsing is finally described in Section 5.2 Pars ing  as Const ra in t  Sat i s fac t ionWhile eliminative approaches are quite customaryfor part-of-speech disambiguation (Padr6, 1996) andunderspecified structural representations (Karlsson,5261990), it has hardly been used as a basis for fullstructural interpretation.
Maruyama (1990) de-scribes full parsing by means of constraint satisfac-tion for the first time.(a)0".
nilThe snake is chased by the cat.1 2 3 4 5 6 7vl = (nd, 2) v2 = (subj,3)(b) v3 = (nil, O) v4 = (ac,3)v5 = (pp, 4) v6 = (nd, 7)vT = (pc, 5)Figure 1: (a) Syntactic dependency tree for an ex-ample utterance: For each word form an unambigu-ous subordination and a label, which characterizesof subordination, are to be found.
(b) Labellings fora set of constraint variables: Each variable corre-sponds to a word form and takes a pairing consistingof a label and a word form as a value.Dependency relations are used to represent hestructural decomposition of natural anguage utter-ances (cf.
Figure la).
By not requiring the intro-duction of non-terminals, dependency structures al-low to determine the initial space of subordinationpossibilities in a straight forward manner.
All wordforms of the sentence can be regarded as constraintvariables and the possible values of these variablesdescribe the possible subordination relations of theword forms.
Initially, all pairings of a possible dom-inating word form and a label describing the kind ofrelation between dominating and dominated wordform are considered as potential value assignmentsfor a variable.
Disambiguation, then, reduces theset of values until finally a unique value has beenobtained for each variable.
Figure lb shows sucha final assignment which corresponds to the depen-dency tree in Figure la.
1Constraints like{X} : Subj : Agreement : X.label=subj -->X$cat=NOUN A XI"cat=VERB A XSnum=XTnumjudge the well-formedness of combinations of sub-ordination edges by considering the lexical prop-erties of the subordinated (XSnum) and the domi-nating (XTnum) word forms, the linear precedence1For illustration purposes, the position indices erve as ameans for the identification ofthe word forms.
A value (nil, O)is used to indicate the root of the dependency tree.
(XTpos) and the labels (X.label).
Therefore, theconditions are stated on structural representationsrather than on input strings directly.
For instance,the above constraint can be paraphrased as follows:Every subordination as a subject requires a noun tobe subordinated and a verb as the dominating wordform which have to agree with respect o number.An interesting property of the eliminative ap-proach is that it allows to treat unexpected inputwithout the necessity to provide an appropriate rulebeforehand: If constraints do not exclude a solutionexplicitly it will be accepted.
Therefore, defaults forunseen phenomena can be incorporated without ad-ditional effort.
Again there is an obvious contrast oconstructive methods which are not able to establisha structural description if a corresponding rule is notavailable.For computational reasons only unary and binaryconstraints are considered, i. e. constraints interre-late at most two dependency relations.
This, cer-tainly, is a rather strong restriction.
It puts severelimitations on the kind of conditions one wishes tomodel (cf.
Section 5 for examples).
As an interme-diate solution, templates for the approximation ofternary constraints have been developed.Harper et al (1994) extended constraint parsingto the analysis of word lattices instead of linear se-quences of words.
This provides not only a reason-able interface to state-of-the-art speech recognizersbut is also required to properly treat lexical ambi-guities.3 Graded Const ra in tsConstraint parsing introduced so far faces at leasttwo problems which are closely related to each otherand cannot easily be reconciled.
On the one hand,there is the difficulty to reduce the ambiguity to asingle interpretation.
In terms of CSP, the constraintparsing problem is said to have too small a tight-ness, i. e. there usually is more than one solution.Certainly, the remaining ambiguity can be furtherreduced by adding additional constraints.
This, onthe other hand, will most probably exclude otherconstructions from being handled properly, becausehighly restrictive constraint sets can easily rendera problem unsolvable and therefore introduce brit-tleness into the parsing procedure.
Whenever be-ing faced with such an overconstrained problem, theprocedure has to retract certain constraints in orderto avoid the deletion of indispensable subordinationpossibilities.Obviously, there is a trade-off between the cover-age of the grammar and the ability to perform thedisambiguation efficiently.
To overcome this prob-lem one wishes to specify exactly which constraintscan be relaxed in case a solution can not be estab-lished otherwise.
Therefore, different ypes of con-527straints are needed in order to express the differ-ent strength of strict conditions, default values, andpreferences.For this purpose every constraint c is annotatedwith a weight w(c) taken from the interval \[0, 1\]that denotes how seriously a violation of this con-straint effects the acceptability of an utterance (cf.Figure 2).
{X} : Subjlnit : Subj : 0.0 :X.label=subj -~ X$cat=NOUN A XJ'cat=VERB{X} : SubjNumber : Subj : 0.1 :X.label--subj -~ XJ.num--Xl"num{X} : SubjOrder : Subj : O.g :X.label--subj -~ XSpos<X'l'pos{X, Y}  : SubjUnique : Subj : 0.0 :X.label=subj A Xl"id--Y'l'id --+ Y.label:flsubjFigure 2: Very restrictive constraint grammar frag-ment for subject reatment in German: Graded con-straints are additionally annotated with a score.The solution of such a partial constraint satisfac-tion problem with scores is the dependency struc-ture of the utterance that violates the fewest and theweakest constraints.
For this purpose the notationof constraint weights is extended to scores for de-pendency structures.
The scores of all constraints cviolated by the structure under consideration s aremultiplied and a maximum selection is carried outto find the solution s' of the PCSP.s' = arg max H w(c)"Cc's)cSince a particular constraint can be violated morethan once by a given structure, the constraintgrade w(c) is raised to the power of n(c,s) whichdenotes the number of violations of the constraint cby the structure s.Different types of conditions can easily be ex-pressed with graded constraints:?
Hard constraints with a score of zero (e. g. con-straint SubjUnique) exclude totally unaccept-able structures from consideration.
This kindof constraints can also be used to initialize thespace of potential solutions (e. g. Subj lnit) .?
Typical well-formedness conditions like agree-ment or word order are specified by means ofweaker constraints with score larger than, butnear to zero, e. g. constraint SubjNumber.?
Weak constraints with score near to one canbe used for conditions that are merely prefer-ences rather than error conditions or that en-code uncertain information.
Some of the phe-nomena one wishes to express as preferencesconcern word order (in German, cf.
subject op-icalization of constraint SubjOrder),  defeasibleselectional restrictions, attachment preferences,attachment defaults (esp.
for partial parsing),mapping preferences, and frequency phenom-ena.
Uncertain information taken from prosodicclues, graded knowledge (e. g. measure of phys-ical proximity) or uncertain domain knowledgeis a typical example for the second type.Since a solution to a CSP with graded constraintsdoes not have to satisfy every single condition,overconstrained problems are no longer unsolvable.Moreover, by deliberately specifying a variety ofpreferences nearly all parsing problems indeed be-come overconstrained now, i. e. no solution fulfillsall constraints.
Therefore, disambiguation to a sin-gle interpretation (or at least a very small solutionset) comes out of the procedure without additionaleffort.
This is also true for utterances that are - -strictly speaking - -  grammatically ambiguous.
Aslong as there is any kind of preference ither fromlinguistic or extra-linguistic sources no enumerationof possible solutions will be generated.Note that this is exactly what is required in mostapplications because subsequent processing stagesusually need only one interpretation rather thanmany.
If under special circumstances more than oneinterpretation of an utterance is requested this kindof information can be provided by defining a thres-hold on the range of admissible scores.The capability to rate constraint violations en-ables the grammar writer to incorporate knowledgeof different kind (e. g. prosodic, syntactic, seman-tic, domain-specific clues) without depending on thegeneral validity of every single condition.
Instead,occasional violations can be accepted as long as aparticular source of knowledge supports the analysisprocess in the long term.Different representational levels can be establishedin order to model the relative autonomy of syntax,semantics, and even other contributions.
These mul-tiple levels must be related to each other by meansof mapping constraints o that evidence from onelevel helps to find a matching interpretation on an-other one.
Since these constraints are defeasible aswell, an inconsistency among different levels mustnot necessarily lead to an overall break down.In order to accommodate a number of represen-tational evels the constraint parsing approach hasto be modified again so that a separate constraintvariable is established for each level and each wordform.
A solution, then, does not consist of a singledependency tree but a whole set of trees.While constraint grades make it possible to weighup different violations of grammatical conditions therepresentation f different levels additionally allowsfor the arbitration among conflicting evidence origi-528nating from very different sources, e. g. among agree-ment conditions and selectional role filler restrictionsor word order regularities and prosodic hints.While constraints encoding specific domain knowl-edge have to be exchanged when one switches to an-other application context other constraint clusterslike syntax can be kept.
Consequently, the multi-level approach which makes the origin of differentdisambiguating information explicit holds great po-tential for reusability of knowledge.4 So lu t ion  methodsIn general, CSPs are NP-complete problems.
A lotof methods have been developed, though, to allowfor a reasonable complexity in most practical cases.Some heuristic methods, for instance, try to arriveat a solution more efficiently at the expense of giv-ing up the property of correctness, i. e. they find theglobally best solution in most cases while they arenot guaranteed to do so in all cases.
This allows toinfluence the temporal characteristics of the parsingprocedure, a possibility which seems especially im-portant in interactive applications: If the system hasto deliver a reasonable solution within a specific timeinterval a dynamic scheduling of computational re-sources depending on the remaining ambiguity andavailable time is necessary (Menzel, 1994, anytimealgorithm).
While different kinds of search are moresuitable with regard to the correctness property, lo-cal pruning strategies lend themselves to resourceadaptive procedures.
Menzel and SchrSder (1998b)give details about the decision procedures for con-straint parsing.5 Grammar  mode l ingFor experimental purposes a constraint grammarhas been set up, which consists of two descriptivelevels, one for syntactic (including morphology andagreement) and one for semantic relations.
Whereasthe syntactical description clearly follows a depen-dency approach, the second main level of our ana-lysis, semantics, is limited to sortal restrictions andpredicate-argument relations for verbs, predicativeadjectives, and predicative nouns.In order to illustrate the interaction of syntacticaland semantical constraints, the following (syntacti-cally correct) sentence is analyzed.
Here the use ofa semantic level excludes or depreciates a readingwhich violates lexical restrictions: Da habe ich einenTermin beim Zahnarzt ("At this time, I have an ap-pointment at the dentist's.")
The preposition beim("at the") is a locational preposition, the noun Zah-narzt ("dentist"), however, is of the sort "human".Thus, the constraint which determines sortal com-patibility for prepositions and nouns is violated:{X} : PrepSortal : Prepositions : 0.3 :XTcat----PREP X$cat---NOUN -~compatible(ont, Xl"sort, XSsort)'Prepositions should agree sortally with their noun.
'Other constraints control attachment preferences.For instance, the sentence am Montag machen witeinen Termin aus has two different readings ("wewill make an appointment, which will take place onMonday" vs. "oll Monday we will meet to make anappointment for another day"), i. e. the attachmentof the prepositional phrase am Montag can not bedetermined without a context.
If the first readingis preferred (the prepositional phrase is attached toausmachen), this can be achieved by a graded con-straint.
It can be overruled, if other features ruleout this possibility.A third possible use for weak constraints are at-tachment defaults, if e. g. a head word needs a cer-tain type of word as a dependent constituent.
When-ever the sentence being parsed does not provide therequired constituent, he weak constraint is violatedand another constituent takes over the function ofthe "missing" one (e. g. nominal use of adjectives).Prosodic information could also be dealt with.Compare Wit miissen noch einen Termin aus-machen ("We still have to make an appointment"vs. "We have to make a further appointment").
Astress on Termin would result in a preference ofthe first reading whereas a stressed noch makes thesecond translation more adequate.
Note that itshould always be possible to outdo weak evidencelike prosodic hints by rules of word order or eveninformation taken from the discourse, e. g. if thereis no previous appointment in the discourse.In addition to the two main description levels anumber of auxiliary ones is employed to circum-vent some shortcomings of the constraint-based ap-proach.
Recall that the CSP has been defined as touniquely assign a dominating node (together withan appropriate label) to each input form (cf.
Fig-ure 1).
Unfortunately, this definition restricts theapproach to a class of comparatively weak well-formedness conditions, namely subordination possi-bilities describing the degree to which a node canfill the valency of another one.
For instance, thepotential of a noun to serve as the grammatical sub-ject of the finite verb (cf.
Figure 2) belongs to thisclass of conditions.
If, on the other hand, the some-what stronger notion of a subordination ecessity(i. e. the requirement to fill a certain valency) isconsidered, an additional mechanism has to be in-troduced.
From a logical viewpoint, constraints ina CSP are universally quantified and do not pro-vide a natural way to accomodate conditions of ex-istence.
However, in the case of subordination e-cessities the effect of an existential quantifier caneasily be simulated by the unique value assignmentprinciple of the constraint satisfaction mechanism it-self.
For that purpose an additional representational529level for the inverse dependency relation is intro-duced for each valency to be saturated (Helzermanand Harper, 1992, cf.
needs-roles).
Dedicated con-straints ensure that the inverse relation can only beestablished if a suitable filler has properly been iden-tified in the input sentence.Another reason to introduce additional auxiliarylevels might be the desire to use a feature inheri-tance mechanism within the structural description.Basically, constraints allow only a passive featurechecking but do not support he active assignmentof feature values to particular nodes in the depen-dency tree.
Although this restriction must be con-sidered a fundamental prerequisite for the strictlylocal treatment ofhuge amounts of ambiguity, it cer-tainly makes an adequate modelling of feature per-colation phenomena r ther difficult.
Again, the useof auxiliary levels provides a solution by allowing totransport the required information along the edgesof the dependency tree by means of appropriately de-fined labels.
For efficiency reasons (the complexityis exponential in the number of features to percolateover the same edge) the application of this techniqueshould be restricted to a few carefully selected phe-nomena.The approach presented in this paper has beentested successfully on some 500 sentences of theVerbmobil domain (Wahlster, 1993).
Currently,there are about 210 semantic onstraints, includingconstraints on auxiliary levels.
The syntax is definedby 240 constraints.
Experiments with slightly dis-torted sentences resulted in correct structural treesin most cases.6 Conc lus ionAn approach to the parsing of dependency struc-tures has been presented, which is based on theelimination of partial structural interpretations bymeans of constraint satisfaction techniques.
Due tothe graded nature of constraints (possibly conflict-ing) evidence from a wide variety of informationalsources can be integrated into a uniform computa-tional mechanism.
A high degree of robustness iintroduced, which allows the parsing procedure tocompensate local constraint violations and to resortto at least partial interpretations if necessary.The approach already has been successfully ap-plied to a diagnosis task in foreign language l arningenvironments (Menzel and Schr5der, 1998a).
Fur-ther investigations are prepared to study the tem-poral characteristics of the procedure in more detail.A system is aimed at, which eventually will be ableto adapt its behavior to external pressure of time.AcknowledgementsThis research as been partly funded by the GermanResearch Foundation "Deutsche Forschungsgemein-schaft" under grant no.
Me 1472/1-1 & Ku 811/3-1.Re ferencesMary P. Harper, L. H. Jamieson, C. D. Mitchell,G.
Ying, S. Potisuk, P. N. Srinivasan, R. Chen,C.
B. Zoltowski, L. L. McPheters, B. Pellom,and R. A. Helzerman.
1994.
Integrating languagemodels with speech recognition.
In Proceedings ofthe AAAI-9~ Workshop on the Integration of Nat-ural Language and Speech Processing, pages 139-146.Mary P. Harper, Randall A. Helzermann, C. B.Zoltowski, B. L. Yeo, Y. Chan, T. Steward, andB.
L. Pellom.
1995.
Implementation issues in thedevelopment of the PARSEC parser.
Software -Practice and Experience, 25(8):831-862.Randall A. Helzerman and Mary P. Harper.
1992.Log time parsing on the MasPar MP-1.
In Pro-ceedings of the 6th International Conference onParallel Processing, pages 209-217.Fred Karlsson.
1990.
Constraint grammar as aframework for parsing running text.
In Proceed-ings of the 13th International Conference on Com-putational Linguistics, pages 168-173, Helsinki.Hiroshi Maruyama.
1990.
Structural disambigua-tion with constraint propagation.
In Proceedingsof the 28th Annual Meeting of the ACL, pages 31-38, Pittsburgh.Wolfgang Menzel and Ingo Schr5der.
1998a.Constraint-based diagnosis for intelligent lan-guage tutoring systems.
In Proceedings ofthe IT~KNOWS Conference at the IFIP '98Congress, Wien/Budapest.Wolfgang Menzel and Ingo SchrSder.
1998b.
De-cision procedures for dependency parsing usinggraded constraints.
In Proc.
of the Joint Con-ference COLING/ACL Workshop: Processing ofDependency-based Grammars, Montreal, CA.Wolfgang Menzel.
1994.
Parsing of spoken languageunder time constraints.
In A. Cohn, editor, Pro-ceedings of the 11th European Conference on Ar-tificial Intelligence, pages 560-564, Amsterdam.Lluis Padr6.
1996.
A constraint satisfaction alter-native to POS tagging.
In Proc.
NLP?IA, pages197-203, Moncton, Canada.E.
Tsang.
1993.
Foundations of Constraint Satisfac-tion.
Academic Press, Harcort Brace and Com-pany, London.Wolfgang Wahlster.
1993.
Verbmobil: Translationof face-to-face dialogs.
In Proceedings of theMachine Translation Summit IV, pages 127-135,Kobe.530
