hnproving Search StrategiesAn Exper iment in BesbFirst ParsingHans ItAUGENEI)ERManfl'ed GEHRKESiemens AGZT ZTI INF 23Otto-Hahn-Ring 68000 Mtinchen 83W.
GermunyAbstractViewing the syntactic analysis of natural anguage as asearch problem, the right choice of parsing strategyplays an important role in the performance of naturallanguage \[arsers.
After a motivatim: of the use ofvarious heuristic riteria, a fl'amework for defining andtesting par:;\[ng strategies i  presented.
On this basissystematic tests on different parsing strategies havebeen performed, the results of which are dicussed.Generally ;hese tests show that a "guided" depth-oriented strategy gives a considerable reduction ofsearch effort eompared to the classical depth.firststrategy.~.. IntroductionParsing natural anguage utterances can be considereda search t,roblem, which is characterized by theapplication of a set of operators (i.e.
the grammar rules)onto the input data (phrase to be processed) in order toyield a ~)nal state (derivation tree).
In practicalapplications which are characterized bygrammars witha large eow.,rage and a non-trlvial complexity of theinput {mea,';ured e.g.
in sentence length and lexiealambiguity) one is confronted with difficulties that seemquite common to various earch problems, namelythe size of the search space andthe selection among multiple solutions,Two (iuite opposite approaches to these problems havebeen proposed.
In tile one approach, the brute force ofexhaustive .,~earch as been used, possibly augmentedwith some ranking scheme tbr the set of parses.
In theother approach, the parsing of natural languageutterances is considered a deterministic process\[Mar80\], where a "wait and set.'"
strategy makes thetlavour of searching through the alternative applicationof different grammar rules disappear, at least forgrammars w{th limited coverage.The approach we are taking to this problem lies betweenthese two ext('emes: ConC'el)tually, it takes the firstview, considering natural language parsing anondeterministic process; fi'om a performance point ofview, it is :tirected towards the approximation ofdeterministic behaviour.
Thus our aim is to develop abest-first parsing strategy which enables tile parser bymeans of heuristic riteria and information tolimit the overall search space as much aspossible to arr ive at tile first parse at low costsachieve the most plausible analysis as thefirst one.With these aims in mind - at present mainlyconcentrating onthe first one - we still want to maintaintile ability of  our nleehanism to find t'm'~her solutions,since we do not assume the order of the analyses to becorrect all the time.
Thus "hem'ist ies" is understood asimproving the problem solving perfornlance withoutaffecting the competence \[Min63\].What we propose is a practically oriented approach tothese problems; it is practical in the sense that ourprimary focus is not to model the human sentenceprocessing meehanisn~ or specify the human parsingstrategy.
We are rather aiming towards thedevelopment of parsing strategies, that are based onheuristic information, enabling the parser to choose theright paths in the search space most of the time.Although psychological results on human sentenceprocessing strategies may be incorporated in theheuristics to be developed - at least as far as they fit inour h'amework and do not assume special properties ofthe underlying processing sehenle we do notunderstand our work as contributing to theeharoeterization of inherent structures of the humansentence processor.Thus our goal is not of an "all or nothing" character; wedo not expect our parser to make the right choice all thetime.
What we do want, however, is to develop a morepragmatic strategy, which, when applied to majorsamples of sentences, is able to give us the first readingwith a minimal overall search effort.After testing some strategies that give the parser moreguidance by increasing the information available at thechoice points, some promising results have emerged.Work in a similar direction on the MCC Lingo project\[Wit86\] also seems to dive some indication for this.2 )72, The Use of Heuristic information in ParsingIn a number of natural language parsers .-especially inthose with practical orientation and grammars withcomprehensive coverage - the problem of dealing withalternative parses has been handled by some sort ofscoring measures for sets of alternative parses alreadyproduced by breadth-first enumeration.This is the case in the DIAGRAM parser, wherearbitrary sub-procedures ( o~called factors) assignlikelihood scores to syntactic analyses \[Rob82\].
In theEPISTLE system, a numerical metric is used forranking multiple parses which is defined on the form ofthe phrase structure being built up \[ttei82\].
And as alast example for that type, the METAL parser performsa scoring of the analyses found, which is based on bothgrammatical and lexical phenomena \[Slo83\].
In allthese examples, the criteria on which the scoring isbased do not influence the parser's behaviour but act assome sort of filter on the parser's results.
The majorchallenge in our approach owever is the application ofsuch and similar scoring criteria on the fly during theparsing process instead of applying them after theparser has performed a blind all-paths analysis.If one thinks of more search intensive applications, likespeech understanding with the high degree ofambiguity in the input in the form of numerous wordhypotheses, the application of such heuristic criteriaduring the parsing process eems to have an even largeradvantage over the filter approach.3.
A Testbed for Modelling Parsing StrategiesIn order to be able to model heuristic parsing strategies,one needs a suitable parsing mechanism which hasenough flexibility tbr such a task.
The most obviouschoice for doing this is active chart parsing \[Kap73\],\[Kay80\] which is a highly general framework forconstructing parsers.
It combines the concept of anactive chart as an extensive bookkeeping mechanismpreventing the parser from performing two identicalprocessing steps twice, with an agenda-driven controlmechanism which enables a very elegant and highlymodularized simulation of different control structures.And it is exactly this second feature that is central forour strategy modelling task (for details ee \[Hau87\]).Since we view the development of a best-first parsingstratcgy as an empirical task, i.e.
as the result of goingthrough a number of define-test-modify cycles to buildup the "final" heuristics, it is necessary (or at leastuseful fi'om a practical point of view) to have availablean environment that enables the user to define andmodify the heuristic function easily and supports him inseeing and checking immediately without much effortthe effects of a modification.The APE system, in which this work is embedded, is anATN grammar development environment which(among other things) offers the functionality needed.
Bymeans of a highly interactive, graphically-oriented userinterface it offers operational facilities that give theuser a number of possibilities for inspecting anddebugging the parser's behaviour under a givenstrategy, as for example an agenda editor, thepossibility to specify strategies and change them duringparsing, and a chart-based fully graphical parserstepper.
An heuristics editor is integrated into APE'suser interface in a straightforward way: in addition tothe possibility of choosing between several predefineduniform and heuristic strategies, the user can define hisown strategies.
The specification of the intendedheuristic function is performed by giving appropriateweighting factors wfi to the various heuristicdimensions in a template-based manner.238After the specification of the values for the variousweighting factors, each expressing the relevance of thethe corresponding criterion, the user is presented withthe arithmetic expression associated with thecorresponding heuristic function (in standard infixnotation), which he can modify further if he finds thesystem defined composition of the weighted criteriaunsatisfactory.
This obviously can lead to modificationsof the heuristic function's range definition, theconsequences of which the user must be aware of whenusing this option (cf.
4.2).
Details of tim heuristicsspecification and manipulation facility are describedelsewhere (\[Hau87\], \[Geh88\]).Although the APE system is based en an ATNframework, the characteristics concerning heuristicinformation for scheduling are independent of theunderlying ATN approach; the only critical point is theassumption of an active chart parsing processingscheme.
Thus these considerations can be applied to anumber of otimr grammar formalisms as well,especially to those belonging to the paradigms of(procedurally and descriptively) augmented phrasestructure grammars.The implementation f the APE system and the workdescribed here has been performed in Interlisp-.D on aSiemens EMS 5822 workstation.4.
Defining Heuristic Strategies4.1 Factors Influencing Heuristic StrategiesThe criteria that can be employed in the specification ofan heuristic function though being of a widely differingnature can be divided into two classes.
Firstly there area number of "external" criteria, which arecharacterized statically in an a priori way.
Theseinclude:(1) Characterization of the plausibilities ofgrammatical rules(This gives the possibility to scale the grammarwith regard to the strength" of the constructions;thus one can divide the grammar into a core andperipheral part.
A quite similar criterion("syntactic preference") is used in \[For82\].
)(2) Different values assigned to the varioushomographic readings of words in the input(For a number of systematic but not equallydistributed homographic ambiguities, as forexamples the noun reading of certain verb forms,this offers an elegant way of supressing the"exotic" reading.
A similar focussing mechanismalso seems to be used during human sentenceprocessing, as indicated in \[Car81\].
)(3) Complexity of the structure of complete(sub)constituents, measured in terms of numberof nodes depth and mode of embedding(Thus grammatical but hardly acceptablestructures, like deep center embeddings forexample, can be "postponed".
)(4) Scoring of word hypotheses(Information of this type becomes relevant withspoken input.
)Besides these criteria there are others which reflectcertain aspects of the parsers internal state, as, e.g.
:(5) The weight of the partial analyse.% which is thevalue of the heuristic function associated with theactive edge characterizing this partial analysis(The overall plausibility of a certain partial parse.is characterized bythis weight.
)(13) The span of an inactive dge as ratio of the edges'span and the total length of the input(With other factors being equal an inactive edgewith a wider span, i.e.
a larger constituent will bepreferable, since this leads to a wider overallspan.
At tirst sight the criterion of length of acoastitueni, sounds a bit awkward, but e.g.
in\[Fca78\] its impact on phrasal attachment isshown.
)(7) The span of an active edge as ratio of the edges'span and the total length of the input(With other thctors being equal an active edgewith a widcr span, i.e.
a larger partial atmlysiswill be prethrable, since this leads to a wideroverall span)(8)'fbe mlmber of i~pnt items left fbrp eocessing, expressed aswhypleftwhyptotalwi~h hvmax being the maximal heuristic value(i.e.
1), whypl(,tt being the number of wordhypotheses left in the remaining input andwLypu)tal being the total number of wordhypotheses.
The applicability of this criterionfl~J'thermorc is coupled to some global threshholdthat defines the point in the input ti'om whichthis factor will be taken into account.
(This sort of information can be used to force thepacser to behave in a resource-oriented manner ifthere are only a few items left to process.
)All this (nibrmation is fhirly inexpensive to computeand making it available to the heuristic function duringthe parse ,:an be accomplished in a straightfbrward wayby attaching this information to the correspondingcomponen.
;s of a task (i.e.
the inactive edge, the activeedge and the grammar rule), tasks being thefundarnental unit in the processing cycle.Besides these more or less syntactic factors one can alsothink of integrating semantic riteria (as the possibilityof referm~tial interpretation of (noun) phrases orappropriate word sense disambiguation tbr example)directly as part of an heuristic strategy in ourfi'ameworl:.
~qlnce the application of the strategy takesplace at a very fine-grained level, where one reasonablymay not expect semantic "feedback" in the form of acorresponding heuristic value hv~,,m all the time (i.e.
ateach choice point), one has to cope with the problem ofhow to deel with an hv that is not defined.
If one adoptsthe conve~ttion that the effect of an hv which no valuehas been supplied for is totally excluded flora the overallheuristic fnnction, one achieves a plausible andattractive style of syntax-semantics interaction.
Thisoffers a good deal of flexibility, with the possibility ofinteraetimL at the word level as well as at the phraselevel without committing to either.4?2?
The Hcm'istic FunctionAssuming that the values tbr the various heuristiccriteria are in the interval \[0,1\], resulting in an overallheuristic measure in the same interval, i.e.
the heuristicfunction hi, has the form specified in (l), there is still thequestion of how these criteria interact, i.e.
how thevalues accumulate.hi'.'
\[0,1\] n =~ \[10,1\] with n being the numberof heuristic riteria(\[)r \ [~ ?
* , ?
, .
he mteractmn of these dttterent heurmtlc values hvj ishandled by a weighting factor wfj that is associated witheach heuristic dimension (such as e.g.
complexity of thestructure).
Tim weighting factor is intended to expressthe importance of the corresponding dimension and hasa range from 0 to 5, with 0 meaning that the dimensiondoes not play a role at all and 5 giving it maximalrelevance,Obviously, this weighting factor has no real qualitativeinterpretation; the only fact it expresses i the relevanceofa hem'istie dimension relative to the other ones.
Thus,fbr each heuristic riterion the actual value is computedby the product of the value of the heuristic dimensionand the corresponding weighting factor, i.e.
wfj * hvj.For the accumulation of the values of tile heuristiccriteria we have chosen the arithmetic mean, thushaving the overall heuristic value defined by formula(H).tbr all the hvis and E wfj * hvjcorresponding wfjs,Z w(i such that the valueofhvj is defined(n)5.
Results5.1 Scenario of the TestFor the interpretation of the presented results it seemsnecessary to specify the experimental conditions underwhich the tests have been performed.
The grammar wehave been using covers the tbllowing subset of' English:declarative sentences, imperative sentences, questions(direct and indirect y/n-questions, direct and indirectwh-questions for NPs, PPs, APs), sententialcomplements for verbs and nouns, complete and reducedrelative clauses, infinitive complements, clausalconjunction, and subordinate clauses.The test sample consisted of a set of 40 sentences andphrases that range from very simple phrases like "theman" to more complex constructions like "John givesthe girl Bill admires abook which he does not expect herto read".
The medium sentence length of the sample is6.5.
The homographic ambiguity factor is 1.3, i.e.
eachword processed is 1.3 times ambiguous on the average.5.2 Discussion of the ResultsWhen processing tile test sample under the variousstrategies, it turned out that there were many strategiesthat showed approximately the same overall behaviour,i.e.
demanded almost the identical search effort.Especially the variation of the weight for the singlefactors in general only shows effects when one contraststhe extreme values for the weighting factor (i.e.
0 and5).The quantitive measures of some selected heuristicfunctions that have been used in a one-path analysismode is shown in figure (IV).239The strategies we used are defined as explicated in (IiI),where AE means overall weight of the incoming activeedge, GR weight of the grammar arc, SIE span of theinactive dge, SP span of the active edge to be continuedwith the inactive edge, IL items left.
Besides theheuristic criteria, another important impact on theparsing strategy is the method o.r insertion of tasks intothe agenda.
It can take place in a local or in a globalmode.
While in the latter case (SortAll insertion mode) a.general and costly reordering of all tasks in the agendais performed, in the first case (SortNewToFrontinsertion mode) only the ordered set of newly generatedtasks is put onto the agenda in a stack-like fashion.insertion mode heuristic functionstratlstrat2strat3strat4SortNewToFront 5 * AE, 3 * SIE, 2 *GR, 5 * SP, 1 * ILSortNewToFront 5 * GR, 5 * AESortAll 5 * GR, 5 * AESortAll 5 * GR, 4 * SP, 4 * IL(IIDThe strategies we discuss here represent the two bestlocally operating ones (stratl, strat2), the best globalone (strat3) and the worst global one (strat4).
Theresults show among other things that the mostpromising strategy takes 59% of the search effort depth-first strategy uses.
Furthermore it can be seen thatwith respect to the two best strategies there is andecrease of the search effort on longer, more complexsentences ofthe sample down to 56% for each.~l'ate.~_g~ one-path: #tasks search effort ~_d_e~ 2765 (1218) lOO (100)stratl 1628 (690) 59 (56)strat2 1702 (685) 62 (56)strat3 2313 (1138) 84 (97)strat4 2830 (1363) 113 (112)The numbers enclosed in brackets are for thesubset of "long" sentences in the sample(with length > = 8).
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
o .
o  .
.
.
.
.
.
.
.
.
.
.
(iv)strate_g~ search effort (_%)depth-first 77stratl 45strat2 47strat3 64strat4 79all, paths: #tasks 3581 corresponding 100%.
.
.
.
.
.
.
.
.
.
, .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
a(v)240How this relates to the overall search space isdocumented in (V).
These numbers correlate the overallsize of the search space to the part of it that has beentraversed by the different strategies.Though we are being far from beleiving that the beststrategy we have worked with is the strategy, there aresome general guidelines.
Thus if we try to reflect on theresults presented here and the material which has beenanalyzed uring our test, the following picture emerges:(1) Static weights on the grammar rnles are ~,seihl.
(2) Span-orientation (i.e.
the tendency to furtherfollow the parse that yields the biggest overallspan) has shown rather drastic positive ffhcts.
(3) Resource-orientation (i.e.
the tendency tocontinue tasks that have almost reached the endof the input with additional emphasis) gives someminor additional improvements.
(4) Local application of the heuristics (i.e.
aheuristically guided depth-first strategy, whichcorresponds to the insertion modeSortNewToFront) instead of a global reorderingof all the pending tasks in the agenda is muchmore effective with the additional advantage ofbeing much less costly.
(5) Finally our experiment has shown that thesimultaneous use of several criteria togetherleads to a reduction of the search effort ascompared to each single's criterion effect.Although the various criteria can locally conflictwith each other in certain configurations, theircmnulative overall effect is stronger than local"disturbances".The basically stack-oriented way of updating theagenda - which leads to an "informed" depth-firststrategy - makes our approach also compatible withmodels that determine the scheduling of the parser'soperation with respect to certain phenomena on thebasis of purely linguistically oriented principles, as thetreatment of syntactic losure in \[For82\].
As long assuch models still retain a certain amount ofnondeterminism with depth-first as a defaultscheduling principle, a guided depth-first strategy ofthe type discussed here may be favourable to an"uninformed" epth-first strategy.6?
Conclusion and OutlookThe results that have come out of our experiments seemto indicate - though only a subset of the potentialcriteria has been taken into account systematically -that heuristics of the type presented can be appliedfruitfully.
We see the extension of the coverage of thegrammar as well as the enlargement of the test sampleas a logical continuation to confirm our results.Beyond that we will apply a heuristic approach similarto the one presented here to spoken input, where thecomplexity is far beyond typed input due to theexistence of a large number of word hypotheses (about5000 for a 6 word sentence on average); thereby the datafor the latter work are provided by the SPICOScontinuous speech understanding project \[Dre87\].\[Car87\]\[Dre87\]\[For82\]\[Fra78\]\[Geh88\]\[Hau86\]\[Hau87\]\[Hei82\]\[Kap73\]\[KayS0\]\[MarS0\]\[Min63\]\[Rob82\]\[Slo83\]\[Wit86\]ReferencesCarpenter, P.A., and Daneman, M., "LexicalRetrieval and Error Recovery in Reading: AModel Based on Eye Fixations".
Journal ofVerbal Learning and Verbal Behaviour Vol.20 No.
2 (1981), 137-160Dreekschmidt, Gaby, "I'he LinguisticComponent in the Speech UnderstandingSystem SPICOS".
In: H. G. Tilhnann, G.Wille~ (ed),  Analyse und Synthesegesprochener Sprache, Hildesheim-Ziirich-New York, Olms Verlag 1987Ford, M., Bresnan, J.W., Kaplan, R.M., "ACompetence Based Theory of SyntacticClosure".
In: Bresnan, J.W.
(ed): 'The MentalRepresentation of Grammatical Relations",Cambridge/Mass, The MIT Press, 1982, 727-796Frazier, L., Fodor, J.D., "The SausageMachine: A New Two-Stage Parsing Model".In: Cognition 6 (1878), 291-325Gehrke, M., tIaugeneder, H., "APE UserManual".
Siemens Report, to appear.Haugeneder, H., Gehrke, M., "A UserFriendly ATN Programming Environment(APE)".
In: Proc.
COLING-86, 399-401.Haugeneder, H., Gehrke, M., "ModellingHeuristic Parsing Strategies".
In: K.
Morik(ed.
), GWAI-87 - l lth German Workshop onArtificial Intelligence, Berlin-Heidelberg-New York, Springer-Verlag, 1987, 84-93Heidorn, G.E., "Experience with an EasilyComputed Metric for Ranking AlternativeParses".
In: Proc.
ACL-82, 82-84.Kaplan, R.M., "A General SyntacticProcessor".
In: Rustin, R. (ed), "NaturalLanguage Processing", New York,Algorithmics Press 1973, 193~241.
:Kay, M., "Algorithm Schemata and DataStructures in Syntactic Processing".
XeroxPARC Tech.
Report No.
CSL-80-12, 1980.Marcus, M., "A Theory of SyntacticRecognition for Natural Language".Cambridge/Mass., The MIT Press 1980.Minsky, M., "Steps Towards Artificial\]:ntelligence".
In: Feigenbaum, E. A. andFeldman, J.
(eds), "Computers and Thought",New York, McGraw-Hill 1963, 406-450.Robinson, J., "DIAGRAM: A Grammar forDialogues".
CACM Vol.
25 No.
1 (1982), 27-47.llloeum, J., "A Status Report on the LRClVIachine Translation System".
In: Proc.Conference on Applied Natural La~.lguageProcessing 1983, 166-173.Wittenburg, K., "A Parser for Portable NLInterfaces Using Graph-Unification-BasedGrammars'.
In Proc.
AAAI-86, 1053-1058.241
