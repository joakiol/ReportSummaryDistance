Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 747?754,Sydney, July 2006. c?2006 Association for Computational LinguisticsAdding Syntax to Dynamic Programming for Aligning Comparable Textsfor the Generation of ParaphrasesSiwei Shen1, Dragomir R. Radev1;2, Agam Patel1, Gu?nes?
Erkan1Department of Electrical Engineering and Computer ScienceSchool of InformationUniversity of MichiganAnn Arbor, MI 48109fshens, radev, agamrp, gerkang@umich.eduAbstractMultiple sequence alignment techniqueshave recently gained popularity in the Nat-ural Language community, especially fortasks such as machine translation, textgeneration, and paraphrase identification.Prior work falls into two categories, de-pending on the type of input used: (a)parallel corpora (e.g., multiple translationsof the same text) or (b) comparable texts(non-parallel but on the same topic).
Sofar, only techniques based on parallel textshave successfully used syntactic informa-tion to guide alignments.
In this paper,we describe an algorithm for incorporat-ing syntactic features in the alignment pro-cess for non-parallel texts with the goal ofgenerating novel paraphrases of existingtexts.
Our method uses dynamic program-ming with alignment decision based onthe local syntactic similarity between twosentences.
Our results show that syntac-tic alignment outrivals syntax-free meth-ods by 20% in both grammaticality and fi-delity when computed over the novel sen-tences generated by alignment-induced fi-nite state automata.1 IntroductionIn real life, we often encounter comparable textssuch as news on the same events reported by dif-ferent sources and papers on the same topic au-thored by different people.
It is useful to recog-nize if one text cites another in cases like newssharing among media agencies or citations in aca-demic work.
Applications of such recognition in-clude machine translation, text generation, para-phrase identification, and question answering, allof which have recently drawn the attention of anumber of researchers in natural language pro-cessing community.Multiple sequence alignment (MSA) is the ba-sis for accomplishing these tasks.
Previous workaligns a group of sentences into a compact wordlattice (Barzilay and Lee, 2003), a finite state au-tomaton representation that can be used to iden-tify commonality or variability among compara-ble texts and generate paraphrases.
Nevertheless,this approach has a drawback of over-generatingungrammatical sentences due to its ?almost-free?alignment.
Pang et al provide a remedy to thisproblem by performing alignment on the Charniakparse trees of the clustered sentences (Pang et al,2003).
Although it is so far the most similar workto ours, Pang?s solution assumes the input sen-tences to be semantically equivalent.
Two otherimportant references for string-based alignmentsalgorithms, mostly with applications in Biology,are (Gusfield, 1997) and (Durbin et al, 1998).In our approach, we work on comparable texts(not necessarily equivalent in their semantic mean-ings) as Barzilay and Lee did.
However, we use lo-cal syntactic similarity (as opposed to lexical simi-larity) in doing the alignment on the raw sentencesinstead of on their parse trees.
Because of the se-mantic discrepancies among the inputs, applyingsyntactic features in the alignment has a larger im-pact on the grammaticality and fidelity of the gen-erated unseen sentences.
While previous work po-sitions the primary focus on the quality of para-phrases and/or translations, we are more interestedin the relation between the use of syntactic fea-tures and the correctness of the sentences beinggenerated, including those that are not paraphrasesof the original input.
Figure 1 illustrates the dif-ference between alignment based solely on lexi-cal similarity and alignment with consideration ofsyntactic features.Ignoring syntax, the word ?Milan?
in both sen-tences is aligned.
But it would unfortunately gen-erate an ungrammatical sentence ?I went to Mi-lan is beautiful?.
Aligning according to syntac-747StartIIMilanMilanwentwentisisAcceptAccepttotoMilanbeautifulbeautiful AcceptStartIIMilanMilanwentwentisistoto MilanMilanAcceptAcceptbeautifulbeautifulAcceptFigure 1: Alignment on lexical similarity and alignment with syntactic features of the sentences ?Milanis beautiful?
and ?I went to Milan?.tic features, on the other hand, would avoid thisimproper alignment by detecting that the syntacticfeature values of the two ?Milan?
differ too much.We shall explain syntactic features and their us-ages later.
In this small example, our syntax-basedalignment will align nothing (the bottom FSA inFigure 1) since ?Milan?
is the only lexically com-mon word in both sentences.
For much largerclusters in our experiments, we are able to pro-duce a significant number of novel sentences fromour alignment with such tightened syntactic con-ditions.
Figure 2 shows one of the actual clustersused in our work that has 18 unique sentences.Two of the many automatically generated gram-matical sentences are also shown.Another piece of related work, (Quirk et al,2004), starts off with parallel inputs and usesmonolingual Statistical Machine Translation tech-niques to align them and generate novel sentences.In our work, the input text does not need to benearly as parallel.The main contribution of this paper is a syntax-based alignment technique for generating novelparaphrases of sentences that describe a par-ticular fact.
Such techniques can be poten-tially useful in multi-document summarizers suchas Newsblaster (http://newsblaster.cs.columbia.edu) and NewsInEssence (http://www.newsinessence.com).
Such sys-tems are notorious for mostly reusing text fromexisting news stories.
We believe that allowingthem to use novel formulations of known facts willmake these systems much more successful.2 Related workOur work is closest in spirit to the two papers thatinspired us (Barzilay and Lee, 2003) and (Panget al, 2003).
Both of these papers describe howmultiple sequence alignment can be used for ex-tracting paraphrases from clustered texts.
Pang etal.
use as their input the multiple human Englishtranslations of Chinese documents provided by theLDC as part of the NIST machine translation eval-uation.
Their approach is to merge multiple parsetrees into a single finite state automaton in whichidentical input subconstituents are merged whilealternatives are converted to parallel paths in theoutput FSA.
Barzilay and Lee, on the other hand,make use of classic techniques in biological se-quence analysis to identify paraphrases from com-parable texts (news from different sources on thesame event).In summary, Pang et al use syntactic align-ment of parallel texts while Barzilay and Leeuse comparable (not parallel) input but ignoresyntax.
Our work differs from the two in thatwe apply syntactic information on aligning com-parable texts and that the syntactic clues weuse are drawn from Chunklink ilk.uvt.nl/?sabine/homepage/software.html out-put, which is further analysis from the syntacticparse trees.Another related paper using multiple sequencealignment for text generation was (Barzilay andLee, 2002).
In that work, the authors were ableto automatically acquire different lexicalizationsof the same concept from ?multiple-parallel cor-pora?.
We also draw some ideas from the Fitch-Margoliash method for building evolutionary trees7481.
A police official said it was a Piper tourist plane and that the crash had set the top floors on fire.2.
According to ABCNEWS aviation expert John Nance, Piper planes have no history of mechanical troubles orother problems that would lead a pilot to lose control.3.
April 18, 2002 8212; A small Piper aircraft crashes into the 417-foot-tall Pirelli skyscraper in Milan,setting the top floors of the 32-story building on fire.4.
Authorities said the pilot of a small Piper plane called in a problem with the landing gear to the Milan?sLinate airport at 5:54 p.m., the smaller airport that has a landing strip for private planes.5.
Initial reports described the plane as a Piper, but did not note the specific model.6.
Italian rescue officials reported that at least two people were killed after the Piper aircraft struck the32-story Pirelli building, which is in the heart of the city s financial district.7.
MILAN, Italy AP A small piper plane with only the pilot on board crashed Thursday into a 30-story landmarkskyscraper, killing at least two people and injuring at least 30.8.
Police officer Celerissimo De Simone said the pilot of the Piper Air Commander plane had sent out adistress call at 5:50 p.m. just before the crash near Milan?s main train station.9.
Police officer Celerissimo De Simone said the pilot of the Piper aircraft had sent out a distress call at5:50 p.m. 11:50 a.m.10.
Police officer Celerissimo De Simone said the pilot of the Piper aircraft had sent out a distresscall at 5:50 p.m. just before the crash near Milan?s main train station.11.
Police officer Celerissimo De Simone said the pilot of the Piper aircraft sent out a distress call at5:50 p.m. just before the crash near Milan?s main train station.12.
Police officer Celerissimo De Simone told The AP the pilot of the Piper aircraft had sent out a distresscall at 5:50 p.m. just before crashing.13.
Police say the aircraft was a Piper tourism plane with only the pilot on board.14.
Police say the plane was an Air Commando 8212; a small plane similar to a Piper.15.
Rescue officials said that at least three people were killed, including the pilot, while dozens wereinjured after the Piper aircraft struck the Pirelli high-rise in the heart of the city s financialdistrict.16.
The crash by the Piper tourist plane into the 26th floor occurred at 5:50 p.m. 1450 GMT on Thursday, saidjournalist Desideria Cavina.17.
The pilot of the Piper aircraft, en route from Switzerland, sent out a distress call at 5:54 p.m. justbefore the crash, said police officer Celerissimo De Simone.18.
There were conflicting reports as to whether it was a terrorist attack or an accident after the pilot ofthe Piper tourist plane reported that he had lost control.1.
Police officer Celerissimo De Simone said the pilot of the Piper aircraft, en route from Switzerland, sentout a distress call at 5:54 p.m. just before the crash near Milan?s main train station.2.
Italian rescue officials reported that at least three people were killed, including the pilot, whiledozens were injured after the Piper aircraft struck the 32-story Pirelli building, which is in the heartof the city s financial district.Figure 2: A comparable cluster of size 18 and 2 novel sentences produced by syntax-based alignment.described in (Fitch and Margoliash, 1967).
Thatmethod and related techniques in Bioinformaticssuch as (Felsenstein, 1995) also make use of a sim-ilarity matrix for aligning a number of sequences.3 Alignment AlgorithmsOur alignment algorithm can be described as mod-ifying Levenshtein Edit Distance by assigning dif-ferent scores to lexically matched words accordingto their syntactic similarity.
And the decision ofwhether to align a pair of words is based on suchsyntax scores.3.1 Modified Levenshtein Edit DistanceThe Levenshtein Edit Distance (LED) is a mea-sure of similarity between two strings named afterthe Russian scientist Vladimir Levenshtein, whodevised the algorithm in 1965.
It is the num-ber of substitutions, deletions or insertions (hence?edits?)
needed to transform one string into theother.
We extend LED to sentence level by count-ing the substitutions, deletions and insertions ofwords necessary to transform a sentence into theother.
We abbreviate this sentence-level edit dis-tance as MLED.
Similar to LED, MLED compu-tation produces an M+1 by N+1 distance matrix,D, given two input sentences of length M and Nrespectively.
This matrix is constructed throughdynamic programming as shown in Figure 3.D[i][j] =8>><>>:0 if j = 00 if i = 0maxD[i   1][j   1] + match;D[i   1][j] + gap;D[i][j   1] + gap!otherwiseFigure 3: Dynamic programming in computingMLED of two sentences of length M and N.?match?
is 2 if the ith word in Sentence 1 andthe jth word in Sentence 2 syntactically match,and is -1 otherwise.
?gap?
represents the scorefor inserting a gap rather than aligning, and is setto -1.
The matching conditions of two words arefar more complicated than lexical equality.
Rather,we judge whether two lexically equal words matchbased on a predefined set of syntactic features.The output matrix is used to guide the align-ment.
Starting from the bottom right entry of thematrix, we go to the matrix entry from which thevalue of the current cell is derived in the recursionof the dynamic programming.
Call the current en-try D[i][j].
If it gets its value from D[i 1][j 1],the ith word in Sentence 1 and the jth word in Sen-tence 2 are either aligned or both aligned to a gapdepending on whether they syntactically match; ifthe value of D[i][j] is derived from D[i][j   1] +749?gap?, the ith word in Sentence 1 is aligned to agap inserted into Sentence 2 (the jth word in Sen-tence 2 is not consumed); otherwise, the jth wordin Sentence 2 is aligned to a gap inserted into Sen-tence 1.Now that we know how to align two sentences,aligning a cluster of sentences is done progres-sively.
We start with the overall most similar pairand then respect the initial ordering of the cluster,aligning remaining sentences sequentially.
Eachsentence is aligned against its best match in thepool of already-aligned ones.
This approach isa hybrid of the Feng-Doolittle?s Algorithm (Fengand Doolittle, 1987) and a variant described in(Fitch and Margoliash, 1967).3.2 Syntax-based AlignmentAs remarked earlier, our alignment scheme judgeswhether two words match according to theirsyntactic similarity on top of lexical equality.The syntactic features are obtained from run-ning Chunklink (Buchholz, 2000) on the Charniakparses of the clustered sentences.3.2.1 Syntactic FeaturesAmong all the information Chunklink provides,we use in particular the part-of-speech tags, theChunk tags, and the syntactic dependence traces.The Chunk tag shows the constituent of a wordand its relative position in that constituent.
It cantake one of the three values, ?O?
meaning that the word is outside of anychunk; ?I-XP?
meaning that this word is inside anXP chunk where X = N, V, P, ADV, ...; ?B-XP?
meaning that the word is at the be-ginning of an XP chunk.From now on, we shall refer to the Chunktag of a word as its IOB value (IOB was namedby Tjong Kim Sang and Jorn Veeenstra (TjongKim Sang and Veenstra, 1999) after Ratnaparkhi(Ratnaparkhi, 1998)).
For example, in the sen-tence ?I visited Milan Theater?, the IOB value for?I?
is B-NP since it marks the beginning of a noun-phrase (NP).
On the other hand, ?Theater?
has anIOB value of I-NP because it is inside a noun-phrase (Milan Theater) and is not at the beginningof that constituent.
Finally, the syntactic depen-dence trace of a word is the path of IOB valuesfrom the root of the tree to the word itself.
Thelast element in the trace is hence the IOB of theword itself.3.2.2 The AlgorithmLexically matched words but with differentPOS are considered not syntactically matched(e.g., race VB vs. race NN).
Hence, our focusis really on pairs of lexically matched words withthe same POS.
We first compare their IOB values.Two IOB values are exactly matched only if theyare identical (same constituent and same position);they are partially matched if they share a commonconstituent but have different position (e.g., B-PPvs.
I-PP); and they are unmatched otherwise.
Fora pair of words with exactly matched IOB values,we assign 1 as their IOB-score; for those with par-tially matched IOB values, 0; and -1 for those withunmatched IOB values.
The numeric values of thescore are from experimental experience.The next step is to compare syntactic depen-dence traces of the two words.
We start with thesecond last element in the traces and go backwardbecause the last one is already taken care of by theprevious step.
We also discard the front element ofboth traces since it is ?I-S?
for all words.
The cor-responding elements in the two traces are checkedby the IOB-comparison described above and thescores accumulated.
The process terminates assoon as one of the two traces is exhausted.
Last,we adjust down the cumulative score by the lengthdifference between the two traces.
Such final scoreis named the trace-score of the two words.We declare ?unmatched?
if the sum of the IOB-score and the trace-score falls below 0.
Otherwise,we perform one last measurement ?
the relativeposition of the two words in their respective sen-tences.
The relative position is defined to be theword?s absolute position divided by the length ofthe sentence it appears in (e.g.
the 4th word of a20-word sentence has a relative position of 0.2).If the difference between two relative positionsis larger than 0.4 (empirically chosen before run-ning the experiments), we consider the two words?unmatched?.
Otherwise, they are syntacticallymatched.The pseudo-code of checking syntactic match isshown in Figure 4.750Algorithm Check Syntactic Match of Two WordsFor a pair of words W1, W2if W16= W2or pos(W1) 6= pos(W2) thenreturn ?unmatched?endifscore := 0iob1:= iob(W1)iob2:= iob(W2)score += compare iobs(iob1; iob2)trace1:= trace(W1)trace2:= trace(W2)score += compare traces(trace1; trace2)if score < 0 thenreturn ?unmatched?endifrelpos1:= pos(W1)/lengthOf(S1)relpos2:= pos(W2)/lengthOf(S2)if jrelpos1  relpos2j  0:4 thenreturn ?unmatched?endifreturn ?matched?Function compare iobs(iob1; iob2)if iob1= iob2thenreturn 1endifif substring(iob1; 1) = substring(iob2; 1) thenreturn 0endifreturn  1Function compare traces(trace1; trace2)Remove first and last elements from both tracesscore := 0i := lengthOf(trace1)   1j := lengthOf(trace2)  1while i  0 and j  0 donext := compare iobs(trace1[i]; trace2[j])score += next  0:5i   j   endwhilescore   = jlengthOf(trace1)  lengthOf(trace2)j  0:5return scoreFigure 4: Algorithm for checking the syntacticmatch between two words.4 Evaluation4.1 Experimental Setup4.1.1 DataThe data we use in our experiment come froma number of sentence clusters on a variety of top-ics, but all related to the Milan plane crash event.This cluster was collected manually from the Webof five different news agencies (ABC, CNN, Fox,MSNBC, and USAToday).
It concerns the April2002 crash of a small plane into a building in Mi-lan, Italy and contains a total of 56 documentspublished over a period of 1.5 days.
To divide thiscorpus into representative smaller clusters, we hada colleague thoroughly read all 56 documents inthe cluster and then create a list of important factssurrounding the story.
We then picked key termsrelated to these facts, such as names (Fasulo - thepilot) and locations (Locarno - the city from whichthe plane had departed).
Finally, we automaticallyclustered sentences based on the presence of thesekey terms, resulting in 21 clusters of topically re-lated (comparable) sentences.
The 21 clusters aregrouped into three categories: 7 in training set, 3in dev-testing set, and the remaining 11 in testingset.
Table 1 shows the name and size of each clus-ter.Cluster Number of SentencesTraining clustersambulance 10belie 14built 6malpensa 4piper 18president 17route 11Dev-test clustershospital 17rescue 12witness 6Test clustersaccident 30cause 18fasulo 33floor 79government 22injur 43linate 21rockwell 9spokes 18suicide 22terror 62Table 1: Experimental clusters.7514.1.2 Different Versions of AlignmentTo test the usefulness of our work, we ran 5 dif-ferent alignments on the clusters.
The first threerepresent different levels of baseline performance(without syntax consideration) whereas the lasttwo fully employ the syntactic features but treatstop words differently.
Table 2 describes the 5 ver-sions of alignment.Run DescriptionV1 Lexical alignment on everything possibleV2 Lexical alignment on everything but commasV3 Lexical alignment on everything but commas and stop wordsV4 Syntactic alignment on everything but commas and stop wordsV5 Syntactic alignment on everything but commasTable 2: Alignment techniques used in the experi-ments.Alignment Grammaticality FidelityV1 2.89 2.98V2 3.00 2.95V3 3.15 3.22V4 3.68 3.59V5 3.47 3.30Table 3: Evaluation results on training and dev-testing clusters.
For the results on the test clusters,see Table 6The motivation of trying such variations is asfollows.
Stop words often cause invalid alignmentbecause of their high frequencies, and so do punc-tuations.
Aligning on commas, in particular, islikely to produce long sentences that contain mul-tiple sentence segments ungrammatically patchedtogether.4.1.3 Training and TestingIn order to get the best possible performanceof the syntactic alignment versions, we use clus-ters in the training and dev-test sets to tune upthe parameter values in our algorithm for check-ing syntactic match.
The parameters in our algo-rithm are not independent.
We pay special atten-tion to the threshold of relative position difference,the discount factor of the trace length differencepenalty, and the scores for exactly matched andpartially matched IOB values.
We try different pa-rameter settings on the training clusters, and applythe top ranking combinations (according to humanjudgments described later) on clusters in the dev-testing set.
The values presented in this paper arethe manually selected ones that yield the best per-formance on the training and dev-testing sets.Experimenting on the testing data, we havetwo hypotheses to verify: 1) the 2 syntactic ver-sions outperform the 3 baseline versions by bothgrammaticality and fidelity (discussed later) of thenovel sentences produced by alignment; and 2)disallowing alignment on stop words and commasenhances the performance.4.2 Experimental ResultsFor each cluster, we ran the 5 alignment versionsand produce 5 FSA?s.
From each FSA (corre-sponding to a cluster A and alignment version i),100 sentences are randomly generated.
We re-moved those that appear in the original cluster.The remaining ones are hence novel sentences,among which we randomly chose 10 to test theperformance of alignment version i on cluster A.In the human evaluation, each sentence receivedtwo scores ?
grammaticality and fidelity.
Thesetwo properties are independent since a sentencecould possibly score high on fidelity even if it isnot fully grammatical.
Four different scores arepossible for both criteria: (4) perfect (fully gram-matical or faithful); (3) good (occasional errors orquite faithful); (2) bad (many grammar errors orunfaithful pieces); and (1) nonsense.4.2.1 Results from the Training PhaseFour judges help our evaluation in the trainingphase.
They are provided with the original clustersduring the evaluation process, yet they are giventhe sentences in shuffled order so that they haveno knowledge about from which alignment ver-sion each sentence is generated.
Table 3 showsthe averages of their evaluation on the 10 clustersin training and dev-testing set.
Each cell corre-sponds to 400 data points as we presented 10 sen-tences per cluster per alignment version to each ofthe 4 judges (10 x 10 x 4 = 400).4.2.2 Results from the Testing PhaseAfter we have optimized the parameter config-uration for our syntactic alignment in the trainingphase, we ask another 6 human judges to evaluateour work on the testing data.
These 6 judges comefrom diverse background including Information,Computer Science, Linguistics, and Bioinformat-ics.
We distribute the 11 testing clusters amongthem so that each cluster gets evaluated by at least3 judges.
The workload for each judge is 6 clus-ters x 5 versions/cluster x 10 sentences/cluster-version = 300 sentences.
Similar to the trainingphase, they receive the sentences in shuffled or-der without knowing the correspondence between752sentences and alignment versions.
Detailed aver-age statistics are shown in Table 4 and Table 5 forgrammaticality and fidelity, respectively.
Each cellis the average over 30 - 40 data points, and noticethe last row is not the mean of the other rows sincethe number of sentences evaluated for each clustervaries.Cluster V1 V2 V3 V4 V5rockwell 2.27 2.93 3.00 3.60 3.03cause 2.77 2.83 3.07 3.10 2.93spokes 2.87 3.07 3.57 3.83 3.50linate 2.93 3.14 3.26 3.64 3.77government 2.75 2.83 3.27 3.80 3.20suicide 2.19 2.51 3.29 3.57 3.11accident 2.92 3.27 3.54 3.72 3.56fasulo 2.52 2.52 3.15 3.54 3.32injur 2.29 2.92 3.03 3.62 3.29terror 3.04 3.11 3.61 3.23 3.63floor 2.47 2.77 3.40 3.47 3.27Overall 2.74 2.75 3.12 3.74 3.29Table 4: Average grammaticality scores on testingclusters.Cluster V1 V2 V3 V4 V5rockwell 2.25 2.75 3.20 3.80 2.70cause 2.42 3.04 2.92 3.48 3.17spokes 2.65 2.50 3.20 3.00 3.05linate 3.15 3.27 3.15 3.36 3.42government 2.85 3.24 3.14 3.81 3.20suicide 2.38 2.69 2.93 3.68 3.23accident 3.14 3.42 3.56 3.91 3.57fasulo 2.30 2.48 3.14 3.50 3.48injur 2.56 2.28 2.29 3.18 3.22terror 2.65 2.48 3.68 3.47 3.20floor 2.80 2.90 3.10 3.70 3.30Overall 2.67 2.69 3.07 3.77 3.23Table 5: Average fidelity scores on testing clusters.2.002.202.402.602.803.003.203.403.603.804.00rockwellcausespokeslinategovernmentsuicideaccidentfasulo injurterrorfloorV 1V 2V 3V 4V 5Figure 5: Performance of 5 alignment versions bygrammaticality.2.002.202.402.602.803.003.203.403.603.804.00rockwellcausespokeslinategovernmentsuicideaccidentfasulo injurterrorfloorV 1V 2V 3V 4V 5Figure 6: Performance of 5 alignment versions byfidelity.4.3 Result AnalysisThe results support both our hypotheses.
For Hy-pothesis I, we see that the performance of thetwo syntactic alignments was higher than the non-syntactic versions.
In particular, Version 4 outper-forms the the best baseline version by 19.9% ongrammaticality and by 22.8% on fidelity.
Our sec-ond hypothesis is also verified ?
disallowing align-ment on stop words and commas yields better re-sults.
This is reflected by the fact that Version 4beats Version 5, and Version 3 wins over the othertwo baseline versions by both criteria.At the level of individual clusters, the syntacticversions are also found to outrival the syntax-blindbaselines.
Applying a t-test on the score sets forthe 5 versions, we can reject the null hypothesiswith 99.5% confidence to ensure that the syntacticalignment performs better.
Similarly, for hypoth-esis II, the same is true for the versions with andwithout stop word alignment.
Figures 5 and 6 pro-vide a graphical view of how each alignment ver-sion performs on the testing clusters.
The clustersalong the x-axis are listed in the order of increas-ing size.We have also done an analysis on interjudgeagreement in the evaluation.
The judges are in-structed about the evaluation scheme individually,and do their work independently.
We do not en-force them to be mutually consistent, as long asthey are self-consistent.
However, Table 6 showsthe mean and standard deviation of human judg-ments (grammaticality and fidelity) on each ver-sion.
The small deviation values indicate a fairlyhigh agreement.Finally, because human evaluation is expensive,we additionally tried to use a language-model ap-753Alignment Gr.
Mean Gr.
StdDev Fi.
Mean Fi.
StdDevV1 2.74 0.11 2.67 0.43V2 2.75 0.08 2.69 0.30V3 3.12 0.07 3.07 0.27V4 3.74 0.08 3.77 0.16V5 3.29 0.16 3.23 0.33Table 6: Mean and standard deviation of humanjudgments.proach in the training phase for automatic eval-uation of grammaticality.
We have used BLEUscores(Papineni et al, 2001), but have observedthat they are not consistent with those of humanjudges.
In particular, BLEU assigns too highscores to segmented sentences that are otherwisegrammatical.
It has been noted in the literaturethat metrics like BLEU that are solely based onN-grams might not be suitable for checking gram-maticality.5 ConclusionIn this paper, we presented a paraphrase genera-tion method based on multiple sequence alignmentwhich combines traditional dynamic program-ming techniques with linguistically motivated syn-tactic information.
We apply our work on compa-rable texts for which syntax has not been success-fully explored in alignment by previous work.
Weshowed that using syntactic features improves thequality of the alignment-induced finite state au-tomaton when it is used for generating novel sen-tences.
The strongest syntax guided alignment sig-nificantly outperformed all other versions in bothgrammaticality and fidelity of the novel sentences.In this paper we showed the effectiveness of us-ing syntax in the alignment of structurally diversecomparable texts as needed for text generation.ReferencesRegina Barzilay and Lillian Lee.
2002.
BootstrappingLexical Choice via Multiple-Sequence Alignment.In Proceedings of EMNLP 2002, Philadelphia.Regina Barzilay and Lillian Lee.
2003.
Learningto Paraphrase: An Unsupervised Approach UsingMultiple-Sequence Alignment.
In Proceedings ofNAACL-HLT03, Edmonton.Sabine Buchholz.
2000.
Readmefor perl script chunklink.pl.http://ilk.uvt.nl/ sabine/chunklink/README.html.Richard Durbin, Sean R. Eddy, Anders Krogh, andGraeme Mitchison.
1998.
Biological SequenceAnalysis.
Probabilistic Models of Proteins and Nu-cleic Acids.
Cambridge University Press.Joseph Felsenstein.
1995.
PHYLIP:Phylogeny Inference Package.http://evolution.genetics.washington.edu/phylip.html.DF.
Feng and Russell F. Doolittle.
1987.
Progres-sive sequence alignment as a prerequisite to correctphylogenetic trees.
Journal of Molecular Evolution,25(4).Walter M. Fitch and Emanuel Margoliash.
1967.Construction of Phylogenetic Trees.
Science,155(3760):279?284, January.Dan Gusfield, 1997.
Algorithms On Strings: A DualView from Computer Science and ComputationalMolecular Biology.
Cambridge University Press.Bo Pang, Kevin Knight, and Daniel Marcu.
2003.Syntax-based Alignment of Multiple Translations:Extracting Paraphrases and Generating New Sen-tences.
In Proceedings of HLT/NAACL 2003, Ed-monton, Canada.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
BLEU: A Method for AutomaticEvaluation of Machine Translation.
Research Re-port RC22176, IBM.Chris Quirk, Chris Brockett, and William Dolan.2004.
Monolingual machine translation for para-phrase generation.
In Dekang Lin and Dekai Wu,editors, Proceedings of EMNLP 2004, pages 142?149, Barcelona, Spain, July.
Association for Com-putational Linguistics.A Ratnaparkhi.
1998.
Maximum Entropy Models forNatural Language Ambiguity Resolution.
Phd.
The-sis, University of Pennsylvania.Erik F. Tjong Kim Sang and Jorn Veenstra.
1999.
Rep-resenting text chunks.
In EACL, pages 173?179.754
