Chinese Tagging Based on Maximum Entropy ModelKa Seng LeongFaculty of Science and Technology ofUniversity of MacauAv.
Padre Tom?s Pereira, Taipa,Macau, Chinama56538@umac.moFai WongFaculty of Science and Technology ofUniversity of Macau, INESC MacauAv.
Padre Tom?s Pereira, Taipa,Macau, Chinaderekfw@umac.moYiping LiFaculty of Science and Technology ofUniversity of MacauAv.
Padre Tom?s Pereira, Taipa,Macau, Chinaypli@umac.moMing Chui DongFaculty of Science and Technology ofUniversity of Macau, INESC MacauAv.
Padre Tom?s Pereira, Taipa,Macau, Chinadmc@inesc-macau.org.moAbstractIn the Fourth SIGHAN Bakeoff, we tookpart in the closed tracks of the wordsegmentation, part of speech (POS)tagging and named entity recognition (NER)tasks.
Particularly, we evaluated our wordsegmentation model on all the corpora,namely Academia Sinica (CKIP), CityUniversity of Hong Kong (CITYU),University of Colorado (CTB), StateLanguage Commission of P.R.C.
(NCC)and Shanxi University (SXU).
For POStagging and NER tasks, our models wereevaluated on CITYU corpus only.
Ourmodels for the evaulation are based on themaximum entropy approach, weconcentrated on the word segmentationtask for the bakeoff and our best officialresults on all the corpora for this task are0.9083 F-score on CITYU, 0.8985 onCKIP, 0.9077 on CTB, 0.8995 on NCC and0.9146 on SXU.1 IntroductionIn the Fourth SIGHAN Bakeoff, besides providingthe evaluation tasks for the word segmentation andNER, it also introduced another important evalua-tion task, POS tagging for Chinese language.
Inthis bakeoff, our models built for the tasks are sim-ilar to that in the work of Ng and Low (2004).
Themodels are based on a maximum entropy frame-work (Ratnaparkhi, 1996; Xue and Shen, 2003).They are trained on the corpora for the tasks fromthe bakeoff.
To understand the model, the imple-mentation of the models is wholly done ourselves.We used Visual Studio .NET 2003 and C++ as theimplementation language.
The Improved IterativeScaling (IIS) (Pietra et al, 1997) is used as the pa-rameter estimation algorithm for the models.
Wetried all the closed track tests of the word segmen-tation, the CITYU closed track tests for POS tag-ging and NER.2 Maximum EntropyIn this bakeoff, our basic model is based on theframework described in the work of Ratnaparkhi(1996) which was applied for English POS tagging.The conditional probability model of theframework is called maximum entropy (Jaynes,1957).
Maximum entropy model is a feature-based,probability model which can include arbitrarynumber of features that other generative modelslike N-gram model, hidden Markov model (HMM)(Rabiner, 1989) cannot do.
The probability modelcan be defined over X ?
Y, where X is the set of138Sixth SIGHAN Workshop on Chinese Language Processingpossible histories and Y is the set of allowablefutures or classes.
The conditional probability ofthe model of a history x and a class y is defined as( , )( | ) ( )if x yiip y x Z x?
????
(1)( , )( ) if x yiy iZ x?
????
(2)where ?
is a parameter which acts as a weight forthe feature in the particular history.
The equation(1) states that the conditional probability of theclass given the history is the product of the weight-ings of all features which are active under the con-sideration of (x, y) pair, normalized over the sumof the products of all the classes.
The normaliza-tion constant is determined by the requirement that( | ) 1yp y x?
?
?for all x.To find the optimized parameters ?
of the condi-tional probability is one of the important processesin building the model.
This can be done through atraining process.
The parameter estimation algo-rithm used for training is Improved Iterative Scal-ing (IIS) (Pietra et al, 1997) in our case.
In train-ing the models for this bakeoff, the training data isgiven in the form of a sequence of characters (forthe tasks of word segmentation and NER) or words(POS tagging) and their classes (tags), the parame-ters ?
can be chosen to maximize the likelihood ofthe training data using p:( , )1 1 11( ) ( , ) ( )j i if x yn n mi i ji i jL p p x y Z x?
???
?
??
??
?
?
(3)But of course, the success of the model dependsheavily on the selection of features for a particulartask.
This will be described in Section 5.3 Chinese Word SegmenterWe concentrated on the word segmentation task inthis bakeoff.
For the Chinese word segmenter, it isbased on the work that treats Chinese word seg-mentation as tagging (Xue and Shen, 2003; Ng andLow, 2004).
Given a Chinese sentence, it assigns aso-called boundary tag to each Chinese characterin the sentence.
There are four possible boundarytags: S for a character which is a single-characterword, B for a character that is the first character ofa multi-character word, E for a character that is thelast character of a multi-character word and M fora character that is neither the first nor last of a mul-ti-character word.
With these boundary tags, theword segmentation becomes a tagging problemwhere each character in Chinese sentences is givenone of the boundary tags which is the most proba-ble one according to the conditional probabilitycalculated by the model.
And then sequences ofcharacters are converted into sequences of wordsaccording to the tags.4 POS Tagger and Named Entity Recog-nizerFor the POS tagging task, the tagger is built basedon the work of Ratnaparkhi (1996) which was ap-plied for English POS tagging.
Because of the timelimitation, we could only try to port our imple-mented maximum entropy model to this POS tag-ging task by using the similar feature set (discussedin Section 5) for a word-based POS tagger as in thework of Ng and Low (2004).
By the way, besidesporting the model to the POS tagging task, it waseven tried in the NER task by using the same fea-ture set (discussed in Section 5) as used for theword segmentation in order to test the performanceof the implemented model.The tagging algorithm for these two tasks is bas-ically the same as used in word segmentation.
Giv-en a word or a character, the model will try to as-sign the most probable POS or NE tag for the wordor character respectively.5 FeaturesTo achieve a successful model for any task by us-ing the maximum entropy model, an important stepis to select a set of useful features for the task.
Inthe following, the feature sets used in the tasks ofthe bakeoff are discussed.5.1 Word Segmentation FeaturesThe feature set used in this task is discussed in ourprevious work (Leong et al, 2007) which is cur-rently the best in our implemented model.
They arethe unigram features: C-2, C-1, C0, C1 and C2, bi-gram features: C-2C-1, C-1C0, C0C1, C1C2 and C-1C1where C0 is the current character, Cn (C-n) is the139Sixth SIGHAN Workshop on Chinese Language Processingcharacter at the nth position to the right (left) of thecurrent character.
For example, given the charactersequence ???????
(Victoria Harbour), whiletaking the character ???
as C0, then C-2 = ??
?, C-1C1 = ???
?, etc.
The boundary tag (S, B, M or E)feature T-1 is also applied, i.e., the boundary tagassigned to the previous character of C0.
And thelast feature WC0: This feature captures the wordcontext in which the current character is found.
Ithas the format ?W_C0?.
For example, the character???
is a character of the word ??????
?.Then this will give the feature WC0 = ??????_?
?.5.2 POS Tagging FeaturesFor this task, because of the time limitation asmentioned in the previous section, we could onlyport our implemented model by using a part of thefeature set which was used in the word-based tag-ger discussed in the work of Ng and Low (2004).The feature set includes: Wn (n = -2 to 2), WnWn+1(n = -2, -1, 0, 1), W-1W1, POS(W-2), POS(W-1),POS(W-2)POS(W-1) where W refers to a word, POSrefers to the POS assigned to the word and n refersto the position of the current word being consi-dered.
For example, while considering this sen-tence taken from the POS tagged corpus of CITYU:??
?/Ng  ?
?/Ac  ??
?/Nc  ?
?/Dc  ??/Vt?
(Hong Kong S.A.R.
is established), taking ?????
as W0, then W-2 = ???
?, W-1W1 = ???
??
?, POS(W-2) = ?Ng?, POS(W-2)POS(W-1) = ?AcDc?, etc.5.3 Named Entity Recognition FeaturesFor the NER task, we directly used the same fea-ture set as for the word segmentation basically.However, because the original NE tagged corpus ispresented in two-column format, where the firstcolumn consists of the character and the second isa tag, a transformation which is to transform theoriginal corpus to a sentence per line format beforecollecting the features or other training data isneeded.
This transformation actually continues toread the lines from the original corpus, whenever ablank line is found, a sentence of characters withNE tags can be formed.After that, the features collected are the unigramfeatures: C-2, C-1, C0, C1 and C2, bigram features:C-2C-1, C-1C0, C0C1, C1C2 and C-1C1, NE tag fea-tures: T-1, WC0 (this feature captures the NE con-text in which the current character is found) whereT-1 refers to the NE tag assigned to the previouscharacter of C0, W refers to the named entity.
Sosimilar to the explanation of features of word seg-mentation, for example, given the sequence fromthe NER tagged corpus of CITYU:  ?
?/N ?/N ?/B-LOC ?
/I-LOC ?
/N?
(One Chinese), whiletaking the character ???
as C0, then C-2 = ??
?, C-1C1 = ???
?, WC0 = ?????
?, etc.For all the experiments conducted, training wasdone with a feature cutoff of 1.6 TestingFor word segmentation task, during testing, given acharacter sequence C1 ?
Cn, the trained model willtry to assign a boundary tag to each character in thesequence based on the probability of the boundarytag calculated.
Then the sequence of characters isconverted into sequence of words according to thetag sequence t1 ?
tn.
But if each character was justassigned the boundary tag with the highest proba-bility, invalid boundary tag sequences would beproduced and wrong word segmentation resultswould be obtained.
In particular, known words thatare in the dictionary of the training corpus aresegmented wrongly because of these invalid tagsequences.
In order to correct these, the invalidboundary tag sequences are collected, such as fortwo-character words, they are ?B B?, ?B S?, ?M S?,?E E?, etc., for three-character words, they are ?BE S?, ?B M S?, etc., and for four-character words,they are ?B M M S?, ?S M M E?, etc.
With theseinvalid boundary tag sequences, some post correc-tion to the word segmentation result can be tried.That is after the model tagger has done the taggingfor a Chinese sentence every time, the invalidboundary tag sequences will be searched within thepreliminary result given by the tagger.
When theinvalid boundary tag sequence is found, the charac-ters corresponding to that invalid boundary tag se-quence will be obtained.
After, the word formed bythese characters is looked up to see if it is indeed aword in the dictionary, if it is, then the correction iscarried out.Another kind of post correction to the wordsegmentation result is to make some guessed cor-rection for some invalid boundary tag sequencessuch as ?B S?, ?S E?, ?B B?, ?E E?, ?B M S?, etc.That is, whenever those tag sequences are met140Sixth SIGHAN Workshop on Chinese Language Processingwithin the preliminary result given by the modeltagger, they will be corrected no matter if there isword in the dictionary formed by the characterscorresponding to the invalid boundary tag se-quence.We believe that similar post correction can beapplied to the NER task.
For example, if such NEtag sequences ?B-PER N?, ?N I-PER N?, etc.
oc-cur in the result, then the characters correspondingto the invalid NE tag sequence can be obtainedagain and looked up in the named entity dictionaryto see if they really form a named entity.
However,we did not have enough time to adapt this for theNER task finally.
Therefore, no such post correc-tion was applied for the NER task in this bakeofffinally.7 Evaluation ResultsWe evaluated our models in the closed tracks ofthe word segmentation, part of speech (POS)tagging and named entity recognition (NER) tasks.Particularly, our word segmentation model wasevaluated on all the corpora, namely AcademiaSinica (CKIP), City University of Hong Kong(CITYU), University of Colorado (CTB), StateLanguage Commission of P.R.C.
(NCC) andShanxi University (SXU).
For POS tagging andNER tasks, our models were evaluated on theCITYU corpus only.
Table 1 shows our officialresults for the word segmentation task in thebakeoff.
The columns R, P and F show the recall,precision and F-score respectively.Run_ID R P Fcityu_a 0.9221 0.8947 0.9082cityu_b 0.9219 0.8951 0.9083ckip_a 0.9076 0.8896 0.8985ckip_b 0.9074 0.8897 0.8985ctb_a 0.9078 0.9073 0.9075ctb_b 0.9077 0.9078 0.9077ncc_a 0.8997 0.8992 0.8995ncc_b 0.8995 0.8992 0.8994sxu_a 0.9186 0.9106 0.9145sxu_b 0.9185 0.9107 0.9146Table 1.
Official Results in the Closed Tracks ofthe Word Segmentation Task on all CorporaWe submitted a few runs for each of the tests ofthe corpora.
Table 1 shows the best two runs foreach of the tests of the corpora for discussion here.The run (a) applied only the post correction to theknown words that are in the dictionary of the train-ing corpus but are segmented wrongly because ofthe invalid boundary tag sequences.
The run (b)applied also the guessed post correction for someinvalid boundary tag sequences in the results asmentioned in Section 6.
From the results above, itcan be seen that the runs with the guessed post cor-rection generally gave a little bit better perfor-mance than those that did not apply.
This showsthat the guess somehow made some good guessesfor some unknown words that appear in the testingcorpora.Table 2 shows our official results for the POStagging task.
The columns A shows the accuracy.The columns IV-R, OOV-R and MT-R show therecall on in-vocabulary words, out-of-vocabularywords and multi-POS words (multi-POS words arethe words in the training corpus and have morethan one POS-tag in either the training corpus ortesting corpus) respectively.
The run (a) used theparamters set which was observed to be theoptimal ones for the model in the training phase.The run (b) used the parameters set of the model inthe last iteration of the training phase.Run_ID A IV-R OOV-R MT-Rcityu_a 0.1890 0.2031 0.0550 0.1704cityu_b 0.2793 0.2969 0.1051 0.2538Table 2.
Official Results in the Closed Track of thePOS Tagging Task on the CITYU CorpusIt can be seen that our results were unexpectedlylow in accuracy.
After releasing the results, wefound that the problem was due to the encodingproblem of our submitted result files.
The problemprobably occurred after the conversion from ourBig5 encoded results to the UTF-16 encodedresults which are required by the bakeoff.Therefore, we did the evaluation ourselves byrunning our POS tagger again, using the officialevaluation program and the truth test set.
Finally,our best result was 0.7436 in terms of accuracy butthis was still far lower than the baseline (0.8425) ofthe CITYU corpus.
This shows that the directporting of English word-based POS tagging toChinese is not effective.Table 3 shows our official results for the NERtask.
The columns R, P and F show the recall,precision and F-score respectively.
Again, similarto the POS tagging task, the run (a) used the141Sixth SIGHAN Workshop on Chinese Language Processingparamters set which was observed to be theoptimal ones for the model in the training phase.The run (b) used the parameters set of the model inthe last iteration of the training phase.Run_ID R P Fcityu_a 0.0874 0.1058 0.0957cityu_b 0.0211 0.0326 0.0256Table 3.
Official Results in the Closed Track of theNER Task on the CITYU CorpusIt can be seen that our results were againunexpectedly low in accuracy.
The cause of suchlow accuracy results was due to parts of the wrongformat of the submitted result files compared withthe correct format of the result file.
So like thePOS tagging task, we did the evaluation ourselvesby running our NE recognizer again.
Finally, ourbest result was 0.5198 in terms of F-score but thiswas again far lower than the baseline (0.5955) ofthe CITYU corpus.
This shows that the similarfeature set for the word segmentation task is noteffective for the NER task.8 ConclusionThis paper reports the use of maximum entropyapproach for implementing models for the threetasks in the Fourth SIGHAN Bakeoff and our re-sults in the bakeoff.
From the results, we got goodexperience and knew the weaknesses of our mod-els.
These help to improve the performance of ourmodels in the future.AcknowledgementsThe research work reported in this paper was par-tially supported by ?Fundo para oDesenvolvimento das Ci?ncias e da Tecnologia?under grant 041/2005/A.ReferencesAdwait Ratnaparkhi.
1996.
A maximum entropy modelfor part-of-speech tagging, in Proceedings of Confe-rence on Empirical Methods in Natural LanguageProcessing, Philadelphia, USA, pages 133-142.Edwin Thompson Jaynes.
1957.
Information Theory andStatistical Mechanics, The Physical Review, 106(4):620-630.Hwee Tou Ng, and Jin Kiat Low.
2004.
Chinese part-of-speech tagging: One-at-a-time or all-at-once?
Word-based or character-based?
In Proceedings of the 2004Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP 2004), Barcelona, Spain,pages 277-284.Ka Seng Leong, Fai Wong, Yiping Li, and Ming ChuiDong.
2007.
Chinese word boundaries detectionbased on maximum entropy model, in Proceedings ofthe 11th International Conference on Enhancementand Promotion of Computational Methods in Engi-neering and Science (EPMESC-XI), Kyoto, Japan.Lawrence Rabiner.
1989.
A tutorial on hidden Markovmodels and selected applications in speech recogni-tion, Proceedings of the IEEE, 77(2): 257?286.Nianwen Xue, and Libin Shen.
2003.
Chinese wordsegmentation as LMR tagging, in Proceedings of the2nd SIGHAN Workshop on Chinese LanguageProcessing, Sapporo, Japan, pages 176-179.Steven Della Pietra, Vincent Della Pietra, and John Laf-ferty.
1997.
Inducing features of random fields.
IEEEtransactions on pattern analysis and machine intelli-gence, 19(4): 380?393.142Sixth SIGHAN Workshop on Chinese Language Processing
