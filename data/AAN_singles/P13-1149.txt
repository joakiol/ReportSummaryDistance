Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1517?1526,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsCompositional-ly Derived Representations ofMorphologically Complex Words in Distributional SemanticsAngeliki Lazaridou and Marco Marelli and Roberto Zamparelli and Marco BaroniCenter for Mind/Brain Sciences (University of Trento, Italy)first.last@unitn.itAbstractSpeakers of a language can construct anunlimited number of new words throughmorphological derivation.
This is a majorcause of data sparseness for corpus-basedapproaches to lexical semantics, such asdistributional semantic models of wordmeaning.
We adapt compositional meth-ods originally developed for phrases to thetask of deriving the distributional meaningof morphologically complex words fromtheir parts.
Semantic representations con-structed in this way beat a strong baselineand can be of higher quality than represen-tations directly constructed from corpusdata.
Our results constitute a novel evalua-tion of the proposed composition methods,in which the full additive model achievesthe best performance, and demonstrate theusefulness of a compositional morphologycomponent in distributional semantics.1 IntroductionEffective ways to represent word meaning areneeded in many branches of natural language pro-cessing.
In the last decades, corpus-based meth-ods have achieved some degree of success in mod-eling lexical semantics.
Distributional semanticmodels (DSMs) in particular represent the mean-ing of a word by a vector, the dimensions of whichencode corpus-extracted co-occurrence statistics,under the assumption that words that are semanti-cally similar will occur in similar contexts (Turneyand Pantel, 2010).
Reliable distributional vectorscan only be extracted for words that occur in manycontexts in the corpus.
Not surprisingly, there isa strong correlation between word frequency andvector quality (Bullinaria and Levy, 2007), andsince most words occur only once even in verylarge corpora (Baroni, 2009), DSMs suffer datasparseness.While word rarity has many sources, one of themost common and systematic ones is the high pro-ductivity of morphological derivation processes,whereby an unlimited number of new words canbe constructed by adding affixes to existing stems(Baayen, 2005; Bauer, 2001; Plag, 1999).1 Forexample, in the multi-billion-word corpus we in-troduce below, perfectly reasonable derived formssuch as lexicalizable or affixless never occur.
Evenwithout considering the theoretically infinite num-ber of possible derived nonce words, and restrict-ing ourselves instead to words that are alreadylisted in dictionaries, complex forms cover a highportion of the lexicon.
For example, morphologi-cally complex forms account for 55% of the lem-mas in the CELEX English database (see Section4.1 below).
In most of these cases (80% accordingto our corpus) the stem is more frequent than thecomplex form (e.g., the stem build occurs 15 timesmore often than the derived form rebuild, and thelatter is certainly not an unusual derived form).DSMs ignore derivational morphology alto-gether.
Consequently, they cannot provide mean-ing representations for new derived forms, nor canthey harness the systematic relation existing be-tween stems and derivations (any English speakercan infer that to rebuild is to build again, whetherthey are familiar with the prefixed form or not)in order to mitigate derived-form sparseness prob-lems.
A simple way to handle derivational mor-1Morphological derivation constructs new words (inthe sense of lemmas) from existing lexical items (re-source+ful?resourceful).
In this work, we do not treat in-flectional morphology, pertaining to affixes that encode gram-matical features such as number or tense (dog+s).
We usemorpheme for any component of a word (resource and -fulare both morphemes).
We use stem for the lexical item thatconstitutes the base of derivation (resource) and affix (pre-fix or suffix) for the element attached to the stem to derivethe new form (-ful).
In English, stems are typically indepen-dent words, affixes bound morphemes, i.e., they cannot standalone.
Note that a stem can in turn be morphologically de-rived, e.g., point+less in pointless+ly.
Finally, we use mor-phologically complex as synonymous with derived.1517phology would be to identify the stem of rare de-rived words and use its distributional vector as aproxy to derived-form meaning.2 The meaning ofrebuild is not that far from that of build, so thelatter might provide a reasonable surrogate.
Still,something is clearly lost (if the author of a textfelt the need to use the derived form, the stem wasnot fully appropriate), and sometimes the jump inmeaning can be quite dramatic (resourceless andresource mean very different things!
).In the past few years there has been much in-terest in how DSMs can scale up to represent themeaning of larger chunks of text such as phrasesor even sentences.
Trying to represent the mean-ing of arbitrarily long constructions by directlycollecting co-occurrence statistics is obviously in-effective and thus methods have been developedto derive the meaning of larger constructions as afunction of the meaning of their constituents (Ba-roni and Zamparelli, 2010; Coecke et al, 2010;Mitchell and Lapata, 2008; Mitchell and Lapata,2010; Socher et al, 2012).
Compositional distri-butional semantic models (cDSMs) of word unitsaim at handling, compositionally, the high produc-tivity of phrases and consequent data sparseness.It is natural to hypothesize that the same methodscan be applied to morphology to derive the mean-ing of complex words from the meaning of theirparts: For example, instead of harvesting a rebuildvector directly from the corpus, the latter could beconstructed from the distributional representationsof re- and build.
Besides alleviating data sparse-ness problems, a system of this sort, that automati-cally induces the semantic contents of morpholog-ical processes, would also be of tremendous theo-retical interest, given that the semantics of deriva-tion is a central and challenging topic in linguisticmorphology (Dowty, 1979; Lieber, 2004).In this paper, we explore, for the first time (ex-cept for the proof-of-concept study in Guevara(2009)), the application of cDSMs to derivationalmorphology.
We adapt a number of compositionmethods from the literature to the morphologicalsetting, and we show that some of these methodscan provide better distributional representations ofderived forms than either those directly harvestedfrom a large corpus, or those obtained by usingthe stem as a proxy to derived-form meaning.
Our2Of course, spotting and segmenting complex words is abig research topic unto itself (Beesley and Karttunen, 2000;Black et al, 1991; Sproat, 1992), and one we completelysidestep here.results suggest that exploiting morphology couldimprove the quality of DSMs in general, extendthe range of tasks that cDSMs can successfullymodel and support the development of new waysto test their performance.2 Related workMorphological induction systems use corpus-based methods to decide if two words are mor-phologically related and/or to segment words intomorphemes (Dreyer and Eisner, 2011; Goldsmith,2001; Goldwater and McClosky, 2005; Goldwater,2006; Naradowsky and Goldwater, 2009; Wicen-towski, 2004).
Morphological induction has re-cently received considerable attention since mor-phological analysis can mitigate data sparseness indomains such as parsing and machine translation(Goldberg and Tsarfaty, 2008; Lee, 2004).
Amongthe cues that have been exploited there is distri-butional similarity among morphologically relatedwords (Schone and Jurafsky, 2000; Yarowsky andWicentowski, 2000).
Our work, however, dif-fers substantially from this track of research.
Wedo not aim at segmenting morphological complexwords or identifying paradigms.
Our goal is toautomatically construct, given distributional rep-resentations of stems and affixes, semantic repre-sentations for the derived words containing thosestems and affixes.
A morphological induction sys-tem, given rebuild, will segment it into re- andbuild (possibly using distributional similarity be-tween the words as a cue).
Our system, givenre- and build, predicts the (distributional seman-tic) meaning of rebuild.Another emerging line of research uses distribu-tional semantics to model human intuitions aboutthe semantic transparency of morphologically de-rived or compound expressions and how these im-pact various lexical processing tasks (Kuperman,2009; Wang et al, 2012).
Although these worksexploit vectors representing complex forms, theydo not attempt to generate them compositionally.The only similar study we are aware of is thatof Guevara (2009).
Guevara found a systematicgeometric relation between corpus-based vectorsof derived forms sharing an affix and their stems,and used this finding to motivate the compositionmethod we term lexfunc below.
However, unlikeus, he did not test alternative models, and he onlypresented a qualitative analysis of the trajectoriestriggered by composition with various affixes.15183 Composition methodsDistributional semantic models (DSMs), alsoknown as vector-space models, semantic spaces,or by the names of famous incarnations such asLatent Semantic Analysis or Topic Models, ap-proximate the meaning of words with vectors thatrecord their patterns of co-occurrence with cor-pus context features (often, other words).
Thereis an extensive literature on how to develop suchmodels and on their evaluation.
Recent surveysinclude Clark (2012), Erk (2012) and Turney andPantel (2010).
We focus here on compositionalDSMs (cDSMs).
Since the very inception of dis-tributional semantics, there have been attempts tocompose meanings for sentences and larger pas-sages (Landauer and Dumais, 1997), but inter-est in compositional DSMs has skyrocketed inthe last few years, particularly since the influen-tial work of Mitchell and Lapata (2008; 2009;2010).
For the current study, we have reimple-mented and adapted to the morphological settingall cDSMs we are aware of, excluding the tensor-product-based models that Mitchell and Lapata(2010) have shown to be empirically disappointingand the models of Socher and colleagues (Socheret al, 2011; Socher et al, 2012), that require com-plex optimization procedures whose adaptation tomorphology we leave to future work.Mitchell and Lapata proposed a set of simpleand effective models in which the composed vec-tors are obtained through component-wise opera-tions on the constituent vectors.
Given input vec-tors u and v, the multiplicative model (mult) re-turns a composed vector c with: ci = uivi.
In theweighted additive model (wadd), the composedvector is a weighted sum of the two input vectors:c = ?u + ?v, where ?
and ?
are two scalars.
Inthe dilation model, the output vector is obtainedby first decomposing one of the input vectors, sayv, into a vector parallel to u and an orthogonalvector.
Following this, the parallel vector is dilatedby a factor ?
before re-combining.
This results in:c = (??
1)?u,v?u+ ?u,u?v.Guevara (2010) and Zanzotto et al (2010) pro-pose the full additive model (fulladd), where thetwo vectors to be added are pre-multiplied byweight matrices: c = Au+BvSince the Mitchell and Lapata and fulladd mod-els were developed for phrase composition, thetwo input vectors were taken to be, very straight-forwardly, the vectors of the two words to be com-posed into the phrase of interest.
In morphologicalderivation, at least one of the items to be composed(the affix) is a bound morpheme.
In our adapta-tion of these composition models, we build boundmorpheme vectors by accumulating the contextsin which a set of derived words containing the rel-evant morphemes occur, e.g., the re- vector aggre-gates co-occurrences of redo, remake, retry, etc.Baroni and Zamparelli (2010) and Coecke etal.
(2010) take inspiration from formal semanticsto characterize composition in terms of functionapplication, where the distributional representa-tion of one element in a composition (the func-tor) is not a vector but a function.
Given thatlinear functions can be expressed by matrices andtheir application by matrix-by-vector multiplica-tion, in this lexical function (lexfunc) model, thefunctor is represented by a matrix U to be multi-plied with the argument vector v: c = Uv.
Inthe case of morphology, it is natural to treat boundaffixes as functions over stems, since affixes en-code the systematic semantic patterns we intendto capture.
Unlike the other composition meth-ods, lexfunc does not require the construction ofdistributional vectors for affixes.
A matrix repre-sentation for every affix is instead induced directlyfrom examples of stems and the corresponding de-rived forms, in line with the intuition that every af-fix corresponds to a different pattern of change ofthe stem meaning.Finally, as already discussed in the Introduc-tion, performing no composition at all but usingthe stem vector as a surrogate of the derived formis a reasonable strategy.
We saw that morphologi-cally derived words tend to appear less frequentlythan their stems, and in many cases the meaningsare close.
Consequently, we expect a stem-only?composition?
method to be a strong baseline inthe morphological setting.4 Experimental setup4.1 Morphological dataWe obtained a list of stem/derived-form pairs fromthe CELEX English Lexical Database, a widelyused 100K-lemma lexicon containing, amongother things, information about the derivationalstructure of words (Baayen et al, 1995).
For eachderivational affix present in CELEX, we extractedfrom the database the full list of stem/derivedpairs matching its most common part-of-speechsignature (e.g., for -er we only considered pairs1519Affix Stem/Der.
Training HQ/Tot.
Avg.POS Items Test Items SDR-able verb/adj 177 30/50 5.96-al noun/adj 245 41/50 5.88-er verb/noun 824 33/50 5.51-ful noun/adj 53 42/50 6.11-ic noun/adj 280 43/50 5.99-ion verb/noun 637 38/50 6.22-ist noun/noun 244 38/50 6.16-ity adj/noun 372 33/50 6.19-ize noun/verb 105 40/50 5.96-less noun/adj 122 35/50 3.72-ly adj/adv 1847 20/50 6.33-ment verb/noun 165 38/50 6.06-ness adj/noun 602 33/50 6.29-ous noun/adj 157 35/50 5.94-y noun/adj 404 27/50 5.25in- adj/adj 101 34/50 3.39re- verb/verb 86 27/50 5.28un- adj/adj 128 36/50 3.23tot */* 6549 623/900 5.52Table 1: Derivational morphology datasethaving a verbal stem and nominal derived form).Since CELEX was populated by semi-automatedmorphological analysis, it includes forms that areprobably not synchronically related to their stems,such as crypt+ic or re+form.
However, we did notmanually intervene on the pairs, since we are in-terested in training and testing our methods in re-alistic, noisy conditions.
In particular, the need topre-process corpora to determine which forms are?opaque?, and should thus be bypassed by our sys-tems, would greatly reduce their usefulness.
Pairsin which either word occurred less than 20 timesin our source corpus (described in Section 4.2 be-low) were filtered out and, in our final dataset, weonly considered the 18 affixes (3 prefixes and 15suffixes) with at least 100 pairs meeting this con-dition.
We randomly chose 50 stem/derived pairs(900 in total) as test data.
The remaining data wereused as training items to estimate the parametersof the composition methods.
Table 1 summarizesvarious characteristics of the dataset3 (the last twocolumns of the table are explained in the next para-graphs).Annotation of quality of test vectors The qual-ity of the corpus-based vectors representing de-rived test items was determined by collecting hu-man semantic similarity judgments in a crowd-sourcing survey.
In particular, we use the similar-ity of a vector to its nearest neighbors (NNs) as aproxy measure of quality.
The underlying assump-3Available from http://clic.cimec.unitn.it/composestion is that a vector, in order to be a good represen-tation of the meaning of the corresponding word,should lie in a region of semantic space populatedby intuitively similar meanings, e.g., we are morelikely to have captured the meaning of car if theNN of its vector is the automobile vector ratherthan potato.
Therefore, to measure the quality ofa given vector, we can look at the average simi-larity score provided by humans when comparingthis very vector with its own NNs.All 900 derived vectors from the test set werematched with their three closest NNs in our se-mantic space (see Section 4.2), thus producing aset of 2, 700 word pairs.
These pairs were admin-istered to CrowdFlower users,4 who were askedto judge the relatedness of the two meanings on a7-point scale (higher for more related).
In orderto ensure that participants were committed to thetask and exclude non-proficient English speakers,we used 60 control pairs as gold standard, consist-ing of either perfect synonyms or completely un-related words.
We obtained 30 judgments for eachderived form (10 judgments for each of 3 neighborcomparisons), with mean participant agreement of58%.
These ratings were averaged item-wise, re-sulting in a Gaussian distribution with a mean of3.79 and a standard deviation of 1.31.
Finally,each test item was marked as high-quality (HQ)if its derived form received an average score of atleast 3, as low-quality (LQ) otherwise.
Table 1 re-ports the proportion of HQ test items for each af-fix, and Table 2 reports some examples of HQ andLQ items with the corresponding NNs.
It is worthobserving that the NNs of the LQ items, while notas relevant as the HQ ones, are hardly random.Annotation of similarity between stem and de-rived forms Derived forms differ in terms ofhow far their meaning is with respect to that oftheir stem.
Certain morphological processes havesystematically more impact than others on mean-ing: For example, the adjectival prefix in- negatesthe meaning of the stem, whereas -ly has the solefunction to convert an adjective into an adverb.But the very same affix can affect different stemsin different ways.
For example, remelt means lit-tle more than to melt again, but rethink has subtlerimplications of changing one?s way to look at aproblem, and while one of the senses of cycling ispresent in recycle, it takes some effort to see theirrelation.4http://www.crowdflower.com1520Affix Type Derived form Neighbors-ist HQ transcendentalist mythologist, futurist, theosophistLQ florist Harrod, wholesaler, stockist-ity HQ publicity publicise, press, publicizeLQ sparsity dissimilarity, contiguity, perceptibility-ment HQ advertisement advert, promotional, advertisingLQ inducement litigant, contractually, voluntarilyin- HQ inaccurate misleading, incorrect, erroneousLQ inoperable metastasis, colorectal, biopsyre- HQ recapture retake, besiege, captureLQ rename defunct, officially, mergeTable 2: Examples of HQ and LQ derived vectors with their NNsWe conducted a separate crowdsourcing studywhere participants were asked to rate the 900test stem/derived pairs for the strength of theirsemantic relationship on a 7-point scale.
Wefollowed a procedure similar to the one de-scribed for quality measurement; 7 judgmentswere collected for each pair.
Participants?
agree-ment was at 60%.
The last column of Ta-ble 1 reports the average stem/derived related-ness (SDR) for the various affixes.
Note thatthe affixes with systematically lower SDR arethose carrying a negative meaning (in-, un-, -less),whereas those with highest SDR do little morethan changing the POS of the stem (-ion, -ly, -ness).
Among specific pairs with very low related-ness we encounter hand/handy, bear/bearable andactive/activist, whereas compulsory/compulsorily,shameless/shamelessness and chaos/chaotic havehigh SDR.
Since the distribution of the averageratings was negatively skewed (mean rating: 5.52,standard deviation: 1.26),5 we took 5 as the ratingthreshold to classify items as having high (HR) orlow (LR) relatedness to their stems.4.2 Distributional semantic space6We use as our source corpus the concatenation ofukWaC, the English Wikipedia (2009 dump) andthe BNC,7 for a total of about 2.8 billion tokens.We collect co-occurrence statistics for the top 20Kcontent words (adjectives, adverbs, nouns, verbs)5The negative skew is not surprising, as derived formsmust have some relation to their stems!6Most steps of the semantic space constructionand composition pipelines were implemented usingthe DISSECT toolkit: https://github.com/composes-toolkit/dissect.7http://wacky.sslmit.unibo.it, http://en.wikipedia.org, http://www.natcorp.ox.ac.ukin lemma format, plus any item from the mor-phological dataset described above that was belowthis rank.
The top 20K content words also con-stitute our context elements.
We use a standardbag-of-words approach, counting collocates in anarrow 2-word before-and-after window.
We ap-ply (non-negative) Pointwise Mutual Informationas weighting scheme and dimensionality reduc-tion by Non-negative Matrix Factorization, settingthe number of reduced-space dimensions to 350.These settings are chosen without tuning, and arebased on previous experiments where they pro-duced high-quality semantic spaces (Boleda et al,2013; Bullinaria and Levy, 2007).4.3 Implementation of composition methodsAll composition methods except mult and stemhave weights to be estimated (e.g., the ?
parame-ter of dilation or the affix matrices of lexfunc).
Weadopt the estimation strategy proposed by Gue-vara (2010) and Baroni and Zamparelli (2010),namely we pick parameter values that optimizethe mapping between stem and derived vectors di-rectly extracted from the corpus.
To learn, say, alexfunc matrix representing the prefix re-, we ex-tract vectors of V/reV pairs that occur with suffi-cient frequency (visit/revisit, think/rethink.
.
.
).
Wethen use least-squares methods to find weights forthe re- matrix that minimize the distance betweeneach reV vector generated by the model given theinput V and the corresponding corpus-observedderived vector (e.g., we try to make the model-predicted re+visit vector as similar as possibleto the corpus-extracted one).
This is a generalestimation approach that does not require task-specific hand-labeled data, and for which simpleanalytical solutions of the least-squares error prob-1521lem exist for all our composition methods.
We useonly the training items from Section 4.1 for esti-mation.
Note that, unlike the test items, these havenot been annotated for quality, so we are adoptingan unsupervised (no manual labeling) but noisy es-timation method.8For the lexfunc model, we use the training itemsseparately to obtain weight matrices represent-ing each affix, whereas for the other models alltraining data are used together to globally de-rive single sets of affix and stem weights.
Forthe wadd model, the learning process results in0.16?affix+0.33?
stem, i.e., the affix contributesonly half of its mass to the composition of thederived form.
For dilation, we stretch the stem(i.e., v of the dilation equation is the stem vector),since it should provide richer contents than the af-fix to the derived meaning.
We found that, on av-erage across the training pairs, dilation weightedthe stem 20 times more heavily than the affix(0.05?affix+1?stem).
We then expect that the di-lation model will have similar performance to thebaseline stem model, as confirmed below.9For all methods, vectors were normalized be-fore composing both in training and in generation.5 Experiment 1: approximatinghigh-quality corpus-extracted vectorsThe first experiment investigates to what extentcomposition models can approximate high-quality(HQ) corpus-extracted vectors representing de-rived forms.
Note that since the test items wereexcluded from training, we are simulating a sce-nario in which composition models must generaterepresentations for nonce derived forms.Cosine similarity between model-generated andcorpus-extracted vectors were computed for allmodels, including the stem baseline (i.e., co-sine between stem and derived form).
The firstrow of Table 3 reports mean similarities.
Thestem method sets the level of performance rel-atively high, confirming its soundness.
Indeed,the parameter-free mult model performs below thebaseline.10 As expected, dilation performs simi-8More accurately, we relied on semi-manual CELEX in-formation to identify derived forms.
A further step towards afully knowledge-free system would be to pre-process the cor-pus with an unsupervised morphological induction system toextract stem/derived pairs.9The other models have thousands of weights to be es-timated, so we cannot summarize the outcome of parameterestimation here.10This result does not necessarily contradict those ofstem mult dil.
wadd fulladd lexfuncAll 0.47 0.39 0.48 0.50 0.56 0.54HR 0.52 0.43 0.53 0.55 0.61 0.58LR 0.32 0.28 0.33 0.38 0.41 0.42Table 3: Mean similarity of composed vectors tohigh-quality corpus-extracted derived-form vec-tors, for all as well as high- (HR) and low-relatedness (LR) test itemslarly to the baseline, while wadd outperforms it,although the effect does not reach significance(p=.06).11 Both fulladd and lexfunc perform sig-nificantly better than stem (p < .001).
Lexfuncprovides a flexible way to account for affixation,since it models it directly as a function mappingfrom and onto word vectors, without requiring avector representation of bound affixes.
The rea-son at the base of its good performance is thusquite straightforward.
On the other hand, it issurprising that a simple representation of boundaffixes (i.e., as vectors aggregating the contextsof words containing them) can work so well, atleast when used in conjunction with the granulardimension-by-dimension weights assigned by thefulladd method.
We hypothesize that these aggre-gated contexts, by providing information about theset of stems an affix combines with, capture theshared semantic features that the affix operates on.When the meaning of the derived form is farfrom that of its stem, the stem baseline should nolonger constitute a suitable surrogate of derived-form meaning.
The LR cases (see Section 4.1above) are thus crucial to understand how wellcomposition methods capture not only stem mean-ing, but also affix-triggered semantics.
The HRand LR rows of Table 3 present the results forthe respective test subsets.
As expected, the stemapproach undergoes a strong drop when perfor-mance is measured on LR items.
At the other ex-treme, fulladd and lexfunc, while also finding theLR cases more difficult, still clearly outperformthe baseline (p<.001), confirming that they cap-ture the meaning of derived forms beyond whattheir stems contribute to it.
The effect of wadd,again, approaches significance when compared tothe baseline (p= .05).
Very encouragingly, bothMitchell and Lapata and others who found mult to be highlycompetitive.
Due to differences in co-occurrence weightingschemes (we use a logarithmically scaled measure, they donot), their multiplicative model is closer to our additive one.11Significance assessed by means of Tukey Honestly Sig-nificant Difference tests (Abdi and Williams, 2010)1522stem mult wadd dil.
fulladd lexfunc-less 0.22 0.23 0.30 0.24 0.38 0.44in- 0.39 0.34 0.45 0.40 0.47 0.45un- 0.33 0.33 0.41 0.34 0.44 0.46Table 4: Mean similarity of composed vectors tohigh-quality corpus-extracted derived-form vec-tors with negative affixesfulladd and lexfunc significantly outperform stemalso in the HR subset (p<.001).
That is, the mod-els provide better approximations of derived formseven when the stem itself should already be a goodsurrogate.
The difference between the two modelsis not significant.We noted in Section 4.1 that forms containingthe ?negative?
affixes -less, un- and in- receivedon average low SDR scores, since negation im-pacts meaning more drastically than other opera-tions.
Table 4 reports the performance of the mod-els on these affixes.
Indeed, the stem baseline per-forms quite poorly, whereas fulladd, lexfunc and,to a lesser extent, wadd are quite effective in thiscondition as well, all performing greatly above thebaseline.
These results are intriguing in light ofthe fact that modeling negation is a challengingtask for DSMs (Mohammad et al, 2013) as well ascDSMs (Preller and Sadrzadeh, 2011).
To the ex-tent that our best methods have captured the negat-ing function of a prefix such as in-, they might beapplied to tasks such as recognizing lexical op-posites, or even simple forms of syntactic nega-tion (modeling inoperable is just a short step awayfrom modeling not operable compositionally).6 Experiment 2: Comparing the qualityof corpus-extracted andcompositionally generated wordsThe first experiment simulated the scenario inwhich derived forms are not in our corpus, sothat directly extracting their representation fromit is not an option.
The second experiment testsif compositionally-derived representations can bebetter than those extracted directly from the corpuswhen the latter is a possible strategy (i.e., the de-rived forms are attested in the source corpus).
Tothis purpose, we focused on those 277 test itemsthat were judged as low-quality (LQ, see Section4.1), which are presumably more challenging togenerate, and where the compositional route couldbe most useful.We evaluated the derived forms generated bycorpus stem wadd fulladd lexfuncAll 2.28 3.26 4.12 3.99 3.09HR 2.29 3.56 4.48 4.31 3.31LR 2.22 2.48 3.14 3.12 2.52Table 5: Average quality ratings of derived vectorsTarget Model Neighborsfloristwadd flora, fauna, ecosystemfulladd flora, fauna, egologistlexfunc ornithologist, naturalist, botanistsparsitywadd sparse, sparsely, densefulladd sparse, sparseness, angularitylexfunc fragility, angularity, smallnessinducementwadd induce, inhibit, inhibitionfulladd induce, inhibition, mediatelexfunc impairment, cerebral, ocularinoperablewadd operable, palliation, biopsyfulladd operable, inoperative, ventilatorlexfunc inoperative, unavoidably, flawrenamewadd name, later, namesakefulladd name, namesake, laterlexfunc temporarily, reinstate, thereafterTable 6: Examples of model-predicted neighborsfor words with LQ corpus-extracted vectorsthe models that performed best in the first exper-iment (fulladd, lexfunc and wadd), as well as thestem baseline, by means of another crowdsourcingstudy.
We followed the same procedure used toassess the quality of corpus-extracted vectors, thatis, we asked judges to rate the relatedness of thetarget forms to their NNs (we obtained on average29 responses per form).The first line of Table 5 reports the averagequality (on a 7-point scale) of the representationsof the derived forms as produced by the modelsand baseline, as well as of the corpus-harvestedones (corpus column).
All compositional modelsproduce representations that are of significantlyhigher quality (p < .001) than the corpus-basedones.
The effect is also evident in qualitativeterms.
Table 6 presents the NNs predicted by thethree compositional methods for the same LQ testitems whose corpus-based NNs are presented inTable 2.
These results indicate that morphemecomposition is an effective solution when the qual-ity of corpus-extracted derived forms is low (andthe previous experiment showed that, when theirquality is high, composition can at least approxi-mate corpus-based vectors).With respect to Experiment 1, we obtain a dif-ferent ranking of the models, with lexfunc beingoutperformed by both wadd and fulladd (p<.001),that are statistically indistinguishable.
The wadd1523composition is dominated by the stem, and bylooking at the examples in Table 6 we notice thatboth this model and fulladd tend to feature thestem as NN (100% of the cases for wadd, 73%for fulladd in the complete test set).
The questionthus arises as to whether the good performance ofthese composition techniques is simply due to thefact that they produce derived forms that are neartheir stems, with no added semantic value from theaffix (a ?stemploitation?
strategy).However, the stemploitation hypothesis is dis-pelled by the observation that both models signifi-cantly outperform the stem baseline (p<.001), de-spite the fact that the latter, again, has good per-formance, significantly outperforming the corpus-derived vectors (p < .001).
Thus, we confirmthat compositional models provide higher qual-ity vectors that are capturing the meaning of de-rived forms beyond the information provided bythe stem.Indeed, if we focus on the third row of Ta-ble 5, reporting performance on low stem-derivedrelatedness (LR) items (annotated as described inSection 4.1), fulladd and wadd still significantlyoutperform the corpus representations (p<.001),whereas the quality of the stem representations ofLR items is not significantly different form that ofthe corpus-derived ones.
Interestingly, lexfunc dis-plays the smallest drop in performance when re-stricting evaluation to LR items; however, since itdoes not significantly outperform the LQ corpusrepresentations, this is arguably due to a floor ef-fect.7 Conclusion and future workWe investigated to what extent cDSMs can gener-ate effective meaning representations of complexwords through morpheme composition.
Severalstate-of-the-art composition models were adaptedand evaluated on this novel task.
Our results sug-gest that morpheme composition can indeed pro-vide high-quality vectors for complex forms, im-proving both on vectors directly extracted from thecorpus and on a stem-backoff strategy.
This re-sult is of practical importance for distributional se-mantics, as it paves the way to address one of themain causes of data sparseness, and it confirms theusefulness of the compositional approach in a newdomain.
Overall, fulladd emerged as the best per-forming model, with both lexfunc and the simplewadd approach constituting strong rivals.
The ef-fectiveness of the best models extended also to thechallenging cases where the meaning of derivedforms is far from that of the stem, including nega-tive affixes.The fulladd method requires a vector represen-tation for bound morphemes.
A first direction forfuture work will thus be to investigate which as-pects of the meaning of bound morphemes arecaptured by our current simple-minded approachto populating their vectors, and to explore alterna-tive ways to construct them, seeing if they furtherimprove fulladd performance.A natural extension of our research is to ad-dress morpheme composition and morphologicalinduction jointly, trying to model the intuition thatgood candidate morphemes should have coherentsemantic representations.
Relatedly, in the cur-rent setting we generate complex forms from theirparts.
We want to investigate the inverse route,namely ?de-composing?
complex words to de-rive representations of their stems, especially forcases where the complex words are more frequent(e.g.
comfort/comfortable).We would also like to apply composition to in-flectional morphology (that currently lies outsidethe scope of distributional semantics), to capturethe nuances of meaning that, for example, distin-guish singular and plural nouns (consider, e.g., thedifference between the mass singular tea and theplural teas, which coerces the noun into a countinterpretation (Katz and Zamparelli, 2012)).Finally, in our current setup we focus on a singlecomposition step, e.g., we derive the meaning ofinoperable by composing the morphemes in- andoperable.
But operable is in turn composed of op-erate and -able.
In the future, we will explore re-cursive morpheme composition, especially sincewe would like to apply these methods to morecomplex morphological systems (e.g., agglutina-tive languages) where multiple morphemes are thenorm.8 AcknowledgmentsWe thank Georgiana Dinu and Nghia The Phamfor helping out with DISSECT-ion and the review-ers for helpful feedback.
This research was sup-ported by the ERC 2011 Starting Independent Re-search Grant n. 283554 (COMPOSES).1524ReferencesHerve?
Abdi and Lynne Williams.
2010.
Newman-Keuls and Tukey test.
In Neil Salkind, Bruce Frey,and Dondald Dougherty, editors, Encyclopedia ofResearch Design, pages 897?904.
Sage, ThousandOaks, CA.Harald Baayen, Richard Piepenbrock, and Leon Gu-likers.
1995.
The CELEX lexical database (re-lease 2).
CD-ROM, Linguistic Data Consortium,Philadelphia, PA.Harald Baayen.
2005.
Morphological productivity.
InRajmund Piotrowski Reinhard Ko?hler, Gabriel Alt-mann, editor, Quantitative Linguistics: An Inter-national Handbook, pages 243?256.
Mouton deGruyter, Berlin, Germany.Marco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InProceedings of EMNLP, pages 1183?1193, Boston,MA.Marco Baroni.
2009.
Distributions in text.
In AnkeLu?deling and Merja Kyto?, editors, Corpus Linguis-tics: An International Handbook, volume 2, pages803?821.
Mouton de Gruyter, Berlin, Germany.Laurie Bauer.
2001.
Morphological Productivity.Cambridge University Press, Cambridge, UK.Kenneth Beesley and Lauri Karttunen.
2000.
Finite-State Morphology: Xerox Tools and Techniques.Cambridge University Press, Cambridge, UK.Alan Black, Stephen Pulman, Graeme Ritchie, andGraham Russell.
1991.
Computational Morphol-ogy.
MIT Press, Cambrdige, MA.Gemma Boleda, Marco Baroni, Louise McNally, andNghia Pham.
2013.
Intensionality was only alleged:On adjective-noun composition in distributional se-mantics.
In Proceedings of IWCS, pages 35?46,Potsdam, Germany.John Bullinaria and Joseph Levy.
2007.
Extractingsemantic representations from word co-occurrencestatistics: A computational study.
Behavior Re-search Methods, 39:510?526.Stephen Clark.
2012.
Vector space models of lexicalmeaning.
In Shalom Lappin and Chris Fox, editors,Handbook of Contemporary Semantics, 2nd edition.Blackwell, Malden, MA.
In press.Bob Coecke, Mehrnoosh Sadrzadeh, and StephenClark.
2010.
Mathematical foundations for a com-positional distributional model of meaning.
Linguis-tic Analysis, 36:345?384.David Dowty.
1979.
Word Meaning and MontagueGrammar.
Springer, New York.Markus Dreyer and Jason Eisner.
2011.
Discover-ing morphological paradigms from plain text usinga Dirichlet process mixture model.
In Proceedingsof EMNLP, pages 616?627, Edinburgh, UK.Katrin Erk.
2012.
Vector space models of word mean-ing and phrase meaning: A survey.
Language andLinguistics Compass, 6(10):635?653.Yoav Goldberg and Reut Tsarfaty.
2008.
A single gen-erative model for joint morphological segmentationand syntactic parsing.
In Proceedings of ACL, pages371?379, Columbus, OH.John Goldsmith.
2001.
Unsupervised learning of themorphology of a natural language.
ComputationalLinguistics, 2(27):153?198.Sharon Goldwater and David McClosky.
2005.
Im-proving statistical MT through morphological anal-ysis.
In Proceedings of EMNLP, pages 676?683,Vancouver, Canada.Sharon Goldwater.
2006.
Nonparametric BayesianModels of Lexical Acquisition.
Ph.D. thesis, BrownUniversity.Emiliano Guevara.
2009.
Compositionality in distribu-tional semantics: Derivational affixes.
In Proceed-ings of the Words in Action Workshop, Pisa, Italy.Emiliano Guevara.
2010.
A regression model ofadjective-noun compositionality in distributional se-mantics.
In Proceedings of GEMS, pages 33?37,Uppsala, Sweden.Graham Katz and Roberto Zamparelli.
2012.
Quanti-fying count/mass elasticity.
In Proceedings of WC-CFL, pages 371?379, Tucson, AR.Victor Kuperman.
2009.
Semantic transparency revis-ited.
Presentation at the 6th International Morpho-logical Processing Conference.Thomas Landauer and Susan Dumais.
1997.
A solu-tion to Plato?s problem: The latent semantic analysistheory of acquisition, induction, and representationof knowledge.
Psychological Review, 104(2):211?240.Young-Suk Lee.
2004.
Morphological analysis for sta-tistical machine translation.
In Proceedings of HLT-NAACL, pages 57?60, Boston, MA.Rochelle Lieber.
2004.
Morphology and Lexical Se-mantics.
Cambridge University Press, Cambridge,UK.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Proceedings ofACL, pages 236?244, Columbus, OH.Jeff Mitchell and Mirella Lapata.
2009.
Languagemodels based on semantic composition.
In Proceed-ings of EMNLP, pages 430?439, Singapore.1525Jeff Mitchell and Mirella Lapata.
2010.
Compositionin distributional models of semantics.
Cognitive Sci-ence, 34(8):1388?1429.Saif Mohammad, Bonnie Dorr, Graeme Hirst, and Pe-ter Turney.
2013.
Computing lexical contrast.
Com-putational Linguistics.
In press.Jason Naradowsky and Sharon Goldwater.
2009.
Im-proving morphology induction by learning spellingrules.
In Proceedings of IJCAI, pages 11?17,Pasadena, CA.Ingo Plag.
1999.
Morphological Productivity: Struc-tural Constraints in English Derivation.
Mouton deGruyter, Berlin, Germany.Anne Preller and Mehrnoosh Sadrzadeh.
2011.
Bellstates and negative sentences in the distributedmodel of meaning.
Electr.
Notes Theor.
Comput.Sci., 270(2):141?153.Patrick Schone and Daniel Jurafsky.
2000.Knowledge-free induction of morphology us-ing latent semantic analysis.
In Proceedings of theConLL workshop on learning language in logic,pages 67?72, Lisbon, Portugal.Richard Socher, Eric Huang, Jeffrey Pennin, AndrewNg, and Christopher Manning.
2011.
Dynamicpooling and unfolding recursive autoencoders forparaphrase detection.
In Proceedings of NIPS, pages801?809, Granada, Spain.Richard Socher, Brody Huval, Christopher Manning,and Andrew Ng.
2012.
Semantic compositionalitythrough recursive matrix-vector spaces.
In Proceed-ings of EMNLP, pages 1201?1211, Jeju Island, Ko-rea.Richard Sproat.
1992.
Morphology and Computation.MIT Press, Cambrdige, MA.Peter Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of se-mantics.
Journal of Artificial Intelligence Research,37:141?188.Hsueh-Cheng Wang, Yi-Min Tien, Li-Chuan Hsu, andMarc Pomplun.
2012.
Estimating semantic trans-parency of constituents of English compounds andtwo-character Chinese words using Latent SemanticAnalysis.
In Proceedings of CogSci, pages 2499?2504, Sapporo, Japan.Richard Wicentowski.
2004.
Multilingual noise-robust supervised morphological analysis using thewordframe model.
In Proceedings of SIGPHON,pages 70?77, Barcelona, Spain.David Yarowsky and Richard Wicentowski.
2000.Minimally supervised morphological analysis bymultimodal alignment.
In Proceedings of ACL,pages 207?216, Hong Kong.Fabio Zanzotto, Ioannis Korkontzelos, FrancescaFalucchi, and Suresh Manandhar.
2010.
Estimat-ing linear models for compositional distributionalsemantics.
In Proceedings of COLING, pages 1263?1271, Beijing, China.1526
