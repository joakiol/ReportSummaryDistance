Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 25?32,Sydney, July 2006. c?2006 Association for Computational LinguisticsMinimum Cut Model for Spoken Lecture SegmentationIgor Malioutov and Regina BarzilayComputer Science and Artificial Intelligence LaboratoryMassachusetts Institute of Technology{igorm,regina}@csail.mit.eduAbstractWe consider the task of unsupervised lec-ture segmentation.
We formalize segmen-tation as a graph-partitioning task that op-timizes the normalized cut criterion.
Ourapproach moves beyond localized com-parisons and takes into account long-range cohesion dependencies.
Our resultsdemonstrate that global analysis improvesthe segmentation accuracy and is robust inthe presence of speech recognition errors.1 IntroductionThe development of computational models of textstructure is a central concern in natural languageprocessing.
Text segmentation is an important in-stance of such work.
The task is to partition atext into a linear sequence of topically coherentsegments and thereby induce a content structureof the text.
The applications of the derived rep-resentation are broad, encompassing informationretrieval, question-answering and summarization.Not surprisingly, text segmentation has been ex-tensively investigated over the last decade.
Fol-lowing the first unsupervised segmentation ap-proach by Hearst (1994), most algorithms assumethat variations in lexical distribution indicate topicchanges.
When documents exhibit sharp varia-tions in lexical distribution, these algorithms arelikely to detect segment boundaries accurately.For example, most algorithms achieve high per-formance on synthetic collections, generated byconcatenation of random text blocks (Choi, 2000).The difficulty arises, however, when transitionsbetween topics are smooth and distributional vari-ations are subtle.
This is evident in the perfor-mance of existing unsupervised algorithms on lessstructured datasets, such as spoken meeting tran-scripts (Galley et al, 2003).
Therefore, a morerefined analysis of lexical distribution is needed.Our work addresses this challenge by castingtext segmentation in a graph-theoretic framework.We abstract a text into a weighted undirectedgraph, where the nodes of the graph correspondto sentences and edge weights represent the pair-wise sentence similarity.
In this framework, textsegmentation corresponds to a graph partitioningthat optimizes the normalized-cut criterion (Shiand Malik, 2000).
This criterion measures both thesimilarity within each partition and the dissimilar-ity across different partitions.
Thus, our approachmoves beyond localized comparisons and takesinto account long-range changes in lexical distri-bution.
Our key hypothesis is that global analysisyields more accurate segmentation results than lo-cal models.We tested our algorithm on a corpus of spo-ken lectures.
Segmentation in this domain is chal-lenging in several respects.
Being less structuredthan written text, lecture material exhibits digres-sions, disfluencies, and other artifacts of sponta-neous communication.
In addition, the output ofspeech recognizers is fraught with high word er-ror rates due to specialized technical vocabularyand lack of in-domain spoken data for training.Finally, pedagogical considerations call for fluenttransitions between different topics in a lecture,further complicating the segmentation task.Our experimental results confirm our hypothe-sis: considering long-distance lexical dependen-cies yields substantial gains in segmentation per-formance.
Our graph-theoretic approach com-pares favorably to state-of-the-art segmentation al-gorithms and attains results close to the range ofhuman agreement scores.
Another attractive prop-25erty of the algorithm is its robustness to noise: theaccuracy of our algorithm does not deteriorate sig-nificantly when applied to speech recognition out-put.2 Previous WorkMost unsupervised algorithms assume that frag-ments of text with homogeneous lexical distribu-tion correspond to topically coherent segments.Previous research has analyzed various facets oflexical distribution, including lexical weighting,similarity computation, and smoothing (Hearst,1994; Utiyama and Isahara, 2001; Choi, 2000;Reynar, 1998; Kehagias et al, 2003; Ji and Zha,2003).The focus of our work, however, is on an or-thogonal yet fundamental aspect of this analysis?
the impact of long-range cohesion dependen-cies on segmentation performance.
In contrast toprevious approaches, the homogeneity of a seg-ment is determined not only by the similarity of itswords, but also by their relation to words in othersegments of the text.
We show that optimizing ourglobal objective enables us to detect subtle topicalchanges.Graph-Theoretic Approaches in Vision Seg-mentation Our work is inspired by minimum-cut-based segmentation algorithms developed for im-age analysis.
Shi and Malik (2000) introducedthe normalized-cut criterion and demonstrated itspractical benefits for segmenting static images.Our method, however, is not a simple applica-tion of the existing approach to a new task.
First,in order to make it work in the new linguisticframework, we had to redefine the underlying rep-resentation and introduce a variety of smoothingand lexical weighting techniques.
Second, thecomputational techniques for finding the optimalpartitioning are also quite different.
Since the min-imization of the normalized cut is NP -completein the general case, researchers in vision have toapproximate this computation.
Fortunately, wecan find an exact solution due to the linearity con-straint on text segmentation.3 Minimum Cut FrameworkLinguistic research has shown that word repeti-tion in a particular section of a text is a device forcreating thematic cohesion (Halliday and Hasan,1976), and that changes in the lexical distributionsusually signal topic transitions.Figure 1: Sentence similarity plot for a Physicslecture, with vertical lines indicating true segmentboundaries.Figure 1 illustrates these properties in a lec-ture transcript from an undergraduate Physicsclass.
We use the text Dotplotting representationby (Church, 1993) and plot the cosine similar-ity scores between every pair of sentences in thetext.
The intensity of a point (i, j) on the plot in-dicates the degree to which the i-th sentence inthe text is similar to the j-th sentence.
The truesegment boundaries are denoted by vertical lines.This similarity plot reveals a block structure wheretrue boundaries delimit blocks of text with highinter-sentential similarity.
Sentences found in dif-ferent blocks, on the other hand, tend to exhibitlow similarity.u1 u2 u3 unFigure 2: Graph-based Representation of TextFormalizing the Objective Whereas previousunsupervised approaches to segmentation restedon intuitive notions of similarity density, we for-malize the objective of text segmentation throughcuts on graphs.
We aim to jointly maximize theintra-segmental similarity and minimize the simi-larity between different segments.
In other words,we want to find the segmentation with a maximallyhomogeneous set of segments that are also maxi-26mally different from each other.Let G = {V,E} be an undirected, weightedgraph, where V is the set of nodes correspond-ing to sentences in the text and E is the set ofweighted edges (See Figure 2).
The edge weights,w(u, v), define a measure of similarity betweenpairs of nodes u and v, where higher scores in-dicate higher similarity.
Section 4 provides moredetails on graph construction.We consider the problem of partitioning thegraph into two disjoint sets of nodes A and B. Weaim to minimize the cut, which is defined to be thesum of the crossing edges between the two sets ofnodes.
In other words, we want to split the sen-tences into two maximally dissimilar classes bychoosing A and B to minimize:cut(A,B) =?u?A,v?Bw(u, v)However, we need to ensure that the two parti-tions are not only maximally different from eachother, but also that they are themselves homoge-neous by accounting for intra-partition node simi-larity.
We formulate this requirement in the frame-work of normalized cuts (Shi and Malik, 2000),where the cut value is normalized by the volumeof the corresponding partitions.
The volume of thepartition is the sum of its edges to the whole graph:vol(A) =?u?A,v?Vw(u, v)The normalized cut criterion (Ncut) is then de-fined as follows:Ncut(A,B) =cut(A,B)vol(A)+cut(A,B)vol(B)By minimizing this objective we simultane-ously minimize the similarity across partitions andmaximize the similarity within partitions.
Thisformulation also allows us to decompose the ob-jective into a sum of individual terms, and formu-late a dynamic programming solution to the mul-tiway cut problem.This criterion is naturally extended to a k-waynormalized cut:Ncutk(V ) =cut(A1, V ?A1)vol(A1)+ .
.
.+cut(Ak, V ?Ak)vol(Ak)where A1 .
.
.
Ak form a partition of the graph,and V ?Ak is the set difference between the entiregraph and partition k.Decoding Papadimitriou proved that the prob-lem of minimizing normalized cuts on graphs isNP -complete (Shi and Malik, 2000).
However,in our case, the multi-way cut is constrained topreserve the linearity of the segmentation.
By seg-mentation linearity, we mean that all of the nodesbetween the leftmost and the rightmost nodes ofa particular partition have to belong to that par-tition.
With this constraint, we formulate a dy-namic programming algorithm for exactly findingthe minimum normalized multiway cut in polyno-mial time:C [i, k] = minj<k[C [i?
1, j] +cut [Aj,k, V ?Aj,k]vol [Aj,k]](1)B [i, k] = argminj<k[C [i?
1, j] +cut [Aj,k, V ?Aj,k]vol [Aj,k]](2)s.t.
C [0, 1] = 0, C [0, k] = ?, 1 < k ?
N (3)B [0, k] = 1, 1 ?
k ?
N (4)C [i, k] is the normalized cut value of the op-timal segmentation of the first k sentences into isegments.
The i-th segment, Aj,k, begins at nodeuj and ends at node uk.
B [i, k] is the back-pointertable from which we recover the optimal sequenceof segment boundaries.
Equations 3 and 4 capturerespectively the condition that the normalized cutvalue of the trivial segmentation of an empty textinto one segment is zero and the constraint that thefirst segment starts with the first node.The time complexity of the dynamic program-ming algorithm is O(KN2), where K is the num-ber of partitions and N is the number of nodes inthe graph or sentences in the transcript.4 Building the GraphClearly, the performance of our model dependson the underlying representation, the definition ofthe pairwise similarity function, and various othermodel parameters.
In this section we provide fur-ther details on the graph construction process.Preprocessing Before building the graph, weapply standard text preprocessing techniques tothe text.
We stem words with the Porter stem-mer (Porter, 1980) to alleviate the sparsity of wordcounts through stem equivalence classes.
We alsoremove words matching a prespecified list of stopwords.27Graph Topology As we noted in the previ-ous section, the normalized cut criterion considerslong-term similarity relationships between nodes.This effect is achieved by constructing a fully-connected graph.
However, considering all pair-wise relations in a long text may be detrimen-tal to segmentation accuracy.
Therefore, we dis-card edges between sentences exceeding a certainthreshold distance.
This reduction in the graphsize also provides us with computational savings.Similarity Computation In computing pair-wise sentence similarities, sentences are repre-sented as vectors of word counts.
Cosine sim-ilarity is commonly used in text segmentation(Hearst, 1994).
To avoid numerical precisionissues when summing a series of very smallscores, we compute exponentiated cosine similar-ity scores between pairs of sentence vectors:w(si, sj) = esi?sj||si||?||sj ||We further refine our analysis by smoothing thesimilarity metric.
When comparing two sentences,we also take into account similarity between theirimmediate neighborhoods.
The smoothing isachieved by adding counts of words that occur inadjoining sentences to the current sentence featurevector.
These counts are weighted in accordanceto their distance from the current sentence:s?i =i+k?j=ie??
(j?i)sj ,where si are vectors of word counts, and ?
is aparameter that controls the degree of smoothing.In the formulation above we use sentences asour nodes.
However, we can also represent graphnodes with non-overlapping blocks of words offixed length.
This is desirable, since the lecturetranscripts lack sentence boundary markers, andshort utterances can skew the cosine similarityscores.
The optimal length of the block is tunedon a heldout development set.Lexical Weighting Previous research hasshown that weighting schemes play an importantrole in segmentation performance (Ji and Zha,2003; Choi et al, 2001).
Of particular concernare words that may not be common in general En-glish discourse but that occur throughout the textfor a particular lecture or subject.
For example, ina lecture about support vector machines, the oc-currence of the term ?SVM?
is not going to con-vey a lot of information about the distribution ofSegments per Total Word ASR WERCorpus Lectures Lecture Tokens AccuracyPhysics 33 5.9 232K 19.4%AI 22 12.3 182K ?Table 1: Lecture Corpus Statisticssub-topics, even though it is a fairly rare termin general English and bears much semantic con-tent.
The same words can convey varying degreesof information across different lectures, and termweighting specific to individual lectures becomesimportant in the similarity computation.In order to address this issue, we introduce avariation on the tf-idf scoring scheme used in theinformation-retrieval literature (Salton and Buck-ley, 1988).
A transcript is split uniformly into Nchunks; each chunk serves as the equivalent ofdocuments in the tf-idf computation.
The weightsare computed separately for each transcript, sincetopic and word distributions vary across lectures.5 Evaluation Set-UpIn this section we present the different corporaused to evaluate our model and provide a briefoverview of the evaluation metrics.
Next, we de-scribe our human segmentation study on the cor-pus of spoken lecture data.5.1 Parameter EstimationA heldout development set of three lectures is-used for estimating the optimal word block lengthfor representing nodes, the threshold distances fordiscarding node edges, the number of uniformchunks for estimating tf-idf lexical weights, thealpha parameter for smoothing, and the length ofthe smoothing window.
We use a simple greedysearch procedure for optimizing the parameters.5.2 CorporaWe evaluate our segmentation algorithm on threesets of data.
Two of the datasets we use are newsegmentation collections that we have compiledfor this study,1 and the remaining set includes astandard collection previously used for evaluationof segmentation algorithms.
Various corpus statis-tics for the new datasets are presented in Table 1.Below we briefly describe each corpus.Physics Lectures Our first corpus consists ofspoken lecture transcripts from an undergraduate1Our materials are publicly available at http://www.csail.mit.edu/?igorm/acl06.html28Physics class.
In contrast to other segmentationdatasets, our corpus contains much longer texts.A typical lecture of 90 minutes has 500 to 700sentences with 8500 words, which corresponds toabout 15 pages of raw text.
We have access bothto manual transcriptions of these lectures and alsooutput from an automatic speech recognition sys-tem.
The word error rate for the latter is 19.4%,2which is representative of state-of-the-art perfor-mance on lecture material (Leeuwis et al, 2003).The Physics lecture transcript segmentationswere produced by the teaching staff of the intro-ductory Physics course at the Massachusetts In-stitute of Technology.
Their objective was to fa-cilitate access to lecture recordings available onthe class website.
This segmentation conveys thehigh-level topical structure of the lectures.
On av-erage, a lecture was annotated with six segments,and a typical segment corresponds to two pages ofa transcript.Artificial Intelligence Lectures Our secondlecture corpus differs in subject matter, lecturingstyle, and segmentation granularity.
The gradu-ate Artificial Intelligence class has, on average,twelve segments per lecture, and a typical segmentis about half of a page.
One segment roughly cor-responds to the content of a slide.
This time thesegmentation was obtained from the lecturer her-self.
The lecturer went through the transcripts oflecture recordings and segmented the lectures withthe objective of making the segments correspondto presentation slides for the lectures.Due to the low recording quality, we were un-able to obtain the ASR transcripts for this class.Therefore, we only use manual transcriptions ofthese lectures.Synthetic Corpus Also as part of our anal-ysis, we used the synthetic corpus created byChoi (2000) which is commonly used in the eval-uation of segmentation algorithms.
This corpusconsists of a set of concatenated segments ran-domly sampled from the Brown corpus.
Thelength of the segments in this corpus ranges fromthree to eleven sentences.
It is important to notethat the lexical transitions in these concatenatedtexts are very sharp, since the segments come fromtexts written in widely varying language styles oncompletely different topics.2A speaker-dependent model of the lecturer was trainedon 38 hours of lectures from other courses using the SUM-MIT segment-based Speech Recognizer (Glass, 2003).5.3 Evaluation MetricWe use the Pk and WindowDiff measures to eval-uate our system (Beeferman et al, 1999; Pevznerand Hearst, 2002).
The Pk measure estimates theprobability that a randomly chosen pair of wordswithin a window of length k words is inconsis-tently classified.
The WindowDiff metric is a vari-ant of the Pk measure, which penalizes false posi-tives on an equal basis with near misses.Both of these metrics are defined with re-spect to the average segment length of texts andexhibit high variability on real data.
We fol-low Choi (2000) and compute the mean segmentlength used in determining the parameter k oneach reference text separately.We also plot the Receiver Operating Character-istic (ROC) curve to gauge performance at a finerlevel of discrimination (Swets, 1988).
The ROCplot is the plot of the true positive rate against thefalse positive rate for various settings of a decisioncriterion.
In our case, the true positive rate is thefraction of boundaries correctly classified, and thefalse positive rate is the fraction of non-boundarypositions incorrectly classified as boundaries.
Incomputing the true and false positive rates, wevary the threshold distance to the true boundarywithin which a hypothesized boundary is consid-ered correct.
Larger areas under the ROC curveof a classifier indicate better discriminative perfor-mance.5.4 Human Segmentation StudySpoken lectures are very different in style fromother corpora used in human segmentation studies(Hearst, 1994; Galley et al, 2003).
We are inter-ested in analyzing human performance on a corpusof lecture transcripts with much longer texts and aless clear-cut concept of a sub-topic.
We define asegment to be a sub-topic that signals a prominentshift in subject matter.
Disregarding this sub-topicchange would impair the high-level understandingof the structure and the content of the lecture.As part of our human segmentation analysis,we asked three annotators to segment the Physicslecture corpus.
These annotators had taken theclass in the past and were familiar with the subjectmatter under consideration.
We wrote a detailedinstruction manual for the task, with annotationguidelines for the most part following the modelused by Gruenstein et al (2005).
The annotatorswere instructed to segment at a level of granularity29O A B CMEAN SEG.
COUNT 6.6 8.9 18.4 13.8MEAN SEG.
LENGTH 69.4 51.5 24.9 33.2SEG.
LENGTH DEV.
39.6 37.4 34.5 39.4Table 2: Annotator Segmentation Statistics for thefirst ten Physics lectures.REF/HYP O A B CO 0 0.243 0.418 0.312A 0.219 0 0.400 0.355B 0.314 0.337 0 0.332C 0.260 0.296 0.370 0Table 3: Pk annotation agreement between differ-ent pairs of annotators.that would identify most of the prominent topicaltransitions necessary for a summary of the lecture.The annotators used the NOMOS annotationsoftware toolkit, developed for meeting segmenta-tion (Gruenstein et al, 2005).
They were providedwith recorded audio of the lectures and the corre-sponding text transcriptions.
We intentionally didnot provide the subjects with the target number ofboundaries, since we wanted to see if the annota-tors would converge on a common segmentationgranularity.Table 2 presents the annotator segmentationstatistics.
We see two classes of segmentationgranularities.
The original reference (O) and anno-tator A segmented at a coarse level with an averageof 6.6 and 8.9 segments per lecture, respectively.Annotators B and C operated at much finer levelsof discrimination with 18.4 and 13.8 segments perlecture on average.
We conclude that multiple lev-els of granularity are acceptable in spoken lecturesegmentation.
This is expected given the length ofthe lectures and varying human judgments in se-lecting relevant topical content.Following previous studies, we quantify thelevel of annotator agreement with the Pk measure(Gruenstein et al, 2005).3 Table 3 shows the an-notator agreement scores between different pairsof annotators.
Pk measures ranged from 0.24 and0.42.
We observe greater consistency at similarlevels of granularity, and less so across the two3Kappa measure would not be the appropriate measure inthis case, because it is not sensitive to near misses, and wecannot make the required independence assumption on theplacement of boundaries.EDGE CUTOFF10 25 50 100 200 NONEPHYSICS (MANUAL)PK 0.394 0.373 0.341 0.295 0.311 0.330WD 0.404 0.383 0.352 0.308 0.329 0.350PHYSICS (ASR)PK 0.440 0.371 0.343 0.330 0.322 0.359WD 0.456 0.383 0.356 0.343 0.342 0.398AIPK 0.480 0.422 0.408 0.416 0.393 0.397WD 0.493 0.435 0.420 0.440 0.424 0.432CHOIPK 0.222 0.202 0.213 0.216 0.208 0.208WD 0.234 0.222 0.233 0.238 0.230 0.230Table 4: Edges between nodes separated beyond acertain threshold distance are removed.classes.
Note that annotator A operated at a levelof granularity consistent with the original refer-ence segmentation.
Hence, the 0.24 Pk measurescore serves as the benchmark with which we cancompare the results attained by segmentation al-gorithms on the Physics lecture data.As an additional point of reference we note thatthe uniform and random baseline segmentationsattain 0.469 and 0.493 Pk measure, respectively,on the Physics lecture set.6 Experimental Results0 0.1 0.2 0.3 0.4 0.500.10.20.30.40.50.60.70.8False Positive RateTrue PositiveRateCutoff=5Cutoff=100Figure 3: ROC plot for the Minimum Cut Seg-menter on thirty Physics Lectures, with edge cut-offs set at five and hundred sentences.Benefits of global analysis We first determinethe impact of long-range pairwise similarity de-pendencies on segmentation performance.
Our30CHOI UI MINCUTPHYSICS (MANUAL)PK 0.372 0.310 0.298WD 0.385 0.323 0.311PHYSICS (ASR TRANSCRIPTS)PK 0.361 0.352 0.322WD 0.376 0.364 0.340AIPK 0.445 0.374 0.383WD 0.478 0.420 0.417CHOIPK 0.110 0.105 0.212WD 0.121 0.116 0.234Table 5: Performance analysis of different algo-rithms using the Pk and WindowDiff measures,with three lectures heldout for development.key hypothesis is that considering long-distancelexical relations contributes to the effectiveness ofthe algorithm.
To test this hypothesis, we discardedges between nodes that are more than a cer-tain number of sentences apart.
We test the sys-tem on a range of data sets, including the Physicsand AI lectures and the synthetic corpus created byChoi (2000).
We also include segmentation resultson Physics ASR transcripts.The results in Table 4 confirm our hypothesis ?taking into account non-local lexical dependencieshelps across different domains.
On manually tran-scribed Physics lecture data, for example, the al-gorithm yields 0.394 Pk measure when taking intoaccount edges separated by up to ten sentences.When dependencies up to a hundred sentences areconsidered, the algorithm yields a 25% reductionin Pk measure.
Figure 3 shows the ROC plotfor the segmentation of the Physics lecture datawith different cutoff parameters, again demon-strating clear gains attained by employing long-range dependencies.
As Table 4 shows, the im-provement is consistent across all lecture datasets.We note, however, that after some point increas-ing the threshold degrades performance, becauseit introduces too many spurious dependencies (seethe last column of Table 4).
The speaker will oc-casionally return to a topic described at the begin-ning of the lecture, and this will bias the algorithmto put the segment boundary closer to the end ofthe lecture.Long-range dependencies do not improve theperformance on the synthetic dataset.
This is ex-pected since the segments in the synthetic datasetare randomly selected from widely-varying doc-uments in the Brown corpus, even spanning dif-ferent genres of written language.
So, effectively,there are no genuine long-range dependencies thatcan be exploited by the algorithm.Comparison with local dependency modelsWe compare our system with the state-of-the-artsimilarity-based segmentation system developedby Choi (2000).
We use the publicly available im-plementation of the system and optimize the sys-tem on a range of mask-sizes and different param-eter settings described in (Choi, 2000) on a held-out development set of three lectures.
To controlfor segmentation granularity, we specify the num-ber of segments in the reference (?O?)
segmen-tation for both our system and the baseline.
Ta-ble 5 shows that the Minimum Cut algorithm con-sistently outperforms the similarity-based baselineon all the lecture datasets.
We attribute this gainto the presence of more attenuated topic transi-tions in spoken language.
Since spoken languageis more spontaneous and less structured than writ-ten language, the speaker needs to keep the listenerabreast of the changes in topic content by intro-ducing subtle cues and references to prior topics inthe course of topical transitions.
Non-local depen-dencies help to elucidate shifts in focus, becausethe strength of a particular transition is measuredwith respect to other local and long-distance con-textual discourse relationships.Our system does not outperform Choi?s algo-rithm on the synthetic data.
This again can be at-tributed to the discrepancy in distributional prop-erties of the synthetic corpus which lacks coher-ence in its thematic shifts and the lecture corpusof spontaneous speech with smooth distributionalvariations.
We also note that we did not try to ad-just our model to optimize its performance on thesynthetic data.
The smoothing method developedfor lecture segmentation may not be appropriatefor short segments ranging from three to elevensentences that constitute the synthetic set.We also compared our method with anotherstate-of-the-art algorithm which does not explic-itly rely on pairwise similarity analysis.
This algo-rithm (Utiyama and Isahara, 2001) (UI) computesthe optimal segmentation by estimating changes inthe language model predictions over different par-titions.
We used the publicly available implemen-31tation of the system that does not require parame-ter tuning on a heldout development set.Again, our method achieves favorable perfor-mance on a range of lecture data sets (See Ta-ble 5), and both algorithms attain results close tothe range of human agreement scores.
The attrac-tive feature of our algorithm, however, is robust-ness to recognition errors ?
testing it on the ASRtranscripts caused only 7.8% relative increase inPk measure (from 0.298 to 0.322), compared toa 13.5% relative increase for the UI system.
Weattribute this feature to the fact that the model isless dependent on individual recognition errors,which have a detrimental effect on the local seg-ment language modeling probability estimates forthe UI system.
The block-level similarity func-tion is not as sensitive to individual word errors,because the partition volume normalization factordampens their overall effect on the derived mod-els.7 ConclusionsIn this paper we studied the impact of long-rangedependencies on the accuracy of text segmenta-tion.
We modeled text segmentation as a graph-partitioning task aiming to simultaneously opti-mize the total similarity within each segment anddissimilarity across various segments.
We showedthat global analysis of lexical distribution im-proves the segmentation accuracy and is robustin the presence of recognition errors.
Combin-ing global analysis with advanced methods forsmoothing (Ji and Zha, 2003) and weighting couldfurther boost the segmentation performance.Our current implementation does not automati-cally determine the granularity of a resulting seg-mentation.
This issue has been explored in thepast (Ji and Zha, 2003; Utiyama and Isahara,2001), and we will explore the existing strategiesin our framework.
We believe that the algorithmhas to produce segmentations for various levels ofgranularity, depending on the needs of the appli-cation that employs it.Our ultimate goal is to automatically generatetables of content for lectures.
We plan to in-vestigate strategies for generating titles that willsuccinctly describe the content of each segment.We will explore how the interaction between thegeneration and segmentation components can im-prove the performance of such a system as awhole.8 AcknowledgementsThe authors acknowledge the support of the National Sci-ence Foundation (CAREER grant IIS-0448168, grant IIS-0415865, and the NSF Graduate Fellowship).
Any opinions,findings, conclusions or recommendations expressed in thispublication are those of the author(s) and do not necessar-ily reflect the views of the National Science Foundation.
Wewould like to thank Masao Utiyama for providing us with animplementation of his segmentation system and Alex Gru-enstein for assisting us with the NOMOS toolkit.
We aregrateful to David Karger for an illuminating discussion onthe Minimum Cut algorithm.
We also would like to acknowl-edge the MIT NLP and Speech Groups, the three annotators,and the three anonymous reviewers for valuable comments,suggestions, and help.ReferencesD.
Beeferman, A. Berger, J. D. Lafferty.
1999.
Statisticalmodels for text segmentation.
Machine Learning, 34(1-3):177?210.F.
Choi, P. Wiemer-Hastings, J. Moore.
2001.
Latent se-mantic analysis for text segmentation.
In Proceedings ofEMNLP, 109?117.F.
Y. Y. Choi.
2000.
Advances in domain independent lineartext segmentation.
In Proceedings of the NAACL, 26?33.K.
W. Church.
1993.
Char align: A program for aligningparallel texts at the character level.
In Proceedings of theACL, 1?8.M.
Galley, K. McKeown, E. Fosler-Lussier, H. Jing.
2003.Discourse segmentation of multi-party conversation.
InProceedings of the ACL, 562?569.J.
R. Glass.
2003.
A probabilistic framework for segment-based speech recognition.
Computer Speech and Lan-guage, 17(2?3):137?152.A.
Gruenstein, J. Niekrasz, M. Purver.
2005.
Meeting struc-ture annotation: Data and tools.
In Proceedings of theSIGdial Workshop on Discourse and Dialogue, 117?127.M.
A. K. Halliday, R. Hasan.
1976.
Cohesion in English.Longman, London.M.
Hearst.
1994.
Multi-paragraph segmentation of exposi-tory text.
In Proceedings of the ACL, 9?16.X.
Ji, H. Zha.
2003.
Domain-independent text segmentationusing anisotropic diffusion and dynamic programming.
InProceedings of SIGIR, 322?329.A.
Kehagias, P. Fragkou, V. Petridis.
2003.
Linear text seg-mentation using a dynamic programming algorithm.
InProceedings of the EACL, 171?178.E.
Leeuwis, M. Federico, M. Cettolo.
2003.
Language mod-eling and transcription of the ted corpus lectures.
In Pro-ceedings of ICASSP, 232?235.L.
Pevzner, M. Hearst.
2002.
A critique and improvementof an evaluation metric for text segmentation.
Computa-tional Linguistics, 28(1):pp.
19?36.M.
F. Porter.
1980.
An algorithm for suffix stripping.
Pro-gram, 14(3):130?137.J.
Reynar.
1998.
Topic segmentation: Algorithms and appli-cations.
Ph.D. thesis, University of Pennsylvania.G.
Salton, C. Buckley.
1988.
Term weighting approachesin automatic text retrieval.
Information Processing andManagement, 24(5):513?523.J.
Shi, J. Malik.
2000.
Normalized cuts and image segmenta-tion.
IEEE Transactions on Pattern Analysis and MachineIntelligence, 22(8):888?905.J.
Swets.
1988.
Measuring the accuracy of diagnostic sys-tems.
Science, 240(4857):1285?1293.M.
Utiyama, H. Isahara.
2001.
A statistical model fordomain-independent text segmentation.
In Proceedings ofthe ACL, 499?506.32
