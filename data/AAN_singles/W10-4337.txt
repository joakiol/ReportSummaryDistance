Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 213?216,The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational LinguisticsInvestigating Clarification Strategies in aHybrid POMDP Dialog ManagerSebastian Varges and Silvia Quarteroni and Giuseppe Riccardi and Alexei V. IvanovDepartment of Information Engineering and Computer ScienceUniversity of Trento, 38050 Povo di Trento, Italy{varges|silviaq|riccardi|ivanov}@disi.unitn.itAbstractWe investigate the clarification strategiesexhibited by a hybrid POMDP dialogmanager based on data obtained from aphone-based user study.
The dialog man-ager combines task structures with a num-ber of POMDP policies each optimized forobtaining an individual concept.
We in-vestigate the relationship between dialoglength and task completion.
In order tomeasure the effectiveness of the clarifica-tion strategies, we compute concept pre-cisions for two different mentions of theconcept in the dialog: first mentions andfinal values after clarifications and simi-lar strategies, and compare this to a rule-based system on the same task.
We ob-serve an improvement in concept precisionof 12.1% for the hybrid POMDP com-pared to 5.2% for the rule-based system.1 IntroductionIn recent years, probabilistic models of dialoghave been introduced into dialog management, thepart of the spoken dialog system that takes the ac-tion decision.
A major motivation is to improverobustness in the face of uncertainty, in particu-lar due to speech recognition errors.
The inter-action is characterized as a dynamic system thatmanipulates its environment by performing dialogactions and perceives feedback from the environ-ment through its sensors.
The original sensory in-formation is obtained from the speech recognition(ASR) results which are typically processed by aspoken language understanding module (SLU) be-fore being passed on to the dialog manager (DM).The seminal work of (Levin et al, 2000) mod-eled dialog management as a Markov DecisionProcess (MDP).
Using reinforcement learning asthe general learning paradigm, an MDP-based di-alog manager incrementally acquires a policy byobtaining rewards about actions it performed inspecific dialog states.
As we found in earlier ex-periments, an MDP can learn to gradually drop theuse of clarification questions if there is no noise.This is due to the fact that clarifications do notimprove the outcome of the dialog, i.e.
the re-ward.
However, with extremely high levels ofnoise, the learner prefers to end the dialog imme-diately (Varges et al, 2009).
In contrast to deliber-ate decision making in the pragmatist tradition ofdialog processing, reinforcement learning can beregarded as low-level decision making.MDPs do not account for the observational un-certainty of the speech recognition results, a keychallenge in spoken dialog systems.
Partially Ob-servable Markov Decision Process (POMDPs) ad-dress this issue by explicitly modeling how the dis-tribution of observations is governed by states andactions.In this work, we describe the evaluation of adivide-and-conquer approach to dialog manage-ment with POMDPs that optimizes policies foracquiring individual concepts separately.
Thismakes optimization much easier and allows us tomodel the confusability of concrete concept valuesexplicitly.
This also means that different clarifica-tion strategies are learned for individual conceptsand even individual concept values.
The use of thePOMDP policies is orchestrated by an explicit taskstructure, resulting in a hybrid approach to dialogmanagement.
The evaluation involved a user studyof 20 subjects in a tourist information domain.
Thesystem is compared against a rule-based baselinesystem in the same domain that was also evaluatedwith 20 subjects.2 Hybrid POMDP dialog managementIn this section we introduce the hybrid POMDP di-alog manager that was used in the data collection.2132.1 Concept-level POMDPsThe domain is a tourist information system thatuses 5 different policies that can be used in 8different task roles (see below).
For each con-cept we optimized an individual policy.
Thenumber of states of the POMDP can be lim-ited to the concept values, for example a loca-tion name such as trento.
The set of ac-tions consists of a question to obtain the concept(e.g.
question-location), a set of clari-fication actions (e.g.
verify-trento) and aset of submit actions (e.g.
submit-trento).POMDP modeling including a heuristically set re-ward structure follows the (simpler) ?tiger prob-lem?
that is well-known in the AI community(Kaelbling et al, 1998): the system has a num-ber of actions to obtain further information whichit can try and repeat in any order until it is readyto commit to a concept value.
For optimization weused the APPL solver (Kurniawati et al, 2008).2.2 Task structure and dialog managementThe use of individual policies is orchestrated byan explicit task structure that activates and de-activates them.
The task structure is essentiallya directed AND-OR graph with a common rootnode.
The dialog manager maintains a separate be-lief distribution for each concept.
Figure 1 showsthe general system architecture with a schematicview of the task structure, and additionally a moredetailed view of an active location node.
In theexample, the root node has already finished andthe system is currently obtaining the location for alodging task.
The term ?role?
refers to a concept?spart in the task, for example a month may be thecheck-in or check-out month for accommodationbooking.At the beginning of a dialog, the task structure isinitialized by activating the root node.
A top levelfunction activates nodes of the task structure andpasses control to that node.
Each node maintainsa belief bc for a concept c, which is used to rankthe available actions by computing the inner prod-uct of policy vectors and belief.
The top-rankedaction am is selected by the system, i.e.
it is ex-ploiting the policy, and passed to the natural lan-guage generator (NLG).
Next, the top-ranked SLUresults for the active node and concept are used asobservation zu,c to update the belief to b?c, whichUser?ASR?TTS?SLU?NLG?PASSIVE?BLOCKED?ACTIVE?BLOCKED?BLOCKED?
BLOCKED?OPEN?
OPEN?am?zu,c?Condi?n :????????????(ac?vity=????????????????????????????????????lodging-??enquiry?????????????????????????????????????lodging-??reserva?n),?ConceptName??????loca?n,?ConceptRole:????????loca?n-??lodging,?Status:????????????????????ACTIVE,?Belief:?Ac?on:????????????????????ques?n-??loca?n.
?zu,d?DM?Figure 1: System architecture with Task Structure(task node example in detailed view)follows the standard method for POMDPs:b?c(s?)
=?s?Sbc(s) T (s, am, s?)
O(a, s?, zu,c)/pzu,c(1)where probability b?c(s?)
is the updated belief ofbeing in state s?, which is computed as the sum ofthe probabilities of transitioning from all previousbelief points s to s?
by taking machine action amwith probability T (s, am, s?)
and observing zu,cwith (smoothed) probability O(am, s?, zu,c).
Nor-malization to obtain a valid probability distribu-tion is performed by dividing by the probability ofthe observation pzu,c .A concept remains active until a submit actionis selected.
At that point, the next active node isretrieved from the task structure and immediatelyused for action selection with an initially uniformbelief.
Submit actions are not communicated tothe user but collected and used for the databasequery at the end of the dialog.Overanswering, i.e.
the user providing more in-formation than directly asked for, is handled by de-layed belief updating: the SLU results are storeduntil the first concept of a matching type becomesactive.
This is a heuristic rule designed to ensurethat a concept is interpreted in its correct role.
Op-erationally, unused SLU results zu,d (where con-cept d 6= c) are passed on to the next activatedtask node (see also figure 1).3 Experiments and data analysisWe conducted user studies with two systems in-volving 20 subjects and 8 tasks in each study.The systems use a Voice XML platform to driveASR and TTS components.
Speech recognition is214Lodging Task Event EnquiryTCR #turns TCR #turnsRule-based DM 75.5% 13.7 66.7% 8.7(40/53) (?=4.8) (28/42) (?=3.3)POMDP-DM 78.1% 23.0 84.3% 14.4(50/64) (?=8.8) (27/32) (?=4.5)Table 1: Task completion and length metricsbased on statistical language models for the open-ing prompt, and is grammar-based otherwise.
Onesystem used the hybrid POMDP-DM, the otheris a rule-based dialog manager that uses explicit,heuristically set confidence thresholds to triggerthe use of clarification questions (Varges et al,2008).Dialog length and task completion Table 1shows task completion rates (?TCR?)
and dura-tions (?#turns?)
for the POMDP and rule-basedsystems.
Task completion in this metric is definedas the number of tasks of a certain type that weresuccessfully concluded.
Duration is measured inthe number of turn pairs consisting of a systemaction followed by a user action.
We combinethe counts for two closely related lodging tasks.The number of tasks is shown in brackets.
Table1 shows that the POMDP-DM successfully con-cludes more and longer lodging tasks and almostas many event tasks.
In general, the POMDP poli-cies can be described as more cautious althoughobviously the dialog length of the rule system de-pends on the chosen thresholds.Concept precision at the value level In orderto measure the effect of the clarification strategiesin both systems, we computed concept precisionsfor two different mentions of a concept in a dialog(table 2): first mentions and final values after clar-ifications and similar strategies.
The rationale forthis metric is that the last mentioned concept valueis the value that the system ultimately obtains fromthe user, which is used in the database query:?
if the system decides not to use clarifications,the only mentioned value is the accepted one,?
if the system verifies and obtains a positiveanswer, the last mentioned value is the ac-cepted one,?
if the system verifies and obtains a negativeanswer, the user will mention a new value(which may or may not be accepted).Thus, this metric is a uniform way of capturingthe obtained values from systems that internallyRule-based DM POMDP-DMfirst final ?% first final ?%a) activity 0.78 0.74 -4.1 0.83 0.88 5.0b) location 0.64 0.74 15.8 0.69 0.73 6.3c) starrating 0.67 0.70 3.4 0.90 0.97 7.7d) month 0.85 0.89 4.3 0.76 0.86 12.7e) day 0.70 0.76 8.3 0.61 0.76 25.3ALL (a-e) 0.74 0.78 5.2 0.74 0.83 12.1Clarifications 0.84 0.85 1.5 0.96 0.87 -8.8Table 2: Concept precision of first vs final valueuse very different dialog managers and representa-tions.
The actual precision of a conceptC is calcu-lated by comparing SLU results to annotations andcounting true positives (matchesM ) and false pos-itives (separated into mismatches N and entirelyun-annotated concepts U ): Prec(C) = MM+N+U .Unrecognized concepts, on the other hand, are re-call related and not counted since they cannot bepart of any system belief.As table 2 clearly shows, the use of clarificationstrategies has a positive effect on concept preci-sion in both systems.
The exception is the preci-sion of concept activity in the rule-based systemfor which the system reprompted rather than ver-ified.1 In table 2, row ?All?
refers to the averageweighted precision of the five concepts.
Both sys-tems start from a similar level of overall precision.The relative improvement of the POMDP-DM forall concepts is 12.1%, compared to 5.2% of therule-based DM.We conducted a statistical significance test bycomputing the delta in the form of three values forindividual data points, i.e.
dialogs, and assigned+1 for all changes from non-match to match, -1for a change in the opposite direction and 0 for ev-erything else (e.g.
from mismatch to mismatch).We found that, although there is a tendency forthe POMDP-DM to perform better, the differenceis not statistically significant at p=0.05 (a possi-ble explanation is the data size since we are usinghuman subjects).We furthermore measured the precision of rec-ognizing ?yes/no?
answers to clarification ques-tions.
In contrast to actual concepts, there is no be-lief distribution for these in the DM since clarifica-tion actions are part of the concept POMDP mod-els.
We are thus dealing with individual one-offrecognition results that should be entirely indepen-dent of each other.
However, as table 2 (bottom)1The second value obtained may be incorrect but abovethe confidence threshold; note that the rule system does notmaintain a belief distribution over values.215shows, the precision of verifications decreases forthe hybrid POMDP system.
A plausible expla-nation for this is the increasing impatience of theusers due to the longer dialog duration.Characterization of dialog strategies Forsome concepts, the best policy is to ask theconcept question once and then verify once beforecommitting to the value (assuming the answer ispositive).
Other policies verify the same valuetwice.
Another learned strategy is to ask the orig-inal concept question twice and then only verifythe value once (assuming that the understoodvalue was the same in both concept questions).
Inother words, the individual concept policies showdifferent types of strategies regarding uncertaintyhandling.
This is in marked contrast to themanually programmed DM that always asks theconcept question once and verifies it if needed(concept activity being the exception).HCI and language generation The domain issufficiently simple to use template-based genera-tion techniques to produce the surface forms ofthe responses.
However, the experiments with thePOMDP-DM highlight some new challenges re-garding HCI aspects of spoken dialog systems: thechoice of actions may not be ?natural?
from theuser?s perspective, for example if the system asksfor a concept twice.
However, it should be possi-ble to better communicate the (change in the) be-lief to the user.4 Related workThe pragmatist tradition of dialog processing usesexplicit representations of dialog structure to takedecisions about clarification actions.
These mod-els are more fine-grained and often deal with writ-ten text, e.g.
(Purver, 2006), whereas in spo-ken dialog systems a major challenge is managingthe uncertainty of the recognition.
Reinforcementlearning approaches to dialog management learndecisions from (often simulated) dialog data in aless deliberative way.
For example, the Hidden In-formation State model (Young et al, 2010) uses areduced summary space that abstracts away manyof the details of observations and dialog state, andmainly looks at the confidence scores of the hy-potheses.
This seems to imply that clarificationstrategies are not tailored toward individual con-cepts and their values.
(Bui et al, 2009) uses fac-tored POMDP representations that seem closest toour approach.
However, the effect of clarificationsdoes not seem to have been investigated.5 ConclusionsWe presented evaluation results for a hybridPOMDP system and compared it to a rule-basedone.
The POMDP system achieves higher con-cept precision albeit at the cost of longer dialogs,i.e.
there is an empirically measurable trade-offbetween concept precision and dialog length.AcknowledgmentsThis work was partially supported by the Eu-ropean Commission Marie Curie ExcellenceGrant for the ADAMACH project (contract No.022593).ReferencesT.H.
Bui, M. Poel, A. Nijholt, and J. Zwiers.
2009.A tractable hybrid DDN-POMDP approach to affec-tive dialogue modeling for probabilistic frame-baseddialogue systems.
Natural Language Engineering,15(2):273?307.Leslie Pack Kaelbling, Michael L. Littman, and An-thony R. Cassandra.
1998.
Planning and acting inpartially observable stochastic domains.
ArtificialIntelligence, 101:99?134.H.
Kurniawati, D. Hsu, andW.S.
Lee.
2008.
SARSOP:Efficient point-based POMDP planning by approxi-mating optimally reachable belief spaces.
In Proc.Robotics: Science and Systems.E.
Levin, R. Pieraccini, and W. Eckert.
2000.
Astochastic model of human-machine interaction forlearning dialog strategies.
IEEE Transactions onSpeech and Audio Processing, 8(1).Matthew Purver.
2006.
CLARIE: Handling clarifica-tion requests in a dialogue system.
Research on Lan-guage and Computation, 4(2-3):259?288, October.Sebastian Varges, Giuseppe Riccardi, and Silvia Quar-teroni.
2008.
Persistent information state in a data-centric architecture.
In Proceedings of the 9th SIG-dial Workshop on Discourse and Dialogue, Colum-bus, Ohio.Sebastian Varges, Giuseppe Riccardi, Silvia Quar-teroni, and Alexei V. Ivanov.
2009.
The explo-ration/exploitation trade-off in reinforcement learn-ing for dialogue management.
In Proceedings ofIEEE Automatic Speech Recognition and Under-standing Workshop (ASRU).S.
Young, M. Gasic, S. Keizer, F. Mairesse, J. Schatz-mann, B. Thomson, and K. Yu.
2010.
The Hid-den Information State Model: a practical frameworkfor POMDP-based spoken dialogue management.Computer Speech and Language, 24:150?174.216
