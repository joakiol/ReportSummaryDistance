Semant ic  Network  Ar ray  P rocessor  as aMass ive ly  Para l le l  Comput ing  P la t fo rm forH igh  Per fo rmance  and  Large-Sca le  Natura l  Language Process ing*Hiroaki KitanoCenter for Machine TranslationCarnegie Mellon UniversityPittsburgh, PA 15213 U.S.A.AbstractThis paper demonstrates the utility ofthe Semantic Network Array Processor(SNAP) as a massively parallel platformfor high performance and large-scale nat-ural language processing systems.
SNAPis an experimental massively parallel ma-chine which is dedicated to, but not lim-ited to, the natural language processing us-hag semantic networks.
In designing theSNAP, we have investigated various natu-ral language processing systems and the-ories to determine the scope of the hard-ware support and a set of micro-codedinstructions to be provided.
As a re-suit, SNAP employs an extended marker-passing model and a dynamically modi-fiable network model.
A set of primi-tive instructions i micro-coded to directlysupport a parallel marker-passing, bit-operations, numeric operations, networkmodifications, and other essential func-tions for natural language processing.
Thispaper demonstrates the utility of SNAPfor various paradigms of natural anguageprocessing.
We have discovered that theSNAP provides milliseconds or microsec-onds performance on several important ap-plicatious uch as the memory-based pars-ing and translation, classificatlon-basedparsing, and VLKB search.
Also, we ar-gue that there are numerous opportunitiesin the NLP community to take advantagesof the comlmtational power of the SNAP.1.
I n t roduct ionIn order to accomplish the high-performance naturallanguage processing, we have designed a highly par-allel machine called Semantic Network Array Proces-sor (SNAP) \[Lee and Moldovan, 1990\].
The goal ofour project is to develop and test the validity of themassively parallel machine for high performance andlarg-scale natural anguage processing.
Thus, the ar-chitecture of the SNAP was determined reflecting ex-tensive analysis of basic operations essential to the"This research is Bupported by the National ScienceFoundation under grant MIP-9009111 and MIP-9009109,and conducted as a part of IMPACT (InternationMConsortium for Massively Parallel Advanced ComputingTechnologies)Dan MoldovanDepartment of Electrical Engineering SystemsUniversity of Southern CaliforniaLos Angeles,  CA 90089-1115 U.S.A.natural language processing.
As a result of the in-vestigation, we have decided to employ an extendedmarker-passing model and a dynamically modifiablenetwork.
Also, a set of primitive instructions i micro-coded to directly support essential operations in nat-ural language systems.Several approach can be taken to use SNAP as aplatform for natural anguage processing systems.
Wecan fully implement NLP system on SNAP, or we canspeed up existing systems by implementing compu-tationally expensive part on SNAP.
We have hnple-mented some of these approaches on SNAP, mid ob-tained extremely high performance (order of millisec-onds for given tasks).In this paper, we describe the design philosophyand architecture of SNAP, and present several ap-proaches toward high performance natural anguageprocessing systems on SNAP.2.
SNAP Arch i tec ture2.1.
Design Ph i losophy of SNAPThe Semantic Network Array Processor (SNAP) isa highly parallel array processor fully optindzed tbrsemantic network processing with a marker-passingmechanism.
The fundermental design decisions arc(1) a semantic network as a knowledge representationscheme, and (2) parallel marker-passing as an infer-ence mechauism.First, the use of a semantic network as a represemtation scheme can be justified from the fact that mostof the representation schemes of current AI and NLPtheories (such as frame, feature structure, sort hier-archy, systemic hoice network, neural network, etc.
)can be mapped onto semantic networks.
Also, tlmreare numbers of systems and models which directly usesemantic networks \[Sown, 1991\].Second, the use of marker-passing can be jus-tified from several aspects.
Obviously, there aremany AI and NLP models which use some formof marker-passing as the central computing princi-ple.
For example, there are significant number of re-search being done on word-sense disambiguation asscene in Waltz and Pollack 1985\] Itendler, 1988\],\[Hirst, 1986, \[Charniak, 1983\], \[Tomabechi, 1987,etc.
All of them assume passing of markers or val-ues among nodes interconnected via some types oflinks.
There are studies to handle syntactic on-ACRES DE COLING-92.
NANaXS, 23-28 AO~" 1992 8 1 3 Paoc.
ov COLING-92.
NAI, rVES, AUG. 23-28, 1992tsmlttamtm4 ~et tmmant|NAP.1 ~ Cam~oan@~a?
c .
J  ~t~ ss^pFigure I: SNAP-1 Architecturestraints using some type of networks which can bemapped onto semantic networks.
Recent studies onthe Classification-Based Parsing \[Kasper, 1989\] andthe Systemic Choice Network \[Carpenter and Pollard~1991\] assume hierarchical networks to represent var-ions linguistic constraints, and the search on thesenetworks can be done by marker-passing.
Also, thereare more radical approaches to implement entire natu-ral language systems using parallel marker-passing asseen in \[Norvig, 1986\], \[Riesbeck and Martin, 1985\],\[Tomabechi, 1987\], and \[Kitano, 1991\].
There are,however, differences in types of information carriedin each marker-passing model.
We will describe ourdesign decisions later.As reported in \[Evett, at.
al., 1990\], however, serialmachines are not suitable for such processing becauseit causes performance degradation as a size of seman-tic network increases.
There are clear needs for highlyparallel machines.
The rest of this section provides abrief overview of the SNAP architecture.2.2.
The Arch i tec tureSNAP consists of a processor array and an array con-troller (Figure 1).
The processor array has processingcells which contain the nodes and hnks of a semanticnetwork.
The SNAP array consists of 160 process-ing elements each of which consists of a TMS320C30DSP chip, local SRAM, etc.
Each processing ele-ments stores 1024 nodes which act as virtual proces-sors.
They are interconnected via a modified hyper-cube network.
The SNAP controller interfaces theSNAP array with a SUN 3/280 host and broadcastsinstructions to control the operation of the array.
Theinstructions for the array are distributed through aglobal bus by the controller.
Propagation of markersand the execution of other instructions can be pro-ceased simultaneously.2.3.
Parallel Marker -Pass ingIn the SNAP, content of the marker are: (1) bit-vector, (2) address, and (3) numeric value (integeror floating point).
In SNAP, the size of the markeris fixed.
According to the classification i  \[Blelloch,1986\], our model is a kind of Finite Message Pass-ing.
There are types of marker-, or message-, pa~ingthat propagates feature structures (or graphs)~ whichare called Unbounded Message Passing.
Although wehave extended our marker-passing model from the tra-ditional bit marker-passing to the complex marker-passing which carries bits, address, and numeric val-ues, we decided not to carry unbounded messages.This is because propagation of feature structures andheavy symbolic operations at each PE are not prac-tical assumptions to make, at least, on current mas-sively parallel machines due to processor power, mem-ory capacity on each PE, and the communication bot-tleneck.
Propagation of feature structures would im-pose serious hardware design problems ince the sizeof the message is unbounded, which means that thedesigner can not be sure if the local memory size issufficient or not until the machine actually runs someapplications.
Also, PEa capable of performing oper-ations to manipulate these messages (such as unifi-cation) would be large in physical size which causesassembly problems when thousands of processors areto be assembled into one machine.
Since we decidenot to support unbounded message passing, we decideto support functionalities attained by the unboundedmessage passing by other means uch as sophisticatedmarker control rules, dynamic network modifications,etc.2.4.
Instruct ion SetsA set of 30 high-level instructions specific to semanticnetwork processing are implemented directly in hard-ware.
These include associative search, marker set-ting and propagation, logical/arithmetic operationsinvolving markers, create and delete nodes and re-lations, and collect a list of nodes with a certainmarker set.
Currently, the instruction set can becalled from C language so that users can develop ap-plications with an extended version of C language.From the programming level, SNAP provides data-parallel programming environment similar to C* ofthe Connection Machine \]Thinking Machines Corp.,1989\], but specialized for semantic network process-ing with marker passing.Particularly important is the marker propagationrules.
Several marker propagation rules are providedto govern the movement of markers.
Marker propa-gation rules enables us to implement guided, or con-straint, marker passing as well as unguided markerpassing.
This is done by specifying the type of linksthat markers can propagate.
The following are someof the propagation rules of SNAP:e Seq(rl, r~): The Seq (sequence) propagation ruleallows the marker to propagate through rl oncethen to r~.?
Spread(rl,r2) : The Spread propagation rule al-lows the marker to travel through a chain of r llinks and then r~ links.
* Comb(rl,r~) : The Comb (combine) propagationrule allows the marker to propagate to all rl andr~ links without limitation.2.5.
Knowledge Representat ion  on SNAPSNAP provides four knowledge representation ele-ments: node, link, node color and link value.
Theseelements offer a wide range of knowledge representa-tion schemes to be mapped on SNAP.
On SNAP) aconcept is represented by a node.
A relation can berepresented by either a node called relation node orAcrEs DE COLING-92, NANTES.
23-28 AOt)r 1992 8 1 4 PROC.
OF COLING-92, NANTES.
AUG. 23-28.
1992a link between two nodes.
The node color indicatesthe type of node.
For example, when representing USCis  in Los Angeles and CW0 ie in Pittsbnrgh~ wemay assign a relation node for IN.
The IN node isshared by the two facts.
In order to prevent the wronginterpretations such as USC in P i t t sburgh  and CSllin Lea Angeles, we assigu IN#I  and IN#2 to twodistinct IN relations, and group the two relation odesby a node color IN.
Each lhlk has assigned to it a linkvalue which indicates the strength of interconcepts re-lations.
This link value supports probabilistic reason-ing and connectionist-like processing.
These four basicelements allow SNAP to support virtually any kindof graph-based knowledge representation formalismssuch as KL-ONE \[Braehman and Schmolze, 1985\],Conceptual Graphs \[Sown, 1984\], KODIAK \[Wilen-sky, 1987\], etc.3.
The  Memory-Based  Natura lLanguage Process ingMemory-baaed NLP is an idea of viewing NLP as amemory activity.
For example, parsing is consideredas a memoryosearch process which identifies imilareases in the past from the memory, and to provideinterpretation based on the identified case.
It can beconsidered as an application of Memory-Baaed l~.ea-soning (MBR) \[Stm~fill and Waltz, 1986\] and Case-Based Reasoning (CBR) \[Riesbeck and Schank, 1989\]to NLP.
This view~ however, counters to traditionalidea to view NLP as arl extensive rule application pro-cess to build up meaning representation.
Some mod-els has been proposed in this direction, such as DirectMemory Access Parsing (DMAP) \[Riesbeck and Mar-tin, 1985\] and q~DMDIALOO \[Kitano, 1991\].
For ar-guments concerning superiority of the metnory-basedapproach over the traditional approach, ace \[Nagao,1984\], \[Riesbeck and Martin, 1985\], and \[Sumita nd\]\[ida, 1991\].DMSNAP is a SNAP implementation of the(I)DMDIALOG speech-to-speech dialogue translationsystem which is based on, in part, the memory-basedapproach.
Naturally, it inherits basic ideas and mech-anisms of the ~DMDIALOG system such as a memory-based approach to natural language processing andparallel marker-passing.
Syntactic onstraint networkis introduced in DMSNAP whereas ODMDIALOG hasbeen assuming unification operation to handle linguis-tic processing.DMSNAP consists of the nlemory network, syntac-tic constraint network, and markers to carry out in-ference.
The memory network and the syntactic on-straint network are compiled from a set of grammarrules written for DMSNAP.Memory  Network  on SNAP The major types ofknowledge required for language translation in DM-SNAP are: a lexicon, a concept ype hierarchy, con-cept sequences, and syntactic onstraints.
Amongthem, the syntactic constraints are represented inthe syntactic onstraint network, and the rest of theknowledge is represented in the memory network.
Thememory network consists of various types of nodessuch as concept sequence class (CSC), lexical itemnode* (LEX), concept nodes (CC) and others.
Nodesare connected by a number of different links such asconcept ahstraction links (ISA), expression links forboth source language and target language (ENG andJPN), Role links (ROLE), constraint links (CON-STRAINT),  contextual llnk~ (CONTEXT) and oth-ers.
A part of the menmry network is shown in Figure2.Markers  The processing of natural anguage on amarker-propagation architecture quires the creationand movement of markers on the memory network.The following types of markers are used: (1) A-Markers indicate activation of nodes.
They propa-gate ttlrough ISA links upward, carry a pointer totile source of activation axtd a cost measure, (2) P-Markers indicate the next possible nodes to be acti-vated.
They are initially placed on the first elementnodes of the CSGs, and move through NEXT linkwhere they collide with A-MARKERs at tile elementnodes, (3) G-Markers indicate activation of nodes intile target language, They carry pointers to the lexi-eal node to he lexicalized, and propagate througtl ISAlinks upward, (4) V-Markers indicate current state ofthe verbalization.
When a V-MARKER collides withthe G-MARKER, the surface string (which is specifiedby the pointer in the G-MARKER) is verbalized, (5)G-Markers indicate contextual priming.
Nodes withC-MAItKERs are contextually primed.
A C-MARKERmoves from the designated contextual root node toother contextually relevant nodes through contextuallinks, and (6) SC-Markers indicate active syntax con-attaints, and primed and/or inhibited nodes by cur-rently active syntactic onstraints.
It also carriespointer to specific nodes.
There are some other mark-era used for control process and tinting; they are notdescribed here.The parsing algorithm is sinular to the shift-reduceparser except hat our algorithms handle ambiguities,parallel processing of each hypothesis, and top-downpredictions of possible next input symbol.
The gen-eration algorithm implemented on SNAP is a versionof the lexically guided bottom-up algorithm which isdescribed in Kitano 1990\].
Details of the algorithmis described in Kitano et.
al., 1991b.DmSNAP can handle various linguistic l)henomenasuch as: lexical ambiguity, structural ambiguity, ref-erencing (pronoun referenee~ definite noun reference,etc), control, and unbounded ependencies.
Linguis-tically complex phenomena are handled using the syn-tactic constraint network (SCN).
The SCN enables theDmSNAP to process entences involving unboundeddependencies, controls without passing feature struc-tures.
Details of the SCN is described in \[Kitano et.ah, 1991h\].
One notable feature of DmSNAP is itscapability to parse and translate sentences in context.In other words, DmSNAP can store results of previ-ous sentences and resolve various levels of ambiguitiesusing the contextual information.
Examples of sen-tences which DmSNAP can handle is shown below.i t  should he noted that each example consists of aset of sentences (not a single sentence isolated fromthe context) ill order to denmnatrate he contextualprocessing capability of the DMSNAP.ACRES DE COLING-92, NANTES, 23-28 Ao~r 1992 8 1 5 PRec.
OF COLING-92, NANTES.
AUO.
23-28, 1992; '~  ln ,~,nc .
Nod.c -e  .m ?
-:~o-q: ' - co~- JFigure 2: Part of Memory NetworkSentence Length Time at(words) 10 MHz ( .
.
.
.
.
)s2; He is at ... 4 0.65s3: He said that ... 10 1.50sS: Eric build ...' 5 0.55s6: Juntas found .. 6 1.00s8: Juntae solved ... 7 1.65Table 1: Execution times for DmSNAPExample Isl John wanted to attend Collng-92.m2 He is at the conference.s3 He said that the quality of the paper is superb.Example IIs4 Dan planned to develop a parallel processingcomputer.s5 Eric built a SNAP simulator.s6 Juntae found bugs in the simulator.s7 Dan tried to persuade Eric to help Juntae modifythe simulator.s8 Juntae solved a problem with the simulator.s9 It was the bug that Juntae mentioned.These sentences in examples are not all the sen-tences which DMSNAP can handle.
Currently, DM-SNAP handles a substantial portion of the ATR con-ference registration domain (vocabulary 450 words,329 sentences) and sentences from other corpora.The following are examples of translation intoJapanese generated by the DmSNAP for the first setof sentences (sl ,  s2 and s3):t l  Jon ha koringu-92 ni sanka shitakatta.t2 Kate ha kalgi ni iru.t3 Kare ha ronbun no shitsu ga subarnshli to itta.DMSNAP completes the parsing in the order ofmilliseconds.
Table 1 shows parsing time for some ofthe example sentences.F3gender malenumber singularperson 3rdF1I gende, male \[ numbv~ singularperson 3rdFigure 3: A part of a simple example of classificationlattice4.
Classification-Based ParsingClassification-Based Parsing is a new parsing modelproposed in \[Kasper, 1989\].
In the classification-basedparsing, feature structures are indexed in the hier-archical network, and an unifiability of two featurestructures are tested by searching the Most SpecificSubsumer (MSS).
The unification, a computationallyexpensive operation which is the computational bot-tleneck of many parsing systems, is replaced by searchin the lattice of pre~indexed feature structures.For example, in Figure 3, the feature structureF3 is a result of successful unification of the featurestructure F1 and F2 (F3 = F1 tA F2).
All featurestructures are pro-indexed in a lattice so that the uni-fication is replaced by an intersection search in thelattice with complex indexing.
To carry out a search,first we set distinct markers on each feature structuresF1 and F2.
For example, set marker M1 on F1, andM2 on F2.
Then, markers M1 and M2 propagate up-ward in the lattice.
M1 and M2 first co-exist at F3.The most simple program (without disjunctions andconjunctions handling) for this operation follows:sot _marker (M1 ,~t) ;n~_marker (M2 , f2 )  ;propagate(M1 ,M1 ,UP,I/P pSPREAD) ;propagate (I(2,M2,UP,UP,SPREID) ;marker_and ( M 1, M2 j M3) ;propagate (M3, re_trap, UP ,UP, SPREAD ) ;cond_clear marker (m_tmp, M3) ;collect nodes (M3) ;Of course, nodes for each feature structure mayneed to be searched from a set of features, insteadof direct marking.
In such a case, a set of markerswill be propagated from each node representing eachfeature, and takes disjunction and conjunction at allnodes representing a feature structure root.
This op-eration can be data-parallel.There are several motivations to use classification-based parsing, some of which are described in \[Knsper,1989\].
The efficiency consideration is one of the majorreasons for using classification-based parsing.
Sinceover 80% of parsing time has been consumed on uni-fication operations, replacing unification by a fasterand functionally equivalent method would substan-tially benefit the overall performance of the system.The classification-based parsing is efficient because (1)it maximizes tructure sharing, (2) it utilizes indexingdependencies, and (3) it avoids redundant computa-tions.
However, these advantages of the classification-AcrEs DE COLING-92, NANTES, 23-28 hO'J'f 1992 8 1 fi PROC.
OF COLING-92, NANTES, AUG. 23-28, 1992Time (u aec.)7$?
?652 3 4 5 o 7 8 Fan-OutTime (~ see.
)32 04 128 256ConeeptlFigure 4: Retrieval Performance on Classification Net-workbased parsing can not be fully obtained if the modelwas implemented on the serial machine.
This is be-cause a search on complex index lattice would becomputationally expensive for serial machines.
Ac-tually, the time-complexity of the sequential classi-fication algorithm is O(Mn2), and that of the re-trieval algorithm is O(R,,~JogM), where M is a num-ber of concepts, n is an average number of prop-erty links per concept, R, .c  is an average number ofroleset relations for one concept.
We can, however,circumvent his problem by using SNAP.
Theoreti-cally, time-complexity of the classification on SNAPis O(loggo~,M), and that of the parallel retrieval isO(FinDa., + 17~), where Fo~t is an average fan-out(average number of suhconcepts for one concept), J~is an average fan-in (average number of superconceptfor one concept), and D.~.
is an average depth of theconcept hierarchy \[Kiln and Moldovan, 1990\].In our model, possible feature structures are pre-computed and indexed using our classification algo-rithms.
While a large set of feature structures needto be stored and indexed, SNAP provides ufficientlylarge memory/processor space to load an entire fea-ture structure lattice.
It is analogous to the idea be-hind the memory-based parsing which pre-expand allpossible syntactic/semantic structures.
Here again,we see the conversion of tlme-complexity into space-complexity.Figure 4 shows performance of retrieval of clas-sitleation lattice with varying fan-out and size.
Theclock cycle is 10 MHz.
It demonstrates that we canattain micro-seconds response for each search.
Giventhe fact that the fastest unification algorithm, even onthe parallel machines, takes over few milliseconds perunification, the performance obtained in our experi-ment promises a significant improvement in parsingspeed for many of the unification-baaed parsers by re~placing unification by classification-based approach.5.
VLBK Search: Integration withthe Knowledge-Based Machineq_?anslationLanguage processing is a knowledge-intensive process.Knowledge-Based Machine Trans-lation (KBMT)\[Goodman and Nirenberg, 1991\] hasbeen proposed and developed baaed on the assumptionthat intensive use of linguistic and world knowledgewould provide lfigh quality automatic trmmlation.One of the central knowledge sources of the KBMTis the ontological hierarchy which encodes abstractionhierarchies of concepts in the given domain, prop~erty information of each concept, etc.
When a parsercreates ambiguous parses or when some parts of themeaning representation (as represented in an interlin-gun) are missing, this knowledge source is accessed todisambiguate or to fill-in missing information.However, as the size of the domain scales up, accesstime to the knowledge source grows to the extent hatcost-effective bulk processing would lint be possible.For exmaple, \[Evett, el.
al., 1990\] reports that ac-cess to large frame systems on serial computers have atime-complexity o fO(M x B 't) where M is the numberof conjuncts in the query, B is the average branchingfactor in the network, and d is the deptb of the net-work.
Thus, even a simplest form of search takes over6 seconds on a VLKB with 28K nodes measured ona single user mode VAX super nfini-computer.
Sincesuch search on a VLKB must be performed severaltimes for each parse, the performance issue would bea major concern.
Considering the fact that VLKBprojects such as CYC \[Lenat and Guha, 1990 andEDR \[EDR, 1988\] aim at VLKBs containing over amillion concepts, the performance of VLKB searchwould be an obvious problem in practical use of theseVLKBs.
In tile massively parallel machines uch asSNAP, we should be able to attain time-complexity ofO(D + M) \[Evett, et.
al., 1990\].We have carried out experiments to measure KBaccess time on SNAP.
Figure 5 shows the search timefor various size of VLKBs ranging from 800 to 64Knodes.
Performance was compared with SUN-4 antithe CM-2 connection machine.
SNAP-1 consistentlyoutperformed other machines (performance curve ofSNAP-1 is hard to see in the figure as it exhibitedexecution time far less than a second.6.
Other ApproachesOne clear extension of the currently implementedmodules is to integrate the classification-baaed parsingand the VLKB search.
The classification-baaed pars-ing carry out high performance syntactic analysis andthe VLKB search would impose semantic onstraints.Integration of these two would require that the SNAP-1 to have a multiple controller because two differentmarker control processes need to he mixed and exe-cuted at the same time.
Currently SNAP-1 has onlyone controller.
This would be one of the major itemsfor tile up-grade of the architecture.
However, theperformance gain by this approach would be signifi-cant and its in,pact can be far reaching because a lotof current NLP research as been carried out on theACflLS DE COLING-92, NANTES, 23-28 AOt~rr 1992 8 1 7 I'ROC.
OF COLING-92, NAN-t~S, Autl.
23-2g, 1992VLKII Retrieval in PACE Benchmark+++ alooo .
.
.
.
.
.
.
.
.
.
.
.
.
.
-.--i~00 .
- -I '.I so00  .
.
.
.
.
.
.
.
.
~ - -12OOO .
.
.
.
.
.
.
.
.
.
.
.
: - -+t looo  ~ .
.
.
.
.
.
.
.
7100  00  +I  .
.
.
.
.
.
.
_ , , ' L -  .
.
.
.
.
.
.
,mm .
.
.
.
.
~ .
.
.
.
.
.m00 .
.
.
.
.
.
.
.
: .
.
.
.
.
.7OO0 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.+0.+0 ~ .
.
.
.
.
.
.
.
," .
.
.
.
.
.
.
.
:+\] :+7'"+ -~) +am - -  + - -  ~ - -  - -  - -  :+oa l  .
.
.
.
.
.
.
.
.
.
.
.
+m .
.
.
.
.
.
.
.
.
NAP-1ooo lm 2o+ +~ 4oo +?~ +ix,Figure 5: Retrieval time vs. KB sizeframework of the unification-based grammar formal-ism and use VLKBs as major knowledge sources.A more radical approacl h however rooted in the tra-ditional model is to fully map the typed unificationgrammars \[Emele and Zajac, 1990 on the SNAP.
Thetyped unification grammar is based on the Typed Fea-ture Structure (TFS) \[Zajac, 1989\] and HPSG \[Pollardand Sag, 1987\], and represents all objects in TFS.Objects includes Phrasal Sign, Lexical Sign, generalprinciples uch as the "Head Feature Principle", the"Subcat Feature Principle", grammar ules such asthe "Complement Head Constituent Order FeaturePrinciple," the "Head Complements Constituent Or-der Feature Principle," and lexical entries.
The lexi-cal entries can be indexed under the lexical hierarchy.In this apporach, all linguistic knowledge is precom-piled into a huge network.
Parsing and generationwill be carried out as a search on this network.
Wehave not yet complete a feasibility study for this ap-proach on SNAP.
However, as of today, we considerthis approach is feasible and expect to attain single-digit millisecond order performance on an actual im-plementation.
The dynamic network modification, ad-dress propagation, and marker propagation rules areespecially useful in implementing this approach.Natural language processing model on semanticnetworks such as \[Norvig, 1986\], SNePS \[Neal andShapiro, 1987\], and TRUMP, KING, ACE~ andSOISOR at GE Lab.
\[Jacobs, 1991\] should fit wellwith the SNAP-1 architecture.
For \[Norvig, 1986\],SNAP provides floating point numbers to be propa-gated.
As for SNePS, the implementation should betrivial, yet we are not sure the level of parallelism gainby the SNePS model.
When the parallelism was foundto be low, the coarse-grain processor may fit well withthis model.
Although we do not have space to discussin this paper, there are, of course, many other NLPand AI models which can be implemented on SNAP.7.
ConclusionIn this paper, we have demonstrated that semanticnetwork array processor (SNAP) speeds up variousnatural anguage processing tasks.
We have demon-atrated this fact using three examples: the memory-based parsing, VLKB processing, and Classification-based parsing.in the memory-based parsing approach, we have at-tained the speed of parsing in the order of millisecondswithout making substantial compromises in linglfisticanalysis.
To the contrary, our model is superior toother traditional natural anguage processing modelsin several aspects, particularly, contextual processing.Next, we have applied the SNAP architecture for anew classification-based parsing model ltere, SNAPiB used to search tile MSS to test tile unifiability ofthe two feature graphs.
We have attained, again, sub +milliseconds order performance per uniflability test.In addition, this approach exhibited esirable scala-bility characteristics.
The search time asymptoticallyresearches to 450 cycles as the size of classification net-work increases.
Also, search tinm decreases as averagefan-out gets larger?
Thee  are natural advantages ofusing parallel machines?SNAP is not only useful for the new and radicalapproach, but also beneficial in speeding up tradi-tional NLP systems uch as KBMT.
We have eval-uated the performance to search VLKB which is themajor knowledge source for the KBMT system.
Wehave attained sub-milliseconds order performance pera search.
Traditionally, on the serial machines, thisprocess has been taking a few seconds posing the ma-jor thread to performance on the scaled up systems.Also, there are many other NLP models (TypedUnification Grammar \[Emele and Zajae, 1990\], SNePS\[Neal and Shapiro, 1987\], and others) which may ex-hibit high performance and desirable scaling propertyon SNAP.Currently, we are designing the SNAP-2 reflectingvarious findings made by the research with SNAP-1.SNAP-2 will be built upon the state-of-the-art VLSItechnologies using RISC architecture.
At least 32Kvirtual nodes will be supported by each processing ele-ment, providing the system with a minimum of 16 mil-lion nodes?
SNAP-2 will feature nmlti-user supports,intelligent I /O, etc.
One of the significant features inSNAP-2 is the introduction of a programmable markerpropagation rules.
This feature allows users to definetheir own and more sophisticated marker propagationrules.In summary, we have shown that the SNAP archi-tecture can be a useful development platform for highperformance and large-scale natural language process-rag.
This has been empirically demonstrated usingSNAP-1.
SNAP-2 is expected to explore opportuni-ties of massively parallel natural anguage processing.References\[Blelloch, 1986\] Blelloeh, G.  E .
,  "C IS :  A Massively Par-aUel Concurrent Rule-Based System," Proceeding ofAClT~ DE COLING-92, NA~rEs.
23-28 hOt'q 1992 8 1 8 Paoc.
OF COLING-92, NANTES, AUG. 23-28, 1992AAAI-86, 1986.\[Bzachman and Schmolse, 1985\] tlrachmau, R. J. andSchmolze, J. G., "An Overview of The KL-ONEKnowledge Representation System," Cognitive Sci-ence 9, 171-216, August 1985.\[Charniak, 1983\] Charniak, E., "Passing markers: A the-ory of contextual influence in language comprehen-sion," Cognitive Science, 7(3), 1983.\[Carpenter and Pollard, 1991\] Carpenter, B, and Pollard,C., "Inclusion, Disjointness and Choice: The Logicof Linguistic Classification," Proc.
of A cbgJ~ 1991.\[EDR, 1988\] Japan Electric Dictionary Research Insti-tute, EDR Electric Dictionaries, Technical Report,Japan Electric Dictionary Research Institute, 1986.\[Emele and Zajac, 1990\] Emele, M. and Zajac, R.,"Typed Unification Grammars," Proc.
of Coting-90,1990.\[Fahlman, 1979\] Fahhnan, S., NETL: A System for RepT~-sen*in 9 and Using Real-World Knowledge, The MITPress, 1979.\[Evett, st. aL, 1990\] Evett~ M., ttendler, J., and Spector,L., PARKA: Parallel Knowledge Representation onthe Connection Machine, UMIACS-TR-90-22, Uni-versity of Maryland, 1990.\[Headier, 1988\] Headier, J., ln~egrating Marker.Passingand Problem-Solving, Lawrence Erlbanm Associates,1988.thirst, 1986\] Hirst, O., Semantic Interpretation and theResolution of Ambiguity, Cambridge UniversityPress, Cambridge, 1986.Is,cobs, 1991\] Jacobs, P., "Integrating Language andMeaning," Sown, J.
(Ed.)
Principles of Semantic Net-works, Morgan Kauflnann, 1991.\[Kn.sper~ 1989\] Kasper, R., "Utfilicatlon and Classifica-tion: An Experiment in Infonuation-B~sed Parsing,"Proceedings of the International Workshop on Pars-ing Technologies, Pittsburgh, 1989.\[Kim and Moldovan, 1999\] Kim, J. and Moldovan, D.,"Parallel Chmsification for Knowledge Representa-tion on SNAP" Proceedings of the 1990 InternationalConference on Parallel Processing, 1990.\[Kit.no, 1991\] Kit.no, It., "~DmDialog: An Experimen-tal Speech-to-Speech Dialogue Translation System,"IEEE Computers June, 1991.\[Kitano and Higuclfi, 1991a1 Kit.no, H. and Higuchi, T.,"Massively Parallel Memory-Based Parsing", Pro-ceedings of IJCAI-9J, 1991.\[Kit.no and Higuclfi, 1991hi Kit.no, H. and Higuchi, T.,"High Performance Memory-Based Translation onIXM2 Massively Parallel Associative Memory Pro-cessor", Proceedings of AAAI-91, 1991.\[Kit.no st. M., 1991a\] Kit.no, ti., Headier, J., Higuchi,T., Moldovan, D., and Waltz, D., "Massively ParallelArtificial Intelligence," Proc.
of lJCAI-91, 1991.\[Kitanoet.
al., 1991b\] Kit.no, H., Moldovan, D., andCha, S., "High Performance Natural Language Pro-cessing on Semantic Network Array Processor," Prve.of IJCAI-91, 1991.\[Kit,no, 1990\] Kitano, H., "Parallel Incremental SentenceProduction for a Model of Simultaneous Interpreta-tion," Dale, R., Mellish, C., and Lock, M.
(Eds.
)Current Research in Natural Language Generation tAcademic Press, London, 1990.\[Kit,no st.
al., 1989\] Kitnno, H., Tomabechi, H., andLevln, L., "Ambiguity Resolution in DmTrans Phm,"Proceedings of the European Chapter of the Associa-tion of Computational Linguistics, 1989.\[Lee and Moldovan, 1990\] Lee, W. and Moldovan, D.,"The Design of a Marker Passing Architecture forKnowledge Processing", Proceedings of AAAI-90,1990.\[Lenat and Guha, 1990\] Lea.t, D.B, and Guha, R.V.,Building Large K~towledge-Based Systems, Addison-Wesley, 1990.\[Nagao, 1984\] Nag.o, M., "A b~ramework o f .
MechanicalTranslation between Japanese and Engllnh by Anal-.ogy Principle," Artificial and Human Intelligence,Ehthorn, A. and Banerji, R.
(Eds.
), Elsevier ScieucePublishers, B.V. 1984.\[Neal aud Shapiro, 1987\] Neal, J. and Shapiro, S.,"Knowledge-Based Parsing," Bole, L., (Ed.)
NaturalLanguage Parsing Systems, Sptinger-Verlag, 1987.\[Goodman and Nirenberg, 1991\]Goodman~ K,, and Nirenberg, S. Knowledge-BasedMachine Translation Project: A Case Study, MorganKauimann, 1991.\[Norvig, 1966\] Norvig, P., Unified Theory of Inference forTest Understanding, Ph.D. Thesis, University of Cal-ifornia Berkeley, 1986.\]Pollard and Sag, 1987\]Pollard, C. and Sag, I., b~formation-Based Syntazand Semantics, Vol.
L" Fundamentals, CSLI LectureNote Series, Chicago University Press, 1987.\[Quilllian, 1968\] Quillian, M. R., "Semantic Memory," Seomastic Information Processing, Minsky, M.
(gd.
),216-270, The MIT press, Cambridge, MA, 1968.\[Riesbeck anti Martin, 1985\] Riesbeck, C. and Martin, C.,"Direct Memory Access Parsing", Yale UniversityReport 3S4, 1985.\[Riesbeck and Schank, 1989\] Riesbeck, C. and Schank,R.
Inside Case-Based Reasoning, Lawrence ErlbaumAssociates, 1989.\[Sown, 1991\] S .
.
.
.
J. F.
(Ed.
), Principles of SemanticNetworks, Morgan Kaufmann, 1991.\[Sown, 1984\] Sown, J. F., ConceptualStrueturen, Reading,Addison Wesley, 1984.\[St,still and Waltz, 1986\] Stnnfill, C., aud Waltz, D.,"Toward Memory-Based Reasoning," Communica-tion of the ACM, 1986.\[Surtdta nd lid,, 19911 Sumita, E., and Iida, 1I, "Ex-perinmnts and Prospects of Example~Bnsed MachineTranslation," Proceedings of A CL-91, 1991.\[Thinking Machines Corp., 1989\] Thinking MachinesCorp., Model CM-~ Technical Summary, TechnicalReport TR-89-1~ 1989.\[Tomabechi, 1987\] Tomabechi, lI., "Direct Memory Ac-cess ~l~anslation', Proceedings of the IJCAI-87, 1987.\[Waltz and Pollack, 1985\] Waltz, 1).L.
and Pollack, J.,"Massively Parallel Parsing: A Strongly InteractiveModel of Natural Language Interpretation" CognitiveScience, 9(1): 51-74, 1985.\[Wilensky, 1987\] Wilensky, R., "Some Problems and Pro-posals for Knowledge Representation", Technical Re-port UCB/CSD 87/361, University of California,Berkeley, Computer Science Division, 1987.\[Zajac, 1989\] Zajac, R., "A Transfer Model Using a TypedFeature Structure Rewriting System with Inheri-tance," Proc.
of A CL-S9, 1989.AL~S DE COLING-92, NA~riazs.
23-28 no(rr 1992 8 1 9 Paoc.
OF COLING-92.
NAN-rE/I, AUG. 23-28.
1992
