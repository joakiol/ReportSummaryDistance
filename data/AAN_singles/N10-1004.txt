Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 28?36,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsAutomatic Domain Adaptation for ParsingDavid McCloskya,baStanford UniversityStanford, CA, USAmcclosky@stanford.eduEugene CharniakbbBrown UniversityProvidence, RI, USAec@cs.brown.eduMark Johnsonc,bcMacquarie UniversitySydney, NSW, Australiamjohnson@science.mq.edu.auAbstractCurrent statistical parsers tend to perform wellonly on their training domain and nearby gen-res.
While strong performance on a few re-lated domains is sufficient for many situations,it is advantageous for parsers to be able to gen-eralize to a wide variety of domains.
Whenparsing document collections involving het-erogeneous domains (e.g.
the web), the op-timal parsing model for each document is typ-ically not obvious.
We study this problem asa new task ?
multiple source parser adapta-tion.
Our system trains on corpora from manydifferent domains.
It learns not only statisticsof those domains but quantitative measures ofdomain differences and how those differencesaffect parsing accuracy.
Given a specific tar-get text, the resulting system proposes linearcombinations of parsing models trained on thesource corpora.
Tested across six domains,our system outperforms all non-oracle base-lines including the best domain-independentparsing model.
Thus, we are able to demon-strate the value of customizing parsing modelsto specific domains.1 IntroductionIn statistical parsing literature, it is common to seeparsers trained and tested on the same textual do-main (Charniak and Johnson, 2005; McClosky etal., 2006a; Petrov and Klein, 2007; Carreras et al,2008; Suzuki et al, 2009, among others).
Unfor-tunately, the performance of these systems degradeson sentences drawn from a different domain.
Thisissue can be seen across different parsing models(Sekine, 1997; Gildea, 2001; Bacchiani et al, 2006;McClosky et al, 2006b).
Given that some aspects ofsyntax are domain dependent (typically at the lexi-cal level), single parsing models tend to not performwell across all domains (see Table 1).
Thus, statis-tical parsers inevitably learn some domain-specificproperties in addition to the more general propertiesof a language?s syntax.
Recently, Daume?
III (2007)and Finkel and Manning (2009) showed techniquesfor training models that attempt to separate domain-specific and general properties.
However, even whengiven models for multiple training domains, it is notstraightforward to determine which model performsbest on an arbitrary piece of novel text.This problem comes to the fore when one wantsto parse document collections where each documentis potentially its own domain.
This shows up par-ticularly when parsing the web.
Recently, therehas been much interest in applying parsers to theweb for the purposes of information extraction andother forms of analysis (c.f.
the CLSP 2009 summerworkshop ?Parsing the Web: Large-Scale SyntacticProcessing?).
The scale of the web demands an au-tomatic solution to the domain detection and adap-tation problems.
Furthermore, it is not obvious thathuman annotators can determine the optimal parsingmodels for each web page.Our goal is to study this exact problem.
We createa new parsing task, multiple source parser adapta-tion, designed to capture cross-domain performancealong with evaluation metrics and baselines.
Ournew task involves training parsing models on labeledand unlabeled corpora from a variety of domains(source domains).
This is in contrast to standard do-main adaptation tasks where there is a single sourcedomain.
For evaluation, one is given a text (targettext) but not the identity of its domain.
The chal-lenge is determining how to best use the available28TestTrain BNC GENIA BROWN SWBD ETT WSJ AverageGENIA 66.3 83.6 64.6 51.6 69.0 66.6 67.0BROWN 81.0 71.5 86.3 79.0 80.9 80.6 79.9SWBD 70.8 62.9 75.5 89.0 75.9 69.1 73.9ETT 72.7 65.3 75.4 75.2 81.9 73.2 73.9WSJ 82.5 74.9 83.8 78.5 83.4 89.0 82.0Table 1: Cross-domain f-score performance of the Charniak (2000) parser.
Averages are macro-averages.Performance drops as training and test domains diverge.
On average, the WSJ model is the most accurate.resources from training to maximize accuracy acrossmultiple target texts.Broadly put, we model how domain differencesinfluence parsing accuracy.
This is done by takingseveral computational measures of domain differ-ences between the target text and each source do-main.
We use these features in a simple linear re-gression model which is trained to predict the accu-racy of a parsing model (or, more generally, a mix-ture of parsing models) on a target text.
To parsethe target text, one simply uses the mixture of pars-ing models with the highest predicted accuracy.
Weshow that our method is able to predict these accu-racies quite well and thus effectively rank parsingmodels formed from mixtures of labeled and auto-matically labeled corpora.In Section 2, we detail recent work on similartasks.
Our regression-based approach is covered inSection 3.
We describe an evaluation strategy in Sec-tion 4.
Section 5 presents new baselines which areintended to give a sense of current approaches andtheir limitations.
The results of our experiments aredetailed in Section 6 where we show that our systemoutperforms all non-oracle baselines.
We concludewith a discussion and future work (Section 7).2 Related workThe closest work to ours is Plank and Sima?an(2008), where unlabeled text is used to group sen-tences from WSJ into subdomains.
The authors cre-ate a model for each subdomain which weights treesfrom its subdomain more highly than others.
Giventhe domain specific models, they consider differentparse combination strategies.
Unfortunately, thesemethods do not yield a statistically significant im-provement.Multiple source domain adaptation has been donefor other tasks (e.g.
classification in (Blitzer etal., 2007; Daume?
III, 2007; Dredze and Cram-mer, 2008)) and is related to multitask learning.Daume?
III (2007) shows that an extremely sim-ple method delivers solid performance on a num-ber of domain adaptation classification tasks.
This isachieved by making a copy of each feature for eachsource domain plus the ?general?
pseudodomain(for capturing domain independent features).
Thisallows the classifier to directly model which featuresare domain-specific.
Finkel and Manning (2009)demonstrate the hierarchical Bayesian extension ofthis where domain-specific models draw from a gen-eral base distribution.
This is applied to classifica-tion (named entity recognition) as well as depen-dency parsing.
These works describe how to trainmodels in many different domains but sidestep theproblem of domain detection.
Thus, our work is or-thogonal to theirs.Our domain detection strategy draws on work inparser accuracy prediction (Ravi et al, 2008; Kawa-hara and Uchimoto, 2008).
These works aim to pre-dict the parser performance on a given target sen-tence.
Ravi et al (2008) frame this as a regressionproblem.
Kawahara and Uchimoto (2008) treat itas a binary classification task and predict whethera specific parse is at a certain level of accuracy orhigher.
Ravi et al (2008) show that their systemcan be used to return a ranking over different parsingmodels which we extend to the multiple domain set-ting.
They also demonstrate that training their modelon WSJ allows them to accurately predict parsingaccuracy on the BROWN corpus.
In contrast, ourmodels are trained over multiple domains to modelwhich factors influence cross-domain performance.293 ApproachWe start with the assumption that all target domainsare mixtures of our source domains.1 Intuitively,these mixtures should give higher probability massto more similar source domains.
This raises thequestion of how to measure the similarity betweendomains.
Our method uses multiple complemen-tary similarity measures between the target and eachsource.
We feed these similarity measures into a re-gression model which learns how domain dissimi-larities hurt parse accuracy.
Thus, to parse a targetdomain, we need only find the input that maximizesthe regression function ?
that is, the highest scoringmixture of source domains.
Our system is similar toRavi et al (2008) in that both use regression to pre-dict f-scores and some of the features are related.3.1 FeaturesOur features are designed to help the regressionmodel determine if a particular source domain mix-ture is well suited for a target domain as well as thequality of a source domain mixture.
While we ex-plored a large number of features, we present hereonly the three that were chosen by our feature selec-tion method (Section 6.2).Two of our features, COSINETOP50 and UN-KWORDS, are designed to approximate how simi-lar the target domain is to a specific source domain.Only the surface form of the target text and auto-matic analyses are available (e.g.
we can tag or parsethe target text, but cannot use gold tags or trees).Relative word frequencies are an important in-dicator of domain.
Cosine similarity uses a spa-tial representation to summarize the word frequen-cies in a corpus as a single vector.
A commonmethod is to represent each corpus as a vector offrequencies of the k most frequent words (Schu?tze,1995).
This method assigns high similarity to do-mains with a large amount of overlap in the high-frequency vocabulary items.
We experimented withseveral orders of magnitude for k (our feature selec-tion method later chose k = 50 ?
see Section 6.2).Our second feature for comparing domains, UN-1This may seem like a major limitation, but as we will showlater, our method works quite well at incorporating self-trained(automatically parsed) corpora which can typically be obtainedfor any domain.KWORDS, returns the percentage of words in onedomain which never appear in the other domain.This can be done on the word type or token level.We opt for tokens since unknown words pose prob-lems for parsing each time they occur.
UNKWORDSprovides the percentage of words in the sourcedomain that are never seen in the target domain.Whereas COSINETOP50 examines how similar thehigh frequency words are from one domain, UN-KWORDS tends to focus on the overlap of low fre-quency words.As described, COSINETOP50 and UNKWORDSare functions only of two source domains and do nottake the mixing weights of source domains into ac-count.
We experimented with several methods of in-corporating mixing weights into the feature value.In practice, the one which worked best for us is todivide the mixture weight of the source domain bythe raw feature value.
This has the nice property thatwhen a source is not used, the adjusted feature valueis zero regardless of the raw feature value.From pilot studies, we learned that a uniform mix-ture of available source domains gave strong results(further details on this in Section 5).
Our last feature,ENTROPY, is intended to let the regression systemleverage this and measures the entropy of the distri-bution over source domains.
This provides a senseof uniformity.3.2 Predicting cross-domain accuracyFor a given source domain mixture, we can createa parsing model by linearly interpolating the pars-ing model statistics from each source domain.
Thekey component of our approach is a domain-awarelinear regression model which predicts how well aspecific parsing model will do on a given target text.The linear regressor is given values from the threefeatures from the previous section (COSINETOP50,UNKWORDS, and ENTROPY) and returns an esti-mate of the f-score the parsing model would achievethe target text.Training data for the regressor consists of ex-amples of source domain mixtures and their ac-tual f-scores on target texts.
To produce this, werandomly sampled source domain mixtures, createdparsing models for those mixtures, and then evalu-ated the parsing models on all of our target texts.We used a simple technique for randomly sam-300 200 400 600 800 1000Number of mixed parsing model samples84.084.585.085.586.086.587.087.5oraclef-scoreFigure 1: Cumulative oracle f-score (averaged overall target domains) as more models are randomlysampled.
Most of the improvement comes the first200 samples indicating that our samples seem to besufficient to cover the space of good source domainmixtures.pling source domain mixtures.
First, we sample thenumber of source domains to use.
We draw valuesfrom an exponential distribution and take their inte-ger value until we obtain a number between two andthe number of source domains.
This is parametrizedso that we typically only use a few corpora but stillhave some chance of using all of them.
Once weknow the number of source domains, we sampletheir identities uniformly at random without replace-ment from the list of all source domains.
Finally,we sample the weights for the source domains uni-formly from a simplex.
The dimension of the sim-plex is the same as the number of source domainsso we end up with a probability distribution over thesampled source domains.In total, we sampled 1,040 source domain mix-tures.
We evaluated each of these source domainmixtures on the six target domains giving us 6,240data points in total.
One may be concerned thatthis is insufficient to cover the large space of sourcedomain mixtures.
However, we show in Figure 1that only about 200 samples are sufficient to achievegood oracle performance2 in practice.2We calculate this by picking the best available model foreach target domain and taking the average of their f-scores.Train TestSource Target Source TargetC \ {t} C \ {t} C \ {t} {t}(a) Out-of-domain evaluationTrain TestSource Target Source TargetC C \ {t} C {t}(b) In-domain evaluationTable 2: List of domains allowed in single round ofevaluation.
In each round, the evaluation corpus is t.C is the set of all target domains.4 EvaluationMultiple-source domain adaptation is a new task forparsing and thus some thought must be given to eval-uation methodology.
We describe two evaluationscenarios which differ in how foreign the target textis from our source domains.
Schemas for these eval-uation scenarios are shown in Table 2.
Note thattraining and testing here refer to training and testingof our regression model, not the parsing models.In the first scenario, out-of-domain evaluation,one target domain is completely removed from con-sideration and only used to evaluate proposed mod-els at test time.
The regressor is trained on trainingpoints that use any of the remaining corpora, C\{t},as sources or targets.
For example, if t = WSJ, wecan train the regressor on all data points which don?tuse WSJ (or any self-trained corpora derived fromWSJ) as a source or target domain.
At test time, weare given the text of WSJ?s test set.
From this, oursystem creates a parsing model using the remainingavailable corpora for parsing the raw WSJ text.This evaluation scenario is intended to evaluatehow well our system can adapt to an entirely newdomain with only raw text from the new domain(for example, parsing biomedical text when noneis available in our list of source domains).
Ide-ally, we would have a large number of web pagesor other documents from other domains which wecould use solely for evaluation.
Unfortunately, atthis time, only a handful of domains have been an-notated with constituency structures under the sameThis can pick different models for each target domain.31annotation guidelines.
Instead, we hold out eachhand-annotated domain, t, (including any automat-ically parsed corpora derived from that source do-main) as a test set in a round-robin fashion.3 Foreach round of the round robin we obtain an f-scoreand we report the mean and variance of the f-scoresfor each model.The second scenario, in-domain evaluation, al-lows the target domain, t, to be used as a sourcedomain in training but not as a target domain.
Thisis intended to evaluate the situation where the targetdomain is not actually that different from our sourcedomains.
The in-domain evaluation can approxi-mate how our system would perform when, for ex-ample, we have WSJ as a source domain and the tar-get text is news from a source other than WSJ.
Thus,our model still has to learn that WSJ and the NorthAmerican News Text corpus (NANC) are good forparsing news text like WSJ without seeing any directevaluations of the sort (WSJ and NANC can be usedin models which are evaluated on all other corpora,though).5 BaselinesGiven that this is a new task for parsing, we neededto create baselines which demonstrate the currentapproaches to multiple-source domain adaptation.One approach is to take all available corpora andmix them together uniformly.4 The UNIFORM base-line does exactly this using the available hand-builttraining corpora.
SELF-TRAINED UNIFORM usesself-trained corpora as well.
In the out-of-domainscenario, these exclude the held out domain, but inthe in-domain setting, the held out domain is in-cluded.
These baselines are similar to the ALL andWEIGHTED baselines in Daume?
III (2007).Another simple baseline is to use the same pars-ing model regardless of target domain.
This is howlarge heterogeneous document collections are typi-cally parsed currently.
We use the WSJ corpus sinceit is the best single corpus for parsing all six targetdomains (see Table 1).
We refer to this baseline asFIXED SET: WSJ.
In the out-of-domain scenario,we fall back to SELF-TRAINED UNIFORM when the3Thus, the schemas in Table 2 are schemas for each round.4Accounting for size so that the larger corpora don?t over-whelm the smaller ones.target domain is WSJ while the in-domain scenariouses the WSJ model throughout.There are several interesting oracle baselines aswell which serve to measure the limits of our ap-proach.
These baselines examine the resultingf-scores of models and pick the best model accord-ing to some criteria.
The first oracle baseline isBEST SINGLE CORPUS which parses each corpuswith the source domain that maximizes performanceon the target domain.
In almost all cases, this base-line selects each corpus to parse itself.Our second oracle baseline, BEST SEEN, choosesthe best parsing model from all those explored foreach test set.
Recall that while training the regres-sion model in Section 3.2, we needed to exploremany possible source domain mixtures to approxi-mate the complete space of mixed parsing models.To the extent that we can fully explore the space ofmixed parsing models, this baseline represents anupper bound for model mixing approaches.
Sincefully exploring the space of possible weightings isintractable, it is not a true upper bound.
While itis theoretically possible to beat this pseudo-upperbound, (indeed, this is the mark of a good domaindetection system) it is far from easy.
We provideBEST SINGLE CORPUS and BEST SEEN for bothin-domain and out-of-domain scenarios.
The out-of-domain scenario restricts the set of possible modelsto those not including the target domain.Finally, we searched for the BEST OVERALLMODEL.
This is the model with the highest aver-age f-score across all six target domains.
This base-line can be thought of as an oracle version of FIXEDSET: WSJ and demonstrates the limit of using a sin-gle parsing model regardless of target domain.
Natu-rally, the very nature of this baseline places it only inthe in-domain evaluation scenario.
Since it was ableto select the model according to f-scores on our sixtarget domains, its performance on domains outsidethat set is not guaranteed.To provide a better sense of the space of mixedparsing models, we also provide the WORST SEENbaseline which picks the worst model available for aspecific target corpus.55This turns out to be GENIA for all corpora other than GENIAand SWBD when the target domain is GENIA.326 ExperimentsOur experiments use the Charniak (2000) generativeparser.
We describe the corpora used in our ninesource and six target domains in Section 6.1.
In Sec-tion 6.2, we provide a greedy strategy for pickingfeatures to include in our regression model.
The re-sults of our experiments are in Section 6.3.6.1 CorporaWe aimed to include as many different domains aspossible annotated under compatible schemes.
Wealso tried to include human-annotated corpora andautomatically labeled corpora (self-trained corporaas in McClosky et al (2006a) which have beenshown to work well across domains).
Our finalset includes text from news (WSJ, NANC), broad-cast news (ETT), literature (BROWN, GUTENBERG),biomedical (GENIA, MEDLINE), spontaneous speech(SWBD), and the British National Corpus (BNC).
Inour experiments, self-trained corpora cannot be usedas target domains since we lack gold annotations andBNC is not used as a source domain due to its size.An overview of our corpora is shown in Table 3.We use news articles portion of the Wall StreetJournal corpus (WSJ) from the Penn Treebank (Mar-cus et al, 1993) in conjunction with the self-trainedNorth American News Text Corpus (NANC, Graff(1995)).
The English Translation Treebank, ETT(Bies, 2007), is the translation6 of broadcast newsin Arabic.
For literature, we use the BROWN cor-pus (Francis and Kuc?era, 1979) and the same di-vision as (Gildea, 2001; Bacchiani et al, 2006;McClosky et al, 2006b).
We also use raw sen-tences which we downloaded from Project Guten-berg7 as a self-trained corpus.
The Switchboard cor-pus (SWBD) consists of transcribed telephone con-versations.
While the original trees include disflu-ency information, we assume our speech corporahave had speech repairs excised (e.g.
using a sys-tem such as Johnson et al (2004)).
Our biomedi-cal data comes from the GENIA treebank8 (Tateisiet al, 2005), a corpus of abstracts from the Med-line database.9 We downloaded additional sentences6The transcription and translation were done by humans.7http://gutenberg.org/8http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/9http://www.ncbi.nlm.nih.gov/PubMed/from Medline for our self-trained MEDLINE corpus.Unlike the other two self-trained corpora, we includetwo versions of MEDLINE.
These differ on whetherthey were parsed using GENIA or WSJ as a basemodel to study the effect on cross-domain perfor-mance.
Finally, we use a small number of sentencesfrom the British National Corpus (BNC) (Foster andvan Genabith, 2008).10 The sentences were chosenrandomly, so each one is potentially from a differentdomain.
On the other hand, BNC can be thought ofas its own domain in that it contains significant lex-ical differences from the American English used inour other corpora.We preprocessed the corpora to standardize manyof the annotation differences.
Thus, our results onthem may be slightly different than other works onthese corpora.
Nevertheless, these changes shouldnot significantly impact overall the performance.6.2 Feature selectionWhile our final model uses only three features, weconsidered many other possible features (not de-scribed due to space constraints).
In order to explorethese without hill climbing on our test data, we cre-ated a round-robin tuning scenario.
Since the out-of-domain evaluation scenario holds out one targetdomain, this gives us six test evaluation rounds.
Foreach of these six rounds, we hold out one of the re-maining five target domains for tuning.
This givesus 30 tuning evaluation rounds and we pick our fea-tures to optimize our aggregate performance over allof them.
A model that performs well in this situationhas proven that it has useful features which transferto unknown target domains.The next step is to determine the loss functionto minimize.
Our primary guide is oracle f-scoreloss which we determine as follows.
We take alltest data points (i.e.
mixed parsing models evalu-ated on the target domain) and predict their f-scoreswith our model.
In particular for this measure, weare interested in the point with the highest predictedf-score.
We take its actual f-score which we callthe candidate f-score.
When tuning, we know thetrue f-scores of all test points.
The difference be-tween the highest f-score (the oracle f-score for10http://nclt.computing.dcu.ie/?jfoster/resources/, downloaded January 8th, 2009.33Corpus Source?
Target?
Average length Train Tune TestBNC ?
28.3 ?
?
1,000BROWN ?
?
20.0 19,786 2,082 2,439ETT ?
?
25.6 2,639 1,029 1,166GENIA ?
?
27.5 14,326 1,361 1,360MEDLINE ?
27.2 278,192 ?
?SWBD ?
?
9.2 92,536 5,895 6,051WSJ ?
?
25.5 39,832 1,346 2,416NANC ?
23.2 915,794 ?
?GUTENBERG ?
26.2 689,782 ?
?MEDLINE ?
27.2 278,192 ?
?Table 3: List of source and target domains, sizes of each division in trees, and average sentence length.Indented rows indicate self-trained corpora parsed using the non-indented row as a base parser.this dataset) and the candidate f-score is the oraclef-score loss.
Ties need to be handled correctly toavoid degenerate models.11 If there is a tie for high-est predicted f-score, the candidate f-score is theone with the lowest actual f-score.
This approachis conservative but ensures that regression modelswhich give everything the same predicted f-score donot receive zero oracle f-score loss.Armed with a tuning regime and a loss function,we created a procedure to pick the combination offeatures to use.
We used a parallelized best-firstsearch procedure.
At each round, it expanded thecurrent best set of features by adding or removingeach feature where ?best?
was determined by the lossfunction.
We explored over 6,000 settings, thoughthe best setting of (UNKWORDS, COSINETOP50,ENTROPY) was found within the first 200 settingsexplored.
The best setting obtains an oracle f-scoreloss of 0.37 and a root mean squared error of 0.48?
these numbers are quite low and show the highaccuracy of our regression model (similar to thosein Ravi et al (2008)).
Additionally, the features arecomplementary in that UNKWORDS focuses on lowfrequency words whereas COSINETOP50 looks onlyat high frequency words and ENTROPY functions asa regularizer.6.3 ResultsWe present an overview of our final results for out-of-domain and in-domain evaluation in Table 4.
The11For example, regression models which assign every parsingmodel the same f-score.results include the f-score macro-averaged over thesix target domains and their standard deviation.In both situations, the FIXED SET: WSJ baselineperforms fairly poorly.
Not surprisingly, assumingall of our target domains are close enough to WSJworks badly for our set of target domains and itdoes particularly poorly on SWBD and GENIA.
Onaverage, the UNIFORM baseline does slightly bet-ter for out-of-domain and over 3% better for in-domain.
UNIFORM actually does fairly well on out-of-domain except on GENIA.
In general, using moresource domains is better which partially explains thesuccess of UNIFORM.
This seems to be the casesince even if a source domain is terribly mismatchedwith the target domain, it may still be able to fillin some holes left by the other source domains.
Ofcourse, if it overpowers more relevant domains, per-formance may suffer.
The SELF-TRAINED UNI-FORM baseline uses even more source domains aswell as the largest ones.
In both scenarios, this dra-matically improves performance and is the secondbest non-oracle system.
This baseline provides moreevidence as to the power of self-training for improv-ing parser adaptation.
If we excluded all self-trainedcorpora, our performance on this task would be sub-stantially worse.
We believe the self-trained cor-pora are beneficial in this task since they help reducedata sparsity of smaller corpora.
The BEST SINGLECORPUS baseline is poor in the out-of-domain sce-nario primarily because the actual best single corpusis excluded by the task specification in most cases.When we move to in-domain, this baseline improves34Oracle Baseline or model Average f-score?
Worst seen 62.0 ?
6.1?
Best single corpus 81.0 ?
2.9Fixed set: WSJ 81.0 ?
3.5Uniform 81.4 ?
3.6Self-trained uniform 83.4 ?
2.5Our model 84.0 ?
2.5?
Best seen 84.3 ?
2.6(a) Out-of-domain evaluationOracle Baseline or model Average f-scoreFixed set: WSJ 82.0 ?
4.8Uniform 85.4 ?
2.4?
Best single corpus 85.6 ?
2.9Self-trained uniform 86.1 ?
2.0?
Best overall model 86.2 ?
1.9Our model 86.9 ?
2.4?
Best seen 87.5 ?
2.1(b) In-domain evaluationTable 4: Baselines and final results for the two multiple-source domain adaptation evaluation scenarios.Results include f-scores, macro-averaged over all six target domains and their standard deviations.but is still worse than SELF-TRAINED UNIFORM onaverage.
It beats SELF-TRAINED UNIFORM primar-ily on WSJ, SWBD, and GENIA indicating that thesethree domains are best when not diluted by others.By definition, the WORST SEEN baseline does terri-bly, almost 20% worse then BEST SINGLE CORPUS.Our model is the best non-oracle system for bothevaluation scenarios.
For out-of-domain evaluation,our system is only 0.3% worse than the BEST SEENmodels for each target domain.
For the in-domainscenario, we are within 0.6% of the BEST SEENmodels.
For a sense of scale, our out-of-domain andin-domain f-scores on WSJ are 83.1% and 89.8%respectively.
Both numbers are quite close to theBEST SEEN baseline.
Additionally, our model is0.7% better than the BEST OVERALL MODEL.
Re-call that the BEST OVERALL MODEL is the singlemodel with the best performance across all six tar-get domains.12 By beating this baseline, we showthat there is value in customizing parsing modelsto the target domain.
It is also interesting that theBEST OVERALL MODEL is only marginally betterthan SELF-TRAINED UNIFORM.
Without any fur-ther information about the target corpus, an unin-formed prior appears best.7 DiscussionWe have shown that for both out-of-domain and in-domain evaluations, our system is well adapted topredicting the effects of domain divergence on pars-12Somewhat surprisingly, the best overall model uses almostentirely self-trained corpora consisting of 9.5% GUTENBERG,60.3% NANC, 26.0% MEDLINE (by GENIA), and 4.2% SWBD.ing accuracy.
Using the parsing model with thehighest predicted f-score leads to great performancein practice.
There is a substantial benefit to doingthis over existing approaches (using the same modelfor all domains or mixing all training data togetheruniformly).
Creating a number of domain-specificmodels and mixing them together as needed is a vi-able approach.One can think of our system as trying to esti-mate document-level context.
Our representation ofthis context is simply a distribution over our sourcedomains, but one can imagine more complex op-tions such as a high-dimensional vector space.
Ad-ditionally, our model separates domain and syntaxestimation, but a future direction is to learn thesejointly.
This would combine our work with (Daume?III, 2007; Finkel and Manning, 2009).We have focused on the Charniak (2000) parser,the first stage in the two stage Charniak and John-son (2005) reranking parser.
Applying our methodsto other generative parsers (such as (Collins, 1999;Petrov and Klein, 2007)) is trivial, but it is less clearhow our methods can be applied to the discrimina-tive reranker component of the two stage parser.
Oneavenue of approach is to incorporate the domain rep-resentation into the feature space, as in Daume?
III(2007) but with more complex domain information.AcknowledgmentsThis work was performed while the first author wasat Brown and supported by DARPA GALE contractHR0011-06-2-0001.
We would like to thank the BLLIPteam and our anonymous reviewers for their comments.35ReferencesMichiel Bacchiani, Michael Riley, Brian Roark, andRichard Sproat.
2006.
MAP adaptation of stochas-tic grammars.
Computer Speech and Language,20(1):41?68.Ann Bies.
2007.
GALE Phase 3 Release 1 - EnglishTranslation Treebank.
Linguistic Data Consortium.LDC2007E105.John Blitzer, Mark Dredze, and Fernando Pereira.
2007.Biographies, bollywood, boom-boxes and blenders:Domain adaptation for sentiment classification.
InAssociation for Computational Linguistics, Prague,Czech Republic.Xavier Carreras, Michael Collins, and Terry Koo.
2008.TAG, dynamic programming, and the perceptron forefficient, feature-rich parsing.
In Proceedings ofCoNLL 2008, pages 9?16, Manchester, England, Au-gust.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminative rerank-ing.
In Proceedings of the ACL 2005, pages 173?180.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of the North American Chapterof the ACL (NAACL), pages 132?139.Michael Collins.
1999.
Head-driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, The Uni-versity of Pennsylvania.Hal Daume?
III.
2007.
Frustratingly easy domain adap-tation.
In Proceedings of ACL 2007, Prague, CzechRepublic.Mark Dredze and Koby Crammer.
2008.
Online methodsfor multi-domain learning and adaptation.
In Proceed-ings of the EMNLP 2008, pages 689?697, Honolulu,Hawaii, October.Jenny Rose Finkel and Christopher D. Manning.
2009.Hierarchical bayesian domain adaptation.
In Proceed-ings of HLT-NAACL 2009, pages 602?610, Boulder,Colorado, June.Jennifer Foster and Josef van Genabith.
2008.
Parserevaluation and the bnc: Evaluating 4 constituencyparsers with 3 metrics.
In Proceedings LREC 2008,Marrakech, Morocco, May.W.
Nelson Francis and Henry Kuc?era.
1979.
Manualof Information to accompany a Standard Corpus ofPresent-day Edited American English, for use withDigital Computers.
Brown University, Providence,Rhode Island.Daniel Gildea.
2001.
Corpus variation and parser per-formance.
In Empirical Methods in Natural LanguageProcessing (EMNLP), pages 167?202.David Graff.
1995.
North American News Text Corpus.Linguistic Data Consortium.
LDC95T21.Mark Johnson, Eugene Charniak, and Matthew Lease.2004.
An improved model for recognizing disfluen-cies in conversational speech.
In Proc.
of the Rich Text2004 Fall Workshop (RT-04F).Daisuke Kawahara and Kiyotaka Uchimoto.
2008.Learning reliability of parses for domain adaptationof dependency parsing.
In Third International JointConference on Natural Language Processing (IJCNLP?08).Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
Comp.
Linguis-tics, 19(2):313?330.David McClosky, Eugene Charniak, and Mark Johnson.2006a.
Effective self-training for parsing.
In Proceed-ings of HLT-NAACL 2006, pages 152?159.David McClosky, Eugene Charniak, and Mark John-son.
2006b.
Reranking and self-training for parseradaptation.
In Proceedings of COLING-ACL 2006,pages 337?344, Sydney, Australia, July.
Associationfor Computational Linguistics.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Human Language Tech-nologies 2007: The Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics; Proceedings of the Main Conference, pages404?411, Rochester, New York, April.
Association forComputational Linguistics.Barbara Plank and Khalil Sima?an.
2008.
Subdomainsensitive statistical parsing using raw corpora.
InProceedings of the Sixth International Language Re-sources and Evaluation (LREC?08), Marrakech, Mo-rocco, May.Sujith Ravi, Kevin Knight, and Radu Soricut.
2008.
Au-tomatic prediction of parser accuracy.
In Proceedingsof the 2008 Conference on Empirical Methods in Nat-ural Language Processing, pages 887?896, Honolulu,Hawaii, October.
Association for Computational Lin-guistics.Hinrich Schu?tze.
1995.
Distributional part-of-speechtagging.
In Proceedings of the 7th conference of theEACL, pages 141?148.Satoshi Sekine.
1997.
The domain dependence of pars-ing.
In Proc.
Applied Natural Language Processing(ANLP), pages 96?102.Jun Suzuki, Hideki Isozaki, Xavier Carreras, and MichaelCollins.
2009.
An empirical study of semi-supervisedstructured conditional models for dependency parsing.In Proceedings EMNLP 2009, pages 551?560, Singa-pore, August.Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, andJun?ichi Tsujii.
2005.
Syntax Annotation for the GE-NIA corpus.
Proceedings of IJCNLP 2005, Compan-ion volume, pages 222?227.36
