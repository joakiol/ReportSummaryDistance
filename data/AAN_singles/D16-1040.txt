Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 414?423,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsRelation Schema Induction using Tensor Factorization with SideInformationMadhav NimishakaviIndian Institute of ScienceBangalore, Indiamadhav@csa.iisc.ernet.inUday Singh SainiIndian Institute of ScienceBangalore, Indiauday.s.saini@gmail.comPartha TalukdarIndian Institute of ScienceBangalore, Indiappt@cds.iisc.ac.inAbstractGiven a set of documents from a specific do-main (e.g., medical research journals), how dowe automatically build a Knowledge Graph(KG) for that domain?
Automatic identifica-tion of relations and their schemas, i.e., typesignature of arguments of relations (e.g., un-dergo(Patient, Surgery)), is an important firststep towards this goal.
We refer to this prob-lem as Relation Schema Induction (RSI).
Inthis paper, we propose Schema Induction us-ing Coupled Tensor Factorization (SICTF), anovel tensor factorization method for relationschema induction.
SICTF factorizes OpenInformation Extraction (OpenIE) triples ex-tracted from a domain corpus along with ad-ditional side information in a principled wayto induce relation schemas.
To the best ofour knowledge, this is the first applicationof tensor factorization for the RSI problem.Through extensive experiments on multiplereal-world datasets, we find that SICTF is notonly more accurate than state-of-the-art base-lines, but also significantly faster (about 14xfaster).1 IntroductionOver the last few years, several techniques to buildKnowledge Graphs (KGs) from large unstructuredtext corpus have been proposed, examples includeNELL (Mitchell et al, 2015) and Google Knowl-edge Vault (Dong et al, 2014).
Such KGs con-sist of millions of entities (e.g., Oslo, Norway, etc.
),their types (e.g., isA(Oslo, City), isA(Norway, Coun-try)), and relationships among them (e.g., cityLo-catedInCountry(Oslo, Norway)).
These KG con-struction techniques are called ontology-guided asthey require as input list of relations, their schemas(i.e., their type signatures, e.g., cityLocatedInCoun-try(City, Country)), and seed instances of each suchrelation.
Listing of such relations and their schemasare usually prepared by human domain experts.The reliance on domain expertise poses signif-icant challenges when such ontology-guided KGconstruction techniques are applied to domainswhere domain experts are either not available or aretoo expensive to employ.
Even when such a domainexpert may be available for a limited time, she maybe able to provide only a partial listing of relationsand their schemas relevant to that particular domain.Moreover, this expert-mediated model is not scal-able when new data in the domain becomes avail-able, bringing with it potential new relations of in-terest.
In order to overcome these challenges, weneed automatic techniques which can discover rela-tions and their schemas from unstructured text dataitself, without requiring extensive human input.
Werefer to this problem as Relation Schema Induction(RSI).In contrast to ontology-guided KG constructiontechniques mentioned above, Open Information Ex-traction (OpenIE) techniques (Etzioni et al, 2011)aim to extract surface-level triples from unstructuredtext.
Such OpenIE triples may provide a suitablestarting point for the RSI problem.
In fact, KB-LDA,414Target task Interpretablelatent factors?Can induce relationschema?Can use NP sideinfo?Can use relationside info?Typed RESCAL (Chang etal., 2014a)Embedding No No Yes NoUniversal Schema (Singhet al, 2015)Link Prediction No No No NoKB-LDA (Movshovitz-Attias and Cohen, 2015)Ontology Induc-tionYes Yes Yes NoSICTF (this paper) Schema Induc-tionYes Yes Yes YesTable 1: Comparison among SICTF (this paper) and other related methods.
KB-LDA is the most related prior method which isextensively compared against SICTF in Section 4a topic modeling-based method for inducing an on-tology from SVO (Subject-Verb-Object) triples wasrecently proposed in (Movshovitz-Attias and Cohen,2015).
We note that ontology induction (Velardi etal., 2013) is a more general problem than RSI, as weare primarily interested in identifying categories andrelations from a domain corpus, and not necessar-ily any hierarchy over them.
Nonetheless, KB-LDAmaybe used for the RSI problem and we use it as arepresentative of the state-of-the-art of this area.Instead of a topic modeling approach, we take atensor factorization-based approach for RSI in thispaper.
Tensors are a higher order generalizationof matrices and they provide a natural way to rep-resent OpenIE triples.
Applying tensor factoriza-tion methods over OpenIE triples to identify relationschemas is a natural approach, but one that has notbeen explored so far.
Also, a tensor factorization-based approach presents a flexible and principledway to incorporate various types of side informa-tion.
Moreover, as we shall see in Section 4, com-pared to state-of-the-art baselines such as KB-LDA,tensor factorization-based approach results in betterand faster solution for the RSI problem.
In this pa-per, we make the following contributions:?
We present Schema Induction using CoupledTensor Factorization (SICTF), a novel andprincipled tensor factorization method whichjointly factorizes a tensor constructed out ofOpenIE triples extracted from a domain corpus,along with various types of additional side in-formation for relation schema induction.?
We compare SICTF against state-of-the-artbaseline on various real-world datasets fromdiverse domains.
We observe that SICTF isnot only significantly more accurate than suchbaselines, but also much faster.
For example,SICTF achieves 14x speedup over KB-LDA(Movshovitz-Attias and Cohen, 2015).?
We have made the data and code available 1.2 Related WorkSchema Induction: Properties of SICTF and otherrelated methods are summarized in Table 12.
Amethod for inducing (binary) relations and the cat-egories they connect was proposed by (Mohamed etal., 2011).
However, in that work, categories andtheir instances were known a-priori.
In contrast,in case of SICTF, both categories and relations areto be induced.
A method for event schema induc-tion, the task of learning high-level representationsof complex events and their entity roles from unla-beled text, was proposed in (Chambers, 2013).
Thisgives the schemas of slots per event, but our goalis to find schemas of relations.
(Chen et al, 2013)and (Chen et al, 2015) deal with the problem offinding semantic slots for unsupervised spoken lan-guage understanding, but we are interested in find-ing schemas of relations relevant for a given domain.Methods for link prediction in the Universal Schemasetting using matrix and a combination of matrixand tensor factorization are proposed in (Riedel etal., 2013) and (Singh et al, 2015), respectively.
In-stead of link prediction where relation schemas areassumed to be given, SICTF focuses on discoveringsuch relation schemas.
Moreover, in contrast to such1https://github.com/malllabiisc/sictf2Please note that not all methods mentioned in the table aredirectly comparable with SICTF, the table only illustrates thedifferences.
KB-LDA is the only method which is directly com-parable.415Figure 1: Relation Schema Induction (RSI) by SICTF, the proposed method.
First, a tensor (X) is constructed to represent OpenIEtriples extracted from a domain corpus.
Noun phrase side information in the form of (noun phrase, hypernym), and relation-relationsimilarity side information are separately calculated and stored in two separate matrices (W and S, respectively).
SICTF thenperforms coupled factorization of the tensor and the two side information matrices to identify relation schemas which are stored inthe core tensor (R) in the output.
Please see Section 3 for details.methods which assume access to existing KGs, thesetting in this paper is unsupervised.Tensor Factorization: Due to their flexibilityof representation and effectiveness, tensor factor-ization methods have seen increased application inKnowledge Graph (KG) related problems over thelast few years.
Methods for decomposing ontolog-ical KGs such as YAGO (Suchanek et al, 2007)were proposed in (Nickel et al, 2012; Chang et al,2014b; Chang et al, 2014a).
In these cases, rela-tion schemas are known in advance, while we areinterested in inducing such relation schemas fromunstructured text.
A PARAFAC (Harshman, 1970)based method for jointly factorizing a matrix andtensor for data fusion was proposed in (Acar et al,2013).
In such cases, the matrix is used to provideauxiliary information (Narita et al, 2012; Erdos andMiettinen, 2013).
Similar PARAFAC-based ideasare explored in Rubik (Wang et al, 2015) to fac-torize structured electronic health records.
In con-trast to such structured data sources, SICTF aimsat inducing relation schemas from unstructured textdata.
Propstore, a tensor-based model for distribu-tional semantics, a problem different from RSI, waspresented in (Goyal et al, 2013).
Even though cou-pled factorization of tensor and matrices constructedout of unstructured text corpus provide a natural andplausible approach for the RSI problem, they havenot yet been explored ?
we fill this gap in this paper.Ontology Induction: Relation Schema Induc-tion can be considered a sub problem of Ontol-ogy Induction (Velardi et al, 2013).
Instead ofbuilding a full-fledged hierarchy over categoriesand relations as in ontology induction, we are par-ticularly interested in finding relations and theirschemas from unstructured text corpus.
We considerKB-LDA3 (Movshovitz-Attias and Cohen, 2015), atopic-modeling based approach for ontology induc-tion, as a representative of this area.
Among all priorwork, KB-LDA is most related to SICTF.
Whileboth KB-LDA and SICTF make use of noun phraseside information, SICTF is also able to exploit rela-tional side information in a principled manner.
InSection 4, through experiments on multiple real-world datasets, we observe that SICTF is not onlymore accurate than KB-LDA but also significantlyfaster with a speedup of 14x.A method for canonicalizing noun and relationphrases in OpenIE triples was recently proposed in(Gala?rraga et al, 2014).
The main focus of this ap-proach is to cluster lexical variants of a single entityor relation.
This is not directly relevant for RSI, as3In this paper, whenever we refer to KB-LDA, we only referto the part of it that learns relations from unstructured data.416we are interested in grouping multiple entities of thesame type into one cluster, and use that to inducerelation schema.3 Our Approach: Schema Induction usingCoupled Tensor Factorization (SICTF)3.1 OverviewSICTF poses the relation schema induction problemas a coupled factorization of a tensor along with ma-trices containing relevant side information.
Over-all architecture of the SICTF system is presentedin Figure 1.
First, a tensor X ?
Rn?n?m+ is con-structed to store OpenIE triples and their scores ex-tracted from the text corpus4.
Here, n and m rep-resent the number of NPs and relation phrases, re-spectively.
Following (Movshovitz-Attias and Co-hen, 2015), SICTF makes use of noun phrase (NP)side information in the form of (noun phrase, hyper-nym).
Additionally, SICTF also exploits relation-relation similarity side information.
These two sideinformation are stored in matrices W ?
{0, 1}n?hand S ?
{0, 1}m?m, where h is the number of hy-pernyms extracted from the corpus.
SICTF then per-forms collective non-negative factorization over X ,W , and S to output matrix A ?
Rn?c+ and the coretensor R ?
Rc?c?m+ .
Each row in A correspondsto an NP, while each column corresponds to an in-duced category (latent factor).
For brevity, we shallrefer to the induced category corresponding to theqth column of A as Aq.
Each entry Apq in the out-put matrix provides a membership score for NP pin induced category Aq.
Please note that each in-duced category is represented using the NPs partic-ipating in it, with the NPs ranked by their member-ship scores in the induced category.
In Figure 1,A2 = [(John, 0.9), (Sam, 0.8), .
.
.]
is an induced cat-egory.Each slice of the core tensor R is a matrix whichcorresponds to a specific relation, e.g., the matrixRundergo highlighted in Figure 1 corresponds to therelation undergo.
Each cell in this matrix corre-sponds to an induced schema connecting two in-duced categories (two columns of the A matrix),with the cell value representing model?s score ofthe induced schema.
For example, in Figure 1,undergo(A2, A4) is an induced relation schema with4R+ is the set of non-negative reals.MEDLINE(hypertension, disease), (hypertension, state), (hypertension,disorder) , (neutrophil, blood element), (neutrophil, effectorcell), (neutrophil, cell type)StackOverflow(image, resource), (image, content), (image, file), (perl, lan-guage), (perl, script), (perl, programs)Table 2: Noun Phrase (NP) side information in the form of(Noun Phrase, Hypernym) pairs extracted using Hearst patternsfrom two different datasets.
Please see Section 3.2 for details.MEDLINE StackOverflow(evaluate, analyze), (evaluate,examine), (indicate, confirm),(indicate, suggest)(provides, confirms), (pro-vides, offers), (allows, lets),(allows, enables)Table 3: Examples of relation similarity side information in theform of automatically identified similar relation pairs.
Pleasesee Section 3.2 for details.score 0.8 involving relation undergo and inducedcategories A2 and A4.In Section 3.2, we present details of the side in-formation used by SICTF, and then in Section 3.3present details of the optimization problem solvedby SICTF.3.2 Side Information?
Noun Phrase Side Information: Through thistype of side information, we would like to cap-ture type information of as many noun phrases(NPs) as possible.
We apply Hearst patterns(Hearst, 1992), e.g., ?<Hypernym> such as<NP>?, over the corpus to extract such (NP,Hypernym) pairs.
Please note that neither hy-pernyms nor NPs are pre-specified, and they areall extracted from the data by the patterns.
Ex-amples of a few such pairs extracted from twodifferent datasets are shown in Table 2.
Theseextracted tuples are stored in a matrix Wn?hwhose rows correspond to NPs and columnscorrespond to extracted hypernyms.
We define,Wij ={1, if NPi belongs to Hypernymj0, otherwise .Please note that we don?t expectW to be a fullyspecified matrix, i.e., we don?t assume that weknow all possible hypernyms for a given NP.?
Relation Side Information: In addition to theside information involving NPs, we would also417like to take prior knowledge about textual rela-tions into account during factorization.
For ex-ample, if we know two relations to be similar toone another, then we also expect their inducedschemas to be similar as well.
Consider thefollowing sentences ?Mary purchased a stuffedanimal toy.?
and ?Janet bought a toy car forher son.?.
From these we can say that both re-lations purchase and buy have the schema (Per-son, Item).
Even if one of these relations ismore abundant than the other in the corpus, westill want to learn similar schemata for both therelations.
As mentioned before, S ?
Rm?m+ isthe relation similarity matrix, where m is thenumber of textual relations.
We define,Sij ={1, if Similarity(Reli, Relj) ?
?0, otherwisewhere ?
is a threshold5.
For the experimentsin this paper, we use cosine similarity overword2vec (Mikolov et al, 2013) vector repre-sentations of the relational phrases.
Examplesof a few similar relation pairs are shown in Ta-ble 3.3.3 SICTF Model DetailsSICTF performs coupled non-negative factorizationof the input triple tensor Xn?n?m along with thetwo side information matrices Wn?h and Sm?m bysolving the following optimization problem.minA,V,Rm?k=1f(Xk, A,Rk) + fnp(W,A, V ) + frel(S,R)(1)where,f(Xk, A,Rk) =?
X:,:,k ?AR:,:,kAT ?2F +?R ?
R:,:,k ?2Ffnp(W,A, V ) = ?np ?W ?AV ?2F +?A ?
A ?2F+ ?V ?
V ?2Ffrel(S,R) = ?relm?i=1m?j=1Sij ?
R:,:,i ?R:,:,j ?2FAi,j ?
0,Vj,r ?
0, Rp,q,k ?
0 (non negative)?
1 ?
i ?
n, 1 ?
r ?
h,1 ?
j, p, q ?
c, 1 ?
k ?
m5For the experiments in this paper, we set ?
= 0.7, arelatively high value, to focus on highly similar relations andthereby justifying the binary S matrix.In the objective above, the first term f(Xk, A,Rk)minimizes reconstruction error for the kth relation,with additional regularization on the R:,:,k matrix6.The second term, fnp(W,A, V ), factorizes the NPside information matrix Wn?h into two matricesAn?c and Vc?h, where c is the number of inducedcategories.
We also enforce A to be non-negative.Typically, we require c  h to get a lower dimen-sional embedding of each NP (rows of A).
Finally,the third term frel(S,R) enforces the requirementthat two similar relations as given by the matrix Sshould have similar signatures (given by the corre-sponding R matrix).
Additionally, we require Vand R to be non-negative, as marked by the (non-negative) constraints.
In this objective, ?R, ?np, ?A,?V , and ?rel are all hyper-parameters.We derive non-negative multiplicative updates forA, Rk and V following the rules proposed in (Leeand Seung, 2000), which has the following generalform:?i = ?i???C(?)???i?C(?)+??i???HereC(?)
represents the cost function of the non-negative variables ?
and ?C(?)??
?i and?C(?)??
?i are thenegative and positive parts of the derivative of C(?
)(M?rup et al, 2008).
(Lee and Seung, 2000) provedthat for ?
= 1, the cost functionC(?)
monotonicallydecreases with the multiplicative updates 7.
C(?)
forSICTF is given in equation (1).
The above procedurewill give the following updates:A ?
A ?
?k(XkARTk +XTk ARk) + ?npWV TA(B?
+ ?AI + ?npV V T )B?
=?k(RkATARTk +RTkATARk)Rk ?
Rk ?ATXkA+ 2 ?relm?j=1RjSkjATARkATA+ D?D?
= 2 ?rel Rkm?j=1Skj + ?RRkV ?
V ?
?npATW?npATAV + ?V V6For brevity, we also refer to R:,:,k as Rk, and similarlyX:,:,k as Xk7We also use ?
= 1.418Dataset # Docs # TriplesMEDLINE 50,216 2,499StackOverflow 5.5m 37,439Table 4: Datasets used in the experiments.In the equations above, ?
is the Hadamard orelement-wise product8.
In all our experiments, wefind the iterative updates above to converge in about10-20 iterations.4 ExperimentsIn this section, we evaluate performance of differ-ent methods on the Relation Schema Induction (RSI)task.
Specifically, we address the following ques-tions.?
Which method is most effective on the RSItask?
(Section 4.3.1)?
How important are the additional side informa-tion for RSI?
(Section 4.3.2)?
What is the importance of non-negativity inRSI with tensor factorization?
(Section 4.3.3)4.1 Experimental SetupDatasets: We used two datasets for the experi-ments in this paper, they are summarized in Table 4.For MEDLINE dataset, we used Stanford CoreNLP(Manning et al, 2014) for coreference resolutionand Open IE v4.09 for triple extraction.
Triples withNoun Phrases that have Hypernym information wereretained.
We obtained the StackOverflow triples di-rectly from the authors of (Movshovitz-Attias andCohen, 2015), which were also prepared using avery similar process.
In both datasets, we use cor-pus frequency of triples for constructing the tensor.Side Information: Seven Hearst patterns suchas ?<hypernym> such as <NP>?, ?<NP> orother <hypernym>?
etc., given in (Hearst, 1992)were used to extract NP side information from theMEDLINE documents.
NP side information for theStackOverflow dataset was obtained from the au-thors of (Movshovitz-Attias and Cohen, 2015).As described in Section 3, word2vec embeddingsof the relation phrases were used to extract relation-similarity based side-information.
This was done for8(A ?B)i,j = Ai,j ?Bi,j9Open IE v4.0: http://knowitall.github.io/openie/both datasets.
Cosine similarity threshold of ?
= 0.7was used for the experiments in the paper.Samples of side information used in the experi-ments are shown in Table 2 and Table 3.
A totalof 2067 unique NP-hypernym pairs were extractedfrom MEDLINE data and 16,639 were from Stack-Overflow data.
25 unique pairs of relation phrasesout of 1172 were found to be similar in MEDLINEdata, whereas 280 unique pairs of relation phrasesout of approximately 3200 were found similar inStackOverflow data.Hyperparameters were tuned using grid searchand the set which gives minimum reconstruction er-ror for both X and W was chosen.
We set ?np =?rel = 100 for StackOverflow, and ?np = 0.05 and?rel = 0.001 for Medline and we use c = 50 for ourexperiments.
Please note that our setting is unsuper-vised, and hence there is no separate train, dev andtest sets.4.2 Evaluation ProtocolIn this section, we shall describe how the inducedschemas are presented to human annotators and howfinal accuracies are calculated.
In factorizationsproduced by SICTF and other ablated versions ofSICTF, we first select a few top relations with bestreconstruction score.
The schemas induced for eachselected relation k is represented by the matrix sliceRk of the core tensor obtained after factorization(see Section 3).
From each such matrix, we iden-tify the indices (i, j) with highest values.
The in-dices i and j select columns of the matrix A. Afew top ranking NPs from the columns Ai and Ajalong with the relation k are presented to the hu-man annotator, who then evaluates whether the tupleRelationk(Ai, Aj) constitutes a valid schema for re-lation k. Examples of a few relation schemas in-duced by SICTF are presented in Table 5.
A humanannotator would see the first and second columns ofthis table and then offer judgment as indicated inthe third column of the table.
All such judgmentsacross all top-reconstructed relations are aggregatedto get the final accuracy score.
This evaluation pro-tocol was also used in (Movshovitz-Attias and Co-hen, 2015) to measure learned relation accuracy.All evaluations were blind, i.e., the annotatorswere not aware of the method that generated theoutput they were evaluating.
Moreover, the anno-419Relation Schema Top 3 NPs in Induced Categories which were presented to annotators Annotator JudgmentStackOveflowclicks(A0, A1) A0: users, client, person validA1: link, image, itemrefreshes(A19, A13) A19: browser, window, tab validA13: page, activity, appcan parse(A41, A17) A41: access, permission, ability invalidA17: image file, header file, zip fileMEDLINEsuffer from(A38, A40) A38: patient, first patient, anesthetized patient validA40: viral disease, renal disease, von recklin ghausen?s diseasehave undergo(A3, A37) A3: fifth patient, third patient, sixth patient validA37: initial liver biopsy, gun biopsy, lymph node biopsyhave discontinue(A41, A20) A41: patient, group, no patient invalidA20: endemic area, this area, fiber areaTable 5: Examples of relation schemas induced by SICTF from the StackOverflow and MEDLINE datasets.
Top NPs from each ofthe induced categories, along with human judgment of the induced schema are also shown.
See Section 4.3.1 for more details.
(a) (b)Figure 2: (a) Relation Schema Induction (RSI) accuracies of different methods on the two datasets.
SICTF, our proposed method,significantly outperforms state-of-the-art method KBLDA.
This is the main result of the paper.
Results for KB-LDA on StackOve-flow are directly taken from the paper.
Please see Section 4.3.1 for details.
(b) Runtime comparison between KB-LDA and SICTF.We observe that SICTF results in 14x speedup over KB-LDA.
Please see Section 4.3.1 (Runtime Comparison) for details.tators are experts in software domain and has high-school level knowledge in medical domain.
Thoughrecall is a desirable statistic to measure, it is verychallenging to calculate it in our setting due to thenon-availability of relation schema annotated text onlarge scale.4.3 Results4.3.1 Effectiveness of SICTFExperimental results comparing performance ofvarious methods on the RSI task in the two datasetsare presented in Figure 2(a).
RSI accuracy is cal-culated based on the evaluation protocol describedin Section 4.2.
Performance number of KB-LDAfor StackOveflow dataset is taken directly from the(Movshovitz-Attias and Cohen, 2015) paper, weused our implementation of KB-LDA for the MED-LINE dataset.
Annotation accuracies from two an-notators were averaged to get the final accuracy.From Figure 2(a), we observe that SICTF outper-forms KB-LDA on the RSI task.
Please note thatthe inter-annotator agreement for SICTF is 88% and97% for MEDLINE and StackOverflow datasets re-spectively.
This is the main result of the paper.In addition to KB-LDA, we also compared SICTFwith PARAFAC, a standard tensor factorizationmethod.
PARAFAC induced extremely poor andsmall number of relation schemas, and hence wedidn?t consider it any further.Runtime comparison: Runtimes of SICTF andKB-LDA over both datasets are compared in Fig-ure 2(b).
From this figure, we find that SICTF isable to achieve a 14x speedup on average over KB-LDA10.
In other words, SICTF is not only able to10Runtime of KB-LDA over the StackOverflow dataset wasobtained from the authors of (Movshovitz-Attias and Cohen,2015) through personal communication.
Our own implementa-tion also resulted in similar runtime over this dataset.420Ablation MEDLINE StackOverflowA1 A2 Avg A1 A2 AvgSICTF 0.64 0.64 0.64 0.96 0.92 0.94SICTF (?rel = 0) 0.60 0.56 0.58 0.83 0.70 0.77SICTF (?np = 0) 0.46 0.40 0.43 0.89 0.90 0.90SICTF (?rel=0, ?np = 0) 0.46 0.50 0.48 0.84 0.33 0.59SICTF (?rel=0, ?np = 0, and no non-negativity constraints ) 0.14 0.10 0.12 0.20 0.14 0.17Table 6: RSI accuracy comparison of SICTF with its ablated versions when no relation side information is used (?rel = 0), whenno NP side information is used (?np = 0), when no side information of any kind is used (?rel = 0, ?np = 0), and when additionallythere are no non-negative constraints.
From this, we observe that additional side information improves performance, validating oneof the central thesis of this paper.
Please see Section 4.3.2 and Section 4.3.3 for details.induce better relation schemas, but also do so at asignificantly faster speed.4.3.2 Importance of Side InformationOne of the central hypothesis of our approach isthat coupled factorization through additional side in-formation should result in better relation schema in-duction.
In order to evaluate this thesis further, wecompare performance of SICTF with its ablated ver-sions: (1) SICTF (?rel = 0), which corresponds tothe setting when no relation side information is used,(2) SICTF (?np = 0), which corresponds to the set-ting when no noun phrases side information is used,and (3) SICTF (?rel = 0, ?np = 0), which corre-sponds to the setting when no side information ofany kind is used.
Hyperparameters are separatelytuned for the variants of SICTF.
Results are pre-sented in the first four rows of Table 6.
From this,we observe that additional coupling through the sideinformation significantly helps improve SICTF per-formance.
This further validates the central thesis ofour paper.4.3.3 Importance of Non-Negativity onRelation Schema InductionIn the last row of Table 6, we also present anablated version of SICTF when no side informa-tion no non-negativity constraints are used.
Com-paring the last two rows of this table, we observethat non-negativity constraints over the A matrixand core tensor R result in significant improvementin performance.
We note that the last row in Ta-ble 6 is equivalent to RESCAL (Nickel et al, 2011)and the fourth row is equivalent to Non-NegativeRESCAL (Krompa?
et al, 2013), two tensor factor-ization techniques.
We also note that none of thesetensor factorization techniques have been previouslyused for the relation schema induction problem.The reason for this improved performance may beexplained by the fact that absence of non-negativityconstraint results in an under constrained factoriza-tion problem where the model often overgeneratesincorrect triples, and then compensates for this over-generation by using negative latent factor weights.In contrast, imposition of non-negativity constraintsrestricts the model further forcing it to commit tospecific semantics of the latent factors in A. Thisimproved interpretability also results in better RSIaccuracy as we have seen above.
Similar benefits ofnon-negativity on interpretability have also been ob-served in matrix factorization (Murphy et al, 2012).5 ConclusionRelation Schema Induction (RSI) is an importantfirst step towards building a Knowledge Graph(KG) out of text corpus from a given domain.While human domain experts have traditionally pre-pared listing of relations and their schemas, thisexpert-mediated model poses significant challengesin terms of scalability and coverage.
In orderto overcome these challenges, in this paper, wepresent SICTF, a novel non-negative coupled ten-sor factorization method for relation schema in-duction.
SICTF is flexible enough to incorporatevarious types of side information during factoriza-tion.
Through extensive experiments on real-worlddatasets, we find that SICTF is not only more accu-rate but also significantly faster (about 14x speedup)compared to state-of-the-art baselines.
As part offuture work, we hope to analyze SICTF further, as-421sign labels to induced categories, and also apply themodel to more domains.AcknowledgementThanks to the members of MALL Lab, IISc whoread our drafts and gave valuable feedback and wealso thank the reviewers for their constructive re-views.
This research has been supported in partby Bosch Engineering and Business Solutions andGoogle.ReferencesEvrim Acar, Morten Arendt Rasmussen, Francesco Savo-rani, Tormod Ns, and Rasmus Bro.
2013.
Understand-ing data fusion within the framework of coupled ma-trix and tensor factorizations.
Chemometrics and In-telligent Laboratory Systems, 129(Complete):53?63.Nathanael Chambers.
2013.
Event schema inductionwith a probabilistic entity-driven model.
In EMNLP,pages 1797?1807.
ACL.Kai-Wei Chang, Wen tau Yih, Bishan Yang, and Christo-pher Meek.
2014a.
Typed tensor decomposition ofknowledge bases for relation extraction.
In Proceed-ings of the 2014 Conference on Empirical Methods inNatural Language Processing.
ACL Association forComputational Linguistics, October.Kai-Wei Chang, Wen-tau Yih, Bishan Yang, and Christo-pher Meek.
2014b.
Typed tensor decomposition ofknowledge bases for relation extraction.
In Proceed-ings of the 2014 Conference on Empirical Methods inNatural Language Processing (EMNLP), pages 1568?1579.Yun-Nung Chen, William Y. Wang, and Alexander I.Rudnicky.
2013.
Unsupervised induction and fill-ing of semantic slots for spoken dialogue systems us-ing frame-semantic parsing.
In 2013 IEEE Workshopon Automatic Speech Recognition and Understanding(ASRU), pages 120?125.
IEEE.Yun-Nung Chen, William Yang Wang, Anatole Gersh-man, and Alexander I. Rudnicky.
2015.
Matrix fac-torization with knowledge graph propagation for unsu-pervised spoken language understanding.
In ACL (1),pages 483?494.
The Association for Computer Lin-guistics.Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, WilkoHorn, Ni Lao, Kevin Murphy, Thomas Strohmann,Shaohua Sun, and Wei Zhang.
2014.
Knowledgevault: A web-scale approach to probabilistic knowl-edge fusion.
In Proceedings of the 20th ACM SIGKDDinternational conference on Knowledge discovery anddata mining, pages 601?610.
ACM.Dora Erdos and Pauli Miettinen.
2013.
Discovering factswith boolean tensor tucker decomposition.
In Pro-ceedings of the 22Nd ACM International Conferenceon Information & Knowledge Management, CIKM?13, pages 1569?1572, New York, NY, USA.
ACM.Oren Etzioni, Anthony Fader, Janara Christensen,Stephen Soderland, and Mausam Mausam.
2011.Open information extraction: The second generation.In IJCAI, volume 11, pages 3?10.Luis Gala?rraga, Geremy Heitz, Kevin Murphy, andFabian Suchanek.
2014.
Canonicalizing Open Knowl-edge Bases.
CIKM.Kartik Goyal, Sujay Kumar, Jauhar Huiying, Li Mrin-maya, Sachan Shashank, and Srivastava Eduard Hovy.2013.
A structured distributional semantic model: In-tegrating structure with semantics.R.
A. Harshman.
1970.
Foundations of the PARAFACprocedure: Models and conditions for an?
explana-tory?
multi-modal factor analysis.
UCLA Working Pa-pers in Phonetics, 16(1):84.Marti A. Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In In Proceedings ofthe 14th International Conference on ComputationalLinguistics, pages 539?545.Denis Krompa?, Maximilian Nickel, Xueyan Jiang, andVolker Tresp.
2013.
Non-negative tensor factorizationwith rescal.
Tensor Methods for Machine Learning,ECML workshop.Daniel D. Lee and H. Sebastian Seung.
2000.
Algo-rithms for non-negative matrix factorization.
In InNIPS, pages 556?562.
MIT Press.Christopher D. Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven J. Bethard, and David McClosky.2014.
The Stanford CoreNLP natural language pro-cessing toolkit.
In Proceedings of 52nd Annual Meet-ing of the Association for Computational Linguistics:System Demonstrations, pages 55?60.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representationsof words and phrases and their compositionality.
InC.J.C.
Burges, L. Bottou, M. Welling, Z. Ghahramani,and K.Q.
Weinberger, editors, Advances in Neural In-formation Processing Systems 26, pages 3111?3119.Curran Associates, Inc.T.
Mitchell, W. Cohen, E. Hruschka, P. Talukdar, J. Bet-teridge, A. Carlson, B. Dalvi, M. Gardner, B. Kisiel,J.
Krishnamurthy, N. Lao, K. Mazaitis, T. Mohamed,N.
Nakashole, E. Platanios, A. Ritter, M. Samadi,B.
Settles, R. Wang, D. Wijaya, A. Gupta, X. Chen,A.
Saparov, M. Greaves, and J. Welling.
2015.
Never-ending learning.
In Proceedings of AAAI.Thahir P. Mohamed, Estevam R. Hruschka, Jr., andTom M. Mitchell.
2011.
Discovering relations be-422tween noun categories.
In Proceedings of the Confer-ence on Empirical Methods in Natural Language Pro-cessing, EMNLP ?11, pages 1447?1455, Stroudsburg,PA, USA.
Association for Computational Linguistics.M.
M?rup, L. K. Hansen, and S. M. Arnfred.
2008.
Al-gorithms for sparse non-negative TUCKER.
NeuralComputation, 20(8):2112?2131, aug.Dana Movshovitz-Attias and William W. Cohen.
2015.Kb-lda: Jointly learning a knowledge base of hierar-chy, relations, and facts.
In Proceedings of the 53rdAnnual Meeting of the Association for ComputationalLinguistics.
Association for Computational Linguis-tics.Brian Murphy, Partha Pratim Talukdar, and Tom MMitchell.
2012.
Learning effective and interpretablesemantic models using non-negative sparse embed-ding.
In COLING, pages 1933?1950.Atsuhiro Narita, Kohei Hayashi, Ryota Tomioka, andHisashi Kashima.
2012.
Tensor factorization usingauxiliary information.
Data Mining and KnowledgeDiscovery, 25(2):298?324.Maximilian Nickel, Volker Tresp, and Hans-PeterKriegel.
2011.
A three-way model for collectivelearning on multi-relational data.
In Lise Getoor andTobias Scheffer, editors, Proceedings of the 28th In-ternational Conference on Machine Learning (ICML-11), ICML ?11, pages 809?816, New York, NY, USA,June.
ACM.Maximilian Nickel, Volker Tresp, and Hans-PeterKriegel.
2012.
Factorizing yago: Scalable machinelearning for linked data.
In Proceedings of the 21stInternational Conference on World Wide Web, WWW?12, pages 271?280, New York, NY, USA.
ACM.Sebastian Riedel, Limin Yao, Andrew McCallum, andBenjamin M. Marlin.
2013.
Relation extractionwith matrix factorization and universal schemas.
InHuman Language Technologies: Conference of theNorth American Chapter of the Association of Com-putational Linguistics, Proceedings, June 9-14, 2013,Westin Peachtree Plaza Hotel, Atlanta, Georgia, USA,pages 74?84.Sameer Singh, Tim Rockta?schel, and Sebastian Riedel.2015.
Towards Combined Matrix and Tensor Factor-ization for Universal Schema Relation Extraction.
InNAACL Workshop on Vector Space Modeling for NLP(VSM).Fabian M Suchanek, Gjergji Kasneci, and GerhardWeikum.
2007.
Yago: a core of semantic knowledge.In Proceedings of WWW.Paola Velardi, Stefano Faralli, and Roberto Navigli.2013.
Ontolearn reloaded: A graph-based algorithmfor taxonomy induction.
Computational Linguistics,39(3):665?707.Yichen Wang, Robert Chen, Joydeep Ghosh, Joshua C.Denny, Abel N. Kho, You Chen, Bradley A. Malin, andJimeng Sun.
2015.
Rubik: Knowledge guided tensorfactorization and completion for health data analytics.In Longbing Cao, Chengqi Zhang, Thorsten Joachims,Geoffrey I. Webb, Dragos D. Margineantu, and Gra-ham Williams, editors, KDD, pages 1265?1274.
ACM.423
