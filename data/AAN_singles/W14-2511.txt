Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 33?37,Baltimore, Maryland, USA, June 26, 2014.c?2014 Association for Computational LinguisticsInformation density, Heaps?
Law, and perception of factiness in newsMiriam BoonTechnology and Social Behavior, Northwestern UniversityEvanston, IL 60208, USAMiriamBoon2012@u.northwestern.eduAbstractSeeking information online can be an exer-cise in time wasted wading through repeti-tive, verbose text with little actual content.Some documents are more densely popu-lated with factoids (fact-like claims) thanothers.
The densest documents are poten-tially the most efficient use of time, likelyto include the most information.
Thussome measure of ?factiness?
might be use-ful to readers.
Based on crowdsourced rat-ings of the factual content of 772 onlinenews articles, we find that after controllingfor widely varying document length usingHeaps?
Law, a significant positive correla-tion exists between perceived factual con-tent and relative information entropy.1 IntroductionIn today?s information-based society, finding ac-curate information is of concern to everyone.There are many obstacles to this goal.
Not all peo-ple are equally skilled at judging the veracity ofa factoid (a term used here to indicate somethingthat is stated as a fact, but that may or may not ac-tually be true.).
Nor is it always easy to find thesingle drop of content you need amidst the oceansof the Internet.
Even for those equipped with bothskill and access, time is always a limiting factor.It is this last problem with which this paper isconcerned.
How can we identify content that mostefficiently conveys the most information, giventhat any information seeker?s time is limited?1.1 The difficulty with factoidsImagine that we must select from a set ofdocuments those that efficiently convey themost information in the fewest words possi-ble; that is, those with the highest factoid rate,count(factoids)/count(words).
A human do-ing this by hand would count the factoids andwords in each document.
Automating this exactapproach would require ?teaching?
the computerto identify unique factoids in a document, whichrequires being able to recognize and discard re-dundant factoids, which requires at least a rudi-mentary understanding of each factoid?s meaning.These are all difficult tasks for a computer.Luckily, to achieve our goal, we don?t need toknow which sentences are factoids.
What we needis a good heuristic estimate of information densitythat computers can easily calculate.1.2 Linking vocabulary to factoidsTo insert new information into a text, an authormust add words, making the document longer.While the new information can sometimes be con-veyed using the same vocabulary as the rest ofthe text, if the information is sufficiently differentfrom what is already present, it will also likely in-troduce new vocabulary words.The result is that the introduction of a new fac-toid into a text is likely to also introduce new vo-cabulary, unless it is redundant.
Thus, the morenon-redundant factoids a text contains, the morevaried the vocabulary of the text is likely to be.1.3 From vocabulary to relative entropyVocabulary is commonly used in connection withShannon?s information entropy to measure suchthings as surprisal, redundancy, perplexity, and, ofcourse, information density (Shannon, 1949; Mc-Farlane et al., 2009).Entropy models text as being created via aMarkov process.
In its most basic form, it can bewritten as:H = ?KL?i=0pilog2pi(1)where K is a constant chosen based on the units,L is the length of the document, and piis theprobability of the ithword.
This equation works33equally well whether it is used for unigrams, bi-grams, or trigrams.Consider for a moment the relationship betweenentropy and length, vocabulary, and the probabil-ity of each vocabulary word.
Entropy increasesas both document length and vocabulary increase.Words with lower probability increase entropymore than those with higher probabilities.
Forthis study, probabilities were calculated based oncorpus-wide frequencies.
This means that, in the-ory, a large number of the words in a documentcould have very low probability.Given two documents of equal length on thesame topic, only one of which is rich in infor-mation, we might wonder why the information-poor document is, relatively speaking, so long orthe information-rich document is so short.
Thiscan be explained by noting that: 1.
?translation?into simpler versions of a language often leads toa longer text, 2. simple versions of languages gen-erally consist of the most common words in thatlanguage, and 3. words that are less common of-ten have more specific, information-dense, com-plex meanings.
Similarly, inefficient word choicestypically make excessive use of highly probablefunction words, which do not increase the entropyas much as less common words.
Thus, we can ex-pect the entropy to be higher for the denser docu-ment.1.4 Controlling for document length withHeaps?
LawWhile entropy may not rise as fast with the repe-tition or addition of highly probable words, how-ever, every word added does still increase the en-tropy.
This follows naturally from the fact that forevery word added, another term is added to thesummation.
We can try to compensate by dividingby document length.
But dividing by documentlength doesn?t remove this dependency.
I proposethat this is because, as Heap?s Law tells us, thevocabulary used in a document has a positive re-lationship with document size (Heaps, 1978).
Tocontrol for this effect, I fit a curve for unigrams,bigrams, and trigrams to create a model for theserelationships; an example can be seen in Figure 1.I then used that model to calculate the expecteddocument length, expected entropy, and relativeentropy, as follows:Lexp= 10(log10v?b)/m(2)Hexp= HLexpL(3)Hrel=HHexp(4)Here the subscript ?exp?
stands for ?expected?and the subscript ?rel?
for ?relative.?
This cal-culation eliminates the dependency on documentlength.Figure 1: Top: As you can see there is a strong re-lationship between document length and entropy.R2=0.992, p > F: < 0.0001.
Bottom: Relativeentropy, which controls for that relationship, nolonger has a significant nor strong relationshipwith document length.
R2=0.0017, p > F: 0.24252 Data and AnalysisTo further pursue the hypothesis that residual en-tropy could be used to identify news articles withlots of factoids, and thus, a sense of ?factiness,?
alabeled data set is necessary.
Lots of websites al-low users to rate articles, but those ratings don?thave anything to do with the presence of factoids.Labeling a data set of adequate size by hand wouldbe tedious, time-consuming, and costly.34Figure 2: Mousing over the question makes thetext ?Is it based on facts or opinions??
appear inpale grey text.
Clicking on the question mark iconnext to the question, ?Is this story factual??
revealsan explanation of what the user should be rating.2.1 Crowdsourcing with NewsTrustFortunately, a project called NewsTrust provided afeasible alternative.
NewsTrust, founded in 2005by Fabrice Florin, created four sets of specific re-view questions designed to inspire reviewers tothink critically about the quality of articles they re-view.
NewsTrust partnered with independent aca-demic researchers Cliff Lampe and Kelly Garrettto validate the review questions.
They jointly ad-ministered a survey in which respondents wereasked to complete one of the review instrumentsregarding either the original version of an articleor blog post, or a degraded version.The independent study found that even theless experienced, less knowledgeable readers wereable to distinguish between the two versions ofthe story.
The shortest review instrument, withonly one question, had the most discriminatingpower, while the slightly longer normative re-view instrument (which added five more ques-tions) yielded responses from non-experts thatmost closely matched those of NewsTrust?s expertjournalists (Lampe and Garrett, 2007; Florin etal., 2006; Florin, 2009).Using their validated survey instrument, New-sTrust created a system that allowed users to readarticles elsewhere, rate them using one of the fourreview instruments, and even rate other NewsTrustusers?
reviews of articles.
Each user has a trust-worthiness rating (which can be bolstered by be-coming validated as a journalist expert), and eacharticle has a composite rating, a certainty level forthat rating, reviews, and ratings of reviews.One of the dimensions of journalistic qualityfor which NewsTrust users rate articles is called?facts?.
This can be taken as an aspect of ?facti-ness?
: the extent to which people perceived thearticle as truthful and factual.
It follows that, tothe extent that the users are making a good-faithattempt to rate articles based on facts regardless ofthe soundness of their judgment about what is oris not true, articles with a high rating for ?facts?should have more factoids, and therefore a higherdensity of information.2.2 Data acquisitionWhen this research project was launched, New-sTrust had recently been acquired by the PoynterInstitute.
Although they were open to making theirdata available for research purposes, they were notyet able to access the data in order to do so.
In-stead, the review data for over 11000 stories fromNewsTrust?s political section were retrieved usingPython, Requests, and Beautiful Soup.
A combi-nation of Alchemy API, digital library archives,and custom scrapers for 19 different publicationwebsites were used to harvest the correspondingarticle texts.It quickly became clear, however, that it wouldnot be possible to completely capture all 11,000articles.
Some of the independent blogs and web-sites no longer existed.
Others had changed theirlink structure, making it difficult to find the cor-rect article.
A great deal of content was behindpaywalls, or simply did not have a webpage struc-ture that lent itself to clean extraction.
As the textwould be used for automated analysis, it was es-sential that the extracted text be as clean of de-tritus as possible.
As a result, the dataset shrankfrom a potential 11,000 rated articles to only 3300for which I could be confident of having cleantext.
Approximately 2600 of those articles havebeen rated by at least one NewsTrust user basedon factiness, and after removing any with fewerthan four facts ratings, the data set shrank fur-ther to only 8051articles.
Unigrams, bigrams, andtrigrams were extracted from these articles usingthe Natural Language Toolkit, NLTK; all text waslowercased, and only alphanumeric words wereincluded.2.3 AnalysisThe relationship between length and vocabularywas modeled using the optimize toolkit fromSciPy, and visualized with MatPlotLib.
The result-1One outlier document was removed.35Figure 3: Top: Bivariate fit for bigrams.
Bottom:Oneway ANOVA for bigrams.ing relationship was used to calculate the relativeentropy for each document.For bivariate analysis, 772 of these documentscould be used2.
But for the oneway analysis,documents needed to be separated into two dis-tinct clusters.
We used Weka?s K-Means cluster-ing algorithm to find the location of three clus-ters.
The 90% confidence interval for each ar-ticle (calculated using the invididual user ratingsfor ?facts?)
was used to determine cluster member-ship.
That is, articles for which that confidenceinterval would overlap with both the upper andlower cluster were discarded (738 documents intotal).
This process was repeated for 80 and 85%confidence levels; they yielded more data points(198 and 458), a higher level of significance, anda lower R2.
A 95% confidence level did not yieldenough articles with a low facts rating to analyze.3 Results and DiscussionThe bivariate analysis showed a small but signif-icant positive relationship between factual rating233 documents with particularly low confidence levels fortheir rating were removedand relative entropy as calculated for unigrams, bi-grams, and to a lesser extent, trigrams.
The resultscan be seen in Table 1 and Figure 3.
These re-lationships strengthened according to the ANOVAfor the more distinct high and low factiness classi-fications.If we accept the assumption that the articlesrated by NewsTrust users as highly factual willcontain a higher density of factoids, then this re-sult supports the hypothesis that relative entropy ispositively correlated with that characteristic.
Con-versely, if we accept the assumption that entropyshould be correlated with factoid density, then thisresult supports the claim that NewsTrust users ef-fectively identify articles that are more informa-tion dense.
Future work on the fact-rated sub-Unigram Bigram TrigramBivar.
R20.033 0.032 0.014p > F < 0.0001 < 0.0001 < 0.0008Oneway R20.086 0.084 0.082p > |t| 0.0154 0.0163 0.0178Table 1: Bivariate analysis (n = 772) and OnewayANOVA (n = 68).corpus has two obvious directions.
First, and mostclosely related to the work described in this pa-per, is the goal of proving either assumption ina more controlled experiment.
If one of theseassumptions can be supported, then it strength-ens the claim about the other, which will be in-teresting from both a linguistic perspective, anda human-computer interaction perspective.
Theother avenue of inquiry that follows naturally fromthis work is to look for other textual features thatmight, in combination, enable the automatic pre-diction of fact ratings based on article text.AcknowledgmentsThis work was partly supported by the Technol-ogy and Social Behavior program at Northwest-ern University, the National Science Foundation,the Knight Foundation, and Google.
Many thanksto Dr. Darren Gergle for his insight on the largerNewsTrust data set, to Dr. Janet Pierrehumbertfor her guidance on entropy and factiness, and toDr.
Larry Birnbaum for his intellectual guidanceas well as his assistance on this paper.36ReferencesFabrice Florin, Cliff Lampe, and Kelly Garrett.
2006.Survey report summary - NewsTrust.Fabrice Florin.
2009.
NewsTrust communications2009 report.
Technical report.Harold Stanley Heaps.
1978.
Information retrieval:Computational and theoretical aspects.
AcademicPress, Inc.Cliff Lampe and R. Kelly Garrett.
2007.
It?s all newsto me: The effect of instruments on ratings provi-sion.
In System Sciences, 2007.
HICSS 2007.
40thAnnual Hawaii International Conference on, page180b180b.Delano J. McFarlane, Noemie Elhadad, and RitaKukafka.
2009.
Perplexity analysis of obesity newscoverage.
AMIA Annual Symposium Proceedings,2009:426?430.
00001.Claude E. Shannon.
1949.
The mathematical theoryof communication.
Urbana, University of IllinoisPress.37
