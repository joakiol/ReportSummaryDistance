Proceedings of the SIGDIAL 2014 Conference, pages 273?281,Philadelphia, U.S.A., 18-20 June 2014.c?2014 Association for Computational LinguisticsOptimizing Generative Dialog State Trackervia Cascading Gradient DescentByung-Jun Lee1, Woosang Lim1, Daejoong Kim2, Kee-Eung Kim11Department of Computer Science, KAIST2LG ElectronicsAbstractFor robust spoken dialog management,various dialog state tracking methods havebeen proposed.
Although discriminativemodels are gaining popularity due to theirsuperior performance, generative modelsbased on the Partially Observable MarkovDecision Process model still remain at-tractive since they provide an integratedframework for dialog state tracking anddialog policy optimization.
Although astraightforward way to fit a generativemodel is to independently train the com-ponent probability models, we present agradient descent algorithm that simultane-ously train all the component models.
Weshow that the resulting tracker performscompetitively with other top-performingtrackers that participated in DSTC2.1 IntroductionSpoken dialog systems, a field rapidly growingwith the spread of smart mobile devices, has todeal with challenges to become a primary user in-terface for natural interaction using conversations.One of the challenges is to maintain the state ofthe dialog in the conversational process, which iscalled dialog state tracking.
The dialog state en-capsulates the information needed to successfullyfinish the dialog, such as users?
goal or requests,and thus it is an essential entity in spoken dia-log systems.
However, the error incurred by Au-tomatic Speech Recognition (ASR) and SpokenLanguage Understanding (SLU) makes the trueuser utterance not directly observable, and thismakes it difficult to figure out the true dialog state.Various methods have been used to constructdialog state trackers.
The traditional methodsused in most commercial systems use hand-craftedrules that typically rely on the most likely resultfrom SLU.
However, these rule-based systems areprone to frequent errors as the most likely resultis not always correct.
Hence, these systems of-ten drive the users to respond using simple key-words and to explicitly confirm everything theysay, which is far from a natural conversational in-teraction.
An accurate tracking of the dialog stateis crucial for natural and efficient dialogs.
On theother hand, modern methods take a statistical ap-proach to calculate the posterior distribution overthe dialog states using multiple results from SLUin order to overcome the error in the most likelySLU result.Statistical dialog state trackers can be catego-rized into two approaches depending on how theposterior calculation is modeled.
The generativeapproach uses the generative model that describeshow the SLU results are generated from the hiddendialog state and uses the Bayes?
rule to calculatethe posterior.
It has been a popular approach forstatistical dialog state tracking, since it naturallyfits into the Partially Observable Markov DecisionProcess (POMDP) (Williams and Young, 2007),an integrated model for dialog state tracking anddialog strategy optimization.
In the POMDP pointof view, the dialog state tracking is essentially be-lief monitoring, which is the task of calculatingposterior distribution over the hidden state giventhe history of observations.
Examples of the dia-log state trackers that take the generative approachinclude (Young et al., 2010; Thomson and Young,2010; Raux and Ma, 2011)On the other hand, the discriminative approachdirectly models the posterior distribution.
Sinceit avoids modeling of unnecessary aspects of thetask, it typically achieves a better tracking accu-racy compared to the generative approach.
Ex-amples of discriminative dialog state trackers in-clude (Lee, 2013; Metallinou et al., 2013).
How-ever, their feature functions often refer to past ob-servations, and it remains yet to be seen whether273the discriminative approach can be successfullyincorporated into POMDP or reinforcement learn-ing (RL) for dialog strategy optimization.This paper is concerned with the generative ap-proach to dialog state tracking.
In our earlierwork (Kim et al., 2013), the optimization of thetracker was carried out independently for eachcomponent model (observation model, user actionmodel, and belief refinement model) that com-prised our tracker.
This was not exactly a properway to train the tracker for overall performanceoptimization.
In this paper, we present an opti-mization method, which we call ?cascading gra-dient descent?, that trains component models si-multaneously.
We show that this approach yieldsa dialog state tracker that performs on par with thebest ones that participated in the second DialogState Tracking Challenge (DSTC2).The rest of the paper is organized as follows:We briefly review the background of our work insection 2, and present our method in section 3.
Wethen explain the DSTC2 dialog domain and the ex-perimental settings in section 4, and discuss the re-sults in section 5.
Finally, we conclude the paperwith the summary and the suggestion for futurework in section 6.2 Background and Related WorkThe dialog state tracking is formalized as fol-lows: In each turn of the dialog, the spoken dia-log system executes system action a, and the userwith goal g responds to the system with utteranceu.
The dialog state in each turn is defined s =(u, g, h), where h is the dialog history encapsulat-ing additional information needed for tracking thedialog state (Williams et al., 2005).
The SLU pro-cesses the user utterance and generates the resultsas an N -best list o = [?u?1, f1?, .
.
.
, ?u?N, fN?
],where u?iis the hypothesized user utterance andfiis its confidence score1.
Without loss of gener-ality, we assume that the last item in the N -bestlist is the null item ?
?, 1?
?N?1i=1fi?, representingthe set of unrecognized user utterances.
The statis-tical dialog state tracker maintains the probabilitydistribution over states, called the belief.2.1 Discriminative Dialog State TrackingDialog state trackers taking the discriminative ap-proach calculates the belief via trained conditional1Here we assume that?N?1i=1fi?
1, acting as a posteriorof N -best list.models that represent the belief directly.
Maxi-mum Entropy is widely used for the discriminativeapproach, which formulates the belief as follows:b?
(g) = P (g|x) = ?
?
exp(wT?
(x)) (1)where ?
is the normalizing constant, x =(u1, a1, g1, .
.
.
, ut, at, gt) is the history of user ac-tions, system actions, and user goals up to the cur-rent dialog turn t, ?(?)
is the vector of featurefunctions on x, and finally, w is the set of modelparameters to be learned from dialog data.According to the formulation, the posteriorcomputation has to be carried out for all possibleuser goals in order to obtain the normalizing con-stant ?.
This is not feasible for real dialog domainsthat have a large number of user goals (the DSTC2dialog domain used on this paper has 371070 usergoals).Consequently, it is important for the discrimina-tive approach to reduce the size of the state space.
(Metallinou et al., 2013) adopts the idea behind theHIS model and confines the set of possible goalsto those appeared in SLU results.
(Lee, 2013) as-sumed conditional independence between dialogstate components to address scalability, and usedconditional random field.2.2 Generative Dialog State TrackingIn contrast, the generative approach to the dialogstate tracking calculates the belief using Bayes?rule, with the belief from the last turn b as aprior and the likelihood given the user utter-ance hypotheses Pr(o|a, g, h).
In the prior work(Williams et al., 2005), the likelihood is factoredand some independence assumptions are made:b?
(g?, h?)
= ?
?uPr(o|u) Pr(u|g?, a) ?
?hPr(h?|g?, u, h, a)?gPr(g?|g, a)b(g, h)(2)where ?
is the normalizing constant and u ismarginalized out in the belief.Scalability became the important issue, just asin the generative approach.
One way to reducethe amount of computation is to group the statesinto partitions, proposed as the Hidden Informa-tion State (HIS) model (Young et al., 2010).
Be-ginning with one root partition with the probabil-ity of 1, partitions are split when the distinctionis required by observations, i.e.
a user utterance274hypothesis from SLU.
This confines the possiblegoal state to the values that have been appeared asSLU hypotheses, and provides scalability withouta loss of accuracy when the coverage of N-best listis large enough to include the true utterance.
Us-ing the HIS model with an additional assumptionthat the user goal does not change (goal transitionfrom one to another is 0), the belief update equa-tion (2) is reformulated as follows:b?(?
?, h?)
= ?
?uPr(o|u) Pr(u|?
?, a) ??hPr(h?|?
?, u, h, a) Pr(??|?
)b(?, h)(3)where ?
is a set of user goals that share the samebelief.
Each probability model in the above equa-tion has a name: Pr(o|u) is called the observationmodel, Pr(u|?
?, a) is called the user action model,Pr(??|?)
is called the belief refinement model, andPr(h?|?
?, u, h, a) is called the history model.In this paper, we used the last turn?s belief of di-alog states as history state and preserved its depen-dence in the observation model to improve perfor-mance.
With the changes, observation model candistinguish user actions without their value.
Forexample, request alternative user action may havethe power of diminishing dominant partitions, andit can only be learnt by the dependence with par-tition confidence.
The belief update formula usedin this paper becomes:b?(??)
= ?
?uPr(o|u, a, h) ?Pr(u|?
?, a) Pr(??|?)b(?
)(4)Other approaches to cope with the scalabil-ity problem in dialog state tracking is to adoptfactorized dynamic Bayesian network by makingconditional independence assumptions among di-alog state components, and use approximate in-ference algorithms such as loopy belief propa-gation (Thomson and Young, 2010) or blockedGibbs sampling (Raux and Ma, 2011).3 Cascading Gradient DescentAlthough equation (4) is an elegant formulationof the dialog state tracking via Bayes rule, therehas not been an integrated learning algorithm thatsimultaneously optimizes component probabilitymodels, i.e.
the observation, the user action, andthe belief refinement models.
Our prior work (Kimet al., 2013) relied on independently training eachcomponent probability model, and then simplyplugging them into (4).
Since the independent op-timization of component probability models doesnot lend itself to the optimization of overall dia-log state tracking performance, we added an ex-tra post-processing step called ?belief transforma-tion?
in order to fine tune the results obtainedfrom equation (4).
Unfortunately, this effort gen-erally resulted in overfitting to the training data.In this paper, we present an integrated learning al-gorithm that simultaneously optimizes the compo-nent probability models of the HIS model.We start with defining an objective functionwhich measures the error of the dialog state track-ing:E =T?t=1?i12(b(?ti)?
rti)2(5)where t is the dialog turn, i is the partition index,rtiis the binary label with value 1 if and only ifthe partition ?ticontains the true user goal.
Notethat our objective function coincides with the `2performance metrics used in DSTC.We then express component probability modelsas functions of features, which are parameterizedby sets of weights, and rewrite equation (4):b(?ti) =?t?
(ut,ft)?otPrwO(ut, ft, at, b(?t?1i)) ?PrwU(ut|?ti, at) PrwR(?ti|?t?1i)b(?t?1i)(6)where wO, wU, and wRrepresent the set of pa-rameters for the observation, the user action, andthe belief refinement models, respectively.Our learning method is basically a gradient de-scent.
The gradient of E with respect to wOisderived as follows:?E?wO=T?t=1?i(b(?ti)?
rti)?b(?ti)?wOBy convenience, we define:?ti=?
(ut,ft)?otPrwO(ut, ft, at, b(?t?1i)) ?PrwU(ut|?ti, at) PrwR(?ti|?t?1i)b(?t?1i)=?
(ut,ft)?otptOptUptRb(?t?1i)?t= (?i?ti)?1, b(?ti) = ?t?ti275and then obtain:?b(?ti)?wO=??ti?wO?
?t+??t?wO?
?ti=??ti?wO?
?t?
b(?ti)?i???ti??wO?
?t,where??ti?wO=(b(?t?1i)?(ut,ft)?ot?ptO?wOptUptR+?b(?t?1i)?wO?
(ut,ft)?otptOptUptR).Gradients for the parameters of other com-ponent probability models are derived similarly.We call our algorithm cascading gradient descentsince the gradient?b(?ti)?w requires computation ofthe gradient in the previous dialog turn?b(?t?1i)?w ,hence reflecting the temporal impact of the param-eter change in throughout the dialog turns.Once we obtain the gradients, we update the pa-rameters using the gradient descentw?O= wO?
?
[?E?wO],w?U= wU?
?
[?E?wU],w?R= wR?
?
[?E?wR]where ?
is the stepsize parameter.
?
is initially setto 0.5 and decreased by multiplying110wheneverthe overall cost function increases.4 Dialog State Tracking in theRestaurant Information DomainThis section describes the dialog domain used forthe evaluation of our dialog tracker and the com-ponent probability models used for the domain.An instruction on how to obtain the dataset anda more detailed description on the dialog domaincan be found in the DSTC2 summary paper (Hen-derson et al., 2014).4.1 Task DescriptionWe used the DSTC2 dialog domain in whichthe user queries the database of local restaurants.The dataset for the restaurant information domainwere originally collected using Amazon Mechani-cal Turk.
A usual dialog proceeds as follows: theuser specifies the constraints (e.g.
type of food,location, etc) or the name of restaurant he wants,and the system offers the name of a restaurant thatqualifies the constraints.
User then accepts the of-fer, and requests for additional information aboutaccepted restaurant.
The dialog ends when all theinformation requested by the user is provided.The dialog state tracker should thereby clar-ify three types of information inside the state:goal, method, and requested.
The goal state iscomposed of name, pricerange, area, and foodslots, which is the information of the constraintsthat the user has.
The method state representswhat method user is using to accomplish his goal,whose value is one of the none, by constraints, byalternatives, by name, or finished.
Lastly, the re-quested state represents the information currentlyrequested by the user, such as the address, phonenumber, postal code, etc.
In this paper, we restrictourselves to tracking the goal states only, but ourtracker can be easily extended to track others aswell.The dialog state tracker updates the belief turnby turn, receiving SLU N-best hypotheses eachwith an SLU confidence score in every turn.
De-spite the large number of states a dialog can have,in the most cases, the coverage of N-best hypothe-ses is enough to limit the consideration of possiblegoal state to values that has been observed in SLUhypotheses.
Consequently, the task of the dialogstate tracker is to generate a set of observed val-ues and their confidence scores for each slot, withthe confidence score corresponding to the poste-rior probability of the goal state being the true goalstate.
The dialog state tracker also maintains a spe-cial goal state, called None, which represents thatthe true goal state has not been observed.
Its poste-rior probability is also computed together with theobserved goal states as a part of the belief update.For the rest of this section, we describe the modelschosen for each component probabilities.4.2 Observation ModelThe observation model that describes the genera-tion of SLU result for the user utterance is definedasPr(o = ?ut, ft?|u, a, h) =?oPrwO(ut, ft, at, b(?t?1i))= ?o11 + exp(?wTO?O(ut, ft, at, b(?t?1i))?
bO)276user action feature : 34 ?
system action feature : 5 ?
type of feature : 3 = 510Inform action : 12[food, pricerange, name, area] offer or?
inform Bias tern[not match, slot match, value match] (always 1)% consistency check with system action canthelp orcanthelp.exceptionAction with values : 8[confirm, deny] expl-conf or Value of user confidence?
?
impl-conf or ?
ft[food, pricerange, name, area] requestAction without values : 14 select[ack, affirm, bye, hello, negate, Value of last turn?s confidencerepeat, reqmore, reqalts, thankyou, confirm-domain or b(?t?1i)request, null, confirm, deny, inform] welcomemsgTable 1: 510 features used in observation model are specified.where ?O(ut, ft, at, b(?t?1i)) is the vector of fea-tures taken from the hypothesized user action ut,its confidence score ftgenerated from SLU, sys-tem action at, and the belief of partition we aredealing with b(?t?1i) from history state.
Normal-ization constante ?ocan be ignored since it is sub-sumed by overall normalization constant ?.
Fea-ture details are specified in table 1.4.3 User Action ModelSimilar to the observation model, the user actionmodel that predicts the user action given the pre-vious system action and user goal is defined asPr(ut|?ti, at) = PrwU(ut|?ti, at)=exp(wTU?U(ut, ?ti, at))?uexp(wTU?U(u, ?ti, at))where ?U(ut, ?ti, at) ?
{0, 1}322is the vector offeatures taken from the (hypothesized) user actionut, system action at, and the partition being up-dated ?ti.
Softmax function is used to normal-ize over possible user actions.
Feature details arespecified in table 2.4.4 Belief Refinement ModelThe belief refinement model predicts how the par-tition of the user goal will evolve at the next dialogturn.
We defined it as a mixture of the empiricaldistribution and the uniform distribution obtainedfrom the training data:PrwR(?ti|?t?1i)=11 + exp(?wR)occurrence(?ti, ?t?1i)occurrence(?t?1i)+(1?11 + exp(?wR))|?ti||?t?1i|where occurrence(?ti, ?t?1i) is the number ofconsecutive dialog turns in the training data withuser goals being consistent with ?t?1iin theprevious turn and ?tiin the current turn, andoccurrence(?t?1i) is defined similarly for a singleturn only.
The ratio of the two, which correspondsto the partition split probability used in (Young etal., 2010), is the first term in the mixture.
On theother hand, if we use this empirical distributiononly, we cannot deal with novel user goals that donot appear in the training data.
Assuming that usergoals are generated from the uniform distribution,the probability that the user goal is in a partition?
is|?|Nwhere |?| is the number of user goals inthe partition ?, and N is the total number of usergoals.
The probability that ?tigets split from ?t?1iis then|?ti||?t?1i|.
Hence, we mix the two probabilitiesfor the resulting model.The mixture weight is the only parameter of thebelief refinement model, which is learned as a partof the cascading gradient descent.
Note that weuse the sigmoid function in order to make the op-timization unconstrained.277user action feature : 35 ?
system action feature : 8 + remaining actions: 42 = 322Inform action : 24 Confirm/deny action : 16[food, pricerange, name, area] [confirm, deny]?
[offer or inform, ?
[not match, slot match, value match] canthelp or [food, pricerange, name, area]% consistency check canthelp.exception, ?with system action expl-conf or [not match, match]?
impl-conf or % consistency check[not match, match] request, with partition% consistency check ?
select] +with partition ?
Remaining system action : 26[not match, match] [confirm-domain or welcomemsg]Action without values : 11 % consistency check ?
[ack, affirm, bye, hello, negate, with partition [24 inform actions, null, others]repeat, reqalts, reqmore, % corresponding user actionsthankyou, request, null]Table 2: 322 features used in user action model are specified.5 Experimental Details5.1 DatasetsThe restaurant information domain used inDSTC2 is arranged into three datasets: train, dev,test.
The first two datasets are labeled with the trueuser goals and user actions to optimize the dialogstate tracker before submission.
The half of the di-alogs are created with artificially degraded speechrecognizers, intended to better distinguish the per-formances of trackers.
Details of each dataset areas below:?
dstc2 train: Composed of 1612 dialogs of11405 turns, produced from two differentdialog managers with a hand-crafted dialogpolicy.?
dstc2 dev: Composed of 506 dialogs of3836 turns, produced from the dialog man-agers used in dstc2 train set.
Most of dialogstate trackers show lower performance on thisdataset than others.?
dstc2 test: Composed of 1117 dialogs of9689 turns, produced from the dialog policytrained by reinforcement learning, which isnot used for the train and dev datasets.We used both train and dev sets as the trainingdata, as if they were one big dataset.
Althoughthe true labels for the test dataset were made pub-lic after the challenge, we did not use these labelsin any way for optimizing our tracker.5.2 Pre-trainingOne of the drawbacks in using gradient descentis convergence to a local optimum.
We also ob-served this phenomena during the training of ourdialog state tracker via cascading gradient descent.Randomized initialization of parameters is a com-mon practice for gradient descent, but given thehigh-dimensionality of the parameter space, therandomized initialization had a limited effect inconverging to a sufficiently good local optimum.We adopted a pre-training phase where the pa-rameters of each component model are optimizedindividually.
Once the pre-training is done foreach component model, we gathered the param-eter values and took them as the initial parametervalue for the cascading gradient descent.
This pre-training phase helped tremendously converging toa good local optimum, and reduced the number ofiterations as well.
We pre-trained the parametersof each component model as follows:?
Observation Model: True user action labelsin the training set are used as targets for theobservation model.
For every user action hy-pothesis in theN -best list, set the target valueto 1 if the user action hypothesis is the trueuser action, and 0 otherwise.
A simple gradi-ent descent was used for pre-training.?
User Action Model: Although the user ac-tion and the system action labels are avail-able, the partition of the user goals is notreadily available.
However, the latter can beeasily obtained by running an unoptimizedtracker.
Thus, using the labels in the train-ing set and the generated partitions, we setthe target value to 1 if the user action hypoth-esis is the true user action and the partition isconsistent with the true user action, and 0 oth-278(a) Evaluation on accuracy metric (higher is better)(b) Evaluation on L2 metric (lower is better) (c) Evaluation on ROCV 2,ca05metric (higher is better)Figure 1: The overall results of proposed method.
Each figure shows the evaluations over dstc2 testdataset by featured metrics (joint accuracy, joint l2, joint roc.v2) in DSTC2.erwise.
A simple gradient descent was alsoused for pre-training.?
Belief Refinement Model: Since there isonly a single parameter for this model, we didnot perform pre-training.5.3 Results and DiscussionTable 3 shows the test set score of tracker im-plemented based on proposed algorithm, with thescore of other trackers submitted to DSTC2.
Wetried 200 random initialised weights to train modelwith proposed algorithm, and learned model withthe lowest training L2 error is picked to showthe result on the test set.
Because we only usedlive SLU and past data to track dialog state, othertracker results with the same condition are selectedto compare with our tracker.The implementation of our algorithm was notready until the DSTC2 deadline.
We participatedas Team 8 using the old optimization methodin (Kim et al., 2013).
As shown in the table 3,the new algorithm shows a substantial improve-ment, achieving almost 15% decrease in the L2error.
Since both trackers are fine-tuned, this im-provement seems to be originated from the newoptimization algorithm.For all three featured metrics used to evalu-ate, tracker constructed with our proposed methodshows competitive performance.
The key to excelbaseline tracker was to discover the relation be-tween user action and system action.
For exam-ple, user actions that tell about the same slot sys-tem was talking about but giving different valueare usually correcting wrong recognitions so far,which should significantly reduce the belief overstate the system was tracking.Due to the objective function that is designed tooptimize L2 error, our tracker shows better perfor-mance at L2 error than the other metrics.
For bothall goal metric and joint goal metric, our trackershows low L2 error when compared to other track-ers while the rank of accuracy metric is not sohigh.
When the fact that our method as a genera-tive state tracker benefits from the ability to be eas-ily incorporated into POMDP framework is con-sidered, only similar performance to other trackersis satisfactory.6 ConclusionIn this paper, we propose a simple method thatoptimizes overall parameters of generative statetracker using ?Cascading Gradient Descent?
al-279All goalTeam 0 1 3 4 6 7 8 9 OursEntry 1 2 0 0 3 0 1 2 4 1 0Accuracy 0.886 0.88 0.837 0.892 0.895 0.884 0.882 0.885 0.894 0.873 0.77 0.886AvgP 0.865 0.852 0.778 0.856 0.853 0.789 0.833 0.843 0.862 0.827 0.782 0.846L2 0.192 0.198 0.289 0.189 0.17 0.197 0.189 0.184 0.179 0.227 0.358 0.186MRR 0.918 0.914 0.87 0.911 0.927 0.917 0.916 0.918 0.922 0.904 0.833 0.918ROCV 1,ca050.777 0.767 0.0 0.778 0.842 0.773 0.786 0.809 0.806 0.635 0.0 0.805ROCV 1,eer0.139 0.133 0.0 0.119 0.103 0.135 0.123 0.116 0.116 0.163 0.219 0.120ROCV 2,ca050.0 0.0 0.0 0.0 0.3 0.27 0.417 0.384 0.154 0.0 0.0 0.197UpdateAcc 0.886 0.881 0.837 0.891 0.895 0.882 0.88 0.883 0.894 0.873 0.769 0.886UpdatePrec 0.898 0.897 0.846 0.904 0.907 0.898 0.895 0.897 0.903 0.886 0.804 0.896Table 3: Test set scores averaged over all goal slots of our proposed algorithm and other trackers arepresented.
The goal slots are composed of food, pricerange, name and area.Joint goalTeam 0 1 3 4 6 7 8 9 OursEntry 1 2 0 0 3 0 1 2 4 1 0Accuracy 0.719 0.711 0.601 0.729 0.737 0.713 0.707 0.718 0.735 0.699 0.499 0.726AvgP 0.678 0.66 0.503 0.659 0.636 0.54 0.619 0.638 0.673 0.583 0.522 0.658L2 0.464 0.466 0.649 0.452 0.406 0.461 0.447 0.437 0.433 0.498 0.76 0.427MRR 0.779 0.757 0.661 0.763 0.804 0.767 0.765 0.772 0.787 0.749 0.608 0.775ROCV 1,ca050.332 0.316 0.096 0.32 0.461 0.324 0.395 0.432 0.349 0.22 0.0 0.438ROCV 1,eer0.256 0.254 0.382 0.249 0.208 0.281 0.241 0.226 0.243 0.299 0.313 0.218ROCV 2,ca050.0 0.0 0.064 0.0 0.321 0.1 0.223 0.207 0.086 0.067 0.0 0.135UpdateAcc 0.489 0.487 0.37 0.495 0.507 0.473 0.466 0.476 0.514 0.459 0.325 0.488UpdatePrec 0.729 0.694 0.677 0.759 0.726 0.748 0.743 0.743 0.703 0.692 0.54 0.71Table 4: Test set scores of joint goal slot of our proposed algorithm and other trackers are presented.
Thejoint goal slot is a slot that is treated as correct when every goal slot is correct.gorithm.
Using proposed method on Hidden In-formation State model, we construct a trackerthat performs competitively with DSTC2 par-ticipants, who mostly adopt discriminative ap-proaches.
Since generative approach has muchmore potential to be extended to more com-plex models or toward different domains such asDSTC3, our tracker has the advantage over theother trackers.Hidden Information State (HIS) model withcascading gradient descent has far more steps ofimprovement remaining.
Although history statein current paper only includes previous partitionbelief due to implementation convenience, utiliz-ing additional history state is the key to improveperformance even more.
History state can in-clude any information depending on how we de-fine the state.
The reason why the discriminativestate tracking methods generally show good per-formance in terms of accuracy is rich set of poten-tially informative features, which can be employedby the history state.In addition to the future improvements with his-tory state, we can consider improving each prob-ability models.
In this paper, probability mod-els are modeled with sigmoid function or soft-max function over weighted features, which is inother words a neural network with no hidden layer.The model used in this paper can naturally devel-oped by adding hidden layers, and ultimately deeplearning techniques could be applicable.
Apply-ing deep learning techniques could help the his-tory state to find out influential hidden features toemploy.AcknowledgmentsThis work was supported by the IT R&D programof MKE/KEIT.
[10041678, The Original Technol-ogy Development of Interactive Intelligent Per-sonal Assistant Software for the Information Ser-vice on multiple domains]ReferencesMatthew Henderson, Blaise Thomson, and JasonWilliams.
2014.
The second dialog state tracking280challenge.
In Proceedings of the SIGdial 2014 Con-ference, Baltimore, U.S.A., June.Daejoong Kim, Jaedeug Choi, Kee-Eung Kim, JungsuLee, and Jinho Sohn.
2013.
A specific analysis ofa dialog state tracker in a challenge.
In Proceedingsof the SIGDIAL 2013 Conference, pages 462?466.Sungjin Lee.
2013.
Structured discriminative modelfor dialog state tracking.
Proceedings of the SIG-DIAL 2013 Conference, pages 442?451.Angeliki Metallinou, Dan Bohus, and Jason DWilliams.
2013.
Discriminative state tracking forspoken dialog systems.
In Proceedings of the 51stAnnual Meeting of the Association for Computa-tional Linguastics, pages 466?475.Antoine Raux and Yi Ma.
2011.
Efficient probabilistictracking of user goal and dialog history for spokendialog systems.
In INTERSPEECH, pages 801?804.Blaise Thomson and Steve Young.
2010.
Bayesianupdate of dialogue state: A pomdp framework forspoken dialogue systems.
Computer Speech & Lan-guage, 24(4):562?588.Jason D Williams and Steve Young.
2007.
Partiallyobservable markov decision processes for spokendialog systems.
Computer Speech & Language,21(2):393?422.Jason D Williams, Pascal Poupart, and Steve Young.2005.
Factored partially observable markov deci-sion processes for dialogue management.
In 4thWorkshop on Knowledge and Reasoning in Practi-cal Dialog Systems, International Joint Conferenceon Artificial Intelligence (IJCAI), pages 76?82.Steve Young, Milica Ga?si?c, Simon Keizer, Franc?oisMairesse, Jost Schatzmann, Blaise Thomson, andKai Yu.
2010.
The hidden information state model:A practical framework for pomdp-based spoken dia-logue management.
Computer Speech & Language,24(2):150?174.281
