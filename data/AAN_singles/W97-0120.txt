A Self-Organlzing Japanese Word Segmenter usingHe-ristic Word Identification and Re-estimationMasaaki NAGATANTT  Information and Communication Systems Laboratories1-I Hilca.rinooka Yokosl~Jc~-Shi Kanagawa, 239 Japannagat a@nttnly, isl.
ntt.
co. jpAbstractWe present a self-organized method to build a stochastic Japanese word segmenter from asmall number of basic words and a large amount of unsegmented training text.
It consists ofa word-based statistical language model, an initial estimation procedure, and a re-estimationprocedure.
Initial word frequencies are estimated by counting all possible longest match stringsbetween the training text and the word list.
The initial word list is au~nented by identifyingwords in the training text using a heuristic rule based on character type.
The word-basedlanguage model is then re-estimated to filter out inappropriate word hypotheses generated bythe initial word identification.
When the word segmeuter is trained on 3.9M character texts and1719 initial words, its word segmentation accuracy is86.3% recall and 82.5% precision.
We findthat the combination of heuristic word identi~cation a d re-estimation is so effective that theinitial word list need not be large.1 In t roduct ionWord segmentation is an important problem for Japanese because word boundaries are not markedin its writing system.
Other Asian languages such as Chinese and Thai have the same problem.Any Japanese NLP application requ/res word segmentation as the first stage because there arephonological nd semantic units whose pronunciation and meaning is not trivially derivable fromthat of the individual characters.
Once word segmentation is done, all established techniques canbe exploited to build practically important applications such as spelling correction \[Nagata, 1996\]and text retrieval \[Nie and Brisebois, 1996\]In a sense, Japanese word segmentation is a solved problem if (and only if) we have plentyof segmented training text.
Around 95% word segmentation accuracy is reported by using aword-based language model and the Viterbi-like dynamic programi-g procedure \[Nagata, 1994,Takeuchi and Matsumoto, 1995, Yamamoto, 1996\].
However, manually segmented corpora are notalways available in a particular target domain and manual segmentation is very expensive.The goal of our research is unsupervised learning of Japanese word segmentation.
That is, tobuild a Japanese word segmenter from a list of initial words and unsegmented training text.
Today,it is easy to obtain a 10K-100K word list from either commercial or public domain on-line Japanesedictionaries.
Gigabytes of Japanese text are readily available from newspapers, patents, HTMLdocuments, etc..Few works have examined unsupervised word segmentation in Japanese.
Both \[Yamamoto, 1996\]and \[Takeuchi and Matsumoto, 1995\] built a word-based language model from unsegmented text203using a re-estimation procedure whose initial segmentation was obtained by a rule-based word seg-reenter.
The utility of this approach is limited because it presupposes the existence of a rule-basedword segmenter like JUMAN \[Matsumoto et al, 1994\].
It is impossible to build a word segmenterfor a new domain without human intervention.For Chinese word segmentation, more self-organized approaches have been tried.
\[Sproat et al, 1996\]built a word unigram model using the Viterbi re-estimation whose initial estimates were derivedfrom the frequencies in the corpus of the strings of each word in the lexicon.
\[Chang et al, 1995\]combined a small seed segmented corpus and a large unsegmented corpus to build a word unigrammodel using the Viterbi re-estimation.
\[Luo and Roukos, 1996\] proposed a re-estimation procedurewhich alternates word segmentation a d word frequency re-estimation on each half of the trainingtext divided into halves.One of the major problems in unsupervised word segmentation is the treatment ofunseen words.\[Sproat et al, 1996\] wrote lexical rules for each productive morphological process, such as pluralnoun formation, Chinese personal names, and transliterations of foreign words.
\[Chang et al, 1995\]used a statistical method called "Two-Class Classifier", which decided whether the string is actuallya word based on the features derived from character N-gram.In this paper, we present a self-organized method to build a Japanese word segmenter f oma small number of basic words and a large amount of unsegmented training text using a novelre-estimation procedure.
The major contribution of this paper is its treatment of unseen words.We devised a statistical word formation model for unseen words which can be re-estimated.
Weshow that it is very effective to combine a heuristic initial word identification method with a re-estimation procedure to filter out inappropriate word hypotheses.
We also devised a new methodto estimate initial word frequencies.Figure 1 shows the configuration of our Japanese word segmenter.
In the following sections, weffirst describe the statistical language model and the word segmentation algorithm.
We then describethe initial word frequency estimation method and the initial word identification method.
Finally,we describe the experiment results of unsupervised word segmentation u der various conditions.m2 Language Model -rid Word Segmentation Algorithm2.1 Word  Segmentat ion  Mode lkLet the input Japanese character sequence be C = ClC2 .. .
cm.
Our goal is to segment it into Iword sequence W = wlw2..,  w,.
The word segmentation task can be defined as finding a wordsegmentation l~ r that maximizes the joint probability of word sequence given character sequence I l lP(W\[C).
Since the maximization is carried out with fixed character sequence C, the word segmenter ?only has to maximize the probability of the word sequence P(W).= arg P (w Ic )  = arg P(W) (1)We approximate he joint probability P(W) by the word unigram model, which is the product ofword unigram probabilities P(wl).
iPCW) = I I  (2)i= lWe used the word unlgram model because of its computational efficiency, l204 !iInital Word \[ IdentificationU~nse~ent e~d =1 Initial Word IFrequency I "\[ W~:edstimat io nFigure I: Block Diagram for the Self-Organizing Japanese Word Segmenter2.2 Unknown Word  Mode lWe defined a statistical word model to assign a reasonable word probability to an arbitrary substringin the input sentence.
It is formally defined as the joint probability of the character sequence c , .
.
.
ckif wi is an lmkaown word.
We decompose it into the product of word length probability and wordspelling probability,P(wil<ImX>) = P(cl ...  ckl<UNX>) = P(k)P(c l .
.
.
cklk) (3)where k is the length of the character sequence and <OlqK> represents unknown word.We assume that word length probability P(lc) obeys a Poisson distribution whose parameteris the average word length A in the training corpus.
This means that we regard word length asthe interval between hidden word boundary markers, which axe randomly placed with an averageinterval equal to the average word length.P(k) = - i)a-1 (k - 1)l 'e-('x-1) (4)We approx4mate he spelling probability given word length P(c l .
.
.
ca\[k) by the product ofcharacter unigram probabilities regardless of word length.aP(c  ... ca) = \ [ I  (s) i=1Character unigram probabilities can be estimated from unsegmented texts.
The average wordlength A can be computed, once the word frequencies in the texts are obtained.,X = E Iw~lC(w~)EC(w~) (6)205'iwhere Iw l and C(w ) are the length and the frequency of word ~i, respectively.
Therefore, theonly parameters we have to (re)estimate in the language model are the word frequencies.o0.70.60.50.40.30.20.1JWord Length Distribution!
I IRaw Counts (all won:Is) oEstimates by Poisson (all words) -+- .Raw Counts (infrequent words) -B--Estimates by Poisson (infrequent words) -x--,,,, '~.V %?
'%B.0 - -  - "O 2 4 6 8 10Word Character LengthFigure 2: Word Length Distribution of the ED~ corpusFigure 2 shows the actual and estimated word length distzibutious in the corpus we used inthe experiment.
It shows two pairs of distributions: word length of all words (~ = 1.6) and thatof words appearing only once (~ -- 4.8).
The latter is expected to be close to the distribution ofunknown words.
Although the estimates by Poisson distribution are not so accurate, they enablesus to make a robust and computationaUy efficient word model.2.3 V i te rb i  Re -es t lmat ionWe used the Viterbi-like dyn~m~c programing procedure described in \[Nagata, 1994\] to get themost likely word segmentation.
The generalized Viterbi algorithm starts from the beginning of theinput sentence, and proceeds character by character.
At each point in the sentence, R looks upthe combination ofthe best partial word segmentation hypothesis ending at the point and all wordhypotheses starting at the point.We used the Viterbi reoestimation procedure to refine the word unigram model because ofits computational efficiency.
It involves applying the above segmentation algorithm to a trainingcorpus, using a set of initial estimates of the word frequencies.
The best analysis of the corpus istaken to be the true analysis, the frequencies are re-estimated, and the algorithm is repeated untilit converges.2063 Initial Word Frequency Estimntion3.1 Longest  MatchWe can get a set of initial estimates of the word frequencies by segmenting the training corpus usinga heuristic (non-stochastic) dictionary-based word segmenter.
In both Japanese and Chinese, oneof the most popular non-stochastic dictionary-based approaches i  the longest match method 1There are many variations of the longest match method, possibly augmented with furtherheuristics.
We used a simple greedy algorithm described in \[Sproat et al, 1996\].
It starts at thebeg6nning of the sentence, finds the longest word starting at that point, and then repeats theprocess tarting at the next character until the end of the sentence is reached.
We chose the greedyalgorithm because it is easy to implement and guaranteed to produce only one segmentation.3.2 S t r ing  F requency\[Sproat et al, 1996\] also proposed another method to estimate a set of initial word frequencieswithout segmenting the corpus.
It derives the initial estimates from the frequencies in the corpus ofthe strings of character making up each word in the dictionary whether or not each string is actuallyan instance of the word in question.
The total number of words in the corpus is derived simply bysumming the string frequency of each word in the dictionary.
Finding (and counting) all instancesof a string W in a large text T can be efficiently accomplished by making a data structure knownas a sUtrLX array, which is basically a sorted list of all the su~ixes of T \[Manber and Myers, 1993\].3.3 Longest  Match  S t r ing  F requencyThe estimates of word frequencies by the above string frequency method tend to inflate a lotespecially in short words, because of double counts.
We devised a slightly improved version whichwe term the "longest match string frequency" method.
It counts the instances of string W1 in textT, unless the instance is also a substring of another string W~ in dictionary D.This method can be implemented by making two suffix arrays, Srr and SD for text T anddictionary D. By using ST, we first make list Lw of all occurrences of string W in the text.
Byusing SD, we then look up all strings IY?
in the dictionary that include W as a substring, and makelist ~ of all their occurrences in the text by using ST.
The longest match string frequency ofword W in text T with respect o dictionary D is obtained by counting the number of elements inthe set difference .LW --/'W-For example, if the input sentence is ~ ~  ~g~- - -~| r .~-~o " (talk about the Asso-ciation of English and the Association of Linguistics) and the dictionary has -r~_~ (linguistics), ~"(language), ~ (language study), ~ (association), and ~ (talk).
Figure 3 shows the differenceof the three methods.The longest match string frequency (lsf) method considers all possible longest matches in thetext, while the greedy longest match (lm) algorithm considers only one possibility.
It is obviousthat the longest match string frequency method remedies the problem that the string frequency(sf) method consistently and inappropriately favors short words.The problem of the longest match string frequency method is that if a word W1 is a substringof other word W2 and if W1 always appears as a substring of W2 in the training text, just like "~Although \[Sproat e al., 1996\] calls it '~maximum matching", we call this method "longest match" according toareview on Chinese word segmentation \[Wu and Tseng, 1993\] and the literal translation of the Japanese name of themethod t '~- -~.207longest match (lm)c, --ostring frequency (sf)longest match string frequency (isf)longest match='~ 1?
~~ oYS~ 1~ o1totalstring frequency11223 .~-lm string freq.101213 5Figure 3: Comparison of the initial word frequency estimation methodsand ~- -~-  in the above example, the frequency estimate of W1 becomes 0.
Although this rarelyhappens for a large training text, we have to smooth the word frequencies.4 In i t ia l  Word  Ident i f i ca t ion  MethodTo a first approximation, a point in the text where character type changes i likely to be a wordboundary.
This is a popular heuristics in Japanese word segmentation.
To help readers understandthe heuristics, we have to give a brief introduction to the Japanese writing system.In contemporary Japanese, there are at least five different types of characters other than punc-tuation maxks: kanji, hiragana, katakana, Roman alphabet, and Arabic numeral.
Kanfi whichmeans 'Chinese character' is used for both Chinese origin words and Japanese words semanticallyequivalent to Chinese characters.
There are two syllabaries hiragana nd katakana.
The formeris used primarily for grammatical function words, such as particles and inflectional endings, whilethe latter is used primarily to transliterate Western origin words.
Roman alphabet is also used forWestern origin words and acronyms.
Arabic numeral is used for numbers.By using just this character type heuristics, a non-stochastic and non-dictionary word segmentercan be made.
Ia fact, using the estimated word frequencies obtaiued by the heuristics results inpoor segmentation accuracy 2.
We found, however, that it is very effective to use the charactertype based word segmenter asa lexical acquisition tool to augment the initial word list.The initial word identification procedwe is as follows.
First, we segment the training corpusby the character type based word segmenter, and make a list of words with frequencies.
We thenfilter out hiragana strings because they are likely to be function words.
We add the extracted word~The word segmentation accuracy of the character type based method was less th~- 60%, while other estimationmethods achieves around 70-80% as we show ia the next section.208I1!I!i!
!IIIIIIiIil!list to the original dictionary witli associated frequencies, whether or not each string is actually aword.
Although there are a lot of erroneous words in the augmented word list, most of them arefiltered out by the re-estimation.
This method works suzprisingly well, as shown in the experiment.5 Experiment5.1 Language DataWe used the EDR Japanese Corpus Version 1.0 \[EDR, 1995\] to train and test the word segmenter.It is a corpus of 5.1 million words (208 thousand sentences).
It contains a variety of Japanesesentences taken from newspapers, magazines, dictionaries, encyclopedias, textbooks, etc.
It has avariety of annotations including word segmentation, pronunciation, and part of speech tag.In this experiment, we randomly selected two sets of training sentences, each consisting of100 thousand sentences.
The fixst tralniug set (training-0) is used to make initial word lists ofvarious sizes.
The second training set (training-I) is used to train various word segmenters.
Fromthe remaining of 8 thousand sentences, we randomly selected 100 test sentences to evaluate theaccuracy of the word segmenters.
Table 1 shows the number of sentences, words, and characters inthe training and test sets 3Table 1: The amount of tr~n~ng and test datatraining-0 trsJning-1Sentences 100000 100000Word Tokens 2460188 2465441Word Types 85966 85967Characters 3897718 3906260test10025389193984Based on the frequency in the manually segmented corpus training-0, we made 7 different initialword lists (D1-D200) whose frequency threshold were !, 2, 5, 10, 50, 100, 200, respectively.
Thesize of the resulting word lists and their out-of-vocabulary rate (OOV rate) in the test sentences areshown in the second and third colnmn~ of Table 2.
For example, D200 consists of words appearingmore than 200 times in training-0.
Although D200 consists of only 826 words, it covers 76.6%(OOV rate 23.4%) of the test sentences.
This is an example of the Zipf law.5.2 Eva luat ion  MeasuresWord Segmentation accuracy is expressed in terms of recall and precision as is done for bracketingof partial parses \[Nagata, 1994, Sproat et al, 1996\].
Let the number of words in the manuallysegmented corpus be Std, the number of words in the output of the word segmenter be Sys, andthe number of matched words be M. Recall is defined as M/Std, and precision is defined as M/Sys.Since it is inconvenient to use both recall and precision all the we also use the F-measureto indicate the overall performance.
The F-measure was originally developed by the informationSTraining-I was used as plain texts that are taken from the same information sou.rce as training-O.
Its wordsegmentation nformation was never used to ensure that tr~;=ing was unsupervised.209retrieval community.
It is calculated byF= f12 x P + R (7)where P is precision, R is recall, and fl is the relative importance given to recall over precision.
Weset fl = 1.0 throughout this experiment.
That is, we put equal importance on recall and precision.5.3 Compar i son  of  Var ious Word  F requency  Es t imat ion  MethodsWe first compared the three frequency estimation methods described in the previous ection: greedylongest match method (lm), string frequency method (sf), and longest match string frequencymethod (lsf).
The sixth, seventh, and eighth columns of Table 2 show the word segmentationaccuracy (F-measure) of each estimation method using different sets of initial words (D1-D200).For comparison, the word segmentation accuracy using real word frequency (wf), computed fromthe manual segmentation f training-1 (not training-0!
), is shown in the fifth column of Table 2.The results are also diagramed in Figure 4.Table 2: Word Segmentation Accuraciesfreq : vocabD1 >1 85966D2 >2 39994D5 >5 18689D10 >10 10941DG0 >50 3159D100 >100 1719D200 >200 826oov wf0.010 0.8930.017 0.8910.037 0.8770.060 0.8590.134 0.7850.181 0.7580.234 0.729hn0.8100.8170.8120.7970.7340.6990.644sf lsfi Im+ct sf+ct lsf+ct0.801 0.807 0.796 0.789 0.7940.815 0.822 0.808 0.802 0.8110.814 0.819 0.818 0.811 0.8160.813 ~0.815 0.828 0.825 0.8280.774 0.776 0.837 0.837 0.8410.749 0.761 0.839 0.840 0.8430.643 0.731 0.828 0.830 0.832First of all, word segmentation accuracy using real word frequencies (wf) significantly (5-10%)outperformed that of any frequency estimation methods.
Among word frequency estimates, thelongest match string frequency method (lsf) consistently outperformed the string frequency method(sf).
The (longest match) string frequency method (sf and lsf) outperformed the greedy longestmatch method (lm) by about 2-5% when the initial word list size was under 20K (from D5 toD200).
In all estimation methods, word segmentation accuracies of D1 are worse than D2, whileD1 is slightly better than D2 in using real word frequencies.5.4 Effect of Augmenting Initial DictionaryWe then compared the three frequency estimation methods (Ira, sf, and lsf) with the initial dic-tionary augmented by the character type based word identification method (ct) described in theprevious ection.
The word identification method collected a list of 108975 word hypotheses fromtrainingol.
The ninth, tenth, and eleventh columns of Table 2 show the word segmentation accuofacies.Augmenting the dictionary ields a significant improvement in word segmentation accuracy.Although the difference between the underlying word frequency estimation methods is small, thelongest match string frequency method generally performs best.
Surprisingly, the best word segmen-tation accuracy is achieved when the very small initial word list of 1719 words (D100) is augmentedIIIIiIIIIIIIIIIII210 !IIIIIIEg?0 120.90.850.80.750.7O,6SWord Segmentation Accuracy/ .
f  ~ ,~;~../ / / -  Longest match SMng Frequency -x--/ I " - -  Longest Match + Character Type/ .
.
f" String Frequency + Character.Type -~,---.." / Longest match ~'ing Prequency + Character i ype -<-- / //"S/O.S  i I , ?
, m , , , , I , .
.
.
.
.
.1000 10000 I(XXX)OVocaburaty ,SizeFigure 4: Initial word list size and word segmentation accuraciesby the heuristic word identification method, where the recall and precision are 86.3% aud 82.5%(F-measure 0.843).5.5 Effect o f  Re-es t imat ionTo investigate he effect of re-estimation, we tested the combination of three initial word lists: D1,D2, D100, and two initial word frequency estimation methods: string frequency method (sf) andlongest match string frequency method au~nented with the word identification method (lsf+ct).We applied the Viterbi re-estimation procedure three times.
It seems further re-estimationbrings no signi~cant change.
At each stage of re-estimation, we measured the word segmentationaccuracy on the test sentences (not the training texts!).
Figure 5 shows the word segmentationaccuracy, the number of word tol~ens in the training texts, and the number of word types in thedictionary at each stage of re-estimation.In general, re-estimation has little impact on word segmentation accuracy.
It gradually improvesthe accuracy when the initial word list is relatively large (D1 and D2), while it worsen the accuracya little when the initial word list is relatively small (D100).
This might correspond with the resultson unsupervised learning performed by an English part of speech tagger.
Although \[Kupiec, 1992\]presented a very sophisticated method of unsupervised learning, \[Elworthy, 1994\] reported thatre-estimation is not always helpful.
We think, however, our results are because we used a worduni~am model; it is too early to conclude that re-estimation is useless for word segmentation, asdiscussed in the next section.It seems the virtue of re-estimation lies in its ability to adjust word frequencies and removingunreliable word hypotheses that are added by heuristic word identification.
The abrupt drop in thenumber of word tokens at the ffirst re-estimation step indicates that the inflated inRial estimates of211Word Segmentation Accura0.85 = ,0.84:0.83~ 0.82~ 0.81= 0.8c o 0.79E 0.78~ 0.770.760.750.74sf-1 -e.---sf-2 -~---sf-100 -e- :Isf-l+Ct -,~--Isf-2+Ct -~---Isf-100+Ct -x---".
.
.
.
~ .
.
.
.
.
E~ .
.
.
.
.I I0 1 2 3Number of ReestimationINumber of Word Tokens Number of Word Types 15.5e+06 ~ 180000 - ,  ,I I I160000 ~ ' B5e+06 - \140ooo - \ |4.5e+06 sf-1 ~.
,sF2 -+- - .
~-~ "\I 120000 - ",., ~- - - - - -x~ sf-100 -e--.
~ .
_...~ .
.
.
.
.
isf-l+ct -x- -  ~ ~ ..... ~ ....  -x- .... .
^^ Isf-2+ct ~ I,....r- 4e+uo -X-': 100000 sf-1 o 3f-lOO,+ct "~ | ,~ "~ ~ Isf-l+ct -x--3.5e+06 ~ 80000 !
'~ Isf-2+ct -,6-.~0+ct -x---Ez z 600003e+06\L__ 2.5e+06 \[- ~ ...... ~ .
.
.
.
I ~.- _~,~-=~ 2e+060 1 2 3Number of Reestimation4000020000.ooo0 i t0 1 2 3Number of ReestimationFigure 5: Word segmentation accuracy, the number of word tokens and word types at each re-estimation stageword frequencies are adjusted to more reasonable values.
The drop in the number of word typesindicates the removal of infrequent words and unzeliable word hypotheses from the dictionary.6 Discussion6.1 The  Nature  of  the  Word  Un igram Mode lFizst, we will clarify the nature of the word unigram model.
ROughly speaking, word unigram basedword segmenters maximize the product of the word frequencies under the fewest word principlewhich subsumes the longest match pzinciple.If two word segmentation hypotheses divers in the number of words, the one with fewer wordsis almost always elected.
For example, the input string is clc2 and the dictionary includes threewords c1~, Cl, c2.
To prefer segmentation hypothesis c11c2 over czc2, the following relation musthold.
C(cic2) C(cz) C(c2)< N N (s)where C(-) represents the word frequency and N is the number of word tokens in the training text.I!IIIIIIII212 ISuppose N is one million.
Even if C(elc2) = 1~ clc2 is preferred unless cl and c2 are highly frequent,say C(e~) ~ C(c2) > 1000.
It is obvious that the segmentation with fewer words are preferred.If two word segmentation hypotheses have the same number of words, the one with largerproduct of word frequencies i  selected.
For example, the input string is c~c2cs and the dictionaryincludes four words e~c~, cs, e~, e2c3.
To prefer segmentation hypothesis e~c21cs over c~\[c2cs, thefollowing relation must hold.V(ClC2) C(C~) V(Cl) V(C2C3)- -  <: (9 )N N N NSince the denominator N is cancelled, it is obvious that the segmentation with larger product offrequencies i  preferzed.6.2 Class i f icat ion o f  Segmentat ion  Er rorsThere are three major types of segmentation errors.
The first type is not an error but the ambiguityresulting from inconsistent manual segmentation, or the intrinsic indeterminacy of Japanese wordsegmentation.
For example, in the manually segmented corpus, we found the string ~- \ [ \ ] .&~(foreign laborer) is identified as one word in some places while in others it is divided into two words~l-m)~ (foreigner) and ~ (laborer).
However, the word unigram based segmenter consistentlyidentifies it as a single word.
We assume 3-5 % of the segmentation "errors" belong to this type.The second type is breakdown of unknown words.
For example, the word ~#~ (funny) issegmented into two word hypotheses ~ (rare) and ~ (strange).
This is because ~'~ is included inthe dictionary.
When a substring of an unknown word coincides with other word in the dictionary,it is very likely to be broken down into the dictionary word and the remaining substring.
This is amajor flaw of our word model using character unigram.
It assigns too little probability to longerword hypotheses, especially more than thee characters.The third type is erroneous longest match.
This happens frequently at the sequence of gram-matical function words vrritten in hiragana.
For e~ample, the phrase ~$1~ (gather) I ?
(INFL) \[(and) \] ~ (come) I ~= (past-AUXV), which means ~came and gathered", is segmented into ~ I-~'C (TOPIC) \[ -~1c (north), because the number of words is fewer.
The larger the initial word listis, the more often a hiragana word happens to coincide with a sequence of other hiragana words,because the number of character types in hiragana is small (< 100).
This is the major reason whyword segmentation accuracy levels off or decreases at a certain point, as the size of the initial wordlist increases.6.3 Class i f icat ion o f  the  Effects of  Be -es t imat ionThere are two types of major changes in segmentation with re-estimation: word boundary adjust-ment and subdivision.
The former moves a word boundary keeping the number of words unchanged.The latter break down a word into two or more words.Re-estimation usually improves a sequence of grammatical function words written in hiraganaat the sentence final predicate phrase if the initial segmentation a d the correct segmentation havethe same number of words.
For example, the incorrect initial segmentation ~ (take away) I(INFL + passive-AUXV) I ~=~ (ball) I ~t~ (not yet) is correctly adjusted to ~i~'l,~ (take away)I ~,h, (INFL + passive-AUXV) I ft. (past-AUXV) I ~ (still) I fr~ (COPULA), which means `` stillbe taken away".Re-estimation subdivides an erroneous longest match if the frequencies of the shorter wordsare significantly large.
For example, the incorrect initial segmentation ~ (restrain) I fr.~ (sea213bream) is correctly subdivided into ~ (restrain) \[ tr.
(want-AUXV) \[ b~ (INFL), which means"want to restrain".One of the most frequent undesirable effects of re-estimation is subdividing an infrequent wordinto highly frequent words, or a frequent word and an unknown word.
For example, the correctinfrequent word ~ (ambassador) is subdivided into two frequent words, ~ (use-ROOT) and(node).As we said before, one of the major virtues of re-estimation is its ability to remove inappropriateword hypotheses generated by the initial word identification procedure.
For example, from thephrase Y~ (Soviet Union) I ~ (made-SUFFIX) I l~  (tank), which means "Soviet Union-madetank", the initial word identifier extracts two word hypotheses Y and ~ K ,  where the formeris written in katakana nd the latter is written in kanfi.
If ~ and ~ is in the dictionary, the twoerroneous word hypotheses >' and ~I~ iK  are removed and the correct word t~ is added to thedictionary after re-estimation.7 Conc lus ion  and  Future  WorkWe have presented a self-organized method that builds a stochastic Japanese word segmenter froma small word list and a large unsegmented text.
We found that it is very effective to augment theinitial word list with automatically extracted words using character type heuristics.
Re-estimationhelps in adjusting word frequencies and removing inappropriate word hypotheses, although it haslittle impact on word segmentation accuracy if the word unigram model is used.The major drawbacks of the current word segmenter is its breakdown of unknown words whosesubstrings coincide with other words in the dictionary, and the erroneous longest match at thesequence of functional words written in hiragana.
The first drawback results from the characterunigram based word model that prefers short words, while the second drawback results from thenature of the word tmigram model which prefers fewest words segmentation.One may argue that we could use the word bigzam model.
However, we don't know how wecan estimate the initial word bigram frequencies from scratch.
One may also argue that we coulduse the character bigram in the word model.
However, the character bigram for the word modelmust be computed from segmented texts.
Both of these suggest hat we need a word segmenterto build a more sophisticated word segmenter.
Therefore, as a next step of our research, we arethinking of using the proposed unigram based word segmenter toobtain the initial estimates of theword bigrams and the word-based character bigr~m~ which will then be refined by a re.estimationprocedure.References\[Chang et al, 1995\] Jing-Shin Chang, Yi-Chung Lin, and Keh-Yih Su.
1995.
Automatic Construc-tion of a Chinese Electronic Dictionary, In Proceedings of WVLC-95, pages 107-120.~DR, 1995\] Japan Electronic Dictionary Resee~rch Institute.
1995.
ED.R Electronic Dictionary Ver-sion I Technical Guide, EDR TR2-003.
Also available as The Structure of the EDR ElectronicDic~onary, ht tp : / / /~ .
11jnet.
or.
jp/edx/.\[Elworthy, 1994\] David Elworthy.
1994.
Does Baum-Welch Re-estimation Help Taggers?
In Pro-ceedings of ANLP-94, pages 53-58.214\[Kupiec, 1992\] Julian Kupiec.
1992.
Robust Part-of-Speech Tagging using a Hidden Maxkov Model.Computer Speech and Language, 6 pages 225-242.\[Luo and Roukos, 1996\] Xiaoqiang Luo and SAllm Roukos.
1996.
An Iterative Algorithm to BuildChinese Language Models, In Procecdings ofACL-96, pages 139-143.\[Matsumoto etal., 1994\] Yuji Matsumoto, S. Kurohashi, T. Utsuro, and Makoto Nagao.
1994.Japanese morphological analysis ystem JUMAN manual (in Japanese).\[Manber and Myers, 1993\] Udi Manber and Gene Myers.
1993.
Suffix Arrays: A New Method forOn-Line String Searches, SIAM J.
Comput., Vol.22, No.5, pp.935-948.\[Nagata, 1994\] Masaaki Nagata.
1994.
A Stochastic Japanese Morphological Analyzer Using aForward-DP Backward-A* N-Best Search Algorithm.
In Proceedings o\] COLING-9~, pages 201-207.\[Nagata, 1996\] Masaaki Nagata.
1996.
Context-Based Spelling Correction for Japanese OCR.
InProceedings ofCOLING-96, pages 806-811.\[Nie and Brisebois, 1996\] Jian-?un Nie and Martin Brisebois.
1996.
On Chinese Text Retrieval.
InProceedings ofSIGIR-96, pages 225-233.\[Sproat et al, 1996\] Richard Sproat, Chilin Shih, William Gale, and Nancy Chang.
1996.
AStochastic Finite-State Word-Segmentation Algorithm for Chinese.
Computational Linguistics,Vol.22, No.3, pages 377-404.\[Takeuchi and Matsumoto, 1995\] Kouichi Takeuchi and Yuji Matsumoto.
1995.
Learning parame-ters of Japanese morphological nalyzer based-on hidden Markov model.
IPSJ Technical ReportSIG-NL, 108-3, pages 13-19 (in Japanese).\[Wu and Tseng, 1993\] Zimln Wu and Gwyneth Tseng.
1993.
Chinese Text Segmentation for TextRetrieval: Achievements and Problems, Journal of A$IS, Vol.44, No.9, pages 532-544.\[Y~mamoto, 1996\] Mi~o Yamamoto.
1996.
A Re-estimation Method for Stochastic Language Mod-eling from Ambiguous Observations, in Proceedings of WVLU-96, pages 155-167.215
