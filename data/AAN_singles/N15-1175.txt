Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1516?1526,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsA Comparison of Update Strategies forLarge-Scale Maximum Expected BLEU TrainingJoern Wuebker, Sebastian Muehr, Patrick Lehnen, Stephan Peitz and Hermann NeyHuman Language Technology and Pattern Recognition GroupRWTH Aachen UniversityAachen, Germany{surname}@cs.rwth-aachen.deAbstractThis work presents a flexible and efficientdiscriminative training approach for statisti-cal machine translation.
We propose to usethe RPROP algorithm for optimizing a max-imum expected BLEU objective and experi-mentally compare it to several other updat-ing schemes.
It proves to be more effi-cient and effective than the previously pro-posed growth transformation technique andalso yields better results than stochastic gra-dient descent and AdaGrad.
We also reportstrong empirical results on two large scaletasks, namely BOLT Chinese?English andWMT German?English, where our final sys-tems outperform results reported by Setiawanand Zhou (2013) and on matrix.statmt.org.
Onthe WMT task, discriminative training is per-formed on the full training data of 4M sen-tence pairs, which is unsurpassed in the litera-ture.1 IntroductionThe main advantage of learning parameters in a dis-criminative fashion is the possibility to directly opti-mize towards a quality or error measure on the taskthat is being performed.
This stands in contrast tothe generative approach, where parameters are cho-sen to maximize likelihood under a generative story,which often bears little correspondence with the ac-tual application of the model.In statistical machine translation (SMT), ex-tending the generative noisy-channel formulation(Brown et al, 1993) as a discriminative, log-linearcombination of multiple models (Och, 2003) has be-come the state of the art.
However, most of thecomponent models are still estimated by heuristicsor generative training.
In this paper, a flexible, effi-cient and easy to implement discriminative trainingscheme for SMT is presented.
It can be applied toany kind and any number of features.
We use theRPROP algorithm to optimize a maximum expectedBLEU objective.
n-best lists approximate the infea-sibly large space of translation hypotheses.
They aregenerated with the application of leave-one-out tomake them more representative with respect to un-seen data.We make the following main contributions:1.
We propose to apply the RPROP algorithmfor maximum expected BLEU training and per-form an experimental comparison with growthtransformation (GT) (He and Deng, 2012;Setiawan and Zhou, 2013), stochastic gradi-ent descent (Auli et al, 2014) and AdaGrad(Green et al, 2013).
RPROP yields supe-rior performance, reaching a total improve-ment of 1.2 BLEU points over our IWSLTGerman?English baseline using 5.22M fea-tures.2.
In terms of time and memory efficiency,RPROP clearly outperforms GT.
The latterneeds to update a much larger number of fea-tures due to its renormalization component.
Onthe IWSLT data, RPROP is 6.4 times faster thanGT and requires a third of the memory.3.
On the WMT German?English task, we per-form discriminative training on 4M sentence1516pairs, which, to the best of our knowledge, is2.4 times the size of the largest training set re-ported in previous work (1.66M sentences in(Simianer et al, 2012)).
This proves the scala-bility of our approach.4.
On two large scale tasks our experimentsshow good improvements over strong base-lines which include recurrent language mod-eling components.
On the Chinese?EnglishDARPA BOLT task, we achieve nearly twicethe improvement reported in (Setiawan andZhou, 2013) on the same test sets which re-sults in a superior final system.
Finally, the bestsingle system reported on matrix.statmt.org isoutperformed by 0.8 BLEU points on the WMTGerman?English newstest2013 set.Our experiments also prove that leave-one-out im-pacts translation quality.This paper is organized as follows.
We review re-lated work in Section 2 and present the translationsystem in Section 3.
In Section 4 we describe thedifferent discriminative update strategies applied inthis work and Section 5 derives the complete max-imum expected BLEU training algorithm.
Finally,experimental results are given in Section 6 and weconclude with Section 7.2 Related WorkDiscriminative training is one of the most active re-search areas in SMT and it can be integrated into thepipeline at various stages.Och (2003) proposed to apply minimum error ratetraining (MERT) to optimize the different featureweights in the log-linear model combination on asmall development data set.
This is still consideredto be the state of the art, but is only capable of opti-mizing a handful of features.
More recently, MIRA(Watanabe et al, 2007; Chiang et al, 2008) and PRO(Hopkins and May, 2011) have been presented asoptimization procedures that can replace MERT andscale to thousands of parameters.In a different line of work, Liang et al (2006) de-scribe a fully discriminative training pipeline, wheremore than one million features are tuned on thetraining data using a perceptron-style update algo-rithm.
The Direct Translation Model 2 introducedby Ittycheriah and Roukos (2007) is similar in thatit also trains millions of features on the trainingdata.
However, the weights are estimated based on amaximum entropy model and the underlying trans-lation paradigm differs from the standard phrase-based model.
Gao and He (2013) use gradient as-cent to train Markov random field models for phrasetranslation.
These models are interpreted as undi-rected phrase compatibility scores rather than trans-lation probabilities.
Thus, as in our work, they arenot subject to a sum-to-one constraint.
Simianer etal.
(2012) propose a distributed setup for large-scalediscriminative training with joint feature selection.The training corpus is divided into several shards,on which features are updated via perceptron-stylegradient descent.
The authors present results show-ing that training on large data sets improves resultsover just using a small development corpus.
Anotherapproach based on the AdaGrad method that scalesto large numbers of sparse features is proposed in(Green et al, 2013; Green et al, 2014).
Differentfrom our work, the authors use either the tuning setsor a small subsample of the training data (15k sen-tences) for discriminative training.A notably different idea is pursued by Yu et al(2013), who present a large-scale training proce-dure that explicitly minimizes search errors.
This isachieved by force-decoding the training data and up-dating at the point where the correct derivation dropsoff the beam.In (Blunsom et al, 2008), conditional randomfields (CRFs) are trained within a hierarchicalphrase-based translation framework.
The hierar-chical phrase-based paradigm is used to model thesearch space in model estimation and search, leav-ing the hypothesis weighting to CRF features.
Theyconstrain search by a beam width for gradient es-timation and update the model with the help of L-BFGS.
In a similar way Lavergne et al (2011) usethe n-gram based approach (Casacuberta and Vidal,2004; Mari?no et al, 2006) to model the reordering,phrase alignment, and the language model.
A CRFis applied to estimate the phrase weights.
Model up-dates are carried out by the RPROP algorithm (Ried-miller and Braun, 1993).
However, both approachesonly improve over constrained baselines.Our work is inspired by (He and Deng, 2012; Seti-awan and Zhou, 2013), where the authors propose to1517train the standard phrasal and lexical channel mod-els with the growth transformation (GT) algorithm.They use n-best lists on the training data and op-timize a maximum expected BLEU objective, thatprovides a clear training criterion, which is missinge.g.
in MIRA estimation.
Auli et al (2014) reportgood results by applying the same objective func-tion to reordering features, which are trained withstochastic gradient descent (SGD).Our work differs in several key aspects: (i)We propose to apply the RPROP algorithm, whichyields superior results to GT, SGD and AdaGrad inour experimental comparison.
(ii) For the first time,we apply maximum expected BLEU training on adata set as large as four million sentence pairs.
(iii)We apply a leave-one-out heuristic (Wuebker et al,2010) to make better use of the training data.
(iv)We apply phrasal, lexical, reordering and triplet fea-tures.
(v) Finally, we do not run MERT after eachtraining iteration, which is expensive for large trans-lation systems.3 Statistical Translation SystemOur work can be applied to any statistical machinetranslation paradigm and we will present results ona standard phrase-based translation system (Koehnet al, 2003) and a hierarchical phrase-based trans-lation system (Chiang, 2005).
The translation pro-cess is implemented as a weighted log-linear com-bination of several models hm,?
(E,F ), where E =e1, .
.
.
, eIdenotes the translation hypothesis, F =f1, .
.
.
, fJthe source sentence, m a model index,and ?
the model parameters.
These models includethe phrase translation and lexical smoothing scoresin both directions, language model (LM) score, dis-tortion penalty, word penalty and phrase penalty(Och and Ney, 2004).
Given a source sentence F ,the models hm,?
(E,F ) and the corresponding log-linear feature weights ?m, the translation decodersearches for the best scoring translation?E:?E = arg maxE{f?
(E,F )} (1)f?
(E,F ) =?m?M?mhm,?
(E,F ) (2)where .
.
.
, ?m, .
.
.
are the model weighting param-eters.
In practice, the Viterbi approximation is ap-plied and for simplicity, in the following we will as-sume the particular derivation for a translation hy-pothesis to be included in the variable E. The log-linear feature weights are optimized with minimumerror rate training (MERT) (Och, 2003).4 Update Strategies4.1 Previously Proposed AlgorithmsThe Growth Transformation (GT) or ExtendedBaum-Welch Algorithm was proposed by He andDeng (2012) for maximum expected BLEU trainingof the standard phrasal and lexical channel models.It is an algorithm to iteratively optimize polynomialsof random variables that are subject to sum-to-onecontraints and is therefore suitable for training prob-ability distributions.
The disadvantage is that eachparameter update requires a renormalization step,which artificially blows up the number of featuresthat need to be changed and has a significant impacton time and memory efficiency.
The update formu-las are derived in (He and Deng, 2012).Stochastic Gradient Descent (SGD) is a well-known and frequently applied training scheme,which is used for maximum expected BLEU train-ing of reordering models by (Auli et al, 2014).
Itperforms the following update:?
(t+1)= ?
(t)+ ?
?
?(t)?
(3)Here, the disadvantage is its high sensitivity to thefixed learning rate ?.
However, as it does not subjectthe features to sum-to-one-contraints, it is consider-ably more time and memory efficient than GT.As an improvement to SGD, AdaGrad (Duchi etal., 2011) is designed for large, sparse feature setsand makes use of an adaptive learning rate.
It wasproposed for MT training by (Green et al, 2013).Although its main area of application are online al-gorithms, it is also applicable in our offline settingand is more robust than SGD due to the adaptivelearning rate.
Following (Green et al, 2013), we ap-ply the approximation with a diagonal outer productmatrix, which is computationally cheap.
This resultsin the update equations?
(t+1)= ?
(t)+ ?
?G?12t?
?(t)?
(4)Gt= Gt?1+ (?(t)?
)2(5)15184.2 RPROPThe resilient backpropagation algorithm (RPROP)proposed by Riedmiller and Braun (1993) is agradient-based optimization algorithm that emprir-ically learns the step size without taking the slopeinto account, making it highly robust and avoidingthe need for a learning rate.
If the gradient switchesalgebraic sign compared to the previous iteration,the last step is reverted and the step size reduced.
Ifthe sign remains the same, the step size is increased.Formally, given a set of parameters ?
and an ob-jective function O(?
), in iteration t each parameter?
?
?
is updated according to?(t+1)=????????????
(t?1), if ?(t?1)??
?
(t)?< 0?
(t)+ ??
(t), else if ?
(t)?> 0?(t)???
(t), else if ?
(t)?< 0?
(t), elsewhere?(t)?:=?O(?(t))?
?denotes the derivative of theobjective function.
The step size ??
(t)> 0 growsor decreases depending on the sign of the gradient:??(t)=??????+???
(t?1), if ?(t?1)??
?
(t)?> 0?????
(t?1), if ?(t?1)??
?
(t)?< 0??
(t?1), elseThe strength parameters 0 < ?
?< 1 ?
?+usuallyhave little impact and are fixed to ?
?= 0.5 and?+= 1.2 throughout this work.
The RPROP algo-rithm is simple and easy to implement.
It has proveneffective for a number of tasks, e.g.
in (Wiesler etal., 2013; Heigold et al, 2011; Lavergne et al, 2011;Hahn et al, 2011).
Different from growth transfor-mation (cf.
Sec.
4.1), it does not assume a proba-bility distribution and performs its updates withouta sum-to-one constraint.Compared to SGD and AdaGrad, RPROP?s prac-tical advantage is the absence of a learning rate thatneeds to be tuned.
Further, we see its theoreti-cal advantage in the empirically learned step size.In the first iterations, RPROP?s updates are con-siderably smaller than with the other strategies, re-sulting in a more careful exploration of the searchspace.
In higher iterations, the update steps forgood features keep growing and we observe an ex-ponential increase of the objective function.
In con-trast, GT, SGD, and AdaGrad determine the size oftheir update step based on the slope of the gradient,which we believe to be misleading given the com-plex topology of the feature space in MT.5 Training5.1 Maximum Expected BLEUFollowing (He and Deng, 2012), we want to opti-mize a maximum expected BLEU objective.
We de-note the universe of possible sentences in the sourcelanguage as F and in the target language as E. Theexpected BLEU score under parameter set ?
withrespect to the joint probability distribution p?
(?, ?)
isdefined as????=?F?F?E?Ep?
(E,F )?
(E) (6)Here, ?
(E) is the BLEU score for target sentence E(assuming the reference translation to be part of themapping ?)
and we use the notation ???
to denotethe expectation.
Enumerating all possible sourceand target sentences F , E is infeasible.
Therefore,we estimate the empirical expectation on a corpusC ?
E ?
F. We denote the source sentences in Cas CFand the size of the corpus as N = |C|.
Thejoint probability p?
(E,F ) is decomposed with thehelp of the Bayes Theorem, resulting in:???
?=?F?CFp(F )?E?E?
(F )p?
(E|F )?
(E) (7)For p(F ) =NFNwe assume the empirical distri-bution within the training corpus, where NFis thecount of sentence F .
The summation over allE ?
Eis sampled with a subset E?
(F ) of the most likelyhypotheses with respect to the parameterized proba-bility p?
(E,F ), which in practice is an n-best listgenerated by the decoder.
Iterating over the cor-pus C = {(E1, F1), .
.
.
, (En, Fn), .
.
.
, (EN, FN)}finally results in????=1NN?n=1?E?E?(Fn)p?(E|Fn)?
(E)We use the same unclipped sentence-level BLEU-4score with smoothed 3-gram and 4-gram precisionsas in (He and Deng, 2012), which we denote as?
(E) = BLEU(E,E?n) with respect to the referencetranslation E?n.1519The normalized posterior translation probabilityp?
(E|F ) from source sentence F to target sentenceE approximates a maximum entropy model normal-ized on sentence level:p?
(E|F ) =e?f?
(E,F )?E??E?
(F )e?f?
(E?,F )(8)The denominator of this probability does not dependon the output sentence.
Thus, the arg max of Equa-tion 8 is equal to the arg max of the translation scorein Equation 1.Maximum Entropy models tend to generalizepoorly, which can be circumvented by regulariza-tion.
He and Deng (2012) use Kullback-Leibler reg-ularization, raising the need of having normalizedmodels hm,?
(E,F ).
We employ the more generalL2-regularization and the objective function is de-fined asO(?)
= log?????
?
?????
?2(9)including the hyper parameter ?
controlling the de-gree of regularization.
The derivative of the objec-tive function, which is needed for the gradient-basedtraining methods, directly follows:?O(?)?
?= ??
?
2?+1????????????(10)With?hm,?
(E,F )?
?= #?
(E,F ) the number of timesfeature ?
fires in the derivation for translation hy-pothesis E given source sentence F , the deriva-tive of p?
(E|F ) is defined as (for ease of notationE?
(Fn) is represented by En)?p?
(E|F )?
?= ?p?
(E|F )?
(11)(#?
(E,F )??E??Enp?
(E?|F )#?
(E?, F ))And the derivative of the expected BLEU is???????=1NN?n=1?E?En?(E)?p?(E|Fn)?
?= ?1NN?n=1(?E?Enp?
(E|F )?(E)#?
(E,F )?(?E?Enp?
(E|F )?(E))?(?E?Enp?
(E|F )#?
(E,F )))(12)This can be more compactly expressed by local ex-pectations ??
?nof the BLEU score and the featurecount #?:??????
?= ?1NN?n=1(??#??n?
???n?#?
?n)In our implementation, #?is moved to the front ofthe equation to obtain common factors that can beused by all parameter updates:???????=1NN?n=1?E?En#?
(E,F )?p?
(E|F )(???n?
?
(E)) (13)5.2 Leave-one-outAlthough He and Deng (2012) claim that it is notnecessary, we apply a leave-one-out heuristic similarto (Wuebker et al, 2010) when generating the n-bestlists on the training data.
The authors have shownthis to effectively counteract over-fitting effects andwe argue that it helps to bring out the full potentialof our discriminative training procedure.When we decode the training data of our transla-tion model, very long and rare phrases can be usedto translate the sentence.
The translation probabil-ity for these phrases, which are often singletons,are generally over-estimated by the heuristic countmodel.
When they are too dominant in the n-bestlists they effectively render the training data use-less, as they are unlikely to generalize to unseendata.
The idea of leave-one-out is that for decodingeach sentence, the global counts of the relative fre-quency estimates are reduced by the local counts ex-tracted from the current sentence pair.
This way, the1520above mentioned rare phrases are penalized and thedecoder is encouraged to use more general phrasestaken from the remainder of the training data.
Sin-gleton phrases are given a fixed penalty.
In thiswork, we apply leave-one-out with all update strate-gies.5.3 FeaturesMaximum expected BLEU training facilitates train-ing of arbitrary features.
In this work we apply fourtypes of features.
(a) A discriminative phrase table,i.e.
one feature for each phrase pair.
(b) Lexical fea-tures, i.e.
one feature for each source-target wordpair that appear within the same phrase.
(c) Sourceand target triplet features (Hasan et al, 2008), i.e.triples of one source and two target words or one tar-get and two source words appearing within a singlephrase pair.
(d) The hierarchical lexicalized reorder-ing model (Galley and Manning, 2008), i.e.
onefeature for each combination of phrase pair, orienta-tion (monotone (M), swap (S) or discontinuous (D))and orientation direction (forward or backward).
GTis only applied with feature set (a), where we re-estimate the two phrasal channel models as was donein (He and Deng, 2012).
With the other update algo-rithms we follow the approach taken in (Auli et al,2014) and condense each feature type into a smallnumber of models for the log-linear combination,which is afterwards tuned with MERT.
(a) and (b)result in a single additional model, (c) in two mod-els (source and target triplets) and (d) in six models({forward,backward}?
{M,S,D}).5.4 Efficient ImplementationThe expected BLEU ???
?is efficiently computed inone iteration over the full n-best list.
As can beseen from Equation 13, the derivative??????
?is ad-ditive with respect to each firing instance of feature?
in the n-best list.
The additive factor only de-pends on the current sentence pair.
Therefore, foreach sentence of the training data we iterate throughits n-best list once to compute the expectation of thesentence-level BLEU score ??
?nand then a secondtime to update the current derivative for each timethe feature fires.
The only thing that needs to bekept in memory is a list of the current derivatives foreach parameter ?.1.
Create the baseline system and run MERT2.
Generate n-best list on training corpus3.
Compute sentence-level BLEU ?
(En)for each hypothesis Enin the list4.
Initialize parameters with ?
= 0, ??
?
?5.
Iterate:a) Compute the derivatives?O(?)?
?b) Perform update and output ?(t)6.
Run MERT on dev with each table ?(t)7.
Select best ?
(t)on dev8.
Evaluate on test setsFigure 1: The complete training algorithm.5.5 Complete Training AlgorithmThe complete training and evaluation procedure isshown in Figure 1.
We start by building a base-line translation system with MERT-optimized modelweights ?.
With the baseline system we generate n-best lists on the training data.
Now, for each trans-lation hypothesis Enof the n-best list, we computethe sentence-level BLEU score ?
(En) and initializethe parameter set for training with the count model.Next, we run the training algorithm for a fixed num-ber of iterations1and output the updated feature val-ues ?
(t)after each iteration t. Finally, we run MERTwith each ?
(t), select the best table on dev and eval-uate on our test sets.6 Experiments6.1 SetupThe experiments are carried out on the IWSLT2013 German?English shared translation task.2For rapid experimentation, the translation model istrained on the in-domain TED portion of the bilin-gual data, which is also used for maximum expectedBLEU training.
However, we use a large 4-gram LMwith modified Kneser-Ney smoothing (Kneser andNey, 1995; Chen and Goodman, 1998), trained withthe SRILM toolkit (Stolcke, 2002).
As additionaldata sources for the LM we use the complete NewsCommentary, Europarl v7 and Common Crawl cor-pora as well as selected parts of the Shuffled News1Note that we keep the ?
weights fixed throughout all itera-tions of maximum expected BLEU training.2http://www.iwslt2013.org1521IWSLT BOLT WMTGerman English Chinese English German EnglishSentences 138K 4.08M 4.09MRun.
Words 2.63M 2.70M 78.3M 85.9M 105M 104MVocabulary 75.4K 50.2K 384K 817K 659K 649KTable 1: Statistics for the bilingual training data of the IWSLT 2013 German?English, the DARPA BOLTChinese?English and the WMT 2014 German?English tasks.and LDC English Gigaword corpora.
The selec-tion is based on cross-entropy difference (Mooreand Lewis, 2010).
This makes for a total of 1.7billion running words for LM training.
The base-line further contains a hierarchical reordering model(HRM) (Galley and Manning, 2008) and a 7-gramword class language model (Wuebker et al, 2013).On IWSLT, all results are averages over three inde-pendent MERT runs, and we evaluate statistical sig-nificance with MultEval (Clark et al, 2011).To confirm our findings, additional experimentsare run on two large-scale tasks over strong baselinesincluding recurrent neural language models.
On theDARPA BOLT Chinese?English task we use ourinternal evaluation system as a baseline.
It is a pow-erful hierarchical phrase-based SMT engine with 19dense features, including an LSTM recurrent neu-ral language model (Sundermeyer et al, 2012) anda hierarchical reordering model (Huck et al, 2013).The 5-gram backoff LM is in total trained on 2.9 bil-lion running words.
We use the same data for tuningand testing as Setiawan and Zhou (2013), namely1275 (tune) and 12393sentences of web data takenfrom LDC2010E30, the NIST MT06 evaluation setand an additional single-reference test set from thediscussion forum (df) domain containing 1124 sen-tence pairs.
Maximum expected BLEU training isperformed on the discussion forum portion of thetraining data, consisting of 67.8K sentence pairs.On the German?English task of the 9th Workshopon Statistical Machine Translation4, both translationmodel and maximum expected BLEU training is per-formed on all available bilingual data.
Our base-line is a phrase-based translation engine with a 4-gram backoff LM trained on 2.5 billion words withlmplz (Heafield et al, 2013), a recurrent neural3named dev in (Setiawan and Zhou, 2013)4http://statmt.org/wmt14/IWSLT de-en # feat.
testbaseline 18 30.4GT (He and Deng, 2012) 6.08M 30.9SGD (Auli et al, 2014) 921K 30.8AdaGrad (Green et al, 2013) 921K 31.1RPROP (this work) 921K 31.3RPROP w/o leave-one-out 921K 30.7RPROP all features 5.22M 31.6Table 2: Results for the IWSLT 2013 German?Englishtask in BLEU [%].
The comparison between updatestrategies is done with feature set (a) and RPROP all fea-tures uses feature sets (a)-(d).
GT, SGD, AdaGrad andRPROP are trained with leave-one-out, unless otherwisespecified.LM, a 7-gram word class LM and the HRM.Bilingual data statistics for all tasks are given inTable 1.
We use the machine translation toolkit Jane(Vilar et al, 2010; Wuebker et al, 2012) and evalu-ate with case-insensitive BLEU [%] (Papineni et al,2002) in all experiments.6.2 Experimental ResultsTable 2 shows the IWSLT results.
We first com-pare the performance of the four update algorithms,for simplicity only on the discriminative phrase ta-ble features.
Different from previous work the n-best lists of the training data were generated withleave-one-out, unless otherwise stated.
In all caseswe tested different values for the regularization pa-rameter ?
and in the case of SGD and AdaGrad alsofor the learning rate ?.
We selected the best con-figurations based on a validation set (test2011).
ForAdaGrad we also experimented with FOBOS regu-larization and feature selection (Duchi and Singer,2009), but did not observe improved results.
As152241.541.641.741.841.94242.15  10  15  20  25  30  35  40expectedBLEU[%]IterationRPROPSGDAdaGradGTFigure 2: Expected BLEU value on IWSLTGerman?English for the different update strate-gies.
Note that growth transformation (GT) applies adifferent regularization term and is therefore not directlycomparable with the other techniques.expected, we found that in all cases regularizationis not strictly necessary - results are barely affectedas long as ?
is sufficiently small - and that SGD ismuch more sensitive to ?
than AdaGrad.
Further,SGD and RPROP need around 25 iterations to reachgood results, where 5-10 iterations are sufficient forGT and AdaGrad.
For a fair comparison, however,we run all algorithms for 40 iterations and select thebest one on a seletion set, namely iterations 19 (Ada-Grad), 23 (GT), 29 (RPROP) and 35 (SGD).
Figure2 shows how the expected BLEU function evolves intraining with different update strategies.
Althoughthe value for GT is not directly comparable to theothers due to a different regularization term, the re-spective characteristics are clearly visible.
SGD ex-hibits a linear growth pattern, GT resembles a loga-rithmic and RPROP an exponential function.
Afterinitially overshooting and then retracting as the reg-ularization kicks in, AdaGrad also displays logarith-mic characteristics.In terms of BLEU RPROP performs best, followedby AdaGrad, GT and SGD, where the RPROP-AdaGrad and AdaGrad-GT differences are small(0.2% BLEU absolute) but statistically significant onthe 95% level.
Altogether, RPROP improves overthe baseline by 0.9 BLEU points, which is statisti-cally significant at the 99% level.
In an additionalexperiment we verified that leave-one-out has a clearBOLT zh-en # feat.
df web MT06baseline 19 18.0 34.1 39.7SGD 12.4M 18.0 34.3 39.8AdaGrad 12.4M 18.3 34.7 40.1RPROP 12.4M 18.7 34.8 40.5Setiawan&Zhou (GT) 150M - 32.7 40.3Table 3: Results for the BOLT Chinese?English taskin BLEU [%] on the discussion forum test set (df), themixed web test set and NIST MT06.
The baseline is ourBOLT evaluation system and contains a recurrent neuralLM.
We compare with (Setiawan and Zhou, 2013) whoapplied maximum expected BLEU training with growthtransformation (GT).
Note that the number of features re-ported by Setiawan and Zhou (2013) is artificially blownup due to renormalization.impact on the results.
The BLEU difference betweenRPROP with and without leave-one-out is 0.6% ab-solute.
By adding lexical, triplet and reordering fea-tures, we get an additional gain and observe a totalimprovement of 1.2 BLEU points over the baselinesystem.Efficiency comparison.
921K discriminativephrase table features are active in our training data.Due to the renormalization component, this resultsin a total of 6.08M features that are updated withGT using the same data.
Consequently, it is less timeand space efficient than the other algorithms.
Withour implementation, GT needed around 16 hoursand 6.7G memory for 40 iterations, where RPROP,AdaGrad and SGD finished after less than 2.5 hoursand required 2.1G memory.For the BOLT task, we directly compare with theGT-trained system in (Setiawan and Zhou, 2013)using the same tune set for MERT and reportingresults on the same test sets, see Table 3.
WithRPROP we achieve nearly twice the improvementreported by Setiawan on both web and MT06 us-ing feature sets (a)-(c)5.
Our baseline on web isalready much stronger and RPROP training yields+0.7 BLEU points, as opposed to +0.44 reported bySetiawan.
On MT06 our baseline system is slightlyworse, but with the larger gain received by RPROPour final system outperforms the one reported by Se-5Reordering features are not applicable to our hiero system.1523WMT de-en # feat.
newstest2013baseline 19 28.3RPROP 45.0M 28.9matrix.statmt.org 14 28.1Table 4: Results for the WMT German?English task inBLEU [%].
The baseline contains a recurrent neural LM.We compare with the best single system that is reportedon matrix.statmt.org, which was submitted by theUnversity of Edinburgh.tiawan by 0.2 BLEU points.
We would like to stressthat this is not a domain adaptation effect, as maxi-mum expected BLEU training was performed on dis-cussion forum (df) data.
On the df test set, on theother hand, we probably can observe domain adap-tation via RPROP training.
The improvement hereis 0.7% BLEU absolute with a single reference, asopposed to four references on web and MT06.
Wealso report results training the same feature sets withSGD and AdaGrad, confirming results we observedon IWSLT.
Here, SGD yields only minor improve-ments.
AdaGrad performs better, but still 0.1 - 0.4BLEU points worse than RPROP.
Running GT is in-feasible in our hierarchical phrase-based setup.Table 4 shows the results on the WMT task.
Thisis our largest setting, where max.
exp.
BLEU train-ing is performed on the full training data with morethan 4M sentence pairs which, to the best of ourknowledge, is unsurpassed in the literature.
Alto-gether, training took more than one month, about3/4 of which were for generating n-best lists by de-coding the training data.
The triplet features did notfinish in time, so we applied the feature sets (a), (b)and (d), 45M features in total.
With a renormaliza-tion step as in GT, this number would grow to 309M.On newstest2013, our baseline already outperformsthe best single system reported on matrix.statmt.orgby 0.2 BLEU points.
The discriminatively trainedfeatures yield an additional improvement of 0.6%BLEU absolute on this high-end system.7 ConclusionWe have experimentally compared several updatestrategies for maximum expected BLEU training.The RPROP algorithm proposed in this work showssuperior performance compared to AdaGrad, growthtransformation (GT) and stochastic gradient descent.In terms of time and memory efficiency, GT isclearly inferior to the other algorithms due to renor-malization.
Applying phrasal, lexical, triplet and re-ordering features, the baseline is improved by 1.2%BLEU absolute on the IWSLT German?Englishtask.
On two large scale tasks we achieve clearlysuperior performance compared to results reportedin the literature.
On BOLT Chinese?English ourdiscriminative training yields nearly twice the im-provement reported by Setiawan and Zhou (2013),resulting in a superior final system.
On WMTGerman?English, we outperform the best singlesystem reported on matrix.statmt.org by 0.8% BLEUabsolute.
Here, we perform maximum expextedBLEU training on more than 4M sentence pairs,which is the largest number reported in the literatureto date.AcknowledgmentsThe research leading to these results has receivedfunding from the European Union Seventh Frame-work Programme (FP7/2007-2013) under grantagreement no287658.
This material is also partiallybased upon work supported by the DARPA BOLTproject under Contract No.
HR0011- 12-C-0015.Any opinions, findings and conclusions or recom-mendations expressed in this material are those ofthe authors and do not necessarily reflect the viewsof DARPA.
We further thank the anonymous review-ers for valuable comments.ReferencesMichael Auli, Michel Galley, and Jianfeng Gao.
2014.Large-scale Expected BLEU Training of Phrase-basedReordering ModelsI.
In Proceedings of the 2014 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 1250?1260, Doha, Qatar,October.Phil Blunsom, Trevor Cohn, and Miles Osborne.
2008.A discriminative latent variable model for statisticalmachine translation.
In Proc.
of ACL-HLT.Peter F. Brown, Stephan A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The Mathemat-ics of Statistical Machine Translation: Parameter Es-timation.
Computational Linguistics, 19(2):263?311,June.1524Francisco Casacuberta and Enrique Vidal.
2004.
Ma-chine translation with inferred stochastic finite-statetransducers.
Computational Linguistics, 30(2):205?225.Stanley F. Chen and Joshuo Goodman.
1998.
An Em-pirical Study of Smoothing Techniques for LanguageModeling.
Technical Report TR-10-98, ComputerScience Group, Harvard University, Cambridge, MA,August.D.
Chiang, Y. Marton, and P. Resnik.
2008.
Online large-margin training of syntactic and structural translationfeatures.
In Conference on Empirical Methods in Nat-ural Language Processing, pages 224?233, Honolulu,Hawaii.David Chiang.
2005.
A Hierarchical Phrase-BasedModel for Statistical Machine Translation.
In Pro-ceedings of the 43rd Annual Meeting on Associationfor Computational Linguistics, pages 263?270, AnnArbor, Michigan, USA, June.Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.Smith.
2011.
Better hypothesis testing for statis-tical machine translation: Controlling for optimizerinstability.
In 49th Annual Meeting of the Associa-tion for Computational Linguistics:shortpapers, pages176?181, Portland, Oregon, June.John Duchi and Yoram Singer.
2009.
Efficient online andbatch learning using forward backward splitting.
Jour-nal of Machine Learning Research, 10:2899?2934,December.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learning andstochastic optimization.
Journal of Machine LearningResearch, 12:2121?2159, July.Michel Galley and Christopher D. Manning.
2008.
Asimple and effective hierarchical phrase reorderingmodel.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, EMNLP?08, pages 848?856, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Jianfeng Gao and Xiaodong He.
2013.
Training MRF-Based Phrase Translation Models using Gradient As-cent.
In Proceedings of the North American Chap-ter of the Association for Computational Linguistics- Human Language Technologies conference (NAACLHLT), pages 450?459, Atlanta, Georgia, USA, Jun.Spence Green, Sida Wang, Daniel Cer, and Christo-pher D. Manning.
2013.
Fast and adaptive onlinetraining of feature-rich translation models.
In 51stAnnual Meeting of the Association for ComputationalLinguistics, pages 311?321, Sofia, Bulgaria, August.Spence Green, Daniel Cer, and Christopher D. Manning.2014.
An empirical comparison of features and tunignfor phrase-based machine translation.
In Proceedingsof the Ninth Workshop on Statistical Machine Trans-lation, pages 466?476, Baltimore, Maryland, USA,June.Stefan Hahn, Marco Dinarelli, Christian Raymond,Fabrice Lefevre, Patrick Lehnen, Renato De Mori,Alessandro Moschitti, Hermann Ney, and GiuseppeRiccardi.
2011.
Comparing Stochastic Approachesto Spoken Language Understanding in Multiple Lan-guages.
IEEE Transactions on Audio, Speech, andLanguage Processing, 19(6):1569?1583, August.Sa?sa Hasan, Juri Ganitkevitch, Hermann Ney, and Jes?usAndr?es-Ferrer.
2008.
Triplet lexicon models for sta-tistical machine translation.
In Conference on Empir-ical Methods in Natural Language Processing, pages372?381, Honolulu, Hawaii, October.
Association forComputational Linguistics.Xiaodong He and Li Deng.
2012.
Maximum ExpectedBLEU Training of Phrase and Lexicon TranslationModels.
In Proceedings of the 50th Annual Meeting ofthe Association for Computational Linguistics (ACL),pages 292?301, Jeju, Republic of Korea, Jul.Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H. Clark,and Philipp Koehn.
2013.
Scalable modified Kneser-Ney language model estimation.
In Proceedings of the51st Annual Meeting of the Association for Compu-tational Linguistics, pages 690?696, Sofia, Bulgaria,August.Georg Heigold, Stefan Hahn, Patrick Lehnen, and Her-mann Ney.
2011.
EM-Style Optimization of Hid-den Conditional Random Fields for Grapheme-to-Phoneme Conversion.
In IEEE International Con-ference on Acoustics, Speech, and Signal Processing,pages 4920?4923, Prague, Czech Republic, May.Mark Hopkins and Jonathan May.
2011.
Tuning as rank-ing.
In In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Processing,pages 1352?1362, Edinburgh, Scotland, July.Matthias Huck, Joern Wuebker, Felix Rietig, and Her-mann Ney.
2013.
A phrase orientation model for hi-erarchical machine translation.
In Workshop on Statis-tical Machine Translation, pages 452?463, Sofia, Bul-garia, August.Abraham Ittycheriah and Salim Roukos.
2007.
DirectTranslation Model 2.
In Proceedings of the NorthAmerican Chapter of the Association for Computa-tional Linguistics - Human Language Technologiesconference (NAACL HLT), pages 57?64, Rochester,NY, USA, Apr.Reinerd Kneser and Hermann Ney.
1995.
Improvedbacking-off for M-gram language modeling.
In Pro-ceedings of the International Conference on Acous-tics, Speech, and Signal Processingw, volume 1, pages181?184, May.1525P.
Koehn, F. J. Och, and D. Marcu.
2003.
StatisticalPhrase-Based Translation.
In Proceedings of the 2003Meeting of the North American chapter of the Associa-tion for Computational Linguistics (NAACL-03), pages127?133, Edmonton, Alberta.Thomas Lavergne, Alexandre Allauzen, Josep MariaCrego, and Franc?ois Yvon.
2011.
From n-gram-based to crf-based translation models.
In Proceedingsof the Sixth Workshop on Statistical Machine Transla-tion, pages 542?553, Edinburgh, Scotland, July.
Asso-ciation for Computational Linguistics.Percy Liang, Alexandre Buchard-C?ot?e, Dan Klein, andBen Taskar.
2006.
An End-to-End Discriminative Ap-proach to Machine Translation.
In Proceedings of the21st International Conference on Computational Lin-guistics and the 44th annual meeting of the Associ-ation for Computational Linguistics, pages 761?768,Sydney, Australia.Jos?e B Mari?no, Rafael E Banchs, Josep M Crego, Adri`ade Gispert, Patrik Lambert, Jos?e A R Fonollosa, andMarta R Costa-juss`a.
2006.
N-gram-based MachineTranslation.
Comput.
Linguist., 32(4):527?549, De-cember.R.C.
Moore and W. Lewis.
2010.
Intelligent Selectionof Language Model Training Data.
In ACL (Short Pa-pers), pages 220?224, Uppsala, Sweden, July.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine transla-tion.
Computational Linguistics, 30(4):417?449, De-cember.Franz Josef Och.
2003.
Minimum Error Rate Train-ing in Statistical Machine Translation.
In Proc.
of the41th Annual Meeting of the Association for Compu-tational Linguistics (ACL), pages 160?167, Sapporo,Japan, July.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a Method for Automatic Eval-uation of Machine Translation.
In Proceedings of the41st Annual Meeting of the Association for Computa-tional Linguistics, pages 311?318, Philadelphia, Penn-sylvania, USA, July.Martin Riedmiller and Heinrich Braun.
1993.
A directadaptive method for faster backpropagation learning:The rprop algorithm.
In IEEE International Confer-ence on Neural Networks, pages 586?591.Hendra Setiawan and Bowen Zhou.
2013.
Discrimina-tive training of 150 million translation parameters andits application to pruning.
In NAACL-HLT 2013, pages335?341, Atlanta, Georgia, June.Patrick Simianer, Stefan Riezler, and Chris Dyer.
2012.Joint Feature Selection in Distributed StochasticLearning for Large-Scale Discriminative Training inSMT.
In Proceedings of the 50th Annual Meetingof the Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 11?21, Jeju Island, Korea,July.
Association for Computational Linguistics.Andreas Stolcke.
2002.
SRILM ?
An Extensible Lan-guage Modeling Toolkit.
In Proc.
of the Int.
Conf.
onSpeech and Language Processing (ICSLP), volume 2,pages 901?904, Denver, CO, September.Martin Sundermeyer, Ralf Schl?uter, and Hermann Ney.2012.
Lstm neural networks for language modeling.In Interspeech, Portland, OR, USA, September.David Vilar, Daniel Stein, Matthias Huck, and HermannNey.
2010.
Jane: Open source hierarchical transla-tion, extended with reordering and lexicon models.
InACL 2010 Joint Fifth Workshop on Statistical MachineTranslation and Metrics MATR, pages 262?270, Upp-sala, Sweden, July.Taro Watanabe, Jun Suzuki, Hajime Tsukada, and HidekiIsozaki.
2007.
Online large-margin training for statis-tical machine translation.
In Proceedings of the 2007Conference on Empirical Methods in Natural Lan-guage Processing, pages 764?773, Prague, Czech Re-public, June.Simon Wiesler, Alexander Richard, Ralf Schl?uter, andHermann Ney.
2013.
A Critical Evaluation ofStochastic Algorithms for Convex Optimization.
InIEEE International Conference on Acoustics, Speech,and Signal Processing, pages 6955?6959, Vancouver,Canada, May.Joern Wuebker, Arne Mauser, and Hermann Ney.
2010.Training phrase translation models with leaving-one-out.
In Proceedings of the 48th Annual Meeting of theAssoc.
for Computational Linguistics, pages 475?484,Uppsala, Sweden, July.Joern Wuebker, Matthias Huck, Stephan Peitz, MalteNuhn, Markus Freitag, Jan-Thorsten Peter, Saab Man-sour, and Hermann Ney.
2012.
Jane 2: Opensource phrase-based and hierarchical statistical ma-chine translation.
In International Conference onComputational Linguistics, pages 483?491, Mumbai,India, December.Joern Wuebker, Stephan Peitz, Felix Rietig, and HermannNey.
2013.
Improving statistical machine translationwith word class models.
In Conference on Empiri-cal Methods in Natural Language Processing, pages1377?1381, Seattle, USA, October.Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao.2013.
Max-violation perceptron and forced decodingfor scalable mt training.
In Conference on Empiri-cal Methods in Natural Language Processing, pages1112?1123, Seattle, USA, October.1526
