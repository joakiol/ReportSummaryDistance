Efficient Bottom-Up ParsingRobert Moore and John DowdingSRI International333 Ravenswood AvenueMenlo Park, CA 94025AbstractThis paper describes a series of experiments aimed atproducing a bottom-up arser that will produce partialparses suitable for use in robust interpretation and stillbe reasonably efficient.
In the course of these experi-ments, we improved parse times by a factor of 18 overour first attempt, ending with a system that was twiceas fast as our previous parser, which relied on strongtop-down constraints.
The major algorithmic variationswe tried are described along with the corresponding per-formance results.IntroductionElsewhere \[1\] we describe a change in our approach toNL processing to allow for more robust methods of in-terpretation.
One consequence of this change is that itrequires a different ype of parsing algorithm from theone we have been using.
In our previous SLS work, wehave used a shift-reduce l ft-corner parser incorporatingstrong top-down constraints derived from the left con-text, to limit the structures built by the parser \[2\].
Withthis parser, no structure is built unless it can combinewith structures already built to contribute to an analysisof the input as a single complete utterance.
If we wantto find grammatical fragments of the input that may beof use in robust interpretation, however, such strong useof top-down constraints i not appropriate.To address this issue, we have built and measured theperformance of a number of bottom-up arsers.
Theseparsers use the same unification grammar as our shift-reduce parser, but they do not impose the strong top-down constraints of the original.
These experimentalparsers fall into two groups: purely bottom-up parsersand bottom-up arsers that use limited top-down con-straints.
The experiments were performed using a fixedgrammar and lexicon for the Air Trave!
Information Sys-tem (ATIS) domain, and an arbitrarily selected test cor-pus of 120 ATIS0 training sentences.
The test grammarcould produce complete parses for 79 of these 120 sen-tences.Pure Bottom-Up ParsingThe first parser we implemented was a straightforward"naive" implementation of the CKY algorithm \[3, 4\]adapted to unification grammar.
In this algorithm, a"chart" is maintained that contains records, or "edges,"for each type of linguistic category that has been foundbetween given start and end positions in a sentence.
Incontext-free parsing, these categories are simply the non-terminal symbols of the grammar.
In a unification gram-mar, they are complex structures that assign values toparticular features of a more general category type.Our naive algorithm simply seeds the chart with edgesfor each possible category for all the words in the sen-tence, and then works left to right constructing addi-tional edges bottom-up.
Each time an edge is added tothe chart, the grammar is searched for rules whose lastcategory on the right-hand side matches the edge justadded to the chart, and the chart is scanned back to theleft for a contiguous equence of edges that match theremaining categories on the right-hand side of the rule.If these are found, then an edge for the category on theleft-hand side of the rule is added to the chart, span-ning the segment of the input covered by the sequenceof edges that matched the right-hand side of the rule.When measured with our test grammar and test cor-pus, our implementation f this algorithm is almost ninetimes slower than our original shift-reduce parser.
Weconjectured that one significant problem was the uncon-strained hypothesization f empty categories or "gaps.
"Our grammar, like many others, allows certain linguis-tic phrase types to be realized as the empty string inorder to simplify the overall structure of the grammar.For example, "What cities does American fly to fromBoston?"
is analyzed as having an empty noun phrasebetween "to" and "from," so that most of the analysiscan be carried out using the same rules that are used toanalyze such sentences as "Does American fly to Dallasfrom Boston?"
Because empty categories are not di-rectly indicated in the word string, our naive bottom-upparser must hypothesize very possible empty categoryat every point in the input.To address this point, we applied a well-known trans-formation to the grammar to eliminate mpty categoriesby adding additional rules.
For each type of empty cat-200egory, we found every case where it would unify with acategory on the right-hand side of a rule, performed theunification, and deleted the unified empty category fromthe rule.
For example, if B can be an empty categorythen from A ~ BC we would derive the rule A ~ C,taking into account he results of unification.
When allsuch derived rules are added to the grammar, all theempty categories can be eliminated.Performing this transformation both reduced the num-ber of edges being generated and speeded up parsing,but only by about 20 percent in each case.
We observedthat the elimination of empty categories had resulted in agrammar with many more unit production rules than theoriginal grammar; that is, rules of the form A ~ B. Thisoccurred because of the large number of cases like theone sketched above, where an empty category matchesone of the categories on the right-hand side of a binarybranching rule.
We determined that the application ofthese unit production rules accounted for more than 60percent of the edges constructed by the parser.Our next thought, therefore, was to try to transformthe grammar to eliminate unit productions as well, butthis process turned out to be, in practical terms, in-tractable.
Eliminating empty categories had increasedthe grammar size but only by about half.
When wetried to eliminate unit productions, processing the firstfour (out of several hundred) grammar ules took a cou-ple of hours of computation time and generated morethan 1800 derived rules.
We abandoned this approach,and instead we eliminated the unit productions from thegrammar by compiling them into a "link table."
Thelink table is basically the transitive closure of the unitproductions, so it is, in effect, a specification of the unitderivations permitted by the grammar, omitting the in-termediate nodes.
This table is then used by the parserto find a path via unit productions between the edges inthe chart and the categories that appear in the nonunitgrammar ules.
This is effectively the same as the CKYalgorithm except hat edges that would be produced byunit derivations are never explicitly created.We also made some modifications to speed up selec-tion of applicable grammar ules.
We added a "skeletal"chart that keeps track of the sequences of general cat-egories (ignoring features) that occur in the chart (orcould be generated using the link table), with the re-striction that the only sequences recorded are those thatare initial segments of the sequence of general categories(ignoring features) on the right-hand side of some gram-mar rule.
Each grammar ule is itself indexed by thesequence of general categories occuring on its right-handside.
For example, if there is some sort of verb spanningposition x through position y in the input and some somesort of noun phrase spanning position y through positionz, the skeletal chart would record that there is a sequenceof type v.xtp ending at point z.
Thus, when the parsersearches for applicable rules to apply to generate newedges in the chart at a particular position, it only con-siders rules which are indexed by an entry in the skeletalchart for that position.Eliminating unit productions by use of the link ta-ble and accessing the grammar ules through the skele-tal chart made the parser substantially faster, but thisparser is still almost hree times slower than the shift re-duce parser on our test corpus using our test grammar.At this point, we seemed to have reached a practicallimit to how fast we could make the parser while stillconstructing essentially every possible edge bottom-up.This parser is in fact almost twice as fast as the shift-reduce parser in terms of time per edge constructed, butit constructs more than four times as many edges.Making Limited Use of ContextOur limited success in constructing a purely bottom-up parser that would be efficient enough for practicaluse with our unification grammar led us to reconsiderwhether it is really necessary to compute every phrasethat can be identified bottom-up in order to use the out-put of the parser in a robust interpretation scheme.
Weagain focused our attention on syntactic gaps.
Althoughwe had dealt effectively with explicitly empty categoriesand with categories generated by the unit productionscreated by the elimination of empty categories, we knewthat many of the additional edges the bottom-up arserwas creating were for larger phrases that implicitly con-tain gaps (e.g., a transitive verb phrase with a missingobject noun phrase), even when there is nothing in thepreceding context o license such a phrase.
We reasonedthat there is little benefit to identifying such phrases,the vast majority of which would be spurious anyway,because unless we can determine the semantic filler of agap, the phrase containing it is unlikely to be of any usein robust interpretation.With this rationale, we have implemented several vari-ants of a bottom-up arsing algorithm that allows us touse limited top-down constraints derived from the left-context o block the formation of just the phrases thatimplicitly contain gaps not licensed by the preceding con-text.
For example, in the sentence we previously dis-cussed, "What cities does American fly to from Boston?
"the interrogative noun phrase "what cities" signals thepossible presence of a noun phrase gap later in the sen-tence.
This licensesfly tofly to from BostonAmerican fly to from Bostondoes American fly to from Bostonall as being legitimate phrases that contain a nounphrase gap.
Without that preceding context, we wouldnot want to consider any of these word strings as legiti-mate phrases.To implement his approach we partitioned the setof grammatical categories into context-independent a dcontext-dependent subsets, with the context-dependentcategories being those that implicitly contain gaps.Defining which categories those are is relatively easy inour grammar, because we have a uniform treatment of201"wh" gaps, usually called "gap-threading" \[5\], so thatevery category that implicitly or explicitly contains agap has a feature gaps in  whose value is something otherthan nu l l .
We have a similar treatment of the fronting ofauxiliary verbs in yes/no questions, controlled by the fea-ture vs tore .
Finally, an additional quirk of our grammarrequired us to treat all relative clauses as context depen-dent categories.
So we defined the context-independentcategories to be those that?
Have nu l l  as the value of gaps in  or lack the featuregaps in ,  and?
Have nu l l  as the value of vs tore  or lack the featurevstore, and?
Are not relative clauses.All other categories are context dependent.These is, of course, simply one of any number ofways that categories could be divided between context-independent and context-dependent.
Our ability tochange these declarations gives us an interesting pa-rameterization of our parser, such that it can be runas anything from a purely bottom-up arser, if all cat-egories are declared context-independent, to one thatuses maximum prediction based on left context like ourshift-reduce parser, if all categories are declared context-dependent.
It would also be possible to derive a candi-date set of context-dependent categories automaticallyor semi-automatically from a corpus.
The candidates forcontext-dependent categories would be those categoriesthat most often fail to contribute to a complete parsewhen found bottom-up.
1The basic parsing algorithm remains the same as inthe purely bottom-up arsers, with a few modifications.After each rule application the resulting category ischecked to see whether it unifies with one of the context-independent categories.
I f  so, the edge for it is added tothe chart with no further checking.
If  not, a test is madeto see whether the category is predicted by the precedingleft context.
I f  so, it is added to the chart; otherwise, itis rejected.The main complexities of the algorithm are in the gen-eration and testing of predictions.
Whenever an edge isadded to the chart, predictions are generated that aresimilar to "dotted rules" or "incomplete dges," exceptthat predictions include only the remaining categories tobe matched, since predictions are not used in a reduc-tion step as they are in other algorithms.
So, if we havea rule of the form A ~ BC and we add an edge for B tothe chart, then we may add a prediction for C followingB.
Whether the prediction is made or not depends on anumber of things, including whether the left-hand side ofthe rule is context-dependent or independent.
In the cur-rent example, if A is a context-independent ca egory, weproceed with the prediction; otherwise, we must checkwhether A itself is predicted.
In addition, predictionsX Tb_is idea arose in response to a question posed by MitchMarcus.can arise from matching part of a previous prediction.If we have predicted AB and we find A, then we canpredict B.In order to minimize the number of predictions made,we make two important checks.
First we check that theprediction actually predicts ome context-dependent ca -egory.
Second, we do a "follow" check, to make sure thatthe predicted category might occur, given the next wordin the input stream.
There are a few other minor re-finements to limit the number of predictions, but theseare the most important ones.
In order to check whethera context-dependent category is predicted by a certainprediction, we consult a "left-corner teachability table"that tells us whether the category we are testing is apossible left corner of the predicted category.When we tested this algorithm, we found that it dra-matically reduced the number of edges generated, andequally dramatically improved parse time.
We notedabove that our best purely bottom-up arser was aboutthree times slower that the shift-reduce parser.
Thisalgorithm proved to be 20 percent faster than the shift-reduce parser on our test corpus and test grammar.Examination of the number and type of edges pro-duced by this weakly-predictive parser led us to ques-tion whether all the refinements that we had made tothe purely bottom-up arsers, in order to deal with theenormous number of edges they produced, were still nec-essary.
We have performed a number of experimentsremoving some of those refinements, with interesting re-sults.
The main effect we observed was that using thelink table to avoid creating edges for categories producedby unit derivations i  no longer productive.
By using thelink table to create explicit edges for those categories, sothat we do not have to use the link table at the timewe match the right-hand sides of rules against he chart,we got a parser that was twice as fast as the shift re-duce parser.
We also found that leaving empty cate-gories in the grammar actually speeded-up this versionof the parser very slightly (about 4 percent).
More edgesand predictions were generated for the empty categories,but this was apparently more than compensated for bythe reduction in the number of grammar ules.Conclus ionsThis paper is, in effect, a narrative of an exercise in al-gorithm design and software engineering.
Unlike mostalgorithms papers, it contains a great deal of detail onwhat did not work, or at least what did not work aswell as had been hoped.
It is also notable because ittalks about practical, rather than theoretical efficiency.Most papers on parsing algorithms focus on theoreticalworst-case time bounds.
Although we have not analyzedit, it seems likely that all the algorithms we tried havethe same polynomial time bound, but the difference inthe constants of proportionality involved makes all thedifference between the algorithms being usable and notusable.
Also, unlike most experimental results on pars-ing, ours are based on a real grammar, being developed202for a real application, ot a toy grammar written only forthe purposes of testing parsing algorithms.
It is unlikelythat the problems with gaps that are absolutely crucialin this exercise would arise in such a toy grammar.In terms of concrete results, the relative performanceof several of the parsers is summarized in the table below.Parser Time # Edg~shift-reduce 1.00 1.00naive bottom-up 8.81 12.52best bottom-up 2.95 4.62best predictive 0.48 1.79es Time/Edge1.000.700.630.27Notice that all the new parsers are significantly fasterthan the shift-reduce parser in terms time per edge gen-erated.
This is undoubtedly due to the high overhead ofthe prediction mechanism used in the shift-reduce parser.It is also interesting to note that among the new parsers,the faster the overall speed of the parser, the faster thetime per edge, also.
This may be somewhat surprising,because of all the additional mechanisms added to thelast two parsers to reduce the number of edges, comparedto the naive bottom-up arser.
Evidently the benefits ofhaving a smaller chart to search outweighed the costs ofthe additional mechanism, even on the basis of time peredge.In summary, our first attempt to produce abottom-upparser was nine times slower than our baseline system;our last attempt was twice as fast.
Thus we achieveda speed up of a factor of 18 over the course of theseexperiments.
We finished not only with a parser thatproduced the additional possible phrases that we wantedfor robust interpretation, but did so much faster than theparser we started with.
Furthermore, we have developedwhat seems to be an important new parsing method forgrammars that allow gaps, and perhaps more generallyfor grammars with a set of categories that can be dividedinto those constrainted mainly internally and those withimportant external constraints.Report AFCRL-65-758, Air Force Cambridge Re-search Laboratory, Bedford, Massachusetts (1965).\[4\] D. H. Younger, "Recognition and Parsing ofContext-Free Languages in Time nS, " Informationand Confrol Vol.
10, No.
2, pp.
189-208 (1967).\[5\] L. Karttunnen, "D-PATR: A Development Envi-ronment for Unification-Based Grammars," Pro-ceedings of the l l th  International Conference onComputational Linguistics, Bonn, West Germany,pp.
74-80 (1986).AcknowledgmentsThis research was supported by the Defense AdvancedResearch Projects Agency under Contract N00014-90-C-0085 with the Office of Naval Research.References\[1\] E. Jackson, D. Appelt, J.
Bear, R. Moore, andA.
Podlozny, A Template Mafcher for Robust NLInterprefa~ioa, Proceedings, Fourth DARPA Work-shop on Speech and Natural Language (February1991).\[2\] R. Moore, D. Appelt, J.
Bear, M. DMrymple, andD.
Moran, SRI's Experience with the ATIS Eval-uation, Proceedings, DARPA Speech and NaturalLanguage Workshop (June 1990).\[3\] T. Kasami, "An Efficient Recognition and SyntaxAlgorithm for Context-Free Languages," Scientific203
