Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 152?162,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsLearning Latent Word Representations for Domain Adaptationusing Supervised Word ClusteringMin Xiao and Feipeng Zhao and Yuhong GuoDepartment of Computer and Information SciencesTemple UniversityPhiladelphia, PA 19122, USA{minxiao,feipeng.zhao,yuhong}@temple.eduAbstractDomain adaptation has been popularly stud-ied on exploiting labeled information from asource domain to learn a prediction model ina target domain.
In this paper, we develop anovel representation learning approach to ad-dress domain adaptation for text classificationwith automatically induced discriminative la-tent features, which are generalizable acrossdomains while informative to the predictiontask.
Specifically, we propose a hierarchicalmultinomial Naive Bayes model with latentvariables to conduct supervised word cluster-ing on labeled documents from both sourceand target domains, and then use the producedcluster distribution of each word as its la-tent feature representation for domain adapta-tion.
We train this latent graphical model us-ing a simple expectation-maximization (EM)algorithm.
We empirically evaluate the pro-posed method with both cross-domain doc-ument categorization tasks on Reuters-21578dataset and cross-domain sentiment classifica-tion tasks on Amazon product review dataset.The experimental results demonstrate that ourproposed approach achieves superior perfor-mance compared with alternative methods.1 IntroductionSupervised prediction models typically require alarge amount of labeled data for training.
However,manually collecting data annotations is expensive inmany real-world applications such as document cat-egorization or sentiment classification.
Recently, do-main adaptation has been proposed to exploit exist-ing labeled data in a related source domain to assistthe prediction model training in the target domain(Ben-David et al 2006; Blitzer et al 2006; Daume?III, 2007; Blitzer et al 2011; Chen et al 2012).
Asan effective tool to reduce annotation effort, domainadaptation has achieved success in various cross-domain natural language processing (NLP) systemssuch as document categorization (Dai et al 2007),sentiment classification (Blitzer et al 2007; Chenet al 2012; Mejova and Srinivasan, 2012; Chenet al 2011), email spam detection (Jiang and Zhai,2007), and a number of other NLP tasks (Blitzeret al 2011; Daume?
III, 2007).One primary challenge of domain adaptation liesin the distribution divergence of the two domainsin the original feature representation space.
For ex-ample, documents about books may contain verydifferent high-frequency words and discriminativewords from documents about kitchen.
A good cross-domain feature representation thus has been viewedas critical for bridging the domain divergence gapand facilitating domain adaptation in the NLP area(Ben-David et al 2006, 2010).
Many domain adap-tation works have been proposed to learn newcross-domain feature representations (Blitzer et al2006, 2011).
Though demonstrated good perfor-mance on certain problems, these works mostly in-duce new feature representations in an unsupervisedway, without taking the valuable label informationinto account.In this work, we present a novel supervised rep-resentation learning approach to discover a latentrepresentation of words which is not only general-izable across domains but also informative to theclassification task.
Specifically, we propose a hier-152archical multinomial Naive Bayes model with la-tent word cluster variables to perform supervisedword clustering on labeled documents from both do-mains.
Our model directly models the relationshipsbetween the observed document label variables andthe latent word cluster variables.
The induced clus-ter representation of each word thus will be infor-mative for the classification labels, and hence dis-criminative for the target classification task.
We trainthis directed graphical model using an expectation-maximization (EM) algorithm, which maximizes thelog-likelihood of the observations of labeled docu-ments.
The induced cluster distribution of each wordcan then be used as its generalizable representa-tion to construct new cluster-based representation ofeach document.
For domain adaptation, we train asupervised learning system with labeled data fromboth domains in the new representation space andapply it to categorize test documents in the target do-main.
In order to evaluate the proposed technique,we conduct extensive experiments on the Reuters-21578 dataset for cross-domain document catego-rization and on Amazon product review dataset forcross-domain sentiment classification.
The experi-mental results show the proposed approach can pro-duce more effective representations than the com-parison domain adaptation methods.2 Related WorkDomain adaptation has recently been popularlystudied in natural language processing and a varietyof domain adaptation approaches have been devel-oped, including instance weighting adaptation meth-ods and feature representation learning methods.Instance weighting adaptation methods improvethe transferability of a prediction model by trainingan instance weighted learning system.
Much work inthis category has been developed to address differ-ent weighting schemas (Sugiyama et al 2007; Wanet al 2011).
Jiang and Zhai (2007) applied instanceweighting algorithms to tackle cross-domain NLPtasks and proposed to remove misleading sourcetraining data and assign less weights to labeled datafrom the source domain than labeled data from thetarget domain.
Dai et al(2007) proposed to increasethe weights of mistakenly predicted instances fromthe target domain and decrease the weights of incor-rectly predicted instances from the source domainduring an iterative training process.Representation learning methods bridge do-main divergence either by differentiating domain-invariant features from domain-specific features(Daume?
III, 2007; Daume?
III et al 2010; Blitzeret al 2011; Finkel and Manning, 2009) or seekinggeneralizable latent features across domains (Blitzeret al 2006, 2007; Prettenhofer and Stein, 2010).Daume?
III (2007); Daume?
III et al(2010) proposeda simple heuristic feature replication method to rep-resent common, source specific and target specificfeatures.
Finkel and Manning (2009) proposed a for-mer version of it based on the use of a hierarchi-cal Bayesian prior.
Blitzer et al(2011) proposeda coupled subspace learning method, which learnstwo projectors, one for each domain, to project theoriginal features into domain-sharing and domain-specific features.
Blitzer et al(2006) proposed astructural correspondence learning (SCL) method tomodel the correlation between pivot features andnon-pivot features.
It uses the correlation to in-duce latent domain-invariant features as augment-ing features for supervised learning.
Extensions ofthis work include improving pivot feature selection(Blitzer et al 2007; Prettenhofer and Stein, 2010),and improving the correlation modeling betweenpivot and non-pivot features (Tan, 2009).The proposed approach in this paper belongs torepresentation learning methods.
However, unlikethe unsupervised representation learning methodsreviewed above, our proposed approach learns gen-eralizable feature representations of words by ex-ploiting data labels from the two domains.3 Learning Latent Word Representationsusing Supervised Word ClusteringIn this paper, we address domain adaptation fortext classification.
Given a source domain DS withplenty of labeled documents, and a target domainDT with a very few labeled documents, the task isto learn a classifier from the labeled documents inboth domains, and use it to classify the unlabeleddocuments in the target domain.
The documents inthe two domains share the same universal vocabu-lary V = {w1, w2, ?
?
?
, wn}, but the word distri-butions in the two domains are typically different.153Therefore, training the classification model directlyfrom the original word feature space V may not gen-eralize well in the target domain.We propose to address this problem by first learn-ing a supervised mapping function ?
: V ??
Zfrom the labeled documents in both domains, whichmaps the input word features in the large vocabu-lary set V into a low dimensional latent feature spaceZ .
By filtering out unimportant details and noises,we expect the low dimensional mapping can cap-ture the intrinsic structure of the input data that isdiscriminative for the classification task and gener-alizable across domains.
In particular, we learn sucha mapping function by conducting supervised wordclustering on the labeled documents using a hierar-chical multinomial Naive Bayes model.
Below, wewill first introduce this supervised word clusteringmodel and then use the mapping function producedto transform documents in different domains into thesame low-dimensional space for training cross do-main text classification systems.3.1 Supervised Word ClusteringGiven all labeled documents from the source andtarget domains, D = {(wt, yt)}Tt=1, where the t-thlabeled document is expressed as a bag of words,wt = {wt1, wt2, ?
?
?
, wtNt}, and its label value isyt ?
Y for Y = {1, ?
?
?
,K}, we propose to per-form supervised word clustering by modeling thedocument-label pair distribution using a hierarchicalmultinomial Naive Bayes model given in Figure 1,which has a middle layer of latent cluster variables.In this plate model, the variable Yt denotes theobserved class label for the t-th document, and allthe label variables, {Yt}Tt=1, share the same multi-nomial distribution ?Y across documents.
The la-tent variable Ct,i denotes the cluster membershipof the word Wt,i, and all the cluster variables,{Ct,i}T,Ntt=1,i=1, share the same set of conditional dis-tributions {?C|y}Ky=1 across documents and words.The variable Wt,i denotes the i-th observed wordin the t-th document, and all the word variables,{Wt,i}T,Ntt=1,i=1, share the same set of conditional dis-tributions {?W |c}mc=1.
Here we assume the numberof word clusters is m. For simplicity, we do not showthe distribution parameter variables in the Figure.Following the Markov property of directed graph-Figure 1: Supervised word clustering model.ical models, we can see that given the cluster vari-able values, the document label variables will becompletely independent of the word variables.
Bylearning this latent directed graphical model, wethus expect the important classification informationexpressed in the input observation words can beeffectively summarized into the latent cluster vari-ables.
This latent model is much simpler than thesupervised topic models (Blei and McAuliffe, 2007),but we will show later that it can suitably produce ageneralizable feature mapping function for domainadaptation.To train the latent graphical model in Fig-ure 1 on labeled documents D, we use a standardexpectation-maximization (EM) algorithm (Demp-ster et al 1977) to maximize the marginal log-likelihood of the observations:LL(D;?)
=?tlogP (yt,wt|?)
(1)The EM algorithm is an iterative procedure.
In eachiteration, it takes an alternative E-step and M-stepto maximize the lower bound of the marginal log-likelihood function.
In our experiments, we startfrom a random initialization of the model parame-ters and the latent variable values, and then performiterative EM updates until converge to a local opti-mal solution.3.2 Induced Word RepresentationAfter training the supervised clustering model usingEM algorithm, a set of local optimal model parame-ters ??
will be returned, which define a joint distri-bution over the three groups of variables in the di-rected graphical model.
Next we define a supervisedlatent feature mapping function ?
from this trained154model to map each word w in the vocabulary V intoa conditional distribution vector over the word clus-ter variable, such as?
(w)=[P (c=1|w,??
), ?
?
?
, P (c=m|w,??)].
(2)The conditional distributions involved in this map-ping function can be computed asP (c|w,??
)=?y?YP (w|c,??
)P (c|y,??
)P (y|??
)P (w)(3)where P (w|c,??)
= ?
?w|c P (c|y,??)
= ?
?c|y andP (y|??)
= ?
?y can be determined from the modelparameters directly, and p(w) can be computed asthe empirical frequency of word w among all theother words in all the training documents.We then define a transformation matrix ?
?Rn?m based on the mapping function ?
defined inEq.
(2), such that ?i: = ?
(wi) where wi is the i-thword in the vocabulary V .
That is, each row of ?is the induced representation vector for one word.
?can be viewed as a soft word clustering matrix, and?i,j denotes the probability of word wi belongs tothe j-th cluster.
Given the original document-wordfrequency matrix Xtr ?
RT?n for the labeled train-ing documents from the two domains, we can con-struct its representations Ztr ?
RT?m in the pre-dictive latent clustering space by performing the fol-lowing transformation:Ztr = Xtr?.
(4)Similarly, we can construct the new representationmatrix Zts for the test data Xts in the target domain.We then train a classification model on the labeleddata Ztr and apply it to classify the test data Zts.4 ExperimentsWe evaluate the proposed approach with experi-ments on cross domain document categorization ofReuters data and cross domain sentiment classifi-cation of Amazon product reviews, comparing to anumber of baseline and existing domain adaptationmethods.
In this section, we report the experimentalsetting and results on these two data sets.4.1 ApproachesWe compared our proposed supervised word cluster-ing approach (SWC) with the following five compar-ison methods for domain adaptation:(1) BOW: This is a bag-of-word baseline method,which trains a SVM classifier with labeled datafrom both domains using the original bag-of-word features.
(2) PLSA: This is an unsupervised word clusteringmethod, which first applies the probabilistic la-tent semantic analysis (PLSA) (Hofmann, 1999)to obtain word clusterings with both labeled andunlabeled data from the two domains and thenuses the soft word clusterings as augmentingfeatures to train SVM classifiers.
(3) FDLDA: This is an alternative supervised wordclustering method we built by training theFast-Discriminative Latent Dirichlet Allocationmodel (Shan et al 2009) with all labeled datafrom the two domains.
After training the model,we used the learned topic distribution p(z) andthe conditional word distributions p(w|z) tocompute the conditional distribution over topicsp(z|w) for each word as the soft clustering of theword.
We then used the soft word clusterings asaugmenting features to train SVM classifiers.
(4) SCL: This is the structural correspondencelearning based domain adaptation method(Blitzer et al 2006).
It first induces generaliz-able features with all data from both domainsby modeling the correlations between pivot fea-tures and non-pivot features, and then uses theproduced generalizable features as augmentingfeatures to train SVM classifiers.
(5) CPSP: This is coupled subspace learning baseddomain adaptation method (Blitzer et al 2011).It first learns two domain projectors using alldata from the two domains by approximatingmulti-view dimensionality reduction, and thenprojects the labeled data to low dimensional la-tent feature space to train SVM Classifiers.We used the LIBSVM package (Chang and Lin,2011) with its default parameter setting to train lin-ear SVM classifiers as the base classification modelfor all comparison methods.155Table 1: Average results (accuracy?standard deviation) for three cross-domain document categorization tasks onReuters-21578 dataset.Task BOW PLSA FDLDA SCL CPSP SWCOrgs vs People 76.07?0.39 76.50?0.10 76.95?0.23 78.71?0.20 77.58?0.21 81.27?0.23Orgs vs Places 73.88?0.58 74.68?0.20 74.87?0.29 76.71?0.23 75.76?0.28 78.33?0.64People vs Places 61.80?0.44 63.36?0.40 63.46?0.40 64.65?0.40 62.73?0.53 67.48?0.204.2 Experiments on Reuters Data SetWe used the popularly studied Reuters-21578dataset (Dai et al 2007), which contains three cross-domain document categorization tasks, Orgs vs Peo-ple, Orgs vs Places, People vs Places.
The sourceand target domains of each task contain documentssampled from different non-overlapping subcate-gories.
From example, the task of Orgs vs Peopleassigns a document into one of the two top cate-gories (Orgs, People), and the source domain doc-uments and the target domain documents are sam-pled from different subcategories of Orgs and Peo-ple.
There are 1237 source documents and 1208 tar-get documents for the task of Orgs vs People, 1016source documents and 1043 target documents for thetask of Orgs vs Places, and 1077 source documentsand 1077 target documents for the task ofPeople vsPlaces.
For each task, we built a unigram vocabularybased on all the documents from the two domainsand represented each document as a feature vectorcontaining term frequency values.4.2.1 Experimental Results for Cross-DomainDocument CategorizationFor each of the three cross-domain document cat-egorization tasks on Reuters-21578 dataset, we usedall the source documents as labeled training datawhile randomly selecting 100 target documents aslabeled training data and setting the rest as unla-beled test data.
For the BOW baseline method, weused the term-frequency features.
The other five ap-proaches are based on representation learning, andwe selected the dimension size of the representationlearning, i.e., the cluster number in our proposed ap-proach, from {5, 10, 20, 50, 100} according to theaverage classification results over 3 runs on the taskof Orgs vs People.
The dimension sizes of the in-duced representations for the five approaches, PLSA,FDLDA, SCL, CPSP and SWC are 20, 20, 100, 100and 20 respectively.We then repeated each experiment 10 times oneach task with different random selections of the 100labeled target documents to compare the six compar-ison approaches.
The average classification resultsin terms of accuracy and standard deviations are re-ported in Table 1.
We can see that by simply combin-ing labeled documents from the two domains with-out adaptation, the BOW method performs poorlyacross the three tasks.
The PLSA method outper-forms the BOW method over all the three tasks withsmall improvements.
The supervised word cluster-ing method FDLDA, though performing slightly bet-ter than the unsupervised clustering method PLSA,produces poor performance comparing to the pro-posed SWC method.
One possible reason is thatthe FDLDA model is not specialized for supervisedword clustering, and it uses a logistic regressionmodel to predict the labels from the word topics,while the final soft word clustering is computed fromthe learned distribution p(z) and p(w|z).
That is,in the FDLDA model the labels only influence theword clusterings indirectly and hence its influencecan be much smaller than the influence of labels asdirect parent variables of the word cluster variablesin the SWC model.
The two domain adaptation ap-proaches, SCL and CPSP, both produce significantimprovements over BOW, PLSA and FDLDA on thetwo tasks of Orgs vs People and Orgs vs Places,while the CPSP method produces slightly inferiorperformance than PLSA and FDLDA on the task ofPeople vs Places.
The proposed method SWC onthe other hand consistently and significantly outper-forms all the other comparison methods across allthe three tasks.We also studied the sensitivity of the proposedapproach with respect to the number of clusters,15620 40 60 80 100606570758085Reuters?21578Number of ClusterAccuracyOrgs vs PeopleOrgs vs PlacesPeople vs PlacesFigure 2: Sensitivity analysis of the proposed approachw.r.t.
the number of clusters for the three cross-domaindocument categorization tasks on Reuters-21578 dataset.i.e., the dimension size of the learned representa-tion.
We experimented with a set of different val-ues m ?
{5, 10, 20, 50, 100} as the number of clus-ters.
For each m value, we used the same experimen-tal setting as above and repeated the experiments 10times to obtain the average comparison results.
Theclassification accuracy results on the three tasks arereported in Figure 2.
We can see that the proposedmethod is not very sensitive to the number of clus-ters, across the set of increasing values we consid-ered, and its performance becomes very stable afterthe cluster number reaches 20.4.2.2 Document Categorization Accuracy vsLabel Complexity in Target DomainWe next conducted experiments to compare thesix approaches by varying the amount of the labeleddata from the target domain.
We tested a set of dif-ferent values, s ?
{100, 200, 300, 400, 500}, as thenumber of labeled documents from the target do-main.
For each different s value, we repeated the ex-periments 10 times by randomly selecting s labeleddocuments from the target domain using the sameexperimental setting as before.
The comparison re-sults across the set of s values are plotted in Fig-ure 3.
We can see that in general the performance ofeach method improves with the increase of the num-ber of labeled documents from the target domain.The proposed method SWC and the domain adapta-tion method SCL clearly outperform the other fourmethods.
Moreover, the proposed method SWC notonly maintains consistent and significant advantagesover all other methods across the range of differ-ent s values, its performance with 300 labeled tar-get instances is even superior to the other methodswith 500 labeled target instances.
All these resultssuggest the proposed approach is very effective foradapting data across domains.4.3 Experiments on Amazon Product ReviewsWe conducted cross-domain sentiment classificationon the widely used Amazon product reviews (Blitzeret al 2007), which contains review documents dis-tributed in four categories: Books(B), DVD(D), Elec-tronics(E) and Kitchen(K).
Each category contains1000 positive and 1000 negative reviews.
We con-structed 12 cross-domain sentiment classificationtasks, one for each source-target domain pair, B2D,B2E, B2K, D2B, D2E, D2K, E2B, E2D, E2K, K2B,K2D, K2E.
For example, the task B2D means thatwe use the Books reviews as the source domain andthe DVD reviews as the target domain.
For each pairof domains, we built a vocabulary with both uni-gram and bigram features extracted from all the doc-uments of the two domains, and then representedeach review document as a feature vector with termfrequency values.4.3.1 Experimental Results for Cross-DomainSentiment ClassificationFor each of the twelve cross-domain sentimentclassification tasks on Amazon product reviews, weused all the source reviews as labeled data and ran-domly selected 100 target reviews as labeled datawhile treating the rest as unlabeled test data.
For thebaseline method BOW, we used binary indicator val-ues as features, which has been shown to work betterthan the term-frequency features for sentiment clas-sification tasks (Pang et al 2002; Na et al 2004).For all the other representation learning based meth-ods, we selected the dimension size of learned repre-sentation according to the average results over 3 runson the B2D task.
The dimension sizes selected forthe methods PLSA, FDLDA, SCL, CPSP, and SWCare 10, 50, 50, 100 and 10, respectively.1150 and 100 are also the suggested values for SCL (Blitzeret al 2007) and CPSP (Blitzer et al 2011) respectively on thiscross-domain sentiment classification dataset.157100 200 300 400 50074767880828486Orgs vs People#Labeled instancesAccuracyBOWPLSAFDLDASCLCPSPSWC100 200 300 400 500727476788082Orgs vs Places#Labeled instancesAccuracyBOWPLSAFDLDASCLCPSPSWC100 200 300 400 5006062646668707274People vs Places#Labeled instancesAccuracyBOWPLSAFDLDASCLCPSPSWCFigure 3: Average classification results for three cross-domain document categorization tasks on Reuters-21578 datasetby varying the amount of labeled training data from the target domain.Table 2: Average results (accuracy?standard deviation) for twelve cross-domain sentiment classification tasks onAmazon product reviews.Task BOW PLSA FDLDA SCL CPSP SWCB2D 76.58?0.14 76.01?0.10 75.95?0.16 80.17?0.16 77.53?0.14 81.66?0.23B2K 75.48?0.34 74.68?0.20 74.87?0.15 78.13?0.21 76.38?0.15 82.26?0.20B2E 72.92?0.37 73.36?0.19 73.46?0.21 74.79?0.19 73.31?0.17 77.04?0.64D2B 74.10?0.29 74.04?0.20 74.08?0.18 78.73?0.23 77.07?0.15 79.95?0.25D2K 75.19?0.33 75.37?0.31 75.44?0.31 76.98?0.19 76.77?0.10 82.13?0.20D2E 73.01?0.34 74.21?0.30 74.09?0.31 75.69?0.25 73.83?0.21 76.98?0.54E2B 67.58?0.24 68.48?0.15 68.44?0.17 70.21?0.16 70.47?0.16 72.11?0.46E2D 70.15?0.27 70.16?0.23 70.06?0.22 72.83?0.25 71.76?0.20 73.81?0.59E2K 82.23?0.12 82.24?0.18 82.26?0.19 84.69?0.11 81.31?0.14 85.33?0.16K2B 70.67?0.18 72.18?0.21 72.18?0.16 73.91?0.21 72.18?0.19 75.78?0.55K2D 71.51?0.26 72.00?0.18 72.05?0.19 74.82?0.26 72.59?0.18 76.88?0.49K2E 80.81?0.12 80.39?0.18 80.46?0.18 82.96?0.11 80.81?0.14 84.78?0.19We then repeated each experiment 10 times basedon different random selections of 100 labeled re-views from the target domain to compare the sixmethods on the twelve tasks.
The average classifica-tion results are reported in Table 2.
We can see thatthe PLSA and FDLDA methods do not show muchadvantage over the baseline method BOW.
CPSPperforms better than PLSA and BOW on many ofthe twelve tasks, but with small advantages, whileSCL outperforms CPSP on most tasks.
The proposedmethod SWC however demonstrates a clear advan-tage over all the other methods and produces the bestresults on all the twelve tasks.We also conducted sensitivity analysis over theproposed approach regarding the number of clus-ters on the twelve cross-domain sentiment classifi-cation tasks, by testing a set of cluster number val-ues m = {5, 10, 20, 50, 100}.
The average resultsare plotted in Figure 5.
Similar as before, we cansee the proposed approach has stable performanceacross the set of different cluster numbers.
More-over, these results also clearly show that domainadaptation is not a symmetric process, as we can seeit is easier to conduct domain adaptation from thesource domain Books to the target domain Kitchen(with an accuracy around 82%), but it is more diffi-cult to make domain adaptation from the source do-main Kitchen to the target domain Books (with an ac-158100 200 300 400 5007476788082B2D#Labeled instancesAccuracyBOWPLSAFDLDASCLCPSPSWC100 200 300 400 500707274767880B2E#Labeled instancesAccuracyBOWPLSAFDLDASCLCPSPSWC100 200 300 400 5007476788082B2K#Labeled instancesAccuracyBOWPLSAFDLDASCLCPSPSWC100 200 300 400 5007274767880D2B#Labeled instancesAccuracyBOWPLSAFDLDASCLCPSPSWC100 200 300 400 5007274767880D2E#Labeled instancesAccuracyBOWPLSAFDLDASCLCPSPSWC100 200 300 400 5007476788082D2K#Labeled instancesAccuracyBOWPLSAFDLDASCLCPSPSWC100 200 300 400 5006668707274E2B#Labeled instancesAccuracyBOWPLSAFDLDASCLCPSPSWC100 200 300 400 500687072747678E2D#Labeled instancesAccuracyBOWPLSAFDLDASCLCPSPSWC100 200 300 400 50076788082848688E2K#Labeled instancesAccuracyBOWPLSAFDLDASCLCPSPSWC100 200 300 400 5006870727476K2B#Labeled instancesAccuracyBOWPLSAFDLDASCLCPSPSWC100 200 300 400 5007072747678K2D#Labeled instancesAccuracyBOWPLSAFDLDASCLCPSPSWC100 200 300 400 500767880828486K2E#Labeled instancesAccuracyBOWPLSAFDLDASCLCPSPSWCFigure 4: Average results (accuracy?standard deviation) for the 12 cross-domain sentiment classification tasks onAmazon product reviews with different numbers of labeled training data from the target domain.15920 40 60 80 10072747678808284BooksNumber of clusterAccuracyDVDKitchenElectronics20 40 60 80 10072747678808284DVDNumber of clusterAccuracyBooksKitchenElectronics20 40 60 80 10070758085ElectronicsNumber of clusterAccuracyBooksDVDKitchen20 40 60 80 1007274767880828486KitchenNumber of clusterAccuracyBooksDVDElectronicsFigure 5: Sensitivity analysis of the proposed approach wrt the number of clusters for the twelve cross-domain senti-ment classification tasks.
Each figure shows experimental results for three tasks with the same source domain.curacy around 75%).
It also shows that the degree ofrelatedness of the two domains is an important factorfor the effectiveness of knowledge adaptation.
Forexample, one can see that it is much easier to con-duct domain adaptation from Kitchen to Electronics(with an accuracy around 84%) than from Kitchen toBooks (with an accuracy around 75%), as Kitchen ismore closely related to Electronics than Books.4.3.2 Sentiment Classification Accuracy vsLabel Complexity in Target DomainSimilar as before, we tested the proposed ap-proach using a set of different values s ?
{100, 200, 300, 400, 500} as the number of labeledreviews from the target domain.
For each given svalue, we conducted the comparison experiments us-ing the same setting above.
The average results arereported in Figure 4.
We can see that the perfor-mance of each approach in general improves withthe increase of the number of labeled reviews fromthe target domain.
The proposed approach maintainsa clear advantage over all the other methods on allthe twelve tasks across different label complexities.All those empirical results demonstrate the effec-tiveness of the proposed approach for cross-domainsentiment classification.4.3.3 Illustration of the Word ClustersFinally, we would also like to demonstrate thehard word clusters produced by the proposed su-pervised word clustering method.
We assign a wordinto the cluster it most likely belongs to accordingto its soft clustering representation, such as c?
=argmaxc P (c|w,??).
Table 3 presents the top repre-sentative words (i.e., the most frequent words) of the10 word clusters produced on the task of B2K.
Wecan see that the first three clusters (C1, C2, and C3)contain words with positive sentiment polarity indifferent degrees.
The two clusters (C4 and C5) con-tain words used to express the degree of opinions.The next four clusters (C6, C7, C8, and C9) containcontent words related to Books or Kitchen.
The lastcluster (C10) contains words of negative sentimentpolarity.
These results demonstrate that the proposedsupervised word clustering can produce task mean-ingful word clusters and hence label-informative la-tent features, which justifies its effectiveness.5 ConclusionIn this paper, we proposed a novel supervised rep-resentation learning method to tackle domain adap-tation by inducing predictive latent features basedon supervised word clustering.
With the soft wordclustering produced, we can transform all docu-ments from the two domains into a unified low-dimensional feature space for effective training ofcross-domain NLP prediction system.
We conductedextensive experiments on cross-domain documentcategorization tasks on Reuters-21578 dataset andcross-domain sentiment classification tasks on Ama-zon product reviews.
Our empirical results demon-strated the efficacy of the proposed approach.ReferencesS.
Ben-David, J. Blitzer, K. Crammer, and F. Pereira.Analysis of representations for domain adapta-160Table 3: Clustering illustration for the task of B2K on Amazon product reviews.C1 recommend excellent wonderful beautiful love powerful happy satisfied outstandingC2 enjoyed fantastic glad i liked nicely was great benefits pleasure amazinglyC3 good and made me most people ordered this standards accurately check outC4 was a kind of basically is only half of first of as if and still anything about have someC5 ever may still going maybe either at least of all totally sort of are veryC6 life work machine size design bottom business picture hand hook gas sink turner shelvesC7 way coffee pan keep cooking maker heat job working children handle meet core wineC8 people us world come fact man place stars during example went short bathroom apple priceC9 pot friends daily light fire tells knew holds keep the continued meal hooked silver windC10 disappointed waste unfortunately worse poorly sorry weak not worth stupid fails awful uselesstion.
In Advances in Neural Information Process-ing Systems (NIPS), 2006.S.
Ben-David, J. Blitzer, K. Crammer, A. Kulesza,F.
Pereira, and J. Vaughan.
A theory of learningfrom different domains.
Machine Learng, 79(1-2):151?175, 2010.D.
Blei and J. McAuliffe.
Supervised topic mod-els.
In Advances in Neural Information Process-ing Systems (NIPS), 2007.J.
Blitzer, R. McDonald, and F. Pereira.
Domainadaptation with structural correspondence learn-ing.
In Proc.
of the Conference on Empir-ical Methods in Natural Language Processing(EMNLP), 2006.J.
Blitzer, M. Dredze, and F. Pereira.
Biographies,bollywood, boom-boxes and blenders: Domainadaptation for sentiment classification.
In Proc.of the Annual Meeting of the Association for Com-putational Linguistics (ACL), 2007.J.
Blitzer, D. Foster, and S. Kakade.
Domain adapta-tion with coupled subspaces.
In Proc.
of the Inter-national Conference on Artificial Intelligence andStatistics (AISTATS), 2011.C.
Chang and C. Lin.
LIBSVM: A library for sup-port vector machines.
ACM Transactions on In-telligent Systems and Technology, 2:27:1?27:27,2011.M.
Chen, K. Weinberger, and J. Blitzer.
Co-trainingfor domain adaptation.
In Advances in Neural In-form.
Process.
Systems (NIPS), 2011.M.
Chen, Z. Xu, K. Weinberger, and F. Sha.Marginalized denoising autoencoders for domainadaptation.
In Proc.
of the International Conf.
onMachine Learning (ICML), 2012.W.
Dai, Q. Yang, G. Xue, and Y. Yu.
Boosting fortransfer learning.
In Proc.
of the InternationalConf.
on Machine Learning (ICML), 2007.H.
Daume?
III.
Frustratingly easy domain adaptation.In Proc.
of the Annual Meeting of the Associationfor Comput.
Linguistics (ACL), 2007.H.
Daume?
III, A. Kumar, and A. Saha.
Co-regularization based semi-supervised domainadaptation.
In Advances in Neural InformationProcessing Systems (NIPS), 2010.A.
Dempster, N. Laird, and D. Rubin.
Maximumlikelihood from incomplete data via the em algo-rithm.
Journal of the royal statistical society, 39(1):1?38, 1977.J.
Finkel and C. Manning.
Hierarchical bayesiandomain adaptation.
In Proc.
of the Conferenceof the North American Chapter of the Associationfor Computational Linguistics (NAACL), 2009.T.
Hofmann.
Probabilistic latent semantic analysis.In Proc.
of the Conference on Uncertainty in Ar-tificial Intelligence (UAI), 1999.J.
Jiang and C. Zhai.
Instance weighting for domainadaptation in nlp.
In Proc.
of the Annual Meetingof the Association for Computational Linguistics(ACL), 2007.Y.
Mejova and P. Srinivasan.
Crossing mediastreams with sentiment: Domain adaptation in161blogs, reviews and twitter.
In Proc.
of the Inter-national AAAI Conference on Weblogs and SocialMedia (ICWSM), 2012.J.
Na, H. Sui, C. Khoo, S. Chan, and Y. Zhou.
Effec-tiveness of simple linguistic processing in auto-matic sentiment classification of product reviews.In Proc.
of the Conf.
of the Inter.
Society forKnowledge Organization, 2004.B.
Pang, L. Lee, and S. Vaithyanathan.
Thumbsup?
: sentiment classification using machine learn-ing techniques.
In Proc.
of the Conference on Em-pirical Methods in Natural Language Processing(EMNLP), 2002.P.
Prettenhofer and B. Stein.
Cross-languagetext classification using structural correspondencelearning.
In Proc.
of the Annual Meeting of theAssociation for Comput.
Linguistics (ACL), 2010.H.
Shan, A. Banerjee, and N. Oza.
Discriminativemixed-membership models.
In Proc.
of the IEEEInter.
Conference on Data Mining (ICDM), 2009.M.
Sugiyama, S. Nakajima, H. Kashima, P. vonBu?nau, and M. Kawanabe.
Direct importance es-timation with model selection and its applicationto covariate shift adaptation.
In Advances in Neu-ral Information Processing Systems (NIPS), 2007.S.
Tan.
Improving scl model for sentiment-transferlearning.
In Proc.
of the Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (NAACL), 2009.C.
Wan, R. Pan, and J. Li.
Bi-weighting domainadaptation for cross-language text classification.In Proc.
of the International Joint Conference onArtificial Intelligence (IJCAI), 2011.162
