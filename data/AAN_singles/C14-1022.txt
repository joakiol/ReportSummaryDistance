Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 213?223, Dublin, Ireland, August 23-29 2014.Sarcasm Detection on Czech and English TwitterTom?a?s Pt?a?cek?
?, Ivan Habernal?and Jun Hong?
?Department of Computer Science and Engineering, Faculty of Applied Sciences,University of West Bohemia, Univerzitn??
8, 306 14 Plze?n, Czech Republictigi@kiv.zcu.cz habernal@kiv.zcu.cz?School of Electronics, Electrical Engineering and Computer Science,Queen?s University Belfast, Belfast BT7 1NN, UKj.hong@qub.ac.ukAbstractThis paper presents a machine learning approach to sarcasm detection on Twitter in two lan-guages ?
English and Czech.
Although there has been some research in sarcasm detection inlanguages other than English (e.g., Dutch, Italian, and Brazilian Portuguese), our work is thefirst attempt at sarcasm detection in the Czech language.
We created a large Czech Twitter cor-pus consisting of 7,000 manually-labeled tweets and provide it to the community.
We evaluatetwo classifiers with various combinations of features on both the Czech and English datasets.Furthermore, we tackle the issues of rich Czech morphology by examining different preprocess-ing techniques.
Experiments show that our language-independent approach significantly outper-forms adapted state-of-the-art methods in English (F-measure 0.947) and also represents a strongbaseline for further research in Czech (F-measure 0.582).1 IntroductionSentiment analysis on social media has been one of the most targeted research topics in NLP in the pastdecade, as shown in several recent surveys (Liu and Zhang, 2012; Tsytsarau and Palpanas, 2012).
Sincethe goal of sentiment analysis is to automatically detect the polarity of a document, misinterpreting ironyand sarcasm represents a big challenge (Davidov et al., 2010).As there is only a weak boundary in meaning between irony, sarcasm and satire (Reyes et al., 2012),we will use only the term sarcasm in this paper.
Bosco et al.
(2013) claim that ?even if there is no agree-ment on a formal definition of irony, psychological experiments have delivered evidence that humans canreliably identify ironic text utterances from an early age in life.?
We have thus decided to rely on the abil-ity of our human annotators to manually label sarcastic tweets to train our classifiers.
Sarcasm generallyreverses the polarity of an utterance from positive or negative into its opposite, which deteriorates theresults of a given NLP task.
Therefore, correct identification of sarcasm can improve the performance.The issue of automatic sarcasm detection has been addressed mostly in English, although there hasbeen some research in other languages, such as Dutch (Liebrecht et al., 2013), Italian (Bosco et al.,2013), or Brazilian Portuguese (Vanin et al., 2013).
To the best of our knowledge, no research has beenconducted in Czech or other Slavic languages.
These languages are challenging for many NLP tasksbecause of their rich morphology and syntax.
This has motivated us to focus our current research on bothEnglish and Czech.Majority of the existing state-of-the-art techniques are language dependent, which rely on language-specific lexical resources.
Since no such resources are available for Czech, we adapt some language-independent methods and also apply various preprocessing steps for sentiment analysis proposed byHabernal et al.
(2013).This paper focuses on document-level sarcasm detection on Czech and English Twitter datasets usingsupervised machine learning methods.
The Czech dataset consists of 7,000 manually labeled tweets,This work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/213the English dataset consists of a balanced distribution and an imbalanced distribution, each contain-ing 100,000 tweets, where hashtag #sarcasm was used as an indicator of sarcastic tweets.
We pro-vide both datasets under Creative Commons BY-NC-SA licence1at http://liks.fav.zcu.cz/sarcasm/.Our research questions were the following: (1) To what extent can the language-independent approachcompete with methods based on lexical language-dependent resources?
(2) Is it possible to reach goodagreement on annotating sarcasm and what typical text properties on Twitter are important for sarcasmdetection?
(3) What is the best preprocessing pipeline that can boost performance on highly-flectiveCzech language and what types of features and classifiers yield the best results?The rest of this article is organized as follows.
Section 2 describes the related work.
In section 3, weoutline our approach to sarcasm detection and describe the selection of features in our approach.
Section4 thoroughly describes the datasets and the annotation process.
Section 5 describes and discusses theexperimental results.
Finally we conclude in Section 6.2 Related WorkExperiments with semi-supervised sarcasm identification on a Twitter dataset (5.9 million tweets) and on66,000 product reviews from Amazon were conducted in (Davidov et al., 2010) and (Tsur et al., 2010).They used 5-fold cross validation on their kNN-like classifier and obtained an F-measure of 0.83 onthe product reviews dataset and 0.55 on the Twitter dataset.
For acquiring the Twitter dataset they usedhashtag #sarcasm as an indicator of sarcastic tweets.
They further created a balanced evaluation set of180 tweets using 15 human annotators via Amazon Mechanical Turk2and achieved an inter-annotatoragreement 0.41 (Fleiss?
?
).Gonz?alez-Ib?a?nez et al.
(2011) experimented with Twitter data divided into three categories (sarcas-tic, positive sentiment and negative sentiment), each containing 900 tweets.
They used the #sarcasmand #sarcastic hashtags to identify sarcastic tweets.
They used two classifiers ?
support vectormachine (SVM) with sequential minimal optimization (SMO) and logistic regression.
They tried var-ious combinations of unigrams, dictionary-based features and pragmatic factors (positive and negativeemoticons and user references), achieving the best result (accuracy 0.65) for sarcastic and non-sarcasticclassification with the combination of SVM with SMO and unigrams.
They employed 3 human judges toannotate 180 tweets (90 sarcastic and 90 non-sarcastic).
The human judges achieved Fleiss?
?
= 0.586,demonstrating the difficulty of sarcasm classification.
Another experiment included 50 sarcastic and 50non-sarcastic (25 positive, 25 negative) tweets with emoticons annotated by two judges.
The automaticclassification and human judges achieved the accuracy of 0.71 and 0.89 respectively.
The inter-annotatoragreement (Cohen?s ?)
was 0.74.Reyes et al.
(2012) proposed features to capture properties of a figurative language such as ambiguity,polarity, unexpectedness and emotional scenarios.
Their corpus consists of five categories (humor, irony,politics, technology and general), each containing 10,000 tweets.
The best result in the classification ofirony and general tweets was F-measure 0.65.In (Reyes et al., 2013) they explored the representativeness and relevance of conceptual features (sig-natures, unexpectedness, style and emotional scenarios).
These features include punctuation marks,emoticons, quotes, capitalized words, lexicon-based features, character n-grams, skip-grams, (Guthrieet al., 2006), and polarity skip-grams.
Their corpus consists of four categories (irony, humor, educationand politics), each containing 10,000 tweets.
Their evaluation was performed on two distributional sce-narios, balanced distribution and imbalanced distribution (25% ironic tweets and 75% tweets from allthree non-ironic categories) using the Naive Bayes and decision trees algorithms from the Weka toolkit(Witten and Frank, 2005).
The classification by the decision trees achieved an F-measure of 0.72 on thebalanced distribution and an F-measure of 0.53 on the imbalanced distribution.The work of Riloff et al.
(2013) identifies one type of sarcasm: contrast between a positive sentimentand negative situation.
They used a bootstrapping algorithm to acquire lists of positive sentiment phrases1http://creativecommons.org/licenses/by-nc-sa/3.0/2http://www.mturk.com214and negative situation phrases from sarcastic tweets.
They proposed a method which classifies tweets assarcastic if it contains a positive predicative that precedes a negative situation phrase in close proximity.Their evaluation on a human-annotated dataset3of 3000 tweets (23% sarcastic) was done using the SVMclassifier with unigrams and bigrams as features, achieving an F-measure of 0.48.
The hybrid approachthat combines the results of the SVM classifier and their contrast method achieved an F-measure of 0.51.Sarcasm and nastiness classification in online dialogues was also explored in (Lukin and Walker, 2013)using bootstrapping, syntactic patterns and a high precision classifier.
They achieved an F-measure of0.57 on their sarcasm dataset.3 Our ApproachThis paper presents the first attempt at sarcasm detection in the Czech language, in which we focus onsupervised machine learning approaches and evaluate their performance.
We selected various n-grams,including unigrams, bigrams, trigrams with frequency greater than three (Liebrecht et al., 2013), and aset of language-independent features, including punctuation marks, emoticons, quotes, capitalized words,character n-grams and skip-grams (Reyes et al., 2013) as our baselines.3.1 ClassificationOur evaluation was performed using the Maximum Entropy (MaxEnt) and Support Vector Machine(SVM) classifiers.
We used Brainy ?
a Java framework for machine learning (Konkol, 2014) ?
withdefault settings (the linear kernel for SVM).
All experiments were conducted in the 5-fold cross vali-dation manner similar to (Davidov et al., 2010; Gonz?alez-Ib?a?nez et al., 2011).
Our motivation to testmultiple classifiers stemmed also from related works which mostly test more than one classifier.
On theother hand, the choice between state-of-the-art linear classifiers might not be much of importance, as themost important is the feature engineering.3.2 FeaturesFor our evaluation we used the most promising language-independent features from the related work andPOS related features.
Feature sets used in our evaluation are described in Table 1.Group FeaturesDescriptionN-gramCharacter n-gramWe used character n-gram features (Blamey et al., 2012).
We set the minimumoccurrence of a particular character n-gram to either 5 or 50, in order to prune thefeature space.
Our character feature set contains 3-grams to 6-grams.N-gramWe used word unigrams, bigrams and trigrams as binary features.
The feature spaceis pruned by the minimum n-gram occurrence set to 3 (Liebrecht et al., 2013).Skip-bigramInstead of using sequences of adjacent words (n-grams) we used skip-grams(Guthrie et al., 2006), which skip over arbitrary gaps.
Reyes et al.
(2013) considerskip-bigrams with 2 or 3 word skips and remove skip-grams with a frequency?
20.PatternPatternPatterns composed of high frequency words (HFWs)4and content words (CWs)5used by (Davidov et al., 2010).
Pattern must contain at least one high frequencyword.
The patterns contain 2-6 HFWs and 1-6 CWs.
We set the minimum occur-rence of a particular pattern to 5.Word-shape patternWe tried to improve pattern features by using word-shape classes for content words.We assign words into one of 24 classes6similar to the function specified in (Bikelet al., 1997).POSPOS characteristicsWe implemented various POS features, e.g., the number of nouns, verbs, and adjec-tives (Ahkter and Soria, 2010), the ratio of nouns to adjectives and verbs to adverbs(Kouloumpis et al., 2011), and number of negative verbs obtained from POS tags.3They used three annotators.
Each annotator was given the same 100 tweets with the sarcasm hashtag and 100 tweetswithout the sarcasm hashtag (the hashtags were removed).
On these tweets the pairwise inter-annotator scores were computed(Cohen?s Kappa ?1= 0.80, ?2= 0.81 and ?3= 0.82).
Then each annotator labeled additional 1000 tweets.4A word whose corpus frequency is more than 1000 words per million plus all punctuation characters.5A word whose corpus frequency is less than 1000 words per million.6We use edu.stanford.nlp.process.WordShapeClassifier with the WORDSHAPECHRIS1 setting.215POS word-shapeUnigram feature consisting of POS and word-shape (see Word-shape pattern).
Thefeature space is pruned by the minimum occurrence set to 5.POS n-gramDirect use of POS n-grams has not shown any significant improvement in sentimentanalysis but it may improve the results of sarcasm detection.
We experimented with3-grams to 6-grams with the minimum n-gram occurrence set to 5.OthersEmoticonsWe used two lists of positive and negative emoticons (Montejo-R?aez et al., 2012).The feature captures the number of occurrences of each class of emoticons withinthe text.Punctuation-basedWe adapted punctuation-based features proposed by (Davidov et al., 2010).
Thisfeature set consists of number of words, exclamation marks, question marks, quota-tion marks and capitalized words normalized by dividing them by the maximal ob-served value multiplied by the averaged maximal value of the other feature groups.PointednessPointedness was used by (Reyes et al., 2013) to distinguish irony.
It focuses onexplicit marks which should reflect a sharp distinction in the information that istransmitted.
The presence of punctuation marks, emoticons, quotes and capitalizedwords has been considered.Extended PointednessThis feature captures the number of occurrences of punctuation marks and emoti-cons as well as the number of words, exclamation marks, question marks, quotationmarks and capitalized words normalized by maximal observed value.Word-caseWe implemented various word-case features that include, e.g., the number of uppercased words, number of words with first letter capital normalized by number ofwords and number of upper cased characters normalized by number of words.Table 1: Descriptions of used feature sets.4 Evaluation DatasetsWe collected datasets using Twitter Search API and Java Language Detector7.
We collected 140,000Czech and 780,000 English tweets, respectively.
Due to lack of support for the Czech language onTwitter, we used the Twitter Search API parameter geocode to acquire tweets posted near Prague.
Forthe English dataset we also collected tweets with the #sarcasm hashtag.
Czech users generally don?tuse the sarcasm (?#sarkasmus?)
or irony (?#ironie?)
hashtag variants8thus we had to annotate theCzech dataset manually.
The final label distribution in datasets is shown in Table 4.4.1 Filtering and NormalizationAll user, URL and hashtag references in tweets have been replaced by ?user?, ?link?
and ?hashtag?respectively.
We also removed all tweets starting with ?RT?
because they refer to previous tweets andtweets containing just combinations of user, link, ?RT?
and hashtags without any additional words.Tokenization of tweets requires proper handling of emoticons and other special character sequencestypical on Twitter.
The Ark-tweet-nlp tool (Gimpel et al., 2011) offers precisely that and although it wasdeveloped and tested in English, it yields satisfactory results in Czech as well.Czech is a highly flective language and uses a lot of diacritics.
However some Czech users typeonly the unaccented characters.9Preliminary experiments showed that removing diacritics yields betterresults, thus we removed diacritics from all tweets.4.2 Czech Dataset AnnotationFirstly we conducted an experiment to determine whether to annotate the original data or the normalizeddata.
We selected two sample sets of 50 tweets containing Czech sarcasm (#sarkasmus) and irony(#ironie) hashtags and other tweets.
One annotator obtained the original data while the other got thenormalized data from the first sample set.
We then tried to give both annotators the original data from thefirst sample set and finally we gave them both the normalized data from the second sample set.
Table 2shows the difficulty of sarcasm identification without the knowledge hidden in hashtags, user and links.7http://code.google.com/p/jlangdetect/8We found only 10 tweets with sarcasm hashtag (?#sarkasmus?)
and 100 tweets with irony hashtag (?#ironie?)
in140,000 collected tweets.9Approximately 10% of collected tweets were without any diacritics.216NormalizedNormalizedTag n sn 35 10s 0 5Cohen?s ?
: 0.412OriginalNormalizedTag n sn 19 10s 5 16Cohen?s ?
: 0.404OriginalOriginalTag n sn 25 4s 3 18Cohen?s ?
: 0.715Table 2: Confusion matrices and annotation agreement (Cohen?s ?)
between two annotators using orig-inal or normalized data.?Basic?
pipe Pipe 2 Pipe 3Tokenizing: ArkTweetNLPPOS tagging: PDT?
Stem: no (Sn) / light (Sl) / HPS (Sh)?Stopwords removal?
?Phonetic: eSpeak (Pe)Table 3: The preprocessing pipes for Czech (top-down).
Combinations of methods are denoted usingthe appropriate labels, e.g.
?Sn?
means 1. tokenizing, 2.
POS-tagging, 3. no stemming and 4. removingstopwords.
eSpeak stands for a phonetic transcription to International Phonetic Alphabet, which shouldreduce the effects of grammar mistakes and misspellings.The most promising results come from the annotation of the original data, thus the rest of the data areannotated in this manner.We randomly selected 7,000 tweets from the collected data for annotation.
The annotators were givenjust simple instructions without an explicit sarcasm definition (see Section 1): ?A tweet is consideredsarcastic when its content is intended ironically / sarcastically without anticipating further information.Offensive utterances, jokes and ironic situations are not considered ironic / sarcastic.
?The complete dataset of 7,000 tweets was independently annotated by two annotators.
The inter-annotator agreement (Cohen?s ?)
between the two annotators is 0.54.
They disagreed on 403 tweets.
Toresolve these conflicts we used a third annotator.The third annotator has been instructed the same way as the other two.
The final ?
agreement was mea-sured between the first two annotators, thus it was not affected by the third annotator.
Kappa agreementsmeasured on the conflicted states (403 tweets) were 0.4 (annotator 1 vs. annotator 3) and 0.6 (annotator2 vs. annotator 3).PreprocessingPreprocessing steps for handling social media texts in Czech were explored in (Habernal et al., 2013).The preprocessing diagram and its variants is depicted in Table 3.
Overall, there are various possible pre-processing ?pipe?
configurations including ?Basic?
pipeline consisting of tokenizing and POS-taggingonly.
We adapted all their preprocessing pipelines.
However, as the number of combinations would betoo large, we report only the settings with better performance.4.3 English DatasetWe collected 780,000 (130,000 sarcastic and 650,000 non-sarcastic) tweets in English.
The #sarcasmhashtag was used as an indicator of sarcastic tweets.
From this corpus we created two distributionalscenarios based on the work of (Reyes et al., 2013).
Refer to Table 4 for the final statistics of the dataset.Part of speech tagging was done using the Ark-tweet-nlp tool (Gimpel et al., 2011).5 ResultsFor each preprocessing pipeline (refer to table 3) we assembled various sets of features and employedtwo classifiers.
Accuracy (micro F-measure) tends to prefer performance on dominant classes in highly217Dataset \ Tweets Sarcastic Non-sarcasticCzech 325 6,675English Balanced 50,000 50,000English Imbalanced 25,000 75,000Table 4: The tweet distributions in datasets.Feature Set \ Pipeline Basic Sh ShPe Sl SlPe Sn SnPeBaseline 1 (B1): n-gram 54.8 55.3 55.2 55.0 55.0 54.4 55.3B1 + pattern 55.1 54.4 54.7 55.1 54.8 54.2 54.5B1 + word-shape pattern 54.6 54.8 55.2 54.4 55.0 54.8 55.1B1 + punctuation-based 54.7 48.8 48.8 48.8 48.8 53.8 55.5B1 + pointedness 55.0 54.7 54.7 55.0 55.9 54.8 54.9B1 + extended pointedness 54.5 48.8 48.8 48.8 48.8 54.7 54.6B1 + POS n-gram 53.4 54.1 54.2 55.3 55.1 54.2 53.9B1 + POS word-shape 55.0 55.6 55.2 54.8 54.6 55.8 54.4B1 + skip-bigram 54.2 54.8 54.2 54.7 56.0 54.6 54.4B1 + POS characteristics + emoticons 55.5 54.7 55.6 55.2 55.4 55.2 53.9B1 + POS characteristics + emoticons + word-case 53.8 56.4 55.5 54.6 55.3 55.9 55.3Character n-gram (3-6, min.
occurrence > 5) 53.0 52.7 53.2 53.9 54.7 52.0 53.2Baseline 2 (B2) 55.0 55.2 55.4 56.8 56.2 54.7 54.0B2 + FS1 52.3 48.8 48.8 48.8 48.8 52.0 52.9B2 + FS1 + FS2 53.0 48.8 48.8 48.8 48.8 52.2 53.6B2 + pattern 55.3 55.4 55.7 56.9 56.6 54.4 53.6B2 + POS word-shape 55.5 55.8 55.4 57.0 56.3 55.3 54.7B2 + POS characteristics + emoticons + word-case 56.1 55.7 55.7 56.9 56.1 55.0 54.3Table 5: Results on the Czech dataset with the MaxEnt classifier.
Macro F-measure, 95% confidenceinterval ?
?1.2.
Best results are in bold.
B2: character n-gram (3-5, min.
occurrence > 50) + skip-bigram + pointedness; FS1: character n-gram (3-6, min.
occurrence > 5) + extended pointedness; FS2:POS word-shape + pattern + POS characteristics + emoticons + word-case.unbalanced datasets (Manning et al., 2008), thus we chose macro F-measure as the evaluation metric(Forman and Scholz, 2010), as it allows us to compare classification results on different datasets.
Forstatistical significance testing, we report confidence intervals at ?
0.05.
Another applicable methodswould be, i.e., two-matched-samples t Test or McNemar?s test (Japkowicz and Shah, 2011).5.1 CzechTables 5 and 6 show the results on the Czech dataset.
The best result (F-measure 0.582) was achievedby the SVM classifier and a feature set enriched with patterns, utilizing stopwords removal and phonetictranscription in the preprocessing step.The importance of the appropriate preprocessing techniques for Czech is evident from the improve-ment of results for various feature sets, e.g., the best result for ?Basic?
pipeline (see line ?B2 + pattern?
).Both baselines show improvement on most preprocessing pipelines.
The most significant difference isvisible on the second baseline with the MaxEnt classifier and the ?Sl?
pipeline where the F-measure is0.018 higher than the ?Basic?
pipeline with no additional preprocessing.
The n-gram baseline was sig-nificantly outperformed by the SVM classifier with feature sets ?B1 + POS characteristics + Emoticons+ Word-case?
and ?B1 + extended pointedness?
on the ?SnPe?
pipeline.Error AnalysisTo get a better understanding of the limitations of our approach, we inspected 100 random tweets from theCzech dataset, which were wrongly classified by the SVM classifier with the best feature combination.218Feature Set \ Pipeline Basic Sh ShPe Sl SlPe Sn SnPeBaseline 1 (B1): n-gram 55.8 54.6 54.5 54.6 55.5 56.0 53.9B1 + pattern 55.6 54.0 54.3 54.6 55.7 55.4 55.6B1 + word-shape pattern 54.9 55.0 53.8 55.2 55.1 55.4 55.3B1 + punctuation-based 55.8 48.8 48.8 48.8 48.8 55.7 53.7B1 + pointedness 55.9 54.5 53.1 54.6 54.3 55.4 54.6B1 + extended pointedness 56.5 48.8 48.8 48.8 48.8 55.8 56.9B1 + POS n-gram 54.0 54.1 54.0 54.7 53.4 54.5 53.9B1 + POS word-shape 55.2 56.4 55.9 55.1 56.0 56.1 55.0B1 + skip-bigram 55.9 55.3 54.8 55.4 55.0 56.1 55.2B1 + POS characteristics + emoticons 55.9 54.5 54.1 54.6 54.2 56.7 55.8B1 + POS characteristics + emoticons + word-case 55.6 54.5 54.3 55.1 55.5 56.3 56.4Character n-gram (3-6, min.
occurrence > 5) 54.6 53.6 53.3 55.2 53.6 53.4 54.9Baseline 2 (B2) 55.9 56.4 56.3 57.0 56.2 57.1 55.8B2 + FS1 52.2 48.8 48.8 48.8 48.8 53.1 52.7B2 + FS1 + FS2 54.0 48.8 48.8 48.8 48.8 54.4 54.3B2 + pattern 56.8 57.0 56.7 56.5 57.5 57.1 58.2B2 + POS word-shape 56.5 56.3 57.2 56.4 56.1 56.3 57.8B2 + POS characteristics + emoticons + word-case 56.2 55.7 55.8 56.0 56.0 57.0 56.0Table 6: Results on the Czech dataset with the SVM classifier.
Macro F-measure, 95% confidenceinterval ?
?1.2.
Best results are in bold.
B2: character n-gram (3-5, min.
occurrence > 50) + skip-bigram + pointedness; FS1: character n-gram (3-6, min.
occurrence > 5) + extended pointedness; FS2:POS word-shape + pattern + POS characteristics + emoticons + word-case.We found 48 false positives and 52 false negatives.
The annotators disagreed upon 10% of these tweets.Non-sarcastic tweets were often about news, reviews, general information and user status updates.
Inmost of the difficult cases of true negatives, the tweet contains a question, insult, opinion or wordplay.Understanding sarcasm in some tweets was often bound with broader common knowledge (e.g., aboutnews or celebrities), the context known only to the author or authors opinion.
Another difficulty posessubtle or sophisticated expression of sarcasm such as ?I?m not sure whether you didn?t overdo a bit thefirst part of the renovation - the demolition.
:)?10or ?Conservatism, once something is in the schoolrules, it must be followed, forever, otherwise anarchy will break out and traditional values will die.
?115.2 EnglishThe results on both balanced and imbalanced English datasets are presented in Table 7.
In most cases theMaxEnt classifier significantly outperforms the SVM classifier.
The combination of majority of features(?B2 + FS1 + FS2?)
with the MaxEnt classifier yields the best results for both balanced and imbalanceddataset distributions.
This suggests that these features are coherent.
While no single feature captures theessence of sarcasm, all features together provide useful linguistic information for detecting sarcasm attextual level.Balanced distribution Both baselines were surpassed by various combinations of feature sets withthe MaxEnt classifier, although in some cases very narrowly (?B1 + punctuation-based?
and ?B1 +pointedness?
feature sets).
Although the SVM classifier has slightly worse results, it still performsreasonably, and we even recorded significant improvement over the baseline for ?B1 + POS word-shape?.The best results were achieved using the MaxEnt classifier with ?B2 + FS1 + FS2?
(F-measure 0.947)and ?B1 + word-shape pattern ?
(F-measure 0.943) feature sets.10?Jestli jste tu prvn??
?c?ast rekonstrukce - demolici - trochu nep?rehnali .
:)?11?Konzervatismus , kdy?z je to jednou ve ?skoln?
?m ?r?adu , tak se to mus??
dodr?zovat , a to nav?zdy , jinak vypukne anarchie atradi?cn??
hodnoty zem?rou .
?219Dataset Balanced ImbalancedClassifier MaxEnt SVM MaxEnt SVMFeature set \ Results Fm CI Fm CI Fm CI Fm CIBaseline 1 (B1): n-gram 93.28 0.16 92.86 0.16 90.76 0.18 90.44 0.18B1 + pattern 94.25 0.14 93.13 0.16 91.86 0.17 90.22 0.18B1 + word-shape pattern 94.33 0.14 93.17 0.16 92.01 0.17 90.35 0.18B1 + punctuation-based 93.32 0.15 92.84 0.16 90.72 0.18 90.43 0.18B1 + pointedness 93.29 0.16 92.99 0.16 91.00 0.18 90.07 0.19B1 + extended pointedness 93.68 0.15 92.61 0.16 91.07 0.18 89.89 0.19B1 + POS n-gram 93.66 0.15 92.64 0.16 91.20 0.18 89.85 0.19B1 + POS word-shape 93.96 0.15 93.19 0.16 91.41 0.17 90.51 0.18B1 + skip-bigram 93.63 0.15 93.17 0.16 90.99 0.18 90.48 0.18B1 + POS characteristics + emoticons 93.97 0.15 91.66 0.17 91.69 0.17 89.39 0.19B1 + POS characteristics + emoticons + word-case 93.96 0.15 91.54 0.17 91.61 0.17 88.89 0.19Character n-gram: (3-6, min.
occurrence > 5) 93.01 0.16 91.73 0.17 90.36 0.18 88.81 0.20Baseline 2 (B2) 92.81 0.16 91.67 0.17 90.65 0.18 88.70 0.20B2 + FS1 93.82 0.15 91.56 0.17 91.21 0.18 88.73 0.20B2 + FS1 + FS2 94.66 0.14 91.39 0.17 92.37 0.16 88.62 0.20B2 + pattern 93.60 0.15 91.66 0.17 90.86 0.18 88.82 0.20B2 + POS word-shape 93.20 0.16 91.65 0.17 90.82 0.18 88.74 0.20B2 + POS characteristics + emoticons + word-case 93.21 0.16 91.07 0.18 89.98 0.19 88.40 0.20Table 7: Results on the English dataset with the MaxEnt and SVM classifiers.
Macro F-measure (Fm)and 95% confidence interval (CI) are in %.
Best results are in bold.
B2: character n-gram (3-5, min.occurrence > 50) + skip-bigram + pointedness; FS1: character n-gram (3-6, min.
occurrence > 5) +extended pointedness; FS2: POS word-shape + pattern + POS characteristics + emoticons + word-case.Imbalanced distribution However, data in the real world do not necessarily resemble the balanceddistribution.
Therefore we have also performed the evaluation on an imbalanced distribution.
The Max-Ent classifier clearly achieves the best results.
This experiment indicates that combinations of features?B2 + FS1 + FS2?
(F-measure 0.924) and ?B1, word-shape pattern?
(F-measure 0.920) yields the bestresults for both balanced and imbalanced dataset distribution.5.3 DiscussionTo explain the huge difference in the performance between English and Czech, we conducted an addi-tional experiment in English.
We sampled the ?big-data?
English corpus (100k Tweets) to obtain thesame distribution as on the ?small-data?
Czech corpus (325 sarcastic and 6,675 non-sarcastic Tweets).Feature combination ?B2 + FS1 + FS2?
achieves an F-measure of 0.734?
0.01 (MaxEnt classifier) and0.729 ?
0.01 (SVM).
This performance drop shows that the amount of training data plays a key role(?
0.92 on ?big-data?
vs. ?
0.73 on ?small-data?).
However, these results are still significantly betterthan in Czech (?
0.58).
This demonstrates that Czech is a challenging language in sarcasm detection, asin other NLP tasks.In addition, we also experimented with the Naive Bayes classifier and with delta TF-IDF featurevariants (Martineau and Finin, 2009; Paltoglou and Thelwall, 2010) in both languages.
However, theperformance was not satisfactory in comparison with the reported results.6 ConclusionsWe investigated supervised machine learning methods for sarcasm detection on Twitter.
As a pilot studyfor sarcasm detection in the Czech language, we provide a large human-annotated Czech Twitter datasetcontaining 7,000 tweets with inter-annotator agreement ?
= 0.54.
The novel contributions of our workinclude the extensive evaluation of two classifiers with various combinations of feature sets on boththe Czech and English datasets as well as a comparison of different preprocessing techniques for the220Czech dataset.
Our approaches significantly outperformed both baselines adapted from related work12in English and achieved F-measure of 0.947 and 0.924 on the balanced and imbalanced datasets, re-spectively.13The best result on the Czech dataset was achieved by the SVM classifier with the featureset enriched with patterns yielding F-measure 0.582.
The whole project is available to the communityunder GPL license at http://liks.fav.zcu.cz/sarcasm/.
We believe that our findings willcontribute to the research outside the mainstream languages and may be applied to sarcasm detection inother Slavic languages, such as Slovak or Polish.6.1 Future workWe approached the problem mainly from the data-driven perspective (annotation, feature engineering,error analysis).
However, we feel that elaborating deep linguistic insights would be helpful to betterunderstand the phenomena of sarcasm on social media (Averbeck, 2013; Averbeck and Hample, 2008;Ivanko et al., 2004; Jorgensen, 1996).There are also possible extensions to the lexical/morphological features ?
either in the direction ofsemi-supervised learning and adding for example features based on latent semantics, topic models, orgraphical models popular in the sentiment analysis field (Habernal and Brychc?
?n, 2013; Brychc?
?n andHabernal, 2013), or the direction of deeper linguistic processing in terms of, e.g., syntax/dependecyparsing (but this has limitation given the nature of Twitter data as well as unavailability of such tools forCzech).
These deserve further investigation and are planned in future work.AcknowledgementsAccess to computing and storage facilities owned by parties and projects contributing to the National GridInfrastructure MetaCentrum, provided under the programme ?Projects of Large Infrastructure for Re-search, Development, and Innovations?
(LM2010005), is greatly appreciated.
Access to the CERIT-SCcomputing and storage facilities provided under the programme Center CERIT Scientific Cloud, part ofthe Operational Program Research and Development for Innovations, reg.
no.
CZ.
1.05/3.2.00/08.0144,is greatly appreciated.ReferencesJulie Kane Ahkter and Steven Soria.
2010.
Sentiment analysis: Facebook status messages.
Technical report,Stanford University.
Final Project CS224N.Joshua M Averbeck and Dale Hample.
2008.
Ironic message production: How and why we produce ironicmessages.
Communication Monographs, 75(4):396?410.Joshua M Averbeck.
2013.
Comparisons of ironic and sarcastic arguments in terms of appropriateness and effec-tiveness in personal relationships.
Argumentation & Advocacy, 50(1).Daniel M Bikel, Scott Miller, Richard Schwartz, and Ralph Weischedel.
1997.
Nymble: a high-performancelearning name-finder.
In Proceedings of the fifth conference on Applied natural language processing, pages194?201.
Association for Computational Linguistics.Ben Blamey, Tom Crick, and Giles Oatley.
2012.
R U : -) or : -( ?
character- vs. word-gram feature selectionfor sentiment classification of OSN corpora.
In Proceedings of AI-2012, The Thirty-second SGAI InternationalConference on Innovative Techniques and Applications of Artificial Intelligence, pages 207?212.
Springer.Cristina Bosco, Viviana Patti, and Andrea Bolioli.
2013.
Developing corpora for sentiment analysis: The case ofirony and senti-tut.
IEEE Intelligent Systems, 28(2):55?63.Tom?a?s Brychc?
?n and Ivan Habernal.
2013.
Unsupervised improving of sentiment analysis using global targetcontext.
In Proceedings of the International Conference Recent Advances in Natural Language ProcessingRANLP 2013, pages 122?128, Hissar, Bulgaria, September.
INCOMA Ltd. Shoumen, BULGARIA.12Word unigrams, bigrams, trigrams (Liebrecht et al., 2013) and a set of language-independent features (punctuation marks,emoticons, quotes, capitalized words, character n-grams and skip-grams.)
(Reyes et al., 2013)13Note that the best result (F-measure 0.715 on the balanced distribution and F-measure 0.533 on the imbalanced distribution)from the related work was achieved by (Reyes et al., 2013) using decision trees classifier.221Dmitry Davidov, Oren Tsur, and Ari Rappoport.
2010.
Semi-supervised recognition of sarcastic sentences in twit-ter and amazon.
In Proceedings of the Fourteenth Conference on Computational Natural Language Learning,CoNLL ?10, pages 107?116, Stroudsburg, PA, USA.
Association for Computational Linguistics.George Forman and Martin Scholz.
2010.
Apples-to-apples in cross-validation studies: Pitfalls in classifier per-formance measurement.
SIGKDD Explor.
Newsl., 12(1):49?57, November.Kevin Gimpel, Nathan Schneider, Brendan O?Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heil-man, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith.
2011.
Part-of-speech tagging for twitter: annota-tion, features, and experiments.
In Proceedings of the 49th Annual Meeting of the Association for ComputationalLinguistics: Human Language Technologies: short papers - Volume 2, HLT ?11, pages 42?47, Stroudsburg, PA,USA.
Association for Computational Linguistics.Roberto Gonz?alez-Ib?a?nez, Smaranda Muresan, and Nina Wacholder.
2011.
Identifying sarcasm in twitter: Acloser look.
In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:Human Language Technologies: Short Papers - Volume 2, HLT ?11, pages 581?586, Stroudsburg, PA, USA.Association for Computational Linguistics.David Guthrie, Ben Allison, Wei Liu, Louise Guthrie, and Yorick Wilks.
2006.
A closer look at skip-grammodelling.
In Proceedings of the 5th international Conference on Language Resources and Evaluation (LREC-2006), pages 1?4.Ivan Habernal and Tom?a?s Brychc??n.
2013.
Semantic spaces for sentiment analysis.
In Ivan Habernal and V?aclavMatou?sek, editors, Text, Speech, and Dialogue, volume 8082 of Lecture Notes in Computer Science, pages484?491.
Springer Berlin Heidelberg.Ivan Habernal, Tom?a?s Pt?a?cek, and Josef Steinberger.
2013.
Sentiment analysis in czech social media usingsupervised machine learning.
In Proceedings of the 4th Workshop on Computational Approaches to Subjectivity,Sentiment and Social Media Analysis, pages 65?74, Atlanta, Georgia, June.
Association for ComputationalLinguistics.Stacey L Ivanko, Penny M Pexman, and Kara M Olineck.
2004.
How sarcastic are you?
individual differencesand verbal irony.
Journal of language and social psychology, 23(3):244?271.Nathalie Japkowicz and Mohak Shah.
2011.
Evaluating Learning Algorithms: A Classification Perspective.Cambridge University Press.Julia Jorgensen.
1996.
The functions of sarcastic irony in speech.
Journal of Pragmatics, 26(5):613 ?
634.Michal Konkol.
2014.
Brainy: A machine learning library.
In Leszek Rutkowski, Marcin Korytkowski, RafalScherer, Ryszard Tadeusiewicz, Lotfi Zadeh, and Jacek Zurada, editors, Artificial Intelligence and Soft Comput-ing, volume 8468 of Lecture Notes in Computer Science, pages 490?499.
Springer International Publishing.Efthymios Kouloumpis, Theresa Wilson, and Johanna Moore.
2011.
Twitter sentiment analysis: The good the badand the OMG!
In Proceedings of the Fifth International Conference on Weblogs and Social Media, Barcelona,Catalonia, Spain, July 17-21, 2011.
The AAAI Press.Christine Liebrecht, Florian Kunneman, and Antal Van den Bosch.
2013.
The perfect solution for detectingsarcasm in tweets #not.
In Proceedings of the 4th Workshop on Computational Approaches to Subjectivity,Sentiment and Social Media Analysis, pages 29?37, Atlanta, Georgia, June.
Association for ComputationalLinguistics.Bing Liu and Lei Zhang.
2012.
A survey of opinion mining and sentiment analysis.
In Mining Text Data, pages415?463.
Springer.Stephanie Lukin and Marilyn Walker.
2013.
Really?
Well.
Apparently bootstrapping improves the performanceof sarcasm and nastiness classifiers for online dialogue.
In Proceedings of the Workshop on Language Analysisin Social Media, pages 30?40, Atlanta, Georgia, June.
Association for Computational Linguistics.Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schtze.
2008.
Introduction to Information Retrieval.Cambridge University Press, New York, NY, USA.Justin Martineau and Tim Finin.
2009.
Delta TFIDF: An improved feature space for sentiment analysis.
InProceedings of the Third International Conference on Weblogs and Social Media, ICWSM 2009, San Jose,California, USA.
The AAAI Press.222A.
Montejo-R?aez, E.
Mart?
?nez-C?amara, M. T.
Mart?
?n-Valdivia, and L. A. Ure?na L?opez.
2012.
Random walkweighting over sentiwordnet for sentiment polarity detection on twitter.
In Proceedings of the 3rd Workshop inComputational Approaches to Subjectivity and Sentiment Analysis, WASSA ?12, pages 3?10, Stroudsburg, PA,USA.
Association for Computational Linguistics.Georgios Paltoglou and Mike Thelwall.
2010.
A study of information retrieval weighting schemes for sentimentanalysis.
In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL?10, pages 1386?1395, Stroudsburg, PA, USA.
Association for Computational Linguistics.Antonio Reyes, Paolo Rosso, and Davide Buscaldi.
2012.
From humor recognition to irony detection: Thefigurative language of social media.
Data Knowl.
Eng., 74:1?12, April.Antonio Reyes, Paolo Rosso, and Tony Veale.
2013.
A multidimensional approach for detecting irony in twitter.Language Resources and Evaluation, 47(1):239?268.Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalindra De Silva, Nathan Gilbert, and Ruihong Huang.
2013.
Sar-casm as contrast between a positive sentiment and negative situation.
In Proceedings of the 2013 Conferenceon Empirical Methods in Natural Language Processing, pages 704?714, Seattle, Washington, USA, October.Association for Computational Linguistics.Oren Tsur, Dmitry Davidov, and Ari Rappoport.
2010.
ICWSM - a great catchy name: Semi-supervised recog-nition of sarcastic sentences in online product reviews.
In William W. Cohen and Samuel Gosling, editors,Proceedings of the Fourth International Conference on Weblogs and Social Media, ICWSM 2010, Washington,DC, USA, May 23-26, 2010.
The AAAI Press.Mikalai Tsytsarau and Themis Palpanas.
2012.
Survey on mining subjective data on the web.
Data Mining andKnowledge Discovery, 24(3):478?514, May.Aline A Vanin, Larissa A Freitas, Renata Vieira, and Marco Bochernitsan.
2013.
Some clues on irony detectionin tweets.
In Proceedings of the 22nd international conference on World Wide Web companion, pages 635?636.International World Wide Web Conferences Steering Committee.Ian H Witten and Eibe Frank.
2005.
Data Mining: Practical machine learning tools and techniques.
MorganKaufmann.223
