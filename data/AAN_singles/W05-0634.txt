Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),pages 217?220, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsSemantic Role Chunking Combining Complementary Syntactic ViewsSameer Pradhan, Kadri Hacioglu, Wayne Ward, James H. Martin and Daniel Jurafsky?Center for Spoken Language Research, University of Colorado, Boulder, CO 80303?Department of Linguistics, Stanford University, Stanford, CA 94305{spradhan,hacioglu,whw,martin}@cslr.colorado.edu, jurafsky@stanford.eduAbstractThis paper describes a semantic role la-beling system that uses features derivedfrom different syntactic views, and com-bines them within a phrase-based chunk-ing paradigm.
For an input sentence, syn-tactic constituent structure parses are gen-erated by a Charniak parser and a Collinsparser.
Semantic role labels are assignedto the constituents of each parse usingSupport Vector Machine classifiers.
Theresulting semantic role labels are con-verted to an IOB representation.
TheseIOB representations are used as additionalfeatures, along with flat syntactic chunks,by a chunking SVM classifier that pro-duces the final SRL output.
This strategyfor combining features from three differ-ent syntactic views gives a significant im-provement in performance over roles pro-duced by using any one of the syntacticviews individually.1 IntroductionThe task of Semantic Role Labeling (SRL) involvestagging groups of words in a sentence with the se-mantic roles that they play with respect to a particu-lar predicate in that sentence.
Our approach is to usesupervised machine learning classifiers to producethe role labels based on features extracted from theinput.
This approach is neutral to the particular setof labels used, and will learn to tag input accordingto the annotated data that it is trained on.
The taskreported on here is to produce PropBank (Kingsburyand Palmer, 2002) labels, given the features pro-vided for the CoNLL-2005 closed task (Carreras andMa`rquez, 2005).We have previously reported on using SVM clas-sifiers for semantic role labeling.
In this work, weformulate the semantic labeling problem as a multi-class classification problem using Support VectorMachine (SVM) classifiers.
Some of these systemsuse features based on syntactic constituents pro-duced by a Charniak parser (Pradhan et al, 2003;Pradhan et al, 2004) and others use only a flat syn-tactic representation produced by a syntactic chun-ker (Hacioglu et al, 2003; Hacioglu and Ward,2003; Hacioglu, 2004; Hacioglu et al, 2004).
Thelatter approach lacks the information provided bythe hierarchical syntactic structure, and the formerimposes a limitation that the possible candidate rolesshould be one of the nodes already present in thesyntax tree.
We found that, while the chunk basedsystems are very efficient and robust, the systemsthat use features based on full syntactic parses aregenerally more accurate.
Analysis of the sourceof errors for the parse constituent based systemsshowed that incorrect parses were a major sourceof error.
The syntactic parser did not produce anyconstituent that corresponded to the correct segmen-tation for the semantic argument.
In Pradhan et al(2005), we reported on a first attempt to overcomethis problem by combining semantic role labels pro-duced from different syntactic parses.
The hope isthat the syntactic parsers will make different errors,and that combining their outputs will improve on217either system alone.
This initial attempt used fea-tures from a Charniak parser, a Minipar parser and achunk based parser.
It did show some improvementfrom the combination, but the method for combin-ing the information was heuristic and sub-optimal.In this paper, we report on what we believe is an im-proved framework for combining information fromdifferent syntactic views.
Our goal is to preserve therobustness and flexibility of the segmentation of thephrase-based chunker, but to take advantage of fea-tures from full syntactic parses.
We also want tocombine features from different syntactic parses togain additional robustness.
To this end, we use fea-tures generated from a Charniak parser and a Collinsparser, as supplied for the CoNLL-2005 closed task.2 System DescriptionWe again formulate the semantic labeling problemas a multi-class classification problem using Sup-port Vector Machine (SVM) classifiers.
TinySVM1along with YamCha2 (Kudo and Matsumoto, 2000;Kudo and Matsumoto, 2001) are used to implementthe system.
Using what is known as the ONE VSALL classification strategy, n binary classifiers aretrained, where n is number of semantic classes in-cluding a NULL class.The general framework is to train separate seman-tic role labeling systems for each of the parse treeviews, and then to use the role arguments output bythese systems as additional features in a semanticrole classifier using a flat syntactic view.
The con-stituent based classifiers walk a syntactic parse treeand classify each node as NULL (no role) or as oneof the set of semantic roles.
Chunk based systemsclassify each base phrase as being the B(eginning)of a semantic role, I(nside) a semantic role, orO(utside) any semantic role (ie.
NULL).
Thisis referred to as an IOB representation (Ramshawand Marcus, 1995).
The constituent level roles aremapped to the IOB representation used by the chun-ker.
The IOB tags are then used as features for aseparate base-phase semantic role labeler (chunker),in addition to the standard set of features used bythe chunker.
An n-fold cross-validation paradigmis used to train the constituent based role classifiers1http://chasen.org/?taku/software/TinySVM/2http://chasen.org/?taku/software/yamcha/and the chunk based classifier.For the system reported here, two full syntacticparsers were used, a Charniak parser and a Collinsparser.
Features were extracted by first generatingthe Collins and Charniak syntax trees from the word-by-word decomposed trees in the CoNLL data.
Thechunking system for combining all features wastrained using a 4-fold paradigm.
In each fold, sepa-rate SVM classifiers were trained for the Collins andCharniak parses using 75% of the training data.
Thatis, one system assigned role labels to the nodes inCharniak based trees and a separate system assignedroles to nodes in Collins based trees.
The other 25%of the training data was then labeled by each of thesystems.
Iterating this process 4 times created thetraining set for the chunker.
After the chunker wastrained, the Charniak and Collins based semantic la-belers were then retrained using all of the trainingdata.Two pieces of the system have problems scalingto large training sets ?
the final chunk based clas-sifier and the NULL VS NON-NULL classifier forthe parse tree syntactic views.
Two techniques wereused to reduce the amount of training data ?
activesampling and NULL filtering.
The active samplingprocess was performed as follows.
We first traina system using 10k seed examples from the train-ing set.
We then labeled an additional block of datausing this system.
Any sentences containing an er-ror were added to the seed training set.
The sys-tem was retrained and the procedure repeated untilthere were no misclassified sentences remaining inthe training data.
The set of examples produced bythis procedure was used to train the final NULL VSNON-NULL classifier.
The same procedure was car-ried out for the chunking system.
After both thesewere trained, we tagged the training data using themand removed all most likely NULLs from the data.Table 1 lists the features used in the constituentbased systems.
They are a combination of featuresintroduced by Gildea and Jurafsky (2002), ones pro-posed in Pradhan et al (2004), Surdeanu et al(2003) and the syntactic-frame feature proposed in(Xue and Palmer, 2004).
These features are ex-tracted from the parse tree being labeled.
In additionto the features extracted from the parse tree beinglabeled, five features were extracted from the otherparse tree (phrase, head word, head word POS, path218PREDICATE LEMMAPATH: Path from the constituent to the predicate in the parse tree.POSITION: Whether the constituent is before or after the predicate.PREDICATE SUB-CATEGORIZATIONHEAD WORD: Head word of the constituent.HEAD WORD POS: POS of the head wordNAMED ENTITIES IN CONSTITUENTS: Person, Organization, Locationand Miscellaneous.PARTIAL PATH: Path from the constituent to the lowest common ancestorof the predicate and the constituent.HEAD WORD OF PP: Head of PP replaced by head word of NP inside it,and PP replaced by PP-prepositionFIRST AND LAST WORD/POS IN CONSTITUENTORDINAL CONSTITUENT POSITIONCONSTITUENT TREE DISTANCECONSTITUENT RELATIVE FEATURES: Nine features representingthe phrase type, head word and head word part of speech of theparent, and left and right siblings of the constituent.SYNTACTIC FRAMECONTENT WORD FEATURES: Content word, its POS and named entitiesin the content wordCLAUSE-BASED PATH VARIATIONS:I.
Replacing all the nodes in a path other than clause nodes with an ?
*?.For example, the path NP?S?VP?SBAR?NP?VP?VBDbecomes NP?S?*S?*?*?VBDII.
Retaining only the clause nodes in the path, which for the aboveexample would produce NP?S?S?VBD,III.
Adding a binary feature that indicates whether the constituentis in the same clause as the predicate,IV.
collapsing the nodes between S nodes which gives NP?S?NP?VP?VBD.PATH N-GRAMS: This feature decomposes a path into a series of trigrams.For example, the path NP?S?VP?SBAR?NP?VP?VBD becomes:NP?S?VP, S?VP?SBAR, VP?SBAR?NP, SBAR?NP?VP, etc.
Weused the first ten trigrams as ten features.
Shorter paths were paddedwith nulls.SINGLE CHARACTER PHRASE TAGS: Each phrase category is clusteredto a category defined by the first character of the phrase label.PREDICATE CONTEXT: Two words and two word POS around thepredicate and including the predicate were added as ten new features.PUNCTUATION: Punctuation before and after the constituent wereadded as two new features.FEATURE CONTEXT: Features for argument bearing constituentswere added as features to the constituent being classified.Table 1: Features used by the constituent-based sys-temand predicate sub-categorization).
So for example,when assigning labels to constituents in a Charniakparse, all of the features in Table 1 were extractedfrom the Charniak tree, and in addition phrase, headword, head word POS, path and sub-categorizationwere extracted from the Collins tree.
We have pre-viously determined that using different sets of fea-tures for each argument (role) achieves better resultsthan using the same set of features for all argumentclasses.
A simple feature selection was implementedby adding features one by one to an initial set offeatures and selecting those that contribute signifi-cantly to the performance.
As described in Pradhanet al (2004), we post-process lattices of n-best de-cision using a trigram language model of argumentsequences.Table 2 lists the features used by the chunker.These are the same set of features that were usedin the CoNLL-2004 semantic role labeling task byHacioglu, et al (2004) with the addition of the twosemantic argument (IOB) features.
For each token(base phrase) to be tagged, a set of features is createdfrom a fixed size context that surrounds each token.In addition to the features in Table 2, it also uses pre-vious semantic tags that have already been assignedto the tokens contained in the linguistic context.
A5-token sliding window is used for the context.SVMs were trained for begin (B) and inside (I)classes of all arguments and an outside (O) class.WORDSPREDICATE LEMMASPART OF SPEECH TAGSBP POSITIONS: The position of a token in a BP using the IOB2representation (e.g.
B-NP, I-NP, O, etc.
)CLAUSE TAGS: The tags that mark token positions in a sentencewith respect to clauses.NAMED ENTITIES: The IOB tags of named entities.TOKEN POSITION: The position of the phrase with respect tothe predicate.
It has three values as ?before?, ?after?
and ?-?
(forthe predicate)PATH: It defines a flat path between the token and the predicateHIERARCHICAL PATH: Since we have the syntax tree for the sentences,we also use the hierarchical path from the phrase being classified to thebase phrase containing the predicate.CLAUSE BRACKET PATTERNSCLAUSE POSITION: A binary feature that identifies whether thetoken is inside or outside the clause containing the predicateHEADWORD SUFFIXES: suffixes of headwords of length 2, 3 and 4.DISTANCE: Distance of the token from the predicate as a numberof base phrases, and the distance as the number of VP chunks.LENGTH: the number of words in a token.PREDICATE POS TAG: the part of speech category of the predicatePREDICATE FREQUENCY: Frequent or rare using a threshold of 3.PREDICATE BP CONTEXT: The chain of BPs centered at the predicatewithin a window of size -2/+2.PREDICATE POS CONTEXT: POS tags of words immediately precedingand following the predicate.PREDICATE ARGUMENT FRAMES: Left and right core argument patternsaround the predicate.DYNAMIC CLASS CONTEXT: Hypotheses generated for two preceedingphrases.NUMBER OF PREDICATES: This is the number of predicates inthe sentence.CHARNIAK-BASED SEMANTIC IOB TAG: This is the IOB tag generatedusing the tagger trained on Charniak treesCOLLINS-BASED SEMANTIC IOB TAG: This is the IOB tag generatedusing the tagger trained on Collins?
treesTable 2: Features used by phrase-based chunker.3 Experimental ResultsTable 3 shows the results obtained on the WSJ de-velopment set (Section 24), the WSJ test set (Section23) and the Brown test set (Section ck/01-03)4 AcknowledgmentsThis research was partially supported by the ARDAAQUAINT program via contract OCG4423B andby the NSF via grants IS-9978025 and ITR/HCI219Precision Recall F?=1Development 80.90% 75.38% 78.04Test WSJ 81.97% 73.27% 77.37Test Brown 73.73% 61.51% 67.07Test WSJ+Brown 80.93% 71.69% 76.03Test WSJ Precision Recall F?=1Overall 81.97% 73.27% 77.37A0 91.39% 82.23% 86.57A1 79.80% 76.23% 77.97A2 68.61% 62.61% 65.47A3 73.95% 50.87% 60.27A4 78.65% 68.63% 73.30A5 75.00% 60.00% 66.67AM-ADV 61.64% 46.05% 52.71AM-CAU 76.19% 43.84% 55.65AM-DIR 53.33% 37.65% 44.14AM-DIS 80.56% 63.44% 70.98AM-EXT 100.00% 46.88% 63.83AM-LOC 64.48% 51.52% 57.27AM-MNR 62.90% 45.35% 52.70AM-MOD 98.64% 92.38% 95.41AM-NEG 98.21% 95.65% 96.92AM-PNC 56.67% 44.35% 49.76AM-PRD 0.00% 0.00% 0.00AM-REC 0.00% 0.00% 0.00AM-TMP 83.37% 71.94% 77.23R-A0 94.29% 88.39% 91.24R-A1 85.93% 74.36% 79.73R-A2 100.00% 37.50% 54.55R-A3 0.00% 0.00% 0.00R-A4 0.00% 0.00% 0.00R-AM-ADV 0.00% 0.00% 0.00R-AM-CAU 0.00% 0.00% 0.00R-AM-EXT 0.00% 0.00% 0.00R-AM-LOC 90.00% 42.86% 58.06R-AM-MNR 66.67% 33.33% 44.44R-AM-TMP 75.00% 40.38% 52.50V 98.86% 98.86% 98.86Table 3: Overall results (top) and detailed results onthe WSJ test (bottom).0086132.
Computer time was provided by NSFARI Grant #CDA-9601817, NSF MRI Grant #CNS-0420873, NASA AIST grant #NAG2-1646, DOESciDAC grant #DE-FG02-04ER63870, NSF spon-sorship of the National Center for Atmospheric Re-search, and a grant from the IBM Shared UniversityResearch (SUR) program.Special thanks to Matthew Woitaszek, Theron Vo-ran and the other administrative team of the Hemi-sphere and Occam Beowulf clusters.
Without thesethe training would never be possible.ReferencesXavier Carreras and Llu?
?s Ma`rquez.
2005. n Introduction to the CoNLL-2005Shared Task: Semantic Role Labeling.
In Proceedings of CoNLL-2005.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic labeling of semantic roles.Computational Linguistics, 28(3):245?288.Kadri Hacioglu and Wayne Ward.
2003.
Target word detection and semanticrole chunking using support vector machines.
In Proceedings of the HumanLanguage Technology Conference, Edmonton, Canada.Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James Martin, and Dan Jurafsky.2003.
Shallow semantic parsing using support vector machines.
TechnicalReport TR-CSLR-2003-1, Center for Spoken Language Research, Boulder,Colorado.Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James Martin, and Daniel Ju-rafsky.
2004.
Semantic role labeling by tagging syntactic chunks.
In Pro-ceedings of the 8th Conference on CoNLL-2004, Shared Task ?
Semantic RoleLabeling.Kadri Hacioglu.
2004.
A lightweight semantic chunking model based on tagging.In Proceedings of the Human Language Technology Conference /North Amer-ican chapter of the Association of Computational Linguistics (HLT/NAACL),Boston, MA.Paul Kingsbury and Martha Palmer.
2002.
From Treebank to PropBank.
InProceedings of the 3rd International Conference on Language Resources andEvaluation (LREC-2002), Las Palmas, Canary Islands, Spain.Taku Kudo and Yuji Matsumoto.
2000.
Use of support vector learning for chunkidentification.
In Proceedings of the 4th Conference on CoNLL-2000 andLLL-2000, pages 142?144.Taku Kudo and Yuji Matsumoto.
2001.
Chunking with support vector machines.In Proceedings of the 2nd Meeting of the North American Chapter of the As-sociation for Computational Linguistics (NAACL-2001).Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James Martin, and Dan Jurafsky.2003.
Semantic role parsing: Adding semantic structure to unstructured text.In Proceedings of the International Conference on Data Mining (ICDM 2003),Melbourne, Florida.Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin, and Dan Jurafsky.2004.
Shallow semantic parsing using support vector machines.
In Proceed-ings of the Human Language Technology Conference/North American chapterof the Association of Computational Linguistics (HLT/NAACL), Boston, MA.Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin, and Dan Jurafsky.2005.
Semantic role labeling using different syntactic views.
In Proceedingsof the Association for Computational Linguistics 43rd annual meeting (ACL-2005), Ann Arbor, MI.L.
A. Ramshaw and M. P. Marcus.
1995.
Text chunking using transformation-based learning.
In Proceedings of the Third Annual Workshop on Very LargeCorpora, pages 82?94.
ACL.Mihai Surdeanu, Sanda Harabagiu, John Williams, and Paul Aarseth.
2003.
Us-ing predicate-argument structures for information extraction.
In Proceedingsof the 41st Annual Meeting of the Association for Computational Linguistics,Sapporo, Japan.Nianwen Xue and Martha Palmer.
2004.
Calibrating features for semantic rolelabeling.
In Proceedings of the Conference on Empirical Methods in NaturalLanguage Processing, Barcelona, Spain.220
