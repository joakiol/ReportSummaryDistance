VERBOCEAN: Mining the Web for Fine-Grained Semantic Verb RelationsTimothy Chklovski and Patrick PantelInformation Sciences InstituteUniversity of Southern California4676 Admiralty WayMarina del Rey, CA  90292{timc, pantel}@isi.eduAbstractBroad-coverage repositories of semantic relationsbetween verbs could benefit many NLP tasks.
Wepresent a semi-automatic method for extractingfine-grained semantic relations between verbs.
Wedetect similarity, strength, antonymy, enablement,and temporal happens-before relations betweenpairs of strongly associated verbs using lexico-syntactic patterns over the Web.
On a set of 29,165strongly associated verb pairs, our extraction algo-rithm yielded 65.5% accuracy.
Analysis oferror types shows that on the relation strength weachieved 75% accuracy.
We provide theresource, called VERBOCEAN, for download athttp://semantics.isi.edu/ocean/.1 IntroductionMany NLP tasks, such as question answering,summarization, and machine translation couldbenefit from broad-coverage semantic resourcessuch as WordNet (Miller 1990) and EVCA (Eng-lish Verb Classes and Alternations) (Levin 1993).These extremely useful resources have very highprecision entries but have important limitationswhen used in real-world NLP tasks due to theirlimited coverage and prescriptive nature (i.e.
theydo not include semantic relations that are plausiblebut not guaranteed).
For example, it may be valu-able to know that if someone has bought an item,they may sell it at a later time.
WordNet does notinclude the relation ?X buys Y?
happens-before ?Xsells Y?
since it is possible to sell something with-out having bought it (e.g.
having manufactured orstolen it).Verbs are the primary vehicle for describingevents and expressing relations between entities.Hence, verb semantics could help in many naturallanguage processing (NLP) tasks that deal withevents or relations between entities.
For taskswhich require canonicalization of natural languagestatements or derivation of plausible inferencesfrom such statements, a particularly valuable re-source is one which (i) relates verbs to one anotherand (ii) provides broad coverage of the verbs in thetarget language.In this paper, we present an algorithm that semi-automatically discovers fine-grained verb seman-tics by querying the Web using simple lexico-syntactic patterns.
The verb relations we discoverare similarity, strength, antonymy, enablement, andtemporal relations.
Identifying these relations over29,165 verb pairs results in a broad-coverage re-source we call VERBOCEAN.
Our approach extendspreviously formulated ones that use surface pat-terns as indicators of semantic relations betweennouns (Hearst 1992; Etzioni 2003; Ravichandranand Hovy 2002).
We extend these approaches intwo ways: (i) our patterns indicate verb conjuga-tion to increase their expressiveness and specificityand (ii) we use a measure similar to mutual infor-mation to account for both the frequency of theverbs whose semantic relations are being discov-ered as well as for the frequency of the pattern.2 Relevant WorkIn this section, we describe application domainsthat can benefit from a resource of verb semantics.We then introduce some existing resources anddescribe previous attempts at mining semanticsfrom text.2.1 ApplicationsQuestion answering is often approached by can-onicalizing the question text and the answer textinto logical forms.
This approach is taken, interalia, by a top-performing system (Moldovan et al2002).
In discussing future work on the system?slogical form matching component, Rus (2002 p.143) points to incorporating entailment and causa-tion verb relations to improve the matcher?s per-formance.
In other work, Webber et al (2002)have argued that successful question answeringdepends on lexical reasoning, and that lexical rea-soning in turn requires fine-grained verb semanticsin addition to troponymy (is-a relations betweenverbs) and antonymy.In multi-document summarization, knowing verbsimilarities is useful for sentence compression andfor determining sentences that have the samemeaning (Lin 1997).
Knowing that a particularaction happens before another or is enabled byanother is also useful to determine the order of theevents (Barzilay et al 2002).
For example, to ordersummary sentences properly, it may be useful toknow that selling something can be preceded byeither buying, manufacturing, or stealing it.
Fur-thermore, knowing that a particular verb has ameaning stronger than another (e.g.
rape vs. abuseand renovate vs. upgrade) can help a system pickthe most general sentence.In lexical selection of verbs in machine transla-tion and in work on document classification, prac-titioners have argued for approaches that dependon wide-coverage resources indicating verb simi-larity and membership of a verb in a certain class.In work on translating verbs with many counter-parts in the target language, Palmer and Wu (1995)discuss inherent limitations of approaches whichdo not examine a verb?s class membership, and putforth an approach based on verb similarity.
Indocument classification, Klavans and Kan (1998)demonstrate that document type is correlated withthe presence of many verbs of a certain EVCAclass (Levin 1993).
In discussing future work, Kla-vans and Kan point to extending coverage of themanually constructed EVCA resource as a way ofimproving the performance of the system.
A wide-coverage repository of verb relations includingverbs linked by the similarity relation will providea way to automatically extend the existing verbclasses to cover more of the English lexicon.2.2 Existing resourcesSome existing broad-coverage resources onverbs have focused on organizing verbs intoclasses or annotating their frames or thematic roles.EVCA (English Verb Classes and Alternations)(Levin 1993) organizes verbs by similarity andparticipation / nonparticipation in alternation pat-terns.
It contains 3200 verbs classified into 191classes.
Additional manually constructed resourcesinclude PropBank (Kingsbury et al 2002), Frame-Net (Baker et al 1998), VerbNet (Kipper et al2000), and the resource on verb selectional restric-tions developed by Gomez (2001).Our approach differs from the above in its focus.We relate verbs to each other rather than organizethem into classes or identify their frames or the-matic roles.
WordNet does provide relations be-tween verbs, but at a coarser level.
We providefiner-grained relations such as strength, enable-ment and temporal information.
Also, in contrastwith WordNet, we cover more than the prescriptivecases.2.3 Mining semantics from textPrevious web mining work has rarely addressedextracting many different semantic relations fromWeb-sized corpus.
Most work on extracting se-mantic information from large corpora has largelyfocused on the extraction of is-a relations betweennouns.
Hearst (1992) was the first followed byrecent larger-scale and more fully automated ef-forts (Pantel and Ravichandran 2004; Etzioni et al2004; Ravichandran and Hovy 2002).
Recently,Moldovan et al (2004) present a learning algo-rithm to detect 35 fine-grained noun phrase rela-tions.Turney (2001) studied word relatedness andsynonym extraction, while Lin et al (2003) presentan algorithm that queries the Web using lexicalpatterns for distinguishing noun synonymy andantonymy.
Our approach addresses verbs and pro-vides for a richer and finer-grained set of seman-tics.
Reliability of estimating bigram counts on theweb via search engines has been investigated byKeller and Lapata (2003).Semantic networks have also been extractedfrom dictionaries and other machine-readable re-sources.
MindNet (Richardson et al 1998) extractsa collection of triples of the type ?ducks havewings?
and ?duck capable-of flying?.
This re-source, however, does not relate verbs to eachother or provide verb semantics.3 Semantic relations among verbsIn this section, we introduce and motivate thespecific relations that we extract.
Whilst the naturallanguage literature is rich in theories of semantics(Barwise and Perry 1985; Schank and Abelson1977), large-coverage manually created semanticresources typically only organize verbs into a flator shallow hierarchy of classes (such as those de-scribed in Section 2.2).
WordNet identifies synon-ymy, antonymy, troponymy, and cause.
As sum-marized in Figure 1, Fellbaum (1998) discusses afiner-grained analysis of entailment, while theWordNet database does not distinguish between,e.g., backward presupposition (forget :: know,where know must have happened before forget)from proper temporal inclusion (walk :: step).
Informulating our set of relations, we have relied onthe finer-grained analysis, explicitly breaking outthe temporal precedence between entities.In selecting the relations to identify, we aimed atboth covering the relations described in WordNetand covering the relations present in our collectionFigure 1.
Fellbaum?s (1998) entailment hierarchy.+Temporal InclusionEntailment-Temporal Inclusion+Troponymy(coextensiveness)march-walk-Troponymy(proper inclusion)walk-stepBackwardPresuppositionforget-knowCauseshow-seeof strongly associated verb pairs.
We relied on thestrongly associated verb pairs, described in Section4.4, for computational efficiency.
The relations weidentify were experimentally found to cover 99 outof 100 randomly selected verb pairs.Our algorithm identifies six semantic relationsbetween verbs.
These are summarized in Table 1along with their closest corresponding WordNetcategory and the symmetry of the relation (whetherV1 rel V2 is equivalent to V2 rel V1).Similarity.
As Fellbaum (1998) and the traditionof organizing verbs into similarity classes indicate,verbs do not neatly fit into a unified is-a (tro-ponymy) hierarchy.
Rather, verbs are often similaror related.
Similarity between action verbs, forexample, can arise when they differ in connota-tions about manner or degree of action.
Examplesextracted by our system include maximize :: en-hance, produce :: create, reduce :: restrict.Strength.
When two verbs are similar, one maydenote a more intense, thorough, comprehensive orabsolute action.
In the case of change-of-stateverbs, one may denote a more complete change.We identify this as the strength relation.
Sampleverb pairs extracted by our system, in the orderweak to strong, are: taint :: poison, permit :: au-thorize, surprise :: startle, startle :: shock.
Someinstances of strength sometimes map to WordNet?stroponymy relation.Strength, a subclass of similarity, has not beenidentified in broad-coverage networks of verbs, butmay be of particular use in natural language gen-eration and summarization applications.Antonymy.
Also known as semantic opposition,antonymy between verbs has several distinct sub-types.
As discussed by Fellbaum (1998), it canarise from switching thematic roles associated withthe verb (as in buy :: sell, lend :: borrow).
There isalso antonymy between stative verbs (live :: die,differ :: equal) and antonymy between siblingverbs which share a parent (walk :: run) or an en-tailed verb (fail :: succeed both entail try).Antonymy also systematically interacts with thehappens-before relation in the case of restitutiveopposition (Cruse 1986).
This subtype is exempli-fied by damage :: repair, wrap :: unwrap.
In termsof the relations we recognize, it can be stated thatrestitutive-opposition(V1, V2) = happens-before(V1, V2), and antonym(V1, V2).
Examples ofantonymy extracted by our system include: assem-ble :: dismantle; ban :: allow; regard :: condemn,roast :: fry.Enablement.
This relation holds between twoverbs V1 and V2 when the pair can be glossed as V1is accomplished by V2.
Enablement is classified asa type of causal relation by Barker and Szpakowicz(1995).
Examples of enablement extracted by oursystem include: assess :: review and accomplish ::complete.Happens-before.
This relation indicates that thetwo verbs refer to two temporally disjoint intervalsor instances.
WordNet?s cause relation, between acausative and a resultative verb (as in buy :: own),would be tagged as instances of happens-before byour system.
Examples of the happens-before rela-tion identified by our system include marry :: di-vorce, detain :: prosecute, enroll :: graduate,schedule :: reschedule, tie :: untie.4 ApproachWe discover the semantic relations describedabove by querying the Web with Google forlexico-syntactic patterns indicative of each rela-tion.
Our approach has two stages.
First, we iden-tify pairs of highly associated verbs co-occurringon the Web with sufficient frequency using previ-ous work by Lin and Pantel (2001), as described inSection 4.4.
Next, for each verb pair, we testedlexico-syntactic patterns, calculating a score foreach possible semantic relation as described inSection 4.2.
Finally, as described in Section 4.3,we compare the strengths of the individual seman-tic relations and, preferring the most specific andthen strongest relations, output a consistent set asthe final output.
As a guide to consistency, we usea simple theory of semantics indicating which se-mantic relations are subtypes of other ones, andwhich are compatible and which are mutually ex-clusive.4.1 Lexico-syntactic patternsThe lexico-syntactic patterns were manually se-lected by examining pairs of verbs in known se-mantic relations.
They were refined to decreasecapturing wrong parts of speech or incorrect se-mantic relations.
We used 50 verb pairs and theoverall process took about 25 hours.We use a total of 35 patterns, which are listed inTable 2 along with the estimated frequency of hits.Table 1.
Semantic relations identified in VERBOCEAN.
Sib-lings in the WordNet column refers to terms with the sametroponymic parent, e.g.
swim and fly.SEMANTICRELATIONEXAMPLEAlignment withWordNetSymmetricsimilarity transform :: integrate synonyms or siblings Ystrength wound :: kill synonyms or siblings Nantonymy open :: close antonymy Yenablement fight :: win cause Nhappens-beforebuy :: sell;marry :: divorcecauseentailment, notemporal inclusionNNote that our patterns specify the tense of the verbsthey accept.
When instantiating these patterns, weconjugate as needed.
For example, ?both Xed andYed?
instantiates on sing and dance as ?both sungand danced?.4.2 Testing for a semantic relationIn this section, we describe how the presence ofa semantic relation is detected.
We test the rela-tions with patterns exemplified in Table 2.
Weadopt an approach inspired by mutual informationto measure the strength of association, denotedSp(V1, V2),  between three entities: a verb pair V1and V2 and a lexico-syntactic pattern p:)()()(),,(),(212121 VPVPpPVpVPVVS p ?
?=The probabilities in the denominator are difficultto calculate directly from search engine results.
Fora given lexico-syntactic pattern, we need to esti-mate the frequency of the pattern instantiated withappropriately conjugated verbs.
For verbs, we needto estimate the frequency of the verbs, but avoidcounting other parts-of-speech (e.g.
chair as anoun or painted as an adjective).
Another issue isthat some relations are symmetric (similarity andantonymy), while others are not (strength, enable-ment, happens-before).
For symmetric relationsonly, the verbs can fill the lexico-syntactic patternin either order.
To address these issues, we esti-mate Sp(V1,V2) using:NCVtohitsNCVtohitsNphitsNVpVhitsVVSvvestP ?????
)"(")"(")(),,(),(212121for asymmetric relations andNCVtohitsNCVtohitsNphitsNVpVhitsNVpVhitsVVSvvestP ????+?
)"(")"(")(*2),,(),,(),(21122121for symmetric relations.Here, hits(S) denotes the number of documentscontaining the string S, as returned by Google.
N isthe number of words indexed by the search engine(N ?
7.2 ?
1011), Cv is a correction factor to obtainthe frequency of the verb V in all tenses from thefrequency of the pattern ?to V?.
Based on severalverbs, we have estimated Cv = 8.5.
Because patterncounts, when instantiated with verbs, could not beestimated directly, we have computed the frequen-cies of the patterns in a part-of-speech tagged500M word corpus and used it to estimate the ex-pected number of hits hitsest(p) for each pattern.We estimated the N with a similar method.We say that the semantic relation Sp indicated bylexico-syntactic pattern p is present between V1and V2 ifSp(V1,V2) > C1As a result of tuning the system on a tuning set of50 verb pairs, C1 = 8.5.Additional test for asymmetric relations.
Forthe asymmetric relations, we require not only that),( 21 VVSP exceed a certain threshold, but thatthere be strong asymmetry of the relation:212211221),,(),,(),(),(CVpVhitsVpVhitsVVSVVSpp >=From the tuning set, C2 = 5.4.3 Pruning identified semantic relationsGiven a pair of semantic relations from the setwe identify, one of three cases can arise: (i) oneTable 2.
Semantic relations and the 35 surface patterns usedto identify them.
Total number of patterns for that relation isshown in parentheses.
In patterns, ?*?
matches any singleword.
Punctuation does not count as words by the searchengine used (Google).SEMANTICRELATIONSurfacePatternsHitsest forpatternsnarrowsimilarity (2)*X ie YXed ie Yed 219,480broadsimilarity (2)*Xed and Yedto X and Y 154,518,326strength (8)X even YXed even YedX and even YXed and even YedY or at least XYed or at least Xednot only Xed but Yednot just Xed but Yed1,016,905enablement (4)Xed * by Ying theXed * by Ying orto X * by Ying theto X * by Ying or2,348,392antonymy (7)either X or Yeither Xs or Yseither Xed or Yedeither Xing or Yingwhether to X or YXed * but Yedto X * but Y18,040,916happens-before(12)to X and then Yto X * and then YXed and then YedXed * and then Yedto X and later YXed and later Yedto X and subsequently YXed and subsequently Yedto X and eventually YXed and eventually Yed8,288,871*narrow- and broad- similarity overlap in their coverageand are treated as a single category, similarity, whenpostprocessed.
Narrow similarity tests for rare patternsand hitsest for it had to be approximated rather thanestimated from the smaller corpus.relation is more specific (strength is more specificthan similarity, enablement is more specific thanhappens-before), (ii) the relations are compatible(antonymy and happens-before), where presence ofone does not imply or rule out presence of theother, and (iii) the relations are incompatible (simi-larity and antonymy).It is not uncommon for our algorithm to identifypresence of several relations, with differentstrengths.
To produce the most likely output, weuse semantics of compatibility of the relations tooutput the most likely one(s).
The rules are asfollows:If the frequency was too low (less than 10 on thepattern ?X * Y?
OR ?Y * X?
OR ?X * * Y?
OR ?Y* * X?
), output that the statements are unrelatedand stop.If happens-before is detected, output presence ofhappens-before (additional relation may still beoutput, if detected).If happens-before is not detected, ignore detec-tion of enablement (because enablement is morespecific than happens-before, but is sometimesfalsely detected in the absence of happens-before).If strength is detected, score of similarity is ig-nored (because strength is more specific than simi-larity).Of the relations strength, similarity, oppositionand enablement which were detected (and not ig-nored), output the one with highest Sp.If nothing has been output to this point, outputunrelated.4.4 Extracting highly associated verb pairsTo exhaustively test the more than 64 millionunordered verb pairs for WordNet?s more than11,000 verbs would be computationally intractable.Instead, we use a set of highly associated verbpairs output by a paraphrasing algorithm calledDIRT (Lin and Pantel 2001).
Since we are able totest up to 4000 verb pairs per day on a single ma-chine (we issue at most 40 queries per test andeach query takes approximately 0.5 seconds), weare able to test several dozen associated verbs foreach verb in WordNet in a matter of weeks.Lin and Pantel (2001) describe an algorithmcalled DIRT (Discovery of Inference Rules fromText) that automatically learns paraphrase expres-sions from text.
It is a generalization of previousalgorithms that use the distributional hypothesis(Harris 1985) for finding similar words.
Instead ofapplying the hypothesis to words, Lin and Pantelapplied it to paths in dependency trees.
Essen-tially, if two paths tend to link the same sets ofwords, they hypothesized that the meanings of thecorresponding paths are similar.
It is from paths ofthe form subject-verb-object that we extract our setof associated verb pairs.
Hence, this paper is con-cerned only with relations between transitiveverbs.A path, extracted from a parse tree, is an expres-sion that represents a binary relation between twonouns.
A set of paraphrases was generated for eachpair of associated paths.
For example, using a1.5GB newspaper corpus, here are the 20 mostassociated paths to ?X solves Y?
generated byDIRT:Y is solved by X, X resolves Y, X findsa solution to Y, X tries to solve Y, Xdeals with Y, Y is resolved by X, X ad-dresses Y, X seeks a solution to Y, Xdoes something about Y, X solution toY, Y is resolved in X, Y is solvedthrough X, X rectifies Y, X copes withY, X overcomes Y, X eases Y, X tacklesY, X alleviates Y, X corrects Y, X is asolution to Y, X makes Y worse, X ironsout YThis list of associated paths looks tantalizinglyclose to the kind of axioms that would prove usefulin an inference system.
However, DIRT only out-puts pairs of paths that have some semantic rela-tion.
We used these as our set to extract finer-grained relations.5 Experimental resultsIn this section, we empirically evaluate the accu-racy of VERBOCEAN1.5.1 Experimental setupWe studied 29,165 pairs of verbs.
ApplyingDIRT to a 1.5GB newspaper corpus2, we extracted4000 paths that consisted of single verbs in therelation subject-verb-object (i.e.
paths of the form?X verb Y?)
whose verbs occurred in at least 150documents on the Web.
For example, from the 20most associated paths to ?X solves Y?
shown inSection 4.4, the following verb pairs were ex-tracted:solves :: resolvessolves :: addressessolves :: rectifiessolves :: overcomessolves :: easessolves :: tacklessolves :: corrects5.2 AccuracyWe classified each verb pair according to thesemantic relations described in Section 2.
If thesystem does not identify any semantic relation fora verb pair, then the system tags the pair as having1 VERBOCEAN is available for download athttp://semantics.isi.edu/ocean/.2 The 1.5GB corpus consists of San Jose Mercury,Wall Street Journal and AP Newswire articles from theTREC-9 collection.no relation.
To evaluate the accuracy of the sys-tem, we randomly sampled 100 of these verb pairs,and presented the classifications to two humanjudges.
The adjudicators were asked to judgewhether or not the system classification was ac-ceptable (i.e.
whether or not the relations output bythe system were correct).
Since the semantic rela-tions are not disjoint (e.g.
mop is both strongerthan and similar to sweep), multiple relations maybe appropriately acceptable for a given verb pair.The judges were also asked to identify their pre-ferred semantic relations (i.e.
those relations whichseem most plausible).
Table 3 shows five randomlyselected pairs along with the judges?
responses.The Appendix shows sample relationships discov-ered by the system.Table 4 shows the accuracy of the system.
Thebaseline system consists of labeling each pair withthe most common semantic relation, similarity,which occurs 33 times.
The Tags Correct columnrepresents the percentage of verb pairs whose sys-tem output relations were deemed correct.
ThePreferred Tags Correct column gives the percent-age of verb pairs whose system output relationsmatched exactly the human?s preferred relations.The Kappa statistic (Siegel and Castellan 1988) forthe task of judging system tags as correct and in-correct is ?
= 0.78 whereas the task of identifyingthe preferred semantic relation has ?
= 0.72.
Forthe latter task, the two judges agreed on 73 of the100 semantic relations.
73% gives an idea of anupper bound for humans on this task.
On these 73relations, the system achieved a higher accuracy of70.0%.
The system is allowed to output the hap-pens-before relation in combination with otherrelations.
On the 17 happens-before relations out-put by the system, 67.6% were judged correct.Ignoring the happens-before relations, we achieveda Tags Correct precision of 68%.Table 5 shows the accuracy of the system oneach of the relations.
The stronger-than relation isa subset of the similarity relation.
Considering acoarser extraction where stronger-than relationsare merged with similarity, the task of judgingsystem tags and the task of identifying the pre-ferred semantic relation both jump to 68.2% accu-racy.
Also, the overall accuracy of the systemclimbs to 68.5%.As described in Section 2, WordNet containsverb semantic relations.
A significant percentageof our discovered relations are not covered byWordNet?s coarser classifications.
Of the 40 verbpairs whose system relation was tagged as correctby both judges in our accuracy experiments andwhose tag was not ?no relation?, only 22.5% ofthem existed in a WordNet relation.5.3 DiscussionThe experience of extracting these semantic rela-tions has clarified certain important challenges.While relying on a search engine allows us toquery a corpus of nearly a trillion words, someissues arise: (i) the number of instances has to beapproximated by the number of hits (documents);(ii) the number of hits for the same query mayfluctuate over time; and (iii) some needed countsare not directly available.
We addressed the latterissue by approximating these counts using asmaller corpus.Table 3.
Five randomly selected pairs along with the system tag (in bold) and the judges?
responses.CORRECT PREFERRED SEMANTIC RELATIONPAIRS WITH SYSTEM TAG (IN BOLD) JUDGE 1 JUDGE 2 JUDGE 1 JUDGE 2X absolve Y is similar to X vindicate Y Yes Yes is similar to is similar toX bottom Y has no relation with X abate Y Yes Yes has no relation with has no relation withX outrage Y happens-after / is stronger than X shock Y Yes Yes happens-before / isstronger thanhappens-before/ is strongerthanX pool Y has no relation with X increase Y Yes No has no relation with can result inX insure Y is similar to X expedite Y No No has no relation with has no relation withTable 4.
Accuracy of system-discovered relations.ACCURACYTagsCorrectPreferred TagsCorrectBaselineCorrectJudge 1 66% 54% 24%Judge 2 65% 52% 20%Average 65.5% 53% 22%Table 5.
Accuracy of each semantic relation.SEMANTICRELATIONSYSTEMTAGSTagsCorrectPreferred TagsCorrectsimilarity 41 63.4% 40.2%strength 14 75.0% 75.0%antonymy 8 50.0% 43.8%enablement 2 100% 100%no relation 35 72.9% 72.9%happens before 17 67.6% 55.9%We do not detect entailment with lexico-syntactic patterns.
In fact, we propose that whetherthe entailment relation holds between V1 and V2depends on the absence of another verb V1' in thesame relationship with V2.
For example, given therelation marry happens-before divorce, we canconclude that divorce entails marry.
But, given therelation buy happens-before sell, we cannot con-clude entailment since manufacture can also hap-pen before sell.
This also applies to the enablementand strength relations.Corpus-based methods, including ours, hold thepromise of wide coverage but are weak on dis-criminating senses.
While we hope that applica-tions will benefit from this resource as is, an inter-esting next step would be to augment it with senseinformation.6 Future workThere are several ways to improve the accuracyof the current algorithm and to detect relationsbetween low frequency verb pairs.
One avenuewould be to automatically learn or manually craftmore patterns and to extend the pattern vocabulary(when developing the system, we have noticed thatdifferent registers and verb types require differentpatterns).
Another possibility would be to usemore relaxed patterns when the part of speechconfusion is not likely (e.g.
?eat?
is a commonverb which does not have a noun sense, and pat-terns need not protect against noun senses whentesting such verbs).Our approach can potentially be extended tomultiword paths.
DIRT actually provides twoorders of magnitude more relations than the 29,165single verb relations (subject-verb-object) we ex-tracted.
On the same 1GB corpus described in Sec-tion 5.1, DIRT extracted over 200K paths and 6Munique paraphrases.
These provide an opportunityto create a much larger corpus of semantic rela-tions, or to construct smaller, in-depth resourcesfor selected subdomains.
For example, we couldextract that take a trip to is similar to travel to, andthat board a plane happens before deplane.If the entire database is viewed as a graph, wecurrently leverage and enforce only local consis-tency.
It would be useful to enforce global consis-tency, e.g.
V1 stronger-than V2, and V2 stronger-than V3 indicates that V1 stronger-than V3, whichmay be leveraged to identify additional relations orinconsistent relations (e.g.
V3 stronger-than V1).Finally, as discussed in Section 5.3, entailmentrelations may be derivable by processing the com-plete graph of the identified semantic relation.7 ConclusionsWe have demonstrated that certain fine-grainedsemantic relations between verbs are present on theWeb, and are extractable with a simple pattern-based approach.
In addition to discovering rela-tions identified in WordNet, such as opposition andenablement, we obtain strong results on strengthrelations (for which no wide-coverage resource isavailable).
On a set of 29,165 associated verbpairs, experimental results show an accuracy of65.5% in assigning similarity, strength, antonymy,enablement, and happens-before.Further work may refine extraction methods andfurther process the mined semantics to derive otherrelations such as entailment.We hope to open the way to inferring implied,but not stated assertions and to benefit applicationssuch as question answering, information retrieval,and summarization.AcknowledgmentsThe authors wish to thank the reviewers for theirhelpful comments and Google Inc. for supportinghigh volume querying of their index.
This researchwas partly supported by NSF grant #EIA-0205111.ReferencesBaker, C.; Fillmore, C.; and Lowe, J.
1998.
TheBerkeley FrameNet project.
In Proceedings ofCOLING-ACL.
Montreal, Canada.Barker, K.; and Szpakowicz, S. 1995.
InteractiveSemantic Analysis of Clause-LevelRelationships.
In Proceedings of PACLING `95.Brisbane.Barwise, J. and Perry, J.
1985.
Semantic innocenceand uncompromising situations.
In: Martinich,A.
P.
(ed.)
The Philosophy of Language.
NewYork: Oxford University Press.
pp.
401?413.Barzilay, R.; Elhadad, N.; and McKeown, K. 2002.Inferring strategies for sentence ordering inmultidocument summarization.
JAIR, 17:35?55.Cruse, D. 1992 Antonymy Revisited: SomeThoughts on the Relationship between Wordsand Concepts, in A. Lehrer and E.V.
Kittay(eds.
), Frames, Fields, and Contrasts, Hillsdale,NJ, Lawrence Erlbaum associates, pp.
289-306.Etzioni, O.; Cafarella, M.; Downey, D.; Kok, S.;Popescu, A.; Shaked, T.; Soderland, S.; Weld,D.
; and Yates, A.
2004.
Web-scale informationextraction in KnowItAll.
To appear in WWW-2004.Fellbaum, C. 1998.
Semantic network of Englishverbs.
In Fellbaum, (ed).
WordNet: AnElectronic Lexical Database, MIT Press.Gomez, F. 2001.
An Algorithm for Aspects ofSemantic Interpretation Using an EnhancedWordNet.
In NAACL-2001, CMU, Pittsburgh.Harris, Z.
1985.
Distributional Structure.
In: Katz,J.
J.
(ed.
), The Philosophy of Linguistics.
NewYork: Oxford University Press.
pp.
26?47.Hearst, M. 1992.
Automatic acquisition ofhyponyms from large text corpora.
In COLING-92.
pp.
539?545.
Nantes, France.Keller, F.and Lapata, M. 2003.
Using the Web toObtain Frequencies for Unseen Bigrams.Computational Linguistics 29:3.Kingsbury, P; Palmer, M.; and Marcus, M. 2002.Adding semantic annotation to the PennTreeBank.
In Proceedings of HLT-2002.
SanDiego, California.Kipper, K.; Dang, H.; and Palmer, M. 2000.
Class-based construction of a verb lexicon.
In Pro-ceedings of AAAI-2000.
Austin, TX.Klavans, J. and Kan, M. 1998.
Document classi-fication: Role of verb in document analysis.
InProceedings COLING-ACL '98.
Montreal,Canada.Levin, B.
1993.
English Verb Classes andAlternations: A Preliminary Investigation.University of Chicago Press, Chicago, IL.Lin, C-Y.
1997.
Robust Automated TopicIdentification.
Ph.D. Thesis.
University ofSouthern California.Lin, D. and Pantel, P. 2001.
Discovery of inferencerules for question answering.
Natural LanguageEngineering, 7(4):343?360.Lin, D.; Zhao, S.; Qin, L.; and Zhou, M. 2003.Identifying synonyms among distributionallysimilar words.
In Proceedings of IJCAI-03.pp.1492?1493.
Acapulco, Mexico.Miller, G. 1990.
WordNet: An online lexicaldatabase.
International Journal of Lexicography,3(4).Moldovan, D.; Badulescu, A.; Tatu, M.; Antohe,D.
; and Girju, R. 2004.
Models for the semanticclassification of noun phrases.
To appear inProceedings of HLT/NAACL-2004 Workshop onComputational Lexical Semantics.Moldovan, D.; Harabagiu, S.; Girju, R.;Morarescu, P.; Lacatusu, F.; Novischi, A.;Badulescu, A.; and Bolohan, O.
2002.
LCC toolsfor question answering.
In Notebook of theEleventh Text REtrieval Conference (TREC-2002).
pp.
144?154.Palmer, M., Wu, Z.
1995.
Verb Semantics forEnglish-Chinese Translation MachineTranslation, 9(4).Pantel, P. and Ravichandran, D. 2004.Automatically labeling semantic classes.
Toappear in Proceedings of HLT/NAACL-2004.Boston, MA.Ravichandran, D. and Hovy, E., 2002.
Learningsurface text patterns for a question answeringsystem.
In Proceedings of ACL-02.Philadelphia, PA.Richardson, S.; Dolan, W.; and Vanderwende, L.1998.
MindNet: acquiring and structuringsemantic information from text.
In Proceedingsof COLING '98.Rus, V. 2002.
Logic Forms for WordNet Glosses.Ph.D.
Thesis.
Southern Methodist University.Schank, R. and Abelson, R. 1977.
Scripts, Plans,Goals and Understanding: An Inquiry intoHuman Knowledge Structures.
LawrenceErlbaum Associates.Siegel, S. and Castellan Jr., N. 1988.Nonparametric Statistics for the BehavioralSciences.
McGraw-Hill.Turney, P. 2001.
Mining the Web for synonyms:PMI-IR versus LSA on TOEFL.
In ProceedingsECML-2001.
Freiburg, Germany.Webber, B.; Gardent, C.; and Bos, J.
2002.Position statement: Inference in questionanswering.
In Proceedings of LREC-2002.
LasPalmas, Spain.Appendix.
Sample relations extracted by our system.SEMANTICRELATION EXAMPLESSEMANTICRELATION EXAMPLESSEMANTICRELATION EXAMPLESsimilaritymaximize :: enhanceproduce :: createreduce :: restrictenablementassess :: reviewaccomplish :: completedouble-click :: clickhappensbeforedetain :: prosecuteenroll :: graduateschedule :: reschedulestrengthpermit :: authorizesurprise :: startlestartle :: shockantonymyassemble :: dismantleregard :: condemnroast :: fry
