Proceedings of the ACL Student Research Workshop, pages 31?37,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsSurvey on parsing three dependency representations for EnglishAngelina Ivanova Stephan Oepen Lilja ?vrelidUniversity of Oslo, Department of Informatics{angelii |oe |liljao }@ifi.uio.noAbstractIn this paper we focus on practical is-sues of data representation for dependencyparsing.
We carry out an experimentalcomparison of (a) three syntactic depen-dency schemes; (b) three data-driven de-pendency parsers; and (c) the influence oftwo different approaches to lexical cate-gory disambiguation (aka tagging) prior toparsing.
Comparing parsing accuracies invarious setups, we study the interactionsof these three aspects and analyze whichconfigurations are easier to learn for a de-pendency parser.1 IntroductionDependency parsing is one of the mainstream re-search areas in natural language processing.
De-pendency representations are useful for a numberof NLP applications, for example, machine trans-lation (Ding and Palmer, 2005), information ex-traction (Yakushiji et al 2006), analysis of ty-pologically diverse languages (Bunt et al 2010)and parser stacking (?vrelid et al 2009).
Therewere several shared tasks organized on depen-dency parsing (CoNLL 2006?2007) and labeleddependencies (CoNLL 2008?2009) and there werea number of attempts to compare various depen-dencies intrinsically, e.g.
(Miyao et al 2007), andextrinsically, e.g.
(Wu et al 2012).In this paper we focus on practical issues of datarepresentation for dependency parsing.
The cen-tral aspects of our discussion are (a) three depen-dency formats: two ?classic?
representations fordependency parsing, namely, Stanford Basic (SB)and CoNLL Syntactic Dependencies (CD), andbilexical dependencies from the HPSG EnglishResource Grammar (ERG), so-called DELPH-INSyntactic Derivation Tree (DT), proposed recentlyby Ivanova et al(2012); (b) three state-of-the artstatistical parsers: Malt (Nivre et al 2007), MST(McDonald et al 2005) and the parser of Bohnetand Nivre (2012); (c) two approaches to word-category disambiguation, e.g.
exploiting commonPTB tags and using supertags (i.e.
specializedERG lexical types).We parse the formats and compare accuraciesin all configurations in order to determine howparsers, dependency representations and grammat-ical tagging methods interact with each other inapplication to automatic syntactic analysis.SB and CD are derived automatically fromphrase structures of Penn Treebank to accommo-date the needs of fast and accurate dependencyparsing, whereas DT is rooted in the formal gram-mar theory HPSG and is independent from anyspecific treebank.
For DT we gain more expres-sivity from the underlying linguistic theory, whichchallenges parsing with statistical tools.
The struc-tural analysis of the schemes in Ivanova et al(2012) leads to the hypothesis that CD and DTare more similar to each other than SB to DT.We recompute similarities on a larger treebank andcheck whether parsing results reflect them.The paper has the following structure: anoverview of related work is presented in Sec-tion 2; treebanks, tagsets, dependency schemesand parsers used in the experiments are introducedin Section 3; analysis of parsing results is dis-cussed in Section 4; conclusions and future workare outlined in Section 5.2 Related workSchwartz et al(2012) investigate which depen-dency representations of several syntactic struc-tures are easier to parse with supervised ver-sions of the Klein and Manning (2004) parser,ClearParser (Choi and Nicolov, 2009), MSTParser, Malt and the Easy First Non-directionalparser (Goldberg and Elhadad, 2010).
The resultsimply that all parsers consistently perform betterwhen (a) coordination has one of the conjuncts asthe head rather than the coordinating conjunction;31A , B and C A , B and C A, B and CFigure 1: Annotation of coordination structure in SB, CD and DT (left to right) dependency formats(b) the noun phrase is headed by the noun ratherthan by determiner; (c) prepositions or subordinat-ing conjunctions, rather than their NP or clause ar-guments, serve as the head in prepositional phraseor subordinated clauses.
Therefore we can expect(a) Malt and MST to have fewer errors on coor-dination structures parsing SB and CD than pars-ing DT, because SB and CD choose the first con-junct as the head and DT chooses the coordinatingconjunction as the head; (b,c) no significant dif-ferences for the errors on noun and prepositionalphrases, because all three schemes have the nounas the head of the noun phrase and the prepositionas the head of the prepositional phrase.Miwa et al(2010) present intristic and extris-tic (event-extraction task) evaluation of six parsers(GDep, Bikel, Stanford, Charniak-Johnson, C&Cand Enju parser) on three dependency formats(Stanford Dependencies, CoNLL-X, and EnjuPAS).
Intristic evaluation results show that allparsers have the highest accuracies with theCoNLL-X format.3 Data and software3.1 TreebanksFor the experiments in this paper we used the PennTreebank (Marcus et al 1993) and the Deep-Bank (Flickinger et al 2012).
The latter is com-prised of roughly 82% of the sentences of the first16 sections of the Penn Treebank annotated withfull HPSG analyses from the English ResourceGrammar (ERG).
The DeepBank annotations arecreated on top of the raw text of the PTB.
Due toimperfections of the automatic tokenization, thereare some token mismatches between DeepBankand PTB.
We had to filter out such sentences tohave consistent number of tokens in the DT, SBand CD formats.
For our experiments we hadavailable a training set of 22209 sentences and atest set of 1759 sentences (from Section 15).3.2 ParsersIn the experiments described in Section 4 we usedparsers that adopt different approaches and imple-ment various algorithms.Malt (Nivre et al 2007): transition-based de-pendency parser with local learning and greedysearch.MST (McDonald et al 2005): graph-baseddependency parser with global near-exhaustivesearch.Bohnet and Nivre (2012) parser: transition-based dependency parser with joint tagger that im-plements global learning and beam search.3.3 Dependency schemesIn this work we extract DeepBank data in the formof bilexical syntactic dependencies, DELPH-INSyntactic Derivation Tree (DT) format.
We ob-tain the exact same sentences in Stanford Basic(SB) format from the automatic conversion of thePTB with the Stanford parser (de Marneffe et al2006) and in the CoNLL Syntactic Dependencies(CD) representation using the LTH Constituent-to-Dependency Conversion Tool for Penn-styleTreebanks (Johansson and Nugues, 2007).SB and CD represent the way to convert PTBto bilexical dependencies; in contrast, DT isgrounded in linguistic theory and captures deci-sions taken in the grammar.
Figure 1 demonstratesthe differences between the formats on the coor-dination structure.
According to Schwartz et al(2012), analysis of coordination in SB and CD iseasier for a statistical parser to learn; however, aswe will see in section 4.3, DT has more expressivepower distinguishing structural ambiguities illus-trated by the classic example old men and women.3.4 Part-of-speech tagsWe experimented with two tag sets: PTB tags andlexical types of the ERG grammar - supertags.PTB tags determine the part of speech (PoS)and some morphological features, such as num-ber for nouns, degree of comparison for adjectivesand adverbs, tense and agreement with person andnumber of subject for verbs, etc.Supertags are composed of part-of-speech, va-lency in the form of an ordered sequence ofcomplements, and annotations that encompasscategory-internal subdivisions, e.g.
mass vs. countvs.
proper nouns, intersective vs. scopal adverbs,32or referential vs. expletive pronouns.
Example ofa supertag: v np is le (verb ?is?
that takes nounphrase as a complement).There are 48 tags in the PTB tagset and 1091supertags in the set of lexical types of the ERG.The state-of-the-art accuracy of PoS-tagging onin-domain test data using gold-standard tokeniza-tion is roughly 97% for the PTB tagset and ap-proximately 95% for the ERG supertags (Ytrest?l,2011).
Supertagging for the ERG grammar is anongoing research effort and an off-the-shelf su-pertagger for the ERG is not currently available.4 ExperimentsIn this section we give a detailed analysis of pars-ing into SB, CD and DT dependencies with Malt,MST and the Bohnet and Nivre (2012) parser.4.1 SetupFor Malt and MST we perform the experimentson gold PoS tags, whereas the Bohnet and Nivre(2012) parser predicts PoS tags during testing.Prior to each experiment with Malt, we usedMaltOptimizer to obtain settings and a featuremodel; for MST we exploited default configura-tion; for the Bohnet and Nivre (2012) parser weset the beam parameter to 80 and otherwise em-ployed the default setup.With regards to evaluation metrics we use la-belled attachment score (LAS), unlabeled attach-ment score (UAS) and label accuracy (LACC) ex-cluding punctuation.
Our results cannot be di-rectly compared to the state-of-the-art scores onthe Penn Treebank because we train on sections0-13 and test on section 15 of WSJ.
Also our re-sults are not strictly inter-comparable because thesetups we are using are different.4.2 DiscussionThe results that we are going to analyze are pre-sented in Tables 1 and 2.
Statistical significancewas assessed using Dan Bikel?s parsing evaluationcomparator1 at the 0.001 significance level.
Weinspect three different aspects in the interpretationof these results: parser, dependency format andtagset.
Below we will look at these three anglesin detail.From the parser perspective Malt and MST arenot very different in the traditional setup with gold1http://nextens.uvt.nl/depparse-wiki/SoftwarePage#scoringPTB tags (Table 1, Gold PTB tags).
The Bohnetand Nivre (2012) parser outperforms Malt on CDand DT and MST on SB, CD and DT with PTBtags even though it does not receive gold PTB tagsduring test phase but predicts them (Table 2, Pre-dicted PTB tags).
This is explained by the fact thatthe Bohnet and Nivre (2012) parser implements anovel approach to parsing: beam-search algorithmwith global structure learning.MST ?loses?
more than Malt when parsing SBwith gold supertags (Table 1, Gold supertags).This parser exploits context features ?POS tag ofeach intervening word between head and depen-dent?
(McDonald et al 2006).
Due to the farlarger size of the supertag set compared to the PTBtagset, such features are sparse and have low fre-quencies.
This leads to the lower scores of pars-ing accuracy for MST.
For the Bohnet and Nivre(2012) parser the complexity of supertag predic-tion has significant negative influence on the at-tachment and labeling accuracies (Table 2, Pre-dicted supertags).
The addition of gold PTB tagsas a feature lifts the performance of the Bohnetand Nivre (2012) parser to the level of perfor-mance of Malt and MST on CD with gold su-pertags and Malt on SB with gold supertags (com-pare Table 2, Predicted supertags + gold PTB, andTable 1, Gold supertags).Both Malt and MST benefit slightly from thecombination of gold PTB tags and gold supertags(Table 1, Gold PTB tags + gold supertags).
Forthe Bohnet and Nivre (2012) parser we also ob-serve small rise of accuracy when gold supertagsare provided as a feature for prediction of PTBtags (compare Predicted PTB tags and PredictedPTB tags + gold supertags sections of Table 2).The parsers have different running times: ittakes minutes to run an experiment with Malt,about 2 hours with MST and up to a day with theBohnet and Nivre (2012) parser.From the point of view of the dependency for-mat, SB has the highest LACC and CD is first-rateon UAS for all three parsers in most of the con-figurations (Tables 1 and 2).
This means that SBis easier to label and CD is easier to parse struc-turally.
DT appears to be a more difficult targetformat because it is both hard to label and attachin most configurations.
It is not an unexpected re-sult, since SB and CD are both derived from PTBphrase-structure trees and are oriented to ease de-pendency parsing task.
DT is not custom-designed33Gold PTB tagsLAS UAS LACCMalt MST Malt MST Malt MSTSB 89.21 88.59 90.95 90.88 93.58 92.79CD 88.74 88.72 91.89 92.01 91.29 91.34DT 85.97 86.36 89.22 90.01 88.73 89.22Gold supertagsLAS UAS LACCMalt MST Malt MST Malt MSTSB 87.76 85.25 90.63 88.56 92.38 90.29CD 88.22 87.27 91.17 90.41 91.30 90.74DT 89.92 89.58 90.96 90.56 92.50 92.64Gold PTB tags + gold supertagsLAS UAS LACCMalt MST Malt MST Malt MSTSB 90.321 89.431 91.901 91.842 94.481 93.261CD 89.591 89.372 92.431 92.772 92.321 92.072DT 90.691 91.192 91.831 92.332 93.101 93.692Table 1: Parsing results of Malt and MST onStanford Basic (SB), CoNLL Syntactic De-pendencies (CD) and DELPH-IN SyntacticDerivation Tree (DT) formats.
Punctuation isexcluded from the scoring.
Gold PTB tags:Malt and MST are trained and tested on goldPTB tags.
Gold supertags: Malt and MSTare trained and tested on gold supertags.
GoldPTB tags + gold supertags: Malt and MST aretrained on gold PTB tags and gold supertags.1 denotes a feature model in which gold PTBtags function as PoS and gold supertags actas additional features (in CPOSTAG field); 2stands for the feature model which exploitsgold supertags as PoS and uses gold PTB tagsas extra features (in CPOSTAG field).Predicted PTB tagsLAS UAS LACCBohnet and Nivre (2012)SB 89.56 92.36 93.30CD 89.77 93.01 92.10DT 88.26 91.63 90.72Predicted supertagsLAS UAS LACCBohnet and Nivre (2012)SB 85.41 89.38 90.17CD 86.73 90.73 89.72DT 85.76 89.50 88.56Pred.
PTB tags + gold supertagsLAS UAS LACCBohnet and Nivre (2012)SB 90.32 93.01 93.85CD 90.55 93.56 92.79DT 91.51 92.99 93.88Pred.
supertags + gold PTBLAS UAS LACCBohnet and Nivre (2012)SB 87.20 90.07 91.81CD 87.79 91.47 90.62DT 86.31 89.80 89.17Table 2: Parsing results of the Bohnetand Nivre (2012) parser on Stanford Ba-sic (SB), CoNLL Syntactic Dependencies(CD) and DELPH-IN Syntactic Deriva-tion Tree (DT) formats.
Parser is trainedon gold-standard data.
Punctuation is ex-cluded from the scoring.
Predicted PTB:parser predicts PTB tags during the testphase.
Predicted supertags: parser pre-dicts supertags during the test phase.
Pre-dicted PTB + gold supertags: parser re-ceives gold supertags as feature and pre-dicts PTB tags during the test phase.
Pre-dicted supertags + gold PTB: parser re-ceives PTB tags as feature and predictssupertags during test phase.34to dependency parsing and is independent fromparsing questions in this sense.
Unlike SB andCD, it is linguistically informed by the underlying,full-fledged HPSG grammar.The Jaccard similarity on our training set is 0.57for SB and CD, 0.564 for CD and DT, and 0.388for SB and DT.
These similarity values show thatCD and DT are structurally closer to each otherthan SB and DT.
Contrary to our expectations, theaccuracy scores of parsers do not suggest that CDand DT are particularly similar to each other interms of parsing.Inspecting the aspect of tagset we conclude thattraditional PTB tags are compatible with SB andCD but do not fit the DT scheme well, while ERGsupertags are specific to the ERG framework anddo not seem to be appropriate for SB and CD.
Nei-ther of these findings seem surprising, as PTB tagswere developed as part of the treebank from whichCD and SB are derived; whereas ERG supertagsare closely related to the HPSG syntactic struc-tures captured in DT.
PTB tags were designed tosimplify PoS-tagging whereas supertags were de-veloped to capture information that is required toanalyze syntax of HPSG.For each PTB tag we collected correspondingsupertags from the gold-standard training set.
Foropen word classes such as nouns, adjectives, ad-verbs and verbs the relation between PTB tagsand supertags is many-to-many.
Unique one-to-many correspondence holds only for possessivewh-pronoun and punctuation.Thus, supertags do not provide extra level ofdetalization for PTB tags, but PTB tags and su-pertags are complementary.
As discussed in sec-tion 3.4, they contain bits of information that aredifferent.
For this reason their combination re-sults in slight increase of accuracy for all threeparsers on all dependency formats (Table 1, GoldPTB tags + gold supertags, and Table 2, PredictedPTB + gold supertags and Predicted supertags +gold PTB).
The Bohnet and Nivre (2012) parserpredicts supertags with an average accuracy of89.73% which is significantly lower than state-of-the-art 95% (Ytrest?l, 2011).When we consider punctuation in the evalua-tion, all scores raise significantly for DT and atthe same time decrease for SB and CD for all threeparsers.
This is explained by the fact that punctu-ation in DT is always attached to the nearest tokenwhich is easy to learn for a statistical parser.4.3 Error analysisUsing the CoNLL-07 evaluation script2 on our testset, for each parser we obtained the error rate dis-tribution over CPOSTAG on SB, CD and DT.VBP, VBZ and VBG.
VBP (verb, non-3rdperson singular present), VBZ (verb, 3rd per-son singular present) and VBG (verb, gerund orpresent participle) are the PTB tags that have errorrates in 10 highest error rates list for each parser(Malt, MST and the Bohnet and Nivre (2012)parser) with each dependency format (SB, CDand DT) and with each PoS tag set (PTB PoSand supertags) when PTB tags are included asCPOSTAG feature.
We automatically collected allsentences that contain 1) attachment errors, 2) la-bel errors, 3) attachment and label errors for VBP,VBZ and VBG made by Malt parser on DT formatwith PTB PoS.
For each of these three lexical cat-egories we manually analyzed a random sampleof sentences with errors and their correspondinggold-standard versions.In many cases such errors are related to the rootof the sentence when the verb is either treated ascomplement or adjunct instead of having a rootstatus or vice versa.
Errors with these groups ofverbs mostly occur in the complex sentences thatcontain several verbs.
Sentences with coordina-tion are particularly difficult for the correct attach-ment and labeling of the VBP (see Figure 2 for anexample).Coordination.
The error rate of Malt, MST andthe Bohnet and Nivre (2012) parser for the coor-dination is not so high for SB and CD ( 1% and2% correspondingly with MaltParser, PTB tags)whereas for DT the error rate on the CPOSTAGSis especially high (26% with MaltParser, PTBtags).
It means that there are many errors onincoming dependency arcs for coordinating con-junctions when parsing DT.
On outgoing arcsparsers also make more mistakes on DT than onSB and CD.
This is related to the difference inchoice of annotation principle (see Figure 1).
Asit was shown in (Schwartz et al 2012), it is harderto parse coordination headed by coordinating con-junction.Although the approach used in DT is harder forparser to learn, it has some advantages: using SBand CD annotations, we cannot distinguish the twocases illustrated with the sentences (a) and (b):2http://nextens.uvt.nl/depparse-wiki/SoftwarePage#scoring35VBP VBD VBDThe figures show that spending rose 0.1 % in the third quarter <.
.
.> and was up 3.8 % from a year ago .rootSB-HDVP-VPHD-CMPMRK-NHrootSP-HDHD-CMP Cl-CLMRK-NHFigure 2: The gold-standard (in green above the sentence) and the incorrect Malt?s (in red below thesentence) analyses of the utterance from the DeepBank in DT format with PTB PoS tagsa) The fight is putting a tight squeeze on prof-its of many, threatening to drive the small-est ones out of business and straining rela-tions between the national fast-food chainsand their franchisees.b) Proceeds from the sale will be used for re-modelling and reforbishing projects, as wellas for the planned MGM Grand hotel/casinoand theme park.In the sentence a) ?the national fast-food?
refersonly to the conjunct ?chains?, while in the sen-tence b) ?the planned?
refers to both conjuncts and?MGM Grand?
refers only to the first conjunct.The Bohnet and Nivre (2012) parser succeeds infinding the correct conjucts (shown in bold font)on DT and makes mistakes on SB and CD in somedifficult cases like the following ones:a) <.
.
.> investors hoard gold and help under-pin its price <.
.
.>b) Then take the expected return and subtractone standard deviation.CD and SB wrongly suggest ?gold?
and ?help?
tobe conjoined in the first sentence and ?return?
and?deviation?
in the second.5 Conclusions and future workIn this survey we gave a comparative experi-mental overview of (i) parsing three dependencyschemes, viz., Stanford Basic (SB), CoNLL Syn-tactic Dependencies (CD) and DELPH-IN Syn-tactic Derivation Tree (DT), (ii) with three lead-ing dependency parsers, viz., Malt, MST and theBohnet and Nivre (2012) parser (iii) exploitingtwo different tagsets, viz., PTB tags and supertags.From the parser perspective, the Bohnet andNivre (2012) parser performs better than Malt andMST not only on conventional formats but also onthe new representation, although this parser solvesa harder task than Malt and MST.From the dependency format perspective, DTappeares to be a more difficult target dependencyrepresentation than SB and CD.
This suggests thatthe expressivity that we gain from the grammartheory (e.g.
for coordination) is harder to learnwith state-of-the-art dependency parsers.
CD andDT are structurally closer to each other than SBand DT; however, we did not observe sound evi-dence of a correlation between structural similar-ity of CD and DT and their parsing accuraciesRegarding the tagset aspect, it is natural thatPTB tags are good for SB and CD, whereas themore fine-grained set of supertags fits DT bet-ter.
PTB tags and supertags are complementary,and for all three parsers we observe slight benefitsfrom being supplied with both types of tags.As future work we would like to run more ex-periments with predicted supertags.
In the absenceof a specialized supertagger, we can follow thepipeline of (Ytrest?l, 2011) who reached the state-of-the-art supertagging accuracy of 95%.Another area of our interest is an extrinsic eval-uation of SB, CD and DT, e.g.
applied to semanticrole labeling and question-answering in order tofind out if the usage of the DT format groundedin the computational grammar theory is beneficialfor such tasks.AcknowledgmentsThe authors would like to thank Rebecca Dridan,Joakim Nivre, Bernd Bohnet, Gertjan van Noordand Jelke Bloem for interesting discussions andthe two anonymous reviewers for comments onthe work.
Experimentation was made possiblethrough access to the high-performance comput-ing resources at the University of Oslo.36ReferencesBernd Bohnet and Joakim Nivre.
2012.
A transition-based system for joint part-of-speech tagging andlabeled non-projective dependency parsing.
InEMNLP-CoNLL, pages 1455?1465.
ACL.Harry Bunt, Paola Merlo, and Joakim Nivre, editors.2010.
Trends in Parsing Technology.
Springer Ver-lag, Stanford.Jinho D Choi and Nicolas Nicolov.
2009.
K-best, lo-cally pruned, transition-based dependency parsingusing robust risk minimization.
Recent Advances inNatural Language Processing V, pages 205?216.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure trees.
InLREC.Yuan Ding and Martha Palmer.
2005.
Machine trans-lation using probabilistic synchronous dependencyinsertion grammars.
In Proceedings of the 43rd An-nual Meeting of the Association for ComputationalLinguistics (ACL?05), pages 541?548, Ann Arbor,Michigan, June.
Association for Computational Lin-guistics.Daniel Flickinger, Yi Zhang, and Valia Kordoni.
2012.DeepBank: a Dynamically Annotated Treebank ofthe Wall Street Journal.
In Proceedings of theEleventh International Workshop on Treebanks andLinguistic Theories, pages 85?96.
Edies Colibri.Yoav Goldberg and Michael Elhadad.
2010.
An effi-cient algorithm for easy-first non-directional depen-dency parsing.
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, HLT ?10, pages 742?750, Stroudsburg, PA,USA.
Association for Computational Linguistics.Angelina Ivanova, Stephan Oepen, Lilja ?vrelid, andDan Flickinger.
2012. Who did what to whom?a contrastive study of syntacto-semantic dependen-cies.
In Proceedings of the Sixth Linguistic Annota-tion Workshop, pages 2?11, Jeju, Republic of Korea,July.
Association for Computational Linguistics.Richard Johansson and Pierre Nugues.
2007.
Ex-tended constituent-to-dependency conversion forEnglish.
In Proceedings of NODALIDA 2007, pages105?112, Tartu, Estonia, May 25-26.Dan Klein and Christopher D. Manning.
2004.Corpus-based induction of syntactic structure: mod-els of dependency and constituency.
In Proceedingsof the 42nd Annual Meeting on Association for Com-putational Linguistics, ACL ?04, Stroudsburg, PA,USA.
Association for Computational Linguistics.Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of English: the Penn Treebank.
Com-putational Linguistics, 19(2):313?330, June.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic?.
2005.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proceedingsof the conference on Human Language Technologyand Empirical Methods in Natural Language Pro-cessing, HLT ?05, pages 523?530, Stroudsburg, PA,USA.
Association for Computational Linguistics.Ryan McDonald, Kevin Lerman, and Fernando Pereira.2006.
Multilingual dependency analysis with a two-stage discriminative parser.
In Proceedings of theTenth Conference on Computational Natural Lan-guage Learning, CoNLL-X ?06, pages 216?220,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, andJun ichi Tsujii.
2010.
Evaluating dependency repre-sentations for event extraction.
In Chu-Ren Huangand Dan Jurafsky, editors, COLING, pages 779?787.Tsinghua University Press.Yusuke Miyao, Kenji Sagae, and Jun?ichi Tsujii.
2007.Towards framework-independent evaluation of deeplinguistic parsers.
In Ann Copestake, editor, Pro-ceedings of the GEAF 2007 Workshop, CSLI Studiesin Computational Linguistics Online, page 21 pages.CSLI Publications.Joakim Nivre, Johan Hall, Jens Nilsson, AtanasChanev, Gu?lsen Eryigit, Sandra Ku?bler, SvetoslavMarinov, and Erwin Marsi.
2007.
MaltParser:A language-independent system for data-driven de-pendency parsing.
Natural Language Engineering,13(2):95?135.Lilja ?vrelid, Jonas Kuhn, and Kathrin Spreyer.
2009.Cross-framework parser stacking for data-driven de-pendency parsing.
TAL, 50(3):109?138.Roy Schwartz, Omri Abend, and Ari Rappoport.
2012.Learnability-based syntactic annotation design.
InProc.
of the 24th International Conference on Com-putational Linguistics (Coling 2012), Mumbai, In-dia, December.
Coling 2012 Organizing Committee.Xianchao Wu, Katsuhito Sudoh, Kevin Duh, HajimeTsukada, and Masaaki Nagata.
2012.
A Compara-tive Study of Target Dependency Structures for Sta-tistical Machine Translation.
In ACL (2), pages 100?104.
The Association for Computer Linguistics.Akane Yakushiji, Yusuke Miyao, Tomoko Ohta, YukaTateisi, and Jun?ichi Tsujii.
2006.
Automatic con-struction of predicate-argument structure patternsfor biomedical information extraction.
In Proceed-ings of the 2006 Conference on Empirical Meth-ods in Natural Language Processing, EMNLP ?06,pages 284?292, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Gisle Ytrest?l.
2011.
Cuteforce: deep deterministicHPSG parsing.
In Proceedings of the 12th Interna-tional Conference on Parsing Technologies, IWPT?11, pages 186?197, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.37
