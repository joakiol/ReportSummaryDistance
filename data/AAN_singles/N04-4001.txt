Using N-Grams to Understand the Nature of SummariesMichele Banko and Lucy VanderwendeOne Microsoft WayRedmond, WA 98052{mbanko, lucyv}@microsoft.comAbstractAlthough single-document summarization is awell-studied task, the nature of multi-document summarization is only beginning tobe studied in detail.
While close attention hasbeen paid to what technologies are necessarywhen moving from single to multi-documentsummarization, the properties of human-written multi-document summaries have notbeen quantified.
In this paper, we empiricallycharacterize human-written summariesprovided in a widely used summarizationcorpus by attempting to answer the questions:Can multi-document summaries that arewritten by humans be characterized asextractive or generative?
Are multi-documentsummaries less extractive than single-document summaries?
Our results suggest thatextraction-based techniques which have beensuccessful for single-document summarizationmay not be sufficient when summarizingmultiple documents.1 IntroductionThe explosion of available online text has made itnecessary to be able to present information in a succinct,navigable manner.
The increased accessibility ofworldwide online news sources and the continuallyexpanding size of the worldwide web place demands onusers attempting to wade through vast amounts of text.Document clustering and multi-document summarizationtechnologies working in tandem promise to ease some ofthe burden on users when browsing related documents.Summarizing a set of documents brings aboutchallenges that are not present when summarizing asingle document.
One might expect that a good multi-document summary will present a synthesis of multipleviews of the event being described over differentdocuments, or present a high-level view of an event thatis not explicitly reflected in any single document.
Auseful multi-document summary may also indicate thepresence of new or distinct information contained withina set of documents describing the same topic (McKeownet.
al., 1999, Mani and Bloedorn, 1999).
To meet theseexpectations, a multi-document summary is required togeneralize, condense and merge information comingfrom multiple sources.Although single-document summarization is a well-studied task (see Mani and Maybury, 1999 for anoverview), multi-document summarization is onlyrecently being studied closely (Marcu & Gerber 2001).While close attention has been paid to multi-documentsummarization technologies (Barzilay et al 2002,Goldstein et al2000), the inherent properties of human-written multi-document summaries have not yet beenquantified.
In this paper, we seek to empiricallycharacterize ideal multi-document summaries in part byattempting to answer the questions: Can multi-documentsummaries that are written by humans be characterizedas extractive or generative?
Are multi-documentsummaries less extractive than single-documentsummaries?
Our aim in answering these questions is todiscover how the nature of multi-document summarieswill impact our system requirements.We have chosen to focus our experiments on the dataprovided for summarization evaluation during theDocument Understanding Conference (DUC).
While werecognize that other summarization corpora may exhibitdifferent properties than what we report, the dataprepared for DUC evaluations is widely used, andcontinues to be a powerful force in shaping directions insummarization research and evaluation.In the following section we describe previous workrelated to investigating the potential for extractivesummaries.
Section 3 describes a new approach forassessing the degree to which a summary can bedescribed as extractive, and reports our findings for bothsingle and multiple document summarization tasks.
Weconclude with a discussion of our findings in Section 4.2 Related WorkJing (2002) previously examined the degree to whichsingle-document summaries can be characterized asextractive.
Based on a manual inspection of 15 human-written summaries, she proposes that for the task ofsingle-document summarization, human summarizersuse a ?cut-and-paste?
approach in which six mainoperations are performed: sentence reduction, sentencecombination, syntactic transformation, reordering,lexical paraphrasing, and generalization or specification.The first four operations are reflected in the constructionof an HMM model that can be used to decompose humansummaries.
According to this model, 81% of summarysentences contained in a corpus of 300 human-writtensummaries of news articles on telecommunications werefound to fit the cut-and-paste method, with the restbelieved to have been composed from scratch.1Another recent study (Lin and Hovy, 2003)investigated the extent to which extractive methods maybe sufficient for summarization in the single-documentcase.
By computing a performance upper-bound forpure sentence extraction, they found that state-of-the-artextraction-based systems are still 15%-24%2  away fromthis limit, and 10% away from average humanperformance.
While this sheds light on how much gaincan be achieved by optimizing sentence extractionmethods for single-document summarization, to ourknowledge, no one has assessed the potential forextraction-based systems when attempting to summarizemultiple documents.3 Using N-gram Sequences toCharacterize SummariesOur approach to characterizing summaries is muchsimpler than what Jing has described and is based on thefollowing idea: if human-written summaries areextractive, then we should expect to see long spans oftext that have been lifted from the source documents toform a summary.Note that this holds under the assumptions made byJing?s model of operations that are performed by humansummarizers.
In the examples of operations given byJing, we notice that long n-grams are preserved(designated by brackets), even in the operations mostlylikely to disrupt the original text:1Jing considers a sentence to have been generated fromscratch if fewer than half of the words were composed ofterms coming from the original document.2The range in potential gains is due to possiblevariations in summary length.Sentence Reduction:Document sentence: When it arrives sometime nextyear in new TV sets, the V-chip will give parents a newand potentially revolutionary device to block outprograms they don?t want their children to see.Summary sentence: [The V-chip will give parents a][device to block out programs they don?t want theirchildren to see.
]Syntactic Transformation:Document sentence: Since annoy.com enablesvisitors to send unvarnished opinions to political andother figures in the news, the company was concernedthat its activities would be banned by the statute.Summary sentence: [Annoy.com enables visitors tosend unvarnished opinions to political and other figuresin the news] and feared the law could put them out ofbusiness.Sentence Combination:Document sentence 1: But it also raises seriousquestions about the privacy of such highly personalinformation wafting about the digital world.Document sentence 2: The issue thus fits squarelyinto the broader debate about privacy and security on theInternet, whether it involves protecting credit cardnumbers or keeping children from offensive information.Summary sentence: [But it also raises] the issue of[privacy of such] [personal information] and this issuehits the nail on the head [in the broader debate aboutprivacy and security on the Internet.
]3.1 Data and ExperimentsFor our experiments we used data made available fromthe 2001 Document Understanding Conference (DUC),an annual large-scale evaluation of summarizationsystems sponsored by the National Institute of Standardsand Technology (NIST).
In this corpus, NIST hasgathered documents describing 60 events, taken from theAssociated Press, Wall Street Journal, FBIS San JoseMercury, and LA Times newswires.
An event isdescribed by between 3 and 20 separate (but notnecessarily unique) documents; on average a clustercontains 10 documents.
Of the 60 available clusters, weused the portion specifically designated for training,which contains a total of 295 documents distributed over30 clusters.As part of the DUC 2001 summarization corpus,NIST also provides four hand-written summaries ofdifferent lengths for every document cluster, as well as100-word summaries of each document.
Since wewished to collectively compare single-documentsummaries against multi-document summaries, we usedthe 100-word multi-document summaries for ouranalysis.
It is important to note that for each cluster, allsummaries (50, 100, 200 and 400-word multi-documentand 100-word per-document) have been written by thesame author.
NIST used a total of ten authors, eachproviding summaries for 3 of the 30 topics.
Theinstructions provided did not differ per task; in bothsingle and multi-document scenarios, the authors weredirected to use complete sentences and told to feel free touse their own words (Over, 2004).To compare the text of human-authored multi-document summaries to the full-text documentsdescribing the events, we automatically broke thedocuments into sentences, and constructed a minimaltiling of each summary sentence.
Specifically, for eachsentence in the summary, we searched for all n-gramsthat are present in both the summary and the documents,placing no restrictions on the potential size of an n-gram.We then covered each summary sentence with the n-grams, optimizing to use as few n-grams as possible (i.e.favoring n-grams that are longer in length).
For thisexperiment, we normalized the data by converting allterms to lowercase and removing punctuation.3.2  ResultsOn average, we found the length of a tile to be 4.47 forsingle-document summaries, compared with 2.33 formulti-document summaries.
We discovered that 61 outof all 1667 hand-written single-document summarysentences exactly matched a sentence in the sourcedocument, however we did not find any sentences forwhich this was the case when examining multi-documentsummaries.We also wanted to study how many sentences arefully tiled by phrases coming from exactly one sentencein the document corpus, and found that while nosentences from the multi-document summaries matchedthis criteria, 7.6% of sentences in the single-documentsummaries could be tiled in this manner.
When trying totile sentences with tiles coming from only one documentsentence, we found that we could tile, on average,  93%of a  single-document sentence in that manner, comparedto an average of 36% of a multi-document sentence.This suggests that for multi-document summarization,we are not seeing any instances of what can beconsidered  single-sentence compression.
Table 1summarizes the findings we have presented in thissection.SingleDoc MultiDocAverage Tile Size(words) 4.47 2.33Max Tile Size(words) 38 24Exact sentencematches 3.7% 0%Complete tilingfrom singlesentence7.6% 0%Table 1.
Comparison of Summary TilingFigure 1 shows the relative frequency with which asummary sentence is optimally tiled using tile-sizes up to25 words in length in both the single and multi-document scenarios.
The data shows that the relativefrequency with which a single-document summarysentence is optimally tiled using n-grams containing 3 ormore words is consistently higher compared to the multi-document case.
Not shown on the histogram (due toinsufficient readability) is that we found 379 tiles (ofapproximately 86,000) between 25 and 38 words longcovering sentences from single-document summaries.No tiles longer than 24 words were found for multi-document summaries.In order to test whether tile samples coming fromtiling of single-document summaries and multi-document summaries are likely to have come from thesame underlying population, we performed two one-tailed unpaired t-tests, in one instance assuming equalvariances, and in the other case asssuming the varianceswere unequal.
For these statistical significance tests, werandomly sampled 100 summary sentences from eachtask, and extracted the lengths of the n-grams found viaminmal tiling.
This resulted in the creation of a sampleof 551 tiles for single-document sentences and 735 tilesfor multi-document sentences.For both tests (performed with ?=0.05), the P-valueswere low enough (0.00033 and 0.000858, respectively)to be able to reject the null hypothesis that the averagetile length coming from single-document summaries isthe same as the average tile length found in multi-document summaries.
We chose to use a one-tailed P-value because based on our experiments we alreadysuspected that the single-document tiles had a largermean.0.000.050.100.150.200.250.300.350.400.450.501 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25Tile LengthSingledoc MultidocFigure 1.
Comparison of Relative Frequencies ofOptimal Tile Lengths4 Conclusions and Future WorkOur experiments show that when writing multi-document summaries, human summarizers do not appearto be cutting and pasting phrases in an extractive fashion.On average, they are borrowing text around the bigramlevel, instead of extracting long sequences of words orfull sentences as they tend to do when summarizing asingle document.
The extent to which humansummarizers form extractive summaries during singleand multi-document summarization was found to bedifferent at a level which is statistically significant.These findings are additionally supported by the fact thatautomatic n-gram-based evaluation measures now beingused to assess predominately extractive multi-documentsummarization systems correlate strongly with humanjudgments when restricted to the usage of unigrams andbigrams, but correlate weakly when longer n-grams arefactored into the equation (Lin & Hovy, 2003).
In thefuture, we wish to apply our method to other corpora,and to explore the extent to which differentsummarization goals, such as describing an event orproviding a biography, affect the degree to whichhumans employ rewriting as opposed to extraction.Despite the unique requirements for multi-documentsummarization, relatively few systems have crossed overinto employing generation and reformulation (McKeown& Radev, 1995, Nenkova, et al 2003).
For the most part,summarization systems continue to be based on sentenceextraction methods.
Considering that humans appear tobe generating summary text that differs widely fromsentences in the original documents, we suspect thatapproaches which make use of generation andreformulation techniques may yield the most promise formulti-document summarization.
We would like toempirically quantify to what extent currentsummarization systems reformulate text, by applying thetechniques presented in this paper to system output.Finally, the potential impact of our findings withrespect to recent evaluation metrics should not beoverlooked.
Caution must be given when employingautomatic evaluation metrics based on the overlap of n-grams between human references and system summaries.When reference summaries do not contain long n-gramsdrawn from the source documents, but are insteadgenerated in the author?s own words, the use of a largenumber of reference summaries becomes more critical.AcknowledgementsThe authors wish to thank Eric Ringger for providingtools used to construct the tiles, and Bob Moore forsuggestions pertaining to our experiments.ReferencesRegina Barzilay, Noemie Elhadad, Kathleen McKeown.2002.
"Inferring Strategies for Sentence Ordering inMultidocument Summarization."
JAIR, 17:35-55.Jade Goldstein, Vibhu Mittal, Mark Kantrowitz andJaime Carbonell, 2000.
Multi-DocumentSummarization by Sentence Extraction.
In theProceedings of the ANLP/NAACL Workshop onAutomatic Summarization.
Seattle, WAHongyan Jing.
2002.
Using Hidden Markov Modeling toDecompose Human-Written Summaries.Computational Linguistics 28(4): 527-543.Chin-Yew Lin, and E.H. Hovy 2003.
AutomaticEvaluation of Summaries Using N-gram Co-occurrence Statistics.
In Proceedings of 2003Language Technology Conference (HLT-NAACL2003), Edmonton, Canada.Chin-Yew Lin and Eduard.H.
Hovy.
2003.
The Potentialand Limitations of Sentence Extraction forSummarization.
In Proceedings of the Workshop onAutomatic Summarization post-conference workshopof HLT-NAACL-2003.
Edmonton, Canada.Daniel Marcu and Laurie Gerber.
2001.
An Inquiry intothe Nature of Multidocument Abstracts, Extracts, andTheir Evaluation.
Proceedings of the NAACL-2001Workshop on Automatic SummarizationInderjeet Mani and Eric Bloedorn.
1999.
Summarizingsimilarities and differences among related documents.Information Retrieval, 1, pp.
35-67.Inderjeet Mani and Mark Maybury (Eds.).
1999.Advances in Automatic Text Summarization.
MITPress, Cambridge MA.Kathy McKeown, Judith Klavans, VasileiosHatzivassiloglou, Regina Barzilay, and Eleazar Eskin.1999.
Towards multidocument summarization byreformulation: Progress and prospects.
In Proceedingsof AAAI.Kathleen R. McKeown and Dragomir R. Radev.Generating summaries of multiple news articles.1995.
In Proceedings, 18th Annual InternationalACM SIGIR Conference on Research andDevelopment in Information Retrieval, pages 74-82.Ani Nenkova, Barry Schiffman, Andrew Schlaiker,Sasha Blair-Goldensohn, Regina Barzilay, SergeySigelman, Vasileios Hatzivassiloglou, and KathleenMcKeown.
2003.
Columbia at the DocumentUnderstanding Conference.
Document UnderstandingConference 2003.Paul Over.
2004.
Personal communication.
