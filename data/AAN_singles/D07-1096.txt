Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp.
915?932,Prague, June 2007. c?2007 Association for Computational LinguisticsThe CoNLL 2007 Shared Task on Dependency ParsingJoakim Nivre??
Johan Hall?
Sandra Ku?bler?
Ryan McDonald?
?Jens Nilsson?
Sebastian Riedel??
Deniz Yuret???Va?xjo?
University, School of Mathematics and Systems Engineering, first.last@vxu.se?Uppsala University, Dept.
of Linguistics and Philology, joakim.nivre@lingfil.uu.se?Indiana University, Department of Linguistics, skuebler@indiana.edu?
?Google Inc., ryanmcd@google.com?
?University of Edinburgh, School of Informatics, S.R.Riedel@sms.ed.ac.uk??Koc?
University, Dept.
of Computer Engineering, dyuret@ku.edu.trAbstractThe Conference on Computational NaturalLanguage Learning features a shared task, inwhich participants train and test their learn-ing systems on the same data sets.
In 2007,as in 2006, the shared task has been devotedto dependency parsing, this year with both amultilingual track and a domain adaptationtrack.
In this paper, we define the tasks of thedifferent tracks and describe how the datasets were created from existing treebanks forten languages.
In addition, we characterizethe different approaches of the participatingsystems, report the test results, and providea first analysis of these results.1 IntroductionPrevious shared tasks of the Conference on Compu-tational Natural Language Learning (CoNLL) havebeen devoted to chunking (1999, 2000), clause iden-tification (2001), named entity recognition (2002,2003), and semantic role labeling (2004, 2005).
In2006 the shared task was multilingual dependencyparsing, where participants had to train a singleparser on data from thirteen different languages,which enabled a comparison not only of parsing andlearning methods, but also of the performance thatcan be achieved for different languages (Buchholzand Marsi, 2006).In dependency-based syntactic parsing, the task isto derive a syntactic structure for an input sentenceby identifying the syntactic head of each word in thesentence.
This defines a dependency graph, wherethe nodes are the words of the input sentence and thearcs are the binary relations from head to dependent.Often, but not always, it is assumed that all wordsexcept one have a syntactic head, which means thatthe graph will be a tree with the single independentword as the root.
In labeled dependency parsing, weadditionally require the parser to assign a specifictype (or label) to each dependency relation holdingbetween a head word and a dependent word.In this year?s shared task, we continue to exploredata-driven methods for multilingual dependencyparsing, but we add a new dimension by also intro-ducing the problem of domain adaptation.
The waythis was done was by having two separate tracks: amultilingual track using essentially the same setupas last year, but with partly different languages, anda domain adaptation track, where the task was to usemachine learning to adapt a parser for a single lan-guage to a new domain.
In total, test results weresubmitted for twenty-three systems in the multilin-gual track, and ten systems in the domain adaptationtrack (six of which also participated in the multilin-gual track).
Not everyone submitted papers describ-ing their system, and some papers describe morethan one system (or the same system in both tracks),which explains why there are only (!)
twenty-onepapers in the proceedings.In this paper, we provide task definitions for thetwo tracks (section 2), describe data sets extractedfrom available treebanks (section 3), report resultsfor all systems in both tracks (section 4), give anoverview of approaches used (section 5), provide afirst analysis of the results (section 6), and concludewith some future directions (section 7).9152 Task DefinitionIn this section, we provide the task definitions thatwere used in the two tracks of the CoNLL 2007Shard Task, the multilingual track and the domainadaptation track, together with some backgroundand motivation for the design choices made.
Firstof all, we give a brief description of the data formatand evaluation metrics, which were common to thetwo tracks.2.1 Data Format and Evaluation MetricsThe data sets derived from the original treebanks(section 3) were in the same column-based formatas for the 2006 shared task (Buchholz and Marsi,2006).
In this format, sentences are separated by ablank line; a sentence consists of one or more to-kens, each one starting on a new line; and a tokenconsists of the following ten fields, separated by asingle tab character:1.
ID: Token counter, starting at 1 for each newsentence.2.
FORM: Word form or punctuation symbol.3.
LEMMA: Lemma or stem of word form, or anunderscore if not available.4.
CPOSTAG: Coarse-grained part-of-speech tag,where the tagset depends on the language.5.
POSTAG: Fine-grained part-of-speech tag,where the tagset depends on the language, oridentical to the coarse-grained part-of-speechtag if not available.6.
FEATS: Unordered set of syntactic and/or mor-phological features (depending on the particu-lar language), separated by a vertical bar (|), oran underscore if not available.7.
HEAD: Head of the current token, which iseither a value of ID or zero (0).
Note that,depending on the original treebank annotation,there may be multiple tokens with HEAD=0.8.
DEPREL: Dependency relation to the HEAD.The set of dependency relations depends onthe particular language.
Note that, dependingon the original treebank annotation, the depen-dency relation when HEAD=0 may be mean-ingful or simply ROOT.9.
PHEAD: Projective head of current token,which is either a value of ID or zero (0), or anunderscore if not available.10.
PDEPREL: Dependency relation to thePHEAD, or an underscore if not available.The PHEAD and PDEPREL were not used at allin this year?s data sets (i.e., they always containedunderscores) but were maintained for compatibilitywith last year?s data sets.
This means that, in prac-tice, the first six columns can be considered as inputto the parser, while the HEAD and DEPREL fieldsare the output to be produced by the parser.
Labeledtraining sets contained all ten columns; blind testsets only contained the first six columns; and goldstandard test sets (released only after the end of thetest period) again contained all ten columns.
All datafiles were encoded in UTF-8.The official evaluation metric in both tracks wasthe labeled attachment score (LAS), i.e., the per-centage of tokens for which a system has predictedthe correct HEAD and DEPREL, but results werealso reported for unlabeled attachment score (UAS),i.e., the percentage of tokens with correct HEAD,and the label accuracy (LA), i.e., the percentage oftokens with correct DEPREL.
One important differ-ence compared to the 2006 shared task is that all to-kens were counted as ?scoring tokens?, including inparticular all punctuation tokens.
The official eval-uation script, eval07.pl, is available from the sharedtask website.12.2 Multilingual TrackThe multilingual track of the shared task was orga-nized in the same way as the 2006 task, with an-notated training and test data from a wide range oflanguages to be processed with one and the sameparsing system.
This system must therefore be ableto learn from training data, to generalize to unseentest data, and to handle multiple languages, possi-bly by adjusting a number of hyper-parameters.
Par-ticipants in the multilingual track were expected tosubmit parsing results for all languages involved.1http://depparse.uvt.nl/depparse-wiki/SoftwarePage916One of the claimed advantages of dependencyparsing, as opposed to parsing based on constituentanalysis, is that it extends naturally to languageswith free or flexible word order.
This explains theinterest in recent years for multilingual evaluationof dependency parsers.
Even before the 2006 sharedtask, the parsers of Collins (1997) and Charniak(2000), originally developed for English, had beenadapted for dependency parsing of Czech, and theparsing methodology proposed by Kudo and Mat-sumoto (2002) and Yamada and Matsumoto (2003)had been evaluated on both Japanese and English.The parser of McDonald and Pereira (2006) hadbeen applied to English, Czech and Danish, and theparser of Nivre et al (2007) to ten different lan-guages.
But by far the largest evaluation of mul-tilingual dependency parsing systems so far was the2006 shared task, where nineteen systems were eval-uated on data from thirteen languages (Buchholz andMarsi, 2006).One of the conclusions from the 2006 shared taskwas that parsing accuracy differed greatly betweenlanguages and that a deeper analysis of the factorsinvolved in this variation was an important problemfor future research.
In order to provide an extendedempirical foundation for such research, we tried toselect the languages and data sets for this year?s taskbased on the following desiderata:?
The selection of languages should be typolog-ically varied and include both new languagesand old languages (compared to 2006).?
The creation of the data sets should involve aslittle conversion as possible from the originaltreebank annotation, meaning that preferenceshould be given to treebanks with dependencyannotation.?
The training data sets should include at least50,000 tokens and at most 500,000 tokens.2The final selection included data from Arabic,Basque, Catalan, Chinese, Czech, English, Greek,Hungarian, Italian, and Turkish.
The treebanks from2The reason for having an upper bound on the training setsize was the fact that, in 2006, some participants could not trainon all the data for some languages because of time limitations.Similar considerations also led to the decision to have a smallernumber of languages this year (ten, as opposed to thirteen).which the data sets were extracted are described insection 3.2.3 Domain Adaptation TrackOne well known characteristic of data-driven pars-ing systems is that they typically perform muchworse on data that does not come from the train-ing domain (Gildea, 2001).
Due to the large over-head in annotating text with deep syntactic parsetrees, the need to adapt parsers from domains withplentiful resources (e.g., news) to domains with lit-tle resources is an important problem.
This prob-lem is commonly referred to as domain adaptation,where the goal is to adapt annotated resources froma source domain to a target domain of interest.Almost all prior work on domain adaptation as-sumes one of two scenarios.
In the first scenario,there are limited annotated resources available in thetarget domain, and many studies have shown thatthis may lead to substantial improvements.
This in-cludes the work of Roark and Bacchiani (2003), Flo-rian et al (2004), Chelba and Acero (2004), Daume?and Marcu (2006), and Titov and Henderson (2006).Of these, Roark and Bacchiani (2003) and Titov andHenderson (2006) deal specifically with syntacticparsing.
The second scenario assumes that there areno annotated resources in the target domain.
This isa more realistic situation and is considerably moredifficult.
Recent work by McClosky et al (2006)and Blitzer et al (2006) have shown that the exis-tence of a large unlabeled corpus in the new domaincan be leveraged in adaptation.
For this shared-task,we are assuming the latter setting ?
no annotated re-sources in the target domain.Obtaining adequate annotated syntactic resourcesfor multiple languages is already a challenging prob-lem, which is only exacerbated when these resourcesmust be drawn from multiple and diverse domains.As a result, the only language that could be feasiblytested in the domain adaptation track was English.The setup for the domain adaptation track was asfollows.
Participants were provided with a large an-notated corpus from the source domain, in this casesentences from the Wall Street Journal.
Participantswere also provided with data from three differenttarget domains: biomedical abstracts (developmentdata), chemical abstracts (test data 1), and parent-child dialogues (test data 2).
Additionally, a large917unlabeled corpus for each data set (training, devel-opment, test) was provided.
The goal of the task wasto use the annotated source data, plus any unlabeleddata, to produce a parser that is accurate for each ofthe test sets from the target domains.3Participants could submit systems in either the?open?
or ?closed?
class (or both).
The closed classrequires a system to use only those resources pro-vided as part of the shared task.
The open class al-lows a system to use additional resources providedthose resources are not drawn from the same domainas the development or test sets.
An example mightbe a part-of-speech tagger trained on the entire PennTreebank and not just the subset provided as train-ing data, or a parser that has been hand-crafted ortrained on a different training set.3 TreebanksIn this section, we describe the treebanks used in theshared task and give relevant information about thedata sets created from them.3.1 Multilingual TrackArabic The analytical syntactic annotationof the Prague Arabic Dependency Treebank(PADT) (Hajic?
et al, 2004) can be considered apure dependency annotation.
The conversion, doneby Otakar Smrz, from the original format to thecolumn-based format described in section 2.1 wastherefore relatively straightforward, although not allthe information in the original annotation could betransfered to the new format.
PADT was one of thetreebanks used in the 2006 shared task but then onlycontained about 54,000 tokens.
Since then, the sizeof the treebank has more than doubled, with around112,000 tokens.
In addition, the morphologicalannotation has been made more informative.
Itis also worth noting that the parsing units in thistreebank are in many cases larger than conventionalsentences, which partly explains the high averagenumber of tokens per ?sentence?
(Buchholz andMarsi, 2006).3Note that annotated development data for the target domainwas only provided for the development domain, biomedical ab-stracts.
For the two test domains, chemical abstracts and parent-child dialogues, the only annotated data sets were the gold stan-dard test sets, released only after test runs had been submitted.Basque For Basque, we used the 3LB Basquetreebank (Aduriz et al, 2003).
At present, the tree-bank consists of approximately 3,700 sentences, 334of which were used as test data.
The treebank com-prises literary and newspaper texts.
It is annotatedin a dependency format and was converted to theCoNLL format by a team led by Koldo Gojenola.Catalan The Catalan section of the CESS-ECESyntactically and Semantically Annotated Cor-pora (Mart??
et al, 2007) is annotated with, amongother things, constituent structure and grammaticalfunctions.
A head percolation table was used forautomatically converting the constituent trees intodependency trees.
The original data only containsfunctions related to the verb, and a function tablewas used for deriving the remaining syntactic func-tions.
The conversion was performed by a team ledby Llu?
?s Ma`rquez and Anto`nia Mart?
?.Chinese The Chinese data are taken from theSinica treebank (Chen et al, 2003), which con-tains both syntactic functions and semantic func-tions.
The syntactic head was used in the conversionto the CoNLL format, carried out by Yu-Ming Hsiehand the organizers of the 2006 shared task, and thesyntactic functions were used wherever it was pos-sible.
The training data used is basically the sameas for the 2006 shared task, except for a few correc-tions, but the test data is new for this year?s sharedtask.
It is worth noting that the parsing units in thistreebank are sometimes smaller than conventionalsentence units, which partly explains the low aver-age number of tokens per ?sentence?
(Buchholz andMarsi, 2006).Czech The analytical syntactic annotation of thePrague Dependency Treebank (PDT) (Bo?hmova?
etal., 2003) is a pure dependency annotation, just asfor PADT.
It was also used in the shared task 2006,but there are two important changes compared tolast year.
First, version 2.0 of PDT was used in-stead of version 1.0, and a conversion script wascreated by Zdenek Zabokrtsky, using the new XML-based format of PDT 2.0.
Secondly, due to the upperbound on training set size, only sections 1?3 of PDTconstitute the training data, which amounts to some450,000 tokens.
The test data is a small subset of thedevelopment test set of PDT.918English For English we used the Wall Street Jour-nal section of the Penn Treebank (Marcus et al,1993).
In particular, we used sections 2-11 for train-ing and a subset of section 23 for testing.
As a pre-processing stage we removed many functions tagsfrom the non-terminals in the phrase structure repre-sentation to make the representations more uniformwith out-of-domain test sets for the domain adapta-tion track (see section 3.2).
The resulting data setwas then converted to dependency structures usingthe procedure described in Johansson and Nugues(2007a).
This work was done by Ryan McDonald.Greek The Greek Dependency Treebank(GDT) (Prokopidis et al, 2005) adopts a de-pendency structure annotation very similar to thoseof PDT and PADT, which means that the conversionby Prokopis Prokopidis was relatively straightfor-ward.
GDT is one of the smallest treebanks inthis year?s shared task (about 65,000 tokens) andcontains sentences of Modern Greek.
Just like PDTand PADT, the treebank contains more than onelevel of annotation, but we only used the analyticallevel of GDT.Hungarian For the Hungarian data, the Szegedtreebank (Csendes et al, 2005) was used.
The tree-bank is based on texts from six different genres,ranging from legal newspaper texts to fiction.
Theoriginal annotation scheme is constituent-based, fol-lowing generative principles.
It was converted intodependencies by Zo?ltan Alexin based on heuristics.Italian The data set used for Italian is a subsetof the balanced section of the Italian Syntactic-Semantic Treebank (ISST) (Montemagni et al,2003) and consists of texts from the newspaper Cor-riere della Sera and from periodicals.
A team ledby Giuseppe Attardi, Simonetta Montemagni, andMaria Simi converted the annotation to the CoNLLformat, using information from two different anno-tation levels, the constituent structure level and thedependency structure level.Turkish For Turkish we used the METU-Sabanc?Turkish Treebank (Oflazer et al, 2003), which wasalso used in the 2006 shared task.
A new test set ofabout 9,000 tokens was provided by Gu?ls?en Eryig?it(Eryig?it, 2007), who also handled the conversion tothe CoNLL format, which means that we could useall the approximately 65,000 tokens of the originaltreebank for training.
The rich morphology of Turk-ish requires the basic tokens in parsing to be inflec-tional groups (IGs) rather than words.
IGs of a singleword are connected to each other deterministicallyusing dependency links labeled DERIV, referred toas word-internal dependencies in the following, andthe FORM and the LEMMA fields may be empty(they contain underscore characters in the data files).Sentences do not necessarily have a unique root;most internal punctuation and a few foreign wordsalso have HEAD=0.3.2 Domain Adaptation TrackAs mentioned previously, the source data is drawnfrom a corpus of news, specifically the Wall StreetJournal section of the Penn Treebank (Marcus et al,1993).
This data set is identical to the English train-ing set from the multilingual track (see section 3.1).For the target domains we used three differentlabeled data sets.
The first two were annotatedas part of the PennBioIE project (Kulick et al,2004) and consist of sentences drawn from eitherbiomedical or chemical research abstracts.
Like thesource WSJ corpus, this data is annotated using thePenn Treebank phrase structure scheme.
To con-vert these sets to dependency structures we used thesame procedure as before (Johansson and Nugues,2007a).
Additional care was taken to remove sen-tences that contained non-WSJ part-of-speech tagsor non-terminals (e.g., HYPH part-of-speech tag in-dicating a hyphen).
Furthermore, the annotationscheme for gaps and traces was made consistent withthe Penn Treebank wherever possible.
As alreadymentioned, the biomedical data set was distributedas a development set for the training phase, whilethe chemical data set was only used for final testing.The third target data set was taken from theCHILDES database (MacWhinney, 2000), in partic-ular the EVE corpus (Brown, 1973), which has beenannotated with dependency structures.
Unfortu-nately the dependency labels of the CHILDES datawere inconsistent with those of the WSJ, biomedi-cal and chemical data sets, and we therefore optedto only evaluate unlabeled accuracy for this dataset.
Furthermore, there was an inconsistency in howmain and auxiliary verbs were annotated for this dataset relative to others.
As a result of this, submitting919Multilingual Domain adaptationAr Ba Ca Ch Cz En Gr Hu It Tu PCHEM CHILDESLanguage family Sem.
Isol.
Rom.
Sin.
Sla.
Ger.
Hel.
F.-U.
Rom.
Tur.
Ger.Annotation d d c+f c+f d c+f d c+f c+f d c+f dTraining data Development dataTokens (k) 112 51 431 337 432 447 65 132 71 65 5Sentences (k) 2.9 3.2 15.0 57.0 25.4 18.6 2.7 6.0 3.1 5.6 0.2Tokens/sentence 38.3 15.8 28.8 5.9 17.0 24.0 24.2 21.8 22.9 11.6 25.1LEMMA Yes Yes Yes No Yes No Yes Yes Yes Yes NoNo.
CPOSTAG 15 25 17 13 12 31 18 16 14 14 25No.
POSTAG 21 64 54 294 59 45 38 43 28 31 37No.
FEATS 21 359 33 0 71 0 31 50 21 78 0No.
DEPREL 29 35 42 69 46 20 46 49 22 25 18No.
DEPREL H=0 18 17 1 1 8 1 22 1 1 1 1% HEAD=0 8.7 9.7 3.5 16.9 11.6 4.2 8.3 4.6 5.4 12.8 4.0% HEAD left 79.2 44.5 60.0 24.7 46.9 49.0 44.8 27.4 65.0 3.8 50.0% HEAD right 12.1 45.8 36.5 58.4 41.5 46.9 46.9 68.0 29.6 83.4 46.0HEAD=0/sentence 3.3 1.5 1.0 1.0 2.0 1.0 2.0 1.0 1.2 1.5 1.0% Non-proj.
arcs 0.4 2.9 0.1 0.0 1.9 0.3 1.1 2.9 0.5 5.5 0.4% Non-proj.
sent.
10.1 26.2 2.9 0.0 23.2 6.7 20.3 26.4 7.4 33.3 8.0Punc.
attached S S A S S A S A A S ADEPRELS for punc.
10 13 6 29 16 13 15 1 10 12 8Test data PCHEM CHILDESTokens 5124 5390 5016 5161 4724 5003 4804 7344 5096 4513 5001 4999Sentences 131 334 167 690 286 214 197 390 249 300 195 666Tokens/sentence 39.1 16.1 30.0 7.5 16.5 23.4 24.4 18.8 20.5 15.0 25.6 12.9% New words 12.44 24.98 4.35 9.70 12.58 3.13 12.43 26.10 15.07 36.29 31.33 6.10% New lemmas 2.82 11.13 3.36 n/a 5.28 n/a 5.82 14.80 8.24 9.95 n/a n/aTable 1: Characteristics of the data sets for the 10 languages of the multilingual track and the developmentset and the two test sets of the domain adaptation track.920results for the CHILDES data was considered op-tional.
Like the chemical data set, this data set wasonly used for final testing.Finally, a large corpus of unlabeled in-domaindata was provided for each data set and made avail-able for training.
This data was drawn from theWSJ,PubMed.com (specific to biomedical and chemicalresearch literature), and the CHILDES data base.The data was tokenized to be as consistent as pos-sible with the WSJ training set.3.3 OverviewTable 1 describes the characteristics of the data sets.For the multilingual track, we provide statistics overthe training and test sets; for the domain adaptationtrack, the statistics were extracted from the develop-ment set.
Following last year?s shared task practice(Buchholz and Marsi, 2006), we use the followingdefinition of projectivity: An arc (i, j) is projectiveiff all nodes occurring between i and j are dominatedby i (where dominates is the transitive closure of thearc relation).In the table, the languages are abbreviated to theirfirst two letters.
Language families are: Semitic,Isolate, Romance, Sino-Tibetan, Slavic, Germanic,Hellenic, Finno-Ugric, and Turkic.
The type of theoriginal annotation is either constituents plus (some)functions (c+f) or dependencies (d).
For the train-ing data, the number of words and sentences aregiven in multiples of thousands, and the averagelength of a sentence in words (including punctua-tion tokens).
The following rows contain informa-tion about whether lemmas are available, the num-ber of coarse- and fine-grained part-of-speech tags,the number of feature components, and the numberof dependency labels.
Then information is given onhow many different dependency labels can co-occurwith HEAD=0, the percentage of HEAD=0 depen-dencies, and the percentage of heads preceding (left)or succeeding (right) a token (giving an indication ofwhether a language is predominantly head-initial orhead-final).
This is followed by the average numberof HEAD=0 dependencies per sentence and the per-centage of non-projective arcs and sentences.
Thelast two rows show whether punctuation tokens areattached as dependents of other tokens (A=Always,S=Sometimes) and specify the number of depen-dency labels that exist for punctuation tokens.
Notethat punctuation is defined as any token belonging tothe UTF-8 category of punctuation.
This means, forexample, that any token having an underscore in theFORM field (which happens for word-internal IGsin Turkish) is also counted as punctuation here.For the test sets, the number of words and sen-tences as well as the ratio of words per sentence arelisted, followed by the percentage of new words andlemmas (if applicable).
For the domain adaptationsets, the percentage of new words is computed withregard to the training set (Penn Treebank).4 Submissions and ResultsAs already stated in the introduction, test runs weresubmitted for twenty-three systems in the multilin-gual track, and ten systems in the domain adaptationtrack (six of which also participated in the multilin-gual track).
In the result tables below, systems areidentified by the last name of the teammember listedfirst when test runs were uploaded for evaluation.
Ingeneral, this name is also the first author of a paperdescribing the system in the proceedings, but thereare a few exceptions and complications.
First of all,for four out of twenty-seven systems, no paper wassubmitted to the proceedings.
This is the case for thesystems of Jia, Maes et al, Nash, and Zeman, whichis indicated by the fact that these names appear initalics in all result tables.
Secondly, two teams sub-mitted two systems each, which are described in asingle paper by each team.
Thus, the systems called?Nilsson?
and ?Hall, J.?
are both described in Hall etal.
(2007a), while the systems called ?Duan (1)?
and?Duan (2)?
are both described in Duan et al (2007).Finally, please pay attention to the fact that thereare two teams, where the first author?s last name isHall.
Therefore, we use ?Hall, J.?
and ?Hall, K.?,to disambiguate between the teams involving JohanHall (Hall et al, 2007a) and Keith Hall (Hall et al,2007b), respectively.Tables 2 and 3 give the scores for the multilingualtrack in the CoNLL 2007 shared task.
The Averagecolumn contains the average score for all ten lan-guages, which determines the ranking in this track.Table 4 presents the results for the domain adapta-tion track, where the ranking is determined based onthe PCHEM results only, since the CHILDES dataset was optional.
Note also that there are no labeled921Team Average Arabic Basque Catalan Chinese Czech English Greek Hungarian Italian TurkishNilsson 80.32(1) 76.52(1) 76.94(1) 88.70(1) 75.82(15) 77.98(3) 88.11(5) 74.65(2) 80.27(1) 84.40(1) 79.79(2)Nakagawa 80.29(2) 75.08(2) 72.56(7) 87.90(3) 83.84(2) 80.19(1) 88.41(3) 76.31(1) 76.74(8) 83.61(3) 78.22(5)Titov 79.90(3) 74.12(6) 75.49(3) 87.40(6) 82.14(7) 77.94(4) 88.39(4) 73.52(10) 77.94(4) 82.26(6) 79.81(1)Sagae 79.90(4) 74.71(4) 74.64(6) 88.16(2) 84.69(1) 74.83(8) 89.01(2) 73.58(8) 79.53(2) 83.91(2) 75.91(10)Hall, J.
79.80(5)* 74.75(3) 74.99(5) 87.74(4) 83.51(3) 77.22(6) 85.81(12) 74.21(6) 78.09(3) 82.48(5) 79.24(3)Carreras 79.09(6)* 70.20(11) 75.75(2) 87.60(5) 80.86(10) 78.60(2) 89.61(1) 73.56(9) 75.42(9) 83.46(4) 75.85(11)Attardi 78.27(7) 72.66(8) 69.48(12) 86.86(7) 81.50(8) 77.37(5) 85.85(10) 73.92(7) 76.81(7) 81.34(8) 76.87(7)Chen 78.06(8) 74.65(5) 72.39(8) 86.66(8) 81.24(9) 73.69(10) 83.81(13) 74.42(3) 75.34(10) 82.04(7) 76.31(9)Duan (1) 77.70(9)* 69.91(13) 71.26(9) 84.95(10) 82.58(6) 75.34(7) 85.83(11) 74.29(4) 77.06(5) 80.75(9) 75.03(12)Hall, K. 76.91(10)* 73.40(7) 69.81(11) 82.38(14) 82.77(4) 72.27(12) 81.93(15) 74.21(5) 74.20(11) 80.69(10) 77.42(6)Schiehlen 76.18(11) 70.08(12) 66.77(14) 85.75(9) 80.04(11) 73.86(9) 86.21(9) 72.29(12) 73.90(12) 80.46(11) 72.48(15)Johansson 75.78(12)* 71.76(9) 75.08(4) 83.33(12) 76.30(14) 70.98(13) 80.29(17) 72.77(11) 71.31(13) 77.55(14) 78.46(4)Mannem 74.54(13)* 71.55(10) 65.64(15) 84.47(11) 73.76(17) 70.68(14) 81.55(16) 71.69(13) 70.94(14) 78.67(13) 76.42(8)Wu 73.02(14)* 66.16(14) 70.71(10) 81.44(15) 74.69(16) 66.72(16) 79.49(18) 70.63(14) 69.08(15) 78.79(12) 72.52(14)Nguyen 72.53(15)* 63.58(16) 58.18(17) 83.23(13) 79.77(12) 72.54(11) 86.73(6) 70.42(15) 68.12(17) 75.06(16) 67.63(17)Maes 70.66(16)* 65.12(15) 69.05(13) 79.21(16) 70.97(18) 67.38(15) 69.68(21) 68.59(16) 68.93(16) 73.63(18) 74.03(13)Canisius 66.99(17)* 59.13(18) 63.17(16) 75.44(17) 70.45(19) 56.14(17) 77.27(19) 60.35(18) 64.31(19) 75.57(15) 68.09(16)Jia 63.00(18)* 63.37(17) 57.61(18) 23.35(20) 76.36(13) 54.95(18) 82.93(14) 65.45(17) 66.61(18) 74.65(17) 64.68(18)Zeman 54.87(19) 46.06(20) 50.61(20) 62.94(19) 54.49(20) 50.21(20) 53.59(22) 55.29(19) 55.24(20) 62.13(19) 58.10(19)Marinov 54.55(20)* 54.00(19) 51.24(19) 69.42(18) 49.87(21) 53.47(19) 52.11(23) 54.33(20) 44.47(21) 59.75(20) 56.88(20)Duan (2) 24.62(21)* 82.64(5) 86.69(7) 76.89(6)Nash 8.65(22)* 86.49(8)Shimizu 7.20(23) 72.02(20)Table 2: Labeled attachment score (LAS) for the multilingual track in the CoNLL 2007 shared task.
Teamsare denoted by the last name of their first member, with italics indicating that there is no correspondingpaper in the proceedings.
The number in parentheses next to each score gives the rank.
A star next to a scorein the Average column indicates a statistically significant difference with the next lower rank.Team Average Arabic Basque Catalan Chinese Czech English Greek Hungarian Italian TurkishNakagawa 86.55(1)* 86.09(1) 81.04(5) 92.86(4) 88.88(2) 86.28(1) 90.13(2) 84.08(1) 82.49(3) 87.91(1) 85.77(3)Nilsson 85.71(2) 85.81(2) 82.84(1) 93.12(3) 84.52(12) 83.59(4) 88.93(5) 81.22(4) 83.55(1) 87.77(2) 85.77(2)Titov 85.62(3) 83.18(7) 81.93(2) 93.40(1) 87.91(4) 84.19(3) 89.73(4) 81.20(5) 82.18(4) 86.26(6) 86.22(1)Sagae 85.29(4)* 84.04(4) 81.19(3) 93.34(2) 88.94(1) 81.27(8) 89.87(3) 80.37(11) 83.51(2) 87.68(3) 82.72(9)Carreras 84.79(5) 81.48(10) 81.11(4) 92.46(5) 86.20(9) 85.16(2) 90.63(1) 81.37(3) 79.92(9) 87.19(4) 82.41(10)Hall, J.
84.74(6)* 84.21(3) 80.61(6) 92.20(6) 87.60(5) 82.35(6) 86.77(12) 80.66(9) 81.71(6) 86.26(5) 85.04(5)Attardi 83.96(7)* 82.53(8) 76.88(11) 91.41(7) 86.73(8) 83.40(5) 86.99(10) 80.75(8) 81.81(5) 85.54(8) 83.56(7)Chen 83.22(8) 83.49(5) 78.65(8) 90.87(8) 85.91(10) 80.14(11) 84.91(13) 81.16(6) 79.25(11) 85.91(7) 81.92(12)Hall, K. 83.08(9) 83.45(6) 78.55(9) 87.80(15) 87.91(3) 78.47(12) 83.21(15) 82.04(2) 79.34(10) 84.81(9) 85.18(4)Duan (1) 82.77(10) 79.04(13) 77.59(10) 89.71(12) 86.88(7) 80.82(10) 86.97(11) 80.77(7) 80.66(7) 84.20(11) 81.03(13)Schiehlen 82.42(11)* 81.07(11) 73.30(14) 90.79(10) 85.45(11) 81.73(7) 88.91(6) 80.47(10) 78.61(12) 84.54(10) 79.33(15)Johansson 81.13(12)* 80.91(12) 80.43(7) 88.34(13) 81.30(15) 77.39(13) 81.43(18) 79.58(12) 75.53(15) 81.55(15) 84.80(6)Mannem 80.30(13) 81.56(9) 72.88(15) 89.81(11) 78.84(17) 77.20(14) 82.81(16) 78.89(13) 75.39(16) 82.91(12) 82.74(8)Nguyen 80.00(14)* 73.46(18) 69.15(18) 88.12(14) 84.05(13) 80.91(9) 88.01(7) 77.56(15) 78.13(13) 80.40(16) 80.19(14)Jia 78.46(15) 74.20(17) 70.24(16) 90.83(9) 83.39(14) 70.41(18) 84.37(14) 75.65(16) 77.19(14) 82.36(14) 75.96(17)Wu 78.44(16)* 77.05(14) 75.77(12) 85.85(16) 79.71(16) 73.07(16) 81.69(17) 78.12(14) 72.39(18) 82.57(13) 78.15(16)Maes 76.60(17)* 75.47(16) 75.27(13) 84.35(17) 76.57(18) 74.03(15) 71.62(21) 75.19(17) 72.93(17) 78.32(18) 82.21(11)Canisius 74.83(18)* 76.89(15) 70.17(17) 81.64(18) 74.81(19) 72.12(17) 78.23(19) 72.46(18) 67.80(19) 79.08(17) 75.14(18)Zeman 62.02(19)* 58.55(20) 57.42(20) 68.50(20) 62.93(20) 59.19(20) 58.33(22) 62.89(19) 59.78(20) 68.27(19) 64.30(19)Marinov 60.83(20)* 64.27(19) 58.55(19) 74.22(19) 56.09(21) 59.57(19) 54.33(23) 61.18(20) 50.39(21) 65.52(20) 64.13(20)Duan (2) 25.53(21)* 86.94(6) 87.87(8) 80.53(8)Nash 8.77(22)* 87.71(9)Shimizu 7.79(23) 77.91(20)Table 3: Unlabeled attachment scores (UAS) for the multilingual track in the CoNLL 2007 shared task.Teams are denoted by the last name of their first member, with italics indicating that there is no correspond-ing paper in the proceedings.
The number in parentheses next to each score gives the rank.
A star next to ascore in the Average column indicates a statistically significant difference with the next lower rank.922LAS UASTeam PCHEM-c PCHEM-o PCHEM-c PCHEM-o CHILDES-c CHILDES-oSagae 81.06(1) 83.42(1)Attardi 80.40(2) 83.08(3) 58.67(3)Dredze 80.22(3) 83.38(2) 61.37(1)Nguyen 79.50(4)* 82.04(4)*Jia 76.48(5)* 78.92(5)* 57.43(5)Bick 71.81(6)* 78.48(1)* 74.71(6)* 81.62(1)* 58.07(4) 62.49(1)Shimizu 64.15(7)* 63.49(2) 71.25(7)* 70.01(2)*Zeman 50.61(8) 54.57(8) 58.89(2)Schneider 63.01(3)* 66.53(3)* 60.27(2)Watson 55.47(4) 62.79(4) 45.61(3)Wu 52.89(6)Table 4: Labeled (LAS) and unlabeled (UAS) attachment scores for the closed (-c) and open (-o) classes ofthe domain adaptation track in the CoNLL 2007 shared task.
Teams are denoted by the last name of theirfirst member, with italics indicating that there is no corresponding paper in the proceedings.
The numberin parentheses next to each score gives the rank.
A star next to a score in the PCHEM columns indicates astatistically significant difference with the next lower rank.attachment scores for the CHILDES data set, for rea-sons explained in section 3.2.
The number in paren-theses next to each score gives the rank.
A star nextto a score indicates that the difference with the nextlower rank is significant at the 5% level using a z-test for proportions.
A more complete presentationof the results, including the significance results forall the tasks and their p-values, can be found on theshared task website.4Looking first at the results in the multilingualtrack, we note that there are a number of systemsperforming at almost the same level at the top of theranking.
For the average labeled attachment score,the difference between the top score (Nilsson) andthe fifth score (Hall, J.)
is no more than half a per-centage point, and there are generally very few sig-nificant differences among the five or six best sys-tems, regardless of whether we consider labeled orunlabeled attachment score.
For the closed class ofthe domain adaptation track, we see a very similarpattern, with the top system (Sagae) being followedvery closely by two other systems.
For the openclass, the results are more spread out, but then thereare very few results in this class.
It is also worth not-ing that the top scores in the closed class, somewhatunexpectedly, are higher than the top scores in the4http://nextens.uvt.nl/depparse-wiki/AllScoresopen class.
But before we proceed to a more detailedanalysis of the results (section 6), we will make anattempt to characterize the approaches representedby the different systems.5 ApproachesIn this section we give an overview of the models,inference methods, and learning methods used in theparticipating systems.
For obvious reasons the dis-cussion is limited to systems that are described bya paper in the proceedings.
But instead of describ-ing the systems one by one, we focus on the basicmethodological building blocks that are often foundin several systems although in different combina-tions.
For descriptions of the individual systems, werefer to the respective papers in the proceedings.Section 5.1 is devoted to system architectures.
Wethen describe the two main paradigms for learningand inference, in this year?s shared task as well as inlast year?s, which we call transition-based parsers(section 5.2) and graph-based parsers (section 5.3),adopting the terminology of McDonald and Nivre(2007).5 Finally, we give an overview of the domainadaptation methods that were used (section 5.4).5This distinction roughly corresponds to the distinctionmade by Buchholz and Marsi (2006) between ?stepwise?
and?all-pairs?
approaches.9235.1 ArchitecturesMost systems perform some amount of pre- andpost-processing, making the actual parsing compo-nent part of a sequential workflow of varying lengthand complexity.
For example, most transition-based parsers can only build projective dependencygraphs.
For languages with non-projective depen-dencies, graphs therefore need to be projectivizedfor training and deprojectivized for testing (Hall etal., 2007a; Johansson and Nugues, 2007b; Titov andHenderson, 2007).Instead of assigning HEAD and DEPREL in asingle step, some systems use a two-stage approachfor attaching and labeling dependencies (Chen et al,2007; Dredze et al, 2007).
In the first step unlabeleddependencies are generated, in the second step theseare labeled.
This is particularly helpful for factoredparsing models, in which label decisions cannot beeasily conditioned on larger parts of the structuredue to the increased complexity of inference.
Onesystem (Hall et al, 2007b) extends this two-stage ap-proach to a three-stage architecture where the parserand labeler generate an n-best list of parses which inturn is reranked.6In ensemble-based systems several base parsersprovide parsing decisions, which are added togetherfor a combined score for each potential dependencyarc.
The tree that maximizes the sum of these com-bined scores is taken as the final output parse.
Thistechnique is used by Sagae and Tsujii (2007) and inthe Nilsson system (Hall et al, 2007a).
It is worthnoting that both these systems combine transition-based base parsers with a graph-based method forparser combination, as first described by Sagae andLavie (2006).Data-driven grammar-based parsers, such as Bick(2007), Schneider et al (2007), and Watson andBriscoe (2007), need pre- and post-processing in or-der to map the dependency graphs provided as train-ing data to a format compatible with the grammarused, and vice versa.5.2 Transition-Based ParsersTransition-based parsers build dependency graphsby performing sequences of actions, or transitions.Both learning and inference is conceptualized in6They also flip the order of the labeler and the reranker.terms of predicting the correct transition based onthe current parser state and/or history.
We can fur-ther subclassify parsers with respect to the model (ortransition system) they adopt, the inference methodthey use, and the learning method they employ.5.2.1 ModelsThe most common model for transition-basedparsers is one inspired by shift-reduce parsing,where a parser state contains a stack of partiallyprocessed tokens and a queue of remaining inputtokens, and where transitions add dependency arcsand perform stack and queue operations.
This typeof model is used by the majority of transition-basedparsers (Attardi et al, 2007; Duan et al, 2007; Hallet al, 2007a; Johansson and Nugues, 2007b; Man-nem, 2007; Titov and Henderson, 2007; Wu et al,2007).
Sometimes it is combined with an explicitprobability model for transition sequences, whichmay be conditional (Duan et al, 2007) or generative(Titov and Henderson, 2007).An alternative model is based on the list-basedparsing algorithm described by Covington (2001),which iterates over the input tokens in a sequen-tial manner and evaluates for each preceding tokenwhether it can be linked to the current token or not.This model is used by Marinov (2007) and in com-ponent parsers of the Nilsson ensemble system (Hallet al, 2007a).
Finally, two systems use models basedon LR parsing (Sagae and Tsujii, 2007; Watson andBriscoe, 2007).5.2.2 InferenceThe most common inference technique in transition-based dependency parsing is greedy deterministicsearch, guided by a classifier for predicting the nexttransition given the current parser state and history,processing the tokens of the sentence in sequen-tial left-to-right order7 (Hall et al, 2007a; Mannem,2007; Marinov, 2007; Wu et al, 2007).
Optionallymultiple passes over the input are conducted until notokens are left unattached (Attardi et al, 2007).As an alternative to deterministic parsing, severalparsers use probabilistic models and maintain a heapor beam of partial transition sequences in order topick the most probable one at the end of the sentence7For diversity in parser ensembles, right-to-left parsers arealso used.924(Duan et al, 2007; Johansson and Nugues, 2007b;Sagae and Tsujii, 2007; Titov and Henderson, 2007).One system uses as part of their parsing pipeline a?neighbor-parser?
that attaches adjacent words anda ?root-parser?
that identifies the root word(s) of asentence (Wu et al, 2007).
In the case of grammar-based parsers, a classifier is used to disambiguatein cases where the grammar leaves some ambiguity(Schneider et al, 2007; Watson and Briscoe, 2007)5.2.3 LearningTransition-based parsers either maintain a classifierthat predicts the next transition or a global proba-bilistic model that scores a complete parse.
To trainthese classifiers and probabilitistic models severalapproaches were used: SVMs (Duan et al, 2007;Hall et al, 2007a; Sagae and Tsujii, 2007), modifiedfinite Newton SVMs (Wu et al, 2007), maximumentropy models (Sagae and Tsujii, 2007), multiclassaveraged perceptron (Attardi et al, 2007) and max-imum likelihood estimation (Watson and Briscoe,2007).In order to calculate a global score or probabil-ity for a transition sequence, two systems used aMarkov chain approach (Duan et al, 2007; Sagaeand Tsujii, 2007).
Here probabilities from the outputof a classifier are multiplied over the whole sequenceof actions.
This results in a locally normalizedmodel.
Two other entries used MIRA (Mannem,2007) or online passive-aggressive learning (Johans-son and Nugues, 2007b) to train a globally normal-ized model.
Titov and Henderson (2007) used an in-cremental sigmoid Bayesian network to model theprobability of a transition sequence and estimatedmodel parameters using neural network learning.5.3 Graph-Based ParsersWhile transition-based parsers use training data tolearn a process for deriving dependency graphs,graph-based parsers learn a model of what it meansto be a good dependency graph given an input sen-tence.
They define a scoring or probability functionover the set of possible parses.
At learning timethey estimate parameters of this function; at pars-ing time they search for the graph that maximizesthis function.
These parsers mainly differ in thetype and structure of the scoring function (model),the search algorithm that finds the best parse (infer-ence), and the method to estimate the function?s pa-rameters (learning).5.3.1 ModelsThe simplest type of model is based on a sum oflocal attachment scores, which themselves are cal-culated based on the dot product of a weight vectorand a feature representation of the attachment.
Thistype of scoring function is often referred to as a first-order model.8 Several systems participating in thisyear?s shared task used first-order models (Schiehlenand Spranger, 2007; Nguyen et al, 2007; Shimizuand Nakagawa, 2007; Hall et al, 2007b).
Canisiusand Tjong Kim Sang (2007) cast the same type ofarc-based factorization as a weighted constraint sat-isfaction problem.Carreras (2007) extends the first-order model toincorporate a sum over scores for pairs of adjacentarcs in the tree, yielding a second-order model.
Incontrast to previous work where this was constrainedto sibling relations of the dependent (McDonald andPereira, 2006), here head-grandchild relations canbe taken into account.In all of the above cases the scoring function isdecomposed into functions that score local proper-ties (arcs, pairs of adjacent arcs) of the graph.
Bycontrast, the model of Nakagawa (2007) considersglobal properties of the graph that can take multi-ple arcs into account, such as multiple siblings andchildren of a node.5.3.2 InferenceSearching for the highest scoring graph (usually atree) in a model depends on the factorization cho-sen and whether we are looking for projective ornon-projective trees.
Maximum spanning tree al-gorithms can be used for finding the highest scor-ing non-projective tree in a first-order model (Hallet al, 2007b; Nguyen et al, 2007; Canisius andTjong Kim Sang, 2007; Shimizu and Nakagawa,2007), while Eisner?s dynamic programming algo-rithm solves the problem for a first-order factoriza-tion in the projective case (Schiehlen and Spranger,2007).
Carreras (2007) employs his own exten-sion of Eisner?s algorithm for the case of projectivetrees and second-order models that include head-grandparent relations.8It is also known as an edge-factored model.925The methods presented above are mostly efficientand always exact.
However, for models that takeglobal properties of the tree into account, they can-not be applied.
Instead Nakagawa (2007) uses Gibbssampling to obtain marginal probabilities of arcs be-ing included in the tree using his global model andthen applies a maximum spanning tree algorithm tomaximize the sum of the logs of these marginals andreturn a valid cycle-free parse.5.3.3 LearningMost of the graph-based parsers were trained usingan online inference-based method such as passive-aggressive learning (Nguyen et al, 2007; Schiehlenand Spranger, 2007), averaged perceptron (Carreras,2007), or MIRA (Shimizu and Nakagawa, 2007),while some systems instead used methods based onmaximum conditional likelihood (Nakagawa, 2007;Hall et al, 2007b).5.4 Domain Adaptation5.4.1 Feature-Based ApproachesOne way of adapting a learner to a new domain with-out using any unlabeled data is to only include fea-tures that are expected to transfer well (Dredze etal., 2007).
In structural correspondence learning atransformation from features in the source domainto features of the target domain is learnt (Shimizuand Nakagawa, 2007).
The original source featuresalong with their transformed versions are then usedto train a discriminative parser.5.4.2 Ensemble-Based ApproachesDredze et al (2007) trained a diverse set of parsersin order to improve cross-domain performance byincorporating their predictions as features for an-other classifier.
Similarly, two parsers trained withdifferent learners and search directions were usedin the co-learning approach of Sagae and Tsujii(2007).
Unlabeled target data was processed withboth parsers.
Sentences that both parsers agreed onwere then added to the original training data.
Thiscombined data set served as training data for one ofthe original parsers to produce the final system.
Ina similar fashion, Watson and Briscoe (2007) used avariant of self-training to make use of the unlabeledtarget data.5.4.3 Other ApproachesAttardi et al (2007) learnt tree revision rules for thetarget domain by first parsing unlabeled target datausing a strong parser; this data was then combinedwith labeled source data; a weak parser was appliedto this new dataset; finally tree correction rules arecollected based on the mistakes of the weak parserwith respect to the gold data and the output of thestrong parser.Another technique used was to filter sentences ofthe out-of-domain corpus based on their similarityto the target domain, as predicted by a classifier(Dredze et al, 2007).
Only if a sentence was judgedsimilar to target domain sentences was it included inthe training set.Bick (2007) used a hybrid approach, where a data-driven parser trained on the labeled training data wasgiven access to the output of a Constraint Grammarparser for English run on the same data.
Finally,Schneider et al (2007) learnt collocations and rela-tional nouns from the unlabeled target data and usedthese in their parsing algorithm.6 AnalysisHaving discussed the major approaches taken in thetwo tracks of the shared task, we will now return tothe test results.
For the multilingual track, we com-pare results across data sets and across systems, andreport results from a parser combination experimentinvolving all the participating systems (section 6.1).For the domain adaptation track, we sum up the mostimportant findings from the test results (section 6.2).6.1 Multilingual Track6.1.1 Across Data SetsThe average LAS over all systems varies from 68.07for Basque to 80.95 for English.
Top scores varyfrom 76.31 for Greek to 89.61 for English.
In gen-eral, there is a good correlation between the topscores and the average scores.
For Greek, Italian,and Turkish, the top score is closer to the averagescore than the average distance, while for Czech, thedistance is higher.
The languages that produced themost stable results in terms of system ranks with re-spect to LAS are Hungarian and Italian.
For UAS,Catalan also falls into this group.
The language that926Setup Arabic Chinese Czech Turkish2006 without punctuation 66.9 90.0 80.2 65.72007 without punctuation 75.5 84.9 80.0 71.62006 with punctuation 67.0 90.0 80.2 73.82007 with punctuation 76.5 84.7 80.2 79.8Table 5: A comparison of the LAS top scores from 2006 and 2007.
Official scoring conditions in boldface.For Turkish, scores with punctuation also include word-internal dependencies.produced the most unstable results with respect toLAS is Turkish.In comparison to last year?s languages, the lan-guages involved in the multilingual track this yearcan be more easily separated into three classes withrespect to top scores:?
Low (76.31?76.94):Arabic, Basque, Greek?
Medium (79.19?80.21):Czech, Hungarian, Turkish?
High (84.40?89.61):Catalan, Chinese, English, ItalianIt is interesting to see that the classes are more easilydefinable via language characteristics than via char-acteristics of the data sets.
The split goes acrosstraining set size, original data format (constituentvs.
dependency), sentence length, percentage of un-known words, number of dependency labels, and ra-tio of (C)POSTAGS and dependency labels.
Theclass with the highest top scores contains languageswith a rather impoverished morphology.
Mediumscores are reached by the two agglutinative lan-guages, Hungarian and Turkish, as well as by Czech.The most difficult languages are those that combinea relatively free word order with a high degree of in-flection.
Based on these characteristics, one wouldexpect to find Czech in the last class.
However, theCzech training set is four times the size of the train-ing set for Arabic, which is the language with thelargest training set of the difficult languages.However, it would be wrong to assume that train-ing set size alone is the deciding factor.
A closerlook at table 1 shows that while Basque and Greekin fact have small training data sets, so do Turk-ish and Italian.
Another factor that may be asso-ciated with the above classification is the percent-age of new words (PNW) in the test set.
Thus, theexpectation would be that the highly inflecting lan-guages have a high PNW while the languages withlittle morphology have a low PNW.
But again, thereis no direct correspondence.
Arabic, Basque, Cata-lan, English, and Greek agree with this assumption:Catalan and English have the smallest PNW, andArabic, Basque, and Greek have a high PNW.
Butthe PNW for Italian is higher than for Arabic andGreek, and this is also true for the percentage ofnew lemmas.
Additionally, the highest PNW can befound in Hungarian and Turkish, which reach higherscores than Arabic, Basque, and Greek.
These con-siderations suggest that highly inflected languageswith (relatively) free word order need more trainingdata, a hypothesis that will have to be investigatedfurther.There are four languages which were included inthe shared tasks on multilingual dependency pars-ing both at CoNLL 2006 and at CoNLL 2007: Ara-bic, Chinese, Czech, and Turkish.
For all four lan-guages, the same treebanks were used, which allowsa comparison of the results.
However, in some casesthe size of the training set changed, and at least onetreebank, Turkish, underwent a thorough correctionphase.
Table 5 shows the top scores for LAS.
Sincethe official scores excluded punctuation in 2006 butincludes it in 2007, we give results both with andwithout punctuation for both years.For Arabic and Turkish, we see a great improve-ment of approximately 9 and 6 percentage points.For Arabic, the number of tokens in the trainingset doubled, and the morphological annotation wasmade more informative.
The combined effect ofthese changes can probably account for the substan-tial improvement in parsing accuracy.
For Turkish,the training set grew in size as well, although only by600 sentences, but part of the improvement for Turk-ish may also be due to continuing efforts in error cor-927rection and consistency checking.
We see that thechoice to include punctuation or not makes a largedifference for the Turkish scores, since non-final IGsof a word are counted as punctuation (because theyhave the underscore character as their FORM value),which means that word-internal dependency linksare included if punctuation is included.9 However,regardless of whether we compare scores with orwithout punctuation, we see a genuine improvementof approximately 6 percentage points.For Chinese, the same training set was used.Therefore, the drop from last year?s top score to thisyear?s is surprising.
However, last year?s top scor-ing system for Chinese (Riedel et al, 2006), whichdid not participate this year, had a score that wasmore than 3 percentage points higher than the sec-ond best system for Chinese.
Thus, if we comparethis year?s results to the second best system, the dif-ference is approximately 2 percentage points.
Thisfinal difference may be attributed to the properties ofthe test sets.
While last year?s test set was taken fromthe treebank, this year?s test set contains texts fromother sources.
The selection of the textual basis alsosignificantly changed average sentence length: TheChinese training set has an average sentence lengthof 5.9.
Last year?s test set alo had an average sen-tence length of 5.9.
However, this year, the averagesentence length is 7.5 tokens, which is a significantincrease.
Longer sentences are typically harder toparse due to the increased likelihood of ambiguousconstructions.Finally, we note that the performance for Czechis almost exactly the same as last year, despite thefact that the size of the training set has been reducedto approximately one third of last year?s training set.It is likely that this in fact represents a relative im-provement compared to last year?s results.6.1.2 Across SystemsThe LAS over all languages ranges from 80.32 to54.55.
The comparison of the system ranks aver-aged over all languages with the ranks for single lan-9The decision to include word-internal dependencies in thisway can be debated on the grounds that they can be parsed de-terministically.
On the other hand, they typically correspond toregular dependencies captured by function words in other lan-guages, which are often easy to parse as well.
It is thereforeunclear whether scores are more inflated by including word-internal dependencies or deflated by excluding them.guages show considerably more variation than lastyear?s systems.
Buchholz and Marsi (2006) reportthat ?
[f]or most parsers, their ranking differs at mosta few places from their overall ranking?.
This year,for all of the ten best performing systems with re-spect to LAS, there is at least one language for whichtheir rank is at least 5 places different from theiroverall rank.
The most extreme case is the top per-forming Nilsson system (Hall et al, 2007a), whichreached rank 1 for five languages and rank 2 fortwo more languages.
Their only outlier is for Chi-nese, where the system occupies rank 14, with aLAS approximately 9 percentage points below thetop scoring system for Chinese (Sagae and Tsujii,2007).
However, Hall et al (2007a) point out thatthe official results for Chinese contained a bug, andthe true performance of their system was actuallymuch higher.
The greatest improvement of a sys-tem with respect to its average rank occurs for En-glish, for which the system by Nguyen et al (2007)improved from the average rank 15 to rank 6.
Twomore outliers can be observed in the system of Jo-hansson and Nugues (2007b), which improves fromits average rank 12 to rank 4 for Basque and Turkish.The authors attribute this high performance to theirparser?s good performance on small training sets.However, this hypothesis is contradicted by their re-sults for Greek and Italian, the other two languageswith small training sets.
For these two languages,the system?s rank is very close to its average rank.6.1.3 An Experiment in System CombinationHaving the outputs of many diverse dependencyparsers for standard data sets opens up the interest-ing possibility of parser combination.
To combinethe outputs of each parser we used the method ofSagae and Lavie (2006).
This technique assigns toeach possible labeled dependency a weight that isequal to the number of systems that included the de-pendency in their output.
This can be viewed asan arc-based voting scheme.
Using these weightsit is possible to search the space of possible depen-dency trees using directed maximum spanning treealgorithms (McDonald et al, 2005).
The maximumspanning tree in this case is equal to the tree that onaverage contains the labeled dependencies that mostsystems voted for.
It is worth noting that variantsof this scheme were used in two of the participating9285 10 15 20Number of Systems8082848688AccuracyUnlabeled AccuracyLabeled AccuracyFigure 1: System Combinationsystems, the Nilsson system (Hall et al, 2007a) andthe system of Sagae and Tsujii (2007).Figure 1 plots the labeled and unlabeled accura-cies when combining an increasing number of sys-tems.
The data used in the plot was the output of allcompeting systems for every language in the mul-tilingual track.
The plot was constructed by sort-ing the systems based on their average labeled accu-racy scores over all languages, and then incremen-tally adding each system in descending order.10 Wecan see that both labeled and unlabeled accuracy aresignificantly increased, even when just the top threesystems are included.
Accuracy begins to degradegracefully after about ten different parsers have beenadded.
Furthermore, the accuracy never falls belowthe performance of the top three systems.6.2 Domain Adaptation TrackFor this task, the results are rather surprising.
A lookat the LAS and UAS for the chemical research ab-stracts shows that there are four closed systems thatoutperform the best scoring open system.
The bestsystem (Sagae and Tsujii, 2007) reaches an LAS of81.06 (in comparison to their LAS of 89.01 for theEnglish data set in the multilingual track).
Consider-ing that approximately one third of the words of thechemical test set are new, the results are noteworthy.The next surprise is to be found in the relativelylow UAS for the CHILDES data.
At a first glance,this data set has all the characteristics of an easy10The reason that there is no data point for two parsers isthat the simple voting scheme adopted only makes sense with atleast three parsers voting.set; the average sentence is short (12.9 words), andthe percentage of new words is also small (6.10%).Despite these characteristics, the top UAS reaches62.49 and is thus more than 10 percentage pointsbelow the top UAS for the chemical data set.
Onemajor reason for this is that auxiliary and mainverb dependencies are annotated differently in theCHILDES data than in the WSJ training set.
As aresult of this discrepancy, participants were not re-quired to submit results for the CHILDES data.
Thebest performing system on the CHILDES corpus isan open system (Bick, 2007), but the distance tothe top closed system is approximately 1 percent-age point.
In this domain, it seems more feasible touse general language resources than for the chemi-cal domain.
However, the results prove that the extraeffort may be unnecessary.7 ConclusionTwo years of dependency parsing in the CoNLLshared task has brought an enormous boost to thedevelopment of dependency parsers for multiple lan-guages (and to some extent for multiple domains).But even though nineteen languages have been cov-ered by almost as many different parsing and learn-ing approaches, we still have only vague ideas aboutthe strengths and weaknesses of different methodsfor languages with different typological characteris-tics.
Increasing our knowledge of the multi-causalrelationship between language structure, annotationscheme, and parsing and learning methods probablyremains the most important direction for future re-search in this area.
The outputs of all systems for alldata sets from the two shared tasks are freely avail-able for research and constitute a potential gold minefor comparative error analysis across languages andsystems.For domain adaptation we have barely scratchedthe surface so far.
But overcoming the bottleneckof limited annotated resources for specialized do-mains will be as important for the deployment ofhuman language technology as being able to handlemultiple languages in the future.
One result fromthe domain adaptation track that may seem surpris-ing at first is the fact that closed class systems out-performed open class systems on the chemical ab-stracts.
However, it seems that the major problem in929adapting pre-existing parsers to the new domain wasnot the domain as such but the mapping from thenative output of the parser to the kind of annotationprovided in the shared task data sets.
Thus, find-ing ways of reusing already invested developmentefforts by adapting the outputs of existing systemsto new requirements, without substantial loss in ac-curacy, seems to be another line of research that maybe worth pursuing.AcknowledgmentsFirst and foremost, we want to thank all the peo-ple and organizations that generously provided uswith treebank data and helped us prepare the datasets and without whom the shared task would havebeen literally impossible: Otakar Smrz, CharlesUniversity, and the LDC (Arabic); Maxux Aranz-abe, Kepa Bengoetxea, Larraitz Uria, Koldo Go-jenola, and the University of the Basque Coun-try (Basque); Ma.
Anto`nia Mart??
Anton?
?n, Llu?
?sMa`rquez, Manuel Bertran, Mariona Taule?, DifdaMonterde, Eli Comelles, and CLiC-UB (Cata-lan); Shih-Min Li, Keh-Jiann Chen, Yu-MingHsieh, and Academia Sinica (Chinese); Jan Hajic?,Zdenek Zabokrtsky, Charles University, and theLDC (Czech); Brian MacWhinney, Eric Davis, theCHILDES project, the Penn BioIE project, andthe LDC (English); Prokopis Prokopidis and ILSP(Greek); Csirik Ja?nos and Zolta?n Alexin (Hun-garian); Giuseppe Attardi, Simonetta Montemagni,Maria Simi, Isidoro Barraco, Patrizia Topi, KirilRibarov, Alessandro Lenci, Nicoletta Calzolari,ILC, and ELRA (Italian); Gu?ls?en Eryig?it, KemalOflazer, and Ruket C?ak?c?
(Turkish).Secondly, we want to thank the organizers of lastyear?s shared task, Sabine Buchholz, Amit Dubey,Erwin Marsi, and Yuval Krymolowski, who solvedall the really hard problems for us and answered allour questions, as well as our colleagues who helpedreview papers: Jason Baldridge, Sabine Buchholz,James Clarke, Gu?ls?en Eryig?it, Kilian Evang, Ju-lia Hockenmaier, Yuval Krymolowski, Erwin Marsi,Bea?ta Megyesi, Yannick Versley, and AlexanderYeh.
Special thanks to Bertjan Busser and ErwinMarsi for help with the CoNLL shared task websiteand many other things, and to Richard Johansson forletting us use his conversion tool for English.Thirdly, we want to thank the program chairsfor EMNLP-CoNLL 2007, Jason Eisner and TakuKudo, the publications chair, Eric Ringger, theSIGNLL officers, Antal van den Bosch, Hwee TouNg, and Erik Tjong Kim Sang, and members of theLDC staff, Tony Castelletto and Ilya Ahtaridis, forgreat cooperation and support.Finally, we want to thank the following people,who in different ways assisted us in the organi-zation of the CoNLL 2007 shared task: GiuseppeAttardi, Eckhard Bick, Matthias Buch-Kromann,Xavier Carreras, Tomaz Erjavec, Svetoslav Mari-nov, Wolfgang Menzel, Xue Nianwen, Gertjan vanNoord, Petya Osenova, Florian Schiel, Kiril Simov,Zdenka Uresova, and Heike Zinsmeister.ReferencesA.
Abeille?, editor.
2003.
Treebanks: Building and UsingParsed Corpora.
Kluwer.I.
Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,A.
Diaz de Ilarraza, A. Garmendia, and M. Oronoz.2003.
Construction of a Basque dependency treebank.In Proc.
of the 2nd Workshop on Treebanks and Lin-guistic Theories (TLT).G.
Attardi, F. Dell?Orletta, M. Simi, A. Chanev, andM.
Ciaramita.
2007.
Multilingual dependency pars-ing and domain adaptation using desr.
In Proc.
of theCoNLL 2007 Shared Task.
EMNLP-CoNLL.E.
Bick.
2007.
Hybrid ways to improve domain inde-pendence in an ML dependency parser.
In Proc.
of theCoNLL 2007 Shared Task.
EMNLP-CoNLL.J.
Blitzer, R. McDonald, and F. Pereira.
2006.
Domainadaptation with structural correspondence learning.
InProc.
of the Conf.
on Empirical Methods in NaturalLanguage Processing (EMNLP).A.
Bo?hmova?, J.
Hajic?, E.
Hajic?ova?, and B. Hladka?.
2003.The PDT: a 3-level annotation scenario.
In Abeille?
(2003), chapter 7, pages 103?127.R.
Brown.
1973.
A First Language: The Early Stages.Harvard University Press.S.
Buchholz and E. Marsi.
2006.
CoNLL-X sharedtask on multilingual dependency parsing.
In Proc.
ofthe Tenth Conf.
on Computational Natural LanguageLearning (CoNLL).S.
Canisius and E. Tjong Kim Sang.
2007.
A constraintsatisfaction approach to dependency parsing.
In Proc.of the CoNLL 2007 Shared Task.
EMNLP-CoNLL.930X.
Carreras.
2007.
Experiments with a high-order pro-jective dependency parser.
In Proc.
of the CoNLL 2007Shared Task.
EMNLP-CoNLL.E.
Charniak.
2000.
A maximum-entropy-inspired parser.In Proc.
of the First Meeting of the North AmericanChapter of the Association for Computational Linguis-tics (NAACL).C.
Chelba and A. Acero.
2004.
Adaptation of maxi-mum entropy capitalizer: Little data can help a lot.
InProc.
of the Conf.
on Empirical Methods in NaturalLanguage Processing (EMNLP).K.
Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang,and Z. Gao.
2003.
Sinica treebank: Design criteria,representational issues and implementation.
In Abeille?
(2003), chapter 13, pages 231?248.W.
Chen, Y. Zhang, and H. Isahara.
2007.
A two-stageparser for multilingual dependency parsing.
In Proc.of the CoNLL 2007 Shared Task.
EMNLP-CoNLL.M.
Collins.
1997.
Three generative, lexicalised mod-els for statistical parsing.
In Proc.
of the 35th AnnualMeeting of the Association for Computational Linguis-tics (ACL).M.
A. Covington.
2001.
A fundamental algorithm fordependency parsing.
In Proc.
of the 39th Annual ACMSoutheast Conf.D.
Csendes, J. Csirik, T. Gyimo?thy, and A. Kocsor.
2005.The Szeged Treebank.
Springer.H.
Daume?
and D. Marcu.
2006.
Domain adaptation forstatistical classifiers.
Journal of Artificial IntelligenceResearch, 26:101?126.M.
Dredze, J. Blitzer, P. P. Talukdar, K. Ganchev,J.
Graca, and F. Pereira.
2007.
Frustratingly hard do-main adaptation for dependency parsing.
In Proc.
ofthe CoNLL 2007 Shared Task.
EMNLP-CoNLL.X.
Duan, J. Zhao, and B. Xu.
2007.
Probabilistic parsingaction models for multi-lingual dependency parsing.In Proc.
of the CoNLL 2007 Shared Task.
EMNLP-CoNLL.G.
Eryig?it.
2007.
ITU validation set for Metu-Sabanc?Turkish Treebank.
URL: http://www3.itu.edu.tr/?gulsenc/papers/validationset.pdf.R.
Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kamb-hatla, A. Luo, N. Nicolov, and S. Roukos.
2004.
Astatisical model for multilingual entity detection andtracking.
In Proc.
of the Human Language TechnologyConf.
and the Annual Meeting of the North AmericanChapter of the Association for Computational Linguis-tics (HLT/NAACL).D.
Gildea.
2001.
Corpus variation and parser perfor-mance.
In Proc.
of the Conf.
on Empirical Methods inNatural Language Processing (EMNLP).J.
Hajic?, O.
Smrz?, P. Zema?nek, J.
S?naidauf, and E. Bes?ka.2004.
Prague Arabic dependency treebank: Develop-ment in data and tools.
In Proc.
of the NEMLAR In-tern.
Conf.
on Arabic Language Resources and Tools.J.
Hall, J. Nilsson, J. Nivre, G. Eryig?it, B. Megyesi,M.
Nilsson, and M. Saers.
2007a.
Single malt orblended?
A study in multilingual parser optimization.In Proc.
of the CoNLL 2007 Shared Task.
EMNLP-CoNLL.K.
Hall, J. Havelka, and D. Smith.
2007b.
Log-linearmodels of non-projective trees, k-best MST parsingand tree-ranking.
In Proc.
of the CoNLL 2007 SharedTask.
EMNLP-CoNLL.R.
Johansson and P. Nugues.
2007a.
Extendedconstituent-to-dependency conversion for English.
InProc.
of the 16th Nordic Conf.
on Computational Lin-guistics (NODALIDA).R.
Johansson and P. Nugues.
2007b.
Incremental depen-dency parsing using online learning.
In Proc.
of theCoNLL 2007 Shared Task.
EMNLP-CoNLL.T.
Kudo and Y. Matsumoto.
2002.
Japanese dependencyanalysis using cascaded chunking.
In Proc.
of the SixthConf.
on Computational Language Learning (CoNLL).S.
Kulick, A. Bies, M. Liberman, M. Mandel, R. Mc-Donald, M. Palmer, A. Schein, and L. Ungar.
2004.Integrated annotation for biomedical information ex-traction.
In Proc.
of the Human Language TechnologyConf.
and the Annual Meeting of the North AmericanChapter of the Association for Computational Linguis-tics (HLT/NAACL).B.
MacWhinney.
2000.
The CHILDES Project: Toolsfor Analyzing Talk.
Lawrence Erlbaum.P.
R. Mannem.
2007.
Online learning for determinis-tic dependency parsing.
In Proc.
of the CoNLL 2007Shared Task.
EMNLP-CoNLL.M.
Marcus, B. Santorini, and M. Marcinkiewicz.
1993.Building a large annotated corpus of English: the PennTreebank.
Computational Linguistics, 19(2):313?330.S.
Marinov.
2007.
Covington variations.
In Proc.
of theCoNLL 2007 Shared Task.
EMNLP-CoNLL.M.
A.
Mart?
?, M.
Taule?, L. Ma`rquez, and M. Bertran.2007.
CESS-ECE: A multilingual and multilevelannotated corpus.
Available for download from:http://www.lsi.upc.edu/?mbertran/cess-ece/.931D.
McClosky, E. Charniak, and M. Johnson.
2006.Reranking and self-training for parser adaptation.
InProceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meetingof the Association for Computational Linguistics.R.
McDonald and J. Nivre.
2007.
Characterizing theerrors of data-driven dependency parsing models.
InProc.
of the Joint Conf.
on Empirical Methods in Nat-ural Language Processing and Computational NaturalLanguage Learning (EMNLP-CoNLL).R.
McDonald and F. Pereira.
2006.
Online learning ofapproximate dependency parsing algorithms.
In Proc.of the 11th Conf.
of the European Chapter of the Asso-ciation for Computational Linguistics (EACL).R.
McDonald, F. Pereira, K. Ribarov, and J. Hajic?.
2005.Non-projective dependency parsing using spanningtree algorithms.
In Proc.
of the Human LanguageTechnology Conf.
and the Conf.
on Empirical Methodsin Natural Language Processing (HLT/EMNLP).S.
Montemagni, F. Barsotti, M. Battista, N. Calzolari,O.
Corazzari, A. Lenci, A. Zampolli, F. Fanciulli,M.
Massetani, R. Raffaelli, R. Basili, M. T. Pazienza,D.
Saracino, F. Zanzotto, N. Nana, F. Pianesi, andR.
Delmonte.
2003.
Building the Italian Syntactic-Semantic Treebank.
In Abeille?
(2003), chapter 11,pages 189?210.T.
Nakagawa.
2007.
Multilingual dependency parsingusing Gibbs sampling.
In Proc.
of the CoNLL 2007Shared Task.
EMNLP-CoNLL.L.-M. Nguyen, T.-P. Nguyen, and A. Shimazu.
2007.
Amultilingual dependency analysis system using onlinepassive-aggressive learning.
In Proc.
of the CoNLL2007 Shared Task.
EMNLP-CoNLL.J.
Nivre, J.
Hall, J. Nilsson, A. Chanev, G. Eryig?it,S.
Ku?bler, S. Marinov, and E. Marsi.
2007.
Malt-Parser: A language-independent system for data-driven dependency parsing.
Natural Language Engi-neering, 13:95?135.K.
Oflazer, B.
Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r.2003.
Building a Turkish treebank.
In Abeille?
(2003),chapter 15, pages 261?277.P.
Prokopidis, E. Desypri, M. Koutsombogera, H. Papa-georgiou, and S. Piperidis.
2005.
Theoretical andpractical issues in the construction of a Greek depen-dency treebank.
In Proc.
of the 4th Workshop on Tree-banks and Linguistic Theories (TLT).S.
Riedel, R.
C?ak?c?, and I. Meza-Ruiz.
2006.
Multi-lingual dependency parsing with incremental integerlinear programming.
In Proc.
of the Tenth Conf.
onComputational Natural Language Learning (CoNLL).B.
Roark and M. Bacchiani.
2003.
Supervised and un-supervised PCFG adaptation to novel domains.
InProc.
of the Human Language Technology Conf.
andthe Annual Meeting of the North American Chap-ter of the Association for Computational Linguistics(HLT/NAACL).K.
Sagae and A. Lavie.
2006.
Parser combination byreparsing.
In Proc.
of the Human Language Technol-ogy Conf.
of the North American Chapter of the Asso-ciation of Computational Linguistics (HLT/NAACL).K.
Sagae and J. Tsujii.
2007.
Dependency parsingand domain adaptation with LR models and parser en-sembles.
In Proc.
of the CoNLL 2007 Shared Task.EMNLP-CoNLL.M.
Schiehlen and Kristina Spranger.
2007.
Global learn-ing of labelled dependency trees.
In Proc.
of theCoNLL 2007 Shared Task.
EMNLP-CoNLL.G.
Schneider, K. Kaljurand, F. Rinaldi, and T. Kuhn.2007.
Pro3Gres parser in the CoNLL domain adap-tation shared task.
In Proc.
of the CoNLL 2007 SharedTask.
EMNLP-CoNLL.N.
Shimizu and H. Nakagawa.
2007.
Structural corre-spondence learning for dependency parsing.
In Proc.of the CoNLL 2007 Shared Task.
EMNLP-CoNLL.I.
Titov and J. Henderson.
2006.
Porting statisticalparsers with data-defined kernels.
In Proc.
of the TenthConf.
on Computational Natural Language Learning(CoNLL).I.
Titov and J. Henderson.
2007.
Fast and robust mul-tilingual dependency parsing with a generative latentvariable model.
In Proc.
of the CoNLL 2007 SharedTask.
EMNLP-CoNLL.R.
Watson and T. Briscoe.
2007.
Adapting the RASPsystem for the CoNLL07 domain-adaptation task.
InProc.
of the CoNLL 2007 Shared Task.
EMNLP-CoNLL.Y.-C. Wu, J.-C. Yang, and Y.-S. Lee.
2007.
Multi-lingual deterministic dependency parsing frameworkusing modified finite Newton method support vectormachines.
In Proc.
of the CoNLL 2007 Shared Task.EMNLP-CoNLL.H.
Yamada and Y. Matsumoto.
2003.
Statistical depen-dency analysis with support vector machines.
In Proc.8th International Workshop on Parsing Technologies(IWPT).932
