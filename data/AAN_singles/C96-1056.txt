GRICE INCORPORATEDCooperativity in Spoken DialogueLaila Dybkj~er, Niels Ole Bernsen and Hans Dybkj~erCentre for Cognitive Science, Roskilde UniversityPO Box 260, DK-4000 Roskilde, Denmarkemails: laila@cog.ruc.dk, nob@cog.ruc.dk, dybkjaer@cog.ruc.dkphone: +45 46 75 77 l 1 fax: +45 46 75 45 02AbstractThe paper presents a consolidated set of princip-les of cooperative spoken human-machine dialo-gue which have the potential tor being turnedinto practically applicable design guidelines.
Theprinciples have been validated in three ways.They were established fi'om a Wizard of Oz sim-ulation corpus used to develop the dialoguemodel for a spoken language dialogue system.Developed independently of Gricean theory,some of the principles were refined through com-parison with Grice's maxims of cooperativity inconversation.
Finally, the principles were testedin the user test of the implemented dialogue sys-tem.
The paper shows that Grice's maxims con-stitute a sub-set of the principles.
The non-Gricean principles and dialogue aspects they in-troduce am presented and discussed.1 IntroductionIn the last four years, we have designed and imple-mented the dialogue component of a spoken languagedialogue system (SLDS) prototype in the domain offlight ticket reservation.
The aim has been to developa realistic, application-oriented prototype whose dia-logue management allows users to perform their res-ervation task in spontaneous and natural spoken lan-guage.
Being well-structured, the ticket reservationtask generally lends itself to system-directed dialoguein which the user answers questions posed by thesystem.
The only user initiative our system permits isthat users may initiate clarification and repair meta-communication through uttering the keywords'repeat' and 'change'.
In designing such a system, itis crucial to reduce the number of situations in whichusers are inclined to take other forms of dialogueinitiative, such as asking questions when they do notunderstand the system's dialogue behaviour or pro-viding information which the system did not ask for(Schegloff et al 1977).
This is why the issue of dia-logue cooperativity came to play a central role in ourdesign of the dialogue structure.
We needed to opti-mist system dialogue cooperativity in order to pre-vent situations uch as those described above.
To thisend, we developed a set of general principles to beobserved in the design of cooperative, spoken human-machine dialogue.
The principles have been validatedin three ways.
Firstly, they were developed on thebasis of a simulated human-nmchine dialogue corpuscollected uring dialogue model design.
Secondly, wecompared the principles with Grice's maxims of co-operative human-human dialogue.
Thirdly, the prin-ciples were tested against he dialogue corpus fi'omthe user test of the implemented system.This paper analyses the relationship between ourprinciples and Grice's maxims.
We first describe howthe principles were developed (Section 2).
We thenjustify the comparison between principles and max-ires (Section 3).
Section 4 compares principles andmaxims.
Section 5 briefly describes how the princi-ples were tested the on user test dialogue corpus, andSection 6 concludes the paper.2 Developing and Testing Principles ofCooperative Human-Machine DialogueThe dialogue model for otu" flight reservation systemwas developed by the Wizard of Oz (WOZ) experi-mental prototyping method in which a person simu-lates the system to be designed (Fraser and Gilbert1991).
Development was iterated until the dialoguemodel satisfied the design constraints on, i.a:, averageuser utterance length.
The dialogues were recorded,transcribed, analysed and used as a basis for ina-provements oil the dialogue model.
We perlormedseven WOZ iterations yielding a transcribed corpus of125 task-oriented human-machine dialogues corre-sponding to approximately seven hours of spokendialogue.
The 94 dialogues that were recorded uringthe last two WOZ iterations were performed by ex-ternal subjects whereas only system designers andcolleagues had participated in the earlier iterations.
Atotal of 24 different subjects were involved in theseven iterations.
Dialogues were based on writtendescriptions of reservation tasks (scenarios).A major concern during WOZ was to detect prob-lems of user-system interaction.
We eventually usedthe following two approaches to systematically dis-cover such problems: ( ) prior to each WOZ iterationwe matched the scenarios to be used against he cur-rent dialogue model.
The model was represented as agraph structure with system phrases in the nodes andexpected contents of user answers along the edges.
Ifa deviation from the graph occurred during the mat-ching process, this would indicate a potential dia-328logue design problem which should be removed, il'possible.
(ii) The recorded dialogues were plottedonto the graph representing the dialogue model.
As in(i), graph deviations indicated potential dialogue de-sign problems.
Deviations were marked and theircauses analysed whereupon the dialogue model wasrevised, if necessary.At the end of the WOZ design phase, we began alllOl'e theoretical, forward-looking exercise.
All theproblenis of inleractioii uncovered dr!ring WOZ woreanalysed and represented asviolations of principles ofcooperative dialogue.
Each problem was considered acase in which the system, in addressing the user, iiadviolated a principle of cooperative dialogue.
Theprinciples of cooperative dialogue were made ex-plicit, based on the problems analysis.
The WOZ cor-pus analysis led to the identification of 14 principlesel: cooperative hun3an-machine dialogue (Section 4)based on analysis o1' 120 examples o1: user-systeillinteraction problems.
\[1: the principles were observedin the design of the system's dialogue behaviour, weassunled, this would serve to reduce the occurrence ofuser diah)gue behaviour that lhe sysiem had not beendesigned to handle.3 Maxims and Principles of CooperativeDialogueWe had developed our principles of cooperative hu-nlan-nmchine dialogue indel)endently o1" Gricean co-,operativity theory (Bernsen et al, 1996a).
\])rior to theuser test (Section 5), we colni)ared the principles with(h'ice's Cooperative Principle and maxims.
In lhisprocess Ihe principles achieved their current lornl asshown in Table 1.
Their original expression is pre-sented in Section 4.
Grice's Cooperative Principle(CP) is a general principle which says thai, to actcooperatively in conversation, oilc should make one's"conversational contribution such as is required, atthe stage at which it occtlrs, by tile accepted purposeor direction of the talk exchange in which one is en-gaged" (Grice 1975).
Grice proposes lhat the CP canbe explicated in terms of four groups of simple max-ires which are not claimed to be jointly exhaustive.The maxims are marked with an asterisk in Table 1.Grice focuses on dialogues in which the interloc-utors want to achieve a shared goal (Grandy 1989,Sarangi and Slembrouck 1992).
In such dialogues, lieclaims, adherence to the maxims is rational because itenstu'es Ihal the interlocutors pursue the shared goalmost efl'iciently.
Task-oriented ialogue, such as thatof our SLDS, is a paradigm case of shared-goal dia-logue.
Grico, however, did not develop the inaxinlswith the purpose of preveuthlg coinmunication failurein shared-goal dialogue.
Rather, his interest lies iri theinl~rences which an interlocutor is able to make whenthe speaker deliberately violates oue of the maxims.I\[te calls such deliberate speaker's messages 'conver-sational implicatures'.
Grice's maxims, althoughhaving been conceived for a dilTerent purpose, never-theless erve the same objective as do ()tit" p,inciples,namely that of achieving the dialogue goal as directlyand smoothly as possible, e.g.
by preventing ques-tions of claril\]cation.
It is exactly when a hunian or,for that inatler, an SLDS, non-deliberately viohttes amaxim, that dialogue clarification problems are likelyto occur.
Thus, the main dil:ference between Grice'swork and ours is that the maxims were developed toaccount for cooperalivity in hUlllall-hUlnal/ dialogue,whereas our principles were developed to aceoulll \['orcooperativity in hunlan-nmchhle dialogue.
Givca theconllnonalily of purpose, it beconies of interest toconlpare principles and Illaxinls.
We waut to showthat the principles include the illaXilllS as a subset endthus provides a corpus-based confirmation of theirvalidity for spoken human-machine dialogue.
More(>vet', the principles manifest aspects of cooperativetask-oriented dialogue which were not addressed byGrice.4 Comparison between Maxims andPrinciplesIn this section we analyse the relationship betweenGrice's nmxiins and our principles el: dialogue coop-erativity.
A \[irst aim is to demonstrate hat a sub-setof the principles are roughly equivalent to tile ntax-hns.
We Ihen argue that the renlaining principles ex-press additional aspects of cooperativity.
The dislinc-lion between I, ri,lcQ)h" andaxlWCt (Table l) is theo-retically intportant because an aspect represents tilel)roperty o1' dialogue addressed by a particular maximor prhlciple.
One result of analysing Ihe rehltionshipbetween principles and nlaxhns is the distinction,shown in the tables, 1)elween ?,eneric and specificprinciples.
Grice's ulaxims are all generic.
A genericprinciple lnay subsunle one or ltlore specific princi-ples which specialise the generic principle to certainclasses of phenomena.
Although important to SIA)Sdesign, specific principles may be less signil\]cant to ageneral account of dialogue cooperativity.4.1 Pr inc ip les wh ich  are Reduc ib le  to Max imsGrice's nmxims of truth and evidence (GP3, (7I)4)have no coui/terparts aniong ()ttr t~,'inciples but inaysimply be inchided among the principles.
The reasonis that one does not design an SLDS in the domain o1'air ticket reservation which provides l:alse or unfounded information to cuslomers.
In other words, themaxims of truth and evidence are so important to thedesign o1: SLI)Ss that they are unlikely to emergeduriug dialogue design problenl-solving.
During sys-toni inll)lenlenlalion , one constantly worries abouttruth and evidence.
It canuoi be allowed, for instance,that the system confirrns infornlatioll which has ilOlbeen checked with the database and which might befalse or impossible.
Grice (1975) observed the i:un-danlental nature of the maxims of truth and evidencein general and GP3 in particuhir (of.
Searle 1992).329Table 1.
The generic and specific principles of cooperativity in dialogue.
The generic principles are expressed at the samelevel of generality as are the Gricean maxims (marked with an *).
Each specific principle is subsumed by a generic principle.The left-hand column characterises the aspect of dialogue addressed by each principle.Dialogue AspectGroup 1: lnfl}rmativenessGroup 2:Truth and evidenceGroup 3:RelewmceGroup 4:MannerGroup 5:Partner asymmetryGroup 6:' Background knowledgeGroup 7:~ P, epair and clarificationGPno.
SPno.GP1GPI SP1GPI SP2GP2GP3GP4GP5GP6GP7GP7 SP3GP8GP9GP I 0GPI0 SP4GPI0 SP5GP11GP 11 SP6GPII SP7GPI2GPI2 SP8GPI3GP 13 ~P9GP13 SPI0GPI3 SPllGeneric or Specific Principlei*Make your contribution as informative as is required (for the current pur-poses of the exchange).Be fully explicit in communicating to users the commitments they havemade., , .Provide feedback on each piece of information provided b~?
the user.
*Do not make ~our contribution more infornmtive than is required.
*Do not say what you believe to be false.
*Do not say that for which you lack adequate evidence.
*Be relevant, i.e.
Be appropriate othe immediate needs tit each stage of thetransaction.
*Avoid obscurity of expressio{I..*Avoid ambiguity:..!
Provide same formulation of the same question (or addre~)'to users every--where in the s},steln's dialo~uc, turns.
*Be brief (avoid unnecessary/t~rolixit~/).
.~.
* Be orderly.Inform the dialogue partners of important non-normal ch?racteristics whichthey should take into account in order to behave cooperatively in diak)gue.Provide clear and comprehensible communication f what the system canand cannot do.I Provide clear and sttfficient instructions tousers on how to interact with thesystem.
'Fake partners' relevant b.ack~round knowledge into account.Take into account possible (and possibly emmeous) user inferences by anal-ogy from related task domains.Separate whenever possihle between tire needs of novice and expert users(user-adaptive dialogue).Take into account legitimate partner expectalions a to your own backgroundk n o w l c d s e .
.
.
.
...Provide sufficient task domain knowledge and inference.Initiate repair or clarification l cta-connnunication in case of comlntlnicationl'ailure.Provide ability to initiate repair it' s~/stem understandin~ has failed.Initiate clarification recta-communication in case of inconsistent user input.hfitiate clarification recta-communication in case of ambifzuous user input.The following principles have counterparts among themaxims:1.
Avoid 'semantical noise' in addressing users.
(1) is a generalised version of GP6 (non-obscurity)and GP7 (non-ambiguity).
Its infelicitous expressionwas due to the fact that we wanted to cover observedambiguity and related phenomena in one principle butfailed to find an appropriate technical term for thepurpose.
(I) may, without any consequence other thanimproved clarity, be replaced by GP6 and GP7.2.
Avoid superfluous or redundant interactionswith users (relative to their contextual needs).
(2) is virtually equivalent o GP2 (do not overdo in-lormativeness) and GP5 (relevance).
Grice observedthe overlap between GP2 and GP5 (Grice 1975).
(2)may, without any consequence other than improvedclarity, be replaced by GP2 and GP5.3.
It should be possible for users to fully exploilthe system's task domain knowledge when theyneed it.
(3) can be considered an application o1' GPI ( in fermativeness) and GP9 (orderliness), as follows.
If thesystem adheres to GPI and GP9, there is a maximumlikelihood that users obtain the task domain knowl-edge they need from the syslem when they need it.Tbe system should say enough and address the task-relevant dialogue topics in an order which is as closeas possible to the order expected hy users.
If  the userexpects ome topic to come tip early in the dialogue,that topic's non-occurrence at its expected "place"may cause a clarif ication sub<lialogue which the330systeln c;_innc)t understand.
In WOZ Iteration 3, forinstance, the system did not ask users about their in-terest in discount fare.
Having expected the topic tocome tip for seine time, users therefore began to in-quire about discount when approaching lhe end of thereservalion dialogue.
(3) may be replaced by GP 1 andGP9 without significant loss.4.
P, educe system lalk as nnich as possible duringindividual dialogue turns.
(4) is near-equivalent to GP8 (brevity).Sunlmarising, the generic principles (1)-(4) mayhe replaced by maxims GPI ,  GP2 and GP5-GP9.These maxilns are capable of perforii/illg the sametask in guiding dialogue design.
In fact, as argtled, themaxims ai'o able to do the better job because they, i.e.GP6 and (IP7, aild GPI and GP9, respectively, spellOtll the illleilded coilteiits ill" two of Ihe princilfles.This provides COl'ptis-l)ased Coilfiiilllit\[Oil I)\[" lilUXililSGPI ,  (1152 and CII:'5-(;P9, i.e.
of ttleir staling basicprinciples of cooperative, task-oriented hulliall-illa-chine dialogue.
However, \[or dialogue design ptirpo-SOS, lhe illaXill/S illtlSt be augnlenled hy task-slJecl:/Tcor domain-sl)ecific princOHes, such as the \[bllowing.5 (SP3).
Provide same fornluhition of the samequcslion (or address) to users everywhere in thesystem's dialogue ttlrlls.
(5) represents an additional lWCCaUtion against theoccurrence of ambigttity in niachine speech.
It can besoeii as a Sl)ccial-purpose application o1' GP'/ (ilon-aulbiguity).6 (SPI).
t~;e fully explicil in et)lllnlunicating to tlS-el'S the COillillitlllents they have Illade,7 (SP2).
Provide feedback Oil e~lch piece of infor-Ination provided by lhe riser.Those principles are closely related.
The novel coot>orativity aspoel they introduce is lhal they require thecooperative speaker to produce a specific dialoguecontl'il~ution which explicitly expresses an intorprola-lion of the intorlocuior's previous diah)guo conlribu-lion(s), provided Ihal the interlocutor has inado ;.ldialogue contribution of a certain lypo, such as aconinlitnlonl Io book a flight.
We propose ihal theseprinciples be suhsunlod by (i l '  1 (infornialiveness).4.2 Prindples hicldng Equivalenls lililOlil4 theMaximsThe principles discussed in this section appear irre-ducible to maxims and thus serve to augment thescope of a theory of cooperativity.4.2.1 Dialogue Partner  AsymmetryDialogue partner asynimctry occurs, roughly, whenOllO or liloi'e of Iho dialogue partners is llot in a nor-lllal conditioll or situation, leer installCO, a dialoguepartner may have a hoalitlg deficiency or be locatedin a particularly noisy environnlcnt, in such cases,dialogue cooperativity depends Oll the taking intoaccotlnt o\[' that participant's pecial characteristics.For obvious reasons, dialogue partner asynmietry isimportant in SI,DS dialogue design.
The machine isnot a nornml dialogue partner and users have to beaware of this if communical ion faihire is to beavoided.
The following two principles address dia-logue parlnor asynllnelry:8 (SP4).
Provide clear and comprehensible com-Inunication ?
)1: what the system can and cannot do.9 (SP5).
I'rovide clear and sufficient instructionsIo users oil hOW \[() interact with tile system.Being limitcd in its task capabilities and intended forwalk-up-and-use application, our SLDS needs to pro-tect itself from unmanageahlc dialogue contributionsby providing users with an upq'ront mental model ofwhat it can and cannot do.
If this inclmtl model is toocomplex, uscrs will not acquire it; and il' the nlodcl istoo situplistic> its remaining details must be providedelsewhere during dialogue.
(8) adds an ilnportantclement to Ihc analysis of dialogue coopcrativily byaiming at inlproving user coopcrativily.
It shows that,at least in hunlan-nlachinc dialogue, coopcraiivity is afornmlly nlorc coniplcx pheuonlcnon than anticipatedby Gricc.
In addition to principles stating how aspeaker should hehavc, principles are needed tic-cording to which the speaker should consider trans-ferring part of the responsibility for cooperation to theinterlocutor.
(9) has a role sitnihu" to lhat of (8).The lnincil)lcs cxanlincd in this section inlroducca new aspect o1 dialogue cooperativity, naniely part-her asymmetry and speaker's consequent obligationto inform the partner(s) of non-nornml speaker char-acteristics, l)ue to lhe latter, the principles cannot besubsumed by any olhcr l)rinciplc or maxim.
We pro-pose Ihat (8) and (9) are both .vl~eci/k' princilJcs sub-sumed by a new generic pri,lcilfle:GPI0.
hlfortn the dialogue parttlors of inll)Ot'tantilOll-il(ll'lllal charactcrislics which they should takeinto accotilll in order tt) behave cooperatively indialogue.4.2.2 llackgrotmd Knowledge10 (GP I I ) .
Take users' relevant backgroundknowledge into account.
(;1511 is expressed at the level of generality of (h'icc'stheory.
The principle explicitly introduces two no-tions: the notion o1' interlocutors' background knowl-edge and that of possible dilTcrcnccs in backgroundknowledge between dilTercnt user populations andindividual users.
(;P1 I appears to be presupposed bymaxinm GPI,  GP2 and G155-(;1'9 in the sense that it isnot possible to adhc,'e to any of Ihese maxims withoutadhering to GP I I .
Moreover, in order to adhere toGPI I, it is necessary lkn" the speaker to recogniserelevant differences among inlerlocutors and inter-locutor groups in Icrms o1' background knowledge.Based on this recognition, a speaker either ah'cadyhas built prior to the dialogue, or adaptively buiktsduring dialogue, a model o1' the interlocutor whichserves to guide speaker coopcrativily.
Increased user331adaptivity in this sense is an important goal in SLDSdesign (Bernsen et al 1994).GPI l cannot be reduced to GPI (informativeness)because, first, GP1 does not refer to the notions ofbackground knowledge and differences in back-ground knowledge among interlocutors.
Second, aspeaker may adhere perfectly to 'exchange purpose'(cf.
GPI)  while ignoring the interlocutor's back-ground knowledge.
For instance, in the user test auser wanted to order a one-way ticket at discountprice.
The system, however, knew that discount isonly possible on return tickets.
It therefore did notoffer the discount option to this user nor did it correctthe user's misunderstanding.
At the end of the dia-logue, the frustrated user asked whether or not dis-count had been granted.
Third, as argued above,GPl l  is presupposed by maxims GPI, GP2 and GP5-GP9.
Grice, however, does not argue that GP1 is pre-supposed by those maxims whereas he does arguethat GP3 (truth) and GP4 (evidence) are presupposedby them (Grice 1975).
For similar reasons, GP5(relevance) (Sperber and Wilson 1987), cannot re-place GPI 1.
Informativeness and relewmce, there-fore, are not only functions of the purpose(s) of theexchange of infornmtion but also of the knowledge ofthe interlocutor.11 (SP8).
Provide sufficient ask domain knowl-edge and inference.
(11) may appear trivial as supportive of the design ofusable information service systems.
However, desig-ners of such systems are continuously confrontedwith questions about what the system should knowand what is just within, or barely outside, the sys-tem's intended or expected onmin of expertise.
Thesystem should behave as a perfect expert vis-~>vis itsusers within its declared domain of expertise, other-wise it is at fault.
In WOZ Iteration 7, for instance, asubject expressed surprise at not having been offeredthe option of being put on a waiting list in a case inwhich a flight was already fully booked.
We becameaware of the problem during the post-experimentalinterview.
However, the subject might just as wellhave asked a question during the dialogue.
Since ( 11 )deals with speaker's knowledge, it cannot be sub-smncd by GPl l .
We therefore propose to introduce anew generic principle which mirrors GP11:GPI2.
Take into account legitimate partner expec-tations as to your own background knowledge.
(11), then, is a specific principle subsumed by GPI2.12 (SP6).
Take into account possible (and possi-bly erroneous) user inferences by analogy fi'omrelated task domains.
(12) is a specific principle subsumed by GP1 l (back-ground knowledge).
It was developed from examplesof user mistmderstandings of the system due to rea-soning by analogy.
For instance, the fact that it ispossible to make reservations of stand-by tickets oninternational ilights may lead users to conclude (erro-neously) that this is also possible on domestic lqights.13 (SP7).
Separate whenever possible between theneeds of novice and expert users (user-adaptivedialogue).
(13) is another specifi'c principle subsumed by GPI I .Interlocutors may belong to different populationswith correspondingly different needs of informationin cooperative dialogue.
For instance, a user who hassuccessftdly used the dialogue system on several oc-casions no longer needs to be introduced to the sys-tem but is capable of launching on the ticket reserva-tion task right away.
A novice user, however, willneed to listen to the system's introdnction to itself.This distinction between the needs of expert andnovice users was introduced in WOZ Iteration 7 whenseveral users had complained that the system talkedtoo much.4.2.3 Meta-eommunicationEven if an SLDS is able to conduct a perfectly coop-erative dialogue, it will need to initiate repair andclarification meta-communication whenever it hasfailed to understand the user, for instance because ofspeech recognition or language understanding failure:14 (SP9).
Provide ability to initiate repair il' sys-tem understanding has failed.
(14) states what the cooperative speaker should do incase of failure to understand utterances made by theinterlocutor.
Our system adheres to (14) in that itcommunicates it failure to understand what the userjust said.
(14) cannot be subsumed by GPI (infor-mativeness) which ignores communication faihne.Together with the new specific principles from theuser test SPI0 and SP11 (Section 2), (14) is a specificprinciple of htmmn-machine dialogue which may besubsumed by:OPl 3.
Initiate repair or clarification meta-commu-nicatkm in case of communication failure.5.
The user testThe set of principles presented in Table 1, apart fi'omSP10 and SPII  (see below), were tested against henser test dialogue corpus (Dybkjmr et al, 1996).
Inthe user test the speech recogniser was simulatedwhereas the other system components were the imple-mented ones, The system was tested with 12 externalusers.
The 57 recorded and transcribed ialogneswere scenario-based and covered the full functional-ity of the system.
We systematically analysed thetranscribed ialogues to detect those deviations be-tween expected and actual user utterances that wouldsignal problems of user-system interaction caused bynon-cooperative dialogue design.
An example is pre-sented below of a piece of dialogue fi'om the user testin which two system cooperativity problems occur (Smeans ystem and U means user):S 1 : Do you want retnrn tickets'?UI: Yes please.$2: Are you particularly interested in discotmt?U2: Yes.332$3: On which date will the journey start?U3: Monday.$4: Monday January 16.
At which tin\]e?U4: 7:20.$5: There is no departure at 7:20.
The closeslother red departure is at 10:50.
Do you want thisdeparlure?US: No thanks.Each identified problem in the dialogue corpus wascategorised according to which principle had beenviolated and described in terms of the sympton\] (S), adiagnosis (D) and a cure (C).
Below is shown thedescription for the two problems in $5.GPI :  Make your contrihution as inl'ormalive as isrequired (for the current purl\]oses o1 the exchan-ge).S: U: interested in discount (red) + out departuretime at 7:20.
S: no departure at 7:20.D: The system provides insufficient informalion.It does not tell that lhere is a blue departure at7:20.C: The system should provide sufficient infornta-lion, e.g.
by telling that there is no red departurebul lhal lhere is a bh,e del)arture at the chosenhour.SPIl l: Initiate clarification recta-communicationin case of inconsistent user input.S: U: interested in discount (red) + out departuretime at 7:20; S: no departure at 7:20.
However,7:20 does exist but wilhout discount.1): S gives priority to discount over time withoutproper reason.C: S should ask U about priority: 7:20 is not adiscount departure.
Red discount can be oblainedon the departures at x, y and z.
Which departuredo you want.
\ [ f lU provides a new departure time:S: do you still want discount.
If U: no; S: non-discount departures\[.It turned out that ahnost all of Ihe 86 system dialogueproblems identified could be ascribed to violations ofthe cooperative principles (Bernsen et al, 1996b).
Weonly had to add two specific f~rinciples o1' meta-contmunication (SPI0 and SPI I  in Tahle 1).
Sincenaeta-comnmnication had not been simulated uringthe WOZ experiments, this came as no SLUT)rise.
Thelollowing GPs and SPs were found violated at leastonce: GPs I, 3, 5, 6, 7, 10, I I, 12, 13 and SPs 2, 4, 5,6,8,  10, I I .The user lest confirmed the broad coverage of theprinciples with respect o cooperative, spoken user-system dialogue.
Less flattering, of course, the testthereby revealed several deficiencies in our coopera--live dialogue design.6 ConclusionComparison between our principles and (;rice's max-ims has shown that there are more generic principlesof cooperativity it\] human-machine dialogue thanthose identified by Grice.
Three groups of principlesreveal aspecls of cooperative dialogue left unaddres-sed hy the maxims.
This produces a lotal o1' sevcndialogue aspects, each of which is addressed by oneor more generic principles (Table 1).
Some genericprinciples subsume specific principles.
It may beasked why Gfice was not aware of the Ihree genericaspects of dialogue partner asymmetry, backgroundknowledge and recta-communication.
It seems obvi-ous that it cannot he because Ihese aspects arc absentfrom human-huntan spoken dialogue.
More plausibly,dialogue partner asymmelry is ahsent from prototypi-cal cases of human-hunmn dialogue; backgroundknowledge is so perwmive as lo he easily ignored; andGrice explicitly was not concerned with dialogue fail-ure pure and simple.The results from the comparison will\] Grice's tllaXilllSand from the user test suggest lhat the principles ofcooperative spoken human-machine dialogue lllayrepresent a step towards a IllOle or less complete \ ] l idpractically applicable set of design gtfidelines forcooperative SIJ)S dialogue.ReferencesI~ernsen, N.O., l)ybkj:er, 11. and l)ybkj;er, I,.
(1996a).
"Coopcralivily in \[hmmn-Machinc and lluman-lltmmnSpoken Dialogue," Discour~'e Pr<)cesses, 21,2, 213-236.Bcrnsen, N.O., I)ybkfier, H. and Dybkj~er, I~.
(1996b).
"l~rillciples for the Design of Cooperative Spoken l lure\]n-Machine Dialogue."
To appear in Proceedings of 1CSLP'96, l~hiladcll~hia, October.t~crnscn, N.O., l)ybkj~er, L. and l)yhkj~er, II.
(1994).
'%dedicated task-oriented ialogue theory in support of spo-ken language dialogue systems design."
Proceedings ojICSLP '94, Yokohama, Seplcmbcr, 875-878.Dybkjmr, L., Bcrnsen, N.O.
and Dybkj~er, 11.
(1996).
"l{wllualion of Spoken l)ialogucs.
User Test wilh a Simu-lated Speech Rccogniscr."
Report 9bj}'om the Danish l'r@eel in &)okel~ l,a/~guage Dialogue Syxlems.
Roskildc Uni-versity, l;ebmary.f:rasef, N.M. and Gilbert, G.N.
( 1991 ).
"Simulating speechsystems."
~'ompuler Speech aml Language 5, 81-99.Grandy, P,.E.
(1989).
"On Grice on language."
The JournalojlVlilosol)t O, 86, 10, 514-25.
(;rice, P. (1975).
"lx~gic and conversation."
In P. Cole &J.L.
Morgan (Eds.
), Synta.r aml semal~lics Vol.
3."
Si)eechacts (41-58).
New York: Academic Press.
Reprinted inGricc, P.: Studies in the way of words.
Cambridge, MA:I lmward University Press 1989.Sarangi, A.K .
and Slcnlbrot.lck, S. (1992).
"Non-cooperation i communication: A reassessmenl of Gficcanpragmatics."
,Iourmd q/Pragmatics 17, I 17-154.Sclmgloff, E.A., Jefferson, G. and Sacks, 11.
(1977).
"Thepreference for self-correction i the organization of repairin conversation."
lzmguage 53,361-82.Searlc, J.R. (1992).
"Conversation."
In Searle, ,I.R.
et al(l:,ds.
), (On) Searle (m com,ers'ation.
Amsterdam: Johnllenjamin's I~ul~lishing Company.Sperber, D. and Wilson, 1).
(1987).
"Prdcis of felevance,communication a d cognition with open peer commentary.
"Behavioral aml Brain Sciences 10, 4, 697-754.333
