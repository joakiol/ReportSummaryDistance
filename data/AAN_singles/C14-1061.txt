Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 644?655, Dublin, Ireland, August 23-29 2014.Unsupervised Coreference Resolutionby Utilizing the Most Informative RelationsNafise Sadat Moosavi and Michael StrubeHeidelberg Institute for Theoretical Studies gGmbHSchloss-Wolfsbrunnenweg 3569118 Heidelberg, Germany{nafise.moosavi|michael.strube}@h-its.orgAbstractIn this paper we present a novel method for unsupervised coreference resolution.
We introduce aprecision-oriented inference method that scores a candidate entity of a mention based on the mostinformative mention pair relation between the given mention entity pair.
We introduce an infor-mativeness score for determining the most precise relation of a mention entity pair regarding thecoreference decisions.
The informativeness score is learned robustly during few iterations of theexpectation maximization algorithm.
The proposed unsupervised system outperforms existingunsupervised methods on all benchmark data sets.1 IntroductionDue to the advent of the internet, the world wide web, social media, the electronic distribution of infor-mation and new means of communication, the amount of text available in many different languages isrising.
Natural language processing (NLP) is in charge of automatic processing this growing data.
NLPresearch has mainly focused on English and very few other languages.
Therefore there is a rich set ofannotated corpora for linguistic analysis tasks for these languages.
However, there are no such corporafor thousands of other languages.
Since unsupervised methods do not require annotated data for learninga model, employing unsupervised methods has become a popular and important area of research in NLP.In this paper, we propose a new precision oriented method for unsupervised coreference resolution.Our method evaluates the candidate entities of mentions based on the most precise relation of eachmention and its candidate entity.
Though we develop and evaluate our method for the English language,we intend to apply it to low resource languages in the future.Common coreference resolution approaches rely on a combination of different features for each de-cision (for an overview over such approaches, see Ng (2010)).
However, a few approaches break downthis combination having precision in mind (Baldwin, 1997; Zhou and Su, 2004; Haghighi and Klein,2009; Lee et al., 2013).
The idea of starting with high precision knowledge is used in various NLP tasksincluding parsing (Borghesi and Favareto, 1982), word alignment (Brown et al., 1993), and named en-tity classification (Collins and Singer, 1999) with different names like ?islands of reliability?, ?steppingstones?, and ?cautiousness?.
Lee et al.
(2013) is a successful recent work that implements this idea as?sieve architecture?.
Lee et al.
(2013) first decide on the basis of more precise features, and then theyextend these decisions by using less precise features in later sieves.
In this system less precise knowledgeis used for extending the decisions made by high precision knowledge.Our proposed inference method goes in the same direction but in a different way.
The probability ofeach coreference decision is computed based on a single relation of a mention-entity.
This single relationis the most precise relation that exist between the mention-entity.
In contrast to Lee et al.
(2013), ourinference method will never take into account less precise relations if more precise ones are present.
Therelative precision of relations can be determined based on our linguistic intuition.
If we would rely onlinguistic intuition, our system would look much like Lee et al.
?s (2013)?s system, except that it processesThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/644all mentions in a single sieve, instead of iterating over all mentions for each input relation.
However, itis not a trivial task to determine the relative importance of relations for each new relation, new domain,or new language.
In this regard, we propose an informativeness score for automatically determining therelative precision of relations.The informativeness score is computed based on the distinguishing power of relations among corefer-ring and non-coreferring mentions.
We learn the informativeness score in an unsupervised way via fewiterations of the Expectation Maximization (EM) algorithm.
Overall, our inference method first finds themost precise relation that a mention has with its candidate entity based on the computed informativenessscores.
It then computes the probability of joining the mention to the entity based on this best relationand its distribution among all candidate entities.We empirically validate our approach on the OntoNotes and ACE data sets, showing that despite beingentirely unsupervised, our system performs well on all benchmark data sets.2 Related WorkEarly coreference resolution systems were mainly rule-based systems (Lappin and Leass, 1994; Bald-win, 1997).
The success of statistical approaches in different NLP tasks together with the availabilityof coreference annotated corpora (like MUC-6 (Chinchor and Sundheim, 2003) and MUC-7 (Chinchor,2001)) facilitated a shift from deploying rule-based methods to machine learning approaches in corefer-ence research in the 1990s.The increasing importance of multilingual processing, brought the deployment of semi-supervisedand unsupervised methods into attention for automatic processing of limited resource languages.
Thereare several works which treat coreference resolution as an unsupervised problem (Cardie and Wagstaff,1999; Angheluta et al., 2004; Haghighi and Klein, 2007; Ng, 2008; Poon and Domingos, 2008; Haghighiand Klein, 2009; Haghighi and Klein, 2010; Kobdani et al., 2011).
We compare our results with theunsupervised systems of Haghighi and Klein (2007), Poon and Domingos (2008), Haghighi and Klein(2009), and Kobdani et al.
(2011).
The Haghighi and Klein (2010) approach is an almost unsupervisedapproach, and we do not include this system in our comparisons.We use the expectation maximization algorithm for unsupervised learning.
EM has been previouslyused for coreference resolution (Cherry and Bergsma, 2005; Ng, 2008; Charniak and Elsner, 2009).Cherry and Bergsma (2005) and Charniak and Elsner (2009) use EM for pronoun resolution, and Ng(2008) models coreference resolution as EM clustering.
The model parameters of Ng (2008) are of theform P (f1, .
.
.
, fk|Cij), where fiis a feature, and Cijcorresponds to the coreference decision of twomentionsmiandmj.
These parameters along with the entity set, are two sets of unknown variables in Ng(2008).
He computes the posterior probabilities of entities in the E-step, and determines the parametersfrom the N-best clustering (i.e.
estimated entities) in the M-step.
Ng (2008) starts from an initial guessabout the entities and determines the parameters based on this initial guess (M-step).
In order to computethe N-best clustering, Ng (2008) uses the Bell tree approach of Luo et al.
(2004).The informativeness scores of mention pair relations (Section 3.2.1) are our unknown parameters.Our inference method only requires the ranking of the informativeness scores (and not their exact val-ues).
Therefore, it is much easier to estimate the ranking of these parameters than parameters likeP (f1, .
.
.
, fk|Cij), and our search space for finding an optimized ranking of the informativeness scoresis very small.
Since it is easier to have an initial guess about the ranking of informativeness scores (ratherthan guessing an initial entity set), we start from an E-step with a random ranking.In our experiments, EM converges very fast regardless of the initial state.
Indeed, in the M-step, weuse our new inference method for computing an estimation of entities.
The use of the EM algorithm inour approach is discussed in more detail in Section 3.3.3 Method DescriptionOur coreference resolution method is a mention-entity approach which works at mention-mention granu-larity for processing candidate entities.
It estimates entities incrementally while processing the mentions.645For resolving each mention, our inference method scores all candidate entities.
For scoring each candi-date entity, it first finds the most informative mention-mention relation that exists between the mentionand the candidate entity.
It then computes the probability of joining the mention to the entity (i.e.
thescore of the candidate entity) based on the distribution of this relation among all candidate entities of themention.In order to find the best mention-mention relation of a mention and an entity, we introduce an informa-tiveness score that scores mention pair relations based on their association with coreference links.
Thismeasure is a global measure, and it is computed based on the association analysis of the mention pairrelations and coreference links on a whole entity set of all input documents.We learn the informativeness score in an unsupervised way by using the EM algorithm.
Inference isperformed at each E-step of the EM iterations.
At each E-step, the whole set of entities is constructedfrom scratch.
The informativeness score of the input relations is computed in the M-step based on theestimated entities of the E-step.3.1 NotationsAssume that M is a mention set of the input document, and each document consists of a set of entities Ein which each entity contains one or more mentions of M .
R = {r1, .
.
.
, rK} is a set of input relationswith the following property:?r ?
R : r(m,n) ?
{0, 1} (1)where m and n are two mentions and r can be any arbitrary relation between two mentions like having aspecific feature-value (in which the feature can be a combinational feature), or a linguistic rule.In order to capture the natural left-to-right ordering of mentions, r(m,n) is zero when n is positionedafter m in the input document.3.2 Inference MethodThe inference method processes mentions in the text from the beginning of a document to its end.
Ini-tially, each mention is in its own entity.
For each mention m ?
M , all partial entities that have beenestimated so far (i.e.
entities constructed while processing mentions which are positioned before m) areconsidered as candidate entities of m (i.e.
Em).For each candidate entity u, the inference method first determines the best relation among all existingmention pair relations betweenm and u that can indicate a coreference link based on the informativenessscore.
We call this relation ru:ru= argmaxr?R(IS(r)?maxn?ur(m,n)) (2)where IS(r) is the informativeness score of the r relation.Apparently, when IS(r)?maxn?ur(m,n) is equal to zero, u will be removed from Em.After finding the most informative relation that exists between m and u (i.e.
ru), we compute theprobability of joining m to u based on ruas follows:Pr[m?
u] =?n?uru(m,n)?v?Em?x?vru(m,x)(3)Equation 3 computes the local distribution of ruamong all entities belonging to Em.
After computingthe probability of Equation 3 for all candidate entities, m will be joined to the u?
that has the highestprobability:u?
= argmaxu?EmPr[m?
u] (4)In case of a tie condition (?u,v?EmPr[m ?
u] = Pr[m ?
v]), u?
will be the entity whose mostinformative relation is more precise than the most informative relation of the other candidates:u?
= argmaxu?E[maxr?R(IS(r)?maxn?ur(n,m))] (5)646After finding the best candidate entity of m, the method proceeds to find the best entity of the nextmention, based on the new updated E.A mentionmwill be left in its own entity in two cases: 1) whenEmis empty, and 2) when the value ofPr[m?
u?]
is below a predefined threshold.
We consider this threshold equal to 0.5 in our experiments.This threshold indicates situations in which less than half of the occurrences of ru?exist between m andu?, and the others are spread among other entities.
This entity can be extended while processing latermentions or it may remain as a singleton.Please note that the inference method does not care about the exact values of {IS(r)}, and it onlyneeds to have a ranking of the informativeness scores for the given relations in order to select the mostinformative one.3.2.1 Informativeness ScoreWe want to score a set of given relations based on their discriminative power in making coreferencedecisions.
From a statistical point of view, this can be expressed as to determine whether the existence ofa relation indicates a coreference link or is due to chance.
In this regard, we can examine the followingtwo hypotheses:Hypothesis 0: P (C = 1|r = 1) = p = P (C = 1|r = 0) (6)Hypothesis 1: P (C = 1|r = 1) = p16= p2= P (C = 1|r = 0) (7)where C ?
{0, 1} is a random variable for coreference decisions.Hypothesis 0 (null hypothesis) formalizes independence (the coreference decisions are independentof relation r).
Hypothesis 1 formalizes dependence, which in case p1p2indicates a strong positiveassociation between r and C. This is the pattern that we are interested in.We use theG2log-likelihood ratio statistics for testing these hypotheses.
The statistics was introducedto the NLP community by Dunning (1993), and is defined as follows:?2 log ?
= 2 ?
logL(H1)L(H0)(8)where L(H) is the likelihood of a hypothesis based on observed data assuming a binomial probabilitydistribution for the existence of r between coreferring mentions.
Asymptotically, ?2 log ?
is ?2dis-tributed with one degree of freedom.Assuming that we have the whole set of entities of input documents, we can use the maximum likeli-hood estimator to compute p1, p2, and p as follows:p1=?u?E?m?u?n?un 6=mr(m,n)?x?M?y?My 6=xr(x, y)p2=?u?E?m?u?n?un 6=m(1?
r(m,n))?x?M?y?My 6=x(1?
r(x, y))p =?u?E?m?u?n?un 6=m1?x?M?y?My 6=x1(9)The log-likelihood ratio statistics can be used both for filtering out non-informative relations and forscoring the remaining relations.
The filtering is done by comparing the value of ?2 log ?
to the desiredthreshold value obtained from the ?2table (15.0 in our experiments) and removing the relations that arenot significant at the desired level.Similar to Dunning (1993), the test statistics can be used as a measure for scoring.
In our formulation,the test statistics scores given mention pair relations based on their association with coreference linksin a way that more precise relations (relations that indicate a coreference link more strongly) will get a647higher score, and less precise relations (relations that are randomly spread among coreferring and non-coreferring mentions) will get a lower score.The formulation of the log-likelihood ratio in Dunning (1993) is a two-tailed statistical test that if p1and p2significantly diverge from each other, the?2 log ?would get a high value.
However, as mentionedabove, we are just interested in the cases that p1is much higher than p2, because, otherwise, coreferencelinks among the mentions which have the relation r in common are less frequent than expected.Therefore, we use the one-sidedness condition as discussed by Kiss and Strunk (2006) for the log-likelihood test.
In this case, a relation r is selected as an informative relation for coreference resolutionwhen the ?2 log ?
is larger than the desired threshold, and also p1> p2:IS(r) ={?2 log ?
if p1> p20 otherwise(10)We compute the values of {IS(r)} based on entities of the whole set of input documents in order tohave a global estimation of the associations in the input data.
In order to have a domain- or genre-specific model, one should learn different {IS(r)} for each different domain/genre.
The domain/genreadaptation is discussed in more detail in the discussion part.3.3 Learning MethodFrom what we have discussed so far, {IS(r)} values and document entities (E) are two unknown sets ofvariables that we want to find.
When {IS(r)} is known, we can estimate entities by using the inferencemethod described in Section 3.2.
When the entities are known, we can compute the {IS(r)} as describedin Section 3.2.1.
We can see that these two steps (i.e.
determining entities and the informativeness scores),correspond to the E- and M-steps of the expectation maximization algorithm, respectively.Expectation maximization is an iterative procedure for computing the maximum likelihood estimatorof a parameter set when only a subset of data is available.
The EM model involves some hidden variables(Z), observed data (X) and a set of unknown parameters (?).
In our modeling, the informativeness scoresare the unknown parameters, the observed data is a set of relations corresponding to R, and entities arehidden variables.In the M-step, the model estimates {IS(r)} by using the association analysis of mention pair relationsand coreference links over the entire entity set of the input documents.
In the E-step, the algorithm per-forms the inference method of Section 3.2 and reconstructs the whole set of entities based on the given{IS(r)} values.
As mentioned before, the inference method only needs the ranking of the informative-ness scores, and therefore different values of {IS(r)} with similar ordering will lead to the same result.Our model starts from an initial E-step, in which the values of {IS(r)} are ranked randomly.
The itera-tion between the E- and M-steps continues until {IS(r)} converges to steady values.
The convergenceand the initial state of the EM algorithm are discussed in more detail in the discussion part.4 Experiments4.1 Mention Pair RelationsHere is the list of pairwise relations that we use for common and proper nouns:?
String match: Two mentions have the same string after removing their post-modifiers.?
Compatible head match: Two mentions have the same head, and the pre-modifiers of the anaphorare a subset of the pre-modifiers of the antecedent.?
Proper head match: Two proper names have the same head, and they do not contain numeric orlocation pre-modifiers.?
Substring: All words of the anaphor appear in the antecedent (possibly in different order).?
Acronym: One mention is an acronym of the other.648For the ACE data, we use additionally the following relations:?
Apposition: Two mentions are in an apposition structure.?
Demonym: One mention is a name for a resident of a place that derives from the name of the place,and the other mention is the place name itself.?
Predicate nominative: The anaphor follows a linking verb and renames or describes the subjectmention.?
Role apposition: The antecedent (with a noun head) is a modifier of a noun phrase whose head isthe anaphor.For the OntoNotes data sets, Same speaker (Lee et al., 2013) is the only feature for resolving pronouns.For the ACE data Relative pronoun (i.e.
the anaphor is a relative pronoun that modifies the head of theantecedent) is also used.
Pronouns, for which we do not have any feature, are linked to the nearestantecedent (based on the Hobbs distance) that currently belongs to a partial entity which is compatiblewith the pronoun.
The compatibility is measured in terms of number, gender, person, animacy, andnamed entity label.
This approach corresponds to the pronoun resolution strategy of the Stanford system.The differences between the relations of the OntoNotes and ACE corpora is due to the fact that thesetwo corpora have different annotation schemes.
Some of the relations mentioned (e.g.
Apposition) areconsidered as coreference relations only in the ACE data.4.2 DataWe evaluate our method on the following data sets:?
OntoNotes-Dev: Development set of the OntoNotes data provided by the CoNLL2012 shared task(Pradhan et al., 2012).
This data set consists of 303 documents.?
OntoNotes-Test: Test set of the OntoNotes data provided by the CoNLL2012 shared task (Pradhanet al., 2012).
This data set consists of 322 documents.?
ACE2004-nwire: Newswire subset of the ACE 2004 data set consisting of 128 documents.
Thissplit of ACE2004 has been utilized in previous work (Poon and Domingos, 2008; Finkel and Man-ning, 2008; Haghighi and Klein, 2009; Lee et al., 2013).?
ACE2004-Culotta-Test: One of the test splits of the ACE 2004 data set that has been used inprevious work (Culotta et al., 2007; Bengtson and Roth, 2008; Haghighi and Klein, 2009; Lee etal., 2013).
This data set consists of 107 documents.?
ACE2003-BNEWS: BNEWS subset of the ACE 2003 data set utilized in Ng (2008) and Kobdaniet al.
(2011) consisting of 51 documents.?
ACE2003-NWIRE: NWIRE subset of the ACE 2003 data set utilized in Ng (2008) and Kobdani etal.
(2011) consisting of 29 documents.4.3 PreprocessingThe mention detection of the Stanford coreference system (Lee et al., 2013) is used for the OntoNotesdata sets.
We use the predicted information in the OntoNotes data sets for named entity labels, andsyntactic roles.
For experiments on the ACE data sets, gold mentions are used, so that comparison withprevious work is possible.
For preprocessing, the Stanford parser (Klein and Manning, 2003) and namedentity recognizer (Finkel et al., 2005) are deployed.We also use the singleton detection of the Stanford system (Recasens et al., 2013) for the OntoNotesdata sets.
When both mentions are detected as a singleton by the singleton detection module, the valueof all their corresponding relations will be set to zero.
In other words, r(m,n) is set to zero when bothn and m have been detected as a singleton.
For examining the effect of the singleton detection module649MUC B3CEAFeAvg.System R P F1 R P F1 R P F1 F1OntoNotes-TestSupervisedBerkeley 67.48 72.97 70.12 54.4 61.94 57.92 53.84 55.48 54.65 60.90IMS 65.23 70.10 76.58 49.41 60.69 54.47 51.34 49.14 50.21 57.42Rule-based Stanford 63.95 65.43 64.68 48.65 56.66 52.35 51.04 46.77 48.81 55.28Unsupervised This Work 65 64.27 64.64 49.96 55.35 52.52 51.82 46.66 49.11 55.42OntoNotes-DevUnsupervisedThis Work 65.05 65.69 65.37 51.78 58.31 54.85 54.26 48.72 51.34 57.19?
Singleton 65.44 63.83 64.62 52.26 56.29 54.2 54.63 46.45 50.21 56.34& Genre 65.09 65.7 65.39 51.84 58.31 54.89 54.26 48.75 51.36 57.21Table 1: Experimental results on OntoNotes data sets.in our inference method, we evaluate our system without this module.
The result is shown in Table 1(specified as ??
Singleton?).
The results of the Stanford system are also reported using the singletondetection module of Recasens et al.
(2013).MUC B3System R P F1 R P F1ACE2003-NWIREThis Work 72.92 86.13 78.98 74.68 90.05 81.65Haghighi07 44.7 55.5 49.5 - - -Ng08 47.0 68.3 55.7 - - -Kobdani11 (UNSEL) 68.6 64.8 66.6 73.6 61.5 67.0ACE2003-BNEWSThis Work 67.36 84.72 75.05 70.35 89.56 78.80Haghighi07 56.8 68.3 62.0 - - -Ng08 56.1 71.4 62.8 - - -Kobdani11 (UNSEL) 65.0 69.5 67.1 65.9 70.2 68.0ACE2004-nwireThis Work 74.77 84.53 79.35 74.21 87.50 80.31Haghighi07 62.3 66.7 64.2 - - -Poon08 71.3 70.5 70.9 - - -Haghighi09 75.09 77.0 76.5 74.5 79.4 76.9ACE2004-Culotta-TestThis Work 68.88 82.42 75.04 73.62 88.87 80.53Haghighi09 77.7 74.8 79.6 78.5 79.6 79.0Table 2: Comparison with other unsupervised systems on ACE data sets.4.4 ResultsWe evaluate our proposed model with the most commonly used metrics for coreference resolution: forthe OntoNotes data sets MUC (Vilain et al., 1995), B3(Bagga and Baldwin, 1998), CEAF (Luo, 2005)and their average F1 as used in the CoNLL 2011 and 2012 shared tasks; for the ACE data sets MUCand B3.
The experimental results for the OntoNotes and ACE data sets are presented in Tables 1 and 2,respectively.On the OntoNotes test set, we compare our method with the three best publicly available coreferencesystems including the Berkeley system (Durrett and Klein, 2013), the IMS system (Bj?orkelund andFarkas, 2012), and the Stanford system (Lee et al., 2013; Recasens et al., 2013).
The Berkeley and IMSsystems are both supervised approaches with a rich set of lexical features.
At the other hand, the Stanfordsystem is a deterministic system with a set of entity-level features that needs to go through all mentionsfor incorporating each of the input features.
The Stanford system is the winner of the CoNLL2011 shared650OntoNotes-DevSame speaker > Compatible head match > Substring > String match > Proper head match > AcronymACE2004-nwireCompatible head match > Substring > Proper head match > String match > Demonym >Apposition > Same speaker > Role apposition > Relative pronoun > Acronym > Predicate nominativeTable 3: The resulting ranking of informativeness scores on different data sets.task.
The IMS system is the 3rd best system on the CoNLL2012 shared task.
The Berkeley system isa state-of-the-art supervised coreference system that outperforms both the Stanford and IMS systems.Despite being totally unsupervised and using pairwise features, the results of our system are on par withthose of the Stanford system (according to the approximate randomization test, there is no significantdifference).
The comparison with this state-of-the-art rule based system (Lee et al., 2013), indicates theeffectiveness of our coreference resolution approach, as it uses the same preprocessing modules and asimpler and smaller set of features.
All results in the Table 1 are reported using the scorer-v71of theCoNLL-2012 shared task (Pradhan et al., 2014).On the ACE data sets, we compare our performance to those of the unsupervised systems mentionedin Section 2.
As Table 2 shows, our method considerably outperforms other unsupervised systems on alldata sets (except only for the MUC measure on the ACE2004-Culotta-Test data set).5 Discussion5.1 Informativeness ScoreAs discussed in Section 3.2.1, we determine the discriminative power of mention pair relations in corefer-ence decisions based on the informativeness score (Equation 10), in which the statistical test is computedon the unsupervised estimated set of entities.
The resulting ranking of the informativeness score for ourinput relations is presented in Table 3 on both OntoNotes and ACE data sets.Another point that needs to be mentioned here is that we are currently using a set of simple andprecise input relations.
While using these input relations, the informativeness score cannot be efficientlyused.
The effectiveness of our informativeness score can be usefully assessed with complex relations(i.e.
combinatorial features).
However, learning of the informativeness scores for complex relations isnot possible in a totally unsupervised configuration and one should at least use an informative initial stateto guide the learning.
We address this issue in our future work.5.2 Domain/Genre AdaptationThe OntoNotes data set has seven genres regarding the type of text?s sources: newswire (NW), broadcastnews (BN), broadcast conversation (BC), magazine (MZ), telephone conversation (TC), web data (WB),pivot text (PT).
Domain or genre adaptation is one of the current obstacles in language processing.
Inorder to test the effect of genre adaptation in our approach, we try a variant of our approach in which theinformativeness scores of the input relations (i.e.
{IS(r)}) are learned separately for each genre.
Theresults of this evaluation are presented in Table 1 by the name ?& Genre?.As can be seen in Table 1, the genre-specific variant of our system is performing as well as the baseversion.
This experiment indicates the robustness of our approach regarding the genre/domain adapta-tion.
It can learn an appropriate approximation of the informativeness scores from a small amount ofdata (i.e.
the data provided for a single genre instead of the data from all genres).
The learned orderingsof the informativeness scores for all genres are presented in Table 4.When evaluated on each genre separately, the system has the best performance on PT, and the worstperformance on the WB genre.
The total ordering of genres based on the performance of our system is1http://conll.cemantix.org/2012/software.html651Broadcast conversation, Web dataSame speaker > Compatible head match > Substring > String match > Proper head match > AcronymTelephone conversationSame speaker > Compatible head match > Substring > String match > Proper head matchBroadcast news, NewswireSubstring > Compatible head match > String match > Proper head match > Same speaker > AcronymPivot textSame speaker > Compatible head match > String match > Substring > Proper head matchMagazineCompatible head match > Substring > String match > Proper head match > Same speaker > AcronymTable 4: The genre-specific ranking of informativeness scores.as follow: PT, MZ, TC, BN, NW, BC, WB.5.3 EM Initial State and ConvergenceFor the initial state of our EM algorithm, we need a ranking of the informativeness scores of the inputrelations.
We try different initial states for the EM algorithm, from an informative ranking based onlinguistic intuition about the precision of input relations to a misleading ranking (the informative orderreversed).
However, in all cases, the EM algorithm leads to the same ranking (as listed in Table 3).
Thisindicates the robustness of our modeling.It is more likely that a more precise relation will also get a higher value for its corresponding joinprobability of Equation 3, because it is unlikely that a precise relation connects a mention to severalcandidate entities.
However, relations with low precision may connect a mention to several differententities, because they are spread over more different entities than relations with higher precision.In our experiments, for all tested initial states, the model converges in 4 iterations on the OntoNotesdata sets and 5 iterations on the ACE data sets.5.4 Promising Alternative for the Stanford SystemOur coreference resolution method is a self- contained approach, that does not need any external linguis-tic knowledge regarding the coreference relations.
However, we can also consider a simple variant ofthis system in which a predefined ordering of features (based on linguistic intuition) is given, like theStanford system.
In this case, the EM algorithm will be no longer needed, and therefore, the algorithmresolves all mentions in a single iteration.Therefore, this variant of our system can be considered as an efficient alternative to the Stanfordsystem, that uses a simpler (pairwise instead of entity-based) and smaller (5 instead of 7 string matches)set of relations, and more importantly processes all mentions in a single iteration (instead of iteratingover all mentions for each relation), and it still performs as well as its entity-based multi-sieve variant.6 ConclusionsIn this paper, we presented a new unsupervised coreference resolution method.
We deploy a newprecision-oriented inference method that decides about joining a mention to a candidate entity basedon only the most informative mention pair relation that exists between the given mention entity pair.In order to determine the most informative relation of a mention and its candidate entity, we introducean informativeness score for scoring mention-mention relations based on their global association with652coreference links.
A relation whose existence strongly indicates a coreference link will get a high score,and a relation which is randomly spread among coreferring and non-coreferring mentions will get a lowscore.
The informativeness score is robustly learned during a very few iterations of the EM algorithm.Our proposed method performs well on all benchmark data sets.
In the future we intend to apply thisrobust and efficient approach to new genres, domains, and also new languages.AcknowledgmentsThe authors would like to thank Sebastian Martschat for his helpful comments.
This work has beenfunded by the Klaus Tschira Foundation, Heidelberg, Germany.
The first author has been supported by aHeidelberg Institute for Theoretical Studies PhD.
scholarship.ReferencesRoxana Angheluta, Patrick Jeuniaux, Rudradeb Mitra, and Marie-Francine Moens.
2004.
Clustering algorithmsfor noun phrase coreference resolution.
In Proceedings of the 7`emes Journ?ees Internationales d?Analyse Statis-tique des Donn?ees Textuelles, Louvain La Neuve, Belgium, 10?12 March 2004, pages 60?70.Amit Bagga and Breck Baldwin.
1998.
Algorithms for scoring coreference chains.
In Proceedings of the 1stInternational Conference on Language Resources and Evaluation, Granada, Spain, 28?30 May 1998, pages563?566.Breck Baldwin.
1997.
CogNIAC: High precision coreference with limited knowledge and linguistic resources.In Proceedings of the ACL Workshop on Operational Factors in Practical, Robust Anaphora Resolution forUnrestricted Text, Madrid, Spain, July 1997, pages 38?45.Eric Bengtson and Dan Roth.
2008.
Understanding the value of features for coreference resolution.
In Proceedingsof the 2008 Conference on Empirical Methods in Natural Language Processing, Waikiki, Honolulu, Hawaii, 25?27 October 2008, pages 294?303.Anders Bj?orkelund and Rich?ard Farkas.
2012.
Data-driven multilingual coreference resolution using resolverstacking.
In Proceedings of the Shared Task of the 16th Conference on Computational Natural LanguageLearning, Jeju Island, Korea, 12?14 July 2012, pages 49?55.Luigi Borghesi and Chiara Favareto.
1982.
Flexible parsing of discretely uttered sentences.
In Proceedings ofthe 9th International Conference on Computational Linguistics, Prague, Czechoslovakia, 5?10 July 1982, pages37?42.Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer.
1993.
The mathematics ofstatistical machine translation: Parameter estimation.
Computational Linguistics, 19(2):263?311.Claire Cardie and Kiri Wagstaff.
1999.
Noun phrase coreference as clustering.
In Proceedings of the 1999SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, CollegePark, Md., 21?22 June 1999, pages 82?89.Eugene Charniak and Micha Elsner.
2009.
EM works for pronoun anaphora resolution.
In Proceedings of the12th Conference of the European Chapter of the Association for Computational Linguistics, Athens, Greece, 30March ?
3 April 2009, pages 148?156.Colin Cherry and Shane Bergsma.
2005.
An expectation maximization approach to pronoun resolution.
Proceed-ings of the Ninth Conference on Computational Natural Language Learning, pages 88?95.Nancy Chinchor and Beth Sundheim.
2003.
Message Understanding Conference (MUC) 6.
LDC2003T13,Philadelphia, Penn: Linguistic Data Consortium.Nancy Chinchor.
2001.
Message Understanding Conference (MUC) 7.
LDC2001T02, Philadelphia, Penn: Lin-guistic Data Consortium.Michael Collins and Yoram Singer.
1999.
Unsupervised models for named entity classification.
In Proceedings ofthe 1999 SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,College Park, Md., 21?22 June 1999, pages 100?110.Aron Culotta, Michael Wick, and Andrew McCallum.
2007.
First-order probabilistic models for coreferenceresolution.
In Proceedings of Human Language Technologies 2007: The Conference of the North AmericanChapter of the Association for Computational Linguistics, Rochester, N.Y., 22?27 April 2007, pages 81?88.653Ted Dunning.
1993.
Accurate methods for the statistics of surprise and coincidence.
Computational Linguistics,19(1):61?74.Greg Durrett and Dan Klein.
2013.
Easy victories and uphill battles in coreference resolution.
In Proceedingsof the 2013 Conference on Empirical Methods in Natural Language Processing, Seattle, Wash., 18?21 October2013, pages 1971?1982.Jenny Rose Finkel and Christopher Manning.
2008.
Enforcing transitivity in coreference resolution.
In Compan-ion Volume to the Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics,Columbus, Ohio, 15?20 June 2008, pages 45?48.Jenny Rose Finkel, Trond Grenager, and Christopher Manning.
2005.
Incorporating non-local information into in-formation extraction systems by Gibbs sampling.
In Proceedings of the 43rd Annual Meeting of the Associationfor Computational Linguistics, Ann Arbor, Mich., 25?30 June 2005, pages 363?370.Aria Haghighi and Dan Klein.
2007.
Unsupervised coreference resolution in a nonparametric Bayesian model.In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, Prague, CzechRepublic, 23?30 June 2007, pages 848?855.Aria Haghighi and Dan Klein.
2009.
Simple coreference resolution with rich syntactic and semantic features.
InProceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, Singapore, 6?7August 2009, pages 1152?1161.Aria Haghighi and Dan Klein.
2010.
Coreference resolution in a modular, entity centered model.
In Proceedingsof Human Language Technologies 2010: The Conference of the North American Chapter of the Association forComputational Linguistics, Los Angeles, Cal., 2?4 June 2010, pages 385?393.Tibor Kiss and Jan Strunk.
2006.
Unsuperivsed multilingual sentence boundary detection.
Computational Lin-guistics, 32(4):485?525.Dan Klein and Christopher D. Manning.
2003.
Accurate unlexicalized parsing.
In Proceedings of the 41st AnnualMeeting of the Association for Computational Linguistics, Sapporo, Japan, 7?12 July 2003, pages 423?430.Hamidreza Kobdani, Hinrich Schuetze, Michael Schiehlen, and Hans Kamp.
2011.
Bootstrapping coreferenceresolution using word associations.
In Proceedings of the 49th Annual Meeting of the Association for Compu-tational Linguistics, Portland, Oreg., 19?24 June 2011, pages 783?792.Shalom Lappin and Herbert J. Leass.
1994.
An algorithm for pronominal anaphora resolution.
ComputationalLinguistics, 20(4):535?561.Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky.
2013.Deterministic coreference resolution based on entity-centric, precision-ranked rules.
Computational Linguistics,39(4):885?916.Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda Kambhatla, and Salim Roukos.
2004.
A mention-synchronous coreference resolution algorithm based on the Bell Tree.
In Proceedings of the 42nd AnnualMeeting of the Association for Computational Linguistics, Barcelona, Spain, 21?26 July 2004, pages 136?143.Xiaoqiang Luo.
2005.
On coreference resolution performance metrics.
In Proceedings of the Human LanguageTechnology Conference and the 2005 Conference on Empirical Methods in Natural Language Processing, Van-couver, B.C., Canada, 6?8 October 2005, pages 25?32.Vincent Ng.
2008.
Unsupervised models for coreference resolution.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Processing, Waikiki, Honolulu, Hawaii, 25?27 October 2008, pages640?649.Vincent Ng.
2010.
Supervised noun phrase coreference research: The first fifteen years.
In Proceedings of the48th Annual Meeting of the Association for Computational Linguistics, Uppsala, Sweden, 11?16 July 2010,pages 1396?1411.Hoifung Poon and Pedro Domingos.
2008.
Joint unsupervised coreference resolution with Markov Logic.
InProceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, Waikiki, Honolulu,Hawaii, 25?27 October 2008, pages 650?659.Sameer Pradhan, Alessandro Moschitti, and Nianwen Xue.
2012.
CoNLL-2012 Shared Task: Modeling multi-lingual unrestricted coreference in OntoNotes.
In Proceedings of the Shared Task of the 16th Conference onComputational Natural Language Learning, Jeju Island, Korea, 12?14 July 2012, pages 1?40.654Sameer Pradhan, Xiaoqiang Luo, Marta Recasens, Eduard Hovy, Vincent Ng, and Michael Strube.
2014.
Scoringcoreference partitions of predicted mentions: A reference implementation.
In Proceedings of the ACL 2014Conference Short Papers, Baltimore, Md., 22?27 June 2014.
To appear.Marta Recasens, Marie-Catherine de Marneffe, and Christopher Potts.
2013.
The life and death of discourseentities: Identifying singleton mentions.
In Proceedings of the 2013 Conference of the North American Chapterof the Association for Computational Linguistics: Human Language Technologies, Atlanta, Georgia, 9?14 June2013, pages 627?633.Marc Vilain, John Burger, John Aberdeen, Dennis Connolly, and Lynette Hirschman.
1995.
A model-theoreticcoreference scoring scheme.
In Proceedings of the 6th Message Understanding Conference (MUC-6), pages45?52, San Mateo, Cal.
Morgan Kaufmann.Guodong Zhou and Jian Su.
2004.
A high-performance coreference resolution system using a constraint-basedmulti-agent strategy.
In the 16th International Conference on Computational Linguistics (COLING).655
