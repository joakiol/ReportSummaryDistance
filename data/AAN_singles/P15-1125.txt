Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1292?1300,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsEntity Hierarchy EmbeddingZhiting Hu, Poyao Huang, Yuntian Deng, Yingkai Gao, Eric P. XingSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USA{zhitingh,poyaoh,yuntiand,yingkaig,epxing}@cs.cmu.eduAbstractExisting distributed representations arelimited in utilizing structured knowledgeto improve semantic relatedness modeling.We propose a principled framework of em-bedding entities that integrates hierarchi-cal information from large-scale knowl-edge bases.
The novel embedding modelassociates each category node of the hi-erarchy with a distance metric.
To cap-ture structured semantics, the entity sim-ilarity of context prediction are measuredunder the aggregated metrics of relevantcategories along all inter-entity paths.
Weshow that both the entity vectors and cat-egory distance metrics encode meaningfulsemantics.
Experiments in entity linkingand entity search show superiority of theproposed method.1 IntroductionThere has been a growing interest in distributedrepresentation that learns compact vectors (a.k.aembedding) for words (Mikolov et al, 2013a),phrases (Passos et al, 2014), and concepts (Hilland Korhonen, 2014), etc.
The induced vectorsare expected to capture semantic relatedness ofthe linguistic items, and are widely used in senti-ment analysis (Tang et al, 2014), machine transla-tion (Zhang et al, 2014), and information retrieval(Clinchant and Perronnin, 2013), to name a few.Despite the impressive success, existing workis still limited in utilizing structured knowledgeto enhance the representation.
For instance, wordand phrase embeddings are largely induced fromplain text.
Though recent knowledge graph em-beddings (Lin et al, 2015; Wang et al, 2014) inte-grate the relational structure among entities, theyprimarily target at link prediction and lack an ex-plicit relatedness measure.In this paper, we propose to improve the dis-tributed representations of entities by integratinghierarchical information from large-scale knowl-edge bases (KBs).
An entity hierarchy groups en-tities into categories which are further organizedto form a taxonomy.
It provides rich structuredknowledge on entity relatedness (Resnik, 1995).Our work goes beyond the previous heuristic useof entity hierarchy which relies on hand-craftedfeatures (Kaptein and Kamps, 2013; Ponzettoand Strube, 2007), and develops a principledoptimization-based framework.
We learn a dis-tance metric for each category node, and mea-sure entity-context similarity under the aggregat-ed metrics of all relevant categories.
The met-ric aggregation encodes the hierarchical propertythat nearby entities tend to share common seman-tic features.
We further provide a highly-efficientimplementation in order to handle large complexhierarchies.We train a distributed representation for the w-hole entity hierarchy of Wikipedia.
Both the entityvectors and the category distance metrics capturemeaningful semantics.
We deploy the embeddingin both entity linking (Han and Sun, 2012) andentity search (Demartini et al, 2010) tasks.
Hi-erarchy embedding significantly outperforms thatwithout structural knowledge.
Our methods alsoshow superiority over existing competitors.To the best of our knowledge, this is the firstwork to learn distributed representations that in-corporates hierarchical knowledge in a principledframework.
Our model that encodes hierarchy bydistance metric learning and aggregation providesa potentially important and general scheme for u-tilizing hierarchical knowledge.The rest of the paper is organized as follows:?2 describes the proposed embedding model; ?3presents the application of the learned embed-ding; ?4 evaluates the approach; ?5 reviews relatedwork; and finally, ?6 concludes the paper.1292e1...e2?.e3???
?..Text Contexte1 e2e3Entity Hierarchyh2h1??1??2?
?1Entity pairs??1.
?3  = ??1?
?1 + ??2?
?2 ??2?
?3(?1,   ?3)?1,  ?2Dist.
metrics?
?1,?2 = ?
?2Figure 1: The model architecture.
The text context of an entity is based on its KB encyclopedia article.The entity hierarchical structure is incorporated through distance metric learning and aggregation.2 Entity Hierarchy EmbeddingThe objective of the embedding model is to finda representation for each entity that is useful forpredicting other entities occurring in its contex-t. We build entity?s context upon KB encyclope-dia articles, where entity annotations are readilyavailable.
We further incorporate the entity hierar-chical structure in the context prediction throughdistance metric learning and aggregation, whichencodes the rich structured knowledge in the in-duced representations.
Our method is flexible andefficient to model large complex DAG-structuredhierarchies.
Figure 1 shows an overview of themodel architecture.2.1 Model ArchitectureOur architecture builds on the skip-gram wordembedding framework (Mikolov et al, 2013b).In the skip-gram model, a set of (target, con-text) word pairs are extracted by sliding a fixed-length context window over a text corpus, and theword vectors are learned such that the similarityof the target- and context-word vectors is maxi-mized.
We generalize both the context definitionand the similarity measure for entity hierarchy em-bedding.Unlike words that can be directly extracted fromplain text, entities are hidden semantics underly-ing their surface forms.
In order to avoid manualannotation cost, we exploit the text corpora fromKBs where the referent entities of surface text arereadily annotated.
Moreover, since a KB encyclo-pedia article typically focuses on describing oneentity, we naturally extend the entity?s context asits whole article, and obtain a set of entity pairsD = {(eT, eC)}, where eTdenotes the target-entity and eCdenotes the context-entity occurringin entity eT?s context.Let E be the set of entities.
For each entity e ?E , the model learns both a ?target vector?
ve?
Rnand ?context vector?
v?e?
Rn, by maximizing thetraining objectiveL =1|D|?
(eT,eC)?Dlog p(eC|eT),(1)where the prediction probability is defined as asoftmax:p(eC|eT) =exp {?d (eT, eC)}?e?Eexp {?d (eT, e)}.
(2)Here d(e, e?)
is the distance between the targetvector of e (i.e., ve) and the context vector of e?
(i.e., v?e?).
We present the design in the following.2.2 Hierarchical ExtensionAn entity hierarchy takes entities as leaf nodes andcategories as internal nodes, which provides keyknowledge sources on semantic relatedness that 1)far-away entities in the hierarchy tend to be se-mantically distant, and 2) nearby entities tend toshare common semantic features.
We aim to en-code this knowledge in our representations.
As K-B hierarchies are large complex DAG structures,we develop a highly-efficient scheme to enablepractical training.Specifically, we associate a separate distancemetric Mh?
Rn?nwith each category h in thehierarchy.
A distance metric is a positive semidef-inite (PSD) matrix.
We then measure the distancebetween two entities under some aggregated dis-tance metric as detailed below.
The local metricsthus not only serve to capture the characteristicsof individual categories, but also make it possibleto share the representation across entities throughmetric aggregation of relevant categories.Metric aggregation Given two entities e and e?,let Pe,e?be the path between them.
One obviousway to define the aggregated metricMe,e??
Rn?nis through a combination of the metrics on the1293eh2h1h3 h4e'?
??
?path 1?
??
?path 2turning nodeFigure 2: Paths in a DAG-structured hierarchy.
Apath P is defined as a sequence of non-duplicatednodes with the property that there exists a turningnode t ?
P such that any two consecutive nodesbefore t are (child, parent) pairs, while consecu-tive nodes after t are (parent, child) pairs.
Thus aturning node is necessarily a common ancestor.path:?h?Pe,e?Mh, leading to a nice property thatthe more nodes a path has, the more distant the en-tities tend to be (as Mhis PSD).
This simple strat-egy, however, can be problematic when the hier-archy has a complex DAG structure, in that therecan be multiple paths between two entities (Fig-ure 2).
Though the shortest path can be selected, itignores other related category nodes and loses richinformation.
In contrast, an ideal scheme shouldnot only mirror the distance in the hierarchy, butalso take into account all possible paths in order tocapture the full aspects of relatedness.However, hierarchies in large KBs can be com-plex and contains combinationally many paths be-tween any two entities.
We propose an efficien-t approach that avoids enumerating paths and in-stead models the underlying nodes directly.
In par-ticular, we extend Pe,e?as the set of all categorynodes included in any of the e ?
e?paths, anddefine the aggregate metric asMe,e?= ?e,e?
?h?Pe,e?piee?,hMh,(3)where {piee?,h} are the relative weights of the cat-egories such that?h?Pe,e?piee?,h= 1.
This servesto balance the size of P across different entitypairs.
We set piee?,h?
(1sh?e+1sh?e?)
with sh?ebeing the average #steps going down from nodeh to node e in the hierarchy (infinite if h is notan ancestor of e).
This implements the intuitionthat an entity (e.g., ?Iphone?)
is more relevant toits immediate categories (e.g., ?Mobile phones?
)than to farther and more generic ancestors (e.g.,?Technology?).
The scaling factor ?e,e?encodesthe distance of the entities in the hierarchy and canbe of various choices.
We set ?e,e?= minh{sh?e+sh?e?}
to mirror the least common ancestor.In Figure 2, Pe,e?= {h2, h3, h4}, and the rel-ative weights of the categories are piee?,h2?
3/2and piee?,h3= piee?,h4?
1.
Category h2is the leastcommon ancestor and ?e,e?= 3.Based on the aggregated metric, the distance be-tween a target entity eTand a context entity eCcanthen be measured asd (eT, eC) = (veT?
v?eC)>MeT,eC(veT?
v?eC).
(4)Note that nearby entities in the hierarchy tend toshare a large proportion of local metrics in Eq 3,and hence can exhibit common semantic featureswhen measuring distance with others.Complexity of aggregation As computing dis-tance is a frequent operation in both training andapplication stages, a highly efficient aggregationalgorithm is necessary in order to handle complexlarge entity hierarchies (with millions of nodes).Our formulation (Eq 3) avoids exhaustive enumer-ation over all paths by modeling the relevant nodesdirectly.
We show that this allows linear complex-ity in the number of children of two entities?
com-mon ancestors, which is efficient in practice.The most costly operation is to find Pe,e?, i.e.,the set of all category nodes that can occur in anyof e?
e?paths.
We use a two-step procedure that(1) finds all common ancestors of entity e and e?that are turning nodes of any e ?
e?paths (e.g.,h2in Figure 2), denoted asQe,e?
; (2) expands fromQe,e?to construct the full Pe,e?.
For the first step,the following theorem shows each common ances-tor can be efficiently assessed by testing only itschildren nodes.
For the second step, it is straight-forward to see that Pe,e?can be constructed by ex-panding Qe,e?with its descendants that are ances-tors of either e or e?.
Other parameters (piee?and?e,e?)
of aggregation can be computed during theabove process.We next provide the theorem for the first step.LetAebe the ancestor nodes of entity e (includinge itself).
For a node h ?
Ae?
Ae?, we defineits critical node thas the nearest (w.r.t the lengthof the shortest path) descendant of h (including hitself) that is in Qe,e??
{e, e?}.
E.g., in Figure 2,th1= h2; th2= h2; th3= e. Let Chbe the set ofimmediate child nodes of h.Theorem 1.
?h ?
Ae?Ae?, h ?
Qe,e?iff it satis-fies the two conditions: (1) |Ch?(Ae?
Ae?)
| ?
2;(2) ?a, b ?
Chs.t.
ta6= tb.1294Proof.
We outline the proof here, and provide thedetails in the appendix.Sufficiency: Note that e, e?/?
Qe,e?.
We provethe sufficiency by enumerating possible situation-s: (i) ta= e, tb= e?
; (ii) ta= e, tb?
Qe,e?
;(iii) ta, tb?
Qe,e?.
For (i): as ta= e, there existsa path e ?
?
?
?
?
a ?
h where any two con-secutive nodes is a (child, parent) pair.
Similarly,there is a path h ?
b ?
?
?
?
?
e?where any t-wo consecutive nodes is a (parent, child) pair.
Itis provable that the two paths intersect only at h,and thus can be combined to form an e?
e?path:e ?
?
?
?
?
a ?
h ?
b ?
?
?
?
?
e?, yielding has a turning node.
The cases (ii) and (iii) can beproved similarly.Necessity: We prove by contradiction.
Supposethat ?a, b ?
Ch?
(Ae?
Ae?)
we have ta= tb.W.l.o.g.
we consider two cases: (i) ta= tb= e,and (ii) ta= tb?
Qe,e?.
It is provable that bothcases will lead to contradiction.Therefore, by checking common ancestors fromthe bottom up, we can construct Qe,e?with timecomplexity linear to the number of all ancestors?children.2.3 LearningFor efficiency, we use negative sampling to refor-mulate the training objective, which is then opti-mized through coordinate gradient ascent.Specifically, given the training data {(eT, eC)}extracted from KB corpora, the representationlearning is formulated as maximizing the objec-tive in Eq 1, subject to PSD constraints on distancemetrics Mh 0, and ?ve?2= ?v?e?2= 1 to avoidscale ambiguity.The likelihood of each data sample is definedas a softmax in Eq 2, which iterates over all en-tities in the denominator and is thus computation-ally prohibitive.
We apply the negative samplingtechnique as in conventional skip-gram model, byreplacing each log probability log p(eC|eT) withlog ?
(?d (eT, eC)) +?ki=1Eei?P (e)[log ?
(?d (eT, ei))] ,where ?
(x) = 1/(1 + exp(?x)) is the sigmoidfunction; and for each data sample we draw k neg-ative samples from the noise distribution P (e) ?U(e)3/4with U(e) being the unigram distribution(Mikolov et al, 2013b).The negative sampling objective is optimizedusing coordinate gradient ascent, as shown in Al-gorithm 1.
To avoid overfitting and improve effi-ciency, in practice we restrict the distance metricsMhto be diagonal (Xing et al, 2002).
Thus thePSD project of Mh(line 17) is simply taking thepositive part for each diagonal elements.Algorithm 1 Entity Hierarchy EmbeddingInput: The training data D = {(eT, eC)},Entity hierarchy,Parameters: n ?
dimension of the embeddingk ?
number of negative samples?
?
gradient learning rateB ?
minibatch size1: Initialize v, v?,M randomly such that ?v?2= ?v?
?2= 1and M  0.2: repeat3: Sample a batch B = {(eT, eC)i}Bi=1from D4: for all (eT, eC) ?
B do5: Compute {P,pi, ?
}eT,eCfor metric aggregation6: Sample negative pairs {(eT, ei)}ki=17: Compute {{P,pi, ?
}eT,ei}ki=1for metric aggrega-tion8: end for9: repeat10: for all e ?
E included in B do11: ve= ve+ ?
?L?ve12: v?e= v?e+ ?
?L?v?e13: ve, v?e= Project to unit sphere(ve, v?e)14: end for15: until convergence16: repeat17: for all h included in B do18: Mh= Mh+ ?
?L?Mh19: Mh= Project to PSD(Mh)20: end for21: until convergence22: until convergenceOutput: Entity vectors v, v?, and category dist.
metricsM3 ApplicationsOne primary goal of learning semantic embeddingis to improve NLP tasks.
The compact represen-tations are easy to work with because they en-able efficient computation of semantic relatedness.Compared to word embedding, entity embeddingis particularly suitable for various language under-standing applications that extract underlying se-mantics of surface text.
Incorporating entity hier-archies further enriches the embedding with struc-tured knowledge.In this section, we demonstrate how the learnedentity hierarchy embedding can be utilized in t-wo important tasks, i.e., entity linking and enti-ty search.
In both tasks, we measure the seman-tic relatedness between entities as the reciprocaldistance defined in Eq 4.
This greatly simpli-fies previous methods which have used varioushand-crafted features, and leads to improved per-formance as shown in our experiments.12953.1 Entity LinkingThe entity linking task is to link surface form-s (mentions) of entities in a document to entitiesin a reference KB.
It is an essential first step fordownstream tasks such as semantic search and K-B construction.
The quality of entity relatednessmeasure is critical for entity linking performance,because of the key observation that entities in adocument tend to be semantically coherent.
Forexample, in sentence ?Apple released an operatingsystem Lion?, The mentions ?Apple?
and ?Lion?refer to Apple Inc. and Mac OS X Lion, respec-tively, as is more coherent than other configura-tions like (fruit apple, animal lion).Our algorithm finds the optimal configurationfor the mentions of a document by maximizingthe overall relatedness among assigned entities, to-gether with the local mention-to-entity compatibil-ity.
Specifically, we first construct a mention-to-entity dictionary based on Wikipedia annotations.For each mention m, the dictionary contains a setof candidate entities and for each candidate entitye a compatibility score P (e|m) which is propor-tional to the frequency that m refers to e. For effi-ciency we only consider the top-5 candidate enti-ties according to P (e|m).
Given a set of mentionsM = {mi}Mi=1in a document, let A = {emi}Mi=1be a configuration of its entity assignments.
Thescore of A is formulated as probabilityP (A|M) ?
?Mi=1P (emi|mi)?Mj=1j 6=i1d(emi, emj)+ ,where for each entity assignment we define itsglobal relatedness to other entity assignments asthe sum of the reciprocal distances ( = 0.01 isa constant used to avoid divide-by-zero).
Directenumeration of all potential configurations is com-putationally prohibitive, we therefore use simulat-ed annealing to search for an optimal solution.3.2 Entity SearchEntity search has attracted a growing interest(Chen et al, 2014b; Balog et al, 2011).
Unlikeconventional web search that finds unorganizedweb pages, entity search retrieves knowledge di-rectly by generating a list of relevant entities in re-sponse to a search request.
The input of the entitysearch task is a natural language questionQ alongwith one or more desired entity categories C. Forexample, a query can be Q =?films directed byAkira Kurosawa?
and C ={Japanese films}.Previous methods typically score candidate en-tities by measuring both the similarity between en-tity content and the query question Q (text match-ing), and the similarity between categories of en-tities and the query categories C (category match-ing).We apply a similar category matching strate-gy as in previous work (Chen et al, 2014b) thatassesses lexical (e.g., head words) similarity be-tween category names, while replacing the textmatching with entity relatedness measure.
Specif-ically, we first extract the underlying entities men-tioned inQ through entity linking, then score eachcandidate entity by its average relatedness to theentities in Q.
For instance, the entity Rashomonwill obtain a high score in the above example asit is highly related with the entity Akira Kurosawain the query.
This scheme not only avoids complexdocument processing (e.g., topic modeling) in tex-t matching, but also implicitly augments the shortquery text with background knowledge, and thusimproves the accuracy and robustness.4 ExperimentsWe validate the quality of our entity representa-tion by evaluating its applications of entity linkingand entity search on public benchmarks.
In theentity linking task, our approach improves the F1score by 10% over state-of-the-art results.
We al-so validate the advantage of incorporating hierar-chical structure.
In the entity search task, our sim-ple algorithm shows competitive performance.
Wefurther qualitatively analyze the entity vectors andcategory metrics, both of which capture meaning-ful semantics, and can potentially open up a widerage of other applications.Knowledge base We use the Wikipedia snap-shot from Jan 12nd, 2015 as our training data andKB.
After pruning administrative information weobtain an entity hierarchy including about 4.1Mentities and 0.8M categories organized into 12layers.
Loops in the original hierarchy are re-moved by deleting bottom-up edges, yielding aDAG structure.
We extract a set of 87.6M entitypairs from the wiki links on Wikipedia articles.We train 100-dimensional vector represen-tations for the entities and distance metrics(100?100 diagonal matrixes) for the categories(we would study the impact of dimensionality inthe future).
We set the batch size B = 500, theinitial learning rate ?
= 0.1 and decrease it by a1296factor of 5 whenever the objective value does notincrease, and the negative sample size k = 5.
Themodel is trained on a Linux machine with 128GRAM and 16 cores.
It takes 5 days to converge.4.1 Entity Linking4.1.1 SetupDataset As our entities based on EnglishWikipedia include not only named entities (e.g.,persons, organizations) but also general concepts(e.g., ?computer?
and ?human?
), we use a stan-dard entity linking dataset IITB1where mentionsof Wikipedia entities are manually annotated ex-haustively.
The dataset contains about 100 docu-ments and 17K mentions in total.
As in the base-line work, we use only the mentions whose refer-ent entities are contained in Wikipedia.Criteria We adopt the common criteria, preci-sion, recall, and F1.
Let A?be the golden stan-dard entity annotations, and A be the annotationsby entity linking model, thenprecision =|A??
A||A|recall =|A??
A||A?|.The F1 score is then computed based on the aver-age precision and recall across all documents.Baselines We compare our algorithm with thefollowing approaches.
All the competitors are de-signed to be able to link general concept mentionsto Wikipedia.CSAW (Kulkarni et al, 2009) has a similarframework as our algorithm.
It measures entityrelatedness using a variation of Jaccard similarityon Wikipedia page incoming links.Entity-TM (Han and Sun, 2012) models an en-tity as a distribution over mentions and words, andsets up a probabilistic generative process for theobserved text.Ours-NoH.
To validate the advantage of incor-porating hierarchical structure, we design a base-line that relies on entity embedding without enti-ty hierarchy.
That is, we obtain entity vectors byfixing the distance metric in Eq 4 as an identitymatrix.4.1.2 ResultsTable 1 shows the performance of the competitors.Our algorithm using the entity hierarchy embed-ding gets 21% to 10% improvement in F1, and1http://www.cse.iitb.ac.in/soumen/doc/CSAW/AnnotMethods Precision Recall F1CSAW 0.65 0.74 0.69Entity-TM 0.81 0.80 0.80Ours-NoH 0.78 0.85 0.81Ours 0.87 0.94 0.90Table 1: Entity linking performanceover 6% and 14% improvements in Precision andRecall, respectively.
The CSAW model devisesa set of entity features based on text content andlink structures of Wikipedia pages, and combinesthem to measure relatedness.
Compared to thesehand-crafted features which are essentially heuris-tic and hard to verify, our embedding model in-duces semantic representations by optimizing a s-ingle well-defined objective.
Note that the em-bedding actually also encodes the Wikipedia inter-page network, as we train on the entity-contextpairs which are extracted from wiki links.The Entity-TM model learns a representationfor each entity as a word distribution.
However, asnoted in (Baroni et al, 2014), the counting-baseddistributional model usually shows inferior perfor-mance than context-predicting methods as ours.Moreover, in addition to the text context, our mod-el integrates the entity hierarchical structure whichprovides rich knowledge of semantic relatedness.The comparison between Ours and Ours-NoH fur-ther reveals the effect of integrating the hierarchyin learning entity vectors.
With entity hierarchy,we obtain more semantically meaningful represen-tations that achieve 9% F1 improvement over en-tity vectors without hierarchical knowledge.4.2 Entity Search4.2.1 SetupDataset We use the dataset from INEX 2009 en-tity ranking track2, which contains 55 queries.
Thegolden standard results of each query contains aset of relevant entities each of which correspondsto a Wikipedia page.Criteria We use the common criteria ofprecision@k, i.e., the percentage of relevantentities in the top-k results (we set k = 10), aswell as precision@R where R is the number ofgolden standard entities for a query.2http://www.inex.otago.ac.nz/tracks/entity-ranking/entity-ranking.asp1297Baselines We compare our algorithm with thefollowing recent competitors.Balog (Balog et al, 2011) develops a proba-bilistic generative model which represents entities,as well as the query, as distributions over bothwords and categories.
Entities are then rankedbased on the KL-divergence between the distribu-tions.K&K (Kaptein and Kamps, 2013) exploitsWikipedia entity hierarchy to derive the content ofeach category, which is in turn used to measurerelatedness with the query categories.
It furtherincorporates inter-entity links for relevance propa-gation.Chen (Chen et al, 2014b) creates for each en-tity a context profile leveraging both the wholedocument (long-range) and sentences around en-tity (short-range) context, and models query textby a generative model.
Categories are weightedbased on the head words and other features.
Ouralgorithm exploits a similar method for categorymatching.Methods Precision@10 Precision@RBalog 0.18 0.16K&K 0.31 0.28Chen 0.55 0.42Ours 0.57 0.46Table 2: Entity search performance.4.2.2 ResultsTable 2 lists the entity search results of the com-petitors.
Our algorithm shows superiority over theprevious best performing methods.
Balog con-structs representations for each entity merely bycounting (and smoothing) its co-occurrence be-tween words and categories, which is inadequateto capture relatedness accurately.
K&K leveragesthe rich resources in Wikipedia such as text, hi-erarchy, and link structures.
However, the hand-crafted features are still suboptimal compared withour learned representations.Chen performs well by combining both long-and short-context of entities, as well as catego-ry lexical similarity.
Our algorithm replaces it-s text matching component with a semantic en-richment step, i.e., grounding entity mentions inthe query text onto KB entities.
This augmentsthe short query with rich background knowledge,facilitating accurate relatedness measure based onour high-quality entity embedding.4.3 Qualitative AnalysisWe qualitatively inspect the learned representa-tions of the entity hierarchy.
The results show thatboth the entity vectors and the category distancemetrics capture meaningful semantics, and can po-tentially boost a wide range of applications such asrecommendation and knowledge base completion.Entity vectors Table 3 shows a list of target en-tities, and their top-4 nearest entities in the wholeentity set or subsets belonging to given categories.Measuring under the whole set (column 2) resultsin nearest neighbors that are strongly related withthe target entity.
For instance, the nearest enti-ties for ?black hole?
are ?faster-than-light?, ?eventhorizon?, ?white hole?, and ?time dilation?, all ofwhich are concepts from physical cosmology andthe theory of relativity.
Similar results can be ob-served from other 3 examples.Even more interesting is to specify a categoryand search for the most related entities under thecategory.
The third column of Table 3 shows sev-eral examples.
E.g., our model found that the mostrelated Chinese websites to Youtube are ?Tudou?,?56.com?, ?Youku?
(three top video hosting ser-vices in China), and ?YinYueTai?
(a major MVsharing site in China).
The high-quality resultsshow that our embedding model is able to discov-er meaningful relationships between entities fromthe complex entity hierarchy and plain text.
Thiscan be a useful feature in a wide range of appli-cations such as semantic search (e.g., looking formovies about black hole), recommendation (e.g.,suggesting TV series of specific genre for kids),and knowledge base completion (e.g., extractingrelations between persons), to name a few.Target entity Most related entitiesblack holeoverall:faster-than-lightevent horizonwhite holetime dilationAmerican films:Hidden Universe 3DHubble (film)Quantum QuestParticle FeverYoutubeoverall:InstagramTwitterFacebookDipdiveChinese websites:Tudou56.comYoukuYinYueTaiHarvardUniversityoverall:Yale UniversityUniversity of PennsylvaniaPrinceton UniversitySwarthmore Collegebusinesspeople in software:Jack DangermondBill GatesScott McNealyMarc ChardonX-Men: Days ofFuture Past (film)overall:Marvel StudiosX-Men: The Last StandX2 (film)Man of Steel (film)children?s television series:Ben 10: Race Against TimeKim Possible: A Sitch in TimeBen 10: Alien ForceStar Wars: The Clone WarsTable 3: Most related entities under specific cate-gories.
?Overall?
represents the most general cat-egory that includes all the entities.1298Xbox 360 gamesXbox Xbox gamesXbox live arcade  gamesWindows 98Windows meWindows 7Windows server 2008 r2Windows 2000snapshots of Windows Vistasnapshots of Microsoft Officesnapshots of Windows 2000snapshots of Microsoft Windowssnapshots of Windows softwaresFigure 3: Distance metric visualization for thesubcategories of the category ?Microsoft?.
The t-SNE (Van der Maaten and Hinton, 2008) algorith-m is used to map the high-dimensional (diagonal)matrixes into the 2D space.Category distance metrics In addition to learn-ing vector representations of entities, we also as-sociate with each category a local distance metricto capture the features of individual category.
Aswe restrict the distance metrics to be diagonal ma-trixes, the magnitude of each diagonal value canbe viewed as how much a category is character-ized by the corresponding dimension.
Categorieswith close semantic meanings are expected to havesimilar metrics.Figure 3 visualizes the metrics of all subcate-gories under the category ?Microsoft?, where weamplify some parts of the figure to showcase theclustering of semantically relevant categories.
Forinstance, the categories of Microsoft Windows op-erating systems, and those of the Xbox games,are embedded close to each other, respectively.The results validate that our hierarchy embeddingmodel can not only encode relatedness betweenleaf entities, but also capture semantic similarityof the internal categories.
This can be helpful intaxonomy refinement and relation discovery.5 Related WorkDistributed representation There has been agrowing interest in distributed representation ofwords.
Skip-gram model (Mikolov et al, 2013a)is one of the most popular methods to learn wordrepresentations.
The model aims to find a rep-resentation for each word that is useful for pre-dicting its context words.
Word-context simi-larity is measured by simple inner product.
Aset of recent works generalizing the basic skip-gram to incorporate dependency context (Levyand Goldberg, 2014), word senses (Chen et al,2014a), and multi-modal data (Hill and Korho-nen, 2014).
However, these work leverages lim-ited structured knowledge.
Our proposed methodgoes beyond skip-gram significantly such that wemeasures entity-context similarity under aggregat-ed distance metrics of hierarchical category n-odes.
This effectively captures the structuredknowledge.
Another research line learn knowl-edge graph embedding (Lin et al, 2015; Wang etal., 2014; Bordes et al, 2013), which models enti-ties as vectors and relations as some operations onthe vector space (e.g., translation).
These work-s aim at relation prediction for knowledge graphcompletion, and can be viewed as a supplement tothe above that extracts semantics from plain text.Utilizing hierarchical knowledge Semantic hi-erarchies are key sources of knowledge.
Previ-ous works (Ponzetto and Strube, 2007; Leacockand Chodorow, 1998) use KB hierarchies to de-fine relatedness between concepts, typically basedon path-length measure.
Recent works (Yogatamaet al, 2015; Zhao et al, 2011) learn representa-tions through hierarchical sparse coding that en-forces similar sparse patterns between nearby n-odes.
Category hierarchies have also been wide-ly used in classification (Xiao et al, 2011; Wein-berger and Chapelle, 2009).
E.g., in (Verma et al,2012) category nodes are endowed with discrim-inative power by learning distance metrics.
Ourapproach differs in terms of entity vector learningand metric aggregation on DAG hierarchy.6 ConclusionIn this paper, we proposed to learn entity hierarchyembedding to boost semantic NLP tasks.
A princi-pled framework was developed to incorporate bothtext context and entity hierarchical structure fromlarge-scale knowledge bases.
We learn a distancemetric for each category node, and measure enti-ty vector similarity under aggregated metrics.
Aflexible and efficient metric aggregation schemewas also developed to model large-scale hierar-chies.
Experiments in both entity linking and enti-ty search tasks show superiority of our approach.The qualitative analysis indicates that our modelcan be potentially useful in a wide range of otherapplications such as knowledge base completionand ontology refinement.
Another interesting as-pect of future work is to incorporate other sourcesof knowledge to further enrich the semantics.AcknowledgmentsThis research is supported by NSF IIS-1218282,IIS-12511827, and IIS-1450545.1299ReferencesKrisztian Balog, Marc Bron, and Maarten De Rijke.2011.
Query modeling for entity search based onterms, categories, and examples.
TOIS, 29(4):22.Marco Baroni, Georgiana Dinu, and Germ?anKruszewski.
2014.
Dont count, predict!
asystematic comparison of context-counting vs.context-predicting semantic vectors.
In Proc.
ofACL, volume 1, pages 238?247.Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.2013.
Translating embeddings for modeling multi-relational data.
In Proc.
of NIPS, pages 2787?2795.Xinxiong Chen, Zhiyuan Liu, and Maosong Sun.2014a.
A unified model for word sense representa-tion and disambiguation.
In Proc.
of EMNLP, pages1025?1035.Yueguo Chen, Lexi Gao, Shuming Shi, Xiaoyong Du,and Ji-Rong Wen.
2014b.
Improving context andcategory matching for entity search.
In Proc.
ofAAAI.St?ephane Clinchant and Florent Perronnin.
2013.
Ag-gregating continuous word embeddings for informa-tion retrieval.
page 100.Gianluca Demartini, Tereza Iofciu, and Arjen PDe Vries.
2010.
Overview of the inex 2009 entityranking track.
In Focused Retrieval and Evaluation,pages 254?264.
Springer.Xianpei Han and Le Sun.
2012.
An entity-topic modelfor entity linking.
In Proc.
of EMNLP, pages 105?115.
Association for Computational Linguistics.Felix Hill and Anna Korhonen.
2014.
Learning ab-stract concept embeddings from multi-modal data:Since you probably can?t see what i mean.
In Proc.of EMNLP, pages 255?265.Rianne Kaptein and Jaap Kamps.
2013.
Exploiting thecategory structure of wikipedia for entity ranking.Artificial Intelligence, 194:111?129.Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan,and Soumen Chakrabarti.
2009.
Collective anno-tation of wikipedia entities in web text.
In Proc.
ofKDD, pages 457?466.
ACM.Claudia Leacock and Martin Chodorow.
1998.
Com-bining local context and wordnet similarity for wordsense identification.
WordNet: An electronic lexicaldatabase, 49(2):265?283.Omer Levy and Yoav Goldberg.
2014.
Dependency-based word embeddings.
In Proc.
of ACL, volume 2,pages 302?308.Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, andXuan Zhu.
2015.
Learning entity and relation em-beddings for knowledge graph completion.
In Proc.of AAAI.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013a.
Efficient estimation of word represen-tations in vector space.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013b.
Distributed representa-tions of words and phrases and their compositional-ity.
In Proc.
of NIPS, pages 3111?3119.Alexandre Passos, Vineet Kumar, and Andrew McCal-lum, 2014.
Lexicon Infused Phrase Embeddings forNamed Entity Resolution, pages 78?86.
Associationfor Computational Linguistics.Simone Paolo Ponzetto and Michael Strube.
2007.Knowledge derived from wikipedia for computingsemantic relatedness.
JAIR, 30(1):181?212.Philip Resnik.
1995.
Using information content to e-valuate semantic similarity in a taxonomy.
In Proc.of IJCAI, pages 448?453.Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Li-u, and Bing Qin.
2014.
Learning sentiment-specificword embedding for twitter sentiment classification.In Proc.
of ACL, pages 1555?1565.Laurens Van der Maaten and Geoffrey Hinton.
2008.Visualizing data using t-sne.
JMLR, 9(2579-2605):85.Nakul Verma, Dhruv Mahajan, Sundararajan Sella-manickam, and Vinod Nair.
2012.
Learning hier-archical similarity metrics.
In Proc.
of CVPR, pages2280?2287.
IEEE.Zhen Wang, Jianwen Zhang, Jianlin Feng, and ZhengChen.
2014.
Knowledge graph and text jointly em-bedding.
In Proc.
of EMNLP.Kilian Q Weinberger and Olivier Chapelle.
2009.Large margin taxonomy embedding for documentcategorization.
In Proc.
of NIPS, pages 1737?1744.Lin Xiao, Dengyong Zhou, and Mingrui Wu.
2011.Hierarchical classification via orthogonal transfer.In Proc.
of ICML, pages 801?808.Eric P Xing, Michael I Jordan, Stuart Russell, and An-drew Y Ng.
2002.
Distance metric learning withapplication to clustering with side-information.
InProc.
of NIPS, pages 505?512.Dani Yogatama, Manaal Faruqui, Chris Dyer, and NoahSmith.
2015.
Learning word representations withhierarchical sparse coding.
Proc.
of ICML.Jiajun Zhang, Shujie Liu, Mu Li, Ming Zhou, andChengqing Zong.
2014.
Bilingually-constrainedphrase embeddings for machine translation.
In Proc.of ACL.Bin Zhao, Fei Li, and Eric P Xing.
2011.
Large-scalecategory structure aware image categorization.
InProc.
of NIPS, pages 1251?1259.1300
