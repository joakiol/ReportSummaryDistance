An Automatic Evaluation Method for Localization OrientedLexicalised EBMT SystemJianmin Yao+, Ming Zhou++, Tiejun Zhao+, Hao Yu+, Sheng Li++School of Computer Science and TechnologyHarbin Institute of Technology,Harbin, China, 150001{james, tjzhao, yu, shengli}@mtlab.hit.edu.cn++Natural Language Computing GroupMicrosoft Research AsiaBeijing, China, 100080Mingzhou@microsoft.comAbstractTo help developing a localization orientedEBMT system, an automatic machinetranslation evaluation method isimplemented which adopts edit distance,cosine correlation and Dice coefficient ascriteria.
Experiment shows that theevaluation method distinguishes wellbetween ?good?
translations and ?bad?
ones.To prove that the method is consistent withhuman evaluation, 6 MT systems are scoredand compared.
Theoretical analysis is madeto validate the experimental results.Correlation coefficient and significance testsat 0.01 level are made to ensure thereliability of the results.
Linear regressionequations are calculated to map theautomatic scoring results to human scorings.IntroductionMachine translation evaluation has always beena key and open problem.
Various evaluationmethods exist to answer either of the twoquestions (Bohan 2000): (1) How can you tell ifa machine translation system is ?good??
And (2)How can you tell which of two machinetranslation systems is ?better??
Since manualevaluation is time consuming and inconsistent,automatic methods are broadly studied andimplemented using different heuristics.
Jones(2000) utilises linguistic information such asbalance of parse trees, N-grams, semanticco-occurrence and so on as indicators oftranslation quality.
Brew C (1994) compareshuman rankings and automatic measures todecide the translation quality, whose criteriainvolve word frequency, POS taggingdistribution and other text features.
Another typeof evaluation method involves comparison of thetranslation result with human translations.Yokoyama (2001) proposed a two-way MTbased evaluation method, which comparesoutput Japanese sentences with the originalJapanese sentence for the word identification,the correctness of the modification, the syntacticdependency and the parataxis.
Yasuda (2001)evaluates the translation output by measuring thesimilarity between the translation output andtranslation answer candidates from a parallelcorpus.
Akiba (2001) uses multiple editdistances to automatically rank machinetranslation output by translation examples.Another path of machine translation evaluationis based on test suites.
Yu (1993) designs a testsuite consisting of sentences with various testpoints.
Guessoum (2001) proposes asemi-automatic evaluation method of thegrammatical coverage machine translationsystems via a database of unfolded grammaticalstructures.
Koh (2001) describes their test suiteconstructed on the basis of fine-grainedclassification of linguistic phenomena.There are many other valuable reports onautomatic evaluation.
All the evaluationmethods show the wisdom of authors in theirutilisation of available tools and resources forautomatic evaluation tasks.
For ourlocalization-oriented lexicalised EBMT systeman automatic evaluation module is implemented.Some string similarity criteria are taken asheuristics.
Experimental results show that thismethod is useful in quality feedback indevelopment of the EBMT system.
Six machinetranslation systems are utilised to test theconsistency between the automatic method andhuman evaluation.
To avoid stochastic errors,significance test and linear correlation arecalculated.
Compared with previous works, oursis special in the following ways: 1) It isdeveloped for localisation-oriented EBMT,which demands higher translation quality.
2)Statistical measures are introduced to verify thesignificance of the experiments.
Linearregression provides a bridge over human andautomatic scoring for systems.The paper is organised as follows: First thelocalization-oriented lexicalised EBMT systemis introduced as the background of evaluationtask.
Second the automatic evaluation method isfurther described.
Both theoretical andimplementation of the evaluation method arefully discussed.
Then six systems are evaluatedboth manually and with our automatic method.Consistency between the two methods isanalysed.
At last before the conclusion, linearcorrelation and significance test validate theresult and exclude the possibility of randomconsistency.1 EBMT Evaluation Solution1.1 EBMT System SetupFrom Figure 1 you can get a general overview ofour EBMT system.Input sentenceTransfer<Phrase Alignment>Translation resultResources(Bilingual andmonolingual)Example Base(Softwaremanual) MatchRecombineFigure 1.
Flowchart of the EBMT SystemThe EBMT system is developed forlocalization purpose, which demands thetranslation to be restricted in style andexpression.
This makes it rational to take stringsimilarity as criterion for translation qualityevaluation.
The solution is useful because inlocalization, an example based machinetranslation system helps only if it outputs thevery high quality translation results.1.2 Evaluation CriteriaThe criteria we utilise for evaluation include editdistance, dice coefficient and cosine correlationbetween (the vectors or word bag sets of) themachine translation and the gold standardtranslation.
Followed is a detailed description ofthe three criteria.The edit distance between two strings s1and s2, is defined as the minimum number ofoperations to become the same(Levenshtein1965).
It gives an indication of how`close' or ?similar?
two strings are.
Denote thelength of a sentence s as |s|.
A two-dimensionalmatrix, m[0...|s1|,0...|s2|] is used to hold the editdistance values.
The algorithm is as follows(Wagner 1974):Step 1 Initialization:For i=0 to |s1|m[i, 0] = i//initializing the columnsFor j=1 to |s2|m[0, j] = j //initializing the rowsStep 2 Iteration:For i=1 to |s1|For j=1 to |s2|if(s1[i] = s2[j]){d=m[i-1,j-1]}//equalityelse{d=m[i-1,j-1]+1}//substringm[i, j]=min(m[i-1,j]+1,m[i,j-1]+1,d)End ForEnd ForStep 3: Result:Return m[i,j]Figure 2.
Algorithm for Edit DistanceThe time complexity of this algorithm isO(|s1|*|s2|).
If s1 and s2 have a `similar' length,about `n' say, this complexity is O(n2).Taking into account the lengths oftranslations, the edit distance is normalised as21)2,1(d2tDistancenormal_edissss+?=(1)Cosine correlation between the vectors oftwo sentences is often used to compute thesimilarity in information retrieval between adocument and a query (Manning 1999).
In ourtask, it is a similarity criterion defined asfollows:?= ?=?
?=?= n1in1i2w2i2w1in1i2i)(w1is2)cos(s1,w(2)Wherew1i = weight of ith term in vector of sentences1,w2i = weight of ith term in vector for sentences2,n = number of words in sum vector of s1 and s2.The cosine correlation reaches maximum valueof 1 when the two strings s1 and s2 are the same,while if none of the elements co-occurs in bothvectors, the cosine value will reach its minimumof 0.Another criterion we utilised is the Dicecoefficient of element sets of strings s1 and s2,21212)2,1(ssssssDice +?=I(3)The Dice coefficient demonstrates theintuitive that good translation tends to havemore common words with standard than badones.
This is especially true for example basedmachine translation for localization purpose.1.3 Relationship Among Similarity CriteriaIn this section we analyse the relationshipbetween the criteria so that we have a betterunderstanding of the experiment results.If weight of all words are 1, i.e.
each word hasthe uniform importance to translation quality,the cosine value becomes very similar to theDice coefficient criterion.
if we assume??
?=else        0rsboth vectoin  occurs ith word  theiff     1bi??
?=else        0s1 ofin vector  occurs ith word  theiff     11ib??
?=else        0s2 ofin vector  occurs ith word  theiff     12ibthen?= ?=?
?=?= n1in1i2w2i2w1in1i2i)(w1is2)cos(s1,w?= ?=?
?== niniibibnibi1 12221121211 1211ssssniniibibnibi?=?= ?=?
?== ISimilar to (3), this is also a calculation of thenumber of words in common The Dicecoefficient and cosine function have commoncharacteristics.
Especially when two strings areof the same length, we have)2,1(21212121112121211 1211)2,1cos(ssDicesssssssssssssssniniibibnibiss=+?==?=?=?= ?=?
?==IIIIThe above equation holds if and only if |s1|== |s2|.
The experimental results will clearlydemonstrate the correspondence between cosinecorrelation and Dice coefficient.
The two valuesbecome more similar as the lengths of the twostrings draw nearer.
They become the samewhen the two sentences are of the same length.The (normalized) edit distance evaluationhas a somewhat different variance from the othertwo values.
Edit distance cares not only howmany words there are in common, but also takesinto account the factor of word order adjustment.For example, take two strings of s1 and s2composed of words,s1 = w1 w2 w3 w4s2 = w1 w3 w2 w4Then,1444221212)2,1( =+?=+?= ssssssDiceI14441 122211)2,1cos( =?=?=?=?
?==niniibibnibiss5.0442221)2,1(d2 tDistancenormal_edi2s2)ce(s1,editDistan=+?=+?==ssssEdit distance and the other two criteria havetheir respective good aspects and shortcomings.So they can complement each other in theevaluation work.In the EBMT development, we sort thetranslations by a combination of the three factors,i.e.
first by Dice coefficient in descending order,then by cosine correlation in descending order,last by normalized edit distance in ascendingorder.
This method makes a simple combinationof the three factors, while no more complexityarises from this combination.2 Experiments and Results2.1 Experimental SetupOur evaluation method is designed to help indeveloping the EBMT system.
It is supposed tosort the translations by quality.
Experimentsshow that it works well sorting the sentences byorder of it?s being good or bad translations.
Inorder to justify the effectiveness of theevaluation method, we also design experimentsto compare the automatic evaluation with humanevaluation.
The result shows good compatibilitybetween the automatic and human evaluationresults.
Followed are details of the experimentalsetup and results.In order to evaluate the performance of ourEBMT system, a sample from a bilingual corpusof Microsoft Software Manual is taken as thestandard test set.
Denote the source sentences inthe test set as set S, and the target T. Sentencesin S are fed into the EBMT system.
We denotethe output translation set as R. Every sentence tiin T is compared with the correspondingsentence ri in R. Evaluation results are got viathe functions cosine(ti, ri), Dice(ti, ri), andnormalized edit distance normal_editDistance(ti,ri).
As discussed in the previous section, goodtranslations tend to have higher values of cosinecorrelation, Dice coefficient and lower editdistance.
After sorting the translations by thesevalues, we will see clearly which sentences aretranslated with high quality and which are not.Knowledge engineers can obtain much helpfinding the weakness of the EBMT system.Some sample sentences and evaluationresults are attached in the Appendix.
In ourexperience, with Dice as example, thetranslations scored above 0.7 are fairly goodtranslations with only some minor faults; thosebetween 0.5 and 0.7 are faulty ones with somegood points; while those scored under 0.4 areusually very bad translations.
From theseexamples, we can see that the three criteriareally help sorting the good translation fromthose bad ones.
This greatly aids the developersto find out the key faults in sentence types andgrammar points.2.2 Comparison with Human EvaluationIn the above descriptions, we have presented ourtheoretical analysis and experimental results ofour string similarity based evaluation method.The evaluation has gained the followingachievements: 1) It helps distinguishing ?good?translations from ?bad?
ones in developing theEBMT system; 2) The scores give us a clearview of the quality of the translations inlocalization based EBMT.
In this section we willmake a direct comparison between humanevaluation and our automatic machineevaluation to test the effectiveness of the stringsimilarity evaluation method.
To tackle thisproblem, we carry out another experiment, inwhich human scoring of systems are comparedwith the machine scoring.The human scoring is carried out with a testsuite of High School English.
Six undergraduatestudents are asked to score the translationsindependent from each other.
The average oftheir scoring is taken as human scoring result.The method is similar to ALPAC scoring system.We score the translations with a 6-point scalesystem.
The best translations are scored 1.
If it?snot so perfect, with small errors, the translationgets a score of 0.8.
If a fatal error occurs in thetranslation but it?s still understandable, a pointof 0.6 is scored.
The worst translation gets 0Table 1.
Human Evaluation of 6 Machine Translation SystemsSystem# #1 #2 #3 #4 #5 #6Error5 5 5% 1 1% 2 2% 4 4% 9 9% 7 7%Error4 4 4% 6 6% 4 4% 7 7% 18 18% 21 21%Error3 7 7% 14 14% 21 21% 23 23% 23 23% 26 26%Error2 14 14% 15 14% 21 21% 19 19% 18 18% 17 17%Error1 15 14% 17 17% 33 32% 16 16% 15 15% 8 8%Perfect 57 56% 49 48% 21 21% 33 32% 19 19% 23 23%Good% 70% 65% 43% 48% 34% 31%Score 81 78 69 68 55 54point of score.
Table 1 shows the manualevaluation results for 6 general-purpose machinetranslation systems available to us.
In table 1,Error5 means the worst translation.
Error4 toError1 are better when the numbering becomessmaller.
A translation is labelled ?Perfect?
whenit?s a translation without any fault in it.?Good%?
is the sum of percent of ?Error1?
and?Perfect?.
Because ?Error1?
translations refer tothose have small imperfections.
?Score?
is theweighted sum of scores of the 6 kinds oftranslations.
E.g.
for machine translation systemMTS1, the score is calculated as follows:811578.0156.0144.072.0405)1(=?+?+?+?+?+?=MTSscoreIn table 2, the human scorings and automaticscorings of the 6 machine translation systems arelisted.
The translations of system #1 are taken asstandard for automatic evaluations, i.e.
allscorings are made on the basis of the result ofsystem #1.
In principle this will introduce someerrors, but we suppose it not so great as toinvalidate the automatic evaluation result.
Thisis also why the scorings of system #1 are 100.The last row labele AutoAver is the average ofautomatic evaluations.Table 2.
Scoring of 6 MT SystemsSystem# #1 #2 #3 #4 #5 #6Human 100 78 69 68 55 54Dice 100 70 57 65 48 56Cosine 100 75 64 72 55 63Edistance 100 78 69 75 63 68AutoAver 100 74 63 71 55 62Figure 3 presents the scorings of Dicecoefficient, cosine correlation, edit distance andthe average of the three automatic criterions in achart, we can clearly see the consistency amongthese parameters.4 05 06 07 08 09 01 0 01 2 3 4 5 6D i c e C o s i n eE d i t D A u t o A v e rFigure 3.
Automatic Scoring of 6 MT SystemsIn Figure 3, the numbers on X-axis are thenumbering of machine translation systems,while the Y-axis denotes the evaluation scores.4050607080901001 2 3 4 5 6Human AutomaticFigure 4.
Scoring of 6 MT SystemsThe human and automatic average scoringis shown in Figure 4.
The Automatic data refersto the average of Dice, cosine correlation andedit distance scorings.
On the whole, human andautomatic evaluations tend to present similarscores for a specific system, e.g.
78/74 forsystem #2, while 69/63 for system #3.3 Result AnalysisThe experimental results and the charts haveshown some intuitionistic relationship amongthe automatic criteria of Dice coefficient, cosinevalue, edit distance and the human evaluationresult.
A more solid analysis is made in thissection to verify this relationship.
Statisticalanalysis is a useful tool to 1) find therelationship between data sets and 2) decidewhether the relationship is significant enough orjust for random errors.The measure of linear correlation is a wayof assessing the degree to which a linearrelationship between two variables is implied byobserved data.
The correlation coefficientbetween variable X and Y is defined asYXssYXCOVYXr ),(),( =(7)whereCOV(X,Y) is the covariance defined by?
??
?= ))((11),( YYXXnYXCOV ii  (8)The symbol meanings are as follows:sX: sample standard deviation of variable XsY: sample standard deviation of variable Yn: sample sizeXi (Yi) : the ith component of variable X (Y)X (Y ): the sample mean of variable X (Y)From its definition, we know that the correlationcoefficient is scale-independent and 11 ???
r .After we get the correlation coefficient r, asignificance test at the level 01.0=?
is madeto verify whether the correlation is real or justdue to random errors.
Linear regression is usedto construct a model that specifies the linearrelationship between the variables X and Y. Ascatter diagram and regression line will bepresented for an intuitionistic view of therelationship.
The results are presented in thegraphs below.
In the graphs, the humanevaluation results are placed on the X axis, whilethe automatic results are on the Y axis.Correlation coefficient and the linear regressionequation are shown below the graphs.
Takinginto the sample size and the correlationcoefficient, the significance level is alsocalculated for the statistical analysis.Figure 5.
Human (X) and AutoAver (Y)Y=8.0+0.89X, P < 0.01r = 0.96, P < 0.01Figure 6.
Human (X) and Dice (Y)Y=6.9+1.03X, P < 0.01r = 0.96, P < 0.01Figure 7.
Human (X) and Cosine (Y)Y=9.3+0.88X, P < 0.01r = 0.96, P < 0.01Figure 8.
Human (X) and Edistance (Y)Y=23.3+0.74X, P < 0.01r = 0.95, P < 0.01It is a property of r that it has a valuedomain of [-1,+1].
A positive r implies that theX and Y tend to increase/decrease together.
Aminus r implies a tendency for Y to decrease asX increases and vice versa.
When there is noparticular relation between X and Y, r tends tohave a value close to zero.
From the aboveanalysis, we can see that the Dice coefficient,cosine, and average of the automatic values arehighly correlated with the human evaluationresults with r=0.96.
P < 0.01 shows the twovariables are strongly correlated with asignificance level beyond the 99%.
While P <0.01 for the linear regression equation has thesame meaning.ConclusionOur evaluation method is designed for thelocalization oriented EBMT system.
This is whywe take string similarity criteria as basis of theevaluation.
In our approach, we take editdistance, dice coefficient and cosine correlationbetween the machine translation results and thestandard translation as evaluation criteria.
Atheoretical analysis is first made so that we canknow clearly the goodness and shortcomings ofthe three factors.
The evaluation has been usedin our development to distinguish badtranslations from good ones.
Significance test at0.01 level is made to ensure the reliability of theresults.
Linear regression and correlationcoefficient are calculated to map the automaticscoring results to human scorings.AcknowledgementsThis work was done while the author visitedMicrosoft Research Asia.
Our thanks go to WeiWang, Jinxia Huang, and Professor ChangningHuang at Microsoft Research Asia and JingZhang, Wujiu Huang at Harbin Institute ofTechnology.
Their help has contributed much tothis paper.ReferencesA.
Guessoum, R. Zantout, Semi-AutomaticEvaluation of the Grammatical Coverage ofMachine Translation Systems, MT Summit?conference, Santiago de Compostela, 2001Brew C, Thompson H.S, Automatic Evaluation ofComputer Generated Text: A Progress Report onthe TextEval Project, Proceedings of the HumanLanguage Technology Workshop, 108-113, 1994.Christopher D. Manning, Hinrich Schutze,Foundations of Statistical Natural LanguageProcessing, the MIT Press, 1999, 530-572Douglas A. Jones, Gregory M. Rusk, 2000, Toward aScoring Function for Quality-Driven MachineTranslation, Proceedings of COLING-2000.Keiji Yasuda, Fumiaki Sugaya, etc, An AutomaticEvaluation Method of Translation Quality UsingTranslation Answer Candidates Queried from aParallel Corpus, MT Summit?
conference, Santiagode Compostela, 2001Language and Machines.
Computers in Translationand Linguistics, (ALPAC report, 1966).
NationalAcademy of Sciences, 1966Niamh Bohan, Elisabeth Breidt, Martin Volk, 2000,Evaluating Translation Quality as Input to ProductDevelopment, 2nd International Conference onLanguage Resources and Evaluation, Athens, 2000.Shoichi Yokoyama, Hideki Kashioka, etc., AnAutomatic Evaluation Method for MachineTranslation using Two-way MT, 8th MT Summitconference, Santiago de Compostela, 2001Sungryong Koh, Jinee Maeng, etc, A Test Suite forEvaluation of English-to-Korean MachineTranslation Systems, MT Summit?
conference,Santiago de Compostela, 2001Shiwen Yu, Automatic Evaluation of Quality forMachine Translation Systems, Machine Translation,8: 117-126, 1993, Kluwer Academic Publishers,printed in the Netherlands.Wagner A.R.
and Fischer M., The string-to-stirngcorrection problem, Journal of the ACM, Vol.
21,No.
1, 168-173V.I.
Levenshtein, Binary codes capable of correctingdeletions, insertions and reversals.
DokladyAkademii Nauk SSSR 163(4) 845-848, 1965Yasuhiro Akiba, Kenji Imamura, and Eiichiro Sumita,Using Multiple Edit Distances to AutomaticallyRank Machine Translation Output, MT Summit?conference, Santiago de Compostela, 2001Appendix: Automatic Evaluation Resultscosine      Dice  edistance* `  standard translation&EBMT translation0.27273     0.27273      44/6=7     ???????MAPI?????
?extendedmapi?0.43301     0.42857     28/6=4     ??????????mail?
?0.53452     0.53333 30/7=4      ??????????role??
?0.62994     0.625     32/4=8      ???????????????
?0.7     0.7      80/16=5      ???????????????????????????????????????
?0.72058     0.72      50/11=4      ????????????????????????
?0.78335     0.78261 46/3=15      ??????????????????????
?0.81786     0.81633 98/20=4      ????????????????????????????????????????????????
?0.8528     0.84211 76/12=6      ?????????????????????????????????????
?0.86772     0.86486 37/2=18      ??????????????????
:0.875      0.875  32/1=32   ???????????????
?0.90889     0.90476 42/2=21      ??????????...????????...
*Notes: The data presented in ?edistance?
is the reciprocal of the normalized edit distance: the numerator is |s1 + s2| in bytes ; thedenominator is the edit distance in Chinese characters or English words.
