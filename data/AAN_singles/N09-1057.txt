Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 503?511,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsMore than Words:Syntactic Packaging and Implicit SentimentStephan Greene?ATG, Inc.1111 19th St, NW Suite 600Washington, DC 20036sgreene@atg.comPhilip ResnikLinguistics / UMIACS CLIP LaboratoryUniversity of MarylandCollege Park, MD 20742resnik@umiacs.umd.eduAbstractWork on sentiment analysis often focuses onthe words and phrases that people use inovertly opinionated text.
In this paper, we in-troduce a new approach to the problem thatfocuses not on lexical indicators, but on thesyntactic ?packaging?
of ideas, which is wellsuited to investigating the identification of im-plicit sentiment, or perspective.
We establish astrong predictive connection between linguis-tically well motivated features and implicitsentiment, and then show how computationalapproximations of these features can be usedto improve on existing state-of-the-art senti-ment classification results.1 IntroductionAs Pang and Lee (2008) observe, the last severalyears have seen a ?land rush?
in research on senti-ment analysis and opinion mining, with a frequentemphasis on the identification of opinions in evalua-tive text such as movie or product reviews.
How-ever, sentiment also may be carried implicitly bystatements that are not only non-evaluative, but noteven visibly subjective.
Consider, for example, thefollowing two descriptions of the same (invented)event:1(a) On November 25, a soldier veered his jeep intoa crowded market and killed three civilians.
(b) On November 25, a soldier?s jeep veered into acrowded market, causing three civilian deaths.
?This work was done while the first author was a student inthe Department of Linguistics, University of Maryland.Both descriptions appear on the surface to be objec-tive statements, and they use nearly the same words.Lexically, the sentences?
first clauses differ only inthe difference between ?s and his to express the rela-tionship between the soldier and the jeep, and in thesecond clauses both kill and death are terms withnegative connotations, at least according to the Gen-eral Inquirer lexicon (Stone, 1966).
Yet the descrip-tions clearly differ in the feelings they evoke: if thesoldier were being tried for his role in what hap-pened on November 25, surely the prosecutor wouldbe more likely to say (1a) to the jury, and the defenseattorney (1b), rather than the reverse.1Why, then, should a description like (1a) be per-ceived as less sympathetic to the soldier than (1b)?If the difference is not in the words, it must be inthe way they are put together; that is, the structureof the sentence.
In Section 2, we offer a specific hy-pothesis about the connection between structure andimplicit sentiment: we suggest that the relationshipis mediated by a set of ?grammatically relevant?
se-mantic properties well known to be important cross-linguistically in characterizing the interface betweensyntax and lexical semantics.
In Section 3, we val-idate this hypothesis by means of a human ratingsstudy, showing that these properties are highly pre-dictive of human sentiment ratings.
In Section 4, weintroduce observable proxies for underlying seman-tics (OPUS), a practical way to approximate the rele-vant semantic properties automatically as features ina supervised learning setting.
In Section 5, we showthat these features improve on the existing state ofthe art in automatic sentiment classification.
Sec-1We refer readers not sharing this intuition to Section 3.503tions 6 and 7 discuss related work and summarize.2 Linguistic MotivationVerbal descriptions of an event often carry alongwith them an underlying attitude toward what is be-ing described.
By framing the same event in differ-ent ways, speakers or authors ?select some aspectsof a perceived reality and make them more salientin a communicating text, in such a way as to pro-mote a particular problem definition, causal inter-pretation, moral evaluation, and/or treatment recom-mendation?
(Entman, 1993, p. 52).
Clearly lexi-cal choices can accomplish this kind of selection,e.g.
choosing to describe a person as a terroristrather than a freedom fighter, or referencing killerwhales rather than orcas.2 Syntactic choices canalso have framing effects.
For example, Ronald Rea-gan?s famous use of the passive construction, ?Mis-takes were made?
(in the context of the Iran-Contrascandal), is a classic example of framing or spin:used without a by-phrase, the passive avoids iden-tifying a causal agent and therefore sidesteps the is-sue of responsibility (Broder, 2007).
A toddler whosays ?My toy broke?
instead of ?I broke my toy?
isemploying the same linguistic strategy.Linguists have long studied syntactic variationin descriptions of the same event, often under thegeneral heading of syntactic diathesis alternations(Levin, 1993; Levin and Hovav, 2005).
This lineof research has established a set of semantic prop-erties that are widely viewed as ?grammatically rel-evant?
in the sense that they enable generalizationsabout syntactic ?packaging?
of meaning within (andacross) the world?s languages.
For example, theverb break in English participates in the causative-inchoative alternation (causative event X broke Ycan also be expressed without overt causation as Ybroke), but the verb climb does not (X also causesthe event in X climbed Y, but that event cannot beexpressed as Y climbed).
These facts about partic-ipation in the alternation turn out to be connectedwith the fact that a breaking event entails a change ofstate in Y but a climbing event does not.
Grammati-cally relevant semantic properties of events and their2Supporters of an endangered species listing in Puget Soundgenerally referred to the animals as orcas, while opponents gen-erally said killer whales (Harden, 2006).participants ?
causation, change of state, and others?
are central not only in theoretical work on lex-ical semantics, but in computational approaches tothe lexicon, as well (e.g.
(Pustejovsky, 1991; Dorr,1993; Wu and Palmer, 1994; Dang et al, 1998)).The approach we propose draws on two influ-ential discussions about grammatically relevant se-mantic properties in theoretical work on lexical se-mantics.
First, Dowty (1991) characterizes gram-matically relevant properties of a verb?s arguments(e.g.
subject and object) via inferences that followfrom the meaning of the verb.
For example, expres-sions like X murders Y or X interrogates Y entailthat subject X caused the event.3 Second, Hopperand Thompson (1980) characterize ?semantic transi-tivity?
using similar properties, connecting semanticfeatures to morphosyntactic behavior across a widevariety of languages.Bringing together Dowty with Hopper andThompson, we find 13 semantic properties or-ganized into three groups, corresponding to thethree components of a canonical transitive clause,expressed as X verb Y in English.4 Proper-ties associated with X involve volitional involve-ment in the event or state, causation of the event,sentience/awareness and/or perception, causing achange of state in Y , kinesis or movement, and ex-istence independent of the event.
Properties asso-ciated with the event or state conveyed by the verbinclude aspectual features of telicity (a defined end-point) and punctuality (the latter of which may beinversely related to a property known as incremen-tal theme).
Properties associated with Y includeaffectedness, change of state, (lack of) kinesis ormovement, and (lack of) existence independent ofthe event.Now, observe that this set of semantic proper-ties involves many of the questions that would nat-urally help to shape one?s opinion about the eventdescribed by veer in (1).
Was anyone or anythingaffected by what took place, and to what degree?Did the event just happen or was it caused?
Did theevent reach a defined endpoint?
Did participation in3Kako (2006) has verified that people make these inferencesbased on X?s syntactic position even when a semantically emptynonsense verb is used.4We are deliberately sidestepping the choice of terminologyfor X and Y, e.g.
proto-Patient, theme, etc.504the event involve conscious thought or intent?
Ourhypothesis is that the syntactic aspects of ?framing?,as characterized by Entman, involve manipulation ofthese semantic properties, even when overt opinionsare not being expressed.
That is, we propose a con-nection between syntactic choices and implicit senti-ment mediated by the very same semantic propertiesthat linguists have already identified as central whenconnecting surface expression to underlying mean-ing more generally.3 Empirical ValidationWe validated the hypothesized connection betweenimplicit sentiment and grammatically relevant se-mantic properties using psycholinguistic methods,by varying the syntactic form of event descriptions,and showing that the semantic properties of descrip-tions do indeed predict perceived sentiment.53.1 Semantic property ratingsMaterials.
Stimuli were constructed using 11verbs of killing, which are widely viewed as proto-typical for the semantic properties of interest here(Lemmens, 1998): X killed Y normally involvesconscious, intentional causation by X of a kineticevent that causes a (rather decisive and clearly ter-minated!)
change of state in Y .
The verbs comprisetwo classes: the ?transitive?
class, involving ex-ternally caused change-of-state verbs (kill, slaugh-ter, assassinate, shoot, poison), and the ?ergative?class (strangle, smother, choke, drown, suffocate,starve), within which verbs are internally caused(McKoon and MacFarland, 2000) or otherwise em-phasize properties of the object.
Variation of syntac-tic description involved two forms: a transitive syn-tactic frame with a human agent as subject (?transi-tive form?, 2a), and a nominalization of the verb assubject and the verb kill as the predicate (?nominal-ized form?, 2b).2(a) The gunmen shot the opposition leader(b) The shooting killed the opposition leaderParticipants and procedure.
A set of 18 vol-unteer participants, all native speakers of English,were presented with event descriptions and asked toanswer questions probing both Dowty?s proto-role5Full details and materials in Greene (2007).properties as well as Hopper and Thompson?s se-mantic transitivity components, responding via rat-ings on a 1-to-7 scale.
For example, the questionsprobing volition were: ?In this event, how likelyis it that ?subject?
chose to be involved?
?, where?subject?
was the gunmen and the shooting, for 2(a-b), respectively.63.2 Sentiment ratingsMaterials.
We used the materials above to con-struct short, newspaper-like paragraphs, each oneaccompanied by a ?headline?
version of the samesyntactic descriptions used above.
For example,given this paragraph:A man has been charged for the suffocation of awoman early Tuesday morning.
City police saythe man suffocated the 24-year-old woman usinga plastic garbage bag.
The woman, who police sayhad a previous relationship with her attacker, wason her way to work when the incident happened.Based on information provided by neighbors, po-lice were able to identify the suspect, who was ar-rested at gunpoint later the same day.the three alternative headlines would be:3(a) Man suffocates 24-year old woman(b) Suffocation kills 24-year-old woman(c) 24-year-old woman is suffocatedSome paragraphs were based on actual news sto-ries.7 In all paragraphs, there is an obvious nomi-nal referent for both the perpetrator and the victim,it is clear that the victim dies, and the perpetratorin the scenario is responsible for the resulting deathdirectly rather than indirectly (e.g.
through negli-6Standard experimental design methods were followed withrespect to counterbalancing, block design, and distractor stim-uli; for example, no participant saw more than one of 2(a) or2(b), and all participants saw equal numbers of transitive andnominalized descriptions.
The phrase In this event was repeatedin each question and emphasized visually in order to encourageparticipants to focus on the particular event described in the sen-tence, rather than on the entities or events denoted in general.7In those cases no proper names were used, to avoid anyinadvertent emotional reactions or legal issues, although the de-scriptions retained emotional impact because we wanted readersto have some emotional basis with which to judge the headlines.505gence).8 The stem of the nominalization always ap-peared in the event description in either verbal ornominal form.Participants and procedure.
A set of 31 volun-teers, all native speakers of English, were presentedwith the paragraph-length descriptions and accom-panying headlines.
As a measure of sentiment, par-ticipants were asked to rate headlines on a 1-to-7scale with respect to how sympathetic they perceivethe headline to be toward the perpetrator.
For exam-ple, given the paragraph and one of the associatedheadlines in (3), a participant would be asked to rate?How sympathetic or unsympathetic is this headlineto the man?
?93.3 Analysis and discussionUnsurprisingly, but reassuringly, an analysis of thesentiment ratings yields a significant effect of syn-tactic form on sympathy toward the perpetrator(F (2, 369) = 33.902, p < .001), using a mixedmodel ANOVA run with the headline form as fixedeffect.
The transitive form of the headline yieldedsignificantly lower sympathy ratings than the nom-inalized or passive forms in pairwise comparisons(both p < .001).
We have thus confirmed empir-ically that Reagan?s ?Mistakes were made?
was awise choice of phrasing on his part.More important, we are now in a position to ex-amine the relationship between syntactic forms andperceived sentiment in more detail.
We performedregression analyses treating the 13 semantic prop-erty ratings plus the identity of the verb as indepen-dent variables to predict sympathy rating as a de-pendent variable, using the 24 stimulus sentencesthat bridged both collections of ratings.10 Consid-8An alert reader may observe that headlines with nominal-ized subjects using the verb kill require some other nominaliza-tion, so they don?t say ?Killing kills victim?.
For these casesin the data, an appropriate nominalization drawn from the eventdescription was used (e.g., explosion).9Again, standard experimental design methods were usedwith respect to block design, distractor stimuli, etc.
The phrasethis headline was emphasized to stress that it is the headlinebeing rated, not the story.
A second question rating sympathytoward the victim was also asked in each case, as an additionaldistractor.10These involved only the transitive and nominalized forms,because many of the questions were inapplicable to the passiveform.
Since the two ratings studies involved different subjectering semantic properties individually, we find thatvolition has the strongest correlation with sympathy(a negative correlation, with r = ?.776), followedby sentience (r = ?.764) and kinesis/movement(r = ?.751).
Although performing a multiple re-gression with all variables for this size dataset is im-possible, owing to overfitting (as a rule of thumb,5 to 10 observed items are necessary per each in-dependent variable), a multiple regression involvingverb, volition, and telicity as independent variablesyields R = .88, R2 = .78 (p < .001).
The value foradjusted R2, which explicitly takes into account thesmall number of observations, is 74.1.In summary, then, this ratings study confirms theinfluence of syntactic choices on perceptions of im-plicit sentiment.
Furthermore, it provides supportfor the idea that this influence is mediated by ?gram-matically relevant?
semantic properties, demonstrat-ing that these accounted for approximately 75% ofthe variance in implicit sentiment expressed by al-ternative headlines describing the same event.4 Observable ApproximationThus far, we have established a predictive connec-tion between syntactic choices and underlying or im-plicit sentiment, mediated by grammatically relevantsemantic properties.
In an ideal world, we could har-ness the predictive power of those properties by us-ing volition, causation, telicity, etc.
as features forregression or classification in sentiment predictiontasks.
Unfortunately, the properties are not directlyobservable, and neither automatic annotators nor la-beled training data currently exist.We therefore pursue a different strategy, which werefer to as observable proxies for underlying seman-tics (OPUS).
It can be viewed as a middle groundbetween relying on construction-level syntactic dis-tinctions (such as the 3-way transitive, nominalizedsubject, passive distinction in Section 3) and an-notation of fine-grained semantic properties.
Thekey idea is to use observable grammatical relations,drawn from the usages of terms determined to berelevant to a domain, as proxies for the underlyingsemantic properties that gave rise to their syntacticrealization using those relations.
Automatically cre-pools, regression models were run over the mean values of eachobservation in the experimental data.506ated features based on those observable proxies arethen used in classification as described in Section 5.In order to identify the set T of terms relevantto a particular document collection, we adopt therelative frequency ratio (Damerau, 1993), R(t) =Rtdomain/Rtreference, where Rtc = ftcNc is the ratio ofterm t?s frequency in corpus c to the size Nc of thatcorpus.
R(t) is a simple but effective comparisonof a term?s prevalence in a particular collection ascompared to a general reference corpus.
We usedthe British National Corpus as the reference becauseit is both very large and representative of text from awide variety of domains and genres.
The thresholdof R(t) permitting membership in T is an experi-mental parameter.OPUS features are defined in terms of syntacticdependency relations involving terms in T .
Given aset D of syntactic dependency relations, features areof the form t : d or d : t, with d ?
D, t ?
T .
Thatis, they are term-dependency pairs extracted fromterm-dependency-term dependency tuples, preserv-ing whether the term is the head or the dependentin the dependency relation.
In addition, we add twoconstruction-specific features: TRANS:v, which rep-resents verb v in a canonical, syntactically transitiveusage, and NOOBJ:v, present when verb v is usedwithout a direct object.11Example 4 shows source text (bolded clause in4a), an illustrative subset of parser dependencies(4b), and corresponding OPUS features (4c):4(a) Life Without Parole does not eliminate the riskthat the prisoner will murder a guard, a visi-tor, or another inmate.
(b) nsubj(murder, prisoner); aux(murder, will);dobj(murder, guard)(c) TRANS:murder, murder:nsubj, nsubj:prisoner,murder:aux, aux:will, murder:dobj, dobj:guardIntuitively the presence of TRANS:murder suggeststhe entire complex of semantic properties discussedin Section 2, bringing together the impliciation ofvolition, causation, etc.
on the part of prisoner(as does nsubj:prisoner), affectedness and change ofstate on the part of guard (as does dobj:guard), andso forth.11We parsed English text using the Stanford parser.The NOOBJ features can capture a habitual read-ing, or in some cases a detransitivizing effect as-sociated with omission of the direct object (Olsenand Resnik, 1997).
The bold text in (5) yieldsNOOBJ:kill as a feature.5(a) At the same time, we should never ignore therisks of allowing the inmate to kill again.In this case, omitting the direct object decreases theextent to which the killing event is interpreted astelic, and it eliminates the possibility of attributingchange-of-state to a specific affected object (muchlike ?Mistakes were made?
avoids attributing causeto a specified subject), placing the phrasing at aless ?semantically transitive?
point on the transi-tivity continuum (Hopper and Thompson, 1980).Some informants find a perceptible increase in neg-ative sentiment toward inmate when the sentence isphrased as in 5(b):5(b) At the same time, we should never ignore therisks of allowing the inmate to kill someoneagain.5 Computational ApplicationHaving discussed linguistic motivation, empiricalvalidation, and practical approximation of seman-tically relevant features, we now present two stud-ies demonstrating their value in sentiment classifica-tion.
For the first study, we have constructed a newdata set particularly well suited for testing our ap-proach, based on writing about the death penalty.
Inour second study, we make a direct comparison withprior state-of-the-art classification using the BitterLemons corpus of Lin et al (2006).5.1 Predicting Opinions of the Death PenaltyCorpus.
We constructed a new corpus for exper-imentation on implicit sentiment by downloadingthe contents of pro- and anti-death-penalty Websites and manually checking, for a large subset,that the viewpoints expressed in documents were asexpected.
The collection, which we will refer toas the DP corpus, comprises documents from fivepro-death-penalty sites and three anti-death-penaltysites, and the corpus was engineered to have an evenbalance, 596 documents per side.1212Details in Greene (2007).507Frequent bigram baseline.
We adopted a super-vised classification approach based on word n-gramfeatures, using SVM classification in the WEKAmachine learning package.
In initial exploration us-ing both unigrams and bigrams, and using both wordforms and stems, we found that performance did notdiffer significantly, and chose stemmed bigrams forour baseline comparisons.
In order to control for thedifference in the number of features available to theclassifier in our comparisons, we use the N most fre-quent stemmed bigrams as the baseline feature setwhere N is matched to number of OPUS featuresused in the comparison condition.OPUS-kill verbs: OPUS features for manuallyselected verbs.
We created OPUS features for 14verbs ?
those used in Section 3, plus murder, exe-cute, and stab and their nominalizations (includingboth event and -er nominals, e.g.
both killing andkiller) ?
generating N = 1016 distinct features.OPUS-domain: OPUS features for domain-relevant verbs.
We created OPUS features for the117 verbs for which the relative frequency ratiowas greater than 1.
This list includes many of thekill verbs we used in Section 3, and introduces,among others, many transitive verbs describing actsof physical force (e.g.
rape, rob, steal, beat, strike,force, fight) as well as domain-relevant verbs suchas testify, convict, and sentence.
Included verbs nearthe borderline included, for example, hold, watch,allow, and try.
Extracting OPUS features for theseverbs yielded N = 7552 features.Evaluation.
Cross-validation at the documentlevel does not test what we are interested in, sincea classifier might well learn to bucket documents ac-cording to Web site, not according to pro- or anti-death-penalty sentiment.
To avoid this difficulty, weperformed site-wise cross-validation.
We restrictedour attention to the two sites from each perspec-tive with the most documents, which we refer to aspro1, pro2, anti1, and anti2, yielding 4-fold cross-validation.
Each fold ftrain,test is defined as con-taining all documents from one pro and one anti sitefor training, using all documents from the remain-ing pro and anti sites for testing.
So, for exam-ple, fold f11,22 uses all documents from pro1 andanti1 in training, and all documents from pro2 andCondition N features SVM accuracyBaseline 1016 68.37OPUS-kill verbs 1016 82.09Baseline 7552 71.96OPUS-domain 7552 88.10Table 1: Results for 4-fold site-wise cross-validation us-ing the DP corpusCondition N features SVM accuracyBaseline 1518 55.95OPUS-frequent verbs 1518 55.95OPUS-kill verbs 1062 66.67Table 2: DP corpus comparison for OPUS features basedon frequent vs. domain-relevant verbsanti2 for testing.13 As Table 1 shows, OPUS fea-tures provide substantial and statistically significantgains (p < .001).As a reality check to verify that it is domain-relevant verb usages and the encoding of events theyembody that truly drives improved classification, weextracted OPUS features for the 14 most frequentverbs found in the DP Corpus that were not in ourmanually created list of kill verbs, along with theirnominalizations.
Table 2 shows the results of a clas-sification experiment using a single train-test split,training on 1062 documents from pro1, pro2, anti1,anti2 and testing on 84 test documents from the sig-nificantly smaller remaining sites.
Using OPUSfeatures for the most frequent non-kill verbs failsto beat the baseline, establishing that it is not sim-ply term frequency, the presence of particular gram-matical relations, or a larger feature set that the kill-verb OPUS model was able to exploit, but rather theproperties of event encodings involving the kill verbsthemselves.5.2 Predicting Points of View in theIsraeli-Palestinian ConflictIn order to make a direct comparison here with priorstate-of-the-art work on sentiment analysis, we re-port on sentiment classification using OPUS featuresin experiments using a publicly available corpus in-volving opposing perspectives, the Bitter Lemons13Site (# of documents): pro1= clarkprosecutor.org (437),pro2= prodeathpenalty.com (117), anti1= deathpenaltyinfo.org(319), anti2= nodeathpenalty.org (212)508(hence BL) corpus introduced by Lin et al (2006).Corpus.
The Bitter Lemons corpus comprises es-says posted at www.bitterlemons.org, which,in the words of the site, ?present Israeli and Pales-tinian viewpoints on prominent issues of concern?.As a corpus, it has a number of interesting proper-ties.
First, its topic area is one of significant interestand considerable controversy, yet the general tenorof the web site is one that eschews an overly shrillor extreme style of writing.
Second, the site is orga-nized in terms of issue-focused weekly editions thatinclude essays with contrasting viewpoints from thesite?s two editors, plus two essays, also contrasting,from guest editors.
This creates a natural balance be-tween the two sides and across the subtopics beingdiscussed.
The BL corpus as prepared by Lin et alcontains 297 documents from each of the Israeli andPalestinian viewpoints, averaging 700-800 words inlength.Lin et al classifiers.
Lin et al report results ondistinguishing Israeli vs. Palestinian perspectivesusing an SVM classifier, a naive Bayes classifierNB-M using maximum a posteriori estimation, and anaive Bayes classifier NB-B using full Bayesian in-ference.
(Document perspectives are labeled clearlyon the site.)
We continue to use the WEKA SVMclassifier, but compare our results to both their SVMand NB-B, since the latter achieved their best results.OPUS features.
As in Section 5.1, we experi-mented with OPUS features driven by automati-cally extracted lists of domain-relevant verbs.
Forthese experiments, we included domain-relevantnouns, and we varied a threshold ?
for the rela-tive frequency ratio, including only terms for whichlog(R(t)) > ?.
In addition, we introduced a gen-eral filter on OPUS features, eliminating syntacticdependency types that do not usefully reflect seman-tically relevant properties: det, predet, preconj, prt,aux, auxpas, cc, punct, complm, mark, rel, ref, expl.Evaluation.
Lin et al describe two test scenar-ios.
In the first, referred to as Test Scenario 1, theytrained on documents written by the site?s guests,and tested on documents from the site?s editors.
TestScenario 2 represents the reverse, training on docu-ments from the site editors and testing on documentsClassification Accuracy, BL CorpusTest Scenario 1 (GeneralFilter)024681012Individual Experiment (?
values and accuracy)TermThreshold(?
)8486889092949698Percent Correct ?
(Verb)?
(Noun)OPUSLin 2006 NB-BLin 2006 SVMClassification Accuracy, BL CorpusTest Scenario 2 (GeneralFilter)024681012Individual Experiment (?
values and accuracy)TermThreshold(?
)6570758085Percent Correct ?
(Verb)?
(Noun)OPUSLin 2006 NB-BLin 2006 SVMFigure 1: Results on the Bitter Lemons corpusfrom guest authors.
As in our site-wise cross vali-dation for the DP corpus, this strategy ensures thatwhat is being tested is classification according to theviewpoint, not author or topic.Figure 1 (top) summarizes a large set of experi-ments for Test Scenario 1, in which we varied thevalues of ?
for verbs and nouns.
Each experiment,using a particular ??
(verbs), ?
(nouns)?, correspondsto a vertical strip on the x-axis.
The points on thatstrip include the ?
values for verbs and nouns, mea-sured by the scale on the y-axis at the left of thefigure; the accuracy of Lin et al?s SVM (88.22% ac-curacy, constant across all our variations); the accu-racy of Lin et al?s NB-B classifier (93.46% accu-racy, constant across all our variations), and the ac-curacy of our SVM classifier using OPUS features,which varies depending on the ?
values.
Across 423experiments, our average accuracy is 95.41%, withthe best accuracy achieved being 97.64%.
Our clas-sifier underperformed NB-B slightly, with accura-cies from 92.93% to 93.27%, in just 8 of the 423experiments.Figure 1 (bottom) provides a similar summary for509experiments in Test Scenario 2.
The first thing to no-tice is that accuracy for all methods is lower than forTest Scenario 1.
This is not terribly surprising: it islikely that training a classifier on the more uniformauthorship of the editor documents builds a modelthat generalizes less well to the more diverse au-thorship of the guest documents (though accuracyis still quite high).
In addition, the editor-authoreddocuments comprise a smaller training set, consist-ing of 7,899 sentences, while the guest documentshave a total of 11,033 sentences, a 28% difference.In scenario 2, we obtain average accuracy across ex-periments of 83.12%, with a maximum of 85.86%,in this case outperforming the 81.48% obtained byLin?s SVM fairly consistently, and in some cases ap-proaching or matching NB-B at 85.85%.6 Related WorkPang and Lee?s (2008) excellent monograph pro-vides a thorough, well organized, and relatively re-cent description of computational work on senti-ment, opinion, and subjectivity analysis.The problem of classifying underlying sentimentin statements that are not overtly subjective is lessstudied within the NLP literature, but it has receivedsome attention in other fields.
These include, for ex-ample, research on content analysis in journalism,media studies, and political economy (Gentzkowand Shapiro, 2006a; Gentzkow and Shapiro, 2006b;Groseclose and Milyo, 2005; Fader et al, 2007); au-tomatic identification of customer attitudes for busi-ness e-mail routing (Durbin et al, 2003).
And, ofcourse, the study of perceptions in politics and me-dia bears a strong family resemblance to real-worldmarketing problems involving reputation manage-ment and business intelligence (Glance et al, 2005).Within computational linguistics, what we callimplicit sentiment was introduced as a topic of studyby Lin et al (2006) under the rubric of identifyingperspective, though similar work had begun earlierin the realm of political science (e.g.
(Laver et al,2003)).
Other recent work focusing on the notion ofperspective or ideology has been reported by Martinand Vanberg (2008) and Mullen and Malouf (2008).Among prior authors, Gamon?s (2004) research isperhaps closest to the work described here, in thathe uses some features based on a sentence?s logicalform, generated using a proprietary system.
How-ever, his features are templatic in nature in that theydo not couple specific lexical entries with their logi-cal form.
Hearst (1992) and Mulder et al (2004) de-scribe systems that make use of argument structurefeatures coupled with lexical information, thoughneither provides implementation details or experi-mental results.In terms of computational experimentation, workby Thomas et al (2006), predicting yes and novotes in corpus of United States Congressional floordebate speeches, is quite relevant.
They combinedSVM classification with a min-cut model on graphsin order to exploit both direct textual evidence andconstraints suggested by the structure of Congres-sional debates, e.g.
the fact that the same individ-ual rarely gives one speech in favor of a bill and an-other opposing it.
We have extend their method touse OPUS features in the SVM and obtained signifi-cant improvements over their classification accuracy(Greene, 2007; Greene and Resnik, in preparation).7 ConclusionsIn this paper we have introduced an approach toimplicit sentiment motivated by theoretical work inlexical semantics, presenting evidence for the role ofsemantic properties in human sentiment judgments.This research is, to our knowledge, the first to drawan explicit and empirically supported connection be-tween theoretically motivated work in lexical se-mantics and readers?
perception of sentiment.
In ad-dition, we have reported positive sentiment classifi-cation results within a standard supervised learningsetting, employing a practical first approximation tothose semantic properties, including positive resultsin a direct comparison with the previous state of theart.Because we computed OPUS features for opin-ionated as well as non-evaluative language in ourcorpora, obtaining overall positive results, we be-lieve these features may also improve conventionalopinion labeling for subjective text.
This will be in-vestigated in future work.AcknowledgmentsThe authors gratefully acknowledge useful discus-sions with Don Hindle and Chip Denman.510ReferencesJohn Broder.
2007.
Familiar fallback for officials: ?mis-takes were made?.
New York Times.
March 14.F.
J. Damerau.
1993.
Generating and evaluating domain-oriented multi-word terms from texts.
InformationProcessing and Management, 29:433?447.Hoa Trang Dang, Karin Kipper, Martha Palmer, andJoseph Rosenzweig.
1998.
Investigating Regu-lar Sense Extensions Based on Intersective LevinClasses.
In ACL/COLING 98, pages 293?299, Mon-treal, Canada, August 10?14.Bonnie J. Dorr.
1993.
Machine Translation: A View fromthe Lexicon.
The MIT Press, Cambridge, MA.David Dowty.
1991.
Thematic Proto-Roles and Argu-ment Selection.
Language, 67:547?619.S.
D. Durbin, J. N. Richter, and D. Warner.
2003.
A sys-tem for affective rating of texts.
In Proc.
3rd Workshopon Operational Text Classification, KDD-2003.Robert M. Entman.
1993.
Framing: Toward clarificationof a fractured paradigm.
Journal of Communication,43(4):51?58.Anthony Fader, Dragomir R. Radev, Michael H. Crespin,Burt L. Monroe, Kevin M. Quinn, and Michael Co-laresi.
2007.
MavenRank: Identifying influentialmembers of the US Senate using lexical centrality.
InProceedings of the Conference on Empirical Methodsin Natural Language Processing (EMNLP).Michael Gamon.
2004.
Sentiment classification on cus-tomer feedback data: noisy data, large feature vectors,and the role of linguistic analysis.
In Proc.
COLING.M.
Gentzkow and J. Shapiro.
2006a.
Media bias andreputation.
Journal of Political Economy, 114:280?316.M.
Gentzkow and J. Shapiro.
2006b.
What drivesmedia slant?
Evidence from U.S. newspapers.http://ssrn.com/abstract=947640.Natalie Glance, Matthew Hurst, Kamal Nigam, MatthewSiegler, Robert Stockton, and Takashi Tomokiyo.2005.
Deriving marketing intelligence from onlinediscussion.
In Proc.
KDD?05, pages 419?428, NewYork, NY, USA.
ACM.Stephan Greene.
2007.
Spin: Lexical Semantics, Tran-sitivity, and the Identification of Implicit Sentiment.Ph.D.
thesis, University of Maryland.T.
Groseclose and J. Milyo.
2005.
A measure of mediabias.
The Quarterly Journal of Economics, 120:1191?1237.B.
Harden.
2006.
On Puget Sound, It?s Orca vs. Inc. TheWashington Post.
July 26, page A3.Marti Hearst.
1992.
Direction-based text interpretationas an information access refinement.
In Paul Jacobs,editor, Text-Based Intelligent Systems, pages 257?274.Lawrence Erlbaum Associates.Paul Hopper and Sandra Thompson.
1980.
Transitivityin Grammar and Discourse.
Language, 56:251?295.E.
Kako.
2006.
Thematic role properties of subjects andobjects.
Cognition, 101(1):1?42, August.Michael Laver, Kenneth Benoit, and John Garry.
2003.Extracting policy positions from political texts usingwords as data.
American Political Science Review,97(2):311?331.M.
Lemmens.
1998.
Lexical perspectives on transitivityand ergativity.
John Benjamins.Beth Levin and Malka Rappaport Hovav.
2005.
Argu-ment Realization.
Research Surveys in Linguistics.Cambridge University Press, New York.Beth Levin.
1993.
English Verb Classes and Alter-nations: A Preliminary Investigation.
University ofChicago Press, Chicago, IL.Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, andAlexander Hauptmann.
2006.
Which side are you on?identifying perspectives at the document and sentencelevels.
In Proceedings of the Conference on NaturalLanguage Learning (CoNLL).Lanny W. Martin and Georg Vanberg.
2008.
A ro-bust transformation procedure for interpreting politicaltext.
Political Analysis, 16(1):93?100.G.
McKoon and T. MacFarland.
2000.
Externally andinternally caused change of state verbs.
Language,pages 833?858.M.
Mulder, A. Nijholt, M. den Uyl, and P. Terpstra.2004.
A lexical grammatical implementation of affect.In Proc.
TSD-04, Lecture notes in computer science3206, pages 171?178).
Springer-Verlag.Tony Mullen and Robert Malouf.
2008.
Taking sides:User classification for informal online political dis-course.
Internet Research, 18:177?190.Mari Broman Olsen and Philip Resnik.
1997.
ImplicitObject Constructions and the (In)transitivity Contin-uum.
In 33rd Proceedings of the Chicago LinguisticSociety, pages 327?336.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and Trends in Infor-mation Retrieval, 2(1-2):1?135.James Pustejovsky.
1991.
The Generative Lexicon.Computational Linguistics, 17(4):409?441.Philip J.
Stone.
1966.
The General Inquirer: A Com-puter Approach to Content Analysis.
The MIT Press.Matt Thomas, Bo Pang, and Lillian Lee.
2006.
Getout the vote: Determining support or oppositionfrom Congressional floor-debate transcripts.
In Proc.EMNLP, pages 327?335.Zhibao Wu and Martha Palmer.
1994.
Verb Semanticsand Lexical Selection.
In Proc.
ACL, pages 133?138,Las Cruces, New Mexico.511
