STRUCTURAL AMBIGUITY  AND LEX ICAL  RELAT IONSDonald Hindle and Mats RoothAT&T Bell Laboratories600 Mountain AvenueMurray Hill, NJ 07974AbstractWe propose that ambiguous prepositional phraseattachment can be resolved on the basis of therelative strength of association of the prepositionwith noun and verb, estimated on the basis of worddistribution in a large corpus.
This work suggeststhat a distributional approach can be effective inresolving parsing problems that apparently call forcomplex reasoning.In t roduct ionPrepositional phrase attachment is the canonicalcase of structural ambiguity, as in the time wornexample,(1) I saw the man with the telescopeThe existence of such ambiguity raises problemsfor understanding and for language models.
Itlooks like it might require xtremely complex com-putation to determine what attaches to what.
In-deed, one recent proposal suggests that resolvingattachment ambiguity requires the construction ofa discourse model in which the entities referred toin a text must be reasoned about (Altmann andSteedman 1988).
Of course, if attachment am-biguity demands reference to semantics and dis-course models, there is little hope in the near termof building computational models for unrestrictedtext to resolve the ambiguity.Structure based ambiguity resolutionThere have been several structure-based proposalsabout ambiguity resolution in the literature; theyare particularly attractive because they are simpleand don't demand calculations in the semantic ordiscourse domains.
The two main ones are:?
Right Association - a constituent tends to at-tach to another constituent immediately to itsright (Kimball 1973).?
Minimal Attachment - a constituent tends toattach so as to involve the fewest additionalsyntactic nodes (Frazier 1978).For the particular case we are concerned with,attachment of a prepositional phrase in a verb +object context as in sentence (1), these two princi-ples - at least in the version of syntax that Frazierassumes - make opposite predictions: Right Asso-ciation predicts noun attachment, while MinimalAttachment predicts verb attachment.Psycholinguistic work on structure-based strate-gies is primarily concerned with modeling the timecourse of parsing and disambiguation, and propo-nents of this approach explicitly acknowledge thatother information enters into determining a finalparse.
Still, one can ask what information is rel-evant to determining a final parse, and it seemsthat in this domain structure-based disambigua-tion is not a very good predictor.
A recent studyof attachment of prepositional phrases in a sam-ple of written responses to a "Wizard of Oz" travelinformation experiment shows that neither RightAssociation or Minimal Attachment account formore than 55% of the cases (Whittemore t al.1990).
And experiments by Taraban and McClel-land (1988) show that the structural models arenot in fact good predictors of people's behavior inresolving ambiguity.Resolving ambiguity through lexicalassociationsWhittemore t al.
(1990) found lexical preferencesto be the key to resolving attachment ambiguity.Similarly, Taraban and McClelland found lexicalcontent was key in explaining people's behavior.Various previous proposals for guiding attachmentdisambiguation by the lexical content of specific229words have appeared (e.g.
Ford, Bresnan, and Ka-plan 1982; Marcus 1980).
Unfortunately, it is notclear where the necessary information about lexi-cal preferences i  to be found.
In the Whittemoreet al study, the judgement of attachment pref-erences had to be made by hand for exactly thecases that their study covered; no precompiled listof lexical preferences was available.
Thus, we areposed with the problem: how can we get a goodlist of lexical preferences.Our proposal is to use cooccurrence of withprepositions in text as an indicator of lexical pref-erence.
Thus, for example, the preposition to oc-curs frequently in the context send NP --, i.e.,after the object of the verb send, and this is evi-dence of a lexical association of the verb send withto.
Similarly, from occurs frequently in the contextwithdrawal --, and this is evidence of a lexical as-sociation of the noun withdrawal with the prepo-sition from.
Of course, this kind of associationis, unlike lexical selection, a symmetric notion.Cooccurrence provides no indication of whetherthe verb is selecting the preposition or vice versa.We will treat the association as a property of thepair of words.
It is a separate matter, which weunfortunately cannot pursue here, to assign theassociation to a particular linguistic licensing re-lation.
The suggestion which we want to exploreis that the association revealed by textual distri-bution - whether its source is a complementationrelation, a modification relation, or something else- gives us information eeded to resolve the prepo-sitional attachment.Discovering Lexical Associa-tion in TextA 13 million word sample of Associated Press newstories from 1989 were automatically parsed bythe Fidditch parser (Hindle 1983), using Church'spart of speech analyzer as a preprocessor (Church1988).
From the syntactic analysis provided bythe parser for each sentence, we extracted a tablecontaining all the heads of all noun phrases.
Foreach noun phrase head, we recorded the follow-ing preposition if any occurred (ignoring whetheror  not the parser attached the preposition to thenoun phrase), and the preceding verb if the nounphrase was the object of that verb.
Thus, we gen-erated a table with entries including those shownin Table 1.In Table 1, example (a) represents a passivizedinstance of the verb blame followed by the prepo-VERBblamecontrolenragesparegrantdetermineHEAD NOUNPASSIVEmoneydevelopmentgovernmentmilitaryaccordradicalWHPROitconcessionflawTable h A sample of the Verb-Noun-Prepositiontable.sition for.
Example (b) is an instance of a nounphrase whose head is money; this noun phraseis not an object of any verb, but is followed bythe preposition for.
Example (c) represents an in-stance of a noun phrase with head noun develop-ment which neither has a following preposition oris the object of a verb.
Example (d) is an instanceof a noun phrase with head government, which isthe object of the verb control but is followed by nopreposition.
Example (j) represents an instance ofthe ambiguity we are concerned with resolving: anoun phrase (head is concession), which is the ob-ject of a verb (grant), followed by a preposition(to).From the 13 million word sample, 2,661,872noun phrases were identified.
Of these, 467,920were recognized as the object of a verb, and753,843 were followed by a preposition.
Of thenoun phrase objects identified, 223,666 were am-biguous verb-noun-preposition triples.Estimating attachment prefer-encesOf course, the table of verbs, nouns and preposi-tions does not directly tell us what the strengthlexical associations are.
There are three potentialsources of noise in the model.
First, the parser insome cases gives us false analyses.
Second, whena preposition follows a noun phrase (or verb), itmay or may not be structurally related to thatnoun phrase (or verb).
(In our terms, it may at-tach to that noun phrase or it may attach some-where else).
And finally, even if we get accu-rate attachment information, it may be that fre-230quency of cooccurrence is not a good indication ofstrength of attachment.
We will proceed to buildthe model of lexical association strength, aware ofthese sources of noise.We want to use the verb-noun-preposition tableto derive a table of bigrams, where the first term isa noun or verb, and the second term is an associ-ated preposition (or no preposition).
To do this weneed to try to assign each preposition that occurseither to the noun or to the verb that it occurswith.
In some cases it is fairly certain that thepreposition attaches to the noun or the verb; inother cases, it is far less certain.
Our approach isto assign the clear cases first, then to use these todecide the unclear cases that can be decided, andfinally to arbitrarily assign the remaining cases.The procedure for assigning prepositions in oursample to noun or verb is as follows:1.
No Preposition - if there is no preposition, thenoun or verb is simply counted with the nullpreposition.
(cases (c-h) in Table 1).2.
Sure Verb Attach 1 - preposition is attachedto the verb if the noun phrase head is a pro-noun.
(i in Table 1)3.
Sure Verb Attach 2 - preposition is attachedto the verb if the verb is passivized (unlessthe preposition is by.
The instances of by fol-lowing a passive verb were left unassigned.
)(a in Table 1)4.
Sure Noun Attach - preposition is attached tothe noun, if the noun phrase occurs in a con-text where no verb could license the preposi-tional phrase (i.e., the noun phrase is in sub-ject or pre-verbal position.)
(b, if pre-verbal)5.
Ambiguous Attach 1 - Using the table of at-tachment so far, if a t-score for the ambiguity(see below) is greater than 2.1 or less than-2.1, then assign the preposition according tothe t-score.
Iterate through the ambiguoustriples until all such attachments are done.
(jand k may be assigned)6.
Ambiguous Attach 2 - for the remaining am-biguous triples, split the attachment betweenthe noun and the verb, assigning .5 to thenoun and .5 to the verb.
(j and k may beassigned)7.
Unsure Attach - for the remaining pairs (allof which are either attached to the precedingnoun or to some unknown element), assignthem to the noun.
(b, if following a verb)This procedure gives us a table of bigrams rep-resenting our guess about what prepositions asso-ciate with what nouns or verbs, made on the basisof the distribution of verbs nouns and prepositionsin our corpus.The procedure  fo r  guess ing  a t tach-mentGiven the table of bigrams, derived as describedabove, we can define a simple procedure for de-termining the attachment for an instance of verb-noun-preposition ambiguity.
Consider the exam-ple of sentence (2), where we have to choose theattachment given verb send, noun soldier, andpreposition into.
(2) Moscow sent more than 100,000 sol-diers into Afganistan .
.
.The idea is to contrast the probability withwhich into occurs with the noun soldier (P(into\[ soldier)) with the probability with which intooccurs with the verb send (P(into \[ send)).
A t-score is an appropriate way to make this contrast(see Church et al to appear).
In general, we wantto calculate the contrast between the conditionalprobability of seeing a particular preposition givena noun with the conditional probability of seeingthat preposition given a verb.P(prep \[ noun) - P(prep \[ verb)t=~/a2(P(prep I noun)) + ~2(e(prep I verb))We use the "Expected Likelihood Estimate"(Church et al, to appear) to estimate the prob-abilities, in order to adjust for small frequencies;that is, given a noun and verb, we simply add 1/2to all bigram frequency counts involving a prepo-sition that occurs with either the noun or the verb,and then recompute the unigram frequencies.
Thismethod leaves the order of t-scores nearly intact,though their magnitude is inflated by about 30%.To compensate for this, the 1.65 threshold for sig-nificance at the 95% level should be adjusted upto about 2.15.Consider how we determine attachment for sen-tence (2).
We use a t-score derived from the ad-justed frequencies in our corpus to decide whetherthe prepositional phrase into Afganistan is at-tached to the verb (root) send/V or to the noun(root) soldier/N.
In our corpus, soldier/N has anadjusted frequency of 1488.5, and send/V has anadjusted frequency of 1706.5; soldier/N occurredin 32 distinct preposition contexts, and send/Via23160 distinct preposition contexts; f(send/V into) =84, f(soidier/N into) = 1.5.From this we calculate the t-score as follows: 1t -P(wlsoldier/ N ) - P(wlsend/ V)~/a2(P(wlsoidier/N)) + c~2(P(wlsend/ V))l(soldier/N into)+ll2 .f(send/V into)+l/2f(soidierlN)+V/2 - -  /(send/V)+V/2\ / / ( ,o ld ie r /N  into)+l/2 /(send/V into)+l\[2 (f(soldierlN)+V/2)2 + (/(send/V)+V/2)~1.s+1/2 84+1/2-.. 1488.5+70/2 -  1706.5-t-70/2 ~, - -8 .811.5+i/2 84+i/21488.5+70/2p -I- 1706.s+70/2)2This figure of-8.81 represents a significant asso-ciation of the preposition into with the verb send,and on this basis, the procedure would (correctly)decide that into should attach to send rather thanto soldier.
Of the 84 send/V into bigrams, 10 wereassigned by steps 2 and 3 ('sure attachements').Test ing At tachment  Prefer-enceTo evaluate the performance of this procedure,first the two authors graded a set of verb-noun-preposition triples as follows.
From the AP newstories, we randomly selected 1000 test sentencesin which the parser identified an ambiguous verb-noun-preposition triple.
(These sentences were se-lected from stories included in the 13 million wordsample, but the particular sentences were excludedfrom the calculation of lexical associations.)
Forevery such tr iple,  each author made a judgementof the correct attachment on the basis of the threewords alone (forced choice - preposition attachesto noun or verb).
This task is in essence the onethat we will give the computer - i.e., to judge theattachment without any more information thanthe preposition and the head of the two possibleattachment sites, the noun and the verb.
Thisgave us two sets of judgements to compare the al-gorithm's performance to.a V is the number of distinct preposition contexts foreither soldier/N or send/V; in this c~se V = 70.
Since70 bigram frequencies f(soldier/N p) are incremented by1/2, the unigram frequency for soldier/N is incrementedby 70/2.J udg ing  cor rect  a t tachmentWe also wanted a standard of correctness for thesetest sentences.
To derive this standard, we to-gether judged the attachment for the 1000 triplesa second time, this time using the full sentencecontext.It turned out to be a surprisingly difficult taskto assign attachment preferences for the test sam-ple.
Of course, many decisions were straightfor-ward; sometimes it is clear that a prepositionalphrase is and argument of a noun or verb.
Butmore than 10% of the sentences seemed problem-atic to at least one author.
There are several kindsof constructions where the attachment decision isnot clear theoretically.
These include idioms (3-4),light verb constructions (5), small clauses (6).
(3) But over t ime, misery has given wayto mending.
(4) The meeting will take place in Quan-rico(5) Bush has said he would not make cutsin Social Security(6) Sides said Francke kept a .38-caliberrevolver in his car 's  glove compartmentWe chose always to assign light verb construc-tions to noun attachment and small clauses to verbattachment.Another source of difficulty arose from caseswhere there seemed to be a systematic ambiguityin attachment.
(7) .
.
.known to frequent he same barsin one neighborhood.
(8) Inaugural officials reportedly weretrying to arrange a reunion for Bush andhis old submarine buddies .
.
.
(9) We have not signed a settlementagreement with themSentence (7) shows a systematic locative am-biguity: if you frequent a bar and the bar is ina place, the frequenting event is arguably in thesame place.
Sentence (8) shows a systematic bene-factive ambiguity: if you arrange something forsomeone, then the thing arranged is also for them.The ambiguity in (9) arises from the fact that ifsomeone is one of the joint agents in the signing ofan agreement, hat person is likely to be a partyto the agreement.
In general, we call an attach-ment systematically ambiguous when, given ourunderstanding of the semantics, situations which232make the interpretation ofone of the attachmentstrue always (or at least usually) also validate theinterpretation of the other attachment.It seems to us that this difficulty in assigningattachment decisions is an important fact that de-serves further exploration.
If it is difficult to de-cide what licenses a prepositional phrase a signif-icant proportion of the time, then we need to de-velop language models that appropriately capturethis vagueness.
For our present purpose, we de-cided to force an attachment choice in all cases, insome cases making the choice on the bases of anunanalyzed intuition.In addition to the problematic ases, a sig-nificant number (120) of the 1000 triples identi-fied automatically as instances of the verb-object-preposition configuration turned out in fact tobe other constructions.
These misidentificationswere mostly due to parsing errors, and in partdue to our underspecifying for the parser exactlywhat configuration to identify.
Examples of thesemisidentifications include: identifying the subjectof the complement clause of say as its object,as in (10), which was identified as (say minis-ters from); misparsing two constituents as a singleobject noun phrase, as in (11), which was identi-fied as (make subject o); and counting non-objectnoun phrases as the object as in (12), identified as(get hell out_oJ).
(10) Ortega also said deputy foreign min-isters from the five governments wouldmeet Tuesday in Managua .
.
.
.
(11) Congress made a deliberate choiceto make this commission subject to theopen meeting requirements .
.
.
(12) Student Union, get the hell out ofChina!Of course these errors are folded into the calcu-lation of associations.
No doubt our bigram modelwould be better if we could eliminate these items,but many of them represent parsing errors thatcannot readily be identified by the parser, so weproceed with these errors included in the bigrams.After agreeing on the 'correct' attachment forthe sample of 1000 triples, we are left with 880verb-noun-preposition triples (having discardedthe 120 parsing errors).
Of these, 586 are nounattachments and 294 verb attachments.Evaluating performanceFirst, consider how the simple structural attach-ment preference schemas perform at predicting theJudge 1I i i i i  4.9 iLA 557 323 85.4 65.9 78.3Table 2: Performance on the test sentences for 2human judges and the lexical association proce-dure (LA).outcome in our test set.
Right Association, whichpredicts noun attachment, does better, since inour sample there are more noun attachments, butit still has an error rate of 33%.
Minimal Attach.meat, interpreted to mean verb attachment, hasthe complementary error rate of 67%.
Obviously,neither of these procedures i particularly impres-sive.Now consider the performance of our attach-ment procedure for the 880 standard test sen-tences.
Table 2 shows the performance for thetwo human judges and for the lexical associationattachment procedure.First, we note that the task of judging attach-ment on the basis of verb, noun and prepositionalone is not easy.
The human judges had overallerror rates of 10-15%.
(Of course this is consid-erably better than always choosing noun attach-ment.)
The lexical association procedure basedon t-scores is somewhat worse than the humanjudges, with an error rate of 22%, but this alsois an improvement over simply choosing the near-est attachment site.If we restrict the lexical association procedureto choose attachment only in cases where its con-fidence is greater than about 95% (i.e., where t isgreater than 2.1), we get attachment judgementson 607 of the 880 test sentences, with an overallerror rate of 15% (Table 3).
On these same sen-tences, the human judges also showed slight im-provement.Underlying RelationsOur model takes frequency of cooccurrence as ev-idence of an underlying relationship, but makesno attempt o determine what sort of relationshipis involved.
It is interesting to see what kindsof relationships the model is identifying.
To in-vestigate this we categorized the 880 triples ac-233\[ choice I % correct \]N V N V totalJudge 1 ~Judge 2LATable 3: Performance on the test sentences for 2human judges and the lexical association proce-dure (LA) for test triples where t > 2.1cording to the nature of the relationship underly-ing the attachment.
In many cases, the decisionwas difficult.
Even the argument/adjunct distinc-tion showed many gray cases between clear partici-pants in an action (arguments) and clear temporalmodifiers (adjuncts).
We made rough best guessesto partition the cases into the following categories:argument, adjunct, idiom, small clause, locativeambiguity, systematic ambiguity, light verb.
Withthis set of categories, 84 of the 880 cases remainedso problematic that we assigned them to categoryother.Table 4 shows the performance ofthe lexical at-tachment procedure for these classes of relations.Even granting the roughness of the categorization,some clear patterns emerge.
Our approach isquitesuccessful at attaching arguments correctly; thisrepresents some confirmation that the associationsderived from the AP sample are indeed the kindof associations previous research as suggested arerelevant o determining attachment.
The proce-dure does better on arguments han on adjuncts,and in fact performs rather poorly on adjuncts ofverbs (chiefly time and manner phrases).
The re-maining cases are all hard in some way, and theperformance t nds to be worse on these cases,showing clearly for a more elaborated model.Sense ConflationsThe initial steps of our procedure constructed atable of frequencies with entries f(z,p), where z isa noun or verb root string, and p is a prepositionstring.
These primitives might be too coarse, inthat they do not distinguish different senses of apreposition, noun, or verb.
For instance, the tem-porM use of in in the phrase in December is identi-fied with a locative use in Teheran.
As a result, theprocedure LA necessarily makes the same attach-relation }count \] %correctargument noun 375 88.5argument verb 103 86.4adjunct noun 91 72.5adjunct verb 101 61.3light verb 19 63.1small clause 13 84.6idiom 20 65.0locative ambiguity 37 75.7systematic ambiguity 37 64.8other 84 61.9Table 4: Performance of the Lexical attachmentprocedure by underlying relationshipment prediction for in December and in Teheranoccurring in the same context.
For instance, LAidentifies the tuple reopen embassy in as an NP at-tachment ( -score 5.02).
This is certainly incorrectfor (13), though not for (14).
2(13) Britain reopened the embassy in De-cember(14) Britain reopened its embassy inTeheranSimilarly, the scalar sense of drop exemplified in(15) sponsors a preposition to, while the sense rep-resented in drop the idea does not.
Identifying thetwo senses may be the reason that LA makes noattachment choice for drop resistance to (derivedfrom (16)), where the score is -0.18.
(15) exports are expected to drop a fur-ther 1.5 percent o 810,000(16) persuade Israeli leaders to drop theirresistance to talks with the PLOWe experimented with the first problem by sub-stituting an abstract preposition in ,MONTH forall occurrences of in with a month name as an ob-ject.
While the tuple reopen embassy in~oMONTHwas correctly pushed in the direction of a verb at-tachment (-1.34), in other cases errors were intro-duced, and there was no compelling eneral im-provement in performance.
In tuples of the formdrop/grow/increase p rcent inJ~MONTH , derivedfrom examples uch as (16), the preposition wasincorrectly attached to the noun percent.2(13) is a phrase from our corpus, while (14) is a con-structed example.234(16) Output at mines and oil wellsdropped 1.8 percent in February(17) ,1.8 percent was dropped by outputat mines and oil wellsWe suspect hat this reveals a problem with ourestimation procedure, not for instance a paucityof data.
Part of the problem may be the fact thatadverbial noun phrase headed by percent in (16)does not passivize or pronominalize, so that thereare no sure verb attachment cases directly corre-sponding to these uses of scalar motion verbs.Comparison with a DictionaryThe idea that lexical preference is a key factorin resolving structural ambiguity leads us natu-rally to ask whether existing dictionaries can pro-vide useful information for disambiguation.
Thereare reasons to anticipate difficulties in this re-gard.
Typically, dictionaries have concentratedon the 'interesting' phenomena ofEnglish, tendingto ignore mundane lexical associations.
However,the Collins Cobuild English Language Dictionary(Sinclair et al 1987) seems particularly appro-priate for comparing with the AP sample for sev-eral reasons: it was compiled on the basis of alarge text corpus, and thus may be less subjectto idiosyncrasy than more arbitrarily constructedworks; and it provides, in a separate field, a di-rect indication of prepositions typically associatedwith many nouns and verbs.
Nevertheless, evenfor Cobuild, we expect o find more concentrationon, for example, idioms and closely bound argu-ments, and less attention to the adjunct relationswhich play a significant role in determining attach-ment preferences.From a machine-readable version of the dictio-nary, we extracted a list of 1535 nouns associatedwith a particular preposition, and of 1193 verbsassociated with a particular preposition after anobject noun phrase.
These 2728 associations aremany fewer than the number of associations foundin the AP sample.
(see Table 5.
)Of course, most of the preposition associationpairs from the AP sample end up being non-significant; of the 88,860 pairs, fewer than half(40,869) occur with a frequency greater than 1,and only 8337 have a t-score greater than 1.65.
Soour sample gives about three times as many sig-nificant preposition associations as the COBUILDdictionary.
Note however, as Table 5 shows, theoverlap is remarkably good, considering the largespace of possible bigrams.
(In our bigram tableSource \[COBUILDAP sampleAP sample ( f  > 1)AP sample(t > 1.65)Total I NOUN I VERB272888,86040,8698,337COBUILD n AP 1,931COBUILD N AP 1,040(t > 1.65)1535 119364,629 24,23131,241 9,6286,307 2,0301,147 784656 384Table 5: Count of noun and verb associations forCOBUILD and the AP samplethere are over 20,000 nouns, over 5000 verbs, andover 90 prepositions.)
On the other hand, thelack of overlap for so many cases - assuming thatthe dictionary and the significant bigrams actuallyrecord important preposition associations - indi-cates that 1) our sample is too small, and 2) thedictionary coverage is widely scattered.First, we note that the dictionary chooses at-tachments in 182 cases of the 880 test sentences.Seven of these are cases where the dictionary findsan association between the preposition and boththe noun and the verb.
In these cases, of course,the dictionary provides no information to help inchoosing the correct attachment.Looking at the 175 cases where the dictionaryfinds one and only one association for the preposi-tion, we can ask how well it does in predicting thecorrect attachment.
Here the results are no betterthan our human judges or than our bigram proce-dure.
Of the 175 cases, in 25 cases the dictionaryfinds a verb association when the correct associa-tion is with the noun.
In 3 cases, the dictionaryfinds a noun association when the correct associa-tion is with the verb.
Thus, overall, the dictionaryis 86% correct.It is somewhat unfair to use a dictionary as asource of disambiguation i formation; there is noreason to expect that a dictionary to provide in-formation on all significant associations; it mayrecord only associations that are interesting forsome reason (perhaps because they are semanti-cally unpredictable.)
Table 6 shows a small sampleof verb-preposition associations from the AP sam-235AP sample COBUILDapproachappropriateapproveapproximatearbitratearguearmarraignarrangearrayarrestarrogateascribeaskassassinateassembleassertassignassistassociateabout (4.1)with (2.4)for (2.5)with (2.5)as(3.2)in (2.4)on (4.1)through (5.9)after (3.4)along_with (6.1)during (3.1)on (2.8)while (3.9)about (4.3)in (2.4)at (3.8)over (5.8)to (5.1)in (2.4)with (6.4)fortobetweenwithwithonforinfortotoabouttoinwithwithTable 6: Verb-(NP)-Preposition associations inAP sample and COBUILD.pie and from Cobuild.
The overlap is considerable,but each source of information provides intuitivelyimportant associations that are missing from theother.ConclusionOur attempt to use lexical associations derivedfrom distribution of lexical items in text showspromising results.
Despite the errors in parsingintroduced by automatically analyzing text, weare able to extract a good list of associations withprepositions, overlapping significantly with an ex-isting dictionary.
This information could easily beincorporated into an automatic parser, and addi-tional sorts of lexical associations could similarlybe derived from text.
The particular approach todeciding attachment by t-score gives results nearlyas good as human judges given the same infor-mation.
Thus, we conclude that it may not benecessary to resort to a complete semantics or todiscourse models to resolve many pernicious casesof attachment ambiguity.It is clear however, that the simple model of at-tachment preference that we have proposed, basedonly on the verb, noun and preposition, is tooweak to make correct attachments in many cases.We need to explore ways to enter more complexcalculations into the procedure.ReferencesAltmman, Gerry, and Mark Steedman.
1988.
Interac-tion with context during human sentence process-ing.
Cognition, 30, 191-238.Church, Kenneth W. 1988.
A stochastic parts programand noun phrase parser for unrestricted text,Proceedings of the Second Conference on AppliedNatural Language Processing, Austin, Texas.Church, Kenneth W., William A. Gale, Patrick Hanks,and Donald Hindle.
(to appear).
Using statisticsin lexical analysis, in Zernik (ed.)
Lexical acqui-sition: using on-line resources to build a lexicon.Ford, Marilyn, Joan Bresnan and Ronald M. Kaplan.1982.
A competence based theory of syntactic lo-sure, in Bresnan, J.
(ed.)
The Mental Represen.tation o.f Grammatical Relations.
MIT Press.Frazier, L. 1978.
On comprehending sentences: Syn-tactic parsing strategies.
PhD.
dissertation, Uni-versity of Connecticut.Hindle, Donald.
1983.
User manual for fidditch, adeterministic parser.
Naval Research LaboratoryTechnical Memorandum 7590-142.Kimball, J.
1973.
Seven principles of surface structureparsing in natural anguage, Cognition, 2, 15-47.Marcus, Mitchell P. 1980.
A theory of syntactic recog-nition for natural anguage.
MIT Press.Sinclair, J., P. Hanks, G. Fox, R. Moon, P. Stock, etal.
1987.
Collins Cobuild English Language Dic-tionary.
Collins, London and Glasgow.Taraban, Roman and James L. McClelland.
1988.Constituent attachment and thematic role as-signment in sentence processing: influences ofcontent-based xpectations, Journal of Memoryand Language, 27, 597-632.Whittemore, Greg, Kathleen Ferrara and Hans Brun-net.
1990.
Empirical study of predictive powersof simple attachment schemes for post-modifierprepositional phrases.
Proceedings of the ~8th An-nual Meeting of the Association for Computa-tional Linguistics, 23-30.236
