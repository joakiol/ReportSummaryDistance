ROBUSTNESS,  PORTABILITY,  AND SCALABIL ITYOF NATURAL LANGUAGE SYSTEMSRaSh WeischedelBBN Systems and Technologies70 Fawcett StreetCambridge, lvIA 021381.
OBJECTIVEIn the DoD, every unit, from the smallest to the largest,communicates through messages.
Messages arefundamental in command and conlrol, intelligence analysis,and in planning and replanning.
Our objective is to createalgorithms that will1) robustly process open source text,identifying relevant messages, and updating adata base based on the relevant messages;2) reduce the effort required in porting naturallanguage (NL) message processing softwareto a new domain from months to weeks; and3) be scalable to broad domains withvocabularies of tens of thousands ofwords.2.
APPROACHOur approach is to apply probabilistic language models andtraining over large corpora in all phases of natural languageprocessing.
This new approach will enable systems toadapt o both new task domains and linguistic expressionsnot seen before by semi-automatically acquiring 1) adomain model, 2) facts required for semantic processing, 3)grammar ules, 4) information about new words, 5)probability models on frequency of occurrence, and 6) rulesfor mapping from representation t  application structure.For instance, a Statistical model of categories ofwords willenable systems to predict he most likely category of aword never encountered bythe system before and to focuson its most likely interpretation i  context, rather thanskipping the word or considering all possibleinterpretations.
Markov modelling techniques will be usedfor this problem.In an analogous way, statistical models of language will bedeveloped and applied at the level of syntax (form), at thelevel of semantics (conten0, and at the contextual level(meaning and impact).3.
RECENT RESULTS?
Consistently achieved high performance in Government-sponsored evaluations (e.g., MUC-3, MUC-4, etc.)
of damextraction systems with significantly less human effort toport the PLUM system to each domain, compared with theeffort reported inporting other high-performing systems.?
Sped up the PLUM data extraction system by a factor ofthree.?
Ported PLUM to a microelectronics domain with onlyseven person weeks of effort.
(Typically, systems areported to a new domain in half a person year or more.)?
Developed a probabilistic model of answer correctnesswhich requires only a set of articles and correct output (thedata that should be extracted for each article) as training.This can be used as a model of confidence or certainty oneach data item extracted by the system from text.?
Successfully applied a statistical text classificationalgorithm in MUC-4.
The algorithm is trainedautomatically from examples of relevant and irrelevanttexts.
The user can specify the degree of certainty desired.?
Distributed POST, our software for statistically labellingwords in text, to several other DARPA contractors (NewMexico State University, New York University, SyracuseUniversity, and the University of Chicago).4.
PLANS FOR THE COMING YEARCreate aptobabilistic model for predicting the most likely(partial) interpretation f an input, whether well-formed,novel, complex, or ill-formed.Develop rocedures for automatically earning template fillrules from examples.Participate inMUC-5 evaluation i all domains.386
