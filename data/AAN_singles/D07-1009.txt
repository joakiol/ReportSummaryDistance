Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
83?91, Prague, June 2007. c?2007 Association for Computational LinguisticsIncremental Text Structuring with Online Hierarchical RankingErdong Chen, Benjamin Snyder and Regina BarzilayComputer Science and Artificial Intelligence LaboratoryMassachusetts Institute of Technology{edc,bsnyder,regina}@csail.mit.eduAbstractMany emerging applications require doc-uments to be repeatedly updated.
Suchdocuments include newsfeeds, webpages,and shared community resources such asWikipedia.
In this paper we address thetask of inserting new information into exist-ing texts.
In particular, we wish to deter-mine the best location in a text for a givenpiece of new information.
For this processto succeed, the insertion algorithm shouldbe informed by the existing document struc-ture.
Lengthy real-world texts are often hier-archically organized into chapters, sections,and paragraphs.
We present an online rank-ing model which exploits this hierarchicalstructure ?
representationally in its featuresand algorithmically in its learning proce-dure.
When tested on a corpus of Wikipediaarticles, our hierarchically informed modelpredicts the correct insertion paragraph moreaccurately than baseline methods.1 IntroductionMany emerging applications require documents tobe repeatedly updated.
For instance, newsfeed ar-ticles are continuously revised by editors as new in-formation emerges, and personal webpages are mod-ified as the status of the individual changes.
This re-vision strategy has become even more prevalent withthe advent of community edited web resources, themost notable example being Wikipedia.
At presentthis process involves massive human effort.
For in-stance, the English language version of Wikipediaaveraged over 3 million edits1 per month in 2006.Even so, many articles quickly become outdated.A system that performs such updates automaticallycould drastically decrease maintenance efforts andpotentially improve document quality.Currently there is no effective way to automati-cally update documents as new information becomesavailable.
The closest relevant text structuring tech-nique is the work on sentence ordering, in which acomplete reordering of the text is undertaken.
Pre-dictably these methods are suboptimal for this newtask because they cannot take advantage of existingtext structure.We introduce an alternative vision of text struc-turing as a process unfolding over time.
Instead ofordering sentences all at once, we start with a well-formed draft and add new information at each stage,while preserving document coherence.
The basicoperation of incremental text structuring is the inser-tion of new information.
To automate this process,we develop a method for determining the best loca-tion in a text for a given piece of new information.The main challenge is to maintain the continuityand coherence of the original text.
These proper-ties may be maintained by examining sentences ad-jacent to each potential insertion point.
However, alocal sentence comparison method such as this mayfail to account for global document coherence (e.g.by allowing the mention of some fact in an inappro-priate section).
This problem is especially acute inthe case of lengthy, real-world texts such as books,technical reports, and web pages.
These documents1http://stats.wikimedia.org/EN/TablesWikipediaEN.htm83are commonly organized hierarchically into sectionsand paragraphs to aid reader comprehension.
Fordocuments where hierarchical information is not ex-plicitly provided, such as automatic speech tran-scripts, we can use automatic segmentation methodsto induce such a structure (Hearst, 1994).
Ratherthan ignoring the inherent hierarchical structure ofthese texts, we desire to directly model such hierar-chies and use them to our advantage ?
both repre-sentationally in our features and algorithmically inour learning procedure.To achieve this goal, we introduce a novel methodfor sentence insertion that operates over a hierarchi-cal structure.
Our document representation includesfeatures for each layer of the hierarchy.
For ex-ample, the word overlap between the inserted sen-tence and a section header would be included as anupper-level section feature, whereas a comparisonof the sentence with all the words in a paragraphwould be a lower-level paragraph feature.
We pro-pose a linear model which simultaneously considersthe features of every layer when making insertiondecisions.
We develop a novel update mechanismin the online learning framework which exploits thehierarchical decomposition of features.
This mecha-nism limits model updates to those features found atthe highest incorrectly predicted layer, without un-necessarily disturbing the parameter values for thelower reaches of the tree.
This conservative updateapproach maintains as much knowledge as possiblefrom previously encountered training examples.We evaluate our method using real-world datawhere multiple authors have revised preexisting doc-uments over time.
We obtain such a corpus fromWikipedia articles,2 which are continuously updatedby multiple authors.
Logs of these updates are pub-licly available, and are used for training and testingof our algorithm.
Figure 1 shows an example of aWikipedia insertion.
We believe this data will moreclosely mirror potential applications than syntheticcollections used in previous work on text structur-ing.Our hierarchical training method yields signifi-cant improvement when compared to a similar non-hierarchical model which instead uses the standard2Data and code used in this paper are available athttp://people.csail.mit.edu/edc/emnlp07/perceptron update of Collins (2002).
We also reporthuman performance on the insertion task in order toprovide a reasonable upper-bound on machine per-formance.
An analysis of these results shows thatour method closes the gap between machine and hu-man performance substantially.In the following section, we provide an overviewof existing work on text structuring and hierarchi-cal learning.
Then, we define the insertion task andintroduce our hierarchical ranking approach to sen-tence insertion.
Next, we present our experimentalframework and data.
We conclude the paper by pre-senting and discussing our results.2 Related WorkText Structuring The insertion task is closely re-lated to the extensively studied problem of sentenceordering.3 Most of the existing algorithms repre-sent text structure as a linear sequence and are drivenby local coherence constraints (Lapata, 2003; Kara-manis et al, 2004; Okazaki et al, 2004; Barzi-lay and Lapata, 2005; Bollegala et al, 2006; El-sner and Charniak, 2007).
These methods inducea total ordering based on pairwise relations betweensentences.
Researchers have shown that identifyingprecedence relations does not require deep semanticinterpretation of input sentences: shallow distribu-tional features are sufficient for accurate prediction.Our approach employs similar features to representnodes at the lowest level of the hierarchy.The key departure of our work from previous re-search is the incorporation of hierarchical structureinto a corpus-based approach to ordering.
While insymbolic generation and discourse analysis a text istypically analyzed as a tree-like structure (Reiter andDale, 1990), a linear view is prevalent in data-drivenmethods to text structuring.4 Moving beyond a lin-ear representation enables us to handle longer textswhere a local view of coherence does not suffice.
Atthe same time, our approach does not require anymanual rules for handling tree insertions, in contrastto symbolic text planners.3Independently and simultaneously with our work, Elsnerand Charniak (2007) have studied the sentence insertion task ina different setting.4Though statistical methods have been used to induce suchtrees (Soricut and Marcu, 2003), they are not used for orderingand other text-structuring tasks.84Shaukat Aziz (born March 6, 1949, Karachi, Pakistan) has been the Finance Minister of Pakistan since November 1999.He was nominated for the position of Prime Minister after the resignation of Zafarullah Khan Jamali on June 6, 2004.EducationAziz attended Saint Patrick?s school, Karachi and Abbottabad Public School.
He graduated with a Bachelor of Science degreefrom Gordon College, Rawalpindi, in 1967.
He obtained an MBA Degree in 1969 from the Institute of BusinessAdministration, Karachi.CareerIn November, 1999, Mr. Aziz became Pakistan?s Minister of Finance.
As Minister of finance, Mr. Aziz also heads theEconomic Coordination Committee of the Cabinet, and the Cabinet Committee on Privatization.Mr.
Aziz was named as Prime Minister by interim Prime Minister Chaudhry Shujaat Hussain after the resignation of ZafarullahKhan Jamali on June 6, 2004.
He is expected to retain his position as Minister of Finance.In 2001, Mr Aziz was declared ?Finance Minister of the Year?
byEuromoney and Banker?s Magazine.Figure 1: An example of Wikipedia insertion.Hierarchical Learning There has been much re-cent research on multiclass hierarchical classifica-tion.
In this line of work, the set of possible la-bels is organized hierarchically, and each input mustbe assigned a node in the resulting tree.
A pro-totype weight vector is learned for each node, andclassification decisions are based on all the weightsalong the path from node to root.
The essence ofthis scheme is that the more ancestors two nodeshave in common, the more parameters they areforced to share.
Many learning methods have beenproposed, including SVM-style optimization (Caiand Hofmann, 2004), incremental least squares es-timation (Cesa-Bianchi et al, 2006b), and percep-tron (Dekel et al, 2004).This previous work rests on the assumption that apredetermined set of atomic labels with a fixed hi-erarchy is given.
In our task, however, the set ofpossible insertion points ?
along with their hierar-chical organization ?
is unique to each input docu-ment.
Furthermore, nodes exhibit rich internal fea-ture structure and cannot be identified across docu-ments, except insofar as their features overlap.
Asis commonly done in NLP tasks, we make use of afeature function which produces one feature vectorfor each possible insertion point.
We then chooseamong these feature vectors using a single weightvector (casting the task as a structured ranking prob-lem rather than a classification problem).
In thisframework, an explicit hierarchical view is no longernecessary to achieve parameter tying.
In fact, eachparameter will be shared by exactly those insertionpoints which exhibit the corresponding feature, bothacross documents and within a single document.Higher level parameters will thus naturally be sharedby all paragraphs within a single section.In fact, when the perceptron update rule of (Dekelet al, 2004) ?
which modifies the weights of everydivergent node along the predicted and true paths ?is used in the ranking framework, it becomes virtu-ally identical with the standard, flat, ranking percep-tron of Collins (2002).5 In contrast, our approachshares the idea of (Cesa-Bianchi et al, 2006a) that?if a parent class has been predicted wrongly, thenerrors in the children should not be taken into ac-count.?
We also view this as one of the key ideasof the incremental perceptron algorithm of (Collinsand Roark, 2004), which searches through a com-plex decision space step-by-step and is immediatelyupdated at the first wrong move.Our work fuses this idea of selective hierarchicalupdates with the simplicity of the perceptron algo-rithm and the flexibility of arbitrary feature sharinginherent in the ranking framework.3 The AlgorithmIn this section, we present our sentence inser-tion model and a method for parameter estima-tion.
Given a hierarchically structured text com-posed of sections and paragraphs, the sentence in-sertion model determines the best paragraph within5The main remaining difference is that Dekel et al (2004)use a passive-aggressive update rule (Crammer et al, 2006) andin doing so enforce a margin based on tree distance.85which to place the new sentence.
To identify theexact location of the sentence within the chosenparagraph, local ordering methods such as (Lapata,2003) could be used.
We formalize the insertion taskas a structured ranking problem, and our model istrained using an online algorithm.
The distinguish-ing feature of the algorithm is a selective correctionmechanism that focuses the model update on the rel-evant layer of the document?s feature hierarchy.The algorithm described below can be applied toany hierarchical ranking problem.
For concreteness,we use the terminology of the sentence insertiontask, where a hierarchy corresponds to a documentwith sections and paragraphs.3.1 Problem FormulationIn a sentence insertion problem, we aregiven a training sequence of instances(s1, T 1, `1), .
.
.
, (sm, T m, `m).
Each instancecontains a sentence s, a hierarchically structureddocument T , and a node ` representing the correctinsertion point of s into T .
Although ` can generallybe any node in the tree, in our problem we needonly consider leaf nodes.
We cast this problem inthe ranking framework, where a feature vector is as-sociated with each sentence-node pair.
For example,the feature vector of an internal, section-level nodemay consider the word overlap between the insertedsentence and the section title.
At the leaf level,features may include an analysis of the overlapbetween the corresponding text and sentence.
Inpractice, we use disjoint feature sets for differentlayers of the hierarchy, though in theory they couldbe shared.Our goal then is to choose a leaf node by takinginto account its feature vector as well as feature vec-tors of all its ancestors in the tree.More formally, for each sentence s and hierarchi-cally structured document T , we are given a set offeature vectors, with one for each node: {?
(s, n) :n ?
T }.
We denote the set of leaf nodes by L(T )and the path from the root of the tree to a node nby P(n).
Our model must choose one leaf nodeamong the set L(T ) by examining its feature vec-tor ?
(s, `) as well as all the feature vectors along itspath: {?
(s, n) : n ?
P(`)}.Input : (s1, T 1, `1), .
.
.
, (sm, T m, `m).Initialize : Set w1 = 0Loop : For t = 1, 2, ..., N :1.
Get a new instance st, T t.2.
Predict ?`t = argmax`?L(T )wt ?
?
(st, `).3.
Get the new label `t.4.
If ?`t = `t:wt+1 ?
wtElse:i?
?
max{i : P(`t)i = P(?`t)i}a ?
P(`t)i?+1b ?
P(?`t)i?+1wt+1 ?
wt + ?
(s, a)?
?
(s, b)Output : wN+1.Figure 2: Training algorithm for the hierarchicalranking model.3.2 The ModelOur model consists of a weight vector w, eachweight corresponding to a single feature.
The fea-tures of a leaf are aggregated with the features of allits ancestors in the tree.
The leaf score is then com-puted by taking the inner product of this aggregatefeature vector with the weights w. The leaf with thehighest score is then selected.More specifically, we define the aggregate featurevector of a leaf ` to be the sum of all features foundalong the path to the root:?
(s, `) =?n?P(`)?
(s, n) (1)This has the effect of stacking together featuresfound in a single layer, and adding the values of fea-tures found at more than one layer.Our model then outputs the leaf with the highestscoring aggregate feature vector:arg max`?L(T )w ?
?
(s, `) (2)Note that by using this criterion, our decodingmethod is equivalent to that of the standard linearranking model.
The novelty of our approach lies inour training algorithm which uses the hierarchicalfeature decomposition of Equation 1 to pinpoint itsupdates along the path in the tree.86n1n2 n3!1 !2 !3 !4(s, T )423 121!?
!Figure 3: An example of a tree with the correspond-ing model scores.
The path surrounded by solid linesleads to the correct node `1.
The path surrounded bydotted lines leads to `3, the predicted output basedon the current model.3.3 TrainingOur training procedure is implemented in the onlinelearning framework.
The model receives each train-ing instance, and predicts a leaf node according to itscurrent parameters.
If an incorrect leaf node is pre-dicted, the weights are updated based on the diver-gence between the predicted path and the true path.We trace the paths down the tree, and only updatethe weights of the features found at the split point.Updates for shared nodes along the paths would ofcourse cancel out.
In contrast to the standard rank-ing perceptron as well as the hierarchical perceptronof (Dekel et al, 2004), no features further down thedivergent paths are incorporated in the update.
Forexample, if the model incorrectly predicts the sec-tion, then only the weights of the section featuresare updated whereas the paragraph feature weightsremain untouched.More formally, let ?`be the predicted leaf node andlet ` 6= ?`be the true leaf node.
Denote by P(`)i theith node on the path from the root to `.
Let i?
bethe depth of the lowest common ancestor of ` and?` (i.e., i?
= max{i : P(`)i = P(?`)i}).
Then theupdate rule for this round is:w ?
w+ ?(s,P(`)i?+1)?
?
(s,P(?`)i?+1)(3)Full pseudo-code for our hierarchical online trainingalgorithm is shown in Figure 2.We illustrate the selective update mechanism onthe simple example shown on Figure 3.
The cor-rect prediction is the node `1 with an aggregate pathscore of 5, but `3 with the higher score of 6 is pre-dicted.
In this case, both the section and the para-graph are incorrectly predicted.
In response to thismistake, the features associated with the correct sec-tion, n2, are added to the weights, and the features ofthe incorrectly predicted section, n3, are subtractedfrom the weights.
An alternative update strategywould be to continue to update the feature weightsof the leaf nodes, `1 and `3.
However, by identifyingthe exact source of path divergence we preserve thepreviously learned balance between leaf node fea-tures.4 FeaturesFeatures used in our experiments are inspired byprevious work on corpus-based approaches for dis-course analysis (Marcu and Echihabi, 2002; Lapata,2003; Elsner et al, 2007).
We consider three typesof features: lexical, positional, and temporal.
Thissection gives a general overview of these features(see code for further details.
)Lexical Features Lexical features have beenshown to provide strong cues for sentence position-ing.
To preserve text cohesion, an inserted sentencehas to be topically close to its surrounding sentences.At the paragraph level, we measure topical over-lap using the TF*IDF weighted cosine similarity be-tween an inserted sentence and a paragraph.
We alsouse a more linguistically refined similarity measurethat computes overlap considering only subjects andobjects.
Syntactic analysis is performed using theMINIPAR parser (Lin, 1998).The overlap features are computed at the sectionlevel in a similar way.
We also introduce an addi-tional section-level overlap feature that computes thecosine similarity between an inserted sentence andthe first sentence in a section.
In our corpus, theopening sentence of a section is typically strongly87indicative of its topic, thus providing valuable cuesfor section level insertions.In addition to overlap, we use lexical featuresthat capture word co-occurrence patterns in coherenttexts.
This measure was first introduced in the con-text of sentence ordering by Lapata (2003).
Givena collection of documents in a specific domain, wecompute the likelihood that a pair of words co-occurin adjacent sentences.
From these counts, we in-duce the likelihood that two sentences are adjacentto each other.
For a given paragraph and an in-serted sentence, the highest adjacency probabilitybetween the inserted sentence and paragraph sen-tences is recorded.
This feature is also computedat the section level.Positional Features These features aim to cap-ture user preferences when positioning new infor-mation into the body of a document.
For instance,in the Wikipedia data, insertions are more likely toappear at the end of a document than at its begin-ning.
We track positional information at the sectionand paragraph level.
At the section level, we recordwhether a section is the first or last of the document.At the paragraph level, there are four positional fea-tures which indicate the paragraph?s position (i.e.,start or end) within its individual section and withinthe document as a whole.Temporal Features The text organization may beinfluenced by temporal relations between underly-ing events.
In temporally coherent text, events thathappen in the same time frame are likely to be de-scribed in the same segment.
Our computation oftemporal features does not require full fledged tem-poral interpretation.
Instead, we extract these fea-tures based on two categories of temporal cues: verbtense and date information.
The verb tense featurecaptures whether a paragraph contains at least onesentence using the same tense as the inserted sen-tence.
For instance, this feature would occur for theinserted sentence in Figure 1 since both the sentenceand chosen paragraph employ the past tense.Another set of features takes into account the re-lation between the dates in a paragraph and those inan inserted sentence.
We extract temporal expres-sions using the TIMEX2 tagger (Mani and Wilson,2000), and compute the time interval for a paragraphbounded by its earliest and latest dates.
We recordthe degree of overlap between the paragraph time in-Section Paragraph Tree DistT1 J1 0.575 0.5 1.85J2 0.7 0.525 1.55T2 J3 0.675 0.55 1.55J4 0.725 0.55 1.45Table 1: Accuracy of human insertions comparedagainst gold standard from Wikipedia?s update log.T1 is a subset of the data annotated by judges J1 andJ2, while T2 is annotated by J3 and J4.terval and insertion sentence time interval.5 Experimental Set-UpCorpus Our corpus consists of Wikipedia articlesthat belong to the category ?Living People.?
Wefocus on this category because these articles arecommonly updated: when new facts about a personare featured in the media, a corresponding entry inWikipedia is likely to be modified.
Unlike entriesin a professionally edited encyclopedia, these arti-cles are collaboratively written by multiple users,resulting in significant stylistic and content varia-tions across texts in our corpus.
This property dis-tinguishes our corpus from more stylistically homo-geneous collections of biographies used in text gen-eration research (Duboue and McKeown, 2003).We obtain data on insertions6 from the update logthat accompanies every Wikipedia entry.
For eachchange in the article?s history, the log records an ar-ticle before and after the change.
From this informa-tion, we can identify the location of every insertedsentence.
In cases where multiple insertions occurover time to the same article, they are treated in-dependently of each other.
To eliminate spam, weplace constraints on inserted sentences: (1) a sen-tence has at least 8 tokens and at most 120 tokens;(2) the MINIPAR parser (Lin, 1998) can identify asubject or an object in a sentence.This process yields 4051 insertion/article pairs,from which 3240 pairs are used for training and 811pairs for testing.
These insertions are derived from1503 Wikipedia articles.
Relative to other corporaused in text structuring research (Barzilay and Lee,2004; Lapata, 2003; Karamanis et al, 2004), texts in6Insertion is only one type of recorded update, others in-clude deletions and sentence rewriting.88our collection are long: an average article has 32.9sentences, organized in 3.61 sections and 10.9 para-graphs.
Our corpus only includes articles that havemore than one section.
When sentences are insertedbetween paragraphs, by convention we treat them aspart of the previous paragraph.Evaluation Measures We evaluate our model us-ing insertion accuracy at the section and paragraphlevel.
This measure computes the percentage ofmatches between the predicted location of the in-sertion and the true placement.
We also report thetree distance between the predicted position and thetrue location of an inserted sentence.
Tree distanceis defined as the length of the path through the treewhich connects the predicted and the true paragraphpositions.
This measure captures section level errors(which raise the connecting path higher up the tree)as well as paragraph level errors (which widen thepath across the tree).Baselines Our first three baselines correspond tonaive insertion strategies.
The RANDOMINS methodrandomly selects a paragraph for a new sentence,while FIRSTINS and LASTINS insert a sentence intothe first and the last paragraph, respectively.We also compare our HIERARCHICAL methodagainst two competitive baselines, PIPELINE andFLAT.
The PIPELINE method separately trains tworankers, one for section selection and one for para-graph selection.
During decoding, the PIPELINEmethod first chooses the best section according tothe section-layer ranker, and then selects the bestparagraph within the chosen section according to theparagraph-layer ranker.
The FLAT method uses thesame decoding criterion as our model (Equation 2),thus making use of all the same features.
However,FLAT is trained with the standard ranking percep-tron update, without making use of the hierarchicaldecomposition of features in Equation 1.Human Performance To estimate the difficultyof sentence insertion, we conducted experimentsthat evaluate human performance on the task.
Fourjudges collectively processed 80 sentence/articlepairs which were randomly extracted from the testset.
Each insertion was processed by two annotators.Table 1 shows the insertion accuracy for eachjudge when compared against the Wikipedia goldstandard.
On average, the annotators achieve 66%accuracy in section placement and 53% accuracySection Paragraph Tree DistRANDOMINS 0.318* 0.134* 3.10*FIRSTINS 0.250* 0.136* 3.23*LASTINS 0.305* 0.215* 2.96*PIPELINE 0.579 0.314* 2.21*FLAT 0.593 0.313* 2.19*HIERARCHY 0.598 0.383 2.04Table 2: Accuracy of automatic insertion meth-ods compared against the gold standard fromWikipedia?s update log.
The third column gives treedistance, where a lower score corresponds to bet-ter performance.
Diacritic * (p < 0.01) indicateswhether differences in accuracy between the givenmodel and the Hierarchical model is significant (us-ing a Fisher Sign Test).in paragraph placement.
We obtain similar re-sults when we compare the agreement of the judgesagainst each other: 65% of section inserts and 48%of paragraph inserts are identical between two anno-tators.
The degree of variability observed in this ex-periment is consistent with human performance onother text structuring tasks such as sentence order-ing (Barzilay et al, 2002; Lapata, 2003).6 ResultsTable 2 shows the insertion performance of ourmodel and the baselines in terms of accuracy andtree distance error.
The two evaluation measures areconsistent in that they yield roughly identical rank-ings of the systems.
Assessment of statistical sig-nificance is performed using a Fisher Sign Test.
Weapply this test to compare the accuracy of the HIER-ARCHICAL model against each of the baselines.The results in Table 2 indicate that the naive inser-tion baselines (RANDOMINS, FIRSTINS, LASTINS)fall substantially behind the more sophisticated,trainable strategies (PIPELINE, FLAT, HIERARCHI-CAL).
Within the latter group, our HIERARCHI-CAL model slightly outperforms the others based onthe coarse measure of accuracy at the section level.However, in the final paragraph-level analysis, theperformance gain of our model over its counterpartsis quite significant.
Moreover, according to tree dis-tance error, which incorporates error at both the sec-tion and the paragraph level, the performance of the89HIERARCHICAL method is clearly superior.
Thisresult confirms the benefit of our selective updatemechanism as well as the overall importance of jointlearning.Viewing human performance as an upper boundfor machine performance highlights the gains of ouralgorithm.
We observe that the gap between ourmethod and human performance at the paragraphlevel is 32% smaller than that between the PIPELINEmodel and human performance, as well as the FLATmodel and human performance.Sentence-level Evaluation Until this point, wehave evaluated the accuracy of insertions at the para-graph level, remaining agnostic as to the specificplacement within the predicted paragraph.
We per-form one final evaluation to test whether the globalhierarchical view of our algorithm helps in deter-mining the exact insertion point.
To make sentence-level insertion decisions, we use a local model inline with previous sentence-ordering work (Lapata,2003; Bollegala et al, 2006).
This model examinesthe two surrounding sentences of each possible in-sertion point and extracts a feature vector that in-cludes lexical, positional, and temporal properties.The model weights are trained using the standardranking perceptron (Collins, 2002).We apply this local insertion model in two dif-ferent scenarios.
In the first, we ignore the globalhierarchical structure of the document and apply thelocal insertion model to every possible sentence pair.Using this strategy, we recover 24% of correct inser-tion points.
The second strategy takes advantage ofglobal document structure by first applying our hier-archical paragraph selection method and only thenapplying the local insertion to pairs of sentenceswithin the selected paragraph.
This approach yields35% of the correct insertion points.
This statisticallysignificant difference in performance indicates thatpurely local methods are insufficient when appliedto complete real-world documents.7 Conclusion and Future WorkWe have introduced the problem of sentence inser-tion and presented a novel corpus-based method forthis task.
The main contribution of our work is theincorporation of a rich hierarchical text representa-tion into a flexible learning approach for text struc-turing.
Our learning approach makes key use ofthe hierarchy by selecting to update only the layerfound responsible for the incorrect prediction.
Em-pirical tests on a large collection of real-world inser-tion data confirm the advantage of this approach.Sentence ordering algorithms too are likely tobenefit from a hierarchical representation of text.However, accounting for long-range discourse de-pendencies in the unconstrained ordering frameworkis challenging since these dependencies only appearwhen a particular ordering (or partial ordering) isconsidered.
An appealing future direction lies in si-multaneously inducing hierarchical and linear struc-ture on the input sentences.
In such a model, treestructure could be a hidden variable that is influ-enced by the observed linear order.We are also interested in further developing oursystem for automatic update of Wikipedia pages.Currently, our system is trained on insertions inwhich the sentences of the original text are not mod-ified.
However, in some cases additional text revi-sions are required to guarantee coherence of the gen-erated text.
Further research is required to automat-ically identify and handle such complex insertions.AcknowledgmentsThe authors acknowledge the support of the Na-tional Science Foundation (CAREER grant IIS-0448168 and grant IIS-0415865) and the Mi-crosoft Research Faculty Fellowship.
Any opin-ions, findings, and conclusions or recommenda-tions expressed above are those of the authorsand do not necessarily reflect the views of theNSF.
Thanks to S.R.K.
Branavan, Eugene Charniak,Michael Collins, Micha Elsner, Jacob Eisenstein,Dina Katabi, Igor Malioutov, Christina Sauper, LukeZettlemoyer, and the anonymous reviewers for help-ful comments and suggestions.
Data used in thiswork was collected and processed by ChristinaSauper.ReferencesRegina Barzilay and Mirella Lapata.
2005.
Modelinglocal coherence: An entity-based approach.
In Pro-ceedings of the ACL, pages 141?148.Regina Barzilay and Lillian Lee.
2004.
Catching thedrift: Probabilistic content models, with applications90to generation and summarization.
In Proceedings ofHLT-NAACL, pages 113?120.Regina Barzilay, Noemie Elhadad, and Kathleen McKe-own.
2002.
Inferring strategies for sentence orderingin multidocument news summarization.
JAIR, 17:35?55.Danushka Bollegala, Naoaki Okazaki, and MitsuruIshizuka.
2006.
A bottom-up approach to sentenceordering for multi-document summarization.
In Pro-ceedings of the COLING/ACL, pages 385?392.Lijuan Cai and Thomas Hofmann.
2004.
Hierarchi-cal document categorization with support vector ma-chines.
In Proceedings of the CIKM, pages 78?87.Nicolo` Cesa-Bianchi, Claudio Gentile, and Luca Zani-boni.
2006a.
Hierarchical classification: Combiningbayes with SVM.
In Proceedings of the ICML, pages177?184.Nicolo` Cesa-Bianchi, Claudio Gentile, and Luca Zani-boni.
2006b.
Incremental algorithms for hierarchicalclassification.
JMLR, 7:31?54.Michael Collins and Brian Roark.
2004.
Incrementalparsing with the perceptron algorithm.
In Proceedingsof the ACL, pages 111?118.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and experi-ments with perceptron algorithms.
In Proceedings ofthe EMNLP, pages 1?8.Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer.
2006.
Online passive-aggressive algorithms.
JMLR, 7:551?585.Ofer Dekel, Joseph Keshet, and Yoram Singer.
2004.Large margin hierarchical classification.
In Proceed-ings of the ICML, pages 209?216.Pablo Duboue and Kathleen McKeown.
2003.
Statis-tical acquisition of content selection rules for naturallanguage generation.
In Proceedings of the EMNLP,pages 121?128.Micha Elsner and Eugene Charniak.
2007.
A genera-tive discourse-new model for text coherence.
Techni-cal Report CS-07-04, Brown University.Micha Elsner, Joseph Austerweil, and Eugene Charniak.2007.
A unified local and global model for discoursecoherence.
In Proceedings of the HLT-NAACL, pages436?443.Marti Hearst.
1994.
Multi-paragraph segmentation ofexpository text.
In Proceedings of the ACL, pages 9?16.Nikiforos Karamanis, Massimo Poesio, Chris Mellish,and Jon Oberlander.
2004.
Evaluating centering-based metrics of coherence for text structuring using areliably annotated corpus.
In Proceedings of the ACL,pages 391?398.Mirella Lapata.
2003.
Probabilistic text structuring: Ex-periments with sentence ordering.
In Proceedings ofthe ACL, pages 545?552.Dekang Lin.
1998.
Dependency-based evaluation ofminipar.
In Proceedings of the Workshop on the Eval-uation of Parsing Systems, LREC, pages 48?56.Inderjeet Mani and George Wilson.
2000.
Robust tem-poral processing of news.
In Proceedings of the ACL,pages 69?76.Daniel Marcu and Abdessamad Echihabi.
2002.
Anunsupervised approach to recognizing discourse rela-tions.
In Proceedings of the ACL, pages 368?375.Naoaki Okazaki, Yutaka Matsuo, and Mitsuru Ishizuka.2004.
Improving chronological sentence ordering byprecedence relation.
In Proceedings of the COLING,pages 750?756.Ehud Reiter and Robert Dale.
1990.
Building NaturalLanguage Generation Systems.
Cambridge UniversityPress, Cambridge.Radu Soricut and Daniel Marcu.
2003.
Sentence leveldiscourse parsing using syntactic and lexical informa-tion.
In Proceedings of the HLT-NAACL, pages 149?156.91
