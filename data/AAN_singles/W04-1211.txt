Creating a Test Corpus of Clinical Notes Manually Tagged for Part-of-SpeechInformationSerguei  PAKHOMOVDivision of Medical InformaticsResearch, Mayo ClinicRochester, MNPakhomov.Serguei@mayo.eduAnni  CODENIBM, T.J. Watson ResearchCenter,Hawthorne, NY 10532anni@us.ibm.comChristopher   CHUTEDivision of MedicalInformatics Research, MayoClinic Rochester, MNChute@mayo.eduAbstractThis paper presents a project whose main goalis to construct a corpus of clinical textmanually annotated for part-of-speechinformation.
We describe and discuss theprocess of training three domain experts toperform linguistic annotation.
We list some ofthe challenges as well as encouraging resultspertaining to inter-rater agreement andconsistency of annotation.
We also presentpreliminary experimental results indicating thenecessity for adapting state-of-the-art POStaggers to the sublanguage domain of medicaltext.1 IntroductionHaving reliable part-of-speech (POS)information is critical to successful implementationof Natural Language Processing (NLP) techniquesfor processing unrestricted text in the biomedicaldomain.
State-of-the-art automated POS taggersachieve accuracy of 93% - 98% and the mostsuccessful implementations are based on statisticalapproaches to POS tagging.
Taggers based onHidden Markoff Model (HMM) technologycurrently appear to be in the lead.
The prime publicdomain examples of such implementations includethe Trigrams?n?Tags tagger (Brandts 2000), Xeroxtagger (Cutting et al 1992) and LT POS tagger(Mikheev 1997).
Maximum Entropy (MaxEnt)based taggers also seem to perform very well(Ratnaparkhi 1996, Jason Baldridge, Tom Morton,and Gann Bierner  http://maxent.sourceforge.net ).One of the issues with statistical POS taggers isthat most of them need a representative amount ofhand-labeled training data either in the form of acomprehensive lexicon and a corpus of untaggeddata or a large corpus of text annotated for POS ora combination of the two.
Currently, most of thePOS tagger accuracy reports are based on theexperiments involving Penn Treebank data(Marcus, 1993).
The texts in Treebank representthe general English domain.
It is not entirely clearhow representative the general English languagevocabulary and structure are of a specialized sub-domain such as clinical reports.A well-recognized problem is that the accuracyof all current POS taggers drops dramatically onunknown words.
For example, while the TnTtagger performs at 97% accuracy on known wordsin the Treebank, the accuracy drops to 89% onunknown words (Brandts, 2000).
The LT POStagger is reported to perform at 93.6-94.3%accuracy on known words and at 87.7-88.7% onunknown words using a cascading unknown word?guesser?
(Mikheev, 1997).
The overall results forboth of  these taggers are much closer to the highend of the spectrum because the rate of theunknown words in the tests performed on the PennTreebank corpus is generally relatively low ?
2.9%(Brandts, 2000).
From these results, we canconclude that the higher the rate of unknownvocabulary, the lower the overall accuracy will be,necessitating the adaptation of the taggers trainedon Penn Treebank  to sublanguage domains withvocabulary that is substantially different from theone represented by the Penn Treebank corpus.Based on the observable differences betweenthe clinical and the general English  discourse andPOS tagging accuracy results on unknownvocabulary, it is reasonable to assume that a taggertrained on general English may not perform as wellon clinical notes, where the percentage of unknownwords will increase.
To test this assumption, a?gold standard?
corpus of clinical notes needs to bemanually annotated for POS information.
Theissues with the annotation process constitute theprimary focus of this paper.We describe an effort to train three medicalcoding experts to mark the text of clinical notes forpart-of-speech information.
The motivation forusing medical coders rather than trained linguists isthreefold.
First of all, due to confidentialityrestrictions, in order to develop a corpus of handlabeled data from clinical notes one can only usepersonnel authorized to access patient information.The only way to avoid it, is to anonymize the notesprior to POS tagging which in itself is a difficultand expensive process (Ruch et al 2000).
Second,medical coding experts are well familiar with62clinical discourse, which helps especially withannotating medicine specific vocabulary.
Third,the fact that POS tagging can be viewed as aclassification task makes the medical codingexperts highly suitable because their primaryoccupation and expertise is in classifying patientrecords for subsequent retrieval.We show that, given a good set of guidelines,medical coding experts can be trained in a limitedamount of time to perform a linguistic task such asPOS annotation at a high level of agreement onboth clinical notes and Penn Treebank data.Finally, we report on a set of training experimentsperformed with the TnT tagger (Brandts, 2000)using the Penn Treebank as well as the newlydeveloped medical corpus..2 AnnotationPrior to this study, the three annotators whoparticipated in it had a substantial experience incoding clinical diagnoses but virtually noexperience in POS markup.
The training processconsisted of a general and rather superficialintroduction to the issues in linguistics as well assome formal training using the POS taggingguidelines developed by Santoriny (1991) fortagging Penn Treebank data.
The formal trainingwas followed by informal discussions of the dataand difficult cases pertinent to the clinical notesdomain which often resulted in slightmodifications to the Penn Treebank guidelines.The annotation process consisted ofpreprocessing and editing.
The pre-processingincludes sentence boundary detection, tokenizationand priming with part-of-speech tags generated bya MaxEnt tagger (Maxent 1.2.4 package (Baldridgeet al)) trained on Penn Treebank data.Automatically annotated notes were then presentedto the domain experts for editing.3 Annotator agreementIn order to establish reliability of the data, weneed to ensure internal as well as externalconsistency of the annotation.
First of all, we needto make sure that the annotators agree amongstthemselves (internal consistency) on how theymark up text for part-of-speech information.Second, we need to find out how closely theannotators generating data for this study agree withthe annotators of an established project such asPenn Treebank (external consistency).
If both testsshow relatively high levels of agreement, then wecan safely assume that the annotators in this studyare able to generate part-of-speech tags forbiomedical data that will be consistent with awidely recognized standard and can workindependently of each other thus tripling theamount of manually annotated data.3.1 MethodsTwo types of measures of consistency werecomputed ?
absolute agreement and Kappacoefficient.
The absolute agreement (Abs Agr) wascalculated by dividing the total number of times allannotators agreed on  a tag over the total numberof tags.Kappa coefficient is given in (1) (Carletta 1996)(1))(1)()(EPEPAPKappa ?
?=where P(A) is the proportion of times theannotators actually agree and P(E) is theproportion of times the annotators are expected toagree due to chance3.The Absolute Agreement is most informativewhen computed over several sets of labels andwhere one of the sets represents the ?authoritative?set.
In this case, the ratio of matches among all thesets including the ?authoritative?
set to the totalnumber of labels shows how close the other setsare to the ?authoritative?
one.
The Kappa statisticis useful in measuring how consistent theannotators are compared to each other as opposedto an authority standard.3.2 Annotator consistencyIn order to test for internal consistency, weanalyzed inter-annotator agreement where the threeannotators tagged the same small corpus of clinicaldictations.File ID Abs agr.
Kappa N Samples1137689 93.24% 0.9527 7551165875 94.59% 0.9622 7951283904 89.79% 0.9302 3921284881 90.42% 0.9328 3971307526 84.43% 0.8943 347Total   2686Average 90.49% 0.9344Table 1.
Annotator agreement results based on 5clinical notes3 A  very detailed explanation of the terms used in the formula forKappa computation as well as concrete examples of how it iscomputed are provided in Poessio and Vieira (1988).63The results were compared and the Kappa-statistic was used to calculate the inter-annotatoragreement.
The results of this experiment aresummarized in Table 1.
For the absoluteagreement, we computed the ratio of how manytimes all three annotators agreed on a tag for agiven token to the total number of tags.Based on the small pilot sample of 5 clinicalnotes (2686 words), the Kappa test showed a veryhigh agreement coefficient ?
0.93.
An acceptableagreement for most NLP classification tasks liesbetween 0.7 and 0.8 (Carletta 1996, Poessio andVieira 1988).
Absolute agreement numbers areconsistent with high Kappa as they show anaverage of 90% of all tags in the test documentsassigned exactly the same way by all threeannotators.The external consistency with the Penn Treebankannotation was computed using a small randomsample of 939 words from the Penn TreebankCorpus annotated for POS information.Annotator Abs agrA1 88.17%A2 87.85%A3 87.85%Average 87.95%Table 2.
Absolute agreement results based on 5clinical notes with an ?authority?
label set.The results in Table 2 show that the threeannotators are on average 88% consistent with theannotators of the Penn Treebank corpus.3.3 Descriptive statistics for the corpus ofclinical notesThe annotation process resulted in a corpus of273 clinical notes annotated with POS tags.
Thecorpus contains 100650 tokens from 8702 typesdistributed across 7299 sentences.
Table 3 displaysfrequency counts for the top most frequentsyntactic categories.Category Count % totalNN 18372 18%IN 8963 9%JJ 8851 9%DT 6796 7%NNP 4794 5%Table 3 Syntactic category distribution in thecorpus of clinical notes.The distribution of syntactic categories suggeststhe predominance of nominal categories, which isconsistent with the nature of clinical notesreporting on various patient characteristics such asdisorders, signs and symptoms.Another important descriptive characteristic ofthis corpus is that the average sentence length is13.79 tokens per sentence, which is relatively shortas compared to the Treebank corpus where theaverage sentence length is 24.16 tokens persentence.
This supports our informal observationof the clinical notes data containing multiplesentence fragments and short diagnosticstatements.
Shorter sentence length implies greaternumber of inter-sentential transitions and thereforeis likely to present a challenge for a stochasticprocess.4 Training a POS tagger on medical dataIn order to test some of our assumptionsregarding how the differences between generalEnglish language and the language of clinical notesmay affect POS tagging, we have trained theHMM-based TnT tagger (Brandts, 2000) withdefault parameters at the tri-gram level both onPenn Treebank and the clinical notes data.
Weshould also note that the tagger relies on asophisticated ?unknown?
word guessing algorithmwhich computes the likelihood of a tag based onthe N last letters of the word, which is meant toleverage the word?s morphology in a purelystatistical manner.The clinical notes data was split at random 10times in 80/20 fashion where 80% of the sentenceswere used for training and 20% were used fortesting.
This technique is a variation on the classic10-fold validation and appears to be more suitablefor smaller amounts of data.We conducted two experiments.
First, wecomputed the correctness of the Treebank modelon each fold of the clinical notes data.
We testedthe Treebank model on the 10 folds rather than thewhole corpus of clinical notes in order to producecorrectness results on exactly the same test data aswould be used for validation tests of models buildfrom the clinical notes data.
Then, we computedthe correctness of each of the 10 models trained oneach training fold of the clinical notes data usingthe corresponding testing fold of the same data fortesting.Table 4 Correctness results for the Treebankmodel.Correctness was computed simply as thepercentage of correct tag assignments of the POStagger (hits) to the total number of tokens in thetest set.
Table 4 summarizes the results of testingthe Treebank model, while Table 5 summarizes theSplit Hits Total CorrectnessAverage 21826.3 24309 89.79%64testing results for the models trained on the clinicalnotes.The average correctness of the Treebank modeltested on clinical notes is ~88%, which isconsiderably lower than the state-of-the-artperformance of the TnT tagger - ~96%.
Trainingthe tagger on a relatively small amount of clinicalnotes data brings the performance much closer tothe state-of-the-art ?
~95%.Table 5 Correctness results for the clinical notesmodel.5 DiscussionThe results of this pilot project are encouraging.It is clear that with appropriate supervision, peoplewho are well familiar with medical content can bereliably trained to carry out some of the taskstraditionally done by trained linguists.This study also indicates that an automatic POStagger trained on data that does not include clinicaldocuments may not perform as well as a taggertrained on data from the same domain.
Acomparison between the Treebank and the clinicalnotes data shows that the clinical notes corpuscontains 3,239 lexical items that are not found inTreebank.
The Treebank corpus contains over40,000 lexical items that are not found in thecorpus of clinical notes.
5,463 lexical items arefound in both corpora.
In addition to this 37% out-of-vocabulary rate (words in clinical notes but notthe Treebank corpus), the picture is furthercomplicated by the differences between the n-gramtag transitions within the two corpora.
Forexample, the likelihood of a DT ?
NN bigram is 1in Treebank and 0.75 in the clinical notes corpus.On the other hand, JJ ?
NN transition in theclinical notes is 1 but in the Treebank corpus it hasa likelihood of 0.73.
This is just to illustrate thefact that not only the ?unknown?
out-of-vocabularyitems may be responsible for the decreasedaccuracy of POS taggers trained on generalEnglish domain and tested on the clinical notesdomain, but the actual n-gram statistics may be amajor contributing factor.6 ConclusionSeveral questions remain unresolved.
First of all,it is unclear how much domain specific data isenough to achieve state-of-the-art performance onPOS tagging.
Second, given that it is somewhateasier to develop lexicons for POS tagging than toannotate corpora, we need to find out howimportant the corpus statistics are as opposed to adomain specific lexicon.
In other words, can weachieve state-of-the-art performance in aspecialized domain by simply adding thevocabulary from the domain to the POS tagger?slexicon?
We intend to address both of thesequestions with further experimentation.7 AcknowledgementsOur thanks go to Barbara Abbot, Pauline Funkand Debora Albrecht for their persistent efforts inthe difficult task of corpus annotation.
This workhas been carried out under the NLM TrainingGrant # T15  LM07041-19.ReferencesBaldridge, J., Morton, T., and Bierner, G URL:http://maxent.sourceforge.netBrandts, T (2000) ?TnT ?
A Statistical Part-of-SpeechTagger.?
In Proc.
NAACL/ANLP-2000.Carletta,  J.
(1996).
Assiessing agreement onclassification tasks: The Kappa statistic.Computational Linguistics, 22(2) pp.
249-254.Cutting, D., Kupiec, J., Pedersen, J, and Sibun, P. A(1992).
Practical POS Tagger.
In Proc.
ANLP?92.Jurafski D. and Martin J.
(2000).
Speech and LanguageProcessing.
Prentice Hall, NJ.Manning, C. and Shutze H. (1999).
Foundations ofStatistical Natural Language Processing.
MIT Press,Cambridge, MA.Marcus, M., B. Santorini, and M. A. Marcinkiewicz(1993).
Building a large annotated corpus of English:the Penn Treebank.
Computational Linguistics 19,297-352.Mikheev, A.
(1997).
Automatic Rule Induction forUnknown-Word Guessing.
Computational Linguistics23(3): 405-423Poessio, M. and Vieira, R. (1988).
?A corpus basedinvestigation of definite description use?Computational Linguistics, pp 186-215.Ratnaparkhi A.
(1996).
A maximum entropy part ofspeech tagger.
In Proceedings of the conference onempirical methods in natural language processing,May 1996, University of PennsylvaniaRuch P, Baud RH, Rassinoux AM, Bouillon P, RobertG.
Medical document anonymization with a semanticlexicon.
Proc AMIA Symp.
2000; 729-33.Santorini B.
(1991).
Part-of-Speech Tagging Guidelinesfor the Penn Treebank Project.
Technical Report.Department of Computer and Information Science,University of Pennsylvania.UMLS.
(2001).
UMLS Knowledge Sources (12th ed.
).Bethesda (MD): National Library of Medicine.Split Hits Total CorrectnessAverage 23018.4 24309 94.69%65
