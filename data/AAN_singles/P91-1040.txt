CONSTRAINT  PROJECTION:  AN EFF IC IENT TREATMENT OFD IS JUNCTIVE  FEATURE DESCRIPT IONSMikio NakanoNTT Basic Research Laboratories3-9-11 Midori-cho, Musashino-shi, Tokyo 180 JAPANe-mail: nakano@atom.ntt.jpAbstractUnification of disjunctive feature descriptionsis important for efficient unification-based pars-ing.
This paper presents constraint projection,a new method for unification of disjunctive fea-ture structures represented by logical constraints.Constraint projection is a generalization of con-straint unification, and is more efficient becauseconstraint projection has a mechanism for aban-doning information irrelevant o a goal specifiedby a list of variables.1 IntroductionUnification is a central operation in recent com-putational inguistic research.
Much work onsyntactic theory and natural language parsingis based on unification because unification-basedapproaches have many advantages over other syn-tactic and computational theories.
Unification-based formalisms make it easy to write a gram-mar.
In particular, they allow rules and lexiconto be written declaratively and do not need trans-formations.Some problems remain, however.
One of themain problems is the computational inefficiencyof the unification of disjunctive feature struc-tures.
Functional unification grammar (FUG)(Kay 1985) uses disjunctive feature structures foreconomical representation f lexical items.
Usingdisjunctive feature structures reduces the num-ber of lexical items.
However, if disjunctive fea-ture structures were expanded to disjunctive nor-mal form (DNF) 1 as in definite clause grammar(Pereira and Warren 1980) and Kay's parser (Kay1985), unification would take exponential time inthe number of disjuncts.
Avoiding unnecessaryexpansion of disjunction is important for efficientdisjunctive unification.
Kasper (1987) and Eiseleand DSrre (1988) have tackled this problem andproposed unification methods for disjunctive fea-ture descriptions.~DNF has a form ?bt Vq~ V?3 V.-.
Vq~n, where ?iincludes no disjunctions.These works are based on graph unificationrather than on term unification.
Graph unifica-tion has the advantage that the number of argu-ments is free and arguments are selected by la-bels so that it is easy to write a grammar andlexicon.
Graph unification, however, has two dis-advantages: it takes excessive time to search fora specified feature and it requires much copying.We adopt term unification for these reasons.Although Eisele and DSrre (1988) have men-tioned that their algorithm is applicable to termunification as well as graph unification, thismethod would lose term unification's advantageof not requiring so much copying.
On the con-trary, constraint unification (CU) (Hasida 1986,Tuda et al 1989), a disjunctive unificationmethod, makes full use of term unification ad-vantages.
In CU, disjunctive feature structuresare represented by logical constraints, particu-larly by Horn clauses, and unification is regardedas a constraint satisfaction problem.
Further-more, solving a constraint satisfaction problemis identical to transforming a constraint into anequivalent and satisfiable constraint.
CU unifiesfeature structures by transforming the constraintson them.
The basic idea of CU is to transformconstraints in a demand-driven way; that is, totransform only those constraints which may notbe satisfiable.
This is why CU is efficient and doesnot require excessive copying.However, CU has a serious disadvantage.
Itdoes not have a mechanism for abandoning irrel-evant information, so the number of argumentsin constraint-terms (atomic formulas) becomesso large that transt'ormation takes much time.Therefore, from the viewpoint of general natu-ral language processing, although CU is suitablefor processing logical constraints with small struc-tures, it is not suitable for constraints with largestructures.This paper presents constraint projection(CP), another method for disjunctive unifica-t ion.
The basic idea of CP is to abandon in-formation irrelevant o goals.
For example, in307bottom-up parsing, if grammar consists of localconstraints as in contemporary unification-basedformalisms, it is possible to abandon informa-tion about daughter nodes after the applicationof rules, because the feature structure of a mothernode is determined only by the feature structuresof its daughter nodes and phrase structure rules.Since abandoning irrelevant information makesthe resulting structure tighter, another applica-tion of phrase structure rules to it will be efficient.We use the term projection in the sense that CPreturns a projection of the input constraint on thespecified variables.We explain how to express disjunctive featurestructures by logical constraints in Section 2.
Sec-tion 3 introduces CU and indicates its disadvan-tages.
Section 4 explains the basic ideas and thealgorithm of CP.
Section 5 presents ome resultsof implementation and shows that adopting CPmakes parsing efficient.2 Expressing Disjunctive FeatureStructures by LogicalConstraintsThis section explains the representation of dis-junctive feature structures by Horn clauses.
Weuse the DEC-10 Prolog notation for writing Hornclauses.First, we can express a feature structure with-out disjunctions by a logical term.
For example,(1) is translated into (2).
FP?'"
\] (1) / agr \[num sinL subj \[agr Inure \[per ~irndg \] \](2) cat (v,agr (sing, 3rd),cat (_, agr (sing, 3rd), _) )The arguments of the functor cat  correspond tothe pos (part of speech), agr (agreement), andsnbj (subject) features.Disjunction and sharing are represented bythe bodies of Horn clauses.
An atomic formulain the body whose predicate has multiple defini-tion clauses represents a disjunction.
For exam-ple, a disjunctive feature structure (3) in FUG(Kay 1985) notation, is translated into (4).
"pos v{ \[numsing .\] } .
.
.~  plural] agr \ [ \ ]  per j 1st t /12nd j'J(3) subj \[ gr!
\[num L agr per(4) p (cat  (v, Agr, cat  (_, Agr ,_ ) ) )?
- not_3s (Agr).p (cat  (n, agr (s ing, 3rd) ,  _) ) .not_3s ( agr ( sing, Per) ): - Ist_or_2nd (Per).not_3s (agr(plural, _)).Ist_or_2nd(Ist).Ist_or_2nd(2nd).Here, the predicate p corresponds to the specifica-tion of the feature structure.
A term p(X) meansthat the variable I is a candidate of the disjunc-tive feature structure specified by the predicatep.
The ANY value used in FUG or the value ofan unspecified feature can be represented by ananonymous variable '_'.We consider atomic formulas to be constraintson the variables they include.
The atomic formulal s t_or_2nd(Per)  in (4) constrains the variablePer to be either 1st or hd .
In a similar way,not_3s (Agr) means that Agr is a term which hasthe form agr(l~um,Per), and that//am is s ing andPer is subject o the constraint l s t_or_2nd(Per )or that }lure is p lu ra l .We do not use or consider predicates with-out their definition clauses because they makeno sense as constraints.
We call an atomicformula whose predicate has definition clausesa constraint-term, and we call a sequence ofconstraint-terms a constraint.
A set of definitionclauses like (4) is called a structure of a constraint.Phrase structure rules are also represented bylogical constraints.
For example, If rules are bi-nary and if L, R, and M stand for the left daughter,the right daughter, and the mother, respectively,they stand in a ternary relation, which we repre-sent as psr(L,R,M).
Each definition clause o fpsrcorresponds to a phrase structure rule.
Clause (5)is an example.
(5) ps r (Sub j ,cat  (v, Agr, Subj ),cat  ( s ,  Agr, _) ) .Definition clauses o fpsr  may have their own bod-ies.If a disjunctive feature structure is specifiedby a constraint-term p(X) and another is specifiedby q(Y), the unification of X and Y is equivalentto the problem of finding X which satisfies (6).
(6) \[p(X),q(X)\]Thus a unification of disjunctive feature struc-tures is equivalent o a constraint satisfactionproblem.
An application of a phrase structurerule also can be considered to be a constraint sat-isfaction problem.
For instance, if categories ofleft daughter and right daughter are stipulated by el(L) and c2(R), computing a mother cate-gory is equivalent to finding M which satisfies con-straint (7).
(7) \ [c l  (L),  c2 (R) ,psr  (L,R, M)\]A Prolog call like (8) realizes this constraint308satisfaction.
(8) : -e l  (L), c2(R) ,psr (L,R,M),assert (c3(M)) ,fail.This method, however, is inefficient.
Since Pro-log chooses one definition clause when multipledefinition clauses are available, it must repeat aprocedure many times.
This method is equivalentto expanding disjunctions to DNF before unifica-tion.3 Constraint Unification and ItsProblemThis section explains constraint unification ~(Hasida 1986, Tuda et al 1989), a method of dis-junctive unification, and indicates its disadvan-tage.3.1 Basic Ideas of Constra int  Unif icat ionAs mentioned in Section 1, we can solve a con-straint satisfaction problem by constraint trans-formation.
What we seek is an efficient algo-rithm of transformation whose resulting structureis guaranteed satisfiability and includes a smallnumber of disjuncts.CU is a constraint transformation systemwhich avoids excessive xpansion of disjunctions.The goal of CU is to transform an input con-straint to a modular constraint.
Modular con-straints are defined as follows.
(9) (Definition: modular) A constraint is mod-ular, iff1.
every argument of every atomic formulais a variable,2.
no variable occurs in two distinct places,and3.
every predicate is modularly defined.A predicate is modularly defined iff the bodies ofits definition clauses are either modular or NIL.For example, (10) is a modular constraint,while (11), (12), and (13) are not modular, whenall the predicates are modularly defined.
(10) \[p(X,Y) ,q(Z,)\](11) \[p(X.X)\](12) \[p(X,?)
,q(Y.Z)\](13) \[pCf(a) ,g(Z))\]Constraint (10) is satisfiable because the predi-cates have definition clauses.
Omitting the proof,a modular constraint is necessarily satisfiable.Transforming a constraint into a modular one isequivalent to finding the set of instances whichsatisfy the constraint.
On the contrary, non-modular constraint may not be satisfiable.
When~Constralnt unification is called conditioned unifi-cation in earlier papers.a constraint is not modular, it is said to have de-pendencies.
For example, (12) has a dependencyconcerning ?.The main ideas of CU are (a) it classi-fies constraint-terms in the input constraint intogroups so that they do not share a variable andit transforms them into modular constraints sepa-rately, and (b) it does not transform odular con-straints.
Briefly, CU processes only constraintswhich have dependencies.
This corresponds toavoiding unnecessary expansion of disjunctions.In CU, the order of processes i  decided accord-ing to dependencies.
This flexibility enables CUto reduce the amount of processing.We explain these ideas and the algorithm ofCU briefly through an example.
CU consists oftwo functions, namely, modularize(constraint)and integrate(constraint).
We can execute CUby calling modularize.
Function modularize di-vides the input constraint into several constraints,and returns a list of their integrations.
If one ofthe integrations fails, modularization also fails.The function integrate creates a new constraint-term equivalent o the input constraint, findsits modular definition clauses, and returns thenew constraint-term.
Functions rnodularize andintegrate call each other.Let  us consider the execution of (14).
(14) modularize(\[p(X, Y), q(Y.
Z), p(A.
B) ,r(A) ,r(C)\])The predicates are defined as follows.
(15) pCfCA),C):-rCA),rCC).
(16) p(a.b).
(17) q(a ,b) .
(18) q(b ,a) .
(19) rCa).
(20) r(b).The input constraint is divided into (21), (22),and (23), which are processed independently(idea (a)).
(21) \[p(x,Y),q(Y,z)\](22) \[p(A,B) ,r(A)\](23) \[r(C)\]If the input constraint were not divided and (21)had multiple solutions, the processing of (22)would be repeated many times.
This is one rea-son for the efficiency of CU.
Constraint (23) is nottransformed because it is already modular (idea(b)).
Prolog would exploit the definition clausesof r and expend unnecessary computation time.This is another eason for CU's efficiency.To transform (21) and (22) into modularconstraint-terms, (24) and (25) are called.
(24) integrate(\[p(X,Y),q(Y, Z)\])(25) integrate(\[p(A,B), r(A)\])309Since (24~ and (25) succeed and returne0(X,Y,Z)" and el(A,B), respectively, (14) re-turns (26).
(26) \[c0(X,Y,Z), el (A,B) ,r(C)\]This modularization would fail if either (24) or(25) failed.Next, we explain integrate through the exe-cution of (24).
First, a new predicate c0 is madeso that we can suppose (27).
(27) cO (X,Y, Z) 4=:#p(X,Y), q(Y,Z)Formula (27) means that (24) returns c0(X,Y,Z)if the constraint \[p(X,Y) ,q(Y,Z)\] is satisfiable;that is, e0(X,?,Z) can be modularly defined sothat c0(X,Y,Z) and p(X,Y),q(Y,Z) constrainX, Y, and Z in the same way.
Next, a targetconstraint-term is chosen.
Although some heuris-tics may be applicable to this choice, we simplychoose the first element p(X,Y) here.
Then, thedefinition clauses of p are consulted.
Note thatthis corresponds to the expansion of a disjunc-tion.First, (15) is exploited.
The head of (15)is unified with p(X,Y) in (27) so that (27) be-comes (28).
(28) c0(~ CA) ,C,Z)C=~r(A) ,r(C) ,q(C,Z)The term p(f(A),C) has been replaced by itsbody r(A),r(C) in the right-hand side of (28).Formula (28) means that cO(f (A) ,C,Z) is true ifthe variables atisfy the right-hand side of (28).Since the right-hand side of (28) is not modu-lar, (29) is called and it must return a constraintlike (30).
(29) modularize(Er(A) ,rCC), qCC, Z)'l)(30) It(A) ,c2(C,Z)\]Then, (31) is created as a definition clause of cO.(31) cOCf(l) ,C,Z):-rCA) ,c2(C,Z).Second, (16) is exploited.
Then, (28) be-comes (32), (33) is called and returns (34), and(35) is created.
(32) c0(a,b,Z) ?==~q(b,Z)(33) modularize( \[q(b,Z) \])(34) \[c3(Z)\](35) cO(a,b,Z):-c3(Z).As a result, (24) returns c0(X,Y,Z) because itsdefinition clauses are made.All the Horn clauses made in this CU invokedby (14) are shown in (36).
(36) c0(fCA) ,C,Z) :-r(A) ,c2(C,Z).c0(a,b,Z) :-c3(Z).c2(a,b).aWe use cn (n = 0, 1, 2,.- -) for the names of newly-made predicates.c2(b,a).c3(a).cl(a,b).When a new clause is created, if the predicate ofa term in its body has only one definition clause,the term is unified with the head of the definitionclause and is replaced by the body.
This opera-tion is called reduction.
For example, the secondclause of (36) is reduced to (37) because c3 hasonly one definition clause.
(37) c0(a,b,a) .CU has another operation called folding.
Itavoids repeating the same type of integrationsso that it makes the transformation efficient.Folding also enables CU to handle some of therecursively-defined predicates such as member andappend.3.2 Parsing with Constraint UnificationWe adopt the CYK algorithm (Aho and Ull-man 1972) for simplicity, although any algorithmsmay be adopted.
Suppose the constraint-termcaZ_n_m(X) means X is the category of a phrasefrom the (n + 1)th word to the ruth word in aninput sentence.
Then, application of a phrasestructure rule is reduced to creating Horn clauseslike (38).
(38) ?at_n_m(M) :-modularize( Ecat_n_k (L),cat_k_m(R),psr(L,R,M)\]).
(2<re<l, 0<n<m - 2, n + l<_k<m - 1,where I is the sentence length.
)The body of the created clause is the constraintreturned by the modularization i the right-handside.
If the modularization fails, the clause is notcreated.3.3 Prob lem of Constraint Unif icationThe main problem of a CU-based parser isthat the number of constraint-term argumentsincreases as parsing proceeds.
For example,cat_0_2(M) is computed by (39).
(39) modularize(\[cat_O_l (L),cat_l_2 (R),psr(L,R,M)\])This returns a constraint like \[cO(L,R,N)\].
Then(40) is created.
(40) cat_0 2(M):-c0(L,R,M).Next, suppose that (40) is exploited in the follow-ing application of rules.
(41) modularize( \[cat_0_2(M),cat_2_3(Rl),psr(M,RI,MI)\])310Then (42) will be called.
(42) modutarize( leo (L, It, H),cat_2_3(R1),psr(H,Rl,M1)\])It returns a constraint like cl(L,R,M,R1,M1).Thus the number of the constraint-term argu-ments increases.This causes computation time explosion fortwo reasons: (a) the augmentation of argumentsincreases the computation time for making newterms and environments, dividing into groups,unification, and so on, and (b) resulting struc-tures may include excessive disjunctions becauseof the ambiguity of features irrelevant to themother categories.4 Constraint ProjectionThis section describes constraint projection (CP),which is a generalization fCU and overcomes thedisadvantage explained in the previous ection.4.1 Basic Ideas of Constra int  Pro ject ionInefficiency of parsing based on CU is caused bykeeping information about daughter nodes.
Suchinformation can be abandoned if it is assumedthat we want only information about mothernodes.
That is, transformation (43) is more usefulin parsing than (44).
(43) rclCL),c2CR),psrCL,a,H)'l ~ \[c3(H)\](44) \[cl  (L), c2(R) ,psr(L,R,H)\]:=~ \[c3(L,R,R)\]Constraint \[c3(M)\] in (43) must be satisfiableand equivalent to the left-hand side concerning H.Since \[c3(M)\] includes only information about H,it must be a normal constraint, which is definedin (45).
(45) (Definition: Normal) A constraint is normaliff(a) it is modular, and(b) each definition clause is a normal defini-tion clause; that is, its body does not includevariables which do not appear in the head.For example, (46) is a normal definition clausewhile (47) is not.
(46) p(a,X) : - r (X) .
(47) q(X) : -s(X,?)
.The operation (43) is generalized into a newoperation constraint projection which is definedin (48).
(48) Given a constraint C and a list of variableswhich we call goal, CP returns a normal con-straint which is equivalent to C concerningthe variables in the goal, and includes onlyvariables in the goal.
* Symbols used:- X, Y .
.
.
.
; lists of variables.- P, Q .
.
.
.
; constraint-terms or sometimes"fail".- P ,  Q .
.
.
.
; constraints or sometimes "fail".- H, ~ .
.
.
.
; lists of constraints.?
p ro jec t (P ,  X) returns a normal constraint (list ofatomic formulas) on X.1.
If P = NIL then return NIL.2.
I fX=NIL ,If not(satisfiable(P)), then return "fail",Else return NIL.3.
II := divide(P).4.
Hin := the list of the members of H whichinclude variables in X.5.
\]-\[ex :--- the list of the members of H otherthan the members of ~in.6.
For each member R of \]\]cx,If not(satisfiable(R)) then return "fail"7.
S := NIL.8.
For each member T of Hi,=:-V  := intersection(X, variables ap-pearing in T).- R := normalize(T, V).If R = 'faT', then return "fail",Else add R to S.9.
Return S.?
normal i ze (S ,  V) returns a normal constraint-term (atomic formula) on V.1.
If S does not include variables appearing inV, and S consists of a modular term, thenReturn S.2.
S := a member of S that includes a variablein V.3.
S' := the rest of S.4.
C := a term c .
(v \ ] ,  v2 ..... vn).
where v\],.... vn  are all the members of V and c .
is anew functor.5.
success-flag := NIL.6.
For each definition clause H :- B. of thepredicate of S:- 0 := mgu(S, H).If 0 = fail, go to the next definitionclause.- X := a list of variables in C8.- Q := pro~ect(append(BO, S'0), X ).If.
Q = fall, then go to the next defini-tton clauseElse add C0:-Q.  to the database withreduction.7.
If success-flag = NIL, then return "fail",else return C.?
mgu returns the most general unifier (Lloyd1984)?
d iv ide(P )  divides P into a number of constraintswhich share no variables and returns the list ofthe constraints.?
sa t i s f iab le (P )  returns T if P is satisfiable, andNIL otherwise.
( sa t i s f iab le  is a slight modifica-tion of modularize of CU.
)Figure 1: Algorithm of Constraint Projection311project(\[p(X,Y),q(Y,Z),p(A,S),r(A),r(e)\],\[X,e\])\[pll,Y\[,qlT.gll \[plA,B),z(l)li \[r(Cll~heck normalize(\[pll,\[l,qlT,Zll,\[g\]) ~a|isfiabilit~cO(l) r(C)I I\[co(l).r(c)\]Figure 2: A Sample Execution of projectCP also divides input constraint C into severalconstraints according to dependencies, and trans-forms them separately.
The divided constraintsare classified into two groups: constraints whichinclude variables in the goal, and the others.We call the former goal-relevant constraints andthe latter goal-irrelevant constraints.
Only goal-relevant constraints are transformed into normalconstraints.
As for goal-irrelevant constraints,only their satisfiability is examined, because theyare no longer used and examining satisfiability iseasier than transforming.
This is a reason for theefficiency of CP.4.2 A lgor i thm of  Const ra int  P ro jec t ionCP consists of two functions, project(constraint,goal(variable list)) and normalize(constraint,goal(variable ist)), which respectively correspondto modularize and integrate in CU.
We can ex-ecute CP by calling project.
The algorithm ofconstraint projection is shown in Figure 14.We explain the algorithm of CP through theexecution of (49).
(49) project(\[p(X,Y) ,q(Y ,Z) ,p(A,B) ,r(A) ,r (C)\],Ix,c\])The predicates are defined in the same way as (15)to (20).
This execution is illustrated in Figure 2.First, the input constraint is divided into (50),(51) and (52) according to dependency.
(50) \[p(x,Y),q(~,z)\](51) \[p(A,B) ,r(h)\](52) \[r(C)\]Constraints (50) and (52) are goal-relevant be-cause they include X and C, respectively.
Since4Since the current version of CP does not have anoperation corresponding to folding, it cannot handlerecursively-defined predicates.normalize( I'p (X, Y) ,q(Y ,Z)\], \[X\]) /?o(x)o ~(x Y) (Y Z) PJ " ,q ?exploit ~p( f ( l ) ,C ) : - r ( l ) , r (C ) .l uni fycO(I(A))o r(A),r(C),q(C,Z)tproject( \ [ r l l l .
, r lC l ,q lC,g) \ ] , \ [ l \ ] )\[rl J l la$$erfcO( f ( l ) ) : - r ( l ) .IcO(I)e~loit ~ p(a,b).
\[ uniJ~cO(a)CO, q(b,g)tr~ojea(ta(b, .z)\], tl)tOa~sertcO(a).IFigure 3: A Sample Execution of normalize(51) is goal-irrelevant, only its satisfiability is ex-amined and confirmed.
If some goal-irrelevantconstraints were proved not satisfiable, the pro-jection would fail.
Constraint (52) is already nor-mal, so it is not processed.
Then (53) is called totransform (50).
(53) normalize ( \[p(X, Y), q(?,  Z) \], \[X\])The second argument (goal) is the list of variablesthat appear in both (50) and the goal of (49).Since this normalization must return a constraintlike \[c0(X)\], (49) returns (54).
(54) \[c0(X) , r(C)\]This includes only variables in the goal.
This con-straint has a tighter structure than (26).Next, we explain the function normalizethrough the execution of (53).
This execution isillustrated in Figure 3.
First, a new term c0(X) ismade so that we can suppose (55).
Its argumentsare all the variables in the goal.
(55) c0 (x)c=~p(x,Y) ,q(Y,Z)The normal definition of cO should be found.Since a target constraint must include a variablein the goal, p(X,Y) is chosen.
The definitionclauses of p are (15) and (16).
(15) pCfCA) ,C) : - rCA) ,r (C) .
(16) p (a ,b ) .The clause (15) is exploited at first.
Its head isunified with p(X,Y) in (55) so that (55) becomes(56).
(If this unification failed, the next definitionclause would be exploited.
)(56) c0 (f  CA)) ?=:?,r (A) , r  (C), q(C, Z)Tlm right-hand side includes ome variables which312do not appear in the left-hand side.
Therefore,(57) is called.
(57) project(\[r(h),r(C),q(C,Z)\], \[AJ)This returns r(A), and (58) is created.
(58) c0(f(a)):-r(A).Second, (16) is exploited and (59) is createdin the same way.
(59) c0(a).Consequently, (53) returns c0(X) becausesome definition clauses of cO have been created.All the Horn clauses created in this CP areshown in (60).
(60) c0(f(A)) : - r (A) .cO(a).Comparing (60) with (36), we see that CP notonly is efficient but also needs less memory spacethan CU.4.3 Pars ing wi th  Constra int  Pro ject ionWe can construct a CYK parser by using CP asin (61).
(61) cat_n_m(M) "-project( \[cat_ n_k (L),cat_k_m(R),psr(L,R,M)\],\[.\] ).
(2<m<l, 0<n<m - 2, n + l<k<m - 1,where l is the sentence length.
)For a simple example, let us consider parsingthe sentence "Japanese work."
by the followingprojection.
(62) project(\[cat_of_japanese(L),cat_of_work (R).psr(L,R,M)\],\[M\] )The rules and leyScon are defined as follows:(63) psr(n(Num,Per),v(Num,Per, Tense),s (Tense)).
(64) cat_of_j apanes e (n (Num, third) ).
(65) cat_of_work (v (Num, Per, present) ): -not_3s (Num, Per).
(66) not_3s (plural,_).
(67) not_3s (singular,Per): -first_or_second(Per).
(68) first_or_second(first).
(69) first_or_second(second).Since the constraint cannot be divided, (70) iscalled.
(70) normalize(\[cat_of_japanese(L),cat_of_work(R),psr(L,R,M)\],\[M\] )The new term c0(M) is made, and (63) is ex-ploited.
Then (71) is to be created if its right-hand side succeeds.
(71) c0(s(Tense)) : -project( \ [cat_of  _\] apanese (n(llum, Per) ),cat_of_work (v(Num, Per ,Tense) \],\[Tense\] ).This projection calls (72).
(72) normalize(\[cat_of_j apanese (n (gum, Per ) ) ,cat_of_work (v ( \]lum, Per, Tens e) ) \],\[Tense\]).New term cl(Tense) is made and (65) is ex-ploited.
Then (73) is to be created if the right-hand side succeeds.
(73) e l (present )  :-project( \[cat_of_j apanese (n(Num, Per) ),not_3s (Num, Per) \],:\]).Since the first of argument of the projection issatisfiable, it returns NIL.
Therefore, (74) is cre-ated, and (75) is created since the right-hand sideof (71) returns cl(Tense).
(74) cl (present).
(75) c0(s (Tense)) :-c l  (Tense).When asserted, (75) is reduced to (76).
(76) c0(s (present ) ) .Consequently, \[c0(M)\] is returned.Thus CP can he applied to CYK parsing, butneedless to say, CP can be applied to parsing al-gorithms other than CYK, such as active chartparsing.5 Imp lementat ionBoth CU and CP have been implemented in SunCommon Lisp 3.0 on a Sun 4 spare station 1.They are based on a small Prolog interpreterwritten in Lisp so that they use the same non-disjunctive unification mechanism.
We also im-plemented three CYK parsers that adopt Prolog,CU, and CP as the disjunctive unification mecha-nism.
Grammar and lexicon are based on ttPSG(Pollard and Sag 1987).
Each lexical item hasabout three disjuncts on average.Table I shows comparison of the computationtime of the three parsers.
It indicates CU is notas efficient as CP when the input sentences arelong.313Input sentenceHe wanted to be a doctor.You were a doctor when you were young.I saw a man with a telescope on the hill.He wanted to be a doctor when he was a student.CPU time (see.
)Prolog CU CP3.88 6.88 5.6429.84 19.54 12.49(out of memory) 245.34 17.3265.27 19.34 14.66Table h Computation Time6 Re la ted  WorkIn the context of graph unification, Carter (1990)proposed a bottom-up parsing method whichabandons information irrelevant o the motherstructures.
His method, however, fails to checkthe inconsistency of the abandoned information.Furthermore, it abandons irrelevant informationafter the application of the rule is completed,while CP abandons goal-irrelevant constraints dy-namically in its processes.
This is another reasonwhy our method is better.Another advantage of CP is that it does notneed much copying.
CP copies only the Hornclauses which are to be exploited.
This is whyCP is expected to be more efficient and need lessmemory space than other disjunctive unificationmethods.Hasida (1990) proposed another methodcalled dependency propagation for overcoming theproblem explained in Section 3.3.
It uses tran-sclausal variables for efficient detection of depen-dencies.
Under the assumption that informa-tion about daughter categories can be abandoned,however, CP should be more efficient because ofits simplicity.7 Conc lud ing  RemarksWe have presented constraint projection, a newoperation for efficient disjunctive unification.
Theimportant feature of CP is that it returns con-straints only on the specified variables.
CP canbe considered not only as a disjunctive unifica-tion method but also as a logical inference sys-tem.
Therefore, it is expected to play an impor-tant role in synthesizing linguistic analyses uchas parsing and semantic analysis, and linguisticand non-linguistic nferences.AcknowledgmentsI would like to thank Kiyoshi Kogure and AkiraShimazu for their helpful comments.
I had pre-cious discussions with KSichi Hasida and HiroshiTuda concerning constraint unification.Re ferencesAho, A. V. and Ullman, J. D. (1972) The Theoryof Parsing, Translation, and Compiling, Vol-ume I: Parsing.
Prentice-Hall.Carter, D. (1990) Eff?cient Disjunctive Unifica-tion for Bottom-Up Parsing.
In Proceedings ofthe 13th International Conference on Computa-tional Linguistics, Volume 3. pages 70-75.Eisele, A. and DSrre, J.
(1988) Unification ofDisjunctive Feature Descriptions.
In Proceedingsof the 26th Annual Meeting of the Associationfor Computational Linguistics.Hasida, K. (1986) Conditioned Unification forNatural Language Processing.
In Proceedings ofthe llth International Conference on Computa-tional Linguistics, pages 85--87.Hasida, K. (1990) Sentence Processing as Con-straint Transformation.
I  Proceedings of the 9thEuropean Conference on Artificial Intelligence,pages 339-344.Kasper, R. T. (1987) A Unification Method forDisjunctive Feature Descriptions.
In Proceedingsof the 25th Annual Meeting of the Associationfor Computational Linguistics, pages 235-242.Kay, M. (1985) Parsing in Functional Unifi-cation Grammar.
In Natural Language Pars-ing: Psychological, Computational nd Theoreti-cal Perspectives, pages 251-278.
Cambridge Uni-versity Press.Lloyd, J. W. (1984) Foundations of Logic Pro-gramming.
Springer-Verlag.Pereira, F. C. N. and Warren, D. H. D.(1980) Definite Clause Grammar for LanguageAnalysis--A Survay of the Formalism and aComparison with Augmented Transition Net-works.
Artificial Intelligence, 13:231-278.Pollard, C. J. and Sag, I.
A.
(1987) Information-Based Syntax and Semantics, Volume 1 Funda-mentals.
CSLI Lecture Notes Series No.13.
Stan-ford:CSLI.Tuda, H., Hasida, K., and Sirai, H. (1989) JPSGParser on Constraint Logic Programming.
InProceedings of 4th Conference of the EuropeanChapter of the Association for ComputationalLinguistics, pages 95-102.314
