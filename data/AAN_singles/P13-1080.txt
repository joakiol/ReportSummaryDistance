Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 811?821,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsShallow Local Multi Bottom-up Tree Transducersin Statistical Machine TranslationFabienne Braune and Nina Seemann and Daniel Quernheim and Andreas MalettiInstitute for Natural Language Processing, University of StuttgartPfaffenwaldring 5b, 70569 Stuttgart, Germany{braunefe,seemanna,daniel,maletti}@ims.uni-stuttgart.deAbstractWe present a new translation model in-tegrating the shallow local multi bottom-up tree transducer.
We perform a large-scale empirical evaluation of our obtainedsystem, which demonstrates that we sig-nificantly beat a realistic tree-to-tree base-line on the WMT 2009 English?Germantranslation task.
As an additional contribu-tion we make the developed software andcomplete tool-chain publicly available forfurther experimentation.1 IntroductionBesides phrase-based machine translation sys-tems (Koehn et al, 2003), syntax-based systemshave become widely used because of their abil-ity to handle non-local reordering.
Those systemsuse synchronous context-free grammars (Chi-ang, 2007), synchronous tree substitution gram-mars (Eisner, 2003) or even more powerful for-malisms like synchronous tree-sequence substitu-tion grammars (Sun et al, 2009).
However, thosesystems use linguistic syntactic annotation at dif-ferent levels.
For example, the systems proposedby Wu (1997) and Chiang (2007) use no linguis-tic information and are syntactic in a structuralsense only.
Huang et al (2006) and Liu et al(2006) use syntactic annotations on the source lan-guage side and show significant improvements intranslation quality.
Using syntax exclusively onthe target language side has also been success-fully tried by Galley et al (2004) and Galley etal.
(2006).
Nowadays, open-source toolkits suchas Moses (Koehn et al, 2007) offer syntax-basedcomponents (Hoang et al, 2009), which allowexperiments without expert knowledge.
The im-provements observed for systems using syntacticannotation on either the source or the target lan-guage side naturally led to experiments with mod-els that use syntactic annotations on both sides.However, as noted by Lavie et al (2008), Liu etal.
(2009), and Chiang (2010), the integration ofsyntactic information on both sides tends to de-crease translation quality because the systems be-come too restrictive.
Several strategies such as(i) using parse forests instead of single parses (Liuet al, 2009) or (ii) soft syntactic constraints (Chi-ang, 2010) have been developed to alleviate thisproblem.
Another successful approach has beento switch to more powerful formalisms, which al-low the extraction of more general rules.
A par-ticularly powerful model is the non-contiguousversion of synchronous tree-sequence substitu-tion grammars (STSSG) of Zhang et al (2008a),Zhang et al (2008b), and Sun et al (2009),which allows sequences of trees on both sides ofthe rules [see also (Raoult, 1997)].
The multibottom-up tree transducer (MBOT) of Arnold andDauchet (1982) and Lilin (1978) offers a middleground between traditional syntax-based modelsand STSSG.
Roughly speaking, an MBOT is anSTSSG, in which all the discontinuities must oc-cur on the target language side (Maletti, 2011).This restriction yields many algorithmic advan-tages over both the traditional models as well asSTSSG as demonstrated by Maletti (2010).
For-mally, they are expressive enough to express allsensible translations (Maletti, 2012)1.
Figure 2displays sample rules of the MBOT variant, called`MBOT, that we use (in a graphical representationof the trees and the alignment).In this contribution, we report on our novel sta-tistical machine translation system that uses an`MBOT-based translation model.
The theoreti-cal foundations of `MBOT and their integrationinto our translation model are presented in Sec-tions 2 and 3.
In order to empirically evaluate the`MBOT model, we implemented a machine trans-1A translation is sensible if it is of linear size increaseand can be computed by some (potentially copying) top-downtree transducer.811S?NP1JJ11Official111NNS12forecasts121VP2VBD21predicted211NP22QP221RB2211just22111CD2212322121NN222%2221Figure 1: Example tree t with indicated positions.We have t(21) = VBD and t|221 is the subtreemarked in red.lation system that we are going to make availableto the public.
We implemented `MBOT insidethe syntax-based component of the Moses opensource toolkit.
Section 4 presents the most im-portant algorithms of our `MBOT decoder.
Weevaluate our new system on the WMT 2009 sharedtranslation task English ?
German.
The trans-lation quality is automatically measured usingBLEU scores, and we confirm the findings by pro-viding linguistic evidence (see Section 5).
Notethat in contrast to several previous approaches, weperform large scale experiments by training sys-tems with approx.
1.5 million parallel sentences.2 Theoretical ModelIn this section, we present the theoretical genera-tive model used in our approach to syntax-basedmachine translation.
Essentially, it is the localmulti bottom-up tree transducer of Maletti (2011)with the restriction that all rules must be shallow,which means that the left-hand side of each rulehas height at most 2 (see Figure 2 for shallowrules and Figure 4 for rules including non-shallowrules).
The rules extracted from the training exam-ple of Figure 3 are displayed in Figure 4.
Thoseextracted rules are forcibly made shallow by re-moving internal nodes.
The application of thoserules is illustrated in Figures 5 and 6.For those that want to understand the innerworkings, we recall the principal model in full de-tail in the rest of this section.
Since we utilize syn-tactic parse trees, let us introduce trees first.
Givenan alphabet ?
of labels, the set T?
of all ?-trees isthe smallest set T such that ?
(t1, .
.
.
, tk) ?
T forall ?
?
?, integer k ?
0, and t1, .
.
.
, tk ?
T .
In-tuitively, a tree t consists of a labeled root node ?followed by a sequence t1, .
.
.
, tk of its children.A tree t ?
T?
is shallow if t = ?
(t1, .
.
.
, tk) with?
?
?
and t1, .
.
.
, tk ?
?.NPQP NN ?
( PPvon AP NN)SNP VBD NP ?
( SNP VAFIN PP VVPP)Figure 2: Sample `MBOT rules.To address a node inside a tree, we use its po-sition, which is a word consisting of positive in-tegers.
Roughly speaking, the root of a tree isaddressed with the position ?
(the empty word).The position iw with i ?
N addresses the po-sition w in the ith direct child of the root.
Inthis way, each node in the tree is assigned aunique position.
We illustrate this notion in Fig-ure 1.
Formally, the positions pos(t) ?
N?
ofa tree t = ?
(t1, .
.
.
, tk) are inductively definedby pos(t) = {?}
?
pos(k)(t1, .
.
.
, tk), wherepos(k)(t1, .
.
.
, tk) =?1?i?k{iw | w ?
pos(ti)} .Let t ?
T?
and w ?
pos(t).
The label of t atposition w is t(w), and the subtree rooted at posi-tion w is t|w.
These notions are also illustrated inFigure 1.
A position w ?
pos(t) is a leaf (in t) ifw1 /?
pos(t).
In other words, leaves do not haveany children.
Given a subset N ?
?, we letleafN (t) = {w ?
pos(t) | t(w) ?
N, w leaf in t}be the set of all leaves labeled by elements of N .When N is the set of nonterminals, we call themleaf nonterminals.
We extend this notion to se-quences t1, .
.
.
, tk ?
T?
byleaf(k)N (t1, .
.
.
, tk) =?1?i?k{iw | w ?
leafN (ti)}.Let w1, .
.
.
, wn ?
pos(t) be (pairwise prefix-incomparable) positions and t1, .
.
.
, tn ?
T?.Then t[wi ?
ti]1?i?n denotes the tree that is ob-tained from t by replacing (in parallel) the subtreesat wi by ti for every 1 ?
i ?
n.Now we are ready to introduce our model,which is a minor variation of the local multibottom-up tree transducer of Maletti (2011).
Let?
and ?
be the input and output symbols, respec-tively, and let N ?
?
??
be the set of nontermi-nal symbols.
Essentially, the model works on pairs?t, (u1, .
.
.
, uk)?
consisting of an input tree t ?
T?812and a sequence u1, .
.
.
, uk ?
T?
of output trees.Such pairs are pre-translations of rank k. The pre-translation ?t, (u1, .
.
.
, uk)?
is shallow if all treest, u1, .
.
.
, uk in it are shallow.Together with a pre-translation we typicallyhave to store an alignment.
Given a pre-translation?t, (u1, .
.
.
, uk)?
of rank k and 1 ?
i ?
k,we call ui the ith translation of t. An align-ment for this pre-translation is an injective map-ping ?
: leaf(k)N (u1, .
.
.
, uk)?
leafN (t)?N suchthat if (w, j) ?
ran(?
), then also (w, i) ?
ran(?
)for all 1 ?
j ?
i.2 In other words, if an alignmentrequests the ith translation, then it should also re-quest all previous translations.Definition 1 A shallow local multi bottom-up treetransducer (`MBOT) is a finite set R of rules to-gether with a mapping c : R ?
R such that everyrule, written t ??
(u1, .
.
.
, uk), contains a shal-low pre-translation ?t, (u1, .
.
.
, uk)?
and an align-ment ?
for it.The components t, (u1, .
.
.
, uk), ?, and c(?
)are called the left-hand side, the right-handside, the alignment, and the weight of therule ?
= t ??
(u1, .
.
.
, uk).
Figure 2 shows twoexample `MBOT rules (without weights).
Overall,the rules of an `MBOT are similar to the rules ofan SCFG (synchronous context-free grammar), butour right-hand sides contain a sequence of treesinstead of just a single tree.
In addition, the align-ments in an SCFG rule are bijective between leafnonterminals, whereas our model permits multi-ple alignments to a single leaf nonterminal in theleft-hand side (see Figure 2).Our `MBOT rules are obtained automaticallyfrom data like that in Figure 3.
Thus, we (word)align the bilingual text and parse it in both thesource and the target language.
In this mannerwe obtain sentence pairs like the one shown inFigure 3.
To these sentence pairs we apply therule extraction method of Maletti (2011).
Therules extracted from the sentence pair of Figure 3are shown in Figure 4.
Note that these rulesare not necessarily shallow (the last two rules arenot).
Thus, we post-process the extracted rulesand make them shallow.
The shallow rules corre-sponding to the non-shallow rules of Figure 4 areshown in Figure 2.Next, we define how to combine rules to formderivations.
In contrast to most other models, we2ran(f) for a mapping f : A?
B denotes the range of f ,which is {f(a) | a ?
A}.SNPJJOfficialNNSforecastsVPVBDpredictedNPQPRBjustCD3NN%SNPADJAOffizielleNNPrognosenVAFINsindVPPPAPPRvonAPADVnurCARD3NN%VVPPausgegangenFigure 3: Aligned parsed sentences.only introduce a derivation semantics that doesnot collapse multiple derivations for the sameinput-output pair.3 We need one final notion.Let ?
= t ??
(u1, .
.
.
, uk) be a rule andw ?
leafN (t) be a leaf nonterminal (occurrence)in the left-hand side.
The w-rank rk(?, w) of therule ?
isrk(?, w) = max {i ?
N | (w, i) ?
ran(?)}
.For example, for the lower rule ?
in Figure 2 wehave rk(?, 1) = 1, rk(?, 2) = 2, and rk(?, 3) = 1.Definition 2 The set ?
(R, c) of weighted pre-translations of an `MBOT (R, c) is the smallestset T subject to the following restriction: If thereexist?
a rule ?
= t??
(u1, .
.
.
, uk) ?
R,?
a weighted pre-translation?tw, cw, (uw1 , .
.
.
, uwkw)?
?
Tfor every w ?
leafN (t) with?
rk(?, w) = kw,4?
t(w) = tw(?
),5 and?
for every iw?
?
leaf(k)N (u1, .
.
.
, uk),6ui(w?)
= uvj (?)
with ?(iw?)
= (v, j),then ?t?, c?, (u?1, .
.
.
, u?k)?
?
T is a weighted pre-translation, where?
t?
= t[w ?
tw | w ?
leafN (t)],3A standard semantics is presented, for example,in (Maletti, 2011).4If w has n alignments, then the pre-translation selectedfor it has to have suitably many output trees.5The labels have to coincide for the input tree.6Also the labels for the output trees have to coincide.813JJOfficial ?
( ADJAOffizielle) NNSforecasts ?
( NNPrognosen) VBDpredicted ?
( VAFINsind ,VVPPausgegangen) RBjust ?
( ADVnur) CD3 ?
( CARD3) NN% ?
( NN%)NPJJ NNS ?
( NPADJA NN) QPRB CD ?
( APADV CARD) NPQP NN ?
( PPAPPRvonAP NN )SNP VPVBD NP?
(SNP VAFIN VPPP VVPP)Figure 4: Extracted (even non-shallow) rules.
We obtain our rules by making those rules shallow.?
c?
= c(?)
?
?w?leafN (t) cw, and?
u?i = ui[iw?
?
uvj | ?(iw?)
= (v, j)] forevery 1 ?
i ?
k.Rules that do not contain any nonterminalleaves are automatically weighted pre-translationswith their associated rule weight.
Otherwise, eachnonterminal leaf w in the left-hand side of a rule ?must be replaced by the input tree tw of a pre-translation ?tw, cw, (uw1 , .
.
.
, uwkw)?, whose root islabeled by the same nonterminal.
In addition, therank rk(?, w) of the replaced nonterminal shouldmatch the number kw of components in the se-lected weighted pre-translation.
Finally, the non-terminals in the right-hand side that are alignedto w should be replaced by the translation that thealignment requests, provided that the nontermi-nal matches with the root symbol of the requestedtranslation.
The weight of the new pre-translationis obtained simply by multiplying the rule weightand the weights of the selected weighted pre-translations.
The overall process is illustrated inFigures 5 and 6.3 Translation ModelGiven a source language sentence e, our transla-tion model aims to find the best corresponding tar-get language translation g?
;7 i.e.,g?
= arg maxg p(g|e) .We estimate the probability p(g|e) through a log-linear combination of component models with pa-rameters ?m scored on the pre-translations ?t, (u)?such that the leaves of t concatenated read e.8p(g|e) ?7?m=1hm(?t, (u)?
)?mOur model uses the following featureshm(?t, (u1, .
.
.
, uk)?)
for a general pre-translation?
= ?t, (u1, .
.
.
, uk)?
:7Our main translation direction is English to German.8Actually, t must embed in the parse tree of e; see Sec-tion 4.
(1) The forward translation weight using the ruleweights as described in Section 2(2) The indirect translation weight using the ruleweights as described in Section 2(3) Lexical translation weight source?
target(4) Lexical translation weight target?
source(5) Target side language model(6) Number of words in the target sentences(7) Number of rules used in the pre-translation(8) Number of target side sequences; here k timesthe number of sequences used in the pre-translations that constructed ?
(gap penalty)The rule weights required for (1) are relativefrequencies normalized over all rules with thesame left-hand side.
In the same fashion the ruleweights required for (2) are relative frequenciesnormalized over all rules with the same right-hand side.
Additionally, rules that were extractedat most 10 times are discounted by multiplyingthe rule weight by 10?2.
The lexical weightsfor (2) and (3) are obtained by multiplying theword translationsw(gi|ej) [respectively,w(ej |gi)]of lexically aligned words (gi, ej) accross (possi-bly discontiguous) target side sequences.9 When-ever a source word ej is aligned to multiple targetwords, we average over the word translations.10h3(?t, (u1, .
.
.
, uk)?
)=?lexical iteme occurs in taverage {w(g|e) | g aligned to e}The computation of the language model esti-mates for (6) is adapted to score partial transla-tions consisting of discontiguous units.
We ex-plain the details in Section 4.
Finally, the count cof target sequences obtained in (7) is actually usedas a score 1001?c.
This discourages rules withmany target sequences.9The lexical alignments are different from the alignmentsused with a pre-translation.10If the word ej has no alignment to a target word, thenit is assumed to be aligned to a special NULL word and thisalignment is scored.814Combining a rule with pre-translations:NPJJ NNS ?
( NPADJA NN)JJOfficial ?
( ADJAOffizielle) NNSforecasts ?
( NNPrognosen)Obtained new pre-translation:NPJJOfficialNNSforecasts?
( NPADJAOffizielleNNPrognosen)Figure 5: Simple rule application.Combining a rule with pre-translations:SNP VBD NP ?
( SNP VAFIN PP VVPP)NPJJOfficialNNSforecasts?
(NPADJAOffizielleNNPrognosen) VBDpredicted ?
( VAFINsind ,VVPPausgegangen)NPQPRBjustCD3NN% ?
(PPvon APADVnurCARD3NN%)Obtained new pre-translation:SNPJJOfficialNNSforecastsVBDpredictedNPQPRBjustCD3NN%?
(SNPADJAOffizielleNNPrognosenVAFINsindPPvon APADVnurCARD3NN%VVPPausgegangen)Figure 6: Complex rule application.SNP VAFIN PP VVPPOffizielle Prognosen ( sind , ausgegangen ) von nur 3 %Figure 7: Illustration of LM scoring.8154 DecodingWe implemented our model in the syntax-basedcomponent of the Moses open-source toolkitby Koehn et al (2007) and Hoang et al (2009).The standard Moses syntax-based decoder onlyhandles SCFG rules; i.e, rules with contiguouscomponents on the source and the target lan-guage side.
Roughly speaking, SCFG rules are`MBOT rules with exactly one output tree.
Wethus had to extend the system to support our`MBOT rules, in which arbitrarily many outputtrees are allowed.The standard Moses syntax-based decoder usesa CYK+ chart parsing algorithm, in which eachsource sentence is parsed and contiguous spans areprocessed in a bottom-up fashion.
A rule is appli-cable11 if the left-hand side of it matches the non-terminal assigned to the full span by the parser andthe (non-)terminal assigned to each subspan.12 Inorder to speed up the decoding, cube pruning (Chi-ang, 2007) is applied to each chart cell in orderto select the most likely hypotheses for subspans.The language model (LM) scoring is directly in-tegrated into the cube pruning algorithm.
Thus,LM estimates are available for all considered hy-potheses.
To accommodate `MBOT rules, we hadto modify the Moses syntax-based decoder in sev-eral ways.
First, the rule representation itself is ad-justed to allow sequences of shallow output treeson the target side.
Naturally, we also had to ad-just hypothesis expansion and, most importantly,language model scoring inside the cube pruningalgorithm.
An overview of the modified pruningprocedure is given in Algorithm 1.The most important modifications are hiddenin lines 5 and 8.
The expansion in Line 5 in-volves matching all nonterminal leaves in the ruleas defined in Definition 2, which includes match-ing all leaf nonterminals in all (discontiguous) out-put trees.
Because the output trees can remaindiscontiguous after hypothesis creation, LM scor-ing has to be done individually over all outputtrees.
Algorithm 2 describes our LM scoring indetail.
In it we use k strings w1, .
.
.
, wk to col-lect the lexical information from the k output com-11Note that our notion of applicable rules differs from thedefault in Moses.12Theoretically, this allows that the decoder ignores unaryparser nonterminals, which could also disappear when wemake our rules shallow; e.g., the parse tree left in the pre-translation of Figure 5 can be matched by a rule with left-hand side NP(Official, forecasts).Algorithm 1 Cube pruning with `MBOT rulesData structures:- r[i, j]: list of rules matching span e[i .
.
.
j]- h[i, j]: hypotheses covering span e[i .
.
.
j]- c[i, j]: cube of hypotheses covering span e[i .
.
.
j]1: for all `MBOT rules ?
covering span e[i .
.
.
j] do2: Insert ?
into r[i, j]3: Sort r[i, j]4: for all (l??
r) ?
r[i, j] do5: Create h[i, j] by expanding all nonterminals in l withbest scoring hypotheses for subspans6: Add h[i, j] to c[i, j]7: for all hypotheses h ?
c[i, j] do8: Estimate LM score for h // see Algorithm 29: Estimate remaining feature scores10: Sort c[i, j]11: Retrieve first ?
elements from c[i, j] // we use ?
= 103ponents (u1, .
.
.
, uk) of a rule.
These strings canlater be rearranged in any order, so we LM-scoreall of them separately.
Roughly speaking, we ob-tain wi by traversing ui depth-first left-to-right.If we meet a lexical element (terminal), then weadd it to the end of wi.
On the other hand, if wemeet a nonterminal, then we have to consult thebest pre-translation ?
?
= ?t?, (u?1, .
.
.
, u?k?
)?, whichwill contribute the subtree at this position.
Sup-pose that u?j will be substituted into the nontermi-nal in question.
Then we first LM-score the pre-translation ?
?
to obtain the string w?j correspond-ing to u?j .
This string w?j is then appended to wi.Once all the strings are built, we score them usingour 4-gram LM.
The overall LM score for the pre-translation is obtained by multiplying the scoresfor w1, .
.
.
, wk.
Clearly, this treats w1, .
.
.
, wk ask separate strings, although they eventually willbe combined into a single string.
Whenever sucha concatenation happens, our LM scoring will au-tomatically compute n-gram LM scores based onthe concatenation, which in particular means thatthe LM scores get more accurate for larger spans.Finally, in the final rule only one component is al-lowed, which yields that the LM indeed scores thecomplete output sentence.Figure 7 illustrates our LM scoring for a pre-translation involving a rule with two (discontigu-ous) target sequences (the construction of the pre-translation is illustrated in Figure 6).
When pro-cessing the rule rooted at S, an LM estimate iscomputed by expanding all nonterminal leaves.
Inour case, these are NP, VAFIN, PP, and VVPP.However, the nodes VAFIN and VVPP are assem-bled from a (discontiguous) tree sequence.
Thismeans that those units have been considered as in-816Algorithm 2 LM scoringData structures:- (u1, .
.
.
, uk): right-hand side of a rule- (w1, .
.
.
, wk): k strings all initially empty1: score = 12: for all 1 ?
i ?
k do3: for all leaves ` in ui (in lexicographic order) do4: if ` is a terminal then5: Append ` to wi6: else7: LM score the best hypothesis for the subspan8: Expand wi by the corresponding w?j9: score = score ?
LM(wi)dependent until now.
So far, the LM scorer couldonly score their associated unigrams.
However,we also have their associated strings w?1 and w?2,which can now be used.
Since VAFIN and VVPPnow become parts of a single tree, we can performLM scoring normally.
Assembling the string weobtainOffizielle Prognosen sind von nur 3 %ausgegangenwhich is scored by the LM.
Thus, we first scorethe 4-grams ?Offizielle Prognosen sind von?, then?Prognosen sind von nur?, etc.5 Experiments5.1 SetupThe baseline system for our experiments is thesyntax-based component of the Moses open-source toolkit of Koehn et al (2007) and Hoanget al (2009).
We use linguistic syntactic anno-tation on both the source and the target languageside (tree-to-tree).
Our contrastive system is the`MBOT-based translation system presented here.We provide the system with a set of SCFG as wellas `MBOT rules.
We do not impose any maximalspan restriction on either system.The compared systems are evaluated on theEnglish-to-German13 news translation task ofWMT 2009 (Callison-Burch et al, 2009).
Forboth systems, the used training data is from the4th version of the Europarl Corpus (Koehn, 2005)and the News Commentary corpus.
Both trans-lation models were trained with approximately1.5 million bilingual sentences after length-ratiofiltering.
The word alignments were generatedby GIZA++ (Och and Ney, 2003) with the grow-diag-final-and heuristic (Koehn et al, 2005).
The13Note that our `MBOT-based system can be applied to anylanguage pair as it involves no language-specific engineering.System BLEUBaseline 12.60`MBOT ?13.06Moses t-to-s 12.72Table 1: Evaluation results.
The starred resultsare statistically significant improvements over theBaseline (at confidence p < 0.05).English side of the bilingual data was parsed us-ing the Charniak parser of Charniak and John-son (2005), and the German side was parsed us-ing BitPar (Schmid, 2004) without the functionand morphological annotations.
Our German 4-gram language model was trained on the Ger-man sentences in the training data augmentedby the Stuttgart SdeWaC corpus (Web-as-CorpusConsortium, 2008), whose generation is detailedin (Baroni et al, 2009).
The weights ?m in thelog-linear model were trained using minimum er-ror rate training (Och, 2003) with the News 2009development set.
Both systems use glue-rules,which allow them to concatenate partial transla-tions without performing any reordering.5.2 ResultsWe measured the overall translation quality withthe help of 4-gram BLEU (Papineni et al, 2002),which was computed on tokenized and lower-cased data for both systems.
The results of ourevaluation are reported in Table 1.
For com-parison, we also report the results obtained bya system that utilizes parses only on the sourceside (Moses tree-to-string) with its standard fea-tures.We can observe from Table 1 that our `MBOT-based system outperforms the baseline.
We ob-tain a BLEU score of 13.06, which is a gain of0.46 BLEU points over the baseline.
This im-provement is statistically significant at confidencep < 0.05, which we computed using the pairwisebootstrap resampling technique of Koehn (2004).Our system is also better than the Moses tree-to-string system.
However this improvement (0.34)is not statistically significant.
In the next section,we confirm the result of the automatic evaluationthrough a manual examination of some transla-tions generated by our system and the baseline.In Table 2, we report the number of `MBOTrules used by our system when decoding the testset.
By lex we denote rules containing only lexical817lex non-term totalcontiguous 23,175 18,355 41,530discontiguous 315 2,516 2,831Table 2: Number of rules used in decoding test(lex: only lexical items; non-term: at least onenonterminal).2-dis 3-dis 4-dis2,480 323 28Table 3: Number of k-discontiguous rules.items.
The label non-term stands for rules contain-ing at least one leaf nonterminal.
The results showthat approx.
6% of all rules used by our `MBOT-system have discontiguous target sides.
Further-more, the reported numbers show that the systemalso uses rules in which lexical items are com-bined with nonterminals.
Finally, Table 3 presentsthe number of rules with k target side componentsused during decoding.5.3 Linguistic AnalysisIn this section we present linguistic evidence sup-porting the fact that the `MBOT-based system sig-nificantly outperforms the baseline.
All exam-ples are taken from the translation of the test setused for automatic evaluation.
We show that whenour system generates better translations, this is di-rectly related to the use of `MBOT rules.Figures 8 and 9 show the ability of our system tocorrectly reorder multiple segments in the sourcesentence where the baseline translates those seg-ments sequentially.
An analysis of the generatedderivations shows that our system produces thecorrect translation by taking advantage of ruleswith discontiguous units on target language side.The rules used in the presented derivations are dis-played in Figures 10 and 11.
In the first example(Figure 8), we begin by translating ?
((smuggle)VB(eight projectiles)NP (into the kingdom)PP)VP?
intothe discontiguous sequence composed of (i) ?(achtgeschosse)NP?
; (ii) ?
(in das ko?nigreich)PP?
and(iii) ?(schmuggeln)VP?.
In a second step we as-semble all sequences in a rule with contiguous tar-get language side and, at the same time, insert theword ?(zu)PTKZU?
between ?
(in das ko?nigreich)PP?and ?
(schmuggeln)VP?.The second example (Figure 9) illustrates amore complex reordering.
First, we trans-VPVB NP PP?
( NPNP, PPPP, VVINFVVINF)STO VP?
( VPNP PP PTKZU VVINF)Figure 10: Used `MBOT rules for verbal reorder-ingVPADV commented on NP?
( NPNP, ADVADV, VPPkommentiert)VPVBZ VP?
( NPNP, VAFINVAFIN, ADVADV, VPPVPP)TOPNP VP?
( TOPNP VAFIN NP ADV VVPP)Figure 11: Used `MBOT rules for verbal reorder-inglate ?
((again)ADV commented on (the problemof global warming)NP)VP?
into the discontigu-ous sequence composed of (i) ?
(das problemder globalen erwa?rmung)NP?
; (ii) ?
(wieder)ADV?and (iii) ?(kommentiert)VPP?.
In a second step,we translate the auxiliary ?(has)VBZ?
by in-serting ?(hat)VAFIN?
into the sequence.
Wethus obtain, for the input segment ?
((has)VBZ(again)ADV commented on (the problem of globalwarming)NP)VP?, the sequence (i) ?
(das problemder globalen erwa?rmung)NP?
; (ii) ?(hat)VAFIN?
;(iii) ?(wieder)ADV?
; (iv) ?(kommentiert)VVPP?.
Ina last step, the constituent ?
(president va?clavklaus)NP?
is inserted between the discontiguousunits ?(hat)VAFIN?
and ?(wieder)ADV?
to form thecontiguous sequence ?
((das problem der glob-alen erwa?rmung)NP (hat)VAFIN (pra?sident va?clavklaus)NP (wieder)ADV (kommentiert)VVPP)TOP?.Figures 12 and 13 show examples where oursystem generates complex words in the targetlanguage out of a simple source language word.Again, an analysis of the generated derivationshows that `MBOT takes advantage of rules hav-ing several target side components.
Examples ofsuch rules are given in Figure 14.
Through itsability to use these discontiguous rules, our sys-tem correctly translates into reflexive or particleverbs such as ?konzentriert sich?
(for the English?focuses?)
or ?besteht darauf ?
(for the English?insist?).
Another phenomenon well handled byour system are relative pronouns.
Pronouns suchas ?that?
or ?whose?
are systematically translated818.
.
.
geplant hatten 8 geschosse in das ko?nigreich zu schmuggeln.
.
.
had planned to smuggle 8 projectiles into the kingdom.
.
.
vorhatten zu schmuggeln 8 geschosse in das ko?nigreichFigure 8: Verbal Reordering (top: our system, bottom: baseline)das problem der globalen erwa?rmung hat pra?sident va?clav klaus wieder kommentiertpresident va?clav klaus has again commented on the problem of global warmingpra?sident va?clav klaus hat wieder kommentiert das problem der globalen erwa?rmungFigure 9: Verbal Reordering (top: our system, bottom: baseline).
.
.
die serbische delegation bestand darauf , dass jede entscheidung .
.
.. .
.
the serbian delegation insisted that every decision .
.
.. .
.
die serbische delegation bestand , jede entscheidung .
.
.Figure 12: Relative Clause (top: our system, bot-tom: baseline).
.
.
die roadmap von bali , konzentriert sich auf die bemu?hungen .
.
.. .
.
the bali roadmap that focuses on efforts .
.
.. .
.
die bali roadmap , konzentriert auf bemu?hungen .
.
.Figure 13: Reflexive Pronoun (top: our system,bottom: baseline)into both both, ?,?
and ?dass?
or ?,?
and ?deren?
(Figure 12).6 Conclusion and Future WorkWe demonstrated that our `MBOT-based machinetranslation system beats a standard tree-to-treesystem (Moses tree-to-tree) on the WMT 2009translation task English ?
German.
To achievethis we implemented the formal model as de-scribed in Section 2 inside the Moses machinetranslation toolkit.
Several modifications werenecessary to obtain a working system.
We publiclyrelease all our developed software and our com-plete tool-chain to allow independent experimentsand evaluation.
This includes our `MBOT decoderINthat?
( $,,, KOUSdass) VBZfocuses?
( VVFINkonzentriert, PRFsich)Figure 14: `MBOT rules generating a relativeclause/reflexive pronounpresented in Section 4 and a separate C++ modulethat we use for rule extraction (see Section 3).Besides the automatic evaluation, we also per-formed a small manual analysis of obtained trans-lations and show-cased some examples (see Sec-tion 5.3).
We argue that our `MBOT approach canadequately handle discontiguous phrases, whichoccur frequently in German.
Other languages thatexhibit such phenomena include Czech, Dutch,Russian, and Polish.
Thus, we hope that our sys-tem can also successfully be applied for other lan-guage pairs, which we plan to pursue as well.In other future work, we want to investigatefull backwards application of `MBOT rules, whichwould be more suitable for the converse transla-tion direction German?
English.
The current in-dependent LM scoring of components has somenegative side-effects that we plan to circumventwith the use of lazy LM scoring.AcknowledgementThe authors thank Alexander Fraser for his ongo-ing support and advice.
All authors were finan-cially supported by the German Research Founda-tion (DFG) grant MA 4959 / 1-1.819ReferencesAndre?
Arnold and Max Dauchet.
1982.
Morphismeset bimorphismes d?arbres.
Theoret.
Comput.
Sci.,20(1):33?93.Marco Baroni, Silvia Bernardini, Adriano Ferraresi,and Eros Zanchetta.
2009.
The WaCky WideWeb: A collection of very large linguistically pro-cessed web-crawled corpora.
Language Resourcesand Evaluation, 43(3):209?226.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Josh Schroeder.
2009.
Findings of the 2009Workshop on Statistical Machine Translation.
InProc.
4th Workshop on Statistical Machine Trans-lation, pages 1?28.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminativereranking.
In Proc.
43rd ACL, pages 173?180.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computat.
Linguist., 33(2):201?228.David Chiang.
2010.
Learning to translate with sourceand target syntax.
In Proc.
48th ACL, pages 1443?1452.Jason Eisner.
2003.
Learning non-isomorphic treemappings for machine translation.
In Proc.
41stACL, pages 205?208.Michel Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a translation rule?In Proc.
HLT-NAACL, pages 273?280.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve Deneefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proc.44th ACL, pages 961?968.Hieu Hoang, Philipp Koehn, and Adam Lopez.
2009.A unified framework for phrase-based, hierarchical,and syntax-based statistical machine translation.
InProc.
6th Int.
Workshop Spoken Language Transla-tion, pages 152?159.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.Statistical syntax-directed translation with extendeddomain of locality.
In Proc.
7th Conf.
Associationfor Machine Translation of the Americas, pages 66?73.Philip Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proc.HLT-NAACL, pages 127?133.Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne, andDavid Talbot.
2005.
Edinburgh system descriptionfor the 2005 IWSLT Speech Translation Evaluation.In Proc.
2nd Int.
Workshop Spoken Language Trans-lation.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProc.
ACL, pages 177?180.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proc.
EMNLP,pages 388?395.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In Proc.
10th Ma-chine Translation Summit, pages 79?86.Alon Lavie, Alok Parlikar, and Vamshi Ambati.
2008.Syntax-driven learning of sub-sentential translationequivalents and translation rules from parsed parallelcorpora.
In Proc.
2nd ACL Workshop on Syntax andStructure in Statistical Translation, pages 87?95.Eric Lilin.
1978.
Une ge?ne?ralisation des transducteursd?e?tats finis d?arbres: les S-transducteurs.
The`se3e`me cycle, Universite?
de Lille.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machinetranslation.
In Proc.
44th ACL, pages 609?616.Yang Liu, Yajuan Lu?, and Qun Liu.
2009.
Improvingtree-to-tree translation with packed forests.
In Proc.47th ACL, pages 558?566.Andreas Maletti.
2010.
Why synchronous tree sub-stitution grammars?
In Proc.
HLT-NAACL, pages876?884.Andreas Maletti.
2011.
How to train your multibottom-up tree transducer.
In Proc.
49th ACL, pages825?834.Andreas Maletti.
2012.
Every sensible extended top-down tree transducer is a multi bottom-up tree trans-ducer.
In Proc.
HLT-NAACL, pages 263?273.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computat.
Linguist., 29(1):19?51.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proc.
41st ACL,pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Weijing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proc.
40thACL, pages 311?318.Jean-Claude Raoult.
1997.
Rational tree relations.Bull.
Belg.
Math.
Soc.
Simon Stevin, 4(1):149?176.Helmut Schmid.
2004.
Efficient parsing of highly am-biguous context-free grammars with bit vectors.
InProc.
20th COLING, pages 162?168.820Jun Sun, Min Zhang, and Chew Lim Tan.
2009.
A non-contiguous tree sequence alignment-based model forstatistical machine translation.
In Proc.
47th ACL,pages 914?922.Web-as-Corpus Consortium.
2008.
SDeWaC ?
a 0.88billion word corpus for german.
Website: http://wacky.sslmit.unibo.it/doku.php.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computat.
Linguist., 23(3):377?403.Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,Chew Lim Tan, and Sheng Li.
2008a.
A treesequence alignment-based tree-to-tree translationmodel.
In Proc.
46th ACL, pages 559?567.Min Zhang, Hongfei Jiang, Haizhou Li, Aiti Aw, andSheng Li.
2008b.
Grammar comparison studyfor translational equivalence modeling and statis-tical machine translation.
In Proc.
22nd Inter-national Conference on Computational Linguistics,pages 1097?1104.821
