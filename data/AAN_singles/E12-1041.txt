Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 398?408,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsLexical surprisal as a general predictor of reading timeIrene Fernandez Monsalve, Stefan L. Frank and Gabriella ViglioccoDivision of Psychology and Language SciencesUniversity College London{ucjtife, s.frank, g.vigliocco}@ucl.ac.ukAbstractProbabilistic accounts of language process-ing can be psychologically tested by com-paring word-reading times (RT) to the con-ditional word probabilities estimated bylanguage models.
Using surprisal as a link-ing function, a significant correlation be-tween unlexicalized surprisal and RT hasbeen reported (e.g., Demberg and Keller,2008), but success using lexicalized modelshas been limited.
In this study, phrase struc-ture grammars and recurrent neural net-works estimated both lexicalized and unlex-icalized surprisal for words of independentsentences from narrative sources.
Thesesame sentences were used as stimuli ina self-paced reading experiment to obtainRTs.
The results show that lexicalized sur-prisal according to both models is a signif-icant predictor of RT, outperforming its un-lexicalized counterparts.1 IntroductionContext-sensitive, prediction-based processinghas been proposed as a fundamental mechanismof cognition (Bar, 2007): Faced with the prob-lem of responding in real-time to complex stim-uli, the human brain would use basic informationfrom the environment, in conjunction with previ-ous experience, in order to extract meaning andanticipate the immediate future.
Such a cognitivestyle is a well-established finding in low level sen-sory processing (e.g., Kveraga et al 2007), buthas also been proposed as a relevant mechanismin higher order processes, such as language.
In-deed, there is ample evidence to show that humanlanguage comprehension is both incremental andpredictive.
For example, on-line detection of se-mantic or syntactic anomalies can be observed inthe brain?s EEG signal (Hagoort et al 2004) andeye gaze is directed in anticipation at depictionsof plausible sentence completions (Kamide et al2003).
Moreover, probabilistic accounts of lan-guage processing have identified unpredictabilityas a major cause of processing difficulty in lan-guage comprehension.
In such incremental pro-cessing, parsing would entail a pre-allocation ofresources to expected interpretations, so that ef-fort would be related to the suitability of suchan allocation to the actually encountered stimulus(Levy, 2008).Possible sentence interpretations can be con-strained by both linguistic and extra-linguisticcontext, but while the latter is difficult to evalu-ate, the former can be easily modeled: The pre-dictability of a word for the human parser can beexpressed as the conditional probability of a wordgiven the sentence so far, which can in turn be es-timated by language models trained on text cor-pora.
These probabilistic accounts of languageprocessing difficulty can then be validated againstempirical data, by taking reading time (RT) on aword as a measure of the effort involved in its pro-cessing.Recently, several studies have followed this ap-proach, using ?surprisal?
(see Section 1.1) as thelinking function between effort and predictabil-ity.
These can be computed for each word in atext, or alternatively for the words?
parts of speech(POS).
In the latter case, the obtained estimatescan give an indication of the importance of syn-tactic structure in developing upcoming-word ex-pectations, but ignore the rich lexical informationthat is doubtlessly employed by the human parser398to constrain predictions.
However, whereas suchan unlexicalized (i.e., POS-based) surprisal hasbeen shown to significantly predict RTs, successwith lexical (i.e., word-based) surprisal has beenlimited.
This can be attributed to data sparsity(larger training corpora might be needed to pro-vide accurate lexical surprisal than for the unlex-icalized counterpart), or to the noise introducedby participant?s world knowledge, inaccessible tothe models.
The present study thus sets out to findsuch a lexical surprisal effect, trying to overcomepossible limitations of previous research.1.1 Surprisal theoryThe concept of surprisal originated in the field ofinformation theory, as a measure of the amount ofinformation conveyed by a particular event.
Im-probable (?surprising?)
events carry more infor-mation than expected ones, so that surprisal is in-versely related to probability, through a logarith-mic function.
In the context of sentence process-ing, if w1, ..., wt?1 denotes the sentence so far,then the cognitive effort required for processingthe next word, wt, is assumed to be proportionalto its surprisal:effort(t) ?
surprisal(wt)= ?
log(P (wt|w1, ..., wt?1)) (1)Different theoretical groundings for this rela-tionship have been proposed (Hale, 2001; Levy2008; Smith and Levy, 2008).
Smith and Levyderive it by taking a scale free assumption: Anylinguistic unit can be subdivided into smaller en-tities (e.g., a sentence is comprised of words, aword of phonemes), so that time to process thewhole will equal the sum of processing times foreach part.
Since the probability of the whole canbe expressed as the product of the probabilities ofthe subunits, the function relating probability andeffort must be logarithmic.
Levy (2008), on theother hand, grounds surprisal in its information-theoretical context, describing difficulty encoun-tered in on-line sentence processing as a result ofthe need to update a probability distribution overpossible parses, being directly proportional to thedifference between the previous and updated dis-tributions.
By expressing the difference betweenthese in terms of relative entropy, Levy shows thatdifficulty at each newly encountered word shouldbe equal to its surprisal.1.2 Empirical evidence for surprisalThe simplest statistical language models that canbe used to estimate surprisal values are n-grammodels or Markov chains, which condition theprobability of a given word only on its n?
1 pre-ceding ones.
Although Markov models theoret-ically limit the amount of prior information thatis relevant for prediction of the next step, theyare often used in linguistic context as an approx-imation to the full conditional probability.
Theeffect of bigram probability (or forward transi-tional probability) has been repeatedly observed(e.g.
McDonald and Shillcock, 2003), and Smithand Levy (2008) report an effect of lexical sur-prisal as estimated by a trigram model on RTsfor the Dundee corpus (a collection of newspapertexts with eye-tracking data from ten participants;Kennedy and Pynte, 2005).Phrase structure grammars (PSGs) have alsobeen amply used as language models (Boston etal., 2008; Brouwer et al 2010; Demberg andKeller, 2008; Hale, 2001; Levy, 2008).
PSGscan combine statistical exposure effects with ex-plicit syntactic rules, by annotating norms withtheir respective probabilities, which can be es-timated from occurrence counts in text corpora.Information about hierarchical sentence structurecan thus be included in the models.
In this way,Brouwer et altrained a probabilistic context-free grammar (PCFG) on 204,000 sentences ex-tracted from Dutch newspapers to estimate lexi-cal surprisal (using an Earley-Stolcke parser; Stol-cke, 1995), showing that it could account forthe noun phrase coordination bias previously de-scribed and explained by Frazier (1987) in termsof a minimal-attachment preference of the humanparser.
In contrast, Demberg and Keller used textsfrom a naturalistic source (the Dundee corpus) asthe experimental stimuli, thus evaluating surprisalas a wide-coverage account of processing diffi-culty.
They also employed a PSG, trained on aone-million-word language sample from the WallStreet Journal (part of the Penn Treebank II, Mar-cus et al 1993).
Using Roark?s (2001) incremen-tal parser, they found significant effects of unlexi-calized surprisal on RTs (see also Boston et alfora similar approach and results for German texts).However, they failed to find an effect for lexical-ized surprisal, over and above forward transitionalprobability.
Roark et al(2009) also looked at the399effects of syntactic and lexical surprisal, using RTdata for short narrative texts.
However, their es-timates of these two surprisal values differ fromthose described above: In order to tease apart se-mantic and syntactic effects, they used Dembergand Keller?s lexicalized surprisal as a total sur-prisal measure, which they decompose into syn-tactic and lexical components.
Their results showsignificant effects of both syntactic and lexicalsurprisal, although the latter was found to holdonly for closed class words.
Lack of a wider effectwas attributed to data sparsity: The models weretrained on the relatively small Brown corpus (overone million words from 500 samples of AmericanEnglish text), so that surprisal estimates for theless frequent content words would not have beenaccurate enough.Using the same training and experimental lan-guage samples as Demberg and Keller (2008),and only unlexicalized surprisal estimates, Frank(2009) and Frank and Bod (2011) focused oncomparing different language models, includingvarious n-gram models, PSGs and recurrent net-works (RNN).
The latter were found to be the bet-ter predictors of RTs, and PSGs could not explainany variance in RT over and above the RNNs,suggesting that human processing relies on linearrather than hierarchical representations.Summing up, the only models taking into ac-count actual words that have been consistentlyshown to simulate human behaviour with natural-istic text samples are bigram models.1 A possi-ble limitation in previous studies can be found inthe stimuli employed.
In reading real newspapertexts, prior knowledge of current affairs is likelyto highly influence RTs, however, this source ofvariability cannot be accounted for by the mod-els.
In addition, whereas the models treat eachsentence as an independent unit, in the text cor-pora employed they make up coherent texts, andare therefore clearly dependent.
Thirdly, the stim-uli used by Demberg and Keller (2008) comprisea very particular linguistic style: journalistic edi-torials, reducing the ability to generalize conclu-sions to language in general.
Finally, failure tofind lexical surprisal effects can also be attributedto the training texts.
Larger corpora are likely tobe needed for training language models on actual1Although Smith and Levy (2008) report an effect of tri-grams, they did not check if it exceeded that of simpler bi-grams.words than on POS (both the Brown corpus andthe WSJ are relatively small), and in addition, theparticular journalistic style of the WSJ might notbe the best alternative for modeling human be-haviour.
Although similarity between the train-ing and experimental data sets (both from news-paper sources) can improve the linguistic perfor-mance of the models, their ability to simulate hu-man behaviour might be limited: Newspaper textsprobably form just a small fraction of a person?slinguistic experience.
This study thus aims totackle some of the identified limitations: Ratherthan cohesive texts, independent sentences, froma narrative style are used as experimental stim-uli for which word-reading times are collected(as explained in Section 3).
In addition, as dis-cussed in the following section, language mod-els are trained on a larger corpus, from a morerepresentative language sample.
Following Frank(2009) and Frank and Bod (2011), two contrastingtypes of models are employed: hierarchical PSGsand linear RNNs.2 Models2.1 Training dataThe training texts were extracted from the writ-ten section of the British National Corpus (BNC),a collection of language samples from a varietyof sources, designed to provide a comprehensiverepresentation of current British English.
A totalof 702,412 sentences, containing only the 7,754most frequent words (the open-class words usedby Andrews et al 2009, plus the 200 most fre-quent words in English) were selected, making upa 7.6-million-word training corpus.
In addition toproviding a larger amount of data than the WSJ,this training set thus provides a more representa-tive language sample.2.2 Experimental sentencesThree hundred and sixty-one sentences, all com-prehensible out of context and containing onlywords included in the subset of the BNC usedto train the models, were randomly selected fromthree freely accessible on-line novels2 (for addi-tional details, see Frank, 2012).
The fictionalnarrative provides a good contrast to the pre-2Obtained from www.free-online-novels.com.Having not been published elsewhere, it is unlikely partici-pants had read the novels previously.400viously examined newspaper editorials from theDundee corpus, since participants did not needprior knowledge regarding the details of the sto-ries, and a less specialised language and stylewere employed.
In addition, the randomly se-lected sentences did not make up coherent texts(in contrast, Roark et al 2009, employed shortstories), so that they were independent from eachother, both for the models and the readers.2.3 Part-of-speech taggingIn order to produce POS-based surprisal esti-mates, versions of both the training and exper-imental texts with their words replaced by POSwere developed: The BNC sentences were parsedby the Stanford Parser, version 1.6.7 (Klein andManning, 2003), whilst the experimental textswere tagged by an automatic tagger (Tsuruokaand Tsujii, 2005), with posterior review and cor-rection by hand following the Penn TreebankProject Guidelines (Santorini, 1991).
By traininglanguage models and subsequently running themon the POS versions of the texts, unlexicalizedsurprisal values were estimated.2.4 Phrase-structure grammarsThe Treebank formed by the parsed BNC sen-tences served as training data for Roark?s (2001)incremental parser.
Following Frank and Bod(2011), a range of grammars was induced, dif-fering in the features of the tree structure uponwhich rule probabilities were conditioned.
Infour grammars, probabilities depended on the left-hand side?s ancestors, from one up to four levelsup in the parse tree (these grammars will be de-noted a1 to a4).
In four other grammars (s1 tos4), the ancestors?
left siblings were also takeninto account.
In addition, probabilities were con-ditioned on the current head node in all grammars.Subsequently, Roark?s (2001) incremental parserparsed the experimental sentences under each ofthe eight grammars, obtaining eight surprisal val-ues for each word.
Since earlier research (Frank,2009) showed that decreasing the parser?s basebeam width parameter improves performance, itwas set to 10?18 (the default being 10?12).2.5 Recurrent neural networkThe RNN (see Figure 1) was trained in threestages, each taking the selected (unparsed) BNCsentences as training data.7,754 word typesprobability distributionover 7,754 word types400400500200Figure 1: Architecture of neural network languagemodel, and its three learning stages.
Numbers indicatethe number of units in each network layer.Stage 1: Developing word representationsNeural network language models can bene-fit from using distributed word representations:Each word is assigned a vector in a continu-ous, high-dimensional space, such that words thatare paradigmatically more similar are closer to-gether (e.g., Bengio et al 2003; Mnih and Hin-ton, 2007).
Usually, these representations arelearned together with the rest of the model, buthere we used a more efficient approach in whichword representations are learned in an unsuper-vised manner from simple co-occurrences in thetraining data.
First, vectors of word co-occurrencefrequencies were developed using Good-Turing(Gale and Sampson, 1995) smoothed frequencycounts from the training corpus.
Values in thevector corresponded to the smoothed frequencieswith which each word directly preceded or fol-lowed the represented word.
Thus, each wordw was assigned a vector (fw,1, ..., fw,15508), suchthat fw,v is the number of times word v directlyprecedes (for v ?
7754) or follows (for v >7754) word w. Next, the frequency counts weretransformed into Pointwise Mutual Information(PMI) values (see Equation 2), following Bulli-naria and Levy?s (2007) findings that PMI pro-duced more psychologically accurate predictionsthan other measures:401PMI(w, v) = log(fw,v?i,j fi,j?i fi,v?j fw,j)(2)Finally, the 400 columns with the highest vari-ance were selected from the 7754?15508-matrixof row vectors, making them more computation-ally manageable, but not significantly less infor-mative.Stage 2: Learning temporal structureUsing the standard backpropagation algorithm,a simple recurrent network (SRN) learned to pre-dict, at each point in the training corpus, the nextword?s vector given the sequence of word vectorscorresponding to the sentence so far.
The totalcorpus was presented five times, each time withthe sentences in a different random order.Stage 3: Decoding predicted wordrepresentationsThe distributed output of the trained SRNserved as training input to the feedforward ?de-coder?
network, that learned to map the dis-tributed representations back to localist ones.This network, too, used standard backpropaga-tion.
Its output units had softmax activation func-tions, so that the output vector constitutes a prob-ability distribution over word types.
These trans-late directly into surprisal values, which were col-lected over the experimental sentences at ten in-tervals over the course of Stage 3 training (afterpresenting 2K, 5K, 10K, 20K, 50K, 100K, 200K,and 350K sentences, and after presenting the fulltraining corpus once and twice).
These will bedenoted by RNN-1 to RNN-10.A much simpler RNN model suffices for ob-taining unlexicalized surprisal.
Here, we usedthe same models as described by Frank and Bod(2011), albeit trained on the POS tags of ourBNC training corpus.
These models employedso-called Echo State Networks (ESN; Jaeger andHaas, 2004), which are RNNs that do not developinternal representations because weights of inputand recurrent connections remain fixed at ran-dom values (only the output connection weightsare trained).
Networks of six different sizes wereused.
Of each size, three networks were trained,using different random weights.
The best andworst model of each size were discarded to reducethe effect of the random weights.3 Experiment3.1 ProcedureText display followed a self-paced readingparadigm: Sentences were presented on a com-puter screen one word at a time, with onset ofthe next word being controlled by the subjectthrough a key press.
The time between wordonset and subsequent key press was recorded asthe RT (measured in milliseconds) on that wordby that subject.3 Words were presented centrallyaligned in the screen, and punctuation marks ap-peared with the word that preceded them.
A fixed-width font type (Courier New) was used, so thatphysical size of a word equalled number of char-acters.
Order of presentation was randomized foreach subject.
The experiment was time-boundedto 40 minutes, and the number of sentences readby each participant varied between 120 and 349,with an average of 224.
Yes-no comprehensionquestions followed 46% of the sentences.3.2 ParticipantsA total of 117 first year psychology students tookpart in the experiment.
Subjects unable to an-swer correctly to more than 20% of the questionsand 47 participants who were non-native Englishspeakers were excluded from the analysis, leavinga total of 54 subjects.3.3 DesignThe obtained RTs served as the dependent vari-able against which a mixed-effects multiple re-gression analysis with crossed random effects forsubjects and items (Baayen et al 2008) was per-formed.
In order to control for low-level lexicalfactors that are known to influence RTs, such asword length or frequency, a baseline regressionmodel taking them into account was built.
Subse-quently, the decrease in the model?s deviance, af-ter the inclusion of surprisal as a fixed factor to thebaseline, was assessed using likelihood tests.
Theresulting ?2 statistic indicates the extent to whicheach surprisal estimate accounts for RT, and canthus serve as a measure of the psychological ac-curacy of each model.However, this kind of analysis assumes that RTfor a word reflects processing of only that word,3The collected RT data are available for download atwww.stefanfrank.info/EACL2012.402but spill-over effects (in which processing diffi-culty at word wt shows up in the RT on wt+1)have been found in self-paced and natural read-ing (Just et al 1982; Rayner, 1998; Rayner andPollatsek, 1987).
To evaluate these effects, thedecrease in deviance after adding surprisal of theprevious item to the baseline was also assessed.The following control predictors were includedin the baseline regression model:Lexical factors:?
Number of characters: Both physical sizeand number of characters have been foundto affect RTs for a word (Rayner and Pollat-sek, 1987), but the fixed-width font used inthe experiment assured number of charactersalso encoded physical word length.?
Frequency and forward transitional proba-bility: The effects of these two factors havebeen repeatedly reported (e.g.
Juhasz andRayner, 2003; Rayner, 1998).
Given the highcorrelations between surprisal and these twomeasures, their inclusion in the baseline as-sures that the results can be attributed to pre-dictability in context, over and above fre-quency and bigram probability.
Frequencywas estimated from occurrence counts ofeach word in the full BNC corpus (writtensection).
The same transformation (nega-tive logarithm) was applied as for computingsurprisal, thus obtaining ?unconditional?
andbigram surprisal values.?
Previous word lexical factors: Lexical fac-tors for the previous word were included inthe analysis to control for spill-over effects.Temporal factors and autocorrelation:RT data over naturalistic texts violate the re-gression assumption of independence of obser-vations in several ways, and important word-by-word sequential correlations exist.
In order to en-sure validity of the statistical analysis, as well asproviding a better model fit, the following factorswere also included:?
Sentence position: Fatigue and practice ef-fects can influence RTs.
Sentence positionin the experiment was included both as linearand quadratic factor, allowing for the model-ing of initial speed-up due to practice, fol-lowed by a slowing down due to fatigue.?
Word position: Low-level effects of word or-der, not related to predictability itself, weremodeled by including word position in thesentence, both as a linear and quadratic fac-tor (some of the sentences were quite long,so that the effect of word position is unlikelyto be linear).?
Reading time for previous word: As sug-gested by Baayen and Milin (2010), includ-ing RT on the previous word can control forseveral autocorrelation effects.4 ResultsData were analysed using the free statistical soft-ware package R (R Development Core Team,2009) and the lme4 library (Bates et al 2011).Two analyses were performed for each languagemodel, using surprisal for either current or pre-vious word as the dependent variable.
Unlikelyreading times (lower than 50ms or over 3000ms)were removed from the analysis, as were clitics,words followed by punctuation, words follow-ing punctuation or clitics (since factors for pre-vious word were included in the analysis), andsentence-initial words, leaving a total of 132,298data points (between 1,335 and 3,829 per subject).4.1 Baseline modelTheoretical considerations guided the selectionof the initial predictors presented above, but anempirical approach led actual regression modelbuilding.
Initial models with the original set offixed effects, all two-way interactions, plus ran-dom intercepts for subjects and items were evalu-ated, and least significant factors were removedone at a time, until only significant predictorswere left (|t| > 2).
A different strategy wasused to assess which by-subject and by item ran-dom slopes to include in the model.
Given thelarge number of predictors, starting from the sat-urated model with all random slopes generatednon-convergence problems and excessively longrunning times.
By-subject and by-item randomslopes for each fixed effect were therefore as-sessed individually, using likelihood tests.
Thefinal baseline model included by-subject randomintercepts, by-subject random slopes for sentenceposition and word position, and by-item slopes forprevious RT.
All factors (random slopes and fixedeffects) were centred and standardized to avoid403-6.6 -6.4 -6.2 -6 -5.8 -5.6 -5.4 -5.2 -501020304050607012345678 91012341234Lexicalized modelsLinguistic accuracy-2.55 -2.5 -2.45 -2.4 -2.35 -2.3 -2.25 -2.2 -2.15 -2.10510152025301234123 451 2346Unlexicalized modelsPSG-a PSG-s RNN(-average surprisal)Psychological accuracy(??
)Figure 2: Psychological accuracy (combined effect of current and previous surprisal) against linguistic accuracyof the different models.
Numbered labels denote the maximum number of levels up in the tree from whichconditional information is used (PSG); point in training when estimates were collected (word-based RNN); ornetwork size (POS-based RNN).multicollinearity-related problems.4.2 Surprisal effectsAll model categories (PSGs and RNNs) producedlexicalized surprisal estimates that led to a signif-icant (p < 0.05) decrease in deviance when in-cluded as a fixed factor in the baseline, with pos-itive coefficients: Higher surprisal led to longerRTs.
Significant effects were also found for theirunlexicalized counterparts, albeit with consider-ably smaller ?2-values.Both for the lexicalized and unlexicalized ver-sions, these effects persisted whether surprisal forthe previous or current word was taken as the in-dependent variable.
However, the effect size wasmuch larger for previous surprisal, indicating thepresence of strong spill-over effects (e.g.
lexical-ized PSG-s3: current surprisal: ?2(1) = 7.29,p = 0.007; previous surprisal: ?2(1) = 36.73,p 0.001).From hereon, only results for the combined ef-fect of both (inclusion of previous and currentsurprisal as fixed factors in the baseline) are re-ported.
Figure 2 shows the psychological accu-racy of each model (?2(2) values) plotted againstits linguistic accuracy (i.e., its quality as a lan-guage model, measured by the negative aver-age surprisal on the experimental sentences: thehigher this value, the ?less surprised?
the modelis by the test corpus).
For the lexicalized models,RNNs clearly outperform PSGs.
Moreover, theRNN?s accuracy increases as training progresses(the highest psychological accuracy is achievedat point 8, when 350K training sentences werepresented).
The PSGs taking into account sib-ling nodes are slightly better than their ancestor-only counterparts (the best psychological modelis PSG-s3).
Contrary to the trend reported byFrank and Bod (2011), the unlexicalized PSGsand RNNs reach similar levels of psychologicalaccuracy, with the PSG-s4 achieving the highest?2-value.Model comparison ?2(2) p-valuePSG over RNN 12.45 0.002RNN over PSG 30.46 0.001Table 1: Model comparison between best performingword-based PSG and RNN.Although RNNs outperform PSGs in the lexi-calized estimates, comparisons between the bestperforming model (i.e.
highest ?2) in each cate-gory showed both were able to explain varianceover and above each other (see Table 1).
It isworth noting, however, that if comparisons aremade amongst models including surprisal for cur-rent, but not previous word, the PSG is unable404to explain a significant amount of variance overand above the RNN (?2(1) = 2.28; p = 0.13).4Lexicalized models achieved greater psychologi-cal accuracy than their unlexicalized counterparts,but the latter could still explain a small amount ofvariance over and above the former (see Table 2).5Model comparison ?2(2) p-valueBest models overall:POS- over word-based 10.40 0.006word- over POS-based 47.02 0.001PSGs:POS- over word-based 6.89 0.032word- over POS-based 25.50 0.001RNNs:POS- over word-based 5.80 0.055word- over POS-based 49.74 0.001Table 2: Word- vs. POS-based models: comparisonsbetween best models overall, and best models withineach category.4.3 Differences across word classesIn order to make sure that the lexicalized sur-prisal effects found were not limited to closed-class words (as Roark et al 2009, report), a fur-ther model comparison was performed by addingby-POS random slopes of surprisal to the modelscontaining the baseline plus surprisal.
If particu-lar syntactic categories were contributing to theoverall effect of surprisal more than others, in-cluding such random slopes would lead to addi-tional variance being explained.
However, thiswas not the case: inclusion of by-POS randomslopes of surprisal did not lead to a significant im-provement in model fit (PSG: ?2(1) = 0.86, p =0.35; RNN: ?2(1) = 3.20, p = 0.07).65 DiscussionThe present study aimed to find further evidencefor surprisal as a wide-coverage account of lan-guage processing difficulty, and indeed, the re-4Best models in this case were PSG-a3 and RNN-7.5Since best performing lexicalized and unlexicalizedmodels belonged to different groups: RNN and PSG, respec-tively, Table 2 also shows comparisons within model type.6Comparison was made on the basis of previous wordsurprisal (best models in this case were PSG-s3 and RNN-9).sults show the ability of lexicalized surprisal toexplain a significant amount of variance in RTdata for naturalistic texts, over and above thataccounted for by other low-level lexical factors,such as frequency, length, and forward transi-tional probability.
Although previous studies hadpresented results supporting such a probabilisticlanguage processing account, evidence for word-based surprisal was limited: Brouwer et al(2010)only examined a specific psycholinguistic phe-nomenon, rather than a random language sample;Demberg and Keller (2008) reported effects thatwere only significant for POS but not word-basedsurprisal; and Smith and Levy (2008) found aneffect of lexicalized surprisal (according to a tri-gram model), but did not assess whether simplerpredictability estimates (i.e., by a bigram model)could have accounted for those effects.Demberg and Keller?s (2008) failure to find lex-icalized surprisal effects can be attributed both tothe language corpus used to train the languagemodels, as well as to the experimental texts used.Both were sourced from newspaper texts: Astraining corpora these are unrepresentative of aperson?s linguistic experience, and as experimen-tal texts they are heavily dependent on partici-pant?s world knowledge.
Roark et al(2009), incontrast, used a more representative, albeit rela-tively small, training corpus, as well as narrative-style stimuli, thus obtaining RTs less dependenton participant?s prior knowledge.
With such anexperimental set-up, they were able to demon-strate the effects of lexical surprisal for RT ofclosed-class, but not open-class, words, whichthey attributed to their differential frequency andto training-data sparsity: The limited Brown cor-pus would have been enough to produce accurateestimates of surprisal for function words, but notfor the less frequent content words.
A larger train-ing corpus, constituting a broad language sample,was used in our study, and the detected surprisaleffects were shown to hold across syntactic cate-gory (modeling slopes for POS separately did notimprove model fit).
However, direct comparisonwith Roark et als results is not possible: Theyemployed alternative definitions of structural andlexical surprisal, which they derived by decom-posing the total surprisal as obtained with a fullylexicalized PSG model.In the current study, a similar approach to thattaken by Demberg and Keller (2008) was used to405define structural (or unlexicalized), and lexical-ized surprisal, but the results are strikingly differ-ent: Whereas Demberg and Keller report a signif-icant effect for POS-based estimates, but not forword-based surprisal, our results show that lexi-calized surprisal is a far better predictor of RTsthan its unlexicalized counterpart.
This is not sur-prising, given that while the unlexicalized mod-els only have access to syntactic sources of in-formation, the lexicalized models, like the hu-man parser, can also take into account lexical co-occurrence trends.
However, when a training cor-pus is not large enough to accurately capture thelatter, it might still be able to model the former,given the higher frequency of occurrence of eachpossible item (POS vs. word) in the training data.Roark et al(2009) also included in their analysisa POS-based surprisal estimate, which lost signif-icance when the two components of the lexical-ized surprisal were present, suggesting that suchunlexicalized estimates can be interpreted only asa coarse version of the fully lexicalized surprisal,incorporating both syntactic and lexical sourcesof information at the same time.
The results pre-sented here do not replicate this finding: The bestunlexicalized estimates were able to explain ad-ditional variance over and above the best word-based estimates.
However, this comparison con-trasted two different model types: a word-basedRNN and a POS-based PSG, so that the observedeffects could be attributed to the model represen-tations (hierarchical vs. linear) rather than to theitem of analysis (POS vs. words).
Within-modelcomparisons showed that unlexicalized estimateswere still able to account for additional variance,although only reaching significance at the 0.05level for the PSGs.Previous results reported by Frank (2009) andFrank and Bod (2011) regarding the higher psy-chological accuracy of RNNs and the inability ofthe PSGs to explain any additional variance inRT, were not replicated.
Although for the word-based estimates RNNs outperform the PSGs, wefound both to have independent effects.
Further-more, in the POS-based analysis, performance ofPSGs and RNNs reaches similarly high levels ofpsychological accuracy, with the best-performingPSG producing slightly better results than thebest-performing RNN.
This discrepancy in the re-sults could reflect contrasting reading styles inthe two studies: natural reading of newspapertexts, or self-paced reading of independent, nar-rative sentences.
The absence of global context,or the unnatural reading methodology employedin the current experiment, could have led to anincreased reliance on hierarchical structure forsentence comprehension.
The sources and struc-tures relied upon by the human parser to elabo-rate upcoming-word expectations could thereforebe task-dependent.
On the other hand, our re-sults show that the independent effects of word-based PSG estimates only become apparent wheninvestigating the effect of surprisal of the previousword.
That is, considering only the current word?ssurprisal, as in Frank and Bod?s analysis, did notreveal a significant contribution of PSGs over andabove RNNs.
Thus, additional effects of PSG sur-prisal might only be apparent when spill-over ef-fects are investigated by taking previous word sur-prisal as a predictor of RT.6 ConclusionThe results here presented show that lexicalizedsurprisal can indeed model RT over naturalistictexts, thus providing a wide-coverage account oflanguage processing difficulty.
Failure of previ-ous studies to find such an effect could be at-tributed to the size or nature of the training cor-pus, suggesting that larger and more general cor-pora are needed to model successfully both thestructural and lexical regularities used by the hu-man parser to generate predictions.
Another cru-cial finding presented here is the importance ofspill-over effects: Surprisal of a word had a muchlarger influence on RT of the following item thanof the word itself.
Previous studies where lexi-calized surprisal was only analysed in relation tocurrent RT could have missed a significant effectonly manifested on the following item.
Whetherspill-over effects are as important for different RTcollection paradigms (e.g., eye-tracking) remainsto be tested.AcknowledgmentsThe research presented here was funded by theEuropean Union Seventh Framework Programme(FP7/2007-2013) under grant number 253803.The authors acknowledge the use of the UCL Le-gion High Performance Computing Facility, andassociated support services, in the completion ofthis work.406ReferencesGerry T.M.
Altmann and Yuki Kamide.
1999.
Incre-mental interpretation at verbs: Restricting the do-main of subsequent reference.
Cognition, 73:247?264.Mark Andrews, Gabriella Vigliocco, and David P. Vin-son.
2009.
Integrating experiential and distribu-tional data to learn semantic representations.
Psy-chological Review, 116:463?498.R.
Harald Baayen and Petar Milin.
2010.
Analyzingreaction times.
International Journal of Psycholog-ical Research, 3:12?28.R.
Harald Baayen, Doug J. Davidson, and Douglas M.Bates.
2008.
Mixed-effects modeling with crossedrandom effects for subjects and items.
Journal ofMemory and Language, 59:390?412.Moshe Bar.
2007.
The proactive brain: usinganalogies and associations to generate predictions.Trends in Cognitive Sciences, 11:280?289.Douglas Bates, Martin Maechler, and Ben Bolker,2011.
lme4: Linear mixed-effects models usingS4 classes.
Available from: http://CRAN.R-project.org/package=lme4 (R package version0.999375-39).Yoshua Bengio, Re?jean Ducharme, Pascal Vincent,and Christian Jauvin.
2003.
A neural probabilis-tic language model.
Journal of Machine LearningResearch, 3:1137?1155.Marisa Ferrara Boston, John Hale, Reinhold Kliegl,Umesh Patil, and Shravan Vasishth.
2008.
Parsingcosts as predictors of reading difficulty: An evalua-tion using the potsdam sentence corpus.
Journal ofEye Movement Research,, 2:1?12.Harm Brouwer, Hartmut Fitz, and John C. J. Hoeks.2010.
Modeling the noun phrase versus sentencecoordination ambiguity in Dutch: evidence fromsurprisal theory.
In Proceedings of the 2010 Work-shop on Cognitive Modeling and ComputationalLinguistics, pages 72?80, Stroudsburg, PA, USA.John A. Bullinaria and Joseph P. Levy.
2007.
Ex-tracting semantic representations from word co-occurrence statistics: A computational study.
Be-havior Research Methods, 39:510?526.Vera Demberg and Frank Keller.
2008.
Data from eye-tracking corpora as evidence for theories of syn-tactic processing complexity.
Cognition, 109:193?210.Stefan L. Frank and Rens Bod.
2011.
Insensitivity ofthe human sentence-processing system to hierarchi-cal structure.
Psychological Science, 22:829?834.Stefan L. Frank.
2009.
Surprisal-based comparisonbetween a symbolic and a connectionist model ofsentence processing.
In Proceedings of the 31st An-nual Conference of the Cognitive Science Society,pages 1139?1144, Austin, TX.Stefan L. Frank.
2012.
Uncertainty reduction as ameasure of cognitive processing load in sentencecomprehension.
Manuscript submitted for publica-tion.Peter Hagoort, Lea Hald, Marcel Bastiaansen, andKarl Magnus Petersson.
2004.
Integration of wordmeaning and world knowledge in language compre-hension.
Science, 304:438?441.John Hale.
2001.
A probabilistic earley parser as apsycholinguistic model.
In Proceedings of the sec-ond meeting of the North American Chapter of theAssociation for Computational Linguistics on Lan-guage technologies, pages 1?8, Stroudsburg, PA.Herbert Jaeger and Harald Haas.
2004.
Harnessingnonlinearity: predicting chaotic systems and savingenergy in wireless communication.
Science, pages78?80.Barbara J. Juhasz and Keith Rayner.
2003.
Investigat-ing the effects of a set of intercorrelated variables oneye fixation durations in reading.
Journal of Exper-imental Psychology: Learning, Memory and Cogni-tion, 29:1312?1318.Marcel A.
Just, Patricia A. Carpenter, and Jacque-line D. Woolley.
1982.
Paradigms and processesin reading comprehension.
Journal of Experimen-tal Psychology: General, 111:228?238.Yuki Kamide, Christoph Scheepers, and Gerry T. M.Altmann.
2003.
Integration of syntactic and se-mantic information in predictive processing: cross-linguistic evidence from German and English.Journal of Psycholinguistic Research, 32:37?55.Alan Kennedy and Joe?l Pynte.
2005.
Parafoveal-onfoveal effects in normal reading.
Vision Research,45:153?168.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Meeting of the Association for ComputationalLinguistics,, pages 423?430.Kestutis Kveraga, Avniel S. Ghuman, and Moshe Bar.2007.
Top-down predictions in the cognitive brain.Brain and Cognition, 65:145?168.Roger Levy.
2008.
Expectation-based syntactic com-prehension.
Cognition, 106:1126?1177.Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of English: the Penn Treebank.
Com-putational Linguistics, 19:313?330.Scott A. McDonald and Richard C. Shillcock.
2003.Low-level predictive inference in reading: the influ-ence of transitional probabilities on eye movements.Vision Research, 43:1735?1751.Andriy Mnih and Geoffrey Hinton.
2007.
Three newgraphical models for statistical language modelling.Proceedings of the 25th International Conference ofMachine Learning, pages 641?648.Keith Rayner and Alexander Pollatsek.
1987.
Eyemovements in reading: A tutorial review.
In407M.
Coltheart, editor, Attention and performanceXII: the psychology of reading., pages 327?362.Lawrence Erlbaum Associates, London, UK.Keith Rayner.
1998.
Eye movements in reading andinformation processing: 20 years of research.
Psy-chological Bulletin, 124:372?422.Brian Roark, Asaf Bachrach, Carlos Cardenas, andChristophe Pallier.
2009.
Deriving lexical and syn-tactic expectation-based measures for psycholin-guistic modeling via incremental top-down parsing.In Proceedings of the 2009 Conference on Empiri-cal Methods in Natural Language Processing: Vol-ume 1 - Volume 1, pages 324?333, Stroudsburg, PA.Brian Roark.
2001.
Probabilistic top-down parsingand language modeling.
Computational Linguis-tics, 27:249?276.Beatrice Santorini.
1991.
Part-of-speech taggingguidelines for the Penn Treebank Project.
Technicalreport, Philadelphia, PA.Nathaniel J. Smith and Roger Levy.
2008.
Optimalprocessing times in reading: a formal model andempirical investigation.
In Proceedings of the 30thAnnual Conference of the Cognitive Science Soci-ety, pages 595?600, Austin,TX.Andreas Stolcke.
1995.
An efficient probabilisticcontext-free parsing algorithm that computes prefixprobabilities.
Computational linguistics, 21:165?201.Yoshimasa Tsuruoka and Jun?ichi Tsujii.
2005.
Bidi-rectional inference with the easiest-first strategy fortagging sequence data.
In Proceedings of the con-ference on Human Language Technology and Em-pirical Methods in Natural Language Processing,pages 467?474, Stroudsburg, PA.408
