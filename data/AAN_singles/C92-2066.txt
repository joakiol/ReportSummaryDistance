Grammaires Stochastiques Lexicalisdes d'Arbres AdjointsRdsumd du papierStochastic Lexicalized Tree-adjoining GrammarsYves SchabesMotivationsLes techniques toch&stiques b4n6ficient aujourd'lmid'un regain de popularit4.
Cependant, les modulesstochastiques utilis~s ont clairement inaddquats pourl'analyse syntaxique des langues naturelles.
Les for-malismes probabilistes qui out dr6 propos4s dans le do~maine de la th4orie de la communication (processus deMarkov et n-grammes) (Pratt, 1942; Shannon, 1948;Shannon, 1951) ont ~te rapidement r6fut6s en linguis-tique.
En effet, ces modules ont incapables de d$crire lasyntaxe de mani~re hi4rarchique (sous forint d'arbre).De plus, les ph6nomSnes portant sur de longues dis-tances ne peuvent pas fitre pris en compte par ces for-malismes.
Les grammaires stochastiques hors coutexte(Booth, 1969) permettent d%laborer une descriptionhi4rarchique de la syntaxc.
Ccpendant, aucune ap-proche utilisant les grammaires stoctlastiques hors con-texte (Lari and Young, 1990; Jelinek, Lafferty, and Mer-cer, 1990) est en pratique aussi efllcace que les processusde Markov ou les n-grammes.
Eu effet, les rSgles horscontexte ne sont pas directement sensibles au mot etdone ?
une distribution de mots.Grammaires Stochastiques Lexi-calis~es d'Arbres AdjointsLes grammaires lexicalisdes d'arbres adjoiuts consistentd'un ensemble d'arbres, chacun a.ssoci4 ?
un mot.
Ellespermettent de localiser la plupart des contraiutes syn-taxiques (par exemple, sujet-verbe, verbe-objet) touten ddcrivant la syntaxe sous forme d'arbres.Dans cc papicr, la notion de derivation des gram-maires lexicalisdes d'arbres adjoints (tree-adjoininggrammars) est modifi6e au cas de derivatious stochas-tiques.
Le nouveau formalisme, les grammaires stochas-tiques lexicalisdes d'arbres adjoints (stochastic lexical-ized tree-adjoining grammars ou SLTAG) , a des pro-pridtds uniques car il maintient la notion de distributioncntrc mot tout en manipulant la syntaxe de maniSrehi6rarchique.AlgorithmesUn algorithme pour calculer la probabilitd 'une phraseest pr4senter dans le papier.Ensuite, un algorithme qui permet de r4estimer lesparam~tres d'une grammaire stochastique l xicalisded'arbres adjoints est ddcrit.
Cette algorithme per-met de r~estimer les param~tres de fa~on 5. aug-menter apr~s chaque it6ration la probabilit6 du cor-pus.
Cette algorithme peut 6tre utilis6 comme algo-rithme d'apprentissage.
La grammaire initiale d'entrdeg4n~re tous les roots de routes les faqons possibles.L'algorithme permct ensuite d'inf4rer unc grammaireb.
partir du corpus.Evaluation ExpdrimentaleNous avons testd l'algorithme de r$estimation sur uncorpus artificiel (Figure 1) et aussi sur les sequencesde parties du discours (Figure 2) du corpus 'ATIS'(Hemphill, Godfrey, and Doddington, 1990).
Dans lesdeux cas, l'algorithme pour les grammaires tochas-tiques lexicalis~es d'arbres adjoints converge plus rapi-dement que celui pour les grammaires hors contexte(Baker, 1979).
Ces expdriences confirment le fait queles grammaires stochastiques lexicalisdes d'arbres ad-joints permettent de mod~liser des distributions entreroots que les grammaires stochastiques hors contexte nepeuvent pas exprimer.1.81.61.41.210.80.60.4, , , , , , , ,SLTAG - -\\ SCFG ..... -" \ \\t t t I I I I I2 3 4 5 6 7 8 9 i0i terat ionFigure 1: Convergence avec un corpus (le phr~qes dulanguage {a"b"ln > 0}i \[ lSLTAG - -SCFG .....5 i0 15 20 25itorationFigure 2: Convergence sur le ATIS CorpusACheS Dr-; COLING-92.
NANTES, 23-28 AOUT 1992 4 2 5 PROC.
OF COLING-92, NANTES, AUG. 23-28, 1992Stochast ic  Lexical ized Tree-Adjo in ing Grammars  *Yves SchabesDept .
of  Computer  & In format ion  Sc ienceUn ivers i ty  of  Pennsy lvan iaPh i lade lph ia ,  PA  19104-6389,  USAschabes@unagi, cis.
upenn, eduAbstractThe notion of stochastic lexicalized tree-adjoininggrammar (SLTAG) is formally defined.
The parametersof a SLTAG correspond to the probability of combiningtwo structures each one associated with a word.
Thecharacteristics of SLTAG are unique and novel since it islexieally sensitive (as N-gram models or Hidden MarkovModels) and yet hierarchical (as stochastic ontext-freegrammars).Then, two basic algorithms for SLTAG arc intro-duced: an algorithm for computing the probability of asentence generated by a SLTAG and an inside-outside-like iterative algorithm for estimating the parametersof a SLTAG given a training corpus.Finally, we should how SLTAG enables to define alexicalized version of stochastic ontext-free grammarsand we report preliminary experiments showing some ofthe advantages of SLTAG over stochastic ontext-freegrammars.1 Mot ivat ionsAlthough stochastic techniques applied to syntax mod-eling have recently regained popularity, current lazl-guage models uffer from obvious inherent inadequacies.Early proposals uch as Markov Models, N-gram mod-els (Pratt,  1942; Shannon, 1948; Shannon, 1951) andtlidden Markov Models were very quickly shown to belinguistically not appropriate for natural language (e.g.Chomsky (1964, pages 13-18)) since they are unable tocapture long distance dependencies or to describe hier-archically the syntax of natural anguages.
Stochasticcontext-free granunar (Booth, 1969) is a hierarchicalmodel more appropriate for natural languages, howevernone of such proposals (Lari and Young, 1990; Jelinek,Lafferty, and Mercer, 1990) perform as well as the sim-pler Markov Models because of the difficulty of captur-ing lexical information.
The parameters of a stochas-tic context-free grammar do not correspond irectly toa distribution over words since distributional phenom-ena over words that are embodied by the application of*This work was partially supported by DARPA Grant N0014-90-31863, ARO Grant DAAL03-89-C-0031 and NSF Grant 1RI90-16592.
We thank Aravind Joshi for suggesting the use of TAGsfor statistical nalysis during a private discussion that followed apresentation bybS'ed Jdinek during the June 1990 meeting of theDARPA Speech and Natural Language Workshop.
We are alsograteful to Peter Braun, FYed Jelinek, Mark Liberman, MitchMarcus, Robert Mercer, Fernando Pereira said Stuart Shieber forproviding vMu~ble comments.more than one context-free rule cannot be captured un-der the context-freeness a sumption.
This leads to thedifficulty of maintaining a standard hierarchical modelwhile capturing lexieal dependencies.This fact prompted researchers in natural languageprocessing to give up hierarchical language models inthe favor of non-hierarchical statistical models overwords (such as word N-grams models).
Probably forlack of a better language model, it has also been ar-gued that the phenomena that such devices cannot cap-ture occur relatively infrequently.
Such argumentationis linguistically not sound.Lexicalized tree-adjoining grammars (LTAG) t com-bine hierarchical structures while being hxieany sensi-tive and are therefore more appropriate for statisticalanalysis of language.
In fact, LTAGs are the simplesthierarchical formalism which can serve as the basis forlexicalizing context-free grammar (Schabes, 1990; Joshiand Sehabes, 1991).LTAG is a tree-rewriting system that combines treesof large domain with adjoining and substitution.
Thetrees found in a TAG take advantage of the available x-tended domain of locality by localizing syntactic depen-dencies (such as finer-gap, subject-verb, verb-objeet)and most semantic dependencies ( uch as predicate-argument relationship).
For example, the followingtrees can be found in a LTAG lexicon:S /kNP,L VIP VPAV NPI NP NP VP* ADVL I I Iuts J~n p~nutJ hungrilySince the elementary trees of a LTAG are minimalsyntactic and semantic units, distributional analysis ofthe combination of these elementary trees based on atraining corpus will inform us about relevant statisticalaspects of the language such as the classes of wordsappearing as arguments of a predicative lement, thedistribution of the adverbs licensed by a specific verb,or the adjectives licensed by a specific noun.This kind of statistical analysis as independently sug-gested in (Resnik, 1991) can be made with LTAGs be-cause of their extended omain of locality but also be-cause of their lexiealized property.lWe attallnle familiarity throughout the paper with TAGs andits lexicallzed variant, See, for instance, (Joehl, 1987), (Schabes,Abeill~, and Joehi, 1988), (Schabes, 1990) or (Joslfi and Schabes,1~1).ACTES DE COLING-92.
NANTES, 23-28 AOUT 1992 4 2 6 PROC.
OF COLING-92, NANTES, AUG. 23-28, 1992In this paper, this intuition is made formally preciseby defining the notion of a stochastic lexicalized tree-adjoining rammar (SLTAG).
We present an algorithmfor computing the probability of a sentence generatedby a SLTAG, and finally we introduce an iterative algo-r ithm for estimathlg the parameters of a SLTAG givena training corpus of text.
This algorithm can eitherbe used for refining the parameters of a SLTAG or forinferring a tree-adjoining grammar frmn a training cor-pus.
We also report preliminary experiments with thisalgorithm.Due to the lack of space, in this paper tim algorithmsare described succinctly without proofs of correctnessand more attention is given to tile concepts and tech-niques used for SLTAG.2 SLTAGhfformally speaking, SLTAGs are defined by assigninga probability to tile event that an elementary tree iscombined (by adjunction or substitution) on a specificnode of another elementary tree.
These events of com-bination are the stochastic processes considered.Since SLTAG are defined on the basis of the deriva-tion and since TAG allows for a notion of derivationindependent from the trees that are derived, a precisemathematical definition of the SLTAG derivation mustbe given.
For this purpose, we use stochastic linear in-dexed grammars (SLIG) to formally express SLTAGsderivations.Linear Indexed grammar (LIG) (Alto, 1968; Gazdar,1985) is a rewriting system in which the non-terminalsymbols are augmented with a stack, in addition torewriting non-terminals, the rules of the grammar canhave the effect of pushing or popping symbols on top oftile stacks that are associated with each non-terminalsymbol.
A specific rule is triggered by the non-termlnalon the left hand side of the rule and the top element ofits associated stack.The productions of a LIG are restricted to copy thestack corresponding to tile non-terminal being rewrit-ten to at most one stack associated with a non-terminalsymbol on tile right hand side of the production?In tile following, \[..p\] refers to a possibly unboundedstack whose top element is p and whose remaining partis schematically written as '..'.
\[$\] represents a stackwhose only element is the bottom of the stack.
While itis possible to define SLIGs in general, we define them forthe particular case where the rules are binary branchingand where tile left hand sides are always incomparable.A stochastic linear indexed grammar, G, is denotedby (VN, VT, VI, S, Prod), where VN is a finite set of non-terminal symbols; VT is a finite set of terminal symbols;VI is a finite set of stack symbols; S E VN is the startsymbol; Prod is a finite set of productions of the form:Xo\[$po\] --* aXo\[..po\] --.
x~\[..m\] x~\[$p~\]x0\[..po\] -~ Xl\[$pd x~\[-.p~\]Xo\[$Po\] --~ Xl\[$pl\] X2\[$p2\]where Xk E Vjv, a E VT and po ~.
VI, Pl,P2 E V\[; P, aprobability distribution which assigns a probability, 0 <P(X\[..z\] ~ A) < 1, to a rule, X\[..x\] -* A ~.
Prodsuch2LIGs have been shown to be weakly eqtfivalent to "Ibee-Adjoining Graramars (V~jay-Shanker, 1987).that tbe sum of the probabilities of all the rules that canbe applied to any non-terminal nnotated with a stackis equal to one.
More precisely if, VX E VN,Vp E VI:~ p(xt..pl -~ A) = 1AP(X \[..p\] --* A) should be interpreted as the probabilitythat X\[..p\] is rewritten as A.A derivation starts from S associated with the emptystack (S\[$\]) and each level of the derivation must bevalidated by a production rule.
The language of a SLIGis defined as follows: L = {w E VT~ \[ S\[$\]~w}.The probability of a derivation is defined as the prod-uct of tile probabilities of all individual rules involved(counting repetition) in the derivation, the derivationbeing validated by a correct configuration of the stackat each level.
The probability of a sentence is then com-puted as the sum of the probabilities of all derivationsof tile sentence.Following tile construction described in (Vijay-Shanker and Weir, 1991), given a LTAG, Glaa, we con-struct an equivalent LIG, G,ua.
Tile constructed LIGgenerates tile same language as Gtag and each deriva-tion of Gtaa corresponds to a unique LIG derivationcorresponds to a unique derivation in G,ua (and con-versely).
In addition, a probability is assigned to eachproduction of the LIG.
For simplicity of explanationand without loss of generality we assume that each nodein an elementary tree in Gt,9 is either a leaf node (i.e.either a foot node or a non-empty terminal node) orbinary branching, a The construction of the equivalentSLIG follows.The non-terminal symbols of Gstia are the two sym-bols 'top' (t) and 'bottom' (b), tile set of terminal sym-bols is the same as the one of Gta9, the set of stacksymbols is the set of nodes (not node labels) found inthe elementary trees of Gla~ augmented with the bot-tom of tile stack ($), and tile start symbol is ' top'  (t).For "all root nodes ~10 of an initial tree whose root islabeled by S, the following starting rules are added:t\[$\] ~ t\[$,t0\] (1)These rules state that a derivation must start from thetop of the root node of some initial tree.
P is the prob-ability that a derivation starts from the initial tree as-sociated with a lexical item and rooted by %.Then, for all node '/ in an elementary tree, the fol-lowing rules are generated.?
If rhT/2 are ttle 2 children of a node r/sucb that r/2 ison the spine (i.e.
subsumes tile foot node), include:b\[..~l ~&' tI$n, lt\[-.,~l (2)Since (2) encodes an immediate domination link de-fined by the tree-adjoining rammar,  its associatedprobability is one.?
Similarly, if thT/~ are the 2 children of a node r/suchthat r h is on the spine (i.e.
subsumes the foot node),include:b\[..rt\] P=-*~ t\["rl~\]t\[$~\] (3)Since (3) encodes a~t immediate domination link de-fined by the tree-adjoining rammar,  its associatedprobability is one.aThe algorlthnm explained ill this paper cart be generalized tolexicadized tree-adjoining granunars that need not be in ChottmkyNormal Form using techniqu?~ similar the one found in (Schabet,1991).ACIES DE COLING-92, NANTES, 23-28 AO~rf 1992 4 2 7 P~oc.
OF COLING-92, NANTES, AUG. 23-28, 1992* If ~/tT/2 are the 2 children of a node q such that  noneof them is on the spine, include:b\[$~\] p~l \]~\[$I~1\]t\[$i~2 \] (4)Since (4) also encodes an immediate domination linkdefined by the tree-adjoining grammar,  its associatedprobability is one.?
If 7?
is a node labeled by a non-terminal symbol andif it does not have an obligatory adjoining constraint,then we need to consider the case that adjunetionmight not take place.
In this ease, include:t\[..~\] L b\[..~\] (5)The probabil ity of rule (5) corresponds to the proba-bility that  no adjunetion takes place at node q.o If t/ is an node on which the auxiliary tree fl canbe adjoined, the adjunetiou of fl can be predicted,therefore (assuming that ~tr is the root node of fl)include:t\["0\] L t\[..rl,,\] (6)The probability of rule (6) corresponds to the proba-bility of adjoining the auxiliary tree whose root nodeis ~/~, say/3, on the node 0 belonging to some elemen-tary tree, say a.4?
If r)!
is tim foot node of an auxiliary tree fl that hasbeen adjoined, then the derivation of the node belowq\] must resume.
In this case, include:b\["0l\] ,~1 b\[..\] (7)The above stochastic production is included withprobabil ity one since the decision of adjunction hasalready been made in rules of the form (6).?
Finally, if r h is the root node of an initial tree thatcan be substituted on a node marked for substitutionr), include:t\[$~\] L t\[S~t\] (g)Here, p is the probability that the initial tree rootedby ~/~ is substituted at node q.
It corresponds tothe probability of substituting the lexicalized initialtree whose root node is 71, say 6, at the node q of alexicalized elementary tree, say a.
5The SLIG constructed as above is well defined if thefollowing equalities hold for all nodes ~l:P(t\[..~/\] ---* b\[..~/\]) + E P(t\[..~/\] --* t\[..q0~\] ) = 1 (9)P(t\[$~/\] ---* t\[$Ol\]) ---- 1 (10)E P(t\[$\] -~ t\[$O0\]) = 1 (11)4Since the granmmr is lexicalized, both trees a and /3 are a~sociated with lexical iter~s, mad the site node for adjtmction ~correuponds to some syntactic modification.
Such llde encapsu-lates S modifiers (e.g.
s~tential adverbs as in "apparently Johnleft"), VP modifiers (e.g.
verb phr~e adverbs as in "John leftabruptly}", NP modifiers (e.g.
relative clauses as in "The manwho left was happy"), N modifiers (e.g.
adtieetive~ asin "preltywoman"), or even sententiM complements (e.g.
John think8 thatHarry is sick).s Among other cases, the probability of thi~ rule corr~ponds tothe probability of filling some argument p(~ition by a lexiealizedtree.
It will encapsulate he distribution for Belectional restrictionsince the position of substitution is taken into account.A gramular satisfying (12) is called consistent.
6E P ( t \ [$ \ ]~w)= 1 (12)wEZ*Beside the distributional phenomena that we mentionedearlier, SLTAG also captures the effect of adjoining con-straints (selective, obligatory or null adjoining) whichare required for tree-adjoining rammar .
73 A lgor i thm for Comput ing  theProbab i l i ty  of  a SentenceWe now define an bottom-up algorithm for SLTAGwhich computes the probability of an input string.
Thealgorithm is an extension of the CKY-type parser fortree-adjoining grammar  (Vijay-Shanker, 1987).
The ex-tended algorithm parses all spans of the input stringand also computes tbelr probability in a bottom-upfashion.Since the string on the frontier of an auxiliary is bro-ken up into two substrings by the foot node, for thepurpose of computing the probability of the sentence,we will consider the probability that  a node derives twosubstrings of the input string.
This entity will be calledthe inside probability.
Its exact definition is given be-low.We will refer to the subsequenee of the input stringw = ax "" aN from position i to j ,  w{'.
It is defined asfollows:w~/'~f { a i+t"  .uj , i f i>_ j '  i f /<  jGiven a string w = at .
.
.
a N and a SLTAG rewrittenas in (1-8) the inside probability, F (pos ,  71, i, j ,  k,l), isdefined for all nodes 7/ contained in an elementary treeand for pos E {t,b}, and for all indices 0 < i < j <k < I < N as follows:(i) If the node 7/does not subsume the foot nodeof (~ (if there is one), then j and k are un-bound and:l~ (pos, ~, i , - ,  - ,  I) d~=l P(pos\[$@~ w~)(it) If the node y/subsumes the foot node 7/!
of e,then:l~ (pos, rL i, j, k, l) a~l P ( pos \ [$@~ w{ b\[$o l lw~ )In (ii), only the top element of the stack matters  inceas a consequence of the eonstrnction of the SLIG, wehave that  if pos\[$tl\]~ w~b\[$rll\]w ~ then for all string7 e V/~ we also have pos\[$Tr/\]~ w~b\[$7~l\]w~.SInitially, all inside probabilities are set to zero.
Then,the computat ion goes bottom-up start ing from the pro-ductions introducing lexieal items: if r/ is a node suchthat  b\[$7/\] --~ a, then:1 i f l= i+ lAa=w~ +t (1~IW(b 'T l ' i ' - ' - ' l )  = 0 otherwise.Then,  the inside probabilities of larger substrings arecomputed bottom-up relying on the recurrence qua-~We will not investigate im conditions under which (12) holds.We conjecture that the techniques used for dmcking the eolmis-tency of stochastic context-free grammars (Booth and Thomp6on,1973)  can  be adapted to SLTAG.r For example, for a given node 0 setting to zero the probabilityo\[ all rules of the forts (6) ht~ the effect of blocking adjunction.8Thls can be seen by obae~.ing that for any node on the pathfrom the root node to the foot node of an auxiliary tree, the stackremains unchanged.ACRES DE COLING-92, NANTES.
23-28 AOt~T 1992 4 2 8 PROC.
OF COLING-92, NANTES.
AUG. 23-28, 1992lions stated in Appendix A.
This computation takesin the worst case O(IGl~N6)-time and O(IGINa)-spacefor a sentence of lengtb N.Once the inside probabilities cmnputed, we obtainthe probability of the sentence flu follows:P(w)aJP(t\[$\]~,~) = Z~(t, $, 0 , - , - ,  Iwl) (14)Wc now consider the problem of re-estimating aSI,TAG.4 Ins ide -Ous ide  A lgor i thm for1%eest imat ing  a SLTAGGiven a set of positive example sentences, W ={wt ' "wK},  we would like to compute the probabil-ity of each rule of a given SLTAG in order to maximizethc probability that the corpus were generated by thisSLTAG.
An algorithm solving this problem can be usedin two different ways.The first use is as a reestimation algorithm.
In ttfisapproach, the input SI,'1'A(~ derives structures that arcreasonable according to some criteria (such as a linguis-tic theory and some a priori kuowledge of the corpus)and the intended use of the algorithm is to refine theprobability of each rule.The second use is as a learning algorithm.
At the firstiteration, a SLTAG which generates all possible struc-tures over a given set of nodes and terminal symbols isused.
Initially the probability of each rule is randomlyassigned and then tile algorithm will re-estimate tbeseprobabilities.Informally speaking, given a first estimate of the pa-rameters of a SLTAG, the algorithm re-estimates theseparameters on the basis of the parses of each sentence ina training corpus obtained by a CKY-tyt)e parser.
Thealgorithm is designed to derive a new estimate aftereach iteration such that the probability of the corpusis increased or equivalently such that tile cross entropyestimate (negative log probability) is decreased:log~(e(r0))l t (W,G) - weW (15)wEWIn order to derive a new estimate, the algorithmneeds to compute for all seutences in W the in-side probabilities and the outside probabilities.
Givena string w = al .
.
.aN,  tbe outside probability,0 ~ (pos, ~, i, j, k, It, is defined for all nodes r I containedin an elementary tree a and for pos E {t,b}, and for allindices 0 < i < j < k < l < N as follows:(it If the node r/does not subsume the foot nodeof a (if there is one), then j and k axe un-bound asld: ..de\] O'?
(P os, O, i, - ,  - ,  t) -P(B"/ C V~ s.t.
t\[$\]=~ Wio pos\[$Ttl\] w~)(ii) If the node ~/does ubsume the foot node ~/!of a then:0 '~ (pos, O, i, j, k, l) aeJ-/'(37 ~ V~* s.t.t \ [$\]~ Wlo pos\[$Trl\] w~ and b\[$7~ll\]~w\])Once the inside probabilities computed, the outsideprobabilities can be computed top-down by consider-ing smaller spans of the input string starting withO"( t ,$ ,O , - , - ,N )  = 1 (by definition).
This is doneby computing the recurrence quations tated in Ap-pendix B.In the following, we assume that r I subsumes the footnode r/l within a same elementary tree, and also that tllsubsumes the foot node ~111 (within a same elementarytree).
The other cases are handled similarly.
Table 1shows the reestimation formulae for the adjoining rules(16) and the null adjoining rules (17).
(16) corresponds to the average number of time thattl .
.
.
.
le L\[..T1\] .-* t\[..yqv\] is used, and (17) to th .
.
.
.
.
.age number of times no adjunction occnrred on T/.
Thedenominators of (16) and of (17) estimate the averagenumber of times that a derivation involves tlLe expan-sion oft\[-.~/\].
The numerator of(16) estimates the aver-age number of times that a derivation involves the rulet\[.-7/\] -~ t\[..Tirfl\].
Therefore, for example, (16) estimatesthe probability of using the rule/\['-~7\] ~ l\["rplt\].The algorittun reiterates until H(W, G) is unchanged(within some epsilon) between two iterations.
Each it-eration of the algoritbm requires at most O(IGIN e)time for each sentence of length N.5 Grammar  In ference  w i thSLTAGThe reestimation algorithm explained in Section 4 canbe used botll to reestimate the paramcters for a SI,TAGderived by some other mean or to infer a grammar fromscratch.
Ill the following, we investigate grammar In-ference from scratch.The initial grammar for the reestimation algoritiimconsists of all SLIG rules for the tress ill Lexical-ized Normal I~brm (ill short LNF) over a given set= {aill .< i _< T} of terminal symbols, with suit-ably assigned non zero probability: 9S 0 $4s h t~ a iThe above normal form is capable not only to de-rive any lexicalized tree-adjoining language, but alsoto impose ally binary bracketing over the strings of thelanguage.
The latter property is important as we wouldlike to be able to use bracketing information in the ilL-put corpus as in (Pereira and Schabes, 1992).The worst case complexity of tim reestimation algo-r ithm given iu Section 4 with respect o the length ofthe input string (O(NS)) makes this approach in gen-eral impractical for LNF grammars.However, if only trees of the form fit a' and a~" (oronly of tile form /~'  and a~) ,  the language generatedis a context-free language and can be handled moreefficiently by the reestimation algorithnL9Adjoining constraints can be u~d in tiffs normal form, Theywill be reflected in the SLIG eq~vaient grammar.
Indices havebeen added on S nodes in order to be able to uniquely refer toeach node in the granunar.AcrEs OE COLING-92, NANTES.
23-28 AOOT 1992 4 2 9 DROC.
OF COLING-92, NANTES, AUG. 23-28, 1992wwPW ) x QW(t\[..~/\] ~ t\[.-r/rp\]) P(t\[-.t/\] ---, t\[..~Tt/t\]) = 1 (16)~wp--- ~ x \[R~0/) + ~_~O'~(t\[..O\] --, t\[..~/r/,\])\]1to~w /3(t\[..r/\] ---+ b\[..~/\]) = 1 (17)Ot?
(t\["r/\] ~ t\["r/rY\]) = Z P(t\["O\]--*t\["O~Y\])?Iw(t'o/ ' i ' r 's ' l )xlW(b'o'r ' j 'k 's)xOW(t'~l' i ' j 'k ' l )  (18)i)r,j~k,t)l/~w(r/) = ~ P(t\[..r/\] ~ b\[..r/\]) x l~(t ,o, i , j ,k , l )  x O~?
(b,)l,i,j,k,l) (19)i,j,k,ITable 1: Keestimation of adjoining rules (16) and null adjoining rules (17)It can be shown that if, only trees of the form ~a~ and~a~ are considered, the reestimation algorithm requiresin the worst case O(Na)-t ime) ?The system consisting of trees of the form ~'  and c~can be seen as a stochastic lexicalized conle~:t-free gram-mars since it generates exactly context-free languageswhile being lexically sensitive.In the following, due to the lack of space, we reportonly few experiments on grammar inference using theserestricted forms of SLTAG and the reestimation algo-rithm given in Section 4.
We compare the results ofthe TAG inside-outside algorithm with the results ofthe inside-outside algorithm for context-free grammars(Baker, 1979).These preliminary experiments suggest that SLTAGachieves faster convergence (and also to a better solu-tion) than stochastic ontext-free grmnmars.5.1 In ferr ing the  Language {a"b"\]n > 0}We consider first an artificial language.
The train-ing corpus consists of 100 sentences in the languageL = {a"b'~ln > 0} randomly generated by a stochasticcontext-free grammar.The initial grammar consists of the trees ~' ,  fl~, c~ aand ab with random probability of adjoining and nulladjoining.The inferred grammar models correctly the languageL.
Its rules of the form (I), (5) or (fi) with high prob-ability follow (any excluded rule of the same form hasprobability at least l0 -a3 times lower than the rulesgiven below).
The structural rules of the form (2), (3),(4) or (7) are not shown since their probability alwaysremain 1.Z?This can be Been by ol~ervin g that, for exaanple inl(posji, i,j,k,I), it i~ nece~y the ea~ that k = l, nnd alsoby noting that k is superfluous.t\[$,Tg\] s:~4 t\[S,lg,78\]t\[$og\] o_~ t\[$,lg,lg\]t\[.-t/~\] z_~,o b\[,.~7~\]t \ [~\ ]  ,..~o b\[,~\]t\[..~\] ~,?
b \ [~\ ]t\[..o~\] 1~0 b\[..o~\]In the above grammar, a node S'k in a tree c~ a or /~associated with the symbol a is referred as t/~, and anode S~ in a tree associated with b as r/~.We also conducted a similar experiment withthe inside-outside algorithm for context-free grammar(Baker, 1979), starting with all pc~sible Chomsky Nor-mal Form rules over 4 non-terminals and the set of ter-minal symbols {a,b} (72 rules).
The inferred grammardoes not quite correctly model the language L. Fur-thermore, the algorithm does not converge as fast as inthe case of SLTAG (See Figure 1).1.81 .61,41.210 .80 .60.4I I I I I I I ISLTAG - -SCFG .
.
.
.
.  "
\2 3 4 5 6 7 8 9 1 0iterationFigure 1: Convergence for the Language {anb"ln > 0}5.2 Exper iments  on the ATIS CorpusWe consider the part-of-speech sequences of the spoken-language transcriptions in the Texas Instruments sub-ACT~ BE COIANG-92.
NANTES, 23-28 AO~' 1992 4 3 0 PROC.
OF COLING-92, NANTES, AUG. 23-28, 1992set of the Air Travel hfformation System (ATIS) corpus(Hemphill, Godfrey, and Doddington, 1990).
This cor-pus is of interest since it has been used for infcrringstochastic ontext-free grammars from partially brack-eted corpora (Pereira and Sehabes, 1992).
We use thedata given by Pereira and Schabes (1992) on raw textand compare with an inferred SLTAG.The initial grammar consists of all trees (96) of theform fl~, a ~ for all 48 terminal symbols for part-of-speech.
As shown in Figure 2, the grannnar convergesvery rapidly to a lower value of the log probabilitythan the stochastic ontext-free grammar eported byPereira and Schabes (1992).161412i0SCFG ..... "i i i t5 10 15 20 25iterationFigure 2: Convergence for ATIS Corpus6 Conc lus ionA novel statistical language model and fundamental -gorithms for this model have been presented.SLTAGs provide a stochastic model both hierarchi-cal and sensitive to lexical information.
They combiaethe advantages of purely |exical models such ms N-gramdistributions or Ilidden Markov Models and the oneof ifierarchical modes as stochastic ontext-free gram-mars without their inhercnt limitations.
The parame-ters of a SLTAG correspond to the probability of com-bining two structures each one associated with a wordand therefore capture linguistically relevant distribu-tions over words.An algorithm for computing the probability of a sen-tence generated by a SLTAG was presented as well asan iterative algorithm for estimating the parameters ofa SLTAG given a training corpus of raw text.
Simi-larly to its context-free counterpart, he reestimationalgorithm can be extended to handle partially parsedcorpora (Pereira and Schabes, 1992).Preliminary experiments with a context-free subsetof SLTAG confirms that SLTAG enables faster conver-gence than stochastic ontext-free grammars (SCFG).This is the case since SCFG are unable to representlexieal influences on distribution except by a statisti-cally and eomputationally impractical proliferation ofnonterminal symbols, whereas SLTAG allows for a lexi-eally sensitive distributional mmlysis while maintaininga hierarchical structure.Furthermore, the techniques explained in this paperapply to other grammatical formalisms uch as combi-natory categorial grammars and modified head gram-mars since they have been proven to be equivalent totree-adjoining grammars and linear indexed grmnmars(Joshi, Vijay-Shanker, and Weir, 1991).Due to the lack of space, only few experiments withSLTAG were reported.
A full version of tile paper willbe available by tile time of the meeting and more exper-imental details will be reported uring the presentationof the paper.In collaboration with Aravind Joshi, FernandoPereira and Stuart Slfieber, we are currently investigat-ing additional algorithnLs and applications for SLTAG,methods for lexical clustering and autonratic onstruc-tion of a SLTAG from a large training corpus.ReferencesAho, A. V. 1968. lndexed grammars - An extensionto context free grammars.
J ACM, 15:647-671.Baker, J.K. 1979.
Trainable grammars tbr speechrecognition.
In Jared J. Wolf and Dennis H. Klatt,editors, Speech communication papers presentacd atthe 97 ~h Meeting of the Acoustical Society of Amer-ica, MIT, Cambridge, MA, June.llooth, Taylor R. and Richard A. Thoml)son.
1973.Applying probability measures to abstract languages.IEEE 7)'aasactions on Computers, C-22(5):442-450,May.Booth, T. 1969.
Probabilistic representation f formallanguages.
In Tenth Annual IEEE Symposium onSwitching and Automata Theory, October.Chomsky, N., 1964.
Syntactic Structures, chapter 2-3,pages 13-18.
Mouton.Gazdar, G. 1985.
Applicability of indexed gr,'unmarsto natural anguages.
Technical Report CSLI-85-34,Center for Study of Language and Information.tlempttill, Charles T., John J. Godfrey, and George ILDoddington.
1990.
The ATIS spoken language sys-tems pilot corpus.
In DARPA Speech and NaturalLaaguage Workshop, Hidden Valley, Pennsylvania,June.Jelinek, F., J. D. Lafferty, and R. L. Mercer.
1990.
Ba-sic methods of probabilistic ontext free grammars.Technical Report RC 16374 (72684), IBM, YorktownHeights, New York 10598.Joshi, Aravind K. and Yves Schabes.
1991.
Tree-adjoiuing grammars and lexiealized grammars.
InMaurice Nivat and Andreas Podelski, editors, Defin-ability and Recognizability ofSets of Trees.
Elsevier.Forthcoming.Joshi, Aravind K., K. Vijay-Simnker, and David Weir.1991.
The convergence of mildly context-sensitivegramnmtical formalisms, in Peter Sells, StuartShieber, and Tom Wasow, editors, Foundational Is-sues in Natural Language Processing.
MIT Press,Cambridge MA.Joshi, Aravind K. 1987.
An Introduction to Tree Ad-joining Grammars.
In A. Manaster-Ramer, editor,Mathematics of Language.
John Beujamins, Amster-dana.Lari, K. and S. J.
Young.
1990.
The estimation ofstochastic ontext-free grmnmars using the Inside-Outside algorithm.
Computer Speech and Language,4:35-56.ACRES DE COL1NG-92, NANTES, 23-28 AO~r 1992 4 3 1 PROr'.. OI: COLING-92, NANTES, AUG. 23-28, 1992Pereira, Fernando and Yves Schabes.
1992.
Inside-outside reest imation from partial ly bracketed cor-pora.
In 20 th Meeting of the Association for Compu-tational Linguistics (ACL '9~), Newark, Delaware.Prat t ,  Fletcher.
1942.
Secret and urgent, the story ofcodes and ciphers.
Blue Ribbon Books.Resnik, Philip.
1991.
Lexicalized tree-adjoining ram-mar  for distr ibutional analysis.
In Penn Review ofLinguistics, Spring.Schabes, Yves, Anne Abeill~, and Aravind K. Joshi.1988.
Pars ing strategies with ' lexicalized' grarnmars:Application to tree adjoining gra~mnars.
In Proceed-ings of the 1~ lh International Conference on Compu-tational Linguistics (COLING'88}, Budapest,  Hun-gary, August .Sehabes, Yves.
1990.
Mathematical nd ComputationalAspects of Lexicalized Grammars.
Ph.D. thesis, Uni-versity of Pennsylvania,  Philadelphia, PA, August.Available as technical report (MS-CIS-90-48, L INCLAB179) from the Department  of Computer  Science.Schabes, Yves.
1991.
An inside-outside algor i thmfor est imat ing the parameters  of a hidden stochasticcontext-free grammar  based on Earley's algorithm.Manuscript.Shannon, C. E. 1948.
A mathemat ica l  theory ofcommunicat ion.
The Bell System Technical Journal,27(3):379-423.Shannon, C. E. 1951.
Predict ion and entropy of printedenglish.
The Bell System Technical Journal, 30:50-64.Vi jay-Shanker, K. and David J. Weir.
1991.
Pars ingconstrained grammar  formalisms.
In preparation.Vi jay-Shanker, K. 1987.
A Study of ?lbee AdjoiningGrammars.
Ph.D.  thesis, Department  of Computerand Information Science, University of Pennsylvmfia.A Comput ing  the  Ins ide  P rob-ab i l i t i esIn the following, the inside and outside probabilities arere\]ative to the input string w. 3 t" stands for the the set offoot nodes, S for the set of nodes on which substitution canoccur, ~ for the set of root nodes of initial trees, and ,4 forthe set of non-terminal nodes of auxiliary trees.
The insideprobability can be computed bottom-up with the followingrecurrence quations.
For all node v/found in an elementarytree, it can be shown that:1.
If b\[$r/\] ~ a, I(b,7, i , - , - , I )  = dl  if / = i+  1 and ifa = w~ +1, 0 otherwise.2.
\] f71 E3 c, l(b,7/,i,j,k,t)= l if i= j and ifk = l, 0 otherwise.3.
If b\[..7\] ~ t\[..Talt\[$7~\]: l(b, 7, i,j,k,I)=E l(t,7j,i, j,k,m) x l(t,7~,m,--,-,t)m=k4.
If b\[..7\] - -  t\[$oa\]t\[..7z\] , l(b, 7, i,j,k,I) =~ I(t, 71, i , - , - ,m) xl(t,72,m,j,k,I)m~i+ l~.
ff b\[$t~\] ~ t\[$~dt\[$7~\], (b, 7, i, - ,  - ,  0 =E l(t 'Tt ' i ' - ' - - 'm) x l(t, 7~,m,-,-, I)m~i+l6.
For all node 7 on which adjunction can be performed:l ( t , , , i , j ,  k, 0 =1(b,,, i, j, k,t) ?
P(t\[..7\] ~ b\[..,l\])+ ?
l(b, 7,r,j,k,s)?
e(t\[..7\] - t\[-.,,id)7.
For all node 7 E S: l(t, 7, i , - , - , l )  =Z l ( t 'T l ' i ' - - ' - - ' l )  ?
P(t\[$7\] ~ t\[$Ta\])'h8.
I ( t ,$, i , - , - , l )= E I(t,7, i,-,-,I)?P(t\[$\] ~ t\[$0\]))lB Comput ing  the  Outs ideP robab i l i t i esThe outside probabilities can be computed top-down recur-sively over smaller spans of the input string once the in-side probabilities have been computed.
First, by definitionwe have: O(t, $, 0 , - ,  - ,  N) = 1.
The following recurrenceequations hold for all node y found in an elementary tree.1.
If 7 E "g, O(t, 7, 0, - ,  - ,  N) = e(t\[$\] ~ t\[$7\]).And for all (i,j) ~ (0, N), O(t,~,i , - , - , j )  =o(t,  ,10, i , - , - , j )  ?
P(@%\] ~ @)~\])2.
If 7 is an interior node which subsumes the foot nodeof the elementary tree it belongs to, O(t, ~, i, j, k, l) =~ O(b,%,i,j,k,q) )?
l(t, 7~, 1,-, -, q)q=t+, ?
P(b\["70\] ~ t\["Tlt\[$7~\])i-1 O(b, qo,p,j,k,l ) )+Z ?
l(t '71'P'- ' - ' i )~=0 x P(b\[.-70\] ~ t\[$7,lt\[..7\])3.
If T/ is an interior node which does not subsume thefoot node of the elementary tree it belongs to, we have:o( t ,7 , i , - , - , t )  =v O(b,)lo,i,-,-,q) )E ?
l ( t ' )h ' l ' - ' - 'q)q=lq-i ?
P(b\[$70\] ~ t\[$7\]t\[$72\])+ ?
I(t,7~,p,-,-,Q?
P(b\[$7ol ~ t\[$7,\]t\[$7\])+ ~ O(b'7?'i'j'Lq)?
I(t, 72,l,j,k,q),=, ~=,+, .=.
?
P(b\[ .m) ~ @71t\[..Td)+ ?
I(t, 71,p,j,k,i)?
P(b\[..%\] ~ t\[..7#\[$7\])4.
If T/ E.4, then: O(t,7, i , j ,k,l)=k - l~(O( t ' ' l ? '
i ' p 'q ' l )  )~o ~ ?
l(t, 7o,j,p,q,k)p=j q=~+, ?
P(t\["7o\] ~ t\[-.%rl\]) ~f~%(o(t,%,i,-,-,t))+ ?
l(t,) lo,j ,- ,- ,k)?
P(t\[$%\] ~ t\[$%7\])5.
If 7 is a node which subsumes the foot node of the ele-mentary tree it belongs to, we have: O(b, 7, i, j, k, I) =O(t,  7, i, j, k, l) ?
e ( t \ [ "7 \ ]  ~ b\[..~/\])+ ?
l(t, 7o,p,i,l,q)% p=o q=* \ x P(t\["7o\]-  t\["7o)?\])6.
And finally, if )1 is a node which does not subsumethe foot node of the elementary tree it belongs to:O(b, 7, i , - , - , t )  =o(t, 7, i, - ,  - ,  t) x P(t\[$7\] ~ b\[$7\])+ ?
l(t,%,p,i,l,q)70 p=o q=~ \ x P(t\[$7o\] ~ t\[$7oY/\])ACRES DE COLING-92, NAme, s. 23-28 ^ o~rr 1992 4 3 2 Paoc.
OF COLING-92, NANTES.
AUG. 23-28, 1992
