Identifying Chemical Names in Biomedical Text:An Investigation of the Substring Co-occurrence Based ApproachesAbstractWe investigate various strategies for findingchemicals in biomedical text using substringco-occurrence information.
The goal is tobuild a system from readily available datawith minimal human involvement.
Ourmodels are trained from a dictionary ofchemical names and general biomedical text.We investigated several strategies includingNa?ve Bayes classifiers and several types ofN-gram models.
We introduced a new way ofinterpolating N-grams that does not requiretuning any parameters.
We also found thetask to be similar to Language Identification.1 IntroductionChemical names recognition is one of the first tasksneeded for building an information extraction system inthe biomedical domain.
Chemicals, especially organicchemicals, are one of the main agents in many processesand relationships such a system would need to find.
Inthis work, we investigate a number of approaches to theproblem of chemical names identification.
We focus onapproaches that use string internal information forclassification, those based on the character co-occurrence statistics within the strings that we wouldlike to classify.
We would also like not to spend muchtime and effort to do manual annotation, and hence usereadily publicly available data for training all themodels.
Because of that, we would be satisfied withonly moderate results.
In the course of thisinvestigation, we have found that N-gram methods workbest given these restrictions on the models.Work has been done on a related task of namedentity recognition (Bikel et al, 1999, Riloff, 1996,Cucerzan, 1999, and others).
The aim of the namedentity task is usually set to find names of people,organizations, and some other similar entities in text.Adding features based on the internal substring patternshas been found useful by Cucerzan et al, 1999.
Forfinding chemicals, internal substring patterns are evenmore important source of information.
Many substringsof chemical names are very characteristic.
For example,seeing "methyl" as a substring of a word is a strongindicator of a chemical name.
The systematic chemicalnames are constructed from substrings like that, buteven the generic names follow certain conventions, andhave many characteristic substrings.In this work, character co-occurrence patterns areextracted from available lists of chemicals that havebeen compiled for other purposes.
We built modelsbased on the difference between strings occurring inchemical names and strings that occur in other words.The use of only string internal information prevents usfrom disambiguating different word senses, but weaccept this source of errors as a minor one.Classification based solely on string internalinformation makes the chemical names recognition tasksimilar to language identification.
In the languageidentification task, these patterns are used to detectstrings from a different language embedded into text.Because chemicals are so different, we can view themas a different language, and borrow some of theLanguage Identification techniques.
Danning, 1994 wasable to achieve good results using character N-grammodels on language identification even on short strings(20 symbols long).
This suggests that his approachmight be successful in chemical names identificationsetting.N-gram based methods were previously used forchemicals recognition.
Wilbur et al, 1999 used allsubstrings of a fixed length N, but they combined thetraining counts in a Bayesian framework, ignoring non-independence of overlapping substring.
They claimedgood performance for their data, but this approachshowed significantly lower performance thanalternatives on our data.
See the results section forAlexander VassermanDepartment of Computer andInformation ScienceUniversity of PennsylvaniaPhiladelphia, PA 19104avasserm@seas.upenn.edumore details.
The difference is that their data iscarefully constructed to contain only chemicals andchemicals of all types in the test data, i.e.
their trainingand testing data is in a very close correspondence.We on the other hand tried to use readily availablechemical lists without putting much manual labor intotheir construction.
Most of our training data comesfrom a single source - National Cancer Institute website- and hence represents only a very specific domain ofchemicals, while testing data is coming from a randomsample from MEDLINE.
In addition, these lists weredesigned for use by human, and hence contain manycomments and descriptions that are not easily separablefor the chemical names themselves.
Several attempts oncleaning these out have been made.
Most aggressiveattempts deleted about half the text from the list.
Whiledeleting many useful names, this improved the resultssignificantly.While we found that N-grams worked best amoungthe approaches we have tried, other approaches are alsopossible.
We did not explore the possibility of usingsubstring as features to a generic classificationalgorithm, such as, for example, support  vectormachines (Burges, 1998).2 Available DataIn order to train a statistical model for recognizingchemicals a list of about 240 thousands entries havebeen download from National Cancer Institute website(freely available at dtp.nci.nih.gov).
Entries are uniquenames of about 45 thousands unique chemicals.
Eachentry includes a name of a chemical possibly followedby alternative references and some comments.
Thisadditional information had to be deleted in order tocompute statistics from chemical names only.
Whilethere were no clean separators between chemical namesand the additional materials, several patterns weredesigned to clean up the list.
Applying those patternsshrunk each entry on average by half.
This cleaningstep has not produced perfect results in both leavingsome unusable material in and deleting some usefulstrings, yet it improved the performance of all methodsdramatically.
Cleaning the list by hand might haveproduced better results, but it would require moreexpertise and take a lot of time and would contradict thegoal of building the system from readily available data.We used text from MEDLINE abstracts to modelgeneral biomedical language.
These were available as apart of the MEDLINE database of bibliographicalrecords for papers in biomedical domain.
Records thathad non-empty abstracts have been extracted.
Fromthose 'title' and 'abstract' fields were taken and cleanedoff from remaining XML tags.Both the list of chemical names (LCN) and the textcorpus obtained from the MED LINE database (MED)were tokenized by splitting on the white spaces.
Whitespace tokenization was used over other possibleapproaches, as the problem of tokenization is very hardfor chemical names, because they contain a lot ofinternal punctuation.
We also wanted to avoid splittingchemical names into tokens that are too small, as theywould contain very little internal information to workwith.
The counts of occurrences of tokens in LCN andMD were used in all experiments to build models ofchemical names and general biomedical text.In addition, 15 abstracts containing chemical nameswere selected from the parts of MEDLINE corpus notused for the creation of the above list.
These abstractshave been annotated by hand and used as developmentand test sets.3 Classification Using SubstringImportance Criteria3.1 Classification ApproachMost obvious approach to this problem is to try tomatch the chemicals in the list against the text and labelonly the matches, i.e.
chemicals that are known from thelist.
This approach is similar to the memory-basedbaseline described by Palmer et al, 1997, where insteadof using precompiled list they memorized all the entriesthat occurred in a training text.A natural extension of matching is a decision list.Each classification rule in the list checks if a substring ispresent in a token.
Matching can be viewed as just anextreme of this approach, where the strings selected intothe decision list are the complete tokens from the LCN(including token boundary information).
Using othersubstrings increases recall, as non-exact matches aredetected, and it also improves precision, as it decreasesthe number of error coming from noise in LCN.While decision list performs better than matching, itsperformance is still unsatisfactory.
Selecting onlyhighly indicative substrings results in high precision, butvery low recall.
Lowering the thresholds and takingmore substrings decreases the precision withoutimproving the recall much until the precision gets verylow.The decision list approach makes each decisionbased on a single substring.
This forces us to selectonly substrings that are extreemly rare outside thechemical names.
This in turn results in extremely lowrecall.
An alternative would be to combine theinformation from multiple substrings into a singledecision using Naive Bayes framework.
This wouldkeep precision from dropping as dramatically when weincrease the number of strings used in classification.We would like to estimate the probability of a tokenbeing a part of a chemical name given the token (string)p(c|s) .
Representing each string as a set of its substringswe need to estimate p(c|s1...sn).
Using Bayes Rule, weget)...ss|c)p(c)/p(...sp(s)...sp(c|s n1n1n1 =  (1)Assuming independence of substrings s1...sn andconditional independence of substrings s1...sn given c,we can rewrite:?
?====n1iin1iin1n1n1)p(s/|c)p(sp(c))...s|c)/p(s...sp(c)p(s)...sp(c|s(2)Now notice that for most applications we would liketo be able to vary precision/recall tradeoff by settingsome threshold t and classifying each string s a s  achemical only ift)p(s/|c)p(sp(c) scpn1iin1ii >= ?
?==)|(  (3)or'/ tp(c)t)p(s/|c)p(sn1iin1ii =>?
?==(4)This allows us to avoid estimation of p(c)(estimating p(c) is hard without any labeled text).
Wecan estimate p(si|c) and p(si) from the LCN and MEDrespectively astokens)(/#) containg tokens(#)( ii ssp =  (5)3.2 Substring SelectionFor this approach, we need to decide what set ofsubstring {si} of s to use to represent s.  We would liketo select a set of non-overlapping substrings to make theindependence assumption more grounded (while it isclear that even non-overlapping substrings are notindependent, assuming independence of overlappingsubstrings clearly causes major problems).
In order todo this we need some measure of usefulness ofsubstrings.
We would like to select substrings that areboth informative and reliable as features, i.e.
thesubstrings fraction of which in LCN is different fromthe fraction of them in MED and which occur oftenenough in LCN.
Once this measure is defined, we canuse dynamic programming algorithm similar to Viterbidecoding to select the set of non-overlapping substringswith maximum value.Kullback-Leibler divergence based measureIf we view the substring frequencies as a distribution,we can ask the question which substrings account forthe biggest contribution to Kullback-Leibler divergence(Cover et al 1991) between distribution given by LCNand that given by MED.
From this view it is reasonableto take p(si|c)*log(p(si|c)/p(si)) as a measure of value ofa substring.
Therefore, the selection criterion would betspcspcsp iii >))(/)|(log()|(   (6)where t is some threshold value.
Notice that thismeasure combines frequency of a substring in chemicalsand the difference between frequencies of occurrencesof the substring in chemicals and non-chemicals.A problem with this approach arises when eitherp(si|c) or p(si) is equal to zero.
In this case, thisselection criterion cannot be computed, yet some of themost valuable strings could have p(si) equal to zero.Therefore, we need to smooth probabilities of thestrings to avoid zero values.
One possibility is toinclude all strings si, such that p(si)=0 and p(si|c)>t',where t'<t is some new threshold needed to avoidselecting very rare strings.
It would be nice though notto introduce an additional parameter.
An alternativewould be to reassign probabilities to all substrings andkeep the selection criterion the same.
It could be done,for example, using Good-Turing smoothing (Good1953).Selection by significance testingA different way of viewing this is to say that we want toselect all the substrings in which we are confident.
Itcan be observed that tokens might contain certainsubstrings that are strong indicators of them beingchemicals.
Useful substrings are the ones that predictsignificantly different from the prior probability ofbeing a chemical.
I.e.
if the frequency of chemicalsamong all tokens is f(c), then s is a useful substring ifthe frequency of chemicals among tokens containing sf(c|s)  is significantly different from f(c).
We test thesignificance by assuming that f(c) is a good estimate forthe prior probability of a token being a chemical p(c),and trying to reject the null hypothesis, that actualprobability of chemicals among tokens that contain s isalso p(c).
If the number of tokens containing s is n(s)and the number of chemicals containing s is c(s) , thenthe selection criterion becomes95.65.1))(1)(()()()()( zcfcfsncfsnsc =>--   (7)This formula is obtained by viewing occurrences of sas Bernoulli trials with probability p(c)  of theoccurrence being a chemical and probability (1-p(c))  ofthe occurrence being non-chemical.
Distributionobtained by n(s) such trials can be approximated withthe normal distribution with mean n(s)p(c) and variancen(s)p(c)(1-p(c)).4 Classification Using N-gram ModelsW e  can estimate probability of a string given class(chemical or non-chemical) as the probability of lettersof the string based on a finite history.)()...|(),...|()(/)()|()|(0101cpssspcssspSpcpcSpScpiiiiii?
?--==(8)where  S is the string to be classified and si are theletters of S.The N-gram approach has been a successfulmodeling technique in many other applications.
It has anumber of advantages over the Bayesian approach.
Inthis framework we can use information from allsubstrings of a token, and not only sets of non-overlapping ones.
There is no (incorrect) independenceassumption, so we get a more sound probability model.As a practical issue, there has been a lot of work doneon smoothing techniques for N-gram models (Chen etal., 1998), so it is easier to use them.4.1 Investigating Usefulness of Different N-gramLengthsAs the first task in investigating N-gram models, weinvestigated usefulness of N-grams of different length.For each n, we constructed a model based on thesubstrings of this length only using Laplaciansmoothing to avoid zero probability.BnnssspBnncssspiNiiNiiiiNiciNiciidddd++?++?-+-+---+-+--1110111101)...|(),...|((9)where N is the length of the N-grams, nii-N+1 and ncii-N+1are the number of occurrences of N-gram sisi-1...si-N-1 inMEDLINE and chemical list respectively, d  is thesmoothing parameter, and B is the number of differentN-grams of length N.The smoothing parameter was tuned for each nindividually using the development data (handannotated MEDLINE abstracts).
The results of theseexperiments showed that 3-grams and 4-grams are mostuseful.
While poor performance by longer N-grams wassomewhat surprising, results indicated that overtrainingmight be an issue for longer N-grams, as the model theyproduce models the training data more precisely.
Whileunexpected, the result is similar to the conclusion inDunning '94 for language identification task.4.2 Interpolated N-gram ModelsIn many different tasks that use N-gram models,interpolated or back-off models have been provenuseful.
The idea here is to use shorter N-grams forsmoothing longer ones.mnnnnnssspmnnnnncssspiiiNiiNiNiNiiNiNiiciiciNiciNicNiNiciNicNii11221111011122111101...)...|(...),...|(llllll+++?+++?-+-+---+-+---+-+---+-+--(10)where lj's are the interpolation coefficients, m and mcare the total number of letters in MEDLINE andchemical list respectively.
lj can generally depend onsi-1...si-N+1 , with the only constraint that all l jcoefficients sum up to one.
One of the main questionfor interpolated models is learning the values for l's.Estimating N different l's for each context si-1...si-N+1 isa hard learning task by itself that requires a lot ofdevelopment data.
There are two fundamentallydifferent ways for dealing with this problem.
Oftengrouping different coefficients together and providingsingle value for each group, or imposing some otherconstraints on the coefficients is used to decrease thenumber of parameters.
The other approach is providinga theory for values of l's without tuning them on thedevelopment data (This is similar in spirit to MinimalDescription Length approach).
We have investigatedseveral different possibilities in both of these twoapproaches.4.3 Computing Interpolation Coefficients: FixedCoefficientsEquation (10) can be rewritten in a slightly differentform:??????????????????????????????
+-+-+-?----+-+----+-+---mnnnnnnnssspiiiiiiNiNiiNiNNiniiNiNii1111122221111101)1(...
)1()1()...|(llllll(11)This form states more explicitly that each N-grammodel is smoothed by all lower models.
An extreme ofthe grouping approach is then to make all lj's equal,and tune this single parameter on the development data.4.4 Computing Interpolation Coefficients:Context Independent CoefficientsRelaxing this constraint and going back to the originalform of equation (10), we can make all lj's independentof their context, so we get only N parameters to tune.When N is small, this can be done even with relativelysmall development set.
We can do this by exploring allpossible settings of these parameters in an Ndimensional grid with small increment.
For larger N  wehave to introduce an additional constraint that l j'sshould lie on some function of j with a smaller numberof parameters.
We have used a quadratic function (2parameters, as one of them is fixed by the constraint thatall lj's have to sum up to 1).
Using higher order of thefunction gives more flexibility, but introduces moreparameters, which would require more developmentdata to tune well.
The quadratic function seems to be agood trade off that provides enough flexibility, but doesnot introduce too many parameters.4.5 Computing Interpolation Coefficients:Confidence Based CoefficientsThe intuition for using interpolated models is that higherlevel N-grams give more information when they arereliable, but lower level N-grams are usually morereliable, as they normally occur more frequently.
Wecan formalize this intuition by computing theconfidence of higher level N-grams and weight themproportionally.
We are trying to estimate p(si|si-1...si-N+1) with the ratio nii-N+1 /ni-1i-N+1.
We can say that ourobservation in the training data was generated by ni-1i-N+1 Bernoulli trials with outcomes either si or any otherletter.
We consider si to be a positive outcome and anyother letter would be a negative outcome.
Given thismodel we have nii-N+1 positive outcomes in ni-1i-N+1Bernoulli trials with probability of positive outcomep(si|si-1...si-N+1).
This means that the estimate given bynii-N+1 /ni-1i-N+1 has the confidence interval of binomialdistribution approximated by normal given by)(2 24223aaazcczczcI++=                 (12)where c = ni-1i-N+1 .Since the true probability is within I of the estimate,the lower level models should not change the estimategiven by the highest-level model by more than I.  Thismeans that lN-1 in the equation (11) should be equal toI.
By recursing the argument we get)(2 24223aaalzcczczcIjjjjjj ++==                (13)where cj = ni-1i-j+2  for j > 1 , and c1 = m.5 Evaluation and ResultsWe performed cross validation experiments on 15 hand-annotated MEDLINE abstracts described in section"Available Data".
Experiments were done by holdingout each abstract, tuning model parameters on 14remaining abstracts, and testing on the held out one.Fifteen such experiments were performed.
The resultsof these experiments were combined by taking weighedgeometric mean of precision results at each recall level.The results were weighted according to the number ofpositive examples in each file to ensure equalcontribution from each example.
Figure 1 shows theresulting precision/recall curves.As we can see, the  N-gram approaches performbetter than the other ones.
The interpolated model withquadratic coefficients needs a lot of development data,so it does not produce good results in our case.
SimpleLaplacian smoothing needs less development data andproduces much better results.
The model withconfidence based coefficients works best.
The graphalso shows the model introduced by Wilbur et al, 1999.It does not perform nearly as well on our data, eventhough it produces very good results on clean data theyhave used.
This (as well as some experiments weperformed that have not been included into this work)suggests that quality of the training data has very strongeffect on the model results.6 Conclusions and Future WorkWe have investigated a number of different approachesto chemical identification using string internalinformation.
We used readily available training data,and a small amount of human annotated text that wasused primarily for testing.
We were able to achievegood performance on general biomedical text takenfrom MEDLINE abstracts.
N-gram models showed thebest performance.
The specific details of parameter00.10.20.30.40.50.60.70.80.90.050.100.150.200.250.300.350.400.450.500.550.600.650.700.750.800.850.900.951.00RecallPrecisionNaive BayesWilbur et alLaplacian SmoothingQuadratic CoefficientsConfidence-Based CoefficientsFig.
1.
Precision/Recall curves for Na?veBayes and N-gram based modelstuning for these models produced small variations in theresults.
We have also introduced a method forcomputing interpolated N-gram model parameterswithout any tuning on development data.
The resultsproduced by this method were slightly better than thoseof other approaches.
We believe this approachperformed better because only one parameter - thelength of N-grams - needed to be tuned on thedevelopment data.
This is a big advantage when littledevelopment data is available.
In general, wediscovered many similarities with previous work onlanguage identification, which suggests that othertechniques introduced for language identification maycarry over well into chemicals identification.As a short term goal we would like to determine N-gram interpolation coeficients by usefulness of thecorresponding context for discrimination.
This wouldincorporate the same techinque as we used for NaiveBayes system, hopefully combining the advantage ofboth approachesThere are other alternatives for learning aclassification rule.
Recently using support vectormachines (Burges 1998) have been a popular approach.More traditionally decision trees (Breiman et al 1984)have been used for simmilar tasks.
It would beinteresting to try these aproaches for our task andcompare them with Naive Bayes and N-gramapproaches discussed here.One limitation of the current system is that it doesnot find the boundaries of chemicals, but only classifiespredetermind tokens as being part of a chemical nameor not.
The system can be improved by removing priortokenization requirment, and attempting to identifychemical name boundaries based on the learnedinformation.In this work we explored just one dimention ofpossible features usefull for finding chemical names.We intent to incorporate other types of featuresincluding context based features with this work.ReferencesT.
Dunning.
1994.
"Statistical identification oflanguage".
Technical Report MCCS 94-273, NewMexico State University.S.
F. Chen and J. Goodman.
1998.
?An EmpiricalStudy of Smoothing Techniques for LanguageModeling,?
TR-10-98, Computer Science Group,Harvard Univ., 1998.W.
John Wilbur, George F. Hazard, Jr., Guy Divita,James G. Mork, Alan R. Aronson, Allen C. Browne.1999.
"Analysis of Biomedical Text for ChemicalNames: A Comparison of Three Methods".Proceedings of AMIA Symposium 1999:181-5.Daniel M. Bikel, Richard Schwartz and Ralph M.Weischedel.
1999.
"An Algorithm that LearnsWhat's in a Name", Machine LearningEllen Riloff.
1996.
"Automatically GeneratingExtraction Patterns from Untagged Text",Proceedings of the Thirteenth National Conferenceon Artificial Intelligence (AAAI-96), pp.
1044-1049Silviu Cucerzan, David Yarowsky.
1999.
"LanguageIndependent Named Entity Recognition CombiningMorphological and Contextual Evidence".Proceedings of 1999 Joint SIGDAT conference onEMNLP and VLC, University of Maryland, MD.D.
D. Palmer, D. S. Day.
1997.
"A Statistical Profile ofNamed Entity Task".
Proceedings of Fifth ACLConference for Applied Natural Language Processing(ANLP -97), Washington D.C.I.
Good.
1953.
"The population frequencies of speciesand the estimation of population parameters".Biometrika, v. 40, pp.
237-264C.J.C.
Burges, 1998.
"A Tutorial on Support VectorMachines for Pattern Recognition," Data Mining andKnowledge Discovery, 2(2), pp.
955-974T.
Cover and J. Thomas, 1991.
?Elements ofInformation Theory?, Wiley, New York.L.
Breiman, J.H.
Friedman, R.A. Olshen, and C.J.Stone, 1984.
"Classification and Regression Trees,"Chapman & Hall, New York.
