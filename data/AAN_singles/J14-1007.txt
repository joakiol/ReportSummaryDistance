Sampling Tree Fragments from ForestsTagyoung Chung?University of RochesterLicheng Fang?
?University of RochesterDaniel Gildea?University of RochesterDaniel S?tefankovic?
?University of RochesterWe study the problem of sampling trees from forests, in the setting where probabilities for eachtree may be a function of arbitrarily large tree fragments.
This setting extends recent workfor sampling to learn Tree Substitution Grammars to the case where the tree structure (TSGderived tree) is not fixed.
We develop a Markov chain Monte Carlo algorithm which corrects forthe bias introduced by unbalanced forests, and we present experiments using the algorithm tolearn Synchronous Context-Free Grammar rules for machine translation.
In this application, theforests being sampled represent the set of Hiero-style rules that are consistent with fixed inputword-level alignments.
We demonstrate equivalent machine translation performance to standardtechniques but with much smaller grammars.1.
IntroductionRecent work on learning Tree Substitution Grammars (TSGs) has developed proceduresfor sampling TSG rules from known derived trees (Cohn, Goldwater, and Blunsom2009; Post and Gildea 2009).
Here one samples binary variables at each node in thetree, indicating whether the node is internal to a TSG rule or is a split point betweentwo rules.
We consider the problem of learning TSGs in cases where the tree structureis not known, but rather where possible tree structures are represented in a forest.
Forexample, we may wish to learn from text where treebank annotation is unavailable,?
Computer Science Dept., University of Rochester, Rochester NY 14627.E-mail: chung@cs.rochester.edu.??
Computer Science Dept., University of Rochester, Rochester NY 14627.E-mail: lfang@cs.rochester.edu.?
Computer Science Dept., University of Rochester, Rochester NY 14627.E-mail: gildea@cs.rochester.edu.?
Computer Science Dept., University of Rochester, Rochester NY 14627.E-mail: stefanko@cs.rochester.edu.Submission received: 26 October 2012; revised version received: 14 March 2013; accepted for publication:4 May 2013.doi:10.1162/COLI a 00170?
2014 Association for Computational LinguisticsComputational Linguistics Volume 40, Number 1but a forest of likely parses can be produced automatically.
Another application onwhich we focus our attention in this article arises in machine translation, where wewant to learn translation rules from a forest representing the phrase decompositions thatare consistent with an automatically derived word alignment.
Both these applicationsinvolve sampling TSG trees from forests, rather than from fixed derived trees.Chappelier and Rajman (2000) present a widely used algorithm for sampling treesfrom forests: One first computes an inside probability for each node bottom?up, andthen chooses an incoming hyperedge for each node top?down, sampling according toeach hyperedge?s inside probability.
Johnson, Griffiths, and Goldwater (2007) use thissampling algorithm in a Markov chain Monte Carlo framework for grammar learning.We can combine the representations used in this algorithm and in the TSG learningalgorithm discussed earlier, maintaining two variables at each node of the forest, onefor the identity of the incoming hyperedge, and another representing whether the nodeis internal to a TSG rule or is a split point.
However, computing an inside probabilityfor each node, as in the first phase of the algorithm of Johnson, Griffiths, and Goldwater(2007), becomes difficult because of the exponential number of TSG rules that can applyat any node in the forest.
Not only is the number of possible TSG rules that can applygiven a fixed tree structure exponentially large in the size of the tree, but the number ofpossible tree structures under a node is also exponentially large.
This problem is par-ticularly acute during grammar learning, as opposed to sampling according to a fixedgrammar, because any tree fragment is a valid potential rule.
Cohn and Blunsom (2010)address the large number of valid unseen rules by decomposing the prior over TSGrules into an equivalent probabilistic context-free grammar; however, this techniqueonly applies to certain priors.
In general, algorithms that match all possible rules arelikely to be prohibitively slow, as well as unwieldy to implement.
In this article, wedesign a sampling algorithm that avoids explicitly computing inside probabilities foreach node in the forest.In Section 2, we derive a general algorithm for sampling tree fragments from forests.We avoid computing inside probabilities, as in the TSG sampling algorithms of Cohn,Goldwater, and Blunsom (2009) and Post and Gildea (2009), but we must correct forthe bias introduced by the forest structure, a complication that does not arise when thetree structure is fixed.
In order to simplify the presentation of the algorithm, we first setaside the complication of large, TSG-style rules, and describe an algorithm for samplingtrees from forests while avoiding computation of inside probabilities.
This algorithm isthen generalized to learn the composed rules of TSG in Section 2.3.As an application of our technique, we present machine translation experiments inthe remainder of the article.
We learn Hiero-style Synchronous Context-Free Grammar(SCFG) rules (Chiang 2007) from bilingual sentences for which a forest of possibleminimal SCFG rules has been constructed fromfixedword alignments.
The constructionof this forest and its properties are described in Section 3.
We make the assumptionthat the alignments produced by a word-level model are correct in order to simplifythe computation necessary for rule learning.
This approach seems safe given that thepipeline of alignment followed by rule extraction has generally remained the state ofthe art despite attempts to learn joint models of alignment and rule decomposition(DeNero, Bouchard-Cote, and Klein 2008; Blunsom et al.
2009; Blunsom and Cohn2010a).
We apply our sampling algorithm to learn the granularity of rule decompositionin a Bayesian framework, comparing sampling algorithms in Section 4.
The end-to-endmachine translation experiments of Section 5 show that our algorithm is able to achieveperformance equivalent to the standard technique of extracting all rules, but results ina significantly smaller grammar.204Chung et al.
Sampling Tree Fragments from Forests2.
Sampling Trees from ForestsAs a motivating example, consider the small example forest of Figure 1.
This forestcontains a total of five trees, one under the hyperedge labeled A, and four under thehyperedge labeled B (the cross-product of the two options for deriving node 4 and thetwo options for deriving node 5).Let us suppose that we wish to sample trees from this forest according to a distribu-tion Pt, and further suppose that this distribution is proportional to the product of theweights of each tree?s hyperedges:Pt(t) ?
?h?tw(h) (1)To simplify the example, suppose that in Figure 1 each hyperedge has weight 1,?h w(h) = 1giving us a uniform distribution over trees:?t Pt(t) = 15A tree can be specified by attaching a variable zn to each node n in the forestindicating which incoming hyperedge is to be used in the current tree.
For example,variable z1 can take values A and B in Figure 1, whereas variables z2 and z3 can onlytake a single value.
We use z to refer to the entire set of variables zn in a forest.
Eachassignment to z specifies a unique tree, ?
(z), which can be found by following theincoming hyperedges specified by z from the goal node of each forest down to theterminals.A naive sampling strategy would be to resample each of these variables zn inorder, holding all others constant, as in standard Gibbs sampling.
If we choose anincoming hyperedge according to the probability Pt(?
(z)) of the resulting tree, holdingall other variable assignments fixed, we see that, because Pt is uniform, we will choose2 3 514ABFigure 1Example forest.205Computational Linguistics Volume 40, Number 1with uniform probability of 1/m among the m incoming hyperedges at each node.
Inparticular, we will choose among the two incoming hyperedges at the root (node 1)with equal probability, meaning that, over the long run, the sampler will spend half itstime in the state for the single tree corresponding to nodes 2 and 3, and only one eighthof its time in each of the four other possible trees.
Our naive algorithm has failed at itsgoal of sampling among the five possible trees each with probability 1/5.Thus, we cannot adopt the simple Gibbs sampling strategy, used for TSG inductionfrom fixed trees, of resampling one variable at a time according to the target distribution,conditioned on all other variables.
The intuitive reason for this, as illustrated by theexample, is the bias introduced by forests that are bushier (that is, havemore derivationsfor each node) in some parts than in others.
The algorithm derived in the remainder ofthis section corrects for this bias, while avoiding the computation of inside probabilitiesin the forest.2.1 Choosing a Stationary DistributionWe will design our sampling algorithm by first choosing a distribution Pz over the setof variables z defined earlier.
We will show correctness of our algorithm by showingthat it is a Markov chain converging to Pz, and that Pz results in the desired distributionPt over trees.The tree specified by an assignment to z will be denoted ?
(z) (see Table 1).
For a treet the vector containing the variables found at nodes in t will be denoted z[t].
This is asubvector of z: for example, in Figure 1, if t chooses A at node 1 then z[t] = (z1, z2, z3),and if t chooses B at node 1 then z[t] = (z1, z4, z5).
We use z[?t] to denote the variablesnot used in the tree t. The vectors z[t] and z[?t] differ according to t, but for any treet, the two vectors form a partition of z.
There are many values of z that correspond tothe same tree, but each tree t corresponds to a unique subvector of variables z[t] and aunique assignment to those specific variables?we will denote this unique assignmentof variables in z[t] by ?(t).
(In terms of ?
and ?
one has for any z and t that ?
(z) = t ifand only if z[t] = ?(t).
)Let Z be the random vector generated by our algorithm.
As long as Pz(?
(Z) = t) =Pt(t) for all trees t, our algorithm will generate trees from the desired distribution.Thus for any tree t the probability Pz(Z[t] = ?
(t)) is fixed to Pt(t), but any distribu-tion over the remaining variables not contained in t will still yield the desired dis-tribution over trees.
Thus, in designing the sampling algorithm, we may choose anydistribution Pz(Z[?t] | Z[t] = ?(t)).
A simple and convenient choice is to makePz(Z[?t] | Z[t] = ?
(t)) uniform.
That is, each incoming hyperedge variable with malternatives assigns each hyperedge probability 1/m.
(In fact, our algorithm can easilyTable 1NotationPt desired distribution on treesz vector of variablesZ random vector over z?
(z) tree corresponding to setting of zZ[t] subset of random variables that occur in tree t?
(t) setting of variables in Z[t]Z[?t] subset of random variables that do not occur in t206Chung et al.
Sampling Tree Fragments from Forestsbe adapted to other product distributions of Pz(Z[?t] | Z[t] = ?(t)).)
This choice ofP(Z[?t] | Z[t] = ?
(t)) determines a unique distribution for Pz:Pz(Z = z) = Pt(?(z))Pz(Z[??
(z)] = z[??
(z)] | Z[?
(z)] = z[?
(z)])= Pt(?(z))?v?z[??
(z)]1deg(v)(2)where deg(v) is the number of possible values for v.We proceed by designing a Gibbs sampler for this Pz.
The sampler resamples vari-ables from z one at a time, according to the joint probability Pz(z) for each alternative.The set of possible values for zn at a node n having m incoming hyperedges consists ofthe hyperedges ej, 1 ?
j ?
m. Let sj be the vector z with the value of zn changed to ej.Note that the ?
(sj)?s only differ at nodes below n.Let z[in(n)] be the vector consisting of the variables at nodes below n (that is,contained in subtrees rooted at n, or ?inside?
n) in the forest, and let z[in(n)] be thevector consisting of variables not under node n. Thus the vectors z[in(n)] and z[in(n)]partition the complete vector z.
We will use the notation z[t ?
in(n)] to represent thevector of variables from z that are both in a tree t and under a node n.For Gibbs sampling, we need to compute the relative probabilities of sj?s.
We nowconsider the two terms of Equation (2) in this setting.
Because of our requirement that Pzcorrespond to the desired distribution Pt, the first term of Equation (2) can be computed,up to a normalization constant, by evaluating our model over trees (Equation (1)).The second term of Equation (2) is a product of uniform distributions that can bedecomposed into nodes below n and all other nodes:Pz(Z[?t] = z[?t] | Z[t] = z[t]) =?v?z[?t?
in(n)]1deg(v)?v?z[?t?
in(n)]1deg(v)(3)where t = ?(z).
Recall that ?
(sj)?s only differ at vertices below n and hencePz(Z[?t] = z[?t] | Z[t] = z[t]) ??v?z[?t?
in(n)]1deg(v)(4)where we emphasize that ?
refers to the relative probabilities of the sj?s, which corre-spond to the options from which the Gibbs sampler chooses at a given step.
We canmanipulate Equation (4) into a more computationally convenient form by multiplyingby the deg(v) term for each node v inside n. Because the terms for all nodes not includedin the current tree cancel each other out, we are left with:Pz(Z[?t] = z[?t] | Z[t] = z[t]) ??v?z[t?
in(n)]deg(v) (5)Note that we only need to consider the nodes z[t] in the current tree, without needingto examine the remainder of the forest at all.207Computational Linguistics Volume 40, Number 1Substituting Equation (5) into Equation (2) gives a simple update rule for use in ourGibbs sampler:Pz(Z(i+1) = sj | Z(i) = z,n is updated) ?
Pt(?(sj))?v?z[?
(sj )?in(n)]deg(v) (6)To make a step of the Markov chain, we compute the right-hand side of Equation (6)for every sj and then choose the next state of the chain Z(i+1) from the correspondingdistribution on the sj?s.
The second term, which we refer to as the density factor, isequal to the total number of trees in the forest under node n. This factor compensatesfor the bias introduced by forests that are bushier in some places than in others, as inthe example of Figure 1.
A related factor, defined on graphs rather than hypergraphs,can be traced back as far as Knuth (1975), who wished to estimate the sum of values atall nodes in a large tree by sampling a small number of the possible paths from the rootto the leaves.
Knuth sampled paths uniformly and independently, rather than using acontinuously evolving Markov chain as in our Gibbs sampler.2.2 Sampling ScheduleIn a standard Gibbs sampler, updates are made iteratively to each variable z1, .
.
.
, zN,and this general strategy can be applied in our case.
However, it may be wasteful tocontinually update variables that are not used by the current tree and are unlikely to beused by any tree.
We propose an alternative sampling schedule consisting of sweepingfrom the root of the current tree down to its leaves, resampling variables at each nodein the current tree as we go.
If an update changes the structure of the current tree, thesweep continues along the new tree structure.
This strategy is shown in Algorithm 1,where v(z, i) denotes the ith variable in a top?down ordering of the variables of thecurrent tree ?(z).
The top?down ordering may be depth-first or breadth-first, amongother possibilities, as long as the variables at each node have lower indices than thevariables at the node?s descendants in the current tree.To show that this sampling schedule will converge to the desired distribution overtrees, we will first show that Pz is a stationary distribution for the transition defined bya single step of the sweep:Lemma 1For any setting of variables z, any top?down ordering v(z, i), and any i, updatingvariable zv(z,i) according to Equation (6) is stationary with respect to the distributionPz defined by Equation (2).Algorithm 1 Sampling algorithmRequire: A function v(z, i) returning the index of the ith variable of z in a top-down ordering ofthe variables of the tree ?
(z).1: i ?
12: while i ?
|Z[?
(z)]| do  Until last node of current tree.3: Resample zv(z,i) according to Equation (6)4: i ?
i + 15: end while208Chung et al.
Sampling Tree Fragments from ForestsProofWe will show that each step of the sweep is stationary for Pz by showing that it satisfiesdetailed balance.
Detailed balance is the condition that, on average, for each pair ofstates z and z?, the number of transitions between the two states is the same in eitherdirection:Pz(Z = z)P(Z(i+1) = z?
| Z(i) = z) = Pz(Z = z?
)P(Z(i+1) = z | Z(i) = z?)
(7)where P(Z(i+1) = z?
| Z(i) = z) is the transition performed by one step of the sweep:P(Z(i+1) = z?
| Z(i) = z) ={Pz(Z=z?
)?z??
Pz(Z=z??
)I(z??
?v(z,i)=z?v(z,i) )if z?
?v(z,i) = z?v(z,i)0 otherwise(8)It is important to observe that, because the resampling step only changes the treestructure below the ith node, the ith node in the new tree remains the same node.
Thatis, after making an update from z to z?, v(z, i) = v(z?, i), and, mathematically:z?
?v(z,i) = z?v(z,i) ?
v(z, i) = v(z?, i) ?
z?
?v(z?,i) = z?v(z,i)?
z?
?v(z?,i) = z?v(z?,i)Thus, the condition in Equation (8) is symmetric in z and z?, and we define the predicatematch(z, z?, i) to be equivalent to this condition.
Substituting Equation (8) into the left-hand side of Equation (7), we have:Pz(Z = z)P(Z(i+1) = z?
| Z(i) = z) ={Pz(Z=z)Pz(Z=z?
)?z??
Pz(Z=z??
)I(z??
?v(z,i)=z?v(z,i) )if match(z, z?, i)0 otherwise(9)By symmetry of the righthand side of Equation (9) in z and z?, we see that Equa-tion (7) is satisfied.
Because detailed balance implies stationarity, Pz is a stationarydistribution of P(Z(i+1) = z?
| Z(i) = z).
This lemma allows us to prove the correctness of our main algorithm:Theorem 1For any top?down sampling schedule v(z, i), and any desired distribution over treesPt that assigns non-zero probability to all trees in the forest, Algorithm 1 will convergeto Pt.ProofBecause Pz is stationary for each step of the sweep, it is stationary for one entire sweepfrom top to bottom.To show that the Markov chain defined by an entire sweep is ergodic, we must showthat it is aperiodic and irreducible.
It is aperiodic because the chain can stay in the sameconfiguration with non-zero probability by selecting the same setting for each variablein the sweep.
The chain is irreducible because any configuration can be reached in afinite number of steps by sorting the variables in topological order bottom?up in theforest, and then, for each variable, executing one sweep that selects a tree that includesthe desired variable with the desired setting.209Computational Linguistics Volume 40, Number 1Because Pz is stationary for the chain defined by entire sweeps, and this chain isergodic, the chain will converge to Pz.
Because Equation (2) guarantees that Pz(?
(Z) =t) = Pt(t), convergence to Pz implies convergence to Pt.
2.3 Sampling Composed RulesOur approach to sampling was motivated by a desire to learn TSG-style grammars,where one grammar rule is the composition of a number of hyperedges in the forest.We extend our sampling algorithm to handle this problem by using the same methodsthat are used to learn a TSG from a single, fixed tree (Cohn, Goldwater, and Blunsom2009; Post and Gildea 2009).
We attach a binary variable to each node in the forestindicating whether the node is a boundary between two TSG rules, or is internal to asingle TSG rule.
Thus, the complete set of variables used by the sampler, z, now consistsof two variables at each node in the forest: one indicating the incoming hyperedge, andone binary boundary variable.
The proof of Section 2.1 carries through, with each newbinary variable v having deg(v) = 2 in Equation (2).
As before, the current setting of zpartitions z into two sets of variables, those used in the current tree, z[?
(z)], and thoseoutside the current tree, z[??(z)].
Given a fixed assignment to z, we can read off boththe current tree and its segmentation into TSG rules.
We modify the tree probability ofEquation (1) to be a product over TSG rules r:Pt(t) ?
?r?tw(r) (10)in order to emphasize that grammar rules are no longer strictly equivalent to hyper-edges in the forest.
We modify the sampling algorithm of Algorithm 1 to make use ofthis definition of Pt and to resample both variables at the current node.
The incominghyperedge variable is resampled according to Equation (6), while the segmentationvariable is simply resampled according to Pt, as the update does not change the setsz[?
(z)] and z[??
(z)].The proof that the sampling algorithm converges to the correct distribution stillapplies in the TSG setting, as it makes use of the partition of z into z[?
(z)] andz[??
(z)], but does not depend on the functional form of the desired distribution overtrees Pt.3.
Phrase Decomposition ForestIn the remainder of this article, we will apply the algorithm developed in the previoussection to the problem of learning rules for machine translation in the context of aHiero-style, SCFG-based system.
As in Hiero, our grammars will make use of a singlenonterminal X, and will contain rules with a mixture of nonterminals and terminals onthe right-hand side, with at most two nonterminal occurrences in the right-hand sideof a rule.
In general, many overlapping rules of varying sizes are consistent with theinput word alignments, meaning that we must address a type of segmentation problemin order to learn rules of the right granularity.
Given the restriction to two right-handside nonterminals, the maximum number of rules that can be extracted from an inputsentence pair is O(n12) in the sentence length, because the left and right boundaries ofthe left-hand side (l.h.s.)
nonterminal and each of the two right-hand side nonterminalscan take O(n) positions in each of the two languages.
This complexity leads us toexplore sampling algorithms, as dynamic programming approaches are likely to be210Chung et al.
Sampling Tree Fragments from Forestsprohibitively slow.
In this section, we show that the problem of learning rules can beanalyzed as a problem of identifying tree fragments of unknown size and shape in aforest derived from the input word alignments for each sentence.
These tree fragmentsare similar to the tree fragments used in TSG learning.
As in TSG learning, each ruleof the final grammar consists of some number of adjacent, minimal tree fragments:one-level treebank expansions in the case of TSG learning and minimal SCFG rules,defined subsequently, in the case of translation.
The internal structure of TSG rules isused during parsing to determine the final tree structure to output, and the internalstructure of machine translation rules will not be used at decoding time.
This distinctionis irrelevant during learning.
A more significant difference from TSG learning is that thesets of minimal tree fragments in our SCFG application come not from a single, knowntree, but rather from a forest representing the set of bracketings consistent with the inputword alignments.We now proceed to precisely define this phrase decomposition forest and discusssome of its theoretical properties.
The phrase decomposition forest is designed to extendthe phrase decomposition tree defined by Zhang, Gildea, and Chiang (2008) in order toexplicitly represent each possible minimal rule with a hyperedge.A span [i, j] is a set of contiguous word indices {i, i + 1, .
.
.
, j ?
1}.
Given an alignedChinese?English sentence pair, a phrase n is a pair of spans n = ([i1, j1], [i2, j2]) such thatChinese words in positions [i1, j1] are aligned only to English words in positions [i2, j2],and vice versa.
A phrase forest H = ?V,E?
is a hypergraph made of a set of hypernodesV and a set of hyperedges E. Each node n = ([i1, j1], [i2, j2]) ?
V is a tight phrase asdefined by Koehn, Och, and Marcu (2003), namely, a phrase containing no unalignedwords at its boundaries.
A phrase n = ([i1, j1], [i2, j2]) covers n?
= ([i?1, j?1], [i?2, j?2]) ifi1 ?
i?1 ?
j?1 ?
j1 ?
i2 ?
i?2 ?
j?2 ?
j2Note that every phrase covers itself.
It follows from the definition of phrases that ifi1 ?
i?1 ?
j?1 ?
j1, then i2 ?
i?2 ?
j?2 ?
j2.
That means we can determine phrase coverage byonly looking at one language side of the phrases.
We are going to use this propertyto simplify the discussion of our proofs.
We also define coverage between two sets ofphrases.
Given two sets of phrases T and T?, we say T?
covers T if for all t ?
T, thereexists a t?
?
T?
such that t?
covers t. We say that two phrases overlap if they intersect,but neither covers the other.If two phrases n = ([i1, j1], [i2, j2]) and n?
= ([i?1, j?1], [i?2, j?2]) intersect, we can take theunion of the two phrases by taking the union of the source and target language spans,respectively.
That is, n1 ?
n2 = ([i1, j1] ?
[i?1, j?1], [i2, j2] ?
[i?2, j?2]).
An important propertyof phrases is that if two phrases intersect, their union is also a phrase.
For example,given that have a date with her and with her today are both valid phrases in Figure 2, havea date with her today must also be a valid phrase.
Given a set T of phrases, we definethe union closure of the phrase set T, denoted??
(T), to be constructed by repeatedlyjoining intersecting phrases until there are no intersecting phrases left.Each edge in E, written as T ?
n, is made of a set of non-intersecting tail nodes T ?V, and a single head node n ?
V that covers each tail node.
Each edge is an SCFG ruleconsistent with the word alignments.
Each tail node corresponds to a right-hand-sidenonterminal in the SCFG rule, and any position included in n but not included in any tailnode corresponds to a right-hand-side terminal in the SCFG rule.
For example, given thealigned sentence pair of Figure 2, the edge {([3, 4], [5, 6]), ([5, 6], [3, 4])} ?
([2, 6], [1, 6]),corresponds to a SCFG rule X ??
X1 ?
X2, have a X2 with X1.211Computational Linguistics Volume 40, Number 1?I??have?a?date?with?
?hertodayFigure 2Example word alignment, with boxes showing valid phrase pairs.
In this example, all individualalignment points are also valid phrase pairs.For the rest of this section, we assume that there are no unaligned words.
Unalignedwords can be temporarily removed from the alignment matrix before building thephrase decomposition forest.
After extracting the forest, they are put back into thealignment matrix.
For each derivation in the phrase decomposition forest, an unalignedword appears in the SCFG rule whose left-hand side corresponds to the lowest forestnode that covers the unaligned word.Definition 1An edge T ?
n is minimal if there does not exist another edge T?
?
n such that T?covers T.A minimal edge is an SCFG rule that cannot be decomposed by factoring out somepart of its right-hand side as a separate rule.
We define a phrase decomposition forestto be made of all phrases from a sentence pair, connected by all minimal SCFG rules.A phrase decomposition forest compactly represents all possible SCFG rules that areconsistent with word alignments.
For the example word alignment shown in Figure 2,the phrase decomposition forest is shown in Figure 3.
Each boxed phrase in Figure 2corresponds to a node in the forest of Figure 3, and hyperedges in Figure 3 representways of building phrases out of shorter phrases.A phrase decomposition forest has the important property that any SCFG ruleconsistent with the word alignment corresponds to a contiguous fragment of somecomplete tree found in the forest.
For example, the highlighted tree fragment of theforest in Figure 3 corresponds to the SCFG rule:X ?
?
X2 ?
X1, have a X1 with X2Thus any valid SCFG rule can be formed by selecting a set of adjacent hyperedges fromthe forest and composing the minimal SCFG rules specified by each hyperedge.
Wewill apply the sampling algorithm developed in Section 2 to this problem of selectinghyperedges from the forest.212Chung et al.
Sampling Tree Fragments from ForestsFigure 3A phrase decomposition forest extracted from the sentence pair ????????
?, I havea date with her today?.
Each edge is a minimal SCFG rule, and the rules at the bottom level arephrase pairs.
Unaligned word ?a?
shows up in the rule X ?
X1X2,X1aX2 after unalignedwords are put back into the alignment matrix.
The highlighted portion of the forest showsan SCFG rule built by composing minimal rules.The structure and size of phrase decomposition forests are constrained by thefollowing lemma:Lemma 2When there exists more than one minimal edge leading to the same head node n =([i1, j1], [i2, j2]), each of these minimal edges is a binary split of phrase pair n, whichgives us either a straight or inverted binary SCFG rule with no terminals.ProofSuppose that there exist two minimal edges T1 ?
n and T2 ?
n leading to node n.Consider the node set we get by taking the union closure of the tail nodes in T1and T2:??
(T1 ?
T2) ?
nFigure 4 shows two cases of this construction.
We show only the spans on the sourcelanguage side, which is enough to determine coverage properties.
Let n have span [i, j]on the source side.
In the first case (left),??
(T1 ?
T2) = {n}.
We know??
(T1 ?
T2) ?
nis also a valid edge because the unions of intersecting phrases are phrases, too.
By thedefinition of union closure,??
(T1 ?
T2) covers both T1 and T2.
Therefore T1 ?
n and213Computational Linguistics Volume 40, Number 1T2 ?
n cannot both be minimal.
In the second case (right),??
(T1 ?
T2) = {n}.
Thismeans that the phrases in T1 ?
T2 overlap one another in a chain covering the entire spanof n. There must exist a phrase n1 = [i, k1] in T1 or T2 that begins at the left boundaryi of n. Without loss of generality, assume that n1 ?
T1.
There must exist another phrasen2 = [k2, j2] ?
T2 that overlaps with n such that k2 < k1 and j2 > k1.
The span [k2, j] isa valid phrase, because it consists of the union closure of all phrases that begin to theright of k2:??
{[i?, j?]
| [i?, j?]
?
T1 ?
T2 ?
i?
?
k2} = {[k2, j]}We also know that n1 ?
n2 = [i, k2] is a valid phrase because the difference of twooverlapping phrases is also a valid phrase.
Therefore k2 is a valid binary split point of n,which means that either T2 is an edge formed by this binary split, or T2 is not minimal.The span [k2, j]?
n1 = [k1, j] is also a valid phrase formed by taking the difference oftwo overlapping phrases, which makes k1 a valid binary split point for n. This makes T1either an edge formed by the binary split at k1, or not a minimal phrase.
Thus, wheneverwe have two minimal edges, both consist of a binary split.
Another interesting property of phrase decomposition forests relates to the lengthof derivations.
A derivation is a tree of minimal edges reaching from a given node allthe way down to the forest?s terminal nodes.
The length of a derivation is the numberof minimal edges it contains.Lemma 3All derivations under a node in a phrase decomposition forest have the same length.ProofThis is proved by induction.
As the base case, all the nodes at the bottom of thephrase decomposition forest have only one derivation of length 1.
For the inductionstep, we consider the two possible cases in Lemma 2.
The case where a node n hasonly a single edge underneath is trivial.
It can have only one derivation length be-cause the children under that single edge already do.
For the case where there aremultiple valid binary splits for a node n at span (i, j), we assume the split points arek1, .
.
.
, ks.
Because the intersection of two phrases is also a phrase, we know that spans(i, k1), (k1, k2), .
.
.
, (ks, j) are all valid phrases, and so is any concatenation of consecutivephrases in these spans.
Any derivation in this sub-forest structure leading from theses + 1 spans to n has length s, which completes the proof under the assumption of theinduction.
??
(T1 ?
T2)T2T1??
(T1 ?
T2)T2i k1k2 j2 jT1Figure 4Sketch of proof for Lemma 2.
In the first case,??
(T1 ?
T2) consists of more than one span, orconsists of one span that is strictly smaller than n. In the second case,??
(T1 ?
T2) = {n}.214Chung et al.
Sampling Tree Fragments from ForestsBecause all the different derivations under the same node in a minimal phrase forestcontain the same number of minimal rules, we call that number the level of a node.
Thefact that nodes can be grouped by levels forms the basis of our fast iterative samplingalgorithm as described in Section 5.3.3.1 Constructing the Phrase Decomposition ForestGiven a word-aligned sentence pair, a phrase decomposition tree can be extracted witha shift-reduce algorithm (Zhang, Gildea, and Chiang 2008).
Whereas the algorithm ofZhang, Gildea, and Chiang (2008) constructs a single tree which compactly representsthe set of possible phrase trees, we wish to represent the set of all trees as a forest.We now describe a bottom?up parsing algorithm, shown in Algorithm 2, for buildingthis forest.
The algorithm considers all spans (i, j) in order of increasing length.
TheCYK-like loop over split points k (line 10) is only used for the case where a phrase canbe decomposed into two phrases, corresponding to a binary SCFG rule with no right-hand side terminals.
By Lemma 2, this is the only source of ambiguity in constructingAlgorithm 2 The CYK-like algorithm for building a phrase decomposition forest fromword-aligned sentence pair ?f, e?.1: Extract all phrase pairs in the form of ([i1, j1], [i2, j2])2: Build a forest node for each phrase pair, and let n(i, j) be the node corresponding to the phrasepair whose source side is [i, j]3: for s = 1 .
.
.
|f | do4: for i = 0 .
.
.
|f | ?
s do5: j ?
i + s6: if n(i, j) exists then7: continue8: end if9: split ?
010: for k = i + 1 .
.
.
j ?
1 do11: if both n(i, k) and n(k, j) exist then12: add edge {n(i, k),n(k, j)} ?
n(i, j)13: split ?
split + 114: end if15: end for16: if split = 0 then17: T ?
?18: l ?
i19: while l < j do20: l?
?
l + 121: for m ?
j .
.
.
l do22: if n(l,m) exists then23: T ?
T ?
n(l,m)24: l?
?
m25: break26: end if27: end for28: l ?
l?29: end while30: add edge T ?
n(i, j)31: end if32: end for33: end for215Computational Linguistics Volume 40, Number 1phrase decompositions.
When no binary split is found (line 16), a single hyperedge ismade that connects the current span with all its maximal children.
(A child is maximalif it is not itself covered by another child.)
This section can produce SCFG rules withmore than two right-hand side nonterminals, and it also produces any rules containingboth terminals and nonterminals in the right-hand side.
Right-hand side nonterminalscorrespond to previously constructed nodes n(l,m) in line 23, and right-hand sideterminals correspond to advancing a position in the string in line 20.The running time of the algorithm is O(n3) in terms of the length of the Chinesesentence f .
The size of the resulting forests depends on the input alignments.
The worstcase in terms of forest size is when the input consists of a monotonic, one-to-one wordalignment.
In this situation, all (i, k, j) tuples correspond to valid hyperedges, and thesize of the output forest is O(n3).
At the other extreme, when given a non-decomposablepermutation as an input alignment, the output forest consists of a single hyperedge.In practice, given Chinese?English word alignments from GIZA++, we find that theresulting forests are highly constrained, and the algorithm?s running time is negligiblein our overall system.
In fact, we find it better to rebuild every forest from a wordalignment every time we re-sample a sentence, rather than storing the hypergraphsacross sampling iterations.4.
Comparison of Sampling MethodsTo empirically verify the sampling methods presented in Section 2, we construct phrasedecomposition forests over which we try to learn composed translation rules.
In thissection, we use a simple probability model for the tree probability Pt in order to studythe convergence behavior of our sampling algorithm.
We will use a more sophisticatedprobability model for our end-to-end machine translation experiments in Section 5.For studying convergence, we desire a simpler model with a probability that can beevaluated in closed form.4.1 ModelWe use a very basic generative model based on a Dirichlet process defined overcomposed rules.
The model is essentially the same as the TSG model used by Cohn,Goldwater, and Blunsom (2009) and Post and Gildea (2009).We define a single Dirichlet process over the entire set of rules.
We draw the ruledistribution G from a Dirichlet process, and then rules from G.G | ?,P0 ?
Dir(?,P0)r | G ?
GFor the base distribution P0, we use a very simple uniform distribution where all rulesof the same size have equal probability:P0(r) = Vf?|rf |Ve?|re|where Vf is the vocabulary size of source language, and |rf | is the length of the sourceside of the rule r. Integrating over G, we get a Chinese restaurant process for theDirichlet process.
Customers in the Chinese restaurant analogy represent translationrule instances in the machine translation setting, and tables represent rule types.
The216Chung et al.
Sampling Tree Fragments from ForestsChinese restaurant has an infinite number of tables, and customers enter one by one andchoose a table to sit at.
Let zi be the table chosen by ith customer.
Then, the probabilityof the customer choosing a table which is already occupied by customers who enteredthe restaurant previously, or a new table, is given by following equations:P(zi = t | z?i) ={nti?1+?
1 ?
t ?
T?i?1+?
t = T + 1where z?i is the current seating arrangement, t is the index of the table, nt is the numberof customers at the table t, and T is the total number of occupied tables in the restaurant.In our model, a table t has a label indicating to which rule r the table is assigned.
Thelabel of a table is drawn from the base distribution P0.If we marginalize over tables labeled with the same rule, we get the followingprobability of choosing r given the current analysis z?i of the data:P(ri = r | z?i) =nr + ?P0(r)n + ?
(11)where nr is the number of times rule r has been observed in z?i, and n is total numberof rules observed in z?i.4.2 Sampling MethodsWe wish to sample from the set of possible decompositions into rules, including com-posed rules, for each sentence in our training data.
We follow the top?down samplingschedule discussed in Section 2 and also implement tree-level rejection sampling as abaseline.Our rejection sampling baseline is a form of Metropolis-Hastings where a new treet is resampled from a simple proposal distribution Q(t), and then either accepted orrejected according the Metropolis-Hastings acceptance rule, as shown in Algorithm 3.As in Algorithm 1, we use v(z, i) to denote a top?down ordering of forest variables.
As inall our experiments, Pt is the current tree probability conditioned on the current trees forall other sentences in our corpus, using Equation (11) as the rule probability in Equation(10).Our proposal distribution samples each variable with uniform probability workingtop?down through the forest.
The proposal distribution for an entire tree is thus:Q(t) =?w?z[t]1deg(w)This does not correspond to a uniform distribution over entire trees for the reasons dis-cussed in Section 2.
However, the Metropolis-Hastings acceptance probability correctsfor this, and thus the algorithm is guaranteed to converge to the correct distribution inthe long term.
We will show that, because the proposal distribution does not re-use anyof the variable settings from the current tree, the rejection sampling algorithm convergesmore slowly in practice than the more sophisticated alternative described in Section 2.2.We now describe in more detail our implementation of the approach of Section 2.2.We define two operations on a hypergraph node n, SAMPLECUT and SAMPLEEDGE, tochange the sampled tree from the hypergraph.
SAMPLECUT(n) chooses whether n is a217Computational Linguistics Volume 40, Number 1Algorithm 3 Metropolis-Hastings sampling algorithm.Require: A function v(z, i) returning the index of the ith variable of z in a top-down ordering ofthe variables of the tree ?
(z).1: i ?
12: while i < |Z[?
(z)]| do3: Sample znewv(z,i) according to uniform(znewv(z,i) )4: i ?
i + 15: end while6: z ?
{znew w/probmin{1, Pt (t(znew ))Q(t(zold ))Pt (t(zold ))Q(t(znew ))}zold otherwisesegmentation point or not, deciding if two rules should merge, while SAMPLEEDGE(n)chooses a hyperedge under n, making an entire new subtree.
Algorithm 4 shows ourimplementation of Algorithm 1 in terms of tree operations and the sampling operationsSAMPLEEDGE(n) and SAMPLECUT(n).4.3 ExperimentsWe used a Chinese?English parallel corpus available from the Linguistic Data Consor-tium (LDC), composed of newswire text.
The corpus consists of 41K sentence pairs,which is 1M words on the English side.
We constructed phrase decomposition forestswith this corpus and ran the top?down sampling algorithm and the rejection samplingalgorithm described in Section 4.2 for one hundred iterations.We used?
= 100 for everyexperiment.
The likelihood of the current state was calculated for every iteration.
Eachsetting was repeated five times, and then we computed the average likelihood for eachiteration.Figure 5 shows a comparison of the likelihoods found by rejection sampling andtop?down sampling.
As expected, we found that the likelihood converged much morequickly with top?down sampling.
Figure 6 shows a comparison between two differentversions of top?down sampling: the first experiment was run with the density factordescribed in Section 2, Equation (6), and the second one was run without the densityfactor.
The density factor has a much smaller effect on the convergence of our algorithmthan does the move from rejection sampling to top?down sampling, such that the dif-ference between the two curves shown in Figure 6 is not visible at the scale of Figure 5.
(The first ten iterations are omitted in Figure 6 in order to highlight the difference.)
Thesmall difference is likely due to the fact that our trees are relatively evenly balanced,Algorithm 4 Top?down sampling algorithm.1: queue.push(root)2: while queue is not empty do3: n = queue.pop()4: SAMPLEEDGE(n)5: SAMPLECUT(n)6: for each child c of node n do7: queue.push(c)8: end for9: end while218Chung et al.
Sampling Tree Fragments from ForestsFigure 5Likelihood graphs for rejection sampling and top?down sampling.Figure 6Likelihood graphs for top?down sampling with and without density factor.
The first teniterations are omitted to highlight the difference.such that the ratio of the density factor for two trees is not significant in comparisonto the ratio of their model probabilities.
Nevertheless, we do find higher likelihoodstates with the density factor than without it.
This shows that, in addition to providinga theoretical guarantee that our Markov chain converges to the desired distribution Ptin the limit, the density factor also helps us find higher probability trees in practice.219Computational Linguistics Volume 40, Number 15.
Application to Machine TranslationThe results of the previous section demonstrate the performance of our algorithm interms of the probabilities of the model it is given, but do not constitute an end-to-endapplication.
In this section we demonstrate its use in a complete machine translationsystem, using the SCFG rules found by the sampler in a Hiero-style MT decoder.
Wediscuss our approach and how it relates to previous work in machine translation inSection 5.1 before specifying the precise probability model used for our experiments inSection 5.2, discussing a technique to speed-up the model?s burn-in in Section 5.3, anddescribing our experiments in Section 5.4.5.1 ApproachA typical pipeline for training current statistical machine translation systems consistsof the following three steps: word alignment, rule extraction, and tuning of featureweights.
Word alignment is most often performed using the models of Brown et al.
(1993) and Vogel, Ney, and Tillmann (1996).
Phrase extraction is performed differentlyfor phrase-based (Koehn, Och, and Marcu 2003), hierarchical (Chiang 2005), and syntax-based (Galley et al.
2004) translation models, whereas tuning algorithms are generallyindependent of the translation model (Och 2003; Chiang, Marton, and Resnik 2008;Hopkins and May 2011).Recently, a number of efforts have been made to combine the word alignment andrule extraction steps into a joint model, with the hope both of avoiding some of theerrors of the word-level alignment, and of automatically learning the decompositionof sentence pairs into rules (DeNero, Bouchard-Cote, and Klein 2008; Blunsom et al.2009; Blunsom and Cohn 2010a; Neubig et al.
2011).
This approach treats both wordalignment and rule decomposition as hidden variables in an EM-style algorithm.
Whilethese efforts have been able to match the performance of systems based on two succes-sive steps for word alignment and rule extraction, they have generally not improvedperformance enough to become widely adopted.
One possible reason for this is theadded complexity and in particular the increased computation time when comparedto the standard pipeline.
The accuracy of word-level alignments from the standardGIZA++ package has proved hard to beat, in particular when large amounts of trainingdata are available.Given this state of affairs, the question arises whether static word alignments canbe used to guide rule learning in a model which treats the decomposition of a sentencepair into rules as a hidden variable.
Such an approach would favor rules which areconsistent with the other sentences in the data, and would contrast with the standardpractice inHiero-style systems of simply extracting all overlapping rules consistent withstatic word alignments.
Constraining the search over rule decomposition with wordalignments has the potential to significantly speed up training of rule decompositionmodels, overcoming one of the barriers to their widespread use.
Rule decompositionmodels also have the benefit of producing much smaller grammars than are achievedwhen extracting all possible rules.
This is desirable given that the size of translationgrammars is one of the limiting computational factors in current systems, necessitatingelaborate strategies for rule filtering and indexing.In this section, we apply our sampling algorithm to learn rules for the Hiero trans-lation model of Chiang (2005).
Hiero is based on SCFG, with a number of constraints onthe form that rules can take.
The grammar has a single nonterminal, and each rule has220Chung et al.
Sampling Tree Fragments from Forestsat most two right-hand side nonterminals.
Most significantly, Hiero allows rules withmixed terminals and nonterminals on the right-hand side.
This has the great benefitof allowing terminals to control re-ordering between languages, but also leads to verylarge numbers of valid rules during the rule extraction process.
We wish to see whether,by adding a learned model of sentence decomposition to Hiero?s original method ofleveraging fixed word-level alignments, we can learn a small set of rules in a system thatis both efficient to train and efficient to decode.
Our approach of beginning with fixedword alignments is similar to that of Sankaran, Haffari, and Sarkar (2011), althoughtheir sampling algorithm reanalyzes individual phrases extracted with Hiero heuristicsrather than entire sentences, and produces rules with no more than one nonterminalon the right-hand side.Most previous works on joint word alignment and rule extraction models wereevaluated indirectly by resorting to heuristic methods to extract rules from learnedword alignment or bracketing structures (DeNero, Bouchard-Cote, and Klein 2008;Zhang et al.
2008; Blunsom et al.
2009; Levenberg, Dyer, and Blunsom 2012), and do notdirectly learn the SCFG rules that are used during decoding.
In this article, we workwith lexicalized translation rules with a mix of terminals and nonterminals, and weuse the rules found by our sampler directly for decoding.
Because word alignments arefixed in our model, any improvements we observe in translation quality indicate thatour model learns how SCFG rules interplay with each other, rather than fixing wordalignment errors.The problem of rule decomposition is not only relevant to the Hiero model.Translation models that make use of monolingual parsing, such as string-to-tree(Galley et al.
2004), and tree-to-string (Liu, Liu, and Lin 2006), are all known tobenefit greatly from learning composed rules (Galley et al.
2006).
In the particularcase of Hiero rule extraction, although there is no explicit rule composition step, theextracted rules are in fact ?composed rules?
in the sense of string-to-tree or tree-to-string rule extraction, because they can be further decomposed into smaller SCFG rulesthat are also consistent with word alignments.
Although our experiments only includethe Hiero model, the method presented in this article is also applicable to string-to-tree and tree-to-string models, because the phrase decomposition forest presented inSection 3 can be extended to rule learning and extraction of other syntax-based MTmodels.5.2 ModelIn this section, we describe a generative model based on the Pitman-Yor process (Pitmanand Yor 1997; Teh 2006) over derivation trees consisting of composed rules.
Bayesianmethods have been applied to a number of segmentation tasks in natural language pro-cessing, including word segmentation, TSG learning, and learning machine translationrules, as a way of controlling the overfitting produced when Expectation Maximizationwould tend to prefer longer segments.
However, it is important to note that the Bayesianpriors in most cases control the size and number of the clusters, but do not explicitlycontrol the size of rules.
In many cases, this type of Bayesian prior alone is not strongenough to overcome the preference for longer, less generalizable rules.
For example,some previous work in word segmentation (Liang and Klein 2009; Naradowsky andToutanova 2011) adopts a ?length penalty?
to remedy this situation.
Because we havethe prior knowledge that longer rules are less likely to generalize and are therefore lesslikely to be a good rule, we adopt a similar scheme to control the length of rules inour model.221Computational Linguistics Volume 40, Number 1In order to explicitly control the length of our rules, we generate a rule r in twostages.
First, we draw the length of a rule |r| =  from a probability distribution definedover positive integers.
We use a Poisson distribution:P(; ?)
= ?e?
?!Because of the factorial in the denominator, the Poisson distribution decays quickly as becomes larger, which is ideal for selecting rule length because we want to encouragelearning of shorter rules and learn longer rules only when there is strong evidence forthem in the data.A separate Pitman-Yor process is defined for the rules of each length .
We drawthe rule distribution G from a Pitman-Yor process, and then rules of length  are drawnfrom G.G | ?, d,P0 ?
PY(?, d,P0)r | G ?
GThe first two parameters, a concentration parameter ?
and a discount parameter d,control the shape of distribution G by controlling the size and the number of clusters.The label of the cluster is decided by the base distribution P0.
Because our alignmentis fixed, we do not need a complex base distribution that differentiates better alignedphrases from others.
We use a uniform distribution where each rule of the same sizehas equal probability.
Since the number of possible shorter rules is smaller than that oflonger rules, we need to reflect this fact and need to have larger uniform probabilityfor shorter rules and smaller uniform probability for longer rules.
We reuse the Poissonprobability for the base distribution, essentially assuming that the number of possiblerules of length  is 1/P(; ?
).The Pitman-Yor process gives us the following probability of choosing r of size given the current analysis z?i of the data:P(ri = r | , z?i) =nr ?
Trd + (Td + ?
)P0(r)n + ?where nr is the number of times rule r has been observed in z?i, Tr is the number oftables (in the Chinese restaurant metaphor) labeled r, and n is the total number of rulesof length  observed in z?i.
Because we have drawn the length of the rule from a Poissondistribution, the rule length probability is multiplied by this equation in order to obtainthe probability of the rule under our model.Keeping track of table assignments during inference requires a lot of book-keeping.In order to simplify the implementation, instead of explicitly keeping track of thenumber of tables for each rule, we estimate the number of tables using the followingequations (Huang and Renals 2010):Tr = ndrT =?r:|r|=ndr222Chung et al.
Sampling Tree Fragments from ForestsIn order to encourage learning rules with smaller parsing complexity and rules withmixed terminals and nonterminals, which are useful for replicating re-orderings thatare seen in the data, we made use of the concept of scope (Hopkins and Langmead2010) in our definition of rule length.
The scope of a rule is defined as the number ofpairs of adjacent nonterminals in the source language right-hand side plus the numberof nonterminals at the beginning or end of the source language right-hand side.
Forexample,X ?
f1X1X2f2X3, X1e1X2X3e2has scope 2 because X1 and X2 are adjacent in the source language and X3 is at theend of the source language right-hand side.
The target side of the rule is irrelevant.The intuition behind this definition is that it measures the number of free indices intothe source language string required during parsing, under the assumption that theterminals provide fixed anchor points into the string.
Thus a rule of scope of k can beparsed in O(nk).
We define the length of a rule to be the number of terminals in thesource and the target side plus the scope of the rule.
This is equivalent to counting thetotal number of symbols in the rule, but only counting a nonterminal if it contributesto parsing complexity.
For example, the length of a rule that consists only of twoconsecutive nonterminals would be 3, and the length of a rule that has two consecutivenonterminals bounded by terminals on both sides would be 3 as well.
This definitionof rule length encourages rules with mixed terminals and nonterminals over rules withonly nonterminals, which tend not to provide useful guidance to the translation processduring decoding.5.3 Stratified SamplingWe follow the same Gibbs sampler introduced in Section 4.2.
The SAMPLEEDGE oper-ation in our Gibbs sampler can be a relatively expensive operation, because the entiresubtree under a node is being changed during sampling.
We observe that in a phrasedecomposition forest, lexicalized rules, which are crucial to translation quality, appearat the bottom level of the forest.
This lexicalized information propagates up the forestas rules get composed.
It is reasonable to constrain initial sampling iterations to workonly on those bottom level nodes, and then gradually lift the constraint.
This not onlymakes the sampler much more efficient, but also gives it a chance to focus on gettingbetter estimates of the more important parameters, before starting to consider nodesat higher levels, which correspond to rules of larger size.
Fortunately, as mentioned inSection 3, each node in a phrase decomposition forest already has a unique level, withlevel 1 nodes corresponding to minimal phrase pairs.
We design the sampler to use astratified sampling process (i.e., sampling level one nodes for K iterations, then level 1and 2 nodes for K iterations, and so on).
We emphasize that when we sample for level2 nodes, level 1 nodes are also sampled, which means parameters for the smaller rulesare given more chance to mix, and thereby settle into a more stable distribution.In our experiments, running the first 100 iterations of sampling with regular sam-pling techniques took us about 18 hours.
However, with stratified sampling, it tookonly about 6 hours.
We also compared translation quality as measured by decodingwith rules from the 100th sample, and by averaging over every 10th sample.
Bothsampling methods gave us roughly the same translation quality as measured in BLEU.We therefore used stratified sampling throughout our experiments.223Computational Linguistics Volume 40, Number 15.4 ExperimentsWe used a Chinese?English parallel corpus available from LDC,1 composed ofnewswire text.
The corpus consists of 41K sentence pairs, which is 1M words on theEnglish side.
We used a 392-sentence development set with four references for parame-ter tuning, and a 428-sentence test set with four references for testing.2 The developmentset and the test set have sentences with less than 30 words.
A trigram language modelwas used for all experiments.
BLEU (Papineni et al.
2002) was calculated for evaluation.5.4.1 Baseline.
For our baseline system, we extract Hiero translation rules using theheuristic method (Chiang 2007), with the standard Hiero rule extraction constraints.We use our in-house SCFG decoder for translation with both the Hiero baseline and oursampled grammars.
Our features for all experiments include differently normalized rulecounts and lexical weightings (Koehn, Och, and Marcu 2003) of each rule.
Weights aretuned using Pairwise Ranking Optimization (Hopkins and May 2011) using the baselinegrammar and development set, then used throughout the experiments.Because our sampling procedure results in a smaller rule table, we also establish ano-singleton baseline to compare our results to a simple heuristic method of reducingrule table size.
The no-singleton baseline discards rules that occur only once and thathave more than one word on the Chinese side during the Hiero rule extraction process,before counting the rules and computing feature scores.5.4.2 Experimental Settings.Model parameters.
For all experiments, we used d = 0.5 for the Pitman-Yor discountparameter, except where we compared the Pitman-Yor process with Dirichlet process(d = 0).
Although we have a separate Pitman-Yor process for each rule length, we usedthe same ?
= 5 for all rule sizes in all experiments, including Dirichlet process experi-ments.
For rule length probability, a Poisson distribution where ?
= 2 was used for allexperiments.Sampling.
The samples are initialized such that all nodes in a forest are set to be seg-mented, and a random edge is chosen under each node.
For all experiments, we ran thesampler for 100 iterations and took the sample from the last iteration to compare withthe baseline.
For stratified sampling, we increased the level we sample at every 10thiteration.
We also tried ?averaging?
samples, where samples from every 10th iterationare merged to a single grammar.
For averaging samples, we took the samples from the0th iteration (initialization) to the 70th iteration at every 10th iteration.3 We decidedon the 70th iteration (last iteration of level 7 sampling) as the last iteration becausewe constrained the sampler not to sample nodes whose span covers more than sevenwords (for SAMPLECUT only, SAMPLECUT always segments for these nodes), and thelikelihood becomes very stable at that point.1 We randomly sampled our data from various different sources (LDC2006E86, LDC2006E93, LDC2002E18,LDC2002L27, LDC2003E07, LDC2003E14, LDC2004T08, LDC2005T06, LDC2005T10, LDC2005T34,LDC2006E26, LDC2005E83, LDC2006E34, LDC2006E85, LDC2006E92, LDC2006E24, LDC2006E92,LDC2006E24).
The language model is trained on the English side of entire data (1.65M sentences,which is 39.3M words).2 They are from newswire portion of NIST MT evaluation data from 2004, 2005, and 2006.3 Not including initialization has negligible effect on translation quality.224Chung et al.
Sampling Tree Fragments from ForestsRule extraction.
Because every tree fragment in the sampled derivation represents atranslation rule, we do not need to explicitly extract the rules; we merely need to collectthem and count them.
However, derivations include purely nonlexical rules that do notconform to the rule constraints of Hiero, and which are not useful for translation.
Toget rid of this type of rule, we prune every rule that has a scope greater than 2.
WhereasHiero does not allow two adjacent nonterminals in the source side, our pruning criterionallows some rules of scope 2 that are not allowed by Hiero.
For example, the followingrule (only source side shown) has scope 2 but is not allowed by Hiero:X ?
w1X1X2w2X3In order to see if these rules have any positive or negative effects on translation, wecompare a rule set that strictly conforms to the Hiero constraints and a rule set thatincludes all the rules of scope 2 or less.5.4.3 Results.
Table 2 summarizes our results.
As a general test of our probability model,we compare the result from initialization and the 100th sample.
The translation perfor-mance of the grammar from the 100th iteration of sampling is much higher than thatof the initialization state.
This shows that states with higher probability in our Markovchain generally do result in better translation, and that the sampling process is able tolearn valuable composed rules.In order to determine whether the composed rules learned by our algorithm areparticularly valuable, we compare them to the standard baseline of extracting all rules.The size of the grammar taken from the single sample (100th sample) is only about 9% ofthe baseline but still produces translation results that are not far worse than the baseline.A simple way to reduce the number of rules in the baseline grammar is to remove allrules that occur only once in the training data and that contain more than a single wordon the Chinese side.
This ?no-singleton?
baseline still leaves us with more rules thanour algorithm, with translation results between those of the standard baseline and ouralgorithm.We also wish to investigate the trade-off between grammar size and translationperformance that is induced by including rules from multiple steps of the samplingprocess.
It is helpful for translation quality to include more than one analysis of eachsentence in the final grammar in order to increase coverage of new sentences.
Averagingsamples also better approximates the long-term behavior of the Markov chain, whereastaking a single sample involves an arbitrary random choice.
When we average eightTable 2Comparisons of decoding results.iteration model pruning #rules dev test time (s)Baseline heuristic Hiero 3.59M 25.5 25.1 809No-singleton heuristic Hiero 1.09M 24.7 24.2 638Sampled 0th (init) Pitman-Yor scope < 3 212K 19.9 19.1 489Sampled 100th Pitman-Yor scope < 3 313K 23.9 23.3 1,214Sampled averaged (0 to 70) Pitman-Yor scope < 3 885K 26.2 24.5 1,488Sampled averaged (0 to 70) Pitman-Yor Hiero 785K 25.6 25.1 532Sampled averaged (0 to 70) Dirichlet scope < 3 774K 24.6 23.8 930225Computational Linguistics Volume 40, Number 1different samples, we get a larger number of rules than from a single sample, but stillonly a quarter as many rules as in the Hiero baseline.
The translation results with eightsamples are comparable to the Hiero baseline (not significantly different accordingto 1,000 iterations of paired bootstrap resampling [Koehn 2004]).
Translation resultsare better with the sampled grammar than with the no-singleton method of reducinggrammar size, while the sampled grammar was smaller than the no-singleton rule set.Thus, averaging samples seems to produce a good trade-off between grammar size andquality.The filtering applied to the final rule set affects both the grammar size and de-coding speed, because rules with different terminal/nonterminal patterns have vary-ing decoding complexities.
We experimented with two methods of filtering the finalgrammar: retaining rules of scope no greater than three, and the more restrictive theHiero constraints.
We do not see a consistent difference in translation quality betweenthese methods, but there is a large impact in terms of speed.
The Hiero constraintsdramatically speeds decoding.
The following is the full list of the Hiero constraints,taken verbatim from Chiang (2007): If there are multiple initial phrase pairs containing the same set ofalignments, only the smallest is kept.
That is, unaligned words arenot allowed at the edges of phrases. Initial phrases are limited to a length of 10 words on either side. Rules are limited to five nonterminals plus terminals on the French side. Rules can have at most two nonterminals, which simplifies the decoderimplementation.
This also makes our grammar weakly equivalent to aninversion transduction grammar (Wu 1997), although the conversionwould create a very large number of new nonterminal symbols. It is prohibited for nonterminals to be adjacent on the French side,a major cause of spurious ambiguity. A rule must have at least one pair of aligned words, so that translationdecisions are always based on some lexical evidence.Of these constraints, the differences between the Hiero constraints and scope filteringare: First, the Hiero constraints limit the number of nonterminals in a rule to no morethan two.
Second, the Hiero constraints do not allow two adjacent nonterminals inthe source side of a rule.
As discussed previously, these two differences limit Hierogrammar to be a subset of scope 2 grammar, whereas the scope-filtered grammar retainsall scope 2 rules.
Among grammars with the Hiero constraint, smaller grammars aregenerally faster.
The relationship between the number of rules and the decoding time isless than linear.
This is because the decoder never considers rules containing sequencesof terminals not present in the source sentence.
As the number of rules grows, we seerules with larger numbers of terminals that in turn apply to fewer input sentences.The sampled grammar has a more pronounced effect of reducing rule table size thandecoding speed.
Our sampling method may be particularly valuable for very large datasets where grammar size can become a limiting factor.Finally, we wish to investigate whether the added power of the Pitman-Yor processgives any benefit over the simpler Dirichlet process prior, using the same modelingof word length in both cases.
We find better translation quality with the Pitman-Yor226Chung et al.
Sampling Tree Fragments from Forestsprocess, indicating that the additional strength of the Pitman-Yor process in suppressinginfrequent rules helps prevent overfitting.6.
ConclusionWe presented a hypergraph sampling algorithm that overcomes the difficulties inherentin computing inside probabilities in applications where the segmentation of the tree intorules is not known.Given parallel text with word-level alignments, we use this algorithm to learnsentence bracketing and SCFG rule composition.
Our rule learning algorithm is basedon a compact structure that represents all possible SCFG rules extracted from word-aligned sentences pairs, and works directly with highly lexicalized model parameters.We show that by effectively controlling overfitting with a Bayesian model, and design-ing algorithms that efficiently sample that parameter space, we are able to learn morecompact grammars with competitive translation quality.
Based on the framework webuilt in this work, it is possible to explore other rule learning possibilities that are knownto help translation quality, such as learning refined nonterminals.Our general sampling algorithm is likely to be useful in settings beyond machinetranslation.
One interesting application would be unsupervised or partially supervisedlearning of (monolingual) TSGs, given text where the tree structure is completely orpartially unknown, as in the approach of Blunsom and Cohn (2010b).AcknowledgmentsThis work was partially funded by NSFgrant IIS-0910611.ReferencesBlunsom, P., T. Cohn, C. Dyer, andM.
Osborne.
2009.
A Gibbs sampler forphrasal synchronous grammar induction.In Proceedings of the Joint Conference of the47th Annual Meeting of the ACL and the4th International Joint Conference on NaturalLanguage Processing of the AFNLP: Volume 2,pages 782?790, Singapore.Blunsom, Phil and Trevor Cohn.
2010a.Inducing synchronous grammars withslice sampling.
In Proceedings of the HumanLanguage Technology: The 11th AnnualConference of the North American Chapter ofthe Association for Computational Linguistics,pages 238?241, Boulder, CO.Blunsom, Phil and Trevor Cohn.
2010b.Unsupervised induction of treesubstitution grammars for dependencyparsing.
In Proceedings of the 2010Conference on Empirical Methods in NaturalLanguage Processing, pages 1,204?1,213,Cambridge, MA.Brown, Peter F., Stephen A. Della Pietra,Vincent J. Della Pietra, and Robert L.Mercer.
1993.
The mathematics ofstatistical machine translation: Parameterestimation.
Computational Linguistics,19(2):263?311.Chappelier, Jean-Ce?dric and Martin Rajman.2000.
Monte-Carlo sampling for NP-hardmaximization problems in the frameworkof weighted parsing.
In Natural LanguageProcessing (NLP 2000), pages 106?117,Patras.Chiang, David.
2005.
A hierarchicalphrase-based model for statisticalmachine translation.
In Proceedings of the43rd Annual Conference of the Associationfor Computational Linguistics (ACL-05),pages 263?270, Ann Arbor, MI.Chiang, David.
2007.
Hierarchicalphrase-based translation.
ComputationalLinguistics, 33(2):201?228.Chiang, David, Yuval Marton, and PhilipResnik.
2008.
Online large-margin trainingof syntactic and structural translationfeatures.
In Conference on EmpiricalMethods in Natural Language Processing(EMNLP-08), pages 224?233, Honolulu, HI.Cohn, Trevor and Phil Blunsom.
2010.Blocked inference in Bayesian treesubstitution grammars.
In Proceedings ofthe 48th Annual Meeting of the Associationfor Computational Linguistics (ACL-10),pages 225?230, Uppsala.Cohn, Trevor, Sharon Goldwater, andPhil Blunsom.
2009.
Inducing compactbut accurate tree-substitution grammars.In Proceedings of Human Language227Computational Linguistics Volume 40, Number 1Technologies: The 2009 Annual Conferenceof the North American Chapter of theAssociation for Computational Linguistics,pages 548?556, Boulder, CO.DeNero, John, Alexandre Bouchard-Cote,and Dan Klein.
2008.
Sampling alignmentstructure under a Bayesian translationmodel.
In Conference on EmpiricalMethods in Natural Language Processing(EMNLP-08), pages 314?323, Honolulu, HI.Galley, Michel, Jonathan Graehl, KevinKnight, Daniel Marcu, Steve DeNeefe,Wei Wang, and Ignacio Thayer.
2006.Scalable inference and training ofcontext-rich syntactic translation models.In Proceedings of the International Conferenceon Computational Linguistics/Associationfor Computational Linguistics (COLING/ACL-06), pages 961?968, Sydney.Galley, Michel, Mark Hopkins, Kevin Knight,and Daniel Marcu.
2004.
What?s in atranslation rule?
In Proceedings of the2004 Meeting of the North AmericanChapter of the Association for ComputationalLinguistics (NAACL-04), pages 273?280,Boston, MA.Hopkins, Mark and Greg Langmead.2010.
SCFG decoding withoutbinarization.
In Proceedings of the2010 Conference on Empirical Methodsin Natural Language Processing,pages 646?655, Cambridge, MA.Hopkins, Mark and Jonathan May.
2011.Tuning as ranking.
In Proceedings of the2011 Conference on Empirical Methodsin Natural Language Processing,pages 1,352?1,362, Edinburgh.Huang, Songfang and Steve Renals.
2010.Power law discounting for n-gramlanguage models.
In Proceedings of theIEEE International Conference on Acoustic,Speech, and Signal Processing (ICASSP?10),pages 5,178?5,181, Dallas, TX.Johnson, Mark, Thomas L. Griffiths, andSharon Goldwater.
2007.
Bayesianinference for PCFGs via Markov chainMonte Carlo.
In Proceedings of the 2007Meeting of the North American Chapterof the Association for ComputationalLinguistics (NAACL-07), pages 139?146,Rochester, NY.Knuth, D. E. 1975.
Estimating the efficiencyof backtrack programs.
Mathematics ofComputation, 29(129):121?136.Koehn, Philipp.
2004.
Statistical significancetests for machine translation evaluation.In 2004 Conference on Empirical Methods inNatural Language Processing (EMNLP),pages 388?395, Barcelona.Koehn, Philipp, Franz Josef Och, andDaniel Marcu.
2003.
Statisticalphrase-based translation.
In Proceedingsof the 2003 Meeting of the North AmericanChapter of the Association for ComputationalLinguistics (NAACL-03), pages 48?54,Edmonton.Levenberg, Abby, Chris Dyer, and PhilBlunsom.
2012.
A Bayesian model forlearning SCFGs with discontiguous rules.In Proceedings of the 2012 Joint Conferenceon Empirical Methods in Natural LanguageProcessing and Computational NaturalLanguage Learning, pages 223?232,Jeju Island.Liang, Percy and Dan Klein.
2009.
OnlineEM for unsupervised models.
InProceedings of Human Language Technologies:The 2009 Annual Conference of the NorthAmerican Chapter of the Association forComputational Linguistics, pages 611?619,Boulder, CO.Liu, Yang, Qun Liu, and Shouxun Lin.2006.
Tree-to-string alignment templatefor statistical machine translation.In Proceedings of the 21st InternationalConference on Computational Linguistics and44th Annual Meeting of the Association forComputational Linguistics, pages 609?616,Sydney.Naradowsky, Jason and Kristina Toutanova.2011.
Unsupervised bilingual morphemesegmentation and alignment withcontext-rich hidden semi-Markov models.In Proceedings of the 49th Annual Meetingof the Association for ComputationalLinguistics: Human Language Technologies,pages 895?904, Portland, OR.Neubig, Graham, Taro Watanabe, EiichiroSumita, Shinsuke Mori, and TatsuyaKawahara.
2011.
An unsupervised modelfor joint phrase alignment and extraction.In Proceedings of the 49th Annual Meetingof the Association for ComputationalLinguistics: Human Language Technologies,pages 632?641, Portland, OR.Och, Franz Josef.
2003.
Minimum error ratetraining for statistical machine translation.In Proceedings of the 41th Annual Conferenceof the Association for ComputationalLinguistics (ACL-03), pages 160?167,Sapporo.Papineni, Kishore, Salim Roukos, ToddWard, and Wei-Jing Zhu.
2002.
BLEU:A method for automatic evaluation ofmachine translation.
In Proceedings of the40th Annual Conference of the Associationfor Computational Linguistics (ACL-02),pages 311?318, Philadelphia, PA.228Chung et al.
Sampling Tree Fragments from ForestsPitman, Jim and Marc Yor.
1997.
Thetwo-parameter Poisson-Dirichletdistribution derived from a stablesubordinator.
Annals of Probability,25(2):855?900.Post, Matt and Daniel Gildea.
2009.
Bayesianlearning of a tree substitution grammar.In Proceedings of the Association forComputational Linguistics (short paper),pages 45?48, Singapore.Sankaran, Baskaran, Gholamreza Haffari,and Anoop Sarkar.
2011.
Bayesianextraction of minimal SCFG rules forhierarchical phrase-based translation.In Proceedings of the Sixth Workshopon Statistical Machine Translation,pages 533?541, Edinburgh.Teh, Yee Whye.
2006.
A hierarchicalBayesian language model based onPitman-Yor processes.
In Proceedingsof the 21st International Conference onComputational Linguistics and 44th AnnualMeeting of the Association for ComputationalLinguistics, pages 985?992, Sydney.Vogel, Stephan, Hermann Ney, andChristoph Tillmann.
1996.
HMM-basedword alignment in statistical translation.In Proceedings of the 16th InternationalConference on Computational Linguistics(COLING-96), pages 836?841, Copenhagen.Wu, Dekai.
1997.
Stochastic inversiontransduction grammars and bilingualparsing of parallel corpora.
ComputationalLinguistics, 23(3):377?403.Zhang, Hao, Daniel Gildea, and DavidChiang.
2008.
Extracting synchronousgrammar rules from word-levelalignments in linear time.
In Proceedingsof the 22nd International Conference onComputational Linguistics (COLING-08),pages 1,081?1,088, Manchester.Zhang, Hao, Chris Quirk, Robert C. Moore,and Daniel Gildea.
2008.
Bayesian learningof non-compositional phrases withsynchronous parsing.
In Proceedings ofthe 46th Annual Meeting of the Associationfor Computational Linguistics (ACL-08),pages 97?105, Columbus, OH.229
