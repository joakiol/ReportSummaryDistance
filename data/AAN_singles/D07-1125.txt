Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp.
1144?1148,Prague, June 2007. c?2007 Association for Computational LinguisticsCovington VariationsSvetoslav MarinovSchool of Humanities and Informatics,University College Sko?vde, 54128 Sko?vde &GSLT, Go?teborg University, 40530 Go?teborg,SwedenSvetoslav.Marinov@his.seAbstractThree versions of the Covington algorithmfor non-projective dependency parsing havebeen tested on the ten different languagesfor the Multilingual track of the CoNLL-X Shared Task.
The results were achievedby using only information about heads anddaughters as features to guide the parserwhich obeys strict incrementality.1 IntroductionIn this paper we focus on two things.
First, we in-vestigate the impact of using different flavours ofCovington?s algorithm (Covington, 2001) for non-projective dependency parsing on the ten differ-ent languages provided for CoNLL-X Shared Task(Nivre et al, 2007).
Second, we test the perfor-mance of a pure grammar-based feature model instrictly incremental fashion.
The grammar model re-lies only on the knowledge of heads and daughtersof two given words, as well as the words themselves,in order to decide whether they can be linked with acertain dependency relation.
In addition, none of thethree parsing algorithms guarantees that the outputdependency graph will be projective.2 Covington?s algorithm(s)In his (2001) paper, Covington presents a ?funda-mental?
algorithm for dependency parsing, whichhe claims has been known since the 1960s but has,up to his paper-publication, not been presentedsystematically in the literature.
We take threeof its flavours, which enforce uniqueness (a.k.a.single-headedness) but do not observe projectivity.The algorithms work one word at a time andattempt to build a connected dependency graph withonly a single left-to-right pass through the input.The three flavours are: Exhaustive Search, HeadFirst with Uniqueness (ESHU), Exhaustive SearchDependents First with Uniqueness (ESDU) andList-based search with Uniqueness (LSU).ESHU ESDUfor i = 1 to n for i = 1 to nfor j = i-1 downto 0 for j = i-1 downto 0if HEAD?
(j,i) if HEAD?
(i,j)LINK(j,i) LINK(i,j)if HEAD?
(i,j) if HEAD?
(j,i)LINK(i,j) LINK(j,i)The yes/no function HEAD?
(w1,w2), checkswhether a word w1 can be a head of a word w2 ac-cording to a grammar G. It also respects the single-head and no-cycle conditions.
The LINK(w1,w2)procedure links word w1 as the head of word w2with a dependency relation as proposed by G. Whentraversing Headlist and Wordlist we start with thelast word added.
(Nivre, 2007) describes an op-timized version of Covington?s algorithm imple-mented in MaltParser (Nivre, 2006) with a runningtime c(n22 ?
n2 ) for an n-word sentence, where c issome constant time in which the LINK operationcan be performed.
However, due to time constraints,we will not bring this version of the algorithm intofocus, but see some preliminary remarks on it withrespect to our parsing model in 6.1144LSU1Headlist := []Wordlist := []while (!end-of-sentence)W := next input word;foreach D in Headlistif HEAD?
(W,D)LINK(W,D);delete D from Headlist;endforeach H in Wordlistif HEAD?
(H,W)LINK(H,W);terminate this foreach loop;endif no head for W was found thenHeadlist := W + Headlist;endWordlist := W + Wordlist;end3 Classifier as an Instant GrammarThe HEAD?
function in the algorithms presentedin 2, requires an ?instant grammar?
(Covington,2001) of some kind, which can tell the parserwhether the two words under scrutiny can be linkedand with what dependency relation.
To satisfythis requirement, we use TiMBL - a Memory-basedlearner (Daelemans et al, 2004) - as a classifier topredict the relation (if any) holding between the twowords.Building heavily on the ideas of History-basedparsing (Black et al, 1993; Nivre, 2006), trainingthe parser means essentially running the parsing al-gorithms in a learning mode on the data in orderto gather training instances for the memory-basedlearner.
In a learning mode, the HEAD?
functionhas access to a fully parsed dependency graph.
Inthe parsing mode, the HEAD?
function in the algo-rithms issues a call to the classifier using featuresfrom the parsing history (i.e.
a partially built depen-dency graph PG).Given words i and j to be linked, and a PG, thecall to the classifier is a feature vector ?
(i,j,PG) =(?1,.
.
.
,?m) (cf.
(Nivre, 2006; Nivre, 2007)).
The1Covington adds W to the Wordlist as soon as it has beenseen, however we have chosen to wait until after all tests havebeen completed.classifier then attempts to map this feature vector toany of predefined classes.
These are all the depen-dency relations, as defined by the treebank and theclass ?NO?
in the cases where no link between thetwo words is possible.4 The Grammar modelThe features used in our history-based model are re-stricted only to the partially built graph PG.
We callthis model a pure grammar-based model since theonly information the parsing algorithms have at theirdisposal is extracted from the graph, such as the headand daughters of the current word.
Preceding wordsnot included in the PG as well as words followingthe current word are not available to the algorithm.In this respect such a model is very restrictive andsuffers from the pitfalls of the incremental process-ing (Nivre, 2004).The motivation for the chosen model, was to ap-proximate a Data Oriented Parsing (DOP) model(e.g.
(Bod et al, 2003)) for Dependency Gram-mar.
Under DOP, analyses of new sentences are pro-duced by combining previously seen tree fragments.However, the tree fragments under the original DOPmodel are static, i.e.
we have a corpus of all possi-ble subtrees derived from a treebank.
Under our ap-proach, these tree fragments are built dynamically,as we try to parse the sentence.
Because of the cho-sen DOP approximation, we have not included in-formation about the preceding and following wordsof the two words to be linked in our feature model.To exemplify our approach, (1) shows a partiallybuild graph and all the words encountered so far andFig.
1 shows two examples of the tree-building op-erations for linking words f and d, and f and a.
(1) a b c d e f .
.
.Given two words i and j to be linked with adependency relation, such that word j precedesword i, the following features describe the modelson which the algorithms have been trained andtested:Word form: i, j, ds(i), ds(j), h(j/i), h(h(j/i))Lemma (if available): i, j, ds(i), ds(j), h(j/i),h(h(j/i))1145HEAD?adb                  cfeHEAD?
HEAD?adfeFigure 1: Application of the HEAD?
function on aninput from the PG in (1)Part-of-Speech: i, j, ds(i), ds(j), h(j/i), h(h(j/i))Dependency type: i, j, ds(i), ds(j), h(j/i),h(h(j/i))Features (if available): i, j, ds(i), ds(j), h(j/i),h(h(j/i))ds(i) means any two daughters (if available)of word i, h(i/j) refers to the head of word i orword j, depending on the direction of applying theHEAD?
function (see Fig 1) and h(h(i/j)) standsfor the head of the head of word i or word j.The basic model, which was used for the largesttraining data sets of Czech and Chinese, includesonly the first four features in every category.
Alarger model used for the datasets of Catalan andHungarian adds the h(j/i) feature from every cate-gory.
The enhanced model used for Arabic, Basque,English, Greek, Italian and Turkish uses the full setof features.
This tripartite division of models wasmotivated only by time- and resource-constraints.The simplest model is for Chinese and uses only 5features while the enhanced model for Arabic for ex-ample uses a total of 39 features.5 Results and SetupTable 1 summarizes the results of testing the threealgorithms on the ten different languages.The parser was written in C#.
Training andtesting were performed on a MacOSX 10.4.9 with2GHz Intel Core2Duo processor and 1GB mem-ory, and a Dell Dimension with 2.80GHz Pentium4 processor and 1GB memory running Mepis Linux.TiMBL was run in client-server mode with defaultsettings (IB1 learning algorithm, extrapolation fromthe most similar example, i.e.
k = 1, initiatedwith the command ?Timbl -S <portnumber> -fESHU ESDU LSUArabic LA: 53.72 LA: 54.00 LA: 53.86UA: 63.58 UA: 63.76 UA: 63.78Basque LA: 49.52 LA: 50.20 LA: 51.24UA: 56.83 UA: 57.81 UA: 58.53Catalan LA: 69.56 LA: 69.80 LA: 69.42UA: 74.32 UA: 74.46 UA: 74.22Chinese LA: 47.57 LA: 50.61 LA: 49.82UA: 53.46 UA: 56.75 UA: 56.02Czech LA: 44.41 LA: 53.66 LA: 53.47UA: 49.20 UA: 60.01 UA: 59.55English LA: 51.05 LA: 51.35 LA: 52.11UA: 53.41 UA: 53.65 UA: 54.33Greek LA: 54.68 LA: 54.62 LA: 55.02UA: 61.55 UA: 61.45 UA: 61.80Hungarian LA: 44.34 LA: 45.11 LA: 44.57UA: 50.12 UA: 50.78 UA: 50.46Italian LA: 61.60 LA: 60.95 LA: 61.52UA: 67.01 UA: 66.25 UA: 66.39Turkish LA: 55.57 LA: 57.01 LA: 56.59UA: 62.13 UA: 63.77 UA: 63.17Table 1: Test results for the 10 languages.
LA isthe Labelled Attachment Score and UA is the Unla-belled Attachment Score<training file>?).
Additionally, we attempted touse Support Vector Machines (SVM) as an alter-native classifier.
However, due to the long trainingtime, results from using SVM were not included buttraining an SVM classifier for some of the languageshas started.6 DiscussionBefore we attempt a discussion on the results pre-sented in Table 1, we give a short summary of the ba-sic word order typology of these languages accord-ing to (Greenberg, 1963).
Table 2 shows whetherthe languages are SVO (subject-verb-object) or SOV(subject-object-verb), or VSO (verb-subject-object);contain Pr (prepositions) or Po (postpositions); NG(noun precedes genitive) or GN (genitive precedesnoun); AN (adjective precedes noun) or NA (nounprecedes adjective).2Greenberg had give varying for the word-order typology ofEnglish.
However, we trusted our own intuition as well as thehint of one of the reviewers.1146Arabic VSO Pr NG NABasque SOV Po GN NACatalan SVO Pr NG NAChinese SVO Po GN ANCzech SVO Pr NG ANEnglish2 SVO Pr GN ANGreek SVO Pr NG ANHungarian SOV Po GN ANItalian SVO Pr NG NATurkish SOV Po ?
ANTable 2: Basic word order typology of the ten lan-guages following Greenberg?s UniversalsLooking at the data in Table 1, several obser-vations can be made.
One is the different perfor-mance of languages from the same language fam-ily, i.e.
Italian, Greek and Catalan.
However, thehead-first (ESHU) algorithm presented better thanthe dependents-first (ESDU) one in all of these lan-guages.
The SOV languages like Hungarian, Basqueand Turkish had preference for the dependent?s firstalgorithms (ESDU and LSU).
The ESDU algorithmalso fared better with the SVO languages, except forItalian.However, the Greenberg?s basic word order ty-pology cannot shed enough light into the perfor-mance of the three parsing algorithms.
One ques-tion that pops up immediately is whether a differ-ent feature-model using the same parsing algorithmswould achieve similar results.
Can the differentperformance be attributed to the treebank annota-tion?
Would another classifier fare better than theMemory-based one?
These questions remain for fu-ture research though.Finally, for the Basque data we attempted totest the optimized version of the Covington algo-rithm (Nivre, 2007) against the three other ver-sions discussed here.
Additionally, since our fea-ture vectors differed from those described in (Nivre,2007), head-dependent-features vs. j-i-features, wechanged them so that all the four algorithms send asimilar feature vector, j-i-features, to the classifier.The preliminary result was that Nivre?s version wasthe fastest, with fewer calls to the LINK procedureand with the smallest training data-set.
However, allthe four algorithms showed about 20% decrease inLA/UA scores.Our first intuition about the results from the testsdone on all the 10 languages was that the classifi-cation task suffered from a highly skewed class dis-tribution since the training instances that correspondto a dependency relation are largely outnumbered bythe ?NO?
class (Canisius et al, 2006).
The recallwas low and we expected the classifier to be able topredict more of the required links.
However, the re-sults we got from additional optimizations we per-formed on Hungarian, following recommendationfrom the anonymous reviewers, may lead to a differ-ent conclusion.
The chosen grammar model, relyingonly on connecting dynamically built partial depen-dency graphs, is insufficient to take us over a certainthreshold.7 ConclusionIn this paper we showed the performance of threeflavours of Covington?s algorithm for non-projectivedependency parsing on the ten languages providedfor the CoNLL-X Shared Task (Nivre et al, 2007).The experiment showed that given the grammarmodel we have adopted it does matter which versionof the algorithm one uses.
The chosen model,however, showed a poor performance and sufferedfrom two major flaws - the use of only partiallybuilt graphs and the pure incremental processing.It remains to be seen how these parsing algorithmswill perform in a parser, with a much richer featuremodel and whether it is worth using differentflavours when parsing different languages or thedifferences among them are insignificant.AcknowledgementsWe would like to thank the two anonymous re-viewers for their valuable comments.
We aregrateful to Joakim Nivre for discussion on theCovington algorithm, Bertjan Busser for helpwith TiMBL, Antal van den Bosch for help withparamsearch, Matthew Johnson for providing thenecessary functionality to his .NET implementationof SVM and Patrycja Jab?on?ska for discussion onthe Greenberg?s Universals.1147ReferencesA.
Abeille?, editor.
2003.
Treebanks: Building and UsingParsed Corpora.
Kluwer.I.
Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,A.
Diaz de Ilarraza, A. Garmendia, and M. Oronoz.2003.
Construction of a Basque dependency treebank.In Proc.
of the 2nd Workshop on Treebanks and Lin-guistic Theories (TLT), pages 201?204.Ezra Black, Frederick Jelinek, John D. Lafferty, David M.Magerman, Robert L. Mercer, and Salim Roukos.1993.
Towards history-based grammars: Using richermodels for probabilistic parsing.
In Meeting of the As-sociation for Computational Linguistics, pages 31?37.R.
Bod, R. Scha, and K. Sima?an, editors.
2003.
DataOriented Parsing.
CSLI Publications, Stanford Uni-versity, Stanford, CA, USA.A.
Bo?hmova?, J.
Hajic?, E.
Hajic?ova?, and B. Hladka?.
2003.The PDT: a 3-level annotation scenario.
In Abeille?
(Abeille?, 2003), chapter 7, pages 103?127.Sander Canisius, Toine Bogers, Antal van den Bosch,Jeroen Geertzen, and Erik Tjong Kim Sang.
2006.Dependency Parsing by Inference over High-recallDependency Predictions.
In CoNLL-X Shared Task onMultitlingual Dependency Parsing.K.
Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang,and Z. Gao.
2003.
Sinica treebank: Design criteria,representational issues and implementation.
In Abeille?
(Abeille?, 2003), chapter 13, pages 231?248.Michael A. Covington.
2001.
A Fundamental Algo-rithm for Dependency Parsing.
In Proceedings of the39th Annual ACM Southeast Conference, pages 95?102, Athens, Georgia, USA.D.
Csendes, J. Csirik, T. Gyimo?thy, and A. Kocsor.
2005.The Szeged Treebank.
Springer.Walter Daelemans, Jakub Zavrel, Ko van der Sloot, andAntal van den Bosch.
2004.
Timbl: Tilburg memorybased learner, version 5.1, reference guide.
Techni-cal report, ILK Technical Report 04-02, available fromhttp://ilk.uvt.nl/downloads/pub/papers/ilk0402.pdf.Joseph H. Greenberg.
1963.
Universals of Language.London: MIT Press.J.
Hajic?, O.
Smrz?, P. Zema?nek, J.
?Snaidauf, and E. Bes?ka.2004.
Prague Arabic dependency treebank: Develop-ment in data and tools.
In Proc.
of the NEMLAR In-tern.
Conf.
on Arabic Language Resources and Tools,pages 110?117.R.
Johansson and P. Nugues.
2007.
Extendedconstituent-to-dependency conversion for English.
InProc.
of the 16th Nordic Conference on ComputationalLinguistics (NODALIDA).M.
Marcus, B. Santorini, and M. Marcinkiewicz.
1993.Building a large annotated corpus of English: the PennTreebank.
Computational Linguistics, 19(2):313?330.M.
A.
Mart?
?, M.
Taule?, L. Ma`rquez, and M. Bertran.2007.
CESS-ECE: A multilingual and multilevelannotated corpus.
Available for download from:http://www.lsi.upc.edu/?mbertran/cess-ece/.S.
Montemagni, F. Barsotti, M. Battista, N. Calzolari,O.
Corazzari, A. Lenci, A. Zampolli, F. Fanciulli,M.
Massetani, R. Raffaelli, R. Basili, M. T. Pazienza,D.
Saracino, F. Zanzotto, N. Nana, F. Pianesi, andR.
Delmonte.
2003.
Building the Italian Syntactic-Semantic Treebank.
In Abeille?
(Abeille?, 2003), chap-ter 11, pages 189?210.J.
Nivre, J.
Hall, S. Ku?bler, R. McDonald, J. Nils-son, S. Riedel, and D. Yuret.
2007.
The CoNLL2007 shared task on dependency parsing.
In Proc.of the CoNLL 2007 Shared Task.
Joint Conf.
on Em-pirical Methods in Natural Language Processing andComputational Natural Language Learning (EMNLP-CoNLL).Joakim Nivre.
2004.
Incrementality in DeterministicDependency Parsing.
In Incremental Parsing: Bring-ing Engineering and Cognition Together, Workshop atACL-2004, pages 50?57, Barcelona, Spain, July, 25.Joakim Nivre.
2006.
Inductive Dependency Parsing.Springer.Joakim Nivre.
2007.
Incremental Non-Projective Depen-dency Parsing.
In Proceedings of NAACL-HLT 2007,Rochester, NY, USA, April 22?27.K.
Oflazer, B.
Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r.2003.
Building a Turkish treebank.
In Abeille?
(Abeille?, 2003), chapter 15, pages 261?277.P.
Prokopidis, E. Desypri, M. Koutsombogera, H. Papa-georgiou, and S. Piperidis.
2005.
Theoretical andpractical issues in the construction of a Greek depen-dency treebank.
In Proc.
of the 4th Workshop on Tree-banks and Linguistic Theories (TLT), pages 149?160.1148
