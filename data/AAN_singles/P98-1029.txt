Classifier Combination for Improved Lexical DisambiguationEric Brill and Jun WuDepartment of Computer ScienceJohns Hopkins UniversityBaltimore, Md.
21218 USA{bril l , junwu} @cs.jhu.eduAbstractOne of the most exciting recent directions inmachine learning is the discovery that thecombination of multiple classifiers oftenresults in significantly better performancethan what can be achieved with a singleclassifier.
In this paper, we first show thatthe errors made from three different state ofthe art part of speech taggers are stronglycomplementary.
Next, we show how thiscomplementary behavior can be used to ouradvantage.
By using contextual cues toguide tagger combination, we are able toderive a new tagger that achievesperformance significantly greater than anyof the individual taggers.IntroductionPart of speech tagging has been a centralproblem in natural language processing formany years.
Since the advent of manuallytagged corpora such as the Brown Corpus andthe Penn Treebank (Francis(1982),Marcus(1993)), the efficacy of machine learningfor training a tagger has been demonstratedusing a wide array of techniques, including:Markov models, decision trees, connectionistmachines, transformations, nearest-neighboralgorithms, and maximum entropy(Weischedel(1993), Black(1992), Schmid(1994),Brill(1995),Daelemans(1995),Ratnaparkhi(1996)).
All of these methods seem to achieve roughlycomparable accuracy.The fact that most machine-learning-based taggers achieve comparable r sults couldbe attributed to a number of causes.
It ispossible that the 80/20 rule of engineering isapplying: a certain number of tagging instancesare relatively simple to disambiguate and aretherefore being successfully tagged by allapproaches, while another percentage isextremely difficult to disambiguate, requiringdeep linguistic knowledge, thereby causing alltaggers to err.
Another possibility could be thatall of the different machine learning techniquesare essentially doing the same thing.
We knowthat the features used by the different algorithmsare very similar, typically the words and tagswithin a small window from the word beingtagged.
Therefore it could be possible that theyall end up learning the same information, just indifferent forms.In the field of machine learning, therehave been many recent results demonstrating theefficacy of combining classifiersJ In this paperwe explore whether classifier combination canresult in an overall improvement in lexicaldisambiguation accuracy.1 Different Tagging AlgorithmsThe experiments described in this paper arebased on four popular tagging algorithms, all ofwhich have readily available implementations.These taggers are described below.1.1 Unigram TaggingThis is by far the simplest of tagging algorithms.Every word is simply assigned its most likelypart of speech, regardless of the context in whichit appears.
Surprisingly, this simple taggingmethod achieves fairly high accuracy.Accuracies of 90-94% are typical.
In theunigram tagger used in our experiments, forwords that do not appear in the lexicon we use aI See Dietterich(1997) for a good summary of thesetechniques.191collection of simple manually-derived heuristicsto guess the proper tag for the word.1.2 N-Gram TaggingN-gram part of speech taggers (Bahl(1976),Church(1992), Weischedel(1993)) are perhapsthe most widely used of tagging algorithms.The basic model is that given a word sequenceW, we try to find the tag sequence T thatmaximizes P(TIW).
This can be done using theViterbi algorithm to find the T that maximizes:P(T)*P(WIT).
In our experiments, we use astandard trigram tagger using deletedinterpolation (Jelinek (1980)) and used suffixinformation for handling unseen words (as wasdone in Weischedel (1993)).1.3 T rans format ion -Based  TaggingIn transformation-based tagging (Brill (1995)),every word is first assigned an initial tag, Thistag is the most likely tag for a word if the wordis known and is guessed based upon propertiesof the word if the word is not known.
Then asequence of rules are applied that change thetags of words based upon the contexts theyappear in.
These rules are applieddeterministically, in the order they appear in thelist.
As a simple example, if race appears in thecorpus most frequently as a noun, it will initiallybe mistagged as a noun in the sentence :We can race all day long.The rule Change a tag from NOUN toVERB if the previous tag is a MODAL would beapplied to the sentence, resulting in the correcttagging.
The environments used for changing atag are the words and tags within a window ofthree words.
For our experiments, we used apublicly available implementation oftransformation-based tagging, 2 retrained on ourtraining set.maximally agnostic with respect to allparameters for which no data exists.
It is a niceframework for combining multiple constraints.Whereas the transformation-based taggerenforces multiple constraints by having multiplerules fire, the maximum-entropy tagger can haveall of these constraints play a role at setting theprobability estimates for the model's parameters.In Ratnaparkhi (1996), a maximum entropytagger is presented.
The tagger uses essentiallythe same parameters as the transformation-basedtagger, but employs them in a different model.For our experiments, we used a publiclyavailable implementation of maximum-entropytagging) retrained on our training set.2 Tagger  Complementar i tyAll experiments presented in this paper were runon the Penn Treebank Wall Street Joumal corpus(Marcus (1993)).
The corpus was divided intoapproximately 80% training and 20% testing,giving us approximately 1.1 million words oftraining data and 265,000 words of test data.The test set was not used in any way in training,so the test set does contain unknown words.In Figure 1 we show the relativeaccuracies of the four taggers.
In parentheseswe include tagger accuracy when onlyambiguous and unknown words are considered?Tagger Accuracy (%) Num ErrorsUnigram 93.26 (87.9) 17856Trigram 96.36 (93.8) 9628Transform.
96.61 (94.3) 8980Max.
Ent.
96.83 (94.7) 8400Figure 1: Relative Tagger AccuraciesNext, we examine just how different heerrors of the taggers are.
We define thecomplementary rate of taggers A and B as :1.4 Maximum-Entropy TaggingThe maximum-entropy framework is aprobabilistic framework where a model is foundthat is consistent with the observed ata and is2 http://www.cs.jhu.edu/-brill3 http://www.cis.upenn.edu/-adwait4 It is typical in tagging papers to give results inambiguity resolution over all words, including wordsthat are unambiguous.
Correctly tagging words thatonly can have one label contributes to the accuracy.We see in Figure 1 that when accuracy is measuredon truly ambiguous words, the numbers are lower.
Inthis paper we stick to the convention of giving resultsfor all words, including unambiguous ones.192# of common errors Comp(A, B) = (I .
.
.
.
.
.  )
* I00# of  errors in A onlyIn other words, Comp(A,B) measures thepercentage of time when tagger A is wrong thattagger B is correct.
In Figure 2 we show thecomplementary rates between the differenttaggers.
For instance, when the maximumentropy tagger is wrong, the transformation-based tagger is right 37.7% of the time, andwhen the transformation-based tagger is wrong,the maximum entropy tagger is right 41.7% ofthe time.Unigram Trigram Transf.
MaxEntUnigram 0 32.1 20.0 34.9Trigram 63.4 0 34.6 33.5Transf.
59.7 39.0 0 37.7MaxEnt 69.4 42.0 41.7 0Figure 2: Comp(A,B).
Row = A, Column = BThe complementary ates are quite high,which is encouraging, since this sets the upperbound on how well we can do in combining thedifferent classifiers.
If all taggers made thesame errors, or if the errors that lower-accuracytaggers made were merely a superset of higher-accuracy tagger errors, then combination wouldbe futile.In addition, a tagger is much more likelyto have misclassified the tag for a word ininstances where there is disagreement with atleast one of the other classifiers than in the casewhere all classifiers agree.
In Figure 3 we see,for instance that while the overall error rate forthe Maximum Entropy tagger is 3.17%, in caseswhere there is disagreement between the fourtaggers the Maximum Entropy tagger error ratejumps to 27.1%.
And discarding the unigramtagger, which is significantly less accurate thanthe others, when there is disagreement betweenthe Maximum Entropy, Transformation-basedand Trigram taggers, the Maximum Entropytagger error rate jumps up to 43.7%.
Thesecases account for 58% of the total errors theMaximum Entropy tagger makes (4833/8400).Next, we check whether taggercomplementarity is additive.
In Figure 4, thefirst row shows the additive error rate an oraclecould achieve on the test set if the oracle couldpick between the different outputs of the taggers.For example, when the oracle can examine theoutput of the Maximum Entropy,Transformation-Based and Trigram taggers, itcould achieve an error rate of 1.62%.
Thesecond row shows the additive error ratereduction the oracle could achieve.
If the oracleis allowed to choose between all four taggers, a55.5% error rate reduction is obtained over theMaximum Entropy tagger error rate.
If theunigram output is discarded, the oracleimprovement drops down to 48.8% overMaximum Entropy tagger error rate.Overall ErrorRateError Rate WhenDisagreementError Rate WhenDisagreement(excludingunigram)Max.Ent Trans- Tri- Uni-form gram gram3.17% 3.39 3.64 6.74!
(8400) (8980) (9628) (17856):27.1 29.9 33.1 73.4(5535) (6115) (6763) (14991)43.7 49.0 54.9(4833) (5413) (6061)Figure 3: DisagreementIndication of ErrorIs A StrongMaxEnt +Transf.% of time all 3.17 1.98are wrong% Oracle 37.7Improvement+Tri- +Uni-gram gram1.62 1.4148.8 55.5Figure 4 : Complementarity Is Additive.From these results, we can conclude that there isat least hope that improvments can be gained bycombining the output of different aggers.
Wecan also conclude that the improvements weexpect are somewhat additive, meaning the moretaggers we combine, the better esults we shouldexpect.3 Tagger CombinationThe fact that the errors the taggers make arestrongly complementary is very encouraging.
Ifall taggers made the exact same errors, therewould obviously be no chance of improvingaccuracy through classifier combination.However, note that the high complementary atebetween tagger errors in itself does notnecessarily imply that there is anything to begained by classifier combination.We ran experiments to determinewhether the outputs of the different taggers193could be effectively combined.
We firstexplored combination via simple majority-winsvoting.
Next, we attempted to automaticallyacquire contextual cues that learned both whichtagger to believe in which contexts and whattags are indicated by different patterns of taggeroutputs.
Both the word environments and thetagger outputs for the word being tagged and itsneighbors are used as cues for predicting theproper tag.3.1 Simple Vot ingThe simplest combination scheme is to have theclassifiers vote.
The part of speech thatappeared as the choice of the largest number ofclassifiers is picked as the answer, with somemethod being specified for breaking ties.
Wetried simple voting, using the MaximumEntropy, Transformation-Based and Trigramtaggers.
In case of ties (all taggers disagree), theMaximum Entropy tagger output is chosen,since this tagger had the highest overall accuracy(this was determined by using a subset of thetraining set, not by using the test set).
Theresults are shown in Figure 5.
Simple votinggives a net reduction in error of 6.9% over thebest of the three taggers.
This difference issignificant at a >99% confidence l vel.Tagger Error Rate Num ErrorsMax Ent 3.2% 8400Simple Voting 3.0% 7823Figure 5 Results of Simple Voting3.2 Contextual  CuesNext, we try to exploit the idiosyncracies of thedifferent taggers.
Although the MaximumEntropy, Transformation-based and Trigramtaggers use essentially the same types ofcontextual information for disambignation, thisinformation is exploited ifferently in each case.Our hope is that there is some regularity to thesedifferences, which would then allow us to learnwhat conditions uggest that we should trust onetagger output over another.We used a version of example-basedlearning to determine whether these taggerdifferences could be exploited.
5 To determine5 Example-based learning has also been appliedsuccesfully in building a single part of speech taggerthe tag of a word, we use the previous word,current word, next word, and the output of eachtagger for the previous, current and next word.See Figure 6.wora.
wo a, Word).,Unigram_Tagj.
I Unigram_Tagj Unigram Tagj.,Tdgram Tagj.
t Tdgram_Tagj Tdgram_Tagj.
tTransform_Tagj.~ Transform_Tagj Transform_Tag~MaxEnt_Tagj.i MaxEnt Tagj MaxEnt_Tagj?,Figure 6 Features Used To Determine TheProper Tag for Word j.For each such context in the training set,we store the probabilities of what correct tagsappeared in that context.
When the tagdistribution for a context has low entropy, it is avery good predictor of the correct ag when theidentical environment occurs in unseen data.The problem is that these environments are veryspecific, and will have low overall recall in anovel corpus.
To account for this, we must backoff to more general contexts when we encounteran environment in the test set that did not occurin the training set.
This is done by specifying anorder in which fields should be ignored until amatch is found.
The back-off ordering is learnedautomatically.We ran two variants of this experiment.In the first case, given an instance in the test set,we find the most specific matching example inthe training set, using the prespecified back-offordering, and see what the most probable tagwas in the training set for that environment.This is then chosen as the tag for the word.
Notethat this method is capable of learning to assigna tag that none of the taggers assigned.
Forinstance, it could be the case that when theUnigram tagger thinks the tag should be X, andthe Trigram and Maximum Entropy taggersthink it should be Y, then the true tag is mostfrequently Z.In the second experiment, we usecontexts to specify which tagger to trust, ratherthan which tag to output.
Again the mostspecific context is found, but here we checkwhich tagger has the highest probability of beingcorrect in this particular context.
For instance,we may learn that the Trigram tagger is mostaccurate at tagging the word up or that theUnigram tagger does best at tagging the word(Daelemans(1996)).194race when the word that follows is and.
Theresults are given in Figure 7.
We see that whilesimple voting achieves an error reduction of6.9%, using contexts to choose a tag gives anerror reduction of 9.8% and using contexts tochoose a tagger gives an error reduction of10.4%.Tagger Error Rate Num ErrorsMax Ent 3.2% 8400Simple Voting 3.0% 7823Context: Pick Tag 2.9% 7580Context: Pick Tagger 2.8% 7529Figure 7 Error  Rate Reduction For DifferentTagger Combination MethodsConclusionIn this paper, we showed that the errordistributions for three popular state of the artpart of speech taggers are highlycomplementary.
Next, we describedexperiments that demonstrated that we canexploit this complementarity o build a taggerthat attains significantly higher accuracy thanany of the individual taggers.In the future, we plan to expand ourrepertoire of base taggers, to determine whetherperformance continues to improve as we addadditional systems.
We also plan to exploredifferent methods for combining classifieroutputs.
We suspect hat the features we havechosen to use for combination are not theoptimal set of features.
We need to carefullystudy the different algorithms to find possiblecues that can indicate where a particular taggerperforms well.
We hope that by following thesegeneral directions, we can further exploitdifferences in classifiers to improve accuracy inlexical disambiguation.ReferencesBlack E., Jelinek F., Lafferty J, Mercer R. andRoukos S. (1992).
Decision Tree Models Appliedto the Labeling of Text with Parts-of-Speech.
DarpaWorkshop on Speech and Natural Language,Harriman, N.Y.Brill, E. (1995).
Transformation-Based Error-DrivenLearning and Natural Language Processing: ACase Study in Part of Speech Tagging.Computational Linguistics.Daelemans W. (1996).
MBT: A Memory-Based Partof Speech Tagger-Generator.
Proceedings of theWorkshop on Very Large Corpora, CopenhagenDietterich T. (1997).
Machine-Learning Research:Four Current Directions.
AI Magazine.
Winter1997, pp97-136.Francis W. and Kucera H. (1982) Frequency analysisof English usage: Lexicon and grammar.Houghton Mifflin.Jelinek F and Mercer R (1980).
InterpolatedEstimation of Markov Source Parameters fromSparse Data.
In Pattern Recognition i  Practice, E.Gelsema nd L. Kanal, Eds.
Amsterdam: North-Holland.Marcus M., Santorini B. and Marcinkiewicz M.(1993) Building a large annotated corpus ofEnglish: the Penn Treebank.
ComputationalLinguistics.Ratnaparkhi A.
(1996).
A Maximum Entropy Part-of-Speech Tagger.
Proceedings of the First EmpiricalMethods in Natural Language ProcessingConference.
Philadelphia, Pa.Schmid H. (1994).
Part of Speech Tagging WithNeural Networks.
Proceedings of COLING,Yokohama, Japan.Weischedel R., Meteer M., Schwartz R., Ramshaw L.and Palmueci, J.
(1993).
Coping with ambiguityand unknown words through probabilistic models.Computational Linguistics.195
