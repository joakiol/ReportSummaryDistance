Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 307?311, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguisticsunimelb: Topic Modelling-based Word Sense InductionJey Han Lau, Paul Cook and Timothy BaldwinDepartment of Computing and Information SystemsThe University of Melbournejhlau@csse.unimelb.edu.au, paulcook@unimelb.edu.au,tb@ldwin.netAbstractThis paper describes our system for sharedtask 13 ?Word Sense Induction for Graded andNon-Graded Senses?
of SemEval-2013.
Thetask is on word sense induction (WSI), andbuilds on earlier SemEval WSI tasks in ex-ploring the possibility of multiple senses be-ing compatible to varying degrees with a sin-gle contextual instance: participants are askedto grade senses rather than selecting a sin-gle sense like most word sense disambigua-tion (WSD) settings.
The evaluation measuresare designed to assess how well a system per-ceives the different senses in a contextual in-stance.
We adopt a previously-proposed WSImethodology for the task, which is based on aHierarchical Dirichlet Process (HDP), a non-parametric topic model.
Our system requiresno parameter tuning, uses the English ukWaCas an external resource, and achieves encour-aging results over the shared task.1 IntroductionIn our previous work (Lau et al 2012) we devel-oped a word-sense induction (WSI) system based ontopic modelling, specifically a Hierarchical Dirich-let Process (Teh et al 2006).
In evaluations overthe SemEval-2007 and SemEval-2010 WSI tasks weachieved performance on par with the current state-of-the art.
The SemEval-2007 and SemEval-2010WSI tasks assumed that each usage of a word hasa single gold-standard sense.
In this paper we applythis WSI method ?off-the-shelf?, with no adaptation,to the novel SemEval-2013 task of ?Word Sense In-duction for Graded and Non-Graded Senses?.
Giventhat the topic model allocates a multinomial distri-bution over topics to each word usage (?document?,in topic modelling terms), the SemEval-2013 WSItask is an ideal means for evaluating this aspect ofthe topic model.2 System DescriptionOur system is based on the WSI methodology pro-posed by Lau et al(2012), and also applied toSemEval-2013 Task 11 on WSI for web snippetclustering (Lau et al to appear).
The core machin-ery of our system is driven by a Latent Dirichlet Al-location (LDA) topic model (Blei et al 2003).
InLDA, the model learns latent topics for a collectionof documents, and associates these latent topics withevery document in the collection.
A topic is repre-sented by a multinomial distribution of words, andthe association of topics with documents is repre-sented by a multinomial distribution of topics, a dis-tribution for each document.
The generative processof LDA for drawing word w in document d is as fol-lows:1. draw latent topic z from document d;2. draw word w from the chosen latent topic z.The probability of selecting word w given a doc-ument d is thus given by:P (w|d) =T?z=1P (w|t = z)P (t = z|d).where t is the topic variable, and T is the number oftopics.307The number of topics, T , is a parameter in LDA.We relax this assumption by extending the modelto be non-parametric, using a Hierarchical DirichletProcess (HDP: (Teh et al 2006)).
HDP learns thenumber of topics based on data, with the concentra-tion parameters ?
and ?0 controlling the variabilityof topics in the documents (for details of HDP pleaserefer to the original paper).To apply HDP to WSI, the latent topics are in-terpreted as the word senses, and the documents areusages that contain the target word of interest.
Thatis, given a target word (e.g.
paper), a ?document?in our application is a sentence context surround-ing the target word.
In addition to the bag of wordssurrounding the target word, we also include posi-tional context word information, which was used inour earlier work (Lau et al 2012).
That is, we in-troduce an additional word feature for each of thethree words to the left and right of the target word.An example of the topic model features is given inTable 1.2.1 Background Corpus and PreprocessingThe test dataset provides us with contextual in-stances for each target word, and these instancesconstitute the documents for the topic model.
Thetext of the test data is tokenised and lemmatised us-ing OpenNLP and Morpha (Minnen et al 2001).Note, however, that there are only 100 instancesfor most target words in the test dataset, and as suchthe dataset may be too small for the topic modelto induce meaningful senses.
To this end, we turnto the English ukWaC ?
a web corpus of approxi-mately 1.9 billion tokens ?
to expand the data, byextracting context sentences that contain the targetword.
Each extracted usage is a three-sentence con-text containing the target word: the original sentencethat contains the actual usage and its preceding andsucceeding sentences.
The extraction of usages fromthe ukWaC significantly increases the amount of in-formation for the topic model to learn the senses forthe target words from.
However, HDP is compu-tationally intensive, so we limit the number of ex-tracted usages from the ukWaC using two samplingapproaches:UNIMELB (5P) Take a 5% random sample of us-ages;UNIMELB (50K) Limit the maximum number ofrandomly-sampled usages to 50,000 instances.The usages from the ukWaC are tokenised andlemmatised using TreeTagger (Schmid, 1994), asprovided by the corpus.To summarise, for each target word we applythe HDP model to the combined collection of thetest instances (provided by the shared task) andthe extracted usages from the English ukWaC (not-ing that each instance/usage corresponds to a topicmodel ?document?).
The topic model learns thesenses/topics for all documents in the collection, butwe only use the sense/topic distribution for the testinstances as they are the ones evaluated in the sharedtask.3 Experiments and ResultsFollowing Lau et al(2012), we use the default pa-rameters (?
= 0.1 and ?0 = 1.0) for HDP.1 For eachtarget word, we apply HDP to induce the senses, anda distribution of senses is produced for each ?docu-ment?
in the model.
To grade the senses for the in-stances in the test dataset, we apply the sense proba-bilities learnt by the topic model as the sense weightswithout any modification.To illustrate the senses induced by our model, wepresent the top-10 words of the induced senses forthe verb strike in Table 2.
Although 13 senses intotal are induced and some of them do not seem verycoherent, only the first 8 senses ?
the more coherentones ?
are observed (i.e., have non-zero probabilityfor any usage) in the test dataset.Two forms of evaluation are used in the task:WSD evaluation and clustering comparison.
ForWSD evaluation, three measures are used: (1)Jaccard Index (JI), which measures the degree ofoverlap between the induced senses and the goldsenses; (2) positionally-weighted Kendall?s tau (KT:(Kumar and Vassilvitskii, 2010)), which measuresthe correlation between the ranking of the inducedsenses and that of the gold senses; and (3) nor-malised discounted cumulative gain (NDCG), which1These settings were considered ?vague?
priors in Teh etal.
(2006).
They were tested in Lau et al(2012) and themodel was shown to be robust under different parameter set-tings.
As such we decided to keep the settings.
The imple-mentation of our WSI system can be accessed via GitHub:https://github.com/jhlau/hdp-wsi.308Target word dogsContext sentence Most breeds of dogs are at most a few hundred years oldBag-of-word features most, breeds, of, are, at, most, a, few, hundred, years, oldPositional word features most #-3, breeds #-2, of #-1, are #1, at #2, most #3Table 1: An example of the topic model features.Sense Num Top-10 Terms1 strike @card@ worker union war iraq week pay government action2 strike hand god head n?t look face fall leave blow3 strike @card@ balance court company case need balance #1 order claim4 strike ball @card@ minute game goal play player shot half5 strike @card@ people fire disaster area road car ship lightning6 @card@ strike new news post deal april home business week7 strike n?t people thing think way life book find new8 @card@ strike coin die john church police age house william9 div ukl syn color hunter text-decoration australian verb condom font-size10 invent rocamadour cost mp3 terminal total wav honor omen node11 training run rush kata performance marathon exercise technique workout interval12 wrong qha september/2000 sayd ?
hawksmoor thyna pan salt common13 zidane offering stone blow zidane #-1 type type #2 zidane #1 blow #3 materialsTable 2: The top-10 terms for each of the senses induced for the verb strike by the HDP model.measures the correlation between the weights ofthe induced senses and that of the gold senses.For clustering comparison, fuzzy normalised mu-tual information (FNMI) and fuzzy b-cubed (FBC)are used.
Note that the WSD systems participat-ing in this shared task are not evaluated with clus-tering comparison metrics, as they do not inducesenses/clusters in the same manner as WSI systems.WSI systems produce senses that are different tothe gold standard sense inventory (WordNet 3.1),and the induced senses are mapped to the gold stan-dard senses using the 80/20 validation setting.
De-tails of this mapping procedure are described in Jur-gens (2012).Results for all test instances are presented in Ta-ble 3.
Note that many baselines are used, only someof which we present in this paper, namely: (1) RAN-DOM ?
label instances with one of three random in-duced senses; (2) SEMCOR MFS ?
label instanceswith the most frequently occurring sense in Semcor;(3) TEST MFS ?
label instances with the most fre-quently occurring sense in the test dataset.
To bench-mark our method, we present one or two of the bestsystems from each team.Looking at Table 3, our system performs encour-agingly well.
Although not the best system, weachieve results close to the best system for each eval-uation measure.Most of the instances in the data were annotatedwith only one sense; only 11% were annotated withtwo senses, and 0.5% with three.
As a result, thetask organisers categorised the instances into single-sense instances and multi-sense instances to bet-ter analyse the performance of participating sys-tems.
Results for single-sense and multi-sense in-stances are presented in Table 4 and Table 5, re-spectively.
Note that for single-sense instances, onlyprecision is used for WSD evaluation as the JaccardIndex, positionally-weighted Kendall?s tau and nor-malised discounted cumulative gain are not applica-ble.
Our system performs relatively well, and trailsmarginally behind the best system in most cases.4 ConclusionWe adopt a WSI methodology from Lau et al(2012)for the task of grading senses in a WSD setting.309System JI KT NDCG FNMI FBCRANDOM 0.244 0.633 0.287 0.018 0.382SEMCOR MFS 0.455 0.465 0.339 ?
?TEST MFS 0.552 0.560 0.412 ?
?AI-KU 0.197 0.620 0.387 0.065 0.390AI-KU (REMOVE5-AD1000) 0.244 0.642 0.332 0.039 0.451LA SAPIENZA (2) 0.149 0.510 0.383 ?
?UOS (TOP-3) 0.232 0.625 0.374 0.045 0.448UNIMELB (5P) 0.218 0.614 0.365 0.056 0.459UNIMELB (50K) 0.213 0.620 0.371 0.060 0.483Table 3: Results for all instances.
The best-performing system is indicated in boldface.System Precision FNMI FBCRANDOM 0.555 0.010 0.359SEMCOR MFS 0.477 ?
?TEST MFS 0.578 ?
?AI-KU 0.641 0.045 0.351AI-KU (REMOVE5-AD1000) 0.628 0.026 0.421UOS (TOP-3) 0.600 0.028 0.414UNIMELB (5P) 0.596 0.035 0.421UNIMELB (50K) 0.605 0.039 0.441Table 4: Results for single-sense instances.
The best-performing system is indicated in boldface.System JI KT NDCG FNMI FBCRANDOM 0.429 0.548 0.236 0.006 0.113SEMCOR MFS 0.283 0.373 0.197 ?
?TEST MFS 0.354 0.426 0.248 ?
?AI-KU 0.394 0.617 0.317 0.029 0.078AI-KU (REMOVE5-AD1000) 0.434 0.586 0.291 0.004 0.116LA SAPIENZA (2) 0.263 0.531 0.365 ?
?UOS (#WN SENSES) 0.387 0.628 0.314 0.036 0.037UNIMELB (5P) 0.426 0.586 0.287 0.019 0.130UNIMELB (50K) 0.414 0.602 0.299 0.021 0.134Table 5: Results for multi-sense instances.
The best-performing system is indicated in boldface.310With no parameter tuning and using only the EnglishukWaC as an external resource, our system performsrelatively well at the task.ReferencesDavid M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet alcation.
Journal of MachineLearning Research, 3:993?1022.David Jurgens.
2012.
An evaluation of graded sensedisambiguation using word sense induction.
In Proc.of the First Joint Conference on Lexical and Com-putational Semantics (*SEM 2012), pages 189?198,Montre?al, Canada.Ravi Kumar and Sergei Vassilvitskii.
2010.
Generalizeddistances between rankings.
In Proc.
of the 19th Inter-national Conference on the World Wide Web (WWW2010), pages 571?580, Raleigh, USA.Jey Han Lau, Paul Cook, Diana McCarthy, David New-man, and Timothy Baldwin.
2012.
Word sense induc-tion for novel sense detection.
In Proc.
of the 13thConference of the EACL (EACL 2012), pages 591?601, Avignon, France.Jey Han Lau, Paul Cook, and Timothy Baldwin.
to ap-pear.
unimelb: Topic modelling-based word sense in-duction for web snippet clustering.
In Proc.
of the 7thInternational Workshop on Semantic Evaluation (Se-mEval 2013).Guido Minnen, John Carroll, and Darren Pearce.
2001.Applied morphological processing of English.
Natu-ral Language Engineering, 7(3):207?223.Helmut Schmid.
1994.
Probabilistic part-of-speech tag-ging using decision trees.
In Proc.
of the Confer-ence on New Methods in Natural Language Process-ing, Manchester, 1994.Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, andDavid M. Blei.
2006.
Hierarchical Dirichlet pro-cesses.
Journal of the American Statistical Associa-tion, 101:1566?1581.311
