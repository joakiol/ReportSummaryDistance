NAACL-HLT 2012 Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data, pages 41?44,Montre?al, Canada, June 7, 2012. c?2012 Association for Computational LinguisticsHRItk: The Human-Robot Interaction ToolKitRapid Development of Speech-Centric Interactive Systems in ROSAbstractDeveloping interactive robots is an extremelychallenging task which requires a broad rangeof expertise across diverse disciplines, includ-ing, robotic planning, spoken language under-standing, belief tracking and actionmanagement.
While there has been a boom inrecent years in the development of reusablecomponents for robotic systems within com-mon architectures, such as the Robot Operat-ing System (ROS), little emphasis has beenplaced on developing components for Human-Robot-Interaction.
In this paper we introduceHRItk (the Human-Robot-Interaction toolkit),a framework, consisting of messaging proto-cols, core-components, and development toolsfor rapidly building speech-centric interactivesystems within the ROS environment.
Theproposed toolkit was specifically designed forextensibility, ease of use, and rapid develop-ment, allowing developers to quickly incorpo-rate speech interaction into existing projects.1 IntroductionRobots that operate along and with humans in settingssuch as a home or office are on the verge of becoming anatural part of our daily environment (Bohren et al,2011, Rosenthal and Veloso 2010, Kanda et al, 2009,Srinivasa et al, 2009).
To work cooperatively in theseenvironments, however, they need the ability to interactwith people, both known and unknown to them.
Naturalinteraction through speech and gestures is a prime can-didate for such interaction, however, the combination ofcommunicative and physical actions, as well as the un-certainty inherent in audio and visual sensing make suchsystems extremely challenging to create.Developing speech and gesture-based interactiverobots requires a broad range of expertise, including,robotic planning, computer vision, acoustic processing,speech recognition, natural language understanding,belief tracking, as well as dialog management and ac-tion selection, among others.
This complexity makes itdifficult for all but very large research groups to devel-op complete systems.
While there has been a boom inrecent years in the development and sharing of reusablecomponents, such as path planning, SLAM and objectrecognition, within common architectures, such as theRobot Operating System (ROS) (Quigley, 2009), littleemphasis has been placed on the development of com-ponents for Human-Robot Interaction although despitethe growing need for research in this area.Prior work in Human-Robot Interaction has gener-ally resulted in solutions for specific robotic platforms(Clodic et al, 2008) or standalone frameworks (Fong etal., 2006) that cannot be easily combined with standardarchitectures used by robotics researchers.
Earlier work(Kanda et al, 2009, Fong et al, 2006) has demonstratedthe possibilities of multimodal and multiparty interac-tion on robotic platforms, however, the tasks and inte-ractions explored until now have been extremelylimited, due to the complexity of infrastructure requiredto support such interactions and the expertise required toeffectively implement and optimize individual compo-nents.
To make significant progress, we believe that acommon, easy to use, and easily extensible infrastruc-ture, similar to that supported by ROS, is required formulti-modal human-robot interaction.
Such a frame-work will allow researchers to rapidly develop initialspeech and gesture-based interactive systems, enablingthem to rapidly deploy systems, observe and collectinteractions in the field and iteratively improve systemcomponents based on observed deficiencies.
By using acommon architecture and messaging framework, com-ponents and component models can easily be upgradedand extended by a community of researchers, while notaffecting other components.Towards this goal we have developed HRItk1(Human-Robot-Interaction toolkit), an infrastructureand set of components for developing speech-centricinteractive systems within the ROS environment.
Theproposed toolkit provides the core components requiredfor speech interaction, including, speech recognition,natural language understanding and belief tracking.
Ad-ditionally it provides basic components for gesture rec-ognition and gaze tracking.1 HRItk is available for download at:http://speech.sv.cmu.edu/HRItkIan Lane1, Vinay Prasad1, Gaurav Sinha1, Arlette Umuhoza1,Shangyu Luo1, Akshay Chandrashekaran1 and Antoine Raux21 Carnegie Mellon University, NASA Ames Research Park, Moffett Field, California, USA2 Honda Research Institute, Mountain View, California, USAlane@cs.cmu.edu, ?araux@honda-?ri.com ?41Figure 1: Overview of core understanding and tracking components within HRItk2 Framework OverviewAn overview of the core components in the toolkit arehighlighted in Figure 1.
We introduce two classes ofcomponents required for speech and multimodal interac-tion into the ROS framework, understanding nodes andtracking services.
Understanding nodes are perceptualcomponents that recognize and understand interactionevents.
Using input from sensors, intermediateprocessing nodes or other understanding components,these nodes generate hypotheses about current user in-put.
Tracking services monitor the long term and conti-nuous aspects of interaction, including user dialog goalsDQG WKH XVHU?V IRFXV RI DWWHQWLRQ.
These services areleveraged by components including Dialog Manage-ment and Action Selection to perform interaction.
Addi-tionally, these services provide context to understandingnodes enabling them to apply context-specificprocessing during the understanding phase.2.1 Data Processing NodesThe understanding components implemented in thiswork heavily leverage existing components developedin ROS (Quigley et al, 2009).
TKHVHLQFOXGHWKH?open-ni_kinect?
 QRGH ZKLFK processes depth-images fromthe Microsoft Kinect sensor, the ?openni_tracker?which performs skeletal tracking, and ?uvccam? QRGHwhich processes color images from external USB cam-eras.
In the near future we also plan to support far-fieldspeech recognition using the HARK_ROS toolkit (Na-kadai et al, 2010).2.2 Understanding NodesUnderstanding nodes recognize and understand eventsobserved during interaction.
As input they use eitherdata obtained directly from sensors, preprocessed datafrom intermediate processing nodes or output from oth-er understanding components.
They either performprocessing on explicit interaction events, such as speechor gesture input, or process continuous input such asjoint position or gaze direction.
The current understand-ing nodes implemented within HRItk are listed in Table1 along with the ROS topics on which they publish.Understanding nodes publish two forms of messag-HV ?state?
PHVVDJHV^READY, START and STOP}, in-dicating the state of the node and whether an interactionevent has been detected, DQG ?hypothesis?
 PHVVDgeswhich enumerate the most likely observed events alongwith a likelihood measure for each.
The specific struc-WXUH RI WKH ?hypothesis?
 PHVVDJH is dependent on theevent being observed.2.3 State Tracking ServicesIn addition to understanding specific events such asutterances or gestures, an interactive system needs totrack longer term and/or continuous aspects of interac-tion.
Such aspects include user goals, which can spanVHYHUDO XWWHUDQFHV LQ D GLDORJ DQG WKH XVHU?V IRFXV RIattention (using, e.g., gaze and posture information).These can be defined as characterizing the state of theworld (i.e.
the user, the interaction, or the environment)at a given time, with possible reference to history.42Table 1: ROS nodes, Topics, Services and Messages implemented within HRItkROS Node Topic / Service (* ) Description of MessagesSpeech Detectionand Recognitionspeech/statespeech/hypothesisspeech/hypothesis/bestspeech/hypothesis/finalspeech/contextState identifying interaction event, each with a unique eventIDPartial and final hypotheses generated during speech recognition.Outputs include 1-best, N-best hypotheses and confusion net-works.
All output contains confidence or component model scoresContext indicating dialog-state, domain, task of current interactionNatural LanguageUnderstandingdialogact/hypothesisdialogact/contextHypotheses of Concept/Value-pairs generated during NLUContext indicating dialog-state, domain, task of current interactionGesture Recognitionhand/hypothesishand/contextHypothesis set of Gesture-Actions with confidence measureContext indicating domain or task of current interactionGaze Trackinggaze/hypothesishand/contextEstimate of gaze directionContext listing visually salient objects within users field of viewDialog StateTrackingdialogstate/statebelief *dialogstate/contextReceives an UPDATED message when the belief changesBelief over the concept set specified in the service requestContext indicating system actions potentially affecting beliefIn addition, states can be significantly larger objectsthan individual event understanding results, which couldunnecessarily consume significant bandwidth if con-stantly broadcast.
Therefore, state tracking modules useROS services rather than topics to communicate theiroutput to other modules.
Any module can send a mes-sage to the tracking service containing a specific queryand will receive in response the matching state or beliefover states.In order to allow components to react to changes inthe state, each state-tracking module publishes anUPDATED message to its state topic whenever a newstate is computed.2.4 Component ImplementationsSpeech Detection and Recognition is performed usinga ROS node developed around the Julius Speech Rec-ognition Engine (Lee and Kawahara, 2009).
We se-lected this engine for its compatibility with HARK(Nakadai et al 2010), and its support of common modelformats.
A wrapper for Julius was implemented in C++to support the ROS messaging architecture listed in Ta-ble 1.
Partial hypotheses are output during decoding,and final hypotheses are provided in 1-best, N-best andConfusion Network formats.
Context is supported vialanguage model switching.In order to develop a Speech Recognition compo-nent for a new task at minimum two component modelsare required, a pronunciation dictionary, and a languagemodel (or recognition grammar).
Within HRItk we pro-vide the tools required to generate these models from aset of labeled example utterances.
We describe the rapidmodel building procedure in Section 4.Natural Language Understanding is implementedusing Conditional Random Fields (Lafferty et al 2001)similar to the approach described in (Cohn, 2007).
Forexample, given WKH LQSXW XWWHUDQFH ?Take this tray tothe kitchen? listed in Table 3, three concept/value pairsare extracted: Action{Carry}, ?Object{tray}, ?Room{kitchen}.
?Similar to the speech recognitioncomponent, the NLU component can be rapidly re-trained using a set of tagged example sentences.Gesture Recognition of simple hand positions is im-plemented using a Kinect depth sensor and previouswork by Fujimura and Xu (2007) for palm/finger seg-mentation.
Currently, the module publishes a hypothesisfor the number of fingers raised by the user, thoughmore complex gestures can be implemented based onthis model.Gaze Tracking is implemented using ASEF filters(Bolme et al, 2009) and geometric projection.
SeparateASEF filters were training to locate the pupils of the leftand right eye as well as their inner and outer corners.Filters were trained on hand-labeled images we col-lected in-house.Dialog State Tracking is in charge of monitoring as-pects of dialog that span multiple turns such as usergoal.
Our implementation is based on the Hound dialogbelief tracking library developed at Honda ResearchInstitute USA.
Currently, our belief tracking model isDynamic Probabilistic Ontology Trees (Raux and Ma2011), which capture the hidden user goal in the form ofa tree-shaped Bayesian Network.
Each node in the GoalNetwork represents a concept that can appear in lan-guage and gesture understanding results.
The structureof the network indicates (assumed) conditional indepen-dence between concepts.
With each new input, the net-work is extended with evidence nodes according to thefinal understanding hypotheses and the system belief isestimated as the posterior probability of user goal nodesgiven the evidence so far.A request to the dialog state tracking service takesthe form of a set of concept names, to which the serviceresponds with an m-best list of concept value assign-ments along with the joint posterior probability.433 Rapid System Build EnvironmentThe models required for the core interaction compo-nents in the system can be build from a single set oflabeled examples ?Examples.txt?
DORQJZLWKDconceptVWUXFWXUH ILOH ?Structure.txt?
used by the Dialog StateTracker as shown in Figure 2.
Running the automaticbuild procedure on these two files will generate 3 newmodels,The data LQ WKH?
([DPSOHVW[W? ILOH LVused to trainthe language model and pronunciation dictionary usedby the Speech Detection and Understanding Node andthe statistical CRF-parser applied in the Natural Lan-guage Understanding component.
Given a set of labeledexamples, the three models listed above are trained au-tomatically without any intervention required from theuser.
Once a system has been deployed, speech input islogged, and can be transcribed and labeled with seman-tic concepts to improve the effectiveness of these com-ponent models.As explained in section 3.5, our dialog state trackerorganizes concepts in a tree structure.
For a given do-main, we specify that structure in a simple text filewhere each line contains a concept followed by thename of the parent concept or the keyword ROOT forthe root of the tree.
Based on this file and on the SLUdata file, the resource building process generates thefiles required by the Hound belief tracker at runtime.7KLV ?RII-the-VKHOI? VWUXFWXUH assumes at each node auniform conditional distribution of children values giv-en the parent value.
These distributions are stored in ahuman-readable text file and can thus be manually up-dated to more informative values.Using the above tools, we have developed a sampleusing the proposed framework for robot navigation task.The entire system can be build from a single set of la-beled examples as shown in Figure 3 used to train thelanguage model and a component to perform actions onthe SLU output.4 ConclusionsIn this paper we introduce HRItk (the Human-Robot-Interaction toolkit), a framework, consisting of messag-ing protocols, components, and development tools forrapidly building speech-centric interactive systemswithin the ROS environment.
The proposed toolkit pro-vides all the core components required for speech inte-raction, including, speech recognition, natural languageunderstanding and belief tracking and initial implemen-tations for gesture recognition and gaze tracking.
Thetoolkit is specifically designed for extensibility, ease ofuse, and rapid development, allowing developers toquickly incorporate speech interaction into existingROS projects.ReferencesBohren J., Rusu R., Jones E., Marder-Eppstein E., PantofaruC., Wise M., Mosenlechner L., Meeussen W., and HolzerS.
2011.
Towards autonomous robotic butlers: Lessonslearned with the PR2, Proc.
ICRA 2011Bolme, S., Draper, B., and  Beveridge, J.
2009.
Average ofSynthetic Exact Filters, Proc.
CVPR 2009.Clodic, A., Cao, H., Alili, S., Montreuil, V., Alami, R. and-Chatila, R. 2008.
Shary: A Supervision System Adapted to-Human-Robot Interaction.
In Proc.
ISER 2008.Cohn, T. 2007.
Scaling conditional random fields for naturallanguage processing.
University of Melbourne.Fong T., Kunz C., Hiatt L. and Bugajska M. 2006.
The Hu-man-Robot Interaction Operating System.
Proc.
HRI 2006.Fujimura, K. and Xu, L. 2007.
Sign recognition using con-strained optimization.
Proc.
ACCV 2007.Kanda, T., Shiomi M., Miyashita Z., Ishiguro H., and HagitaN.
2009.
An affective guide robot in a shopping mall.
InProc.
HRI 2009Lafferty J., McCallum A., and Pereira F..
Conditional randomfields: Probabilistic models for segmenting and labelingsequence data.
In Intl.
Conf.
on Machine Learning, 2001.Lee, A. and Kawahara, T. 2009.
Recent Development of Open-Source Speech Recognition Engine Julius.
Proc.
Asia-Pacific Signal and Information Processing Association An-nual Summit and Conference (APSIPA ASC), 2009.Nakadai, K., Takahashi, T., Okuno, H.G., Nakajima, H., Ha-segawa, Y., and Tsujino, H. 2010.
Design and Implementa-tion of Robot Audition System "HARK" .Quigley, M., Gerkey, B., Conley, K., Faust, J., Foote, T.Leibs, J., Berger, E., Wheeler, R. and Ng, A.
2009.
ROS:an open-source robot operating system.
Proc.
Open-sourceSoftware Workshop, ICRA 2009.Raux, A. and Ma, Y.
2011.
Efficient Probabilistic Tracking ofUser Goal and Dialog History for Spoken Dialog Systems.Proc.
Interspeech 2011.Rosenthal S., Veloso M. 2010.
Using Symbiotic Relationshipswith Humans to Help Robots Overcome Limitations.
InWorkshop for Collaborative Human/AI Control for Interac-tive Experiences 2010.Srinivasa S., Ferguson D., Helfrich C., Berenson D., Collet A.,Diankov R., Gallagher G., Hollinger G., Kuffner J., Vande-Weghe M. 2009.
Herb: A Home Exploring Robotic Butler.Autonomous Robots, 2009Examples.txt<Tagged example sentence> <Action>@Room{kitchen} ?
None ?on ?the ?
@Floor{fifth} ?floor ?
None ?take ?this ?
@Object{package} ?
?to ?
@Room{room ?123} ?
?
Carry ?Structure.txt<Node> <Parent>Room ?
?
ROOT ?Floor ?
Room ?Object ?
RoomFigure 2: Training examples for robot navigation task44
