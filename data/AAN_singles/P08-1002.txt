Proceedings of ACL-08: HLT, pages 10?18,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsDistributional Identification of Non-Referential PronounsShane BergsmaDepartment of Computing ScienceUniversity of AlbertaEdmonton, AlbertaCanada, T6G 2E8bergsma@cs.ualberta.caDekang LinGoogle, Inc.1600 Amphitheatre ParkwayMountain ViewCalifornia, 94301lindek@google.comRandy GoebelDepartment of Computing ScienceUniversity of AlbertaEdmonton, AlbertaCanada, T6G 2E8goebel@cs.ualberta.caAbstractWe present an automatic approach to deter-mining whether a pronoun in text refers toa preceding noun phrase or is instead non-referential.
We extract the surrounding tex-tual context of the pronoun and gather, froma large corpus, the distribution of words thatoccur within that context.
We learn to reliablyclassify these distributions as representing ei-ther referential or non-referential pronoun in-stances.
Despite its simplicity, experimentalresults on classifying the English pronoun itshow the system achieves the highest perfor-mance yet attained on this important task.1 IntroductionThe goal of coreference resolution is to determinewhich noun phrases in a document refer to the samereal-world entity.
As part of this task, coreferenceresolution systems must decide which pronouns re-fer to preceding noun phrases (called antecedents)and which do not.
In particular, a long-standingchallenge has been to correctly classify instances ofthe English pronoun it.
Consider the sentences:(1) You can make it in advance.
(2) You can make it in Hollywood.In sentence (1), it is an anaphoric pronoun refer-ring to some previous noun phrase, like ?the sauce?or ?an appointment.?
In sentence (2), it is part of theidiomatic expression ?make it?
meaning ?succeed.
?A coreference resolution system should find an an-tecedent for the first it but not the second.
Pronounsthat do not refer to preceding noun phrases are callednon-anaphoric or non-referential pronouns.The word it is one of the most frequent words inthe English language, accounting for about 1% oftokens in text and over a quarter of all third-personpronouns.1 Usually between a quarter and a half ofit instances are non-referential (e.g.
Section 4, Ta-ble 3).
As with other pronouns, the preceding dis-course can affect it?s interpretation.
For example,sentence (2) can be interpreted as referential if thepreceding sentence is ?You want to make a movie?
?We show, however, that we can reliably classify apronoun as being referential or non-referential basedsolely on the local context surrounding the pronoun.We do this by turning the context into patterns andenumerating all the words that can take the place ofit in these patterns.
For sentence (1), we can ex-tract the context pattern ?make * in advance?
andfor sentence (2) ?make * in Hollywood,?
where ?
*?is a wildcard that can be filled by any token.
Non-referential distributions tend to have the word it fill-ing the wildcard position.
Referential distributionsoccur with many other noun phrase fillers.
For ex-ample, in our n-gram collection (Section 3.4), ?makeit in advance?
and ?make them in advance?
occurroughly the same number of times (442 vs. 449), in-dicating a referential pattern.
In contrast, ?make it inHollywood?
occurs 3421 times while ?make them inHollywood?
does not occur at all.These simple counts strongly indicate whether an-other noun can replace the pronoun.
Thus we cancomputationally distinguish between a) pronounsthat refer to nouns, and b) all other instances: includ-ing those that have no antecedent, like sentence (2),1e.g.
http://ucrel.lancs.ac.uk/bncfreq/flists.html10and those that refer to sentences, clauses, or impliedtopics of discourse.
Beyond the practical value ofthis distinction, Section 3 provides some theoreticaljustification for our binary classification.Section 3 also shows how to automatically extractand collect counts for context patterns, and how tocombine the information using a machine learnedclassifier.
Section 4 describes our data for learningand evaluation, It-Bank: a set of over three thousandlabelled instances of the pronoun it from a varietyof text sources.
Section 4 also explains our com-parison approaches and experimental methodology.Section 5 presents our results, including an interest-ing comparison of our system to human classifica-tion given equivalent segments of context.2 Related WorkThe difficulty of non-referential pronouns has beenacknowledged since the beginning of computationalresolution of anaphora.
Hobbs (1978) notes his algo-rithm does not handle pronominal references to sen-tences nor cases where it occurs in time or weatherexpressions.
Hirst (1981, page 17) emphasizes theimportance of detecting non-referential pronouns,?lest precious hours be lost in bootless searchesfor textual referents.?
Mu?ller (2006) summarizesthe evolution of computational approaches to non-referential it detection.
In particular, note the pio-neering work of Paice and Husk (1987), the inclu-sion of non-referential it detection in a full anaphoraresolution system by Lappin and Leass (1994), andthe machine learning approach of Evans (2001).There has recently been renewed interest innon-referential pronouns, driven by three primarysources.
First of all, research in coreference resolu-tion has shown the benefits of modules for generalnoun anaphoricity determination (Ng and Cardie,2002; Denis and Baldridge, 2007).
Unfortunately,these studies handle pronouns inadequately; judg-ing from the decision trees and performance fig-ures, Ng and Cardie (2002)?s system treats all pro-nouns as anaphoric by default.
Secondly, whilemost pronoun resolution evaluations simply excludenon-referential pronouns, recent unsupervised ap-proaches (Cherry and Bergsma, 2005; Haghighi andKlein, 2007) must deal with all pronouns in unre-stricted text, and therefore need robust modules toautomatically handle non-referential instances.
Fi-nally, reference resolution has moved beyond writ-ten text into in spoken dialog.
Here, non-referentialpronouns are pervasive.
Eckert and Strube (2000)report that in the Switchboard corpus, only 45%of demonstratives and third-person pronouns have anoun phrase antecedent.
Handling the common non-referential instances is thus especially vital.One issue with systems for non-referential detec-tion is the amount of language-specific knowledgethat must be encoded.
Consider a system that jointlyperforms anaphora resolution and word alignmentin parallel corpora for machine translation.
For thistask, we need to identify non-referential anaphora inmultiple languages.
It is not always clear to whatextent the features and modules developed for En-glish systems apply to other languages.
For exam-ple, the detector of Lappin and Leass (1994) labels apronoun as non-referential if it matches one of sev-eral syntactic patterns, including: ?It is Cogv-ed thatSentence,?
where Cogv is a ?cognitive verb?
suchas recommend, think, believe, know, anticipate, etc.Porting this approach to a new language would re-quire not only access to a syntactic parser and a listof cognitive verbs in that language, but the devel-opment of new patterns to catch non-referential pro-noun uses that do not exist in English.Moreover, writing a set of rules to capture thisphenomenon is likely to miss many less-commonuses.
Alternatively, recent machine-learning ap-proaches leverage a more general representation ofa pronoun instance.
For example, Mu?ller (2006)has a feature for ?distance to next complementizer(that, if, whether)?
and features for the tokens andpart-of-speech tags of the context words.
Unfor-tunately, there is still a lot of implicit and explicitEnglish-specific knowledge needed to develop thesefeatures, including, for example, lists of ?seem?verbs such as appear, look, mean, happen.
Sim-ilarly, the machine-learned system of Boyd et al(2005) uses a set of ?idiom patterns?
like ?on theface of it?
that trigger binary features if detected inthe pronoun context.
Although machine learned sys-tems can flexibly balance the various indicators andcontra-indicators of non-referentiality, a particularfeature is only useful if it is relevant to an examplein limited labelled training data.Our approach avoids hand-crafting a set of spe-11cific indicator features; we simply use the distribu-tion of the pronoun?s context.
Our method is thusrelated to previous work based on Harris (1985)?sdistributional hypothesis.2 It has been used to deter-mine both word and syntactic path similarity (Hin-dle, 1990; Lin, 1998a; Lin and Pantel, 2001).
Ourwork is part of a trend of extracting other importantinformation from statistical distributions.
Dagan andItai (1990) use the distribution of a pronoun?s con-text to determine which candidate antecedents can fitthe context.
Bergsma and Lin (2006) determine thelikelihood of coreference along the syntactic pathconnecting a pronoun to a possible antecedent, bylooking at the distribution of the path in text.
Theseapproaches, like ours, are ways to inject sophisti-cated ?world knowledge?
into anaphora resolution.3 Methodology3.1 DefinitionOur approach distinguishes contexts where pro-nouns cannot be replaced by a preceding nounphrase (non-noun-referential) from those wherenouns can occur (noun-referential).
Although coref-erence evaluations, such as the MUC (1997) tasks,also make this distinction, it is not necessarilyused by all researchers.
Evans (2001), for exam-ple, distinguishes between ?clause anaphoric?
and?pleonastic?
as in the following two instances:(3) The paper reported that it had snowed.
It wasobvious.
(clause anaphoric)(4) It was obvious that it had snowed.
(pleonastic)The word It in sentence (3) is considered referen-tial, while the word It in sentence (4) is considerednon-referential.3 From our perspective, this inter-pretation is somewhat arbitrary.
One could also saythat the It in both cases refers to the clause ?that ithad snowed.?
Indeed, annotation experiments usingvery fine-grained categories show low annotation re-liability (Mu?ller, 2006).
On the other hand, thereis no debate over the importance nor the definitionof distinguishing pronouns that refer to nouns fromthose that do not.
We adopt this distinction for our2Words occurring in similar contexts have similar meanings3The it in ?it had snowed?
is, of course, non-referential.work, and show it has good inter-annotator reliabil-ity (Section 4.1).
We henceforth refer to non-noun-referential simply as non-referential, and thus con-sider the word It in both sentences (3) and (4) asnon-referential.Non-referential pronouns are widespread in nat-ural language.
The es in the German ?Wie geht esIhnen?
and the il in the French ?S?il vous pla??t?
areboth non-referential.
In pro-drop languages that mayomit subject pronouns, there remains the questionof whether an omitted pronoun is referential (Zhaoand Ng, 2007).
Although we focus on the Englishpronoun it, our approach should differentiate anywords that have both a structural and a referentialrole in language, e.g.
words like this, there andthat (Mu?ller, 2007).
We believe a distributional ap-proach could also help in related tasks like identify-ing the generic use of you (Gupta et al, 2007).3.2 Context DistributionOur method extracts the context surrounding a pro-noun and determines which other words can take theplace of the pronoun in the context.
The extractedsegments of context are called context patterns.
Thewords that take the place of the pronoun are calledpattern fillers.
We gather pattern fillers from a largecollection of n-gram frequencies.
The maximumsize of a context pattern depends on the size of n-grams available in the data.
In our n-gram collection(Section 3.4), the lengths of the n-grams range fromunigrams to 5-grams, so our maximum pattern sizeis five.
For a particular pronoun in text, there are fivepossible 5-grams that span the pronoun.
For exam-ple, in the following instance of it:... said here Thursday that it is unnecessary to continue ...We can extract the following 5-gram patterns:said here Thursday that *here Thursday that * isThursday that * is unnecessarythat * is unnecessary to* is unnecessary to continueSimilarly, we extract the four 4-gram patterns.Shorter n-grams were not found to improve perfor-mance on development data and hence are not ex-tracted.
We only use context within the current sen-tence (including the beginning-of-sentence and end-of-sentence tokens) so if a pronoun occurs near asentence boundary, some patterns may be missing.12Pattern Filler Type String#1: 3rd-person pron.
sing.
it/its#2: 3rd-person pron.
plur.
they/them/their#3: any other pronoun he/him/his/,I/me/my, etc.#4: infrequent word token ?UNK?#5: any other token *Table 1: Pattern filler typesWe take a few steps to improve generality.
Wechange the patterns to lower-case, convert sequencesof digits to the # symbol, and run the Porter stem-mer4 (Porter, 1980).
To generalize rare names, weconvert capitalized words longer than five charac-ters to a special NE tag.
We also added a few simplerules to stem the irregular verbs be, have, do, andsaid, and convert the common contractions ?nt, ?s,?m, ?re, ?ve, ?d, and ?ll to their most likely stem.We do the same processing to our n-gram corpus.We then find all n-grams matching our patterns, al-lowing any token to match the wildcard in place ofit.
Also, other pronouns in the pattern are allowedto match a corresponding pronoun in an n-gram, re-gardless of differences in inflection and class.We now discuss how to use the distribution of pat-tern fillers.
For identifying non-referential it in En-glish, we are interested in how often it occurs as apattern filler versus other nouns.
However, deter-mining part-of-speech in a large n-gram corpus isnot simple, nor would it easily extend to other lan-guages.
Instead, we gather counts for five differ-ent classes of words that fill the wildcard position,easily determined by string match (Table 1).
Thethird-person plural they (#2) reliably occurs in pat-terns where referential it also resides.
The occur-rence of any other pronoun (#3) guarantees that atthe very least the pattern filler is a noun.
A matchwith the infrequent word token ?UNK?
(#4) (ex-plained in Section 3.4) will likely be a noun becausenouns account for a large proportion of rare words ina corpus.
Gathering any other token (#5) also mostlyfinds nouns; inserting another part-of-speech usually4Adapted from the Bow-toolkit (McCallum, 1996).
Ourmethod also works without the stemmer; we simply truncatethe words in the pattern at a given maximum length (see Sec-tion 5.1).
With simple truncation, all the pattern processing canbe easily applied to other languages.Pattern Filler Counts#1 #2 #3 #5sai here NE that * 84 0 291 3985here NE that * be 0 0 0 93NE that * be unnecessari 0 0 0 0that * be unnecessari to 16726 56 0 228* be unnecessari to continu 258 0 0 0Table 2: 5-gram context patterns and pattern-filler countsfor the Section 3.2 example.results in an unlikely, ungrammatical pattern.Table 2 gives the stemmed context patterns for ourrunning example.
It also gives the n-gram countsof pattern fillers matching the first four filler types(there were no matches of the ?UNK?
type, #4).3.3 Feature Vector RepresentationThere are many possible ways to use the abovecounts.
Intuitively, our method should identify asnon-referential those instances that have a high pro-portion of fillers of type #1 (i.e., the word it), whilelabelling as referential those with high counts forother types of fillers.
We would also like to lever-age the possibility that some of the patterns may bemore predictive than others, depending on where thewildcard lies in the pattern.
For example, in Table 2,the cases where the it-position is near the beginningof the pattern best reflect the non-referential natureof this instance.
We can achieve these aims by or-dering the counts in a feature vector, and using a la-belled set of training examples to learn a classifierthat optimally weights the counts.For classification, we define non-referential aspositive and referential as negative.
Our feature rep-resentation very much resembles Table 2.
For eachof the five 5-gram patterns, ordered by the positionof the wildcard, we have features for the logarithmof counts for filler types #1, #2, ... #5.
Similarly,for each of the four 4-gram patterns, we provide thelog-counts corresponding to types #1, #2, ... #5 aswell.
Before taking the logarithm, we smooth thecounts by adding a fixed number to all observed val-ues.
We also provide, for each pattern, a feature thatindicates if the pattern is not available because theit-position would cause the pattern to span beyondthe current sentence.
There are twenty-five 5-gram,twenty 4-gram, and nine indicator features in total.13Our classifier should learn positive weights on thetype #1 counts and negative weights on the othertypes, with higher absolute weights on the more pre-dictive filler types and pattern positions.
Note thatleaving the pattern counts unnormalized automati-cally allows patterns with higher counts to contributemore to the prediction of their associated instances.3.4 N-Gram DataWe now describe the collection of n-grams and theircounts used in our implementation.
We use, to ourknowledge, the largest publicly available collection:the Google Web 1T 5-gram Corpus Version 1.1.5This collection was generated from approximately 1trillion tokens of online text.
In this data, tokens ap-pearing less than 200 times have been mapped to the?UNK?
symbol.
Also, only n-grams appearing morethan 40 times are included.
For languages wheresuch an extensive n-gram resource is not available,the n-gram counts could also be taken from the page-counts returned by an Internet search engine.4 Evaluation4.1 Labelled It DataWe need labelled data for training and evaluation ofour system.
This data indicates, for every occurrenceof the pronoun it, whether it refers to a precedingnoun phrase or not.
Standard coreference resolutiondata sets annotate all noun phrases that have an an-tecedent noun phrase in the text.
Therefore, we canextract labelled instances of it from these sets.
Wedo this for the dry-run and formal sets from MUC-7(1997), and merge them into a single data set.Of course, full coreference-annotated data is aprecious resource, with the pronoun it making uponly a small portion of the marked-up noun phrases.We thus created annotated data specifically for thepronoun it.
We annotated 1020 instances in a col-lection of Science News articles (from 1995-2000),downloaded from the Science News website.
Wealso annotated 709 instances in the WSJ portion ofthe DARPA TIPSTER Project (Harman, 1992), and279 instances in the English portion of the EuroparlCorpus (Koehn, 2005).A single annotator (A1) labelled all three datasets, while two additional annotators not connected5Available from the LDC as LDC2006T13Data Set Number of It % Non-ReferentialEuroparl 279 50.9Sci-News 1020 32.6WSJ 709 25.1MUC 129 31.8Train 1069 33.2Test 1067 31.7Test-200 200 30.0Table 3: Data sets used in experiments.with the project (A2 and A3) were asked to sepa-rately re-annotate a portion of each, so that inter-annotator agreement could be calculated.
A1 andA2 agreed on 96% of annotation decisions, whileA1-A3, and A2-A3, agreed on 91% and 93% of de-cisions, respectively.
The Kappa statistic (Jurafskyand Martin, 2000, page 315), with P(E) computedfrom the confusion matrices, was a high 0.90 for A1-A2, and 0.79 and 0.81 for the other pairs, around the0.80 considered to be good reliability.
These are,perhaps surprisingly, the only known it-annotation-agreement statistics available for written text.
Theycontrast favourably with the low agreement seen oncategorizing it in spoken dialog (Mu?ller, 2006).We make all the annotations available in It-Bank,an online repository for annotated it-instances.6It-Bank also allows other researchers to distributetheir it annotations.
Often, the full text of articlescontaining annotations cannot be shared because ofcopyright.
However, sharing just the sentences con-taining the word it, randomly-ordered, is permissibleunder fair-use guidelines.
The original annotatorsretain their copyright on the annotations.We use our annotated data in two ways.
Firstof all, we perform cross-validation experiments oneach of the data sets individually, to help gauge thedifficulty of resolution on particular domains andvolumes of training data.
Secondly, we randomlydistribute all instances into two main sets, a trainingset and a test set.
We also construct a smaller testset, Test-200, containing only the first 200 instancesin the Test set.
We use Test-200 for human experi-ments and error analysis (Section 5.2).
Table 3 sum-marizes all the sets used in the experiments.6www.cs.ualberta.ca/?bergsma/ItBank/.
It-Bank also con-tains an additional 1,077 examples used as development data.144.2 Comparison ApproachesWe represent feature vectors exactly as describedin Section 3.3.
We smooth by adding 40 to allcounts, equal to the minimum count in the n-gramdata.
For classification, we use a maximum entropymodel (Berger et al, 1996), from the logistic re-gression package in Weka (Witten and Frank, 2005),with all default parameter settings.
Results withour distributional approach are labelled as DISTRIB.Note that our maximum entropy classifier actuallyproduces a probability of non-referentiality, whichis thresholded at 50% to make a classification.As a baseline, we implemented the non-referentialit detector of Lappin and Leass (1994), labelled asLL in the results.
This is a syntactic detector, apoint missed by Evans (2001) in his criticism: thepatterns are robust to intervening words and modi-fiers (e.g.
?it was never thought by the committeethat...?)
provided the sentence is parsed correctly.7We automatically parse sentences with Minipar, abroad-coverage dependency parser (Lin, 1998b).We also use a separate, extended version ofthe LL detector, implemented for large-scale non-referential detection by Cherry and Bergsma (2005).This system, also for Minipar, additionally detectsinstances of it labelled with Minipar?s pleonastic cat-egory Subj.
It uses Minipar?s named-entity recog-nition to identify time expressions, such as ?it wasmidnight,?
and provides a number of other patternsto match common non-referential it uses, such asin expressions like ?darn it,?
?don?t overdo it,?
etc.This extended detector is labelled as MINIPL (forMinipar pleonasticity) in our results.Finally, we tested a system that combines theabove three approaches.
We simply add the LL andMINIPL decisions as binary features in the DISTRIBsystem.
This system is called COMBO in our results.4.3 Evaluation CriteriaWe follow Mu?ller (2006)?s evaluation criteria.
Pre-cision (P) is the proportion of instances that we la-bel as non-referential that are indeed non-referential.Recall (R) is the proportion of true non-referentialsthat we detect, and is thus a measure of the coverage7Our approach, on the other hand, would seem to be suscep-tible to such intervening material, if it pushes indicative contexttokens out of the 5-token window.System P R F AccLL 93.4 21.0 34.3 74.5MINIPL 66.4 49.7 56.9 76.1DISTRIB 81.4 71.0 75.8 85.7COMBO 81.3 73.4 77.1 86.2Table 4: Train/Test-split performance (%).of the system.
F-Score (F) is the geometric averageof precision and recall; it is the most common non-referential detection metric.
Accuracy (Acc) is thepercentage of instances labelled correctly.5 Results5.1 System ComparisonTable 4 gives precision, recall, F-score, and accu-racy on the Train/Test split.
Note that while the LLsystem has high detection precision, it has very lowrecall, sharply reducing F-score.
The MINIPL ap-proach sacrifices some precision for much higherrecall, but again has fairly low F-score.
To ourknowledge, our COMBO system, with an F-Scoreof 77.1%, achieves the highest performance of anynon-referential system yet implemented.
Even moreimportantly, DISTRIB, which requires only minimallinguistic processing and no encoding of specific in-dicator patterns, achieves 75.8% F-Score.
The dif-ference between COMBO and DISTRIB is not statis-tically significant, while both are significantly bet-ter than the rule-based approaches.8 This providesstrong motivation for a ?light-weight?
approach tonon-referential it detection ?
one that does not re-quire parsing or hand-crafted rules and ?
is easilyported to new languages and text domains.Since applying an English stemmer to the con-text words (Section 3.2) reduces the portability ofthe distributional technique, we investigated the useof more portable pattern abstraction.
Figure 1 com-pares the use of the stemmer to simply truncating thewords in the patterns at a certain maximum length.Using no truncation (Unaltered) drops the F-Scoreby 4.3%, while truncating the patterns to a length offour only drops the F-Score by 1.4%, a differencewhich is not statistically significant.
Simple trunca-tion may be a good option for other languages wherestemmers are not readily available.
The optimum8All significance testing uses McNemar?s test, p<0.0515687072747678801  2  3  4  5  6  7  8  9  10F-ScoreTruncated word lengthStemmed patternsTruncated patternsUnaltered patternsFigure 1: Effect of pattern-word truncation on non-referential it detection (COMBO system, Train/Test split).System Europl.
Sci-News WSJ MUCLL 44.0 39.3 21.5 13.3MINIPL 70.3 61.8 22.0 50.7DISTRIB 79.7 77.2 69.5 68.2COMBO 76.2 78.7 68.1 65.9COMBO4 83.6 76.5 67.1 74.7Table 5: 10-fold cross validation F-Score (%).truncation size will likely depend on the length ofthe base forms of words in that language.
For real-world application of our approach, truncation alsoreduces the table sizes (and thus storage and look-up costs) of any pre-compiled it-pattern database.Table 5 compares the 10-fold cross-validation F-score of our systems on the four data sets.
Theperformance of COMBO on Europarl and MUC isaffected by the small number of instances in thesesets (Section 4, Table 3).
We can reduce data frag-mentation by removing features.
For example, if weonly use the length-4 patterns in COMBO (labelled asCOMBO4), performance increases dramatically onEuroparl and MUC, while dipping slightly for thelarger Sci-News and WSJ sets.
Furthermore, select-ing just the three most useful filler type counts asfeatures (#1,#2,#5), boosts F-Score on Europarl to86.5%, 10% above the full COMBO system.5.2 Analysis and DiscussionIn light of these strong results, it is worth consid-ering where further gains in performance might yetbe found.
One key question is to what extent a lim-ited context restricts identification performance.
Wefirst tested the importance of the pattern length bySystem P R F AccDISTRIB 80.0 73.3 76.5 86.5COMBO 80.7 76.7 78.6 87.5Human-1 92.7 63.3 75.2 87.5Human-2 84.0 70.0 76.4 87.0Human-3 72.2 86.7 78.8 86.0Table 6: Evaluation on Test-200 (%).using only the length-4 counts in the DISTRIB sys-tem (Train/Test split).
Surprisingly, the drop in F-Score was only one percent, to 74.8%.
Using onlythe length-5 counts drops F-Score to 71.4%.
Neitherare statistically significant; however there seems tobe diminishing returns from longer context patterns.Another way to view the limited context is to ask,given the amount of context we have, are we mak-ing optimum use of it?
We answer this by seeinghow well humans can do with the same information.As explained in Section 3.2, our system uses 5-gramcontext patterns that together span from four-to-the-left to four-to-the-right of the pronoun.
We thus pro-vide these same nine-token windows to our humansubjects, and ask them to decide whether the pro-nouns refer to previous noun phrases or not, basedon these contexts.
Subjects first performed a dry-run experiment on separate development data.
Theywere shown their errors and sources of confusionwere clarified.
They then made the judgments unas-sisted on the final Test-200 data.
Three humans per-formed the experiment.
Their results show a rangeof preferences for precision versus recall, with bothF-Score and Accuracy on average below the perfor-mance of COMBO (Table 6).
Foremost, these resultsshow that our distributional approach is already get-ting good leverage from the limited context informa-tion, around that achieved by our best human.It is instructive to inspect the twenty-five Test-200instances that the COMBO system classified incor-rectly, given human performance on this same set.Seventeen of the twenty-five COMBO errors werealso made by one or more human subjects, suggest-ing system errors are also mostly due to limited con-text.
For example, one of these errors was for thecontext: ?it takes an astounding amount...?
Here, thenon-referential nature of the instance is not apparentwithout the infinitive clause that ends the sentence:?...
of time to compare very long DNA sequences16with each other.
?Six of the eight errors unique to the COMBO sys-tem were cases where the system falsely said thepronoun was non-referential.
Four of these couldhave referred to entire sentences or clauses ratherthan nouns.
These confusing cases, for both hu-mans and our system, result from our definitionof a referential pronoun: pronouns with verbal orclause antecedents are considered non-referential(Section 3.1).
If an antecedent verb or clause isreplaced by a nominalization (Smith researched...to Smith?s research), a referring pronoun, in thesame context, becomes referential.
When we inspectthe probabilities produced by the maximum entropyclassifier (Section 4.2), we see only a weak bias forthe non-referential class on these examples, reflect-ing our classifier?s uncertainty.
It would likely bepossible to improve accuracy on these cases by en-coding the presence or absence of preceding nomi-nalizations as a feature of our classifier.Another false non-referential decision is for thephrase ?...
machine he had installed it on.?
The it isactually referential, but the extracted patterns (e.g.
?he had install * on?)
are nevertheless usually filledwith it.9 Again, it might be possible to fix such ex-amples by leveraging the preceding discourse.
No-tably, the first noun-phrase before the context is theword ?software.?
There is strong compatibility be-tween the pronoun-parent ?install?
and the candidateantecedent ?software.?
In a full coreference resolu-tion system, when the anaphora resolution modulehas a strong preference to link it to an antecedent(which it should when the pronoun is indeed refer-ential), we can override a weak non-referential prob-ability.
Non-referential it detection should not bea pre-processing step, but rather part of a globally-optimal configuration, as was done for general nounphrase anaphoricity by Denis and Baldridge (2007).The suitability of this kind of approach to correct-ing some of our system?s errors is especially obviouswhen we inspect the probabilities of the maximumentropy model?s output decisions on the Test-200set.
Where the maximum entropy classifier makesmistakes, it does so with less confidence than whenit classifies correct examples.
The average predicted9This example also suggests using filler counts for the word?the?
as a feature when it is the last word in the pattern.probability of the incorrect classifications is 76.0%while the average probability of the correct classi-fications is 90.3%.
Many incorrect decisions areready to switch sides; our next step will be to usefeatures of the preceding discourse and the candi-date antecedents to help give them a push.6 ConclusionWe have presented an approach to detecting non-referential pronouns in text based on the distribu-tion of the pronoun?s context.
The approach is sim-ple to implement, attains state-of-the-art results, andshould be easily ported to other languages.
Our tech-nique demonstrates how large volumes of data canbe used to gather world knowledge for natural lan-guage processing.
A consequence of this researchwas the creation of It-Bank, a collection of thou-sands of labelled examples of the pronoun it, whichwill benefit other coreference resolution researchers.Error analysis reveals that our system is gettinggood leverage out of the pronoun context, achiev-ing results comparable to human performance givenequivalent information.
To boost performance fur-ther, we will need to incorporate information frompreceding discourse.
Future research will also testthe distributional classification of other ambiguouspronouns, like this, you, there, and that.
Anotheravenue of study will look at the interaction betweencoreference resolution and machine translation.
Forexample, if a single form in English (e.g.
that)is separated into different meanings in another lan-guage (e.g., Spanish demonstrative ese, nominal ref-erence e?se, abstract or statement reference eso, andcomplementizer que), then aligned examples pro-vide automatically-disambiguated English data.
Wecould extract context patterns and collect statisticsfrom these examples like in our current approach.In general, jointly optimizing translation and coref-erence is an exciting and largely unexplored re-search area, now partly enabled by our portable non-referential detection methodology.AcknowledgmentsWe thank Kristin Musselman and Christopher Pinchak for as-sistance preparing the data, and we thank Google Inc. for shar-ing their 5-gram corpus.
We gratefully acknowledge supportfrom the Natural Sciences and Engineering Research Councilof Canada, the Alberta Ingenuity Fund, and the Alberta Infor-matics Circle of Research Excellence.17ReferencesAdam L. Berger, Stephen A. Della Pietra, and VincentJ.
Della Pietra.
1996.
A maximum entropy approachto natural language processing.
Computational Lin-guistics, 22(1):39?71.Shane Bergsma and Dekang Lin.
2006.
Bootstrap-ping path-based pronoun resolution.
In COLING-ACL, pages 33?40.Adrianne Boyd, Whitney Gegg-Harrison, and Donna By-ron.
2005.
Identifying non-referential it: a machinelearning approach incorporating linguistically moti-vated patterns.
In ACL Workshop on Feature Engi-neering for Machine Learning in NLP, pages 40?47.Colin Cherry and Shane Bergsma.
2005.
An expecta-tion maximization approach to pronoun resolution.
InCoNLL, pages 88?95.Ido Dagan and Alan Itai.
1990.
Automatic processing oflarge corpora for the resolution of anaphora references.In COLING, volume 3, pages 330?332.Pascal Denis and Jason Baldridge.
2007.
Joint determi-nation of anaphoricity and coreference using integerprogramming.
In NAACL-HLT, pages 236?243.Miriam Eckert and Michael Strube.
2000.
Dialogue acts,synchronizing units, and anaphora resolution.
Journalof Semantics, 17(1):51?89.Richard Evans.
2001.
Applying machine learning to-ward an automatic classification of it.
Literary andLinguistic Computing, 16(1):45?57.Surabhi Gupta, Matthew Purver, and Dan Jurafsky.
2007.Disambiguating between generic and referential ?you?in dialog.
In ACL Demo and Poster Sessions, pages105?108.Aria Haghighi and Dan Klein.
2007.
Unsupervisedcoreference resolution in a nonparametric Bayesianmodel.
In ACL, pages 848?855.Donna Harman.
1992.
The DARPA TIPSTER project.ACM SIGIR Forum, 26(2):26?28.Zellig Harris.
1985.
Distributional structure.
In J.J.Katz, editor, The Philosophy of Linguistics, pages 26?47.
Oxford University Press, New York.Donald Hindle.
1990.
Noun classification frompredicate-argument structures.
In ACL, pages 268?275.Graeme Hirst.
1981.
Anaphora in Natural LanguageUnderstanding: A Survey.
Springer Verlag.Jerry Hobbs.
1978.
Resolving pronoun references.
Lin-gua, 44(311):339?352.Daniel Jurafsky and James H. Martin.
2000.
Speech andlanguage processing.
Prentice Hall.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In MT Summit X, pages79?86.Shalom Lappin and Herbert J. Leass.
1994.
An algo-rithm for pronominal anaphora resolution.
Computa-tional Linguistics, 20(4):535?561.Dekang Lin and Patrick Pantel.
2001.
Discovery of infer-ence rules for question answering.
Natural LanguageEngineering, 7(4):343?360.Dekang Lin.
1998a.
Automatic retrieval and clusteringof similar words.
In COLING-ACL, pages 768?773.Dekang Lin.
1998b.
Dependency-based evaluation ofMINIPAR.
In LREC Workshop on the Evaluation ofParsing Systems.Andrew Kachites McCallum.
1996.
Bow:A toolkit for statistical language modeling,text retrieval, classification and clustering.http://www.cs.cmu.edu/?mccallum/bow.MUC-7.
1997.
Coreference task definition (v3.0, 13 Jul97).
In Proceedings of the Seventh Message Under-standing Conference (MUC-7).Christoph Mu?ller.
2006.
Automatic detection of non-referential It in spoken multi-party dialog.
In EACL,pages 49?56.Christoph Mu?ller.
2007.
Resolving It, This, and That inunrestricted multi-party dialog.
In ACL, pages 816?823.Vincent Ng and Claire Cardie.
2002.
Identifyinganaphoric and non-anaphoric noun phrases to improvecoreference resolution.
In COLING, pages 730?736.Chris D. Paice and Gareth D. Husk.
1987.
Towards theautomatic recognition of anaphoric features in Englishtext: the impersonal pronoun ?it?.
Computer Speechand Language, 2:109?132.Martin F. Porter.
1980.
An algorithm for suffix stripping.Program, 14(3):130?137.Ian H. Witten and Eibe Frank.
2005.
Data Mining: Prac-tical machine learning tools and techniques.
MorganKaufmann, second edition.Shanheng Zhao and Hwee Tou Ng.
2007.
Identificationand resolution of Chinese zero pronouns: A machinelearning approach.
In EMNLP, pages 541?550.18
