Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 21?30,Honolulu, October 2008. c?2008 Association for Computational LinguisticsRegular Expression Learning for Information ExtractionYunyao Li, Rajasekar Krishnamurthy, Sriram Raghavan, Shivakumar VaithyanathanIBM Almaden Research CenterSan Jose, CA 95120{yunyaoli, rajase, rsriram}@us.ibm.com, shiv@almaden.ibm.comH.
V. Jagadish?Department of EECSUniversity of MichiganAnn Arbor, MI 48109jag@umich.eduAbstractRegular expressions have served as the dom-inant workhorse of practical information ex-traction for several years.
However, there hasbeen little work on reducing the manual ef-fort involved in building high-quality, com-plex regular expressions for information ex-traction tasks.
In this paper, we propose Re-LIE, a novel transformation-based algorithmfor learning such complex regular expressions.We evaluate the performance of our algorithmon multiple datasets and compare it against theCRF algorithm.
We show that ReLIE, in ad-dition to being an order of magnitude faster,outperforms CRF under conditions of limitedtraining data and cross-domain data.
Finally,we show how the accuracy of CRF can be im-proved by using features extracted by ReLIE.1 IntroductionA large class of entity extraction tasks can be ac-complished by the use of carefully constructed reg-ular expressions (regexes).
Examples of entitiesamenable to such extractions include email ad-dresses and software names (web collections), creditcard numbers and social security numbers (emailcompliance), and gene and protein names (bioinfor-matics), etc.
These entities share the characteristicthat their key representative patterns (features) areexpressible in standard constructs of regular expres-sions.
At first glance, it may seem that constructing?Supported in part by NSF 0438909 and NIH 1-U54-DA021519.a regex to extract such entities is fairly straightfor-ward.
In reality, robust extraction requires the useof rather complex expressions, as illustrated by thefollowing example.Example 1 (Phone number extraction).
An obviouspattern for identifying phone numbers is ?blocks ofdigits separated by hyphens?
represented as R1 =(\d+\-)+\d+.1 While R1 matches valid phone numberslike 800-865-1125 and 725-1234, it suffers from both?precision?
and ?recall?
problems.
Not only does R1produce incorrect matches (e.g., social security numberslike 123-45-6789), it also fails to identify valid phonenumbers such as 800.865.1125, and (800)865-CARE.
Animproved regex that addresses these problems is R2 =(\d{3}[-.\ ()]){1,2}[\dA-Z]{4}.While multiple machine learning approaches havebeen proposed for information extraction in recentyears (McCallum et al, 2000; Cohen and McCal-lum, 2003; Klein et al, 2003; Krishnan and Man-ning, 2006), manually created regexes remain awidely adopted practical solution for informationextraction (Appelt and Onyshkevych, 1998; Fukudaet al, 1998; Cunningham, 1999; Tanabe and Wilbur,2002; Li et al, 2006; DeRose et al, 2007; Zhu et al,2007).
Yet, with a few notable exceptions, which wediscuss later in Section 1.1, there has been very littlework in reducing this human effort through the useof automatic learning techniques.
In this paper, wepropose a novel formulation of the problem of learn-1Throughout this paper, we use the syntax of the standardJava regex engine (Java, 2008).21ing regexes for information extraction tasks.
Wedemonstrate that high quality regex extractors can belearned with significantly reduced manual effort.
Tomotivate our approach, we first discuss prior workin the area of learning regexes and describe some ofthe limitations of these techniques.1.1 Learning Regular ExpressionsThe problem of inducing regular languages frompositive and negative examples has been studied inthe past, even outside the context of informationextraction (Alquezar and Sanfeliu, 1994; Dupont,1996; Firoiu et al, 1998; Garofalakis et al, 2000;Denis, 2001; Denis et al, 2004; Fernau, 2005;Galassi and Giordana, 2005; Bex et al, 2006).Much of this work assumes that the target regexis small and compact thereby allowing the learn-ing algorithm to exploit this information.
Consider,for example, the learning of patterns motivated byDNA sequencing applications (Galassi and Gior-dana, 2005).
Here the input sequence is viewedas multiple atomic events separated by gaps.
Sinceeach atomic event is easily described by a small andcompact regex, the problem reduces to one of learn-ing simple regexes.
Similarly, in XML DTD infer-ence (Garofalakis et al, 2000; Bex et al, 2006), itis possible to exploit the fact that the XML docu-ments of interest are often described using simpleDTDs.
E.g., in an online books store, each bookhas a title, one or more authors and price.
This in-formation can be described in a DTD as ?book?
??title??author?
+ ?price?.
However, as shown in Ex-ample 1, regexes for information extraction rely onmore complex constructs.In the context of information extraction, priorwork has concentrated primarily on learning regexesover relatively small alphabet sizes.
A commontheme in (Soderland, 1999; Ciravegna, 2001; Wuand Pottenger, 2005; Feldman et al, 2006) is theproblem of learning regexes over tagged tokensproduced by other text-processing steps such asPOS tagging, morphological analysis, and gazetteermatching.
Thus, the alphabet is defined by the spaceof possible tags output by these analysis steps.
Asimilar approach has been proposed in (Brill, 2000)for POS disambiguation.
In contrast, our paper ad-dresses extraction tasks that require ?fine-grained?control to accurately capture the structural featuresof the entity of interest.
Consequently, the domainof interest consists of all characters thereby dramat-ically increasing the size of the alphabet.
To enablethis scale-up, the techniques presented in this paperexploit advanced syntactic constructs (such as char-acter classes and quantifiers) supported by modernregex languages.Finally, we note that almost all of the above de-scribed work define the learning problem over arestricted class of regexes.
Typically, the restric-tions involve either disallowing or limiting the use ofKleene disclosure and disjunction operations.
How-ever, our work imposes no such restrictions.1.2 ContributionsIn a key departure from prior formulations, thelearning algorithm presented in this work takes asinput not just labeled examples but also an initialregular expression.
The use of an initial regex hastwo major advantages.
First, this expression pro-vides a natural mechanism for a domain expert toprovide domain knowledge about the structure of theentity being extracted.
Second, as we show in Sec-tion 2, the space of output regular expressions un-der consideration can be meaningfully restricted byappropriately defining their relationship to the inputexpression.
Such a principled approach to restrictthe search space permits the learning algorithm toconsider complex regexes in a tractable manner.
Incontrast, prior work defined a tractable search spaceby placing restrictions on the target class of regularexpressions.
Our specific contributions are:?
A novel regex learning problem consisting of learn-ing an ?improved?
regex given an initial regex andlabeled examples?
Formulation of this learning task as an optimizationproblem over a search space of regexes?
ReLIE, a regex learning algorithm that employstransformations to navigate the search space?
Extensive experimental results over multipledatasets to show the effectiveness of ReLIE anda comparison study with the Conditional RandomField (CRF) algorithm?
Finally, experiments that demonstrate the benefitsof using ReLIE as a feature extractor for CRF andpossibly other machine learning algorithms.222 The Regex Learning ProblemConsider the task of identifying instances of someentity E .
Let R0 denote the input regex provided bythe user and let M(R0 ,D) denote the set of matchesobtained by evaluating R0 over a document col-lection D. Let Mp(R0 ,D) = {x ?
M(R0 ,D) :x instance of E} and Mn(R0 ,D) = {x ?
M(R0 ,D) :x not an instance of E} denote the set of positive andnegative matches for R0 .
Note that a match is pos-itive if it corresponds to an instance of the entity ofinterest and is negative otherwise.
The goal of ourlearning task is to produce a regex that is ?better?than R0 at identifying instances of E .Given a candidate regex R, we need a mechanismto judge whether R is indeed a better extractor forE than R0 .
To make this judgment even for just theoriginal document collection D, we must be able tolabel each instance matched byR (i.e., each elementof M(R,D)) as positive or negative.
Clearly, thiscan be accomplished if the set of matches producedbyR are contained within the set of available labeledexamples, i.e., if M(R,D) ?
M(R0 ,D).
Based onthis observation, we make the following assumption:Assumption 1.
Given an input regex R0 over some al-phabet ?, any other regexR over ?
is a candidate for ourlearning algorithm only if L(R) ?
L(R0 ).
(L(R) denotesthe language accepted by R).Even with this assumption, we are left with a po-tentially infinite set of candidate regexes from whichour learning algorithm must choose one.
To explorethis set in a principled fashion, we need a mecha-nism to move from one element in this space to an-other, i.e., from one candidate regex to another.
Inaddition, we need an objective function to judge theextraction quality of each candidate regex.
We ad-dress these two issues below.Regex Transformations To systematically ex-plore the search space, we introduce the concept ofregex transformations.Definition 1 (Regex Transformation).
LetR?
denotethe set of all regular expressions over some alphabet ?.
Aregex transformation is a function T : R?
?
2R?
suchthat ?R?
?
T (R), L(R?)
?
L(R).For example, by replacing different occurrencesof the quantifier + in R1 from Example 1 withspecific ranges (such as {1,2} or {3}), we obtainexpressions such as R3 = (\d+\-){1,2}\d+ andR4 = (\d{3}\-)+\d+.
The operation of replacingquantifiers with restricted ranges is an example of aparticular class of transformations that we describefurther in Section 3.
For the present, it is sufficientto view a transformation as a function applied to aregexR that produces, as output, a set of regexes thataccept sublanguages of L(R).
We now define thesearch space of our learning algorithm as follows:Definition 2 (Search Space).
Given an input regex R0and a set of transformations T , the search space of ourlearning algorithm is T (R0 ), the set of all regexes ob-tained by (repeatedly) applying the transformations in Tto R0 .For instance, if the operation of restricting quanti-fiers that we described above is part of the transfor-mation set, then R3 and R4 are in the search spaceof our algorithm, given R1 as input.Objective Function We now define an objectivefunction, based on the well known F-measure, tocompare the extraction quality of different candidateregexes in our search space.
Using Mp(R,D) (resp.Mn(R,D)) to denote the set of positive (resp.
nega-tive) matches of a regex R, we defineprecision(R,D) =Mp(R,D)Mp(R,D) + Mn(R,D)recall(R,D) =Mp(R,D)Mp(R0,D)F(R,D) =2 ?
precision(R,D) ?
recall(R,D)precision(R,D) + recall(R,D)The regex learning task addressed in this papercan now be formally stated as the following opti-mization problem:Definition 3 (Regex Learning Problem).
Givenan input regex R0 , a document collection D, labeledsets of positive and negative examples Mp(R0 ,D) andMn(R0 ,D), and a set of transformations T , compute theoutput regex Rf = argmaxR?T (R0 ) F(R,D).3 Instantiating Regex TransformationsIn this section, we describe how transformationscan be implemented by exploiting the syntactic con-structs of modern regex engines.
To help with ourdescription, we introduce the following task:Example 2 (Software name extraction).
Consider thetask of identifying names of software products in text.A simple pattern for this task is: ?one or more capital-ized words followed by a version number?, representedas R5 = ([A-Z]\w*\s*)+[Vv]?(\d+\.?
)+.23When applied to a collection of University webpages, we discovered that R5 identified correct in-stances such as Netscape 2.0, Windows 2000 andInstallation Designer v1.1.
However, R5 also ex-tracted incorrect instances such as course numbers(e.g.
ENGLISH 317), room numbers (e.g.
Room330), and section headings (e.g.
Chapter 2.2).
Toeliminate spurious matches such as ENGLISH 317,let us enforce the condition that ?each word is asingle upper-case letter followed by one or morelower-case letters?.
To accomplish this, we focuson the sub-expression of R5 that identifies capital-ized words, R51 = ([A-Z]\w*\s*)+, and replace itwith R51a = ([A-Z][a-z]*\s*)+.
The regex result-ing from R5 by replacing R51 with R51a will avoidmatches such as ENGLISH 317.An alternate way to improve R5 is by explicitlydisallowing matches against strings like ENGLISH,Room and Chapter.
To accomplish this, we canexploit the negative lookahead operator supportedin modern regex engines.
Lookaheads are specialconstructs that allow a sequence of charactersto be checked for matches against a regex with-out the characters themselves being part of thematch.
As an example, (?
!Ra)Rb (??!?
beingthe negative lookahead operator) returns matchesof regex Rb but only if they do not match Ra.Thus, by replacing R51 in our original regex withR51b =(?!
ENGLISH|Room|Chapter)[A-Z]\w*\s*,we produce an improved regex for software names.The above examples illustrate the general prin-ciple of our transformation technique.
In essence,we isolate a sub-expression of a given regex R andmodify it such that the resulting regex accepts a sub-language of R. We consider two kinds of modifica-tions ?
drop-disjunct and include-intersect.
In drop-disjunct, we operate on a sub-expression that corre-sponds to a disjunct and drop one or more operandsof that disjunct.
In include-intersect, we restrict thechosen sub-expression by intersecting it with someother regex.
Formally,Definition 4 (Drop-disjunct Transformation).
LetR ?
R?
be a regex of the form R = Ra?
(X)Rb,where ?
(X) denotes the disjunction R1|R2| .
.
.
|Rn ofany non-empty set of regexes X = {R1, R2, .
.
.
, Rn}.The drop-disjunct transformation DD(R,X, Y ) for someY ?
X, Y 6= ?
results in the new regex Ra?
(Y )Rb.Definition 5 (Include-Intersect Transformation).
Let.\W \s \w[a-zA-Z] \d|[0-9] _[a-z] [A-Z]Figure 1: Sample Character Classes in RegexR ?
R?
be a regex of the form R = RaXRb for someX ?
R?, X 6= ?.
The include-intersect transformationII(R,X, Y ) for some Y ?
R?, Y 6= ?
results in the newregex Ra(X ?
Y )Rb.We state the following proposition (proof omit-ted in the interest of space) that guarantees that bothdrop-disjunct and include-intersect restrict the lan-guage of the resulting regex, and therefore are validtransformations according to Definition 1.Proposition 1.
Given regexes R,X1, Y1, X2 and Y2from R?
such that DD(R,X1, Y1) and II(R,X2, Y2)are applicable, L(DD(R,X1, Y1)) ?
L(R) andL(II(R,X2, Y2)) ?
L(R).We now proceed to describe how we use differ-ent syntactic constructs to apply drop-disjunct andinclude-intersect transformations.Character Class Restrictions Characterclasses are short-hand notations for denotingthe disjunction of a set of characters (\d isequivalent to (0|1...|9); \w is equivalent to(a|.
.
.|z|A|.
.
.|Z|0|1.
.
.|9| ); etc.
).2 Figure 1illustrates a character class hierarchy in whicheach node is a stricter class than its parent (e.g.,\d is stricter than \w).
A replacement of any ofthese character classes by one of its descendantsis an instance of the drop-disjunct transformation.Notice that in Example 2, when replacing R51 withR51a , we were in effect applying a character classrestriction.Quantifier Restrictions Quantifiers are used todefine the range of valid counts of a repetitive se-quence.
For instance, a{m,n} looks for a sequenceof a?s of length at least m and at most n. Sincequantifiers are also disjuncts (e.g., a{1,3} is equiv-alent to a|aa|aaa), the replacement of an expres-sion R{m,n} with an expression R{m1, n1} (m ?m1 ?
n1 ?
n) is an instance of the drop-disjuncttransformation.
For example, given a subexpres-sion of the form a{1,3}, we can replace it with2Note that there are two distinct character classes \W and \w24one of a{1,1}, a{1,2}, a{2,2}, a{2,3}, or a{3,3}.Note that, before applying this transformation, wild-card expressions such as a+ and a* are replaced bya{0,maxCount} and a{1,maxCount} respectively,where maxCount is a user configured maximumlength for the entity being extracted.Negative Dictionaries Observe that the include-intersect transformation (Definition 5) is applicablefor every possible sub-expression of a given regexR.
Note that a valid sub-expression in R is anyportion of R where a capturing group can be intro-duced.3 Consider a regex R = RaXRb with a sub-expression X; the application of include-intersectrequires another regex Y to yieldRa(X?Y )Rb.
Wewould like to construct Y such thatRa(X ?Y )Rb is?better?
than R for the task at hand.
Therefore, weconstruct Y as ?Y ?
where Y ?
is a regex constructedfrom negative matches ofR.
Specifically, we look ateach negative match of R and identify the substringof the match that corresponds to X .
We then applya greedy heuristic (see below) to these substrings toyield a negative dictionary Y ?.
Finally, the trans-formed regexRa(X?
?Y ?
)Rb is implemented usingthe negative lookahead expression Ra(?!
Y?
)XRb.Greedy Heuristic for Negative Dictionaries Im-plementation of the above procedure requires cer-tain judicious choices in the construction of the neg-ative dictionary to ensure tractability of this trans-formation.
Let S(X) denote the distinct stringsthat correspond to the sub-expression X in the neg-ative matches of R.4 Since any subset of S(X)is a candidate negative dictionary, we are left withan exponential number of possible transformations.In our implementation, we used a greedy heuris-tic to pick a single negative dictionary consistingof all those elements of S(X) that individuallyimprove the F-measure.
For instance, in Exam-ple 2, if the independent substitution of R51 with(?
!ENGLISH)[A-Z]\w*\s*, (?
!Room)[A-Z]\w*\s*, and (?
!Chapter)[A-Z]\w*\s* each im-proves the F-measure, we produce a nega-tive dictionary consisting of ENGLISH, Room, andChapter.
This is precisely how the disjunctENGLISH|Room|Chapter is constructed in R51b .3For instance, the sub-expressions of ab{1,2}c are a,ab{1,2}, ab{1,2}c, b, b{1,2}, b{1,2}c, and c.4S(X) can be obtained automatically by identifying the sub-string corresponding to the group X in each entry in Mn(R,D)Procedure ReLIE(Mtr ,Mval,R0 ,T )//Mtr : set of labeled matches used as training data//Mval: set of labeled matches used as validation data// R0 : user-provided regular expression// T : set of transformationsbegin1.
Rnew = R02.
do {3. for each transformation ti ?
T4.
Candidatei=ApplyTransformations(Rnew, ti)5. let Candidates =?i Candidatei6.
let R?
= argmaxR?Candidates F(R,Mtr)7. if (F(R?,Mtr) <= F(Rnew,Mtr)) return Rnew8.
if (F(R?,Mval) < F(Rnew,Mval)) return Rnew9.
Rnew = R?10. }
while(true)endFigure 2: ReLIE Search Algorithm4 ReLIE Search AlgorithmFigure 2 describes the ReLIE algorithm for theRegex Learning Problem (Definition 3) based on thetransformations described in Section 3.
ReLIE is agreedy hill climbing search procedure that chooses,at every iteration, the regex with the highest F-measure.
An iteration in ReLIE consists of:?
Applying every transformation on the current regexRnew to obtain a set of candidate regexes?
From the candidates, choosing the regex R?
whoseF-measure over the training dataset is maximumTo avoid overfitting, ReLIE terminates when eitherof the following conditions is true: (i) there is noimprovement in F-measure over the training set;(ii) there is a drop in F-measure when applying R?on the validation set.The following proposition provides an upperbound for the running time of the ReLIE algorithm.Proposition 2.
Given any valid set of inputs Mtr,Mval, R0 , and T , the ReLIE algorithm terminates in atmost |Mn(R0 ,Mtr )| iterations.
The running time of thealgorithm TTotal(R0 ,Mtr ,Mval) ?
|Mn(R0 ,Mtr )| ?t0 , where t0 is the time taken for the first iteration of thealgorithm.Proof.
With reference to Figure 2, in each iteration, theF-measure of the ?best?
regex R?
is strictly better thanRnew.
Since L(R?)
?
L(Rnew), R?
eliminates at leastone additional negative match compared toRnew.
Hence,the maximum number of iterations is |Mn(R0 ,Mtr )|.For a regular expression R, let ncc(R) and nq(R) de-note, respectively, the number of character classes andquantifiers in R. The maximum number of possible sub-expressions in R is |R|2, where |R| is the length of R.Let MaxQ(R) denote the maximum number of ways in25which a single quantifier appearing in R can be restrictedto a smaller range.
Let Fcc denote the maximum fanout5of the character class hierarchy.
Let TReEval(D) denotethe average time taken to evaluate a regex over datasetD.Let Ri denote the regex at the beginning of iterationi.
The number of candidate regexes obtained by applyingthe three transformations isNumRE(Ri,Mtr) ?
ncc(Ri)?Fcc+nq(Ri)?MaxQ(Ri)+|Ri|2The time taken to enumerate the character class andquantifier restriction transformations is proportional tothe resulting number of candidate regexes.
The timetaken for the negative dictionaries transformation is givenby the running time of the greedy heuristic (Section 3).The total time taken to enumerate all candidate regexes isgiven by (for some constant c)TEnum(Ri,Mtr) ?
c ?
(ncc(Ri) ?
Fcc + nq(Ri) ?MaxQ(Ri)+ |Ri|2 ?Mn(Ri,Mtr) ?
TReEval(Mtr))Choosing the best transformation involves evaluatingeach candidate regex over the training and validation cor-pus and the time taken for this step isTPickBest(Ri,Mtr,Mval) = NumRE(Ri,Mtr)?
(TReEval(Mtr) + TReEval(Mval))The total time taken for an iteration can be written asTI(Ri,Mtr,Mval) =TEnum(Ri,Mtr)+ TPickBest(Ri,Mtr,Mval)It can be shown that the time taken in each iterationdecreases monotonically (details omitted in the interest ofspace).
Therefore, the total running time of the algorithmis given byTTotal(R0 ,Mtr ,Mval) =?TI(Ri,Mtr,Mval)?
|Mn(R0 ,Mtr )| ?
t0 .where t0 = TI(R0 ,Mtr ,Mval) is the running timeof the first iteration of the algorithm.5 ExperimentsIn this section, we present an empirical study ofthe ReLIE algorithm using four extraction tasks overthree real-life data sets.
The goal of this study is toevaluate the effectiveness of ReLIE in learning com-plex regexes and to investigate how it compares withstandard machine learning algorithms.5.1 Experimental SetupData Set The datasets used in our experiments are:?
EWeb: A collection of 50,000 web pages crawledfrom a corporate intranet.5Fanout is the number of ways in which a character classmay be restricted as defined by the hierarchy (e.g.
Figure 1).?
AWeb: A set of 50,000 web pages obtained fromthe publicly available University of Michigan Webpage collection (Li et al, 2006), including a sub-collection of 10,000 pages (AWeb-S).?
Email: A collection of 10,000 emails obtainedfrom the publicly available Enron email collec-tion (Minkov et al, 2005).Extraction Tasks SoftwareNameTask, CourseNum-berTask and PhoneNumberTask were evaluated onEWeb, AWeb and Email, respectively.
Since webpages have large number of URLs, to keep the la-beling task manageable, URLTask was evaluated onAWeb-S.Gold Standard For each task, the gold standardwas created by manually labeling all matches for theinitial regex.
Note that only exact matches with thegold standard are considered correct in our evalua-tions.
6Comparison Study To evaluate ReLIE for entityextraction vis-a-vis existing algorithms, we used thepopular conditional random field (CRF).
Specifi-cally, we used the MinorThird (Cohen, 2004) imple-mentation of CRF to train models for all four extrac-tion tasks.
For training the CRF we provided it withthe set of positive and negative matches from the ini-tial regex with a context of 200 characters on eitherside of each match7.
Since it is unlikely that usefulfeatures are located far away from the entity, we be-lieve that 200 characters on either side is sufficientcontext.
The CRF used the base features describedin (Cohen et al, 2005).
To ensure fair compari-son with ReLIE, we also included the matches corre-sponding to the input regex as a feature to the CRF.In practice, more complex features (e.g., dictionar-ies, simple regexes) derived by domain experts areoften provided to CRFs.
However, such features canalso be used to refine the initial regex given to ReLIE.Hence, with a view to investigating the ?raw?
learn-ing capability of the two approaches, we chose torun all our experiments without any additional man-ually derived features.
In fact, the patterns learnedby ReLIE through transformations are often similar6The labeled data will be made publicly available athttp://www.eecs.umich.edu/db/regexLearning/.7Ideally, we would have preferred to let MinorThird extractappropriate features from complete documents in the training-set but could not get it to load our large datasets.26(a) SoftwareNameTask0.50.60.70.80.9110% 40% 80%Percentage of Data Used for TrainingF-MeasureReLIE CRF(b) CourseNumberTask0.50.60.70.80.9110% 40% 80%Percentage of Data Used for TrainingF-MeasureReLIE CRF(c) URLTask0.50.60.70.80.9110% 40% 80%Percentage of Data Used for TrainingF-MeasureReLIE CRF(d) PhoneNumberTask0.50.60.70.80.9110% 40% 80%Percentage of Data Used for TrainingF-MeasureReLIE CRFFigure 3: Extraction QualityaaFor SoftwareNameTask, with 80% training data we could not obtain results for CRF as the programfailed repeatedly during the training phase.to the features that domain experts may provide toCRF.
We will revisit this issue in Section 5.4.Evaluation We used the standard F-measure toevaluate the effectiveness of ReLIE and CRF.
We di-vided each dataset into 10 equal parts and used X%of the dataset for training (X=10, 40 and 80), 10%for validation, and remaining (90-X)% for testing.All results are reported on the test set.5.2 ResultsFour extraction tasks were chosen to reflect the enti-ties commonly present in the three datasets.?
SoftwareNameTask: Extracting software names suchas Lotus Notes 8.0, Open Office Suite 2007.?
CourseNumberTask: Extracting university coursenumbers such as EECS 584, Pharm 101.?
PhoneNumberTask: Extracting phone numbers suchas 1-800-COMCAST, (425)123 5678.?
URLTask: Extracting URLs such ashttp:\\www.abc.com and lsa.umich.edu/ foo/.8This section summarizes the results of our empir-ical evaluation comparing ReLIE and CRF.8URLTask may appear to be simplistic.
However, extractingURLs without the leading protocol definitions (e.g.
http) canbe challenging.Raw Extraction Quality The cross-validated re-sults across all four tasks are presented in Figure 3.?
With 10% training data, ReLIE outperforms CRFon three out of four tasks with a difference in F-measure ranging from 0.1 to 0.2.?
As training data increases, both algorithms performbetter with the gap between the two reducing forall the four tasks.
For CourseNumberTask and URL-Task, CRF does slightly better than ReLIE for largertraining dataset.
For the other two tasks, ReLIE re-tains its advantage over CRF.9The above results indicate that ReLIE performscomparably with CRF with a slight edge in condi-tions of limited training data.
Indeed, the capabilityto learn high-quality extractors using a small train-ing set is important because labeled data is often ex-pensive to obtain.
For precisely this same reason, wewould ideally like to learn the extractors once andthen apply them to other datasets as needed.
Sincethese other datasets may be from a different domain,we next performed a cross-domain test (i.e., training9For SoftwareNameTask, with 80% training data we couldnot obtain results for CRF as the program failed repeatedly dur-ing the training phase.27and testing on different domains).Task(Training, Testing)Data for Training 10% 40% 80%ReLIE CRF ReLIE CRF ReLIE CRFSoftwareNameTask(EWeb,AWeb) 0.920 0.297 0.977 0.503 0.971 N/AURLTask(AWeb-S,Email) 0.690 0.209 0.784 0.380 0.801 0.507PhoneNumberTask(Email,AWeb) 0.357 0.130 0.475 0.125 0.513 0.120Table 1: Cross Domain Test (F-measure).TechniqueSoftwareNameTask CourseNumberTask URLTask PhoneNumberTasktraining testing training testing training testing training testingReLIE 511.7 20.6 69.3 18.4 73.8 7.7 39.4 1.1CRF 7597.0 2315.8 482.5 75.4 438.7 53.8 434.8 57.7t(ReLIE)t(CRF) 0.067 0.009 0.144 0.244 0.168 0.143 0.091 0.019Table 2: Average Training/Testing Time (sec)(with 40% data for training)Task(Extra Feature)Data for Training 10% 40% 80%CRF C+RL CRF C+RL CRF C+RLCourseNumberTask(Negative Dictionary) 0.553 0.624 0.644 0.764 0.854 0.845PhoneNumberTask(Quantifier) 0.695 0.893 0.820 0.937 0.821 0.964Table 3: ReLIE as Feature Extractor (C+RL is CRF enhanced withfeatures learned by ReLIE).Cross-domain Evaluation Table 1summarizes the results of trainingthe algorithms on one data set andtesting on another.
The scenarioschosen are: (i) SoftwareNameTasktrained on EWeb and tested onAWeb, (ii) URLTask trained on AWeband tested on Email, and (iii) Pho-neNumberTask trained on Emailand tested on AWeb.10 We cansee that ReLIE significantly out-performs CRF for all three tasks,even when provided with a largetraining dataset.
Compared to test-ing on the same dataset, there is areduction in F-measure (less than0.1 in many cases) when the regexlearned by ReLIE is applied to a dif-ferent dataset, while the drop forCRF is much more significant (over 0.5 in manycases).11Training Time Another issue of practical consid-eration is the efficiency of the learning algorithm.Table 2 reports the average training and testing timefor both algorithms on the four tasks.
On average Re-LIE is an order of magnitude faster than CRF in bothbuilding the model and applying the learnt model.Robustness to Variations in Input Regexes Thetransformations done by ReLIE are based on thestructure of the input regex.
Therefore given differ-ent input regexes, the final regexes learned by ReLIEwill be different.
To evaluate the impact of the struc-ture of the input regex on the quality of the regexlearned by ReLIE, we started with different regexes12for the same task.
We found that ReLIE is robustto variations in input regexes.
For instance, on Soft-wareNameTask, the standard deviation in F-measure10We do not report results for CourseNumberTask as coursenumbers are specific to academic webpages and do not appearin the other two domains11Similar cross-domain performance deterioration for a ma-chine learning approach has been observed by (Guo et al,2006).12Recall that the search space of ReLIE is limited by L(R0)(Assumption 1).
Thus to ensure meaningful comparison, forthe same task any two given input regexes R0 and R?0 are cho-sen in such a way that although their structures are different,Mp(R0,D) = Mp(R?0,D) and Mn(R0,D) = Mn(R?0,D).of the final regexes generated from six different in-put regexes was less than 0.05.
Further details of thisexperiment are omitted in the interest of space.5.3 DiscussionThe results of our comparison study (Figure 3) in-dicates that for raw extraction quality ReLIE has aslight edge over CRF for small training data.
How-ever, in cross-domain performance (Table 1) ReLIEis significantly better than CRF (by 0.41 on aver-age) .
To understand this discrepancy, we examinedthe final regex learned by ReLIE and compared thatwith the features learned by CRF.
Examples of ini-tial regexes with corresponding final regexes learntby ReLIE with 10% training data are listed in Ta-ble 4.
Recall, from Section 3, that ReLIE transfor-mations include character class restrictions, quanti-fier restrictions and addition of negative dictionar-ies.
For instance, in the SoftwareNameTask, the finalregex listed was obtained by restricting [a-zA-Z]to [a-z], \w to [a-zA-Z], and adding the nega-tive dictionary (Copyright|Fall| ?
?
?
|Issue).
Sim-ilarly, for the PhoneNumberTask, the final regexinvolved two negative dictionaries (expressed as(?!
[,]) and (?!
[,:])) 13 and quantifier restric-tions (e.g.
the first [A-Z\d]{2,4} was transformed13To obtain these negative dictionaries, ReLIE not onlyneeds to correctly identify the dictionary entries from negativematches but also has to place the corresponding negative looka-head expression at the appropriate place in the regex.28SoftwareNameTaskR0 \b([A-Z][a-zA-Z]{1,10}\s){1,5}\s*(\w{0,2}\d[\.]?){1,4}\bRfinal\b((?!
(Copyright|Page|Physics|Question| ?
?
?
|Article|Issue))[A-Z][a-z]{1,10}\s){1,5}\s*([a-zA-Z]{0,2}\d[\.]?
){1,4}\bPhoneNumberTask R0 \b(1\W+)?\W?\d{3,3}\W*\s*\W?[A-Z\d]{2,4}\s*\W?
[A-Z\d]{2,4}\bRfinal \b(1\W+)?\W?\d{3,3}((?![,])\W*)\s*\W?[A-Z\d]{3,3}\s*((?![,:])\W?
)[A-Z\d]{3,4}\bCourseNumberTask R0 \b([A-Z][a-zA-Z]+)\s+\d{3,3}\bRfinal \b(((?!
(At|Between| ?
?
?Contact|Some|Suite|Volume))[A-Z][a-zA-Z]+))\s+\d{3,3}\bURLTask R0 \b(\w+://)?(\w+\.){0,2}\w+\.\w+(/[?\s]+){0,20}\bRfinal\b((?!
(Response 20010702 1607.csv| ?
?
?))((\w+://)?(\w+\.){0,2}\w+\.(?!
(ppt| ?
?
?doc))[a-zA-Z]{2,3}))(/[?\s]+){0,20}\bTable 4: Sample Regular Expressions Learned by ReLIE(R0: input regex; Rfinal: final regex learned; the parts of R0modified by ReLIE and the corresponding parts in Rfinal are highlighted.
)into [A-Z\d]{3,3}).After examining the features learnt by CRF, it wasclear that while CRF could learn features such as thenegative dictionary it is unable to learn character-level features.
This should not be surprising sinceour CRF was trained with primarily tokens as fea-tures (cf.
Section 5.1).
While this limitation was lessof a factor in experiments involving data from thesame domain (some effects were seen with smallertraining data), it does explain the significant differ-ence between the two algorithms in cross-domaintasks where the vocabulary can be significantly dif-ferent.
Indeed, in practical usage of CRF, the mainchallenge is to come up with additional complex fea-tures (often in the form of dictionary and regex pat-terns) that need to be given to the CRF (Minkov etal., 2005).
Such complex features are largely hand-crafted and thus expensive to obtain.
Since the Re-LIE transformations are operations over characters,a natural question to ask is: ?Can the regex learnedby ReLIE be used to provide features to CRF??
Weanswer this question below.5.4 ReLIE as Feature Extractor for CRFTo understand the effect of incorporating ReLIE-identified features into CRF, we chose the two tasks(CourseNumberTask and PhoneNumberTask) with theleast F-measure in our experiments to determine rawextraction quality.
We examined the final regex pro-duced by ReLIE and manually extracted portionsto serve as features.
For example, the negativedictionary learned by ReLIE for the CourseNumber-Task (At|Between| ?
?
?
|Volume) was incorporated asa feature into CRF.
To help isolate the effects, foreach task, we only incorporated features correspond-ing to a single transformation: negative dictionar-ies for CourseNumberTask and quantifier restrictionsfor PhoneNumberTask.
The results of these experi-ments are shown in Table 3.
The first point worthy ofnote is that performance has improved in all but onecase.
Second, despite the F-measure on CourseNum-berTask being lower than PhoneNumberTask (presum-ably more potential for improvement), the improve-ments on PhoneNumberTask are significantly higher.This observation is consistent with our conjecturein Section 5.1 that CRF learns token-level features;therefore incorporating negative dictionaries as extrafeature provides only limited improvement.
Admit-tedly more experiments are needed to understand thefull impact of incorporating ReLIE-identified fea-tures into CRF.
However, we do believe that this isan exciting direction of future research.6 Summary and Future WorkWe proposed a novel formulation of the problem oflearning complex character-level regexes for entityextraction tasks.
We introduced the concept of regextransformations and described how these could berealized using the syntactic constructs of modernregex languages.
We presented ReLIE, a powerfulregex learning algorithm that exploits these ideas.Our experiments demonstrate that ReLIE is very ef-fective for certain classes of entity extraction, partic-ularly under conditions of cross-domain and limitedtraining data.
Our preliminary results also indicatethe possibility of using ReLIE as a powerful featureextractor for CRF and other machine learning algo-rithms.
Further investigation of this aspect of ReLIEpresents an interesting avenue of future work.AcknowledgmentsWe thank the anonymous reviewers for their insight-ful and constructive comments and suggestions.
Weare also grateful for comments from David Gondekand Sebastian Blohm.29ReferencesR.
Alquezar and A. Sanfeliu.
1994.
Incremental gram-matical inference from positive and negative data usingunbiased finite state automata.
In SSPR.Douglas E. Appelt and Boyan Onyshkevych.
1998.
Thecommon pattern specification language.
In TIPSTERTEXT PROGRAM.Geert Jan Bex et al 2006.
Inference of concise DTDsfrom XML data.
In VLDB.Eric Brill.
2000.
Pattern-based disambiguation for natu-ral language processing.
In SIGDAT.William W. Cohen and Andrew McCallum.
2003.
Infor-mation Extraction from the World Wide Web.
in KDDWilliam W. Cohen.
2004.
Minorthird: Methods foridentifying names and ontological relations in textusing heuristics for inducing regularities from data.http://minorthird.sourceforge.net.William W. Cohen et al 2005.
Learning to UnderstandWeb Site Update Requests.
In IJCAI.Fabio Ciravegna.
2001.
Adaptive information extractionfrom text by rule induction and generalization.
In IJ-CAI.H.
Cunningham.
1999.
JAPE ?
a java annotation patternsengine.Francois Denis et al 2004.
Learning regular languagesusing RFSAs.
Theor.
Comput.
Sci., 313(2):267?294.Francois Denis.
2001.
Learning regular languagesfrom simple positive examples.
Machine Learning,44(1/2):37?66.Pedro DeRose et al 2007.
DBLife: A Community In-formation Management Platform for the Database Re-search Community In CIDRPierre Dupont.
1996.
Incremental regular inference.
InICGI.Ronen Feldman et al.
2006.
Self-supervised RelationExtraction from the Web.
In ISMIS.Henning Fernau.
2005.
Algorithms for learning regularexpressions.
In ALT.Laura Firoiu et al 1998.
Learning regular languagesfrom positive evidence.
In CogSci.K.
Fukuda et al 1998.
Toward information extraction:identifying protein names from biological papers.
PacSymp Biocomput., 1998:707?718Ugo Galassi and Attilio Giordana.
2005.
Learning regu-lar expressions from noisy sequences.
In SARA.Minos Garofalakis et al 2000.
XTRACT: a system forextracting document type descriptors from XML doc-uments.
In SIGMOD.Hong Lei Guo et al 2006.
Empirical Study on thePerformance Stability of Named Entity RecognitionModel across Domains In EMNLP.Java Regular Expressions.
2008. http://java.sun.com/javase/6/docs/api/java/util/regex/package-summary.html.Dan Klein et al 2003.
Named Entity Recognition withCharacter-Level Models.
In HLT-NAACL.Vijay Krishnan and Christopher D. Manning.
2006.
AnEffective Two-Stage Model for Exploiting Non-LocalDependencies in Named Entity Recognition.
In ACL.Yunyao Li et al 2006.
Getting work done on the web:Supporting transactional queries.
In SIGIR.Andrew McCallum et al 2000.
Maximum EntropyMarkov Models for Information Extraction and Seg-mentation.
In ICML.Einat Minkov et al 2005.
Extracting personal namesfrom emails: Applying named entity recognition to in-formal text.
In HLT/EMNLP.Stephen Soderland.
1999.
Learning information extrac-tion rules for semi-structured and free text.
MachineLearning, 34:233?272.Lorraine Tanabe and W. John Wilbur 2002.
Tagginggene and protein names in biomedical text.
Bioinfor-matics, 18:1124?1132.Tianhao Wu and William M. Pottenger.
2005.
A semi-supervised active learning algorithm for informationextraction from textual data.
JASIST, 56(3):258?271.Huaiyu Zhu, Alexander Loeser, Sriram Raghavan, Shiv-akumar Vaithyanathan 2007.
Navigating the intranetwith high precision.
In WWW.30
