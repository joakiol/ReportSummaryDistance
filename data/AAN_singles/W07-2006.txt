Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 30?35,Prague, June 2007. c?2007 Association for Computational LinguisticsSemEval-2007 Task 07: Coarse-Grained English All-Words TaskRoberto NavigliUniversita` di Roma ?La Sapienza?Dipartimento di InformaticaVia Salaria, 00198 - Roma Italynavigli@di.uniroma1.itKenneth C. LitkowskiCL Research9208 Gue RoadDamascus MD 20872ken@clres.comOrin HargravesLexicographerorinhargraves@googlemail.comAbstractThis paper presents the coarse-grained En-glish all-words task at SemEval-2007.
Wedescribe our experience in producing acoarse version of the WordNet sense inven-tory and preparing the sense-tagged corpusfor the task.
We present the results of par-ticipating systems and discuss future direc-tions.1 IntroductionIt is commonly thought that one of the major obsta-cles to high-performance Word Sense Disambigua-tion (WSD) is the fine granularity of sense inven-tories.
State-of-the-art systems attained a disam-biguation accuracy around 65% in the Senseval-3all-words task (Snyder and Palmer, 2004), whereWordNet (Fellbaum, 1998) was adopted as a ref-erence sense inventory.
Unfortunately, WordNet isa fine-grained resource, encoding sense distinctionsthat are difficult to recognize even for human an-notators (Edmonds and Kilgarriff, 2002).
MakingWSD an enabling technique for end-to-end applica-tions clearly depends on the ability to deal with rea-sonable sense distinctions.The aim of this task was to explicitly tackle thegranularity issue and study the performance of WSDsystems on an all-words basis when a coarser setof senses is provided for the target words.
Giventhe need of the NLP community to work on freelyavailable resources, the solution of adopting a dif-ferent computational lexicon is not viable.
On theother hand, the production of a coarse-grained senseinventory is not a simple task.
The main issueis certainly the subjectivity of sense clusters.
Toovercome this problem, different strategies can beadopted.
For instance, in the OntoNotes project(Hovy et al, 2006) senses are grouped until a 90%inter-annotator agreement is achieved.
In contrast,as we describe in this paper, our approach is basedon a mapping to a previously existing inventorywhich encodes sense distinctions at different levelsof granularity, thus allowing to induce a sense clus-tering for the mapped senses.We would like to mention that another SemEval-2007 task dealt with the issue of sense granularityfor WSD, namely Task 17 (subtask #1): Coarse-grained English Lexical Sample WSD.
In this paper,we report our experience in organizing Task 07.2 Task SetupThe task required participating systems to annotateopen-class words (i.e.
nouns, verbs, adjectives, andadverbs) in a test corpus with the most appropriatesense from a coarse-grained version of the WordNetsense inventory.2.1 Test CorpusThe test data set consisted of 5,377 words of run-ning text from five different articles: the first three(in common with Task 17) were obtained from theWSJ corpus, the fourth was the Wikipedia entry forcomputer programming1, the fifth was an excerpt ofAmy Steedman?s Knights of the Art, biographies ofItalian painters2.
We decided to add the last two1http://en.wikipedia.org/wiki/Computer programming2http://www.gutenberg.org/etext/52930article domain words annotatedd001 JOURNALISM 951 368d002 BOOK REVIEW 987 379d003 TRAVEL 1311 500d004 COMPUTER SCIENCE 1326 677d005 BIOGRAPHY 802 345total 5377 2269Table 1: Statistics about the five articles in the testdata set.texts to the initial dataset as we wanted the corpus tohave a size comparable to that of previous editionsof all-words tasks.In Table 1 we report the domain, number of run-ning words, and number of annotated words for thefive articles.
We observe that articles d003 and d004are the largest in the corpus (they constitute 51.87%of it).2.2 Creation of a Coarse-Grained SenseInventoryTo tackle the granularity issue, we produced acoarser-grained version of the WordNet sense inven-tory3 based on the procedure described by Navigli(2006).
The method consists of automatically map-ping WordNet senses to top level, numbered entriesin the Oxford Dictionary of English (ODE, (Soanesand Stevenson, 2003)).
The semantic mapping be-tween WordNet and ODE entries was obtained intwo steps: first, we disambiguated with the SSI algo-rithm (Navigli and Velardi, 2005) the definitions ofthe two dictionaries, together with additional infor-mation (hypernyms and domain labels); second, foreach WordNet sense, we determined the best match-ing ODE coarse entry.
As a result, WordNet sensesmapped to the same ODE entry were assigned to thesame sense cluster.
WordNet senses with no matchwere associated with a singleton sense.In contrast to the automatic method above, thesense mappings for all the words in our test cor-pus were manually produced by the third author, anexpert lexicographer, with the aid of a mapping in-terface.
Not all the words in the corpus could bemapped directly for several reasons: lacking entriesin ODE (e.g.
adjectives underlying and shivering),3We adopted WordNet 2.1, available from:http://wordnet.princeton.edudifferent spellings (e.g.
after-effect vs. aftereffect,halfhearted vs. half-hearted, etc.
), derivatives (e.g.procedural, gambler, etc.).
In most of the cases, weasked the lexicographer to map senses of the orig-inal word to senses of lexically-related words (e.g.WordNet senses of procedural were mapped to ODEsenses of procedure, etc.).
When this mapping wasnot straightforward, we just adopted the WordNetsense inventory for that word.We released the entire sense groupings (those in-duced from the manual mapping for words in thetest set plus those automatically derived on the otherwords) and made them available to the participants.2.3 Sense AnnotationAll open-class words (i.e.
nouns, verbs, adjectives,and adverbs) with an existing sense in the WordNetinventory were manually annotated by the third au-thor.
Multi-word expressions were explicitly iden-tified in the test set and annotated as such (this wasmade to allow a fair comparison among systems in-dependent of their ability to identify multi-word ex-pressions).We excluded auxiliary verbs, uncovered phrasaland idiomatic verbs, exclamatory uses, etc.
Theannotator was allowed to tag words with multiplecoarse senses, but was asked to make a single senseassignment whenever possible.The lexicographer annotated an overall numberof 2,316 content words.
47 (2%) of them were ex-cluded because no WordNet sense was deemed ap-propriate.
The remaining 2,269 content words thusconstituted the test data set.
Only 8 of them were as-signed more than one sense: specifically, two coarsesenses were assigned to a single word instance4 andtwo distinct fine-grained senses were assigned to 7word instances.
This was a clear hint that the senseclusters were not ambiguous for the vast majority ofwords.In Table 2 we report information about the pol-ysemy of the word instances in the test set.
Over-all, 29.88% (678/2269) of the word instances weremonosemous (according to our coarse sense inven-tory).
The average polysemy of the test set with thecoarse-grained sense inventory was 3.06 comparedto an average polysemy with the WordNet inventory4d005.s004.t01531polysemy N V A R allmonosemous 358 86 141 93 678polysemous 750 505 221 115 1591total 1108 591 362 208 2269Table 2: Statistics about the test set polysemy (N =nouns, V = verbs, A = adjectives, R = adverbs).of 6.18.2.4 Inter-Annotator AgreementRecent estimations of the inter-annotator agreementwhen using the WordNet inventory report figures of72.5% agreement in the preparation of the Englishall-words test set at Senseval-3 (Snyder and Palmer,2004) and 67.3% on the Open Mind Word Expert an-notation exercise (Chklovski and Mihalcea, 2002).As the inter-annotator agreement is often consid-ered an upper bound for WSD systems, it was de-sirable to have a much higher number for our task,given its coarse-grained nature.
To this end, besidethe expert lexicographer, a second author indepen-dently performed part of the manual sense mapping(590 word senses) described in Section 2.2.
Thepairwise agreement was 86.44%.We repeated the same agreement evaluation onthe sense annotation task of the test corpus.
A sec-ond author independently annotated part of the testset (710 word instances).
The pairwise agreementbetween the two authors was 93.80%.
This figure,compared to those in the literature for fine-grainedhuman annotations, gives us a clear indication thatthe agreement of human annotators strictly dependson the granularity of the adopted sense inventory.3 BaselinesWe calculated two baselines for the test corpus: arandom baseline, in which senses are chosen atrandom, and the most frequent baseline (MFS), inwhich we assign the first WordNet sense to eachword in the dataset.Formally, the accuracy of the random baselinewas calculated as follows:BLRand = 1|T ||T |?i=11|CoarseSenses(wi)|where T is our test corpus, wi is the i-th wordinstance in T , and CoarseSenses(wi) is the set ofcoarse senses for wi according to the sense cluster-ing we produced as described in Section 2.2.The accuracy of the MFS baseline was calculatedas:BLMFS = 1|T ||T |?i=1?
(wi, 1)where ?
(wi, k) equals 1 when the k-th sense ofword wi belongs to the cluster(s) manually associ-ated by the lexicographer to word wi (0 otherwise).Notice that our calculation of the MFS is based onthe frequencies in the SemCor corpus (Miller et al,1993), as we exploit WordNet sense rankings.4 Results12 teams submitted 14 systems overall (plus twosystems from a 13th withdrawn team that we willnot report).
According to the SemEval policy fortask organizers, we remark that the system labelledas UOR-SSI was submitted by the first author (thesystem is based on the Structural Semantic Inter-connections algorithm (Navigli and Velardi, 2005)with a lexical knowledge base composed by Word-Net and approximately 70,000 relatedness edges).Even though we did not specifically enrich the al-gorithm?s knowledge base on the task at hand, welist the system separately from the overall ranking.The results are shown in Table 3.
We calcu-lated a MFS baseline of 78.89% and a random base-line of 52.43%.
In Table 4 we report the F1 mea-sures for all systems where we used the MFS as abackoff strategy when no sense assignment was at-tempted (this possibly reranked 6 systems - markedin bold in the table - which did not assign a senseto all word instances in the test set).
Comparedto previous results on fine-grained evaluation exer-cises (Edmonds and Kilgarriff, 2002; Snyder andPalmer, 2004), the systems?
results are much higher.On the other hand, the difference in performancebetween the MFS baseline and state-of-the-art sys-tems (around 5%) on coarse-grained disambiguationis comparable to that of the Senseval-3 all-words ex-ercise.
However, given the novelty of the task webelieve that systems can achieve even better perfor-32System A P R F1NUS-PT 100.0 82.50 82.50 82.50NUS-ML 100.0 81.58 81.58 81.58LCC-WSD 100.0 81.45 81.45 81.45GPLSI 100.0 79.55 79.55 79.55BLMFS 100.0 78.89 78.89 78.89UPV-WSD 100.0 78.63 78.63 78.63TKB-UO 100.0 70.21 70.21 70.21PU-BCD 90.1 69.72 62.80 66.08RACAI-SYNWSD 100.0 65.71 65.71 65.71SUSSX-FR 72.8 71.73 52.23 60.44USYD 95.3 58.79 56.02 57.37UOFL 92.7 52.59 48.74 50.60SUSSX-C-WD 72.8 54.54 39.71 45.96SUSSX-CR 72.8 54.30 39.53 45.75UOR-SSI?
100.0 83.21 83.21 83.21Table 3: System scores sorted by F1 measure (A =attempted, P = precision, R = recall, F1 = F1 mea-sure, ?
: system from one of the task organizers).mance by heavily exploiting the coarse nature of thesense inventory.In Table 5 we report the results for each of thefive articles.
The interesting aspect of the table isthat documents from some domains seem to havepredominant senses different from those in Sem-Cor.
Specifically, the MFS baseline performs morepoorly on documents d004 and d005, from theCOMPUTER SCIENCE and BIOGRAPHY domainsrespectively.
We believe this is due to the fact thatthese documents have specific predominant senses,which correspond less often to the most frequentsense in SemCor than for the other three documents.It is also interesting to observe that different systemsperform differently on the five documents (we high-light in bold the best performing systems on eacharticle).Finally, we calculated the systems?
performanceby part of speech.
The results are shown in Table6.
Again, we note that different systems show dif-ferent performance depending on the part-of-speechtag.
Another interesting aspect is that the perfor-mance of the MFS baseline is very close to state-of-the-art systems for adjectives and adverbs, whereasit is more than 3 points below for verbs, and around5 for nouns.System F1NUS-PT 82.50NUS-ML 81.58LCC-WSD 81.45GPLSI 79.55BLMFS 78.89UPV-WSD 78.63SUSSX-FR 77.04TKB-UO 70.21PU-BCD 69.72RACAI-SYNWSD 65.71SUSSX-C-WD 64.52SUSSX-CR 64.35USYD 58.79UOFL 54.61UOR-SSI?
83.21Table 4: System scores sorted by F1 measure withMFS adopted as a backoff strategy when no senseassignment is attempted (?
: system from one of thetask organizers).
Systems affected are marked inbold.System N V A RNUS-PT 82.31 78.51 85.64 89.42NUS-ML 81.41 78.17 82.60 90.38LCC-WSD 80.69 78.17 85.36 87.98GPLSI 80.05 74.45 82.32 86.54BLMFS 77.44 75.30 84.25 87.50UPV-WSD 79.33 72.76 84.53 81.25TKB-UO 70.76 62.61 78.73 74.04PU-BCD 71.41 59.69 66.57 55.67RACAI-SYNWSD 64.02 62.10 71.55 75.00SUSSX-FR 68.09 51.02 57.38 49.38USYD 56.06 60.43 58.00 54.31UOFL 57.65 48.82 25.87 60.80SUSSX-C-WD 52.18 35.64 42.95 46.30SUSSX-CR 51.87 35.44 42.95 46.30UOR-SSI?
84.12 78.34 85.36 88.46Table 6: System scores by part-of-speech tag (N= nouns, V = verbs, A = adjectives, R = adverbs)sorted by overall F1 measure (best scores are markedin bold, ?
: system from one of the task organizers).33d001 d002 d003 d004 d005System P R P R P R P R P RNUS-PT 88.32 88.32 88.13 88.13 83.40 83.40 76.07 76.07 81.45 81.45NUS-ML 86.14 86.14 88.39 88.39 81.40 81.40 76.66 76.66 79.13 79.13LCC-WSD 87.50 87.50 87.60 87.60 81.40 81.40 75.48 75.48 80.00 80.00GPLSI 83.42 83.42 86.54 86.54 80.40 80.40 73.71 73.71 77.97 77.97BLMFS 85.60 85.60 84.70 84.70 77.80 77.80 75.19 75.19 74.20 74.20UPV-WSD 84.24 84.24 80.74 80.74 76.00 76.00 77.11 77.11 77.10 77.10TKB-UO 78.80 78.80 72.56 72.56 69.40 69.40 70.75 70.75 58.55 58.55PU-BCD 77.16 67.94 75.52 67.55 64.96 58.20 68.86 61.74 64.42 60.87RACAI-SYNWSD 71.47 71.47 72.82 72.82 66.80 66.80 60.86 60.86 59.71 59.71SUSSX-FR 79.10 57.61 73.72 53.30 74.86 52.40 67.97 48.89 65.20 51.59USYD 62.53 61.69 59.78 57.26 60.97 57.80 60.57 56.28 47.15 45.51UOFL 61.41 59.24 55.93 52.24 48.00 45.60 53.42 47.27 44.38 41.16SUSSX-C-WD 66.42 48.37 61.31 44.33 55.14 38.60 50.72 36.48 42.13 33.33SUSSX-CR 66.05 48.10 60.58 43.80 59.14 41.40 48.67 35.01 40.29 31.88UOR-SSI?
86.14 86.14 85.49 85.49 79.60 79.60 86.85 86.85 75.65 75.65Table 5: System scores by article (best scores are marked in bold, ?
: system from one of the task organizers).5 Systems DescriptionIn order to allow for a critical and comparative in-spection of the system results, we asked the partici-pants to answer some questions about their systems.These included information about whether:1. the system used semantically-annotated andunannotated resources;2. the system used the MFS as a backoff strategy;3. the system used the coarse senses provided bythe organizers;4. the system was trained on some corpus.We believe that this gives interesting informationto provide a deeper understanding of the results.
Wesummarize the participants?
answers to the question-naires in Table 7.
We report about the use of seman-tic resources as well as semantically annotated cor-pora (SC = SemCor, DSO = Defence Science Organ-isation Corpus, SE = Senseval corpora, OMWE =Open Mind Word Expert, XWN = eXtended Word-Net, WN = WordNet glosses and/or relations, WND= WordNet Domains), as well as information aboutthe use of unannotated corpora (UC), training (TR),MFS (based on the SemCor sense frequencies), andthe coarse senses provided by the organizers (CS).As expected, several systems used lexico-semanticinformation from the WordNet semantic networkand/or were trained on the SemCor semantically-annotated corpus.Finally, we point out that all the systems perform-ing better than the MFS baseline adopted it as abackoff strategy when they were not able to output asense assignment.6 Conclusions and Future DirectionsIt is commonly agreed that Word Sense Disambigua-tion needs emerge and show its usefulness in end-to-end applications: after decades of research in thefield it is still unclear whether WSD can providea relevant contribution to real-world applications,such as Information Retrieval, Question Answering,etc.
In previous Senseval evaluation exercises, state-of-the-art systems achieved performance far below70% and even the agreement between human anno-tators was discouraging.
As a result of the discus-sion at the Senseval-3 workshop in 2004, one of theaims of SemEval-2007 was to tackle the problemsat the roots of WSD.
In this task, we dealt with thegranularity issue which is a major obstacle to bothsystem and human annotators.
In the hope of over-coming the current performance upper bounds, we34System SC DSO SE OMWE XWN WN WND OTHER UC TR MFS CSGPLSI?
?
?
?
?
?
?
?
?
?
?
?LCC-WSD?
?
?
?
?
?
?
?
?
?
?
?NUS-ML?
?
?
?
?
?
?
?
?
?
?
?NUS-PT?
?
?
?
?
?
?
Parallel corpus ?
?
?
?PU-BCD?
?
?
?
?
?
?
?
?
?
?
?RACAI-SYNWSD ?
?
?
?
?
?
?
?
?
?
?
?SUSSX-C-WD ?
?
?
?
?
?
?
?
?
?
?
?SUSSX-CR ?
?
?
?
?
?
?
?
?
?
?
?SUSSX-FR ?
?
?
?
?
?
?
?
?
?
?
?TKB-UO ?
?
?
?
?
?
?
?
?
?
?
?UOFL ?
?
?
?
?
?
?
?
?
?
?
?UOR-SSI?
?
?
?
?
?
?
?
SSI LKB ?
?
?
?UPV-WSD ?
?
?
?
?
?
?
?
?
?
?
?USYD?
?
?
?
?
?
?
?
?
?
?
?Table 7: Information about participating systems (SC = SemCor, DSO = Defence Science OrganisationCorpus, SE = Senseval corpora, OMWE = Open Mind Word Expert, XWN = eXtended WordNet, WN =WordNet glosses and/or relations, WND = WordNet Domains, UC = use of unannotated corpora, TR = useof training, MFS = most frequent sense backoff strategy, CS = use of coarse senses from the organizers, ?
:system from one of the task organizers).proposed the adoption of a coarse-grained sense in-ventory.
We found the results of participating sys-tems interesting and stimulating.
However, somequestions arise.
First, it is unclear whether, giventhe novelty of the task, systems really achieved thestate of the art or can still improve their performancebased on a heavier exploitation of coarse- and fine-grained information from the adopted sense inven-tory.
We observe that, on a technical domain suchas computer science, most supervised systems per-formed worse due to the nature of their training set.Second, we still need to show that coarse senses canbe useful in real applications.
Third, a full coarsesense inventory is not yet available: this is a majorobstacle to large-scale in vivo evaluations.
We be-lieve that these aspects deserve further investigationin the years to come.AcknowledgmentsThis work was partially funded by the Interop NoE (508011),6th European Union FP.
We would like to thank Martha Palmerfor providing us the first three texts of the test corpus.ReferencesTim Chklovski and Rada Mihalcea.
2002.
Building a sensetagged corpus with open mind word expert.
In Proc.
of ACL2002 Workshop on WSD: Recent Successes and Future Di-rections.
Philadelphia, PA.Philip Edmonds and Adam Kilgarriff.
2002.
Introduction to thespecial issue on evaluating word sense disambiguation sys-tems.
Journal of Natural Language Engineering, 8(4):279?291.Christiane Fellbaum, editor.
1998.
WordNet: an ElectronicLexical Database.
MIT Press.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
Ontonotes: The90% solution.
In Proceedings of the Human Language Tech-nology Conference of the NAACL, Comp.
Volume, pages 57?60, New York City, USA.George A. Miller, Claudia Leacock, Randee Tengi, and Ross T.Bunker.
1993.
A semantic concordance.
In Proceedings ofthe ARPA Workshop on Human Language Technology, pages303?308, Princeton, NJ, USA.Roberto Navigli and Paola Velardi.
2005.
Structural seman-tic interconnections: a knowledge-based approach to wordsense disambiguation.
IEEE Transactions on Pattern Analy-sis and Machine Intelligence (PAMI), 27(7):1063?1074.Roberto Navigli.
2006.
Meaningful clustering of senses helpsboost word sense disambiguation performance.
In Proc.
ofthe 44th Annual Meeting of the Association for Computa-tional Linguistics joint with the 21st International Confer-ence on Computational Linguistics (COLING-ACL 2006),pages 105?112.
Sydney, Australia.Benjamin Snyder and Martha Palmer.
2004.
The english all-words task.
In Proc.
of ACL 2004 SENSEVAL-3 Workshop,pages 41?43.
Barcelona, Spain.Catherine Soanes and Angus Stevenson, editors.
2003.
OxfordDictionary of English.
Oxford University Press.35
