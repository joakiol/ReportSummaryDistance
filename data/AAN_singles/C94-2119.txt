Generalizing Automatically Generated SelectionalPatternsRalph  Gr i shman and John  Ster l ingComlmter  Sc ience  Depar tment ,  New York  Un ivers i ty715 Broadway,  7 th  F loor ,  New York ,  NY  10003, U.S .A .
{gr i sh lnan ,s ter l ing  } (O) cs .nyu .cduAbst rac tFrequency information on co-occurrence patterns canbe atttomatically collected from a syntactically ana-lyzed corpus; this information can then serve as the ba-sis for selectional constraints when analyzing new text;from the same domain.
Tiffs information, however, isnecessarily incomplete.
We report on measurements ofthe degree of selectional coverage obtained with ditt\~r-ent sizes of corpora.
We then describe a technique forusing the corpus to identify selectionally similar terms,and for using tiffs similarity to broaden the seleetionalcoverage for a tixed corpus size.1 In t roduct ionSelectional constraints pecify what combinations ofwords are acceptable or meaningful in particular syn-tactic relations, such as subject-verb-object or head-modifier relations.
Such constraints are necessary forthe accurate analysis of natural anguage text+ Accord-ingly, the acquisition of these constraints is an essen-tial yet time-consuming part of porting a natural lan-guage system to a new domain.
Several research groupshave attempted to automate this process by collectingco-occurrence patterns (e.g., subject-verb-ol)ject pat-terns) from a large training corpus.
These patternsare then used as the source of seleetional constraintsin attalyzing new text.The initial successes of this approach raise the ques-tion of how large a training corpus is required.
Anyanswer to this question must of course be relative tothe degree of coverage required; the set of selectionalpatterns will never be 100% complete, so a large cor-pus will always provide greater coverage.
We attemptto shed to some light on this question by processinga large corpus of text from a broad domain (businessnews) and observing how selectional coverage increaseswith domain size.In many cases, there are limits on the amount oftraining text, available.
We therefore also consider howcoverage can be increased using a tixed amount of text.The most straightforward acquisition procedures buildselectional patterns containing only the specific wordcombinations found in the training corpus.
(areatercoverage can be obtained by generalizing fl'om thepatterns collected so that patterns with semanticallyrelated words will also be considered acceptable.
Inmost cases this has been (lotto using manually-createdword classes, generalizing fi'oul specific words to theirclasses \[12,1,10\].
If a pre-existing set of classes is used(as in \[10\]), there is a risk that the classes awdlablemay not match the needs of the task.
If classes arecreated specifically to capture selectional constraints,there lnay be a substantial manual I>urden in movingto a new domain, since at least some of the semanticword classes will be domain-specillc.We wish to avoid this manual component by auto:marital ly identifying semantically related words.
Thiscan be done using the co-occurrence data, i.e., by idea:tifying words which occur in the same contexts (for ex-ample, verbs which occur with the same subjects andobjects).
From the co-occurrence data o110 Call coiil.-pute a similarity relation between words \[8,7\].
Thissimilarity information can then be used in several ways.One approach is to form word clusters based on thissimilarity relation \[8\].
This approach was taken bySekine et al at UMIST, who then used these chlstersto generalize the semantic patterns \[11\].
l'ereira et al\[9\] used a variant of this approach, "soft clusters", inwhich words can be members of difl'erent clusters todifl'eren t degrees.An alternative approach is to use the word similar-ity information directly, to inDr information about thelikelihood of a co-occurrence pattern from informationabont patterns involving similar words.
This is theapproach we have adopted for our current experiments\[6\], and which has also been employed by 17)agan etal.
\[2\].
We corl:lttttl;e from the co+occurrence data a"confitsion matrix", which measures the interchange-ability of words in particular contexts.
We then usethe confllsion matrix directly to geueralize the selllan-tic patterns.2 Acqu i r ing  Semant ic  Pat te rnsBased on a series of experitnents over tile past twoyears \[5,6\] we have developed the following procedure742for  acqu i r ing  s(;m&lll; ic i )attern.q f rom a 1;ext (:()i'l)US:l, l)~rsc the t rMning corpus using a hro~d-cover~g(~~r~i, i l l l\]~tr) a\[ ld i'(~ll\]~triT, e Lli(!
|)arse,~ I,() I ) roducesol l lethi l /g Mdil to ~ul l , F ( l  f-.sl, ructure, wi th  exp l i cil,ly lal)olod synl;icctic i'ela, tions su0h a,q SUII.1 li\]CTand O l:lJ F, CT. i2.
Extract froHi (,he regul~rizcd l);u'se ;~ series of~riplcs of I;hc fornihelm syntactic-reh~tion head-otLargumenl,/mod i{ ic rWc wi l l  use l, he iiotM, ion< 'lui~' 'll/j > for ~ilch ;ttr iple, ~li(I < r u:j > for a rt~laJ:ion--arguin<~nt pah'.3.
(',Olnptite tho h'eqliency P' of e~tc:h head and e~chtr iple in the corpllS.
If a, SelitCilce prodliCe8 Npm'ses, ~ l, rilAe gOlier~ti,ed froln a, singh~ pa, rse hasweight 1IN in the t, of, M.Wc wi l l  IlS(~ th(~ l io t , l t t ion  1'~(< 'll)irw j >)  \['or tim f roq l ion0y  o f  a tr iple, ;Uld P'A(,,l('wi) t()r the  fr l !q i le l lCywil, h w\[i ich w i a4il)eai's as ~t hea,{t iii a plti'se i,I'C(L 7For exaniple> the S(~IIIrOI/C.
{2Mary likes young lhiguists froni I,hn(n'ick.would produc(: the regul~trized synthetic structure(s like (subjcci.
(up Mary))(/,bj(,ci; (nil l iuguist (a-pos young)(ffol, i  (np I , i : , ,Mck)) ) ) )fl'Olli which i,hc fo l lowing fol ir  t,r ipl0s arc gjelit~i';tl,e(\[:like subject Marylike ,lbj(~ot l iugl i istlhig;uist *>t)os yOIlll if,l inguist froin l , iulerick(~iwm tJlc fre(lllCncy hifori i lM;iol l  l+'~ we C~tll l, henestiin~tte the prob;d)i l i i ,y i, ha, i, ~ part icular  head wial)pem's wi th  a parl, iculnr ;tl'~lilllOiil, or uiodi l ier< r 'wj > :/"_(< "2~ "~"~, >)~Tl i is  probahi l i ty  infornl~ctiou would l, hcn bc used inscoi'hlg altei'naA;ive p~trse (,rots, \[,'or (,lie eva, itla, tiOll l)0-lOW: howcvcr ,  wc  wil l  I18(7 l,h(~ t'r(~tltl011cy tl&~a, l"  directly.Stall 3 (i, he I, riples exLr~-tcl,i{ni) nchl(tes ;t liilllil)(~r ofspccial cases:l-hie wlt.h SOlllowh&l, ii1o1"1: i'egli|aril,;tl01oil \[|l{tli iS I'l()litl ill I J"( i;in pal'ticill&r~ l)a.ssivl: strll(:l.lircs &l'(l CilllVCl>t.ed tO clnTeSllOlidillgact ive  f( )\['illS,~N,)Ce that l'),<,<,~(wi) isdifferent ~?oni /a(w i apl>ears as a head in a triple) sin(c a singlehc&d in a l)&rse trec ln~ly l)rodu(:e sorer&| slich triples, one foreaCll &rgtllllClit OF nio(li|i(!r of t lutt head.
(a) if ;t vm'b has a sep~l~i-M)le lmrt ic lc  (e.g., "ouL" in"c,~u-ry out") ,  this is hi;filched t,o the head (to crc-~,i;e the  ticked carry-oul) ttl l(I l\]()tu IH'o~Li, cd sis ;% Sel)~trate rclatioii.
I)ilfereilt p~vq.iclcs often corl'oSpolidto very dilfcrenl, senses of ~ w~rb, so this avoidscoutlt~thig i, he subject ~md object d istr i ln i t ious ofthese different selises.
(b)  i f  the Vel'll is (<tie >>, We genera, l;(~ a i'eltttion bc-COml)lcmcnl bei,weeu the suliject and the pre(licat(~COil liJiClll(!lli;.
((:) tr iples in which either I,he heard or l, he a, rg~ttlll(!ll{ is;3.
I)l 'OliOtll l  ;IA'(~ disc~crdcd(d)  l,rilflCs in which the aa'<gliln(~nt i~ ;i, su\[ioi'din~tte(:l~uise ~cre disc*ci'dcd (lihis iuchides sut)or(Ihi;~l;etol l  junct ions ;uul wn't)s l,a, king cl~tils;_c\] ~_n'gunienl,s)(e) l.l'iples indic:cling negMiio/i (with &ll ;M'<ff.iil\[10ill, Of"not" or "newer") are ignored3 Genera l i z ing  Semant ic  Pat-ternsThe proc.rdurc described M)ow ~, producers a. set, of I~'c(luencies 3,11d I)r(dmbilit.y esl.intatcs based on Sl)ccilicwordm The "trnditi<mM" ~tpproach to gcnerMizing tiff<;inl\:,rma, tion ha~ I:,ccu i;o assign the word,'-; t,() a set orScIIl~tllti(' C\[asges, g%n(\[ thri l l  \[,0 collect the f req i le i i cy  in-\[br|m~tion  COlnbinations of sen,antic la.sscs \[ 12, 1\].Since ~t; least some of t, hese classes will be domainSl)ecilic , there has \[)t!ell inl.erest in mltomating the ac-quisition of these classes ~ts well.
This c~m be doneby ch,st,(,ring l,ogetohcr words which appear in the s;m,ccontexl.. Sl.arting from the lile of l;riplcs, thi:s involves:I. collecting for e;~l:h woM i;he \['re(lucilcy with whichit occurs in each possilAc context; f .r  cx~mplc, forn nou,l we wouhl collect the frequency with whichit occurs as the slll)jeci; ;till\[ 1.he object ot'(~ach verb2.
del ini ,g a similarity lll{~tSill'(': between words,which rellccts thc  tll l ltl l ' Jer Of CO\]\[III\[IOll COIIt(!XL8 illwhich l;ticy nppc~r3.
foruiing clusters Imsod on this similarity m{~asul'cSuch a procedure was performed by Sekinc ct M. ~tt\[lMIS'l' \[l l\]; these chnsl.crs were then manually r(!v iewed ~tnd the  i 'eStl l t i l t~ c lus ters  wet'(!
used 1() F, Clml'-M izc  SelC:Ctioll:rll pa, tllern,'s.
A s i tni la,  r afq)roa,ch Lo wordcluster rormatiou w~s dcscril}cd by llirschum.u (,t al.in 1975 V\].
M(iro , ' ( .
.
.<y,  rer,,ira, .
t  ;~1.
\[(4 l , .v .
(l,,-sci'ibcd ;~ word chlstei'hlg nicthod ushig "soft cinsl,ers':hi which a, word C;lll belong to several chlsl,er,q, withdilN~i'enl, chtsl,er menll)ership I,'obalfilith~s,(Jlusl, er creal;iou has (,he ;Ldwull, ago l,ha.I, the clusl, ers;tr0 aAIlCll~tl)lc l;o ln~Ullt;cl review and correction, () l lLhe other haucl, ol ir  experience i l ldicates 1,h;~t stlcccs.qrul chlster g~01if;ra, l io i l  depends Oil ral;her dclh:~i;c ad-jltsl;li~ienl, of the chlstcri l lg criteria.
We haw~ l, hcrcfore743elected to try an approach which directly uses a formof similarity measnre to smooth (generalize) the prob-.abilities.Co-occurrence smoothing is a method which hasbeen recently proposed for smoothing n-gram models\[3\].a The core of this method involves the computationof a co-occurrence matrix (a matr ix of eonfl, sion prob-abilities) Pc:(wj Iw0, which indicates the prol)ability ofword wj occurring in contexts in which word wi occurs,averaged over these contexts.
:L,(wj lwi) : ~ P(wjI,~)P(.~I'~,OE.
P(,o~ Is)v(,,,~l,~)V(s)p(wi)where the sum is over the set of all possible contexts .In applying this technique to the triples we have col-lected, we have initially chosen to generalize (smoothover) the first element of triple.
Thus, in triples of theform wordl relation word2 we focus on wordl, treatingrelation attd word2 as the context::'.
(~,~lw5= ~ rO.
:l < , '~j  >) .
v (< ,.w, > In,:)r:v~F(< ~ ,- ~j >) / , (< w~ ,, ~"5 >)Informally, we ear, say that a large value of /)C'(,,il,)I)indicates that wi is selectionally (semantically) accept-able in the syntactic contexts where word w~ appears.For example, looking at the verb "convict", we see thatthe largest values of P(:(eonvict, x) are for a: = "acquit"and x = "indict", indicating that "convict" is selec-tionally acceptable in contexts where words "acquit"or "indict" appear (see Figure 4 for a larger example).How do we use this information to generalize thetriples obtained from the corpus'?
Suppose we are in-terested in determining (.he acceptability of the patternconvict-object-owner, ven though this triple does notapl)ear in our training corpus.
Since "convict" canappear in contexts in which "acquit" or "indict" appear, and the patterns acquit-object-owner and indicbo/)ject-owner appear in the corpus, we can conchldethai, the pattern convict-object-owner is acceptabletoo.
More formally, we compute a smoothed triplesfrequency lP.s' from the observed frequency /i' by aver-aging over all words w~, incorporating frequency infor-mation for w~ to the extent that its contexts are alsosuitable contexts for wi::':~*(< *,:i ,.
,,,j >) -- ~ r"("'il*";)" ::(< ,,,~ ,, ,,:j >)~tJlit or(ler to avoid the generation of confltsion table en-tries from a single shared context (which quite oftenawe wish to thank  R ichard  Schwartz  of BBN for referr ing usto this method  &lid article.is the result of an incorrect I)arse), we apply a filterin generating Pc: for i ?
j,  we generate a non-zeroPc(wj Iw j  only if the wi and wj appear it* at leant twoeo i t l lnon  contexts ,  and  there  is some eOlnn lon  contextin which both words occur at least twice, l,'urther-more, if the value computed by the formula for Pc' isless than some thresbold re:, the value is taken to bezero; we have used rc = 0.001 in the experiments re-ported below.
(These tilters are not applied for thecase i = j; the diagonal elements of the confusionmatrix are always eomputed exactly.)
Because thesefilters may yeild an an-normalized confltsion matrix(i.e., E~ t>(*vJlv'i) < l), we renorn, alize the n\]atrixso that }~,.j \[g,(wi\[wi ) = 1.A similar approach to pattern generalization, usinga sirnilarity measnre derived fi'om co-occurrence data,has been recently described by l)agan et a\].
\[2\].
Theh'approach dill'ers from the one described here in twosign*titan* regards: their co-occurrence data is basedon linear distance within the sentence, rather than onsyntactic relations, and they use a different similaritymeasure, based on mutual information.
The relativemerits of the two similarity rneasures may need to beresolved empirically; however, we believe *bat, there isa v i r tue  to  our  l lOn-sy ln lnet r i c  lileaSlll'e~ becat l se  8tll)-stitutibil ity in seleetional contexts is not a symmetricrelation .44 Eva luat ion4.1 Eva luat ion  Metr icWe have previously \[5\] described two methods for theevaluation of semantic onstraints.
For tile current ex--periments, we have used one of these methods, wherethe constraints are evaluated against a set of manuallyclassitied semantic triples.
'~For this (waluation, we select a small test corpus sep-arate fl'om the training corpus.
We parse the corpus,regularize the parses, and extract triples just as we didtbr the semantic acquisition phase.
We then manuallyclassify each triph" as valid or invalid, depending onwhether or not it arises fl'om the correct parse for thesentence.
GWe then estahlish a threshold 7' for the weightedtriples counts in our training set, and deline4 If v:l allows a hi'o,taler range of argu lnents  than w2, thenwe can replace w2 by vq ,  but  llOIb giC(~ versa,  For (':xanlple~ w(;can repla(:e "speak" (which takes a human subject)  by "sleep"(which takes an an imate  subject) ,  and still have a selectional lyvalid pat tern ,  \])tit.
not the other  wety around.~"l'his is s imi lar  to tests conducted by Pcreira ct al.
\[9\] andl )agan et al \[2\].
The cited tests, howevcl', were based ,m selectedwords or word pairs of h igh frequency, whereas ore" test setsinvolve a representat ive set of h igh and low frequency triples.s t i f f s  is a different cr i ter ion fl'om the one used in our earl ierpapers .
In our earl ier work, we marked  a tr iple as wdid if itcould be valid in some sentence in the domain.
We found that  itwas very (lilIicult to apply such a s tandard  consistmltly, and havetherefore changed to a cr i ter ion based on an indiv idual  sentence.744recall0.700.600.500.400.300.200.10RII l I \ ]O 0 0 ??
'boo0 o~%oOo?%0.00 i- -7 l ?
?0,60 0.65 0.70 0.75 0.80 0.85precisionFigm'c 1: l/.acall/prccision trade-oi l  using eutire cor-pus.vq numl',er of l.rilllcs in test set which were ('.lassitiedas vMid and which a,F,l)em'ed iu t ra in ing sct withcount > "/'V__ llllllll)or oV tril)lcs in tcsl, set which were classilicdas valid m,d which nl)pearc(I in t ra in ing set withCOIlI/I.
< ~/'i I- lmn,I)er of tril)lcs in t,est set.
which were classitlcdas inwdid and which al)peared ilt trahf ing set withCO\[llti, > "\['and then delincv -Ireca l l  = .
.
.
.t J i  + v_I ) rec i ,~hm _-: _ vq~ ..v+ + iqBy wu'ying the l, hreshold, we can a~lcct dill\wenttrade-olfs ()f recall and precisioli (at high threshold, weseh~ct: only a small  n, ,mher of triph:s which apl)earedfrequ(mtly and in which we l.hereforc have \]ligh conli--(h!nce, t;hus obta in ing a high precision lm(, \]()w recall;conversely, at a h)w t, hrcshohl we adndt  a uuuch l a rgernund)er of i.riplcs, obt,aiuiug ~ high recall but lowerprecisiol 0.4.2  .t .s~ DataThe trai,fing and Icst corpora were taken from the WallStreet ,hmrnaJ.
In order to get higher-qual ity parsesor  I,\]lcse ,q(ml;elices, we disahlcd some of the recoverymechanisms normal ly t>ed in our parser.
Of the 57,366scnte,lCCS hi our t,rMidng corpus, we ohtMned comph%epars('s Ibr 34,414 and parses of initial substr ings for anaddit ional  12,441 s(mtenccs.
These i)m'ses were th(mregularized aim reduced to t,riph~s.
Wc gcnerat;(;d atotal of 27q,233 dist inct triples from the corpus.The test corpus used to generate l,he triph~s whichwere mamlal ly classified consisl,ed of l0 artMcs,  also0.600.580.560.540.520.500.48recall 0.460.440.420.400.380.360.340?0X00x.
.
.
.
.
.
1- .
.
.
.
7 .
.
.
.
.
.
.
1 -  -I0 25 50 75 100percent;~ge of corp l l sFigure 2: Growth of recall as a f imction of corpus size(percentage of totM corpus used), o = at 72% l)reci -siou; ?
= max immn reca\[\], regardless of precision; x :-I)redicted values R)r m~Lximum recallD<)m the Wall S~;reet Journal ,  dist inct from those inthe tra in ing set.
These articles produced a tcs(.
set;containing a totM of i{)32 triples, of which 1107 werevalid ;rod 825 were \[nvMid.4.3  Resu l ts4.3.1 Growth  w i th  Cor l )us  S izeWc began by generat ing triples from the entire corpusand cwdmLt, ing the selectional patterns as <lescribedabove; tile resulth/g recall/ l)recision curve generatedby wu'ying the threshold is shown in Figure 1.To see how pattern coverage iwl)roves with corpussize, we divided our tra in ing corpus into 8 segmentsand coHll/uted sets of tril)lcs based on the lirst Seglllell|,,the Ih'st two segments, etc.
We show iu Figure 2 a plotof recall vs. corpus size, both at ~ consl, ant precision of72% and for max imum recall regardless of precision .7The rate of g;rowth of the max imum recall cau beunderstood in teruls of the frequency distr ibut ion oftriples.
In our earlier work \[4\] we lit the growth datato curw~s of the form 1 -exp( - f ia : ) ,  on tile assump-t.ion that  all selectional imtterns are t~qually likely.This lttay have 1)ee|l a roughly accurate assumpt ion forthat  app\] ication, involving semantic-class based pat-terns (rather t, han word-based l);-ttl;erns), and a rathersharply circumscribed sublanguage (m(xlical reports).For the (word level) pal;i ,crl ls described here, howevcr,the distr ibut ion is quite skewed, with a small numberof very-high-frequency l)atl,erns, a which results in di\[:rN,, (1,tta point is shown for 72% precision for the first seg-l l l ( : i l t  & lone  } ;e( :a l lSe  we ~tl'c n l ) \ [  ab le  to  re&oh ;t prcci .%lOl l  o f  72~with a single seglnent.a'l 'hc number  of highq're(luency patterns  is m:(:enl, u;tted by7451000number 100oftripleswiththisfrequency i0%%%**..-..I00f10fl'equency of triple in training corpusl"igure 3: Distribution of fre(tuencies of triples in train-ing corpus.
Vertical scale shows number of triples witha given frequency.fereat growth curves.
Figure 3 plots the number ofdistinct triples per unit frequency, as a function of fi'e-quency, for the entire training corpus, This data canbe very closely approximated by a fimction of tile formN(t,') = al  ;'-~, where r~ = 2.9.
9q'o derive a growth curve for inaxinmln recall, wewill assunle that the fl'equeney distribution for triplesselected at random follows the same tbrm.
Let I)(7)represent he probability that a triple chosen at ran-dorn is a particular triple T. l,et P(p) be the densityof triples with a given probability; i.e., the nmnber oftriples with probal)ilities between p and p + ( is eP(p)(for small e).
Then we are ass,,ming that P(p) = ~p-~,for p ranging fl'om some minimum probability Pmin to1.
For a triple T, the probability that we would lindat least one instance of it in n corpus of w triples isapproximately i -- c -~p(T).
The lnaximum recall for acorpus of ~- triples is the probability of a given triple(the "test triple") being selected at random, multipliedby the probability that that triple was found in thetraining corpus, summed over a.ll triples:~)(r) .
(1 - e-~"("'))7'which can be coral)uteri using the density function~ 1 P '  P (P ) '  (1 e-"V)dpm, nf l -~(1 c -T~' = rap.
p - .
)alp,,~iaBy selecting an appropriate value of a (and correspond-ing l),~i,~ so that the total probability is 1), we can get athe fact that  our lcxicM scmmcr replaces all identit iablc COlll-lYally lllLllleS by tile token a-company ,  all C/llTellcy values by a-currency, etc.
Many of the highest frequency triples involve suchtokens.9Thls is quite shnilm' to a Zipf's law distribution, for whichw f 'c(bondlw)eurobondforaymortgageobjectivemarriagenotematuritysubsidyveterancommitmentdebentureactivismmilecouponsecurityyieldissue0.1330.1280.0930.0890.0710.0680.0570.0d60.0460.0460.0440.043().0380.0380.0370.0360.035Figure 4: Nouns closely related to the IIOUII 'q)ond":ranked by t): .good match to the actual maximum recall values; thesecomputed values are shown as x in Figure 2.
Except\['or the smallest data set, the agreement is quite goodconsidering the wwy simple assumpt, ions made.4.3.2 Smooth ingIn order to increase our coverage (recall), we then ap-plied the smoothing procedure to the triples fi'om ourtraining corpus.
In testing our procedure, we lirst gen-erated the confusioll matrix Pc and examined some ofthe entries, l"igure 4 shows the largest entries in f'c forthe noun "bond", a common word in the Wall StreetJournal.
It is clear that (with some odd exceptions)most of tile words with high t) :  wtlues are semanti-cally related to the original word.
'lk) evaluate the etl\~ctiveness of our smoothing pro-cedure, we have plotted recall vs. precision graphs forboth unsmoothed and smoothed frequency data.
Theresults are shown in l,'igure 5.
Over tile range of preci-sions where the two curves overlap, the smoothed ataperforms better at low precision/high recall, whereasthe unsmoothed ata is better at high precision/lowrecall.
In addition, smoothing substantially extendsthe level of recall which can be achieved for a givencorpus size, although at some sacrilice in precision.Intuitively we can understand why these curvesshould (:ross as they do.
Smoothing introduces a cer-tain degree of additional error.
As is evident from Fig-ure 4, some of the confllsion matrix entries arc spuri-ous, arising from such SOllrces as incorrect l)arses andthe conIlation of word senses.
In addition, some of thetriples being generalized are themselves incorrect (notethat even at high threshold the precision is below 90%).The net result is that a portion (roughly 1/3 to 1/5) of7460.80recldl0.700.600.500.40O.3O0.200.6Oo~' .+ l,,ll, ++?
~ ?
-  o o o*~.
OaO 0oo* .
o"t 8?
, ?
?o o- -  \[ .
.
.
.
I - -  T - -  ?0.65 0.70 0,75 0.80precisionl"ip;ure 5: Benel:its of stnoot,hiug for la.r,gcsl eorl)us: o-- unstn<>othe(I da.i,&, ?
=: sH,oothed (tati~.the tril>les added by smoothing ~l'e incorreet.
At lowlevels of 1)recision, (,his l)ro({uces a. net gldn on t.he l)re -eision/rec+dl curve; +tt, highe,' levels o1' precision, '?hereix a. net loss.
In a.ny event, smoothing (toes allow forsul)stlml, ially higher levels o1' recall than are possiblewithout+ smoothing.5 Conc lus ionWe \]tltve demonstrated how select.tonal p;~tl;erns ca.n lyeattt;otn~tic;dly acquired from it corpus, ~md how seleet iond e,:)w~ragc gr~t(hlally it~ere~.tses with the size (,f thetra.ining eorl+us.
We h~tve +dso domonst;r;tted thatI'~)r it given corpus size eovcr++ge can I)e sig,tilicantlyimproved 1)y using 1,he corpus 1;o identify selectionallyrelated ternts, and using these simila.rit.ies to generltlizethe patterns tybserved in the training eorums.
'l'his isconsisteut with other reeo,tt results using ,+eHtted l,eeh-niques \[2,9\].We believe th+tl; Lhese Lt:ehniques e+tn I)e ft,t'ther im-proved in several wa.ys.
The exl)erin,ent.s rel)orl.e+labove ha+ve only get'~ra\]ized over t, ht, lit'st; (head) po-sitioI, of t,he triples; we need to melrstu'o the eIl'<~ct ofgcncrldizhtg ow~r the al'gum<mt l~OSh;ion as w<ql.
Withbtrger eorpor~ it, I~,~ty itlso be I>asil)l{~ to use lirl+ger pat-terns, including in p~trtieular st,b.jeet-verl)-~d).ieet i)~tl;.terns, ;tnd thus reduce the confusion due to tre~tt, ing(li|t'et'e,lL words senses its eOlllltlOll eontexLs.References\[I\] .ling-Shin (~haatg, Yih-l:en l,uo, +rod l(eh--YihSu.
G\[)SM: A eo;enera.lized l)robtd)ilislic s<.~tllltllt;icmodel for n.lnbiguil, y resolution.
It+ l+rocccdiP+\[I s ofthe ,\[\]Olh Annual Meeting of tile Assn.
for (/o~tpu-rational Linguistic.s, l);tges 177 18/t, Newark, I)E,June 1992.\[2\] Ido l)aglm, Hh~ml M~u'cus, ;tud ~qh~ul M~rkovilch.
(3ontextual word similarity ~md estintnti(m fromspa.rse (lal, c~.
In l"l"oeeedings of the ,"7/.st An~lualMt:t'.li~g +4" lht Assn.
for Co~ltp'atatio~tal Li?Jg'llis-tics, pa.ges 3l 37,( lolunibus, 011,. lune 1993.\[5;\] U. Essen +rod V. Steinbiss.
(~ooccta'renc~stnoothing for st.oehltst;ie langua.ge rnodeliug.
It,1(7A,%',<?1'9?~?, pages I 161 I 164, S;I.tl I"rll.tl{:isel~,(3A, M;i.y 1992.\[14\] R. (;visluna.n, I,.
l/irschma.n, and N.T.
Nllctn.
Dis-.eovery procedures for sul)l~mgui~ge s lcet, iot~;tl pitt-terns: Initi+d exl)eriments.
(/oTitp+ttatio'~tal Li'a-(luislic.% 1213):205 16, 1986.\[5\] l{+~rll)h (lrishm+m aztd ,lohn Sterling.
A cquisitiottof seleetion~d i)aAA,erns.
In l'roc.
14th lnt'l (/o~+\]~(/o~l~pulatio?lal Ling~lislics (COLIN(?
92), N~mtes,France, July 1!
)92.\[6\] R.~dph (~t'ishmatl ~md John S(.erlhlg.
SutootAtingof autotn++t, ic.ally generltted selecl, ionttl consLr++int.s.Ill \[s?
'occt,.diTt\[\]s oj Ihc \[htTtta~t L(t?ty\]~ta.qt' '\]'t!c\[ti~olo(\]\[qWorkshop, l'rineel, on, N J, March 1993.\[7\] l)onldd Ilindle.
Noun el~,ssitica, tAon I'rol,tpredica, t+e-~+rgutnent struel;ures.
In l'?
'occeding.s o\]the ,?Slh An+l+tal Meeting of the Ass'it.
\]br (;o+++pu-talio~.al Ling+ui.slics, i)a.ges 268 275, Jitne 1990.\[8\] l,ynet\[,e ll irschtnlm, H.idl)h (h'isht,t+L,l, ;ttk(l Ni~ot,tiS;tger.
(~r&tnlrlitl+ieltlly-I'~itsed ~tUl;Oln&Lic wordcli~ss t'ortnlttion, l~tJ'or~ttalio?t l'~'occ.~si~t:l a',dManage.mcnl, 11(1/2):3D 57, 1975.\[9\] l"ern+mdo l'ereiri% N~d't;;di Tishl)y, imd I,illilm Lee.l)istributiona, l clustering of English words.
Inl"r'occedi~l\[ls of lhe <Ylsl An~vual Mcelin:l of theAss?t.
j'o't" (/omp,zlalional Li?ig+tistics, pltges 183l'()0, (',olund)us, ()11, June 1903.\[10\] Philip R.esnik.
A elass-b;tsed :tlH)t'oach to lexi-cal discovery, lu I'rocecdi?tgs of tile ,YOlh A?t'nualMecli'ag oJ" tht Assn.
for (\]o~lpulalional L4'~./li~is-lies, Newark, I)F,, .lun+~ 1992.\[1 I\] S~ttoshi Sekine, Soti~ A,~a.niadou, .lere,,,y (~*tr,'oll,;u,d .lun'iehi :l'sujii.
15nguistie knowl<>dg;e get,-el'i;~l, or.
hi l'roc, l/~lh, htt'l (\]onfl (/omp,lialio?lalLi?tg~ti.slies ((/OLIN(I 9,?3), pa.ges 5G0 5(i6, N;tnl.es,I:,'a.nce, .luly 19.92.\[12\] l:'a.ola.
Vela.rdi, M;tria.
'l'eros~t Pazienza, andMiehel+~ Fa.solo.
lIow to encode semantic knowledge: A lnethod for lne~+ning represent~tti{m andcompuLer-~dded ~cquisiticm.
(7o?np+tlalio'nal I,i~l-guislics, 17(2):153 170, I,1t(.
)1.747
