Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 66?71,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsA Language-Independent Neural Network for Event DetectionXiaocheng Feng1, Lifu Huang2, Duyu Tang1, Bing Qin1, Heng Ji2, Ting Liu11Harbin Institute of Technology, Harbin, China{xcfeng, dytang, qinb, tliu}@ir.hit.edu.cn2Rensselaer Polytechnic Institute, Troy, USA{huangl7, jih}@rpi.eduAbstractEvent detection remains a challenge dueto the difficulty at encoding the word se-mantics in various contexts.
Previousapproaches heavily depend on language-specific knowledge and pre-existing nat-ural language processing (NLP) tools.However, compared to English, not alllanguages have such resources and toolsavailable.
A more promising approachis to automatically learn effective featuresfrom data, without relying on language-specific resources.
In this paper, we de-velop a hybrid neural network to cap-ture both sequence and chunk informationfrom specific contexts, and use them totrain an event detector for multiple lan-guages without any manually encoded fea-tures.
Experiments show that our approachcan achieve robust, efficient and accurateresults for multiple languages (English,Chinese and Spanish).1 IntroductionEvent detection aims to extract event triggers(most often a single verb or noun) and classifythem into specific types precisely.
It is a cru-cial and quite challenging sub-task of event ex-traction, because the same event might appear inthe form of various trigger expressions and an ex-pression might represent different event types indifferent contexts.
Figure 1 shows two examples.In S1, ?release?
is a verb concept and a trigger for?Transfer-Money?
event, while in S2, ?release ?
isa noun concept and a trigger for ?Release-Parole?event.Most of previous methods (Ji et al, 2008; Liaoet al, 2010; Hong et al, 2011; Li et al, 2013; Li etal., 2015b) considered event detection as a classi-S2:   The court decides Anwar ?s earliest release date   is April.ccompdet nsubjpossnnamodp?s copnsubjDT NN  VBZ NNP     ?s        JJS             NN       NNS  VBZ NNPS1:   The European   Unit   will  release   20   million   euros   to   Iraq.prepdrobjnum pobjDT NNP  NNP MD      VB       CD       CD          NNS     IN    NNPdetnumauxnnPersonOrganizationTransfer-MoneyNumberCalendarRelease-ParolecluescluesnsubjFigure 1: Event type and syntactic parser resultsof an example sentence.fication problem and designed a lot of lexical andsyntactic features.
Although such approaches per-form reasonably well, features are often derivedfrom language-specific resources and the output ofpre-existing natural language processing toolkits(e,g., name tagger and dependency parser), whichmakes these methods difficult to be applied to dif-ferent languages.
Sequence and chunk are twotypes of meaningful language-independent struc-tures for event detection.
For example, in S2,when predicting the type of a trigger candidate ?release?, the forward sequence information suchas ?court?
can help the classifier label ?release?as a trigger of a ?Release-Parole?
event.
How-ever, for feature engineering methods, it is hardto establish a relation between ?court?
and ?re-lease?, because there is no direct dependency pathbetween them.
In addition, considering S1, ?Eu-ropean Union?
and ?20 million euros?
are twochunks, which indicate that this sentence is relatedto an organization and financial activities.
Thesecluese are very helpful to infer ?release?
as a trig-ger of a ?Transfer-Money?
event.
However, chun-kers and parsers are only available for a few high-resource languages and their performance varies alot.66The                   European                   Unit                     will                      release 20                     million                   euros ?SoftMaxLSTMBLSTMFLook upBV FV C3C2Concatenatewith CNNLSTM LSTM LSTM LSTM LSTM LSTMLSTM LSTM LSTM LSTMEvent TriggerLSTM LSTMLSTMLSTMLSTMLSTM.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Figure 2: An illustration of our model for event trigger extraction (here the trigger candidate is ?release?
).Fvand Bvare the output of Bi-LSTM and C2, C3are the output of CNN with convolutional filters withwidths of 2 and 3.Recently, deep learning techniques have beenwidely used in modeling complex structures andproven effective for many NLP tasks, such as ma-chine translation (Bahdanau et al, 2014), rela-tion extraction (Zeng et al, 2014) and sentimentanalysis (Tang et al, 2015a).
Bi-directional longshort-term memory (Bi-LSTM) model (Schusteret al, 1997) is a two-way recurrent neural network(RNN) (Mikolov et al, 2010) which can captureboth the preceding and following context informa-tion of each word.
Convolutional neural network(CNN) (LeCun et al, 1995) is another effectivemodel for extracting semantic representations andcapturing salient features in a flat structure (Liu etal., 2015), such as chunks.
In this work, we de-velop a hybrid neural network incorporating twotypes of neural networks: Bi-LSTM and CNN, tomodel both sequence and chunk information fromspecific contexts.
Taking advantage of word se-mantic representation, our model can get rid ofhand-crafted features and thus be easily adaptedto multiple languages.We evaluate our system on the event detectiontask for various languages for which ground-truthevent detection annotations are available.
In En-glish event detection task, our approach achieved73.4% F-score with average 3.0% absolute im-provement compared to state-of-the-art.
For Chi-nese and Spanish, the experiment results are alsocompetitive.
We demonstrate that our combinedmodel outperforms traditional feature-based meth-ods with respect to generalization performanceacross languages due to: (i) its capacity to modelsemantic representations of each word by captur-ing both sequence and chunk information.
(ii) theuse of word embeddings to induce a more generalrepresentation for trigger candidates.2 Our ApproachIn this section, we introduce a hybrid neural net-works, which combines Bi-directional LSTM (Bi-LSTM) and convolutional neural networks to learna continuous representation for each word in asentence.
This representation is used to predictwhether the word is an event trigger or not.
Specif-ically, we first use a Bi-LSTM to encode semanticsof each word with its preceding and following in-formation.
Then, we add a convolutional neuralnetwork to capture structure information from lo-cal contexts.2.1 Bi-LSTMIn this section we describe a Bidirectional LSTMmodel for event detection.
Bi-LSTM is a typeof bidirectional recurrent neural networks (RNN),which can simultaneously model word represen-tation with its preceding and following informa-tion.
Word representations can be naturally con-sidered as features to detect triggers and theirevent types.
As show in (Chen et al, 2015), wetake all the words of the whole sentence as the in-put and each token is transformed by looking upword embeddings.
Specifically, we use the Skip-Gram model to pre-train the word embeddings torepresent each word (Mikolov et al, 2013; Bah-danau et al, 2014).We present the details of Bi-LSTM for eventtrigger extraction in Figure 2.
We can see thatBi-LSTM is composed of two LSTM neural net-works, a forward LSTMFto model the preced-67.
.
.
.
.
.Max-PoolingFeature Map 1 Feature Map 2 Feature Map nConvolutionLookup WEPFW1 W2 W3 Wn.
.
.
.
.C3Figure 3: CNN structure.ing contexts, and a backward LSTMBto modelthe following contexts respectively.
The inputof LSTMFis the preceding contexts along withthe word as trigger candidate, and the input ofLSTMBis the following contexts plus the wordas trigger candidate.
We run LSTMFfrom the be-ginning to the end of a sentence, and run LSTMBfrom the end to the beginning of a sentence.
Af-terwards, we concatenate the output Fvof LSTMFand Bvof LSTMBas the output of Bi-LSTM.
Onecould also try averaging or summing the last hid-den vectors of LSTMFand LSTMBas alterna-tives.2.2 Convolution Neural NetworkAs the convolutional neural network (CNN) isgood at capturing salient features from a sequenceof objects (Liu et al, 2015), we design a CNNto capture some local chunks.
This approach hasbeen used for event detection in previous studies(Nguyen and Grishman, 2015; Chen et al, 2015).Specifically, we use multiple convolutional filterswith different widths to produce local context rep-resentation.
The reason is that they are capableof capturing local semantics of n-grams of variousgranularities, which are proven powerful for eventdetection.
In our work, multiple convolutional fil-ters with widths of 2 and 3 encode the semantics ofbigrams and trigrams in a sentence.
This local in-formation can also help our model fix some errorsdue to lexical ambiguity.An illustration of CNN with three convo-lutional filters is given in Figure 3.
Letus denote a sentence consisting of n wordsas {w1, w2, ...wi, ...wn}, and each word wiismapped to its embedding representation ei?
Rd.In addition, we add a position feature (PF), whichis defined as the relative distance between the cur-rent word and the trigger candidate.
A convolu-tional filter is a list of linear layers with shared pa-rameters.
We feed the output of a convolutionalfilter to a MaxPooling layer and obtain an outputvector with fixed length.2.3 OutputAt the end, we concatenate the bidirectional se-quence features: F and B, which are learned fromthe Bi-LSTM, and local context features: C2andC3, which are the output of CNN with convolu-tional filters with width of 2 and 3, as a single vec-tor O = [F,B,C2, C3].
Then, we exploit a soft-max approach to identify trigger candidates andclassify each trigger candidate as a specific eventtype.2.4 TrainingIn our model, the loss function is the cross-entropyerror of event trigger identification and triggerclassification.
We initialize all parameters to forma uniform distribution U(?0.01, 0.01).
We set thewidths of convolutional filters as 2 and 3.
Thenumber of feature maps is 300 and the dimensionof the PF is 5.
Table 1 illustrates the setting param-eters used for three languages in our experiments(Zeiler, 2012).3 ExperimentsIn this section, we will describe the detailed exper-imental settings and discuss the results.
We eval-uate the proposed approach on various languages(English, Chinese and Spanish) with Precision (P),Recall (R) and F-measure (F).
Table 1 shows thedetailed description of the data sets used in our ex-periments.
We abbreviate our model as HNN (Hy-brid Neural Networks).3.1 Baseline MethodsWe compare our approach with the followingbaseline methods.
(1) MaxEnt, a basesline feature-based method,which trains a Maximum Entropy classifier withsome lexical and syntactic features (Ji et al, 2008).
(2) Cross-Event (Liao et al, 2010), usingdocument-level information to improve the perfor-mance of ACE event extraction.
(3) Cross-Entity (Hong et al, 2011), extractingevents using cross-entity inference.
(4) Joint Model (Li and Ji, 2014), a joint struc-tured perception approach, incorporating multi-level linguistic features to extract event triggersand arguments at the same time so that local pre-dictions can be mutually improved.68LanguageWord Embedding Gradient Learning Method Data Setscorpus dim method parameters Corpus Train Dev TestEnglish NYT 300 SGD learning rate r = 0.03 ACE2005 529 30 40Chinese Gigaword 300 Adadelta p = 0.95, ?
= 1e?6ACE2005 513 60 60Spanish Gigaword 300 Adadelta p = 0.95, ?
= 1e?6ERE 93 12 12Table 1: Hyperparameters and # of documents used in our experiments on three languages.ModelTrigger Identification Trigger ClassificationP R F P R FMaxEnt 76.2 60.5 67.4 74.5 59.1 65.9Cross-Event N/A N/A N/A 68.7 68.9 68.8Cross-Entity N/A N/A N/A 72.9 64.3 68.3Joint Model 76.9 65.0 70.4 73.7 62.3 67.5PR N/A N/A N/A 68.9 72.0 70.4CNN 80.4 67.7 73.5 75.6 63.6 69.1RNN 73.2 63.5 67.4 67.3 59.9 64.2LSTM 78.6 67.4 72.6 74.5 60.7 66.9Bi-LSTM 80.1 69.4 74.3 81.6 62.3 70.6HNN 80.8 71.5 75.9 84.6 64.9 73.4Table 2: Comparison of different methods on En-glish event detection.
(5) Pattern Recognition (Miao and Grishman,2015), using a pattern expansion technique to ex-tract event triggers.
(6) Convolutional Neural Network (Chen et al,2015), which exploits a dynamic multi-poolingconvolutional neural network for event trigger de-tection.3.2 Comparison On EnglishTable 2 shows the overall performance of all meth-ods on the ACE2005 English corpus.
We cansee that our approach significantly outperformsall previous methods.
The better performance ofHNN can be further explained by the followingreasons: (1) Compared with feature based meth-ods, such as MaxEnt, Cross-Event, Cross-Entity,and Joint Model, neural network based methods(including CNN, Bi-LSTM, HNN) performs betterbecause they can make better use of word semanticinformation and avoid the errors propagated fromNLP tools which may hinder the performance forevent detection.
(2) Moreover, Bi-LSTM can cap-ture both preceding and following sequence in-formation, which is much richer than dependencypath.
For example, in S2, the semantic of ?court?can be delivered to release by a forward sequencein our approach.
It is an important clue which canhelp to predict ?release?
as a trigger for ?Release-Parole?.
For explicit feature based methods, theycan not establish a relation between ?court?
and?release?, because they belong to different clauses,and there is no direct dependency path betweenthem.
While in our approach, the semantics of?court?
can be delivered to release by a forwardsequence.
(3) Cross-entity system achieves higherrecall because it uses not only sentence-level in-formation but also document-level information.
Itutilizes event concordance to predict a local trig-ger?s event type based on cross-sentence infer-ence.
For example, an ?attack?
event is morelikely to occur with ?killed?
or ?die?
event ratherthan ?marry?
event.
However, this method heav-ily relies on lexical and syntactic features, thusthe precision is lower than neural network basedmethods.
(4) RNN and LSTM perform slightlyworse than Bi-LSTM.
An obvious reason is thatRNN and LSTM only consider the preceding se-quence information of the trigger, which may misssome important following clues.
Considering S1again, when extracting the trigger ?releases?, bothmodels will miss the following sequence ?20 mil-lion euros to Iraq?.
This may seriously hinder theperformance of RNN and LSTM for event detec-tion.3.3 Comparison on ChineseFor Chinese, we follow previous work (Chen et al,2012) and employ Language Technology Platform(Liu et al, 2011) to do word segmentation.Table 3 shows the comparison results betweenour model and the state-of-the-art methods (Li etal., 2013; Chen et al, 2012).
MaxEnt (Li et al,2013) is a pipeline model, which employs human-designed lexical and syntactic features.
Rich-Cis developed by Chen et al (2012), which alsoincorporates Chinese-specific features to improveChinese event detection.
We can see that ourmethod outperforms methods based on human de-signed features for event trigger identification andachieves comparable F-score for event classifica-tion.3.4 Spanish ExtractionTable 4 presents the performance of our methodon the Spanish ERE corpus.
The results show that69ModelTrigger Identification Trigger ClassificationP R F P R FMaxEnt 50.0 77.0 60.6 47.5 73.1 57.6Rich-C 62.2 71.9 66.7 58.9 68.1 63.2HNN 74.2 63.1 68.2 77.1 53.1 63.0Table 3: Results on Chinese event detection.HNN approach performed better than LSTM andBi-LSTM.
It indicates that our proposed modelcould achieve the best performance in multiplelanguages than other neural network methods.
Wedid not compare our system with other systems(Tanev et al, 2009), because they reported the re-sults on a non-standard data set .ModelTrigger Identification Trigger ClassificationP R F P R FLSTM 62.2 52.9 57.2 56.9 32.6 41.6Bi-LSTM 76.2 63.1 68.7 61.5 42.2 50.1HNN 81.4 65.2 71.6 66.3 47.8 55.5Table 4: Results on Spanish event detection.4 Related WorkEvent detection is a fundamental problem in infor-mation extraction and natural language process-ing (Li et al, 2013; Chen et al, 2015), whichaims at detecting the event trigger of a sentence(Ji et al, 2008).
The majority of existing methodsregard this problem as a classification task, anduse machine learning methods with hand-craftedfeatures, such as lexical features (e.g., full word,pos tag), syntactic features (e.g., dependency fea-tures) and external knowledge features (WordNet).There also exists some studies leveraging richerevidences like cross-document (Ji et al, 2008),cross-entity (Hong et al, 2011) and joint inference(Li and Ji, 2014).Despite the effectiveness of feature-based meth-ods, we argue that manually designing featuretemplates is typically labor intensive.
Besides,feature engineering requires expert knowledge andrich external resources, which is not always avail-able for some low-resource languages.
Further-more, a desirable approach should have the abil-ity to automatically learn informative representa-tions from data, so that it could be easily adaptedto different languages.
Recently, neural networkemerges as a powerful way to learn text represen-tation automatically from data and has obtainedpromising performances in a variety of NLP tasks.For event detection, two recent studies (Nguyenand Grishman, 2015; Chen et al, 2015) exploreneural network to learn continuous word represen-tation and regard it as the feature to infer whether aword is a trigger or not.
Nguyen (2015) presenteda convolutional neural network with entity type in-formation and word position information as extrafeatures.
However, their system limits the con-text to a fixed window size which leads the loss ofword semantic representation for long sentences.We introduce a hybrid neural network to learncontinuous word representation.
Compared withfeature-based approaches, the method here doesnot require feature engineering and could be di-rectly applied to different languages.
Comparedwith previous neural models, we keep the advan-tage of convolutional neural network (Nguyen andGrishman, 2015) in capturing local contexts.
Be-sides, we also incorporate a Bi-directional LSTMto model the preceding and following informationof a word as it has been commonly accepted thatLSTM is good at capturing long-term dependen-cies in a sequence (Tang et al, 2015b; Li et al,2015a).5 ConclusionsIn this work, We introduce a hybrid neural net-work model, which incorporates both bidirectionalLSTMs and convolutional neural networks to cap-ture sequence and structure semantic informationfrom specific contexts, for event detection.
Com-pared with traditional event detection methods,our approach does not rely on any linguistic re-sources, thus can be easily applied to any lan-guages.
We conduct experiments on various lan-guages ( English, Chinese and Spanish.
Empiricalresults show our approach achieved state-of-the-art performance in English and competitive resultsin Chinese.
We also find that bi-directional LSTMis powerful for trigger extraction in capturing pre-ceding and following contexts in long distance.6 AcknowledgmentsThe authors give great thanks to Ying Lin (RPI)and Shen Liu for (HIT) the fruitful discussions.We also would like to thank three anonymous re-viewers for their valuable comments and sugges-tions.
RPI co-authors were supported by the U.S.DARPA LORELEI Program No.
HR0011-15-C-0115, DARPA DEFT Program No.
FA8750-13-2-0041 and NSF CAREER Award IIS-1523198.70ReferencesDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2014.
Neural machine translation by jointlylearning to align and translate.
arXiv preprintarXiv:1409.0473.Chen Chen, V Incent Ng, and et al 2012.
Joint model-ing for chinese event extraction with rich linguisticfeatures.
In In COLING.
Citeseer.Yubo Chen, Liheng Xu, Kang Liu, Daojian Zeng,and Jun Zhao.
2015.
Event extraction via dy-namic multi-pooling convolutional neural networks.In Proceedings of the 53rd Annual Meeting of theAssociation for Computational Linguistics and the7th International Joint Conference on Natural Lan-guage Processing, volume 1, pages 167?176.Yu Hong, Jianfeng Zhang, Bin Ma, Jianmin Yao,Guodong Zhou, and Qiaoming Zhu.
2011.
Us-ing cross-entity inference to improve event extrac-tion.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies-Volume 1, pages 1127?1136.
Association for Computational Linguistics.Heng Ji, Ralph Grishman, and et al 2008.
Refiningevent extraction through cross-document inference.In ACL, pages 254?262.Yann LeCun, Yoshua Bengio, and et al 1995.
Convo-lutional networks for images, speech, and time se-ries.
The handbook of brain theory and neural net-works, 3361(10).Qi Li and Heng Ji.
2014.
Incremental joint extractionof entity mentions and relations.
In Proceedings ofthe Association for Computational Linguistics.Qi Li, Heng Ji, and Liang Huang.
2013.
Joint eventextraction via structured prediction with global fea-tures.
In ACL (1), pages 73?82.Jiwei Li, Dan Jurafsky, and Eudard Hovy.
2015a.When are tree structures necessary for deeplearning of representations?
arXiv preprintarXiv:1503.00185.Jiwei Li, Minh-Thang Luong, and Dan Juraf-sky.
2015b.
A hierarchical neural autoencoderfor paragraphs and documents.
arXiv preprintarXiv:1506.01057.Shasha Liao, Ralph Grishman, and et al 2010.
Us-ing document level cross-event inference to improveevent extraction.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Lin-guistics, pages 789?797.
Association for Computa-tional Linguistics.Ting Liu, Wanxiang Che, and Zhenghua Li.
2011.Language technology platform.
Journal of ChineseInformation Processing, 25(6):53?62.Yang Liu, Furu Wei, Sujian Li, Heng Ji, Ming Zhou,and Houfeng Wang.
2015.
A dependency-basedneural network for relation classification.
arXivpreprint arXiv:1507.04646.Fan Miao and Ralph Grishman.
2015.
Improving eventdetection with active learning.
In EMNLP.Tomas Mikolov, Martin Karafi?at, Lukas Burget, JanCernock`y, and Sanjeev Khudanpur.
2010.
Recur-rent neural network based language model.
In IN-TERSPEECH, volume 2, page 3.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representa-tions of words and phrases and their compositional-ity.
In Advances in neural information processingsystems, pages 3111?3119.Thien Huu Nguyen and Ralph Grishman.
2015.
Eventdetection and domain adaptation with convolutionalneural networks.
Volume 2: Short Papers, page 365.Mike Schuster, Kuldip K Paliwal, and et al 1997.Bidirectional recurrent neural networks.
Signal Pro-cessing, IEEE Transactions on, 45(11):2673?2681.Hristo Tanev, Vanni Zavarella, Jens Linge, MijailKabadjov, Jakub Piskorski, Martin Atkinson, andRalf Steinberger.
2009.
Exploiting machine learn-ing techniques to build an event extraction systemfor portuguese and spanish.
Linguam?atica, 1(2):55?66.Duyu Tang, Bing Qin, and Ting Liu.
2015a.
Docu-ment modeling with gated recurrent neural networkfor sentiment classification.
EMNLP.Duyu Tang, Bing Qin, and Ting Liu.
2015b.
Docu-ment modeling with gated recurrent neural networkfor sentiment classification.
In Proceedings of the2015 Conference on Empirical Methods in NaturalLanguage Processing, pages 1422?1432.Matthew D Zeiler.
2012.
Adadelta: an adaptive learn-ing rate method.
arXiv preprint arXiv:1212.5701.Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,Jun Zhao, et al 2014.
Relation classification viaconvolutional deep neural network.
In COLING,pages 2335?2344.71
