Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 987?996,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsClassifying Arguments by SchemeVanessa Wei FengDepartment of Computer ScienceUniversity of TorontoToronto, ON, M5S 3G4, Canadaweifeng@cs.toronto.eduGraeme HirstDepartment of Computer ScienceUniversity of TorontoToronto, ON, M5S 3G4, Canadagh@cs.toronto.eduAbstractArgumentation schemes are structures or tem-plates for various kinds of arguments.
Giventhe text of an argument with premises and con-clusion identified, we classify it as an instanceof one of five common schemes, using featuresspecific to each scheme.
We achieve accura-cies of 63?91% in one-against-others classifi-cation and 80?94% in pairwise classification(baseline = 50% in both cases).1 IntroductionWe investigate a new task in the computational anal-ysis of arguments: the classification of argumentsby the argumentation schemes that they use.
An ar-gumentation scheme, informally, is a framework orstructure for a (possibly defeasible) argument; wewill give a more-formal definition and examples inSection 3.
Our work is motivated by the need to de-termine the unstated (or implicitly stated) premisesthat arguments written in natural language normallydraw on.
Such premises are called enthymemes.For instance, the argument in Example 1 consistsof one explicit premise (the first sentence) and a con-clusion (the second sentence):Example 1 [Premise:] The survival of the entireworld is at stake.
[Conclusion:] The treaties and covenants aimingfor a world free of nuclear arsenals and other con-ventional and biological weapons of mass destruc-tion should be adhered to scrupulously by all na-tions.Another premise is left implicit ?
?Adhering tothose treaties and covenants is a means of realizingsurvival of the entire world?.
This proposition is anenthymeme of this argument.Our ultimate goal is to reconstruct the en-thymemes in an argument, because determiningthese unstated assumptions is an integral part of un-derstanding, supporting, or attacking an entire argu-ment.
Hence reconstructing enthymemes is an im-portant problem in argument understanding.
We be-lieve that first identifying the particular argumenta-tion scheme that an argument is using will help tobridge the gap between stated and unstated proposi-tions in the argument, because each argumentationscheme is a relatively fixed ?template?
for arguing.That is, given an argument, we first classify its ar-gumentation scheme; then we fit the stated proposi-tions into the corresponding template; and from thiswe infer the enthymemes.In this paper, we present an argument schemeclassification system as a stage following argumentdetection and proposition classification.
First in Sec-tion 2 and Section 3, we introduce the backgroundto our work, including related work in this field,the two core concepts of argumentation schemes andscheme-sets, and the Araucaria dataset.
In Section 4and Section 5 we present our classification system,including the overall framework, data preprocessing,feature selection, and the experimental setups.
Inthe remaining section, we present the essential ap-proaches to solve the leftover problems of this paperwhich we will study in our future work, and discussthe experimental results, and potential directions forfuture work.9872 Related workArgumentation has not received a great deal of at-tention in computational linguistics, although it hasbeen a topic of interest for many years.
Cohen(1987) presented a computational model of argu-mentative discourse.
Dick (1987; 1991a; 1991b) de-veloped a representation for retrieval of judicial de-cisions by the structure of their legal argument ?
anecessity for finding legal precedents independent oftheir domain.
However, at that time no corpus of ar-guments was available, so Dick?s system was purelytheoretical.
Recently, the Araucaria project at Uni-versity of Dundee has developed a software tool formanual argument analysis, with a point-and-click in-terface for users to reconstruct and diagram an ar-gument (Reed and Rowe, 2004; Rowe and Reed,2008).
The project also maintains an online repos-itory, called AraucariaDB, of marked-up naturallyoccurring arguments collected by annotators world-wide, which can be used as an experimental corpusfor automatic argumentation analysis (for details seeSection 3.2).Recent work on argument interpretation includesthat of George, Zukerman, and Nieman (2007), whointerpret constructed-example arguments (not natu-rally occurring text) as Bayesian networks.
Othercontemporary research has looked at the automaticdetection of arguments in text and the classificationof premises and conclusions.
The work closest toours is perhaps that of Mochales and Moens (2007;2008; 2009a; 2009b).
In their early work, they fo-cused on automatic detection of arguments in legaltexts.
With each sentence represented as a vector ofshallow features, they trained a multinomial na?
?veBayes classifier and a maximum entropy model onthe Araucaria corpus, and obtained a best averageaccuracy of 73.75%.
In their follow-up work, theytrained a support vector machine to further classifyeach argumentative clause into a premise or a con-clusion, with an F1 measure of 68.12% and 74.07%respectively.
In addition, their context-free grammarfor argumentation structure parsing obtained around60% accuracy.Our work is ?downstream?
from that of Mochalesand Moens.
Assuming the eventual success of their,or others?, research program on detecting and clas-sifying the components of an argument, we seek todetermine how the pieces fit together as an instanceof an argumentation scheme.3 Argumentation schemes, scheme-sets,and annotation3.1 Definition and examplesArgumentation schemes are structures or templatesfor forms of arguments.
The arguments need not bedeductive or inductive; on the contrary, most argu-mentation schemes are for presumptive or defeasiblearguments (Walton and Reed, 2002).
For example,argument from cause to effect is a commonly usedscheme in everyday arguments.
A list of such argu-mentation schemes is called a scheme-set.It has been shown that argumentation schemesare useful in evaluating common arguments as falla-cious or not (van Eemeren and Grootendorst, 1992).In order to judge the weakness of an argument, a setof critical questions are asked according to the par-ticular scheme that the argument is using, and theargument is regarded as valid if it matches all therequirements imposed by the scheme.Walton?s set of 65 argumentation schemes (Wal-ton et al, 2008) is one of the best-developed scheme-sets in argumentation theory.
The five schemes de-fined in Table 1 are the most commonly used ones,and they are the focus of the scheme classificationsystem that we will describe in this paper.3.2 Araucaria datasetOne of the challenges for automatic argumentationanalysis is that suitable annotated corpora are stillvery rare, in spite of work by many researchers.In the work described here, we use the Araucariadatabase1, an online repository of arguments, as ourexperimental dataset.
Araucaria includes approxi-mately 660 manually annotated arguments from var-ious sources, such as newspapers and court cases,and keeps growing.
Although Araucaria has sev-eral limitations, such as rather small size and lowagreement among annotators2, it is nonetheless oneof the best argumentative corpora available to date.1http://araucaria.computing.dundee.ac.uk/doku.php#araucaria argumentation corpus2The developers of Araucaria did not report on inter-annotator agreement, probably because some arguments are an-notated by only one commentator.988Argument from examplePremise: In this particular case, the individual ahas property F and also property G.Conclusion: Therefore, generally, if x has prop-erty F, then it also has property G.Argument from cause to effectMajor premise: Generally, if A occurs, then B will(might) occur.Minor premise: In this case, A occurs (might oc-cur).Conclusion: Therefore, in this case, B will(might) occur.Practical reasoningMajor premise: I have a goal G.Minor premise: Carrying out action A is a meansto realize G.Conclusion: Therefore, I ought (practicallyspeaking) to carry out this action A.Argument from consequencesPremise: If A is (is not) brought about, good (bad)consequences will (will not) plausibly occur.Conclusion: Therefore, A should (should not) bebrought about.Argument from verbal classificationIndividual premise: a has a particular property F.Classification premise: For all x, if x has propertyF, then x can be classified as having propertyG.Conclusion: Therefore, a has property G.Table 1: The five most frequent schemes and their defini-tions in Walton?s scheme-set.Arguments in Araucaria are annotated in a XML-based format called ?AML?
(Argument MarkupLanguage).
A typical argument (see Example 2)consists of several AU nodes.
Each AU node is acomplete argument unit, composed of a conclusionproposition followed by optional premise proposi-tion(s) in a linked or convergent structure.
Each ofthese propositions can be further defined as a hier-archical collection of smaller AUs.
INSCHEME isthe particular scheme (e.g., ?Argument from Con-sequences?)
of which the current proposition is amember; enthymemes that have been made explicitare annotated as ?missing = yes?.Example 2 Example of argument markup fromAraucaria<TEXT>If we stop the free creation of art, we will stopthe free viewing of art.</TEXT><AU><PROP identifier="C" missing="yes"><PROPTEXT offset="-1">The prohibition of the free creation of art shouldnot be brought about.</PROPTEXT><INSCHEME scheme="Argument from Consequences"schid="0" /></PROP><LA><AU><PROP identifier="A" missing="no"><PROPTEXT offset="0">If we stop the free creation of art, we willstop the free viewing of art.</PROPTEXT><INSCHEME scheme="Argument from Consequences"schid="0" /></PROP></AU><AU><PROP identifier="B" missing="yes"><PROPTEXT offset="-1">The prohibition of free viewing of art is notacceptable.</PROPTEXT><INSCHEME scheme="Argument from Consequences"schid="0" /></PROP></AU></LA></AU>There are three scheme-sets used in the anno-tations in Araucaria: Walton?s scheme-set, Katzavand Reed?s (2004) scheme-set, and Pollock?s (1995)scheme-set.
Each of these has a different set ofschemes; and most arguments in Araucaria aremarked up according to only one of them.
Ourexperimental dataset is composed of only thosearguments annotated in accordance with Walton?sscheme-set, within which the five schemes shown inTable 1 constitute 61% of the total occurrences.4 Methods4.1 Overall frameworkAs we noted above, our ultimate goal is to recon-struct enthymemes, the unstated premises, in an ar-gument by taking advantage of the stated proposi-tions; and in order to achieve this goal we need tofirst determine the particular argumentation schemethat the argument is using.
This problem is de-picted in Figure 1.
Our scheme classifier is thedashed round-cornered rectangle portion of this989Detectingargumentative textARGUMENTATIVESEGMENTPremise /conclusionclassifierCONCLUSIONPREMISE #1PREMISE #2Scheme classifierTEXTARGUMENTATIONSCHEMEArgumenttemplate fitterCONSTRUCTEDENTHYMEMEFigure 1: Overall framework of this research.overall framework: its input is the extracted con-clusion and premise(s) determined by an argumentdetector, followed by a premise / conclusion classi-fier, given an unknown text as the input to the entiresystem.
And the portion below the dashed round-rectangle represents our long-term goal ?
to recon-struct the implicit premise(s) in an argument, givenits argumentation scheme and its explicit conclusionand premise(s) as input.
Since argument detectionand classification are not the topic of this paper, weassume here that the input conclusion and premise(s)have already been retrieved, segmented, and classi-fied, as for example by the methods of Mochales andMoens (see Section 2 above).
And the scheme tem-plate fitter is the topic of our on-going work.4.2 Data preprocessingFrom all arguments in Araucaria, we first ex-tract those annotated in accordance with Walton?sscheme-set.
Then we break each complex AUnode into several simple AUs where no conclusionor premise proposition nodes have embedded AUnodes.
From these generated simple arguments, weextract those whose scheme falls into one of the fivemost frequent schemes as described in Table 1.
Fur-thermore, we remove all enthymemes that have beeninserted by the annotator and ignore any argumentwith a missing conclusion, since the input to our pro-posed classifier, as depicted in Figure 1, cannot haveany access to unstated argumentative propositions.The resulting preprocessed dataset is composed of393 arguments, of which 149, 106, 53, 44, and 41respectively belong to the five schemes in the ordershown in Table 1.4.3 Feature selectionThe features used in our work fall into two cat-egories: general features and scheme-specific fea-tures.4.3.1 General featuresGeneral features are applicable to arguments belong-ing to any of the five schemes (shown in Table 2).For the features conLoc, premLoc, gap, andlenRat, we have two versions, differing in termsof their basic measurement unit: sentence-basedand token-based.
The final feature, type, indicateswhether the premises contribute to the conclusionin a linked or convergent order.
A linked argument(LA) is one that has two or more inter-dependentpremise propositions, all of which are necessary tomake the conclusion valid, whereas in a conver-gent argument (CA) exactly one premise proposi-tion is sufficient to do so.
Since it is observed thatthere exists a strong correlation between type andthe particular scheme employed while arguing, webelieve type can be a good indicator of argumenta-tion scheme.
However, although this feature is avail-able to us because it is included in the Araucaria an-notations, its value cannot be obtained from raw textas easily as other features mentioned above; but it ispossible that we will in the future be able to deter-mine it automatically by taking advantage of somescheme-independent cues such as the discourse re-lation between the conclusion and the premises.4.3.2 Scheme-specific featuresScheme-specific features are different for eachscheme, since each scheme has its own cue phrasesor patterns.
The features for each scheme are shownin Table 3 (for complete lists of features see Feng(2010)).
In our experiments in Section 5 below, allthese features are computed for all arguments; but990conLoc: the location (in token or sentence) of theconclusion in the text.premLoc: the location (in token or sentence) ofthe first premise proposition.conFirst: whether the conclusion appears beforethe first premise proposition.gap: the interval (in token or sentence) betweenthe conclusion and the first premise proposi-tion.lenRat: the ratio of the length (in token or sen-tence) of the premise(s) to that of the conclu-sion.numPrem: the number of explicit premise propo-sitions (PROP nodes) in the argument.type: type of argumentation structure, i.e., linkedor convergent.Table 2: List of general features.the features for any particular scheme are used onlywhen it is the subject of a particular task.
For ex-ample, when we classify argument from examplein a one-against-others setup, we use the scheme-specific features of that scheme for all arguments;when we classify argument from example againstargument from cause to effect, we use the scheme-specific features of those two schemes.For the first three schemes (argument from ex-ample, argument from cause to effect, and practi-cal reasoning), the scheme-specific features are se-lected cue phrases or patterns that are believed to beindicative of each scheme.
Since these cue phrasesand patterns have differing qualities in terms of theirprecision and recall, we do not treat them all equally.For each cue phrase or pattern, we compute ?confi-dence?, the degree of belief that the argument of in-terest belongs to a particular scheme, using the dis-tribution characteristics of the cue phrase or patternin the corpus, as described below.For each argument A, a vector CV = {c1, c2, c3}is added to its feature set, where each ci indicatesthe ?confidence?
of the existence of the specific fea-tures associated with each of the first three schemes,schemei.
This is defined in Equation 1:ci =1Nmi?k=1(P (schemei|cpk) ?
dik) (1)Argument from example8 keywords and phrases including for example,such as, for instance, etc.
; 3 punctuation cues: ?:?,?
;?, and ??
?.Argument from cause to effect22 keywords and simple cue phrases including re-sult, related to, lead to, etc.
; 10 causal and non-causal relation patterns extracted from WordNet(Girju, 2003).Practical reasoning28 keywords and phrases including want, aim, ob-jective, etc.
; 4 modal verbs: should, could, must,and need; 4 patterns including imperatives and in-finitives indicating the goal of the speaker.Argument from consequencesThe counts of positive and negative propositionsin the conclusion and premises, calculated fromthe General Inquirer2.Argument from verbal classificationThe maximal similarity between the central wordpairs extracted from the conclusion and thepremise; the counts of copula, expletive, and neg-ative modifier dependency relations returned bythe Stanford parser3 in the conclusion and thepremise.2 http://www.wjh.harvard.edu/?inquirer/3 http://nlp.stanford.edu/software/lex-parser.shtmlTable 3: List of scheme-specific features.Here mi is the number of scheme-specific cuephrases designed for schemei; P (schemei|cpk) is theprior probability that the argument A actually be-longs to schemei, given that some particular cuephrase cpk is found in A; dik is a value indicat-ing whether cpk is found in A; and the normaliza-tion factor N is the number of scheme-specific cuephrase patterns designed for schemei with at leastone support (at least one of the arguments belongingto schemei contains that cue phrase).
There are twoways to calculate dik, Boolean and count: in Booleanmode, dik is treated as 1 if A matches cpk; in countmode, dik equals to the number of times A matchescpk; and in both modes, dik is treated as 0 if cpk isnot found inA.991For argument from consequences, since the arguerhas an obvious preference for some particular con-sequence, sentiment orientation can be a good in-dicator for this scheme, which is quantified by thecounts of positive and negative propositions in theconclusion and premise.For argument from verbal classification, there ex-ists a hypernymy-like relation between some pair ofpropositions (entities, concepts, or actions) locatedin the conclusion and the premise respectively.
Theexistence of such a relation is quantified by the max-imal Jiang-Conrath Similarity (Jiang and Conrath,1997) between the ?central word?
pairs extractedfrom the conclusion and the premise.
We parse eachsentence of the argument with the Stanford depen-dency parser, and a word or phrase is considered tobe a central word if it is the dependent or governor ofseveral particular dependency relations, which basi-cally represents the attribute or the action of an en-tity in a sentence, or the entity itself.
For example,if a word or phrase is the dependent of the depen-dency relation agent, it is therefore considered as a?central word?.
In addition, an arguer tends to useseveral particular syntactic structures (copula, exple-tive, and negative modifier) when using this scheme,which can be quantified by the counts of those spe-cial relations in the conclusion and the premise(s).5 Experiments5.1 TrainingWe experiment with two kinds of classification: one-against-others and pairwise.
We build a prunedC4.5 decision tree (Quinlan, 1993) for each differentclassification setup, implemented by Weka Toolkit3.65 (Hall et al, 2009).One-against-others classification A one-against-others classifier is constructed for each of the fivemost frequent schemes, using the general featuresand the scheme-specific features for the scheme ofinterest.
For each classifier, there are two possi-ble outcomes: target scheme and other; 50% of thetraining dataset is arguments associated with tar-get scheme, while the rest is arguments of all theother schemes, which are treated as other.
One-against-other classification thus tests the effective-5http://cs.waikato.ac.nz/ml/wekaness of each scheme?s specific features.Pairwise classification A pairwise classifier isconstructed for each of the ten possible pairingsof the five schemes, using the general features andthe scheme-specific features of the two schemes inthe pair.
For each of the ten classifiers, the train-ing dataset is divided equally into arguments be-longing to scheme1 and arguments belonging toscheme2, where scheme1 and scheme2 are two dif-ferent schemes among the five.
Only features asso-ciated with scheme1 and scheme2 are used.5.2 EvaluationWe experiment with different combinations of gen-eral features and scheme-specific features (discussedin Section 4.3).
To evaluate each experiment, weuse the average accuracy over 10 pools of randomlysampled data (each with baseline at 50%6) with 10-fold cross-validation.6 ResultsWe first present the best average accuracy (BAA) ofeach classification setup.
Then we demonstrate theimpact of the feature type (convergent or linked ar-gument) on BAAs for different classification setups,since we believe type is strongly correlated withthe particular argumentation scheme and its value isthe only one directly retrieved from the annotationsof the training corpus.
For more details, see Feng(2010).6.1 BAAs of each classification setuptarget scheme BAA dik base typeexample 90.6 count token yescause 70.4 Boolean/ counttoken noreasoning 90.8 count sentence yesconsequences 62.9 ?
sentence yesclassification 63.2 ?
token yesTable 4: Best average accuracies (BAAs) (%) of one-against-others classification.6We also experiment with using general features only, butthe results are consistently below or around the sampling base-line of 50%; therefore, we do not use them as a baseline here.992example cause reason-ingconse-quencescause 80.6reasoning 93.1 94.2consequences 86.9 86.7 97.9classification 86.0 85.6 98.3 64.2Table 5: Best average accuracies (BAAs) (%) of pairwiseclassification.Table 4 presents the best average accuracies ofone-against-others classification for each of the fiveschemes.
The subsequent three columns list theparticular strategies of features incorporation underwhich those BAAs are achieved (the complete set ofpossible choices is given in Section 4.3.):?
dik: Boolean or count ?
the strategy of com-bining scheme-specific cue phrases or patternsusing either Boolean or count for dik.?
base: sentence or token ?
the basic unit of ap-plying location- or length-related general fea-tures.?
type: yes or no ?
whether type (convergent orlinked argument) is incorporated into the fea-ture set.As Table 4 shows, one-against-others classifica-tion achieves high accuracy for argument from ex-ample and practical reasoning: 90.6% and 90.8%.The BAA of argument from cause to effect is onlyjust over 70%.
However, with the last two schemes(argument from consequences and argument fromverbal classification), accuracy is only in the low60s; there is little improvement of our system overthe majority baseline of 50%.
This is probably dueat least partly to the fact that these schemes do nothave such obvious cue phrases or patterns as theother three schemes which therefore may requiremore world knowledge encoded, and also becausethe available training data for each is relatively small(44 and 41 instances, respectively).
The BAA foreach scheme is achieved with inconsistent choicesof base and dik, but the accuracies that resulted fromdifferent choices vary only by very little.Table 5 shows that our system is able to correctlydifferentiate between most of the different schemepairs, with accuracies as high as 98%.
It has poorperformance (64.0%) only for the pair argumentfrom consequences and argument from verbal clas-sification; perhaps not coincidentally, these are thetwo schemes for which performance was poorest inthe one-against-others task.6.2 Impact of type on classification accuracyAs we can see from Table 6, for one-against-othersclassifications, incorporating type into the featurevectors improves classification accuracy in mostcases: the only exception is that the best average ac-curacy of one-against-others classification betweenargument from cause to effect and others is obtainedwithout involving type into the feature vector ?but the difference is negligible, i.e., 0.5 percent-age points with respect to the average difference.Type also has a relatively small impact on argumentfrom verbal classification (2.6 points), compared toits impact on argument from example (22.3 points),practical reasoning (8.1 points), and argument fromconsequences (7.5 points), in terms of the maximaldifferences.Similarly, for pairwise classifications, as shownin Table 7, type has significant impact on BAAs, es-pecially on the pairs of practical reasoning versusargument from cause to effect (17.4 points), prac-tical reasoning versus argument from example (22.6points), and argument from verbal classification ver-sus argument from example (20.2 points), in termsof the maximal differences; but it has a relativelysmall impact on argument from consequences ver-sus argument from cause to effect (0.8 point), andargument from verbal classification versus argumentfrom consequences (1.1 points), in terms of averagedifferences.7 Future WorkIn future work, we will look at automatically clas-sifying type (i.e., whether an argument is linked orconvergent), as type is the only feature directly re-trieved from annotations in the training corpus thathas a strong impact on improving classification ac-curacies.Automatically classifying type will not be easy,because sometimes it is subjective to say whether apremise is sufficient by itself to support the conclu-sion or not, especially when the argument is about993target scheme BAA-t BAA-no t max diff min diff avg diffexample 90.6 71.6 22.3 10.6 14.7cause 70.4 70.9 ?0.5 ?0.6 ?0.5reasoning 90.8 83.2 8.1 7.5 7.7consequences 62.9 61.9 7.5 ?0.6 4.2classification 63.2 60.7 2.6 0.4 2.0Table 6: Accuracy (%) with and without type in one-against-others classification.
BAA-t is best average accuracy withtype, and BAA-no t is best average accuracy without type.
max diff, min diff, and avg diff are maximal, minimal, andaverage differences between each experimental setup with type and without type while the remaining conditions arethe same.scheme1 scheme2 BAA-t BAA-no t max diff min diff avg diffcause example 80.6 69.7 10.9 7.1 8.7reasoning example 93.1 73.1 22.8 19.1 20.1reasoning cause 94.2 80.5 17.4 8.7 13.9consequences example 86.9 76.0 13.8 6.9 10.1consequences cause 87.7 86.7 3.8 ?1.5 ?0.1consequences reasoning 97.9 97.9 10.6 0.0 0.8classification example 86.0 74.6 20.2 3.7 7.1classification cause 85.6 76.8 9.0 3.7 7.1classification reasoning 98.3 89.3 8.9 4.2 8.3classification consequences 64.0 60.0 6.5 ?1.3 1.1Table 7: Accuracy (%) with and without type in pairwise classification.
Column headings have the same meanings asin Table 6.personal opinions or judgments.
So for this task,we will initially focus on arguments that are (or atleast seem to be) empirical or objective rather thanvalue-based.
It will also be non-trivial to deter-mine whether an argument is convergent or linked?
whether the premises are independent of one an-other or not.
Cue words and discourse relations be-tween the premises and the conclusion will be onehelpful factor; for example, besides generally flagsan independent premise.
And one premise may beregarded as linked to another if either would becomean enthymeme if deleted; but determining this in thegeneral case, without circularity, will be difficult.We will also work on the argument template fitter,which is the final component in our overall frame-work.
The task of the argument template fitter is tomap each explicitly stated conclusion and premiseinto the corresponding position in its scheme tem-plate and to extract the information necessary for en-thymeme reconstruction.
Here we propose a syntax-based approach for this stage, which is similar totasks in information retrieval.
This can be best ex-plained by the argument in Example 1, which usesthe particular argumentation scheme practical rea-soning.We want to fit the Premise and the Conclusion ofthis argument into the Major premise and the Con-clusion slots of the definition of practical reasoning(see Table 1), and construct the following conceptualmapping relations:1.
Survival of the entire world ??
a goal G2.
Adhering to the treaties and covenants aimingfor a world free of nuclear arsenals and otherconventional and biological weapons of massdestruction ??
action AThereby we will be able to reconstruct the missingMinor premise ?
the enthymeme in this argument:Carrying out adhering to the treaties andcovenants aiming for a world free of nucleararsenals and other conventional and biological994weapons of mass destruction is a means of real-izing survival of the entire world.8 ConclusionThe argumentation scheme classification system thatwe have presented in this paper introduces a newtask in research on argumentation.
To the best ofour knowledge, this is the first attempt to classifyargumentation schemes.In our experiments, we have focused on the fivemost frequently used schemes in Walton?s scheme-set, and conducted two kinds of classification: inone-against-others classification, we achieved over90% best average accuracies for two schemes, withother three schemes in the 60s to 70s; and in pair-wise classification, we obtained 80% to 90% bestaverage accuracies for most scheme pairs.
The poorperformance of our classification system on otherexperimental setups is partly due to the lack of train-ing examples or to insufficient world knowledge.Completion of our scheme classification systemwill be a step towards our ultimate goal of recon-structing the enthymemes in an argument by the pro-cedure depicted in Figure 1.
Because of the signifi-cance of enthymemes in reasoning and arguing, thisis crucial to the goal of understanding arguments.But given the still-premature state of research of ar-gumentation in computational linguistics, there aremany practical issues to deal with first, such as theconstruction of richer training corpora and improve-ment of the performance of each step in the proce-dure.AcknowledgmentsThis work was financially supported by the Natu-ral Sciences and Engineering Research Council ofCanada and by the University of Toronto.
We aregrateful to Suzanne Stevenson for helpful commentsand suggestions.ReferencesRobin Cohen.
1987.
Analyzing the structure of ar-gumentative discourse.
Computational Linguistics,13(1?2):11?24.Judith Dick.
1987.
Conceptual retrieval and case law.In Proceedings, First International Conference on Ar-tificial Intelligence and Law, pages 106?115, Boston,May.Judith Dick.
1991a.
A Conceptual, Case-relation Repre-sentation of Text for Intelligent Retrieval.
Ph.D. thesis,Faculty of Library and Information Science, Univer-sity of Toronto, April.Judith Dick.
1991b.
Representation of legal text for con-ceptual retrieval.
In Proceedings, Third InternationalConference on Artificial Intelligence and Law, pages244?252, Oxford, June.Vanessa Wei Feng.
2010.
Classifying argu-ments by scheme.
Technical report, Depart-ment of Computer Science, University of Toronto,November.
http://ftp.cs.toronto.edu/pub/gh/Feng-MSc-2010.pdf.Sarah George, Ingrid Zukerman, and Michael Niemann.2007.
Inferences, suppositions and explanatory exten-sions in argument interpretation.
User Modeling andUser-Adapted Interaction, 17(5):439?474.Roxana Girju.
2003.
Automatic detection of causal re-lations for question answering.
In Proceedings of theACL 2003 Workshop on Multilingual Summarizationand Question Answering, pages 76?83, Morristown,NJ, USA.
Association for Computational Linguistics.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA data mining software: an update.SIGKDD Explorations Newsletter, 11(1):10?18.Jay J. Jiang and David W. Conrath.
1997.
Semanticsimilarity based on corpus statistics and lexical taxon-omy.
In International Conference Research on Com-putational Linguistics (ROCLING X), pages 19?33.Joel Katzav and Chris Reed.
2004.
On argumentationschemes and the natural classification of arguments.Argumentation, 18(2):239?259.Raquel Mochales and Marie-Francine Moens.
2008.Study on the structure of argumentation in case law.
InProceedings of the 2008 Conference on Legal Knowl-edge and Information Systems, pages 11?20, Amster-dam, The Netherlands.
IOS Press.Raquel Mochales and Marie-Francine Moens.
2009a.Argumentation mining: the detection, classificationand structure of arguments in text.
In ICAIL ?09: Pro-ceedings of the 12th International Conference on Arti-ficial Intelligence and Law, pages 98?107, New York,NY, USA.
ACM.Raquel Mochales and Marie-Francine Moens.
2009b.Automatic argumentation detection and its role in lawand the semantic web.
In Proceedings of the 2009Conference on Law, Ontologies and the Semantic Web,pages 115?129, Amsterdam, The Netherlands.
IOSPress.Marie-Francine Moens, Erik Boiy, Raquel MochalesPalau, and Chris Reed.
2007.
Automatic detection995of arguments in legal texts.
In ICAIL ?07: Proceed-ings of the 11th International Conference on ArtificialIntelligence and Law, pages 225?230, New York, NY,USA.
ACM.John L. Pollock.
1995.
Cognitive Carpentry: ABlueprint for How to Build a Person.
Bradford Books.The MIT Press, May.J.
Ross Quinlan.
1993.
C4.5: Programs for machinelearning.
Machine Learning, 16(3):235?240.Chris Reed and Glenn Rowe.
2004.
Araucaria: Softwarefor argument analysis, diagramming and representa-tion.
International Journal of Artificial IntelligenceTools, 14:961?980.Glenn Rowe and Chris Reed.
2008.
Argument diagram-ming: The Araucaria project.
In Knowledge Cartog-raphy, pages 163?181.
Springer London.Frans H. van Eemeren and Rob Grootendorst.
1992.Argumentation, Communication, and Fallacies: APragma-Dialectical Perspective.
Routledge.Douglas Walton and Chris Reed.
2002.
Argumenta-tion schemes and defeasible inferences.
In Workshopon Computational Models of Natural Argument, 15thEuropean Conference on Artificial Intelligence, pages11?20, Amsterdam, The Netherlands.
IOS Press.Douglas Walton, Chris Reed, and Fabrizio Macagno.2008.
Argumentation Schemes.
Cambridge UniversityPress.996
