Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 297?305,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPRobust Machine Translation Evaluation with Entailment Features?Sebastian Pado?Stuttgart Universitypado@ims.uni-stuttgart.deMichel Galley, Dan Jurafsky, Chris ManningStanford University{mgalley,jurafsky,manning}@stanford.eduAbstractExisting evaluation metrics for machine translationlack crucial robustness: their correlations with hu-man quality judgments vary considerably across lan-guages and genres.
We believe that the main reasonis their inability to properly capture meaning: A goodtranslation candidate means the same thing as thereference translation, regardless of formulation.
Wepropose a metric that evaluates MT output based ona rich set of features motivated by textual entailment,such as lexical-semantic (in-)compatibility and ar-gument structure overlap.
We compare this metricagainst a combination metric of four state-of-the-art scores (BLEU, NIST, TER, and METEOR) intwo different settings.
The combination metric out-performs the individual scores, but is bested by theentailment-based metric.
Combining the entailmentand traditional features yields further improvements.1 IntroductionConstant evaluation is vital to the progress of ma-chine translation (MT).
Since human evaluation iscostly and difficult to do reliably, a major focus ofresearch has been on automatic measures of MTquality, pioneered by BLEU (Papineni et al, 2002)and NIST (Doddington, 2002).
BLEU and NISTmeasure MT quality by using the strong correla-tion between human judgments and the degree ofn-gram overlap between a system hypothesis trans-lation and one or more reference translations.
Theresulting scores are cheap and objective.However, studies such as Callison-Burch et al(2006) have identified a number of problems withBLEU and related n-gram-based scores: (1) BLEU-like metrics are unreliable at the level of individualsentences due to data sparsity; (2) BLEU metricscan be ?gamed?
by permuting word order; (3) forsome corpora and languages, the correlation to hu-man ratings is very low even at the system level;(4) scores are biased towards statistical MT; (5) thequality gap between MT and human translations isnot reflected in equally large BLEU differences.
?This paper is based on work funded by the Defense Ad-vanced Research Projects Agency through IBM.
The contentdoes not necessarily reflect the views of the U.S. Government,and no official endorsement should be inferred.This is problematic, but not surprising: The met-rics treat any divergence from the reference as anegative, while (computational) linguistics has longdealt with linguistic variation that preserves themeaning, usually called paraphrase, such as:(1) HYP: However, this was declared terrorismby observers and witnesses.REF: Nevertheless, commentators as well aseyewitnesses are terming it terrorism.A number of metrics have been designed to accountfor paraphrase, either by making the matching moreintelligent (TER, Snover et al (2006)), or by usinglinguistic evidence, mostly lexical similarity (ME-TEOR, Banerjee and Lavie (2005); MaxSim, Chanand Ng (2008)), or syntactic overlap (Owczarzak etal.
(2008); Liu and Gildea (2005)).
Unfortunately,each metrics tend to concentrate on one particu-lar type of linguistic information, none of whichalways correlates well with human judgments.Our paper proposes two strategies.
We first ex-plore the combination of traditional scores into amore robust ensemble metric with linear regression.Our second, more fundamental, strategy replacesthe use of loose surrogates of translation qualitywith a model that attempts to comprehensively as-sess meaning equivalence between references andMT hypotheses.
We operationalize meaning equiv-alence by bidirectional textual entailment (RTE,Dagan et al (2005)), and thus predict the qual-ity of MT hypotheses with a rich RTE feature set.The entailment-based model goes beyond existingword-level ?semantic?
metrics such as METEORby integrating phrasal and compositional aspectsof meaning equivalence, such as multiword para-phrases, (in-)correct argument and modificationrelations, and (dis-)allowed phrase reorderings.
Wedemonstrate that the resulting metric beats both in-dividual and combined traditional MT metrics.
Thecomplementary features of both metric types canbe combined into a joint, superior metric.297HYP: Three aid workers were kidnapped.REF: Three aid workers were kidnapped by pirates.no entailment entailmentHYP: The virus did not infect anybody.REF: No one was infected by the virus.entailmententailmentFigure 1: Entailment status between an MT systemhypothesis and a reference translation for equiva-lent (top) and non-equivalent (bottom) translations.2 Regression-based MT Quality PredictionCurrent MTmetrics tend to focus on a single dimen-sion of linguistic information.
Since the importanceof these dimensions tends not to be stable acrosslanguage pairs, genres, and systems, performanceof these metrics varies substantially.
A simple strat-egy to overcome this problem could be to combinethe judgments of different metrics.
For example,Paul et al (2007) train binary classifiers on a fea-ture set formed by a number of MT metrics.
Wefollow a similar idea, but use a regularized linearregression to directly predict human ratings.Feature combination via regression is a super-vised approach that requires labeled data.
As weshow in Section 5, this data is available, and theresulting model generalizes well from relativelysmall amounts of training data.3 Textual Entailment vs. MT EvaluationOur novel approach to MT evaluation exploits thesimilarity between MT evaluation and textual en-tailment (TE).
TE was introduced by Dagan etal.
(2005) as a concept that corresponds moreclosely to ?common sense?
reasoning patterns thanclassical, strict logical entailment.
Textual entail-ment is defined informally as a relation betweentwo natural language sentences (a premise P anda hypothesis H) that holds if ?a human reading Pwould infer that H is most likely true?.
Knowledgeabout entailment is beneficial for NLP tasks such asQuestion Answering (Harabagiu and Hickl, 2006).The relation between textual entailment and MTevaluation is shown in Figure 1.
Perfect MT outputand the reference translation entail each other (top).Translation problems that impact semantic equiv-alence, e.g., deletion or addition of material, canbreak entailment in one or both directions (bottom).On the modelling level, there is common groundbetween RTE and MT evaluation: Both have todistinguish between valid and invalid variation todetermine whether two texts convey the same in-formation or not.
For example, to recognize thebidirectional entailment in Ex.
(1), RTE must ac-count for the following reformulations: synonymy(However/Nevertheless), more general semanticrelatedness (observers/commentators), phrasal re-placements (and/as well as), and an active/passivealternation that implies structural change (is de-clared/are terming).
This leads us to our main hy-pothesis: RTE features are designed to distinguishmeaning-preserving variation from true divergenceand are thus also good predictors in MT evaluation.However, while the original RTE task is asymmet-ric, MT evaluation needs to determine meaningequivalence, which is a symmetric relation.
We dothis by checking for entailment in both directions(see Figure 1).
Operationally, this ensures we detecttranslations which either delete or insert material.Clearly, there are also differences between thetwo tasks.
An important one is that RTE assumesthe well-formedness of the two sentences.
This isnot generally true in MT, and could lead to de-graded linguistic analyses.
However, entailmentrelations are more sensitive to the contribution ofindividual words (MacCartney andManning, 2008).In Example 2, the modal modifiers break the entail-ment between two otherwise identical sentences:(2) HYP: Peter is certainly from Lincolnshire.REF: Peter is possibly from Lincolnshire.This means that the prediction of TE hinges oncorrect semantic analysis and is sensitive to mis-analyses.
In contrast, human MT judgments behaverobustly.
Translations that involve individual errors,like (2), are judged lower than perfect ones, butusually not crucially so, since most aspects arestill rendered correctly.
We thus expect even noisyRTE features to be predictive for translation quality.This allows us to use an off-the-shelf RTE systemto obtain features, and to combine them using aregression model as described in Section 2.3.1 The Stanford Entailment RecognizerThe Stanford Entailment Recognizer (MacCartneyet al, 2006) is a stochastic model that computesmatch and mismatch features for each premise-hypothesis pair.
The three stages of the systemare shown in Figure 2.
The system first uses arobust broad-coverage PCFG parser and a deter-ministic constituent-dependency converter to con-struct linguistic representations of the premise and298Stage 3: Feature computation (w/ numbers of features)Premise: India buys 1,000 tanks.Hypothesis: India acquires arms.Stage 1: Linguistic analysisIndiabuys1,000 tankssubj dobjIndiaacquiresarmssubj dobjStage 2: AlignmentIndiabuys1,000 tankssubj dobjIndiaacquiresarmssubj dobj0.91.00.7Alignment (8):Semanticcompatibility(34):Insertions anddeletions (20):Preservation ofreference (16):Structuralalignment (28):Overall alignment qualityModality, Factivity, Polarity,Quantification, Lexical-semanticrelatedness, TenseFelicity of appositions and adjuncts,Types of unaligned materialLocations, Dates, EntitiesAlignment of main verbs andsyntactically prominent words,Argument structure (mis-)matchesFigure 2: The Stanford Entailment Recognizerthe hypothesis.
The results are typed dependencygraphs that contain a node for each word and la-beled edges representing the grammatical relationsbetween words.
Named entities are identified, andcontiguous collocations grouped.
Next, it identifiesthe highest-scoring alignment from each node inthe hypothesis graph to a single node in the premisegraph, or to null.
It uses a locally decomposablescoring function: The score of an alignment is thesum of the local word and edge alignment scores.The computation of these scores make extensiveuse of about ten lexical similarity resources, in-cluding WordNet, InfoMap, and Dekang Lin?s the-saurus.
Since the search space is exponential inthe hypothesis length, the system uses stochastic(rather than exhaustive) search based on Gibbs sam-pling (see de Marneffe et al (2007)).Entailment features.
In the third stage, the sys-tem produces roughly 100 features for each alignedpremise-hypothesis pair.
A small number of themare real-valued (mostly quality scores), but mostare binary implementations of small linguistic the-ories whose activation indicates syntactic and se-mantic (mis-)matches of different types.
Figure 2groups the features into five classes.
Alignmentfeatures measure the overall quality of the align-ment as given by the lexical resources.
Semanticcompatibility features check to what extent thealigned material has the same meaning and pre-serves semantic dimensions such as modality andfactivity, taking a limited amount of context intoaccount.
Insertion/deletion features explicitly ad-dress material that remains unaligned and assess itsfelicity.
Reference features ascertain that the twosentences actually refer to the same events and par-ticipants.
Finally, structural features add structuralconsiderations by ensuring that argument structureis preserved in the translation.
See MacCartney etal.
(2006) for details on the features, and Sections5 and 6 for examples of feature firings.Efficiency considerations.
The use of deep lin-guistic analysis makes our entailment-based met-ric considerably more heavyweight than traditionalMT metrics.
The average total runtime per sentencepair is 5 seconds on an AMD 2.6GHz Opteron core?
efficient enough to perform regular evaluations ondevelopment and test sets.
We are currently investi-gating caching and optimizations that will enablethe use of our metric for MT parameter tuning in aMinimum Error Rate Training setup (Och, 2003).4 Experimental Evaluation4.1 ExperimentsTraditionally, human ratings for MT quality havebeen collected in the form of absolute scores on afive- or seven-point Likert scale, but low reliabil-ity numbers for this type of annotation have raisedconcerns (Callison-Burch et al, 2008).
An alter-native that has been adopted by the yearly WMTevaluation shared tasks since 2008 is the collectionof pairwise preference judgments between pairs ofMT hypotheses which can be elicited (somewhat)more reliably.
We demonstrate that our approachworks well for both types of annotation and differ-ent corpora.
Experiment 1 models absolute scoreson Asian newswire, and Experiment 2 pairwisepreferences on European speech and news data.4.2 EvaluationWe evaluate the output of our models both on thesentence and on the system level.
At the sentencelevel, we can correlate predictions in Experiment 1directly with human judgments with Spearman?s ?
,299a non-parametric rank correlation coefficient appro-priate for non-normally distributed data.
In Experi-ment 2, the predictions cannot be pooled betweensentences.
Instead of correlation, we compute ?con-sistency?
(i.e., accuracy) with human preferences.System-level predictions are computed in bothexperiments from sentence-level predictions, as theratio of sentences for which each system providedthe best translation (Callison-Burch et al, 2008).We extend this procedure slightly because real-valued predictions cannot predict ties, while humanraters decide for a significant portion of sentences(as much as 80% in absolute score annotation) to?tie?
two systems for first place.
To simulate thisbehavior, we compute ?tie-aware?
predictions asthe percentage of sentences where the system?s hy-pothesis was assigned a score better or at most ?worse than the best system.
?
is set to match thefrequency of ties in the training data.Finally, the predictions are again correlated withhuman judgments using Spearman?s ?
.
?Tie aware-ness?
makes a considerable practical difference,improving correlation figures by 5?10 points.14.3 Baseline MetricsWe consider four baselines.
They are small regres-sion models as described in Section 2 over com-ponent scores of four widely used MT metrics.
Toalleviate possible nonlinearity, we add all featuresin linear and log space.
Each baselines carries thename of the underlying metric plus the suffix -R.2BLEUR includes the following 18 sentence-levelscores: BLEU-n and n-gram precision scores(1?
n?
4); BLEU brevity penalty (BP); BLEUscore divided by BP.
To counteract BLEU?s brittle-ness at the sentence level, we also smooth BLEU-nand n-gram precision as in Lin and Och (2004).NISTR consists of 16 features.
NIST-n scores(1?
n?
10) and information-weighted n-gramprecision scores (1?
n?
4); NIST brevity penalty(BP); and NIST score divided by BP.1Due to space constraints, we only show results for ?tie-aware?
predictions.
See Pado?
et al (2009) for a discussion.2The regression models can simulate the behaviour of eachcomponent by setting the weights appropriately, but are strictlymore powerful.
A possible danger is that the parameters over-fit on the training set.
We therefore verified that the threenon-trivial ?baseline?
regression models indeed confer a bene-fit over the default component combination scores: BLEU-1(which outperformed BLEU-4 in the MetricsMATR 2008 eval-uation), NIST-4, and TER (with all costs set to 1).
We foundhigher robustness and improved correlations for the regressionmodels.
An exception is BLEU-1 and NIST-4 on Expt.
1 (Ar,Ch), which perform 0.5?1 point better at the sentence level.TERR includes 50 features.
We start with thestandard TER score and the number of each of thefour edit operations.
Since the default uniform costdoes not always correlate well with human judg-ment, we duplicate these features for 9 non-uniformedit costs.
We find it effective to set insertion costclose to 0, as a way of enabling surface variation,and indeed the new TERp metric uses a similarlylow default insertion cost (Snover et al, 2009).METEORR consists of METEOR v0.7.4.4 Combination MetricsThe following three regression models implementthe methods discussed in Sections 2 and 3.MTR combines the 85 features of the four base-line models.
It uses no entailment features.RTER uses the 70 entailment features describedin Section 3.1, but no MTR features.MT+RTER uses all MTR and RTER features,combining matching and entailment evidence.35 Expt.
1: Predicting Absolute ScoresData.
Our first experiment evaluates the modelswe have proposed on a corpus with traditional an-notation on a seven-point scale, namely the NISTOpenMT 2008 corpus.4 The corpus contains trans-lations of newswire text into English from threesource languages (Arabic (Ar), Chinese (Ch), Urdu(Ur)).
Each language consists of 1500?2800 sen-tence pairs produced by 7?15 MT systems.We use a ?round robin?
scheme.
We optimizethe weights of our regression models on two lan-guages and then predict the human scores on thethird language.
This gauges performance of ourmodels when training and test data come from thesame genre, but from different languages, whichwe believe to be a setup of practical interest.
Foreach test set, we set the system-level tie parameter?
so that the relative frequency of ties was equalto the training set (65?80%).
Hypotheses generallyhad to receive scores within 0.3?0.5 points to tie.Results.
Table 1 shows the results.
We first con-centrate on the upper half (sentence-level results).The predictions of all models correlate highly sig-nificantly with human judgments, but we still seerobustness issues for the individual MT metrics.3Software for RTER and MT+RTER is available fromhttp://nlp.stanford.edu/software/mteval.shtml.4Available from http://www.nist.gov.300Evaluation Data Metricstrain test BLEUR METEORR NISTR TERR MTR RTER MT+RTERSentence-levelAr+Ch Ur 49.9 49.1 49.5 50.1 50.1 54.5 55.6Ar+Ur Ch 53.9 61.1 53.1 50.3 57.3 58.0 62.7Ch+Ur Ar 52.5 60.1 50.4 54.5 55.2 59.9 61.1System-levelAr+Ch Ur 73.9 68.4 50.0 90.0?
92.7?
77.4?
81.0?Ar+Ur Ch 38.5 44.3 40.0 59.0?
51.8?
47.7 57.3?Ch+Ur Ar 59.7?
86.3?
61.9?
42.1 48.1 59.7?
61.7?Table 1: Expt.
1: Spearman?s ?
for correlation between human absolute scores and model predictions onNIST OpenMT 2008.
Sentence level: All correlations are highly significant.
System level: ?
: p<0.05.METEORR achieves the best correlation for Chi-nese and Arabic, but fails for Urdu, apparently themost difficult language.
TERR shows the best resultfor Urdu, but does worse than METEORR for Ara-bic and even worse than BLEUR for Chinese.
TheMTR combination metric alleviates this problem tosome extent by improving the ?worst-case?
perfor-mance on Urdu to the level of the best individualmetric.
The entailment-based RTER system outper-forms MTR on each language.
It particularly im-proves on MTR?s correlation on Urdu.
Even thoughMETEORR still does somewhat better than MTRand RTER, we consider this an important confirma-tion for the usefulness of entailment features in MTevaluation, and for their robustness.5In addition, the combined model MT+RTER isbest for all three languages, outperforming METE-ORR for each language pair.
It performs consid-erably better than either MTR or RTER.
This is asecond result: the types of evidence provided byMTR and RTER appear to be complementary andcan be combined into a superior model.On the system level (bottom half of Table 1),there is high variance due to the small number ofpredictions per language, and many predictions arenot significantly correlated with human judgments.BLEUR, METEORR, and NISTR significantly pre-dict one language each (all Arabic); TERR, MTR,and RTER predict two languages.
MT+RTER isthe only model that shows significance for all threelanguages.
This result supports the conclusions wehave drawn from the sentence-level analysis.Further analysis.
We decided to conduct a thor-ough analysis of the Urdu dataset, the most difficultsource language for all metrics.
We start with a fea-5These results are substantially better than the performanceour metric showed in the MetricsMATR 2008 challenge.
Be-yond general enhancement of our model, we attribute the lessgood MetricsMATR 2008 results to an infelicitous choiceof training data for the submission, coupled with the largeamount of ASR output in the test data, whose disfluenciesrepresent an additional layer of problems for deep approaches.20 40 60 80 1000.420.460.500.54% Training data MT08 Ar+ChSpearman'srhoonMT08 Urlllll l l l ll l ll l l l l lllMetricsMt?RteRRteRMtRMetRFigure 3: Experiment 1: Learning curve (Urdu).ture ablation study.
Removing any feature groupfrom RTER results in drops in correlation of at leastthree points.
The largest drops occur for the struc-tural (?
= ?11) and insertion/deletion (?
= ?8)features.
Thus, all feature groups appear to con-tribute to the good correlation of RTER.
However,there are big differences in the generality of thefeature groups: in isolation, the insertion/deletionfeatures achieve almost no correlation, and need tobe complemented by more robust features.Next, we analyze the role of training data.
Fig-ure 3 shows Urdu average correlations for modelstrained on increasing subsets of the training data(10% increments, 10 random draws per step; Arand Ch show similar patterns.)
METEORR does notimprove, which is to be expected given the modeldefinition.
RTER has a rather flat learning curvethat climbs to within 2 points of the final correla-tion value for 20% of the training set (about 400sentence pairs).
Apparently, entailment features donot require a large training set, presumably becausemost features of RTER are binary.
The remainingtwo models, MTR and MT+RTER, show clearerbenefit from more data.
With 20% of the total data,they climb to within 5 points of their final perfor-mance, but keep slowly improving further.301REF: I shall face that fact today.HYP: Today I will face this reality.
[doc WL-34-174270-7483871, sent 4, system1]Gold: 6METEORR: 2.8RTER: 6.1?
Only function words unaligned (will, this)?
Alignment fact/reality: hypernymy is okin upward monotone contextREF: What does BBC?s Haroon Rasheed say after a visit to Lal Masjid JamiaHafsa complex?
There are no underground tunnels in Lal Masjid or JamiaHafsa.
The presence of the foreigners could not be confirmed as well.
Whatbecame of the extremists like Abuzar?HYP: BBC Haroon Rasheed Lal Masjid, Jamia Hafsa after his visit to AuobMedical Complex says Lal Masjid and seminary in under a land mine, notalso been confirmed the presence of foreigners could not be, such as Abu bythe extremist?
[doc WL-12-174261-7457007, sent 2, system2]Gold: 1METEORR: 4.5RTER: 1.2?
Hypothesis root node unaligned?
Missing alignments for subjects?
Important entities in hypothesis cannot bealigned?
Reference, hypothesis differ in polarityTable 2: Expt.
1: Reference translations and MT output (Urdu).
Scores are out of 7 (higher is better).Finally, we provide a qualitative comparison ofRTER?s performance against the best baseline met-ric, METEORR.
Since the computation of RTERtakes considerably more resources than METEORR,it is interesting to compare the predictions of RTERagainst METEORR.
Table 2 shows two classes ofexamples with apparent improvements.The first example (top) shows a good translationthat is erroneously assigned a low score by ME-TEORR because (a) it cannot align fact and reality(METEORR aligns only synonyms) and (b) it pun-ishes the change of word order through its ?penalty?term.
RTER correctly assigns a high score.
Thefeatures show that this prediction results from twosemantic judgments.
The first is that the lack ofalignments for two function words is unproblem-atic; the second is that the alignment between factand reality, which is established on the basis ofWordNet similarity, is indeed licensed in the cur-rent context.
More generally, we find that RTERis able to account for more valid variation in goodtranslations because (a) it judges the validity ofalignments dependent on context; (b) it incorpo-rates more semantic similarities; and (c) it weighsmismatches according to the word?s status.The second example (bottom) shows a very badtranslation that is scored highly by METEORR,since almost all of the reference words appear eitherliterally or as synonyms in the hypothesis (markedin italics).
In combination with METEORR?s con-centration on recall, this is sufficient to yield amoderately high score.
In the case of RTER, a num-ber of mismatch features have fired.
They indicateproblems with the structural well-formedness ofthe MT output as well as semantic incompatibil-ity between hypothesis and reference (argumentstructure and reference mismatches).6 Expt.
2: Predicting Pairwise PreferencesIn this experiment, we predict human pairwise pref-erence judgments (cf.
Section 4).
We reuse thelinear regression framework from Section 2 andpredict pairwise preferences by predicting two ab-solute scores (as before) and comparing them.6Data.
This experiment uses the 2006?2008 cor-pora of the Workshop on Statistical MachineTranslation (WMT).7 It consists of data from EU-ROPARL (Koehn, 2005) and various news com-mentaries, with five source languages (French, Ger-man, Spanish, Czech, and Hungarian).
As trainingset, we use the portions of WMT 2006 and 2007that are annotated with absolute scores on a five-point scale (around 14,000 sentences produced by40 systems).
The test set is formed by the WMT2008 relative rank annotation task.
As in Experi-ment 1, we set ?
so that the incidence of ties in thetraining and test set is equal (60%).Results.
Table 4 shows the results.
The left resultcolumn shows consistency, i.e., the accuracy onhuman pairwise preference judgments.8 The pat-tern of results matches our observations in Expt.
1:Among individual metrics, METEORR and TERRdo better than BLEUR and NISTR.
MTR and RTERoutperform individual metrics.
The best result by awide margin, 52.5%, is shown by MT+RTER.6We also experimented with a logistic regression modelthat predicts binary preferences directly.
Its performance iscomparable; see Pado?
et al (2009) for details.7Available from http://www.statmt.org/.8The random baseline is not 50%, but, according to ourexperiments, 39.8%.
This has two reasons: (1) the judgmentsinclude contradictory and tie annotations that cannot be pre-dicted correctly (raw inter-annotator agreement on WMT 2008was 58%); (2) metrics have to submit a total order over thetranslations for each sentence, which introduces transitivityconstraints.
For details, see Callison-Burch et al (2008).302Segment MTR RTER MT+RTER GoldREF: Scottish NHS boards need to improve criminal records checks foremployees outside Europe, a watchdog has said.HYP: The Scottish health ministry should improve the controls on extra-community employees to check whether they have criminal precedents,said the monitoring committee.
[1357, lium-systran]Rank: 3 Rank: 1 Rank: 2 Rank: 1REF: Arguments, bullying and fights between the pupils have extendedto the relations between their parents.HYP: Disputes, chicane and fights between the pupils transposed inrelations between the parents.
[686, rbmt4]Rank: 5 Rank: 2 Rank: 4 Rank: 5Table 3: Expt.
2: Reference translations and MT output (French).
Ranks are out of five (smaller is better).Feature set Consis-tency (%)System-levelcorrelation (?
)BLEUR 49.6 69.3METEORR 51.1 72.6NISTR 50.2 70.4TERR 51.2 72.5MTR 51.5 73.1RTER 51.8 78.3MT+RTER 52.5 75.8WMT 08 (worst) 44 37WMT 08 (best) 56 83Table 4: Expt.
2: Prediction of pairwise preferenceson the WMT 2008 dataset.The right column shows Spearman?s ?
for thecorrelation between human judgments and tie-aware system-level predictions.
All metrics predictsystem scores highly significantly, partly due to thelarger number of systems compared (87 systems).Again, we see better results for METEORR andTERR than for BLEUR and NISTR, and the indi-vidual metrics do worse than the combination mod-els.
Among the latter, the order is: MTR (worst),MT+RTER, and RTER (best at 78.3).WMT 2009.
We submitted the Expt.
2 RTERmetric to the WMT 2009 shared MT evaluationtask (Pado?
et al, 2009).
The results provide fur-ther validation for our results and our general ap-proach.
At the system level, RTER made third place(avg.
correlation ?
= 0.79), trailing the two top met-rics closely (?
= 0.80, ?
= 0.83) and making thebest predictions for Hungarian.
It also obtained thesecond-best consistency score (53%, best: 54%).Metric comparison.
The pairwise preference an-notation of WMT 2008 gives us the opportunity tocompare the MTR and RTER models by comput-ing consistency separately on the ?top?
(highest-ranked) and ?bottom?
(lowest-ranked) hypothesesfor each reference.
RTER performs about 1.5 per-cent better on the top than on the bottom hypothe-ses.
The MTR model shows the inverse behavior,performing 2 percent worse on the top hypothe-ses.
This matches well with our intuitions: We seesome noise-induced degradation for the entailmentfeatures, but not much.
In contrast, surface-basedfeatures are better at detecting bad translations thanat discriminating among good ones.Table 3 further illustrates the difference betweenthe top models on two example sentences.
In the topexample, RTER makes a more accurate predictionthan MTR.
The human rater?s favorite translationdeviates considerably from the reference in lexi-cal choice, syntactic structure, and word order, forwhich it is punished by MTR (rank 3/5).
In contrast,RTER determines correctly that the propositionalcontent of the reference is almost completely pre-served (rank 1).
In the bottom example, RTER?sprediction is less accurate.
This sentence was ratedas bad by the judge, presumably due to the inap-propriate main verb translation.
Together with thesubject mismatch, MTR correctly predicts a lowscore (rank 5/5).
RTER?s attention to semantic over-lap leads to an incorrect high score (rank 2/5).Feature Weights.
Finally, we make two observa-tions about feature weights in the RTER model.First, the model has learned high weights notonly for the overall alignment score (which be-haves most similarly to traditional metrics), but alsofor a number of binary syntacto-semantic matchand mismatch features.
This confirms that thesefeatures systematically confer the benefit we haveshown anecdotally in Table 2.
Features with a con-sistently negative effect include dropping adjuncts,unaligned or poorly aligned root nodes, incompat-ible modality between the main clauses, personand location mismatches (as opposed to generalmismatches) and wrongly handled passives.
Con-303versely, higher scores result from factors such ashigh alignment score, matching embeddings underfactive verbs, and matches between appositions.Second, good MT evaluation feature weights arenot good weights for RTE.
Some differences, par-ticularly for structural features, are caused by thelow grammaticality of MT data.
For example, thefeature that fires for mismatches between depen-dents of predicates is unreliable on the WMT data.Other differences do reflect more fundamental dif-ferences between the two tasks (cf.
Section 3).
Forexample, RTE puts high weights onto quantifierand polarity features, both of which have the poten-tial of influencing entailment decisions, but are (atleast currently) unimportant for MT evaluation.7 Related WorkResearchers have exploited various resources to en-able the matching between words or n-grams thatare semantically close but not identical.
Banerjeeand Lavie (2005) and Chan and Ng (2008) useWordNet, and Zhou et al (2006) and Kauchakand Barzilay (2006) exploit large collections ofautomatically-extracted paraphrases.
These ap-proaches reduce the risk that a good translationis rated poorly due to lexical deviation, but do notaddress the problem that a translation may containmany long matches while lacking coherence andgrammaticality (cf.
the bottom example in Table 2).Thus, incorporation of syntactic knowledge hasbeen the focus of another line of research.
Amigo?et al (2006) use the degree of overlap between thedependency trees of reference and hypothesis as apredictor of translation quality.
Similar ideas havebeen applied by Owczarzak et al (2008) to LFGparses, and by Liu and Gildea (2005) to featuresderived from phrase-structure tress.
This approachhas also been successful for the related task ofsummarization evaluation (Hovy et al, 2006).The most comparable work to ours is Gime?nezand Ma?rquez (2008).
Our results agree on the cru-cial point that the use of a wide range of linguisticknowledge in MT evaluation is desirable and im-portant.
However, Gime?nez and Ma?rquez advocatethe use of a bottom-up development process thatbuilds on a set of ?heterogeneous?, independentmetrics each of which measures overlap with re-spect to one linguistic level.
In contrast, our aimis to provide a ?top-down?, integrated motivationfor the features we integrate through the textualentailment recognition paradigm.8 Conclusion and OutlookIn this paper, we have explored a strategy for theevaluation of MT output that aims at comprehen-sively assessing the meaning equivalence betweenreference and hypothesis.
To do so, we exploit thecommon ground between MT evaluation and theRecognition of Textual Entailment (RTE), both ofwhich have to distinguish valid from invalid lin-guistic variation.
Conceputalizing MT evaluationas an entailment problem motivates the use of arich feature set that covers, unlike almost all earliermetrics, a wide range of linguistic levels, includinglexical, syntactic, and compositional phenomena.We have used an off-the-shelf RTE system tocompute these features, and demonstrated that aregression model over these features can outper-form an ensemble of traditional MT metrics in twoexperiments on different datasets.
Even though thefeatures build on deep linguistic analysis, they arerobust enough to be used in a real-world setting, atleast on written text.
A limited amount of trainingdata is sufficient, and the weights generalize well.Our data analysis has confirmed that each of thefeature groups contributes to the overall success ofthe RTE metric, and that its gains come from itsbetter success at abstracting away from valid vari-ation (such as word order or lexical substitution),while still detecting major semantic divergences.We have also clarified the relationship between MTevaluation and textual entailment: The majority ofphenomena (but not all) that are relevant for RTEare also informative for MT evaluation.The focus of this study was on the use of an ex-isting RTE infrastructure for MT evaluation.
Futurework will have to assess the effectiveness of individ-ual features and investigate ways to customize RTEsystems for the MT evaluation task.
An interestingaspect that we could not follow up on in this paperis that entailment features are linguistically inter-pretable (cf.
Fig.
2) and may find use in uncoveringsystematic shortcomings of MT systems.A limitation of our current metric is that it islanguage-dependent and relies on NLP tools inthe target language that are still unavailable formany languages, such as reliable parsers.
To someextent, of course, this problem holds as well forstate-of-the-art MT systems.
Nevertheless, it mustbe an important focus of future research to developrobust meaning-based metrics for other languagesthat can cash in the promise that we have shownfor evaluating translation into English.304ReferencesEnrique Amigo?, Jesu?s Gime?nez, Julio Gonzalo, andLlu?
?s Ma`rquez.
2006.
MT Evaluation: Human-like vs. human acceptable.
In Proceedings of COL-ING/ACL 2006, pages 17?24, Sydney, Australia.Satanjeev Banerjee and Alon Lavie.
2005.
METEOR:An automatic metric for MT evaluation with im-proved correlation with human judgments.
In Pro-ceedings of the ACL Workshop on Intrinsic and Ex-trinsic Evaluation Measures, pages 65?72, Ann Ar-bor, MI.Chris Callison-Burch, Miles Osborne, and PhilippKoehn.
2006.
Re-evaluating the role of BLEUin machine translation research.
In Proceedings ofEACL, pages 249?256, Trento, Italy.Chris Callison-Burch, Cameron Fordyce, PhilippKoehn, Christof Monz, and Josh Schroeder.
2008.Further meta-evaluation of machine translation.
InProceedings of the ACL Workshop on Statistical Ma-chine Translation, pages 70?106, Columbus, OH.Yee Seng Chan and Hwee Tou Ng.
2008.
MAXSIM: Amaximum similarity metric for machine translationevaluation.
In Proceedings of ACL-08: HLT, pages55?62, Columbus, Ohio, June.Ido Dagan, Oren Glickman, and Bernardo Magnini.2005.
The PASCAL recognising textual entailmentchallenge.
In Proceedings of the PASCAL Chal-lenges Workshop on Recognising Textual Entailment,Southampton, UK.Marie-Catherine de Marneffe, Trond Grenager, BillMacCartney, Daniel Cer, Daniel Ramage, Chloe?Kiddon, and Christopher D. Manning.
2007.
Align-ing semantic graphs for textual inference and ma-chine reading.
In Proceedings of the AAAI SpringSymposium, Stanford, CA.George Doddington.
2002.
Automatic evaluation ofmachine translation quality using n-gram cooccur-rence statistics.
In Proceedings of HLT, pages 128?132, San Diego, CA.Jesu?s Gime?nez and Llu?
?s Ma?rquez.
2008.
Het-erogeneous automatic MT evaluation through non-parametric metric combinations.
In Proceedings ofIJCNLP, pages 319?326, Hyderabad, India.Sanda Harabagiu and Andrew Hickl.
2006.
Methodsfor using textual entailment in open-domain ques-tion answering.
In Proceedings of ACL, pages 905?912, Sydney, Australia.Eduard Hovy, Chin-Yew Lin, Liang Zhou, and JunichiFukumoto.
2006.
Automated summarization evalu-ation with basic elements.
In Proceedings of LREC,Genoa, Italy.David Kauchak and Regina Barzilay.
2006.
Paraphras-ing for automatic evaluation.
In Proceedings of HLT-NAACL, pages 455?462.Phillip Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In Proceedings of theMT Summit X, Phuket, Thailand.Chin-Yew Lin and Franz Josef Och.
2004.
ORANGE:a method for evaluating automatic evaluation met-rics for machine translation.
In Proceedings of COL-ING, pages 501?507, Geneva, Switzerland.Ding Liu and Daniel Gildea.
2005.
Syntactic featuresfor evaluation of machine translation.
In Proceed-ings of the ACL Workshop on Intrinsic and ExtrinsicEvaluation Measures, pages 25?32, Ann Arbor, MI.Bill MacCartney and Christopher D. Manning.
2008.Modeling semantic containment and exclusion innatural language inference.
In Proceedings of COL-ING, pages 521?528, Manchester, UK.Bill MacCartney, Trond Grenager, Marie-Catherinede Marneffe, Daniel Cer, and Christopher D. Man-ning.
2006.
Learning to recognize features ofvalid textual entailments.
In Proceedings of NAACL,pages 41?48, New York City, NY.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofACL, pages 160?167, Sapporo, Japan.Karolina Owczarzak, Josef van Genabith, and AndyWay.
2008.
Evaluating machine translation withLFG dependencies.
Machine Translation, 21(2):95?119.Sebastian Pado?, Michel Galley, Dan Jurafsky, andChristopher D. Manning.
2009.
Textual entailmentfeatures for machine translation evaluation.
In Pro-ceedings of the EACL Workshop on Statistical Ma-chine Translation, pages 37?41, Athens, Greece.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedingsof ACL, pages 311?318, Philadelphia, PA.Michael Paul, Andrew Finch, and Eiichiro Sumita.2007.
Reducing human assessment of machinetranslation quality to binary classifiers.
In Proceed-ings of TMI, pages 154?162, Sko?vde, Sweden.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A studyof translation edit rate with targeted human annota-tion.
In Proceedings of AMTA, pages 223?231, Cam-bridge, MA.Matthew Snover, Nitin Madnani, Bonnie J. Dorr, andRichard Schwartz.
2009.
Fluency, adequacy, orHTER?
Exploring different human judgments witha tunable MT metric.
In Proceedings of the EACLWorkshop on Statistical Machine Translation, pages259?268, Athens, Greece.Liang Zhou, Chin-Yew Lin, and Eduard Hovy.
2006.Re-evaluating machine translation results with para-phrase support.
In Proceedings of EMNLP, pages77?84, Sydney, Australia.305
