Keyword Translation Accuracy and Cross-Lingual QuestionAnswering in Chinese and JapaneseTeruko MitamuraCarnegie MellonUniversityPittsburgh, PA USAteruko@cs.cmu.eduMengqiu WangCarnegie MellonUniversityPittsburgh, PA USAmengqiu@cs.cmu.eduHideki ShimaCarnegie MellonUniversityPittsburgh, PA USAhideki@cs.cmu.eduFrank LinCarnegie MellonUniversityPittsburgh, PA USAfrank+@cs.cmu.eduAbstractIn this paper, we describe the extensionof an existing monolingual QA systemfor English-to-Chinese and English-to-Japanese cross-lingual question answer-ing (CLQA).
We also attempt to charac-terize the influence of translation onCLQA performance through experimen-tal evaluation and analysis.
The paperalso describes some language-specific is-sues for keyword translation in CLQA.1 IntroductionThe JAVELIN system is a modular, extensiblearchitecture for building question-answering(QA) systems (Nyberg, et al, 2005).
Since theJAVELIN architecture is language-independent,we extended the original English version ofJAVELIN for cross-language question answering(CLQA) in Chinese and Japanese.
The sameoverall architecture was used for both systems,allowing us to compare the performance of thetwo systems.
In this paper, we describe how weextended the monolingual system for CLQA (seeSection 3).
Keyword translation is a crucial ele-ment of the system; we describe our translationmodule in Section 3.2.
In Section 4, we evaluatethe end-to-end CLQA systems using three differ-ent translation methods.
Language-specifictranslation issues are discussed in Section 5.2 Javelin ArchitectureThe JAVELIN system is composed of four mainmodules: the Question Analyzer (QA), RetrievalStrategist (RS), Information eXtractor (IX) andAnswer Generator (AG).
Inputs to the system areprocessed by these modules in the order listedabove.
The QA module is responsible for parsingthe input question, assigning the appropriate an-swer type to the question, and producing a set ofkeywords.
The RS module is responsible forfinding documents containing answers to thequestion, using keywords produced by the QAmodule.
The IX module finds and extracts an-swers from the documents based on the answertype, and then produces a ranked list of answercandidates.
The AG module normalizes and clus-ters the answer candidates to rerank and generatea final ranked list.
The overall monolingual ar-chitecture is shown in Figure 1.3 Extension for Cross-Lingual QABecause of JAVELIN?s modular design, signifi-cant changes to the monolingual architecturewere not required.
We customized the system inorder to handle Unicode characters and ?plug in?cross-lingual components and resources.For the Question Analyzer, we created theKeyword Translator, a sub-module for translat-ing keywords.
The Retrieval Strategist wasadapted to search in multilingual corpora.
TheInformation Extractors use language-independentextraction algorithms.
The Answer Generatoruses language-specific sub-modules for normali-zation, and a language-independent algorithm foranswer ranking.
The overall cross-lingual archi-tecture is shown in Figure 2.
The rest of this sec-tion explains the details of each module.3.1 Question AnalyzerThe Question Analyzer (QA) is responsible forextracting information from the input question inorder to formulate a representation of theEACL 2006 Workshop on Multilingual Question Answering - MLQA0631Figure1: Javelin Monolingual Architecture Figure2: Javelin Architecture with Cross-LingualExtensioninformation required to answer the question.Input questions are processed using the RASPparser (Korhonen and Briscoe, 2004), and themodule output contains three main components:a) selected keywords; b) the answer type (e.g.numeric-expression, person-name, location); andc) the answer subtype (e.g.
author, river, city).The selected keywords are words or phraseswhich are expected to appear in documents withcorrect answers.
In order to reduce noise in thedocument retrieval phase, we use stop-word liststo eliminate high-frequency terms; for example,the term ?old?
is not included as a keyword for?how-old?
questions.We extended the QA module with a keywordtranslation sub-module, so that translated key-words can be used to retrieve documents frommultilingual corpora.
This straightforward ap-proach has been used by many other CLQA sys-tems.
An alternative approach is to first translatethe whole question sentence from English to thetarget language, and then analyze the translatedquestion.
Our reasons for favoring keywordtranslation are two-fold.
First, to translate thequestion to the target language and analyze it, wewould have to replace the English NLP compo-nents in the Question Analyzer with their coun-terparts for the target language.
In contrast, key-word translation decouples the question analysisfrom the translation, and requires no languagespecific resources during question analysis.
Thesecond reason is that machine translation is notperfect, and therefore the resulting translation(s)for the question may be incomplete or ungram-matical, thus adding to the complexity of theanalysis task.
One could argue that when trans-lating the full sentence instead of just the key-words, we can better utilize state-of-art machinetranslation techniques because more context in-formation is available.
But for our application, anaccurate translation of functional words (such asprepositions or conjunctions) is less important.We focus more on words that carry more contentinformation, such as verbs and nouns.
We willpresent more detail on the use of contextual in-formation for disambiguation in the next section.In some recent work (Kwok, 2005, Mori andKawagishi, 2005), researchers have combinedthese two approaches, but to date no studies havecompared their effectiveness.3.2 Translation ModuleThe Translation Module (TM) is used by the QAmodule to translate keywords into the languageof the target corpus.
Instead of combining multi-ple translation candidates with a disjunctivequery operator (Isozaki et al, 2005), the TM se-lects the best combination of translated keywordsfrom several sources: Machine Readable Dic-tionaries (MRDs), Machine Translation systems(MTs) and Web-mining-Based Keyword Trans-lators (WBMTs) (Nagata et al, 2001, Li et al,2003).
For translation from English to Japanese,we used two MRDs, eight MTs and one WBMT.If none of them return a translation, the word istransliterated into kana for Japanese (for detailson transliteration, see Section 5.2).
For transla-tion from English to Chinese, we used one MRD,three MTs and one WBMT.
After gathering allpossible translations for every keyword, the TMuses a noisy channel model to select the bestcombination of translated keywords.
The TMestimates model statistics using the World WideWeb.
Details of the translation selection methodare described in the rest of this subsection.The Noisy Channel Model: In the noisy channelmodel, an undistorted signal passes through anoisy channel and becomes distorted.
Given thedistorted signal, we are to find the original, un-distorted signal.
IBM applied the noisy channelmodel idea to translation of sentences fromaligned parallel corpora, where the source lan-guage sentence is the distorted signal, and theEACL 2006 Workshop on Multilingual Question Answering - MLQA0632target language sentence is the original signal(Brown et al, 1990).
We adopt this model fordisambiguating keyword translation, with thesource language keyword terms as the distortedsignal and the target language terms as the origi-nal signal.
The TM's job is to find the target lan-guage terms given the source language terms, byfinding the probability of the target languageterms given the source language terms  P(T|S).Using Bayes' Rule, we can break the equationdown to several components:)()|()()|(SPTSPTPSTP?=Because we are comparing probabilities of dif-ferent translations of the same source keywordterms, we can simplify the problem to be:)|()()|( TSPTPSTP ?=We can now reduce the equation to two compo-nents.
P(T) is the language model and P(S|T) isthe translation model.
If we assume independ-ence among the translations of individual terms,we can represent the translation probability of akeyword by the product of the probabilities ofthe individual term translations:?=iii tsPTSP )|()|(Estimating Probabilities using the WorldWide Web: For estimating the probabilities ofthe translation model and the language model,we chose to gather statistics from the WorldWide Web.
There are three advantages in utiliz-ing the web for gathering translation statistics: 1)it contains documents written in many differentlanguages, 2) it has high coverage of virtually alltypes of words and phrases, and 3) it is con-stantly updated.
However, we also note that theweb contains a lot of noisy data, and building upweb statistics is time-consuming unless one hasdirect access to a web search index.Estimating Translation Model Probabilities:We make an assumption that terms that are trans-lations of each other co-occur more often inmixed-language web pages than terms that arenot translations of each other.
This assumption isanalogous to Turney?s work on the co-occurrence of synonyms (Turney, 2001).
Wethen define the translation probability of eachkeyword translation as:?=jjiijiijii tscotscotsP)),(log()),(log()|(,,,Where si is the i-th term in the source languageand ti,j is the j-th translation candidate for si.
Lethits be a number of web pages retrieved from acertain search engine.
co(si, t i,j) is the hits givena query si and ti,j., where log is applied to adjustthe count so that translation probabilities can stillbe comparable at higher counts.Estimating Language Model Probabilities: Inestimating the language model, we simply obtainhits given a conjunction of all the candidateterms in the target language, and divide thatcount by the sum of the occurrences of the indi-vidual terms:?=iintotttcoTP)(),...,()( 21The final score of a translation candidate for aquery is the product of the translation modelscore P(S|T) and the language model score P(T).Smoothing and Pruning: As with most statisti-cal calculations in language technologies, there isa data sparseness problem when calculating thelanguage model score.
Also, because statisticsare gathered real-time by accessing a remotesearch engine via internet, it can take a long timeto process a single query when there is a largenumber of translation candidates.
We describemethods for smoothing the language model andpruning the set of translation candidates below.The data sparseness problem occurs whenthere are many terms in the query, and the termsare relatively rare keywords.
When calculatingthe language model score, it is possible that noneof the translation candidates appear on any webpage.
To address this issue, we propose a "mov-ing-window smoothing" algorithm:?
When the target keyword co-occurrencecount with n keywords is below a setthreshold for all of the translation candi-dates, we use a moving window of sizen-1 that "moves" through the keywordsin sequence, splitting the set of keywordsinto two sets, each with n-1 keywords.?
If the co-occurrence count of all of thesesets of keywords is above the threshold,return the product of the language modelEACL 2006 Workshop on Multilingual Question Answering - MLQA0633score of these two sets as the languagemodel score.?
If not, decrease the window and repeatuntil either all of the split sets are abovethe threshold or n = 1.The moving window smoothing techniquegradually relaxes the search constraint withoutlosing the "connectivity" of keywords (there isalways overlap in the split parts) before finallybacking off to just the individual keywords.However, there are two issues worth noting withthis approach:1.
"Moving-window smoothing" assumesthat keywords that are next to each otherare also more semantically related,which may not always be the case.2.
"Moving-window smoothing" tends togive the keywords near the middle of thequestion more weight, which may not bedesirable.A better smoothing technique may be usedwith trying all possible "splits" at each stage, butthis would greatly increase the time cost.
There-fore, we chose the moving-window smoothing asa trade-off between a more robust smoothingtechnique that tries all possible split combina-tions and no smoothing at all.The set of possible translation candidates isproduced by creating all possible combinationsof the translations of individual keywords.
For aquestion with n keywords and an average of mpossible translations per keyword, the number ofpossible combinations is mn.
This quickly be-comes intractable as we have to access a searchengine at least mn times just for the languagemodel score.
Therefore, pruning is needed to cutdown the number of translation candidates.
Weprune possible translation candidates twice dur-ing each run, using early and late pruning:1.
Early Pruning: We prune possible trans-lations of the individual keywords beforecombining them to make all possibletranslations of a query.
We use a verysimple pruning heuristic based on targetword frequency using a word frequencylist.
Very rare translations produced by aresource are not considered.2.
Late Pruning: We prune possible transla-tion candidates of the entire set of key-words after calculating translation prob-abilities.
Since the calculation of thetranslation probabilities requires littleaccess to the web, we can calculate onlythe language model score for the top Ncandidates with the highest translationscore and prune the rest.An Example of English to Chinese KeywordTranslation Selection: Suppose we translate thefollowing question from English to Chinese.
"What if Bush leaves Iraq?
"Three keywords are extracted: ?Bush?,?leaves?, and ?Iraq.?
Using two MT systems andan MRD, we obtain the following translations:i=1 i=2 i=3Source Bush leaves IraqTarget j=1 ??
??
??
?Target j=2 ??
?
?Table 1.
E-C Keyword Translation"Bush" and "leaves" both have two transla-tions because they are ambiguous keywords,while "Iraq" is unambiguous.
Translation (1,1)means bush as in a shrub, and translation (1,2)refers to the person named Bush.
Translation(2,1) is the verb "to go away", and translation(2,2) is the noun for leaf.
Note that we would liketranslation (1,2) and translation (2,1) becausethey match the sense of the word intended by theuser.
Now we can create all possible combina-tions of the keywords in the target language:"??
??
???""??
??
???""??
??
???""??
??
???
"Query "Bush""??""Bush""??""leaves""??""leaves""??""Iraq""???
"hits 3790 41100 5780 7240 24500Table 2.
Translation Pair Page CountsCandidate Translation Score"??
??
???"
0.215615"??
??
???"
0.221219"??
??
???"
0.277970"??
??
???"
0.285195Table 3.
Translation ScoresEACL 2006 Workshop on Multilingual Question Answering - MLQA0634By calculating hits, we obtain the statistics andthe translation scores shown in Table 2 and 3.Now we can proceed to use the search engine toobtain language model statistics, which we use toobtain the language model.
Then, together withthe translation model score, we calculate theoverall score1.Query ??
??
??
??
??
?hits 428K 459K 1490K 1100K 9590KTable 4.
Individual Term Page CountsQuery hits"??
??
???"
1200"??
??
???"
455"??
??
???"
17300"??
??
???"
2410Table 5.
Target Language Query Page CountsCand Translation Language Overall??????
?2.1562E-1 1.0428E-4 2.2483E-5??????
?2.2122E-1 4.0925E-5 9.0533E-6??????
?2.7797E-1 1.4993E-3 4.1675E-4??????
?2.8520E-1 2.1616E-4 6.1649E-5Table 6.
Translation Score, Language ModelScore, and Overall ScoreAs shown in Table 6, we select the most prob-able combination of translated keywords with thehighest overall score (the third candidate), whichis the correct translation of the English keywords.3.3 Retrieval StrategiesThe Retrieval Strategist (RS) module retrievesdocuments from a corpus in response to a query.For document retrieval, the RS uses the Lemur3.0 toolkit (Ogilvie and Callan, 2001).
Lemursupports structured queries using operators suchas Boolean AND, Synonym, Ordered/Un-Ordered Window and NOT.
An example of astructured query is shown below:1  For simplicity, we don?t apply smoothingand pruning.#BAND( #OD4(????
??)?
?#SYN(*organization *person) )In formulating a structured query, the RS uses anincremental relaxation technique, starting froman initial query that is highly constrained; thealgorithm searches for all the keywords and datatypes in close proximity to each other.
The prior-ity is based on a function of the likely answertype, keyword type (word, proper name, orphrase) and the inverse document frequency ofeach keyword.
The query is gradually relaxeduntil the desired number of relevant documents isretrieved.3.4 Information ExtractionIn the JAVELIN system, the Information Ex-tractor (IX) is not a single module that uses oneextraction algorithm; rather, it is an abstract in-terface which allows different information ex-tractor implementations to be plugged intoJAVELIN.
These different extractors can be usedto produce different results for comparison, orthe results of running them all in parallel can bemerged.
Here we will describe just one of theextractors, the one which is currently the bestalgorithm in our CLQA experiment: the Light IX.The Light IX module uses simple, distance-based algorithms to find a named entity thatmatches the expected answer type and is ?clos-est?
to all the keywords according to some dis-tance measure.
The algorithm considers as an-swer candidates only those terms that are taggedas named entities which match the desired an-swer type.
The score for an answer candidate ais calculated as follows:)()()( aDistScoreaOccScoreaScore ?+?= ?
?where ?
+ ?
= 1, OccScore is the occurrencescore and DistScore is the distance score.
BothOccScore and DistScore return a number be-tween zero and one, and likewise Score returns anumber between zero and one.
Usually, ?
ismuch smaller than ?.
The occurrence score for-mula is:nkExistaOccScoreni i?
== 1 )()(where a is the answer candidate and ki is the i-thkeyword, and n is the number of keywords.
Existreturns 1 if the i-th keyword exists in the docu-ment, and 0 otherwise.
The distance score forEACL 2006 Workshop on Multilingual Question Answering - MLQA0635each answer candidate is calculated according tothe following formula:nkaDistaDistScorenii?
== 1 ),(1)(This formula produces a score between zeroand one.
If the i-th keyword does not exist in adocument, the equation inside the summationwill return zero.
If the i-th keyword appears morethan once in the document, the one closest to theanswer candidate is considered.
An additionalrestriction is that the answer candidate cannot beone of the keywords.
The Dist function is thedistance measure, which has two definitions:1.
),(),( batTokensAparbaDist =2.
)),(log(),( batTokensAparbaDist =The first definition simply counts the numberof tokens between two terms.
The second defini-tion is a logarithmic measure.
The function re-turns the number of tokens from a to b; if a and bare adjacent, the count is 1; if a and b are sepa-rated by one token, the count is 2, and so on.
Atoken can either be a character or a word; for theE-C, we used character-based tokenization,whereas for the E-J, we use word-based tokeni-zation.
By heuristics obtained from training re-sults, we used the linear Dist measure for E-Cand logarithmic Dist measure for E-J in theevaluation.This algorithm is a simple statistical approachwhich requires no language-specific externaltools beyond word segmentation and a named-entity tagger.
It is not as sophisticated as otherapproaches which perform deep linguistic analy-sis, but one advantage is faster adaptation to mul-tiple languages.
In our experiments, this simplealgorithm performs at the same level as a FST-based approach (Nyberg, et al 2005).3.5 Answer GeneratorThe task of the Answer Generator (AG) moduleis to produce a ranked list of answer candidatesfrom the IX output.
The AG is designed to nor-malize answer candidates by resolving represen-tational differences (e.g.
in how numbers, dates,etc.
are expressed in text).
This canonicalizationmakes it possible to combine answer candidatesthat differ only in surface form.Even though the AG module plays an impor-tant role in JAVELIN, we did not use its full po-tential in our E-C and E-J systems, since welacked some language-specific resources re-quired for multilingual answer merging.4 Evaluation and Effect of TranslationAccuracyTo evaluate the effect of translation accuracy onthe overall performance of the CLQA system, weconducted several experiments using differenttranslation methods.
Three different runs werecarried out for both the E-C and E-J systems,using the same 200-question test set and thedocument corpora provided by the NTCIRCLQA task.
The first run was a fully automaticrun using the original translation module in theCLQA system; the result is exactly same as theone we submitted to NTCIR5 CLQA.
For thesecond run, we manually translated the keywordsthat were selected by the Question Analyzermodule.
This translation was done by looking atonly the selected keywords, but not the originalquestion.
For both E-C and E-J tasks, the NTCIRorganizers provided the translations for the Eng-lish questions, which we assume are the gold-standard translations.
Taking advantage of thisresource, in the third run we simply looked upthe corresponding term for each English keywordfrom the gold-standard translation of the ques-tion.
The results for these runs are shown in Ta-ble 7 and 8 below.TranslationAccuracyTop1Top1+URun 1 69.3% 15 (7.5%) 23 (11.5%)Run 2 85.5% 16 (8.0%) 31 (15.5%)Run 3 100% 18 (9.0%) 38 (19.0%)Table 7.
Effect of Translation (E-C)TranslationAccuracyTop1Top1+URun 1 54.2% 20 (10.0%) 25 (12.5%)Run 2 81.2% 19 (9.5%) 30 (15.0%)Run 3 100% 18 (9.0%) 31 (15.5%)Table 8.
Effect of Translation (E-J)We found that in the NTCIR task, the sup-ported/correct document set was not complete.Some answers judged as unsupported were in-deed well supported, but the supporting docu-ment did not appear in NTCIR's correct docu-ment set.
Therefore, we think the Top1+U col-umn is more informative for this evaluation.From Table 7 and 8, it is obvious that the overallperformance increases as translation accuracyEACL 2006 Workshop on Multilingual Question Answering - MLQA0636increases.
From Run1 to Run2, we eliminated allthe overt translation errors produced by the sys-tem, and also corrected word-sense errors.
Thenfrom Run2 to Run3, we made different lexicalchoices among the seemingly all correct transla-tions of a word.
This type of inappropriatenesscannot be classified as an error, but it makes adifference in QA systems, especially at the docu-ment retrieval stage.
For example, the phrase"Kyoto Protocol" can have two valid transla-tions: ????
or ?????.
Both translationswould be understandable to a human, but the sec-ond translation will appear much more frequentlythan the first one in the document set.
This typeof lexical choice is hard to make, because wewould need either subtle domain-specific knowl-edge, or knowledge about the target corpus; nei-ther is easily obtainable.Comparing Run 1 and 3 in Table 8, we seethat improving keyword translation had lessoverall impact on the E-J system.
Informationextraction (including named entity identification)did not perform as well in E-J.
We also com-pared the translation effect on cross-lingualdocument retrieval (Figure 3).
As we can see,Run 3 retrieved supporting documents more fre-quently in rank 1 than in Run 1 or 2.
From thesepreliminary investigations, it would seem thatinformation extraction and/or answer generationmust be improved for English-Japanese CLQA.Figure3: Comparison of three runs: Cross-lingualdocument retrieval performance in E-J5 Translation IssuesIn this section, we discuss language specific key-word translation issues for Chinese and JapaneseCLQA.5.1 ChineseOne prominent problem in Chinese keywordtranslation is word sense disambiguation.
Inquestion answering systems, the translation re-sults are used directly in information retrieval,which exhibits a high dependency on the lexicalform of a word but not so much on the meaning.In other words, having a different lexical formfrom the corresponding term in corpora is thesame as having a wrong translation.
For exam-ple, to translate the word ?bury?
into Chinese,our system gives a translation of ?
, whichmeans ?bury?
as the action of digging a hole,hiding some items in the hole and then coveringit with earth.
But the desired translation, as itappears in the document is ?
, which means?bury?
too, but specifically for burial in funerals.Even more challenging are regional languagedifferences.
In our system, for example, the cor-pora are newswire articles written in TraditionalChinese from Taiwan, and if we use an MT sys-tem that produces translations in Simplified Chi-nese followed by conversion to Traditional Chi-nese, we may run into problems.
The MT systemgenerates Simplified Chinese translations first,which may suggest that the translation resourcesit uses were written in Simplified Chinese andoriginate from mainland China.
In mainlandChina and in Taiwan, people commonly use dif-ferent words for describing the same thing, espe-cially for proper nouns like foreign names.
Table9 lists some examples.
Therefore if the MT sys-tem generates its output using text frommainland China, it may produce a different wordthan the one used in Taiwan, which may not ap-pear in the corpora.
This could lead to failure indocument retrieval.English  Mainland China TaiwanBand ??
?
?Computer Game ????
?
?World GuinnessRecord???????
?????
?The Catcher inthe Rye???????
???
?Nelson ???
??
?Salinger ???
??
?CreutzfeldtJakob Disease?????
???
?Luc Besson ??
??
??
?Pavarotti ????
???
?Table 9.
Different Translation in Chinese5.2 JapaneseRepresentational Gaps: One of the advantagesof using structured queries and automatic queryformulation in the RS is that the system is able tohandle slight representational gaps between aEACL 2006 Workshop on Multilingual Question Answering - MLQA0637translated query and corresponding target wordsin the corpus.For example, Werner Spies appears as ?????
?
?????
in our Japanese preproc-essed corpus and therefore ?????
????
?, which is missing a dot between last and firstname, is a wrong translation if our retrievalmodule only allows exact match.
Lemur supportsan Ordered Distance Operator where the termswithin a #ODN operator must be found within Nwords of each other in the text in order to con-tribute to the document's belief value.
This en-ables us to bridge the representational gaps; suchas when #OD1(?????
?????)
does notmatch any words in the corpus, #OD2(??????????)
is formulated in the next step in or-der to capture ?????
?
????
?.Transliteration in WBMT: After detectingJapanese nouns written in romaji (e.g.
Funaba-shi), we transliterated them into hiragana for abetter result in WBMT.
This is because we areassuming higher positive co-occurrence betweenkana and kanji (i.e.
????
and ??)
than be-tween romaji and kanji (i.e.
funabashi and??
).When there are multiple transliteration candi-dates, we iterate through each candidate.Document Retrieval in Kana: Suppose we aregoing to transliterate Yusuke.
This romaji can bemapped to kana characters with relatively lessambiguity (i.e.
???
, ????
), when com-pared to their subsequent transliteration to kanji(i.e.
?
?, ?
?, ?
?, ?
?, ??
etc.).
Therefore,indexing kana readings in the corpus and query-ing in kana is sometimes a useful technique forCLQA, given the difficulty in converting romajito kana and romaji to kanji.To implement this approach, the Japanese cor-pus was first preprocessed by annotating namedentities and by chunking morphemes.
Then, weannotated a kana reading for each named entity.At query time, if there is no translation foundfrom other resources, the TM transliterates ro-maji to kana as a back-off strategy.6 ConclusionWe described how we extended an existingmonolingual (English) system for CLQA (Eng-lish to Chinese and English to Japanese), includ-ing a translation disambiguation techniquewhich uses a noisy channel model with probabil-ity estimations using web as corpora.
We dis-cussed the influence of translation accuracy onCLQA by presenting experimental results andanalysis.
We concluded by introducing somelanguage-specific issues for keyword translationfrom English to Chinese and Japanese which wehope to address in ongoing research.AcknowledgementsThis work is supported by the Advanced Re-search and Development Activity (ARDA)?sAdvanced Question Answering for Intelligent(AQUAINT) Program.ReferencesBrown, P., J. Cocke, S.D.
Pietra, V.D.
Pietra, F.Jelinek., J. Lafferty, R. Mercer, and P. Roossin.1990.
A Statistical Approach to Machine Transla-tion.
Computational Linguistics, 16(2):38?45.Isozaki, H., K. Sudoh and H. Tsukada.
2005.
NTT?sJapanese-English Cross-Language Question An-swering System.
In Proceedings of the NTCIRWorkshop 5 Meeting, pages 186-193.Korhonen, A. and E. Briscoe.
2004.
Extended Lexi-cal-Semantic Classification of English Verbs.
Pro-ceedings of the HLT/NAACL '04 Workshop onComputational Lexical Semantics, pages 38-45.Kwok, K., P. Deng, N. Dinstl and S. Choi.
2005.NTCIR-5 English-Chinese Cross Language Ques-tion-Answering Experiments using PIRCS.
In Pro-ceedings of the NTCIR Workshop 5 Meeting.Li, Hang, Yunbo Cao, and Cong Li.
2003.
Using Bi-lingual Web Data To Mine and Rank Translations,IEEE Intelligent Systems 18(4), pages 54-59.Mori, T. and M. Kawagishi.
2005.
A Method of CrossLanguage Question-Answering Based on MachineTranslation and Transliteration.
In Proceedings ofthe NTCIR Workshop 5 Meeting.Nagata,  N., T. Saito, and K. Suzuki.
2001.
Using theWeb as a Bilingual Dictionary, In Proceedings ofACL 2001 Workshop Data-Driven Methods in Ma-chine Translation, pages 95-102Nyberg, E., R. Frederking, T. Mitamura, J. M. Bilotti,K.
Hannan, L. Hiyakumoto, J. Ko, F. Lin, L. Lita,V.
Pedro, A. Schlaikjer.
2005.
JAVELIN I and II inTREC2005.
In Proceedings of TREC 2005.Ogilvie, P. and J. Callan.
2001.
Experiments Usingthe Lemur Toolkit.
In Proceedings of the 2001 TextREtrieval Conference (TREC 2001), pages 103-108.Turney, P.D.
2001, Mining the Web for synonyms:PMI-IR versus LSA on  TOEFL, Proceedings of theTwelfth European Conference on Machine Learn-ing, pages 491-502.EACL 2006 Workshop on Multilingual Question Answering - MLQA0638
