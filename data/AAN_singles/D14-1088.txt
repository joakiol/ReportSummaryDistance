Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 810?819,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsTaxonomy Construction Using Syntactic Contextual EvidenceLuu Anh Tuan#1, Jung-jae Kim#2, Ng See Kiong?3#School of Computer Engineering, Nanyang Technological University, Singapore1anhtuan001@e.ntu.edu.sg,2jungjae.kim@ntu.edu.sg?Institute for Infocomm Research, Agency for Science, Technology and Research, Singapore3skng@i2r.a-star.edu.sgAbstractTaxonomies are the backbone of manystructured, semantic knowledge resources.Recent works for extracting taxonomicrelations from text focused on collect-ing lexical-syntactic patterns to extract thetaxonomic relations by matching the pat-terns to text.
These approaches, however,often show low coverage due to the lack ofcontextual analysis across sentences.
Toaddress this issue, we propose a novel ap-proach that collectively utilizes contextualinformation of terms in syntactic struc-tures such that if the set of contexts ofa term includes most of contexts of an-other term, a subsumption relation be-tween the two terms is inferred.
We ap-ply this method to the task of taxonomyconstruction from scratch, where we intro-duce another novel graph-based algorithmfor taxonomic structure induction.
Our ex-periment results show that the proposedmethod is well complementary with previ-ous methods of linguistic pattern matchingand significantly improves recall and thusF-measure.1 IntroductionTaxonomies that are backbone of structured on-tology knowledge have been found to be use-ful for many areas such as question answering(Harabagiu et al., 2003), document clustering(Fodeh et al., 2011) and textual entailment (Gef-fet and Dagan, 2005).
There have been an in-creasing number of hand-crafted, well-structuredtaxonomies publicly available, including WordNet(Miller, 1995), OpenCyc (Matuszek et al., 2006),and Freebase (Bollacker et al., 2008).
However,the manual curation of those taxonomies is time-consuming and human experts may miss relevantterms.
As such, there are still needs to extend ex-isting taxonomies or even to construct new tax-onomies from scratch.The previous methods for identifying taxo-nomic relations (i.e.
is-a relations) from text canbe generally classified into two categories: statis-tical and linguistic approaches.
The former in-cludes co-occurrence analysis (Budanitsky, 1999),term subsumption (Fotzo and Gallinari, 2004) andclustering (Wong et al., 2007).
The main idea be-hinds these techniques is that the terms that fre-quently co-occur may have taxonomic relation-ships.
Such approaches, however, usually sufferfrom low accuracy, though relatively high cover-age, and heavily depend on the choice of featuretypes and datasets.
Most previous methods of thelinguistic approach, on the other hand, rely on thelexical-syntactic patterns (e.g.
A is a B, A such asB) (Hearst, 1992).
Those patterns can be manu-ally created (Kozareva et al., 2008; Wentao et al.,2012), chosen via automatic bootstrapping (Wid-dows and Dorow, 2002; Girju et al., 2003) or iden-tified from machine-learned classifiers (Navigli etal., 2011).
The pattern matching methods gen-erally achieve high precision, but low coveragedue to the lack of contextual analysis across sen-tences.
In this paper, we introduce a novel statisti-cal method and shows that when combined with apattern matching method, it shows significant per-formance improvement.The proposed statistical method, called syntac-tic contextual subsumption (SCS), compares thesyntactic contexts of terms for the taxonomic re-lation identification, instead of the usage of bag-of-words model by the previous statistical meth-ods.
We observe that the terms in taxonomic rela-tions may not occur in the same sentences, but insimilar syntactic structures of different sentences,and that the contexts of a specific term are oftenfound in the contexts of a general term but not viceversa.
By context of a term, we mean the set ofwords frequently have a particular syntactic rela-tion (e.g.
Subject-Verb-Object) with the term in a810given corpus.
Given two terms, the SCS methodcollects from the Web pre-defined syntactic rela-tions of each of the terms and checks if the syntac-tic contexts of a term properly includes that of theother term in order to determine their taxonomicrelation.
The method scores each taxonomic rela-tion candidate based on the two measures of Web-based evidence and contextual set inclusion, andas such, is able to find implicit subsumption rela-tions between terms across sentences.
The SCSshows itself (Section 3.1) to be complementary tolinguistic pattern matching.After the relation identification, the identifiedtaxonomic relations should be integrated into agraph for the task of taxonomy construction fromscratch or associated with existing concepts of agiven taxonomy via is-a relations (Snow et al.,2006).
In this step of taxonomic structure con-struction, there is a need for pruning incorrectand redundant relations.
Previous methods for thepruning task (Kozareva and Hovy, 2010; Velardi etal., 2012) treat the identified taxonomic relationsequally, and the pruning task is thus reduced tofinding the best trade-off between path length andthe connectivity of traversed nodes.
This assump-tion, however, is not always true due to the factthat the identified taxonomic relations may havedifferent confidence values, and the relations withhigh confidence values can be incorrectly elimi-nated during the pruning process.
We thus proposea novel method for the taxonomy induction by uti-lizing the evidence scores from the relation iden-tification method and the topological properties ofthe graph.
We show that it can effectively pruneredundant edges and remove loops while preserv-ing the correct edges of taxonomy.We apply the proposed methods of taxonomicrelation identification and taxonomy induction tothe task of constructing a taxonomy from a giventext collection from scratch.
The resultant systemconsists of three modules: Term extraction andfiltering (Section 2.1), taxonomic relation iden-tification (Section 2.2), and taxonomy induction(Section 2.3).
The outputs of the term extrac-tion/filtering module are used as inputs of the tax-onomic relation identification, such that the tax-onomic relation identification module checks ifthere is a taxonomic relation between each pairof terms from the term extraction/filtering module.The taxonomy induction module gets the identi-fied taxonomic relation set as the input, and out-puts the final optimal taxonomy by pruning redun-dant and incorrect relations.2 Methodology2.1 Term Extraction and FilteringThe first step to construct taxonomies is to col-lect candidate terms from text documents in thedomain of interest.
Like most of linguistic ap-proaches, we use pre-defined linguistic filters toextract candidate terms, including single-wordterms and multi-word terms which are noun ornoun phrases in sentences.
These terms arethen preprocessed by removing determiners andlemmatization.The candidate terms collected are then filteredto select the terms that are most relevant to thedomain of interest.
Many statistical techniquesare developed for the filtering, such as TF -IDF ,domain relevance (DR), and domain consensus(DC) (Navigli and Velardi, 2004).
DR measuresthe amount of information that a term t captureswithin a domain of interest Di, compared to othercontrasting domains (Dj), whileDC measures thedistributed use of a term t across documents d ina domain Di.
Since three measures have pros andcons, and might be complementary to each other,our term filtering method is thus the linear combi-nation of them:TS(t,Di) = ??
TFIDF (t,Di)+ ?
?DR(t,Di) + ?
?DC(t,Di)(1)We experimented (see Section 3) with differentvalues of ?, ?
and ?, and found that the methodshows the best performance when the values for ?and ?
are 0.2 and 0.8 and the value for ?
is be-tween 0.15 and 0.35, depending on the size of thedomain corpus.2.2 Taxonomic Relation IdentificationIn this section, we present three taxonomic rela-tion identification methods which are adopted inour system.
First, two methods of string inclusionwith WordNet and lexical-syntactic pattern match-ing, which were commonly used in the literaturewill be introduced with some modifications.
Then,a novel syntactic contextual subsumption methodto find implicit relations between terms across sen-tences by using contextual evidence from syntacticstructures and Web data will be proposed.
Finally,these three methods will be linearly combined to811Notation Meaningt1?
t2t1is a hypernym of t2t1?
t2t1semantically equals or is sim-ilar to t2t1?WNt2t1is a direct or inherited hyper-nym of t2according to WordNett1?WNt2t1and t2belong to the samesynset of WordNetTable 1: Notationsform an integrating solution for taxonomic rela-tion identification.
Given two terms t1and t2, Ta-ble 1 summarizes important notations used in thispaper.2.2.1 String Inclusion with WordNet (SIWN)One simple way to check taxonomic relation is totest string inclusion.
For example, ?terrorist orga-nization?
is a hypernym of ?foreign terrorist orga-nization?, as the former is a substring of the lat-ter.
We propose an algorithm to extend the stringinclusion test by using WordNet, which will benamed SIWN.
Given a candidate general term tgand a candidate specific term ts, the SIWN al-gorithm examines tgfrom left to right (designat-ing each word in tgto be examined as wg) tocheck if there is any word (ws) in tssuch thatwg?WNwsor wg?WNws, and identifiesthe taxonomic relation between two terms if ev-ery word of tghas a corresponding word in ts(with at least one ?WNrelation).
For example,consider two terms: ?suicide attack?
and ?worldtrade center self-destruction bombing?.
Because?attack?
?WN?bombing?
and ?suicide?
?WN?self-destruction?, according to SIWN algorithm,we conclude that ?suicide attack?
is the hypernymof ?world trade center self-destruction bombing?.Given two terms t1and t2, the evidence scorefor SIWN algorithm is calculated as follows:ScoreSIWN(t1, t2) ={1 if t1 ?
t2via SIWN0 otherwise(2)2.2.2 Lexical-syntactic PatternExtending the ideas of Kozareva and Hovy (2010)and Navigli et al.
(2011), we propose a methodof extracting taxonomic relations by matchinglexical-syntactic patterns to the Web data.Definition 1 (Syntactic patterns).
Given two termst1and t2, Pat(t1, t2) is defined as the set of thefollowing patterns:?
?t1such as t2??
?t1, including t2??
?t2is [a|an] t1??
?t2is a [kind|type] of t1??
?t2, [and|or] other t1?, where t1and t2are replaced with actual termsand [a|b] denotes a choice between a and b.Given candidate general term t1and candi-date specific term t2, the lexical-syntactic pattern(LSP) method works as follows:1.
Submit each phrase in Pat(t1, t2) to a Websearch engine as a query.
The number ofthe search results of the query is denoted asWH(t1, t2).2.
Calculate the following evidence score:ScoreLSP(t1, t2) =log(WH(t1, t2))1 + log(WH(t2, t1))(3)3.
If ScoreLSP(t1, t2) is greater than a thresh-old value then t1?
t2.While most lexical-syntactic pattern meth-ods in the literature only consider the value ofWH(t1, t2) in checking t1?
t2(Wentao et al.,2012), we take into account both WH(t1, t2) andWH(t2, t1).
The intuition of formula (3) is that ift1 is a hypernym of t2 then the size of WH(t1, t2)will be much larger than that of WH(t2, t1),which means the lexical-syntactic patterns aremore applicable for the ordered pair (t1, t2) than(t2, t1).2.2.3 Syntactic Contextual SubsumptionThe LSP method performs well in recognizingthe taxonomic relations between terms in thesentences containing those pre-defined syntacticpatterns.
This method, however, has a majorshortcoming: it cannot derive taxonomic relationsbetween two terms occurring in two differentsentences.
We thus propose a novel syntacticcontextual subsumption (SCS) method which uti-lizes contextual information of terms in syntacticstructure (i.e.
Subject-Verb-Object in this study)and Web data to infer implicit taxonomic relations812between terms across sentences.
Note that thechosen syntactic structure Subject-Verb-Objectis identical to the definition of non-taxonomicrelations in the literature (Buitelaar et al., 2004),where the Verb indicate non-taxonomic relationsbetween Subject and Object.
In this subsection,we first present the method to collect thosenon-taxonomic relations.
Then we present indetail the ideas of the SCS method and how wecan use it to derive taxonomic relations in practice.A.
Non-taxonomic Relation IdentificationFollowing previous approaches to non-taxonomic relation identification, e.g.
(Ciaramitaet al., 2005), we use the Stanford parser (Kleinand Manning, 2003) to identify the syntacticstructures of sentences and extract triples of(Subject, Verb, Object), where Subject and Objectare noun phrases.We further consider the following issues: First,if a term (or noun phrase) includes a preposition,we remove the prepositional phrase.
However, ifthe headword of a term is a quantitative noun like?lot?, ?many?
or ?dozen?
and it is modified by thepreposition ?of?, we replace it with the headwordof the object of the preposition ?of?.
For example,we can extract the triples (people, need, food)and (people, like, snow) from the following sen-tences, respectively:?
?People in poor countries need food??
?A lot of people like snow?Second, if the object of a verb is in a verb form,we replace it with, if any, the object of the em-bedded verb.
For example, we can extract thetriple (soldier, attack, terrorist) from the fol-lowing sentence:?
?The soldiers continue to attack terrorists?Third, if a term has a coordinate structure witha conjunction like ?and?
or ?or?, we split it into allcoordinated noun phrases and duplicate the tripleby replacing the term with each of the coordinatednoun phrases.
For example, we can extract thetriples ofR(girl, like, dog) andR(girl, like, cat)from the following sentence:?
?The girl likes both dogs and cats?Given two terms t1, t2and a non-taxonomic re-lation r, some notations which will be used here-after are shown below:?
R(t1, r, t2): t1, r, and t2have a (Subject,Verb, Object) triple.?
?
(t1, t2): the set of relations r such that thereexists R(t1, r, t2) or R(t2, r, t1).B.
Syntactic Contextual Subsumption MethodThe idea of the SCS method derived from thefollowing two observations.Observation 1.
Given three terms t1, t2, t3, and anon-taxonomic relation r, if we have two triplesR(t1, r, t3) and R(t2, r, t3) (or R(t3, r, t1) andR(t3, r, t2)), t1and t2may be in taxonomic rela-tion.For example, given two triples R(Al-Qaeda, at-tack, American) and R(Terrorist group, attack,American), a taxonomic relation Terrorist group?
Al-Qaeda can be induced.
However, it is notalways guaranteed to induce a taxonomic rela-tions from such a pair of triples, for example fromR(animal, eat, meat) and R(animal, eat, grass).The second observation introduced hereafter willprovide more chance to infer taxonomic relation-ship.Definition 2 (Contextual set of a term).
Givena term t1and a non-taxonomic relation r,S(t1, r, ?subj?)
denotes the set of terms t2suchthat there exists triple R(t1, r, t2).
Similarly,S(t1, r, ?obj?)
is the set of terms t2such thatthere exists triple R(t2, r, t1).Observation 2.
Given two terms t1, t2, and a non-taxonomic relation r, if S(t1, r, ?subj?)
mostlycontains S(t2, r, ?subj?)
but not vice versa, thenmost likely t1is a hypernym of t2.
Similarly, ifS(t1, r, ?obj?)
mostly contains S(t2, r, ?obj?)
butnot vice versa, then most likely t1is a hypernym oft2.For example, assume that S(animal, eat,?subj?)
= {grass, potato, mouse, insects, meat,wild boar, deer, buffalo} and S(tiger, eat, ?subj?
)= {meat, wild boar, deer, buffalo}.
SinceS(animal, eat, ?subj?)
properly contains S(tiger,eat, ?subj?
), we can induce animal ?
tiger.Based on Observation 2, our strategy to infertaxonomic relations is to first find the contextualset of terms via the evidence of syntactic structuresand Web data, and then compute the score of theset inclusion.
The detail of the method is presentedhereafter.813Definition 3.
Given two terms t1, t2and a non-taxonomic relation r, C(t1, t2, r, ?subj?)
denotesthe number of terms t3such that there existsboth triples R(t1, r, t3) and R(t2, r, t3).
Simi-larly, C(t1, t2, r, ?obj?)
is the number of termst3such that there exists both relations R(t3, r, t1)and R(t3, r, t2).Given the pair of a candidate general term t1and a candidate specific term t2, we extract theirnon-taxonomic relations from corpora extractedfrom the Web, and use them to determine the tax-onomic relation between t1and t2as follows:1.
Find from a domain corpus the relation r andtype ?
such that:C(t1, t2, r,?)
= maxr???(t1,t2)???{?subj?,?obj?
}C(t1, t2, r?,??)2.
If type ?
is ?subj?, collect the first 1,000search results of the query ?t1r?
usingthe Google search engine, designated asCorpus?t1.
In the same way, constructCorpus?t2with the query ?t2r?.
If ?
is ?obj?,two queries ?r t1?
and ?r t2?
are submittedinstead to collect Corpus?t1and Corpus?t2,respectively.3.
Find the sets of S(t1, r,?)
and S(t2, r,?
)from Corpus?t1and Corpus?t2, respectively,using the non-taxonomic relation identifica-tion method above.4.
Calculate the following evidence score forSCS method:ScoreSCS=[|S(t1, r,?
)?S(t2, r,?
)||S(t2, r,?
)|+(1?|S(t1, r,?
)?S(t2, r,?
)||S(t1, r,?)|)]?
log(|S(t1, r,?
)|+ |S(t2, r,?
)|)(4)The basic idea of the contextual subsumptionscore in our method is that if t1is a hyper-nym of t2then the set S(t1, r,?)
will mostlycontain S(t2, r,?)
but not vice versa.
The in-tuition of formula (5) is inspired by Jaccardsimilarity coefficient.
We then multiply thescore with the log value of total size of twosets to avoid the bias of small set inclusion.5.
If ScoreSCS(t1, t2) is greater than a thresh-old value, then we have t1 ?
t2.2.2.4 Combined MethodIn our study, we linearly combine three methodsas follows:1.
For each ordered pair of terms (t1, t2) calcu-late the total evidence score:Score(t1, t2) = ??
ScoreSIWN(t1, t2)+ ?
?
ScoreLSP(t1, t2)+ ?
?
ScoreSCS(t1, t2)(5)2.
If Score(t1, t2) is greater than a thresholdvalue, then we have t1?
t2.We experimented with various combinations ofvalues for ?, ?
and ?, and found that the methodshows the best performance when the value of ?
is0.5, ?
is between 0.35 and 0.45, and ?
is between0.15 and 0.25, depending on the domain corpussize.2.3 Taxonomy InductionThe output of the taxonomic relation identifica-tion module is a set of taxonomic relations T .In this section, we will introduce a graph-basedalgorithm (Algorithm 1) to convert this set intoan optimal tree-structured taxonomy, as well asto eliminate incorrect and redundant relations.Denote e(t1, t2) as an directed edge from t1to t2,the algorithm consists of three steps which will bedescribed hereafter with the corresponding linesin Algorithm 1.Step 1: Initial hypernym graph creation(line 1 - 16) This step is to construct a connecteddirected graph from the list of taxonomic rela-tions.
The idea is to add each taxonomic relationt1?
t2as a directed edge from parent nodet1to child node t2, and if t1does not have anyhypernym term, t1will become a child node ofROOT node.
The result of this step is a con-nected graph containing all taxonomic relationswith the common ROOT node.Step 2: Edge weighting (line 17) This stepis to calculate the weight of each edge in thehypernym graph.
Unlike the algorithm of Velardiet al.
(2012) and Kozareva and Hovy (2010)where every taxonomic relation is treated equally,we assume the confidence of each taxonomicrelation is different, depending on the amount of814Algorithm 1 Taxonomy Induction AlgorithmInput: T : the taxonomic relation setOutput: V : the vertex set of resultant taxonomy;E: the edge set of resultant taxonomy;1: Initialize V = {ROOT}, E = ?
;2: for each taxonomic relation (t1?
t2) ?
T do3: E = E ?
{e(t1, t2)}4: if t1??
V then5: V = V ?
{t1}6: end if7: if t2??
V then8: V = V ?
{t2}9: end if10: if ?
e(t3, t1) ?
E with t3?= ROOT then11: E = E ?
{e(ROOT, t1)}12: end if13: if ?
e(ROOT, t2) ?
E then14: E = E \ {e(ROOT, t2)}15: end if16: end for17: edgeWeighting(V,E);18: graphPruning(V,E);evidence it has.
Thus, the hypernym graph edgeswill be weighted as follows:w(e(t1, t2)) ={1 if t1= ROOTScore(t1, t2) otherwise(6)Note that the Score value in formula (6) is de-termined by the taxonomic relation identificationprocess described in Section 2.2.4.Step 3: Graph pruning (line 18) The hy-pernym graph generated in Step 1 is not anoptimal taxonomy as it may contain many redun-dant edges or incorrect edges which together formin a loop.
In this step, we aim at producing anoptimal taxonomy by pruning the graph basedon our edge weighting strategy.
A maximumspanning tree algorithm, however, cannot beapplied as the graph is directed.
For this purpose,we apply Edmonds?
algorithm (Edmonds, 1967)for finding a maximum optimum branching of aweighted directed graph.
Using this algorithm,we can find a subset of the current edge set, whichis the optimized taxonomy where every non-rootnode has in-degree 1 and the sum of the edgeweights is maximized.
Figure 1 shows an exampleof the taxonomy induction process.3 Experiment ResultsWe evaluated our methods for taxonomy construc-tion against the following text collections of fivedomains:?
Artificial Intelligence (AI) domain: 4,119 pa-pers extracted from the IJCAI proceedingsfrom 1969 to 2011 and the ACL archivesfrom year 1979 to 2010.
The same datasetused in the work of Velardi et al.
(2012).?
Terrorism domain: 104 reports of the USstate department, titled ?Patterns of GlobalTerrorism (1991-2002)?1.
A report containsabout 1,500 words.?
Animals, Plants and Vehicles domains: Col-lections of Web pages crawled by usingthe bootstrapping algorithm described byKozareva et al.
(2008).
Navigli et al.
(2011)and Kozareva and Hovy (2010) used thesedatasets to compare their outputs againstWordNet sub-hierarchies.There are two experiments performed in this sec-tion: 1) Evaluating the construction of new tax-onomies for Terrorism and AI domains, and 2)Comparing our results with the gold-standardWordNet sub-hierarchies.
Note that in the experi-ments, the threshold value we used for ScoreLSPis 1.9, ScoreSCSis 1.5 and Score is 2.1.3.1 Constructing new taxonomies for AI andTerrorism domainsReferential taxonomy structures such as WordNetor OpenCyc are widely used in semantic analyt-ics applications.
However, their coverage is lim-ited to common well-known areas, and many spe-cific domains like Terrorism and AI are not wellcovered in those structures.
Therefore, an auto-matic method which can induce taxonomies forthose specific domains from scratch can greatlycontribute to the process of knowledge discovery.First, we applied our taxonomy constructionsystem to the AI domain corpus.
We comparedthe taxonomy constructed by our system with thatobtained by Velardi et al.
(2012), and show thecomparison results in Table 2.
Notice that in thiscomparison, to be fair, we use the same set ofterms that was used in (Velardi et al., 2012).
Theresult shows that our approach can extract 9.8%1http://www.fas.org/irp/threat/terror.htm815Figure 1: An example of taxonomy induction.
(a) Initial weighted hypernym graph.
(b) Final optimaltaxonomy, where we prune two redundant edges (group, International terrorist organization), (Militantgroup, Hezbollah) and remove the loop by cutting an incorrect edge (Al-Qaeda, Terrorist organization).more taxonomic relations and achieve 7% betterterm coverage than Velardi?s approach.Our system Velardi?s system#vertex 1839 1675#edge 1838 1674Average depth 6.2 6Max depth 10 10Term coverage 83% 76%Table 2: Comparison of our system with (Velardiet al., 2012)We also applied our system to the Terrorismcorpus.
The proposed taxonomic relation identifi-cation algorithm extracts a total of 976 taxonomicrelations, from which the taxonomy induction al-gorithm builds the optimal taxonomy.
The totalnumber of vertices in the taxonomy is 281, and thetotal number of edges is 280.
The average depthof the trees is 3.1, with the maximum depth 6.
Inaddition, term coverage (the ratio of the numberof terms in the final optimal trees to the numberof terms obtained by the term suggestion/filteringmethod) is 85%.To judge the contribution of each of taxonomicrelation identification methods described in Sec-tion 2.2 to the overall system, we alternately runthe system for the AI and Terrorism domains withdifferent combinations of the three methods (i.e.SIWN, LSP, and SCS) as shown in Table 3.
Notethat we employed only the first two modules ofterm suggestion/filtering and taxonomic relationidentification except the last module of taxonomyNo.
of extracted relationsTerrorism AI domainSCS 484 1308SIWN 301 984LSP 527 1537SIWN + LSP 711 2203SCS + SIWN + LSP 976 3122Table 3: The number of taxonomic relations ex-tracted by different methods.induction for this experiment.
Table 3 shows thenumber of the taxonomic relations extracted byeach of the combinations.
Since SIWN and LSPare commonly used by previous taxonomic rela-tion identification systems, we consider the com-bination of SIWN + LSP as the baseline of theexperiment.
The results in Table 3 show that thethree methods are all well complementary to eachother.
In addition, the proposed SCS method cancontribute up to about 27% - 29% of all the iden-tified taxonomic relations, which were not discov-ered by the other two baseline methods.Percentage of correct relationsTerrorism AI domainSCS 91% 88%SIWN 96% 91%LSP 93% 93%SCS + SIWN + LSP 92% 90%Table 4: Estimated precision of taxonomic relationidentification methods in 100 extracted relations.816Animals domain Plants domain Vehicles domainOur Kozareva Navigli Our Kozareva Navigli Our Kozareva Navigli#Correct relations 2427 1643 N.A.
1243 905 N.A.
281 246 N.A.Term coverage 96% N.A.
94% 98% N.A.
97% 97% N.A.
96%Precision 95% 98% 97% 95% 97% 97% 93% 99% 91%Recall 56% 38% 44% 53% 39% 38% 69% 60% 49%F-measure 71% 55% 61% 68% 56% 55% 79% 75% 64%Table 5: Comparison of (Navigli et al., 2011), (Kozareva and Hovy, 2010) and our system against Word-Net in three domains: Animals, Plants and Vehicles.We further evaluated the precision of each in-dividual taxonomic relation identification method.For AI and Terrorism domains, we again run thesystem with each of the three methods and with alltogether, and then randomly select 100 extractedtaxonomic relations each time.
These selected tax-onomic relations are then examined by two do-main experts to check the correctness.
The evalua-tion results are given in Table 4.
Note that only thefirst two modules of term suggestion/filtering andtaxonomic relation identification are employed forthis experiment as well.
The SIWN and LSPmeth-ods achieve high precision because they are basedon the gold-standard taxonomy hierarchy Word-Net and on the well-defined patterns, respectively.In contrast, the SCS method ambitiously looksfor terms pairs that share similar syntactic con-texts across sentences, though the contextual ev-idence is restricted to certain syntactic structures,and thus has a slightly lower precision comparedto the other two methods.In short, the SCS method is complementary tothe baseline methods, significantly improving thecoverage of the combined methods, when its pre-cision is comparable to those of the baseline meth-ods.
We performed next experiments to show thatthe SCS method overall has synergistic impact toimprove the F-measure of the combined methods.3.2 Evaluation against WordNetIn this experiment, we constructed taxonomiesfor three domains Animals, Plants and Vehicles,and then checked whether the identified relationscan be found in the WordNet, and which relationsin WordNet are not found by our method.
Notethat in this comparison, to be fair, we changed ouralgorithm to avoid using WordNet in identifyingtaxonomic relations.
Specifically, in the SIWNalgorithm, all operations of ??WN?
are replacedwith normal string-matching comparison, and all??WN?
relations are falsified.
The evaluationuses the following measures:Precision =#relations found in WordNet and by the method#relations found by the methodRecall =#relations found in WordNet and by the method#relations found in WordNetWe also compared our results with those ob-tained by the approaches of Navigli et al.
(2011)and Kozareva and Hovy (2010), where theyalso compared their resultant taxonomies againstWordNet.
In this comparison, all the three ap-proaches (i.e.
ours, the two previous methods)use the same corpora and term lists.
The com-parison results are given in Table 5.
?N.A.
?value means that this parameter is not applicable tothe corresponding method.
The results show thatour approach achieves better performance than theother two approaches, in terms of both the num-ber of correctly extracted taxonomic relations andthe term coverage.
Our system has a slightlylower precision than that of (Navigli et al., 2011)and (Kozareva and Hovy, 2010) due to the SCSmethod, but it significantly contributes to improvethe recall and eventually the F-measure over theother two systems.To judge the effectiveness of our proposed tax-onomy induction algorithm described in Section2.3, we compared it with the graph-based algo-rithm of Velardi et al.
(2012).
Recall that in this al-gorithm, they treat all taxonomic relations equally,and the pruning task is reduced to finding the besttrade-off between path length and the connectiv-ity of traversed nodes.
For each of five domains(i.e.
Terrorism, AI, Animals, Plants and Vehicles),we alternately run the two taxonomy inductionalgorithms over the same taxonomic relation setproduced by our taxonomic relation identificationprocess.
For Terrorism and AI domains, we ran-domly pick up 100 edges in each resultant taxon-817omy and ask two domain experts to judge for thecorrectness.
For Animals, Plants and Vehicles do-mains, we check the correctness of the edges in re-sultant taxonomies by comparing them against thecorresponding sub-hierarchies in WordNet.
Theevaluation is given in Table 6.
The results showthat the proposed taxonomy induction algorithmcan achieve better performance than the algorithmof Velardi et al.
(2012).
This may be due to the factthat our algorithm considers the scores of the iden-tified taxonomic relations from the relation identi-fication module, and thus is more precise in elim-inating incorrect relations during the pruning pro-cess.Percentage of correct edgesOur algorithm Velardi?s algorithmTerrorism 94% 90%AI 93% 88%Animals 95% 93%Plants 95% 92%Vehicles 93% 92%Table 6: Comparison of our taxonomy inductionalgorithms and that of Velardi et al.
(2012).In addition, when comparing Tables 4 and 6, wecan find that the precision of taxonomic relationsafter the pruning process is higher than that beforethe pruning process, which proves that the pro-posed taxonomy induction algorithm effectivelytrims the incorrect relations of Terrorism and AItaxonomies, leveraging the percentage of correctrelations 2% - 3% up.For the SCS method, besides the triple Subject-Verb-Object, we also explore other syntacticstructures like Noun-Preposition-Noun and Noun-Adjective-Noun.
For example, from the sentence?I visited Microsoft in Washington?, the triple(Microsoft, in, Washington) is extracted usingNoun-Preposition-Noun structure.
Similarly, fromthe sentence ?Washington is a beautiful city?, thetriple (Washington, beautiful, city) is extracted us-ing Noun-Adjective-Noun structure.
We then usethe triples for the contextual subsumption methoddescribed in Section 2.2.3, and test the methodagainst the Animals, Plants and Vehicles domains.The results are then compared against WordNetsub hierarchies.
The experiment results in Table7 show that the triples of Subject-Verb-Object givethe best performance compared to the other syn-tactic structures.
These can be explained as theS-V-O N-P-N N-A-NAnimals domainPrecision 95% 68% 72%Recall 56% 52% 47%F-measure 71% 59% 57%Plants domainPrecision 95% 63% 66%Recall 53% 41% 43%F-measure 68% 50% 52%Vehicles domainPrecision 93% 59% 60%Recall 69% 45% 48%F-measure 79% 51% 53%Table 7: Comparison of three syntactic struc-tures: S-V-O (Subject-Verb-Object), N-P-N(Noun-Preposition-Noun) and N-A-N (Noun-Adjective-Noun).number of triples of two types Noun-Preposition-Noun and Noun-Adjective-Noun are smaller thanthat of Subject-Verb-Object, and the number ofVerb is much greater than number of Prepositionor Adjective.All experiment results are available athttp://nlp.sce.ntu.edu.sg/wiki/projects/taxogen.4 ConclusionIn this paper, we proposed a novel method of iden-tifying taxonomic relations using contextual evi-dence from syntactic structure and Web data.
Thismethod is proved well complementary with pre-vious method of linguistic pattern matching.
Wealso present a novel graph-based algorithm to in-duce an optimal taxonomy from a given taxo-nomic relation set.
The experiment results showthat our system can generally achieve better per-formance than the state-of-the-art methods.
Inthe future, we will apply the proposed taxon-omy construction method to other domains suchas biomedicine and integrate it into other frame-works such as ontology authoring.ReferencesK.
Bollacker, C. Evans, P. Paritosh, T. Sturge and J.Taylor.
2008.
Freebase: a collaboratively createdgraph database for structuring human knowledge.In proceedings of the ACM SIGMOD InternationalConference onManagement of Data, pp.
1247-1250.A.
Budanitsky.
1999.
Lexical semantic relatedness818and its application in natural language process-ing.
Technical Report CSRG-390, Computer Sys-tems Research Group, University of Toronto.P.
Buitelaar, D. Olejnik and M. Sintek.
2004.
AProt?eg?e Plug-in for Ontology Extraction from TextBased on Linguistic Analysis.
In proceedings of the1st European Semantic Web Symposium, pp.
31-44.M.
Ciaramita, A. Gangemi, E. Ratsch, J. Saric and I.Rojas.
2005.
Unsupervised Learning of SemanticRelations Between Concepts of a Molecular BiologyOntology.
In proceedings of the 19th InternationalJoint Conference on Artificial Intelligence, pp.
659-664.J.
Edmonds.
1967.
Optimum branchings.
Journal ofResearch of the National Bureau of Standards, 71,pp.
233-240.S.
Fodeh, B. Punch and P. N. Tan.
2011.
On Ontology-driven Document Clustering Using Core SemanticFeatures.
Knowledge and information systems,28(2), pp.
395-421.H.
N. Fotzo and P. Gallinari.
2004.
Learning ?Gen-eralization/Specialization?
Relations between Con-cepts - Application for Automatically Building The-matic Document Hierarchies.
In proceedings of the7th International Conference on Computer-AssistedInformation Retrieval.M.
Geffet and I. Dagan.
2005.
The Distributional In-clusion Hypotheses and Lexical Entailment.
In pro-ceedings of the 43rd Annual Meeting of the ACL,pp.
107-114.R.
Girju, A. Badulescu, and D. Moldovan.
2003.Learning Semantic Constraints for the AutomaticDiscovery of Part-Whole Relations.
In proceedingsof the NAACL, pp.
1-8.S.
M. Harabagiu, S. J. Maiorano and M. A. Pasca.2003.
Open-Domain Textual Question AnsweringTechniques.
Natural Language Engineering, 9(3):pp.
1-38.M.
A. Hearst.
1992.
Automatic Acquisition of Hy-ponyms from Large Text Corpora.
In proceedingsof the 14th Conference on Computational Linguis-tics, pp.
539-545.D.
Klein and C. D. Manning.
2003.
Accurate Unlexi-calized Parsing.
In proceedings of the 41st AnnualMeeting of the ACL, pp.
423-430.Z.
Kozareva, E. Riloff, and E. H. Hovy.
2008.
Se-mantic Class Learning from the Web with HyponymPattern Linkage Graphs.
In proceedings of the 46thAnnual Meeting of the ACL, pp.
1048-1056.Z.
Kozareva and E. Hovy.
2010.
A Semi-supervisedMethod to Learn and Construct Taxonomies Usingthe Web.
In proceedings of the Conference on Em-pirical Methods in Natural Language Processing, pp.1110-1118.C.
Matuszek, J. Cabral, M. J. Witbrock and J. DeO-liveira.
2006.
An Introduction to the Syntax andContent of Cyc.
In proceedings of the AAAI SpringSymposium: Formalizing and Compiling Back-ground Knowledge and Its Applications to Knowl-edge Representation and Question Answering, pp.44-49.G.
A. Miller.
1995.
WordNet: a Lexical Database forEnglish.
Communications of the ACM, 38(11), pp.39-41.R.
Navigli and P. Velardi, 2004.
Learning DomainOntologies from Document Warehouses and Dedi-cated Web Sites.
Computational Linguistics, 30(2),pp.
151-179.R.
Navigli, P. Velardi and S. Faralli.
2011.
A Graph-based Algorithm for Inducing Lexical Taxonomiesfrom Scratch.
In proceedings of the 20th Interna-tional Joint Conference on Artificial Intelligence,pp.
1872-1877.R.
Snow, D. Jurafsky and A. Y. Ng.
2006.
SemanticTaxonomy Induction from Heterogenous Evidence.In proceedings of the 21st International Conferenceon Computational Linguistics, pp.
801-808.P.
Velardi, S. Faralli and R. Navigli.
2012.
OntolearnReloaded: A Graph-based Algorithm for TaxonomyInduction.
Computational Linguistics, 39(3), pp.665-707.W.
Wentao, L. Hongsong, W. Haixun, and Q. Zhu.2012.
Probase: A probabilistic taxonomy for textunderstanding.
In proceedings of the ACM SIG-MOD International Conference on Management ofData, pp.
481-492.D.
Widdows and B. Dorow.
2002.
A Graph Model forUnsupervised Lexical Acquisition.
In proceedingsof the 19th International Conference on Computa-tional Linguistics, pp.
1-7.W.
Wong, W. Liu and M. Bennamoun.
2007.
Tree-traversing ant algorithm for term clustering basedon featureless similarities.
Data Mining and Knowl-edge Discovery, 15(3), pp.
349-381.819
