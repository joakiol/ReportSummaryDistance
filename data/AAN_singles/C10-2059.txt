Coling 2010: Poster Volume, pages 516?524,Beijing, August 2010Effective Constituent Projection across LanguagesWenbin Jiang and Yajuan Lu?
and Yang Liu and Qun LiuKey Laboratory of Intelligent Information ProcessingInstitute of Computing TechnologyChinese Academy of Sciences{jiangwenbin, lvyajuan, yliu, liuqun}@ict.ac.cnAbstractWe describe an effective constituent pro-jection strategy, where constituent pro-jection is performed on the basis of de-pendency projection.
Especially, a novelmeasurement is proposed to evaluate thecandidate projected constituents for a tar-get language sentence, and a PCFG-styleparsing procedure is then used to searchfor the most probable projected con-stituent tree.
Experiments show that, theparser trained on the projected treebankcan significantly boost a state-of-the-artsupervised parser.
When integrated into atree-based machine translation system, theprojected parser leads to translation per-formance comparable with using a super-vised parser trained on thousands of anno-tated trees.1 IntroductionIn recent years, supervised constituent parsing hasbeen well studied and achieves the state-of-the-artfor many resource-rich languages (Collins, 1999;Charniak, 2000; Petrov et al, 2006).
Becauseof the cost and difficulty in treebank construc-tion, researchers have also investigated the utiliza-tion of unannotated text, including the unsuper-vised parsing which totally uses unannotated data(Klein and Manning, 2002; Klein and Manning,2004; Bod, 2006; Seginer, 2007), and the semi-supervised parsing which uses both annotated andunannotated data (Sarkar, 2001; Steedman et al,2003; McClosky et al, 2006).Because of the higher complexity and lowerperformance of unsupervised methods, as well asthe need of reliable priori knowledge in semi-supervised methods, it seems promising to projectthe syntax structures from a resource-rich lan-guage to a resource-scarce one across a bilingualcorpus.
Lots of researches have so far been de-voted to dependency projection (Hwa et al, 2002;Hwa et al, 2005; Ganchev et al, 2009; Smithand Eisner, 2009).
While for constituent projec-tion there is few progress.
This is due to the factthat the constituent syntax describes the languagestructure in a more detailed way, and the degree ofisomorphism between constituent structures ap-pears much lower.In this paper we propose for constituent pro-jection a stepwise but totally automatic strategy,which performs constituent projection on the ba-sis of dependency projection, and then use a con-straint EM optimization algorithm to optimizedthe initially projected trees.
Given a word-alignedbilingual corpus with source sentences parsed, wefirst project the dependency structures of theseconstituent trees to the target sentences using adynamic programming algorithm, then we gener-ate a set of candidate constituents for each targetsentence and design a novel evaluation functionto calculate the probability of each candidate con-stituent, finally, we develop a PCFG-style parsingprocedure to search for the most probable pro-jected constituent tree in the evaluated candidateconstituent set.
In addition, we design a constraintEM optimization procedure to decrease the noisein the initially projected constituent treebank.Experimental results validate the effectivenessof our approach.
On the Chinese-English FBIScorpus, we project the English parses producedby the Charniak parser across to the Chinese sen-516tences.
A berkeley parser trained on this pro-jected treebank can effectively boost the super-vised parsers trained on bunches of CTB trees.Especially, the supervised parser trained on thesmaller CTB 1.0 benefits a significant F-measureincrement of more than 1 point from the projectedparser.
When using the projected parser in a tree-based translation model (Liu et al, 2006), weachieve translation performance comparable withusing a state-of-the-art supervised parser trainedon thousands of CTB trees.
This surprising re-sult gives us an inspiration that better translationwould be achieved by combining both projectedparsing and supervised parsing into a hybrid pars-ing schema.2 Stepwise Constituent ProjectionWe first introduce the dynamic programming pro-cedure for dependency projection, then describethe PCFG-style algorithm for constituent projec-tion which is conducted on projected dependentstructures, and finally show the constraint EMprocedure for constituent optimization.2.1 Dependency ProjectionFor dependency projection we adopt a dynamicprogramming algorithm, which searches the mostprobable projected target dependency structureaccording to the source dependency structure andthe word alignment.In order to mitigate the effect of word alignmenterrors, multiple GIZA++ (Och and Ney, 2000) re-sults are combined into a compact representationcalled alignment matrix.
Given a source sentencewith m words, represented as E1:m, and a targetsentence with n words, represented as F1:n, theirword alignment matrix A is an m ?
n matrix,where each element Ai,j denotes the probabilityof the source word Ei aligned to the target wordFj .Using P (DF |DE , A) to denote the probabilityof the projected target dependency structure DFconditioned on the source dependency structureDE and the alignment matrix A, the projection al-gorithm aims to findD?F = argmaxDFP (DF |DE , A) (1)Algorithm 1 Dependency projection.1: Input: F , and Pe for all word pairs in F2: for ?i, j?
?
?1, |F |?
in topological order do3: buf ?
?4: for k?
i..j ?
1 do ?
all partitions5: for l ?
V[i, k] and r ?
V[k + 1, j] do6: insert DERIV(l, r, Pe) into buf7: insert DERIV(r, l, Pe) into buf8: V[i, j]?
top K derivations of buf9: Output: the best derivation of V[1, |F |]10: function DERIV(p, c, Pe)11: d?
p ?
c ?
{p ?
rooty c ?
root} ?
new derivation12: d ?
evl ?
EVAL(d, Pe) ?
evaluation function13: return dP (DF |DE , A) can be factorized into each depen-dency edge xy y in DFP (DF |DE , A) =?xyy?DFPe(xy y|DE , A)Pe can then be obtained by simple accumulationacross all possible situations of correspondencePe(xy y|DE , A)=?1?x?,y??|E|Ax,x?
?Ay,y?
?
?
(x?, y?|DE)where ?
(x?, y?|DE) is a 0-1 function that equals1 only if the dependent relation x?
y y?
holds inDE .The search procedure needed by the argmax op-eration in equation 1 can be effectively solvedby the Chu-Liu-Edmonds algorithm used in (Mc-Donald et al, 2005).
In this work, however, weadopt a more general and simple dynamic pro-gramming algorithm as shown in Algorithm 1,in order to facilitate the possible expansions.
Inpractice, the cube-pruning strategy (Huang andChiang, 2005) is used to speed up the enumera-tion of derivations (loops started by line 4 and 5).2.2 Constituent ProjectionThe PCFG-style parsing procedure searches forthe most probable projected constituent tree ina shrunken search space determined by the pro-jected dependency structure and the target con-stituent tree.
The shrunken search space can bebuilt as following.
First, we generates the candi-date constituents of the source tree and the can-didate spans of the target sentence, so as to enu-merate the candidate constituents of the target sen-tence.
Then we compute the consistent degree for517each pair of candidate constituent and span, andfurther estimate the probability of each candidateconstituent for the target sentence.2.2.1 Candidate Constituents and SpansFor the candidate constituents of the sourcetree, using only the original constituents imposesa strong hypothesis of isomorphism on the con-stituent projection between two languages, sinceit requires that each couple of constituent and spanmust be strictly matched.
While for the candi-date spans of the target sentences, using all sub-sequences makes the search procedure suffer frommore perplexity.
Therefore, we expand the candi-date constituent set and restrict the candidate spanset:?
Candidate Constituent: Suppose a produc-tion in the source constituent tree, denoted asp ?
c1c2..ch..c|p|, and ch is the head childof the parent p. Each constituent, p or c, is atriple ?lb, rb, nt?, where nt denotes its non-terminal, while lb and rb represent its left-and right bounds of the sub-sequence that theconstituent covers.
The candidate constituentset of this production consists the head ofthe production itself, and a set of incompleteconstituents,{?l, r, p ?
nt?
?|c1 ?
lb ?
l ?
ch ?
lb?ch ?
rb ?
r ?
c|p| ?
rb?
(l < ch ?
lb ?
r > ch ?
rb)}where the symbol ?
indicates an incompletenon-terminal.
The candidate constituent setof the entire source tree is the unification ofthe sets extracted from all productions of thetree.?
Candidate Span: A candidate span of the tar-get sentence is a tuple ?lb, rb?, where lb andrb indicate the same as in a constituent.
Wedefine the candidate span set as the spans ofall regular dependent segments in the corre-sponding projected dependency structure.
Aregular dependency segment is a dependentsegment that every modifier of the root is acomplete dependency structure.
Suppose adependency structure rooted at word p, de-noted as clL..cl2cl1 x p y cr1cr2..crR, ithas L (L ?
0) modifiers on its left and R(R ?
0) modifiers on its right, each of themis a smaller complete dependency structure.Then the word p itself is a regular depen-dency segment without any modifier, and{cli..cl1 x py cr1..crj |0 ?
i ?
L?0 ?
j ?
R?
(i > 0 ?
j > 0)}is a set of regular dependency structures withat least one modifier.
The regular depen-dency segments of the entire projected de-pendency structure can simply be accumu-lated across all dependency nodes.2.2.2 Span-to-Constituent CorrespondenceAfter determining the candidate constituent setof the source tree, denoted as ?E , and the can-didate span set of the target sentence, denoted as?F , we then calculate the consistent degree foreach pair of candidate constituent and candidatespan.Given a candidate constituent ?
?
?E and acandidate span ?
?
?F , their consistent degreeC(?, ?|A) is the probability that they are alignedto each other according to A.We display the derivations from bottom to up.First, we define the alignment probability from aword i in the span ?
to the constituent ?
asP (i 7?
?|A) =???lb?j??
?rbAi,j?j Ai,jThen we define the alignment probability from thespan ?
to the constituent ?
asP (?
7?
?|A) =???lb?i??
?rbP (i 7?
?|A)Note that we use i to denote both a word and its in-dex for simplicity without causing confusion.
Fi-nally, we define C(?,?|A) asC(?, ?|A) = P (?
7?
?|A)?
P (?
7?
?|AT ) (2)Where P (?
7?
?|AT ) denotes the alignmentprobability from the constituent ?
to the span ?, itcan be calculated in the same manner.5182.2.3 Constituent Projection AlgorithmThe purpose of constituent projection is to findthe most probable projected constituent tree forthe target sentence conditioned on the source con-stituent tree and the word alignmentT?F = argmaxTF?
?FP (TF |TE, A) (3)Here, we use ?F to denote the set of candidateconstituents of the target sentence?F = ?F ?NT (?E)= {?F |?
(?F ) ?
?F ?
nt(?F ) ?
NT (?E)}where ?(?)
and nt(?)
represent the span and thenon-terminal of a constituent respectively, andNT (?)
represents the set of non-terminals ex-tracted from a constituent set.
Note that TF is asubset of ?F if we treat a tree as a set of con-stituents.The probability of the projected tree TF can befactorized into the probabilities of the projectedconstituents that composes the treeP (TF |TE , A) =??F?TFP?
(?F |TE , A)while the probability of the projected source con-stituent can be defined as a statistics of span-to-constituent- and constituent-to-constituent consis-tent degreesP?
(?F |TE , A) =??E?
?E C(?F , ?E |A)??E?
?E C(?
(?F ), ?E |A)where C(?F , ?E |A) in the numerator denotes theconsistent degree for each pair of constituents,which can be calculated based on that of span andconstituent described in Formula 2C(?F , ?E) ={0 if ?F ?
nt 6= ?E ?
ntC(?
(?F ), ?E) elseAlgorithm 2 shows the pseudocode for con-stituent projection.
A PCFG-style parsing pro-cedure searches for the best projected constituenttree in the constrained space determined by ?F .Note that the projected trees are binarized, and canbe easily recovered according to the asterisks atthe tails of non-terminals.Algorithm 2 Constituent projection.1: Input: ?F , ?F , and P?
for all spans in ?F2: for ?i, j?
?
?
in topological order do3: buf ?
?4: for p ?
?F s.t.
?
(p) = ?i, j?
do5: for k?
i..j ?
1 do ?
all partitions6: for l ?
V[i, k] and r ?
V[k + 1, j] do7: insert DERIV(l, r, p, P?)
into buf8: V[i, j]?
top K derivations of buf9: Output: the best derivation of V[1, |F |]10: function DERIV(l, r, p, P?
)11: d?
l ?
r ?
{p} ?
new derivation12: d ?
evl ?
EVAL(d, P?)
?
evaluation function13: return d2.3 EM OptimizationSince the constituent projection is conducted oneach sentence pair separately, the projected tree-bank is apt to suffer from more noise caused byfree translation and word alignment error.
It canbe expected that an EM iteration over the wholeprojected treebank will lead to trees with higherconsistence.We adopt the inside-outside algorithm to im-prove the quality of the initially projected tree-bank.
Different from previous works, all expecta-tion and maximization operations for a single treeare performed in a constrained space determinedby the candidate span set of the projected targetdependency structure.
That is to say, all the sum-mation operations, both for calculating ?/?
valuesand for re-estimating the rule probabilities, onlyconsider the spans in the candidate span set.
Thismeans that the projected dependency structuresare supposed believable, and the noise is mainlyintroduced in the following constituent projectionprocedure.Here we give an overall description of the tree-bank optimization procedure.
First, an initialPCFG grammar G0F is estimated from the originalprojected treebank.
Then several iterations of ?/?calculation and rule probability re-estimation areperformed.
For example in the i-the iteration, ?/?values are calculated based on the current gram-mar Gi?1F , afterwards the optimized grammar GiFis obtained based on these ?/?
values.
The itera-tive procedure terminates when the likelihood ofwhole treebank increases slowly.
Finally, with theoptimized grammar, a constrained PCFG parsingprocedure is conducted on each of the initial pro-519jected trees, so as to obtain an optimized treebank.3 Applications of Constituent ProjectionThe most direct contribution of constituent pro-jection is pushing an initial step for the statis-tical constituent parsing of resource-scarce lan-guages.
It also has some meaningful applica-tions even for the resource-rich languages.
Forinstances, the projected treebank, due to its largescale and high coverage, can used to boost an tra-ditional supervised-trained parser.
And, the parsertrained on the projected treebank can adopted toconduct tree-to-string machine translation, sinceit give parsing results with larger isomorphismwith the target language than a supervised-trainedparser dose.3.1 Boost an Traditional ParserWe first establish a unified framework for the en-hanced parser where a projected parser is adoptedto guide the parsing procedure of the baselineparser.For a given target sentence S, the enhancedparser selected the best parse T?
among the setof candidates ?
(S) according to two evaluationfunctions, given by the baseline parser B and theprojected guide parser G, respectively.T?
= argmaxT??
(S)P (T |B)?
P (T |G)?
(4)These two evaluation functions can be integrateddeeply into the decoding procedure (Carreras etal., 2008; Zhang and Clark, 2008; Huang, 2008),or can be integrated at a shallow level in a rerank-ing manner (Collins, 2000; Charniak and John-son, 2005).
For simplicity and generability, weadopt the reranking strategy.
In k-best reranking,?
(S) is simply a set of candidate parses, denotedas {T1, T2, ..., Tk}, and we use the single parse ofthe guide parser, TG, to re-evaluate these candi-dates.
Formula 4 can be redefined asT?
(TG) = argmaxT??
(S)w ?
f(T, TG) (5)Here, f(T, TG) and w represent a high dimen-sional feature representation and a correspond-ing weight vector, respectively.
The first featuref1(T, TG) = logP (T |B) is the log probabilityof the baseline parser, while the remaining fea-tures are integer-valued guide features, and eachof them represents the guider parser?s predicationresult for a particular configuration in candidateparse T , so as to utilize the projected parser?sknowledge to guide the parsing procedure of thetraditional parser.In our work a guide feature is composed of twoparts, the non-terminal of a certain constituent ?in the candidate parse T ,1 and the non-terminalat the corresponding span ?(?)
in the projectedparse TG.
Note that in the projected parse thisspan does not necessarily correspond to a con-stituent.
In such situations, we simply use thenon-terminal of the constituent that just be ableto cover this span, and attach a asterisk at the tailof this non-terminal.
Here is an example of theguide featuresf100(T, TG) = V P ?
T ?
PP?
?
TGIt represents that a V P in the candidate parse cor-responds to a segment of a PP in the projectedparse.
The quantity of its weight w100 indicateshow probably a span can be predicated as V P ifthe span corresponds to a partial PP in the pro-jected parse.We adopt the perceptron algorithm to trainthe reranker.
To reduce overfitting and pro-duce a more stable weight vector, we also usea refinement strategy called averaged parameters(Collins, 2002).3.2 Using in Machine TranslationResearchers have achieved promising improve-ments in tree-based machine translation (Liu etal., 2006; Huang et al, 2006).
Such models usea parsed tree as input and converts it into a targettree or string.
Given a source language sentence,first we use a traditional source language parserto parse the sentence to obtain the syntax tree T ,and then use the translation decoder to search forthe best derivation d?, where a derivation d is a se-quence of transformations that converts the sourcetree into the target language stringd?
= argmaxd?DP (d|T ) (6)1Using non-terminals as features brings no improvementin the reranking experiments, so as to examine the impact ofthe projected parser.520Here D is the candidate set of d, and it is deter-mined by the source tree T and the transformationrules.Since the tree-based models are based onthe synchronous transformational grammars, theysuffer much from the isomerism between thesource syntax and the target sentence structure.Considering that the parsed tree produced by aprojected parser may have larger isomorphismwith the target language, it would be a promis-ing idea to adopt the projected parser to parse theinput sentence for the subsequent translation de-coding procedure.4 ExperimentsIn this section, we first invalidate the effect of con-stituent projection by evaluating a parser trainedon the projected treebank.
Then we investigatetwo applications of the projected parser: boostingan traditional supervised-trained parser, and inte-gration in a tree-based machine translation sys-tem.
Following the previous works, we depict theparsing performance by F-score on sentences withno more than 40 words, and evaluate the transla-tion quality by the case-sensitive BLEU-4 metric(Papineni et al, 2002) with 4 references.4.1 Constituent ProjectionWe perform constituent projection from Englishto Chinese on the FBIS corpus, which contains239K sentence pairs with about 6.9M/8.9M wordsin Chinese/English.
The English sentences areparsed by the Charniak Parser and the dependencystructures are extracted from these parses accord-ing to the head-finding rules of (Yamada andMatsumoto, 2003).
The word alignment matrixesare obtained by combining the 10-best results ofGIZA++ according to (Liu et al, 2009).We first project the dependency structures fromEnglish to Chinese according to section 2.1, andthen project the constituent structures accordingto section 2.2.
We define an assessment criteriato evaluate the confidence of the final projectedconstituent treec = n?P (DF |DE , A) ?
P (TF |TE , A)where n is the word count of a Chinese sentencein our experiments.
A series of projected Chi-Thres c #Resrv Cons-F1 Span-F10.5 12.6K 23.9 32.70.4 17.8K 23.9 33.40.3 27.2K 25.4 35.70.2 45.1K 26.6 38.00.1 87.0K 27.8 40.4Table 1: Performances of the projected parserson the CTB test set.
#Resrv denotes the amountof reserved trees within threshold c. Cons-F1 isthe traditional F-measure, while Span-F1 is the F-measure without consideration of non-terminals.nese treebanks with different scales are obtainedby specifying different c as the filtering threshold.The state-of-the-art Berkeley Parser is adopted totrain on these treebanks because of its high per-formance and independence of head word infor-mation.Table 1 shows the performances of these pro-jected parsers on the standard CTB test set, whichis composed of sentences in chapters 271-300.We find that along with the decrease of the filter-ing threshold c, more projected trees are reservedand the performance of the projected parser con-stantly increases.
We also find that the traditionalF-value, Cons-F1, is obviously lower than the onewithout considering non-terminals, Span-F1.
Thisindicates that the constituent projection procedureintroduces more noise because of the higher com-plexity of constituent correspondence.
In all therest experiments, however, we simply use the pro-jected treebank filtered by threshold c = 0.1 anddo not try any smaller thresholds, since it alreadytakes more than one weak to train the BerkeleyParser on the 87 thousands trees resulted by thisthreshold.The constrained EM optimization proceduredescribed in section 2.3 is used to alleviate thenoise in the projected treebank, which may becaused by free translation, word alignment errors,and projection on each single sentence pair.
Fig-ure 1 shows the log-likelihood on the projectedtreebank after each EM iteration.
It is obvious thatthe log-likelihood increases very slowly after 10iterations.
We terminate the EM procedure after40 iterations.Finally we train the Berkeley Parser on the op-timized projected treebank, and test its perfor-521-65-64-63-62-61-60-59-580  5  10  15  20  25  30  35  40Log-likelihoodEM iterationFigure 1: Log-likelihood of the 87K-projectedtreebank after each EM interation.Train Set Cons-F1 Span-F1Original 87K 27.8 40.4Optimized 87K 22.8 40.2Table 2: Performance of the parser trained on theoptimized projected treebank, compared with thatof the original projected parser.Train Set Baseline Bst-Ini Bst-OptCTB 1.0 75.6 76.4 76.9CTB 5.0 85.2 85.5 85.7Table 3: Performance improvement brought bythe projected parser to the baseline parsers trainedon CTB 1.0 and CTB 5.0, respectively.
Bst-Ini/Bst-Opt: boosted by the parser trained on theinitial/optimized projected treebank.mance on the standard CTB test set.
Table 2shows the performance of the parser trained onthe optimized projected treebank.
Unexpectedly,we find that the constituent F1-value of the parsertrained on the optimized treebank drops sharplyfrom the baseline, although the span F1-value re-mains nearly the same.
We assume that the EMprocedure gives the original projected treebankmore consistency between each single tree whilethe revised treebank deviates from the CTB anno-tation standard, but it needs to be validated by thefollowing experiments.4.2 Boost an Traditional ParserThe projected parser is used to help the rerankingof the k-best parses produced by another state-of-the-art parser, which is called the baseline parserfor convenience.
In our experiments we choosethe revised Chinese parser (Xiong et al, 2005)707274767880828486881000  10000ParsevalF-score(%)Scale of treebank (log)CTB 1.0CTB 5.0baselineboosted parserFigure 2: Boosting performance of the projectedparser on a series of baseline parsers that aretrained on treebanks of different scales.based on Collins model 2 (Collins, 1999) as thebaseline parser.2The baseline parser is respectively trained onCTB 1.0 and CTB 5.0.
For both corpora wefollow the traditional corpus splitting: chapters271-300 for testing, chapters 301-325 for devel-opment, and else for training.
Experimental re-sults are shown in Table 3.
We find that bothprojected parsers bring significant improvement tothe baseline parsers.
Especially the later, althoughperforms worse on CTB standard test set, gives alarger improvement than the former.
This to somedegree confirms the previous assumption.
How-ever, more investigation must be conducted in thefuture.We also observe that for the baseline parsertrained on the much larger CTB 5.0, the boost-ing performance of the projected parser is rela-tively lower.
To further investigate the regularitythat the boosting performance changes accordingto the scale of training treebank of the baselineparser, we train a series of baseline parsers withdifferent amounts of trees, then use the projectedparser trained on the optimized treebank to en-hance these baseline parsers.
Figure 2 shows theexperimental results.
From the curves we can seethat the smaller the training corpus of the baselineparser, the more significant improvement can beobtained.
This is a good news for the resource-scarce languages that have no large treebanks.2The Berkeley Parser fails to give k-best parses for somesentences when trained on small treebanks, and these sen-tences have to be deleted in the k-best reranking experiments.5224.3 Using in Machine TranslationWe investigate the effect of the projected parserin the tree-based translation model on Chinese-to-English translation.
A series of contrast transla-tion systems are built, each of which uses a super-vised Chinese parser (Xiong et al, 2005) trainedon a particular amount of CTB trees.We use the FBIS Chinese-English bitext as thetraining corpus, the 2002 NIST MT Evaluationtest set as our development set, and the 2005 NISTMT Evaluation test set as our test set.
We first ex-tract the tree-to-string translation rules from thetraining corpus by the algorithm of (Liu et al,2006), and train a 4-gram language model onthe Xinhua portion of GIGAWORD corpus withKneser-Ney smoothing using the SRI LanguageModeling Toolkit (Stolcke and Andreas, 2002).Then we use the standard minimum error-ratetraining (Och, 2003) to tune the feature weightsto maximize the system.s BLEU score.Figure 3 shows the experimental results.
Wefind that the translation system using the projectedparser achieves the performance comparable withthe one using the supervised parser trained onCTB 1.0.
Considering that the F-score of the pro-jected parser is only 22.8%, which is far below ofthe 75.6% F-score of the supervised parser trainedon CTB 1.0, we can give more confidence to theassumption that the projected parser is apt to de-scribe the syntax structure of the counterpart lan-guage.
This surprising result also gives us an in-spiration that better translation would be achievedby combining projected parsing and supervisedparsing into hybrid parsing schema.5 ConclusionThis paper describes an effective strategy for con-stituent projection, where dependency projectionand constituent projection are consequently con-ducted to obtain the initial projected treebank,and an constraint EM procedure is then per-formed to optimized the projected trees.
Theprojected parser, trained on the projected tree-bank, significantly boosts an existed state-of-the-art supervised-trained parser, especially trained ona smaller treebank.
When using the projectedparser in tree-based translation, we achieve the0.2200.2300.2400.2500.2600.2701000  10000BLEUscoreScale of treebank (log)use projected parserCTB 1.0CTB 5.0use supervised parsersFigure 3: Performances of the translation systems,which use the projected parser and a series of su-pervised parsers trained CTB trees.translation performance comparable with using asupervised parser trained on thousands of human-annotated trees.As far as we know, this is the first time thatthe experimental results are systematically re-ported about the constituent projection and its ap-plications.
However, many future works needto do.
For example, more energy needs to bedevoted to the treebank optimization, and hy-brid parsing schema that integrates the strengthsof both supervised-trained parser and projectedparser would be valuable to be investigated forbetter translation.AcknowledgmentsThe authors were supported by 863 State KeyProject No.
2006AA010108, National NaturalScience Foundation of China Contract 60873167,Microsoft Research Asia Natural Language Pro-cessing Theme Program grant (2009-2010), andNational Natural Science Foundation of ChinaContract 90920004.
We are grateful to the anony-mous reviewers for their thorough reviewing andvaluable suggestions.ReferencesBod, Rens.
2006.
An all-subtrees approach to unsu-pervised parsing.
In Proceedings of the COLING-ACL.Carreras, Xavier, Michael Collins, and Terry Koo.2008.
Tag, dynamic programming, and the percep-tron for efficient, feature-rich parsing.
In Proceed-ings of the CoNLL.523Charniak, Eugene and Mark Johnson.
2005.
Coarse-to-fine-grained n-best parsing and discriminativereranking.
In Proceedings of the ACL.Charniak, Eugene.
2000.
A maximum-entropy-inspired parser.
In Proceedings of the NAACL.Collins, Michael.
1999.
Head-driven statistical mod-els for natural language parsing.
In Ph.D. Thesis.Collins, Michael.
2000.
Discriminative reranking fornatural language parsing.
In Proceedings of theICML, pages 175?182.Collins, Michael.
2002.
Discriminative training meth-ods for hidden markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof the EMNLP, pages 1?8, Philadelphia, USA.Ganchev, Kuzman, Jennifer Gillenwater, and BenTaskar.
2009.
Dependency grammar induction viabitext projection constraints.
In Proceedings of the47th ACL.Huang, Liang and David Chiang.
2005.
Better k-bestparsing.
In Proceedings of the IWPT, pages 53?64.Huang, Liang, Kevin Knight, and Aravind Joshi.
2006.Statistical syntax-directed translation with extendeddomain of locality.
In Proceedings of the AMTA.Huang, Liang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In Proceedings ofthe ACL.Hwa, Rebecca, Philip Resnik, Amy Weinberg, andOkan Kolak.
2002.
Evaluating translational corre-spondence using annotation projection.
In Proceed-ings of the ACL.Hwa, Rebecca, Philip Resnik, Amy Weinberg, ClaraCabezas, and Okan Kolak.
2005.
Bootstrap-ping parsers via syntactic projection across paral-lel texts.
In Natural Language Engineering, vol-ume 11, pages 311?325.Klein, Dan and Christopher D. Manning.
2002.
Agenerative constituent-context model for improvedgrammar induction.
In Proceedings of the ACL.Klein, Dan and Christopher D. Manning.
2004.
Cor-pusbased induction of syntactic structure: Modelsof dependency and constituency.
In Proceedings ofthe ACL.Liu, Yang, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machinetranslation.
In Proceedings of the ACL.Liu, Yang, Tian Xia, Xinyan Xiao, and Qun Liu.
2009.Weighted alignment matrices for statistical machinetranslation.
In Proceedings of the EMNLP.McClosky, David, Eugene Charniak, and Mark John-son.
2006.
Reranking and self-training for parseradaptation.
In Proceedings of the ACL.McDonald, Ryan, Fernando Pereira, Kiril Ribarov, andJan Hajic?.
2005.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proceedingsof HLT-EMNLP.Och, Franz J. and Hermann Ney.
2000.
Improvedstatistical alignment models.
In Proceedings of theACL.Och, Franz Joseph.
2003.
Minimum error rate train-ing in statistical machine translation.
In Proceed-ings of the 41th Annual Meeting of the Associationfor Computational Linguistics, pages 160?167.Papineni, Kishore, Salim Roukos, Todd Ward, andWeijing Zhu.
2002.
Bleu: a method for automaticevaluation of machine translation.
In Proceedingsof the ACL.Petrov, Slav, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and in-terpretable tree annotation.
In Proceedings of theACL.Sarkar, Anoop.
2001.
Applying co-training methodsto statistical parsing.
In Proceedings of NAACL.Seginer, Yoav.
2007.
Fast unsupervised incrementalparsing.
In Proceedings of the ACL.Smith, David and Jason Eisner.
2009.
Parser adap-tation and projection with quasi-synchronous gram-mar features.
In Proceedings of EMNLP.Steedman, Mark, Miles Osborne, Anoop Sarkar,Stephen Clark, Rebecca Hwa, Julia Hockenmaier,Paul Ruhlen, Steven Baker, and Jeremiah Crim.2003.
Bootstrapping statistical parsers from smalldatasets.
In Proceedings of the EACL.Stolcke and Andreas.
2002.
Srilm - an extensible lan-guage modeling toolkit.
In Proceedings of the Inter-national Conference on Spoken Language Process-ing, pages 311?318.Xiong, Deyi, Shuanglong Li, Qun Liu, and ShouxunLin.
2005.
Parsing the penn chinese treebank withsemantic knowledge.
In Proceedings of IJCNLP2005, pages 70?81.Yamada, H and Y Matsumoto.
2003.
Statistical de-pendency analysis using support vector machines.In Proceedings of IWPT.Zhang, Yue and Stephen Clark.
2008.
A tale oftwo parsers: investigating and combining graph-based and transition-based dependency parsing us-ing beam-search.
In Proceedings of EMNLP.524
