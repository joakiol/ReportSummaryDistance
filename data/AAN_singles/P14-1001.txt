Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1?12,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsLearning Ensembles of Structured Prediction RulesCorinna CortesGoogle Research111 8th Avenue,New York, NY 10011corinna@google.comVitaly KuznetsovCourant Institute251 Mercer Street,New York, NY 10012vitaly@cims.nyu.eduMehryar MohriCourant Institute and Google Research251 Mercer Street,New York, NY 10012mohri@cims.nyu.eduAbstractWe present a series of algorithms with the-oretical guarantees for learning accurateensembles of several structured predictionrules for which no prior knowledge is as-sumed.
This includes a number of ran-domized and deterministic algorithms de-vised by converting on-line learning al-gorithms to batch ones, and a boosting-style algorithm applicable in the context ofstructured prediction with a large numberof labels.
We also report the results of ex-tensive experiments with these algorithms.1 IntroductionWe study the problem of learning accurate en-sembles of structured prediction experts.
Ensem-ble methods are widely used in machine learn-ing and have been shown to be often very effec-tive (Breiman, 1996; Freund and Schapire, 1997;Smyth and Wolpert, 1999; MacKay, 1991; Fre-und et al, 2004).
However, ensemble methods andtheir theory have been developed primarily for bi-nary classification or regression tasks.
Their tech-niques do not readily apply to structured predic-tion problems.
While it is straightforward to com-bine scalar outputs for a classification or regres-sion problem, it is less clear how to combine struc-tured predictions such as phonemic pronuncia-tion hypotheses, speech recognition lattices, parsetrees, or alternative machine translations.Consider for example the problem of devisingan ensemble method for pronunciation, a crit-ical component of modern speech recognition(Ghoshal et al, 2009).
Often, several pronunci-ation models or experts are available for transcrib-ing words into sequences of phonemes.
Thesemodels may have been derived using other ma-chine learning algorithms or they may be based oncarefully hand-crafted rules.
In general, none ofthese pronunciation experts is fully accurate andeach expert may be making mistakes at differentpositions along the output sequence.
One can hopethat a model that patches together the pronuncia-tion of different experts could achieve a superiorperformance.Similar ensemble structured prediction problemsarise in other tasks, including machine translation,part-of-speech tagging, optical character recogni-tion and computer vision, with structures or sub-structures varying with each task.
We seek totackle all of these problems simultaneously andconsider the general setting where the label or out-put associated to an input x ?
X is a structurey ?
Y that can be decomposed and representedby l substructures y1, .
.
.
, yl.
For the pronuncia-tion example just discussed, x is a specific wordor word sequence and y its phonemic transcrip-tion.
A natural choice for the substructures ykisthen the individual phonemes forming y. Otherpossible choices include n-grams of consecutivephonemes or more general subsequences.We will assume that the loss function consideredadmits an additive decomposition over the sub-structures, as is common in structured prediction.We also assume access to a set of structured pre-diction experts h1, .
.
.
, hpthat we treat as blackboxes.
Given an input x ?
X , each expert pre-dicts a structure hj(x) = (h1j(x), .
.
.
, hlj(x)).
Thehypotheses hjmay be the output of a structuredprediction algorithm such as Conditional RandomFields (Lafferty et al, 2001), Averaged Perceptron(Collins, 2002), StructSVM (Tsochantaridis et al,2005), Max Margin Markov Networks (Taskar etal., 2004) or the Regression Technique for Learn-ing Transductions (Cortes et al, 2005), or someother algorithmic or human expert.
Given a la-beled training sample (x1,y1), .
.
.
, (xm,ym), ourobjective is to use the predictions of these experts1to form an accurate ensemble.Variants of the ensemble problem just formulatedhave been studied in the past in the natural lan-guage processing and machine learning literature.One of the most recent, and possibly most rele-vant studies for sequence data is that of (Nguyenand Guo, 2007), which is based on the forwardstepwise selection introduced by (Caruana et al,2004).
However, one disadvantage of this greedyapproach is that it can be proven to fail to selectan optimal ensemble of experts even in favorablecases where a specialized expert is available foreach local prediction (Cortes et al, 2014a).
En-semble methods for structured prediction based onbagging, random forests and random subspaceshave also been proposed in (Kocev et al, 2013).One of the limitations of this work is that it isapplicable only to a very specific class of tree-based experts introduced in that paper.
Similarly, aboosting approach was developed in (Wang et al,2007) but it applies only to local experts.
In thecontext of natural language processing, a varietyof different re-ranking techniques have been pro-posed for somewhat related problems (Collins andKoo, 2005; Zeman and?Zabokrtsk?y, 2005; Sagaeand Lavie, 2006; Zhang et al, 2009).
But, re-ranking methods do not combine predictions atthe level of substructures, thus the final predic-tion of the ensemble coincides with the predictionmade by one of the experts, which can be shown tobe suboptimal in many cases.
Furthermore, thesemethods typically assume the use of probabilisticmodels, which is not a requirement in our learningscenario.
Other ensembles of probabilistic mod-els have also been considered in text and speechprocessing by forming a product of probabilis-tic models via the intersection of lattices (Mohriet al, 2008), or a straightforward combination ofthe posteriors from probabilistic grammars trainedusing EM with different starting points (Petrov,2010), or some other rather intricate techniquesin speech recognition (Fiscus, 1997).
Finally, analgorithm of (MacKay, 1997) is another exampleof an ensemble method for structured predictionthough it is not addressing directly the problem weare considering.Most of the references just mentioned do not give arigorous theoretical justification for the techniquesproposed.
We are not aware of any prior theoret-ical analysis for the ensemble structured predic-tion problem we consider.
Here, we present twofamilies of algorithms for learning ensembles ofstructured prediction rules that both perform wellin practice and enjoy strong theoretical guaran-tees.
In Section 3, we develop ensemble methodsbased on on-line algorithms.
To do so, we extendexisting on-line-to-batch conversions to our moregeneral setting.
In Section 4, we present a newboosting-style algorithm which is applicable evenwith a large set of classes as in the problem weconsider, and for which we present margin-basedlearning guarantees.
Section 5 reports the resultsof our extensive experiments.12 Learning scenarioAs in standard supervised learning problems, weassume that the learner receives a training sampleS = ((x1,y1), .
.
.
, (xm,ym)) ?
X ?
Y of mlabeled points drawn i.i.d.
according to the somedistribution D used both for training and testing.We also assume that the learner has access to a setof p predictors h1, .
.
.
, hpmapping X to Y to de-vise an accurate ensemble prediction.
Thus, forany input x ?
X , he can use the prediction ofthe p experts h1(x), .
.
.
, hp(x).
No other infor-mation is available to the learner about these p ex-perts, in particular the way they have been trainedor derived is not known to the learner.
But, wewill assume that the training sample S is distinctfrom what may have been used for training the al-gorithms that generated h1(x), .
.
.
, hp(x).To simplify our analysis, we assume that the num-ber of substructures l ?
1 is fixed.
This does notcause any loss of generality so long as the maxi-mum number of substructures is bounded, whichis the case in all the applications we consider.The quality of the predictions is measured by aloss function L : Y ?
Y ?
R+that can be de-composed as a sum of loss functions `k: Yk?R+over the substructure sets Yk, that is, for ally = (y1, .
.
.
, yl) ?
Y with yk?
Ykand y?=(y?1, .
.
.
, y?l) ?
Y with y?k?
Yk,L(y,y?)
=l?k=1`k(yk, y?k).
(1)We will assume in all that follows that the lossfunction L is bounded: L(y,y?)
?
M for all1This paper is a modified version of (Cortes et al, 2014a)to which we refer the reader for the proofs of the theoremsstated and a more detailed discussion of our algorithms.2(y,y?)
for some M > 0.
A prototypical exampleof such loss functions is the normalized Hammingloss LHam, which is the fraction of substructuresfor which two labels y and y?disagree, thus in thatcase `k(yk, y?k) =1lIyk6=y?kand M = 1.3 On-line learning approachIn this section, we present an on-line learning so-lution to the ensemble structured prediction prob-lem just discussed.
We first give a new formula-tion of the problem as that of on-line learning withexpert advice, where the experts correspond to thepaths of an acyclic automaton.
The on-line algo-rithm generates at each iteration a distribution overthe path-experts.
A critical component of our ap-proach consists of using these distributions to de-fine a prediction algorithm with favorable gener-alization guarantees.
This requires an extensionof the existing on-line-to-batch conversion tech-niques to the more general case of combining dis-tributions over path-experts, as opposed to com-bining single hypotheses.3.1 Path expertsEach expert hjinduces a set of substructure hy-potheses h1j, .
.
.
, hlj.
As already discussed, oneparticular expert may be better at predicting thekth substructure while some other expert may bemore accurate at predicting another substructure.Therefore, it is desirable to combine the substruc-ture predictions of all experts to derive a more ac-curate prediction.
This leads us to considering anacyclic finite automaton G such as that of Figure 1which admits all possible sequences of substruc-ture hypotheses, or, more generally, a finite au-tomaton such as that of Figure 2 which only allowsa subset of these sequences.An automaton such as G compactly represents aset of path experts: each path from the initialvertex 0 to the final vertex l is labeled with asequence of substructure hypotheses h1j1, .
.
.
, hljland defines a hypothesis which associates to inputx the output h1j1(x) ?
?
?hljl(x).
We will denote byH the set of all path experts.
We also denote byh each path expert defined by h1j1, .
.
.
, hljl, withjk?
{1, .
.
.
, p}, and denote by hkits kth sub-structure hypothesis hkjk.
Our ensemble structureprediction problem can then be formulated as thatof selecting the best path expert (or collection of Figure 1: Finite automaton G of path experts.path experts) in G. Note that, in general, the pathexpert selected does not coincide with any of theoriginal experts h1, .
.
.
, hp.3.2 On-line algorithmUsing an automaton G, the size of the pool of ex-perts H we consider can be very large.
For ex-ample, in the case of the automaton of Figure 1,the size of the pool of experts is pl, and thus isexponentially large with respect to p. But, sincelearning guarantees in on-line learning admit onlya logarithmic dependence on that size, they re-main informative in this context.
Nevertheless,the computational complexity of most on-line al-gorithms also directly depends on that size, whichcould make them impractical in this context.
But,there exist several on-line solutions precisely de-signed to address this issue by exploiting the struc-ture of the experts as in the case of our path ex-perts.
These include the algorithm of (Takimotoand Warmuth, 2003) denoted by WMWP, whichis an extension of the (randomized) weighted-majority (WM) algorithm of (Littlestone and War-muth, 1994) to more general bounded loss func-tions combined with the Weight Pushing (WP) al-gorithm of (Mohri, 1997); and the Follow the Per-turbed Leader (FPL) algorithm of (Kalai and Vem-pala, 2005).
The WMWP algorithm admits a morefavorable regret guarantee than the FPL algorithmin our context and our discussion will focus onthe use of WMWP for the design of our batch al-gorithm.
However, we have also fully analyzedand implemented a batch algorithm based on FPL(Cortes et al, 2014a).As in the standard WM algorithm (Littlestone andWarmuth, 1994), WMWP maintains at each roundt ?
[1, T ], a distribution ptover the set of all ex-perts, which in this context are the path expertsh ?
H. At each round t ?
[1, T ], the algo-rithm receives an input sequence xt, incurs the lossEh?pt[L(h(xt),yt)] =?hpt(h)L(h(xt),yt) andmultiplicatively updates the distribution weightper expert:?h ?
H, pt+1(h)=pt(h)?L(h(xt),yt)?h??Hpt(h?)?L(h?
(xt),yt), (2)3  Figure 2: Alternative experts automaton.where ?
?
(0, 1) is some fixed parameter.
Thenumber of paths is exponentially large in p and thecost of updating all paths is therefore prohibitive.However, since the loss function is additive in thesubstructures and the updates are multiplicative,it suffices to maintain instead a weight wt(e) pertransition e, following the updatewt+1(e)=wt(e)?`e(xt,yt)?orig(e?)=orig(e)wt(e?)?`e?
(xt,yt)(3)where `e(xt,yt) denotes the loss incurred by thesubstructure predictor labeling e for the input xtand output yt, and orig(e?)
denotes the originstate of a transition e?
(Takimoto and Warmuth,2003).
Thus, the cost of the update is then linearin the size of the automaton.
To use the result-ing weighted automaton for sampling, the weightpushing algorithm is used, whose complexity isalso linear in the size of the automaton (Mohri,1997).3.3 On-line-to-batch conversionThe WMWP algorithm does not produce a se-quence of path experts, rather, a sequence of dis-tributions p1, .
.
.
, pTover path experts.
Thus, theon-line-to-batch conversion techniques describedin (Littlestone, 1989; Cesa-Bianchi et al, 2004;Dekel and Singer, 2005) do not readily apply.
In-stead, we propose a generalization of the tech-niques of (Dekel and Singer, 2005).
The conver-sion consists of two steps: extract a good collec-tion of distributions P ?
{p1, .
.
.
, pT}; next useP to define an accurate hypothesis for prediction.For a subset P ?
{p1, .
.
.
, pT}, we define?
(P)=1|P|?pt?P?h?Hpt(h)L(h(xt),yt)+M?log1?|P|=1|P|?pt?P?ewt(e)`e(xt),yt)+M?log1?|P|,where ?
> 0 is a fixed parameter.
With this defini-tion, we choose P?as a minimizer of ?
(P) oversome collection P of subsets of {p1, .
.
.
, pT}:P??
argminP?P?(P).
The choice of P is re-stricted by computational considerations.
One nat-ural option is to let P be the union of the suf-fix sets {pt, .
.
.
, pT}, t = 1, .
.
.
, T .
We willassume in what follows that P includes the set{p1, .
.
.
, pT}.Next, we define a randomized algorithm based onP?.
Given an input x, the algorithm consists ofrandomly selecting a path h according top(h) =1|P?|?pt?P?pt(h), (4)and returning the prediction h(x).
Note that com-puting and storing p directly is not efficient.
Tosample from p, we first choose pt?
P?uniformlyat random and then sample a path h according tothat pt.
Sampling a path according to ptcan bedone efficiently using the weight pushing algo-rithm.
Note that once an input x is received, thedistribution p over the path experts h induces aprobability distribution pxover the output spaceY .
It is not hard to see that sampling a predic-tion y according to pxis statistically equivalent tofirst sampling h according to p and then predictingh(x).
We will denote by HRandthe randomizedhypothesis thereby generated.An inherent drawback of randomized solutionssuch as the one just described is that for the sameinput x the user can receive different predictionsover time.
Randomized solutions are also typi-cally more costly to store.
A collection of distri-butions P can also be used to define a determin-istic prediction rule based on the scoring functionapproach.
The majority vote scoring function isdefined by?hMVote(x,y) =l?k=1(1|P?|?pt?P?p?j=1wt,kj1hkj(x)=yk).
(5)The majority vote algorithm denoted by HMVoteis then defined for all x ?
X , by HMVote(x) =argmaxy?Y?hMVote(x,y).
For an expert automa-ton accepting all path experts such as that of Fig-ure 1, the maximizer of?hMVotecan be found veryefficiently by choosing y such that ykhas the max-imum weight in position k.In the next section, we present learning guaranteesfor HRandand HMVote.
For a more extensive dis-4cussion of alternative prediction rules, see (Corteset al, 2014a).3.4 Batch learning guaranteesWe first present learning bounds for the random-ized prediction rule HRand.
Next, we upper boundthe generalization error of HMVotein terms of thatofHRand.Theorem 1.
For any ?
> 0, with probabil-ity at least 1 ?
?
over the choice of the sample((x1,y1), .
.
.
, (xT,yT)) drawn i.i.d.
according toD, the following inequalities hold:E[L(HRand(x),y)]?
infh?HE[L(h(x),y)]+ 2M?l log pT+ 2M?log2?T.For the normalized Hamming loss LHam, thebound of Theorem 1 holds with M = 1.We now upper bound the generalization error ofthe majority-vote algorithm HMVotein terms ofthat of the randomized algorithm HRand, which,combined with Theorem 1, immediately yieldsgeneralization bounds for the majority-vote algo-rithmHMVote.Proposition 2.
The following inequality relatesthe generalization error of the majority-vote algo-rithm to that of the randomized one:E[LHam(HMVote(x),y)]?2E[LHam(HRand(x),y)],where the expectations are taken over (x,y)?Dand h?p.Proposition 2 suggests that the price to pay forderandomization is a factor of 2.
More refinedand more favorable guarantees can be provenfor the majority-vote algorithm (Cortes et al,2014a).4 Boosting-style algorithmIn this section, we devise a boosting-style al-gorithm for our ensemble structured predictionproblem.
The variants of AdaBoost for multi-class classification such as AdaBoost.MH or Ad-aBoost.MR (Freund and Schapire, 1997; Schapireand Singer, 1999; Schapire and Singer, 2000) can-not be readily applied in this context.
First, thenumber of classes to consider here is quite large,as in all structured prediction problems, since it isexponential in the number of substructures l. Forexample, in the case of the pronunciation prob-lem where the number of phonemes for Englishis in the order of 50, the number of classes is 50l.But, the objective function for AdaBoost.MH orAdaBoost.MR as well as the main steps of the al-gorithms include a sum over all possible labels,whose computational cost in this context would beprohibitive.
Second, the loss function we consideris the normalized Hamming loss over the substruc-tures predictions, which does not match the multi-class losses for the variants of AdaBoost.2Finally,the natural base hypotheses for this problem admita structure that can be exploited to devise a moreefficient solution, which of course was not part ofthe original considerations for the design of thesevariants of AdaBoost.4.1 Hypothesis setsThe predictor HBoostreturned by our boosting al-gorithm is based on a scoring function?h : X ?Y ?
R, which, as for standard ensemble algo-rithms such as AdaBoost, is a convex combinationof base scoring functions?ht:?h =?Tt=1?t?ht, with?t?
0.
The base scoring functions used in our al-gorithm have the form?
(x,y) ?
X ?
Y,?ht(x,y) =l?k=1?hkt(x,y).In particular, these can be derived from the pathexperts in H by letting hkt(x,y) = 1hkt(x)=yk.Thus, the score assigned to y by the base scoringfunction?htis the number of positions at which ymatches the prediction of path expert htgiven in-put x. HBoostis defined as follows in terms of?h orhts:?x ?
X ,HBoost(x) = argmaxy?Y?h(x,y)We remark that the analysis and algorithm pre-sented in this section are also applicable with ascoring function that is the product of the scores2(Schapire and Singer, 1999) also present an algorithmusing the Hamming loss for multi-class classification, but thatis a Hamming loss over the set of classes and differs fromthe loss function relevant to our problem.
Additionally, themain steps of that algorithm are also based on a sum over allclasses.5at each substructure k as opposed to a sum, thatis,?h(x,y) =l?k=1(T?t=1?t?hkt(x,y)).This can be used for example in the case wherethe experts are derived from probabilistic mod-els.4.2 ESPBoost algorithmTo simplify our exposition, the algorithm thatwe now present uses base learners of the formhkt(x,y) = 1hkt(x)=yk.
The general case can behandled in the same fashion with the only dif-ference being the definition of the direction andstep of the optimization procedure described be-low.
For any i ?
[1,m] and k ?
[1, l], wedefine the margin of?hkfor point (xi,yi) by?
(?hk,xi,yi) =?hk(xi, yki)?maxyk6=yki?hk(xi, yk).We first derive an upper bound on the empiricalnormalized Hamming loss of a hypothesisHBoost,with?h =?Tt=1?t?ht.Lemma 3.
The following upper bound holds forthe empirical normalized Hamming loss of the hy-pothesisHBoost:E(x,y)?S[LHam(HBoost(x),y)]?1mlm?i=1l?k=1exp(?T?t=1?t?
(?hkt,xi,yi)).The proof of this lemma as well as that of sev-eral other theorems related to this algorithm canbe found in (Cortes et al, 2014a).In view of this upper bound, we consider the ob-jective function F : RN?
R defined for all ?
=(?1, .
.
.
, ?N) ?
RNbyF (?)
=1mlm?i=1l?k=1exp(?N?j=1?j?
(?hkj,xi,yi)),where h1, .
.
.
, hNdenote the set of all path ex-perts in H. F is a convex and differentiable func-tion of ?.
Our algorithm, ESPBoost (EnsembleStructured Prediction Boosting), is defined by theapplication of coordinate descent to the objectiveF .
Algorithm 1 shows the pseudocode of the ESP-Boost.Algorithm 1 ESPBoost AlgorithmInputs: S = ((x1,y1), .
.
.
, (xm,ym)); set ofexperts {h1, .
.
.
, hp}for i = 1 to m and k = 1 to l doD1(i, k)?1mlend forfor t = 1 to T doht?
argminh?HE(i,k)?Dt[1hk(xi) 6=yki]t?
E(i,k)?Dt[1hkt(xi) 6=yki]?t?12log1?ttZt?
2?t(1?
t)for i = 1 to m and k = 1 to l doDt+1(i, k)?exp(??t?
(ehkt,xi,yi))Dt(i,k)Ztend forend forReturn?h =?Tt=1?t?htLet ?t?1?
RNdenote the vector obtained aftert ?
1 iterations and etthe tth unit vector in RN.We denote byDtthe distribution over [1,m]?
[1, l]defined byDt(i, k) =1mlexp(??t?1u=1?u?
(?hku,xi,yi))At?1where At?1is a normalization factor, At?1=1ml?mi=1?lk=1exp(??t?1u=1?u?
(?hku,xi,yi)).The direction etselected at the tth round is theone minimizing the directional derivative, thatisdF (?t?1+ ?et)d??????=0=?m?i=1l?k=1?
(?hkt,xi,yi)Dt(i, k)At?1=[2?i,k:hkt(xi) 6=ykiDt(i, k)?
1]At?1=(2t?
1)At?1,where tis the average error of htgiven byt=m?i=1l?k=1Dt(i, k)1hkt(xi) 6=yki= E(i,k)?Dt[1hkt(xi) 6=yki].The remaining steps of our algorithm can be de-termined as in the case of AdaBoost.
In particu-lar, given the direction et, the best step ?tis ob-tained by solving the equationdF (?t?1+?tet)d?t=60, which admits the closed-form solution ?t=12log1?tt.
The distribution Dt+1can be ex-pressed in terms of Dtwith the normalization fac-tor Zt= 2?t(1?
t).Our weak learning assumption in this context isthat there exists ?
> 0 such that at each round,tverifies t<12?
?.
Note that, at each round,the path expert htwith the smallest error tcan bedetermined easily and efficiently by first findingfor each substructure k, the hktthat is the best withrespect to the distribution weights Dt(i, k).Observe that, while the steps of our algorithm aresyntactically close to those of AdaBoost and itsmulti-class variants, our algorithm is distinct anddoes not require sums over the exponential numberof all possible labelings of the substructures and isquite efficient.4.3 Learning guaranteesWe have derived both a margin-based generaliza-tion bound in support of the ESPBoost algorithmand a bound on the empirical margin loss.For any ?
> 0, define the empirical margin loss ofHBoostby the following:?R?(?h???1)=1mlm?i=1l?k=11?(ehk,xi,yi)????
?1,where?h is the corresponding scoring function.The following theorem can be proven using themulti-class classification bounds of (Koltchinskiiand Panchenko, 2002; Mohri et al, 2012) as canbe shown in (Cortes et al, 2014a).Theorem 4.
Let F denote the set of func-tions HBoostwith?h =?Tt=1?t?htfor some?1, .
.
.
, ?t?
0 and ht?
H for all t ?
[1, T ].
Fix?
> 0.
Then, for any ?
> 0, with probability atleast 1?
?, the following holds for allHBoost?
F:E(x,y)?D[LHam(HBoost(x),y)] ??R?(?h??
?1)+2?ll?k=1|Yk|2Rm(Hk) +?logl?2m,where Rm(Hk) denotes the Rademacher com-plexity of the class of functionsHk= {x 7?
?hkt: j ?
[1, p], y ?
Yk}.Table 1: Average Normalized Hamming Loss,ADS1 and ADS2.
?ADS1= 0.95, ?ADS2= 0.95,TSLE= 100, ?
= 0.05.ADS1, m = 200 ADS2, m = 200HMVote0.0197 ?
0.00002 0.2172 ?
0.00983HFPL0.0228 ?
0.00947 0.2517 ?
0.05322HCV0.0197 ?
0.00002 0.2385 ?
0.00002HFPL-CV0.0741 ?
0.04087 0.4001 ?
0.00028HESPBoost0.0197 ?
0.00002 0.2267 ?
0.00834HSLE0.5641 ?
0.00044 0.2500 ?
0.05003HRand0.1112 ?
0.00540 0.4000 ?
0.00018Best hj0.5635 ?
0.00004 0.4000This theorem provides a margin-based guaranteefor convex ensembles such as those returned byESPBoost.
The following theorem further pro-vides an upper bound on the empirical margin lossfor ESPBoost.Theorem 5.
Let?h denote the scoring function re-turned by ESPBoost after T ?
1 rounds.
Then, forany ?
> 0, the following inequality holds:?R?(?h???1)?
2TT?t=1?1??t(1?
t)1+?.As in the case of AdaBoost (Schapire et al, 1997),it can be shown that for ?
< ?, 1??t(1?
t)1+??
(1 ?
2?)1??
(1 + 2?
)1+?< 1 and the right-handside of this bound decreases exponentially withT .5 ExperimentsWe used a number of artificial and real-worlddata sets for our experiments.
For each data set,we performed 10-fold cross-validation with dis-joint training sets.3We report the average errorfor each task.
In addition to the HMVote, HRandand HESPBoosthypotheses, we experimented withtwo algorithms discussed in more detail in (Corteset al, 2014a): a cross-validation on-line-to-batch conversion of the WMWP algorithm, HCV,a majority-vote on-line-to-batch conversion withFPL, HFPL, and a cross-validation on-line-to-batch conversion with FPL, HFPL-CV.
Finally, wecompare with theHSLEalgorithm of (Nguyen andGuo, 2007).5.1 Artificial data setsOur artificial data set, ADS1 and ADS2 simulatethe scenarios described in Section 1.
In ADS1 the3For the OCR data set, these subsets are predefined.7kth expert has a high accuracy on the kth position,in ADS2 an expert has low accuracy in a fixed setof positions.For the first artificial data set, ADS1, we used lo-cal experts h1, .
.
.
, hpwith p = 5.
To generatethe data we chose an arbitrary Markov chain overthe English alphabet and sampled 40,000 randomsequences each consisting of 10 symbols.
Eachof the five experts was designed to have a certainprobability of making a mistake at each position inthe sequence.
Expert hjcorrectly predicted posi-tions 2j?1 and 2j with probability 0.97 and otherpositions with probability 0.5.
We forced expertsto make similar mistakes by making them select anadjacent alphabet symbol in case of an error.
Forexample, when a mistake was made on a symbol b,the expert prediction was forced to be either a or c.The second artificial data set, ADS2, modeled thecase of rather poor experts.
ADS2 was generatedin the same way as ADS1, but the expert predic-tions were different.
This time each expert mademistakes at four out of the ten distinct random po-sitions in each sequence.Table 1 reports the results of our experiments.For all experiments with the algorithms HRand,HMVote, and HCV, we ran the WMWP algorithmfor T = m rounds with the ?
values listed inthe caption of Table 1, generating distributionsP ?
{p1, .
.
.
, pT}.
For P we used the collectionof all suffix sets {pt, .
.
.
, pT} and ?
= 0.05.
Forthe algorithms based on FPL, we used  = 0.5/pl.The same parameter choices were used for thesubsequent experiments.As can be seen from Table 1, in both cases,HMVote, our majority-vote algorithm based on ouron-line-to-batch conversion using the WMWP al-gorithm (together with most of the other on-linebased algorithms), yields a significant improve-ment over the best expert.
It also outperformsHSLE, which in the case of ADS1 even fails tooutperform the best hj.
After 100 iterations onADS1, the ensemble learned by HSLEconsists ofa single expert, which is why it leads to such apoor performance.It is also worth pointing out that HFPL-CVandHRandfail to outperform the best model on ADS2set.
This is in total agreement with our theoreti-cal analysis since, in this case, any path expert hasexactly the same performance and the error of theTable 2: Average Normalized Hamming Loss forADS3.
?ADS1= 0.95, ?ADS2= 0.95, TSLE=100, ?
= 0.05.HMVote0.1788 ?
0.00004HFPL0.2189 ?
0.04097HCV0.1788 ?
0.00004HFPL-CV0.3148 ?
0.00387HESPBoost0.1831 ?
0.00240HSLE0.1954 ?
0.00185HRand0.3196 ?
0.00018Best hj0.2957 ?
0.00005Table 3: Average Normalized Hamming Loss,PDS1 and PDS2.
?PDS1= 0.85, ?PDS2= 0.97,TSLE= 100, ?
= 0.05.PDS1, m = 130 PDS2, m = 400HMVote0.2225 ?
0.00301 0.2323 ?
0.00069HFPL0.2657 ?
0.07947 0.2337 ?
0.00229HCV0.2316 ?
0.00189 0.2364 ?
0.00080HFPL-CV0.4451 ?
0.02743 0.4090 ?
0.01388HESPBoost0.3625 ?
0.01054 0.3499 ?
0.00509HSLE0.3130 ?
0.05137 0.3308 ?
0.03182HRand0.4713 ?
0.00360 0.4607 ?
0.00131Best hj0.3449 ?
0.00368 0.3413 ?
0.00067best path expert is an asymptotic upper bound onthe errors of these algorithms.
The superior perfor-mance of the majority-vote-based algorithms sug-gests that these algorithms may have an advantageover other prediction rules beyond what is sug-gested by our learning bounds.We also synthesized a third data set, ADS3.
Here,we simulated the case where each expert special-ized in predicting some subset of the labels.
Inparticular, we generated 40,000 random sequencesover the English alphabet in the same way as forADS1 and ADS2.
To generate expert predictions,we partitioned the alphabet into 5 disjoint subsetsAj.
Expert j always correctly predicted the labelin Ajand the probability of correctly predictingthe label not in Ajwas set to 0.7.
To train the en-semble algorithms, we used a training set of sizem = 200.The results are presented in Table 2.
HMVote,HCVand HESPBoostachieve the best performance onthis data set with a considerable improvement inaccuracy over the best expert hj.
We also ob-serve as for the ADS2 experiment that HRandandHFPL-CVfail to outperform the best model and ap-proach the accuracy of the best path expert onlyasymptotically.8Table 4: Average edit distance, PDS1 and PDS2.
?PDS1= 0.85, ?PDS2= 0.97, TSLE= 100,?
= 0.05.PDS1, m = 130 PDS2, m = 400HMVote0.8395 ?
0.01076 0.9626 ?
0.00341HFPL1.0158 ?
0.34379 0.9744 ?
0.01277HCV0.8668 ?
0.00553 0.9840 ?
0.00364HFPL-CV1.8044 ?
0.09315 1.8625 ?
0.06016HESPBoost1.3977 ?
0.06017 1.4092 ?
0.04352HSLE1.1762 ?
0.12530 1.2477 ?
0.12267HRand1.8962 ?
0.01064 2.0838 ?
0.00518Best hj1.2163 ?
0.00619 1.2883 ?
0.002195.2 Pronunciation data setsWe had access to two proprietary pronunciationdata sets, PDS1 and PDS2.
In both sets, eachexample is an English word, typically a propername.
For each word, 20 possible phonemic se-quences are available, ranked by some pronuncia-tion model.
Since the true pronunciation was notavailable, we set the top sequence to be the tar-get label and used the remaining as the predictionsmade by the experts.
The only difference betweenPDS1 and PDS2 is their size: 1,313 words forPDS1 and 6,354 for PDS2.In both cases, on-line based algorithms, specif-ically HMVote, significantly outperform the bestmodel as well as HSLE, see Table 3.
The poorperformance ofHESPBoostis due to the fact that theweak learning assumption is violated after 5-8 iter-ations and hence the algorithm terminates.It can be argued that for this task the edit-distanceis a more suitable measure of performance thanthe average Hamming loss.
Thus, we also re-port the results of our experiments in terms of theedit-distance in Table 4.
Remarkably, our on-linebased algorithms achieve a comparable improve-ment over the performance of the best model inthe case of edit-distance as well.5.3 OCR data setRob Kassel?s OCR data set is available for down-load from http://ai.stanford.edu/?btaskar/ocr/.
It contains 6,877 word instances with a to-tal of 52,152 characters.
Each character is rep-resented by 16 ?
8 = 128 binary pixels.
Thetask is to predict a word given its sequence ofpixel vectors.
To generate experts, we used severalsoftware packages: CRFsuite (Okazaki, 2007) andSVMstruct, SVMmulticlass(Joachims, 2008), andTable 5: Average Normalized Hamming Loss,TR1 and TR2.
?TR1= 0.95, ?TR2= 0.98,TSLE= 100, ?
= 0.05.TR1, m = 800 TR2, m = 1000HMVote0.0850 ?
0.00096 0.0746 ?
0.00014HFPL0.0859 ?
0.00110 0.0769 ?
0.00218HCV0.0843 ?
0.00006 0.0741 ?
0.00011HFPL-CV0.1093 ?
0.00129 0.1550 ?
0.00182HESPBoost0.1041 ?
0.00056 0.1414 ?
0.00233HSLE0.0778 ?
0.00934 0.0814 ?
0.02558HRand0.1128 ?
0.00048 0.1652 ?
0.00077Best hj0.1032 ?
0.00007 0.1415 ?
0.00005the Stanford Classifier (Rafferty et al, 2014).
Wetrained these algorithms on each of the predefinedfolds of the data set and generated predictions onthe test fold using the resulting models.Our results (see (Cortes et al, 2014a)) show thatensemble methods lead only to a small improve-ment in performance over the best hj.
This is be-cause here the best model hjdominates all otherexperts and ensemble methods cannot benefit frompatching together different outputs.5.4 Penn Treebank data setThe part-of-speech task, POS, consists of label-ing each word of a sentence with its correctpart-of-speech tag.
The Penn Treebank 2 dataset is available through LDC license at http://www.cis.upenn.edu/?treebank/ and contains251,854 sentences with a total of 6,080,493 tokensand 45 different parts-of-speech.For the first experiment, TR1, we used 4 disjointtraining sets to produce 4 SVMmulticlassmod-els and 4 maximum entropy models using theStanford Classifier.
We also used the union ofthese training sets to devise one CRFsuite model.For the second experiment, TR2, we trained 5SVMstructmodels.
The same features were usedfor both experiments.
For the SVM algorithms, wegenerated 267,214 bag-of-word binary features.The Stanford Classifier and CRFsuite packagesuse internal routines to generate features.The results of the experiments are summarized inTable 5.
For TR1, our on-line ensemble meth-ods improve over the best model.
Note that HSLEhas the best average loss over 10 runs for this ex-periment.
This comes at a price of much higherstandard deviation which does not allow us to con-clude that the difference in performance betweenour methods and HSLEis statistically significant.9Table 6: Average Normalized Hamming Loss,SDS.
l ?
4, ?
= 0.97, ?
= 0.05, TSLE= 100.p = 5, m = 1500 p = 10, m = 1200HMVote0.2465 ?
0.00248 0.2606 ?
0.00320HFPL0.2500 ?
0.00248 0.2622 ?
0.00316HCV0.2504 ?
0.00576 0.2755 ?
0.00212HFPL-CV0.2726 ?
0.00839 0.3219 ?
0.01176HESPBoost0.2572 ?
0.00062 0.2864 ?
0.00103HSLE0.2572 ?
0.00061 0.2864 ?
0.00102HRand0.2877 ?
0.00480 0.3430 ?
0.00468Best hj0.2573 ?
0.00060 0.2865 ?
0.00101In fact, on two runs, HSLEchooses an ensembleconsisting of a single expert and fails to outper-form the best model.5.5 Speech recognition data setFor our last set of experiments, we used anotherproprietary speech recognition data set, SDS.
Eachexample in this data set is represented by a se-quence of length l ?
[2, 15].
Therefore, for train-ing we padded the true labels and the expert pre-dictions to normalize the sequence lengths.
Foreach of the 22,298 examples, there are between2 and 251 expert predictions available.
Since theensemble methods we presented assume that thepredictions of all p experts are available for eachexample in the training and test sets, we needed torestrict ourselves to the subsets of the data whereat least some fixed number of expert predictionswere available.
In particular, we considered p =5, 10, 20 and 50.
For each value of p we used onlythe top p experts in our ensembles.Our initial experiments showed that, as in the caseof OCR data set, ensemble methods offer only amodest increase in performance over the best hj.This is again largely due to the dominant perfor-mance of the best expert hj.
However, it was ob-served that the accuracy of the best model is a de-creasing function of l, suggesting that ensemblealgorithm may be used to improve performancefor longer sequences.
Subsequent experimentsshow that this is indeed the case: when trainingand testing with l ?
4, ensemble algorithms out-perform the best model.
Table 6 and Table 7 sum-marize these results for p = 5, 10, 20, 50.Our results suggest that the following simplescheme can be used: for short sequences use thebest expert model and for longer sequences, usethe ensemble model.
A more elaborate variant ofthis algorithm can be derived based on the obser-Table 7: Average Normalized Hamming Loss,SDS.
l ?
4, ?
= 0.97,?
= 0.05, TSLE= 100.p = 20, m = 900 p = 50, m = 700HMVote0.2773 ?
0.00139 0.3217 ?
0.00375HFPL0.2797 ?
0.00154 0.3189 ?
0.00344HCV0.2986 ?
0.00075 0.3401 ?
0.00054HFPL-CV0.3816 ?
0.01457 0.4451 ?
0.01360HESPBoost0.3115 ?
0.00089 0.3426 ?
0.00071HSLE0.3114 ?
0.00087 0.3425 ?
0.00076HRand0.3977 ?
0.00302 0.4608 ?
0.00303Best hj0.3116 ?
0.00087 0.3427 ?
0.00077vation that the improvement in accuracy of the en-semble model over the best expert increases withthe number of experts available.6 ConclusionWe presented a broad analysis of the problem ofensemble structured prediction, including a seriesof algorithms with learning guarantees and exten-sive experiments.
Our results show that our al-gorithms, most notably HMVote, can result in sig-nificant benefits in several tasks, which can be ofa critical practical importance.
We also reportedvery favorable results for HMVotewhen used withthe edit-distance, which is the standard loss usedin many applications.
A natural extension of thiswork consists of devising new algorithms and pro-viding learning guarantees specific to other lossfunctions such as the edit-distance.
While weaimed for an exhaustive study, including multi-ple on-learning algorithms, different conversionsto batch and derandomizations, we are aware thatthe problem we studied is very rich and admitsmany more facets and scenarios that we plan to in-vestigate in the future.
Finally, the boosting-stylealgorithm we presented can be enhanced using re-cent theoretical and algorithmic results on deepboosting (Cortes et al, 2014b).AcknowledgmentsWe warmly thank our colleagues Francoise Bea-ufays and Fuchun Peng for kindly extracting andmaking available to us the pronunciation data sets,Cyril Allauzen for providing us with the speechrecognition data, and Richard Sproat and BrianRoark for help with other data sets.
This work waspartly funded by the NSF award IIS-1117591 andthe NSERC PGS D3 award.10References[Breiman1996] Leo Breiman.
1996.
Bagging predic-tors.
Machine Learning, 24(2):123?140.
[Caruana et al2004] R. Caruana, A. Niculescu-Mizil,G.
Crew, and A. Ksikes.
2004.
Ensemble selectionfrom libraries of models.
In Proceedings of ICML,pages 18?.
[Cesa-Bianchi et al2004] N. Cesa-Bianchi, A. Con-coni, and C. Gentile.
2004.
On the generalization abil-ity of on-line learning algorithms.
IEEE Transactionson Information Theory, 50(9):2050?2057.
[Collins and Koo2005] Michael Collins and Terry Koo.2005.
Discriminative reranking for natural languageparsing.
Computational Linguistics, 31(1):25?70.
[Collins2002] M. Collins.
2002.
Discriminative train-ing methods for hidden Markov models: theory and ex-periments with perceptron algorithms.
In Proceedingsof ACL, pages 1?8.
[Cortes et al2005] C. Cortes, M. Mohri, and J. Weston.2005.
A general regression technique for learningtransductions.
In Proceedings of ICML 2005, pages153?160, New York, NY, USA.
ACM.
[Cortes et al2014a] Corinna Cortes, Vitaly Kuznetsov,and Mehryar Mohri.
2014a.
Ensemble methods forstructured prediction.
In Proceedings of ICML.
[Cortes et al2014b] Corinna Cortes, Mehryar Mohri,and Umar Syed.
2014b.
Deep boosting.
In Proceed-ings of the Fourteenth International Conference on Ma-chine Learning (ICML 2014).
[Dekel and Singer2005] O. Dekel and Y.
Singer.
2005.Data-driven online to batch conversion.
In Advances inNIPS 18, pages 1207?1216.
[Fiscus1997] Jonathan G Fiscus.
1997.
Post-processing system to yield reduced word error rates:Recognizer output voting error reduction (rover).
InProceedings of the 1997 IEEE ASRU Workshop, pages347?354, Santa Barbara, CA.
[Freund and Schapire1997] Y. Freund and R. Schapire.1997.
A decision-theoretic generalization of on-linelearning and application to boosting.
Journal of Com-puter and System Sciences, 55(1):119?139.
[Freund et al2004] Yoav Freund, Yishay Mansour, andRobert E. Schapire.
2004.
Generalization bounds foraveraged classifiers.
The Annals of Statistics, 32:1698?1722.
[Ghoshal et al2009] Arnab Ghoshal, Martin Jansche,Sanjeev Khudanpur, Michael Riley, and Morgan Ulin-ski.
2009.
Web-derived pronunciations.
In Proceed-ings of ICASSP, pages 4289?4292.
[Joachims2008] T. Joachims.
2008.
Support vectormachines for complex outputs.
[Kalai and Vempala2005] A. Kalai and S. Vempala.2005.
Efficient algorithms for online decision prob-lems.
Journal of Computer and System Sciences,71(3):291?307.
[Kocev et al2013] D. Kocev, C. Vens, J. Struyf, andS.
Deroski.
2013.
Tree ensembles for predicting struc-tured outputs.
Pattern Recognition, 46(3):817?833,March.
[Koltchinskii and Panchenko2002] Vladmir Koltchin-skii and Dmitry Panchenko.
2002.
Empirical margindistributions and bounding the generalization error ofcombined classifiers.
Annals of Statistics, 30.
[Lafferty et al2001] J. Lafferty, A. McCallum, andF.
Pereira.
2001.
Conditional random fields: Proba-bilistic models for segmenting and labeling sequencedata.
In Proceedings of ICML, pages 282?289.
[Littlestone and Warmuth1994] N. Littlestone andM.
Warmuth.
1994.
The weighted majority algorithm.Information and Computation, 108(2):212?261.
[Littlestone1989] N. Littlestone.
1989.
From on-lineto batch learning.
In Proceedings of COLT 2, pages269?284.
[MacKay1991] David J. C. MacKay.
1991.
Bayesianmethods for adaptive models.
Ph.D. thesis, CaliforniaInstitute of Technology.
[MacKay1997] David J.C. MacKay.
1997.
Ensemblelearning for hidden markov models.
Technical report,Cavendish Laboratory, Cambridge UK.
[Mohri et al2008] Mehryar Mohri, Fernando C. N.Pereira, and Michael Riley.
2008.
Speech recognitionwith weighted finite-state transducers.
In Handbook onSpeech Processing and Speech Communication, PartE: Speech recognition.
Springer-Verlag.
[Mohri et al2012] Mehryar Mohri, Afshin Ros-tamizadeh, and Ameet Talwalkar.
2012.
Foundationsof Machine Learning.
The MIT Press.
[Mohri1997] Mehryar Mohri.
1997.
Finite-state trans-ducers in language and speech processing.
Computa-tional Linguistics, 23(2):269?311.
[Nguyen and Guo2007] N. Nguyen and Y. Guo.
2007.Comparison of sequence labeling algorithms and ex-tensions.
In Proceedings of ICML, pages 681?688.
[Okazaki2007] N. Okazaki.
2007.
CRFsuite: a fast im-plementation of conditional random fields (crfs).
[Petrov2010] Slav Petrov.
2010.
Products of randomlatent variable grammars.
In HLT-NAACL, pages 19?27.
[Rafferty et al2014] A. Rafferty, A. Kleeman, J. Finkel,and C. Manning.
2014.
Stanford classifer.
[Sagae and Lavie2006] K. Sagae and A. Lavie.
2006.Parser combination by reparsing.
In Proceedings ofHLT/NAACL, pages 129?132.
[Schapire and Singer1999] Robert E. Schapire andYoram Singer.
1999.
Improved boosting algorithms11using confidence-rated predictions.
Machine Learning,37(3):297?336.
[Schapire and Singer2000] Robert E. Schapire andYoram Singer.
2000.
Boostexter: A boosting-basedsystem for text categorization.
Machine Learning,39(2-3):135?168.
[Schapire et al1997] Robert E. Schapire, Yoav Freund,Peter Bartlett, and Wee Sun Lee.
1997.
Boosting themargin: A new explanation for the effectiveness of vot-ing methods.
In ICML, pages 322?330.
[Smyth and Wolpert1999] Padhraic Smyth and DavidWolpert.
1999.
Linearly combining density estimatorsvia stacking.
Machine Learning, 36:59?83, July.
[Takimoto and Warmuth2003] E. Takimoto and M. K.Warmuth.
2003.
Path kernels and multiplicative up-dates.
JMLR, 4:773?818.
[Taskar et al2004] B. Taskar, C. Guestrin, andD.
Koller.
2004.
Max-margin Markov networks.
InAdvances in NIPS 16.
MIT Press, Cambridge, MA.
[Tsochantaridis et al2005] I. Tsochantaridis,T.
Joachims, T. Hofmann, and Y. Altun.
2005.Large margin methods for structured and interde-pendent output variables.
JMLR, 6:1453?1484,December.
[Wang et al2007] Q. Wang, D. Lin, and D. Schuur-mans.
2007.
Simple training of dependency parsersvia structured boosting.
In Proceedings of IJCAI 20,pages 1756?1762.
[Zeman and?Zabokrtsk?y2005] D. Zeman andZ.?Zabokrtsk?y.
2005.
Improving parsing accu-racy by combining diverse dependency parsers.
InProceedings of IWPT 9, pages 171?178.
[Zhang et al2009] H. Zhang, M. Zhang, C. Tan, andH.
Li.
2009.
K-best combination of syntactic parsers.In Proceedings of EMNLP: Volume 3, pages 1552?1560.12
