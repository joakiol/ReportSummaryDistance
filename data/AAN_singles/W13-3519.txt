Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 173?182,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsCollapsed Variational Bayesian Inference for PCFGsPengyu WangDepartment of Computer ScienceUniversity of OxfordOxford, OX1 3QD, United KingdomPengyu.Wang@cs.ox.ac.ukPhil BlunsomDepartment of Computer ScienceUniversity of OxfordOxford, OX1 3QD, United KingdomPhil.Blunsom@cs.ox.ac.ukAbstractThis paper presents a collapsed variationalBayesian inference algorithm for PCFGsthat has the advantages of two dominantBayesian training algorithms for PCFGs,namely variational Bayesian inference andMarkov chain Monte Carlo.
In three kindsof experiments, we illustrate that our al-gorithm achieves close performance to theHastings sampling algorithm while usingan order of magnitude less training time;and outperforms the standard variationalBayesian inference and the EM algorithmswith similar training time.1 IntroductionProbabilistic context-free grammars (PCFGs) arecommonly used in parsing and grammar inductionsystems (Johnson, 1998; Collins, 1999; Klein andManning, 2003; Matsuzaki et al 2005).
The tra-ditional method for estimating the parameters ofPCFGs from terminal strings is the inside-outside(IO) algorithm (Baker, 1979).
As a special in-stance of the Expectation-Maximization (EM) al-gorithm (Dempster et al 1977), based on the prin-ciple of maximum-likelihood estimation (MLE),the standard IO algorithm learns relatively uni-form probability distributions for grammars, whilethe true distributions can be highly skewed (John-son et al 2007).
In order to encourage sparsegrammars and avoid overfitting, recent researchfor training PCFGs has drifted away from MLE infavor of Bayesian inference algorithms that makeeither deterministic or stochastic approximations(Kurihara and Sato, 2006; Johnson et al 2006;Johnson et al 2007).Variational Bayesian inference (VB) (Kuriharaand Sato, 2006) for PCFGs extends EM and placesno constraints when updating parameters in the Mstep.
By minimising the divergence between thetrue posterior and an approximate one in whichthe strong dependencies between the parametersand latent variables are broken, this determinis-tic algorithm efficiently converges to an inaccu-rate and only locally optimal solution like EM.Alternatively, Johnson et al(2007) proposed twoMarkov Chain Monte Carlo algorithms for PCFGsthat can reach the true posterior after convergence.However, it is often difficult to diagnose a sam-pler?s convergence, and mixing is notoriously slowfor distributions with tightly coupled hidden vari-ables such as PCFGs, especially when the data setsare large.
Therefore, there remains a challenge formore efficient, but also accurate and deterministicinference algorithms for PCFGs.In this paper, we present a collapsed variationalBayesian inference (CVB) algorithm for PCFGs.It has the same computational complexity as thestandard variational Bayesian inference, but offersalmost the same performance as the stochastic al-gorithms due to its weak assumptions.
The idea ofoperating VB in the collapsed space was proposedby Teh et al(2007) and Sung et al(2008), and itwas successfully applied to ?bag-of-words?
mod-els such as latent Dirichlet alcation (LDA) (Tehet al 2007) and mixture of Gaussian (Sung et al2008), where the latent variables are conditionallyindependent given the parameters.
By combiningthe CVB idea and the dynamic programming tech-niques used in structurally dependent models, wedeliver a both efficient and accurate algorithm fortraining PCFGs and other structured natural lan-guage models.The rest of the paper is structured as follows.We begin with the Bayesian models of PCFGs,and relate the existing training algorithms.
Sec-tion 3 introduces collapsed variational Bayesianinference for ?bag-of-words?
models (defined inSection 3.1).
We discuss the difficulty in apply-ing such inference to structured models, followedby an approximate CVB algorithm for PCFGs.173An alternative approach is also included in brief.In Section 4, we validate our CVB algorithm inthree simple experiments.
They are inferring asparse grammar that describes the morphology ofthe Sotho language (Johnson et al 2007), unsu-pervised dependency parsing (Klein and Manning,2004) and supervised parsing with latent annota-tions (Matsuzaki et al 2005).
Section 5 concludeswith future work.2 Approximate inference for PCFGs2.1 DefinitionsA PCFG is a tuple (T,N, S,R, ?
), where T , N ,R and ?
are the finite sets of terminals, non-terminals, rules and parameters respectively, andS ?
N is the start symbol.
We adopt a similarnotation to Johnson et al(2007), and assume thatthe context free grammar G = (T,N, S,R) is inChomsky normal form and the empty string  /?
T .Hence, each rule r ?
R takes either the formA ?
BC or A ?
w, where A,B,C ?
N andw ?
T .
Let ?A??
be the probability of derivationrule A ?
?, where ?
ranges over (N ?
N) ?
T .In the Bayesian setting, we place Dirichlet priorswith hyperparameters ?A = {?A??}
on each ?A= {?A??
}.Given a corpus of sentences w = (w1, ..., wn)and the corresponding hidden parse trees t =(t1, ..., tn), the joint probability distribution of pa-rameters and variables is1:P (w, t, ?|?)
=P (?|?
)n?i=1PG(wi, ti|?
)=( ?A?NPD(?A|?A))?r?R?fr(t)r(1)PD(?A|?A) =1B(?A)?r?RA?
?r?1rB(?A) =?r?RA ?(?r)?
(?r?RA ?r)where fr(t) is the frequency of product rule r inall the parse trees t, and RA is the set of ruleswith left-hand side A.
For a Dirichlet distributionPD(?A|?A), B(?A) is the normalization constantthat can be written in terms of the gamma function?
(i.e.
the generalised factorial function).1Strictly speaking, for each (w, t) pair, if a hidden tree tis arbitrary, we need to include two delta functions, namely?
(w = yield(t)) and ?
(G ??
t).
We assume that both deltafunctions are true, otherwise the probability of such pair is 0.2.2 Variational Bayesian inferenceThe standard inside-outside algorithm for PCFGsbelongs to the general EM class, which is furthera subclass of VB (Beal, 2003).
VB maximises thenegative free energy ?F(Q(t, ?
)), a lower boundof the log marginal likelihood of the observationlogP (w|?).
This is equivalent to minimising theKullback-Leibler divergence.logP (w|?)
?
?F(Q(t, ?))=EQ(t,?
)[logP (w, t, ?|?)]?
EQ(t,?
)[logQ(t, ?
)]Q(t, ?)
is an approximate posterior, where the pa-rameters and hidden variables are assumed to beindependent.
Thus, it is factorised:Q(t, ?)
?
Q(t)Q(?)
(2)This strong independence assumption allows forthe separate updates of Q(t) and Q(?)
iteratively,optimising the negative free energy ?F(Q(z, ?
)).For the traditional IO algorithm using maximumlikelihood estimation, Q(?)
is further assumed tobe degenerate, i.e.
Q(?)
= ?(?
= ??
).E step: Q(t) ?
exp(EQ(?
)[logP (w, t, ?
)])M step: ??
= argmax?P (w, t, ?
)In the E step, we update Q(t).
For each tree t,Q(t) ?
PG(w, t|??)=?r?R(?
?r)fr(t) (3)The distribution over parse tree Q(t) is intractableto compute as its normalization requires summingover all possible parse trees producing w. We usedynamic programming to compute inside and out-side probabilities recursively with the aim of accu-mulating the expected counts.E[fA?BC(t)|w] ?
?0?i<j<k?|w|POUT(A, i, k)?
?A?BCPIN(B, i, j)PIN(C, j, k)E[fA?w(t)|w] ?
?0?i?|w|POUT(A, i)??A?wi?
(wi = w)where PIN(A, i, k) is the inside probability of ob-servation wi,k = wi, ..., wk given A is the root ofthe subtree, and POUT(A, i, k) is the probability ofA spanning (i, k), together with the rest of w.174In the M step, we find the optimal ??
based onthe MLE principle:??A??
=E[fA??(t)|w]?A???
?RA E[fA???(t)|w]E[fA??
(t)|w] =n?i=1E[fA??
(ti)|wi]VB inference is the generalisation of EM in thesense that it allows arbitrary parametric forms ofQ(?).
Thus, the update equation in the M step is:Q(?)
?
exp(EQ(t)[logP (w, t, ?|?
)])By the conjugacy property, the new Q(?)
is stillin Dirichlet distribution form except with updatedhyperparameters as shown by Kurihara and Sato(2006).
Instead, Beal (2003) suggested an equiva-lent mean parameters ??.
Based on implementationof the EM algorithm, we only need a minor modi-fication in the M step.??A??
=m(E[fA??(t)|w]+?A??)m(?A????RA(E[fA???(t)|w]+?A???
))m(x) = exp(?
(x))where ?
(x) = ??
(x)?x is the digamma function.From the joint distribution in (1) proportionalto the true posterior, we notice that the parame-ters and hidden variables are intimately coupled.Fluctuations in the parameters can induce changesin the hidden variables and vice-versa.
Hence, theindependence assumption in (2) and Figure 1(d)seems too strong, leading to inaccurate local max-imums, although it allows for efficient and deter-ministic updates in EM and VB.
The dependenciesbetween parameters and hidden variables are keptintact for the remaining algorithms in this paper.2.3 Markov Chain Monte CarloThe standard Gibbs sampler for PCFGs iterativelysamples the parameters ?
and all the parse treest.
Its mixing can be slowed by again the strongdependencies between the parameters and hiddenvariables.
Instead of reparsing all the hidden treest for each sample of ?, collapsed Gibbs sampling(CGS) improves upon Gibbs sampling in terms ofconvergence speed by integrating out the param-eters, and sampling directly from P (t|w, ?)
in acomponent-wise manner.
Thus, it also deals withthe dependencies exactly.By using the conjugacy property, we can easilycompute the marginal distribution of w and t:P (w, t|?)
=?
?PG(w, t|?)PD(?|?
)d?=?A?NB(fA(t) + ?A)B(?A)(4)where we define fA(t) to be a vector of rule fre-quencies in t indexed by A ?
?
?
RA.
Hence,the conditional distribution for a parse tree ti givenall others is:P (ti|wi,w?i, t?i, ?)
?
P (wi, ti|w?i, t?i, ?
)=?A?NB(fA(t) + ?A)B(fA(t?i) + ?A)(5)where w?i and t?i denote all other sentences andtrees.
It is noticeable that sampling a parse treefrom the above conditional distribution is difficult.The frequencies fA(t) effectively mean that theproduction probabilities are dependent on the cur-rent parse tree ti.
That is rule parameters can beupdated on the fly inside a parse tree, which pro-hibits efficient dynamic programming tricks.In order to solve this problem, Johnson et al(2007) proposed a Hastings sampler that specifiedan alternative rule probabilities ?H of a proposaldistribution P (ti|wi, ?H), where?HA??
=fA??
(t?i) + ?A???A????RA(fA???
(t?i) + ?A???
)The rule probabilities ?H are based on the statisticscollected from all other parse trees, and they arefixed for the conditional distribution of the currentparse tree.
Therefore, by using a variant of insidealgorithm (Goodman, 1998), one can efficientlysample a parse tree, which will be either acceptedor rejected based on the Metropolis choice.The MCMC based algorithms do not make anyassumptions at all, and they can converge to thetrue posterior, either in joint or collapsed space asshown in Figure 1(b), 1(c).
However, one needs tohave experience about the number of samples tobe collected and the burn-in period.
For compu-tationally intensive tasks such as learning PCFGsfrom a large corpus, a sufficiently large numberof samples are required to decrease the samplingvariance.
Therefore, MCMC algorithms improvesthe performance over EM and VB at the cost ofmuch more training time.175Figure 1: Graphical representations of the PCFG with n = 3 trees (a), and the (approximate) posteriorsfor Gibbs sampling (b), collapsed Gibbs sampling (c), variational Bayesian inference (d), and collapsedvariational Bayesian inference (e).
We use dashed lines to depict the weak dependencies.3 Collapsed variational Bayesianinference3.1 For bag-of-words modelsLeveraging the insight that a sampling algorithmin collapsed space mixes faster than the standardone, Teh et al(2007) proposed a similar argumentthat a VB inference algorithm in collapsed spaceis more effective than the standard one.
Followingthe success in LDA (Teh et al 2007), a number ofresearch results have been accumulated around ap-plying CVB to a variety of ?bag-of-words?
mod-els (Sung et al 2008; Sato et al 2012; Wang andBlei, 2012).Formally, we define a model to be independentand identically distributed (i.i.d.)
(or informally?bag-of-words?)
if its hidden variables are condi-tionally independent given the parameters.
LDA,IBM word alignment model 1 and 2, and variousfinite mixture models are typical examples.For an i.i.d.
model, integrating out parametersinduces dependencies that spread over many hid-den variables, and thus the dependency betweenany two variables is very weak.
This provides anideal setting to apply the mean field method (i.e.fully factorized VB), as its underlying assumptionis that any variable depends on only the summarystatistics collected from other variables called thefield, and any particular variable?s impact on thefield is very small.
Hence, the mean field assump-tion is better satisfied in collapsed space with veryweak dependencies than in joint space with strongdependencies.
As a result, we expect that VB incollapsed space can achieve more accurate resultsthan the standard VB, and the results would bevery close to the true posterior.Even in collapsed space, CVB remains a deter-ministic algorithm that updates the posterior dis-tributions over the hidden variables just like VBand EM.
Therefore, we expect CVB to be compu-tationally efficient as well.3.2 For structured NLP modelsWe notice that the basic condition for applying theCVB algorithm to a specific model is for the modelto be i.i.d., such that the hidden variables are onlyweakly dependent in collapsed space, providing anideal condition to operate VB.
However, the i.i.d.condition is certainly not true for structured NLPmodels such as hidden Markov models (HMMs)and PCFGs.
Given the shape of a parse tree, ahidden variable is strongly dependent on its par-ent, siblings and children, and weakly dependenton the rest.
Even worse, to infer a grammar fromterminal strings, we don?t even have access to theshape of parse trees, let ale analyzing the depen-dencies of hidden variables inside trees.Although the PCFG model is not i.i.d.
at thevariable level, we can lift the idea of CVB up tothe tree level.
As our research domain is thoselarge scale applications in language processing, acommon feature of those problems is that thereare usually many sentences, each of which has ahidden parse tree behind it.
Hence, we may con-sider each sentence together with its parse tree tobe drawn i.i.d.
from the same set of parameters.Therefore, at the tree level, a PCFG can be con-sidered as an i.i.d.
model as shown in Figure 1(a)and thus, it can be fitted in the CVB frameworkas described in Section 3.1.
We summarise the as-176Q(ti) ??A?N?A??
?RA exp(EQ(t?i)[log(?fA??
(ti)?1j=0 (fA??
(t?i) + ?A??
+ j))])?(PA???
fA???
(ti))?1j=0 exp(EQ(t?i)[log(?A????RA(fA??
(t?i) + ?A???
+ j))])Figure 2: The exact mean field update in collapsed space for the parse tree ti.Q(ti) ??r=A??
?R( EQ(t?i)[fA??
(t?i)] + ?A???A???(EQ(t?i)[fA???
(t?i)] + ?A???
))fr(ti)Figure 3: The approximate mean field update in collapsed space for the parse tree ti.sumptions made by each algorithm in Figure 1(b-e) before presenting the CVB algorithm formally.The CVB algorithm for the PCFG model keepsthe dependencies between the parameters and thehidden parse trees in an exact fashion:Q(t, ?)
= Q(t)Q(?|t)We factorise Q(t) by breaking only the weak de-pendencies between parse trees, while keeping theinside dependencies intact, as we don?t make fur-ther assumptions about Q(t) for each t.Q(t) ?n?i=1Q(ti)By the above factorisations, we compute the neg-ative variational free energy ?F(Q(t)Q(?|t)) asfollows:?F(Q(t)Q(?|t))=EQ(t)Q(?|t)[logP (w, t, ?|?)?
logQ(t)Q(?|t)]=EQ(t)[EQ(?|t)[log P (w, t, ?|?
)Q(?|t) ]?
logQ(t)]Maximizing ?F(Q(t)Q(?|t)) requires to updateQ(?|t) and Q(t) in turn.
In particular, Q(?|t) isset equal to the true posterior P (?|w, t, ?
):?F(Q(t)P (?|w, t))=EQ(t)[EP (?|w,t,?
)[log P (w, t, ?|?
)P (?|w, t, ?)
]?
logQ(t)]=EQ(t)[logP (w, t|?)?
logQ(t)]Finally, we update the approximate posterior foreach parse tree t by using the mean field methodin the collapsed space:Q(ti) ?
exp(EQ(t?i)[logP (wi, ti|w?i, t?i, ?
)])(6)The inner term P (wi, ti|w?i, t?i, ?)
in the aboveequation is just the unnormalized collapsed Gibbssampling in (5).
Plugging in (5), and expandingterms such asB(?A) and ?
(x), we obtain an exactcomputation of Q(ti) in Figure 2.The exact computation is both intractable andexpensive.
The intractability comes from the sim-ilar problem as in the collapsed Gibbs samplingthat we are unable to calculate the normalisationterm ?ti Q(ti).
Hence, we follow Johnson et al2007) to approximate it by using only the statis-tics from other sentences, namely ?H and ignoringthe local contribution.P (wi, ti|w?i, t?i, ?)
??A???R(?HA??)fA??
(ti)(7)We discuss the accuracy of (7) in Section 3.3.
Forthose expensive computations of the expected logcounts in Figure 2, Teh et al(2007) and Sung etal.
(2008) suggested the use of a linear Gaussianapproximation based on the law of large numbers.EQ(t?i)[log(fA??
(t?i) + ?A??)]?
log(EQ(t?i)[fA??
(t?i)] + ?A??)
(8)Substituting (7) into (6), and employing the linearapproximation, we derive an approximate CVB al-gorithm as shown in Figure 3.
In addition, its formis much more simplified and interpretable com-pared with the exact computation in Figure 2.The surprising similarity between the approxi-mate CVB update in Figure 3 and E step update in(3) indicates that the dynamic programming usedin both EM and VB can take over from now.
Torun inside-outside recursion, the EM algorithmemploys the parameters ??
based on maximumlikelihood estimation; the VB algorithm employs177the mean parameters ??
; and our CVB algorithmemploys the parameters ?CVB computed from theexpected counts of all other sentences.The implementation can be easily achieved bymodifying code of the EM algorithm.
We keeptrack of the expected counts at global level, sub-tract the local mean counts for ti before update,run the inside-outside recursion using ?CVB, andfinally add the updated distribution back into theglobal counts.
Therefore, we only need to replacethe parameters with the expected counts, and makeupdate after each sentence; the core of the inside-outside implementation remains the same.Our CVB algorithm bears some similarities tothe online EM algorithm with maximum a pos-terior (MAP) updates (Neal and Hinton, 1998;Liang and Klein, 2009), but they differ in severalways.
The online EM algorithm updates each treeti based on the statistics of all the trees, optimisingthe same objective function p(w|?)
as the batchEM algorithm.
MAP estimation searches for theoptimal posterior p(w|?)p(?).
On the other hand,our CVB algorithm optimises the data likelihoodp(w).
The smoothing effects for the MAP estima-tion (?A??
?
1) prevent the use of sparse priors,whereas the CVB algorithm (?A??)
overcomessuch difficulty by parameter integration.3.3 DiscussionBreaking the weak dependencies between hiddenvariables and employing the linear approximationhave been argued to be accurate (Teh et al 2007;Sung et al 2008; Sato and Nakagawa, 2012), andthey are the standard procedures in applying theCVB algorithms to i.i.d.
models.In our CVB algorithm for PCFGs, we introducean extra approximation in (7), which we argue isaccurate.
Theoretically, the inaccuracy only oc-curs when there are repeated rules in a parse tree asshown in Figure 2, so the same rule seen later usesa slightly different probability.
Even if the inac-curacy indeed occurs, in our described scenario ofmany sentences, the local contribution from a sin-gle sentence is small compared with the statisticsfrom all other sentences.
Empirically, we replicatethe experiment of Setho language by Johnson et al(2007) in Section 4.1, and we find that the sampledtrees based on ?H never get rejected, illustrating anacceptance rate close to 100%, and meaning that?H is a very accurate Metropolis proposal.
Sinceall the assumptions made by the CVB algorithmFigure 4: A fragment of a tree structureare reasonable and weak, we expect its results tobe close to true posteriors.3.4 An alternative approachWe briefly sketch an alternative CVB algorithm atthe variable level for completeness.For a structured NLP model with its shape tobe fixed such as the PCFG with latent annotations(PCFG-LA) (Matsuzaki et al 2005) (See defini-tion in Section 4.3), we can simply ignore all thedependencies between the hidden variables in thecollapsed space, despite whether they are strong(for adjacent nodes) or weak (for others).
Al-though it seems that we have made unreasonableassumptions, it is not transparent which is worsecomparing with the assumptions in the standardVB.
Following this assumption, we can derive aCVB algorithm similar to the corresponding localsampling algorithm that samples one hidden vari-able at a time.
For example, the approximate pos-terior over the subtype of the node A in the abovetree fragment in Figure 4 is updated follows:q(A = a)?
E[fB?aC(t?A)] + ?E[fB(t?A)] + |RB|?
?E[fa?DE(t?A)] + ?E[fa(t?A)] + |Ra|?where we use A to denote the node position, anda to denote its hidden subtype.
q(A = a) meansthe probability of node A being in subtype a. Inaddition, we need to take into account the distribu-tions over its adjacent variables.
In our case, A isstrongly dependent on nodesB,C,D,E, and onlyweakly dependent on other variables (not shown inthe above tree fragment) via global counts, e.g.
:E[fB?aC(t?A)]=?b?cq(B = b)q(C = c)E[fb?ac(t?A)]However, it is not obvious how to use this alter-native approach in general, and the performancesof resulting algorithms remain unclear.
Therefore,we implement only the CVB algorithm at the treelevel in Section 3.2 for our experiments.1784 ExperimentsWe conduct three simple experiments to validateour CVB algorithm for PCFGs.
In Section 4.1, weillustrate the significantly reduced training timeof our CVB algorithm compared to the relatedHastings algorithm; whereas in later two sections,we demonstrate the increased performance of ourCVB algorithm compared to the correspondingVB and EM algorithms.4.1 Inferring sparse grammarsFirstly, we conduct the same experiment of in-ferring sparse grammars describing the morphol-ogy of the Sotho language as in Johnson et al(2007).
We use the same corpus of unsegmentedSotho verb types from CHILDES (MacWhinneyand Snow, 1985), and define the same initial CFGproductions by allowing each non-terminal to emitany substrings in the corpus as terminals plus fivepredefined morphological rules at the top level.We randomly withhold 10% of the verb typesfrom the corpus for testing, and use the rest 90%for training.
Both algorithms are evaluated bytheir per word perplexity on the test data set withprior set to 10?5 as suggested by Johnson et al(2007).
We run 5 times with random starts, andreport the averaged results in Figure 5.
The Hast-ings algorithm2 takes roughly 1,000 iterations toconverge, while our CVB algorithm reaches theconvergence even before 10 iterations, consumingonly a fraction of training time (CVB: 1.5 minutes;Hastings: 20 minutes).
As well as little differencemargin in final perplexities shown in Figure 5, wealso evaluated segmentation quality measured bythe F1 scores, and again the difference is trivial(CVB: 29.8%, Hastings: 31.3%).4.2 Dependency model with valenceAs a second empirical validation of our CVB in-ference algorithm, we apply it to unsupervisedgrammar induction with the popular DependencyModel with Valence (DMV) (Klein and Manning,2004).
Although the original maximum likelihoodformulation of this model has long since been sur-passed by more advanced models, all of the state-of-the-art approaches to unsupervised dependencyparsing still have DMV at their core (Headden IIIet al 2009; Blunsom and Cohn, 2010; Spitkovskyet al 2012).
As such we believe demonstrating2Annealing is not used in order to facilitate the perplexitycalculation in the test set.0 5 10 15 206789101112Number of Iterations (CVB)Test PerplexityCVBHastings0 500 1000 1500 2000Number of Iterations (Hastings)Figure 5: Perplexities averaged over 5 runs on theextracted corpus of Sotho verbs.improved inference on this core model will enablefuture improvements to more complex models.We evaluate a Dirichlet-Multinomial formula-tion of DMV in the standard fashion by train-ing on sections 2-21 and testing on section 23 ofthe Penn.
Wall Street Journal treebank (Marcuset al 1993).
We initialise our models using theoriginal harmonic initialiser (Klein and Manning,2004).
Figure 6 displays the directed accuracy re-sults for DMV model trained with CVB and VBwith Dirichlet ?
parameters of either 1 or 0.1, aswell as the previously reported MLE result.
Inboth cases we see superior results for CVB infer-ence, providing evidence that CVB may be a bet-ter choice of inference algorithm for Bayesian for-mulations of generative grammar induction mod-els such as DMV.4.3 PCFG with latent annotationsThe vanilla PCFGs estimated by simply taking theempirical rule frequencies off treebanks are not ac-curate models to capture the syntactic structures inmost natural languages as demonstrated by Char-niak (1997) and Klein and Manning (2003).
Ourthird experiment is to apply the CVB algorithmto the PCFGs with latent annotations (PCFGs-LA) (Matsuzaki et al 2005), where each non-terminal symbol is augmented with hidden vari-ables (or subtypes).
Given a parsed corpus, train-ing a PCFG-LA yields a finer grammar with theautomatically induced features represented by thesubtypes.
For example, an augmented binary ruletakes the form A[a] ?
B[b]C[c], where a, b, c ?
[1, H] are the hidden subtypes, and H denotes thenumber of subtypes for each non-terminal.1791.0 0.10.450.460.470.480.49Bayesian PriorsF1 ScoresEMVBCVBFigure 6: DMV trained by EM, VB and CVB.
F1scores on section 23, WSJ.Objective Precision Recall F1 ExactEM 75.84 72.92 74.35 11.13VB 76.98 73.32 75.11 11.49CVB 78.85 76.98 77.90 12.56Table 1: PCFG-LA (2 subtypes) trained by EM,VB and CVB.
Precision, Recall, F1 scores, Exactmatch scores on section 23, WSJ.We follow the same experiment set-up as DMV,and report the results on the section 23, using thebest grammar tested on the development set (sec-tion 22) from 5 random runs for each algorithm.We adopt Petrov et al(2006)?s methods to processthe data: right binarising and replacing infrequentwords with the generic unknown word marker forEnglish, and to initialise: adding 1% randomnessto the parameters ?0 to start the EM training.
Wecalculate the expected counts from (G, ?0) to ini-tialise our VB and CVB algorithms.In Table 1, when each non-terminal is split into2 hidden subtypes, we show that our CVB algo-rithm outperforms the EM and VB algorithms interms of all the evaluation objectives.
We alsoinvestigate the hidden state space with higher di-mensions (4,8,16 subtypes), and find our CVB al-gorithm retains the advantages over the other two,whereas the VB algorithm fails to surpass the EMalgorithm as reported in Figure 7.5 Conclusion and future workIn this paper we have presented a collapsed vari-ational Bayesian inference algorithm for PCFGs.We make use of the common scenario where thedata consists of multiple short sentences, such that1 2 4 8 160.650.70.750.80.850.9Number of Hidden StatesF1 ScoresEMVBCVBFigure 7: PCFG-LA (2,4,8,16 subtypes) trained byEM, VB and CVB.
F1 scores on section 23, WSJ.we can ignore the local dependencies induced bycollapsing the parameters.
The assumptions in ourCVB algorithm are reasonable for a range of pars-ing applications and justified in three tasks by theempirical observations: it produces more accurateresults than standard VB, and close results to sam-pling with significantly less training time.While not state-of-the-art, the models we havedemonstrated our CVB algorithm on underlie anumber of high performance grammar inductionand parsing systems (Cohen and Smith, 2009;Blunsom and Cohn, 2010; Petrov and Klein, 2007;Liang et al 2007).
Therefore, our work naturallyextends to employing our CVB algorithm in moreadvanced models such as hierarchical splitting andmerging system used in Berkeley parser (Petrovand Klein, 2007), and generalising our CVB al-gorithm to the non-parametric models such as treesubstitution grammars (Blunsom and Cohn, 2010)and infinite PCFGs (Liang et al 2007).We have also sketched an alternative CVB al-gorithm which makes a harsher independence as-sumption for the latent variables but then requiresno approximation of the variational posterior byperforming inference individually for each parsenode.
This model breaks some strong dependen-cies within parse trees, but if we expect the pos-terior to be highly skewed by using a sparse prior,the product of constituent marginals may well be agood approximation.
We leave further explorationof this algorithm for future work.AcknowledgmentsWe would like to thank Mark Johnson for the dataused in Section 4.1 and valuable advice.180ReferencesJames K. Baker.
1979.
Trainable grammars for speechrecognition.
The Journal of the Acoustical Societyof America, 65(S1):S132.Matthew Beal.
2003.
Variational Algorithms for Ap-proximate Bayesian Inference.
Ph.D. thesis, TheGatsby Computational Neuroscience Unit, Univer-sity College London.Phil Blunsom and Trevor Cohn.
2010.
Unsupervisedinduction of tree substitution grammars for depen-dency parsing.
In Proceedings of the 2010 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1204?1213, Cambridge, MA, Oc-tober.
Association for Computational Linguistics.Eugene Charniak.
1997.
Statistical parsing witha context-free grammar and word statistics.
InProceedings of the fourteenth national conferenceon artificial intelligence and ninth conference onInnovative applications of artificial intelligence,AAAI?97/IAAI?97, pages 598?603.
AAAI Press.Shay B. Cohen and Noah A. Smith.
2009.
Sharedlogistic normal distributions for soft parameter ty-ing in unsupervised grammar induction.
In NAACL?09: Proceedings of Human Language Technolo-gies: The 2009 Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics, pages 74?82, Morristown, NJ,USA.
Association for Computational Linguistics.Michael Collins.
1999.
Head-Driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via theEM algorithm.
Journal of the Royal Statistics Soci-ety, Series B, 39(1):1?38.Joshua T. Goodman.
1998.
Parsing inside-out.Ph.D.
thesis, Cambridge, MA, USA.
Adviser-StuartShieber.William P. Headden III, Mark Johnson, and David Mc-Closky.
2009.
Improving unsupervised depen-dency parsing with richer contexts and smoothing.In Proceedings of Human Language Technologies:The 2009 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 101?109, Boulder, Colorado, June.Mark Johnson, Thomas L. Griffiths, and Sharon Gold-water.
2006.
Adaptor grammars: A framework forspecifying compositional nonparametric bayesianmodels.
In NIPS.Mark Johnson, Thomas Griffiths, and Sharon Gold-water.
2007.
Bayesian inference for PCFGs viaMarkov chain Monte Carlo.
In Proc.
of the 7th Inter-national Conference on Human Language Technol-ogy Research and 8th Annual Meeting of the NAACL(HLT-NAACL 2007), pages 139?146, Rochester,New York, April.Mark Johnson.
1998.
PCFG models of linguis-tic tree representations.
Computational Linguistics,24:613?632.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics - Volume 1, ACL ?03, pages 423?430, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Dan Klein and Christopher D. Manning.
2004.Corpus-based induction of syntactic structure: mod-els of dependency and constituency.
In ACL ?04:Proceedings of the 42nd Annual Meeting on Associ-ation for Computational Linguistics, page 478.Kenichi Kurihara and Taisuke Sato.
2006.
Variationalbayesian grammar induction for natural language.In Proceedings of the 8th international conferenceon Grammatical Inference: algorithms and appli-cations, ICGI?06, pages 84?96, Berlin, Heidelberg.Springer-Verlag.Percy Liang and Dan Klein.
2009.
Online EM for un-supervised models.
In Proceedings HLT/NAACL.Percy Liang, Slav Petrov, Michael Jordan, and DanKlein.
2007.
The infinite PCFG using hierarchi-cal Dirichlet processes.
In Proc.
of the 2007 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP-2007), pages 688?697, Prague,Czech Republic.Brian MacWhinney and Catherine Snow.
1985.
Thechild language data exchange system.
Child Lan-guage.Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of English: the Penn treebank.
Compu-tational Linguistics, 19(2):313?330.Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsu-jii.
2005.
Probabilistic cfg with latent annotations.In Proceedings of the 43rd Annual Meeting on As-sociation for Computational Linguistics, ACL ?05,pages 75?82, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Radford Neal and Geoffrey E. Hinton.
1998.
A view ofthe em algorithm that justifies incremental, sparse,and other variants.
In Learning in Graphical Mod-els, pages 355?368.
Kluwer Academic Publishers.Slav Petrov and Dan Klein.
2007.
Improved infer-ence for unlexicalized parsing.
In Human LanguageTechnologies 2007: The Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics; Proceedings of the Main Confer-ence.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, andinterpretable tree annotation.
In Proceedings ofthe 21st International Conference on Computational181Linguistics and the 44th annual meeting of the As-sociation for Computational Linguistics, ACL-44,pages 433?440, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Issei Sato and Hiroshi Nakagawa.
2012.
Rethinkingcollapsed variational bayes inference for LDA.
InProceedings of the 29th International Conference onMachine Learning.Issei Sato, Kenichi Kurihara, and Hiroshi Nakagawa.2012.
Practical collapsed variational bayes infer-ence for hierarchical dirichlet process.
In Proceed-ings of the 18th ACM SIGKDD international con-ference on Knowledge discovery and data mining,KDD ?12, pages 105?113, New York, NY, USA.ACM.Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Ju-rafsky.
2012.
Three dependency-and-boundarymodels for grammar induction.
In Proceedings ofthe 2012 Conference on Empirical Methods in Nat-ural Language Processing and Computational Nat-ural Language Learning (EMNLP-CoNLL 2012).Jaemo Sung, Zoubin Ghahramani, and Sung-YangBang.
2008.
Latent-space variational Bayes.
IEEETrans.
Pattern Anal.
Mach.
Intell., 30(12), Decem-ber.Yee Whye Teh, David Newman, and Max Welling.2007.
A collapsed variational Bayesian inferencealgorithm for latent Dirichlet alcation.
In In Ad-vances in Neural Information Processing Systems,volume 19.Chong Wang and David Blei.
2012.
Truncation-freestochastic variational inference for bayesian non-parametric models.
In Neural Information Process-ing Systems.182
