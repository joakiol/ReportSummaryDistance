REVERSIBLE MACHINE TRANSLATION: WHAT TO DO WHEN THELANGUAGES DON'T LINE UPJames Barnett, Inderjeet Mani, Paul Martin, and Elaine RichMCC3500 West Balcones Center Dr.Austin, TX 78759'Abst rac tIn this paper ,we deal with issues that face aninterlingua-based, reversible machine translationsystem when the~ literal meaning of the source textis not identical to the literal meaning of the nat-ural target translation.
We present an algorithmfor lexical choice that handles uch cases and thatrelies exclusively, on reversible, monolingual lin-guistic descriptions and a language-independentdomain knowledge base.1 In t roduct ionMachine translation is an obvious application forreversible natural language systems, since bothunderstanding and generation are important partsof the process.
There are several arguments forthis view (for e:kample, \[Isabelle, 89\]), includingreducing the total cost of adding a new languageand making it easier to maintain and validate theresulting system'.Reversible MT systems, just like the broaderclass of MT systems as a whole, fall into tworoughly defined families: transfer systems and in-terlingua (or pivot systems).
Reversible trans-fer systems (e.g., \[van Noord, 90\], \[Zajac, 90\],\[Dymetman, 8811 and \[Strzalkowski, 90\]) exploitthree reversible subsystems: one to analyze thesource text, one!
to perform the transfer, and athird to generate the target text.
Interlingua-based systems (e.g., Ultra \[Farwell, 90\]), on theother hand, require only two reversible compo-nents: one to analyze the source text into the in-terlingua representation, and one to generate thetarget text f rom that representation.
In this pa-per, we will focus on issues that arise in the designof interlingua-based MT systems.The simplest model of a reversible, interlingua-based system contains two components: one ana-lyzes the source text to create the interlingua rep-resentation and the other maps from that to thetarget text.
Unfortunately, the real situation is61not that simple, for several reasons, including twothat we will focus on here:?
This model assumes that the same infor-mation is present in the target text asin the source.
But in some eases, whichhave been called translation mismatches\[Kameyama, 91\], information is either addedto or deleted from the source in creating thetarget.
We will show some examples of thisbelow in Section 2.
In these eases, the sim-ple reversible system we outlined above wouldproduce unacceptable translations.?
Although the notion of a reversible systemthat describes the set of legal translationsis reasonably clearcut, the notion of pre-ferred translation i s  more difficult to de-fine \[van Noord, 90\], \[Barnett, 91d\].
In somecases, which have been called translation di-vergences \[Dorr, 90\], the most natural trans-lation differs from the source in some signifi-cant way (e.g., its focus).Of course, in many cases, both of these issuesoccur together and interact.
In this paper, wepresent some techniques for dealing with theseproblems.
These techniques have three impor-tant properties: They require purely declarative,reversible descriptions of the languages that areinvolved.
They require only monolingual facts.Thus new languages can be added to the systemwithout any changes to the descriptions of anyother languages.
And they are stated in a waythat enables their performance to increase gradu-ally along with the power of the underlying knowl-edge base.2 Trans la t ion  D ivergencesand  MismatchesIn this section, we examine some examples inwhich the source and target languages do not lineup.
Then, in the rest of the paper, we will outlineour solution to these problems.1.
English: "The clogs were running down thestreet.
"Japanese :  "inu ga toori-o hashitte-ita."(lit.
"dog run (along) the street.
")In English, noun phrases must be marked fornumber.
In the natural Japanese translation,number information is absent.2.
Engl ish:  "I saw a fish in the water.
"Spanish:  "Vi un pez en el agua.
"English: "I ate a fish.
"Spanish:  "Comi un pescado.
"Spanish makes a distinction between a fish inits natural state ("pez") and a fish that hasbeen caught for food ("pescado").
"Pez" isalso the default form in case it is not clearor does not matter what state the fish is in.But it cannot be used if it is clear that thefish has been caught.
To get the transla-tion right, it is necessary to infer extra in-formation about the fish, using other knowl-edge that is available either from the rest ofthe sentence or from the larger discourse con-text.
Similarly, to reverse the process and gofrom Spanish to English, it is necessary, inthe case of "pescado", to throw away infor-mation lest we produce the unnatural trans-lation, "I ate a caught fish."
It is importantto note, though, that this information cannotbe thrown away during understanding, sinceit would be important if we were translatinginto another language that made the samedistinction.
It must be preserved until thepoint at which generation into the target lan-guage takes place.3.
English: "I know him.
"Spanish:  "Lo Conozco.
"Engl ish:  "I know the answer.
"Spanish:  "Se la respuesta.
"Here the issue is the correct translation be-tween English "know" and the two Spanishverbs "conocer" (to be acquainted with some-one) and "saber" (to know a fact).
This ex-ample is similar to the previous one exceptthat here there is no default form.
Spanishdoes not have a word that includes these twodifferent events.. English: "I have a headache.
"Japanese :  "Atama ga itai."
(literally, "myhead hurts")Here the problem is more difficult.
No longeris it an issue of a single lexical item for whichthere is not an exact match in the targetlanguage.
Instead, the texts in the two lan-guages differ at the level of an entire phrase,with each language choosing a phrase that de-scribes the situation from a different point ofview.
In English, we seem to describe an ob-ject, "a headache", while Japanese describesthe state of a head hurting.The examples that we have just discussed illus-trate three different categories of semantic differ-ences between languages:Mismatches caused by semantically signifi-cant differences in morphology and syntax,e.g., Example 1.
Other common examples in-volve the presence or absence of markings forgender, number, tense, aspect, and level ofpoliteness.Mismatches caused by lexical differences,where one language has a word that the otherlacks, e.g., Examples 2 and 3.Divergences, in which the two languages de-scribe the same state of the world in differ-ent ways, as in Example 4.
In some of thesecases, identical information is conveyed (inthe sense that the semantic interpretation ofthe source implies that of the target and viceversa), but in some cases (and depending onthe particular model of the world that is be-ing used to define implication) the semanticcontent of the two forms will not be identi-cal, so many cases of divergence also containmismatches.62Mismatches and divergences are typicallyviewed as translation (transfer) problems.
Butin an interlingua-based system it becomes clearthat they are primarily problems for generation.The source language analyzer produces an inter-lingua representation, which the target generatormust render into the target language.
In cases ofmismatch or divergence, doing this requires ma-nipulating the interlingua expression itself since itdoes not already correspond exactly to the struc-ture of the target string that should be produced.But actually, the fact that the expressions in theinterlingua representation came from linguistic ex-pressions in a source language as opposed to fromsome other source (for example, the output of aproblem-solving system) is irrelevant except for afew special caseh in which the form of the sourcelanguage xpreshions can provide help in makinggeneration decis~ions.
So, in the rest of this paper,we will present r a generation-centered treatmentof mismatches that relies entirely on reversible,monolingual descriptions of the two languages.3 The KBNL MT SystemFigure 1 shows a schematic description of the MTsystem that we :are building.
All of the represen-tatations in the ifigure, except he source and tar-get language str'ings, are described in terms thatare drawn from'~a knowledge base (KB) that de-scribes the domain(s) of discourse.
In additionto providing a common set of terms that enablemeanings to be:defined, this backend knowledgebase is important because it provides the abilityto reason about imeanings and thus the ability toadd to the target text information that was omit-ted from the source.
We will assume that all theKB-based representations can be treated as setsof logical assertions (although they can of coursebe implemented in a variety of ways, including theframe-based system \[Crawford, 90\] that we are us-ing).SOURCE LANGUAGESTRING understand~SOURCEKBLFmapping t~o~ interlingt~ ~TARGET LANGUAGESTRINGtactical /generatio/9/TARGETKBLF /strategicgeneration ~/INTERLINGUA EXPRESSIONIIKNOWLE~DGE BASEEXPRESSIONFigure 1: An Iu~erlingua-Based Architecture forMTTo translate a sentence, this system must do thefollowing things:* Map the source sentence into an internal rep-resentation Of what was said.
We call this63the source DRS; it is isomorphic to the Dis-course Representation Structures describedin \[Kamp, 84\] and \[Heim, 82\], except hat itsterms are taken from the backend knowledgebase rather than from the words of the sourcelanguage.Map the source DRS into the interlingua,which is equivalent to the source DRS, bothin form and in content.
Thus it contains as-sertions corresponding to exactly what wassaid in the source.Map the interlingua expression to a targetDRS.
At this point, decisions about what tosay in the target text must be made.
Someassertions in the interlingua may be dropped.Some new assertions may be added.
Somegroups of assertions may be replaced by oth-ers that are equivalent with respect o the KBbut more appropriate as a basis for a naturalsounding text in the target language.Map the target DRS into a target string.
Un-fortunately, it is often not possible to enforcea clean separation between these last two gen-eration steps, so it may be necessary for themto interact and to inform each other, as shownin by the loop in the figure.We have implemented an MT system for En-glish and Spanish in this framework.
It is based onthe KBNL system \[Barnett, 91a\], which has twokey components: Lucy a language understand-ing system, and Koko, a language generation sys-tem.
Both Lucy and Koko use a common agenda-based blackboard for communication and control.And they both exploit a generic KB interface\[Barnett, 91b\], so they can run on any KB thatcontains the necessary domain knowledge.
We as-sume (in contrast o some other interlingua-basedMT systems, e.g., \[Uchida, 89\]), that the KB, andthus the interlingua, has not been designed withany particular set of languages in mind.Lucy and Koko have been designed to use a sin-gle, reversible linguistic description \[Barnett, 90\],so that a language need only be specified onceand can then serve as both a source and a tar-get.
The syntactic component of this system isbased on an extension of Categorial UnificationGrammar, which serves as the phrase-structurecomponent of an LFG-style f-structure represen-tation.
Semantic processing in both systems ismostly compositional, and is driven by a sharedlexicon that describes the meanings of words interms of the backend KB.
Declarative rules forhandling phenomena such as metonymy and nouncompounding are also shared between the two sys-tems, although they are compiled into separateforms to support understanding and generation.We have used this approach to build a reversibleEnglish/Spanish MT system.Since much of the discussion below will centeraround strategies for lexical choice during gener-ation, we will devote the rest of this section to abrief description of Koko's generation algorithm.In the current implementation, Koko handles onlythe tactical generation phase of Figure 1.
It takesas its input a DRS that contains the meaning thatis to be realized, and, optionally, an f-structurethat describes the syntactic form that the realiza-tion should take 1.
In Section 6, we will discussextending it to handle the task of generating thebest target DR.S.
In addition to a set of semanticassertions, the DKS contains a distinguished vari-able that points to the discourse entity that thesource utterance is 'about'.
For example, in Ex-ample 2 above, this discourse ntity would be thefish.Given this input as a goal, Koko uses thesemantic-head riven algorithm described in\[Calder, 89\] to generate a phrase whose syntaxand semantics atisfy the goal (this algorithm isa special case, suited for categorial grammars, ofthe algorithm described in \[Shieber, 90\]).
The al-gorithm works by peeling off lexical functors andrecursing on their arguments until it bottoms outin an atomic constituent 2.
At each recursive stepof this algorithm, a lexical look-up procedure isinvoked.
This procedure attempts to find a lexi-cat item that matches the current goal.
Once thislexical item, called the semantic head, is found,the algorithm proceeds both top-down and bot-tom up.
If the semantic head is a functor, it pro-ceeds top-down trying to solve the sub-goal(s) forits argument(s).
We use here a notion of goalsatisfaction where a solution (a constituent) sai-l The f-structure can be as specific as desired.
It maycontain no more than the target category or it may evenspecify which words to use.
We do not make use of f-structure specifications in the lexical choice Mgorithms dis-cussed here.2In a categorial grammar,  most  of the syntactic in-formation is contained in the lexicai items.
For exam-ple, where a phrase-structure grammar  might have a ruleS ~ NP  V NP ,  a categorial g rammar  will assign the cat-egory S \NP /NP  to a verb.
The category says, in effect,that  the verb wants to combine with an NP to its right,and one to its left to form a full S. Any such constituentthat takes at least one argument is called a \]unctor, whilea const ituent with no arguments i called atomic.isfies a goal if it has identical semantics and its f-structure is a supergraph of the goal's f-structure.Once a sub-goal is satisfied, the algorithm worksbottom-up by applying (unary) grammar ules tothe argument constituent alone, or (binary) rulesto combine it with the functor.
The algorithm ter-minates when a complete constituent that satisfiesthe goal is found.We now describe the lexical choice componentof this generation procedure in more detail.
Thiscomponent is driven by a reverse index that or-ganizes words by the KB concepts that occur inthe word's meaning.
To find a lexical item thatsatisfies a particular generation goal, the lexicalchoice procedure performs a kind of classificationoperation; it looks at the semantic assertions inthe goal and finds candidate words that matchsome or all of those assertions.
Words that oper-ate syntactically as functors are acceptable venif they match only partially; the recursive partof the process will attempt o match the remain-ing assertions with words that can serve as thefunctor's arguments.
Words that operate syntac-tically as atomic constituents must match all theassertions in order to succeed since there is no ad-ditional way to match any assertions that are leftover .Unfortunately, in the simple form in which itwas just stated, this algorithm for lexical choicefails to handle cases of semantic mismatch be-tween source and target languages.
This is be-cause it takes as input the assertions that werederived from the source text and expects to gen-erate a target text that exactly covers those sameassertions.
In the rest of this paper, we describemodifications to this algorithm that handle casessuch as the ones in Examples 1-4.644 Forced/Unforced Distinc-tionsSemantic mismatches of the kind shown in Ex-ample 1 arise from morphological differences be-tween languages.
When an inflection is syntacti-cally obligatory in a language and it also carriessemantic information, a speaker of that languageis forced to specify facts that can be left out inother languages.
For example, speakers of En-glish are forced to specify number on NPs, whichJapanese does not require.
Speakers of Japanese,in turn, have to indicate the level of formality ofthe discourse as well as the social relation betweenthe participants.
Verb tense, on the other hand,iiis obligatory in both languages.To implement his, we alter the grammar ofeach language to mark as Forced all assertionsthat come from syntactically obligatory inflec-tions.
The marking indicates that the assertionis forced, and records the type of inflection (e.g.,number or tense) that forced it.
Then we mustconsider two modifications to the basic procedurefor lexical look-up: one in which forced assertionsfrom the source text can be dropped from thetarget because they are not required and one inwhich there are forced distinctions in the targetand the corresponding assertions were not presentanywhere in the source (i.e., they are not forcedin the source nor was the information explicitlyvolunteered.)
:We first consider the case in which forced as-sertions from the source are not also required inthe target language.
In general they should bedropped.
The exception is when there is an asser-tion that carries !important information and wouldhave been volunteered but did not have to be sinceit was forced anyway.
This is relatively rare, de-tecting it is in general difficult, and it requires rea-soning within the current discourse context.
Wedescribe here what happens if we assume that theforced assertion should not be carried over.
Tohandle this, we !modify the procedure for lexicallook-up to accept partial matches in which asser-tions that are marked as having been forced inthe source language but that are not forced bythe target grammar are ignored.
In Example 1,for instance, we will allow "inu", which has nonumber assertions, to match the goal 3(dog x) (> (quant i ty  x) 1)Notice, though, that we will still reject anyproposed match that conflicts with a forcedassertion.
4 For example, if there is a forced sin-gular assertion in the source we will not allow aplural lexical form to be used in the target.
Butwe will accept ~ a match a word that makes nocommitment at all about number.The more difficult case is the one in which thetarget language forces a distinction that is notmade in the source.
In this case, some informa-tion must be added to the target text.
In somecases, the information can be derived from thelarger discourse ~context.
In other cases, it maybe possible to ask the user.
And, if both of these3We are  using a representat ion of plurals and masste rms based on \[Link, 83\].4 Two assert ions conflict if they assign incompat ib le  val-ues to a s lo t /proper ty  of an object.fail, the system must have a default.
This case isvery similar to what happens in Examples 2 and3, in which the lexicons of the source and targetlanguages fail to match.
In all three cases, thereis no target word that corresponds exactly to theset of source assertions but there are some num-ber of target words that correspond to the sourceassertions augmented with some additional infor-mation.
We will deal with this problem in detailin the next section.
5655 Lexical  ChoiceNow we consider those cases in which differencesin the lexicons of the source and target languagescause assertions to be either added or dropped.To solve this problem, we need to introduce thenotion of marked and unmarked lexieal forms.
6We define this notion as follows.
Consider a setS of objects or events (which may or may not bea class), and assume that the lexical item L isassociated with S. Now consider one or more spe-cializations (subsets) of S, each of which is definedto have some particular value along some relevantdimension.
The case we are concerned with is thefollowing:1.
There is some subset SS along some dimen-sion D and there is a lexical item LL (distinctfrom L) associated with SS.
In other words,there is a specialized word for this specializedclass.2.
Although L can be used to describe any el-ement of S whose value along dimension Dis unknown, it is infelicitous to use L ratherthan LL to describe an object that is clearlyan element of SS.
By "clearly" here we meanby inspection of the nearby context of the dis-course .In this case, we define L to be an unmarkedform along dimension D and LL to be a markedfo rm.To illustrate this definition, we return to thepez/pescado example.
Let S be the set of fish.
InSpanish, L is then "pez".
But there is a subsetSS of caught fish, and LL is "pescado".
It is in-felicitous to use the word "pez" when it is clearfrom context that the fish has been eaught.
So5See, in part icular ,  step 58 for a t reatment  of exact lythis case.6The marked/unmarked  dist inct ion that  we are exploit-ing here is analogous to the more  tradit ional  one that  isused in morpho logy \[ Jakobson, 66\].
"pez" is unmarked along the dimension of beingcaught, and "pescado" is marked.
The Englishword, "fish", is neither marked nor unmarked.It is important o note here that the choice be-tween a general word and more specific words doesnot always involve a distinction between markedand unmarked terms.
For example, the choice (inEnglish or Spanish) between "fish" and words forits subclasses "trout", "salmon", etc.
is free in thesense that it is perfectly acceptable to use "fish"even when we know the object in question is atrout (unless the fact that it is a trout is relevantto the conversation, in which case we are violatingGricean principles.
)Though there seem to be some cross-linguisticgeneralities about markedness (e.g., that marked-ness is rare along dimensions that are defined bynatural classes), it is a language-specific act thatccrtain words are marked along certain dimen-sions, and these facts must be acquired along withthe grammar of the language.
Acquiring these dis-tinctions will be a substantial amount of work, butthe work is necessary even in non-reversible mono-lingual systems.
For example, a Spanish languagequestion answering system needs to know that thechoice between "trucha" (trout) and "pez" is dif-ferent (and freer) than that between "pez" and"pescado", and that "pez" is the default for thelatter distinction.
Thus, the use of markedness inour lexical choice algorithm is independently mo-tivated, and is not something that has to be addedjust to get reversible machine translation to work.We can now state the algorithm for lexicalchoice.
This algorithm appears to be a complexenumeration of a set of special cases, and in somesense it is.
The reason is that it is actually twoprocesses overlaid on top of each other.
The first isa generation process that deals with the need toadd and subtract information but that does notdepend on the the fact that the DRS it is workingwith came from a linguistic source.
The secondis the fact that there are a few places where factsabout the source text and the source lexicon canbe used to provide guidance to the general purposegeneration algorithm.
For a longer discussion ofthe interaction between these two processes, see\[Barnett, 91c\].The lexical choice procedure takes as input alist of assertions that describe a set S of objects orevents.
The list is structured, with all assertionsarising from a single source lexical item groupedtogether.There are places in this algorithm where appealis made to a knowledge base, its associated infer-66ence mechanisms, and a knowledge-based modelof the current discourse context.
We mark theseplaces with ($).
The performance of this algo-r ithm is tied to the ability of the underlying KBto provide accurate answers to these questions ei-ther by reasoning or by asking a user.
In eachcase, we describe a default strategy that can beused in the case of incomplete knowledge in theKB.There are also places in the algorithm whereconsiderations of meaning alone allow more thanone possible lexical choice, and stylistic factorsmust be considered.
We mark these places with(#) .
The performance of this algorithm in thesecases is tied to our ability to extract statements ofstyle from the source text and to use those state-ments, as well as stylistic preferences within thetarget language, to make choices that best achievethe desired style.Algor i thm:  Modi f ied  Lex ica l  Cho ice1.
If there is a word for S in the target language,then we want to do a straightforward transla-tion except in the case where there was also asingle word for S in the source language butthe speaker chose not to use it and to usea descriptive phrase instead (for example, ina definition of the word).
7 In that case, weneed to preserve that free choice by using aphrase in the target as well.
So check to see ifthere is a single word for S in the source butthe assertions that define S came from morethan one lexical item.
In this case, split theassertions into two subgoals, one for the headand one for the modifiers and recursively callthis algorithm.2.
If there is a word W for S in the targetlanguage and the redundancy check definedabove failed, then if W is not unmarked in thetarget lexicon, use it.
If there is more thanone, then (#)  choose the one with the stylethat best matches the style of the source.3.
If there is a word W for S in the target lan-guage but it is unmarked along some set of di-mensions D, then we need to see if we shoulduse one of the more specific marked formsrather than W. (For example, "fish" in En-glish will map to the unmarked Spanish form7Notice that checking for this case would not be nec-essary in a straightforward transfer system.
It is only anissue here because we want  to be able, when appropriate,to use words that are available in the target but were notin the source."pez".)
So' for each element of D, examineall of the available marked forms.
For each ofthem, do:(a)(b)Check'to see if there is a correspondingmarked word in the source language.
Ifthere is, then since it was not used inthe source we do not need to considerusing !t in the target either, so we canskip tl~is form.Otherwise , ($) check (using some fixedeffort level) to see whether the addi-tional ~information that would licensethis form can be inferred from the dis-course i context.
If it can, then selectthat form.
(For example, the infor-matio 0 that licenses "pescado" will beavailable for the source sentence, "I atefish for dinner.")
If there are synony-mous fiaarked forms, (#)  use style as abasis fbr choosing.If none of the marked forms is chosen, thenuse the unmarked form.
s4.
There is no Word for S in the target language.
(For example, this happens in translatingSpanish "pcscado " or "pescado blanco" intoEnglish, or English "know" into Spanish.)
Inthis case, we must do one of two things:?
See if tlhere is a more specific word thatcan be ~shown to be applicable.
,,?
Use a more general word and add mod-ifiers as necessary to communicate theadditional information.
9Neither of these operations can be done onan entire phrase at once.
So we must peel as-sertions off ?nd pass them and the remainderof the assertion list recursively to this algo-rithm.
But ',we need to distinguish betweenadditional ififormation that was volunteered(e.g., "blanCo" ) and so should definitely berendered in tile target, and additional infor-mation thatlwas forced by the lexicon of thesource language (e.g., the fact that the fishhad been caught).
So we need to keep to-gether all thee assertions that came from as It could in principle happen,  if there are lots of dimen-sions, that  more th in  one marked form will be found.
Wehave not found any!examples of this, though, so  we havenot considered how to choose among them.9See \[Sondheimer, 88\] for a discussion of various possi-bilities in picking addit ional modifiers..67single source word.
To do that, we must peeloff groups of assertions that came from sin-gle lexical items rather than individual asser-tions.An additional complication is that there maybe a single word in the target language fora combination of modifiers that required sev-eral words in the source.
Or there may be aword for the head combined with a modifierother than the last one.
The only way to findsuch words is to peel off modifiers in all possi-ble orders one at a time, two at a time, threeat a time, and so forth.
So, if the assertionsthat describe S came from more than one lex-ical item in the source, examine all combina-tions of ways to peel off modifiers (keepingtogether assertions that came from a singlelexical item), and recursively invoke this algo-rithm on the peeled off part and the remain-der, doing the remainder first and strippingfrom the peeled off part any assertions thatare subsumed by the choice of a renderingfor the remainder.
If more than one distincttarget expression results from this process,(#)  use the target language stylistic rules tochoose among them.There is no word for S in the target languageand all the assertions that describe S camefrom a single lexical item in the source.
(Forexample, this happens in translating Span-ish "pescado" into English or English "know"into Spanish or Japanese "inu" into English.
)(a) First consider the possibility that thereis a word that is more specific in thesense that it supplies morphological in-formation that is required (forced) in thetarget language.
If there is a set of suchwords, call that set SS.i.
For each element of SS, ($) checkto see Whether the additional infor-mation that would license it can beinferred from the discourse context(just as in Step 3b above).
If it can,then select that word.ii.
If there is not enough informationpresent in the context o license anyof the elements of SS, then select heone that is labeled as default.This path will handle the case we de-scribed in Section 4 where a syntacticdistinction that was absent in the sourcetext is forced in the target language.For example, it will handle translatingJapanese "inu" into English: since theconcept Dog will point to both the sin-gular and plural forms of "dog", one ofthese forms must be chosen.
(b) If there was no set SS in the last step, wenext consider the possibility that thereis a word that is more specific in someother way.
Loop until there are no fur-ther specializations of S for which thetarget language contains lexical items:i.
Let SS be the set of immediatespecializations of S (the first timethrough) or the previous value of SSminus all rejected entries (all othertimes).ii.
For each element of SS, check to seewhether it or any of its specializa-tions is lexicalized in the target lan-guage.
If not, eliminate it (and allits descendants) from further con-sideration.iii.
For each remaining element of SS,($) check to see whether the ad-ditional information that would li-cense it can be inferred from thediscourse context (just as in Steps3b and 5a above).
If it can, and ifit itself is lexicalized, select its lex-icalization.
(For example, in trans-lating English "know" into Spanish,this step should succeed for either"saber" or "conocer".
)If, during step iii, the additional require-ments for any element of SS are provento be unsatisfiable in the current dis-course context, eliminate it (and its de-scendants) from further consideration.
(c) If no more specific word is found, wemust use a more general one.
($) Traceup the knowledge base generalization hi-erarchy from S until a set that doeshave a rendering in the target languageis found.
(For example, in translating"pescado", we trace up to the conceptFish.)
Call this P and recursively in-voke this algorithm to realize P in thetarget language.
If there is more thanone candidate for P, then follow all pathsfor the remainder of this algorithm and(#)  use stylistic rules, such as brevity orpreservation of focus, to choose among(d)the resulting expressions.
This particu-lar path will result in the translation ofSpanish "pescado" as "fish".We must also compute the set of asser-tions that would enable a classifier todistinguish S from P (in other words,all the information that we would bethrowing away if we just described S asP).
Call this C. Now we need to decidewhether to translate C. We should dothat if C was volunteered in the sourcebut not if it was forced by the source lex-icon.
So check the source lexicon for P. Ifthere is an entry that is not unmarked onany dimension included in C, then theadditional information was volunteered.Recursively invoke this algorithm on Cto render it.
If there is no entry or thereis one that is unmarked on one or moredimensions included in C (as it will be inthe case of the concept Fish that we willuse in translating "pescado" in Example2) then do:i.
For each such dimension, ($) check(using a fixed effort level) whetherthe information given is bothnonobvious (i.e., it will not be in-ferable by the reader of the targetfrom context) and important for thesense of the text.
If it can be shownto be, 1?
then recursively invoke thisalgorithm to render it.
Otherwise(as for example with the fact thatthe fish was caught), drop it.ii.
For all the remaining assertions inC, recursively invoke this algorithm.6 Translation DivergenceNow we briefly consider cases of translation diver-gence, such as the one in Example 4 above.
Theremust be two parts to the solution to this prob-lem.
First we consider the case in which, for agiven DRS, there is more than one grammatically1?As an example of a case where it is necessary to rendersuch information, consider translating the Japanese word"gohtm" into English.
"Gohtm" is the unmarked form forrice.
It also means pecifically "cooked rice", in contrast tothe marked form, "kome", which means raw rice.
Supposethat "gohtm" is being used in a recipe that specificallyrequires cooked rice.
Then it is important that the modifier"cooked" be rendered explicitly because it matters, yet itis not inferable since raw rice is also a possible (and in facteven more common) ingredient.68acceptable rendering, but one is preferred.
Here,it is necessary to extend the notion of marked-ness so that it applies not just to individual lexicalitems but also to grammatical structures.
Just asin the lexical case, a marked form, if it is appli-cable, must blo~ck the use of any unmarked form.The natural forms must then be marked, and theywill block the Use of "grammatical" but unnatu-ral forms.
One common way to implement thisnotion of a mdrked grammatical form is to usephrasal exicons in which the prefered forms arelisted directly and the more general grammar isonly used whenl no stored phrases match.But we must also consider the case in which thenatural form cannot be generated irectly fromthe DRS.
Rather, it is first necessary to derivea related (possibly equivalent) DRS and then togenerate from that.
This is the process that wedescribed as strategic generation i Figure 1.
Butnow the question arises: how do we choose amongthe candidate DRSs and their corresponding tar-get strings?
T~ae answer is again that markedforms should block unmarked ones.
The simplestway to implement this is to derive all the equiv-alent DRS struc~tures, generate from all of them,and then rank the results.
There may be more ef-ficient ways of doing this, particularly in the casethat patterns of ~narked forms can be used to com-pile preferences into DRS forms, but we have notyet begun to lo0k seriously at this issue.7 ConclusionIn this paper, we have described an approachto machine translation that has three importantproperties:?
It treats many problems of translation tis-match and divergence as primarily problemsof generation from a flexible semantic repre-sentation lahguage rather than as translationproblems per se.?
It relies exclusively on reversible, monolin-gual descriptions of all of the languages ittreats.
Although some comparisons of thesource and target lexicons are required, theycan be done;automatically (and cached if de-sired).
No language-pair information must beexplicitly provided.?
It is stated in a way that enables its perfor-mance to in'crease steadily with the perfor-mance of the underlying knowledge base andreasoning system.This approach does, however, require some ad-ditional information that is not normally presenteither in monolingual NL systems or MT systems.Some of this information must be provided as partof the definition of each language.
This includes:The labeling of syntactic assertions as forcedor unforced.
This information is only usefulfor MT, but it is very easy to provide.The labeling of marked/unmarked distinc-tions along various dimensions.
This requiresmore work, but it is also useful even in purelymonolingual generation systems, since theymay be given sets of assertions for which thereis no exact match.Some additional information must also bepassed along during the understanding process.In particular, the grouping together of assertionsthat came from the same lexical item must be pre-served.8 AcknowledgementsWe'd like to thank the other members of theKBNL project: Chinatsu Aone, Jim Blevins, BillBohrer, Dilip D'Souza, Susann Luper-Foy, KevinKnight, Juan Carlos Martinez, and David New-man for their contributions to this paper.59References\[Barnett, 90\] J. Barnett and I. Mani, "Using Bidi-rectional Semantic Rules for Generation",Proceedings of the Fifth International Work-shop on Natural Language Generation, pp.47-53, Dawson, Pa., 3-6 June, 1990.\[Barnett, 91a\] J. Barnett, D. D'Souza, K. Knight,I.
Mani, P. Martin, E. Rich, C. Aone, J.Blevins, W. Bohrer, S. Luper-Foy, J.C. Mar-tinez, and D. Newman, "Knowledge-BasedNatural Language Processing: the KBNLSystem", MCC Technical Report ACT-NL-123-91, 1991.\[Barnett, 91b\] J. Barnett, E. Rich, and D. Wrob-lewski, "A Functional Interface to a Knowl-edge Base for Use by a Natural LanguageProcessing System", MCC Technical ReportACT-NL-019-91, 1991.\[Barnett, 91c\] J. Barnett, I. Mani, E. Rich, C.Aone, K. Knight, and J. C. Martinez, "Cap-turing Language-Specific Semantic Distinc-tions in lnterlingua-Based MT", in Proceed-ings of MT Summit 3, Washington, D.C.,1991.\[Barnett, 91d\] J. Barnett and I. Mani, "SharedPreferences", Proceedings of the ACL Work-shop on Reversible Grammars in NaturalLanguage Processing, Berkeley, 1991.\[Calder, 89\] J. Calder, M. Reape, and H. Zeevat,"An Algorithm for Generation i  UnificationCategorial Grammar", Proceedings of the 4thConference of the European Chapter of theACL, pp.
233-240, Manchester, 10-12 April,1989.\[Crawford, 90\]J. Crawford, "Access-Limited Logic-A Lan-guage for Knowledge Representation", Ph.D.Thesis, The University of Texas as Austin,1990.\[Dorr, 90\] B. Dorr, "Solving Thematic Diver-gences in Machine Translation", Proceedingsof the 28th Annual Meeting of the A CL, Pitts-burgh, 1990.\[Dymetman, 88\] M. Dymetman and P. Isabelle,"Reversible Logic Grammars for MachineTranslation", Proceedings of the Second In-ternational Confgerence on Theoretical andMethodological Issues in Machine Transla-tion of Natural Languages, 1988.\[Farwell, 90\] D. Farwell and Y. Wilks, "Ultra: AMulti-Lingual Machine Translator", Memo-randa in Computer and Cognitive ScienceMCCS-90-202, Computing Research Labora-tory, New Mexico State University, 1990.\[Heim, 82\] I. Heim, "The Semantics of Definiteand Indefinite Noun Phrases", University ofMassachusetts, Ph.D. Dissertation, 1982.\[Isabelle, 89\] P. Isabelle, "Towards Reversible MTSystems", in Proceedings of MT Summit H,1989.\[Jakobson, 66\] R. Jakobson, "Zur Struktur desRussischen Verbums", in Hamp, House-holder, and Austerlitz, eds., Readings inLinguistics, II, Chicago: The University ofChicago Press, 1966.70\[Kameyama, 91\] M. Kameyama, R. Ochitani,and S. Peters, "Resolving Translation Mis-matches with Information Flow", Proceed-ings of the 29th Annual Meeting of the ACL,Berkeley, 1991.\[Kamp, 84\] H. Kamp, "A Theory of Truth and Se-mantic Representation", in M. Groenendijk,J.
Janssen, and M. Stokhoff, eds., FormalMethods in the Study of Language, Dor-drecht: Forts, 1984.\[Link, 83\] G. Link, "The Logical Analysis of Plu-rals and Mass Terms: A Lattice-TheoreticalApproach", in R. Baeuerle et al (eds.
),Meaning, Use and Interpretation.
Berlin: de-Gruyter, 1983.\[Shieber, 90\] S. Shieber, G. van Noord, F. Pereiraand R. Moore, "Semantic-Head-Driven Gen-eration", Computational Linguistics 16(I),March, 1990.\[Sondheimer, 88\] N. K. Sondheimer, S. Cumming,and R. N. Albano, "How to Realize a Con-cept: Lexical Selection and the ConceptualNetwork in Text Generation", in Proceedingsof the Workshop on Theoretical and Compu-tational Issues in Lexical Semantics, 1988.\[Strzalkowski, 90\] T. Strzalkowski, "ReversibleLogic Grammars for Parsing and Genera-tion", Computational Intelligence 6(3), 1990.\[Uchida, 89\] H. Uchida and M. Zhu, "An Interlin-gun for Multilingual Machine Translation",Shizengengoshori, 1989.
5.
19., Japan, 1989.\[van Noord, 90\] G. Van Noord, "ReversibleUnification-Based Machine Translation", inProceedings of COLING-90, Helsinki, 1990.\[Zajac, 90\] R. Zajac, "A Relational Approach toTranslation", in Proceedings of the Third In-ternational Conference on Theoretical andMethodological Issues in Machine Transla-tion of Natural Language, Austin, Texas,1990.
