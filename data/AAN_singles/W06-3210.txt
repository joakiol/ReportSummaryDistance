Proceedings of the Eighth Meeting of the ACL Special Interest Group on Computational Phonology at HLT-NAACL 2006, pages 79?88,New York City, USA, June 2006. c?2006 Association for Computational LinguisticsA Naive Theory of Affixation and an Algorithm for ExtractionHarald Hammarstro?mDept.
of Computing ScienceChalmers University of Technology412 96, Gothenburg Swedenharald2@cs.chalmers.seAbstractWe present a novel approach to the unsu-pervised detection of affixes, that is, to ex-tract a set of salient prefixes and suffixesfrom an unlabeled corpus of a language.The underlying theory makes no assump-tions on whether the language uses a lotof morphology or not, whether it is pre-fixing or suffixing, or whether affixes arelong or short.
It does however make theassumption that 1. salient affixes have tobe frequent, i.e occur much more oftenthat random segments of the same length,and that 2. words essentially are vari-able length sequences of random charac-ters, e.g a character should not occur infar too many words than random withouta reason, such as being part of a very fre-quent affix.
The affix extraction algorithmuses only information from fluctation offrequencies, runs in linear time, and is freefrom thresholds and untransparent itera-tions.
We demonstrate the usefulness ofthe approach with example case studies ontypologically distant languages.1 IntroductionThe problem at hand can be described as follows:Input : An unlabeled corpus of an arbitrary naturallanguageOutput : A (possibly ranked) set of prefixes andsuffixes corresponding to true prefixes and suf-fixes in the linguistic sense, i.e well-segmentedand with grammatical meaning, for the lan-guage in question.Restrictions : We consider only concatenative mor-phology and assume that the corpus comes al-ready segmented on the word level.The theory and practice of the problem is relevantor even essential in fields such as child language ac-quisition, information retrieval and, of course, thefuller scope of computational morphology and itsfurther layers of application (e.g Machine Transla-tion).The reasons for attacking this problem in an un-supervised manner include advantages in elegance,economy of time and money (no annotated resourcesrequired), and the fact that the same technology maybe used on new languages.An outline of the paper is as follows: we startwith some notation and basic definitions, with whichwe describe the theory that is intended to modelthe essential behaviour of affixation in natural lan-guages.
Then we describe in detail and with ex-amples the thinking behind the affix extraction al-gorithm, which actually requires only a few lines todefine mathematically.
Next, we present and discusssome experimental results on typologically differentlanguages.
The paper then finishes with a brief butcomprehensive characterization of related work andits differences to our work.
At the very end we statethe most important conclusions and ideas on futurecomponents of unsupervised morphological analy-sis.792 A Naive Theory of AffixationNotation and definitions:?
w, s, b, x, y, .
.
.
?
??
: lowercase-letter vari-ables range over strings of some alphabet?
andare variously called words, segments, strings,etc.?
s / w: s is a terminal segment of the word wi.e there exists a (possibly empty) string x suchthat w = xs?
W,S, .
.
.
?
??
: capital-letter variables rangeover sets of words/strings/segments?
fW (s) = |{w ?
W |s / w}|: the number ofwords in W with terminal segment s?
SW = {s|s / w ?
W}: all terminal segmentsof the words in W?
| ?
|: is overloaded to denote both the length ofa string and the cardinality of a setAssume we have two sets of random strings oversome alphabet ?:?
Bases B = {b1, b2, .
.
.
, bm}?
Suffixes S = {s1, s2, .
.
.
, sn}Such that:Arbitrary Character Assumption (ACA) Eachcharacter c ?
?
should be equally likely in anyword-position for any member of B or S.Note that B and S need not be of the same car-dinality and that any string, including the emptystring, could end up belonging to both B and S.They need neither to be sampled from the samedistribution; pace the requirement, the distributionsfrom which B and S are drawn may differ in howmuch probability mass is given to strings of differentlengths.
For instance, it would not be violation if Bwere drawn from a a distribution favouring stringsof length, say, 42 and S from a distribution with astrong bias for short strings.Next, build a set of affixed words W ?
{bs|b ?B, s ?
S}, that is, a large set whose members areconcatenations of the form bs for b ?
B, s ?
S,such that:Frequent Flyer Assumption (FFA) : The mem-bers of S are frequent.
Formally: Given anys ?
S: fW (s) >> fW (x) for all x such that 1.|x| = |s|; and 2. not x / s?
for all s?
?
S).In other words, if we call s ?
S a true suffix and wecall x an arbitrary segment if it neither a true suffixnor the terminal segment of a true suffix, then anytrue suffix should have much higher frequency thanan arbitrary segment of the same length.One may legimately ask to what extent words ofreal natural languages fit the construction model ofW , with the strong ACA and FFA assumptions, out-lined above.
For instance, even though natural lan-guages often aren?t written phonemically, it is nothard to come up with languages that have phonotac-tic constraints on what may appear at the beginningor end of a word, e.g, Spanish *st- may not begina word and yields est- instead.
Another violationof ACA is that (presumably all (Ladefoged, 2005))languages disallow or disprefer a consonant vs. avowel conditioned by the vowel/consonant status ofits predecessor.
However, if a certain element occurswith less frequency than random (the best examplewould be click consonants which, in some languagese.g Eastern !Xo?o (Traill, 1994), occur only initially),this is not a problem to the theory.As for FFA, we may have breaches such as Bibli-cal Aramaic (Rosenthal, 1995) where an old -a?
el-ement appears on virtually everywhere on nouns,making it very frequent, but no longer has any syn-chronic meaning.
Also, one can doubt the require-ment that an affix should need to be frequent; forinstance, the Classical Greek inflectional (lackingsynchronic internal segmentation) alternative medial3p.
pl.
aorist imperative ending -????
(Blomqvistand Jastrup, 1998), is not common at all.Just how realistic the assumptions are is an empir-ical question, whose answer must be judged by ex-periments on the relevant languages.
In the absenseof fully annotated annotated test sets for diverse lan-guages, and since the author does not have access tothe Hutmegs/CELEX gold standard sets for Finnishand English (Creutz and Linde?n, 2004), we can onlygive some guidelining experimental data.ACA On a New Testament corpus of Basque(Leizarraga, 1571) we computed the probabil-ity of a character appearing in the initial, sec-80Positions Distance||p1 ?
p2|| 0.47||p1 ?
p3|| 0.36||p1 ?
p4|| 0.37||p2 ?
p3|| 0.34||p2 ?
p4|| 0.23||p3 ?
p4|| 0.18Table 1: Difference between character distributionsaccording to word position.ond, third or fourth position of the word.
SinceBasque is entirely suffixing, if it complied toACA, we?d expect the distributions to be simi-lar.
However, if we look at the difference of thedistributions in terms of variation distance be-tween two probability distributions (||p?
q|| =12?x |p(x) ?
q(x)|), it shows that they dif-fer considerably ?
especially the initial positionproves more special (see table 1).FFA As for the FFA, we checked a corpus of bibleportions of Warlpiri (Yal, 1968 2001).
This waschosen because it is one of the few languagesknown to the author where data was availableand which has a decent amount of frequent suf-fixes which are also long, e.g case affixes aretypically bisyllabic phonologically and five-ishcharacters long orthographically.
Since the or-thography used marked segmentation, it waseasy to compute FFA statistics on the wordsas removed from segmentation marking.
Com-paring with the lists in (Nash, 1980, Ch.
2) itturns out that FFA is remarkably stable for allgrammatical suffixes occuring in the outermostlayer.
There are however the expected kindof breaches; e.g a tense suffix -ku combinedwith a last vowel -u which is frequent in somefrequent preceding affixes making the terminalsegment -uku more frequent than some genuinethree-letter suffixes.The language known to the author which hasshown the most systematic disconcord with theFFA is Haitian Creole (also in bible corpusexperiments (Hai, 2003 1999)).
Haitian cre-ole has very little morphology of its own butowes the lion?s share of it?s words from French.French derivational morphemes abound inthese words, e.g -syon, which have been care-fully shown by (Lefebvre, 2004) not to be pro-ductive in Haitian Creole.
Thus, the little mor-phology there is in Haitian creole is very dif-ficult to get at without also getting the Frenchrelics.3 An Algorithm for Affix ExtractionThe key question is, if words in natural languagesare constructed as W explained above, can we re-cover the segmentation?
That is, can we find B andS, given only W ?
The answer is yes, we can par-tially decide this.
To be more specific, we can com-pute a score Z such that Z(x) > Z(y) if x ?
SWand y /?
SW .
In general, the converse need not hold,i.e if both x, y ?
SW , or both x, y /?
SW , thenit may still be that Z(x) > Z(y).
This is equiva-lent to constructing a ranked list of all possible seg-ments, where the true members of SW appear at thetop, and somewhere down the list the junk, i.e non-members of SW , start appearing and fill up the restof the list.
Thus, it is not said where on the list thetrue-affixes/junk border begins, just that there is aconsistent such border.Now, how should this list be computed?
Given theFFA, it?s tempting to look at frequencies alone, i.ejust go through all words and make a list of all seg-ments, ranking them by frequency?
This won?t do itbecause 1. it doesn?t compensate between segmentsof different length; naturally, short segments will bemore frequent than long ones, solely by virtue oftheir shortness 2. it overcounts ill-segmented trueaffixes, e.g -ng will invariably get a higher (or equal)count than -ing.
What we will do is a modificationof this strategy, because 1. can easily be amendedby subtracting estimated prior frequencies (underACA) and there is a clever way of tackling 2.
Notethat, to amend 2., when going through w and eachs/w, it would be nice if we could count s only whenit is well-segmented in w. We are given only W sothis information is not available to us, but, the FFAassumption let?s us make a local guess of it.We shall illustrate the idea with an example of anevolving frequency curve of a word ?playing?
andits segmentations ?playing?, ?aying?, ?ying?, ?ing?,?ng?, ?g?
(W being the set of words from an Eng-lish bible corpus (Eng, 1977)).
Figure 1 shows a810100200300400500600700800laying  aying  ying  ing  ng  gfeplayingFigure 1: The observed fW (s) and expected eW (s)frequency for s / w = playing.frequency curve fW (s) and its expected frequencycurve eW (s).
The expected frequency of a suffix sdoesn?t depend on the actual characters of s and isdefined as:eW (s) = |W | ?1r|s|Where r is the size of the alphabet under the assump-tion that its characters are uniformly distributed.
Wedon?t simply use 26 in the case of lowercase Englishsince not all characters are equally frequent.
Insteadwe estimate the size of a would-be uniform distribu-tion from the entropy of the distribution of the char-acters in W .
This gives r ?
18.98 for English andother languages with a similar writing practice.Next, define the adjusted frequency as the differ-ence between the observed frequency and the ex-pected frequency:f ?W (s) = fW (s)?
eW (s)It is the slope of this curve that predicts the presenceof a good split.
Figure 2 shows the appearance ofthis curve again exemplified by ?playing?.After these examples, we are ready to define thesegmentation score of a suffix relative to a word Z :SW ?W ?
Q:ZW (s, w) ={0 if not s / wf ?
(si)?f ?
(si?1)|f ?
(si?1)|if s = si(w) for some iTable 2 shows the evolution of exact values fromthe running example.To move from a Z-score for a segment that is rel-ative to a word we simply sum over all words to get?2024681012141618ng  gZplaying  laying  aying  ying  ingFigure 2: The slope of the f ?W (s) curve for s / w =playing.Input: A text corpus CStep 1.
Extract the set of words W from C (thus allcontextual and word-frequency information isdiscarded)Step 2.
Calculate ZW (s, w) for each w ?
W ands / wStep 3.
Accumulate ZW (s) =?w?W Z(s, w)Table 3: Summary of affix-extraction algorithm.the final score Z : SW ?
Q:ZW (s) =?w?WZ(s, w) (1)To be extra clear, the FFA assumption is ?ex-ploited?
in two ways.
On the one hand, frequentaffixes get many oppurtunities to get a score (whichcould, however, be negative) in the final sum overw ?
W .
On the other hand, the frequency is whatmake up the appearance of the slope that predicts thesegmentation point.The final Z-score in equation 1 is the one thatpurports to have the property that Z(x) > Z(y) ifx ?
SW and y /?
SW ?
at least if purged (see be-low).
A summary of the algorithm described in thissection is displayed in table 3.The time-complexity bounding factor is the num-ber of suffixes, i.e the cardinality of SW , which islinear (in the size of the input) if words are boundedin length by a constant and quadratic in the (really)worst case if not.82s playing laying aying ying ing ng gf(s) 1 4 12 40 706 729 756eW (s) 0.00 0.00 0.00 0.10 1.90 36.0 684f(s)?
eW (s) 0.99 3.99 11.9 39.8 704 692 71.0Z(s,?playing?)
0.00 2.99 1.99 2.32 16.6 -0.0 -0.8Table 2: Exact values of frequency curves and scores from the running ?playing?
example.1028682.0 ing 111264.0 ling594208.0 ed 111132.0 ent371145.0 s 109725.0 ating337464.0 ?s 109125.0 ate326250.0 ation 108228.0 an289536.0 es 97020.0 ies238853.5 e 94560.0 ts222256.0 er 81648.0 ically191889.0 ers 81504.0 ment172800.0 ting 78669.0 led168288.0 ly 77900.0 ering159408.0 ations 74976.0 er?s143775.0 ted 73988.0 y130960.0 able .
.
.
.
.
.116352.0 ated -26137.9 l113364.0 al -38620.6 m113280.0 ness -78757.3 aTable 4: Top 30 and bottom 3 extracted suffixesfor English.
47178 unique words yielded a total of154407 ranked suffixes.4 Experimental ResultsFor a regular English 1 million token newspapercorpus we get the top 30 plus bottom 3 suffixes asshown in table 4.English has little affixation compared to e.g Turk-ish which is at the opposite end of the typologicalscale (Dryer, 2005).
The corresponding results forTurkish on a bible corpus (Tur, 1988) is shown intable 5.The results largely speak for themselves but somecomments are in order.
As is easily seen from thelists, some suffixes are suffixes of each other so onecould purge the list in some way to get only themost ?competitive?
suffixes.
One purging strategywould be to remove x from the list if there is a z1288402.4 i 33756.55 ler151056.9 er 29816.53 da142552.6 in 29404.49 di141603.3 im 28337.89 le134403.2 en 26580.41 dan130794.5 e 26373.54 r127352.0 an 24183.99 ti113482.6 a 22527.26 un82581.95 ya 21388.71 iniz78447.74 ar 20993.87 sin76353.77 ak 20117.60 ik68730.00 n 18612.14 li64761.37 ir 18316.45 ek53021.67 la .
.
.
.
.
.47218.78 ini -38091.8 t44858.18 lar -240917.5 l37229.14 iz -284460.1 sTable 5: Top 30 and bottom 3 extracted suffixesfor Turkish.
56881 unique words yielded a total of175937 ranked suffixes.such that x = yx and Z(z) > Z(x) (this wouldremove e.g -ting if -ing is above it on the list).
Amore sophisticated purging method is the following,which does slightly more.
First, for a word w ?
Wdefine its best segmentation as: Segment(w) =argmaxs/wZ(s).
Then purge by keeping only thosesuffixes which are the best parse for at least oneword: S?W = {s ?
SW |?wSegment(w) = s}.Such purging kicks out the bulk of ?junk?
suf-fixes.
Table 4 shows the numbers for English, Turk-ish and the virtually affixless Maori (Bauer et al,1993).
It should noted that ?junk?
suffixes still re-main after purging ?
typically common stem-finalcharacters ?
and that there is no simple relationbetween the number of suffixes left after purgingand the amount of morphology of the language inquestion.
Otherwise we would have expected themorphology-less Maori to be left with no, or 28-ish,83Language Corpus Tokens |W | |SW | |S?W |Maori (Mao, 1996) 1101665 8354 23007 78English (Eng, 1977) 917634 12999 39845 63Turkish (Tur, 1988) 574592 56881 175937 122Table 6: Figures for different languages on the ef-fects on the size of the suffix list after purging.suffixes or at least less than English.A good sign is that the purged list and its orderseems to be largely independent of corpus size (aslong as the corpus is not very small) but we do getsome significant differences between bible Englishand newspaper English.We have chosen to illustrate using affixes but themethod readily generalizes to prefixes as well andeven prefixes and suffixes at the same time.
Asan example of this, we show top-10 purged prefix-suffix scores in the same table also for some typo-logically differing languages in table 7.
Again, weuse bible corpora for cross-language comparability(Swedish (Swe, 1917) and Swahili (Swa, 1953)).The scores have been normalized in each languageto allow cross-language comparison ?
which, judg-ing from the table, seems meaningful.
Swahili is anexclusively prefixing language but verbs tend to endin -a (whose status as a morpheme is the linguisticsense can be doubted), whereas Swedish is suffix-ing, although some prefixes are or were productivein word-formation.A full discussion of further aspects such as a moreinformed segmentation of words, peeling of multi-ple suffix layers and purging of unwanted affixes re-quires, is beyond the scope of this paper.5 Related WorkFor reasons of space we cannot cite and commentevery relevant paper even in the narrow view ofhighly unsupervised extraction of affixes from rawcorpus data, but we will cite enough to cover eachline of research.
The vast fields of word segmenta-tion for speech recognition or for languages whichdo not mark word boundaries will not be covered.In our view, segmentation into lexical units is a dif-ferent problem than that of affix extraction since thefrequencies of lexical items are different, i.e occurSwedish English Swahilifo?r- 0.097 -eth 0.086 -a 0.100-en 0.086 -ing 0.080 wa- 0.095-na 0.036 -ed 0.063 ali- 0.065-ade 0.035 -est 0.036 nita- 0.059-a 0.034 -th 0.035 aka- 0.049-ar 0.033 -es 0.034 ni- 0.046-er 0.033 -s 0.033 ku- 0.044-as 0.032 -ah 0.026 ata- 0.042-s 0.031 -er 0.026 ha- 0.032-de 0.031 -ation 0.019 a- 0.031. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Table 7: Comparative figures for prefix vs. suffixdetection.
The high placement of English -eth and-ah are due to the fact that the bible version used hasdrinketh, sitteth etc and a lot of personal names in-ah.much more sparsely.
Results from this area whichhave been carried over or overlap with affic detec-tion will however be taken into account.
A lot ofthe papers cited have a wider scope and are still use-ful even though they are critisized here for having anon-optimal affix detection component.Many authors trace their approches back to twoearly papers by Zellig Harris (Harris, 1955; Har-ris, 1970) which count letter successor varieties.The basic procedure is to ask how many differentphonemes occur (in various utterances e.g a corpus)after the first n phonemes of some test utterance andpredict that segmentation(s) occur where the numberof succesors reaches a peak.
For example, if we haveplay, played, playing, player, players, playgroundand we wish to test where to segment plays, the suc-cesor count for the prefix pla would be 1 becauseonly y occurs after whereas the number of succes-sors of play peak at three (i.e {e, i, g}).
Althoughthe heuristic has had some success it was shown (invarious interpretations) as early as (Hafer and Weiss,1974) that it is not really sound ?
even for English.A slightly better method is to compile a set of wordsinto a trie and predict boundaries at nodes with highactitivity (e.g (Johnson and Martin, 2003; Schoneand Jurafsky, 2001; Kazakov and Manandhar, 2001)and earlier papers by the same authors), but this notsound either as non-morphemic short common char-acter sequences also show significant branching.84The algorithm in this paper is differs significantlyfrom the Harris-inspired varieties.
First, we donot record the number of phonemes/character of agiven prefix/suffix but the total number of contin-uations.
In the example above, that would be theset {ed, ing, er, ers, ground} rather than the three-member set of continuing phonemes/characters.Secondly, segmentation of a given word is not theimmediate objective and what amounts to identifi-cation of the end of a lexical (thus generally low-frequency) item is not within the direct reach of themodel.
Thirdly, and most importantly, the algorithmin this paper looks at the slope of the frequencycurve not at peaks in absolute frequency.A different approach, sometimes used in com-plement of other sources of information, is to se-lect aligned pairs (or sets) of strings that share along character sequence (work includes (Jacquemin,1997; Yarowsky and Wicentowski, 2000; Baroni etal., 2002; Clark, 2001)).
A notable advantage is thatone is not restricted to concatenative morphology.Many publications (C?avar et al, 2004; Brent etal., 1995; Goldsmith et al, 2001; De?jean, 1998;Snover et al, 2002; Argamon et al, 2004; Gold-smith, 2001; Creutz and Lagus, 2005; Neuvel andFulop, 2002; Baroni, 2003; Gaussier, 1999; Sharmaet al, 2002; Wicentowski, 2002; Oliver, 2004),and various other works by the same authors, de-scribe strategies that use frequencies, probabilities,and optimization criteria, often Minimum Descrip-tion Length (MDL), in various combinations.
So far,all these are unsatisfactory on two main accounts; onthe theretical side, they still owe an explanation ofwhy compression or MDL should give birth to seg-mentations coinciding with morphemes as linguisti-cally defined.
On the experimental side, thresholds,supervised/developed parametres and selective inputstill cloud the success of reported results, which, inany case, aren?t wide enough to sustain some toorash language independence claims.To be more specific, some MDL approaches aimto minimize the description of the set of words inthe input corpus, some to describe all tokens inthe corpus, but, none aims to minimize, what onewould otherwise expect, the set of possible wordsin the language.
More importantly, none of the re-viewed works allow any variation in the descrip-tion language (?model?)
during the minimizationsearch.
Therefore they should be more properly la-beled ?weighting schemes?
and it?s an open questionwhether their yields correspond to linguistic analy-sis.
Given an input corpus and a traditional linguis-tic analysis, it is trivial to show that it is possible todecrease description length (according to the givenschemes) by stepping away from linguistic analysis.Moreover, various forms of codebook compression,such as Lempel-Ziv compression, yield shorter de-scription but without any known linguistic relevanceat all.
What is clear, however, apart from whether itis theoretically motivated, is that MDL approachesare useful.A systematic test of segmentation algorithms overmany different types of languages has yet to be pub-lished.
For three reasons, it will not be undertakenhere either.
First, as e.g already Manning (1998)notes for sandhi phenomena, it is far from clearwhat the gold standard should be (even though wemay agree or agree to disagree on some familiarEuropean languages).
Secondly, segmentation al-gorithms may have different purposes and it mightnot make good sense to study segmentation in isola-tion from induction of paradigms.
Lastly, and mostimportantly, all of the reviewed techniques (Wicen-towski, 2004; Wicentowski, 2002; Snover et al,2002; Baroni et al, 2002; Andreev, 1965; C?avaret al, 2004; Snover and Brent, 2003; Snover andBrent, 2001; Snover, 2002; Schone and Jurafsky,2001; Jacquemin, 1997; Goldsmith and Hu, 2004;Sharma et al, 2002; Clark, 2001; Kazakov andMan-andhar, 1998; De?jean, 1998; Oliver, 2004; Creutzand Lagus, 2002; Creutz and Lagus, 2003; Creutzand Lagus, 2004; Hirsima?ki et al, 2003; Creutzand Lagus, 2005; Argamon et al, 2004; Gaussier,1999; Lehmann, 1973; Langer, 1991; Flenner, 1995;Klenk and Langer, 1989; Goldsmith, 2001; Gold-smith, 2000; Hu et al, 2005b; Hu et al, 2005a;Brent et al, 1995), as they are described, havethreshold-parameters of some sort, explicitly claimnot to work well for an open set of languages, orrequire noise-free all-form input (Albright, 2002;Manning, 1998; Borin, 1991).
Therefore it is notpossible to even design a fair test.In any event, we wish to appeal to the merits ofdeveloping a theory in parallel with experimentation?
as opposed to only ad hoc result chasing.
If wehave a theory and we don?t get the results we want,85wemay scrutinize the assumptions behind the theoryin order to modify or reject it (understanding whywe did so).
Without a theory there?s no telling whatto do or how to interpret intermediate numbers in along series of calculations.6 ConclusionWe have presented a new theory of affixation and aparameter-less efficient algorithm for collecting af-fixes from raw corpus data of an arbitrary language.Depending on one?s purposes with it, a cut-off pointfor the collected list is still missing, or at least, wedo not consider that matter here.
The results are verypromising and competitive but at present we lackformal evaluation in this respect.
Future directionsalso include a more specialized look into the relationbetween affix-segmentation and paradigmatic varia-tion and further exploits into layered morphology.7 AcknowledgementsThe author has benefited much from discussionswith Bengt Nordstro?m.ReferencesAdam C. Albright.
2002.
The Identification of Bases inMorphological Paradigms.
Ph.D. thesis, University ofCalifornia at Los Angeles.Nikolai Dmitrievich Andreev, editor.
1965.
Statistiko-kombinatornoe modelirovanie iazykov.
AkademiaNauk SSSR, Moskva.Shlomo Argamon, Navot Akiva, Amihood Amit, andOren Kapah.
2004.
Efficient unsupervised recursiveword segmentation using minimum description length.In COLING-04, 22-29 August 2004, Geneva, Switzer-land.Marco Baroni, Johannes Matiasek, and Harald Trost.2002.
Unsupervised discovery of morphologically re-lated words based on orthographic and semantic simi-larity.
In Proceedings of the Workshop on Morpholog-ical and Phonological Learning of ACL/SIGPHON-2002, pages 48?57.Marco Baroni.
2003.
Distribution-driven morpheme dis-covery: A computational/experimental study.
Year-book of Morphology, pages 213?248.Winifred Bauer, William Parker, and Te KareongawaiEvans.
1993.
Maori.
Descriptive Grammars.
Rout-ledge, London & New York.Jerker Blomqvist and Poul Ole Jastrup.
1998.
GrekiskGrammatik: Graesk grammatik.
Akademisk Forlag,K?benhavn, 2 edition.Lars Borin.
1991.
The Automatic Induction of Morpho-logical Regularities.
Ph.D. thesis, University of Upp-sala.Michael R. Brent, S. Murthy, and A. Lundberg.
1995.Discovering morphemic suffixes: A case study in min-imum description length induction.
In Fifth Interna-tional Workshop on Artificial Intelligence and Statis-tics, Ft. Lauderdale, Florida.Damir C?avar, Joshua Herring, Toshikazu Ikuta, Paul Ro-drigues, and Giancarlo Schrementi.
2004.
On in-duction of morphology grammars and its role in boot-strapping.
In Gerhard Ja?ger, Paola Monachesi, GeraldPenn, and Shuly Wintner, editors, Proceedings of For-mal Grammar 2004, pages 47?62.Alexander Clark.
2001.
Learning morphology with pairhidden markov models.
In ACL (Companion Volume),pages 55?60.Mathias Creutz and Krista Lagus.
2002.
Unsuperviseddiscovery of morphemes.
In Proceedings of the 6thWorkshop of the ACL Special Interest Group in Com-putational Phonology (SIGPHON), Philadelphia, July2002, pages 21?30.
Association for ComputationalLinguistics.Mathias Creutz and Krista Lagus.
2003.
Unsuperviseddiscovery of morphemes.
In Proceedings of the 6thWorkshop of the ACL Special Interest Group in Com-putational Phonology (SIGPHON), Philadelphia, July2002, pages 21?30.
Association for ComputationalLinguistics.Mathias Creutz and Krista Lagus.
2004.
Induction ofa simple morphology for highly-inflecting languages.In Proceedings of the 7th Meeting of the ACL Spe-cial Interest Group in Computational Phonology (SIG-PHON), pages 43?51.
Barcelona.Mathias Creutz and Krista Lagus.
2005.
Unsupervisedmorpheme segmentation and morphology inductionfrom text corpora using morfessor 1.0.
Technical re-port, Publications in Computer and Information Sci-ence, Report A81, Helsinki University of Technology,March.Mathias Creutz and Krister Linde?n.
2004.
Morphemesegmentation gold standards for finnish and english.publications in computer and information science, re-port a77, helsinki university of technology.
Technicalreport, Publications in Computer and Information Sci-ence, Report A77, Helsinki University of Technology,October.86Herve?
De?jean.
1998.
Concepts et alorithmes pourla de?couverte des structures formelles des langues.Ph.D.
thesis, Universite?
de Caen Basse Normandie.Matthew S. Dryer.
2005.
Prefixing versus suffix-ing in inflectional morphology.
In Bernard Comrie,Matthew S. Dryer, David Gil, and Martin Haspelmath,editors, World Atlas of Language Structures, pages110?113.
Oxford University Press.1977.
The holy bible, containing the old and new testa-ments and the apocrypha in the authorized king jamesversion.
Thomas Nelson, Nashville, New York.Gudrun Flenner.
1995.
Quantitative morphseg-mentierung im spanischen auf phonologischer basis.Sprache und Datenverarbeitung, 19(2):63?78.
Alsocited as: Computatio Linguae II, 1994, pp.
1994 aswell as Sprache und Datenverarbeitung 19(2):31-62,1994.E?ric Gaussier.
1999.
Unsupervised learning of deriva-tional morphology from inflectional lexicons.
In Pro-ceedings of the 37th Annual Meeting of the Associa-tion for Computational Linguistics (ACL-1999).
Asso-ciation for Computational Linguistics, Philadephia.John Goldsmith and Yu Hu.
2004.
From signatures to fi-nite state automata.
Technical report TR-2005-05, De-partment of Computer Science, University of Chicago.John Goldsmith, Derrick Higgins, and Svetlana Soglas-nova.
2001.
Automatic language-specific stem-ming in information retrieval.
In Carol Peters, edi-tor, Cross-Language Information Retrieval and Eval-uation: Proceedings of the CLEF 2000 Workshop,Lecture Notes in Computer Science, pages 273?283.Springer-Verlag, Berlin.John Goldsmith.
2000.
Linguistica: An automaticmorphological analyzer.
In A. Okrent and J. Boyle,editors, Proceedings from the Main Session of theChicago Linguistic Society?s thirty-sith Meeting.John Goldsmith.
2001.
Unsupervised learning of themorphology of natural language.
Computational Lin-guistics, 27(2):153?198.Margaret A. Hafer and Stephen F. Weiss.
1974.
Wordsegmentation by letter successor varieties.
Informa-tion and Storge Retrieval, 10:371?385.2003 [1999].
Bib la.
American Bible Society.Zellig S. Harris.
1955.
From phoneme to morpheme.Language, 31(2):190?222.Zellig S. Harris.
1970.
Morpheme boundaries withinwords: Report on a computer test.
In Zellig S. Harris,editor, Papers in Structural and Transformational Lin-guistics, volume 1 of Formal Linguistics Series, pages68?77.
D. Reidel, Dordrecht.Teemu Hirsima?ki, Mathias Creutz, Vesa Siivola, andMikko Kurimo.
2003.
Unlimited vocabulary speechrecognition based on morphs discovered in an unsu-pervised manner.
In Proceedings of Eurospeech 2003,Geneva, pages 2293?2996.
Geneva, Switzerland.Yu Hu, Irina Matveeva, John Goldsmith, and ColinSprague.
2005a.
Refining the SED heuristic formorpheme discovery: Another look at Swahili.
InProceedings of the Workshop on PsychocomputationalModels of Human Language Acquisition, pages 28?35,Ann Arbor, Michigan, June.
Association for Computa-tional Linguistics.Yu Hu, Irina Matveeva, John Goldsmith, and ColinSprague.
2005b.
Using morphology and syntax to-gether in unsupervised learning.
In Proceedings ofthe Workshop on Psychocomputational Models of Hu-man Language Acquisition, pages 20?27, Ann Arbor,Michigan, June.
Association for Computational Lin-guistics.Christian Jacquemin.
1997.
Guessing morphology fromterms and corpora.
In Proceedings, 20th Annual In-ternational ACM SIGIR Conference on Research andDevelopment in Information Retrieval (SIGIR ?97),Philadelphia, PA.Howard Johnson and Joel Martin.
2003.
Unsuper-vised learning of morphology for english and inukti-tut.
In HLT-NAACL 2003, Human Language Technol-ogy Conference of the North American Chapter of theAssociation for Computational Linguistics, May 27 -June 1, Edmonton, Canada, volume Companion Vol-ume - Short papers.Dimitar Kazakov and Suresh Manandhar.
1998.
A hy-brid approach to word segmentation.
In C. D. Page,editor, Proceedings of the 8th International Workshopon Inductive Logic Programming (ILP-98) in Madi-son, Wisconsin, USA, volume 1446 of Lecture Notesin Artificial Intelligence.
Springer-Verlag, Berlin.Dimitar Kazakov and Suresh Manandhar.
2001.
Un-supervised learning of word segmentation rules withgenetic algorithms and inductive logic programming.Machine Learning, 43:121?162.Ursula Klenk and Hagen Langer.
1989.
Morphologicalsegmentation without a lexicon.
Literary and Linguis-tic Computing, 4(4):247?253.Peter Ladefoged.
2005.
Vowels and Consonants.
Black-well, Oxford, 2 edition.Hagen Langer.
1991.
Ein automatisches Morphseg-mentierungsverfahren fu?r deutsche Wortformen.
Ph.D.thesis, Georg-August-Universita?t zu Go?ttingen.Claire Lefebvre.
2004.
Issues in the study of Pidgin andCreole languages, volume 70 of Studies in LanguageCompanion Series.
John Benjamins, Amsterdam.87Hubert Lehmann.
1973.
Linguistische Modellbildungund Methodologie.
Max Niemeyer Verlag, Tu?bingen.Pp.
71-76 and 88-93.Joanes Leizarraga.
1571.
Iesus krist gure iaunaren tes-tamentu berria.
Pierre Hautin, Inprimizale, Roxellan.
[NT only].Christopher D. Manning.
1998.
The segmentation prob-lem in morphology learning.
In Jill Burstein and Clau-dia Leacock, editors, Proceedings of the Joint Confer-ence on New Methods in Language Processing andComputational Language Learning, pages 299?305.Association for Computational Linguistics, Somerset,New Jersey.1996.
Maori bible.
The British & Foreign Bible Society,London, England.David G. Nash.
1980.
Topics in Warlpiri Grammar.Ph.D.
thesis, Massachusetts Institute of Technology.Sylvain Neuvel and Sean A. Fulop.
2002.
Unsuper-vised learning of morphology without morphemes.
InWorkshop on Morphological and Phonological Learn-ing at Association for Computational Linguistics 40thAnniversary Meeting (ACL-02), July 6-12, pages 9?15.ACL Publications.A.
Oliver.
2004.
Adquisicio?
d?informacio?
le`xica i mor-fosinta`ctica a partir de corpus sense anotar: apli-cacio?
al rus i al croat.
Ph.D. thesis, Universitat deBarcelona.Franz Rosenthal.
1995.
A grammar of biblical Aramaic,volume 5 of Porta linguarum Orientalium.
Harras-sowitz, Wiesbaden, 6 edition.Patrick Schone and Daniel Jurafsky.
2001.
Knowledge-free induction of inflectional morphologies.
In Pro-ceedings of the North American Chapter of the Asso-ciation for Computational Linguistics, Pittsburgh, PA,2001.Utpal Sharma, Jugal Kalita, and Rajib Das.
2002.
Unsu-pervised learning of morphology for building lexiconfor a highly inflectional language.
In Proceedings ofthe 6th Workshop of the ACL Special Interest Group inComputational Phonology (SIGPHON), Philadelphia,July 2002, pages 1?10.
Association for ComputationalLinguistics.Matthew G. Snover and Michael R. Brent.
2001.
Abayesian model for morpheme and paradigm identifi-cation.
In Proceedings of the 39th Annual Meeting ofthe Association for Computational Linguistics (ACL-2001), pages 482?490.
Morgan Kaufmann Publishers.Matthew G. Snover and Michael R. Brent.
2003.
A prob-abilistic model for learning concatenative morphology.In S. Becker, S. Thrun, and K. Obermayer, editors, Ad-vances in Neural Information Processing Systems 15,pages 1513?1520.
MIT Press, Cambridge, MA.Matthew G. Snover, Gaja E. Jarosz, and Michael R.Brent.
2002.
Unsupervised learning of morphol-ogy using a novel directed search algorithm: Takingthe first step.
In Workshop on Morphological andPhonological Learning at Association for Computa-tional Linguistics 40th Anniversary Meeting (ACL-02),July 6-12.
ACL Publications.Matthew G. Snover.
2002.
An unsupervised knowledgefree algorithm for the learning of morphology in nat-ural languages.
Master?s thesis, Department of Com-puter Science, Washington University.1953.
Maandiko matakatifu ya mungu yaitwaya biblia,yaani agano la kale na agano jipya, katika lugha yakiswahili.
British and Foreign Bible Society, London,England.1917.
Gamla och nya testamentet: de kanoniskabo?ckerna.
Norstedt, Stockgholm.Anthony Traill.
1994.
A !Xo?o?
Dictionary, volume 9 ofQuellen zur Khoisan-Forschung/Research in KhoisanStudies.
Ru?diger Ko?ppe Verlag, Ko?ln.1988.
Turkish bible.
American Bible Society, Tulsa, Ok-lahoma.Richard Wicentowski.
2002.
Modeling and LearningMultilingual Inflectional Morphology in a MinimallySupervised Framework.
Ph.D. thesis, Johns HopkinsUniversity, Baltimore, MD.Richard Wicentowski.
2004.
Multilingual noise-robustsupervised morphological analysis using the word-frame model.
In Proceedings of the ACL Special Inter-est Group on Computational Phonology (SIGPHON),pages 70?77.1968?2001.
Bible: selections in warlpiri.
Summer Insti-tute of Linguistics.
Document 0650 of the AboriginalStudies Electronic Data Archive (ASEDA), AIATSIS(Australian Institute of Aboriginal and Torres Strait Is-lander Studies), Canberra.David Yarowsky and Richard Wicentowski.
2000.
Min-imally supervised morphological analysis by multi-modal alignment.
In Proceedings of the 38th AnnualMeeting of the Association for Computational Linguis-tics (ACL-2000), pages 207?216.88
