THE VOYAGER SPEECH UNDERSTANDING SYSTEM:  A PROGRESS REPORT*Victor Zue, James Glass, David Goodine, Hong Leung,Michael Phillips, Joseph Polifroni, and Stephanie SeneffSpoken Language Systems GroupLaboratory for Computer ScienceMassachusetts Institute of TechnologyCambridge, Massachusetts 02139ABSTRACTAs part of the DARPA Spoken Language System program, we recently initiated an effort in spokenlanguage understanding.
A spoken language system addresses applications in which speech is used forinteractive problem solving between a person and a computer.
In these applications, not only must thesystem convert he speech signal into text, it must also understand the linguistic structure of a sentence inorder to generate the correct response.
This paper describes our early experience with the development ofthe MIT VOYAGER spoken language system.INTRODUCTIONRecently, we have been directing our research effort towards poken language understanding as partof the DARPA Spoken Language System program.
The project is motivated by the belief that many ofthe applications suitable for human/machine interaction using speech typically involve interactive problemsolving.
That is, in addition to converting the speech signal to text, the computer must also understand thelinguistic structure of a sentence in order to generate the correct response.
We have focused our attentionon three main issues.
First, the system must integrate speech recognition with natural language in order toachieve speech understanding.
Second, the system must have a realistic application domain, and be able totranslate spoken input into appropriate actions.
Finally, the system must begin to deal with spontaneousspeech, since people do not always utter grammatically well-formed sentences during a spoken dialogue.Over the past six months, we have constructed the skeleton of a spoken language system.
The purposeof this paper is to describe the various components of this system.
In related activities, we have collected asizeable spontaneous speech database, and have used the data for analyses, ystem training and evaluation.The collection and analysis of the spontaneous speech database, and the preliminary evaluation of our spokenlanguage system are described in two companion papers that appear elsewhere in these proceedings \[1,2\].TASK DESCRIPT IONIn order to explore issues related to a fully-interactive spoken language system, we have selected a taskin which the system knows about the physical environment ofa specific geographical rea as well as certainobjects inside this area, and can provide assistance on how to get from one location to another within thisarea.
The system, which we call VOYAGER, currently focuses on the the city of Cambridge, Massachusetts,between MIT and Harvard University, as shown in Figure 1.
It can answer a number of different ypes ofquestions about certain hotels, restaurants, hospitals, and other objects within this region.
At the moment,VOYAGER has a vocabulary of 324 words.
Within this limited domain of knowledge, it is our hope that*This research was supported by DARPA under Contract N00014-89-J-1332, monitored through the Office of Naval Research.51Figure 1: A display showing the geographical region known to the VOYAGER system.VOYAGER will eventually be able to handle any reasonable query that a native speaker is likely to initiate.As time progresses, VOYAGER'S knowledge base will undoubtedly grow.SYSTEM DESCRIPT IONVOYAGER is made up of three components.
The speech recognition component converts the speech signalinto a set of word hypotheses.
The natural anguage component then provides a linguistic interpretationof the set of words.
The parse generated by the natural anguage component is subsequently transformedinto a set of query functions, which are passed to the back-end for response generation.
The back-end is anenhanced version of a direction assistance program developed by Jim Davis of MIT's Media Laboratory \[3\].We will describe ach component in sequence, paying particular attention to those parts that have not beenpreviously reported.SPEECH RECOGNIT ION COMPONENTThe first component of VOYAGER uses the SUMMIT speech recognition system developed in our group.SUMMIT places heavy emphasis on the extraction of phonetic information from the speech signal.
It achievesspeech recognition by explicitly detecting acoustic landmarks in the signal in order to facilitate acoustic-phonetic feature xtraction.
The system can be trained automatically, since it does not rely on extensiveknowledge ngineering.
The design philosophy, implementation, and evaluation of the SUMMIT system havebeen described in detail previously \[4\].
As a result, we will only report in this paper modifications to thesystem since the last workshop.
These include the development of a new module for lexical expansion via52phonological rules, and a new corrective training procedure.Lexical ExpansionThe original SUMMIT system used a phonological expansion capability provided to us by SRI \[6\].
Withinthe last year, however, we have decided to rewrite this part of the system in order to establish increasedflexibility and speed.
The new version, named MARBLE, offers several new properties.
A canonic set ofphonemes i represented by a set of default values for distinctive features.
Specified allophonic informationdue to context dependencies can be represented in the particular instance of the phoneme generated in aword lattice.
Thus, for instance, when a word-final/s/and a word-initial/s/merge, the result ing/s/canbe marked as \[+geminate\].
This information can then be incorporated into the scoring for the particularallophone.
The allophonic slot can also be used to indicate place of articulation of adjacent consonants, forexample, to facilitate the decoding of context-dependent models.
The rule-writing process is also straight-forward, and it is simple to keep track of the rule ordering.
Finally, the time it takes to expand a lexicon hasbeen reduced.
We believe this new rule system will be a powerful tool for effectively representing contextdependencies.Corrective TrainingThe training of SUMMIT is performed iteratively, after being initialized on the TIMIT database \[4,5\].
Foreach iteration, the recognizer computes the best alignment between a path in the acoustic phonetic networkand a path in the lexical network, i.e., the recognized output.
The recognizer also computes a forced alignmentusing only the correct string of words.
The system then trains the next iteration of phonetic models based onthe matches between lexical arcs and phonetic segments in the forced alignments.
The recognizer also adjustslexical arc weights based on a comparison of the number of times the arc was used incorrectly (present inthe best alignment but not in the forced alignment) o the number of times the arc was missed (present inthe forced alignment but not in the best alignment).
The goal of this corrective training procedure is toequalize the number of times an arc is missed and the number of times the arc is used in the wrong way.If sufficient training data is not available for a particular arc, then the weights are derived by collapsing itwith other phonetically-similar arcs.Presently, lexical decoding is accomplished by using the Viterbi algorithm to find the best path thatmatches the acoustic-phonetic network with the lexical network.
Since the speech recognition and naturallanguage components are not as yet fully integrated, we currently use a word-pair language model with aperplexity of 22 to constrain the search space.NATURAL LANGUAGE COMPONENTIn the context of a spoken language system, the natural language component should perform two criticalfunctions: 1) to provide constraint for the recognizer component, and 2) to provide an interpretation f themeaning of the sentence to the back end.
Our natural anguage system, TINA, was specifically designed tomeet hese two needs.
The basic design of TINA has been described elsewhere \[7\], and therefore will only bebriefly mentioned here.
Instead, we would like to focus on the issue of how to incorporate semantics into theparses.
We have found that an enrichment of the parse tree with semantically loaded categories at the lowerlevels leads to both improved word predictions and a relatively straightforward interface with the back end.General DescriptionThe grammar is entered as a set of simple context-free rules which are automatically converted to a53shared network structure.
The nodes in the network are augmented with constraint filters (both syntacticand semantic) that operate only on locally available parameters.
Typically, several independent hypothesesare simultaneously active, and parameter modifications include protection of shared information, such that aparallel implementation would be possible.
Efficient memory use is achieved through a recycling mechanismfor the node structures, uch that they become available in a resource pool whenever their current assignmentis completed.
These issues will become important as we move towards a fully integrated system.One of the key features of TINA is that all arcs in the network are associated with probabilities, acquiredautomatically from a set of example sentences.
It is important o note that the probabilities are establishednot on the rule productions but rather on arcs connecting sibling pairs in a shared structure for a numberof linked rules.
For instance, all occurrences of SUBJECT are pooled together for probability assignmentson their children, regardless of the structural positions of these occurrences within a clause.
The effect ofsuch pooling is essentially a hierarchical bigram model.
We believe this mechanism offers the capability ofgenerating probabilities in a reasonable way by sharing counts on syntactically/semanticaily identical unitsin differing structural environments.Semantic FilteringFor VOYAGER, we were interested in designing a parser that could handle all reasonable ways a personmight request information within the domain, but that would also reject any ill-formed sentences, on thegrounds of both semantic and syntactic anomalies.
Building such a tight grammar not only leads to avery low perplexity for the recognizer, but also virtually eliminates the problem of multiple parses.
This isbecause all parses that are syntactically legitimate but semantically anomalous are weeded out.
It has theadded benefit of improving computation time, if the semantic onstraints are integrated early in the parsingprocess, more or less at the first chance of resolution.We also wanted to maintain our criterion that a node should only have access to information locallyavailable to it by default.
That is, it should not be allowed to hunt back through the parse tree lookingfor a resolution of, for example, number agreement.
By default, all nodes pass along the parameters passedto them by near relatives.
The hard part is to come up with a compact representation that containsall information ecessary to carry out the constraints.
In terms of syntax, there are patterns describingproperties uch as person, number, case, and determiner.
Semantics are represented by patterns that includean automatic hierarchical inheritance of broader properties from more specific ones.
Thus for example thesemantic ategory Restaurant automatically acquires Building and Place as additional semantic features.
Inaddition to the syntactic features, nodes also pass along semantic features that are automatically reset bydesignated nodes, such as terminal vocabulary entries.The two slots, Current-Focus and F loat -0b jec t  that are used for dealing with gaps, turned out toalso be very useful for providing semantic onstraint.
In fact, we decided to take the approach of only usingthese two parameters for semantic filtering, to see whether in fact that would be adequate.
Their use inthe gap mechanism is described elsewhere \[7\], but for clarification we will briefly review it here.
Generatorsare nodes that enter their subparse into the Current-Focus lot.
Activators move the Current-Focus intothe F loat -0b jec t  position, for their children.
Absorbers can accept he F loat-Obj  ect  as their subparse, inplace of something from the input stream.
The net result of this mechanism, aside from its intended use ingap resolution, is that it provides a second-order memory system for identifying the semantic ategories ofcertain key content words in the history.As an example, consider the sentence, "(What street)~ is the Hyatt on (ti)?
?
The Q-Subject places "whatstreet" into the CurzenZ-Focus slot, but this unit is activated to F loat -0b jec t  status by the following Be-Question.
The Subject node refills the now empty Current-Focus with "the Hyatt."
The node A-Street, anabsorber, can accept he F loat -0b j  ect  as a solution, but only if there is tight agreement in semantics, i.e.,it requires the identifier Street.
Thus a sentence such as "(What restaurant)~ is the Hyatt on (t~)" would failon semantic grounds.
Furthermore, the node On-Street imposes emantic restrictions on the Current-Focus.Thus the sentence "(What street)~ is Cambridge on (ts)?"
would fail because On-Street does not permit54Region as the semantic ategory for the Current-Focus, "Cambridge.
"The Current-Focus always contains the subject whenever a verb is proposed, and therefore it is easy tospecify filtering constraints for subject-verb relationships.
Thus for example, the verb "intersect" demandsthat its subject be Street and the verb "eat" demands person.
We have not yet incorporated probabilitiesinto the semantic predictions, mainly because our domain is simple enough that they don't seem necessary.However, in principle probabilities could be added.
Furthermore, these probabilities could be acquiredautomatically b  parsing a collection of sentences and counting semantic o-occurrence patterns.An indicator of how well our semantic restrictions are doing can be obtained by running the sentencegenerator with the semantic filters in place.
Table 1 gives a list of five consecutively generated sentencesfrom the Voyager domain.
For the most part, generated sentences are now well-formed both semanticallyand syntactically.1.
Do you know the most direct route to Broadway Avenue from here?2.
Can I get Chinese cuisine at Legal's?3.
I would like to wMk to the subway stop from any hospital.4.
Locate a T-stop in Inman Square.5.
What kind of restaurant is located around Mount Auburn in Kendall Square of East Cambridge?Table 1: Sample sentences generated consecutively b  the VOYAGER version of TINA.APPL ICAT ION BACK-ENDAfter an utterance has been processed by TINA, it is passed to an interface component which constructsa command function from the natural language parse.
This function is subsequently passed to the back-endwhere a response isgenerated.
In this section, we will describe VOYAGER's current command framework, theinterface between TINA and the back-end, and some of the discourse capabilities of the back-end.Command FrameworkWe will illustrate the current command framework of VOYAGER by way of the simple example shownbelow:Query: Where is the nearest bank to MIT?Function: (LOCATE (NEAREST (BANK n i l )  (SCHOOL "HIT")))LOCATE is an example of a major function that determines the primary action to be performed by thecommand.
It shows the physical location of an object or set of objects on the map.
Table 2 lists some of themajor functions currently implemented in VOYAGER.Functions uch as BANK and SCHOOL in the above example access the database to return an object ora set of objects.
Such functions earch for all entries that match the string pattern provided.
When nullarguments are provided, all possible candidates are returned from the database.
Thus, for example, (SCHOOL"HIT") and (BANK n?l) will return the objects MIT and all known banks, respectively.
Table 3 containssome examples of current data functions.Finally, there are a number of functions in VOYAGER that act as filters, whereby the subset hat fulfillssome requirements are returned.
The function (NEAREST X y), for example, returns the object in the set Xthat is closest o the object y.
Table 4 contains everal examples of filter functions.Note that these functions can be nested, so that they can quite easily construct a complicated object.For example, "the Chinese restaurant on Main Street nearest to the hotel in Harvard Square that is closestto City Hall" would be represented by,55(NEAREST(ON-STREET(SERVE (RESTAURANT n i l )  "Chinese")(STREET "Main .
.
.
.
S t reet" ) )(NEAREST(IN-REGION (HOTEL n i l )  (SQUARE "Harvard"))(PUBLIC-BUILDING "Ci ty  Ha l l " ) ) )LOCATEDESCRIPTIONPROPERTYDISTANCEDIRECTIONSlocate a set of objectsdescribe a set of objectsidentify a property of a set of objectscompute distance between two objectscompute directions between two objectsTable 2: Examples of some major functions in the VOYAGER back-end.STREETADDRESSINTERSECTIONSQUAREreturn a set of streetsreturn a set of addressesreturn a set of intersections of two streetsreturn a set of squaresTable 3: Some examples of data functions in the VOYAGER back-end.AT0N-STREETSERVENEARESTthe subset of objects that are at a locationthe subset of objects that are on a streetthe subset of objects that serve a particular kind of foodthe single object of a set that is nearest o a locationTable 4: Examples of filter functions in the VOYAGER back-end.In ter face  between TINA and  Back-EndOur natural language component does not produce a logical form that is a separate ntity from theparse itself.
Instead, structural roles such as Subject and Dir-Object are an integral part of the parse tree.Furthermore, prepositional phrases are given case-frame-like identities uch as From-Loc and In-Region \[8\].Because of the availability of such semantic labels within the parse tree, the nested command sequencerequired by the back-end can be generated by a recursive procedure operating directly on the parse tree.The parses are transformed to a set of commands in a two-stage process.
The first stage establishes themajor function of the sentence, and the second stage fills in any arguments required by the major function.Each stage uses a list of entries that contain a parse pattern, a back-end function specification, and oneor more argument specifications.
In the first stage, each parse pattern corresponds to a sequence of one ormore nodes in the parse tree and can specify a hierarchical constraint between certain nodes.
Each argumentspecification corresponds to one or more entries in the second-stage list.
In the second stage, a parse pattern56can only be a single node, and each argument specification may be either one or more entries in the second-stage list, a terminal node, or a null value.
Terminal nodes return a string from the parse tree, such as"MIT".
In most cases, each function specification corresponds to a single back-end function or a null value.When present, a function will be called with its associated arguments, uch as (SCHOOL "HIT").
A functionspecification can also designate that the arguments are wrapped around each other.
This mechanism isuseful for generating nested filtering operations.When a parse is passed to the interface component, he patterns in the first-stage list are compared tothe parse tree.
Whenever a match is found, any argument specifications are passed one at a time to thesecond stage for resolution.
If the argument specification is a single node, only the portion of the parse treefound below this node is processed, thus restricting the domain of the second stage analysis.
If there aremultiple entries in an argument specification, the first one found is processed in the same way as a singlenode.In our previous example, the presence of the word "where" in a trace would result in the specification ofLOChTE as the major function.
In this case, the first node found in the associated argument specification is aSubject.
The portion of the parse tree found below Subject is' thus passed to the second stage.
In the secondstage analysis the Subject node would find an A-Place node in the subparse.
Evaluation of this node wouldsubsequently generate two arguments (BhNK n i l )  and (NEhREST (SCHOOL "HIT")) which are wrapped toproduce the desired result.D iscourse Capabi l i t iesThe discourse capabilities of the current VOYAGER system are simplistic but nonetheless effective inhandling the majority of the interactions within the designated task.
Currently, anaphora resolution is dealtwith in the back-end as opposed to the natural anguage component.
We will describe briefly here how adiscourse history is maintained, and how the system keeps track of incomplete requests, querying the userfor more information as needed to fill in ambiguous material.Two slots are reserved for discourse history.
The first slot refers to the location of the user, which canbe set or referred to.
The second slot refers to the most recently referenced set of objects.
This slot can bea single object, a set of objects, or two separate objects in the case where the previous command involved acalculation involving both a source and a destination.
Because of the location slots, user queries can includepronominal reference, such as "What is their address?"
or "How far is it from here?
"VOYAGER can also handle ambiguous queries, in which a function argument has either no value or multiplevalues, when a single value is required.
Examples of ambiguous queries would be "How far is a bank?"
sincethere are several banks, or "How far is MIT?"
when no default location has been specified.
VOYAGERpoints out such ambiguity to the user, by asking for specific clarification.
The ambiguous command is alsopushed onto a stack of incompletely specified commands.
When the user provides additional informationthat is evaluated successfully, the top command in the stack is popped for reevaluation.
If the additionalinformation is not sufficient o resolve the original command, the command is again pushed onto the stack,with the new information incorporated.
In the case where the clarification is also ambiguous, it is pushedonto the stack itself, until it can be clarified.
A protection mechanism automatically clears the history stackwhenever the user decides to abandon that line of discussion before all ambiguous queries are clarified.
Anexample dialogue illustrating clarification capabilities is given in Table 5.DISCUSSIONThis paper summarizes the present status of our spoken language system development effort.
In particu-lar, we focused on those aspects of VOYAGER that have not been reported previously.
The system is clearlyin its infancy, since it has been in existence for only a few months.
In the coming months, however, we hopeto improve its performance and expand its capabilities.Our strategy for system development was to bring up a complete working system as soon as possible, so57User:Voyager:User:Voyager:User:VoyagerUser:Voyager:Can you tell me how to get to the Chinese restaurant?Where are you?I am near a bank.I know of many banks.
Can you specify which one you mean?The one on Main Street.The only bank on Main Street hat I know of is the Baybank at226 Main Street in Cambridge.The Chinese restaurants hat I know of are Hong Kong and Royal East.Can you specify which one you mean?The one nearest me.The nearest Chinese restaurant to the Baybank at 226 Main Streetthat I know of is the Royal East.I will try to find the best way to go from the Baybank at 226 Main Streetto the Royal East.
(- Directions -)Table 5: Example dialogue illustrating clarification capabilities.that we may study component behavior from a system's perspective.
As a result, we devoted most of ourresources to constructing missing components, as opposed to refining existing ones.
Nevertheless, we havebegun to improve phonetic recognition accuracy of SUMMIT by incorporating context-dependent phonememodels.
Preliminary evaluation i dicates that phonetic recognition accuracy is improved by about 5% withthese models on test utterances drawn from TIMIT.
However, we have not incorporated this refinement intoVOYAGEa.
We view the improvement of phonetic recognition accuracy as one of the most critical areas topursue in the near future.In order to provide an early indication of baseline performance, we have recently started to evaluateVOYAGER using the spontaneous speech database that we have collected.
Performance evaluation for spokenlanguage systems is an important issue with which the entire research community is beginning to grapple.In a companion paper, we report our own experience with spoken language system evaluation.Finally, we should again point out that currently the speech recognition and natural language componentsare connected in a serial manner, in which the recognizer proposes a string of words and passes it along forlinguistic analysis.
Such a loose coupling is again a reflection of our desire to bring up a working system at thesacrifice of flexibility and overall performance.
In the long run, we firmly believe that speech recognition andnatural language must be fully integrated.
Experiments in a fully integrated system have been under way,and encouraging results have been obtained.
We hope to be able to report on this and other improvementsin the near future.AcknowledgmentsWe gratefully acknowledge Jim Davis of MIT's Media Laboratory for providing us with the map drawing,path finding, and response generation parts of the VOYAGER back end.References\[1\] Zue, V., Daly, N., Glass, J., Goodine, D., Leung, H., Phillips, M., Polifroni, J., Seneff, S., and Soclof,M., "The Collection and Preliminary Analysis of a Spontaneous Speech Database," These Proceedings.\[2\] Zue, V., Glass, J., Goodine, D., Leung, H., Phillips, M., Polifroni, J., and Seneff, S., "PreliminaryEvaluation of the VOYAGER Spoken Language System," These Proceedings.58\[3\] Davis, J.R. and Trobaugh, T.F., "Directional Assistance," Technical Report 1, MIT Media LaboratorySpeech Group, December 1987.\[4\] Zue, V., Glass, J., Phillips, M., and Seneff, S., "The MIT SUMMIT Speech Recognition System: AProgress Report," Proceedings off the First DARPA Speech and Natural Language Workshop, pp.
178-189, February, 1989.\[5\] Fisher, W., Doddington, G., and Goudie-Marsha\]l, K. "The DARPA Speech Recognition ResearchDatabase: Specification and Status," Proceedings of the DARPA Speech Recognition Workshop, ReportNo.
SAIC-86/1546, February, 1986.\[6\] Weintraub, M., and J. Bernstein, "RULE: A System for Constructing Recognition Lexicons," Proc.DARPA Speech Recognition Workshop, Report No.
SAIC-87/1644, pp.
44-48, February 1987.\[7\] Seneff, S., "TINA: A Probabilistic Syntactic Parser for Speech Understanding Systems," Proceedings ofthe First DARPA Speech and Natural Language Workshop, pp.
168-178, February, 1989.\[8\] Fillmore, C., The Case for Case.
In Universals in Linguistic Theory, E. Bach and R.T. Harms, Eds.,Holt, Rinehart, and Winston, Chicago, pp.
1-90, 1968.59
