Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 632?643, Dublin, Ireland, August 23-29 2014.Interpretation of Chinese Discourse Connectivesfor Explicit Discourse Relation RecognitionHen-Hsen Huang, Tai-Wei Chang, Huan-Yuan Chen, and Hsin-Hsi ChenDepartment of Computer Science and Information EngineeringNational Taiwan UniversityNo.
1, Sec.
4, Roosevelt Road, Taipei, 10617 Taiwan{hhhuang, twchang}@nlg.csie.ntu.edu.tw;{b00902057, hhchen}@ntu.edu.twAbstractThis paper addresses the specific features of Chinese discourse connectives, including types(word-pair and single-word), linking directions (forward and backward linking), positions andambiguous degrees, and discusses how they affect the discourse relation recognition.
A semi-supervised learning method is proposed to learn the probability distributions of discourse func-tions of connectives from a small labeled dataset and a big unlabeled dataset.
The statisticslearned from the dataset demonstrates some interesting linguistic phenomena such as connec-tive synonyms sharing similar distributions, multiple discourse functions of connectives, andcouple-linking elements providing strong clues for discourse relation resolution.1 IntroductionDiscourse relation labeling determines how two discourse units cohere to each other.
A discourse unitmay be a clause, a sentence, or a group of sentences.
The labeled relation has many potential applica-tions.
Coherence is considered as a metric to evaluate the essay writing by essay scorer (Lin et al.,2011).
Discourse relations are used to order sentences in an event in a summarization system (Der-czynski and Gaizauskas, 2013).
Sentiment transition of two clausal arguments is identified based ontheir discourse relation in sentiment analysis (Hutchinson, 2004; Zhou et al., 2011; Wang et al., 2012;Huang et al., 2013).The pioneer research of discourse has been established by Hobbs (1985), Polanyi (1988), Hovy andMaier (1992), and Asher and Lascarides (1995).
Various discourse relation types have been defined inthe frameworks such as Sanders et al.
(1992), Hovy and Maier (1992), RST-DT (Carlson et al., 2002),Wolf and Gibson (2005), and PDTB (Prasad et al., 2008).
Temporal, Contingency, Comparison, andExpansion, the four classes on the top level of PDTB sense hierarchy, are common used in the dis-course relation labeling tasks.
When two arguments are temporally related, they form a Temporal rela-tion.
The Contingency relation talks about the situation that the event in one argument casually affectsthe event in the other argument.
Comparison is used to show the difference between two arguments.The last one relation, Expansion, is the most common.
An Expansion relation either expands the in-formation for one argument in the other one or continues the narrative flow.In the recent years, discourse relation recognition has been studied for different languages (Afan-tenos et al., 2012, Cartoni et al., 2013).
In explicit English discourse relation labeling tasks, the accu-racy of the approach using just the connectives is already quite high, 93.67%, and incorporating thesyntactic features raises performance to 94.15% (Pitler and Nenkova, 2009).
In our previous work, weinvestigate Chinese intra-sentential relation detection and show an accuracy of 81.63% and an F-scoreof 71.11% in the two-way classification (Contingency vs.
Comparison relations) when connectives areThis work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/.632introduced as features (Huang and Chen, 2012a).
We also report an accuracy of 27.10% and an F-score of 24.27% in the four-way inter-sentential relation classification when only connectives are used(Huang and Chen, 2011).
Sporleder and Lascarides (2008) point out some English connectives are of-ten ambiguous between multiple discourse relations or between discourse and non-discourse usage,and Roze et al.
(2010) report the ambiguity of French connectives.
This issue also occurs in Chinese.Zhou et al.
(2012) propose a framework to identify the ambiguous Chinese discourse connectives, andreport an F-score of 74.81% in the four-way classification at the intra-sentential level.The above discourse relation labeling tasks are done on the datasets of different size for differentlanguages at the intra-/inter-sentential levels, thus the results cannot be compared directly.
However,these works show a tendency: discourse connectives are useful clues for explicit discourse relationrecognition, and the uses of Chinese connectives in discourse relation labeling are more challengingthan those of English connectives.
In comparison with English, the connectives in Chinese are moreand their parts of speech are diverse.
There are 100 English explicit connectives annotated in thePDTB 2.0.
In Chinese, the linguists report a list of 808 discourse connectives (Cheng and Tian, 1989;Cheng, 2006).
In addition, the Chinese discourse connectives have a variety of parts-of-speech.
Forexample, ??
(ji?
sh?, suppose) is a verb and listed as a discourse connective of the Contingency rela-tion.The following examples address some specific features of Chinese discourse connectives.
On theone hand, the two words, ????
(su?
r?n, although) and ????
(d?n sh?, but), which form a word-pairconnective, appear in the two discourse units shown in (S1), respectively.
These two units demonstratea Comparison relation.
On the other hand, ????
(su?
r?n, although) and ????
(d?n sh?, but) can ap-pear individually as single-word connectives shown in (S2)-(S6).
The two discourse units have differ-ent discourse relations when the single-word connectives appear at different positions, i.e., (S2): Com-parison, (S3): Comparison, (S4): Expansion, (S5): Comparison, and (S6): Expansion.
Furthermore,the short word ???
(?r) can be an individual connective, which is interpreted as ????
(and), ????
(but), or ????
(thus), and serves as functions of Expansion, Comparison, and Contingency, respec-tively.
In addition, it can be linked with ????
(su?
r?n, although) and ????
(y?n w?i, because) to beword-pair connectives, which are interpreted as Comparison and Contingency functions in (S7) and(S8), respectively.
These examples demonstrate word-pair connectives composed of a same word andother words may have different discourse functions, so does the same single-word connective at dif-ferent positions.
(S1) ????????????????
(Although Tom is smart, he doesn?t study hard.
)(S2) ??????????????
(Although Tom is smart, he doesn?t study hard.
)(S3) ??????????????
(He sweated a lot, although he went only a few miles.
)(S4) ????????????????????
(I'll read, even if I really feel spider terrible.
)(S5) ??????????????
(Tom is smart, but he doesn?t study hard.
)(S6) ?????????????
(But in Paris, he gave up studying medicine.
)(S7) ??????????????
(Although you did not say, I knew that smell.
)(S8) ??????????????
(Because he came home late, he was scolded by his mother.
)In this paper, we investigate special features of Chinese discourse connectives and apply the resultsto discourse relation labeling.
A semi-supervised learning algorithm is proposed to estimate the proba-bility distribution of the discourse functions of each connective.
We address the issue of ambiguitybetween multiple discourse relations of Chinese connectives.
The ambiguity between discourse andnon-discourse usages is not our focus in this paper.
This paper is organized as follows.
Section 2 anal-yses the types of Chinese connectives and their forward/backward linking properties.
Section 3 pre-sents a semi-supervised method to deal with the probability distributions of discourse functions ofChinese connectives and discourse relation labeling.
The experimental results are shown and discussed.In Section 4, we further introduce the discourse relation labeler to annotate 302,293 unlabeled sen-tences and analyze the linguistic phenomena of discourse connectives.
We conclude this work in Sec-tion 5.6332 Types of Discourse ConnectivesFrom the surface form, there are three kinds of linking elements in Chinese (Li and Thompson, 1981):forward-linking elements, backward-linking elements, and couple-linking elements.
Discourse con-nectives are such kinds of linking elements.
A discourse unit containing a forward-linking (backward-linking) element is linked with its next (previous) discourse unit.
A couple-linking element is a pair ofwords that exist in two discourse units (Chen, 1994).Figure 1 shows connectives and their linking direction.
The word-pair connective ???...???(su?
r?n?d?n sh?, although?but) in (S1) is a couple-linking element.
A single-word connective mayfunction as a forward-linking element and/or a backward-linking element.
It may be a word appearingin a word-pair connective, e.g., ????
(su?
r?n, although), or a word existing individually, e.g., ????(y?
j?, and).
A single-word connective which is the first (the second) word of a word-pair connectivemay function as a forward-linking (backward-linking) element.
The single-word connective ????(su?
r?n, although) in (S2) is a typical example.
It keeps the major discourse function, i.e., Comparison,of the word-pair connective that it belongs when it appears in the first discourse unit.
In contrast, itmay become ambiguous when its position is reversed from the first to the second (i.e., S3 and S4).
Itmay link to the previous or the next discourse units.
S5 and S6 have the similar behaviors.
The single-word ????
(d?n sh?, but) in (S5) shows a backward-linking.
In (S6), it is shifted to the first positionand becomes ambiguous.
It may be linked to the previous, or to the next discourse units.
The correctinterpretation depends on the context.
These phenomena show a single-word connective may have dif-ferent senses when it is not at its original position.Figure 1: Examples for forward linkging and backward linking.In this study, we collect 808 discourse connectives based on Cheng and Tian (1989), Cheng (2006),and Lu (2007).
The discourse connective lexicon contains 319 single-word and 489 word-pair connec-tives.
Initially, each connective is associated with only one discourse function manually by linguists.634For example, the word-pair connective, ???...???
(su?
r?n?d?n sh?, although?but), is assigned aComparison function.
The assignment is one-to-one mapping, thus it cannot capture the complete dis-course functions of Chinese connectives.
Table 1 shows an overview of the discourse connective lexi-con.
In this lexicon, Expansion is the majority, and Comparison is the minority.
The percentages ofContingency and Expansion are close.
Temporal is the third largest discourse function.
Intuitively, thediscourse connective lexicon cannot cover all their senses.
To learn the probability distribution of thediscourse functions of a connective needs a large-scale discourse corpus.
Compared with RST-DT(Carlson et al., 2002) and PDTB (Prasad et al., 2008), Chinese discourse corpora are not publiclyavailable (Zhou and Xue, 2012; Huang and Chen, 2012b).Discourse Function Number of Connectives Examples of Single-Word and Word-Pair Discourse ConnectivesTemporal 151 (18.69%) ??
(ji?
zhe, then), ??...??
(zu?
ch?
?xi?n z?i, first...now)Contingency 261 (32.30%) ??
(y?n w?i, because), ?...?
(r?
?z?, if ... then)Comparison 87 (10.77%) ??
(j?
sh?, even if), ????
(j?n gu?n?d?n, although?but)Expansion 309 (38.24%) ??
(l?ng w?i, besides), ?????
(b?
j?n?
?r qi?, not only?but also)Table 1: A Chinese discourse connective lexicon.3 Learning Discourse Functions of ConnectivesThis section proposes a semi-supervised learning method to learn the interpretation of discourse con-nectives from an incomplete and sparse dataset.3.1 A Semi-Supervised Learning AlgorithmGiven a pair of discourse units ds1 and ds2 containing an explicit connective c, a discourse relationclassifier drc aims at selecting a relation r from the set {Temporal, Contingency, Comparison, Expan-sion} to illustrate how ds1 and ds2 cohere to each other.
The connective c may be a word-pair c1?c2,where c1 and c2 appear in ds1 and ds2, respectively.
It may be a single word appearing in ds1 or ds2.Each discourse unit is mapped into a representation.
Various features from different linguistic levelshave been explored in the related work (Huang and Chen, 2011; Huang and Chen, 2012a; Zhou et al,2011; Zhou et al., 2012).
We adopt some of their features shown as follows.
Here we focus in particu-lar on the probability distributions of the discourse functions and the positions of connectives.Length.
This feature includes the word counts of ds1 and ds2.Punctuation.
The punctuation at the end of ds2 is regarded as a feature.
The possible punctuationincludes a full stop, a question mark, or an exclamation mark.
The punctuation at the end of ds1 isdropped from the features because it is always a comma.Words.
The bags of words in ds1 and ds2 are considered.Hypernym.
The bags of hypernyms of the words in ds1 and ds2 are considered.
A Chinese thesau-rus, Tongyici Cilin1, is consulted.
The categorization scheme at the fourth level is adopted.Shared Word.
The number of words shared in ds1 and ds2 is considered as a feature.Collocated Word.
Collocated words are word pairs mined from the training set.
The first and thesecond words of a pair come from ds1 and ds2, respectively.POS.
The bags of parts of speech in ds1 and ds2 are considered.Polarity.
Polarity and discourse relation may be related (Huang et al., 2013; Zhou, et al., 2011).For example, a Comparison relation implies its two discourse units are contrasting, and some contrastsare presented with different polarities.
We estimate the polarity of ds1 and ds2 by a lexicon-based ap-proach.
The polarity score and the existence of negation are taken as features.Discourse Connective.
A discourse connective c is represented as a probability distribution of dis-course functions denoted by a quadruple (P(c,temporal), P(c,contingency), P(c,comparison), P(c,expansion)), whereP(c,temporal), P(c,contingency), P(c,comparison), and P(c,expansion) indicate the probabilities of the four discourse func-tions of c, such that P(c,temporal)+P(c,contingency)+P(c,comparison)+P(c,expansion)=1.
Section 3.3 shows how we as-sign the probabilities to each connective in different experimental settings.Position.
The linguistic phenomena discussed in Section 2 show a single-word connective at dif-ferent position may play different discourse function.
Thus, the position of c is considered as a feature.1 http://ir.hit.edu.cn/635Because the number of Chinese connectives is large (e.g., 808 Chinese connectives in our lexicon)and the large-scale labeled Chinese discourse corpus is not available, how to learn the probability dis-tribution is a challenging issue.
This paper proposes a semi-supervised learning method as follows.
Itspseudo code is shown in Algorithm 1.
(1) Train a 4-way discourse relation classifier drc with the training set and LIBSVM (Chang andLin, 2011).
(2) Initialize probability distributions of unknown connectives in the test set (see experiments).
(3) Use drc to label all the instances in the test set.
(4) Compute the new probability distribution of discourse functions of each connective based onthe labeled results in the current run.
Maximum likelihood estimation is adopted.
(5) Repeat (3) and (4) until the number of label changes between two successive runs is below 1%.Algorithm 1.
Probability Estimation for the Discourse Functions of ConnectivesInput:D={Temporal, Contingency, Comparison, Expansion}: a set of discourse relations and discoursefunctions for argument pairs and discourse connectives,C={c1, c2, ?, cn}: a set of n discourse connectives,S={s1, s2, ?, sp}: a set of p labeled argument-pairs [sa1, sa2] containing connective c?CS?C, eachwith a label d?D, where CS is a set of connectives appearing in S,T={t1, t2, ?, tq}: a set of q unlabeled argument-pairs [ta1, ta2] containing connective c?CT?C, whereCT is a set of connectives appearing in T.Output:Q={q1, q2, ?, qn}: a probability distribution qi for connective ci?C.Method:1.
Initialization1) Train a classifier drc using S.2) Initialize the probability distribution with equal weight, (0.25, 0.25, 0.25, 0.25), for connec-tive c ?
CT-CS, and build Q(0).3) i ?
02.
Relation labelingFor each t ?
T, estimate the probabilities of four discourse relations, P(t,temporal), P(t,contingency),P(t.comparison), and P(t.expansion), using the classifier drc with Q(i).3.
Updating the probability distribution1) For each c ?
C, compute the average probability of each discourse relation among the argu-ment-pairs containing c in T:P(c,tempora)l ?
Average of P(t,temporal) for all t containing c in T.P(c,contingency) ?
Average of P(t,contingency) for all t containing c in T.P(c,comparison) ?
Average of P(t,comparison) for all t containing c in T.P(c,expansion) ?
Average of P(t,expansion) for all t containing c in T.2) Form a new Q(i+1)3) i ?
i+14.
Repeat steps 2-3 until the ratio of the number of label changes by previous and current runs is lessthan 1%.5.
Q ?
Q(i)3.2 Experimental SetupFor the corpus study of discourse connectives and discourse relations, we refer to a public availableChinese Web POS tagged corpus (Yu et al., 2012).
This Chinese POS-tagged corpus is developedbased on the ClueWeb09 dataset (CMU, 2009), where Chinese material is the second largest.
To cap-ture the discourse functions of individual connectives more accurately, the following three criteria areused to sample sentences:1.
A sentence should contain only two clauses.2.
A sentence should contain exact one discourse connective.6363.
The lengths of both clauses in a sentence are no more than 20 Chinese characters.Total 7,601 sentences composed of two discourse units linked by a connective are sampled from apublic available Chinese Web POS tagged corpus (Yu et al., 2012).
Each sentence is annotated with amost likely discourse relation selected from {Comparison, Contingency, Comparison, Expansion} bythree annotators guided by an instruction manual.
The majority is taken as the ground truth.
A mentoris involved to make a final decision for the tie conditions.
The inter-agreement among the annotators is0.41 in Fleiss?
Kappa values, which is a moderate agreement.
The discourse category with the lowestinter-annotation agreement is Temporal, which annotators usually confuse with Expansion.
It showsthe difficulty to distinguish Temporal and Expansion even by human.
Table 2 shows the statistics ofthe corpus.
More than 50% of pairs are annotated with Expansion relation.
The second largest group isContingency relation.
The percentages of Temporal and Comparison relations are near.
Only 359 con-nectives appear in the corpus.
That reflects the incompleteness issue.Discourse Relation # Instances PercentageTemporal 846 11.13%Contingency 1,594 20.97%Comparison 926 12.18%Expansion 4,235 55.72%Table 2: Statistics of the experimental discourse corpus.This Chinese discourse corpus is used for training and testing.
We set up the experiments to simu-late the scenario of estimating the probability distributions of discourse functions of the unknown con-nectives based on the information in the training set.
We evaluate the experimental results by 5-foldcross-validation.
To ensure the discourse connectives appearing in the test set are mutual exclusive ofthose connectives in the training set, we split the discourse connectives into 5 mutual exclusive setsand split all the 7,601 sentences into 5 folds according to the 5 sets of discourse connectives.The kernel of our SVM classifier is the radial basis function.
The two parameters, cost c and gammag, are optimized by the grid-search algorithm within the range c ?
{2-5, 2-3, 2-1, ?, 215} and g ?
{2-15,2-13, 2-11, ?, 23}.3.3 Results and DiscussionsTo demonstrate the performance of our proposed semi-supervised learning methods, the following fivemodels are experimented and compared.M0:  Label the relation between two discourse units linked by a connective c based on the c?s dis-course function defined in the connective lexicon.
M0 is considered as a baseline model.M1: Train a 4-way discourse relation classifier drc with the training set, then initialize the functionprobability distributions of the unknown connectives to (0.25, 0.25, 0.25, 0.25), and finally la-bel all the pairs of discourse units by the classifier drc.
M1 is a supervised-learning method.M2: M2 model is similar to M1 model except that the probability distribution (p(c,temporal), p(c,contingency),p(c,comparison), p(c,expansion)) of an unknown connective is initialized based on its setting in the con-nective lexicon.
The probability of the unique function is set to 1, and the others are set to 0.M3: M3 is a semi-supervised learning method.
In testing, the function probability distributions ofthe unknown connectives are initialized to (0.25, 0.25, 0.25, 0.25).
Discourse relation labelingand probability distribution updating are done iteratively.
Finally, all the test instances are la-beled, and probability distributions of discourse functions are learned for all test connectives.M4: M4 is similar to M3 except that the initial probability distributions are set based on the connec-tive lexicon.Table 3 compares the performances of these five models.
The average tendency isM4>M3>M2>M1>M0.
It shows the proposed two semi-supervised learning methods are significantlybetter than the baseline model M0 and the two supervised-learning methods M1 and M2 at p=0.001.The best model is M4, but the performance differences between M3 and M4 are not significant.
Itdemonstrates that both the two initial assignments, i.e., equal-weight assignment and lexicon-based637assignment, are effective.
If a connective is not listed in the lexicon due to its coverage, we can stillderive its probability distribution starting from the equal-weight approach.We further examine the individual performance of each discourse relation.
Comparing M1 and M3,the semi-supervised classifier (M3) outperforms the supervised classifier (M1) in all three metrics inall the four relations except recall and F-score in the Temporal relation.
Because more than one half ofthe pairs of discourse units annotated with Temporal relation whose discourse connectives have Ex-pansion function in the connective lexicon, some discourse-units of Temporal relation are misclassi-fied as Expansion relation.
That is why the recall is dropped by 8.22% in M3.
The precisions of all thefour relations are increased.
In particular, the precisions of Temporal, Contingency, and Comparisongain more than 10%.
The overall F-score is increased 6.61%.Moreover, M4 is better than M2 in F-score for all the relations.
In particular, the precisions of Tem-poral, Contingency, and Comparison recognition by M4 are greatly increased.
In other words, theboosting algorithm tends to correct those instances that are originally misclassified into the Expansionrelation.
The t-test also confirms M4 has a significant improvement over M2 at p=0.001.The semi-supervised algorithm learns the probability distributions of discourse functions of the un-known connectives from the test instances, so that their size may affect the performance.
Figure 2 ana-lyzes how the number of test instances of a connective affects the performance.
Each point (x, y) inthis figure denote a connective, where x is its total occurrences in the test set, and y is its F-score inFigure 2(a) and its precision/recall in Figure 2(b).
We can find (1) many connectives have good per-formance, (2) connectives containing more test instances demonstrate better performance, and (3)connectives containing fewer instances are sensitive to the evaluation.
We treat the probability distri-bution of discourse functions of each connective as a vector of four real numbers and compute the co-sine similarity among the distributions of connectives derived by the connective lexicon, human anno-tators, and our best model M4.
When the 114 connectives containing more than 10 instances arecounted, the average cosine similarity between our model and human is 0.940, and the average cosinesimilarity between the connective lexicon and human is 0.767.Metric Model Temporal Contingency Comparison Expansion AverageM0 0.3933 0.7124 0.5092 0.7364 0.6656M1 0.5618 0.6005 0.5982 0.7147 0.6595Precision M2 0.5024 0.7038 0.5332 0.7529 0.6879M3 0.6682 0.7652 0.7749 0.7254 0.7334M4 0.6708 0.7773 0.7869 0.7373 0.7344M0 0.3757 0.6014 0.6588 0.7389 0.6600M1 0.5371 0.5098 0.4154 0.8114 0.6694Recall M2 0.4808 0.5808 0.6207 0.7578 0.6731M3 0.4549 0.5387 0.5065 0.9015 0.7276M4 0.4480 0.5803 0.5821 0.8985 0.7299M0 0.3843 0.6522 0.5744 0.7376 0.6606M1 0.5492 0.5515 0.4903 0.7600 0.6644F-score M2 0.4913 0.6364 0.5736 0.7553 0.6805M3 0.5413 0.6323 0.6126 0.8039 0.7305M4 0.5372 0.6645 0.6691 0.8099 0.7322Table 3: Performance comparisons among models.
(a) F-Score                                                      (b) Precision/RecallFigure 2: Effects of the number of test instances for each connective on relation labeling.6384 Further Analyses on a Big DatasetWe further apply the best model (M4) to predict the probability distributions of discourse functions ofconnectives on a big dataset.
For each discourse connective c, up to 500 sentences composed of twodiscourse units linked by c are randomly selected from the Chinese Web POS tagged corpus (Yu et al.,2012).
The limitation of 500 is set to reduce the imbalance among the discourse connectives.
Someconnectives appear quite often in the dataset, e.g., the connective ???
(y?, also).
Some connectivesappear less than 500 times, e.g., ???????
(qi?n w?n?b?
r?n, must...otherwise) occurs only 212times.
Finally, total 302,293 sentences are extracted and predicted.
Because the dataset is very large, itis not easy to evaluate each pair of discourse units.
We examine the linguistic phenomena instead.
Alexicon of the probability distributions of connectives estimated by M4 is available athttp://nlg.csie.ntu.edu.tw/ntu-discourse/.We sort the discourse connectives by the ratios of their largest relations.
In this way, the top connec-tives in this order almost contain one relation.
They can be considered to be less ambiguous.
The topten connectives which appear 500 times are shown in Table 4.
Note the bracket notation [ds1, ds2] de-notes the discourse units where connectives appear.
The discourse function defined in the discourseconnective lexicon specified in Section 2 is marked in bold.
The probabilities of the major discoursefunction of these connectives are larger than 0.89.
The distribution is consistent with the human as-signment except the last connective ???...???
(ch?
f?i...b?
r?n, unless...otherwise), which is as-signed to Contingency in the lexicon.
This connective denotes a negated cause-effect relation betweends1 and ds2 in which ds2 is the effect when ds1 is not satisfied.
In such a case, ds1 and ds2 show clearcontrast, so that it is reasonable to label this connective with a higher probability of the Comparisonrelation.
There are two groups of synonyms in the list: (1) ???...???
(su?
r?n?b?
gu?, alt-hough?but) and ???...???
(su?
r?n?k?
sh?, although?but), and (2) ?????
(ji?n y?n zh?, inshort) and ??????
(ji?n ?r y?n zh?, in short).
Table 4 shows that synonyms share similar distribu-tions.
The cosine similarities of their probability distributions are 0.99996 and 0.99952, respectively.The probability of each discourse function of each connective c is the average of the probabilitiesestimated by the classifier, thus the distributions reported by our model is not completely identical tothe empirical distribution.
For example, all the instances containing the connective ???...???
(su?r?n?b?
gu?, although?but) are labeled with the major discourse function Expansion, but the esti-mated probability of Expansion of this connective is 93.47%.We also sort the discourse connectives by the ratio of their second largest relations.
In this manner,the top connectives in this order may have two major discourse functions.
In other words, they areambiguous.
Table 5 shows the top ten estimated ambiguous discourse connectives.
It is interesting thatExpansion is one of the two major discourse functions, and the other one shown in bold is the dis-course function defined in the connective lexicon.
The discourse connectives ?????
(j?n ji?
zhe,then), ????
(xi?n z?i, now), ????
(w?i l?i, in the future), and ????
(zh?ng y?, finally), whichare defined to have Temporal function in the lexicon, frequently occur in the discourse units with Ex-pansion relation.
The estimated distribution of the connective ???
(?r, and; but; thus) is consistentwith the human interpretation, i.e., it has multiple discourse functions.Chinese single-word connectives are usually put together with other words to form word-pair con-nectives.
Tables 6 and 7 show examples for ????
(su?
r?n, although) and ????
(su?
y?, so),Discourse Connectives [ds1, ds2] Temporal (%) Contingency (%) Comparison (%) Expansion (%)[??
?, ?]
([in short, ?])
2.78 2.08 1.67 93.47[?
?, ??]
([although, but]) 0.77 1.80 92.70 4.74[??
?, ?]
([in other words, ?])
3.63 2.82 1.53 92.02[?
?, ??]
([although, but]) 0.93 2.11 91.58 5.37[?
?, ??]
([since, therefore]) 1.41 91.07 0.97 6.55[??
?, ] ([after all, ?])
3.17 3.95 2.97 89.91[?, ???]
([?, after all]) 3.13 4.34 2.84 89.69[???
?, ] ([in short, ?])
5.07 3.20 2.25 89.48[?
?, ??]
([or, or]) 3.94 4.51 2.16 89.39[?
?, ??]
([unless, otherwise]) 1.04 3.71 89.33 5.93Table 4: Top 10 less-ambiguous connectives estimated by using a big dataset.639Discourse Connectives [ds1, ds2] Temporal (%) Contingency (%) Comparison (%) Expansion (%)[??
?, ?]
([then, ?])
48.71 5.12 1.70 44.46[?, ??]
([?, even though]) 3.93 5.23 46.48 44.36[?
?, ?]
([now, ?])
44.31 7.42 3.42 44.85[?, ??]
([?, although]) 3.60 3.68 44.17 48.55[?
?, ?]
([so that, ?])
3.96 49.83 2.05 44.16[?
?, ??]
([now, in the future]) 47.05 6.41 3.10 43.44[?
?, ?]
([only, then]) 4.34 43.30 9.33 43.03[?
?, ?]
([in the future, ?])
48.21 6.15 2.85 42.79[?, ?]
([?, and; but; thus])  3.72 6.13 42.78 47.37[?, ??]
([?, finally]) 42.39 6.13 2.99 48.49Table 5: Some ambiguous connectives estimated by using a big dataset.respectively.
The former is often connected with a word in the second discourse unit to form a couple-linking, while the latter is connected with a word in the first one.
We can find word-pair connectivesare less ambiguous than single-word connectives in different probabilities.
The former (???
?, su?r?n, although) tends to have Comparison function.
When the word-pair connectives are shorten to sin-gle-word connectives, the probability to have Comparison function becomes lower.
The connective????
(su?
r?n, although) in the first argument still has probability 0.7639 to have Comparison func-tion.
When ????
(su?
r?n, although) is moved to the second argument, the probability to serve asComparison function is decreased to 0.4417, which is even lower than that of Expansion function.
Itshows that couple-linking elements provide strong clue to determine discourse relation.
Besides, a sin-gle-word connective has some tendency to function as either forward linking or backward linking.
Forexample, ????
(su?
r?n, although) is a forward-linking element.
Normally, it will link the first dis-course unit containing it with the second one.
When it appears in the second discourse unit, it becomesambiguous.
The connectives containing ????
(su?
y?, so) have the similar effects.
It tends to be abackward linking element, so its companion appears in the first discourse unit.
Its probability to haveContingency function decreases from a word-pair connective to a single-word connective.
When itappears in the first discourse unit, it may link to the previous sentence at the inter-sentential level.Some Chinese short words like ???
(?r) is often a part of word-pair connectives.
Table 8 shows 10words which are often connected with ???
(?r) to form word-pair connectives.
The word-pair connec-tives tend to have one major function.
When the word-pair connective is ?abbreviated?
to a single-Discourse Connectives [ds1, ds2] Temporal (%) Contingency (%) Comparison (%) Expansion (%)[??,??]
([although, but]) 0.77 1.80 92.70 4.74[??,??]
([although, but]) 0.93 2.11 91.58 5.37[??,??]
([while, however]) 1.04 2.03 90.76 6.17[??,??]
([although, but]) 1.14 2.62 88.49 7.74[??,?]
([although, but]) 1.48 2.89 87.54 8.09[??,?]
([although, still]) 2.70 3.43 85.20 8.68[??,?]
([although, still]) 3.06 4.10 81.03 11.81[??,?]
([although, while]) 2.86 5.09 79.23 12.82[??,??]
([although, still]) 3.68 5.70 77.23 13.39[??,??]
([although, still]) 3.51 8.54 75.26 12.69[??,?]
([although, still]) 4.24 3.71 74.58 17.47[?
?, ?]
([although, ?])
3.46 5.28 76.39 14.87[?, ??]
([?, although]) 3.60 3.68 44.17 48.55Table 6: Effects of single-word and word-pair connectives containing ????
(su?
r?n, although).Discourse Connectives [ds1, ds2] Temporal (%) Contingency (%) Comparison (%) Expansion (%)[?
?, ??]
([because, so]) 1.64 85.25 1.77 11.35[?, ??]
([because, so]) 2.26 83.20 1.82 12.72[?
?, ??]
([because, so]) 2.69 78.03 2.35 16.93[?
?, ??]
([since, so]) 1.68 67.32 6.37 24.63[?, ??]
([?, so]) 2.82 50.67 5.29 41.22[?
?, ?]
([so, ?])
5.71 50.61 2.50 41.18Table 7: Effects of single-word and word-pair connectives containing ????
(so).640word connective, it becomes ambiguous.
The discourse function depends on which word-pair connec-tive it is mapped.
The determination relies on contextual information.Table 9 further shows the effects of positions of single-word connectives.
The major discourse func-tion of the first 7 sets of connectives is changed when the connectives are shifted from the first dis-course unit to the second one.
In contrast, the last 3 sets of connectives keep their major discoursefunction no matter whether they are placed in the first or the second discourse unit.
The only differ-ence is the probability to serve as the major discourse function is changed.
For example, the probabil-ity of the connective ?????
(zh?
b?
gu?, only; just; merely) to have Comparison function is in-creased from 0.6920 to 0.8501 when it is shifted from the first discourse unit to the second one.Discourse Connectives [ds1, ds2] Temporal (%) Contingency (%) Comparison (%) Expansion (%)[?
?, ?]
([not only, but]) 2.19 4.13 4.92 88.76[?
?, ?]
([not only, but]) 2.41 4.56 10.13 82.89[?
?, ?]
([not only, but]) 3.20 5.14 10.55 81.11[?
?, ?]
([since, but]) 3.99 13.87 13.42 68.72[?
?, ?]
([of course, while]) 1.16 2.76 80.82 15.24[?
?, ?]
([although, while]) 2.86 5.09 79.23 12.82[?
?, ?]
([although, while]) 2.76 43.61 79.16 13.71[?
?, ?]
([because, so]) 2.02 79.01 2.16 16.81[?, ?]
([because, so]) 3.21 71.03 2.28 23.49[?
?, ?]
([because, so]) 3.11 49.12 7.52 40.26[?, ?]
([?, and; but; thus]) 3.71 6.13 42.78 47.37[?, ?]
([and; but; thus, ?])
5.47 8.55 17.00 68.98Table 8: Effects of single-word and word-pair connectives containing ???
(and, but, so).Discourse Connectives [ds1, ds2] Temporal (%) Contingency (%) Comparison (%) Expansion (%)[?
?, ?]
([therefore, ?])
6.26 64.30 1.66 27.77[?, ??]
([?, therefore]) 3.54 28.32 5.15 62.99[?
?, ?]
([as long as, ?])
2.68 66.02 5.33 25.98[?, ??]
([?, as long as]) 2.57 5.49 4.23 87.71[?
?, ?]
([if, ?])
3.51 57.15 7.47 31.87[?, ??]
([?, if]) 3.31 5.21 5.33 86.16[?
?, ?]
([however, ?])
8.17 9.20 23.12 59.51[?, ??]
([?, however]) 2.26 2.39 80.97 14.38[?
?, ?]
([but, ?])
8.56 7.72 20.87 62.86[?, ??]
([?, but]) 2.32 2.90 75.76 19.02[?
?, ?]
([even though, ?])
3.55 5.04 75.65 15.75[?, ??]
([?, even though]) 3.93 5.23 46.48 44.36[?
?, ?]
([now, ?])
44.31 7.42 3.42 44.85[?, ??]
([?, now]) 8.03 2.88 3.60 85.49[?, ?]
([and, ?])
7.14 8.43 3.14 81.29[?, ?]
([?, and]) 4.62 3.79 2.38 89.22[?
?, ?]
([as well as, ?])
4.83 9.88 2.69 82.60[?, ??]
([?, as well as]) 4.20 4.29 2.33 89.18[??
?, ?]
([merely, ?])
3.54 4.76 69.20 22.50[?, ???]
([?, merely]) 1.48 2.00 85.01 11.50Table 9: Effects of positions of single-word connectives.5 ConclusionIn this paper, we address the issue of the ambiguous discourse functions of Chinese connectives indiscourse relation labeling and propose a semi-supervised learning method to estimate the probabilitydistribution of discourse functions of connectives.
We examine the constructions of Chinese connec-tives and their effects on the discourse relation recognition.
The proposed approach learns the proba-bility distributions of discourse functions of Chinese connectives from a small labeled dataset and abig unlabeled dataset.
The results reflect many interesting linguistic phenomena.
We compare the am-biguity degrees of single-word and word-pair connectives, and show the effects of the positions of sin-gle-word connectives on the discourse functions.
The discourse relation recognizer integrating the641probability distributions and contextual information significantly outperforms the approaches withoutthe knowledge.This methodology can be extended to estimate the probability distribution of discourse functions ofconnectives on much finer relation categories.
In the current experiments, we focus on explicit dis-course relation recognition.
The 302,293 labeled sentences in Section 4 can be regarded as a trainingcorpus for implicit discourse relation recognition.
Those labeled sentences composed of unambiguousconnectives will be sampled from the reference corpus for training an implicit discourse relationrecognition system.
Furthermore, how to employ the learned probability distributions to deal with dis-course units containing multiple connectives will be investigated.
In the future, we will tell out the dis-course connective and non-discourse connective uses of words and explore their interpretations on thediscourse relation recognition.
Besides, we will make use of the probability distributions to the relationlabeling on more than two clauses and further extend the methodology to experiments at the inter-sentence level.AcknowledgementsThis research was partially supported by Ministry of Science and Technology, Taiwan, under thegrants 101-2221-E-002-195-MY3 and 102-2221-E-002-103-MY3, and 2012 Google Research Award.We are also very thankful to the anonymous reviewers for their helpful comments to revise this paper.ReferencesStergos Afantenos, Nicholas Asher, Farah Benamara, Myriam Bras, C?cile Fabre, Mai Ho-dac, Anne Le Dra-oulec, Philippe Muller, Marie-Paule P?ry-Woodley, Laurent Pr?vot, Josette Rebeyrolle, Ludovic Tanguy, Ma-rianne Vergez-Couret, and Laure Vieu.
2012.
An Empirical Resource for Discovering Cognitive Principles ofDiscourse Organisation: the ANNODIS Corpus.
In Proceedings of the18th International Conference on Lan-guage Resources and Evaluation (LREC 2012), pages 2727-2734, Istanbul, Turkey.Nicholas Asher and Alex Lascarides.
1995.
Lexical Disambiguation in a Discourse Context.
Journal of Seman-tics, 12(1):69-108, Oxford University Press.Lynn Carlson, Daniel Marcu, and Mary E. Okurowski.
2002.
RST Discourse Treebank.
Linguistic Data Consor-tium, Philadelphia.Bruno Cartoni, Sandrine Zufferey, and Thomas Meyer.
2013.
Annotating the Meaning of Discourse Connectivesby Looking at their Translation: The Translation Spotting Technique.
Dialogue and Discourse, 4(2):65-86.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIBSVM: A Library for Support Vector Machines.
ACM Transac-tions on Intelligent Systems and Technology, 2:27:1-27:27.Hsin-Hsi Chen.
1994.
The Contextual Analysis of Chinese Sentences with Punctuation Marks.
Literal and Lin-guistic Computing, 9(4):281-289.Shou-Yi Cheng.
2006.
Corpus-Based Coherence Relation Tagging in Chinese Discourse.
Master Thesis, Na-tional Chiao Tung University, Hsinchu, Taiwan.Xianghui Cheng and Xiaolin Tian.
1989.
Xian dai Han yu (????
), San lian shu dian (????
), HongKong.CMU 2009.
ClueWeb09, http://lemurproject.org/clue-web09.php/Leon Derczynski and Robert Gaizauskas.
2013.
Temporal Signals Help Label Temporal Relations.
In Proceed-ings of the 51st Annual Meeting of the Association for Computational Linguistics, Volume 2: Short Papers,pages 645-650, Sofia, Bulgaria.Jerry R. Hobbs.
1985.
On the Coherence and Structure of Discourse, Report No.
CSLI-85-37, Center for theStudy of Language and Information, Stanford University.
http://www.isi.edu/~hobbs/ocsd.pdfEduard H. Hovy and Elisabeth Maier.
1992.
Parsimonious or Profligate: How Many and Which Discourse Struc-ture Relations?
No.
ISI/RR-93-373.
Information Sciences Institute, University of Southern California, Marinadel Rey.Hen-Hsen Huang and Hsin-Hsi Chen.
2011.
Chinese Discourse Relation Recognition.
In Proceedings of the 5thInternational Joint Conference on Natural Language Processing (IJCNLP 2011), pages 1442-1446, ChiangMai, Thailand.642Hen-Hsen Huang and Hsin-Hsi Chen.
2012a.
Contingency and Comparison Relation Labeling and StructurePrediction in Chinese Sentences.
In Proceedings of the 13th Annual Meeting of the Special Interest Group onDiscourse and Dialogue (SIGDIAL 2012), pages 261-269, Seoul, South Korea.Hen-Hsen Huang and Hsin-Hsi Chen.
2012b.
An Annotation System for Development of Chinese DiscourseCorpus.
In Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012)Demonstration Papers, pages 223-230, Mumbai, India.Hen-Hsen Huang, Chi-Hsin Yu, Tai-Wei Chang, Cong-Kai Lin, and Hsin-Hsi Chen.
2013.
Analyses of the As-sociation between Discourse Relation and Sentiment Polarity with a Chinese Human-Annotated Corpus.
InProceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 70-78, So-fia, Bulgaria.Ben Hutchinson.
2004.
Acquiring the Meaning of Discourse Markers.
In Proceedings of the 42nd Annual Meet-ing of the Association for Computational Linguistics (ACL 2004), pages 684-691, Barcelona, Spain.Charles N. Li, Sandra A. Thompson.
1981.
Mandarin Chinese: A Functional Reference Grammar.
University ofCalifornia Press.Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011.
Automatically Evaluating Text Coherence Using DiscourseRelations.
In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL2011), pages 997-1006, Portland, Oregon, USA.Shuxiang Lu.
2007.
Eight Hundred Words of The Contemporary Chinese (Xian dai Han yu Ba bai Ci).
ChinaSocial Sciences Press.Emily Pitler and Ani Nenkova.
2009.
Using Syntax to Disambiguate Explicit Discourse Connectives in Text.
InProceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 13-16, Suntec, Singapore.Livia Polanyi.
1988.
A Formal Model of the Structure of Discourse.
Journal of Pragmatics, 12(5-6):601-638.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber.2008.
The Penn Discourse TreeBank 2.0.
In Proceedings of the 6th Language Resources and Evaluation Con-ference (LREC 2008), pages 2961-2968, Marrakech, Morocco.Charlotte Roze, Laurence Danlos, and Philippe Muller.
2010.
LEXCONN: a French Lexicon of Discourse Con-nectives.
In Proceedings of the 8th International Workshop on Multidisciplinary Approaches to Discourse(MAD 2010), Moissac.Ted J. M. Sanders, Wilbert P. M. Spooren, and Leo G. M. Noordman.
1992.
Toward a Taxonomy of CoherenceRelations.
Discourse Processes, 15(1):1-35.Caroline Sporleder and Alex Lascarides.
2008.
Using Automatically Labelled Examples to Classify RhetoricalRelations: A Critical Assessment.
Natural Language Engineering, 14(3):369-416, Cambridge UniversityPress.Fei Wang, Yunfang Wu, and Likun Qiu.
2012.
Exploiting Discourse Relations for Sentiment Analysis.
In Pro-ceedings of the 24th International Conference on Computational Linguistics (COLING 2012), Posters, pages1311-1320, Mumbai, India.Florian Wolf and Edward Gibson.
2005.
Representing Discourse Coherence: A Corpus-Based Study.
Computa-tional Linguistics, 31(2):249-287.Chi-Hsin Yu, Yi-jie Tang and Hsin-Hsi Chen.
2012.
Development of a Web-scale Chinese Word N-gram Corpuswith Parts of Speech Information.
In Proceedings the 8th International Conference on Language Resourcesand Evaluation (LREC 2012), pages 320-324, Istanbul, Turkey.Lanjun Zhou, Binyang Li, Wei Gao, Zhongyu Wei and Kam-Fai Wong.
2011.
Unsupervised Discovery of Dis-course Relations for Eliminating Intra-sentence Polarity Ambiguities.
In Proceedings of Conference on Em-pirical Methods in Natural Language Processing (EMNLP 2011), pages 162-171, Edinburgh, UK.Lanjun Zhou, Wei Gao, Binyang Li, Zhongyu Wei and Kam-Fai Wong.
2012.
Cross-lingual Identification ofAmbiguous Discourse Connectives for Resource-Poor Language.
In Proceedings of the 24th InternationalConference on Computational Linguistics (COLING 2012), pages 1409-1418, Mumbai, India.Yuping Zhou and Nianwen Xue.
2012.
PDTB-style Discourse Annotation of Chinese Text.
In Proceedings of the50th Annual Meeting of the Association for Computational Linguistics (ACL 2012), pages 67-77, Jeju Island,Korea.643
