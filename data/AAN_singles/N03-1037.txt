A Web-Trained Extraction Summarization SystemLiang Zhou and Eduard HovyUSC Information Sciences Institute4676 Admiralty WayMarina del Rey, CA 90292-6695{liangz, hovy}@isi.eduAbstractA serious bottleneck in the development oftrainable text summarization systems is theshortage of training data.
Constructing suchdata is a very tedious task, especially becausethere are in general many different correctways to summarize a text.
Fortunately we canutilize the Internet as a source of suitabletraining data.
In this paper, we present asummarization system that uses the web as thesource of training data.
The procedure involvesstructuring the articles downloaded fromvarious websites, building adequate corpora of(summary, text) and (extract, text) pairs,training on positive and negative data, andautomatically learning to perform the task ofextraction-based summarization at a levelcomparable to the best DUC systems.1    IntroductionThe task of an extraction-based text summarizer is toselect from a text the most important sentences that arein size a small percentage of the original text yet still asinformative as the full text (Kupiec et al, 1995).Typically, trainable summarization systems characterizeeach sentence according to a set of predefined featuresand then learn from training material which featurecombinations are indicative of good extract sentences.In order to learn the characteristics of indicativesummarizing sentences, a large enough collection of(summary, text) pairs must be provided to the system.Research in automated text summarization isconstantly troubled by the difficulty of finding orconstructing large collections of (extract, text) pairs.Usually, (abstract, text) pairs are available and can beeasily obtained (though not in sufficient quantity tosupport fully automated learning for large domains).
Butabstract sentences are not identical to summarysentences and hence make direct comparison difficult.Therefore, some algorithms have been introduced togenerate (extract, text) pairs expanded from (abstract,text) inputs (Marcu, 1999).The explosion of the World Wide Web has madeaccessible billions of documents and newspaper articles.If one could automatically find short forms of longerdocuments, one could build large training sets overtime.
However, one cannot today retrieve short and longtexts on the same topic directly.News published on the Internet is an exception.Although it is not ideally organized, the topicorientation and temporal nature of news makes itpossible to impose an organization and thereby obtain atraining corpus on the same topic.
We hypothesize thatweekly articles are sophisticated summaries of dailyones, and monthly articles are summaries of weeklyones, as shown in Figure 1.
Under this hypothesis, howaccurate an extract summarizer can one train?
In thispaper we first describe the corpus reorganization, thenin Section 3 the training data formulation and thesystem, the system evaluation in Section 4, and finallyfuture work in Section 5.dailyweeklymonthlyFigure 1.
Corpus structure.Edmonton, May-June 2003Main Papers , pp.
205-211Proceedings of HLT-NAACL 20032    Corpus Construct2.1 Download Initial CollectionThe Yahoo Full Coverage Collection (YFCC) wasdownloaded from http://fullcoverage.yahoo.com duringDecember 2001.
The full coverage texts weredownloaded based on a snapshot of the linksin Yahoo Full Coverage at that time.
A spidthe top eight categories: U.S., World, Business,Technolo tertainment,Sports.
A y were saved iindex page that contained the headline and its full textURL.
A page fetcher then nloaded all the paglisted in the snapshot indexUnder the eight casubcategories, 216590 new2.2 PreprocessingAll the articles in thefollowing.
Each article is iwith actual contents buriedand markings.
Identifying tprocess (Finn et al, 2001main body of the articletemplates, and then finformation embedded in theach opening and closing taname indicates the contenclosing tags are images or jdiscarded.The clean texts are thebreaker, Lovin?s stemmer,converted into standard XMorganizationThe news articles posted under Yahoo Full Coverageare from 125 different web publishers.
Except for somewell-known sites, the publishing frequencies for the restof the sites are not known.
But Yahoo tends to use thosepublishers over and over again, leaving for eachr a tra ublishing habit.
Our system recordsishing or each article from each publisherchronologically, and then calculates the publishingfrequency for each publisher.
Over all the articles fromblisher,  comp the minimumpublishing gap (MPG) between two articles.
If the MPGis less than 3 days or the MPG is unknown in the case ofpublishers seen  once in the YFCC, then this3           6        7  dowfile.tegories, there are 46s articles.YFCC are preprocessedn the original raw html forin layers of irrelevant tahe text body is a challengin).
The system identifies tusing a set of retrievurther eliminates uselee main body by considering set.
For example, if the tts between the opening anust meta-info, the contentsn processed by a sentena part-of-speech tagger, anL form.A19998     10 11       2Figure 2: The hierarchicaes 3asmgsghealssgagdiscedpublisher is labegreater than 3 dweekly publisheare labeled as moFor each articit as a daily, wdomain under erestructured intoyear, weeks ofweek.
The vishierarchical strucategory World i3 SystemRecognizingfrom the web reconstant shortagtaking a closer eone notices that ffrica20002    3    ?
13            4 WDayArticlesl structure for domain onlygy, Science, Health, Enll news links in each categorandn an a pu the systemled as a daily pubays but less thanr.
Publishers withnthly publishers.le in the collectioeekly, or monthach category in ta hierarchy by yeeach month, andualization of acture of the ds shown in Figure(summary, text)pository is the kee of summarizatioxamination of thor each day, there20012    3    ?eekAfrica.
utes  containeder crawledpublishethe publil of pdate fion 2.3 Chronological Relisher.
If the MPG is15, it is labeled as aall other MPG valuesn, the system relabelsly publication.
Eachhe collection is thenar, months within thefinally days of eachn example of theomain Africa under2.pairs automaticallyy to overcoming then training data.
Aftere reorganized YFCC,are a number ofYearMontharticles published that update the progress of a particularnews topic.
Daily articles are published by identifieddaily publishers.
Then at the end of each week, there areseveral weekly articles published by weekly publisherson the same topic.
At the end of each month, again thereare articles on the same topic posted by publisherslabeled as monthly publishers.
There is a commonthematic connection between the daily articles and theweekly articles, and between the weekly articles and themonthly articles.
The daily articles on a particular eventare more detailed, and are written step-by-step as it washappening.
The weekly articles review the daily articlesand recite important snippets from the daily news.
Themonthly articles are written in a more condensedfashion quoting from the weeklies.Instead of asking human judges to identifyinformative sentences in documents, and sincebeautifully written ?summaries?
are already available,we need to align the sentences from the daily articleswith each weekly article sentence, and align weeklyarticle sentences with each monthly article sentence, inorder to collect the (summary, text) pairs and eventuallygenerate (extract, text) pairs.
The pairs are constructedat both sentence and document levels.3.1 AlignmentIn our system, three methods for sentence-level anddocument-level alignment are investigated:?
extraction-based: Marcu (1999) introduces analgorithm that produces corresponding extractsgiven (abstract, text) tuples with maximalsemantic similarity.
We duplicated this algorithmbut replaced inputs with (summary, text), parentsand their respective children in the hierarchicaldomain tree.
Thus for example, the summary is amonthly article when the text is a weekly article ora weekly article when the text is a daily one.
Westart with the cosine-similarity metric stated in(Marcu 1999) and keep deleting sentences that arenot related to the summary document until anymore deletion would result in a drop in similaritywith the summary.
The resulting set of sentences isthe extract concerning the topic discussed in thesummary.
It forms the pair (extract, text).
If thereis more than one summary for a particular text(nonsummary article), the resulting extracts willvary if the summary articles are written on thesame event, but are focused on differentperspectives.
Thus, a summary article may bealigned with several extracts and extractsgenerated from a single text may align with manysummaries.
The relationship amongst summaries,extracts, and texts forms a network topology.To generate sentence level alignment, wereplaced the input with (summary sentence, text)pairs.
Starting  with  a  nonsummary text,the sentences that are irrelevant to the summarysentence are deleted repeatedly, resulting in thepreservation of sentences similar in meaning to thesummary sentence.
For each sentence in thesummary, it is aligned with a number ofnonsummary sentences to form (summarysentence, nonsummary sentences) pairs.
Thisalignment is done for each sentence of thesummary articles.
Finally for each nonsummarywe group together all the aligned sentences to formthe pair (extract, text).?
similarity-based: inspired by sentence alignmentfor multilingual parallel corpora in MachineTranslation (Church, 1993; Fung and Church,1994; Melamed, 1999), we view the alignmentbetween sentences from summaries and sentencesfrom nonsummaries as the alignment ofmonolingual parallel texts at the sentence level.
Inevery domain of the YFCC, each article isrepresented as a vector in a vector space whereeach dimension is a distinct non-stop wordappearing in this domain.
Measuring the cosine-similarity between two articles, we can decidewhether they are close semantically.
This methodhas been widely used in Information Retrieval(Salton, 1975).
To extend this idea, we measurethe cosine-similarity between two sentences, onefrom a summary (weekly or monthly article) andthe other one from a nonsummary (daily or weeklyarticle).
If the similarity score between the twocrosses a predetermined threshold, the twosentences are aligned to form the pair (summarysentence, text sentence).
The relationship betweensentences is many-to-many.
With any particularnonsummary article, sentences that are alignedwith summary sentences form the extract and thepair (extract, text).?
summary-based: concerned with the noise thatmay accompany similarity calculations fromextraction-based and similarity-based alignments,we align an entire summary article with all itsnonsummary articles published in the same timeperiod, as determined from the previouslydescribed chronological reorganization.
Thealignment results are pairs of the format (summary,texts).
One summary can only be aligned with acertain group of nonsummaries.
Each nonsummarycan be aligned with many summaries.
No sentencelevel alignment is done with this method.3.2 Training DataThe main goal of a leaning-based extractionsummarization system is to learn the ability to judgewhether a particular sentence in a text appear in theextract or not.
Therefore, two sets of training data areneeded, one indicative enough for the system to select asentence to be in the extract (labeled as positive data),the other indicative enough for the system to keep thesentence from being added to the extract (labeled asnegative data).
For each of the alignment methods, weproduce summary training data and nonsummarytraining data for each domain in the YFCC.From extraction-based and similarity-based alignmentmethods, for each nonsummary article, there are twosets of sentences, the set of sentences that compose theextract with the respect to some summary article oralign with summary sentences, and the rest of thesentences that are not related to the summary or aligned.The two sets of sentences over all articles in the domainform the positive and negative training data sets.Using summary-based alignment, all the summaryarticles are in the positive training set, and all thenonsummary material is in the negative set.
Full textsare used.3.3 Bigram Estimates Extract DesirabilityWe treat each domain independently.
Using a bigrammodel, we estimate the desirability of a sentenceappearing in the extract P(S) from the summary trainingdata as:P(S) = P(w1 | start) P(w2 | w1)?P(wn | wn-1)We estimate the desirability of a sentence notappearing in the extract P?
(S) from the nonsummarytraining data as:P?
(S) = P?
(w1 | start) P?
(w2 | w1)?P?
(wn | wn-1)For each domain in the YFCC, a summary bigramtable and a nonsummary bigram table are created.3.4 Extraction ProcessZajic et al (2002) used a Hidden Markov Model aspart of their headline generation system.
In our system,we started with a similar idea of a lattice for summaryextraction.
In Figure 3, E states emit sentences that aregoing to be in the extract, and N states emit all othersentences.
Given an input sentence, if P(S) is greaterthan P?
(S), it means that the sentence has a higherdesirability of being an extraction sentence; otherwise,the sentence will not be included in the resulting extract.After reading in the last sentence from the input, theextract is created by traversing the path from start stateto end state and only outputting the sentences emittedby the E states.The extracts generated are in size shorter than theoriginal texts.
However, the number of sentences that Estates emit cannot be predetermined.
This results inunpredictable extract length.
Most frequently, longerextracts are produced.
The system needs more controlover how long extracts will be in order for meaningfulevaluation to be conducted.To follow up on the lattice idea, we used thefollowing scoring mechanism:R = P(S) / P?
(S)R indicates the desirability ratio of the sentence being inthe extract over it being left out.
For each sentence fromthe input, it is assigned an R score.
Then all thesentences with their R scores are sorted in descendingorder.
With respect to the length restriction, we chooseonly the top n R-scored sentences.3.5 Selecting the Training DomainThere are 463 domains under the 8 categories ofYFCC, meaning 463 paired summary-bigram andnonsummary-bigram tables.
On average for eachdomain, the summary-bigram table contains 20000entries; the nonsummary-bigram table contains 173000entries.
When an unknown text or a set of unknowntexts come in to be summarized, the system needs toselect the most appropriate pair of bigram tables tocreate the extract.
The most desirable domain for anunknown text or texts contains articles focusing on thesame issues as the unknown ones.
Two methods areused:?
topic signature (Lin and Hovy, 2000): a topicsignature is a family of related terms {topic,signature}, where topic is the target concept andsignature is a vecto  related ms.
The topic ine formula is assigned with the domain ame.
Tonstruct the set of related words, w considerN2 N3 N1esE2 E3 E1Figure 3.
Lattice.
thcoonly nou becausemajor issues discusthose issues evolver of are onsed in the dd.
Each n terinterested in the ns  we lyomain, noun in th ne ot in howe domainreceives a tf.idf score.
30 top-scoring nouns areselected to be the signature representing thedomain.
For each test text, its signature iscomputed with the same tf.idf method against eachdomain.
The domain that has the highest numberof overlaps in signature words is selected and itsbigram tables are used to construct the extract ofthe test text.
The following table illustrates.
Inputsare three sets of 10 documents each from theDUC01 training corpus concerning the topics onAfrica, earthquake, and Iraq, respectively.
Thescores are the total overlaps between a domain andeach individual test set.
The Three sets are allcorrectly classified.domain/input Africa Earthquake IraqAfrica 24 10 9Earthquake 7 20 8Iraq 48 8 97?
hierarchical signature: each domain is given aname when it was downloaded.
The name gives adescription of the domain at the highest level.Since the name is the most informative word, if wegather the words that most frequently co-occurwithin the sentence(s) that contain the name itself,a list of less informative but still important wordscan become part of the domain signature.
Usingthis list of words, we find another list of words thatmost frequently co-occur with each of themindividually.
Therefore, a three-layer hierarchicaldomain signature can be created: level one, thedomain name; level two, 10 words with the highestco-occurrences with the domain name; level three,10 words that most frequently co-occur with leveltwo signatures.
Again only nouns are considered.For example, for domain on Iraq, the level onesignature is ?Iraq?
; level two signatures are?Saddam?, ?sanction?, ?weapon?, ?Baghdad?, andetc.
; third level signatures are ?Gulf?, ?UN?,?Arab?, ?security?, etc.
The document signaturefor the test text is computed the same way as in thetopic signature method.
Overlap between thedomain signature and the document signature iscomputed with a different scoring system, inwhich the weights are chosen by hand.
If level oneis matched, add 10 points; for each match at leveltwo, add 2 points; for each match at level three,add 1 point.
The domain that receives the highestpoints will be selected.
A much deeper signaturehierarchy can be created recursively.
Throughexperiment, we see that a three-level signaturesuffices.
The following table shows the effects ofthis method:domain/input Africa Earthquake IraqAfrica 86 7 41Earthquake 7 74 0Iraq 15 26 202Since it worked well for our test domains, weemployed the topic-signature method in selectingtraining domains.4 Evaluation4.1 Alignment ChoiceTo determine which of the alignment methods ofSection 3.1 is best, we need true summaries, notmonthly or weekly articles from the web.
We tested theequivalencies of the three methods on three sets ofarticles from the DUC01 training corpus, whichincludes human-generated ?gold standard?
summaries.They are on the topics of Africa, earthquake, and Iraq.The following table shows the results of thisexperiment.
Each entry demonstrates the cosinesimilarity, using the tf.idf score, of the extractsgenerated by the system using training data created fromthe alignment method in the column, compare to thesummaries generated by human.extraction similarity summaryAfrica 0.273 0.304 0.293Earthquake 0.318 0.332 0.342Iraq 0.234 0.246 0.247We see that all three methods produce roughly equalextracts, when compared with the gold standardsummaries.
The summary-based alignment method isthe least time consuming and the most straightforwardmethod to use in practice.4.2 System PerformanceThere are 30 directories in the DUC01 testing corpus.All articles in each directory are used to make theselection of its corresponding training domain, asdescribed in Section 3.5.
Even if no domain completelycovers the event, the best one is selected by the system.To evaluate system performance on summarycreation, we randomly selected one article from eachdirectory from the DUC01 testing corpus, for eacharticle, there are three human produced summaries.
Oursystem summarizes each article three times with thelength restriction respectively set to the lengths of thethree human summaries.
We also evaluated the DUC01single-document summarization baseline system results(first 100 words from each document) to set a lowerbound.
To see the upper bound, each human generatedsummary is judged against the other two humansummaries on the same article.
DUC01 top performer,system from SMU, in single-document summarization,was also evaluated.
In all, 30 * 3!
human summaryjudgments, 30 * 3 baseline summary judgments, 30SMU system judgments, and 30 * 3 system summaryjudgments are made.
The following table is theevaluation results using the SEE system version 1.0 (Lin2002), with visualization in Figure 4.
Summary modelunits are graded as full, partial, or none in completenessin coverage with the peer model units.
And Figure 5shows an example of the comparison between thehuman-created summary and the system-generatedextract.SRECALL SPRECISON LRECALL LPRECISIONBaseline 0.246 0.306 0.301 0.396System 0.452 0.341 0.577 0.509SMU 0.499 0.482 0.583 0.672Human 0.542 0.500 0.611 0.585Performance Evaluation00.10.20.30.40.50.60.70.8SRECALL SPRECI LRECALL LPRECIbaselinesystemSMUhumanThe performance is reported on four metrics.
Recallmeasures how well a summarizer retains originalcontent.
Precision measures how well a systemgenerates summaries.
SRECALL and SPRECISION arethe strict recall and strict precision that take intoconsideration only units with full completeness in unitcoverage.
LRECALL and LPRECISION are the lenientrecall and lenient precision that count units with partialand full completeness in unit coverage.
Extractsummaries that are produced by our system hascomparable performance in recall with SMU, meaningthat the coverage of important information   is   good.But    our   system   shows weakness in precision due tothe fact that each sentence in the system-generatedextract is not compressed in any way.
Each sentence inthe extract has high coverage over the human summary.But sentences that have no value have also beenincluded in the result.
This causes long extracts onaverage, hence, the low average in precision measure.Since our sentence ranking mechanism is based ondesirability, sentences at the end of the extract are lessdesirable and can be removed.
This needs furtherinvestigation.
Clearly there is the need to reduce the sizeof the generated summaries.
In order to produce simpleand concise extracts, sentence compression needs to beperformed (Knight and Marcu, 2000).Despite the problems, however, our system?sperformance places it at equal level to the top-scoringsystems in DUC01.
Now that the DUC02 material isalso available, we will compare our results to their top-scoring system as well.4.3 ConclusionOne important stage in developing a learning-basedextraction summarization system is to find sufficientand relevant collections of (extract, text) pairs.
This taskis also the most difficult one since resources ofconstructing the pairs are scarce.
To solve thisbottleneck, one wonders whether the  web can be seenas a vast repository that is waiting to be tailored in orderto fulfill our quest in finding summarization trainingdata.
We have discovered a way to find short forms oflonger documents and have built an extraction-basedsummarizer learning from reorganizing news articlesfrom the World Wide Web and performing at a levelcomparable to DUC01 systems.
We are excited aboutthe power of how reorganization of the web newsarticles has brought us and will explore this idea in othertasks of natural language processing.5 Future WorkFigure 4.
System performance.
Multi-document summarization naturally comesinto picture for future development.
Our corpusorganization itself is in the form of multiple articlesbeing summarized into one (monthly or weekly).
Howdo we learn and use this structure to summarize a newset of articles?Headline generation is another task that we canapproach equipped with our large restructured webcorpus.We believe that the answers to these questions areembedded in the characteristics of the corpus, namelythe WWW, and are eager to discover them in the nearfuture.AcknowledgementWe want to thank Dr. Chin-Yew Lin for making theYahoo Full Coverage Collection download.ReferencesKenneth W. Church.
1993.
Char align: A program foraligning parallel texts at the character level.
InProceedings of the Workshop on Very LargeCorpora: Academic and Industrial Perspectives,ACL.
Association for Computational Linguistics,1993.Aidan Finn, Nicholas Kushmerick, and Barry Smyth.2001.
Fact or fiction: Content classification fordigital libraries.
In Proceedings of NSF/DELOSWorkshop on Personalization and Recommender.Pascale Fung and Kenneth W. Church.
1994.
K-vec: Anew approach for aligning parallel texts.
InProceedings from the 15th International Conferenceon Computational Linguistics, Kyoto.Kevin Knight and Daniel Marcu (2000).
Statistics-Based Summarization--Step One: SentenceCompression.
The 17th National Conference of theAmerican Association for Artificial IntelligenceAAAI'2000, Outstanding Paper Award, Austin,Texas, July 30-August 3, 2000.Julian Kupiec, Jan Pedersen, and Francine Chen.
1995.A trainable document summarizer.
In SIGIR ?95,Proceedings of the 18th Annual International ACMSIGIR Conference on Research and Development inInformation Retrieval.
Seattle, Washington, USA.,pages 68-73.
ACM Press.Human-produced summary System-produced summaryA major earthquake registering 7.2 on the Richterscale shook the Solomon Islands in the SouthPacific today .It was the largest earthquake in the Solomons sincea 7.4 quake on Nov .
5 , 1978 and the strongest inthe world in the five months .An 8.3 quake hit the Macquarie Islands south ofAustralia on May 23 .The preliminary reading of 7.2 is slightly strongerthan the 7.1 magnitude earthquake that hit the SanFrancisco Bay area Oct .
17 .Major earthquakes in the Solomons usually don'tcause much damage or many casualties becausethe area is sparsely populated and not extensivedeveloped .lyA major earthquake registering 7.2 on the Richter scaleshook the Solomon Islands in the South Pacific today,the U.S. Geological Survey says.The preliminary reading of 7.2 is slightly stronger thanthe 7.1 magnitude earthquake that hit the San FranciscoBay area Oct. 17.It was the largest earthquake in the Solomons since a 7.4quake on Nov. 5, 1978.There were no immediate reports of injury or damage.An 8.3 quake hit the Macquarie Islands south ofAustralia on May 23.The Richter scale is a measure of ground motion asrecorded on seismographs.Thus a reading of 7.5 reflects an earthquake 10 timesstronger than one of 6.5.Figure 5.
Comparison of summaries generated by human and system.Chin-Yew Lin.
2001.
Summary evaluation environment.http://www.isi.edu/~cyl/SEE.Chin-Yew Lin and Eduard Hovy.
2000.
The automatedacquisition of topic signatures for textsummarization.
In Proceedings of the 18thInternational Conference on ComputationalLinguistics (COLING 2000), Saarbr?cken, Germany,July 31- August 4, 2000.Daniel Marcu.
1999.
The automatic construction oflarge-scale corpora for summarization research.
The22nd International ACM SIGIR Conference onResearch and Development in Information Retrieval(SIGIR'99), pages 137-144, Berkeley, CA, August1999.I.
Dan Melamed.
1999.
Bitext maps and alignment viapattern recognition.
Computational Linguistics,25(1):107-130.Gerard Salton.
1975.
A vector space model forinformation retrieval.
Communications of the ACM,18(11):613-620, November 1975.David Zajic, Bonnie Dorr, and Richard Schwartz.
2002.Automatic headline generation for newspaperstories.
In Proceedings of the ACL-2002 Workshopon Text Summarization, Philadelphia, PA, 2002.
