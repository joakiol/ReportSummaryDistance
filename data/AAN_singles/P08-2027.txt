Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 105?108,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsA Unified Syntactic Model for Parsing Fluent and Disfluent Speech?Tim MillerUniversity of Minnesotatmill@cs.umn.eduWilliam SchulerUniversity of Minnesotaschuler@cs.umn.eduAbstractThis paper describes a syntactic representationfor modeling speech repairs.
This representa-tion makes use of a right corner transform ofsyntax trees to produce a tree representationin which speech repairs require very few spe-cial syntax rules, making better use of trainingdata.
PCFGs trained on syntax trees using thismodel achieve high accuracy on the standardSwitchboard parsing task.1 IntroductionSpeech repairs occur when a speaker makes a mis-take and decides to partially retrace an utterance inorder to correct it.
Speech repairs are common inspontaneous speech ?
one study found 30% of dia-logue turns contained repairs (Carletta et al, 1993)and another study found one repair every 4.8 sec-onds (Blackmer and Mitton, 1991).
Because of therelatively high frequency of this phenomenon, spon-taneous speech recognition systems will need to beable to deal with repairs to achieve high levels ofaccuracy.The speech repair terminology used here followsthat of Shriberg (1994).
A speech repair consists ofa reparandum, an interruption point, and the alter-ation.
The reparandum contains the words that thespeaker means to replace, including both words thatare in error and words that will be retraced.
The in-terruption point is the point in time where the streamof speech is actually stopped, and the repairing ofthe mistake can begin.
The alteration contains the?This research was supported by NSF CAREER award0447685.
The views expressed are not necessarily endorsed bythe sponsors.words that are meant to replace the words in thereparandum.Recent advances in recognizing spontaneousspeech with repairs (Hale et al, 2006; Johnson andCharniak, 2004) have used parsing approaches ontranscribed speech to account for the structure in-herent in speech repairs at the word level and above.One salient aspect of structure is the fact that thereis often a good deal of overlap in words betweenthe reparandum and the alteration, as speakers maytrace back several words when restarting after an er-ror.
For instance, in the repair .
.
.
a flight to Boston,uh, I mean, to Denver on Friday .
.
.
, there is an exactmatch of the word ?to?
between reparandum and re-pair, and a part of speech match between the words?Boston?
and ?Denver?.Another sort of structure in repair is what Lev-elt (1983) called the well-formedness rule.
Thisrule states that the constituent started in the reparan-dum and repair are ultimately of syntactic types thatcould be grammatically joined by a conjunction.
Forexample, in the repair above, the well-formednessrule says that the repair is well formed if the frag-ment .
.
.
a flight to Boston and to Denver.
.
.
is gram-matical.
In this case the repair is well formed sincethe conjunction is grammatical, if not meaningful.The approach described here makes use of a trans-form on a tree-annotated corpus to build a syntacticmodel of speech repair which takes advantage of thestructure of speech repairs as described above, whilealso providing a representation of repair structurethat more closely adheres to intuitions about whathappens when speakers make repairs.1052 Speech repair representationThe representational scheme used for this workmakes use of a right-corner transform, a way ofrewriting syntax trees that turns all right recursioninto left recursion, and leaves left recursion as is.As a result, constituent structure is built up dur-ing recognition in a left-to-right fashion, as wordsare read in.
This arrangement is well-suited torecognition of speech with repairs, because it al-lows for constituent structure to be built up usingfluent speech rules up until the moment of interrup-tion, at which point a special repair rule may be ap-plied.
This property will be examined further in sec-tion 2.3, following a technical description of the rep-resentation scheme.2.1 Binary branching structureIn order to obtain a linguistically plausible right-corner transform representation of incomplete con-stituents, the Switchboard corpus is subjected to apre-process transform to introduce binary-branchingnonterminal projections, and fold empty categoriesinto nonterminal symbols in a manner similar to thatproposed by Johnson (1998b) and Klein and Man-ning (2003).
This binarization is done in in sucha way as to preserve linguistic intuitions of headprojection, so that the depth requirements of right-corner transformed trees will be reasonable approx-imations to the working memory requirements of ahuman reader or listener.Trees containing speech repairs are reduced in ar-ity by merging repair structure lower in the tree,when possible.
As seen in the left tree below, 1 re-pair structure is annotated in a flat manner, whichcan lead to high-arity rules which are sparsely repre-sented in the data set, and thus difficult to learn.
Thisproblem can be mitigated by using the rewrite ruleshown below, which turns an EDITED-X constituentinto the leftmost child of a tree of type X, as long asthe original flat tree had X following an EDITED-X constituent and possibly some editing term (ET)categories.
The INTJ category (?uh?,?um?,etc.)
andthe PRN category (?I mean?, ?that is?, etc.)
are con-sidered to be editing term categories when they lie1Here, all Ai denote nonterminal symbols, and all ?i denotesubtrees; the notation A1:?1 indicates a subtree ?1 with labelA1; and all rewrites are applied recursively, from leaves to root.between EDITED-X and X constituents.A0EDITEDA1:?1ET* A1:?2 ?3 ?A0A1EDITED-A1A1:?1ET* A1:?2?32.2 Right-corner transformBinarized trees2 are then transformed into right-corner trees using transform rules similar to thosedescribed by Johnson(1998a).
This right-cornertransform is simply the left-right dual of a left-corner transform.
It transforms all right recursivesequences in each tree into left recursive sequencesof symbols of the form A1/A2, denoting an incom-plete instance of category A1 lacking an instance ofcategory A2 to the right.Rewrite rules for the right-corner transform areshown below:A1?1 A2?2 A3:?3?A1A1/A2?1A2/A3?2A3:?3A1A1/A2:?1 A2/A3?2?3 .
.
.
?A1A1/A3A1/A2:?1 ?2?3 .
.
.Here, the first rewrite rule is applied iteratively(bottom-up on the tree) to flatten all right recursion,using incomplete constituents to record the originalnonterminal ordering.
The second rule is then ap-plied to generate left recursive structure, preservingthis ordering.The incomplete constituent categories created bythe right corner transform are similar in form andmeaning to non-constituent categories used in Com-binatorial Categorial Grammars (CCGs) (Steedman,2000).
Unlike CCGs, however, a right corner trans-formed grammar does not allow backward functionapplication, composition, or raising.
As a result, itdoes not introduce spurious ambiguity between for-ward and backward operations, but cannot be takento explicitly encode argument structure, as CCGscan.2All super-binary branches remaining after the above pre-process are ?nominally?
decomposed into right-branching struc-tures by introducing intermediate nodes with labels concate-nated from the labels of its children, delimited by underscores106EDITED [-NP]NP [-UNF]NPDTtheJJfirstNNkindPP [-UNF]INofNP [-UNF]NNinvasionPP-UNFINofFigure 1: Standard tree repair structure, with -UNF prop-agation as in (Hale et al, 2006) shown in brackets.EDITED-NPNP/PPNP/NPNP/PPNPNP/NNNP/NNDTtheJJfirstNNkindINofNPinvasionPP-UNFofFigure 2: Right-corner transformed tree with repair struc-ture2.3 Application to speech repairAn example speech repair from the Switchboard cor-pus can be seen in Figures 1 and 2, in which the samerepair fragment is shown in a standard state such asmight be used to train a probabilistic context freegrammar, and after the right-corner transform.
Fig-ure 1 also shows, in brackets, the augmented anno-tation used by Hale et al(2006).
This scheme con-sisted of adding -X to an EDITED label which pro-duced a category X, as well as propagating the -UNFlabel at the right corner of the tree up through everyparent below the EDITED root.The standard annotation (without -UNF propaga-tion) is deficient because even if an unfinished con-stituent like PP-UNF is correctly recognized, and thespeaker is essentially in an error state, there may beseveral partially completed constituents above ?
inFigure 1, the NP, PP, and NP above the PP-UNF.These constituents need to be completed, but usingthe standard annotation there is only one chance tomake use of the information about the error that hasoccurred ?
the NP ?
NP PP-UNF rule.
Thus, by thetime the error section is completed, there is no infor-mation by which a parsing algorithm could chooseto reduce the topmost NP to EDITED other than in-dependent rule probabilities.The approach used by (Hale et al, 2006) worksbecause the information about the transition to an er-ror state is propagated up the tree, in the form of the-UNF tags.
As the parsing chart is filled in bottomup, each rule applied is essentially coming out of aspecial repair rule set, and so at the top of the treethe EDITED hypothesis is much more likely.
How-ever, this requires that several fluent speech rulesfrom the data set be modified for use in a specialrepair grammar, which not only reduces the amountof available training data, but violates our intuitionthat most reparanda are fluent up until the actual editoccurs.The right corner transform model works in a dif-ferent way, by building up constituent structure fromleft to right.
In Figure 2, the same fragment isshown as it appears in the training data for this sys-tem.
With this representation, the problem noticedby Hale and colleagues (2006) has been solved ina different way, by incrementally building up left-branching rather than right-branching structure, sothat only a single special error rule is required at theend of the constituent.
Whereas the -UNF propa-gation scheme often requires the entire reparandumto be generated from a speech repair rule set, thisscheme only requires one special rule, where themoment of interruption actually occurred.This is not only a pleasing parsimony, but it re-duces the number of special speech repair rules thatneed to be learned and saves more potential exam-ples of fluent speech rules, and therefore potentiallymakes better use of limited data.3 EvaluationThe evaluation of this system was performed onthe Switchboard corpus, using the mrg annotationsin directories 2 and 3 for training, and the filessw4004.mrg to sw4153.mrg in directory 4 for evalu-ation, following Johnson and Charniak(2004).The input to the system consists of the terminalsymbols from the trees in the corpus section men-tioned above.
The terminal symbol strings are firstpre-processed by stripping punctuation and other107System Parseval F EDIT FBaseline 60.86 42.39CYK (H06) 71.16 41.7RCT 68.36 64.41TAG-based model (JC04) ?
79.7Table 1: Baseline results are from a standard CYK parserwith binarized grammar.
We were unable to find the cor-rect configuration to match the baseline results from Haleet al RCT results are on the right-corner transformedgrammar (transformed back to flat treebank-style treesfor scoring purposes).
CYK and TAG lines show relevantresults from related work.non-vocalized terminal symbols, which could notbe expected from the output of a speech recognizer.Crucially, any information about repair is strippedfrom the input, including partial words, repair sym-bols 3, and interruption point information.
While anintegrated system for processing and parsing speechmay use both acoustic and syntactic information tofind repairs, and thus may have access to some ofthis information about where interruptions occur,this experiment is intended to evaluate the use of theright corner transform and syntactic information onparsing speech repair.
To make a fair comparison tothe CYK baseline of (Hale et al, 2006), the recog-nizer was given correct part-of-speech tags as inputalong with words.The results presented here use two standard met-rics for assessing accuracy of transcribed speechwith repairs.
The first metric, Parseval F-measure,takes into account precision and recall of all non-terminal (and non pre-terminal) constituents in a hy-pothesized tree relative to the gold standard.
Thesecond metric, EDIT-finding F, measures precisionand recall of the words tagged as EDITED in thehypothesized tree relative to those tagged EDITEDin the gold standard.
F score is defined as usual,2pr/(p + r) for precision p and recall r.The results in Table 1 show that this system per-forms comparably to the state of the art in over-all parsing accuracy and reasonably well in edit de-tection.
The TAG system (Johnson and Charniak,2004) achieves a higher EDIT-F score, largely as aresult of its explicit tracking of overlapping words3The Switchboard corpus has special terminal symbols indi-cating e.g.
the start and end of the reparandum.between reparanda and alterations.
A hybrid systemusing the right corner transform and keeping infor-mation about how a repair started may be able toimprove EDIT-F accuracy over this system.4 ConclusionThis paper has described a novel method for pars-ing speech that contains speech repairs.
This systemachieves high accuracy in both parsing and detectingreparanda in text, by making use of transformationsthat create incomplete categories, which model thereparanda of speech repair well.ReferencesElizabeth R. Blackmer and Janet L. Mitton.
1991.
Theo-ries of monitoring and the timing of repairs in sponta-neous speech.
Cognition, 39:173?194.Jean Carletta, Richard Caley, and Stephen Isard.
1993.A collection of self-repairs from the map task cor-pus.
Technical report, Human Communication Re-search Centre, University of Edinburgh.John Hale, Izhak Shafran, Lisa Yung, Bonnie Dorr, MaryHarper, Anna Krasnyanskaya, Matthew Lease, YangLiu, Brian Roark, Matthew Snover, and Robin Stew-art.
2006.
PCFGs with syntactic and prosodic indica-tors of speech repairs.
In Proceedings of the 45th An-nual Conference of the Association for ComputationalLinguistics (COLING-ACL).Mark Johnson and Eugene Charniak.
2004.
A tag-basednoisy channel model of speech repairs.
In Proceed-ings of the 42nd Annual Meeting of the Associationfor Computational Linguistics (ACL ?04), pages 33?39, Barcelona, Spain.Mark Johnson.
1998a.
Finite state approximation ofconstraint-based grammars using left-corner grammartransforms.
In Proceedings of COLING/ACL, pages619?623.Mark Johnson.
1998b.
PCFG models of linguistic treerepresentation.
Computational Linguistics, 24:613?632.Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
In Proceedings of the 41stAnnual Meeting of the Association for ComputationalLinguistics, pages 423?430.William J.M.
Levelt.
1983.
Monitoring and self-repair inspeech.
Cognition, 14:41?104.Elizabeth Shriberg.
1994.
Preliminaries to a Theory ofSpeech Disfluencies.
Ph.D. thesis, University of Cali-fornia at Berkeley.Mark Steedman.
2000.
The syntactic process.
MITPress/Bradford Books, Cambridge, MA.108
