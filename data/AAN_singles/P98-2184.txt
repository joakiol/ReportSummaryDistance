How Verb Subcategorization Frequencies Are Affected By Corpus ChoiceDouglas RolandUniversity of ColoradoDepartment of LinguisticsBoulder, CO 80309-0295Douglas.Roland@colorado.eduDaniel JurafskyUniversity of ColoradoDept.
of Linguistics & Inst.
of Cognitive ScienceBoulder, CO 80309-0295jurafsky @ colorado.eduAbstractThe probabilistic relation between verbs andtheir arguments plays an important role inmodern statistical parsers and supertaggers,and in psychological theories of languageprocessing.
But these probabilities arecomputed in very different ways by the twosets of researchers.
Computational linguistscompute verb subcategorization probabilitiesfrom large corpora while psycholinguistscompute them from psychological studies(sentence production and completion tasks).Recent studies have found differencesbetween corpus frequencies andpsycholinguistic measures.
We analyzesubcategorization frequencies from fourdifferent corpora: psychological sentenceproduction data (Connine t al.
1984), writtentext (Brown and WSJ), and telephoneconversation data (Switchboard).
We findtwo different sources for the differences.Discourse influence is a result of how verbuse is affected by different discourse typessuch as narrative, connected iscourse, andsingle sentence productions.
Semanticinfluence is a result of different corpora usingdifferent senses of verbs, which have differentsubcategorization frequencies.
We concludethat verb sense and discourse type play animportant role in the frequencies observed indifferent experimental and corpus basedsources of verb subcategorization frequencies.1 In t roduct ionThe probabilistic relation between verbs and theirarguments plays an important role in modernstatistical parsers and supertaggers (Charniak1995, Collins 1996/1997, Joshi and Srinivas 1994,Kim, Srinivas, and Trueswell 1997, Stolcke et al1997), and in psychological theories of languageprocessing (Clifton et al 1984, Ferfeira &McClure 1997, Gamsey et al 1997, Jurafsky 1996,MacDonald 1994, Mitchell & Holmes 1985,Tanenhaus et al 1990, Trueswell et al 1993).These probabilities are computed in very differentways by the two sets of researchers.Psychological studies use methods such assentence completion and sentence production forcollecting verb argument structure probabilities.In sentence completion, subjects are asked tocomplete a sentence fragment.
Garnsey at al.
(1997) used a proper name followed by a verb,such as "Debbie remembered ."
Insentence subjects are asked to write any sentencecontaining a given verb.
An example of this typeof study is Connine t al.
(1984).An alternative to these psychological methods isto use corpus data.
This can be doneautomatically with unparsed corpora (Briscoe andCarroll 1997, Manning 1993, Ushioda et al 1993),from parsed corpora such as Marcus et al's (1993)Treebank (Merlo 1994, Framis 1994) or manuallyas was done for COMLEX (Macleod andGrishman 1994).
The advantage of any of thesecorpus methods is the much greater amount ofdata that can be used, and the much more naturalcontexts.
This seems to make it preferable todata generated in psychological studies.Recent studies (Merlo 1994, Gibson et al 1996)have found differences between corpusfrequencies and experimental measures.
Thissuggests that corpus-based frequencies andexperiment-based frequencies may not beinterchangeable.
To clarify the nature of thedifferences between various corpora and to findthe causes of these differences, we analyzed1122psychological sentence production data (Connineetal.
1984), written discourse (Brown and WSJfrom Penn Treebank - Marcus et al 1993), andconversational data (Switchboard - Godfrey et al1992).
We found that the subcategorizationfrequencies in each of these sources are different.We performed three experiments to (1) find thecauses of general differences between corpora, (2)measure the size of these differences, and (3) findverb specific differences.
The rest of this paperdescribes our methodology and the two sources ofsubcategorization probability differences:discourse influence and semantic influence.2 MethodologyFor the sentence production data, we used thenumbers published in the original Connine et alpaper as well as the original data, which we wereable to review thanks to the generosity of CharlesClifton.
The Connine data (CFJCF) consists ofexamples of 127 verbs, each classified asbelonging to one of 15 subcategorization frames.We added a 16th category for direct quotations(which appeared in the corpus data but not theConnine data).
Examples of these categories,taken from the Brown Corpus, appear in figure 1below.
There are approximately 14,000 verbtokens in the CFJCF data set.For the BC, WSJ, and SWBD data, we countedsubcategorizations using tgrep scripts based on thePenn Treebank.
We automatically extracted andcategorized all examples of the 127 verbs used inthe Cormine study.
We used the same verbsubcategorization categories as the Connine study.There were approximately 21,000 relevant verbtokens in the Brown Corpus, 25,000 relevant verb\[O\] Barbara sked, as they heard the front door close.\[PP\] Guerrillas were racing \[toward him\].3 \[mf-S\] Hank thanked them and promised \[to observe the rules\].4 \[inf-S\]/PP/ Labor fights \[to change its collar from blue to white\].5 \[wh-S\] I know now \[why the students insisted that I go to Hiroshima even when I told them I didn'twant to\].6 \[that-S\] She promised \[that she would soon take a few day's leave and visit the uncle she had neverseen, on the island of Oyajima --which was not very far from Yokosuka\].7 \[verb-ing\] But I couldn't help \[thinking that Nadine and WaUy were getting just what they deserved\].\[perception Far off, in the dusk, he heard \[voices singing, muffled but strong\].complement.\]9 \[NP\] The turtle immediately withdrew into its private council room to study \[the phenomenon\].10 \[NP\]\[NP\] The mayor of the town taught \[them\] \[English and French\].11 \[NP\]\[PP\] They bought \[rustled cattle\] \[from the outlaw\], kept him supplied with guns andammunition, harbored his men in their houses.12 \[NP\]\[inf-S\] She had assumed before then that one day he would ask \[her\] [to marry him\].13 INP\]\[wh-S\] I asked \[Wisman\] \[what would happen if he broke out the go codes and tried to starttransmitting one\].14 \[NPl\[that-S\] But, in departing, Lewis begged \[Breasted\] \[that there be no liquor in the apartment at theGrosvenor on his return\], and he took with him the fast thirty galleys of Elmer Gantry.15 \[passive\] A cold supper was ordered and a bottle of port.16 Quotes He writes \["Confucius held that in times of stress, one should take short views - only up tolunchtime.
"\]Figure 1 - examples of each subcategorization frame from Brown Corpus1123tokens in the Wall Street Journal Corpus, and10,000 in Switchboard.
Unlike the Connine data,where all verbs were equally represented, thefrequencies of each verb in the corpora varied.For each calculation where individual verbfrequency could affect the outcome, wenormalized for frequency, and eliminated verbswith less than 50 examples.
This left 77 out of127 verbs in the Brown Corpus, 74 in the WallStreet Journal, and only 30 verbs in Switchboard.This was not a problem with the Connine datawhere most verbs had approximately 100 tokens.3 Exper iment  1The purpose of the first experiment is to analyzethe general (non-verb-specific) differencesbetween argument structure frequencies in thedata sources.
In order to do this, the data for eachverb in the corpus was normalized to remove theeffects of verb frequency.
The averagefrequency of each subcategorization frame wascalculated for each corpus.
The averagefrequencies for each of the data sources were thencompared.3.1 Resu l tsWe found that the three corpora consisting ofconnected discourse (BC, WSJ, SWBD) shared acommon set of differences when compared to theCFJCF sentence production data.
There werethree general categories of differences between thecorpora, and all can be related to discourse type.These categories are:(1) passive sentences(2) zero anaphora(3) quotations3.1.1 Passive SentencesThe CFJCF single sentence productions had thesmallest number of passive sentences.
Theconnected spoken discourse in Switchboard hadmore passives, followed by the written discoursein the Wall Street Journal and the Brown Corpus.Data SourceCFJCFSwitchboard 2.2%Wall Street Journal 6.7%Brown Corpus% passive sentences0.6%7.8%Passive is generally used in English to emphasizethe undergoer (to keep the topic in subjectposition) and/or to de-emphasize the identity ofthe agent (Thompson 1987).
Both of thesereasons are affected by the type of discourse.
Ifthere is no preceding discourse, then there is nopre-existing topic to keep in subject position.
Inaddition, with no context for the sentence, there isless likely to be a reason to de-emphasize theagent of the sentence.3.1.2 Zero AnaphoraThe increase in zero anaphora (not overtlymentioning understood arguments) is caused bytwo factors.
Generally, as the amount ofsurrounding context increases (going from singlesentence to connected iscourse) the need toovertly express all of the arguments with a verbdecreases.Data Source % \[0\] subcat frameCFJCF 7%Wall Street Journal 8%Brown 13 %Switchboard 18 %Verbs that can describe actions (agree, disappear,escape, follow, leave, sing, wait) were typicallyused with some form of argument in singlesentences, uch as:"I had a test that day, so I really wanted to escapefrom school."
(CFJCF data).Such verbs were more likely to be used withoutany arguments in connected discourse as in:"She escaped , crawled through the usual minefields, under barbed wire, was shot at, swam ariver, and we finally picked her up in Linz.
"(Brown Corpus)In this case, the argument of "escaped",("imprisonment") was understood from theprevious sentence.
Verbs of propositionalattitude (agree, guess, know, see, understand) aretypically used transitively in written corpora andsingle-sentence production:"I guessed the right answer on the quiz.
"(CFJCF).In spoken discourse, these verbs are more likely tobe used metalinguistically, with the previous1124discourse contribution understood as the argumentof the verb:"I see."
(Switchboard)"I guess."
(Switchboard)3.1.3 Quotaa'onsQuotations are usually used in narrative, which ismore likely in connected iscourse than in anisolated sentence.
This difference mainly effectsverbs of communication (e.g.
answer, ask, call,describe, read, say, write).Data SourceCFJCFSwitchboard 0%Brown 4%Wall Street Journal 6%Percent DirectQuotation0%These verbs are used in corpora to discuss detailsof the contents of communication:"Turning to the reporters, she asked, "Did youhear her?
"'(Brown)In single sentence production, they are used todescribe the (new) act of communication itself ?
"He asked a lot of questions at school."
(CFJCF)We are currently working on systematicallyidentifying indirect quotes in the corpora and theCFJCF data to analyze in more detail how they fitin to this picture.4 Exper iment  2Our first experimentfactors were thesuggested that discourseprimary cause ofsubcategorization differences.
One way to testthis hypothesis is to eliminate discourse factorsand see if this removes subcategorizationdifferences.We measure the difference between the way a verbis used in two different corpora by counting thenumber of sentences (per hundred) where a verb inone corpus would have to be used with a differentsubcategorization in order for the two corpora toyield the same subcategorization frequencies.This same number can also be calculated for theoverall subcategorization frequencies of twocorpora to show the overall difference between thetwo corpora.Our procedure for measuring the effect ofdiscourse is as follows (illustrated using passiveas an example):1.
Measure the difference between two corporaWSJ vs CFJCF)100  [owsJ I5.0% []CFJCF[0.0%% Passive - WSJ vs CFJCF2.
Remove differences caused by discourseeffects (based on BC vs CFJCF).
CFJCF has22% the number of passives that BC has.iii!!
!iiii!i iiiiiii)0%,mr'IBC I[ ]CFJCFI% Passive - BC vs CFJCFWe then linearly scale the number of passivesfound in WSJ to reflect the difference foundbetween BC and CFJCF.00  !tiiii!iiiiiiiiii!tiiii)iiiiiiiiiiiiiii)5.0%0 .0%~ .........r'lWSJ-mapped[] CFJCF% Passive - WSJ (adjusted) vs CFJCF3.
re-measure the difference between twocorpora (WSJ vs CFJCF)4. amount of improvement = size of discourseeffectThis method was applied to the passive, quote,and zero subcat frames, since these are the onesthat show discourse-based differences.
Before1125the mapping, WSJ has a difference of 17frames/100 overall difference when comparedwith CFJCF.
After the mapping, the differenceis only 9.6 frames/100 overall difference.
Thisindicates that 43% of the overall cross-verbdifferences between these two corpora are causedby discourse ffects.We use this mapping procedure to measure thesize and consistency of the discourse ffects.
Amore sophisticated mapping procedure would beappropriate for other purposes ince the verbs withthe best matches between corpora are actuallymade worse by this mapping procedure.5 Experiment 3Argument preference was also affected by verbsemantics.
To examine this effect, we took twosample ambiguous verbs, "charge" and "pass".We hand coded them for semantic senses in eachof the corpora we used as follows:Examples of 'charge' taken from BC.accuse: "His petition charged mental cruelty.
"attack: "When he charged Mickey was ready.
"money: "... 20 per cent ... was all he charged thetraders.
"Examples of 'pass' taken from BC.movement: "Blue Throat's men spotted him ... as hepassed.
"law" 'q'he President noted that Congress last yearpassed a law providing grants ..."transfer: "He asked, when she passed him a glass.
"test: "Those who T stayed had * to pass tests.
"We then asked two questions:1.
Do different verb senses have differentargument structure preferences?2.
Do different corpora have different verbsense preferences, and therefore potentiallydifferent argument structure preferences?For both verbs examined (pass and charge) therewas a significant effect of verb sense on argumentstructure probabilities (by X 2 p <.001 for 'charge'and p <.001 for 'pass').
The following chartshows a sample of this difference:that NP NPPP  passiveCharge(accuse) 32 0 24 25Sample Frames and Senses from WSJWe then analyzed how often each sense was usedin each of the corpora and found that there wasagain a significant difference (by X 2 p <.001 for'charge' ~ nd p <.001 for 'pass').e~0 E136916BC 22 15 4WSJ 88 1 7SWBD 1Senses of 'Charge' used in each cot0)USBCWSJSWBD13611032 16 2 4476 31 8 225 2 1 0Senses of 'Pass' used in each corpusThis analysis hows that it is possible for shifts inthe relative frequency of each of a verbs senses toinfluence the observed subcat frequencies.We are currently extending our study to see if verbsenses have constant subcategorizationfrequencies across corpora.
This would be usefulfor word sense disambiguation and for parsing.If the verb sense is known, then a parser could usethis information to help look for likely arguments.If the subcatagorization is known, then adisambiguator could use this information to findthe sense of the verb.
These could be used tobootstrap each other relying on the heuristic thatonly one sense is used within any discourse (Gale,Church, & Yarowsky 1992).6 EvaluationWe had previously hoped to evaluate the accuracyof our treebank induduced subcategorizationprobabilities by comparing them with theCOMLEX hand-coded probabilities (Macleod and1126Grishman 1994), but we used a different set ofsubcategorization frames than COMLEX.Instead, we hand checked a random sample of ourdata for errors.to find arguments that were located to the left ofthe verb.
This is because arbitrary amounts ofstructure can intervene, expecially in the case oftraces.The error rate in our data is between 3% and 7%for all verbs excluding 'say' type verbs such as'answer', 'ask', 'call', 'read', 'say', and 'write'.The error rate is given as a range due to thesubjectivity of some types of errors.
The errorscan be divided into two classes; errors which aredue to mis-parsed sentences in Treebank ~, anderrors which are due to the inadequacy of oursearch strings in indentifying certain syntactic9atterns.Treebank-based rrorsPP attachment 1%verb+particle vsverb+PP 2%NP/adverbial distinction 2%misc.
miss-parsed sentences 1%Errors based on our search strinl~smissed traces and displaced arguments 1%"say" verbs missing quotes 6%Error rate by categoryIn trying to estimate the maximum amount oferror in our data, we found cases where it waspossible to disagree with the parses/tags given inTreebank.
Treebank examples given belowinclude prepositional attachinent (1), the verb-particle/preposition distinction (2), and theNP/adverbial distinction (3).1.
"Sam, I thought you \[knew \[everything\]~\[about Tokyo\]pp\]" (BC)2.
"...who has since moved \[on to othermethods\]pp?"
(BC)3.
"Gross stopped \[bricfly\]Np?, then went on.
"(Be)Missed traces and displaced argument errors werea result of the difficulty in writing search strings1 All of our search patterns are based only on theinformation available in the Treebank 1coding system,since the Brown Corpus is only available in thisscheme.
The error rate for corpora available inTreebank 2 form would have been lower had we usedall available information.Six percent of the data (overall) was improperlyclassified ue to the failure of our search patternsto identify all of the quote-type arguments whichoccur in 'say' type verbs.
The identification ofthese elements is particularly problematic due tothe asyntactic nature of these arguments, rangingfrom a sound (He said 'Argh!')
to complexsentences.
The presence or absense of quotationmarks was not a completely reliable indicator ofthese arguments.
This type of error affects onlya small subset of the total number of verbs.
27%of the examples of these verbs were mis-classified,always by failing to find a quote-type argument ofthe verb.
Using separate search strings for theseverbs would greatly improve the accuracy of thesesearches.Our eventual goal is to develop a set of regularexpressions that work on fiat tagged corporainstead of TreeBank parsed structures to allow usto gather information from larger corpora thanhave been done by the TreeBank project (seeManning 1993 and Gahl 1998).7 Conclus ionWe find that there are significant differencesbetween the verb subcategorization frequenciesgenerated through experimental methods andcorpus methods, and between the frequencies foundin different corpora.
We have identified twodistinct sources for these differences.
Discourseinfluences are caused by the changes in the wayslanguage is used in different discourse types andare to some extent predictable from the discoursetype of the corpus in question.
Semanticinfluences are based on the semantic ontext of thediscourse.
These differences may be predictablefrom the relative frequencies ofeach of the possiblesenses of the verbs in the corpus.
An extensiveanalysis of the frame and sense frequencies ofdifferent verbs across different corpora is needed toverify this.
This work is presently being carriedout by us and others (Baker, Fillmore, & Lowe1998).
It is certain, however, that verb sense and1127discourse type play an important role in thefrequencies observed in different experimental andcorpus based sources of verb subcategorizationfrequenciesAcknowledgmentsThis project was supported by the generosity of theNSF via NSF 1RI-9704046 and NSF 1RI-9618838 andthe Committee on Research and Creative Work at thegraduate school of the University of Colorado,Boulder.
Many thanks to Giulia Bencini, CharlesClifton, Charles Fillmore, Susanne Gahl, MichelleGregory, Uli Heid, Paola Merlo, Bill Raymond, andPhilip Resnik.ReferencesBaker, C. Fillmore, C., & Lowe, J.B. (1998) Framenet.ACL 1998Biber, D. (1993) Using Register-Diversified Corpora forGeneral Language Studies.
Computational Linguistics,19/2, pp.
219-241.Briscoe T. and Carrol J.
(1997) Automatic Extraction ofSubcategorization from Corpora.Charniak, E. (1997) Statistical parsing with a context-freegrammar and word statistics.
Proceedings of theFourteenth National Conference on Artificial IntelligenceAAAI Press, Menlo Park.Clifton, C., Fraz&r, L,, & Connine, C. (1984) Lexicalexpectations in sentence comprehension.
Journal ofVerbal Learning and Verbal Behavior, 23, 696-708.Collins, M. J.
(1996) A new statistical parser based onbigram lexical dependencies.
In Proceedings of ACL-96,184--191, Santa Cruz, CA.Collins, M. J.
(1997) Three generative, lexicalised modelsfor statistical parsing.
In Proceedings of A CL-97.Connine, Cynthia, Fernanda Ferreira, Charlie Jones,Charles Clifton and Lyn Frazier.
(1984) Verb FramePreference: Descriptive Norms.
Journal ofPsycholinguistic Research 13, 307-319Ferreira, F., and McClure, K.K.
(1997).
Parsing ofGarden-path Sentences with Reciprocal Verbs.Language and Cognitive Processes, 12, 273-306.Framis, F.R.
(1994).
An experiment on learningappropriate selectional restrictions from a parsed corpus.Manuscript.Gahl, S. (1998).
Automatic extraction of subcorpora basedon subcategorization frames from a part-of-speech taggedcorpus.
Proceedings of A CL-98, Montreal.Gale, W.A., Church, K.W., and Yarowsky, D. (1992).
OneSense Per Discourse.
Darpa Speech and NaturalLanguage Workshop.Garnsey, S. M., Pearlmutter, N. J., Myers, E. & Lotocky, M.A.
(1997).
The contributions of verb bias and plausibilityto the comprehension of temporarily ambiguoussentences.
Journal of Memory and Language, 37, 58-93.Gibson, E., Schutze, C., & Salomon, A.
(1996).
Therelationship between the frequency and the processingcomplexity of linguistic structure.
Journal ofPsycholinguistic Research 25(1), 59-92.Godfrey, J., E. Holliman, J. McDaniel.
(1992)SWITCHBOARD : Telephone speech corpus forresearch and development.
Proceedings of ICASSP-92,517--520, San Francisco.Joshi, A.
& B. Srinivas.
(1994) Disambiguation of superparts of speech (or supertags): almost parsing.Proceedings of COLING '94.Juliano, C., and Tanenhaus, M.K.
Contingent frequencyeffects in syntactic ambiguity resolution.
In proceedings ofthe 15th annual conference of the cognitive sciencesociety, LEA: Hillsdale, NJ.Jurafsky, D. (1996) A probabilistic model of lexical andsyntactic access and disambiguation.
CognitiveScience, 20, 137-194.Lafferty, J., D. Sleator, and D. Temperley.
(1992)Grammatical trigrams: A probabilistic model of linkgrammar.
In Proceedings of the 1992 AAA1 FallSymposium on Probabilistic Approaches to NaturalLanguage.MacDonald, M. C. (1994) Probabilistic onstraints andsyntactic ambiguity resolution.
Language and CognitiveProcesses 9.157--201.MacDonald, M. C., Pearlmutter, N. J.
& Seidenberg, M. S.(1994).
The lexical nature of syntactic ambiguityresolution.
Psychological Review, 101, 676-703.Macleod, C. & Grishman, R. (1994) COMLEX SyntaxReference Manual Version 1 .2 .
Linguistic DataConsortium, University of Pennsylvania.Manning, C. D. (1993) Automatic Acquisition of a LargeSubcategorization Dictionary from Corpora.
Proceedingsof ACL-93, 235-242.Marcus, M.P., Santorini, B.
& Marcinkiewicz, M.A.. (1993)Building a Large Annotated Corpus of English: The PennTreebank.
Computational Linguistics 19.2:313-330.Marcus, M. P., Kim, G. Marcinkiewicz, M.A., Maclntyre, R.,Ann Bies, Ferguson, M., Katz, K., and Schasberger, B..(1994) The Penn Treebank: Annotating predicateargument structure.
ARPA Human LanguageTechnology Workshop, Plainsboro, NJ, 114-119.Meyers, A., Macleod, C., and Grishman, R.. (1995)Comlex Syntax 2.0 manual for tagged entries.Merlo, P. (1994).
A Corpus-Based Analysis of VerbContinuation Frequencies for Syntactic Processing.Journal of Pyscholinguistic Research 23.6.
'435-457.Mitchell, D. C. and 1I.
M. Holmes.
(1985) The role ofspecific information about the verb in parsing sentenceswith local structural ambiguity.
Journal of Memory andLanguage 24.542--559.Stolcke, A., C. Chelba, D. Engle, V. Jimenez, h Mangu, H.Printz, E. Ristad, R. Rosenfeld, D. Wu, F. Jelinek and S.Khudanpur.
(1997) Dependency Language Modeling.Center for Language and Speech Processing ResearchNote No.
24.
Johns Hopkins University, Baltimore.Thompson, S.A. (1987) The Passive in English: A DiscoursePerspective.
In Channon, Robert & Shockey, Linda(Eds.)
In Honor of llse Lehiste/llse LehistePuhendusteos.
Dordrecht: Foris, 497-511.Trueswell, J., M. Tanenhaus and C. KeUo.
(1993) Verb-Specific Constraints in Sentence Processing: SeparatingEffects of Lexical Preference from Garden-Paths.
Journalof Experimental Psychology: Learning, Memory andCognition 19.3, 528-553Trueswell, J.
& M. Tanenhaus.
(1994) Toward a lexicalistframework for constraint-based syntactic ambiguityresolution.
In C. Clifton, K. Rayner & L. Frazier (Eds.
)Perspectives on Sentence Processing.
Hillsdale, N J:Erlbaum, 155-179.Ushioda, A., Evans, D., Gibson, T. & Waibel, A.
(1993)The automatic acquisition of frequencies of verbsubcategorization frames from tagged corpora.
InBoguraev, B.
& Pustejovsky, J. eds.
SIGLEX ACLWorkshop of Acquisition of Lexical Knowledge from Text.Columbus, Ohio: 95-1061128
