Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 85?89,Athens, Greece, 30 March ?
31 March 2009. c?2009 Association for Computational LinguisticsThe TALP-UPC phrase-based translation system for EACL-WMT 2009Jos?
A.R.
Fonollosa and Maxim Khalilov and Marta R. Costa-juss?
andJos?
B. Mari?o and Carlos A. Henr?quez Q. and Adolfo Hern?ndez H. andRafael E. BanchsTALP Research CenterUniversitat Polit?cnica de Catalunya, Barcelona 08034{adrian,khalilov,mruiz,canton,carloshq,adolfohh,rbanchs}@talp.upc.eduAbstractThis study presents the TALP-UPC sub-mission to the EACL Fourth Worskhopon Statistical Machine Translation 2009evaluation campaign.
It outlines the ar-chitecture and configuration of the 2009phrase-based statistical machine transla-tion (SMT) system, putting emphasis onthe major novelty of this year: combina-tion of SMT systems implementing differ-ent word reordering algorithms.Traditionally, we have concentrated onthe Spanish-to-English and English-to-Spanish News Commentary translationtasks.1 IntroductionTALP-UPC (Center of Speech and LanguageApplications and Technology at the UniversitatPolit?cnica de Catalunya) is a permanent par-ticipant of the ACL WMT shared translationstasks, traditionally concentrating on the Spanish-to-English and vice versa language pairs.
In thispaper, we describe the 2009 system?s architectureand design describing individual components anddistinguishing features of our model.This year?s system stands aside from theprevious years?
configurations which were per-formed following an N -gram-based (tuple-based)approach to SMT.
By contrast to them, thisyear we investigate the translation models (TMs)interpolation for a state-of-the-art phrase-basedtranslation system.
Inspired by the work pre-sented in (Schwenk and Est?ve, 2008), we attackthis challenge using the coefficients obtained forthe corresponding monolingual language models(LMs) for TMs interpolation.On the second step, we have performedadditional word reordering experiments, com-paring the results obtained with a statisti-cal method (R. Costa-juss?
and R. Fonollosa,2009) and syntax-based algorithm (Khalilov andR.
Fonollosa, 2008).
Further the outputs ofthe systems were combined selecting the trans-lation with the Minimum Bayes Risk (MBR) al-gorithm (Kumar, 2004) that allowed significantlyoutperforming the baseline configuration.The remainder of this paper is organized asfollows: Section 2 presents the TALP-UPC?09phrase-based system, along with the translationmodels interpolation procedure and other minornovelties of this year.
Section 3 reports on the ex-perimental setups and outlines the results of theparticipation in the EACL WMT 2009 evaluationcampaign.
Section 4 concludes the paper with dis-cussions.2 TALP-UPC phrase-based SMTThe system developed for this year?s sharedtask is based on a state-of-the-art SMT sys-tem implemented within the open-source MOSEStoolkit (Koehn et al, 2007).
A phrase-based trans-lation is considered as a three step algorithm:(1) the source sequence of words is segmentedin phrases, (2) each phrase is translated into tar-get language using translation table, (3) the targetphrases are reordered to be inherent in the targetlanguage.A bilingual phrase (which in the context of SMTdo not necessarily coincide with their linguisticanalogies) is any pair of m source words and ntarget words that satisfies two basic constraints:(1) words are consecutive along both sides of thebilingual phrase and (2) no word on either side ofthe phrase is aligned to a word outside the phrase.Given a sentence pair and a corresponding word-to-word alignment, phrases are extracted follow-ing the criterion in (Och and Ney, 2004).
Theprobability of the phrases is estimated by relativefrequencies of their appearance in the training cor-pus.85Classically, a phrase-based translation systemimplements a log-linear model in which a foreignlanguage sentence fJ1 = f1, f2, ..., fJ is trans-lated into another language eI1 = e1, e2, ..., eI bysearching for the translation hypothesis e?I1 maxi-mizing a log-linear combination of several featuremodels (Brown et al, 1990):e?I1 = argmaxeI1{ M?m=1?mhm(eI1, fJ1 )}where the feature functions hm refer to the systemmodels and the set of ?m refers to the weights cor-responding to these models.2.1 Translation models interpolationWe implemented a TM interpolation strategy fol-lowing the ideas proposed in (Schwenk and Es-t?ve, 2008), where the authors present a promis-ing technique of target LMs linear interpolation;in (Koehn and Schroeder, 2007) where a log-linearcombination of TMs is performed; and specifi-cally in (Foster and Kuhn, 2007) where the authorspresent various ways of TM combination and ana-lyze in detail the TM domain adaptation.In the framework of the evaluation campaign,there were two Spanish-to-English parallel train-ing corpora available: Europarl v.4 corpus (about50M tokens) and News Commentary (NC) corpus(about 2M tokens).
The test dataset provided bythe organizers this year was from the news do-main, so we considered the Europarl training cor-pus as "out-of-domain" data and the News Com-mentary as "in-domain" training material.
Unfor-tunately, the in-domain corpus is much smaller insize, however the Europarl corpus can be also usedto increase the final translation and reordering ta-bles in spite of its different nature.A straightforward approach to the TM interpo-lation would be an iterative TM reconstruction ad-justing scale coefficients on each step of the loopwith use of the highest BLEU score as a maxi-mization criterion.However, we did not expect a significant gainfrom this time-consumption strategy and we de-cided to follow a simpler approach.
In the pre-sented results, we obtained the best interpola-tion weight following the standard entropy-basedoptimization of the target-side LM.
We adjustthe weight coefficient ?Europarl (?NC = 1 ?
?Europarl) of the linear interpolation of the target-side LMs:P (w) = ?Europarl ?
PwEuroparl + ?NC ?
PwNC (1)where PwEuroparl and PwNC are probabilities as-signed to the word sequence w by the LM esti-mated on Europarl and NC data, respectively.The scale factor values are automatically opti-mized to obtain the lowest perplexity ppl(w) pro-duced by the interpolated LM P (w).
We used thestandard script compute ?
best ?
mix from theSRI LM package (Stolcke, 2002) for optimization.On the next step, the optimized coefficients?Europarl and ?NC are generalized on the interpo-lated translation and reordering models.
In otherwords, reordering and translation models are in-terpolated using the same weights which yield thelowest perplexity for LM interpolation.The word-to-word alignment was obtained fromthe joint (merged) database (Europarl + NC).Then, we separately computed the translation andreordering tables corresponding to the in- and out-of-domain parts of the joint alignment.
The finaltables, as well as the final target LM were obtainedusing linear interpolation.
The weights were se-lected using a minimum perplexity criterion esti-mated on the corresponding interpolated combina-tion of the target-side LMs.The optimized coefficient values are: for Span-ish: NC weight = 0.526, Europarl weight = 0.474;for English: NC weight = 0.503, Europarl weight= 0.497.
The perplexity results obtained usingmonolingual LMs and the 2009 development set(English and Spanish references) can be found inTable 1, while the corresponding improvement inBLEU score is presented in Section 3.3 and sum-mary of the obtained results (Table 4).Europarl NC InterpolatedEnglish 463.439 489.915 353.305Spanish 308.802 347.092 246.573Table 1: Perplexity results obtained on the Dev2009 corpus and the monolingual LMs.Note that the corresponding reordering modelsare interpolated with the same weights.2.2 Statistical Machine ReorderingThe idea of the Statistical Machine Reordering(SMR) stems from the idea of using the power-ful techniques developed for SMT and to translate86the source language (S) into a reordered sourcelanguage (S?
), which more closely matches theorder of the target language.
To infer more re-orderings, it makes use of word classes.
To cor-rectly integrate the SMT and SMR systems, bothare concatenated by using a word graph which of-fers weighted reordering hypotheses to the SMTsystem.
The details are described in (?
).2.3 Syntax-based ReorderingSyntax-based Reordering (SBR) approach dealswith the word reordering problem and is based onnon-isomorphic parse subtree transfer as describedin details in (Khalilov and R. Fonollosa, 2008).Local and long-range word reorderings aredriven by automatically extracted permutation pat-terns operating with source language constituents.Once the reordering patterns are extracted, theyare further applied to monotonize the bilingualcorpus in the same way as shown in the previ-ous subsection.
The target-side parse tree is con-sidered as a filter constraining reordering rules tothe set of patterns covered both by the source- andtarget-side subtrees.2.4 System CombinationOver the past few years the MBR algorithm uti-lization to find the best consensus outputs of dif-ferent translation systems has proved to improvethe translation accuracy (Kumar, 2004).
The sys-tem combination is performed on the 200-bestlists which are generated by the three systems:(1) MOSES-based system without pre-translationmonotonization (baseline), (2) MOSES-basedSMT enhanced with SMR monotonization and (3)MOSES-based SMT augmented with SBR mono-tonization.
The results presented in Table 4 showthat the combined output significantly outperformsthe baseline system configuration.3 Experiments and resultsWe followed the evaluation baseline instructions 1to train the MOSES-based translation system.In some experiments we used MBR decod-ing (Kumar and Byrne, 2004) with the smoothedBLEU score as a similarity criteria, that al-lowed gaining 0.2 BLEU points comparing to thestandard procedure of outputting the translationwith the highest probability (HP).
We applied theMoses implementation of this algorithm to the list1http://www.statmt.org/wmt09/baseline.htmlof 200 best translations generated by the TALP-UPC system.
The results obtained over the official2009 Test dataset can be found in Table 2.Task HP MBREsEn 24.48 24.62EnEs 23.46 23.64Table 2: MBR versus MERT decoding.The "recase" script provided within the base-line was supplemented with and additional mod-ule, which restore the original case for unknownwords (many of them are proper names and loos-ing of case information leads to a significant per-formance degradation).3.1 Language modelsThe target-side language models were estimatedusing the SRILM toolkit (Stolcke, 2002).
We triedto use all the available in-domain training mate-rial: apart from the corresponding portions of thebilingual NC corpora we involved the followingmonolingual corpora:?
News monolingual corpus (49M tokens forEnglish and 49M for Spanish)?
Europarl monolingual corpus (about 504Mtokens for English and 463M for Spanish)?
A collection of News development and testsets from previous evaluations (151K tokensfor English and 175K for Spanish)?
A collection of Europarl development andtest sets from previous evaluations (295K to-kens for English and 311K for Spanish)Five LMs per language were estimated on thecorresponding datasets and interpolated follow-ing the maximum perplexity criteria.
Hence, thelarger LMs incorporating in- and out-of-domaindata were used in decoding.3.2 Spanish enclitics separationFor the Spanish portion of the corpus we imple-mented an enclitics separation procedure on thepreprocessing step, i.e.
the pronouns attached tothe verb were separated and contractions as delor al were splitted into de el or a el.
Conse-quently, training data sparseness due to Spanishmorphology was reduced improving the perfor-mance of the overall translation system.
As a87post-processing, the segmentation was recoveredin the English-to-Spanish direction using target-side Part-of-Speech tags (de Gispert, 2006).3.3 ResultsThe automatic scores provided by the WMT?09organizers for TALP-UPC submissions calculatedover the News 2009 dataset can be found in Ta-ble 3.
BLEU and NIST case-insensitive (CI) andcase-sensitive (CS) metrics are considered.Task Bleu CI Bleu CS NIST CI NIST CSEsEn 25.93 24.54 7.275 7.017EnEs 24.85 23.37 6.963 6.689Table 3: BLEU and NIST scores for preliminaryofficial test dataset 2009 (primary submission)with 500 sentences excluded.The TALP-UPC primary submission wasranked the 3rd among 28 presented translationsfor the Spanish-to-English task and the 4th for theEnglish-to-Spanish task among 9 systems.The following system configurations and the in-ternal results obtained are reported:?
Baseline: Moses-based SMT, as proposedon the web-page of the evaluation campaignwith Spanish enclitics separation and modi-fied version of ?recase?
tool,?
Baseline+TMI: Baseline enhanced with TMinterpolation as described in subsection 2.1,?
Baseline+TMI+MBR: the same as the latterbut with MBR decoding,?
Baseline+TMI+SMR: the same as Base-line+TMI but with SMR technique applied tomonotonize the source portion of the corpus,as described in subsection 2.2,?
Baseline+SBR: the same as Baseline but withSBR algorithm applied to monotonize thesource portion of the corpus, as described insubsection 2.3,?
System Combination: a combined output ofthe 3 previous systems done with the MBRalgorithm, as described in subsection 2.4.Impact of TM interpolation and MBR decod-ing is more significant for the English-to-Spanishtranslation task, for which the target-side mono-lingual corpus is smaller than for the Spanish-to-English translation.We did not have time to meet the evalua-tion deadline for providing the system combi-nation output.
Nevertheless, during the post-evaluation period we performed the experimentsreported in the last three lines of Table 4 (Base-line+TMI+SMR, Baseline+SBR and System com-bination).Note that the results presented in Table 4 differfrom the ones which can be found the Table 3 dueto selective conditions of preliminary evaluationdone by the Shared Task organizers.System News 2009 Test CI News 2009 Test CSSpanish-to-EnglishBaseline 25.82 24.37Baseline+TMI 25.84 24.47Baseline+TMI+MBR (Primary) 26.04 24.62Baseline+SMR 24.95 23.62Baseline+SBR 24.24 22.89System combination 26.44 25.00English-to-SpanishBaseline 24.56 23.05Baseline+TMI 25.01 23.41Baseline+TMI+MBR (Primary) 25.16 23.64Baseline+SMR 24.09 22.65Baseline+SBR 23.52 22.05System combination 25.39 23.86Table 4: Experiments summary.884 ConclusionsIn this paper, we present the TALP-UPC phrase-based translation system developed for the EACL-WMT 2009 evaluation campaign.
The major nov-elties of this year are translation models interpola-tion done in linear way and combination of SMTsystems implementing different word reorderingalgorithms.
The system was ranked pretty well forboth translation tasks in which our institution hasparticipated.Unfortunately, the promising reordering tech-niques and the combination of their outputs werenot applied within the evaluation deadline, how-ever we report the obtained results in the paper.5 AcknowledgmentsThis work has been funded by the Spanish Gov-ernment under grant TEC2006-13964-C03 (AVI-VAVOZ project).ReferencesP.
Brown, J. Cocke, S. Della Pietra, V. Della Pietra,F.
Jelinek, J.D.
Lafferty, R. Mercer, and P.S.Roossin.
1990.
A statistical approach to machinetranslation.
Computational Linguistics, 16(2):79?85.A.
de Gispert.
2006.
Introducing linguistic knowledgeinto Statistical Machine Translation.
Ph.D. thesis,Universitat Polit?cnica de Catalunya, December.G.
Foster and R. Kuhn.
2007.
Mixture-model adap-tation for SMT.
In In Annual Meeting of the Asso-ciation for Computational Linguistics: Proc.
of theSecond Workshop on Statistical Machine Transla-tion (WMT), pages 128?135, Prague, Czech Repub-lic, June.M.
Khalilov and J. R. Fonollosa.
2008.
A new subtree-transfer approach to syntax-based reordering for sta-tistical machine translation.
Technical report, Uni-versitat Polit?cnica de Catalunya.Ph.
Koehn and J. Schroeder.
2007.
Experiments in do-main adaptation for statistical machine translation.In In Annual Meeting of the Association for Compu-tational Linguistics: Proc.
of the Second Workshopon Statistical Machine Translation (WMT), pages224?227, Prague, Czech Republic, June.Ph.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Con-stantin, and E. Herbst.
2007.
Moses: open-sourcetoolkit for statistical machine translation.
In Pro-ceedings of the Association for Computational Lin-guistics (ACL) 2007, pages 177?180.Sh.
Kumar and W. Byrne.
2004.
Minimum bayes-riskdecoding for statistical machine translation.
In InHLTNAACL?04, pages 169?176.Sh.
Kumar.
2004.
Minimum Bayes-Risk Techniques inAutomatic Speech Recognition and Statistical Ma-chine Translation.
Ph.D. thesis, Johns Hopkins Uni-versity.F.
Och and H. Ney.
2004.
The alignment templateapproach to statistical machine translation.
Compu-tational Linguistics, 3(4):417?449, December.M.
R. Costa-juss?
and J. R. Fonollosa.
2009.
AnNgram reordering model.
Computer Speech andLanguage.
ISSN 0885-2308, accepted for publica-tion.H.
Schwenk and Y. Est?ve.
2008.
Data selection andsmoothing in an open-source system for the 2008nist machine translation evaluation.
In Proceedingsof the Interspeech?08, pages 2727?2730, Brisbane,Australia, September.A.
Stolcke.
2002.
SRILM: an extensible languagemodeling toolkit.
In Proceedings of the Int.
Conf.on Spoken Language Processing, pages 901?904.89
