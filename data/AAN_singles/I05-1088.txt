R. Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
1017 ?
1028, 2005.?
Springer-Verlag Berlin Heidelberg 2005Extracting Terminologically Relevant Collocations in theTranslation of Chinese Monograph*Byeong-Kwu Kang, Bao-Bao Chang, Yi-Rong Chen, and Shi-Wen YuThe Institute of Computational Linguistics, Peking University, Beijing, 100871?China{kbg43, chbb, chenyr, yusw}@pku.edu.cnAbstract.
This paper suggests a methodology which is aimed to extract theterminologically relevant collocations for translation purposes.
Our basic idea isto use a hybrid method which combines the statistical method and linguisticrules.
The extraction system used in our work operated at three steps: (1)Tokenization and POS tagging of the corpus; (2) Extraction of multi-word unitsusing statistical measure; (3) Linguistic filtering to make use of syntacticpatterns and stop-word list.
As a result, hybrid method using linguistic filtersproved to be a suitable method for selecting terminological collocations, it hasconsiderably improved the precision of the extraction which is much higherthan that of purely statistical method.
In our test, hybrid method combining?Log-likelihood ratio?
and ?linguistic rules?
had the best performance in theextraction.
We believe that terminological collocations and phrases extracted inthis way, could be used effectively either to supplement existing terminologicalcollections or to be used in addition to traditional reference works.1   IntroductionCommunication between different individuals and nations is not always easy,especially when more than one language is involved.
This kind of communication caninclude translation problems, which can be solved by the translators who bridge thegap between two different languages.Through the past decade, China and Korea have been undergoing large economic,cultural exchange, which invariably affects all aspects of communication, particularlytranslation.
New international contacts, foreign investments as well as cross-culturalcommunication have caused an enormous increase in the volume of translationsproduced and required.
But by now, most of all this translation work has beenconducted by translators alone, which bears the burden of an enormous translationtask to them.In order to accomplish these tasks with maximum efficiency and quality, a newtranslation method supported by computer technology has been suggested.
MAHT,also known as computer-assisted translation involves some interaction betweentranslator and the computer.
It seems to be more suited for the needs of many*  This work has been supported by The National Basic Research Program of China(973program, No.
2004CB318102) and the 863 program (No.
2001AA114210, 2002AA117010).1018 B.-K. Kang et alorganizations which have to handle the translation of the documents.
Computer-assisted translation systems are based on ?translation memory?
and ?terminologydatabases?.
With translation memory tools, translators have immediate access toprevious translations of the text, which they can then accept or modify.Terminology management systems also can prove very useful in supportingtranslator?s work [2, 11].
Most translators use some sort of glossary or terminologydatabase, especially in the translation of the technical documents or academicmonograph.
Many translation bureaux have the collection of the terminology databases.
But time pressure and costs make it difficult to get glossary building task donefully manually.
Thus there is a pressing need for the tool which is computationallysupported.
For Chinese, other than for English, terminology management tools are notso sophisticated that they could provide wide enough coverage to be directly usablefor the translators.We are contemplating, in this article, situations where computational support issought to extract the term candidate, construct or enhance such terminologydatabases.
Our work will be more focused on the problem of terminologically relevantcollocation extraction.In order to extract multiword terms from the domain corpus, three main strategieshave been proposed in the literature.
First, linguistic rule-based systems propose toextract relevant terms by making use of parts of speech, lexicons, syntax or otherlinguistic structure [2, 4].
This methodology is language dependent rather thanlanguage independent, and the system requires highly specialized linguistictechniques to identify the possible candidate terms.
Second, purely statistical systemsextract discriminating multiword terms from the text corpora by means of associationmeasures [5, 6, 7].
As they use plain text corpora and only require the informationappearing in texts, such systems are highly flexible and extract relevant unitsindependently from the domain and the language of the input text.
Finally, hybridmethodologies define co-occurrences of interest in terms of syntactical patterns andstatistical regularities [1, 3, 9].There is no question that the term extraction work comes into play when the toolsare parameterized in such a way as to provide as much relevant material (maximizingrecall and precision), and as little ?noise?
as possible.
As seen in the literature,neither purely rule-based approach nor statistic based approach could bring anencouraging result alone[3, 4].
The main problem is the "noise".
So we need to find acombined technique for reducing this ?noise?.
In this paper, we have taken a hybridapproach which combines the linguistic rules and statistical method.
First, we applieda linguistic filter which selects candidates from the corpus.
Second, the statisticalmethod was used to extract the word class  combinations.
And then, the results ofseveral experiments were evaluated and compared with each other.2   Methodology OverviewThe basic idea in our work is that the extraction tool operates on pre-processed corpuswhich contains the results of tokenizing word and word class annotation (POS-tagging).
Figure1 contains an annotated sentence from one of the Chinese academicmonograph[18].Extracting Terminologically Relevant Collocations in the Translation 1019<s id=2>?
?/p  ?
?/n  ?
?/n  ?/u  ?
?/d  ??
?/v  ?/w  ?
?/n  ??
?/d  ?
?/a  ?/u  ?
?/v  ?/p  ?
?/n  ?
?/n  ?/p  ??
?/n  ?
?/v  ?
?/n  ?/wFig.
1.
Sample annotated text (tagged by the Peking University Tagger)And the extraction routine used in our work operated at three steps:(1)Tokenization and POS Tagging; (2)Extraction of the candidates from the corpus;(3)Linguistic  filtering(making use of syntactic patterns and stop-word list).
Theschema in Figure2 summarizes the three steps of pre-processing and extracting theterm candidate.
The extraction is automatic once the appropriate templates aredesigned.Fig.
2.
Simplified schema of term extraction from a corpus3   Statistical MethodStatistical methods in computational linguistics generally share the fundamentalapproach to language viewed as a string of characters, tokens or other units, wherepatterns are discovered on the basis of their recurrence and co-occurrence.Accordingly, when we approach the extraction of multi-word terms from a statisticalpoint of view, we initially retrieve the word sequences which are not only frequent intheir occurrence but also collocating each other.Before a statistical methodology could be developed, some characteristics of termsin Chinese had to be established.
In Chinese, the length of terms can vary from singleword to multi-words(n-gram), with the majority of entries being less than 4-worditems, usually two word items(bi-gram) (See in 4.3).
The number of n-grams with n>4Raw CorpusAnnotated  textList of extraction resultList of Term candidatefilteredStep 2.
Extraction based on statistical informationStep 3.
Linguistic  filteringStep 1.
Tokenization and POS Tagging1020 B.-K. Kang et alis very small, and the occurrence of which is also rare.
Therefore, the problems of bi-grams, tri-grams and 4-grams are primarily taken into considerations in our work.Now let us consider the correlation between two neighboring words A and B.Assuming that these two words are terminologically relevant units, we can intuitivelyexpect that they occur more often than random chance.
From a statistical point ofview, this probability can be measured by several statistical methods, such as ?co-occurrence frequency?, ?Mutual Information?, ?Dice coefficient?, ?Chi-square test?,?log-likelihood?, etc[1, 6, 15].Table 1 lists several statistical measures which have been widely used in extractingcollocations.
In table 1: XY represents any two word item?
X stands for all wordsexcept X?
N is the size of corpus?
Xf  and XP  are frequency and probability of Xrespectively?
XYf  and XYP  are frequency and probability of XY respectively?Andassuming that two words X and Y are independent of each other, the formulas arerepresented as follows:Table 1.
Statistical methods used in multi word extractionMethod FormulaFrequency(Freq) XYfMutual Information(MI) 2logXYX YPP PDice Formula(Dice)2 XYX Yff f+Log-likelihood(Log-L)( )2 log( ) ( )YXY XYfX Y X YffXY XY X Y XYP P P PP P P P?Chi-squared(Chi)2( )( )( )( )( )XY XY XY XYXY XYXY XY XY XY XY XYN f f f ff f f f f f f f?+ + + +For the purposes of this work, we used these five statistics to measure thecorrelation of neighboring words.
The statistical criterion of judgments is the value ofmeasures which can judge the probability whether they belong to the rigidcollocations or not.
From a statistical point of view, we can say that if the value ofmeasure is high, the two word combination is more likely to be a rigid collocation.And XY could be accepted as a collocation if its statistical value is larger than a giventhreshold.
Those bi-gram candidates with correlation coefficient smaller than a pre-defined threshold are considered to occur randomly and should be discarded.
Othersare sorted according to their correlation coefficient in descending order.Tri-gram and 4-gram candidates were processed in the same way.
To compute thecorrelation coefficient of all tri-grams, we just considered a tri-gram as theExtracting Terminologically Relevant Collocations in the Translation 1021combination of one bi-gram and one word, and then calculated their correlationcoefficient.
Similarly, a 4-gram was considered either as the combination of a tri-gram and a word, or the combination of two bi-grams [12].As mentioned before, our methodology was tested on pre-processed corpus whichcontained the result of word class annotation.
The extraction test was delivered on wordsequence (POS tags) combinations.
And the test corpus was a Chinese academicmonograph [18].
The size of this corpus is 0.2 million Chinese characters, includingabout 5,000 sentences.
In our test, the extraction of multi-word units was based on65,663 candidate bi-grams.
Among these candidates, when their correlation coefficientswere higher than a given threshold, they were considered as multi-word unit, and thensorted in descending order.
The results of experiment are shown in Figure3.0102030405060708090100 200 300 400 500 1000Highly valued candidatesPrecision(%)MIDICELogLChiFig.
3.
Comparison of Extraction Performance between different statistical measuresTable 2.
The sample result sorted by Chi Square value1stWord 2ndWord Chi LogL DICE MI????
7822.14 1278.48 517.581 5.28183????
4233.43 42.8348 2520 10.4636??
?
?3085.64 160.647 4560 7.59925??
?
1461.41 424.818 767.36 3.90891??
?
809.168 38.2226 844 7.66964???
?752.637 124.353 787.243 5.16173??
?
619.694 111.527 1600 5.02194??
?582.425 52.0341 516.444 6.40501??
?
549.119 286.884 17037.1 2.66906??
?336.283 58.0757 2166.67 5.13281???
296.196 52.8541 544.348 4.96744?
?
228.596 122.119 523.597 2.26671022 B.-K. Kang et alAn examination of the results first showed a significant difference in precision.Checked by hand, the precisions of Chi-square value and Log-likelihood ratio wererelatively high.
In contrast, the precisions of Mutual information and Dice formulawere not so ideal.Considering the size of the corpus and the terminological richness of the texts, thisresult is not very encouraging.
Regardless of any statistical measure, the precision andcoverage of the extraction are not so high that could be directly used in the applicationsystem.More over, as shown in table 2, the purely statistical system extracts all multi-wordunits regardless of their types, so that we can also find sequences like ???[zengjia](add)?
[le](auxiliary word)?, ???
[butong](different)?
[de](auxiliaryword)?, ??[ye](also)?
[shi](be)?, ???[zhuanhuan](change)?
[wei](become)?, etc.,for which we have no use in terminology.
Clearly the output must be thoroughlyfiltered before the result can be used in any productive way.On the whole, the somewhat disappointing outcome of the statistical methodprovoked us to rethink the methodology and tried to include more linguisticinformation in the extraction of terminology.4   Hybrid Method Combining Statistical Method and LinguisticRulesTo improve the precision and recall of the extraction system, it was decided to usetwo criteria determining whether a sequence was terminologically relevant or not.
Thefirst was to use the frequent syntactic patterns of terms.
The idea underlying thismethod is that multi-word terms are constructed according to more or less fixedsyntactic patterns, and if such patterns are identified for each language, it is possibleto extract them from a POS tagged corpus.
The second was to use a stop-word filterthat a term can never begin or end in a stop-word.
This would filter out things notrelevant with the domain-specific collocation or term.4.1   Syntactic Patterns of Terms in ChineseBefore a methodology for extracting the terminologically relevant word units could bedeveloped, some characteristics of terms in Chinese had to be established.
We wereespecially interested in the following: How many words do terms usually have inChinese?
What is the structure of multi-word units in terms of syntax and morphology?What kind of terms can be successfully retrieved by computational methods?To find answers to the above questions, an existing terminology database could beused as a sample.
Because the source text to be tested in our work is related withcomputational or linguistic domain, we selected the terminology database ofcomputational linguistics which was constructed by Peking University.
This termbank currently contains over 6,500 entries in English and Chinese.An analysis of 6,500 term entries in Chinese showed that the length of terms canvary from 1 to over 6 words, with the majority of entries being two-word items,usually a ?noun+noun?
sequence.
The second most frequent type is a single-wordterm.
As less than 5% of all entries exceed 4 words and single word terms can beExtracting Terminologically Relevant Collocations in the Translation 1023identified with the use of monolingual or bilingual dictionary 1 , we decided thatautomatic extraction should be limited to sequences of 2-4 words.0 1000 2000 3000 40001 word2 word3 word4 word5 word6 wordetc0 500 1000 1500n+nvn+nn+vnn+va+nb+nv+vv+nvn+Ngn+Ngd+vetcFig.
4.
Length of Chinese terms     Fig.
5.
Syntactic patterns of two word termsAs the next step we manually analyzed the syntactic patterns of Chinese terms andordered them according to frequency.
These patterns were needed for the second partof the experiment, the ?linguistically motivated?
filtering.
According to the analysisof the existing terms, multi-word terms have some kinds of fixed syntactic patterns.
Inmany cases, these syntactic patterns are based on the combinations of different twoword classes, such as ?noun+noun?, ?gerend verb+noun?, ?adjective+noun?,?noun+suffix?
etc.
We found that there were about 30 syntactic patterns whichcovered almost 95% in the two word combinations.
Therefore, we decided that thesepatterns could be used filtering in the extraction.
In figure 6, certain types of wordcombinations are more typical for technical vocabulary than for general language.More than three word combinations also can be divided into two small parts whosesyntactic structures are the same as those of two word terms.
For example: ?(n+n)+n?,?
(vn+n)+n?, ?
(v+n)+(n+vn)?, ?
(a+n)+(vn+n)?, etc.
Therefore when we extractedthree-word or four-word units, we didn?t set another syntactic rule for them.
We justconsidered tri-gram as the combination of one bi-gram and one word.
Similarly, 4-gram was considered as the combination of different two bi-grams.Although we admit that these syntactic patterns are typical for certain type oftechnical prose only, we don?t think that they could filter out all the irrelevant units.
If1To extract a glossary of terms from a corpus, we must first identify single-word terms.
But itmight be slightly confusing for the computer to identify the single word terms alone.
So wewould like to set aside this problem for the sake of achieving efficiency.
But we believe thatthe translator might not be troubled with single terms if he has some kind of dictionary in thetranslation of the source text.1024 B.-K. Kang et alwe extract all combinations of a certain POS-shape, additional filters are neededafterwards, to identify those combinations which are terminologically relevant.Char*Patterns={"n+n","vn+n","n+vn","n+v","a+n","b+n","v+v","v+n","vn+Ng","n+Ng","d+v","m+n","h+n","f+v","a+v","f+n","j+n","a+Ng","vn+k","b+vn","b+Ng","Ag+n","v+Ng","a+nz","vn+v","nz+n","b+k","v+k","j+n","nz+v",null};Fig.
6.
The syntactic patterns for filtering24.2   Stop-Word Filter in ChineseWhen we examine multi word units regardless of their type, we can easily find somewords which have no use in terminology.
These irrelevant or meaningless data is anoise for extracting desired data.
To resolve this problem, we can make use of the stopword list to be filtered.
In the system, it would filter out things irrelevant with thedomain-specific collocation or term.
But how can we make the set of stop words?Indeed, the stop word list is rather flexible than firmly fixed in their usage.
Wheneverthe words are frequent and meaningless in text, they can be stop words in a given task.For practical purposes, we used the word frequency data of the large technicaldomain corpora which was constructed by Beijing Language and Cultural University.In this data, we randomly selected the 2,000 words most highly frequent in theirusage.
And then we examined whether the frequent words were terminologicallyrelevant or not.
The analysis of the word data showed that 77.6% were domaindependent which could be the part of term, and 22.4% were general words.
It meansthat terminologically irrelevant words amounted to about 450 words of the highlyfrequent 2000 words in technical corpora.
The results are shown in Table 3.Table 3.
The results of analysis on the high frequency wordsFrequency TerminologicallyRelevant wordsTerminologicallyIrrelevant words Example1-100 44(44%) 56(56%) ?
(aux), ?
(be), ?
(and), ?
(at), ?
(middle), etc.101-200 58(58%) 42(42%) ??(provide),?(to),?(serveas),??
(possess), etc.201-500 229(76.3%) 71(23.7%) ?
(good),??
(for),?(some),?(only),??
(other), etc.501-1000 408(81.6%) 92(18.4%) ?
?
(quite), ?
(see), ?
?
(arose), ??
(indicate),etc.1001-2000 813(81.3%) 187(18.7%) ??(leave),??(engage),??(even),??
(need not),etcTotal 1552(77.6%) 448(22.4%)2These POS patterns are based on the tag sets of Peking University.Extracting Terminologically Relevant Collocations in the Translation 1025According to these analyzed data, we made the set of stop words which amountedto about 450 words.
And we used them for filtering out the frequent, meaninglesswords in a given text before the output can be used in any productive way.5   ExperimentsThe hybrid methods combining statistical measure and linguistic rules were tested onpre-processed corpus.
Based on the statistical method, the extraction test was limitedto the boundary of the frequent syntactic patterns first, and then filtered out by thestop word list.
Three different statistical measures were used to enhance the precisionof the extraction, such as Log-likelihood ratio, Chi-square test and Mutualinformation.
Because of the poor performance in our first test, Dice formula was notused in hybrid method any more.
Therefore, we have delivered three differentexperiments using like ?LogL + Liguistic Filter?, ?Chi + Liguistic Filter?, ?MI +Liguistic Filter?
methods.In Figure 7, we present the comparative results of precision rate among thesedifferent experiments.
In order to measure the precision rate of the result, we used thegrammatical criterion: A multi word n-gram could be considered as accurate result ifit is grammatically appropriate.
By grammatical appropriation, we refer to compoundnoun phrase or compound verb phrase, since with majority of multi-word terms havethese structures.As a result, hybrid method using linguistic filters proved to be a suitable method forselecting terminological collocations, and it has considerably improved the precision ofthe extraction.
The precision was much higher than that of purely statistical method,retrieving appropriate result almost 10%-20% higher than in the first experiment.
Inour test, hybrid method combining ?Log-likelihood ratio?
and ?linguistic rules?
had thebest performance in the extraction.
The precision was higher than 90%.
According totheir performance, the results of different experiments can be arranged like:LogL+Filter   >   Chi+Filter  >  MI+Filter  >  LogL  >  Chi  >  MI  >  Dice020406080100120100 200 300 400 500 1000Highly valued candidatesPrecision(%)MIDICELogLChiMI+FilterLogL+FilterChi+FilterFig.
7.
Comparison of Extraction Performance between statistical measures and hybrid measure1026 B.-K. Kang et alIn the analysis of the extraction data, we examined the precision of every 100multi-word candidates which sorted in descending order.
Considering the size ofcorpus, we compared the results within the highly valued 1000 candidates.
A sampleof the highly valued output is seen in Table 4.Table 4.
The sample result sorted by Log-likelihood ratio1stWord 2ndWord LogL+Filter CHI+Filter MI+Filter??
??
1026.38 3748.65 4.20189??
??
1020.43 5102.98 4.93017??
??
981.323 3651.52 4.23672??
??
899.731 7805.59 6.16647??
??
734.213 2284.06 3.76964??
???
718.016 14931.3 7.80401???
???
557.888 13569.4 8.11656??
??
537.196 2361.49 4.60008?
??
500.011 12919.7 8.26776??
??
363.259 3535.04 6.19858??
??
355.499 2053.22 5.29117?
??
345.551 6733.13 7.55539??
??
339.45 6092.73 7.41208??
??
329.536 1061.09 3.76944??
??
316.792 8130.74 8.08733As seen in Table 4, although not all these units would be considered terms in thetraditional sense of the word, most of them either contain terms or includeterminologically relevant collocations.
Besides, our extraction started from these twoword items, expanded to extract multi-word units like three word or four word units.Finally we could extract multi word units such as the following sample:Table 5.
The sample of multi-word termsTerminologically relevant unitsTwo word units?
?
?
?
(grammatical function), ?
?
?
?
(directionalcomplement),  ?????
(specification), ????
(containerclassifier), ????
(usage frequency), etc.Three word units??????
(grammatical knowledge-base), ??????
(Chinese Information Processing), ?
?
?
?
?
?
(speechrecognition system), etc.Four word units????????
(MT system design), ????????
(language information processing technology), ???????
(context free grammar), etc.On the whole, as we think that the performance of the extraction was quite good,this method could be applicable in the translation system.Extracting Terminologically Relevant Collocations in the Translation 10276   Conclusions and Future WorkThe paper presents a methodology for the extraction of terminological collocationsfrom academic documents for translation purposes.
It shows that statistical methodsare useful because they can automatically extract all the possible multi word unitsaccording to the correlation coefficient.
But the purely statistical system extracts allmulti-word units regardless of their types, so that we also find sequences which aremeaningless in terminology.
Clearly the output must be thoroughly filtered before theresult can be used in any productive way.
To improve the precision of the extractionsystem, we decided to use linguistic rules determining whether a sequence wasterminologically relevant or not.
The frequent syntactic patterns of terminology andthe stop-word list were used to filter out the irrelevant candidates.
As a consequence,hybrid method using linguistic filters proved to be a suitable method for selectingterminological collocations, and it has considerably improved the precision of theextraction.
The precision was much higher than that of purely statistical method.We believe that terminological collocations and phrases extracted in this way,could be used effectively either to supplement existing terminological collections orto be used in addition to traditional reference works.In future we envisage the development of techniques for the alignment of exacttranslation equivalents of multi-word terms in Chinese and Korean, and one way ofdoing so is by finding correspondences between syntactic patterns in both languages.Translation memory systems already store translations in a format similar to a parallelcorpus, and terminology tools already involve functions such as ?auto-translate?
thatstatistically calculate the most probable translation equivalent.
By refining thesefunctions and making them language specific, we could soon be facing a newgeneration of tools for translators.
It remains to be seen, however, whether they canreally be implemented into translation environments on broad scale.References1.
Chang Bao-Bao, Extraction of Translation Equivalent Pairs from Chinese-English ParallelCorpus, Terminology Standardization and Information Technology, pp24-29, 2002.2.
Bourigault, D. Lexter, A Natural Language Processing Tool for Terminology Extraction.In Proceedings of  7th EURALEX International Congress, 1996.3.
Daille, B.
Study and Implementation of Combined Techniques for Automatic Extraction ofTerminology.
In The balancing act combining symbolic and statistical approaches tolanguage.
MIT Press,  1995.4.
Ulrich Heid, A linguistic bootstrapping approach to the extraction of term candidates fromGerman text, http://www.ims.uni-stuttgart.de/~uli/papers.html, 2000 .5.
Sayori Shimohata, Toshiyuki Sugio, JunjiI Nagata, Retrieving Domain-SpecificCollocations By Co-Occurrences and Word Order Constraints, ComputationalIntelligence, Vol 15, pp92-100, 1999.6.
Shengfen Luo, Maosong Sun Nation,Two-Character Chinese Word Extraction Based onHybrid of Internal and Contextual Measures, 20037.
Smadja, F. Retrieving Collocations From Text: XTRACT.
In Computational Linguistics,19(1) (pp 143--177).1993.1028 B.-K. Kang et al8.
David Vogel, Using Generic Corpora to Learn Domain-Specific Terminology, Workshopon Link Analysis for Detecting Complex Behavior, 20039.
Dias, G. & Guillor?, S. & Lopes, J.G.P.
Multiword Lexical Units Extraction.
InProceedings of the International Symposium on Machine Translation and ComputerLanguage Information Processing.
Beijing, China.
1999.10.
Feng Zhi-Wei, An Introduction to Modern Terminology, Yuwen press, China, 1997.11.
Ga?l Dias etc, Combining Linguistics with Statistics for Multiword Term Extraction, InProc.
of Recherche d'Informations Assistee par Ordinateur, 2000.12.
Huang Xuan-jing & Wu Li-de & Wang Wen-xin, Statistical Acquisition of TerminologyDictionary, the Fifth Workshop on Very Large Corpora, 199713.
Jiangsheng Yu?
?
?Automatic Detection of Collocation http://icl.pku.edu.cn/yujs/ 200314.
Jong-Hoon Oh, Jae-Ho Kim, Key-Sun Choi, Automatic Term Recognition Through EMAlgorithm, http://nlplab.kaist.ac.kr/, 200315.
Patrick Schone and Daniel Jurafsky, Is Knowledge-Free Induction of Multiword UnitDictionary Headwords a Solved Problem?, In proceedings of EMNLP, 2001.16.
Philip Resnik, I. Dan Melamed, Semi-Automatic Acquisition of Domain-SpecificTranslation Lexicons, Proceedings of the fifth conference on Applied natural languageprocessing, pp 340-347, 1997.17.
Sui Zhi-Fang, Terminology Standardization using the NLP Technology, Issues in ChineseInformation Processing,pp341-352, 2003.18.
Yu Shi-wen, A Complete Specification on The Grammatical Knowledge-base ofContemporary Chinese, Qinghua Univ.
Press, 2003
