Know When to Hold 'Em: Shuffling Determinist ical ly in a Parserfor Nonconcatenat ive Grammars*Rober t  T .
Kasper ,  M ike  Ca lcagno ,  and Pau l  C.  Dav isDepartment  of Linguistics, Ohio State University222 Oxley Hall1712 Neil AvenueColumbus, OH 43210 U.S.A.Email: {kasper,calcagno,pcdavis) @ling.ohio-state.eduAbst rac tNonconcatenative constraints, uch as the shuffle re-lation, are frequently employed in grammatical nal-yses of languages that have more flexible ordering ofconstituents han English.
We show how it is pos-sible to avoid searching the large space of permuta-tions that results from a nondeterministic applica-tion of shuffle constraints.
The results of our imple-mentation demonstrate hat deterministic applica-tion of shuffle constraints yields a dramatic improve-ment in the overall performance of a head-cornerparser for German using an HPSG-style grammar.1 I n t roduct ionAlthough there has been a considerable amount ofresearch on parsing for constraint-based grammarsin the HPSG (Head-driven Phrase Structure Gram-mar) framework, most computational implementa-tions embody the limiting assumption that the con-stituents of phrases are combined only by concate-nation.
The few parsing algorithms that have beenproposed to handle more flexible linearization con-straints have not yet been applied to nontrivialgrammars using nonconcatenative constraints.
Forexample, van Noord (1991; 1994) suggests that thehead-corner parsing strategy should be particularlywell-suited for parsing with grammars that admitdiscontinuous constituency, illustrated with what hecalls a "tiny" fragment of Dutch, but his more re-cent development of the head-corner parser (van No-ord, 1997) only documents its use with purely con-catenative grammars.
The conventional wisdom hasbeen that the large search space resulting from theuse of such constraints (e.g., the shuffle relation)makes parsing too inefficient for most practical ap-plications.
On the other hand, grammatical nal-yses of languages that have more flexible orderingof constituents han English make frequent use ofconstraints of this type.
For example, in recentwork by Dowty (1996), Reape (1996), and Kathol" This  research was sponsored in part by National ScienceFoundation grant SBR-9410532, and in part by a seed grantfrom the Ohio State University Office of Research; the opin-ions expressed here are solely those of the authors.
(1995), in which linear order constraints are takento apply to domains distinct from the local treesformed by syntactic ombination, the nonconcate-native shuff le  relation is the basic operation bywhich these word order domains are formed.
Reapeand Kathol apply this approach to various flexibleword-order constructions in German.A small sampling of other nonconcatenative op-erations that have often been employed in linguisticdescriptions includes Bach's (1979) wrapping oper-ations, Pollard's (1984) head-wrapping operations,and Moortgat's (1996) extraction and infixation op-erations in (categorial) type-logical grammar.What is common to the proposals of Dowty,Reape, and Kathol, and to the particular analysisimplemented here, is the characterization f nat-ural language syntax in terms of two interrelatedbut in principle distinct sets of constraints: (a) con-straints on an unordered hierarchical structure, pro-jected from (grammatical-relational r semantic) va-lence properties of lexical items; and (b) constraintson the linear order in which elements appear.
Inthis type of framework, constraints on linear ordermay place conditions on the the relative order ofconstituents that are not siblings in the hierarchicalstructure.
To this end, we follow Reape and Katholand utilize order domains, which are associated witheach node of the hierarchical structure, and serveas the domain of application for linearization con-straints.In this paper, we show how it is possible to avoidsearching the large space of permutations that re-sults from a nondeterministic application of shuffleconstraints.
By delaying the application of shuffleconstraints until the linear position of each elementis known, and by using an efficient encoding of theportions of the input covered by each element of anorder domain, shuffle constraints can be applied de-terministically.
The results of our implementationdemonstrate hat this optimization of shuffle con-straints yields a dramatic improvement i  the overallperformance ofa head-corner parser for German.The remainder of the paper is organized as fol-lows: ?2 introduces the nonconcatenative fragment663(1) Seiner Freundin liess er ihn helfenhis(DAT) friend(FEM) allows he(NOM) him(ACC) help'He allows him to help his friend.
'(2) Hilft sie ihr schnellhelp she(NOM) her(DAT) quickly'Does she help her quickly?
'(3) Der Vater denkt dass sie ihr seinen Sohn helfen liessThe(NOM) father thinks that she(NOM) her(DAW) his(ACe) son help allows'The father thinks that she allows his son to help her.
'(4)r_decldorr~_objPHON seiner FreundinSYNSEM NPTOPO vfr dom_obj \]I zio.I' |SWSEM V| 'LTOPO cf Jdom_obj \]PHON er |SYNSEM NP| 'TOPO m/ J" dom_obj \]PHON ihn ISYNSEM NP I 'TOPO mf 1(5) .
\[,o o4\] .
.
\[ o.ow\] .
\ [ ,o .o .4Figure 1: Linear order of German clauses.dora_obj \]PHON herren ISYNSEM W /TOPO vc .IS \[DOM(\[seiner F eundin\],\[liess\],\[er\],\[ihn\],\[helfen\])\]VP \[DOM(\[seiner Freundin\],\[liess\],\[ihn\],\[hel\]en\])\] NPIVP \[DOM(\[seiner F eundin\],\[liess\],\[helfen\])\] NP erV \[DOM(\[liess\],\[helfen\])\] NP \[DOM(\[seiner\],\[lareundin\])\] ihnV V N DetI I I Iliess helfen Freundin seinerFigure 2: Hierarchical  s t ructure  of sentence (1).of German which forms the basis of our study; ?3describes the head-corner parsing algorithm that weuse in our implementation; ?4 discusses details of theimplementation, and the optimization of the shuffleconstraint isexplained in ?5; ?6 compares the perfor-mance of the optimized and non-optimized parsers.2 A German Grammar  F ragmentThe fragment is based on the analysis of Germanin Kathol's (1995) dissertation.
Kathol's approachis a variant of HPSG, which merges insights fromboth Reape's work and from descriptive accounts ofGerman syntax using topological fields (linear posi-tion classes).
The fragment covers (1) root declara-tive (verb-second) sentences, (2) polar interrogative(verb-first) clauses and (3) embedded subordinate(verb-final) clauses, as exemplified in Figure 1.The linear order of constituents in a clause is rep-resented by an order domain (DOM), which is a listof domain objects, whose relative order must satisfya set of linear precedence (LP) constraints.
The or-der domain for example (1) is shown in (4).
Noticethat each domain object contains a TOPO attribute,whose value specifies a topological field that par-tially determines the object's linear position in thelist.
Kathol defines five topological fields for Germanclauses: Vorfeld (v\]), Comp/Left Sentence Bracket(c\]), Mittelfeld (m\]), Verb Cluster/Right SentenceBracket (vc), and Nachfeld (nO).
These fields are or-dered according to the LP constraints shown in (5).The hierarchical structure of a sentence, on theother hand, is constrained by a set of immediatedominance (ID) schemata, three of which are in-cluded in our fragment: Head-Argument (where "Ar-gument" subsumes complements, subjects, and spec-ifiers), Adjunct-Head, and Marker-Head.
The Head-664Argument schema is shown below, along with theconstraints on the order domain of the mother con-stituent.
In all three schemata, the domain of a non-head daughter is compacted into a single domain ob-ject, which is shuffled together with the domain ofthe head daughter to form the domain of the mother.
(6) Head-Argument Schema (simplified)" r MEAD \[-?\] \]sv sE  Ls,.,Bo  T 171JDOM \ [ \ ]L s,.,Bc,,,T (D ID  JL \ [ \ ]A shuffle(~, compaction(~), V~)A order_constraints (V~)The hierarchical structure of (1) is shown by theunordered tree of Figure 2, where head daughtersappear on the left at each branch.
Focusing onthe NP seiner Freundin in the tree, it is compactedinto a single domain object, and must remain so,but its position is not fixed relative to the otherarguments of liess (which include the raised argu-ments of helfen).
The shuffle constraint allows thissingle, compacted omain object to be realized invarious permutations with respect o the other ar-guments, subject to the LP constraints, which areimplemented by the order_constraints predicatein (6).
Each NP  argument may be assigned eithervfor mfas its TOPO value, subject to the constraintthat root declarative clauses must contain exactlyone element in the vf field.
In this case, seiner Fre-undin is assigned vf, while the other NP  argumentsof liess are in m~ However, the following permuta-tions of (1) are also grammatical, in which er andihn are assigned to the vf field instead:(7) a. Er liess ihn seiner Freundin helfen.b.
Ihn liess er seiner Freundin helfen.Comparing the hierarchical structure in Figure 2with the linear order domain in (4), we see that somedaughters in the hierarchical structure are realizeddiscontinuously in the order domain for the clause(e.g., the verbal complex liess helfen).
In such cases,nonconcatenative constraints, such as shuffle, canprovide a more succinct analysis than concatenativerules.
This situation is quite common in languageslike German and Japanese, where word order is nottotally fixed by grammatical relations.3 Head-Corner  Pars ingThe grammar described above has a number ofproperties relevant o the choice of a parsing strat-egy.
First, as in HPSG and other constraint-basedgrammars, the lexicon is information-rich, and thecombinatory or phrase structure rules are highlyschematic.
We would thus expect a purely top-down algorithm to be inefficient for a grammar ofthis type, and it may even fail to terminate, for thesimple reason that the search space would not beadequately constrained by the highly general combi-natory rules.Second, the grammar is essentially nonconcatena-tive, i.e., constituents of the grammar may appeardiscontinuously in the string.
This suggests that astrict left-to-right or right-to-left approach may beless efficient than a bidirectional or non-directionalapproach.Lastly, the grammar is head-driven, and we wouldthus expect he most appropriate parsing algorithmto take advantage of the information that a semantichead provides.
For example, a head usually providesinformation about the remaining daughters that theparser must find, and (since the head daughter in aconstruction is in many ways similar to its mothercategory) effective top-down identification of candi-date heads should be possible.One type of parser that we believe to be partic-ularly well-suited to this type of grammar is thehead-corner parser, introduced by van Noord (1991;1994) based on one of the parsing strategies ex-plored by Kay (1989).
The head-corner parser canbe thought of as a generalization of a left-cornerparser (Rosenkrantz and Lewis-II, 1970; Matsumotoet al, 1983; Pereira and Shieber, 1987).
1The outstanding features of parsers of this typeare that they are head-driven, of course, and thatthey process the string bidirectionally, starting froma lexical head and working outward.
The key ingre-dients of the parsing algorithm are as follows:?
Each grammar rule contains a distinguisheddaughter which is identified as the head of therule.
2?
The relation head-corner is defined as the reflexiveand transitive closure of the head relation.?
In order to prove that an input string can beparsed as some (potentially complex) goal cat-egory, the parser nondeterministically selects apotential head of the string and proves that thishead is the head-corner of the goal.?
Parsing proceeds from the head, with a rule beingchosen whose head daughter can be instantiatedby the selected head word.
The other daughtersof the rule are parsed recursively in a bidirec-tional fashion, with the result being a slightlylarger head-corner.lln fact, a head-corner parser for a grammar in which thehead daughter in each rule is the leftmost daughter will func-tion as a left-corner parser.2Note that the fragment of the previous ection has thisproperty.?
665?
The process succeeds when a head-corner isconstructed which dominates the entire inputstring.4 Imp lementat ionWe have implemented the German grammar andhead-corner parsing algorithm described in ?2 and?3 using the ConTroll formalism (GStz and Meurers,1997).
ConTroll is a constraint logic programmingsystem for typed feature structures, which supportsa direct implementation f HPSG.
Several propertiesof the formalism are crucial for the approach to lin-earization that we are investigating: it does not re-quire the grammar to have a context-free backbone;it includes definite relations, enabling the definitionof nonconcatenative constraints, such as shuf f le ;and it supports delayed evaluation of constraints.The ability to control when relational contraints areevaluated is especially important in the optimiza-tion of shuffle to be discussed next (?5).
ConTrollalso allows a parsing strategy to be specified withinthe same formalism as the grammar.
3 Our imple-mentation of the head-corner parser adapts van No-ord's (1997) parser to the ConTroll environment.5 Shuf f l ing  Determin is t i ca l lyA standard efinition of the shuffle relation is givenbelow as a Prolog predicate.shuffle (unoptimized version)shuffle(IS, \[\] , \[\]).shuffle(\[XISi\], $2, \[XIS3\]) :-shuffle(SI,S2,S3).shuffle(S1, \[XIS2S, \[XIS3\]) :-shuffle(S1,S2,S3).The use of a shuffle constraint reflects the factthat several permutations of constituents may begrammatical.
If we parse in a bottom-up fashion,and the order domains of two daughter constituentsare combined as the first two arguments of shuf f le ,multiple solutions will be possible for the motherdomain (the third argument of shuf f le ) .
For ex-ample, in the structure shown earlier in Figure 2,when the domain (\[liess\],\[helfen\]) is combined withthe compacted omain element (\[seiner Freundin\]),shuf f le  will produce three solutions:(8) a.
(\[liess\],\[helfen\],\[seiner Freundin\] )b.
(\[liess\],\[seiner Freundin\],\[helfen\] )c. (\[seiner Freundin\],\[liess\],\[helfen\] )This set of possible solutions is further constrainedin two ways: it must be consistent with the linear3An interface from ConqYoll to the underlying Prolog en-vironment was also developed to support some optimizationsof the parser, such as memoization and the operations overbitstrings described in ?5.precedence constraints defined by the grammar, andit must yield a sequence of words that is identicalto the input sequence that was given to the parser.However, as it stands, the correspondence with theinput sequence is only checked after an order do-main is proposed for the entire sentence.
The or-der domains of intermediate phrases in the hierar-chical structure are not directly constrained by thegrammar, since they may involve discontinuous sub-sequences of the input sentence.
The shuffle con-straint is acting as a generator of possible order do-mains, which are then filtered first by LP constraintsand ultimately by the order of the words in the in-put sentence.
Although each possible order domainthat satisfies the LP constraints i a grammatical se-quence, it is useless, in the context of parsing, to con-sider those permutations whose order diverges fromthat of the input sentence.
In order to avoid thisvery inefficient generate-and-test behavior, we needto provide a way for the input positions covered byeach proposed constituent to be considered sooner,so that the only solutions produced by the shuffleconstraint will be those that correspond to the or-der of words in the actual input sequence.Since the portion of the input string covered byan order domain may be discontinuous, we cannotjust use a pair of endpoints for each constituent asin chart parsers or DCGs.
Instead, we adapt a tech-nique described by Reape (1991), and use bitstringcodes to represent the portions of the input coveredby each element in an order domain.
If the inputstring contains n words, the code value for each con-stituent will be a bitstring of length n. If elementi of the bitstring is 1, the constituent contains theith word of the sentence, and if element i of thebitstring is 0, the constituent does not contain theith word.
Reape uses bitstring codes for a tabularparsing algorithm, different from the head-corner al-gorithm used here, and attributes the original ideato Johnson (1985).The optimized version of the shuffle relation is de-fined below, using a notation in which the argumentsare descriptions of typed feature structures.
The ac-tual implementation f relations in the ConTroll for-malism uses a slightly different notation, but we usea more familiar Prolog-style notation here.
44Symbols beginning with an upper-case letter are vari-ables, while lower-case symbols are either attr ibute labels(when followed by ':') or the types of values (e.g., he_ l is t ) .666~, shuffle (optimized version)shuffle(\[\], \[\], \[\]).shuffle((Sl&ne_list), \[\], Sl).shuffle(\[\], (S2&ne_list), $2).shuffle(Sl, $2, S3) :-Sl=\[(code:Cl) l_\], S2=\[(code:C2) l_\],code_prec (Cl, C2, Bool),shuf f le_d (Bool, Sl, $2, S3).Y, shuffle_d(Bool, \[HI\[T1\], \[H2JT2\], List).7, Bool=true: HI precedes H2Y, Bool=false: H1 does not precede H2shuffle_d(true, \[HI{S1\], S2, \[H1\]S3\]) :-may_precede_all (H1, S2),shuffle (Sl, S2, S3).shuffle_d(false, Sl, \[H2{S2\], \[H21S3\]) :-may_pre cede_all (H2, S i),shuffle (Sl, S2, S3).This revision of the shuffle relation uses twoauxiliary relations, code_prec and shuffle_d.code_prec compares two bitstrings, and yields aboolean value indicating whether the first string pre-cedes the second (the details of the implementationare suppressed).
The result of a comparison be-tween the codes of the first element of each domain isused to determine which element must appear firstin the resulting domain.
This is implemented byusing the boolean result of the code comparison toselect a unique disjunct of the shuff le_d relation.The shuff le_d relation also incorporates an opti-mization in the checking of LP constraints.
As eachelement is shuffled into the result, it only needs to bechecked for LP acceptability with the elements of theother argument list, because the LP constraints havealready been satisfied on each of the argument do-mains.
Therefore, LP acceptability no longer needsto be checked for the entire order domain of eachphrase, and the call to o rder_const ra in ts  can beeliminated from each of the phrasal schemata.In order to achieve the desired effect of makingshuffle constraints deterministic, we must delay theirevaluation until the code attributes of the first ele-ment of each argument domain have been instanti-ated to a specific string.
Using the analogy of a cardgame, we must hold the cards (delay shuffling) untilwe know what their values are (the codes must beinstantiated).
The delayed evaluation is enforced bythe following declarations in the ConTroll system,where argn:?type specifies that evaluation shouldbe delayed until the value of the nth argument ofthe relation has a value more specific than type:delay (code_prec,(argl : @string & arg2 : @string) ).delay (shuffle_d, argl : ?bool).With the addition of CODE values to each domainelement, the input to the shuffle constraint in ourprevious example is shown below, and the uniquesolution for MDom is the one corresponding to (8c).
(9) shu~e((\[ PHON liess \] \[PHON hel/en 1LCODE 001000 ' LCODE 000001 )'( \[CODE 110000 J )' MDom)6 Performance ComparisonIn order to evaluate the reduction in the search spacethat is achieved by shuffling deterministically, theparser with the optimized shuffle constraints andthe parser with the nonoptimized constraints wereeach tested with the same grammar of German ona set of 30 sentences of varying length, complexityand clause types.
Apart from the redefinition of theshuffle relation, discussed in the previous section,the only differences between the grammars used forthe optimized and unoptimized tests are the addi-tion of CODE values for each domain element in theoptimized version and the constraints necessary topropagate these code values through the intermedi-ate structures used by the parser.A representative sample of the tested sentencesis given in Table 2 (because of space limitations,English glosses are not given, but the words haveall been glossed in ?2), and the performance r sultsfor these 12 sentences are listed in Table 1.
Foreach version of the parser, time, choice points, andcalls are reported, as follows: The time measurement(Time) 5 is the amount of CPU seconds (on a SunSPARCstation 5) required to search for all possibleparses, choice points (ChoicePts) records the num-ber of instances where more than one disjunct mayapply at the time when a constraint is resolved, andcalls (Calls) lists the number of times a constraintis unfolded.
The number of calls listed includes allconstraints evaluated by the parser, not only shuffleconstraints.
Given the nature of the ConTroll imple-mentation, the number of calls represents the mostbasic number of steps performed by the parser at alogical level.
Therefore, the most revealing compar-ison with regard to performance improvement be-tween the optimized and nonoptimized versions isthe call factor, given in the last column of Table 1.The call factor for each sentence is the number ofnonoptimized calls divided by the number of opti-mized calls.
For example, in T1, Er hilfl ihr, theversion using the nonoptimized shuffle was requiredto make 4.1 times as many calls as the version em-ploying the optimized shuffle.The deterministic shuffle had its most dramaticimpact on longer sentences and on sentences con-5The absolute time values are not very significant, be-cause the ConTroll system is currently implemented asaninterpreter unning in Prolog.
However, the relative time dif-ferences between sentences confirm that the number of callsroughly reflects the total work required by the parser.667NonoptimizedTime(sec) ChoicePtsT1 1 5.6 61T2 1 I0.0 80T3 1 24.3 199T4 1 25.0 199T5 1 51.4 299T6 2 463.5 2308T7 2 465.1 2308T8 1 305.7 1301T9 1 270.5 1187T10 1 2063.4 6916T l l  1 3368.9 8833T12 1 8355.0 19235OptimizedCalls Time(sec) ChoicePts Calls359 1.8 20 88480 3.6 29 1311362 4.9 44 2001377 5.2 45 2112757 6.2 49 24122972 32.4 209 97423080 26.6 172 8159622 52.1 228 9427201 48.0 214 102444602 253.8 859 417674703 176.5 536 2565129513 528.1 1182 4937Table 1: Compar i son  of  Resu l ts  for Se lected Sentences4.13.76.86.511.423.628.310.27.010.729.126.2 ITableT1.
Er hilft ihr.T2.
Hilft er seiner Freundin?T3.
Er hilft ihr schnell.T4.
Hilft er ihr schnell?T5.
Liess er ihr ihn helfen?T6.
Er liess ihn ihr schnell helfen.T7.
Liess er ihn ihr schnell helfen?TS.
Der Vater liess seiner Freundin seinenSohn helfen.T9.
Sie denkt dass er ihr hilft.T10.
Sie denkt dass er ihr schnell hilft.T l l .
Sie denkt dass er ihr ihn helfen liess.T12.
Sie denkt dass er seiner Freundinseinen Sohn helfen liess.2: Se lected Sentencestaining adjuncts.
For instance, in T7, a verb-firstsentence containing the adjunct schnell, the opti-mized version outperformed the nonoptimized by acall factor of 28.3.
From these results, the utilityof a deterministic shuffle constraint is clear.
In par-ticular, it should be noted that avoiding useless re-sults for shuffle constraints prunes away many largebranches from the overall search space of the parser,because shuffle constraints are imposed on each nodeof the hierarchical structure.
Since we use a largelybottom-up strategy, this means that if there are nsolutions to a shuffle constraint on some daughternode, then all of the constraints on its mother nodehave to be solved n times.
If we avoid producingn - 1 useless olutions to shuffle, then we also avoidn - 1 attempts to construct all of the ancestors tothis node in the hierarchical structure.7 Conc lus ionWe have shown that eliminating the nondetermin-ism of shuffle constraints overcomes one of the pri-mary inefficiencies of parsing for grammars that usediscontinuous order domains.
Although bitstringcodes have been used before in parsers for discon-tinuous constituents, we are not aware of any priorresearch that has demonstrated the use of this tech-nique to eliminate the nondeterminism of relationalconstraints on word order.
Additionally, we expectthat the applicability of bitstring codes is not limitedto shuffle contraints, and that the technique couldbe straightforwardly generalized for other noncon-catenative constraints.
In fact, some way of record-ing the input positions associated with each con-stituent is necessary to eliminate spurious ambigui-ties that arise when the input sentence contains morethan one occurrence of the same word (cf.
van No-ord's (1994) discussion of nonminimality).
For con-catenative grammars, each position can be repre-sented by a simple remainder of the input list, buta more general encoding, such as the bitstrings usedhere, is needed for grammars using nonconcatenativeconstraints.Re ferencesEmmon Bach.
1979.
Control in montague grammar.Linguistic Inquiry, 10:515-553.David R. Dowty.
1996.
Toward a minimalist he-ory of syntactic structure.
In Arthur Horck andWietske Sijtsma, editors, Discontinuous Con-stituency, Berlin.
Mouton de Gruyter.Thilo GStz and Walt Detmar Meurers.
1997.The ConTroll system as large grammar develop-ment platform.
In Proceedings of the Workshopon Computational Environments for Grammar668Development and Linguistic Engineering (EN-VGRAM) held at ACL-97, Madrid, Spain.Mark Johnson.
1985.
Parsing with discontinuousconstituents.
In Proceedings of the 23 ra AnnualMeeting of the Association for ComputationalLinguistics, pages 127-132, Chicago, IL, July.Andreas Kathol.
1995.
Linearization-based GermanSyntax.
Ph.D. thesis, The Ohio State University.Martin Kay.
1989.
Head-driven parsing.
In Proceed-ings of the First International Workshop on Pars-ing Technologies.
Carnegie Mellon University.Y.
Matsumoto, H. Tanaka, H. Hirakawa, H. Miyoshi,and H. Yasukawa.
1983.
BUP: a bottom up parserembedded in prolog.
New Generation Computing,1(2).Michael Moortgat.
1996.
Generalized quantifiersand discontinuous type constructors.
In ArthurHorck and Wietske Sijtsma, editors, Discontinu-ous Constituency, Berlin.
Mouton de Gruyter.Fernando C.N.
Pereira nd Stuart M. Shieber.
1987.Prolog and Natural Language Analysis.
CSLI Lec-ture Notes Number 10, Stanford, CA.Carl Pollard.
1984.
Generalized Phrase StructureGrammars, Head Grammars and Natural Lan-guage.
Ph.D. thesis, Stanford University.Michael Reape.
1991.
Parsing bounded iscontin-uous constituents: Generalizations of some com-mon algorithms.
In Proceedings of the First Com-putational Linguistics in the Netherlands Day,OTK, University of Utrecht.Mike Reape.
1996.
Getting things in order.
InArthur Horck and Wietske Sijtsma, editors, Dis-continuous Constituents.
Mouton de Gruyter,Berlin.D.J.
Rosenkrantz and P.M. Lewis-II.
1970.
Deter-ministic left corner parsing.
In IEEE Conferenceof the 11th Annual Symposium on Switching andAutomata Theory, pages 139-152.Gertjan van Noord.
1991.
Head corner parsing fordiscontinuous constituency.
In Proceedings of the29 th Annual Meeting of the Association for Com-putational Linguistics, pages 114-121, Berkeley,CA, June.Gertjan van Noord.
1994.
Head corner parsing.In C.J.
Rupp, M.A.
Rosner, and R.L.
Johnson,editors, Constraints, Language and Computation,pages 315-338.
Academic Press.Gertjan van Noord.
1997.
An efficient implemen-tation of the head-corner parser.
ComputationalLinguistics, 23(3):425-456.669
