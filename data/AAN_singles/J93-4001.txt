The Interface between Phrasal andFunctional ConstraintsJohn T. Maxwell III*Xerox Palo Alto Research CenterRonald M. Kaplan tXerox Palo Alto Research CenterMany modern grammatical formalisms divide the task of linguistic specification into a context-free component of phrasal constraints and a separate component of attribute-value orfunctionalconstraints.
Conventional methods for recognizing the strings of a language also divide into twoparts so that they can exploit the different computational properties of these components.
This?
paper focuses on the interface between these components as a source of computational complexitydistinct from the complexity internal to each.
We first analyze the common hybrid strategy inwhich a polynomial context-free parser is modified to interleave functional constraint solvingwith context-free constituent analysis.
This strategy depends on the property of monotonicity inorder to prune unnecessary computation.
We describe a number of other properties that can beexploited for computational dvantage, and we analyze some alternative interface strategies basedon them.
We present he results of preliminary experiments hat generally support our intuitiveanalyses.
A surprising outcome is that under certain circumstances an algorithm that does nopruning in the interface may perform significantly better than one that does.1.
Int roduct ionA wide range of modern grammatical formalisms divide the task of linguistic spec-ification either explicitly or implicitly into a context-free component of phrasal con-straints and a separate component of attribute-value or functional constraints.
Lexical-Functional Grammar (Kaplan and Bresnan 1982), for example, is very explicit in as-signing both a phrase structure tree and an attribute-value functional structure toevery sentence of a language.
Generalized Phrase Structure Grammar (Gazdar, Klein,Pullum, and Sag 1985) assigns a phrase structure tree whose categories are attribute-value structures.
For Functional Unification Grammar (Kay 1979) and other unificationformalisms that evolved from it (such as HPSG \[Pollard and Sag 1987\]), the phrasestructure is more implicit, showing up as the record of the control strategy that re-cursively reinstantiates the collection of attribute-value constraints from the grammar.For Definite Clause Grammars (Pereira and Warren 1980) the phrase structure is im-plicit in the unification of the concealed string-position variables and the recursivereinstantiation f the additional logic variables that carry functional information.The computational problem of recognizing whether a given string belongs to thelanguage of a grammar also divides into two parts, since it must be determined thatthe string satisfies both the phrasal and functional constraints.
These two types ofconstraints have different computational properties.
It is well known that context-freephrase structure constraints can be solved in time polynomial in the length of theinput sentence, whereas all known algorithms for solving Boolean combinations of?
3333 Coyote Hill Rd, Palo Alto, CA 94304.
E-mail: maxwell.parc@xerox.comt 3333 Coyote Hill Rd, Palo Alto, CA 94304.
E-mail: kaplan.parc@xerox.com?
1994 Association for Computational LinguisticsComputational Linguistics Volume 19, Number 4equality or unification constraints in the worst-case run in time exponential in size ofthe constraint system.There have been a number of approaches for implementing such hybrid constraintsystems.
In one approach the context-free constraints are converted to the form ofmore general functional constraints o that a general-purpose constraint satisfactionmethod can uniformly solve all constraints.
While this has the advantage of simplicityand elegance, it usually gains no advantage from the special properties of the context-free subsystem.
The original implementation for Definite Clause Grammars followedthis strategy by translating the grammar into equivalent Prolog clauses and using thegeneral Prolog interpreter to solve them.On the other hand, functional constraints of a sufficiently restricted kind can betranslated into context-free phrasal constraints and solved with special purpose mech-anisms.
This is true, for example, of all GPSG feature constraints.
In the extreme,a GPSG could be completely converted to an equivalent context-free one and pro-cessed with only phrasal mechanisms, but the fast polynomial bound may then beoverwhelmed by an enormous grammar-size constant, making this approach compu-tationally infeasible for any realistic grammar (Barton, Berwick, and Ristad 1987).More common approaches involve hybrid implementations that attempt o takeadvantage of the special computational properties of phrasal constraints while alsohandling the general expressiveness of arbitrary feature constraints.
Although thissounds good in principle, it turns out to be hard to accomplish in practice.
An ob-vious first approach, for example, is to solve the context-free constraints first usingfamiliar polynomial algorithms (Earley 1970; Kaplan 1973; Younger 1967), and thento enumerate the resulting phrase structure trees.
Their corresponding functional con-straints are solved by converting to disjunctive normal form (DNF) and using alsowell-known general purpose constraint algorithms (Nelson and Oppen 1980; Knight1989).This configuration involves a simple composition of well-understood techniquesbut has proven to be a computational disaster.
The phrasal mechanisms compute inpolynomial time a compact representation f all possible trees, each of which presentsa potentially exponential problem for the constraint solver to solve.
If the phrasalcomponent is not properly restricted, there can be an infinite number of such treesand the whole system is undecidable (Kaplan and Bresnan 1982).
But even with anappropriate restriction on valid phrase structures, such as LFG's prohibition againstnonbranching dominance chains, the number of such trees can be exponential in thelength of the sentence.
Thus, even though a context-free parser can very quickly de-termine that those trees exist, if the grammar is exponentially ambiguous then thenet effect is to produce an exponential number of potentially exponential functionalconstraint problems.This is an important observation.
There have been several successful efforts inrecent years to develop solution algorithms for Boolean combinations of functionalconstraints that are polynomial for certain special, perhaps typical, cases (Kasper 1987;Maxwell and Kaplan 1989; D6rre and Eisele 1990; Nakano 1991).
But even if the func-tional constraints could always be solved in polynomial time (for instance, if therewere no disjunctions), the simple composition of phrasal constraints and functionalconstraints would still in the worst case be exponential in sentence length.
This expo-nential does not come from either of the components independently; rather, it lies inthe interface between them.Of course, simple composition is not the only strategy for solving hybrid constraintsystems.
A typical approach involves interleaving phrasal and functional processing.The functional constraints associated with each constituent are incrementally solved572John T. Maxwell and Ronald M. Kaplan Phrasal and Functional Constraintsas the constituent is being constructed, and the constituent is discarded if those con-straints prove to be unsatisfiable.
Although this interface strategy avoids the blatantexcesses of simple composition, we show below that in the worst case it is also expo-nential in sentence length.
However, it is too early to conclude that there is no sub-exponential interface strategy, since the computational properties of this interface havenot yet been extensively investigated.
This paper maps out a space of interface possi-bilities, describes alternative strategies that can provide exponential improvements incertain common situations, and suggests a number of areas for further exploration.2.
Interleaved PruningWe begin by examining in more detail the common hybrid strategy in which a poly-nomial context-free parser is modified to interleave functional constraint solving withcontext-free constituent analysis.
All known polynomial parsers make essentiallyequivalent use of a well-formed substring table (Sheil 1976), so we can illustrate thecomputational properties of interleaved strategies in general by focusing on the famil-iar operations of active-chart parsing (Kaplan 1973; Kay 1980; Thompson 1983).
Thereare, of course, other popular parsers, such as the generalized LR(k) parser (Tomita1986); however, in the worst case these are known not to be polynomial (Johnson 1989)unless a chartlike mechanism is added (Schabes 1991), and so they raise no new inter-face issues.
Here and in the remainder of this paper we assume the restriction againstnonbranching dominance chains to guarantee t rmination of the parsing computation.2.1 The Active Chart ParserRecall that the chart in an active-chart parser contains edges that record how variousportions of the input string match the categorial sequences specified by different rules.An inactive edge spans a substring that satisfies all the categorial requirements of arule and thus represents the fact that a constituent has been completely identified.An active edge spans a substring that matches only part of a rule and representsa constituent whose daughters have only been partially identified.
An active edgemay span an empty substring at a particular string position and indicate that no rulecategories have yet been matched; such an edge represents he unconfirmed hypothesisthat a constituent of the rule's type starts at that string position.The chart is initialized by adding inactive dges corresponding to the lexical itemsand at least one empty active edge before the first word.
The active edge representsthe hypothesis that an instance of the root category starts at the beginning of the inputstring.
The computation proceeds according to the following fundamental rules: First,whenever an active edge is added to the chart, then a new edge is created for eachof the inactive dges to its right whose category can be used to extend the rule-matchone step further.
The new edge records the extended match and spans the combinedsubstrings of the active and inactive dges.
Also, for each category that can extend theactive edge, a new empty edge is created to hypothesize the existence of a constituentof that type beginning to the right of the active edge.
Second, whenever an inactiveedge is added to the chart, a new edge is similarly created for each active edge to itsleft whose rule-match can be extended by the category of the inactive edge.
Newlycreated edges are added to the chart and spawn further computations only if they arenot equivalent to edges that were added in previous teps.
Thus, in Figure 1, only onenew edge n is created for the four different ways of combining the active edges axwith the inactive dges iy.The polynomial behavior of this algorithm for a context-free grammar dependscrucially on the fact that equivalent edges are proscribed and that the number of573Computational Linguistics Volume 19, Number 41---"Figure 1Context-free edge creation.distinct edges is polynomial in sentence length.
In the context-free case, two edgesare equivalent if they span the same substring and impose exactly the same require-ments for further matching of the same rule.
The polynomial bound on the number ofdistinct edges comes from the fact that equivalence does not depend on the internalsubstructure of previously matched aughter constituents (Sheil 1976).
The chart datastructures are carefully organized to make equivalent edges easy to detect.Conceptually, the chart is only used for determining whether or not a string be-longs to the language of a context-free grammar, and by itself does not give any treesfor that string.
A parse-forest variation of the chart can be created by annotating eachedge with all of the combinations of active and inactive edges that it could comefrom (these annotations are ignored for the purpose of equivalence).
This representa-tion can be used to read out quickly each of the trees that is allowed by the grammar.Note that a parse-forest representation still only requires pace polynomial in sentencelength since there are only a polynomial number of ways for each of the edges to beconstructed out of edges with the same termination points.2.2 Augmenting the Active Chart Parser with Functional ConstraintsThe main benefit of the chart algorithm is that subtrees are not recomputed when theyare incorporated as daughters in alternative trees.
It is possible to retain this benefitwhile also allowing functional constraints to be processed as constituents are beinganalyzed.
Edges are augmented so that they also record the functional constraintsassociated with a constituent.
The constraints associated with lexical items are storedin the initial inactive dges that correspond to them.
Whenever a new edge is createdfrom an active and an inactive, its constraints are formed by conjoining together theconstraints of those edges with the constraints specified on the rule category thatmatches the inactive dge.
Having collected the constraints for each edge in this way,we know that the input string is grammatical if it is spanned by a root-categoryedge whose constraints are satisfiable.
Note that for this to be the case, the notion ofequivalence must also be augmented to take account of the constraints: two edges areequivalent now if, in addition to satisfying the conditions pecified above, they havethe same constraints (or perhaps only logically equivalent ones).These augmentations impose a potentially serious computational burden, as illus-trated in Figure 2.
Here, ~x and ~by represent the constraints associated with ax and iy,respectively.
Although we are still carrying out the steps of the polynomial context-freealgorithm, the behavior is no longer polynomial.
The constraints of an edge includethose from the particular rule-categories that match against its daughter edges, withdifferent daughter matches resulting in different constraints.
The net effect is that therecan be a different set of constraints for every way in which a particular category can574John T. Maxwell and Ronald M. Kaplan Phrasal and Functional Constraints2 ' ?2 ~ i2 " 92Figure 2Augmented edge creation.Figure 3The advantage ofpruning.be realized over a given substring.
If the phrase structure grammar is exponentiallyambiguous, there will be exponentially many ways of building at least one constituent,and there will be exponentially many edges in the chart (distinguished by their con-straints).
Thus we retain the time benefit of avoiding subtree recomputation, but thealgorithm becomes exponential in the worst case.2.3 The Advantage of PruningThis strategy has proved to be very appealing, however, because it does offer compu-tational advantages over the simple composition approach.
Under this regime everyedge, not just the spanning roots, has its own constraints, and we can therefore deter-mine the satisfiability of every edge as it is being constructed.
If the constraint systemis monotonic and the constraints for a particular edge are determined tobe unsatisfi-able, then that edge is discarded.
The effect of this is to prune from the search spaceall edges that might otherwise have been constructed from unsatisfiable ones.
This isillustrated in Figure 3, where S\[?\] denotes the solution of G and X indicates that a solu-tion is unsatisfiable.
Since ?1 is unsatisfiable, nl and n2 never get built.
Pruning nl andn2 does not eliminate any valid solutions, since we know that their constraints wouldalso have been unsatisfiable.
Thus, by incrementally gathering and solving functionalconstraints, we can potentially eliminate from later consideration a number of trees575Computational Linguistics Volume 19, Number 4exponential in sentence l ngth.
In some cases it may only take a polynomial amount ofwork to determine all solutions even though the phrasal constraints are exponentiallyambiguous.A familiar variation on the pruning strategy is to use the solutions associatedwith daughter constituents when computing a solution for a mother's constraints.This can have a significant effect, since it avoids recomputing the solutions to thedaughters' constraints in the process of solving those of the mother.
However, thereis a technical issue that needs to be addressed.
Since a daughter edge may be used bymore than one mother, its solution cannot be changed estructively without he riskof introducing cross-talk between independent mothers.
One way to avoid this is tocopy the daughter solutions before merging them together, but this can be expensive.In recent years, there has been a great deal of attention devoted to this problem, and anumber of different techniques have been advanced to reduce the amount of copying(Karttunen 1986; Wroblewski 1987; Godden 1990; Tomabechi 1991).2.4 Still ExponentialAlthough pruning can eliminate an exponential number of trees, this strategy is stillexponential in sentence length in the worst case when the grammar is exponentiallyambiguous with few constituents hat are actually pruned.
There are two cases wherefew constituents are actually pruned.
One is true ambiguity, as occurs with unrestrictedprepositional phrase attachment.
The grammar for PPs in English is well known to beexponentially ambiguous (Church and Patil 1982).
If there are no functional or semanticrestrictions on how the PPs attach, then none of the possibilities will be pruned and theinterleaved pruning strategy, just like simple composition, will produce an exponentialnumber of constituents spanning a string of prepositional phrases.The other case where few constituents are actually pruned is when most candidatesolutions are eliminated high in the tree, for example, because they are incompleterather than inconsistent.
In LFG (Kaplan and Bresnan 1982) functional constraints areincomplete when a predicate requires grammatical functions that are not realized inthe string.
(The requirement that predicate argument frames be completely filled isencoded in different but equivalent ways in other formalisms.)
This can occur when,say, a verb requires a SUBJ and an OBJ, but the tree only provides a SUBJ.
Since edgesconstructed from an incomplete dge may themselves be complete, incomplete dgescannot be discarded from the chart.In sum, although the interleaved bottom-up strategy does permit some edges to bediscarded and prunes the exponentially many trees that might be built on top of them,it does not in general eliminate the exponential explosion at the phrasal-functional in-terface.
In fact, some researchers have observed that an augmented chart, even withinterleaved pruning, may actually be worse than general constraint satisfaction algo-rithms because of the exponential space required to cache intermediate r sults (Varile,Damas, and van Noord, personal communications).3.
Exploitable PropertiesMonotonicity is one of several constraint system properties that can be exploited toproduce different interface strategies.
Other properties include independence, concise-ness, order invariance, and constraint system overlap.
In the remainder of this sectionwe discuss these properties and outline some techniques for exploiting them.
In thefollowing sections we give examples of interface algorithms that incorporate some o fthese techniques.
Finally, we compare the performance of these algorithms on a samplegrammar and some sample sentences.576John T. Maxwell and Ronald M. Kaplan Phrasal and Functional Constraints3.1 MonotonicityA system of constraints i monotonic f no deduction is ever retracted when new con-straints are conjoined.
This means that if ~ is unsatisfiable, then ~ A ~ is also unsatis-fiable for arbitrary ~, so that ?
can be completely ignored.
This property is exploited,for instance, in unification algorithms that terminate as soon as an inconsistency isdetected.
In order for this to be a useful heuristic, it must be easy to determine thatis unsatisfiable and hard to solve ~ A ~.
In the interleaved pruning strategy, deter-mining that a constituent's constraints are unsatisfiable can be expensive, but this costis often offset by the exponential number of edges that may be eliminated when aconstituent is discarded.
In general, the usefulness of the interleaved pruning strategyis determined by the fraction of edges that are pruned.3.2 IndependenceTwo systems of constraints are independent if no new constraints can be deduced whenthe systems are conjoined.
In particular, two disjunctions Vi ~i and Vj ~j are indepen-dent if there are no i, j and atomic formula X such that ~i A ~j --+ ~ and ~(~i ~ X) and~(~j --* X).
If two systems of constraints are independent, then it can be shown thattheir conjunction is satisfiable if and only if they are both satisfiable in isolation.
Thisis because there is no way of deriving false from the conjunction of any subconstraintsif false was not already implied by one of those subconstraints by itself.
Indepen-dence is most advantageous when the systems contain disjunctions, ince there is noneed to multiply into disjunctive normal form in order to determine the satisfiabilityof the conjunction.
This can save an amount of work exponential in the number ofdisjunctions, modulo the cost of determining or producing independence.One example of an algorithm that exploits independence is the context-free chartparser.
Since sister constituents are independent of each other, their satisfiability canbe determined separately.
This is what makes a context-free chart parser polynomialinstead of exponential.
There are also several disjunctive unification algorithms thatexploit independence, such as constraint unification (Hasida 1986; Nakano 1991), con-texted unification (Maxwell and Kaplan 1989), and unification based on disjunctivefeature logic (D6rre and Eisele 1990).We say that a system of constraints i in free-choice form if it is a conjunction ofindependent disjunctions and all of the disjuncts are satisfiable.
This means that wecan freely choose one disjunct from each disjunction, and the result of conjoining thesedisjuncts together is guaranteed to be satisfiable.
If recursively all of the disjunctsare also in free-choice form, then we have a nested free-choice form.
The parse-forestrepresentation for the chart discussed earlier is an example of a nested free-choiceform.
The advantage of such a form is that an exponential number of solutions (trees)can be represented in polynomial space.
In general, any system of constraints in free-choice form can produce a number of solutions exponential in the size of the system.Each solution only requires a polynomial number of disjunctive choices to produce.3.3 ConcisenessWe say that a constraint system (or solution) is concise if its size is a polynomial functionof the input that it was derived from.
Most systems of constraints that have beenconverted to DNF are not concise, since in general converting a system of constraintsto DNF produces a system that is exponential in the size of the original.
Free-choicesystems may or may not be concise.
However, the constraint systems that tend to arisein solving grammatical descriptions are often concise when kept in free-choice form.It is an important but often overlooked property of parse-forest representations ofcontext-free charts that they are concise.
All of the solutions of even an exponentially577Computational Linguistics Volume 19, Number 4ambiguous context-free grammar can be represented in a structure whose size is cubicin the size of the input string and quadratic in the size of the grammar.
So far, therehas been little attention to the problem of developing algorithms for hybrid systemsthat exploit this property of the chart.A constraint system may be made concise by factoring the constraints.
A disjunc-tion can be factored if there is a common part to all of its disjunctions.
That is, thedisjunction (A A 6~ )V (A A 62) V... (A A 6n) can be reduced to A A (61 V 62 V... 6n ).
Anotheradvantage of factoring is that under certain circumstances it can improve the effec-tiveness of the pruning and partitioning techniques mentioned above.
For instance,suppose that two disjunctions are conjoined, one with factor A and the other withfactor B, and that A A B --* FALSE.
Then if A and B are factored out and processedbefore the residual disjunctions, then the disjunctions don't have to be multiplied out.In a similar manner, if A and B are independent of the residual disjunctions, and theresidual disjunctions are also independent of each other, then factoring A and B outfirst would allow the problem to be partitioned into three independent sub-problems,and again the disjunctions would not have to be multiplied out.
Thus under somecircumstances, factoring can save an exponential mount of work.
In Section 5 wediscuss an interface algorithm based on factoring.3.4 Order InvariancePhrasal constraint systems and functional constraint systems commonly used for lin-guistic description have the property that they can be processed in any order withoutchanging the final result.
Although the order in which the constraints are processeddoesn't change the result in any way, it can have a dramatic impact on how quicklysolutions can be found or non-solutions discarded.
Unfortunately, we do not knowin advance which order will find solutions or discard non-solutions in the shortestamount of time, and so we depend on heuristics that choose an order that is thoughtmore likely to evaluate solutions quickly.
The question of processing order can be bro-ken down into three parts: the order in which functional constraints are processed, theorder in which phrasal constraints are processed, and the order in which functionaland phrasal constraints are processed relative to one another.There has been a lot of effort directed toward finding the best order for processingfunctional constraints.
Kasper observed that separating constraints into disjunctive andnondisjunctive parts and processing the nondisjunctive constraints first can improveperformance when the nondisjunctive constraints are unsatisfiable (Kasper 1987).
Ithas also been observed that the order in which features are unified can have an effect,and that it is better to unify morpho-syntactic features before structural features.
Bothof these approaches reorder the constraints so that pruning is more effective, takingadvantage of the monotonicity of functional constraints.Research in context-free parsing has led to methods that can process phrasal con-straints in any order and still maintain a polynomial time bound (e.g., Sheil 1976).However, in an interleaved strategy the order in which phrasal constraints are eval-uated can make a substantial performance difference.
This is because it determinesthe order in which the functional constraints are processed.
The particular interleavedstrategy discussed above effectively builds constituents and thus solves functionalconstraints in a bottom-up order.
An alternative strategy might build constituents op-down and prune daughters whenever the collection of top-down functional constraintsare unsatisfiable.
It is also possible to process constituents in a head-driven order (Kay1989) or to utilize an opportunistic slands-of-certainty heuristic (Stock, Falcone, andInsinnamo 1988).578John T. Maxwell and Ronald M. Kaplan Phrasal and Functional Constraints: {SD2 A ~\], S\[42 A ~2\]}Figure 4Noninterleaved pruning.The relative processing order of phrasal and functional constraints i not as well-studied.
There has been relatively uncritical acceptance of the basic interleaved ar-rangement.
Another possibility might be to process all of the functional constraintsbefore the phrasal constraints.
An example of this kind of strategy is a semantic-drivenalgorithm, where subjects and objects are chosen from the string for their semanticproperties, and then phrasal constraints are checked to determine whether the con-nection makes sense syntactically.
In Section 4 we describe still another algorithm inwhich all of the phrasal constraints are processed before any of the functional con-straints and discuss the advantages of this order.3.5 Constraint System OverlapAs we mentioned in the introduction, the division between phrasal and functionalconstraints i  somewhat fluid.
All phrasal constraints can be converted into functionalconstraints, and some functional constraints can be converted into phrasal constraints.Turning all of the phrasal constraints into functional constraints obscures their specialcomputational properties.
On the other hand, turning all of the functional constraintsinto phrasal constraints i impractical even when possible because of the huge gram-mar that usually results.
So it seems that the ideal is somewhere inbetween, but where?In Section 7, we observe that moving the boundary between phrasal and functionalconstraints can have a striking computational dvantage in some cases.4.
Noninterleaved PruningWe now consider a pruning strategy that does not interleave the processing of phrasaland functional constraints.
Instead, all of the phrasal constraints are processed first,and then all of the functional constraints are collected and processed.
This takes ad-vantage of the fact that our constraint systems are order-invariant.
In the first step, anunmodified context-free chart parser processes the phrasal constraints and producesa parse-forest representation f all the legal trees.
In the second step, the parse-forestis traversed in a recursive descent starting from the root-spanning edge.
At each edgein the parse forest he solutions of the daughter edges are first determined recursivelyand then combined to produce solutions for the mother edge.
For each way that theedge can be constructed, the daughter solutions of that way are conjoined and solved.If a daughter edge has no solutions, then there is no need to extract he solutions ofany remaining sisters.
The resulting set of solutions is cached on the mother in casethe mother is also part of another tree.
This process is illustrated in Figure 4.
Note579Computational LinguisticsTc IsFigure 5Parse forest.Volume 19, Number 4Bill saw the girl with the telescopethat this strategy differs from simple composition in that the functional componentoperates on edges in the chart rather than individually enumerated trees.The first step of this strategy is polynomial in sentence length since we can usea context-free algorithm that does not accumulate constraints for each constituent.The second step may be exponential since it does accumulate constraints for eachedge and the constraints can encode all possible sub-trees for that edge.
However,this method filters the functional computation using the global well-formedness ofthe phrase structure constraints.
The performance can be significantly better than aninterleaved approach if an exponentially ambiguous ub-tree fits into no completeparse tree.
The disadvantage of this approach is that edges that might have beeneliminated by the functional constraints have to be processed by the chart parser.However, this can at most add a polynomial amount of work, since the chart parseris in the worst case polynomial.
Of course, this approach still incurs the overhead ofcopying, since it caches olutions on each edge.5.
Factored ExtractionWe now examine an interface algorithm that is very different from both interleavedand noninterleaved pruning.
Instead of focusing on pruning, this strategy focuseson factoring.
We call this strategy a factored extraction strategy because it extracts aconcise set of functional constraints from a chart and then passes the constraints toa constraint solver.
Unlike the pruning strategies, constraints are not solved on anedge-by-edge basis: only the constraints for the spanning root edge are solved.
Thusthis is a noninterleaved strategy.As with the noninterleaved pruning strategy, the first step is to build a chartbased on the context-free grammar alone.
This can be done in polynomial time usingthe active chart parser, and has the advantage of filtering constituents hat are not partof some spanning tree for the sentence.The second step is to extract the system of constraints associated with the spanningroot edge.
Consider the parse forest for the sentence Bill saw the girl with the telescopegiven in Figure 5.
All of the constituents hat are not part of a spanning tree havealready been eliminated (for instance, the S that spans Bill saw the girl).
The lettersa through v represent lexical and grammatical constraints.
For instance, a stands forthe lexical constraints for Bill as an NP, and u stands for the grammatical constraint580John T. Maxwell and Ronald M. Kaplan Phrasal and Functional Constraints(fs SUBJ) = fNP(Bit0, indicating that the NP that dominates Bill is the subject of S.Structural ambiguity is represented by a bracket over the ambiguous constituents.
Inthis case, there is only one structural ambiguity, the one between the VPs that spanthe string saw the girl with the telescope.
They represent two different ways of attachingthe PP; the first attaches it to saw, and the second attaches it to girl.We extract he system of constraints for this sentence by starting from the S at thetop and conjoining the result of recursively extracting constraints from its daughters.For constituents hat are ambiguous, we disjoin the result of extracting the constraintsof the ambiguous constituents.
In addition, we cache the constraints of each node thatwe encounter, so that even if a node can be incorporated in more than one parse,we need only extract its constraints once.
Note that since we are not caching solvedconstraints, there can be no cross-talk between constituents and copying is thereforenot required.
The result of this process is a re-entrant structure that is polynomial inthe length of the string.
If the re-entrant structure were expanded, it would producethe following:aAuA\ [ (bApAcAhAdAiAqAeAIA f  A jAgAkAmAr)  V (bAsAcAhAdAiAnAeAIA f  A jAgAkAmAoAt ) \ ]  AvHowever, instead of expanding the constraints, we make them smaller by factoringcommon elements out of the disjunctions.
For instance, the b constraint is commonto both disjuncts, and hence can be factored into the conjunctive part.
Also, since thep and s constraints identically encode the relationship between the verb and the VP,they can also be factored.
In general, we factor disjunctions on a node by node basisand cache the results on each node, to avoid repeating the factoring computation.
Al-though a straight-forward implementation for factoring two sets of constraints wouldbe quadratic in the number of edges, a linear factoring algorithm is possible if the con-straints are sorted by string position and height in the tree (as they are in the exampleabove).
Factoring produces the following system of constraints:aAuAbAcAhAdAiAeAIAf AjAgAkAmApA\[(qAr) V(nAoAt)\]AvWe can make factoring even more effective by doing some simple constraint anal-ysis.
In LFG, for example, the head of a constituent is usually annotated with theconstraint T--~.
This equality means that the head can be substituted for the motherwithout affecting satisfiability.
This substitution tends to increase the number of com-mon constraints, and thus increases the potential for factoring.
In this example, q andt become the same since the NPs have the same head and n becomes tautologicallytrue since its only function is to designate the head.
This means that the disjunctioncan be reduced to just r V o:aAuAbAcAhAdAiAeAIA f  A jAgAkAmApAqA(rVo)  AvThus the resulting system of constraints i  completely conjunctive xcept for the ques-tion of where the PP attaches.
This is the ideal functional characterization for thissentence.
This approach produces an effect similar to Bear and Hobbs (1988), onlywithout requiring special mechanisms.
It also avoids the objections that Wittenburgand Barnett (1988) raise to a canonical representation for PP attachment, such as al-ways attaching low.
The only point at which special linguistic knowledge is utilized isthe last step, where constraint analysis depends on the fact that heads can be substi-tuted for mothers in LFG.
Similar head-dependent analyses may also be possible for581Computational Linguistics Volume 19, Number 4other grammatical theories, but factoring can make the constraint system substantiallysmaller even without his refinement.Factoring is advantageous whenever a node participates in all of the sub-trees ofanother node.
For example, this occurs frequently in adjunct attachment, aswe haveseen.
It also occurs when a lexical item has the same category in all the parses ofa sentence, which permits all the constraints associated with that lexical item to befactored out to the top level.
Another advantage of the extraction algorithm comesfrom the fact that it does not solve the constraints on a per-edge basis, so that copyingis not an issue for the phrasal-functional interface (although it still may be an issueinternal to some functional constraint solvers).The major disadvantage of factored extraction is that no pruning is done in theinterface.
This is left for the functional constraint solver, which may or may not knowhow to prune constraints based on their dependencies in the chart.
Without pruning,the solver may do an exponential mount of futile work.
In the next two sections wedescribe ways to get both pruning and factoring in the same algorithm.6.
Factored PruningIt is relatively easy to add factoring to the noninterleaved pruning strategy.
Rememberthat in that strategy the result of processing an edge is a disjunction of solutions, onefor each alternative sequence of daughter edges.
We can factor these solutions beforeany of them is used by higher edges (note that this is easier to do in a noninterleavedstrategy than in an interleaved one).
That is, if there are any common sub-parts, thenthe result will be a conjunction of these sub-parts with a residue of disjunctions.
Thisis very similar to the factoring in factored extraction, except hat we are no longerable to take advantage of the phrasally motivated groupings of constraints o rapidlyidentify large common sub-parts.
Instead we must factor at the level of individualconstraints, ince the solving process tends to destroy these groupings.The advantage of factored pruning over factored extraction is that we can prune,although at the cost of having to copy solutions.
In the next section we will describea complementary strategy that has the effect of adding pruning to factored extractionwithout losing its noncopying character.7.
Selective Feature MovementSo far we have examined how the properties of monotonicity, independence, con-ciseness, and order invariance can be exploited in the phrasal-functional interface.To conclude our discussion of interface strategies, we now consider how constraintsystem overlap can be exploited.
As we have noted, many functional constraints canin principle be converted to phrasal constraints.
Although converting all such func-tional constraints i  a bad idea, it can be quite advantageous to convert some of them;namely, those constraints hat would enable the context-free parser to prune the spaceof constituents.Consider a grammar with the following two rules (using LFG notation \[Kaplanand Bresnan 1982\]):S ___, S' ) NP VP~,E (T ADJUNCT) (T SUBJ) =1 T=,~(,~ COMPL) = +582John T. Maxwell and Ronald M. Kaplan Phrasal and Functional ConstraintsRuleS !
____,COMP }(1 COMPL) = + Se(T COMPL) = - T=~The first rule says that an S consists of an NP and a VP optionally preceded by an S !.The functional constraints assert hat the functional structure corresponding to the NPis the SUBJ of the one corresponding to the S, the VP's f-structure is the head, and thef-structure of the S !
is an adjunct whose COMPL feature is +.
According to the secondrule, an S !
consists of an S optionally preceded by a COMP (the e stands for the emptystring).
If the COMP is present, then the COMPL feature will be +; otherwise it willbe - .
These rules allow for sentences such as Because John kissed Sue, Mary was jealous,but exclude sentences such as *John kissed Sue, Mary was jealous.The difficulty with these rules is that they license the context-free parser to postu-late an initial S !
for a sentence such as Bill drank a few beers.
This S t will eventually beeliminated when its functional constraints are processed, because of the contradictoryconstraints on the value of the COMPL feature.
An interleaved strategy would avoidbuilding any edges on top of this spurious constituent (for example, an S with aninitial adjunct).
However, a noninterleaved strategy may build an exponential numberof unnecessary trees on top of this S !, especially if such a string is the prefix of a longersentence.
If we convert he COMPL functional requirements into equivalent phrasalones, the context-free parser will not postulate an initial S ~ for sentences like these.This can be done by splitting the S !
rule into distinct categories S~OMPL+ and S~OMPL_as follows:RuleS S~OMPL+ ) NP VP~C (T ADJUNCT) (T SUBJ) -----~, T---,~(~ COMPL) = +RuleSCOMPL+ COMP S(W COMPL) = + W=~RuleSCOMP L- e S(T COMPL) = - T=~!
With these rules the context-free parser would fail to find an SCOMPL+ in the sentenceBill drank a few beers.
Thus the S with an initial adjunct and many otherwise possibletrees would never be built.
In general, this approach notices local inconsistencies inthe grammar and changes the categories and rules to avoid encountering them.Moving features into the constituent space has the effect of increasing the num-ber of categories and rules in the grammar.
In the worst case, the size of the chartgrows linearly with the number of categories, and computation time grows quadrati-cally in the size of the grammar (Younger 1967; Earley 1970).
Just considering the costof phrasal processing, we have increased the grammar size and therefore have pre-sumably made the worst case performance worse.
However, if features are carefullyselected so as to increase the amount of pruning done by the chart, the net effect may583Computational Linguistics Volume 19, Number 4be that even though the grammar allows more types of constituents, the chart mayend up with fewer instances.It is interesting to compare this technique to the restriction proposal in Shieber(1985).
Both approaches select functional features to be moved forward in processingorder in the hope that some processing will be pruned.
Shieber's approach changesthe processing order of functional constraints so that some of them are processedtop-down instead of bottom-up.
Our approach takes a different tack, actually convert-ing some of the functional constraints into phrasal constraints.
Thus Shieber's doesits pruning using functional mechanisms whereas our approach prunes via standardphrasal operations.8.
Some Performance MeasuresIn the foregoing sections we outlined a few specific interface strategies, each of whichincorporates a different combination of techniques for exploiting particular constraintsystem properties.
We argued that each of these techniques can make a substantialperformance difference under certain circumstances.
In this section we report the re-sults of some preliminary computational comparisons that we conducted to determinewhether these techniques can make a practical difference in parsing times.
Our resultsare only suggestive because the comparisons were based on a single grammar anda small sample of sentences.
Nevertheless, the patterns we observed are interestingin part because they reinforce our intuitions but also because they lead to a deeperunderstanding of the underlying computational issues.We conducted our comparisons by first fixing a base grammar and 20 test sen-tences and then varying along three different dimensions.
The LFG grammar wasdeveloped by Jeff Goldberg and Annie Zaenen for independent purposes and cameto our attention because of its poor performance using previously implemented algo-rithms.
The test sentences were derived from a compiler textbook and are given inthe appendix.
One dimension that we explored was selective feature movement.
Weproduced a descriptively equivalent variation of the base grammar by choosing cer-tain functional constraints o move into the phrasal domain.
A second dimension wasthe choice of strategy.
We compared the interleaved pruning, noninterleaved prun-ing, factored pruning, and factored extraction strategies discussed above.
As a finaldimension we compared two different unification algorithms.8.1 Grammar VariantsThe Goldberg-Zaenen base grammar was designed to have broad coverage over a setof complex syntactic onstructions involving predicate-argument relations.
It does nothandle noun-noun compounds, and so these are hyphenated in the test sentences.The grammar was written primarily to capture linguistic generalizations, and littleattention was paid to performance issues.
We measured performance on the 20 testsentences using this grammar in its original form.
We also measured performanceon a variant of this grammar produced by converting certain function requirementsinto phrasal constraints.
We determined which constraints to move by running theinterleaved pruning strategy on the base grammar and identifying which constraintscaused constituents o be locally unsatisfiable.
We then modified the grammar andlexicon by hand so that those constraints were reflected in the categories of the con-stituents.
Examination of the results prompted us to split five categories:?
VP was split into VPINF+ and VPINF_ , where (T INF) = q- is true ofVPINFq_ , and (T INF) ~ + is true of VPINF--.584John T. Maxwell and Ronald M. Kaplan Phrasal and Functional ConstraintsTable 1Strategies and techniques.Strategy Interleaving Per-edge solving Pruning FactoringSimple composition .
.
.
.Interleaved pruning yes yes yes - -Non-interleaved pruning - -  yes yes - -Factored pruning - -  yes yes yesFactored extraction - -  - -  - -  yes?
V was split into VAUX, VOBL, MTRANS, and MOTHER, where VAUX is anauxiliary verb, MOB L is a verb with an oblique argument, VTRAN S is atransitive verb, and MOTHER is anything else.?
N was split into NOBL+ and INIOBL_, where NOBL+ takes an obliqueargument and NOB L_ does not.?
COMP was split into COMPcoMPL + and COMPcoMPL-, whereCOMPcoMPL+ has (T COMPL) = q- and COMPcoMPL- has (T COMPL) = - .?
PP was split into PPPRED and PPPCASE, where PPPRED has a predicate andPPPcASE has a PCASE (is used as an oblique argument).All of these splits were into mutually exclusive classes.
For instance, in the PP caseevery use of a preposition in the grammar had either a PCASE or a predicate but notboth.8.2 Strategy VariantsTable 1 summarizes the combination of techniques used in the strategies we havementioned in this paper.
The simple composition strategy is the naive first imple-mentation discussed in the introduction; it is included in the table only as a pointof reference.
Factored extraction is the only other interface strategy that does not doper-edge solving and caching, and therefore does not require a special copying algo-rithm.
Obviously, the listed strategies do not instantiate all possible combinations ofthe techniques we have outlined.
In all the strategies we use an active chart parser forthe phrasal component.8.3 Unifier VariantsUnification is a standard technique for determining the satisfiability of and buildingattribute-value models for systems of functional constraints with equality.
In recentyears there has been a considerable amount of research devoted to the development ofunification algorithms that perform well when confronted with disjunctive constraintsystems (Hasida 1986; Maxwell and Kaplan 1989; D6rre and Eisele 1990; Nakano 1991).Some of these unifiers take advantage of the same properties of constraint systems thatwe have discussed in this paper.
For example, Kasper's algorithm takes advantage ofmonotonicity and order invariance to achieve improved performance when pruning ispossible.
It works by first determining the satisfiability of the conjunctive constraints,and then checking disjuncts one at a time to find those that are inconsistent withthe conjunctive part.
Finally, the disjuncts that remain are multiplied into DNF.
Ourcontexted unification algorithm (Maxwell and Kaplan 1989) also allows for pruning but585Computational Linguistics Volume 19, Number 4Table 2Mean scaled computation time.Grammar Strategy Benchmark ContextedBaseModifiedInterleaved pruning 100 42Noninterleaved pruning 71 25Factored pruning --  23Factored extraction >1000 >1000Interleaved pruning 38 26Noninterleaved pruning 29 19Factored pruning - -  13Factored extraction 21 7in addition takes advantage of independence to achieve its performance.
It works byobjectifying the disjunctions so that the constraints can be put into conjunctive normalform (CNF).
This algorithm has the advantage that if disjunctions are independent,they do not have to be multiplied out.
These unifiers depend on different properties,so we have included both variants in our comparisons to see whether there are anyinteractions with the different interface strategies.
In the discussion below, we call theunifier that we implemented based on Kasper's technique the "benchmark" unifier.8.4 Resu l ts  and D iscuss ionWe implemented each of the four strategies and two unifiers in our computational en-vironment, except hat, because of resource limitations, we did not implement factoredpruning for the benchmark unifier.
We then parsed the 20 test sentences using the twogrammars for each of these configurations.
We measured the compute time for eachparse and averaged these across all the sentences.
The results are shown in Table 2.
Tomake comparisons easier, the mean times in this table have been arbitrarily scaled sothat the mean for the interleaved pruning strategy with the benchmark unifier is 100.The most striking aspect of this table is that it contains a wide range of values.We can conclude ven from this limited experiment that the properties and techniqueswe have discussed o in fact have practical significance.
The strategy in the fourthline ran much longer than we were willing to measure, while every other combinationbehaved in a quite reasonable way.
Since the fourth line is the only combinationthat does neither functional nor phrasal pruning, this demonstrates how importantpruning is.Looking at the grammar variants, we see that in all cases performance is substan-tially better for the modified grammar than for the base grammar.
This is in agreementwith Nagata 1992's finding that a medium-grain phrase structure grammar performsbetter than either a coarse-grain or fine-grain grammar.
The modified grammar in-creases the amount of pruning that is done by the chart because we carefully selectedfeatures for this effect.
The fact that this improves performance for even the pruningstrategies i perhaps urprising, since the same number of inconsistencies are beingencountered.
However, with the modified grammar the inconsistencies are being en-countered earlier, and hence prune more.
This effect is strongest for the factored ex-traction algorithm since inconsistencies are never detected by the interface; they areleft for the unifier to discover.Turning to the interface strategies, we see that noninterleaved pruning is always586John T. Maxwell and Ronald M. Kaplan Phrasal and Functional ConstraintsTable 3Maximum scaled computation time.Grammar StrategyBaseModifiedBenchmark ContextedInterleaved pruning 691 314Noninterleaved pruning 421 182Factored pruning - -  135Factored extraction >20000 >20000Interleaved pruning 112 104Noninterleaved pruning 101 74Factored pruning - -  43Factored extraction 126 15better than interleaved pruning.
This is also as expected, because the noninterleavedstrategy has the benefit of global phrasal pruning as well as incremental functionalpruning.
Nagata (1992) reports similar results with early and late unification.
Non-interleaved pruning is not as efficient as factored pruning, however.
This shows thatfactoring is an important technique once the benefits of pruning have been obtained.The factored extraction strategy exhibits the most interesting pattern of results, sinceit shows both the worst and the best performance in the table.
It gives the worstperformance with the base grammar, as discussed above.
It gives the overall bestperformance for the modified grammar with the contexted unifier.
This takes advan-tage of the best arrangement for pruning (in the chart), and its contexted unifier canbest operate on its factored constraints.
The next best performance is the combinationof factored pruning with the modified grammar and the contexted unifier.
Althoughboth strategies take advantage of factoring and pruning, factored pruning does worsebecause it must pay the cost of copying the solutions that it caches at each edge.Finally, the type of unifier also made a noticeable difference.
The contexted unifieris always faster than the benchmark one when they can be compared.
This is to beexpected because, as mentioned above, the contexted unifier both prunes and takesadvantage of independence.
The benchmark unifier only prunes.Average computing time is one way of evaluating the effects of these differentcombinations, ince it gives a rough performance estimate across a variety of differentsentences.
However, the degree of variability between sentences i  also important formany practical purposes.
A strategy with good average performance may be unac-ceptable if it takes an unpredictably large amount of time on some sentences.
Table 3,which shows the computing time of the worst sentence in each cell, gives a sense ofthe inter-sentence variability.
These values use the same scale as Table 2.This table supports roughly the same conclusions as Table 2.
There is a wide rangeof values, the modified grammar is better than the base, and the contexted unifier isfaster than the benchmark one.
In many cells, the maximum values are substantiallylarger than the corresponding means, thus indicating how sensitive these algorithmscan be to variations among sentences.
There is an encouraging result, however.
Justas the lowest mean value appears for factored extraction with the modified grammarand contexted unifier, so does the lowest maximum.
Moreover, that cell has the lowestratio of maximum to mean, almost 2.
Thus, not only is this particular combinationthe fastest, it is also much less sensitive to variations between sentences.
However,factored extraction is very sensitive to the amount of pruning done by the phrasal587Computational Linguistics Volume 19, Number 4constraints, and thus may not be the best strategy when it is impractical to performappropriate grammar modifications.
In this situation, factored pruning may be thebest choice because it is almost as fast as factored extraction but is much less sensitiveto grammar variations.9.
Concluding RemarksAs we discussed in the introduction, the interleaved pruning strategy is substantiallybetter than simple composition and so it is no surprise that it is a widely used and littlequestioned interface strategy.
However, it is only one point in a complex and multi-dimensional space of possibilities, and not necessarily the optimal point at that.
Weoutlined a number of alternative strategies, and presented preliminary measurementsto suggest that factored extraction may give better overall results, although it is verysensitive to details of the grammar.
Factored pruning also gives good results and isless sensitive to the grammar.
The good results of these two strategies how howimportant i is to take advantage both of monotonicity and independence and of thepolynomial nature of the phrasal constraints.The investigations summarized in this paper suggest several directions for futureresearch.
One direction would aim at developing a grammar compiler that automati-cally selects and moves the best set of features.
A compiler could hide this transforma-tion from the grammar developer or end user, so that it would be considered merelya performance optimization and not a change of linguistic analysis.
Another esearchdirection might focus on a way of adding functional pruning to the factored extractionalgorithm so that it would be less sensitive to variations in the grammar.At a more general level, our explorations have illustrated the richness of the spaceof phrasal-functional interface possibilities, and the potential value of examining theseissues in much greater detail.
Of course, further experimental work using other gram-mars and larger corpora are necessary to confirm the preliminary results we haveobtained.
We also need more formal analyses of the computation complexity of inter-face strategies to support he intuitive characterizations that we have presented in thispaper.
We believe that the context-free nature of phrasal constraints has not yet beenfully exploited in the construction of hybrid constraint processing systems and thatfurther esearch in this area can still lead to significant performance improvements.ReferencesBarton, G. Edward; Berwick, Robert C.; andRistad, Eric Sven (1987).
ComputationalComplexity and Natural Language.
The MITPress.Bear, John, and Hobbs, Jerry R.
(1988).
"Localizing expression of ambiguity."
InProceedings, Second Conference on AppliedNatural Language Processing.
235-241.Church, Kenneth W., and Patil, Ramesh(1982).
"Coping with syntactic ambiguityor how to put the block in the box on thetable."
Computational Linguistics, 8(3-4),139-149.D6rre, Jochen, and Eisele, Andreas (1990).
"Feature logic with disjunctiveunification."
In Proceedings, COLING-90.Earley, J.
(1970).
"An efficient context-freealgorithm."
Communications of the ACM,13, 94-102.Gazdar, Gerald; Klein, Ewan; Pullum,Geoffrey; and Sag, Ivan.
(1985).Generalized Phrase Structure Grammar.Harvard University Press.Godden, K. (1990).
"Lazy unification."
InProceedings ofthe 28th Annual Meeting of theACL.Hasida, K. (1986).
"Conditioned unificationfor natural language processing."
InProceedings ofCOLING-86, 85-87.Johnson, Mark.
(1989).
The computationalcomplexity of Tomita's algorithm."
InProceedings, International Workshop onParsing Technologies.
203-208.Kaplan, Ronald M. (1973) "Amulti-processing approach to naturallanguage."
In Proceedings, 1973 NationalComputer Conference.
Montvale, N.J.,435--440.588John T. Maxwell and Ronald M. Kaplan Phrasal and Functional ConstraintsKaplan, Ronald M., and Bresnan, Joan(1982).
"Lexical-functional grammar: Aformal system for grammaticalrepresentation."
In The MentalRepresentation f Grammatical Relations,edited by Joan Bresnan, 173-281.
MITPress.Karttunen, Lauri (1986).
"D-PATR: Adevelopment environment forunification-based grammars."
InProceedings, COLING-86, Bonn, Germany.Kasper, Robert (1987).
"A unificationmethod for disjunctive featuredescriptions."
In Proceedings, 25th AnnualMeeting of the ACL.Kay, Martin (1979).
"Functional Grammar.
"In Proceedings, 5th Annual Meeting of theBerkeley Linguistic Society.Kay, Martin (1980).
"Algorithm schemataand data structures in syntacticprocessing."
In Readings in NaturalLanguage Processing, edited by Barbara J.Grosz, Karen Sparck-Jones, andBonnie Lynn Webber, 35-70.
MorganKaufmann.Kay, Martin (1989).
"Head-driven parsing.
"In Proceedings, International Workshop onParsing Technologies.
52~2.Knight, Kevin (1989).
"Unification: Amultidisciplinary survey."
ACMComputing Surveys, 21(1), 93-124.Maxwell, John T. III, and Kaplan, Ronald M.(1989).
"An overview of disjunctiveconstraint satisfaction."
In Proceedings,International Workshop on ParsingTechnologies.Nagata, Masaaki (1992).
"An empiricalstudy on rule granularity and unificationinterleaving toward an efficientunification-based parsing system."
InProceedings, COLING-92.
177-183.Nakano, Mikio (1991).
"Constraintprojection: An efficient reatment ofdisjunctive feature descriptions."
InProceedings, 29th Annual Meeting of the ACL.307-314.Nelson, Greg, and Oppen, Derek C.
(1980).
"Fast decision procedures based oncongruence closure."
Journal of the ACM,27(3), 356-364.Pereira, Fernando C. N., and Warren, DavidH.
D. (1980).
"Definite clause grammarsfor language analysis--a survey of theformalism and a comparison withaugmented transition etworks."
ArtificialIntelligence, 13(3), 231-278.Pollard, Carl, and Sag, Ivan (1987).Information-Based Syntax and Semantics.CSLI Lecture Notes, Volume 13.
Stanford,CA.Schabes, Yves (1991).
"Polynomial time andspace shift-reduce parsing of arbitrarycontext-free grammars."
In Proceedings,29th Annual Meeting of the ACL.
106-113.Sheil, Beau (1976).
"Observations oncontext-free parsing."
In Proceedings,COLING-76.Shieber, Stuart (1985).
"Using restriction toextend parsing algorithms for complexfeature-based formalisms."
In Proceedings,23rd Annual Meeting of the ACL.Stock, Oliviero; Falcone, Rino; andInsinnamo, Patrizia (1988).
"Islandparsing and bidirectional charts."
InProceedings, COLING-88.
636-641.Thompson, Henry (1983).
"MCHART: aflexible, modular chart parsing system.
"In Proceedings, AAAI-83.
408-410.Tomabechi, Hideto (1991).
"Quasi-destructive graph unification."
InProceedings, Second International Workshopon Parsing Technology.
164-171.Tomita, Masaru (1986).
Efficient Parsing forNatural Language.
Kluwer AcademicPublishers.Wittenburg, Kent, and Barnett, Jim (1988).
"Canonical representation in NLPsystems design."
In Proceedings, SecondConference on Applied Natural LanguageProcessing.
253-259.Wroblewski, David A.
(1987).
"Nondestructive graph unification."
InProceedings, AAAI-87.Younger, D. H. (1967).
"Recognition andparsing of context-free languages in timen3. "
Information and Control, 10, 189-208.589Computational Linguistics Volume 19, Number 4Appendix A" Test Sentences1.
These normally include syntactic analyses.2.
The phases are largely independent of the target-machine.3.
Those phases depend primarily on the source-language.4.
Code-optimization is done by the front-end as well.5.
However there has been success in this direction.6.
Often the phases are collected into a front-end.7.
Generally these portions do not depend on the source-language.8.
The front-end consists of those phases that depend primarily on thesource-language.9.
If the back-end is designed carefully it may not be necessary to redesignthe back-end.10.
It produces a compiler for the same source-language on a differentmachine.11.
It has become fairly routine to take the front-end of a compiler.12.
It is not even necessary to redesign much of the back-end.13.
The front-end consists of those phases that depend primarily on thesource-language.14.
It is also tempting to compile several different languages into the sameintermediate language.15.
The back-end also includes those portions of the compiler that dependon the target-machine.16.
This matter is discussed in Chapter 9.17.
The front-end also includes the error-handling that goes along with thesephases.18.
It is tempting to use a common back-end for the different front-ends.19.
Because of subtle differences in the viewpoints of the different languagesthere has been only limited success in that direction.20.
It has become routine to redo its associated back-end to produce acompiler for the same source-language on a different machine.590
