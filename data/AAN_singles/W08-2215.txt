Resolving Paraphrases toSupport Modeling LanguagePerception in anIntelligent AgentSergei NirenburgMarjorie McShaneStephen BealeUniverstity of Maryland Baltimore County (USA)email: sergei@umbc.eduAbstractWhen interacting with humans, intelligent agents must be able not onlyto understand natural language inputs but also to remember them and linktheir content with the contents of their memory of event and object in-stances.
As inputs can come in a variety of forms, linking to memorycan be successful only when paraphrasing relations are established be-tween the meaning of new input and the content of the agent?s memory.This paper discusses a variety of types of paraphrases relevant to this taskand describes the way we implement this capability in a virtual patientapplication.179180 Nirenburg, McShane, and Beale1 Overview of and Rationale for Studying ParaphraseParaphrase, under any of its many definitions, is ubiquitous in language use.
It couldbe likened to reference, both in function and in the complexity of its detection andresolution.
Indeed, there are many ways to express a given idea in language: onecan use a canonical word/phrase (dog), a synonymous terse locution (mutt, pooch,canine, man?s best friend), or an explanatory description that can be of any length andinclude one or more specific salient features (a pet that barks; one of the two mostcommon four-legged domesticated mammals in the USA that is not a cat).
Althoughthese locutions are not semantically identical, they are functionally equivalent in manycontexts, meaning that they can permit a person or intelligent agent to carry out thesame types of reasoning.No matter which of the above locutions is used to express the idea of dog, a personor an artificial intelligent agent should be able resolve it to the concept DOG in his/itsworld model.
Such resolution, or ?anchoring?, permits other knowledge about theentity to be leveraged for reasoning: for example, the sentence Our pooch has a longtail should be construed as perfectly normal, whereas Our pooch wrote a grocery listshould be understood as impossible in its direct sense since dogs cannot be agents ofwriting.
Such incongruence should, in turn, suggest either a non-real world or the useof pooch as a nickname for some person or intelligent agent, like an automatic grocerylist writing system.1.1 Work by OthersParaphrase is a difficult problem: at its deepest, it centrally involves semantics, which,due to its inherent complexity, can be addressed only in limited ways in current NLPwork.
As a result, most contributions devoted to paraphrase can be described as syn-tactic or ?light semantic.?
In some contributions, processing semantics is constrainedto finding synonyms, hyponyms, etc., in a manually constructed word net, like Word-Net or any of its progeny.
Some others do not rely on a manually constructed knowl-edge resource but, rather, aim to determine distributional clustering of similar wordsin corpora (see, e.g.
Pereira et al (1993) or Lin (2001)).
A few approaches to dealingwith paraphrase actually go beyond the detection and use of synonyms.
For exam-ple, Lapata (2001) seeks to interpret the meanings of contextually elastic adjectives(such as fast, which means different things in fast highway and fast eater) by semi-automatically constructing paraphrases for phrases that include such adjectives.
Theseparaphrases use the original noun and the adjective (or any of its synonyms, taken froma hand-constructed list) in its adverbial form and add a corpus-derived candidate verbintended to explain the meaning of the adjective.
Results are evaluated by humanjudgments of whether a paraphrase (e.g., highway travel quickly) is appropriate as anexplanation of the meaning of fast in fast highway.Ibrahim et al (2003) pursue the more immediate goal of supporting a question-answering system.
Creating paraphrases for questions helps to expand the queriesto the textual resources that are mined for answers.
In an early version of this sys-tem, such paraphrase rules ?
which included a combination of lexical and syntactictransformations ?
were created by hand (Katz and Levin, 1988).
The new approachfollows the methodology of Lin and Pantel (2001) for dynamically determining para-phrases in a corpus bymeasuring the similarity of paths between nodes in syntactic de-Resolving Paraphrases to Support Modeling Language Perception 181pendency trees.
This method was applied to pairs of sentences from different Englishtranslations of the same text.
(The idea of using a monolingual ?sentence-aligned?corpus is due to Barzilay and McKeown (2001).)
Ibrahim et al (2003) then suggesta set of heuristics for the subsentential-level matching of nouns and pronouns whichleads to the specification of paraphrases in terms of rules such as X became a statein Y ?
X was admitted to the Union in Y.
The reported precision of the process isabout 41%, while the upper bound is given at about 65%.1 Ibrahim et al state that?question answering should be performed at the level of ?key relations?
in additionto keywords.?
We believe that it is even better to use key word senses rather thankey words, and to include key relations of a semantic and pragmatic nature ?
thoughsyntactic information should be retained as a valuable source of heuristics for spec-ifying semantic relations.
We believe that we have developed enabling technologiesand resources that allow us, at this time, to process paraphrase by relying on meaningrepresentations rather than just syntactic dependencies and text-level relations.One system that has an application area similar to ours is the one developed byBoonthum (2004).
Boonthum is developing an automatic tutoring application thatwill be enhanced by paraphrase recognition.
To process paraphrase, she automaticallyconverts natural language sentences into Conceptual Graphs (Sowa, 1983) and com-pares the graphs of two candidate paraphrases using various metrics.
This system,unlike ours, works at the level of strings (not concepts), does not automatically carryout disambiguation, and cannot handle complex sentences or long spans of text.Since paraphrase recognition, when viewed broadly, is a very challenging task,some developers choose to focus on a narrow application area.
One such system,reported in Brun and Hag?ge (2003), detects paraphrases in texts about toxic prod-ucts.
Developers hand create rules using lexical and structural information, and sys-tem output is logical structures like PHYS_FORM(acetone,liquid), which means thatthe physical form of acetone is liquid.
The approach taken in this work seems veryappropriate for this narrow domain of interest.1.2 Our research methodologyThe research methodology we adopt has the following features, which will serve toorient it in the landscape of work by others.
This methodology:?
addresses paraphrase within an application;?
takes into account the needs of question answering and ?
more broadly speak-ing ?
dialog processing;?
integrates paraphrasing due to different types of agent perception: the percep-tion of language and the perception of non-linguistic inputs, like interoception(sensitivity to stimuli originating in the body, e.g., symptoms of a disease);?
uses an agent?s memories as both the source of paraphrase detection and as thetarget to which new memories are linked; and1We believe that the low upper bound is due to the way the problem was framed.
In cases wherethe semantic differences among candidate paraphrases are important (not ?benign?
), the inter-respondentagreement, we believe, will be higher.182 Nirenburg, McShane, and Beale?
has provisions for including conceptual paraphrases, which are different waysof describing the same object or event that must be interpreted using the onto-logical knowledge available to specific agents.The initial experimentation that we are reporting covers a relatively narrow domainbut we hypothesize that the same methodology can be used in other domains, withcertain modifications related to ontological and lexical coverage.1.3 Maryland Virtual Patient (MVP)The application that drives our current research is Maryland Virtual Patient (MVP),which is an agent-oriented simulation and tutoring system.
In MVP, a human userplays the role of a physician in training who must diagnose and treat open-ended sim-ulations of patients, with or without the help of a virtual mentor agent (e.g.
McShaneet al, 2007).
The virtual patient is, itself, a ?double?
agent, comprised of: (a) a physio-logical agent that lives over time and responds in realistic ways to disease progressionand interventions, and (b) a cognitive agent that experiences symptoms, decides whento consult a physician, makes decisions about its lifestyle, treatment options, etc., andcommunicates with a human user using natural language.
The system currently coverssix diseases of the esophagus, so many of our examples will come from this subdo-main of medicine.As should be clear even from this brief overview, MVP is a reasoning-intensiveapplication.
Both physiological simulation and NLP are supported by hand-crafted,ontologically grounded knowledge that includes:1. a general purpose ontology with broad and deep coverage of medical concepts,including ontological scripts describing disease progression and treatment, theplans and goals of patients and physicians, clinical best practices, medical in-terviews and dialog in general2.
a lexiconwhose entries include a syntactic structure, a semantic structure (linkedto the ontology), and calls to procedural semantic routines (e.g., to provide forthe reference resolution of pronouns and other deictics)3. a fact repository, which is a memory of assertions, as contrasted with the ontol-ogy, which covers knowledge of types.All knowledge in the MVP environment is recorded using the metalanguage ofdescription of Ontological Semantics (Nirenburg and Raskin, 2004).
The MVP ap-plication will serve as a concrete example for the discussion of paraphrase processingin applications that include intelligent agents.
However, the analysis is readily gen-eralizable and could be applied to any system that would benefit from paraphraseunderstanding.This paper will not discuss all types of paraphrase and how OntoSem (the imple-mentation of the theory of Ontological Semantics) handles them, even though oneof the core contributions of OntoSem is the robust handling of lexical and syntacticparaphrase by automatically deriving identical meaning representations for inputs thatcontain such paraphrases (for discussion see Nirenburg and Raskin, 2004, Chapter 8).Here we focus on just a few of the more ?compositional-semantic?
types of paraphraseand our theoretical and implementation-oriented solutions to treating them.Resolving Paraphrases to Support Modeling Language Perception 1832 Paraphrase-Oriented EventualitiesEach agent in MVP is supplied with its own ontology, lexicon and fact repository(i.e., memory), which can be enriched on the fly in various ways based on the agent?sactivities ?
be they linguistic, interoceptive, or other.
In order for the language-endowed agents (on whom we focus here) to operate intelligently ?
as when answer-ing questions posed by the human user or learning new facts he presents to them ?they must be able to interpret language input, remember the content of that input,and attempt to match/link that content with memories already stored in their factrepository.
Linking new information to old memories is a standing goal of all intel-ligent agents, and in MVP it is triggered automatically for each new input.
A corecapability enabling such linking is the recognition and resolution of paraphrase.We will show how various types of paraphrase are handled as part of agent memorymanagement in the OntoSem environment.
More specifically, we focus on creatingand linking new knowledge from linguistic input, not on the use of this knowledge forreasoning.
Memory management (e.g., modeling forgetting and generalizing) is alsoa key enabling technology, but one whose description lies outside of the scope of thispaper.Having generated a meaning representation (MR) for a textual input, the intelligentagent must consider the following eventualities in deciding on how to remember thecontent of this input.
The eventualities in boldface (numbers 5, 6 and 7) are those thatwe will be discussing in some detail below.1.
The newly input MR is identical to a stored memory2.
The newly input MR is identical to a stored memory except for metadata values:the identity of the speaker, the time stamp, etc.
This can be viewed as typecoreference.
For example, in the MVP environment, if the agent coughs everyday, is every cough a new instance or is it better remembered as a generalizedaction with a given periodicity?
The answer here depends in a large part onthe event generalization capabilities of an agent (a component of its memorymanagement capabilities): indeed, even in real life one cannot be immune fromthe failure to realize that a certain sequence of events is actually better viewedas a single periodic event.3.
The newly input MR contains a subset or a superset of properties of a storedmemory.
For example, the new input can describe only the location of a symp-tom but not its severity, whereas remembered instances of this same symptommay overtly list its severity and various other properties.
Note that the informa-tion about which properties are applicable to a particular concept is stored in theontology; the memory (fact repository) contains information about those of theproperties that were overtly perceived by the agent.4.
The new input is similar to a stored memory but one or more properties hasa different value.
For example, an input could specify one level of symptomseverity while stored instances of the symptom may specify different values ofthis property.184 Nirenburg, McShane, and Beale5.
The newly input MR (or a component of it) is related to a stored memoryvia ontological subsumption, meronymy or location.6.
The newly input MR is related to a stored memory as the latter?s precondi-tion or effect.7.
The newly input MR is related to a stored memory via ?ontological para-phrase.?8.
The new input is not related to any stored memory because different conceptsare used, there are conflicting property values, etc.
For example, a symptomexperienced by somebody other than the given agent may be known to the agent,but the agent will certainly not interpret it as coreferential with knowledge aboutits own symptoms.For case 1, the new information is interpreted as confirmation of the existing mem-ory, it is not stored as a separate memory.
For case 2, the choice of storing instancesindividually or grouping them into a recurring event is determined by the agent?s mem-ory management activities.
For cases 3-8 reasoning must be carried out to determineif there is a match or not.
In our current implementation, if a stored MR unifies withthe newly input MR, the two MRs are judged to be paraphrases.
With respect to case4, this is a simplification because significant differences in values of properties in thetwo MRs under comparison should be used as heuristics voting against declaring thetwo MRs paraphrases.
However, this level of analysis requires the establishment ofa scale of relevancy on all the properties of a given concept, a task that we defer tofuture system releases.
For case 8, the new information should be stored as a newmemory.
Let us consider eventualities 5-7 in more detail.2.1 The newly input MR (or a component of it) is related to a stored memoryvia ontological subsumption, meronymy or locationThere is much variability in the use of language, which can result from lack of knowl-edge of more precise terminology or from a person?s understanding that certain kindsof underspecificity are entirely acceptable.
For example, one can say Let?s eat at yourplace rather than specifying whether we mean a house, a condominium or a studioapartment; and one can say Does your arm hurt?
rather than asking Does the brokenbone in your arm hurt or, even more specifically, Does your ulna hurt?When attempting to match new textual input with a stored memory, the question is,how close do the compared MRs have to be in order to be considered a match?
Animportant consideration when making this judgment is the application.
In the dialogapplication we are developing, the notion of sincerity conditions plays an importantrole.
That is, the VP expects the physician to ask it questions that it can answer; there-fore, it should try hard?
and search broadly, if necessary?
to come up with the clos-est memories that will permit it to generate a response.
In McShane et al (2008) wesuggest an algorithm that determines when two closely related MRs are close enoughto be considered identical.
The algorithm involves following three types of ontologicallinks ?
subsumption, meronymy and location; if a match is found within the ?lower?
(i.e., domain-specific) ontology, then the related elements are considered a paraphrase.Let us show how this paraphrase processing works using a concrete example.Resolving Paraphrases to Support Modeling Language Perception 185The physician asks the virtual patient, Do you have any discomfort in your esoph-agus?
The MR for that question is as follows.
(REQUEST-INFO-1(THEME MODALITY-1.VALUE))(MODALITY-1(TYPE EPISTEMIC)(SCOPE DISCOMFORT-1))(DISCOMFORT-1(EXPERIENCER HUMAN-1)(LOCATION ESOPHAGUS-1))(ESOPHAGUS-1(PART-OF-OBJECT HUMAN-1))The interrogativemood gives rise to the instance of request-info, whose theme is thevalue of epistemic modality that scopes over the proposition headed by DISCOMFORT-1.
(If the event actually happened, then the value of epistemic modality is 1; if it didnot happen, then the value is 0).The event DISCOMFORT is experienced by HUMAN-1, which will be linked to aspecific human (the interlocutor) via reference resolution (reference resolution is car-ried out on every referring expression in OntoSem).
The LOCATION of the DISCOM-FORT is the ESOPHAGUS of that HUMAN.If we extract the core meaning of this question, abstracting away from the interrog-ative elements, we have:(DISCOMFORT-1(EXPERIENCER HUMAN-1)(LOCATION ESOPHAGUS-1))(ESOPHAGUS-1(PART-OF-OBJECT HUMAN-1))Let us assume that the patient has stored memories about its discomfort in a differ-ent way, as an undifferentiated symptom in its chest:(SYMPTOM-1(EXPERIENCER HUMAN-1)(LOCATION CHEST-1))(CHEST-1(PART-OF-OBJECT HUMAN-1))Note that the patient stores memories of interoception directly, translating the out-put of its physiological agent into a memory that is in keeping with its own ontology.This translation is necessary because the agent?s own ontology is a ?lay?
ontology ?one lacking highly specified medical subtrees.
The ontology available to the phys-iological agent, by contrast, is an ?expert?
ontology that is rich enough in medicalknowledge to support disease simulation and treatment (see McShane et al, 2008).186 Nirenburg, McShane, and BealeThe MR components in boldface are the ones that must be matched.
DISCOMFORTis a child of SYMPTOM, forming a subsumption link of only one jump.
ESOPHAGUShas a LOCATION of CHEST, and they are both PART-OF-OBJECT the human in ques-tion.
Therefore, according to our matching algorithm ?
in conjunction with the factthat the VP assumes sincerity conditions in its conversations with the physician ?
theVP?s memory of this event sufficiently matches the physician?s question and the VPcan respond affirmatively to the question: i.e., the VP has a memory of the symptomthe physician is asking about.In discussing the next two paraphrase-oriented phenomena we will shift to a dif-ferent application area not because the medical domain lacks examples, but becauseunderstanding them would require too much background knowledge.
The applicationarea we will posit is an agent that is a personal companion, with the agent?s job beingto uphold its end of an open-ended conversation.2.2 The newly input MR is related to a stored memory as the latter?sprecondition or effectConsider the following dialog snippet between an elderly woman, Anne, and an intel-ligent agent that serves as her ?conversational companion?
:Anne: You know, my husband and I went to Rome for our honeymoon.Agent: Is that so?Anne: Yes.
We ate such great artichokes in Trastevere!As the agent participates in this conversation, it creates and stores memories of themeaning of Anne?s utterances.
In analyzing Anne?s final utterance, the agent shouldcreate a link between Anne and her husband being in Trastevere, and Anne and herhusband traveling to Rome.OntoSem produces the following core meaning representation for the statementabout traveling to Rome:(TRAVEL-EVENT-1(AGENT SET-1)(DESTINATION ROME)(TIME (< find-anchor-time)))(SET-1(MEMBERS HUMAN-1 HUMAN-2))Some details are omitted in the above structure, as they are not relevant to ourexposition here.
Find-anchor-time is a meaning procedure that triggers a search inthe metadata or in the text itself for the time of the event, which is before the time ofspeech.
The above meaning representation will be stored by the agent in its memory.The meaning representation (again, omitting some details) for the statement abouteating in Trastevere will be:(INGEST(AGENT SET-1) ; coreferential with SET-1 above(THEME ARTICHOKE-1)(LOCATION TRASTEVERE)Resolving Paraphrases to Support Modeling Language Perception 187(TIME (< find-anchor-time)))(MODALITY-1(TYPE EVALUATIVE)(SCOPE ARTICHOKE-1)(VALUE 1)(ATTRIBUTED-TO HUMAN-1))At this point the agent must check whether the above MR should be linked in theagent?s memory to the memory of the travel event processed earlier.
In this case, amatch is found ?
that is, the agent can establish that a precondition of the secondevent is among the effects of the first one.
Specifically, the linking occurs because:1. the agent?s ontology contains the description of a complex event (a script)TRAVEL-EVENT, where it is listed that an ef fect of traveling to X is being in X;2. the agent?s ontology contains the knowledge that a precondition for an INGESTevent taking place at LOCATION Y with AGENT X is that X is at LOCATION Y;3. the agent?s fact repository contains the knowledge that Trastevere is a neighbor-hood in Rome.2.3 The newly input MR is related to a stored memory via ontologicalparaphraseThe third source of paraphrase we will discuss is what we call ontological paraphrase.This occurs when more than one metalanguage representation means the same thing.In an environment with only artificial agents, where communication can be carried outwithout resorting to natural language, such paraphrase should be excluded to the extentpossible.
However, in environments (like MVP) where meaning representations canbe generated from natural language, this eventuality is more difficult to avoid.
This isbecause a) basic meaning representations are produced on the basis of lexicon entriesfor words and phrases appearing in the sentence; and b) a word or phrase can be usedin a particular sentence to render a narrower or broader meaning than it has in general.Now, in creating basic meaning representations, OntoSem uses concepts that are listedin the lexicon entries for the appropriate senses of the input words (in this paper wedo not describe OntoSem?s approach to word sense disambiguation and determinationof semantic dependencies), and these concepts cannot reflect broadening or narrowingusages of the word.
A good example of this phenomenon is the following: One cansay (a) go to London by plane or (b) fly to London, and these inputs will generatedifferent MRs:(a) (MOTION-EVENT-7 (DESTINATION London) (INSTRUMENT AIRPLANE))(b) (AERIAL-MOTION-EVENT-19 (DESTINATION London)).This is because the semantics of the appropriate sense of go is explained using theconcept MOTION-EVENT, while the semantics of the appropriate sense of fly usesAERIAL-MOTION-EVENT.
In the former structure, the head event instance is moregeneral than in the latter.
In fact, the corresponding ontological concepts stand in a188 Nirenburg, McShane, and Bealedirect subsumption relation.
If one chooses to use a concept that is higher in the onto-logical hierarchy, one may have to add further overt constraints to the meaning repre-sentation (like the one about the INSTRUMENT of the MOTION-EVENT above).
If onechooses the lower-level, narrower ontological concept to start with, such constraintsmay be inherent in its definition (as is the case with AERIAL-MOTION-EVENT).
Thispreference is the inverse of the lexical choice in text generation off of text meaningrepresentations (for details see Nirenburg and Nirenburg, 1988).OntoSem can yield either of the above basic text meaning representations.
In manyapplications ?
for example, in interlingua-based machine translation ?
this wouldbe quite benign.
However, it is possible to create extended meaning representationssuch that the above variability is eliminated.
The method we use for this purpose relieson the dynamic tightening or relaxation of selectional restrictions and is described indetail in Mahesh et al (1997).
Note that different paraphrases will still be producedfor inputs that, while referring to the same event instance, describe it with a differentdegree of vagueness or underspecificity (see Section 2.1 above).The fact that the two meaning representations above are paraphrases of one an-other can be automatically detected using a fairly simple heuristic: the ontologicaldescription of AERIAL-MOTION-EVENT includes the following property-value pairs:AERIAL-MOTION-EVENTIS-A MOTION-EVENTINSTRUMENT AIRPLANE HELICOPTER BALLOONSince the head of one of the MRs is an ancestor of the other, and the property-valuepairs in the ancestor-based MR unify with the ontological definition of the descendant(the head of the other MR), these two structures are deemed to be paraphrases.As we see from this example, world knowledge stored in the ontology is leveragedto carry out the reasoning needed to detect that the abovementioned formal structuresare paraphrases.
Such situations are somewhat similar to ?bridging references?
in theliterature devoted to reference resolution (e.g.
Poesio et al, 2004) because a knowl-edge bridge is needed to aid in the reference resolution of the entity.A common source of this type of paraphrase derives from decisions about how tobuild the ontology.
Ontology building is a complex task with ?the lesser of the evils?decisions to be made at every turn.
Two ontologies can be equally valid and yet lookquite different.
One of the most difficult aspects of ontology building is deciding whena new concept is needed.
Let us continue with the example of taking a trip.
A smallexcerpt from the MOTION-EVENT subtree of our ontology is as follows:MOTION-EVENTAERIAL-MOTIONLIQUID-CONTACT-MOTIONSURFACE-CONTACT-MOTIONTRAVEL-EVENT.
.
.As we can see, rather than having a single MOTION-EVENT lexically supplementedby property-value pairs that distinguish between types of motion, we have variousResolving Paraphrases to Support Modeling Language Perception 189types of motion being represented as different ontological concepts.
This means thatwhen different kinds of motion are referred to ?
even if they describe the same real-world event ?
they will instantiate different concepts in MR and we will be facedwith the problem of matching at the level of MR.
Whereas this matching problemcan be seen as a vote for constraining the number of ontological concepts, there arepractical reasons for not wanting to overdo this: for example, MRs are much harder toread and evaluate when lexical senses are described using property-value pairs ratherthan simply pointing to an iconic ontological concept that holds the description.
Ofcourse, the use of iconic concepts results in lower expressive power of an ontology,which affects the reasoning capabilities of agents in memory management, goal- andplan-based reasoning and the more complex cases of language understanding.2.4 Theoretical NotesThis work derives from the theoretical assumption that in order for agents to show trulyintelligent behavior their memory must be well managed.
What is actually stored as amemory, however, is a complex question.
For example, if someone were to describe atrip to New York and never referred to it as ?trip to NY?
but rather said that he ?wasin NY?
(and the interlocutor knows that he doesn?t live there), the interlocutor mightstill save the memory as(TRAVEL-EVENT-1(AGENT HUMAN-1)(DESTINATION New York)).So the ?grain size?
of memories is a compelling and complex problem.
However,we must deflect a deep study of the question What is memory, agreeing with Minsky(2006) that lingering over definitions that might never be truly precise does not supportpractical progress.Describing this work in broad terms risks conveying the impression that it is trivial,either conceptually or in terms of implementation.
In fact, both of these facets of thework are quite complex, involving extensive theoretical and practical decision-makingat every step ?
one of the reasons, perhaps, why it is not being broadly pursued.The standard counterargument that resource development for such an approach is tooexpensive does not hold up when one considers the cost of semantically annotatingcorpora and then building machine learning engines to exploit such corpora.
Anothercriticism of knowledge-based approaches is that they are too narrow in coverage.
In-deed, one cannot cover broad corpora at a deep level all at once; however, the insightsgained in carrying out this kind of work, and its potential to significantly enhance thecurrent state of the art in NLP, are really quite exciting.
And, of course, there are noa priori preferences for starting with breadth and striving for depth over the oppositestrategy of starting with depth and striving for breadth.One final point must be mentioned.
The OntoSem environment that forms thesubstrate for the work described here is used for applications that are much broaderthan MVP.
Recent applications include question-answering and information extrac-tion.
Thus, the ongoing development of the ontology, lexicon, fact repository andsemantic analyzer benefits a range of application areas.190 Nirenburg, McShane, and Beale3 State of DevelopmentWe have developed a complete simulated physiological agent that covers diseases ofthe esophagus and is capable of realistic physiological responses to even unexpectedinterventions.
We have developed a tool for the fast creation of large libraries ofphysiological agents that feature different diseases, different genetic and behavioralpredispositions and different realistic disease progressions.
We have implemented acognitive agent capable of interoception, perception through language, goal- and plan-based reasoning (within the domain of doctor-patient interaction), memory manage-ment, (simulated) physical action and (real) verbal action.
With respect to interocep-tion, we have developed a simulation of how the cognitive agent (the cognitive side ofthe ?double?
agent) perceives signals (symptoms) from its physiological agent coun-terpart.
Even though a single knowledge representation substrate is used for modelingboth agents, the interoception simulation process involves paraphrase.We have developed a set of knowledge resources covering relevant knowledgeabout the world (the ontology), past events remembered by the agent (the fact reposi-tory) and knowledge about language (represented, largely, in the agent?s lexicon).The operational OntoSem semantic analyzer is used as the basic tool for creatingmeaning representations from language inputs.
The latter already reflect results of theresolution of many kinds of paraphrase as a matter of course.Content specification for the agent?s dialog turns is addressed in the implementedgoal- and plan-based reasoning module of the cognitive agent.
Surface generation ofagent dialog turns has, at this point, been implemented in a limited fashion, on thebasis of prefabricated open textual patterns linked to types of text meaning represen-tations that are output by the content specification module.
In the next release of MVPwe intend to incorporate a text realizer such as YAG (McRoy et al, 2003).Upcoming evaluations will continue to help to debug the system and the underlyingknowledge; and, more importantly for the topic of this paper, data will be collectedfor an evaluation of the dialog component of the system.The quality of the system?s treatment of paraphrase will be judged at two levels.First, the appropriateness of the agent?s dialog responses will provide a practical,application-based measure of the quality of paraphrase processing, though blame as-signment for anymiscues will pose a complication.
Second, we will be able to directlyinspect the agent?s memory for traces of the resolution of paraphrase against remem-bered concept instances and judge the appropriateness of these results against humandecisions.
To facilitate this, we will provide textual glosses to meaning representa-tions comprising the agent?s memory.
The above regimen is the most economical wayto evaluate our approach because an important prerequisite for evaluating the perfor-mance of paraphrase resolution algorithms in our environment is the creation of thefact repository (i.e., the agent?s memory), against which the comparisons and linkingoccur.
In the proposed testing regimen, this memory will be augmented and man-aged as a result of the operation of the system itself, so that there will be no need forcreating it just for the purposes of evaluation.Resolving Paraphrases to Support Modeling Language Perception 191ReferencesBarzilay, R. and K. McKeown (2001).
Extracting paraphrases from a parallel corpus.In Proceedings of the 39th Annual Meeting of the Association for ComputationalLinguistics (ACL-2001).Boonthum, C. (2004).
iSTART: Paraphrase recognition.
In Proceedings of the StudentResearch Workshop at ACL 2004, pp.
31?36.Brun, C. and C. Hag?ge (2003).
Normalization and paraphrasing using symbolicmethods.
In Proceedings of the Second International Workshop on Paraphrasing(IWP 2003).Ibrahim, A., B. Katz, and J. Lin (2003).
Extracting structural paraphrases from alignedmonolingual corpora.
In Proceedings of the Second International Workshop onParaphrasing (IWP 2003).Katz, B. and B. Levin (1988).
Exploiting lexical regularities in designing naturallanguage systems.
In Proceedings of the 12th International Conference on Com-putational Linguistics (COLING-1988).Lapata, M. (2001).
A corpus-based account of regular polysemy: The case of context-sensitive adjectives.
In Proceedings of the Second Meeting of the North AmericanChapter of the Association for Computational Linguistics (NAACL-2001).Lin, D. (2001).
Extracting collocations from text corpora.
In Proceedings of the FirstWorkshop on Computational Terminology.Lin, D. and P. Pantel (2001).
DIRT - discovery of inference rules from text.
In Pro-ceedings of the ACM SIGKDD Conference Conference on Knowledge Discoveryand Data Mining.Mahesh, K., S. Nirenburg, and S. Beale (1997).
If you have it, flaunt it: Using fullontological knowledge for word sense disambiguation.
In Proceedings of TMI-97.McRoy, S. W., S. Channarukul, and S. S. Ali (2003).
An augmented template-basedapproach to text realization.
Natural Language Engineering 9(4), 381?420.McShane, M., S. Nirenburg, and S. Beale (2008).
Two kinds of paraphrase in model-ing embodied cognitive agents.
In Proceedings of the Naturally-Inspired ArtificialIntelligence AAAI Fall Symposium.McShane, M., S. Nirenburg, S. Beale, B. Jarrell, and G. Fantry (2007).
Knowledge-based modeling and simulation of diseases with highly differentiated clinical man-ifestations.
In Proceedings of the 11th Conference on Artificial Intelligence inMedicine (AIME 07).Minsky, M. L. (2006).
The Emotion Machine.
Simon & Schuster.Nirenburg, S. and I. Nirenburg (1988).
A framework for lexical selection in naturallanguage generation.
In Proceedings of COLING-88.192 Nirenburg, McShane, and BealeNirenburg, S. and V. Raskin (2004).
Ontological Semantics.
MIT Press.Pereira, F., N. Tishby, and L. Lee (1993).
Distributional clustering of English words.In Proceedings of the 30th Annual Meeting of the Association for ComputationalLinguistics (ACL-1991).Poesio, M., R. Mehta, A. Maroudas, and J. Hitzeman (2004).
Learning to resolvebridging references.
In Proceedings of the 42nd Annual Meeting on Associationfor Computational Linguistics.Sowa, J. F. (1983).
Conceptual Structures: Information Processing in Mind and Ma-chine.
Addison-Wesley.
