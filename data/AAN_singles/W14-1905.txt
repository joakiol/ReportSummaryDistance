Proceedings of the 5th Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 29?37,Baltimore, Maryland USA, August 26 2014. c?2014 Association for Computational LinguisticsIndividuality-preserving Voice Conversion for Articulation DisordersUsing Dictionary Selective Non-negative Matrix FactorizationRyo Aihara, Tetsuya Takiguchi, Yasuo ArikiGraduate School of System Informatics, Kobe University1-1, Rokkodai, Nada, Kobe, 6578501, Japanaihara@me.cs.scitec.kobe-u.ac.jp,takigu@kobe-u.ac.jp,ariki@kobe-u.ac.jpAbstractWe present in this paper a voice conver-sion (VC) method for a person with an ar-ticulation disorder resulting from athetoidcerebral palsy.
The movements of suchspeakers are limited by their athetoidsymptoms, and their consonants are of-ten unstable or unclear, which makes itdifficult for them to communicate.
Inthis paper, exemplar-based spectral con-version using Non-negative Matrix Factor-ization (NMF) is applied to a voice withan articulation disorder.
In order to pre-serve the speaker?s individuality, we use acombined dictionary that was constructedfrom the source speaker?s vowels and tar-get speaker?s consonants.
However, thisexemplar-based approach needs to holdall the training exemplars (frames), andit may cause mismatching of phonemesbetween input signals and selected ex-emplars.
In this paper, in order to re-duce the mismatching of phoneme align-ment, we propose a phoneme-categorizedsub-dictionary and a dictionary selec-tion method using NMF.
The effective-ness of this method was confirmed bycomparing its effectiveness with that ofa conventional Gaussian Mixture Model(GMM)-based and conventional NMF-based method.1 IntroductionIn this study, we focused on a person with anarticulation disorder resulting from the athetoidtype of cerebral palsy.
About two babies in 1,000are born with cerebral palsy (Hollegaard et al.,2013).
Cerebral palsy results from damage to thecentral nervous system, and the damage causesmovement disorders.
Cerebral palsy is classifiedinto the following types: 1)spastic, 2)athetoid,3)ataxic, 4)atonic, 5)rigid, and a mixture of thesetypes (Canale and Campbell, 2002).Athetoid symptoms develop in about 10-15% ofcerebral palsy sufferers (Hollegaard et al., 2013).In the case of a person with this type of articulationdisorder, his/her movements are sometimes moreunstable than usual.
That means their utterances(especially their consonants) are often unstable orunclear due to the athetoid symptoms.
Athetoidsymptoms also restrict the movement of their armsand legs.
Most people suffering from athetoidcerebral palsy cannot communicate by sign lan-guage or writing, so there is great need for voicesystems for them.In this paper, we propose a voice conversion(VC) method for articulation disorders.
Regard-ing speech recognition for articulation disorders,the recognition rate using a speaker-independentmodel which is trained by well-ordered speech, is3.5% (Matsumasa et al., 2009).
This result im-plies that the utterance of a person with an artic-ulation disorder is difficult to understand for peo-ple who have not communicated with them before.In recent years, people with an articulation dis-order may use slideshows and a previously syn-thesized voice when they give a lecture.
How-ever, because their movement is restricted by theirathetoid symptoms, to make slides or synthesizetheir voice in advance is hard for them.
Peo-ple with articulation disorders desire a VC sys-tem that converts their voice into a clear voicethat preserves their voice?s individuality.
Rudz-icz et al.
(Rudzicz, 2011; Rudzicz, 2014) proposedspeech adjustment method for people with articu-lation disorders based on the observations from thedatabase.In (Aihara et al., 2014), we proposedindividuality-preserving VC for articulationdisorders.
In our VC, source exemplars andtarget exemplars are extracted from the parallel29training data, having the same texts uttered bythe source and target speakers.
The input sourcesignal is expressed with a sparse representationof the source exemplars using Non-negativeMatrix Factorization (NMF).
By replacing asource speaker?s exemplar with a target speaker?sexemplar, the original speech spectrum is replacedwith the target speaker?s spectrum.
People witharticulation disorders wish to communicate bytheir own voice if they can; therefore, we pro-posed a combined-dictionary, which consists ofa source speaker?s vowels and target speaker?swell-ordered consonants.
In the voice of a personwith an articulation disorder, their consonants areoften unstable and that makes their voices unclear.Their vowels are relatively stable comparedto their consonants.
Hence, by replacing thearticulation-disordered basis of consonants only,a voice with an articulation disorder is convertedinto a non-disordered voice that preserves theindividuality of the speaker?s voice.In this paper, we propose advancedindividuality-preserving VC using NMF.
Inorder to avoid a mixture of the source and targetspectra in a converted phoneme, we applied aphoneme-categorized dictionary and a dictionaryselection method to our VC using NMF.
Inconventional NMF-based VC, the number ofdictionary frames becomes large because thedictionary holds all the training exemplar frames.Therefore, it may cause phoneme mismatchingbetween input signals and selected exemplars andsome frames of converted spectra might be mixedwith the source and target spectra.
In this paper,a training exemplar is divided into a phoneme-categorized sub-dictionary, and an input signal isconverted by using the selected sub-dictionary.The effectiveness of this method was confirmedby comparing it with a conventional NMF-basedmethod and a conventional Gaussian MixtureModel (GMM)-based method.The rest of this paper is organized as follows:In Section 2, related works are introduced.
In Sec-tion 3, the basic idea of NMF-based VC is de-scribed.
In Section 4, our proposed method is de-scribed.
In Section 5, the experimental data areevaluated, and the final section is devoted to ourconclusions.2 Related WorksVoice conversion (VC) is a technique for convert-ing specific information in speech while maintain-ing the other information in the utterance.
One ofthe most popular VC applications is speaker con-version (Stylianou et al., 1998).
In speaker con-version, a source speaker?s voice individuality ischanged to a specified target speaker?s so that theinput utterance sounds as though a specified targetspeaker had spoken it.There have also been studies on several tasksthat make use of VC.
Emotion conversion is atechnique for changing emotional information ininput speech while maintaining linguistic informa-tion and speaker individuality (Veaux and Robet,2011).
In recent years, VC has been used for auto-matic speech recognition (ASR) or speaker adap-tation in text-to-speech (TTS) systems (Kain andMacon, 1998).
These studies show the varied usesof VC.Many statistical approaches to VC have beenstudied (Valbret et al., 1992).
Among these ap-proaches, the Gaussian mixture model (GMM)-based mapping approach (Stylianou et al., 1998)is widely used.
In this approach, the conversionfunction is interpreted as the expectation valueof the target spectral envelope.
The conversionparameters are evaluated using Minimum Mean-Square Error (MMSE) on a parallel training set.A number of improvements in this approach havebeen proposed.
Toda et al.
(Toda et al., 2007)introduced dynamic features and the global vari-ance (GV) of the converted spectra over a timesequence.
Helander et al.
(Helander et al., 2010)proposed transforms based on partial least squares(PLS) in order to prevent the over-fitting problemassociated with standard multivariate regression.There have also been approaches that do not re-quire parallel data that make use of GMM adapta-tion techniques (Lee andWu, 2006) or eigen-voiceGMM (EV-GMM) (Toda et al., 2006).In the field of assistive technology, Nakamuraet al.
(Nakamura et al., 2012; Nakamura et al.,2006) proposed GMM-based VC systems that re-construct a speaker?s individuality in electrolaryn-geal speech and speech recorded by NAM micro-phones.
These systems are effective for electrola-ryngeal speech and speech recorded by NAM mi-crophones however, because these statistical ap-proaches are mainly proposed for speaker con-version, the target speaker?s individuality will be30changed to the source speaker?s individuality.
Peo-ple with articulation disorders wish to communi-cate by their own voice if they can and there is aneeds for individuality-preserving VC.Text-to-speech synthesis (TTS) is a famousvoice application that is widely researched.
Veauxet al.
(Veaux et al., 2012) used HMM-based speechsynthesis to reconstruct the voice of individu-als with degenerative speech disorders resultingfrom Amyotrophic Lateral Sclerosis (ALS).
Ya-magishi et al.
(Yamagishi et al., 2013) proposeda project named ?Voice Banking and Reconstruc-tion?.
In that project, various types of voices arecollected and they proposed TTS for ALS usingthat database.
The difference between TTS andVC is that TTS needs text input to synthesizespeech, whereas VC does not need text input.
Inthe case of people with articulation disorders re-sulting from athetoid cerebral palsy, it is difficultfor them to input text because of their athetoidsymptoms.Our proposed NMF-based VC (Takashima etal., 2012) is an exemplar-based method usingsparse representation, which is different from theconventional statistical method.
In recent years,approaches based on sparse representations havegained interest in a broad range of signal process-ing.
In approaches based on sparse representa-tions, the observed signal is represented by a lin-ear combination of a small number of bases.
Insome approaches for source separation, the atomsare grouped for each source, and the mixed sig-nals are expressed with a sparse representation ofthese atoms.
By using only the weights of theatoms related to the target signal, the target sig-nal can be reconstructed.
Gemmeke et al.
(Gem-meke et al., 2011) also propose an exemplar-basedmethod for noise-robust speech recognition.
Inthat method, the observed speech is decomposedinto the speech atoms, noise atoms, and theirweights.
Then the weights of the speech atoms areused as phonetic scores (instead of the likelihoodsof hidden Markov models) for speech recognition.In (Takashima et al., 2012), we proposed noise-robust VC using NMF.
The noise exemplars,which are extracted from the before- and after-utterance sections in an observed signal, are usedas the noise-dictionary, and the VC process iscombined with an NMF-based noise-reductionmethod.
On the other hand, NMF is one of theclustering methods.
In our exemplar-based VC, ifthe phoneme label of the source exemplar is given,we can discriminate the phoneme of the input sig-nal by using NMF.
In this paper, we proposed adictionary selection method using this property ofNMF.3 Voice Conversion Based onNon-negative Matrix Factorization3.1 Basic IdeaIn the exemplar-based approach, the observed sig-nal is represented by a linear combination of asmall number of bases.xl ?
?Jj=1 ajhj,l = Ahl (1)xl represents the l-th frame of the observation.aj and hj,l represent the j-th basis and theweight, respectively.
A = [a1 .
.
.aJ ] and hl =[h1,l .
.
.
hJ,l]T are the collection of the bases andthe stack of weights.
In this paper, each basis de-notes the exemplar of the spectrum, and the col-lection of exemplarA and the weight vector hl arecalled the ?dictionary?
and ?activity?, respectively.When the weight vector hl is sparse, the observedsignal can be represented by a linear combinationof a small number of bases that have non-zeroweights.
Eq.
(1) is expressed as the inner productof two matrices using the collection of the framesor bases.X ?
AH (2)X = [x1, .
.
.
,xL], H = [h1, .
.
.
,hL].
(3)L represents the number of the frames.Fig.
1 shows the basic approach of ourexemplar-based VC, where D,L, and J representthe numbers of dimensions, frames, and bases,respectively.
Our VC method needs two dictio-naries that are phonemically parallel.
As repre-sents a source dictionary that consists of the sourcespeaker?s exemplars and At represents a targetdictionary that consists of the target speaker?s ex-emplars.
These two dictionaries consist of thesame words and are aligned with dynamic timewarping (DTW) just as conventional GMM-basedVC is.
Hence, these dictionaries have the samenumber of bases.This method assumes that when the source sig-nal and the target signal (which are the same wordsbut spoken by different speakers) are expressedwith sparse representations of the source dictio-nary and the target dictionary, respectively, the ob-31tained activity matrices are approximately equiv-alent.
Fig.
2 shows an example of the activitymatrices estimated from a Japanese word ?ikioi?(?vigor?
in English), where one is uttered by amale, the other is uttered by a female, and eachdictionary is structured from just one word ?ikioi?as the simple example.As shown in Fig.
2, these activities have highenergies at similar elements.
For this reason, weassume that when there are parallel dictionaries,the activity of the source features estimated withthe source dictionary may be able to be substi-tuted with that of the target features.
Therefore,the target speech can be constructed using the tar-get dictionary and the activity of the source signalas shown in Fig.
1.
In this paper, we use Non-negative Matrix Factorization (NMF), which is asparse coding method in order to estimate the ac-tivity matrix.sXD L sAtAParallel dataJCopysHsHtX?Sourcespectral features(D x L)Source and targetdictionaries(D x J)Convertedspectral features(D x L) Activity ofsource signal(J x L)ActivityestimationConstructionFigure 1: Basic approach of NMF-based voiceconversionBasisID insourcedictionaryFrame of source speech Basis IDin target dictionaryFrame of target speech20 40 60 80 100 120 14050100150200250 20 40 60 80 100 12050100150200250Figure 2: Activity matrices for parallel utterances3.2 Individuality-preserving VoiceConversion Using Combined DictionaryIn order to make a parallel dictionary, some pairsof parallel utterances are needed, where each pairconsists of the same text.
One is spoken by a per-son with an articulation disorder (source speaker),and the other is spoken by a physically unim-paired person (target speaker).
Spectrum en-velopes, which are extracted from parallel utter-ances, are phonemically aligned by using DTW.In order to estimate activities of source featuresprecisely, segment features, which consist of someconsecutive frames, are constructed.
Target fea-tures are constructed from consonant frames of thetarget?s aligned spectrum and vowel frames of thesource?s aligned spectrum.
Source and target dic-tionaries are constructed by lining up each of thefeatures extracted from parallel utterances.The vowels voiced by a speaker strongly indi-cate the speaker?s individuality.
On the other hand,consonants of people with articulation disordersare often unstable.
Fig.
3(a) shows an exampleof the spectrogram for the word ?ikioi?
(?vigor?in English) of a person with an articulation dis-order.
The spectrogram of a physically unim-paired person speaking the same word is shown inFig.
3(b).
In Fig.
3(a), the area labeled ?k?
is notclear, compared to the same region in to Fig.
3(b).These figures indicate that consonants of peoplewith articulation disorders are often unstable andthis deteriorates their voice intelligibility.
In or-der to preserve their voice individuality, we usea ?combined-dictionary?
that consists of a sourcespeaker?s vowels and target speaker?s consonants.We replace the target dictionary As in Fig.
1with the ?combined-dictionary?.
Input sourcefeatures Xs, which consist of an articulation-disordered spectrum and its segment features, aredecomposed into a linear combination of basesfrom the source dictionary As by NMF.
Theweights of the bases are estimated as an activityHs.
Therefore, the activity includes the weight in-formation of input features for each basis.
Then,the activity is multiplied by a combined-dictionaryin order to obtain converted spectral features X?t,which are represented by a linear combination ofbases from the source speaker?s vowels and tar-get speaker?s consonants.
Because the source andtarget are parallel phonemically, the bases used inthe converted features are phonemically the sameas that of the source features.3.3 ProblemsIn the NMF-based approach described in Sec.
3.2,the parallel dictionary consists of the parallel train-ing data themselves.
Therefore, as the numberof the bases in the dictionary increases, the input32signal comes to be represented by a linear com-bination of a large number of bases rather than asmall number of bases.
When the number of basesthat represent the input signal becomes large, theassumption of similarity between source and tar-get activities may be weak due to the influence ofthe mismatch between the input signal and the se-lected bases.
Moreover, in the case of a combined-dictionary, the input articulation-disordered spec-trum may come to be represented by a combi-nation of vowels and consonants.
We assumethat this problem degrades the performance of ourexemplar-based VC.
Hence, we use a phoneme-categorized sub-dictionary in place of the largedictionary in order to reduce the number of thebases that represent the input signal and avoid themixture of vowels and consonants.0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90100020003000400050006000Frequency [Hz]Time [Sec]i k            i oi(a) Spoken by a person with an articulation disorder0.2 0.4 0.6 0.8 10100020003000400050006000Frequency [Hz]Time [Sec]i k           i oi(b) Spoken by a physically unimpaired person0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90100020003000400050006000Frequency [Hz]Time [Sec]i k            i oi(c) Converted by NMF-based VCFigure 3: Examples of spectrogram //i k i oi4 Non-negative Matrix FactorizationUsing a Phoneme-categorizedDictionary4.1 Phoneme-categorized DictionaryFig.
4 shows how to construct the sub-dictionary.As and At imply the source and target dictionarywhich hold all the bases from training data.
Thesedictionaries are divided into K dictionaries.
Inthis paper, the dictionaries are divided into 10 cat-egories according to the Japanese phoneme cate-gories shown in Table 1.In order to select the sub-dictionary, a?categorizing-dictionary?, which consists of therepresentative vector from each sub-dictionary, isconstructed.
The representative vectors for eachphoneme category consist of the mean vectors ofthe Gaussian Mixture Model (GMM).p(x(k)n ) =Mk?m=1?
(k)m N(x(k)n ,?
(k)m ,?
(k)m ) (4)Mk, ?
(k)m , ?
(k)m and ?
(k)m represent the numberof the Gaussian mixture, the weights of mixture,mean and variance of the m-th mixture of theGaussian, in the k-th sub-dictionary, respectively.Each parameter is estimated by using an EM algo-rithm.The basis of the categorizing-dictionary, whichcorresponds to the k-th sub-dictionary ?sk, is rep-resented using the estimated phoneme GMM asfollows:?k = [?
(k)1 , .
.
.
,?
(k)Mk ] (5)?sk = [x(k)1 , .
.
.
,x(k)Nk ] (6)Nk represents the number of frames of the k-thsub-dictionary.
The categorizing-dictionary ?
isgiven as follows:?
= [?1, .
.
.
,?K ] (7)4.2 Dictionary Selection and VoiceConversionFig.
5 shows the flow of the dictionary selectionand VC.
The input spectral features Xs are rep-resented by a linear combination of bases fromthe categorizing-dictionary ?.
The weights of the33......Sourcetraining speechTargettraining speechSpectralenvelopesAtAParallel Dictionariess1?
sk?DP-matching......?CategorizingDictionarySTRAIGHTSTRAIGHTCategorizationphoneme categorizedsub-dictionariestk 1+?sk 1+?tK?sK?s1?
sk?Vowel sub-dictionaries Consonant sub-dictionariesRepresentative vectorsArticulation-disordered spectrumWell-orderedspectrumVowels are replaced withsource speaker?s spectrumFigure 4: Making a sub-dictionarybases are represented as activitiesHs?.Xs ?
?Hs?
s.t.
Hs?
?
0 (8)Xs = [xs1, .
.
.
,xsL] (9)Hs?
= [hs?1, .
.
.
,hs?L] (10)hs?l = [hs?1l, .
.
.
,hs?K l]T (11)hs?kl = [hs?1l, .
.
.
, hs?Mkl]T (12)Then, the l-th frame of input feature xsl is rep-resented by a linear combination of bases from thesub-dictionary of the source speaker.
The sub-dictionary ?sk?, which corresponds to xl, is se-lected as follows:k?
= arg maxk11?Mkhs?kl= arg maxkMk?m=1hs?ml (13)xl = ?sk?hk?,l (14)The activity hl,k?
in Eq.
(14) is estimated from theselected source speaker sub-dictionary.If the selected sub-dictionary ?sk?
is related toconsonants, the l-th frame of the converted spec-tral feature y?l is constructed by using the activityand the sub-dictionary of the target speaker ?tk?.y?l = ?tk?hk?,l (15)On the other hand, if the selected sub-dictionary?sk?
is related to vowels, the l-th frame of the con-verted spectral feature y?l is constructed by usingthe activity and the sub-dictionary of the sourcespeaker ?sk?.y?l = ?sk?hk?,l (16)Table 1: Japanese phoneme categoriesCategory phonemea ae evowels i io ou uplosives Q, b, d, dy, g, gy, k, ky, p, tfricatives ch, f, h, hy, j, s, sh, ts, zconsonants nasals m, my ny, Nsemivowels w,yliquid r, rys l?hs1?CategorizingDictionarylxSelect the sub-dictionaryCopyl-th frame of input spectral featuresl-th frame of convertedspectral features?ly?s l1?hs lK?hs lk?hActivitytK?sK?sk 1+?tk 1+?s lMk kkk ?h1 ?= 1maxarg?????lx?
?Activity lk,?hphoneme-categorized sub-dictionariess1?Figure 5: NMF-based voice conversion using cat-egorized dictionary5 Experimental Results5.1 Experimental ConditionsThe proposed VC technique was evaluated bycomparing it with the conventional NMF-basedmethod (Aihara et al., 2014) (referred to as the?sample-based method?
in this paper) and theconventional GMM-based method (Stylianou etal., 1998) using clean speech data.
We recorded432 utterances (216 words, each repeated twotimes) included in the ATR Japanese speech34database (Kurematsu et al., 1990).
The speech sig-nals were sampled at 12 kHz and windowed witha 25-msec Hamming window every 10 msec.
Aphysically unimpaired Japanese male in the ATRJapanese speech database, was chosen as a targetspeaker.In the proposed and sample-based methods,the number of dimensions of the spectral fea-ture is 2,565.
It consists of a 513-dimensionalSTRAIGHT spectrum (Kawahara et al., 1999)and its consecutive frames (the 2 frames com-ing before and the 2 frames coming after).
TheGaussian mixture, which is used to constructa categorizing-dictionary, is 1/500 of the num-ber of bases of each sub-dictionary.
The num-ber of iterations for estimating the activity inthe proposed and sample-based methods was300.
In the conventional GMM-based method,MFCC+?MFCC+?
?MFCC is used as a spectralfeature.
Its number of dimensions is 74.
The num-ber of Gaussian mixtures is set to 64, which is ex-perimentally selected.In this paper, F0 information is converted usinga conventional linear regression based on the meanand standard deviation (Toda et al., 2007).
Theother information such as aperiodic components,is synthesized without any conversion.We conducted a subjective evaluation of 3 top-ics.
A total of 10 Japanese speakers took partin the test using headphones.
For the ?listeningintelligibility?
evaluation, we performed a MOS(Mean Opinion Score) test (?INTERNATIONALTELECOMMUNICATION UNION?, 2003).
Theopinion score was set to a 5-point scale (5: excel-lent, 4: good, 3: fair, 2: poor, 1: bad).
Twenty-twowords that are difficult for a person with an artic-ulation disorder to utter were evaluated.
The sub-jects were asked about the listening intelligibilityin the articulation-disordered voice, the voice con-verted by our proposed method, and the GMM-based converted voice.On the ?similarity?
evaluation, the XAB testwas carried out.
In the XAB test, each subject lis-tened to the articulation-disordered voice.
Thenthe subject listened to the voice converted by thetwo methods and selected which sample soundedmost similar to the articulation-disordered voice.On the ?naturalness?
evaluation, a paired com-parison test was carried out, where each subjectlistened to pairs of speech converted by the twomethods and selected which sample sounded morenatural.0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90100020003000400050006000Frequency [Hz]Time [Sec]i k            i oi(a) Converted by proposed method0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90100020003000400050006000Frequency [Hz]Time [Sec]i k            i oi(b) Converted by GMM-based VCFigure 6: Examples of converted spectrogram //i ki oi5.2 Results and DiscussionFig.
6(a) and 6(b) show examples of convertedspectrograms using our proposed method and theconventional GMM-based method, respectively.In Fig.
6(a), there are fewer misconversions in thevowel part compared to Fig.
3(c).
Moreover, byusing GMM-based conversion, the area labeled?oi?
becomes unclear compared to NMF-basedconversion.Fig.
7 shows the results of the MOS test for lis-tening intelligibility.
The error bars show a 95%confidence score; thus, our proposed VC methodis shown to be able to improve the listening intel-ligibility and clarity of consonants.
On the otherhand, GMM-based conversion can improve theclarity of consonants, but it deteriorates the lis-tening intelligibility.
This is because GMM-basedconversion has the effect of noise resulting frommeasurement error.
Our proposed VCmethod alsohas the effect of noise, but it is less than that cre-ated by GMM-based conversion.Fig.
8 shows the results of the XAB test onthe similarity to the source speaker and natural-ness of the converted voice.
The error bars show a95% confidence score.
Our proposed VC methodobtained a higher score than Sample-based andGMM-based conversion on similarity.
Fig.
935shows the preference score on the naturalness.
Theerror bars show a 95% confidence score.
Our pro-posed VC also method obtained a higher scorethan Sample-based and GMM-based conversionmethods in regard to naturalness.22.12.22.32.42.52.62.7Proposed GMM sourceMOSMOS MOSMOSFigure 7: Results of MOS test on listening intelli-gibility00.10.20.30.40.50.60.70.80.91NMF GMMPreference ScorePreference ScorePreference ScorePreference Score00.10.20.30.40.50.60.70.80.91Proposed NMF 00.10.20.30.40.50.60.70.80.91Proposed GMMFigure 8: Preference scores for the individuality00.10.20.30.40.50.60.70.80.91NMF GMMPreference ScorePreference ScorePreference ScorePreference Score00.10.20.30.40.50.60.70.80.91Proposed NMF 00.10.20.30.40.50.60.70.80.91Proposed GMMFigure 9: Preference scores for the naturalness6 ConclusionWe proposed a spectral conversion method basedon NMF for a voice with an articulation disorder.Our proposed method introduced a dictionary-selection method for conventional NMF-basedVC.
Experimental results demonstrated that ourVC method can improve the listening intelligibil-ity of words uttered by a person with an articu-lation disorder.
Moreover, compared to conven-tional GMM-based VC and conventional NMF-based VC, our proposed VC method can preservethe individuality of the source speaker?s voice andthe naturalness of the voice.
In this study, therewas only one subject person, so in future experi-ments, we will increase the number of subjects andfurther examine the effectiveness of our method.ReferencesR.
Aihara, R. Takashima, T. Takiguchi, and Y. Ariki.2014.
A preliminary demonstration of exemplar-based voice conversion for articulation disor-ders using an individuality-preserving dictionary.EURASIP Journal on Audio, Speech, and MusicProcessing, 2014:5, doi:10.1186/1687-4722-2014-5.S.
T. Canale and W. C. Campbell.
2002.
Campbell?soperative orthopaedics.
Technical report, Mosby-Year Book.J.
F. Gemmeke, T. Viratnen, and A. Hurmalainen.2011.
Exemplar-based sparse representations fornoise robust automatic speech recognition.
IEEETrans.
Audio, Speech and Language Processing, vol.19, no.
7, pages 2067?2080.E.
Helander, T. Virtanen, J. Nurminen, and M. Gab-bouj.
2010.
Voice conversion using partial leastsquares regression.
IEEE Trans.
Audio, Speech,Lang.
Process., vol.
18, Issue:5, pages 912?921.Mads Vilhelm Hollegaard, Kristin Skogstrand, PoulThorsen, Bent Norgaard-Pedersen, David MichaelHougaard, and Jakob Grove.
2013.
Joint analysisof SNPs and proteins identifies regulatory IL18 genevariations decreasing the chance of spastic cerebralpalsy.
Human Mutation, Vol.
34, pages 143?148.INTERNATIONAL TELECOMMUNICATIONUNION.
2003.
Methods for objective and subjec-tive assessment of quality.
ITU-T RecommendationP.800.A.
Kain and M. W. Macon.
1998.
Spectral voice con-version for text-to-speech synthesis.
in ICASSP, vol.1, pages 285?288.H.
Kawahara, I. Masuda-Katsuse, and A.de Cheveigne.1999.
Restructuring speech representations usinga pitch-adaptive time-frequency smoothing and aninstantaneous-frequencybased F0 extraction: possi-ble role of a repetitive structure in sounds.
SpeechCommunication, 27(3-4):187?207.A.
Kurematsu, K. Takeda, Y. Sagisaka, S. Katagiri,H.
Kuwabara, and K. Shikano.
1990.
ATR Japanesespeech database as a tool of speech recognition andsynthesis.
Speech Communication, 9:357?363.C.
H. Lee and C. H. Wu.
2006.
Map-based adaptationfor speech conversion using adaptation data selec-tion and non-parallel training.
in Interspeech, pages2254?2257.36H.
Matsumasa, T. Takiguchi, Y. Ariki, I. Li, andT.
Nakabayachi.
2009.
Integration of metamodeland acoustic model for dysarthric speech recogni-tion.
Journal of Multimedia, Volume 4, Issue 4,pages 254?261.K.
Nakamura, T. Toda, H. Saruwatari, and K. Shikano.2006.
Speaking aid system for total laryngectomeesusing voice conversion of body transmitted artificialspeech.
in Interspeech, pages 148?151.K.
Nakamura, T. Toda, H. Saruwatari, and K. Shikano.2012.
Speaking-aid systems using GMM-based voice conversion for electrolaryngeal speech.Speech Communication, 54(1):134?146.F.
Rudzicz.
2011.
Acoustic transformations to im-prove the intelligibility of dysarthric speech.
inproc.
the Second Workshop on Speech and LanguageProcessing for Assistive Technologies.F.
Rudzicz.
2014.
Adjusting dysarthric speech sig-nals to be more intelligible.
in Computer Speech andLanguage, 27(6), September, pages 1163?1177.Y.
Stylianou, O. Cappe, and E. Moilines.
1998.
Con-tinuous probabilistic transform for voice conver-sion.
IEEE Trans.
Speech and Audio Processing,6(2):131?142.R.
Takashima, T. Takiguchi, and Y. Ariki.
2012.Exemplar-based voice conversion in noisy environ-ment.
in SLT, pages 313?317.T.
Toda, Y. Ohtani, and K. Shikano.
2006.
Eigenvoiceconversion based on Gaussian mixture model.
in In-terspeech, pages 2446?2449.T.
Toda, A.
Black, and K. Tokuda.
2007.
Voice con-version based on maximum likelihood estimation ofspectral parameter trajectory.
IEEE Trans.
Audio,Speech, Lang.
Process., 15(8):2222?2235.H.
Valbret, E. Moulines, and J. P. Tubach.
1992.
Voicetransformation using PSOLA technique.
SpeechCommunication, vol.
11, no.
2-3, pp.
175-187.C.
Veaux and X. Robet.
2011.
Intonation conversionfrom neutral to expressive speech.
in Interspeech,pages 2765?2768.C.
Veaux, J. Yamagishi, and S. King.
2012.
Us-ing HMM-based speech synthesis to reconstruct thevoice of individuals with degenerative speech disor-ders.
in Interspeech, pages 1?4.J.
Yamagishi, Christophe Veaux, Simon King, andSteve Renals.
2013.
Speech synthesis technolo-gies for individuals with vocal disabilities: Voicebanking and reconstruction.
Acoustical Science andTechnology, Vol.
33 (2012) No.
1, pages 1?5.37
