Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 314?323,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsSemi-Supervised Word Sense Disambiguation Using Word Embeddingsin General and Specific DomainsKaveh TaghipourDepartment of Computer ScienceNational University of Singapore13 Computing DriveSingapore 117417kaveh@comp.nus.edu.sgHwee Tou NgDepartment of Computer ScienceNational University of Singapore13 Computing DriveSingapore 117417nght@comp.nus.edu.sgAbstractOne of the weaknesses of current supervisedword sense disambiguation (WSD) systems isthat they only treat a word as a discrete en-tity.
However, a continuous-space represen-tation of words (word embeddings) can pro-vide valuable information and thus improvegeneralization accuracy.
Since word embed-dings are typically obtained from unlabeleddata using unsupervised methods, this methodcan be seen as a semi-supervised word sensedisambiguation approach.
This paper investi-gates two ways of incorporating word embed-dings in a word sense disambiguation settingand evaluates these two methods on some Sen-sEval/SemEval lexical sample and all-wordstasks and also a domain-specific lexical sam-ple task.
The obtained results show that suchrepresentations consistently improve the ac-curacy of the selected supervised WSD sys-tem.
Moreover, our experiments on a domain-specific dataset show that our supervised base-line system beats the best knowledge-basedsystems by a large margin.1 IntroductionBecause of the ambiguity of natural language, manywords can have different meanings in different con-texts.
For example, the word ?bank?
has two differ-ent meanings in ?the bank of a river?
and ?a bankloan?.
While it seems simple for humans to iden-tify the meaning of a word according to the con-text, word sense disambiguation (WSD) (Ng andLee, 1996; Lee and Ng, 2002) is a difficult taskfor computers and thus requires sophisticated meansto achieve its goal.
Part of this ambiguity may beresolved by considering part-of-speech (POS) tagsbut the word senses are still highly ambiguous evenfor the same part-of-speech.
Machine translationis probably the most important application of wordsense disambiguation.
In machine translation, dif-ferent senses of a word cause a great amount of am-biguity for automated translation and it negativelyaffects the results.
Hence, an accurate WSD systemcan benefit machine translation significantly and im-prove the results (Chan et al, 2007; Carpuat and Wu,2007; Vickrey et al, 2005).
Moreover, Zhong andNg (2012) have shown that word sense disambigua-tion improves information retrieval by proposing amethod to use word senses in a language modelingapproach to information retrieval.The rest of this paper is organized as follows.Section 2 gives a literature review of related work,including a review of semi-supervised word sensedisambiguation and distributed word representationcalled word embeddings.
The method and frame-work used in this paper are explained in Section 3.Finally, we evaluate the system in Section 4 and con-clude the paper in Section 5.2 Related WorkThe method that we use in this paper is a semi-supervised learning method which incorporatesknowledge from unlabeled datasets by using wordembeddings.
This section is a literature review ofprevious work on semi-supervised word sense dis-ambiguation and various methods of obtaining wordembeddings.3142.1 Semi-Supervised Word SenseDisambiguationAmong various types of semi-supervised learningapproaches, co-training and self-training are prob-ably the most common.
These methods randomlyselect a subset of a large unlabeled dataset andclassify these samples using one (self-training) ortwo (co-training) classifiers, trained on a smallerset of labeled samples.
After assigning labels tothe new samples, these methods select the samplesthat were classified with a high confidence (accord-ing to a selection criterion) and add them to the setof labeled data.
These methods have been used inthe context of word sense disambiguation.
Mihal-cea (2004) used both co-training and self-trainingto make use of unlabeled datasets for word sensedisambiguation.
Mihalcea also introduced a tech-nique for combining co-training and majority vot-ing, called smoothed co-training, and reported im-proved results.
Another related study was done by(Pham et al, 2005).
In (Pham et al, 2005), somesemi-supervised learning techniques were used forword sense disambiguation.
Pham et al employedco-training and spectral graph transduction meth-ods in their experiments and obtained significant im-provements over a supervised method.Another semi-supervised learning method usedfor word sense disambiguation is Alternating Struc-ture Optimization (ASO), first introduced by (Andoand Zhang, 2005) and later applied to word sensedisambiguation tasks by (Ando, 2006).
This al-gorithm learns a predictive structure shared be-tween different problems (disambiguation of a tar-get word).
Semi-supervised application of the ASOalgorithm was shown to be useful for word sensedisambiguation and improvements can be achievedover a supervised predictor (Ando, 2006).This paper uses a different method proposed by(Turian et al, 2010) that can be applied to a widevariety of supervised tasks in natural language pro-cessing.
This method uses distributed word rep-resentations (word embeddings) as additional fea-ture functions in supervised tasks and is shown toimprove the accuracy of named-entity recognition(NER) and chunking.
In this paper, we also followthe same approach for word sense disambiguation.The key idea is that a system without a continuous-space representation of words ignores the similar-ity of words completely and relies only on their dis-crete form.
However, when a distributed representa-tion for words is added to the system, the classifiercan make use of the notion of similarity of wordsand learn the relationships between class labels andwords.In addition to using raw word embeddings, wealso propose a method to adapt embeddings for eachclassification task.
Since word embeddings do notinclude much task-specific discriminative informa-tion, we use a neural network to modify word vec-tors to tune them for our WSD tasks.
We show thatthis process results in improved accuracy comparedto raw word embeddings.Recently, obtaining word embeddings in an un-supervised manner from large text corpora has at-tracted the attention of many researchers (Collobertand Weston, 2008; Mnih and Hinton, 2009; Mikolovet al, 2013a; Mikolov et al, 2013b).
Subsequently,there have been some published word embeddingsand some software for training word embeddings.For word sense disambiguation, there are veryfew open source programs.
Since we are inter-ested in a fully supervised WSD tool, IMS (It MakesSense) (Zhong and Ng, 2010) is selected in ourwork.
This system allows addition of extra featuresin a simple way and hence is a good choice for test-ing the effect of word embeddings as additional fea-tures.
Moreover, the scores reported for IMS arecompetitive with or better than state-of-the-art sys-tems (Zhong and Ng, 2010).2.2 Word EmbeddingsThere are several types of word representations.
Aone-hot representation is a vector where all com-ponents except one are set to zero and the compo-nent at the index associated with a word is set toone.
This type of representation is the sparsest wordrepresentation and does not carry any informationabout word similarity.
Another popular approachis to use the methods mainly applied in informationretrieval.
Latent Semantic Analysis (LSA) and La-tent Dirichlet Allocation (LDA) are such examples,and word representations produced by these meth-ods can also be used in other applications.
However,a dense distributed representation for words (wordembeddings) can learn more complex relationships315between words and hence, it can be useful in a widerange of applications.
We only focus on word em-beddings in this paper and apply them to word sensedisambiguation.Word embeddings are distributed representationsof words and contain some semantic and syntacticinformation (Mikolov et al, 2013c).
Such represen-tations are usually produced by neural networks.
Ex-amples of such neural networks are (log-)linear net-works (Mikolov et al, 2013a), deeper feed-forwardneural networks (Bengio et al, 2003; Collobertand Weston, 2008), or recurrent neural networks(Mikolov et al, 2010).
Moreover, it has been shownthat deep structures may not be needed for word em-beddings estimation (Lebret et al, 2013) and shal-low structures can obtain relatively high quality rep-resentations for words (Mikolov et al, 2013b).In this paper, we have used the word embeddingscreated and published by (Collobert and Weston,2008).
Throughout this paper, we refer to these wordembeddings as ?CW?.
This method is proposed in(Collobert and Weston, 2008) and explained furtherin (Collobert et al, 2011).
The authors use a feed-forward neural network to produce word representa-tions.
In order to train the neural network, a largetext corpus is needed.
Collobert and Weston (2008)use Wikipedia (Nov. 2007 version containing 631million words) and Reuters RCV1 (containing 221million words) (Lewis et al, 2004) as their text cor-pora and Stochastic Gradient Descent (SGD) as thetraining algorithm.
The training algorithm selects awindow of text randomly and then replaces the mid-dle word with a random word from the dictionary.Then the original window of text and the corruptedone is given to the neural network.
The neural net-work computes f(x) and f(x(w)), where x is theoriginal window of text, x(w)is the same window oftext with the middle word replaced by word w, andf(.)
is the function that the neural network repre-sents.
After computing f(x) and f(x(w)), the train-ing algorithm uses a pairwise ranking cost functionto train the network.
The training algorithm mini-mizes the cost function by updating the parameters(including word embeddings) and as a consequenceof using the pairwise ranking cost function, this neu-ral network tends to assign higher scores to validwindows of texts and lower scores to incorrect ones.After training the neural network, the word vectorsFigure 1: The neural network architecture for adaptationof word embeddings.in the lookup table layer form the word embeddingsmatrix.
Collobert et al (2011) have made this ma-trix available for public use and it can be accessedonline1.3 MethodIn this section, we first explain our novel task-specific method of adapting word embeddings andthen describe our framework in which raw oradapted word embeddings are included in our wordsense disambiguation system.
To the best of ourknowledge, the use of word embeddings for semi-supervised word sense disambiguation is novel.3.1 Adaptation of Word EmbeddingsWord embeddings capture some semantic and syn-tactic information and usually similar words havesimilar word vectors in terms of distance measures.However, in a classification task, it is better for wordembeddings to also include some task specific dis-criminative information.
In order to add such in-formation to word embeddings, we modify wordvectors using a neural network (Figure 1) to obtainadapted word embeddings.
This section explainsthis process in detail.The neural network that we used to adapt wordembeddings is similar to the window approach net-work introduced by (Collobert and Weston, 2008).This neural network includes the following layers:1http://ml.nec-labs.com/senna316?
Lookup table layer: This layer includes threelookup tables.
The first lookup table assigns avector to each input word, as described earlier.The second lookup table maps each word to avector with respect to the capitalization feature.Finally, the third lookup table maps each wordto its corresponding vector based on the word?sPOS tag.?
Dropout layer: In order to avoid overfitting, weadded a dropout layer (Hinton et al, 2012) tothe network to make use of its regularization ef-fect.
During training, the dropout layer copiesthe input to the output but randomly sets someof the entries to zero with a probability p, whichis usually set to 0.5.
During testing, this layerproduces the output by multiplying the inputvector by 1 ?
p (see (Hinton et al, 2012) formore information).?
Output layer: This layer linearly maps the inputvector X to a C-dimensional vector Y (equa-tion 1) and then applies a SoftMax operation(Bridle, 1990) over all elements of Y (equation2):Y = WX + b (1)p(t|I) =exp(Yt)?Cj=1exp(Yj)1 ?
t ?
C (2)where I is the input window of text and t isa class label (sense tag in WSD).
The outputof the output layer can be interpreted as a con-ditional probability p(t|I) over tags given theinput text.This architecture is similar to the network used by(Collobert and Weston, 2008) but it does not includea hidden layer.
Since the number of training samplesfor each word type in WSD is relatively small, wedid not use a hidden layer to decrease the model sizeand consequently overfitting, as much as possible.Moreover, we added the dropout layer and observedincreased generalization accuracy subsequently.In order to train the neural network, we usedStochastic Gradient Descent (SGD) and error back-propagation to minimize negative log-likelihoodcost function for each training example (I, t) (equa-tion 3).?
log p(t|I) = log(C?j=1exp(Yj))?
Yt(3)During the training process, the inputs are the win-dows of text surrounding the target word with theirassigned POS tags.
We used fixed learning rate(0.01) during training, with no momentum.Since the objective is to adapt word embeddingsusing the neural network, we initialized the lookuptable layer parameters using pre-trained word em-beddings and trained a model for each target wordtype.
After the training process completes, the mod-ified word vectors form our adapted word embed-dings, which will be used in exactly the same wayas the original embeddings.
Section 3.2 explains theway we use word embeddings to improve a super-vised word sense disambiguation system.3.2 FrameworkThe supervised system that we used for word sensedisambiguation is an open source tool named IMS(Zhong and Ng, 2010).
This software extracts threetypes of features and then uses Support Vector Ma-chines (SVM) as the classifier.
The three types offeatures implemented in IMS are explained below.?
POS tags of surrounding words: IMS uses thePOS tags of all words in a window size of 7,surrounding the target ambiguous word.
POStag features are limited to the current sentenceand neighboring sentences are not considered.?
Surrounding words: Additionally, the sur-rounding words of a target word (after remov-ing stop words) are also used as features inIMS.
However, unlike POS tags, the words oc-curring in the immediately adjacent sentencesare also included.?
Local collocations: Finally, 11 local colloca-tions around the target word are considered asfeatures.
These collocations also cover a win-dow size of 7, where the target word is in themiddle.All mentioned features are binary features andwill be used by the classifier in the next phase.
Af-ter extracting these features, the classifier (SVM) is317used to train a model for each target word.
In the testphase, the model is used to classify test samples andassign a sense tag to each sample.This supervised framework with separate featureextraction and classification phases makes it easy toadd any number of features and in our case, wordembeddings.
In order to make use of word embed-dings trained by a neural network, we follow theapproach of (Turian et al, 2010) and include wordembeddings for all words in the surrounding win-dow of text, given a target ambiguous word.
We usethe words from immediately adjacent sentences ifthe window falls beyond the current sentence bound-aries.
Since each word type has its own classificationmodel, we do not add the word embeddings for thetarget word because the same vector will be used inall training and test samples and will be useless.After extraction of the three mentioned types offeatures, d.(w ?
1) features will be added to eachsample, where d is the word embeddings dimensionand w is the number of words in the window of textsurrounding the target word (window size).
w is oneof the hyper-parameters of our system that can betuned for each word type separately.
However, sincethe training sets for some of the benchmark tasks aresmall, tuning the window size will not be consistentover different tuning sets.
Thus, we decided to selectthe same window size for all words in a task andtune this parameter on the whole tuning set instead.After augmenting the features, a model is trained forthe target word and then the classifier can be used toassign the correct sense to each test sample.However, since the original three types of featuresare binary, newly added real-valued word embed-dings do not fit well into the model and they tendto decrease performance.
This problem is addressedin (Turian et al, 2010) and a simple solution is toscale word embeddings.
The following conversionis suggested by (Turian et al, 2010):E ?
?
?
E/stddev(E) (4)where ?
is a scalar hyper-parameter denoting thedesired standard deviation, E is the word embed-dings matrix and stddev(.)
is the standard devia-tion function, which returns a scalar for matrix E.However, different dimensions of word embeddingvectors may have different standard deviations andEquation 4 may not work well.
In this case, per-dimension scaling will make more sense.
In order toscale the word embeddings matrix, we use Equation5 in our experiments:Ei?
?
?
Ei/stddev(Ei), i : 1, 2, ..., d (5)where Eidenotes the ithdimension of word embed-dings.
Like (Turian et al, 2010), we also found that?
= 0.1 is a good choice for the target standard de-viation and works well.4 Results and DiscussionWe evaluate our word sense disambiguation systemexperimentally by using standard benchmarks.
Thetwo major tasks in word sense disambiguation arelexical sample task and all-words task.
For eachtask, we explain our experimental setup first andthen present the results of our experiments for thetwo mentioned tasks.
Although most benchmarksare general domain test sets, a few domain-specifictest sets also exist (Koeling et al, 2005; Agirre et al,2010).4.1 Lexical Sample TasksWe have evaluated our system on SensEval-2 (SE2)and SensEval-3 (SE3) lexical sample tasks and alsothe domain-specific test set (we call it DS05) pub-lished by (Koeling et al, 2005).
This subsection de-scribes our experiments and presents the results ofthese tasks.4.1.1 Experimental SetupMost lexical sample tasks provide separate train-ing and test sets.
Some statistics about these tasksare given in Table 1.SE2 SE3 DS05#Word types 73 57 41#Training samples 8,611 8,022 -#Test samples 4,328 3,944 10,272Table 1: Statistics of lexical sample tasksThe DS05 dataset does not provide any traininginstances.
In order to train models for DS05 (andlater for the SE3 all-words task), we generated train-ing samples for the top 60% most frequently occur-ring polysemous content words in Brown Corpus,318using the approach described in (Ng et al, 2003;Chan and Ng, 2005).
This dataset is automaticallycreated by processing parallel corpora without anymanual sense annotation effort.
We used the fol-lowing six English-Chinese parallel corpora: HongKong Hansards, Hong Kong News, Hong KongLaws, Sinorama, Xinhua News, and the Englishtranslations of Chinese Treebank.
Similar to (Zhongand Ng, 2010), we obtained word alignments usingGIZA++ (Och and Ney, 2000).
Then, for each En-glish word, the aligned Chinese word is used to findthe corresponding sense tag for the English word.Finally, we made use of examples from the DSO cor-pus (Ng and Lee, 1996) and SEMCOR (Miller et al,1994) as part of our training data.
Table 2 showssome statistics of our training data.POS #word typesAdj.
5,129Adv.
28Noun 11,445Verb 4,705Total 21,307Table 2: Number of word types in each part-of-speech(POS) in our training setSince the dataset used by (Zhong and Ng, 2010)does not cover the specific domains of DS05 (Sportsand Finance), we added a few samples from thesedomains to improve our baseline system.
For eachtarget word, we randomly selected 5 instances (asentence including the target word) for Sports do-main and 5 instances for Finance domain from theReuters (Rose et al, 2002) dataset?s Sports and Fi-nance sections and manually sense annotated them.Annotating 5 instances per word and domain takesabout 5 minutes.
To make sure that these instancesare not the same samples in the test set, we filteredout all documents containing at least one of the testinstances and selected our training samples from therest of the collection.
After removing samples withunclear tags, we added the remaining instances (187instances for Sports domain and 179 instances forFinance domain) to our original training data (Zhongand Ng, 2010).
We highlight this setting in our ex-periments by ?CC?
(concatenation).We used the published CW word embeddings andset the word embeddings dimension to 50 in all ourexperiments.
Finally, in order to tune the windowsize hyper-parameter, we randomly split our train-ing sets into two parts.
We used 80% for trainingmodels and the remaining 20% for evaluation.
Aftertuning the window size, we used the original com-plete training set for training our models.4.1.2 ResultsIn order to select a value for the window size pa-rameter, we performed two types of tuning.
The firstmethod, which (theoretically) can achieve higher ac-curacies, is per-word tuning.
Since each word typehas its own model, we can select different windowsizes for different words.
The second method, onthe other hand, selects the same value for the win-dow size for all word types in a task, and we call itper-task tuning.Although, per-word tuning achieved very high ac-curacies on the held-out development set, we ob-served that it performed poorly on the test set.
More-over, the results of per-word tuning are not stableand different development sets lead to different win-dow sizes and also fluctuating accuracies.
This is be-cause the available training sets are small and using20% of these samples as the development set meansthat the development set only contains a small num-ber of samples.
Thus the selected development setsare not proper representatives of the test sets and thetuning process results in overfitting the parameters(window sizes) to the development sets, with lowgeneralization accuracy.
However, per-task tuningis relatively stable and performs better on the testsets.
Thus we have selected this method of tuningin all our experiments.
Mihalcea (2004) also reportsthat per-word tuning of parameters is not helpful anddoes not result in improved performance.We also evaluated our system separately on theword types in each part-of-speech (POS) for SE2and SE3 lexical sample tasks.
The results are in-cluded in Table 3 and Table 4.
According to thesetables, word embeddings do not affect all POS typesuniformly.
For example, on SE2, the improvementachieved on verbs is much larger than the other twoPOS types and on SE3, adjectives benefited fromword embeddings more than nouns and verbs.
How-ever, this table also shows that improvements fromword embeddings are consistent over POS types and319both lexical sample tasks.SE2POS #word types baseline CW (17)Adj.
15 67.45% 67.72%Noun 29 69.39% 69.38%Verb 29 60.45% 61.89%Table 3: The scores for each part-of-speech (POS) onSE2 lexical sample tasks.
The window size is shown in-side brackets.SE3POS #word types baseline CW (9)Adj.
5 45.93% 47.81%Noun 32 73.44% 73.83%Verb 20 74.12% 74.17%Table 4: The scores for each part-of-speech (POS) onSE3 lexical sample tasks.
The window size is shown in-side brackets.Finally, we evaluated the effect of word embed-dings and the adaptation process.
Table 5 summa-rizes our findings on SE2 and SE3 lexical sampletasks.
According to this table, both types of wordembeddings lead to improvements on lexical sampletasks.
We also performed a one-tailed paired t-test tosee whether the improvements are statistically sig-nificant over the baseline (IMS).
The improvementsobtained using CW word embeddings over the base-line are significant (p < 0.05) in both lexical sampletasks.
Furthermore, the results show that adaptedword embeddings achieve higher scores than rawword embeddings.
We have included the scores ob-tained by the first and the second best participatingsystems in these lexical sample tasks and also theMost Frequent Sense (MFS) score.
The results alsoshow that the Dropout layer increases performancesignificantly.
The reason behind this observation isthat without Dropout, word embeddings are overfit-ted to the training data and when they are used asextra features in IMS, the classifier does not gen-eralize well to the test set.
Since adaptation with-out Dropout leads to worse performance, we includeDropout in all other experiments and only report re-sults obtained using Dropout.Similarly, Table 6 presents the results obtainedSE2 SE3IMS (baseline) 65.3% 72.7%IMS + CW 66.1%*(17) 73.0%*(9)IMS + adapted CW 66.2%*(5) 73.4%*(7)?
Dropout 65.4% (7) 72.7% (7)Rank 1 system 64.2% 72.9%Rank 2 system 63.8% 72.6%MFS 47.6% 55.2%Table 5: Lexical sample task results.
The values insidebrackets are the selected window sizes and statisticallysignificant (p < 0.05) improvements are marked with ?
*?.from our experiments on the DS05 dataset.
In thistable, as explained earlier, ?CC?
denotes the addi-tional manually tagged instances.
For comparisonpurposes, we included the results reported by twostate-of-the-art knowledge-based systems, namelyPPRw2w(Agirre et al, 2014) and Degree (Ponzettoand Navigli, 2010).Table 6 shows that IMS performs worse thanPPRw2won Sports and Finance domains but IMS +CC outperforms PPRw2w.
One of the reasons be-hind this observation is unseen sense tags.
For ex-ample, in the sentence ?the winning goal came withless than a minute left to play2?, the sense tag forword ?goal?
is ?goal%1:04:00::?.
However, the train-ing data for IMS does not contain any sample withthis sense tag and so it is impossible for IMS to as-sign this tag to any test instances.
On the other hand,the manually annotated instances (CC) include sam-ples with this tag and therefore IMS + CC is able toassociate a target word with this sense tag.According to Table 6, adding word embeddingsresults in improved performance over the baseline(IMS + CC).
Moreover, adapting word embeddingsis found to increase accuracy in most cases.4.2 All-Words TaskWe also evaluated the performance of our system onthe SensEval-3 (SE3) all-words task.
Next, we ex-plain our setup and then present the results of ourevaluation.2This example is taken from WordNet v3.1.320BNC Sports Finance TotalIMS 48.7% 41.4% 53.4% 47.8%IMS + CC (baseline) 51.7% 55.7% 62.1% 56.4%IMS + CC + CW (3) 51.9% 56.1%*62.3% 56.7%*IMS + CC + adapted CW (3) 52.3%*57.1%*62.0% 57.1%*PPRw2w37.7% 51.5% 59.3% 49.3%Degree - 42.0% 47.8% -Table 6: DS05 task results.
The values inside brackets are the selected window sizes and statistically significant (p <0.05) improvements over ?IMS + CC?
are marked with ?
*?.4.2.1 Experimental SetupAll-words tasks do not provide any training sam-ples and only include a test set (see Table 7).
In orderto train our system for SE3 all-words task, we usedthe automatically labeled training samples used ear-lier for training models for DS05 (see section 4.1.1).Table 2 shows some statistics about our training set.SE3#Word types 963#Test samples 2,041Table 7: Statistics of SE3 all-words taskSimilar to the lexical sample tasks, we tune oursystem on 20% of the original training set.
After ob-taining window size parameter via tuning, we trainon the whole training set and test on the given stan-dard test set.4.2.2 ResultsThe results of the evaluation on SE3 all-wordstask are given in Table 8.
This table shows thatCW word embeddings improve the accuracy.
Sim-ilar to the results obtained for the lexical sampletasks, we observe some improvement by adaptingword embeddings for SE3 all-words task as well.For comparison purposes, we have included the offi-cial scores of rank 1 and rank 2 participating systemsin SE3 all-words task and the WordNet first sense(WNs1) score.5 ConclusionSupervised word sense disambiguation systems usu-ally treat words as discrete entities and consequentlyignore the concept of similarity between words.However, by adding word embeddings, some of theSE3IMS (baseline) 67.6%IMS + CW 68.0%*(9)IMS + adapted CW 68.2%*(9)Rank 1 system 65.2%Rank 2 system 64.6%WNs1 62.4%Table 8: SE3 all-words task results.
The values insidebrackets are the selected window sizes and statisticallysignificant (p < 0.05) improvements over the IMS base-line are marked with ?
*?.samples that cannot be discriminated based on theoriginal features (surrounding words, collocations,POS tags) have more chances to be classified cor-rectly.
Moreover, word embeddings are likely tocontain valuable linguistic information too.
Hence,adding continuous-space representations of wordscan provide valuable information to the classifierand the classifier can learn better discriminative cri-teria based on such information.In this paper, we exploited a type of word embed-dings obtained by feed-forward neural networks.
Wealso proposed a novel method (i.e., adaptation) toadd discriminative information to such embeddings.These word embeddings were then added to a super-vised WSD system by augmenting the original bi-nary feature space with real-valued representationsfor all words occurring in a window of text.
Weevaluated our system on two general-domain lexicalsample tasks, an all-words task, and also a domain-specific dataset and showed that word embeddingsconsistently improve the accuracy of a supervisedword sense disambiguation system, across differentdatasets.
Moreover, we observed that adding dis-321criminative information by adapting word embed-dings further improves the accuracy of our wordsense disambiguation system.AcknowledgmentsThis research is supported by the Singapore Na-tional Research Foundation under its InternationalResearch Centre @ Singapore Funding Initiativeand administered by the IDM Programme Office.ReferencesEneko Agirre, Oier Lopez de Lacalle, Christiane Fell-baum, Shu-Kai Hsieh, Maurizio Tesconi, MonicaMonachini, Piek Vossen, and Roxanne Segers.
2010.SemEval-2010 task 17: All-words word sense disam-biguation on a specific domain.
In Proceedings of the5th International Workshop on Semantic Evaluation,pages 75?80.Eneko Agirre, Oier L?opez de Lacalle, and Aitor Soroa.2014.
Random walks for knowledge-based wordsense disambiguation.
Computational Linguistics,40(1):57?84.Rie Kubota Ando and Tong Zhang.
2005.
A frameworkfor learning predictive structures from multiple tasksand unlabeled data.
Journal of Machine Learning Re-search, 6:1817?1853.Rie Kubota Ando.
2006.
Applying alternating structureoptimization to word sense disambiguation.
In Pro-ceedings of the Tenth Conference on ComputationalNatural Language Learning, pages 77?84.Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Research,3:1137?1155.John S Bridle.
1990.
Probabilistic interpretation of feed-forward classification network outputs, with relation-ships to statistical pattern recognition.
Neurocomput-ing, pages 227?236.Marine Carpuat and Dekai Wu.
2007.
Improving sta-tistical machine translation using word sense disam-biguation.
In Proceedings of the 2007 Joint Confer-ence on Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learn-ing, pages 61?72.Yee Seng Chan and Hwee Tou Ng.
2005.
Scaling upword sense disambiguation via parallel texts.
In Pro-ceedings of the 20th National Conference on ArtificialIntelligence, pages 1037?1042.Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007.Word sense disambiguation improves statistical ma-chine translation.
In Proceedings of the 45th AnnualMeeting of the Association for Computational Linguis-tics, pages 33?40.Ronan Collobert and Jason Weston.
2008.
A unified ar-chitecture for natural language processing: Deep neu-ral networks with multitask learning.
In Proceed-ings of the 25th International Conference on MachineLearning, pages 160?167.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.Journal of Machine Learning Research, 12:2493?2537.Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky,Ilya Sutskever, and Ruslan Salakhutdinov.
2012.
Im-proving neural networks by preventing co-adaptationof feature detectors.
Computing Research Repository,abs/1207.0580.Rob Koeling, Diana McCarthy, and John Carroll.
2005.Domain-specific sense distributions and predominantsense acquisition.
In Proceedings of the Conferenceon Human Language Technology and Empirical Meth-ods in Natural Language Processing, pages 419?426.R?emi Lebret, Jo?el Legrand, and Ronan Collobert.
2013.Is deep learning really necessary for word embed-dings?
In Neural Information Processing Systems:Deep Learning Workshop.Yoong Keok Lee and Hwee Tou Ng.
2002.
An empiri-cal evaluation of knowledge sources and learning algo-rithms for word sense disambiguation.
In Proceedingsof the 2002 Conference on Empirical Methods in Nat-ural Language Processing, pages 41?48.David D. Lewis, Yiming Yang, Tony G. Rose, and FanLi.
2004.
RCV1: A new benchmark collection for textcategorization research.
Journal of Machine LearningResearch, 5:361?397.Rada Mihalcea.
2004.
Co-training and self-training forword sense disambiguation.
In Proceedings of the8th Conference on Computational Natural LanguageLearning, pages 33?40.Tomas Mikolov, Martin Karafi?at, Luk?a?s Burget, Jan?Cernock`y, and Sanjeev Khudanpur.
2010.
Recurrentneural network based language model.
In Proceedingsof the Eleventh Annual Conference of the InternationalSpeech Communication Association.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013a.
Efficient estimation of word represen-tations in vector space.
In Proceedings of Workshopat International Conference on Learning Representa-tions.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado,and Jeffrey Dean.
2013b.
Distributed representationsof words and phrases and their compositionality.
InAdvances in Neural Information Processing Systems26, pages 3111?3119.322Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013c.
Linguistic regularities in continuous spaceword representations.
In Proceedings of the 2013 Con-ference of the North American Chapter of the Associa-tion for Computational Linguistics: Human LanguageTechnologies, pages 746?751.George A. Miller, Martin Chodorow, Shari Landes, Clau-dia Leacock, and Robert G. Thomas.
1994.
Using asemantic concordance for sense identification.
In Pro-ceedings of the Workshop on Human Language Tech-nology, pages 240?243.Andriy Mnih and Geoffrey E. Hinton.
2009.
A scalablehierarchical distributed language model.
In Advancesin Neural Information Processing Systems 21, pages1081?1088.Hwee Tou Ng and Hian Beng Lee.
1996.
Integrat-ing multiple knowledge sources to disambiguate wordsense: An exemplar-based approach.
In Proceedingsof the 34th Annual Meeting of the Association forComputational Linguistics, pages 40?47.Hwee Tou Ng, Bin Wang, and Yee Seng Chan.
2003.Exploiting parallel texts for word sense disambigua-tion: An empirical study.
In Proceedings of the 41stAnnual Meeting of the Association for ComputationalLinguistics, pages 455?462.Franz Josef Och and Hermann Ney.
2000.
Improved sta-tistical alignment models.
In Proceedings of the 38thAnnual Meeting of the Association for ComputationalLinguistics, pages 440?447.Thanh Phong Pham, Hwee Tou Ng, and Wee SunLee.
2005.
Word sense disambiguation with semi-supervised learning.
In Proceedings of the 20thNational Conference on Artificial Intelligence, pages1093?1098.Simone Paolo Ponzetto and Roberto Navigli.
2010.Knowledge-rich word sense disambiguation rivalingsupervised systems.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics, pages 1522?1531.Tony Rose, Mark Stevenson, and Miles Whitehead.2002.
The Reuters Corpus Volume 1 ?
from yes-terday?s news to tomorrow?s language resources.
InProceedings of the Third International Conference onLanguage Resources and Evaluation, pages 827?832.Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.2010.
Word representations: A simple and generalmethod for semi-supervised learning.
In Proceed-ings of the 48th Annual Meeting of the Association forComputational Linguistics, pages 384?394.David Vickrey, Luke Biewald, Marc Teyssier, andDaphne Koller.
2005.
Word-sense disambiguation formachine translation.
In Proceedings of the Conferenceon Human Language Technology and Empirical Meth-ods in Natural Language Processing, pages 771?778.Zhi Zhong and Hwee Tou Ng.
2010.
It Makes Sense: awide-coverage word sense disambiguation system forfree text.
In Proceedings of the 48th Annual Meetingof the Association for Computational Linguistics Sys-tem Demonstrations, pages 78?83.Zhi Zhong and Hwee Tou Ng.
2012.
Word sense disam-biguation improves information retrieval.
In Proceed-ings of the 50th Annual Meeting of the Association forComputational Linguistics, pages 273?282.323
