Coling 2010: Poster Volume, pages 605?613,Beijing, August 2010Best Topic Word Selection for Topic LabellingJey Han Lau,??
David Newman,??
Sarvnaz Karimi?
and Timothy Baldwin???
NICTA Victoria Research Laboratory?
Dept of Computer Science and Software Engineering, University of Melbourne?
Dept of Computer Science, University of Californiajhlau@csse.unimelb.edu.au, newman@uci.edu, skarimi@unimelb.edu.au, tb@ldwin.netAbstractThis paper presents the novel task of besttopic word selection, that is the selectionof the topic word that is the best label fora given topic, as a means of enhancing theinterpretation and visualisation of topicmodels.
We propose a number of featuresintended to capture the best topic word,and show that, in combination as inputs toa reranking model, we are able to consis-tently achieve results above the baseline ofsimply selecting the highest-ranked topicword.
This is the case both when trainingin-domain over other labelled topics forthat topic model, and cross-domain, us-ing only labellings from independent topicmodels learned over document collectionsfrom different domains and genres.1 IntroductionIn the short time since its inception, topic mod-elling (Blei et al, 2003) has become a main-stream technique for tasks as diverse as multi-document summarisation (Haghighi and Vander-wende, 2009), word sense discrimination (Brodyand Lapata, 2009), sentiment analysis (Titov andMcDonald, 2008) and information retrieval (Weiand Croft, 2006).
For many of these tasks, themultinomial topics learned by the topic model canbe interpreted natively as probabilities, or mappedonto a pre-defined discrete class set.
However,for tasks where the learned topics are providedto humans as a first-order output, e.g.
for use indocument collection analysis/navigation, it can bedifficult for the end-user to interpret the rich sta-tistical information encoded in the topics.
Thisresearch is concerned with making topics morereadily human interpretable, by selecting a singleterm with which to label the topic.Although topics are formally a multinomial dis-tribution over terms, with every term having finiteprobability in every topic, topics are usually dis-played by printing the top-10 terms (i.e.
the 10most probable terms) in the topic.
These top-10terms typically account for about 30% of the topicmass for reasonable setting of number of topics,and usually provide sufficient information to de-termine the subject area and interpretation of atopic, and distinguish one topic from another.Our research task can be illustrated via the top-10 terms in the following topic, learned from abook collection.
Terms wi are presented in de-scending order of P (wi|tj) for the topic tj :trout fish fly fishing water angler stream rodflies salmonClearly the topic relates to fishing, and indeed,the fourth term fishing is an excellent label for thetopic.
The task is thus termed best word or mostrepresentative word selection, as we are selectingthe label from the closed set of the top-N topicwords in that topic.Naturally, not all topics are equally coherent,however, and the lower the topic coherence, themore difficult the label selection task becomes.For example:oct sept nov aug dec july sun lite adv globeappears to conflate months with newspapernames, and no one of these topic words is able tocapture the topic accurately.
As such, our method-ology presupposes an automatic means of ratingtopics for coherence.
Fortunately, recent researchby Newman et al (2010) has shown that this isachievable at levels approaching human perfor-mance, meaning that this is not an unreasonableassumption.Labelling topics has applications across a di-verse range of tasks.
Our original interest in the605problem stems from work in document collectionvisualisation/navigation, and the realisation thatpresenting users with topics natively (e.g.
as rep-resented by the top-N terms) is ineffective, andwould be significantly enhanced if we could au-tomatically predict succinct labels for each topic.Another application area where labelling has beenshown to enhance the utility of topic models is se-lectional preference learning via topic modelling(Ritter et al, to appear).
Here, topic labelling viataxonomic classes (e.g.
WordNet synsets) can leadto better topic generalisation, in addition to betterhuman readability.This paper is based around the assumption thatan appropriate label for a topic can be foundamong the high-ranking (high probability) termsin that topic.
We assess the suitability of each termby way of comparison with other high-rankingterms in that same topic, using simple pointwisemutual information and conditional probabilities.We first experiment with a simple ranking methodbased on the component scores, and then moveon to using those scores, along with features fromWordNet and from the original topic model, in aranking support vector regression (SVR) frame-work.
Our experiments demonstrate that we areable to perform the task significantly better thanthe baseline of selecting the topic word of high-est marginal probability, including when trainingthe ranking model on labelled topics from otherdocument collections.2 Related WorkPredictably, there has been significant work on in-terpreting topics in the context of topic modelling.Topic are conventionally interpreted via the top-N words in each topic (Blei et al, 2003; Grif-fiths and Steyvers, 2004), or alternatively by post-hoc manual labelling of each topic based on do-main knowledge and subjective interpretation ofeach topic (Wang and McCallum, 2006; Mei etal., 2006).Mei et al (2007) proposed various approachesfor automatically suggesting phrasal labels fortopics, based on first extracting phrases from thedocument collection, and subsequently rankingthe phrases based on KL divergence with a giventopic.Magatti et al (2009) proposed a method for la-belling topics induced by hierarchical topic mod-elling, based on ontological alignment with theGoogle Directory (gDir) hierarchy, and optionallyexpanding topics based on a thesaurus or Word-Net.
Preliminary experiments suggest the methodhas promise, but the method crucially relies onboth a hierarchical topic model and a pre-existingontology, so has limited applicability.Over the general task of labelling a learned se-mantic class, Pantel and Ravichandran (2004) pro-posed the use of lexico-semantic patterns involv-ing each member of that class to learn a (usu-ally hypernym) label.
The proposed method wasshown to perform well over the semantically ho-mogeneous, fine-grained clusters learned by CBC(Pantel and Lin, 2002), but for the coarse-grained,heterogeneous topics learned by topic modelling,it is questionable whether it would work as well.The first works to report on human scoring oftopics were Chang et al (2009) and Newman etal.
(2010).
The first study used a novel but syn-thetic intruder detection task where humans eval-uate both topics (that had an intruder word), andassignment of topics to documents (that had an in-truder topic).
The second study had humans di-rectly score topics learned by a topic model.
Thislatter work introduced the pointwise mutual infor-mation (PMI) score to model human scoring.
Fol-lowing this work, we use PMI as features in theranking SVR model.3 MethodologyOur task is to predict which words annotatorstend to select as most representative or best wordswhen presented with a list of ten words.
Sinceannotators are not generally unanimous in theirchoice of best word, we formulate this as a rank-ing task, and treat the top-1, 2 and 3 system-ranked items as the best words, and compare thatto the top-1, 2 and 3 words chosen most frequentlyby annotators.
In this section, we describe the fea-tures that may be useful for this ranking task.
Westart with features motivated by word association.An obvious idea is that the most representativeword should be readily evoked by other wordsin the topic.
For example, given a list of words?space, earth, moon, nasa, mission?, which is a606Space Exploration topic, space could arguably bethe most representative word.
This is becauseit is natural to think about the word space afterseeing the words earth, moon and nasa individ-ually.
A good candidate for best word could bethe word that has high average conditional proba-bility given each of the other words.
To calculateconditional probability, we use word counts fromthe entire collection of English Wikipedia articles.Conditional probability is defined as:P (wi|wj) =P (wi, wj)P (wj),where i 6= j and P (wi, wj) is the probability ofobserving both wi and wj in the same sliding win-dow, and P (wi) is the overall probability of wordwi in the corpus.
In the above example, evoked bymeans that space would fill the slot of wi.
The av-erage conditional probability for word wi is givenby:avg-CP1(wi) = 19?jP (wi|wj),for j = 1 .
.
.
10, j 6= i (this range of indices ap-plies to all following average quantities).In other cases, we have the flip situation, wherethe most representative word may evoke (ratherthan be evoked by) other words in the list of tenwords.
Imagine a NASCAR Racing topic, whichhas a list of words ?race, car, nascar, driver, rac-ing?.
Given the word nascar, words from the listsuch as race, car, racing and driver might cometo mind because nascar is heavily associated withthese words.
Therefore, a good candidate, wi,might also correlate with high P (wj |wi).
As be-fore, the average conditional probability (here de-noted with CP2) for word wi is given by:avg-CP2(wi) = 19?jP (wj |wi).Another approach to measuring word associa-tion is by calculating pointwise mutual informa-tion (PMI) between word pairs.
Unlike condi-tional probability, PMI is symmetric and thus theorder of words in a pair does not matter.
Wecalculate PMI using word counts from EnglishWikipedia as follows:PMI(wi, wj) = log P (wi, wj)P (wi)P (wj) .The average PMI for word wi is given by:avg-PMI(wi) = 19?jPMI(wi, wj).The topic model produces an ordered list ofwords for each topic, and the ordering is given bythe marginal probability of each word given thattopic, P (wi|tj).
The ranking of words based onthese probabilities indicates the importance of aword in a topic, and it is also a feature that we usefor predicting the most representative word.We also observe that sometimes the most repre-sentative words are generalized concepts of otherwords.
As such, hypernym relations could be an-other feature that may be relevant to predicting thebest word.
To this end, we use WordNet to findhypernym relations between pairs of words in atopic and obtain a set of boolean-valued relation-ships for each topic word.Our last feature is the distributional similar-ity scores of Pantel et al (2009), as trained overWikipedia.1 This takes the form of representingthe distributional similarity between each pairingof terms sim(wi|wj); if wi is not in the top-200most similar terms for a given wj , we assume it tohave a similarity of 0.While the above features can be used aloneto get a ranking on the ten topic words, we canalso use various combinations of features in areranking model such as support vector regres-sion (SVMrank: Joachims (2006)).
Applying thefeatures described above ?
conditional probabil-ities, PMI, WordNet hypernym relations, the topicmodel word rank, and Pantel?s distributional simi-larity score ?
as features for SVMrank, a rankingof words is produced and candidates for the mostrepresentative word are selected by choosing thetop-ranked words.607NEWS stock market investor fund trading investment firm exchange ...police gun officer crime shooting death killed street victim ...food restaurant chef recipe cooking meat meal kitchen eat...patient doctor medical cancer hospital heart blood surgery ...BOOKS loom cloth thread warp weaving machine wool cotton yarn ...god worship religion sacred ancient image temple sun earth ...crop land wheat corn cattle acre grain farmer manure plough ...sentence verb noun adjective grammar speech pronoun ...Figure 1: Selected topics from the two collections(each line is one topic, with fewer than ten topicwords displayed because of limited space)4 DatasetsWe used two collections of text documents fromdifferent genres for our experiments.
The first col-lection (NEWS) was created by selecting 55,000news articles from the LDC Gigaword corpus.The second collection (BOOKS) was 12,000 En-glish language books selected from the Inter-net Archive American Libraries collection.
TheNEWS and BOOKS collections provide a diverserange of content for topic modeling.
In the firstcase ?
news articles from the past decade writtenby journalists ?
each article usually attempts toclearly and concisely convey information to thereader, and hence the learned topics tend to befairly interpretable.
For BOOKS (with publicationdates spanning more than a century), the writingstyle often uses lengthy and descriptive prose, soone sees a different style to the learned topics.The input to the topic model is a bag-of-wordsrepresentation of the collection of text documents,where word counts are preserved, but word orderis lost.
After performing fairly standard tokeniza-tion and limited lemmatisation, and creating a vo-cabulary of terms that occurred at least ten times,each corpus was converted into its bag-of-wordsrepresentation.
We learned topic models for thetwo collections, choosing a setting of T = 200topics for NEWS and T = 400 topics for BOOKS.After computing the PMI-score for each topic (ac-cording to Newman et al (2010)), we selected 60topics with high PMI-score, and 60 topics withlow PMI-score, from both corpora, resulting in atotal of 240 topics for human evaluation.The 240 topics selected for human scoring were1Accessed from http://demo.patrickpantel.com/Content/LexSem/thesaurus.htm.Features DescriptionPMI Pointwise mutual informationCP1 Conditional probability P (wi|?
)CP2 Conditional probability P (?|wi)TM Rank Original topic model word rankHypernym WordNet hypernym relationshipsPDS Pantel distributional similarity scoreTable 1: Description of feature setseach evaluated by between 10 and 20 users.
Forthe two topic models, we used the conventionalapproach of displaying each topic with its top-10terms.
In a typical survey, a user was asked toevaluate anywhere from 60 to 120 topics.
The in-structions asked the user to perform the followingtasks, for each topic in the survey: (a) score thetopic for ?usefulness?
or ?coherence?
on a scaleof 1 to 3; and (b) select the single best word thatexemplifies the topic (when score=3).From both NEWS and BOOKS, the 40 topicswith the highest average human scores had rela-tively complete data for the ?best word?
selectiontask (i.e.
every time a user gave a topics score=3,they also selected a ?best word?).
The remain-der of this paper is concerned with the 40 NEWStopics and 40 BOOKS topics where we had ?bestword?
data from the annotators.
Sample topicsfrom these two sets are given in Figure 1.To measure presentational bias (i.e.
the extentto which annotators tend to choose a word seenearlier rather than later, particularly when armedwith the knowledge that words are presented in or-der of probability), we reissued a survey using the40 NEWS topics to ten additional annotators, butthis time the top-10 topic words were presentedin random order.
Again, these ten new annotatorswere asked to select the best word.5 ExperimentsWe used average PMI and conditional probabili-ties, CP1 and CP2, to rank the ten words in eachtopic.
Candidates for the best words were selectedby choosing the top-1, 2 and 3 ranked words.We used the following weighted scoring func-tion for evaluation:Best-N score =?Ni=1 n(wrevi)?Ni=1 n(wi)608Features Best-1 Best-2 Best-3Baseline 0.35 0.50 0.59PMI 0.25 0.38 0.49CP1 0.30 0.42 0.51CP2 0.15 0.27 0.45Upper bound 0.48 ?
?Table 2: Best-1,2,3 scores for ranking with singlefeature sets (PMI and both conditional probabili-ties) for NEWSFeatures Best-1 Best-2 Best-3Baseline 0.38 0.48 0.60PMI 0.25 0.38 0.49CP1 0.30 0.38 0.47CP2 0.15 0.30 0.49Upper bound 0.64 ?
?Table 3: Best-1,2,3 scores for ranking with singlefeature sets (PMI and both conditional probabili-ties) for BOOKSwhere wrevi is the ith term ranked by the systemand wi is the ith most popular term selected byannotators; revi gives the index of the word wiin the annotator?s list; and n(w) is the number ofvotes given by annotators for word w.The baseline is obtained using the original wordrank produced by the topic model based on topicword probabilities P (wi|tj).
An upperbound iscalculated by evaluating the decision of an annota-tor against others for each topic.
This upperboundsignifies the maximum accuracy for human anno-tators on average; since the annotators were askedto pick a single best word in the survey, only theBest-1 upperbound can be obtained.The Best-1/2/3 results are summarized in Ta-ble 2 for NEWS and Table 3 for BOOKS.
TheseBest-N scores are computed just using the singlefeature of PMI, CP1 and CP2 (each in turn) to rankthe words in each topic.
None of these featuresalone produces a result that exceeds baseline per-formance.To make better use of all the features describedin Section 3, namely the PMI score, conditionalprobabilities (both directions), topic model wordrank, WordNet Hypernym relationships and Pan-tel?s distributional similarity score, we build aranking classifier using SVMrank and evaluatingFeature Set Best-1 Best-2 Best-3Baseline 0.35 0.50 0.59All Features 0.43 0.56 0.62?PMI 0.45 (+0.02) 0.52 (?0.04) 0.62 (?0.00)?CP1 0.35 (?0.08) 0.49 (?0.07) 0.57 (?0.05)?CP2 0.40 (?0.03) 0.50 (?0.06) 0.61 (?0.01)?TM Rank 0.40 (?0.03) 0.52 (?0.04) 0.57 (?0.05)?Hypernym 0.43 (?0.00) 0.57 (+0.01) 0.62 (?0.00)?PDS 0.43 (?0.00) 0.53 (?0.03) 0.62 (?0.00)Upper bound 0.48 ?
?Table 4: SVR-based best topic word results forNEWS for all six feature types, and feature abla-tion over each (numbers in brackets show the rel-ative change over the full feature set)Feature Set Best-1 Best-2 Best-3Baseline 0.38 0.48 0.60All Features 0.40 0.51 0.62?PMI 0.38 (?0.02) 0.51 (?0.00) 0.63 (+0.01)?CP1 0.33 (?0.07) 0.47 (?0.04) 0.56 (?0.06)?CP2 0.40 (?0.00) 0.50 (?0.01) 0.64 (+0.02)?TM Rank 0.35 (?0.05) 0.49 (?0.02) 0.63 (+0.01)?Hypernym 0.40 (?0.00) 0.50 (?0.01) 0.61 (?0.01)?PDS 0.45 (+0.05) 0.48 (?0.03) 0.67 (+0.05)Upper bound 0.64 ?
?Table 5: SVR-based best topic word results forBOOKS for all six feature types, and feature abla-tion over each (numbers in brackets show the rel-ative change over the full feature set)using 10-fold cross validation.
Our first approachis to use the entire set of features to train the clas-sifier.
Following this, we also measure the effectof each feature by ablating (removing) one fea-ture at a time.
The drop in Best-N score indicateswhich features are the strongest predictors of thebest words (a larger drop in score indicates thatfeature is more important).
The results for Best-1,Best-2 and Best-3 scores are summarized in Ta-ble 4 for NEWS, and Table 5 for BOOKS (averagedacross the 10 iterations of cross validation).We then produced a condensed set of features,consisting of the conditional probabilities, theoriginal topic model word rank and the WordNethypernym relationships.
This ?best?
set of fea-tures is used to make predictions of best words.Results are improved in most cases, and are sum-marized in Table 6 for both NEWS and BOOKS.609Dataset Best-1 Best-2 Best-3NEWSBaseline 0.35 0.50 0.59Best Feat.
Set 0.45 0.50 0.65Upper bound 0.48 ?
?BOOKSBaseline 0.38 0.48 0.60Best Feat.
Set 0.48 0.56 0.66Upper bound 0.64 ?
?Table 6: Results with the best feature set com-pared to the baselineDataset Best-1 Best-2 Best-3NEWS baseline 0.35 0.50 0.59BOOKS ?
NEWS 0.38 0.56 0.62NEWS upper bound 0.48 ?
?BOOKS baseline 0.38 0.48 0.60NEWS ?
BOOKS 0.48 0.56 0.65BOOKS upper bound 0.64 ?
?Table 7: Results for cross-domain learningWe also tested whether the SVM classifiercould be trained using data from one domain, andrun on data from another domain.
Using our twodatasets as these different domains, we trained amodel using BOOKS data and made predictionsfor NEWS, and then we trained a model usingNEWS data and made predictions for BOOKS.The results, shown in Table 7, indicate thatwe are still able to outperform the baseline, evenwhen the ranking classifier is trained on a differ-ent domain.
In fact, when we trained a modelusing NEWS, we saw almost no drop in perfor-mance for predicting best words for BOOKS, andimprovement is seen for Best-2 score from NEWS.This implies that the SVM classifier generalizeswell across domains and suggests the possibilityof having a fixed training model to predict bestwords for any data.In these experiments, topic words are presentedin the original order that the topic model produces,i.e.
in descending order of probability of a wordunder a topic P (wi|tj).
We noticed that the firstwords of the topics are frequently selected as thebest words by annotators, and suspected that thiswas introducing a bias towards the first word.
Asour baseline scores are derived from this topicword ordering, such a bias could give rise to anartificially high baseline.To investigate this effect, we ran a second anno-Word Order Best-1 Best-2 Best-3Original 0.35 0.50 0.59Randomized 0.23 0.33 0.46Table 8: Reduction of baseline scores for NEWSwhen words are presented in random order to an-notators.2 4 6 8 100.00.10.20.30.4RankFractionofhumanselectedbestwordorderedrandomFigure 2: Bias for humans selecting the best word,when the topic words are presented in their origi-nal ordering (ordered) or randomised (random)tation exercise over the same set of topics (but dif-ferent annotators), to obtain a new set of best wordannotations for NEWS, with the topic words pre-sented in random order.
In Figure 2, we plot thecumulative proportion of words selected as bestword by the annotators across the topics, in thecase of the random topic word order, mapping thetopic words back onto their original ranks in thetopic model.
A slight drop can be observed in theproportion of first- and second-ranked topic wordsbeing selected when we randomise the topic wordorder.
When we recalculate the baseline accuracyfor NEWS on the basis of the new set of annota-tions, we observe an appreciable drop in the scores(see Table 8).6 DiscussionFrom the experiments in Section 5, perhaps thefirst thing to observe is: (a) the high performanceof the baseline, and (b) the relatively low (Best-1) upper bound accuracy for the task.
The first isperhaps unsurprising, given that it represents the610topic model?s own interpretation of the word(s)which are most representative of that topic.
In thissense, we have set our sights high in attempting tobetter the baseline.
The upper bound accuracy isa reflection of both the inter-annotator agreement,and the best that we can meaningfully expect todo for the task.
That is, any result higher than thiswould paradoxically suggest that we are able to dobetter at a task than humans, where we are evalu-ating ourselves relative to the labellings of thosehumans.
The upper bound for NEWS was slightlyless than 0.5, indicating that humans agree on thebest topic word only 50% of the time.
To betterunderstand what is happening here, consider thefollowing topic from Figure 1:health drug patient medical doctor hospitalcare cancer treatment diseaseThis is clearly a coherent topic, but at least twotopic words suggest themselves as labels: healthand medical.
By way of having between 10 and 20annotators (uniquely) label a given topic, and in-terpreting the multiple labellings probabilistically,we are side-stepping the inter-annotator agree-ment issue, but ultimately, for the Best-1 evalu-ation, we are forced to select one term only, andconsider any alternative to be wrong.
Because an-notators selected only one best topic word, we un-fortunately have no way of performing Best-2 orBest-3 upper bound evaluation and deal with top-ics such as this, but would expect the numbers torise appreciably.Looking at the original feature rankings in Ta-bles 2 and 3, no clear picture emerges as to whichof the three methods (PMI, CP1 and CP2) wasmost successful, but there were certainly clear dif-ferences in the relative numbers for each, point-ing to possible complementarity in the scoring.This expectation was born out in the results forthe reranking model in Tables 4 and 5, where thecombined feature set surpassed the baseline in allcases, and feature ablation tended to lead to a dropin results, with the single most effective feature setbeing CP1 (P (wi|?
)), followed by CP2 (P (?|wi))and topic model rank.
The lexical semantic fea-tures of WordNet hypernymy and PDS (Pantel?sdistributional similarity) were the worst perform-ers, often having no or negative impact on the re-sults.Comparing the best results for the SVR-basedreranking model and the upper bound Best-1score, we approach the upper bound performancefor NEWS, but are still quite a way off withBOOKS when training in-domain.
This is encour-aging, but a slightly artificial result in terms of thebroader applicability of this research, as what itmeans in practical terms is that if we can accessmulti-annotator best word labelling for the ma-jority of topics in a given topic model, we canuse those annotations to predict the best word forthe remainder of the topics with reasonably suc-cess.
When we look to the cross-domain results,however, we see that we almost perfectly replicatethe best-achieved Best-1, Best-2 and Best-3 in-domain results for BOOKS by training on NEWS(making no use of the annotations for BOOKS).Applying the annotations for BOOKS to NEWS isless successful in terms of Best-1 accuracy, but weactually achieve higher Best-2, and largely mir-ror the Best-3 results as compared to the best ofthe in-domain results in Table 6.
This leads tothe much more compelling conclusion that we cantake annotations from an independent topic model(based on a completely unrelated document col-lection), and apply them to successfully model thebest topic word for a new topic model, withoutrequiring any additional annotation.
As we nowhave two sets of topics multiply-annotated for bestwords, this result suggests that we can perform thebest topic word selection task with high successover novel topic models.We carried out manual analysis of topics wherethe model did particularly poorly, to get a sensefor how and where our model is being led astray.One such example is the topic:race car nascar driver racing cup winston teamgordon seasonwhere the following topic words were selected byour annotators: nascar (8 people), race (2 peo-ple), and racing (2 people).
First, we observe thesplit between race and racing, where more judi-cious lemmatisation/stemming would make boththe annotation easier and the evaluation cleaner.The SVR model tends to select more common,general terms, so in this case chose race as thebest word, and ranked nascar third.
This is one611instance were nascar evokes all of the other wordseffectively, but not conversely (racing is asso-ciated with many events/sports beyond nascar,e.g.
).Another topic where our model had difficultywas:window nave aisle transept chapel tower archpointed arches roofwhere our best model selected nave, while the hu-man annotators selected chapel (6 people), arch(2 people), nave, roof , tower and transept (1 per-son each).
Clearly, our annotators struggled tocome up with a best word here, despite the topicagain being coherent.
This is an obvious candi-date for labelling with a hypernym/holonym ofthe topic words (e.g.
church or church architec-ture), and points to the limitations of best word la-belling ?
there are certainly many topics wherebest word labelling works, as our upper boundanalysis demonstrated, but there are equally manytopics where the most natural label is not foundin the top-ranked topic words.
While this pointsto slight naivety in the current task set up ?
weare forcing annotators to label words with topicwords, where we know that this is sub-optimalfor a significant number of topics ?
we contendthat our numbers suggest that: (a) consistent besttopic word labelling is possible at least 50% ofthe time; and (b) we have developed a methodwhich is highly adept at labelling these topics.
Asa way forward, we intend to relax the constrainton the topic label needing to be based on a topicword, and explore the possibility of predictingwhich topics are best labelled with topic words,and which require independent labels.
For topicswhich can be labelled with topic words, we canuse the methodology developed here, and for top-ics where this is predicted to be sub-optimal, weintend to build on the work of Mei et al (2007),Pantel and Ravichandran (2004) and others in se-lecting phrasal/hypernym labels for topics.
We arealso interested in applying the methodology pro-posed herein to the closely-related task of intruderword, or worst topic word, detection, as proposedby Chang et al (2009).Finally, looking to the question of the impact ofthe presentation order of the topic words on bestword selection, it would appear that our baselineis possibly an over-estimate (based on Table 8).Having said that, the flipside of the bias is that itleads to more consistency in the annotations, andtends to help in tie-breaking of examples such asrace and racing from above, for example.
In sup-port of this claim, the upper bound Best-1 accu-racy of the randomised annotations, relative to theoriginal gold-standard is 0.44, slightly below theoriginal upper bound for NEWS.
More work isneeded to determine the real impact of this biason the overall task setup and evaluation.7 ConclusionThis paper has presented the novel task of besttopic word selection, that is the selection of thetopic word that is the best label for a given topic.We proposed a number of features intended tocapture the best topic word, and demonstratedthat, while they were relatively unsuccessful inisolation, in combination as inputs to a rerank-ing model, we were able to consistently achieveresults above the baseline of simply selecting thehighest-ranked topic word, both when training in-domain over other labelled topics for that topicmodel, and cross-domain, using only labellingsfrom independent topic models learned over docu-ment collections from different domains and gen-res.AcknowledgementsNICTA is funded by the Australian government as repre-sented by Department of Broadband, Communication andDigital Economy, and the Australian Research Councilthrough the ICT centre of Excellence programme.
DN hasalso been supported by a grant from the Institute of Museumand Library Services, and a Google Research Award.ReferencesBlei, D.M., A.Y.
Ng, and M.I.
Jordan.
2003.
LatentDirichlet alocation.
Journal of Machine LearningResearch, 3:993?1022.Brody, S. and M. Lapata.
2009.
Bayesian word senseinduction.
In Proceedings of the 12th Conferenceof the EACL (EACL 2009), pages 103?111, Athens,Greece.Chang, J., J. Boyd-Graber, S. Gerrish, C. Wang, andD.
Blei.
2009.
Reading tea leaves: How humansinterpret topic models.
In Proceedings of the 23rd612Annual Conference on Neural Information Process-ing Systems (NIPS 2009), pages 288?296, Vancou-ver, Canada.Griffiths, T. and M. Steyvers.
2004.
Finding scien-tific topics.
Proceedings of the National Academyof Sciences, 101:5228?5235.Haghighi, A. and L. Vanderwende.
2009.
Explor-ing content models for multi-document summariza-tion.
In Proceedings of the North American Chapterof the Association for Computational Linguistics ?Human Language Technologies 2009 (NAACL HLT2009), pages 362?370, Boulder, USA.Joachims, T. 2006.
Training linear SVMs in lin-ear time.
In Proceedings of the ACM Conferenceon Knowledge Discovery and Data Mining (KDD),pages 217?226, Philadelphia, USA.Magatti, D., S. Calegari, D. Ciucci, and F. Stella.
2009.Automatic labeling of topics.
In Proceedings of theInternational Conference on Intelligent Systems De-sign and Applications, pages 1227?1232, Pisa, Italy.Mei, Q., C. Liu, H. Su, and C. Zhai.
2006.
A prob-abilistic approach to spatiotemporal theme patternmining on weblogs.
In Proceedings of the 15thInternational World Wide Web Conference (WWW2006), pages 533?542.Mei, Q., X. Shen, and C. Zhai.
2007.
Automatic la-beling of multinomial topic models.
In Proceedingsof the 13th ACM SIGKDD International Conferenceon Knowledge Discovery and Data Mining (KDD2007), pages 490?499, San Jose, USA.Newman, D., J.H.
Lau, K. Grieser, and T. Baldwin.2010.
Automatic evaluation of topic coherence.In Proceedings of Human Language Technologies:The 11th Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics (NAACL HLT 2010), pages 100?108, LosAngeles, USA.Pantel, P. and D. Lin.
2002.
Discovering wordsenses from text.
In Proceedings of the 8th ACMSIGKDD International Conference on KnowledgeDiscovery and Data Mining, pages 613?619, Ed-monton, Canada.Pantel, P. and D. Ravichandran.
2004.
Automati-cally labeling semantic classes.
In Proceedings ofthe 4th International Conference on Human Lan-guage Technology Research and 5th Annual Meet-ing of the NAACL (HLT-NAACL 2004), pages 321?328, Boston, USA.Pantel, P., E. Crestan, A. Borkovsky, A-M. Popescu,and V. Vyas.
2009.
Web-scale distributional sim-ilarity and entity set expansion.
In Proceedings ofthe 2009 Conference on Empirical Methods in Nat-ural Language Processing (EMNLP 2009), pages938?947, Singapore.Ritter, A, Mausam, and O Etzioni.
to appear.
A la-tent Dirichlet alocation method for selectional pref-erences.
In Proceedings of the 48th Annual Meetingof the ACL (ACL 2010), Uppsala, Sweden.Titov, I. and R. McDonald.
2008.
Modeling on-line reviews with multi-grain topic models.
In Pro-ceedings of the 17th International World Wide WebConference (WWW 2008), pages 111?120, Beijing,China.Wang, X. and A. McCallum.
2006.
Topics over time:A non-Markov continuous-time model of topicaltrends.
In Proceedings of the 12th ACM SIGKDDInternational Conference on Knowledge Discoveryand Data Mining (KDD 2006), pages 424?433,Philadelphia, USA.Wei, S. and W.B.
Croft.
2006.
LDA-based documentmodels for ad-hoc retrieval.
In Proceedings of 29thInternational ACM-SIGIR Conference on Researchand Development in Information Retrieval (SIGIR2006), pages 178?185, Seattle, USA.613
