Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1244?1249,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsChain based RNN for Relation ClassificationJavid Ebrahimi and Dejing DouDepartment of Computer and Information Science, University of OregonEugene, Oregon 97403, USA{javid,dou}@cs.uoregon.eduAbstractWe present a novel approach for relation clas-sification, using a recursive neural network(RNN), based on the shortest path betweentwo entities in a dependency graph.
Previ-ous works on RNN are based on constituency-based parsing because phrasal nodes in a parsetree can capture compositionality in a sen-tence.
Compared with constituency-basedparse trees, dependency graphs can representrelations more compactly.
This is particu-larly important in sentences with distant en-tities, where the parse tree spans words thatare not relevant to the relation.
In suchcases RNN cannot be trained effectively in atimely manner.
However, due to the lack ofphrasal nodes in dependency graphs, applica-tion of RNN is not straightforward.
In orderto tackle this problem, we utilize dependencyconstituent units called chains.
Our experi-ments on two relation classification datasetsshow that Chain based RNN provides a shal-lower network, which performs considerablyfaster and achieves better classification results.1 IntroductionRelation extraction is the task of finding relationsbetween entities in text, which is useful for sev-eral tasks such as information extraction, summa-rization, and question answering (Wu and Weld,2010).
For instance, in the sentence: those ?cancers?were caused by radiation ?exposures,?
the two enti-ties have a cause-effect relation.
As reported in de-tail (SaraWagi, 2008), one approach to the probleminvolves supervised methods where the models relyon lexical, syntactic, and semantic features to clas-sify relations between pairs of entities.
The down-side of this approach is that one has to retrain themodel for other domains with different target rela-tions.
Thus it is not scalable to the web, where thou-sands of (previously-unseen) relations exist (Bankoet al, 2007).
To address this problem, Open Infor-mation Extraction is proposed, which does not re-quire supervision.
In these systems (Banko et al,2007; Mausam et al, 2012), patterns based on lex-ical, syntactic, POS, and dependency features areextracted.
While these patterns give good preci-sion, they suffer from low recall (Banko and Etzioni,2008).
This is because they fail to extract patternswhich have not been pre-specified, and thereby areunable to generalize.Recursive Neural Network (RNN) has proven tobe highly successful in capturing semantic compo-sitionality in text and has improved the results ofseveral Natural Language Processing tasks (Socheret al, 2012; Socher et al, 2013).
Previous ap-plications of Recursive Neural Networks (RNN) tosupervised relation extraction (Socher et al, 2012;Hashimoto et al, 2013; Khashabi, 2013) are basedon constituency-based parsers.
These RNNs mayspan words that do not contribute to the relation.
Weinvestigate the incorporation of dependency parsinginto RNN that can give a more compact representa-tion of relations.Our contribution is introducing a compositionalaccount of dependency graphs that can matchRNN?s recursive nature, and can be applied to re-lation classification.
We study different data struc-tures that incorporate dependency trees into RNNs.1244One of these structures produces a compact full bi-nary tree that compared with the constituency-basedRNN, has higher classification accuracy and savesup to 70% in the training time.2 Related WorkAt the core of deep learning techniques for NLP, liesthe vector based word representation, which mapswords to an n-dimensional space.
Having word vec-tors as parameters makes neural models flexible infinding different word embeddings for separate tasks(Collobert and Weston, 2008).
Recursive NeuralNetwork (RNN) is a recursive deep architecture thatcan learn feature representation of words, phrasesand sentences.As an example, in (Socher et al, 2010), eachnode in the parse tree is associated with a vectorand at each internal node p, there exists a compo-sition function that takes its input from its childrenc1?
Rnand c2?
Rn.p = f(c1, c2) = tanh(W[c1c2]+ b) (1)The matrix W ?
Rn?2nis the global compositionparameter, b is the bias term, and the output of thefunction p ?
Rnis another vector in the space of in-puts.
Socher et al (2012) propose Matrix-VectorRecursive Neural Network (MV-RNN), where in-stead of using only vectors for words, an additionalmatrix for each word is used to capture operator se-mantics in language.
To apply RNN to relation clas-sification, they find the path in the parse tree betweenthe two entities and apply compositions bottom up.Hashimoto et al (2013) follow the same design butintroduce a different composition function.
Theymake use of word-POS pairs and use untied weightsbased on phrase categories of the pair.Socher et al (2014) introduce a dependency-based RNN that extracts features from a dependencygraph whose composition function has major differ-ences from ours.
Their function consists of a linearsum of unary compositions, while our function is abinary composition of children.
Our work is alsorelated to (Bunescu and Mooney, 2005), where thesimilarity between the words on the path connect-ing two entities in the dependency graph is used todevise a Kernel function.3 Chain based RNNWhile constituency-based parsing seems to be a rea-sonable choice for compositionality in general, itmay not be the best choice for all NLP tasks.
In par-ticular, for relation classification, one may prefer touse a structure that encodes more information aboutthe relations between the words in a sentence.
Tothis end, we use dependency-based parsing that pro-vides a one-to-one correspondence between nodes ina dependency graph (DG).DGs are significantly different from constituencyparse trees since they lack phrasal nodes.
More pre-cisely, the internal nodes where the nonlinear com-binations take place, do not exist in DGs.
There-fore, we modify the original RNN and present adependency-based RNN for relation classification.In our experiments, we restrict ourselves to treeswhere each dependent has only one head.
We alsouse the example in Figure 1 for better illustration; inthis example the arguments of the relation are childand cradle.wrappedchild[arg1]thewascarefullyintocradle[arg2]theFigure 1: DG: the child was carefully wrapped into the cradle.We apply compositions on the words on the short-est path between entities.
From a linguistic point ofview, this type of composition is related to the con-cept of chain or dependency constituent unit in DGs(Osborne, 2005).Chain: The words A ... B ... C ... (orderirrelevant) form a chain iff A immediatelydominates (is the parent of) B and C, or ifA immediately dominates B and B imme-diately dominates C.1245Based on this definition, child wrapped, into cradle,wrapped into cradle, child wrapped into cradle allqualify as a chain while child was does not.
To il-lustrate the motivation to use dependency parsing,consider the sentence:The hidden ?camera,?
found by a securityguard, was hidden in a business card-sized?box?
placed at an unmanned ATM.The shortest path between entities is:camera?
found?
hidden?
in?
boxUsing dependency parsing, we only need four com-positions for this chain, which results in 86% de-crease against constituency-based parsing.Now with all words represented as vectors, weneed to find a reduced dimensional representation ofthe chain in fixed size.
To this end, we transform thischain to a data structure, the root of which representsthe extracted features.3.1 Fixed StructureWe cannot use an off-the-shelf syntax parser to cre-ate a tree for the chain because the chain may notnecessarily be a coherent English statement.
Thus,we build two Directed Acyclic Graph (DAG) struc-tures by heuristics.
The idea is to start from ar-gument(s) and recursively combine dependent-headpairs to the (common) ancestor i.e., each head iscombined with the subtree below itself.
In the sim-plest case: a?
b results in p = f(a, b).The subtlety of this approach lies in the treatmentof the word with two dependents.
We use two meth-ods to handle such a node: 1) including it in onlyone composition as in Figure 2 or 2) including it intwo compositions and sum their results as in Figure3.Both structures produce a DAG where each inter-nal node has two children and there is only one nodewith two non-leaf children.
We now prove that thisgreedy algorithm results in a full binary tree for thefirst case.
We skip the proof of the algorithm for thesecond case which produces a full binary DAG.Lemma: There is at most one node with exactly twonone-leaf children in the tree.Proof.
If one of the arguments is an ancestor ofthe other argument e.g., arg1 ?
... ?
arg2, thenx7= f(x5, x6)x5= f(x1, x2) x6= f(x3, x4)x1= child x2= wrappedx3= intox4= cradleFigure 2: a fixed tree exampleobviously every head on the chain has exactly onedependent.
Combination of each head and its sub-tree?s output vector results in a full binary node inthe tree.
If the arguments have a common ancestorp e.g., arg1 ?
... p ... ?
arg2, then that particularnode has two dependents.
In this case, the parent iscombined with either its left or right subtrees, and itsresult is combined with the output of the other child.No other head has this property; otherwise, p is notthe common ancestor.Theorem: The algorithm converts a chain to a fullbinary tree.Proof.
The leaves of the tree are words of the chain.By applying the lemma, there exists one root and allinternal nodes have exactly two children.Note that we only consider dependency trees asthe input; so each pair of arguments has a uniquecommon ancestor.
Concretely, having a connectedgraph leads to at least one such ancestor and havingonly one head for each node (being a tree) leads toexactly one such ancestor.3.2 Predicted Tree StructureInstead of using a deterministic approach to cre-ate the tree, we can use Recursive Autoencoders(RAE) to find the best representation of the chain.This model is similar to (Socher et al, 2011) withsome modification in implementation.
Socher et al(2011) use a semi supervised method where the ob-jective function is a weighted sum of the supervisedand unsupervised error.
We achieved better resultswith a pipeline where first, during pre-training, theunsupervised autoencoder predicts the structure ofRNN and then during training, the supervised crossentropy error is minimized.1246x8=?i?6,7xix6= f(x1, x2) x7= f(x2, x5)x1= child x2= wrappedx5= f(x3, x4)x3= intox4= cradleFigure 3: a fixed DAG example4 LearningTo predict the label of the relation, a softmax clas-sifier is added on top of the tree.
i.e., yi=softmax(PTnWlabel) where L ?
Rk, k is the num-ber of classes, and Pnis the final vector on top ofthe tree for sentence n. The objective function is thesum of cross entropy error at all the nodes, for all thesentences in the training set.E(?)
= ?
?n?ktknlog ykn+?2?
?
?2(2)The vectors for target, predicted labels, and regular-ization parameters are denoted by tn, ynand ?
re-spectively.
We initialize the word vectors with pre-trained 50-dimensional words from (Collobert andWeston, 2008) and initialize other parameters by anormal distribution with mean of 0 and standard de-viation of 0.01.
Derivatives are computed by back-propagation through structure (Goller and Kuchler,1996) and L-BFGS is used for optimization.5 ExperimentsIn this section we discuss our experimental resultson two datasets for relation classification.
To derivethe dependency tree for each sentence, we use arc-eager MaltParser (Goldberg and Nivre, 2012).
Weset the hyper-parameters through a validation set forthe first dataset and use them for the second datasettoo.
Similar to the previous works, a few internalfeatures were also added e.g., depth of the tree, dis-tance between entities, context words, and the typeof dependencies in our model.
We found that usingdependency types inside the composition function asin typed functions worsens the results.5.1 SemEval-2010 Task 8This data set consists of 10017 sentences and ninetypes of relations between nominals (Hendrickx etal., 2010).
Table 1 compares the results of our treebased chain RNN (C-RNN), DAG based chain RNN(DC-RNN) and the autoencoder based one (C-RNN-RAE) with other RNN models and the best systemparticipating (Rink and Harabagiu, 2010) in the task.Evaluation of the systems is done by comparing theF-measure of their best runs.
The best system (Rinkand Harabagiu, 2010) uses SVM with many sets offeatures.
We add some external features using super-sense sequence tagger (Ciaramita and Altun, 2006).Adding POS tags, WordNet hypernyms, and namedentity tags (NER) of the two arguments helps C-RNN improve the results.We implement SDT-RNN (Socher et al, 2014)which has similar complexity as our model but hassignificantly lower F-measure.
SDT-RNN also per-forms much better when considering only the wordson the path between entities; confirming our hy-pothesis about the effectiveness of chains.
This canbe attributed to the intuitive advantage of depen-dency trees where the shortest path between entitiescaptures most of the information about the relation(Bunescu and Mooney, 2005).As it can bee seen in Table 1, C-RNN achievesthe best results.
The baseline RNN, uses a globalcomposition function andR50vectors for each word.We also use the same number of model parameters.The advantage of our approach is that our modelsare computationally less expensive compared withother RNN models.
MV-RNN (Socher et al, 2012)uses an additional matrix R50?50for each word,resulting in a 50 fold increase in the number ofmodel parameters.
POS-RNN (Hashimoto et al,2013) uses untied weight matrices and POS basedword vectors that results in about 100% increase inthe number of model parameters compared with C-RNN.Relations with long distances between entities areharder to classify.
This is illustrated in Figure 4where MV-RNN and C-RNN are compared.
Con-sidering three bins for the distance between two en-1247Method F-measure Feature setsRNN 74.8 -SDT-RNN 75.12 -MV-RNN 79.1 -POS-RNN 79.4 -DC-RNN 77.36 -C-RNN-RAE 78.78 -C-RNN 79.68 -SVM 82.2 POS, WordNet, Levine classes,PropBank, FrameNet, TextRun-ner, paraphrases, Google n-grams,NormLex-Plus, morphological fea-tures, dependency parse featuresMV-RNN 82.4 POS, NER, WordNetC-RNN 82.66 POS, NER, WordNetTable 1: Results on SemEval 2010 relation classification task with thefeature sets used.
C-RNN outperforms all RNN based models.
By in-cluding three extra features, it achieves the state-of-the-art performance.tities, the figure shows what fraction of test instancesare misclassified in each bin.
Both classifiers makemore errors when the distance between entities islonger than 10.
The performance of the two classi-fiers for distances less than five is quite similar whileC-RNN has the advantage in classifying more rela-tions correctly when the distance increases.5.2 SemEval-2013 Task 9.bTo further illustrate the advantage of C-RNN overMV-RNN, we evaluate our work on another data set.See Table 2.
In this task, the goal is to extract inter-actions between drug mentions in text.
The corpus(Segura-Bedmar et al, 2013) consists of 1,017 textsthat were manually annotated with a total of 5021drug-drug interactions of four types: mechanism, ef-fect, advise and int.Method Precision Recall F=measureMV-RNN 74.07 65.53 67.84C-RNN 75.31 66.19 68.64Table 2: Results on SemEval 2013 Drug-Drug Interaction task5.3 Training TimeDependency graphs can represent relations morecompactly by utilizing only the words on the shortestpath between entities.
C-RNN uses a sixth of neuralcomputations of MV-RNN.
More precisely, there isan 83% decrease in the number of tanh evaluations.Consequently, as demonstrated by Figure 5, C-RNNruns 3.21 and 1.95 times faster for SemEval 2010and SemEval 2013 respectively.range of entities distanced < 5 5 <= d < 10 10 <= dmissclassificationrate00.050.10.150.20.250.30.350.40.450.5MV-RNNC-RNNFigure 4: Misclassification based on entities distance in threebins.
More errors occur with entities separated by more than tenwords.
C-RNN performs better in bottleneck long distances.SemEval-2010 SemEval-2013runningtime05001000150020002500MV-RNNC-RNNFigure 5: Training time measured by seconds.
Experimentswere run on a cluster node with 6 core 2.66GHz cpu.6 ConclusionsRecently, Recursive Neural Network (RNN) hasfound a wide appeal in the Machine Learning com-munity.
This deep architecture has been applied inseveral NLP tasks including relation classification.We present an RNN architecture based on a com-positional account of dependency graphs.
The pro-posed RNN model is based on the shortest path be-tween entities in a dependency graph.
The resultingshallow network is superior for supervised learningin terms of speed and accuracy.
We improve theclassification results and save up to 70% in train-ing time compared with a constituency-based RNN.
The limitation of our Chain based RNN is that itassumes the named entities to be known in advance.This requires a separate named entity recognizer andcannot extract the entities jointly with the relationclassifier.AcknowledgmentThis work is partially supported by the NIH grantR01GM103309.1248ReferencesMichele Banko and Oren Etzioni.
2008.
The tradeoffsbetween open and traditional relation extraction.
InProceedings of ACL, pages 28?36.Michele Banko, Michael J Cafarella, Stephen Soderland,Matt Broadhead, and Oren Etzioni.
2007.
Open in-formation extraction from the web.
In Proceedings ofIJCAI, pages 2670?2676.Razvan Bunescu and Raymond J. Mooney.
2005.
AShortest Path Dependency Kernel for Relation Extrac-tion .
In Proceedings of HLT/EMNLP, pages 724?731.Massimiliano Ciaramita and Yasemin Altun.
2006.Broad-coverage sense disambiguation and informationextraction with a supersense sequence tagger.
In Pro-ceedings of EMNLP, pages 594?602.R.
Collobert and J. Weston.
2008.
A unified archi-tecture for natural language processing: Deep neuralnetworks with multitask learning.
In Proceedings ofICML, pages 160?167.Yoav Goldberg and Joakim Nivre.
2012.
A dynamic ora-cle for arc-eager dependency parsing.
In Proceedingsof COLING, pages 959?976.Christoph Goller and Andreas Kuchler.
1996.
Learningtask-dependent distributed representations by back-propagation through structure.
In Proceedings ofICNN, pages 347?352.Kazuma Hashimoto, Makoto Miwa, Yoshimasa Tsu-ruoka, and Takashi Chikayama.
2013.
Simple cus-tomization of recursive neural networks for seman-tic relation classification.
In Proceedings of EMNLP,pages 1372?1376.Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, PreslavNakov, Diarmuid?O S?eaghdha, Sebastian Pad?o, MarcoPennacchiotti, Lorenza Romano, and Stan Szpakow-icz.
2010.
Semeval-2010 task 8: Multi-way classifi-cation of semantic relations between pairs of nominals.In Proceedings of SemEval, pages 33?38.Daniel Khashabi.
2013.
On the recursive neural net-works for relation extraction and entity recognition.Technical report, UIUC.Mausam, Michael D Schmitz, Robert E. Bart, StephenSoderland, and Oren Etzioni.
2012.
Open LanguageLearning for Information Extraction .
In Proceedingsof EMNLP, pages 523?534.Timothy Osborne.
2005.
Beyond the constituent: a de-pendency grammar analysis of chains.
Folia Linguis-tica, 39(3-4):251?297.Bryan Rink and Sanda Harabagiu.
2010.
Utd: Classi-fying semantic relations by combining lexical and se-mantic resources.
In Proceedings of SemaEval, pages256?259.Sunita SaraWagi.
2008.
Information Extraction.
InFoundations and Trends in Databases,Volume 1 Issue3, pages 261?377.Isabel Segura-Bedmar, Paloma Mart?
?nez, and Mar?
?a Her-rero Zazo.
2013.
Semeval-2013 task 9 : Extrac-tion of drug-drug interactions from biomedical texts(DDIExtraction 2013).
In Proceedings of SemEval,pages 341?350.Richard Socher, Christopher D. Manning, and Andrew Y.Ng.
2010.
Learning Continuous Phrase Representa-tions and Syntactic Parsing with Recursive Neural Net-works.
In Deep Learning and Unsupervised FeatureLearning Workshop, NIPS, pages 1?9.Richard Socher, Jeffrey Pennington, Eric H. Huang, An-drew Y. Ng, and Christopher D. Manning.
2011.Semi-Supervised Recursive Autoencoders for Pre-dicting Sentiment Distributions.
In Proceedings ofEMNLP, pages 151?161.Richard Socher, Brody Huval, Christopher D. Manning,and Andrew Y. Ng.
2012.
Semantic Compositional-ity Through Recursive Matrix-Vector Spaces.
In Pro-ceedings of EMNLP, pages 1201?1211.Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,Christopher D. Manning, Andrew Y. Ng, and Christo-pher Potts.
2013.
Recursive Deep Models for Seman-tic Compositionality Over a Sentiment Treebank.
InProceedings of EMNLP, pages 1631?1642.Richard Socher, Andrej Karpathy, Quoc V. Le, Christo-pher D. Manning, and Andrew Y. Ng.
2014.Grounded compositional semantics for finding and de-scribing images with sentences.
TACL, 2:207?218.Fei Wu and Daniel S. Weld.
2010.
Open information ex-traction using wikipedia.
In Proceeding of ACL, pages118?127.1249
