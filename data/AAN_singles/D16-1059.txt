Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 616?626,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsRecursive Neural Conditional Random Fieldsfor Aspect-based Sentiment AnalysisWenya Wang??
Sinno Jialin Pan?
Daniel Dahlmeier?
Xiaokui Xiao?
?Nanyang Technological University, Singapore?SAP Innovation Center Singapore?
{wa0001ya, sinnopan, xkxiao}@ntu.edu.sg, ?
{d.dahlmeier}@sap.comAbstractIn aspect-based sentiment analysis, extract-ing aspect terms along with the opinions be-ing expressed from user-generated content isone of the most important subtasks.
Previ-ous studies have shown that exploiting con-nections between aspect and opinion terms ispromising for this task.
In this paper, we pro-pose a novel joint model that integrates recur-sive neural networks and conditional randomfields into a unified framework for explicit as-pect and opinion terms co-extraction.
Theproposed model learns high-level discrimina-tive features and double propagates informa-tion between aspect and opinion terms, simul-taneously.
Moreover, it is flexible to incor-porate hand-crafted features into the proposedmodel to further boost its information extrac-tion performance.
Experimental results on thedataset from SemEval Challenge 2014 task 4show the superiority of our proposed modelover several baseline methods as well as thewinning systems of the challenge.1 IntroductionAspect-based sentiment analysis (Pang and Lee,2008; Liu, 2011) aims to extract important infor-mation, e.g., opinion targets, opinion expressions,target categories, and opinion polarities, from user-generated content, such as microblogs, reviews, etc.This task was first studied by Hu and Liu (2004a;2004b), followed by Popescu and Etzioni (2005),Zhuang et al (2006), Zhang et al (2010), Qiu etal.
(2011), Li et al (2010).
In aspect-based senti-ment analysis, one of the goals is to extract explicitaspects of an entity from text, along with the opin-ions being expressed.
For example, in a restaurantreview ?I have to say they have one of the fastest de-livery times in the city.
?, the aspect term is deliverytimes, and the opinion term is fastest.Among previous work, one of the approachesis to accumulate aspect and opinion terms from aseed collection without label information, by utiliz-ing syntactic rules or modification relations betweenthem (Qiu et al, 2011; Liu et al, 2013b).
In theabove example, if we know fastest is an opinionword, then delivery times is probably inferred to bean aspect because fastest is its modifier.
However,this approach largely relies on hand-coded rules andis restricted to certain Part-of-Speech (POS) tags,e.g., opinion words are restricted to be adjectives.Another approach focuses on feature engineeringbased on predefined lexicons, syntactic analysis,etc.
(Jin and Ho, 2009; Li et al, 2010).
A sequencelabeling classifier is then built to extract aspect andopinion terms.
This approach requires extensive ef-forts for designing hand-crafted features and onlycombines features linearly for classification whichignores higher order interactions.To overcome the limitations of existing methods,we propose a novel model, named Recursive Neu-ral Conditional Random Fields (RNCRF).
Specif-ically, RNCRF consists of two main components.The first component is a recursive neural network(RNN)1 (Socher et al, 2010) based on a depen-dency tree of each sentence.
The goal is to learna high-level feature representation for each word inthe context of each sentence and make the represen-tation learning for aspect and opinion terms inter-active through the underlying dependency structureamong them.
The output of the RNN is then fed intoa Conditional Random Field (CRF) (Lafferty et al,2001) to learn a discriminative mapping from high-1Note that in this paper, RNN stands for recursive neuralnetwork instead of recurrent neural network.616level features to labels, i.e., aspects, opinions, orothers, so that context information can be well cap-tured.
Our main contributions are to use RNN forencoding aspect-opinion relations in high-level rep-resentation learning and to present a joint optimiza-tion approach based on maximum likelihood andbackpropagation to learn the RNN and CRF com-ponents simultaneously.
In this way, the label in-formation of aspect and opinion terms can be duallypropagated from parameter learning in CRF to rep-resentation learning in RNN.
We conduct expensiveexperiments on the dataset from SemEval challenge2014 task 4 (subtask 1) (Pontiki et al, 2014) to ver-ify the superiority of RNCRF over several baselinemethods as well as the winning systems of the chal-lenge.2 Related Work2.1 Aspects and Opinions Co-ExtractionHu et al (2004a) proposed to extract product aspectsthrough association mining, and opinion terms byaugmenting a seed opinion set using synonyms andantonyms in WordNet.
In follow-up work, syntacticrelations are further exploited for aspect/opinion ex-traction (Popescu and Etzioni, 2005; Wu et al, 2009;Qiu et al, 2011).
For example, Qiu et al (2011) usedsyntactic relations to double propagate and augmentthe sets of aspects and opinions.
Although the abovemodels are unsupervised, they heavily depend onpredefined rules for extraction and are restricted tospecific types of POS tags for product aspects andopinions.
Jin et al (2009), Li et al (2010), Jakobet al (2010) and Ma et al (2010) modeled the ex-traction problem as a sequence tagging problem andproposed to use HMMs or CRFs to solve it.
Thesemethods rely on rich hand-crafted features and donot consider interactions between aspect and opin-ion terms explicitly.
Another direction is to use wordalignment model to capture opinion relations amonga sentence (Liu et al, 2012; Liu et al, 2013a).
Thismethod requires sufficient data for modeling desiredrelations.Besides explicit aspects and opinions extraction,there are also other lines of research related toaspect-based sentiment analysis, including aspectclassification (Lakkaraju et al, 2014; McAuley etal., 2012), aspect rating (Titov and McDonald, 2008;Wang et al, 2011; Wang and Ester, 2014), domain-specific and target-dependent sentiment classifica-tion (Lu et al, 2011; Ofek et al, 2016; Dong et al,2014; Tang et al, 2015).2.2 Deep Learning for Sentiment AnalysisRecent studies have shown that deep learning mod-els can automatically learn the inherent semantic andsyntactic information from data and thus achievebetter performance for sentiment analysis (Socher etal., 2011b; Socher et al, 2012; Socher et al, 2013;Glorot et al, 2011; Kalchbrenner et al, 2014; Kim,2014; Le and Mikolov, 2014).
These methods gen-erally belong to sentence-level or phrase/word-levelsentiment polarity predictions.
Regarding aspect-based sentiment analysis, Irsoy et al (2014) ap-plied deep recurrent neural networks for opinion ex-pression extraction.
Dong et al (2014) proposedan adaptive recurrent neural network for target-dependent sentiment classification, where targets oraspects are given as input.
Tang et al (2015) usedLong Short-Term Memory (LSTM) (Hochreiter andSchmidhuber, 1997) for the same task.
Neverthe-less, there is little work in aspects and opinions co-extraction using deep learning models.To the best of our knowledge, the work of Liuet al (2015) and Yin et al (2016) are the most re-lated to ours.
Liu et al (2015) proposed to com-bine recurrent neural network and word embeddingsto extract explicit aspects.
However, the proposedmodel simply uses recurrent neural network on topof word embeddings, and thus its performance heav-ily depends on the quality of word embeddings.
Inaddition, it fails to explicitly model dependency re-lations or compositionalities within certain syntacticstructure in a sentence.
Recently, Yin et al (2016)proposed an unsupervised learning method to im-prove word embeddings using dependency path em-beddings.
A CRF is then trained with the embed-dings independently in the pipeline.Different from (Yin et al, 2016), our model doesnot focus on developing a new unsupervised wordembedding methods, but encodes the information ofdependency paths into RNN for constructing syntac-tically meaningful and discriminative hidden repre-sentations with labels.
Moreover, we integrate RNNand CRF into a unified framework and develop ajoint optimization approach, instead of training word617embeddings and a CRF separately as in (Yin et al,2016).
Note that Weiss et al (2015) proposed tocombine deep learning and structured learning forlanguage parsing which can be learned by structuredperceptron.
However, they also separate neural net-work training with structured prediction.2.3 Recursive Neural NetworksAmong deep learning methods, RNN has shownpromising results on various NLP tasks, such aslearning phrase representations (Socher et al, 2010),sentence-level sentiment analysis (Socher et al,2013), language parsing (Socher et al, 2011a), andquestion answering (Iyyer et al, 2014).
The treestructures used for RNNs include constituency treeand dependency tree.
In a constituency tree, all thewords lie at leaf nodes, each internal node repre-sents a phrase or a constituent of a sentence, and theroot node represents the entire sentence (Socher etal., 2010; Socher et al, 2012; Socher et al, 2013).In a dependency tree, each node including terminaland nonterminal nodes, represents a word, with de-pendency connections to other nodes (Socher et al,2014; Iyyer et al, 2014).
The resultant model isknown as dependency-tree RNN (DT-RNN).
An ad-vantage of using dependency tree over the other isthe ability to extract word-level representations con-sidering syntactic relations and semantic robustness.Therefore, we adopt DT-RNN in this work.3 Problem StatementSuppose that we are given a training set of cus-tomer reviews in a specific domain, denoted by S={s1, ..., sN}, where N is the number of review sen-tences.
For any si?S, there may exist a set of aspectterms Ai = {ai1, ..., ail}, where each aij ?
Ai canbe a single word or a sequence of words expressingexplicitly some aspect of an entity, and a set of opin-ion terms Oi = {oi1, ..., oim}, where each oir canbe a single word or a sequence of words expressingthe subjective sentiment of the comment holder.
Thetask is to learn a classifier to extract the set of aspectterms Ai and the set of opinion terms Oi from eachreview sentence si?S.This task can be formulated as a sequence tag-ging problem by using the BIO encoding scheme.Specifically, each review sentence si is composedof a sequence of words si = {wi1, ..., wini}.
Eachword wip ?
si is labeled as one out of the following5 classes: BA (beginning of aspect), IA (inside of as-pect), BO (beginning of opinion), IO (inside of opin-ion), and O (others).
Let L= {BA, IA,BO, IO,O}.We are also given a test set of review sentences de-noted by S?={s?1, ..., s?N ?
}, where N ?
is the numberof test reviews.
For each test review s?i?S?, our ob-jective is to predict the class label y?iq ?
L for eachword w?iq.
Note that a sequence of predictions withBA at the beginning followed by IA are indicationof one aspect, which is similar for opinion terms.24 Recursive Neural CRFsAs described in Section 1, RNCRF consists of twomain components: 1) a DT-RNN to learn a high-level representation for each word in a sentence, and2) a CRF to take the learned representation as inputto capture context around each word for explicit as-pect and opinion terms extraction.
Next, we presentthese two components in details.4.1 Dependency-Tree RNNsWe begin by associating each word w in our vo-cabulary with a feature vector x ?
Rd, which cor-responds to a column of a word embedding matrixWe ?
Rd?v, where v is the size of the vocabulary.For each sentence, we build a DT-RNN based on thecorresponding dependency parse tree with word em-beddings as initialization.
An example of the depen-dency parse tree is shown in Figure 1(a), where eachedge starts from the parent and points to its depen-dent with a syntactic relation.In a DT-RNN, each node n, including leaf nodes,internal nodes and the root node, in a specific sen-tence is associated with a word w, an input featurevector xw, and a hidden vector hn?Rd of the samedimension as xw.
Each dependency relation r is as-sociated with a separate matrix Wr?Rd?d.
In addi-tion, a common transformation matrixWv?Rd?d isintroduced to map the word embedding xw at noden to its corresponding hidden vector hn.2In this work we focus on extraction of aspect and opinionterms, not polarity predictions on opinion terms.
Polarity pre-diction can be done by either post-processing on the extractedopinion terms or redefining the BIO labels by encoding the po-larity information.618(a) Example of a dependency tree.
(b) Example of a DT-RNN tree structure.
(c) Example of a RNCRF structure.Figure 1: Examples of dependency tree, DT-RNN structure and RNCRF structure for a review sentence.Along with a particular dependency tree, a hiddenvector hn is computed from its own word embeddingxw at node n with the transformation matrixWv andits children?s hidden vectors hchild(n) with the cor-responding relation matrices {Wr}?s.
For instance,given the parse tree shown in Figure 1(a), we firstcompute the leaf nodes associated with the words Iand the using Wv as follows,hI = f(Wv ?
xI + b),hthe = f(Wv ?
xthe + b),where f is a nonlinear activation function and b is abias term.
In this paper, we adopt tanh(?)
as the ac-tivation function.
Once the hidden vectors of all theleaf nodes are generated, we can recursively gener-ate hidden vectors for interior nodes using the corre-sponding relation matrix Wr and the common trans-formation matrix Wv as follows,hfood = f(Wv ?
xfood +WDET ?
hthe + b),hlike = f(Wv ?
xlike +WDOBJ ?
hfood+WNSUBJ ?
hI + b).The resulting DT-RNN is shown in Figure 1(b).
Ingeneral, a hidden vector for any node n associatedwith a word vector xw can be computed as follows,hn = f?
?Wv ?
xw + b+?k?KnWrnk ?
hk??
, (1)where Kn denotes the set of children of node n, rnkdenotes the dependency relation between node n andits child node k, and hk is the hidden vector of thechild node k. The parameters of DT-RNN, ?RNN ={Wv,Wr,We, b}, are learned during training.4.2 Integration with CRFsCRFs are a discriminative graphical model for struc-tured prediction.
In RNCRF, we feed the outputof DT-RNN, i.e., the hidden representation of eachword in a sentence, to a CRF.
Updates of parametersfor RNCRF are carried out successively from thetop to bottom, by propagating errors through CRFto the hidden layers of RNN (including word em-beddings) using backpropagation through structure(BPTS) (Goller and Ku?chler, 1996).Formally, for each sentence si, we denote the in-put for CRF by hi, which is generated by DT-RNN.Here hi is a matrix with columns of hidden vec-tors {hi1, ..., hini} to represent a sequence of words{wi1, ..., wini} in a sentence si.
The model com-putes a structured output yi = {yi1, ..., yini} ?Y,where Y is a set of possible combinations of la-bels in label set L. The entire structure can berepresented by an undirected graph G = (V,E)with cliques c ?
C. In this paper, we employedlinear-chain CRF, which has two different cliques:unary cliques (U) representing input-output connec-tion, and pairwise cliques (P) representing adjacentoutput connections, as shown in Figure 1(c).
Dur-ing inference, the model aims to output y?
with themaximum conditional probability p(y|h).
(We dropthe subscript i here for simplicity.)
The probabilityis computed from potential outputs of the cliques:p(y|h) = 1Z(h)?c?C?c(h,yc), (2)where Z(h) is the normalization term, and?c(h,yc) is the potential of clique c, computed as?c(h,yc) = exp ?Wc, F (h,yc)?, where the RHS isthe exponential of a linear combination of featurevector F (h,yc) for clique c, and the weight vec-tor Wc is tied for unary and pairwise cliques.
We619Figure 2: An example for computing input-ouputpotential for the second position like.also incorporate a context window of size 2T + 1when computing unary potentials (T is a hyper-parameter).
Thus, the potential of unary clique atnode k can be written as?U (h, yk) = exp((W0)yk ?hk +T?t=1(W?t)yk ?hk?t+T?t=1(W+t)yk ?
hk+t), (3)where W0, W+t and W?t are weight matrices of theCRF for the current position, the t-th position to theright, and the t-th position to the left within contextwindow, respectively.
The subscript yk indicates thecorresponding row in the weight matrix.For instance, Figure 2 shows an example of win-dow size 3.
At the second position, the input featuresfor like are composed of the hidden vectors at posi-tion 1 (hI), position 2 (hlike) and position 3 (hthe).Therefore, the conditional distribution for the entiresequence y in Figure 1(c) can be calculated asp(y|h)= 1Z(h) exp( 4?k=1(W0)yk ?hk+4?k=2(W?1)yk ?hk?1+3?k=1(W+1)yk ?hk+1+3?k=1Vyk,yk+1),where the first three terms in the exponential of theRHS consider unary clique while the last term con-siders the pairwise clique with matrix V represent-ing pairwise state transition score.
For simplicityin description on parameter updates, we denote thelog-potential for clique c ?
{U,P} by gc(h,yc) =?Wc, F (h,yc)?.4.3 Joint Training for RNCRFThrough the objective of maximum likelihood, up-dates for parameters of RNCRF are first conductedon the parameters of the CRF (unary weight matri-ces ?U = {W0,W+t,W?t} and pairwise weightmatrix V ) by applying chain rule to log-potentialupdates.
Below is the gradient for ?U (updates forV are similar through the log-potential of pairwiseclique gP (y?k, y?k+1)):4?U =?
?
log p(y|h)?gU (h, y?k)?
?gU (h, y?k)?
?U, (4)where?
?
log p(y|h)?gU (h, y?k)= ?
(1yk=y?k ?
p(y?k|h)), (5)and y?k represents possible label configuration ofnode k. The hidden representations of each wordand the parameters of DT-RNN are updated sub-sequently by applying chain rule with (5) throughBPTS as follows,4hroot = ?
?
log p(y|h)?gU (h, y?root)?
?gU (h, y?root)?hroot , (6)4hk 6=root = ?
?
log p(y|h)?gU (h, y?k)?
?gU (h, y?k)?hk+4hpar(k) ?
?hpar(k)?hk, (7)4?RNN =K?k=1?
?
log p(y|h)?hk?
?hk?
?RNN, (8)where hroot represents the hidden vector of the wordpointed by ROOT in the corresponding DT-RNN.Since this word is the topmost node in the tree, itonly inherits error from the CRF output.
In (7),hpar(k) denotes the hidden vector of the parent nodeof node k in DT-RNN.
Hence the lower nodes re-ceive error from both the CRF output and error prop-agation from parent node.
The parameters withinDT-RNN, ?RNN, are updated by applying chainrule with respect to updates of hidden vectors, andaggregating among all associated nodes, as shownin (8).
The overall procedure of RNCRF is summa-rized in Algorithm 1.5 DiscussionThe best performing system (Toh and Wang, 2014)for SemEval challenge 2014 task 4 (subtask 1) em-ployed CRFs with extensive hand-crafted featuresincluding those induced from dependency trees.However, their experiments showed that the additionof the features induced from dependency relationsdoes not improve the performance.
This indicates620Algorithm 1 Recursive Neural CRFsInput: A set of customer review sequences: S ={s1, ..., sN}, and feature vectors of d dimensions for eachword {xw}?s, window size T for CRFsOutput: Parameters: ?={?RNN,?U , V}Initialization: Initialize We using word2vec.
Initialize Wvand {Wr}?s randomly with uniform distribution between[?
?6?2d+1 ,?6?2d+1].
Initialize W0, {W+t}?s, {W?t}?s, V ,and b with all 0?sfor each sentence si do1: Use DT-RNN (1) to generate hi2: Compute p(yi|hi) using (2)3: Use the backpropagation algorithm to update parame-ters ?
through (4)-(8)end forthe difficulty of incorporating dependency structureexplicitly as input features, which motivates the de-sign of our model to use DT-RNN to encode depen-dency between words for feature learning.
The mostimportant advantage of RNCRF is the ability to learnthe underlying dual propagation between aspect andopinion terms from the tree structure itself.
Specif-ically as shown in Figure 1(c), where the aspect isfood and the opinion expression is like.
In the de-pendency tree, food depends on like with the relationDOBJ.
During training, RNCRF computes the hid-den vector hlike for like, which is also obtained fromhfood.
As a result, the prediction for like is affectedby hfood.
This is one-way propagation from foodto like.
During backpropagation, the error for like ispropagated through a top-down manner to revise therepresentation hfood.
This is the other-way propa-gation from like to food.
Therefore, the dependencystructure together with the learning approach help toenforce the dual propagation of aspect-opinion pairsas long as the dependency relation exists, either di-rectly or indirectly.5.1 Adding Linguistic/Lexicon FeaturesRNCRF is an end-to-end model, where feature engi-neering is not necessary.
However, it is flexible to in-corporate light hand-crafted features into RNCRF tofurther boost its performance, such as features fromPOS tags, name-list, or sentiment lexicon.
Thesefeatures could be appended to the hidden vector ofeach word, but keep fixed during training, unlikelearnable neural inputs and the CRF weights as de-scribed in Section 4.3.
As will be shown in exper-Domain Training Test TotalRestaurant 3,041 800 3,841Laptop 3,045 800 3,845Total 6,086 1,600 7,686Table 1: SemEval Challenge 2014 task 4 datasetiments, RNCRF without any hand-crafted featuresslightly outperforms the best performing systemsthat involve heavy feature engineering efforts, andRNCRF with light feature engineering can achieveeven better performance.6 Experiment6.1 Dataset and Experimental SetupWe evaluate our model on the dataset from SemEvalChallenge 2014 task 4 (subtask 1), which includesreviews from two domains: restaurant and laptop3.The detailed description of the dataset is given inTable 1.
As the original dataset only includes manu-ally annotate labels for aspect terms but not for opin-ion terms, we manually annotated opinion terms foreach sentence by ourselves to facilitate our experi-ments.For word vector initialization, we train word em-beddings with word2vec (Mikolov et al, 2013) onthe Yelp Challenge dataset4 for the restaurant do-main and on the Amazon reviews dataset5 (McAuleyet al, 2015) for the laptop domain.
The Yelp datasetcontains 2.2M restaurant reviews with 54K vocab-ulary size.
For the Amazon reviews, we only ex-tracted the electronic domain that contains 1M re-views with 590K vocabulary size.
We vary differ-ent dimensions for word embeddings and chose 300for both domains.
Empirical sensitivity studies ondifferent dimensions of word embeddings are alsoconducted.
Dependency trees are generated usingStanford Dependency Parser (Klein and Manning,2003).
Regarding CRFs, we implement a linear-chain CRF using CRFSuite (Okazaki, 2007).
Be-cause of the relatively small size of training dataand a large number of parameters, we perform pre-training on the parameters of DT-RNN with cross-3Experiments with more publicly available datasets, e.g.restaurant review dataset from SemEval Challenge 2015 task12 will be conducted in our future work.4http://www.yelp.com/dataset challenge5http://jmcauley.ucsd.edu/data/amazon/links.html621entropy error, which is a common strategy for deeplearning (Erhan et al, 2009).
We implement mini-batch stochastic gradient descent (SGD) with a batchsize of 25, and an adaptive learning rate (AdaGrad)initialized at 0.02 for pretraining of DT-RNN, whichruns 4 epochs for the restaurant domain and 5 epochsfor the laptop domain.
For parameter learning of thejoint model RNCRF, we implement SGD with a de-caying learning rate initialized at 0.02.
We also trywith varying context window size, and use 3 for thelaptop domain and 5 for the restaurant domain, re-spectively.
All parameters are chosen by cross vali-dation.As discussed in Section 5.1, hand-crafted featurescan be easily incorporated into RNCRF.
We gen-erate three types of simple features based on POStags, name-list and sentiment lexicon to show fur-ther improvement by incorporating these features.Following (Toh and Wang, 2014), we extract twosets of name list from the training data for eachdomain, where one includes high-frequency aspectterms, and the other includes high-probability as-pect words.
These two sets are used to construct twolexicon features, i.e.
we build a 2D binary vector:if a word is in a set, the corresponding value is 1,otherwise 0.
For POS tags, we use Stanford POStagger (Toutanova et al, 2003), and convert themto universal POS tags that have 15 different cate-gories.
We then generate 15 one-hot POS tag fea-tures.
For sentiment lexicon, we use the collection ofcommonly used opinion words (around 6,800) (Huand Liu, 2004a).
Similar to name list, we create a bi-nary feature to indicate whether the word belongs toopinion lexicon.
We denote by RNCRF+F the pro-posed model with the three types of features.Compared to the winning systems of SemEvalChallenge 2014 task 4 (subtask 1), RNCRF or RN-CRF+F uses additional labels of opinion terms fortraining.
Therefore, to conduct fair comparison ex-periments with the winning systems, we implementRNCRF-O by omitting opinion labels to train ourmodel (i.e., labels become ?BA?, ?IA?, ?O?).
Ac-cordingly, we denote by RNCRF-O+F the RNCRF-O model with the three additional types of hand-crafted features.6.2 Experimental ResultsWe compare our model with several baselines:?
CRF-1: a linear-chain CRF with standard lin-guistic features including word string, stylis-tics, POS tag, context string, and context POStags.?
CRF-2: a linear-chain CRF with both stan-dard linguistic features and dependency infor-mation including head word, dependency rela-tions with parent token and child tokens.?
LSTM: an LSTM network built on top of wordembeddings proposed by (Liu et al, 2015).
Wekeep original settings in (Liu et al, 2015) butreplace their word embeddings with ours (300dimension).
We try different hidden layer di-mensions (50, 100, 150, 200) and reported thebest result with size 50.?
LSTM+F: the above LSTM model with thethree additional types of hand-crafted featuresas with RNCRF.?
SemEval-1, SemEval-2: the top two winningsystems for SemEval challenge 2014 task 4(subtask 1).?
WDEmb+B+CRF6: the model proposedby (Yin et al, 2016) using word and de-pendency path embeddings combined withlinear context embedding features, dependencycontext embedding features and hand-craftedfeatures (i.e., feature engineering) as CRFinput.The comparison results are shown in Table 2 for boththe restaurant domain and the laptop domain.
Notethat we provide the same annotated dataset (both as-pect labels and opinion labels are included for train-ing) for CRF-1, CRF-2 and LSTM for fair compar-ison.
It is clear that our proposed model RNCRFachieves superior performance compared with mostof the baseline models.
The performance is even bet-ter by adding simple hand-crafted features, i.e., RN-CRF+F, with 0.92% and 3.87% absolute improve-ment over the best system in the challenge for aspectextraction for the restaurant domain and the laptopdomain, respectively.
This shows the advantage of6We report the best results from the original paper (Yin etal., 2016).622Restaurant LaptopModels Aspect Opinion Aspect OpinionSemEval-1 84.01 - 74.55 -SemEval-2 83.98 - 73.78 -WDEmb+B+CRF 84.97 - 75.16 -CRF-1 77.00 78.95 66.21 71.78CRF-2 78.37 78.65 68.35 70.05LSTM 81.15 80.22 72.73 74.98LSTM+F 82.99 82.90 73.23 77.67RNCRF-O 82.73 - 74.52 -RNCRF-O+F 84.25 - 77.26 -RNCRF 84.05 80.93 76.83 76.76RNCRF+F 84.93 84.11 78.42 79.44Table 2: Comparison results in terms of F1 scores.combining high-level continuous features and dis-crete hand-crafted features.
Though CRFs usuallyshow promising results in sequence tagging prob-lems, they fail to achieve comparable performancewhen lacking of extensive features (e.g., CRF-1).
Byadding dependency information explicitly in CRF-2, the result only improves slightly for aspect ex-traction.
Alternatively, by incorporating dependencyinformation into a deep learning model (e.g., RN-CRF), the result shows more than 7% improvementfor aspect extraction and 2% for opinion extraction.By removing the labels for opinion terms,RNCRF-O produces inferior results than RNCRFbecause the effect of dual propagation of aspect andopinion pairs disappears with the absence of opinionlabels.
This verifies our previous assumption thatDT-RNN could learn the interactive effects withinaspects and opinions.
However, the performance ofRNCRF-O is still comparable to the top systems andeven better with the addition of simple linguistic fea-tures: 0.24% and 2.71% superior than the best sys-tem in the challenge for the restaurant domain andthe laptop domain, respectively.
This shows the ro-bustness of our model even without additional opin-ion labels.LSTM has shown comparable results for aspectextraction (Liu et al, 2015).
However, in theirwork, they used well-pretrained word embeddingsby training with large corpus or extensive externalresources, e.g., chunking, and NER.
To comparetheir model with RNCRF, we re-implement LSTMwith the same word embedding strategy and label-ing resources as ours.
The results show that ourRestaurant LaptopModels Aspect Opinion Aspect OpinionDT-RNN+SoftMax 72.45 69.76 66.11 64.66CRF+word2vec 82.57 78.83 63.62 56.96RNCRF 84.05 80.93 76.83 76.76RNCRF+POS 84.08 81.48 77.04 77.45RNCRF+NL 84.24 81.22 78.12 77.20RNCRF+Lex 84.21 84.14 77.15 78.56RNCRF+F 84.93 84.11 78.42 79.44Table 3: Impact of different components.model outperforms LSTM in aspect extraction by2.90% and 4.10% for the restaurant domain andthe laptop domain, respectively.
We conclude thata standard LSTM model fails to extract the rela-tions between aspect and opinion terms.
Even withthe addition of same linguistic features, LSTM isstill inferior than RNCRF itself in terms of as-pect extraction.
Moreover, our result is compara-ble with WDEmb+B+CRF in the restaurant domainand better in the laptop domain (+3.26%).
Note thatWDEmb+B+CRF appended dependency context in-formation into CRF while our model encode suchinformation into high-level representation learning.To test the impact of each component of RNCRFand the three types of hand-crafted features, we con-duct experiments on different model settings:?
DT-RNN+SoftMax: rather than using a CRF,a softmax classifier is used on top of DT-RNN.?
CRF+word2vec: a linear-chain CRF withword embeddings only without using DT-RNN.?
RNCRF+POS/NL/Lex: the RNCRF modelwith POS tag or name list or sentiment lexiconfeature(s).The comparison results are shown in Table 3.
Sim-ilarly, both aspect and opinion term labels are pro-vided for training for each of the above mod-els.
Firstly, RNCRF achieves much better re-sults compared to DT-RNN+SoftMax (+11.60% and+10.72% for the restaurant domain and the lap-top domain in aspect extraction).
This is becauseDT-RNN fails to fully exploit context informationfor sequence labeling, which, in contrast, can beachieved by CRF.
Secondly, RNCRF outperformsCRF+word2vec, which proves the importance of62325 50 75 100 125 150 175 200 225 250 275 300 325 350 375 400dimension0.720.740.760.780.800.820.840.860.88f1-scoreaspectopinion(a) On the restaurant domain.25 50 75 100 125 150 175 200 225 250 275 300 325 350 375 400dimension0.550.600.650.700.750.800.85f1-scoreaspectopinion(b) On the laptop domain.Figure 3: Sensitivity studies on word embeddings.DT-RNN for modeling interactions between aspectsand opinions.
Hence, the combination of DT-RNNand CRF inherits the advantages from both mod-els.
Moreover, by separately adding hand-craftedfeatures, we can observe that name-list-based fea-tures and the sentiment lexicon feature are most ef-fective for aspect extraction and opinion extraction,respectively.
This may be explained by the fact thatname-list-based features usually contain informativeevident for aspect terms and sentiment lexicon pro-vides explicit indication about opinions.Besides the comparison experiments, we alsoconduct sensitivity test for our proposed model interms of word vector dimensions.
We tested a set ofdifferent dimensions ranging from 25 to 400, with25 increment.
The sensitivity plot is shown in Fig-ure 3.
The performance for aspect extraction issmooth with different vector lengths for both do-mains.
For restaurant domain, the result is stablewhen dimension is larger than or equal to 100, withthe highest at 325.
For the laptop domain, the bestresult is at dimension 300, but with relatively smallvariations.
For opinion extraction, the performancereaches a good level when the dimension is largerthan or equal to 75 for the restaurant domain and125 for the laptop domain.
This proves the stabilityand robustness of our model.7 ConclusionWe have presented a joint model, RNCRF, thatachieves the state-of-the-art performance for explicitaspect and opinion term extraction on a benchmarkdataset.
With the help of DT-RNN, high-level fea-tures can be learned by encoding the underlying dualpropagation of aspect-opinion pairs.
RNCRF com-bines the advantages of DT-RNNs and CRFs, andthus outperforms the traditional rule-based meth-ods in terms of flexibility, because aspect terms andopinion terms are not only restricted to certain ob-served relations and POS tags.
Compared to fea-ture engineering methods with CRFs, the proposedmodel saves much effort in composing features, andit is able to extract higher-level features obtainedfrom non-linear transformations.AcknowledgementsThis research is partially funded by the EconomicDevelopment Board and the National ResearchFoundation of Singapore.
Sinno J. Pan thanks thesupport from Fuji Xerox Corporation through jointresearch on Multilingual Semantic Analysis and theNTU Singapore Nanyang Assistant Professorship(NAP) grant M4081532.020.ReferencesLi Dong, Furu Wei, Chuanqi Tan, Duyu Tang, MingZhou, and Ke Xu.
2014.
Adaptive recursive neuralnetwork for target-dependent twitter sentiment classi-fication.
In ACL, pages 49?54.Dumitru Erhan, Pierre-Antoine Manzagol, Yoshua Ben-gio, Samy Bengio, and Pascal Vincent.
2009.
Thedifficulty of training deep architectures and the effectof unsupervised pre-training.
In AISTATS, pages 153?160.Xavier Glorot, Antoine Bordes, and Yoshua Bengio.2011.
Domain adaptation for large-scale sentimentclassification: A deep learning approach.
In ICML,pages 97?110.624Christoph Goller and Andreas Ku?chler.
1996.
Learningtask-dependent distributed representations by back-propagation through structure.
In ICNN, pages 347?352.Sepp Hochreiter and Ju?rgen Schmidhuber.
1997.
Longshort-term memory.
Neural Computation, 9(8):1735?1780.Minqing Hu and Bing Liu.
2004a.
Mining and summa-rizing customer reviews.
In KDD, pages 168?177.Minqing Hu and Bing Liu.
2004b.
Mining opinion fea-tures in customer reviews.
In AAAI, pages 755?760.Ozan I?rsoy and Claire Cardie.
2014.
Opinion min-ing with deep recurrent neural networks.
In EMNLP,pages 720?728.Mohit Iyyer, Jordan L. Boyd-Graber, LeonardoMax Batista Claudino, Richard Socher, andHal Daume?
III.
2014.
A neural network forfactoid question answering over paragraphs.
InEMNLP, pages 633?644.Niklas Jakob and Iryna Gurevych.
2010.
Extractingopinion targets in a single- and cross-domain settingwith conditional random fields.
In EMNLP, pages1035?1045.Wei Jin and Hung Hay Ho.
2009.
A novel lexicalizedhmm-based learning framework for web opinion min-ing.
In ICML, pages 465?472.Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-som.
2014.
A convolutional neural network for mod-elling sentences.
In ACL, pages 655?665.Yoon Kim.
2014.
Convolutional neural networks for sen-tence classification.
In EMNLP, pages 1746?1751.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In ACL, pages 423?430.John D. Lafferty, Andrew McCallum, and Fernando C. N.Pereira.
2001.
Conditional random fields: Probabilis-tic models for segmenting and labeling sequence data.In ICML, pages 282?289.Himabindu Lakkaraju, Richard Socher, and Christo-pher D. Manning.
2014.
Aspect specific senti-ment analysis using hierarchical deep learning.
InNIPS Workshop on Deep Learning and RepresentationLearning.Quoc V. Le and Tomas Mikolov.
2014.
Distributed rep-resentations of sentences and documents.
In ICML,pages 1188?1196.Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu,Ying-Ju Xia, Shu Zhang, and Hao Yu.
2010.Structure-aware review mining and summarization.
InCOLING, pages 653?661.Kang Liu, Liheng Xu, and Jun Zhao.
2012.
Opiniontarget extraction using word-based translation model.In EMNLP-CoNLL, pages 1346?1356.Kang Liu, Liheng Xu, Yang Liu, and Jun Zhao.
2013a.Opinion target extraction using partially-supervisedword alignment model.
In IJCAI, pages 2134?2140.Qian Liu, Zhiqiang Gao, Bing Liu, and Yuanlin Zhang.2013b.
A logic programming approach to aspect ex-traction in opinion mining.
In WI, pages 276?283.Pengfei Liu, Shafiq Joty, and Helen Meng.
2015.
Fine-grained opinion mining with recurrent neural networksand word embeddings.
In EMNLP, pages 1433?1443.Bing Liu.
2011.
Web Data Mining: Exploring Hy-perlinks, Contents, and Usage Data.
Second Edition.Data-Centric Systems and Applications.
Springer.Yue Lu, Malu Castellanos, Umeshwar Dayal, andChengXiang Zhai.
2011.
Automatic construction of acontext-aware sentiment lexicon: An optimization ap-proach.
In WWW, pages 347?356.Tengfei Ma and Xiaojun Wan.
2010.
Opinion targetextraction in Chinese news comments.
In COLING,pages 782?790.Julian McAuley, Jure Leskovec, and Dan Jurafsky.
2012.Learning attitudes and attributes from multi-aspect re-views.
In ICDM, pages 1020?1025.Julian McAuley, Christopher Targett, Qinfeng Shi, andAnton van den Hengel.
2015.
Image-based recom-mendations on styles and substitutes.
In SIGIR, pages43?52.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word represen-tations in vector space.
CoRR, abs/1301.3781.Nir Ofek, Soujanya Poria, Lior Rokach, Erik Cam-bria, Amir Hussain, and Asaf Shabtai.
2016.
Un-supervised commonsense knowledge enrichment fordomain-specific sentiment analysis.
Cognitive Com-putation, 8(3):467?477.Naoaki Okazaki.
2007.
CRFsuite: a fast im-plementation of conditional random fields (CRFs).http://www.chokkan.org/software/crfsuite/.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and Trends in Infor-mation Retrieval, 2(1-2).Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Har-ris Papageorgiou, Ion Androutsopoulos, and SureshManandhar.
2014.
Semeval-2014 task 4: Aspectbased sentiment analysis.
In SemEval, pages 27?35.Ana-Maria Popescu and Oren Etzioni.
2005.
Extract-ing product features and opinions from reviews.
InEMNLP, pages 339?346.Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.2011.
Opinion word expansion and target extractionthrough double propagation.
Computational Linguis-tics, 37(1):9?27.625Richard Socher, Christopher D. Manning, and Andrew Y.Ng.
2010.
Learning Continuous Phrase Representa-tions and Syntactic Parsing with Recursive Neural Net-works.
In NIPS, pages 1?9.Richard Socher, Cliff C. Lin, Andrew Y. Ng, and Christo-pher D. Manning.
2011a.
Parsing natural scenes andnatural language with recursive neural networks.
InICML, pages 129?136.Richard Socher, Jeffrey Pennington, Eric H. Huang, An-drew Y. Ng, and Christopher D. Manning.
2011b.Semi-Supervised Recursive Autoencoders for Predict-ing Sentiment Distributions.
In EMNLP, pages 151?161.Richard Socher, Brody Huval, Christopher D. Manning,and Andrew Y. Ng.
2012.
Semantic CompositionalityThrough Recursive Matrix-Vector Spaces.
In EMNLP,pages 1201?1211.Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,Christopher D. Manning, Andrew Y. Ng, and Christo-pher Potts.
2013.
Recursive deep models for se-mantic compositionality over a sentiment treebank.
InEMNLP, pages 1631?1642.Richard Socher, Andrej Karpathy, Quoc V. Le, Christo-pher D. Manning, and Andrew Y. Ng.
2014.Grounded compositional semantics for finding and de-scribing images with sentences.
TACL, 2:207?218.Duyu Tang, Bing Qin, Xiaocheng Feng, and Ting Liu.2015.
Target-dependent sentiment classification withlong short term memory.
CoRR, abs/1512.01100.Ivan Titov and Ryan T. McDonald.
2008.
A joint modelof text and aspect ratings for sentiment summarization.In ACL, pages 308?316.Zhiqiang Toh and Wenting Wang.
2014.
DLIREC: As-pect term extraction and term polarity classificationsystem.
In SemEval, pages 235?240.Kristina Toutanova, Dan Klein, Christopher D. Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In NAACL,pages 173?180.Hao Wang and Martin Ester.
2014.
A sentiment-alignedtopic model for product aspect rating prediction.
InEMNLP, pages 1192?1202.Hongning Wang, Yue Lu, and ChengXiang Zhai.
2011.Latent aspect rating analysis without aspect keywordsupervision.
In KDD, pages 618?626.David Weiss, Chris Alberti, Michael Collins, and SlavPetrov.
2015.
Structured training for neural networktransition-based parsing.
In ACL-IJCNLP, pages 323?333.Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.2009.
Phrase dependency parsing for opinion mining.In EMNLP, pages 1533?1541.Yichun Yin, Furu Wei, Li Dong, Kaimeng Xu, MingZhang, and Ming Zhou.
2016.
Unsupervised wordand dependency path embeddings for aspect term ex-traction.
In IJCAI, pages 2979?2985.Lei Zhang, Bing Liu, Suk Hwan Lim, and EamonnO?Brien-Strain.
2010.
Extracting and ranking prod-uct features in opinion documents.
In COLING, pages1462?1470.Li Zhuang, Feng Jing, and Xiao-Yan Zhu.
2006.
Moviereview mining and summarization.
In CIKM, pages43?50.626
