Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 176?181,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsBetter Hypothesis Testing for Statistical Machine Translation:Controlling for Optimizer InstabilityJonathan H. Clark Chris Dyer Alon Lavie Noah A. SmithLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USA{jhclark,cdyer,alavie,nasmith}@cs.cmu.eduAbstractIn statistical machine translation, a researcherseeks to determine whether some innovation(e.g., a new feature, model, or inference al-gorithm) improves translation quality in com-parison to a baseline system.
To answer thisquestion, he runs an experiment to evaluate thebehavior of the two systems on held-out data.In this paper, we consider how to make suchexperiments more statistically reliable.
Weprovide a systematic analysis of the effects ofoptimizer instability?an extraneous variablethat is seldom controlled for?on experimen-tal outcomes, and make recommendations forreporting results more accurately.1 IntroductionThe need for statistical hypothesis testing for ma-chine translation (MT) has been acknowledged sinceat least Och (2003).
In that work, the proposedmethod was based on bootstrap resampling and wasdesigned to improve the statistical reliability of re-sults by controlling for randomness across test sets.However, there is no consistently used strategy thatcontrols for the effects of unstable estimates ofmodel parameters.1 While the existence of opti-mizer instability is an acknowledged problem, it isonly infrequently discussed in relation to the relia-bility of experimental results, and, to our knowledge,there has yet to be a systematic study of its effects on1We hypothesize that the convention of ?trusting?
BLEUscore improvements of, e.g., > 1, is not merely due to an ap-preciation of what qualitative difference a particular quantita-tive improvement will have, but also an implicit awareness thatcurrent methodology leads to results that are not consistentlyreproducible.hypothesis testing.
In this paper, we present a seriesof experiments demonstrating that optimizer insta-bility can account for substantial amount of variationin translation quality,2 which, if not controlled for,could lead to incorrect conclusions.
We then showthat it is possible to control for this variable with ahigh degree of confidence with only a few replica-tions of the experiment and conclude by suggestingnew best practices for significance testing for ma-chine translation.2 Nondeterminism and OtherOptimization PitfallsStatistical machine translation systems consist of amodel whose parameters are estimated to maximizesome objective function on a set of developmentdata.
Because the standard objectives (e.g., 1-bestBLEU, expected BLEU, marginal likelihood) arenot convex, only approximate solutions to the op-timization problem are available, and the parame-ters learned are typically only locally optimal andmay strongly depend on parameter initialization andsearch hyperparameters.
Additionally, stochasticoptimization and search techniques, such as mini-mum error rate training (Och, 2003) and Markovchain Monte Carlo methods (Arun et al, 2010),3constitute a second, more obvious source of noisein the optimization procedure.This variation in the parameter vector affects thequality of the model measured on both development2This variation directly affects the output translations, andso it will propagate to both automated metrics as well as humanevaluators.3Online subgradient techniques such as MIRA (Crammer etal., 2006; Chiang et al, 2008) have an implicit stochastic com-ponent as well based on the order of the training examples.176data and held-out test data, independently of any ex-perimental manipulation.
Thus, when trying to de-termine whether the difference between two mea-surements is significant, it is necessary to control forvariance due to noisy parameter estimates.
This canbe done by replication of the optimization procedurewith different starting conditions (e.g., by runningMERT many times).Unfortunately, common practice in reporting ma-chine translation results is to run the optimizer onceper system configuration and to draw conclusionsabout the experimental manipulation from this sin-gle sample.
However, it could be that a particu-lar sample is on the ?low?
side of the distributionover optimizer outcomes (i.e., it results in relativelypoorer scores on the test set) or on the ?high?
side.The danger here is obvious: a high baseline resultpaired with a low experimental result could lead to auseful experimental manipulation being incorrectlyidentified as useless.
We now turn to the question ofhow to reduce the probability falling into this trap.3 Related WorkThe use of statistical hypothesis testing has grownapace with the adoption of empirical methods innatural language processing.
Bootstrap techniques(Efron, 1979; Wasserman, 2003) are widespreadin many problem areas, including for confidenceestimation in speech recognition (Bisani and Ney,2004), and to determine the significance of MT re-sults (Och, 2003; Koehn, 2004; Zhang et al, 2004;Zhang and Vogel, 2010).
Approximate randomiza-tion (AR) has been proposed as a more reliable tech-nique for MT significance testing, and evidence sug-gests that it yields fewer type I errors (i.e., claiminga significant difference where none exists; Riezlerand Maxwell, 2005).
Other uses in NLP includethe MUC-6 evaluation (Chinchor, 1993) and pars-ing (Cahill et al, 2008).
However, these previousmethods assume model parameters are elements ofthe system rather than extraneous variables.Prior work on optimizer noise in MT has fo-cused primarily on reducing optimizer instability(whereas our concern is how to deal with optimizernoise, when it exists).
Foster and Kuhn (2009) mea-sured the instability of held-out BLEU scores across10 MERT runs to improve tune/test set correlation.However, they only briefly mention the implicationsof the instability on significance.
Cer et al (2008)explored regularization of MERT to improve gener-alization on test sets.
Moore and Quirk (2008) ex-plored strategies for selecting better random ?restartpoints?
in optimization.
Cer et al (2010) analyzedthe standard deviation over 5 MERT runs when eachof several metrics was used as the objective function.4 ExperimentsIn our experiments, we ran the MERT optimizer tooptimize BLEU on a held-out development set manytimes to obtain a set of optimizer samples on two dif-ferent pairs of systems (4 configurations total).
Eachpair consists of a baseline system (System A) and an?experimental?
system (System B), which previousresearch has suggested will perform better.The first system pair contrasts a baseline phrase-based system (Moses) and experimental hierarchi-cal phrase-based system (Hiero), which were con-structed from the Chinese-English BTEC corpus(0.7M words), the later of which was decoded withthe cdec decoder (Koehn et al, 2007; Chiang, 2007;Dyer et al, 2010).
The second system pair con-trasts two German-English Hiero/cdec systems con-structed from the WMT11 parallel training data(98M words).4 The baseline system was trained onunsegmented words, and the experimental systemwas constructed using the most probable segmenta-tion of the German text according to the CRF wordsegmentation model of Dyer (2009).
The Chinese-English systems were optimized 300 times, and theGerman-English systems were optimized 50 times.Our experiments used the default implementationof MERT that accompanies each of the two de-coders.
The Moses MERT implementation uses 20random restart points per iteration, drawn uniformlyfrom the default ranges for each feature, and, at eachiteration, 200-best lists were extracted with the cur-rent weight vector (Bertoldi et al, 2009).
The cdecMERT implementation performs inference over thedecoder search space which is structured as a hyper-graph (Kumar et al, 2009).
Rather than using restartpoints, in addition to optimizing each feature inde-pendently, it optimizes in 5 random directions per it-eration by constructing a search vector by uniformlysampling each element of the vector from (?1, 1)and then renormalizing so it has length 1.
For allsystems, the initial weight vector was manually ini-tialized so as to yield reasonable translations.4http://statmt.org/wmt11/177Metric System Avg ssel sdev stestBTEC Chinese-English (n = 300)BLEU ?System A 48.4 1.6 0.2 0.5System B 49.9 1.5 0.1 0.4MET ?System A 63.3 0.9 - 0.4System B 63.8 0.9 - 0.5TER ?System A 30.2 1.1 - 0.6System B 28.7 1.0 - 0.2WMT German-English (n = 50)BLEU ?System A 18.5 0.3 0.0 0.1System B 18.7 0.3 0.0 0.2MET ?System A 49.0 0.2 - 0.2System B 50.0 0.2 - 0.1TER ?System A 65.5 0.4 - 0.3System B 64.9 0.4 - 0.4Table 1: Measured standard deviations of different au-tomatic metrics due to test-set and optimizer variability.sdev is reported only for the tuning objective functionBLEU.Results are reported using BLEU (Papineni etal., 2002), METEOR5 (Banerjee and Lavie, 2005;Denkowski and Lavie, 2010), and TER (Snover etal., 2006).4.1 Extraneous variables in one systemIn this section, we describe and measure (on the ex-ample systems just described) three extraneous vari-ables that should be considered when evaluating atranslation system.
We quantify these variables interms of standard deviation s, since it is expressedin the same units as the original metric.
Refer toTable 1 for the statistics.Local optima effects sdev The first extraneousvariable we discuss is the stochasticity of the opti-mizer.
As discussed above, different optimizationruns find different local maxima.
The noise due tothis variable can depend on many number of fac-tors, including the number of random restarts used(in MERT), the number of features in a model, thenumber of references, the language pair, the portionof the search space visible to the optimizer (e.g.
10-best, 100-best, a lattice, a hypergraph), and the sizeof the tuning set.
Unfortunately, there is no proxy toestimate this effect as with bootstrap resampling.
Tocontrol for this variable, we must run the optimizermultiple times to estimate the spread it induces onthe development set.
Using the n optimizer samples,with mi as the translation quality measurement of5METEOR version 1.2 with English ranking parameters andall modules.the development set for the ith optimization run, andm is the average of all mis, we report the standarddeviation over the tuning set as sdev:sdev =???
?n?i=1(mi ?m)2n?
1A high sdev value may indicate that the optimizer isstruggling with local optima and changing hyperpa-rameters (e.g.
more random restarts in MERT) couldimprove system performance.Overfitting effects stest As with any optimizer,there is a danger that the optimal weights for a tuningset may not generalize well to unseen data (i.e., weoverfit).
For a randomized optimizer, this means thatparameters can generalize to different degrees overmultiple optimizer runs.
We measure the spread in-duced by optimizer randomness on the test set met-ric score stest, as opposed to the overfitting effect inisolation.
The computation of stest is identical to sdevexcept that the mis are the translation metrics cal-culated on the test set.
In Table 1, we observe thatstest > sdev, indicating that optimized parameters arelikely not generalizing well.Test set selection ssel The final extraneous vari-able we consider is the selection of the test set it-self.
A good test set should be representative ofthe domain or language for which experimental ev-idence is being considered.
However, with only asingle test corpus, we may have unreliable resultsbecause of idiosyncrasies in the test set.
This canbe mitigated in two ways.
First, replication of ex-periments by testing on multiple, non-overlappingtest sets can eliminate it directly.
Since this is notalways practical (more test data may not be avail-abile), the widely-used bootstrap resampling method(?3) also controls for test set effects by resamplingmultiple ?virtual?
test sets from a single set, makingit possible to infer distributional parameters such asthe standard deviation of the translation metric over(very similar) test sets.6 Furthermore, this can bedone for each of our optimizer samples.
By averag-ing the bootstrap-estimated standard deviations over6Unlike actually using multiple test sets, bootstrap resam-pling does not help to re-estimate the mean metric score due totest set spread (unlike actually using multiple test sets) since themean over bootstrap replicates is approximately the aggregatemetric score.178optimizer samples, we have a statistic that jointlyquantifies the impact of test set effects and optimizerinstability on a test set.
We call this statistic ssel.Different values of this statistic can suggest method-ological improvements.
For example, a large ssel in-dicates that more replications will be necessary todraw reliable inferences from experiments on thistest set, so a larger test set may be helpful.To compute ssel, assume we have n indepen-dent optimization runs which produced weight vec-tors that were used to translate a test set n times.The test set has ` segments with references R =?R1, R2, .
.
.
, R`?.
Let X = ?X1,X2, .
.
.
,Xn?where each Xi = ?Xi1, Xi2, .
.
.
, Xi`?
is the list oftranslated segments from the ith optimization runlist of the ` translated segments of the test set.
Foreach hypothesis output Xi, we construct k bootstrapreplicates by drawing ` segments uniformly, with re-placement, from Xi, together with its correspondingreference.
This produces k virtual test sets for eachoptimization run i.
We designate the score of the jthvirtual test set of the ith optimization run with mij .If mi = 1k?kj=1 mij , then we have:si =???
?k?j=1(mij ?mi)2k ?
1ssel =1nn?i=1si4.2 Comparing Two SystemsIn the previous section, we gave statistics aboutthe distribution of evaluation metrics across a largenumber of experimental samples (Table 1).
Becauseof the large number of trials we carried out, we canbe extremely confident in concluding that for bothpairs of systems, the experimental manipulation ac-counts for the observed metric improvements, andfurthermore, that we have a good estimate of themagnitude of that improvement.
However, it is notgenerally feasible to perform as many replicationsas we did, so here we turn to the question of howto compare two systems, accounting for optimizernoise, but without running 300 replications.We begin with a visual illustration how opti-mizer instability affects test set scores when com-paring two systems.
Figure 1 plots the histogramof the 300 optimizer samples each from the twoBTEC Chinese-English systems.
The phrase-based46 47 48 49 50 51BLEU0510152025303540ObservationCountFigure 1: Histogram of test set BLEU scores for theBTEC phrase-based system (left) and BTEC hierarchicalsystem (right).
While the difference between the systemsis 1.5 BLEU in expectation, there is a non-trivial regionof overlap indicating that some random outcomes will re-sult in little to no difference being observed.0.6 0.3 0.0 0.3 0.6 0.9BLEU difference0.00.10.20.30.40.50.6Probabilityofobservation 1 sample3 samples5 samples10 samples50 samplesFigure 2: Relative frequencies of obtaining differencesin BLEU scores on the WMT system as a function of thenumber of optimizer samples.
The expected differenceis 0.2 BLEU.
While there is a reasonably high chance ofobserving a non-trivial improvement (or even a decline)for 1 sample, the distribution quickly peaks around theexpected value given just a few more samples.system?s distribution is centered at the samplemean 48.4, and the hierarchical system is centeredat 49.9, a difference of 1.5 BLEU, correspond-ing to the widely replicated result that hierarchi-cal phrase-based systems outperform conventionalphrase-based systems in Chinese-English transla-tion.
Crucially, although the distributions are dis-tinct, there is a non-trivial region of overlap, andexperimental samples from the overlapping regioncould suggest the opposite conclusion!To further underscore the risks posed by this over-lap, Figure 2 plots the relative frequencies withwhich different BLEU score deltas will occur, as afunction of the number of optimizer samples used.When is a difference significant?
To determinewhether an experimental manipulation results in a179statistically reliable difference for an evaluation met-ric, we use a stratified approximate randomization(AR) test.
This is a nonparametric test that approxi-mates a paired permutation test by sampling permu-tations (Noreen, 1989).
AR estimates the probability(p-value) that a measured difference in metric scoresarose by chance by randomly exchanging sentencesbetween the two systems.
If there is no significantdifference between the systems (i.e., the null hypoth-esis is true), then this shuffling should not changethe computed metric score.
Crucially, this assumesthat the samples being analyzed are representativeof all extraneous variables that could affect the out-come of the experiment.
Therefore, we must includemultiple optimizer replications.
Also, since metricscores (such as BLEU) are in general not compa-rable across test sets, we stratify, exchanging onlyhypotheses that correspond to the same sentence.Table 2 shows the p-values computed by AR, test-ing the significance of the differences between thetwo systems in each pair.
The first three rows illus-trate ?single sample?
testing practice.
Depending onluck with MERT, the results can vary widely frominsignificant (at p > .05) to highly significant.The last two lines summarize the results of the testwhen a small number of replications are performed,as ought to be reasonable in a research setting.
Inthis simulation, we randomly selected n optimizeroutputs from our large pool and ran the AR test todetermine the significance; we repeated this proce-dure 250 times.
The p-values reported are the p-values at the edges of the 95% confidence interval(CI) according to AR seen in the 250 simulated com-parison scenarios.
These indicate that we are verylikely to observe a significant difference for BTECat n = 5, and a very significant difference by n = 50(Table 2).
Similarly, we see this trend in the WMTsystem: more replications leads to more significantresults, which will be easier to reproduce.
Based onthe average performance of the systems reported inTable 1, we expect significance over a large enoughnumber of independent trials.5 Discussion and RecommendationsNo experiment can completely control for all pos-sible confounding variables.
Nor are metric scores(even if they are statistically reliable) a substitutefor thorough human analysis.
However, we believethat the impact of optimizer instability has been ne-p-valuen System A System B BTEC WMT1 high low 0.25 0.951 median median 0.15 0.131 low high 0.0003 0.003p-value (95% CI)5 random random 0.001?0.034 0.001?0.3850 random random 0.001?0.001 0.001?0.33Table 2: Two-system analysis: AR p-values for threedifferent ?single sample?
scenarios that illustrate differ-ent pathological scenarios that can result when the sam-pled weight vectors are ?low?
or ?high.?
For ?random,?we simulate an experiments with n optimization replica-tions by drawing n optimized system outputs from ourpool and performing AR; this simulation was repeated250 times and the 95% CI of the AR p-values is reported.glected by standard experimental methodology inMT research, where single-sample measurementsare too often used to assess system differences.
Inthis paper, we have provided evidence that optimizerinstability can have a substantial impact on results.However, we have also shown that it is possible tocontrol for it with very few replications (Table 2).We therefore suggest:?
Replication be adopted as standard practice inMT experimental methodology, especially inreporting results;7?
Replication of optimization (MERT) and testset evaluation be performed at least three times;more replications may be necessary for experi-mental manipulations with more subtle effects;?
Use of the median system according to a trustedmetric when manually analyzing system out-put; preferably, the median should be deter-mined based on one test set and a second testset should be manually analyzed.AcknowledgmentsWe thank Michael Denkowski, Kevin Gimpel, KennethHeafield, Michael Heilman, and Brendan O?Connor forinsightful feedback.
This research was supported in partby the National Science Foundation through TeraGrid re-sources provided by Pittsburgh Supercomputing Centerunder TG-DBS110003; the National Science Foundationunder IIS-0713402, IIS-0844507, IIS-0915187, and IIS-0915327; the DARPA GALE program, the U. S. ArmyResearch Laboratory, and the U. S. Army Research Of-fice under contract/grant number W911NF-10-1-0533.7Source code to carry out the AR test for multiple optimizersamples on the three metrics in this paper is available fromhttp://github.com/jhclark/multeval.180ReferencesA.
Arun, B. Haddow, P. Koehn, A. Lopez, C. Dyer,and P. Blunsom.
2010.
Monte Carlo techniquesfor phrase-based translation.
Machine Translation,24:103?121.S.
Banerjee and A. Lavie.
2005.
METEOR: An auto-matic metric for mt evaluation with improved corre-lation with human judgments.
In Proc.
of ACL 2005Workshop on Intrinsic and Extrinsic Evaluation Mea-sures for MT and/or Summarization.N.
Bertoldi, B. Haddow, and J.-B.
Fouet.
2009.
Im-proved minimum error rate training in Moses.
PragueBulletin of Mathematical Linguistics, No.
91:7?16.M.
Bisani and H. Ney.
2004.
Bootstrap estimates forconfidence intervals in ASR performance evaluation.In Proc.
of ICASSP.A.
Cahill, M. Burke, R. O?Donovan, S. Riezler, J. vanGenabith, and A.
Way.
2008.
Wide-coverage deepstatistical parsing using automatic dependency struc-ture annotation.
Computational Linguistics, 34(1):81?124.D.
Cer, D. Jurafsky, and C. D. Manning.
2008.
Regular-ization and search for minimum error rate training.
InProc.
of WMT.D.
Cer, C. D. Manning, and D. Jurafsky.
2010.
The bestlexical metric for phrase-based statistical mt systemoptimization.
In Human Language Technologies: The2010 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 555?563.
Proc.
of ACL, June.D.
Chiang, Y. Marton, and P. Resnik.
2008.
Online large-margin training of syntactic and structural translationfeatures.
In Proc.
of EMNLP.D.
Chiang.
2007.
Hierarchical phrase-based translation.Computational Linguistics, 33(2):201?228.N.
Chinchor.
1993.
The statistical significance of theMUC-5 results.
Proc.
of MUC.K.
Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,and Y.
Singer.
2006.
Online passive-aggressive al-gorithms.
Journal of Machine Learning Research,7:551?585.M.
Denkowski and A. Lavie.
2010.
Extending theMETEOR machine translation evaluation metric to thephrase level.
In Proc.
of NAACL.C.
Dyer, J. Weese, A. Lopez, V. Eidelman, P. Blunsom,and P. Resnik.
2010. cdec: A decoder, alignment,and learning framework for finite-state and context-free translation models.
In Proc.
of ACL.C.
Dyer.
2009.
Using a maximum entropy model to buildsegmentation lattices for MT.
In Proc.
of NAACL.B.
Efron.
1979.
Bootstrap methods: Another look at thejackknife.
The Annals of Statistics, 7(1):1?26.G.
Foster and R. Kuhn.
2009.
Stabilizing minimum errorrate training.
Proc.
of WMT.P.
Koehn, A. Birch, C. Callison-burch, M. Federico,N.
Bertoldi, B. Cowan, C. Moran, C. Dyer, A. Con-stantin, and E. Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proc.
ofACL.P.
Koehn.
2004.
Statistical significance tests for machinetranslation evaluation.
In Proc.
of EMNLP.S.
Kumar, W. Macherey, C. Dyer, and F. Och.
2009.Efficient minimum error rate training and minimumBayes-risk decoding for translation hypergraphs andlattices.
In Proc.
of ACL-IJCNLP.R.
C. Moore and C. Quirk.
2008.
Random restartsin minimum error rate training for statistical machinetranslation.
In Proc.
of COLING, Manchester, UK.E.
W. Noreen.
1989.
Computer-Intensive Methodsfor Testing Hypotheses: An Introduction.
Wiley-Interscience.F.
J. Och.
2003.
Minimum error rate training in statisticalmachine translation.
In Proc.
of ACL.K.
Papineni, S. Roukos, T. Ward, and W.-j.
Zhu.
2002.BLEU: a method for automatic evaluation of machinetranslation.
In Proc.
of ACL.S.
Riezler and J. T. Maxwell.
2005.
On some pitfallsin automatic evaluation and significance testing forMT.
In Proc.
of the Workshop on Intrinsic and Extrin-sic Evaluation Methods for Machine Translation andSummarization.M.
Snover, B. Dorr, C. Park, R. Schwartz, L. Micciulla,and J. Makhoul.
2006.
A study of translation edit ratewith targeted human annotation.
In Proc.
of AMTA.L.
Wasserman.
2003.
All of Statistics: A Concise Coursein Statistical Inference.
Springer.Y.
Zhang and S. Vogel.
2010.
Significance tests of auto-matic machine translation metrics.
Machine Transla-tion, 24:51?65.Y.
Zhang, S. Vogel, and A. Waibel.
2004.
InterpretingBLEU/NIST scores: How much improvement do weneed to have a better system?
In Proc.
of LREC.181
