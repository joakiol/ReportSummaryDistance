/IIII//////II////II/Look-Back and Look-Ahead in the Conversion ofHidden Markov Models  into Finite State TransducersAndrd KempeXerox Research Centre Europe - Grenoble Laboratory6, chemin de Maupertuis - 38240 Meylan - Franceandre, kempe@xrce,  xerox, tomhttp://www, xrce.
xerox, com/research/mlttAbstractThis paper describes the conversion of a Hid-den Markov Model into a finite state trans-ducer that closely approximates the behaviorof the stochastic model.
In some cases thetransducer is equivalent o the HMM.
Thisconversion is especially advantageous forpart-of-speech tagging because the resulting trans-ducer can be composed with other transducersthat encode correction rules for the most fre-quent tagging errors.
The speed of tagging isalso improved.
The described methods havebeen implemented and successfully tested.1 IntroductionThis paper presents an algorithm 1 which approxi-mates a Hidden Markov Model (HMM) by a finite-state transducer (FST).
We describe one applica-tion, namely part-of-speech tagging.
Other poten-tial applications may be found in areas where bothHMMs and finite-state technology are applied, suchas speech recognition, etc.
The algorithm has beenfully implemented.An HMM used for tagging encodes, like a trans-ducer, a relation between two languages.
One lan-guage contains sequences of ambiguity classes ob-tained by looking up in a lexicon all words of a sen-tence.
The other language contains equences of tagsobtained by statistically disambiguating the class se-quences.
From the outside, an HMM tagger behaveslike a sequential transducer that deterministicallymaps every class sequence to a tag sequence, e.g.
:\[DET, PRO\] \[ADJ,NOUN\] \[ADJ,NOUN\] ...... \[END\] (i)DET ADJ NOUN ...... ENDaThere are other (dillerent) algorithms for HMMto FST conversion: An unpublished one by Julian M.Kupiec and John T. Maxwell (p.c.
), and n-type and s-type approximation by Kempe (1997).The main advantage of transforming an HMM isthat the resulting transducer can be handled by fi-nite state calculus.
Among others, it can be com-posed with transducers that encode:?
correction rules for the most frequent aggingerrors which are automatically generated (Brill,1992; Roche and Schabes, 1995) or manuallywritten (Chanod and Tapanainen, 1995), in or-der to significantly improve tagging accuracy -9 .These rules may include long-distance depen-dencies not handled by ttMM taggers, and canconveniently be expressed by the replace oper-ator (Kaplan and Kay, 1994; Karttunen, 1995;Kempe and Karttunen, 1996).?
further steps of text analysis, e.g.
light parsingor extraction of noun phrases or other phrases(Ait-Mokhtar and Chanod, 1997).These compositions enable complex text analysis tobe performed by a single transducer.The speed of tagging by an FST is up to six timeshigher than with the original HMM.The motivation for deriving the FST from anHMM is that the tIMM can be trained and con-verted with little manual effort.An HMM transducer builds on the data (probabil-ity matrices) of the underlying HMM.
The accuracyof this data has an impact on the tagging accuracyof both the HMM itself and the derived transducer.The training of the HMM can be done on either atagged or untagged corpus, and is not a topic of thispaper since it is exhaustively described in the liter-ature (Bahl and Mercer, 1976; Church, 1988).An HMM can be identically represented by aweighted FST in a straightforward way.
We are,however, interested in non-weighted transducers.2Automatically derived rules require less work thanmanually written ones but are unlikely to yield betterresults because they would consider relatively limitedcontext and simple relations only.Kempe 29 Look-Back and Look-Ahead in the Conversion of HMMsAndr~ Kempe (1998) Look-Back and Look-Ahead in the Conversion of Hidden Markov Models into Finite StateTransducers.
In D.M.W.
Powers (ed.)
NeMLaP3/CoNLL98: New Methods in Language Processing and Computational NaturalLanguage Learning, ACL, pp 29-37.2 b-Type ApproximationThis section presents a method that approximatesa (first order) Hidden Markov Model (HMM) by afinite-state transducer (FST), called b-type approxi-mation s. Regular expression operators used in thissection are explained in the annex.Looking up, in a lexicon the word sequence of asentence produces a unique sequence of ambiguityclasses.
Tagging the sentence by means of a (firstorder) ttMM consists of finding the most probabletag sequence T given this class sequence C (eq.
1,fig.
1).
The joint probability of the sequences C andT can be estimated by:p(C ,  T )  = p(c i  .... c .
, t z  .... tn) =11,r(t,) b(c~ Its).
\]- I  a(t~ Iti- ~) b(c~ Its)i=2(2)2.1 Bas ic  IdeaThe determination of a tag of a particular word can-not be made separately from the other tags.
Tagscan influence each other over a long distance viatransition probabilities.In this approach, an ambiguity class is disam-biguated with respect to a context.
A context con-sists of a sequence of ambiguity classes limited atboth ends by some selected tag 4.
For the left con-text of length/3 we use the term look-back, and forthe right context of length a we use the term look-ahead.Wi.3 Wi.2 Wi.i Wi Wt+ t Wt?
2 Wi+ 3 wordsCi-3 Ci-I i~i'i Ci Ci+i Ci+2 Ci?3 classest l  : ;I 1 ~1 :1 i-3 ti'l ti+l t+2 ti*3t2+-3 tP-2~ tP-I l~ I  t~l x t~ ti2+3 m~Lt~.
l J '~  t~.z t 3 i-3Figure 1: Disambiguation of classes betweentwo selected tagsa look-ahead istance of a = 2.
Actually, the twoselected tags t~_ 2 and t~+ 2 allow not only the disam-biguation of the class ci but of all classes inbetween,i.e.
c i - t ,  ci and ci+l.We approximate the tagging of a whole sentenceby tagging subsequences with selected tags at bothends (fig.
1), and then overlapping them.
The mostprobable paths in the tag space of a sentence, i.e.valid paths according to this approach, can be foundas sketched in figure 2.w I w z w 3 w 4 w s w~ w 7 w~ words# c i c 2 C 3 c 4 c 5 c 6 .c 7 c 8 # classest t t t - t  t t# t ~ ~  t ~"~r-'--t ~..t # ~gsFigure 2: Two valid paths through the tagspace of a sentencew~ w 2 w 3 w 4 w 5 w 6 w7 w8 wordsc i c 2 c 3 c 4 C 5 c 6 c 7 C s # classesxW./ tagsFigure 3: Incompatible sequences in the tagspace of a sentenceA valid path consists of an ordered set of overlap-ping sequences .in which each member overlaps withits neighbour except for the first or last tag.
Therecan be more than one valid path in the tag spaceof a sentence (fig.
2).
Sets of sequences that do notoverlap in such a way are incompatible according tothis model, and do not constitute valid paths (fig.
3).In figure 1, the tag t~ can be selected from the classci because it is between two selected tags d which aret~_ 2 at a look-back distance of fl = 2 and t~2+2 atZName given by the author, to distinguish the algo-rithm from n-type and s-type approximation (Kempe,1997).4The algorithm is explained for a first order HMM.
Inthe case of a second order HMM, b-type sequences mustbegin and end with two selected tags rather than one.2.2 b -Type  SequencesGiven a length ~ of look-back and a length a of look-ahead, we generate for every class co, every look-back sequence t_~ c-a+1 ... c-z, and every look-ahead sequence ci ... ca-1 ta, a b-type sequenced:t_~ c- ,+z ... c-z co cl ... c~-z t~ (3)Kempe 30 Look-Back and Look-Ahead inthe Conversion of HMMsIIIIIII|IIIIIIIilIII!IIilIIIIIIIIIIIIIIIII!IIFor example:CONJ \[DET, PRON\] lAD J, NOUN, VERB\] [NOUI~, VERB\] VERB (4)Each such original b-type sequence (eq.
3,4; fig.
4)is disambiguated based on a first order HMM.
Herewe use the Viterbi algorithm (Viterbi, 1967; Ra-biner, 1990) for efficiency.look-back look-ahead-~, ~- I  .
.
.
- I  0 I .
.
.
a - I  a pos i t ionsz:-.a/'~J\a a I a~a J a a ( I /~;l t~-~ t_~-V, ...---- t_, ~ to----- t,----...--~ to.~-:r=~to 3 ~stransition probabili~" b cla~ probabili~(~r - "(~r" "(~r" "(~ original b-t~pe s quenceFigure 4: b-Type sequenceFor an original b-type sequence, the joint proba-bility of its class sequence C with its tag sequence T(fig.
4), can be estimated by:p(C, T) = p(c_~+~ ... e~_~ , t-z ... t~) =\[i=~S+la(t,lt,_~) b(cilti)\].a(t~lt~_~) (5)At every position in the look-back sequence andin the look-ahead sequence, a boundary # may oc-cur, i.e.
a sentence beginning or end.
No look-back(~?
= 0) or no look-ahead (a = 0) is also allowed.The above probability estimation (eq.
5) can thenbe expressed more generally (fig.
4) as:p(C, T) = p,~,~ .p,,~e~e .
p,,e (6)with P~tart beingPsta~t = a(t-Z+zlt-S) for selected tag t_ z (7)P~t~.t = rr(t-z+z) for boundary ~ (8)P, ta~ = 1 for ~3=0 (9)with prniddle beinga-1Prniaate = b(c-a+z It-z+1)" H a(tilti_i) b(cilti)i= -Z+2for a+#> 0 (10)PmiddZe = b(colto) for a+/~=0 (11)and with Pend beingPe,~a =a(ta\[t.a-z) for selected tag ta (12)Pend = 1 for boundary # or a=0 (13)When the most likely tag sequence isfound for anoriginal b-type sequence, the class co in the middleposition (eq.
3) is associated with its most likely tagto.
We formulate constraints for the other tags t_ zand ta and classes c_z+1...c_ z and Cl...ca_ I of theoriginal b-type sequence.
Thus we obtain a taggedb-type sequence s." (14) - c_/~+l .-.C_ 2 C0:~0 C2- '"~a-1 tastating that to is the most probable tag in the classco if it is preceded by t B~ cS(Z-z)...cB2 c m andfollowed by c a l  cA:...c A(~-I)  ta%In expression 14 the subscripts --/3 -B+I...0...~-Ia denote the position of the tag or class in the b-typesequence, and the superscripts Bfl B(/~-I)...B1 andA1.
.
.A (o -1)  Aa express constraints for precedingand following tags and classes which are part of otherb-type sequences.
In the exampleS:CONI-B2 \[DET, PRON\]-B1\[ADJ,NOUN, v~aB\]:~a\[~ao~,v~aB}-al V~B-A2 (15)ADJ is the most likely tag in the class\[?1~J,IY0trN,vFalB\] if it is preceded by the tag C0NJtwo positions back (B2), by the class \[DET,PRON'Ione position back (B1), and followed by the classI'NOUlY,VEI~\] one position ahead (A1) and by thetag VERB two positions ahead (A2).Boundaries are denoted by a particular symboland can occur at the edge of the look-back and look-ahead sequence:t B~ c s(t~-l) ...c B2 c B1 c:t  c Ax c A1 ...c A(a-1) #An (16)t s# c ~(~-l) ...c ~ c B1 c:t c A1 c A1 ...#A(~--Z) (17)#Be C~(~-Z) ...CB2 cBZ c:t #AZ (18)#BZ c:t #AZ (19)#B2 cBl c:t c A' c ~I ...cA(?-I) t a~ (20)For example:~-B2 \[DET, PRONI-B1\[ADJ, NOUN, V~B\]: ADJ(21)SRegular expression perators used in this article areexplained in the annex.Kempe 31 Look-Back and Look-Ahead in the Conversion o f  HMMsC0NJ---B2 \[DET, PRON\]-B1\[ADJ, NOON, VF23\] : NOUN#-A~ (22)Note that look-back of length ,3 and look-ahead oflength a also include all sequences shorter than 3 or~, respectively, that are limited by #.For a given length 3 of look-back and a length aof look-ahead, we generate very possible original b-type sequence (eq.
3), disambiguate it statistically(eq.
5-13), and encode the tagged b-type sequenceBi (eq.
14) as an FST.
All sequences Bi are thenunioned?B = U B; (23){and we generate a preliminary tagger model B"B" = lOB \].
(24)where all sequences Bi can occur in any orderand number (including zero times) because no con-straints have yet been applied.2.3 Concatenat ion ConstraintsTo ensure a correct concatenation f sequences Bi,we have to make sure that every Bi is preceded andfollowed by other Bi according to what is encodedin the look-back and look-ahead constraints.
E.g.the sequence in example (21) must be preceded bya sentence beginning, #, and the class \[DET,PRON\]and followed by the class \[NOON, VERB\] and the tagVERB.We create constraints for preceding and followingtags, classes and sentence boundaries.
For the look-back, a particular tag ti or class cj is required for aparticular distance of 6 < -1, byS:R'(ti) ='\[-\[?
* tl \[\ut\]* ~t \[\ut\].\]'(-$-1}\] t/B(-~) ?.\] (25)R'(cj) =' \ [ ' \ [% cj \[\%\]* \[% \[\%\],\]'(-$-I)\] ci(-~) ?.\] (26)for 6 < -1with ?t and ?c being the union of all tags and allclasses respectively.A sentence beginning, #, is required for a partic-ular look-back distance of 6<-1 ,  on the side of thetags, by:R'(#) =-\[ "\[ \[\~t\], [~t \[\~t\],\]'(-~-1)\] #8(-~ ?,\] (2r)for J < -1In the case of look-ahead we require for a partic-ular distance of 6 > 1, a particular tag ti or class cjor a sentence nd, #,  on the side of the tags, in asimilar way by:n~(t,) =-\[?, t, ~s -{ \[\?t\], ~t \[\~t\],\]-(~-ll t  ?,\]\] (2s)a%,) =-\[ ?, ~ -\[ \[\?4* \[o \[\~\]*\]'(~-~1 c, ?,\]\] (29)n'(#) =-\[ ?, #.,6 -\[ \[\?t\], \[?t \[\~t\],\]-(~-l)\]\] (30)for J> lAll tags ti are required for the look-back only atthe distance of 6 = -3  and for the look-ahead onlyat the distance of 6 = a.
All classes cj are requiredfor distances of 6 E \ [ -3  + 1, -1\] and 6 E \[1, a, - 1\].Sentence boundaries, #, are required for distancesof 6 E \ [ -3 , -1 \ ]  and 6 E \[1, a\].We create the intersection Rt of all tag con-straints, the intersection Re of all class constraints,and the intersection R# of all sentence boundaryconstraints:R, = N R,(t,) (31)i ~ \[I,.\]e {-~,~}Ro = N R%) (32)j ~ ll,ml6 E \[-3+l,--l\]U\[l,a--l\]a# = n a~(#) (33)~ \[-~,- I\]u\[I,.,\]All constraints are enforced by composition withthe preliminary tagger model B" (eq.
24).
The classconstraint Rc is composed on the upper side of B"which is the side of the classes (eq.
14), and boththe tag constraint Rt and the boundary constraint 6R# are composed on the lower side of B', which isthe side of the tagsS:B'" = Rc .o.
B" .o.
Rt .o.
R# (34)Having ensured correct concatenation, we deleteall symbols r that have served to constrain tags,classes or boundaries, using Dr:6The boundary constraint R# could alternatively becomputed for and composed on the side of the classes.The transducer which encodes R# would then, however,be bigger because the number of classes is bigger thanthe number of tags.Kempe 32 Look-Back and Look-Ahead in the Conversion of HMMsIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII!IIIIIIIIIIIIIIIIIIIIIIIIIIIIIID, = r -> \[\] (36)By composing r B'" (eq.
34) on the lower side withDr and on the upper side with the inverted relationDr.i ,  we obtain the final tagger model B:B = D,.
i  .o.B'" .o.
Dr (37)We call the model a b-type model, the correspond-ing FST a b-type transducer, and the whole algo-rithm leading from the HMM to the transducer, ab-type approximation of an HMM.2.4 Proper t ies  of b -Type  TransducersThere are two groups of b-type transducers with dif-ferent properties: FSTs without look-back and/orwithout look-ahead (19-a = 0) and FSTs with bothlook-back and look-ahead (8"a > 0).
Both acceptany sequence of ambiguity classes.b-Type FSTs with $.cr =0 are always sequential.They map a class sequence that corresponds to theword sequence of a sentence, always to exactly onetag sequence.
Their tagging accuracy and similaritywith the underlying HMM increases with growingfl + or.
A b-type FST with $ = 0 and a = 0 is equiva-lent to an nO-type FST, and with $ = 1 and a = 0 itis equivalent to an nl-type FST (Kempe, 1997).b-Type FSTs with $ .a  > 0 are in general not se-quential.
For a class sequence they deliver a set ofdifferent ag sequences, which means that the tag-ging results are ambiguous.
This set is never empty,and the most probable tag sequence according to theunderlying HMM is always in this set.
The longerthe look-back distance $ and the look-ahead istancea are, the larger the FST and the smaller the set ofresulting tag sequences.
For sufficiently large $+a,this set may contain always only one tag sequence.In this case the FST is equivalent to the underlyingHMM.
For reasons of size however, this FST maynot be computable for particular HMMs (see.
4).3 An  Imp lemented  F in i te -S ta te  TaggerThe implemented tagger equires three transducerswhich represent a lexicon, a guesser and an approx-imation of an HMM mentioned above.Both the lexicon and guesser are sequential, i.e.deterministic on the input side.
They both unam-biguously map a surface form of any word that theyaccept to the corresponding ambiguity class (fig.
5,col.
1 and 2): First of all, the word is looked for in therFor efficiency reasons, we actually do not delete theconstraint symbols r by composition.
We rather tra-verse the network, and overwrite every symbol r withthe empty string symbol e. In the following determiniza-tion of the network, all ~ are eliminated.lexicon.
If this fails, it is looked for in the guesser.
Ifthis equally fails, it gets the label \[UNKNOWN\] whichdenotes the ambiguity class of unknown words.
Tagprobabilities in this class are approximated by tagsof words that appear only once in the training cor-pus.As soon as an input token gets labeled with thetag class of sentence nd symbols (fig.
5: \[SENT\] ),the tagger stops reading words from the input.
Atthis point, the tagger has read and stored the wordsof a whole sentence (fig.
5, col. 1) and generated thecorresponding sequence of classes (fig.
5, col. 2).The class sequence is now mapped to a tag se-quence (fig.
5, col. 3) using the HMM transducer.
Ab-type FST is not sequential in general (sec.
2.4),so to obtain a unique tagging result, the finite-statetagger can be run in a special mode, where only thefirst, result found is retained, and the tagger doesnot look for other results .
Since paths through anFST have no particular order, the result retained israndom.The tagger outputs the stored word and tag se-quence of the sentence, and continues in the sameway with the remaining sentences of the corpus.The \[AT\] ATshare Ll~, VB\] NNof \[IN\] IN.
.
,tripled \[VBD, VBN\] VBD.ithin \[IN ,RB\] INthat \[CS ,DT.WPS\] DTspan \[NN,VB, VBD\] NNof \[IN\] INt ime \['NN, VB\] NN\[SENT\] s~.wrFigure 5: Tagging a sentenceThe tagger can be run in a statistical mode ,, herethe number of tag sequences found per sentence iscounted.
These numbers give an overview of thedegree of non-sequentiality of the concerned b-typetransducer (sec.
2.4).8This mode of retaining the first result only is notnecessary with n-type and s-type transducers which areboth sequential (Kempe, 1997).Kempe 33 Look-Back and Look-Ahead in the Conversion of HMMsTransduceror HMMI HMMI Accuracy \] Tagging speed Transducer size Creationtest corp.
I in words/sec timein % I ultra2 Isparc20 #states I #arcs\] .
ultra2t 97.351 48341 16241 ~1 ~l_~ls nI-FST ,733 I,,3, J  ,80 i ,,iOpi 15,225 22 ios+nl-FST 1M, F8) 96.12 22 001 9 969 329 42 560 :.. 4 minb-FST (/~=0, a=0), =nO 87.21 26 585 11 000 1 181 6 se~b-FST (fl=l,a=0), =nl 95.16 26 585 11 600 37 6 697 11 secb-FST (~=2,a=0) 95.32 21 268 7 089 3 663 663 003 4 h 11b-FST (fl=0, a=l) 93.691 199391 877 I 252 40243 12secb-FST (fl=0,a=2) 93.92 19 334 114 10 554 l 246 686 j0 minb-FST (fl=2, a=l) "97.34 15 191 6 510 54 578 18 402 055 2 h 17b-FST (fl=3, a=l) FST was not computableLanguage: EnglishCorpora: 19 944 words for HMM training, 19 934 words for testTag set: 36 tags, 181 classes* Multiple, i.e.
ambiguous tagging results: Only first result retainedTypes of FST  (Finite-State Transducers) :n0, nl n-type transducers (Kempe, 1997)s+nl (IM,FS) s-type transducer (Kempe, 1997),with subsequences of frequency > 8, from a training corpusof I 000 000 words, completed with nl-typeb (fl=2,a=l) b-type transducer (sec.
2), with look-back of 2 and look-ahead of iComputers:ultra2 1 CPU, 512 MBytes physical RAM, 1.4 GBytes virtual RAMspare20 1 CPU, 192 MBytes physical RAM, 827 MBytes virtual RAMTable 1: Accuracy, speed, size and creation time of some HMM transducers4 Experiments and ResultsThis section compares different FSTs with eachother and with the original ttMM.As expected, the FSTs perform tagging fasterthan the HMM.Since all FSTs are approximations of HMMs, theyshow lower tagging accuracy than the ttMMs.
In thecase of FSTs with fl > 1 and a = 1, this difference inaccuracy is negligible.
Improvement in accuracy canbe expected since these FSTs can be composed withFSTs encoding correction rules for frequent errors(sec.
1).For all tests below an English corpus, lexicon andguesser were used, which were originally annotatedwith 74 different ags.
We automatically recoded thetags in order to reduce their number, i.e.
in somecases more than one of the original tags were recodedinto one and the same new tag.
We applied differentrecodings, thus obtaining English corpora, lexiconsand guessers with reduced tag sets of 45, 36, 27, 18and 9 tags respectively.FSTs with f l=  2 and ~ = 1 and with f l=  1 anda = 2 were equivalent, in all cases where they couldbe computed.Table 1 compares different FSTs for a tag set of36 tags.The b-type FST with no look-back and no look-ahead which is equivalent to an n0-type FST(Kempe, 1997), shows the lowest tagging accuracy(b-FST ()3=0, a=0) :  87.21%).
It is also the small-est transducer (1 state and 181 arcs, as many astag classes) and can be created faster than the otherFSTs (6 sec.
).The highest accuracy is obtained with a b-typeFST with f l=  2 and a = 1 (b-FST ( /3=2,~=1):97.34 %) and with an s-type FST (Kempe, 1997)trained on 1 000 000 words (s+nl-FST (1M, F1):97.33 %).
In these two cases the difference in accu-racy with respect o the underlying ttMM (97.35 %)is negligible.
In this particular test, the s-type FSTcomes out ahead because it is considerably smallerthan the b-type FST.The size of a b-type FST increases with the sizeof the tag set and with the length of look-back pluslook-ahead, ~+c~.
Accuracy improves with growingb-Type FSTs may produce ambiguous tagging re-suits (sec.
2.4)'.
In such instances only the first resultwas retained (see.
3).Kempe 34 Look-Back and Look-Ahead inthe Conversion of HMMsIIIIIIIIIIIiIIIIIIIIIIIIIIIIIIIIIIIIIIIIilIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIITransduceror HMMHMMs+nl FST (1M, F1) 96.7699.89s+nl-FST (1M, F8) 95.0997.00b-FST (fl=0,a=0), =n0 83.5384.00b-FST (/3=l,a=0), =nl 94.1995.61b-FST (fl=2, a=0)b-FST (fl=0, a=l ) 92.7993.64b-FST (fl=0, c~=2) 93.4694.35b-FST (fl=l, a--'l } *94.94*97.86b-FST (B=2, a=l)b-FST (/3=3, a=l)Tagging accuracy and agreement with the tIMMfor tag sets of different sizes297 cls.
214 cls.
181 cls.
119 els.
97 ds.
67 ?ls.I 96"781 96.92 \] 97"351 97"071 96 73 I' 95.76 I96.8899.9395.2597.3583.7184.4094.0995.9294.2896.0992.4793.4192.7793.70"95.14?97.9397.33 97.06 96.72 95.7499.90 99.95 99.95 99.9496.12 96.36 96.05 95.2998.15 98.90 98.99 98.9687.21 94.47 94.24 93.8688.04 96.03 96.22 95.7695.16 95.60 95.17 94.1496.90 97.75 97.66 96.7495.32 95.71 95.31 94.2297.01 97.84 97.77 96.8393.69 95.26 95.19 94.6494.67 96.87 97.06 97.0993.92 95.37 95.30 94.8094.90 96.99 97.20 97.29"95.78 "96.78 "96.59 "95.36"98.11 ?99.58 "99.72 *99.26"97.34 ?97.06 "96.73 ?95.73*99.97 ?99.98 *100.00 *99.9795.76100.00Language: EnglishCorpora: 19 944 words for HMM training, 19 934 words for testTypes of FST (Finite-State Transducers) cf.
table 1\[ 9998 1 7?6  IMultiple, i.e.
ambiguous tagging results: Only first result retainedTagging accuracy of 97.06 %,and agreement ofFST with HMM tagging results of 99.98 %Transducer could not be computed, for reasons of size.Table 2: Tagging accuracy and agreement ofthe FST tagging results with thoseof the underlying HMM, for tag sets of different sizesTable 2 shows the tagging accuracy and the agree-ment of the tagging results with the results of theunderlying HMM for different FSTs and tag sets ofdifferent sizes.To get results that are almost equivalent to thoseof an HMM, a b-type FST needs at least a look-backof/5 = 2 and a look-ahead of a = 1 or vice versa.For reasons of size, this kind of FST could only becomputed for tag sets with 36 tags or less.
A b-typeFST with/5 = 3 and a = 1 could only be computedfor the tag set with 9 tags.
This FST gave exactlythe same tagging results as the underlying HMM.Table 3 illustrates which of the b-type FSTs aresequential, i.e.
always produce exactly one taggingresult, and which of the FSTs are non-sequential.For all tag sets, the FSTs with no look-back(/5 = 0) and/or no look-ahead (a = 0) behaved se-quentially.
Here 100 % of the tagged sentences hadonly one result.
Most of the other FSTs (/5.
o~ > 0)behaved non-sequentially.
For example, in the caseof 27 tags w i th l3=l  anda=l ,  90.08%of thetagged sentences had one result, 9.46 % had two re-sults, 0.23 % had tree results, etc.Non-sequentiality decreases with growing look-back and look-ahead,/5+c~, and should completelydisappear with sufficiently large/5+~.
Such b-typeFSTs can, however, only be computed for small tagsets.
We could compute this kind of FST only forthe case of 9 tags with/5=3 and a=l.The set of alternative tag sequences for a sentence,produced by a b-type FST with/5, a > 0, alwayscontains the tag sequence that corresponds with theresult of the underlying HMM.Kempe 35 Look-Back and Look-Ahead inthe Conversion of HMMsI Sentences with n tagging resultsTransducer (in %)n= 11 n= 21n= 31n= 41 5-8\] 9-1674 tags, 297 dasses (origina~ tag set)b-FST (fl.a=0) I 1?
?1b-FST (fl=l,a=l) 75.14120.18 t 0.341 3.421 0.801 0.11b-FST (~=2,a=l) FST was not computable45 tags, 214 classes (reduced tag set)b-rSZ(a.4=0) I 1?
?1 I l I Ib-FST (fl=1,4=1)175.71119.731 0.68\[ 3.191 0.68\]b-FST (fl=2,4=1)\[ FST was not computable36 tags, 181 classes (reduced tag set)b-FST (fl-a=0) 100b-FST (fl=1,4=1) 78.56 17.90 0.34 2.85 0.34b-FST (/3=2,4=1) 99.77 0.2327 tags, 119 classes (reduced tag set)b-FST (/3-4=0} 100b-FST (fl=1,a=l) 90.08 9.46 0.23~ 0.11 0.11b-FST (fl=2,a=l) 99.77 0.2318 tags, 97 classes (reduced tag set)b-FST (fl-a=0) 100b-FST (fl=l,4=l)\[93.04 6.84 0.11b-FST (fl--2,a--1)199.89 0.119 tags, 67 classes (reduced tag set)b-FST (fl-4=0) 1001b.-FST (fl=l,4=l) 86.66112.43 0.91b-FST (fl=2,4=1) 99.771 0.23b-FST (fl=3,4=1) 100Language: English 1ITest corpus: 19 934 words, 877 sentences\[Types of FST (Finite-State Transducers) cf.
table 1Table 3: Percentage of sentences with a par-ticular number of tagging results5 Conc lus ion  and  Future  ResearchThe algorithm presented in this paper describes theconstruction of a finite-state transducer (FST) thatapproximates the behaviour of a Hidden MarkovModel (HMM) in part-of-speech tagging.The algorithm, called b-type approximation, useslook-back and look-ahead of freely selectable ngth.The size of the FSTs grows with both the size ofthe tag set and the length of the look-back plus look-ahead.
Therefore, to keep the FST at a computablesize, an increase in the length of the look-back orlook-ahead, requires a reduction of the number oftags.
In the case of small tag sets (e.g.
36 tags), thelook-back and look-ahead can be sufficiently largeto obtain an FST that is almost equivalent to theoriginal HMM.In some tests s-type FSTs (Kempe, 1997) andb-type FSTs reached equal tagging accuracy.
Inthese cases s-type FSTs are smaller because theyencode the most frequent ambiguity class sequencesof a training corpus very accurately and all othersequences less accurately, b-Type FSTs encode allsequences with the same accuracy.
Therefore, ab-type FST can reach equivalence with the originalHMM,  but an s-type FST  cannot.The algorithms of both conversion and tagging arefully implemented.The main advantage of transforming an HMM isthat the resulting FST can be handled by finite statecalculus ~and thus be directly composed with otherFSTs.The tagging speed of the FSTs is up to six timeshigher than the speed of the original HMM.Future  research will include the composition ofHMM transducers with, among others:?
FSTs that encode correction rules for the mostfrequent tagging errors in order to significantlyimprove tagging accuracy (above the accuracyof the underlying HMM).
These rules can ei-ther be extracted automatically from a corpus(Brill, 1992) or written manually (Chanod andTapanalnen, 1995).
* FSTs for light parsing, phrase extraction andother text analysis (Ait-Mokhtar and Chanod,1997).An HMM transducer can be composed with oneor more of these FSTs in order to perform complextext analysis by a single FST.ANNEX:  Regu lar  Express ion OperatorsBelow, a and b designate symbols, A and B designatelanguages, and R and Q designate relations betweentwo languages.
More details on the followingoperators and pointers to finite-state literature canbe found inhttp://w~, xrce.
xerox, com/research/ml~t/f s~-A\aA*A^na -> bComplement (negation).
Set of all stringsexcept hose from the language A.Term complement.
Any symbol otherthan a.Kleene star.
Language A zero or moretimes concatenated with itself.A n times.
Language A n times concate-nated with itself.Replace.
Relation where every a on theupper side gets mapped to a b on thelower side.9A large library of finite-state functions is availableat Xerox.Kempe 36 Look-Back and Look-Ahead in the Conversion of HMMsIIIII!IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII!IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIa:bR.iA BR.o.q0 or \[\]?Symbol pair with a on the upper and bon the lower side.Inverse relation where both sides are ex-changed with respect o R.Concatenation ofall strings of A with allstrings of B.Composition of the relations R and .Empty string (epsilon).Any symbol in the known alphabet andits extensionsAcknowledgementsI am grateful to all colleagues who helped me, par-ticularly to Lauri Karttunen (XRCE Grenoble) forextensive discussion, and to Julian Kupiec (XeroxPARC) for sending me information on his own re-lated work.
Many thanks to Irene Maxwell for cor-recting various versions of the paper.Re ferencesAit-Mokhtar, Salah and Chanod, Jean-Pierre(1997).
Incremental Finite-State Parsing.
In theProceedings of the 5th Conference of Applied .Nat-ural Language Processing (ANLP).
ACL, pp.
72-79.
Washington, DC, USA.Bahl, Lalit R. and Mercer, Robert L. (1976).
Partof Speech Assignment by a Statistical Decision Al-gorithm.
In IEEE international Symposium onInformation Theory.
pp.
88-89.
Ronneby.Brill, Eric (1992).
A Simple Rule-Based Part-of-Speech Tagger.
In the Proceedings of the 3rd con-ference on Applied Natural Language Processing,pp.
152-155.
Trento, Italy.Chanod, Jean-Pierre and Tapanainen, Pasi (1995).Tagging French - Comparing a Statistical and aConstraint Based Method.
In the Proceedings ofthe 7th conference of the EACL, pp.
149-156.ACL.
Dublin, Ireland.
cmp-lg/9S03003Church, Kenneth W. (1988).
A Stochastic PartsProgram and Noun Phrase Parser for UnrestrictedText.
In Proceedings of the 2nd Conference onApplied Natural Language Processing.
ACL, pp.136-143.Kaplan, Ronald M. and Kay, Martin (1994).
Regu-lar Models of Phonological Rule Systems.
In Com-putational Linguistics.
20:3, pp.
331-378.Karttunen, Lauri (1995).
The Replace Operator.
Inthe Proceedings of the 33rd Annual Meeting of theAssociation for Computational Linguistics.
Cam-bridge, MA, USA.
cmp-lg/9504032Kempe, Andrd and Karttunen, Lauri (1996).
Par-allel Replacement in Finite State Calculus.
Inthe Proceedings of the 16th International Confer-ence on Computational Linguistics, pp.
622-627.Copenhagen, Denmark.
crap-lg/9607007Kempe, Andrd (1997).
Finite State Transducers Ap-proximating Hidden Markov Models.
In the Pro-ceedings of the 35th Annual Meeting of the Associ-ation for Computational Linguistics, pp.
460-467.Madrid, Spain.
crap-lg/9707006Rabiner, Lawrence R. (1990).
A Tutorial on Hid-den Markov Models and Selected Applications inSpeech Recognition.
In Readings in Speech Recog-nition (eds.
A. Waibel, K.F.
Lee).
Morgan Kauf-mann Publishers, Inc. San Mateo, CA., USA.Roche, Emmanuel and Schabes, Yves (1995).
Deter-ministic Part-of-Speech Tagging with Finite-StateTransducers.
In Computational Linguistics.
Vol.21, No.
2, pp.
227-253.Viterbi, A.J.
(1967).
Error Bounds for Convolu-tional Codes and an Asymptotical Optimal De-coding Algorithm.
In Proceedings of IEEE, vol.61, pp.
268-278.Kempe 37 Look-Back and Look-Ahead inthe Conversion of HMMsmmmmmmmmmmmmmmmmm
