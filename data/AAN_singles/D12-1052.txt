Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 568?578, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsA Beam-Search Decoder for Grammatical Error CorrectionDaniel Dahlmeier1 and Hwee Tou Ng1,21NUS Graduate School for Integrative Sciences and Engineering2Department of Computer Science, National University of Singapore{danielhe,nght}@comp.nus.edu.sgAbstractWe present a novel beam-search decoder forgrammatical error correction.
The decoderiteratively generates new hypothesis correc-tions from current hypotheses and scores thembased on features of grammatical correctnessand fluency.
These features include scoresfrom discriminative classifiers for specific er-ror categories, such as articles and preposi-tions.
Unlike all previous approaches, ourmethod is able to perform correction of wholesentences with multiple and interacting er-rors while still taking advantage of powerfulexisting classifier approaches.
Our decoderachieves an F1 correction score significantlyhigher than all previous published scores onthe Helping Our Own (HOO) shared task dataset.1 IntroductionGrammatical error correction is an important prob-lem in natural language processing (NLP) that hasattracted an increasing amount of interest overthe last few years.
Grammatical error correctionpromises to provide instantaneous accurate feedbackto language learners, e.g., learners of English as aSecond Language (ESL).The dominant paradigm that underlies most er-ror correction systems to date is multi-class clas-sification.
A classifier is trained to predict a wordfrom a confusion set of possible correction choices,given some feature representation of the surround-ing sentence context.
During testing, the classifierpredicts the most likely correction for each test in-stance.
If the prediction differs from the observedword used by the writer and the classifier is suffi-ciently confident in its prediction, the observed wordis replaced by the prediction.
Although considerableprogress has been made, the classification approachsuffers from some serious shortcomings.
Each clas-sifier corrects a single word for a specific error cat-egory individually.
This ignores dependencies be-tween the words in a sentence.
Also, by conditioningon the surrounding context, the classifier implicitlyassumes that the surrounding context is free of gram-matical errors, which is often not the case.
Finally,the classifier typically has to commit to a single one-best prediction and is not able to change its deci-sion later or explore multiple corrections.
Instead ofcorrecting each word individually, we would like toperform global inference over corrections of wholesentences which can contain multiple and interact-ing errors.An alternative paradigm is to view error correc-tion as a statistical machine translation (SMT) prob-lem from ?bad?
to ?good?
English.
While this ap-proach can naturally correct whole sentences, a stan-dard SMT system cannot easily incorporate mod-els for specific grammatical errors.
It also suffersfrom the paucity of error-annotated training data forgrammar correction.
As a result, applying a stan-dard SMT system to error correction does not pro-duce good results, as we show in this work.In this work, we present a novel beam-search de-coder for grammatical error correction that com-bines the advantages of the classification approachand the SMT approach.
Starting from the origi-nal input sentence, the decoder performs an itera-tive search over possible sentence-level hypotheses568to find the best sentence-level correction.
In eachiteration, a set of proposers generates new hypothe-ses by making incremental changes to the hypothe-ses found so far.
A set of experts scores the newhypotheses on criteria of grammatical correctness.These experts include discriminative classifiers forspecific error categories, such as articles and prepo-sitions.
The decoder model calculates the overall hy-pothesis score for each hypothesis as a linear com-bination of the expert scores.
The weights of the de-coder model are discriminatively trained on a devel-opment set of error-annotated sentences.
The high-est scoring hypotheses are kept in the search beamfor the next iteration.
This search procedure contin-ues until the beam is empty or the maximum numberof iterations has been reached.
The highest scoringhypothesis is returned as the sentence-level correc-tion.
We evaluate our proposed decoder in the con-text of the Helping Our Own (HOO) shared task ongrammatical error correction (Dale and Kilgarriff,2011).
Our decoder achieves an F1 score of 25.48%which improves upon the current state of the art.The remainder of this paper is organized as fol-lows.
The next section gives an overview of relatedwork.
Section 3 describes the proposed beam-searchdecoder.
Sections 4 and 5 describe the experimentalsetup and results, respectively.
Section 6 providesfurther discussion.
Section 7 concludes the paper.2 Related WorkIn this section, we summarize related work in gram-matical error correction.
For a more detailed review,the readers can refer to (Leacock et al2010).The classification approach to error correction hasmainly focused on correcting article and prepositionerrors (Knight and Chander, 1994; Han et al2006;Chodorow et al2007; Tetreault and Chodorow,2008; Gamon, 2010; Dahlmeier and Ng, 2011b; Ro-zovskaya and Roth, 2011).
The advantage of theclassification approach is that it can make use ofpowerful machine learning algorithms in connectionwith arbitrary features from the sentence context.Typical features include surrounding N-grams, part-of-speech (POS) tags, chunks, etc.
In fact, a consid-erable amount of research effort has been invested infinding better features.The SMT approach to error corrections has re-ceived comparatively less attention.
Brockett etal.
(2006) use an SMT system to correct errors in-volving mass noun errors.
Because no large anno-tated learner corpus was available, the training datawas created artificially from non-learner text.
Leeand Seneff (2006) describe a lattice-based correc-tion systemwith a domain-specific grammar for spo-ken utterances from the flight domain.
The work in(De?silets and Hermet, 2009) uses simple round-triptranslation with a standard SMT system to correctgrammatical errors.
Dahlmeier and Ng (2011a) cor-rect collocation errors using phrase-based SMT andparaphrases induced from the writer?s native lan-guage.
Park and Levy (2011) propose a noisy chan-nel model for error correction.
While their motiva-tion to correct whole sentences is similar to ours,their proposed generative method differs substan-tially from our discriminative decoder.
Park andLevy?s model does not allow the use of discrim-inative expert classifiers as our decoder does, butinstead relies on a bigram language model to findgrammatical corrections.
Indeed, they point out thatthe language model often fails to distinguish gram-matical and ungrammatical sentences.To the best of our knowledge, our work is the firstdiscriminatively trained decoder for whole-sentencegrammatical error correction.3 DecoderIn this section, we describe the proposed beam-search decoder and its components.The task of the decoder is to find the best hypoth-esis (i.e., the best corrected sentence) for a given in-put sentence.
To accomplish this, the decoder needsto be able to perform two tasks: generating newhypotheses from current ones, and discriminatinggood hypotheses from bad ones.
This is achievedby two groups of modules which we call proposersand experts, respectively.
Proposers take a hypothe-sis and generate a set of new hypotheses, where eachnew hypothesis is the result of making an incremen-tal change to the current hypothesis.
Experts scorehypotheses on particular aspects of grammaticality.This can be a general language model score, or theoutput of classifiers for particular error categories,for example for article and preposition usage.
Theoverall score for a hypothesis is a linear combina-569tion of the expert scores.
Note that in our decoder,each hypothesis corresponds to a complete sentence.This makes it easy to apply syntactic processing,like POS tagging, chunking, and dependency pars-ing, which provides necessary features for the expertmodels.
The highest scoring hypotheses are kept inthe search beam for the next iteration.
The searchends when the beam is empty or the maximum num-ber of iterations has been reached.
The highest scor-ing hypothesis found during the search is returned asthe sentence-level correction.
The modular designof the decoder makes it easy to extend the modelto new error categories by adding specific proposersand experts without having to change the decodingalgorithm.3.1 ProposersThe proposers generate new hypotheses, given a hy-pothesis.
Because the number of possible hypothe-ses grows exponentially with the sentence length,enumerating all possible hypotheses is infeasible.Instead, each proposer only makes a small incre-mental change to the hypothesis in each iteration.
Achange corresponds to a correction of a single wordor phrase.
We experiment with the following pro-posers in this work.
Additional proposers for othererror categories can easily be added to the decoder.?
Spelling Generate a set of new hypotheses, byreplacing a misspelled word with each correc-tion proposed by a spellchecker.?
Articles For each noun phrase (NP), generatetwo new hypotheses by changing the observedarticle.
Possible article choices are a/an, the,and the empty article .?
Prepositions For each prepositionalphrase (PP), generate a set of new hy-potheses by changing the observed preposition.For each preposition, we define a confusion setof possible corrections.?
Punctuation insertion Insert commas, peri-ods, and hyphens based on a set of simple rules.?
Noun number For each noun, change its num-ber from singular to plural or vice versa.3.2 ExpertsThe experts score hypotheses on particular aspectsof grammaticality to help the decoder to discrim-inate grammatical hypotheses from ungrammaticalones.
We employ two types of expert models.
Thefirst type of expert model is a standard N-gram lan-guage model.
The language model expert is not spe-cialized for any particular type of error.
The secondtype of experts is based on linear classifiers and isspecialized for particular error categories.
We usethe following classifier experts in our work.
The fea-tures for the classifier expert models include featuresfrom N-grams, part-of-speech (POS) tags, chunks,web-scale N-gram counts, and dependency parsetrees.
Additional experts can easily be added to thedecoder.?
Article expert Predict the correct article for anoun phrase.?
Preposition expert Predict the correct preposi-tion for a prepositional phrase.?
Noun number expert Predict whether a nounshould be in the singular or plural form.The outputs of the experts are used as hypothesisfeatures in the decoder, as described in the next sec-tion.3.3 Hypothesis FeaturesEach hypothesis is associated with a vector of real-valued features which are indicators of grammatical-ity and are computed from the output of the expertmodels.
We call these features hypothesis featuresto distinguish them from the features of the expertclassifiers.
The simplest hypothesis feature is thelog probability of the hypothesis under the N-gramlanguage model expert.
To avoid a bias towardsshorter hypotheses, we normalize the probability bythe length of the hypothesis:scorelm =1|h|logPr(h), (1)where h is a hypothesis sentence and |h| is the hy-pothesis length in tokens.For the classifier-based experts, we define twotypes of features.
The first is the average score of570the hypothesis under the expert model:scoreavg =1nn?i=1(uT f(xhi , yhi )), (2)where u is the expert classifier weight vector, xhi andyhi are the feature vector and the observed class, re-spectively, for the i-th instance extracted from thehypothesis h (e.g., the i-th NP in the hypothesisfor the article expert), and f is a feature map thatcomputes the expert classifier features.
The averagescore reflects how much the expert model ?likes?
thehypothesis.
The second expert score, which we calldelta score, is the maximum difference between thehighest scoring class and the observed class in anyinstance from the hypothesis:scoredelta = maxi,y(uT f(xhi , y) ?
uT f(xhi , yhi )).
(3)Generally speaking, the delta score measures howmuch the model ?disagrees?
with the hypothesis.Finally, each hypothesis has a number of correc-tion count features that keep track of how many cor-rections have been made to the hypothesis so far.
Forexample, there is a feature that counts how often thearticle correction  ?
the has been applied.
We alsoadd aggregated correction count features for eacherror category, e.g., how many article correctionshave been applied in total.
The correction count fea-tures allow the decoder to learn a bias against over-correcting sentences and to learn which types of cor-rections are more likely and which are less likely.3.4 Decoder ModelThe hypothesis features described in the previoussubsection are combined to compute the score of ahypothesis according to the following linear model:s = wT fE(h), (4)where w is the decoder model weight vector andfE is a feature map that computes the hypothesisfeatures described above, given a set of experts E.The weight vector w is tuned on a development setof error-annotated sentences using the PRO rankingoptimization algorithm (Hopkins and May, 2011).11We also experimented with the MERT algorithm (Och,2003) but found that PRO achieved better results.PRO performs decoder parameter tuning through apair-wise ranking approach.
The algorithm starts bysampling hypothesis pairs from the N-best list of thedecoder.
The metric score for each hypothesis in-duces a ranking of the two hypotheses in each pair.The task of finding a weight vector that correctlyranks hypotheses can then be reduced to a simple bi-nary classification task.
In this work, we use PRO tooptimize the F1 correction score, which is defined inSection 4.2.
PRO requires a sentence-level score foreach hypothesis.
As F1 score is not decomposable,we optimize sentence-level F1 score which servesas an approximation of the corpus-level F1 score.Similarly, Hopkins and May optimize a sentence-level BLEU approximation (Lin and Och, 2004) in-stead of the corpus-level BLEU score (Papineni etal., 2002).
We observed that optimizing sentence-level F1 score worked well in practice in our experi-ments.3.5 Decoder SearchGiven a set of proposers, experts, and a tuned de-coder model, the decoder can be used to correctnew unseen sentences.
This is done by performinga search over possible hypothesis candidates.
Thedecoder starts with the input sentence as the initialhypothesis, i.e., assuming that all words are correct.It then performs a beam search over the space ofpossible hypotheses to find the best hypothesis cor-rection h?
for an input sentence e. The search pro-ceeds in iterations until the beam is empty or themaximum number of iterations has been reached.
Ineach iteration, the decoder takes each hypothesis inthe beam and generates new hypothesis candidatesusing all the available proposers.
The hypothesesare evaluated by the expert models that compute thehypothesis features and finally scored using the de-coder model.
As the search space grows exponen-tially, it is infeasible to perform exhaustive search.Therefore, we prune the search space by only ac-cepting the most promising hypotheses to the poolof hypotheses for future consideration.
If a hypothe-sis has a higher score than the best hypothesis foundin previous iterations, it is definitely added to thepool.
Otherwise, we use a simulated annealing strat-egy where hypotheses with a lower score can still beaccepted with a certain probability which dependson the difference between the hypothesis score and571hand?
handsIn?
ForOn?
Abouthands?
handIn?
AtIn?
IntoOn?
ByIn?
OfIn?
AtIn?
Forthe?
an In?
Onthe?
anIn?
For?
anIn?
AtOn?
ToIn?Withthe?
In?With?
theIn?WithTo the otherhand ..score = 6.32About the otherhand ..score = 9.71For other hands ..score = 7.00On an other hand..score = 2.48On other hand ..score = 4.94...At other hands ..score = 5.34At the otherhands ..score = 6.05For the otherhands ..score = 9.05With the otherhands ..score = 5.25In the other hand , they might be rightscore = 11.69......In other hands , they might be right .score = 9.10Into the otherhand ..score = 5.47For the otherhand ..score = 10.75At the otherhand ..score = 9.40In an other hand..score = 3.96In the other hands , they might be right .score = 9.63In an other ..score = -1.58On the other hand , they might be rightscore = 15.36Of the otherhand ..score = 8.94In other hand ..score = 8.29...With other hands ..score = 6.31By the otherhand ..score = 5.80With the otherhand ..score = 8.69Figure 1: Example of a search tree produced by the beam-search decoder for the input In other hands, they might beright.
The highest scoring hypothesis found is On the other hand, they might be right.
Some hypotheses are omitteddue to space constraints.the score of the best hypothesis and the ?tempera-ture?
of the system.
We lower the temperature aftereach iteration according to an exponential coolingschedule.
Hypotheses that have been explored be-fore are not considered again to avoid cycles in thesearch.
From all hypotheses in the pool, we selectthe top k hypotheses and add them to the beam forthe next search iteration.
The decoding algorithmis shown in Algorithm 1.
The decoder can be con-sidered an anytime algorithm (Russell and Norvig,2010), as it has a current best hypothesis correctionavailable at any point of the search, while graduallyimproving the result by searching for better hypothe-ses.
An example of a search tree produced by ourdecoder is shown in Figure 1.The decoding algorithm shares some similaritieswith the beam-search algorithm frequently used inSMT.
There is however a difference between SMTdecoding and grammar correction decoding that isworth pointing out.
In SMT decoding, every inputword needs to be translated exactly once.
In con-trast, in grammar correction decoding, the majorityof the words typically do not need any correction(in the HOO data, for example, there are on aver-age 6 errors per 100 words).
On the other hand,some words might require multiple corrections, forexample spelling correction followed by noun num-ber correction.
Errors can also be inter-dependent,where correcting one word makes it necessary tochange another word, for example to preserve agree-ment.
Our decoding algorithm has the option to cor-rect some words multiple times, while leaving otherwords unchanged.4 ExperimentsWe evaluate our decoder in the context of the HOOshared task on grammatical error correction.
Thegoal of the task is to automatically correct errors inacademic papers from NLP.
The readers can refer tothe overview paper (Dale and Kilgarriff, 2011) fordetails.
We compare our proposed method with twobaselines: a phrase-based SMT system (described inSection 4.3) and a pipeline of classifiers (describedin Section 4.4).4.1 DataWe split the HOO development data into an equalsized training (HOO-TRAIN) and tuning (HOO-TUNE) set.
The official HOO test data (HOO-TEST)is used for evaluation.
In the HOO shared task, par-ticipants were allowed to raise objections regardingthe gold-standard annotations (corrections) of thetest data after the test data was released.
As a result,the gold-standard annotations could be biased in fa-572Algorithm 1 The beam-search decoding algorithm.
e:original sentence, w: decoder weight vector, P : set ofproposers, E: set of experts, k: beam width, M : maxi-mum number of iterations, T, c: initial temperature andcooling schedule for simulated annealing (0 < c < 1).procedure decode(e, w, P , E, k, M )1: beam?
{e}2: previous?
{e}3: hbest ?
e4: sbest ?
wT fE(hbest)5: i?
06: while beam 6= ?
?
i < M do7: pool?
{}8: for all h ?
beam do9: for all p ?
P do10: for all h?
?
p.propose(h) do11: if h?
?
previous then12: continue13: previous?
previous ?
{h?
}14: sh?
?
wT fE(h?
)15: if accept(sh?
, sbest, T ) then16: pool?
pool ?
{(h?, sh?
)}17: beam?
?18: for all (h, sh) ?
nbest(pool, k) do19: beam?
beam ?
{h}20: if sh > sbest then21: hbest ?
h22: sbest ?
sh23: T ?
T ?
c24: i?
i + 125: return hbestprocedure accept(sh, sbest, T )1: ?
?
sh ?
sbest2: if ?
> 0 then3: return true4: if exp( ?T ) > random() then5: return true else return falsevor of specific systems participating in the sharedtask.
We obtain both the original and the final offi-cial gold-standard annotations and report evaluationresults on both annotations.We use the ACL Anthology2 as training data forthe expert models.
We crawl all non-OCR docu-ments from the anthology, except those documentsthat overlap with the HOO data.
Section headers,references, etc.
are automatically removed.
TheWeb 1T 5-gram corpus (Brants and Franz, 2006) isused for language modeling and collecting web N-gram counts.
Table 1 gives an overview of the datasets.2http://www.aclweb.org/anthology-new/Data Set Sentences TokensHOO-TRAIN 467 11,373HOO-TUNE 472 11,435HOO-TEST 722 18,790ACL-ANTHOLOGY 943,965 22,465,690Table 1: Overview of the data sets.4.2 EvaluationWe evaluate performance by computing precision,recall, and F1 correction score without bonus as de-fined in the official HOO report (Dale and Kilgar-riff, 2011)3.
F1 correction score is simply the F1measure (van Rijsbergen, 1979) between the correc-tions (called edits in HOO) proposed by a systemand the gold-standard corrections.
Let {e1, .
.
.
, en}be a set of test sentences and let {g1, .
.
.
,gn} bethe set of gold-standard edits for the sentences.
Let{h1, .
.
.
,hn} be the set of corrected sentences out-put by a system.
One difficulty in the evaluation isthat the set of system edits {d1, .
.
.
,dn} betweenthe test sentences and the system outputs is ambigu-ous.
For example, assume that the original test sen-tence is The data is similar with test set., the systemoutput is The data is similar to the test set., and thegold-standard edits are two corrections with ?
to, ?
the that change with to to and insert the be-fore test set.
The official HOO scorer however ex-tracts a single system edit with ?
to the for thisinstance.
As the extracted system edit is differentfrom the gold-standard edits, the system would beconsidered wrong, although it proposes the exactsame corrected sentence as the gold standard ed-its.
This problem has also been recognized by theHOO shared task organizers (see (Dale and Kilgar-riff, 2011), Section 5).Our MaxMatch (M2) scorer (Dahlmeier and Ng,2012) overcomes this problem through an efficientalgorithm that computes the set of system editswhich has the maximum overlap with the gold-standard edits.
We use the M2 scorer as the mainevaluation metric in our experiments.
Additionally,we also report results with the official HOO scorer.Once the set of system edits is extracted, precision,recall, and F1 measure are computed as follows.3?Without bonus?
means that a system does not receive extracredit for not making corrections that are considered optional inthe gold standard.573P =?ni=1 |di ?
gi|?ni=1 |di|(5)R =?ni=1 |di ?
gi|?ni=1 |gi|(6)F1 = 2 ?P ?RP + R(7)We note that the M2 scorer and the HOO scorer ad-here to the same score definition and only differ inthe way the system edits are computed.
For statisti-cal significance testing, we use sign-test with boot-strap re-sampling (Koehn, 2004) with 1,000 sam-ples.4.3 SMT BaselineWe build a baseline error correction system, usingthe MOSES SMT system (Koehn et al2007).
Wordalignments are created automatically on ?good-bad?parallel text from HOO-TRAIN using GIZA++ (Ochand Ney, 2003), followed by phrase extraction us-ing the standard heuristic (Koehn et al2003).
Themaximum phrase length is 5.
Parameter tuning isdone on the HOO-TUNE data with the PRO al-gorithm (Hopkins and May, 2011) implemented inMOSES.
The optimization objective is sentence-level BLEU (Lin and Och, 2004).
We note that theobjective function is not the same as the final evalu-ation F1 score.
Also, the training and tuning data aresmall by SMT standards.
The aim for the SMT base-line is not to achieve a state-of-the-art system, but toserve as the simplest possible baseline that uses onlyoff-the-shelf software.4.4 Pipeline BaselineThe second baseline system is a pipeline ofclassifier-based and rule-based correction steps.Each step takes sentence segmented plain text as in-put, corrects one particular error category, and feedsthe corrected text into the next step.
No search orglobal inference is applied.
The correction steps are:1.
Spelling errors2.
Article errors3.
Preposition errors4.
Punctuation errors5.
Noun number errorsWe use the following tools for syntactic process-ing: OpenNLP4 for POS tagging, YamCha (KudoandMatsumoto, 2003) for constituent chunking, andthe MALT parser (Nivre et al2007) for depen-dency parsing.
For language modeling, we use Ran-dLM (Talbot and Osborne, 2007).For spelling correction, we use GNU Aspell5.Words that contain upper-case characters inside theword or are shorter than four characters are excludedfrom spell checking.
The spelling dictionary is aug-mented with all words that appear at least 10 timesin the ACL-ANTHOLOGY data set.Article correction is cast as a multi-class clas-sification problem.
As the learning algorithm,we choose multi-class confidence-weighted (CW)learning (Crammer et al2009) which has beenshown to perform well for NLP problems with highdimensional and sparse feature spaces.
The possi-ble classes are the articles a, the, and the empty ar-ticle .
The article an is normalized as a and re-stored later using a rule-based heuristic.
We con-sider all NPs that are not pronouns and do not have anon-article determiner, e.g., this, that.
The classifieris trained on over 5 million instances from ACL-ANTHOLOGY.
We use a combination of featuresproposed by (Rozovskaya et al2011) (which in-clude lexical and POS N-grams, lexical head words,etc.
), web-scale N-gram count features from theWeb 1T 5-gram corpus following (Bergsma et al2009), and dependency head and child features.During testing, a correction is proposed if the pre-dicted article is different from the observed articleused by the writer, and the difference between theconfidence score for the predicted article and theconfidence score for the observed article is largerthan a threshold.
Threshold parameters are tunedvia a grid-search on the HOO-TUNE data.
We tunea separate threshold value for each class.Preposition correction and noun number correc-tion are analogous to article correction.
They differonly in terms of the classes and the features.
Forpreposition correction, the classes are 36 frequentEnglish prepositions6.
The features are surrounding4http://opennlp.sourceforge.net5http://aspell.net6about, along, among, around, as, at, beside, besides, be-tween, by, down, during, except, for, from, in, inside, into, of,off, on, onto, outside, over, through, to, toward, towards, under,574lexical N-grams, web-scale N-gram counts, and de-pendency features following (Tetreault et al2010).The preposition classifier is trained on 1 milliontraining examples from the ACL-ANTHOLOGY.
Fornoun number correction, the classes are singularand plural.
The features are lexical N-grams, web-scale N-gram counts, dependency features, the nounlemma, and a binary countability feature.
The nounnumber classifier is trained on over 5 million exam-ples from ACL-ANTHOLOGY.
During testing, thesingular or plural word surface form is generated us-ing WordNet (Fellbaum, 1998) and simple heuris-tics.
Punctuation correction is done using a set ofsimple rules developed on the HOO developmentdata.At the end of every correction step, all proposedcorrections are filtered using a 5-gram languagemodel from the Web 1T 5-gram corpus and onlycorrections that strictly increase the normalized lan-guage model score of the sentence are applied.4.5 DecoderWe experiment with different decoder configura-tions with different proposers and expert models.In the simplest configuration, the decoder only hasthe spelling proposer and the language model ex-pert.
We then add the article proposer and expert,the preposition proposer and expert, the punctua-tion proposer, and finally the noun number proposerand expert.
We refer to the final configuration withall proposers and experts as the full decoder model.Note that error categories are corrected jointly andnot in sequential steps as in the pipeline.To make the results directly comparable to thepipeline, the decoder uses the same resources as thepipeline.
As the expert models, we use a 5-gramlanguage model from the Web 1T 5-gram corpuswith the Berkeley LM (Pauls and Klein, 2011)7 inthe decoder and the CW-classifiers described in thelast section.
The spelling proposer uses the samespellchecker as the pipeline, and the punctuationproposer uses the same rules as the pipeline.
Thebeam width and the maximum number of iterationsare set to 10.
In earlier experiments, we found thatlarger values had no effect on the result.
The simu-underneath, until, up, upon, with, within, without7Berkeley LM is written in Java and was easier to integrateinto our Java-based decoder than RandLM.lated annealing temperature T is initialized to 10 andthe exponential cooling schedule c is set to 0.9.
Thedecoder weight vector is initialized as follows.
Theweight for the language model score and the weightsfor the classifier expert average scores are initializedto 1.0, and the weights for the classifier expert deltascores are initialized to ?1.0.
The weights for thecorrection count features are initialized to zero.
ForPRO optimization, we use the HOO-TUNE data andthe default PRO parameters from (Hopkins andMay,2011): we sample 5,000 hypothesis pairs from theN-best list (N = 100) for every input sentence andkeep the top 50 sample pairs with the highest dif-ference in F1 measure.
The weights are optimizedusing MegaM (Daume?
III, 2004) and interpolatedwith the previous weight vector with an interpola-tion parameter of 0.1.
We normalize feature val-ues to avoid having features on a larger scale dom-inate features on a smaller scale.
We linearly scaleall hypothesis features to a unit interval [0, 1].
Theminimum and maximum values for each feature areestimated from the development data.
We use anearly stopping criterion that terminates PRO if theobjective function on the tuning data drops.
To bal-ance the skewed data where samples without errorsgreatly outnumber samples with errors, we give ahigher weight to sample pairs where the decoderproposed a valid correction.
We found a weight of20 to work well, based on initial experiments onthe HOO-TUNE data.
We keep all these parametersfixed for all experiments.5 ResultsThe complete results of our experiments are shownin Table 2.
Each row contains the results for oneerror correction system.
Each system is scored onthe original and official gold-standard annotations,both with the M2 scorer and the official HOO scorer.This results in four sets of precision, recall, and F1scores for each system.
The best published resultto date on this data set is the UI Run1 system fromthe HOO shared task.
We include their system as areference point.We make the following observations.
First, thescores on the official gold-standard annotations arehigher compared to the original gold-standard an-notations.
We note that the gap between the two575System Original gold-standard Official gold-standardM2 scorer HOO scorer M2 scorer HOO scorerP R F1 P R F1 P R F1 P R F1UI Run1 40.86 11.21 17.59 38.13 10.42 16.37 54.61 14.57 23.00 50.72 13.34 21.12P R F1 P R F1 P R F1 P R F1SMT 9.84 7.77 8.68 15.25 5.31 7.87 23.35 7.38 11.21 15.82 5.30 7.93Pipeline P R F1 P R F1 P R F1 P R F1Spelling 50.00 0.79 1.55 40.00 0.64 1.25 50.00 0.76 1.49 40.00 0.61 1.20+ Articles 30.86 10.23 15.36 28.04 9.55 14.25 34.42 10.97 16.64 31.78 10.41 15.68+ Prepositions 27.44 11.90 16.60 24.82 11.15 15.38 30.54 12.77 18.01 27.90 12.04 16.82+ Punctuation 28.91 14.55 19.36 ?
26.57 13.91 18.25 ?
32.88 15.99 21.51 30.63 15.41 20.50+ Noun number 28.77 16.13 20.67 ?
24.68 14.22 18.04 ?
32.34 17.50 22.71 28.36 15.71 20.22Decoder P R F1 P R F1 P R F1 P R F1Spelling 36.84 0.69 1.35 22.22 0.41 0.80 36.84 0.66 1.30 22.22 0.42 0.83+ Articles 19.84 12.59 15.40 17.99 12.00 14.39 22.45 13.72 17.03 ?
20.70 13.27 16.16+ Prepositions 22.62 14.26 17.49 ?
19.30 12.95 15.50 24.84 15.14 18.81 ?
21.36 13.78 16.74+ Punctuation 24.27 18.09 20.73 ??
20.40 16.24 18.08 27.13 19.58 22.75 ?
23.07 17.65 19.99+ Noun number 30.28 19.17 23.48 ??
24.29 16.24 19.46 ??
33.59 20.53 25.48 ??
27.30 17.55 21.36 ?Table 2: Experimental results on HOO-TEST.
Precision, recall, and F1 score are shown in percent.
The best F1 scorefor each system is highlighted in bold.
Statistically significant improvements (p < 0.01) over the pipeline baseline aremarked with an asterisk (?).
Statistically significant improvements over the UI Run1 system are marked with a dagger(?).
All improvements of the pipeline and the decoder over the SMT baseline are statistically significant.annotations is the largest for the UI Run1 systemwhich confirms the suspected bias of the officialgold-standard annotations in favor of participatingsystems.
Second, the scores computed with the M2scorer are higher than the scores computed with theofficial HOO scorer.
With more error categories andmore ambiguity in the edits segmentation, the gapbetween the scorers widens.
In the case of the fullpipeline and decoder model, the HOO scorer evenshows a decrease in F1 score when the score actu-ally goes up as shown by the M2 scorer.
We there-fore focus on the scores of the M2 scorer from nowon.
The SMT baseline achieves 8.68% and 11.21%F1 on the original and official gold standard, respec-tively.
Although the worst system in our experi-ments, it would still have claimed the third placein the HOO shared task.
One problem is certainlythe small amount of training data.
Another reason isthat the phrase-based model is unaware of syntacticstructure and cannot express correction rules of theform NP ?
the NP .
Instead, it has to have seenthe exact correction rule, e.g., house ?
the house,in the training data.
As a result, the model does notgeneralize well.
The pipeline achieves state-of-the-art results.
Each additional correction step improvesthe score.
Our proposed decoder achieves the bestresult.
When only a few error categories are cor-rected, the pipeline and the decoder are close to eachother.
When more error categories are added, thegap between the pipeline and the decoder becomeslarger.
The full decoder model achieves an F1 scoreof 23.48% and 25.48% on the original and officialgold standard, respectively, which is statistically sig-nificantly better than both the pipeline system andthe UI Run1 system.6 DiscussionAs pointed out in Section 3.5, the majority of sen-tences require zero or few corrections.
Therefore,the depth of the search tree is typically small.
In ourexperiments, the average depth of the search tree isonly 1.9 (i.e., 0.9 corrections per sentence) on thetest set.
Usually, the search depth will be one largerthan the number of corrections made, since the de-coder will explore the next level of the search treebefore deciding that none of the new hypotheses arebetter than the current best one.
On the other hand,there are many possible hypotheses that can be pro-posed for any sentence.
The breadth of the searchtree is therefore quite large.
In our experiments, thedecoder explored on average 99 hypotheses per sen-tence on the test set.576PRO iteration P R F11 14.13 20.17 16.622 19.71 20.85 20.273 23.12 21.03 22.024 24.35 20.85 22.475 25.53 20.51 22.756 26.27 20.34 22.937 27.25 20.68 23.528 26.73 19.83 22.77Table 3: PRO tuning of the full decoder model on HOO-TUNEFeature Weighta?
the -1.3660a?
 0.5253the?
a -0.9997the?
 0.0532?
a 0.0694?
the -0.0529Table 4: Example of PRO-tuned weights for article cor-rection count features for the full decoder model.We found that PRO tuning is very important toachieve good performance for our decoder.
Mostimportantly, PRO tunes the correction count featuresthat bias the decoder against over-correcting sen-tences thus improving precision.
But PRO is alsoable to improve recall during tuning.
Table 3 showsthe trajectory of the performance for the full decodermodel during PRO tuning on HOO-TUNE.
AfterPRO tuning has converged, we inspect the learnedweight vector and observe some interpretable pat-terns learned by PRO.
First, the language modelscore and all classifier expert average scores receivepositive weights, while all classifier expert deltascores receive negative weights, in line with our ini-tial intuition described in Section 3.3.
Second, mostcorrection count features receive negative weights,thus acting as a bias against correction if it is notnecessary.
Finally, the correction count features re-veal which corrections are more likely and which areless likely.
For example, article replacement errorsare less common in the HOO-TUNE data than arti-cle insertions or deletions.
The weights learned forthe article correction count features shown in Table 4reflect this.Although our decoder achieves state-of-the-art re-sults, there remain many error categories which thedecoder currently cannot correct.
This includes, forexample, verb form errors (Much research (have ?has) been put into .
.
. )
and lexical choice errors (The(concerned ?
relevant) relation .
.
.
).
We believethat our decoder provides a promising framework tobuild grammatical error correction systems that in-clude these types of errors in the future.7 ConclusionWe have presented a novel beam-search decoder forgrammatical error correction.
The model performsend-to-end correction of whole sentences with mul-tiple, interacting errors, is discriminatively trained,and incorporates existing classifier-based models forerror correction.
Our decoder achieves an F1 correc-tion score of 25.48% on the HOO shared task whichoutperforms the current state of the art on this dataset.AcknowledgmentsThis research is supported by the Singapore Na-tional Research Foundation under its InternationalResearch Centre @ Singapore Funding Initiativeand administered by the IDM Programme Office.ReferencesS.
Bergsma, D. Lin, and R. Goebel.
2009.
Web-scale N-gram models for lexical disambiguation.
In Proceed-ings of IJCAI.T.
Brants and A. Franz.
2006.
Web 1T 5-gram corpusversion 1.1.
Technical report, Google Research.C.
Brockett, W. B. Dolan, and M. Gamon.
2006.
Cor-recting ESL errors using phrasal SMT techniques.
InProceedings of COLING-ACL.M.
Chodorow, J. Tetreault, and N.R.
Han.
2007.
De-tection of grammatical errors involving prepositions.In Proceedings of the 4th ACL-SIGSEM Workshop onPrepositions.K.
Crammer, M. Dredze, and A. Kulesza.
2009.
Multi-class confidence weighted algorithms.
In Proceedingsof EMNLP.D.
Dahlmeier and H.T.
Ng.
2011a.
Correcting seman-tic collocation errors with L1-induced paraphrases.
InProceedings of EMNLP.D.
Dahlmeier and H.T.
Ng.
2011b.
Grammatical errorcorrection with alternating structure optimization.
InProceedings of ACL.D.
Dahlmeier and H.T.
Ng.
2012.
Better evaluation forgrammatical error correction.
In Proceedings of HLT-NAACL.577R.
Dale and A. Kilgarriff.
2011.
Helping Our Own: TheHOO 2011 pilot shared task.
In Proceedings of the2011 European Workshop on Natural Language Gen-eration.H.
Daume?
III.
2004.
Notes on CG and LM-BFGSoptimization of logistic regression.
Paper available athttp://pub.hal3.name#daume04cg-bfgs,implementation available at http://hal3.name/megam/.A.
De?silets and M. Hermet.
2009.
Using automaticroundtrip translation to repair general errors in secondlanguage writing.
In Proceedings of MT-Summit XII.C.
Fellbaum, editor.
1998.
WordNet: An electronic lexi-cal database.
MIT Press, Cambridge,MA.M.
Gamon.
2010.
Using mostly native data to correcterrors in learners?
writing: A meta-classifier approach.In Proceedings of HLT-NAACL.N.R.
Han, M. Chodorow, and C. Leacock.
2006.
De-tecting errors in English article usage by non-nativespeakers.
Natural Language Engineering, 12(02).M.
Hopkins and J.
May.
2011.
Tuning as ranking.
InProceedings of EMNLP.K.
Knight and I. Chander.
1994.
Automated posteditingof documents.
In Proceedings of AAAI.P.
Koehn, F.J. Och, and D. Marcu.
2003.
Statisti-cal phrase-based translation.
In Proceedings of HLT-NAACL.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,and E. Herbst.
2007.
Moses: Open source toolkit forstatistical machine translation.
In Proceedings of ACLDemonstration Session.P.
Koehn.
2004.
Statistical significance tests for machinetranslation evaluation.
In Proceedings of EMNLP.T Kudo and Y. Matsumoto.
2003.
Fast methods forkernel-based text analysis.
In Proceedings of ACL.C.
Leacock, M. Chodorow, M. Gamon, and J. Tetreault.2010.
Automated Grammatical Error Detection forLanguage Learners.
Morgan & Claypool Publishers.J.
Lee and S. Seneff.
2006.
Automatic grammar correc-tion for second-language learners.
In Proceedings ofInterspeech.C.-Y.
Lin and F.J. Och.
2004.
ORANGE: a method forevaluating automatic evaluation metrics for machinetranslation.
In Proceedings of COLING.J.
Nivre, J.
Hall, J. Nilsson, A. Chanev, G. Eryigit,S.
Ku?bler, S. Marinov, and M. Marsi.
2007.
Malt-Parser: A language-independent system for data-driven dependency parsing.
Natural Language Engi-neering, 13.F.J.
Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
ComputationalLinguistics, 29(1).F.J.
Och.
2003.
Minimum error rate training in statisticalmachine translation.
In Proceedings of ACL.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2002.BLEU: A method for automatic evaluation of machinetranslation.
In Proceedings of ACL.Y.
A.
Park and R. Levy.
2011.
Automated wholesentence grammar correction using a noisy channelmodel.
In Proceedings of ACL.A.
Pauls and D. Klein.
2011.
Faster and smaller N-gramlanguage models.
In Proceedings of ACL-HLT.A.
Rozovskaya and D. Roth.
2011.
Algorithm selec-tion and model adaptation for ESL correction tasks.
InProceedings of ACL-HLT.A.
Rozovskaya, M. Sammons, J. Gioja, and D. Roth.2011.
University of Illinois system in HOO text cor-rection shared task.
In Proceedings of the GenerationChallenges Session at the 13th European Workshop onNatural Language Generation.S.
Russell and P. Norvig, 2010.
Artificial Intelligence: AModern Approach, chapter 27.
Prentice Hall.D.
Talbot and M. Osborne.
2007.
Randomised languagemodelling for statistical machine translation.
In Pro-ceedings of ACL.J.
Tetreault and M. Chodorow.
2008.
The ups and downsof preposition error detection in ESL writing.
In Pro-ceedings of COLING.J.
Tetreault, J.
Foster, and M. Chodorow.
2010.
Usingparse features for preposition selection and error de-tection.
In Proceedings of ACL.C.
J. van Rijsbergen.
1979.
Information Retrieval.
But-terworth, 2nd edition.578
