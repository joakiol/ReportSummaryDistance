Proceedings of ACL-08: HLT, pages 1012?1020,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsGeneralizing Word Lattice TranslationChristopher Dyer?, Smaranda Muresan, Philip Resnik?Laboratory for Computational Linguistics and Information ProcessingInstitute for Advanced Computer Studies?Department of LinguisticsUniversity of MarylandCollege Park, MD 20742, USAredpony, smara, resnik AT umd.eduAbstractWord lattice decoding has proven useful inspoken language translation; we argue that itprovides a compelling model for translation oftext genres, as well.
We show that prior workin translating lattices using finite state tech-niques can be naturally extended to more ex-pressive synchronous context-free grammar-based models.
Additionally, we resolve asignificant complication that non-linear wordlattice inputs introduce in reordering mod-els.
Our experiments evaluating the approachdemonstrate substantial gains for Chinese-English and Arabic-English translation.1 IntroductionWhen Brown and colleagues introduced statisticalmachine translation in the early 1990s, their key in-sight ?
harkening back to Weaver in the late 1940s ?was that translation could be viewed as an instanceof noisy channel modeling (Brown et al, 1990).They introduced a now standard decomposition thatdistinguishes modeling sentences in the target lan-guage (language models) from modeling the rela-tionship between source and target language (trans-lation models).
Today, virtually all statistical trans-lation systems seek the best hypothesis e for a giveninput f in the source language, according toe?
= arg maxePr(e|f) (1)An exception is the translation of speech recogni-tion output, where the acoustic signal generally un-derdetermines the choice of source word sequencef .
There, Bertoldi and others have recently foundthat, rather than translating a single-best transcrip-tion f , it is advantageous to allow the MT decoder toconsider all possibilities for f by encoding the alter-natives compactly as a confusion network or lattice(Bertoldi et al, 2007; Bertoldi and Federico, 2005;Koehn et al, 2007).Why, however, should this advantage be limitedto translation from spoken input?
Even for text,there are often multiple ways to derive a sequenceof words from the input string.
Segmentation ofChinese, decompounding in German, morpholog-ical analysis for Arabic ?
across a wide rangeof source languages, ambiguity in the input givesrise to multiple possibilities for the source word se-quence.
Nonetheless, state-of-the-art systems com-monly identify a single analysis f during a prepro-cessing step, and decode according to the decisionrule in (1).In this paper, we go beyond speech translationby showing that lattice decoding can also yield im-provements for text by preserving alternative anal-yses of the input.
In addition, we generalize latticedecoding algorithmically, extending it for the firsttime to hierarchical phrase-based translation (Chi-ang, 2005; Chiang, 2007).Formally, the approach we take can be thought ofas a ?noisier channel?, where an observed signal ogives rise to a set of source-language strings f ?
?F(o) and we seeke?
= arg maxemaxf ?
?F(o)Pr(e, f ?|o) (2)= arg maxemaxf ?
?F(o)Pr(e)Pr(f ?|e, o) (3)= arg maxemaxf ?
?F(o)Pr(e)Pr(f ?|e)Pr(o|f ?).
(4)Following Och and Ney (2002), we use the maxi-mum entropy framework (Berger et al, 1996) to di-rectly model the posterior Pr(e, f ?|o) with parame-ters tuned to minimize a loss function representing1012the quality only of the resulting translations.
Thus,we make use of the following general decision rule:e?
= arg maxemaxf ?
?F(o)M?m=1?m?m(e, f?, o) (5)In principle, one could decode according to (2)simply by enumerating and decoding each f ?
?F(o); however, for any interestingly large F(o) thiswill be impractical.
We assume that for many in-teresting cases of F(o), there will be identical sub-strings that express the same content, and thereforea lattice representation is appropriate.In Section 2, we discuss decoding with this modelin general, and then show how two classes of trans-lation models can easily be adapted for lattice trans-lation; we achieve a unified treatment of finite-stateand hierarchical phrase-based models by treatinglattices as a subcase of weighted finite state au-tomata (FSAs).
In Section 3, we identify and solveissues that arise with reordering in non-linear FSAs,i.e.
FSAs where every path does not pass throughevery node.
Section 4 presents two applications ofthe noisier channel paradigm, demonstrating sub-stantial performance gains in Arabic-English andChinese-English translation.
In Section 5 we discussrelevant prior work, and we conclude in Section 6.2 DecodingMost statistical machine translation systems modeltranslational equivalence using either finite statetransducers or synchronous context free grammars(Lopez, to appear 2008).
In this section we discussthe issues associated with adapting decoders fromboth classes of formalism to process word lattices.The first decoder we present is a SCFG-based de-coder similar to the one described in Chiang (2007).The second is a phrase-based decoder implementingthe model of Koehn et al (2003).2.1 Word latticesA word lattice G = ?V,E?
is a directed acyclicgraph that formally is a weighted finite state automa-ton (FSA).
We further stipulate that exactly one nodehas no outgoing edges and is designated the ?endnode?.
Figure 1 illustrates three classes of wordlattices.01x 2ay3bc0 1ax?2b 3dc0 1a 2b 3cFigure 1: Three examples of word lattices: (a) sentence,(b) confusion network, and (c) non-linear word lattice.A word lattice is useful for our purposes becauseit permits any finite set of strings to be representedand allows for substrings common to multiple mem-bers of the set to be represented with a single pieceof structure.
Additionally, all paths from one node toanother form an equivalence class representing, inour model, alternative expressions of the same un-derlying communicative intent.For translation, we will find it useful to encodeG in a chart based on a topological ordering of thenodes, as described by Cheppalier et al (1999).
Thenodes in the lattices shown in Figure 1 are labeledaccording to an appropriate numbering.The chart-representation of the graph is a triple of2-dimensional matrices ?F,p,R?, which can be con-structed from the numbered graph.
Fi,j is the wordlabel of the jth transition leaving node i.
The cor-responding transition cost is pi,j .
Ri,j is the nodenumber of the node on the right side of the jth tran-sition leaving node i.
Note that Ri,j > i for all i, j.Table 1 shows the word lattice from Figure 1 repre-sented in matrix form as ?F,p,R?.0 1 2a 1 1 b 1 2 c 1 3a 13 1 b 1 2 c12 3x 13 1 d12 3 13 1x 12 1 y 1 2 b12 3a 12 2 c12 3Table 1: Topologically ordered chart encoding of thethree lattices in Figure 1.
Each cell ij in this table is atriple ?Fij ,pij ,Rij?10132.2 Parsing word latticesChiang (2005) introduced hierarchical phrase-basedtranslation models, which are formally basedon synchronous context-free grammars (SCFGs).Translation proceeds by parsing the input using thesource language side of the grammar, simultane-ously building a tree on the target language side viathe target side of the synchronized rules.
Since de-coding is equivalent to parsing, we begin by present-ing a parser for word lattices, which is a generaliza-tion of a CKY parser for lattices given in Cheppalieret al (1999).Following Goodman (1999), we present our lat-tice parser as a deductive proof system in Figure 2.The parser consists of two kinds of items, the firstwith the form [X ?
?
?
?, i, j] representing rulesthat have yet to be completed and span node i tonode j.
The other items have the form [X, i, j] andindicate that non-terminal X spans [i, j].
As withsentence parsing, the goal is a deduction that coversthe spans of the entire input lattice [S, 0, |V | ?
1].The three inference rules are: 1) match a terminalsymbol and move across one edge in the lattice 2)move across an -edge without advancing the dot inan incomplete rule 3) advance the dot across a non-terminal symbol given appropriate antecedents.2.3 From parsing to MT decodingA target language model is necessary to generate flu-ent output.
To do so, the grammar is intersected withan n-gram LM.
To mitigate the effects of the combi-natorial explosion of non-terminals the LM intersec-tion entails, we use cube-pruning to only considerthe most promising expansions (Chiang, 2007).2.4 Lattice translation with FSTsA second important class of translation models in-cludes those based formally on FSTs.
We present adescription of the decoding process for a word latticeusing a representative FST model, the phrase-basedtranslation model described in Koehn et al (2003).Phrase-based models translate a foreign sentencef into the target language e by breaking up f intoa sequence of phrases fI1, where each phrase f i cancontain one or more contiguous words and is trans-lated into a target phrase ei of one or more contigu-ous words.
Each word in f must be translated ex-actly once.
To generalize this model to word lattices,it is necessary to choose both a path through the lat-tice and a partitioning of the sentence this inducesinto a sequence of phrases fI1.
Although the numberof source phrases in a word lattice can be exponen-tial in the number of nodes, enumerating the possibletranslations of every span in a lattice is in practicetractable, as described by Bertoldi et al (2007).2.5 Decoding with phrase-based modelsWe adapted the Moses phrase-based decoder totranslate word lattices (Koehn et al, 2007).
Theunmodified decoder builds a translation hypothesisfrom left to right by selecting a range of untrans-lated words and adding translations of this phrase tothe end of the hypothesis being extended.
When nountranslated words remain, the translation process iscomplete.The word lattice decoder works similarly, onlynow the decoder keeps track not of the words thathave been covered, but of the nodes, given a topo-logical ordering of the nodes.
For example, assum-ing the third lattice in Figure 1 is our input, if theedge with word a is translated, this will cover twountranslated nodes [0,1] in the coverage vector, eventhough it is only a single word.
As with sentence-based decoding, a translation hypothesis is completewhen all nodes in the input lattice are covered.2.6 Non-monotonicity and unreachable nodesThe changes described thus far are straightfor-ward adaptations of the underlying phrase-basedsentence decoder; however, dealing properly withnon-monotonic decoding of word lattices introducessome minor complexity that is worth mentioning.
Inthe sentence decoder, any translation of any span ofuntranslated words is an allowable extension of apartial translation hypothesis, provided that the cov-erage vectors of the extension and the partial hypoth-esis do not intersect.
In a non-linear word lattice,a further constraint must be enforced ensuring thatthere is always a path from the starting node of thetranslation extension?s source to the node represent-ing the nearest right edge of the already-translatedmaterial, as well as a path from the ending node ofthe translation extension?s source to future translatedspans.
Figure 3 illustrates the problem.
If [0,1] istranslated, the decoder must not consider translating1014Axioms:[X ?
?
?, i, i] : w(Xw??
?
?, ??)
?
G, i ?
[0, |V | ?
2]Inference rules:[X ?
?
?
Fj,k?, i, j] : w[X ?
?Fj,k ?
?, i,Rj,k] : w ?
pj,k[X ?
?
?
?, i, j] : w[X ?
?
?
?, i,Rj,k] : w ?
pj,kFj,k = [Z ?
?
?X?, i, k] : w1 [X ?
?
?, k, j] : w2[Z ?
?X ?
?, i, j] : w1 ?
w2Goal state:[S ?
?
?, 0, |V | ?
1]Figure 2: Word lattice parser for an unrestricted context free grammar G.0 1x2ayFigure 3: The span [0, 3] has one inconsistent covering,[0, 1] + [2, 3].
[2,3] as a possible extension of this hypothesis sincethere is no path from node 1 to node 2 and thereforethe span [1,2] would never be covered.
In the parserthat forms the basis of the hierarchical decoder de-scribed in Section 2.3, no such restriction is neces-sary since grammar rules are processed in a strictlyleft-to-right fashion without any skips.3 Distortion in a non-linear word latticeIn both hierarchical and phrase-based models, thedistance between words in the source sentence isused to limit where in the target sequence their trans-lations will be generated.
In phrase based transla-tion, distortion is modeled explicitly.
Models thatsupport non-monotonic decoding generally includea distortion cost, such as |ai ?
bi?1 ?
1| where ai isthe starting position of the foreign phrase f i and bi?1is the ending position of phrase f i?1 (Koehn et al,2003).
The intuition behind this model is that sincemost translation is monotonic, the cost of skippingahead or back in the source should be proportionalto the number of words that are skipped.
Addition-ally, a maximum distortion limit is used to restrict0 1x2a y3b cdFigure 4: Distance-based distortion problem.
What is thedistance between node 4 to node 0?the size of the search space.In linear word lattices, such as confusion net-works, the distance metric used for the distortionpenalty and for distortion limits is well defined;however, in a non-linear word lattice, it poses theproblem illustrated in Figure 4.
Assuming the left-to-right decoding strategy described in the previoussection, if c is generated by the first target word, thedistortion penalty associated with ?skipping ahead?should be either 3 or 2, depending on what path ischosen to translate the span [0,3].
In large lattices,where a single arc may span many nodes, the possi-ble distances may vary quite substantially dependingon what path is ultimately taken, and handling thisproperly therefore crucial.Although hierarchical phrase-based models donot model distortion explicitly, Chiang (2007) sug-gests using a span length limit to restrict the win-dow in which reordering can take place.1 The de-coder enforces the constraint that a synchronous rulelearned from the training data (the only mechanismby which reordering can be introduced) can span1This is done to reduce the size of the search space and be-cause hierarchical phrase-based translation models are inaccu-rate models of long-distance distortion.1015Distance metric MT05 MT06Difference 0.2943 0.2786Difference+LexRO 0.2974 0.2890ShortestP 0.2993 0.2865ShortestP+LexRO 0.3072 0.2992Table 2: Effect of distance metric on phrase-based modelperformance.maximally ?
words in f .
Like the distortion costused in phrase-based systems, ?
is also poorly de-fined for non-linear lattices.Since we want a distance metric that will restrictas few local reorderings as possible on any path,we use a function ?
(a, b) returning the length of theshortest path between nodes a and b.
Since this func-tion is not dependent on the exact path chosen, it canbe computed in advance of decoding using an all-pairs shortest path algorithm (Cormen et al, 1989).3.1 Experimental resultsWe tested the effect of the distance metric on trans-lation quality using Chinese word segmentation lat-tices (Section 4.1, below) using both a hierarchicaland phrase-based system modified to translate wordlattices.
We compared the shortest-path distancemetric with a baseline which uses the difference innode number as the distortion distance.
For an ad-ditional datapoint, we added a lexicalized reorder-ing model that models the probability of each phrasepair appearing in three different orientations (swap,monotone, other) in the training corpus (Koehn etal., 2005).Table 2 summarizes the results of the phrase-based systems.
On both test sets, the shortest pathmetric improved the BLEU scores.
As expected,the lexicalized reordering model improved transla-tion quality over the baseline; however, the improve-ment was more substantial in the model that used theshortest-path distance metric (which was already ahigher baseline).
Table 3 summarizes the results ofour experiment comparing the performance of twodistance metrics to determine whether a rule has ex-ceeded the decoder?s span limit.
The pattern is thesame, showing a clear increase in BLEU for theshortest path metric over the baseline.Distance metric MT05 MT06Difference 0.3063 0.2957ShortestP 0.3176 0.3043Table 3: Effect of distance metric on hierarchical modelperformance.4 Exploiting Source Language AlternativesChinese word segmentation.
A necessary firststep in translating Chinese using standard modelsis segmenting the character stream into a sequenceof words.
Word-lattice translation offers two possi-ble improvements over the conventional approach.First, a lattice may represent multiple alternativesegmentations of a sentence; input represented inthis way will be more robust to errors made by thesegmenter.2 Second, different segmentation granu-larities may be more or less optimal for translatingdifferent spans.
By encoding alternatives in the in-put in a word lattice, the decision as to which granu-larity to use for a given span can be resolved duringdecoding rather than when constructing the system.Figure 5 illustrates a lattice based on three differentsegmentations.Arabic morphological variation.
Arabic orthog-raphy is problematic for lexical and phrase-basedMT approaches since a large class of functional el-ements (prepositions, pronouns, tense markers, con-junctions, definiteness markers) are attached to theirhost stems.
Thus, while the training data may pro-vide good evidence for the translation of a partic-ular stem by itself, the same stem may not be at-tested when attached to a particular conjunction.The general solution taken is to take the best pos-sible morphological analysis of the text (it is of-ten ambiguous whether a piece of a word is partof the stem or merely a neighboring functional el-ement), and then make a subset of the bound func-tional elements in the language into freestanding to-kens.
Figure 6 illustrates the unsegmented Arabicsurface form as well as the morphological segmen-tation variant we made use of.
The limitation of thisapproach is that as the amount and variety of train-ing data increases, the optimal segmentation strat-egy changes: more aggressive segmentation results2The segmentation process is ambiguous, even for nativespeakers of Chinese.101601?2??4?????3????5?6???7"8?9???10?11??
?12"Figure 5: Sample Chinese segmentation lattice using three segmentations.in fewer OOV tokens, but automatic evaluation met-rics indicate lower translation quality, presumablybecause the smaller units are being translated lessidiomatically (Habash and Sadat, 2006).
Lattices al-low the decoder to make decisions about what gran-ularity of segmentation to use subsententially.4.1 Chinese Word Segmentation ExperimentsIn our experiments we used two state-of-the-art Chi-nese word segmenters: one developed at HarbinInstitute of Technology (Zhao et al, 2001), andone developed at Stanford University (Tseng et al,2005).
In addition, we used a character-based seg-mentation.
In the remaining of this paper, we use csfor character segmentation, hs for Harbin segmenta-tion and ss for Stanford segmentation.
We built twotypes of lattices: one that combines the Harbin andStanford segmenters (hs+ss), and one which usesall three segmentations (hs+ss+cs).Data and Settings.
The systems used in theseexperiments were trained on the NIST MT06 Evalcorpus without the UN data (approximatively 950Ksentences).
The corpus was analyzed with the threesegmentation schemes.
For the systems using wordlattices, the training data contained the versions ofthe corpus appropriate for the segmentation schemesused in the input.
That is, for the hs+ss condition,the training data consisted of two copies of the cor-pus: one segmented with the Harbin segmenter andthe other with the Stanford segmenter.3 A trigramEnglish language model with modified Kneser-Neysmoothing (Kneser and Ney, 1995) was trained onthe English side of our training data as well as por-tions of the Gigaword v2 English Corpus, and wasused for all experiments.
The NIST MT03 test setwas used as a development set for optimizing the in-terpolation weights using minimum error rate train-3The corpora were word-aligned independently and thenconcatenated for rule extraction.ing (Och, 2003).
The testing was done on the NIST2005 and 2006 evaluation sets (MT05, MT06).Experimental results: Word-lattices improvetranslation quality.
We used both a phrase-basedtranslation model, decoded using our modified ver-sion of Moses (Koehn et al, 2007), and a hierarchi-cal phrase-based translation model, using our modi-fied version of Hiero (Chiang, 2005; Chiang, 2007).These two translation model types illustrate the ap-plicability of the theoretical contributions presentedin Section 2 and Section 3.We observed that the coverage of named entities(NEs) in our baseline systems was rather poor.
Sincenames in Chinese can be composed of relativelylong strings of characters that cannot be translatedindividually, when generating the segmentation lat-tices that included cs arcs, we avoided segmentingNEs of type PERSON, as identified using a ChineseNE tagger (Florian et al, 2004).The results are summarized in Table 4.
We seethat using word lattices improves BLEU scores bothin the phrase-based model and hierarchical model ascompared to the single-best segmentation approach.All results using our word-lattice decoding for thehierarchical models (hs+ss and hs+ss+cs) are sig-nificantly better than the best segmentation (ss).4For the phrase-based model, we obtain significantgains using our word-lattice decoder using all threesegmentations on MT05.
The other results, whilebetter than the best segmentation (hs) by at least0.3 BLEU points, are not statistically significant.Even if the results are not statistically significantfor MT06, there is a high decrease in OOV itemswhen using word-lattices.
For example, for MT06the number of OOVs in the hs translation is 484.4Significance testing was carried out using the bootstrap re-sampling technique advocated by Koehn (2004).
Unless other-wise noted, all reported improvements are signficant at at leastp < 0.05.1017surface wxlAl ftrp AlSyf kAn mEZm AlDjyj AlAElAmy m&ydA llEmAd .segmented w- xlAl ftrp Al- Syf kAn mEZm Al- Djyj Al- AElAmy m&ydA l- Al- EmAd .
(English) During the summer period , most media buzz was supportive of the general .Figure 6: Example of Arabic morphological segmentation.The number of OOVs decreased by 19% for hs+ssand by 75% for hs+ss+cs.
As mentioned in Section3, using lexical reordering for word-lattices furtherimproves the translation quality.4.2 Arabic Morphology ExperimentsWe created lattices from an unsegmented version ofthe Arabic test data and generated alternative arcswhere clitics as well as the definiteness marker andthe future tense marker were segmented into tokens.We used the Buckwalter morphological analyzer anddisambiguated the analysis using a simple unigrammodel trained on the Penn Arabic Treebank.Data and Settings.
For these experiments wemade use of the entire NIST MT08 training data,although for training of the system, we used a sub-sampling method proposed by Kishore Papineni thataims to include training sentences containing n-grams in the test data (personal communication).For all systems, we used a 5-gram English LMtrained on 250M words of English training data.The NIST MT03 test set was used as developmentset for optimizing the interpolation weights usingMER training (Och, 2003).
Evaluation was car-ried out on the NIST 2005 and 2006 evaluation sets(MT05, MT06).Experimental results: Word-lattices improvetranslation quality.
Results are presented in Table5.
Using word-lattices to combine the surface formswith morphologically segmented forms significantlyimproves BLEU scores both in the phrase-based andhierarchical models.5 Prior workLattice Translation.
The ?noisier channel?
modelof machine translation has been widely used in spo-ken language translation as an alternative to select-ing the single-best hypothesis from an ASR systemand translating it (Ney, 1999; Casacuberta et al,2004; Zhang et al, 2005; Saleem et al, 2005; Ma-tusov et al, 2005; Bertoldi et al, 2007; Mathias,2007).
Several authors (e.g.
Saleem et al (2005)and Bertoldi et al (2007)) comment directly onthe impracticality of using n-best lists to translatespeech.Although translation is fundamentally a non-monotonic relationship between most languagepairs, reordering has tended to be a secondary con-cern to the researchers who have worked on latticetranslation.
Matusov et al (2005) decodes monoton-ically and then uses a finite state reordering modelon the single-best translation, along the lines ofBangalore and Riccardi (2000).
Mathias (2007)and Saleem et al (2004) only report results ofmonotonic decoding for the systems they describe.Bertoldi et al (2007) solve the problem by requiringthat their input be in the format of a confusion net-work, which enables the standard distortion penaltyto be used.
Finally, the system described by Zhanget al (2005) uses IBM Model 4 features to translatelattices.
For the distortion model, they use the maxi-mum probability value over all possible paths in thelattice for each jump considered, which is similarto the approach we have taken.
Mathias and Byrne(2006) build a phrase-based translation system as acascaded series of FSTs which can accept any inputFSA; however, the only reordering that is permittedis the swapping of two adjacent phrases.Applications of source lattices outside of the do-main of spoken language translation have been farmore limited.
Costa-jussa` and Fonollosa (2007) takesteps in this direction by using lattices to encodemultiple reorderings of the source language.
Dyer(2007) uses confusion networks to encode mor-phological alternatives in Czech-English translation,and Xu et al (2005) takes an approach very similarto ours for Chinese-English translation and encodesmultiple word segmentations in a lattice, but whichis decoded with a conventionally trained translationmodel and without a sophisticated reordering model.The Arabic-English morphological segmentationlattices are similar in spirit to backoff translationmodels (Yang and Kirchhoff, 2006), which consideralternative morphological segmentations and simpli-1018MT05 MT06(Source Type) BLEU BLEUcs 0.2833 0.2694hs 0.2905 0.2835ss 0.2894 0.2801hs+ss 0.2938 0.2870hs+ss+cs 0.2993 0.2865hs+ss+cs.lexRo 0.3072 0.2992MT05 MT06(Source Type) BLEU BLEUcs 0.2904 0.2821hs 0.3008 0.2907ss 0.3071 0.2964hs+ss 0.3132 0.3006hs+ss+cs 0.3176 0.3043(a) Phrase-based model (b) Hierarchical modelTable 4: Chinese Word Segmentation ResultsMT05 MT06(Source Type) BLEU BLEUsurface 0.4682 0.3512morph 0.5087 0.3841morph+surface 0.5225 0.4008MT05 MT06(Source Type) BLEU BLEUsurface 0.5253 0.3991morph 0.5377 0.4180morph+surface 0.5453 0.4287(a) Phrase-based model (b) Hierarchical modelTable 5: Arabic Morphology Resultsfications of a surface token when the surface tokencan not be translated.Parsing and formal language theory.
There hasbeen considerable work on parsing word lattices,much of it for language modeling applications inspeech recognition (Ney, 1991; Cheppalier and Raj-man, 1998).
Additionally, Grune and Jacobs (2008)refines an algorithm originally due to Bar-Hillel forintersecting an arbitrary FSA (of which word latticesare a subset) with a CFG.
Klein and Manning (2001)formalize parsing as a hypergraph search problemand derive an O(n3) parser for lattices.6 ConclusionsWe have achieved substantial gains in translationperformance by decoding compact representationsof alternative source language analyses, rather thansingle-best representations.
Our results generalizeprevious gains for lattice translation of spoken lan-guage input, and we have further generalized theapproach by introducing an algorithm for latticedecoding using a hierarchical phrase-based model.Additionally, we have shown that although wordlattices complicate modeling of word reordering, asimple heuristic offers good performance and en-ables many standard distortion models to be useddirectly with lattice input.AcknowledgmentsThis research was supported by the GALE programof the Defense Advanced Research Projects Agency,Contract No.
HR0011-06-2-0001.
The authors wishto thank Niyu Ge for the Chinese named-entity anal-ysis, Pi-Chuan Chang for her assistance with theStanford Chinese segmenter, and Tie-Jun Zhao andCongui Zhu for making the Harbin Chinese seg-menter available to us.ReferencesS.
Bangalore and G. Riccardi.
2000.
Finite state modelsfor lexical reordering in spoken language translation.In Proc.
Int.
Conf.
on Spoken Language Processing,pages 422?425, Beijing, China.A.L.
Berger, V.J.
Della Pietra, and S.A. Della Pietra.1996.
A maximum entropy approach to natural lan-guage processing.
Comput.
Linguist., 22(1):39?71.N.
Bertoldi and M. Federico.
2005.
A new decoder forspoken language translation based on confusion net-works.
In Proceedings of the IEEE Automatic SpeechRecognition and Understanding Workshop.N.
Bertoldi, R. Zens, and M. Federico.
2007.
Speechtranslation by confusion network decoding.
In Pro-ceeding of ICASSP 2007, Honolulu, Hawaii, April.P.F.
Brown, J. Cocke, S. Della-Pietra, V.J.
Della-Pietra,F.
Jelinek, J.D.
Lafferty, R.L.
Mercer, and P.S.Roossin.
1990.
A statistical approach to machinetranslation.
Computational Linguistics, 16:79?85.F.
Casacuberta, H. Ney, F. J. Och, E. Vidal, J. M. Vilar,S.
Barrachina, I. Garcia-Varea, D. Llorens, C. Mar-1019tinez, S. Molau, F. Nevado, M. Pastor, D. Pico, A. San-chis, and C. Tillmann.
2004.
Some approaches tostatistical and finite-state speech-to-speech translation.Computer Speech & Language, 18(1):25?47, January.J.
Cheppalier and M. Rajman.
1998.
A generalized CYKalgorithm for parsing stochastic CFG.
In Proceedingsof the Workshop on Tabulation in Parsing and Deduc-tion (TAPD98), pages 133?137, Paris, France.J.
Cheppalier, M. Rajman, R. Aragues, and A. Rozen-knop.
1999.
Lattice parsing for speech recognition.In Sixth Conference sur le Traitement Automatique duLangage Naturel (TANL?99), pages 95?104.D.
Chiang.
2005.
A hierarchical phrase-based model forstatistical machine translation.
In Proc.
of the 43rdAnnual Meeting of the Association for ComputationalLinguistics (ACL?05), pages 263?270.D.
Chiang.
2007.
Hierarchical phrase-based translation.Computational Linguistics, 33(2):201?228.T.H.
Cormen, C. E. Leiserson, and R. L. Rivest, 1989.Introduction to Algorithms, pages 558?565.
The MITPress and McGraw-Hill Book Company.M.
Costa-jussa` and J.A.R.
Fonollosa.
2007.
Analy-sis of statistical and morphological classes to gener-ate weighted reordering hypotheses on a statistical ma-chine translation system.
In Proc.
of the Second Work-shop on SMT, pages 171?176, Prague.C.
Dyer.
2007.
Noisier channel translation: translationfrom morphologically complex languages.
In Pro-ceedings of the Second Workshop on Statistical Ma-chine Translation, Prague, June.R.
Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kamb-hatla, X. Luo, N Nicolov, and S Roukos.
2004.
Astatistical model for multilingual entity detection andtracking.
In Proc.
of HLT-NAACL 2004, pages 1?8.J.
Goodman.
1999.
Semiring parsing.
ComputationalLinguistics, 25:573?605.D.
Grune and C.J.
H. Jacobs.
2008.
Parsing as intersec-tion.
Parsing Techniques, pages 425?442.N.
Habash and F. Sadat.
2006.
Arabic preprocessingschemes for statistical machine translation.
In Proc.
ofNAACL, New York.D.
Klein and C. D. Manning.
2001.
Parsing with hyper-graphs.
In Proceedings of IWPT 2001.R.
Kneser and H. Ney.
1995.
Improved backing-off form-gram language modeling.
In Proceedings of IEEEInternation Conference on Acoustics, Speech, and Sig-nal Processing, pages 181?184.P.
Koehn, F.J. Och, and D. Marcu.
2003.
Statisticalphrase-based translation.
In Proceedings of NAACL2003, pages 48?54.P.
Koehn, A. Axelrod, A. Birch Mayne, C. Callison-Burch, M. Osborne, and D. Talbot.
2005.
Edinburghsystem description for the 2005 IWSLT speech trans-lation evaluation.
In Proc.
of IWSLT 2005, Pittsburgh.P.
Koehn, H. Hoang, A. Birch Mayne, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,and E. Herbst.
2007.
Moses: Open source toolkitfor statistical machine translation.
In Annual Meetingof the Association for Computation Linguistics (ACL),Demonstration Session, pages 177?180, Jun.P.
Koehn.
2004.
Statistical significance tests for machinetranslation evluation.
In Proc.
of the 2004 Conf.
onEMNLP, pages 388?395.A.
Lopez.
to appear 2008.
Statistical machine transla-tion.
ACM Computing Surveys.L.
Mathias and W. Byrne.
2006.
Statistical phrase-based speech translation.
In IEEE Conf.
on Acoustics,Speech and Signal Processing.L.
Mathias.
2007.
Statistical Machine Translationand Automatic Speech Recognition under Uncertainty.Ph.D.
thesis, The Johns Hopkins University.E.
Matusov, S. Kanthak, and H. Ney.
2005.
On the in-tegration of speech recognition and statistical machinetranslation.
In Proceedings of Interspeech 2005.H.
Ney.
1991.
Dynamic programming parsing forcontext-free grammars in continuous speech recogni-tion.
IEEE Transactions on Signal Processing, 39(2).H.
Ney.
1999.
Speech translation: Coupling of recogni-tion and translation.
In Proc.
of ICASSP, pages 517?520, Phoenix.F.
Och and H. Ney.
2002.
Discriminitive trainingand maximum entropy models for statistical machinetranslation.
In Proceedings of the 40th Annual Meet-ing of the ACL, pages 295?302.S.
Saleem, S.-C. Jou, S. Vogel, and T. Schulz.
2005.
Us-ing word lattice information for a tighter coupling inspeech translation systems.
In Proc.
of ICSLP, JejuIsland, Korea.H.
Tseng, P. Chang, G. Andrew, D. Jurafsky, and C. Man-ning.
2005.
A conditional random field word seg-menter.
In Fourth SIGHANWorkshop on Chinese Lan-guage Processing.J.
Xu, E. Matusov, R. Zens, and H. Ney.
2005.
Inte-grated Chinese word segmentation in statistical ma-chine translation.
In Proc.
of IWSLT 2005, Pittsburgh.M.
Yang and K. Kirchhoff.
2006.
Phrase-based back-off models for machine translation of highly inflectedlanguages.
In Proceedings of the EACL 2006, pages41?48.R.
Zhang, G. Kikui, H. Yamamoto, and W. Lo.
2005.A decoding algorithm for word lattice translation inspeech translation.
In Proceedings of the 2005 Inter-national Workshop on Spoken Language Translation.T.
Zhao, L. Yajuan, Y. Muyun, and Y. Hao.
2001.
In-creasing accuracy of chinese segmentation with strat-egy of multi-step processing.
In J Chinese InformationProcessing (Chinese Version), volume 1, pages 13?18.1020
