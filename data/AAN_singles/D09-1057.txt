Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 543?550,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPInvestigation of Question Classifier in Question AnsweringZhiheng HuangEECS DepartmentUniversity of Californiaat BerkeleyCA 94720-1776, USAzhiheng@cs.berkeley.eduMarcus ThintIntelligent Systems Research CenterBritish Telecom GroupChief Technology Officemarcus.2.thint@bt.comAsli CelikyilmazEECS DepartmentUniversity of Californiaat BerkeleyCA 94720-1776, USAasli@cs.berkeley.eduAbstractIn this paper, we investigate how an ac-curate question classifier contributes toa question answering system.
We firstpresent a Maximum Entropy (ME) basedquestion classifier which makes use ofhead word features and their WordNet hy-pernyms.
We show that our question clas-sifier can achieve the state of the art per-formance in the standard UIUC questiondataset.
We then investigate quantitativelythe contribution of this question classifierto a feature driven question answering sys-tem.
With our accurate question classifierand some standard question answer fea-tures, our question answering system per-forms close to the state of the art usingTREC corpus.1 IntroductionQuestion answering has drawn significant atten-tion from the last decade (Prager, 2006).
It at-tempts to answer the question posed in naturallanguage by providing the answer phrase ratherthan the whole documents.
An important step inquestion answering (QA) is to classify the ques-tion to the anticipated type of the answer.
Forexample, the question of Who discovered x-raysshould be classified into the type of human (indi-vidual).
This information would narrow down thesearch space to identify the correct answer string.In addition, this information can suggest differentstrategies to search and verify a candidate answer.In fact, the combination of question classificationand the named entity recognition is a key approachin modern question answering systems (Voorheesand Dang, 2005).The question classification is by no means triv-ial: Simply using question wh-words can notachieve satisfactory results.
The difficulty liesin classifying the what and which type questions.Considering the example What is the capital of Yu-goslavia, it is of location (city) type, while Whatis the pH scale is of definition type.
As withthe previous work of (Li and Roth, 2002; Li andRoth, 2006; Krishnan et al, 2005; Moschitti etal., 2007), we propose a feature driven statisticalquestion classifier (Huang et al, 2008).
In partic-ular, we propose head word feature and augmentsemantic features of such head words using Word-Net.
In addition, Lesk?s word sense disambigua-tion (WSD) algorithm is adapted and the depth ofhypernym feature is optimized.
With further aug-ment of other standard features such as unigrams,we can obtain accuracy of 89.0% using ME modelfor 50 fine classes over UIUC dataset.In addition to building an accurate questionclassifier, we investigate the contribution of thisquestion classifier to a feature driven question an-swering rank model.
It is worth noting that, mostof the features we used in question answering rankmodel, depend on the question type information.For instance, if a question is classified as a type ofsport, we then only care about whether there aresport entities existing in the candidate sentences.It is expected that a fine grained named entity rec-ognizer (NER) should make good use of the accu-rate question type information.
However, due tothe lack of a fine grained NER tool at hand, weemploy the Stanford NER package (Finkel et al,2005) which identifies only four types of namedentities.
Even with such a coarse named entityrecognizer, the experiments show that the questionclassifier plays an important role in determiningthe performance of a question answering system.The rest of the paper is organized as follow-ing.
Section 2 reviews the maximum entropymodel which are used in both question classifica-tion and question answering ranking.
Section 3presents the features used in question classifica-tion.
Section 4 presents the question classification543accuracy over UIUC question dataset.
Section 5presents the question answer features.
Section 6illustrates the results based on TREC question an-swer dataset.
And Section 7 draws the conclusion.2 Maximum Entropy ModelsMaximum entropy (ME) models (Berger et al,1996; Manning and Klein, 2003), also known aslog-linear and exponential learning models, pro-vide a general purpose machine learning techniquefor classification and prediction which has beensuccessfully applied to natural language process-ing including part of speech tagging, named entityrecognition etc.
Maximum entropy models can in-tegrate features from many heterogeneous infor-mation sources for classification.
Each featurecorresponds to a constraint on the model.
Givena training set of (C,D), where C is a set of classlabels and D is a set of feature represented datapoints, the maximal entropy model attempts tomaximize the log likelihoodlogP (C|D,?)
=?(c,d)?
(C,D)logexp?i?ifi(c, d)?c?exp?j?jfi(c, d),(1)where fi(c, d) are feature indicator functions.
Weuse ME models for both question classificationand question answer ranking.
In question answercontext, such function, for instance, could be thepresence or absence of dictionary entities (as pre-sented in Section 5.2) associated with a particularclass type (either true or false, indicating a sen-tence can or cannot answer the question).
?iarethe parameters need to be estimated which reflectsthe importance of fi(c, d) in prediction.3 Question Classification FeaturesLi and Roth (2002) have developed a machinelearning approach which uses the SNoW learningarchitecture.
They have compiled the UIUC ques-tion classification dataset1 which consists of 5500training and 500 test questions.2 All questions inthe dataset have been manually labeled accordingto the coarse and fine grained categories as shownin Table 1, with coarse classes (in bold) followedby their fine classes.The UIUC dataset has laid a platform for thefollow-up research including (Hacioglu and Ward,2003; Zhang and Lee, 2003; Li and Roth, 2006;1Available at http://12r.cs.uiuc.edu/?cogcomp/Data/QA/QC.2Test questions are from TREC 10.Table 1: 6 coarse and 50 fine Question types de-fined in UIUC question dataset.ABBR letter desc NUMabb other manner codeexp plant reason countENTITY product HUMAN dateanimal religion group distancebody sport individual moneycolor substance title ordercreative symbol desc othercurrency technique LOC perioddis.med.
term city percentevent vehicle country speedfood word mountain tempinstrument DESC other sizelang definition state weightKrishnan et al, 2005; Moschitti et al, 2007).
Incontrast to Li and Roth (2006)?s approach whichmakes use of a very rich feature set, we proposeto use a compact yet effective feature set.
The fea-tures are briefly described as following.
More de-tailed information can be found at (Huang et al,2008).Question wh-word The wh-word feature is thequestion wh-word in given questions.
For ex-ample, the wh-word of question What is thepopulation of China is what.Head Word head word is defined as one singleword specifying the object that the questionseeks.
For example the head word of Whatis a group of turkeys called, is turkeys.
Thisis different to previous work including (Liand Roth, 2002; Krishnan et al, 2005) whichhas suggested a contiguous span of words(a group of turkeys in this example).
Thesingle word definition effectively avoids thenoisy information brought by non-head wordof the span (group in this case).
A syntac-tic parser (Petrov and Klein, 2007) and theCollins rules (Collins, 1999) are modified toextract such head words.WordNet Hypernym WordNet hypernyms areextracted for the head word of a given ques-tion.
The classic Lesk algorithm (Lesk, 1986)is used to compute the most probable sensefor a head word in the question context, andthen the hypernyms are extracted based onthat sense.
The depth of hypernyms is set to544six with trial and error.3 Hypernyms featurescapture the general terms of extracted headword.
For instance, the head word of ques-tion What is the proper name for a femalewalrus is extracted as walrus and its directhypernyms such as mammal and animal areextracted as informative features to predictthe correct question type of ENTY:animal.Unigram words Bag of words features.
Suchfeatures provide useful question context in-formation.Word shape Five word shape features, namely allupper case, all lower case, mixed case, alldigits, and other are used to serve as a coarsenamed entity recognizer.4 Question Classification ExperimentsWe train a Maximum Entropy model using theUIUC 5500 training questions and test over the500 test questions.
Tables 2 shows the accuracy of6 coarse class and 50 fine grained class, with fea-tures being fed incrementally.
The question classi-fication performance is measured by accuracy, i.e.,the proportion of the correctly classified questionsamong all test questions.
The baseline using theTable 2: Question classification accuracy using in-cremental feature sets for 6 and 50 classes overUIUC split.6 class 50 classwh-word 46.0 46.8+ head word 92.2 82.0+ hypernym 91.8 85.6+ unigram 93.0 88.4+ word shape 93.6 89.0wh-head word results in 46.0% and 46.8% respec-tively for 6 coarse and 50 fine class classification.The incremental use of head word boosts the accu-racy significantly to 92.2% and 82.0% for 6 and 50classes.
This reflects the informativeness of suchfeature.
The inclusion of hypernym feature within6 depths boosts 3.6% for 50 classes, while result-ing in slight loss for 6 coarse classes.
The furtheruse of unigram feature leads to 2.8% gain in 50classes.
Finally, the use of word shape leads to0.6% accuracy increase for 50 classes.
The best3We performed 10 cross validation experiment over train-ing data and tried various depths of 1, 3, 6, 9 and ?, with ?signifies that no depth constraint is imposed.accuracies achieved are 93.6% and 89.0% for 6and 50 classes respectively.The individual feature contributions were dis-cussed in greater detail in (Huang et al, 2008).Also, The SVM (rathern than ME model) was em-ployed using the same feature set and the resultswere very close (93.4% for 6 class and 89.2% for50 class).
Table 3 shows the feature ablation ex-periment4 which is missing in that paper.
Theexperiment shows that the proposed head wordand its hypernym features play an essential rolein building an accurate question classifier.Table 3: Question classification accuracy by re-moving one feature at a time for 6 and 50 classesover UIUC split.6 class 50 classoverall 93.6 89.0- wh-word 93.6 89.0- head word 92.8 88.2- hypernym 90.8 84.2- unigram 93.6 86.8- word shape 93.0 88.4Our best result feature space only consists of13?697 binary features and each question has 10to 30 active features.
Compared to the over featuresize of 200?000 in Li and Roth (2002), our featurespace is much more compact, yet turned out to bemore informative as suggested by the experiments.Table 4 shows the summary of the classificationaccuracy of all question classifiers which were ap-plied to UIUC dataset.5 Our results are summa-rized in the last row.In addition, we have performed the 10 crossvalidation experiment over the 5500 UIUC train-ing corpus using our best model.
The result is89.05?1.25 and 83.73?1.61 for 6 and 50 classes,6which outperforms the best result of 86.1?1.1 for6 classes as reported in (Moschitti et al, 2007).5 Question Answer FeaturesFor a pair of a question and a candidate sentence,we extract binary features which include CoNLLnamed entities presence feature (NE), dictionary4Remove one feature at a time from the entire feature set.5Note (1) that SNoW accuracy without the related worddictionary was not reported.
With the semantically relatedword dictionary, it achieved 91%.
Note (2) that SNoW with asemantically related word dictionary achieved 84.2% but theother algorithms did not use it.6These results are worse than the result over UIUC split;as the UIUC test data includes a larger percentage of easilyclassified question types.545Table 4: Accuracy of all question classifiers whichwere applied to UIUC dataset.Algorithm 6 class 50 classLi and Roth, SNoW ?
(1) 78.8(2)Hacioglu et al, SVM+ECOC ?
80.2-82Zhang & Lee, Linear SVM 87.4 79.2Zhang & Lee, Tree SVM 90.0 ?Krishnan et al, SVM+CRF 93.4 86.2Moschitti et al, Kernel 91.8 ?Maximum Entropy Model 93.6 89.0entities presence feature (DIC), numerical entitiespresence feature (NUM), question specific feature(SPE), and dependency validity feature (DEP).5.1 CoNLL named entities presence featureWe use Stanford named entity recognizer (NER)(Finkel et al, 2005) to identify CoNLL style NEs7as possible answer strings in a candidate sentencefor a given type of question.
In particular, if thequestion is ABBR type, we tag CoNLL LOC,ORG and MISC entities as candidate answers; Ifthe question is HUMAN type, we tag CoNLL PERand ORG entities; And if the question is LOCtype, we tag CoNLL LOC and MISC entities.
Forother types of questions, we assume there is nocandidate CoNLL NEs to tag.
We create a binaryfeature NE to indicate the presence or absence oftagged CoNLL entities.
Further more, we cre-ate four binary features NE-PER, NE-LOC, NE-ORG, and NE-MISC to indicate the presence oftagged CoNLL PER, LOC, ORG and MISC enti-ties.5.2 Dictionary entities presence featureAs four types of CoNLL named entities are notenough to cover 50 question types, we include the101 dictionary files compiled in the Ephyra project(Schlaefer et al, 2007).
These dictionary files con-tain names for specific semantic types.
For exam-ple, the actor dictionary comprises a list of actornames such as Tom Hanks and Kevin Spacey.
Foreach question, if the head word of such question(see Section 3) matches the name of a dictionaryfile, then each noun phrase in a candidate sentenceis looked up to check its presence in the dictio-nary.
If so, a binary DIC feature is created.
Forexample, for the question What rank did Chester7Person (PER), location (LOC), organization (ORG), andmiscellaneous (MISC).Nimitz reach, as there is a military rank dictionarymatches the head word rank, then all the nounphrases in a candidate sentence are looked up inthe military rank dictionary.
As a result, a sen-tence contains word Admiral will result in the DICfeature being activated, as such word is present inthe military rank dictionary.Note that an implementation tip is to allow theproximity match in the dictionary look up.
Con-sider the question What film introduced Jar JarBinks.
As there is a match between the ques-tion head word film and the dictionary namedfilm, each noun phrase in the candidate sentenceis checked.
However, no dictionary entities havebeen found from the candidate sentence Best playsJar Jar Binks, a floppy-eared, two-legged creaturein ?Star Wars: Episode I ?
The Phantom Men-ace?, although there is movie entitled Star WarsEpisode I: The Phantom Menace in the dictionary.Notice that Star Wars: Episode I ?
The PhantomMenace in the sentence and the dictionary entityStar Wars Episode I: The Phantom Menace do nothave exactly identical spelling.
The use of prox-imity look up which allows edit distance being lessthan 10% error can resolve this.5.3 Numerical entities presence featureThere are so far no match for question types ofNUM (as shown in Table 1) including NUM:countand NUM:date etc.
These types of questionsseek the numerical answers such as the amount ofmoney and the duration of period.
It is natural tocompile regular expression patterns to match suchentities.
For example, for a NUM:money typedquestion What is Rohm and Haas?s annual rev-enue, we compile NUM:money regular expressionpattern which matches the strings of number fol-lowed by a currency sign ($ and dollars etc).
Suchpattern is able to identify 4 billion $ as a candidateanswer in the candidate sentence Rohm and Haas,with 4 billion $ in annual sales...
There are 13 pat-terns compiled to cover all numerical types.
Wecreate a binary feature NUM to indicate the pres-ence of possible numerical answers in a sentence.5.4 Specific featuresSpecific features are question dependent.
For ex-ample, for question When was James Dean born,any candidate sentence matches the pattern JamesDean (number - number) is likely to answer suchquestion.
We create a binary feature SPE to indi-cate the presence of such match between a ques-546tion and a candidate sentence.
We list all questionand sentence match patterns which are used in ourexperiments as following:when born feature 1 The question begins with when is/wasand follows by a person name and then follows by keyword born; The candidate sentence contains such per-son name which follows by the pattern of (number -number).when born feature 2 The question begins with when is/wasand follows by a person name and then follows by keyword born; The candidate sentence contains such per-son name, a NUM:date entity, and a key word born.where born feature 1 The question begins with whereis/was and follows by a person name and then followsby key word born; The candidate sentence containssuch person name, a NER LOC entity, and a key wordborn.when die feature 1 The question begins with when did andfollows by a person name and then follows by key worddie; The candidate sentence contains such person namewhich follows by the pattern of (number - number).when die feature 2 The question begins with when did andfollows by a person name and then follows by keyword die; The candidate sentence contains such personname, a NUM:date entity, and a key word died.how many feature The question begins with how many andfollows by a noun; The candidate sentence contains anumber and then follows by such noun.cooccurrent Feature This feature takes two phrase argu-ments, if the question contains the first phrase and thecandidate sentence contains the second, such featurewould be activated.Note that the construction of specific featuresrequire the access to aforementioned extractednamed entities.
For example, the when born fea-ture 2 pattern needs the information whether acandidate sentence contains a NUM:date entityand where born feature 1 pattern needs the in-formation whether a candidate sentence containsa NER LOC entity.
Note also that the patterns ofwhen born feature and when die feature havesimilar structure and thus can be simplified in im-plementation.
How many feature can be usedto identify the sentence Amtrak annually servesabout 21 million passengers for question Howmany passengers does Amtrak serve annually.
Thecooccurrent feature is the most general one.
Anexample of cooccurrent feature would take thearguments of marry and husband, or marry andwife.
Such feature would be activated for ques-tion Whom did Eileen Marie Collins marry andcandidate sentence ... were Collins?
husband,Pat Youngs, an airline pilot...
It is worth notingthat the two arguments are not necessarily differ-ent.
For example, they could be both established,which makes such feature activated for questionWhen was the IFC established and candidate sen-tence IFC was established in 1956 as a member ofthe World Bank Group.
The reason why we use thecooccurrence of the word established is due to itsmain verb role, which may carry more informationthan other words.5.5 Dependency validity featuresLike (Cui et al, 2004), we extract the dependencypath from the question word to the common word(existing in both question and sentence), and thepath from candidate answer (such as CoNLL NEand numerical entity) to the common word foreach pair of question and candidate sentence usingStanford dependency parser (Klein and Manning,2003; Marneffe et al, 2006).
For example, forquestion When did James Dean die and candidatesentence In 1955, actor James Dean was killed ina two-car collision near Cholame, Calif., we ex-tract the pathes of When:advmod:nsubj:Dean and1955:prep-in:nsubjpass:Dean for question andsentence respectively, where advmod and nsubjetc.
are grammatical relations.
We propose thedependency validity feature (DEP) as following.For all paired paths between a question and a can-didate sentence, if at least one pair of path in whichall pairs of grammatical relations have been seenin the training, then the DEP feature is set to betrue, false otherwise.
That is, the true validity fea-ture indicates that at least one pair of path betweenthe question and candidate sentence is possible tobe a true pair (ie, the candidate noun phrase in thesentence path is the true answer).6 Question Answer ExperimentsRecall that most of the question answer featuresdepend on the question classifier.
For instance,the NE feature checks the presence or absence ofCoNLL style named entities subject to the clas-sified question type.
In this section, we evaluatehow the quality of question classifiers affects thequestion answering performance.6.1 Experiment setupWe use TREC99-03 factoid questions for trainingand TREC04 factoid questions for testing.
To fa-cilitate the comparison to others work (Cui et al,2004; Shen and Klakow, 2006), we first retrieveall relevant documents which are compiled by KenLitkowski8 to create training and test datasets.
We8Available at http://trec.nist.gov/data/qa.html.547then apply key word search for each question andretrieve the top 20 relevant sentences.
We createa feature represented data point using each pair ofquestion and candidate sentence and label it eithertrue or false depending on whether the sentencecan answer the given question or not.
The labelingis conducted by matching the gold factoid answerpattern against the candidate sentence.There are two extra steps performed for train-ing set but not for test data.
In order to constructa high quality training set, we manually check thecorrectness of the training data points and removethe false positive ones which cannot support thequestion although there is a match to gold answer.In addition, in order to keep the training data wellbalanced, we keep maximum four false data points(question answer pair) for each question but nolimit over the true label data points.
In doing so,we use 1458 questions to compile 8712 trainingdata points and among them 1752 have true labels.Similarly, we use 202 questions to compile 4008test data points and among them 617 have true la-bels.We use the training data to train a maximumentropy model and use such model to rank testdata set.
Compared with a classification task (suchas the question classifier), the ranking process re-quires one extra step: For data points which sharethe same question, the probabilities of being pre-dicted as true label are used to rank the data points.In align with the previous work, performance isevaluated using mean reciprocal rank (MRR), top1 prediction accuracy (top1) and top 5 predictionaccuracy (top5).
For the test data set, 157 amongthe 202 questions have correct answers found inretrieved sentences.
This leads to the upper boundof MRR score being 77.8%.To evaluate how the quality of question clas-sifiers affects the question answering, we havecreated three question classifiers: QC1, QC2and QC3.
The features which are used to trainthese question classifiers and their performanceare shown in Table 5.
Note that QC3 is the bestquestion classifier we obtained in Section 4.Table 5: Features used to train and the perfor-mance of three question classifiers.Name features 6 class 50 classQC1 wh-word 46.0 46.8QC2 wh-word+ head 92.2 82.0QC3 All 93.6 89.06.2 Experiment resultsThe first experiment is to evaluate the individ-ual contribution of various features derived usingthree question classifiers.
Table 6 shows the base-line result and results using DIC, NE, NE-4, REG,SPE, and DEP features.
The baseline is the keyword search without the use of maximum entropymodel.
As can be seen, the question classifiersdo not affect the DIC feature at all, as DIC fea-ture does not depend on question classifiers.
Bet-ter question classifier boosts considerable gain forNE, NE-4 and REG in their contribution to ques-tion answering.
For example, the best questionclassifier QC3 outperforms the worst one (QC1)by 1.5%, 2.0%, and 2.0% MRR scores for NE,NE-4 and REG respectively.
However, it is sur-prising that the MRR and top5 contribution of NEand NE-4 decreases if QC1 is replaced by QC2, al-though the top1 score results in performance gainslightly.
This unexpected results can be partiallyexplained as follows.
For some questions, evenQC2 produces correct predictions, the errors ofNE and NE-4 features may cause over-confidentscores for certain candidate sentences.
As SPE andDEP are not directly dependent on question clas-sifier, their individual contribution only changesslightly or remains the same for different ques-tion classifiers.
If the best question classifier isused, the most important features are SPE andREG, which can individually boost the MRR scoreover 54%, while the others result in less significantgains.We now incrementally use various features andthe results are show in Table 6 as well.
As canbe seen, the more features and the better questionclassifier are used, the higher performance the MEmodel has.
The inclusion of REG and SPE resultsin significant boost for the performance.
For ex-ample, if the best question classifier QC3 is used,the REG results in 6.9% and 8% gain for MRRand top1 scores respectively.
This is due to a largeportion of NUM type questions in test dataset.
TheSPE feature contributes significantly to the per-formance due to its high precision in answeringbirth/death time/location questions.
NE and NE-4result in reasonable gains while DEP feature con-tributes little.
However, this does not mean thatDEP is not important, as once the model reaches ahigh MRR score, it becomes hard to improve.Table 6 clearly shows that the question typeclassifier plays an essential role in a high perfor-548Table 6: Performance of individual and incremental feature sets for three question classifiers.IndividualFeature MRR Top1 Top5QC1 QC2 QC3 QC1 QC2 QC3 QC1 QC2 QC3Baseline 49.9 49.9 49.9 40.1 40.1 40.1 59.4 59.4 59.4DIC 49.5 49.5 49.5 42.6 42.6 42.6 60.4 60.4 60.4NE 48.5 47.5 50.0 40.6 40.6 42.6 61.9 60.9 63.4NE-4 49.5 48.5 51.5 41.6 42.1 44.6 62.4 61.9 64.4REG 52.0 54.0 54.0 44.1 47.0 47.5 64.4 65.3 65.3SPE 55.0 55.0 55.0 48.5 48.5 48.5 64.4 64.4 64.4DEP 51.0 51.5 52.0 43.6 44.1 44.6 65.3 65.8 65.8IncrementalBaseline 49.9 49.9 49.9 40.1 40.1 40.1 59.4 59.4 59.4+DIC 49.5 49.5 49.5 42.6 42.6 42.6 60.4 60.4 60.4+NE 50.0 48.5 51.0 43.1 42.1 44.6 62.9 61.4 64.4+NE-4 51.5 50.0 53.0 44.1 43.6 46.0 63.4 62.9 65.8+REG 55.0 56.9 59.9 48.0 51.0 54.0 68.3 68.8 71.8+SPE 60.4 62.4 65.3 55.4 58.4 61.4 70.8 70.8 73.8+DEP 61.4 62.9 66.3 55.9 58.4 62.4 71.8 71.8 73.8mance question answer system.
Assume all thefeatures are used, the better question classifier sig-nificantly boosts the overall performance.
For ex-ample, the best question classifier QC3 outper-forms the worst QC1 by 4.9%, 6.5%, and 2.0%for MRR, top1 and top5 scores respectively.
Evencompared to a good question classifier QC2, thegain of using QC3 is still 3.4%, 4.0% and 2.0%for MRR, top1 and top5 scores respectively.
Onecan imagine that if a fine grained NER is available(rather than the current four type coarse NER), thepotential gain is much significant.The reason that the question classifier affectsthe question answering performance is straightfor-ward.
As a upstream source, the incorrect classi-fication of question type would confuse the down-stream answer search process.
For example, forquestion What is Rohm and Haas?s annual rev-enue, our best question classifier is able to clas-sify it into the correct type of NUM:money andthus would put $ 4 billion as a candidate answer.However, the inferior question classifiers misclas-sify it into HUM:ind type and thereby could notreturn a correct answer.
Figure 1 shows the indi-vidual MRR scores for the 42 questions (amongthe 202 test questions) which have different pre-dicted question types using QC3 and QC2.
For al-most all test questions, the accurate question clas-sifier QC3 achieves higher MRR scores comparedto QC2.Table 7 shows performance of various questionanswer systems including (Tanev et al, 2004; Wuet al, 2005; Cui et al, 2004; Shen and Klakow,0 5 10 15 20 25 30 35 40 4500.10.20.30.40.50.60.70.80.91question idMRRQC3QC2Figure 1: Individual MRR scores for questionswhich have different predicted question types us-ing QC3 and QC2.2006) and this work which were applied to thesame training and test datasets.
Among all the sys-tems, our model can achieve the best MRR scoreof 66.3%, which is close to the state of the art of67.0%.
Considering the question answer featuresused in this paper are quite standard, the boost ismainly due to our accurate question classifier.Table 7: Various system performance comparison.System MRR Top1 Top5Tanev et al 2004 57.0 49.0 67.0Cui et al 2004 60.0 53.0 70.0Shen and Klakow, 2006 67.0 62.0 74.0This work 66.3 62.4 73.85497 ConclusionIn this paper, we have presented a question clas-sifier which makes use of a compact yet effi-cient feature set.
The question classifier outper-forms previous question classifiers over the stan-dard UIUC question dataset.
We further investi-gated quantitatively how the quality of questionclassifier impacts the performance of question an-swer system.
The experiments showed that an ac-curate question classifier plays an essential rolein question answering system.
With our accuratequestion classifier and some standard question an-swer features, our question answering system per-forms close to the state of the art.AcknowledgmentsWe wish to thank the three anonymous review-ers for their invaluable comments.
This re-search was supported by British Telecom grantCT1080028046 and BISC Program of UC Berke-ley.ReferencesA.
L. Berger, S. A. D. Pietra, and V. J. D. Pietra.
1996.A maximum entropy approach to natural languageprocessing.
Computational Linguistics, 22(1):39?71.M.
Collins.
1999.
Head-driven statistical models fornatural language parsing.
PhD thesis, University ofPennsylvania.H.
Cui, K Li, R. Sun, T. Chua, and M. Kan. 2004.
Na-tional university of singapore at the trec-13 questionanswering.
In Proc.
of TREC 2004, NIST.J.
Finkel, T. Grenager, and C. Manning.
2005.
In-corporating non-local information into informationextraction systems by Gibbs sampling.
In Proc.
ofACL, pages 363-370.D.
Hacioglu and W. Ward.
2003.
Question classifica-tion with support vector machines and error correct-ing codes.
In Proc.
of the ACL/HLT, vol.
2, pages28?30.Z.
Huang, M. Thint, and Z. Qin.
2008.
Question clas-sification using head words and their hypernyms.
InProc.
of the EMNLP.D.
Klein and C. D. Manning.
2003.
Accurate unlexi-calized parsing.
In Proc.
of ACL 2003, vol.
1, pages423?430.V.
Krishnan, S. Das, and S. Chakrabarti.
2005.
En-hanced answer type inference from questions usingsequential models.
In Proc.
of the HLT/EMNLP.M.
Lesk.
1986.
Automatic sense disambiguation us-ing machine readable dictionaries: how to tell a pinecone from an ice cream cone.
In ACM Special Inter-est Group for Design of Communication Proceed-ings of the 5th annual international conference onSystems documentation, pages 24?26.X.
Li and D. Roth.
2002.
Learning question classi-fiers.
In the 19th international conference on Com-putational linguistics, vol.
1, pages 1-7.X.
Li and D. Roth.
2006.
Learning question classifiers:the role of semantic information.
Natural LanguageEngineering, 12(3):229?249.C.
D. Manning and D. Klein.
2003.
Optimization,maxent models, and conditional estimation with-out magic.
Tutorial at HLT-NAACL 2003 and ACL2003.M.
D. Marneffe, B. MacCartney and C. D. Manning.2006.
Generating typed dependency parses fromphrase structure parses.
In Proc.
of LREC 2006.A.
Moschitti, S. Quarteroni, R. Basili and S. Manand-har 2007.
Exploiting syntactic and shallow seman-tic kernels for question answer classification.
InProc.
of ACL 2007, pages 776-783.S.
Petrov and D. Klein.
2007.
Improved inference forunlexicalized parsing.
In Proc.
of the HLT-NAACL.J.
Prager.
2006.
Open-domain question-answering.In Foundations and Trends in Information Retrieval,vol.
1, pages 91-231, 2006.N.
Schlaefer, J. Ko, J. Betteridge, G. Sautter, M. Pathakand E. Nyberg.
2007.
Semantic extensions of theEphyra QA system for TREC 2007.
In Proc.
of theTREC 2007.D.
Shen and D. Klakow.
2006.
Exploring correlationof dependency relation paths for answer extraction.In Proc.
of the ACL 2006.H.
Tanev, M. Kouylekov, and B. Magnini.
2004.Combining linguistic processing and web mining forquestion answering: Itc-irst at TREC-2004.
In Proc.of the TREC 2004, NIST.E.
M. Voorhees and H. T. Dang.
2005.
Overview ofthe TREC 2005 question answering track.
In Proc.of the TREC 2005, NIST.M.
Wu, M. Duan, S. Shaikh, S. Small, and T. Strza-lkowski.
2005.
University at Albanys ILQUA inTREC 2005.
In Proc.
of the TREC 2005, NIST.D.
Zhang and W. S. Lee.
2003.
Question classificationusing support vector machines.
In The ACM SIGIRconference in information retrieval, pages 26?32.550
