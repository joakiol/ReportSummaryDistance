Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 49?54,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsA Web-based Evaluation Framework for Spatial Instruction-Giving SystemsSrinivasan Janarthanam, Oliver Lemon, and Xingkun LiuInteraction LabSchool of Mathematical and Computer SciencesHeriot Watt University, Edinburghsc445,o.lemon,x.liu@hw.ac.ukAbstractWe demonstrate a web-based environment fordevelopment and testing of different pedes-trian route instruction-giving systems.
Theenvironment contains a City Model, a TTSinterface, a game-world, and a user GUI in-cluding a simulated street-view.
We describethe environment and components, the metricsthat can be used for the evaluation of pedes-trian route instruction-giving systems, and theshared challenge which is being organised us-ing this environment.1 IntroductionGenerating navigation instructions in the real worldfor pedestrians is an interesting research problemfor researchers in both computational linguisticsand geo-informatics (Dale et al, 2003; Richter andDuckham, 2008).
These systems generate verbalroute directions for users to go from A to B, andtechniques range from giving ?a priori?
route direc-tions (i.e.
all route information in a single turn) andincremental ?in-situ?
instructions, to full interactivedialogue systems (see section 4).
One of the majorproblems in developing such systems is in evaluat-ing them with real users in the real world.
Such eval-uations are expensive, time consuming and painstak-ing to organise, and are carried out not just at the endof the project but also during the development cycle.Consequently, there is a need for a common platformto effectively compare the performances of verbalnavigation systems developed by different teams us-ing a variety of techniques (e.g.
a priori vs. in-situor rule-based vs. machine learning).This demonstration system brings together exist-ing online data resources and software toolkits tocreate a low-cost framework for evaluation of pedes-trian route instruction systems.
We have built aweb-based environment containing a simulated realworld in which users can simulate walking on thestreets of real cities whilst interacting with differ-ent navigation systems.
This evaluation frameworkwill be used in the near future to evaluate a series ofinstruction-giving dialogue systems.2 Related workThe GIVE challenge developed a 3D virtual in-door environment for development and evaluationof indoor pedestrian navigation instruction systems(Koller et al, 2007; Byron et al, 2007).
In thisframework, users can walk through a building withrooms and corridors, similar to a first-person shootergame.
The user is instructed by a navigation sys-tem that generates route instructions.
The basic ideawas to have several such navigation systems hostedon the GIVE server and evaluate them in the samegame worlds, with a number of users over the in-ternet.
Conceptually our work is very similar to theGIVE framework, but its objective is to evaluate sys-tems that instruct pedestrian users in the real world.The GIVE framework has been successfully used forcomparative evaluation of several systems generat-ing instructions in virtual indoor environments.Another system, ?Virtual Navigator?, is a simu-lated 3D environment that simulates the real worldfor training blind and visually impaired people tolearn often-used routes and develop basic naviga-tion skills (McGookin et al, 2010).
The framework49uses haptic force-feedback and spatialised auditoryfeedback to simulate the interaction between usersand the environment they are in.
The users simulatewalking by using arrow keys on a keyboard and byusing a device that works as a 3D mouse to simulatea virtual white cane.
Auditory clues are providedto the cane user to indicate for example the differ-ence between rush hour and a quiet evening in theenvironment.
While this simulated environment fo-cusses on the providing the right kind of tactile andauditory feedback to its users, we focus on provid-ing a simulated environment where people can lookat landmarks and navigate based on spatial and vi-sual instructions provided to them.User simulation modules are usually developedto train and test reinforcement learning based in-teractive spoken dialogue systems (Janarthanam andLemon, 2009; Georgila et al, 2006; Schatzmann etal., 2006).
These agents replace real users in interac-tion with dialogue systems.
However, these modelssimulate the users?
behaviours in addition to the en-vironment in which they operate.
Users?
dialogueand physical behaviour are dependent on a numberof factors such as a user?s preferences, goals, knowl-edge of the environment, environmental constraints,etc.
Simulating a user?s behaviour realistically basedon many such features requires large amounts ofdata.
In contrast to this approach, we propose a sys-tem where only the spatial and visual environment issimulated.See section 4 for a discussion of different pedes-trian navigation systems.3 ArchitectureThe evaluation framework architecture is shown infigure 1.
The server side consists of a broker module,navigation system, gameworld server, TTS engine,and a city model.
On the user?s side is a web-basedclient that consists of the simulated real-world andthe interaction panel.3.1 Game-world moduleWalking aimlessly in the simulated real world can bea boring task.
Therefore, instead of giving web usersnavigation tasks from A to B, we embed navigationtasks in a game-world overlaid on top of the simu-lated real world.
We developed a ?treasure hunting?game which consists of users solving several piecesof a puzzle to discover the location of the treasurechest.
In order to solve the puzzle, they interact withgame characters (e.g.
a pirate) to obtain clues as towhere the next clue is.
This sets the user a number ofnavigation tasks to acquire the next clues until theyfind the treasure.
In order to keep the game interest-ing, the user?s energy depletes as time goes on andthey therefore have limited time to find the treasure.Finally, the user?s performance is scored to encour-age users to return.
The game characters and enti-ties like keys, chests, etc.
are laid out on real streetsmaking it easy to develop a game without develop-ing a game-world.
New game-worlds can be easilyscripted using Javascript, where the location (lati-tude and longitude) and behaviour of the game char-acters are defined.
The game-world module servesgame-world specifications to the web-based client.3.2 BrokerThe broker module is a web server that connects theweb clients to their corresponding different naviga-tion systems.
This module ensures that the frame-work works for multiple users.
Navigation systemsare instantiated and assigned to new users when theyfirst connect to the broker.
Subsequent messagesfrom the users will be routed to the assigned navi-gation system.
The broker communicates with thenavigation systems via a communication platformthereby ensuring that different navigation systemsdeveloped using different languages (such as C++,Java, Python, etc) are supported.3.3 Navigation systemThe navigation system is the central component ofthis architecture, which provides the user instruc-tions to reach their destinations.
Each navigationsystem is run as a server remotely.
When a user?sclient connects to the server, it instantiates a navi-gation system object and assigns it to the user ex-clusively.
Every user is identified using a unique id(UUID), which is used to map the user to his/her re-spective navigation system.
The navigation systemis introduced in the game scenario as a buddy sys-tem that will help the user in his objective: find thetreasure.
The web client sends the user?s location tothe system periodically (every few seconds).50Figure 1: Evaluation framework architecture3.4 TTS engineAlongside the navigation system we use the Cere-proc text-to-speech engine that converts the utter-ances of the system into speech.
The URL of theaudio file is then sent to the client?s browser whichthen uses the audio plugin to play the synthesizedspeech to the user.
The TTS engine need not be usedif the output modality of the system is just text.3.5 City ModelThe navigation system is supported by a databasecalled the City Model.
The City Model is a GISdatabase containing a variety of data required to sup-port navigation tasks.
It has been derived from anopen-source data source called OpenStreetMaps1.
Itconsists of the following:?
Street network data: the street network dataconsists of nodes and ways representing junc-tions and streets.?
Amenities: such as ATMs, public toilets, etc.?
Landmarks: other structures that can serve aslandmarks.
E.g.
churches, restaurants, etc.The amenities and landmarks are represented asnodes (with latitude and longitude information).
TheCity Model interface API consists of a number of1www.openstreetmaps.orgsubroutines to access the required information suchas the nearest amenity, distance or route from A to B,etc.
These subroutines provide the interface betweenthe navigation systems and the database.3.6 Web-based clientThe web-based client is a JavaScript/HTML pro-gram running on the user?s web browser software(e.g.
Google Chrome).
A snapshot of the webclientis shown in figure 2.
It has two parts: the streetviewpanel and the interaction panel.Streetview panel: the streetview panel presents asimulated real world visually to the user.
Whenthe page loads, a Google Streetview client (GoogleMaps API) is created with an initial user coordinate.Google Streetview is a web service that renders apanoramic view of real streets in major cities aroundthe world.
This client allows the web user to get apanoramic view of the streets around the user?s vir-tual location.
A gameworld received from the serveris overlaid on the simulated real world.
The user canwalk around and interact with game characters usingthe arrow keys on his keyboard or the mouse.
As theuser walks around, his location (stored in the formof latitude and longitude coordinates) gets updatedlocally.
Streetview also returns the user?s point ofview (0-360 degrees), which is also stored locally.Interaction panel: the web-client also includes an51interaction panel that lets the user interact with hisbuddy navigation system.
In addition to user lo-cation information, users can also interact with thenavigation system using textual utterances or theirequivalents.
We provide users with two types of in-teraction panel: a GUI panel and a text panel.
In theGUI panel, there are GUI objects such as buttons,drop-down lists, etc.
which can be used to constructrequests and responses to the system.
By clickingthe buttons, users can send abstract semantic repre-sentations (dialogue actions) that are equivalent totheir textual utterances.
For example, the user canrequest a route to a destination by selecting the streetname from a drop down list and click on the Sendbutton.
Similarly, users can click on ?Yes?, ?No?,?OK?, etc.
buttons to respond to the system?s ques-tions and instructions.
In the text panel, on the otherhand, users are free to type any request or responsethey want.
Of course, both types of inputs are parsedby the navigation system.
We also plan to add an ad-ditional input channel that can stream user speech tothe navigation system in the future.4 Candidate Navigation SystemsThis framework can be used to evaluate a varietyof navigation systems.
Route navigation has beenan interesting research topic for researchers in bothgeoinformatics and computational linguistics alike.Several navigation prototype systems have been de-veloped over the years.
Although there are severalsystems that do not use language as a means of com-munication for navigation tasks (instead using geo-tagged photographs (Beeharee and Steed, 2006; Hi-ley et al, 2008), haptics (Bosman et al, 2003), mu-sic (Holland et al, 2002; Jones et al, 2008), etc), wefocus on systems that generate instructions in natu-ral language.
Therefore, our framework does not in-clude systems that generate routes on 2D/3D mapsas navigation aids.Systems that generate text/speech can be furtherclassified as follows:?
?A priori?
systems: these systems generateroute instructions prior to the users touring theroute.
These systems describe the entire routebefore the user starts navigating.
Several webservices exist that generate such lists of step-by-step instructions (e.g.
Google/Bing direc-tions).?
?In-situ?
or incremental route instruction sys-tems: these systems generate route instructionsincrementally along the route.
e.g.
CORAL(Dale et al, 2003).
They keep track of theuser?s location and issue the next instructionwhen the user reaches the next node on theplanned route.
The next instruction tells theuser how to reach the new next node.
Somesystems do not keep track of the user, but re-quire the user to request the next instructionwhen they reach the next node.?
Interactive navigation systems: these systemsare both incremental and interactive.
e.g.DeepMap (Malaka and Zipf, 2000).
Thesesystems keep track of the user?s location andproactively generate instructions based on userproximity to the next node.
In addition, theycan interact with users by asking them ques-tions about entities in their viewshed.
For ex-ample ?Can you see a tower at about 100 feetaway??.
Questions like these will let the systemassess the user?s location and thereby adapt itsinstruction to the situated context.5 Evaluation metricsNavigation systems can be evaluated using twokinds of metrics using this framework.
Objectivemetrics such as time taken by the user to finisheach navigation task and the game, distance trav-elled, number of wrong turns, etc.
can be directlymeasured from the environment.
Subjective met-rics based on each user?s ratings of different featuresof the system can be obtained through user satisfac-tion questionnaires.
In our framework, users are re-quested to fill in a questionnaire at the end of thegame.
The questionnaire consists of questions aboutthe game, the buddy, and the user himself, for exam-ple:?
Was the game engaging??
Would you play it again (i.e.
another similargameworld)??
Did your buddy help you enough?52Figure 2: Snapshot of the web client?
Were the buddy instructions easy to under-stand??
Were the buddy instructions ever wrong or mis-placed??
If you had the chance, will you choose the samebuddy in the next game??
How well did you know the neighbourhood ofthe gameworld before the game?6 Evaluation scenariosWe aim to evaluate navigation systems under a vari-ety of scenarios.?
Uncertain GPS: GPS positioning available insmartphones is erroneous (Zandbergen andBarbeau, 2011).
Therefore, one scenario forevaluation would be to test how robustly nav-igation systems handle erroneous GPS signalsfrom the user?s end.?
Output modalities: the output of navigationsystems can be presented in two modalities:text and speech.
While speech may enable ahands-free eyes-free navigation, text displayedon navigation aids like smartphones may in-crease cognitive load.
We therefore believe itwill be interesting to evaluate the systems inboth conditions and compare the results.?
Noise in user speech: for systems that takeas input user speech, it is important to handlenoise in such a channel.
Noise due to wind andtraffic is most common in pedestrian scenarios.Scenarios with different levels of noise settingscan be evaluated.?
Adaptation to users: returning users may havelearned the layout of the game world.
An inter-esting scenario is to examine how navigationsystems adapt to user?s increasing spatial andvisual knowledge.Errors in GPS positioning of the user and noisein user speech can be simulated at the server end,thereby creating a range of challenging scenarios toevaluate the robustness of the systems.7 The Shared ChallengeWe plan to organise a shared challenge for outdoorpedestrian route instruction generation, in which avariety of systems can be evaluated.
Participatingresearch teams will be able to use our interfacesand modules to develop navigation systems.
Eachteam will be provided with a development toolkit53and documentation to setup the framework in theirlocal premises for development purposes.
Devel-oped systems will be hosted on our challenge serverand a web based evaluation will be organised in con-sultation with the research community (Janarthanamand Lemon, 2011).8 Demonstration systemAt the demonstration, we will present the evaluationframework along with a demo navigation dialoguesystem.
The web-based client will run on a laptopusing a high-speed broadband connection.
The nav-igation system and other server modules will run ona remote server.AcknowledgmentsThe research has received funding from theEuropean Community?s Seventh FrameworkProgramme (FP7/2007-2013) under grantagreement no.
216594 (SPACEBOOK projectwww.spacebookproject.org).ReferencesAshweeni K. Beeharee and Anthony Steed.
2006.
A nat-ural wayfinding exploiting photos in pedestrian navi-gation systems.
In Proceedings of the 8th conferenceon Human-computer interaction with mobile devicesand services (2006).S.
Bosman, B. Groenendaal, J. W. Findlater, T. Visser,M.
de Graaf, and Panos Markopoulos.
2003.
Gen-tleGuide: An Exploration of Haptic Output for IndoorsPedestrian Guidance.
In Proceedings of 5th Interna-tional Symposium, Mobile HCI 2003, Udine, Italy.D.
Byron, A. Koller, J. Oberlander, L. Stoia, andK.
Striegnitz.
2007.
Generating Instructions in Vir-tual Environments (GIVE): A challenge and evaluationtestbed for NLG.
In Proceedings of the Workshop onShared Tasks and Comparative Evaluation in NaturalLanguage Generation.Robert Dale, Sabine Geldof, and Jean-Philippe Prost.2003.
CORAL : Using Natural Language Generationfor Navigational Assistance.
In Proceedings of theTwenty-Sixth Australasian Computer Science Confer-ence (ACSC2003), 4th7th February, Adelaide, SouthAustralia.Kallirroi Georgila, James Henderson, and Oliver Lemon.2006.
User simulation for spoken dialogue systems:Learning and evaluation.
In Proceedings of Inter-speech/ICSLP, pages 1065?1068.Harlan Hiley, Ramakrishna Vedantham, Gregory Cuel-lar, Alan Liuy, Natasha Gelfand, Radek Grzeszczuk,and Gaetano Borriello.
2008.
Landmark-based pedes-trian navigation from collections of geotagged photos.In Proceedings of the 7th International Conference onMobile and Ubiquitous Multimedia (MUM) 2008.S.
Holland, D. Morse, and H. Gedenryd.
2002.
Audio-gps: Spatial audio navigation with a minimal atten-tion interface.
Personal and Ubiquitous Computing,6(4):253?259.Srini Janarthanam and Oliver Lemon.
2009.
A User Sim-ulation Model for learning Lexical Alignment Policiesin Spoken Dialogue Systems.
In European Workshopon Natural Language Generation.Srini Janarthanam and Oliver Lemon.
2011.
TheGRUVE Challenge: Generating Routes under Uncer-tainty in Virtual Environments.
In Proceedings ofENLG / Generation Challenges.M.
Jones, S. Jones, G. Bradley, N. Warren, D. Bainbridge,and G. Holmes.
2008.
Ontrack: Dynamically adapt-ing music playback to support navigation.
Personaland Ubiquitous Computing, 12(7):513?525.A.
Koller, J. Moore, B. Eugenio, J. Lester, L. Stoia,D.
Byron, J. Oberlander, and K. Striegnitz.
2007.Shared Task Proposal: Instruction Giving in VirtualWorlds.
In Workshop on Shared Tasks and Compar-ative Evaluation in Natural Language Generation.Rainer Malaka and Er Zipf.
2000.
Deep Map - chal-lenging IT research in the framework of a tourist in-formation system.
In Information and CommunicationTechnologies in Tourism 2000, pages 15?27.
Springer.D.
McGookin, R. Cole, and S. Brewster.
2010.
Vir-tual navigator: Developing a simulator for independentroute learning.
In Proceedings of Workshop on HapticAudio Interaction Design 2010, Denmark.Kai-Florian Richter and Matt Duckham.
2008.
Simplestinstructions: Finding easy-to-describe routes for navi-gation.
In Proceedings of the 5th international confer-ence on Geographic Information Science.Jost Schatzmann, Karl Weilhammer, Matt Stuttle, andSteve Young.
2006.
A survey of statistical user sim-ulation techniques for reinforcement-learning of dia-logue management strategies.
The Knowledge Engi-neering Review, 21:97?126.P.
A. Zandbergen and S. J. Barbeau.
2011.
Positionalaccuracy of assisted gps data from high-sensitivitygps-enabled mobile phones.
Journal of Navigation,64(3):381?399.54
