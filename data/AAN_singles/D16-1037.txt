Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 382?391,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsVariational Neural Discourse Relation RecognizerBiao Zhang1, Deyi Xiong2?, Jinsong Su1, Qun Liu3,4, Rongrong Ji1, Hong Duan1, Min Zhang2Xiamen University, Xiamen, China 3610051Provincial Key Laboratory for Computer Information Processing TechnologySoochow University, Suzhou, China 2150062ADAPT Centre, School of Computing, Dublin City University3Key Laboratory of Intelligent Information Processing,Institute of Computing Technology, Chinese Academy of Sciences4zb@stu.xmu.edu.cn, {jssu, rrji, hduan}@xmu.edu.cnqun.liu@dcu.ie, {dyxiong, minzhang}@suda.edu.cnAbstractImplicit discourse relation recognition is acrucial component for automatic discourse-level analysis and nature language understand-ing.
Previous studies exploit discriminativemodels that are built on either powerful man-ual features or deep discourse representations.In this paper, instead, we explore generativemodels and propose a variational neural dis-course relation recognizer.
We refer to thismodel as VarNDRR.
VarNDRR establishes adirected probabilistic model with a latent con-tinuous variable that generates both a dis-course and the relation between the two ar-guments of the discourse.
In order to per-form efficient inference and learning, we in-troduce neural discourse relation models toapproximate the prior and posterior distribu-tions of the latent variable, and employ theseapproximated distributions to optimize a repa-rameterized variational lower bound.
This al-lows VarNDRR to be trained with standardstochastic gradient methods.
Experiments onthe benchmark data set show that VarNDRRcan achieve comparable results against state-of-the-art baselines without using any manualfeatures.1 IntroductionDiscourse relation characterizes the internal struc-ture and logical relation of a coherent text.
Automat-ically identifying these relations not only plays animportant role in discourse comprehension and gen-eration, but also obtains wide applications in many?Corresponding authorother relevant natural language processing tasks,such as text summarization (Yoshida et al, 2014),conversation (Higashinaka et al, 2014), question an-swering (Verberne et al, 2007) and information ex-traction (Cimiano et al, 2005).
Generally, discourserelations can be divided into two categories: explicitand implicit, which can be illustrated in the follow-ing example:The company was disappointed by the rul-ing.
because The obligation is totally un-warranted.
(adapted from wsj 0294)With the discourse connective because, these twosentences display an explicit discourse relationCONTINGENCY which can be inferred easily.
Oncethis discourse connective is removed, however, thediscourse relation becomes implicit and difficult tobe recognized.
This is because almost no surface in-formation in these two sentences can signal this re-lation.
For successful recognition of this relation, inthe contrary, we need to understand the deep seman-tic correlation between disappointed and obligationin the two sentences above.
Although explicit dis-course relation recognition (DRR) has made greatprogress (Miltsakaki et al, 2005; Pitler et al, 2008),implicit DRR still remains a serious challenge dueto the difficulty in semantic analysis.Conventional approaches to implicit DRR oftentreat the relation recognition as a classification prob-lem, where discourse arguments and relations are re-garded as the inputs and outputs respectively.
Gen-erally, these methods first generate a representationfor a discourse, denoted as x1 (e.g., manual fea-1Unless otherwise specified, all variables in the paper, e.g.,x,y, z are multivariate.
But for notational convenience, we382amssymb amsmathzx y?
?NFigure 1: Graphical illustration for VarNDRR.
Solidlines denote the generative model p?(x|z)p?
(y|z),dashed lines denote the variational approximationq?
(z|x,y) to the posterior p(z|x,y) and q??
(z|x) to theprior p(z) for inference.
The variational parameters ?
arelearned jointly with the generative model parameters ?.tures in SVM-based recognition (Pitler et al, 2009;Lin et al, 2009) or sentence embeddings in neu-ral networks-based recognition (Ji and Eisenstein,2015; Zhang et al, 2015)), and then directly modelthe conditional probability of the corresponding dis-course relation y given x, i.e.
p(y|x).
In spite oftheir success, these discriminative approaches relyheavily on the goodness of discourse representa-tion x.
Sophisticated and good representations ofa discourse, however, may make models suffer fromoverfitting as we have no large-scale balanced data.Instead, we assume that there is a latent continu-ous variable z from an underlying semantic space.It is this latent variable that generates both dis-course arguments and the corresponding relation,i.e.
p(x,y|z).
The latent variable enables us tojointly model discourse arguments and their rela-tions, rather than conditionally model y on x. How-ever, the incorporation of the latent variable makesthe modeling difficult due to the intractable compu-tation with respect to the posterior distribution.Inspired by Kingma and Welling (2014) as wellas Rezende et al (2014) who introduce a variationalneural inference model to the intractable posteriorvia optimizing a reparameterized variational lowerbound, we propose a variational neural discourse re-lation recognizer (VarNDRR) with a latent contin-uous variable for implicit DRR in this paper.
Thekey idea behind VarNDRR is that although the pos-terior distribution is intractable, we can approximateit via a deep neural network.
Figure 1 illustrates thetreat them as univariate variables in most cases.
Additionally,we use bold symbols to denote variables, and plain symbols todenote values.graph structure of VarNDRR.
Specifically, there aretwo essential components:?
neural discourse recognizer As a discourse xand its corresponding relation y are indepen-dent with each other given the latent variable z(as shown by the solid lines), we can formulatethe generation of x and y from z in the equa-tion p?
(x,y|z) = p?(x|z)p?(y|z).
These twoconditional probabilities on the right hand sideare modeled via deep neural networks (see sec-tion 3.1).?
neural latent approximator VarNDRR assumesthat the latent variable can be inferred from dis-course arguments x and relations y (as shownby the dash lines).
In order to infer the la-tent variable, we employ a deep neural net-work to approximate the posterior q?
(z|x,y)as well as the prior q??
(z|x) (see section 3.2),which makes the inference procedure efficient.We further employ a reparameterization tech-nique to sample z from q?
(z|x,y) that not onlybridges the gap between the recognizer and theapproximator but also allows us to use the stan-dard stochastic gradient ascent techniques foroptimization (see section 3.3).The main contributions of our work lie in two as-pects.
1) We exploit a generative graphic model forimplicit DRR.
To the best of our knowledge, thishas never been investigated before.
2) We developa neural recognizer and two neural approximatorsspecifically for implicit DRR, which enables boththe recognition and inference to be efficient.We conduct a series of experiments for Englishimplicit DRR on the PDTB-style corpus to evaluatethe effectiveness of our proposed VarNDRR model.Experiment results show that our variational modelachieves comparable results against several strongbaselines in term of F1 score.
Extensive analysison the variational lower bound further reveals thatour model can indeed fit the data set with respect todiscourse arguments and relations.2 Background: Variational AutoencoderThe variational autoencoder (VAE) (Kingma andWelling, 2014; Rezende et al, 2014), which formsthe basis of our model, is a generative model that canbe regarded as a regularized version of the standard383autoencoder.
With a latent random variable z, VAEsignificantly changes the autoencoder architecture tobe able to capture the variations in the observed vari-able x.
The joint distribution of (x, z) is formulatedas follows:p?
(x, z) = p?(x|z)p?
(z) (1)where p?
(z) is the prior over the latent variable, usu-ally equipped with a simple Gaussian distribution.p?
(x|z) is the conditional distribution that modelsthe probability of x given the latent variable z. Typi-cally, VAE parameterizes p?
(x|z) with a highly non-linear but flexible function approximator such as aneural network.The objective of VAE is to maximize a variationallower bound as follows:LV AE(?, ?
;x) = ?KL(q?(z|x)||p?(z))+Eq?
(z|x)[log p?
(x|z)] ?
log p?
(x)(2)where KL(Q||P ) is Kullback-Leibler divergence be-tween two distributions Q and P .
q?
(z|x) is anapproximation of the posterior p(z|x) and usuallyfollows a diagonal Gaussian N (?, diag(?2)) whosemean ?
and variance ?2 are parameterized by again,neural networks, conditioned on x.To optimize Eq.
(2) stochastically with respect toboth ?
and ?, VAE introduces a reparameterizationtrick that parameterizes the latent variable z with theGaussian parameters ?
and ?
in q?(z|x):z?
= ?+ ?
 (3)where  is a standard Gaussian variable, and  de-notes an element-wise product.
Intuitively, VAElearns the representation of the latent variable notas single points, but as soft ellipsoidal regions in la-tent space, forcing the representation to fill the spacerather than memorizing the training data as isolatedrepresentations.
With this trick, the VAE model canbe trained through standard backpropagation tech-nique with stochastic gradient ascent.3 The VarNDRR ModelThis section introduces our proposed VarNDRRmodel.
Formally, in VarNDRR, there are two ob-served variables, x for a discourse and y for the cor-responding relation, and one latent variable z. Aszyx1 x2p?(x|z)p?
(y|z)h?1 h?2Figure 2: Neural networks for conditional probabilitiesp?
(x|z) and p?(y|z).
The gray color denotes real-valuedrepresentations while the white and black color 0-1 rep-resentations.illustrated in Figure 1, the joint distribution of thethree variables is formulated as follows:p?
(x,y, z) = p?
(x,y|z)p(z) (4)We begin with this distribution to elaborate the ma-jor components of VarNDRR.3.1 Neural Discourse RecognizerThe conditional distribution p(x,y|z) in Eq.
(4)shows that both discourse arguments and the corre-sponding relation are generated from the latent vari-able.
As shown in Figure 1, x is d-separated fromy by z.
Therefore the discourse x and the corre-sponding relation y is independent given the latentvariable z.
The joint probability can be thereforeformulated as follows:p?
(x,y, z) = p?(x|z)p?
(y|z)p(z) (5)We use a neural model q??
(z|x) to approximate theprior p(z) conditioned on the discourse x (see thefollowing section).
With respect to the other twoconditional distributions, we parameterize them vianeural networks as shown in Figure 2.Before we describe these neural networks, it isnecessary to briefly introduce how discourse rela-tions are annotated in our training data.
The PDTBcorpus, used as our training data, annotates implicitdiscourse relations between two neighboring argu-ments, namely Arg1 and Arg2.
In VarNDRR, werepresent the two arguments with bag-of-word rep-resentations, and denote them as x1 and x2.To model p?
(x|z) (the bottom part in Figure 2),we project the representation of the latent variable384z ?
Rdz onto a hidden layer:h?1 = f(Wh?1z + bh?1)h?2 = f(Wh?2z + bh?1)(6)where h?1 ?
Rdh?1 , h?2 ?
Rdh?2 , W?
is the transfor-mation matrix, b?
is the bias term, du denotes thedimensionality of vector representations of u andf(?)
is an element-wise activation function, such astanh(?
), which is used throughout our model.Upon this hidden layer, we further stack a Sig-moid layer to predict the probabilities of correspond-ing discourse arguments:x?1 = Sigmoid(Wx?1h?1 + bx?1)x?2 = Sigmoid(Wx?2h?2 + bx?2)(7)here, x?1 ?
Rdx1 and x?2 ?
Rdx2 are the real-valued representations of the reconstructed x1 andx2 respectively.2 We assume that p?
(x|z) is a mul-tivariate Bernoulli distribution because of the bag-of-word representation.
Therefore the logarithm ofp(x|z) is calculated as the sum of probabilities ofwords in discourse arguments as follows:log p(x|z)=?ix1,i log x?1,i + (1?
x1,i) log(1?
x?1,i)+?jx2,j log x?2,j + (1?
x2,j) log(1?
x?2,j)(8)where ui,j is the jth element in ui.In order to estimate p?
(y|z) (the top part in Fig-ure 2), we stack a softmax layer over the multilayer-perceptron-transformed representation of the latentvariable z:y?
= SoftMax(Wy?MLP(z) + by?)
(9)y?
?
Rdy , and dy denotes the number of discourserelations.
MLP projects the representation of latentvariable z into a dm-dimensional space through fourinternal layers, each of which has dimension dm.Suppose that the true relation is y ?
Rdy , the log-arithm of p(y|z) is defined as:log p(y|z) =dy?i=1yi log y?i (10)2Notice that the equality of dx1 = dx2 , dh?1 = dh?2 is notnecessary though we assume so in our experiments.
?x1h1log ?2h2x2yhyq??(z|x)q?
(z|x,y)Figure 3: Neural networks for Gaussian parameters ?and log ?
in the approximated posterior q?
(z|x,y) andprior q??
(z|x).In order to precisely estimate these conditionalprobabilities, our model will force the representationz of the latent variable to encode semantic informa-tion for both the reconstructed discourse x?
(Eq.
(8))and predicted discourse relation y?
(Eq.
(10)), whichis exactly what we want.3.2 Neural Latent ApproximatorFor the joint distribution in Eq.
(5), we can definea variational lower bound that is similar to Eq.
(2).The difference lies in two aspects: the approximateprior q??
(z|x) and posterior q?(z|x,y).
We modelboth distributions as a multivariate Gaussian distri-bution with a diagonal covariance structure:N (z;?, ?2I)The mean ?
and s.d.
?
of the approximate distribu-tion are the outputs of the neural network as shownin Figure 3, where the prior and posterior have dif-ferent conditions and independent parameters.Approximate Posterior q?
(z|x,y) is modeled tocondition on both observed variables: the discoursearguments x and relations y.
Similar to the calcula-tion of p?
(x|z), we first transform the input x and yinto a hidden representation:h1 = f(Wh1x1 + bh1)h2 = f(Wh2x2 + bh2)hy = f(Whyy + bhy)(11)where h1 ?
Rdh1 , h2 ?
Rdh2 and hy ?
Rdhy .33Notice that dh1/dh2 are not necessarily equal to dh?1/dh?2 .385We then obtain the Gaussian parameters of theposterior ?
and log ?2 through linear regression:?
= W?1h1 +W?2h2 +W?yhy + b?log ?2 = W?1h1 +W?2h2 +W?yhy + b?
(12)where ?, ?
?
Rdz .
In this way, this posterior ap-proximator can be efficiently computed.Approximate Prior q??
(z|x) is modeled to condi-tion on discourse arguments x alone.
This is basedon the observation that discriminative models areable to obtain promising results using only x. There-fore, assuming the discourse arguments encode theprior information for discourse relation recognitionis meaningful.The neural model for prior q??
(z|x) is the same asthat (i.e.
Eq (11) and (12)) for posterior q?
(z|x,y)(see Figure 3), except for the absence of discourserelation y.
For clarity , we use ??
and ??
to denotethe mean and s.d.
of the approximate prior.With the parameters of Gaussian distribution, wecan access the representation z using different sam-pling strategies.
However, traditional sampling ap-proaches often breaks off the connection betweenrecognizer and approximator, making the optimiza-tion difficult.
Instead, we employ the reparameteri-zation trick (Kingma and Welling, 2014; Rezende etal., 2014) as in Eq.
(3).
During training, we sam-ple the latent variable using z?
= ?
+ ?
; duringtesting, however, we employ the expectation of z inthe approximate prior distribution, i.e.
set z?
= ??
toavoid uncertainty.3.3 Parameter LearningWe employ the Monte Carlo method to estimate theexpectation over the approximate posterior, that isEq?
(z|x,y)[log p?(x,y|z)].
Given a training instance(x(t), y(t)), the joint training objective is defined:L(?, ?)
' ?KL(q?
(z|x(t), y(t))||q??
(z|x(t)))+ 1LL?l=1log p?
(x(t), y(t)|z?
(t,l)) (13)where z?
(t,l) = ?
(t) + ?
(t)  (l) and (l) ?
N (0, I)L is the number of samples.
The first term is the KLdivergence of two Gaussian distributions which canbe computed and differentiated without estimation.Algorithm 1 Parameter Learning Algorithm ofVarNDRR.Inputs: A, the maximum number of iterations;M , the number of instances in one batch;L, the number of samples;?, ??
Initialize parametersrepeatD ?
getRandomMiniBatch(M)?
getRandomNoiseFromStandardGaussian()g??
?,?L(?, ?
;D, )?, ??
parameterUpdater(?, ?
; g)until convergence of parameters (?, ?)
or reach themaximum iteration ARelation #Instance NumberTrain Dev TestCOM 1942 197 152CON 3342 295 279EXP 7004 671 574TEM 760 64 85Table 1: Statistics of implicit discourse relations for thetraining (Train), development (Dev) and test (Test) sets inPDTB.Maximizing this objective will minimize the differ-ence between the approximate posterior and prior,thus making the setting z?
= ??
during testing rea-sonable.
The second term is the approximate ex-pectation of Eq?
(z|x,y)[log p?
(x,y|z)], which is alsodifferentiable.As the objective function in Eq.
(13) is differen-tiable, we can optimize both the model parameters ?and variational parameters ?
jointly using standardgradient ascent techniques.
The training procedurefor VarNDRR is summarized in Algorithm 1.4 ExperimentsWe conducted experiments on English implicit DRRtask to validate the effectiveness of VarNDRR.44.1 DatasetWe used the largest hand-annotated discourse cor-pus PDTB 2.05 (Prasad et al, 2008) (PDTB here-after).
This corpus contains discourse annotationsover 2,312 Wall Street Journal articles, and is or-ganized in different sections.
Following previouswork (Pitler et al, 2009; Zhou et al, 2010; Lan et4Source code is available athttps://github.com/DeepLearnXMU/VarNDRR.5http://www.seas.upenn.edu/ pdtb/386Model Acc P R F1R & X (2015) - - - 41.00J & E (2015) 70.27 - - 35.93SVM 63.10 22.79 64.47 33.68SCNN 60.42 22.00 67.76 33.22VarNDRR 63.30 24.00 71.05 35.88(a) COM vs OtherModel Acc P R F1(R & X (2015)) - - - 53.80(J & E (2015)) 76.95 - - 52.78SVM 62.62 39.14 72.40 50.82SCNN 63.00 39.80 75.29 52.04VarNDRR 53.82 35.39 88.53 50.56(b) CON vs OtherModel Acc P R F1(R & X (2015)) - - - 69.40(J & E (2015)) 69.80 - - 80.02SVM 60.71 65.89 58.89 62.19SCNN 63.00 56.29 91.11 69.59VarNDRR 57.36 56.46 97.39 71.48(c) EXP vs OtherModel Acc P R F1(R & X (2015)) - - - 33.30(J & E (2015)) 87.11 - - 27.63SVM 66.25 15.10 68.24 24.73SCNN 76.95 20.22 62.35 30.54VarNDRR 62.14 17.40 97.65 29.54(d) TEM vs OtherTable 2: Classification results of different models on the implicit DRR task.
Acc=Accuracy, P=Precision, R=Recall,and F1=F1 score.al., 2013; Zhang et al, 2015), we used sections 2-20 as our training set, sections 21-22 as the test set.Sections 0-1 were used as the development set forhyperparameter optimization.In PDTB, discourse relations are annotated in apredicate-argument view.
Each discourse connectiveis treated as a predicate that takes two text spans asits arguments.
The discourse relation tags in PDTBare arranged in a three-level hierarchy, where thetop level consists of four major semantic classes:TEMPORAL (TEM), CONTINGENCY (CON), EX-PANSION (EXP) and COMPARISON (COM).
Be-cause the top-level relations are general enough tobe annotated with a high inter-annotator agreementand are common to most theories of discourse, in ourexperiments we only use this level of annotations.We formulated the task as four separate one-against-all binary classification problems: each toplevel class vs. the other three discourse relationclasses.
We also balanced the training set by resam-pling training instances in each class until the num-ber of positive and negative instances are equal.
Incontrast, all instances in the test and development setare kept in nature.
The statistics of various data setsis listed in Table 1.4.2 SetupWe tokenized all datasets using Stanford NLPToolkit6.
For optimization, we employed the Adam6http://nlp.stanford.edu/software/corenlp.shtmlalgorithm (Kingma and Ba, 2014) to update param-eters.
With respect to the hyperparameters M,L,Aand the dimensionality of all vector representations,we set them according to previous work (Kingmaand Welling, 2014; Rezende et al, 2014) and pre-liminary experiments on the development set.
Fi-nally, we set M = 16, A = 1000, L = 1, dz =20, dx1 = dx2 = 10001, dh1 = dh2 = dh?1 = dh?2 =dm = dhy = 400, dy = 2 for all experiments.7.
Allparameters of VarNDRR are initialized by a Gaus-sian distribution (?
= 0, ?
= 0.01).
For Adam,we set ?1 = 0.9, ?2 = 0.999 with a learning rate0.001.
Additionally, we tied the following parame-ters in practice: Wh1 and Wh2 , Wx?1 and Wx?2 .We compared VarNDRR against the followingtwo different baseline methods:?
SVM: a support vector machine (SVM) classi-fier8 trained with several manual features.?
SCNN: a shallow convolutional neural networkproposed by Zhang et al (2015).We also provide results from two state-of-the-artsystems:?
Rutherford and Xue (2015) convert explicitdiscourse relations into implicit instances.?
Ji and Eisenstein (2015) augment discourserepresentations via entity connections.7There is one dimension in dx1 and dx2 for unknown words.8http://svmlight.joachims.org/3871 -1270.24 25.30122 -207.21 26.043743 -210.21 26.641654 -182.84 24.707855 -182.17 23.630146 -178.72 247 -177.09 27.615068 -174.33 24.704349 -170.43 25.936610 -166.63 22.8310511 -163.22 22.9411812 -159.37 25.180913 -155.23 27.7268114 -150.78 28.7445915 -146.27 29.2953316 -141.61 30.7036217 -136.9 29.5566518 -132.44 28.8252719 -128.22 29.7835520 -124.19 26.2295121 -120.32 12.4137922 -117.68 28.4294223 -113.3 29.5384624 -110.01 30.3393225 -106.99 29.6363626 -104.15 29.6758127 -101.29 27.8688528 -98.89 27.3255829 -96.5 28.0991730 -94.19 29.2682931 -92.15 22.9038932 -90.16 29.3624233 -88.39 29.1423834 -86.65 27.8630535 -84.92 12.1212136 -83.46 29.759337 -81.93 28.6637938 -80.44 30.6651639 -79.16 29.1900640 -77.8 34.9304541 -76.65 29.9306242 -75.48 38.4180843 -74.41 42.204344 -73.35 0.98039245 -72.38 26.6021846 -71.4 19.5804247 -70.51 28.5046748 -69.65 26.2216949 -68.79 20.1257950 -68 24.1987251 -67.18 28.1645652 -66.44 26.6787753 -65.76 28.5326154 -65.04 0-1400-1200-1000-800-600-400-20000510152025303540451 101 201 301 401 501 601 701 801 901Dev Train(a) COM vs Other1 -774.47 31.844222 -178.34 29.579383 -174.75 36.314364 -172.55 31.856295 -168.78 30.787046 -164.1 23.262417 -159.04 39.742678 -153.3 42.93429 -147.32 19.2592610 -141.37 011 -135.17 34.4410912 -129.05 19.4852913 -123.05 014 -117.5 015 -112.36 9.6969716 -107.69 30.1568217 -103.41 18.9189218 -99.5 21.0290819 -95.89 18.0555620 -92.55 32.9563821 -89.57 25.2676722 -86.76 38.0566823 -84.18 7.50605324 -81.77 49.3177425 -79.56 29.1105126 -77.86 28.0612227 -75.62 14.7492628 -73.78 32.9914529 -72.12 24.4258930 -70.57 40.3768531 -69.09 27.4472232 -67.72 26.6441833 -66.42 25.5639134 -65.28 37.6923135 -64.09 036 -63.03 42.1168737 -62.01 038 -61.03 34.8027839 -60.12 35.7664240 -59.21 39.2108541 -58.42 39.1143942 -57.67 32.7819543 -56.94 34.1246344 -56.24 42.2282145 -55.53 42.3690246 -54.95 28.4090947 -54.29 41.1417348 -53.69 42.8904449 -53.1 40.8518950 -52.63 4051 -52.08 40.112252 -51.58 41.7604953 -51.11 43.6730154 -50.63 44.73976-900-800-700-600-500-400-300-200-100001020304050601 101 201 301 401 501 601 701 801 901Dev Train(b) CON vs Other1 -475.58 59.253842 -167.78 62.
1923 -158.7 62.418884 -150.99 71.324765 -143.16 66.07756 -135.89 59.298787 -129.95 57.020678 -124.59 40.040049 -119.7 53.45710 -115.08 55.8320411 -110.79 57.9277912 -106.82 56.642813 -103.11 53.0884814 -99.68 51.2775315 -96.55 45.9136816 -93.65 47.8008317 -91.01 46.4150918 -88.61 54.8956719 -86.4 53.9344320 -84.37 49.9136421 -82.5 48.9285722 -80.76 44.0438923 -79.2 49.9549124 -77.81 56.5610925 -76.43 50.3964826 -75.2 49.7427127 -74.09 57.3161528 -72.99 53.3543829 -71.98 50.3025130 -71.06 56.9005431 -70.23 58.1196632 -69.39 50.7640133 -68.6 52.676334 -67.87 53.7 89535 -67.21 53.23 536 -66.56 53.5298737 -65.93 56.7771138 -65.33 55.2250239 -64.78 52.3303440 -64.27 54.6431341 -63.75 54.0455642 -63.25 54.6448143 -62.77 51.954444 -62.35 55.7427345 -61.93 57.1854146 -61.55 55.2870147 -61.12 54.6982448 -60.8 55.3962349 -60.41 55.0239250 -60.08 54.0497251 -59.77 54.6912652 -59.43 52.5203353 -59.13 55.8754954 -58.82 56.77711-500-450-400-350-300-250-200-150-100-500010203040506070801 101 201 3 1 401 501 601 701 801 901Dev Train(c) EXP vs Other1 -2621.6 10.062892 -197.63 10.515 53 -193.15 11.111114 -198.57 0.
79825 -315.79 11.533056 -244.37 6.9841277 -264.03 11.235968 -301.25 10.970469 -188.89 4.18118510 -186.88 011 -187.06 012 -184.16 1.89274413 -182.4 1.36986314 -180.22 9.92430615 -178.27 10.0810116 -176.39 5.23560217 -174.71 018 -173.06 019 -171.49 020 -169.66 021 -167.42 022 -164.95 023 -162.28 024 -159.85 025 -157.64 026 -155.43 027 -153.12 028 -150.95 029 -148.96 030 -146.91 031 -145.03 032 -143.22 033 -141.17 034 -139.52 035 -137.63 036 -135.8 037 -134.07 10.580238 -132.44 2.15827339 -130.8 10.943440 -129.51 3.88349541 -127.7 12.1687942 -126.55 21.5753443 -124.58 10.9025944 -123.23 10.6571945 -121.78 10.8444446 -120.3 2.23463747 -118.94 11.3886148 -117.44 11.5183249 -116.18 4.47284350 -114.76 11.7163451 -113.71 11.5501552 -112.34 11.634953 -111.18 11.6379354 -109.95 3.951368-3000-2500-2000-1500-1000-500005101520251 101 201 3 1 401 501 601 701 801 901Dev Train(d) TEM vs OtherFigure 4: Illustration of the variational lower bound (blue color) on the training set and F-score (brown color) on thedevelopment set.
Horizontal axis: the epoch numbers; Vertical axis: the F score for relation classification (left) andthe estimated average variational lower bound per datapoint (right).Features used in SVM are taken from the state-of-the-art implicit discourse relation recognitionmodel, including Bag of Words, Cross-ArgumentWord Pairs, Polarity, First-Last, First3, ProductionRules, Dependency Rules and Brown cluster pair(Rutherford and Xue, 2014).
In order to collect bagof words, production rules, dependency rules, andcross-argument word pairs, we used a frequency cut-off of 5 to remove rare features, following Lin etal.
(2009).4.3 Classification ResultsBecause the development and test sets are imbal-anced in terms of the ratio of positive and negativeinstances, we chose the widely-used F1 score as ourmajor evaluation metric.
In addition, we also pro-vide the precision, recall and accuracy for furtheranalysis.
Table 2 summarizes the classification re-sults.From Table 2, we observe that the proposed VarN-DRR outperforms SVM on COM/EXP/TEM andSCNN on EXP/COM according to their F1 scores.Although it fails on CON, VarNDRR achieves thebest result on EXP/COM among these three mod-els.
Overall, VarNDRR is competitive in compar-ison with these two baselines.
With respect to theaccuracy, our model does not yield substantial im-provements over the two baselines.
This may be be-cause that we used the F1 score rather than the accu-racy, as our selection criterion on the developmentset.
With respect to the precision and recall, ourmodel tends to produce relatively lower precisionsbut higher recalls.
This suggests that the improve-ments of VarNDRR in terms of F1 scores mostlybenefits from the recall values.Comparing with the state-of-the-art results of pre-vious work (Ji and Eisenstein, 2015; Rutherford andXue, 2015), VarNDRR achieves comparable resultsin term of the F1 scores.
Specifically, VarNDRR out-performs Rutherford and Xue (2015) on EXP, and Jiand Eisenstein (2015) on TEM.
However, the accu-racy of our model fails to surpass these models.
Weargue that this is because both baselines use manymanual features designed with prior human knowl-edge, but our model is purely neural-based.Additionally, we find that the performance of ourmodel is proportional to the number of training in-stances.
This suggests that collecting more traininginstances (in spite of the noises) may be beneficialto our model.4.4 Variational Lower Bound AnalysisIn addition to the classification performance, the ef-ficiency in learning and inference is another concern388for variational methods.
Figure 4 shows the trainingprocedure for four tasks in terms of the variationallower bound on the training set.
We also provideF1 scores on the development set to investigate therelations between the variational lower bound andrecognition performance.We find that our model converges toward the vari-ational lower bound considerably fast in all exper-iments (within 100 epochs), which resonates withthe previous findings (Kingma and Welling, 2014;Rezende et al, 2014).
However, the change trend ofthe F1 score does not follow that of the lower boundwhich takes more time to converge.
Particularly tothe four discourse relations, we further observe thatthe change paths of the F1 score are completely dif-ferent.
This may suggest that the four discourse re-lations have different properties and distributions.In particular, the number of epochs when the bestF1 score reaches is also different for the four dis-course relations.
This indicates that dividing the im-plicit DRR into four different tasks according to thetype of discourse relations is reasonable and betterthan performing DRR on the mixtures of the fourrelations.5 Related WorkThere are two lines of research related to our work:implicit discourse relation recognition and varia-tional neural model, which we describe in succes-sion.Implicit Discourse Relation Recognition Due tothe release of Penn Discourse Treebank (Prasad etal., 2008) corpus, constantly increasing efforts aremade for implicit DRR.
Upon this corpus, Pilteret al (2009) exploit several linguistically informedfeatures, such as polarity tags, modality and lexicalfeatures.
Lin et al (2009) further incorporate con-text words, word pairs as well as discourse parseinformation into their classifier.
Following this di-rection, several more powerful features have beenexploited: entities (Louis et al, 2010), word em-beddings (Braud and Denis, 2015), Brown clusterpairs and co-reference patterns (Rutherford and Xue,2014).
With these features, Park and Cardie (2012)perform feature set optimization for better featurecombination.Different from feature engineering, predictingdiscourse connectives can indirectly help the rela-tion classification (Zhou et al, 2010; Patterson andKehler, 2013).
In addition, selecting explicit dis-course instances that are similar to the implicit onescan enrich the training corpus for implicit DRR andgains improvement (Wang et al, 2012; Lan et al,2013; Braud and Denis, 2014; Fisher and Sim-mons, 2015; Rutherford and Xue, 2015).
Very re-cently, neural network models have been also usedfor implicit DRR due to its capability for represen-tation learning (Ji and Eisenstein, 2015; Zhang et al,2015).Despite their successes, most of them focus on thediscriminative models, leaving the field of genera-tive models for implicit DRR a relatively uninvesti-gated area.
In this respect, the most related work toours is the latent variable recurrent neural networkrecently proposed by Ji et al (2016).
However, ourwork differs from theirs significantly, which can besummarized in the following three aspects: 1) theyemploy the recurrent neural network to represent thediscourse arguments, while we use the simple feed-forward neural network; 2) they treat the discourserelations directly as latent variables, rather than theunderlying semantic representation of discourses; 3)their model is optimized in terms of the data likeli-hood, since the discourse relations are observed dur-ing training.
However, VarNDRR is optimized un-der the variational theory.Variational Neural Model In the presence of con-tinuous latent variables with intractable posteriordistributions, efficient inference and learning in di-rected probabilistic models is required.
Kingma andWelling (2014) as well as Rezende et al (2014)introduce variational neural networks that employan approximate inference model for intractable pos-terior and reparameterized variational lower boundfor stochastic gradient optimization.
Kingma etal.
(2014) revisit the approach to semi-supervisedlearning with generative models and further developnew models that allow effective generalization froma small labeled dataset to a large unlabeled dataset.Chung et al (2015) incorporate latent variables intothe hidden state of a recurrent neural network, whileGregor et al (2015) combine a novel spatial atten-tion mechanism that mimics the foveation of humaneyes, with a sequential variational auto-encodingframework that allows the iterative construction of389complex images.We follow the spirit of these variational models,but focus on the adaptation and utilization of themonto implicit DRR, which, to the best of our knowl-edge, is the first attempt in this respect.6 Conclusion and Future WorkIn this paper, we have presented a variational neuraldiscourse relation recognizer for implicit DRR.
Dif-ferent from conventional discriminative models thatdirectly calculate the conditional probability of therelation y given discourse arguments x, our modelassumes that it is a latent variable from an underly-ing semantic space that generates both x and y. Inorder to make the inference and learning efficient,we introduce a neural discourse recognizer and twoneural latent approximators as our generative and in-ference model respectively.
Using the reparameteri-zation technique, we are able to optimize the wholemodel via standard stochastic gradient ascent algo-rithm.
Experiment results in terms of classificationand variational lower bound verify the effectivenessof our model.In the future, we would like to exploit the utiliza-tion of discourse instances with explicit relations forimplicit DRR.
For this we can start from two direc-tions: 1) converting explicit instances into pseudoimplicit instances and retraining our model; 2) de-veloping a semi-supervised model to leverage se-mantic information inside discourse arguments.
Fur-thermore, we are also interested in adapting ourmodel to other similar tasks, such as nature languageinference.AcknowledgmentsThe authors were supported by National Natural Sci-ence Foundation of China (Grant Nos 61303082,61672440, 61402388, 61622209 and 61403269),Natural Science Foundation of Fujian Province(Grant No.
2016J05161), Natural Science Founda-tion of Jiangsu Province (Grant No.
BK20140355),Research fund of the Provincial Key Laboratoryfor Computer Information Processing Technology inSoochow University (Grant No.
KJS1520), and Re-search fund of the Key Laboratory for IntelligenceInformation Processing in the Institute of Comput-ing Technology of the Chinese Academy of Sciences(Grant No.
IIP2015-4).
We also thank the anony-mous reviewers for their insightful comments.ReferencesChloe?
Braud and Pascal Denis.
2014.
Combining nat-ural and artificial examples to improve implicit dis-course relation identification.
In Proc.
of COLING,pages 1694?1705, August.Chloe?
Braud and Pascal Denis.
2015.
Comparing wordrepresentations for implicit discourse relation classifi-cation.
In Proc.
of EMNLP, pages 2201?2211.Junyoung Chung, Kyle Kastner, Laurent Dinh, KratarthGoel, Aaron C. Courville, and Yoshua Bengio.
2015.A recurrent latent variable model for sequential data.In Proc.
of NIPS.Philipp Cimiano, Uwe Reyle, and Jasmin S?aric?.
2005.Ontology-driven discourse analysis for informationextraction.
Data & Knowledge Engineering, 55:59?83.Robert Fisher and Reid Simmons.
2015.
Spectral semi-supervised discourse relation classification.
In Proc.of ACL-IJCNLP, pages 89?93, July.Karol Gregor, Ivo Danihelka, Alex Graves, and DaanWierstra.
2015.
DRAW: A recurrent neural networkfor image generation.
CoRR, abs/1502.04623.Ryuichiro Higashinaka, Kenji Imamura, Toyomi Me-guro, Chiaki Miyazaki, Nozomi Kobayashi, HiroakiSugiyama, Toru Hirano, Toshiro Makino, and Yoshi-hiro Matsuo.
2014.
Towards an open-domain conver-sational system fully based on natural language pro-cessing.
In Proc.
of COLING, pages 928?939.Yangfeng Ji and Jacob Eisenstein.
2015.
One vector isnot enough: Entity-augmented distributed semanticsfor discourse relations.
TACL, pages 329?344.Yangfeng Ji, Gholamreza Haffari, and Jacob Eisenstein.2016.
A latent variable recurrent neural networkfor discourse-driven language models.
In Proc.
ofNAACL, pages 332?342, June.Diederik P. Kingma and Jimmy Ba.
2014.
Adam:A method for stochastic optimization.
CoRR,abs/1412.6980.Diederik P Kingma and Max Welling.
2014.
Auto-Encoding Variational Bayes.
In Proc.
of ICLR.Diederik P. Kingma, Shakir Mohamed, Danilo JimenezRezende, and Max Welling.
2014.
Semi-supervisedlearning with deep generative models.
In Proc.
ofNIPS, pages 3581?3589.Man Lan, Yu Xu, and Zhengyu Niu.
2013.
LeveragingSynthetic Discourse Data via Multi-task Learning forImplicit Discourse Relation Recognition.
In Proc.
ofACL, pages 476?485, Sofia, Bulgaria, August.390Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng.
2009.Recognizing implicit discourse relations in the PennDiscourse Treebank.
In Proc.
of EMNLP, pages 343?351.Annie Louis, Aravind Joshi, Rashmi Prasad, and AniNenkova.
2010.
Using entity features to classify im-plicit discourse relations.
In Proc.
of SIGDIAL, pages59?62, Tokyo, Japan, September.Eleni Miltsakaki, Nikhil Dinesh, Rashmi Prasad, AravindJoshi, and Bonnie Webber.
2005.
Experiments onsense annotations and sense disambiguation of dis-course connectives.
In Proc.
of TLT2005.Joonsuk Park and Claire Cardie.
2012.
Improving Im-plicit Discourse Relation Recognition Through Fea-ture Set Optimization.
In Proc.
of SIGDIAL, pages108?112, Seoul, South Korea, July.Gary Patterson and Andrew Kehler.
2013.
Predictingthe presence of discourse connectives.
In Proc.
ofEMNLP, pages 914?923.Emily Pitler, Mridhula Raghupathy, Hena Mehta, AniNenkova, Alan Lee, and Aravind K Joshi.
2008.
Eas-ily identifiable discourse relations.
Technical Reports(CIS), page 884.Emily Pitler, Annie Louis, and Ani Nenkova.
2009.
Au-tomatic sense prediction for implicit discourse rela-tions in text.
In Proc.
of ACL-AFNLP, pages 683?691,August.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind K Joshi, and Bonnie LWebber.
2008.
The penn discourse treebank 2.0.
InLREC.
Citeseer.Danilo Jimenez Rezende, Shakir Mohamed, and DaanWierstra.
2014.
Stochastic backpropagation and ap-proximate inference in deep generative models.
InProc.
of ICML, pages 1278?1286.Attapol Rutherford and Nianwen Xue.
2014.
Discover-ing implicit discourse relations through brown clusterpair representation and coreference patterns.
In Proc.of EACL, pages 645?654, April.Attapol Rutherford and Nianwen Xue.
2015.
Improv-ing the inference of implicit discourse relations viaclassifying explicit discourse connectives.
In Proc.
ofNAACL-HLT, pages 799?808, May?June.Suzan Verberne, Lou Boves, Nelleke Oostdijk, and Peter-Arno Coppen.
2007.
Evaluating discourse-based an-swer extraction for why-question answering.
In Proc.of SIGIR, pages 735?736.Xun Wang, Sujian Li, Jiwei Li, and Wenjie Li.
2012.
Im-plicit discourse relation recognition by selecting typ-ical training examples.
In Proc.
of COLING, pages2757?2772.Yasuhisa Yoshida, Jun Suzuki, Tsutomu Hirao, andMasaaki Nagata.
2014.
Dependency-based discourseparser for single-document summarization.
In Proc.
ofEMNLP, pages 1834?1839, October.Biao Zhang, Jinsong Su, Deyi Xiong, Yaojie Lu, HongDuan, and Junfeng Yao.
2015.
Shallow convolutionalneural network for implicit discourse relation recogni-tion.
In Proc.
of EMNLP, September.Zhi-Min Zhou, Yu Xu, Zheng-Yu Niu, Man Lan, Jian Su,and Chew Lim Tan.
2010.
Predicting discourse con-nectives for implicit discourse relation recognition.
InProc.
of COLING, pages 1507?1514.391
