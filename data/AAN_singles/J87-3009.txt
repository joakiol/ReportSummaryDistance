THE SELF-EXTENDING PHRASAL LEXICON*Uri Zernik**Michael G. DyerComputer  Science Depar tmentUniversity of Cal i forniaLos  Angeles, Cal i fornia 90024Lexical representation so far has not been extensively investigated in regard to language acquisition.Existing computational linguistic systems assume that text analysis and generation take place inconditions of complete lexical knowledge.
That is, no unknown elements are encountered in processingtext.
It turns out however, that productive as well as non-productive word combinations require adequateconsideration.
Thus, assuming the existence of a complete lexicon at the outset is unrealistic, especiallywhen considering such word combinations.Three new problems regarding the structure and the contents of the phrasal lexicon arise whenconsidering the need for dynamic acquisition.
First, when an unknown element is encountered in text,information must be extracted in spite of the existence of an unknown.
Thus, generalized lexical patternsmust be employed in forming an initial hypothesis, in absence of more specific patterns.
Second, sensesof single words and particles must be utilized in forming new phrases.
Thus the lexicon must containinformation about single words, which can then supply clues for phrasal pattern analysis andapplication.
Third, semantic lues must be used in forming new syntactic patterns.
Thus, lexical entriesmust appropriately integrate syntax and semantics.We have employed a Dynamic Hierarchical Phrasal Lexicon (DHPL) which has three features: (a) lexicalentries are given as entire phrases and not as single words, (b) lexical entries are organized as ahierarchy by generality, and (c) there is not separate body of grammar ules: grammar is encoded withinthe lexical hierarchy.
A language acquisition model, embodied by the program RINA, uses DHPL inacquiring new lexical entries from examples in context hrough a process of hypothesis formation anderror correction.
In this paper we show how the proposed lexicon supports language acquisition.1.
INTRODUCTIONExamination of the language acquisition task sheds lighton the nature of the lexicon, illuminating issues whichhave been ignored by existing linguistic systems\[Wilks75, Kay79, Bresnan82b, Gazdar85\].
Current sys-tems restrict heir account o analysis and generation oftext, by making the assumption that a fixed, completelexicon exists at the outset.
This assumption provesunrealistic for two reasons: First, due to the huge size ofthe lexicon (especially when including idioms andphrases) it is difficult to manually encode the entirelexicon.
This problem is further aggravated as people*This research was supported in part by a grant from the InitialTeaching Alphabet (ITA) Foundation.
**Uri Zernik's new address is: General Electric, Research andDevelopment Center, P.O.
Box 8 Schenectady NY 12301.continuously invent new idiosyncratic word combina-tions, which are then introduced into general speech.Second, word meanings must often be custom tailoredto the domain (e.g., bug in computer applications), sincepeople assign different meanings to words in variousjargons.
Therefore, computational linguistic models arerequired to learn lexical items in context, the waypeople learn new words and phrases.Learning commonly occurs when the learner detectsa gap in his or her knowledge.
In analysis, such adiscrepancy can be detected when a new word or phraseis encountered.
Learning involves three issues: (a)detecting the discrepancy in the first place, (b) formingan initial hypothesis about the new phrase, and (c)refining and generalizing this hypothesis through a proc-ess of error correction \[Granger77, Langley82,Selfridge82, Zernik85b\].
These three issues impose newrequirements on the lexicon, regarding (a) itsCopyright 1987 by the Association for Computational Linguistics.
Permission tocopy without fee all or part of this material isgranted providedthat the copies are not made for direct commercial dvantage and the CL reference and this copyright notice are included on the first page.
Tocopy otherwise, or to republish, requires afee and/or specific permission.0362-613X/87/030308-327503.00308 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987Uri Zernik and Michael G. Dyer The Self-Extending Phrasal Lexiconcontents-the way individual entries are encoded, and(b) its structure-the way entries are organized.The need to detect discrepancies affects the contentsof the lexicon.
Both semantic and syntactic discrepan-cies must be detected, and correction strategies must beassociated with various types of errors.
Thus, lexicalentries hould not be underspecified, lest they will allowdiscrepancies to slip by unnoticed.The need to generalize affects the structure of thelexicon.
In order to make an initial hypothesis about anew element, it is important to glean from the text asmuch information as possible.
This requirement is prob-lematic: the text cannot be analyzed since an element isunknown; but on the other hand, for the element to beacquired, the text must be analyzed.
The solution forthis bootstrapping problem is to employ a lexical hier-archy by generality.
When a specific pattern does notexist for a precise matching against he new element,one can apply a more general pattern, which albeit beingless informative, does match the new element.Thus, we propose mploying a Dynamic HierarchicalPhrasal Lexicon (DHPL) which has three features: (a)lexical entries are given as entire phrases and not assingle words, (b) phrases are organized in a hierarchy bygenerality, and (c) there is not separate grammar; gram-mar is encoded in general lexical phrases.
The programRINA \[Zernik86b, Zernik87a\] employs DHPL in mod-eling language acquisition.
In particular, the programmodels second language acquisition of English phrasesand idioms.
The linguistic oncepts being acquired arecomplex enough, so that neither a human learner, nor acomputer program can acquire their complete behaviorthrough a single example.
Thus the initial hypothesismight be incorrect.
Capturing incorrect hypotheses gen-erated by humans, and simulating them by the computerprogram is essential for practical and theoretical rea-sons.
First, the human user of the program will relate tothe human-like rrors generated by the program.
Con-sequently he may present the program with constructivecounterexamples.
Second, human errors, such as errorsof overgeneralization, reveal otherwise inaccessiblecognitive processes and internal structures.
Thus, er-rors made by human learners play a central role inconstructing a cognitive model of acquisition.
Subse-quently, observed human behavior is analyzed in termsof the computer program RINA.1.1 THE LINGUISTIC BEHAVIORRINA receives examples from a user who teaches hernew phrases.
When RINA encounters a new phrase,she creates ahypothesis about its behavior, and accord-ingly she generates an example to demonstrate h r stateof knowledge.
Communication between the programand the user is only through a sequence of examples -there is no way to discuss yntax and semantics explic-itly.
(1) LEARNING NEW PHRASESIn the following dialog, RINA encounters an unknownphrase, throw the  book a t  somebody.User: AI Capone went on trial.The judge threw the book at him.RINA: He threw a book at him?User: No.
The judge threw the book at him.RINA: He punished him severely?RINA is familiar with the single words throw and book.However, the entire figurative phrase is not in herlexicon.
RINA first attempts a literal interpretationusing a phrase existing in the lexicon (throw an object).When this interpretation fails she realizes the existenceof an unknown, and tries to form the meaning of the newphrase by using (a) the context, and (b) the single wordsin the phrase.
(2) PROCESSING AN UNKNOWNIn the next dialog, RINA encounters a new word,goggled.User:RINA:Jenny goggled John to come over.Jenny told John that he must/can/will come toher.RINA manages to extract useful information from thesentence in spite of the missing element.
In particular,RINA's hypothesis ncludes three points:(a) Jenny's unknown act is a kind ofmtrans* (an act ofcommunication).
(b) The actor of the communicated event (comingover) is John.
(c) The communicated vent is a conditional plan forthe future (in contrast, for example, to the case:Jenny goggled John that he came over).Normally, properties of an embedded phrase (e.g., tocome over) are determined by the definition of theembedding verb.
RINA manages to make an initialhypothesis even though the embedding verb (goggle) isunknown, by using generalized knowledge of phraseinteraction.
(The structure of a sentence: Personl gog-gled Person2 to do Act3 implies mtrans such as ask,tell and instruct, in contrast to Personl goggled to doAct2 which implies an mbuild, such as decide).
Thehypothesis must be abstract, since RINA cannot deter-mine at this point whether this mtrans act comes in thesense of allow (can come over), or instruct (must comeover).
Yet, even this hypothesis may turn out to beincorrect.
For example, goggle could mean seduce, orinfluence in some other way.
In either, it is important tocome up with a hypothesis which provides a basis forfurther modification.
*Conceptual c asses uch as mtrans, mbuild, select-plan, are based onsemantic representation.
Several of these elements are taken fromSchank's \[Schank77\] system of primitive acts, goals and plans.Mtrans for example represents he transfer of mental information, andmbuild represents he construction of new information in memory.The particular scheme chosen is not so important as the fact thatsyntactic lasses (such as verbs) are organized phrasally in terms ofconceptual categories.Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 309Uri Zernik and Michael G. Dyer The Self-Extending Phrasal Lexicon(3) RESOLVING AN AMBIGUITYAS with human listeners, computer parsers must also beable to interpret ext successfully only when suppliedthe appropriate context \[Zernik86a\].
Consider the fol-lowing sentence:User: She took it up with her dad.RINA: ?Imagine a person hearing a fragment of a conversationbetween two unknown people, or alternatively, a com-puter program being given this sentence in isolation.Clearly, in the absence of a context, this sentence doesnot make complete sense.
The pronouns, she and itcannot be resolved in absence of referents which havebeen introduced in the discourse.
In addition, the samephrase will mean different hings in different contexts.Consider these two examples.plan (John decided to go home).
This behavior does notcapture verbs such as suggest, require or ten  (Johntold to go sounds incorrect).
The speaker faced ageneration task in presence of incomplete l xical knowl-edge about suggest and require, and he resorted tousing generalized knowledge.
Using such knowledge,an idea could be communicated, albeit grammaticallyincorrectly.Therefore, the lexicon must maintain phrases atvarious level of generality, to cope with different de-grees of partial knowledge.
(2) USING LINGUISTIC CLUESMeaning representation is extracted from the context.For example, given the text below,AI Capone went on trial.The judge threw the book at him.User: Jenny wanted to buy a new car.She took it up with her dad.RINA: She discussed the issue with her dad.User: Jenny started jogging.She took it up with her dad.RINA: She started an activity with him.Since the same sentence can be interpreted in two waysin two different contexts, a question is raised regardingdisambiguation.
What is the impact of the context onphrase selection?1.2 ISSUES IN LANGUAGE ACQUISITIONThree lexical representation issues must be addressed inmodeling language acquisition.
(1) USING GENERALIZATIONSAs shown in the sentence below,Jenny goggled John to come over.the system must cope with unknown elements.
Parts ofthe text must be examined to some extent, in spite of thepresence of the unknown.
Ideally, each element in thetext is matched by a lexical phrase.
Since no suchphrase exists for a precise matching of the unknownelement, a generalized phrase must be used to recoverat least partial information.
However, by the nature ofgeneralization, the more generalized the matchingphrase, the less informative it is.Typical errors of overgeneralization were generatedin a version of this paper by the first author, who is asecond language speaker:?
The third phrase requires to generalize the initialnotion.
(Section 6)?
Wilensky suggested to represent knowledge as adatabase of rules.
(Section 3.2)In both cases, the learner applied the wrong generalizedphrase, which accounts for verbs such as decide andRINA guessed that throw the  book a t  somebody meansto punish that person severely.
However, the contextmight consist of many concepts, some appropriate andsome inappropriate ( .g.
: did the judge acquit AI or didhe punish him?).
Thus, a basic task is feature extrac-tion.
In extracting features, the system must utilizeclues provided by single words.
For example, what isthe significance of the particle at?
How does it contrib-ute to the construction of the meaning?
An experimentwith second language speakers reveals, predictably,that using a different preposition leads to a differentlearning result.
When the given text is:AI Capone went on trial.The judge threw the book to him.language learners formed the hypothesis that the judgeactually acquited the defendant.
Thus, the lexicon mustmaintain senses for single words such as at and to thatcould be used as linguistic lues in feature xtraction.
(3) USING SEMANTIC CLUESThe system must hypothesize the scope and variabilityof the new phrases.
Which one of the phrases belowbest captures the syntax of the new phrase: the judgethrew the book at him?He threw something at him.He threw a book at him.He threw the book at him.Each one of these patterns could be the specification ofthe new phrase.
In determining degree of specificity thesystem must consult semantic lues extracted uringparsing.
For example, since no actual book exists in thecontext, then the reference the book is assumed to be afixed literal.
In contrast, consider the context below:The judge was holding the third volume of tax law.He threw the book at AI.310 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987Uri Zernik and Michael G. Dyer The Self-Extending Phrasal LexiconIn this context, an instance of a book is found in thecontext (i.e., the third volume), and a different hypoth-esis is made about he the generality of the new pattern.Thus, semantic discrepancies in parsing must be utilizedin determining both scope and generality of syntacticpatterns.2.
ACCOUNTING FOR IDIOMACITY IN THE LEXICONWhat are the contents of the lexicon to be acquired?Traditionally, the lexicon has been viewed as a list ofwords, specifying syntactic and semantic properties foreach entry.
However, since in our theory, the lexiconprovides the sole linguistic database, it must include avariety of linguistic knowledge types, not just propertiesof single words.
Here the lexicon is extended in twoways: towards the specific by bringing in idioms, andtowards the general by including rammar.2.1 IDIOMS AS EQUAL CITIZENSAre idioms, such as throw the book at, a class apart, tobe distinguished from "normal" phrases, which abideby grammar rules?
The first to proclaim "equal rights"for idioms was Becker \[Becker75\], who called for asystematic treatment for the variety of phrases in thelanguage.
Consider these phrases:We will be looking forward to seeing you guys.He is cheap.
He will not pay $5 let alne $8.So much for superficial solutions.Productive as well as non-productive phrasesshould reside in the lexicon.These phrases defy traditional text-book grammar anal-ysis, however, they possess their own grammar.
Forexample, it sounds odd to say lie is cheap;  He will notpay $8 let alne $5 \[Fillmore87\].
(Is the behavior of aswell as analogous to the behavior of let alne?)
Suchlinguistic phenomena cannot be ignored merely bytagging it as idiomatic, since idioms turn out to beubiquitous in people's peech.
Hardly can a sentence befound which behaves according to textbook grammar.There is a need therefore for a systematic treatment ofidiosyncracy \[Fillmore87\].
Furthermore, linguisticknowledge cannot be strictly divided into grammar rulesand lexical items.
Rather, there is an entire range ofitems: some very specific, in the sense that they pertainto a small number of instances, and some very general,pertaining to a large number of instances.
The formerhave been called "lexical items", and the latter"grammar rules".
However, it is not possible to definea clear borderline between such two distinct groups, aselements could be found at all levels of generality, notjust at the two ends of the spectrum.
On one end, thephrase it is raining cats and dogs is very idiomatic.On other end, the phrase in John took the spoon fromUary is an instance of a general verb, to take, whichmay appear in many other ways.
However, consider thephrase John took the  i ssue  up w i th  h i s  dad.
Is this anidiom, or is it just an instance of the general verb totake?2.2 PRODUCTIVE VS. NON-PRODUCTIVE PHRASESIn the phrasal approach \[Wilensky84\] rather than main-taining lexical entries for single words, the lexiconmaintains entire phrases.
For example, the lexicon willcontain many phrases involving the word throw.
Con-sider these phrases as they appear in the followingsentences.
(I) He threw her off by a single inaccurate clue.
(2) He threw a wild party for her graduation.
(3) He threw up his whole breakfast.
(4) He threw his weight around.
(5) He threw a temper tantrum.
(6) He threw a stone at the kitchen window.
(7) He threw out that old chapter of his dissertation.
(8) He threw out the garbage.
(9) He threw the banana peel away.
(10) He threw in the towel.
(11) He threw the book at his students.
(12) He threw it.
His answer was totally incorrect.To a certain extent, all the phrases above derive theirmeanings from the meaning of the verb to throw.However, the issue here is whether a single genericlexical entry for throw can suffice to produce the mean-ings of all those sentences.
In example (6) (he threw astone), the phrase for throw is used in its generic formand meaning: to throw a physical object means to propelthat object hrough the air.
Sentence (9) (he threw awaya banana peel) too can be interpreted using the genericphrase.
In sentence (8) (he threw out the garbage), onthe other hand, the derivation of the meaning using thegeneric phrase is less direct, as it requires analysis at thelevel of plans and goals.
Throwing an object causes theobject to become inaccessible.
Thus throwing out thegarbage does not necessarily mean throwing it in the airas much as getting rid of it.The meanings of the other sentences are even moredetached from the generic meaning.
The meaning ofthrow the book (11) at is not a mere composition of themeanings of the single words, but requires extraneousknowledge from the trial situations.
Neither a person,nor a computer program can produce the meaning of thephrase if the context is not given.
Sentence (4) (he threwhis weight around) introduces a metaphor \[LakoffS0\] inwhich a person's authority is compared to a weight,being used in a careless way.
Sentence (2) (he threw aparty) as well as sentence (5) (he threw a tempertantrum), use a different meaning of throw (to throw anevent) which can hardly be related to its original mean-ing.
Finally, sentence (12) (he threw it) represents anovel, yet still understandable, use of the word throw(as in he blew it).Non-productive phrases are those in which the mean-ing of the entire phrase cannot be produced from themeanings of its constituents.
Such phrases should bemaintained in the lexicon as distinct entries.
In fact,Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 311Uri Zernik and Michael G. Dyer The Self-Extending Phrasal Lexiconeven productive phrases, such as to throw out thegarbage, should be maintained as distinct entries.
Evenif the meaning can be produced each time from thesingle words, an objective of an efficient system is tocompile knowledge whenever possible, and to minimizeunnecessary derivations.
Thus, phrases in the lexiconcan be viewed as linguistic episodes indexed and com-piled for further use.
Such knowledge is redundant inregard to language parsing (the meaning could be de-rived from the constituents again and again).
However,this is not the case in language generation, where unlessthe phrase is stored, it is unlikely to be generated againby the system.
Thus, both productive and non-produc-tive phrases must be stored in the lexicon.2.3 FIXED VS.
VARIABLE PHRASESAs another example of lexical phrases, consider phrasesinvolving the word at:(13) John left school at noon.
(14) He actually stayed at school for an hour.
(15) He dabbled at the piano for a while.
(16) John aimed the ball at Mary.
(17) The criminal is still at large.
(18) Mary did not feel at ease in John's presence.
(19) This is what I am trying to get at.
(20) Did you understand anything at all?
(21) Please come at once!
(22) John looked at Mary.
(23) Fred lives at New-York.
(produced by a secondlanguage speaker.
)Certain phrases are fixed, in the sense that they do nottake any variation.
For example, at l a rge ,  at al l ,  or atonce are such fixed phrases.
One cannot say, for exam-ple, at twice.
However, other phrases might be mu-tated and still maintain their basic meaning.
For exam-ple, at noon, at midn ight ,  at the hour ,  etc., convey ameaning of sharp timing.
Another meaning sharedamong a set of phrases is described by the followingsentences:(15) He dabbled at the piano for a while.
(24) He nibbled at the corn.
(25) He is playing at AI programming.The use of the proposition at here implies an aimless,unfocused activity marking the difference between play-ing the piano and playing at the piano.
Similarly, the setof sentences:(22) John looked at Mary.
(26) Spot sniffed at Mary.
(27) Mary glanced at John.share the implication that the sensory act was directedat the object.Which ones of these phrases hould be maintained inthe lexicon?
Fixed, idiosyncratic phrases such as atlarge, at once, and at al l  must be maintained in thelexicon.
Otherwise they cannot be predicted by thesystem.
However, the dilemma arises regarding vari-able phrases, such as in (22), (26) and (27).
The questionis whether to maintain all instances of a certain variablephrase or to maintain a single generalized entry whichencompasses them all.
We argue that both must bemaintained.
Specific phrases must be maintained ascompiled, easy to access knowledge, while generalphrases, which can derive many specific phrases, mustbe maintained too so that the system has a predictivepower.
Using such generalized phrases, the system canhandle instances which have not been previously en-countered.In fact, specific "canned" phrases could not accountfor the following generation task, concerning the selec-tion of appropriate prepositions in the followingsentences:(28) There is one teacher {in on at} our school, which Ireally like.
(29) I stayed late {in on at} school.Notice that since both sentences involve the wordschool, it could not be used as a discriminator.
Unlessthe lexicon maintains general predicates for the use ofin, at, and on, the generator cannot select the appro-priate preposition i  each case.
Clearly, it is difficult tocapture the intuition of a native speaker in forming thegeneral senses of these prepositions.
An approximationof this intuition can be captured by modeling a second-language speaker who might "incorrectly" generate asentence such as (23) above:(23) Fred lives at New York.Although it does not sound right to an English speaker,this sentence reflects the notion of that particularspeaker.2.40VERSPECIFICATION AND UNDERSPECIFICATIONLexical entries should not be either underspecified oroverspecified.
Unless the lexical phrases are fully spec-ified, they cannot serve in disambiguation.
On the otherhand, overspecification should also be avoided.
Indeed,in encoding lexicons there is a temptation to overspe-cify.
Consider the following pairs of examples in regardto lexical constraints:He kicked the bucket.Mary was taken by thecar dealer.He put his foot down.She laid down the law.He took on Goliath.The bucket was kicked.The car dealer took her.He put down his foot.She laid the law down.He took on him.There is a tendency to incorporate in the lexiconsyntactic restrictions which will prevent he instanceson the right.
For example, kick the bucket would bemarked as active-voice-only.
This is in contrast o thephrase bury the hatchet which maintain its figurativeflavor also in the passive voice: the hatchet was buriedby Israel and Egypt.We believe that this behavior is not dictated by an312 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987Uri Zernik and Michael G. Dyer The Self-Extending Phrasal Lexiconarbitrary, ad hoc syntactic restriction, rather it reflectsthe conceptual representation f the phrase as it hasbeen shaped in the acquisition process \[Zernik87b\].
Theacquisition of the phrase bury the hatchet was based ona metaphor, and generalized from single-word mean-ings.
Bury was generalized into disenable-use, and thereferent he hatchet was generalized to a tool, theavailability of which is a precondition for an activeconflict.
Therefore, the reference the hatchet stands fora certain generalized object.
On the other hand, kickthe bucket was learned as a whole chunk, since theunderlying metaphor remained unresolved.
Thus, thereferent he bucket is maintained as a literal not asso-ciated with any concept.
Due to this difference, theremay arise a discourse function for passivizing bury thehatchet.
However, since there is no referent for thebucket, there will never occur the need to passivize thatphrase.
Therefore, marking the phrase pattern as active-voice-only is redundant (albeit correct).Another issue is verb-modifier separation, i.e.
: Davidtook on Goliath vs.
He took him on.
How can thelexicon account for this separation phenomenon?
Agrossly overspecified rule claims that pronouns (andonly pronouns) separate such two-word verbs.
How-ever, there are counterexamples such as:He took that ugly giant on.
(where the separation is by a lengthy reference).
There-fore the rule must be revised to relate the phenomenonto given and new references.
A given, or an alreadyresolved reference, can separate, while a new referencecannot be placed between the verb andits modifier.
Webelieve that this behavior should not be specified by thelexicon, rather the generation decision is according todiscourse functions.Overspecified lexical entries can always be contra-dicted by instances in context.
In order to avoid thesuch contradictions we take the approach of maintain-ing syntactic specifications of lexical entries at appro-priate levels, and use conceptual representation toaccount for apparently syntactic restrictions.3.
LEXICAL REPRESENTATION: PREVIOUS WORKDHPL is a continuation ofefforts in three distinct areas.First, in integrating the underlying situation as part ofthe lexical entry, we extend previous work on lexicalpresupposition.
Second, we modify Wilensky's methodof lexical representation foruse in language acquisition.Third, we examine Bresnan's ystem of linguistic rep-resentation, which proves problematic in light of theacquisition task, and compare it to DHPL's representa-tion.presupposition of the utterance, is described by Keenan(1971) as follows*:The presuppositions of a sentence are those conditionsthat the world must meet in order for the sentence tomake literal sense.
Thus if some such condition is notmet, for some sentence S, then either S makes nosense at all or else it is understood in some nonliteralway, for example as a joke or metaphor.Despite this definition of presupposition asa conditionfor application of lexical knowledge, presupposition hasbeen studied as a means for generation and propagationof inferences, reversing its role as a condition.
In\[Gazdar79, Karttunen79, Keenan71\] the goal has beento compute the part of the sentence which is alreadygiven, by applying "backward" reasoning, i.e.
: fromthe sentence the king of France is bald determine ifindeed there is a king in France, or from the sentence itwas not John who broke the g lass ,  determine whethersomebody indeed broke the glass.
Rather than usingpresuppositions to develop further inferences, we inves-tigate how presuppositions are actually applied accord-ing to Keenan's definition above, namely, in determin-ing appropriate utterance interpretations.Fillmore \[Fillmore78\] introduced lexical presupposi-tion to describe situations in which lexical items mayappear.
He described the meanings of judgement wordssuch as accuse, c r i t i c i ze ,  blame, and pra i se ,  by sepa-rating the entire meaning into (a) a statement (theillocutionary act), and (b) a presupposition.
We illus-trate this distinction by comparing the meanings ofcr i t ic ize and accuse in the following sentences:(30) John criticized Mary for adjourning the meeting.
(31) John accused Mary of adjourning the meeting.In both sentences, John referred to a hypothetical ct,namely adjourning the meeting.
In (30), it is presupposedthat Mary committed the act (a test for determiningpresupposition is invariance under negation: John didnot criticize Mary of adjourning the meeting still impliesthat Mary committed the act), while it is stated that theact is judged negatively.
In (31), on the other hand, it isstated that Mary committed the act, while it is presup-posed that the act is negative.We believe Fillmore's approach is suitable also forthe task of language acquisition, since learning involvesfactoring out the statement of a phrase from the entiresurrounding context.
We have further pursued Fillmo-re's notion in utilizing lexical presupposition i  specifictasks such as disambiguation, i dexing, and accountingfor communicative goals \[Gasser86a\].Presupposition must be distinguished from precondi-tion.
Consider the following text.3.1 LEXICAL PRESUPPOSITIONA message might be conveyed by an utterance beyondits straightforward illocution.
That message, called theJohn ran into a pedestrian on a red light.He managed to explain it away in court.
*(See also \[Grice75\] and \[Fauconnier85\] Ch.
3)Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 313Uri Zernik and Michael G. Dyer The Self-Extending Phrasal LexiconThe lexical phrase under consideration is explain away.The presupposition for the application of the phrase isthe entire situation in which the phrase typically ap-pears.
A person is attempting to justify a certain plan-ning failure.
The precondition for the enablement oftheact, on the other hand, is a planning element from thedomain itself.
One precondition i the story above couldbe the judge's permission for John to stand up in courtand defend his own case.
Another trivial example is thesentence below.selection is by specificity, namely, the most specificphrase is selected.An additional layer was added to this work by Jacobs\[Jacobs85\] who noticed the need for inheritance andhierarchy in the lexicon.
Concepts in memory areorganized in a hierarchy of categories, through whichmore specific concepts can inherit features from moregeneral ones.
Concepts in the lexicon, namely lexicalitems, should be organized through the same generaldiscipline.
This approach enjoys three advantages:John threw a rock at Mary.There is no presupposition for the generic phrase personthrow phys-obj.
This phrase may appear in almost anycontext.
However, from a planning point of view, for aperson to throw a rock she must first grasp the rock inher hand.
In contrast o presupposition, such planninginformation should not reside in the lexicon.
In fact, anyinformation which could be derived by means of generalworld knowledge does not belong in the lexicon.Dyer \[Dyer83\] has described text comprehension asan integrated cognitive process.
Parsing, he claimed,cannot be separated from other cognitive tasks such asmemory update and retrieval.
Accordingly, search de-mons were introduced in lexical entries to performmemory retrieval.
For example, consider the differencebetween the two sentences.
(32) John made up his mind.
(33) He decided to go swimming.In parsing sentence (33) the selected plan, namely goingswimming, is mentioned explicitly.
However, in sen-tence (32) neither the plan nor the problem to beresolved are mentioned explicitly.
Therefore, a searchdemon associated with the phrase make up one' s mind isdispatched to retrieve from memory the problem underconsideration by the actor of the phrase.
One of theobjectives of DHPL's representation is to eliminatesuch procedural knowledge.
Lexical presuppositionserves the task of memory retrieval.
The mechanismswe use are unification and variable binding.3.2 LANGUAGE AS A KNOWLEDGE-BASED SYSTEMWilensky \[Wilensky81\] promoted the view of languageprocessing as a knowledge-based task.
Accordingly, hesuggested representing linguistic knowledge as a data-base of rules given at various levels of generality.
Thebasic representation element is called a phrase, given asa pattern-concept pair.
For example, the phrase in thesentence:?
Modularity: Adding a new entry does not require anyglobal modification.?
Declarativeness: The representation is neutral withrespect o parsing and generation.
The representationdoes not reflect any programming style (beyond basicslot-filler notation) and it does not reflect the mecha-nism of any particular parser.?
Uniformity: Modifying the level of generality of aphrase does not require a change of the phrase beyondthe single feature being updated (generalized orspecified).These properties make the system more amenable tomodeling language processing \[Kay79\] and acquisition\[Mitchell82\].3.3 LFG AND LANGUAGE ACQUISITIONBresnan's \[Bresnan82a\] linguistic representation, lexi-calfunctional grammar (LFG), is a system with a "flat"lexicon, which does not define a hierarchy of generali-zations.
LFG is contrasted here with DHPL's hierar-chical approach, and it is examined here in regard tolearning \[Pinker84\].
In LFG there are two lexical entriesrepresenting the word ask, as it appears in the followingsentences.
(34) John asked to leave.
(35) John asked Mary to leaveThe corresponding lexical entries are given respectivelybelow.ask: v:pred = "ask(sub j, v-comp)"subj = v-comp's ubj (subj-equi)ask: v:pred = "ask(sub j, obj,  v-comp)"obj = v-comp's ubj (obj-equi)Figure 1: LFG representation of ASKJohn dropped out of police academy.is given as the phrasepattern ?x:person drop out of ?y:schoolconcept goal of person ?x, pursue-education atinstitute ?y, terminated unsuccessfullyParsing is viewed as a process of rule (phrase) applica-tion.
When more than one rule is applicable (ambiguity),The meaning of ask is given as the predicate ask whichtakes either two or three arguments.
There is no generalnotion which captures the similarities in the behavior ofthe two specific entries.
In the hierarchical approach, onthe other hand, the behavior of ask is described in thebroader context of the infinitive interaction betweenphrases.
The schematic hierarchy is given in Figure 2below:314 Computational Linguistics, Volume 13, Numbers 3-4, July.December 1987Ur i  Zernik and Michael G. Dyer The Self-Extending Phrasal LexiconP !
equi-rule / \communication-verbstell promiseP2 askplanning-verbsFigure 2: ASK as Part of a Broader HierarchyAI Capone went on trial.The judge threw the book at him.The underlying knowledge is the the trial script, whichcaptures the basic events taking place in court.
(a) The Prosecutor communicates his arguments.
(b) The Defendant communicates his arguments.
(c) The Judge decides (select-plan) either:(1) Punish (thwart a goal of) Defendant.
(2) Do not punish him.Figure 3: The Acts in $TrialIn this scheme, there is a single phrase for ask (P2).
Thisphrase draws properties from a more general phrase(P1) which defines the general equi rule in complement-taking English verbs.
In this representation, the behav-ior of ask is inherited from the general phrase P1 andthere is no need to duplicate specificcases.LFG current theory does not facilitate such hierar-chies.
In absence of hierarchy and inheritance, there isa need for duplication of the learning effort which canlead to serious flaws in modeling human behavior.
Forexample, the word promise presents an exception to thegeneral equi rule.
Consider John promised  Mary to go, incontrast o John asked Mary %o go.
The latter impliesthat John is the actor of the future act of going (Johnpromised that he will go, but John asked that Mary go).In learning this behavior of promise, children make anerror by hypothesizing the default equi rule, thus com-mitting an error of overgeneralization (a child mightsay: Dad promised Tommy to drive the big car alonemeaning "Tommy will drive the car").
In LFG it isimpossible to model this behavior since generalizationsdo not exist.
Indeed, Pinker \[Pinker84\] accounted forthis error, but the equi rule he resorted to is not part ofthe LFG system itself.
Moreover, through LFG it isimpossible to recover from overgeneralization.
Nor-mally people recover from overgeneralizations by beinggiven a counterexample (No.
Dad promised Tommy totake him to Disneyland).
However, since neither Bres-nan nor Pinker attempt to represent meanings of wordssuch as take and drive - -  the meanings are actuallyrepresented as the symbols "take" and "drive" - it isimpossible to make the necessary semantic inferencesfor error recovery.
Thus, without the ability to gener-alize and without an appropriate representation f con-cepts, LFG as currently defined, cannot account forthese behaviors in learning.This script, as shown in Figure 3, consists of a sequenceof four events, in which the characters are the judge, theprosecutor, and a defendant.
In addition, there isknowledge of the character's goals.
The prosecutor isinterested in thwarting apreservation goal - p-freedom,p-property of the defendant.
The defendant attempts toblock this goal thwart.
Both parties advance their casesby trying to convince the judge.
By this representationthe meaning of the phrase to throw the book at some-body means to punish him severely, based on events (a)and (1) in the script.Another situation, involving the same script, is pre-sented in the following text.John ran over a pedestrian.He failed to explain it away in court,and he went to jailIn this case the phrase explain away pertains to theunderlying oal-plan situation, given in Figure 4 below.John experienced a planning-failure (failed plan of driv-driving accidentcoming late ~failureJ judgewife (authority'~L calua.,~P'~'~ J~ 'execute  J.p lan-b lock  " - ' - - - "~ ~;un\ ]sOe~n " goal-thwartargue fpresewatiO~reserve licensek goal ~reserve relationFigure 4: The Goal-Plan Structure for explain away4.
REPRESENTING THE CONTEXTThe semantics of entries in the lexicon draw from thevarious contexts in which they have been applied.
Herewe represent contexts using scripts, plans, goals, andrelationships \[Schank77, Dyer83, Dyer86b\].
Considerthe context in reading the text:ing safely).
John's preservation goal of freedom isthreatened.
A plan for preserving this goal is convincingthe judge as to why John himself was not at fault.
Thissecond plan is executed and it fails also.
Thus, hisp-goal fails.Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 315Uri Zernik and Michael G. Dyer The Self-Extending Phrasal Lexicon.Notice that the same goal-plan schema exists also inthe case of the next story:Joe forgot to put away the dirty dishes.When his wife came home, he argued it awayby telling her he had been working.5.1 BASIC PHRASE STRUCTUREConsider the marked clause in the following text.For years they tried to prosecute A1 Capone.Finally, a judge threw the book at himfor income-tax evasion.The phrase argue away also involves a prior plan failure,a thwarted p-goal (p-social-relation) and a recovery planof convincing the other party.
This underlying schema isa presupposition.
It holds whether Joe fails to argue itaway or whether he manages to argue it away.
Since thesame plan-goal schema underlies both phrases (up to thespecific plan: argue vs. explain), they both can beviewed as instances of a more general phrase.Many other phrases draw their meanings in terms ofsuch general plan-goal structures.
Consider the phrasesin the next sentences:This machine was idling away for hours.They stayed at home, and argued away for hours.The class was boring.
John sat near the windowdreaming away.In all these sentences there is a similar underlyingsituation, shown in Figure 5 below.play work .
~_ dream number cruncA .v.
Iechieve ~onlllCt~/ achieveFigure 5: The Goal-Plan Structure for idle awayIn this schema resource competition (the resource istime) exists for an agent between two competing tasks,and that agent subordinates the important goal.The fact that phrase representation can be elevatedto a level of general plans and goals is very significant.It implies that a relatively small number of structurescan represent phrases whose instances can be usedacross many domains.This clause is derived from a lexical phrase which isgiven as the following simplified template:phrase pattern:presupposition:concept:Personl throw the book at Person2.Personl is an authority for Person2.Personl punishes person2 severely.This lexical phrase is a triple associating a linguisticpattern with its semantic oncept and presupposition.The pattern specifies the syntactic appearance in text.The presupposition specifies the surrounding context,while the concept specifies the meaning added by thephrase itself.
Phrase presupposition, distinguished fromphrase concept, is introduced inDHPL's representationsince it solves three problems: (a) in disambiguation itprovides a discrimination condition for phrase selec-tion, (b) in acquisition it allows the incorporation Of thecontext of the example as part of the phrase, and (c) ingeneration it provides an indexing scheme for phrasediscrimination and triggering.The role of the three slots in a phrase template maybe better understood by the way they are applied inparsing the text above.
The clause is parsed in foursteps:(1) The pattern is matched successfully against hetext.
Consequently, Personl and Person2 arebound to the judge and to AI Capone respectively(as the person class restrictions imposed by thepattern are satisfied).
(2) The presupposition associated with the pattern isvalidated using the concepts in the context.
Usingknowledge of human relationships, it is inferredthat the judge presents an authority to Capone.
(3) Since both (1) and (2) are successful, then thepattern itself is instantiated, adding to the context:The judge punished At Capone severly.
(4) Steps (1)-(3) are repeated for each relevant lexicalentry.
If more than one entry is instantiated, thenthe concept with the best match is selected.5.
ORGANIZING THE LEXICONRetrieval and update are the operations required ofmemory \[Kolodner84\], and of the lexicon in particular.The objective in DHPL is to retrieve lexical entries atvarious levels of generality.
The structure of the lexiconis specified by (a) the structure of a single lexicalelement, and (b) the global structure in which elementsare organized.
(1) ACTUAL SLOT-FILLER NOTATIONThe actual representation f the phrase is implementedusing GATE's \[Mueller87\] slot-filler language, as shownbelow.
In particular notice in that notation that therepresentation f a phrase, which is a linguistic object,is not different than the representation f other objectsin the database.316 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987Uri Zernik and Michael G. Dyer The Self-Extending Phrasal Lexiconcomment X throw the book at Ypattern ?x throw ( the book ) ( at ?y)presupposition(authority high ?xlow ?y)concept (auth-punish from ?xto ?y)Figure 6: The Phrase NotationNotice that the phrase consists of three main parts:pattern, concept and presupposition (the comment is forreference only).
(2) CASE-FRAME REPRESENTATIONThe pattern of the phrase above can be written as:?x throw <the book> <at ?y>This is an abbreviation which stands for the full notationgiven below.subject (case-frameclass personinstance ?x)verb (verbroot throw)objectl (case-framedeterminer theroot book)object2 (case-framemarker atclass personinstance ?y)This full notation has three features:(1) The pattern is constructed of four case frames\[Carbonel184\].
(2) Case frames are named.
For example, object2 isthe name of the case frame given as:marker atclass personinstance ?yThis case is referred to as the lexical subject o bedistinguished from the surface subject (the elementactually preceding the verb in the text).
(3) Case frames are unordered, namely no order isimposed among the case frames.
In no place in thecase frame is it mentioned, for example, that thelexical subject should precede the verb or follow it(or not appear at all).
Case ordering, thus, isinherited from general linguistic patterns, as shownlater in this paper.
(4) Case frames contain both semantic and syntacticproperties.
For example, objectl defines thenamed constituents the and book, while object2defines the class person.Since not all properties are given explicitly within thepattern itself, there is a need for an inheritance scheme.Properties uch as case order (e.g.
active and passivevoice), and word-order of the syntactic onstituentswithin cases (e.g.
the determiner the precedes the rootbook) are inherited from general linguistic patterns.5.2 THE GLOBAL STRUCTUREWhile varying in generality, lexical entries are repre-sented uniformly throughout.
The lexicon can beviewed as a collection of triples (Pattern-Concept-Pre-supposition), as shown in Figure 7, which are retrievedfor parsing and for generation tasks, and become oper-ational by unification.throw the book explain away ( i~"  ~ exolainA ~ passive voice,?~.
(P~ throw P~ ~awayout ~ plan ~,?
~L~J f~  fP'~ argueI ; I  take it up w~ , (~  oble' "~ct equi 'P~.JP~/'d~ \[~ \] boOK ~ ~ promise~at  ~equi  1~0 c?mmandFigure 7: The Lexicon as a Collection of TriplesTo facilitate learning, these triples are organized inhierarchies by generality.
In a hierarchical scheme, thebottom nodes are very specific and idiomatic while theones at the top are more general.
Phrases may reside at,and inherit from, more than one hierarchy.
For exam-ple, the phrase to take on can inherit from the hierarchyof take as well as from the hierarchy of on (a hierarchywhich defines properties of verb modifiers).
Four oper-ations, implemented as forms of unification, and aredefined by this representation.
They are: (a) interactionbetween two unrelated phrases, (b) inheritance betweentwo related phrases (one more general than the other),(c) generalization, and (d) discrimination of a phrase,which both update its level of generality.
Three hierar-chy schemes are given in the following sections todemonstrate hree aspects of the system: (a) phraseinteraction through the infinitive construction, (b) word-sense representation, and (c) case-order.6.
REPRESENTING THE INFINITIVEConsider the following pair of clauses in the sentencesbelow:Judge Wilson threw the book at him.Judge Wilson decided to throw the book at him.Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 317Uri Zernik and Michael G. Dyer The Self-Extending Phrasal LexiconParsing the first sentence is carried out simply as alexicon lookup: a phrase is found in the lexicon, and itsconcept is instantiated.
Parsing the second sentence ismore complex since no single lexical phrase is matchedfor throw.
For one thing, the subject does not precedethe verb throw as anticipated by the lexical pattern.Identifying the implicit subject involves knowledge ofphrase interaction.
Properties of phrase interaction(through the infinitive form \[Kiparsky71\]) are repre-sented by a hierarchy below.P I equi-rule,.,//see I planhear feel tell P3 ask commandFigure 8: The Hierarchy for Phrase InteractionThe names of the individual nodes are mnemonic, andare used for reference only.
Each such node is a fullpattern-concept-presupposition triple(the presupposi-tion may not appear).
The nodes in Figure 8 are de-scribed as follows:(a) The most general node (P1) denotes the basic equirule, which stands for the following object:comment the general equi behaviorpattern(subject instance ?x)(verb root ?v)(object instance ?y)(comp patternsubject instance (and ?x ?y)verb form infinitiveconcept ?z)concept(act actor ?xobject ?z)In this phrase, notice in particular the complement(comp), which defines the embedded phrase.
Theimplicit subject of the embedded phrase is taken aseither (1) the object of the embedding phrase, ifthat object exists, or (2) the subject of the embed-ding phrase, if the object does not exist.
(b) Middle-level nodes encompass classes of verbs.For example, P2 encompasses communicationverbs such as ask, tell, instruct, etc., sharecertain features.
It is represented as follows:comment communnication verbspattern(subject instance ?x)(verb root ?v)(object instance ?y)(cornp patternsubjectverbconcept ?z)concept(mtransinstance (and ?x ?y)form infinitiveactor ?xobject plan ?z)This phrase is similar to the phrase P1.
However,it includes information specific to that class ofverbs.
It defines hared syntactic features: subject,verb, object, complement (where the complemen-tizer is to).
It also defines hared semantic proper-ties: (a) the equi-rule, (b) the concept of thecomplement, which is a hypothetical, future plancommunicated by the actor.
(c) Specific nodes give the behavior of individualverbs, such as the phrases for decide (a planningverb) and command (a communication verb).comment X decide to Zpattern (subject instance ?x)(verb root decide)(comp patternsubjectverbconcept ?z)concept (select-plan actor ?xobject plan ?z)instance ?xform infinitivecomment X command Y to Zpattern (subject(verb(object(compinstance ?x)root command)instance ?y)patternsubject instance ?yverb form infinitiveconcept ?z)presupposition(authorityhigh ?xlow ?x)concept (rattans actortoobject?X?y(goal instance ?zgoal-of ?x))Each one of these phrases adds on the informationspecific to the denoted verb.
According to thisrepresentation ?x command ?y to ?z means that ?x318 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987Uri Zernik and Michael G. Dyer The Self-Extending Phrasal Lexiconwho presents an authority to ?y, tells ?y that ?z isa goal of ?x.~ 1 equi-ruleJunificationthrow the bookP2equi-rulecome overP2.
Junlflca tionFigure 10: Interaction with a Generalized Phraseincluded:Figure 9: Interaction of Two Specific Phrases(d) Episodes such as P4, which include specific in-stances of a phrase, are indexed to the phrase.
Forexample, P4 is the situation in which God com-mands Moses to approach the Mountain.
Thisepisode contains the semantic ingredients consti-tuting the meaning of the phrase.The hierarchy of Figure 8 is used by four processingtasks.6.1 PHRASE INTERACTIONThe analysis of the sentence below:Judge Wilson decided to throw the book at him.involves the interaction of two specific phrases, asshown schematically in Figure 9.
The two specificlexical phrases involved are the entries for decide (theembedding phrase, P1, elaborated in item (c) at thebeginning of Section 6 above) and for throw the book(the embedded phrase, P2, described in Figure 6 above).The unification of these two phrases guarantees that: (a)the subject of P1 is the subject of P2, and (b) the conceptof the P2 (denoted by ?z) is plugged in the plan slot ofP1.
The interaction of these two phrases yields thecompound concept:select-planactor wilson.1plan (auth-punishactor wilsonlto eapone.2)This concept conveys the meaning of the entire sen-tence.6.2 PARSING AN UNKNOWNIn contrast o the previous example, consider the anal-ysis of a sentence in which an unknown word isMary goggled John to come over.In analyzing this sentence, no lexical phrase is found toaccount for the word goggle.
Therefore, the meaning ofthe entire sentence cannot be produced.
Yet, even apartial meaning cannot be produced for the knownclause, to come over, since it is intertwined with theunknown clause Mary gogg led  John.
In order to over-come this obstacle, the interaction involves a moregeneral phrase as shown in Figure 10.
In contrast oFigure 9, here no specific phrase could be found forgoggze, and it is necessary to select the generalizedphrase, P1, which encompasses communication verbsin general.
For come over, on the other hand, thereexists a specific entry in the lexicon, P2, thus a gener-alization is not sought for.
The partial meaning con-structed for the sentence, in absence of a phrase forgoggle is:mtransactor mary.1to john.2object (ptransactor john.2to mary.l)Thus, even when the particular phrase does not exist,the parser is able to construct an initial hypothesis,based on a generalization.In fact, the selection of the generalized phrase is notunambiguous.
The nature of the selected phrase isrestricted by two schemes: (a) the hierarchy in Figure 8above, and (b) the persuade plan box \[Schank77\] whichprovides the planning options available for a person inpersuading another person to act (overpower, threaten,promise, steal, etc.).
Accordingly, goggle could have aswell conveyed meanings uch as:(36) Mary pushed John to come over.
(influence verb)(37) Mary let John come over.
(help verb)(38) Mary threatened John to come over.
(promiseverb)Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 319Uri Zernik and Michael G. Dyer The Self-Extending Phrasal LexiconIndeed option (38) is not available in English, however,since the phrase is yet unknown to the learner, thisoption must be given consideration.6.30VERGENERALIZATION AND RECOVERYIn the case that the word promise does not exist in thelexicon, the program behaves as follows:User: John promised Mary to come over.RINA: John told Mary that she must/can come to him.In using the generalized phrase, RINA unified inappro-priately the roles.
This is an error of overgeneralizationwhich is typical of children learning new vocabularyitems.6.4 ERROR RECOVERYThe user can correct the program by giving an explicitexample.User: No.
John promised Mary to come to her place.By using few inferences (e.g., person ?x does not cometo the same person ?x), RINA figures out the confusionin the role-binding and corrects appropriately the phrasefor promise, as given below:comment  X promise Y to Zpat tern  (subject(verb(object(comppresuppos i t ion(goalconcept  (mtransi ns tance  ?x)root  p romise)instance ?y)patternsubjectverbconcept ?z)goal-of ?y)actor ?xto ?yobject (plan ?z))i ns tance  ?xform infinitiveNotice two interesting points regarding the semantics ofpromise: (a) ?x (the embedding subject) is always thesubject of the embedded phrase, and (b) the act ?z ispresupposed to be a goal of ?y.
?x is the subject of theembedded act, and the act ?z is presupposed tobe a goalof ?y.7.
HANDLING WORD SENSESBy its nature, the phrasal approach is oriented towardsthe representation f entire groups of words.
However,single words, such as up, at, and away must also berepresented.
Three issues are involved in representingsuch words.7.1 ASSIGNING MEANINGS TO PARTICLESCompare the following two sentences:(39) John looked up at Mary.
(40) John looked at Mary.The meanings of the two sentences are given below*:(39) (40)attend attendobject eyes object eyesactor john.3 actor john.3to mary.4 to mary.4direction vertical-positiveThe contribution of the particle up is given as (directionvertical-positive).
The role of the particle in the nextsentence is less obvious.
(41) John flew away from the scene of the crime.What is the contribution of the word away to themeaning of sentence (41)?
For instance, how is themeaning of sentence (41) different han the meaning ofsentence (42) below?
(42) John flew to Alaska.7.2 RESOLVING WORD-SENSE AMBIGUITYIs the contribution of away identical in all the sentences(43)-(46), or are there several meanings involved?
(43) John flew away from the scene of the crime.
(44) John did not put away the clean dishes.
(45) He managed to argue it away with his wife.
(46) This machine was idling away for hours.For example, consider two appearances of the produc-tion argue away which involve two different senses ofaway:(47) His lawyer can argue away any tax violation.
(48) He is a bum.
He can argue away for hours withoutconvincing anybody.The first sense implies success in deceiving the author-ities (as in get away with), while the second senseimplies a waste of time (as in idle away).
If there is morethan one sense for away, then how is the appropriatemeaning selected in each instance?
In our lexicon, thereare two phrases for argue away, which are disambigu-ated by matching their presuppositions with the con-text.
The two phrases are:*Another phrase, John looked up to Mary, in contrast to John lookedup at Mary, is not processed as a simple production of the particles,since it involves the entire phrase "X look up to Y".320 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987Uri Zernik and Michael G. Dyer The Self-Extending Phrasal Lexiconpattern ?x ( ?v away ) ?ypresupposit ion?y is a planning failure by ?x?g is ?x's goal thwarted by authority punishment ?z?v is a communication act by ?x to avert ?zconceptact ?v is successful, and ?z is avertedpattern ?x ( ?v away )presupposit ionact ?v serves no goal of ?xact ?v consumes a useful resource (time)conceptact ?v is selected by ?xFigure lh Two Different Senses for argue awayThe appropriate phrase is selected in each context bymatching the presupposition.7.3 DETERMINING LEVEL OF GENERALITYWhich is the appropriate alternative for representing thephrase in sentence (49)?
(49) He managed to argue it away with his wife.
(a) Is it as "f ixed" phrase as given below?
(b)pattern: ?x away ?y ?zconcept: ?x managed to explain event ?yto person ?z by arguing.Or is it a "variable" phrase as given next:pattern: ?x away ?yconcept: ?x managed to explain event ?yto person ?z by act ?v.Answers for these dilemmas are given by the hierarchyin Figure 12 below:(a) The most general phrase (P1) denotes the generalproperties of English verb modifiers.
The modifierfollows the verb, but separation is allowed (i.e.
: heexp la ined  i t  away VS.  he  exp la ined  away  h i sla tes t  goo  f).P !
verb modifierP3b / / ~ '~ a ~ ~  ainst ~'~o ut waste timeid le / /  P3, ~ ~utinaway / get away become placesino wRn \ ~store away / ~P4 inaccessible ~ awayexolain aroue / k stackaway away run walk away/ kaway awayP5 P6Figure 12: The Hierarchy for "Away"ing conveyed by words such as away (P2), up anddown.
The pattern for P2, for example is <?vaway> where ?v can be any verb.
(c) Nodes at the third level convey word senses whichencompass classes of specific phrases.
For exam-ple, P3a (convince) conveys the meaning encom-passing both  explain it away and  argue it away,while P3b (waste time) conveys the meaning en-compassing both idle away and sing away.
Thesetwo phrases (P3a and P3b) are elaborated here:pattern ?x ( argue away ) ?ypresupposit ion?y is a planning failure by ?x?g is ?x's goal thwarted by authority punishment ?z?v (argue) is a communication act by ?x to avert ?zconceptact ?v (arguing) is successful, and ?z is avertedpattern ?x ( argue away )presupposit ionact ?v (arguing) serves no goal of ?xact ?v consumes a useful resource (time)conceptact ?v (arguing) is selected by ?xThese two phrases generalize respectively the phrasesin Figure 11.
(d) Nodes at the next level denote specific phrases, orproductions, such as run away, argue away (P4),idle away, etc.
Such phrases are given in Figure 11for two cases of argue away.
(e) Nodes at the bottom level describe episodes inwhich instances of phrases were encountered (e.g.,the instances A1 Capone argued i t  away in court(P5), J ohn  Smi th  a rgued  i t  away  w i th  h i s  w i fe  a reindexed to the phrase <?x argue ?y away>).On the face of it, it seems that levels (a) and (d) aresufficient for all parsing and generation purposes.
Whatis the function of levels (b), (c), and (e)?7.4 ANALYZING A NEW PRODUCTIONThese intermediate l vels of generalization facilitate theanalysis of new productions uch as:(50) John tried to describe it away in court.Sentence (50) introduces a new production to the readerof this paper.
Yet, the reader should be able to resolvethe new production by using the generalized linguisticpattern P3a in Figure 12.7.5 LEARNING FROM EXAMPLESIn the previous example we have assumed an existinggeneralized phrase P3a, which was used in predicting aspecific phrase.
When such a generality does not exist,learning must be done by induction from specific exam-ples.
The following set of examples provide episodesComputational Linguistics, Volume 13, Numbers 3-4, July-December 1987 321Uri Zernik and Michael G. Dyer The Self-Extending Phrasal Lexiconfrom which RINA can hypothesize the meaning of thephrase to take on.altogether?
This information is contained in a case-order hierarchy (Figure 14 below) in the lexicon.
(51) David took on Goliath.
(52) The Celtics took on the Lakers.
(53) Finally, I took on the hardest questionon the midterm.So far we have shown two ways of deriving newphrases: First, a new phrase can be generalized fromindexed episodes (which include instances in context).However, learning is easier when a generalized tem-plate already exists, in which case learning is accom-plished by applying a generality \[Zernik85a\].S<V<Oact iv / /~~le f tvoice .
.. dislocationpassive ngn.t .
oislocation voiceFigure 14: Case-Order Hierarchyaway on \ / \P 3 a convince continue ?
?<?x away> <?x on> <?x on> / \  'take on hang onhold on/  larou~ / describeexDlain aw~ episode1away episode2episode3The patterns for the passive and the active voice, forexample, are given in the figure below.P2:subject  (location bef) (marker none)verb (location ref) (voice active)object1 (location aft)object2 (location aft)P3:subject  (location any)verb (location ref) (voice passive)object1 (location bef) (marker none)object2 (location aft)Figure 13: Top-Down vs. Bottom-Up PropagationFigure 13 shows two learning processes: describe i taway is deduced top-down from an existing generalconcept (P3a).
On the other hand, take on is inducedbottom-up from the set of specific episodes such asDavid and Goliath, the Celtics vs. the Lakers, and themidterm.
There is no generalized concept which couldserve as a short cut.In matching sentences (54) and (55) above, the patternP0 inherits case-order properties from these generallinguistic patterns.
For example, after inheriting thepassive voice for matching sentence (55), the patternaugmented by inheritance from P3 would be:PI:subject  (location aft) (marker by) (class person)verb (location ref) (voice passive) (root throw)object1 (location bef)(marker none) (root book)object2 (location aft) (marker at) (class person)8.
INHERITING CASE ORDERConsider the lexical pattern given as a set of fourunordered case-frames:An even more general pattern exists which captures thebasic SVO structure of the language.
This phrase isgiven at the top of the hierarchy:PO: ?y throw < book> <at ?x>Since ordering is not specified explicitly in pattern P0,then how can this pattern match sentences such as:(54) The judge threw the book at AI.
(active voice)(55) The book was thrown at him.
(passive voice)(56) AI he decided to throw the book at,but John he gave a break.
(left dislocation)(57) "Take it easy!"
said the prosecutor.
(right dislocation)Under what condition does the lexical subject precedethe verb, and when can the lexical subject be omittedP0:pat ternsubject (location hef)(marker none) (instance ?x)verb (location ref)object1 (location aft) (marker none) (instance ?y)object2 (location aft) (marker ?m) (instance ?z)conceptact (actor ?x) (reeepient ?y) (?m ?z)What is the use of that general SVO phrase?
This phraseis called for in absence of more specific knowledge.Children who have not yet mastered specific case-structure patterns resort to this pattern.
For example, a2-year-old child might incorrectly understand:322 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987Uri Zernik and Michael G. Dyer The Self-Extending Phrasal LexiconMary was fed by John.as if Mary actually fed John.
Adults too, in case ofmissing knowledge, might resort to this generality inmaking sense out of sentences.9.
FIGURATIVE PHRASE ACQUISITION: A PROCESS MODELSo far, we have assumed the existence of necessaryphrases in the lexicon.
However, in reality a programmay encounter new phrases in the text.
Thus, theprogram must accomplish two objectives: (a) parse thetext in spite of the unknown element, and (b) acquireinformation about the unknown element for future en-counters.
Consider the situation in which the figurativephrase is first encountered.User: The mobster eluded prosecution for years.Last month, they threw the book at himfor income-tax evasion.RINA: The prosecutor propelled a book at him?User: No.
A judge threw the book at him.RINA: The judge threw the book at him.
He found himguilty.And later on:User: The dean of the school threw the book at John.RINA: He punished him.There are three stages in the acquisition process:(1) Apply the literal interpretation.
(2) Acquire the figurative phrase.
(3) Generalize the new phrase beyond the specificcontext.9.1 LITERAL INTERPRETATIONIn the absence of the appropriate phrase in the lexicon,RINA utilizes other available knowledge sources,namely (a) the literal interpretation a d (b) the context.The literal interpretation is given by the phrase:pat ternconcept?x:person throw ?y:phys-obj ( at ?y )propel actor ?xobject ?yto (location-of ?z)Figure 15: Propel a Phys-Objknowledge, and the fact that a discrepancy has beendetected.9.2 LEARNING BY FEATURE EXTRACTIONIn constructing the new hypothesis, the program mustextract he relevant features from the given episode.
(a) The initial phrase presupposition is taken to be theentire trial script.
(b) The pattern is extracted from the sample sentence.
(c) The concept is extracted from the script.In extracting either the pattern or the concept, theproblem is to distinguish between features which arerelevant and should be taken in as part of the phrase,and features which are irrelevant and thus should be leftout.
Moreover, some features hould be taken as is,where other features must be abstracted before they canbe incorporated.9.3 FORMING THE PATTERNFour rules are used in extracting the linguistic patternfrom the sentence:Last month, they threw the book at himfor income-tax evasion.
(1) Initially, use an existing literal pattern.
In this case,the initial pattern is:patternl:?x:person throw: ?z:phys-obj <at ?y:person>(2)(a)(b)Examine other cases in the sample sentence, andinclude cases in the pattern which could not beinterpreted by general interpretation.
There aretwo such cases:Last month could be interpreted as a general timeadverb (i.e.
: last year he was still enrolled atUCLA, the vacation started last week, etc.
).For income-tax evasion can be interpreted as aelement-paid-for adverb (i.e.
: he paid dearly for hiscrime, he was sentenced for a murder he did notcommit, etc.
).This phrase describes propelling an object in order to hitanother person.
Notice that no presupposition is spec-ified.
General phrases uch as take, give, catch, andthrow do not have a expressed presupposition since theycan be applied in many situations.
* The literal interpre-tation fails by plan/goal analysis.
In the context laiddown by the first phrase (prosecution has active-goal topunish the criminal), "propelling a book" does notserve the prosecution's goals.
In spite of the discrep-ancy, RINA spells out that interpretation above with aquestion mark, The prosecutor  p rope l led  a book  a thim.'?
to notify the user about her current state of(3)Thus, both these cases are excluded.Variablize references which can be instantiated inthe context.
In this case ?x is the Judge and ?y isthe Defendant.
They are maintained as variables,as opposed to case (4):(4) Freeze references which cannot be instantiated in* Notice the distinction between preconditions and presupposition.While a precondition for"throwing a ball" is "first holding it", this isnot part of the phrase presupposition.
Conditions which are impliedby common sense or world knowledge do not belong in the lexicon.Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 323Uri Zernik and Michael G. Dyer The Self-Extending Phrasal Lexiconthe context: Since no referent is found for thereference the book, that reference is taken as afrozen part of the pattern instead of the case?z:phys-obj.The resulting pattern is:pattern2:?x:person throw: <the book> <at ?y:person>9.4 FORMING THE CONCEPTIn selecting the concept of the phrase, there are fourpossibilities, namely the events shown in Figure 3(Section 4).
The choice of the appropriate one amongthese four events is facilitated by linguistic clues.
Asopposed to the phrase they threw the book to him whichimplies cooperation between the characters, the phrasethey threw the book at him implies a goal conflictbetween the characters.
At implies not taking acknow-ledgement protocols into consideration.
E g., x throwsthe rock to y implies that x catches y's attention, andgets acknowledgement for y's receipt of the rock.
Onthe other hand, x throws the  rock  a t  y implies that ymay not be aware or ready to receive the rock.
Thisanalysis applies also to talk at vs. talk to, etc.
Sincethis property is shared among many verbs, it is encodedin the lexicon as a general phrase:pattern ?x:person ?v:verb ?y:phys-obj ( at ?y )concept propel actor ?xobject ?yto (location-of ?z)mode no-acknowledgeFigure 16: Propel At, a General PhraseNotice that rather than having a specific root, thepattern of this phrase leaves out the root of the verb asa variable.
From lack of acknowledgement, a goalconflict may be inferred.goalclass p-healthstatus thwartedgoal-of ?zUsing this concept as a search pattern, the "punish-ment-decision" is selected from $trial.
Thus, the phraseacquired so far is:pat tern  ?x:person throw ( the book ) ( at ?y )concept  auth-punish actor ?xto ?ypresuppos i t iontrialjudge ?xdefendant ?yFigure 17: The Acquired Phrase9.5 PHRASE GENERALIZATIONAlthough RINA has acquired the phrase in a specificcontext, she might hear the phrase in a different con-text.
She should be able to transfer the phrase acrossspecific contexts by generalization.
RINA generalizesphrase meanings by analogical mapping.
Thus, whenhearing the sentence below, an analogy is found be-tween the two contexts.The third time he caught John cheating in an exam,the professor threw the book at him.The trial-script is indexed to a general authority rela-tionship.
The actions in a trial are explained by theexistence of that relationship.
For example, by sayingsomething to the Judge, the Defendant does not dictatethe outcome of the situation.
He merely informs theJudge with some facts in order to influence the verdict.On the other hand, by his decision, the Judge doesdetermine the outcome of the situation since he presentsan authority.
Three similarities are found between the$trial and the scene involving John and the professor.
(a) The authority relationship between ?x and ?y.
(b) A law-violation by ?y.
(c) A decision by ?x.Therefore, the phrase presupposition is generalizedfrom the specific trial-script into the general authority-decree situation which encompasses both examples.10.
CURRENT STATUS AND LIMITATIONSThe lexical theory (DHPL) described in this paperunderlies the program RINA described in the firstauthor's dissertation \[Zernik87c\].
The program RINA iscurrently implemented in T \[Rees84\] (a dialect ofSCHEME), on an APOLLO workstation using GATE's\[Mueller87\] unification language.
RINA's lexicon in-cludes more than 200 phrases including grammaticforms, word senses, and idioms.
A "micro" version ofthe program, which carries out basic parsing and learn-ing functions is included as an appendix of the disser-tation \[Zernik87c\].
RINA can engage in learning ses-sions by using a variety of learning strategies.
However,RINA's performance is limited in four ways.
(1) Only fragments of the English grammar have beenimplemented in the current version of DHPL.More work is required for extending systematicallythe set of constructs handled by DHPL.
(2) Text generation has not been at the focus of ourwork, and RINA's generation capabilities need tobe enhanced.
(3) Aspects of discourse and dialog have been ac-counted for by simple heuristics.
The input/outputdialogs of the program are restricted to a smallnumber of examples.
(4) Transfer plays a major role in second-languageacqisition, as elements in language I are manifested324 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987Uri Zernik and Michael G. Dyer The Self-Extending Phrasal Lexiconin performing in language II.
Accordingly, multiplelanguages in one model must not be represented inisolation.
The aspect of transfer and cross-lin-guistic interference is modeled by Michael Gasser\[Gasser86b\].11.
FUTURE WORKOpen research issues are (a) generation of examples, (b)learning and forgetting, (c) concept generalization, and(d) handling irony.ll.1 GENERATION OF EXAMPLESWe have identified a difference between generationtasks in general, where the generator describes a stateof affairs in the world, and our specific task of examplegeneration.
In example generation, the program is re-quired to demonstrate its own state of knowledge.
Forinstance, one dialog given earlier proceeds as follows:User: Greg wanted to buy a new car.He took it up with his dad.RINA: He took up the car with his dad?The explicit reference the  car is important since itconveys RINA's failure in acquiring the phrase.
Howcould a program decide to generate the car (and not it)in contrast o he (and not Greg)?
The research issue is:how a program or a person can test out its notion of aphrase.
Examples must be generated to examine theboundary conditions in which the phrase can still beapplied.
This issue has not been investigated so far.11.2 LEARNING AND FORGETTINGTwo related issues are system stability and obsoles-cence, or forgetting.
Stability concerns the ease withwhich well-established knowledge can be modified.
Ifthe behavior of the program is too dynamic, then itmight easily get thrown off by one esoteric, or incorrectuse of a phrase.
It is not desirable that an adult nativespeaker would get his lexicon ruined by listening to asecond language speaker.
Forgetting involves inacces-sibility of unused phrases, or getting rid of incorrecthypotheses.
Are incorrect hypotheses simply de-stroyed, or is there a more realistic model of obsoles-cence?
These two issues involve quantitative reasoningwhich require implementation of strength of links andactivation.
These kind of problems demonstrate thelimitations of a strictly qualitative approach, such asours, which rely on manipulation of logical proposi-tions, and it raises the need for quantitative approachessuch as connectionism \[Waltz85, McClelland86\], andspreading activation \[Anderson84, Charniak83\].11.3 CONCEPT GENERALIZATIONProliferation of knowledge is the process we try toapproximate.
The ubiquitous dilemma in comparing twoconcepts is whether a generalization exists for both, orwhether they are distinct concepts.
For example, con-sider the following sequence of examples in teaching thephrase to take on.
(57) David took on Goliath.
(58) I took on my elder brother.
(59) I took on a new job.
(60) We took on a new systems programmer.
(61) This piece of paper took on the shape of abutterfly.The second phrase can share the concept acquired forthe first one, namely ?x decided to fight ?y.
The thirdphrase; however, requires one to generalize the initialnotion since it now appears as ?x accepted a challengepresented by ?y.
However, can a generalization befound to encompass the fourth phrase?
Notice thatalthough a very general concept which encompasses allof the given examples could be found (?x has somethingto do with ?y), however, the effectiveness of such ageneralized notion is totally diminished.
Therefore, ashared concept should be sought at the appropriate l velof generality.11.4 DEVIATIONAL USES OF LANGUAGESo far, the notion of lexical presupposition has not beendeveloped according to its agreed functional definition.It is agreed that lexical presupposition presents felicityconditions for phrase application.
When these condi-tions are violated, phrases sound awkward, ironic, orsimply incorrect.
Consider the sentences below:(62) We refused to let our baby stay up all night, so hethrew the book at us.
He yelled and screamed forhours.
(63) My pals asked me how I got straight A's.
Imanaged to explain it away by telling them it was abureaucratic mistake.In each one of these sentences, a lexical presuppositionis being violated.
Our baby, as we all know, is not reallyan authority, as required of the actor of the phrasethrow the book.
Therefore, Sentence (62) sounds ironic.A presuppositional condition is violated also in sentence(63).
The entire presupposition states: (a) a planningfailure by the actor, (b) a threatening act by a socialauthority, and (c) an explanation act taken to block thatpunishment.
Now, getting A's is not a planning failure,rather it is a fortuitous success, which makes thesituation humorous.
Consider the next pair of sen-tences:(64) I made an appointment with my advisor.
I methim on time.
(65) I made an appointment with my advisor.
I ran intohim on time.Both run into and meet make the same statement: wocharacters got into a physical proximity.
However,since run into presupposes an unplanned, surprisingelement which does not exist in the situation, sentence(65) sounds incorrect.
In contrast o previous researchin which presupposition was used for deriving second-ary inferences which are mostly redundant, we suggestusing presuppositions for disambiguation, detection ofComputational Linguistics, Volume 13, Numbers 3-4, July-December 1987 325Uri Zernik and Michael G. Dyer The Self-Extending Phrasal Lexiconirony \[Dyer86a\], and even for generation of irony by acomputer (by applying phrases in situations where apresuppositional condition has been slightly mutated).12.
CONCLUSIONSWe have shown how the Dynamic Hierarchical PhrasalLexicon (DHPL) supports language analysis, and lan-guage acquisition.
We accounted for a dynamic lan-guage behavior by promoting four aspects of lexicalrepresentation:Phrases: The lexicon contains entire phrases, account-ing uniformly for an entire range including productiveas well as non-productive phrase.Hierarchy: The lexicon organizes in a hierarchy,phrases ranging from specific "lexical entries" at thebottom, to general "grammar ules" at the top.Lexical Presupposition: Contextual conditions are incor-porated into the lexicon through lexical presupposi-tions.
Presuppositions account for disambiguation iparsing, and for phrase selection in generation.Integration of Syntax and Semantics: Phrases specify arelation (in the logical sense) between syntax and se-mantics.
Thus, the question whether any lexical featureis syntax or whether it is semantics, becomes insignifi-cant.
For example, consider thematic roles for a phrasesuch as promise (Section 6.4).
Are they syntactic or arethey semantic?
They can be viewed as either.Using this representation we have shown three resultsin language processing:Coping with Lexical Gaps: The hierarchical structure ofthe lexicon enables parsing of text even when certainlexical elements are unknown.
A partial meaning for thetext, which serves as an initial hypothesis, is formed byapplying eneral knowledge when specific knowledge ismissing.Using Lexical Clues: In learning meanings of phrases wehave used "linguistic clues".
For instance, the word atin the judge threw the book at AI, supports the learningprocess of that idiom.
What is the justification fordrawing inferences from apparently vague senses ofwords?
In making the lexicon amenable as a linguisticdatabase, from which inference rules can be drawn, wehave systematically organized words in a hierarchy,representing words such as at, to, around and away.Thus, the use of linguistic clues per se is not inappro-priate; however, all linguistic clues used in a reasoningsystem, must be drawn from a well-organized lexicon.Knowledge Propagation through Generalization and Spe-cialization: Hierarchy is a precondition for learning bygeneralization.
Through the hierarchical scheme, thereare two ways of propagating knowledge: First, bottom-up-from instantiated episodes up towards specificphrases, and even higher to generalized word senses.Second, top-down-generalized word senses are propa-gated down for prediction of new specific phrases.
Inboth cases, effective learning depends on the existenceof a well refined hierarchy.
Any linguistic system mustaccommodate not only for spanning a static language,but also for augmenting the original linguistic systemitself.
In DHPL we have shown how, for a variety oflinguistic features, the lexicon itself can be augmentedthrough linguistic experiences.
Thus we have accom-plished a dynamic linguistic behavior.ACKNOWLEDGEMENTThe authors are indebted to Erik Mueller and MikeGasser for help in developing the ideas in this paper.
Wealso thank numerous second language speakers whoinadvertently contributed interesting errors.REFERENCESAnderson, John R. 1984 The Architecture of the Mind.
HarvardUniversity Press: Cambridge, MassBecker, Joseph D. 1975 The Phrasal Lexicon.
In Proceedings Inter-disciplinary Workshop on Theoretical Issues in Natural LanguageProcessing.
Cambridge, Massachusets June 70-73.Bresnan, J.
1982 Control and Complementation.
I  J. Bresnan, TheMental Representation of Grammatical Relations.
Cambridge,MA: The MIT Press.
(a)Bresnan, J.; R. Kaplan; J. Bresnan.
1982 Lexical-Functional Gram-mar.
In The Mental Representation of Grammatical RelationsMIT Press, Cambridge MA (b)Carbonell, J. G.; P. J. Hayes.
1984 Coping with Extragrammaticality.Proceedings Coling84.
Stanford, California 437-443.Charniak, E. Passing Markers: A Theory of Contextual Influence inLanguage Comprehension.
Cognitive Science 7 3 1983Dyer, M.; M. Flowers; J. Reeves.
1986 A Computer Model of IronyRecognition in Narrative Understanding.
Advances in Computingand the Humanities 1 1 (a)Dyer, M. G. 1983 In-Depth Understanding: A Computer Model ofIntegrated Processing for Narrative Comprehension.
MIT Press,Cambridge, MADyer, M. G.; U. Zernik.
1986 Encoding and Acquiring FigurativePhrases in the Phrasal Lexicon.
Proceedings 24th Annual Meetingof the Association for Computational Linguistics, New York NY(b)Fauconnier, Gilles.
1985 Mental Spaces: Aspects of Meaning Con-struction in Natural Language.
MIT Press, Cambridge MAFillmore, C. J.
1978 On the Organization of Semantics Information inthe Lexicon.
Proceedings Chicago Linguistic SocietyFillmore, C.; P. Kay; M. O'Connor.
1987 Regularity and ldiomaticityin Grammatical Constructions: The Case of Let Alone.
UCBerkeley, Department of Linguistics, Unpublished ManuscriptGasser, M. 1986 Memory Organization in the Bilingual/Second Lan-guage Learner: A Computational Approach.
Proceedings EasternStates Conference on Linguistics (ESCOL).
Chicago, IL (a)Gasser, M.; M. G. Dyer.
1986 Speak of the Devil: RepresentingDeictic and Speech Act Knowledge in an Integrated LexicalMemory.
Proceedings 8th Conference of the Cognitive ScienceSociety.
Amherst, MA, August 1986 (b)Gazdar, Gerold.
1979 A Solution to the Projection Problem.
InChoon-Kyu Oh, David A. Dinneen, Syntax and Semantics(Volume 11: Presupposition).
New-York, Academic Press 57-87Gazdar, G.; E. Klein; G. Pullum; I.
Sag.
1985 Generalized PhraseStructure Grammar.
Harvard University Press, Cambridge, MAGranger, R. H. 1977 FOUL-UP: A Program That Figures OutMeanings of Words from Context.
Proceedings Fifth IJCA1.Cambridge, Massachusets, August 172-178Grice, H. P. 1975 Logic and Conversation.
In P. Cole, J. Morgan,Syntax and Semantics (Volume 3: Speech Acts).
NY AcademicPressJacobs, P. S. 1985 A Knowledge-Based Approach to LanguageProduction.
UC Berkeley, Computer Science Division, UCB/CSD86/254, Berkeley, CA, August Ph.D. DissertationKarttunen, L.; S. Peter.
1979 Conventional Implicature.
In C. K. Oh,D.
Dinneen, Syntax and Semantics (Volume 11, Presupposition).NY Academic Press326 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987Uri Zernik and Michael G. Dyer The Self-Extending Phrasal LexiconKay, Martin.
1979 Functional Grammar.
Proceedings 5th AnnualMeeting of the Berkeley Linguistic Society, Berkeley, California142-158Keenan, L. Edward.
1971 Two Kinds of Presupposition i  NaturalLanguage.
In Charles Fillmore, D. T. Langendoen, Studies in Lin-guistic Semantics.
New York, Holt, Reinhart and Winston, 44-52Kiparsky, P.; C. Kiparsky.
1971 Fact.
In D. Steinberg, L. Jakobovits,Semantics, an Interdisciplinary Reader.
Cambridge, England,Cambridge University PressKolodner, J. L. 1984 Retrieval and Organizational Strategies inConceptual Memory: A Computer Model.
Lawrence ErlbaumAssociates, Hillsdale NJLakoff, George; Mark Johnson.
1980 Metaphors We Live By.
TheUniversity of Chicago Press, Chicago and LondonLangley, Pat.
1982 Language Acquisition Through Error Recovery.Cognition and Brain Theory 5 3 211-255McClelland, J. L.; D. E. Rumelhart.
1986 Parallel Distributed Proc-essing.
MIT Press, Cambridge, MAMitchell, T. M. 1982 Generalization as Search.
Artificial Intelligence18 203-226Mueller, Erik T. 1987 GATE Reference Manual (Second Edition)UCLA, Computer Science Department UCLA-AI-87-6 LosAngeles, CAPinker, S. 1984 Language Learnability and Language Development.Harvard University Press, Cambridge, MARees, Jonathan; Norman Adams; James Meehan.
1984 The T Manual.Computer Science Department, Yale University, New Haven CTSchank, R.; R. Abelson.
1977 Scripts, Plans, Goals, and Understand-ing.
Lawrence Erlbaum Associates, Hillsdale, New JerseySelfridge, Malory.
1982 Why Do Children Misunderstand ReversiblePassives?
The CHILD Program Learns to Understand PassiveSentences.
Proceedings AAA1-82.
Pittsburgh, Pennsylvania, Au-gust 251-257Waltz, D. L.; J.
B. Pollack.
1985 Massively Parallel Parsing: AStrongly Interactive Model of Natural Language Interpretation.Cognitive Science 9 1Wilensky, R.; Y. Arens; D. Chin.
1984 Talking to UNIX in English:an Overview of UC.
Communications of the ACM 27 6 June574-593Wilensky, R. 1981 A Knowledge-Based Approach to Natural Lan-guage Processing: A Progress Report.
Proceedings Seventh Inter-national Joint Conference on Artificial Intelligence, Vancouver,CanadaWilks, Y.
1975 Preference Semantics.
In E. Keenan, The FormalSemantics of Natural Language.
Cambridge, BritainZernik, U.; M. G. Dyer.
1985 Failure-Driven Aquisition of FigurativePhrases by Second Language Speakers.
Proceedings of the 7thAnnual Conference of the Cognitive Science Society.
Irvine, CA(a)Zernik, U.; M. G. Dyer.
1985 Towards a Self-Extending PhrasalLexicon.
Proceedings 23rd Annual Meeting of the Association forComputational Linguistics.
Chicago, IL, July (b)Zernik, U.; M. G. Dyer.
1986 Disambiguation and Acquisitionthrough the Phrasal Lexicon.
Proceedings llth InternationalConference on Computational Linguistics.
Bonn, Germany (a)Zernik, U.; M. G. Dyer.
1986 Language Acquisition: LearningPhrases in Context.
In T. Mitchell, J. Carbonell, R. Michalsky,Machine Learning: A Guide to Current Research.
Boston, MA,Kluwer (b).Zernik, U.
1987 How Do Machine-Learning Paradigms Fare inLanguage Acquisition?
Proceedings Fourth International Work-shop on Machine Learning.
Irvine, CA, June (a)Zernik, U.
1987 Acquiring Idioms from Examples in Context: Learn-ing by Explanation.
Proceedings 13th Annual Meeting of theBerkeley Linguistic Society.
Berkeley, California, February (b)Zernik, U.
1987 Strategies in Language Acquisition: LearningPhrases from Examples in Context.
UCLA-AI-87-1 LA, CA Ph.D.Dissertation (c)Zernik, U.
1987 "Learning Idioms with and without Explanation"lOth International Joint Conference on Artificial Intelligence,Milan (d)Zernik, U.
1987 "Language Acquisition: Learning a Hierarchy OfPhrases," lOth lnsternational Joint Conf.
On Artificial Intelli-gence, Milan (e)Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 327
