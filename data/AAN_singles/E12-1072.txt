Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 706?715,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsElliphant: Improved Automatic Detection ofZero Subjects and Impersonal Constructions in SpanishLuz Rello?NLP and Web Research GroupsUniv.
Pompeu FabraBarcelona, SpainRicardo Baeza-YatesYahoo!
ResearchBarcelona, SpainRuslan MitkovResearch Group inComputational LinguisticsUniv.
of Wolverhampton, UKAbstractIn pro-drop languages, the detection ofexplicit subjects, zero subjects and non-referential impersonal constructions is cru-cial for anaphora and co-reference resolu-tion.
While the identification of explicitand zero subjects has attracted the atten-tion of researchers in the past, the auto-matic identification of impersonal construc-tions in Spanish has not been addressed yetand this work is the first such study.
Inthis paper we present a corpus to under-pin research on the automatic detection ofthese linguistic phenomena in Spanish anda novel machine learning-based methodol-ogy for their computational treatment.
Thisstudy also provides an analysis of the fea-tures, discusses performance across twodifferent genres and offers error analysis.The evaluation results show that our systemperforms better in detecting explicit sub-jects than alternative systems.1 IntroductionSubject ellipsis is the omission of the subject ina sentence.
We consider not only missing refer-ential subject (zero subject) as manifestation ofellipsis, but also non-referential impersonal con-structions.Various natural language processing (NLP)tasks benefit from the identification of ellip-tical subjects, primarily anaphora resolution(Mitkov, 2002) and co-reference resolution (Ngand Cardie, 2002).
The difficulty in detect-ing missing subjects and non-referential pronounshas been acknowledged since the first studies on?
This work was partially funded by a ?La Caixa?
grantfor master students.the computational treatment of anaphora (Hobbs,1977; Hirst, 1981).
However, this task is of cru-cial importance when processing pro-drop lan-guages since subject ellipsis is a pervasive phe-nomenon in these languages (Chomsky, 1981).For instance, in our Spanish corpus, 29% of thesubjects are elided.Our method is based on classification of all ex-pressions in subject position, including the recog-nition of Spanish non-referential impersonal con-structions which, to the best of our knowledge,has not yet been addressed.
The necessity of iden-tifying such kind of elliptical constructions hasbeen specifically highlighted in work about Span-ish zero pronouns (Ferra?ndez and Peral, 2000)and co-reference resolution (Recasens and Hovy,2009).The main contributions of this study are:?
A public annotated corpus in Spanish tocompare different strategies for detecting ex-plicit subjects, zero subjects and impersonalconstructions.?
The first ML based approach to this problemin Spanish and a thorough analysis regardingfeatures, learnability, genre and errors.?
The best performing algorithms to automati-cally detect explicit subjects and impersonalconstructions in Spanish.The remainder of the paper is organized as fol-lows.
Section 2 describes the classes of Spanishsubjects, while Section 3 provides a literature re-view.
Section 4 describes the creation and the an-notation of the corpus and in Section 5 the ma-chine learning (ML) method is presented.
Theanalysis of the features, the learning curves, the706genre impact and the error analysis are all detailedin Section 6.
Finally, in Section 7, conclusionsare drawn and plans for future work are discussed.This work is an extension of the first author mas-ter?s thesis (Rello, 2010) and a preliminary ver-sion of the algorithm was presented in Rello et al(2010).2 Classes of Spanish SubjectsLiterature related to ellipsis in NLP (Ferra?ndezand Peral, 2000; Rello and Illisei, 2009a; Mitkov,2010) and linguistic theory (Bosque, 1989; Bru-cart, 1999; Real Academia Espan?ola, 2009) hasserved as a basis for establishing the classes ofthis work.Explicit subjects are phonetically realized andtheir syntactic position can be pre-verbal or post-verbal.
In the case of post-verbal subjects (a), thesyntactic position is restricted by some conditions(Real Academia Espan?ola, 2009).
(a) Carecera?n de validez las disposiciones que con-tradigan otra de rango superior.1The dispositions which contradict higher rangeones will not be valid.Zero subjects (b) appear as the result of a nomi-nal ellipsis.
That is, a lexical element ?the ellipticsubject?, which is needed for the interpretation ofthe meaning and the structure of the sentence, iselided; therefore, it can be retrieved from its con-text.
The elision of the subject can affect the en-tire noun phrase and not just the noun head whena definite article occurs (Brucart, 1999).
(b) ?
Fue refrendada por el pueblo espan?ol.
(It) was countersigned by the people of Spain.The class of impersonal constructions isformed by impersonal clauses (c) and reflex-ive impersonal clauses with particle se (d) (RealAcademia Espan?ola, 2009).
(c) No hay matrimonio sin consentimiento.
(There is) no marriage without consent.
(d) Se estara?
a lo que establece el apartado siguiente.
(It) will be what is established in the next section.1All the examples provided are taken from our corpus.In the examples, explicit subjects are presented in italics.Zero subjects are presented by the symbol ?
and in the En-glish translations the subjects which are elided in Spanish aremarked with parentheses.
Impersonal constructions are notexplicitly indicated.3 Related WorkIdentification of non-referential pronouns, al-though a crucial step in co-reference and anaphoraresolution systems (Mitkov, 2010),2 has been ap-plied only to the pleonastic it in English (Evans,2001; Boyd et al 2005; Bergsma et al 2008)and expletive pronouns in French (Danlos, 2005).Machine learning methods are known to performbetter than rule-based techniques for identifyingnon-referential expressions (Boyd et al 2005).However, there is some debate as to which ap-proach may be optimal in anaphora resolutionsystems (Mitkov and Hallett, 2007).Both English and French texts use an ex-plicit word, with some grammatical information(a third person pronoun), which is non-referential(Mitkov, 2010).
By contrast, in Spanish, non-referential expressions are not realized by exple-tive or pleonastic pronouns but rather by a certainkind of ellipsis.
For this reason, it is easy to mis-take them for zero pronouns, which are, in fact,referential.Previous work on detecting Spanish subject el-lipsis focused on distinguishing verbs with ex-plicit subjects and verbs with zero subjects (zeropronouns), using rule-based methods (Ferra?ndezand Peral, 2000; Rello and Illisei, 2009b).
TheFerra?ndez and Peral algorithm (2000) outper-forms the (Rello and Illisei, 2009b) approachwith 57% accuracy in identifying zero subjects.In (Ferra?ndez and Peral, 2000), the implementa-tion of a zero subject identification and resolutionmodule forms part of an anaphora resolution sys-tem.ML based studies on the identification ofexplicit non-referential constructions in Englishpresent accuracies of 71% (Evans, 2001), 87.5%(Bergsma et al 2008) and 88% (Boyd et al2005), while 97.5% is achieved for French (Dan-los, 2005).
However, in these languages, non-referential constructions are explicit and not omit-ted which makes this task more challenging forSpanish.4 CorpusWe created and annotated a corpus composedof legal texts (law) and health texts (psychiatric2In zero anaphora resolution, the identification of zeroanaphors first requires that they be distinguished from non-referential impersonal constructions (Mitkov, 2010).707papers) originally written in peninsular Spanish.The corpus is named after its annotated content?Explicit Subjects, Zero Subjects and ImpersonalConstructions?
(ESZIC es Corpus).To the best of our knowledge, the existing cor-pora annotated with elliptical subjects belong toother genres.
The Blue Book (handbook) andLexesp (journalistic texts) used in (Ferra?ndez andPeral, 2000) contain zero subjects but not imper-sonal constructions.
On the other hand, the Span-ish AnCora corpus based on journalistic texts in-cludes zero pronouns and impersonal construc-tions (Recasens and Mart?
?, 2010) while the Z-corpus (Rello and Illisei, 2009b) comprises legal,instructional and encyclopedic texts but has no an-notated impersonal constructions.The ESZIC corpus contains a total of 6,827verbs including 1,793 zero subjects.
Except forAnCora-ES, with 10,791 elliptic pronouns, ourcorpus is larger than the ones used in previous ap-proaches: about 1,830 verbs including zero andexplicit subjects in (Ferra?ndez and Peral, 2000)(the exact number is not mentioned in the pa-per) and 1,202 zero subjects in (Rello and Illisei,2009b).The corpus was parsed by Connexor?s Ma-chinese Syntax (Connexor Oy, 2006), which re-turns lexical and morphological information aswell as the dependency relations between wordsby employing a functional dependency grammar(Tapanainen and Ja?rvinen, 1997).To annotate our corpus we created an annota-tion tool that extracts the finite clauses and theannotators assign to each example one of the de-fined annotation tags.
Two volunteer graduate stu-dents of linguistics annotated the verbs after onetraining session.
The annotations of a third volun-teer with the same profile were used to computethe inter-annotator agreement.
During the anno-tation phase, we evaluated the adequacy and clar-ity of the annotation guidelines and established atypology of the rising borderline cases, which isincluded in the annotation guidelines.Table 1 shows the linguistic and formal criteriaused to identify the chosen categories that servedas the basis for the corpus annotation.
For eachtag, in addition to the two criteria that are crucialfor identifying subject ellipsis ([?
elliptic] and[?
referential]) a combination of syntactic, se-mantic and discourse knowledge is also encodedduring the annotation.
The linguistic motivationfor each of the three categories is shown againstthe thirteen annotation tags to which they belong(Table 1).Afterwards, each of the tags are grouped in oneof the three main classes.?
Explicit subjects: [- elliptic, + referential].?
Zero subjects: [+ elliptic, + referential].?
Impersonal constructions: [+ elliptic, - refer-ential].Of these annotated verbs, 71% have an explicitsubject, 26% have a zero subject and 3% belongto an impersonal construction (see Table 2).Number of instances Legal Health AllExplicit subjects 2,739 2,116 4,855Zero subjects 619 1,174 1,793Impersonals 71 108 179Total 3,429 3,398 6,827Table 2: Instances per class in ESZIC Corpus.To measure inter-annotator reliability we useFleiss?
Kappa statistical measure (Fleiss, 1971).We extracted 10% of the instances of each of thetexts of the corpus covering the two genres.Fleiss?
Kappa Legal Health AllTwo Annotators 0.934 0.870 0.902Three Annotators 0.925 0.857 0.891Table 3: Inter-annotator Agreement.In Table 3 we present the Fleiss kappa inter-annotator agreement for two and three annota-tors.
These results suggest that the annotationis reliable since it is common practice among re-searchers in computational linguistics to consider0.8 as a minimum value of acceptance (Artsteinand Poesio, 2008).5 Machine Learning ApproachWe opted for an ML approach given that ourprevious rule-based methodology improved only0.02 over the 0.55 F-measure of a simple base-line (Rello and Illisei, 2009b).
Besides, ML basedmethods for the identification of explicit non-referential constructions in English appear to per-form better than than rule-based ones (Boyd et al2005).708LINGUISTIC INFORMATION PHONETICREALIZATIONSYNTACTICCATEGORYVERBALDIATHESISSEMANTICINTERPR.DISCOURSEAnnotationCategoriesAnnotationTagsEllipticnounphraseEll.
nounphraseheadNominalsubjectActive ActiveparticipantReferentialsubjectExplicit subject ?
?
+ + + +ExplicitsubjectReflex passivesubject?
?
+ + ?
+Passive subject ?
?
+ ?
?
+Omitted subject + ?
+ + + +Omitted subjecthead?
+ + + + +Non-nominalsubject?
?
?
+ + +ZerosubjectReflex passiveomitted subject+ ?
+ + ?
+Reflex pass.
omit-ted subject head?
+ + + ?
+Reflex pass.
non-nominal subject?
?
?
+ ?
+Passive omittedsubject+ ?
+ ?
?
+Pass.
non-nominalsubject?
?
?
?
?
+ImpersonalconstructionReflex imp.
clause(with se)?
?
n/a ?
n/a ?Imp.
construction(without se)?
?
n/a + n/a ?Table 1: ESZIC Corpus Annotation Tags.5.1 FeaturesWe built the training data from the annotated cor-pus and defined fourteen features.
The linguisti-cally motivated features are inspired by previousML approaches in Chinese (Zhao and Ng, 2007)and English (Evans, 2001).
The values for the fea-tures (see Table 4) were derived from informationprovided both by Connexor?s Machinese Syntaxparser and a set of lists.We can describe each of the features as broadlybelonging to one of ten classes, as follows:1 PARSER: the presence or absence of a sub-ject in the clause, as identified by the parser.We are not aware of a formal evaluation ofConnexor?s accuracy.
It presents an accu-racy of 74.9% evaluated against our corpusand we used it as a simple baseline.2 CLAUSE: the clause types considered are:main clauses, relative clauses starting with acomplex conjunction, clauses starting with asimple conjunction, and clauses introducedusing punctuation marks (commas, semi-colons, etc).
We implemented a methodto identify these different types of clauses,as the parser does not explicitly mark theboundaries of clauses within sentences.
Themethod took into account the existence of afinite verb, its dependencies, the existence ofconjunctions and punctuation marks.3 LEMMA: lexical information extracted fromthe parser, the lemma of the finite verb.4-5 NUMBER, PERSON: morphological infor-mation of the verb, its grammatical numberand its person.6 AGREE: feature which encodes the tense,mood, person, and number of the verb in theclause, and its agreement in person, number,709Feature Definition Value1 PARSER Parsed subject True, False2 CLAUSE Clause type Main, Rel, Imp, Prop, Punct3 LEMMA Verb lemma Parser?s lemma tag4 NUMBER Verb morphological number SG, PL5 PERSON Verb morphological person P1, P2, P36 AGREE Agreement in person, number, tense FTFF, TTTT, FFFF, TFTF, TTFF, FTFT, FTTF, TFTT,and mood FFFT, TTTF, FFTF, TFFT, FFTT, FTTT, TFFF, TTFT7 NHPREV Previous noun phrases Number of noun phrases previous to the verb8 NHTOT Total noun phrases Number of noun phrases in the clause9 INF Infinitive Number of infinitives in the clause10 SE Spanish particle se True, False11 A Spanish preposition a True, False12 POSpre Four parts of the speech previous to 292 different values combining the parser?sthe verb POS tags14 POSpos Four parts of the speech following 280 different values combining the parser?sthe verb POS tags14 VERBtype Type of verb: copulative, impersonal CIPX, XIXX, XXXT, XXPX, XXXI, CIXX, XXPT, XIPX,pronominal, transitive and intransitive XIPT, XXXX, XIXI, CXPI, XXPI, XIPI, CXPXTable 4: Features, definitions and values.tense, and mood with the preceding verb inthe sentence and also with the main verb ofthe sentence.37-9 NHPREV, NHTOT, INF: the candidates forthe subject of the clause are represented bythe number of noun phrases in the clause thatprecede the verb, the total number of nounphrases in the clause, and the number of in-finitive verbs in the clause.10 SE: a binary feature encoding the presenceor absence of the Spanish particle se when itoccurs immediately before or after the verbor with a maximum of one token lying be-tween the verb and itself.
Particle se occursin passive reflex clauses with zero subjectsand in some impersonal constructions.11 A: a binary feature encoding the presence orabsence of the Spanish preposition a in theclause.
Since the distinction between passivereflex clauses with zero subjects and imper-sonal constructions sometimes relies on theappearance of preposition a (to, for, etc.
).For instance, example (e) is a passive reflexclause containing a zero subject while exam-ple (s) is an impersonal construction.3In Spanish, when a finite verb appears in a subordinateclause, its tense and mood can assist in recognition of thesefeatures in the verb of the main clause and help to enforcesome restrictions required by this verb, especially when bothverbs share the same referent as subject.
(e) Se admiten los alumnos que reu?nan los req-uisitos.?
(They) accept the students who fulfill therequirements.
(f) Se admite a los alumnos que reu?nan los req-uisitos.
(It) is accepted for the students who fulfillthe requirements.12-3 POSpre, POSpos: the part of the speech(POS) of eight tokens, that is, the 4-gramspreceding and the 4-grams following the in-stance.14 VERBtype: the verb is classified as copula-tive, pronominal, transitive, or with an im-personal use.4 Verbs belonging to more thanone class are also accommodated with dif-ferent feature values for each of the possiblecombinations of verb type.5.2 EvaluationTo determine the most accurate algorithm for ourclassification task, two comparisons of learningalgorithms implemented in WEKA (Witten andFrank, 2005) were carried out.
Firstly, the classi-fication was performed using 20% of the traininginstances.
Secondly, the seven highest perform-ing classifiers were compared using 100% of the4We used four lists provided by Molino de Ideas s.a. con-taining 11,060 different verb lemmas belonging to the RoyalSpanish Academy Dictionary (Real Academia Espan?ola,2001).710Class P R F Acc.Explicit subj.
90.1% 92.3% 91.2% 87.3%Zero subj.
77.2% 74.0% 75.5% 87.4%Impersonals 85.6% 63.1% 72.7% 98.8%Table 5: K* performance (87.6% accuracy for ten-foldcross validation).training data and ten-fold cross-validation.
Thecorpus was partitioned into training and testedusing ten-fold cross-validation for randomly or-dered instances in both cases.
The lazy learn-ing classifier K* (Cleary and Trigg, 1995), us-ing a blending parameter of 40%, was the bestperforming one, with an accuracy of 87.6% forten-fold cross-validation.
K* differs from otherinstance-based learners in that it computes the dis-tance between two instances using a method mo-tivated by information theory, where a maximumentropy-based distance function is used (Clearyand Trigg, 1995).
Table 5 shows the resultsfor each class using ten-fold cross-validation.In contrast to previous work, the K* algorithm(Cleary and Trigg, 1995) was found to provide themost accurate classification in the current study.Other approaches have employed various clas-sification algorithms, including JRip in WEKA(Mu?ller, 2006), with precision of 74% and recallof 60%, and K-nearest neighbors in TiMBL: bothin (Evans, 2001) with precision of 73% and recallof 69%, and in (Boyd et al 2005) with precisionof 82% and recall of 71%.Since there is no previous ML approach for thistask in Spanish, our baselines for the explicit sub-jects and the zero subjects are the parser outputand the previous rule-based work with the high-est performance (Ferra?ndez and Peral, 2000).
Forthe impersonal constructions the baseline is a sim-ple greedy algorithm that classifies as an imper-sonal construction every verb whose lemma is cat-egorized as a verb with impersonal use accordingto the RAE dictionary (Real Academia Espan?ola,2001).Our method outperforms the Connexor parserwhich identifies the explicit subjects but makes nodistinction between zero subjects and impersonalconstructions.
Connexor yields 74.9% overall ac-curacy and 80.2% and 65.6% F-measure for ex-plicit and elliptic subjects, respectively.To compare with Ferra?ndez and Peral(Ferra?ndez and Peral, 2000) we do considerAlgorithm ExplicitsubjectsZerosubjectsImpersonalsRAE ?
?
70.4%Connexor 71.7% 83.0%Ferr./Peral 79.7% 98.4% ?Elliphant 87.3% 87.4% 98.8%Table 6: Summary of accuracy comparison with previ-ous work.it without impersonal constructions.
We achievea precision of 87% for explicit subjects comparedto 80%, and a precision of 87% for zero subjectscompared to their 98%.
The overall accuracyis the same for both techniques, 87.5%, but ourresults are more balanced.
Nevertheless, theapproaches and corpora used in both studies aredifferent, and hence it is not possible to do a faircomparison.
For example, their corpus has 46%of zero subjects while ours has only 26%.For impersonal constructions our method out-performs the RAE baseline (precision 6.5%,recall 77.7%, F-measure 12.0% and accuracy70.4%).
Table 6 summarizes the comparison.
Thelow performance of the RAE baseline is due to thefact that verbs with impersonal use are often am-biguous.
For these cases, we first tagged them asambiguous and then, we defined additional crite-ria after analyzing then manually.
The resultingannotated criteria are stated in Table 1.6 AnalysisThrough these analyses we aim to extract the mosteffective features and the information that wouldcomplement the output of an standard parser toachieve this task.
We also examine the learningprocess of the algorithm to find out how many in-stances are needed to train it efficiently and de-termine how much Elliphant is genre dependent.The analyses indicate that our approach is robust:it performs nearly as well with just six features,has a steep learning curve, and seems to general-ize well to other text collections.6.1 Best FeaturesWe carried out three different experiments to eval-uate the most effective group of features, andthe features themselves considering the individ-ual predictive ability of each one along with theirdegree of redundancy.Based on the following three feature selection711methods we can state that there is a complex andbalanced interaction between the features.6.1.1 Grouping FeaturesIn the first experiment we considered the 11groups of relevant ordered features from the train-ing data, which were selected using each WEKAattribute selection algorithm and performed theclassifications over the complete training data, us-ing only the different groups features selected.The most effective group of six features (NH-PREV, PARSER, NHTOT, POSpos, PERSON,LEMMA) was the one selected by WEKA?s Sym-metricalUncertAttribute technique, which givesan accuracy of 83.5%.
The most frequentlyselected features by all methods are PARSER,POSpos, and NHTOT, and they alone get an accu-racy of 83.6% together.
As expected, the two pairsof features that perform best (both 74.8% accu-racy) are PARSER with either POSpos or NHTOT.Based on how frequent each feature is selectedby WEKA?s attribute selection algorithms, we canrank the features as following: (1) PARSER,(2) NHTOT, (3) POSpos, (4) NHPREV and (5)LEMMA.6.1.2 ?Complex?
vs. ?Simple?
FeaturesSecond, a set of experiments was conductedin which features were selected on the basisof the degree of computational effort needed togenerate them.
We propose two sets of fea-tures.
One group corresponds to ?simple?
fea-tures, whose values can be obtained by trivialexploitation of the tags produced in the parser?soutput (PARSER, LEMMA, PERSON, POSpos,POSpre).
The second group of features, ?com-plex?
features (CLAUSE, AGREE, NHPREV,NHTOT, VERBtype) have values that required theimplementation of more sophisticated modules toidentify the boundaries of syntactic constituentssuch as clauses and noun phrases.
The accuracyobtained when the classifier exclusively exploits?complex?
features is 82.6% while for ?simple?features is 79.9%.
No impersonal constructionsare identified when only ?complex?
features areused.6.1.3 One-left-out FeatureIn the third experiment, to estimate the weightof each feature, classifications were made inwhich each feature was omitted from the train-ing instances that were presented to the classifier.Omission of all but one of the ?simple?
featuresled to a reduction in accuracy, justifying their in-clusion in the training instances.
Nevertheless, themajority of features present low informativenessexcept for feature A which does not make anymeaningful contribution to the classification.
Thefeature PARSER presents the greatest differencein performance (86.3% total accuracy); however,this is no big loss, considering it is the main fea-ture.
Hence, as most features do not bring a sig-nificant loss in accuracy, the features need to becombined to improve the performance.6.2 Learning AnalysisThe learning curve of Figure 1 (left) presents theincrease of the performance obtained by Elliphantusing the training data randomly ordered.
Theperformance reaches its plateau using 90% of thetraining instances.
Using different ordering of thetraining set we obtain the same result.Figure 1 (right) presents the precision for eachclass and overall in relation to the number of train-ing instances for each one of them.
Recall growssimilarly to precision.
Under all conditions, sub-jects are classified with a high precision since theinformation given by the parser (collected in thefeatures) achieves an accuracy of 74.9% for theidentification of explicit subjects.The impersonal construction class has thefastest learning curve.
When utilizing a trainingset of only 163 instances (90% of the trainingdata), it reaches a precision of 63.2%.
The un-stable behaviour for impersonal constructions canbe attributed to not having enough training datafor that class, since impersonals are not frequentin Spanish.
On the other hand, the zero subjectclass is learned more gradually.The learning curve for the explicit subject classis almost flat due to the great variety of subjectsoccurring in the training data.
In addition, reach-ing a precision of 92.0% for explicit subjects us-ing just 20% of the training data is far more ex-pensive in terms of the number of training in-stances (978) as seen in Figure 1 (right).
Actually,with just 20% of the training data we can alreadyachieve a precision of 85.9%.This demonstrates that Elliphant does not needvery large sets of expensive training data andis able to reach adequate levels of performancewhen exploiting far fewer training instances.
Infact, we see that we only need a modest set of71283.0083.6084.2084.8085.4086.0086.6010% 20% 30% 40% 50% 60% 70% 80% 90% 100%Precision Recall F-measure85.6%85.3%85.8%85.7%85.2%85.8%86.3%86.4%85.9%85.5%86.0%86.5% 86.6%%49.0055.2961.5767.8674.1480.4386.7193.0010% 20% 30% 40% 50% 60% 70% 80% 90% 100%49897814611929243328983400389943864854354537735898 109412491416159317931671732496682129146179Explicit subjectsZero subjectsImpersonalconstructionsOverall163103Precision(%)Figure 1: Learning curve for precision, recall and F-measure (left) and with respect to the number of instancesof each class (right) for a given percentage of training data.annotated instances (fewer than 1,500) to achievegood results.6.3 Impact of GenreTo examine the influence of the different text gen-res on this method, we divided our training datainto two subgroups belonging to different genres(legal and health) and analyze the differences.A comparative evaluation using ten-fold cross-validation over the two subgroups shows that El-liphant is more successful when classifying in-stances of explicit subjects in legal texts (89.8%accuracy) than health texts (85.4% accuracy).This may be explained by the greater uniformityof the sentences in the legal genre compared toones from the health genre, as well as the fact thatthere are a larger number of explicit subjects in thelegal training data (2,739 compared with 2,116 inthe health texts).
Further, texts from the healthgenre present the additional complication of spe-cialized named entities and acronyms, which areused quite frequently.
Similarly, better perfor-mance in the detection of zero subjects and imper-sonal sentences in the health texts may be due totheir more frequent occurrence and hence greaterlearnability.Training/Testing Legal Health AllLegal 90.0% 86.8% 89.3%Health 86.8% 85.9% 88.7%All 92.5% 93.7% 87.6%Table 7: Accuracy of cross-genre training and testingevaluation (ten-fold evaluation).We have also studied the effect of training theclassifier on data derived from one genre and test-ing on instances derived from a different genre.Table 7 shows that instances from legal textsare more homogeneous, as the classifier obtainshigher accuracy when testing and training only onlegal instances (90.0%).
In addition, legal textsare also more informative, because when both le-gal and health genres are combined as trainingdata, only instances from the health genre showa significant increased accuracy (93.7%).
Theseresults reveal that the health texts are the most het-erogeneous ones.
In fact, we also found subsets ofthe legal documents where our method achievesan accuracy of 94.6%, implying more homoge-neous texts.6.4 Error AnalysisSince the features of the system are linguisti-cally motivated, we performed a linguistic anal-ysis of the erroneously classified instances to findout which patterns are more difficult to classifyand which type of information would improve themethod (Rello et al 2011).We extract the erroneously classified instancesof our training data and classify the errors.
Ac-cording to the distribution of the errors per class(Table 8) we take into account the following fourclasses of errors for the analysis: (a) impersonalconstructions classified as zero subjects, (b) im-personal constructions classified as explicit sub-jects, (c) zero subjects classified as explicit sub-jects, and (d) explicit subjects classified as zerosubjects.
The diagonal numbers are the true pre-dicted cases.
The classification of impersonalconstructions is less balanced than the ones forexplicit subjects and zero subjects.
Most of thewrongly identified instances are classified as ex-plicit subject, given that this class is the largestone.
On the other hand, 25% of the zero subjectsare classified as explicit subject, while only 8% of713the explicit subjects are identified as zero subjects.Class Zero Explicit Impers.subjects subjectsZero subj.
1327 453 (c) 13Explicit subj.
368 (d) 4481 6Impersonals 25 (a) 41 (b) 113Table 8: Confusion Matrix (ten-fold validation).For the analysis we first performed an explo-ration of the feature values which allows us togenerate smaller samples of the groups of errorsfor the further linguistic analyses.
Then, we ex-plore the linguistic characteristics of the instancesby examining the clause in which the instance ap-pears in our corpus.
A great variety of differentpatterns are found.
We mention only the linguisticcharacteristics in the errors which at least doublethe corpus general trends.In all groups (a-d) there is a tendency of usingthe following elements: post-verbal prepositions,auxiliary verbs, future verbal tenses, subjunctiveverbal mode, negation, punctuation marks ap-pearing before the verb and the preceding nounphrases, concessive and adverbial subordinateclauses.
In groups (a) and (b) the lemma of theverb may play a relevant role, for instance verbhaber (?there is/are?)
appears in the errors seventimes more than in the training while verb tratar(?to be about?, ?to deal with?)
appears 12 timesmore.
Finally, in groups (c) and (d) we noticethe frequent occurrence of idioms which includeverbs with impersonal uses, such as es decir (?thatis to say?)
and words which can be subject on theirown i.e.
ambos (?both?)
or todo (?all?
).7 Conclusions and Future WorkIn this study we learn which is the most accurateapproach for identifying explicit subjects and im-personal constructions in Spanish and which arethe linguistic characteristics and features that helpto perform this task.
The corpus created is freelyavailable online.5 Our method complements pre-vious work on Spanish anaphora resolution by ad-dressing the identification of non-referential con-structions.
It outperforms current approaches inexplicit subject detection and impersonal con-structions, doing better than the parser for every5ESZIC es Corpus is available at: http://luzrello.com/Projects.html.class.A possible future avenue to explore could beto combine our approach with Ferra?ndez andPeral (Ferra?ndez and Peral, 2000) by employingboth algorithms in sequence: first Ferra?ndez andPeral?s algorithm to detect all zero subjects andthen ours to identify explicit subjects and imper-sonals.
Assuming that the same accuracy could bemaintained, on our data set the combined perfor-mance could potentially be in the range of 95%.Future research goals are the extrinsic evalua-tion of our system by integrating our system inNLP tasks and its adaptation to other Romancepro-drop languages.
Finally, we believe that ourML approach could be improved as it is the firstattempt of this kind.AcknowledgementsWe thank Richard Evans, Julio Gonzalo and theanonymous reviewers for their wise comments.ReferencesR.
Artstein and M. Poesio.
2008.
Inter-coder agree-ment for computational linguistics.
ComputationalLinguistics, 34(4):555?596.S.
Bergsma, D. Lin, and R. Goebel.
2008.
Distri-butional identification of non-referential pronouns.In Proceedings of the 46th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies (ACL/HLT-08), pages 10?18.I.
Bosque.
1989.
Clases de sujetos ta?citos.
In JulioBorrego Nieto, editor, Philologica: homenaje a An-tonio Llorente, volume 2, pages 91?112.
Serviciode Publicaciones, Universidad Pontificia de Sala-manca, Salamanca.A.
Boyd, W. Gegg-Harrison, and D. Byron.
2005.Identifying non-referential it: a machine learningapproach incorporating linguistically motivated pat-terns.
In Proceedings of the ACL Workshop on Fea-ture Engineering for Machine Learning in NaturalLanguage Processing.
43rd Annual Meeting of theAssociation for Computational Linguistics (ACL-05), pages 40?47.J.
M. Brucart.
1999.
La elipsis.
In I. Bosqueand V. Demonte, editors, Grama?tica descriptiva dela lengua espan?ola, volume 2, pages 2787?2863.Espasa-Calpe, Madrid.N.
Chomsky.
1981.
Lectures on Government andBinding.
Mouton de Gruyter, Berlin, New York.J.G.
Cleary and L.E.
Trigg.
1995.
K*: an instance-based learner using an entropic distance measure.In Proceedings of the 12th International Conferenceon Machine Learning (ICML-95), pages 108?114.714Connexor Oy, 2006.
Machinese language model.L.
Danlos.
2005.
Automatic recognition of Frenchexpletive pronoun occurrences.
In Robert Dale,Kam-Fai Wong, Jiang Su, and Oi Yee Kwong, ed-itors, Natural language processing.
Proceedings ofthe 2nd International Joint Conference on NaturalLanguage Processing (IJCNLP-05), pages 73?78,Berlin, Heidelberg, New York.
Springer.
LectureNotes in Computer Science, Vol.
3651.R.
Evans.
2001.
Applying machine learning: towardan automatic classification of it.
Literary and Lin-guistic Computing, 16(1):45?57.A.
Ferra?ndez and J. Peral.
2000.
A computational ap-proach to zero-pronouns in Spanish.
In Proceedingsof the 38th Annual Meeting of the Association forComputational Linguistics (ACL-2000), pages 166?172.J.
L. Fleiss.
1971.
Measuring nominal scale agree-ment among many raters.
Psychological Bulletin,76(5):378?382.G.
Hirst.
1981.
Anaphora in natural language under-standing: a survey.
Springer-Verlag.J.
Hobbs.
1977.
Resolving pronoun references.
Lin-gua, 44:311?338.R.
Mitkov and C. Hallett.
2007.
Comparing pronounresolution algorithms.
Computational Intelligence,23(2):262?297.R.
Mitkov.
2002.
Anaphora resolution.
Longman,London.R.
Mitkov.
2010.
Discourse processing.
In AlexanderClark, Chris Fox, and Shalom Lappin, editors, Thehandbook of computational linguistics and naturallanguage processing, pages 599?629.
Wiley Black-well, Oxford.C.
Mu?ller.
2006.
Automatic detection of nonrefer-ential it in spoken multi-party dialog.
In Proceed-ings of the 11th Conference of the European Chap-ter of the Association for Computational Linguistics(EACL-06), pages 49?56.V.
Ng and C. Cardie.
2002.
Identifying anaphoricand non-anaphoric noun phrases to improve coref-erence resolution.
In Proceedings of the 19th Inter-national Conference on Computational Linguistics(COLING-02), pages 1?7.Real Academia Espan?ola.
2001.
Diccionario de lalengua espan?ola.
Espasa-Calpe, Madrid, 22 edi-tion.Real Academia Espan?ola.
2009.
Nueva grama?tica dela lengua espan?ola.
Espasa-Calpe, Madrid.M.
Recasens and E. Hovy.
2009.
A deeperlook into features for coreference resolution.
InLalitha Devi Sobha, Anto?nio Branco, and RuslanMitkov, editors, Anaphora Processing and Applica-tions.
Proceedings of the 7th Discourse Anaphoraand Anaphor Resolution Colloquium (DAARC-09),pages 29?42.
Springer, Berlin, Heidelberg, NewYork.
Lecture Notes in Computer Science, Vol.5847.M.
Recasens and M.A.
Mart??.
2010.
Ancora-co: Coreferentially annotated corpora for Spanishand Catalan.
Language resources and evaluation,44(4):315?345.L.
Rello and I. Illisei.
2009a.
A comparative studyof Spanish zero pronoun distribution.
In Proceed-ings of the International Symposium on Data andSense Mining, Machine Translation and ControlledLanguages, and their application to emergenciesand safety critical domains (ISMTCL-09), pages209?214.
Presses Universitaires de Franche-Comte?,Besanc?on.L.
Rello and I. Illisei.
2009b.
A rule-based approachto the identification of Spanish zero pronouns.
InStudent Research Workshop.
International Confer-ence on Recent Advances in Natural Language Pro-cessing (RANLP-09), pages 209?214.L.
Rello, P. Sua?rez, and R. Mitkov.
2010.
A machinelearning method for identifying non-referential im-personal sentences and zero pronouns in Spanish.Procesamiento del Lenguaje Natural, 45:281?287.L.
Rello, G. Ferraro, and A. Burga.
2011.
Error analy-sis for the improvement of subject ellipsis detection.Procesamiento de Lenguaje Natural, 47:223?230.L.
Rello.
2010.
Elliphant: A machine learning methodfor identifying subject ellipsis and impersonal con-structions in Spanish.
Master?s thesis, ErasmusMundus, University of Wolverhampton & Univer-sitat Auto`noma de Barcelona.P.
Tapanainen and T. Ja?rvinen.
1997.
A non-projectivedependency parser.
In Proceedings of the 5th Con-ference on Applied Natural Language Processing(ANLP-97), pages 64?71.I.
H. Witten and E. Frank.
2005.
Data mining: practi-cal machine learning tools and techniques.
MorganKaufmann, London, 2 edition.S.
Zhao and H.T.
Ng.
2007.
Identification and resolu-tion of Chinese zero pronouns: a machine learningapproach.
In Proceedings of the 2007 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning (EMNLP/CNLL-07), pages 541?550.715
