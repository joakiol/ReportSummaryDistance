Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 187?192,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsVSEM: An open library for visual semantics representationElia BruniUniversity of Trentoelia.bruni@unitn.itJasper UijlingsUniversity of Trentojrr@disi.unitn.itUlisse BordignonUniversity of Trentoulisse.bordignon@unitn.itIrina SergienyaUniversity of Trentoirina.sergienya@unitn.itAdam LiskaUniversity of Trentoadam.liska@unitn.itAbstractVSEM is an open library for visual se-mantics.
Starting from a collection oftagged images, it is possible to auto-matically construct an image-based rep-resentation of concepts by using off-the-shelf VSEM functionalities.
VSEM is en-tirely written in MATLAB and its object-oriented design allows a large flexibilityand reusability.
The software is accompa-nied by a website with supporting docu-mentation and examples.1 IntroductionIn the last years we have witnessed great progressin the area of automated image analysis.
Importantadvances, such as the introduction of local featuresfor a robust description of the image content (seeMikolajczyk et al(2005) for a systematic review)and the bag-of-visual-words method (BoVW)1 fora standard representation across multiple images(Sivic and Zisserman, 2003), have contributed tomake image analysis ubiquitous, with applicationsranging from robotics to biology, from medicine tophotography.Two facts have played a key role in the rapid ad-vance of these ideas.
First, the introduction of verywell defined challenges which have been attractingalso a wide community of ?outsiders" specializedin a variety of disciplines (e.g., machine learning,neural networks, graphical models and natural lan-guage processing).
Second, the sharing of effec-tive, well documented implementations of cuttingedge image analysis algorithms, such as OpenCV21Bag-of-visual-words model is a popular technique forimage classification inspired by the traditional bag-of-wordsmodel in Information Retrieval.
It represents an image withdiscrete image-describing features.
Visual words are iden-tified by clustering a large corpus of lower-level continuousfeatures.2http://opencv.org/and VLFeat.3A comparable story can be told about automatictext analysis.
The last decades have seen a longseries of successes in the processing of large textcorpora in order to extract more or less structuredsemantic knowledge.
In particular, under the as-sumption that meaning can be captured by patternsof co-occurrences of words, distributional seman-tic models such as Latent Semantic Analysis (Lan-dauer and Dumais, 1997) or Topic Models (Bleiet al 2003) have been shown to be very effectiveboth in general semantic tasks such as approximat-ing human intuitions about meaning, as well as inmore application-driven tasks such as informationretrieval, word disambiguation and query expan-sion (Turney and Pantel, 2010).
And also in thecase of automated text analysis, a wide range ofmethod implementations are at the disposal of thescientific community.4Nowadays, given the parallel success of the twodisciplines, there is growing interest in makingthe visual and textual channels interact for mutualbenefit.
If we look at the image analysis commu-nity, we discover a well established tradition ofstudies that exploit both channels of information.For example, there is a relatively extended amountof literature about enhancing the performance onvisual tasks such as object recognition or image re-trieval by replacing a purely image-based pipelinewith hybrid methods augmented with textual in-formation (Barnard et al 2003; Farhadi et al2009; Berg et al 2010; Kulkarni et al 2011).Unfortunately, the same cannot be said of theexploitation of image analysis from within the textcommunity.
Despite the huge potential that au-tomatically induced visual features could repre-sent as a new source of perceptually grounded3http://www.vlfeat.org/4See for example the annotated list of corpus-basedcomputational linguistics resources at http://www-nlp.stanford.edu/links/statnlp.html.187semantic knowledge,5 image-enhanced models ofsemantics developed so far (Feng and Lapata,2010; Bruni et al 2011; Leong and Mihalcea,2011; Bergsma and Goebel, 2011; Bruni et al2012a; Bruni et al 2012b) have only scratchedthis great potential and are still considered asproof-of-concept studies only.One possible reason of this delay with respect tothe image analysis community might be ascribedto the high entry barriers that NLP researchersadopting image analysis methods have to face.
Al-though many of the image analysis toolkits areopen source and well documented, they mainly ad-dress users within the same community and there-fore their use is not as intuitive for others.
Thefinal goal of libraries such VLFeat and OpenCVis the representation and classification of images.Therefore, they naturally lack of a series of com-plementary functionalities that are necessary tobring the visual representation to the level of se-mantic concepts.6To fill the gap we just described, we presenthereby VSEM,7 a novel toolkit which allows theextraction of image-based representations of con-cepts in an easy fashion.
VSEM is equipped withstate-of-the-art algorithms, from low-level featuredetection and description up to the BoVW repre-sentation of images, together with a set of new rou-tines necessary to move from an image-wise to aconcept-wise representation of image content.
Ina nutshell, VSEM extracts visual information in away that resembles how it is done for automatictext analysis.
Thanks to BoVW, the image con-tent is indeed discretized and visual units some-how comparable to words in text are produced (thevisual words).
In this way, from a corpus of im-ages annotated with a set of concepts, it is pos-sible to derive semantic vectors of co-occurrencecounts of concepts and visual words akin to therepresentations of words in terms of textual collo-cates in standard distributional semantics.
Impor-5In recent years, a conspicuous literature of studies hassurfaced, wherein demonstration was made of how text basedmodels are not sufficiently good at capturing the environmentwe acquire language from.
This is due to the fact that theyare lacking of perceptual information (Andrews et al 2009;Baroni et al 2010; Baroni and Lenci, 2008; Riordan andJones, 2011).6The authors of the aforementioned studies usually referto words instead of concepts.
We chose to call them conceptsto account for the both theoretical and practical differencesstanding between a word and the perceptual information itbrings along, which we define its concept.7http://clic.cimec.unitn.it/vsem/tantly, the obtained visual semantic vectors can beeasily combined with more traditional text-basedvectors to arrive at a multimodal representation ofmeaning (see e.g.
(Bruni et al 2011)).
It hasbeen shown that the resulting multimodal modelsperform better than text-only models in semantictasks such as approximating semantic similarityand relatedness ((Feng and Lapata, 2010; Bruni etal., 2012b)).VSEM functionalities concerning image anal-ysis is based on VLFeat (Vedaldi and Fulkerson,2010).
This guarantees that the image analysis un-derpinnings of the library are well maintained andstate-of-the-art.The rest of the paper is organized as follows.In Section 2 we introduce the procedure to obtainan image-based representation of a concept.
Sec-tion 3 describes the VSEM architecture.
Section4 shows how to install and run VSEM throughan example that uses the Pascal VOC data set.Section 5 concludes summarizing the material anddiscussing further directions.2 BackgroundAs shown by Feng and Lapata (2010), Bruni etal.
(2011) and Leong and Mihalcea (2011), it ispossible to construct an image-based representa-tion of a set of target concepts by starting from acollection of images depicting those concepts, en-coding the image contents into low-level features(e.g., SIFT) and scaling up to a higher level rep-resentation, based on the well-established BoVWmethod to represent images.
In addition, as shownby Bruni et al(2012b), better representations canbe extracted if the object depicting the concept isfirst localized in the image.More in detail, the pipeline encapsulating thewhole process mentioned above takes as input acollection of images together with their associatedtags and optionally object location annotations.
Itsoutput is a set of concept representation vectorsfor individual tags.
The following steps are in-volved: (i) extraction of local image features, (ii)visual vocabulary construction, (iii) encoding thelocal features in a BoVW histogram, (iv) includingspatial information with spatial binning, (v) aggre-gation of visual words on a per-concept basis inorder to obtain the co-occurrence counts for eachconcept and (vi) transforming the counts into asso-ciation scores and/or reducing the dimensionalityof the data.
A brief description of the individual188feature extractionFigure 1: An example of a visual vocabulary cre-ation pipeline.
From a set of images, a larger setof features are extracted and clustered, forming thevisual vocabulary.steps follows.Local features Local features are designed tofind local image structures in a repeatable fash-ion and to represent them in robust ways that areinvariant to typical image transformations, suchas translation, rotation, scaling, and affine defor-mation.
Local features constitute the basis ofapproaches developed to automatically recognizespecific objects (Grauman and Leibe, 2011).
Themost popular local feature extraction method is theScale Invariant Feature Transform (SIFT), intro-duced by Lowe (2004).
VSEM uses the VLFeatimplementation of SIFT.Visual vocabulary To obtain a BoVW repre-sentation of the image content, a large set of lo-cal features extracted from a large corpus of im-ages are clustered.
In this way the local fea-ture space is divided into informative regions (vi-sual words) and the collection of the obtained vi-sual words is called visual vocabulary.
k-meansis the most commonly used clustering algorithm(Grauman and Leibe, 2011).
In the special caseof Fisher encoding (see below), the clustering ofthe features is performed with a Gaussian mixturemodel (GMM), see Perronnin et al(2010).
Fig-ure 1 exemplifies a visual vocabulary constructionpipeline.
VSEM contains both the k-means andthe GMM implementations.Encoding The encoding step maps the local fea-tures extracted from an image to the correspond-ing visual words of the previously created vocab-ulary.
The most common encoding strategy iscalled hard quantization, which assigns each fea-ture to the nearest visual word?s centroid (in Eu-clidean distance).
Recently, more effective encod-ing methods have been introduced, among whichthe Fisher encoding (Perronnin et al 2010) hasbeen shown to outperform all the others (Chatfieldet al 2011).
VSEM uses both the hard quantiza-tion and the Fisher encoding.Spatial binning A consolidated way of intro-ducing spatial information in BoVW is the use ofspatial histograms (Lazebnik et al 2006).
Themain idea is to divide the image into several (spa-tial) regions, compute the encoding for each regionand stack the resulting histograms.
This techniqueis referred to as spatial binning and it is imple-mented in VSEM.
Figure 2 exemplifies the BoVWpipeline for a single image, involving local fea-tures extraction, encoding and spatial binning.feature extraction spatial binningencodingFigure 2: An example of a BoVW representationpipeline for an image.
Figure inspired by Chatfieldet al(2011).
Each feature extracted from the tar-get image is assigned to the corresponding visualword(s).
Then, spatial binning is performed.Moreover, the input of spatial binning can befurther refined by introducing localization.
Threedifferent types of localization are typically used:global, object, and surrounding.
Global extractsvisual information from the whole image and it isalso the default option when the localization in-formation is missing.
Object extracts visual infor-mation from the object location only and the sur-rounding extracts visual information from outsidethe object location.
Localization itself can eitherbe done by humans (or ground truth annotation)but also by existing localization methods (Uijlingset al 2013).For localization, VSEM uses annotated objectlocations (in the format of bounding boxes) of thetarget object.Aggregation Since each concept is representedby multiple images, an aggregation function forpooling the visual word occurrences across imageshas to be defined.
As far as we know, the sumfunction has been the only function utilized so far.An example for the aggregation step is sketched in189=cataggregationr ti+++Figure 3: An example of a concept representa-tion pipeline for cat.
First, several images depict-ing a cat are represented as vectors of visual wordcounts and, second, the vectors are aggregated intoone single concept vector.figure 3.
VSEM offers an implementation of thesum function.Transformations Once the concept-representing visual vectors are built, two typesof transformation can be performed over them torefine their raw visual word counts: associationscores and dimensionality reduction.
So far,the vectors that we have obtained represent co-occurrence counts of visual words with concepts.The goal of association scores is to distinguishinteresting co-occurrences from those that are dueto chance.
In order to do this, VSEM implementstwo versions of mutual information (pointwiseand local), see Evert (2005).On the other hand, dimensionality reductionleads to matrices that are smaller and easier towork with.
Moreover, some techniques are ableto smooth the matrices and uncover latent dimen-sions.
Common dimensionality reduction methodsare singular value decomposition (Manning et al2008), non-negative matrix factorization (Lee andSeung, 2001) and neural networks (Hinton andSalakhutdinov, 2006).
VSEM implements the sin-gular value decomposition method.3 Framework designVSEM offers a friendly implementation of thepipeline described in Section 2.
The framework isorganized into five parts, which correspond to anequal number of MATLAB packages and it is writ-ten in object-oriented programming to encouragereusability.
A description of the packages follows.?
datasets This package contains the codethat manages the image data sets.
We al-ready provide a generic wrapper for sev-eral possible dataset formats (VsemDataset).
Therefore, to use a new image data settwo solutions are possible: either write anew class which extends GenericDataset oruse directly VsemDataset after having rear-ranged the new data as described in helpVsemDataset.?
vision This package contains the code forextracting the bag-of-visual-words represen-tation of images.
In the majority of cases,it can be used as a ?black box?
by the user.Nevertheless, if the user wants to add newfunctionalities such as new features or encod-ings, this is possible by simply extending thecorresponding generic classes and the classVsemHistogramExtractor.?
concepts This is the package that dealswith the construction of the image-based rep-resentation of concepts.
concepts is themost important package of VSEM.
It ap-plies the image analysis methods to obtain theBoVW representation of the image data andthen aggregates visual word counts concept-wise.
The main class of this package isConceptSpace, which takes care of storingconcepts names and vectors and providesmanaging and transformation utilities as itsmethods.?
benchmarks VSEM offers a benchmarkingsuite to assess the quality of the visual con-cept representations.
For example, it can beused to find the optimal parametrization ofthe visual pipeline.?
helpers This package contains supportingclasses.
There is a general helpers withfunctionalities shared across packages andseveral package specific helpers.4 Getting startedInstallation VSEM can be easily installed byrunning the file vsemSetup.m.
Moreover, pascal-DatasetSetup.m can be run to download and placethe popular dataset, integrating it in the currentpipeline.190Documentation All the MATLAB commandsof VSEM are self documented (e.g.
help vsem)and an HTML version of the MATLAB commanddocumentation is available from the VSEM web-site.The Pascal VOC demo The Pascal VOC demoprovides a comprehensive example of the work-ings of VSEM.
From the demo file pascalVQDemo.mmultiple configurations are accessible.
Addi-tional settings are available and documented foreach function, class or package in the toolbox (seeDocumentation).Running the demo file executes the followinglines of code and returns as output ConceptSpace,which contains the visual concept representationsfor the Pascal data set.% Create a matlab structure with the% whole set of images in the Pascal% dataset alg with their annotationdataset = datasets.VsemDataset(configuration.imagesPath,?annotationFolder?,configuration.annotationPath);% Initiate the class that handles% the extraction of visual features.featureExtractor = vision.features.PhowFeatureExtractor();% Create the visual vocabularyvocabulary = KmeansVocabulary.trainVocabulary(dataset,featureExtractor);% Calculate semantic vectorsconceptSpace = conceptExtractor.extractConcepts(dataset,histogramExtractor);% Compute pointwise mutual% informationconceptSpace = conceptSpace.reweight();% Conclude the demo, computing% the similarity of correlation% measures of the 190 possible% pair of concepts from the Pascal% dataset against a gold standard[correlationScore, p-value] =similarityBenchmark.computeBenchmark(conceptSpace,similarityExtractor);5 ConclusionsWe have introduced VSEM, an open library for vi-sual semantics.
With VSEM it is possible to ex-tract visual semantic information from tagged im-ages and arrange such information into conceptrepresentations according to the tenets of distri-butional semantics, as applied to images insteadof text.
To analyze images, it uses state-of-the-arttechniques such as the SIFT features and the bag-of-visual-words with spatial pyramid and Fisherencoding.
In the future, we would like to addautomatic localization strategies, new aggregationfunctions and a completely new package for fusingimage- and text-based representations.ReferencesMark Andrews, Gabriella Vigliocco, and David Vin-son.
2009.
Integrating experiential and distribu-tional data to learn semantic representations.
Psy-chological Review, 116(3):463?498.Kobus Barnard, Pinar Duygulu, David Forsyth, Nandode Freitas, David Blei, and Michael Jordan.
2003.Matching words and pictures.
Journal of MachineLearning Research, 3:1107?1135.Marco Baroni and Alessandro Lenci.
2008.
Conceptsand properties in word spaces.
Italian Journal ofLinguistics, 20(1):55?88.Marco Baroni, Eduard Barbu, Brian Murphy, and Mas-simo Poesio.
2010.
Strudel: A distributional seman-tic model based on properties and types.
CognitiveScience, 34(2):222?254.Tamara Berg, Alexander Berg, and Jonathan Shih.2010.
Automatic attribute discovery and characteri-zation from noisy Web data.
In ECCV, pages 663?676, Crete, Greece.Shane Bergsma and Randy Goebel.
2011.
Using vi-sual information to predict lexical preference.
InProceedings of RANLP, pages 399?405, Hissar, Bul-garia.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet alcation.
Journal of Ma-chine Learning Research, 3:993?1022.Elia Bruni, Giang Binh Tran, and Marco Baroni.
2011.Distributional semantics from text and images.
InProceedings of the EMNLP GEMS Workshop, pages22?32, Edinburgh, UK.Elia Bruni, Gemma Boleda, Marco Baroni, andNam Khanh Tran.
2012a.
Distributional semanticsin Technicolor.
In Proceedings of ACL, pages 136?145, Jeju Island, Korea.Elia Bruni, Jasper Uijlings, Marco Baroni, and NicuSebe.
2012b.
Distributional semantics with eyes:Using image analysis to improve computational rep-resentations of word meaning.
In Proceedings ofACM Multimedia, pages 1219?1228, Nara, Japan.Ken Chatfield, Victor Lempitsky, Andrea Vedaldi, andAndrew Zisserman.
2011.
The devil is in the de-tails: an evaluation of recent feature encoding meth-ods.
In Proceedings of BMVC, Dundee, UK.191Stefan Evert.
2005.
The Statistics of Word Cooccur-rences.
Dissertation, Stuttgart University.Ali Farhadi, Ian Endres, Derek Hoiem, and DavidForsyth.
2009.
Describing objects by their at-tributes.
In Proceedings of CVPR, pages 1778?1785, Miami Beach, FL.Yansong Feng and Mirella Lapata.
2010.
Visual infor-mation in semantic representation.
In Proceedingsof HLT-NAACL, pages 91?99, Los Angeles, CA.Kristen Grauman and Bastian Leibe.
2011.
Visual Ob-ject Recognition.
Morgan & Claypool, San Fran-cisco.Geoffrey Hinton and Ruslan Salakhutdinov.
2006.
Re-ducing the dimensionality of data with neural net-works.
Science, 313(5786):504 ?
507.Girish Kulkarni, Visruth Premraj, Sagnik Dhar, SimingLi, Yejin Choi, Alexander C. Berg, and Tamara L.Berg.
2011.
Baby talk: Understanding and gener-ating simple image descriptions.
In Proceedings ofCVPR, Colorado Springs, MSA.Thomas Landauer and Susan Dumais.
1997.
A solu-tion to Plato?s problem: The latent semantic analysistheory of acquisition, induction, and representationof knowledge.
Psychological Review, 104(2):211?240.Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce.2006.
Beyond bags of features: Spatial pyramidmatching for recognizing natural scene categories.In Proceedings of CVPR, pages 2169?2178, Wash-ington, DC.Daniel D. Lee and H. Sebastian Seung.
2001.
Algo-rithms for non-negative matrix factorization.
In InNIPS, pages 556?562.
MIT Press.Chee Wee Leong and Rada Mihalcea.
2011.
Goingbeyond text: A hybrid image-text approach for mea-suring word relatedness.
In Proceedings of IJCNLP,pages 1403?1407.David Lowe.
2004.
Distinctive image features fromscale-invariant keypoints.
International Journal ofComputer Vision, 60(2), November.Chris Manning, Prabhakar Raghavan, and HinrichSch?tze.
2008.
Introduction to Information Re-trieval.
Cambridge University Press, Cambridge,UK.K.
Mikolajczyk, T. Tuytelaars, C. Schmid, A. Zisser-man, J. Matas, F. Schaffalitzky, T. Kadir, and L. V.Gool.
2005.
A Comparison of Affine Region De-tectors.
International Journal of Computer Vision,65(1).Florent Perronnin, Jorge Sanchez, and ThomasMensink.
2010.
Improving the fisher kernel forlarge-scale image classification.
In Proceedings ofECCV, pages 143?156, Berlin, Heidelberg.Brian Riordan and Michael Jones.
2011.
Redundancyin perceptual and linguistic experience: Comparingfeature-based and distributional models of semanticrepresentation.
Topics in Cognitive Science, 3(2):1?43.Josef Sivic and Andrew Zisserman.
2003.
VideoGoogle: A text retrieval approach to object match-ing in videos.
In Proceedings of ICCV, pages 1470?1477, Nice, France.Peter Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of se-mantics.
Journal of Artificial Intelligence Research,37:141?188.J.R.R.
Uijlings, K.E.A.
van de Sande, T. Gevers, andA.W.M.
Smeulders.
2013.
Selective search for ob-ject recognition.
IJCV.Andrea Vedaldi and Brian Fulkerson.
2010.
Vlfeat?
an open and portable library of computer visionalgorithms.
In Proceedings of ACM Multimedia,pages 1469?1472, Firenze, Italy.192
