Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2042?2047,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsDoes ?well-being?
translate on Twitter?Laura K. Smith1 Salvatore Giorgi1 Rishi Solanki2 Johannes C. Eichstaedt1H.
Andrew Schwartz3 Muhammad Abdul-Mageed4 Anneke Buffone1 and Lyle H. Ungar51Department of Psychology, University of Pennsylvania2Electrical and Systems Engineering, University of Pennsylvania3Computer Science, Stony Brook University4Library, Archival and Information Studies, University of British Columbia5Computer and Information Science, University of Pennsylvanialasm@sas.upenn.edu, sgiorgi@sas.upenn.eduAbstractWe investigate whether psychological well-being translates across English and Span-ish Twitter, by building and comparingsource language and automatically translatedweighted lexica in English and Spanish.
Wefind that the source language models performsubstantially better than the machine trans-lated versions.
Moreover, manually correct-ing translation errors does not improve modelperformance, suggesting that meaningful cul-tural information is being lost in translation.Further work is needed to clarify when au-tomatic translation of well-being lexica is ef-fective and how it can be improved for cross-cultural analysis.1 IntroductionInterest in sentiment analysis spans academic andcommercial domains, with wide-ranging applica-tions (Pang and Lee, 2008; Liu, 2012).
While themajority of tools for sentiment analysis have beendeveloped for English text, ideally sentiment andemotion could be analyzed across many languages.Does one need to build models for each language ofinterest, or can models be applied cross-culturally?More generally, how do cultures differ in the lan-guage they use to express sentiment and feeling?Sentiment in resource-poor languages has com-monly been assessed by first translating text into En-glish and then applying an English sentiment model(Mohammad et al, 2016).
This approach is eco-nomical and efficient, as building each model of in-terest in every target language is resource-intensive.Yet it is not clear how much culturally specificinformation and accuracy are lost in the transla-tion process, and specifically how this varies acrosslanguages, cultures, linguistic content, and corpora(e.g., social media vs. news).
While extensive workhas demonstrated that automatic machine transla-tion (MT) methods are competitive when translat-ing opinion in news and blogs, less research has ex-amined the translation of sentiment on social me-dia, and specifically on Twitter, known for its re-striction of individual exchanges to short samples oftext (140 characters) and informal language.
More-over, research has not focused on translating subjec-tive well-being specifically.Beyond sentiment, this paper investigates how ex-pressions of personal well-being translate betweenEnglish and Spanish on Twitter.
We have Englishand Spanish speakers annotate Tweets in their na-tive language for five components of subjective well-being (positive emotion, engagement, positive re-lationships, meaning, and accomplishment) (Selig-man, 2011).
We then compare how well modelstrained and tested in the same language compareto (a) models developed in one language, and thentranslated (using Google Translate) to the other lan-guage (e.g., how well English models translated toSpanish work on Spanish Tweets) and (b) how wellmodels developed in one language work on Tweetstranslated from another language (e.g., how well En-glish models work on Tweets translated from Span-ish to English).2 Related WorkThere is a vast literature on sentiment analysis whichspace precludes us from surveying; see (Liu, 2012)2042for an excellent overview.
A small but rapidly grow-ing camp is developing methods to estimate per-sonality and emotion, asking ?how does she feel?
?rather than ?how much does she like the product??
(Mohammad and Kiritchenko, 2015; Park et al,2014).
In social media, the well-being of individualsas well as communities has been studied, on variousplatforms such as Facebook and Twitter (Bollen etal., 2011; Schwartz et al, 2013; Eichstaedt et al,2015; Schwartz et al, 2016).2.1 Translating sentimentPast work has, on the whole, regarded state-of-the-art automatic translation for sentiment analysis opti-mistically.
In assessing statistical MT, (Balahur andTurchi, 2012) found that modern SMT systems canproduce reliable training data for languages otherthan English.
Comparative evaluations between En-glish and Romanian (Mihalcea et al, 2007) and En-glish and both Spanish and Romanian (Banea et al,2008) based on the English MPQA sentiment datasuggest that, in spite of word ambiguity in eitherthe source or target language, automatic translationis a viable alternative to the construction of mod-els in target languages.
Wan (2008) shows that itis useful to improve a system in a target language(Chinese) by applying ensemble methods exploit-ing sentiment-specific data and lexica from the tar-get language and a source language (English).
Morerecent work has examined how sentiment changeswith translation between English and Arabic, alsofinding that automatic translation of English textsyields competitive results (Abdul-Mageed and Diab,2014; Mohammad et al, 2016).
However, translatedtexts tend to lose sentiment information such that thetranslated data is more neutral than the source lan-guage (Salameh et al, 2015).It is less obvious how well expressions of emo-tion or subjective well-being translate between lan-guages and cultures; the words for liking a phoneor TV may be more similar across cultures than theones for finding life and relationships satisfying, orwork meaningful and engaging.2.2 Well-beingIn contrast to classic sentiment analysis, well-beingis not restricted to positive and negative emo-tion.
In 2011, the psychologist Martin Selig-man proposed PERMA (Seligman, 2011), a five-dimensional model of well-being where ?P?
standsfor positive emotion, ?E?
is engagement, ?R?
is pos-itive relationships, ?M?
is meaning, and ?A?
is asense of accomplishment.
PERMA is of interest tothis translation context because while the ?P?
dimen-sion maps relatively cleanly onto traditional con-ceptions of sentiment (i.e., positive and negativeemotion), PERMA also includes social and cogni-tive components which may be expressed with morevariation across languages and cultures.
In recentwork, Schwartz et al (2016) developed an EnglishPERMA model using Facebook data.
In this pa-per, we adopt a similar method when building ourmessage-level models over Tweets.Governments around the world are increasinglydedicating resources to the measurement of well-being to complement traditional economic indica-tors such as gross domestic product.
Being able tomeasure well-being across regions is not only be-coming more important for institutions and policy-makers, but also for private sector entities that wantto assess and promote the well-being of their orga-nizations and customers.
This raises the importanceof translation, given that resources for the measure-ment of well-being are disproportionately availablein English.3 MethodsWe collected Spanish data using the Twitter API,gathering 15.3 million geolocated Tweets betweenSeptember and November 2015 using a lati-tude/longitude bounding box around Spain.
This setwas reduced to messages containing only Spanishusing the Language Identification (LangID) Pythonpackage (Lui and Baldwin, 2012).
We restricted tomessages with an 80% or higher Spanish confidencescore as given by LangID.
This resulted in 6.1 mil-lion Tweets from 290,000 users.
We selected 5,100random messages from this set for annotation.
En-glish Tweets were similarly collected using the Twit-ter API, restricted to the US, and filtered to be (pri-marily) in English.3.1 Annotating message-level dataAmazon?s Mechanical Turk (MTurk) was used toannotate the 5,000 random English (American)2043Tweets1.
CrowdFlower, an online crowdsourcingplatform similar to MTurk, but more widely usedin Europe, was used to annotate our 5,100 randomSpanish Tweets1.
As the Tweets exclusively camefrom Spain, raters were restricted to fluent Spanishspeakers who live in Spain.On both MTurk and CrowdFlower, separate anno-tation tasks were set up for each of the 10 PERMAcomponents (positive and negative dimensions forthe 5 components).
Workers were given the defini-tion of the PERMA construct, directions on how toperform the task, and were presented with an exam-ple annotation task.
During the task workers wereasked to indicate ?to what extent does this messageexpress?
the construct in question on a scale from 1(?Not at all?)
to 7 (?Extremely?).
Directions werepresented in English for the English task, and inSpanish for the Spanish task.
The Spanish instruc-tions were translated manually from English by abilingual English-Spanish speaker and verified by anadditional bilingual speaker.In the English task, two raters assessed each mes-sage.
If the raters disagreed by more than 3 points,a rating was obtained from a third rater.
It provedmore difficult to get raters for the Spanish task, evenon CrowdFlower.
In some cases we were unable toobtain even a single annotation for a given Tweet andPERMA component.3.2 Developing weighted lexicaTweets were tokenzied using an emoticon-aware to-kenizer, ?happy fun tokenizer?1.
We then extractedunigrams and bigrams from each corpus, yieldingvocabularies of 5,430 and 4,697 ?words?
in Englishand Spanish, respectively.
The presence/absence ofthese unigrams and bigrams in each Tweet were usedas features in Lasso (L1 penalized regression) (Tib-shirani, 1996) models to predict the average anno-tation score for each of the crowdsourced PERMAlabels.
Separate models, each consisting of regres-sion weights for each term in the lexicon, were builtfor each of the ten (five positive and five nega-tive) PERMA components in both English and Span-ish1.
Each model was validated using 10-fold crossvalidation, with Pearson correlations averaged overthe 10 positive/negative PERMA components.
Re-1 Available at www.wwbp.org.sults are presented in Table 1.
The models werethen transformed into a predictive lexicon using themethods described in (Sap et al, 2014), where theweights in the lexicon were derived from the aboveLasso regression model.Model rSpanish 0.36English 0.36Table 1: Performance as measured by Pearson r correlation av-eraged over the 10 positive/negative PERMA components using10-fold cross validation.3.3 Translating the modelsWe used Google Translate to translate both the orig-inal English and Spanish Tweets and the words inthe models.
We also created versions of the trans-lated models in which we manually corrected appar-ent translation errors for 25 terms with the largestregression coefficients for each of the 10 PERMAcomponents (the top 250 terms for each model).3.4 Comparative evaluationWe evaluated how well the different modelsworked, computing the Pearson correlations be-tween message-level PERMA scores predicted fromthe different models and the ground-truth annota-tions.
Lexica were built on 80% of the messagesand then evaluated on the remaining 20%.
Figure1 shows test accuracies.
Comparing the Englishand Spanish source language and machine translatedmodels, we observe substantially better performancewhen models were built over the same language theyare applied to, i.e., using models built in Spanish topredict on Spanish Tweets.
Translating the mod-els (e.g., translating an English model to Spanishand using it on Spanish Tweets) or translating theTweets (e.g., translating Spanish Tweets to Englishand using an English model) work substantially lesswell, with translating the Tweets giving marginallybetter performance than translating the models.
Fi-nally, we translate both the model and Tweets, giv-ing slightly better performance than translating theTweets alone.
Complete PERMA lexica were thenbuilt over the entire message sets for public release.3.5 Error AnalysisTo quantify the errors in translation, we took the25 top-weighted words in each component of the2044Figure 1: Performance (Pearson r correlation) between ground-truth annotations and predicted lexica scores averaged over the10 PERMA components.PERMA lexicon (250 terms total) and manuallytranslated them with the help of a native Spanishspeaker.
The manual translations were then com-pared against the automatic translations.
Out of thetop 25 words we calculated the percentage of cor-rect automatic translations (when manual and auto-matic translations matched) and averaged the per-centages across positive and negative PERMA com-ponents.
The average percentage of correct transla-tions is listed in Table 2 as correct trans.These correctly translated terms were then com-pared to the terms in the opposite source model (i.e.,after translating English PERMA to Spanish, wecompared the translations with Spanish PERMA).We calculated the percentage of the top 250 trans-lated words missing in the 250 top words of thesource lexicon for each PERMA component and av-eraged over the 10 components.
This value is re-ported in Table 2 as missing terms.
For terms thatappeared in both the translated and source lexica wecompared their respective weights, calculating bothpercentage of terms in which the weights were ofdifferent signs and percentage of terms with sub-stantially different weights.
Again, these percent-ages were averaged over the 10 PERMA compo-nents.
Percentages are reported in Table 2 as oppsign and weight diff, respectively.
To be considered?substantially different?
the two weights must differby a factor of 2.
It is worth noting that at no pointwere the translated and source weights equal (withina tolerance of 10?5).We then looked at the errors term by term.
Out ofthe 500 terms considered (top 250 words per sourcesourcelangcorrecttransmissingtermsoppsignweightdiffEnglish 83% 81% 0.5% 6.9%Spanish 74% 91% 0.0% 4.8%Table 2: Summary of translation errors.
Percentages are av-eraged over the 10 PERMA components.
Source lang is thelanguage of the model which was translated, correct trans is thepercentage of correct automatically translated words, missingterms is the percentage of correct automatic translations withinthe 250 top terms that did not appear in the top 250 words ofother source model, opp sign is the percentage of terms whosesign switched between models, and weight diff is the percent-age of terms whose weights between the two models were offby a factor of two.PERMA term weight(en)weight(es)%chgPOS M(en)mundo*(world) 0.42 -0.18 143NEG A(en)odio**(hate) 0.29 2.19 87NEG M(en)nadie***(no one) 0.23 0.24 4.2NEG R(es)sad**(triste) 1.70 0.0012 100NEG P(es)hate***(odio) 1.81 1.75 3.3Table 3: Examples of specific errors.
Error types are denotedby asterisks: * denotes a change in sign, ** denotes the largestchange in weight and *** denotes the smallest change in weightper source model.
Language listed under each PERMA cate-gory is the language of the source model that was translated.The % chg column is percentage change relative to the largerweight.
For clarity, under each term we include its translation.language) only one term weight changed signs be-tween models: ?mundo?
(world).
The weight forthis term in the translated English to Spanish modelwas 0.42 whereas the weight in the Spanish modelwas -0.18, amounting to a 140% change.
Next, foreach source model we report terms with the largestand smallest differences in weight.
These terms andweights are reported in Table 3.
The language ab-breviation (?en?
or ?es?)
listed under each PERMAcomponent is used to denote the source language wetranslated from.
For example, (en) indicates thatwe started with English PERMA, translated it intoSpanish and then compared to Spanish PERMA.20454 DiscussionThe difference in performance between source andmachine translated models can be attributed to a fewmain problems.
First, the translation might be in-accurate (e.g., from our corpus, ?te?
is not in fact?tea?).
We manually corrected translation errors inthe prediction models with the help of a native Span-ish speaker, but found that translation error accountsfor marginal discrepancy between the source lan-guage and machine translated models.A second source of errors are translations whichare technically accurate, yet do not translate cultur-ally.
For instance, even though ?andaluces?
trans-lated correctly into ?Andalusians,?
?Andalusia?
(anautonomous community in Spain) does not invokethe same cultural meaning in English as it does forSpaniards.
A machine would be hard-pressed totranslate ?Andalusia?
into a relevant region withinthe U.S. that might invoke similar popular sentiment.Although Spanish and American people share someholidays, musicians, and sports heroes, many ofthese differ (e.g., ?Iker Casillas?
is not well knownin the U.S. and ?La selectividad?
may be similar tothe ?SATs,?
but this is not captured in MT).A third source of error stems from cultural dif-ferences, with certain topics resonating differentlycross-culturally.
For instance, when comparing thehighest weighted positive terms across PERMA, re-ligious language (e.g., ?god,?
?blessed?)
appears inEnglish but not Spanish, fitting with the popular no-tion that Americans are more religious than Euro-peans.
Spanish PERMA?s positive emotion com-ponent contains multiple highly weighted instancesof laughter; none have high weights in the Englishmodel.
Highly weighted English negative emo-tion terms are marked by general aggression (e.g.,?kill,?
?stupid?)
whereas the highest weighted Span-ish terms include derogatory terms for disliked peo-ple (e.g., ?douchebag,?
?fool?).
The American posi-tive relationship component is marked by words like?friend?
and ?friends,?
while ?sister?
is weightedmore highly in Spanish PERMA.Note that this is fundamentally a problem of do-main adaptation rather than MT, as our error analy-sis revealed that the majority of top-weighted termswere exclusive to one source model.
Different cul-tures use different words (or at least vastly differentword frequencies) when revealing the same kind ofwell-being.
Exploring where the sentiment around asimilar concept diverges across languages can pro-vide insight to researchers studying cross-culturalvariation.4.1 LimitationsThis work has significant limitations.
First, the En-glish and Spanish annotation processes, though keptas similar as possible, were not identical; annota-tions were gathered on different platforms, and dueto our difficulty in recruiting Spanish raters, our totalannotations per message varied across tasks.
Addi-tionally, the models were built over relatively smallcorpora of 5,000 English Tweets and 5,100 Span-ish Tweets.
These Tweets came from different timeperiods, which may further reduce similarity be-tween the Spanish and English corpora.
Finally, ourmethod does not account for the presence of varioussub-cultures within the United States and Spain.5 ConclusionIn this work, we investigated how well expressionsof subjective well-being translate across English andSpanish Twitter, finding that the source languagemodels performed substantially better than the ma-chine translated versions.
Moreover, manually cor-recting translation errors in the top 250 terms of thelexica did not improve model performance, suggest-ing that meaningful cultural information was lost intranslation.Our findings suggest that further work isneeded to understand when automatic translation oflanguage-based models will lead to competitive sen-timent translation on social media and how suchtranslations can be improved.
Cultural differencesseem more important than language differences, atleast for the tasks we studied here.
We expect thatlanguage indicators of personality and emotion willsimilarly translate poorly, but that remains to bestudied.AcknowledgmentsThe authors acknowledge support from the Temple-ton Religion Trust (grant TRT-0048) and Bioibe?rica.2046ReferencesMuhammad Abdul-Mageed and Mona T Diab.
2014.Sana: A large scale multi-genre, multi-dialect lexiconfor arabic subjectivity and sentiment analysis.
In Pro-ceedings of the 9th edition of the Language Resourcesand Evaluation Conference, LREC, pages 1162?1169.Alexandra Balahur and Marco Turchi.
2012.
Multilin-gual sentiment analysis using machine translation?
InProceedings of the 3rd Workshop on ComputationalApproaches to Subjectivity and Sentiment Analysis,WASSA, pages 52?60.Carmen Banea, Rada Mihalcea, Janyce Wiebe, andSamer Hassan.
2008.
Multilingual subjectivity analy-sis using machine translation.
In Proceedings of the2008 Conference on Empirical Methods in NaturalLanguage Processing, EMNLP, pages 127?135.Johan Bollen, Huina Mao, and Alberto Pepe.
2011.Modeling public mood and emotion: Twitter sentimentand socio-economic phenomena.
In Proceedings ofthe Fifth International Conference on Weblogs and So-cial Media, ICWSM, pages 450?453.Johannes C Eichstaedt, H Andrew Schwartz, Margaret LKern, Gregory Park, Darwin R Labarthe, Raina MMerchant, et al 2015.
Psychological language ontwitter predicts county-level heart disease mortality.Psychological Science, 26(2):159?169.Bing Liu.
2012.
Sentiment analysis and opinion min-ing.
Synthesis lectures on human language technolo-gies, 5(1):1?167.Marco Lui and Timothy Baldwin.
2012. langid.
py: Anoff-the-shelf language identification tool.
In Proceed-ings of the ACL 2012 system demonstrations, ACL,pages 25?30.Rada Mihalcea, Carmen Banea, and Janyce Wiebe.
2007.Learning multilingual subjective language via cross-lingual projections.
In Proceedings of the 45th AnnualMeeting of the Association for Computational Linguis-tics, ACL, pages 976?983.Saif M Mohammad and Svetlana Kiritchenko.
2015.
Us-ing hashtags to capture fine emotion categories fromtweets.
Computational Intelligence, 31(2):301?326.Saif M Mohammad, Mohammad Salameh, and SvetlanaKiritchenko.
2016.
How translation alters sentiment.Journal of Artificial Intelligence Research, 55:95?130.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and Trends in Infor-mation Retrieval, 2(1?2):1?135.Greg Park, H Andrew Schwartz, Johannes C Eichstaedt,Margaret L Kern, David J Stillwell, Michal Kosinski,et al 2014.
Automatic personality assessment throughsocial media language.
Journal of Personality and So-cial Psychology, 108:934?952.Mohammad Salameh, Saif M Mohammad, and SvetlanaKiritchenko.
2015.
Sentiment after translation: Acase-study on arabic social media posts.
In Proceed-ings of the 2015 Conference of the North AmericanChapter of the Association for Computational Linguis-tics: Human Language Technologies, NAACL, pages767?777.Maarten Sap, Greg Park, Johannes C Eichstaedt, Mar-garet L Kern, David J Stillwell, Michal Kosinski, et al2014.
Developing age and gender predictive lexicaover social media.
In Proceedings of the 2014 Con-ference on Empirical Methods In Natural LanguageProcessing, EMNLP, pages 1146?1151.H Andrew Schwartz, Johannes C Eichstaedt, Margaret LKern, Lukasz Dziurzynski, Richard E Lucas, MeghaAgrawal, et al 2013.
Characterizing geographic vari-ation in well-being using tweets.
In Proceedings of the7th International AAAI Conference on Weblogs andSocial Media, ICWSM.H Andrew Schwartz, Maarten Sap, Margaret L Kern,Johannes C Eichstaedt, Adam Kapelner, MeghaAgrawal, et al 2016.
Predicting individual well-beingthrough the language of social media.
In Biocom-puting 2016: Proceedings of the Pacific Symposium,pages 516?527.Martin EP Seligman.
2011.
Flourish.
Free Press, NewYork, NY.Robert Tibshirani.
1996.
Regression shrinkage and se-lection via the lasso.
Journal of the Royal StatisticalSociety.
Series B (Methodological), pages 267?288.Xiaojun Wan.
2008.
Using bilingual knowledge and en-semble techniques for unsupervised Chinese sentimentanalysis.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Processing,EMNLP, pages 553?561.2047
