Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 549?554,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsCoarse-grained Argumentation Features for Scoring Persuasive EssaysDebanjan Ghosh?, Aquila Khanam?, Yubo Han?and Smaranda Muresan?
?School of Communication and Information, Rutgers University, NJ, USA?Department of Computer Science, Columbia University, NY, USA?Center for Computational Learning Systems, Columbia University, NY, USAdebanjan.ghosh@rutgers.edu, {ak3654,yh2635,smara@columbia.edu}AbstractScoring the quality of persuasive essaysis an important goal of discourse analy-sis, addressed most recently with high-level persuasion-related features such asthesis clarity, or opinions and their targets.We investigate whether argumentation fea-tures derived from a coarse-grained argu-mentative structure of essays can help pre-dict essays scores.
We introduce a setof argumentation features related to ar-gument components (e.g., the number ofclaims and premises), argument relations(e.g., the number of supported claims)and typology of argumentative structure(chains, trees).
We show that these fea-tures are good predictors of human scoresfor TOEFL essays, both when the coarse-grained argumentative structure is man-ually annotated and automatically pre-dicted.1 IntroductionPersuasive essays are frequently used to assess stu-dents?
understanding of subject matter and to eval-uate their argumentation skills and language pro-ficiency.
For instance, the prompt for a TOEFL(Test of English as a Foreign Language) persua-sive writing task is:Do you agree or disagree with the followingstatement?
It is better to have broad knowledgeof many academic subjects than to specialize inone specific subject.
Use specific reasons and ex-amples to support your answer.Automatic essay scoring systems generally usefeatures based on grammar usage, spelling, style,and content (e.g., topics, discourse) (Attali andBurstein, 2006; Burstein, 2003).
However, recentwork has begun to explore the impact of high-level persuasion-related features, such as opinionsand their targets, thesis clarity and argumentationschemes (Farra et al, 2015; Song et al, 2014;Ong et al, 2014; Persing and Ng, 2015).
In thispaper, we investigate whether argumentation fea-tures derived from a coarse-grained, general argu-mentative structure of essays are good predictorsof holistic essay scores.
We use the argumen-tative structure proposed by Stab and Gurevych(2014a): argument components (major claims,claims, premises) and argument relations (sup-port, attack).
Figure 1(i) shows an extract froman essay written in response to the above prompt,labeled with a claim and two premises.
The ad-vantage of having a simple annotation scheme istwo-fold: it allows for more reliable human an-notations and it enables better performance for ar-gumentation mining systems designed to automat-ically identify the argumentative structure (Staband Gurevych, 2014b).The paper has two main contributions.
First,we introduce a set of argumentation features re-lated to three main dimensions of argumentativestructure: 1) features related to argument compo-nents such as the number of claims in an essay,number of premises, fraction of sentences contain-ing argument components; 2) features related toargument relations such as the number and per-centage of supported and unsupported claims; and3) features related to the typology of argumenta-tive structure such as number of chains (see Fig-ure 1(ii) for and example of chain) and trees (Sec-tion 3).
On a dataset of 107 TOEFL essays man-ually annotated with the argumentative structureproposed by Stab and Gurevych (2014a) (Section2), we show that using all the argumentation fea-tures predicts essay scores that are highly corre-lated with human scores (Section 3).
We discusswhat features are correlated with high scoring es-549Figure 1: (i) Essay extract showing a claim and two premises and (ii) the corresponding argumentativestructure (i.e., chain).says vs. low scoring essays.
Second, we showthat the argumentation features extracted based onargumentative structures automatically predictedby a state-of-the-art argumentation mining system(Stab and Gurevych, 2014b) are also good predic-tors of essays scores (Section 4).12 Data and AnnotationWe use a set of 107 essays from TOEFL11 cor-pus that was proposed for the first shared task ofNative Language Identification (Blanchard et al,2013).
The essays are sampled from 2 prompts:P1 (shown in the Introduction) and P3:Do you agree or disagree with the followingstatement?
Young people nowadays do not giveenough time to helping their communities.
Usespecific reasons and examples to support youranswer.Each essay is associated with a score: high,medium, or low.
From prompt P1, we selected25 high, 21 medium, and 16 low essays, while forprompt P3 we selected 15 essays for each of thethree scores.For annotation, we used the coarse-grainedargumentative structure proposed by Stab andGurevych (2014a): argument components (ma-jor claim, claim, premises) and argument rela-tions (support/attack).
The unit of annotation isa clause.
Our annotated dataset, TOEFLarg,includes 107 major claims, 468 claims, 603premises, and 641 number of sentences that donot contain any argument component.
To mea-sure the inter-annotator agreement we calculatedP/R/F1 measures, which are used to account forfuzzy boundaries (Wiebe et al, 2005).
The F11The annotated dataset, TOEFLarg, is available athttps://github.com/debanjanghosh/argessay ACL2016/measure for overlap matches (between two anno-tators) for argument components is 73.98% and forargument relation is 67.56%.3 Argumentation Features for PredictingEssays ScoresA major contribution of this paper is a thoroughanalysis of the key features derived from a coarse-grained argumentative structure that are correlatedwith essay scores.
Based on our annotations, wepropose three groups of features (Table 1).
Thefirst group consists of features related to argumentcomponents (AC) such as the number of claims,number of premises, fraction of sentences contain-ing argument components.
One hypothesis is thatan essay with a higher percentage of argumenta-tive sentences will have a higher score.
The sec-ond group consists of features related to argumentrelations (AR), such as the number and percentageof supported claims (i.e., claims that are supportedby at least one premise) and the number and per-centage of dangling claims (i.e., claims with nosupporting premises).
In low scoring essays, testtakers often fail to justify their claims with properpremises and this phenomenon is captured by thedangling claims feature.
In contrary, in high scor-ing essays, it is common to find many claims thatare justified by premises.
We also consider thenumber of attack relations and attacks against themajor claim.
Finally, the third group consists offeatures related to the typology of argument struc-tures (TS) such as the number of argument chains(Chain), number of argument trees of height =1 (Treeh=1) and the number of argument treesof height > 1 (Treeh>1).
We define an argu-ment chain when a claim is supported by a chainof premises.
We define Treeh=1as a tree struc-ture of height 1 with more than one leaves, wherethe root is a claim and the leaves are premises550Figure 2: Typology of Argumentative Structure: Examples of (i) Treeh>1; (ii) Chain; (iii) Treeh=1FeatureGroupId Argumentation Feature Description1 # of ClaimsAC 2 # of Premises3,4 # and fraction of sentences containingargument components5, 6 # and % of supported ClaimsAR 7, 8 # and % of dangling Claims9 # of Claims supporting Major Claim10,11# of total Attacks and Attacks againstMajor Claim12 # of Argument ChainsTS 13 # of Argument Treeh=114 # of Argument Treeh>1Table 1: Argumentation Featuresor claims.
Finally, Treeh>1is a tree structure ofheight > 1, where the root is a claim and the inter-nal nodes and leaves are either supporting claimsor supporting premises.
Figure 2 shows examplesof a Treeh>1structure, a Chain structure, anda Treeh=1structure.
The dark nodes representclaims (C), lighter nodes can be either claims orpremises (C/P) and white nodes are premises (P).Figure 1 shows an extract from an essays and thecorresponding Chain structure.To measure the effectiveness of the abovefeatures in predicting the holistic essay scores(high/medium/low) we use Logistic Regression(LR) learners and evaluate the learners usingquadratic-weighted kappa (QWK) against the hu-man scores, a methodology generally used for es-say scoring (Farra et al, 2015).
QWK corrects forchance agreement between the system predictionand the human prediction, and it takes into ac-count the extent of the disagreement between la-bels.
Table 2 reports the performance for the threefeature groups as well as their combination.
Ourbaseline feature (bl) is the number of sentences inthe essay, since essay length has been shown tobe generally highly correlated with essay scores(Chodorow and Burstein, 2004).
We found that allthree feature groups individually are strongly cor-related with the human scores, much better thanFeatures Correlationsbl 0.535AC 0.758AR 0.671TS 0.691bl + AC 0.770bl + AR 0.743bl + TS 0.735AC + AR + TS 0.784bl + AC + AR + TS 0.803Table 2: Correlation of LR (10 fold CV) with hu-man scores.the baseline feature, and the AC features have thehighest correlation.
We also see that although thenumber of claims and premises can affect the scoreof an essay, the argumentative structures (i.e., howthe claims and premises are connected in an essay)are also important.
Combining all features givesthe highest QWK score (0.803).We also looked at what features are associ-ated with high scoring essays vs. low scoring es-says.
Based on the regression coefficients, we ob-serve that the high ?number and % of danglingclaims?
are strong features for low scoring es-says, whereas the ?fraction of sentences contain-ing argument components?
(AC feature), ?numberof supported claims?
(AR feature), and ?numberof Treeh=1structures?
and ?number of Treeh>1structures?
(TS features) have the highest correla-tion with high scoring essays.
For example, in agood persuasive essay, test takers are inclined touse multiple premises (e.g., reasons or examples)to support a claim, which is captured by the TSand AR features.
In addition, we notice that at-tack relations are sparse, as was the case in Staband Gurevych (2014b) dataset and thus the coef-ficients for attack relations features (#10, #11 inTable 1) are negligible.In summary, our findings contribute to researchon essay scoring, showing that argumentation fea-tures are good predictors of essay scores, besidesspelling, grammar, and stylistic properties of text.5514 Automatic Extraction ofArgumentation Features for PredictingEssay ScoresTo automatically generate the argumentation fea-tures (Table 1), we first need to identify the argu-mentative structures: argument components (ma-jor claim, claim, and premise) and relations (sup-port/attack).
We use the approach proposed byStab and Gurevych (2014b).2For argument com-ponent identification, we categorize clauses to oneof the four classes (major claim (MC), claim (C),premise (P ), and None).
For argument relationidentification, given a pair of argument clausesArg1and Arg2the classifier decides whether thepair holds a support (S) or non-support (NS)relation (binary classification).
For each essay,we extract all possible combinations of Arg1andArg2from each paragraph as training data (654S and 2503 NS instances; attack relations arefew and included in NS).
We do not considerrelations that may span over multiple paragraphsto reduce number of non-support instances.
Forboth tasks we use Lexical features (e.g., uni-grams, bigrams, trigrams, modal verbs, adverbs,word-pairs for relation identification), Structuralfeatures (e.g., number of tokens/punctuations inargument, as well as in the sentence containingthe argument, argument position in essay, para-graph position (paragraph that contains the argu-ment)), Syntactic features (e.g., production rulesfrom parse trees, number of clauses in the ar-gument), and Indicators (discourse markers se-lected from the three top-level Penn DiscourseTree Bank (PDTB) relation senses: Comparison,Contingency, and Expansion (Prasad et al, 2008)).We use two settings for the classification ex-periments using libSVM (Chang and Lin, 2011)for both argument component and relation identi-fication.
In the first setting, we used the datasetof 90 high quality persuasive essays from (Staband Gurevych, 2014b) (S&G) as training and useTOEFLargfor testing (out-of-domain setting).In the second setting (in-domain), we randomlysplit the TOEFLarginto 80% training and 20%for testing (sampled equally from each category(MC, C, P , and None for argument compo-nents; S and NS for relations)).
Table 3 and 4present the classification results for identifying ar-2In future work, we plan to use the authors?
improved ap-proach and larger dataset released after the acceptance of thispaper (Stab and Gurevych, 2016).Feature Type MC C P NoneAll features 50.0 44.3 48.6 97.7top100 60.8 36.2 54.1 97.7Table 3: F1 for argument components (out-of-domain setting)Feature Type MC C P NoneAll features 78.6 53.2 64.0 96.1top100 53.8 64.5 69.2 96.2Table 4: F1 for argument components (in-domainsetting)gument components in the first and second setting,respectively.
We ran experiments for all differ-ent features groups and observe that with the ex-ception of the P class, the F1 scores for all theother classes is comparable to the results reportedby Stab and Gurevych (2014b).
One explanationof having lower performance on the P (premise)category is that the S&G dataset used for train-ing has higher quality essays, while 2/3 of ourTOEFLargdataset consists of medium and lowscoring essays (the writing style for providing rea-sons or example can differ between high and lowscoring essays).
When we select the top 100 fea-tures (?top100?)
using Information Gain (Hall etal., 2009) the F1 scores for the P class improves.The results in Table 4 show that when training andtesting on same type of essays the results are bet-ter for all categories except for MC when usingthe ?top100?
setup.Table 5 shows the results for relation identifi-cation in the first setting (out-of-domain).
TheF1 score of identifying support relations is 84.3%(or 89% using top100), much higher than re-ported by Stab and Gurevych (2014b).
We ob-tain similar results when training and testing onTOEFLarg.
We observe that two specific fea-ture groups, Structural and Lexical, individu-ally achieve high F1 scores and when combinedwith other features, they assist the classifier inreaching F1 scores in high 80s%.
There can betwo explanations for this: 1) essays in TOEFLarghave multiple short paragraphs where the posi-tion features such as position of the arguments inthe essay and paragraph (Structural group) arestrong indicators for argument relations; and 2)due to short paragraphs, the percentage of NS in-stances are less than in the S&G dataset, hence theLexical features (i.e., word-pairs between Arg1and Arg2) perform very well.552Feature Type S NSAll features 84.3 95.0top100 89.0 97.1Table 5: F1 for argument relations (out-of-domainsetting)Features CorrelationsAC 0.669AR 0.460TS 0.311AC + AR + TS 0.728All features 0.737Table 6: Correlation of LR (10 fold CV) with pre-dicted results.Based on the automatic identification of the ar-gument components and relations, we generate theargumentation features to see whether they stillpredict essays scores that are highly correlatedwith human scores.
Since our goal is to comparewith the manual annotation setup, we use the firstsetting, where we train on the S&G dataset andtest on our TOEFLargdataset.
We select the bestsystem setup (top100 for both tasks; Table 3 and5).
We ran Logistic Regression learners and eval-uated their performance using QWK scores.
Ta-ble 6 shows that the argumentative features relatedto argument relations (AR) and the typology ofargument structures (TS) extracted based on theautomatically predicated argumentative structureperform worse compared to the scores based onmanual annotations (Table 2).
Our error analy-sis shows that this is due to the wrong predictionof argument components, specifically wrongly la-beling claims as premises (Table 3).
AR and TSfeatures rely on correctly identifying the claims,and thus a wrong prediction affects the features inthese two groups, even if the accuracy of supportsrelations is high.
This also explains why the argu-ment components (AC) features still have a highcorrelation with human scores (0.669).
When weextracted the argumentation features using gold-standard argument components and predicted ar-gument relations, the correlation of AR and TSfeatures improved to 0.576 and 0.504, respectivelyand the correlation of all features reached 0.769.5 Related WorkResearchers have begun to study the impact of fea-tures specific to persuasive construct on studentessay scores (Farra et al, 2015; Song et al, 2014;Ong et al, 2014; Persing and Ng, 2013; Persingand Ng, 2015).
Farra et al (2015) investigate theimpact of opinion and target features on TOEFLessays scores.
Our work looks a step further by ex-ploring argumentation features.
Song et al (2014)show that adding features related to argumenta-tion schemes (from manual annotation) as part ofan automatic scoring system increases the corre-lation with human scores.
We show that argu-mentation features are good predictors of humanscores for TOEFL essays, both when the coarse-grained argumentative structure is manually anno-tated and automatically predicted.
Persing and Ng(2015) proposed a feature-rich approach for mod-eling argument strength in student essays, wherethe features are related to argument components.Our work explores features related to argumentcomponents, relations and typology of argumentstructures, showing that argument relation featuresshow best correlation with human scores (based onmanual annotation).6 ConclusionWe show that argumentation features derived froma coarse-grained, argumentative structure of es-says are helpful in predicting essays scores thathave a high correlation with human scores.
Ourmanual annotation study shows that features re-lated to argument relations are particularly useful.Our experiments using current methods for the au-tomatic identification of argumentative structureconfirms that distinguishing between claim andpremises is a particularly hard task.
This led tolower performance in predicting the essays scoresusing automatically generate argumentation fea-tures, especially for features related to argumentrelations and typology of structure.
As future workwe plan to improve the automatic methods foridentifying argument components similar to Staband Gurevych (2016), and to use the dataset in-troduced by Persing and Ng (2015) to investigatehow our argumentation features impact the argu-ment strength score rather than the holistic essayscore.AcknowledgementsThis paper is based on work supported by theDARPA-DEFT program.
The views expressed arethose of the authors and do not reflect the officialpolicy or position of the Department of Defenseor the U.S. Government.
The authors thank theanonymous reviewers for helpful comments.553ReferencesYigal Attali and Jill Burstein.
2006.
Automated essayscoring with e-raterR?
v. 2.
The Journal of Technol-ogy, Learning and Assessment, 4(3).Daniel Blanchard, Joel Tetreault, Derrick Higgins,Aoife Cahill, and Martin Chodorow.
2013.
Toefl11:A corpus of non-native english.
ETS Research Re-port Series, 2013(2):i?15.Jill Burstein.
2003.
The e-raterR?
scoring engine:Automated essay scoring with natural language pro-cessing.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIB-SVM: A library for support vector machines.
ACMTransactions on Intelligent Systems and Technology,2:27:1?27:27.Martin Chodorow and Jill Burstein.
2004.
Beyondessay length: Evaluating e-raterR?
?s performanceon toeflR?
essays.
ETS Research Report Series,2004(1):i?38.Noura Farra, Swapna Somasundaran, and Jill Burstein.2015.
Scoring persuasive essays using opinions andtheir targets.
In Proceedings of the Tenth Workshopon Innovative Use of NLP for Building EducationalApplications, pages 64?74, Denver, Colorado, June.Association for Computational Linguistics.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H Witten.2009.
The weka data mining software: an update.ACM SIGKDD explorations newsletter, 11(1):10?18.Nathan Ong, Diane Litman, and AlexandraBrusilovsky.
2014.
Ontology-based argumentmining and automatic essay scoring.
ACL 2014,page 24.Isaac Persing and Vincent Ng.
2013.
Modeling the-sis clarity in student essays.
In Proceedings of the51st Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages260?269.Isaac Persing and Vincent Ng.
2015.
Modeling ar-gument strength in student essays.
In Proceedingsof the 53rd Annual Meeting of the Association forComputational Linguistics and the 7th InternationalJoint Conference on Natural Language Processing(Volume 1: Long Papers), pages 543?552.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind K Joshi, and Bon-nie L Webber.
2008.
The penn discourse treebank2.0.
In LREC.
Citeseer.Yi Song, Michael Heilman, Beata Beigman Klebanov,and Paul Deane.
2014.
Applying argumentationschemes for essay scoring.
In Proceedings of theFirst Workshop on Argumentation Mining, pages69?78.Christian Stab and Iryna Gurevych.
2014a.
Annotat-ing argument components and relations in persua-sive essays.
In Proceedings of the 25th InternationalConference on Computational Linguistics (COLING2014), pages 1501?1510.Christian Stab and Iryna Gurevych.
2014b.
Identi-fying argumentative discourse structures in persua-sive essays.
In Conference on Empirical Methods inNatural Language Processing (EMNLP 2014)(Oct.2014), Association for Computational Linguistics,p.
(to appear).Christian Stab and Iryna Gurevych.
2016.
Parsing ar-gumentation structure in persuasive essays.
arXivpreprint, arxiv.org/abs/1604.07370.Janyce Wiebe, Theresa Wilson, and Claire Cardie.2005.
Annotating expressions of opinions and emo-tions in language.
Language resources and evalua-tion, 39(2-3):165?210.554
