Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 359?369,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsAnchors Regularized: Adding Robustness and Extensibilityto Scalable Topic-Modeling AlgorithmsThang NguyeniSchool and UMIACS,University of Marylandand National Library of Medicine,National Institutes of Healthdaithang@umiacs.umd.eduYuening HuComputer ScienceUniversity of Marylandynhu@cs.umd.eduJordan Boyd-GraberiSchool and UMIACSUniversity of Marylandjbg@umiacs.umd.eduAbstractSpectral methods offer scalable alternativesto Markov chain Monte Carlo and expec-tation maximization.
However, these newmethods lack the rich priors associated withprobabilistic models.
We examine Arora etal.
?s anchor words algorithm for topic mod-eling and develop new, regularized algo-rithms that not only mathematically resem-ble Gaussian and Dirichlet priors but alsoimprove the interpretability of topic models.Our new regularization approaches makethese efficient algorithms more flexible; wealso show that these methods can be com-bined with informed priors.1 IntroductionTopic models are of practical and theoretical inter-est.
Practically, they have been used to understandpolitical perspective (Paul and Girju, 2010), im-prove machine translation (Eidelman et al, 2012),reveal literary trends (Jockers, 2013), and under-stand scientific discourse (Hall et al, 2008).
The-oretically, their latent variable formulation hasserved as a foundation for more robust modelsof other linguistic phenomena (Brody and Lapata,2009).Modern topic models are formulated as a la-tent variable model.
Like hidden Markov mod-els (Rabiner, 1989, HMM), each token comes fromone of K unknown distributions.
Unlike a HMM,topic models assume that each document is an ad-mixture of these hidden components called topics.Posterior inference discovers the hidden variablesthat best explain a dataset.
Typical solutions useMCMC (Griffiths and Steyvers, 2004) or variationalEM (Blei et al, 2003), which can be viewed as localoptimization: searching for the latent variables thatmaximize the data likelihood.An exciting vein of new research providesprovable polynomial-time alternatives.
These ap-proaches provide solutions to hidden Markov mod-els (Anandkumar et al, 2012), mixture mod-els (Kannan et al, 2005), and latent variable gram-mars (Cohen et al, 2013).
The key insight is not todirectly optimize observation likelihood but to in-stead discover latent variables that can reconstructstatistics of the assumed generative model.
Unlikesearch-based methods, which can be caught in lo-cal minima, these techniques are often guaranteedto find global optima.These general techniques can be improved bymaking reasonable assumptions about the models.For example, Arora et al (2012b)?s approach for in-ference in topic models assume that each topic hasa unique ?anchor?
word (thus, we call this approachanchor).
This approach is fast and effective; be-cause it only uses word co-occurrence information,it can scale to much larger datasets than MCMC orEM alternatives.
We review the anchor method inSection 2.Despite their advantages, these techniques arenot a panacea.
They do not accommodate therich priors that modelers have come to expect.Priors can improve performance (Wallach et al,2009), provide domain adaptation (Daum?e III,2007; Finkel and Manning, 2009), and guide mod-els to reflect users?
needs (Hu et al, 2013).
InSection 3, we regularize the anchor method totrade-off the reconstruction fidelity with the penaltyterms that mimic Gaussian and Dirichlet priors.Another shortcoming is that these models havenot been scrutinized using standard NLP evalua-tions.
Because these approaches emerged fromthe theory community, anchor?s evaluations, whenpresent, typically use training reconstruction.
InSection 4, we show that our regularized models cangeneralize to previously unseen data?as measuredby held-out likelihood (Blei et al, 2003)?and aremore interpretable (Chang et al, 2009; Newmanet al, 2010).
We also show that our extension tothe anchor method enables new applications: for359K number of topicsV vocabulary sizeM document frequency: minimum documents an an-chor word candidate must appear inQ word co-occurrence matrixQi,j= p(w1= i, w2= j)?Q conditional distribution of Q?Qi,j= p(w1= j |w2= i)?Qi,?row i of?QA topic matrix, of size V ?KAj,k= p(w = j | z = k)C anchor coefficient of size K ?
VCj,k= p(z = k |w = j)S set of anchor word indexes {s1, .
.
.
sK}?
regularization weightTable 1: Notation used.
Matrices are in bold(Q,C), sets are in script Sexample, using an informed priors to discover con-cepts of interest.Having shown that regularization does improveperformance, in Section 5 we explore why.
Wediscuss the trade-off of training data reconstructionwith sparsity and why regularized topics are moreinterpretable.2 Anchor Words: Scalable Topic ModelsIn this section, we briefly review the anchormethod and place it in the context of topic modelinference.
Once we have established the anchorobjective function, in the next section we regularizethe objective function.Rethinking Data: Word Co-occurrence Infer-ence in topic models can be viewed as a black box:given a set of documents, discover the topics thatbest explain the data.
The difference between an-chor and conventional inference is that while con-ventional methods take a collection of documentsas input, anchor takes word co-occurrence statis-tics.
Given a vocabulary of size V , we representthis joint distribution asQi,j= p(w1= i, w2= j),each cell represents the probability of words appear-ing together in a document.Like other topic modeling algorithms, the outputof the anchor method is the topic word distribu-tions A with size V ?
K, where K is the totalnumber of topics desired, a parameter of the al-gorithm.
The kthcolumn of A will be the topicdistribution over all words for topic k, and Aw,kisthe probability of observing type w given topic k.Anchors: Topic Representatives The anchormethod (Arora et al, 2012a) is based on the sepa-rability assumption (Donoho and Stodden, 2003),which assumes that each topic contains at least onenamesake ?anchor word?
that has non-zero proba-bility only in that topic.
Intuitively, this means thateach topic has unique, specific word that, whenused, identifies that topic.
For example, while?run?, ?base?, ?fly?, and ?shortstop?
are associatedwith a topic about baseball, only ?shortstop?
is un-ambiguous, so it could serve as this topic?s anchorword.Let?s assume that we knew what the anchorwords were: a set S that indexes rows in Q. Nowconsider the conditional distribution of word i,the probability of the rest of the vocabulary givenan observation of word i; we represent this as?Qi,?,as we can construct this by normalizing the rows ofQ.
For an anchor word sa?
S, this will look likea topic;?Q?shortstop?,?will have high probabilityfor words associated with baseball.The key insight of the anchor algorithm is thatthe conditional distribution of polysemous non-anchor words can be reconstructed as a linear com-bination of the conditional distributions of anchorwords.
For example,?Q?fly?,?could be recon-structed by combining the anchor words ?insecta?,?boeing?, and ?shortshop?.
We represent the coeffi-cients of this reconstruction as a matrix C, whereCi,k= p(z = k |w = i).
Thus, for any word i,?Qi,???sk?SCi,k?Qsk,?.
(1)The coefficient matrix is not the usual output of atopic modeling algorithm.
The usual output is theprobability of a word given a topic.
The coefficientmatrix C is the probability of a topic given a word.We use Bayes rule to recover the topic distributionp(w = i|z = k) ?Ai,k?
p(z = k|w = i)p(w = i)= Ci,k?j?Qi,j(2)where p(w) is the normalizer of Q to obtain?Qw,?.The geometric argument for finding the anchorwords is one of the key contributions of Arora etal.
(2012a) and is beyond the scope of this paper.The algorithms in Section 3 use the anchor selec-tion subroutine unchanged.
The difference in ourapproach is in how we discover the anchor coeffi-cients C.From Anchors to Topics After we have the an-chor words, we need to find the coefficients that360best reconstruct the data?Q (Equation 1).
Aroraet al (2012a) chose the C that minimizes the KLdivergence between?Qi,?and the reconstructionbased on the anchor word?s conditional word vec-tors?sk?SCi,k?Qsk,?,Ci,?= argminCi,?DKL???Qi,?||?sk?SCi,k?Qsk,???.
(3)The anchor method is fast, as it only de-pends on the size of the vocabulary once the co-occurrence statistics Q are obtained.
However, itdoes not support rich priors for topic models, whileMCMC (Griffiths and Steyvers, 2004) and varia-tional EM (Blei et al, 2003) methods can.
Thisprevents models from using priors to guide themodels to discover particular themes (Zhai et al,2012), or to encourage sparsity in the models (Yaoet al, 2009).
In the rest of this paper, we correctthis lacuna by adding regularization inspired byBayesian priors to the anchor algorithm.3 Adding RegularizationIn this section, we add regularizers to the anchorobjective (Equation 3).
In this section, we brieflyreview regularizers and then add two regularizers,inspired by Gaussian (L2, Section 3.1) and Dirich-let priors (Beta, Section 3.2), to the anchor objec-tive function (Equation 3).Regularization terms are ubiquitous.
They typ-ically appear as an additional term in an opti-mization problem.
Instead of optimizing a func-tion just of the data x and parameters ?, f(x, ?
),one optimizes an objective function that includesa regularizer that is only a function of parame-ters: f(w, ?)
+ r(?).
Regularizers are critical instaid methods like linear regression (Ng, 2004),in workhorse methods such as maximum entropymodeling (Dud?
?k et al, 2004), and also in emergingfields such as deep learning (Wager et al, 2013).In addition to being useful, regularization termsare appealing theoretically because they often corre-spond to probabilistic interpretations of parameters.For example, if we are seeking the MLE of a proba-bilistic model parameterized by ?, p(x|?
), addinga regularization term r(?)
=?Li=1?2icorrespondsto adding a Gaussian priorf(?i) =1?2pi?2exp{?
?2i2?2}(4)Corpus Train Dev Test VocabNIPS 1231 247 262 1218220NEWS 11243 3760 3726 81604NYT 9255 2012 1959 34940Table 2: The number of documents in the train,development, and test folds in our three datasets.and maximizing log probability of the posterior(ignoring constant terms) (Rennie, 2003).3.1 L2RegularizationThe simplest form of regularization we can add isL2regularization.
This is similar to assuming thatprobability of a word given a topic comes from aGaussian distribution.
While the distribution overtopics is typically Dirichlet, Dirichlet distributionshave been replaced by logistic normals in topicmodeling applications (Blei and Lafferty, 2005)and for probabilistic grammars of language (Cohenand Smith, 2009).Augmenting the anchor objective with an L2penalty yieldsCi,?=argminCi,?DKL???Qi,?||?sk?SCi,k?Qsk,??
?+ ??Ci,??
?i,?
?22, (5)where regularization weight ?
balances the impor-tance of a high-fidelity reconstruction against theregularization, which encourages the anchor coeffi-cients to be close to the vector ?.
When the meanvector ?
is zero, this encourages the topic coeffi-cients to be zero.
In Section 4.3, we use a non-zeromean ?
to encode an informed prior to encouragetopics to discover specific concepts.3.2 Beta RegularizationThe more common prior for topic models is aDirichlet prior (Minka, 2000).
However, we cannotapply this directly because the optimization is doneon a row-by-row basis of the anchor coefficientmatrix C, optimizing C for a fixed word w for andall topics.
If we want to model the probability ofa word, it must be the probability of word w in atopic versus all other words.Modeling this dichotomy (one versus all othersin a topic) is possible.
The constructive definitionof the Dirichlet distribution (Sethuraman, 1994)states that if one has a V -dimensional multinomial?
?
Dir(?1.
.
.
?V), then the marginal distribution361of ?wfollows ?w?
Beta(?w,?i 6=w?i).
This isthe tool we need to consider the distribution of asingle word?s probability.This requires including the topic matrix as partof the objective function.
The topic matrix is a lin-ear transformation of the coefficient matrix (Equa-tion 2).
The objective for beta regularization be-comesCi,?=argminCi,?DKL???Qi,?||?sk?SCi,k?Qsk,????
?
?sk?Slog (Beta(Ai,k; a, b)), (6)where ?
again balances reconstruction against theregularization.
To ensure the tractability of thisalgorithm, we enforce a convex regularization func-tion, which requires that a > 1 and b > 1.
If weenforce a uniform prior?EBeta(a,b)[Ai,k] =1V?and that the mode of the distribution is also1V,1this gives us the following parametric form for aand b:a =xV+ 1, and b =(V ?
1)xV+ 1 (7)for real x greater than zero.3.3 Initialization and ConvergenceEquation 5 and Equation 6 are optimized using L-BFGS gradient optimization (Galassi et al, 2003).We initialize C randomly from Dir(?)
with ?
=60V(Wallach et al, 2009).
We update C after opti-mizing all V rows.
The newly updated C replacesthe old topic coefficients.
We track how muchthe topic coefficients C change between two con-secutive iterations i and i + 1 and represent it as?C ?
?Ci+1?Ci?2.
We stop optimization when?C ?
?.
When ?
= 0.1, the L2and unregularizedanchor algorithm converges after a single iteration,while beta regularization typically converges afterfewer than ten iterations (Figure 4).4 Regularization Improves Topic ModelsIn this section, we measure the performance ofour proposed regularized anchor word algorithms.We will refer to specific algorithms in bold.
Forexample, the original anchor algorithm is an-chor.
Our L2regularized variant is anchor-L2,1For a, b < 1, the expected value is still the uniformdistribution but the mode lies at the boundaries of the simplex.This corresponds to a sparse Dirichlet distribution, which ouroptimization cannot at present model.and our beta regularized variant is anchor-beta.To provide conventional baselines, we also com-pare our methods against topic models from varia-tional inference (Blei et al, 2003, variational) andMCMC (Griffiths and Steyvers, 2004; McCallum,2002, MCMC).We apply these inference strategies on three di-verse corpora: scientific articles from the NeuralInformation Processing Society (NIPS),2Internetnewsgroups postings (20NEWS),3and New YorkTimes editorials (Sandhaus, 2008, NYT).
Statisticsfor the datasets are summarized in Table 2.
We spliteach dataset into a training fold (70%), develop-ment fold (15%), and a test fold (15%): the trainingdata are used to fit models; the development set areused to select parameters (anchor thresholdM , doc-ument prior ?, regularization weight ?
); and finalresults are reported on the test fold.We use two evaluation measures, held-out likeli-hood (Blei et al, 2003, HL) and topic interpretabil-ity (Chang et al, 2009; Newman et al, 2010, TI).Held-out likelihood measures how well the modelcan reconstruct held-out documents that the modelhas never seen before.
This is the typical evaluationfor probabilistic models.
Topic interpretability is amore recent metric to capture how useful the topicscan be to human users attempting to make sense ofa large datasets.Held-out likelihood cannot be computed withexisting anchor algorithms, so we use the topicdistributions learned from anchor as input to a ref-erence variational inference implementation (Bleiet al, 2003) to compute HL.
This requires an ad-ditional parameter, the Dirichlet prior ?
for theper-document distribution over topics.
We select ?using grid search on the development set.To compute TI and evaluate topic coherence,we use normalized pairwise mutual informa-tion (NPMI) (Lau et al, 2014) over topics?
twentymost probable words.
Topic coherence is com-puted against the NPMI of a reference corpus.
Forcoherence evaluations, we use both intrinsic andextrinsic text collections to compute NPMI.
Intrin-sic coherence (TI-i) is computed on training anddevelopment data at development time and on train-ing and test data at test time.
Extrinsic coherence(TI-e) is computed from English Wikipedia articles,with disjoint halves (1.1 million pages each) fordistinct development and testing TI-e evaluation.2http://cs.nyu.edu/?roweis/data.html3http://qwone.com/?jason/20Newsgroups/362ll l l llllllllll lllll l ll l l l?392?390?388?4720?4710?4700?4690?4680?890.0?887.5?885.0?882.520NEWSNIPSNYT100 300 500 700 900Document Frequency MHLScore l ll l lllllll l llllllll ll l l l0.020.030.040.050.060.070.0550.0600.0650.060.070.080.090.1020NEWSNIPSNYT100 300 500 700 900Document Frequency MTI?i ScoreFigure 1: Grid search for document frequency M for our datasets with 20 topics (other configurations notshown) on development data.
The performance on both HL and TI score indicate that the unregularizedanchor algorithm is very sensitive to M .
The M selected here is applied to subsequent models.Topics l 20 40 60 80Beta L2l l l l l llllll l l l l llllll l l l l llllll l l l l llllll l l l l lllllll l l l llllll l l l l llllll l l l l llllll l l l l llllll l l l l llllll l l l l llllll l l l l lllll?410?405?400?395?390?4800?4750?4700?4650?920?910?900?890?88020NEWSNIPSNYT00.01 0.1 0.5 1 00.01 0.1 0.5 1Regularization Weight ?HLScoreTopics l 20 40 60 80Beta L2ll l ll llllll l ll l llllll ll l l llllll l lll llllll ll l l lllllll l l l llllll l l l l llllll l l l l llllll l l l l lllllll l l l llllll l l l l llllll l l l l lllll0.020.040.060.080.100.020.040.060.080.060.090.120.1520NEWSNIPSNYT0 0.01 0.1 0.5 1 0 0.01 0.1 0.5 1Regularization Weight ?TI?i ScoreFigure 2: Selection of ?
based on HL and TI scores on the development set.
The value of ?
= 0 isequivalent to the original anchor algorithm; regularized versions find better solutions as the regularizationweight ?
becomes non-zero.4.1 Grid Search for Parameters onDevelopment SetAnchor Threshold A good anchor word musthave a unique, specific context but also explainother words well.
A word that appears only oncewill have a very specific cooccurence pattern butwill explain other words?
coocurrence poorly be-cause the observations are so sparse.
As discussedin Section 2, the anchor method uses documentfrequency M as a threshold to only consider wordswith robust counts.Because all regularizations benefit equallyfrom higher-quality anchor words, we use cross-validation to select the document frequency cut-off M using the unregularized anchor algorithm.Figure 1 shows the performance of anchor withdifferent M on our three datasets with 20 topics forour two measures HL and TI-i.Regularization Weight Once we select a cutoffM for each combination of dataset, number of top-ics K and a evaluation measure, we select a reg-ularization weight ?
on the development set.
Fig-ure 2 shows that beta regularization framework im-proves topic interpretability TI-i on all datasets andimproved the held-out likelihood HL on 20NEWS.The L2regularization also improves held-out like-lihood HL for the 20NEWS corpus (Figure 2).In the interests of space, we do not show thefigures for selecting M and ?
using TI-e, which issimilar to TI-i: anchor-beta improves TI-e score onall datasets, anchor-L2improves TI-e on 20NEWSand NIPS with 20 topics and NYT with 40 topics.4.2 Evaluating RegularizationWith document frequency M and regularizationweight ?
selected from the development set, we363compare the performance of those models on thetest set.
We also compare with standard implemen-tations of Latent Dirichlet Allocation: Blei?s LDAC(variational) and Mallet (mcmc).
We run 100 iter-ations for LDAC and 5000 iterations for Mallet.Each result is averaged over three random runsand appears in Figure 3.
The highly-tuned, widely-used implementations uniformly have better held-out likelihood than anchor-based methods, but themuch faster anchor methods are often comparable.Within anchor-based methods, L2-regularizationoffers comparable held-out likelihood as unregular-ized anchor, while anchor-beta often has betterinterpretability.
Because of the mismatch betweenthe specialized vocabulary of NIPS and the general-purpose language of Wikipedia, TI-e has a highvariance.4.3 Informed RegularizationA frequent use of priors is to add information to amodel.
This is not possible with the existing an-chor method.
An informed prior for topic modelsseeds a topic with words that describe a topic of in-terest.
In a topic model, these seeds will serve as a?magnet?, attracting similar words to the topic (Zhaiet al, 2012).We can achieve a similar goal with anchor-L2.Instead of encouraging anchor coefficients to bezero in Equation 5, we can instead encourage wordprobabilities to close to an arbitrary mean ?i,k.This vector can reflect expert knowledge.One example of a source of expert knowledgeis Linguistic Inquiry and Word Count (Pennebakerand Francis, 1999, LIWC), a dictionary of key-words related to sixty-eight psychological conceptssuch as positive emotions, negative emotions, anddeath.
For example, it associates ?excessive, estate,money, cheap, expensive, living, profit, live, rich,income, poor, etc.?
for the concept materialism.We associate each anchor word with its closestLIWC category based on the cooccurrence matrixQ.
This is computed by greedily finding the an-chor word that has the highest cooccurrence scorefor any LIWC category: we define the score of acategory to anchor word wskas?iQsk,i, where iranges over words in this category; we compute thescores of all categories to all anchor words; thenwe find the highest score and assign the category tothat anchor word; we greedily repeat this processuntil all anchor words have a category.Given these associations, we create a goal mean?i,k.
If there are Lianchor words associated withLIWC word i, ?i,k=1Liif this keyword i is associ-ated with anchor word wskand zero otherwise.We apply anchor-L2with informed priors onNYT with twenty topics and compared the topicsagainst the original topics from anchor.
Table 3shows that the topic with anchor word ?soviet?,when combined with LIWC, draws in the new words?bush?
and ?nuclear?
; reflecting the threats of forceduring the cold war.
For the topic with topic word?arms?, when associated with the LIWC categorywith the terms ?agree?
and ?agreement?, drawsin ?clinton?, who represented a more conciliatoryforeign policy compared to his republican prede-cessors.5 DiscussionHaving shown that regularization can improve theanchor topic modeling algorithm, in this sectionwe discuss why these regularizations can improvethe model and the implications for practitioners.Efficiency Efficiency is a function of the numberof iterations and the cost of each iteration.
Bothanchor and anchor-L2require a single iteration,although the latter?s iteration is slightly more ex-pensive.
For beta, as described in Section 3.2,we update anchor coefficients C row by row, andthen repeat the process over several iterations untilit converges.
However, it often converges withinten iterations (Figure 4) on all three datasets: thisrequires much fewer iterations than MCMC or vari-ational inference, and the iterations are less expen-sive.
In addition, since we optimize each row Ci,?independently, the algorithm can be easily paral-lelized.Sensitivity to Document Frequency While theoriginal anchor is sensitive to the document fre-quency M (Figure 1), adding regularization makesthis less critical.
Both anchor-L2and anchor-betaare less sensitive to M than anchor.To highlight this, we compare the topics of an-chor and anchor-beta whenM = 100.
As Table 4shows, the words ?article?, ?write?, ?don?
and?doe?
appear in most of anchor?s topics.
Whileanchor-L2also has some bad topics, it still can findreasonable topics, demonstrating anchor-beta?sgreater robustness to suboptimal M .L2(Sometimes) Improves Generalization AsFigure 2 shows, anchor-L2sometimes improvesheld-out development likelihood for the smaller364Algorithm l anchor anchor?beta anchor?L2 MCMC variational20NEWSl l l ll l l lll l l?410?405?400?395?3900.030.040.050.060.070.060.080.10HLTI?eTI?i20 40 60 80topic numberNIPSlll lll lll l l l?4580?4560?4540?4520?4500?4480?44600.080.090.100.110.060.070.080.09HLTI?eTI?i20 40 60 80topic numberNYTl ll lll l ll l l l?880?870?8600.070.080.090.080.100.120.14HLTI?eTI?i20 40 60 80topic numberFigure 3: Comparing anchor-beta and anchor-L2against the original anchor and the traditional vari-ational and MCMC on HL score and TI score.
variational and mcmc provide the best held-out gener-alization.
anchor-beta sometimes gives the best TI score and is consistently better than anchor.
Thespecialized vocabulary of NIPS causes high variance for the extrinsic interpretability evaluation (TI-e).Topic Shared Words Original (Top, green) vs.
Informed L2(Bottom, orange)sovietamerican make president soviet unionwar yearsgorbachev moscow russian force economic world europe politi-cal communist lead reform germany countrymilitary state service washington bush army unite chief troopsofficer nuclear time weekdistrictassembly board city county districtmember state yorkrepresentative manhattan brooklyn queens election bronx councilisland local incumbent housing municipalpeople party group social republican year make years friendvote compromise millionpeaceamerican force government israel peacepolitical president state unitewashingtonwar military country minister leaders nation world palestinianisraeli electionoffer justice aid deserve make bush years fair clinton handarmsarms bush congress force iraq make northnuclear president state washington weaponadministration treaty missile defense war military koreareaganagree agreement american accept unite share clintonyearstradeadministration america american countryeconomic government make president statetrade unite washingtonworld market japan foreign china policy price politicalbusiness economy congress year years clinton bushbuyTable 3: Examples of topic comparison between anchor and informed anchor-L2.
A topic is labeledwith the anchor word for that topic.
The bold words are the informed prior from LIWC.
With an informedprior, relevant words appear in the top words of a topic; this also draws in other related terms (red).20NEWS corpus.
However, the ?
selected on devel-opment data does not always improve test set per-formance.
This, in Figure 3, anchor-beta closelytracks anchor.
Thus, L2regularization does nothurt generalization while imparting expressivenessand robustness to parameter settings.Beta Improves Interpretability Figure 3 showsthat anchor-beta improves topic interpretability(TI) compared to unregularized anchor methods.
Inthis section, we try to understand why.We first compare the topics from the originalanchor against anchor-beta to analyze the topicsqualitatively.
Table 5 shows that beta regulariza-tion promotes rarer words within a topic and de-motes common words.
For example, in the topicabout hockey with the anchor word game, ?run?and ?good?
?ambiguous, polysemous words?inthe unregularized topic are replaced by ?playoff?365Topic anchor anchor-betafrequentlyarticle write don doe make time people goodfile questionarticle write don doe make people time goodemail filedebatewrite article people make don doe god key gov-ernment timepeople make god article write don doe keypoint governmentwings game team write wings article win red playhockey yeargame team wings win red hockey play seasonplayer fanstats player team write game article stats year goodplay doestats player season league baseball fan team in-dividual playoff nhlcompile program file write email doe windows call prob-lem run doncompile program code file ftp advance packageerror windows sunTable 4: Topics from anchor and anchor-beta with M = 100 on 20NEWS with 20 topics.
Each topic isidentified with its associated anchor word.
When M = 100, the topics of anchor suffer: the four coloredwords appear in almost every topic.
anchor-beta, in contrast, is less sensitive to suboptimal M .llll l l l l l l l l l l l l l l l l l0102030400 5 10 15 20Iteration?CDataset l 20NEWS NIPS NYTFigure 4: Convergence of anchor coefficient C foranchor-beta.
?C is the difference of current Cfrom theC at the previous iteration.
C is convergedwithin ten iterations for all three datasets.and ?trade?
in the regularized topic.
These wordsare less ambiguous and more likely to make senseto a consumer of topic models.Figure 5 shows why this happens.
Comparedto the unregularized topics from anchor, the betaregularized topic steals from the rich and creates amore uniform distribution.
Thus, highly frequentwords do not as easily climb to the top of the distri-bution, and the topics reflect topical, relevant wordsrather than globally frequent terms.6 ConclusionA topic model is a popular tool for quickly get-ting the gist of large corpora.
However, runningsuch an analysis on these large corpora entail asubstantial computational cost.
While techniquessuch as anchor algorithms offer faster solutions, itcomes at the cost of the expressive priors commonin Bayesian formulations.This paper introduces two different regulariza-tions that offer users more interpretable modelsand the ability to inject prior knowledge withoutsacrificing the speed and generalizability of theunderlying approach.
However, one sacrifice thatthis approach does make is the beautiful theoreticalguarantees of previous work.
An important pieceof future work is a theoretical understanding ofgeneralizability in extensible, regularized models.Incorporating other regularizations could furtherimprove performance or unlock new applications.Our regularizations do not explicitly encouragesparsity; applying other regularizations such as L1could encourage true sparsity (Tibshirani, 1994),and structured priors (Andrzejewski et al, 2009)could efficiently incorporate constraints on topicmodels.These regularizations could improve spectral al-gorithms for latent variables models, improving theperformance for other NLP tasks such as latent vari-able PCFGs (Cohen et al, 2013) and HMMs (Anand-kumar et al, 2012), combining the flexibility androbustness offered by priors with the speed andaccuracy of new, scalable algorithms.AcknowledgmentsWe would like to thank the anonymous reviewers,Hal Daum?e III, Ke Wu, and Ke Zhai for their help-ful comments.
This work was supported by NSFGrant IIS-1320538.
Boyd-Graber is also supportedby NSF Grant CCF-1018625.
Any opinions, find-ings, conclusions, or recommendations expressedhere are those of the authors and do not necessarilyreflect the view of the sponsor.366computer drive game god power?20?15?10?50?20?15?10?50anchoranchor?betaRank of word in topic (topic shown by anchor word)logp(word| topic)Figure 5: How beta regularization influences the topic distribution.
Each topic is identified with itsassociated anchor word.
Compared to the unregularized anchor method, anchor-beta steals probabilitymass from the ?rich?
and prefers a smoother distribution of probability mass.
These words often tend tobe unimportant, polysemous words common across topics.Topic Shared Words anchor (Top, green) vs. anchor-beta (Bottom, orange)computer computer means science screensystem phone university problem doe work windows internetsoftware chip mac set fax technology information dataquote mhz pro processor ship remote print devices complex cpuelectrical transfer ray engineering serial reducepowerpower play period supplyground light battery enginecar good make high problem work back turn control currentsmall timecircuit oil wire unit water heat hot ranger input total joe pluggodgod jesus christian bible faith church life christ beliefreligion hell word lord truth lovepeople make things true doesin christianity atheist peace heavengamegame team player play win fan hockey season baseballred wings score division league goal leaf cup torontorun goodplayoff tradedrivedrive disk hard scsi controller card floppy ide mac busspeed monitor switch apple cable internal port megproblem workram pinTable 5: Comparing topics?labeled by their anchor word?from anchor and anchor-beta.
With betaregularization, relevant words are promoted, while more general words are suppressed, improving topiccoherence.ReferencesAnimashree Anandkumar, Daniel Hsu, and Sham M.Kakade.
2012.
A method of moments for mixturemodels and hidden markov models.
In Proceedingsof Conference on Learning Theory.David Andrzejewski, Xiaojin Zhu, and Mark Craven.2009.
Incorporating domain knowledge into topicmodeling via Dirichlet forest priors.
In Proceedingsof the International Conference of Machine Learn-ing.Sanjeev Arora, Rong Ge, Yoni Halpern, David M.Mimno, Ankur Moitra, David Sontag, Yichen Wu,and Michael Zhu.
2012a.
A practical algorithmfor topic modeling with provable guarantees.
CoRR,abs/1212.4777.Sanjeev Arora, Rong Ge, and Ankur Moitra.
2012b.Learning topic models - going beyond svd.
CoRR,abs/1204.1956.David M. Blei and John D. Lafferty.
2005.
Correlatedtopic models.
In Proceedings of Advances in NeuralInformation Processing Systems.David M. Blei, Andrew Ng, and Michael Jordan.
2003.Latent Dirichlet alocation.
Journal of MachineLearning Research, 3.Samuel Brody and Mirella Lapata.
2009.
Bayesianword sense induction.
In Proceedings of the Euro-pean Chapter of the Association for ComputationalLinguistics, Athens, Greece.Jonathan Chang, Jordan Boyd-Graber, Chong Wang,Sean Gerrish, and David M. Blei.
2009.
Readingtea leaves: How humans interpret topic models.
InProceedings of Advances in Neural Information Pro-cessing Systems.Shay B. Cohen and Noah A. Smith.
2009.
Shared lo-gistic normal distributions for soft parameter tyingin unsupervised grammar induction.
In Conferenceof the North American Chapter of the Associationfor Computational Linguistics.367Shay Cohen, Karl Stratos, Michael Collins, Dean P.Foster, and Lyle Ungar.
2013.
Experiments withspectral learning of latent-variable PCFGs.
In Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics.Hal Daum?e III.
2007.
Frustratingly easy domain adap-tation.
In Proceedings of the Association for Com-putational Linguistics.David Donoho and Victoria Stodden.
2003.
Whendoes non-negative matrix factorization give correctdecomposition into parts?
page 2004.
MIT Press.Miroslav Dud?
?k, Steven J. Phillips, and Robert E.Schapire.
2004.
Performance guarantees for reg-ularized maximum entropy density estimation.
InProceedings of Conference on Learning Theory.Vladimir Eidelman, Jordan Boyd-Graber, and PhilipResnik.
2012.
Topic models for dynamic translationmodel adaptation.
In Proceedings of the Associationfor Computational Linguistics.Jenny Rose Finkel and Christopher D. Manning.
2009.Hierarchical bayesian domain adaptation.
In Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics, Morristown, NJ,USA.Mark Galassi, Jim Davies, James Theiler, Brian Gough,Gerard Jungman, Michael Booth, and Fabrice Rossi.2003.
Gnu Scientific Library: Reference Manual.Network Theory Ltd.Thomas L. Griffiths and Mark Steyvers.
2004.
Find-ing scientific topics.
Proceedings of the NationalAcademy of Sciences, 101(Suppl 1):5228?5235.David Hall, Daniel Jurafsky, and Christopher D. Man-ning.
2008.
Studying the history of ideas usingtopic models.
In Proceedings of Emperical Methodsin Natural Language Processing.Yuening Hu, Jordan Boyd-Graber, Brianna Satinoff,and Alison Smith.
2013.
Interactive topic modeling.Machine Learning Journal.Matt L. Jockers.
2013.
Macroanalysis: Digital Meth-ods and Literary History.
Topics in the Digital Hu-manities.
University of Illinois Press.Ravindran Kannan, Hadi Salmasian, and Santosh Vem-pala.
2005.
The spectral method for general mixturemodels.
In Proceedings of Conference on LearningTheory.Ken Lang.
2007.
20 newsgroups data set.Jey Han Lau, David Newman, and Timothy Baldwin.2014.
Machine reading tea leaves: Automaticallyevaluating topic coherence and topic model quality.In Proceedings of the European Chapter of the Asso-ciation for Computational Linguistics.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://www.cs.umass.edu/ mccallum/mallet.Thomas P. Minka.
2000.
Estimating adirichlet distribution.
Technical report, Mi-crosoft.
http://research.microsoft.com/en-us/um/people/minka/papers/dirichlet/.David Newman, Jey Han Lau, Karl Grieser, and Timo-thy Baldwin.
2010.
Automatic evaluation of topiccoherence.
In Conference of the North AmericanChapter of the Association for Computational Lin-guistics.Andrew Y. Ng.
2004.
Feature selection, l1 vs. l2 regu-larization, and rotational invariance.
In Proceedingsof the International Conference of Machine Learn-ing.Michael Paul and Roxana Girju.
2010.
A two-dimensional topic-aspect model for discoveringmulti-faceted topics.
In Association for the Advance-ment of Artificial Intelligence.James W. Pennebaker and Martha E. Francis.
1999.Linguistic Inquiry and Word Count.
Lawrence Erl-baum, 1 edition, August.Lawrence R. Rabiner.
1989.
A tutorial on hiddenMarkov models and selected applications in speechrecognition.
Proceedings of the IEEE, 77(2):257?286.Jason Rennie.
2003.
On l2-norm regularization andthe Gaussian prior.Sam Roweis.
2002.
NIPS 1-12 Dataset.Evan Sandhaus.
2008.
The NewYork Times annotated corpus.http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2008T19.Jayaram Sethuraman.
1994.
A constructive definitionof Dirichlet priors.
Statistica Sinica, 4:639?650.Robert Tibshirani.
1994.
Regression shrinkage and se-lection via the lasso.
Journal of the Royal StatisticalSociety, Series B, 58:267?288.Stefan Wager, Sida Wang, and Percy Liang.
2013.Dropout training as adaptive regularization.
In Pro-ceedings of Advances in Neural Information Pro-cessing Systems, pages 351?359.Hanna Wallach, David Mimno, and Andrew McCal-lum.
2009.
Rethinking LDA: Why priors matter.In Proceedings of Advances in Neural InformationProcessing Systems.Limin Yao, David Mimno, and Andrew McCallum.2009.
Efficient methods for topic model inferenceon streaming document collections.
In KnowledgeDiscovery and Data Mining.368Ke Zhai, Jordan Boyd-Graber, Nima Asadi, and Mo-hamad Alkhouja.
2012.
Mr. LDA: A flexible largescale topic modeling package using variational infer-ence in mapreduce.
In Proceedings of World WideWeb Conference.369
