Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 279?285,Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational LinguisticsAn Approach to the Automated Evaluation of Pipeline Architectures inNatural Language Dialogue SystemsEliza Margaretha?
and David DeVaultUSC Institute for Creative Technologies, 12015 Waterfront Dr., Playa Vista, CA 90094elizam@coli.uni-saarland.dedevault@ict.usc.eduAbstractWe present an approach to performing auto-mated evaluations of pipeline architectures innatural language dialogue systems.
Our ap-proach addresses some of the difficulties thatarise in such automated evaluations, includ-ing the lack of consensus among human an-notators about the correct outputs within theprocessing pipeline, the availability of multi-ple acceptable system responses to some userutterances, and the complex relationship be-tween system responses and internal process-ing results.
Our approach includes the devel-opment of a corpus of richly annotated tar-get dialogues, simulations of the pipeline pro-cessing that could occur in these dialogues,and an analysis of how system responses varybased on internal processing results within thepipeline.
We illustrate our approach in two im-plemented virtual human dialogue systems.1 IntroductionNatural language dialogue systems are typically im-plemented as complex modular systems, with arange of internal modules performing tasks suchas automatic speech recognition (ASR), naturallanguage understanding (NLU), dialogue manage-ment (DM), natural language generation (NLG),and speech synthesis (TTS).
A common design isfor systems to adopt a pipeline architecture.
In apipeline, each user utterance is processed in a se-ries of successive processing steps, with the outputof each module serving as the input of the next mod-ule, until the system?s response is determined.
?Now at Saarland University, Germany.While there are many approaches to dialogue sys-tem evaluation (see e.g.
(Walker et al, 1997; Eck-ert et al, 1997; Walker, 2005)), in many ways, theprimary data for assessing the performance of a di-alogue system comes from the collection of live in-teractive dialogues between an implemented systemand members of its intended user population.
Yet,live dialogue-based evaluation suffers from a num-ber of limitations and drawbacks.
Each dialogue setcan be expensive and time-consuming to collect, andmay only reflect a specific version of a system underactive development.
Additional effort is also gener-ally necessary to identify specific system responsesas problematic or unacceptable.
Further annotationand analysis is then necessary to diagnose and pin-point the cause of the problematic responses, so thatthe relevant pipeline module(s) may be improved.In this paper, we present and discuss an approachto performing automated evaluations of pipeline ar-chitectures.
Our approach involves the developmentof a corpus of annotated target dialogues, startingfrom Wizard-of-Oz data.
Our automated evaluationassesses the support for these target dialogues in apipeline system architecture.
It is not designed as asubstitute for live system evaluations, but rather asa complement to them which may help to alleviatesome of these challenges to understanding systemperformance and streamlining development.
In par-ticular, unlike the PARADISE framework (Walkeret al, 1997), which aims to evaluate dialogue agentstrategies ?
by relating overall user satisfaction tovarious other metrics (task success, efficiency mea-sures, and qualitative measures) ?
our approachtakes the agent?s dialogue strategy for granted (in279utteranceusertypedresponsesystem(speech act)NLU userspeech act DMFigure 1: Simplified pipeline architecture.the form of a set of target dialogues that exemplifythe desired strategy), and instead zooms in and aimsto directly evaluate the dialogue system?s modulepipeline.
Specifically, our approach quantifies theability of the pipeline to replicate the processingsteps needed to reproduce a set of target responses.In our analysis, we place a special emphasis on thepossible lack of consensus among human annotatorsabout what the processing results should be.
We donot aim to further analyze the system?s live dialoguebehavior in terms of user satisfaction, task success,or other global measures.2 Research SettingThe work presented in this paper has been designedto support the dialogue behavior of two virtual hu-man systems, the SimCoach and Tactical Ques-tioning (TACQ) systems.
SimCoach (Rizzo et al,2011) is an on-going project aiming at empower-ing military personnel and their significant otherswith online healthcare assistance for Post-TraumaticStress Disorder (PTSD), depression, and family-related problems.
The SimCoach character encour-ages users to talk about any concerns or problemsthey may have.
TACQ (Gandhe et al, 2008) is de-signed to support simulation and training for tacticalquestioning skills, and provides virtual humans whohave information but will not answer certain ques-tions unless the user cooperates by agreeing to theirrequests, offering promises in their favor, and so on.In this work, we have developed target dialogues forthe Amani character, who has been an eyewitness ofa recent shooting incident.For simplicity, in the experiments reported in thispaper, we have used simplified versions of these twodialogue systems.
The simplification removes ASRfrom TACQ,1 and removes NLG and TTS from bothsystems.
This yields a simple two-module pipelinearchitecture that we depict in Figure 1.
Note thatthe input to NLU is a typed English utterance, and1SimCoach always uses an instant messaging style typed in-put interface.the output of the NLU module (also the input to theDM module) is a speech act representation.
The out-put of the DM, which we treat here as the system?sresponse to the user, is also a speech act represen-tation.
Both of these systems use statistical classi-fication models for NLU (Leuski and Traum, 2010;Sagae et al, 2009), and finite state machine modelsfor DM (Gandhe et al, 2008; Rizzo et al, 2011).3 Target DialoguesTarget dialogues are annotated versions of dialoguesa system designer would like the system to support.3.1 Developing Target DialoguesWizard-of-Oz (WoZ) and role play dialogues pro-vide valuable data to designers of dialogue systems,especially in the form of natural dialogue data andinsights into human-level performance and strate-gies for the specific dialogue task.
However, in prac-tice, system builders may not be able to implementall of the strategies and competences of the wizardsor role players, and simplifications may be needed.SimCoach target dialogues were developed froma collection of 10 WoZ dialogues in which clini-cians (wizards) and veterans (users) interacted witheach other.
We also built Amani target dialogues forTACQ starting from 19 WoZ dialogues.
Each userutterance and wizard?s response was annotated witha target NLU speech act and one or more target DMspeech acts (i.e., the system response).2 The 10 Sim-Coach target dialogues contain 376 user utterancesand 547 target system response speech acts.
The 19Amani target dialogues contain 317 user utterancesand 354 target system response speech acts.
For ex-cerpts of the SimCoach and Amani target dialogues,see Tables A.1 and A.2 in the Appendix.To create our target dialogues, we adjusted theWoZ dialogues to reflect a number of system de-sign limitations as well as wizard deviations fromthe desired dialogue policy.
These changes includedremoving unsupported wizard utterances and sub-dialogues, inserting or reordering system responsesdue to wizard mistakes, and introducing clarificationsubdialogues for unsupported user utterances.2For both SimCoach and TACQ, the DM may generate oneor multiple speech acts in response to a user utterance.2803.2 Formalizing Target DialoguesLet P = ?p1, ..., pk?
be the pipeline in a system con-taining k modules.
We use St to denote the pipelinestate, which includes the internal states of any mod-ules that maintain an internal state, at time t.For a user input xt that occurs at time t, whenthe pipeline state is St, we write A(P, St, xt) =?y1, ..., yk?
to represent the actual sequence of out-puts from the pipeline modules, where yi is the out-put of module pi for i = 1...k.For a variety of reasons, these actual module out-puts may differ from the target module outputs forthis input and pipeline state.
Let T (P, St, xt) =?z1, ..., zk?
be the target pipeline response to inputxt, i.e.
the sequence of target outputs from each ofthe pipeline modules.A target dialogue D = ?
(x1, T1), ..., (xN , TN )?,then, is a sequence of user inputs and correspondingtarget pipeline responses.
Specifically, for time t =1...N , Tt = T (P, S?t , xt) = ?z1, ..., zk?
is the targetpipeline response to input xt, where S?t is the targetpipeline state at each time t.An important detail is that the target pipeline stateS?t is the state that the pipeline would be in if allprevious user inputs had triggered exactly the tar-get pipeline responses.
Formally, let S?1 be the ini-tial state of the dialogue system pipeline.
Then, letS?t+1 = update(S?t , xt, Tt), where we use an updatefunction to capture the effect on the internal state ofthe pipeline of the target response Tt to xt.
Note thatthe target pipeline state may differ from the actualpipeline state, if an actual pipeline response differsfrom the target pipeline response.
For example, ifa previous user utterance was misunderstood by anNLU module, then at run-time, the actual informa-tion state inside the DM module would reflect thisearlier misunderstanding, while the target pipelinestate would include a corrected version of the in-formation state.
Using corrected information states,and corrected pipeline states more generally, enablesthe utterances within a target dialogue to be consid-ered independently in a pipeline evaluation.3We can say that a pipeline P is compatible with3It also highlights how our pipeline evaluation results do nottranslate directly into performance metrics for live dialogues,as deviations and errors in system responses in live dialoguesmay affect the subsequent interaction in ways that are difficultto predict and deviate substantially from the target dialogues.User Utterance NLU Speech Act DM ResponseHaving difficultysleeping... baddreams.. Wake upa few times everynightanswer.observable.sleeping-problemsquestion.depression-pre-check-list.1answer.observable.wakeup-genericquestion.depression-pre-check-list.1answer.observable.wakeup-nightmarequestion.ptsd-pre-checklist.1Table 1: Sample of Different NLU Speech Actsa target dialogue D = ?
(x1, T1), ..., (xN , TN )?
iffA(P, S?t , xt)[k] = Tt[k] for all t = 1...N .
In otherwords, for every user utterance, the actual systemresponse, as emitted by the last (kth) module in thepipeline, matches the target system response.4 Boththe SimCoach and TACQ pipelines are compatiblein this sense with their target dialogues (Section 3.1).3.2.1 Addressing the Lack of ConsensusA considerable challenge in the improvement ofpipeline performance is the lack of consensus aboutthe desired internal processing steps: different sys-tem designers or human annotators often disagreeabout what the intermediate results should be.
Forexample, in a system such as TACQ or SimCoach,there may be substantial disagreement among hu-man annotators about the correct NLU output foreach utterance; see e.g.
(Artstein et al, 2009).
Table1 exemplifies 3 different possible NLU speech actannotations for a user utterance to SimCoach.
Notethat for the first two, the DM outputs the same sys-tem response (which incidentally is the target re-sponse).
However, the third speech act yields adifferent response.
In our automated evaluations,rather than trying to resolve all disagreements, ourapproach is to characterize the frequency with whichthese kinds of phenomena occur in the pipeline.To support this analysis, for a target dialogueD = ?
(x1, T1), ..., (xN , TN )?, we assume then thateach input xt is associated not only with the targetpipeline response Tt, but also with a collection of an-notations At = ?a1, ..., ak?.
These annotations maybe derived from a number of independent sources4A technical detail: for both SimCoach and TACQ, the DMsometimes emits multiple speech acts; to accommodate thesecases, for now we treat the target DM output as a set of speechacts A, and count each actual output DM speech act as an in-dependent match if it matches any speech act in A (ignoringorder).
A more complex matching scheme could be employed.281S = {s1, ..., sl}, and we write ai(s) = wi to denotethe correct output wi for module pi according to an-notation source s ?
S .
These independent ?anno-tation sources?
might be human annotators, or com-peting module algorithms, for example.We can then capture the hypothetical effect of us-ing annotation source s in place of some module piwithin the pipeline.
To do so, we consider the effectof replacing the output of module pi with ai(s), andusing this as the input to subsequent modules in thepipeline.
Let P ki+1 = ?pi+1, ..., pk?
be the remainderof the pipeline, starting at module pi+1.
For inputxt, we can notate the hypothetical pipeline response,if module i were replaced by annotation source s,by H(P ki+1, S?t , ai(s)) = ?yi+1, ..., yk?.
We will writehs\it for the hypothetical system response to the userinput at time t, if source s were substituted for theoutput of module i: hs\it = H(P ki+1, S?t , ai(s))[k] =yk.
For a target dialogue of length N , we can sum-marize the frequency with which the hypotheticalpipeline response would match the target system re-sponse by a performance measure:Pstrict =1NN?t=1match(hs\it , Tt[k])where match(x, y) = 1 if x = y and 0 otherwise.5A second form of lack of consensus issue is theexistence of multiple acceptable system responseswithin a system.
Returning to the example in Ta-ble 1, system designers might decide that either ofthe two system responses here would be accept-able.
In some cases, actual NLU outputs which dif-fer from the target NLU output will simply result inthe system giving alternative acceptable system re-sponses, as in this example.
In other cases, they maylead to unacceptable system responses.We measure the frequency with which these phe-nomena occur as follows.
For a target dialogueD = ?
(x1, T1), ..., (xN , TN )?, let each input xt beassociated with a set Rt = {r1, ..., rm} of systemresponses which differ from the target system re-sponse Tt[k], but are also acceptable in design terms.Given these alternative responses, we can then de-fine a more permissive performance measure:Pmultiple =1NN?t=1match(hs\it , Tt[k], Rt)5This strict agreement measure can be easily generalized tomeasure the proportion of matches in a set of target dialogues.NLUspeech actsourcePercent of NLUspeech actsidentical to...(N=317)Percent of systemresponse speechacts identical to...(N=354)thetargetNLUspeechact(target)the targetor otheracceptableNLUspeech act(humanall)a targetsystemresponsespeechacta target oracceptablesystemresponsespeech acttarget 100% 100% 99.4% 100%human1 79.3% 95.4% 84.2% 88.4%human2 76.7% 99.7% 86.7% 93.8%human3 59.3% 90.2% 69.6% 78.8%NPCEditor 42.3% 50.5% 55.3% 57.4%Table 2: TACQ Amani Evaluation Resultswherematch(hs\it , Tt[k], Rt) =????
?1 if hs\it = Tt[k]1 if hs\it ?
Rt0 otherwise.4 Results4.1 Annotations and Results for TACQWe collected a range of annotations for the 19 TACQAmani target dialogues, including 6 sources of NLUspeech acts for the 317 user utterances: target (thetarget NLU speech act for each utterance); 3 inde-pendent human annotations of the best NLU speechact for each utterance; humanall (a set containingall of the alternative acceptable NLU speech actsfor each utterance, according to the same single re-searcher who prepared target); and NPCEditor, theNLU speech act output from NPCEditor (Leuski andTraum, 2010), the NLU module for TACQ.We analyzed the effect of differing NLU speechact sources on the responses given by the system.We present the results in Table 2.
(For a de-tailed processing example, see Table A.2 in the Ap-pendix.)
The first (leftmost) column of numbersshows the percentage of NLU speech acts from eachsource that are identical to the target NLU speechact.
These results highlight how human annotatorsdo not always agree with each other, or with thetarget.
The agreement among the human annota-tors themselves, measured by Krippendorf?s alpha(Krippendorff, 2007) is 0.599 (see also (Artstein etal., 2009)).
In the second column of numbers, wetabulate the frequency with which the NLU speechacts are present in humanall.
While these numbers282are higher, they do not reach 100% for the humanannotators, suggesting that a single annotator is un-likely to be able to circumscribe all the NLU speechacts that other annotators might find acceptable.Despite the frequent disagreements among humanannotators, this evaluation shows that the impact onthe target system responses is less than might be ex-pected.
In the third column of numbers, we calculatePstrict which measures the effect of using each ofNLU sources, in place of the NLU module?s actualoutput, on the pipeline?s ability to produce the tar-get response.
As the table implies, the pipeline oftenproduces the target system response (third column)even when the NLU source disagrees with the target(first column).
Indeed, for all the NLU sources ex-cept for target, the pipeline is significantly morelikely to produce the target system response than theNLU source is to produce the target NLU speech act(Wilcoxon test, p < 0.001 for each source).We also calculate Pmultiple (last column) whichmeasures the effect of using each NLU source onthe pipeline?s ability to produce either the target orany other acceptable system response.
As the ta-ble shows, the actual system responses are often ac-ceptable when they differ from the target responses.Although this effect seems weaker for NPCEditor,Wilcoxon tests reveal that for every source otherthan target, the differences between Pstrict andPmultiple are significant at p < 0.005.
This evalu-ation confirms that the pipeline is significantly morelikely to deliver an acceptable system response thana target response, and helps quantify to what ex-tent NLU outputs that differ from the target remainproblematic for the pipeline performance.4.2 Annotations and Results for SimCoachWe gathered a set of annotations for the 10 Sim-Coach target dialogues, including 3 sources of NLUspeech acts for the 376 user utterances: target,human1, and mxNLU (the NLU speech act outputfrom mxNLU (Sagae et al, 2009), the NLU mod-ule for SimCoach).
We present the evaluation re-sults in Table 3.
As the table shows, our indepen-dent human annotator often disagreed with the targetNLU speech act.
Despite the 72.1% agreement rate,the system?s response to the human NLU speech actagreed with the target response 93.3% of the time.In comparison, mxNLU shows somewhat higherNLU speechact sourceNLU speech actsidentical to target(N = 376)System responsespeech acts identicalto target (N = 547)target 100% 100%human1 72.1% 93.3%mxNLU 75.3% 91.1%Table 3: SimCoach Evaluation Resultsagreement (75.3%) with the target NLU annotation.While this might at first suggest ?super-human?NLU performance, in reality it is because the targetNLU annotation was constructed in very close con-sultation with the training data for mxNLU.6 Despiteshowing higher agreement with target NLU speechacts, the system responses were not more likely tomatch the target system responses with mxNLU.The explanation is that disagreements for mxNLUwere more serious, reflecting more misunderstand-ings and failures to understand than occur with a hu-man annotator, and more deviations from the targetresponses.
This highlights the value of looking be-yond the performance of individual modules.5 Conclusions and Future WorkWe have presented an approach to performing au-tomated evaluations of pipeline architectures, anddemonstrated its application in two implementedvirtual human dialogue systems.
The pipeline eval-uation provided several insights into the currentpipeline performance, including what performancewould be attainable if human-level NLU were possi-ble.
In future work, we would like to expand beyondour simplified two-module pipeline, and investigatethe connection between our automated pipeline eval-uations and performance in live dialogues.AcknowledgmentsWe thank our reviewers, Sudeep Gandhe, Fab-rizio Morbini, and David Traum.
The projecteffort described here has been sponsored by theU.S.
Army Research, Development, and Engineer-ing Command (RDECOM).
Statements and opin-ions expressed do not necessarily reflect the positionor the policy of the United States Government, andno official endorsement should be inferred.6The exact target dialogue utterances were not in themxNLU training data, but similar utterances were inspected inconstructing the target dialogues.283ReferencesR.
Artstein, S. Gandhe, M. Rushforth, and D. Traum.2009.
Viability of a simple dialogue act scheme fora tactical questioning dialogue system.
In SemDialWorkshop, pages 43?50.W.
Eckert, E. Levin, and R. Pieraccini.
1997.
User mod-eling for spoken dialogue system evaluation.
In Proc.IEEE ASR Workshop, pages 80?87.S.
Gandhe, D. DeVault, A. Roque, B. Martinovski,R.
Artstein, A. Leuski, and et al 2008.
From do-main specification to virtual humans: An integratedapproach to authoring tactical questioning characters.Proceedings of Interspeech-08.K Krippendorff.
2007.
Computing krippendorff?s alpha-reliability, June.A.
Leuski and D. Traum.
2010.
NPCEditor: A tool forbuilding question-answering characters.
In LREC.A.
Rizzo, B. Lange, J.G.
Buckwalter, E. Forbell, J. Kim,K.
Sagae, J. Williams, B.O.
Rothbaum, J. Difede,G.
Reger, T. Parsons, and P. Kenny.
2011.
An intel-ligent virtual human system for providing healthcareinformation and support.
In et al Westwood, J.D., ed-itor, Technology and Informatics.
IOS Press.K.
Sagae, G. Christian, D. DeVault, and D. R. Traum.2009.
Towards natural language understanding of par-tial speech recognition results in dialogue systems.
InShort Paper Proceedings of NAACL HLT.M.
A. Walker, D. J. Litman, C. A. Kamm, A.
A. Kamm,and A. Abella.
1997.
Paradise: A framework for eval-uating spoken dialogue agents.
pages 271?280.M.
A. Walker.
2005.
Can we talk?
methods for evalu-ation and training of spoken dialogue systems.
Lan-guage Resources and Evaluation, 39(1):pp.
65?75.284Appendixt User Utterance (xt) Target NLU SpeechAct (t1)Target SystemResponse (t2)Textual Version of Target System Response9 my husband seems distant,and we have been arguing alot more latelyanswer.observable.family-problemquestion.bio-info.has-kidsDoes he have children?10 yes, 2 answer.yes question.family-pre-checklist.6In his family, do people collaborate together tofind the best way to solve problems?Table A.1: Excerpt from a SimCoach Target Dialogue.t User Utterance (xt) Source of NLUSpeech ActNLU Speech Act (gloss) System Response Speech Acts (gloss)1 hi amani.
NPCEditor hello hellotarget NLU hello hello2 i wanted to talk to you aboutthe recent shooting thatoccurredNPC Editor Tell me more about the_ incident location of the_incident is the_shoptarget NLU Is amani willing to talk?
amani is willing to talk3 do you know who wasresponsible?NPC Editor What is perpetrator of the_ incident ?
perpetrator of the_incident is Saiftarget NLU What is name of strange_man ?
player should offer ?give-safety?Table A.2: Excerpt from a TACQ target dialogue, including pipeline module processing.285
