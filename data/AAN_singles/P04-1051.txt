Computing Locally Coherent DiscoursesErnst AlthausLORIAUniversite?
Henri Poincare?Vand?uvre-le`s-Nancy, Francealthaus@loria.frNikiforos KaramanisSchool of InformaticsUniversity of EdinburghEdinburgh, UKN.Karamanis@sms.ed.ac.ukAlexander KollerDept.
of Computational LinguisticsSaarland UniversitySaarbru?cken, Germanykoller@coli.uni-sb.deAbstractWe present the first algorithm that computes opti-mal orderings of sentences into a locally coherentdiscourse.
The algorithm runs very efficiently on avariety of coherence measures from the literature.We also show that the discourse ordering problemis NP-complete and cannot be approximated.1 IntroductionOne central problem in discourse generation andsummarisation is to structure the discourse in away that maximises coherence.
Coherence is theproperty of a good human-authored text that makesit easier to read and understand than a randomly-ordered collection of sentences.Several papers in the recent literature (Mellish etal., 1998; Barzilay et al, 2002; Karamanis and Ma-nurung, 2002; Lapata, 2003; Karamanis et al, 2004)have focused on defining local coherence, whichevaluates the quality of sentence-to-sentence transi-tions.
This is in contrast to theories of global coher-ence, which can consider relations between largerchunks of the discourse and e.g.
structures them intoa tree (Mann and Thompson, 1988; Marcu, 1997;Webber et al, 1999).
Measures of local coherencespecify which ordering of the sentences makes forthe most coherent discourse, and can be based e.g.on Centering Theory (Walker et al, 1998; Brennanet al, 1987; Kibble and Power, 2000; Karamanisand Manurung, 2002) or on statistical models (Lap-ata, 2003).But while formal models of local coherence havemade substantial progress over the past few years,the question of how to efficiently compute an order-ing of the sentences in a discourse that maximiseslocal coherence is still largely unsolved.
The fun-damental problem is that any of the factorial num-ber of permutations of the sentences could be theoptimal discourse, which makes for a formidablesearch space for nontrivial discourses.
Mellish etal.
(1998) and Karamanis and Manurung (2002)present algorithms based on genetic programming,and Lapata (2003) uses a graph-based heuristic al-gorithm, but none of them can give any guaranteesabout the quality of the computed ordering.This paper presents the first algorithm that com-putes optimal locally coherent discourses, and es-tablishes the complexity of the discourse orderingproblem.
We first prove that the discourse order-ing problem for local coherence measures is equiva-lent to the Travelling Salesman Problem (TSP).
Thismeans that discourse ordering is NP-complete, i.e.there are probably no polynomial algorithms for it.Worse, our result implies that the problem is noteven approximable; any polynomial algorithm willcompute arbitrarily bad solutions on unfortunate in-puts.
Note that all approximation algorithms for theTSP assume that the underlying cost function is ametric, which is not the case for the coherence mea-sures we consider.Despite this negative result, we show that by ap-plying modern algorithms for TSP, the discourse or-dering problem can be solved efficiently enough forpractical applications.
We define a branch-and-cutalgorithm based on linear programming, and evalu-ate it on discourse ordering problems based on theGNOME corpus (Karamanis, 2003) and the BLLIPcorpus (Lapata, 2003).
If the local coherence mea-sure depends only on the adjacent pairs of sentencesin the discourse, we can order discourses of up to 50sentences in under a second.
If it is allowed to de-pend on the left-hand context of the sentence pair,computation is often still efficient, but can becomeexpensive.The structure of the paper is as follows.
We willfirst formally define the discourse ordering problemand relate our definition to the literature on local co-herence measures in Section 2.
Then we will provethe equivalence of discourse ordering and TSP (Sec-tion 3), and present algorithms for solving it in Sec-tion 4.
Section 5 evaluates our algorithms on exam-ples from the literature.
We compare our approachto various others in Section 6, and then conclude inSection 7.2 The Discourse Ordering ProblemWe will first give a formal definition of the prob-lem of computing locally coherent discourses, anddemonstrate how some local coherence measuresfrom the literature fit into this framework.2.1 DefinitionsWe assume that a discourse is made up of discourseunits (depending on the underlying theory, thesecould be utterances, sentences, clauses, etc.
), whichmust be ordered to achieve maximum local coher-ence.
We call the problem of computing the optimalordering the discourse ordering problem.We formalise the problem by assigning a cost toeach unit-to-unit transition, and a cost for the dis-course to start with a certain unit.
Transition costsmay depend on the local context, i.e.
a fixed num-ber of discourse units to the left may influence thecost of a transition.
The optimal ordering is the onewhich minimises the sum of the costs.Definition 1.
A d-place transition cost function fora set U of discourse units is a function cT : Ud ?R.
Intuitively, cT (un|u1, .
.
.
, ud?1) is the cost ofthe transition (ud?1, ud) given that the immediatelypreceding units were u1, .
.
.
, ud?2.A d-place initial cost function for U is a functioncI : Ud ?
R. Intuitively, cI(u1, .
.
.
, ud) is thecost for the fact that the discourse starts with thesequence u1, .
.
.
, ud.The d-place discourse ordering problem is de-fined as follows: Given a set U = {u1, .
.
.
, un},a d-place transition cost function cT and a (d ?
1)-place initial cost function cI , compute a permutationpi of {1, .
.
.
, n} such thatcI(upi(1), .
.
.
, upi(d?1))+n?d+1?i=1cT (upi(i+d?1)|upi(i), .
.
.
, upi(i+d?2))is minimal.The notation for the cost functions is suggestive:The transition cost function has the character of aconditional probability, which specifies that the costof continuing the discourse with the unit ud dependson the local context u1, .
.
.
, ud?1.
This local con-text is not available for the first d ?
1 units of thediscourse, which is why their costs are summarilycovered by the initial function.2.2 Centering-Based Cost FunctionsOne popular class of coherence measures is basedon Centering Theory (CT, (Walker et al, 1998)).
Wewill briefly sketch its basic notions and then showhow some CT-based coherence measures can be castinto our framework.The standard formulation of CT e.g.
in (Walker etal., 1998), calls the discourse units utterances, andassigns to each utterance ui in the discourse a listCf(ui) of forward-looking centres.
The membersof Cf(ui) correspond to the referents of the NPsin ui and are ranked in order of prominence, thefirst element being the preferred centre Cp(ui).
Thebackward-looking centre Cb(ui) of ui is defined asthe highest ranked element of Cf(ui) which also ap-pears in Cf(ui?1), and serves as the link betweenthe two subsequent utterances ui?1 and ui.
Eachutterance has at most one Cb.
If ui and ui?1 haveno forward-looking centres in common, or if ui isthe first utterance in the discourse, then ui does nothave a Cb at all.Based on these concepts, CT classifies the tran-sitions between subsequent utterances into differ-ent types.
Table 1 shows the most common clas-sification into the four types CONTINUE, RETAIN,SMOOTH-SHIFT, and ROUGH-SHIFT, which are pre-dicted to be less and less coherent in this order(Brennan et al, 1987).
Kibble and Power (2000)define three further classes of transitions: COHER-ENCE and SALIENCE, which are both defined in Ta-ble 1 as well, and NOCB, the class of transitionsfor which Cb(ui) is undefined.
Finally, a transitionis considered to satisfy the CHEAPNESS constraint(Strube and Hahn, 1999) if Cb(ui) = Cp(ui?1).Table 2 summarises some cost functions from theliterature, in the reconstruction of Karamanis et al(2004).
Each line shows the name of the coherencemeasure, the arity d from Definition 1, and the ini-tial and transition cost functions.
To fit the defini-tions in one line, we use terms of the form fk, whichabbreviate applications of f to the last k argumentsof the cost functions, i.e.
f(ud?k+1, .
.
.
, ud).The most basic coherence measure, M.NOCB(Karamanis and Manurung, 2002), simply assignsto each NOCB transition the cost 1 and to every othertransition the cost 0.
The definition of cT (u2|u1),which decodes to nocb(u1, u2), only looks at thetwo units in the transition, and no further context.The initial costs for this coherence measure are al-ways zero.The measure M.KP (Kibble and Power, 2000)sums the value of nocb and the values of three func-tions which evaluate to 0 if the transition is cheap,salient, or coherent, and 1 otherwise.
This is an in-stance of the 3-place discourse ordering problem be-cause COHERENCE depends on Cb(ui?1), which it-self depends on Cf(ui?2); hence nocoh must takeCOHERENCE: COHERENCE?
:Cb(ui) = Cb(ui?1) Cb(ui) 6= Cb(ui?1)SALIENCE: Cb(ui) = Cp(ui) CONTINUE SMOOTH-SHIFTSALIENCE?
: Cb(ui) 6= Cp(ui) RETAIN ROUGH-SHIFTTable 1: COHERENCE, SALIENCE and the table of standard transitionsd initial cost cI(u1, .
.
.
, ud?1) transition cost cT (ud|u1, .
.
.
, ud?1)M.NOCB 2 0 nocb2M.KP 3 nocb2 + nocheap2 + nosal2 nocb2 + nocheap2 + nosal2 + nocoh3M.BFP 3 (1?
nosal2, nosal2, 0, 0) (cont3, ret3, ss3, rs3)M.LAPATA 2 ?
logP (u1) ?
logP (u2|u1)Table 2: Some cost functions from the literature.three arguments.Finally, the measure M.BFP (Brennan et al,1987) uses a lexicographic ordering on 4-tupleswhich indicate whether the transition is a CON-TINUE, RETAIN, SMOOTH-SHIFT, or ROUGH-SHIFT.
cT and all four functions it is computed fromtake three arguments because the classification de-pends on COHERENCE.
As the first transition in thediscourse is coherent by default (it has no Cb), wecan compute cI by distinguishing RETAIN and CON-TINUE via SALIENCE.
The tuple-valued cost func-tions can be converted to real-valued functions bychoosing a sufficiently large number M and usingthe value M3 ?
cont + M2 ?
ret + M ?
ss + rs.2.3 Probability-Based Cost FunctionsA fundamentally different approach to measure dis-course coherence was proposed by Lapata (2003).It uses a statistical bigram model that assigns eachpair ui, uk of utterances a probability P (uk|ui) ofappearing in subsequent positions, and each utter-ance a probability P (ui) of appearing in the initialposition of the discourse.
The probabilities are es-timated on the grounds of syntactic features of thediscourse units.
The probability of the entire dis-course u1 .
.
.
un is the product P (u1) ?
P (u2|u1) ?.
.
.
?
P (un|un?1).We can transform Lapata?s model straightfor-wardly into our cost function framework, as shownunder M.LAPATA in Table 2.
The discourse thatminimizes the sum of the negative logarithms willalso maximise the product of the probabilities.
Wehave d = 2 because it is a bigram model in whichthe transition probability does not depend on theprevious discourse units.3 Equivalence of Discourse Ordering andTSPNow we show that discourse ordering and the travel-ling salesman problem are equivalent.
In order to dothis, we first redefine discourse ordering as a graphproblem.d-place discourse ordering problem (dPDOP):Given a directed graph G = (V,E), a nodes ?
V and a function c : V d ?
R, compute asimple directed path P = (s = v0, v1, .
.
.
, vn)from s through all vertices in V which min-imises?n?d+1i=0 c(vi, vi+1, .
.
.
, vi+d?1).
Wewrite instances of dPDOP as (V,E, s, c).The nodes v1, .
.
.
, vn correspond to the discourseunits.
The cost function c encodes both the initialand the transition cost functions from Section 2 byreturning the initial cost if its first argument is the(new) start node s.Now let?s define the version of the travellingsalesman problem we will use below.Generalised asymmetric TSP (GATSP): Given adirected graph G = (V,E), edge weights c :E ?
R, and a partition (V1, .
.
.
, Vk) of thenodes V , compute the shortest directed cyclethat visits exactly one node of each Vi.
Wecall such a cycle a tour and write instances ofGATSP as ((V1, .
.
.
, Vk), E, c).The usual definition of the TSP, in which everynode must be visited exactly once, is the specialcase of GATSP where each Vi contains exactly onenode.
We call this case asymmetric travelling sales-man problem, ATSP.ATSP 2PDOP   Figure 1: Reduction of ATSP to 2PDOPWe will show that ATSP can be reduced to2PDOP, and that any dPDOP can be reduced toGATSP.3.1 Reduction of ATSP to 2PDOPFirst, we introduce the reduction of ATSP to2PDOP, which establishes NP-completeness ofdPDOP for all d > 1.
The reduction is approxi-mation preserving, i.e.
if we can find a solution of2PDOP that is worse than the optimum only by afactor of  (an -approximation), it translates to asolution of ATSP that is also an -approximation.Since it is known that there can be no polynomial al-gorithms that compute -approximations for generalATSP, for any  (Cormen et al, 1990), this meansthat dPDOP cannot be approximated either (unlessP=NP): Any polynomial algorithm for dPDOP willcompute arbitrarily bad solutions on certain inputs.The reduction works as follows.
Let G =((V1, .
.
.
, Vk), E, c) be an instance of ATSP, andV = V1 ?
.
.
.
?
Vk.
We choose an arbitrary nodev ?
V and split it into two nodes vs and vt. We as-sign all edges with source node v to vs and all edgeswith target node v to vt (compare Figure 1).
Finallywe make vs the source node of our 2PDOP instanceG?.For every tour in G, we have a path in G?
startingat vs visiting all other nodes (and ending in vt) withthe same cost by replacing the edge (v, u) out ofv by (vs, u) and the edge (w, v) into v by (w, vt).Conversely, for every path starting at vs visiting allnodes, we have an ATSP tour of the same cost, sinceall such paths will end in vt (as vt has no outgoingedges).An example is shown in Fig.
1.
The ATSP in-stance on the left has the tour (1, 3, 2, 1), indicatedby the solid edges.
The node 1 is split into the twonodes 1s and 1t, and the tour translates to the path(1s, 3, 2, 1t) in the 2PDOP instance.3.2 Reduction of dPDOP to GATSPConversely, we can encode an instance G =(V,E, s, c) of dPDOP as an instance G?
=3PDOP GATSP   	   	 	     Figure 2: Reduction of dPDOP to GATSP.
Edges tothe source node [s, s] are not drawn.
((V ?u)u?V , E?, c?)
of GATSP, in such a way that theoptimal solutions correspond.
The cost of traversingan edge in dPDOP depends on the previous d ?
1nodes; we compress these costs into ordinary costsof single edges in the reduction to GATSP.The GATSP instance has a node [u1, .
.
.
, ud?1]for every d ?
1-tuple of nodes of V .
It has an edgefrom [u1, .
.
.
, ud?1] to [u2, .
.
.
, ud?1, ud] iff thereis an edge from ud?1 to ud in G, and it has an edgefrom each node into [s, .
.
.
, s].
The idea is to en-code a path P = (s = u0, u1, .
.
.
, un) in G asa tour TP in G?
that successively visits the nodes[ui?d+1, .
.
.
ui], i = 0, .
.
.
n, where we assume thatuj = s for all j ?
0 (compare Figure 2).The cost of TP can be made equal to the cost of Pby making the cost of the edge from [u1, .
.
.
, ud?1]to [u2, .
.
.
, ud] equal to c(u1, .
.
.
ud).
(We set c?
(e)to 0 for all edges e between nodes with first compo-nent s and for the edges e with target node [sd?1].
)Finally, we define V ?u to be the set of all nodes in G?with last component u.
It is not hard to see that forany simple path of length n in G, we find a tour TPin G?
with the same cost.
Conversely, we can findfor every tour in G?
a simple path of length n in Gwith the same cost.Note that the encoding G?
will contain many un-necessary nodes and edges.
For instance, all nodesthat have no incoming edges can never be used in atour, and can be deleted.
We can safely delete suchunnecessary nodes in a post-processing step.An example is shown in Fig.
2.
The 3PDOPinstance on the left has a path (s, 3, 1, 2), whichtranslates to the path ([s, s], [s, 3], [3, 1], [1, 2]) inthe GATSP instance shown on the right.
This pathcan be completed by a tour by adding the edge([1, 2], [s, s]), of cost 0.
The tour indeed visits eachV ?u (i.e., each column) exactly once.
Nodes with lastcomponent s which are not [s, s] are unreachableand are not shown.For the special case of d = 2, the GATSP is sim-ply an ordinary ATSP.
The graphs of both problemslook identical in this case, except that the GATSPinstance has edges of cost 0 from any node to thesource [s].4 Computing Optimal OrderingsThe equivalence of dPDOP and GATSP implies thatwe can now bring algorithms from the vast litera-ture on TSP to bear on the discourse ordering prob-lem.
One straightforward method is to reduce theGATSP further to ATSP (Noon and Bean, 1993);for the case d = 2, nothing has to be done.
Thenone can solve the reduced ATSP instance; see (Fis-chetti et al, 2001; Fischetti et al, 2002) for a recentsurvey of exact methods.We choose the alternative of developing a newalgorithm for solving GATSP directly, which usesstandard techniques from combinatorial optimisa-tion, gives us a better handle on optimising the al-gorithm for our problem instances, and runs moreefficiently in practice.
Our algorithm translatesthe GATSP instance into an integer linear pro-gram (ILP) and uses the branch-and-cut method(Nemhauser and Wolsey, 1988) to solve it.
Integerlinear programs consist of a set of linear equationsand inequalities, and are solved by integer variableassignments which maximise or minimise a goalfunction while satisfying the other conditions.Let G = (V,E) be a directed graph and S ?
V .We define ?+(S) = {(u, v) ?
E | u ?
S and v 6?S} and ??
(S) = {(u, v) ?
E | u /?
S and v ?
S},i.e.
?+(S) and ??
(S) are the sets of all incomingand outgoing edges of S, respectively.
We assumethat the graph G has no edges within one partitionVu, since such edges cannot be used by any solution.With this assumption, GATSP can be phrased as anILP as follows (this formulation is similar to the oneproposed by Laporte et al (1987)):min?e?Ecexes.t.?e?
?+(v)xe =?e???
(v)xe ?
v ?
V (1)?e???
(Vi)xe = 1 1 ?
i ?
n (2)?e?
?+(?i?IVi)xe ?
1 I ?
{1, .
.
.
, n} (3)xe ?
{0, 1}We have a binary variable xe for each edge e ofthe graph.
The intention is that xe has value 1 ife is used in the tour, and 0 otherwise.
Thus thecost of the tour can be written as?e?E cexe.
Thethree conditions enforce the variable assignment toencode a valid GATSP tour.
(1) ensures that all inte-ger solutions encode a set of cycles.
(2) guaranteesthat every partition Vi is visited by exactly one cy-cle.
The inequalities (3) say that every subset of thepartitions has an outgoing edge; this makes sure asolution encodes one cycle, rather than a set of mul-tiple cycles.To solve such an ILP using the branch-and-cutmethod, we drop the integrality constraints (i.e.
wereplace xe ?
{0, 1} by 0 ?
xe ?
1) and solvethe corresponding linear programming (LP) relax-ation.
If the solution of the LP is integral, we foundthe optimal solution.
Otherwise we pick a variablewith a fractional value and split the problem intotwo subproblems by setting the variable to 0 and 1,respectively.
We solve the subproblems recursivelyand disregard a subproblem if its LP bound is worsethan the best known solution.Since our ILP contains an exponential number ofinequalities of type (3), solving the complete LPsdirectly would be too expensive.
Instead, we startwith a small subset of these inequalities, and test(efficiently) whether a solution of the smaller LPviolates an inequality which is not in the currentLP.
If so, we add the inequality to the LP, resolveit, and iterate.
Otherwise we found the solution ofthe LP with the exponential number of inequalities.The inequalities we add by need are called cuttingplanes; algorithms that find violated cutting planesare called separation algorithms.To keep the size of the branch-and-cut tree small,our algorithm employs some heuristics to find fur-ther upper bounds.
In addition, we improve lowerbound from the LP relaxations by adding further in-equalities to the LP that are valid for all integral so-lutions, but can be violated for optimal solutions ofthe LP.
One major challenge here was to find separa-tion algorithms for these inequalities.
We cannot gointo these details for lack of space, but will discussthem in a separate paper.5 EvaluationWe implemented the algorithm and ran it on someexamples to evaluate its practical efficiency.
Theruntimes are shown in Tables 3 and 4 for an imple-mentation using a branch-and-cut ILP solver whichis free for all academic purposes (ILP-FS) and acommercial branch-and-cut ILP solver (ILP-CS).Our implementations are based on LEDA 4.4.1Instance Size ILP-FS ILP-CSlapata-10 13 0.05 0.05coffers1 M.NOCB 10 0.04 0.02cabinet1 M.NOCB 15 0.07 0.01random (avg) 20 0.09 0.07random (avg) 40 0.28 0.17random (avg) 60 1.39 0.40random (avg) 100 6.17 1.97Table 3: Some runtimes for d = 2 (in seconds).
(www.algorithmic-solutions.com) forthe data structures and the graph algorithms andon SCIL 0.8 (www.mpi-sb.mpg.de/SCIL)for implementing the ILP-based branch-and-cutalgorithm.
SCIL can be used with differentbranch-and-cut core codes.
We used CPLEX9.0 (www.ilog.com) as commercial core andSCIP 0.68 (www.zib.de/Optimization/Software/SCIP/) based on SOPLEX 1.2.2a(www.zib.de/Optimization/Software/Soplex/) as the free implementation.
Note thatall our implementations are still preliminary.
Thesoftware is publicly available (www.mpi-sb.mpg.de/?althaus/PDOP.html).We evaluate the implementations on three classesof inputs.
First, we use two discourses from theGNOME corpus, taken from (Karamanis, 2003), to-gether with the centering-based cost functions fromSection 2: coffers1, containing 10 discourse units,and cabinet1, containing 15 discourse units.
Sec-ond, we use twelve discourses from the BLLIPcorpus taken from (Lapata, 2003), together withM.LAPATA.
These discourses are 4 to 13 discourseunits long; the table only shows the instance withthe highest running time.
Finally, we generate ran-dom instances of 2PDOP of size 20?100, and of3PDOP of size 10, 15, and 20.
A random instance isthe complete graph, where c(u1, .
.
.
, ud) is chosenuniformly at random from {0, .
.
.
, 999}.The results for the 2-place instances are shownin Table 3, and the results for the 3-place instancesare shown in Table 4.
The numbers are runtimes inseconds on a Pentium 4 (Xeon) processor with 3.06GHz.
Note that a hypothetical baseline implementa-tion which naively generates and evaluates all per-mutations would run over 77 years for a discourseof length 20, even on a highly optimistic platformthat evaluates one billion permutations per second.For d = 2, all real-life instances and all randominstances of size up to 50 can be solved in less thanone second, with either implementation.
The prob-lem becomes more challenging for d = 3.
Here thealgorithm quickly establishes good LP bounds forInstance Size ILP-FS ILP-CScoffers1 M.KP 10 0.05 0.05coffers1 M.BFP 10 0.08 0.06cabinet1 M.KP 15 0.40 1.12cabinet1 M.BFP 15 0.39 0.28random (avg) 10 1.00 0.42random (avg) 15 35.1 5.79random (avg) 20 - 115.8Table 4: Some runtimes for d = 3 (in seconds).the real-life instances, and thus the branch-and-cuttrees remain small.
The LP bounds for the randominstances are worse, in particular when the numberof units gets larger.
In this case, the further opti-misations in the commercial software make a bigdifference in the size of the branch-and-cut tree andthus in the solution time.An example output for cabinet1 with M.NOCBis shown in Fig.
3; we have modified referring ex-pressions to make the text more readable, and havemarked discourse unit boundaries with ?/?
and ex-pressions that establish local coherence with squarebrackets.
This is one of many possible optimal so-lutions, which have cost 2 because of the two NOCBtransitions at the very start of the discourse.
Detailson the comparison of different centering-based co-herence measures are discussed by Karamanis et al(2004).6 Comparison to Other ApproachesThere are two approaches in the literature that aresimilar enough to ours that a closer comparison isin order.The first is a family of algorithms for discourseordering based on genetic programming (Mellish etal., 1998; Karamanis and Manurung, 2002).
This isa very flexible and powerful approach, which can beapplied to measures of local coherence that do notseem to fit in our framework trivially.
For exam-ple, the measure from (Mellish et al, 1998) looks atthe entire discourse up to the current transition forsome of their cost factors.
However, our algorithmis several orders of magnitude faster where a directcomparison is possible (Manurung, p.c.
), and it isguaranteed to find an optimal ordering.
The non-approximability result for TSP means that a genetic(or any other) algorithm which is restricted to poly-nomial runtime could theoretically deliver arbitrar-ily bad solutions.Second, the discourse ordering problem we havediscussed in this paper looks very similar to the Ma-jority Ordering problem that arises in the contextof multi-document summarisation (Barzilay et al,Both cabinets probably entered England in the early nineteenth century / after the French Revolution causedthe dispersal of so many French collections.
/ The pair to [this monumental cabinet] still exists in Scotland./ The fleurs-de-lis on the top two drawers indicate that [the cabinet] was made for the French King LouisXIV.
/ [It] may have served as a royal gift, / as [it] does not appear in inventories of [his] possessions.
/Another medallion inside shows [him] a few years later.
/ The bronze medallion above [the central door]was cast from a medal struck in 1661 which shows [the king] at the age of twenty-one.
/ A panel of marquetryshowing the cockerel of [France] standing triumphant over both the eagle of the Holy Roman Empire and thelion of Spain and the Spanish Netherlands decorates [the central door].
/ In [the Dutch Wars] of 1672 - 1678,[France] fought simultaneously against the Dutch, Spanish, and Imperial armies, defeating them all.
/ [Thecabinet] celebrates the Treaty of Nijmegen, which concluded [the war].
/ The Sun King?s portrait appearstwice on [this work].
/ Two large figures from Greek mythology, Hercules and Hippolyta, Queen of theAmazons, representatives of strength and bravery in war appear to support [the cabinet].
/ The decoration on[the cabinet] refers to [Louis XIV?s] military victories.
/ On the drawer above the door, gilt-bronze militarytrophies flank a medallion portrait of [the king].Figure 3: An example output based on M.NOCB.2002).
The difference between the two problems isthat Barzilay et al minimise the sum of all costsCij for any pair i, j of discourse units with i < j,whereas we only sum over the Cij for i = j ?
1.This makes their problem amenable to the approxi-mation algorithm by Cohen et al (1999), which al-lows them to compute a solution that is at least halfas good as the optimum, in polynomial time; i.e.this problem is strictly easier than TSP or discourseordering.
However, a Majority Ordering algorithmis not guaranteed to compute good solutions to thediscourse ordering problem, as Lapata (2003) as-sumes.7 ConclusionWe have shown that the problem of ordering clausesinto a discourse that maximises local coherence isequivalent to the travelling salesman problem: Eventhe two-place discourse ordering problem can en-code ATSP.
This means that the problem is NP-complete and doesn?t even admit polynomial ap-proximation algorithms (unless P=NP).On the other hand, we have shown how to encodethe discourse ordering problems of arbitrary arityd into GATSP.
We have demonstrated that mod-ern branch-and-cut algorithms for GATSP can eas-ily solve practical discourse ordering problems ifd = 2, and are still usable for many instances withd = 3.
As far as we are aware, this is the first al-gorithm for discourse ordering that can make anyguarantees about the solution it computes.Our efficient implementation can benefit genera-tion and summarisation research in at least two re-spects.
First, we show that computing locally co-herent orderings of clauses is feasible in practice,as such coherence measures will probably be ap-plied on sentences within the same paragraph, i.e.on problem instances of limited size.
Second, oursystem should be a useful experimentation tool indeveloping new measures of local coherence.We have focused on local coherence in this paper,but it seems clear that notions of global coherence,which go beyond the level of sentence-to-sentencetransitions, capture important aspects of coherencethat a purely local model cannot.
However, our al-gorithm can still be useful as a subroutine in a morecomplex system that deals with global coherence(Marcu, 1997; Mellish et al, 1998).
Whether ourmethods can be directly applied to the tree struc-tures that come up in theories of global coherence isan interesting question for future research.Acknowledgments.
We would like to thankMirella Lapata for providing the experimental dataand Andrea Lodi for providing an efficiency base-line by running his ATSP solver on our inputs.
Weare grateful to Malte Gabsdil, Ruli Manurung, ChrisMellish, Kristina Striegnitz, and our reviewers forhelpful comments and discussions.ReferencesR.
Barzilay, N. Elhadad, and K. R. McKeown.2002.
Inferring strategies for sentence orderingin multidocument news summarization.
Journalof Artificial Intelligence Research, 17:35?55.S.
Brennan, M. Walker Friedman, and C. Pollard.1987.
A centering approach to pronouns.
InProc.
25th ACL, pages 155?162, Stanford.W.
Cohen, R. Schapire, and Y.
Singer.
1999.
Learn-ing to order things.
Journal of Artificial Intelli-gence Research, 10:243?270.T.
H. Cormen, C. E. Leiserson, and R. L. Rivest.1990.
Introduction to Algorithms.
MIT Press,Cambridge.M.
Fischetti, A. Lodi, and P. Toth.
2001.
Solv-ing real-world ATSP instances by branch-and-cut.
Combinatorial Optimization.M.
Fischetti, A. Lodi, and P. Toth.
2002.
Exactmethods for the asymmmetric traveling salesmanproblem.
In G. Gutin and A. Punnen, editors, TheTraveling Salesman Problem and its Variations.Kluwer.N.
Karamanis and H. M. Manurung.
2002.Stochastic text structuring using the principle ofcontinuity.
In Proceedings of INLG-02, pages81?88, New York.N.
Karamanis, M. Poesio, C. Mellish, and J. Ober-lander.
2004.
Evaluating centering-based met-rics of coherence for text structuring using a re-liably annotated corpus.
In Proceedings of the42nd ACL, Barcelona.N.
Karamanis.
2003.
Entity Coherence for De-scriptive Text Structuring.
Ph.D. thesis, Divisionof Informatics, University of Edinburgh.R.
Kibble and R. Power.
2000.
An integratedframework for text planning and pronominalisa-tion.
In Proc.
INLG 2000, pages 77?84, MitzpeRamon.M.
Lapata.
2003.
Probabilistic text structuring: Ex-periments with sentence ordering.
In Proc.
41stACL, pages 545?552, Sapporo, Japan.G.
Laporte, H. Mercure, and Y. Nobert.
1987.
Gen-eralized travelling salesman problem through nsets of nodes: the asymmetrical case.
DiscreteApplied Mathematics, 18:185?197.W.
Mann and S. Thompson.
1988.
Rhetorical struc-ture theory: A theory of text organization.
Text,8(3):243?281.D.
Marcu.
1997.
From local to global coherence:A bottom-up approach to text planning.
In Pro-ceedings of the 14th AAAI, pages 629?635.C.
Mellish, A. Knott, J. Oberlander, andM.
O?Donnell.
1998.
Experiments usingstochastic search for text planning.
In Proc.
9thINLG, pages 98?107, Niagara-on-the-Lake.G.L.
Nemhauser and L.A. Wolsey.
1988.
Integerand Combinatorial Optimization.
John Wiley &Sons.C.E.
Noon and J.C. Bean.
1993.
An efficient trans-formation of the generalized traveling salesmanproblem.
Information Systems and OperationalResearch, 31(1).M.
Strube and U. Hahn.
1999.
Functional center-ing: Grounding referential coherence in informa-tion structure.
Computational Linguistics, 25(3).M.
Walker, A. Joshi, and E. Prince.
1998.
Center-ing in naturally occuring discourse: An overview.In M. Walker, A. Joshi, and E. Prince, edi-tors, Centering Theory in Discourse, pages 1?30.Clarendon Press, Oxford.B.
Webber, A. Knott, M. Stone, and A. Joshi.
1999.What are little trees made of: A structural andpresuppositional account using Lexicalized TAG.In Proc.
36th ACL, pages 151?156, College Park.
