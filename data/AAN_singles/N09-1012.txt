Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 101?109,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsImproving Unsupervised Dependency Parsingwith Richer Contexts and SmoothingWilliam P. Headden III, Mark Johnson, David McCloskyBrown Laboratory for Linguistic Information Processing (BLLIP)Brown UniversityProvidence, RI 02912{headdenw,mj,dmcc}@cs.brown.eduAbstractUnsupervised grammar induction models tendto employ relatively simple models of syntaxwhen compared to their supervised counter-parts.
Traditionally, the unsupervised mod-els have been kept simple due to tractabil-ity and data sparsity concerns.
In this paper,we introduce basic valence frames and lexi-cal information into an unsupervised depen-dency grammar inducer and show how thisadditional information can be leveraged viasmoothing.
Our model produces state-of-the-art results on the task of unsupervised gram-mar induction, improving over the best previ-ous work by almost 10 percentage points.1 IntroductionThe last decade has seen great strides in statisti-cal natural language parsing.
Supervised and semi-supervised methods now provide highly accurateparsers for a number of languages, but require train-ing from corpora hand-annotated with parse trees.Unfortunately, manually annotating corpora withparse trees is expensive and time consuming so forlanguages and domains with minimal resources it isvaluable to study methods for parsing without re-quiring annotated sentences.In this work, we focus on unsupervised depen-dency parsing.
Our goal is to produce a directedgraph of dependency relations (e.g.
Figure 1) whereeach edge indicates a head-argument relation.
Sincethe task is unsupervised, we are not given any ex-amples of correct dependency graphs and only takewords and their parts of speech as input.
Mostof the recent work in this area (Smith, 2006; Co-hen et al, 2008) has focused on variants of theThe big dog barksFigure 1: Example dependency parse.Dependency Model with Valence (DMV) by Kleinand Manning (2004).
DMV was the first unsu-pervised dependency grammar induction system toachieve accuracy above a right-branching baseline.However, DMV is not able to capture some of themore complex aspects of language.
Borrowing someideas from the supervised parsing literature, wepresent two new models: Extended Valence Gram-mar (EVG) and its lexicalized extension (L-EVG).The primary difference between EVG and DMV isthat DMV uses valence information to determine thenumber of arguments a head takes but not their cat-egories.
In contrast, EVG allows different distri-butions over arguments for different valence slots.L-EVG extends EVG by conditioning on lexical in-formation as well.
This allows L-EVG to potentiallycapture subcategorizations.
The downside of addingadditional conditioning events is that we introducedata sparsity problems.
Incorporating more valenceand lexical information increases the number of pa-rameters to estimate.
A common solution to datasparsity in supervised parsing is to add smoothing.We show that smoothing can be employed in an un-supervised fashion as well, and show that mixingDMV, EVG, and L-EVG together produces state-of-the-art results on this task.
To our knowledge, this isthe first time that grammars with differing levels ofdetail have been successfully combined for unsuper-vised dependency parsing.A brief overview of the paper follows.
In Section2, we discuss the relevant background.
Section 3presents how we will extend DMV with additional101features.
We describe smoothing in an unsupervisedcontext in Section 4.
In Section 5, we discuss searchissues.
We present our experiments in Section 6 andconclude in Section 7.2 BackgroundIn this paper, the observed variables will be a corpusof n sentences of text s = s1 .
.
.
sn, and for eachword sij an associated part-of-speech ?ij .
We denotethe set of all words as Vw and the set of all parts-of-speech as V?
.
The hidden variables are parse treest = t1 .
.
.
tn and parameters ??
which specify a dis-tribution over t. A dependency tree ti is a directedacyclic graph whose nodes are the words in si.
Thegraph has a single incoming edge for each word ineach sentence, except one called the root of ti.
Anedge from word i to word j means that word j isan argument of word i or alternatively, word i is thehead of word j.
Note that each word token may bethe argument of at most one head, but a head mayhave several arguments.If parse tree ti can be drawn on a plane above thesentence with no crossing edges, it is called projec-tive.
Otherwise it is nonprojective.
As in previouswork, we restrict ourselves to projective dependencytrees.
The dependency models in this paper will beformulated as a particular kind of Probabilistic Con-text Free Grammar (PCFG), described below.2.1 Tied Probabilistic Context Free GrammarsIn order to perform smoothing, we will find useful aclass of PCFGs in which the probabilities of certainrules are required to be the same.
This will allowus to make independence assumptions for smooth-ing purposes without losing information, by givinganalogous rules the same probability.Let G = (N ,T , S,R, ?)
be a Probabilistic Con-text Free Grammar with nonterminal symbols N ,terminal symbols T , start symbol S ?
N , set ofproductions R of the form N ?
?, N ?
N , ?
?
(N ?
T )?.
Let RN indicate the subset of R whoseleft-hand sides are N .
?
is a vector of length |R|, in-dexed by productions N ?
?
?
R.
?N??
specifiesthe probability that N rewrites to ?.
We will let ?Nindicate the subvector of ?
corresponding to RN .A tied PCFG constrains a PCFG G with a tyingrelation, which is an equivalence relation over rulesthat satisfies the following properties:1.
Tied rules have the same probability.2.
Rules expanding the same nonterminal arenever tied.3.
If N1 ?
?1 and N2 ?
?2 are tied then the ty-ing relation defines a one-to-one mapping be-tween rules in RN1 and RN2 , and we say thatN1 and N2 are tied nonterminals.As we see below, we can estimate tied PCFGs usingstandard techniques.
Clearly, the tying relation alsodefines an equivalence class over nonterminals.
Thetying relation allows us to formulate the distribu-tions over trees in terms of rule equivalence classesand nonterminal equivalence classes.
Suppose R?
isthe set of rule equivalence classes and N?
is the setof nonterminal equivalence classes.
Since all rulesin an equivalence class r?
have the same probability(condition 1), and since all the nonterminals in anequivalence class N?
?
N?
have the same distribu-tion over rule equivalence classes (condition 1 and3), we can define the set of rule equivalence classesR?N?
associated with a nonterminal equivalence classN?
, and a vector ??
of probabilities, indexed by ruleequivalence classes r?
?
R?
.
??N?
refers to the sub-vector of ??
associated with nonterminal equivalenceclass N?
, indexed by r?
?
R?N?
.
Since rules in thesame equivalence class have the same probability,we have that for each r ?
r?, ?r = ?
?r?.Let f(t, r) denote the number of times rule r ap-pears in tree t, and let f(t, r?)
= ?r?r?
f(t, r).
Wesee that the complete data likelihood isP (s, t|?)
= ?r??R??r?r?
?f(t,r)r =?r??R???f(t,r?
)r?That is, the likelihood is a product of multinomi-als, one for each nonterminal equivalence class, andthere are no constraints placed on the parameters ofthese multinomials besides being positive and sum-ming to one.
This means that all the standard es-timation methods (e.g.
Expectation Maximization,Variational Bayes) extend directly to tied PCFGs.Maximum likelihood estimation provides a pointestimate of ??.
However, often we want to incorpo-rate information about ??
by modeling its prior distri-bution.
As a prior, for each N?
?
N?
we will specify a102Dirichlet distribution over ??N?
with hyperparameters?N?
.
The Dirichlet has the density function:P (??N?
|?N? )
=?(?r??R?N?
?r?)?r??R?N?
?(?r?)?r??R?N????r??1r?
,Thus the prior over ??
is a product of Dirichlets,whichis conjugate to the PCFG likelihood function (John-son et al, 2007).
That is, the posterior P (?
?|s, t, ?
)is also a product of Dirichlets, also factoring into aDirichlet for each nonterminal N?
, where the param-eters ?r?
are augmented by the number of times ruler?
is observed in tree t:P (?
?|s, t, ?)
?
P (s, t|??
)P (??|?)??r??R???f(t,r?)+?r?
?1r?We can see that ?r?
acts as a pseudocount of the num-ber of times r?
is observed prior to t.To make use of this prior, we use the VariationalBayes (VB) technique for PCFGs with Dirichlet Pri-ors presented by Kurihara and Sato (2004).
VB es-timates a distribution over ??.
In contrast, Expec-tation Maximization estimates merely a point esti-mate of ??.
In VB, one estimates Q(t, ??
), calledthe variational distribution, which approximates theposterior distribution P (t, ?
?|s, ?)
by minimizing theKL divergence of P from Q.
Minimizing the KLdivergence, it turns out, is equivalent to maximiz-ing a lower bound F of the log marginal likelihoodlog P (s|?
).log P (s|?)
?
?t??
?Q(t, ??)
log P (s, t, ??|?
)Q(t, ??)
= FThe negative of the lower bound, ?F , is sometimescalled the free energy.As is typical in variational approaches, Kuri-hara and Sato (2004) make certain independence as-sumptions about the hidden variables in the vari-ational posterior, which will make estimating itsimpler.
It factors Q(t, ??)
= Q(t)Q(??)
=?ni=1 Qi(ti)?N??N?
Q(??N?
).
The goal is to recoverQ(??
), the estimate of the posterior distribution overparameters and Q(t), the estimate of the posteriordistribution over trees.
Finding a local maximum ofF is done via an alternating maximization of Q(??
)and Q(t).
Kurihara and Sato (2004) show that eachQ(??N? )
is a Dirichlet distribution with parameters?
?r = ?r + EQ(t)f(t, r).2.2 Split-head Bilexical CFGsIn the sections that follow, we frame various de-pendency models as a particular variety of CFGsknown as split-head bilexical CFGs (Eisner andSatta, 1999).
These allow us to use the fast Eisnerand Satta (1999) parsing algorithm to compute theexpectations required by VB in O(m3) time (Eis-ner and Blatz, 2007; Johnson, 2007) where m is thelength of the sentence.1In the split-head bilexical CFG framework, eachnonterminal in the grammar is annotated with a ter-minal symbol.
For dependency grammars, theseannotations correspond to words and/or parts-of-speech.
Additionally, split-head bilexical CFGs re-quire that each word sij in sentence si is representedin a split form by two terminals called its left partsijL and right part sijR.
The set of these parts con-stitutes the terminal symbols of the grammar.
Thissplit-head property relates to a particular type of de-pendency grammar in which the left and right depen-dents of a head are generated independently.
Notethat like CFGs, split-head bilexical CFGs can bemade probabilistic.2.3 Dependency Model with ValenceThe most successful recent work on dependencyinduction has focused on the Dependency Modelwith Valence (DMV) by Klein and Manning (2004).DMV is a generative model in which the head ofthe sentence is generated and then each head recur-sively generates its left and right dependents.
Thearguments of head H in direction d are generatedby repeatedly deciding whether to generate anothernew argument or to stop and then generating theargument if required.
The probability of decidingwhether to generate another argument is conditionedon H , d and whether this would be the first argument(this is the sense in which it models valence).
WhenDMV generates an argument, the part-of-speech ofthat argument A is generated given H and d.1Efficiently parsable versions of split-head bilexical CFGsfor the models described in this paper can be derived using thefold-unfold grammar transform (Eisner and Blatz, 2007; John-son, 2007).103Rule DescriptionS ?
YH Select H as rootYH ?
LH RH Move to split-head representationLH ?
HL STOP | dir = L, head = H,val = 0LH ?
L1H CONT | dir = L, head = H, val = 0L?H ?
HL STOP | dir = L, head = H,val = 1L?H ?
L1H CONT | dir = L, head = H, val = 1L1H ?
YA L?H Arg A | dir = L, head = HFigure 2: Rule schema for DMV.
For brevity, we omitthe portion of the grammar that handles the right argu-ments since they are symmetric to the left (all rules arethe same except for the attachment rule where the RHS isreversed).
val ?
{0, 1} indicates whether we have madeany attachments.The grammar schema for this model is shown inFigure 2.
The first rule generates the root of the sen-tence.
Note that these rules are for ?H,A ?
V?
sothere is an instance of the first schema rule for eachpart-of-speech.
YH splits words into their left andright components.
LH encodes the stopping deci-sion given that we have not generated any argumentsso far.
L?H encodes the same decision after generat-ing one or more arguments.
L1H represents the distri-bution over left attachments.
To extract dependencyrelations from these parse trees, we scan for attach-ment rules (e.g., L1H ?
YA L?H) and record thatA depends on H .
The schema omits the rules forright arguments since they are symmetric.
We showa parse of ?The big dog barks?
in Figure 3.2Much of the extensions to this work have fo-cused on estimation procedures.
Klein and Manning(2004) use Expectation Maximization to estimatethe model parameters.
Smith and Eisner (2005) andSmith (2006) investigate using Contrastive Estima-tion to estimate DMV.
Contrastive Estimation max-imizes the conditional probability of the observedsentences given a neighborhood of similar unseensequences.
The results of this approach vary widelybased on regularization and neighborhood, but oftenoutperforms EM.2Note that our examples use words as leaf nodes but in ourunlexicalized models, the leaf nodes are in fact parts-of-speech.SYbarksLbarksL1barksYdogLdogL1dogYTheLTheTheLRTheTheRL?dogL1dogYbigLbigbigLRbigbigRL?dogdogLRdogdogRL?barksbarksLRbarksbarksRFigure 3: DMV split-head bilexical CFG parse of ?Thebig dog barks.
?Smith (2006) also investigates two techniques formaximizing likelihood while incorporating the lo-cality bias encoded in the harmonic initializer forDMV.
One technique, skewed deterministic anneal-ing, ameliorates the local maximum problem by flat-tening the likelihood and adding a bias towards theKlein and Manning initializer, which is decreasedduring learning.
The second technique is structuralannealing (Smith and Eisner, 2006; Smith, 2006)which penalizes long dependencies initially, grad-ually weakening the penalty during estimation.
Ifhand-annotated dependencies on a held-out set areavailable for parameter selection, this performs farbetter than EM; however, performing parameter se-lection on a held-out set without the use of gold de-pendencies does not perform as well.Cohen et al (2008) investigate using BayesianPriors with DMV.
The two priors they use are theDirichlet (which we use here) and the Logistic Nor-mal prior, which allows the model to capture correla-tions between different distributions.
They initializeusing the harmonic initializer of Klein and Manning(2004).
They find that the Logistic Normal distri-bution performs much better than the Dirichlet withthis initialization scheme.Cohen and Smith (2009), investigate (concur-104Rule DescriptionS ?
YH Select H as rootYH ?
LH RH Move to split-head representationLH ?
HL STOP | dir = L, head = H,val = 0LH ?
L?H CONT | dir = L, head = H, val = 0L?H ?
L1H STOP | dir = L, head = H,val = 1L?H ?
L2H CONT | dir = L, head = H, val = 1L2H ?
YA L?H Arg A | dir = L, head = H,val = 1L1H ?
YA HL Arg A | dir = L, head = H,val = 0Figure 4: Extended Valence Grammar schema.
As be-fore, we omit rules involving the right parts of words.
Inthis case, val ?
{0, 1} indicates whether we are generat-ing the nearest argument (0) or not (1).rently with our work) an extension of this, theShared Logistic Normal prior, which allows differ-ent PCFG rule distributions to share components.They use this machinery to investigate smoothingthe attachment distributions for (nouns/verbs), andfor learning using multiple languages.3 Enriched ContextsDMV models the distribution over arguments iden-tically without regard to their order.
Instead, wepropose to distinguish the distribution over the argu-ment nearest the head from the distribution of sub-sequent arguments.
3Consider the following changes to the DMVgrammar (results shown in Figure 4).
First, we willintroduce the rule L2H ?
YA L?H to denote the deci-sion of what argument to generate for positions notnearest to the head.
Next, instead of having L?H ex-pand to HL or L1H , we will expand it to L1H (attachto nearest argument and stop) or L2H (attach to non-nearest argument and continue).
We call this the Ex-tended Valence Grammar (EVG).As a concrete example, consider the phrase ?thebig hungry dog?
(Figure 5).
We would expect thatdistribution over the nearest left argument for ?dog?to be different than farther left arguments.
The fig-3McClosky (2008) explores this idea further in an un-smoothed grammar....LdogL1dogYTheTheL TheRL?dogL1dogYbigbigL bigRL?dogdogL...LdogL?dogL2dogYTheTheL TheRL?dogL1dogYbigbigL bigRdogLFigure 5: An example of moving from DMV to EVGfor a fragment of ?The big dog.?
Boxed nodes indicatechanges.
The key difference is that EVG distinguishesbetween the distributions over the argument nearest thehead (big) from arguments farther away (The).ure shows that EVG allows these two distributions tobe different (nonterminals L2dog and L1dog) whereasDMV forces them to be equivalent (both use L1dog asthe nonterminal).3.1 LexicalizationAll of the probabilistic models discussed thus farhave incorporated only part-of-speech information(see Footnote 2).
In supervised parsing of both de-pendencies and constituency, lexical information iscritical (Collins, 1999).
We incorporate lexical in-formation into EVG (henceforth L-EVG) by extend-ing the distributions over argument parts-of-speechA to condition on the head word h in addition to thehead part-of-speech H , direction d and argument po-sition v. The argument word a distribution is merelyconditioned on part-of-speech A; we leave refiningthis model to future work.In order to incorporate lexicalization, we extendthe EVG CFG to allow the nonterminals to be anno-tated with both the word and part-of-speech of thehead.
We first remove the old rules YH ?
LH RHfor each H ?
V?
.
Then we mark each nonter-minal which is annotated with a part-of-speech asalso annotated with its head, with a single excep-tion: YH .
We add a new nonterminal YH,h for eachH ?
V?
, h ?
Vw, and the rules YH ?
YH,h andYH,h ?
LH,h RH,h.
The rule YH ?
YH,h cor-responds to selecting the word, given its part-of-speech.1054 SmoothingIn supervised estimation one common smoothingtechnique is linear interpolation, (Jelinek, 1997).This section explains how linear interpolation canbe represented using a PCFG with tied rule proba-bilities, and how one might estimate smoothing pa-rameters in an unsupervised framework.In many probabilistic models it is common to esti-mate the distribution of some event x conditioned onsome set of context information P (x|N(1) .
.
.
N(k))by smoothing it with less complicated condi-tional distributions.
Using linear interpolationwe model P (x|N(1) .
.
.
N(k)) as a weighted aver-age of two distributions ?1P1(x|N(1), .
.
.
, N(k)) +?2P2(x|N(1), .
.
.
, N(k?1)), where the distributionP2 makes an independence assumption by droppingthe conditioning event N(k).In a PCFG a nonterminal N can encode a collec-tion of conditioning events N(1) .
.
.
N(k), and ?N de-termines a distribution conditioned on N(1) .
.
.
N(k)over events represented by the rules r ?
RN .
Forexample, in EVG the nonterminal L1NN encodesthree separate pieces of conditioning information:the direction d = left , the head part-of-speechH = NN , and the argument position v = 0;?L1NN?YJJ NNL represents the probability of gener-ating JJ as the first left argument of NN .
Sup-pose in EVG we are interested in smoothing P (A |d,H, v) with a component that excludes the headconditioning event.
Using linear interpolation, thiswould be:P (A | d,H, v) = ?1P1(A | d,H, v)+?2P2(A | d, v)We will estimate PCFG rules with linearly interpo-lated probabilities by creating a tied PCFG whichis extended by adding rules that select between themain distribution P1 and the backoff distribution P2,and also rules that correspond to draws from thosedistributions.
We will make use of tied rule proba-bilities to make the independence assumption in thebackoff distribution.We still use the original grammar to parse the sen-tence.
However, we estimate the parameters in theextended grammar and then translate them back intothe original grammar for parsing.More formally, suppose B ?
N is a set of non-terminals (called the backoff set) with conditioningevents N(1) .
.
.
N(k?1) in common (differing in aconditioning event N(k)), and with rule sets of thesame cardinality.
If G is our model?s PCFG, we candefine a new tied PCFG G?
= (N ?,T , S,R?, ?
),where N ?
= N ?
{N b?
| N ?
B, ?
?
{1, 2}},meaning for each nonterminal N in the backoffset we add two nonterminals N b1 , N b2 represent-ing each distribution P1 and P2.
The new ruleset R?
= (?N?N ?R?N ) where for all N ?
Brule set R?N ={N ?
N b?
| ?
?
{1, 2}}, mean-ing at N in G?
we decide which distribution P1, P2to use; and for N ?
B and ?
?
{1, 2} ,R?Nb?
={N b?
?
?
| N ?
?
?
RN}indicating adraw from distribution P?.
For nonterminals N 6?
B,R?N = RN .
Finally, for each N,M ?
B wespecify a tying relation between the rules in R?Nb2and R?Mb2 , grouping together analogous rules.
Thishas the effect of making an independence assump-tion about P2, namely that it ignores the condition-ing event N(k), drawing from a common distributioneach time a nonterminal N b2 is rewritten.For example, in EVG to smooth P (A = DT |d = left ,H = NN , v = 0) with P2(A = DT |d = left , v = 0) we define the backoff set tobe{L1H | H ?
V?}.
In the extended grammar wedefine the tying relation to form rule equivalenceclasses by the argument they generate, i.e.
for eachargument A ?
V?
, we have a rule equivalence class{L1b2H ?
YA HL | H ?
V?
}.We can see that in grammar G?
each N ?
B even-tually ends up rewriting to one of N ?s expansions ?in G. There are two indirect paths, one through N b1and one through N b2 .
Thus this defines the proba-bility of N ?
?
in G, ?N?
?, as the probability ofrewriting N as ?
in G?
via N b1 and N b2 .
That is:?N??
= ?N?Nb1?Nb1??
+ ?N?Nb2?Nb2?
?The example in Figure 6 shows the probability thatL1dog rewrites to Ybig dogL in grammar G.Typically when smoothing we need to incorporatethe prior knowledge that conditioning events thathave been seen fewer times should be more stronglysmoothed.
We accomplish this by setting the Dirich-let hyperparameters for each N ?
N b1 , N ?
N b2decision to (K, 2K), where K = |RNb1 | is the num-ber of rewrite rules for A.
This ensures that themodel will only start to ignore the backoff distribu-106PG0BB@L1dogYbig dogL1CCA= PG?0BBBBBBB@L1dogL1b1dogYbig dogL1CCCCCCCA+ PG?0BBBBBBB@L1dogL1b2dogYbig dogL1CCCCCCCAFigure 6: Using linear interpolation to smooth L1dog ?Ybig dogL: The first component represents the distri-bution fully conditioned on head dog, while the secondcomponent represents the distribution ignoring the headconditioning event.
This later is accomplished by tyingthe rule L1b2dog ?
Ybig dogL to, for instance, L1b2cat ?Ybig catL, L1b2fish ?
Ybig fishL etc.tion after having seen a sufficiently large number oftraining examples.
44.1 Smoothed Dependency ModelsOur first experiments examine smoothing the dis-tributions over an argument in the DMV and EVGmodels.
In DMV we smooth the probability of argu-ment A given head part-of-speech H and direction dwith a distribution that ignores H .
In EVG, whichconditions on H , d and argument position v we backoff two ways.
The first is to ignore v and use back-off conditioning event H, d. This yields a backoffdistribution with the same conditioning informationas the argument distribution from DMV.
We call thisEVG smoothed-skip-val.The second possibility is to have the backoffdistribution ignore the head part-of-speech H anduse backoff conditioning event v, d. This assumesthat arguments share a common distribution acrossheads.
We call this EVG smoothed-skip-head.
Aswe see below, backing off by ignoring the part-of-speech of the head H worked better than ignoringthe argument position v.For L-EVG we smooth the argument part-of-speech distribution (conditioned on the head word)with the unlexicalized EVG smoothed-skip-headmodel.5 Initialization and Search issuesKlein and Manning (2004) strongly emphasize theimportance of smart initialization in getting goodperformance from DMV.
The likelihood function isfull of local maxima and different initial parametervalues yield vastly different quality solutions.
Theyoffer what they call a ?harmonic initializer?
which4We set the other Dirichlet hyperparameters to 1.initializes the attachment probabilities to favor ar-guments that appear more closely in the data.
Thisstarts EM in a state preferring shorter attachments.Since our goal is to expand the model to incor-porate lexical information, we want an initializa-tion scheme which does not depend on the detailsof DMV.
The method we use is to create M sets ofB random initial settings and to run VB some smallnumber of iterations (40 in all our experiments) foreach initial setting.
For each of the M sets, themodel with the best free energy of the B runs isthen run out until convergence (as measured by like-lihood of a held-out data set); the other models arepruned away.
In this paper we use B = 20 andM = 50.For the bth setting, we draw a random samplefrom the prior ??(b).
We set the initial Q(t) =P (t|s, ??
(b)) which can be calculated using theExpectation-Maximization E-Step.
Q(??)
is then ini-tialized using the standard VB M-step.For the Lexicalized-EVG, we modify this proce-dure slightly, by first running MB smoothed EVGmodels for 40 iterations each and selecting the bestmodel in each cohort as before; each L-EVG dis-tribution is initialized from its corresponding EVGdistribution.
The new P (A|h,H, d, v) distributionsare set initially to their corresponding P (A|H, d, v)values.6 ResultsWe trained on the standard Penn Treebank WSJ cor-pus (Marcus et al, 1993).
Following Klein and Man-ning (2002), sentences longer than 10 words afterremoving punctuation are ignored.
We refer to thisvariant as WSJ10.
Following Cohen et al (2008),we train on sections 2-21, used 22 as a held-out de-velopment corpus, and present results evaluated onsection 23.
The models were all trained using Varia-tional Bayes, and initialized as described in Section5.
To evaluate, we follow Cohen et al (2008) in us-ing the mean of the variational posterior Dirichletsas a point estimate ???.
For the unsmoothed modelswe decode by selecting the Viterbi parse given ??
?, orargmaxtP (t|s, ???
).For the smoothed models we find the Viterbi parseof the unsmoothed CFG, but use the smoothed prob-abilities.
We evaluate against the gold standard107Model Variant Dir.
Acc.DMV harmonic init 46.9*DMV random init 55.7 (8.0)DMV log normal-families 59.4*DMV shared log normal-families 62.4?DMV smoothed 61.2 (1.2)EVG random init 53.3 (7.1)EVG smoothed-skip-val 62.1 (1.9)EVG smoothed-skip-head 65.0 (5.7)L-EVG smoothed 68.8 (4.5)Table 1: Directed accuracy (DA) for WSJ10, section 23.*,?
indicate results reported by Cohen et al (2008), Co-hen and Smith (2009) respectively.
Standard deviationsover 10 runs are given in parenthesesdependencies for section 23, which were extractedfrom the phrase structure trees using the standardrules by Yamada and Matsumoto (2003).
We mea-sure the percent accuracy of the directed dependencyedges.
For the lexicalized model, we replaced allwords that were seen fewer than 100 times with?UNK.?
We ran each of our systems 10 times, andreport the average directed accuracy achieved.
Theresults are shown in Table 1.
We compare to workby Cohen et al (2008) and Cohen and Smith (2009).Looking at Table 1, we can first of all see thebenefit of randomized initialization over the har-monic initializer for DMV.
We can also see a largegain by adding smoothing to DMV, topping eventhe logistic normal prior.
The unsmoothed EVG ac-tually performs worse than unsmoothed DMV, butboth smoothed versions improve even on smoothedDMV.
Adding lexical information (L-EVG) yields amoderate further improvement.As the greatest improvement comes from movingto model EVG smoothed-skip-head, we show in Ta-ble 2 the most probable arguments for each val, dir,using the mean of the appropriate variational Dirich-let.
For d = right, v = 1, P (A|v, d) largely seemsto acts as a way of grouping together various verbtypes, while for d = left, v = 0 the model findsthat nouns tend to act as the closest left argument.Dir,Val Arg Prob Dir,Val Arg Probleft, 0 NN 0.65 right, 0 NN 0.26NNP 0.18 RB 0.23DT 0.12 NNS 0.12IN 0.11left, 1 CC 0.35 right, 1 IN 0.78RB 0.27IN 0.18Table 2: Most likely arguments given valence and direc-tion, according to smoothing distributionP (arg|dir, val)in EVG smoothed-skip-head model with lowest free en-ergy.7 ConclusionWe present a smoothing technique for unsupervisedPCFG estimation which allows us to explore moresophisticated dependency grammars.
Our methodcombines linear interpolation with a Bayesian priorthat ensures the backoff distribution receives proba-bility mass.
Estimating the smoothed model requiresrunning the standard Variational Bayes on an ex-tended PCFG.
We used this technique to estimate aseries of dependency grammars which extend DMVwith additional valence and lexical information.
Wefound that both were helpful in learning English de-pendency grammars.
Our L-EVG model gives thebest reported accuracy to date on the WSJ10 corpus.Future work includes using lexical informationmore deeply in the model by conditioning argumentwords and valence on the lexical head.
We suspectthat successfully doing so will require using muchlarger datasets.
We would also like to explore us-ing our smoothing technique in other models suchas HMMs.
For instance, we could do unsupervisedHMM part-of-speech induction by smooth a tritagmodel with a bitag model.
Finally, we would like tolearn the parts-of-speech in our dependency modelfrom text and not rely on the gold-standard tags.AcknowledgementsThis research is based upon work supported byNational Science Foundation grants 0544127 and0631667 and DARPA GALE contract HR0011-06-2-0001.
We thank members of BLLIP for their feed-back.108ReferencesShay B. Cohen and Noah A. Smith.
2009.
Shared lo-gistic normal distributions for soft parameter tying inunsupervised grammar induction.
In Proceedings ofNAACL-HLT 2009.Shay B. Cohen, Kevin Gimpel, and Noah A. Smith.2008.
Logistic normal priors for unsupervised prob-abilistic grammar induction.
In Advances in NeuralInformation Processing Systems 21.Michael Collins.
1999.
Head-driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, The Uni-versity of Pennsylvania.Jason Eisner and John Blatz.
2007.
Program transforma-tions for optimization of parsing algorithms and otherweighted logic programs.
In Proceedings of the 11thConference on Formal Grammar.Jason Eisner and Giorgio Satta.
1999.
Efficient pars-ing for bilexical context-free grammars and head-automaton grammars.
In Proceedings of ACL 1999.Frederick Jelinek.
1997.
Statistical Methods for SpeechRecognition.
The MIT Press, Cambridge, Mas-sachusetts.Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-ter.
2007.
Bayesian inference for PCFGs via Markovchain Monte Carlo.
In Proceedings of NAACL 2007.Mark Johnson.
2007.
Transforming projective bilexicaldependency grammars into efficiently-parsable CFGswith unfold-fold.
In Proceedings of ACL 2007.Dan Klein and Christopher Manning.
2002.
A genera-tive constituent-context model for improved grammarinduction.
In Proceedings of ACL 2002.Dan Klein and Christopher Manning.
2004.
Corpus-based induction of syntactic structure: Models of de-pendency and constituency.
In Proceedings of ACL2004, July.Kenichi Kurihara and Taisuke Sato.
2004.
An applica-tion of the variational bayesian approach to probabilis-tics context-free grammars.
In IJCNLP 2004 Work-shop Beyond Shallow Analyses.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19(2):313?330.David McClosky.
2008.
Modeling valence effects in un-supervised grammar induction.
Technical Report CS-09-01, Brown University, Providence, RI, USA.Noah A. Smith and Jason Eisner.
2005.
Guiding unsu-pervised grammar induction using contrastive estima-tion.
In International Joint Conference on ArtificialIntelligence Workshop on Grammatical Inference Ap-plications.Noah A. Smith and Jason Eisner.
2006.
Annealing struc-tural bias in multilingual weighted grammar induction.In Proceedings of COLING-ACL 2006.Noah A. Smith.
2006.
Novel Estimation Methods forUnsupervised Discovery of Latent Structure in NaturalLanguage Text.
Ph.D. thesis, Department of ComputerScience, Johns Hopkins University.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statisticaldependency analysis with support vector machines.
InIn Proceedings of the International Workshop on Pars-ing Technologies.109
