The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 316?325,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsSense-Specific Lexical Information for Reading AssistanceSoojeong EomGeorgetown Universityse48@georgetown.eduMarkus DickinsonIndiana Universitymd7@indiana.eduRebecca SachsGeorgetown Universityrrs8@georgetown.eduAbstractTo support vocabulary acquisition and read-ing comprehension in a second language, wehave developed a system to display sense-appropriate examples to learners for difficultwords.
We describe the construction of thesystem, incorporating word sense disambigua-tion, and an experiment we conducted testingit on a group of 60 learners of English as asecond language (ESL).
We show that sense-specific information in an intelligent readingsystem helps learners in their vocabulary ac-quisition, even if the sense information con-tains some noise from automatic processing.We also show that it helps learners, to someextent, with their reading comprehension.1 Introduction and MotivationReading texts in a second language presents thelanguage learner with a number of comprehensionproblems, including the problem of interpretingwords that are unknown or are used in unfamiliarways.
These problems are exacerbated by the preva-lence of lexical ambiguity.
Landes et al (1998) re-port that more than half the content words in Englishtexts are lexically ambiguous, with the most frequentwords having a large number of meanings.
Theword face, for example, is listed in WordNet (Fell-baum, 1998) with twelve different nominal senses;although not all are equally prevalent, there is stillmuch potential for confusion.To address this, we have designed an online read-ing assistant to provide sense-specific lexical in-formation to readers.
By sense-specific, we referto information applicable only for one given sense(meaning) of a word.
In this paper, we focus onthe system design and whether such a system can bebeneficial.
Our experiment with learners illustratesthe effectiveness of such information for vocabularyacquisition and reading comprehension.The problem of lexical ambiguity in reading com-prehension is a significant one.
While dictionar-ies can help improve comprehension and acquisition(see, e.g., Prichard, 2008), lexical ambiguity maylead to misunderstandings and unsuccessful vocabu-lary acquisition (Luppescu and Day, 1993), as learn-ers may become confused when trying to locate anappropriate meaning for an unknown word amongnumerous sense entries.
Luppescu and Day showedthat readers who use a (printed) dictionary have im-proved comprehension and acquisition, but to thedetriment of their reading speed.For electronic dictionaries as well, lexical am-biguity remains a problem (Koyama and Takeuchi,2004; Laufer and Hill, 2000; Leffa, 1992; Prichard,2008), as readers need specific information about aword as it is used in context in order to effectivelycomprehend the text and thus learn the word.
Kulka-rni et al (2008) demonstrated that providing readerswith sense-specific information led learners to sig-nificantly better vocabulary acquisition than provid-ing them with general word meaning information.We have developed an online system to providevocabulary assistance to learners of English as aSecond Language (ESL), allowing them to clickon unfamiliar words and see lexical information?target word definitions and examples?relevant tothat particular usage.
We discuss previous online316systems in section 2.
Importantly, the examples wepresent are from the COBUILD dictionary (Sinclair,2006), which is designed for language learners.
Topresent these for any text, our system must map au-tomatic word sense disambiguation (WSD) systemoutput (using WordNet senses (Fellbaum, 1998)) toCOBUILD, as covered in section 3, where we alsodescribe general properties of the web system.The main contribution of this work is to investi-gate whether high-quality sense-specific lexical in-formation presented in an intelligent reading systemhelps learners in their vocabulary acquisition andreading comprehension and to investigate the effectof automatic errors on learning.
We accordingly askthe following research questions:1.
Does sense-specific lexical information facili-tate vocabulary acquisition to a greater extentthan: a) no lexical information, and b) lexicalinformation on all senses of each chosen word?2.
Does sense-specific lexical information facili-tate learners?
reading comprehension?The method and analysis for investigating thesequestions with a group of 60 ESL learners is given insection 4, and the results are discussed in section 5.2 BackgroundWhile there are many studies in second languageacquisition (SLA) on providing vocabulary andreading assistance (e.g., Prichard, 2008; Luppescuand Day, 1993), we focus on outlining intelligentcomputer-assisted language learning (ICALL) sys-tems here (see also discussion in Dela Rosa and Es-kenazi, 2011).
Such systems hold the promise of al-leviating some problems of acquiring words whilereading by providing information specific to eachword as it is used in context (Nerbonne and Smit,1996; Kulkarni et al, 2008).
The GLOSSER-RuGsystem (Nerbonne and Smit, 1996) disambiguateson the basis of part of speech (POS).
This is helpfulin distinguishing verbal and nominal uses, for ex-ample, but is, of course, ineffective when a wordhas more than one sense in the same POS (e.g.,face).
More effective is the REAP Tutor (Heilmanet al, 2006), which uses word sense disambigua-tion to provide lexicographic information and hasbeen shown to benefit learners by providing sense-specific lexical information (Dela Rosa and Eske-nazi, 2011; Kulkarni et al, 2008).We build from this work by further demonstrat-ing the utility of sense-specific information.
Whatdistinguishes our work is how we build from the no-tion that the lexical information provided needs tobe tuned to the capacities of ESL learners.
For ex-ample, definitions and illustrative examples shouldmake use of familiar vocabulary if they are to aidlanguage learners; example sentences directly takenfrom corpora or from the web seem less appropriatebecause the information in them might be less ac-cessible (Groot, 2000; Kilgarriff et al, 2008; Segleret al, 2002).
On the other hand, examples con-structed by lexicographers for learner dictionariestypically control for syntactic and lexical complexity(Segler et al, 2002).
We thus make use of examplesfrom a dictionary targeting learners.Specifically, we make use of the examples fromthe Collins COBUILD Student?s Dictionary (Sin-clair, 2006), as it is widely used by ESL learners.The content in COBUILD is based on actual Englishusage and derived from analysis of a large corpus ofwritten and spoken English, thereby providing au-thentic examples (Sinclair, 2006).
COBUILD alsofocuses on collocations in choosing example sen-tences, so that the example sentences present nat-ural, reliable expressions, which can play an im-portant role in learners?
vocabulary acquisition andreading comprehension.
We discuss this resourcemore in section 3.3.3 The web systemTo support vocabulary acquisition and reading com-prehension for language learners, we have designeda system for learners to upload texts and click onwords in order to obtain sense-appropriate examplesfor difficult words while reading, as shown in fig-ure 1.
Although the experiment reported upon herefocused on 2 preselected texts, the system is able topresent lexical information for any content words.Beyond the web interface, the system has three com-ponents: 1) a system manager, 2) a natural languageprocessing (NLP) server, and 3) a lexical database.317Figure 1: A screenshot showing the effect of clicking on unveiling and receiving sense-specific information3.1 System managerThe system manager controls the interaction amongeach learner, the NLP server, and the lexicaldatabase.
When the manager receives a raw text asan input from the learner, it first sends the input textto the server and returns an analyzed text (i.e., tok-enized, POS-tagged, and sense-tagged) back to thelearner, with content words made clickable.
Then,when the learner clicks on a word while reading, themanager sends the word with its sense informationto the lexical database and brings the word with itssense-specific lexical information back to the learnerfrom the lexical database.Upon completion of the reading, the managersends the learner to a page of tests?i.e., a read-ing test and a vocabulary test, as described in sec-tion 4?and records the responses.3.2 NLP preprocessingTo convert raw input into a linguistically-analyzedtext, the system relies on several basic NLP modulesfor tokenizing, lemmatizing, POS tagging, and col-location identification.
Although for some internaltesting with different WSD systems we used otherthird-party software (e.g., the Stanford POS tagger(Toutanova et al, 2003)), our word sense disam-biguator (see below) provides tokenization, lemma-tization, and POS tagging, as well as collocationidentification.
Since the words making up a colloca-tion may be basic, learners can easily overlook them,and so we intend to improve this module in the fu-ture, to reduce underflagging of collocations.3.3 Lexical databaseThe lexical database is used to provide a sense-appropriate definition and example sentences of aninput word to a learner.
To obtain the sense-appropriate information, we must perform wordsense disambiguation (WSD) on the input text.
Weuse SenseRelate::AllWords (SR:AW) (Pedersen andKolhatkar, 2009) to perform WSD of input texts, asthis system has broad coverage of content words.Given that SR:AW does not outperform the most fre-quent sense (MFS) baseline, we intend to exploreusing the MFS in the future, as well as other WSDsystems, such as SenseLearner (Mihalcea and Cso-mai, 2005).
However, the quality of SR:AW (F-measure of 54?61% on different corpora) is suffi-cient to explore in our system and gives us a pointto work from.
Indeed, as we will see in section 5.3,318while SR:AW makes errors, vocabulary learning is,in some ways, perhaps not dramatically impeded.Even with a WSD system, pointing to appropriateexamples is complicated by the fact that the databaseof learner-appropriate examples is from one repos-itory (COBUILD, see section 2), while automaticWSD systems generally use senses from another(WordNet).
The lexical database, then, is indexed byWordNet senses, each of which points to an appro-priate corresponding COBUILD sense.
While wewould prefer disambiguating COBUILD senses di-rectly, we are not aware of any systems which dothis or any COBUILD sense-tagged data to train asystem on.
If the benefits for vocabulary acquisitiongained by providing learner-friendly examples fromCOBUILD merit it, future work could explore build-ing a collection of COBUILD-tagged data to traina WSD system?perhaps a semi-automatic processusing the automatic system we describe next.To build a lexical database covering all words, webuilt a word sense alignment (WSA) system; thisis also in line with a related research agenda in-vestigating the correspondences between sense in-ventories (Eom et al, 2012).
Space limitations pre-clude a more detailed discussion, but the WSA sys-tem works by running SR:AW on COBUILD exam-ples in order to induce a basic alignment structurebetween WordNet and COBUILD.
We then post-process this structure, relying on a heuristic of favor-ing flatter alignment structures?i.e., links spreadout more evenly between senses in each inventory.1Iteratively replacing one link with another, to giveflatter structures, we weight each type of proposedalignment and accept a new alignment if the weightcombined with the probability originally assignedby the WSD system is the best improvement overthat of the original alignment structure.
After allthese steps, the alignments give the lexical databasefor linking WSD output to COBUILD senses.We consider alignment structures wherein eachWordNet sense maps to exactly one COBUILDsense, to match the task at hand, i.e., mapping eachdisambiguated WordNet sense to a single set ofCOBUILD examples.
This assumption also makespostprocessing feasible: instead of considering an1The general idea is to use information about the alignmentstructure as a whole; flatter alignments is a convenient heuristic,in lieu of having any other additional information.exponential number of alignment structures, we con-sider only a polynomial number.Having collected alignment judgments from lin-guistics students and faculty, we evaluated the sys-tem against a small set of nine words, covering 63WordNet senses (Eom et al, 2012).
The WSA sys-tem had a precision of 42.7% (recall=44.5%) whenevaluating against the most popular sense, but aprecision of 60.7% (recall=36.5%) when evaluatingagainst all senses that seem to be related.
We focuson precision since it is important to know whethera learner is being pointed to a correct set of exam-ples or not; whether there are other possibly relevantexamples to show is less important.
In Eom et al(2012), we discuss some difficulties of aligning be-tween the two resources in the general case; whilesome senses go unaligned between the resources,this was not the case for the words used in this study.For this study, since we use pre-determined in-put texts, we also created gold-standard information,where each word in the text is manually given a linkto the appropriate COBUILD information; note thathere there is no intermediate WordNet sense to ac-count for.
This lets us gauge: a) whether the gold-standard information is helpful to learners, and b)comparatively speaking, what the effects are of us-ing the potentially noisy information provided by thefunctioning system.4 The studyWe now turn to evaluating whether this set-upof providing sense-specific lexical information canlead learners to improve their vocabulary acquisitionand their reading comprehension.4.1 Method4.1.1 ParticipantsThe participants were recruited from three univer-sities and a private institute in Seoul, Korea, giv-ing 60 participants (34 male, 26 female).
Theyranged in age from 21 to 39 (avg.=23.8) and thelength of studying English ranged from 8 to 25 years(avg.=11.32).The 40 participants from the three universitieswere taking English courses to prepare for Englishproficiency testing.
The 20 participants from theprivate institute were mostly university graduates319taking teacher training courses designed for ele-mentary English teachers.
All participants wereintermediate-level learners, scoring between 15 and21 on the reading section of the TOEFL iBT R?.
Wetargeted intermediate learners, so as to test the sys-tem with learners generally able to understand texts,yet still encounter many unknown words.The 60 participants were randomly assigned toone of four groups, with 15 participants in eachgroup.
The first three received some treatment,while the fourth was a control group:1.
Gold Senses (GS): reading with support of goldstandard sense-specific lexical information2.
System Senses (SS): reading with support ofsystem-derived sense-specific lexical informa-tion3.
All Senses (AS): reading with support of lexi-cal information of all senses of the chosen word4.
No Senses (NS): reading without any supportof lexical informationFor example, when presented with the examplein (1), if chains is clicked, the GS learners see thecorrect sense, as in (2a), along with associated ex-ample sentences (not shown).
The automatic systemhappens to be incorrect, so the SS learners see a re-lated, though incorrect, sense and examples, as in(2b).
The AS learners will see those two senses andexamples, as well as the three others for chain.
Andthe NS learners have no chance to click on a word.
(1) There?s a chance that there will be new itemsif you shop at any of the retail chains thatuse the ?fast fashion?
model of business.
(2) a.
Gold: A chain of shops, hotels, or otherbusinesses is a number of them ownedby the same person or company.b.
System: A chain of things is a group ofthem existing or arranged in a line.4.1.2 MaterialsReading texts After piloting various reading textsand drawing on the ESL teaching experience oftwo of the authors, two texts deemed appropriatefor learners at the (high-)intermediate level wereadopted: Fashion Victim (adapted from Focus onVocabulary 1: Bridging Vocabulary (Schmitt et al,Fashion Victim Sleep Researchresilient.a, chain.n,conscience.n, cradle.n,expenditure.n, mend.v,outfit.n, sector.n,unveil.valternate.a, trivial.a,deliberately.r, aspect.n,fatigue.n, obedience.n,agitate.v, banish.v,indicate.v, resist.v,trigger.vTable 1: Target words used in the study2011), 589 words) and Sleep Research (adaptedfrom The Official SAT Study Guide (The CollegeBoard, 2009), 583 words).The texts were modified to simplify their syntax,to use more ambiguous words in order to allow fora stronger test of the system, and to shorten them toabout 600 words.
The texts were placed in the onlinesystem, and all content words were made clickable.Target words A total of 20 target words (9 fromFashion Victim, 11 from Sleep Research) were se-lected by piloting a number of possible words with20 learners from a similar population and identify-ing ones which were the most unfamiliar, which alsohad multiple senses.
They appear in table 1.Reading comprehension tests For reading com-prehension, two tests were developed, each with4 multiple-choice and 6 true-false questions.
Thequestions focused on general content, and partici-pants could not refer back to the text to answer thequestions.
For the multiple-choice questions, morethan one answer could be selected, and each choicewas scored as 1 or 0 (e.g., for 5 choices, the maxi-mum score for the question was 5); for the true-falsequestions, answers were scored simply 1 or 0.
Themaximum score for a test was 21.Vocabulary tests There were one pretest and fourimmediate posttests, one of which had the same for-mat as the pretest.
The pretest and all immediateposttests had the same 30 words (20 target and 10distractor words).
Of 10 distractors, five were wordsappearing in the text (obscure.a, correlation.n, in-tervention.n, discipline.v, facilitate.v), and five weretarget words but used with a sense that was differentfrom the one used in the reading passage (deliber-ately.r, chain.n, outfit.n, mend.v, indicate.v).
Eachtest consisted of a word bank and sentences with320blanks (cf.
Kim, 2008).
For the pretest, the sentenceswere taken from other sources, whereas the posttestsentences came from the reading texts themselves.Although we used four posttests in order to testdifferent kinds of vocabulary learning (giving moreor fewer hints at meaning), we focus on one posttestin this paper, the one which matches the form of thepretest.
Each correct answer was scored as 1; incor-rect as 0.4.1.3 ProcedureThe pretest was administered two weeks beforethe actual experiment and posttests, so as to preventlearners from focusing on those words.
Participantswho knew more than 16 out of the 20 target wordswere excluded from the experiment.After reading one text, learners took a readingcomprehension test.
Then, they did the same for thesecond text.
After these two rounds, they took theseries of vocabulary posttests.4.1.4 Data analysisWe ran a variety of tests to analyze the data.2First, we ran Levene?s test of homogeneity of vari-ances, to test whether the variances of the error be-tween groups were equal at the outset of the study.This makes it clearer that the effects from the maintests are due to the variables of interest and not frominherent differences between groups (Larson-Hall,2010).Secondly, to test the first research question aboutwhether participants show better vocabulary acqui-sition with sense-specific lexical information, weused a repeated-measures analysis of variance (RMANOVA).
Time (pre/post) was the within-subjectvariable and Group (GS, SS, AS, NS) was thebetween-subject.
Post-hoc pairwise comparisonswere run in the case of significant results, to deter-mine which groups differed from each other.
Wealso examined the pre-post gain only for the targetwords which were clicked and for which we mightthus expect more improvement.Thirdly, to test the second research question aboutwhether participants improved in reading compre-hension, we used a one-way ANOVA, with readingcomprehension scores as a dependent variable and2We used SPSS,version 20.0, http://www-01.ibm.com/software/analytics/spss/Pretest PosttestMean SD Mean SDGS 10.73 (54%) 3.43 15.93 (80%) 3.96SS 10.93 (55%) 2.82 15.47 (77%) 3.80AS 10.87 (54%) 3.34 13.47 (67%) 3.83NS 10.87 (54%) 3.25 11.27 (56%) 3.39Table 3: Descriptive statistics across groups for vocabu-lary acquisition (Mean = average, SD = standard devia-tion, percentage out of all 20 answers in parentheses)the four groups as an independent variable, to ex-plore if there was any significant main effect of thegroup on reading comprehension scores.
Post-hoctests were then used, in order to determine specifi-cally which groups differed from each other.In order to gauge the effect of automatic systemerrors?distinguishing the SS (System Senses) andGS (Gold Senses) conditions?on vocabulary acqui-sition, we also examined target words where the sys-tem gave incorrect information.5 Results and Discussion5.1 Vocabulary acquisitionSince the first research question is to examine theimprovement between the pretest and the posttest,the test of homogeneity of variance was carriedout to ensure that the pretest/posttest scores of theparticipants across the four groups showed similarvariances.
Levene?s test of homogeneity of vari-ances suggested that the 4 groups could be con-sidered to have similar variances on both the pre-test (F (3, 55) = 0.49, p = 0.69) and the post-test(F (3, 56) = 0.13, p = 0.94), meaning that this as-sumption underlying the use of ANOVA was met.Looking at the descriptive statistics in table 3,none of the groups differed from each other by morethan a quarter of a point (or 1 percentage point) onthe pretest.
Thus, the groups are also comparablewith respect to their levels of performance on thepre-test.Turning to the results of the treatments in ta-ble 3, the four groups show larger differences ontheir posttest.
The GS and SS groups show the clear-est gains, suggesting greater vocabulary acquisitionthan the AS and NS groups, as expected.
If we lookat percentage gain, GS gained 26% and SS 23%,321Partial Obs.Source df df2 F p Eta2 PowerTest of Within-Subjects EffectsTime 1 56 62.67 <0.01 0.53 1.00Time*Group 3 56 7.20 <0.01 0.28 0.98Test of Between-Subjects EffectsGroup 3 56 1.71 0.18 0.08 0.42Table 2: Results of RM ANOVA comparing vocabulary test scores across the four groups over timewhile AS gained only 13% and NS 2%.In order to examine whether the above differ-ences among groups were statistically significant, arepeated-measures ANOVA was run on those pretestand posttest scores, with Group as the between-subject variable and Time as the within-subject vari-able.
The results of the RM ANOVA are presentedin table 2.With respect to the within-subject variable, the ef-fect of Time shows a statistically significant differ-ence (F (1, 56) = 62.67, p < .001, partial eta2=0.53).
In other words, not considering Group, thereis evidence of improvement from pre to posttest.Most crucially related to the first research ques-tion about whether the groups would have differentamounts of vocabulary acquisition over time, we seea significant Time*Group effect (F (3, 56) = 7.20,p < .001, partial eta2= 0.28).
The partial eta2 val-ues for Time (0.53) and Time*Group (0.28) in ta-ble 2 represent large effect sizes which thus providestrong evidence for the differences.Two sets of post-hoc comparisons were con-ducted.
The first comparisons, in table 4, show sig-nificant mean differences between the pretest andposttest for three groups (GS, SS, AS), whereas nosignificant difference is observed in the NS group,meaning that the three groups who received lexi-cal information showed improvement whereas thegroup who received no information did not.Then, a second set of post-hoc tests were run tocompare the three groups which showed significantpre-post gains (GS, SS, AS).
In table 5, the ContrastEstimate (Est.)
looks at the differences in the meanpre-post gains and shows that the GS group is sig-nificantly different from the AS group, whereas thedifference between the mean gains of the SS and ASgroups is not quite significant.
(The GS-SS contrastMean Std.Group I J Diff.
Error pGS pre post -5.20 0.80 <0.01SS pre post -4.23 0.80 <0.01AS pre post -2.60 0.80 <0.01NS pre post -0.40 0.80 0.62Table 4: Post-hoc comparisons for Time*Group, for vo-cabulary acquisitionGroupContrast Est.
Sig.GS-AS 2.60 0.02SS-AS 1.93 0.09GS-SS 0.67 0.56Table 5: Contrast results for Time*Group, where the de-pendent variable is the difference in mean pre-post gainsis non-significant.)
In other words, these post-hoccomparisons on the Time*Group interaction effectfound a significant difference between the GS andAS groups in their vocabulary learning over time,with the GS group showing greater pretest-posttestimprovement, whereas the SS?s group apparent ad-vantage over the AS group with their mean gains fellslightly short of statistical signficance.Clicked words In addition to analyzing learners?performance on the overall scores of their pretestand posttest, we examine their performance overtheir pretest and posttest only on words they clickedwhile reading, as well as how much they clicked.In the three treatments, we find: GS, 28.27 wordsclicked on average (7.00 target words); SS, 21.80(5.93); and AS, 20.87 (5.60).
Although these dif-ferences are not statistically significant, the appar-ent trend may suggest that the GS group realized322Pretest PosttestMean SD Mean SD GainGS 40% 32% 85% 22% 45%SS 25% 18% 81% 25% 56%AS 23% 25% 68% 32% 45%Table 6: Descriptive statistics for vocabulary acquisitionfor clicked words (percentage correct)they could get high-quality lexical information fromclicking words and so clicked more often.Examining only clicked target words, the test ofhomogeneity confirmed the error variance of all par-ticipants were equivalent at the outset of the study(p = 0.15).
The percentages correct of the wordsthat were clicked in the pretest and posttest are intable 6.
The pre to post gain here conveys a gen-eral trend: for the words participants clicked on, theyshowed improvement, with larger gains than for allwords (compare the best gain of 26% in table 3).As with all words, in the RM ANOVA the effectof Time shows a statistically significant difference(F (1, 42) = 96.20, p < 0.01).
However, the ef-fect of Time*Group shows no significant differencein this case (F (2, 42) = 0.60, p = 0.55).Despite non-significance, two potentially interest-ing points emerge which can be followed up on inthe future: 1) descriptively speaking, the SS groupshows the largest gain between pretest and posttest(56%); and 2) the AS group shows as much improve-ment as the GS group (45%).
This may come fromthe fact that the number of senses listed for manyclicked words was small enough (e.g., 2?3) to findan appropriate sense.
Future work could investigatea greater number of target words to verify and shedmore light on these trends.Discussion In sum, our results suggest a positiveanswer to the first research question about whethersense-specific lexical information leads learners tobetter vocabulary acquisition.
The results fromseveral different analyses suggest that: 1) learn-ers provided with lexical information during read-ing have more vocabulary acquisition, with sense-specific information having a greater increase; 2)learning gains appear to be greater for the subset ofclicked target words than for all words (though fur-ther research is needed to substantiate this); and 3)Mean SDGS 35.80 (85%) 3.98SS 37.07 (88%) 2.46AS 34.93 (83%) 3.08NS 33.27 (79%) 3.69Table 7: Descriptive statistics for reading comprehensionSource df df2 F pGroup 3 56 4.01 0.01Table 8: Results of one-way ANOVA for reading com-prehension scoresthey seem to check the meaning more when disam-biguated correctly (again needing further research).5.2 Reading comprehensionThe second research question explores whethersense-specific lexical information facilitates readingcomprehension.
The descriptive statistics for read-ing comprehension mean scores of the four groupsare in table 7.
The difference among the readingcomprehension mean scores of the four groups waswithin about 4 points, corresponding to a 9% differ-ence (SS, 88%; NS, 79%).
The GS and SS groupshave the highest values, but only small differences.In order to examine whether the above differ-ences among groups were statistically significant,a one-way ANOVA was run on reading compre-hension scores.
The test of homogeneity of vari-ances confirmed the error variances were equivalent(p = 0.42).
The results of the one-way ANOVA arein table 8.As shown, the effect of Group shows a sta-tistically significant difference, indicating that thegroups are different in their reading comprehension(F (3, 56) = 4.01, p = 0.01).
With this significantdifference in reading comprehension performance, itis necessary to locate where the differences existedamong the groups.
Tukey post0hoc tests comparedall four groups in pairs and revealed a significantdifference between the SS group and the NS group(p = 0.007), with no significant differences betweenthe other pairs.3To some extent, the results support the idea that3GS vs. SS: p = 0.68; GS vs. AS: p = 0.87; GS vs. NS:p = 0.12; SS vs. AS: p = 0.24; AS vs. NS: p = 0.46.323System Pretest Posttest AccuracyAppropriate + (16) + (14) 88% (14/16)- (42) + (32) 76% (32/42)Inappropriate + (12) + (10) 83% (10/12)- (18) + (9) 50% (9/18)Table 9: Pre/Posttest performance for SS condition,summed over learners, broken down by whether systemsense was appropriate (+ = learner got correct; - = learnergot incorrect; numbers in parentheses = actual values)sense-specific lexical information facilitates learn-ers?
reading comprehension.
Curiously, the GSgroup, which received more accurate sense infor-mation than the SS group, was not found to outper-form the control group (p = 0.12)?despite descrip-tively showing slightly higher reading comprehen-sion scores.
This issue warrants future investigation.5.3 Quality of sense informationWe have observed some differences between theGold Senses (GS) and System Senses (SS) con-ditions, but we still want to explore to what ex-tent the learners in SS group were impacted bywords which were incorrectly disambiguated.
Therewere nine words which the automatic system incor-rectly assigned senses to (inappropriate target-sensewords),4 and eleven words which it correctly as-signed.
One can see the different performance forthese two types in table 9, for words that learnersclicked on.There are two take-home points from this table.First, when learners were correct in the pretest, theygenerally did not un-learn that information, regard-less of whether they were receiving correct sense in-formation or not (88% vs. 83%).
This is important,as it seems to indicate that wrong sense informationis not leading learners astray.
However, the secondpoint is that when learners were wrong in the pretest,they were in general able to learn the sense with cor-rect information (76%), but not as effectively whengiven incorrect information (50%).
This, unsurpris-ingly, shows the value of correct sense information.4aspect.n, chain.n, conscience.n, expenditure.n, sector.n, ag-itate.v, banish.v, indicate.v, resist.v6 Summary and OutlookWe have developed a web system for displayingsense-specific information to language learners andtested it on a group of 60 ESL learners.
We showedthat sense-specific information in an intelligent read-ing system can help learners in their vocabulary ac-quisition and, to some extent, may also help withoverall reading comprehension.
We also showedpreliminary results suggesting that learners mightlearn more of the words whose definitions theycheck than words they simply encounter while read-ing.
We can also be optimistic that, while there isstill much room for improvement in presenting senseinformation automatically, errors made by the sys-tem do not seem to interfere with language learners?previously-known meanings.There are a number of avenues to pursue in the fu-ture.
One thing to note from the results was that thegroup receiving help in the form of all senses (AS)demonstrated relatively high performance in vo-cabulary acquisition and reading comprehension, attimes similar to the groups receiving sense-specificinformation (GS, SS).
This may be related to thesmall number of sense entries of the target words(average = 2.95), and a further study should be doneon target words with more sense entries, in additionto validating some of the preliminary results pre-sented in this paper regarding clicked words.
Sec-ondly, the word sense disambiguation methods andconstruction of the lexical database can be improvedto consistently provide more accurate sense infor-mation.
Finally, as mentioned earlier, there are pre-processing improvements to be made, such as im-proving the search for collocations.AcknowledgmentsWe would like to thank Stephanie Dickinson andChelsea Heaven from the Indiana Statstical Consult-ing Center (ISCC) for their assistance, as well asGraham Katz and the three anonymous reviewers fortheir useful comments.ReferencesKevin Dela Rosa and Maxine Eskenazi.
2011.
Im-pact of word sense disambiguation on orderingdictionary definitions in vocabulary learning tu-tors.
In Proceedings of FLAIRS 2011.324Soojeong Eom, Markus Dickinson, and GrahamKatz.
2012.
Using semi-experts to derive judg-ments on word sense alignment: a pilot study.
InProceedings of LREC-12.Christiane Fellbaum, editor.
1998.
WordNet: AnElectronic Lexical Database.
The MIT Press,Cambridge, MA.Peter J. M. Groot.
2000.
Computer assisted sec-ond language vocabulary acquisition.
LanguageLearning and Technology, 4(1):60?81.Michael Heilman, Kevyn Collins-Thompson, JamieCallan, and Maxine Eskenazi.
2006.
Classroomsuccess of an intelligent tutoring system for lexi-cal practice and reading comprehension.
In Pro-ceedings of the 9th International Conference onSpoken Language Processing.Adam Kilgarriff, Milos Husa?k, Katy McAdam,Michael Rundell, and Pavel Rychly?.
2008.
Gdex:Automatically finding good dictionary examplesin a corpus.
In Proceedings of EURALEX-08.Barcelona.YouJin Kim.
2008.
The role of task-induced involve-ment and learner proficiency in L2 vocabulary ac-quisition.
Language Learning, 58:285?325.Toshiko Koyama and Osamu Takeuchi.
2004.
Howlook-up frequency affects EFL learning: An em-pirical study on the use of handheld-electronicdictionaries.
In Proceedings of CLaSIC 2004,pages 1018?1024.Anagha Kulkarni, Michael Heilman, Maxine Eske-nazi, and Jamie Callan.
2008.
Word sense disam-biguation for vocabulary learning.
In Ninth Inter-national Conference on Intelligent Tutoring Sys-tems.Shari Landes, Claudia Leacock, and Randee I.Tengi.
1998.
Building semantic concordances.
InChristiane Fellbaum, editor, WordNet: an elec-tronic lexical database, chapter 8, pages 199?216.MIT.Jenifer Larson-Hall.
2010.
A guide to doing statis-tics in second language research using SPSS.Routledge, New York, NY.Baita Laufer and Monica Hill.
2000.
What lexi-cal information do L2 learners select in a CALLdictionary and how does it affect word retention?Language Learning and Technology, 3(2):58?76.Vilson J. Leffa.
1992.
Making foreign language textscomprehensible for beginners: An experimentwith an electronic glossary.
System, 20(1):63?73.S.
Luppescu and R. R. Day.
1993.
Reading, dictio-naries, and vocabulary learning.
Language Learn-ing, 43:263?287.Rada Mihalcea and Andras Csomai.
2005.
Sense-Learner: Word sense disambiguation for all wordsin unrestricted text.
In Proceedings of the ACLInteractive Poster and Demonstration Sessions,pages 53?56.
Ann Arbor, MI.John Nerbonne and Petra Smit.
1996.
GLOSSER-RuG: in support of reading.
In Proceedings ofCOLING-96.Ted Pedersen and Varada Kolhatkar.
2009.
Word-Net::SenseRelate::AllWords - a broad coverageword sense tagger that maximizes semantic relat-edness.
In Proceedings of HLT-NAACL-09.
Boul-der, CO.Caleb Prichard.
2008.
Evaluating L2 readers?
vo-cabulary strategies and dictionary use.
Reading ina Foreign Language, 20(2):216?231.Diane Schmitt, Norbert Schmitt, and David Mann.2011.
Focus on Vocabulary 1: Bridging Vocabu-lary.
Pearson ESL, second edition.Thomas Segler, Helen Pain, and Antonella So-race.
2002.
Second language vocabulary acqui-sition and learning strategies in ICALL environ-ments.
Computer Assisted Language Learning,15(4):409?422.John Sinclair, editor.
2006.
Collins COBUILDAdvanced Lerner?s English Dictionary.
HarperCollins.The College Board.
2009.
The Official SAT StudyGuide.
College Board, second edition.Kristina Toutanova, Dan Klein, Christopher D.Manning, and Yoram Singer.
2003.
Feature-richpart-of-speech tagging with a cyclic dependencynetwork.
In Proceedings of HLT-NAACL 2003,pages 252?259.325
