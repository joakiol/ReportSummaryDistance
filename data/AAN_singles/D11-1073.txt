Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 793?803,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsA Cascaded Classification Approach to Semantic Head RecognitionLukas Michelbacher Alok Kothari Martin Forst?Christina Lioma Hinrich Schu?tzeInstitute for NLPUniversity of Stuttgart{michells,kotharak,liomaca}@ims.uni-stuttgart.de?Microsoftmartin.forst@microsoft.comAbstractMost NLP systems use tokenization as partof preprocessing.
Generally, tokenizers arebased on simple heuristics and do not recog-nize multi-word units (MWUs) like hot dogor black hole unless a precompiled list ofMWUs is available.
In this paper, we proposea new cascaded model for detecting MWUsof arbitrary length for tokenization, focusingon noun phrases in the physics domain.
Weadopt a classification approach because ?
un-like other work on MWUs ?
tokenization re-quires a completely automatic approach.
Weachieve an accuracy of 68% for recognizingnon-compositional MWUs and show that ourMWU recognizer improves retrieval perfor-mance when used as part of an information re-trieval system.1 IntroductionMost NLP systems use tokenization as part of pre-processing.
Generally, tokenizers are based on sim-ple heuristics and do not recognize multi-word units(MWUs) like hot dog or black hole.
Our long-termgoal is to build MWU-aware tokenizers that are usedas part of the standard toolkit for NLP preprocessingalongside part-of-speech and named-entity tagging.We define an MWU as a sequence of words thathas properties that cannot be inferred from the com-ponent words (cf.
e.g.
Manning and Schu?tze (1999,Ch.
5), Sag et al (2002)).
The most importantof these properties is non-compositionality, the factthat the meaning of a phrase cannot be predictedfrom the meanings of its component words.
For ex-ample, a hot dog is not a hot animal but a sausage ina bun and a black hole in astrophysics is a region ofspace with special properties, not a dark cavity.The correct recognition of MWUs is an importantbuilding block of many NLP tasks.
For example, ininformation retrieval (IR) the query hot dog shouldnot retrieve documents that only contain the wordshot and dog individually, outside of the phrase hotdog.In this study, we focus on noun phrases in thephysics domain.
For specialized domains such asphysics, adaptable and reliable MWU recognitionis of particular importance because comprehensiveand up-to-date lists of MWUs are not availableand would have to be created by hand.
We chosenoun phrases because domain-specific terminologyis commonly encoded in noun phrase MWUs; othertypes of phrases ?
e.g., verb constructions ?
rarelygive rise to fixed domain-specific multi-word se-quences that should be treated as a unit.We cast the task of MWU tokenization as seman-tic head recognition in this paper.
The importance ofsyntactic heads for many NLP tasks is generally ac-cepted.
For example, in coreference resolution iden-tity of syntactic heads is predictive of coreference;in parse disambiguation, the syntactic head of a nounphrase is a powerful feature for resolving attachmentambiguities.
However, in all of these cases, the syn-tactic head is only an approximation of the informa-tion that is really needed; the underlying assumptionmade when using the syntactic head as a substitutefor the entire phrase is that the syntactic head is rep-resentative of the phrase.
This is not the case whenthe phrase is non-compositional.We define the semantic head of a noun phrase asthe non-compositional part of a phrase.
Semanticheads would serve most NLP tasks better than syn-tactic heads.
For example, a coreference resolutionsystem is misled if it looks at syntactic heads to de-793termine possible coreference of a hot dog .
.
.
the dogin I first ate a hot dog and then fed the dog.
This isnot the case for a system that makes the decisionbased on the semantic heads hot dog of a hot dogand dog of the dog.The specific NLP application we evaluate in thispaper is information retrieval.
We will show that se-mantic head recognition improves the performanceof an information retrieval system.We introduce a cascaded classification frameworkfor recognizing semantic heads that allows us to treatnoun phrases of arbitrary length.
We use a numberof previously proposed features for recognizing non-compositionality and semantic heads.
In addition,we compare three features that measure contextualsimilarity.Our main contributions in this paper are as fol-lows.
First, we introduce the notion of semantichead, in analogy to syntactic head, and propose se-mantic head recognition as a new component of NLPpreprocessing.
Second, we develop a cascaded clas-sification framework for semantic head recognition.Third, we investigate the utility of contextual simi-larity for detecting non-compositionality and showthat it significantly enhances a baseline semantichead recognizer.
However, we also identify a num-ber of challenges of using contextual similarity inhigh-confidence semantic head recognition.
Fourth,we show that our approach to semantic head recog-nition improves the performance of an IR system.Section 2 discusses previous work.
In Section 3we introduce semantic heads and present our cas-caded model for semantic head recognition.
In Sec-tion 4, we describe our data and three different mea-sures of contextual similarity.
Section 5 introducesthe classifier and its features.
Section 6 presentsclassification results and discussion.
Section 7 de-scribes the information retrieval experiments.
InSection 8 we present our conclusions.2 Related WorkWhile there is a large number of publications onMWUs and collocation extraction, the general prob-lem of automatic MWU detection for the specificpurpose of tokenization has not been investigatedbefore to our knowledge.The classic approach to identifying collocationsand MWUs is to apply statistical association mea-sures (AMs) to n-grams extracted from a corpus?
often combined with various linguistic heuris-tics and other filters, resulting in candidate lists.Choueka (1988) and the XTRACT system (Smadja,1993) are well-known examples of this approach.More recent approaches such as Pecina (2010)and Ramisch et al (2010) combine classifiers withassociation measures.
Although our approach isclassification-based as well, our data set has a morerealistic size than Pecina (2010)?s (1 billion wordsvs 1.5 million words) and we work on noun phrasesof arbitrary length (instead of just bigrams).
Themwetoolkit1 by Ramisch et al (2010) aims tobe a software package for lexicographers and itsfeatures are limited to a small set of associationmeasures that do not consider marginal frequencies.Neither of these two studies includes evaluation inthe context of an application.Lin (1999) defines a decision criterion for non-compositional phrases based on the change in themutual information of a phrase when substitutingone word for a similar one based on an automaticallyconstructed thesaurus.
The method reaches 15.7%precision and 13.7% recall.In terms of the extraction of domain-specificMWUs, cross-language methods have been pro-posed that make use of the fact that an MWU in onelanguage might be expressed as a single word in an-other.
Caseli et al (2009) utilize word alignmentsin a parallel corpus; Attia et al (2010) exploit thelinks between article names of different-languageWikipedias to search for many-to-one translations.We did not pursue a cross-language approach be-cause we strive for a self-contained method of MWUrecognition that operates on a single textual re-source.Non-compositionality and distributional se-mantics.
In recent years, a number of studies haveinvestigated the relationship between distributionalsemantics and non-compositionality.
These studiescompute the similarity between words and phrasesrepresented as semantic vectors in a word spacemodel.
A semantic vector of a word is the accumu-lation of the particular contexts in which the word1http://sourceforge.net/projects/mwetoolkit/794appears.
The underlying idea is similar to Lin?s:the meaning of a non-compositional phrase some-how deviates from what one would expect given thesemantic vectors of parts of the phrase.
The stan-dard measure to compare semantic vectors is cosinesimilarity.
The questions that arise are (i) whichvectors to compare, (ii) how to combine the vectorsof the parts and (iii) from what point on a certaindissimilarity indicates non-compositionality.
To ourknowledge, there are no generally accepted answersto these questions.Regarding (i), Schone and Jurafsky (2001) com-pare the semantic vector of a phrase p and the vec-tors of its component words in two ways: one in-cludes the contexts of p in the construction of thesemantic vectors of the parts and one does not.
Re-garding (ii), they suggest weighted or unweightedsums of the semantic vectors of the parts.Baldwin et al (2003) investigate semantic decom-posability of noun-noun compounds and verb con-structions.
They address (i) by comparing the se-mantic vectors of phrases with the vectors of theirparts individually to detect meaning changes; e.g.,they compare vice president to vice and president.We propose a new method that compares phraseswith their alternative phrases, in the spirit of Lin(1999)?s substitution approach (see Section 4.3).Our rationale is that context features should bebased on contexts that are syntactically similar to thephrase in question.With respect to (iii), the above-mentioned studiesuse ad hoc thresholds to separate compositional andnon-compositional phrases but do not offer a princi-pled decision criterion.2 In contrast, we train a sta-tistical classifier to learn a decision criterion.There is a larger body of work concerning non-compositionality which revolves around the prob-lem of literal (compositional) vs. non-literal (non-compositional) usage of idiomatic verb construc-tions like to break the ice or to spill the beans.Some studies approach the problem with semanticvector comparisons in the style of Schone and Ju-rafsky (2001), e.g Katz and Giesbrecht (2006) andCook et al (2007).
Other approaches use word-alignment (e.g.
Moiro?n and Tiedemann (2006)) or2Lin (1999) uses a well-defined criterion but his approach isnot based on vector similarity.a combination of heuristic and linguistic features(e.g.
Diab and Bhutada (2009), Li and Sporleder(2010)).
Even though there is some methodologi-cal overlap between our approach and some of theverb-oriented studies, we believe that verb construc-tions have properties that are quite different fromnoun phrases.
For example, our definition of alter-native vector relies on the fact that most noun phraseMWUs are fixed and exhibit no syntactic variability.In contrast, verb constructions are often discontinu-ous.The motivation for most work on MWU detec-tion is lexicography, terminology extraction or thecreation of machine-readable dictionaries.
Our mo-tivation ?
tokenization in a preprocessing setting ?
isdifferent from this earlier work.3 Semantic Heads and Cascaded ModelWe cast the task of MWU tokenization as seman-tic head recognition in this paper.
We define thesemantic head of a noun phrase as the largest non-compositional part of the phrase that contains thesyntactic head.
For example, black hole is the se-mantic head of unusual black hole and afterglow isthe semantic head of bright optical afterglow; in thelatter case syntactic and semantic heads coincide.Semantic heads would serve most NLP tasks bet-ter than syntactic heads.
The attachment ambiguityof the last noun phrase in he bought the hot dogs in apacket can be easily resolved for the semantic headhot dogs (food is often in a packet), but not as easilyfor the syntactic head dogs (dogs are usually not inpackets).
Indeed, we will show in Section 7 that se-mantic head recognition improves the performanceof an IR system.The semantic head is either a single noun or a non-compositional noun phrase.
In the latter case, themodifier(s) introduce(s) a non-compositional, un-predictable shift of meaning; hot shifts the mean-ing of dog from live animal to food.
In contrast,the compositional meaning shift caused by smallin small dog is transparent.
The semantic head al-ways contains the syntactic head; for compositionalphrases, syntactic head and semantic head are iden-tical.To determine the semantic head of a phrase, weuse a cascaded classification approach.
The cascade795(1) neutron star(2) unusual black hole(3) bright optical afterglow(4) small moment of inertiaFigure 1: Example phrases with modifiers.
Peripheralelements are set in italics, syntactic heads in bold.comes into play in all aspects of our study: the rat-ing experiments with human subjects, data extrac-tion, feature design and classification itself.We need a cascade because we want to recog-nize the semantic head in noun phrases of arbitrarylength.
The starting point is a phrase of length n:p = w1 .
.
.
wn.
We distinguish between the syntac-tic head of a phrase and the remaining words, themodifiers.
Figure 1 shows phrases of varying syn-tactic complexity.
The syntactic head is marked inbold.
The model accommodates pre-nominal modi-fiers as in examples (1) through (3) and post-nominalmodifiers like PPs in example (4).Among the modifiers, there is a distinguished ele-ment, the peripheral element u (italicized in the ex-amples).
The remaining words are called the restv.
We can now represent any phrase p as p = uv.3The element u is always the outermost modifier.
of -PPs are treated as a single modifier and they takeprecedence over pre-nominal modification becausethis analysis is dominant in our gold standard data.This means that in the phrase small moment of iner-tia, small (and not of inertia) is the peripheral ele-ment u.Cascaded classification then operates as shown inFigure 2.
In each iteration, the classifier decideswhether the relation between the current peripheralelement u and the rest v is compositional (C) or non-compositional (NC).
If the relation is NC, process-ing stops and uv is returned as the semantic headof p. If the relation is compositional, u is discardedand classification continues with v as the new inputphrase, which again is represented in the form u?v?.In case there is no more peripheral element u, i.e.the new v is a single word, it is returned as the se-mantic head of p.Table 1 shows two examples.
For the fully com-positional phrase bright optical afterglow, the pro-3We use the abstract representation p = uv even though ucan appear after v in the surface form of p.function recognize semantic head(p)u?
peripheral(p)v ?
rest(p)while decision(u, v) 6= NC dou?
peripheral(v)if u = ?
thenreturn vv ?
rest(v)return uvFigure 2: Cascaded classification of pstep u v decision1 bright optical afterglow C2 optical afterglow C3 ?
afterglow1 small moment of inertia C2 of inertia moment NCTable 1: Cascaded decision processescess runs all the way down to the syntactic head af-terglow which is also the semantic head.
In the sec-ond case, the process stops earlier, in step 2, becausethe classifier finds that the relation between momentand of inertia is NC.
This means that the semantichead of small moment of inertia is moment of iner-tia.4 Corpus and Feature Definitions4.1 Candidate phrasesAs our corpus, we use the iSearch collection, aone billion word collection of documents from thephysics domain (Lykke et al, 2010).
We tokenizedthe collection by splitting on white space and addingsentence boundaries and part-of-speech tags to theoutput.
With part-of-speech information, the iden-tification of MWU candidates is easy, fast and reli-able.We extracted all noun phrases from the collectionthat consist of a head noun with up to four modifiers?
almost all domain-specific terminology in our col-lection is captured by this pattern.
The pre-nominalmodifiers can be nouns, proper nouns, adjectives orcardinal numbers.The baseline accuracy of a classifier that alwayschooses compositionality is very high (> 90%) for796V = v V 6= vU = u O11 O12 = R1U 6= u O21 O22 = R2= C1 = C2 = NTable 2: 2-by-2 contingency tables with observed andmarginal frequenciesphrases of the type [noun] of the/a [noun] (sg.)(e.g.
rest of the paper) and [noun] of [noun] (pl.)(e.g.
series of papers).
We therefore restrict post-nominal modifiers to prepositional phrases with theword of followed by a non-modified, indefinite, sin-gular noun, e.g., speed of light or moment of inertia.Out of all phrases extracted with part-of-speechpatterns, we keep only the ones that appear more of-ten than 50 times because it is hard to compute re-liable features for less frequent phrases.
All experi-ments were carried out with lemmatized word forms.We refer to lemmas as words if not noted otherwise.4.2 Association measuresStatistical association measures are frequently usedfor MWU detection and collocation extraction (e.g.Schone and Jurafsky (2001), Evert and Krenn(2001), Pecina (2010)).We use all measures used by Schone and Jurafsky(2001) that can be derived from a phrase?s contin-gency table.
These measures are Student?s t-score,z-score, ?2, pointwise mutual information (MI),Dice coefficient, frequency, log-likelihood (G2) andsymmetric conditional probability.We define the AMs in Table 3 based on the no-tation for the contingency table shown in Table 2(cf.
Evert (2004)).
Oij is observed frequency andEij = RiCjN expected frequency.The AMs are designed to deal with two randomvariables U and V that traditionally represent singlewords.
In our model, we use U to represent periph-eral elements u and V for rests v.association measure formulastudent?s t-score (amt) O11?E11?O11z-score (amz) O11?E11?E11chi-square (am?2)?i,j(Oij?Eij)2Eijpointwise mutual infor-mation (amMI ) logO11E11Dice coefficient (amD) 2O11R1+C1frequency (amf ) O11log-likelihood (amG2) 2?i,jOij log OijEijsymmetric conditionalprobability (amscp)O112R1C1Table 3: Association measures4.3 Word space modelAs our baseline, we use two methods of compar-ing semantic vectors: sj1 and sj2, both introducedby Schone and Jurafsky (2001).
They experimentedwith variants of sj1 and sj2, but found no large differ-ences.
In addition, we introduce our own approachalt.Method sj1 compares the semantic vector of aphrase p with the sum of the vectors of its parts.Method sj2 is like sj1, except the contexts of p arenot part of the semantic vectors of the parts.
Methodalt compares the semantic vector of a phrase with itsalternative vector.
In the definitions below, s repre-sents a vector similarity measure, w(p) a general se-mantic vector of a phrase p and w?
(wi) the semanticvector of a partwi of a phrase p that does not includethe contexts of occurrences of wi that were part of pitself.sj1 s(w(black hole), w(black) + w(hole))sj2 s(w(black hole), w?
(black) + w?
(hole))alt s(w(black hole),?uw(u, hole)); u 6= blackFor the third comparison, we build the alternativevector as follows.
For a phrase p = uv with pe-ripheral element u and rest v, we call the phrase797p?
= u?v an alternative phrase if the rest v is thesame and u?
6= u.
E.g., giant star is an alternativephrase of neutron star and isolated neutron star isan alternative of young neutron star.
The alterna-tive vector of p is then the semantic vector that iscomputed from the contexts of all of p?s alternativephrases.
The alternative vector is a representationof the contexts of v except for those modified by u.This technique bears resemblance to the substitutionapproach of Lin (1999).
The difference is that herelies on a similarity thesaurus for substitution andmonitors the change in mutual information for eachsubstitution individually whereas we substitute withgeneral alternative modifiers and combine the alter-native contexts into one vector for comparison.Previous work has compared the semantic vectorof a phrase with the vectors of its components.
Ourapproach is more ?head-centric?
and only comparesphrases in the same syntactic configuration.
Ourquestion is: Is the typical context of the head holeif it occurs with a modifier that is not black differentfrom when it occurs with the modifier black?We used a bag-of-words model and a window of?10 words for contexts to create semantic vectors.We only kept the content words in the window whichwe defined as words that are tagged as either a noun,verb, adjective or adverb.
To add information aboutthe variability of syntactic contexts in which phrasesoccur, we add the words immediately before and af-ter the phrase with positional markers (?1 and +1,respectively) to the vector.
These words were notsubject to the content-word filter.
The dimension-ality of the vectors is then 3V where V is the sizeof the vocabulary: V dimensions each for bag-of-words, left and right syntactic contexts.
We did notinclude vectors for the stop word of for sj1 and sj2.4.4 Non-compositionality judgmentsSince the domain of the corpus is physics, highlyspecialized vocabulary had to be judged.
We em-ployed domain experts as raters (one engineeringand two physics graduate students).In line with the cascaded model, the raters whereasked to identify the semantic head of each candi-date phrase.
If at least two raters agreed on a seman-tic head of a phrase we made this choice the seman-tic head in the gold standard.
The final gold standardcomprises 1560 phrases.We computed raw agreement of each rater withthe gold standard as the percentage of correctly rec-ognized semantic heads ?
this is the task that theclassifier addresses.
Agreement is quite high at86.5%, 88.3% and 88.5% for the three raters.
Inaddition, we calculated chance-corrected agreementwith Cohen?s ?
on the first decision task against thegold standard (see Section 6).
As expected, agree-ment decreases, but is still substantial at 74.0%,78.2% and 71.8% for the three raters.5 ClassifierWe use the Stanford maximum entropy classifier forour experiment.4 We randomly split the data into atraining set of 1300 and a held-out test set of 260pairs.We use the eight AMs and the cosine similari-ties simsj1, simsj2 and simalt described in Sec-tion 4.3 as features for the classifier.
Cosine similar-ity should be small if a phrase is non-compositionaland large if it is compositional.
In other words, if thecontexts of the candidate phrase are too dissimilar tothe contexts of the sum of its parts or to the alterna-tive phrases, then we suspect non-compositionality.Feature values are binned into 5 bins.
We ap-plied a log transformation to the four AMs with largevalues: amf , amG2 , am?2 and amz .
For our ap-plication there is little difference between statisticalsignificance at p < .001 and p < .00001.
Thelog transformation reduces the large gap in magni-tude between high significance and very high signif-icance.
If co-occurrence of u and v in uv is belowchance, then we set the association scores to 0 sincethis is an indication of compositionality (even if it ishighly significant).Since AMs have been shown to be correlated (e.g.Pecina (2010)), we first perform feature selection onthe AM features.
We tested accuracy of all 2r ?
1non-empty combinations of the r = 8 AM featureson the task of deciding whether the first decisionduring the classification of a phrase was C or NC.We then selected those AM features that were partof at least one top 10 result in each fold.
Those fea-tures were amt, amf and amscp.The main experiment combines these three se-4http://nlp.stanford.edu/software/classifier.shtml798lected AM features with all possible subsets of con-text features.
We train on the 1300-element trainingset and test on the 260-element test set.6 Results and DiscussionWe ran three evaluation modes: dec-1st, dec-all, andsemh.
Mode dec-1st only evaluates the first deci-sion for each phrase; the baseline in this case is .554since 55.4% of the first decisions are C. In modedec-all, we evaluate all decisions that were made inthe course of recognizing the semantic head.
Thismode emphasizes the correct recognition of seman-tic heads in phrases where multiple correct decisionsin a row are necessary.
We define the confidencefor multi-decision classification as the product ofthe confidence values of all intermediate decisions.There is no obvious baseline for dec-all because thenumber of decisions depends on the classifier ?
aclassifier whose first decision on a four-word phraseis NC makes one decision, another one may makethree.
The mode semh evaluates how many semanticheads were recognized correctly.
This mode directlyevaluates the task of semantic head recognition.
Thebaseline for semh is the tokenizer that always returnsthe syntactic head; this baseline is .488.5 Table 4shows 8?
3 runs, corresponding to the three modestested on the AM features (amt, amf , and amscp)and the eight possible subsets of the three contextfeatures.For all modes, the best result is achieved with baseAMs combined with the simalt feature; the accura-cies are .692, .703 and .680.
The improvements overthe baselines (for dec-1st and semh) are statisticallysignificant at p < .01 (binomial test, n = 260).For semh, accuracy without any context featuresis .603; this is significantly better than the .488 base-line (p < .01).
Performance with only the base AMfeatures is significantly lower than the best contextfeature experiment (.680) at p < .01 and signifi-cantly lower than the worst context feature exper-iment (.653) at p < .1.
However, the differencesbetween the context feature runs are not significant.When the semantic head recognizer processes aphrase, there are four possible results.
Result rsemh:5The baseline could be improved with simple heuristics, e.g.
?uv contains capital letter??
NC.
However, this feature onlyresults in a 2% improvement compared to the baseline.type freq definitionrsemh 92 sem.
head correct (6= synt.
head)rsynth 85 sem.
head correct (= synt.
head)r+ 48 sem.
head too longr?
35 sem.
head too shortall 260Table 5: Distribution of result typesthe semantic head is correctly recognized and it isdistinct from the syntactic head.
Result rsynth: thesemantic head is correctly recognized and it is iden-tical to the syntactic head.
Result r+: the semantichead is not correctly recognized because the cascadewas stopped too early, i.e., a compositional modifierthat should have been removed was kept.
Result r?
:the semantic head is not correctly recognized be-cause the cascade was stopped too late, i.e., a modi-fier causing a non-compositional meaning shift wasremoved.
Table 5 shows the distribution of resulttypes.
It shows that r+ is the more common error:the classifier more often regards compositional rela-tions as non-compositional than vice versa.Table 6 shows the top 20 classifications wherethe semantic head was not the same as the syntac-tic head sorted by confidence in descending order.In the third column ?phrase .
.
.
?
we list the candi-dates with semantic heads in bold.
The columns tothe right show the predicted semantic head and thefeature values.
All five errors in the list are of typer+.Two r+ phrases are schematic view and many oth-ers.
The two phrases are clearly compositional andthe classifier failed even though the context featurepoints in the direction of compositionality with avalue greater than .5.
It can be argued that many oth-ers is a trivial example that does not require complexmachinery to be identified as compositional, e.g.
byusing a stop list.
We included it in the analysis sincewe want to be able to process arbitrary phrases with-out additional hand-crafted resources.Another incorrect classification occurs with thephrase massive star birth6 for which star birth wasannotated as the semantic head.
Here we have a casewhere the peripheral element massive does not mod-6i.e.
the birth of a massive star, a certain type of star withvery high mass799mode baseline context feature context feature subsetssimalt - ?
?
?
?
- - -simsj1 - - ?
?
?
?
- ?simsj2 - - - ?
?
- ?
?dec-1st .554 .604 .692 .669 .685 .677 .654 .654 .662dec-all - .615 .703 .681 .696 .688 .666 .669 .675semh .488 .603 .680 .657 .673 .665 .653 .653 .661Table 4: Performance for base AM features plus context feature subsets.
A ???
indicates the use of the correspondingcontext feature.ify the syntactic head birth but massive star is itselfa complex modifier.
In the test set, 5% of the phrasesexhibit structural ambiguities of this type.
Our sys-tem cannot currently deal with this phenomenon.The remaining r+ phrases are peculiar velocityand local group.
However, Wikipedia lists bothphrases with an individual entry defining the formeras the true velocity of an object, relative to a restframe7 and the latter as the group of galaxies thatincludes Earth?s galaxy, the Milky Way8.
Both def-initions provide evidence for non-compositionalitysince the velocity is not peculiar (as in strange) andthe scope of local is not clear without further knowl-edge.
Arguably, in these cases our method chose ajustifiable semantic head, but the raters disagreed.9For NLP preprocessing, it is acceptable to sacri-fice recall and only make high-confidence decisionson semantic heads.
A tokenizer that reliably detectsa subset of MWUs is better than one that recognizesnone.
However, our attempts to use the simalt rec-ognizer (bold in Table 4) in this way were not suc-cessful.
Precision is .680 for confidence > .7 anddoes not exceed .770 for higher confidence values.To understand this effect, we analyzed the distri-bution of simalt scores.
Surprisingly, moderate sim-ilarity between .4 and .6 is a more reliable indicatorfor NC than low similarity < .3.
Our intuition forusing distributional semantics in Section 2 was thatlow similarity indicates non-compositionality.
This7http://en.wikipedia.org/wiki/Peculiar_velocity8http://en.wikipedia.org/wiki/Local_group9Further evidence that local group is non-compositional isthe fact that one of the domain experts annotated the phrase asnon-compositional but was overruled by the other two.does not seem to hold for the lowest similarity val-ues possibly because they are often extreme casesin terms of distribution and frequency and then giverise to unreliable decisions.
This means that the con-text features enhance the overall performance of theclassifier, but they are unreliable and do not supportthe high-confidence decisions we need in NLP pre-processing.For comparison, the classifier that only uses AMfeatures achieves 90% precision at 14% recall withconfidence > .7 ?
although it has lower overall ac-curacy than the simalt recognizer.
We are still inthe process of analyzing these results and decided touse the AM-only recognizer for the IR experimentbecause it has more predictable performance.In summary, the results show that, for the recogni-tion of semantic heads, basic AMs offer a significantimprovement over the baseline.
We have shown thatsome wrong decisions are defensible even thoughthe gold standard data suggests otherwise.
Contextfeatures further increase performance significantly,but surprisingly, they are not of clear benefit fora high-confidence classifier that is targeted towardsrecognizing a smaller subset of semantic heads withhigh confidence.7 Information Retrieval ExperimentTypically, IR systems do not process non-compositional phrases as one semantic entity,missing out on potentially important informationcaptured by non-compositionality.
This sectionillustrates one way of adjusting the retrieval processso that non-compositional phrases are processed assemantic entities that may enhance retrieval perfor-mance.
The underlying hypothesis is that, given800c.
type phrase (semantic head in bold) predicted semantic head amt amf amcp simalt.99 rsemh ellipsoidal figure of equilibrium ellipsoidal figure of equilibrium 18.03 325 6.23e-01 .219.99 rsemh point spread function point spread function 95.03 9056 2.33e-01 .529.99 r+ massive star birth massive star birth 19.99 402 4.81e-03 .134.98 rsemh high angular resolution imaging high angular resolution imaging 13.07 179 1.27e-03 .173.98 rsemh integral field spectrograph integral field spectrograph 24.20 586 4.12e-02 .279.98 r+ local group local group 153.54 24759 8.73e-03 .650.98 rsemh neutral kaon system neutral kaon system 1.38 108 4.17e-03 .171.97 rsemh IRAF task IRAF task 49.07 2411 2.96e-02 .517.92 rsemh easy axis easy axis 44.66 2019 2.79e-03 .599.89 r+ schematic view schematic view 40.56 1651 8.06e-03 .612.87 rsemh differential resistance differential resistance 31.71 1034 6.38e-04 .548.86 rsemh TiO band TiO band 36.84 1372 2.21e-03 .581.86 r+ many others many others 97.76 9806 6.54e-03 .708.86 rsemh VLBA observation VLBA observation 43.95 2004 9.35e-04 .648.85 r+ peculiar velocity peculiar velocity 167.63 28689 2.37e-02 .800.84 rsemh computation time computation time 43.80 1967 1.35e-03 .657.83 rsemh Land factor Land factor 21.15 453 6.30e-04 .360.83 rsemh interference filter interference filter 31.44 1002 1.27e-03 .574.83 rsemh line formation calculations line formation calculations 14.20 203 1.96e-03 .381.82 rsemh Wess-Zumino-Witten term Wess-Zumino-Witten term 9.60 94 8.12e-05 .291Table 6: The 20 most confident classifications where the prediction is semantic head 6= syntactic head.
?c.?
= confi-dencea query that contains a non-compositional phrase,boosting the retrieval weight of documents thatcontain this phrase will improve overall retrievalperformance.We do this boosting using Indri?s10 combinationof the language modeling and inference networkapproaches (Metzler and Croft, 2004), which al-lows assigning different degrees of belief to differ-ent parts of the query.
This belief can be drawn fromany suitable external evidence of relevance.
In ourcase, this source of evidence is the knowledge thatcertain query terms constitute a non-compositionalphrase.
Under this approach, and using the #weightand #combine operators for combining beliefs, therelevance of a documentD to a queryQ is computedas the probability that D generates Q, P (Q|D):P (Q|D) =?t?QP (t|D)wtW (W =?t?Qwt) (1)where t is a term and wt is the belief weight as-signed to t. The higher wt is, the higher the rankof documents containing t. In this work, we dis-10http://www.lemurproject.org/tinguish between two types of query terms: termsoccurring in non-compositional phrases (Qnc), andthe remaining query terms (Qc).
Terms t ?
Qncreceive belief weight wnc and terms t ?
Qc beliefweight wc, (wnc + wc = 1 and wnc, wc ?
[0, 1]).To boost the ranking of documents containing non-compositional phrases, we increase wnc at the ex-pense of wc.
We estimate P (t|D) in Eq.
1 usingDirichlet smoothing (Zhai and Lafferty, 2002).We use Indri for indexing and retrieval withoutremoving stopwords or stemming.
This choice ismotivated by two reasons: (i) We do not have adomain-specific stopword list or stemmer.
(ii) Base-line performance is higher when keeping stopwordsand without stemming, rather than without stop-words and with stemming.We use the iSearch collection discussed in Sec-tion 4.
It comprises 453,254 documents and aset of 65 queries with relevance assessments.
Tomatch documents to queries without any treat-ment of non-compositionality (baseline run), weuse the Kullback-Leibler language model withDirichlet smoothing (KL-Dir) (Zhai and Lafferty,2002).
We applied the preprocessing described801run MAP REC P20baseline 0.0663 770 0.1385real NC 0.0718 844 0.1538pseudo NC1 0.0664 788 0.1385pseudo NC2 0.0658 782 0.1462pseudo NC3 0.0671 777 0.1477pseudo NC4 0.0681 807 0.1462pseudo NC5 0.0670 783 0.1423Table 7: IR performance without considering non-compositionality (baseline), versus boosting real andpseudo non-compositionality (real NC, pseudo NCi).in Section 4 to the queries and identified non-compositional phrases with the base AM classifierfrom Section 5.
Our approach for boosting theweight of these non-compositional phrases usesthe same retrieval model enhanced with beliefweights as described in Eq.
1 (real NC run).
Inaddition, we include five runs that boost the weightof pseudo non-compositional phrases that werecreated randomly from the query text (pseudo NCruns).
These pseudo non-compositional phraseshave exactly the same length as the observed non-compositional phrases for each query.
We measureretrieval performance in terms of mean averageprecision (MAP), precision at 20 (P20), and recall(REC, number of relevant documents retrieved?
total is 2878).
For each evaluation measureseparately, we tune the following parameters andreport the best performance: (i) the smoothingparameter ?
of the KL-Dir retrieval model (?
?
{100, 500, 800, 1000, 2000, 3000, 4000, 5000, 8000,10000}, following Zhai and Lafferty (2002)); (ii)the belief weights wnc, wc ?
{0.1, .
.
.
, 0.9} in stepsof 0.1 while preserving wnc + wc = 1 at all times.Table 7 displays retrieval performance of ourapproach against the baseline and five runs withpseudo non-compositional phrases.
We see a 9.61%improvement in the number of relevant retrieveddocuments over the baseline.
MAP and P20 alsoshow improvements.
Our approach is better thanany of the 5 random runs on all three metrics ?
theprobability of getting such a good result by chanceis 125 < .05, and thus the improvements are statis-tically significant.
On doing a query-wise analysisof MAP scores, we find that large improvementsover the baseline occur when a non-compositionalphrase aligns with what the user is looking for.
Thesystem seems to retrieve more relevant documentsin that case.
E.g., the improvement in MAP is0.0977 for query #19.
The user was looking for?articles .
.
.
on making tunable vertical cavity sur-face emitting laser diodes?
and laser diodes wasone of the non-compositional phrases recognized.On the other hand, a decrease in MAP occurs fornon-compositional phrases unrelated to the infor-mation need.
In query #4 the user is looking for?protein-protein interaction, the surface charge dis-tribution of these proteins and how this has been in-vestigated with Electrostatic Force Microscopy?
andthough non-compositional phrases such as Force Mi-croscopy are recognized, these do not reflect the coreinformation need ?The proteins of interest are theAvidin-Biotin and IgG-anti-IgG systems?.8 ConclusionWe have presented an approach to improving to-kenization in NLP preprocessing that is based onthe notion of semantic head.
Semantic heads are?
in analogy to syntactic heads ?
the core meaningunits of phrases that cannot be further semanticallydecomposed.
To perform semantic head recogni-tion for tokenization, we defined a novel cascadedmodel and implemented it as a statistical classifierthat used previously proposed and new context fea-tures.
We have shown that the classifier significantlyoutperforms the baseline and that context featuresincrease performance.
We reached an accuracy of68% and argued that even a semantic head recog-nizer restricted to high-confidence decisions is use-ful ?
because reliably recognizing a subset of se-mantic heads is better than recognizing none.
Weshowed that context features increase the accuracyof the classifier, but undermine the confidence as-sessments of the classifier, a result we are still ana-lyzing.
Finally, we showed that even in its prelim-inary current form the semantic head recognizer isable to improve the performance of an IR system.AcknowledgmentsThis work was funded by DFG projects SFB 732 andWordGraph.
We also thank the anonymous review-ers for their comments.802ReferencesMohammed Attia, Antonio Toral, Lamia Tounsi, PavelPecina, and Josef van Genabith.
2010.
Automatic ex-traction of arabic multiword expressions.
In Proceed-ings of the 2010 Workshop on Multiword Expressions,pages 19?27, Beijing, China.
Coling 2010 OrganizingCommittee.Timothy Baldwin, Colin Bannard, Takaaki Tanaka, andDominic Widdows.
2003.
An empirical model ofmultiword expression decomposability.
In Proceed-ings of the ACL 2003 Workshop on Multiword Expres-sions, pages 89?96, Sapporo, Japan.
Association forComputational Linguistics.Helena Caseli, Aline Villavicencio, Andre?
Machado,and Maria Jose?
Finatto.
2009.
Statistically-drivenalignment-based multiword expression identificationfor technical domains.
In Proceedings of the 2009Workshop on Multiword Expressions, pages 1?8, Sin-gapore.
Association for Computational Linguistics.Yaacov Choueka.
1988.
Looking for needles in ahaystack.
In Proceedings of RIAO88, pages 609?623.Paul Cook, Afsaneh Fazly, and Suzanne Stevenson.2007.
Pulling their weight: Exploiting syntactic formsfor the automatic identification of idiomatic expres-sions in context.
In Proceedings of the 2007 on Mul-tiword Expressions, pages 41?48, Prague, Czech Re-public.
Association for Computational Linguistics.Mona Diab and Pravin Bhutada.
2009.
Verb noun con-struction mwe token classification.
In Proceedings ofthe 2009 Workshop on Multiword Expressions, pages17?22, Singapore.
Association for Computational Lin-guistics.Stefan Evert and Brigitte Krenn.
2001.
Methods for thequalitative evaluation of lexical association measures.In Proceedings of the 39th Annual Meeting on Associ-ation for Computational Linguistics, pages 188?195.Association for Computational Linguistics.Stefan Evert.
2004.
The Statistics of Word Cooccur-rences: Word Pairs and Collocations.
Ph.D. thesis, In-stitut fu?r maschinelle Sprachverarbeitung (IMS), Uni-versita?t Stuttgart.Graham Katz and Eugenie Giesbrecht.
2006.
Auto-matic identification of non-compositional multi-wordexpressions using latent semantic analysis.
In Pro-ceedings of the 2006 Workshop on Multiword Expres-sions, pages 12?19, Sydney, Australia.
Association forComputational Linguistics.Linlin Li and Caroline Sporleder.
2010.
Linguistic cuesfor distinguishing literal and non-literal usages.
InColing 2010: Posters, pages 683?691, Beijing, China.Coling 2010 Organizing Committee.Dekang Lin.
1999.
Automatic identification of non-compositional phrases.
In Proceedings of the 37thAnnual Meeting of the Association for ComputationalLinguistics, pages 317?324, College Park, Maryland,USA.
Association for Computational Linguistics.Marianne Lykke, Birger Larsen, Haakon Lund, and Pe-ter Ingwersen.
2010.
Developing a test collection forthe evaluation of integrated search.
In Advances in In-formation Retrieval, 32nd European Conference on IRResearch, ECIR 2010, Milton Keynes, UK, March 28-31, 2010.
Proceedings, pages 627?630.Christopher D. Manning and Hinrich Schu?tze.
1999.Foundations of Statistical Natural Language Process-ing.
The MIT Press, Cambridge, MA.Donald Metzler and W. Bruce Croft.
2004.
Combiningthe language model and inference network approachesto retrieval.
Inf.
Process.
Manage., 40(5):735?750.B.V.
Moiro?n and Jo?rg Tiedemann.
2006.
Identify-ing Idiomatic Expressions Using Automatic Word-Alignment.
In Multi-Word-Expressions in a Multilin-gual Context, page 33.Pavel Pecina.
2010.
Lexical association measures andcollocation extraction.
Language Resources and Eval-uation, 44(1-2):138?158.Carlos Ramisch, Aline Villavicencio, and ChristianBoitet.
2010. mwetoolkit: a framework for multiwordexpression identification.
In Proceedings of the Sev-enth conference on International Language Resourcesand Evaluation (LREC?10), Valletta, Malta.Ivan A.
Sag, Timothy Baldwin, Francis Bond, AnnCopestake, and Dan Flickinger.
2002.
Multiword ex-pressions: A pain in the neck for nlp.
In Proceedingsof the 3rd International Conference on Intelligent TextProcessing and Computational Linguistics, pages 1?15, Mexico City.Patrick Schone and Daniel Jurafsky.
2001.
Isknowledge-free induction of multiword unit dictionaryheadwords a solved problem?
In Proceedings ofthe 2001 Conference on Empirical Methods in Natu-ral Language Processing, pages 100?108, Pittsburgh,Pennsylvania, USA.
Association for ComputationalLinguistics.Frank Smadja.
1993.
Retrieving collocations from text:Xtract.
Computational linguistics, 19(1):143?177.ChengXiang Zhai and John D. Lafferty.
2002.
Two-stagelanguage models for information retrieval.
In SIGIR,pages 49?56.
ACM.803
