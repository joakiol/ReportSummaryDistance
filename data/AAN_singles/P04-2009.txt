Robust VPE detection using Automatically Parsed TextLeif Arda NielsenDepartment of Computer ScienceKing?s College Londonnielsen@dcs.kcl.ac.ukAbstractThis paper describes a Verb Phrase El-lipsis (VPE) detection system, built forrobustness, accuracy and domain inde-pendence.
The system is corpus-based,and uses machine learning techniqueson free text that has been automaticallyparsed.
Tested on a mixed corpus com-prising a range of genres, the systemachieves a 70% F1-score.
This system isdesigned as the first stage of a completeVPE resolution system that is input freetext, detects VPEs, and proceeds to findthe antecedents and resolve them.1 IntroductionEllipsis is a linguistic phenomenon that has re-ceived considerable attention, mostly focusing onits interpretation.
Most work on ellipsis (Fiengoand May, 1994; Lappin, 1993; Dalrymple et al,1991; Kehler, 1993; Shieber et al, 1996) is aimedat discerning the procedures and the level of lan-guage processing at which ellipsis resolution takesplace, or ambiguous and difficult cases.
The detec-tion of elliptical sentences or the identification ofthe antecedent and elided clauses within them areusually not dealt with, but taken as given.
Noisy ormissing input, which is unavoidable in NLP appli-cations, is not dealt with, and neither is focusingon specific domains or applications.
It thereforebecomes clear that a robust, trainable approach isneeded.An example of Verb Phrase Ellipsis (VPE),which is detected by the presence of an auxiliaryverb without a verb phrase, is seen in example 1.VPE can also occur with semi-auxiliaries, as in ex-ample 2.
(1) John3 {loves his3 wife}2.
Bill3 does1 too.
(2) But although he was terse, he didn?t {rage atme}2 the way I expected him to1.Several steps of work need to be done for ellip-sis resolution :1.
Detecting ellipsis occurrences.
First, elidedverbs need to be found.2.
Identifying antecedents.
For most cases ofellipsis, copying of the antecedent clause isenough for resolution (Hardt, 1997).3.
Resolving ambiguities.
For cases where am-biguity exists, a method for generating thefull list of possible solutions, and suggestingthe most likely one is needed.This paper describes the work done on the firststage, the detection of elliptical verbs.
First, pre-vious work done on tagged corpora will be sum-marised.
Then, new work on parsed corpora willbe presented, showing the gains possible throughsentence-level features.
Finally, experiments us-ing unannotated data that is parsed using an auto-matic parser are presented, as our aim is to pro-duce a stand-alone system.We have chosen to concentrate on VP ellipsisdue to the fact that it is far more common thanother forms of ellipsis, but pseudo-gapping, an ex-ample of which is seen in example 3, has also beenincluded due to the similarity of its resolution toVPE (Lappin, 1996).
Do so/it/that and so doinganaphora are not handled, as their resolution is dif-ferent from that of VPE (Kehler and Ward, 1999).
(3) John writes plays, and Bill does novels.2 Previous workHardt?s (1997) algorithm for detecting VPE in thePenn Treebank (see Section 3) achieves precisionlevels of 44% and recall of 53%, giving an F11of 48%, using a simple search technique, whichrelies on the parse annotation having identifiedempty expressions correctly.In previous work (Nielsen, 2003a; Nielsen,2003b) we performed experiments on the BritishNational Corpus using a variety of machine learn-ing techniques.
These earlier results are not di-rectly comparable to Hardt?s, due to the differ-ent corpora used.
The expanded set of results aresummarised in Table 1, for Transformation BasedLearning (TBL) (Brill, 1995), GIS based Max-imum Entropy Modelling (GIS-MaxEnt) (Ratna-parkhi, 1998), L-BFGS based Maximum EntropyModelling (L-BFGS-MaxEnt)2 (Malouf, 2002),Decision Tree Learning (Quinlan, 1993) andMemory Based Learning (MBL) (Daelemans etal., 2002).Algorithm Recall Precision F1TBL 69.63 85.14 76.61Decision Tree 60.93 79.39 68.94MBL 72.58 71.50 72.04GIS-MaxEnt 71.72 63.89 67.58L-BFGS-MaxEnt 71.93 80.58 76.01Table 1: Comparison of algorithms1Precision, recall and F1 are defined as :Recall = No(correct ellipses found)No(all ellipses in test) (1)Precision = No(correct ellipses found)No(all ellipses found) (2)F1 = 2?
Precision?RecallPrecision+Recall (3)2Downloadable fromhttp://www.nlplab.cn/zhangle/maxent toolkit.htmlFor all of these experiments, the training fea-tures consisted of lexical forms and Part of Speech(POS) tags of the words in a three word for-ward/backward window of the auxiliary beingtested.
This context size was determined empir-ically to give optimum results, and will be usedthroughout this paper.
The L-BFGS-MaxEnt usesGaussian Prior smoothing which was optimizedfor the BNC data, while the GIS-MaxEnt has asimple smoothing option available, but this dete-riorates results and is not used.
MBL was usedwith its default settings.While TBL gave the best results, the softwarewe used (Lager, 1999) ran into memory problemsand proved problematic with larger datasets.
Deci-sion trees, on the other hand, tend to oversimplifydue to the very sparse nature of ellipsis, and pro-duce a single rule that classifies everything as non-VPE.
This leaves Maximum Entropy and MBL forfurther experiments.3 Corpus descriptionThe British National Corpus (BNC) (Leech, 1992)is annotated with POS tags, using the CLAWS-4tagset.
A range of V sections of the BNC, contain-ing around 370k words3 with 645 samples of VPEwas used as training data.
The separate test dataconsists of around 74k words4 with 200 samplesof VPE.The Penn Treebank (Marcus et al, 1994) hasmore than a hundred phrase labels, and a numberof empty categories, but uses a coarser tagset.
Amixture of sections from the Wall Street Journaland Brown corpus were used.
The training sec-tion5 consists of around 540k words and contains522 samples of VPE.
The test section6 consists ofaround 140k words and contains 150 samples ofVPE.4 Experiments using the Penn TreebankTo experiment with what gains are possiblethrough the use of more complex data such as3Sections CS6, A2U, J25, FU6, H7F, HA3, A19, A0P,G1A, EWC, FNS, C8T4Sections EDJ, FR35Sections WSJ 00, 01, 03, 04, 15, Brown CF, CG, CL,CM, CN, CP6Sections WSJ 02, 10, Brown CK, CRparse trees, the Penn Treebank is used for the sec-ond round of experiments.
The results are pre-sented as new features are added in a cumulativefashion, so each experiment also contains the datacontained in those before it.Words and POS tagsThe Treebank, besides POS tags and categoryheaders associated with the nodes of the parsetree, includes empty category information.
For theinitial experiments, the empty category informa-tion is ignored, and the words and POS tags areextracted from the trees.
The results in Table 2are seen to be considerably poorer than those forBNC, despite the comparable data sizes.
This canbe accounted for by the coarser tagset employed.Algorithm Recall Precision F1MBL 47.71 60.33 53.28GIS-MaxEnt 34.64 79.10 48.18L-BFGS-MaxEnt 60.13 76.66 67.39Table 2: Initial results with the TreebankClose to punctuationA very simple feature, that checks for auxiliariesclose to punctuation marks was tested.
Table 3shows the performance of the feature itself, char-acterised by very low precision, and results ob-tained by using it.
It gives a 2% increase in F1 forMBL, 3% for GIS-MaxEnt, but a 1.5% decreasefor L-BFGS-MaxEnt.This brings up the point that the individual suc-cess rate of the features will not be in direct cor-relation with gains in overall results.
Their contri-bution will be high if they have high precision forthe cases they are meant to address, and if theyproduce a different set of results from those al-ready handled well, complementing the existingfeatures.
Overlap between features can be usefulto have greater confidence when they agree, butlow precision in the feature can increase false pos-itives as well, decreasing performance.
Also, thesmall size of the test set can contribute to fluctua-tions in results.Heuristic BaselineA simple heuristic approach was developed toform a baseline.
The method takes all auxiliariesAlgorithm Recall Precision F1close-to-punctuation 30.06 2.31 4.30MBL 50.32 61.60 55.39GIS-MaxEnt 37.90 79.45 51.32L-BFGS-MaxEnt 57.51 76.52 65.67Table 3: Effects of using the close-to-punctuationfeature(SINV(ADVP-PRD-TPC-2 (RB so) )(VP (VBZ is)(ADVP-PRD (-NONE- *T*-2) ))(NP-SBJ (PRP$ its)(NN balance) (NN sheet) ))Figure 1: Fragment of sentence from Treebankas possible candidates and then eliminates themusing local syntactic information in a very simpleway.
It searches forwards within a short range ofwords, and if it encounters any other verbs, adjec-tives, nouns, prepositions, pronouns or numbers,classifies the auxiliary as not elliptical.
It also doesa short backwards search for verbs.
The forwardsearch looks 7 words ahead and the backwardssearch 3.
Both skip ?asides?, which are taken to besnippets between commas without verbs in them,such as : ?...
papers do, however, show ...?.
Thisfeature gives a 4.5% improvement for MBL (Table4), 4% for GIS-MaxEnt and 3.5% for L-BFGS-MaxEnt.Algorithm Recall Precision F1heuristic 48.36 27.61 35.15MBL 55.55 65.38 60.07GIS-MaxEnt 43.13 78.57 55.69L-BFGS-MaxEnt 62.09 77.86 69.09Table 4: Effects of using the heuristic featureSurrounding categoriesThe next feature added is the categories of the pre-vious branch of the tree, and the next branch.
So inthe example in Figure 1, the previous category ofthe elliptical verb is ADVP-PRD-TPC-2, and thenext category NP-SBJ.
The results of using thisfeature are seen in Table 5, giving a 3.5% boost toMBL, 2% to GIS-MaxEnt, and 1.6% to L-BFGS-MaxEnt.Algorithm Recall Precision F1MBL 58.82 69.23 63.60GIS-MaxEnt 45.09 81.17 57.98L-BFGS-MaxEnt 64.70 77.95 70.71Table 5: Effects of using the surrounding cate-goriesAuxiliary-final VPFor auxiliary verbs parsed as verb phrases (VP),this feature checks if the final element in the VPis an auxiliary or negation.
If so, no main verbcan be present, as a main verb cannot be followedby an auxiliary or negation.
This feature was usedby Hardt (1993) and gives a 3.5% boost to perfor-mance for MBL, 6% for GIS-MaxEnt, and 3.4%for L-BFGS-MaxEnt (Table 6).Algorithm Recall Precision F1Auxiliary-final VP 72.54 35.23 47.43MBL 63.39 71.32 67.12GIS-MaxEnt 54.90 77.06 64.12L-BFGS-MaxEnt 71.89 76.38 74.07Table 6: Effects of using the Auxiliary-final VPfeatureEmpty VPHardt (1997) uses a simple pattern check to searchfor empty VP?s identified by the Treebank, (VP(-NONE- *?
*)), which achieves 60% F1 on ourtest set.
Our findings are in line with Hardt?s, whoreports 48% F1, with the difference being due tothe different sections of the Treebank used.It was observed that this search may be too re-strictive to catch some examples of VPE in the cor-pus, and pseudo-gapping.
Modifying the searchpattern to be ?
(VP (-NONE- *?*)?
instead im-proves the feature itself by 10% in F1 and givesthe results seen in Table 7, increasing MBL?s F1 by10%, GIS-MaxEnt by 14% and L-BFGS-MaxEntby 11.7%.Algorithm Recall Precision F1Empty VP 54.90 97.67 70.29MBL 77.12 77.63 77.37GIS-MaxEnt 69.93 88.42 78.10L-BFGS-MaxEnt 83.00 88.81 85.81Table 7: Effects of using the improved Empty VPfeatureEmpty categoriesFinally, including empty category informationcompletely, such that empty categories are treatedas words and included in the context.
Table 8shows that adding this information results in a 4%increase in F1 for MBL, 4.9% for GIS-MaxEnt,and 2.5% for L-BFGS-MaxEnt.Algorithm Recall Precision F1MBL 83.00 79.87 81.41GIS-MaxEnt 76.47 90.69 82.97L-BFGS-MaxEnt 86.27 90.41 88.29Table 8: Effects of using the empty categories5 Experiments with AutomaticallyParsed dataThe next set of experiments use the BNC andTreebank, but strip POS and parse information,and parse them automatically using two differentparsers.
This enables us to test what kind of per-formance is possible for real-world applications.5.1 Parsers usedCharniak?s parser (2000) is a combination prob-abilistic context free grammar and maximum en-tropy parser.
It is trained on the Penn Treebank,and achieves a 90.1% recall and precision averagefor sentences of 40 words or less.Robust Accurate Statistical Parsing (RASP)(Briscoe and Carroll, 2002) uses a combination ofstatistical techniques and a hand-crafted grammar.RASP is trained on a range of corpora, and usesa more complex tagging system (CLAWS-2), likethat of the BNC.
This parser, on our data, gener-ated full parses for 70% of the sentences, partialparses for 28%, while 2% were not parsed, return-ing POS tags only.5.2 Reparsing the TreebankThe results of experiments using the two parsers(Table 9) show generally similar performance.Compared to results on the original treebank withsimilar data (Table 6), the results are 4-6% lower,or in the case of GIS-MaxEnt, 4% lower or 2%higher, depending on parser.
This drop in per-formance is not surprising, given the errors in-troduced by the parsing process.
As the parsersdo not generate empty-category information, theiroverall results are 14-20% lower, compared tothose in Table 8.The success rate for the features used (Table10) stay the same, except for auxiliary-final VP,which is determined by parse structure, is only halfas successful for RASP.
Conversely, the heuristicbaseline is more successful for RASP, as it relieson POS tags, which is to be expected as RASP hasa more detailed tagset.Feature Rec Prec F1Charniak close-to-punct 34.00 2.47 4.61heuristic baseline 45.33 25.27 32.45auxiliary-final VP 51.33 36.66 42.77RASP close-to-punct 71.05 2.67 5.16heuristic baseline 74.34 28.25 40.94auxiliary-final VP 22.36 25.18 23.69Table 10: Performance of features on re-parsedTreebank data5.3 Parsing the BNCExperiments using parsed versions of the BNCcorpora (Table 11) show similar results to the orig-inal results (Table 1) - except L-BFGS-MaxEntwhich scores 4-8% lower - meaning that the addedinformation from the features mitigates the errorsintroduced in parsing.
The performance of the fea-tures (Table 12) remain similar to those for the re-parsed treebank experiments.Feature Rec Prec F1Charniak close-to-punct 48.00 5.52 9.90heuristic baseline 44.00 34.50 38.68auxiliary-final VP 53.00 42.91 47.42RASP close-to-punct 55.32 4.06 7.57heuristic baseline 84.77 35.15 49.70auxiliary-final VP 16.24 28.57 20.71Table 12: Performance of features on parsed BNCdata5.4 Combining BNC and Treebank dataCombining the re-parsed BNC and Treebank datadiversifies and increases the size of the test data,making conclusions drawn empirically more reli-able, and the wider range of training data makesit more robust.
This gives a training set of 1167VPE?s and a test set of 350 VPE?s.
The resultsin Table 13 show little change from the previousexperiments.6 Conclusion and Future workThis paper has presented a robust system for VPEdetection.
The data is automatically tagged andparsed, syntactic features are extracted and ma-chine learning is used to classify instances.
Threedifferent machine learning algorithms, MemoryBased Learning, GIS-based and L-BFGS-basedmaximum entropy modeling are used.
They givesimilar results, with L-BFGS-MaxEnt generallygiving the highest performance.
Two parsers wereused, Charniak?s and RASP, achieving similar re-sults.To summarise the findings :?
Using the BNC, which is tagged with a com-plex tagging scheme but has no parse data, itis possible to get 76% F1 using lexical formsand POS data alone?
Using the Treebank, the coarser taggingscheme reduces performance to 67%.Adding extra features, including sentence-level ones, raises this to 74%.
Adding emptycategory information gives 88%, comparedto previous results of 48% (Hardt, 1997)?
Re-parsing the Treebank data , top perfor-mance is 63%, raised to 68% using extra fea-tures?
Parsing the BNC, top performance is 71%,raised to 72% using extra features?
Combining the parsed data, top performanceis 67%, raised to 71% using extra featuresThe results demonstrate that the method can beapplied to practical tasks using free text.
Next,we will experiment with an algorithm (Johnson,2002) that can insert empty-category informationinto data from Charniak?s parser, allowing replica-tion of features that need this.
Cross-validation ex-periments will be performed to negate the effectsthe small test set may cause.As machine learning is used to combine vari-ous features, this method can be extended to otherforms of ellipsis, and other languages.
However,a number of the features used are specific to En-glish VPE, and would have to be adapted to suchcases.
It is difficult to extrapolate how successfulMBL GIS-MaxEnt L-BFGS-MaxEntRec Prec F1 Rec Prec F1 Rec Prec F1Charniak Words + POS 54.00 62.30 57.85 38.66 79.45 52.01 56.66 71.42 63.19+ features 58.00 65.41 61.48 50.66 73.78 60.07 65.33 72.05 68.53RASP Words + POS 55.92 66.92 60.93 43.42 56.89 49.25 51.63 79.00 62.45+ features 57.23 71.31 63.50 61.84 72.30 66.66 62.74 73.84 67.84Table 9: Results on re-parsed data from the TreebankMBL GIS-MaxEnt L-BFGS-MaxEntRec Prec F1 Rec Prec F1 Rec Prec F1Charniak Words + POS 66.50 63.63 65.03 55.00 75.86 63.76 71.00 70.64 70.82+ features 67.50 67.16 67.33 65.00 75.58 69.89 71.00 73.19 72.08RASP Words + POS 61.92 63.21 62.56 64.46 54.04 58.79 65.34 70.96 68.04+ features 71.06 73.29 72.16 73.09 61.01 66.51 70.29 67.29 68.76Table 11: Results on parsed data from the BNCMBL GIS-MaxEnt L-BFGS-MaxEntRec Prec F1 Rec Prec F1 Rec Prec F1Charniak Words + POS 62.28 69.20 65.56 54.28 77.86 63.97 65.14 69.30 67.15+ features 65.71 71.87 68.65 63.71 72.40 67.78 70.85 69.85 70.35RASP Words + POS 63.61 67.47 65.48 59.31 55.94 57.37 57.46 71.83 63.84+ features 68.48 69.88 69.17 67.61 71.47 69.48 70.14 72.17 71.14Table 13: Results on parsed data using the combined datasetsuch approaches would be based on current work,but it can be expected that they would be feasible,albeit with lower performance.ReferencesEric Brill.
1995.
Transformation-based error-driven learning and natural lan-guage processing: A case study in part-of-speech tagging.
ComputationalLinguistics, 21(4):543?565.E.
Briscoe and J. Carroll.
2002.
Robust accurate statistical annotation of gen-eral text.
In Proceedings of the 3rd International Conference on LanguageResources and Evaluation, Las Palmas, Gran Canaria.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Meeting ofthe North American Chapter of the ACL, page 132.Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and Antal van den Bosch.2002.
Tilburg memory based learner, version 4.3, reference guide.
Down-loadable from http://ilk.kub.nl/downloads/pub/papers/ilk0210.ps.gz.Mary Dalrymple, Stuart M. Shieber, and Fernando Pereira.
1991.
Ellipsis andhigher-order unification.
Linguistics and Philosophy, 14:399?452.Robert Fiengo and Robert May.
1994.
Indices and Identity.
MIT Press, Cam-bridge, MA.Daniel Hardt.
1993.
VP Ellipsis: Form, Meaning, and Processing.
Ph.D.thesis, University of Pennsylvania.Daniel Hardt.
1997.
An empirical approach to vp ellipsis.
ComputationalLinguistics, 23(4).Mark Johnson.
2002.
A simple pattern-matching algorithm for recoveringempty nodes and their antecedents.
In Proceedings of the 40th AnnualMeeting of the Association for Computational Linguistics.Andrew Kehler and Gregory Ward.
1999.
On the semantics and pragmat-ics of ?identifier so?.
In Ken Turner, editor, The Semantics/PragmaticsInterface from Different Points of View (Current Research in the Seman-tics/Pragmatics Interface Series, Volume I).
Amsterdam: Elsevier.Andrew Kehler.
1993.
A discourse copying algorithm for ellipsis andanaphora resolution.
In Proceedings of the Sixth Conference of the Euro-pean Chapter of the Association for Computational Linguistics (EACL-93),Utrecht, the Netherlands.Torbjorn Lager.
1999.
The mu-tbl system: Logic programming tools fortransformation-based learning.
In Third International Workshop on Com-putational Natural Language Learning (CoNLL?99).
Downloadable fromhttp://www.ling.gu.se/ lager/mutbl.html.Shalom Lappin.
1993.
The syntactic basis of ellipsis resolution.
In S. Bermanand A. Hestvik, editors, Proceedings of the Stuttgart Ellipsis Workshop, Ar-beitspapiere des Sonderforschungsbereichs 340, Bericht Nr.
29-1992.
Uni-versity of Stuttgart, Stuttgart.Shalom Lappin.
1996.
The interpretation of ellipsis.
In Shalom Lappin, ed-itor, The Handbook of Contemporary Semantic Theory, pages 145?175.Oxford: Blackwell.G.
Leech.
1992.
100 million words of english : The British National Corpus.Language Research, 28(1):1?13.Robert Malouf.
2002.
A comparison of algorithms for maximum entropyparameter estimation.
In Proceedings of the Sixth Conference on NaturalLanguage Learning (CoNLL-2002), pages 49?55.M.
Marcus, G. Kim, M. Marcinkiewicz, R. MacIntyre, M. Bies, M. Fergu-son, K. Katz, and B. Schasberger.
1994.
The Penn Treebank: Annotat-ing predicate argument structure.
In Proceedings of the Human LanguageTechnology Workshop.
Morgan Kaufmann, San Francisco.Leif Arda Nielsen.
2003a.
A corpus-based study of verb phrase ellipsis.
InProceedings of the 6th Annual CLUK Research Colloquium.Leif Arda Nielsen.
2003b.
Using machine learning techniques for VPE detec-tion.
In Proceedings of RANLP.R.
Quinlan.
1993.
C4.5: Programs for Machine Learning.
San Mateo, CA:Morgan Kaufmann.Adwait Ratnaparkhi.
1998.
Maximum Entropy Models for Natural LanguageAmbiguity Resolution.
Ph.D. thesis, University of Pennsylvania.Stuart Shieber, Fernando Pereira, and Mary Dalrymple.
1996.
Interactions ofscope and ellipsis.
Linguistics and Philosophy, 19(5):527?552.
