Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 132?138,24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational LinguisticsRobust Sense-Based Sentiment ClassificationBalamurali A R1 Aditya Joshi2 Pushpak Bhattacharyya21 IITB-Monash Research Academy, IIT Bombay2Dept.
of Computer Science and Engineering, IIT BombayMumbai, India - 400076{balamurali,adityaj,pb}@cse.iitb.ac.inAbstractThe new trend in sentiment classification isto use semantic features for representationof documents.
We propose a semantic spacebased on WordNet senses for a superviseddocument-level sentiment classifier.
Not onlydoes this show a better performance for sen-timent classification, it also opens opportuni-ties for building a robust sentiment classifier.We examine the possibility of using similar-ity metrics defined on WordNet to address theproblem of not finding a sense in the trainingcorpus.
Using three popular similarity met-rics, we replace unknown synsets in the testset with a similar synset from the training set.An improvement of 6.2% is seen with respectto baseline using this approach.1 IntroductionSentiment classification is a task under SentimentAnalysis (SA) that deals with automatically taggingtext as positive, negative or neutral from the perspec-tive of the speaker/writer with respect to a topic.Thus, a sentiment classifier tags the sentence ?Themovie is entertaining and totally worth your money!
?in a movie review as positive with respect to themovie.
On the other hand, a sentence ?The movie isso boring that I was dozing away through the secondhalf.?
is labeled as negative.
Finally, ?The movie isdirected by Nolan?
is labeled as neutral.
For the pur-pose of this work, we follow the definition of Panget al (2002) & Turney (2002) and consider a binaryclassification task for output labels as positive andnegative.Lexeme-based (bag-of-words) features are com-monly used for supervised sentiment classifica-tion (Pang and Lee, 2008).
In addition to this, therealso has been work that identifies the roles of dif-ferent parts-of-speech (POS) like adjectives in sen-timent classification (Pang et al, 2002; Whitelaw etal., 2005).
Complex features based on parse treeshave been explored for modeling high-accuracy po-larity classifiers (Matsumoto et al, 2005).
Textparsers have also been found to be helpful in mod-eling valence shifters as features for classifica-tion (Kennedy and Inkpen, 2006).
In general, thework in the context of supervised SA has focused on(but not limited to) different combinations of bag-of-words-based and syntax-based models.The focus of this work is to represent a documentas a set of sense-based features.
We ask the follow-ing questions in this context:1.
Are WordNet senses better features as com-pared to words?2.
Can a sentiment classifier be made robust withrespect to features unseen in the training cor-pus using similarity metrics defined for con-cepts in WordNet?We modify the corpus by Ye et al (2009) for thepurpose of our experiments related to sense-basedsentiment classification.
To address the first ques-tion, we show that the approach that uses senses (ei-ther manually annotated or obtained through auto-matic WSD techniques) as features performs betterthan the one that uses words as features.Using senses as features allows us to achieve ro-bustness for sentiment classification by exploitingthe definition of concepts (sense) and hierarchicalstructure of WordNet.
Hence to address the secondquestion, we replace a synset not present in the testset with a similar synset from the training set us-ing similarity metrics defined on WordNet.
Our re-sults show that replacement of this nature provides aboost to the classification performance.The road map for the rest of the paper is as fol-lows: Section 2 describes the sense-based featuresthat we use for this work.
We explain the similarity-based replacement technique using WordNet synsets132in section 3.
Details about our experiments are de-scribed in Section 4.
In section 5, we present ourresults and discussions.
We contextualize our workwith respect to other related works in section 6.
Fi-nally, section 7 concludes the paper and points tofuture work.2 WordNet Senses as FeaturesIn their original form, documents are said to be inlexical space since they consist of words.
When thewords are replaced by their corresponding senses,the resultant document is said to be in semanticspace.WordNet 2.1 (Fellbaum, 1998) has been used asthe sense repository.
Each word/lexeme is mappedto an appropriate synset in WordNet based onits sense and represented using the correspondingsynset id of WordNet.
Thus, the word love is dis-ambiguated and replaced by the identifier 21758160which consists of a POS category identifier 2 fol-lowed by synset offset identifier 1758160.
Thispaper refers to POS category identifier along withsynset offset as synset identifiers or as senses.2.1 MotivationWe describe three different scenarios to show theneed of sense-based analysis for SA.
Consider thefollowing sentences as the first scenario.1.
?Her face fell when she heard that she hadbeen fired.?2.
?The fruit fell from the tree.
?The word ?fell?
occurs in different senses in thetwo sentences.
In the first sentence, ?fell?
has themeaning of ?assume a disappointed or sad expres-sion, whereas in the second sentence, it has themeaning of ?descend in free fall under the influenceof gravity?.
A user will infer the negative polarityof the first sentence from the negative sense of ?fell?in it.
This implies that there is at least one sense ofthe word ?fell?
that carries sentiment and at least onethat does not.In the second scenario, consider the following ex-amples.1.
?The snake bite proved to be deadly for theyoung boy.?2.
?Shane Warne is a deadly spinner.
?The word deadly has senses which carry oppositepolarity in the two sentences and these senses as-sign the polarity to the corresponding sentence.
Thefirst sentence is negative while the second sentenceis positive.Finally in the third scenario, consider the follow-ing pair of sentences.1.
?He speaks a vulgar language.?2.
?Now that?s real crude behavior!
?The words vulgar and crude occur as synonymsin the synset that corresponds to the sense ?conspic-uously and tastelessly indecent?.
The synonymousnature of words can be identified only if they arelooked at as senses and not just words.As one may observe, the first scenario shows thata word may have some sentiment-bearing and somenon-sentiment-bearing senses.
In the second sce-nario, we show that there may be different sensesof a word that bear sentiments of opposite polarity.Finally, in the third scenario, we show how a sensecan be manifested using different words, i.e., wordsin a synset.
The three scenarios motivate the use ofsemantic space for sentiment prediction.2.2 Sense versus Lexeme-based FeatureRepresentationsWe annotate the words in the corpus with theirsenses using two sense disambiguation approaches.As the first approach, manual sense annotationof documents is carried out by two annotators ontwo subsets of the corpus, the details of which aregiven in Section 4.1.
The experiments conducted onthis set determine the ideal case scenario- the skylineperformance.As the second approach, a state-of-art algorithmfor domain-specific WSD proposed by Khapra etal.
(2010) is used to obtain an automatically sense-tagged corpus.
This algorithm called iterative WSDor IWSD iteratively disambiguates words by rank-ing the candidate senses based on a scoring function.The two types of sense-annotated corpus lead usto four feature representations for a document:1.
A group of word senses that have been manu-ally annotated (M)1332.
A group of word senses that have been anno-tated by an automatic WSD (I)3.
A group of manually annotated word sensesand words (both separately as features) (Sense+ Words(M))4.
A group of automatically annotated wordsenses and words (both separately as features)(Sense + Words(I))Our first set of experiments compares the four fea-ture representations to find the feature representa-tion with which sentiment classification gives thebest performance.
Sense + Words(M) and Sense+ Words(I) are used to overcome non-coverage ofWordNet for some noun synsets.3 Similarity Metrics and Unknown Synsets3.1 Synset Replacement AlgorithmUsing WordNet senses provides an opportunity touse similarity-based metrics for WordNet to reducethe effect of unknown features.
If a synset encoun-tered in a test document is not found in the trainingcorpus, it is replaced by one of the synsets presentin the training corpus.
The substitute synset is deter-mined on the basis of its similarity with the synsetin the test document.
The synset that is replaced isreferred to as an unseen synset as it is not known tothe trained model.For example, consider excerpts of two reviews,the first of which occurs in the training corpus whilethe second occurs in the test corpus.1.
?
In the night, it is a lovely city and... ?2.
?
The city has many beautiful hot spots for hon-eymooners.
?The synset of ?beautiful?
is not present in the train-ing corpus.
We evaluate a similarity metric for allsynsets in the training corpus with respect to thesense of beautiful and find that the sense of lovely isclosest to it.
Hence, the sense of beautiful in the testdocument is replaced by the sense of lovely which ispresent in the training corpus.The replacement algorithm is described inAlgorithm 1.
The term concept is used in placeof synset though the two essentially mean thesame in this context.
The algorithm aims to find aconcept temp concept for each concept in the testcorpus.
The temp concept is the concept closest tosome concept in the training corpus based on thesimilarity metrics.
The algorithm follows from thefact that the similarity value for a synset with itselfis maximum.Input: Training Corpus, Test Corpus,Similarity MetricOutput: New Test CorpusT:= Training Corpus;X:= Test Corpus;S:= Similarity metric;train concept list = get list concept(T) ;test concept list = get list concept(X);for each concept C in test concept list dotemp max similarity = 0 ;temp concept = C ;for each concept D in train concept list dosimilarity value = get similarity value(C,D,S);if (similarity value > temp max similarity) thentemp max similarity= similarity value;temp concept = D ;endendreplace synset corpus(C,temp concept,X);endReturn X ;Algorithm 1: Synset replacement using similaritymetricThe for loop over C finds a concept temp conceptin the training corpus with the maximumsimilarity value.
The method replace synset corpusreplaces the concept C in the test corpus withtemp concept in the test corpus X.3.2 Similarity Metrics UsedWe evaluate the benefit of three similarity metrics,namely LIN?s similarity metric, Lesk similaritymetric and Leacock and Chodorow (LCH) similaritymetric for the synset replacement algorithm stated.These runs generate three variants of the corpus.We compare the benefit of each of these metrics bystudying their sentiment classification performance.The metrics can be described as follows:LIN: The metric by Lin (1998) uses the infor-mation content individually possessed by two con-cepts in addition to that shared by them.
The infor-mation content shared by two concepts A and B isgiven by their most specific subsumer (lowest super-134ordinate(lso).
Thus, this metric defines the similaritybetween two concepts assimLIN (A,B) =2?
logPr(lso(A,B))logPr(A) + logPr(B)(1)Lesk: Each concept in WordNet is definedthrough gloss.
To compute the Lesk similar-ity (Banerjee and Pedersen, 2002) between A andB, a scoring function based on the overlap of wordsin their individual glosses is used.Leacock and Chodorow (LCH): To measuresimilarity between two concepts A and B, Leacockand Chodorow (1998) compute the shortest paththrough hypernymy relation between them under theconstraint that there exists such a path.
The finalvalue is computed by scaling the path length by theoverall taxonomy depth (D).simLCH(A,B) = ?
log(len(A,B)2D)(2)4 ExperimentationWe describe the variants of the corpus generated andthe experiments in this section.4.1 Data PreparationWe create different variants of the dataset by Ye etal.
(2009).
This dataset contains 600 positive and591 negative reviews about seven travel destinations.Each review contains approximately 4-5 sentenceswith an average number of words per review being80-85.To create the manually annotated corpus, two hu-man annotators annotate words in the corpus withsenses for two disjoint subsets of the original cor-pus by Ye et al (2009).
The inter-annotation agree-ment for a subset(20 positive reviews) of the corpusshowed 91% sense overlap.
The manually annotatedcorpus consists of 34508 words with 6004 synsets.The second variant of the corpus contains wordsenses obtained from automatic disambiguation us-ing IWSD.
The evaluation statistics of the IWSD isshown in Table 1.
Table 1 shows that the F-score fornoun synsets is high while that for adjective synsetsis the lowest among all.
The low recall for adjec-tive POS based synsets can be detrimental to classi-fication since adjectives are known to express directsentiment (Pang et al, 2002).POS #Words P(%) R(%) F-Score(%)Noun 12693 75.54 75.12 75.33Adverb 4114 71.16 70.90 71.03Adjective 6194 67.26 66.31 66.78Verb 11507 68.28 67.97 68.12Overall 34508 71.12 70.65 70.88Table 1: Annotation Statistics for IWSD; P- Precision,R-Recall4.2 Experimental SetupThe experiments are performed using C-SVM (lin-ear kernel with default parameters1) available as apart of LibSVM2 package.
We choose to use SVMsince it performs the best for sentiment classification(Pang et al, 2002).
All results reported are averageof five-fold cross-validation accuracies.To conduct experiments on words as features, wefirst perform stop-word removal.
The words arenot stemmed as per observations by (Leopold andKindermann, 2002).
To conduct the experimentsbased on the synset representation, words in thecorpus are annotated with synset identifiers alongwith POS category identifiers.
For automatic sensedisambiguation, we used the trained IWSD engine(trained on tourism domain) from Khapra et al(2010).
These synset identifiers along with POS cat-egory identifiers are then used as features.
For re-placement using semantic similarity measures, weused WordNet::Similarity 2.05 package by Pedersenet al (2004).To evaluate the result, we use accuracy, F-score,recall and precision as the metrics.
Classificationaccuracy defines the ratio of the number of true in-stances to the total number of instances.
Recall iscalculated as a ratio of the true instances found tothe total number of false positives and true posi-tives.
Precision is defined as the number of trueinstances divided by number of true positives andfalse negatives.
Positive Precision (PP) and Posi-tive Recall (PR) are precision and recall for positivedocuments while Negative Precision (NP) and Nega-tive Recall (NR) are precision and recall for negativedocuments.
F-score is the weighted precision-recall1C=0.0,=0.00102http://www.csie.ntu.edu.tw/ cjlin/libsvm135Feature Representation Accuracy PF NF PP NP PR NRWords 84.90 85.07 84.76 84.95 84.92 85.19 84.60Sense (M) 89.10 88.22 89.11 91.50 87.07 85.18 91.24Sense + Words (M) 90.20 89.81 90.43 92.02 88.55 87.71 92.39Sense (I) 85.48 85.31 85.65 87.17 83.93 83.53 87.46Sense + Words(I) 86.08 86.28 85.92 85.87 86.38 86.69 85.46Table 2: Classification Results; M-Manual, I-IWSD, W-Words, PF-Positive F-score(%), NF-Negative F-score (%),PP-Positive Precision (%), NP-Negative Precision (%), PR-Positive Recall (%), NR-Negative Recall (%)score.5 Results and Discussions5.1 Comparison of various featurerepresentationsTable 2 shows results of classification for differentfeature representations.
The baseline for our resultsis the unigram bag-of-words model (Words).An improvement of 4.2% is observed in the ac-curacy of sentiment prediction when manually an-notated sense-based features (M) are used in placeof word-based features (Words).
The precision ofboth the classes using features based on semanticspace is also better than one based on lexeme space.Reported results suggest that it is more difficult todetect negative sentiment than positive sentiment(Gindl and Liegl, 2008).
However, using sense-based representation, it is important to note that neg-ative recall increases by around 8%.The combined model of words and manually an-notated senses (Sense + Words (M)) gives the bestperformance with an accuracy of 90.2%.
This leadsto an improvement of 5.3% over the baseline accu-racy 3.One of the reasons for improved performance isthe feature abstraction achieved due to the synset-based features.
The dimension of feature vector isreduced by a factor of 82% when the document isrepresented in synset space.
The reduction in dimen-sionality may also lead to reduction in noise (Cun-ningham, 2008).A comparison of accuracy of different sense rep-resentations in Table 2 shows that manual disam-3The improvement in results of semantic space is found tobe statistically significant over the baseline at 95% confidencelevel when tested using a paired t-test.biguation performs better than using automatic al-gorithms like IWSD.
Although overall classificationaccuracy improvement of IWSD over baseline ismarginal, negative recall also improves.
This bene-fit is despite the fact that evaluation of IWSD engineover manually annotated corpus gave an overall F-score of 71% (refer Table 1).
For a WSD enginewith a better accuracy, the performance of sense-based SA can be boosted further.Thus, in terms of feature representation of docu-ments, sense-based features provide a better overallperformance as compared to word-based features.5.2 Synset replacement using similarity metricsTable 3 shows the results of synset replacement ex-periments performed using similarity metrics de-fined in section 3.
The similarity metric value NAshown in the table indicates that synset replacementis not performed for the specific run of experiment.For this set of experiments, we use the combina-tion of sense and words as features (indicated bySenses+Words (M)).Synset replacement using a similarity metricshows an improvement over using words alone.However, the improvement in classification accu-racy is marginal compared to sense-based represen-tation without synset replacement (Similarity Met-ric=NA).Replacement using LIN and LCH metrics givesmarginally better results compared to the vanilla set-ting in a manually annotated corpus.
The same phe-nomenon is seen in the case of IWSD based ap-proach4.
The limited improvement can be due tothe fact that since LCH and LIN consider only IS-A4Results based on LCH and LIN similarity metric for auto-matic sense disambiguation is not statistically significant with?=0.05136Features Representa-tionSM A PF NFWords (Baseline) NA 84.90 85.07 84.76Sense+Words (M) NA 90.20 89.81 90.43Sense+Words (I) NA 86.08 86.28 85.92Sense+Words (M) LCH 90.60 90.20 90.85Sense+Words (M) LIN 90.70 90.26 90.97Sense+Words (M) Lesk 91.12 90.70 91.38Sense+Words (I) LCH 85.66 85.85 85.52Sense+Words (I) LIN 86.16 86.37 86.00Sense+Words (I) Lesk 86.25 86.41 86.10Table 3: Similarity Metric Analysis using differentsimilarity metrics with synsets and a combinations ofsynset and words; SM-Similarity Metric, A-Accuracy,PF-Positive F-score(%), NF-Negative F-score (%)relationship in WordNet, the replacement happensonly for verbs and nouns.
This excludes adverbsynsets which we have shown to be the best featuresfor a sense-based SA system.Among all similarity metrics, the best classifica-tion accuracy is achieved using Lesk.
The systemperforms with an overall classification accuracy of91.12%, which is a substantial improvement of 6.2%over baseline.
Again, it is only 1% over the vanillasetting that uses combination of synset and words.However, the similarity metric is not sophisticated asLIN or LCH.
A good metric which covers all POScategories can provide substantial improvement inthe classification accuracy.6 Related WorkThis work deals with studying benefit of a wordsense-based feature space to supervised sentimentclassification.
This work assumes the hypothesisthat word sense is associated with the sentiment asshown by Wiebe and Mihalcea (2006) through hu-man interannotator agreement.Akkaya et al (2009) and Martn-Wanton et al(2010) study rule-based sentiment classification us-ing word senses where Martn-Wanton et al (2010)uses a combination of sentiment lexical resources.Instead of a rule-based implementation, our workleverages on benefits of a statistical learning-basedmethods by using a supervised approach.
Rentoumiet al (2009) suggest an approach to use word sensesto detect sentence level polarity using graph-basedsimilarity.
While Rentoumi et al (2009) targets us-ing senses to handle metaphors in sentences, we dealwith generating a general-purpose classifier.Carrillo de Albornoz et al (2010) create an emo-tional intensity classifier using affective class con-cepts as features.
By using WordNet synsets as fea-tures, we construct feature vectors that map to alarger sense-based space.Akkaya et al (2009), Martn-Wanton et al (2010)and Carrillo de Albornoz et al (2010) deal withsentiment classification of sentences.
On the otherhand, we associate sentiment polarity to a documenton the whole as opposed to Pang and Lee (2004)which deals with sentiment prediction of subjectiv-ity content only.
Carrillo de Albornoz et al (2010)suggests expansion using WordNet relations whichwe perform in our experiments.7 Conclusion & Future WorkWe present an empirical study to show that sense-based features work better as compared to word-based features.
We show how the performance im-pact differs for different automatic and manual tech-niques.
We also show the benefit using WordNetbased similarity metrics for replacing unknown fea-tures in the test set.
Our results support the fact thatnot only does sense space improve the performanceof a sentiment classification system but also opensopportunities for building robust sentiment classi-fiers that can handle unseen synsets.Incorporation of syntactical information alongwith semantics can be an interesting area ofwork.
Another line of work is in the context ofcross-lingual sentiment analysis.
Current solutionsare based on machine translation which is veryresource-intensive.
Using a bi-lingual dictionarywhich maps WordNet across languages can prove tobe an alternative.ReferencesCem Akkaya, Janyce Wiebe, and Rada Mihalcea.
2009.Subjectivity word sense disambiguation.
In Proc.
ofEMNLP ?09, pages 190?199, Singapore.Satanjeev Banerjee and Ted Pedersen.
2002.
An adaptedlesk algorithm for word sense disambiguation usingwordnet.
In Proc.
of CICLing?02, pages 136?145,London, UK.137Jorge Carrillo de Albornoz, Laura Plaza, and PabloGervs.
2010.
Improving emotional intensity clas-sification using word sense disambiguation.
Specialissue: Natural Language Processing and its Appli-cations.
Journal on Research in Computing Science,46:131?142.Pdraig Cunningham.
2008.
Dimension reduction.
InMachine Learning Techniques for Multimedia, Cogni-tive Technologies, pages 91?112.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
Bradford Books.Stefan Gindl and Johannes Liegl, 2008.
Evaluation ofdifferent sentiment detection methods for polarity clas-sification on web-based reviews, pages 35?43.Alistair Kennedy and Diana Inkpen.
2006.
Sentimentclassification of movie reviews using contextual va-lence shifters.
Computational Intelligence, 22(2):110?125.Mitesh Khapra, Sapan Shah, Piyush Kedia, and PushpakBhattacharyya.
2010.
Domain-specific word sensedisambiguation combining corpus basedand wordnetbased parameters.
In Proc.
of GWC?10, Mumbai, In-dia.Claudia Leacock and Martin Chodorow.
1998.
Com-bining local context with wordnet similarity for wordsense identification.
In WordNet: A Lexical ReferenceSystem and its Application.Edda Leopold and Jo?rg Kindermann.
2002.
Text catego-rization with support vector machines.
how to repre-sent texts in input space?
Machine Learning, 46:423?444.Dekang Lin.
1998.
An information-theoretic definitionof similarity.
In In Proc.
of the 15th International Con-ference on Machine Learning, pages 296?304.Tamara Martn-Wanton, Alexandra Balahur-Dobrescu,Andres Montoyo-Guijarro, and Aurora Pons-Porrata.2010.
Word sense disambiguation in opinion mining:Pros and cons.
In Proc.
of CICLing?10, Madrid,Spain.Shotaro Matsumoto, Hiroya Takamura, and ManabuOkumura.
2005.
Sentiment classification using wordsub-sequences and dependency sub-trees.
In Proc.of PAKDD?05,, Lecture Notes in Computer Science,pages 301?311.Bo Pang and Lillian Lee.
2004.
A sentimental education:Sentiment analysis using subjectivity summarizationbased on minimum cuts.
In Proc.
of ACL?04, pages271?278, Barcelona, Spain.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Found.
Trends Inf.
Retr., 2:1?135.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
sentiment classification using ma-chine learning techniques.
volume 10, pages 79?86.Ted Pedersen, Siddharth Patwardhan, and Jason Miche-lizzi.
2004.
Wordnet::similarity: measuring the relat-edness of concepts.
In Demonstration Papers at HLT-NAACL?04, pages 38?41.Vassiliki Rentoumi, George Giannakopoulos, VangelisKarkaletsis, and George A. Vouros.
2009.
Sen-timent analysis of figurative language using a wordsense disambiguation approach.
In Proc.
of the In-ternational Conference RANLP?09, pages 370?375,Borovets, Bulgaria.Peter Turney.
2002.
Thumbs up or thumbs down?
Se-mantic orientation applied to unsupervised classifica-tion of reviews.
In Proc.
of ACL?02, pages 417?424,Philadelphia, US.Casey Whitelaw, Navendu Garg, and Shlomo Argamon.2005.
Using appraisal groups for sentiment analysis.In Proc.
of CIKM ?05, pages 625?631, New York, NY,USA.Janyce Wiebe and Rada Mihalcea.
2006.
Word senseand subjectivity.
In Proc.
of COLING-ACL?06, pages1065?1072.Qiang Ye, Ziqiong Zhang, and Rob Law.
2009.
Senti-ment classification of online reviews to travel destina-tions by supervised machine learning approaches.
Ex-pert Systems with Applications, 36(3):6527 ?
6535.138
