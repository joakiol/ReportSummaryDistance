Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 529?539,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsRandom Walk Inference and Learning in A Large Scale Knowledge BaseNi LaoCarnegie Mellon University5000 Forbes AvenuePittsburgh, PA 15213nlao@cs.cmu.eduTom MitchellCarnegie Mellon University5000 Forbes AvenuePittsburgh, PA 15213tom.mitchell@cs.cmu.eduWilliam W. CohenCarnegie Mellon University5000 Forbes AvenuePittsburgh, PA 15213wcohen@cs.cmu.eduAbstractWe consider the problem of performing learn-ing and inference in a large scale knowledgebase containing imperfect knowledge withincomplete coverage.
We show that a softinference procedure based on a combinationof constrained, weighted, random walksthrough the knowledge base graph can beused to reliably infer new beliefs for theknowledge base.
More specifically, weshow that the system can learn to inferdifferent target relations by tuning the weightsassociated with random walks that followdifferent paths through the graph, using aversion of the Path Ranking Algorithm (Laoand Cohen, 2010b).
We apply this approach toa knowledge base of approximately 500,000beliefs extracted imperfectly from the webby NELL, a never-ending language learner(Carlson et al, 2010).
This new systemimproves significantly over NELL?s earlierHorn-clause learning and inference method:it obtains nearly double the precision at rank100, and the new learning method is alsoapplicable to many more inference tasks.1 IntroductionAlthough there is a great deal of recent researchon extracting knowledge from text (Agichtein andGravano, 2000; Etzioni et al, 2005; Snow etal., 2006; Pantel and Pennacchiotti, 2006; Bankoet al, 2007; Yates et al, 2007), much lessprogress has been made on the problem of drawingreliable inferences from this imperfectly extractedknowledge.
In particular, traditional logicalinference methods are too brittle to be used to makecomplex inferences from automatically-extractedknowledge, and probabilistic inference methods(Richardson and Domingos, 2006) suffer fromscalability problems.
This paper considers theproblem of constructing inference methods that canscale to large knowledge bases (KB?s), and that arerobust to imperfect knowledge.
The KB we consideris a large triple store, which can be represented as alabeled, directed graph in which each entity a is anode, each binary relation R(a, b) is an edge labeledR between a and b, and unary concepts C(a) arerepresented as an edge labeled ?isa?
between thenode for the entity a and a node for the conceptC.
We present a trainable inference method thatlearns to infer relations by combining the results ofdifferent random walks through this graph, and showthat the method achieves good scaling properties androbust inference in a KB containing over 500,000triples extracted from the web by the NELL system(Carlson et al, 2010).1.1 The NELL Case StudyTo evaluate our approach experimentally, we studyit in the context of the NELL (Never EndingLanguage Learning) research project, which is aneffort to develop a never-ending learning system thatoperates 24 hours per day, for years, to continuouslyimprove its ability to read (extract structured factsfrom) the web (Carlson et al, 2010).
NELL beganoperation in January 2010.
As of March 2011,NELL had built a knowledge base containing severalmillion candidate beliefs which it had extracted fromthe web with varying confidence.
Among these,529NELL had fairly high confidence in approximatelyhalf a million, which we refer to as NELL?s(confident) beliefs.
NELL had lower confidence in afew million others, which we refer to as its candidatebeliefs.NELL is given as input an ontology that defineshundreds of categories (e.g., person, beverage,athlete, sport) and two-place typed relations amongthese categories (e.g., atheletePlaysSport(?athlete?,?sport?
)), which it must learn to extract from theweb.
It is also provided a set of 10 to 20 positiveseed examples of each such category and relation,along with a downloaded collection of 500 millionweb pages from the ClueWeb2009 corpus (Callanand Hoy, 2009) as unlabeled data, and access to100,000 queries each day to Google?s search engine.Each day, NELL has two tasks: (1) to extractadditional beliefs from the web to populate itsgrowing knowledge base (KB) with instances of thecategories and relations in its ontology, and (2) tolearn to perform task 1 better today than it couldyesterday.
We can measure its learning competenceby allowing it to consider the same text documentstoday as it did yesterday, and recording whether itextracts more beliefs, more accurately today.1NELL uses a large-scale semi-supervised multi-task learning algorithm that couples the trainingof over 1500 different classifiers and extractionmethods (see (Carlson et al, 2010)).
Althoughmany of the details of NELL?s learning methodare not central to this paper, two points shouldbe noted.
First, NELL is a multistrategy learningsystem, with components that learn from different?views?
of the data (Blum and Mitchell, 1998): forinstance, one view uses orthographic features ofa potential entity name (like ?contains capitalizedwords?
), and another uses free-text contexts inwhich the noun phrase is found (e.g., ?X frequentlyfollows the bigram ?mayor of?
?).
Second, NELLis a bootstrapping system, which self-trains on itsgrowing collection of confident beliefs.1.2 Knowledge Base Inference: Horn ClausesAlthough NELL has now grown a sizable knowl-edge base, its ability to perform inference over this1NELL?s current KB is available online athttp://rtw.ml.cmu.edu.Eli Manning GiantsAthletePlaysForTeamHinesWard SteelersAthletePlaysForTeam NFLTeamPlaysInLeagueMLBTeamPlaysInLeagueTeamPlaysInLeagueFigure 1: An example subgraph.knowledge base is currently very limited.
At presentits only inference method beyond simple inheritanceinvolves applying first order Horn clause rules toinfer new beliefs from current beliefs.
For example,it may use a Horn clause such asAthletePlaysForTeam(a, b) (1)?
TeamPlaysInLeague(b, c)?
AthletePlaysInLeague(a,c)to infer that AthletePlaysInLeague(HinesWard,NFL),if it has already extracted the beliefs in thepreconditions of the rule, with variables a, b and cbound to HinesWard, PittsburghSteelers and NFLrespectively as shown in Figure 1.
NELL currentlyhas a set of approximately 600 such rules, whichit has learned by data mining its knowledge baseof beliefs.
Each learned rule carries a conditionalprobability that its conclusion will hold, given thatits preconditions are satisfied.NELL learns these Horn clause rules usinga variant of the FOIL algorithm (Quinlan andCameron-Jones, 1993), henceforth N-FOIL.N-FOIL takes as input a set of positive andnegative examples of a rule?s consequent(e.g., +AthletePlaysInLeague(HinesWard,NFL),?AthletePlaysInLeague(HinesWard,NBA)), anduses a ?separate-and-conquer?
strategy to learn aset of Horn clauses that fit the data well.
EachHorn clause is learned by starting with a generalrule and progressively specializing it, so that itstill covers many positive examples but covers fewnegative examples.
After a clause is learned, theexamples covered by that clause are removed fromthe training set, and the process repeats until nopositive examples remain.Learning first-order Horn clauses is computation-ally expensive?not only is the search space large,but some Horn clauses can be costly to evaluate(Cohen and Page, 1995).
N-FOIL uses two tricksto improve its scalability.
First, it assumes thatthe consequent predicate is functional?e.g., that530each Athlete plays in at most one League.
Thismeans that explicit negative examples need notbe provided (Zelle et al, 1995): e.g., if Ath-letePlaysInLeague(HinesWard,NFL) is a positiveexample, then AthletePlaysInLeague(HinesWard,c?
)for any other value of c?
is negative.
In general,this constraint guides the search algorithm towardHorn clauses that have fewer possible instantiations,and hence are less expensive to match.
Second,N-FOIL uses ?relational pathfinding?
(Richardsand Mooney, 1992) to produce general rules?i.e.,the starting point for a predicate R is foundby looking at positive instances R(a, b) of theconsequent, and finding a clause that correspondsto a bounded-length path of binary relations thatlink a to b.
In the example above, a start clausemight be the clause (1).
As in FOIL, the clauseis then (potentially) specialized by greedily addingadditional conditions (like ProfessionalAthlete(a))or by replacing variables with constants (eg,replacing c with NFL).For each N-FOIL rule, an estimated conditionalprobability P?
(conclusion|preconditions) is calcu-lated using a Dirichlet prior according toP?
= (N+ +m ?
prior)/(N+ +N?
+m) (2)where N+ is the number of positive instancesmatched by this rule in the FOIL training data,N?
is the number of negative instances matched,m = 5 and prior = 0.5.
As the results belowshow, N-FOIL generally learns a small number ofhigh-precision inference rules.
One important roleof these inference rules is that they contribute tothe bootstrapping procedure, as inferences made byN-FOIL increase either the number of candidatebeliefs, or (if the inference is already a candidate)improve NELL?s confidence in candidate beliefs.1.3 Knowledge Base Inference: GraphRandom WalksIn this paper, we consider an alternative approach,based on the Path Ranking Algorithm (PRA) of Laoand Cohen (2010b), described in detail below.
PRAlearns to rank graph nodes b relative to a querynode a. PRA begins by enumerating a large set ofbounded-length edge-labeled path types, similar tothe initial clauses used in NELL?s variant of FOIL.These path types are treated as ranking ?experts?,each performing a random walk through the graph,constrained to follow that sequence of edge types,and ranking nodes b by their weights in the resultingdistribution.
Finally, PRA combines the weightscontributed by different ?experts?
using logisticregression to predict the probability that the relationR(a, b) is satisfied.As an example, consider a path from a to b viathe sequence of edge types isa, isa?1 (the inverse ofisa), and AthletePlaysInLeague, which correspondsto the Horn clauseisa(a, c) ?
isa?1(c, a?)
(3)?
AthletePlaysInLeague(a?, b)?
AthletePlaysInLeague(a, b)Suppose a random walk starts at a query node a(say a=HinesWard).
If HinesWard is linked to thesingle concept node ProfessionalAthlete via isa, thewalk will reach that node with probability 1 afterone step.
If A is the set of ProfessionalAthlete?sin the KB, then after two steps, the walk will haveprobability 1/|A| of being at any a?
?
A.
If L isthe set of athletic leagues and ` ?
L, let A` be theset of athletes in league `: after three steps, the walkwill have probability |A`|/|A| of being at any pointb ?
L. In short, the ranking associated with thispath gives the prior probability of a value b being anathletic league for a?which is useful as a feature ina combined ranking method, although not by itself ahigh-precision inference rule.Note that the rankings produced by this ?expert?will change as the knowledge base evolves?forinstance, if the system learns about proportionallymore soccer players than hockey players over time,then the league rankings for the path of clause (3)will change.
Also, the ranking is specific to thequery node a.
For instance, suppose the KB containsfacts which reflect the ambiguity of the team name?Giants?2 as in Figure 1.
Then the path for clause (1)above will give lower weight to b = NFL for a =EliManning than to b = NFL for a = HinesWard.The main contribution of this paper is to introduceand evaluate PRA as an algorithm for makingprobabilistic inference in large KBs.
Compared toHorn clause inference, the key characteristics of thisnew inference method are as follows:2San Francisco?s Major-League Baseball and New York?sNational Football League teams are both called the ?Giants?.531?
The evidence in support of inferring a relationinstance R(a, b) is based on many existingpaths between a and b in the current KB,combined using a learned logistic function.?
The confidence in an inference is sensitive tothe current state of the knowledge base, and thespecific entities being queried (since the pathsused in the inference have these properties).?
Experimentally, the inference method yieldsmany more moderately-confident inferencesthan the Horn clauses learned by N-FOIL.?
The learning and inference are more efficientthan N-FOIL, in part because we can exploitefficient approximation schemes for randomwalks (Lao and Cohen, 2010a).
The resultinginference is as fast as 10 milliseconds per queryon average.The Path Ranking Algorithm (PRA) we use issimilar to that described elsewhere (Lao and Cohen,2010b), except that to achieve efficient modellearning, the paths between a and b are determinedby the statistics from a population of trainingqueries rather than enumerated completely.
PRAuses random walks to generate relational featureson graph data, and combine them with a logisticregression model.
Compared to other relationalmodels (e.g.
FOIL, Markov Logic Networks), PRAis extremely efficient at link prediction or retrievaltasks, in which we are interested in identifying toplinks from a large number of candidates, instead offocusing on a particular node pair or joint inferences.1.4 Related WorkThe TextRunner system (Cafarella et al, 2006)answers list queries on a large knowledge baseproduced by open domain information extrac-tion.
Spreading activation is used to measurethe closeness of any node to the query termnodes.
This approach is similar to the randomwalk with restart approach which is used as abaseline in our experiment.
The FactRank system(Jain and Pantel, 2010) compares different ways ofconstructing random walks, and combining themwith extraction scores.
However, the shortcomingof both approaches is that they ignore edge typeinformation, which is important for achieving highaccuracy predictions.The HOLMES system (Schoenmackers et al,2008) derives new assertions using a few manuallywritten inference rules.
A Markov networkcorresponding to the grounding of these rules tothe knowledge base is constructed for each query,and then belief propagation is used for inference.In comparison, our proposed approach discoversinference rules automatically from training data.Similarly, the Markov Logic Networks (Richard-son and Domingos, 2006) are Markov networksconstructed corresponding to the grounding of rulesto knowledge bases.
In comparison, our proposedapproach is much more efficient by avoiding theharder problem of joint inferences and by leveragingefficient random walk schemes (Lao and Cohen,2010a).Below we describe our approach in greater detail,provide experimental evidence of its value forperforming inference in NELL?s knowledge base,and discuss implications of this work and directionsfor future research.2 ApproachIn this section, we first describe how we formulatelink (relation) prediction on a knowledge base asa ranking task.
Then we review the Path RankingAlgorithm (PRA) introduced by Lao and Cohen(2010b; 2010a).
After that, we describe twoimprovements to the PRA method to make it moresuitable for the task of link prediction in knowledgebases.
The first improvement helps PRA dealwith the large number of relations typical of largeknowledge bases.
The second improvement aims atimproving the quality of inference by applying lowvariance sampling.2.1 Learning with NELL?s Knowledge BaseFor each relationR in the knowledge base we train amodel for the link prediction task: given a concept a,find all other concepts b which potentially have therelationR(a, b).
This prediction is made based on anexisting knowledge base extracted imperfectly fromthe web.
Although a model can potentially benefitfrom predicting multiple relations jointly, such jointinference is beyond the scope of this work.532To ensure a reasonable number of traininginstances, we generate labeled training examplequeries from 48 relations which have more than100 instances in the knowledge base.
We createtwo tasks for each relation?i.e., predicting b givena and predicting a given b?
yielding 96 tasks inall.
Each node a which has relation R in theknowledge base with any other node is treated as atraining query, the actual nodes b in the knowledgebase known to satisfy R(a, b) are treated as labeledpositive examples, and any other nodes are treatedas negative examples.2.2 Path Ranking Algorithm ReviewWe now review the Path Ranking Algorithmintroduced by Lao and Cohen (2010b).
A relationpath P is defined as a sequence of relationsR1 .
.
.
R`, and in order to emphasize the typesassociated with each step, P can also be written asT0 R1???
.
.
.
R`??
T`, where Ti = range(Ri) =domain(Ri+1), and we also define domain(P ) ?T0, range(P ) ?
T`.
In the experiments in thispaper, there is only one type of node which we calla concept, which can be connected through differenttypes of relations.
In this notation, relations like ?theteam a certain player plays for?, and ?the league acertain player?s team is in?
can be expressed by thepaths below (respectively):P1 : conceptAtheletePlayesForTeam???????????????
conceptP2 : conceptAtheletePlayesForTeam???????????????
conceptTeamPlaysInLeagure??????????????
conceptFor any relation path P = R1 .
.
.
R` and aseed node s ?
domain(P ), a path constrainedrandom walk defines a distribution hs,P recursivelyas follows.
If P is the empty path, then definehs,P (e) ={1, if e = s0, otherwise (4)If P = R1 .
.
.
R` is nonempty, then let P ?
=R1 .
.
.
R`?1, and definehs,P (e) =?e?
?range(P ?
)hs,P ?(e?)
?
P (e|e?
;R`), (5)where P (e|e?
;R`) = R`(e?,e)|R`(e?,?
)| is the probability ofreaching node e from node e?
with a one step randomwalk with edge type R`.
R(e?, e) indicates whetherthere exists an edge with type R that connect e?
to e.More generally, given a set of paths P1, .
.
.
, Pn,one could treat each hs,Pi(e) as a path feature forthe node e, and rank nodes by a linear model?1hs,P1(e) + ?2hs,P2(e) + .
.
.
?nhs,Pn(e)where ?i are appropriate weights for the paths.
Thisgives a ranking of nodes e related to the query nodes by the following scoring functionscore(e; s) =?P?P`hs,P (e)?P , (6)where P` is the set of relation paths with length?
`.Given a relation R and a set of node pairs{(si, ti)} for which we know whether R(si, ti) istrue or not, we can construct a training datasetD = {(xi, yi)}, where xi is a vector of all thepath features for the pair (si, ti)?i.e., the j-thcomponent of xi is hsi,Pj (ti), and where yi is aboolean variable indicating whetherR(si, ti) is true.We then train a logistic function to predict theconditional probability P (y|x; ?).
The parametervector ?
is estimated by maximizing a regularizedform of the conditional likelihood of y given x. Inparticular, we maximize the objective functionO(?)
=?ioi(?)?
?1|?|1 ?
?2|?|2, (7)where ?1 controls L1-regularization to help struc-ture selection, and ?2 controls L2-regularizationto prevent overfitting.
oi(?)
is the per-instanceweighted log conditional likelihood given byoi(?)
= wi[yi ln pi + (1?
yi) ln(1?
pi)], (8)where pi is the predicted probability p(yi =1|xi; ?)
= exp(?Txi)1+exp(?Txi) , and wi is an importanceweight to each example.
A biased samplingprocedure selects only a small subset of negativesamples to be included in the objective (see (Lao andCohen, 2010b) for detail).2.3 Data-Driven Path FindingIn prior work with PRA, P` was defined as allrelation paths of length at most `.
When the numberof edge types is small, one can generate P` by533Table 1: Number of paths in PRA models of maximumpath length 3 and 4.
Averaged over 96 tasks.`=3 `=4all paths up to length L 15, 376 1, 906, 624+query support?
?
= 0.01 522 5016+ever reach a target entity 136 792+L1 regularization 63 271enumeration; however, for domains with a largenumber of edge types (e.g., a knowledge base), it isimpractical to enumerate all possible relation pathseven for small `.
For instance, if the number ofedge types related to each node type is 100, eventhe number of length three paths types easily reachesmillions.
For other domains like parsed naturallanguage sentences, useful relation paths can be aslong as ten relations (Minkov and Cohen, 2008).
Inthis case, even with smaller number of possible edgetypes, the total number of relation paths is still toolarge for systematic enumeration.In order to apply PRA to these domains, wemodify the path generation procedure in PRA toproduce only relation paths which are potentiallyuseful for the task.
Define a query s to be supportinga path P if hs,P (e) 6= 0 for any entity e. We requirethat any path node created during path finding needsto be supported by at least a fraction ?
of the trainingqueries si, as well as being of length no more than` (In the experiments, we set ?
= 0.01) We alsorequire that in order for a relation path to be includedin the PRA model, it must retrieve at least one targetentity ti in the training set.
As we can see fromTable 1, together these two constraints dramaticallyreduce the number of relation paths that need to beconsidered, relative to systematically enumeratingall possible relation paths.
L1 regularization reducesthe size of the model even more.The idea of finding paths that connects nodes in agraph is not new.
It has been embodied previously infirst-order learning systems (Richards and Mooney,1992) as well as N-FOIL, and relational databasesearching systems (Bhalotia et al, 2002).
Theseapproaches consider a single query during pathfinding.
In comparison, the data-driven path findingmethod we described here uses statistics from apopulation of queries, and therefore can potentiallydetermine the importance of a path more reliably.Table 2: Comparing PRA with RWR models.
MRRs andtraining times are averaged over 96 tasks.`=2 `=3MRR Time MRR TimeRWR(no train) 0.271 0.456RWR 0.280 3.7s 0.471 9.2sPRA 0.307 5.7s 0.516 15.4s2.4 Low-Variance SamplingLao and Cohen (2010a) previously showed thatsampling techniques like finger printing and particlefiltering can significantly speedup random walkwithout sacrificing retrieval quality.
However, thesampling procedures can induce a loss of diversityin the particle population.
For example, consider anode in the graph with just two out links with equalweights, and suppose we are required to generatetwo walkers starting from this node.
A disappointingresult is that with 50 percent chance both walkerswill follow the same branch, and leave the otherbranch with no probability mass.To overcome this problem, we apply a techniquecalled Low-Variance Sampling (LVS) (Thrun etal., 2005), which is commonly used in roboticsto improve the quality of sampling.
Instead ofgenerating independent samples from a distribution,LVS uses a single random number to generate allsamples, which are evenly distributed across thewhole distribution.
Note that given a distributionP (x), any number r in [0, 1] points to exactly onex value, namely x = argminj?m=1..j P (m) ?r.
Suppose we want to generate M samples fromP (x).
LVS first generates a random number r inthe interval [0,M?1].
Then LVS repeatedly addsthe fixed amount M?1 to r and chooses x valuescorresponding to the resulting numbers.3 ResultsThis section reports empirical results of applyingrandom walk inference to NELL?s knowledge baseafter the 165th iteration of its learning process.
Wefirst investigate PRA?s behavior by cross validationon the training queries.
Then we compare PRA andN-FOIL?s ability to reliably infer new beliefs, byleveraging the Amazon Mechanical Turk service.5343.1 Cross Validation on the Training QueriesRandom Walk with Restart (RWR) (also calledpersonalized PageRank (Haveliwala, 2002)) is ageneral-purpose graph proximity measure whichhas been shown to be fairly successful for manytypes of tasks.
We compare PRA to two versionsof RWR on the 96 tasks of link prediction withNELL?s knowledge base.
The two baseline methodsare an untrained RWR model and a trained RWRmodel as described by Lao and Cohen (2010b).
(Inbrief, in the trained RWR model, the walker willprobabilistically prefer to follow edges associatedwith different labels, where the weight for each edgelabel is chosen to minimize a loss function, such asEquation 7.
In the untrained model, edge weightsare uniform.)
We explored a range of values forthe regularization parameters L1 and L2 using crossvalidation on the training data, and we fix bothL1 and L2 parameters to 0.001 for all tasks.
Themaximum path length is fixed to 3.3Table 2 compares the three methods using5-fold cross validation and the Mean ReciprocalRank (MRR)4 measure, which is defined as theinverse rank of the highest ranked relevant resultin a set of results.
If the the first returnedresult is relevant, then MRR is 1.0, otherwise,it is smaller than 1.0.
Supervised training cansignificantly improve retrieval quality (p-value=9 ?10?8 comparing untrained and trained RWR), andleveraging path information can produce furtherimprovement (p-value=4?
10?4 comparing trainedRWR with PRA).
The average training time for apredicate is only a few seconds.We also investigate the effect of low-variancesampling on the quality of prediction.
Figure 2 com-pares independent and low variance sampling whenapplied to finger printing and particle filtering (Laoand Cohen, 2010a).
The horizontal axis correspondsto the speedup of random walk compared withexact inference, and the vertical axis measures thequality of prediction by MRR with three fold crossvalidation on the training query set.
Low-variance3Results with maximum length 4 are not reported here.Generally models with length 4 paths produce slightly betterresults, but are 4-5 times slower to train4For a set of queries Q,MRR = 1|Q|?q?Q1rank of the first correct answer for q0.40.50 1 2 3 4 5MRRRandom Walk SpeedupExactIndependent FingerprintingLow Variance FingerprintingIndependent FilteringLow Variance Filtering10k1k10010k1k100kFigure 2: Compare inference speed and quality over 96tasks.
The speedup is relative to exact inference, which ison average 23ms per query.sampling can improve prediction for both fingerprinting and particle filtering.
The numbers on thecurves indicate the number of particles (or walkers).When using a large number of particles, the particlefiltering methods converge to the exact inference.Interestingly, when using a large number of walkers,the finger printing methods produce even betterprediction quality than exact inference.
Lao andCohen noticed a similar improvement on retrievaltasks, and conjectured that it is because the samplinginference imposes a regularization penalty on longerrelation paths (2010a).3.2 Evaluation by Mechanical TurkThe cross-validation result above assumes that theknowledge base is complete and correct, whichwe know to be untrue.
To accurately comparePRA and N-FOIL?s ability to reliably infer newbeliefs from an imperfect knowledge base, weuse human assessments obtained from AmazonMechanical Turk.
To limit labeling costs, andsince our goal is to improve the performance ofNELL, we do not include RWR-based approachesin this comparison.
Among all the 24 functionalpredicates, N-FOIL discovers confident rules for8 of them (it produces no result for the other 16predicates).
Therefore, we compare the qualityof PRA to N-FOIL on these 8 predicates only.Among all the 72 non-functional predicates?which535Table 3: The top two weighted PRA paths for tasks on which N-FOIL discovers confident rules.
c stands for concept.ID PRA Path (Comment)athletePlaysForTeam1 c athletePlaysInLeague????????????????
c leaguePlayers??????????
c athletePlaysForTeam????????????????
c (teams with many players in the athlete?s league)2 c athletePlaysInLeague????????????????
c leagueTeams??????????
c teamAgainstTeam?????????????
c (teams that play against many teams in the athlete?s league)athletePlaysInLeague3 c athletePlaysSport?????????????
c players??????
c athletePlaysInLeague????????????????
c (the league that players of a certain sport belong to)4 c isa???
c isa?1?????
c athletePlaysInLeague????????????????
c (popular leagues with many players)athletePlaysSport5 c isa???
c isa?1?????
c athletePlaysSport?????????????
c (popular sports of all the athletes)6 c athletePlaysInLeague????????????????
c superpartOfOrganization??????????????????
c teamPlaysSport????????????
c (popular sports of a certain league)stadiumLocatedInCity7 c stadiumHomeTeam??????????????
c teamHomeStadium??????????????
c stadiumLocatedInCity????????????????
c (city of the stadium with the same team)8 c latitudeLongitude?????????????
c latitudeLongitudeOf???????????????
c stadiumLocatedInCity????????????????
c (city of the stadium with the same location)teamHomeStadium9 c teamPlaysInCity?????????????
c cityStadiums??????????
c (stadiums located in the same city with the query team)10 c teamMember??????????
c athletePlaysForTeam????????????????
c teamHomeStadium??????????????
c (home stadium of teams which share players with the query)teamPlaysInCity11 c teamHomeStadium??????????????
c stadiumLocatedInCity????????????????
c (city of the team?s home stadium)12 c teamHomeStadium??????????????
c stadiumHomeTeam??????????????
c teamPlaysInCity?????????????
c (city of teams with the same home stadium as the query)teamPlaysInLeague13 c teamPlaysSport????????????
c players??????
c athletePlaysInLeague????????????????
c (the league that the query team?s members belong to)14 c teamPlaysAgainstTeam?????????????????
c teamPlaysInLeague??????????????
c (the league that the query team?s competing team belongs to)teamPlaysSport15 c isa???
c isa?1?????
c teamPlaysSport????????????
c (sports played by many teams)16 c teamPlaysInLeague??????????????
c leagueTeams??????????
c teamPlaysSport????????????
c (the sport played by other teams in the league)Table 4: Amazon Mechanical Turk evaluation for the promoted knowledge.
Using paired t-test at task level, PRA isnot statistically different from N-FOIL for p@10 (p-value=0.3), but is significantly better for p@100 (p-value=0.003)PRA N-FOILTask Pmajority #Paths p@10 p@100 p@1000 #Rules #Query p@10 p@100 p@1000athletePlaysForTeam 0.07 125 0.4 0.46 0.66 1(+1) 7 0.6 0.08 0.01athletePlaysInLeague 0.60 15 1.0 0.84 0.80 3(+30) 332 0.9 0.80 0.24athletePlaysSport 0.73 34 1.0 0.78 0.70 2(+30) 224 1.0 0.82 0.18stadiumLocatedInCity 0.05 18 0.9 0.62 0.54 1(+0) 25 0.7 0.16 0.00teamHomeStadium 0.02 66 0.3 0.48 0.34 1(+0) 2 0.2 0.02 0.00teamPlaysInCity 0.10 29 1.0 0.86 0.62 1(+0) 60 0.9 0.56 0.06teamPlaysInLeague 0.26 36 1.0 0.70 0.64 4(+151) 30 0.9 0.18 0.02teamPlaysSport 0.42 21 0.7 0.60 0.62 4(+86) 48 0.9 0.42 0.02average 0.28 43 0.79 0.668 0.615 91 0.76 0.38 0.07teamMember 0.01 203 0.8 0.64 0.48companiesHeadquarteredIn 0.05 42 0.6 0.54 0.60publicationJournalist 0.02 25 0.7 0.70 0.64producedBy 0.19 13 0.5 0.58 0.68 N-FOIL does not produce resultscompetesWith 0.19 74 0.6 0.56 0.72 for non-functional predicateshasOfficeInCity 0.03 262 0.9 0.84 0.60teamWonTrophy 0.24 56 0.5 0.50 0.46worksFor 0.13 62 0.6 0.60 0.74average 0.11 92 0.650 0.620 0.615536N-FOIL cannot be applied to?PRA exhibits a widerange of performance in cross-validation.
The are 43tasks for which PRA obtains MRR higher than 0.4and builds a model with more than 10 path features.We randomly sampled 8 of these predicates to beevaluated by Amazon Mechanical Turk.Table 3 shows the top two weighted PRA featuresfor each task on which N-FOIL can successfullylearn rules.
These PRA rules can be categorized intobroad coverage rules which behave like priors overcorrect answers (e.g.
1-2, 4-6, 15), accurate ruleswhich leverage specific relation sequences (e.g.
9,11, 14), rules which leverage information about thesynonyms of the query node (e.g.
7-8, 10, 12),and rules which leverage information from a localneighborhood of the query node (e.g.
3, 12-13, 16).The synonym paths are useful, because an entitymay have multiple names on the web.
We findthat all 17 general rules (no specialization) learnedby N-FOIL can be expressed as length two relationpaths such as path 11.
In comparison, PRA exploresa feature space with many length three paths.For each relation R to be evaluated, we generatetest queries s which belong to domain(R).
Querieswhich appear in the training set are excluded.
Foreach query node s, we applied a trained model(either PRA or N-FOIL) to generate a ranked listof candidate t nodes.
For PRA, the candidatesare sorted by their scores as in Eq.
(6).
ForN-FOIL, the candidates are sorted by the estimatedaccuracies of the rules as in Eq.
(2) (which generatethe candidates).
Since there are about 7 thousand(and 13 thousand) test queries s for each functional(and non-functional) predicate R, and there are(potentially) thousands of candidates t returned foreach query s, we cannot evaluate all candidates ofall queries.
Therefore, we first sort the queries s foreach predicate R by the scores of their top rankedcandidate t in descending order, and then calculateprecisions at top 10, 100 and 1000 positions for thelist of result R(sR,1, tR,11 ), R(sR,2, tR,21 ), ..., wheresR,1 is the first query for predicate R, tR,11 is its firstcandidate, sR,2 is the second query for predicate R,tR,21 is its first candidate, so on and so forth.
Toreduce the labeling load, we judge all top 10 queriesfor each predicate, but randomly sample 50 out ofthe top 100, and randomly sample 50 out of theTable 5: Comparing Mechanical Turk workers?
votedassessments with our gold standard labels based on 100samples.AMT=F AMT=TGold=F 25% 15%Gold=T 11% 49%top 1000.
Each belief is evaluated by 5 workersat Mechanical Turk, who are given assertions like?Hines Ward plays for the team Steelers?, as wellas Google search links for each entity, and thecombination of both entities.
Statistics showsthat the workers spend on average 25 seconds tojudge each belief.
We also remove some workers?judgments which are obviously incorrect5.
Wesampled 100 beliefs, and compared their voted resultto gold-standard labels produced by one author ofthis paper.
Table 5 shows that 74% of the time theworkers?
voted result agrees with our judgement.Table 4 shows the evaluation result.
ThePmajority column shows for each predicate theaccuracy achieved by the majority prediction: givena query R(a, ?
), predict the b that most oftensatisfies R over all possible a in the knowledgebase.
Thus, the higher Pmajority is, the simplerthe task.
Predicting the functional predicatesis generally easier predicting the non-functionalpredicates.
The #Query column shows the numberof queries on which N-FOIL is able to match anyof its rules, and hence produce a candidate belief.For most predicates, N-FOIL is only able to produceresults for at most a few hundred queries.
Incomparison, PRA is able to produce results for 6,599queries on average for each functional predicate, and12,519 queries on average for each non-functionalpredicate.
Although the precision at 10 (p@10) ofN-FOIL is comparable to that of PRA, precisionat 10 and at 1000 (p@100 and p@1000) are muchlower6.The #Path column shows the number of pathslearned by PRA, and the #Rule column shows thenumber of rules learned by N-FOIL, with the num-bers before brackets correspond to unspecializedrules, and the numbers in brackets correspond to5Certain workers label all the questions with the sameanswer6If a method makes k predictions, and k < n, then p@n isthe number correct out of the k predictions, divided by n537specialized rules.
Generally, specialized rules havemuch smaller recall than unspecialized rules.
There-fore, the PRA approach achieves high recall partiallyby combining a large number of unspecialized paths,which correspond to unspecialized rules.
However,learning more accurate specialized paths is part ofour future work.A significant advantage of PRA over N-FOIL isthat it can be applied to non-functional predicates.The last eight rows of Table 4 show PRA?sperformance on eight of these predicates.
Comparedto the result on functional predicates, precisionsat 10 and at 100 of non-functional predicatesare slightly lower, but precisions at 1000 arecomparable.
We note that for some predicatesprecision at 1000 is better than at 100.
Aftersome investigation we found that for many relations,the top portion of the result list is more diverse:i.e.
showing products produced by different com-panies, journalist working at different publications.While the lower half of the result list is morehomogeneous: i.e.
showing relations concentratedon one or two companies/publications.
On theother hand, through the process of labeling theMechanical Turk workers seem to build up a priorabout which company/publication is likely to havecorrect beliefs, and their judgments are positivelybiased towards these companies/publications.
Thesetwo factors combined together result in positive biastowards the lower portion of the result list.
In futurework we hope to design a labeling strategy whichavoids this bias.4 Conclusions and Future WorkWe have shown that a soft inference procedure basedon a combination of constrained, weighted, randomwalks through the knowledge base graph can beused to reliably infer new beliefs for the knowledgebase.
We applied this approach to a knowledgebase of approximately 500,000 beliefs extractedimperfectly from the web by NELL.
This newsystem improves significantly over NELL?s earlierHorn-clause learning and inference method: itobtains nearly double the precision at rank 100.
Theinference and learning are both very efficient?ourexperiment shows that the inference time is as fastas 10 milliseconds per query on average, and thetraining for a predicate takes only a few seconds.There are several prominent directions for futurework.
First, inference starting from both the querynodes and target nodes (Richards and Mooney,1992) can be much more efficient in discoveringlong paths than just inference from the query nodes.Second, inference starting from the target nodesof training queries is a potential way to discoverspecialized paths (with grounded nodes).
Third,generalizing inference paths to inference trees orgraphs can produce more expressive random walkinference models.
Overall, we believe that randomwalk is a promising way to scale up relationallearning to domains with very large data sets.AcknowledgmentsThis work was supported by NIH under grantR01GM081293, by NSF under grant IIS0811562,by DARPA under awards FA8750-08-1-0009 andAF8750-09-C-0179, and by a gift from Google.We thank Geoffrey J. Gordon for the suggestionof applying low variance sampling to random walkinference.
We also thank Bryan Kisiel for help withthe NELL system.ReferencesEugene Agichtein and Luis Gravano.
2000.
Snowball:extracting relations from large plain-text collections.In Proceedings of the fifth ACM conference on Digitallibraries - DL ?00, pages 85?94, New York, New York,USA.
ACM Press.Michele Banko, Michael J. Cafarella, Stephen Soderland,Matthew Broadhead, and Oren Etzioni.
2007.
OpenInformation Extraction from the Web.
In IJCAI, pages2670?2676.Gaurav Bhalotia, Arvind Hulgeri, Charuta Nakhe,Soumen Chakrabarti, and S. Sudarshan.
2002.Keyword searching and browsing in databases usingbanks.
ICDE, pages 431?440.Avrim Blum and Tom Mitchell.
1998.
Combininglabeled and unlabeled data with co-training.
InProceedings of the eleventh annual conference onComputational learning theory - COLT?
98, pages92?100, New York, New York, USA.
ACM Press.MJ Cafarella, M Banko, and O Etzioni.
2006.
RelationalWeb Search.
In WWW.Jamie Callan and Mark Hoy.
2009.
Clueweb09 data set.http://boston.lti.cs.cmu.edu/Data/clueweb09/.Andrew Carlson, Justin Betteridge, Bryan Kisiel, BurrSettles, Estevam R. Hruschka Jr., and Tom M.538Mitchell.
2010.
Toward an Architecture forNever-Ending Language Learning.
In AAAI.William W. Cohen and David Page.
1995.
Polyno-mial learnability and inductive logic programming:Methods and results.
New Generation Comput.,13(3&4):369?409.Oren Etzioni, Michael Cafarella, Doug Downey, Ana-maria Popescu, Tal Shaked, Stephen Soderl, Daniel S.Weld, and Er Yates.
2005.
Unsupervised named-entity extraction from the web: An experimental study.Artificial Intelligence, 165:91?134.Taher H. Haveliwala.
2002.
Topic-sensitive pagerank.
InWWW, pages 517?526.Alpa Jain and Patrick Pantel.
2010.
Factrank: Randomwalks on a web of facts.
In COLING, pages 501?509.Ni Lao and William W. Cohen.
2010a.
Fast query exe-cution for retrieval models based on path-constrainedrandom walks.
KDD.Ni Lao and William W. Cohen.
2010b.
Relationalretrieval using a combination of path-constrainedrandom walks.
Machine Learning.Einat Minkov and William W. Cohen.
2008.
Learninggraph walk based similarity measures for parsed text.EMNLP, pages 907?916.Patrick Pantel and Marco Pennacchiotti.
2006.
Espresso:Leveraging Generic Patterns for Automatically Har-vesting Semantic Relations.
In ACL.J.
Ross Quinlan and R. Mike Cameron-Jones.
1993.FOIL: A Midterm Report.
In ECML, pages 3?20.Bradley L. Richards and Raymond J. Mooney.
1992.Learning relations by pathfinding.
In Proceedingsof the Tenth National Conference on Artificial Intel-ligence (AAAI-92), pages 50?55, San Jose, CA, July.Matthew Richardson and Pedro Domingos.
2006.Markov logic networks.
Machine Learning.Stefan Schoenmackers, Oren Etzioni, and Daniel S.Weld.
2008.
Scaling Textual Inference to the Web.In EMNLP, pages 79?88.Rion Snow, Daniel Jurafsky, and Andrew Y. Ng.
2006.Semantic Taxonomy Induction from HeterogenousEvidence.
In ACL.Sebastian Thrun, Wolfram Burgard, and Dieter Fox.2005.
Probabilistic Robotics (Intelligent Robotics andAutonomous Agents).
The MIT Press.Alexander Yates, Michele Banko, Matthew Broadhead,Michael J. Cafarella, Oren Etzioni, and StephenSoderland.
2007.
TextRunner: Open InformationExtraction on the Web.
In HLT-NAACL (Demonstra-tions), pages 25?26.John M. Zelle, Cynthia A. Thompson, Mary ElaineCaliff, and Raymond J. Mooney.
1995.
Inducinglogic programs without explicit negative examples.In Proceedings of the Fifth International Workshopon Inductive Logic Programming (ILP-95), pages403?416, Leuven, Belgium.539
