Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 783?792,Honolulu, October 2008. c?2008 Association for Computational LinguisticsA Generative Model for Parsing Natural Language to MeaningRepresentationsWei Lu1, Hwee Tou Ng1,2, Wee Sun Lee1,21Singapore-MIT Alliance2Department of Computer ScienceNational University of Singaporeluwei@nus.edu.sg{nght,leews}@comp.nus.edu.sgLuke S. ZettlemoyerCSAILMassachusetts Institute of Technologylsz@csail.mit.eduAbstractIn this paper, we present an algorithm forlearning a generative model of natural lan-guage sentences together with their for-mal meaning representations with hierarchi-cal structures.
The model is applied to thetask of mapping sentences to hierarchical rep-resentations of their underlying meaning.
Weintroduce dynamic programming techniquesfor efficient training and decoding.
In exper-iments, we demonstrate that the model, whencoupled with a discriminative reranking tech-nique, achieves state-of-the-art performancewhen tested on two publicly available cor-pora.
The generative model degrades robustlywhen presented with instances that are differ-ent from those seen in training.
This allowsa notable improvement in recall compared toprevious models.1 IntroductionTo enable computers to understand natural humanlanguage is one of the classic goals of research innatural language processing.
Recently, researchershave developed techniques for learning to map sen-tences to hierarchical representations of their under-lying meaning (Wong and Mooney, 2006; Kate andMooney, 2006).One common approach is to learn some form ofprobabilistic grammar which includes a list of lexi-cal items that models the meanings of input wordsand also includes rules for combining lexical mean-ings to analyze complete sentences.
This approachperforms well but is constrained by the use of a sin-gle, learned grammar that contains a fixed set oflexical entries and productions.
In practice, sucha grammar may lack the rules required to correctlyparse some of the new test examples.In this paper, we develop an alternative approachthat learns a model which does not make use ofan explicit grammar but, instead, models the cor-respondence between sentences and their meaningswith a generative process.
This model is definedover hybrid trees whose nodes include both natu-ral language words and meaning representation to-kens.
Inspired by the work of Collins (2003), thegenerative model builds trees by recursively creatingnodes at each level according to a Markov process.This implicit grammar representation leads to flexi-ble learned models that generalize well.
In practice,we observe that it can correctly parse a wider rangeof test examples than previous approaches.The generative model is learned from data thatconsists of sentences paired with their meaning rep-resentations.
However, there is no explicit labelingof the correspondence between words and meaningtokens that is necessary for building the hybrid trees.This creates a challenging, hidden-variable learningproblem that we address with the use of an inside-outside algorithm.
Specifically, we develop a dy-namic programming parsing algorithm that leads toO(n3m) time complexity for inference, where n isthe sentence length and m is the size of meaningstructure.
This approach allows for efficient train-ing and decoding.In practice, we observe that the learned generativemodels are able to assign a high score to the correctmeaning for input sentences, but that this correctmeaning is not always the highest scoring option.783To address this problem, we use a simple rerank-ing approach to select a parse from a k-best list ofparses.
This pipelined approach achieves state-of-the-art performance on two publicly available cor-pora.
In particular, the flexible generative modelleads to notable improvements in recall, the totalpercentage of sentences that are correctly parsed.2 Related WorkIn Section 9, we will compare performance withthe three existing systems that were evaluated onthe same data sets we consider.
SILT (Kate et al,2005) learns deterministic rules to transform eithersentences or their syntactic parse trees to meaningstructures.
WASP (Wong and Mooney, 2006) is asystem motivated by statistical machine translationtechniques.
It acquires a set of synchronous lexicalentries by running the IBM alignment model (Brownet al, 1993) and learns a log-linear model to weightparses.
KRISP (Kate and Mooney, 2006) is a dis-criminative approach where meaning representationstructures are constructed from the natural languagestrings hierarchically.
It is built on top of SVMstructwith string kernels.Additionally, there is substantial related researchthat is not directly comparable to our approach.Some of this work requires different levels of super-vision, including labeled syntactic parse trees (Geand Mooney, 2005; Ge and Mooney, 2006).
Othersdo not perform lexical learning (Tang and Mooney,2001).
Finally, recent work has explored learningto map sentences to lambda-calculus meaning rep-resentations (Wong and Mooney, 2007; Zettlemoyerand Collins, 2005; Zettlemoyer and Collins, 2007).3 Meaning RepresentationWe restrict our meaning representation (MR) for-malism to a variable free version as presented in(Wong and Mooney, 2006; Kate et al, 2005).A training instance consists of a natural languagesentence (NL sentence) and its corresponding mean-ing representation structure (MR structure).
Con-sider the following instance taken from the GEO-QUERY corpus (Kate et al, 2005):The NL sentence ?How many states donot have rivers ??
consists of 8 words, in-cluding punctuation.
The MR is a hierarchical treeQUERY : answer (NUM)NUM : count (STATE)STATE : exclude (STATE STATE)STATE : state (all) STATE : loc 1 (RIVER)RIVER : river (all)Figure 1: An example MR structurestructure, as shown in Figure 1.Following an inorder traversal of this MR tree, wecan equivalently represent it with the following listof meaning representation productions (MR produc-tions):(0) QUERY : answer (NUM)(1) NUM : count (STATE)(2) STATE : exclude (STATE1 STATE2)(3) STATE : state (all)(4) STATE : loc 1 (RIVER)(5) RIVER : river (all)Each such MR production consists of three com-ponents: a semantic category, a function symbolwhich can be omitted (considered empty), and a listof arguments.
An argument can be either a child se-mantic category or a constant.
Take production (1)for example: it has a semantic category ?NUM?, afunction symbol ?count?, and a child semantic cate-gory ?STATE?
as its only argument.
Production (5)has ?RIVER?
as its semantic category, ?river?
as thefunction symbol, and ?all?
is a constant.4 The Generative ModelWe describe in this section our proposed generativemodel, which simultaneously generates a NL sen-tence and an MR structure.We denote a single NL word as w, a contiguoussequence of NL words as w, and a complete NLsentence as w?.
In the MR structure, we denote asemantic category as M. We denote a single MRproduction as ma, or Ma : p?
(Mb,Mc), where Mais the semantic category for this production, p?
is thefunction symbol, and Mb,Mc are the child semanticcategories.
We denote ma as an MR structure rootedby an MR production ma, and m?a an MR structurefor a complete sentence rooted by an MR productionma.The model generates a hybrid tree that representsa sentence w?
= w1 .
.
.
w2 .
.
.
paired with an MRstructure m?a rooted by ma.784Mamaw1 Mbmb.
.
.
.
.
.w2 Mcmc.
.
.
.
.
.Figure 2: The generation processFigure 2 shows part of a hybrid tree that is gen-erated as follows.
Given a semantic category Ma,we first pick an MR production ma that has the formMa : p?
(Mb,Mc), which gives us the function sym-bol p?
as well as the child semantic categories Mband Mc.
Next, we generate the hybrid sequence ofchild nodes w1 Mb w2 Mc, which consists of NLwords and semantic categories.After that, two child MR productions mb and mcare generated.
These two productions will in turngenerate other hybrid sequences and productions, re-cursively.
This process produces a hybrid tree T ,whose nodes are either NL words or MR produc-tions.
Given this tree, we can recover a NL sentencew by recording the NL words visited in depth-firsttraversal order and can recover an MR structure mby following a tree-specific traversal order, definedby the hybrid-patterns we introduce below.
Figure 3gives a partial hybrid tree for the training examplefrom Section 3.
Note that the leaves of a hybrid treeare always NL tokens.. .
.STATESTATE : exclude (STATE STATE)STATESTATE : state(all)statesdo not STATESTATE : loc 1(RIVER)have RIVERRIVER : river(all)riversFigure 3: A partial hybrid treeWith several independence assumptions, theprobability of generating?w?, m?,T ?
is defined as:P(w?, m?,T ) = P(Ma) ?
P(ma|Ma) ?
P(w1 Mb w2 Mc|ma)?P(mb|ma, arg = 1) ?
P(.
.
.
|mb)?P(mc|ma, arg = 2) ?
P(.
.
.
|mc) (1)where ?arg?
refers to the position of the child se-mantic category in the argument list.Motivated by Collins?
syntactic parsing models(Collins, 2003), we consider the generation processfor a hybrid sequence from an MR production as aMarkov process.Given the assumption that each MR productionhas at most two semantic categories in its arguments(any production can be transformed into a sequenceof productions of this form), Table 1 includes the listof all possible hybrid patterns.# RHS Hybrid Pattern # Patterns0 m ?
w 11 m ?
[w]Y[w] 42 m ?
[w]Y[w]Z[w] 8m ?
[w]Z[w]Y[w] 8Table 1: A list of hybrid patterns, [] denotes optionalIn this table, m is an MR production, Y and Zare respectively the first and second child seman-tic category in m?s argument list.
The symbol wrefers to a contiguous sequence of NL words, andanything inside [] can be optionally omitted.
Thelast row contains hybrid patterns that reflect reorder-ing of one production?s child semantic categoriesduring the generation process.
For example, con-sider the case that the MR production STATE :exclude (STATE1 STATE2) generates a hybrid se-quence STATE1 do not STATE2, the hybrid patternm ?
YwZ is associated with this generation step.For the example hybrid tree in Figure 2, we candecompose the probability for generating the hybridsequence as follows:P(w1 Mb w2 Mc|ma) = P(m ?
wYwZ|ma) ?
P(w1|ma)?P(Mb|ma, w1) ?
P(w2|ma, w1,Mb)?P(Mc|ma, w1,Mb, w2) ?
P(END|ma, w1,Mb, w2,Mc) (2)Note that unigram, bigram, or trigram assump-tions can be made here for generating NL words andsemantic categories.
For example, under a bigramassumption, the second to last term can be writtenas P(Mc|ma, w1,Mb, w2) ?
P(Mc|ma, wk2), wherewk2 is the last word in w2.
We call such additionalinformation that we condition on, the context.Note that our generative model is different fromthe synchronous context free grammars (SCFG) ina number of ways.
A standard SCFG produces acorrespondence between a pair of trees while ourmodel produces a single hybrid tree that represents785the correspondence between a sentence and a tree.Also, SCFGs use a finite set of context-free rewriterules to define the model, where the rules are possi-bly weighted.
In contrast, we make use of the moreflexible Markov models at each level of the genera-tive process, which allows us to potentially producea far wider range of possible trees.5 Parameter EstimationThere are three categories of parameters used in themodel.
The first category of parameters modelsthe generation of new MR productions from theirparent MR productions: e.g., P(mb|ma, arg = 1);the second models the generation of a hybrid se-quence from an MR production: e.g., P(w1|ma),P(Mb|ma, w1); the last models the selection of a hy-brid pattern given an MR production, e.g., P(m ?wY|ma).
We will estimate parameters from all cate-gories, with the following constraints:1.?m?
?
(m?|m j, arg=k)=1 for all j and k = 1, 2.These parameters model the MR structures, andcan be referred to as MR model parameters.2.
?t ?
(t|m j,?
)=1 for all j, where t is a NL word,the ?END?
symbol, or a semantic category.
?is the context associated with m j and t.These parameters model the emission of NLwords, the ?END?
symbol, and child semanticcategories from an MR production.
We callthem emission parameters.3.
?r ?
(r|m j) = 1 for all j, where r is a hybridpattern listed in Table 1.These parameters model the selection of hybridpatterns.
We name them pattern parameters.With different context assumptions, we reach dif-ferent variations of the model.
In particular, we con-sider three assumptions, as follows:Model I We make the following assumption:?
(tk|m j,?)
= P(tk|m j) (3)where tk is a semantic category or a NL word, andm j is an MR production.In other words, generation of the next NL worddepends on its direct parent MR production only.Such a Unigram Model may help in recall (the num-ber of correct outputs over the total number of in-puts), because it requires the least data to estimate.Model II We make the following assumption:?
(tk|m j,?)
= P(tk|m j, tk?1) (4)where tk?1 is the semantic category or NL word tothe left of tk, i.e., the previous semantic category orNL word.In other words, generation of the next NL worddepends on its direct parent MR production as wellas the previously generated NL word or semanticcategory only.
This model is also referred to as Bi-gram Model.
This model may help in precision (thenumber of correct outputs over the total number ofoutputs), because it conditions on a larger context.Model III We make the following assumption:?
(tk|m j,?)
=12 ?
(P(tk|m j) + P(tk|m j, tk?1))(5)We can view this model, called the MixgramModel, as an interpolation between Model I and II.This model gives us a balanced score for both preci-sion and recall.5.1 Modeling Meaning RepresentationThe MR model parameters can be estimated inde-pendently from the other two.
These parameters canbe viewed as the ?language model?
parameters forthe MR structure, and can be estimated directly fromthe corpus by simply reading off the counts of occur-rences of MR productions in MR structures over thetraining corpus.
To resolve data sparseness problem,a variant of the bigram Katz Back-Off Model (Katz,1987) is employed here for smoothing.5.2 Learning the Generative ParametersLearning the remaining two categories of parametersis more challenging.
In a conventional PCFG pars-ing task, during the training phase, the correct cor-respondence between NL words and syntactic struc-tures is fully accessible.
In other words, there is asingle deterministic derivation associated with eachtraining instance.
Therefore model parameters canbe directly estimated from the training corpus bycounting.
However, in our task, the correct corre-spondence between NL words and MR structures isunknown.
Many possible derivations could reachthe same NL-MR pair, where each such derivationforms a hybrid tree.786The hybrid tree is constructed using hidden vari-ables and estimated from the training set.
An effi-cient inside-outside style algorithm can be used formodel estimation, similar to that used in (Yamadaand Knight, 2001), as discussed next.5.2.1 The Inside-Outside Algorithm with EMIn this section, we discuss how to estimate theemission and pattern parameters with the Expecta-tion Maximization (EM) algorithm (Dempster et al,1977), by using an inside-outside (Baker, 1979) dy-namic programming approach.Denote ni ?
?mi, wi?
as the i-th training instance,where mi and wi are the MR structure and the NLsentence of the i-th instance respectively.
We alsodenote nv ?
?mv, wv?
as an aligned pair of MRsubstructure and contiguous NL substring, wherethe MR substructure rooted by MR production mvwill correspond to (i.e., hierarchically generate) theNL substring wv.
The symbol h is used to de-note a hybrid sequence, and the function Parent(h)gives the unique MR substructure-NL subsequencepair which can be decomposed as h. Parent(nv) re-turns the set of all possible hybrid sequences un-der which the pair nv can be generated.
Similarly,Children(h) gives the NL-MR pairs that appear di-rectly below the hybrid sequence h in a hybrid tree,and Children(n) returns the set of all possible hybridsequences that n can be decomposed as.
Figure 4gives a packed tree structure representing the rela-tions between the entities.hp1 ?
Parent(nv) .
.
.
.
.
.
hpm ?
Parent(nv)nv?
?
?mv?
, wv?
?
nv ?
?mv, wv?hc1 ?
Children(nv) .
.
.
.
.
.
hcn ?
Children(nv)Hybrid Sequence ContainsCan be Decomposed AsFigure 4: A packed tree structure representing the relationsbetween hybrid sequences and NL-MR pairsThe formulas for computing inside and outsideprobabilities as well as the equations for updatingparameters are given in Figure 5.
We use a CKY-style parse chart for tracking the probabilities.5.2.2 SmoothingIt is reasonable to believe that different MR pro-ductions that share identical function symbols arelikely to generate NL words with similar distribu-tion, regardless of semantic categories.
For example,The inside (?)
probabilities are defined as?
If nv ?
?mv, wv?
is leaf?
(nv) = P(wv|mv) (6)?
If nv ?
?mv, wv?
is not leaf?
(nv) =?h?Children(nv)(P(h|mv) ??nv??Children(h)?(nv?
))(7)The outside (?)
probabilities are defined as?
If nv ?
?mv, wv?
is root?
(nv) = 1 (8)?
If nv ?
?mv, wv?
is not root?
(nv) =?h?Parent(nv)(?(Parent(h))?P(h|Parent(h))??nv??Children(h),v?,v?(nv?
))(9)Parameter Update?
Update the emission parameterThe count ci(t, mv,?k), where t is a NL wordor a semantic category, for an instance pair ni ?
?mi, wi?
:ci(t, mv,?k) =1?
(ni) ??
(t,mv ,?k) in h?Children(mv)(?
(niv)?P(h|mv) ??niv??Children(h)?(niv?
))The emission parameter is re-estimated as:??
(t|mv,?k) =?i ci(t, mv,?k)?t?
?i ci(t?, mv,?k)(10)?
Update the pattern parameterThe count ci(r, mv), where r is a hybrid pattern,for an instance pair ni ?
?mi, wi?
:ci(r, mv) =1?
(ni) ??
(r,mv) in h?Children(mv)(?
(niv)?P(h|mv) ??niv??Children(h)?(niv?
))The pattern parameter is re-estimated as:??
(r|mv) =?i ci(r, mv)?r?
?i ci(r?, mv)(11)Figure 5: The inside/outside formulas as well as updateequations for EMRIVER : largest (RIVER) and CITY : largest (CITY)are both likely to generate the word ?biggest?.In view of this, a smoothing technique is de-ployed.
We assume half of the time words can787be generated from the production?s function symbolalone if it is not empty.
Mathematically, assumingma with function symbol pa, for a NL word or se-mantic category t, we have:?(t|ma,?)
={ ?e(t|ma,?)
If pa is empty(?e(t|ma,?)
+ ?e(t|pa,?
))/2 otherwisewhere ?e models the generation of t from an MRproduction or its function symbol, together with thecontext ?.6 A Dynamic Programming Algorithm forInside-Outside ComputationThough the inside-outside approach already em-ploys packed representations for dynamic program-ming, a naive implementation of the inference algo-rithm will still require O(n6m) time for 1 EM iter-ation, where n and m are the length of the NL sen-tence and the size of the MR structure respectively.This is not very practical as in one of the corpora welook at, n and m can be up to 45 and 20 respectively.In this section, we develop an efficient dynamicprogramming algorithm that enables the inferenceto run in O(n3m) time.
The idea is as follows.
In-stead of treating each possible hybrid sequence asa separate rule, we efficiently aggregate the alreadycomputed probability scores for hybrid sequencesthat share identical hybrid patterns.
Such aggregatedscores can then be used for subsequent computa-tions.
By doing this, we can effectively avoid a largeamount of redundant computations.
The algorithmsupports both unigram and bigram context assump-tions.
For clarity and ease of presentation, we pri-marily make the unigram assumption throughout ourdiscussion.We use ?
(mv, wv) to denote the inside probabil-ity for mv-wv pair, br[mv, wv, c] to denote the aggre-gated probabilities for the MR sub-structure mv togenerate all possible hybrid sequences based on wvwith pattern r that covers its c-th child only.
In addi-tion, we use w(i, j) to denote a subsequence of w withstart index i (inclusive) and end index j (exclusive).We also use ?r~mv, wv to denote the aggregated in-side probability for the pair ?mv, wv?, if the hybridpattern is restricted to r only.
By definition we have:?
(mv, wv) =?r?(r|mv)?
?r~mv, wv??
(END|mv) (12)Relations between ?r and br can also be estab-lished.
For example, if mv has one child semanticcategory, we have:?m?wY~mv, wv = bm?wY[mv, wv, 1] (13)For the case when mv has two child semantic cat-egories as arguments, we have, for example:?m?wYZw~mv, w(i, j) =?i+2?k?
j?2bm?wY[mv, w(i,k), 1]?bm?Yw[mv, w(k, j), 2] (14)Note that there also exist relations amongst bterms for more efficient computation, for example:bm?wY[mv, w(i, j), c] = ?(wi|mv)?
(bm?wY[mv, w(i+1, j), c] + bm?Y[mv, w(i+1, j), c])(15)Analogous but more complex formulas are usedfor computing the outside probabilities.
Updating ofparameters can be incorporated into the computationof outside probabilities efficiently.7 DecodingIn the decoding phase, we want to find the optimalMR structure m??
given a new NL sentence w?:m??
= arg maxm?P(m?|w?)
= arg maxm?
?TP(m?,T |w?)
(16)where T is a possible hybrid tree associated withthe m?-w?
pair.
However, it is expensive to computethe summation over all possible hybrid trees.
Wetherefore find the most likely hybrid tree instead:m?
?=arg maxm?maxTP(m?,T |w?
)=arg maxm?maxTP(w?, m?,T ) (17)We have implemented an exact top-k decoding al-gorithm for this task.
Dynamic programming tech-niques similar to those discussed in Section 6 canalso be applied when retrieving the top candidates.We also find the Viterbi hybrid tree given a NL-MR pair, which can be done in an analogous way.This tree will be useful for reranking.8 Reranking and Filtering of PredictionsDue to the various independence assumptions wehave made, the model lacks the ability to expresssome long range dependencies.
We therefore post-process the best candidate predictions with a dis-criminative reranking algorithm.788Feature Type Description Example1.
Hybrid Rule A MR production and its child hybrid form f1 : STATE : loc 1(RIVER) ?
have RIVER2.
Expanded Hybrid Rule A MR production and its child hybrid form expanded f2 : STATE : loc 1(RIVER) ?
?have, RIVER : river(all)?3.
Long-range Unigram A MR production and a NL word appearing below in tree f3 : STATE : exclude(STATE STATE) ?
rivers4.
Grandchild Unigram A MR production and its grandchild NL word f4 : STATE : loc 1(RIVER) ?
rivers5.
Two Level Unigram A MR production, its parent production, and its child NL word f5 : ?RIVER : river(all), STATE : loc 1(RIVER)?
?
rivers6.
Model Log-Probability Logarithm of base model?s joint probability log (P?
(w, m,T )).Table 2: All the features used.
There is one feature for each possible combination, under feature type 1-5.
It takes value 1 ifthe combination is present, and 0 otherwise.
Feature 6 takes real values.8.1 The Averaged Perceptron Algorithm withSeparating PlaneThe averaged perceptron algorithm (Collins, 2002)has previously been applied to various NLP tasks(Collins, 2002; Collins, 2001) for discriminativereranking.
The detailed algorithm can be found in(Collins, 2002).
In this section, we extend the con-ventional averaged perceptron by introducing an ex-plicit separating plane on the feature space.Our reranking approach requires three compo-nents during training: a GEN function that definesfor each NL sentence a set of candidate hybrid trees;a single correct reference hybrid tree for each train-ing instance; and a feature function ?
that defines amapping from a hybrid tree to a feature vector.
Thealgorithm learns a weight vector w that associates aweight to each feature, such that a score w??
(T ) canbe assigned to each candidate hybrid tree T .
Givena new instance, the hybrid tree with the highest scoreis then picked by the algorithm as the output.In this task, the GEN function is defined as theoutput hybrid trees of the top-k (k is set to 50 in ourexperiments) decoding algorithm, given the learnedmodel parameters.
The correct reference hybrid treeis determined by running the Viterbi algorithm oneach training NL-MR pair.
The feature function isdiscussed in section 8.2.While conventional perceptron algorithms usuallyoptimize the accuracy measure, we extend it to allowoptimization of the F-measure by introducing an ex-plicit separating plane on the feature space that re-jects certain predictions even when they score high-est.
The idea is to find a threshold b after w islearned, such that a prediction with score below bgets rejected.
We pick the threshold that leads to theoptimal F-measure when applied to the training set.8.2 FeaturesWe list in Table 2 the set of features we used.
Ex-amples are given based on the hybrid tree in Figure3.
Some of the them are adapted from (Collins andKoo, 2005) for a natural language parsing task.
Fea-tures 1-5 are indicator functions (i.e., it takes value1 if a certain combination as the ones listed in Table2 is present, 0 otherwise), while feature 6 is real val-ued.
Features that do not appear more than once inthe training set are discarded.9 EvaluationOur evaluations were performed on two corpora,GEOQUERY and ROBOCUP.
The GEOQUERY cor-pus contains MR defined by a Prolog-based lan-guage used in querying a database on U.S. geogra-phy.
The ROBOCUP corpus contains MR defined bya coaching language used in a robot coaching com-petition.
There are in total 880 and 300 instances forthe two corpora respectively.
Standard 10-fold crossvalidations were performed and the micro-averagedresults are presented in this section.
To make oursystem directly comparable to previous systems, allour experiments were based on identical training andtest data splits of both corpora as reported in the ex-periments of Wong and Mooney (2006).9.1 Training MethodologyGiven a training set, we first run a variant of IBMalignment model 1 (Brown et al, 1993) for 100 iter-ations, and then initialize Model I with the learnedparameter values.
This IBM model is a word-to-word alignment model that does not model wordorder, so we do not have to linearize the hierarchi-cal MR structure.
Given this initialization, we trainModel I for 100 EM iterations and use the learnedparameters to initialize Model II which is trained foranother 100 EM iterations.
Model III is simply aninterpolation of the above two models.
As for thereranking phase, we initialize the weight vector withthe zero vector 0, and run the averaged perceptronalgorithm for 10 iterations.7899.2 Evaluation MethodologyFollowing Wong (2007) and other previous work,we report performance in terms of Precision (per-centage of answered NL sentences that are correct),Recall (percentage of correctly answered NL sen-tences, out of all NL sentences) and F-score (har-monic mean of Precision and Recall).Again following Wong (2007), we define the cor-rect output MR structure as follows.
For the GEO-QUERY corpus, an MR structure is considered cor-rect if and only if it retrieves identical results asthe reference MR structure when both are issued asqueries to the underlying Prolog database.
For theROBOCUP corpus, an MR structure is consideredcorrect if and only if it has the same string represen-tation as the reference MR structure, up to reorder-ing of children of MR productions whose functionsymbols are commutative, such as and, or, etc.9.3 Comparison over Three ModelsModel GEOQUERY (880) ROBOCUP (300)Prec.
Rec.
F Prec.
Rec.
FI 81.3 77.1 79.1 71.1 64.0 67.4II 89.0 76.0 82.0 82.4 57.7 67.8III 86.2 81.8 84.0 70.4 63.3 66.7I+R 87.5 80.5 83.8 79.1 67.0 72.6II+R 93.2 73.6 82.3 88.4 56.0 68.6III+R 89.3 81.5 85.2 82.5 67.7 74.4Table 3: Performance comparison over three models(Prec.
:precision, Rec.
:recall, +R: with reranking)We evaluated the three models, with and with-out reranking.
The results are presented in Table 3.Comparing Model I and Model II, we noticed thatfor both corpora, Model I in general achieves bet-ter recall while Model II achieves better precision.This observation conforms to our earlier expecta-tions.
Model III, as an interpolation of the above twomodels, achieves a much better F-measure on GEO-QUERY corpus.
However, it is shown to be less ef-fective on ROBOCUP corpus.
We noticed that com-pared to the GEOQUERY corpus, ROBOCUP corpuscontains longer sentences, larger MR structures, anda significant amount of non-compositionality.
Thesefactors combine to present a challenging problem forparsing with the generative model.
Interestingly, al-though Model III fails to produce better best pre-dictions for this corpus, we found that its top-k listcontains a relatively larger number of correct pre-dictions than Model I or Model II.
This indicatesthe possibility of enhancing the performance withreranking.The reranking approach is shown to be quite ef-fective.
We observe a consistent improvement inboth precision and F-measure after employing thereranking phase for each model.9.4 Comparison with Other ModelsAmong all the previous models, SILT, WASP, andKRISP are directly comparable to our model.
Theyrequired the same amount of supervision as our sys-tem and were evaluated on the same corpora.We compare our model with these models in Ta-ble 4, where the performance scores for the previoussystems are taken from (Wong, 2007).
For GEO-QUERY corpus, our model performs substantiallybetter than all the three previous models, with a no-table improvement in the recall score.
In fact, if welook at the recall scores alone, our best-performingmodel achieves a 6.7% and 9.8% absolute improve-ment over two other state-of-the-art models WASPand KRISP respectively.
This indicates that over-all, our model is able to handle over 25% of theinputs that could not be handled by previous sys-tems.
On the other hand, in terms of F-measure,we gain a 4.1% absolute improvement over KRISP,which leads to an error reduction rate of 22%.
Onthe ROBOCUP corpus, our model?s performance isalso ranked the highest1.System GEOQUERY (880) ROBOCUP (300)Prec.
Rec.
F Prec.
Rec.
FSILT 89.0 54.1 67.3 83.9 50.7 63.2WASP 87.2 74.8 80.5 88.9 61.9 73.0KRISP 93.3 71.7 81.1 85.2 61.9 71.7Model III+R 89.3 81.5 85.2 82.5 67.7 74.4Table 4: Performance comparison with other directly com-parable systems9.5 Performance on Other LanguagesAs a generic model that requires minimal assump-tions on the natural language, our model is naturallanguage independent and is able to handle variousother natural languages than English.
To validatethis point, we evaluated our system on a subset of1We are unable to perform statistical significance tests be-cause the detailed performance for each fold of previously pub-lished research work is not available.790the GEOQUERY corpus consisting of 250 instances,with four different NL annotations.As we can see from Table 5, our model is ableto achieve performance comparable to WASP as re-ported by Wong (2007).System English SpanishPrec.
Rec.
F Prec.
Rec.
FWASP 95.42 70.00 80.76 91.99 72.40 81.03Model III+R 91.46 72.80 81.07 95.19 79.20 86.46System Japanese TurkishPrec.
Rec.
F Prec.
Rec.
FWASP 91.98 74.40 82.86 96.96 62.40 75.93Model III+R 87.56 76.00 81.37 93.82 66.80 78.04Table 5: Performance on different natural languages forGEOQUERY-250 corpusOur model is generic, which requires no domain-dependent knowledge and should be applicable toa wide range of different domains.
Like all re-search in this area, the ultimate goal is to scale tomore complex, open-domain language understand-ing problems.
In future, we would like to create alarger corpus in another domain with multiple natu-ral language annotations to further evaluate the scal-ability and portability of our approach.10 ConclusionsWe presented a new generative model that simulta-neously produces both NL sentences and their cor-responding MR structures.
The model can be effec-tively applied to the task of transforming NL sen-tences to their MR structures.
We also developeda new dynamic programming algorithm for efficienttraining and decoding.
We demonstrated that thisapproach, augmented with a discriminative rerank-ing technique, achieves state-of-the-art performancewhen tested on standard benchmark corpora.In future, we would like to extend the currentmodel to have a wider range of support of MR for-malisms, such as the one with lambda-calculus sup-port.
We are also interested in investigating ways toapply the generative model to the inverse task: gen-eration of a NL sentence that explains a given MRstructure.AcknowledgmentsThe authors would like to thank Leslie Pack Kael-bling for her valuable feedback and comments onthis research.
The authors would also like to thankthe anonymous reviewers for their thoughtful com-ments on this paper.
The research is partially sup-ported by ARF grant R-252-000-240-112.ReferencesJ.
K. Baker.
1979.
Trainable grammars for speech recog-nition.
Journal of the Acoustical Society of America,65:S132.P.
F. Brown, S. A. D. Pietra, V. J. D. Pietra, and R. L.Mercer.
1993.
The mathematics of statistical machinetranslation: Parameter estimation.
Computational Lin-guistics, 19(2):263?311.M.
Collins and T. Koo.
2005.
Discriminative rerankingfor natural language parsing.
Computational Linguis-tics, 31(1):25?70.M.
Collins.
2001.
Ranking algorithms for named-entityextraction: boosting and the voted perceptron.
In Pro-ceedings of the 40th Annual Meeting of the Associa-tion for Computational Linguistics (ACL 2002), pages489?496.M.
Collins.
2002.
Discriminative training methods forhidden Markov models: theory and experiments withperceptron algorithms.
In Proceedings of the 2002Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP 2002), pages 1?8.M.
Collins.
2003.
Head-driven statistical models fornatural language parsing.
Computational Linguistics,29(4):589?637.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via the EMalgorithm.
Journal of the Royal Statistical Society,39(1):1?38.R.
Ge and R. J. Mooney.
2005.
A statistical semanticparser that integrates syntax and semantics.
In Pro-ceedings of the Ninth Conference on ComputationalNatural Language Learning (CoNLL 2005), pages 9?16.R.
Ge and R. J. Mooney.
2006.
Discriminative rerank-ing for semantic parsing.
In Proceedings of the 21stInternational Conference on Computational Linguis-tics and the 44th Annual Meeting of the Associationfor Computational Linguistics (COLING/ACL 2006),pages 263?270.R.
J. Kate and R. J. Mooney.
2006.
Using string-kernelsfor learning semantic parsers.
In Proceedings of the21st International Conference on Computational Lin-guistics and the 44th Annual Meeting of the Asso-ciation for Computational Linguistics (COLING/ACL2006), pages 913?920.R.
J. Kate, Y. W. Wong, and R. J. Mooney.
2005.
Learn-ing to transform natural to formal languages.
In Pro-ceedings of the Twentieth National Conference on Ar-tificial Intelligence (AAAI 2005), pages 1062?1068.791S.
Katz.
1987.
Estimation of probabilities from sparsedata for the language model component of a speechrecognizer.
IEEE Transactions on Acoustics, Speech,and Signal Processing, 35(3):400?401.L.
R. Tang and R. J. Mooney.
2001.
Using multipleclause constructors in inductive logic programming forsemantic parsing.
In Proceedings of the 12th Euro-pean Conference on Machine Learning (ECML 2001),pages 466?477.Y.
W. Wong and R. J. Mooney.
2006.
Learning forsemantic parsing with statistical machine translation.In Proceedings of the Human Language TechnologyConference of the North American Chapter of the As-sociation for Computational Linguistics (HLT-NAACL2006), pages 439?446.Y.
W. Wong and R. J. Mooney.
2007.
Learning syn-chronous grammars for semantic parsing with lambdacalculus.
In Proceedings of the 45th Annual Meetingof the Association for Computational Linguistics (ACL2007), pages 960?967.Y.
W. Wong.
2007.
Learning for Semantic Parsing andNatural Language Generation Using Statistical Ma-chine Translation Techniques.
Ph.D. thesis, The Uni-versity of Texas at Austin.K.
Yamada and K. Knight.
2001.
A syntax-based sta-tistical translation model.
In Proceedings of the 39thAnnual Meeting of the Association for ComputationalLinguistics, pages 523?530.L.
S. Zettlemoyer and M. Collins.
2005.
Learning tomap sentences to logical form: Structured classifica-tion with probabilistic categorial grammars.
In Pro-ceedings of the 21st Conference on Uncertainty in Ar-tificial Intelligence.L.
S. Zettlemoyer and M. Collins.
2007.
Online learningof relaxed CCG grammars for parsing to logical form.In Proceedings of the 2007 Joint Conference on Em-pirical Methods in Natural Language Processing andComputational Natural Language Learning (EMNLP-CoNLL 2007), pages 678?687.792
