Proceedings of the ACL 2007 Demo and Poster Sessions, pages 57?60,Prague, June 2007. c?2007 Association for Computational LinguisticsSupport Vector Machines for Query-focused Summarization trained andevaluated on Pyramid dataMaria FuentesTALP Research CenterUniversitat Polite`cnica de Catalunyamfuentes@lsi.upc.eduEnrique AlfonsecaComputer Science DepartamentUniversidad Auto?noma de MadridEnrique.Alfonseca@gmail.comHoracio Rodr?
?guezTALP Research CenterUniversitat Polite`cnica de Catalunyahoracio@lsi.upc.eduAbstractThis paper presents the use of SupportVector Machines (SVM) to detect rele-vant information to be included in a query-focused summary.
Several SVMs aretrained using information from pyramidsof summary content units.
Their per-formance is compared with the best per-forming systems in DUC-2005, using bothROUGE and autoPan, an automatic scor-ing method for pyramid evaluation.1 IntroductionMulti-Document Summarization (MDS) is the taskof condensing the most relevant information fromseveral documents in a single one.
In terms of theDUC contests1, a query-focused summary has toprovide a ?brief, well-organized, fluent answer to aneed for information?, described by a short query(two or three sentences).
DUC participants have tosynthesize 250-word sized summaries for fifty setsof 25-50 documents in answer to some queries.In previous DUC contests, from 2001 to 2004, themanual evaluation was based on a comparison witha single human-written model.
Much informationin the evaluated summaries (both human and auto-matic) was marked as ?related to the topic, but notdirectly expressed in the model summary?.
Ideally,this relevant information should be scored during theevaluation.
The pyramid method (Nenkova and Pas-sonneau, 2004) addresses the problem by using mul-tiple human summaries to create a gold-standard,1http://www-nlpir.nist.gov/projects/duc/and by exploiting the frequency of information inthe human summaries in order to assign importanceto different facts.
However, the pyramid method re-quires to manually matching fragments of automaticsummaries (peers) to the Semantic Content Units(SCUs) in the pyramids.
AutoPan (Fuentes et al,2005), a proposal to automate this matching process,and ROUGE are the evaluation metrics used.As proposed by Copeck and Szpakowicz (2005),the availability of human-annotated pyramids con-stitutes a gold-standard that can be exploited in or-der to train extraction models for the summary au-tomatic construction.
This paper describes severalmodels trained from the information in the DUC-2006 manual pyramid annotations using SupportVector Machines (SVM).
The evaluation, performedon the DUC-2005 data, has allowed us to discoverthe best configuration for training the SVMs.One of the first applications of supervised Ma-chine Learning techniques in summarization was inSingle-Document Summarization (Ishikawa et al,2002).
Hirao et al (2003) used a similar approachfor MDS.
Fisher and Roark (2006)?s MDS system isbased on perceptrons trained on previous DUC data.2 ApproachFollowing the work of Hirao et al (2003) andKazawa et al (2002), we propose to train SVMsfor ranking the candidate sentences in order of rele-vance.
To create the training corpus, we have usedthe DUC-2006 dataset, including topic descriptions,document clusters, peer and manual summaries, andpyramid evaluations as annotated during the DUC-2006 manual evaluation.
From all these data, a set57of relevant sentences is extracted in the followingway: first, the sentences in the original documentsare matched with the sentences in the summaries(Copeck and Szpakowicz, 2005).
Next, all docu-ment sentences that matched a summary sentencecontaining at least one SCU are extracted.
Note thatthe sentences from the original documents that arenot extracted in this way could either be positive (i.e.contain relevant data) or negative (i.e.
irrelevant forthe summary), so they are not yet labeled.
Finally,an SVM is trained, as follows, on the annotated data.Linguistic preprocessing The documents fromeach cluster are preprocessed using a pipe of generalpurpose processors performing tokenization, POStagging, lemmatization, fine grained Named Enti-ties (NE)s Recognition and Classification, anaphoraresolution, syntactic parsing, semantic labeling (us-ing WordNet synsets), discourse marker annotation,and semantic analysis.
The same tools are used forthe linguistic processing of the query.
Using thesedata, a semantic representation of the sentence isproduced, that we call environment.
It is a semantic-network-like representation of the semantic units(nodes) and the semantic relations (edges) holdingbetween them.
This representation will be used tocompute the (Fuentes et al, 2006) lexico-semanticmeasures between sentences.Collection of positive instances As indicated be-fore, every sentence from the original documentsmatching a summary sentence that contains at leastone SCU is considered a positive example.
We haveused a set of features that can be classified into threegroups: those extracted from the sentences, thosethat capture a similarity metric between the sentenceand the topic description (query), and those that tryto relate the cohesion between a sentence and all theother sentences in the same document or collection.The attributes collected from the sentences are:?
The position of the sentence in its document.?
The number of sentences in the document.?
The number of sentences in the cluster.?
Three binary attributes indicating whether thesentence contains positive, negative and neutraldiscourse markers, respectively.
For instance,what?s more is positive, while for example andincidentally indicate lack of relevance.?
Two binary attributes indicating whetherthe sentence contains right-directed discoursemarkers (that affect the relevance of fragmentafter the marker, e.g.
first of all), or discoursemarkers affecting both sides, e.g.
that?s why.?
Several boolean features to mark whether thesentence starts with or contains a particularword or part-of-speech tag.?
The total number of NEs included in the sen-tence, and the number of NEs of each kind.?
SumBasic score (Nenkova and Vanderwende,2005) is originally an iterative procedure thatupdates word probabilities as sentences are se-lected for the summary.
In our case, word prob-abilities are estimated either using only the setof words in the current document, or using allthe words in the cluster.The attributes that depend on the query are:?
Word-stem overlapping with the query.?
Three boolean features indicating whether thesentence contains a subject, object or indirectobject dependency in common with the query.?
Overlapping between the environment predi-cates in the sentence and those in the query.?
Two similarity metrics calculated by expandingthe query words using Google.?
SumFocus score (Vanderwende et al, 2006).The cohesion-based attributes 2 are:?
Word-stem overlapping between this sentenceand the other sentences in the same document.?
Word-stem overlapping between this sentenceand the other sentences in the same cluster.?
Synset overlapping between this sentence andthe other sentences in the same document.?
Synset overlapping with other sentences in thesame collection.Model training In order to train a traditionalSVM, both positive and negative examples are nec-essary.
From the pyramid data we are able to iden-tify positive examples, but there is not enough ev-idence to classify the remaining sentences as posi-tive or negative.
Although One-Class Support Vec-tor Machine (OSVM) (Manevitz and Yousef, 2001)can learn from just positive examples, according toYu et al (2002) they are prone to underfitting andoverfitting when data is scant (which happens in2The mean, median, standard deviation and histogram of theoverlapping distribution are calculated and included as features.58this case), and a simple iterative procedure calledMapping-Convergence (MC) algorithm can greatlyoutperform OSVM (see the pseudocode in Figure 1).Input: positive examples, POS, unlabeled examples UOutput: hypothesis at each iteration h?1, h?2, ..., h?k1.
Train h to identify ?strong negatives?
in U :N1 := examples from U classified as negative by hP1 := examples from U classified as positive by h2.
Set NEG := ?
and i := 13.
Loop until Ni = ?,3.1.
NEG := NEG ?
Ni3.2.
Train h?i from POS and NEG3.3.
Classify Pi by h?i:Ni+1 = examples from Pi classified as negativePi+1 = examples from Pi classified as positive5.
Return {h?1, h?2, ..., h?k}Figure 1: Mapping-Convergence algorithm.The MC starts by identifying a small set of in-stances that are very dissimilar to the positive exam-ples, called strong negatives.
Next, at each iteration,a new SVM h?i is trained using the original positiveexamples, and the negative examples found so far.The set of negative instances is then extended withthe unlabeled instances classified as negative by h?i.The following settings have been tried:?
The set of positive examples has been collectedeither by matching document sentences to peersummary sentences (Copeck and Szpakowicz,2005) or by matching document sentences tomanual summary sentences.?
The initial set of strong negative examples forthe MC algorithm has been either built auto-matically as described by Yu et al (2002), orbuilt by choosing manually, for each cluster, thetwo or three automatic summaries with lowestmanual pyramid scores.?
Several SVM kernel functions have been tried.For training, there were 6601 sentences from theoriginal documents, out of which around 120 werenegative examples and either around 100 or 500 pos-itive examples, depending on whether the documentsentences had been matched to the manual or thepeer summaries.
The rest were initially unlabeled.Summary generation Given a query and a set ofdocuments, the trained SVMs are used to rank sen-tences.
The top ranked ones are checked to avoid re-dundancy using a percentage overlapping measure.3 Evaluation FrameworkThe SVMs, trained on DUC-2006 data, have beentested on the DUC-2005 corpus, using the 20 clus-ters manually evaluated with the pyramid method.The sentence features were computed as describedbefore.
Finally, the performance of each systemhas been evaluated automatically using two differ-ent measures: ROUGE and autoPan.ROUGE, the automatic procedure used in DUC,is based on n-gram co-occurrences.
Both ROUGE-2(henceforward R-2) and ROUGE-SU4 (R-SU4) hasbeen used to rank automatic summaries.AutoPan is a procedure for automatically match-ing fragments of text summaries to SCUs in pyra-mids, in the following way: first, the text in theSCU label and all its contributors is stemmed andstop words are removed, obtaining a set of stemvectors for each SCU.
The system summary text isalso stemmed and freed from stop words.
Next, asearch for non-overlapping windows of text whichcan match SCUs is carried.
Each match is scoredtaking into account the score of the SCU as well asthe number of matching stems.
The solution whichglobally maximizes the sum of scores of all matchesis found using dynamic programming techniques.According to Fuentes et al (2005), autoPan scoresare highly correlated to the manual pyramid scores.Furthermore, autoPan also correlates well with man-ual responsiveness and both ROUGE metrics.33.1 ResultsPositive Strong neg.
R-2 R-SU4 autoPanpeer pyramid scores 0.071 0.131 0.072(Yu et al, 2002) 0.036 0.089 0.024manual pyramid scores 0.025 0.075 0.024(Yu et al, 2002) 0.018 0.063 0.009Table 1: ROUGE and autoPan results using different SVMs.Table 1 shows the results obtained, from whichsome trends can be found: firstly, the SVMstrained using the set of positive examples obtainedfrom peer summaries consistently outperform SVMstrained using the examples obtained from the man-ual summaries.
This may be due to the fact that the3In DUC-2005 pyramids were created using 7 manual sum-maries, while in DUC-2006 only 4 were used.
For that reason,better correlations are obtained in DUC-2005 data.59number of positive examples is much higher in thefirst case (on average 48,9 vs. 12,75 examples percluster).
Secondly, generating automatically a setwith seed negative examples for the M-C algorithm,as indicated by Yu et al (2002), usually performsworse than choosing the strong negative examplesfrom the SCU annotation.
This may be due to thefact that its quality is better, even though the amountof seed negative examples is one order of magnitudesmaller in this case (11.9 examples in average).
Fi-nally, the best results are obtained when using a RBFkernel, while previous summarization work (Hiraoet al, 2003) uses polynomial kernels.The proposed system attains an autoPan value of0.072, while the best DUC-2005 one (Daume?
III andMarcu, 2005) obtains an autoPan of 0.081.
The dif-ference is not statistically significant.
(Daume?
IIIand Marcu, 2005) system also scored highest in re-sponsiveness (manually evaluated at NIST).However, concerning ROUGE measures, the bestparticipant (Ye et al, 2005) has an R-2 score of0.078 (confidence interval [0.073?0.080]) and an R-SU4 score of 0.139 [0.135?0.142], when evaluatedon the 20 clusters used here.
The proposed sys-tem again is comparable to the best system in DUC-2005 in terms of responsiveness, Daume?
III andMarcu (2005)?s R-2 score was 0.071 [0.067?0.074]and R-SU4 was 0.126 [0.123?0.129] and it is betterthan the DUC-2005 Fisher and Roark supervised ap-proach with an R-2 of 0.066 and an R-SU4 of 0.122.4 Conclusions and future workThe pyramid annotations are a valuable source ofinformation for training automatically text sum-marization systems using Machine Learning tech-niques.
We explore different possibilities for apply-ing them in training SVMs to rank sentences in orderof relevance to the query.
Structural, cohesion-basedand query-dependent features are used for training.The experiments have provided some insights onwhich can be the best way to exploit the annota-tions.
Obtaining the positive examples from the an-notations of the peer summaries is probably betterbecause most of the peer systems are extract-based,while the manual ones are abstract-based.
Also, us-ing a very small set of strong negative example seedsseems to perform better than choosing them auto-matically with Yu et al (2002)?s procedure.In the future we plan to include features from ad-jacent sentences (Fisher and Roark, 2006) and userouge scores to initially select negative examples.AcknowledgmentsWork partially funded by the CHIL project, IST-2004506969.ReferencesT.
Copeck and S. Szpakowicz.
2005.
Leveraging pyramids.
InProc.
DUC-2005, Vancouver, Canada.Hal Daume?
III and Daniel Marcu.
2005.
Bayesian summariza-tion at DUC and a suggestion for extrinsic evaluation.
InProc.
DUC-2005, Vancouver, Canada.S.
Fisher and B. Roark.
2006.
Query-focused summarizationby supervised sentence ranking and skewed word distribu-tions.
In Proc.
DUC-2006, New York, USA.M.
Fuentes, E. Gonza`lez, D. Ferre?s, and H.
Rodr??guez.
2005.QASUM-TALP at DUC 2005 automatically evaluated withthe pyramid based metric autopan.
In Proc.
DUC-2005.M.
Fuentes, H.
Rodr?
?guez, J. Turmo, and D. Ferre?s.
2006.FEMsum at DUC 2006: Semantic-based approach integratedin a flexible eclectic multitask summarizer architecture.
InProc.
DUC-2006, New York, USA.T.
Hirao, J. Suzuki, H. Isozaki, and E. Maeda.
2003.
Ntt?smultiple document summarization system for DUC2003.
InProc.
DUC-2003.K.
Ishikawa, S. Ando, S. Doi, and A. Okumura.
2002.
Train-able automatic text summarization using segmentation ofsentence.
In Proc.
2002 NTCIR 3 TSC workshop.H.
Kazawa, T. Hirao, and E. Maeda.
2002.
Ranking SVM andits application to sentence selection.
In Proc.
2002 Workshopon Information-Based Induction Science (IBIS-2002).L.M.
Manevitz and M. Yousef.
2001.
One-class SVM for docu-ment classification.
Journal of Machine Learning Research.A.
Nenkova and R. Passonneau.
2004.
Evaluating content se-lection in summarization: The pyramid method.
In Proc.HLT/NAACL 2004, Boston, USA.A.
Nenkova and L. Vanderwende.
2005.
The impact offrequency on summarization.
Technical Report MSR-TR-2005-101, Microsoft Research.L.
Vanderwende, H. Suzuki, and C. Brockett.
2006.
Mi-crosoft research at DUC 2006: Task-focused summarizationwith sentence simplification and lexical expansion.
In Proc.DUC-2006, New York, USA.S.
Ye, L. Qiu, and T.S.
Chua.
2005.
NUS at DUC 2005: Under-standing documents via concept links.
In Proc.
DUC-2005.H.
Yu, J. Han, and K. C-C. Chang.
2002.
PEBL: Positiveexample-based learning for web page classification usingSVM.
In Proc.
ACM SIGKDD International Conference onKnowledge Discovery in Databases (KDD02), New York.60
