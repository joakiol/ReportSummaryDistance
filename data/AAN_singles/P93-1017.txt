Lexica l ized Context -F ree  GrammarsYves  Schabes  and  R ichard  C .
WatersMi tsub ish i  E lectr ic  Research Laborator ies201 Broadway,  Cambr idge ,  MA 02139e-mail :  s( ;habes@merl .com and dick((~merl.coinLexicalized context-free grammar (LCFG) isan attractive compromise between the parsing ef-ficiency of context-free grammar (CFC) and theelegance and lexical sensitivity of lexicalized tree-adjoining rammar (LTAG).
LCFC is a restrictedform of LTAG that can only generate context-free languages and can be parsed in cubic time.However, LCF(I supports much of the elegance ofLTAG's analysis of English and shares with LTAGthe ability to lexicalize CF(I;s without changingthe trees generated.MotivationContext-free grammar (CFG) has been a well ac-cepted framework for computational linguistics fora long time.
While it has drawbacks, including theinability to express ome linguistic constructions,it has the virtue of being computationally efficient,O(n3)-time in the worst case.Recently there has been a gain in interest inthe so-called 'mildly' context-sensitive formalisms(Vijay-Shanker, 1987; Weir, 1988; Joshi, Vijay-Shanker, and Weir, 1991; Vijay-Shanker and Weir,1993a) that generate only a small superset ofcontext-free languages.
One such formalism is lex-icalized tree-adjoining grammar (LTAG) (Schabes,Abeill~, and Joshi, 1988; Abeillfi et al, 1990; Joshiand Schabes, 1992), which provides a numberof attractive properties at the cost of decreasedefficiency, O(n6)-time in the worst case (Vijay-Shanker, 1987; Schabes, 1991; Lang, 1990; Vijay-Shanker and Weir, 1993b).An LTAG lexicon consists of a set of trees eachof which contains one or more lexical items.
Theseelementary trees can be viewed as the elementaryclauses (including their transformational variants)in which the lexical items participate.
The treesare combined by substitution and adjunction.LTAC supports context-sensitive features thatcan capture some language constructs not cap-tured by CFG.
However, the greatest virtue ofLTAG is that it is lexicalized and supports ex-tended domains of locality.
The lexical nature ofLTAC is of linguistic interest, since it is believedthat the descriptions of many linguistic phenom-ena are dependent upon lexical data.
The ex-tended domains allow for the localization of mostsyntactic and semantic dependencies (e.g., filler-gap and predicate-argument relationships).A fllrther interesting aspect of LTAG is itsability to lexicalize CFCs.
One can convert a CFCinto an LTAG that preserves the original trees(Joshi and Schabes, 1992).Lexicalized context-free grammar (LCFG) isan attractive compromise between LTAG andCFG, that combines many of the virtues of LTAGwith the efficiency of CFG.
LCFC is a restrictedform of LTAG that places further limits on the el-ementary trees that are possible and on the wayadjunction can be performed.
These restrictionslimit LCFG to producing only context-free lan-guages and allow LCFC to be parsed in O(n3) -time in the worst ease.
However, LCFC retainsmost of the key features of LTAG enumeratedabove.In particular, most of the current LTAG gram-mar for English (Abeilld et al, 1990) follows therestrictions of LCFG.
This is of significant practi-cal interest because it means that the processingof these analyses does not require more computa-tional resources than CFGs.In addition, any CFG can be transformedinto an equivalent LCFC that generates the sametrees (and therefore the same strings).
This re-sult breaks new ground, because heretofore v-ery method of lexicalizing CFCs required context-sensitive operations (Joshi and Schabes, 1992).The following sections briefly, define LCFG,discuss its relationship to the current LTAG gram-mar for English, prove that LC, FC can be used tolexicalize CFC, and present a simple cubic-timeparser for LCFC.
These topics are discussed ingreater detail in Schabes and Waters (1993).121Lex ica l i zed  Context -F ree  GrammarsLike an LTAG, an LC'FG consists of two sets oftrees: initial trees, which are combined by substi-tution and auxiliary trees, which are combined byadjunction.
An LCFG is lexicalized in the sensethat every initial and auxiliary tree is required tocontain at least one terminal symbol on its fron-tier.More precisely, an LCFG is a five-tuple(Z, NT, I, A, ,5'), where ~ is a set of terminal sym-bols, NT is a set of non-terminal symbols, I andA are sets of trees labeled by terminal and non-terminal symbols, and ,5' is a distinguished non-terminal start symbol.Each initial tree in the set I satisfies the fol-lowing requirements.
(i) Interior nodes are labeled by non-terminal symbols.
(ii) The nodes on the frontier of the treeconsist of zero or more non-terminalsymbols and one or more terminal sym-bols.
(iii) The non-terminal symbols on thefrontier are marked for substitution.
Byconvention, this is annotated in dia-grams using a down arrow (l).Each auxiliary tree in the set A satisfies thefollowing requirements.
(i) Interior nodes are labeled by non-terminal symbols.
(ii) The nodes on the frontier consist ofzero or more non-terminal symbols andone or more terminal symbols.
(iii) All but one of the non-terminal sym-bols on the frontier are marked for sub-stitution.
(iv) The remaining non-terminal on thefrontier of the tree is called the foot.
Thelabel on the foot must be identical tothe label on the root node of the tree.By convention, the foot is indicated indiagrams using an asterisk (.).
(v) the foot must be in either the leftmostor the rightmost position on the frontier.Figure 1, shows seven elementary trees thatmight appear in an LCFG for English.
The treescontaining 'boy', 'saw', and 'left' are initial trees.The remainder are attxiliary trees.Auxiliary trees whose feet are leftrnost arecalled left recursive.
Similarly, auxiliary treeswhose feet are rightrnost are called righl recursiveauxiliary trees.
The path from the root of an aux-iliary tree to the foot is called the spine.NP VP N VPA /k A /XD$ N V VP* A N* VP* AdvI I I iboy seems pretty smoothlySS NPi,~(+wh) S SNPo$ VP NP o VP NPo$ VPA I \[V SI*NA ?i V V NPI$I I Ithink left sawFigure 1: Sample trees.In LCF(I, trees can be combined with substi-tution and adjunction.
As illustrated in Figure 2,substitution replaces a node marked for substitu-tion with a copy of an initial tree.Adjunction inserts a copy of an auxiliary treeinto another tree in place of an interior node thathas the same label as the foot of the auxiliary tree.The subtree that was previously connected to theinterior node is reconnected to the foot of the copyof the auxiliary tree.
If the auxiliary tree is left re-cursive, this is referred to as left recursive adjunc-tion (see Figure 3).
If the auxiliary tree is rightrecursive, this is referred to as right recursive ad-junction (see Figure 4).Crucially, adjunction is constrained by requir-ing that a left recursive auxiliary tree cannot beadjoined on any node that is on the spine of aright recursive auxiliary tree and a right recursiveauxiliary tree cannot be adjoined on the spine ofa left recursive auxiliary tree.An LCFG derivation must start with an initialtree rooted in S. After that, this tree can be re-peatedly extended using substitution and adjunc-tion.
A derivation is complete when every frontiernode is labeled with a terminal symbol.The difference between LCFG and LTAG isFigure 2: Substitution.122/ AAAFigure 3: Left recursive adjunction.~ A *  = "A%Figure 4: Right recursive adjunction.that LTAG allows the foot of an auxiliary treeto appear anywhere on the frontier and places nolimitations on the interaction of auxiliary trees.In this unlimited situation, adjunction encodesstring wrapping and is therefore more power-ful than concatenation (see Figure 5).
However,the restrictions imposed by LCFG guarantee thatno context-sensitive operations can be achieved.They limit the languages that can be generated byLCFGs to those that can be generated by CFGs.Coverage of LCFG and LTAGThe power of LCFG is significantly less thanLTAG.
Surprisingly, it turns out that there areonly two situations where the current LTAG gram-mar for English (Abeilld et al, 1990) fails to satisfythe restrictions imposed by LCFG.The first situation, concerns certain verbs thattake more than one sentential complement.
An ex-ample of such a verb is deduce, which is associatedwith the following auxiliary tree.SNPo$ VPV Sl* PPI A deduce P Sz,I,I fromSince this tree contains a foot node in the cen-ter of its frontier, it is not part of an LCFG.
Hav-ing the foot on the first sentential complement isconvenient, because it allows one to use the stan-dard LTAG wh-analyses, which depends on thew2 ~ W4%Figure 5: Adjunction in LTAG.existence of an initial tree where the filler and gapare local.
This accounts nicely for the pair of sen-tences below.
However, other analyses of wh ques-tions may not require the use of the auxiliary treeabove.
(1) John deduced that Mary watered thegrass from seeing the hose.
(2) What did John deduce that Mary wa-tered from seeing the hose.The second situation, concerns the way thecurrent LTAG explains the ambiguous attach-ments of adverbial modifiers.
For example, in thesentence:(3) John said Bill left yesterday.the attachment of yesterday is ambiguous.
Thetwo different LTAG derivations indicated in Fig-ure 6 represent his conveniently.Unfortunately, in LCFG the high attachmentof yesterday is forbidden since a right auxiliarytree (corresponding to yesterday) is adjoined onthe spines of a left auxiliary tree (corresponding toJohn said).
However, one could avoid this prob-lem by designing a mechanism to recover the highattachment reading from the low one.Besides the two cases presented above, thecurrent LTAG for English uses only left and rightrecursive auxiliary trees and does not allow anySNP .
.~  ",~ ........IJohn V S* .
.
.
.
.
.
.
.
.
.
.
.
VP "-::.
A said S ."
"'?
/ \,~t o- VP* ADVNP VP II I yesterdayBill VIleftFigure 6: Two LTAG derivations for John said Billleft yesterday.123interaction along the spine of these two kinds oftrees.
This agrees with the intuition that mostEnglish analyses do not require a context-sensitiveoperation.LCFG.
However, as shown below, combining ex-tended substitution with restricted adjunction al-lows strong lexicalization of CFG, without intro-ducing greater parsing complexity than CFG.Lex ica l i za t ion  of  CFGsThe lexicalization of grammar formalisms is of in-terest from a number of perspectives.
It is of in-terest from a linguistic perspective, because mostcurrent linguistic theories give lexical accounts of anumber of phenomena that used to be consideredpurely syntactic.
It is of interest from a computa-tional perspective, because lexicalized grammarscan be parsed significantly more efficiently thannon-lexicalized ones (Schabes and Joshi, 1990).Formally, a grammar is said 'lexicalized' (Sch-abes, Abeill~., and Joshi, 1988) if it consists of:,, a finite set of elementary structures of finite size,each of which c, ontains an overt (i.e., non-empty)lexical item.?
a finite set of operations for creating derivedstructures.The overt lexical item in an elementary struc-ture is referred to as its anchor.
A lexicalizedgrammar can be organized as a lexicon where eachlexical item is associated with a finite number ofstructures for which that item is the anchor.In general, CFGs are not lexicalized since rulessuch as ,5' --* NP VP  that do not locally introducelexical items are allowed.
In contrast, the well-known Creibach Normal Form (CNF) for CFCsis lexicalized, because very production rule is re-quired to be of the form A --+ ac~ (where a is aterminal symbol, A a non-terminal symbol and aa possibly empty string of non-terminal symbols)and therefore locally introduces a lexical item a.It can be shown that for any CFG (.7 (that doesnot derive the empty string), there is a CNF gram-mar (.7 ~ that derives the same language.
However,it may be impossible for the set of trees producedby (7 ~ to be the same as the set of trees producedby G.Therefore, CNF achieves a kind of lexicaliza-tion of CFGs.
However, it is only a weak lexical-ization, because the set of trees is not necessarilypreserved.
As discussed in the motivation section,strong lexicalization that preserves tree sets is pos-sible using LTAG.
However, this is achieved at thecost of significant additional parsing complexity.Heretofore, several attempts have been madeto lexicalize CFC with formalisms weaker thanLTAG, but without success.
In particular, it isnot sufficient o merely extend substitution so thatit applies to trees.
Neither is it sutficient o relysolely on the kind restricted adjunction used byTheorem I f  G = (~,NT ,  P,S)  is a finitelyambiguous CFG which does not generate theempty .string (?
), then there is an LCFG (7 ~ =(~, NT,  I, A, S) generating the same language andtree set as (7.
Furthermore (7' can be chosen .sothat it utilizes only lefl-recursive auxiliary trees.As usual in the above, a CFG (.7 is a four-tuple, (E, NT, P, S), where N is a set of terminalsymbols, NT is a set of non-terminal symbols, P isa set of production rules that rewrite non-terminalsymbols to strings of terminal and non-terminalsymbols, and S is a distinguished non-terminalsymbol that is the start symbol of any derivation.To prove the theorem we first prove a some-what weaker theorem and then extend the proofto the flfll theorem.
In particular, we assume forthe moment that the set of rules for (.7 does notcontain any empty rules of the form A ~ e.S tep  1 We begin the construction of (7 ~ by con-structing a directed graph LCG that we call theleft corner derivation graph.
Paths in LCG cor-respond to leftmost paths from root to frontier in(partial) derivation trees rooted at non-terminalsymbols in (1.L(TG contains a node for every symbol in E UNT and an arc for every rule in P as follows.For each terminal and non-terminal symbolX in G create a node in LCG labeled withX.
For each rule X --+ Ya  in G create adirected arc labeled with X ~ Ya  from thenode labeled with X to the node labeled Y.As an example, consider the example CFG inFigure 7 and the corresponding L(TG shown inFigure 8.The significance of L( ;G is that there is a one-to-one correspondence b tween paths in LCG end-ing on a non-terminal and left corner derivations inG.
A left corner derivation in a CFG is a partialderivation starting from any non-terminal whereevery expanded node (other than the root) is theleftmost child of its parent and the left corner is anon-terminal.
A left corner derivation is uniquelyidentified by the list of rules applied.
Since G doesnot have any empty rules, every rule in (7 is rep-resented in L(;'G.
Therefore, every path in LCGending on a terminal corresponds to a left cornerderivation in (7 and vice versa.124S---+A A,5' --+ B AA - -+B BB--+ A SB ----+ bFigure 7: An example grammar.S ---~ B AS -~.A AS ~- A BB--+ A SB--+bbFigure 8: The LC(;  created by Step 1.S tep  2 The set of initial trees I for G' is con-structed with reference to L(TG.
In particular, aninitial tree is created corresponding to each non-cyclic path in L(/G that starts at a non-terminalsymbol X and ends on a terminal symbol y.
(Anon-cyclic path is a path that does not touch anynode twice.
)For each non-cyclic path in LCG from X toy, construct an initial tree T as follows.
Startwith a root labeled X.
Apply the rules in thepath one after another, always expanding theleft corner node of T. While doing this, leaveall the non-left corner non-terminal symbolsin T unexpanded, and label them as substi-tution nodes.Given the previous example grammar, thisstep produces the initial trees shown in Figure 9.Each initial tree created is lexicalized, becauseeach one has a non-terminal symbol as the leftcorner element of its frontier.
There are a finitenumber of initial trees, because the number of non-cyclic paths in LCG must be finite.
Each initialtree is finite in size, because ach non-cyclic pathin LCG is finite in length.Most importantly, The set of initial trees isthe set of non-recursive left corner derivations in(,'.SA ASB B$ B AS B B$I I Ib b bFigure 9: Initial trees created by Step 2.S tep  3 This step constructs a set of left-recursive auxiliary trees corresponding to thecyclic path segments in L(TG that were ignored inthe previous step.
In particular, an attxiliary treeis created corresponding to each minimM cyclicpath in LCG that starts at a non-terminM sym-bol.For each minimal cycle in LCG from X to it-self, construct an auxiliary tree T by startingwith a root labeled X and repeatedly expand-ing left, corner frontier nodes using the rulesin the path as in Step 2.
When all the rules inthe path have been used, the left corner fron-tier node in T will be labeled X.
Mark thisas the foot node of T. While doing the above,leave all the other non-terminal symbols in Tunexpanded, and label them all substitutionnodes.The LC( ;  in Figure 8 has two minimal cyclicpaths (one from A to A via B and one from B toB via A).
This leads to the the two auxiliary treesshown in Figure 10, one for A and one for B.The attxiliary trees generated in this step arenot, necessarily lexicalized.
There are a finite num-ber of auxiliary trees, since the number of minimalcyclic paths in G must be finite.
Each auxiliarytree is finite in size, because ach minimal-cycle inLCG is finite in length.The set of trees that can he created by corn-biding the initial trees from Step 2 with the auxil-iary trees from Step 3 by adjoining auxiliary treesalong the left edge is the set of every left cornerderivation in (,'.
To see this, consider that ev-ery path in L( ;G can be represented as an initialnon-cyclic path with zero or more minimal cyclesinserted into it.The set of trees that can be created by corn-biding the initial trees from Step 2 with the auxil-iary trees from Step 3 using both substitution andadjunction is the set of every derivation in G. Tosee this, consider that every derivation in G canbe decomposed into a set of left corner derivationsin G that are combined with substitution.
In par-ticular, whenever a non-terminal node is not theleftmost child of its parent, it is the head of a sep-A BB B$ A S$A* S$ B* B$Figure 10: Auxiliary trees created by Step 3.125arate left corner derivation.S tep  4 This step lexicalizes the set of auxiliarytrees built in step 3, without altering the trees thatcan be derived.For each auxiliary tree T built in step 3, con-sider the frontier node A just to the right ofthe foot.
If this node is a terminal do nothing.Otherwise, remove T from the set of auxiliarytrees replace it with every tree that can beconstructed by substituting one of the initialtrees created in Step 2 for the node A in T.In the case of our continuing example, Step 4results in the set of auxiliary trees in Figure 11.Note that since G is finitely ambiguous, theremust be a frontier node to the right of the foot ofan attxiliary tree T. If not, then T would corre-spond to a derivation X:~X in G and 6' would beinfinitely ambiguous.After Step 4, every auxiliary tree is lexicalized,since every tree that does not have a terminal tothe right of its foot is replaced by one or more treesthat do.
Since there were only a finite number offinite initial and auxiliary trees to start with, thereare still only a finite number of finite attxiliarytrees.The change in the auxiliary trees caused byStep 4 does not alter the set of trees that can beproduced in any way, because the only change thatwas made was to make substitutions that could bemade anyway, and when a substitutable node waseliminated, this was only done after every possiblesubstitution at that node was performed.Note that the initial trees are left anchoredand the auxiliary trees are ahnost left anchoredin the sense that the leftmost frontier node otherthan the foot is a terminal.
This facilitates effi-cient left to right parsing.AAB B$A* S B B$A AS A* S A S$B B$ B AS B* BI I Ib b bFigure 1 l: Auxiliary trees created by Step 4.The procedure above creates a lexicalizedgrammar that generates exactly the same trees asG and therefore the same strings.
The only re-maining issue is the additional assumption that Gdoes not contain any empty rules.If (; contains an empty rule A ~ e one firstuses standard methods to transform (; into anequivalent grammar H that does not have anysuch rule.
When doing this, create a table showinghow each new rule added is related to the emptyrules removed.
Lexicalize H producing H'  usingthe procedure above.
Derivations in H '  result inelements of the tree set of H. By means of the ta-ble recording the relationship between (; and H,these trees can be converted to derivations in G.\[\]Add i t iona l  i ssuesThere are several places in the algorithm wheregreater freedom of choice is possible.
For instance,when lexicalizing the auxiliary trees created inStep 3, you need not do anything if there is anyfrontier node that is a terminal and you can chooseto expand any frontier node you want.
For in-stance you might want to choose the node thatcorresponds to the smallest number of initial trees.Alternatively, everywhere in the procedure,the word 'left' can be replaced by 'r ight' and viceversa.
This results in the creation of a set of rightanchored initial trees and right recursive auxiliarytrees.
This can be of interest when the right cor-ner derivation graph has less cycles than the leftcorner one.The number of trees in G' is related to thenumber of non-cyclic and minimal cycle paths inLCG.
In the worst case, this number rises veryfast as a function of the number of arcs in LCG,(i.e., in the number of rules in G).
(A fully con-nected graph of n 2 arcs  between n nodes has n!acyclic paths and n!
minimal cycles.)
However, inthe typical case, this kind of an explosion of treesis unlikely.Just as there can be many ways for a CF(~to derive a given string, there can be many waysfor an LCFG to derive a given tree.
For maximalefficiency, it would be desirable for the grammarG' produced by the procedure above to have noambiguity in they way trees are derived.
Unfortu-nately, the longer the minimal cycles in LCG, thegreater the tree-generating ambiguity the proce-dure will introduce in G'.
However, by modifyingthe procedure to make use of constraints on whatattxiliary trees are allowed to adjoin on what nodesin which initial trees, it should be possible to re-duce or even eliminate this ambiguity.All these issues are discussed at greater length126in Schabes and Waters (1993).Pars ing  LCFGSince LCFG is a restricted case of tree-adjoininggrammar (TAG), standard O(nG)-time TAGparsers (Vijay-Shanker, 1987; Schabes, 1991;Lang, 1990) can be used for parsing LCFG.
Fur-ther, they can be straightforwardly modified to re-quire at most O(n4)-tirne when applied to LCFG.However, this still does not take fifll advantage ofthe context-freeness of LCFC.This section describes a simple I)ottom-uprecognizer for LCFG that is in the style of theCKY parser for (IT(I;.
The virtue of this algo-rithm is that it shows in a simple manner how theO(n3)-time worst case complexity can be achievedfor LCFG.
Schabes and Waters (1993) describes amore practical and more elaborate (Earley-style)recognizer for LCFC, which achieves the samebounds.Suppose that G = (E, NT, I ,A ,S)  is anLCFG and that a l ' "a ,~ is an input string.
Wecan assume without loss of generality 1 that everynode in I U A has at most two children.Let 71 be a node in an elementary tree (identi-fied by the name of the tree and the position of thenode in the tree).
The central concept of the al-gorithrn is the concepts of spanning and covering.71 spans a string ai+l .
.
.a j  if and only if there issome tree derived by ('; for which it is the case thatthe fringe of the subtree rooted at 71 is ai+l "" "aj.In particular, a non-terminal node spans aj if andonly if the label on the node is aj.
A non-terrninalnode spans ai+ 1 .
.
.a j  if and only if ai+l .
.
.a j  isthe concatenation i left, to right order of stringsspanned by the children of the node.?
If 7 / does not subsume the foot node of an aux-iliary tree then: 71 covers the string ai+ 1 .
.
.
a j  ifand only if it spans a i+ l "  .
a j .?
If 7 / is on the spine of a right recursive auxiliarytree T then: 71 covers ai+l."
.aj if and only if7 / spans some strin~ that is the concatenationof ai+l - .
.a j  and a string spanned by the footof T. (This situation is illustrated by the rightdrawing in Figure 12, in which 7 / is labeled withB.)?
If 71 is on the spine of a left recursive auxiliarytree T then: 71 covers ai+\] " .aj if and only if 71spans some string that is the concatenation of astring spanned by the foot of T and ai+l .
.
.a j .
(This situation is illustrated by the left drawingin Figure 12, in which 71 is labeled with B.
)lit can be easily shown that by adding new nodes('4 "~ any L ,F(., can be transformed into an equivalentLC, FG satisfying this condition.A,  ai+l-.- aj ai+l.., aj A*Figure 12: Coverage of nodes on the spine.The algorithm stores pairs of the form (71, pos)in an n by n array C. In a pair, pos is either t (fortop) or b (for bottom).
For every node 7l in everyelementary tree in (;, the algorithm guarantees thefollowing.?
('l,b) e C\[i,j\] if and only if,I covers ai+l .
.
.a j .?
('l,t) E C\[i,j\] if and only if ('l,b} E C,\[i,j\] orai+l .
.
.a j  is the concatenation (in either order)of a string covered by 7 / and a string covered byan auxiliary tree that can be adjoined on 71 .The algorithm fills the upper diagonal portionof the array C\[i, j\] (0 < i < j _< n) for increasingvalues of j - i.
The process tarts by placing eachfoot node in every cell C'\[i,i\] and each terminalnode 71 in every cell C\[i, i + 1\] where 71 is labeleda i+ l  ?The algorithm then considers all possible waysof combining covers into longer covers.
In particu-lar, it fills the cells C\[i, i + k\] for increasing valuesof k by combining elements from the cells C\[i, j\]and C\[ j , i  + k\] for all j such that i < j < i + k.There are three situations where combination ispossible: sibling concatenation, left recursive con-catenation, and right recursive concatenation.Sibling concatenation is illustrated in Fig-ure 13.
Suppose that there is a node 7/0 (labeled B)with two children 711 (labeled A) and 712 (labeledA').
If (711 , t) E C\[i, j\] and ('12, t} E (7\[j, i + k\] then('1o, b) E C\[i, i + k\].Left recursive concatenation is illustrated inFigure 14.
Here, the cover of a node is combinedwith the cover of a left auxiliary tree that can beadjoined at the node.
Right recursive concatena-tion, which is shown in Figure 15 is analogous.For simplicity, the recognizer is written intwo parts.
A main procedure and a subpro-cedure Add(node, pos, i , j ) ,  which adds the pair(node, pos) into C\[i, j\].a.
.
.
.a.
ai+ 1 t+ l  J a j+ l ' "ak  "'" akFigure 13: Sibling concatenation.127Procedure recogn izerbegin;; foot node initialization ( ,\[z, i\])for i = 0 to nfor all foot node 71 in A callAdd0/, b, i, i);; terminal node initialization ((;\[i, i + 1\])fo r i=0 ton- lfor all node 71 in A U I labeled by ai+lcall Add0/, t, i, i + 1);; induction (G'\[i, i + k\] = (;\[i, j\] + (:\[j, i + k\])for k = 2 to nfor i = 0 to n -  kfo r j= i+ 1 to i+k-1;; sibling concatenationif (711 , l) 6 C,\[i, j\]and (712, t) e C\[j, i + k\]and r/1 is the left sibling of 7/2with common parent 71othen Add(710 , b, i, i + k);; left recursive concatenationif {71, b) E C\[i, j\]and (p, t} e (,'\[/, i + k\]and p is the root node of a left recursiveauxil iary tree that can adjoin on rlthen Add0j , t, i, i + k);; right recursive concatenationif {'l, b) e (;\[j, i + k\]and (p, t) E C\[i, j\]and p is the root node of a right recursiveauxiliary tree that can adjoin on 7 Ithen Add(r/, t, i, i + k)if (7/, z) e c\[0, 7qand 71 is labeled by ,5'and 71 is the root node of an initial tree in Ithen return acceptanceotherwise return rejectionendNote that the sole purl)ose of the codes t and bis to insure that only one auxiliary tree can adjoinon a node.
The procedure could easily be mod-ified to account for other constraints on the wayderivation should proceed such as those suggestedfor LTAGs (Schabes and Shieber, 1992).The procedure Add puts a pair into the arrayC.
If the pair is already present, nothing is (lone.However, if it is new, it is added to (7 and otherpairs may be added as well.
These correspond tocases where the coverage is not increased: whena node is the only child of its parent, when theA/2.,+ /2...ai+l... ~ A*a .
...a k J+ l  ai+l'"akFigure 14: Left recursive concatenation.A/2.,.
.
.
.
A* ... ak aj+i.,  a k ai+ 1 aj ai+ 1Figure 15: Right recursive concatenation.node is recognized without adjunction, and whensubstitution occurs.Procedure Add(r/ ,  pos, i, j )beginPut (rl, pos) in C,\[i, j\]if pos = t and r I is the only child of a parent Itcall Add(#, b, i, j )if pos = t and r?
is the root node of aninitial tree, for each substitution node pat which 71 can substitute call Add(p, t, i, j );; no adjunctionif pos = bif the node 7/does not have an OA constraintcall Add(r/, t, i, j )endThe O(n 3) complexity of the recognizer fol-lows from the three nested induction loops on k, iand j.
(Although the procedure Add is definedrecursively, the number of pairs added to (7 isbounded by a constant hat is independent of sen-tence length.
)By recording how each pair was introduced ineach cell of the array C, one can easily extend therecognizer to produce all derivations of the input.ConclusionLCFG combines much of the power of LTAG withtile computat ional  efficiency of CFG.
It supportsmost of the same linguistic analysis supported byLTAC.
In particular, most of the current LTAGfor English falls into LCFG.
In addition, LCFCcan lexicalize CFG without altering the trees pro-duced.
Finally, LCFG can be parsed in O(n3)-time.There are many directions in which the workon LCFG described here could be extended.
In128particular, one could consider stochastic exten-sions, LP~ parsing, and non-deterministic LR pars-ing.AcknowledgmentsWe thank John Coleman who, by question-ing whether the context-sensitivity of stochasticLTAG was actually being used for English, trig-gered this work.
We thank Aravind Joshi, Fer-nando Pereira, Stuart Shieber and B. Srinivas forvaluable discussions.REFERENCESAbeilld, Anne, Kathleen M. Bishop, Sharon Cote,and Yves Schabes.
1990.
A lexicalized treeadjoining grammar for English.
Technical Re-port MS-CIS-90-24, Department of Computerand Information Science, University of Penn-sylvania.Joshi, Aravind K. and Yves Schabes.
1992.
Tree-adjoining grammars and lexicalized gram-mars.
In Maurice Nivat and Andreas Podel-ski, editors, Tree Automata and Languages.Elsevier Science.Joshi, Aravind K., K. Vijay-Shanker, and DavidWeir.
1991.
The convergence of mildlycontext-sensitive grammatical formalisms.
InPeter Sells, Stuart Shieber, and Tom Wasow,editors, Foundational Issues in Nalural Lan-guage Processing.
MIT Press, Cambridge MA.Lang, Bernard.
1990.
The systematic onstruc-tions of Earley parsers: Application to theproduction of O(n 6) Earley parsers for TreeAdjoining Grammars.
In Proceedings of theIst International Workshop on Tree AdjoiningGrammars, Dagstuhl C, astle, FRG, August.Schabes, Yves, Anne Abeill6, and Aravind K.Joshi.
1988.
Parsing strategies with 'lexical-ized' grammars: Application to tree adjoininggrammars.
In Proceedings of the 12 th Interna-tional Conference on Computational Linguis-tics (COLING'88), Budapest, Hungary, An-gust.Schabes, Yves and Aravind K. Joshi.
1990.
Pars-ing with lexicalized tree adjoining grammar.In Masaru Tomita, editor, C, urrent Issuesin Parsing Technologies.
Kluwer AccademicPublishers.Schabes, Yves and Stuart Shieber.
1992.
An al-ternative conception of tree-adjoining deriva-tion.
In 20 th Meeting of the Association for(,'omputational Linguistics (A CL '92).Schabes, Yves and Richard C. Waters.
1993.
Lex-icalized context-free grammar: A cubic-timeparsable formalism that strongly lexicalizescontext-free grammar.
Technical Report 93-04, Mitsubishi Electric Research Laboratories,201 Broadway.
Cambridge MA 02139.Schabes, Yves.
1991.
The valid prefix prop-erty and left to right parsing of tree-adjoininggrammar.
In Proceedings of the second Inter-national Workshop on Parsing Technologies,Cancan, Mexico, February.Vijay-Shanker, K. and David Weir.
1993a.
Theequivalence of four extensions of context-freegrammars.
To appear in Mathematical Sys-tems Theory.Vijay-Shanker, K. and \[)avid Weir.
1993b.
Pars-ing some constrained grammar formalisms.To appear in Computational Linguistics.Vijay-Shanker, K. 1987.
A Study of Tree Adjoin-ing Grammars.
Ph.D. thesis, Department ofComputer and Information Science, Univer-sity of Pennsylvania.Weir, David J.
1988.
Character-izing Mildly Context-,5?nsitive Grammar For-malisms.
Ph.D. thesis, Department of Com-puter and Information Science, University ofPennsylvania.129
