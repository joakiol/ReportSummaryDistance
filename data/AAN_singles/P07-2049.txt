Proceedings of the ACL 2007 Demo and Poster Sessions, pages 193?196,Prague, June 2007. c?2007 Association for Computational LinguisticsMeasuring Importance and Query Relevance in Topic-focusedMulti-document SummarizationSurabhi Gupta and Ani Nenkova and Dan JurafskyStanford UniversityStanford, CA 94305surabhi@cs.stanford.edu, {anenkova,jurafsky}@stanford.eduAbstractThe increasing complexity of summarization systemsmakes it difficult to analyze exactly which mod-ules make a difference in performance.
We carriedout a principled comparison between the two mostcommonly used schemes for assigning importance towords in the context of query focused multi-documentsummarization: raw frequency (word probability) andlog-likelihood ratio.
We demonstrate that the advan-tages of log-likelihood ratio come from its known dis-tributional properties which allow for the identifica-tion of a set of words that in its entirety defines theaboutness of the input.
We also find that LLR is moresuitable for query-focused summarization since, un-like raw frequency, it is more sensitive to the integra-tion of the information need defined by the user.1 IntroductionRecently the task of multi-document summarizationin response to a complex user query has receivedconsiderable attention.
In generic summarization,the summary is meant to give an overview of theinformation in the documents.
By contrast, whenthe summary is produced in response to a user queryor topic (query-focused, topic-focused, or generallyfocused summary), the topic/query determines whatinformation is appropriate for inclusion in the sum-mary, making the task potentially more challenging.In this paper we present an analytical study of twoquestions regarding aspects of the topic-focused sce-nario.
First, two estimates of importance on wordshave been used very successfully both in generic andquery-focused summarization: frequency (Luhn,1958; Nenkova et al, 2006; Vanderwende et al,2006) and loglikelihood ratio (Lin and Hovy, 2000;Conroy et al, 2006; Lacatusu et al, 2006).
Whileboth schemes have proved to be suitable for sum-marization, with generally better results from log-likelihood ratio, no study has investigated in whatrespects and by how much they differ.
Second, thereare many little-understood aspects of the differencesbetween generic and query-focused summarization.For example, we?d like to know if a particular wordweighting scheme is more suitable for focused sum-marization than others.
More significantly, previousstudies show that generic and focused systems per-form very similarly to each other in query-focusedsummarization (Nenkova, 2005) and it is of interestto find out why.To address these questions we examine the twoweighting schemes: raw frequency (or word proba-bility estimated from the input), and log-likelihoodratio (LLR) and two of its variants.
These metricsare used to assign importance to individual contentwords in the input, as we discuss below.Word probability R(w) = nN , where n is the num-ber of times the word w appeared in the input and Nis the total number of words in the input.Log-likelihood ratio (LLR) The likelihood ratio ?
(Manning and Schutze, 1999) uses a backgroundcorpus to estimate the importance of a word and itis proportional to the mutual information betweena word w and the input to be summarized; ?
(w) isdefined as the ratio between the probability (undera binomial distribution) of observing w in the inputand the background corpus assuming equal proba-bility of occurrence of w in both and the probabilityof the data assuming different probabilities for w inthe input and the background corpus.LLR with cut-off (LLR(C)) A useful propertyof the log-likelihood ratio is that the quantity193?2 log(?)
is asymptotically well approximated by?2 distribution.
A word appears in the input sig-nificantly more often than in the background corpuswhen ?2 log(?)
> 10.
Such words are called signa-ture terms in Lin and Hovy (2000) who were the firstto introduce the log-likelihood weighting scheme forsummarization.
Each descriptive word is assignedan equal weight and the rest of the words have aweight of zero:R(w) = 1 if (?2 log(?
(w)) > 10), 0 otherwise.This weighting scheme has been adopted in severalrecent generic and topic-focused summarizers (Con-roy et al, 2006; Lacatusu et al, 2006).LLR(CQ) The above three weighting schemes as-sign a weight to words regardless of the user queryand are most appropriate for generic summarization.When a user query is available, it should informthe summarizer to make the summary more focused.In Conroy et al (2006) such query sensititivity isachieved by augmenting LLR(C) with all contentwords from the user query, each assigned a weightof 1 equal to the weight of words defined by LLR(C)as topic words from the input to the summarizer.2 DataWe used the data from the 2005 Document Under-standing Conference (DUC) for our experiments.The task is to produce a 250-word summary in re-sponse to a topic defined by a user for a total of 50topics with approximately 25 documents for eachmarked as relevant by the topic creator.
In com-puting LLR, the remaining 49 topics were used as abackground corpus as is often done by DUC partic-ipants.
A sample topic (d301) shows the complexityof the queries:Identify and describe types of organized crime thatcrosses borders or involves more than one country.
Namethe countries involved.
Also identify the perpetrators in-volved with each type of crime, including both individualsand organizations if possible.3 The ExperimentIn the summarizers we compare here, the variousweighting methods we describe above are used toassign importance to individual content words in theinput.
The weight or importance of a sentence S inGENERIC FOCUSEDFrequency 0.11972 0.11795(0.11168?0.12735) (0.11010?0.12521)LLR 0.11223 0.11600(0.10627?0.11873) (0.10915?0.12281)LLR(C) 0.11949 0.12201(0.11249?0.12724) (0.11507?0.12950)LLR(CQ) not app 0.12546(.11884?.13247)Table 1: SU4 ROUGE recall (and 95% confidenceintervals) for runs on the entire input (GENERIC) andon relevant sentences (FOCUSED).the input is defined asWeightR(S) =?w?SR(w) (1)where R(w) assigns a weight for each word w.For GENERIC summarization, the top scoring sen-tences in the input are taken to form a generic extrac-tive summary.
In the computation of sentence im-portance, only nouns, verbs, adjectives and adverbsare considered and a short list of light verbs are ex-cluded: ?has, was, have, are, will, were, do, been,say, said, says?.
For FOCUSED summarization, wemodify this algorithm merely by running the sen-tence selection algorithm on only those sentencesin the input that are relevent to the user query.
Insome previous DUC evaluations, relevant sentencesare explicitly marked by annotators and given to sys-tems.
In our version here, a sentence in the input isconsidered relevant if it contains at least one wordfrom the user query.For evaluation we use ROUGE (Lin, 2004) SU4recall metric1, which was among the official auto-matic evaluation metrics for DUC.4 ResultsThe results are shown in Table 1.
The focused sum-marizer using LLR(CQ) is the best, and it signif-icantly outperforms the focused summarizer basedon frequency.
Also, LLR (using log-likelihood ra-tio to assign weights to all words) perfroms signif-icantly worse than LLR(C).
We can observe sometrends even from the results for which there is nosignificance.
Both LLR and LLR(C) are sensitive tothe introduction of topic relevance, producing some-what better summaries in the FOCUSED scenario1-n 2 -x -m -2 4 -u -c 95 -r 1000 -f A -p 0.5 -t 0 -d194compared to the GENERIC scenario.
This is not thecase for the frequency summarizer, where using onlythe relevant sentences has a negative impact.4.1 Focused summarization: do we need queryexpansion?In the FOCUSED condition there was little (for LLRweighting) or no (for frequency) improvement overGENERIC.
One possible explanation for the lack ofclear improvement in the FOCUSED setting is thatthere are not enough relevant sentences, making itimpossible to get stable estimates of word impor-tance.
Alternatively, it could be the case that manyof the sentences are relevant, so estimates from therelevant portion of the input are about the same asthose from the entire input.To distinguish between these two hypotheses, weconducted an oracle experiment.
We modified theFOCUSED condition by expanding the topic wordsfrom the user query with all content words from anyof the human-written summaries for the topic.
Thisincreases the number of relevant sentences for eachtopic.
No automatic method for query expansion canbe expected to give more accurate results, since thecontent of the human summaries is a direct indica-tion of what information in the input was importantand relevant and, moreover, the ROUGE evaluationmetric is based on direct n-gram comparison withthese human summaries.Even under these conditions there was no signif-icant improvement for the summarizers, each get-ting better by 0.002: the frequency summarizer getsR-SU4 of 0.12048 and the LLR(CQ) summarizerachieves R-SU4 of 0.12717.These results seem to suggest that considering thecontent words in the user topic results in enough rel-evant sentences.
Indeed, Table 2 shows the mini-mum, maximum and average percentage of relevantsentences in the input (containing at least one con-tent words from the user the query), both as definedby the original query and by the oracle query ex-pansion.
It is clear from the table that, on aver-age, over half of the input comprises sentences thatare relevant to the user topic.
Oracle query expan-sion makes the number of relevant sentences almostequivalent to the input size and it is thus not sur-prising that the corresponding results for content se-lection are nearly identical to the query independentOriginal query Oracle query expansionMin 13% 52%Average 57% 86%Max 82% 98%Table 2: Percentage of relevant sentences (contain-ing words from the user query) in the input.
Theoracle query expansion considers all content wordsform human summaries of the input as query words.runs of generic summaries for the entire input.These numbers indictate that rather than findingways for query expansion, it might instead be moreimportant to find techniques for constraining thequery, determining which parts of the input are di-rectly related to the user questions.
Such techniqueshave been described in the recent multi-strategy ap-proach of Lacatusu et al (2006) for example, whereone of the strategies breaks down the user topicinto smaller questions that are answered using ro-bust question-answering techniques.4.2 Why is log-likelihood ratio better thanfrequency?Frequency and log-likelihood ratio weighting forcontent words produce similar results when appliedto rank all words in the input, while the cut-offfor topicality in LLR(C) does have a positive im-pact on content selection.
A closer look at thetwo weighting schemes confirms that when cut-offis not used, similar weighting of content words isproduced.
The Spearman correlation coefficient be-tween the weights for words assigned by the twoschemes is on average 0.64.
At the same time, it islikely that the weights of sentences are dominatedby only the top most highly weighted words.
Inorder to see to what extent the two schemes iden-tify the same or different words as the most impor-tant ones, we computed the overlap between the 250most highly weighted words according to LLR andfrequency.
The average overlap across the 50 setswas quite large, 70%.To illustrate the degree of overlap, we list beloware the most highly weighted words according toeach weighting scheme for our sample topic con-cerning crimes across borders.LLR drug, cocaine, traffickers, cartel, police, crime, en-forcement, u.s., smuggling, trafficking, arrested, government,seized, year, drugs, organised, heroin, criminal, cartels, last,195official, country, law, border, kilos, arrest, more, mexican, laun-dering, officials, money, accounts, charges, authorities, cor-ruption, anti-drug, international, banks, operations, seizures,federal, italian, smugglers, dealers, narcotics, criminals, tons,most, planes, customsFrequency drug, cocaine, officials, police, more, last, gov-ernment, year, cartel, traffickers, u.s., other, drugs, enforce-ment, crime, money, country, arrested, federal, most, now, traf-ficking, seized, law, years, new, charges, smuggling, being, of-ficial, organised, international, former, authorities, only, crimi-nal, border, people, countries, state, world, trade, first, mexican,many, accounts, according, bank, heroin, cartelsIt becomes clear that the advantage of likelihoodratio as a weighting scheme does not come frommajor differences in overall weights it assigns towords compared to frequency.
It is the signifi-cance cut-off for the likelihood ratio that leads tonoticeable improvement (see Table 1).
When thisweighting scheme is augmented by adding a scoreof 1 for content words that appear in the user topic,the summaries improve even further (LLR(CQ)).Half of the improvement can be attributed to thecut-off (LLR(C)), and the other half to focusingthe summary using the information from the userquery (LLR(CQ)).
The advantage of likelihood ra-tio comes from its providing a principled criterionfor deciding which words are truly descriptive of theinput and which are not.
Raw frequency provides nosuch cut-off.5 ConclusionsIn this paper we examined two weighting schemesfor estimating word importance that have been suc-cessfully used in current systems but have not to-date been directly compared.
Our analysis con-firmed that log-likelihood ratio leads to better re-sults, but not because it defines a more accurate as-signment of importance than raw frequency.
Rather,its power comes from the use of a known distributionthat makes it possible to determine which words aretruly descriptive of the input.
Only when such wordsare viewed as equally important in defining the topicdoes this weighting scheme show improved perfor-mance.
Using the significance cut-off and consider-ing all words above it equally important is key.Log-likelihood ratio summarizer is more sensitiveto topicality or relevance and produces summariesthat are better when it take the user request into ac-count than when it does not.
This is not the case fora summarizer based on frequency.At the same time it is noteworthy that the genericsummarizers perform about as well as their focusedcounterparts.
This may be related to our discoverythat on average 57% of the sentences in the doc-ument are relevant and that ideal query expansionleads to a situation in which almost all sentencesin the input become relevant.
These facts couldbe an unplanned side-effect from the way the testtopics were produced: annotators might have beeninfluenced by information in the input to be sum-marizied when defining their topic.
Such observa-tions also suggest that a competitive generic summa-rizer would be an appropriate baseline for the topic-focused task in future DUCs.
In addition, includingsome irrelavant documents in the input might makethe task more challenging and allow more room foradvances in query expansion and other summary fo-cusing techniques.ReferencesJ.
Conroy, J. Schlesinger, and D. O?Leary.
2006.
Topic-focusedmulti-document summarization using an approximate oraclescore.
In Proceedings of the COLING/ACL?06 (Poster Ses-sion).F.
Lacatusu, A. Hickl, K. Roberts, Y. Shi, J. Bensley, B. Rink,P.
Wang, and L. Taylor.
2006.
Lcc?s gistexter at duc 2006:Multi-strategy multi-document summarization.
In Proceed-ings of DUC?06.C.
Lin and E. Hovy.
2000.
The automated acquisition of topicsignatures for text summarization.
In Proceedings of COL-ING?00.C.
Lin.
2004.
Rouge: a package for automatic evaluation ofsummaries.
In Proceedings of the Workshop on Text Sum-marization Branches Out (WAS 2004).H.
P. Luhn.
1958.
The automatic creation of literature abstracts.IBM Journal of Research and Development, 2(2):159?165.C.
Manning and H. Schutze.
1999.
Foundations of StatisticalNatural Language Processing.
MIT Press.A.
Nenkova, L. Vanderwende, and K. McKeown.
2006.A compositional context sensitive multi-document summa-rizer: Exploring the factors that influence summarization.
InProceedings of ACM SIGIR?06.A.
Nenkova.
2005.
Automatic text summarization of newswire:lessons learned from the document understanding confer-ence.
In Proceedings of AAAI?05.L.
Vanderwende, H. Suzuki, and C. Brockett.
2006.
Microsoftresearch at duc 2006: Task-focused summarization with sen-tence simplification and lexical expansion.
In Proceedings ofDUC?06.196
