Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 257?260,Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational LinguisticsSemantic Specificity in Spoken Dialogue RequestsBen HixonHunter College of TheCity University of New YorkNew York, NY, USAshixon@hunter.cuny.eduRebecca J. PassonneauColumbia UniversityNew York, NY, USAbecky@cs.columbia.eduSusan L. EpsteinHunter College, andThe Graduate Center of TheCity University of New YorkNew York, NY, USAsusan.epstein@hunter.cuny.eduAbstractAmbiguous or open-ended requests to a di-alogue system result in more complex dia-logues.
We present a semantic-specificity met-ric to gauge this complexity for dialogue sys-tems that access a relational database.
An ex-periment where a simulated user makes re-quests to a dialogue system shows that seman-tic specificity correlates with dialogue length.1 IntroductionA dialogue system (DS) and its users have asymmet-ric knowledge.
The DS has access to knowledge theuser is not privy to, and the user has intentions thatthe DS attempts to recognize.
When the user?s inten-tions are difficult for her to specify fully, the user andDS must collaborate to formulate the intention.
Thethesis of this work is that a DS can assess the speci-ficity of its knowledge with respect to the user inten-tions it is designed to address.
Our principal resultis that, for a DS that queries a relational database,measures of the ambiguity of database attributes canbe used both to assess the scope of the DS?s taskand to guide its dialogue strategy.
To demonstrateour thesis, we have developed a semantic specificitymetric applicable to any DS that queries a relationaldatabase.
This metric measures the degree to whichone or more attributes can uniquely specify an itemin the database.
Attributes whose values are moreoften ambiguous have lower semantic specificity.CheckItOut is a book request DS that referencesa copy of the catalogue at the Heiskell Braille andTalking Book Library with its 71,166 books (Epsteinet al, In Press).
We focus on three book attributes:AUTHOR, TITLE and CALL NUMBER.
Only the lat-ter is guaranteed to identify a unique book.
Of the64,907 distinct TITLE values, a large majority returna unique book (N=59,236; 91.3%).
Of the 28,045distinct AUTHOR values, about two thirds return aunique book (N=17,980; 64.1%).Query return Distinct Distinctsize TITLE values AUTHOR values1 59236 179802 5234 43773 345 1771...10 2 168...184 ?
1Total 64907 28045Table 1: When used as a query, many TITLE values returnunique books, but AUTHOR values are less specific.To compare the specificity of TITLE and AUTHOR,we calculated query return size, the number of dis-tinct books in the Heiskell database returned by eachpossible attribute value.
Table 1 tallies how manyattribute values have the same query return size.
TI-TLE partitions the books into 10 subsets, where thetwo most ambiguous TITLE values, Collected Sto-ries and Sanctuary, each return 10 distinct books.AUTHOR produces 89 subsets; its most ambiguousvalue, Louis L?Amour, returns 184 distinct books.Clearly, TITLE has higher specificity than AUTHOR.After a survey of related work, this paper defines asemantic specificity metric that is a weighted sum of257the number of query return sizes for one or more at-tributes.
We show through simulation that dialoguelength varies with semantic specificity for a DS witha simple system-initiative dialogue strategy.2 Related WorkLittle work has been reported on measures of therelationship between dialogue complexity and thesemantic structure of a DS application?s database.Zadrozny (1995) proposes Q-Complexity, whichroughly corresponds to vocabulary size, and is es-sentially the number of questions that can be askedabout a database.
Pollard and Bierman (2000) de-scribe a similar measure that considers the numberof bits required to distinguish every object, attribute,and relationship in the semantic space.Gorin et al (2000) distinguish between semanticand linguistic complexity of calls to a spoken DS.Semantic complexity is measured by inheritance re-lations between call types, the number of type labelsper call, and how often calls are routed to humanagents.
Linguistic complexity is measured by utter-ance length, vocabulary size and perplexity.Popescu et al (2003) identify a class of ?seman-tically tractable?
natural language questions that canbe mapped to an SQL query to return the question?sunique correct answer.
Ambiguous questions withmultiple correct answers are not considered seman-tically tractable.
Polifroni and Walker (2008) ad-dress how to present informative options to userswho are exploring a database, for example, to choosea restaurant.
When a query returns many options,their system summarizes the return using attributevalue pairs shared by many of the members.3 Semantic SpecificityThe database queried by a DS can be regarded asthe system?s knowledge.
Consequently, the seman-tic structure of the database and the way it is popu-lated constrain the requests the system can addressand how much information the user must provide.Intuitively, Table 1 shows that TITLE has a highersemantic specificity than AUTHOR.
Our goal is toquantify the query ambiguity engendered by the in-stantiation of any database table.Often a user does not know in advance whichcombination of attribute values uniquely communi-cates her intent to the system.
In addition, the DSdoes not know what the user wants until it has of-fered an item that the user confirms, whether ex-plicitly or implicitly.
The remainder of this sectiondefines the specificity of individual and multiple at-tributes with respect to a set of database instances.3.1 Specificity for Single AttributesWhen a user requests information about one or moreentities, the request can map to many more databaseinstances than intended.
Let I be a set of instances(rows) in a database relation, and let ?
be an attributeof I with values V that occur in I .
Denote by q(v, ?
)the query return size for v ?
V on ?, the number ofinstances of I returned by the query ?
= v. When-ever q(v, ?)
= 1, the query returns exactly one in-stance in I; attributes with more such values havehigher specificity.
If q(v, ?)
= 1 for every v, then ?is maximally specific with respect to I .Let Q?
be the set of d?
distinct query return sizesq(v, ?)
returned on I .
We call Q?
the query returnsize partition for ?.
Q?
induces a partition of Vinto subsets Vj , j ?
Q?
such that a query on everyvalue in a given subset returns the same number ofinstances.
Table 1 shows two such partitions.
Wenow define the specificity S(?, I) of attribute ?withrespect to I as a weighted sum of the sizes of thesubsets in the partition induced by ?, normalized by|I|, the number of instances in I:S(?, I) = 1|I|?j?Q?w(j) ?
|Vj | (1)The weight function w in (1) addresses the num-ber of distinct values in each subset of Q?.
A largerquery return size indicates a more ambiguous at-tribute, one less able to distinguish among instancesin I .
To produce specificity values in the range [0, 1],w(j) should decrease as j increases, but not penal-ize any query that returns a single instance, that is,w(1) = 1.
The faster w decreases, the more it pe-nalizes an ambiguous attribute.
Here we take as wthe inverse of the query return size, w(j) = 1j .For our CheckItOut example, equation (1) scoresTITLE?s specificity as 0.871 and AUTHOR?s speci-ficity much lower, at 0.300.
This matches our intu-ition.
The third attribute with which a user can ordera book, CALL NUMBER, was designed as a primarykey and so has a perfect specificity of 1.000.2583.2 Specificity for Multiple AttributesThe specificity of a set ?
= {?1, ?2, ..., ?k} of k at-tributes on a set of instances I measures to what de-gree a combination (one value for each attribute in?)
specifies a restricted set of instances in I .
Let Vbe the combinations for ?
that occur in I , and letq(v, ?)
be the query return size for v ?
V .
ThenQ?
, the set of d?
distinct query return sizes, inducesa partition on V into subsets Vj , j ?
Q?
where com-binations in the same subset return the same numberof instances.
We take w(j, k) = 1jk to penalize am-biguity more heavily when there are more attributes.Then the specificity of ?
with respect to I isS(?, I) = 1|I|?j?Q?w(j, k) ?
|Vj | (2)Using this equation, the specificity of ?
={TITLE, AUTHOR} is 0.880.
Interestingly, this is notmuch higher than the 0.871 TITLE specificity alone,which indicates that, in this particular databaseinstantiation, AUTHOR has little ability to disam-biguate a TITLE query.
This is because many?books?
in the Heiskell catalog appear in two for-mats, Braille and audio.
This duplication creates anambiguity that is better resolved by prompting theuser for CALL NUMBER or FORMAT.
In some cases,a value for FORMAT might still result in ambiguity;for example, different recorded readers produce dif-ferent audio versions of the same title and author.In contrast, the large difference between AUTHOR?svery low specificity (0.300) and that of {TITLE, AU-THOR} (0.880) suggests that, given an ambiguousauthor, it would in general be a good strategy for theDS to then prompt the user for the title.Because specificity is a function of a database in-stantiation, specificity can be used to guide dialoguestrategy.
For the books in Heiskell?s catalogue thatcannot be uniquely identified by AUTHOR and TITLEalone, it can be determined a priori that some bookrequests cannot be disambiguated without additionalattribute values.4 Specificity in Simulated DialoguesA DS faced with an ambiguous query should entera disambiguation subdialogue.
It can either offer alist of matching instances that must be individuallyconfirmed or rejected by the user, or indicate to theuser that an attribute value is ambiguous and requestadditional information.
In general, a less specificuser request should increase the dialogue length.We tested this hypothesis with a simulated userthat interacted with the CheckItOut book-order DSthrough a text interface.
As in our work where hu-man subjects called CheckItOut, a new scenario foreach call included ?patron?
identification data and alist of four books with their authors, titles, and callnumbers.
In each dialogue, the simulator receiveda scenario, the DS confirmed the simulator?s ?iden-tity,?
and then the simulator requested the books.The simulator uses a rule-based dialogue strat-egy with some parameters, such as persistence inre-ordering the same book, that can be set or ran-domized to represent a set of real users more real-istically.
For this experiment, the simulator was setto request books only by author, and to be willing toengage in a disambiguation subdialogue of arbitrarylength.
When the system cannot uniquely identify arequested book, it begins a disambiguation subdia-logue, an example of which is shown in Figure 1.To avoid addressing information presentation issuessuch as those explored in (Polifroni and Walker,2008), CheckItOut followed a simple strategy of of-fering each next candidate book in a query return,and user studies with CheckItOut restricted query re-turn size to a maximum of three books.
For the sim-ulations, we expect an inverse relationship betweenspecificity and dialogue length.S: NEXT BOOK?U: Dan SimmonsS: DID YOU SAY DAN SIMMONS?U: YesS: WOULD YOU LIKE ENDYMION BY THIS AUTHOR?U: NoS: WOULD YOU LIKE THE CROOK FACTORYBY THIS AUTHOR?U: YesFigure 1: Sample disambiguation subdialogue.We randomly sampled Heiskell?s full set of71,166 books to create five smaller instantiationsof 1000 books each.
We deliberately sampledat different frequencies within each subset of theoriginal partition induced by AUTHOR, so thatS(AUTHOR, T ) for instantiation T ranged from0.3528 to 1.000.
For each instantiation we simulated25 dialogues.
Conditions of relatively lower speci-2590.0 0.2 0.4 0.6 0.8 1.0Author specificity3540455055Mean dialogue lengthFigure 2: Dialogue length averaged across 25 simulateddialogues for each run of 5 different attribute specificitylevels, shown with 95% confidence intervals.ficity result in more dialogues like the one shown inFigure 1, with multiple turn exchanges where the DSoffers the simulator different books by the requestedauthor.
As specificity approaches 1.0, the first bookoffered by the DS is more frequently the requestedbook, so no disambiguation is required, and the min-imum dialogue length of 43 turns is achieved.
Fig-ure 2 compares the mean dialogue length for eachsub-instantiation to its author specificity, and clearlyshows that dialogue length increases as author speci-ficity decreases.
The error bars shrink as specificityincreases because there is less variation in dialoguelength when there are fewer candidate books forCheckItOut to offer.5 Conclusion and Future WorkSemantic specificity has two important applications.Because it predicts how likely a value for a databaseattribute (or a combination for a set of attributes) isto return a single database instance, semantic speci-ficity can help formulate subdialogues with a prior-ity order in which the DS should prompt users forattributes.
Because it is a predictor for dialoguelength, semantic specificity could also be used toevaluate whether a DS dialogue strategy incurs theexpected costs.
Of course, many factors other thansemantic specificity affect DS dialogue complexity,particularly the relation between users?
utterancesand the semantics of the database.
In the examplesgiven here, the way users refer to books correspondsdirectly to attribute values in the database.
Otherdomains may require a more complex procedure tomap between the semantics of the database and thesemantics of natural language expressions.Finally, how well semantic specificity with re-spect to a database instantiation predicts dialoguelength depends in part on how closely the databaseattributes correspond to information that users canreadily provide.
Here, AUTHOR and TITLE are con-venient both for users and for the database seman-tics.
However, the maximally specific CALL NUM-BER is often unknown to the user.
For DSs wherethe database attributes differ from those that can beextracted from user utterances, we intend to exploreenhanced or additional metrics to predict dialoguelength and guide dialogue strategy.AcknowledgmentsWe thank Julia Hirschberg for helpful comments,and Eric Schneider for help with the user simulator.National Science Foundation awards IIS-0745369,IIS-0744904 and IIS-084966 funded this project.ReferencesSusan L. Epstein, Rebecca J. Passonneau, Tiziana Ligo-rio, and Joshua Gordon.
In Press.
Data mining to sup-port human-machine dialogue for autonomous agents.In Proceedings of Agents and Data Mining Interaction(ADMI 2011).
Springer-Verlag.A.
L. Gorin, J. H. Wright, G. Riccardi, A. Abella, andT.
Alonso.
2000.
Semantic information processing ofspoken language.
In Proceedings of ATR Workshop onMultiLingual Speech Communication, pages 13?16.Joseph Polifroni and Marilyn Walker.
2008.
Intensionalsummaries as cooperative responses in dialogue: Au-tomation and evaluation.
In Proceedings of ACL-08:HLT, pages 479?487, Columbus, Ohio, June.
Associa-tion for Computational Linguistics.Shannon Pollard and Alan W. Bierman.
2000.
A measureof semantic complexity for natural language systems.In NAACL-ANLP 2000 Workshop: Syntactic and Se-mantic Complexity in Natural Language ProcessingSystems, pages 42?46, Stroudsburg, PA, USA.
Asso-ciation for Computational Linguistics.Ana-Maria Popescu, Oren Etzioni, and Henry Kautz.2003.
Towards a theory of natural language inter-faces to databases.
In Proceedings of the 8th inter-national conference on Intelligent user interfaces, IUI?03, pages 149?157, New York, NY, USA.
ACM.Wlodek Zadrozny.
1995.
Measuring semantic com-plexity.
In Moshe Koppel, Eli Shamir, and MartinGolumbic, editors, Proceedings of the Fourth Bar IlanSymposium on Foundations of Artificial Intelligence(BISFAI 1995), pages 245?254.260
