PARSING WITH A SMALL  D ICT IONARY FOR APPL ICAT IONS SUCH AS TEXTTO SPEECHDouglas D. O'ShaughnessyINRS-Telecommunications3 Place du CommerceNuns' Island, Quebec H3E 1H6 CanadaWhile the general problem of parsing all English text is as yet unsolved, there are practical applicationsfor text processors of limited parsing capability.
In automatic synthesis of speech from text, for example,speech quality is highly dependent on realistic prosodic patterns.
Current synthesizers have difficultyobtaining sufficient linguistic information from an input text to specify prosody properly.
When peoplespeak, they often use the syntactic structure of the text message to determine when to pause and whichwords to stress.
Previous work on natural language processing generally assumes access to a largedictionary so that parts of speech are known for virtually all possible words in an input text.
However,some practical natural language systems are constrained to limit computer memory and access time byminimizing dictionary size.
Furthermore, in most published text to speech work, the parsing problemis only briefly mentioned, or parsing occurs on a local basis, ignoring important syntactic structures thatencompass the entire sentence.
The system described here recognizes function words and some contentwords, and uses syntactic constraints to estimate which words are likely to form phrases.
This paper isthe first to report on parsing details specifically for speech synthesis, while using only a small dictionary(of about 300 words).1 INTRODUCTIONParsing a sentence requires information about he partsof speech of its words.
Previous work on naturallanguage parsing has generally assumed that parts ofspeech are known for all words in an input text (Marcus1980, Grishman 1986).
For example, the EPISTLEsystem (Jensen 1983, Heidorn 1982) employs a 130,000-word dictionary.
Although a small dictionary of 200-300words suffices for the function words (e.g., preposi-tions, pronouns), being able to identify nouns and verbshas required much larger dictionaries.
Locating theverbs in a sentence is particularly useful to specifyingprosody, because pauses often occur immediately be-fore or after a verb group.
The system described in thispaper recognizes all function words and some contentwords, and uses syntactic onstraints o estimate whichwords are likely to form phrases.
It is compared tosimilar systems using dictionaries in excess of 2,000words, which have been only partially described in theliterature (Dewar 1969, Bachenko 1986).
To the author'sknowledge, these latter systems are the only other onesthat have attempted parsing on arbitrary text withdictionaries of fewer than 10,000 words.
Because theparser described here has access only to a very smalldictionary, it cannot exploit many of the advances inparsing in recent years.
What is explained below, how-Computational Linguistics, Volume 15, Number 2, June 1989ever, is that accurate parsing need not require largedictionaries.1.1 SYNTHESIS APPLICATIONSThe input for automatic speech synthesis ystems cantake several forms.
In question-answer applications, auser may access a data base with information stored innon-textual form, e.g., tables or numbers.
Such a sys-tem can use a very limited grammar in formulating thesyntactic structure of the output speech ("concept ospeech": Young 1979).
In some future systems, thequeries may be in the form of speech, and automaticspeech recognizers will extract prosody and syntaxpatterns, which can in turn be of assistance in synthe-sizing responses.A more immediate synthesis application is automatictext to speech synthesis (Klatt 1987).
The conversion ofarbitrary English text to speech is useful in aids for theblind and in general voice response systems.
Visuallyhandicapped people (few of whom know Braille) canhave direct access to the vast wealth of printed infor-mation via an optical character reader and a text tospeech synthesizer.
Concerning voice response, muchinformation i  data bases is in the form of text; with anautomatic text to speech system, people could tele-phone a remote data base and hear a vocal version ofthe information.
The queries must be entered through0362-613X/89/010097-108-$03.00 97Douglas D. O'Shaughnessy Parsing with a Small Dictionary for Applications such as Text to Speechthe telephone keypad or via speech of isolated words(where prosody and syntax plays no role), but theoutput speech can be in the form of sentences.In synthesis from a text of English sentences, thenaturalness and intelligibility of the output speech ishighly dependent upon realistic prosodic patterns(O'Shaughnessy 1983a).
Current synthesizers have dif-ficulty obtaining sufficient linguistic information froman input text to specify prosody properly.
The syntacticstructure of the text, in particular, is a major factor indetermining where a speaker should pause, whichwords to stress, and how to use pitch rises and falls.However, the problem of parsing natural English, evenusing a large dictionary indicating parts of speech for allpossible words, is as yet unsolved.
English allows manysyntactic onstructions, which one recognizes whenreading a text aloud.
Text to speech systems, especiallywhen pronouncing sentences with few punctuationmarks, perform much more poorly than humans do.
Insome systems, the problem is further complicated be-cause the number of entries in the dictionary must beminimized for economy.
Such systems usually employletter to phoneme rules, and a small dictionary topronounce words for which the rules are inadequate.For certain words, knowledge of their syntactic role isimperative for proper pronunciation; e.g., refuse, wind,lives, separate use different sounds depending onwhether they act as noun, verb, or adjective.Very little work on parsing sentences for speechsynthesis purposes has been reported.
This paper is thefirst to give parsing details specifically for synthesiswhile using a dictionary of fewer than 300 words.
Inmost other references, the parsing problem is onlymentioned in passing (Flanagan 1970; Coker 1973; Klatt1987).
The most documented system, MITalk-79 (Allen1987), uses a large dictionary and treats parsing only ona local basis, ignoring important syntactic structuresthat encompass the entire sentence.Restricting the dictionary to a few hundred entrieslimits the ability of a parser to correctly analyze alltexts.
For text to speech, however, it is unnecessary tohave a complete parse of the text to be spoken.
Thedictionary and pronunciation rules must be powerfulenough to avoid mistakes in the translation of lettersinto phonemes, of course.
But syntactic structure isuseful mostly in specifying prosody, e.g., when topause, which words to stress, and whether to raise orlower pitch at the end of a sentence.
Syntactic informa-tion sufficient to specify prosody rarely requires acomplete parse.
Positions of major syntactic boundariesand identification of stressed words are of major con-cern.
Confusions between nouns and adjectives, forinstance, have little bearing on prosody.
Using a flexibleparser, moreover, minimizes the chance of meeting anunparsable text (Weischedel 1980).
A parsing failure insynthesis ystems is only serious if it results in anincorrect prosodic assignment that adversely affects theintelligibility or correct interpretation of the output98speech.
Whi\]le a local parsing error in one part of asentence may lead to errors elsewhere in the sentence,many minor errors that occur in our parser due to use ofa small dictionary have little effect on the importantaspects of the global sentence parse.1.2 SYNTAX AND PROSODYThe relationship between a text and its prosody iscomplex.
Speakers vary pitch, duration, and intensity(the aspects of prosody) primarily to highlight certainwords for the listener and to partition the utterance intoshort segments for easier perceptual processing(O'Shaughnessy 1983b).
Speakers tend to pause atmajor syntactic boundaries, but the frequency andduration of the pauses also reflect the length of thephrases (measured by the number of words or syllables)between pauses (Gee 1983).
Syntactic boundaries arealso often cued, in addition to pauses, by a pitch riseand lengthening of the final syllable prior to the bound-ary.
Speakers have much freedom in choosing whereand how long to pause and which words to emphasize;such choices are motivated by their desire to commu-nicate meaning to a listener.
Thus the semantics of atext is as important for specifying prosody as its syn-tactic structure (Selkirk 1984).
Unfortunately, auto-matic semantic analysis of arbitrary text is very difficultand not feasible for text to speech (for concept tospeech synthesis, on the other hand, semantics may bemore readily obtained).
Since syntactic structure corre-lates well with prosody in speech spoken at a normalrate, without emotional nd other contextual influences,the parse of a text is a feasible alternative to semanticanalysis for text to speech prosody.Besides indicating likely pause locations, the othermajor way that syntax influences prosody is that speak-ers stress important words, i.e., words that are unex-pected by listeners and add most to the informationcontent of an utterance.
Thus most function words arenot stressed.
A dictionary that identifies the functionwords can cue a synthesizer to stress all other words.The amount of stress a word receives is proportional toits importance (or its unexpectedness).
Small functionwords occurring in syntactically restricted (and thussomewhat redundant) positions rarely have semanticimportance.
As far as part of speech is concerned,howew,'r, the words with the greatest stress are includedin our dictionary: sentential dverbs, not, modal verbs,quantifiers, and interrogative words tend to be morestressed than nouns, verbs, and adjectives (O'Shaugh-nessy 1983b).
Among the unidentified words, there is nolarge variance in stress due to part of speech, andtherefore no need to further specify them for stresspurposes.
Due to the large prosodic effects at pauses,howew,~r, we must try to specify the syntactic role ofsuch words sufficiently to find pause locations.Syntactic structure also affects prosody in otherways besides pausing and stress (O'Shaughnessy 1979).Pitch rises sharply at the end of a question asking for aComputational Linguistics, Volume 15, Number 2, June 1989Douglas D. O'Shaughnessy Parsing with a Small Dictionary for Applications uch as Text to Speechyes or no response.
Parenthetic expressions, oftenoffset from the main part of a sentence by commas orparentheses, are uttered with reduced pitch.
Vocativescan be distinguished from appositives by different pitchpatterns.
Parsing information is useful to a synthesizerto handle all of the above effects.2 SEGMENTING SENTENCES INTO PHRASESFor text to speech, the primary task for a parser is tosegment a sentence into phrasal units each containing afew words.
Such units often act as prosodic (intona-tional) groups.
Pauses are usually restricted to come atthe boundaries of these units, and the final word (amongothers) in each unit is usually stressed.
Determining thehigher-level syntactic structure that links these phrasalunits together is often more difficult.Thus parsing for speech synthesis can employ twostrategies: local and global.
The local strategy typicallyoperates first and goes left to right through each sen-tence, i.e., as the words enter the system.
For real timeapplications, it may be important o output parsingresults even before the final words of a sentence areavailable.
Since a global strategy may require xaminingas much as the entire sentence, it may revise the parseof the early parts of a sentence as later words areanalyzed.
The global analysis hould also attempt left toright (real time) analysis; this is adequate for mostsentences, but ones with complex syntactic structure(e.g., unpunctuated subordinate clauses) often requireexamination of the entire sentence for a correct parse.Sentence-final punctuation (!
?)
can significantly affectthe sentence's prosody (and parse, to a lesser extent).In the case of long sentences with such final punctua-tion, however, the prosodic hanges due to the punctu-ation primarily affect only the last clause.Locally, our parser groups words likely to act asprosodic units.
This means composing various phrases(perhaps maller than traditional linguistic units) out ofcomponent words: 1. noun group (NG), which consistsof a noun and its immediately preceding words (e.g.,article, quantifier); 2. verb group (VG), which consistsof a verbal word optionally preceded by modal andauxiliary verbs; 3. prepositional phrase (PP), whichconsists of a preposition followed by a NG; 4. adjectivalphrase (AdjP), which consists of an adjective, possiblypreceded by an adverb; 5. adverbial phrase (AdvP) (seeAppendix l).
(This list is similar to that in Bachenko1986.)
For the purposes of local parsing, NGs and VGsare more useful units than noun phrases (NPs), whichconsist of an NG followed by PPs or AdjPs, and verbphrases (VPs), which consist of a VG followed by itscomplement(s).
Pauses are rare within word sequencescorresponding to the five basic phrasal units noted here,but often occur within an NP or VP.
To help locatephrase boundaries, the parser exploits constraints onword order in NGs and VGs; when normal orderComputational Linguistics, Volume 15, Number 2, June 1989appears to have been violated, it is likely that a phraseboundary has occurred at the point of deviation.The problem of sentence segmentation is assisted bypunctuation marks (e.g., commas), which often occur atclause boundaries.
However, many sentences havelittle internal punctuation.
The word sequences thatcolons and semicolons delimit can be treated as sen-tences for prosodic purposes.
Left marks (quotes, pa-rentheses, brackets, braces) act as phrase-introducingmarks, and corresponding right marks terminatephrases.
Both dashes and commas tend to partition thesentence into clauses and phrases, and are likely placesfor pauses and prosodic marking, especially as thelengths of the phrases they delimit increase in size.However, commas are not restricted to delimiting majorsyntactic units.
In lists of two or more units (of similarsyntactic identity), commas are often internal to majorphrases (e.g., "foxes, mice, and birds" forms a singleNP).
Although the words just prior to such commas areoften prosodically marked with pitch rises, pauses areusually reserved for boundaries between long or majorphrases.
Furthermore, lists of words containing com-mas do not always employ a coordinate conjunction(e.g., "a slimy, round, large, red fish").
It often makeslittle difference to the prosody of such phrases whetherthe commas are present or not; thus one cannot reateach comma as a syntactic boundary.3 PREPROCESSING (TEXT FORMATING)In text to speech, special processing is needed for textentries not in word form (e.g., digits, abbreviations),which must be converted into corresponding words.This preprocessing can assist in grouping words.
Ab-breviations often represent words that are closely linkedto adjacent words; e.g., measurement abbreviations(sec., mi., oz.)
are usually preceded by a numeral orquantifier, forming a NG.
Four classes of abbreviationsdepend on the direction of linkage with adjacent words(examples are given in parentheses): 1. left (Jr., in.,Blvd.
); 2. right (Mr., Mrs., Prof., Fig.
); 3. either (Tues.,Dept., St.); 4. both (vs., cu.).
Virtually all abbreviationsform NGs with immediately adjacent words (e.g., MainSt., Mr. Jones), although some in the fourth class maylink words on a broader scale (e.g., "vs."  can link twoarbitrarily long NPs).Other text preprocessing of use to parsing concernshyphens, capital etters, and contractions.
Hyphenatedwords (e.g., tongue-in-cheek) are treated as nouns,unless all their components are numerals (e.g., forty-one), which fall into the numeral category.
A string ofcapitalized words is considered to be a phrasal unit,because it is likely to be a NG and be spoken without apause.
Words consisting entirely of capital etters areusually acronyms and, like digit sequences, act as nouns(or adjectives).
Most contractions are uniquely con-verted to words ('ve ~ have); for others the conversionis not unique but the ambiguity has no effect on parsing99Douglas D. O'Shaughnessy Parsing with a Small Dictionary for Applications such as Text to Speech('d ~ had or would).
Most contractions involve auxil-iary verbs and have minimal prosodic effects.
On theother hand, -n't ( = not) is important prosodically sincethe preceding syllable becomes tressed.
The contrac-t ion 's  can either be a verb (is, has) or act as apossessive adjective (John's); thus the parser mustallow two possibilities.
Heuristic rule,; help here: 1.after a pronoun, -'s is verbal; 2. a possessive contrac-tion is usually followed by an adjective or a noun; 3. theverbal -'s usually precedes a verb participle (e.g., an-ing or -ed word).
Confusions here do not have severeprosodic effects because pauses do not occur right after-'s contractions.4 WORD DICTIONARY AND PROCEDURES FOR EACHPART OF SPEECHThe parsing dictionary consists of about 300 words,each labeled with 1-3 possible parts of speech.
About 50of the words have 2-3 possible classifications (e.g., " i t"can be either a subject or object pronoun; "more" canact as noun, quantifier, or adverb).
For words withmultiple syntax possibilities, the most probable is triedfirst, and the others are only used in case of parsingfailure.The parts of speech that the parser employs can begrouped into classes, which are subdivided according tothe useful parsing features that distinguish words.
Thedictionary contains about 60% function words and 40%content words.
The largest classes of function words arethe prepositions and conjunctions, each having about13% of the dictionary words.
They are followed (inorder of decreasing size) by auxiliary verbs, pronouns,numerals, quantifiers, and articles.
The dictionary con-tent words are dominated by the adverbs (25% of thedictionary), with common verbs making up most of therest.
The remaining thousands of words that are not inthe dictionary fall into four classes: noun, adjective,verb, and adverb.
Adverbs not in the dictionary end in-ly and are thus identified by their suffix.
Hence wordsnot recognized by the dictionary are limited to act asverbs or nouns.
(In terms of parsing for prosody, little islost by treating adjectives as nouns, since they oftenoccur interchangeably in similar positions, e.g., as acomplement after a verb or as modifiers in an NG.
)The role of the dictionary then is to specify thesyntactic functions of all words that belong to smallclasses of words.
A word dictionary of 300 entries,augmented by a suffix analyzer (described in Section 5)using fewer than 60 suffixes, is sufficient to identify allwords except those acting as nouns, adjectives, andverbs.
These latter classes are open and contain atheoretically unlimited number of words.
The power ofthe parser can be increased by including some of thesewords (e.g., verbs with irregular endings or which taketwo objects), but the dictionary rapidly increases in sizein such cases with only limited benefits for prosody.At the local parsing level, the system accepts each100new word (from left to right), searches the dictionary fora match, and, if successful, attempts to link the newword to the immediately preceding words to formsyntactic phrasal units (NGs, PPs, VGs, AdvPs, AdjPs).When the new word is syntactically incompatible withprior words (for reasons described below), a newphrasal unit is started.
The procedure is detailed belowand is organized according to the part of speech of eachword found in the dictionary.
(In the following discus-sion, word examples are given in parentheses.)I.
A preposition introduces a PP and thus starts anew phrasal unit (which may be grouped later,at the global evel, with a prior NG and ensuingPPs to form a NP).
Some prepositions (despite,besides) may precede gerunds, while others(about) can precede to + infinitive (where thesequence then forms an infinitival phrase), andthose in a third subclass (instead, because) canmerge with an ensuing "of" .
By distinguishingthese subclasses, the parser can better decidewhether or not to link a preposition with ensu-ing words; e.g., when a non-gerund-precedingpreposition such as "under" is followed by agerund, a syntactic boundary separates the twowords (which indicates that "under" eitherends a clause or is acting as an adverb).
Onepreposition is special: " to"  can be followed byeither an NG or an infinitive (an infinitive isassumed if a content word follows "to").2.
A conjunction also indicates the start of a newphrasal unit.
For those that introduce depen-dent clauses or phrases (when), no link is madewith preceding words; for coordinate conjunc-tions (and, but) the global parser later attemptsto merge phrasal units adjacent to the conjunc-tion into a larger unit.
Some conjunctions (un-less) may precede gerunds and participles,while others (because) can only precedeclauses.While relative pronouns (where, that) and adjectives(whose) can act as conjunctions in starting clauses,such clauses function as NPs (and serve as a subjector object in a clause for the higher-level parse),whereas clauses introduced by other conjunctionsare not directly linked to adjacent words.
The pres-ence of a Wh-word at the start of a sentence (perhapsright after a preposition) indicates that the sentence isa Wh-question if a verb immediately ensues (e.g.,With whom does he eat?
vs. What he eats is fish.
).When meeting the word "as" ,  the parser looks for anensuing "as"  to link into a larger phrase (e.g., as bluea fish as I could find).3.
A pronoun usually acts as a one-word NG andthus, at the local level, can only link with animmediately prior preposition (exception:"we'", "you" can act as a quantifier--e.g., youblue meanies).
Certain pronouns only functionas subject NGs (I, we) and indicate that theComputational Linguistics, Volume 15, Number 2, June 1989Douglas D. O'Shaughnessy Parsing with a Small Dictionary for Applications uch as Text to Speechensuing words form a VG.
Others behave asobject NGs (us) and indicate that the prior wordis either a verb or a preposition.
Reflexivepronouns (words ending in -self) behave pro-sodically like adverbs and are stressed; it isunnecessary to see if a reflexive pronounmatches the preceding word, since reflexivestend to act prosodically as sentential adverbs,getting their own stress contour (e.g., in "Suehit John himself/herself", whether the pronounlinks to "Sue" or " John" makes little prosodicdifference).
Some pronouns act as adjectives,which either must be followed by the rest of aNG (possessive pronouns--our), stand bythemselves (ours), or have both options (de-monstrative pronouns--those).4.
A quantifier generally starts an NG, sometimesjointly with an article (such a, a little, the other).Certain quantifiers indicate number (singular--much, every; plural--many, some), which isuseful in locating the end of the NG (e.g., in"Every cat walks home.
.
. "
,  the word"walks", potentially a plural noun, cannot bepart of the subject).
Except for predeterminers(almost), only one quantifier can occur in a NG;thus a sequence of two quantifiers is brokeninto two NGs.
After a comparative quantifier(more), the parser looks for "than" to form alarger unit (e.g., more NG than \[NP, S\] acts asa NP).5.
An article always starts a new NG (except aftercertain quantifiers).
In the case of a/an, the NGis singular and thus should not terminate with aplural noun (exceptions: a great many, a \[hun-dred, thousand, million\]).6.
Numerals form a class of words of marginalutility to the parser.
The cardinals (one, ten) areuseful for specifying the number of its NG (e.g.,in "After one night dogs swam home.
. "
,"dogs" cannot be part of the initial PP), but theordinals (first, third) only serve to possiblyindicate NG boundaries (e.g., In summer third-rate movies a re .
.
. )
.
A cardinal numeral is thelast function word of a NG (exception--oneanother); e.g., in "With those two some menwin", the quantifier "some" starts a new NG.7.
Adverbs form a class of content words, but,excluding words ending in -ly, it is the smallestsuch class and is feasible to be included in thedictionary.
Identifying adverbs is especiallyuseful for a prosodic parser because they tendto be strongly stressed and their syntactic func-tions help parse the sentence.
Each adverb hasone of three roles: 1. as a sentential adverb(seldom), which can appear virtually anywherein the sentence, and thus should be ignoredwhen looking for syntactic structure; 2. modi-fying (and following) a verb (aloud), whichComputational Linguistics, Volume 15, Number 2, June 1989helps locate one-word VGs; and 3. modifying anadverb or adjective (quite), which labels theensuing word.
For example, in "has actuallyeaten", the adverb "actually" should be ig-nored as part of the VG; in "Large fish swimaway", "swim" is identified as a verb becauseit precedes the class-two adverb "away"; in"Very hungry people eat food", "hungry"must be an adjective (following the class-threeadverb "very") and thus "people" cannot be averb.
The adverb "not" has other parse func-tions: except when following an auxiliary verb,"not" starts a phrasal unit, either an NG (e.g.,At the beach, not swimming is dumb) or a verbcomplement (e.g., Are blue fish not cold?).8.
Auxiliary verbs (forms of be, have, do) andModal verbs are very useful in parsing becausethey initiate a VG (and thus terminate a preced-ing phrase) and often indicate the number of thesubject; e.g., in "The fear animals show istemporary", animals how" can be identifiedas a subordinate clause because "is" must havea singular subject and the plural "animals"must be the subject of a relative clause.9.
A few common Verbs (made, read) are includedin the dictionary because identifying eachclause's verb is important for prosody.
Mostuseful are past tense verbs (kept) that do notend in -ed, because these irregular verbs appearfrequently and are not easily identified by suffixanalysis.
The 2,000-word system noted earlier(Bratley 1968) deviates ignificantly from oursystem by including a large number of verbs,with each entry noting how many objects (0, 1,or 2) are expected to follow the verb.
There areseveral reasons we do not use a list of intransi-tive verbs (die) and two-object verbs (gave,offer): 1. their large number; 2. an indirectobject is optional--thus the parser cannot relyon its presence; 3. it is often difficult o tell i fasequence of unidentified words after a VGforms one or two NGs.
Thus our parser allowsfor 0-2 NGs after each verb.10.
A few common Nouns are in the dictionary toaid in specifying number.
Virtually all pluralnouns end in -s, but a few common ouns do not(people, men, women).
Thus our parser as-sumes that unidentified words not ending in -sare singular.5 MORPH DECOMPOSITION AND DICTIONARYA 300-word dictionary suffices to recognize almost halfthe words in general text, but the syntactic role of mostcontent words remains unspecified.
Since many Englishcontent words end in suffixes that help identify theirpart of speech, it is useful to try to classify words notfound in the dictionary by their endings.
The MITalk101Douglas D. O'Shaughnessy Parsing with a Small Dictionary for Applications uch as Text to Speechsystem (Allen 1987) decomposes ach such word into allits component "morphs" (prefixes, roots, and suffixes).Our simpler approach just looks at the final letters inthese words to locate likely suffixes (-ness, -able).
Alist of about 60 suffixes (ordered from longest o short-est) is compared to the endings of such words.
Eachsuffix is associated with its most likely part of speech(as determined from an analysis of English words).
Onlywords with two or more syllables are examined forpossible suffixes (except for the -s suffix), since virtu-ally all words with suffixes have a root morph contain-ing one or more syllables, to which the suffixes attach.If a letter to phoneme translation is unavailable, asimplistic syllable counter can suffice in which a seriesof adjacent letters from the set \[a,e,i,o,u,y\] (exceptword-final -e) counts as one syllable.
(While this mis-counts some words, it suffices for finding suffix eligiblewords; e.g., one-syllable words like "worked" areaccepted, and words like "fable" and "size" are re-jected.
)If a word ends in -s, special rules are invoked (unlessthe preceding letter is i, u, or s): the -s is stripped off,and the suffixes are re-examined on the shortened word.For this second pass, any suffix ending in -y is modifiedto end in -ie (e.g., to label "identifies" as a verb, theverbal suffix -ify must be changed to -ifie in the presenceof the final -s).
If a suffix is found for the shortenedword, the corresponding part of speech applies, but theword is noted as being either plural (in the case of anoun) or third-person singular present active (forverbs).
If no further suffix match is found (other thanthe original -s), then the two possibilities (plural nounand singular verb) are retained.
While the noun/verbambiguity remains (before examining context), wordsending in -s are very useful when examining adjacentwords for number agreement.
Two passes are madethrough the suffix dictionary only if the word ends in -s.This is different from MITalk word decomposition,which, using a large morph dictionary, continues tostrip off as many affixes as possible, until the root formis left.
To the extent part of speech can be determinedfrom decomposition without a large dictionary, analysisof the last suffix (two suffixes when the word ends in -s)is sufficient.Many word terminations uniquely specify a part ofspeech.
Words whose final letters match one of thefollowing suffixes are considered identified for syntaxpurposes: nouns (-ity, -or, -ship, -time, -ness, -sm),adjectives (-ous, -ful, -less, -ic), verbs (-sist, -the, -ify),gerunds (-ing), and numerals (-teen).
Other word end-ings are probable indicators of part of speech (e.g.,-ment -->noun).
Several tentative verb endings, primar-ily past tense forms (-ed, -ught, -ung), are includedbecause past tense verbs do not provide parser assis-tance through number agreement rules (past ense verbscan accept both plural and singular subjects).6 PARSING ALGORITHMThe system uses a bottom-up arser based on a contextfree grammar, with constraints on permissible syntacticgroupings (see Appendix A).
As in an augmented tran-sition network (Woods 1970), the algorithm of IF-THEN procedures involves transitions between statesand their consequences.
The consequences of the con-ditional actions of the rules involve the gradual con-struction of a parse tree.
The states of the networkcorrespond to different syntactic ontexts and differentstages of parser tree development; e.g., as each word isexamined, a transition is taken out of the state specifiedby the prior words, depending on the part of speech ofeach new word.
Each state has a possible outgoingtransition for an unknown part of speech, to handle thefact that many words are classified only as being"content words."
For a recognized word with 2-3possible parts of speech or a word with a tentativesuffix, the most likely transition is taken and backtrack-ing occurs if an inconsistency is met or no successfulparse results by the end of the sentence (the "depthfirst" approach).Other parsers, with access to a large dictionaryspecifying part of speech and other attributes for virtu-ally all words, can operate with a tight, completelydetailed grammar and produce complete parse trees.This may be necessary for natural language understand-ing, but is not needed for specifying prosody.
Ourrestricted ictionary (especially the lack of any knowl-edge of attributes for virtually all nouns and verbs)forces us to weaken the grammar of English and tooutput incomplete parse trees.
For prosody, it sufficesto label phrases and locate their boundaries.6.1 CONTROL PROCEDUREFor each new word in the input text, a procedure (asdescribed in Section 4) corresponding to the (possiblytentative) part of speech is called to: 1. combine theword with immediately prior words to form a localphrasal unit (perhaps renaming the unit as the new wordis added), or 2. decide that the word starts a new phrasalunit.
If the word is not identified by the dictionaries, atentative part of speech is estimated from context, andthe same procedure is followed.
As each new phrasalunit is started (case 2), the parser operates at the globallevel to link the previous phrasal unit to earlier units, toidentify clausal units (e.g., main and dependent clauses)and their components (e.g., subject and object NPs).
Togroup words together, we exploit restrictions on wordorder in phrases, as well as on word and numberagreement in adjacent phrases.If the system finds an inconsistency (e.g., no legalparse according to the grammar, or a violation of thesyntactic onstraints), it rejects the current parse andbacktracks to the last tentative decision concerningeither grouping of words in phrasal units or choice ofpart of speech.
An alternative is chosen and the parse102 Computational Linguistics, Volume 15, Number 2, June 1989Douglas D. O'Shaughnessy Parsing with a Small Dictionary for Applications uch as Text to Speechcontinues from that point.
Given the large degree ofambiguity caused by the small dictionary and wordswith several syntactic roles, it is more efficient toproceed depth first using the most likely choices, thanto process possible paths in parallel.
This is especiallytrue given that we desire only a parse tree sufficientlydetailed to predict prosody; many of the trees producedby parallel paths would correspond to equivalent pro-sodic patterns.
The depth first approach is least efficientwhen a tentative decision early in a long sentence mustbe reversed; e.g., in "That  fear men have is stupid",deciding that " that"  is a demonstrative only comesafter analyzing the rest of the sentence.6.2 CASE OF AN UNIDENTIFIED WORDTo identify the part of speech of most nouns, adjectives,and verbs, context analysis must be invoked.
Oneaspect of exploiting context is ensuring that words areconsistent in number within a NG and between a subjectNG and its VG; e.g., if a subject NG is plural, anensuing one-word verb should not end in -s. Numberrules are useful since the parser is often faced with asequence of Unidentified words: 1.
If none of the wordsends in -s, they are likely to form a single NG; 2. if thelast one ends in -s, it could be a verb, with the otherpreceding words forming a singular subject NG, or theentire sequence could be a plural NG; 3. if the penulti-mate word ends in -s, the last word is likely to be a verb,with preceding words forming a plural NG.A major problem is locating phrase boundaries thatare not marked by function words.
NGs that commencewith a function word are easily found, but some NGsconsist of only adjectives and nouns.
An especiallydifficult, yet prosodically important, case concerns lo-cating the boundaries of embedded clauses not offset bycommas.
For example, in "The turtles (that) men seeswim well", the parser could see a dictionary output ofArticle + Unknown + Unknown + Unknown + Un-known + Adverb (assuming that " that"  is not presentand "men"  is not in the dictionary).
In this case, amongthe unknown words only the first ends in -s, so agree-ment rules try to label "men"  as a verb, leaving "seeswim" as a NG.
By including the (relatively few)common plural nouns that do not end in -s (e.g., men,women) in the word dictionary, however, the situationhere can be clarified.
If  "men"  is found as a pluralnoun, then the ensuing word (see) is marked as a verb,and the following word (swim) is labeled as the verb for"the turtles" with "men see" as an embedded clause.Number agreement rules are invoked primarily whenplural words are identified.
Inside subordinate clausesand phrases, a singular NG may often precede an activeverb not ending in -s; e.g., I insist that he eat.Specifying the part of speech for an unidentifiedword X is based primarily on the role of its immediatelypreceding word W. If X starts a sentence, it is called anoun unless the immediately following word is anintroductory word; this latter case is that of an impera-tive, where X is a tenseless verb.
X is also called a nounwhen: 1.
W is an article, quantifier, demonstrative,numeral, adjective, preposition, gerund, subordinateconjunction, or "whose" ;  2. the sentence starts withthat X; or 3.
X is preceded by a two-word infinitivephrase (i.e., to + a singular unknown word).
X is calleda verb after "who"  or a sentential adverb.
The remain-ing cases depend upon X's number.
A singular X (i.e.,not ending in -s or ending in -is, -ss, -us) is called a verbafter: 1. an auxiliary or modal verb (He will work)(unless the sentence starts with such a verb--Can fearrule?
); 2. a sequence of verb + coordinate conjunction(He ate and ran); 3. a subject pronoun (He ran), 4. aplural NG (The bells ring); otherwise, it is named a noun(blue cheese).
A plural X is called a verb after: 1. asingular NG (a rat smells); 2. a relative pronoun (whateats); 3. a singular subject pronoun (He swims); other-wise it is named a noun (communications engineers).
Inseveral of these cases, the choices are successful onlyfor a majority of sentences, because a universallycorrect choice is impossible before examining laterwords (if then).
Thus, every time a word unidentified bythe dictionary is tentatively labeled here, the alternativechoice (noun or verb) is stored in a stack for use if alater parse failure causes backtracking to this word.
Forexample, in "What cats do is unclear",  "cats"  is firstlabeled a verb, but then the ensuing identified verbs "dois" force "cats"  to be renamed a noun because theyeach need a subject.6.3 INDEPENDENT PHRASESSentences often have phrases that do not directly mod-ify the subject, verb, or object of a clause.
Instead, theyare either parenthetical (e.g., he said, you know) ormodify a clause as a whole (AdvPs).
The parser easilyhandles such expressions when they are offset bypunctuation.
Without punctuation, however, the prob-lem is more difficult but necessary to solve, since suchexpressions usually have their own prosodic grouping.A common independent phrase is a temporal adver-bial---a NG introduced by a function word, where thefinal noun deals with time (e.g., last week, three times amonth, next Tuesday).
The expression tends to act as asentential adverbial, occurring at any of a number oflocations in a sentence.
One way to identify theseexpressions i to list in the dictionary all nouns dealingwith time (e.g., second, day), but the list is apparentlylarge (e.g., semester, period, etc.).
We avoid that ap-proach, since these expressions are readily isolated(although not so easily labeled) in virtually all casessince they commence with a function word that causesit to be recognized as a NG.
Such a NG can be confusedwith a subject or object NG, however.
The only risk isthat, in a sentence with short phrases, a temporaladverbial might be syntactically merged with an adja-cent VG as its subject or object (and as a result, not beprosodically separate).
Since short temporal adverbialsare not always isolated prosodically, such a risk is notComputational Linguistics, Volume 15, Number 2, June 1989 103Douglas D. O'Shaughnessy Parsing with a Small Dictionary for Applications such as Text to Speechserious.
The only case where a temporal adverbial doesnot commence with a function word, and thus risksbeing merged with a preceding NG, occurs when thetime noun is plural and is followed by a comparativeadjective (e.g., days later); thus the parser looks forsuch situations in possible long NGs.6.4 INTRODUCTORY FUNCTION WORDSThe word "that" can act as a noun, a demonstrativeadjective, or (most often) a relative pronoun.
The parsertries the most common role first, where "that" must befollowed at least by a subject NG and a VG, forming asubordinate clause.
Unlike that-clauses, clauses intro-duced by other relative pronouns (Wh-words) mayfollow a preposition or may act as the subject or objectNG of the clause.
Such a clause may end with apreposition if a subject NG precedes the VG (e.g., in"Who\[mever\] John gave it to was not clear").The other use of clauses with "that" or a Wh-word isfollowing (and modifying) a NG.
In such cases, "that"functions exactly like the Wh-words, except hat "that"is often deleted (The man (that) John gave it to washere).
Such clauses must directly follow the NG theymodify; adverbials and other parenthetic expressionsare not allowed.
Finding the end of such clauses (andthe beginning, in the case of a deleted "that") is animportant task for the parser.
In the example above, thesequence "to was" forces a syntactic boundary, and theparser searches for a moved constituent, which stronglysuggests the presence of a subordinate clause; theunlabeled words "John gave" would then be inter-preted as a subject NG followed by a verb.Certain NG introducers impose a number constrainton the NG.
A NG starting with "a(n)", "this", or"every"  cannot end with a plural word (exception: if" few" or "great many" follows "a").
Thus, if theparser encounters a series of unidentified words after asingular NG introducer, it assigns the first plural wordto an ensuing phrasal unit (e.g., in "When I bought hisfour people gasped", a boundary is placed between"this" and "four").
(If a plural word ensues directly, itis treated as an adjective, e.g., "a communicationsengineer".)
When a plural numeral occurs in a singularNG (e.g., a blue four-foot ladder), then the parser linksthe numeral to the next word to act as an adjective.6.5 GLOBAL GROUPINGEach sentence is assumed to have a subject (perhapsimplied, in the case of imperatives) and a verb.
Normalorder is subject NG, VG, and 0-2 object NPs.
(PPs andAdvPs are free to occur at any NG or VG boundary.
)Any of these elements can consist of a set of coordi-nated units (e.g., the VG could be "ate, drank, andslept").
The parser marks any deviation from normalorder as a potential syntactic boundary; e.g., the initi-ation of a second NG during the course of an apparentNG is a cue to a possible clause boundary.
In general,deviations from the expected order of events (e.g., a PP104initiating a clause, rather than after a NG or VG) areoccasions when a speaker tends to mark syntax prosod-ically.Short sentences may not greatly mark NP-VG-NPboundaries prosodically, but long sentences usually do,depending on the relative sizes of the phrases.
In a longNP, pauses are likely to occur at NG---PP boundaries.For coordinated units in a phrase, pauses are morelikely at the coordination points (i.e., at punctuationand/or just be, fore conjunctions).
Adverbials, especiallyAdvPs of more than one word, tend to function asseparate prosodic units.An indirect object NG (if present) usually immedi-ately follows the VG (without interruption by an AdvP).If an object NG is a simple pronoun, it attaches prosod-ically to an adjacent phrase; if two object NGs (otherthan pronouns) follow a verb, each will be assigned itsown prosodic group.6.6 COORDINATED WORDS AND PHRASESOne syntactic structure with strong stress effects is thatof parallel contrast, where two (or more) concepts arecontrasted in structures either joined by coordinateconjunctions or in a list separated by commas; e.g., in"John ate fish, while Joe ate beef", the repeated verb isless stressed than normally and the other contentwords, paired in parallel clauses, are more stressed.This parallelism is sometimes extended to the pointwhere the repeated word may be dropped (i.e., gappingmay occur).
Such ellipsis can drastically alter prosody(e.g., inserting a pause after "Susan" in "John boughtfish, and Susan juice").
The most common form deletesthe verb from the second of two conjoined clauses.
Itmay be identified by the parser in cases of coordinatedunits where the first unit is a clause and the second unitlacks a verb.In cases of conjoined phrases (i.e., ABC .
.
.
GHconjunction JK .
.
.
PQ--where the letters representwords), the parser assumes local coordination andsearches for the smallest units to link.
After eliminatingparenthetic expressions and adverbials et off by punc-tuation, the parser considers linking phrases up to thenearest punctuation.
To link, the phrases must have thesame general structure.
Thus, if J is a preposition, theparser searches to the left for a preposition to match.
Ifthe conjunction links two prepositions (i.e., H is apreposition) (e.g., "He went into and through thehouse"), the prepositions are stressed, as part of thegeneral process of stressing the parallel constituents incoordinated phrases.
If ABC .
.
.
GH and JK .
.
.
PQboth form clauses, the conjunction is viewed as linkingthe two clauses (the same holds when JK .
.
.
PQ startswith a VG, under the assumption that the impliedsubject is common to both clauses).English has certain word sequences which assist ingrouping coordinated units.
When linking two phraseswith "and",  the scope of the first unit may be cued bythe word "both";  thus the parser, in its search leftwardComputational Linguistics, Volume 15, Number 2, June 1989Douglas D. O'Shaughnessy Parsing with a Small Dictionary for Applications such as Text to Speechfrom "and" looks for a possible "both".
Similar actionsare taken for the following pairs: e i ther .
.
,  or, ne i ther?
.
.
nor,  whether .
.
,  or  not ,  and  not  on ly  .
.
.
but .6.7 COMPARATIVE STRUCTURESPhrases involving comparative words (e.g., more, as)are useful to identify because they often group wordstogether prosodically.
The discovery of "more" or"less" sets a flag looking for a "than" to link with,setting up a parallel syntactic structure.
"More/less"can act either as a noun (by itself), an adverb, or aquantifier-adjective.
If "than" ensues immediately, itlinks tightly to the preceding word; if, on the otherhand, some words intervene, a syntactic boundary isindicated right before "than".
If "than" appears with-out a preceding "more" or "less", then the parsersearches leftward to link "than" with an -er  word in thefunctional role of an adjective (i.e., adjacent to apossible noun, or after a "be"  verb).
Phrases with "as"are more diverse than "more/less" phrases.
"As"followed by a clause is a subordinate clause; "as"followed by a NG is a PP.
Either of these wordsequences can be preceded by the  same + NP  to forma larger prosodic unit.
Another as-structure is such  +NP  + as  + ..
(e.g., such men as these).6.8 QUESTION ANALYSISMost yes-no questions are marked with subject-verbinversion at the start of the final main clause of thesentence.
Such a final clause usually starts with a modalor auxiliary verb, which is immediately followed by thesubject NG, and then the rest of the VG (if the VGcontains more than one verbal).
However, virtually anysentence or phrase can be turned into a yes-no questionby simply adding a question mark at the end.
Thussentences ending with a question mark are assumed tobe yes-no questions, unless an unbound Wh-word (e.g.,where, what) is found in the main clause of the sen-tence.
We distinguish bound and unbound Wh-words,since questions in which the Wh-word is bound to arelative clause (e.g., This is where we went?)
are yes-noquestions.
The parser, however, is generally capable ofdetermining whether each Wh-word in the sentence isbound and whether it lies in the final main clause (e.g.,in "Did you say who's there?," pitch rises at the end).Subject-verb inversion is fairly easy to identify.
Ithas the structure of an auxiliary or modal verb, followedby a subject NG, and then by the rest of the VG.
Theword "not"  may occur before or after the subject, andan introductory clause (or AdvP) may precede.
In thecase of a lack of punctuation (e.g., in "When he camedid he eat?
"), the unexpected appearance of an auxil-iary verb usually helps note the clause boundary.
Sincea VG can consist simply of an auxiliary, in these yes-noquestions the subject NG can be followed immediatelyby a complement or object (e.g., Is the man blue?
).While sentences like "Has John pneumonia?"
are the-oretically grammatical, they are rare, and questionversions of "John has pneumonia" and "John did thework" usually involve the insertion of a conjugatedform of "do"  (e.g., "Did John do the work?").
As anexample of how prosody can be greatly affected bysubtle syntax differences: a Wh-word usually cuesterminal falling pitch, but in "Did you see what hedid?
", pitch rises.7 COMPARISON TO OTHER SYSTEMSCompared to an earlier 2,000-word system, this parseris almost as successful for the 54 sentences on which itwas evaluated (Thorne 1968, Dewar 1969) (see groups Iand 2 in Appendix B).
That parser, not destined forprosodic needs, found two legal parses for four of thesentences that our parser found but one: The cat  adoresf i sh ;  F red  gave  the dog  b iscu i t s ;  he  observed  the manw i th  the te lescope;  I d is l ike  p lay ing  cards .
A text tospeech system requires a single output; e.g., in the firstexample, we choose the parse where "adores" is theverb (and not "fish").
In a sentence such as "The boyscouts ran", a similar choice would be wrong, but theonly way to avoid this is to include many hundreds ofverbs in the dictionary.
Of the 54 sentences, only five(relatively minor) failures occurred: 1.
"Chew gum"was parsed, not as an imperative, but as in "Fear won";unless "chew" is in the dictionary or the system isbiased toward imperatives for short sentences, thisambiguity is not easily resolved; 2. in "He rolled up thebright red carpet", it is impossible to label "up"  asadverb or preposition without a significantly complexsemantic omponent; 3. the same comment holds for"Fred gave the dog biscuits", with regard to determin-ing whether there is one or two objects involved.
Theother two mistakes were ones of incorrectly groupingcorrectly identified words (see Appendix B).Our parser was also tested on the 39-sentence (456-word) set of Bachenko (1986) .
They claimed only onemistaken parse among the 39 sentences (but did notindicate their parsing output), while our parser madeone mistake each in four of their sentences.
That oursystem with only a 300-word dictionary is virtually assuccessful as ones using dictionaries more than sixtimes larger shows the adequacy of our parser.
Englishhas enough syntactic redundancy toallow correct label-ing of words' part of speech and location of phraseboundaries using function words and constraint rules asdescribed in this paper.
In the relatively infrequentcases where our system makes mistakes, they are rarelyof the type that would cause incorrect intonation interms of misplaced stress, but rather would cause somepauses to occur at secondary syntactic boundariesinstead of at major ones.8 CONCLUSIONWe have described a parser suitable for certain textprocessing applications where a complete parse may notComputational Linguistics, Volume 15, Number 2, June 1989 105Douglas D. O'Shaughnessy Parsing with a Small Dictionary for Applications uch as Text to Speechbe necessary.
For example, specifying prosody in a textto speech system basically requires only three things:knowing where to pause (major syntactic boundaries),which words to stress (distinguishing content and func-tion words), and whether the sentence requires a pitchfall or rise at the end (is it a yes-no question?).
Theparser uses a 300-word dictionary to identify commonwords and a set of linguistic constraints to determinelikely syntactic structure.
The system finds syntacticboundaries where a speaker reading the same textwould likely pause, and labels each word with a part ofspeech with sufficient accuracy to assign proper stress.REFERENCESAllen, Jon; Hunnicutt, M. S.; and Klatt, Dennis 1987 From Text toSpeech: The MITalk System.
Cambridge University Press, Cam-bridge, England.Bachenko, Joan; Fitzpatrick, Eileen; and Wright, C.E.
1986 TheContribution of Parsing to Prosodic Phrasing in an ExperimentalText to Speech System.
Proceedings of the Association forComputational Linguistics Conference: 145-155.Bratley, P. and Dakin, D.J.
1968 A Limited Dictionary for SyntacticAnalysis.
In: Dale and Michie (eds.
), Machine Intelligence 2:173-181.Carlson, Rolf and Granstrom, Bjorn 1986 Linguistic Processing in theKTH Multi-lingual Text to Speech System.
Proceedings of theInternational Conference of Acoustics, Speech and Signal Proc-essing (ICASSP): 2403-2406.Coker, Cecil; Umeda, Noriko; and Browman, Catherine 1973 Auto-matic Synthesis from Ordinary English Text.
IEEE TransactionsAudio & Electracoustics AU--21: 293-298.Dewar, H.; Bratley, P.; and Thorne, J. P. 1969 A Program for theSyntactic Analysis of English Sentences.
Communications oftheACM 12(8): 476-479.Flanagan, James; Coker, Cecil; Rabiner, Lawrence; Schafer, Ronald;and Umeda, Noriko.
1970 Synthetic Voices for Computers.
IEEESpectrum.
7(10): 22-45.Frazier, Lyn 1985 Syntactic Complexity.
In: Dowty, Kattunen, andZwicky (eds.
), Natural Language Parsing.
Cambridge UniversityPress, Cambridge, England 129-189.Gee, James and Grosjean, Franqois 1983 Performance Structures: APsycholinguistic and Linguistic Appraisal.
Cognitive Psychology15:411-458.Grishman, Ralph 1986 Computational Linguistics: An Introduction.Cambridge University Press, Cambridge, England.Heidorn, G.E.
; Jensen, K.; Miller, L.A.; Byrd, R.J.; and Chodorow,M.S.
1982 The EPISTLE Text Critiquing System.
IBM SystemsJournal 21(3): 305-326.Jensen, K.; Heidorn, G.E.
; Miller, L.A.; and Ravin, Y.
1983 ParseFitting and Prose Fixing: Getting a Hold on Ill-formedness.American Journal of Computational Linguistics 9(3-4): 147-160.Klatt, Dennis 1987 Review of Text to Speech Conversion for English.Journal of the Acoustical Society of America 82(3): 737-793.Marcus, Mitchell 1980 A Theory of Syntactic Recognition for NaturalLanguage.
MIT Press, Cambridge, MA.Milne, Robert 1986 Resolving Lexical Ambiguity in a DeterministicParser.
Computational Linguistics 12(1): 1-12.O'Shaughnessy, Douglas 1979 Linguistic Features in FundamentalFrequency Patterns.
Journal of Phonetics 7:119-145.O'Shaughnessy, Douglas 1983a Automatic Speech Synthesis.
IEEECommunications Magazine 21(9): 26-34.O'Shaughnessy, Douglas and Allen, Jonathan 1983b Linguistic Mo-dality Effects on Fundamental Frequency in Speech.
Journal ofthe Acoustical Society of America 74(4): 1155-1171.Selkirk, Elisabet:h 1984 Phonology and Syntax.
MIT Press, Cam-bridge, MA.Tennant, H~u-ry 1981 Natural Language Processing.
Petrocelli, NY.Thorne, J. P.; Bratley, P.; and Dewar, H. 1968 The Syntactic Analysisof English by Machine.
Machine Intelligence 3: 281-309.Weischedel, Ralph; and Black, John.
1980 Responding Intelligently toUnparsable Inputs.
American Journal of Computational Linguis-tics 6(2): 97-109.Woods, William 1970 Transition Network Grammars for NaturalLanguage Analysis.
Communicatons ofACM 13: 591-606.Young, S. J.; and Fallside, F. 1979 Speech Synthesis from Concept:A Method for Speech Output from Information Systems.
Journalof the Acoustical Society of America 66(3): 685-695.APPENDIX AThe bottom-up arser employs the following grammar,which is intentionally loose to allow as much as possibleof English text to be accepted.
We are not concernedhere with detecting ungrammatical sentences, but ratherwith finding likely syntactic structure.Abbreviations: adj-adjective; AdjP-adjectivephrase; adv-adverb; AdvP-adverb phrase; art-article;aux-auxiliary verb; beaux-"be" verb; card-cardinalnumeral; cj-conjunction; Cl-clause; coord-coordinateconjunction; doaux-"do" verb; ger-gerund; havaux-"have" verb; int-iriterrogative; mod-modal verb; n-noun; NG-noun group; NP-noun phrase; ord-ordinalnumeral; part-past participle; Ph-phrase; pred-prede-terminer; PP-prepositional phrase; Pr-prepositionalgroup; prep--preposition; prn-pronoun; qt-quantifier;S-sentence; v-verb; VG-verb group; VP-verb phrase.
(Note: A unit in parentheses is optional; * indicates aconcatenation of one or more units; for multiple unitsenclosed by brackets, choose one unit.
)Grammar:S "--'> \[(cj) Cl\]*C1 ~ Ph*Ph ~ NP, PP, AdvP, VPVP-~' VG, beaux AdjP, VG coord VGAdjP---, (adv) \[adj, part\] (coord (adv) \[adj, part\]), "asadj as" \[NG, C1\]PP - ,  Pr NP, Pr int VGNG--~ (pred) (\[art, (qt) (ord) (card)\])(AdjP)* \[n, ger\]*, prn, intVG --~ (\[mod, doaux\]) v, mod \[be, have been\]\[part, ger\], mod have part, beaux \[part, ger\],havaux part, havaux been \[part, ger\]NP-~ (\[we, you\]) NG, (\[that, int\]) C1 (prep), NGAdjP, \[more, less, -er\] (NG) than \[C1, NG\],(not) \[ger, to v\] (\[NP, PP, CI\]), NG coord NG,as (many, few, adj a NG) as (NG, CI)Pr ~ prep (coord prep)AdvP --~ Freq a NP, as adv as \[C1, adj\],(Freq) \[more, less\] adv than CIFreq ~ once, twice, \[qt, card\] timesOther rules/constraints: 1.
CI must contain a NG and aVG (except for the main clause, which may begin withan imperative (unconjugated) VG and no NG); 2. sen-tential adverbs and PPs may appear adjacent o any106 Compultationalt Linguistics, Volume 15, Number 2, June 1989Douglas D. O'Shaughnessy Parsing with a Small Dictionary for Applications uch as Text to Speechverb; 3.
"not" may follow any modal or auxiliary verb,4.
"all" may follow any modal or auxiliary verb, after aplural subject; 5. each subject NG must agree with itsVG in number.APPENDIX BThe following shows the parse results for three groupsof sentences (96 in all), which had formed the test setsfor three previous articles in the literature (Dewar 1969;Thorne 1968; Milne 1986).
A capital letter code (theoutput of our local parse algorithm) is placed after eachcorresponding sequence of words, and a slash is notedat points where the global parser predicted a majorsyntactic boundary.
The codes are: N-noun group; V-verb group; P-prepositional phrase; I-infinitival phrase;A-adverbial phrase; S-personal pronoun; R-relativepronoun; G-gerund phrase; C-conjunction; X-auxiliaryverb.
Parsing mistakes are noted with numbers and areexplained below.GROUP 1John N helped V Mary N.John N helped V the girl N.The boy N helped V the girl N.Why R did X the chicken N cross V the road N?Chew N gum V.IWe S are going V to London P.Last week N / we S visited V John N.It S is easy V / to make I a mistake N.Anyone N can make V a mistake N.How difficult N was X it S to find digs I?He S observed V the girl N with the telescope P.He S observed her V with the telescope P.Which magazines N do X you S prefer V?The cat N and C dog N play V.Candy N is dandy V / but C liquor N is quicker V.I S like V bathing beauties N.Will X you S tell V John N / to bring back I the bookN?I S can give you V a rough estimate N.Did X you S see V the house N / he S built V?The film N / which R Punch N recommended V / wasbanned V.The policeman N stopped V and C ques-tioned him V.He S likes V / reading G Shakespeare's play N / andC performing them N.Take V an egg N and C beat it V.The butler N did V / what R we S wanted him V to doI.Where R have X you S and C your father N been Vhiding G?GROUP 2She S visited him V yesterday N.He S must have moved V.The cat N adores V fish N.This cat N adores V fish N.Go V.Mary N hates V my teasing N her S.The boy N / who R kissed V the girl N / laugheduproariously V.The boy N / who R the girl N kissed V / laugheduproariously V.The boy N / the girl N kissed V / laughed uproari-ously V.Fred N gave V the dog biscuits N. 2Fred N lost V the dog biscuits N.Pick V a ripe banana N.When R did X John N say V / he S would come V?He S observed V the man N with the telescope P.Power N corrupts V.He S is waiting V.Playing cards G intrigues me V.Playing cards G intrigue me V.I S dislike V playing cards G.The rascal N / who R John N claimed V / committedV the crime N has escaped V.Whose book N did X you S say you V wanted V?
3When R he S has fixed V dates N / he S will ring usV.The queen's ister's husband N took V good photo-graphs N.A lawyer N / who R cheats V the clients N / he S seesV / deserves V censure N.Has X the portrait N / they S bought V / disappearedV?He S rolled V up the bright red carpet p.4She S handed V John N / a pear N and C Mary N / anapple N.The plants N / he S watered V and C tended V /flourished V.Are X the elephant N and C the kangaroo N / he Sadopted obeying him V?
5GROUP 3The old N can get V in A / for C half N price V. 6The large student residence N blocks V my view N.I S know V / that R boy N is bad V. 7I S know V / that R boys N are bad V.What boy N did it V?What boys N do V / is not V my business N. sThe trash N can be smelly V.The trash can N was smelly V.Which boy N wants V a fish N?Which boys N want V fish N?The river N / which R I S saw V / has V many fish N.What boy N wants V a fish N ?What boys want N is V fish N. 9What R blocks V the road N?What blocks N are V in the road P?What R climbs V trees N?What climbs N did X you S do X ?I S know V / that R boy N should do it V. ~0I S know V / that R boys N should do it V.Computational Linguistics, Volume 15, Number 2, June 1989 107Douglas D. O'Shaughnessy Parsing with a Small Dictionary for Applications such as Text to SpeechThat R deer N ate V everything N in my garden P /surprised me V.That deer ate everything N in my garden P last Anight V.  11I S know V / that hit Mary N. 12I S know V / that N will be true V.I S know V / that R boys N are mean V.I S know V / that R Tom N will hit V Mary N.I S told V the girl N / that R I S liked V the story N.I S told V the girl N / whom R I S liked V the storyN.
13I S told V the girl N / the story N that R I S liked V.Have X the students N take V the exam N.Have X the students N taken V the exam N?The soup pot cover handle screw N is red V.The soup pot N cover V handles N screw tightly V. 14The soup pot cover handle N screws tightly V.The soup pot cover handle screws N are red V.Which years N do X you S have V costs N figures Nfor P?Do X you S have V a count of the number of sales Vrequests N and C the number of requests N filledV?The trash N can N was taken out V.The trash N can be taken out V.The paper will N was destroyed V.The paper N will be destroyed V.Let X the paper N will N be read V.Will X the paper N can be re-used V?
t5NOTES1.
Noun-verb confusion.*2.
Separating the direct and indirect objects is difficult here.3.
The second "you"  is treated as a direct object, rather than asa subject.4.
The adverb "up"  is mislabeled as a preposition.5.
"Obeying h im" is not syntactically linked to "are" .6.
The sequence "in for" causes " for"  to be treated as aconjunction.*7.
Without a semantic component,  " that"  is mislabeled as arelative pronoun.8.
"What  boys"  is treated as a unit.*9.
"What  boys want" is treated as a unit.*10.
Same error as in (7).11.
The first four words are grouped as a noun phrase.12.
"That"  is mislabeled as a demonstrative."!3.
The end of the embedded clause is not assigned a majorboundary.14.
"Cover"  is mislabeled as a verb.15.
"Can"  is mislabeled as a verb.The mistakes labeled with an asterisk do not cause any intonationerrors, and most of the other sentences which caused syntacticproblems would have only slight intonational variations as a result ofthe mistakes.108 Computational Linguistics, Volume 15, Number 2, June 1989
