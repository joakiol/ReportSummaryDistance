Learning Dependencies between CaseFrame SlotsHang Li*Real World Computing PartnershipNaok i  Abe*Real World Computing PartnershipA theoretically sound method for learning dependencies between case frame slots is proposed.
Inparticular, the problem is viewed as that of estimating a probability distribution over the caseslots represented by a dependency graph (a dependency forest).
Experimental results indicate thatthe proposed method can bring about a small improvement in disambiguation, but the resultsare largely consistent with the assumption often made in practice that case slots are mutuallyindependent, at least when the data size is at the level that is currently available.1.
IntroductionWe address the problem of automatically acquiring case frame patterns (or their nearequivalents, selectional patterns and subcategorization patterns) from large corpusdata.
In our view, the acquisition of case frame patterns involves the following threesubtasks: (1) extracting case frame instances from corpus data, i2) generalizing caseframe slots within the case frames, and (3) learning dependencies that exist betweenthe (generalized) case frame slots.
We consider here the third of these subtasks andpropose a method of learning dependencies between case frame slots.The term "dependency" refers to the relationship that may exist between caseslots and that indicates strong co-occurrence between the values of those case slots.For example, consider the following sentences:(1) She flies jets.
(2) That airline company flies jets.
(3) She flies Japan Airlines.
(4) *That airline company flies Japan Airlines.We see that airline company can be the value of the argl (subject) slot, when the value ofthe arg2 (direct object) slot is airplane but not when it is airline company.
These sentencesindicate that the possible values of case slots depend in general on those of others:dependencies between case slots exist.
1The knowledge of dependencies between case slots is useful in various tasks in nat-ural language processing, especially in analyzing sentences involving multiple prepo-sitional phrases, such as The girl will fly a jet from Tokyo to Beijing.
Note in this example* c/o C&C Media Res.
Labs.
NEC, 4-1-1 Miyazaki Miyamae-ku, Kawasaki, 216-8555 Japan.
E-mail:{lihang, abe}@ccm.d.nec.co.jp1 One may argue that these xamples involve different word senses of fly, and in general, that if wordsenses were disambiguated there would be no dependency between case slots.
But, with thatinterpretation, word senses would have to be automatically disambiguated given the corpus data, andwe would find ourselves left with the same problem.
Furthermore, word senses are in general difficultto define precisely, and we feel it is better not to rely on this notion in a natural language application,unless it is necessary.
(~) 1999 Association for Computational LinguisticsComputational Linguistics Volume 25, Number 2Table 1Example case frames generated by a class-based model.Case Frame Frequency(fly (argl (person))(arg2 (airplane))) 3(fly (argl (company))(arg2 (airplane))) 2(fly (argl (person))(arg2 (company))) 1(fly (argl (person))(to (place/)) 1(fly (argl (person) )(from (place/)(to (place))) 1(fly (argl (company))(from (place))(to (place))) 1that the slot of from and that of to should be considered ependent and the attach-ment site of one of the prepositional phrases (case slots) can be determined by thatof the other with high accuracy and confidence.
There has not been a general methodproposed to date, however, that learns dependencies between case slots.
Methods ofresolving ambiguities have been based, for example, on the assumption that case slotsare mutually independent (Hindle and Rooth 1991), or at most two case slots are de-pendent (Collins and Brooks 1995).
In this article, we propose an efficient and generalmethod of learning dependencies between case frame slots.2.
Learning MethodSuppose that we have frequency data of the type shown in Table 1 automaticallyextracted from a corpus, in which words in the slots are replaced by the classes theybelong to.
We assume that case frame instances with a given verb are generated by adiscrete joint probability distribution of the formPy(Xl, x2 .
.
.
.
.
xn),where Py stands for the verb, and each of the random variables Xi, i = 1, 2 .
.
.
.
, n,represents a case slot.
We then formulate the dependencies between case slots as theprobabilistic dependencies between these random variables.Such a joint distribution can be represented by three alternative types of prob-abilistic models according to the type of values each random variable Xi assumes.When Xi assumes a word or a special symbol "0" as its value, we refer to the corre-sponding model as a word-based model.
Here 0 indicates the absence of the case slotin question.
When Xi assumes a word-class (such as (person) or (company)) or 0 asits value, the corresponding model is called a class-based model.
When Xi takes on 1or 0 as its value, we call the model a slot-based model.
Here the value of 1 indicatesthe presence of the case slot in question, and 0 the absence thereof.The number of parameters in a joint distribution will be exponential, however, ifwe allow interdependencies among all of the variables (even the slot-based model has0(2 n) parameters), and thus their accurate and efficient estimation may not be feasi-ble in practice.
It is often assumed implicitly in statistical natural anguage processingthat case slots (or the corresponding random variables) are mutually independent.Although assuming that they are mutually independent would drastically reduce thenumber of parameters (e.g., under the independence assumption, the number of pa-rameters in a slot-based model becomes O(n)), as illustrated above, this assumptionis not necessarily valid in practice.284Li and Abe Learning DependenciesWhat seems to be true in practice is that some case slots are in fact dependent onone another, but that the overwhelming majority of them are mutually independent,due partly to the fact that usually only a few case slots are obligatory; the others areoptional.
(Optional case slots are not necessarily independent, but if two optional caseslots are randomly selected, it is very likely that they are independent of one another.
)Thus the target joint distribution is likely to be approximatable asthe product of lower-order component distributions, and thus has, in fact, a reasonably small number ofparameters.In general, any n-dimensional joint distribution can be written asnP(X1, X2,.. .,Xn) = 1-I P(Xm, I Xm, .
.
.
.
.
Xrni_l)i----1for some permutation (ml, m2 .
.
.
.
.
m,) of (1, 2 .
.
.
.
.
n), letting P(Xml I Xmo) denote P(Xml).A plausible assumption regarding the dependencies between these random variablesis that each variable directly depends on at most one other variable.
This is one of thesimplest assumptions that can be made to relax the independence assumption.
For ex-ample, if the joint distribution P(X1, X2, X3) over 3 random variables X1, X2, X3 can bewritten (approximated) asP(X1).P(X2 I X1).P(X3 I X2), it (approximately) satisfies uchan assumption.
We call such a distribution a dependency forest model.
A dependencyforest model can be represented by a dependency forest (i.e., a set of dependency trees),whose nodes represent random variables (each labeled with a number of parameters),and whose directed links represent the dependencies that exist between these randomvariables.
A dependency forest model is thus a restricted form of the Bayesian etwork(note that it is also an extension of the first-order Markov chain model).Now we turn to the problem of how to select the best dependency forest modelfrom among all possible ones to approximate a target joint distribution based on inputdata.
This problem has already been investigated in the area of machine learning andrelated fields.
In particular, Suzuki (1993) has devised an algorithm, based on theMinimum Description Length (MDL) principle (Rissanen 1989), which estimates thetarget joint distribution as a dependency forest model (see Li \[1998\] for a derivation ofthis algorithm from MDL).
Figure 1 shows this algorithm, for which k i and kj denote,Algorithm:Let T := 0, V = {{Xi},i = 1,2,..-,n};Calculate the value O(Xi, Xj) for all node pairs (Xi, Xj);O(Xi, Xj) = I(Xi, Xj) - (ki - 1).
(kj - 1).
~ where mutual information 2.N  ' / \I ( Xi, X i) = 7~:,c x,,xj~ xj I P( xi, xj ) log P(xi, xj ) - P( xl, xj ) log( P(xi) .
P( xj ) ) )Sort the node pairs in descending order of 0, and store them into queue Q;while (max(x,,)G)eQ O(Xi, Xj) > 0) doRemove arg max(x,,xj)eQ O(Xi, Xj) from Q;i fXi  and Xj belong to different sets W1,W2 in Vthen Replace W1 and W2 in V with W1 U W2, and add edge (Xi,Xj) to T;Output T as the set of edges of the dependency forest.Figure 1The learning algorithm.285Computational Linguistics Volume 25, Number 2Xrg2 N = 9 0 Xaval X~,~ X fro m X{e NNkarg 1 = 2 Xargl -0.28 -0.16 -0.18 Xargl Xtokarg2 = 3 Xarg2 0.11 0.57kfrom = 2 Xfrom 0.28 /kto ~- 2 Xtq X fro mFigure 2A dependency forest as case frame patterns.respectively, the number of possible values assumed by random variables Xi and Xj,and N the input data size.
We employ Suzuki's algorithm to learn case frame patternsas a dependency forest model.Let us now consider an example to see how the algorithm works.
Suppose thatthe input data is as given in Table 1 and there are four nodes (random variables)Xargl,Xarg2,Xfrom , and Xto.
The table in Figure 2 shows the 0 values for all node pairs.Our program actually induces the set of possible values of a random variable fromthose in the input data and thus here kargl = 2.
Probabilities are calculated based onmaximum-likelihood estimation.
Also, the base of logarithm is 2, and 0 ?
log 0 = 0 isassumed.
The dependency forest in Figure 2 is then constructed.
The dependency forestindicates that there is dependency between the to slot and the arg2 slot, and betweenthe to slot and the from slot, but the argl slot is independent from all others.
Note thatthis algorithm in fact outputs a forest consisting of labeled free trees.
A labeled freetree is a tree in which each node is uniquely associated with a label and the root nodeis left unspecified.
After applying the algorithm, any node can be designated as theroot of that tree, since the dependency models based on the same labeled free tree areall equivalent (cf.
Li 1998).3.
Experimental Evaluation3.1 Experiment 1: Slot-based ModelIn the first experiment, we used the proposed method to learn slot-based ependen-cies.
As training data, we used the entire bracketed ata of the Wall Street Journalcorpus (Penn Treebank).
We extracted case frame data from the corpus using someheuristic rules.
There were 3,678 verbs for which case frame instances were extracted.We considered only the 354 verbs for which more than 50 case frame instances wereextracted, since dependencies are not likely to be found with smaller data sizes (seeSection 3.4.)
Also, we only considered the 12 most frequently occurring case slots (argl,arg2, on, in, for, at, by, from, to, as, with, against) and ignored the others.Example Case Frame Patterns.
We acquired slot-based case frame patterns for the 354verbs using our method.
There were on the average 484/354 = 1.4 dependency linksacquired for each of the 354 verbs.
We found that there were some verbs whose arg2slot is dependent on a preposition (referred to as p) slot.
Table 2 shows some of theverbs with large P(Xarg2 = 1, Xp = 1) values.
We also found that there were some verbshaving preposition slots that depend on each other (referred to as pl and p2).
Table 2also shows some of the verbs with large P(Xpl = 1, Xp2 = 1) values.
The dependenciesfound by our method seem to agree with human intuition.286Li and Abe Learning DependenciesTable 2Verbs and their dependent case slots.Verb Dependent Slots Examplegain arg2 tocompare arg2 withinvest arg2 inconvert arg2 toadd arg2 towithdraw arg2 fromprepare arg2 forfile arg2 againstsell arg2 tolose arg2 torange from toclimb from torise from toshift from tosoar from tofall from toincrease from toclimb from inapply to forboost from togain 10 to 100compare profit with estimateinvest share in fundconvert share to cashadd 1 to 3withdraw application from officeprepare case for trialfile suit against companysell facility to firmlose million to 10%range from 100 to 200climb from million to millionrise from billion to billionshift from stock to bondsoar from 10% to 20%fall from million to millionincrease from million to millionclimb from million in periodapply to commission for permissionboost from 1% to 2%Perplexity Reduction.
We evaluated the acquired case frame patterns (using the slot-based models) for all of the 354 verbs in terms of reduction in test data perplexity.
2We conducted the evaluation through a 10-fold cross validation.
That is, to acquirecase frame patterns for a given verb, we used nine-tenths of the case frames for thatverb as training data, saving what remained for use as test data, and then calculatedthe test data perplexity.
We repeated this process 10 times and calculated the averageperplexity.
We then compared this with the average perplexity for independent models,which were acquired based on the assumption that each case slot is independent.The experimental results indicate that for some verbs the use of dependency forestmodels results in a reduction of perplexity as compared to that of independent models.For 30 of the 354 verbs (8%), perplexity reduction exceeded 10%, while the averageperplexity reduction overall was only 1%.
Nonetheless, it seems safe to say that, withthe currently available amount of data, the dependency forest model  is more suitableas a representation for the true model of case frames than the independent model, atleast for 8% of the 354 verbs.3.2 Experiment 2: Slot-based DisambiguationIn order to quantitatively evaluate the acquired knowledge of case slot dependencies,we conducted a PP-attachment disambiguation experiment, in which we make a de-cision of the following type: Whether from Tokyo should be attached to fly or jet in thesentence he will fly a jet from Tokyo.2 Test data perplexity is a measure of how well an estimated probability model predicts future data, andis defined as 2 H(PT'PM), H(PT, PM) = -- ~-~x PT(X) ?
logPM(x), where PM(X) denotes the estimatedmodel, PT(X) the empirical distribution of the test data.
It is in general the case that the smallerperplexity amodel has, the closer to the true model it is.287Computational Linguistics Volume 25, Number 2A simple way to disambiguate is to compare the following likelihood values, basedon acquired slot-based models,Pfly(Xarg2 = 1, Xfrom = 1)" Pjet(Xfrom = 0),andPfly(Xarg2 = 1, Xfrorn = 0)-Pjet(Xfrom = 1)assuming that there are only two case slots arg2 and from for the verb fly, and thereis only one case slot from for the noun jet.
This means that we need to comparePfly(Xfrom = l \[ Xarg2 = l) " (1 -  Pjet(Xfrom = l) I= Pfly(Xfmm = 1 \[ Xarg  2 = 1)  - -  P f l y (X f rom = 1 \[ Xarg2 = 1)-Pjet(Xfrom = 1)and(1 -  Pfly(Xfrom ~- 1 I Xarg2 = 1))" Pjet(Xfrom = 1)= Pjet(Xfrom = 1) - Pfly(Xfrom = 1 \[ Xarg2 -~ 1))" Pjet(Xfrom = 1).Since the second term is common to both expressions, we actually only need to com-parePfly(Xfrom = 1 \[Xarg 2 = 1)andPjet(Xfrom = 1).Obviously, if we assume that case slots are independent, then we only need to comparePfly(Xfrom = 1) and Piet(Xfrom = 1).
This is nearly equivalent to the disambiguafionmethod proposed by Hindle and Rooth (1991).
Their method, referred to here as theLexical Association (LA) method, actually compares the two probabilities by meansof hypothesis testing.
Specifically, it calculates the so-called t-score, which is a statisticabout the difference between the two probabilities.Here, we first employ the proposed dependency learning method to judge ifslots Xarg2 and Xfrom with respect o verb fly are mutually dependent.
Then, if theyare dependent, we make the disambiguation decision based on the t-score betweenPfly(Xfrom = 1 \[ Xfrom = 1) and P je t (X f rom = 1).
Otherwise, we consider the two slotsindependent and make a decision based on the t-score between Pfly(Xfrom = 1) andPjet(Xfrom = 1).
We call this disambiguation method DepenLA, since it is a naturalextension of LA.First, we randomly selected the files under one directory of the Wall Street Journalcorpus, containing roughly 1/26 of the entire bracketed corpus data, and extracted(v, nl, p, n2) quadruples (e.g., (fly, jet, from, Tokyo)) as test data.
We then extracted caseframes from the remaining bracketed corpus data as we did in Experiment I and usedthem as training data.
We repeated this process 10 times and obtained 10 data setsconsisting of different training data and test data.
In each training data set, there wereroughly 128, 000 case frames on the average for verbs and roughly 59, 000 case framesfor nouns.
On the average, there were 820 quadruples in each test data set.We used these 10 data sets to conduct cross-validation on the disambiguationaccuracy.
We used the training data to acquire dependency forest models, which wethen used to perform disambiguation  the test data based on DepenLA.
We alsotested the performance of LA for comparison.
We set the threshold for the t-score to1.28 for both methods.
In both cases, some quadruples remained whose attachment288Li and Abe Learning DependenciesTable 3PP-attachrnent disambiguation results.Method Accuracy(%) Accuracy for 11% of Data (%)Default 56.2 55.3LA 78.1 + 0.5 93.8 -4- 0.8DepenLA 78.4 + 0.5 97.5 :t: 0.5sites could not be determined.
In such cases, we made a default decision--namely,we forcibly attached (p, n2) to v. This is because we found empirically for our data setthat for what remains after applying LA or DepenLA, it is most likely that (p, n2) goeswith v. Table 3 summarizes the results, which are evaluated in terms of disambiguationaccuracy, averaged over the ten trials.
Table 3 also gives the standard eviation (+) ofeach accuracy value, calculated based on the assumption that the 10 cross-validationtrials are statistically independent.We found that as a whole it is still difficult to judge if DepenLA significantly im-proves upon LA, indicating that it is almost justifiable to rely on the independenceassumption i practice, at least when the data size is at the level that is currently avail-able.
For about 11% of the verbs, however, for which the dependencies are detected andare strong enough (i.e., P(Xp = 1 I Xarg2 = 1) > 0.2 or  P(Xp = 1 \[ Xarg 2 = 1) < 0.002),DepenLA significantly improves upon LA.
(The cutoff points were set heuristicallyby observing the obtained ependencies, but not after the accuracy was calculated.
)It is interesting to note that on the subset of data for which DepenLA is foundto significantly improve upon LA, LA is already doing quite well: 93.8% as com-pared to 78.1%.
We found that in cases in which dependencies are detected (i.e.,P(Xp = 1 I Xarg2 = 1~ >> P(Xp = 1) or the converse), the probability value P(Xp = 1)is already highly discriminative, that is, either very large or very small.
This is prob-ably due to the fact that the verbs for which dependencies are detected are those forwhich the amount of training data is sufficient (relative to the inherent difficulty ofdisambiguation for that verb), and hence that are easy to disambiguate.3.3 Experiment 3: Class-based ModelWe also used the proposed method to acquire case frame patterns as class-based de-pendency forest models, using the 354 verbs in Experiment 1.As before, we consideredonly the 12 most frequent slots.
We generalized the values of the case slots within thesecase frames using the method proposed in Li and Abe (1998) to obtain class-based caseframe data.
We then used these data as input to the learning algorithm.
The resultswere rather discouraging: very few case slots were determined tobe dependent in thecase frame patterns.
To be more precise, there were on the average only 64/354 - 0.2dependency links found for each verb.
This is because the number of parameters in aclass-based model was large as compared to the size of the data we had available.These experimental results seem to justify the commonly made assumption thatclass-based case slots, and hence word-based case slots, are mutually independent,when the data size available is at the level of what is currently provided by the PennTreebank.3.4 Experiment 4: SimulationIn order to test how large a data size is required to estimate a dependency forest model,we conducted the following experiment.
We defined an artificial model in the form of289Computational Linguistics Volume 25, Number 2o~.5Q32.521.510.5010.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
J"Model1" ", "Model2" -~--.
"Model3" - o--100 1000Data Size10000Figure 3Simulation results.7 .
.
.
.
.
.
.
.
, .
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
#"Model1" -.e-,-6 "Model2" -+---"Model3" -EZ--5 .~--~>":43 .-~---*---+--2 , /  P~, ,~- .
.
ra .
, : ' :~ .
....~ .
.
.
.
.
.
.
.
010 100 1000 10000Data Sizea dependency forest model and generated ata according to its distribution.
We thenused the data obtained to estimate a model, and evaluated the estimated model bymeasuring the KL divergence between the estimated model and the true model.
Wealso checked the number of dependency links in the obtained model.
We repeatedlygenerated ata and observed the learning curve, namely the relationship between thedata size and the KL divergence between the estimated and true models, and therelationship between the data size and the number of dependencies in the estimatedmodel.
We repeated this experiment on three artificial models.
Figure 3 shows theresults of these experiments averaged over 10 trials.
The number of parameters inModel 1, Model 2, and Model 3 are 18, 30, and 44 respectivel~ and the number oflinks in them 1, 3, and 5.
Note that the KL divergence between the estimated modeland the true model converges to 0, as expected.
Also note that the number of links inthe estimated model converges to the correct value (1, 3, and 5) in each of the threeexamples.These simulation results verify that the dependencies between case slots can be ac-curately learned when there is enough data, given that the true model is representableas a dependency forest model.
We also see that to estimate a model reasonably accu-rately, the data size required is as large as 5 to 10 times the number of parameters.
Forexample, for the KL divergence to go below 0.1, we need more than 200 examples,which is roughly 5 to 10 times the number of parameters.
(Recall that there were only354 verbs, having frequencies greater than 50 in our experiments, ee Subsection 3.1.
)In Experiment 2, there were 12 binary-valued random variables associated witheach verb, and hence its distribution is thought o be approximatable by a model witha comparable number of parameters.
So having 50 to 100 examples for each of theseverbs approaches the minimum data size required for reasonable stimation of theirdependencies.
In Experiment 3 there were also 12 slots, but each slot could take onroughly 10 classes as its values and thus a class-based model tended to have about120 parameters.
The corpus data available to us was in no way sufficient for this case.4.
SummaryWe have proposed a method of learning dependencies between case slots, based onan estimation method for dependency forest models.
When using slot-based models,it was found empirically that some case slots are dependent, and when such depen-dencies are detected, using that knowledge of dependencies can significantly improvePP-attachment disambiguation accuracy.
For class-based models, however, most caseslots were judged independent.
These empirical findings indicate that the assumption290Li and Abe Learning Dependenciesoften made in practice that case slots in a class-based model are mutual ly independentis indeed justified, at least with the data size currently available in the Penn Treebank.AcknowledgmentsWe are grateful to Mr. K. Nakamura, Mr.T.
Fujita, and Dr. S. Doi of NEC and Ms.Y.
Yamaguchi of NIS for theirencouragement and collaboration.
We thankthe anonymous CL reviewers for theirvaluable comments and criticisms.ReferencesCollins, Michael and James Brooks.
1995.Prepositional phrase attachment througha backed-off model.
Proceedings ofthe 3rdWorkshop on Very Large Corpora.Hindle, Donald and Mats Rooth.
1991.Structural ambiguity and lexical relations.Proceedings ofthe 29th Annual Meeting,pages 229-236.
Association forComputational Linguistics.Li, Hang.
1998.
A Probabilistic Approach toLexical Semantic Knowledge Acquisition andStructural Disambiguation.
Ph.D. Thesis,University of Tokyo.Li, Hang and Naoki Abe.
1998.
Generalizingcase frames using a thesaurus and theMDL principle.
Computational Linguistics,24(2):217-244.Rissanen, Jorma.
1989.
Stochastic Complexityin Statistical Inquiry.
World ScientificPublishing Co., Singapore.Suzuki, Joe.
1993.
A construction ofBayesian etworks from databases basedon an MDL principle.
Proceedings ofthe 9thConference on Uncertainty in ArtificialIntelligence, pages 266-273.291
