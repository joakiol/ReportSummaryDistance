Automatic WordNet mapping using word sense disambiguation*Changki LeeGeunbae LeerNatural Language Processing LabDept.
of Computer Science and EngineeringPohang University of Science & TechnologySan 31, Hyoja-Dong, Pohang, 790-784, Korea{leeck,gblee }@postech.ac.krSeo JungYunNatural Language Processing LabDept.
of Computer ScienceSogang UniversitySinsu-dong 1, Mapo-gu, Seoul, Koreaseojy@ ccs.sogang.ac.krAbstractThis paper presents the automaticconstruction of a Korean WordNet frompre-existing lexical resources.
A set ofautomatic WSD techniques i described forlinking Korean words collected from abilingual MRD to English WordNet synsets.We will show how individual linkingprovided by each WSD method is thencombined to produce aKorean WordNet fornouns.1 IntroductionThere is no doubt on the increasingimportance of using wide coverage ontologiesfor NLP tasks especially for informationretrieval and cross-language informationretrieval.
While these ontologies exist in English,there are very few available wide rangeontologies for other languages.
Manualconstruction of the ontology by experts is themost reliable technique but is costly and highlytime-consuming.
This is the reason for manyresearchers having focused on massiveacquisition of lexical knowledge and semanticinformation from pre-existing lexical resourcesas automatically aspossible.This paper presents a novel approach forautomatic WordNet mapping using word sensedisambiguafion.
The method has been applied tolink Korean words from a bilingual dictionary toEnglish WordNet synsets.To clarify the description, an example is given.To link the first sense of Korean word"gwan-mog" to WordNet synset, we employ abilingual Korean-English dictionary.
The firstsense of 'gwan-mog' has 'bush' as a translationin English and 'bush' has five synsets inWordNet.
Therefore the first sense of'gwan-mog" has five candidate synsets.Somehow we decide a synset {shrub, bush}among five candidate synsets and link the senseof 'gwan-mog' to this synset.As seen from this example, when we link thesenses of Korean words to WordNet synsets,there are semantic ambiguities.
To remove theambiguities we develop new word sensedisambiguation heuristics and automatic mappingmethod to construct Korean WordNet based onthe existing English WordNet.This paper is organized as follows.
In section 2,we describe multiple heuristics for word sensedisambiguation for sense linking.
In section 3, weexplain the method of combination for theseheuristics.
Section 4 presents ome experimentresults, and section 5 will discuss some relatedresearches.
Finally we draw some conclusionsand future researches in section 6.
The automaticmapping-based Korean WordNet plays a naturalKorean-English bilingual thesaurus, o it will bedirectly applied to Korean-English cross-lingualinformation retrieval as well as Koreanmonolingual information retrieval.2 Multiple heuristics for word sensedisambiguationAs the mapping method escribed in this paperhas been developed for combining multipleindividual solutions, each single heuristic must beseen as a container for some part of the linguisticknowledge needed to disarnbiguate the* This research was supported by KOSEF special purpose basic research (1997.9- 2000.8 #970-1020-301-3)Corresponding author142ambiguous WordNet synsets.
Therefore, not asingle heuristic is suitable to all Korean wordscollected from a bilingual dictionary.
We willdescribe each individual WSD (word sensedisambiguation) heuristic for Korean wordmapping into corresponding English senses.Korean word English word WordNet synsetwsl= WS 2. .
.W$ nFigure 1: Word-to-Concept AssociationFigure 1 shows the Korean word to WordNetsynset association.
The j-th sense of Koreanword kw has m translations in English and nWordNet synsets as candidate senses.
Eachheuristic is applied to the candidate senses (ws,,.
.
.
.
ws) and provides cores for them.2.1 Heuristic 1: Maximum SimilarityThis heuristic comes from our previousKorean WSD research (Lee and Lee, 2000) andassumes that all the translations in English forthe same Korean word sense are semanticallysimilar.
So this heuristic provides the maximumscore to the sense that is most similar to thesenses of the other translations.
This heuristic isapplied when the number of translations for thesame Korean word sense is more than 1.
Thefollowing formula explains the idea.Hi(s,) = max support( s,, ew~) - 1~'~, (n-1)+a k,=lwhere EWi = (ewl s, ~ synset(ew)}In this formula, Hi(s) is a heuristic score ofsynset s, s is a candidate synset, ew is atranslation into English, n is the number oftranslations and synset(ew) is the set of synsetsof the translation ew.
So Ew becomes the set oftranslations which have the synset s r. Theparameter txcontrols the relative contribution ofcandidate synsets in different number oftranslations: as the value of a increases, thecandidate synsets in smaller number oftranslations get relatively less weight (a=0.5 wastuned experimentally), support(s,ew) calculatesthe maximum similarity with the synset s and thetranslation ew, which is defined as :support(si, ew) = max S(si, s)sEsynset( ew )S2) = ~ S im( st, s2) if sire(s,, s2) _> 0 S(sl,l 0 otherwiseSimilarity measures lower than a threshold 0are considered to be noise and are ignored.
In ourexperiments, 0=0.3 was used.
sim(s,s2) computesthe conceptual similarity between concepts s~ andsz as in the following formula :sim(sl, s2)= 2 x level(MSCA(sl, s:))level(sO + level(s2)where MSCA(sl,s2) represents he most specificcommon ancestor of concepts s~ and s2 andlevel(s) refers to the depth of concept s from theroot node in the WordNetL2.2 Heuristic 2: Prior ProbabilityThis heuristic provides prior probability toeach sense of a single translation as score.Therefore we will give maximum score to thesynset of a monosemous translation, that is, thetranslation which has only one correspondingsynset.
The following formula explains the idea.H2(s,) = max P(s, l ew)?,n, E EW~where EWi = {ew I si ~ synset(ew) }1 P ( si I ewj ) -~ - -n jwhere si ~ synset(ewj), nj = Isyr et( w,)lIn this formula, n is the number of synsets ofthe translation e~t~.2.3 Heuristic 3: Sense Ordering(Gale et al, 1992) reports that word sensedisambiguation would be at least 75% correct if asystem assigns the most frequently occurringsense.
(Miller et al, 1994) found that automaticI We use English WordNet version 1.6- L143assignment of polysemous words in BrownCorpus to senses in WordNet was 58% correctwith a heuristic of most frequently occurringsense.
We adopt these previous results todevelop sense ordering heuristic.The sense ordering heuristic provides themaximum score to the most frequently usedsense of a Ixanslation.
The following formulaexplains the heuristic.H3(sO = max SO(s,,ew)ewEEW,where EW, = {ew I si ~ synset(ew) }otSO(s,,ew) x~where si ~ synset(ew)^ synset(ew) is sorted by frequency^ s, is the x-  th synset in synset(ew)In this formula, x refers to the sense order of sin synset(ew): x is 1 when s, is the mostfrequently used sense of ew.
The informationabout the sense order of synsets of an Englishword was extracted from the WordNet.0.8prob0.70.$o.50.40.30.2io .1  IOO.#"-  " " t l .
A A =!
2 \] 4 5 $ 7 8 9ordm"Figure 2: Sense distribution in SemCorThe value a=0.705 and fl=2.2 was acquiredfrom a regression of Figure 2 semcor corpus 2data distribution.2.4 Heuristic 4: IS-A relationThis heuristic is based on the following facts:2 semcor is a sense tagged corpus from part ofBrown corpus.I f  two Korean words have an IS-A relation,their translations in English should alsohave an IS-A relation.Korean word English word WordNetFigure 3: IS-A relationFigure 3 explains IS-A relation heuristic.
Infigure 3, hkw is a hypemym of a Korean word kwand hew is a translation of hkw and ew is atranslation of kw.This heuristic assigns score 1 to the synsetswhich satisfy the above assumption according tothe following formula:H,(s,) = max 1R(si, ew)ew~EW,where EWi = {ewl si ~ synset(ew) }IR (s , ,ew)={;  otherwisefflsa(s"si)where si ~ synset(ew) , sj ~ synset(hew)In this formula, lsA(s,,s 2) returns true if s, is akind of s 2.2.5 Heuristic 5: Word MatchThis heuristic assumes that related conceptswill be expressed using the same content words.Given two definitions - that of the bilingualdictionary and that of the WordNet - thisheuristic computes the total amount of sharedcontent words.Hs(si) = max WM (si, ew)where EW~ = (ewl s~ ~ synset(ew) }WM (si, ew) = sim(X,Yi)s im(X,Y)  = IX n YIIx rlIn this formula, X is the set of content words inEnglish examples of bilingual dictionary and Y is144the set of content words of definition andexample of the synset s, in WordNet.2.6 Heuristic 6: CooccurrenceThis heuristic uses cooccurrence measureacquired from the sense tagged Koreandefinition sentences of bilingual dictionary.
Tobuild sense tagged corpus, we use the definitionsentences which have monosemous translationin bilingual dictionary.
And we uses the 25semantic tags of WordNet as sense tag :n6(s,) = max p(t, I x)xeDefwith p =,  - Z(l_a)/2 ?
~ ' (1~ ~)p(ti l x) = Freq(ti, x)Freq(x)In this formula, Defis the set of content wordsof a Korean definition sentence, t is a semantictag corresponding to the synset s and n refers toFreq(x).3 Combining heuristics with decision treelearningGiven a candidate synset of a translation and6 individual heuristic scores, our goal is to useall these 6 scores in combination to classify thesynset as linking or discarding.The combination ofheuristics i  performed bydecision tree learning for non-linear relationship.Each internal node of a decision tree is a choicepoint, dividing an individual method into rangesof possible values.
Each leaf node is labeledwith a classification (linking or disc~ding).
Themost popular method of decision tree induction,employed here, is C4.5 (Quinlan, 1993).Figure 4 shows a training phase in decision,tree based combination method.
In the trainingphase, the candidate synset ws k of a Koreanword is manually classified as linking ordiscarding and get assigned scores by eachheuristic.
A training data set is constructed bythese scores and manual classification.
Thetraining data set is used to optimize a model forcombining heuristics./ /~  Heur,st,c 3 ~ ~K~ Heuristic 4 ~ ('~'~"~\ 1 x -H Ig2  l I Oeoi  oo\[ Classfflcatzon I ~ -~" \[ tree learningFigure 4: Training phaseHeuristic 2Heuristic 3 n L=nklng orHeunsUc 4 DiscardingHeunstlc 5Heunst=c 6Figure 5: Mapping phaseFigure 5 shows a mapping phase.
In themapping phase, the new candidate synset ws~ of aKorean word is rated using 6 heuristics, and thenthe decision tree, which is learned in the trainingphase, classifies w& as linking or discarding.
Thesynset classified as linking is linked to thecorresponding Korean word.4 EvaluationIn this section, we evaluate the performance ofeach six heuristics as well as the combinationmethod.
To evaluate the performance ofWordNetmapping, the candidate synsets of 3260 senses ofKorean words in bilingual dictionary wasmanually classified as linking or discarding.We define 'precision' as the proportion ofcorrectly linked senses of Korean words to all thelinked senses of Korean words in a test set.
Wealso define 'coverage' as the proportion of linkedsenses of Korean words to all the senses ofKorean words in a test set.Table 1 contains the results for each heuristicevaluated individually against the manuallyclassified ata.
The test set here consists of the3260 manually classified senses.In general, the results of each heuristic seem tobe poor, but are always better than the randomchoice baseline.
The best heuristic according to145the precision is the maximum similarity heuristic.But it was applied to only 59.51% of 3260senses of Korean words.
The results of eachheuristic are better than the random mapping,with a statistically significance at the 99% level.RandommappingHeuristic 1Precision(%)49.8575.21Coverage(%)100.059.51Heuristic 2 74.66 100.0Heuristic 3 71.87 100.0Heuristic 4 55.49 29.36Heuristic 5 : 56.48 63.01Heuristic 6 67.24 64.14Table 1: Individual heuristics performanceSummingLogistic regressionDecisioin treePreeisi0n(%)84.6186.4193.59Coverage(%)100.0100.077.12Table 2: Performance and comparison of thedecision tree based combinationWe performed 10-fold cross validation toevaluate the performance of the combination ofall the heuristics using the decision tree - wesplit the data into ten parts, reserved one part asa validation set, trained the decision tree on theother nine parts and then evaluate the reservedpart.
This process is repeated nine times usingeach of the other nine parts as a validation set.Table 2 shows the results of the other trials ofthe combination of all the heuristics.
Summingis a way to simply sum all the scores of eachheuristic.
Then the candidate synset which hasthe highest summation of the scores is selected.Logistic regression, as described in (Hosmer andLemeshow, 1989), is a popular technique forbinary classification.
This technique applies aninverse logit function and employs the iterativereweighted least squares algorithm.
Thistechnique determines the weight of eachheuristic.With the combination of the heuristics usingsumming, we obtained an improvement overmaximum similarity heuristic (heuristic 1) of 9%,maintaining a coverage 100%.
The decision treeis able to correctly map 93.59% of the senses ofKorean words in bilingual dictionary,maintaining a coverage 77.12%.Applying the decision tree to combine all theheuristics for all Korean words in bilingualdictionary, we obtain a preliminary version of theKorean WordNet containing 21654 senses of17696 Korean nouns with an accuracy of 93.59%(-2-0.84% with 99% confidence).5 Related worksSeveral attempts have been performed toautomatically produce multilingual ontologies.
(Knight & Luk 1994) focuses on the constructionof Sensus, a large knowledge base for supportingthe Pangloss Machine Translation system,merging ontologies (ONTOS and UpperModel)and WordNet with monolingual and bilingualdictionaries.
(Okumura & Hovy 1994) describes asemi-automatic method for associating a Japaneselexicon to an ontology using a Japanese/Englishbilingual dictionary as a 'bridge'.
Several exicalresources and techniques are combined in(Atserias et al, 1997) to map Spanish words froma bilingual dictionary to WordNet.
In (Farreres etal., 1998), use of a taxonomic structure derivedfrom a monolingual MRD is proposed as an aidto the mapping process.This research is contrasted that it utilizedbilingual dictionary to build monolingualthesaurus based on the existing popular lexicalresources and used the combination of multipleunsupervided WSD heuristics.6 ConclusionThis paper has explored the automaticconstruction of a Korean WordNet frompre-existing lexical resources - English wordNetand Korean/English bilingual dictionary.
Wepresented several techniques for word sensedisambiguation and their application todisambiguate the translations in bilingualdictionary.
We obtained a preliminary version ofthe Korean WordNet containing 21654 senses of17696 Korean nouns.
In a series of experiments,we observed that the accuracy of mapping is over90%.ReferencesAtserias J., Climent S., Farreras J., Rigau G. andRodriguez H. (1997) Combining MultipleMethods for the Automatic Construction of146Muldlingual WordNets.
In proceeding of theConference on Recent Advances on NLP.Farreres X., Rigau G., and Rodnguez H.. (1998)Using WordNet for building WordNets.
InProceedings of COLING-ACL Workshop on Usageof WordNet in Natural Language ProcessingSystems.Gale W., Church K., and Yarowsky D. (1992)Estimating upper and lower bounds on theperformance of word-sense disarnbiguationprograms.
In Proceeding of 3Oth Annual Meetingof the Association for Computational Linguistics.Hosmer Jr. and Lemeshow S. (1989) AppliedLogistic Regression.
Wiley, New York.Knight K. and Luk S. (1994) Building a large-scaleknowledge base for machine translation.
InProceeding of the American Association forArtificial Intelligence.Miller G. (1990) Five papers on WordNet.Special Issue of International Journal ofLexicography.Miller G., Chodorow M., Landes S., Leacock C. andThomas R.. (1994) Using a semanticconcordance for sense identification.
InProceedings of the Human Language TechnologyWorkshop.Okumura A. and Hovy E. (1994) BuildingJapanese-English Dictionary based on Ontology forMachine Translation.
In Proceedings of ARPAWorkshop on Human Language Technology.Quinlan R.. (1993) C4.5: Programs For MachineLearning.
Morgan Kaufmann Publishers.Seungwoo Lee and Geunbae Lee.
(2000)Unsupervised Noun Sense Disambiguafion UsingLocal Context and Co-occurrence.
In Journal ofKorean Information Science Society.
(in press)147
