Proceedings of EACL '99Parsing with an Extended Domain of LocalityJ ohn  Car ro l l ,  N ico las  N ico lov ,  O lga  Shaumyan,  Mar t ine  Smets  &: Dav id  Wei rSchool  of  Cogn i t ive  and  Comput ing  SciencesUn ivers i ty  of  SussexBr ighton ,  BN1 9QH,  UKAbst ractOne of the claimed benefits of Tree Ad-joining Grammars is that they have anextended omain of locality (EDOL).
Weconsider how this can be exploited tolimit the need for feature structure uni-fication during parsing.
We comparetwo wide-coverage l xicalized grammarsof English, LEXSYS and XTAG, findingthat the two grammars exploit EDOL indifferent ways.1 In t roduct ionOne of the most basic properties of Tree AdjoiningGrammars (TAGS) is that they have an extendeddomain  o f  local i ty  (EDOL) (Joshi, 1994).
Thisrefers to the fact that the elementary trees thatmake up the grammar are larger than the cor-responding units (the productions) that are usedin phrase-structure ule-based frameworks.
Theclaim is that in Lexicalized TAGS (LTAGS) the el-ementary trees provide a domain of locality largeenough to state co-occurrence r lationships be-tween a lexical item (the anchor  of the elemen-tary tree) and the nodes it imposes constraintson.
We will call this the extended domain  oflocal i ty hypothes is .For example, wh-movement can be expressedlocally in a tree that will be anchored by a verbof which an argument is extracted.
Consequently,features which are shared by the extraction siteand the wh-word, such as case, do not need to bepercolated, but are directly identified in the tree.Figure 1 shows a tree in which the case featureat the extraction site and the wh-word share thesame value31The anchor~ substitution and foot nodes of treesare marked with the symbols o, $ and.
,  respectively.Words in parenthesis are included in trees to provideexamples of strings this tree can derive.Much of the research on TAGS tail be seen asillustrating how its EDOL can be exploited in vari-ous ways.
However, to date, only indirect evidencehas been given regarding the beneficial effects ofthe EDOL on parsing efficiency.
The argument,due to Schabes (1990), is that benefits to parsingarise from lexicalization, and that lexicalization isonly possible because of the EDOL.
A parser deal-ing with a lexicalized grammar needs to consideronly those elementary structures that can be as-sociated with the lexical items appearing in theinput.
This can substantially reduce the effectivegrammar size at parse time.
The argument thatan EDOL is required for lexicalization is based onthe observation that not every set of trees thatcan be generated by a CFG can be generated bya lexicalized CFG.
But does the EDOL have anyother more direct effects on parsing efficiency?On the one hand, it is a consequence of theEDOL that wide-coverage LTAGS are larger thantheir rule-based counterparts.
With larger ele-mentary structures, generalizations are lost re-garding the internal structure of the elementarytrees.
Since parse time depends on grammar size,this could have an adverse ffect on parsing effi-ciency.
However, the problem of grammar size inTAG has to some extent been addressed both withrespect o grammar encoding (Evans et al, 1995;Candito, 1996) and parsing (Joshi and Srinivas,1994; Evans and Weir, 1998).On the other hand, if the EDOL hypothesis holdsfor those dependencies that are being checked bythe parser, then the burden of passing feature val-ues around during parsing will be less than in arule-based framework.
If all dependencies thatthe parser is checking can be stated directly withinthe elementary structures of the grammar, theydo not need to be comPuted ynamically duringthe parsing process by means of feature percola-tion.
For example, there is no need to use a slashfeature to establish filler-gap dependencies overunbounded istances across the tree if the EDOL269Proceedings ofEACL '99Thus, passive sentences such as The scheme wassingled out by a recent Government report arefound difficult 3, despite the presence of the syn-tactic cues was, -ed and by.
We therefore replacepassive constructions with corresponding activeforms.
We are currently integrating further rulesto split conjoined sentences and extract embeddedclauses.
Syntactic simplification operates itera-tively until a configuration is reached that cannotbe simplified.
This approach is broadly similar tothat proposed by (Chandrasekar et al, 1996).One of the many challenges in syntactic simplifi-cation is the observed effect of the total length of atext being increased when longer sentences are re-placed by multiple shorter ones.
Also, the removalof cohesive devices such as conjunctions may re-sult in anaphora crossing sentence boundaries.
Tomaintain text coherence and cohesion (Grodzin-sky et al, 1993) an anaphor is replaced by its ref-erent if the containing sentence is split.Lexica l  Simpl i f ier The lexical simplifier(based on (Devhn, 1999; Devlin and Tait, 1998))replaces content words with simpler synonyms.It first retrieves a set of synonyms for each wordfrom WordNet (Miller et al, 1993), then, accord-ing to the user's desired level of simplification, theoriginal word plus a percentage of the synonymlist are looked up in the Oxford PsycholinguisticDatabase (Quinlan, 1992) for the correspondingKucera-Francis frequencies.
The word with thehighest frequency is selected.Morpho log ica l  Generator  Simplificationworks on the inflectionally analysed text, sothe last stage is morphological generation.
Thegenerator is simply an inverted version of themorphological analyser described above.
Theinversion is performed automatically (Minnen andCarroll, Submitted), so any improvements madeto the analyser are reflected in the generator atno extra cost.
Finally, inter-word spelling changes(e.g.
a apple ---* an apple), auxiliary reduction,etc.
are performed.3 Eva luat ionWe will perform an experimental evaluation of thesystem with the help of aphasic participants whoare matched to the extent hat none display visu-ally related reading difficulties, which would con-found the results, and all possess a sufficientlyhigh reading ability--determined at the time ofthe experiment by using an aphasia assessmentbattery.
As the system is a general tool aimed at3Semantically reversible sentences such as The boywas kissed by the girl are even more difficult, sinceeither noun phrase could be the subject.all aphasics, the participants will not be screenedfor aphasia type.
The readability of the simpli-fied text and the usability of the system will beassessed by observation and interview; questionswill be posed to gauge subjects' comprehension fboth explicit and implicit material.ReferencesB.
Baldwin.
1997.
CogNIAC: high precision coref-erence with limited knowledge and linguistic re-sources.
In Proceedings of a Workshop sponsoredby the A CL (Operational Factors in Practical, Ro-bust Anaphora Resolution for Unrestricted Texts),Universidad Nacional de Educacion a Distancia,Madrid, Spain.E.
Briscoe and J. Carroll.
1995.
Developing and eval-uating a probabilistic LR parser of part-of-speechand punctuation labels.
In Proceedings of the ~thA CL/SIGPARSE International Workshop on Pars-ing Technologies, pages 48-58.J.
Carroll and E. Briscoe.
1996.
Apportioning de-velopment effort in a probabilistic LR parsing sys-tem through evaluation.
In Proceedings of theA CL/SICDA T Conference on Empirical Methods inNatural Language Processing, pages 92-100.R.
Chandrasekar, C. Doran, and B. Srinivas.
1996.Motivations and methods for text simplification.In Proceedings of the 16th Conference on Compu-tational Linguistics (COLING-96).H.
Cunningham, Y. Wilks, and R. Gaizauskas.
1996.CATE--a general architecture for text engineering.In Proceedings of the 16th Conference on Computa-tional Linguistics (COLINC-96).S.
Devlin and J. Tait.
1998.
The use of a psy-cholinguistic database in the simplification of textfor aphasic readers.
In J. Nerbonne.
LinguisticDatabases.
Lecture Notes.
Stanford, USA: CSLIPublications.S.
Devlin.
1999.
Simplifying natural language text foraphasic readers.
Ph.D. Dissertation, University ofSunderland, UK.D.
Elworthy.
1994.
Does Baum Welch re-estimationhelp taggers?
In Proceedings of the 4th ACL Con-ference on Applied Natural Language Processing,pages 53-58.Y.
Grodzinsky, K. Wexler, Y. Chien, S. Marakovitz,and J. Solomon.
1993.
The breakdown of bindingrelations.
Brain and Language, 45(3):396-422.G.
Miller, R. Beckwith, C. Fellbaum, D. Cross,K.
Miller, and R. Tengi.
1993.
Five papers onWordNet.
Technical report, Princeton University,Princeton, N.J.G.
Minnen and J. Carroll.
Submitted.
Fast and ro-bust morphological generation in a practical NLPsystem.M.
Osborne.
Submitted.
Minimum descriptionlength-based models for practical grammar induc-tion.P.
Quinlan.
1992.
The Oxford PsycholinguisticDatabase.
Oxford University Press.270
