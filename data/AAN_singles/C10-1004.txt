Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 28?36,Beijing, August 2010Multilingual Subjectivity: Are More Languages Better?Carmen Banea, Rada MihalceaDepartment of Computer ScienceUniversity of North Texascarmenbanea@my.unt.edurada@cs.unt.eduJanyce WiebeDepartment of Computer ScienceUniversity of Pittsburghwiebe@cs.pitt.eduAbstractWhile subjectivity related research inother languages has increased, most of thework focuses on single languages.
Thispaper explores the integration of featuresoriginating from multiple languages intoa machine learning approach to subjectiv-ity analysis, and aims to show that thisenriched feature set provides for more ef-fective modeling for the source as wellas the target languages.
We show notonly that we are able to achieve over75% macro accuracy in all of the six lan-guages we experiment with, but also thatby using features drawn from multiplelanguages we can construct high-precisionmeta-classifiers with a precision of over83%.1 IntroductionFollowing the terminology proposed by (Wiebeet al, 2005), subjectivity and sentiment analysisfocuses on the automatic identification of privatestates, such as opinions, emotions, sentiments,evaluations, beliefs, and speculations in naturallanguage.
While subjectivity classification labelstext as either subjective or objective, sentiment orpolarity classification adds an additional level ofgranularity, by further classifying subjective textas either positive, negative or neutral.To date, a large number of text processing ap-plications have used techniques for automatic sen-timent and subjectivity analysis, including auto-matic expressive text-to-speech synthesis (Alm etal., 1990), tracking sentiment timelines in on-lineforums and news (Balog et al, 2006; Lloyd et al,2005), and mining opinions from product reviews(Hu and Liu, 2004).
In many natural languageprocessing tasks, subjectivity and sentiment clas-sification has been used as a first phase filtering togenerate more viable data.
Research that benefitedfrom this additional layering ranges from ques-tion answering (Yu and Hatzivassiloglou, 2003),to conversation summarization (Carenini et al,2008), and text semantic analysis (Wiebe and Mi-halcea, 2006; Esuli and Sebastiani, 2006a).Although subjectivity tends to be preservedacross languages ?
see the manual study in (Mi-halcea et al, 2007), (Banea et al, 2008) hypoth-esize that subjectivity is expressed differently invarious languages due to lexicalization, formalversus informal markers, etc.
Based on this obser-vation, our research seeks to answer the followingquestions.
First, can we reliably predict sentence-level subjectivity in languages other than English,by leveraging on a manually annotated Englishdataset?
Second, can we improve the English sub-jectivity classification by expanding the featurespace through the use of multilingual data?
Sim-ilarly, can we also improve the classifiers in theother target languages?
Finally, third, can we ben-efit from the multilingual subjectivity space andbuild a high-precision subjectivity classifier thatcould be used to generate subjectivity datasets inthe target languages?The paper is organized as follows.
We intro-duce the datasets and the general framework inSection 2.
Sections 3, 4, and 5 address in turn eachof the three research questions mentioned above.Section 6 describes related literature in the areaof multilingual subjectivity.
Finally, we draw ourconclusions in Section 7.2 Multilingual DatasetsCorpora that are manually annotated for subjec-tivity, polarity, or emotion, are available in onlyselect languages, since they require a consider-able amount of human effort.
Due to this im-pediment, the focus of this paper is to create amethod for extrapolating subjectivity data devel-28SubjP SubjR SubjF ObjP ObjR ObjF AllP AllR AllF90.4% 34.2% 46.6% 82.4% 30.7% 44.7% 86.7% 32.6% 47.4%Table 1: Results obtained with a rule-based subjectivity classifier on the MPQA corpus (Wiebe andRiloff, 2005)oped in a source language and to transfer it toother languages.
Multilingual feature spaces aregenerated to create even better subjectivity classi-fiers, outperforming those trained on the individ-ual languages alone.We use the Multi-Perspective Question An-swering (MPQA) corpus, consisting of 535English-language news articles from a varietyof sources, manually annotated for subjectivity(Wiebe et al, 2005).
Although the corpus is an-notated at the clause and phrase levels, we usethe sentence-level annotations associated with thedataset in (Wiebe and Riloff, 2005).
A sentenceis labeled as subjective if it has at least one pri-vate state of strength medium or higher.
Other-wise the sentence is labeled as objective.
From theapproximately 9700 sentences in this corpus, 55%of them are labeled as subjective, while the restare objective.
Therefore, 55% represents the ma-jority baseline on this corpus.
(Wiebe and Riloff,2005) apply both a subjective and an objectiverule-based classifier to the MPQA corpus data andobtain the results presented in Table 1.1In order to generate parallel corpora to MPQAin other languages, we rely on the method we pro-posed in (Banea et al, 2008).
We experiment withfive languages other than English (En), namelyArabic (Ar), French (Fr), German (De), Roma-nian (Ro) and Spanish (Es).
Our choice of lan-guages is motivated by several reasons.
First,we wanted languages that are highly lexicalizedand have clear word delimitations.
Second, wewere interested to cover languages that are simi-lar to English as well as languages with a com-pletely different etymology.
Consideration wasgiven to include Asian languages, such as Chi-nese or Japanese, but the fact that their script with-1For the purpose of this paper we follow this abbreviationstyle: Subj stands for subjective, Obj stands for objective,and All represents overall macro measures, computed overthe subjective and objective classes; P, R, F, and MAcc cor-respond to precision, recall, F-measure, and macro-accuracy,respectively.out word-segmentation preprocessing does not di-rectly map to words was a deterrent.
Finally, an-other limitation on our choice of languages is theneed for a publicly available machine translationsystem between the source language and each ofthe target languages.We construct a subjectivity annotated corpusfor each of the five languages by using machinetranslation to transfer the source language datainto the target language.
We then project the orig-inal sentence level English subjectivity labelingonto the target data.
For all languages, other thanRomanian, we use the Google Translate service,2a publicly available machine translation enginebased on statistical models.
The reason Roma-nian is not included in this group is that, at thetime when we performed the first experiments,Google Translate did not provide a translation ser-vice for this language.
Instead, we used an al-ternative statistical translation system called Lan-guageWeaver,3 which was commercially avail-able, and which the company kindly allowed usto use for research purposes.The raw corpora in the five target lan-guages are available for download athttp://lit.csci.unt.edu/index.php/Downloads,while the English MPQA corpus can be obtainedfrom http://www.cs.pitt.edu/mpqa.Given the specifics of each language, we em-ploy several preprocessing techniques.
For Ro-manian, French, English, German and Spanish,we remove all the diacritics, numbers and punc-tuation marks except - and ?.
The exceptions aremotivated by the fact that they may mark contrac-tions, such as En: it?s or Ro: s-ar (may be), andthe component words may not be resolved to thecorrect forms.
For Arabic, although it has a dif-ferent encoding, we wanted to make sure to treatit in a way similar to the languages with a Roman2http://www.google.com/translate t3http://www.languageweaver.com/29alphabet.
We therefore use a library4 that mapsArabic script to a space of Roman-alphabet letterssupplemented with punctuation marks so that theycan allow for additional dimensionality.Once the corpora are preprocessed, each sen-tence is defined by six views: one in the origi-nal source language (English), and five obtainedthrough automatic translation in each of the tar-get languages.
Multiple datasets that cover allpossible combinations of six languages taken onethrough six (a total of 63 combinations) are gen-erated.
These datasets feature a vector for eachsentence present in MPQA (approximately 9700).The vector contains only unigram features in onelanguage for a monolingual dataset.
For a mul-tilingual dataset, the vector represents a cumu-lation of monolingual unigram features extractedfrom each view of the sentence.
For example, oneof the combinations of six taken three is Arabic-German-English.
For this combination, the vectoris composed of unigram features extracted fromeach of the Arabic, German and English transla-tions of the sentence.We perform ten-fold cross validation and trainNa?
?ve Bayes classifiers with feature selection oneach dataset combination.
The top 20% of the fea-tures present in the training data are retained.
Fordatasets resulting from combinations of all lan-guages taken one, the classifiers are monolingualclassifiers.
All other classifiers are multilingual,and their feature space increases with each addi-tional language added.
Expanding the feature setby encompassing a group of languages enables usto provide an answer to two problems that can ap-pear due to data sparseness.
First, enough trainingdata may not be available in the monolingual cor-pus alone in order to correctly infer labeling basedon statistical measures.
Second, features appear-ing in the monolingual test set may not be presentin the training set and therefore their informationcannot be used to generate a correct classification.Both of these problems are further explainedthrough the examples below, where we make thesimplifying assumption that the words in italicsare the only potential carriers of subjective con-tent, and that, without them, their surrounding4Lingua::AR::Word PERL library.contexts would be objective.
Therefore, their as-sociation with an either objective or subjectivemeaning imparts to the entire segment the samelabeling upon classification.To explore the first sparseness problem, let usconsider the following two examples extractedfrom the English version of the MPQA dataset,followed by their machine translations in German:?En 1: rights group Amnesty Interna-tional said it was concerned about thehigh risk of violence in the aftermath?
?En 2: official said that US diplomatsto countries concerned are authorizedto explain to these countries?
?De 1: Amnesty International sagte, essei besorgt u?ber das hohe Risiko vonGewalt in der Folgezeit?
?De 2: Beamte sagte, dass US-Diplomaten betroffenen La?nderberechtigt sind, diese La?nder zuerkla?ren?We focus our discussion on the word con-cerned, which in the first example is used in itssubjective sense, while in the second it carries anobjective meaning (as it refers to a group of coun-tries exhibiting a particular feature defined ear-lier on in the context).
The words in italics inthe German contexts represent the translations ofconcerned into German, which are functionallydifferent as they are shaped by their surroundingcontext.
By training a classifier on the English ex-amples alone, under the data sparseness paradigm,the machine learning model may not differentiatebetween the word?s objective and subjective useswhen predicting a label for the entire sentence.However, appending the German translation to theexamples generates additional dimensions for thismodel and allows the classifier to potentially dis-tinguish between the senses and provide the cor-rect sentence label.For the second problem, let us consider twoother examples from the English MPQA and theirrespective translations into Romanian:?En 3: could secure concessions on Tai-wan in return for supporting Bush on is-sues such as anti-terrorism and?30Lang SubjP SubjR SubjF ObjP ObjR ObjF AllP AllR AllF MAccEn 74.01% 83.64% 78.53% 75.89% 63.68% 69.25% 74.95% 73.66% 73.89% 74.72%Ro 73.50% 82.06% 77.54% 74.08% 63.40% 68.33% 73.79% 72.73% 72.94% 73.72%Es 74.02% 82.84% 78.19% 75.11% 64.05% 69.14% 74.57% 73.44% 73.66% 74.44%Fr 73.83% 83.03% 78.16% 75.19% 63.61% 68.92% 74.51% 73.32% 73.54% 74.35%De 73.26% 83.49% 78.04% 75.32% 62.30% 68.19% 74.29% 72.90% 73.12% 74.02%Ar 71.98% 81.47% 76.43% 72.62% 60.78% 66.17% 72.30% 71.13% 71.30% 72.22%Table 2: Na?
?ve Bayes learners trained on six individual languages?En 4: to the potential for changefrom within America.
Supporting ourschools and community centres is agood?
?Ro 3: ar putea asigura concesii cuprivire la Taiwan, ?
?n schimb pentrusust?inerea lui Bush pe probleme cum arfi anti-terorismului s?i?
?Ro 4: la potent?ialul de schimbare dininteriorul Americii.
Sprijinirea s?colilenoastre s?i centre de comunitate este unbun?In this case, supporting is used in both English ex-amples in senses that are both subjective; the wordis, however, translated into Romanian through twosynonyms, namely sust?inerea and sprijinirea.
Letus assume that sufficient training examples areavailable to strengthen a link between support-ing and sust?inerea, and the classifier is presentedwith a context containing sprijinirea, unseen inthe training data.
A multilingual classifier may beable to predict a label for the context using the co-occurrence metrics based on supporting and ex-trapolate a label when the context contains boththe English word and its translation into Roma-nian as sprijinirea.
For a monolingual classifier,such an inference is not possible, and the fea-ture is discarded.
Therefore a multi-lingual classi-fier model may gain additional strength from co-occurring words across languages.3 Question 1Can we reliably predict sentence-level sub-jectivity in languages other than English, byleveraging on a manually annotated Englishdataset?In (Banea et al, 2008), we explored several meth-ods for porting subjectivity annotated data froma source language (English) to a target language(Romanian and Spanish).
Here, we focus on thetransfer of manually annotated corpora throughthe usage of machine translation by projecting theoriginal sentence level annotations onto the gener-ated parallel text in the target language.
Our aimis not to improve on that method, but rather to ver-ify that the results are reliable across a number oflanguages.
Therefore, we conduct this experimentin several additional languages, namely French,German and Arabic, and compare the results withthose obtained for Spanish and Romanian.Table 2 shows the results obtained using Na?
?veBayes classifiers trained in each language individ-ually, with a macro accuracy ranging from 71.30%(for Arabic) to 73.89% (for English).5 As ex-pected, the English machine learner outperformsthose trained on other languages, as the originallanguage of the annotations is English.
However,it is worth noting that all measures do not deviateby more than 3.27%, implying that classifiers builtusing this technique exhibit a consistent behavioracross languages.4 Question 2Can we improve the English subjectivity clas-sification by expanding the feature spacethrough the use of multilingual data?
Simi-larly, can we also improve the classifiers in theother target languages?We now turn towards investigating the impact onsubjectivity classification of an expanded featurespace through the inclusion of multilingual data.In order to methodically assess classifier behavior,we generate multiple datasets containing all pos-5Note that the experiments conducted in (Banea et al,2008) were made on a different test set, and thus the resultsare not directly comparable across the two papers.31No lang SubjP SubjR SubjF ObjP ObjR ObjF AllP AllR AllF1 73.43% 82.76% 77.82% 74.70% 62.97% 68.33% 74.07% 72.86% 73.08%2 74.59% 83.14% 78.63% 75.70% 64.97% 69.92% 75.15% 74.05% 74.28%3 75.04% 83.27% 78.94% 76.06% 65.75% 70.53% 75.55% 74.51% 74.74%4 75.26% 83.36% 79.10% 76.26% 66.10% 70.82% 75.76% 74.73% 74.96%5 75.38% 83.45% 79.21% 76.41% 66.29% 70.99% 75.90% 74.87% 75.10%6 75.43% 83.66% 79.33% 76.64% 66.30% 71.10% 76.04% 74.98% 75.21%Table 3: Average measures for a particular number of languages in a combination (from one throughsix) for Na?
?ve Bayes classifiers using a multilingual spacesible combinations of one through six languages,as described in Section 2.
We then train Na?
?veBayes learners on the multilingual data and av-erage our results per each group comprised of aparticular number of languages.
For example, forone language, we have the six individual classi-fiers described in Section 3; for the group of threelanguages, the average is calculated over 20 pos-sible combinations; and so on.Table 3 shows the results of this experiment.We can see that the overall F-measure increasesfrom 73.08% ?
which is the average over one lan-guage ?
to 75.21% when all languages are takeninto consideration (8.6% error reduction).
Wemeasured the statistical significance of these re-sults by considering on one side the predictionsmade by the best performing classifier for one lan-guage (i.e., English), and on the other side thepredictions made by the classifier trained on themultilingual space composed of all six languages.Using a paired t-test, the improvement was foundto be significant at p = 0.001.
It is worth men-tioning that both the subjective and the objectiveprecision measures increase to 75% when morethan 3 languages are considered, while the overallrecall level stays constant at 74%.To verify that the improvement is due indeedto the addition of multilingual features, and it isnot a characteristic of the classifier, we also testedtwo other classifiers, namely KNN and Rocchio.Figure 1 shows the average macro-accuracies ob-tained with these classifiers.
For all the classi-fiers, the accuracies of the multilingual combina-tions exhibit an increasing trend, as a larger num-ber of languages is used to predict the subjectivityannotations.
The Na?
?ve Bayes algorithm has thebest performance, and a relative error rate reduc-0.60.650.70.750.81  2  3  4  5  6Number of languagesNBKNNRocchioFigure 1: Average Macro-Accuracy per group oflanguages (combinations of 6 taken one throughsix)tion in accuracy of 8.25% for the grouping formedof six languages versus one, while KNN and Roc-chio exhibit an error rate reduction of 5.82% and9.45%, respectively.
All of these reductions arestatistically significant.In order to assess how the proposed multilin-gual expansion improves on the individual lan-guage classifiers, we select one language at a timeto be the reference, and then compute the aver-age accuracies of the Na?
?ve Bayes learner acrossall the language groupings (from one through six)that contain the language.
The results from thisexperiment are illustrated in Figure 2.
The base-line in this case is represented by the accuracy ob-tained with a classifier trained on only one lan-guage (this corresponds to 1 on the X-axis).
Asmore languages are added to the feature space,we notice a steady improvement in performance.When the language of reference is Arabic, we ob-tain an error reduction of 15.27%; 9.04% for Ro-320.720.730.740.750.761  2  3  4  5  6Number of languagesArDeEnEsFrRoFigure 2: Average macro-accuracy progressionrelative to a given languagemanian; 7.80% for German; 6.44% for French;6.06% for Spanish; and 4.90 % for English.
Evenif the improvements seem minor, they are consis-tent, and the use of a multilingual feature set en-ables every language to reach a higher accuracythan individually attainable.In terms of the best classifiers obtained foreach grouping of one through six, English pro-vides the best accuracy among individual clas-sifiers (74.71%).
When considering all possiblecombinations of six classifiers taken two, Germanand Spanish provide the best results, at 75.67%.Upon considering an additional language to themix, the addition of Romanian to the German-Spanish classifier further improves the accuracyto 76.06%.
Next, the addition of Arabic resultsin the best performing overall classifier, with anaccuracy of 76.22%.
Upon adding supplementallanguages, such as English or French, no furtherimprovements are obtained.
We believe this isthe case because German and Spanish are able toexpand the dimensionality conferred by Englishalone, while at the same time generating a moreorthogonal space.
Incrementally, Romanian andArabic are able to provide high quality featuresfor the classification task.
This behavior suggeststhat languages that are somewhat further apart aremore useful for multilingual subjectivity classifi-cation than intermediary languages.5 Question 3Can we train a high precision classifier with agood recall level which could be used to gen-erate subjectivity datasets in the target lan-guages?Since we showed that the inclusion of multilingualinformation improves the performance of subjec-tivity classifiers for all the languages involved, wefurther explore how the classifiers?
predictions canbe combined in order to generate high-precisionsubjectivity annotations.
As shown in previouswork, a high-precision classifier can be used toautomatically generate subjectivity annotated data(Riloff and Wiebe, 2003).
Additionally, the dataannotated with a high-precision classifier can beused as a seed for bootstrapping methods, to fur-ther enrich each language individually.We experiment with a majority vote meta-classifier, which combines the predictions of themonolingual Na?
?ve Bayes classifiers described inSection 3.
For a particular number of languages(one through six), all possible combinations oflanguages are considered.
Each combination sug-gests a prediction only if its component classifiersagree, otherwise the system returns an ?unknown?prediction.
The averages are computed across allthe combinations featuring the same number oflanguages, regardless of language identity.The results are shown in Table 4.
Themacro precision and recall averaged across groupsformed using a given number of languages arepresented in Figure 3.
If the average monolingualclassifier has a precision of 74.07%, the precisionincreases as more languages are considered, witha maximum precision of 83.38% obtained whenthe predictions of all six languages are consid-ered (56.02% error reduction).
It is interesting tonote that the highest precision meta-classifier forgroups of two languages includes German, whilefor groups with more than three languages, bothArabic and German are always present in the topperforming combinations.
English only appearsin the highest precision combination for one, fiveand six languages, indicating the fact that the pre-dictions based on Arabic and German are morerobust.We further analyze the behavior of each lan-guage considering only those meta-classifiers thatinclude the given language.
As seen in Figure 4,all languages experience a boost in performance33No lang SubjP SubjR SubjF ObjP ObjR ObjF AllP AllR AllF1 73.43% 82.76% 77.82% 74.70% 62.97% 68.33% 74.07% 72.86% 73.08%2 76.88% 76.39% 76.63% 80.17% 54.35% 64.76% 78.53% 65.37% 70.69%3 78.56% 72.42% 75.36% 82.58% 49.69% 62.02% 80.57% 61.05% 68.69%4 79.61% 69.50% 74.21% 84.07% 46.54% 59.89% 81.84% 58.02% 67.05%5 80.36% 67.17% 73.17% 85.09% 44.19% 58.16% 82.73% 55.68% 65.67%6 80.94% 65.20% 72.23% 85.83% 42.32% 56.69% 83.38% 53.76% 64.46%Table 4: Average measures for a particular number of languages in a combination (from one throughsix) for meta-classifiers0.50.550.60.650.70.750.80.851  2  3  4  5  6Number of languagesMacro-PrecisionMacro-RecallFigure 3: Average Macro-Precision and Recallacross a given number of languagesas a result of paired language reinforcement.
Ara-bic gains an absolute 11.0% in average precisionwhen considering votes from all languages, ascompared to the 72.30% baseline consisting of theprecision of the classifier using only monolingualfeatures; this represents an error reduction in pre-cision of 66.71%.
The other languages experi-ence a similar boost, including English which ex-hibits an error reduction of 50.75% compared tothe baseline.
Despite the fact that with each lan-guage that is added to the meta-classifier, the re-call decreases, even when considering votes fromall six languages, the recall is still reasonably highat 53.76%.The results presented in table 4 are promis-ing, as they are comparable to the ones obtainedin previous work.
Compared to (Wiebe et al,2005), who used a high-precision rule-based clas-sifier on the English MPQA corpus (see Table 1),our method has a precision smaller by 3.32%, buta recall larger by 21.16%.
Additionally, unlike0.710.720.730.740.750.760.770.780.790.80.810.820.830.841  2  3  4  5  6Number of languagesArDeEnEsFrRoFigure 4: Average Macro-Precision relative to agiven language(Wiebe et al, 2005), which requires language-specific rules, making it applicable only to En-glish, our method can be used to construct a high-precision classifier in any language that can beconnected to English via machine translation.6 Related WorkRecently, resources and tools for sentiment anal-ysis developed for English have been used asa starting point to build resources in other lan-guages, via cross-lingual projections or mono-lingual and multi-lingual bootstrapping.
Severaldirections were followed, focused on leveragingannotation schemes, lexica, corpora and auto-mated annotation systems.
The English annota-tion scheme developed by (Wiebe et al, 2005)for opinionated text lays the groundwork for theresearch carried out by (Esuli et al, 2008) whenannotating expressions of private state in the Ital-ian Content Annotation Bank.
Sentiment andsubjectivity lexica such as the one included with34the OpinionFinder distribution (Wiebe and Riloff,2005), the General Inquirer (Stone et al, 1967), orthe SentiWordNet (Esuli and Sebastiani, 2006b)were transfered into Chinese (Ku et al, 2006; Wu,2008) and into Romanian (Mihalcea et al, 2007).English corpora manually annotated for subjec-tivity or sentiment such as MPQA (Wiebe et al,2005), or the multi-domain sentiment classifica-tion corpus (Blitzer et al, 2007) were subjectedto experiments in Spanish, Romanian, or Chineseupon automatic translation by (Banea et al, 2008;Wan, 2009).
Furthermore, tools developed for En-glish were used to determine sentiment or sub-jectivity labeling for a given target language bytransferring the text to English and applying anEnglish classifier on the resulting data.
The labelswere then transfered back into the target language(Bautin et al, 2008; Banea et al, 2008).
These ex-periments are carried out in Arabic, Chinese, En-glish, French, German, Italian, Japanese, Korean,Spanish, and Romanian.The work closest to ours is the one proposedby (Wan, 2009), who constructs a polarity co-training system by using the multi-lingual viewsobtained through the automatic translation ofproduct-reviews into Chinese and English.
Whilethis work proves that leveraging cross-lingual in-formation improves sentiment analysis in Chineseover what could be achieved using monolingualresources alone, there are several major differ-ences with respect to the approach we are propos-ing here.
First, our training set is based solelyon the automatic translation of the English corpus.We do not require an in-domain dataset availablein the target language that would be needed forthe co-training approach.
Our method is thereforetransferable to any language that has an English-totarget language translation engine.
Further, we fo-cus on using multi-lingual data from six languagesto show that the results are reliable and replicableacross each language and that multiple languagesaid not only in conducting subjectivity research inthe target language, but also in improving the ac-curacy in the source language as well.
Finally,while (Wan, 2009) research focuses on polaritydetection based on reviews, our work seeks to de-termine sentence-level subjectivity from raw text.7 ConclusionOur results suggest that including multilingualinformation when modeling subjectivity can notonly extrapolate current resources available forEnglish into other languages, but can also improvesubjectivity classification in the source languageitself.
We showed that we can improve an Englishclassifier by using out-of-language features, thusachieving a 4.90% error reduction in accuracywith respect to using English alone.
Moreover, wealso showed that languages other than English canachieve an F-measure in subjectivity annotationof over 75%, without using any manually craftedresources for these languages.
Furthermore, bycombining the predictions made by monolingualclassifiers using a majority vote learner, we areable to generate sentence-level subjectivity anno-tated data with a precision of 83% and a recalllevel above 50%.
Such high-precision classifiersmay be later used not only to create subjectivity-annotated data in the target language, but also togenerate the seeds needed to sustain a language-specific bootstrapping.To conclude and provide an answer to the ques-tion formulated in the title, more languages arebetter, as they are able to complement each other,and together they provide better classification re-sults.
When one language cannot provide suffi-cient information, another one can come to therescue.AcknowledgmentsThis material is based in part upon work supportedby National Science Foundation awards #0917170and #0916046.
Any opinions, findings, and con-clusions or recommendations expressed in thismaterial are those of the authors and do not nec-essarily reflect the views of the National ScienceFoundation.ReferencesAlm, Cecilia Ovesdotter, Dan Roth, and Richard Sproat.1990.
Emotions from text: machine learning for text-based emotion prediction.
Intelligence.Balog, Krisztian, Gilad Mishne, and Maarten De Rijke.2006.
Why Are They Excited?
Identifying and Explain-ing Spikes in Blog Mood Levels.
In Proceedings of the3511th Conference of the European Chapter of the Associa-tion for Computational Linguistics (EACL-2006), Trento,Italy.Banea, Carmen, Rada Mihalcea, Janyce Wiebe, and SamerHassan.
2008.
Multilingual Subjectivity Analysis UsingMachine Translation.
In Proceedings of the 2008 Con-ference on Empirical Methods in Natural Language Pro-cessing (EMNLP-2008), pages 127?135, Honolulu.Bautin, Mikhail, Lohit Vijayarenu, and Steven Skiena.
2008.International Sentiment Analysis for News and Blogs.
InProceedings of the International Conference on Weblogsand Social Media (ICWSM-2008), Seattle, Washington.Blitzer, John, Mark Dredze, and Fernando Pereira.
2007.Biographies, Bollywood, Boom-boxes and Blenders: Do-main Adaptation for Sentiment Classification.
In Pro-ceedings of the 45th Annual Meeting of the Associationof Computational (ACL-2007), pages 440?447, Prague,Czech Republic.
Association for Computational Linguis-tics.Carenini, Giuseppe, Raymond T Ng, and Xiaodong Zhou.2008.
Summarizing Emails with Conversational Cohe-sion and Subjectivity.
In Proceedings of the Associationfor Computational Linguistics: Human Language Tech-nologies (ACL- HLT 2008), pages 353?361, Columbus,Ohio.Esuli, Andrea and Fabrizio Sebastiani.
2006a.
DeterminingTerm Subjectivity and Term Orientation for Opinion Min-ing.
In Proceedings of the 11th Meeting of the EuropeanChapter of the Association for Computational Linguistics(EACL-2006), volume 2, pages 193?200, Trento, Italy.Esuli, Andrea and Fabrizio Sebastiani.
2006b.
SentiWord-Net: A Publicly Available Lexical Resource for OpinionMining.
In Proceedings of the 5th Conference on Lan-guage Resources and Evaluation, pages 417?422.Esuli, Andrea, Fabrizio Sebastiani, and Ilaria C Urciuoli.2008.
Annotating Expressions of Opinion and Emotionin the Italian Content Annotation Bank.
In Proceedingsof the Sixth International Language Resources and Eval-uation (LREC-2008), Marrakech, Morocco.Hu, Minqing and Bing Liu.
2004.
Mining and Summariz-ing Customer Reviews.
In Proceedings of ACM Confer-ence on Knowledge Discovery and Data Mining (ACM-SIGKDD-2004), pages 168?177, Seattle, Washington.Ku, Lun-wei, Yu-ting Liang, and Hsin-hsi Chen.
2006.Opinion Extraction, Summarization and Tracking inNews and Blog Corpora.
In Proceedings of AAAI-2006Spring Symposium on Computational Approaches to An-alyzing Weblogs, number 2001, Boston, Massachusetts.Lloyd, Levon, Dimitrios Kechagias, and Steven Skiena,2005.
Lydia : A System for Large-Scale News Analysis( Extended Abstract ) News Analysis with Lydia, pages161?166.
Springer, Berlin / Heidelberg.Mihalcea, Rada, Carmen Banea, and Janyce Wiebe.
2007.Learning Multilingual Subjective Language via Cross-Lingual Projections.
In Proceedings of the 45th AnnualMeeting of the Association of Computational Linguistics(ACL-2007), pages 976?983, Prague, Czech Republic.Riloff, Ellen and Janyce Wiebe.
2003.
Learning Extrac-tion Patterns for Subjective Expressions.
In Proceedingsof the Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP-2003), pages 105?112, Sap-poro, Japan.Stone, Philip J, Marshall S Smith, Daniel M Ogilivie, andDexter C Dumphy.
1967.
The General Inquirer: A Com-puter Approach to Content Analysis.
/.
The MIT Press,1st edition.Wan, Xiaojun.
2009.
Co-Training for Cross-Lingual Senti-ment Classification.
In Proceedings of the 47th AnnualMeeting of the Association for Computational Linguis-tics and the 4th International Joint Conference on NaturalLanguage Processing of the Asian Federation of NaturalLanguage Processing (ACL-IJCNLP 2009), Singapore.Wiebe, Janyce and Rada Mihalcea.
2006.
Word Sense andSubjectivity.
In Proceedings of the joint conference ofthe International Committee on Computational Linguis-tics and the Association for Computational Linguistics(COLING-ACL-2006), Sydney, Australia.Wiebe, Janyce and Ellen Riloff.
2005.
Creating Subjec-tive and Objective Sentence Classifiers from UnannotatedTexts.
In Proceeding of CICLing-05, International Con-ference on Intelligent Text Processing and ComputationalLinguistics, pages 486?497, Mexico City, Mexico.Wiebe, Janyce, Theresa Wilson, and Claire Cardie.
2005.Annotating Expressions of Opinions and Emotions inLanguage.
Language Resources and Evaluation, 39(2-3):165?210.Wu, Yejun.
2008.
Classifying attitude by topic aspect forEnglish and Chinese document collections.Yu, Hong and Vasileios Hatzivassiloglou.
2003.
Towardsanswering opinion questions: Separating facts from opin-ions and identifying the polarity of opinion sentence.
InProceedings of the Conference on Empirical Methodsin Natural Language Processing (EMNLP-2003), pages129?136, Sapporo, Japan.36
