Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in NLP, pages 40?47,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsIdentifying non-referential it: a machine learning approach incorporatinglinguistically motivated patternsAdriane BoydDepartment of LinguisticsThe Ohio State University1712 Neil Ave.Columbus, OH 43210adriane@ling.osu.eduWhitney Gegg-Harrison & Donna ByronDepartment of Computer Science and EngineeringThe Ohio State University2015 Neil Ave.Columbus, OH 43210{geggharr,dbyron}@cse.osu.eduAbstractIn this paper, we present a machine learn-ing system for identifying non-referentialit.
Types of non-referential it are ex-amined to determine relevant linguisticpatterns.
The patterns are incorporatedas features in a machine learning systemwhich performs a binary classification ofit as referential or non-referential in aPOS-tagged corpus.
The selection of rel-evant, generalized patterns leads to a sig-nificant improvement in performance.1 IntroductionThe automatic classification of it as either referen-tial or non-referential is a topic that has been rel-atively ignored in the computational linguistics lit-erature, with only a handful of papers mentioningapproaches to the problem.
With the term ?non-referential it?, we mean to refer to those instancesof it which do not introduce a new referent.
In theprevious literature these have been called ?pleonas-tic?, ?expletive?, and ?non-anaphoric?.
It is impor-tant to be able to identify instances of non-referentialit to generate the correct semantic interpretation ofan utterance.
For example, one step of this task is toassociate pronouns with their referents.
In an auto-mated pronoun resolution system, it is useful to beable to skip over these instances of it rather than at-tempt an unnecessary search for a referent for them,The authors would like to thank the GE Foundation Facultyfor the Future grant for their support of this project.
We wouldalso like to thank Detmar Meurers and Erhard Hinrichs for theirhelpful advice and feedback.only to end up with inaccurate results.
The task ofidentifying non-referential it could be incorporatedinto a part-of-speech tagger or parser, or viewed asan initial step in semantic interpretation.We develop a linguistically-motivated classifi-cation for non-referential it which includes fourtypes of non-referential it: extrapositional, cleft,weather/condition/time/place, and idiomatic, eachof which will be discussed in more detail in Section2.
A subset of the BNC Sampler Corpus (Burnard,1995) was chosen for our task because of its ex-tended tagset and high tagging accuracy.
Non-referential it makes up a significant proportion of theoccurrences of it in our corpus, which contains a se-lection of written texts of various genres, approxi-mately one-third prose fiction, one-third newspapertext, and one-third other non-fiction.
In our corpus,there are 2337 instances of it, 646 of which are non-referential (28%).
It appears in over 10% of the sen-tences in our corpus.
The corpus is described in fur-ther detail in Section 3.Previous research on this topic is fairly lim-ited.
Paice and Husk (1987) introduces a rule-basedmethod for identifying non-referential it and Lappinand Leass (1994) and Denber (1998) describe rule-based components of their pronoun resolution sys-tems which identify non-referential it.
Evans (2001)describes a machine learning system which classi-fies it into seven types based on the type of referent.Their approaches are described in detail in Section4.
In Section 5 we describe our system which com-bines and extends elements of the systems developedby Paice and Husk (1987) and Evans (2001), and theresults are presented in Section 6.402 ClassificationThe first step is to create a classification system forall instances of it.
Though the goal is the binary clas-sification of it as referential or non-referential, anannotation scheme is used which gives more detailabout each instance of non-referential it, since theyoccur in a number of constructions.
The main typesof non-referential it are taken from the CambridgeGrammar of the English Language in the sectionon ?Special uses of it?, Section 2.5, Huddleston andPullum (2002).
Five main uses are outlined: extra-positional, cleft, weather/condition/time/place, id-iomatic, and predicative.
As noted in the CambridgeGrammar, predicative it seems to be more referen-tial that the other types of non-referential it.
Pred-icative it can typically be replaced with a demonstra-tive pronoun.
Consider the example: It is a drearyday.
It can be replaced with This with no change ingrammaticality and no significant change in mean-ing: This is a dreary day.
In contrast, replacing theother types of it with this results in nonsense, e.g.,*This seems that the king is displeased.For our purposes, if a particular it can be re-placed with a demonstrative pronoun and the result-ing sentence is still grammatical and has no signif-icant change in meaning, this it is referential andtherefore annotated as referential.
The demonstra-tive pronoun replacement test is not quite perfect(e.g., *This is a dreary day in Paris), but no suchinstances of predicative it were found in the corpusso predicative it is always classified as referential.This leaves four types of it, each of which are de-scribed in detail below.
The main examples for eachtype are taken from the corpus.
See Section 3 fordetails about the corpus.2.1 ExtrapositionalWhen an element of a sentence is extraposed, it isoften inserted as a placeholder in the original posi-tion of the now extraposed element.
Most often, itappears in the subject position, but it can also ap-pear as an object.
Example (1) lists a few instancesof extrapositional it from our corpus.
(1) a.
It has been confirmed this week that politi-cal parties will no longer get financial sub-sidies.b.
She also made it clear that Conductive Ed-ucation is not the only method.c.
You lead life, it seems to me, like some rit-ual that demands unerring performance.The extraposed element is typically a subordinateclause, and the type of clause depends on lexicalproperties of verbs and adjectives in the sentence,see (2).
(2) * It was difficult that X.It was difficult to X.
* It was clear to X.It was clear that X.As (1c) shows, extrapositional it can also appearas part of a truncated extrapositional phrase as a kindof parenthetical comment embedded in a sentence.2.2 CleftIt appears as the subject of it-cleft sentences.
Whenan it-cleft sentence is formed, the foregroundedphrase becomes the complement of the verb be andthe rest of sentence is backgrounded in a relativeclause.
The foregrounded phrase in a cleft sentencecan be a noun phrase, prepositional phrase, adjectivephrase, adverb phrase, non-finite clause, or contentclause.
(3) a.
It was the military district commanderwho stepped in to avoid bloodshed.
(nounphrase)b.
It is on this point that the views of theSACP and some Soviet policymakers di-vide.
(prepositional phrase)c. ?Tis glad I am to ?ear it, me lord.
(adjectivephrase)Additionally, the foregrounded phrase can some-times be fronted:(4) He it was who ushered in the new head of state.More context than the immediate sentence isneeded to accurately identify it-cleft sentences.First, clefts with a foregrounded noun phrase are am-biguous between cleft sentences (5a) and sentenceswhere the noun phrase and relative clause form aconstituent (5b).41(5) a.
A: I heard that the general stepped in toavoid bloodshed.B: No, it was the military district comman-der who stepped in.b.
A: Was that the general being interviewedon the news?B: No, it was the military district comman-der who stepped in to avoid bloodshed.Due to this ambiguity, we expect that it may bedifficult to classify clefts.
In addition, there are dif-ficulties because the relative clause does not alwaysappear in full.
In various situations the relative pro-noun can be omitted, the relative clause can be re-duced, or the relative clause can be omitted entirely.2.3 Weather/Condition/Time/PlaceIt appears as the subject of weather and otherrelated predicates involving condition, time, andplace/distance:(6) a.
It was snowing steadily outside.b.
It was about midnight.c.
It was no distance to Mutton House.d.
It was definitely not dark.2.4 IdiomaticIn idioms, it can appear as the subject, object, orobject of a preposition.
(7) a.
After three weeks it was my turn to go tothe delivery ward at Fulmer.b.
Cool it!c.
They have not had an easy time of it.2.5 General NotesNon-referential it is most often the subject of a sen-tence, but in extrapositional and idiomatic cases, itcan also be the object.
Idioms are the only caseswhere non-referential it is found as the object of apreposition.3 CorpusThe BNC Sampler Corpus (Burnard, 1995) was cho-sen for its extended tagset and high tagging accu-racy.
The C7 tagset used for this corpus has a uniqueProse fiction 32%Newspaper text 38%Other non-fiction 30%Table 1: Text types in our corpus# of Instances % of Inst.Extrapositional 477 20.4%Cleft 119 5.1%Weather 69 2.9%Idiomatic 46 2.0%Referential 1626 69.6%Total 2337 100%Table 2: Instances of it in our corpustag for it, which made the task of identifying all oc-currences of it very simple.
We chose a subset con-sisting of 350,000 tokens from written texts in vari-ety of genres.
The breakdown by text type can beseen in Table 1.The two lead authors independently annotatedeach occurence with one of the labels shown in Ta-ble 2 and then came to a joint decision on the fi-nal annotation.
The breakdown of the instances of itin our corpus is shown in Table 2.
There are 2337occurrences of it, 646 of which are non-referential(28%).
Ten percent of the corpus, taken from allsections, was set aside as test data.
The remainingsection, which contains 2100 instances of it, becameour training data.4 Previous ResearchPaice and Husk (1987) reports a simple rule-basedsystem that was used to identify non-referential it inthe technical section of the Lancaster-Oslo/BergenCorpus.
Because of the type of text, the distributionof types of non-referential it is somewhat limited, sothey only found it necessary to write rules to matchextrapositional and cleft it (although they do men-tion two idioms found in the corpus).
The corpuswas plain text, so their rules match words and punc-tuation directly.Their patterns find it as a left bracket and searchfor a right bracket related to the extrapositional andcleft grammatical patterns (to, that, etc.).
For theextrapositional instances, there are lists of wordswhich are matched in between it and the right42Accuracy 92%Precision 93%Recall 97%Table 3: Paice and Husk (1987): ResultsAccuracy 79%Precision 80%Recall 31%Table 4: Replicating Paice and Husk (1987)bracket.
The word lists are task-status words (STA-TUS), state-of-knowledge words (STATE), and a listof prepositions and related words (PREP), which isused to rule out right brackets that could potentiallybe objects of prepositions.
Patterns such as ?it STA-TUS to?
and ?it !PREP that?
were created.
Theleft bracket can be at most 27 words from the rightbracket and there can be either zero or two or morecommas or dashes between the left and right brack-ets.
Additionally, their system had a rule to matchparenthetical it: there is a match when it appears im-mediately following a comma and another commafollows within four words.
Their results, shown inTable 3, are impressive.We replicated their system and ran it on our test-ing data, see Table 4.
Given the differences in texttypes, it is not surprising that their system did notperform as well on our corpus.
The low recall seemsto show the limitations of fixed word lists, while thereasonably high precision shows that the simple pat-terns tend to be accurate in the cases where they ap-ply.Lappin and Leass (1994) and Denber (1998) men-tion integrating small sets of rules to match non-referential it into their larger pronoun resolution sys-tems.
Lappin and Leass use two words lists anda short set of rules.
One word list is modal adjec-tives (necessary, possible, likely, etc.)
and the otheris cognitive verbs (recommend, think, believe, etc.
).Their rules are as follows:It is Modaladj that SIt is Modaladj (for NP) to VPIt is Cogv-ed that SIt seems/appears/means/follows (that) SNP makes/finds it Modaladj (for NP) to VPAccuracy 71%Precision 73%Recall 69%Table 5: Evans (2001): Results, Binary Classifica-tionIt is time to VPIt is thanks to NP that STheir rules are mainly concerned with extraposi-tional it and they give no mention of cleft it.
Theygive no direct results for this component of theirsystem, so it is not possible to give a comparison.Denber (1998) includes a slightly revised and ex-tended version of Lappin and Leass?s system andadds in detection of weather/time it.
He suggestsusing WordNet to extend word lists.Evans (2001) begins by noting that a significantpercentage of instances of it do not have simplenominal referents and describes a system which usesa memory-based learning (MBL) algorithm to per-form a 7-way classification of it by type of refer-ent.
We consider two of his categories, pleonas-tic and stereotypic/idiomatic, to be non-referential.Evans created a corpus with texts from the BNCand SUSANNE corpora and chose to use a memory-based learning algorithm.
A memory-based learn-ing algorithm classifies new instances on the basis oftheir similarity to instances seen in the training data.Evans chose the k-nearest neighbor algorithm fromthe Tilburg Memory-Based Learner (TiMBL) pack-age (Daelemans et al, 2003) with approximately 35features relevant to the 7-way classification.
Al-though his system was created for the 7-way classi-fication task, he recognizes the importance of the bi-nary referential/non-referential distinction and givesthe results for the binary classification of pleonasticit, see Table 5.
His results for the classification ofidiomatic it (33% precision and 0.7% recall) showthe limitations of a machine learning system givensparse data.We replicated Evans?s system with a simplified setof features to perform the referential/non-referentialclassification of it.
We did not include features thatwould require chunking or features that seemed rel-evant only for distinguishing kinds of referential it.The following thirteen features are used:43Accuracy 76%Precision 57%Recall 60%Table 6: Replicating Evans (2001)1-8. four preceding and following POS tags9-10.
lemmas of the preceding and following verbs11.
lemma of the following adjective12.
presence of that following13.
presence of an immediately preceding preposi-tionUsing our training and testing data with the samealgorithm from TiMBL, we obtained results similarto Evans?s, shown in Table 6.
The slightly higheraccuracy is likely due to corpus differences or thereduced feature set which ignores features largelyrelevant to other types of it.Current state-of-the-art reference resolution sys-tems typically include filters for non-referentialnoun phrases.
An example of such a system is Ngand Cardie (2002), which shows the improvementin reference resolution when non-referential nounphrases are identified.
Results are not given for thespecific task of identifying non-referential it, so a di-rect comparison is not possible.5 MethodAs seen in the previous section, both rule-basedand machine learning methods have been shown tobe fairly effective at identifying non-referential it.Rule-based methods look for the grammatical pat-terns known to be associated with non-referential itbut are limited by fixed word lists; machine learningmethods can handle open classes of words, but areless able to generalize about the grammatical pat-terns associated with non-referential it from a smalltraining set.Evans?s memory-based learning system showed aslight integration of rules into the machine learningsystem by using features such as the presence of fol-lowing that.
Given the descriptions of types of non-referential it from Section 2, it is possible to createmore specific rules which detect the fixed grammat-ical patterns associated with non-referential it suchas it VERB that or it VERB ADJ to.
Many of thesepatterns are similar to Paice and Husk?s, but hav-ing part-of-speech tags allows us to create more gen-eral rules without reference to specific lexical items.If the results of these rule matches are integratedas features in the training data for a memory-basedlearning system along with relevant verb and ad-jective lemmas, it becomes possible to incorporateknowledge about grammatical patterns without cre-ating fixed word lists.
The following sections exam-ine each type of non-referential it and describe thepatterns and features that can be used to help auto-matically identify each type.5.1 Extrapositional itExtrapositional it appears in a number of fairly fixedpatterns, nine of which are shown below.
Interven-ing tokens are allowed between the words in the pat-terns.
F4-6 are more general versions of F1-3 butare not as indicative of non-referential it, so it usefulto keep them separate even though ones that matchF1-3 will also match F4-6.
F7 applies when it is theobject of a verb.
To simplify patterns like F8, allverbs in the sentence are lemmatized with morpha(Minnen et al, 2001) before the pattern matchingbegins.F1 it VERB ADJ thatF2 it VERB ADJwhat/which/where/whether/why/howF3 it VERB ADJ toF4 it VERB thatF5 it VERB what/which/where/whether/why/howF6 it VERB toF7 it ADJ that/toF8 it be/seem as ifF9 it VERB COMMAFor each item above, the feature consists of thedistance (number of tokens) between it and the endof the match (the right bracket such that or to).By using the distance as the feature, it is possibleto avoid specifying a cutoff point for the end of amatch.
The memory-based learning algorithm canadapt to the training data.
As discussed in Sec-tion 2.1, extraposition is often lexically triggered,so the specific verbs and adjectives in the sentenceare important for its classification.
For this reason,it is necessary to include information about the sur-rounding verbs and adjectives.
The nearby full verbs44(as opposed to auxiliary and modal verbs) are likelyto give the most information, so we add features forthe immediately preceding full verb (for F7), thefollowing full verb (for F1-F6), and the followingadjective (for F1-3,7).
The verbs were lemmatizedwith morpha and added as features along with thefollowing adjective.F10 lemma of immediately preceding full verbF11 lemma of following full verb within currentsentenceF12 following adjective within current sentence5.2 Cleft itTwo patterns are used for cleft it:F13 it be who/which/thatF14 it who/which/thatAs mentioned in the previous section, all verbs inthe sentence are lemmatized before matching.
Like-wise, these features are the distance between it andthe right bracket.
Feature F14 is used to match acleft it in a phrase with inverted word order.5.3 Weather/Condition/Time/Place itIdeally, the possible weather predicates could belearned automatically from the following verbs, ad-jectives, and nouns, but the list is so open that itis better in practice to specify a fixed list.
Theweather/time/place/condition predicates were takenfrom the training set and put into a fixed list.
Somegeneralizations were made (e.g., adding the namesof all months, weekdays, and seasons), but the listcontains mainly the words found in the training set.There are 46 words in the list.
As Denber men-tioned, WordNet could be used to extend this list.A feature is added for the distance to the nearestweather token.The following verb lemma feature (F10) addedfor extrapositional it is the lemma of the follow-ing full verb, but in many cases the verb followingweather it is the verb be, so we also added a binaryfeature for whether the following verb is be.F15 distance to nearest weather tokenF16 whether the following verb is be5.4 Idiomatic itIdioms can be identified by fixed patterns.
All verbsin the sentence are lemmatized and the followingpatterns, all found as idioms in our training data, areused:if/when it come to pull it offas it happen fall to itcall it a NOUN ask for iton the face of it be it not forhave it not been for like it or notShort idiom patterns such as ?cool it?
and ?watchit?
were found to overgeneralize, so only idioms in-cluding at least three words were used.
A binaryfeature was added for whether an idiom pattern wasmatched for the given instance of it (F17).
In addi-tion, two common fixed patterns were included as aseparate feature:it be ... timeit be ... my/X?s turnF17 whether an idiom pattern was matchedF18 whether an additional fixed pattern wasmatched5.5 Additional RestrictionsThere are a few additional restrictions on the patternmatches involving length and punctuation.
The firstrestriction is on the distance between the instanceof it and the right bracket (that, to, who, etc.).
Onthe basis of their corpus, Paice and Husk decidedthat the right bracket could be at most 27 wordsaway from it.
Instead of choosing a fixed distance,features based on pattern matches are the distance(number of tokens) between it and the right bracket.The system looks for a pattern match between itand the end of the sentence.
The end of a sentenceis considered to be punctuation matching any of thefollowing: .
; : ?
! )
] .
(Right parenthesis orbracket is only included if a matching left parenthe-sis or bracket has not been found before it.)
If thereis anything in paired parentheses in the remainder ofthe sentence, it is omitted.
Quotes are not consistentindicators of a break in a sentence, so they are ig-nored.
If the end of a sentence is not located within50 tokens, the sentence is truncated at that point andthe system looks for the patterns within those tokens.45As Paice and Husk noted, the presence of a sin-gle comma or dash between it and the right bracketis a good sign that the right bracket is not rele-vant to whether the instance of it is non-referential.When there are either zero or two or more commasor dashes it is difficult to come to any conclusionwithout more information.
Therefore, when the to-tal comma count or total dash count between it andthe right bracket is one, the pattern match is ignored.Additionally, unless it occurs in an idiom, it isalso never the object of a preposition, so there isan additional feature for whether it is preceded bya preposition.F19 whether the previous word is a prepositionFinally, the single preceding and five followingsimplified part-of-speech tags were also included.The part-of-speech tags were simplified to their firstcharacter in the C7 tagset, adverb (R) and nega-tive (X) words were ignored, and only the first in-stance in a sequence of tokens of the same simplifiedtype (e.g., the first of two consecutive verbs) was in-cluded in the set of following tags.F20-25 surrounding POS tags, simplified6 ResultsTraining and testing data were generated from ourcorpus using the the 25 features described in theprevious section.
Given Evans?s success and thelimited amount of training data, we chose to alsouse TiMBL?s k-nearest neighbor algorithm (IB1).In TiMBL, the distance metric can be calculatedin a number of ways for each feature.
The nu-meric features use the numeric metric and the re-maining features (lemmas, POS tags) use the de-fault overlap metric.
Best performance is achievedwith gain ratio weighting and the consideration of2 nearest distances (neighbors).
Because of overlapin the features for various types of non-referentialit and sparse data for cleft, weather, and idiomaticit, all types of non-referential it were considered atthe same time and the output was a binary classifi-cation of each instance of it as referential or non-referential.
The results for our TiMBL classifier(MBL) are shown in Table 7 alongside our resultsusing a decision tree algorithm (DT, described be-low) and the results from our replication of EvansOur MBLClassifierOur DTClassifierRepl.
ofEvansAccuracy 88% 81% 76%Precision 82% 82% 57%Recall 71% 42% 60%Table 7: ResultsExtrapositional 81%Cleft 45%Weather 57%Idiomatic 60%Referential 94%Table 8: Recall by Type for MBL Classifier(2001).
All three systems were trained and evalu-ated with the same data.All three systems perform a binary classifica-tion of each instance of it as referential or non-referential, but each instance of non-referential itwas additionally tagged for type, so the recall foreach type can be calculated.
The recall by type canbeen seen in Table 8 for our MBL system.
Given thatthe memory-based learning algorithm is using previ-ously seen instances to classify new ones, it makessense that the most frequent types have the highestrecall.
As mentioned in Section 2.2, clefts can bedifficult to identify.Decision tree algorithms seem suited to this kindof task and have been used previously, but C4.5(Quinlan, 1993) decision tree algorithm did not per-form as well as TiMBL on our data, compare theTiMBL results (MBL) with the C4.5 results (DT) inTable 7.
This may be because the verb and adjectivelemma features (F10-F12) had hundreds of possiblevalues and were not as useful in a decision tree as inthe memory-based learning algorithm.With the addition of more relevant, generalizedgrammatical patterns, the precision and accuracyhave increased significantly, but the same cannot besaid for recall.
Because many of the patterns aredesigned to match specific function words as theright bracket, cases where the right bracket is omit-ted (e.g., extraposed clauses with no overt comple-mentizers, truncated clefts, clefts with reduced rela-tive clauses) are difficult to match.
Other problem-atic cases include sentences with a lot of intervening46material between it and the right bracket or simpleidioms which cannot be easily differentiated.
Theresults for cleft, weather, and idiomatic it may alsobe due in part to sparse data.
When only 2% of theinstances of it are of a certain type, there are fewerthan one hundred training instances, and it can bedifficult for the memory-based learning method tobe very successful.7 ConclusionThe accurate classification of it as referential or non-referential is important for natural language taskssuch as reference resolution (Ng and Cardie, 2002).Through an examination of the types of construc-tions containing non-referential it, we are able to de-velop a set of detailed grammatical patterns associ-ated with non-referential it.
In previous rule-basedsystems, word lists were created for the verbs andadjectives which often occur in these patterns.
Sucha system can be limited because it is unable to adaptto new texts, but the basic grammatical patternsare still reasonably consistent indicators of non-referential it.
Given a POS-tagged corpus, the rele-vant linguistic patterns can be generalized over part-of-speech tags, reducing the dependence on brittleword lists.
A machine learning algorithm is ableto adapt to new texts and new words, but it is lessable to generalize about the linguistic patterns froma small training set.
To be able to use our knowl-edge of relevant linguistic patterns without having tospecify lists of words as indicators of certain typesof it, we developed a machine learning system whichincorporates the relevant patterns as features along-side part-of-speech and lexical information.
Twoshort lists are still used to help identify weather itand a few idioms.
The k-nearest neighbors algo-rithm from the Tilburg Memory Based Learner isused with 25 features and achieved 88% accuracy,82% precision, and 71% recall for the binary classi-fication of it as referential or non-referential.Our classifier outperforms previous systems inboth accuracy and precision, but recall is still a prob-lem.
Many instances of non-referential it are diffi-cult to identify because typical clues such as com-plementizers and relative pronouns can be omitted.Because of this, subordinate and relative clausescannot be consistently identified given only a POS-tagged corpus.
Improvements could be made in thefuture by integrating chunking or parsing into thepattern-matching features used in the system.
Thiswould help in identifying extrapositional and cleft it.Knowledge about context beyond the sentence levelwill be needed to accurately identify certain types ofcleft, weather, and idiomatic constructions.ReferencesL.
Burnard, 1995.
Users reference guide for the BritishNational Corpus.
Oxford.Walter Daelemans, Jakub Zavrel, Ko van der Sloot, andAntal van den Bosch.
2003.
TiMBL: Tilburg Mem-ory Based Learner, version 5.0, Reference Guide.
ILKTechnical Report 03-10.
Technical report.Michel Denber.
1998.
Automatic resolution of anaphorain English.
Technical report, Imaging Science Divi-son, Eastman Kodak Co.Richard Evans.
2001.
Applying machine learning to-ward an automatic classification of It.
Literary andLinguistic Computing, 16(1):45 ?
57.Rodney D. Huddleston and Geoffrey K. Pullum.
2002.The Cambridge Grammar of the English Language.Cambridge University Press, Cambridge.Shalom Lappin and Herbert J. Leass.
1994.
An Algo-rithm for Pronominal Anaphora Resolution.
Compu-tational Linguistics, 20(4):535?561.Guido Minnen, John Caroll, and Darren Pearce.
2001.Applied morphological processing of English.
Natu-ral Language Engineering, 7(3):207?223.Vincent Ng and Claire Cardie.
2002.
Identifyinganaphoric and non-anaphoric noun phrases to improvecoreference resolution.
Proceedings of the 19th In-ternational Conference on Computational Linguistics(COLING-2002).C.
D. Paice and G. D. Husk.
1987.
Towards an automaticrecognition of anaphoric features in English text; theimpersonal pronoun ?it?.
Computer Speech and Lan-guage, 2:109 ?
132.J.
Ross Quinlan.
1993.
C4.5: Programs for MachineLearning.
Morgan Kaufmann.47
