Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 13?18,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsMeasuring the Semantic Similarity of TextsCourtney Corley and Rada MihalceaDepartment of Computer ScienceUniversity of North Texas{corley,rada}@cs.unt.eduAbstractThis paper presents a knowledge-basedmethod for measuring the semantic-similarity of texts.
While there is a largebody of previous work focused on find-ing the semantic similarity of conceptsand words, the application of these word-oriented methods to text similarity has notbeen yet explored.
In this paper, we in-troduce a method that combines word-to-word similarity metrics into a text-to-text metric, and we show that this methodoutperforms the traditional text similaritymetrics based on lexical matching.1 IntroductionMeasures of text similarity have been used for along time in applications in natural language pro-cessing and related areas.
One of the earliest ap-plications of text similarity is perhaps the vectorialmodel in information retrieval, where the documentmost relevant to an input query is determined byranking documents in a collection in reversed or-der of their similarity to the given query (Salton andLesk, 1971).
Text similarity has been also used forrelevance feedback and text classification (Rocchio,1971), word sense disambiguation (Lesk, 1986), andmore recently for extractive summarization (Saltonet al, 1997b), and methods for automatic evaluationof machine translation (Papineni et al, 2002) or textsummarization (Lin and Hovy, 2003).The typical approach to finding the similarity be-tween two text segments is to use a simple lexicalmatching method, and produce a similarity scorebased on the number of lexical units that occur inboth input segments.
Improvements to this simplemethod have considered stemming, stop-word re-moval, part-of-speech tagging, longest subsequencematching, as well as various weighting and normal-ization factors (Salton et al, 1997a).
While success-ful to a certain degree, these lexical matching simi-larity methods fail to identify the semantic similarityof texts.
For instance, there is an obvious similaritybetween the text segments I own a dog and I havean animal, but most of the current text similaritymetrics will fail in identifying any kind of connec-tion between these texts.
The only exception to thistrend is perhaps the latent semantic analysis (LSA)method (Landauer et al, 1998), which representsan improvement over earlier attempts to use mea-sures of semantic similarity for information retrieval(Voorhees, 1993), (Xu and Croft, 1996).
LSA aimsto find similar terms in large text collections, andmeasure similarity between texts by including theseadditional related words.
However, to date LSA hasnot been used on a large scale, due to the complex-ity and computational cost associated with the algo-rithm, and perhaps also due to the ?black-box?
ef-fect that does not allow for any deep insights intowhy some terms are selected as similar during thesingular value decomposition process.In this paper, we explore a knowledge-basedmethod for measuring the semantic similarity oftexts.
While there are several methods previ-ously proposed for finding the semantic similar-ity of words, to our knowledge the application ofthese word-oriented methods to text similarity hasnot been yet explored.
We introduce an algorithm13that combines the word-to-word similarity metricsinto a text-to-text semantic similarity metric, and weshow that this method outperforms the simpler lex-ical matching similarity approach, as measured in aparaphrase identification application.2 Measuring Text Semantic SimilarityGiven two input text segments, we want to auto-matically derive a score that indicates their similar-ity at semantic level, thus going beyond the simplelexical matching methods traditionally used for thistask.
Although we acknowledge the fact that a com-prehensive metric of text semantic similarity shouldtake into account the relations between words, aswell as the role played by the various entities in-volved in the interactions described by each of thetwo texts, we take a first rough cut at this problemand attempt to model the semantic similarity of textsas a function of the semantic similarity of the com-ponent words.
We do this by combining metrics ofword-to-word similarity and language models intoa formula that is a potentially good indicator of thesemantic similarity of the two input texts.2.1 Semantic Similarity of WordsThere is a relatively large number of word-to-wordsimilarity metrics that were previously proposed inthe literature, ranging from distance-oriented mea-sures computed on semantic networks, to metricsbased on models of distributional similarity learnedfrom large text collections.
From these, we chose tofocus our attention on six different metrics, selectedmainly for their observed performance in naturallanguage processing applications, e.g.
malapropismdetection (Budanitsky and Hirst, 2001) and wordsense disambiguation (Patwardhan et al, 2003), andfor their relatively high computational efficiency.We conduct our evaluation using the followingword similarity metrics: Leacock & Chodorow,Lesk, Wu & Palmer, Resnik, Lin, and Jiang & Con-rath.
Note that all these metrics are defined be-tween concepts, rather than words, but they can beeasily turned into a word-to-word similarity metricby selecting for any given pair of words those twomeanings that lead to the highest concept-to-conceptsimilarity.
We use the WordNet-based implemen-tation of these metrics, as available in the Word-Net::Similarity package (Patwardhan et al, 2003).We provide below a short description for each ofthese six metrics.The Leacock & Chodorow (Leacock andChodorow, 1998) similarity is determined as:Simlch = ?
loglength2 ?
D (1)where length is the length of the shortest path be-tween two concepts using node-counting, and D isthe maximum depth of the taxonomy.The Lesk similarity of two concepts is defined as afunction of the overlap between the correspondingdefinitions, as provided by a dictionary.
It is basedon an algorithm proposed in (Lesk, 1986) as a solu-tion for word sense disambiguation.The Wu and Palmer (Wu and Palmer, 1994) simi-larity metric measures the depth of the two conceptsin the WordNet taxonomy, and the depth of the leastcommon subsumer (LCS), and combines these fig-ures into a similarity score:Simwup =2 ?
depth(LCS)depth(concept1) + depth(concept2)(2)The measure introduced by Resnik (Resnik, 1995)returns the information content (IC) of the LCS oftwo concepts:Simres = IC(LCS) (3)where IC is defined as:IC(c) = ?
log P (c) (4)and P (c) is the probability of encountering an in-stance of concept c in a large corpus.The next measure we use in our experiments is themetric introduced by Lin (Lin, 1998), which buildson Resnik?s measure of similarity, and adds a nor-malization factor consisting of the information con-tent of the two input concepts:Simlin =2 ?
IC(LCS)IC(concept1) + IC(concept2)(5)Finally, the last similarity metric we consider isJiang & Conrath (Jiang and Conrath, 1997), whichreturns a score determined by:Simjnc =1IC(concept1) + IC(concept2) ?
2 ?
IC(LCS)(6)142.2 Language ModelsIn addition to the semantic similarity of words, wealso want to take into account the specificity ofwords, so that we can give a higher weight to a se-mantic matching identified between two very spe-cific words (e.g.
collie and sheepdog), and give lessimportance to the similarity score measured betweengeneric concepts (e.g.
go and be).
While the speci-ficity of words is already measured to some extentby their depth in the semantic hierarchy, we are re-inforcing this factor with a corpus-based measure ofword specificity, based on distributional informationlearned from large corpora.Language models are frequently used in naturallanguage processing applications to account for thedistribution of words in language.
While word fre-quency does not always constitute a good measure ofword importance, the distribution of words across anentire collection can be a good indicator of the speci-ficity of the words.
Terms that occur in a few docu-ments with high frequency contain a greater amountof discriminatory ability, while terms that occur innumerous documents across a collection with a highfrequency have inherently less meaning to a docu-ment.
We determine the specificity of a word us-ing the inverse document frequency introduced in(Sparck-Jones, 1972), which is defined as the totalnumber of documents in the corpus, divided by thetotal number of documents that include that word.In the experiments reported in this paper, we use theBritish National Corpus to derive the document fre-quency counts, but other corpora could be used tothe same effect.2.3 Semantic Similarity of TextsProvided a measure of semantic similarity betweenwords, and an indication of the word specificity, wecombine them into a measure of text semantic sim-ilarity, by pairing up those words that are found tobe most similar to each other, and weighting theirsimilarity with the corresponding specificity score.We define a directional measure of similarity,which indicates the semantic similarity of a text seg-ment Ti with respect to a text segment Tj .
This def-inition provides us with the flexibility we need tohandle applications where the directional knowledgeis useful (e.g.
entailment), and at the same time itgives us the means to handle bidirectional similaritythrough a simple combination of two unidirectionalmetrics.For a given pair of text segments, we start by cre-ating sets of open-class words, with a separate setcreated for nouns, verbs, adjectives, and adverbs.In addition, we also create a set for cardinals, sincenumbers can also play an important role in the un-derstanding of a text.
Next, we try to determine pairsof similar words across the sets corresponding to thesame open-class in the two text segments.
For nounsand verbs, we use a measure of semantic similaritybased on WordNet, while for the other word classeswe apply lexical matching1.For each noun (verb) in the set of nouns (verbs)belonging to one of the text segments, we try to iden-tify the noun (verb) in the other text segment that hasthe highest semantic similarity (maxSim), accord-ing to one of the six measures of similarity describedin Section 2.1.
If this similarity measure results in ascore greater than 0, then the word is added to the setof similar words for the corresponding word classWSpos2.
The remaining word classes: adjectives,adverbs, and cardinals, are checked for lexical sim-ilarity with their counter-parts and included in thecorresponding word class set if a match is found.The similarity between the input text segments Tiand Tj is then determined using a scoring functionthat combines the word-to-word similarities and theword specificity:sim(Ti, Tj)Ti =?pos(?wk?
{WSpos}(maxSim(wk) ?
idfwk ))?wk?
{Tipos}idfwk(7)This score, which has a value between 0 and 1, isa measure of the directional similarity, in this casecomputed with respect to Ti.
The scores from bothdirections can be combined into a bidirectional sim-ilarity using a simple average function:sim(Ti, Tj) =sim(Ti, Tj)Ti + sim(Ti, Tj)Tj2 (8)1The reason behind this decision is the fact that most of thesemantic similarity measures apply only to nouns and verbs, andthere are only one or two relatedness metrics that can be appliedto adjectives and adverbs.2All similarity scores have a value between 0 and 1.
Thesimilarity threshold can be also set to a value larger than 0,which would result in tighter measures of similarity.15Text Segment 1: The jurors were taken into the courtroom ingroups of 40 and asked to fill out a questionnaire.?
SetNN = {juror, courtroom, group, questionnaire}SetV B = {be, take, ask, fill}SetRB = {out}SetCD = {40}Text Segment 2: About 120 potential jurors were being askedto complete a lengthy questionnaire.?
SetNN = {juror, questionnaire}SetV B = {be, ask, complete}SetJJ = {potential, lengthy}SetCD = {120}Figure 1: Two text segments and their correspondingword class sets3 A Walk-Through ExampleWe illustrate the application of the text similaritymeasure with an example.
Given two text segments,as shown in Figure 1, we want to determine a scorethat reflects their semantic similarity.
For illustrationpurposes, we restrict our attention to one measure ofword-to-word similarity, the Wu & Palmer metric.First, the text segments are tokenized, part-of-speech tagged, and the words are inserted into theircorresponding word class sets.
The sets obtained forthe given text segments are illustrated in Figure 1.Starting with each of the two text segments, andfor each word in its word class sets, we determinethe most similar word from the corresponding set inthe other text segment.
As mentioned earlier, weseek a WordNet-based semantic similarity for nounsand verbs, and only lexical matching for adjectives,adverbs, and cardinals.
The word semantic similar-ity scores computed starting with the first text seg-ment are shown in Table 3.Text 1 Text 2 maxSim IDFjurors jurors 1.00 5.80courtroom jurors 0.30 5.23questionnaire questionnaire 1.00 3.57groups questionnaire 0.29 0.85were were 1.00 0.09taken asked 1.00 0.28asked asked 1.00 0.45fill complete 0.86 1.29out ?
0 0.0640 ?
0 1.39Table 1: Wu & Palmer word similarity scores forcomputing text similarity with respect to text 1Next, we use equation 7 and determine the seman-tic similarity of the two text segments with respectto text 1 as 0.6702, and with respect to text 2 as0.7202.
Finally, the two figures are combined intoa bidirectional measure of similarity, calculated as0.6952 based on equation 8.Although there are a few words that occur in bothtext segments (e.g.
juror, questionnaire), there arealso words that are not identical, but closely related,e.g.
courtroom found similar to juror, or fill whichis related to complete.
Unlike traditional similar-ity measures based on lexical matching, our metrictakes into account the semantic similarity of thesewords, resulting in a more precise measure of textsimilarity.4 EvaluationTo test the effectiveness of the text semantic simi-larity metric, we use this measure to automaticallyidentify if two text segments are paraphrases ofeach other.
We use the Microsoft paraphrase cor-pus (Dolan et al, 2004), consisting of 4,076 trainingpairs and 1,725 test pairs, and determine the numberof correctly identified paraphrase pairs in the cor-pus using the text semantic similarity measure as theonly indicator of paraphrasing.
In addition, we alsoevaluate the measure using the PASCAL corpus (Da-gan et al, 2005), consisting of 1,380 test?hypothesispairs with a directional entailment (580 developmentpairs and 800 test pairs).For each of the two data sets, we conduct twoevaluations, under two different settings: (1) An un-supervised setting, where the decision on what con-stitutes a paraphrase (entailment) is made using aconstant similarity threshold of 0.5 across all exper-iments; and (2) A supervised setting, where the op-timal threshold and weights associated with varioussimilarity metrics are determined through learningon training data.
In this case, we use a voted percep-tron algorithm (Freund and Schapire, 1998)3.We evaluate the text similarity metric built on topof the various word-to-word metrics introduced inSection 2.1.
For comparison, we also compute threebaselines: (1) A random baseline created by ran-domly choosing a true or false value for each textpair; (2) A lexical matching baseline, which only3Classification using this algorithm was determined optimalempirically through experiments.16counts the number of matching words between thetwo text segments, while still applying the weightingand normalization factors from equation 7; and (3)A vectorial similarity baseline, using a cosine sim-ilarity measure as traditionally used in informationretrieval, with tf.idf term weighting.
For compari-son, we also evaluated the corpus-based similarityobtained through LSA; however, the results obtainedwere below the lexical matching baseline and are notreported here.For paraphrase identification, we use the bidirec-tional similarity measure, and determine the sim-ilarity with respect to each of the two text seg-ments in turn, and then combine them into a bidi-rectional similarity metric.
For entailment identifi-cation, since this is a directional relation, we onlymeasure the semantic similarity with respect to thehypothesis (the text that is entailed).We evaluate the results in terms of accuracy, rep-resenting the number of correctly identified true orfalse classifications in the test data set.
We also mea-sure precision, recall and F-measure, calculated withrespect to the true values in each of the test data sets.Tables 2 and 3 show the results obtained in theunsupervised setting, when a text semantic similar-ity larger than 0.5 was considered to be an indica-tor of paraphrasing (entailment).
We also evaluate ametric that combines all the similarity measures us-ing a simple average, with results indicated in theCombined row.The results obtained in the supervised setting areshown in Tables 4 and 5.
The optimal combinationof similarity metrics and optimal threshold are nowdetermined in a learning process performed on thetraining set.
Under this setting, we also compute anadditional baseline, consisting of the most frequentlabel, as determined from the training data.5 Discussion and ConclusionsFor the task of paraphrase recognition, incorporatingsemantic information into the text similarity mea-sure increases the likelihood of recognition signifi-cantly over the random baseline and over the lexi-cal matching baseline.
In the unsupervised setting,the best performance is achieved using a method thatcombines several similarity metrics into one, for anoverall accuracy of 68.8%.
When learning is used tofind the optimal combination of metrics and optimalthreshold, the highest accuracy of 71.5% is obtainedMetric Acc.
Prec.
Rec.
FSemantic similarity (knowledge-based)J & C 0.683 0.724 0.846 0.780L & C 0.680 0.724 0.838 0.777Lesk 0.680 0.724 0.838 0.777Lin 0.679 0.717 0.855 0.780W & P 0.674 0.722 0.831 0.773Resnik 0.672 0.725 0.815 0.768Combined 0.688 0.741 0.817 0.777BaselinesLexMatch 0.661 0.722 0.798 0.758Vectorial 0.654 0.716 0.795 0.753Random 0.513 0.683 0.500 0.578Table 2: Text semantic similarity for paraphraseidentification (unsupervised)Metric Acc.
Prec.
Rec.
FSemantic similarity (knowledge-based)J & C 0.573 0.543 0.908 0.680L & C 0.569 0.543 0.870 0.669Lesk 0.568 0.542 0.875 0.669Resnik 0.565 0.541 0.850 0.662Lin 0.563 0.538 0.878 0.667W & P 0.558 0.534 0.895 0.669Combined 0.583 0.561 0.755 0.644BaselinesLexMatch 0.545 0.530 0.795 0.636Vectorial 0.528 0.525 0.588 0.555Random 0.486 0.486 0.493 0.489Table 3: Text semantic similarity for entailmentidentification (unsupervised)by combining the similarity metrics and the lexicalmatching baseline together.For the entailment data set, although we do notexplicitly check for entailment, the directional sim-ilarity computed for textual entailment recognitiondoes improve over the random and lexical matchingbaselines.
Once again, the combination of similar-ity metrics gives the highest accuracy, measured at58.3%, with a slight improvement observed in thesupervised setting, where the highest accuracy wasmeasured at 58.9%.
Both these figures are compet-itive with the best results achieved during the PAS-CAL entailment evaluation (Dagan et al, 2005).Although our method relies on a bag-of-words ap-proach, as it turns out the use of measures of seman-tic similarity improves significantly over the tradi-tional lexical matching metrics4.
We are nonetheless4The improvement of the combined semantic similarity met-ric over the simpler lexical matching measure was found to bestatistically significant in all experiments, using a paired t-test(p < 0.001).17Metric Acc.
Prec.
Rec.
FSemantic similarity (knowledge-based)Lin 0.702 0.706 0.947 0.809W & P 0.699 0.705 0.941 0.806L & C 0.699 0.708 0.931 0.804J & C 0.699 0.707 0.935 0.805Lesk 0.695 0.702 0.929 0.800Resnik 0.692 0.705 0.921 0.799Combined 0.715 0.723 0.925 0.812BaselinesLexMatch 0.671 0.693 0.908 0.786Vectorial 0.665 0.665 1.000 0.799Most frequent 0.665 0.665 1.000 0.799Table 4: Text semantic similarity for paraphraseidentification (supervised)Metric Acc.
Prec.
Rec.
FSemantic similarity (knowledge-based)L & C 0.583 0.573 0.650 0.609W & P 0.580 0.570 0.648 0.607Resnik 0.579 0.572 0.628 0.598Lin 0.574 0.568 0.620 0.593J & C 0.575 0.566 0.643 0.602Lesk 0.573 0.566 0.633 0.597Combined 0.589 0.579 0.650 0.612BaselinesLexMatch 0.568 0.573 0.530 0.551Most frequent 0.500 0.500 1.000 0.667Vectorial 0.479 0.484 0.645 0.553Table 5: Text semantic similarity for entailmentidentification (supervised)aware that a bag-of-words approach ignores many ofimportant relationships in sentence structure, such asdependencies between words, or roles played by thevarious arguments in the sentence.
Future work willconsider the investigation of more sophisticated rep-resentations of sentence structure, such as first orderpredicate logic or semantic parse trees, which shouldallow for the implementation of more effective mea-sures of text semantic similarity.ReferencesA.
Budanitsky and G. Hirst.
2001.
Semantic distance in word-net: An experimental, application-oriented evaluation of fivemeasures.
In Proceedings of the NAACL Workshop on Word-Net and Other Lexical Resources, Pittsburgh, June.I.
Dagan, O. Glickman, and B. Magnini.
2005.
The PASCALrecognising textual entailment challenge.
In Proceedings ofthe PASCAL Workshop.W.B.
Dolan, C. Quirk, and C. Brockett.
2004.
Unsuper-vised construction of large paraphrase corpora: Exploitingmassively parallel news sources.
In Proceedings of the20th International Conference on Computational Linguis-tics, Geneva, Switzerland.Y.
Freund and R.E.
Schapire.
1998.
Large margin classifica-tion using the perceptron algorithm.
In Proceedings of the11th Annual Conference on Computational Learning The-ory, pages 209?217, New York, NY.
ACM Press.J.
Jiang and D. Conrath.
1997.
Semantic similarity based oncorpus statistics and lexical taxonomy.
In Proceedings ofthe International Conference on Research in ComputationalLinguistics, Taiwan.T.
K. Landauer, P. Foltz, and D. Laham.
1998.
Introduction tolatent semantic analysis.
Discourse Processes, 25.C.
Leacock and M. Chodorow.
1998.
Combining local contextand WordNet sense similiarity for word sense disambigua-tion.
In WordNet, An Electronic Lexical Database.
The MITPress.M.E.
Lesk.
1986.
Automatic sense disambiguation using ma-chine readable dictionaries: How to tell a pine cone from anice cream cone.
In Proceedings of the SIGDOC Conference1986, Toronto, June.C.Y.
Lin and E.H. Hovy.
2003.
Automatic evaluation ofsummaries using n-gram co-occurrence statistics.
In Pro-ceedings of Human Language Technology Conference (HLT-NAACL 2003), Edmonton, Canada, May.D.
Lin.
1998.
An information-theoretic definition of similar-ity.
In Proceedings of the 15th International Conference onMachine Learning, Madison, WI.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.
Bleu:a method for automatic evaluation of machine translation.In Proceedings of the 40th Annual Meeting of the Associ-ation for Computational Linguistics (ACL 2002), Philadel-phia, PA, July.S.
Patwardhan, S. Banerjee, and T. Pedersen.
2003.
Using mea-sures of semantic relatedness for word sense disambiguation.In Proceedings of the Fourth International Conference onIntelligent Text Processing and Computational Linguistics,Mexico City, February.P.
Resnik.
1995.
Using information content to evaluate seman-tic similarity.
In Proceedings of the 14th International JointConference on Artificial Intelligence, Montreal, Canada.J.
Rocchio, 1971.
Relevance feedback in information retrieval.Prentice Hall, Ing.
Englewood Cliffs, New Jersey.G.
Salton and M.E.
Lesk, 1971.
Computer evaluation of index-ing and text processing, pages 143?180.
Prentice Hall, Ing.Englewood Cliffs, New Jersey.G.
Salton, , and A. Bukley.
1997a.
Term weighting approachesin automatic text retrieval.
In Readings in Information Re-trieval.
Morgan Kaufmann Publishers, San Francisco, CA.G.
Salton, A. Singhal, M. Mitra, and C. Buckley.
1997b.
Auto-matic text structuring and summarization.
Information Pro-cessing and Management, 2(32).K.
Sparck-Jones.
1972.
A statistical interpretation of termspecificity and its applicatoin in retrieval.
Journal of Doc-umentation, 28(1):11?21.E.
Voorhees.
1993.
Using wordnet to disambiguate wordsenses for text retrieval.
In Proceedings of the 16th annualinternational ACM SIGIR conference, Pittsburgh, PA.Z.
Wu and M. Palmer.
1994.
Verb semantics and lexical se-lection.
In Proceedings of the 32nd Annual Meeting of theAssociation for Computational Linguistics, Las Cruces, NewMexico.J.
Xu and W. B. Croft.
1996.
Query expansion using local andglobal document analysis.
In Proceedings of the 19th annualinternational ACM SIGIR conference, Zurich, Switzerland.18
