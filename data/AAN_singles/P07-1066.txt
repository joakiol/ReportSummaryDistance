Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 520?527,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsBilingual-LSA Based LM Adaptation for Spoken Language TranslationYik-Cheung Tam and Ian Lane and Tanja SchultzInterACT, Language Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213{yct,ian.lane,tanja}@cs.cmu.eduAbstractWe propose a novel approach to crosslinguallanguage model (LM) adaptation based onbilingual Latent Semantic Analysis (bLSA).A bLSA model is introduced which enableslatent topic distributions to be efficientlytransferred across languages by enforcinga one-to-one topic correspondence duringtraining.
Using the proposed bLSA frame-work crosslingual LM adaptation can be per-formed by, first, inferring the topic poste-rior distribution of the source text and thenapplying the inferred distribution to the tar-get language N-gram LM via marginal adap-tation.
The proposed framework also en-ables rapid bootstrapping of LSA modelsfor new languages based on a source LSAmodel from another language.
On Chineseto English speech and text translation theproposed bLSA framework successfully re-duced word perplexity of the English LM byover 27% for a unigram LM and up to 13.6%for a 4-gram LM.
Furthermore, the pro-posed approach consistently improved ma-chine translation quality on both speech andtext based adaptation.1 IntroductionLanguage model adaptation is crucial to numerousspeech and translation tasks as it enables higher-level contextual information to be effectively incor-porated into a background LM improving recogni-tion or translation performance.
One approach isto employ Latent Semantic Analysis (LSA) to cap-ture in-domain word unigram distributions whichare then integrated into the background N-gramLM.
This approach has been successfully appliedin automatic speech recognition (ASR) (Tam andSchultz, 2006) using the Latent Dirichlet Alloca-tion (LDA) (Blei et al, 2003).
The LDA model canbe viewed as a Bayesian topic mixture model withthe topic mixture weights drawn from a Dirichletdistribution.
For LM adaptation, the topic mixtureweights are estimated based on in-domain adapta-tion text (e.g.
ASR hypotheses).
The adapted mix-ture weights are then used to interpolate a topic-dependent unigram LM, which is finally integratedinto the background N-gram LM using marginaladaptation (Kneser et al, 1997)In this paper, we propose a framework to per-form LM adaptation across languages, enabling theadaptation of a LM from one language based on theadaptation text of another language.
In statisticalmachine translation (SMT), one approach is to ap-ply LM adaptation on the target language based onan initial translation of input references (Kim andKhudanpur, 2003; Paulik et al, 2005).
This schemeis limited by the coverage of the translation model,and overall by the quality of translation.
Since thisapproach only allows to apply LM adaptation af-ter translation, available knowledge cannot be ap-plied to extend the coverage.
We propose a bilingualLSA model (bLSA) for crosslingual LM adaptationthat can be applied before translation.
The bLSAmodel consists of two LSA models: one for eachside of the language trained on parallel documentcorpora.
The key property of the bLSA model is that520the latent topic of the source and target LSA mod-els can be assumed to be a one-to-one correspon-dence and thus share a common latent topic spacesince the training corpora consist of bilingual paral-lel data.
For instance, say topic 10 of the ChineseLSA model is about politics.
Then topic 10 of theEnglish LSA model is set to also correspond to pol-itics and so forth.
During LM adaptation, we firstinfer the topic mixture weights from the source textusing the source LSA model.
Then we transfer theinferred mixture weights to the target LSA modeland thus obtain the target LSA marginals.
The chal-lenge is to enforce the one-to-one topic correspon-dence.
Our proposal is to share common variationalDirichlet posteriors over the topic mixture weightsof a document pair in the LDA-style model.
Thebeauty of the bLSA framework is that the modelsearches for a common latent topic space in an un-supervised fashion, rather than to require manual in-teraction.
Since the topic space is language indepen-dent, our approach supports topic transfer in multi-ple language pairs in O(N) where N is the number oflanguages.Related work includes the Bilingual Topic Ad-mixture Model (BiTAM) for word alignment pro-posed by (Zhao and Xing, 2006).
Basically, theBiTAM model consists of topic-dependent transla-tion lexicons modeling Pr(c|e, k) where c, e andk denotes the source Chinese word, target Englishword and the topic index respectively.
On theother hand, the bLSA framework models Pr(c|k)and Pr(e|k) which is different from the BiTAMmodel.
By their different modeling nature, the bLSAmodel usually supports more topics than the BiTAMmodel.
Another work by (Kim and Khudanpur,2004) employed crosslingual LSA using singularvalue decomposition which concatenates bilingualdocuments into a single input supervector beforeprojection.We organize the paper as follows: In Section 2,we introduce the bLSA framework including La-tent Dirichlet-Tree Allocation (LDTA) (Tam andSchultz, 2007) as a correlated LSA model, bLSAtraining and crosslingual LM adaptation.
In Sec-tion 3, we present the effect of LM adaptation onword perplexity, followed by SMT experiments re-ported in BLEU on both speech and text input inSection 3.3.
Section 4 describes conclusions and fu-ASR hypoChinese LSA English LSAChinese N?gram LM English N?gram LMChinese ASR Chinese?>English SMTChinese?EnglishAdapt AdaptMT hypoTopic distributionParallel document corpusChinese text English textFigure 1: Topic transfer in bilingual LSA model.ture works.2 Bilingual Latent Semantic AnalysisThe goal of a bLSA model is to enforce a one-to-one topic correspondence between monolingualLSA models, each of which can be modeled usingan LDA-style model.
The role of the bLSA modelis to transfer the inferred latent topic distributionfrom the source language to the target language as-suming that the topic distributions on both sides areidentical.
The assumption is reasonable for paralleldocument pairs which are faithful translations.
Fig-ure 1 illustrates the idea of topic transfer betweenmonolingual LSA models followed by LM adapta-tion.
One observation is that the topic transfer can bebi-directional meaning that the ?flow?
of topic canbe from ASR to SMT or vice versa.
In this paper,we only focus on ASR-to-SMT direction.
Our tar-get is to minimize the word perplexity on the targetlanguage through LM adaptation.
Before we intro-duce the heuristic of enforcing a one-to-one topiccorrespondence, we describe the Latent Dirichlet-Tree Allocation (LDTA) for LSA.2.1 Latent Dirichlet-Tree AllocationThe LDTA model extends the LDA model in whichcorrelation among latent topics are captured using aDirichlet-Tree prior.
Figure 2 illustrates a depth-twoDirichlet-Tree.
A tree of depth one simply falls backto the LDA model.
The LDTA model is a generativemodel with the following generative process:1.
Sample a vector of branch probabilities bj ?521Dir(.)Dir(.)
Dir(.)Dir(.
)topic 1 topic 4Latent topics topic Kj=1j=2 j=3Figure 2: Dirichlet-Tree prior of depth two.Dir(?j) for each node j = 1...J where ?j de-notes the parameter (aka the pseudo-counts ofits outgoing branches) of the Dirichlet distribu-tion at node j.2.
Compute the topic proportions as:?k =?jcb?jc(k)jc (1)where ?jc(k) is an indicator function which setsto unity when the c-th branch of the j-th nodeleads to the leaf node of topic k and zero other-wise.
The k-th topic proportion ?k is computedas the product of branch probabilities from theroot node to the leaf node of topic k.3.
Generate a document using the topic multino-mial for each word wi:zi ?
Mult(?
)wi ?
Mult(?.zi)where ?.zi denotes the topic-dependent uni-gram LM indexed by zi.The joint distribution of the latent variables (topicsequence zn1 and the Dirichlet nodes over childbranches bj) and an observed document wn1 can bewritten as follows:p(wn1 , zn1 , bJ1 ) = p(bJ1 |{?j})n?i?wizi ?
?ziwhere p(bJ1 |{?j}) =J?jDir(bj;?j)?
?jcb?jc?1jcSimilar to LDA training, we apply the variationalBayes approach by optimizing the lower bound ofthe marginalized document likelihood:L(wn1 ; ?,?
)=Eq[logp(wn1 , zn1 , bJ1 ; ?
)q(zn1 , bJ1 ; ?
)]=Eq[log p(wn1 |zn1 )] + Eq[logp(zn1 |bJ1 )q(zn1 )]+Eq[logp(bJ1 ; {?j})q(bJ1 ; {?j})]where q(zn1 , bJ1 ; ?)
=?ni q(zi) ?
?Jj q(bj) is a fac-torizable variational posterior distribution over thelatent variables parameterized by ?
which are deter-mined in the E-step.
?
is the model parameters fora Dirichlet-Tree {?j} and the topic-dependent uni-gram LM {?wk}.
The LDTA model has an E-stepsimilar to the LDA model:E-Step:?jc = ?jc +n?iK?kqik ?
?jc(k) (2)qik ?
?wik ?
eEq[log ?k] (3)whereEq[log ?k] =?jc?jc(k)Eq[log bjc]=?jc?jc(k)(?(?jc)??
(?c?jc))where qik denotes q(zi = k) meaning the variationaltopic posterior of word wi.
Eqn 2 and Eqn 3 areexecuted iteratively until convergence is reached.M-Step:?wk ?n?iqik ?
?
(wi, w) (4)where ?
(wi, w) is a Kronecker Delta function.
Thealpha parameters can be estimated with iterativemethods such as Newton-Raphson or simple gradi-ent ascent procedure.2.2 Bilingual LSA trainingFor the following explanations, we assume that oursource and target languages are Chinese and En-glish respectively.
The bLSA model training is a522two-stage procedure.
At the first stage, we traina Chinese LSA model using the Chinese docu-ments in parallel corpora.
We applied the varia-tional EM algorithm (Eqn 2?4) to train a ChineseLSA model.
Then we used the model to computethe term eEq[log ?k] needed in Eqn 3 for each Chinesedocument in parallel corpora.
At the second stage,we apply the same eEq [log ?k] to bootstrap an EnglishLSA model, which is the key to enforce a one-to-onetopic correspondence.
Now the hyper-parameters ofthe variational Dirichlet posteriors of each node inthe Dirichlet-Tree are shared among the Chinese andEnglish model.
Precisely, we apply only Eqn 3 withfixed eEq [log ?k] in the E-step and Eqn 4 in the M-stepon {?wk} to bootstrap an English LSA model.
No-tice that the E-step is non-iterative resulting in rapidLSA training.
In short, given a monolingual LSAmodel, we can rapidly bootstrap LSA models of newlanguages using parallel document corpora.
Noticethat the English and Chinese vocabulary sizes do notneed to be similar.
In our setup, the Chinese vo-cabulary comes from the ASR system while the En-glish vocabulary comes from the English part of theparallel corpora.
Since the topic transfer can be bi-directional, we can perform the bLSA training in areverse manner, i.e.
training an English LSA modelfollowed by bootstrapping a Chinese LSA model.2.3 Crosslingual LM adaptationGiven a source text, we apply the E-step to estimatevariational Dirichlet posterior of each node in theDirichlet-Tree.
We estimate the topic weights on thesource language using the following equation:??
(CH)k ??jc(?jc?c?
?jc?
)?jc(k)(5)Then we apply the topic weights into the target LSAmodel to obtain an in-domain LSA marginals:PrEN (w) =K?k=1?
(EN)wk ?
??
(CH)k (6)We integrate the LSA marginal into the target back-ground LM using marginal adaptation (Kneser et al,1997) which minimizes the Kullback-Leibler diver-gence between the adapted LM and the backgroundLM:Pra(w|h) ?(Prldta(w)Prbg(w))??
Prbg(w|h) (7)Likewise, LM adaptation can take place on thesource language as well due to the bi-directional na-ture of the bLSA framework when target-side adap-tation text is available.
In this paper, we focus onLM adaptation on the target language for SMT.3 Experimental SetupWe evaluated our bLSA model using the Chinese?English parallel document corpora consisting of theXinhua news, Hong Kong news and Sina news.
Thecombined corpora contains 67k parallel documentswith 35M Chinese (CH) words and 43M English(EN) words.
Our spoken language translation sys-tem translates from Chinese to English.
The Chinesevocabulary comes from the ASR decoder while theEnglish vocabulary is derived from the English por-tion of the parallel training corpora.
The vocabularysizes for Chinese and English are 108k and 69k re-spectively.
Our background English LM is a 4-gramLM trained with the modified Kneser-Ney smooth-ing scheme using the SRILM toolkit on the sametraining text.
We explore the bLSA training in bothdirections: EN?CH and CH?EN meaning that anEnglish LSA model is trained first and a ChineseLSA model is bootstrapped or vice versa.
Exper-iments explore which bootstrapping direction yieldbest results measured in terms of English word per-plexity.
The number of latent topics is set to 200 anda balanced binary Dirichlet-Tree prior is used.With an increasing interest in the ASR-SMT cou-pling for spoken language translation, we also eval-uated our approach with Chinese ASR hypothesesand compared with Chinese manual transcriptions.We are interested to see the impact due to recog-nition errors on the ASR hypotheses compared tothe manual transcriptions.
We employed the CMU-InterACT ASR system developed for the GALE2006 evaluation.
We trained acoustic models withover 500 hours of quickly transcribed speech data re-leased by the GALE program and the LM with over800M-word Chinese corpora.
The character errorrates on the CCTV, RFA and NTDTV shows in theRT04 test set are 7.4%, 25.5% and 13.1% respec-tively.523Topic index Top words?CH-40?
flying, submarine, aircraft, air, pilot, land, mission, brand-new?EN-40?
air, sea, submarine, aircraft, flight, flying, ship, test?CH-41?
satellite, han-tian, launch, space, china, technology, astronomy?EN-41?
space, satellite, china, technology, satellites, science?CH-42?
fire, airport, services, marine, accident, air?EN-42?
fire, airport, services, department, marine, air, serviceTable 1: Parallel topics extracted by the bLSAmodel.
Top words on the Chinese side are translatedinto English for illustration purpose.-3.05e+08-3e+08-2.95e+08-2.9e+08-2.85e+08-2.8e+08-2.75e+08-2.7e+082  4  6  8  10  12  14  16  18  20Trainingloglikelihood# of training iterationsbootstrapped EN LSAmonolingual EN LSAFigure 3: Comparison of training log likelihood ofEnglish LSA models bootstrapped from a ChineseLSA and from a flat monolingual English LSA.3.1 Analysis of the bLSA modelBy examining the top-words of the extracted paral-lel topics, we verify the validity of the heuristic de-scribed in Section 2.2 which enforces a one-to-onetopic correspondence in the bLSA model.
Table 1shows the latent topics extracted by the CH?ENbLSA model.
We can see that the Chinese-Englishtopic words have strong correlations.
Many of themare actually translation pairs with similar word rank-ings.
From this viewpoint, we can interpret bLSA asa crosslingual word trigger model.
The result indi-cates that our heuristic is effective to extract parallellatent topics.
As a sanity check, we also examine thelikelihood of the training data when an English LSAmodel is bootstrapped.
We can see from Figure 3that the likelihood increases monotonically with thenumber of training iterations.
The figure also showsthat by sharing the variational Dirichlet posteriorsfrom the Chinese LSA model, we can bootstrap anEnglish LSA model rapidly compared to monolin-gual English LSA training with both training proce-dures started from the same flat model.LM (43M) CCTV RFA NTDTVBG EN unigram 1065 1220 1549+CH?EN (CH ref) 755 880 1113+EN?CH (CH ref) 762 896 1111+CH?EN (CH hypo) 757 885 1126+EN?CH (CH hypo) 766 896 1129+CH?EN (EN ref) 731 838 1075+EN?CH (EN ref) 747 848 1087Table 2: English word perplexity (PPL) on the RT04test set using a unigram LM.3.2 LM adaptation resultsWe trained the bLSA models on both CH?EN andEN?CH directions and compared their LM adapta-tion performance using the Chinese ASR hypothe-ses (hypo) and the manual transcriptions (ref) as in-put.
We adapted the English background LM usingthe LSA marginals described in Section 2.3 for eachshow on the test set.We first evaluated the English word perplexity us-ing the EN unigram LM generated by the bLSAmodel.
Table 2 shows that the bLSA-based LMadaptation reduces the word perplexity by over 27%relative compared to an unadapted EN unigram LM.The results indicate that the bLSA model success-fully leverages the text from the source language andimproves the word perplexity on the target language.We observe that there is almost no performance dif-ference when either the ASR hypotheses or the man-ual transcriptions are used for adaptation.
The resultis encouraging since the bLSA model may be in-sensitive to moderate recognition errors through theprojection of the input adaptation text into the latenttopic space.
We also apply an English translationreference for adaptation to show an oracle perfor-mance.
The results using the Chinese hypotheses arenot too far off from the oracle performance.
Anotherobservation is that the CH?EN bLSA model seemsto give better performance than the EN?CH bLSAmodel.
However, their differences are not signifi-cant.
The result may imply that the direction of thebLSA training is not important since the latent topicspace captured by either language is similar whenparallel training corpora are used.
Table 3 shows theword perplexity when the background 4-gram En-glish LM is adapted with the tuning parameter ?
set524LM (43M, ?
= 0.7) CCTV RFA NTDTVBG EN 4-gram 118 212 203+CH?EN (CH ref) 102 191 179+EN?CH (CH ref) 102 198 179+CH?EN (CH hypo) 102 193 180+EN?CH (CH hypo) 103 198 180+CH?EN (EN ref) 100 186 176+EN?CH (EN ref) 101 190 176Table 3: English word perplexity (PPL) on the RT04test set using a 4-gram LM.1001051101151201250.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1EnglishWordPerplexityBetaCCTV (CER=7.4%)BG 4-gram+bLSA (CH reference)+bLSA (CH ASR hypo)+bLSA (EN reference)Figure 4: Word perplexity with different ?
usingmanual reference or ASR hypotheses on CCTV.to 0.7.
Figure 4 shows the change of perplexity withdifferent ?.
We see that the adaptation performanceusing the ASR hypotheses or the manual transcrip-tions are almost identical on different ?
with an op-timal value at around 0.7.
The results show that theproposed approach successfully reduces the perplex-ity in the range of 9?13.6% relative compared to anunadapted baseline on different shows when ASRhypotheses are used.
Moreover, we observe simi-lar performance using ASR hypotheses or manualChinese transcriptions which is consistent with theresults on Table 2.
On the other hand, it is interest-ing to see that the performance gap from the oracleadaptation is somewhat related to the degree of mis-match between the test show and the training condi-tion.
The gap looks wider on the RFA and NTDTVshows compared to the CCTV show.3.3 Incorporating bLSA into Spoken LanguageTranslationTo investigate the effectiveness of bLSA LM adap-tation for spoken language translation, we incorpo-rated the proposed approach into our state-of-the-artphrase-based SMT system.
Translation performancewas evaluated on the RT04 broadcast news evalua-tion set when applied to both the manual transcrip-tions and 1-best ASR hypotheses.
During evalua-tion two performance metrics, BLEU (Papineni etal., 2002) and NIST, were computed.
In both cases, asingle English reference was used during scoring.
Inthe transcription case the original English referenceswere used.
For the ASR case, as utterance segmen-tation was performed automatically, the number ofsentences generated by ASR and SMT differed fromthe number of English references.
In this case, Lev-enshtein alignment was used to align the translationoutput to the English references before scoring.3.4 Baseline SMT SetupThe baseline SMT system consisted of a non adap-tive system trained using the same Chinese-Englishparallel document corpora used in the previous ex-periments (Sections 3.1 and 3.2).
For phrase extrac-tion a cleaned subset of these corpora, consisting of1M Chinese-English sentence pairs, was used.
SMTdecoding parameters were optimized using man-ual transcriptions and translations of 272 utterancesfrom the RT04 development set (LDC2006E10).SMT translation was performed in two stages us-ing an approach similar to that in (Vogel, 2003).First, a translation lattice was constructed by match-ing all possible bilingual phrase-pairs, extractedfrom the training corpora, to the input sentence.Phrase extraction was performed using the ?PESA?
(Phrase Pair Extraction as Sentence Splitting) ap-proach described in (Vogel, 2005).
Next, a searchwas performed to find the best path through the lat-tice, i.e.
that with maximum translation-score.
Dur-ing search reordering was allowed on the target lan-guage side.
The final translation result was thathypothesis with maximum translation-score, whichis a log-linear combination of 10 scores consist-ing of Target LM probability, Distortion Penalty,Word-Count Penalty, Phrase-Count and six Phrase-Alignment scores.
Weights for each componentscore were optimized to maximize BLEU-score onthe development set using MER optimization as de-scribed in (Venugopal et al, 2005).525Translation Quality - BLEU (NIST)SMT Target LM CCTV RFA NTDTV ALLManual TranscriptionBaseline LM: 0.162 (5.212) 0.087 (3.854) 0.140 (4.859) 0.132 (5.146)bLSA (bLSA-Adapted LM): 0.164 (5.212) 0.087 (3.897) 0.143 (4.864) 0.134 (5.162)1-best ASR OutputCER (%) 7.4 25.5 13.1 14.9Baseline LM: 0.129 (4.15) 0.051 (2.77) 0.086 (3.50) 0.095 (3.90)bLSA (bLSA-Adapted LM): 0.132 (4.16) 0.050 (2.79) 0.089 (3.53) 0.096 (3.91)Table 4: Translation performance of baseline and bLSA-Adapted Chinese-English SMT systems on manualtranscriptions and 1-best ASR hypotheses3.5 Performance of Baseline SMT SystemFirst, the baseline system performance was evalu-ated by applying the system described above to thereference transcriptions and 1-best ASR hypothesesgenerated by our Mandarin speech recognition sys-tem.
The translation accuracy in terms of BLEU andNIST for each individual show (?CCTV?, ?RFA?,and ?NTDTV?
), and for the complete test-set, areshown in Table 4 (Baseline LM).
When applied tothe reference transcriptions an overall BLEU scoreof 0.132 was obtained.
BLEU-scores ranged be-tween 0.087 and 0.162 for the ?RFA?, ?NTDTV?
and?CCTV?
shows, respectively.
As the ?RFA?
showcontained a large segment of conversational speech,translation quality was considerably lower for thisshow due to genre mismatch with the training cor-pora of newspaper text.For the 1-best ASR hypotheses, an overall BLEUscore of 0.095 was achieved.
For the ASR case,the relative reduction in BLEU scores for the RFAand NTDTV shows is large, due to the significantlylower recognition accuracies for these shows.
BLEUscore is also degraded due to poor alignment of ref-erences during scoring.3.6 Incorporation of bLSA AdaptationNext, the effectiveness of bLSA based LM adapta-tion was evaluated.
For each show the target En-glish LM was adapted using bLSA-adaptation, asdescribed in Section 2.3.
SMT was then applied us-ing an identical setup to that used in the baseline ex-periments.The translation accuracy when bLSA adaptationwas incorporated is shown in Table 4.
When ap-00.020.040.060.080.10.120.140.16CCTV RFA NTDTV All showsBLEUBaseline-LM bLSA Adapted LMFigure 5: BLEU score for those 25% utteranceswhich resulted in different translations after bLSAadaptation (manual transcriptions)plied to the manual transcriptions, bLSA adaptationimproved the overall BLEU-score by 1.7% relative(from 0.132 to 0.134).
For all three shows bLSAadaptation gained higher BLEU and NIST metrics.A similar trend was also observed when the pro-posed approach was applied to the 1-best ASR out-put.
On the evaluation set a relative improvement inBLEU score of 1.0% was gained.The semantic interpretation of the majority of ut-terances in broadcast news are not affected by topiccontext.
In the experimental evaluation it was ob-served that only 25% of utterances produced differ-ent translation output when bLSA adaptation wasperformed compared to the topic-independent base-line.
Although the improvement in translation qual-ity (BLEU) was small when evaluated over the en-tire test set, the improvement in BLEU score for526these 25% utterances was significant.
The trans-lation quality for the baseline and bLSA-adaptivesystem when evaluated only on these utterances isshown in Figure 5 for the manual transcription case.On this subset of utterances an overall improvementin BLEU of 0.007 (5.7% relative) was gained, witha gain of 0.012 (10.6% relative) points for the ?NT-DTV?
show.
A similar trend was observed when ap-plied to the 1-best ASR output.
In this case a rel-ative improvement in BLEU of 12.6% was gainedfor ?NTDTV?, and for ?All shows?
0.007 (3.7%)was gained.
Current evaluation metrics for trans-lation, such as ?BLEU?, do not consider the rela-tive importance of specific words or phrases duringtranslation and thus are unable to highlight the trueeffectiveness of the proposed approach.
In futurework, we intend to investigate other evaluation met-rics which consider the relative informational con-tent of words.4 ConclusionsWe proposed a bilingual latent semantic modelfor crosslingual LM adaptation in spoken languagetranslation.
The bLSA model consists of a set ofmonolingual LSA models in which a one-to-onetopic correspondence is enforced between the LSAmodels through the sharing of variational Dirich-let posteriors.
Bootstrapping a LSA model for anew language can be performed rapidly with topictransfer from a well-trained LSA model of anotherlanguage.
We transfer the inferred topic distribu-tion from the input source text to the target lan-guage effectively to obtain an in-domain target LSAmarginals for LM adaptation.
Results showed thatour approach significantly reduces the word per-plexity on the target language in both cases usingASR hypotheses and manual transcripts.
Interest-ingly, the adaptation performance is not much af-fected when ASR hypotheses were used.
We eval-uated the adapted LM on SMT and found that theevaluation metrics are crucial to reflect the actualimprovement in performance.
Future directions in-clude the exploration of story-dependent LM adap-tation with automatic story segmentation instead ofshow-dependent adaptation due to the possibility ofmultiple stories within a show.
We will investigatethe incorporation of monolingual documents for po-tentially better bilingual LSA modeling.AcknowledgmentThis work is partly supported by the Defense Ad-vanced Research Projects Agency (DARPA) underContract No.
HR0011-06-2-0001.
Any opinions,findings and conclusions or recommendations ex-pressed in this material are those of the authors anddo not necessarily reflect the views of DARPA.ReferencesD.
Blei, A. Ng, and M. Jordan.
2003.
Latent DirichletAllocation.
In Journal of Machine Learning Research,pages 1107?1135.W.
Kim and S. Khudanpur.
2003.
LM adaptation usingcross-lingual information.
In Proc.
of Eurospeech.W.
Kim and S. Khudanpur.
2004.
Cross-lingual latentsemantic analysis for LM.
In Proc.
of ICASSP.R.
Kneser, J. Peters, and D. Klakow.
1997.
Languagemodel adaptation using dynamic marginals.
In Proc.of Eurospeech, pages 1971?1974.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.BLEU: A method for automatic evaluation of machinetranslation.
In Proc.
of ACL.M.
Paulik, C. Fu?gen, T. Schaaf, T. Schultz, S. Stu?ker, andA.
Waibel.
2005.
Document driven machine transla-tion enhanced automatic speech recognition.
In Proc.of Interspeech.Y.
C. Tam and T. Schultz.
2006.
Unsupervised languagemodel adaptation using latent semantic marginals.
InProc.
of Interspeech.Y.
C. Tam and T. Schultz.
2007.
Correlated latent seman-tic model for unsupervised language model adaptation.In Proc.
of ICASSP.A.
Venugopal, A. Zollmann, and A. Waibel.
2005.
Train-ing and evaluation error minimization rules for statis-tical machine translation.
In Proc.
of ACL.S.
Vogel.
2003.
SMT decoder dissected: Word reorder-ing.
In Proc.
of ICNLPKE.S.
Vogel.
2005.
PESA: Phrase pair extraction as sentencesplitting.
In Proc.
of the Machine Translation Summit.B.
Zhao and E. P. Xing.
2006.
BiTAM: Bilingual topicadmixture models for word alignment.
In Proc.
ofACL.527
