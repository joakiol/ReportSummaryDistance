Word Association and MI-Trigger-based Language ModelingGuoDong ZHOU KimTeng LUADepartment of Information Systems and Computer ScienceNational University of SingaporeSingapore 119260{zhougd, luakt} @iscs.nus.edu.sgAbstractThere exists strong word association in naturallanguage.
Based on mutual information, thispaper proposes a new MI-Trigger-based modelingapproach to capture the preferred relationshipsbetween words over a short or long distance.
Boththe distance-independent(DI) and distance-dependent(DD) MI-Trigger-based models areconstructed within a window.
It is found thatproper MI-Trigger modeling is superior to wordbigram model and the DD MI-Trigger modelshave better performance than the DI MI-Triggermodels for the same window size.
It is also foundthat the number of the trigger pairs in an MI-Trigger model can be kept to a reasonable sizewithout losing too much of its modeling power.Finally, it is concluded that the preferredrelationships between words are useful tolanguage disambiguation and can be modeledefficiently by the MI-Trigger-based modelingapproach.Introduct ionIn natural language there always exist manypreferred relationships between words.Lexicographers always use the concepts ofcollocation, co-occurrence and lexis to describethem.
Psychologists also have a similar concept:word association.
Two highly associated wordpairs are "not only/but also" and "doctor/nurse".Psychological experiments in \[Meyer+75\]indicated that the human's reaction to a highlyassociated word pair was stronger and faster thanthat to a poorly associated word pair.The strength of word association can bemeasured by mutual information.
By computingmutual information of a word pair, we can getmany useful preference information from thecorpus, such as the semantic preference betweennoun and noun(e.g.
"doctor/nurse"), the particularpreference between adjective andnoun(e.g.
"strong/currency'), andsolid structure(e.g."pay/attention")\[Calzolori90\].
Theseinformation are useful for automatic sentencedisambiguation.
Similar research includes\[Church90\], \[Church+90\], Magerman+90\],\[Brent93\], \[Hiddle+93\], \[Kobayashi+94\] and\[Rosenfeld94\].In Chinese, a word is made up of one or morecharacters.
Hence, there also exists preferredrelationships between Chinese characters.\[Sproat+90\] employed a statistical method togroup neighboring Chinese characters in asentence into two-character words by making useof a measure of character association based onmutual information.
Here, we will focus insteadon the preferred relationships between words.The preference relationships between wordscan expand from a short to long distance.
WhileN-gram models are simple in language modelingand have been successfully used in many tasks,they have obvious deficiencies.
For instance, N-gram models can only capture the short-distancedependency within an N-word window wherecurrently the largest practical N for naturallanguage is three and many kinds of dependenciesin natural language occur beyond a three-wordwindow.
While we can use conventional N-grammodels to capture the short-distance d pendency,the long-distance dependency should also beexploited properly.The purpose of this paper is to study thepreferred relationships between words over ashort or long distance and propose a newmodeling approach to capture such phenomena inthe Chinese language.1465This paper is organized as follows: Section 1defines the concept of trigger pair.
The criteria ofselecting a trigger pair are described in Section 2while Section 3 describes how to measure thestrength of a trigger pair.
Section 4 describestrigger-based language modeling.
Section 5 givesone of its applications: PINYIN-to-CharacterConversion.
Finally, a conclusion is given.1 Concept of Trigger PairBased on the above description, we decide to usethe trigger pair\[Rosenfeld94\] as the basic conceptfor extracting the word association information ofan associated word pair.
If a word A is highlyassociated with another word B, then (A --~ B)is considered a "trigger pair", with A being thetrigger and B the triggered word.
When Aoccurs in the document, it triggers B,  causing itsprobability estimate to change.
A and B can bealso extended to word sequences.
For simplicity.here we will concentrate on the triggerrelationships between single words although theideas can be extended to longer word sequences.How to build a trigger-based language model?There remain two problems to be solved: 1) howto select a trigger pair?
2) how to measure atrigger pair'?We will discuss them separately in the next twosections.2 Selecting Trigger PairEven if we can restrict our attention to the triggerpair (A, B) where A and B are both single words,the number of such pairs is too large.
Therefore,selecting a reasonable number of the mostpowerful trigger pairs is important o a trigger-based language model.2.1 Window SizeThe most obvious way to control the number ofthe trigger pairs is to restrict the window size,which is the maximum distance between thetrigger pair.
In order to decide on a reasonablewindow size, we must know how much thedistance between the two words in the trigger pairaffects the word probabilities.Therefore, we construct the long-distanceWord Bigram(WB) models for distance-d = 1,2, .... 100.
The distance-100 is used as acontrol, since we expect no significantinformation after that distance.
We compute theconditional perplexity\[Shannon5 l\] for each long-distance WB model.Conditional perplexity is a measure of theaverage number of possible choices there are tbr aconditional distribution.
The conditionalperplexity of a conditional distribution withconditional entropy H(Y\]X) is defined to be2 H(rtx) .
Conditional Entropy is the entropy of aconditional distribution.
Given two randomvariables ) (and Y, a conditional probabilitymass function Prrx(YlX), and a marginalprobability mass function Pr (Y), the conditionalentropy of Y given X,  H(Y\]X) is defined as:H(YIX)=-~-,~.Px.r(x,y)Iog: Prlx(ylx) (1)x.~Xy,EYFor a large enough corpus, the conditionalperplexity is usually an indication of the amountof information conveyed by the model: the lowerthe conditional perplexity, the more imbrmation itconveys and thus a better model.
This is becausethe model captures as much as it can of thatinformation, and whatever uncertainty remainsshows up in the conditional perplexity.
Here, thetraining corpus is the XinHua corpus, which hasabout 57M(million) characters or 29M words.From Table 1 we find that the conditionalperplexity is lowest for d = 1, and it increasessignificantly as we move through d = 2, 3, 4, 5and 6.
For d = 7, 8, 9, 10, 11, the conditionalperplexity increases lightly.
We conclude thatsignificant information exists only in the last 6words of the history.
However, in this paper werestrict maximum window size to 10.Distance Perplexity230Distance12 575 8 15313 966 9 15804 1157 10 15995 1307 11 16116 1410 100 1674Perplexity1479Table 1: Conditional perplexities of the long-distance WB models for different distances2.2 Selecting Trigger PairGiven a window, we define two events:1466w : { w is the next word }w o : { wo occurs somewhere in the window}Considering a particular trigger (A ~ B) ,  weare interested in the correlation between the twoevents A o and B.A simple way to assess the significance of thecorrelation between the two events A o and B inthe trigger(A ~ B) is to measure their crossproduct ratio(CPR).
One often used measure isthe logarithmic measure of that quality, whichhas units of bits and is defined as:P(Ao,B)P(Ao,B)log CPR(Ao, B) = log (2)P(A o , B)P(A o , B)where P(X o, Y) is the probability of a word pair(X , , ,  Y) occurring in the window.Although the cross product ratio measure issimple, it is not enough in determining the utilityof a proposed trigger pair.
Consider a highlycorrelated pair consisting of two rare words(}~}~ -+ \[~ ~) ,  and compare it to a less wcllcorrelated, but more common pair( \ [~- -~?)
.
An occurrence of the word"~}~"(tai l  of tree) provides more informationabout the word "\[~ ~o~ ,,,,.
re ,~,~.
tpu white) than anoccurrence of the word "\[~ ~'(doctor)  about theword "~?
"(nurse) .
Nevertheless, since theword " \ [~"  is likely to be much more commonin the test data, its average utility may be muchhigher.
If we can afford to incorporate only oneof the two pairs into our trigger-based model, thetrigger pa i r ( \ [~  ---> ~?)
may bc preferable.Therefore, an alternative measure of theexpected benefit provided by A o in predicting Bis the average mutual information(AMI) betweenthe two:P(AoB) AMI(Ao; B) = P(A o, B) logP(Ao)P(B)+ P(Ao,-B)Iog P(AoB)P(Ao)P(B)+ P(A-'~o,B)log P(__AoB)P(Ao)P(B)P(A o B)+ P(A o, B) log e(-~oo)P(-~) (3)Obviously, Equation 3 takes the jointprobability into consideration.
We use thisequation to select the trigger pairs.
In relatedworks, \[Rosenfeld94\] used this equation and\[Church+90\] used a variant of the first term toautomatically identify the associated word pairs.3 Measuring Trigger PairConsidering a trigger pair (A, --~ B) selected byaverage mutual information AMI  ( A o ; B) asshown in Equation 3, mutual informationMI(Ao;B)  reflects the degree of preferencerelationship between the two words in the triggerpair, which can be computed as tbllows:MI(Ao;B) =log P(Ao,B) (4)P(A o ).
P(B)where P(X) is the probability of the word Xoccurred in the corpus and P(A,B) is theprobability of the word pair(A,B)  occurred inthe window.Several properties of mutual information areapparent:?
M I (Ao ;B  ) is deferent from MI(Bo;A) ,i.e.
mutual information is ordering dependent.
* If A, and B are independent, thenMI(A ;  B) = O.In the above equations, the mutual informationMI(A  o;B) reflects the change of theinformation content when the two words A o andB are correlated.
This is to say, the higher thevalue of MI(Ao;B) ,  the stronger affinity thewords A o and B have.
Therefore, we use mutualinformation to measure the preferencerelationship degree of a trigger pair.5 MI-Trigger-based ModelingAs discussed above, we can restrict he number ofthe trigger pairs using a reasonable window size,select the trigger pairs using average mutualinformation and then measure the trigger pairsusing mutual information.
In this section, we willdescribe in greater detail about how to build atrigger-based model.
As the triggers are mainlydetermined by mutual information, we call themMI-Triggers.
To build a concrete MI-Triggermodel, two factors have to be considered.1467Obviously one is the window size.
As we haverestricted the maximum window size to 10, wewill experiment on 10 different windowsizes(ws = 1,2,...,10).Another one is whether to measure an MI-Trigger in a distance-independent(DI) or distance-dependent(DD) way.
While a DI MI-Triggermodel is simple, a DD MI-Trigger model has thepotential of modeling the word association betterand is expected to have better performancebecause many of the trigger pairs are distance-dependent.
We have studied this issue using theXinHua corpus of 29M words by creating anindex file that contains.
For every word, a recordof all of its occurrences with distance-dependentco-occurrence statistics.
Some examples areshown in Table 2, which shows that"jl~_/~_"("the more/the more") has the highestcorrelation when the distance is 2, that"~<{l~I/~_l~l.
"("not nly/but also") has thehighest correlation when the distances are 3, 4and 5, and that "1~'?-~ / ~?
"("doctor/nurse")has the highest correlation when the distances are1 and 2.
After manually browsing hundreds ofthe trigger pairs, we draw following conclusions:* Different trigger pairs display differentbehaviors.. Behaviors of trigger pairs are distance-dependent and should be measured in a distance-dependent way.?
Most of the potential of triggers isconcentrated on high-frequency words.
(1~," - I : - - -~)  is indeed more useful than(~  ~ ?~ ~) .Distance ~.~/L~_ ~/~ ~I I~ /~?1 0 0 242 3848 5 153 72 24 14 65 18 15 45 14 06 45 4 07 40 2 08 23 3 09 9 2 110 8 4 0Table 2: The occurrence frequency of  wordpairs as a function o f  distanceTo compare the effects of the above twofactors, 20 MI-trigger models(in which DI andDD MI-Trigger models with a window size of 1are same) are built.
Each model differs indifferent window sizes, and whether theevaluation is done in the DI or DD way.Moreover, for ease of comparison, each MI-Trigger model includes the same number of thebest trigger pairs.
In our experiments, only thebest 1M trigger pairs are included.
Experiments todetermine the effects of different numbers of thetrigger pairs in a trigger-based model will beconducted in Section 5.For simplicity, we represent a trigger pair asXX-ws-MI-Trigger, and call a trigger-basedmodel as the XX-ws-MI-Trigger model, whileXX represents DI or DD and ws represents thewindow size.
For example, the DD-6-MI-Triggermodel represents a distance-dependent MI-Trigger-based model with a window size of 6.All the models are built on the XinHua corpusof 29M words.
Let's take the DD-6-MI-Triggermodel as a example.
We filter about28 x 28 x 6M(with six different distances andwith about 28000 Chinese words in the lexicon)possible DD word pairs.
As a first step, only wordpairs that co-occur at least 3 times are kept.
Thisresults in 5.7M word pairs.
Then selected byaverage mutual information, the best IM wordpairs are kept as trigger pairs.
Finally, the best1M MI-Trigger pairs are measured by mutualinformation.
In this way, we build a DD-6-MI-Trigger model which includes the best 1M triggerpairs.Since the MI-Trigger-based models measurethe trigger pairs using mutual information whichonly reflects the change of information contentwhen the two words in the trigger pair arecorrelated, a word unigram model is combinedwith them.
Given S=w~w2.
.
.w n, we canestimate the logarithmic probability log P (S) .For a DI- ws MI-Trigger-based model,"1 log P(S) = ~ og P(wi)i=12 max(ld-ws)+~ ~OI-ws-M1-Trigger(wj ~ w~) (5)i=n j=i-Iand for a DD-ws-MI-Trigger-based model,"1 log P(S) = Z og P(wi)1=114682 max{ I, i -  ws)+ ~" ~ DD - ws - M!
- Tngger f .
, )  --* wi, i  - j + 1) (6)i=n j=i-Iwhere ws is the windows size and i -  j + 1 isthe distance between the words w. and w i .
Thefirst item in each of Equation 5 and 6 is thelogarithmic probability of S using a wordunigram model and the second one is the valuecontributed to the MI-Trigger pairs in the MI-Trigger model.In order to measure the efficiency of the MI-Trigger-based models, the conditionalperplexities of the 20 different models (each has1M trigger pairs) are computed from the XinHuacorpus of 29M words and are shown in Table 3.WindowSizeDis tance  -Independent301Distance -Dependent3012 288 2593 280 2384 272 2215 267 2106 262 2017 270 2168 275 2279 282 24110 287 252Table 3: The conditional perplexities of the 20different MI-Trigger models5 P INYIN-to-Charaeter  Convers ionAs an application of the MI-Trigger-basedmodeling, a PINYIN-to-Character Conversion(PYCC) system is constructed.
In fact, PYCC hasbeen one of the basic problems in Chineseprocessing and the subjects of many researchersin the last decade.
Current approaches include:The longest word preference algorithm\[Chen+87\] with some usage learning methods\[Sakai+93\].
This approach is easy to implement,but the hitting accuracy is limited to 92% evenwith large word dictionaries.?
The rule-based approach \[Hsieh+89\] \[Hsu94\].This approach is able to solve the related lexicalambiguity problem efficiently and the hittingaccuracy can be enhanced to 96%.?
The statistical approach \[Sproat92\] \[Chen93\].This approach uses a large corpus to compute theN-gram and then uses some statistical ormathematical models, e.g.
HMM, to find theoptimal path through the lattice of possiblecharacter transliterations.
The hitting accuracycan be around 96%.
* The hybrid approach using both the rules andstatistical data\[Kuo96\].
The hitting accuracy canbe close to 98%.In this section, we will apply the MI-Trigger-based models in the PYCC application.
For easeof comparison, the PINYIN counterparts of 600Chinese sentences(6104 Chinese characters) fromChinese school text books are used for testing.The PYCC recognition rates of different MI-Trigger models are shown in Table 4.WindowSizeDis tance  -Independent93.6%Dis tance  -Dependent93.6%2 94.4% 95.5%3 94.7% 96.1%4 95.0% 96.3%5 95.2% 96.5%6 95.3% 96.6%7 94.9% 96.4%8 94.6% 96.2%9 94.5% 96.1%10 94.3% 95.8%Table 4: The PYCC recognitionMI-Trigger modelsNo.
of the MI-Trigger Pairs0100,000200,000400,000rates for the 20Perplexity RecognitionRate1967 85.3%67235829390.7%92.6%600,000800,0001,000,0001,500,0002,000,0003,000~0004,000,0005,000,0006,000,00094.2%260 95.5%224 96.3%201 96.6%193 96.9%186 97.2%183 97.2%181 97.3%178 97.6%175 97.7%Table 5: The effect of different numbers of thetrigger pairs on the PYCC recognition ratesTable 4 shows that the DD-MI-Trigger modelshave better performances than the DI-MI-Triggermodels for the same window size.
Therefore, thepreferred relationships between words should be1469modeled in a DD way.
It is also found that thePYCC recongition rate can reach up to 96.6%.As it was stated above, all the MI-Triggermodels only include the best 1M trigger pairs.One may ask: what is a reasonable number of thetrigger pairs that an MI-Trigger model shouldinclude?
Here, we will examine the effect ofdifferent numbers of the trigger pairs in an MI-Trigger model on the PINYIN-to-Characterconversion rates.
We use the DD-6-MI-Triggermodel and the result is shown in Table 5.We can see from Table 5 that the recognitionrate rises quickly from 90.7% to 96.3% as thenumber of MI-Trigger pairs increases from100,000 to 800,000 and then it rises slowly from96.6% to 97.7% as the number of MI-Triggersincreases from 1,000,000 to 6,000,000.
Therefore,the best 800,000 trigger pairs should at least beincluded in the DD-6-MI-Trigger model.ParameterNumbersModel WordUnigram28,0001967WordBigram28,00027.8 x I0 8230DD-6-MI-Trigger5 x 10 ~ * 2,~.t~)0= 5.0 x 10 ~178PerplexityTable 6: Comparison of word umgram, bigramand MI-Trigger modelIn order to evaluate the efficiency of MI-Trigger-based language modeling, we compare itwith word unigram and bigram models.
Bothword unigram and word bigram models aretrained on the XinHua corpus of 29M words.
Theresult is shown in Table 6.
Here the DD-6-MI-Trigger model with 5M trigger pairs is used.Table 6 shows that?
The MI-Trigger model is superior to wordunigram and bigram models.
The conditionalperplexity of the DD-6-MI-Trigger model is lessthan that of word bigram model and much lessthan the word unigram model.?
The parameter number of the MI-Triggermodel is much less than that of word bigrammodel.One of the most powerful abilities of a personis to properly combine different knowledge.
Thisalso applies to PYCC.
The word bigram modeland the MI-Trigger model are merged by linearinterpolation as follows:log PMeR~ED (S) = (1 - a)-log Ps,~.~,, (S)+a .
log PMt_r,i~g~,.
( S) (7)n where S = w~ = w~w2.
.
.w  .
and a is the weightof the word bigram model.
Here the DD-6-MI-Trigger model with 5M trigger pairs is applied.The result is shown in Table 7.Table 7 shows that the recognition rate reachesup to 98.7% when the N-gram weight is 0.3 andthe MI-Trigger weight isMI-Trigger Weight0.00.7.Reco~,nition Rate96.2%0.1 96.5%0.2 97.3%0.3 97.7%0.4 98.2%0.5 98.3%0.6 98.6%0.7 98.7%0.8 98.5%0.9 98.2%1.0 97.6%Table 7: The PYCC recognition rates of wordbigram and MI-Trigger mergingThrough the experiments, it has been proventhat the merged model has better esults over bothword bigram and Ml-Trigger models.
Comparedto the pure word bigram model, the merged modelalso captures the long-distance dependency ofword pairs using the concept of mutualinformation.
Compared to the MI-trigger modelwhich only captures highly correlated word pairs,the merged model also captures poorly correlatedword pairs within a short distance by using theword bigram model.ConclusionThis paper proposes a new MI-Trigger-basedmodeling approach to capture the preferredrelationships between words by using the conceptof trigger pair.
Both the distance-independent(DI)and distance-dependent(DD) MI-Trigger-basedmodels are constructed within a window.
It isfound that?
The long-distance dependency is useful tolanguage disambiguation a d should be modeledproperly in natural language processing.1470?
The DD MI-Trigger models have betterperformance than the DI MI-Trigger models forthe same window size.?
The number of the trigger pairs in an MI-Trigger model can be kept to a reasonable sizewithout losing too much of its modeling power.?
The MI-Trigger-based language modeling hasbetter performance than the word bigram modelwhile the parameter number of the MI-Triggermodel is much less than that of the word bigrammodel.
The PINYIN-to-Character conversion ratereaches up to 97.7% by using the MI-Triggermodel.
The recognition rate further eaches up to98.7% by proper word bigram and MI-Triggermerging.References\[Brent93\] Brent M. "From Grammar to Lexicon:Unsupervised Learning of Lexical Syntax".Computational Linguistics, Vol.
19, No.2,pp,263-311, June 1993.\[Calzolori90\] Calzolori N. "Acquisition ofLexical Information from a Large TextualItalian Corpus".
Proc.
of COLING.
Vol.2,pp.54-59, 1990.\[Chen+87\] Chen S.I.
et al "The ContinuousConversion Algorithm of Chinese Character'sPhonetic Symbols to Chinese Character".
Proc.of National Computer Symposium, Taiwan,pp.437-442.
1987.\[Chen93\] Chen J.K. "A Mathematical Model forChinese Input".
Computer Processing ofChinese & Oriental Languages.
Vol.
7, pp.75-84, 1993.\[Church90\] Church K. "Word AssociationNorms, Mutual Information and Lexicography".Computational Linguistics, Vol.
16, No.
1, pp.22-29.
1990.\[Church+90\] Church K. et al "Enhanced GoodTuring and Cat-Cal: Two New Methods forEstimating Probabilities of English Bigrams".Computer, Speech and Language, Vol.5, pp.19-54, 1991.\[Hindle+93\] Hindle D. et al "StructuralAmbiguity and Lexical Relations".Computational Linguistics, Vol.19, No.l,pp.
103-120, March 1993.\[Hsieh+89\] Hsieh M.L.
et al " A GrammaticalApproach to Convert Phonetic Symbols intoCharacters".
Proc.
of National ComputerSymposium.
Taiwan, pp.453-461, 1989.\[Hsu94\] Hsu W.L.
"Chinese Parsing in aPhoneme-to-Character Conversion Systembased on Semantic Pattern Matching'" ChineseProcessing of Chinese & Oriental Languages.Vol.8, No.2, pp.227-236, 1994.\[Kobayashi+94\] Kobayashi T. et al "Analysis ofJapanese Compound Nouns using CollocationalInformation".
Proc.
of COLLVG.
pp.865-970,1994.\[Kuo96\] Kuo J.J. "Phonetic-Input-to-CharacterConversion System for Chinese Using SyntacticConnection Table and Semantic Distance".Computer Processing of Chinese & OrientalLanguages.
Vol.
10, No.2, pp.
195-210, 1996.\[Magerman+90\] Magerman D. et al "Parsing aNatural Language Using Mutual InformationStatistics", Proc.
of AAAI, pp.984-989, 1990.\[Meyer+75\] Meyer D. et al "Loci of contextualeffects on visual word recognition".
InAttentionand Performance V, edited by P.Rabbitt andS.Dornie.
Acdemic Press, pp.98-116, 1975.\[Rosenfeld94\] Rosenfeld R. "Adaptive StatisticalLanguage Modeling: A Maximum EntropyApproach".
Ph.D. Thesis.
Carneige MellonUniversity, April 1994.\[Sakai+93\] Sakai T. et al "An Evaluation ofTranslation Algorithms and Learning Methodsin Kana to Kanji Translation".
InformationProcessing Society of Japan.
Vol.34, No.12,pp.2489-2498, 1993.\[Shannon51\] Shannon C.E.
"Prediction andEntropy of Printed English".
Bell SystemsTechnical Journal, Vol.30, pp.50-64, 1951.\[Sproat+90\] Sproat R. et al "A Statistical Methodfor Finding Word Boundaries in Chinese Text".Computer Processing of Chinese & OrientalLanguages.
Vol.4, No.4, pp.335-351, 1990.\[Sproat92\] Sproat R. "An Application ofStatistical Optimization with DynamicProgramming to Phonemic-Input-to-CharacterConversion for Chinese".
Proc.
of ROCLING.Taiwan, pp.379-390, 1992.1471
