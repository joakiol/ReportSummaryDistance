Syntax-Based Alignment: Supervised or Unsupervised?Hao Zhang and Daniel GildeaComputer Science DepartmentUniversity of RochesterRochester, NY 14627AbstractTree-based approaches to alignment modeltranslation as a sequence of probabilistic op-erations transforming the syntactic parse treeof a sentence in one language into that of theother.
The trees may be learned directly fromparallel corpora (Wu, 1997), or provided by aparser trained on hand-annotated treebanks (Ya-mada and Knight, 2001).
In this paper, wecompare these approaches on Chinese-Englishand French-English datasets, and find that au-tomatically derived trees result in better agree-ment with human-annotated word-level align-ments for unseen test data.1 IntroductionStatistical approaches to machine translation, pio-neered by Brown et al (1990), estimate parame-ters for a probabilistic model of word-to-word cor-respondences and word re-orderings directly fromlarge corpora of parallel bilingual text.
In re-cent years, a number of syntactically motivated ap-proaches to statistical machine translation have beenproposed.
These approaches assign a parallel treestructure to the two sides of each sentence pair, andmodel the translation process with reordering oper-ations defined on the tree structure.
The tree-basedapproach allows us to represent the fact that syn-tactic constituents tend to move as unit, as well assystematic differences in word order in the gram-mars of the two languages.
Furthermore, the treestructure allows us to make probabilistic indepen-dence assumptions that result in polynomial timealgorithms for estimating a translation model fromparallel training data, and for finding the highestprobability translation given a new sentence.Wu (1997) modeled the reordering process withbinary branching trees, where each productioncould be either in the same or in reverse order goingfrom source to target language.
The trees of Wu?sInversion Transduction Grammar were derived bysynchronously parsing a parallel corpus, using agrammar with lexical translation probabilities at theleaves and a simple grammar with a single nonter-minal providing the tree structure.
While this gram-mar did not represent traditional syntactic categoriessuch as verb phrases and noun phrases, it servedto restrict the word-level alignments considered bythe system to those allowable by reordering opera-tions on binary trees.
This restriction correspondsto intuitions about the alignments that could be pro-duced by systematic differences between the twolanguage?s grammars, and allows for a polynomialtime algorithm for finding the highest-probabilityalignment, and for re-estimation of the lexical trans-lation and grammar probabilities using the Expecta-tion Maximization algorithm.Yamada and Knight (2001) present an algorithmfor estimating probabilistic parameters for a simi-lar model which represents translation as a sequenceof re-ordering operations over children of nodes ina syntactic tree, using automatic parser output forthe initial tree structures.
This gives the translationmodel more information about the structure of thesource language, and further constrains the reorder-ings to match not just a possible bracketing as in Wu(1997), but the specific bracketing of the parse treeprovided.In this paper, we make a direct comparisonof a syntactically unsupervised alignment model,based on Wu (1997), with a syntactically super-vised model, based on Yamada and Knight (2001).We use the term syntactically supervised to indicatethat the syntactic structure in one language is givento the training procedure.
It is important to note,however, that both algorithms are unsupervised inthat they are not provided any hand-aligned train-ing data.
Rather, they both use Expectation Maxi-mization to find an alignment model by iterativelyimproving the likelihood assigned to unaligned par-allel sentences.
Our evaluation is in terms of agree-ment with word-level alignments created by bilin-gual human annotators.
We describe each of themodels used in more detail in the next two sections,including the clone operation of Gildea (2003).
Thereader who is familiar with these models may pro-ceed directly to our experiments in Section 4, andfurther discussion in Section 5.2 The Inversion Transduction GrammarThe Inversion Transduction Grammar of Wu (1997)can be thought as a a generative process which si-multaneously produces strings in both languagesthrough a series of synchronous context-free gram-mar productions.
The grammar is restricted to bi-nary rules, which can have the symbols in the righthand side appear in the same order in both lan-guages, represented with square brackets:X ?
[Y Z]or the symbols may appear in reverse order in thetwo languages, indicated by angle brackets:X ?
?Y Z?Individual lexical translations between Englishwords e and French words f take place at the leavesof the tree, generated by grammar rules with a singleright hand side symbol in each language:X ?
e/fGiven a bilingual sentence pair, a synchronousparse can be built using a two-dimensional exten-sion of chart parsing, where chart items are indexedby their nonterminal Y and beginning and endingpositions l, m in the source language string, and be-ginning and ending positions i, j in the target lan-guage string.
For Expectation Maximization train-ing, we compute inside probabilities ?
(Y, l, m, i, j)from the bottom up as outlined below:for all l,m, n such that 1 ?
l < m < n < Ns dofor all i, j, k such that 1 < i < j < k < Nt dofor all rules X ?
Y Z ?
G do?
(X, l, n, i, k)+=P ([Y Z]|X)?
(Y, l,m, i, j)?
(Z,m, n, j, k)?
(X, l, n, i, k)+=P (?Y Z?|X)?
(Y,m, n, i, j)?
(Z, l,m, j, k)end forend forend forA similar recursion is used to compute outsideprobabilities for each chart item, and the insideand outside probabilities are combined to derive ex-pected counts for occurrence of each grammar rule,including the rules corresponding to individual lex-ical translations.
In our experiments we use a gram-mar with a start symbol S, a single preterminal C,and two nonterminals A and B used to ensure thatonly one parse can generate any given word-levelalignment (ignoring insertions and deletions) (Wu,1997; Zens and Ney, 2003).
The individual lexicaltranslations produced by the grammar may includea NULL word on either side, in order to representinsertions and deletions.3 The Tree-To-String ModelThe model of Yamada and Knight (2001) can bethought of as a generative process taking a tree inone language as input and producing a string inthe other through a sequence of probabilistic oper-ations.
If we follow the process of an English sen-tence?s transformation into French, the English sen-tence is first given a syntactic tree representation bya statistical parser (Collins, 1999).
As the first stepin the translation process, the children of each nodein the tree can be re-ordered.
For any node withm children, m!
re-orderings are possible, each ofwhich is assigned a probability Porder conditionedon the syntactic categories of the parent node and itschildren.
As the second step, French words can beinserted at each node of the parse tree.
Insertions aremodeled in two steps, the first predicting whether aninsertion to the left, an insertion to the right, or noinsertion takes place with probability Pins , condi-tioned on the syntactic category of the node and thatof its parent.
The second step is the choice of the in-serted word Pt(f |NULL), which is predicted with-out any conditioning information.
The final step,a French translation of each original English word,at the leaves of the tree, is chosen according to adistribution Pt(f |e).
The French word is predictedconditioned only on the English word, and each En-glish word can generate at most one French word,or can generate a NULL symbol, representing dele-tion.
Given the original tree, the re-ordering, inser-tion, and translation probabilities at each node areindependent of the choices at any other node.
Theseindependence relations are analogous to those of astochastic context-free grammar, and allow for effi-cient parameter estimation by an inside-outside Ex-pectation Maximization algorithm.
The computa-tion of inside probabilities ?, outlined below, con-siders possible reorderings of nodes in the originaltree in a bottom-up manner:for all nodes ?i in input tree T dofor all k, l such that 1 < k < l < N dofor all orderings ?
of the children ?1...?m of ?idofor all partitions of span k, l intok1, l1...km, lm do?
(?i, k, l)+=Porder (?|?i)?mj=1 ?
(?j , kj , lj)end forend forend forend forAs with Inversion Transduction Grammar, manyalignments between source and target sentences arenot allowed.
As a minimal example, take the tree:ABX YZOf the six possible re-orderings of the three ter-minals, the two which would involve crossing thebracketing of the original tree (XZY and YZX)are not allowed.
While this constraint gives us away of using syntactic information in translation,it may in many cases be too rigid.
In part to dealwith this problem, Yamada and Knight (2001) flat-ten the trees in a pre-processing step by collapsingnodes with the same lexical head-word.
This allows,for example, an English subject-verb-object (SVO)structure, which is analyzed as having a VP nodespanning the verb and object, to be re-ordered asVSO in a language such as Arabic.
Larger syntacticdivergences between the two trees may require fur-ther relaxation of this constraint, and in practice weexpect such divergences to be frequent.
For exam-ple, a nominal modifier in one language may showup as an adverbial in the other, or, due to choicessuch as which information is represented by a mainverb, the syntactic correspondence between the twosentences may break down completely.
While hav-ing flatter trees can make more reorderings possiblethan with the binary Inversion Transduction Gram-mar trees, fixing the tree in one language generallyhas a much stronger opposite effect, dramatically re-stricting the number of permissible alignments.3.1 Tree-to-String With CloningIn order to provide more flexibility in alignments, acloning operation was introduced for tree-to-stringalignment by Gildea (2003).
The model is modifiedto allow for a copy of a (translated) subtree from theEnglish sentences to occur, with some cost, at anypoint in the resulting French sentence.
For example,in the case of the input treeABX YZa clone operation making a copy of node 3 as a newchild of B would produce the tree:ABX Z YZThis operation, combined with the deletion of theoriginal node Z, produces the alignment (XZY)that was disallowed by the original tree reorderingmodel.The probability of adding a clone of original node?i as a child of node ?j is calculated in two steps:first, the choice of whether to insert a clone under?j , with probability Pins(clone|?j), and the choiceof which original node to copy, with probabilityPclone(?i|clone = 1) =Pmakeclone(?i)?k Pmakeclone(?k)where Pmakeclone is the probability of an originalnode producing a copy.
In our implementation,Pins(clone) is estimated by the Expectation Max-imization algorithm conditioned on the label of theparent node ?j , and Pmakeclone is a constant, mean-ing that the node to be copied is chosen from all thenodes in the original tree with uniform probability.4 ExperimentsWe trained our translation models on a parallelcorpus of Chinese-English newswire text.
We re-stricted ourselves to sentences of no more than 25words in either language, resulting in a training cor-pus of 18,773 sentence pairs with a total of 276,113Chinese words and 315,415 English words.
TheChinese data were automatically segmented into to-kens, and English capitalization was retained.
Wereplace words occurring only once with an unknownword token, resulting in a Chinese vocabulary of23,783 words and an English vocabulary of 27,075words.
Our hand-aligned data consisted of 48 sen-tence pairs also with less than 25 words in eitherlanguage, for a total of 788 English words and 580Chinese words.
A separate development set of 49sentence pairs was used to control overfitting.
Thesesets were the data used by Hwa et al (2002).
Thehand aligned test data consisted of 745 individualaligned word pairs.
Words could be aligned one-to-many in either direction.
This limits the perfor-mance achievable by our models; the IBM modelsallow one-to-many alignments in one direction only,while the tree-based models allow only one-to-onealignment unless the cloning operation is used.Our French-English experiments were based ondata from the Canadian Hansards made available byUlrich German.
We used as training data 20,000sentence pairs of no more than 25 words in ei-ther language.
Our test data consisted of 447 sen-tence pairs of no more than 30 words, hand alignedby Och and Ney (2000).
A separate developmentset of 37 sentences was used to control overfitting.We used of vocabulary of words occurring at least10 times in the entire Hansard corpus, resulting in19,304 English words and 22,906 French words.Our test set is that used in the alignment evalua-tion organized by Mihalcea and Pederson (2003),though we retained sentence-initial capitalization,used a closed vocabulary, and restricted ourselvesto a smaller training corpus.
We parsed the Englishside of the data with the Collins parser.
As an ar-tifact of the parser?s probability model, it outputssentence-final punctuation attached at the lowestlevel of the tree.
We raised sentence-final punctu-ation to be a daughter of the tree?s root before train-ing our parse-based model.
As our Chinese-Englishtest data did not include sentence-final punctuation,we also removed it from our French-English test set.We evaluate our translation models in terms ofagreement with human-annotated word-level align-ments between the sentence pairs.
For scoringthe viterbi alignments of each system against gold-standard annotated alignments, we use the align-ment error rate (AER) of Och and Ney (2000),which measures agreement at the level of pairs ofwords:AER = 1 ?
|A ?
GP | + |A ?
GS ||A| + |GS |where A is the set of word pairs aligned by theautomatic system, GS is the set marked in thegold standard as ?sure?, and GP is the set markedas ?possible?
(including the ?sure?
pairs).
In ourChinese-English data, only one type of alignmentwas marked, meaning that GP = GS .
For a betterunderstanding of how the models differ, we breakthis figure down into precision:P = |A ?
GP ||A|and recall:R = |A ?
GS ||GS |Since none of the systems presented in this com-parison make use of hand-aligned data, they maydiffer in the overall proportion of words that arealigned, rather than inserted or deleted.
This affectsthe precision/recall tradeoff; better results with re-spect to human alignments may be possible by ad-justing an overall insertion probability in order tooptimize AER.Table 1 provides a comparison of results using thetree-based models with the word-level IBM models.IBM Models 1 and 4 refer to Brown et al (1993).We used the GIZA++ package, including the HMMmodel of Och and Ney (2000).
We ran Model 1 forthree iterations, then the HMM model for three iter-ations, and finally Model 4 for two iterations, train-ing each model until AER began to increase on ourheld-out cross validation data.
?Inversion Transduc-tion Grammar?
(ITG) is the model of Wu (1997),?Tree-to-String?
is the model of Yamada and Knight(2001), and ?Tree-to-String, Clone?
allows the nodecloning operation described above.
Our tree-basedmodels were initialized from uniform distributionsfor both the lexical translation probabilities and thetree reordering operations, and were trained untilAER began to rise on our held-out cross-validationdata, which turned out to be four iterations for thetree-to-string models and three for the InversionTransduction Grammar.
French-English results areshown in Table 2.
Here, IBM Model 1 was trainedfor 12 iterations, then the HMM model for 5 iter-ations and Model 4 for 5 iterations.
The ITG andtree-to-string models were both trained for 5 itera-tions.
A learning curve for the Inversion Transduc-tion Grammar, is shown in Figure 1, showing bothperplexity on held-out data and alignment error rate.In general we found that while all models would in-crease in AER if trained for too many iterations, theincreases were of only a few percent.5 DiscussionThe Inversion Transduction Grammar significantlyoutperforms the syntactically supervised tree-to-string model of Yamada and Knight (2001).
Thetree-to-string and IBM models are roughly equiva-lent.
Adding the cloning operation improves tree-to-string results by 2% precision and recall.
It isparticularly significant that the ITG gets higher re-call than the other models, when it is the only modelentirely limited to one-to-one alignments, boundingthe maximum recall it can achieve.Our French-English experiments show only smalldifferences between the various systems.
Overall,performance on French-English is much better thanfor Chinese-English.
French-English has less re-ordering overall, as shown by the percentage of pro-ductions in the viterbi ITG parses that are inverted:14% for French-English in comparison to 23% forChinese-English.One possible explanation for our results is parsererror.
While we describe our system as ?syntacti-AlignmentPrecision Recall Error RateIBM Model 1 .56 .42 .52IBM Model 4 .67 .43 .47Inversion Transduction Grammar .68 .52 .40Tree-to-String w/ Clone .65 .43 .48Tree-to-String w/o Clone .63 .41 .50Table 1: Alignment results on Chinese-English corpus.
Higher precision and recall correspond to loweralignment error rate.AlignmentPrecision Recall Error RateIBM Model 1 .63 .71 .34IBM Model 4 .83 .83 .17Inversion Transduction Grammar .82 .87 .16Tree-to-String w/ Clone .84 .85 .15Table 2: French-English results.cally supervised?, in fact this supervision comes inthe form of the annotation of the Wall Street Journaltreebank on which the parser is trained, rather thanparses for our parallel training corpus.
In particular,the text we are parsing has a different vocabularyand style of prose from the WSJ treebank, and oftenthe fluency of the English translations leaves some-thing to be desired.
While both corpora consist ofnewswire text, a typical WSJ sentencePierre Vinken, 61 years old, will join theboard as a nonexecutive director Nov. 29.contrasts dramatically withIn the past when education on opposingCommunists and on resisting Russia wasstressed, retaking the mainland and uni-fying China became a slogan for the au-thoritarian system, which made the uni-fication under the martial law a tool foroppressing the Taiwan people.a typical sentence from our corpus.While we did not have human-annotated gold-standard parses for our training data, we did havehuman annotated parses for the Chinese side of ourtest data, which was taken from the Penn ChineseTreebank (Xue et al, 2002).
We trained a secondtree-to-string model in the opposite direction, us-ing Chinese trees and English strings.
The Chi-nese training data was parsed with the Bikel (2002)parser, and used the Chinese Treebank parses forour test data.
Results are shown in Table 3.
Becausethe ITG is a symmetric, generative model, the ITGresults in Table 3 are identical to those in Table 1.While the experiment does not show a significantimprovement, it is possible that better parses for thetraining data might be equally important.Even when the automatic parser output is correct,the tree structure of the two languages may not cor-respond.
Dorr (1994) categorizes sources of syntac-tic divergence between languages, and Fox (2002)analyzed a parallel French-English corpus, quanti-fying how often parse dependencies cross when pro-jecting an English tree onto a French string.
Evenin this closely related language pair with gener-ally similar word order, crossed dependencies werecaused by such common occurrences as adverbmodification of a verb, or the correspondence of?not?
to ?ne pas?.
Galley et al (2004) extract trans-lation rules from a large parsed parallel corpus thatextend in scope to tree fragments beyond a singlenode; we believe that adding such larger-scale op-erations to the translation model is likely to signifi-cantly improve the performance of syntactically su-pervised alignment.The syntactically supervised model has beenfound to outperform the IBM word-level alignmentmodels of Brown et al (1993) for translation byYamada and Knight (2002).
An evaluation for thealignment task, measuring agreement with humanjudges, also found the syntax-based model to out-perform the IBM models.
However, a relativelysmall corpus was used to train both models (2121Japanese-English sentence pairs), and the evalua-tions were performed on the same data for training,meaning that one or both models might be signifi-cantly overfitting.Zens and Ney (2003) provide a thorough analy-sis of alignment constraints from the perspective ofdecoding algorithms.
They train the models of Wu1 2 3 4 5 6 7 8 9400500600700Perplexity0.40.450.50.55IterationsAERFigure 1: Training curve for ITG model, showing perplexity on cross-validation data, and alignment errorrate on a separate hand-aligned dataset.AlignmentPrecision Recall Error RateInversion Transduction Grammar .68 .52 .40Tree-to-String, automatic parses .61 .48 .46Tree-to-String, gold parses .61 .52 .44Table 3: Chinese Tree to English String(1997) as well as Brown et al (1993).
Decoding,meaning exact computation of the highest probabil-ity translation given a foreign sentence, is not pos-sible in polynomial time for the IBM models, andin practice decoders search through the space of hy-pothesis translations using a set of additional, hardalignment constraints.
Zens and Ney (2003) com-pute the viterbi alignments for German-English andFrench-English sentences pairs using IBM Model5, and then measure how many of the resultingalignments fall within the hard constraints of bothWu (1997) and Berger et al (1996).
They findhigher coverage for an extended version of ITG thanfor the IBM decoding constraint for both languagepairs, with the unmodified ITG implementation cov-ering about the same amount of German-Englishdata as IBM, and significantly less French-Englishdata.
These results show promise for ITG as a ba-sis for efficient decoding, but do not address whichmodel best aligns the original training data, as IBM-derived alignments were taken as the gold standard,rather than human alignments.
We believe that ourresults show that syntactically-motivated models area promising general approach to training translationmodels as well to searching through the resultingprobability space.Computational complexity is an issue for the tree-based models presented here.
While training theIBM models with the GIZA++ software takes min-utes, the tree-based EM takes hours.
With our C im-plementation, one iteration of the syntactically su-pervised model takes 50 CPU hours, which can beparallelized across machines.
Our tree-based mod-els are estimated with complete EM, while the train-ing procedure for the IBM models samples from anumber of likely alignments when accumulating ex-pected counts.
Because not every alignment is legalwith the tree-based models, the technique of sam-pling by choosing likely alignments according to asimpler model is not straightforward.
Nonetheless,we feel that training times can be improved with theright pruning and sampling techniques, as will benecessary to train on the much larger amounts datanow available, and on longer sentences.6 ConclusionWe present a side-by-side comparison of syntacti-cally supervised and unsupervised tree-based align-ment, along with the non tree-based IBM Model 4.For Chinese-English, using trees helps the align-ment task, but a data-derived tree structure givesbetter results than projecting automatic Englishparser output onto the Chinese string.
The French-English task is easier overall, and exhibits smallerdifferences between the systems.Acknowledgments We are very grateful to Re-becca Hwa for assistance with the Chinese-Englishdata, and to everyone who helped make the re-sources we used available to the research commu-nity.
This work was partially supported by NSF ITRIIS-09325646.ReferencesAdam Berger, Peter Brown, Stephen Della Pietra,Vincent Della Pietra, J. R. Fillett, Andrew Kehler,and Robert Mercer.
1996.
Language transla-tion apparatus and method of using context-based tanslation models.
United States patent5,510,981.Daniel M. Bikel.
2002.
Design of a multi-lingual,parallel-processing statistical parsing engine.
InProceedings ARPA Workshop on Human Lan-guage Technology.Peter F. Brown, John Cocke, Stephen A. DellaPietra, Vincent J. Della Pietra, Frederick Je-linek, John D. Lafferty, Robert L. Mercer, andPaul S. Roossin.
1990.
A statistical approach tomachine translation.
Computational Linguistics,16(2):79?85, June.Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
1993.
Themathematics of statistical machine translation:Parameter estimation.
Computational Linguis-tics, 19(2):263?311.Michael John Collins.
1999.
Head-driven Statisti-cal Models for Natural Language Parsing.
Ph.D.thesis, University of Pennsylvania, Philadelphia.Bonnie J. Dorr.
1994.
Machine translation diver-gences: A formal description and proposed solu-tion.
Computational Linguistics, 20(4):597?633.Heidi J.
Fox.
2002.
Phrasal cohesion and statisti-cal machine translation.
In In Proceedings of the2002 Conference on Empirical Methods in Natu-ral Language Processing (EMNLP 2002), pages304?311.Michel Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a translationrule?
In Proceedings of the Human LanguageTechnology Conference/North American Chapterof the Association for Computational Linguistics(HLT/NAACL).Daniel Gildea.
2003.
Loosely tree-based alignmentfor machine translation.
In Proceedings of the41th Annual Conference of the Association forComputational Linguistics (ACL-03), pages 80?87, Sapporo, Japan.Rebecca Hwa, Philip Resnik, Amy Weinberg, andOkan Kolak.
2002.
Evaluating translational cor-respondence using annotation projection.
In Pro-ceedings of the 40th Annual Conference of theAssociation for Computational Linguistics (ACL-02).Rada Mihalcea and Ted Pederson.
2003.
An eval-uation exercise for word alignment.
In HLT-NAACL 2003 Workshop on Building and UsingParallel Texts: Data Driven Machine Translationand Beyond, pages 1?10, Edmonton, Alberta.Franz Josef Och and Hermann Ney.
2000.
Im-proved statistical alignment models.
In Proceed-ings of the 38th Annual Conference of the Asso-ciation for Computational Linguistics (ACL-00),pages 440?447, Hong Kong, October.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel cor-pora.
Computational Linguistics, 23(3):377?403.Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.2002.
Building a large-scale annotated chinesecorpus.
In Proceedings of the 19th.
InternationalConference on Computational Linguistics (COL-ING 2002), Taipei, Taiwan.Kenji Yamada and Kevin Knight.
2001.
A syntax-based statistical translation model.
In Proceed-ings of the 39th Annual Conference of the Asso-ciation for Computational Linguistics (ACL-01),Toulouse, France.Kenji Yamada and Kevin Knight.
2002.
A de-coder for syntax-based statistical MT.
In Pro-ceedings of the 40th Annual Conference of theAssociation for Computational Linguistics (ACL-02), Philadelphia, PA.Richard Zens and Hermann Ney.
2003.
A compar-ative study on reordering constraints in statisticalmachine translation.
In Proceedings of the 40thAnnual Meeting of the Association for Computa-tional Linguistics, Sapporo, Japan.
