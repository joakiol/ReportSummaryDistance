Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1577?1588,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsA Simple Word Trigger Method for Social Tag SuggestionZhiyuan Liu, Xinxiong Chen and Maosong SunDepartment of Computer Science and TechnologyState Key Lab on Intelligent Technology and SystemsNational Lab for Information Science and TechnologyTsinghua University, Beijing 100084, China{lzy.thu, cxx.thu}@gmail.com, sms@tsinghua.edu.cnAbstractIt is popular for users in Web 2.0 era tofreely annotate online resources with tags.To ease the annotation process, it has beengreat interest in automatic tag suggestion.
Wepropose a method to suggest tags according tothe text description of a resource.
By consid-ering both the description and tags of a givenresource as summaries to the resource writtenin two languages, we adopt word alignmentmodels in statistical machine translation tobridge their vocabulary gap.
Based on thetranslation probabilities between the words indescriptions and the tags estimated on a largeset of description-tags pairs, we build a wordtrigger method (WTM) to suggest tags accord-ing to the words in a resource description.Experiments on real world datasets show thatWTM is effective and robust compared withother methods.
Moreover, WTM is relativelysimple and efficient, which is practical forWeb applications.1 IntroductionIn Web 2.0, Web users often use tags to collect andshare online resources such as Web pages, photos,videos, movies and books.
Table 1 shows a bookentry annotated with multiple tags by users1.
Onthe top of Table 1 we list the title and a shortintroduction of the novel ?The Count of MonteCristo?.
The bottom half of Table 1 shows theannotated tags, each of which is followed by anumber in bracket, the total number of users who1The original record is obtained from the book reviewwebsite Douban (www.douban.com) in Chinese.
Here wetranslate it to English for comprehension.use the tag to annotate this book.
Since the tags ofa resource are annotated collaboratively by multipleusers, we also name these tags as social tags.
Fora resource, we refer to the additional information,such as the title and introduction of a book, asdescription, and the user-annotated social tags asannotation.DescriptionTitle: The Count of Monte CristoIntro: The Count of Monte Cristo is one of the mostpopular fictions by Alexandre Dumas.
The writing ofthe work was completed in 1844.
...AnnotationDumas (2748), Count of Monte Cristo (2716), foreignliterature (1813), novel (1345), France (1096), classic(1062), revenge (913), famous book (759), ...Table 1: An example of social tagging.
The numberin the bracket after each tag is the total count of usersthat annotate the tag on this book.Social tags concisely indicate the main contentof the given resource, and potentially reflect userinterests.
Social tagging has thus been widelystudied and successfully applied in recommendersystems (Eck et al, 2007; Yanbe et al, 2007; Zhouet al, 2010), trend detection and tracking (Hothoet al, 2006), personalization (Wetzker et al, 2010),advertising (Mirizzi et al, 2010), etc.The task of automatic social tag suggestion isto automatically recommend tags for a user whenhe/she wants to annotate a resource.
Social tagsuggestion, as a crucial component for social tag-ging systems, can help users annotate resources.Moreover, social tag suggestion is usually consid-ered as an equivalent problem to modeling social1577tagging behaviors, which is playing a more and moreimportant role in social computing and informationretrieval (Wang et al, 2007).Most online resources contain descriptions, whichusually contain much resource information.
Forexample, on a book review website, each book entrycontains a title, the author(s) and an introductionof the book.
Some researchers thus proposeto automatically suggest tags based on resourcedescriptions, which are collectively known as thecontent-based approach.One may think to suggest tags by selectingimportant words from descriptions.
This is far fromenough because descriptions and annotations areusing diverse vocabularies, usually referred to as avocabulary gap problem.
Take the book entry inTable 1 for instance, the word ?popular?
used in thedescription contrasts the tags ?classic?
and ?famousbook?
in the annotation; the word ?novel?
is used inthe description, while most users annotate with thetag ?fiction?.
The vocabulary gap usually reflects intwo main issues:?
Some tags in the annotation do appear in thecorresponding description, but they may not bestatistically significant.?
Some tags may even not appear in the descrip-tion.It is not trivial to reduce the vocabulary gap andfind the semantic correspondence between descrip-tions and annotations.
By regarding both the de-scription and the annotation as parallel summariesof a resource, we use word alignment models instatistical machine translation (SMT) (Brown etal., 1993) to estimate the translation probabilitiesbetween the words in descriptions and annotations.SMT has been successfully applied in many ap-plications to bridge vocabulary gap.
For detaileddescriptions of related work, readers can refer toSection 2.2.
In this paper, besides employing wordalignment models to social tagging, we also proposea method to efficiently build description-annotationpairs for sufficient learning translation probabilitiesby word alignment models.Based on the learned translation probabilitiesbetween words in descriptions and annotations,we regard the tagging behavior as a word triggerprocess:1.
A user reads the resource description to realizeits substance by seeing some important wordsin the description.2.
Triggered by these important words, the usertranslates them into the corresponding tags, andannotates the resource with these tags.Based on this perspective, we build a simple wordtrigger method (WTM) for social tag suggestion.
InFig.
1, we use a simple example to show the basicidea of using word trigger for social tag suggestion.In this figure, some words in the first sentence of thebook description in Table 1 are triggered to the tagsin annotation.Figure 1: An example of the word trigger methodfor suggesting tags given a description.2 Related Work2.1 Social Tag SuggestionPrevious work has been proposed to automaticsocial tag suggestion.Many researchers built tag suggestion systemsbased on collaborative filtering (CF) (Herlocker etal., 1999; Herlocker et al, 2004), a widely usedtechnique in recommender systems (Resnick andVarian, 1997).
These collaboration-based methodstypically base their suggestions on the tagginghistory of the given resource and user, without con-sidering resource descriptions.
FolkRank (Jaschkeet al, 2008) and Matrix Factorization (Rendle et al,2009) are representative CF methods for social tagsuggestion.
Most of these methods suffer from thecold-start problem, i.e., they are not able to performeffective suggestions for resources that no one hasannotated yet.The content-based approach for social tag sug-gestion remedies the cold-start problem of the1578collaboration-based approach by suggesting tagsaccording to resource descriptions.
Therefore, thecontent-based approach plays an important role insocial tag suggestion.Some researchers regarded social tag suggestionas a classification problem by considering each tagas a category label (Ohkura et al, 2006; Mishne,2006; Lee and Chun, 2007; Katakis et al, 2008;Fujimura et al, 2008; Heymann et al, 2008).Various classifiers such as Naive Bayes, kNN, SVMand neural networks have been explored to solve thesocial tag suggestion problem.There are two issues emerging from theclassification-based methods:?
The annotations provided by users are noisy,and the classification-based methods can nothandle the issue well.?
The training cost and classification cost ofmany classification-based methods are usuallyin proportion to the number of classificationlabels.
These methods may thus be inefficientfor a real-world social tagging system, wherehundreds of thousands of unique tags should beconsidered as classification labels.Inspired by the popularity of latent topic modelssuch as Latent Dirichlet Allocation (LDA) (Blei etal., 2003), various methods have been proposed tomodel tags using generative latent topic models.One intuitive approach is assuming that both tagsand words are generated from the same set of latenttopics.
By representing both tags and descriptionsas the distributions of latent topics, this approachsuggests tags according to their likelihood giventhe description (Krestel et al, 2009; Si and Sun,2009).
Bundschus et al (2009) proposed a jointlatent topic model of users, words and tags.
Iwataet al (2009) proposed an LDA-based topic model,Content Relevance Model (CRM), which aimed atfinding the content-related tags for suggestion.
Em-pirical experiments showed that CRM outperformedboth classification methods and Corr-LDA (Blei andJordan, 2003), a generative topic model for contentsand annotations.Most latent topic models have to pre-specify thenumber of topics before training.
We can either usecross validation to determine the optimal numberof topics or employ the infinite topic models, suchas Hierarchical Dirichlet Process (HDP) (Teh et al,2006) and nested Chinese Restaurant Process (Bleiet al, 2010), to automatically adjust the numberof topics during training.
Both solutions areusually computationally complicated.
What is moreimportant, topic-based methods suggest tags bymeasuring the topical relevance of tags and resourcedescriptions.
The latent topics are of concept-levelwhich are usually too general to precisely suggestthose specific tags such as named entities, e.g.,the tags ?Dumas?
and ?Count of Monte Cristo?
inTable 1.
To remedy the problem, Si et al (2010)proposed a generative model, Tag Allocation Model(TAM), which considers the words in descriptionsas the possible topics to generate tags.
However,TAM assumes each tag can only have at most oneword as its reason.
This is against the fact that a tagmay be annotated triggered by multiple words in thedescription.It should also be noted that social tag suggestion isdifferent from automatic keyphrase extraction (Tur-ney, 2000; Frank et al, 1999; Liu et al, 2009a; Liuet al, 2010b; Liu et al, 2011).
Keyphrase extractionaims at selecting terms from the given documentto represent the main topics of the document.
Onthe contrary, in social tag suggestion, the suggestedtags do not necessarily appear in the given resourcedescription.
We can thus regard social tag sugges-tion as a task of selecting appropriate tags froma controlled tag vocabulary for the given resourcedescription.2.2 Applications of SMTSMT techniques have been successfully used inmany tasks of information retrieval and naturallanguage processing to bridge the vocabulary gapbetween two types of objects.
Some typical tasks aredocument information retrieval (Berger and Laffer-ty, 1999; Murdock and Croft, 2004; Karimzadehganand Zhai, 2010), question answering (Berger et al,2000; Echihabi and Marcu, 2003; Soricut and Brill,2006; Riezler et al, 2007; Surdeanu et al, 2008;Xue et al, 2008), query expansions (Riezler et al,2007; Riezler et al, 2008; Riezler and Liu, 2010),paraphrasing (Quirk et al, 2004; Zhao et al, 2010a;Zhao et al, 2010b), summarization (Banko et al,2000), collocation extraction (Liu et al, 2009b;1579Liu et al, 2010c), keyphrase extraction (Liu etal., 2011), sentiment analysis (Dalvi et al, 2009),computational advertising (Ravi et al, 2010), andimage/video annotation and retrieval (Duygulu etal., 2002; Jeon et al, 2003).3 Word Trigger Method for Social TagSuggestion3.1 Method FrameworkWe describe the word trigger method (WTM) forsocial tag suggestion as a 3-stage process:1.
Preparing description-annotation pairs.Given a collection of annotated resources, we firstprepare description-annotation pairs for learningtranslation probabilities using word alignment mod-els.2.
Learning a translation model.
Given acollection of description-annotation pairs, we adoptIBM Model-1, a widely used word alignment model,to learn the translation probabilities between wordsin descriptions and tags in annotations.3.
Suggesting tags given a resource description.After building translation probabilities betweenwords and tags, given a resource description, wefirst compute the trigger power of each word in thedescription and then suggest tags according to theirtranslation probabilities from the triggered words.Before introducing the method in details, weintroduce the notations.
In a social tagging system,a resource is denoted as r ?
R, where R is the set ofall resources.
Each resource contains a descriptionand an annotation containing a set of tags.
Thedescription dr of resource r can be regarded as a bagof words wr = {(wi, ei)}Nri=1, where ei is the countof word wi and Nr is the number of unique wordsin r. The annotation ar of resource r is representedas tr = {(ti, ei)}Mri=1, where ei is the count of tag tiand Mr is the number of unique tags for r.3.2 Preparing Description-Annotation PairsLearning translation probabilities requires a paralleltraining dataset consisting of a number of alignedsentence pairs.
We assume the description and theannotation of a resource as being written in twodistinct languages.
We thus prepare our paralleltraining dataset by pairing descriptions with anno-tations.The annotation of a resource is a bag of tags withno position information.
We thus select IBM Model-1 (Brown et al, 1993) for training, which does nottake word position information into account on bothsides for each aligned pair.In a social tagging system, the length of aresource description is usually limited to hundredsof words.
Meanwhile, it is common that somepopular resources are annotated by multiple userswith thousands of tags.
For example, the tagDumas is annotated by 2, 748 users for the bookin Table 1.
We have to deal with the length-unbalance between a resource description and itscorresponding annotation for two reasons.?
It is impossible to list all annotated tags onthe annotation side of a description-annotationpair.
The performance of word alignmentmodels will also suffer from the unbalancedlength of sentence pairs in the parallel trainingdata set (Och and Ney, 2003).?
Moreover, the annotated tags may have differ-ent importance for the resource.
It would beunfair to treat these tags without distinction.Here we propose a sampling method to pre-pare length-balanced description-annotation pairsfor word alignment.
The basic idea is to samplea bag of tags from the annotation according to tagweights and make the generated bag of tags withcomparable length with the description.We consider two parameters when sampling tags.First, we have to select a tag weighting type forsampling.
In this paper, we investigate two straight-forward sampling types, including tag frequen-cy (TFt) within the annotation and tag-frequencyinverse-resource-frequency (TF-IRFt).
Given re-source r, TFt and TF-IRFt of tag t are definedas TFt = et/?t et and TF-IRFt = et/?t et ?log(|R|/|?r?R Iet>0|), where |?r?R Iet>0| in-dicates the number of resources that have beenannotated with tag t.Another parameter is the length ratio between thedescription and the sampled annotation.
We denotethe ratio as ?
= |wr|/|tr|, where |wr| is the numberof words in the description and |tr| is the number oftags in the annotation.15803.3 Learning Translation Probabilities UsingWord Alignment ModelsSuppose the source language is resource descriptionand the target language is resource annotation.In IBM Model-1, the relationship of the sourcelanguage w = wJ1 and the target language t = tI1is connected via a hidden variable describing analignment mapping from source position j to targetposition aj :Pr(wJ1 |tI1) =?aJ1Pr(wJ1 , aJ1 |tI1).
(1)The alignment aJ1 also contains empty-word align-ments aj = 0 which align source words to thean empty word.
IBM Model-1 can be trainedusing Expectation-Maximization (EM) algorithm inan unsupervised fashion, and obtains the translationprobabilities of two vocabularies, i.e., Pr(w|t),where t is a tag and w is a word.IBM Model-1 only produces one-to-many align-ments from source language to target language.The learned model is thus asymmetric.
We willlearn translation models on two directions: one isregarding descriptions as the source language andannotations as the target language, and the other isin reverse direction of the pairs.
We denote the firstmodel as Prd2a and the latter as Pra2d.
We furtherdefine Pr(t|w) as the harmonic mean of the twomodels:Pr(t|w) ?
(?/Pr d2a(t|w)+(1??
)/Pr a2d(t|w))?1,(2)where ?
is the harmonic factor to combine the twomodels.
When ?
= 1 or ?
= 0, it simply uses modelPrd2a or Pra2d correspondingly.3.4 Tag Suggestion Using Triggered Words andTranslation ProbabilitiesWhen given the description of a resource, we canrank tags by computing the scores:Pr(t|d = wd) =?w?wdPr(t|w) Pr(w|d), (3)in which Pr(w|d) is the trigger power of the word win the description, which indicates the importance ofthe word.
According to the ranking scores, we cansuggest the top-ranked tags to users.Here we explore three methods to compute thetrigger power of a word in a resource description:TF-IRFw, TextRank and their product.
TF-IRFw andTextRank are two most widely adopted methods forkeyword extraction.Similar to TF-IRFt mentioned in Section 3.2, TF-IRFw considers both the local importance (TFw) andglobal specification (IRFw).TextRank (Mihalcea and Tarau, 2004) is a graph-based method to compute term importance.
Givena resource description, TextRank first builds a termgraph by connecting the terms in the descriptionaccording to their semantic relations, and then runPageRank algorithm (Page et al, 1998) to measurethe importance of each term in the graph.
Readerscan refer to (Mihalcea and Tarau, 2004) for detailedinformation.We also use the product of TF-IRFw and Tex-tRank to weight terms, which potentially takes bothglobal information and term relations into account.Emphasize Tags Appearing In Description forWTM (EWTM) In some social tagging systems,the tags that appear in the resource description aremore likely to be selected by users for annotation.Therefore, we propose to emphasize the tags in thedescription by ranking tags as followsPr(t|d) =?w?wd(?It(w)+(1??)
Pr(t|w))Pr(w|d),(4)where It(w) is an indicator function which getsvalue 1 when t = w and 0 when t 6= w; and ?
isthe smooth factor with range ?
?
[0.0, 1.0].
When?
= 1.0, it suggests tags simply according to theirtrigger powers within the description, while when?
= 0.0, it does not emphasize the tags appearing inthe description and just suggests according to theirtranslation probabilities.
In Section 4.4, we willshow the performance of EWTM.4 Experiments4.1 Datasets and Evaluation MetricsDatasets In our experiments, we select two realworld datasets which are of diverse properties toevaluate our methods.
In Table 2 we show thedetailed statistical information of the two datasets.1581Data R W T N?w N?tBOOK 70, 000 174, 748 46, 150 211.6 3.5BIBTEX 158, 924 91, 277 50, 847 5.8 2.7Table 2: Statistical information of two datasets.
R,W , T , N?w and N?t are the number of resources, thevocabulary of descriptions, the vocabulary of tags,the average number of words in each descriptionand the average number of tags in each resource,respectively.The first dataset, denoted as BOOK, is obtainedfrom a popular Chinese book review website www.douban.com, which contains the descriptions ofbooks and the tags collaboratively annotated byusers.
The second dataset, denoted as BIBTEX, isobtained from an English online bibliography web-site www.bibsonomy.org2.
The dataset containsthe descriptions for academic papers (including thetitle and note for each paper) and the tags annotatedby users.
As shown in Table 2, the average length ofdescriptions in the BIBTEX dataset is much shorterthan the BOOK dataset.
Moreover, the BIBTEXdataset does not provide how many times each tagis annotated to a resource.Evaluation Metrics We use precision, recall andF-measure to evaluate the performance of tag sug-gestion methods.
For a resource, we denote theoriginal tags (gold standard) as Ta, the suggestedtags as Ts, and the correctly suggested tags as Ts ?Ta.
Precision, recall and F-measure are defined asp = |Ts ?
Ta||Ts|, r = |Ts ?
Ta||Ta|, F = 2pr(p + r) .
(5)The final evaluation scores are computed by micro-averaging (i.e., averaging on resources of test set).We perform 5-fold cross validation for each methodon all two datasets.
In experiments, the number ofsuggested tags M ranges from 1 to 10.4.2 Comparing ResultsBaseline Methods We select four content-basedalgorithms as the baselines for comparison: NaiveBayes (NB) (Manning et al, 2008), k nearestneighbor algorithm (kNN) (Manning et al, 2008),2The dataset can be obtained from http://www.kde.cs.uni-kassel.de/bibsonomy/dumpsContent Relevance (CRM) model (Iwata et al,2009) and Tag Allocation Model (TAM) (Si et al,2010).NB and kNN are two representative classificationmethods.
NB is a simple generative model, whichmodels the probability of each tag t given descrip-tion d asPr(t|d) ?
Pr(t)?w?dPr(w|t).
(6)Pr(t) is estimated by the frequency of the resourcesannotated with the tag t. Pr(w|t) is estimated by thefrequency of the word w in the resource descriptionsannotated with the tag t. kNN is a widely usedclassification method for tag suggestion, whichrecommends tags to a resource according to theannotated tags of similar resources measured usingvector space models (Manning et al, 2008).CRM and TAM are selected to represent topic-based methods for tag suggestion.
CRM is an LDA-based generative model.
The number of latent topicsK is the key parameter for CRM.
In experiments, weevaluated the performance of CRM with different Kvalues, and here we only show the best one obtainedby setting K = 1, 024.
TAM is also a generativemodel which considers the words in descriptions asthe topics to further generate tags for the resource.We set parameters for TAM as in (Si et al, 2010).For comparison, we denote our method as WTM.Complexity Analysis We compare the complexityof these methods.
We denote the number of trainingiterations in CRM, TAM and WTM as I 3, andthe number of topics in CRM as K. For thetraining phase, the complexity of NB is O(RN?wN?t),kNN is O(1), TAM is O(IRN?wN?t), CRM isO(IKRN?wN?t), and WTM is O(IRN?wN?t)4.
Whensuggesting for a given resource description withlength Nw, the complexity of NB is O(NwT ),kNN is O(RN?wN?t), CRM is O(IKNwT ), TAM3In fact, the numbers of iterations of the three methods aredifferent from each other.
For simplicity, here we denote themusing the same notation.4In more detail, the training phase of WTM containspreparing parallel training dataset with O(RN?t) and learningtranslation probabilities using word alignment models withO(IRN?wN?t), where I is the number of iterations for learningtranslation probabilities, and N?t is the average number of tagsfor each resource after sampling.1582is O(INwT ) and WTM is O(NwT ).
From theanalysis, we can see that WTM is a relatively simplemethod for both training and suggestion.
This isespecially valuable because WTM also shows goodeffectiveness for tag suggestion compared with othermethods as we will shown later.Parameter Settings We use GIZA++ (Och andNey, 2003)5 as IBM Model-1 to learn transla-tion probabilities using description-annotation pairsfor WTM.
The experimental results of WTM areobtained by setting parameters as follows: tagweighting type as TF-IRFt, length ratio ?
= 1,harmonic factor ?
= 0.5 and the type of word triggerstrength as TF-IRFw.
The influence of parameters toWTM can be found in Section 4.3.Experiment Results and Analysis In Fig.
2 weshow the precision-recall curves of NB, kNN, CRMand WTM on two datasets.
Each point of aprecision-recall curve represents different numbersof suggested tags from M = 1 (bottom right, withhigher precision and lower recall) to M = 10(upper left, with higher recall but lower precision)respectively.
The closer the curve to the upper right,the better the overall performance of the method.From Fig.
2, we observe that:?
WTM consistently performs the best on bothdatasets.
This indicates that WTM is robust andeffective for tag suggestion.?
The advantage of WTM is more significant onthe BOOK dataset.
The reason is that WTMcan take a good advantage of annotation countinformation of tags compared to other methods.?
The average length of resource descriptions isshort in the BIBTEX dataset, which makesit difficult to determine the trigger powers ofwords.
But even on the BIBTEX datasetwith no count information of tags, WTM stilloutperforms other methods especially whenrecommending first several tags.To further demonstrate the performance of WTMand other baseline methods, in Table 3 we show the5GIZA++ is freely available on code.google.com/p/giza-pp.
The toolkit is widely used for word alignment inSMT.
In this paper, we use the default setting of parameters fortraining.0.10.150.20.250.30.350.40.450.50.550.60.650.15  0.2  0.25  0.3  0.35  0.4  0.45  0.5  0.55  0.6  0.65RecallPrecisionWAMNBkNNCRMTAM(a) BOOK0.10.150.20.250.30.350.40.450.50.550.05  0.1  0.15  0.2  0.25  0.3  0.35  0.4RecallPrecisionWAMNBkNNCRMTAM(b) BIBTEXFigure 2: Performance comparison between NB,kNN, CRM, TAM and WTM on two datasets.precision, recall and F-measure of NB, kNN, CRM,TAM and WTM on BOOK dataset when suggestingM = 3 tags6.
Due to the limit of space, we onlyshow the variance of F-measure.
In fact, WTMachieves its best performance when M = 2, wherethe F-measure of WTM is 0.370, outperformingboth CRM (F = 0.263) and TAM (F = 0.277) byabout 10%.An Example In Table 4 we show top 10 tagssuggested by NB, CRM, TAM and WTM for thebook in Table 1.
The number in bracket afterthe name of each method is the count of correctlysuggested tags.
The correctly suggested tags aremarked in bold face.
We select not to show6We select to show this number because it is near the averagenumber of tags for BOOK dataset1583Method Precision Recall F-measureNB 0.271 0.302 0.247?
0.004kNN 0.280 0.314 0.258?
0.002CRM 0.292 0.323 0.266?
0.004TAM 0.310 0.344 0.283?
0.001WTM 0.368 0.452 0.355?
0.002Table 3: Comparing results of NB, kNN, CRM,TAM and WTM on BOOK dataset when suggestingM = 3 tags.the results of kNN because the tags suggested bykNN are totally unrelated to the book due to theinsufficient finding of nearest neighbors.From Table 4, we observe that NB, CRM andTAM, as generative models, tend to suggest generaltags such as ?novel?, ?literature?, ?classic?
and?France?, and fail in suggesting specific tags such as?Alexandre Dumas?
and ?Count of Monte Cristo?.On the contrary, WTM succeeds in suggesting bothgeneral and specific tags related to the book.NB (+6): novel, foreign literature, literature, his-tory, Japan, classic, France, philosophy, America,biographyCRM (+5): novel, foreign literature, literature, bi-ography, philosophy, culture, France, British, comic,historyTAM (+5): novel, sociology, finance, foreign liter-ature, France, literature, biography, France litera-ture, comic, ChinaWTM (+7): novel, Alexandre Dumas, history,Count of Monte Cristo, foreign literature, biogra-phy, suspense, comic, America, FranceTable 4: Top 10 tags suggested by NB, CRM, TAMand WTM for the book in Table 1.In Table 5, we list four important words (usingTF-IRFw as weighting metric) of the description andtheir corresponding tags with the highest translationprobabilities.
The values in brackets are the proba-bility of tag t given word w, Pr(t|w).
For each word,we eliminated the tags with the probability less than0.1.
We can see that the translation probabilities canmap the words in descriptions to their semanticallycorresponding tags in annotations.Count of Monte Cristo: Count of Monte Cristo(0.728), Alexandre Dumas (0.270), .
.
.Alexandre Dumas: Alexandre Dumas (0.966), .
.
.revenge: foreign literature (0.168), classic (0.130),martial arts (0.123), Alexandre Dumas (0.122), .
.
.France: France (0.99), .
.
.Table 5: Four important words (in bold face) in thebook description in Table 1 and their correspondingtags with the highest translation probabilities.4.3 Parameter InfluencesWe explore the parameter influences to WTM forsocial tag suggestion.
The parameters includeharmonic factor, length ratio, tag weighting types,and types of word trigger strength.
When inves-tigating one parameter, we set other parametersto be the values inducing the best performanceas mentioned in Section 4.2.
Finally, we alsoinvestigate the influence of training data size forsuggestion performance.
In experiments we findthat WTM reveals similar trends on both the BOOKdataset and the BIBTEX dataset.
We thus only showthe experimental results on the BOOK dataset foranalysis.Harmonic Factor In Fig.
3 we investigate theinfluence of harmonic factor via the curves of F-measure of WTM versus the number of suggestedtags on the BOOK dataset when harmonic factor ?ranges from 0.0 to 1.0.
As shown in Section 3.3,harmonic factor ?
controls the proportion betweenmodel Prd2a and Pra2d.From Fig.
3, we observe that neither single modelPrd2a (?
= 1.0) nor Pra2d (?
= 0.0) achievesthe best performance.
When the two models arecombined by harmonic mean, the performance isconsistently better, especially when ?
ranges from0.2 to 0.6.
This is reasonable because IBM Model-1 constrains that only the term in source languagecan be aligned to multiple terms in target language,which makes the translation probability learned by asingle model be asymmetric.Length Ratio Fig.
4 shows the influence of lengthratios on the BOOK dataset.
From the figure, weobserve that the performance for tag suggestion isrobust as the length ratio varies, except when theratio breaks the default restriction of GIZA++ (i.e.,1584?
= 10)7.0.220.240.260.280.30.320.340.360.381 2 3 4 5 6 7 8 9 10F-measureNumber of Suggested Tags?
= 0.0?
= 0.2?
= 0.4?
= 0.5?
= 0.6?
= 0.8?
= 1.0Figure 3: F-measure of WTM versus the number ofsuggested tags on the BOOK dataset when harmonicfactor ?
ranges from 0.0 to 1.0.0.20.220.240.260.280.30.320.340.360.381 2 3 4 5 6 7 8 9 10F-measureNumber of Suggested Tags?
= 10/1?
= 10/3?
= 10/5?
= 1/1?
= 1/2?
= 1/5Figure 4: F-measure of WTM versus the number ofsuggested tags on the BOOK dataset when lengthratio ?
ranges from 10/1 to 1/5.Tag Weighting Types The influence of twoweighting types, TFt and TF-IRFt, on social tagsuggestion when M = 3 on the BOOK datasetis shown in Table 6.
TF-IRFt tends to select thetags more specific to the resource while TFt tendsto select the most popular tags, because the latterdoes not consider global information (the IRFt part).7GIZA++ restricts the values of length ratio within [ 19 , 9] bysetting parameter maxfertility=10.
From Fig.
4, we cansee when ?
= 10, the performance becomes much worse sinceGIZA++ will cut off the sentences out of range.Table 6 verifies the analysis, where TF-IRFt isslightly better than TFt.Weighting Precision Recall F-measureTFt 0.356 0.437 0.342?
0.002TF-IRFt 0.368 0.452 0.355?
0.002Table 6: Evaluation results for different tag weight-ing types when M = 3 on the BOOK dataset.Methods for Computing Word Trigger PowerIn Table 7, we show the performance of social tagsuggestions on the BOOK dataset with differentmethods for computing word trigger power.
Fromthe table, we can see that there is not significantdifference between TF-IRFw and the product of TF-IRFw and TextRank, while TextRank itself performsthe worst.
This indicates that TextRank is lesscompetitive to measure word trigger power since itdoes not take global information into consideration.Weighting Precision Recall F-measureTF-IRFw 0.368 0.452 0.355?
0.002TextRank 0.345 0.424 0.332?
0.002Product 0.368 0.451 0.354?
0.002Table 7: Evaluation results for different methods forcomputing word trigger powers when M = 3 on theBOOK dataset.Training Data Size We investigate the influenceof training data size for social tag suggestion.
Asshown in Fig.
5, we increased the training data sizefrom 8, 000 to 56, 000 step by 8, 000, and carriedout evaluation on 4, 000 resources.
The figure showsthat:?
When the training data size is small (e.g.,8, 000), WTM can still achieve good sugges-tion performance.?
As the training data size increases, the perfor-mance of WTM improves, while the improve-ment speed declines.The observation indicates that WTM does notrequire huge-size dataset to achieve good perfor-mance.15850.20.250.30.350.40.450.50.550.60.650.1  0.2  0.3  0.4  0.5  0.6  0.7RecallPrecision8,00016,00024,00032,00040,00048,00056,000Figure 5: Precision-recall curves when the trainingdata size increases from 8, 000 thousand to 56, 000thousand on the BOOK dataset.Conclusion By analyzing the influences of pa-rameters on WTM, we find that WTM is robust toparameter variations.4.4 Performance of EWTMAt the end of this section, we investigate theperformance of EWTM for social tag suggestion.Here we simply set the smooth factor ?
= 0.5.As shown in Table 8, EWTM improves theperformance of WTM (in Table 7) on the BOOKdataset when using TF-IRFw and the product as themethods for computing the word trigger powers,but decays when using TextRank.
This verifiesthat TF-IRFw is the best method to measure wordtrigger powers for WTM.
Table 8 indicates thatemphasizing the tags appearing in the descriptionsmay enhance the suggestion power of the wordtrigger method.Weighting Precision Recall F-measureTF-IRFw 0.385 0.472 0.371?
0.001TextRank 0.344 0.423 0.332?
0.002Product 0.374 0.457 0.360?
0.001Table 8: The evaluation results of EWTM with dif-ferent methods for computing word trigger powerswhen M = 3 on the BOOK dataset.However, the performance of EWTM on theBIBTEX dataset decays much compared to WTM.The F-measure of EWTM is only F = 0.229compared with WTM F = 0.267.
The main reasonof the decay is that: the resource descriptions inthe BIBTEX dataset are usually too short to providesufficient information to precisely emphasize tags.In this case, EWTM may emphasize wrong tags anddrop correct tags.The experimental results on EWTM suggest that,the performance of EWTM is heavily influenced bythe length of resource descriptions.
Therefore, wehave to analyze the characteristics of social taggingsystems to decide whether to emphasize the tags thatappear in the corresponding resource descriptions.As future work, we will investigate the influenceof the smooth factor ?
to EWTM.
It is also worthto investigate the problem when combining withcollaboration-based methods for social tag sugges-tion.5 Conclusion and Future WorkIn this paper, we present a new perspective to socialtagging and propose the word trigger method forsocial tag suggestion based on word alignment instatistical machine translation.
Experiments showthat our method is effective and efficient for socialtag suggestion compared to other baselines.There are still several open problems that shouldbe further investigated:1.
We can exploit other word alignment methodslike log-linear models (Liu et al, 2010a) forsocial tag suggestion.2.
We will ensemble WTM with other content-based and collaboration-based methods to builda practical social tag suggestion system.3.
WTM and EWTM can only suggest the tagsthat have appeared in translation models.
Infuture, we plan to incorporate keyphrase ex-traction in social tag suggestion to make itsuggest more appropriate tags not only fromtranslation models but also from the resourcedescriptions.AcknowledgmentsThis work is supported by the National NaturalScience Foundation of China (NSFC) under GrantNo.
60873174.
The authors would like to thankPeng Li for his insightful suggestions and thank theanonymous reviewers for their helpful comments.1586ReferencesM.
Banko, V.O.
Mittal, and M.J. Witbrock.
2000.Headline generation based on statistical translation.
InProceedings of ACL, pages 318?325.A.
Berger and J. Lafferty.
1999.
Information retrieval asstatistical translation.
In Proceedings of SIGIR, pages222?229.A.
Berger, R. Caruana, D. Cohn, D. Freitag, andV.
Mittal.
2000.
Bridging the lexical chasm: statisticalapproaches to answer-finding.
In Proceedings ofSIGIR, pages 192?199.D.M.
Blei and M.I.
Jordan.
2003.
Modeling annotateddata.
In Proceedings of SIGIR, pages 127?134.D.M.
Blei, A.Y.
Ng, and M.I.
Jordan.
2003.
Latentdirichlet alocation.
JMLR, 3:993?1022.D.M.
Blei, T.L.
Griffiths, and M.I.
Jordan.
2010.The nested chinese restaurant process and bayesiannonparametric inference of topic hierarchies.
Journalof the ACM, 57(2):7.P.F.
Brown, V.J.D.
Pietra, S.A.D.
Pietra, and R.L.Mercer.
1993.
The mathematics of statistical machinetranslation: Parameter estimation.
Computationallinguistics, 19(2):263?311.M.
Bundschus, S. Yu, V. Tresp, A. Rettinger, M. Dejori,and H.P.
Kriegel.
2009.
Hierarchical bayesian modelsfor collaborative tagging systems.
In Proceedings ofICDM, pages 728?733.N.
Dalvi, R. Kumar, B. Pang, and A. Tomkins.
2009.
Atranslation model for matching reviews to objects.
InProceeding of CIKM, pages 167?176.P.
Duygulu, K. Barnard, J.
De Freitas, and D. Forsyth.2002.
Object recognition as machine translation:Learning a lexicon for a fixed image vocabulary.Proceedings of ECCV, pages 97?112.A.
Echihabi and D. Marcu.
2003.
A noisy-channelapproach to question answering.
In Proceedings ofACL, pages 16?23.D.
Eck, P. Lamere, T. Bertin-Mahieux, and S. Green.2007.
Automatic generation of social tags for musicrecommendation.
In Proceedings of NIPS, pages 385?392.E.
Frank, G.W.
Paynter, I.H.
Witten, C. Gutwin, and C.G.Nevill-Manning.
1999.
Domain-specific keyphraseextraction.
In Proceedings of IJCAI, pages 668?673.S.
Fujimura, KO Fujimura, and H. Okuda.
2008.Blogosonomy: Autotagging any text using bloggers?knowledge.
In Proceedings of WI, pages 205?212.J.L.
Herlocker, J.A.
Konstan, A. Borchers, and J. Riedl.1999.
An algorithmic framework for performingcollaborative filtering.
In Proceedings of SIGIR, pages230?237.J.L.
Herlocker, J.A.
Konstan, L.G.
Terveen, and J.T.Riedl.
2004.
Evaluating collaborative filtering recom-mender systems.
ACM Transactions on InformationSystems, 22(1):5?53.P.
Heymann, D. Ramage, and H. Garcia-Molina.
2008.Social tag prediction.
In Proceedings of SIGIR, pages531?538.A.
Hotho, R. Jaschke, C. Schmitz, and G. Stumme.2006.
Trend detection in folksonomies.
SemanticMultimedia, pages 56?70.T.
Iwata, T. Yamada, and N. Ueda.
2009.
Modelingsocial annotation data with content relevance using atopic model.
In Proceedings of NIPS, pages 835?843.R.
Jaschke, L. Marinho, A. Hotho, L. Schmidt-Thieme,and G. Stumme.
2008.
Tag recommendations insocial bookmarking systems.
AI Communications,21(4):231?247.J.
Jeon, V. Lavrenko, and R. Manmatha.
2003.
Automat-ic image annotation and retrieval using cross-mediarelevance models.
In Proceedings of SIGIR, pages119?126.M.
Karimzadehgan and C.X.
Zhai.
2010.
Estimation ofstatistical translation models based on mutual informa-tion for ad hoc information retrieval.
In Proceedingsof SIGIR, pages 323?330.I.
Katakis, G. Tsoumakas, and I. Vlahavas.
2008.
Mul-tilabel text classification for automated tag suggestion.ECML PKDD Discovery Challenge 2008, page 75.R.
Krestel, P. Fankhauser, and W. Nejdl.
2009.
Latentdirichlet alocation for tag recommendation.
InProceedings of ACM RecSys, pages 61?68.S.O.K.
Lee and A.H.W.
Chun.
2007.
Automatictag recommendation for the web 2.0 blogosphereusing collaborative tagging and hybrid ann semanticstructures.
In Proceedings of WSEAS, pages 88?93.Z.
Liu, P. Li, Y. Zheng, and M. Sun.
2009a.
Clusteringto find exemplar terms for keyphrase extraction.
InProceedings of EMNLP, pages 257?266.Z.
Liu, H. Wang, H. Wu, and S. Li.
2009b.
Collocationextraction using monolingual word alignment method.In Proceedings of EMNLP, pages 487?495.Y.
Liu, Q. Liu, and S. Lin.
2010a.
Discriminativeword alignment by linear modeling.
ComputationalLinguistics, 36(3):303?339.Z.
Liu, W. Huang, Y. Zheng, and M. Sun.
2010b.
Au-tomatic keyphrase extraction via topic decomposition.In Proceedings of EMNLP, pages 366?376.Z.
Liu, H. Wang, H. Wu, and S. Li.
2010c.
Improvingstatistical machine translation with monolingual collo-cation.
In Proceedings of ACL, pages 825?833.Z.
Liu, X. Chen, Y. Zheng, and M. Sun.
2011.
Automatickeyphrase extraction by bridging vocabulary gap.
InProceedings of CoNLL, pages 135?144.1587C.D.
Manning, P. Raghavan, and H. Schtze.
2008.Introduction to information retrieval.
CambridgeUniversity Press New York, NY, USA.R.
Mihalcea and P. Tarau.
2004.
TextRank: Bringingorder into texts.
In Proceedings of EMNLP, pages404?411.R.
Mirizzi, A. Ragone, T. Di Noia, and E. Di Sciascio.2010.
Semantic tags generation and retrieval foronline advertising.
In Proceedings of CIKM, pages1089?1098.G.
Mishne.
2006.
Autotag: a collaborative approachto automated tag assignment for weblog posts.
InProceedings of WWW, pages 953?954.V.
Murdock and W.B.
Croft.
2004.
Simple translationmodels for sentence retrieval in factoid question an-swering.
In Proceedings of SIGIR.F.J.
Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
Computationallinguistics, 29(1):19?51.T.
Ohkura, Y. Kiyota, and H. Nakagawa.
2006.
Browsingsystem for weblog articles based on automated folk-sonomy.
In Proceedings of WWW.L.
Page, S. Brin, R. Motwani, and T. Winograd.
1998.The pagerank citation ranking: Bringing order tothe web.
Technical report, Stanford Digital LibraryTechnologies Project.C.
Quirk, C. Brockett, and W. Dolan.
2004.
Monolingualmachine translation for paraphrase generation.
InProceedings of EMNLP, volume 149.S.
Ravi, A. Broder, E. Gabrilovich, V. Josifovski,S.
Pandey, and B. Pang.
2010.
Automatic generationof bid phrases for online advertising.
In Proceedingsof WSDM, pages 341?350.S.
Rendle, L. Balby Marinho, A. Nanopoulos, andL.
Schmidt-Thieme.
2009.
Learning optimal rankingwith tensor factorization for tag recommendation.
InProceedings of KDD, pages 727?736.P.
Resnick and H.R.
Varian.
1997.
Recommendersystems.
Communications of the ACM, 40(3):56?58.S.
Riezler and Y. Liu.
2010.
Query rewriting usingmonolingual statistical machine translation.
Compu-tational Linguistics, 36(3):569?582.S.
Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal, andY.
Liu.
2007.
Statistical machine translation for queryexpansion in answer retrieval.
In Proccedings of ACL,pages 464?471.S.
Riezler, Y. Liu, and A. Vasserman.
2008.
Translatingqueries into snippets for improved query expansion.
InProceedings of COLING, pages 737?744.X.
Si and M. Sun.
2009.
Tag-LDA for scalable real-time tag recommendation.
Journal of ComputationalInformation Systems, 6(1):23?31.X.
Si, Z. Liu, and M. Sun.
2010.
Modeling socialannotations via latent reason identification.
IEEEIntelligent Systems, 25(6):42 ?
49.R.
Soricut and E. Brill.
2006.
Automatic questionanswering using the web: Beyond the factoid.
Infor-mation Retrieval, 9(2):191?206.M.
Surdeanu, M. Ciaramita, and H. Zaragoza.
2008.Learning to rank answers on large online qa collec-tions.
In Proceedings of ACL, pages 719?727.Y.W.
Teh, M.I.
Jordan, M.J. Beal, and D.M.
Blei.2006.
Hierarchical dirichlet processes.
Journal ofthe American Statistical Association, 101(476):1566?1581.P.D.
Turney.
2000.
Learning algorithms for keyphraseextraction.
Information Retrieval, 2(4):303?336.F.Y.
Wang, K.M.
Carley, D. Zeng, and W. Mao.
2007.Social computing: From social informatics to socialintelligence.
IEEE Intelligent Systems, 22(2):79?83.R.
Wetzker, C. Zimmermann, C. Bauckhage, and S. Al-bayrak.
2010.
I tag, you tag: translating tags foradvanced user models.
In Proceedings of WSDM,pages 71?80.X.
Xue, J. Jeon, and W.B.
Croft.
2008.
Retrieval modelsfor question and answer archives.
In Proceedings ofSIGIR, pages 475?482.Y.
Yanbe, A. Jatowt, S. Nakamura, and K. Tanaka.
2007.Can social bookmarking enhance search in the web?In Proceedings of JCDL, pages 107?116.S.
Zhao, H. Wang, X. Lan, and T. Liu.
2010a.
Leverag-ing multiple mt engines for paraphrase generation.
InProceedings of COLING, pages 1326?1334.S.
Zhao, H. Wang, and T. Liu.
2010b.
Paraphrasing withsearch engine query logs.
In Proceedings of COLING,pages 1317?1325.T.C.
Zhou, H. Ma, M.R.
Lyu, and I.
King.
2010.UserRec: A user recommendation framework in socialtagging systems.
In Proceedings of AAAI, pages1486?1491.1588
