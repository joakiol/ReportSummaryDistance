Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 934?944,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsAutomated Whole Sentence Grammar Correction Using a Noisy ChannelModelY.
Albert ParkDepartment of Computer Science and Engineering9500 Gilman DriveLa Jolla, CA 92037-404, USAyapark@ucsd.eduRoger LevyDepartment of Linguistics9500 Gilman DriveLa Jolla, CA 92037-108, USArlevy@ucsd.eduAbstractAutomated grammar correction techniqueshave seen improvement over the years, butthere is still much room for increased perfor-mance.
Current correction techniques mainlyfocus on identifying and correcting a specifictype of error, such as verb form misuse orpreposition misuse, which restricts the correc-tions to a limited scope.
We introduce a noveltechnique, based on a noisy channel model,which can utilize the whole sentence contextto determine proper corrections.
We showhow to use the EM algorithm to learn the pa-rameters of the noise model, using only a dataset of erroneous sentences, given the properlanguage model.
This frees us from the bur-den of acquiring a large corpora of correctedsentences.
We also present a cheap and effi-cient way to provide automated evaluation re-sults for grammar corrections by using BLEUand METEOR, in contrast to the commonlyused manual evaluations.1 IntroductionThe process of editing written text is performed byhumans on a daily basis.
Humans work by firstidentifying the writer?s intent, and then transform-ing the text so that it is coherent and error free.
Theycan read text with several spelling errors and gram-matical errors and still easily identify what the au-thor originally meant to write.
Unfortunately, cur-rent computer systems are still far from such ca-pabilities when it comes to the task of recogniz-ing incorrect text input.
Various approaches havebeen taken, but to date it seems that even manyspell checkers such as Aspell do not take contextinto consideration, which prevents them from find-ing misspellings which have the same form as validwords.
Also, current grammar correction systemsare mostly rule-based, searching the text for de-fined types of rule violations in the English gram-mar.
While this approach has had some success infinding various grammatical errors, it is confined tospecifically defined errors.In this paper, we approach this problem by mod-eling various types of human errors using a noisychannel model (Shannon, 1948).
Correct sentencesare produced by a predefined generative proba-bilistic model, and lesioned by the noise model.We learn the noise model parameters using anexpectation-maximization (EM) approach (Demp-ster et al, 1977; Wu, 1983).
Our model allows usto deduce the original intended sentence by lookingfor the the highest probability parses over the entiresentence, which leads to automated whole sentencespelling and grammar correction based on contex-tual information.In Section 2, we discuss previous work, followedby an explanation of our model and its implementa-tion in Sections 3 and 4.
In Section 5 we presenta novel technique for evaluating the task of auto-mated grammar and spelling correction, along withthe data set we collected for our experiments.
Ourexperiment results and discussion are in Section 6.Section 7 concludes this paper.2 BackgroundMuch of the previous work in the domain of auto-mated grammar correction has focused on identi-934fying grammatical errors.
Chodorow and Leacock(2000) used an unsupervised approach to identify-ing grammatical errors by looking for contextualcues in a ?2 word window around a target word.To identify errors, they searched for cues which didnot appear in the correct usage of words.
Eeg-olofsson and Knutsson (2003) used rule-based meth-ods to approach the problem of discovering preposi-tion and determiner errors of L2 writers, and var-ious classifier-based methods using Maximum En-tropy models have also been proposed (Izumi et al,2003; Tetreault and Chodorow, 2008; De Felice andPulman, 2008).
Some classifier-based methods canbe used not only to identify errors, but also to deter-mine suggestions for corrections by using the scoresor probabilities from the classifiers for other possi-ble words.
While this is a plausible approach forgrammar correction, there is one fundamental dif-ference between this approach and the way humansedit.
The output scores of classifiers do not take intoaccount the observed erroneous word, changing thetask of editing into a fill-in-the-blank selection task.In contrast, editing makes use of the writer?s erro-neous word which often encompasses informationneccessary to correctly deduce the writer?s intent.Generation-based approaches to grammar correc-tion have also been taken, such as Lee and Sen-eff (2006), where sentences are paraphrased into anover-generated word lattice, and then parsed to se-lect the best rephrasing.
As with the previously men-tioned approaches, these approaches often have thedisadvantage of ignoring the writer?s selected wordwhen used for error correction instead of just errordetection.Other work which relates to automated grammarcorrection has been done in the field of machinetranslation.
Machine translation systems often gen-erate output which is grammatically incorrect, andautomated post-editing systems have been created toaddress this problem.
For instance, when translat-ing Japanese to English, the output sentence needsto be edited to include the correct articles, since theJapanese language does not contain articles.
Knightand Chander (1994) address the problem of select-ing the correct article for MT systems.
These typesof systems could also be used to facilitate grammarcorrection.While grammar correction can be used on the out-put of MT systems, note that the task of grammarcorrection itself can also be thought of as a machinetranslation task, where we are trying to ?translate?
asentence from an ?incorrect grammar?
language toa ?correct grammar?
language.
Under this idea, theuse of statistical machine translation techniques tocorrect grammatical errors has also been explored.Brockett et al (2006) uses phrasal SMT techniquesto identify and correct mass noun errors of ESL stu-dents.
De?silets and Hermet (2009) use a round-triptranslation from L2 to L1 and back to L2 to cor-rect errors using an SMT system, focusing on errorswhich link back to the writer?s native language.Despite the underlying commonality between thetasks of machine translation and grammar correc-tion, there is a practical difference in that the fieldof grammar correction suffers from a lack of goodquality parallel corpora.
While machine translationhas taken advantage of the plethora of translateddocuments and books, from which various corporahave been built, the field of grammar correction doesnot have this luxury.
Annotated corpora of gram-matical errors do exist, such as the NICT JapaneseLearner of English corpus and the Chinese LearnerEnglish Corpus (Shichun and Huizhong, 2003), butthe lack of definitive corpora often makes obtainingdata for use in training models a task within itself,and often limits the approaches which can be taken.Using classification or rule-based systems forgrammatical error detection has proven to be suc-cessful to some extent, but many approaches are notsufficient for real-world automated grammar correc-tion for various of reasons.
First, as we have alreadymentioned, classification systems and generation-based systems do not make full use of the givendata when trying to make a selection.
This limits thesystem?s ability to make well-informed edits whichmatch the writer?s original intent.
Second, many ofthe systems start with the assumption that there isonly one type of error.
However, ESL students oftenmake several combined mistakes in one sentence.These combined mistakes can throw off error detec-tion/correction schemes which assume that the restof the sentence is correct.
For example, if a studenterroneously writes ?much poeple?
instead of ?manypeople?, a system trying to correct ?many/much?
er-rors may skip correction of much to many because itdoes not have any reference to the misspelled word935?poeple?.
Thus there are advantages in looking at thesentence as a whole, and creating models which al-low several types of errors to occur within the samesentence.
We now present our model, which sup-ports the addition of various types of errors into onecombined model, and derives its response by usingthe whole of the observed sentence.3 Base ModelOur noisy channel model consists of two main com-ponents, a base language model and a noise model.The base language model is a probabilistic lan-guage model which generates an ?error-free?
sen-tence1 with a given probability.
The probabilisticnoise model then takes this sentence and decideswhether or not to make it erroneous by insertingvarious types of errors, such as spelling mistakes,article choice errors, wordform choice errors, etc.,based on its parameters (see Figure 1 for example).Using this model, we can find the posterior proba-bility p(Sorig|Sobs) using Bayes rule where Sorig isthe original sentence created by our base languagemodel, and Sobs is the observed erroneous sentence.p(Sorig|Sobs) =p(Sobs|Sorig)p(Sorig)p(Sobs)For the language model, we can use variousknown probabilistic models which already have de-fined methods for learning the parameters, such asn-gram models or PCFGs.
For the noise model, weneed some way to learn the parameters for the mis-takes that a group of specified writers (such as Ko-rean ESL students) make.
We address this issue inSection 4.Using this model, we can find the highest likeli-hood error-free sentence for an observed output sen-tence by tracing all possible paths from the languagemodel through the noise model and ending in the ob-served sentence as output.4 ImplementationTo actually implement our model, we use a bigrammodel for the base language model, and variousnoise models which introduce spelling errors, ar-ticle choice errors, preposition choice errors, etc.1In reality, the language model will most likely produce sen-tences with errors as seen by humans, but from the modelingperspective, we assume that the language model is a perfect rep-resentation of the language for our task.Figure 1: Example of noisy channel modelAll models are implemented using weighted finite-state tranducers (wFST).
For operations on the wF-STs, we use OpenFST (Allauzen et al, 2007), alongwith expectation semiring code supplied by MarkusDryer for Dreyer et al (2008).4.1 Base language modelThe base language model is a bigram model imple-mented by using a weighted finite-state transducer(wFST).
The model parameters are learned fromthe British National Corpus modified to use Amer-ican English spellings with Kneser-Ney smoothing.To lower our memory usage, only bigrams whosewords are found in the observed sentences, or aredetermined to be possible candidates for the correctwords of the original sentence (due to the noise mod-els) are used.
While we use a bigram model here forsimplicity, any probabilistic language model havinga tractable intersection with wFSTs could be used.For the bigram model, each state in the wFST rep-resents a bigram context, except the end state.
Thearcs of the wFST are set so that the weight is the bi-gram probability of the output word given the con-text specified by the from state, and the output wordis a word of the vocabulary.
Thus, given a set of nwords in the vocabulary, the language model wFSThad one start state, from which n arcs extended toeach of their own context states.
From each of thesenodes, n + 1 arcs extend to each of the n contextstates and the end state.
Thus the number of statesin the language model is n + 2 and the number ofarcs is O(n2).4.2 Noise modelsFor our noise model, we created a weighted finite-state transducer (wFST) which accepts error-free in-put, and outputs erroneous sentences with a spec-ified probability.
To model various types of humanerrors, we created several different noise models and936Figure 2: Example of noise modelcomposed them together, creating a layered noisemodel.
The noise models we implement are spellingerrors, article choice errors, preposition choice er-rors, and insertion errors, which we will explain inmore detail later in this section.The basic design of each noise wFST starts withan initial state, which is also the final state of thewFST.
For each word found in the language model,an arc going from the initial state to itself is created,with the input and output values set as the word.These arcs model the case of no error being made.In addition to these arcs, arcs representing predictionerrors are also inserted.
For example, in the articlechoice error model, an arc is added for each possible(input, output) article pair, such as a:an for makingthe mistake of writing an instead of a.
The weightsof the arcs are the probabilities of introducing errors,given the input word from the language model.
Forexample, the noise model shown in Figure 2 showsa noise model in which a will be written correctlywith a probability of 0.9, and will be changed to anor the with probabilities 0.03 and 0.07, respectively.For this model to work correctly, the setting of theprobabilities for each error is required.
How this isdone is explained in Section 4.3.4.2.1 Spelling errorsThe spelling error noise model accounts forspelling errors made by writers.
For spelling er-rors, we allowed all spelling errors which werea Damerau-Levenshtein distance of 1 (Damerau,1964; Levenshtein, 1966).
While allowing a DL dis-tance of 2 or higher may likely have better perfor-mance, the model was constrained to a distance of 1due to memory constraints.
We specified one param-eter ?n for each possible word length n. This param-eter is the total probability of making a spelling errorfor a given word length.
For each word length wedistributed the probability of each possible spellingerror equally.
Thus for word length n, we haven deletion errors, 25n substitution errors, n ?
1transposition errors, and 26(n + 1) insertion er-rors, and the probability for each possible error is?nn+25n+n?1+26(n+1) .
We set the maximum wordlength for spelling errors to 22, giving us 22 param-eters.4.2.2 Article choice errorsThe article choice error noise model simulates in-correct selection of articles.
In this model we learnn(n?1) parameters, one for each article pair.
Sincethere are only 3 articles (a, an, the), we only have 6parameters for this model.4.2.3 Preposition choice errorsThe preposition choice error noise model simu-lates incorrect selection of prepositions.
We takethe 12 most commonly misused prepositions by ESLwriters (Gamon et al, 2009) and specify one param-eter for each preposition pair, as we do in the articlechoice error noise model, giving us a total of 132parameters.4.2.4 Wordform choice errorsThe wordform choice error noise model simulateschoosing the incorrect wordform of a word.
For ex-ample, choosing the incorrect tense of a verb (e.g.went?go), or the incorrect number marking on anoun or verb (e.g.
are?is) would be a part of thismodel.
This error model has one parameter for everynumber of possible inflections, up to a maximum of12 inflections, giving us 12 parameters.
The param-eter is the total probability of choosing the wronginflection of a word, and the probability is spreadevenly between each possible inflection.
We usedCELEX (Baayen et al, 1995) to find all the possiblewordforms of each observed word.4.2.5 Word insertion errorsThe word insertion error model simulates the ad-dition of extraneous words to the original sentence.We create a list of words by combining the prepo-sitions and articles found in the article choice andpreposition choice errors.
We assume that the wordson the list have a probability of being inserted erro-neously.
There is a parameter for each word, which937is the probability of that word being inserted.
Thuswe have 15 parameters for this noise model.4.3 Learning noise model parametersTo achieve maximum performance, we wish to learnthe parameters of the noise models.
If we had alarge set of erroneous sentences, along with a hand-annotated list of the specific errors and their correc-tions, it would be possible to do some form of super-vised learning to find the parameters.
We looked atthe NICT Japanese Learner of English (JLE) corpus,which is a corpus of transcripts of 1,300 Japaneselearners?
English oral proficiency interview.
Thiscorpus has been annotated using an error tagset(Izumi et al, 2004).
However, because the JLE cor-pus is a set of transcribed sentences, it is in a differ-ent domain from our task.
The Chinese Learner En-glish Corpus (CLEC) contains erroneous sentenceswhich have been annotated, but the CLEC corpushad too many manual errors, such as typos, as wellas many incorrect annotations, making it very diffi-cult to automate the processing.
Many of the correc-tions themselves were also incorrect.
We were notable to find of a set of annotated errors which fit ourtask, nor are we aware that such a set exists.
Instead,we collected a large data set of possibly erroneoussentences from Korean ESL students (Section 5.1).Since these sentences are not annotated, we need touse an unsupervised learning method to learn our pa-rameters.To learn the parameters of the noise models, weassume that the collected sentences are random out-put of our model, and train our model using theEM algorithm.
This was done by making use ofthe V -expectation semiring (Eisner, 2002).
TheV -expectation semiring is a semiring in which theweight is defined as R?0 ?
V , where R can be usedto keep track of the probability, and V is a vectorwhich can be used to denote arc traversal counts orfeature counts.
The weight for each of the arcs in thenoise models was set so that the real value was theprobability and the vector V denoted which choice(having a specified error or not) was made by select-ing the arc.
We create a generative language-noisemodel by composing the language model wFSTwiththe noise model wFSTs, as shown in Figure 3.
Byusing the expectation semiring, we can keep track ofthe probability of each path going over an erroneous01a:a/0.62an:an/0.43cat:cat/14ear:ear/15?:?/1?
:?/10a:a/0.95[0.95,0]a:an/0.05[0,0.05]an:an/1cat:cat/1ear:ear/10 1a:a/0.57[0.57,0]a:an/0.03[0,0.03]2an:an/0.43cat:cat/14ear:ear/15?:?/1?
:?/1Figure 3: Example of language model (top) and noisemodel (middle) wFST composition.
The vector of theV -expectation semiring weight is in brackets.
The firstvalue of the vector denotes no error being made on writ-ing ?a?
and the second value denotes the error of writing?an?
instead of ?a?arc or non-erroneous arc.Once our model is set up for the E step using theinitial parameters, we must compute the expectednumber of noise model arc traversals for use in cal-culating our new parameters.
To do this, we need tofind all possible paths resulting in the observed sen-tence as output, for each observed sentence.
Then,for each possible path, we need to calculate the prob-ability of the path given the output sentence, and getthe expected counts of going over each erroneousand error-free arc to learn the parameters of the noisemodel.
To find a wFST with just the possible pathsfor each observed sentence, we can compose thelanguage-noise wFST with the observed sentencewFST.
The observed sentence wFST is created inthe following manner.
Given an observed sentence,an initial state is created.
For each word in the sen-tence, in the order appearing in the sentence, a newstate is added, and an arc is created going from thepreviously added state to the newly added state.
Thenew arc takes the observed word as input and alsouses it as output.
The weight/probability for eacharc is set to 1.
Composing the sentence wFST withthe language-noise wFST has the effect of restrictingthe new wFST to only have sentences which out-put the observed sentence from the language-noisewFST.
We now have a new wFST where all validpaths are the paths which can produce the observed938sentence.
To find the total weight of all paths, wefirst change all input and output symbols into theempty string.
Since all arcs in this wFST are ep-silon arcs, we can use the epsilon-removal operation(Mohri, 2002), which will reduce the wFST to onestate with no arcs.
This operation combines the to-tal weight of all paths into the final weight of thesole state, giving us the total expectation value forthat sentence.
By doing this for each sentence, andadding the expectation values for each sentence, wecan easily compute the expectation step, from whichwe can find the maximizing parameters and updateour parameters accordingly.4.4 Finding the maximum likelihood correctionOnce the parameters are learned, we can use ourmodel to find the maximum likelihood error-freesentence.
This is done by again creating the lan-guage model and noise model with the learned pa-rameters, but this time we set the weights of thenoise model to just the probabilities, using the logsemiring, since we do not need to keep track of ex-pected values.
We also set the language model inputfor each arc to be the same word as the output, in-stead of using an empty string.
Once again, we com-pose the language model with the noise models.
Wecreate a sentence wFST using the observed sentencewe wish to correct, the same way the observed sen-tence wFST for training was created.
This is nowcomposed with the language-noise wFST.
Now allwe need to do is find the shortest path (when usingminus-log probabilities) of the new wFST, and theinput to that path will be our corrected sentence.5 ExperimentWe now present the data set and evaluation tech-nique used for our experiments.5.1 Data SetTo train our noise models, we collected around25,000 essays comprised of 478,350 sentences writ-ten by Korean ESL students preparing for theTOEFL writing exam.
These were collected fromopen web postings by Korean ESL students ask-ing for advice on their writing samples.
In orderto automate the process, a program was written todownload the posts, and discard the posts that weredeemed too short to be TOEFL writing samples.Also discarded were the posts that had a ?[re?
or?re..?
in the title.
Next, all sentences containingKorean were removed, after which some characterswere changed so that they were in ASCII form.
Theremaining text was separated into sentences solelyby punctuation marks ., !, and ?.
This resulted in the478,350 sentences stated above.
Due to the process,some of the sentences collected are actually sen-tence fragments, where punctuation had been mis-used.
For training and evaluation purposes, the dataset was split into a test set with 504 randomly se-lected sentences, an evaluation set of 1017 randomlyselected sentences, and a training set composed ofthe remaining sentences.5.2 Evaluation techniqueIn the current literature, grammar correction tasksare often manually evaluated for each output cor-rection, or evaluated by taking a set of proper sen-tences, artificially introducing some error, and see-ing how well the algorithm fixes the error.
Man-ual evaluation of automatic corrections may be thebest method for getting a more detailed evaluation,but to do manual evaluation for every test output re-quires a large amount of human resources, in termsof both time and effort.
In the case where artificiallesioning is introduced, the lesions may not alwaysreflect the actual errors found in human data, andit is difficult to replicate the actual tendency of hu-mans to make a variety of different mistakes in asingle sentence.
Thus, this method of evaluation,which may be suitable for evaluating the correctionperformance of specific grammatical errors, wouldnot be fit for evaluating our model?s overall perfor-mance.
For evaluation of the given task, we haveincorporated evaluation techniques based on currentevaluation techniques used in machine translation,BLEU (Papineni et al, 2002) and METEOR (Lavieand Agarwal, 2007).Machine translation addresses the problem ofchanging a sentence in one language to a sentence ofanother.
The task of correcting erroneous sentencescan also be thought of as translating a sentence froma given language A, to another language B, where Ais a broken language, and B is the correct language.Under this context, we can apply machine trans-lation evaluation techniques to evaluate the perfor-mance of our system.
Our model?s sentence correc-939tions can be thought of as the output translation to beevaluated.
In order to use BLEU and METEOR, weneed to have reference translations on which to scoreour output.
As we have already explained in section5.1, we have a collection of erroneous sentences, butno corrections.
To obtain manually corrected sen-tences for evaluation, the test and evaluation set sen-tences and were put on Amazon Mechanical Turk asa correction task.
Workers residing in the US wereasked to manually correct the sentences in the twosets.
Workers had a choice of selecting ?Impossi-ble to understand?, ?Correct sentence?, or ?Incorrectsentence?, and were asked to correct the sentencesso no spelling errors, grammatical errors, or punctu-ation errors were present.
Each sentence was givento 8 workers, giving us a set of 8 or fewer correctedsentences for each erroneous sentence.
We askedworkers not to completely rewrite the sentences, butto maintain the original structure as much as pos-sible.
Each hit was comprised of 6 sentences, andthe reward for each hit was 10 cents.
To ensure thequality of our manually corrected sentences, a nativeEnglish speaker research assistant went over each ofthe ?corrected?
sentences and marked them as cor-rect or incorrect.
We then removed all the incorrect?corrections?.Using our manually corrected reference sen-tences, we evaluate our model?s correction perfor-mance using METEOR and BLEU.
Since METEORand BLEU are fully automated after we have our ref-erence translations (manual corrections), we can runevaluation on our tests without any need for furthermanual input.
While these two evaluation methodswere created for machine translation, they also havethe potential of being used in the field of grammarcorrection evaluation.
One difference between ma-chine translation and our task is that finding the rightlemma is in itself something to be rewarded in MT,but is not sufficient for our task.
In this respect, eval-uation of grammar correction should be more strict.Thus, for METEOR, we used the ?exact?
module forevaluation.To validate our evaluation method, we ran a sim-ple test by calculating the METEOR and BLEUscores for the observed sentences, and comparedthem with the scores for the manually corrected sen-tences, to test for an expected increase.
The scoresfor each correction were evaluated using the set ofMETEOR BLEUOriginal ESL sentences 0.8327 0.7540Manual corrections 0.9179 0.8786Table 1: BLEU and METEOR scores for ESL sentencesvs manual corrections on 100 randomly chosen sentencesMETEOR BLEUAspell 0.824144 0.719713Spelling noise model 0.825001 0.722383Table 2: Aspell vs Spelling noise modelcorrected sentences minus the correction sentencebeing evaluated.
For example, let us say we have theobserved sentence o, and correction sentences c1, c2,c3 and c4 from Mechanical Turk.
We run METEORand BLEU on both o and c1 using c2, c3 and c4 asthe reference set.
We repeat the process for o and c2,using c1, c3 and c4 as the reference, and so on, untilwe have runMETEOR and BLEU on all 4 correctionsentences.
With a set of 100 manually labeled sen-tences, the average METEOR score for the ESL sen-tences was 0.8327, whereas the corrected sentenceshad an average score of 0.9179.
For BLEU, the av-erage scores were 0.7540 and 0.8786, respectively,as shown in Table 1.
Thus, we have confirmed thatthe corrected sentences score higher than the ESLsentence.
It is also notable that finding correctionsfor the sentences is a much easier task than findingvarious correct translations, since the task of editingis much easier and can be done by a much larger setof qualified people.6 ResultsFor our experiments, we used 2000 randomly se-lected sentences for training, and a set of 1017 an-notated sentences for evaluation.
We also set asidea set of 504 annotated sentences as a developmentset.
With the 2000 sentence training, the perfor-mance generally converged after around 10 itera-tions of EM.6.1 Comparison with AspellTo check how well our spelling error noise model isdoing, we compared the results of using the spellingerror noise model with the output results of usingthe GNU Aspell 0.60.6 spelling checker.
Since we940METEOR ?
?
BLEU ?
?ESL Baseline 0.821000 0.715634Spelling only 0.825001 49 5 0.722383 53 8Spelling, Article 0.825437 55 6 0.723022 59 9Spelling, Preposition 0.824157 52 17 0.720702 55 19Spelling, Wordform 0.825654 81 25 0.723599 85 27Spelling, Insertion 0.825041 52 5 0.722564 56 8Table 3: Average evaluation scores for various noise models run on 1017 sentences, along with counts of sentenceswith increased (?)
and decreased (?)
scores.
All improvements are significant by the binomial test at p < 0.001are using METEOR and BLEU for our evaluationmetric, we needed to get a set of corrected sentencesfor using Aspell.
Aspell lists the suggested spellingcorrections of misspelled words in a ranked order, sowe replaced each misspelled word found by Aspellwith the word with the highest rank (lowest score)for the Aspell corrections.
One difference betweenAspell and our model is that Aspell only correctswords which do not appear in the dictionary, whileour method looks at all words, even those found inthe dictionary.
Thus our model can correct wordswhich look correct by themselves, but seem to beincorrect due to the bigram context.
Another differ-ence is that Aspell has the capability to split words,whereas our model does not allow the insertion ofspaces.
A comparison of the scores is shown in Ta-ble 2.
We can see that our model has better per-formance, due to better word selection, despite theadvantage that Aspell has by using phonological in-formation to find the correct word, and the disadvan-tage that our model is restricted to spellings whichare within a Damerau-Levenstein distance of 1.
Thisis due to the fact that our model is context-sensitive,and can use other information in addition to the mis-spelled word.
For example, the sentence ?In contast,high prices of products would be the main reasonfor dislike.?
was edited in Aspell by changing ?con-tast?
to ?contest?, while our model correctly selected?contrast?.
The sentence ?So i can reach the theaterin ten minuets by foot?
was not edited by Aspell, butour model changed ?minuets?
to ?minutes?.
Anotherdifference that can be seen by looking through theresults is that Aspell changes every word not foundin the dictionary, while our algorithm allows wordsit has not seen by treating them as unknown tokens.Since we are using smoothing, these tokens are leftin place if there is no other high probability bigramto take its place.
This helps leave intact the propernouns and words not in the vocabulary.6.2 Noise model performance and outputOur next experiment was to test the performance ofour model on various types of errors.
Table 3 showsthe BLEU and METEOR scores of our various errormodels, along with the number of sentences achiev-ing improved and reduced scores.
As we have al-ready seen in section 6.1, the spelling error modelincreases the evaluation scores from the ESL base-line.
Adding in the article choice error model andthe word insertion error models in addition to thespelling error noise model increases the BLEU scoreperformance of finding corrections.
Upon observ-ing the outputs of the corrections on the develop-ment set, we found that the corrections changinga to an were all correct.
Changes between a andthe were sometimes correct, and sometimes incor-rect.
For example, ?which makes me know a exis-tence about?
was changed to ?which makes me knowthe existence about?, ?when I am in a trouble.?
waschanged to ?when I am in the trouble.
?, and ?manypeople could read a nonfiction books?
was changedto ?many people could read the nonfiction books?.For the last correction, the manual corrections allchanged the sentence to contain ?many people couldread a nonfiction book?, bringing down the evalu-ation score.
Overall, the article corrections whichwere being made seemed to change the sentence forthe better, or left it at the same quality.The preposition choice error model decreased theperformance of the system overall.
Looking throughthe development set corrections, we found that manycorrect prepositions were being changed to incorrectprepositions.
For example, in the sentence ?Distrustabout desire between two have been growing in their941relationship.
?, about was changed to of, and in ?Astime goes by, ...?, by was changed to on.
Since thesechanges were not found in the manual corrections,the scores were decreased.For wordform errors, the BLEU and METEORscores both increased.
While the wordform choicenoise model had the most sentences with increasedscores, it also had the most sentences with decreasedscores.
Overall, it seems that to correct wordformerrors, more context than just the preceding and fol-lowing word are needed.
For example, in the sen-tence ?There are a lot of a hundred dollar phones inthe market.
?, phones was changed to phone.
To inferwhich is correct, you would have to have access tothe previous context ?a lot of?.
Another example is?..., I prefer being indoors to going outside ...?, wheregoing was changed to go.
These types of cases illus-trate the restrictions of using a bigram model as thebase language model.The word insertion error model was restricted toarticles and 12 prepositions, and thus did not makemany changes, but was correct when it did.
Onething to note is that since we are using a bigrammodel for the language model, the model itself isbiased towards shorter sentences.
Since we only in-cluded words which were needed when they wereused, we did not run into problems with this bias.When we tried including a large set of commonlyused words, we found that many of the words werebeing erased because of the bigrams models proba-bilistic preference for shorter sentences.6.3 Limitations of the bigram language modelBrowsing through the development set data, wefound that many of our model?s incorrect ?correc-tions?
were the result of using a bigram model as ourlanguage model.
For example, ?.., I prefer being in-doors to going outside in that...?
was changed to ?..,I prefer being indoors to go outside in that...?.
Fromthe bigram model, the probabilities p(go to) andp(outside go) are both higher than p(going to) andp(outside going), respectively.
To infer that goingis actually correct, we would need to know the previ-ous context, that we are comparing ?being indoors?to ?going outside?.
Unfortunately, since we are usinga bigram model, this is not possible.
These kind oferrors are found throughout the corrections.
It seemslikely that making use of a language model whichcan keep track of this kind of information would in-crease the performance of the correction model bypreventing these kinds of errors.7 Conclusion and future workWe have introduced a novel way of finding grammarand spelling corrections, which uses the EM algo-rithm to train the parameters of our noisy channelapproach.
One of the benefits of this approach is thatit does not require a parallel set of erroneous sen-tences and their corrections.
Also, our model is notconfined to a specific error, and various error modelsmay be added on.
For training our noise model, allthat is required is finding erroneous data sets.
De-pending on which domain you are training on, thiscan also be quite feasible as we have shown by ourcollection of Korean ESL students?
erroneous writ-ing samples.
Our data set could have been for ESLstudents of any native language, or could also be adata set of other groups such as young native En-glish speakers, or the whole set of English speakersfor grammar correction.
Using only these data sets,we can train our noisy channel model, as we haveshown using a bigram language model, and a wFSTfor our noise model.
We have also shown how to useweighted finite-state transducers and the expectationsemiring, as well as wFST algorithms implementedin OpenFST to train the model using EM.
For evalu-ation, we have introduced a novel way of evaluatinggrammar corrections, using MT evaluation methods,which we have not seen in other grammar correctionliterature.
The produced corrections show the re-strictions of using a bigram language model.
For fu-ture work, we plan to use a more accurate languagemodel, and add more types of complex error models,such as word deletion and word ordering error mod-els to improve performance and address other typesof errors.AcknowledgmentsWe are grateful to Randy West for his input andassistance, and to Markus Dreyer who provided uswith his expectation semiring code.
We would alsolike to thank the San Diego Supercomputer Centerfor use of their DASH high-performance computingsystem.942ReferencesAllauzen, C., Riley, M., Schalkwyk, J., Skut, W.,and Mohri, M. (2007).
OpenFst: A generaland efficient weighted finite-state transducer li-brary.
In Proceedings of the Ninth InternationalConference on Implementation and Applicationof Automata, (CIAA 2007), volume 4783 of Lec-ture Notes in Computer Science, pages 11?23.Springer.
http://www.openfst.org.Baayen, H. R., Piepenbrock, R., and Gulikers, L.(1995).
The CELEX Lexical Database.
Release 2(CD-ROM).
Linguistic Data Consortium, Univer-sity of Pennsylvania, Philadelphia, Pennsylvania.Brockett, C., Dolan, W. B., and Gamon, M. (2006).Correcting ESL errors using phrasal SMT tech-niques.
In Proceedings of the 21st InternationalConference on Computational Linguistics and the44th annual meeting of the Association for Com-putational Linguistics, ACL-44, pages 249?256,Morristown, NJ, USA.
Association for Computa-tional Linguistics.Chodorow, M. and Leacock, C. (2000).
An unsu-pervised method for detecting grammatical errors.In Proceedings of the 1st North American chapterof the Association for Computational Linguisticsconference, pages 140?147, San Francisco, CA,USA.
Morgan Kaufmann Publishers Inc.Damerau, F. J.
(1964).
A technique for computerdetection and correction of spelling errors.
Com-mun.
ACM, 7:171?176.De Felice, R. and Pulman, S. G. (2008).
A classifier-based approach to preposition and determiner er-ror correction in L2 English.
In Proceedings ofthe 22nd International Conference on Computa-tional Linguistics - Volume 1, COLING ?08, pages169?176, Morristown, NJ, USA.
Association forComputational Linguistics.Dempster, A. P., Laird, N. M., and Rubin, D.
B.(1977).
Maximum likelihood from incompletedata via the EM algorithm.
Journal of theRoyal Statistical Society.
Series B (Methodolog-ical), 39(1):pp.
1?38.De?silets, A. and Hermet, M. (2009).
Using auto-matic roundtrip translation to repair general errorsin second language writing.
In Proceedings of thetwelfth Machine Translation Summit, MT SummitXII, pages 198?206.Dreyer, M., Smith, J., and Eisner, J.
(2008).
Latent-variable modeling of string transductions withfinite-state methods.
In Proceedings of the 2008Conference on Empirical Methods in NaturalLanguage Processing, pages 1080?1089, Hon-olulu, Hawaii.
Association for ComputationalLinguistics.Eeg-olofsson, J. and Knutsson, O.
(2003).
Au-tomatic grammar checking for second languagelearners - the use of prepositions.
In In Nodalida.Eisner, J.
(2002).
Parameter estimation for prob-abilistic finite-state transducers.
In Proceedingsof the 40th Annual Meeting of the Associationfor Computational Linguistics (ACL), pages 1?8,Philadelphia.Gamon, M., Leacock, C., Brockett, C., Dolan,W.
B., Gao, J., Belenko, D., and Klementiev,A.
(2009).
Using statistical techniques and websearch to correct ESL errors.
In Calico Journal,Vol 26, No.
3, pages 491?511, Menlo Park, CA,USA.
CALICO Journal.Izumi, E., Uchimoto, K., and Isahara, H. (2004).The NICT JLE corpus exploiting the languagelearnersspeech database for research and educa-tion.
In International Journal of the Computer, theInternet and Management, volume 12(2), pages119?125.Izumi, E., Uchimoto, K., Saiga, T., Supnithi, T., andIsahara, H. (2003).
Automatic error detection inthe Japanese learners?
English spoken data.
InProceedings of the 41st Annual Meeting on Asso-ciation for Computational Linguistics - Volume 2,ACL ?03, pages 145?148, Morristown, NJ, USA.Association for Computational Linguistics.Knight, K. and Chander, I.
(1994).
Automatedpostediting of documents.
In Proceedings of thetwelfth national conference on Artificial intelli-gence (vol.
1), AAAI ?94, pages 779?784, MenloPark, CA, USA.
American Association for Artifi-cial Intelligence.Lavie, A. and Agarwal, A.
(2007).
Meteor: an au-tomatic metric for MT evaluation with high levelsof correlation with human judgments.
In StatMT?07: Proceedings of the Second Workshop on943Statistical Machine Translation, pages 228?231,Morristown, NJ, USA.
Association for Computa-tional Linguistics.Lee, J. and Seneff, S. (2006).
Automatic grammarcorrection for second-language learners.
In Pro-ceedings of Interspeech.Levenshtein, V. I.
(1966).
Binary codes capable ofcorrecting deletions, insertions, and reversals.
So-viet Physics Doklady, 10:707?710.Mohri, M. (2002).
Generic epsilon-removaland input epsilon-normalization algorithms forweighted transducers.
In International Journal ofFoundations of Computer Science 13, pages 129?143.Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.(2002).
Bleu: a method for automatic evaluationof machine translation.
In ACL, pages 311?318.Shannon, C. (1948).
A mathematical theory ofcommunications.
Bell Systems Technical Journal,27(4):623?656.Shichun, G. and Huizhong, Y.
(2003).
ChineseLearner English Corpus.
Shanghai Foreign Lan-guage Education Press.Tetreault, J. R. and Chodorow, M. (2008).
Theups and downs of preposition error detection inESL writing.
In Proceedings of the 22nd Interna-tional Conference on Computational Linguistics- Volume 1, COLING ?08, pages 865?872, Mor-ristown, NJ, USA.
Association for ComputationalLinguistics.Wu, C.-F. J.
(1983).
On the convergence propertiesof the EM algorithm.
Ann.
Statist., 11(1):95?103.944
