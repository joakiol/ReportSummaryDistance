Parsing and Case Analysis in TANKATERRY COPECK, SYLVAIN DELISLE, STAN SZPAKOWICZDepartment ofComputer ScienceUniversity of OttawaOttawa, Ontario, Canada, KIN 6N5{terry, sylvain, szpak}@csi.uottawa.caABSTRACTThe TANKA project seeks to build a model of atechnical domain by semi-automatically process-ing unedited English text that describes this do-main.
Each sentence is parsed and conceptualelements are extracted from the parse.
Conceptsare derived from the Case structure of a sentence,and added to a conceptual network that rep-resents knowledge about the domain.
TheDIPETr parser has a particularly broad coverageof English syntax; its newest version can alsoprocess entence fragments.
The HAIKU subsys-tem is responsible for user-assisted semanticinterpretation.
It contains aCase Analyzer modu-le that extracts phrases marking concepts fromthe parse and uses its past processing experienceto derive the most likely Case realizations ofeach with almost no a priori  semantic know-ledge.
The user must validate these selections.
Akey issue in our research is minimizing thenumber of interactions with the user by intelli-gently generating the alternatives offered.BACKGROUNDThis project is a long-term undertaking of theKnowledge Acquisition Lab.
Previously we pre-sented its overall design (Szpakowicz 1990), dis-cussed elements of the Conceptual KnowledgeProcessor (Szpakowicz & Koperczak 1990; Yang& Szpakowicz 1990, 1991a, 1991b), and de-scribed the parser and Case Analyzer (Delisle1990; Delisle & Szpakowicz 1991; Copeck et al1990).
This paper updates and summarizes thelast three publications.
TANKA (Text ANalysisfor Knowledge Acquisition) is implemented inQuintus Prolog on Sun workstations.THE DIPETT PARSERTANKA requires a broad-coverage parserbecause it uses a limited semantic model basedon Case relations, and domain-specific know-ledge is not available to it a priori.
Without richsemantics, yntax is the only basis for inferringmeaning.
In TANKA, the broader the parser'scoverage, the more accurate the ultimateknowledge representation can be.
This is inopposition to approaches in which semanticknowledge is fed in beforehand, and syntax islimited to restricted patterns or even justkeywords.
Our approach lies at the other end ofthe spectrum: we are concerned with realisticlarge-scale texts and need realistic syntacticcoverage.
This enables HAIKU, the interactivesemantic interpreter, to extract overt meaningfrom DIPETr's detailed parse trees, and helpsorganize interaction with the user.DIPETT (Domain-Independent Parser forEnglish Technical Texts) is a linguistic-theory-neutral parser with a broad surface-syntacticcoverage of English.
It handles most sentences inour unedited sample text, a guide to the fourthgeneration database language Quiz.
DIPETI"scoverage ncompasses very fundamental syn-tactic structure in the language, includingcoordination, and most syntactic phenomenaencountered in typical expository technical texts.The core of its grammar is based on general andNLP-oriented English grammars, in particular,Quirk et al (1985) and Winograd (1983).DIPETr's major components are a dictionary, alexical analyzer, a syntactic analyzer, a memo-rizing device with a helper mechanism, plus itsown trace mechanism.
An input is usually givento the lexical analyzer and then to the syntacticanalyzer', this makes for conceptually clear andeasily implemented models.
More than half ofthe parser's 5000 lines of code are DCG rules.
A15-word sentence can typically be processed in15 to 20 seconds CPU on a Sun SparcStation.Novel  features of DIPETT are a dynamicdictionary expansion facility, its memorizingdevice (well-formed substring table), a helper(error explanation mechanism), and an internaltrace mechanism for debugging.The parser's urface-syntactic dictionary con-tains most English function words.
It includes atable that associates legal adverbial particles withverbs (this is used to disambignate panicles andprepositions).
Another table contains wordgroups such as "as much as" or "even if" thatAcrEs DE COLING-92.
NANTES, 23-28 AOb~r 1992 1 0 0 S PROC.
OF COLING-92, NANTES.
AUG. 23-28, 1992usually play the same role as single functionwords.
The dictionary will be expanded withsemantic information when it is integrated withthe Case Analyzer.
The lexical analyzer builds alist of annotated words with the root form andthe syntactic parameters.
If the input contains aword for which the dictionary has no entry, thismodule allows the user to augment the dictionarydynamically.
Such temporary additions are savedon a file for future permanent addition.DIPETr 's  grammar ecognizes the followingmajor syntactic units: sentence (simple, complexand multiply-coordinated), question, verb phrase(simple and conjoined), verbal clause, comple-ment, subordinate clause, adverbial clause, nounphrase (simple and conjoined) and their substan-tive forms, that-clause, relative clause, trig-clause, to-infinitive clause, whether-if clause,noun phrase post-modifier (e.g.
appositive),prepositional phrase (simple and conjoined),noun pre- and post-modifier, determinative,adjectival phrase.The purpose of the memorizer is to minimize thereparsing of syntactic substructures that are re-considered on backtracking.
The helper showsthe user information that may help identify thereasons for an input's rejection.
Both features canbe switched on or off for the session.
These twomodules use notes--assertions that record essen-tial syntactic information about major well-formed substrings that constitute the preposit-ional, noun and verb phrases.
A note stores asubstring, its type and its syntactic structureproduced by the parser.
Corresponding DCGrules contain Prolog assertions invoked if theuser has activated the memorizer or the helper.Testing and fine-tuning a complex parser can bedifficult.
Prolog debugging facilities are oftencumbersome for logic grammars where it is onlyinteresting to know what rule is being examinedby the parser, for which part of the input string,and what has been successfully recognized.
Wehave therefore implemented our own trace trw~h-anism which employs trace instructions (acti-vated by a flag) inserted in all rules related toprepositional, noun and verb phrases.
The parserimplementor can activate and control the tracemechanism through amenu interface.Conjoined verb phrases and sentences are usuallyvery expensive to parse.
We have devised twolook-ahead mechanisms totreat co-ordination ef-ficiently.
These mechanisms check the lexicalcategories of tokens ahead in the input string.The f'trst looks for coordinated clauses, while thesecond checks inputs that are supposed tocontain at least one verb (such as the to-infinitiveclause).
This information is used by the parser toidentify potential joining-points for conjoinedsentences and to avoid applying rules that cannotsucceed.
The parser also handles elided modalsand auxiliaries in conjoined verb phrases.
Forexample, "John has printed the letters and readthe report" is analyzed as "\[\[John\] \[\[has printedthe letters\] and \[has printed the report\]\]\]".Scoping of negation and adverbs in conjoinedverbs is handled, too.
For example "John did notaccidentally print and read my personalmessages" is analyzed as "\[\[John\] \[\[did notaccidentally print\] and \[did not accidentallyread\]\] \[my personal messages\]\]".DIPE'I'F does not have access to semantic know-ledge, so prepositional phrase (PP) attachmentmust use syntax-based heuristics.
Two examples:an 'of' PP is attached to the preceding noun bydefault; if a PP which is not an initial modifieroccurs in a pre-verbal position, it is attached tothe noun (whatever the preposition may be).CURRENT WORK IN DIPETI'It is our experience that sooner or later an extra-grammatical or highly ambiguous input will en-gage the parser in an excessively lengthy compu-tation.
We must be able to deal with such ex-txeme situations because our knowledge acquisi-tion method requires finding, for any input, thefirst parse tree that is linguistically reasonable.The reshuffling of the tree's components i leftto HAIKU.
At present, we discontinue a parseoperation that exceeds the time allowed for asingle parse (specified by the user at the begin-ning of a session).
Timing-out in this mannercauses loss of information from a partiallyparsed sentence, but it is preferable to the user'swaiting unrealistically long for the system'sfeedback.
DIPE'I'I" also applies look-ahead andheuristics to fail unpromising partial parsesquickly (e.g.
it will not try verb phrase analysis ifthere is no verb).
This helps produce the firstreasonable parse tree as fast as possible.The ultimate goal of the TANKA system is toprocess free-form technical texts.
Texts oftencontain on-textual material such as tables or ex-mnples (e.g.
data, programs, results).
We assumeall non-textual elements have been removedfrom our source texts, but each removal leaves a"hole" behind.
Most holes are located betweensentences and do not affect the structure of thetext, but some cause fragments to appear in thetext.
Fragments are valid sub-structures ofEnglish sentences, such as "For example" in"For ~xample,  > SORT ON DATE JO INED D."Acids oE COLING-92.
Nnr, rES, 23-28 ^OOT 1992 1 0 0 9 l'aoC.
OF COLING-92~ N^rcrF.s, AUO.
23-28, 1992DIPETr can parse such fragments.Three areas of grammar are currently underactive development in DIPE'IT:1) References: the parser will be capable of re-solving simple references, in particular anaphora,on syntactic grounds alone (we mean referenceswhose resolution requires little or no semanticknowledge)---see Hobbs (1978).2) Topic and focus: the parser will maintainsome knowledge about opic and focus.
As a firstindication, a text's title should tell us about itstopic while the current input indicates focus; thiscould benefit the Conceptual KnowledgeProcessor in TANKA by tentatively relating thetopic to a cluster in the conceptual network.3) Paragraph parsing: the parser's default modeof operation is one sentence at a time.
Parsinglonger inputs, a number of consecutive sentencesCLASS CASE ABBR.PARTICIPANT123456SPACE789101112TIME1314151617CAUSALITY18192021QUALITY22232425262728Agent AGTBeneficiary BENFExperiencer EXPRInstrument INSTObject OBJRecipient RECPDirection DIRLocation_at LATLocation_from LFRMLocation_to LTOLocation_through LTRUOrientation ORNTFrequency FREQTime_at TATTime_from TFRMTime_to 'ITOTime_through qTRUCause CAUSContradiction CNTREffect EFFPurpose PURPAccompaniment ACMPContent CONTManner MANRMaterial MATRMeasure MEASOrder ORDValue VALFigure 1.
Cases Used in TANKAor even paragraphs, means much more elaborateprocessing than parsing single sentences.Nothing is gained by simply finding a sequenceof parse trees--one for each sentence, in order;,see Jensen (1989) for a similar statement.
Wehave plans for a more intelligent type of parsingthat would be able to summarize the contents ofthese longer inputs by highlighting the mainconceptual elements more closely related to thecurrent opic (see Zadrozny & Jensen (1991) fora theory of the paragraph).
Topic and focusinformation will probably help here.CASE ANALYSIS WITH LEARNINGIn TANKA, knowledge is expressed in terms ofentities engaged in acts that serve to link theminto a graph; see Sowa (1984) for a general dis-cussion of this type of representation.
This graphis the conceptual network that TANKA will grad-ually build for a technical text.
It is constructedfrom Case frames of verbs recognized in the sen-tence.
We have put together a set of Cases suit-able for our class of domains; it is inspired bylists found in Fillmore (1968), Bruce (1975),Grimes (1975), Cook (1979), Larson (1984) andSparck Jones & Boguraev (1987).
This set(Figure 1) is not entirely settled; we continue toreview the work of other authors and we are cur-renfly testing our selections against hose Somers(1987) presents in his Case grid.Case Analysis (CA) extracts the acts and Caseconstellations around them from the structureproduced by the parser on a sentence-by-sent-ence basis.
Only one parse is used, but thesystem will allow the user to override all itssuggestions.
Subsequent processing can adjustthe understanding of a sentence enough toencompass most alternative parses and only failsto cover situations when a word can be legiti-mately parsed twice as different parts of speech.Items extracted from the parse are mapped quitedirectly into Case structures.
A verb denotes anact.
A Case is marked most often by a prepos-ition or an adverb, and a noun or nominalization(marked by a preposition) serves as a Caseobject.
Initial processing of a parse tree identifieselements of interest; others such as nounmodifiers are not used by CA but are kept in therepresentation for the Conceptual KnowledgeProcessor.
Two questions must then be answeredfor each Case-Marker in TANKA: to which verbdoes it attach, and which Case does it realize?The HAIKU module does not attempt to answerthese questions itself, at least not in a definitiveway.
It asks the user to answer by selectingamong alternatives in a list, which may includeACRES DE COLING-92.
NANTES.
23-28 aOt\]T 1992 I 0 I 0 PROC.
OF COLING-92.
NANTES, AUG. 23-28.
1992syntactic elements from the original sentencecopied exactly, i l lustrative phrases andsentences, and possibly short descriptions of themeaning of Cases.
Our goal is to minimize thenumber of interactions the user must engage in togive the right answer.
This can be done by lettingall answers be specified in one interaction, andthat in turn is possible if HAIKU proposescorrect Case-Marker attachments and semanticsat the outset.
In practice a minimum of two in-teractions per complex sentence appear to benecessary, one to correctly link Case Markers toverbs and a second to validate Case Markersemantics for each verb.
Our work on HAIKUthus concentrates on ensuring it produces thecorrect configuration, preferably on the firstinteraction.Attachment of Case-Markers to verbs is inferredsolely from the parse structure.
Semantics couldhelp were they known in advance (a verb hasonly one Case of a given type) but semantic in-ference is also aided by knowledge of syntax andsomething must come first.
Once the user hasendorsed an assignment of Case-Markers toverbs, each clause in the nested structure of coor-dinated and subordinated clauses received fromthe parser is considered in isolation.
Because thepattern of Case-Markers (CMP) associated with agiven verb is known when the second userinteraction is undertaken, HAIKU can check adictionary of these patterns to see if this par-titular one has been encountered arlier with anyverb.
If it has, the matching CMPs will be order-ed according to a closeness metric discussedbelow.
Otherwise HAIKU will use this closenessmetric to search its CMP dictionary for the pat-tern that most nearly resembles the input CMP.This pattern may lack certain Case-Markers,have extra ones, or not match on both grounds.However a candidate pattern will a lways  befound, it will be the best possible, and HAIKUcan provide additional, next-best patterns shouldthe fast be deemed unsatisfactory.For example, the sentence ~Tho parce l  wasmoved f rom the house  to the ear" has theCI%'\[P SUBJ-OBJ-FROM-TO (where SUBJ is nilhere), associated with the verb move.
Adictionary of CMPs is searched to see if thispattern has previously been associated withmove.
If not, the analyzer will look at the entryfor move.
Suppose it finds { SUBJ-OBJ, SUBJ-OBJ-WITH, SUBJ-FROM-AT}.
It could try to add Casealternatives realized by FROM and TO to theSUBJ-OBJ pattern, or it might return to the CMPdictionary and seek an instance of SUBJ-OBJ-FROM-TO associated with a different verb.Eventually the algorithm selects the CMP closestto the input pattern.
Closeness is a metric basedon factors such as the number, types andagreement of CMs in each pattern and the verbassociated with each (Copeck et al 1992).
It maybe extended to use a very simple noun semanticsfor Case Objects or counts of the frequency ofprevious election.The HAIKU dict ionaries--an incrementallygrowing store of verb-CMP associations, CasePatterns and examples- -are searched forsentences that exemplify the Case Patterns as-sociated with the CMPs.
For example, if SUBJ-OBJ-FROM-TO is associated with take, the sent-ence might be "our guests  took the t ra inf rom Mont rea l  to Ot tawa" .
The  sentence isshown to the user, who can accept he underlyingCase Pattern as correct, edit it by invoking amode whereby a new Case is associated with aselected Case-Marker, or ask to see the nextsentence in the list.
The decision to view anothersentence will probably be dictated by the numberof changes required in the pattern illustrated bythe current example.
The user's selections areused to update the HAIKU dictionaries and tofreeze the sense and structure of the conceptualfragment expressed by the clause which thepattern represents: the system has learned a newpattern of Case Markers, associated them with aparticular verb, and recorded the meaning theyconvey in this instance.
The resulting conceptualfragment is then passed on to the ConceptualKnowledge lh'ocessor to be integrated into themain Conceptual Network.The representation produced by HAIKU isessentially a reorganized parse tree, augmentedwith elements of meaning.
Discourse relationscommunicated by conjunctions (e.g.
causality)are not analyzed by CA.
The representation alsoincludes constituents irrelevant o the overallCase structure of the sentence, e.g.
adjectives,relative clauses, PPs attached to nouns, clauseswith stative verbs expressing noun-nounrelations, and so on.
These are passed to the nextmodule of TANKA, the Mini-Network Builder.FUTURE RESEARCHThe new version of DIPE'IT is operational.
It isnow being integrated into the INTELLA system(Delisle et al 1991)which combines text analysiswith explanation-based l arning.
A Case Analy-sis prototype is running and work in this area isactively under way.
It includes investigating thecharacter of technical texts, validating the set ofCases used in TANKA, refining the process ofACIT~ DE COLING-92, NANTES, 23-28 AO6T 1992 1 0 1 1 PREC.
OF COLING-92, NANTES, AUG. 23-28, 1992confirming the design principles behind exam-pie-driven interaction with the user by experi-ment.
A re- implementation of the HAIKUmodule will be completed in the coming months.CONCLUSIONWe have presented the DIPETI' parser and theCase Analyzer- - the main elements of  the lin-guistic part of the TANKA system.
TANKA willprocess unedited technical text and acquireknowledge about its domain.
We want to analyzecomplete documents with as little user assistanceas possible.
This means that we must considerincomplete and problematic inputs, althoughtheir rate of occurrence should be low in a well-edited text.
We have ensured robust low-levelprocessing of text in order to facilitate almostautomatic recognition of its structure.
At theother end of the spectrum, we plan to handle freesegments of text.
In contrast with other ap-proaches to language understanding, we do notassume a complete semantic model apriori.
Thisimposes certain limitations on what can be pro-cessed automatically; we will minimize user in-teraction.
We hope that we have made clear ourinterest in practical NLP, which we regard as im-portant given the increasing interest in usingNLP techniques to assist in acquiring knowledgefrom text.
We believe such techniques will beused more and more commonly for knowledgeacquisition tasks and may establish a new trendin the design of tools for knowledge ngineers.ACKNOWLEDGMENTSThis work has been supported by the Natural Sciences andEngineering Research Council of Canada nd Cogoos Inc.REFERENCESBRUCE, B.
(1975).
"Case Systems f~?
Natm-al Language",Artificial Intelligence, 6(4), 293-326.COOK, W.A.
(1979).
Case Grammar: Development oftheMatrix Model (1970-1978), Georgetown Univ.
Press,Washington DC.COPECK, T., Delisle, S. & Szpakowicz, S.
(1990).
"Intelligent Case Analysis in the TANKA System",Univ.
of Ottawa, Dept.
of Computer Science, TR-90-24.COPECK, T., Delisle, S. & Szpakowicz, S.
(1992).
"Semantic Analysis in TANKA" (in prel~'ation).DELISLE, S. & Szpakowicz, S. (1991).
"A Broad-Coverage Parser for Knowledge Acquisition fromTechnical Texts", Proc of the F~fth lnt Conf on Symbolicand Logical Computing, Madison, SD, 169-183.DELISLE, S. (1990).
"A Parser for Processing TechnicalTexts with a Large Coverage of English", Univ.
ofOttawa, Dept.
of Computer Science, TR-90-25.DELISLE, S., Matwin, S., Wang, J.
& Zulmn, L.
(1991).
"Explanation-based L arning Helps Acquire Knowledgefrom Natural Language Texts", Proc Sixth lntSymposium on Methodologies for Intelligent Systems,Charlotte, NC, Oct. 1991, 326-337.FILLMORE, C. (1968).
"The Case for Case", in E. Bachand R.T. Harms (eds.
), Universals in Linguistic Theory,Holt, Reinhnrt and Winston, Chicago, IL.GRIMES, J.
(1975).
The Thread of Discourse, Mouton,The Hague.HOBBS, J.
(1978).
"Resolving Pronoun References",Lingua 44, 311-338.JENSEN, K. (1989).
"A Broil-coverage Nalural LanguageAnalysis System", Proc Int Workshop on Par$in 8Technologies (Pittsburgh, PA), 425-441.LARSON, M. (1984).
Meaning-Based Translation: AGuide to Cross-language Equivalence, Unive~rsity Pressof America, Lanham, NY.QUIRK, R., Greenbaum, S., I.e.ech, (3.
& Svartvik, J.(1985).
A Comprehensive Grammar of the EnglishLanguage, Lon gman .SOMERS, H.L.
(1987) Valency and Case inComputational Linguistics.
Edinburgh University Press.SOWA, J.
(1984).
Conceptual Structures: InformationProcessing in Mind and Machine, Addison-Wesley,Reading, IdA.SPARCK-JONES, K. & Bogoraev, B.
(1987).
"A Note onthe Study of Cases", ComputationalLinguistics, 13(1-2),65-68.SZPAKOWICZ, S. & Koperczak, Z.
(1990).
"Mixed-Strategy Matching in Conceptual Networks".
Z. W. Ras,M.
Zemankova and M. L. Emrich (eds.)
Methodologiesfor Intelligent Systems 5.
North-Holland, 321-328.SZPAKOWlCZ, S. (1990).
"Semi-antomatic acquisition ofconceptual structure from technical textS", Int J of Man-Machine Studies, 33,385-397.WINOGRAD, T. (1983).
Language as a Cognitive Process(Syntax), Addison-Wesley.YANG, L. & Szpakowicz, S. (1990).
"Path-finding inNetworks".
Proc SEAR CC '90 South East Asia RegionalComputer Confederation Conf,, Manila, Dee.
1990.YANG, L. & Szpakowicz, S. (1991a).
"Inheritance inConceptual Networks".
Proc Sixth lnt Symposium onMethodologies for Intelligent Systems, Charlotte, NC,191-202.YANG, L. & Szpakowicz, S. (1991b).
"Planning inConceptual Networks".
F. Dehne, F. Finia and W.W.Koczkodaj (eds.)
Advances in Computing andInformation - ICCI "91.Lecture Notes in ComputerScience, vol.
497, Springer-Vctlag, 669-671.ZADROZNY, W. & Jensen, K. (1991).
"Semantics ofParagraphs", Computational I-J'nguistics, 17(2), 171-209.ACT~ DE COLING-92.
NANTES, 23-28 AO~f 1992 1 0 1 2 Pgoc.
OF COLING-92.
NANTES.
AUG. 2.3-28, 1992
