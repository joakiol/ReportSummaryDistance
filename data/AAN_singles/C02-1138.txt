Towards Automatic Generation of Natural Language GenerationSystemsJohn Chen?, Srinivas Bangalore?, Owen Rambow?, and Marilyn A. Walker?Columbia University?
AT&T Labs?Research?New York, NY 10027 Florham Park, NJ 07932{jchen,rambow}@cs.columbia.edu {srini,walker}@research.att.comAbstractSystems that interact with the user via naturallanguage are in their infancy.
As these systemsmature and become more complex, it would bedesirable for a system developer if there werean automatic method for creating natural lan-guage generation components that can producequality output efficiently.
We conduct experi-ments that show that this goal appears to berealizable.
In particular we discuss a naturallanguage generation system that is composed ofSPoT, a trainable sentence planner, and FER-GUS, a stochastic surface realizer.
We showhow these stochastic NLG components can bemade to work together, that they can be portedto new domains with apparent ease, and thatsuch NLG components can be integrated in areal-time dialog system.1 IntroductionSystems that interact with the user via naturallanguage are in their infancy.
As these systemsmature and become more complex, it wouldbe desirable for a system developer if therewere automatic methods for creating naturallanguage generation (NLG) components thatcan produce quality output efficiently.
Stochas-tic methods for NLG may provide such auto-maticity, but most previous work (Knight andHatzivassiloglou, 1995), (Langkilde and Knight,1998), (Oh and Rudnicky, 2000), (Uchimoto etal., 2000), (Bangalore and Rambow, 2000) con-centrate on the specifics of individual stochasticmethods, ignoring other issues such as integra-bility, portability, and efficiency.
In contrast,this paper investigates how different stochasticNLG components can be made to work togethereffectively, whether they can easily be ported tonew domains, and whether they can be inte-grated in a real-time dialog system.Request(DEPART?DATE)Surface GeneratorFERGUSTTSSPoTDialog ManagerSentence PlannerDMImp?conf(N)soft?mergeText to SpeechImplicit?confirm(NEWARK)Implicit?confirm(DALLAS)periodImp?conf(D)Flying from Newark toDallas.
What date wouldyou like to leave?Request(D?D)Figure 1: Components of an NLG system.Recall the basic tasks in NLG.
During textplanning, content and structure of the targettext are determined to achieve the overall com-municative goal.
During sentence planning, lin-guistic means?in particular, lexical and syn-tactic means?are determined to convey smallerpieces of meaning.
During realization, the spec-ification chosen in sentence planning is trans-formed into a surface string by linearizing andinflecting words in the sentence (and typically,adding function words).
Figure 1 shows howsuch components cooperate to generate textcorresponding to a set of communicative goals.Our work addresses both the sentence plan-ning stage and the realization stage.
The sen-tence planning stage is embodied by the SPoTsentence planner (Walker et al, 2001), whilethe surface realization stage is embodied by theFERGUS surface realizer (Bangalore and Ram-bow, 2000).
We extend the work of (Walker etal., 2001) and (Bangalore and Rambow, 2000)in various ways.
We show that apparently eachof SPoT and FERGUS can be ported to differ-ent domains with little manual effort.
We thenshow that these two components can work to-gether effectively.
Finally, we show the on-lineintegration of FERGUS with a dialog system.2 Testing the Domain Independenceof Sentence PlanningIn this section, we address the issue of theamount of effort that is required to port a sen-tence planner to new domains.
In particular,we focus on the SPoT sentence planner.
Theflexibility of the training mechanism that SPoTemploys allows us to perform experiments thatprovide evidence for its domain independence.Being a sentence planner, SPoT chooses ab-stract linguistic resources (meaning-bearing lex-emes, syntactic constructions) for a text plan.
Atext plan is a set of communicative goals whichis assumed to be output by a dialog manager ofa spoken dialog system.
The output of SPoT isa set of ranked sentence plans, each of which isa binary tree with leaves labeled by the commu-nicative goals of the text plan.SPoT divides the sentence planning task intotwo stages.
First, the sentence-plan-generator(SPG) generates 12-20 possible sentence plansfor a given input text plan.
These are gener-ated randomly by incrementally building eachsentence plan according to some probabilitydistribution.
Second, the sentence-plan-ranker(SPR) ranks the resulting set of sentence plans.SPR is trained for this task via RankBoost (Fre-und et al, 1998), a machine learning algorithm,using as training data sets of sentence plansranked by human judges.In porting SPoT to a new domain, this lastpoint seems to be a hindrance.
New train-ing data in the new domain ranked by hu-man judges might be needed in order to trainSPoT.
To the contrary, our experiments thatshow that this need not be the case.
We par-tition the set of all features used by (Walker etal., 2001) to train SPoT into three subsets ac-cording to their level of domain and task de-pendence.
Domain independent features arefeatures whose names include only closed-classwords, e.g.
?in,?
or names of operations that in-crementally build the sentence plan, e.g.
merge.Domain-dependent, task-independent featuresare those whose names include open class wordsFeatures Used Mean Score S.D.all 4.56 0.68domain-independent 4.55 0.69task-independent 4.20 0.99task-dependent 3.90 1.19Table 1: Results for subsets of features used totrain SPoTspecific to this domain, e.g.
?travel?
or thenames of the role slots, e.g.
$DEST-CITY.
Do-main dependent, task dependent features arefeatures whose names include the value of a rolefiller for the domain, e.g.
?Albuquerque.
?We have trained and tested SPoT with thesedifferent feature subsets using the air-travel do-main corpus of 100 text plans borrowed from(Walker et al, 2001), using five fold cross-validation.
Results are shown in Table 2 us-ing t-tests with the modified Bonferroni statis-tic for multiple comparisons.
Scores can rangefrom 1.0 (worst) to 5.0 (best).
The results in-dicate that the domain independent feature setperforms as well as all the features (t = .168, p= .87), but that both the task independent (t= 6.25, p = 0.0) and the task dependent (t =4.58, p = 0.0) feature sets perform worse.3 Automation in Training a SurfaceRealizerAs with the sentence planning task, there is thepossibility that the task of surface realizationmay be made to work in different domains withrelatively little manual effort.
Here, we performexperiments using the FERGUS surface realizerto determine whether this may be so.
We re-view the FERGUS architecture, enumerate re-sources required to train FERGUS, recapitulateprevious experiments that indicate how theseresources can be automatically generated, andfinally show how similar ideas can be used toport FERGUS to different domains with littlemanual effort.3.1 Description of the FERGUSSurface RealizerGiven an underspecified dependency tree repre-senting one sentence as input, FERGUS outputsthe best surface string according to its stochas-tic modeling.
Each node in the input tree corre-sponds to a lexeme.
Nodes that are related bygrammatical function are linked together.
Sur-face ordering of the lexemes remains unspecifiedin the tree.FERGUS consists of three models: treechooser, unraveler, and linear precedencechooser.
The tree chooser associates a su-pertag (Bangalore and Joshi, 1999) from a tree-adjoining grammar (TAG) with each node inthe underspecified dependency tree.
This par-tially specifies the output string?s surface order;it is constrained by grammatical constraints en-coded by the supertags (e.g.
subcategorizationconstraints, voice), but remains free otherwise(e.g.
ordering of modifiers).
The tree chooseruses a stochastic tree model (TM) to select asupertag for each node in the tree based on lo-cal tree context.
The unraveler takes the re-sulting semi-specified TAG derivation tree andcreates a word lattice corresponding to all ofthe potential surface orderings consistent withthis tree.
Finally, the linear precedence (LP)chooser finds the best path through the wordlattice according to a trigram language model(LM), specifying the output string completely.Certain resources are required in order totrain FERGUS.
A TAG grammar is needed?the source of the supertags with which thesemi-specified TAG derivation tree is annotated.There needs to be a treebank in order to ob-tain the stochastic model TM driving the treechooser.
There also needs to be a corpus of sen-tences in order to train the language model LMrequired for the LP chooser.3.2 Labor-Minimizing Approaches toTraining FERGUSThe resources that are needed to train FER-GUS seem quite labor intensive to develop.
But(Bangalore et al, 2001) show that automati-cally generated version of these resources canbe used by FERGUS to obtain quality output.Two kinds of TAG grammar are used in (Ban-galore et al, 2001).
One kind is a manually de-veloped, broad-coverage grammar for English:the XTAG grammar (XTAG-Group, 2001).
Itconsists of approximately 1000 tree frames.
Dis-advantages of using XTAG are the consider-able amount of human labor expended in itsdevelopment and the lack of a treebank basedon XTAG?the only way to estimate parame-ters in the TM is to rely on a heuristic map-ping of XTAG tree frames onto a pre-existingtreebank (Bangalore and Joshi, 1999).
Anotherkind of grammar is a TAG automatically ex-tracted from a treebank using the techniques of(Chen, 2001) (cf.
(Chiang, 2000), (Xia, 1999)).These techniques extract a linguistically mo-tivated TAG using heuristics programmed us-ing a modicum of human labor.
They nullifythe disadvantages of using the XTAG grammar,but they introduce potential complications?notably, an extracted grammar?s size is oftenmuch larger than that of XTAG, typically morethan 2000 tree frames, potentially leading to alarger sparse data problem, and also the result-ing grammar is not hand-checked.Two kinds of treebank are used in (Bangaloreet al, 2001).
One kind is the Penn Treebank(Marcus et al, 1993).
It consists of approxi-mately 1,000,000 words of hand-checked, brack-eted text.
The text consists of Wall Street Jour-nal news articles.
The other kind of treebank isthe BLLIP corpus (Charniak, 2000).
It con-sists of approximately 40,000,000 words of textthat has been parsed by a broad-coverage sta-tistical parser.
The text consists of Wall StreetJournal news and newswire articles.
The ad-vantage of the former is that it has been hand-checked, whereas the latter has the advantageof being easily produced and hence can easilybe enlarged.
(Bangalore et al, 2001) experimentally de-termine how the quality and quantity of theresources used in training FERGUS affect theoutput quality of the generator.
They find thatwhile a better quality annotated corpus (PennTreebank) results in better model accuracy thana lower quality corpus (BLLIP) of the same size,an (easily-obtained) larger lower quality corpusresults in a model that eclipses a smaller, betterquality treebank.
Also, the model that is ob-tained when using an automatically extractedgrammar yields comparable output quality tothe model that is obtained when using a hand-crafted (XTAG) grammar.3.3 Automating Adaptation ofFERGUS to a New DomainThis paper is about minimizing the amount ofmanual labor that is required to port NLG com-ponents to different domains.
(Bangalore etal., 2001) perform all of their experiments onthe same domain of Wall Street Journal newsarticles.
In contrast, in this section we showthat FERGUS can be adapted to the domain ofair-travel reservation dialogs with minimal hu-man effort.
We show that out-of-domain train-ing data can be used instead of in-domain datawithout drastically compromising output qual-ity.
We also show that partially parsed in-domain training data can be effectively usedto train the TM.
Finally, we show that usingan in-domain corpus to train the LM can helpthe output quality, even if that corpus is ofsmall size.
In this section, we first describe thetraining resources that are used in these exper-iments.
We subsequently describe the experi-ments themselves and their results.Various corpora are used in these experi-ments.
For training, there are two distinctcorpora.
First, there is the previously in-troduced Penn Treebank (PTB).
As thealternative, there is a human-human corpus ofdialogs (HH) from Carnegie Mellon University.The HH corpus consists of approximately13,000 words in the air-travel reservationdomain.
This is not exactly the target domainbecause human-human interaction differsfrom human-computer interaction which isour true target domain.
From this raw text,an LDA parser (Bangalore and Joshi, 1999)trained using the XTAG-based Penn Treebankcreates a partially-parsed, non-hand-checkedtreebank.
Test data consists of about 2,200words derived from Communicator templatedata.
Communicator templates are hand-crafted surface strings of words interspersedwith slot names.
An example is ?What timewould you, traveling from $ORIG-CITYto $DEST-CITY like to leave??
The testdata is derived from all strings like these, withduplicates, in the Communicator system byreplacing the slot names with fillers accordingto a probability distribution.
Furthermore,dependency parses are assigned to the resultingstrings by hand.In the first series of experiments, we ascertainthe output quality of FERGUS using the XTAGgrammar on different training corpora.
We varythe TM?s training corpus to be either PTB orHH.
We do the same for the LM?s training cor-pus.
Assessing the output quality of a generatoris a complex issue.
Here, we select as our met-ric understandability accuracy, defined in (Ban-galore et al, 2000) as quantifying the differ-PTB TM HH TMPTB LM 0.30 0.38HH LM 0.37 0.41Table 2: Average understandability accuraciesusing XTAG-Based FERGUS for various kindsof training dataPTB TMPTB LM 0.39HH LM 0.33Table 3: Average understandability accuraciesusing automatically-extracted grammar basedFERGUS for various kinds of training dataence between the generator output, in terms ofboth dependency tree and surface string, andthe desired reference output.
(Bangalore et al,2000) finds this metric to correlate well with hu-man judgments of understandability and qual-ity.
Understandability accuracy varies betweena high score of 1.0 and a low score which maybe less than zero.The results of our experiments are shown inTable 2.
We conclude that despite its smallersize, and despite its being only automatically-and partially- parsed, using the in-domain HHis more effective than using the out-of-domainPTB for training the TM.
Similarly, HH is moreeffective than PTB for training the LM.
Thebest result is obtained by using HH to train boththe TM and the LM; this result (0.41) is com-parable to the result obtained by using matchedPTB training and test data (0.43) that is usedin (Bangalore et al, 2001).The second series of experiments investi-gates the output quality of FERGUS usingautomatically-extracted grammars.
In these ex-periments, the TM is always trained on PTBbut not HH.
It is the type of training data thatis used to train the LM, either PTB or HH,that is varied.
The results are shown in Ta-ble 3.
Note that these scores are in the samerange as those obtained when training FER-GUS using XTAG.
Also, these scores show thatwhen using automatically-extracted grammars,training LM using a large, out-of-domain cor-pus (PTB) is more beneficial than training LMusing a small, in-domain corpus (HH).We can now draw various conclusions abouttraining FERGUS in a new domain.
Con-sider training the TM.
It is not necessaryto use a handwritten TAG in the new do-main; a broad-coverage hand-written TAG oran automatically-extracted TAG will give com-parable results.
Also, instead of requiring ahand-checked treebank in the new domain, par-tially parsed data in the new domain is ade-quate.
Now consider training the LM.
Our ex-periments show that a small corpus in the newdomain is a viable alternative to a large corpusthat is out of the domain.4 Integration of SPoT withFERGUSWe have seen evidence that both SPoT andFERGUS may be easily transferable to a newdomain.
Because the output of a sentence plan-ner usually becomes the input of a surface real-izer, questions arise such as whether SPoT andFERGUS can be made to work together in anew domain and what is the output quality ofthe combined system.
We will see that an ad-dition of a rule-based component to FERGUSwill be necessary in order for this integrationto occur.
We will subsequently see that theoutput quality of the resulting combination ofSPoT and FERGUS is quite good.Integration of SPoT as described in Section 2and FERGUS as described in Section 3 is notautomatic.
The reason is that the output ofSPoT is a deep syntax tree (Mel?c?uk, 1998)whereas hitherto the input of FERGUS hasbeen a surface syntax tree.
The primary dis-tinguishing characteristic of a deep syntax treeis that it contains features for categories such asdefiniteness for nouns, or tense and aspect forverbs.
In contrast, a surface syntax tree real-izes these features as function words.
However,there is a one-to-one mapping from features ofa deep syntax tree to function words in the cor-responding surface syntax tree.
Therefore, inte-grating SPoT with FERGUS is basically a mat-ter of performing this mapping.
We have addeda rule-based component (RB) as the new firststage of FERGUS to do just that.
Note that itis erroneous to think that RB makes choices be-tween different generation options because thereis a one-to-one mapping between features andfunction words.PTB TM HH TMPTB LM 0.48 0.47HH LM 0.73 0.68Table 4: Average understandability accuraciesof SPoT-integrated, XTAG-Based FERGUS forvarious kinds of training dataAfter the addition of RB to FERGUS, weevaluate the output quality of the combinationof SPoT and FERGUS.
Only the XTAG gram-mar is used in this experiment.
As in previousexperiments with the XTAG grammar, there iseither the option of training using HH or PTBderived data for either the TM or LM, giving atotal of four possibilities.Test data is obtained by output strings thatare produced by the combination of SPoT andthe RealPro surface realizer (Lavoie and Ram-bow, 1998).
RealPro has the advantage of pro-ducing high quality surface strings, but at thecost of having to be hand-tuned to a particu-lar domain.
It is this cost we are attempting tominimize by using FERGUS.
Only those sen-tence plans produced by SPoT ranked 3.0 orgreater by human judges are used.
The surfacerealization of these sentence plans yields a testcorpus of 2,200 words.As shown in Table 4, the performance ofSPoT and FERGUS combined is quite high.Also note that in terms of training the LM, out-put quality is markedly better when HH is usedrather than PTB.
Furthermore, note that thereis a smaller difference between using PTB orHH to train TM when compared to previous re-sults shown in Table 2.
This seems to indicatethat the TM?s effect on output quality dimin-ishes because of addition of RB to FERGUS.5 On-line Integration of FERGUSwith a Dialog SystemCertain statistical natural language processingsystems can be quite slow, usually because ofthe large search space that these systems mustexplore.
It is therefore uncertain whether a sta-tistical NLG component can be integrated into areal-time dialog system.
Investigating the mat-ter in FERGUS?s case, we have experimentedwith integrating FERGUS into Communicator,a mixed-initiative, airline travel reservation sys-tem.
We begin by explaining how Communica-tor manages surface generation without FER-GUS.
We then delineate several possible kindsof integration.
Finally, we describe our experi-ences with one kind of integration.Communicator performs only a rudimentaryform of surface generation as follows.
Thedialog manager of Communicator issues a setof communicative goals that are to be realized.Surface template strings are selected basedon this set, such as ?What time would you,traveling from $ORIG-CITY to $DEST-CITYlike to leave??
The slot names in thesestrings are then replaced with fillers accordingto the dialog manager?s state.
The resultingstrings are then piped to a text-to-speechcomponent (TTS) for output.There are several possibilities as to how FER-GUS may supplant this system.
One possibilityis off-line integration.
In this case, the set of allpossible sets communicative goals for which thedialog manager requires realization are matchedwith a set of corresponding surface syntax trees.The latter set is input to FERGUS, which gen-erates a set of surface template strings, which inturn is used to replace the manually created sur-face template strings that are an original partof Communicator.
Since these changes are pre-compiled, the resulting version of Communica-tor is therefore as fast the original.
On the otherhand, off-line integration may be unmanageableif the set of sets of communicative goals is verylarge.
In that case, only the alternative of on-line integration is palatable.
In this approach,each surface template string in Communicator isreplaced with its corresponding surface syntaxtree.
At points in a dialog where Communicatorrequires surface generation, it sends the appro-priate surface syntax trees to FERGUS, whichgenerates surface strings.We have implemented the on-line integrationof FERGUS with Communicator.
Our experi-ments show that FERGUS is fast enough to beused in for this purpose, the average time forFERGUS to generate output strings for one di-alog turn being only 0.28 seconds.6 Conclusions and Future WorkWe have performed experiments that provideevidence that components of a statistical NLGsystem may be ported to different domainswithout a huge investment in manual labor.These components include a sentence planner,SPoT, and a surface realizer, FERGUS.
SPoTseems easily portable to different domains be-cause it can be trained well using only domain-independent features.
FERGUS may also besaid to be easily portable because our experi-ments show that the quality and quantity of in-domain training data need not be high and plen-tiful for decent results.
Even if in-domain datais not available, we show that out-of-domaintraining data can be used with adequate results.By integrating SPoT with FERGUS, we havealso shown that different statistical NLG com-ponents can be made to work well together.
In-tegration was achieved by adding a rule-basedcomponent to FERGUS which transforms deepsyntax trees into surface syntax trees.
The com-bination of SPoT and FERGUS performs withhigh accuracy.
Post-integration, there is a di-minishing effect of TM on output quality.Finally, we have shown that a statistical NLGcomponent can be integrated into a dialog sys-tem in real time.
In particular, we replace thehand-crafted surface generation of Communica-tor with FERGUS.
We show that the resultingsystem performs with low latency.This work may be extended in different di-rections.
Our experiments showed promising re-sults in porting to the domain of air travel reser-vations.
Although this is a reasonably-sized do-main, it would be interesting to see how ourfindings vary for broader domains.
Our experi-ments used a partially parsed version of the HHcorpus.
We would like to compare its use asTM training data in relation to using a fullyparsed version of HH, and also a hand-checkedtreebank version of HH.
We would also like toinvestigate the possibility of interpolating mod-els based on different kinds of training data inorder to ameliorate data sparseness.
Our ex-periments focused on integration between theNLG components of sentence planning and sur-face generation.
We would like to explore thepossibility of further integration, in particularintegrating these components with TTS.
Thiswould provide the benefit of enabling the use ofsyntactic and semantic information for prosodyassignment.
Also, although FERGUS was inte-grated with SPoT relatively easily, it does notnecessarily follow that FERGUS can be inte-grated easily with other kinds of components.It may be worthwhile to envision a redesignedversion of FERGUS whose input can be flexiblyunderspecified in order to accommodate differ-ent kinds of modules.7 AcknowledgmentsThis work was partially funded by DARPA un-der contract MDA972-99-3-0003.ReferencesSrinivas Bangalore and A. K. Joshi.
1999.
Su-pertagging: An approach to almost parsing.Computational Linguistics, 25(2).Srinivas Bangalore and Owen Rambow.
2000.Exploiting a probabilistic hierarchical modelfor generation.
In Proceedings of the 18thInternational Conference on ComputationalLinguistics (COLING 2000).Srinivas Bangalore, Owen Rambow, and SteveWhittaker.
2000.
Evaluation metrics for gen-eration.
In Proceedings of the First Interna-tional Conference on Natural Language Gen-eration, Mitzpe Ramon, Israel.Srinivas Bangalore, John Chen, and OwenRambow.
2001.
Impact of quality and quan-tity of corpora on stochastic generation.
InProceedings of the 2001 Conference on Em-pirical Methods in Natural Langauge Process-ing, Pittsburgh, PA.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings of First An-nual Meeting of the North American Chap-ter of the Association for Computational Lin-guistics, Seattle, WA.John Chen.
2001.
Towards Efficient Statis-tical Parsing Using Lexicalized Grammati-cal Information.
Ph.D. thesis, University ofDelaware.David Chiang.
2000.
Statistical parsing with anautomatically-extracted tree adjoining gram-mar.
In Proceedings of the the 38th AnnualMeeting of the Association for ComputationalLinguistics, pages 456?463, Hong Kong.Yoav Freund, Raj Iyer, Robert E. Schapire, andYoram Singer.
1998.
An efficient boostingalgorithm for combining preferences.
In Ma-chine Learning: Proceedings of the FifteenthInternational Conferece.Kevin Knight and V. Hatzivassiloglou.
1995.Two-level many-paths generation.
In Pro-ceedings of the 33rd Annual Meeting of theAssociation for Computational Linguistics,Boston, MA.Irene Langkilde and Kevin Knight.
1998.
Gen-eration that exploits corpus-based statisti-cal knowledge.
In Proceedings of the 17thInternational Conference on ComputationalLinguistics and the 36th Annual Meeting ofthe Association for Computational Linguis-tics, Montreal, Canada.Benoit Lavoie and Owen Rambow.
1998.
Aframework for customizable generation ofmulti-modal presentations.
In Proceedings ofthe 17th International Conference on Com-putational Linguistics and the 36th AnnualMeeting of the Association for ComputationalLinguistics, Montreal, Canada.Mitchell Marcus, Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Buildinga large annotated corpus of english: thepenn treebank.
Computational Linguistics,19(2):313?330.Igor A. Mel?c?uk.
1998.
Dependency Syntax:Theory and Practice.
State University of NewYork Press, New York, NY.Alice H. Oh and Alexander I. Rudnicky.2000.
Stochastic language generation for spo-ken dialog systems.
In Proceedings of theANLP/NAACL 2000 Workshop on Conver-sational Systems, pages 27?32, Seattle, WA.Kiyotaka Uchimoto, Masaki Murata, Qing Ma,Satoshi Sekine, and Hitoshi Isahara.
2000.Word order acquisition from corpora.
In Pro-ceedings of the 18th International Confer-ence on Computational Linguistics (COLING?00), Saarbru?cken, Germany.Marilyn A. Walker, Owen Rambow, and Mon-ica Rogati.
2001.
Spot: A trainable sentenceplanner.
In Proceedings of the Second Meetingof the North American Chapter of the Asso-ciation for Computational Linguistics, pages17?24.Fei Xia.
1999.
Extracting tree adjoining gram-mars from bracketed corpora.
In Fifth Natu-ral Language Processing Pacific Rim Sympo-sium (NLPRS-99), Beijing, China.The XTAG-Group.
2001.
A LexicalizedTree Adjoining Grammar for English.Technical report, University of Penn-sylvania.
Updated version available athttp://www.cis.upenn.edu/?xtag.
