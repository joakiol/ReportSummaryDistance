Product Named Entity Recognition Based on Hierarchical HiddenMarkov Model?Feifan Liu, Jun Zhao, Bibo Lv, Bo XuNational Laboratory of Pattern RecognitionInstitute of Automation Chinese Academy of SciencesBeijing P.O.
Box 2728, 100080{ffliu,jzhao,bblv,xubo}@nlpr.ia.ac.cnHao YuFUJITSU R&DXiao Yun Road No.26Chao Yang District, Beijing, 100016yu@frdc.fujitsu.comAbstractA hierarchical hidden Markov model(HHMM) based approach of productnamed entity recognition (NER) fromChinese free text is presented in this pa-per.
Characteristics and challenges inproduct NER is also investigated andanalyzed deliberately compared withgeneral NER.
Within a unified statis-tical framework, the approach we pro-posed is able to make probabilisticallyreasonable decisions to a global opti-mization by leveraging diverse rangeof linguistic features and knowledgesources.
Experimental results show thatour approach performs quite well in twodifferent domains.1 IntroductionNamed entity recognition(NER) plays a sig-nificantly important role in information extrac-tion(IE) and many other applications.
Previousstudy on NER is mainly focused either on theproper name identification of person(PER), lo-cation(LOC), organization(ORG), time(TIM) andnumeral(NUM) expressions almost in news do-main, which can be viewed as general NER, orother named entity (NE) recognition in specificdomain such as biology.As far as we know, however, there is little priorresearch work conducted by far on product named0This work was supported by the Natural Sciences Foun-dation of China(60372016,60272041) and the Natural Sci-ence Foundation of Beijing(4052027).entity recognition which can be crucial and valu-able in many business IE applications, especiallywith the increasing research interest in MarketIntelligence Management(MIM), Enterprise Con-tent Management (ECM) [Pierre 2002] and etc.This paper describes a prototype system forproduct named entity recognition, ProNER, inwhich a HHMM-based approach is employed.Within a unified statistical framework, the ap-proach based on a mixture model is able to makeprobabilistically reasonable decisions to a globaloptimization by exploiting diverse range of lin-guistic features and knowledge sources.
Experi-mental results show that ProNER performs quitewell in two different domains.2 Related WorkUp to now not much work has been done onproduct named entity recognition, nor systematicanalysis of characteristics for this task.
[Pierre2002] developed an English NER system capableof identifying product names in product views.
Itemployed a simple Boolean classifier for identi-fying product name, which was constructed fromthe list of product names.
The method is sim-ilar to token matching and has a limitation forproduct NER applications.
[Bick et al 2004] rec-ognized named entities including product namesbased on constraint grammar based parser forDanish.
This rule-based approach is highly de-pendent on the performance of Danish parser andsuffers from its weakness in system portability.[C.
Niu et al 2003] presented a bootstrapping ap-proach for English named entity recognition us-ing successive learners of parsing-based decision40System Statistical Model Linguistic Feature Combinative Points[Zhang et al 2003] HMM semantic role, tokens pattern rules[Sun et al 2002] class-based LM word form, NE category cue words list[Tsai et al 2004] ME model tokens knowledge representationTable 1: Comparison between several Chinese NER systems1list and HMM, and promising experiment results(F-measure: 69.8%) on product NE (correspond-ing to our PRO) were obtained.
Its main advan-tage lies in that manual annotation of a sizabletraining corpus can be avoided, but it suffers fromtwo problems, one is that it is difficult to find suf-ficient concept-based seeds needed in bootstrap-ping for the coverage of the variations of PROsubcategories, another it is highly dependent onparser performance as well.Research on product NER is still at its earlystage, especially in Chinese free text collec-tions.
However, considerable amount of workhas been done in the last decade on the gen-eral NER task and biological NER task.
Thetypical machine learning approaches for EnglishNE are transformation-based learning[Aberdeenet al 1995], hidden Markov model[Bikel etal.
1997], maximum entropy model[Borthwick,1999], support vector machine learning[Eunji Yiet al 2004], unsupervised model[Collins et al1999]and etc.For Chinese NER, the prevailing methodologyapplied recently also lie in machine learning com-bining other knowledge base or heuristic rules,which can be compared on the whole in three as-pects showed in Table 1.In short, the trend in NER is to adopt a statis-tical framework which try to exploit some knowl-edge base as well as different level of text featureswithin and outside NEs.
Further those ideas, wepresent a hybrid approach based on HHMM [S.Fine et al 1998] which will be described in de-tail.3 Problem Statements and Analysis3.1 Task Definition3.1.1 Definition of Product Named EntityIn our study, only three kinds of prod-uct named entities are considered, namely1Note: LM(language model); ME(maximum entropy).Brand Name(BRA), Product Type(TYP), ProductName(PRO), and BRA and TYP are often embed-ded in PRO.
In the following two examples, thereare two BRA NEs, one TYP NE and one PRONE all of which belong to the family of productnamed entities.Exam 1: ??
(Benq)/BRA ??(brand)??
?
?
?
(market shares)?
?(steadily)??
(ascend)bExam 2: ?
?(corporation)?
?(will)??
(deliver) [Canon/BRA 334?
(ten thou-sand)?
?(pixels)?
?(digital)?
?
(camera)Pro90IS/TYP]/PRObBrand Name refer to proper name of producttrademark such as ???(Benq)?
in Exam 1.Product Type is a kind of product named en-tities indicating version or series information ofproduct, which can consist of numbers, Englishcharacters, or other symbols such as ?+?
and ?-?
etc.In our study, two principles should be fol-lowed.
(1) Chinese characters are not considered tobe TYP, nor subpart of TYP although some ofthem can contain version or series information.For instance, in ?2005????
(happy newyear)?(version)??
(cell phone)?, here ?????
(happy new year)?
(version)?should not beconsidered as a TYP.
(2) Numbers are essential elements in prod-uct type entity.
For instance, in ?PowerShot??(series)??(digital)??
(camera)?, ?Pow-erShot?
is not considered as a TYP, however,in ?PowerShot S10 ??(digital)??
(camera)?,?PowerShot S10?
can make up of a TYP.Product Name, as showed above in Exam 2, isa kind of product named entities expressing self-contained proper name for some specified productin real world compared to BRA and TYP whichonly express one attribute of product.
i.e.
a PRONE must be assigned with distinctly discrimina-tive information which can not shared with othergeneral product-related expressions.41(1) Product-related expressions which are em-bedded with either BRA or TYP can be qual-ified to be a PRO entity.
e.g.
?BenQ??(flash)?(disk)?
is a PRO entity, but the gen-eral product-related expression ???(flash)??(market)??(investigation)?
cannot make upof a PRO entity.
(2) Product-related expressions indicatingsome specific version or series information whichis unique for a BRA can also be considered as aPRO entity.
e.g.
?DIGITAL IXUS??(series)??(digital)?
?(camera)?
is a PRO because?DIGITAL IXUS?
series is unique for Canonproduct, but ??
?(intelligent)?(version)??
(cell phone)?
is not a PRO because the at-tribute of ?intelligent version?
can be assigned toany cell phone product.3.1.2 Product Named Entity RecognitionProduct named entity recognition involves theidentification of product-related proper namesin free text and their classification into differ-ent kinds of product named entities, referring toPRO, TYP and BRA in this paper.In comparisonwith general NER, nested product NEs should betagged separately rather than being tagged just asa single item, shown as Figure 1.3.2 Challenges for Product Named EntityRecognition?For general named entities, there are somecues which are very useful for entity recogni-tion, such as ???
(city), ????(Inc.
), and etc.
Incomparison, product named entities have no suchnamed conventions and cues, resulting in higherboundary ambiguities and more complex NE can-didate triggering difficulties.
?In comparison with general NER, more chal-lenges in product NER result from miscellaneousclassification ambiguities.
Many entities withidentical form can be a kind of general named en-tity, a kind of product named entity, or just com-mon words.
?In comparison with general named entities,product named entities show more flexible vari-ant forms.
The same entity can be expressed inseveral different forms due to spelling variation,word permutation and etc.
This also compoundsthe difficulties in product named entity recogni-tion.
?In comparison with general named entities,it is more frequent that product named entities arenested as Figure 1 illustrates.
More efforts haveto be made to identify such named entities sepa-rately.3.3 Our SolutionsWe adopt the following strategies in triggeringand disambiguating process respectively.
(1) As to product NER, it?s pivotal to controlthe triggering candidates efficiently for the bal-ance between precision and recall.
Here we usethe knowledge base such as brand word list, andother heuristic information which can be easilyacquired.
(2)After triggering candidates, we try to em-ploy a statistical model to make the most ofmulti-level context information mentioned abovein disambiguation.
We choose hierarchical hid-den Markov model (HHMM) [S. Fine et al 1998]for its more powerful ability to model the multi-plicity of length scales and recursive nature of se-quences.424 Hybrid Approach for Product NERecognition4.1 Overall Workflow of ProNER?Preprocessing: Segment, POS tagging andgeneral NER is primarily conducted using our off-shelf SegNer2.0 toolkit on input text.
?Generating Product NE Candidates: First,BRA or ORG and TYP are triggered by brandword list and some word features respectively.Here we categorize the triggering word featuresinto six classes: alphabet string, alphanumericstring, digits, alphabet string with fullwidth, dig-its with fullwidth and other symbols except Chi-nese characters.
Then PRO are triggered by BRAand TYP candidates as well as some clue wordsindicating type information to some extent suchas ???
(version), ????(series).
In this step themodel structure(topology) of HHMM[S. Fine etal.
1998] is dynamically constructed, and someconjunction words or punctuations and specifiedmaximum length of product NE are used to con-trol it.
?Disambiguating Candidates: In this mod-ule, boundary and classification ambiguities be-tween candidates are resolved simultaneously.And Viterbi algorithm is applied for most-likelystate sequences based on the HHMM topology.4.2 Integration with Heuristic InformationTo get more efficient control in triggering processabove, we try to integrate some heuristic informa-tion.
The heuristic rules we used are as domain-independent as possible in order that they canbe integrated with statistical model systematicallyrather than just some tricks on it.
(1) Stop Word List:Common English words, English brand word,and some punctuations are extracted automati-cally from training set to make up of stop wordlist for TYP; by co-occurrence statistics betweenORG and its contexts, some words are extractedfrom the contexts to make up of stop word listfor PRO in order to overcome the case that brandword is prone to bind its surroundings to be aPRO.
(2) Constrain Rules:Rule 1: For the highly frequent pattern ???+?????
(number + English quantifierES PS5IS2IS1IS0ES PS1 PS2 PS4PS3ES0.2 0.50.30.7  0.3 0.5 0.30.70.20.3Figure 2 Structure of Hierarchical HiddenMarkov Model (HHMM)word), all the corresponding TYP candidates trig-gered by categorized word features(CWF) shouldbe removed.Rule 2: Product NE candidates in which somebinate symbols don?t match each other should beremoved.Rule 3: Unreasonable symbols such as ?-?
or?:?
should not occur in the beginning or end ofproduct NE candidates.4.3 HHMM for product NER applicationBy HHMM [S. Fine et al 1998] the productNER can be formulated as a tagging problem us-ing Viterbi algorithm.
Unlike traditional HMMin POS tagging, here the topology of HHMM isnot fixed and internal states can be also a similarstochastic model on themselves, called internalstates compared to production states which willemit only observations.Our HHMM structure actually consists of threelevel approximately illustrated as figure 2 inwhich IS denotes internal state, PS denotes pro-duction state and ES denote end state at ev-ery level.
For our application, an input se-quence from our SegNer2.0 toolkit can be formal-ized as w1/t1w2/t2 .
.
.
wi/ti .
.
.
wn/tn, amongwhich wi and ti is the ith word and its part-of-speech, n is the number of words.
The POStag set here is the combination of tag set fromPeking University(PKU-POS) and our generalNE categories(GNEC) including PER(person),LOC(location), ORG(organization), TIM(time ex-pression), NUM(numeric expression).
Thereforewe can construct our HHMM model by the stateset {S} consisting of {GNEC}, {BRA, PRO,TYP}, and {V} as well as the observation set {O}consisting of {V} which is the word set fromtraining data.
That is to say, the word forms43in {V} which are not included in NEs are alsoviewed as production states.In our model, only PRO are internal state whichmay activate other production states such as BRAand TYP resulting in recursive HMM.
In consis-tence with S. Fine?s work, qdi (1?
d ?
D) is usedto indicate the ith state in the dth level of hierar-chy.
So, the product NER problem is to find themost-likely state activation sequence Q*, a multi-scale list of states, based on the dynamic topol-ogy of HHMM given a observation sequence W= w1w2 .
.
.
wi .
.
.
wn, formulated as follows basedon Bayes rule (P (W )=1).Q?= argmaxQP (Q|W )= argmaxQP (Q)P (W |Q)(1)From the root node of HHMM, activity flowsto all other nodes at different levels according totheir transition probability.
For description conve-nience, we take the kth level as example(activatedby the mth state at the k-1th level).P (Q) ?= p(qk1 |qk?1m )?
??
?vertical transitionhorizontal transition?
??
?p(qk2 |qk1 )|qk|?j=3p(qkj |qkj?1, qkj?2)(2)P (W |Q)=???????????????
?=|qkPS |?j=1p([wqkj ?begin...wqkj ?end]|qkj )if qkj /?
{IS}activate other states recursivelyif qkj ?
{IS}(3)Where |qk| is the number of all states and |qkPS |is the number of production states in the kth level;wqkj ?begin...wqkj ?end indicates the word sequencecorresponding to the state qkj .
(1) In equation (3), if qkj ?
{{GNEC},{V}},p([wqkj ?begin...wqkj ?end]|qkj )=1, because we as-sume that the general NER results from the pre-ceding toolkit are correct;(2) If qkj = PRO, production states in the(k+1)th level will be activated by this internalstate through equation (2),(3) and go back whenarriving at an end state, thus hierarchical compu-tation is implemented;(3) If qkj =BRA, we assign equation (3) a con-stant value in that BRA candidates consist of onlya single brand word in our method.
In additionbrand word can also generate ORG candidates,thus we can assign equation (3) as follows.p([wqkj ?begin...wqkj ?end]|qkj = BRA) = 0.5 (4)(4) If qkj = TY P , categorized word fea-tures(CWFs) defined in section 4.1 are applied,i.e.
the words associated with the current state arereplaced with their CWFs (WC) acting as obser-vations.
Then we can compute the emission prob-ability of this TYP production state as the follow-ing equation, among which |qkj | is the length ofobservation sequence associated with the currentstate.p([wqkj ?begin...wqkj ?end]|qkj = TY P )?=p(wc1|begin)p(end|wc|qkj |)|qkj |?m=2p(wcm|wcm?1)All the parameters in every level of HHMM canbe acquired using maximum likelihood methodwith smoothing from training data.4.4 Mixture of Two Hierarchical HiddenMarkov ModelsNow we have implemented a simple HHMMfor product NER.
Note that in the abovemodel(HHMM-1), we exploit both internal andexternal features of product NEs only at lev-els of simply semantic classification and justword form.
To achieve our motivation in sec-tion 3.3, we construct another HHMM(HHMM-2) for exploiting multi-level contexts by mixingwith HHMM-1.In HHMM-2, the difference from HHMM-1lies in the state set SII and observation set OII .Because the input text will be processed by seg-ment, POS tagging and general NER, as a alterna-tive, we can also take T=t1t2 .
.
.
ti .
.
.
tn as obser-vation sequence, i.e.
OII={PKU-POS}.
Accord-ingly, SII= {{PKU-POS}, {GNEC}, BRA, TYP,44Data Sets PRO BRA TYP PER LOC ORGDataSetPRO1.2 12,432 5,047 10,606 424 1,733 4,798OpenTestSet 1800 803 1364 39 207 614CloseTestSet 1553 513 1296 55 248 619Table 2: Overview of Data SetsPRO}, among which PRO is internal state.
Sim-ilarly, the problem is formulated as follows withHHMM-2.Q?II = argmaxQIIP (QII |T )= argmaxQIIP (QII)P (T |QII) (5)The description and computation of HHMM-2is similar to HHMM-1 and is omitted here.We can see that besides making use of semanticclassification of NEs in common, HHMM-1 andHHMM-2 exploit word form and part-of-speech(POS) features respectively.
Word form featuresmake the model more discriminative, while POSfeatures result in robustness.
Intuitively, the mix-ture of these two models is desirable for higherperformance in product NER by balancing the ro-bustness and discrimination which can be formu-lated in logarithmic form as follows.
(Q?, Q?II)= argmaxQ,QII{log(P (Q)) + log(P (W |Q))+ ?
[log(P (QII)) + log(P (T |QII))]} (6)Where ?
is a tuning parameter for adjusting theweight of two models.5 Experiments and analysis5.1 Data Set PreparationA large number of web pages in mobile phoneand digital domain are compiled into text collec-tions, DataSetPRO, on which multi-level process-ing were performed.
Our final version, DataSet-PRO1.2, consists of 1500 web pages, roughly1,000,000 Chinese characters.
Randomly se-lected 140 texts (digital 70, mobile phone 70) areseparated from DataSetPRO1.2 as our OpenTest-Set, the rest as TrainingSet, from which 160 textsare extracted as CloseTestSet.
Table 2 illustratesthe overview of them.5.2 ExperimentsDue to various and flexible forms of product NEs,though some boundaries of recognized NEs areinconsistent with manual annotation, they are alsoreasonable.
So soft evaluation is also appliedin our experiments to make the evaluation morereasonable.
The main idea is that a discountscore will be given to recognized NEs with wrongboundary but correct detection and classification.However, strict evaluation only score completelycorrect ones.All the results is conducted on OpenTestSet un-less it is particularly specified.
Also, the evalu-ation scores used below are obtained mainly by45Digital Domain (?
?8)Product NEs Close Test Open TestPrecision Recall F-measure Precision Recall F-measurePRO 0.864 0.799 0.830 0.762 0.744 0.753TYP 0.903 0.906 0.905 0.828 0.944 0.882BRA 0.824 0.702 0.758 0.723 0.705 0.714Mobile Phone Domain (?
?8)Product NEs Close Test Open TestPrecision Recall F-measure Precision Recall F-measurePRO 0.917 0.935 0.926 0.799 0.856 0.827TYP 0.959 0.976 0.967 0.842 0.886 0.864BRA 0.911 0.741 0.818 0.893 0.701 0.785Table 3: Experimental Results in Digital and Mobile Phone Domainsoft metrics, and strict scores are also given forcomparison in experiment 3.1.
Evaluation on the Influence of ?
in the Mix-ture Model.In the mixture model denoted as equation (6),the ?
value reflects the different contribution oftwo individual models to the overall system per-formance.
The larger ?, the more contributionmade by HHMM-2.
Figure 3, 4, 5 illustrate thevarying curves of recognition performance withthe ?
value on PRO, TYP, BRA respectively.Note that, if ?
equal to 1 then two modelsare mixed with equivalent weight.
We can seethat, as ?
goes up, the F-measures of PRO andTYP increase obviously firstly, and begin to godown slightly after a period of growing flat.
Itcan be explained that HHMM-2 mainly exploitspart-of-speech and general NER features whichcan relieve the sparseness problem to some ex-tent, which is more serious in HHMM-1 due tousing lower level of contextual information suchas word form.
However, as ?
becomes larger,the problem of imprecise modeling in HHMM-2 will be more salient and begin to illustrate aside-effect in the mixture model.
Whereas, theinfluence of ?
on BRA is negligible because itscandidates are triggered by the relatively reliableknowledge base and its sub-model in HHMM isassigned a constant as shown in equation(4).Summings-up:(1) Mixture with HHMM-2 can make up theweakness of HHMM-1.
(2) HHMM-2 can make more contributionsto the mixture model under the conditions thatlimited annotated data is available at present.
Inour system, ?
is assigned to 8 based on above ex-perimental results.2.
Evaluation on the portability of ProNER intwo domains.First, we can see from Table 3 that ProNERhave achieved fairly high performance in bothdigital and mobile phone domain.
This can val-idate to some extent the portability of our sys-tem?which is consistent with our initial motiva-tion.Second, the results also show that our systemperforms slightly better in mobile phone domainfor both close test and open test.
This can be ex-plained that there are more challenging ambigui-ties in digital domain due to more complex prod-uct taxonomy and more flexible variants of prod-uct NEs.Summings-up: The results provide promisingevidence on the portability of our system to dif-ferent domains though there are some differencesbetween them.3.
Evaluation on the efficiency of the mixturemodel and the improvement of the triggeringcontrol with heuristics.In table 4, ?1?
denotes HHMM-1; ?2?
denotesHHMM-2; ?+?
means the mixture model; ?
*?means integrating with heuristics mentioned insection 4.2.The results reveal that the mixture model out-performs each individual model with both softand strict metrics.
Also, the results show thatheuristic information can increase the F-measureof PRO and TYP by 10 points or so for both indi-46HHMMBRA TYP PROstrictscoresoftscorestrictscoresoftscorestrictscoresoftscore1 0.68 0.72 0.57 0.66 0.52 0.611* 0.70 0.74 0.70 0.80 0.63 0.722 0.67 0.73 0.66 0.74 0.61 0.682* 0.70 0.74 0.76 0.85 0.70 0.761+2 0.70 0.75 0.67 0.77 0.67 0.721+2* 0.72 0.76 0.76 0.87 0.75 0.80Table 4: Improvement results (F-measure) withheuristics and model mixturevidual model and the mixture model.
Addition-ally we can see that HHMM-2 performs betteron the whole than HHMM-1, which is consistentwith experiment 1 that heavier weights should beassigned to HHMM-2 in the mixture model.Summings-up:(1) Either HHMM-1 or HHMM-2 can notperform quite well independently, but systemat-ical integration of them can achieve obvious per-formance improvement due to the leverage of di-verse levels of linguistic features by their efficientinteraction.
(2) Heuristic information can highly enhancethe performance for both individual model and themixture model.6 Conclusions and Future WorkThis paper presented a hierarchical HMM (hiddenMarkov model) based approach of product namedentity recognition from Chinese free text.
By uni-fying some heuristic rules into a statistical frame-work based on a mixture model of HHMM, theapproach we proposed can leverage diverse rangeof linguistic features and knowledge sources tomake probabilistically reasonable decisions for aglobal optimization.
The prototype system webuilt achieved the overall F-measure of 79.7%,86.9%, 75.8% corresponding to PRO, TYP, BRArespectively, which also provide experimental ev-idence to some extent on its portability to differ-ent domains.Our future work will focus on the following:(1) Using long dependency information;(2) Integrating segment, POS tagging, generalNER and product NER to avoid error spread.ReferencesJohn M. Pierre.
(2002) Mining Knowledge from TextCollections Using Automatically Generated Meta-data.
In: Procs of Fourth International Conferenceon Practical Aspects of Knowledge Management.Michael Collins and Yoram Singer.
(1999) Unsuper-vised Models for Named Entity Classification.
In:Proc.
of EMNLP/VLC-99.Eunji Yi, Gary Geunbae Lee, and Soo-Jun Park.
(2004) SVM-based Biological Named EntityRecognition using Minimum Edit-Distance FeatureBoosted by Virtual Examples.
In: Proceedings ofthe First International Joint Conference on NaturalLanguage Processing (IJCNLP-04).Bick, Eckhard (2004) A Named Entity Recognizer forDanish.
In: Proc.
of 4th International Conf.
on Lan-guage Resources and Evaluation,pp:305-308.Jian Sun, Jianfeng Gao, Lei Zhang, Ming Zhou,Changning Huang.
(2002) Chinese Named EntityIdentification Using Class-based Language Model.In: COLING 2002.
Taipei, Taiwan.Huaping Zhang, Qun Liu, Hongkui Yu, Xueqi Cheng,Shuo Bai.
Chinese Named Entity Recognition Us-ing Role Model.
Special Iissue ?Word Formationand Chinese Language processing?
of the Inter-national Journal of Computational Linguistics andChinese Language Processing, 8(2),2003, pp:29-60Aberdeen, John et al (1995)MITRE: Description ofthe ALEMBIC System Used for MUC-6.
Proc.
ofMUC-6, pp.
141-155D.M.
Bikel, S. Miller, R. Schwartz, R.
Weischedel.
(1997) Nymble: a High-Performance LearningName-finder.
In: Fifth Conference on Applied Nat-ural Language Processing, pp 194-201.Borthwick.
A.
(1999) A Maximum Entropy Approachto Named Entity Recognition.
PhD Dissertation.Tzong-Han Tsai, S.H.
Wu, C.W.
Lee, Cheng-WeiShih, and Wen-Lian Hsu.
(2004) Mencius: A Chi-nese Named Entity Recognizer Using the Maxi-mum Entropy-based Hybrid Model.
InternationalJournal of Computational Linguistics and ChineseLanguage Processing, Vol.
9, No 1.Cheng Niu, W. Li, J.h.
Ding and R.K. Srihari.
(2003) ABootstrapping Approach to Named Entity Classifi-cation Using Successive Learners.
In: Proceedingsof the 41st ACL, Sapporo, Japan, pp:335-342.S.
Fine, Y.
Singer, N. Tishby.
(1998) The HierarchicalHidden Markov Model: Analysis and Applications.Machine Learning.
32(1), pp:41-6247
