Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 725?735,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsMultiword Expression Identification with Tree Substitution Grammars:A Parsing tour de force with FrenchSpence Green*, Marie-Catherine de Marneffe?, John Bauer*, and Christopher D.
Manning*?
*Computer Science Department, Stanford University?Linguistics Department, Stanford University{spenceg,mcdm,horatio,manning}@stanford.eduAbstractMultiword expressions (MWE), a known nui-sance for both linguistics and NLP, blur thelines between syntax and semantics.
Previouswork onMWE identification has relied primar-ily on surface statistics, which perform poorlyfor longer MWEs and cannot model discontin-uous expressions.
To address these problems,we show that even the simplest parsing mod-els can effectively identify MWEs of arbitrarylength, and that Tree Substitution Grammarsachieve the best results.
Our experiments showa 36.4% F1 absolute improvement for Frenchover an n-gram surface statistics baseline, cur-rently the predominant method for MWE iden-tification.
Our models are useful for severalNLP tasks in which MWE pre-grouping hasimproved accuracy.1 IntroductionMultiword expressions (MWE) have long been achallenge for linguistic theory and NLP.
There isno universally accepted definition of the term, butMWEs can be characterized as ?idiosyncratic inter-pretations that cross word boundaries (or spaces)?
(Sag et al, 2002) such as traffic light, or as ?fre-quently occurring phrasal units which are subjectto a certain level of semantic opaqueness, or non-compositionality?
(Rayson et al, 2010).MWEs are often opaque fixed expressions, al-though the degree to which they are fixed can vary.Some MWEs do not allow morphosyntactic varia-tion or internal modification (e.g., in short, but *inshorter or *in very short).
Other MWEs are ?semi-fixed,?
meaning that they can be inflected or undergointernal modification.
The type of modification is of-ten limited, but not predictable, so it is not possibleto enumerate all variants (Table 1).French English?
terme in the near term?
court terme in the short term?
tr?s court terme in the very short term?
moyen terme in the mediumterm?
long terme in the long term?
tr?s long terme in the very long termTable 1: Semi-fixed MWEs in French and English.
TheFrench adverb ?
terme ?in the end?
can be modified bya small set of adjectives, and in turn some of these ad-jectives can be modified by an adverb such as tr?s ?very?.Similar restrictions appear in English.Merging known MWEs into single tokens hasbeen shown to improve accuracy for a variety ofNLP tasks: dependency parsing (Nivre and Nilsson,2004), constituency parsing (Arun andKeller, 2005),sentence generation (Hogan et al, 2007), and ma-chine translation (Carpuat andDiab, 2010).
Most ex-periments use gold MWE pre-grouping or language-specific resources like WordNet.
For unlabeled text,the best MWE identification methods, which arebased on surface statistics (Pecina, 2010), sufferfrom sparsity induced by longer n-grams (Ramischet al, 2010).
A dilemma thus exists: MWE knowl-edge is useful, but MWEs are hard to identify.In this paper, we show the effectiveness of statis-tical parsers for MWE identification.
Specifically,Tree Substitution Grammars (TSG) can achieve a36.4% F1 absolute improvement over a state-of-the-art surface statistics method.
We choose French,which has pervasive MWEs, for our experiments.Parsing models naturally accommodate discontinu-ous MWEs like phrasal verbs, and provide syntac-tic subcategorization.
By contrast, surface statisticsmethods are usually limited to binary judgements forcontiguous n-grams or dependency bigrams.725FTB (train) WSJ (train)Sentences 13,449 39,832Tokens 398,248 950,028#Word Types 28,842 44,389#Tag Types 30 45#Phrasal Types 24 27Per SentenceDepth (?/?2) 4.03 / 0.360 4.18 / 0.730Breadth (?/?2) 13.5 / 6.79 10.7 / 4.59Length (?/?2) 29.6 / 17.3 23.9 / 11.2Constituents (?)
20.3 19.6?
Constituents / ?
Length 0.686 0.820Table 2: Gross corpus statistics for the pre-processed FTB(training set) andWSJ (sec.
2-21).
The FTB sentences arelonger with broader syntactic trees.
The FTB POS tag sethas 33% fewer types than theWSJ.
The FTB dev set OOVrate is 17.77% vs. 12.78% for the WSJ.Type #Total #Single %Single %TotalMWN noun 9,680 2,737 28.3 49.7MWADV adverb 3,852 449 11.7 19.8MWP prep.
3,526 342 9.70 18.1MWC conj.
814 73 8.97 4.18MWV verb.
585 243 41.5 3.01MWD det.
328 69 21.0 1.69MWA adj.
324 126 38.9 1.66MWPRO pron.
266 33 12.4 1.37MWCL clitic 59 1 1.69 0.30MWET foreign 24 18 0.75 0.12MWI interj.
4 2 0.50 0.0219,462 4,093 21.0% 100.0%Table 3: Frequency distribution of the 11 MWE subcate-gories in the FTB (training set).
MWEs account for 7.08%of the bracketings and 13.0% of the tokens in the treebank.Only 21% of the MWEs occur once (?single?
).We first introduce a new instantiation of theFrench Treebank that, unlike previous work, does notuse gold MWE pre-grouping.
Consequently, our ex-perimental results also provide a better baseline forparsing raw French text.2 French Treebank SetupThe corpus used in our experiments is the FrenchTreebank (Abeill?
et al (2003), version from June2010, hereafter FTB).
In French, there is a linguis-tic tradition of lexicography which compiles listsof MWEs occurring in the language.
For exam-ple, Gross (1986) shows that dictionaries containabout 1,500 single-word adverbs but that French con-tains over 5,000 multiword adverbs.
MWEs occurin every part-of-speech (POS) category (e.g., nountrousse de secours ?first-aid kit?
; verb faire main-basse [do hand-low] ?seize?
; adverb comme dans dubeurre [as in butter] ?easily?
; adjective ??
part en-ti?re?
?wholly?
).The FTB explicitly annotates MWEs (also calledcompounds in prior work).
We used the subset ofthe corpus with functional annotations, not for thoseannotations but because this subset is known to bemore consistently annotated.
POS tags for MWEsare given not only at the MWE level, but also inter-nally: most tokens that constitute an MWE also havea POS tag.
Table 2 compares this part of the FTB tothe WSJ portion of the Penn Treebank.2.1 PreprocessingThe FTB requires significant pre-processing prior toparsing.Tokenization We changed the default tokenizationfor numbers by fusing adjacent digit tokens.
For ex-ample, 500 000 is tagged as an MWE composed oftwo words 500 and 000.
We made this 500000 andretained the MWE POS, although we did not markthe new token as an MWE.
For consistency, we usedone token for punctuated numbers like ?17,9?.MWE Tagging We marked MWEs with a flatbracketing in which the phrasal label is the MWE-level POS tag with an ?MW?
prefix, and the preter-minals are the internal POS tags for each terminal.The resulting POS sequences are not always uniqueto MWEs: they appear in abundance elsewhere inthe corpus.
However, some MWEs contain normallyungrammatical POS sequences (e.g., adverb ?
la vavite ?in a hurry?
: PDVADV [at the goes quick]), andsome words appear only as part of an MWE, such asinsu in ?
l?insu de ?to the ignorance of?.Labels We augmented the basic FTB label set?which contains 14 POS tags and 19 phrasal tags?intwo ways.
First, we added 16 finer-grained POS tagsfor punctuation.1 Second, we added the 11 MWE1Punctuation tag clusters?as used in the WSJ?did not im-prove accuracy.
Enriched tag sets like that of Crabb?
and Can-dito (2008) could also be investigated and compared to our re-sults since Evalb is insensitive to POS tags.726labels shown in Table 3, resulting in 24 total phrasalcategories.Corrections Historically, the FTB suffered fromannotation errors such as missing POS and phrasaltags (Arun and Keller, 2005).
We found that thisproblem has been largely resolved in the current re-lease.
However, 1,949 tokens and 36 MWE spansstill lacked tags.
We restored the labels by first as-signing each token its most frequent POS tag else-where in the treebank, and then assigning the mostfrequent MWE phrasal category for the resultingPOS sequence.2Split We used the 80/10/10 split described byCrabb?
and Candito (2008).
However, they used aprevious release of the treebank with 12,531 trees.3,391 trees have been added to the present version.We appended these extra trees to the training set, thusretaining the same development and test sets.2.2 Comparison to Prior FTB RepresentationsOur pre-processing approach is simple and auto-matic3 unlike the three major instantiations of theFTB that have been used in previous work:Arun-Cont and Arun-Exp (Arun and Keller,2005): Two instantiations of the full 20,000 sentencetreebank that differed principally in their treatment ofMWEs: (1) Cont, in which the tokens of eachMWEwere concatenated into a single token (en moyenne?
en_moyenne); (2)Exp, in which theyweremarkedwith a flat structure.
For both representations, theyalso gave results in which coordinated phrase struc-tures were flattened.
In the published experiments,they mistakenly removed half of the corpus, believ-ing that the multi-terminal (per POS tag) annotationsof MWEs were XML errors (Schluter and Genabith,2007).MFT (Schluter andGenabith, 2007): Manual revi-sion to 3,800 sentences.
Major changes included co-ordination raising, an expanded POS tag set, and the273 of the unlabeled word types did not appear elsewherein the treebank.
All but 11 of these were nouns.
We manuallyassigned the correct tags, but we would not expect a negativeeffect by deterministically labeling all of them as nouns.3We automate tree manipulation with Tregex/Tsurgeon(Levy and Andrew, 2006).
Our pre-processing package is avail-able at http://nlp.stanford.edu/software/lex-parser.shtml.correction of annotation errors.
Like Arun-Cont,MFT contains concatenated MWEs.FTB-UC (Candito and Crabb?, 2009): An in-stantiation of the functionally annotated section thatmakes a distinction between MWEs that are ?syn-tactically regular?
and those that are not.
Syntacti-cally regular MWEs were given internal structure,while all other MWEs were concatenated into sin-gle tokens.
For example, nouns followed by ad-jectives, such as loi agraire ?land law?
or Unionmon?taire et ?conomique ?monetary and economicUnion?
were considered syntactically regular.
Theyare MWEs because the choice of adjective is arbi-trary (loi agraire and not *loi agricole, similarly to?coal black?
but not *?crow black?
for example), buttheir syntactic structure is not intrinsic to MWEs.In such cases, FTB-UC gives the MWE a conven-tional analysis of an NP with internal structure.
Suchanalysis is indeed sufficient to recover the mean-ing of these semantically compositional MWEs thatare extremely productive.
On the other hand, theFTB-UC loses information about MWEs with non-compositional semantics.Almost all work on the FTB has followed Arun-Cont and used goldMWEpre-grouping.
As a result,most results for French parsing are analogous to earlyresults for Chinese, which used gold word segmen-tation, and Arabic, which used gold clitic segmenta-tion.
Candito et al (2010) were the first to acknowl-edge and address this issue, but they still used FTB-UC (with some pre-grouped MWEs).
Since the syn-tax and definition of MWEs is a contentious issue,we take a more agnostic view?which is consistentwith that of the FTB annotators?and leave them to-kenized.
This permits a data-oriented approach toMWE identification that is more robust to changesto the status of specific MWE instances.To set a baseline prior to grammar development,we trained the Stanford parser (Klein and Manning,2003) with no grammar features, achieving 74.2%labeled F1 on the development set (sentences ?
40words).
This is lower than the most recent results ob-tained by Seddah (2010).
However, the results arenot comparable: the data split was different, theymade use of morphological information, and moreimportantly they concatenated MWEs.
The focus of727our work is on models and data representations thatenable MWE identification.3 MWEs in Lexicon-GrammarThe MWE representation in the FTB is close tothe one proposed in the Lexicon-Grammar (Gross,1986).
In the Lexicon-Grammar, MWEs are classi-fied according to their global POS tags (noun, verb,adverb, adjective), and described in terms of the se-quence of the POS tags of the words that constitutethe MWE (e.g., ?N de N?
garde d?enfant [guard ofchild] ?daycare?, pied de guerre [foot of war] ?at theready?).
In other words, MWEs are represented by aflat structure.
The Lexicon-Grammar distinguishesbetween units that are fixed and have to appear as is(en tout et pour tout [in all and for all] ?in total?)
andunits that accept some syntactic variation such as ad-mitting the insertion of an adverb or adjective, or thevariation of one of the words in the expression (e.g.,a possessive as in ?from the top of one?s hat?).
It alsonotes whether the MWE displays some selectionalpreferences (e.g., it has to be preceded by a verb orby an adjective).Our FTB instantiation is largely consistent withthe Lexicon-Grammar.
Recall that we defined differ-ent MWE categories based on the global POS.
Wenow detail three of the categories.MWN The MWN category consists of propernouns (1a), foreign common nouns (1b), as well ascommon nouns.
The common nouns appear in sev-eral syntactically regular sequences of POS tags (2).Multiword nouns allow inflection (singular vs. plu-ral) but no insertion.
(1) a. London Sunday Times, Los Angelesb.
week - end, mea culpa, joint - venture(2) a. NA: corpsm?dical ?medical staff?, dettepublique ?public debt?b.
N PN:mode d?emploi ?instruction man-ual?c.
N N: num?ro deux ?number two?, mai-sonm?re [housemother] ?headquarters?,gr?ve surprise ?sudden strike?d.
N P D N: imp?t sur le revenu ?incometax?, ministre de l?
?conomie ?financeminister?MWA Multiword adjectives appear with differentPOS sequences (3).
They include numbers such asvingt et uni?me ?21st?.
Some items in (3b) allow in-ternal variation: some adverbs or adjectives can beadded to both examples given (?
tr?s haut risque, detoute derni?re minute).
(3) a. P N: d?antan [from before] ?old?, enquestion ?under discussion?b.
P A N: ?
haut risque ?high-risk?, dederni?re minute [from the last minute]?at the eleventh hour?c.
A C A: pur et simple [pure and simple]?straightforward?, noir et blanc ?blackand white?MWV Multiword verbs also appear in several POSsequences (4).
All verbs allow number and tense in-flections.
Some MWVs containing a noun or an ad-jective allow the insertion of a modifier (e.g., don-ner grande satisfication ?give great satisfaction?
),whereas others do not.
When an adverb intervenesbetween the main verb and its complement, the FTBmarks the two parts of the MWV discontinuously(e.g., [MWV [V prennent]] [ADV d?j?]
[MWV [P en] [Ncause]] ?already take into account?).
(4) a. V N: avoir lieu ?take place?, donner sat-isfaction ?give satisfaction?b.
V P N: mettre en place ?put in place?,entrer en vigueur ?to come into effect?c.
V P ADV: mettre ?
mal [put at bad]?harm?, ?tre ?
m?me [be at same] ?beable?d.
V D N P N: tirer la sonnette d?alarme?ring the alarm bell?, avoir le vent enpoupe ?to have the wind astern?4 Parsing ModelsWe develop two parsers for French with the goalof improving MWE identification.
The first is amanually-annotated grammar that we incorporateinto the Stanford parser.
Manual annotation results inhuman interpretable grammars that can inform futuretreebank annotation decisions.
Moreover, the gram-mar can be used as the base distribution in our sec-ond model, a Probabilistic Tree Substitution Gram-mar (PTSG) parser.
PTSGs learn parameters for tree728Feature States Tags F1 ?F1?
4325 31 74.21tagPA 4509 215 76.94 +2.73markInf 4510 216 77.42 +0.48markPart 4511 217 77.73 +0.31markVN 5986 217 78.32 +0.59markCoord 7361 217 78.45 +0.13markDe 7521 233 79.11 +0.66markP 7523 235 79.34 +0.23markMWE 7867 235 79.23 ?0.11Table 4: Effects on grammar size and labeled F1 for eachof the manual state splits (development set, sentences ?40 words).
markMWE decreases overall accuracy, butincreases both the number of correctly parsed trees (by0.30%) and per category MWE accuracy.fragments larger than basic CFG rules.
PTSG rulesmay also be lexicalized.
This means that commonlyobserved collocations?some of which areMWEs?can be stored in the grammar.4.1 Stanford ParserWe configure the Stanford parser with settings thatare effective for other languages: selective parent an-notation, lexicon smoothing, and factored parsing.We use the head-finding rules of Dybro-Johansen(2004), which we find to yield an approximately1.0% F1 development set improvement over those ofArun (2004).
Finally, we include a simple unknownword model consisting entirely of surface features:- Nominal, adjectival, verbal, adverbial, and plu-ral suffixes- Contains a digit or punctuation- Is capitalized (except the first word in a sen-tence)- Consists entirely of capital letters- If none of the above, add a one- or two-charactersuffixCombined with the grammar features, this unknownword model yields 97.3% tagging accuracy on thedevelopment set.4.1.1 Grammar DevelopmentTable 4 lists the symbol refinements used in ourgrammar.
Most of the features are POS splits asmany phrasal tag splits did not lead to any improve-ment.
Parent annotation of POS tags (tagPA) cap-tures information about the external context.
mark-Inf and markPart accomplish a finite/nonfinite dis-tinction: they respectively specify whether the verbis an infinitive or a participle based on the type ofthe grandparent node.
markVN captures the notionof verbal distance as in Klein and Manning (2003).We opted to keep the COORD phrasal tag, andto capture parallelism in coordination, we mark CO-ORD with the type of its child (NP, AP, VPinf, etc.
).markDe identifies the preposition de and its variants(du, des, d?)
which is very frequent and appears inseveral different contexts.
markP identifies preposi-tions which introduce PPs modifying a noun.
Mark-ing other kinds of prepositional modifiers (e.g., verb)did not help.
markMWE adds an annotation to sev-eral MWE categories for frequently occuring POSsequences.
For example, we mark MWNs that occurmore than 600 times (e.g., ?N P N?
and ?N N?
).4.2 DP-TSG ParserA shortcoming of CFG-based grammars is that theydo not explicitly capture idiomatic usage.
For exam-ple, consider the two utterances:(5) a.
He [MWV kicked the bucket] .b.
He [VP kicked [NP the pail]] .The examples in (5) may be equally probable and re-ceive the same analysis under a PCFG; words aregenerated independently.
However, recall that inour representation, (5a) should receive a flat analysisas MWV, whereas (5b) should have a conventionalanalysis of the verb kicked and its two arguments.An alternate view of parsing is one in which newutterances are built from previously observed frag-ments.
This is the original motivation for data ori-ented parsing (DOP) (Bod, 1992), in which ?id-iomaticity is the rule rather than the exception?
(Scha, 1990).
If we have seen the collocation kickedthe bucket several times before, we should store thatwhole fragment for later use.We consider a variant of the non-parametric PTSGmodel of Cohn et al (2009) in which tree fragmentsare drawn from a Dirichlet process (DP) prior.4The DP-TSG can be viewed as a DOP model withBayesian parameter estimation.
A PTSG is a 5-tuple?V,?, R,?,??
where c ?
V are non-terminals;4Similar models were developed independently byO?Donnell et al (2009) and Post and Gildea (2009).729?c DP concentration parameter for each c ?
VP0(e|c) CFG base distributionx Set of non-terminal nodes in the treebankS Set of sampling sites (one for each x ?
x)S A block of sampling sites, where S ?
Sb = {bs}s?S Binary variables to be sampled (bs = 1 ?frontier node)z Latent state of the segmented treebankm Number of sites s ?
S s.t.
bS = 1n = {nc,e} Sufficient statistics of z?nS:m Change in counts by setting m sites in STable 5: DP-TSG model notation.
For consistency, welargely follow the notation of Liang et al (2010).
Notethat z = (b,x), and as such z = ?c, e?.t ?
?
are terminals; e ?
R are elementary trees;5?
?
V is a unique start symbol; and ?c,e ?
?
areparameters for each tree fragment.
A PTSG deriva-tion is created by successively applying the substitu-tion operator to the leftmost frontier node (denotedby c+).
All other nodes are internal (denoted by c?
).In the supervised setting, DP-TSG grammar ex-traction reduces to a segmentation problem.
We havea treebank T that we segment into the set R, a pro-cess that we model with Bayes?
rule:p(R | T ) ?
p(T | R) p(R) (1)Since the tree fragments completely specify eachtree, p(T | R) is either 0 or 1, so all work is per-formed by the prior over the set of elementary trees.The DP-TSG contains a DP prior for each c ?
V(Table 5 defines further notation).
We generate ?c, e?tuples as follows:?c|c, ?c, P0(?|c) ?
DP (?c, P0)e|?c ?
?cThe data likelihood is given by the latent state z andthe parameters ?
: p(z|?)
=?z?z ?nc,e(z)c,e .
Integrat-ing out the parameters, we have:p(z) =?c?V?e(?cP0(e|c))nc,e(z)?nc,?
(z)c(2)where xn = x(x + 1) .
.
.
(x + n ?
1) is the risingfactorial.
(?A.1 contains ancillary details.
)Base Distribution The base distribution P0 is thesame maximum likelihood PCFG used in the Stan-5We use the terms tree fragment and elementary tree inter-changeably.NP+PUNC-(1)?N+JacquesN-ChiracPUNC+(2)?Figure 1: Example of two conflicting sites of the sametype.
Define the type of a site t(z, s) def= (?ns:0,?ns:1).Sites (1) and (2) above have the same type since t(z, s1) =t(z, s2).
However, the two sites conflict since the prob-abilities of setting bs1 and bs2 both depend on counts forthe tree fragment rooted at NP.
Consequently, sites (1) and(2) are not exchangeable: the probabilities of their assign-ments depend on the order in which they are sampled.ford parser.6,7 After applying the manual state splits,we perform simple right binarization, collapse unaryrules, and replace rare words with their signatures(Petrov et al, 2006).For each non-terminal type c, we learn a stop prob-ability sc ?
Beta(1, 1).
Under P0, the probability ofgenerating a rule A+ ?
B?
C+ composed of non-terminals isP0(A+ ?
B?
C+) = pMLE(A ?
B C)sB(1?sC)(3)For lexical insertion rules, we add a penalty propor-tional to the frequency of the lexical item:P0(c ?
t) = pMLE(c ?
t)p(t) (4)where p(t) is equal to the MLE unigram probabil-ity of t in the treebank.
Lexicalizing a rule makes itvery specific, so we generally want to avoid lexical-ization with rare words.
Empirically, we found thatthis penalty reduces overfitting.Type-based Inference Algorithm To learn the pa-rameters ?
we use the collapsed, block Gibbs sam-pler of Liang et al (2010).
We sample binary vari-ables bs associated with each non-terminal node/sitein the treebank.
The key idea is to select a blockof exchangeable sites S of the same type that do notconflict (Figure 1).
Since the sites in S are exchange-able, we can set bS randomly so long as we know m,the number of sites with bs = 1.
Because this algo-rithm is a not a contribution of this paper, we referthe reader to Liang et al (2010).6The Stanford parser is a product model, so the results in ?5.1include the contribution of a dependency parser.7Bansal and Klein (2010) also experimented with symbol re-finement in an all-fragments (parametric) TSG for English.730After each Gibbs iteration, we sample each sc di-rectly using binomial-Beta conjugacy.
We re-samplethe DP concentration parameters ?c with the auxil-iary variable procedure of West (1995).Decoding We compute the rule score of each treefragment from a single grammar sample as follows:?c,e =nc,e(z) + ?cP0(e|c)nc,?
(z) + ?c(5)To make the grammar more robust, we also includeall CFG rules in P0 with zero counts inn.
Scores forthese rules follow from (5) with nc,e(z) = 0.For decoding, we note that the derivations of aTSG are a CFGparse forest (Vijay-Shanker andWeir,1993).
As such, we can use a Synchronous ContextFree Grammar (SCFG) to translate the 1-best parseto its derivation.
Consider a unique tree fragment eirooted at X with frontier ?, which is a sequence ofterminals and non-terminals.
We encode this frag-ment as an SCFG rule of the form[X ?
?
, X ?
i, Y1, .
.
.
, Yn] (6)where Y1, .
.
.
, Yn is the sequence of non-terminalnodes in ?.8 During decoding, the input is re-written as a sequence of tree fragment (rule) indices{i, j, k, .
.
.
}.
Because the TSG substitution operatoralways applies to the leftmost frontier node, we candeterministically recover the monolingual parse withtop-down re-writes of ?.The SCFG formulation has a practical benefit: wecan take advantage of the heavily-optimized SCFGdecoders for machine translation.
We use cdec(Dyer et al, 2010) to recover the Viterbi derivationunder a DP-TSG grammar sample.5 Experiments5.1 Standard Parsing ExperimentsWe evaluate parsing accuracy of the Stanford andDP-TSG models (Table 6).
For comparison, we alsoinclude the Berkeley parser (Petrov et al, 2006).9For the DP-TSG, we initialized all bs with fair cointosses and ran for 400 iterations, after which likeli-hood stopped improving.8This formulation is due to Chris Dyer.9Training settings: right binarization, no parent annotation,six split-merge cycles, and random initialization.Leaf Ancestor EvalbCorpus Sent LP LR F1 EX%PA-PCFG 0.793 0.812 68.1 67.0 67.6 10.5DP-TSG 0.823 0.842 75.6 76.0 75.8 15.1Stanford 0.843 0.861 77.8 79.0 78.4 17.5Berkeley 0.880 0.891 82.4 82.0 82.2 21.4Table 6: Standard parsing experiments (test set, sentences?
40 words).
All parsers exceed 96% tagging accuracy.Berkeley and DP-TSG results are the average of three in-dependent runs.We report two different parsing metrics.
Evalbis the standard labeled precision/recall metric.10Leaf Ancestor measures the cost of transformingguess trees to the reference (Sampson and Babar-czy, 2003).
It was developed in response to the non-terminal/terminal ratio bias of Evalb, which penal-izes flat treebanks like the FTB.
The range of thescore is between 0 and 1 (higher is better).
We reportmicro-averaged (whole corpus) and macro-averaged(per sentence) scores.In terms of parsing accuracy, the Berkeley parserexceeds both Stanford and DP-TSG.
This is consis-tent with previous experiments for French by Sed-dah et al (2009), who show that the Berkeley parseroutperforms other models.
It also matches the or-dering for English (Cohn et al, 2010; Liang et al,2010).
However, the standard baseline for TSGmod-els is a simple parent-annotated PCFG (PA-PCFG).For English, Liang et al (2010) showed that a similarDP-TSG improved over PA-PCFG by 4.2% F1.
ForFrench, our gain is a more substantial 8.2% F1.5.2 MWE Identification ExperimentsTable 7 lists overall and per-category MWE identifi-cation results for the parsing models.
Although DP-TSG is less accurate as a general parsing model, it ismore effective at identifying MWEs.The predominant approach to MWE identificationis the combination of lexical association measures(surface statistics) with a binary classifier (Pecina,2010).
A state-of-the-art, language independentpackage that implements this approach for higherorder n-grams is mwetoolkit (Ramisch et al,2010).11 In Table 8 we compare DP-TSG to both10Available at http://nlp.cs.nyu.edu/evalb/ (v.20080701).11Available at http://multiword.sourceforge.net/.
See ?A.2 for731#gold Stanford DP-TSG BerkeleyMWET 3 0.0 0.0 0.0MWV 26 64.0 57.7 50.7MWA 8 26.1 32.2 29.8MWN 456 64.1 67.6 67.1MWD 15 70.3 65.5 70.1MWPRO 17 73.7 78.0 76.2MWADV 220 74.6 72.7 70.4MWP 162 81.3 80.5 77.7MWC 47 83.5 83.5 80.8954 70.1 71.1 69.6Table 7: MWE identification per category and overall re-sults (test set, sentences ?
40 words).
MWI and MWCLdo not occur in the test set.Model F1mwetoolkit All 15.4PA-PCFG 32.6mwetoolkit Filter 34.7PA-PCFG+Features 63.1DP-TSG 71.1Table 8: MWE identification F1 of the best parsing modelvs.
the mwetoolkit baseline (test set, sentences ?
40words).
PA-PCFG+Features includes the grammar fea-tures in Table 4, which is the CFG from which the TSG isextracted.
For mwetoolkit, All indicates the inclusionof all n-grams in the training corpus.
Filter indicates pre-filtering of the training corpus by removing rare n-grams(see ?A.2 for details).mwetoolkit and the CFG from the which the TSGis extracted.
The TSG-based parsing model outper-forms mwetoolkit by 36.4% F1 while providingsyntactic subcategory information.6 DiscussionAutomatic learning methods run the risk of produc-ing uninterpretable models.
However, the DP-TSGmodel learns useful generalizations over MWEs.
Asample of the rules is given in Table 9.
Some spe-cific sequences like ?
[MWN [coup de N]]?
are part ofthe grammar: such rules can indeed generate quitea few MWEs, e.g., coup de pied ?kick?, coup decoeur, coup de foudre ?love at first sight?, coup demain ?help?, coup d?
?tat, coup de gr?ce (note thatonly some of these MWEs are seen in the trainingconfiguration details.MWN MWV MWPsoci?t?s de N sous - V de l?ordre deprix de N faire N y compriscoup de N V les moyens au N deN d?
?tat V de N en N deN de N V en N ADV deN ?
NTable 9: Sample of the TSG rules learned.MWNNtourPdeNpasse--Npasse(a) ReferenceNPNtourPPPdeNPMWNNpasse--Npasse(b) DP-TSGFigure 2: Example of an MWE error for tour de passe-passe ?magic trick?.
(dev set)data).
For MWV, ?V de N?
as in avoir de cesse ?giveno peace?, perdre de vue [lose from sight] ?forget?,prendre de vitesse [take from speed] ?outpace?
), islearned.
For prepositions, the grammar stores fullsubtrees of MWPs, but can also generalize the struc-ture of very frequent sequences: ?en N de?
occurs inmanymultiword prepositions (e.g., en compagnie de,en face de, en mati?re de, en terme de, en cours de,en faveur de, en raison de, en fonction de).
The TSGgrammar thus provides a categorization of MWEsconsistent with the Lexicon-Grammar.
It also learnsverbal phrases which contain discontinuous MWVsdue to the insertion of an adverb or negation such as?
[VN [MWV va] [MWADV d?ailleurs] [MWV bon train]]?
[go indeed well], ?
[VN [MWV a] [ADV jamais] [MWV?t?
question d?]]?
[has never been in question].A significant fraction of errors for MWNs occurwith adjectives that are not recognized as part of theMWE.
For example, since ?tablissements priv?s ?pri-vate corporation?
is unseen in the training data, it isnot found.
Sometimes the parser did not recognizethe whole structure of an MWE.
Figure 2 shows anexample where the parser only found a subpart of theMWN tour de passe-passe ?magic trick?.Other DP-TSG errors are due to inconsistencies inthe FTB annotation.
For example, sous pr?texte que732MWCPsousNpr?texteCque(a) ReferencePPPsousNPNpr?texteSsubCque(b) ReferenceFigure 3: Example of an inconsistent FTB annotation forsous pr?texte que ?on the pretext of?.
?on the pretext of?
is tagged as both MWC and as aregular PP structure (Figure 3).
However, the parseralways assigns a MWC structure, which is a betteranalysis than the gold annotation.
We expect thatmore consistent annotation would help the DP-TSGmore than the CFG-based parsers.The DP-TSG is not immune to false positives: inLe march?
national, fait-on remarquer, est enfin enr?gression .
.
.
?The national economy, people at lastnote, is going down?
the parser tags march?
nationalasMWN.
As noted, the boundary of what should andshould not count as an MWE can be fuzzy, and it istherefore hard to assess whether or not this should bean MWE.
The FTB does not mark it as one.There are multiple examples were the DP-TSGfound the MWE whereas Stanford (its base distribu-tion) did not, such as in Figure 4.
Note that the ?NP N?
structure is quite frequent for MWNs, but theTSG correctly identifies the MWADV in emplois ?domicile [jobs at home] ?homeworking?.7 Related WorkThere is a voluminous literature on MWE identi-fication.
Here we review closely related syntax-based methods.12 The linguistic and computa-tional attractiveness of lexicalized grammars formodeling idiosyncratic constructions in French wasidentified by Abeill?
(1988) and Abeill?
and Sch-abes (1989).
They manually developed a smallTree Adjoining Grammar (TAG) of 1,200 elemen-tary trees and 4,000 lexical items that includedMWEs.
The classic statistical approach to MWEidentification, Xtract (Smadja, 1993), used an in-12See Seretan (2011) for a comprehensive survey of syntax-based methods for MWE identification.
For an overview of n-gram methods like mwetoolkit, see Pecina (2010).MWNNcampagnePdeNpromotion(a) DP-TSGNPNcampagnePPPdeNPNpromotion(b) StanfordNPNemploisMWADVP?Ndomicile(c) DP-TSGNPNemploisPPP?NPNdomicile(d) StanfordFigure 4: Correct analyses by DP-TSG.
(dev set)cremental parser in the third stage of its pipelineto identify predicate-argument relationships.
Lin(1999) applied information-theoretic measures toautomatically-extracted dependency relations to findMWEs.
To our knowledge, Wehrli (2000) was thefirst to use syntactically annotated corpora to im-prove a parser for MWE identification.
He pro-posed to rank analyses of a symbolic parser basedon the presence of collocations, although details ofthe ranking function were not provided.The most similar work to ours is that of Nivreand Nilsson (2004), who converted a Swedish cor-pus into two versions: one in which MWEs wereleft as tokens, and one in which they were merged.On the first version, they showed that a deterministicdependency parser could identify MWEs at 71.1%F1, albeit without subcategory information.
Onthe second version?which simulated perfect MWEidentification?they showed that labeled attachmentimproved by about 1%.Recent statistical parsing work on French has in-cluded Stochastic Tree Insertion Grammars (STIGs),which are related to TAGs, but with a restricted ad-junction operation.13 Seddah et al (2009) and Sed-dah (2010) showed that STIGs underperform CFG-based parsers on the FTB.
In their experiments,MWEs were concatenated.13TSGs differ from TAGs and STIGs in that they do not in-clude an adjunction operator.7338 ConclusionThe main result of this paper is that an existing sta-tistical parser can achieve a 36.4% F1 absolute im-provement for MWE identification over a state-of-the-art n-gram surface statistics package.
Parsersalso provide syntactic subcategorization, and do notrequire pre-filtering of the training data.
We havealso demonstrated that TSGs can capture idiomaticusage better than a PCFG.While the DP-TSG, whichis a relatively new parsing model, still lags state-of-the-art parsers in terms of overall labeling accuracy,we have shown that it is already very effective forother tasks like MWE identification.
We plan to im-prove the DP-TSG by experimenting with alternateparsing objectives (Cohn et al, 2010), lexical rep-resentations, and parameterizations of the base dis-tribution.
A particularly promising base distributionis the latent variable PCFG learned by the Berkeleyparser.
However, initial experiments with this distri-bution were negative, so we leave further develop-ment to future work.We chose French for these experiments due to thepervasiveness ofMWEs and the availability of an an-notated corpus.
However, MWE lists and syntactictreebanks exist for many of the world?s major lan-guages.
We will investigate automatic conversion ofthese treebanks (by flattening MWE bracketings) forMWE identification.A AppendixA.1 Notes on the Rising FactorialThe rising factorial?also known as the ascendingfactorial or Pochhammer symbol?arises in the con-text of samples from a Dirichlet process (see Prop.3 of Antoniak (1974) for details).
For a positive in-teger n and a complex number x, the rising factorialxn is defined14 byxn = x(x + 1) .
.
.
(x + n?
1)=n?j=1(x + j ?
1) (7)The rising factorial can be generalized to a com-plex number ?
with the gamma function:x?
= ?
(x + ?)?
(x) (8)14We adopt the notation of Knuth (1992).where x0 ?
1.In our type-based sampler, we computed (7) di-rectly in a dynamic program.
We found that (8) wasprohibitively slow for sampling.A.2 mwetoolkit ConfigurationWe configured mwetoolkit15 with the four stan-dard lexical features: the maximum likelihood esti-mator, Dice?s coefficient, pointwise mutual informa-tion (PMI), and Student?s t-score.
We added the POSsequence for each n-gram as a single feature.
We re-moved the web counts features to make the experi-ments comparable.
To compensate for the absenceof web counts, we computed the lexical features us-ing the gold lemmas from the FTB instead of usingan automatic lemmatizer.Since MWE n-grams only account for a smallfraction of the n-grams in the corpus, we filtered thetraining and test sets by removing all n-grams thatoccurred once.
To further balance the proportion ofMWEs, we trained on all valid MWEs plus 10x ran-domly selected non-MWE n-grams.
This proportionmatches the fraction of MWE/non-MWE tokens inthe FTB.
Since we generated a random training set,we reported the average of three independent runs.We created feature vectors for the training n-grams and trained a binary Support Vector Machine(SVM) classifier with Weka (Hall et al, 2009).
Al-though mwetoolkit defaults to a linear kernel,we achieved higher accuracy on the development setwith an RBF kernel.The FTB is sufficiently large for the corpus-basedmethods implemented in mwetoolkit.
Ramischet al (2010)?s experiments were on Genia, whichcontains 18k sentences and 490k tokens, similar tothe FTB.
Their test set had 895 sentences, smallerthan ours.
They reported 30.6% F1 for their taskagainst an Xtract baseline, which only obtained 7.3%F1.
These results are comparable inmagnitude to ourFTB results.Acknowledgments We thank Marie Candito, Chris Dyer,Dan Flickinger, Percy Liang, Carlos Ramisch, Djam?Seddah, and Val Spitkovsky for their helpful comments.The first author is supported by a National Defense Sci-ence and Engineering Graduate (NDSEG) fellowship.15We re-implemented mwetoolkit in Java for compatibil-ity with Weka and our pre-processing routines.734ReferencesA.
Abeill?
and Y. Schabes.
1989.
Parsing idioms in lexicalizedTAGs.
In EACL.A.
Abeill?, L. Cl?ment, and A. Kinyon, 2003.
Building a tree-bank for French, chapter 10.
Kluwer.A.
Abeill?.
1988.
Parsing Frenchwith TreeAdjoiningGrammar:some linguistic accounts.
In COLING.C.
E. Antoniak.
1974.
Mixtures of Dirichlet processes with ap-plications to Bayesian nonparametric problems.
The Annalsof Statistics, 2(6):1152?1174.A.
Arun and F. Keller.
2005.
Lexicalization in crosslinguisticprobabilistic parsing: The case of French.
In ACL.A.
Arun.
2004.
Statistical parsing of the French treebank.
Tech-nical report, University of Edinburgh.M.
Bansal and D. Klein.
2010.
Simple, accurate parsing withan all-fragments grammar.
In ACL.R.
Bod.
1992.
A computation model of language performance:Data-Oriented Parsing.
In COLING.M.
Candito and B. Crabb?.
2009.
Improving generative statisti-cal parsing with semi-supervised word clustering.
In IWPT.M.
Candito, B.
Crabb?, and P. Denis.
2010.
Statistical Frenchdependency parsing: treebank conversion and first results.
InLREC.M.
Carpuat and M. Diab.
2010.
Task-based evaluation of mul-tiword expressions: a pilot study in statistical machine trans-lation.
In HLT-NAACL.T.
Cohn, S. Goldwater, and P. Blunsom.
2009.
Inducing compactbut accurate tree-substitution grammars.
In HLT-NAACL.T.
Cohn, P. Blunsom, and S. Goldwater.
2010.
Inducing tree-substitution grammars.
JMLR, 11:3053?3096, Nov.B.
Crabb?
and M. Candito.
2008.
Exp?riences d?analyse syn-taxique statistique du fran?ais.
In TALN.A.
Dybro-Johansen.
2004.
Extraction automatique de gram-maires ?
partir d?un corpus fran?ais.
Master?s thesis, Univer-sit?
Paris 7.C.
Dyer, A. Lopez, J. Ganitkevitch, J.Weese, F. Ture, et al 2010.cdec: A decoder, alignment, and learning framework forfinite-state and context-free translation models.
In ACL Sys-tem Demonstrations.M.
Gross.
1986.
Lexicon-Grammar: the representation of com-pound words.
In COLING.M.
Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, andI.
H. Witten.
2009.
The WEKA data mining software: anupdate.
SIGKDD Explorations Newsletter, 11:10?18.D.
Hogan, C. Cafferkey, A. Cahill, and J. van Genabith.
2007.Exploiting multi-word units in history-based probabilisticgeneration.
In EMNLP-CoNLL.D.
Klein and C. D. Manning.
2003.
Accurate unlexicalizedparsing.
In ACL.D.
E. Knuth.
1992.
Two notes on notation.
American Mathe-matical Monthly, 99:403?422, May.R.
Levy and G. Andrew.
2006.
Tregex and Tsurgeon: tools forquerying and manipulating tree data structures.
In LREC.P.
Liang, M. I. Jordan, and D. Klein.
2010.
Type-based MCMC.In HLT-NAACL.D.
Lin.
1999.
Automatic identification of non-compositionalphrases.
In ACL.J.
Nivre and J. Nilsson.
2004.
Multiword units in syntactic pars-ing.
In Methodologies and Evaluation of Multiword Units inReal-World Applications (MEMURA).T.
J. O?Donnell, J.
B. Tenenbaum, and N. D. Goodman.
2009.Fragment grammars: Exploring computation and reuse inlanguage.
Technical report, MIT Computer Science and Arti-ficial Intelligence Laboratory Technical Report Series, MIT-CSAIL-TR-2009-013.P.
Pecina.
2010.
Lexical association measures and collocationextraction.
Language Resources and Evaluation, 44:137?158.S.
Petrov, L. Barrett, R. Thibaux, and D. Klein.
2006.
Learningaccurate, compact, and interpretable tree annotation.
In ACL.M.
Post and D. Gildea.
2009.
Bayesian learning of a tree sub-stitution grammar.
In ACL-IJCNLP, Short Papers.C.
Ramisch, A. Villavicencio, and C. Boitet.
2010. mwe-toolkit: a framework for multiword expression identifi-cation.
In LREC.P.
Rayson, S. Piao, S. Sharoff, S. Evert, and B. Moir?n.
2010.Multiword expressions: hard going or plain sailing?
Lan-guage Resources and Evaluation, 44:1?5.I.
A.
Sag, T. Baldwin, F. Bond, A. Copestake, and D. Flickinger.2002.
Multiword expressions: A pain in the neck for NLP.In CICLing.G.
Sampson and A. Babarczy.
2003.
A test of the leaf-ancestormetric for parse accuracy.
Natural Language Engineering,9:365?380.R.
Scha, 1990.
Taaltheorie en taaltechnologie: competence enperformance, pages 7?22.
Landelijke Vereniging van Neer-landici (LVVNjaarboek).N.
Schluter and J. Genabith.
2007.
Preparing, restructuring,and augmenting a French treebank: Lexicalised parsers orcoherent treebanks?
In Pacling.D.
Seddah, M. Candito, and B. Crabb?.
2009.
Cross parserevaluation and tagset variation: a French treebank study.
InIWPT.D.
Seddah.
2010.
Exploring the Spinal-STIG model for parsingFrench.
In LREC.V.
Seretan.
2011.
Syntax-Based Collocation Extraction, vol-ume 44 of Text, Speech, and Language Technology.
Springer.F.
Smadja.
1993.
Retrieving collocations from text: Xtract.Computational Linguistics, 19:143?177.K.
Vijay-Shanker and D. J. Weir.
1993.
The use of shared forestsin tree adjoining grammar parsing.
In EACL.E.
Wehrli.
2000.
Parsing and collocations.
In Natural Lan-guage Processing?NLP 2000, volume 1835 of Lecture Notesin Computer Science, pages 272?282.
Springer.M.
West.
1995.
Hyperparameter estimation in Dirichlet processmixture models.
Technical report, Duke University.735
