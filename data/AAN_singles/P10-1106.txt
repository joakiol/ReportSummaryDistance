Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1040?1047,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsAn Exact A* Method for Deciphering Letter-Substitution CiphersEric Corlett and Gerald PennDepartment of Computer ScienceUniversity of Toronto{ecorlett,gpenn}@cs.toronto.eduAbstractLetter-substitution ciphers encode a docu-ment from a known or hypothesized lan-guage into an unknown writing system oran unknown encoding of a known writingsystem.
It is a problem that can occur ina number of practical applications, such asin the problem of determining the encod-ings of electronic documents in which thelanguage is known, but the encoding stan-dard is not.
It has also been used in rela-tion to OCR applications.
In this paper, weintroduce an exact method for decipher-ing messages using a generalization of theViterbi algorithm.
We test this model on aset of ciphers developed from various websites, and find that our algorithm has thepotential to be a viable, practical methodfor efficiently solving decipherment prob-lems.1 IntroductionLetter-substitution ciphers encode a documentfrom a known language into an unknown writ-ing system or an unknown encoding of a knownwriting system.
This problem has practical sig-nificance in a number of areas, such as in readingelectronic documents that may use one of manydifferent standards to encode text.
While this is nota problem in languages like English and Chinese,which have a small set of well known standard en-codings such as ASCII, Big5 and Unicode, thereare other languages such as Hindi in which thereis no dominant encoding standard for the writingsystem.
In these languages, we would like to beable to automatically retrieve and display the in-formation in electronic documents which use un-known encodings when we find them.
We alsowant to use these documents for information re-trieval and data mining, in which case it is impor-tant to be able to read through them automatically,without resorting to a human annotator.
The holygrail in this area would be an application to ar-chaeological decipherment, in which the underly-ing language?s identity is only hypothesized, andmust be tested.
The purpose of this paper, then,is to simplify the problem of reading documentsin unknown encodings by presenting a new algo-rithm to be used in their decipherment.
Our algo-rithm operates by running a search over the n-gramprobabilities of possible solutions to the cipher, us-ing a generalization of the Viterbi algorithm thatis wrapped in an A* search, which determines ateach step which partial solutions to expand.
Itis guaranteed to converge on the language-model-optimal solution, and does not require restarts orrisk falling into local optima.
We specifically con-sider the problem of finding decodings of elec-tronic documents drawn from the internet, andwe test our algorithm on ciphers drawn from ran-domly selected pages of Wikipedia.
Our testingindicates that our algorithm will be effective in thisdomain.It may seem at first that automatically decoding(as opposed to deciphering) a document is a sim-ple matter, but studies have shown that simple al-gorithms such as letter frequency counting do notalways produce optimal solutions (Bauer, 2007).If the text from which a language model is trainedis of a different genre than the plaintext of a cipher,the unigraph letter frequencies may differ substan-tially from those of the language model, and sofrequency counting will be misleading.
Becauseof the perceived simplicity of the problem, how-ever, little work was performed to understand itscomputational properties until Peleg and Rosen-feld (1979), who developed a method that repeat-edly swaps letters in a cipher to find a maximumprobability solution.
Since then, several differentapproaches to this problem have been suggested,some of which use word counts in the languageto arrive at a solution (Hart, 1994), and some of1040which treat the problem as an expectation max-imization problem (Knight et al, 2006; Knight,1999).
These later algorithms are, however, highlydependent on their initial states, and require anumber of restarts in order to find the globally op-timal solution.
A further contribution was made by(Ravi and Knight, 2008), which, though publishedearlier, was inspired in part by the method pre-sented here, first discovered in 2007.
Unlike thepresent method, however, Ravi and Knight (2008)treat the decipherment of letter-substitution ci-phers as an integer programming problem.
Cleverthough this constraint-based encoding is, their pa-per does not quantify the massive running timesrequired to decode even very short documentswith this sort of approach.
Such inefficiency indi-cates that integer programming may simply be thewrong tool for the job, possibly because languagemodel probabilities computed from empirical dataare not smoothly distributed enough over the spacein which a cutting-plane method would attempt tocompute a linear relaxation of this problem.
Inany case, an exact method is available with a muchmore efficient A* search that is linear-time in thelength of the cipher (though still horribly exponen-tial in the size of the cipher and plain text alpha-bets), and has the additional advantage of beingmassively parallelizable.
(Ravi and Knight, 2008)also seem to believe that short cipher texts aresomehow inherently more difficult to solve thanlong cipher texts.
This difference in difficulty,while real, is not inherent, but rather an artefact ofthe character-level n-gram language models thatthey (and we) use, in which preponderant evidenceof differences in short character sequences is nec-essary for the model to clearly favour one letter-substitution mapping over another.
Uniform char-acter models equivocate regardless of the length ofthe cipher, and sharp character models with manyzeroes can quickly converge even on short ciphersof only a few characters.
In the present method,the role of the language model can be acutely per-ceived; both the time complexity of the algorithmand the accuracy of the results depend crucially onthis characteristic of the language model.
In fact,we must use add-one smoothing to decipher textsof even modest lengths because even one unseenplain-text letter sequence is enough to knock outthe correct solution.
It is likely that the methodof (Ravi and Knight, 2008) is sensitive to this aswell, but their experiments were apparently fixedon a single, well-trained model.Applications of decipherment are also exploredby (Nagy et al, 1987), who uses it in the con-text of optical character recognition (OCR).
Theproblem we consider here is cosmetically relatedto the ?L2P?
(letter-to-phoneme) mapping prob-lem of text-to-speech synthesis, which also fea-tures a prominent constraint-based approach (vanden Bosch and Canisius, 2006), but the constraintsin L2P are very different: two different instancesof the same written letter may legitimately map totwo different phonemes.
This is not the case inletter-substitution maps.2 TerminologySubstitution ciphers are ciphers that are definedby some permutation of a plaintext alphabet.
Ev-ery character of a plaintext string is consistentlymapped to a single character of an output stringusing this permutation.
For example, if we tookthe string ?hello world?
to be the plaintext, thenthe string ?ifmmp xpsme?
would be a cipherthat maps e to f , l to m, and so on.
It is easyto extend this kind of cipher so that the plaintextalphabet is different from the ciphertext alphabet,but still stands in a one to one correspondence toit.
Given a ciphertext C, we say that the set ofcharacters used inC is the ciphertext alphabet ?C ,and that its size is nC .
Similarly, the entire possi-ble plaintext alphabet is ?P , and its size is is nP .Since nC is the number of letters actually usedin the cipher, rather than the entire alphabet it issampled from, we may find that nC < nP evenwhen the two alphabets are the same.
We refer tothe length of the cipher string C as clen.
In theabove example, ?P is { , a, .
.
.
z} and nP = 27,while ?C = { , e, f, i,m, p, s, x}, clen = 11 andnC = 8.Given the ciphertext C, we say that a partialsolution of size k is a map ?
= {p1 : c1, .
.
.
pk :ck}, where c1, .
.
.
, ck ?
?C and are distinct, andp1, .
.
.
, pk ?
?P and are distinct, and where k ?nC .
If for a partial solution ?
?, we have that ?
??
?, then we say that ??
extends ?.
If the size of ??
isk+1 and ?
is size k, we say that ??
is an immediateextension of ?.
A full solution is a partial solutionof size nC .
In the above example, ?1 = { : , d :e} would be a partial solution of size 2, and ?2 ={ : , d : e, g : m} would be a partial solutionof size 3 that immediately extends ?1.
A partialsolution ?T { : , d : e, e : f, h : i, l : m, o :1041p, r : s, w : x} would be both a full solution andthe correct one.
The full solution ?T extends ?1but not ?2.Every possible full solution to a cipher C willproduce a plaintext string with some associatedlanguage model probability, and we will considerthe best possible solution to be the one that givesthe highest probability.
For the sake of concrete-ness, we will assume here that the language modelis a character-level trigram model.
This plain-text can be found by treating all of the length clenstrings S as being the output of different charac-ter mappings from C. A string S that results fromsuch a mapping is consistent with a partial solu-tion ?
iff, for every pi : ci ?
?, the character posi-tions of C that map to pi are exactly the characterpositions with ci in C.In our above example, we had C =?ifmmp xpsme?, in which case we hadclen = 11.
So mappings from C to?hhhhh hhhhh?
or ?
hhhhhhhhhh?
wouldbe consistent with a partial solution of size 0,while ?hhhhh hhhhn?
would be consistent withthe size 2 partial solution ?
= { : , n : e}.3 The AlgorithmIn order to efficiently search for the most likely so-lution for a ciphertext C, we conduct a search ofthe partial solutions using their trigram probabil-ities as a heuristic, where the trigram probabilityof a partial solution ?
of length k is the maximumtrigram probability over all strings consistent withit, meaning, in particular, that ciphertext letters notin its range can be mapped to any plaintext letter,and do not even need to be consistently mapped tothe same plaintext letter in every instance.
Givena partial solution ?
of length n, we can extend ?by choosing a ciphertext letter c not in the rangeof ?, and then use our generalization of the Viterbialgorithm to find, for each p not in the domain of?, a score to rank the choice of p for c, namely thetrigram probability of the extension ?p of ?.
If westart with an empty solution and iteratively choosethe most likely remaining partial solution in thisway, storing the extensions obtained in a priorityheap as we go, we will eventually reach a solutionof size nC .
Every extension of ?
has a probabil-ity that is, at best, equal to that of ?, and everypartial solution receives, at worst, a score equalto its best extension, because the score is poten-tially based on an inconsistent mapping that doesnot qualify as an extension.
These two observa-tions taken together mean that one minus the scoreassigned by our method constitutes a cost functionover which this score is an admissible heuristic inthe A* sense.
Thus the first solution of size nCwill be the best solution of size nC .The order by which we add the letters c to par-tial solutions is the order of the distinct cipher-text letters in right-to-left order of their final oc-currence in C. Other orderings for the c, such asmost frequent first, are also possible though lesselegant.1Algorithm 1 Search AlgorithmOrder the letters c1 .
.
.
cnC by rightmost occur-rence in C, rnC < .
.
.
< r1.Create a priority queue Q for partial solutions,ordered by highest probability.Push the empty solution ?0 = {} onto thequeue.while Q is not empty doPop the best partial solution ?
from Q.s = |?|.if s = nC thenreturn ?elseFor all p not in the range of ?, push theimmediate extension ?p onto Q with thescore assigned to table cell G(rs+1, p, p)by GVit(?, cs+1, rs+1) if it is non-zero.end ifend whileReturn ?Solution Infeasible?.Our generalization of the Viterbi algorithm, de-picted in Figure 1, uses dynamic programming toscore every immediate extension of a given partialsolution in tandem, by finding, in a manner con-sistent with the real Viterbi algorithm, the mostprobable input string given a set of output sym-bols, which in this case is the cipher C. Unlike thereal Viterbi algorithm, we must also observe theconstraints of the input partial solution?s mapping.1We have experimented with the most frequent first regi-men as well, and it performs worse than the one reported here.Our hypothesis is that this is due to the fact that the most fre-quent character tends to appear in many high-frequency tri-grams, and so our priority queue becomes very long becauseof a lack of low-probability trigrams to knock the scores ofpartial solutions below the scores of the extensions of theirbetter scoring but same-length peers.
A least frequent firstregimen has the opposite problem, in which their rare oc-currence in the ciphertext provides too few opportunities topotentially reduce the score of a candidate.1042A typical decipherment involves multiple runs ofthis algorithm, each of which scores all of the im-mediate extensions, both tightening and loweringtheir scores relative to the score of the input par-tial solution.
A call GVit(?, c, r) manages this byfilling in a table G such that for all 1 ?
i ?
r, andl, k ?
?P , G(i, l, k) is the maximum probabilityover every plaintext string S for which:?
len(S) = i,?
S[i] = l,?
for every p in the domain of ?, every 1 ?
j ?i, if C[j] = ?
(p) then S[j] = p, and?
for every position 1 ?
j ?
i, if C[j] = c,then S[j] = k.The real Viterbi algorithm lacks these final twoconstraints, and would only store a single cell atG(i, l).
There, G is called a trellis.
Ours is larger,so so we will refer to G as a greenhouse.The table is completed by filling in the columnsfrom i = 1 to clen in order.
In every column i,we will iterate over the values of l and over thevalues of k such that k : c and l : are consistentwith ?.
Because we are using a trigram charactermodel, the cells in the first and second columnsmust be primed with unigram and bigram proba-bilities.
The remaining probabilities are calculatedby searching through the cells from the previoustwo columns, using the entry at the earlier columnto indicate the probability of the best string up tothat point, and searching through the trigram prob-abilities over two additional letters.
Backpointersare necessary to reference one of the two languagemodel probabilities.
Cells that would produce in-consistencies are left at zero, and these as well ascells that the language model assigns zero to canonly produce zero entries in later columns.In order to decrease the search space, we add thefurther restriction that the solutions of every threecharacter sequence must be consistent: if the ci-phertext indicates that two adjacent letters are thesame, then only the plaintext strings that map thesame letter to each will be considered.
The num-ber of letters that are forced to be consistent isthree because consistency is enforced by remov-ing inconsistent strings from consideration duringtrigram model evaluation.Because every partial solution is only obtainedby extending a solution of size one less, and ex-tensions are only made in a predetermined orderof cipher alphabet letters, every partial solution isonly considered / extended once.GVit is highly parallelizable.
The nP ?nP cellsof every column i do not depend on each other ?only on the cells of the previous two columns i?1and i?2, as well as the language model.
In our im-plementation of the algorithm, we have written theunderlying program in C/C++, and we have usedthe CUDA library developed for NVIDIA graphicscards to in order to implement the parallel sectionsof the code.4 ExperimentThe above algorithm is designed for application tothe transliteration of electronic documents, specif-ically, the transliteration of websites, and it hasbeen tested with this in mind.
In order to gain re-alistic test data, we have operated on the assump-tion that Wikipedia is a good approximation of thetype of language that will be found in most inter-net articles.
We sampled a sequence of English-language articles from Wikipedia using their ran-dom page selector, and these were used to createa set of reference pages.
In order to minimize thecommon material used in each page, only the textenclosed by the paragraph tags of the main body ofthe pages were used.
A rough search over internetarticles has shown that a length of 1000 to 11000characters is a realistic length for many articles, al-though this can vary according to the genre of thepage.
Wikipedia, for example, does have entriesthat are one sentence in length.
We have run twogroups of tests for our algorithm.
In the first setof tests, we chose the mean of the above lengthsto be our sample size, and we created and decoded10 ciphers of this size (i.e., different texts, samesize).
We made these cipher texts by appendingthe contents of randomly chosen Wikipedia pagesuntil they contained at least 6000 characters, andthen using the first 6000 characters of the result-ing files as the plaintexts of the cipher.
The textlength was rounded up to the nearest word whereneeded.
In the second set of tests, we used a singlelong ciphertext, and measured the time requiredfor the algorithm to finish a number of prefixes ofit (i.e., same text, different sizes).
The plaintext forthis set of tests was developed in the same way asthe first set, and the input ciphertext lengths con-sidered were 1000, 3500, 6000, 8500, 11000, and13500 characters.1043Greenhouse Array(a) (b) (c) (d)...lmn...zl w ?
?
?
y t g ?
?
?
g u?
?
?
e f g ?
?
?
zFigure 1: Filling the Greenhouse Table.
Each cell in the greenhouse is indexed by a plaintext letter anda character from the cipher.
Each cell consists of a smaller array.
The cells in the array give the bestprobabilities of any path passing through the greenhouse cell, given that the index character of the arraymaps to the character in column c, where c is the next ciphertext character to be fixed in the solution.
Theprobability is set to zero if no path can pass through the cell.
This is the case, for example, in (b) and (c),where the knowledge that ?
?
maps to ?
?
would tell us that the cells indicated in gray are unreachable.The cell at (d) is filled using the trigram probabilities and the probability of the path at starting at (a).In all of the data considered, the frequency ofspaces was far higher than that of any other char-acter, and so in any real application the charactercorresponding to the space can likely be guessedwithout difficulty.
The ciphers we have consid-ered have therefore been simplified by allowingthe knowledge of which character corresponds tothe space.
It appears that Ravi and Knight (2008)did this as well.
Our algorithm will still work with-out this assumption, but would take longer.
In theevent that a trigram or bigram would be found inthe plaintext that was not counted in the languagemodel, add one smoothing was used.Our character-level language model used wasdeveloped from the first 1.5 million characters ofthe Wall Street Journal section of the Penn Tree-bank corpus.
The characters used in the lan-guage model were the upper and lower case let-ters, spaces, and full stops; other characters wereskipped when counting the frequencies.
Further-more, the number of sequential spaces allowedwas limited to one in order to maximize contextand to eliminate any long stretches of white space.As discussed in the previous paragraph, the spacecharacter is assumed to be known.When testing our algorithm, we judged the timecomplexity of our algorithm by measuring the ac-tual time taken by the algorithm to complete itsruns, as well as the number of partial solutionsplaced onto the queue (?enqueued?
), the numberpopped off the queue (?expanded?
), and the num-ber of zero-probability partial solutions not en-queued (?zeros?)
during these runs.
These latternumbers give us insight into the quality of trigramprobabilities as a heuristic for the A* search.We judged the quality of the decoding by mea-suring the percentage of characters in the cipheralphabet that were correctly guessed, and also theword error rate of the plaintext generated by oursolution.
The second metric is useful because alow probability character in the ciphertext may beguessed wrong without changing as much of theactual plaintext.
Counting the actual number ofword errors is meant as an estimate of how usefulor readable the plaintext will be.
We did not countthe accuracy or word error rate for unfinished ci-phers.We would have liked to compare our resultswith those of Ravi and Knight (2008), but themethod presented there was simply not feasible1044Algorithm 2 Generalized Viterbi AlgorithmGVit(?, c, r)Input: partial solution ?, ciphertext character c,and index r into C.Output: greenhouse G.Initialize G to 0.i = 1for All (l, k) such that ?
?
{k : c, l : Ci} isconsistent doG(i, l, k) = P (l).end fori = 2for All (l, k) such that ?
?
{k : c, l : Ci} isconsistent dofor j such that ?
?
{k : c, l : Ci, j : Ci?1} isconsistent doG(i, l, k) = max(G(i, l, k), G(0, j, k)?P (l|j))end forend fori = 3for (l, k) such that ?
?
{k : c, l : Ci} is consis-tent dofor j1, j2 such that ??
{k : c, j2 : C[i?2], j1 :C[i?
1], l : Ci} is consistent doG(i, l, k) = max(G(i, l, k), G(i?2, j2, k)?
P (j1|j2)?
P (l|j2j1)).end forend forfor i = 4 to r dofor (l, k) such that ?
?
{k : c, l : Ci} is con-sistent dofor j1, j2 such that ?
?
{k : c, j2 :C[i?2], j1 : C[i?1], l : Ci} is consistentdoG(i, l, k) = max(G(i, l, k),G(i?2, j2, k)?P (j1|j2j2(back))?
P (l|j2j1)).end forend forend foron texts and (case-sensitive) alphabets of this sizewith the computing hardware at our disposal.5 ResultsIn our first set of tests, we measured the time con-sumption and accuracy of our algorithm over 10ciphers taken from random texts that were 6000characters long.
The time values in these tables aregiven in the format of (H)H:MM:SS.
For this setof tests, in the event that a test took more than 12hours, we terminated it and listed it as unfinished.This cutoff was set in advance of the runs basedupon our armchair speculation about how long onemight at most be reasonably expected to wait fora web-page to be transliterated (an overnight run).The results from this run appear in Table 1.
Allrunning times reported in this section were ob-tained on a computer running Ubuntu Linux 8.04with 4 GB of RAM and 8 ?
2.5 GHz CPU cores.Column-level subcomputations in the greenhousewere dispatched to an NVIDIA Quadro FX 1700GPU card that is attached through a 16-lane PCIExpress adapter.
The card has 512 MB of cachememory, a 460 MHz core processor and 32 shaderprocessors operating in parallel at 920 MHz each.In our second set of tests, we measured the timeconsumption and accuracy of our algorithm overseveral prefixes of different lengths of a single13500-character ciphertext.
The results of this runare given in Table 2.The first thing to note in this data is that the ac-curacy of this algorithm is above 90 % for all ofthe test data, and 100% on all but the smallest 2ciphers.
We can also observe that even when thereare errors (e.g., in the size 1000 cipher), the worderror rate is very small.
This is a Zipf?s Law effect?
misclassified characters come from poorly at-tested character trigrams, which are in turn foundonly in longer, rarer words.
The overall high ac-curacy is probably due to the large size of thetexts relative to the uniticity distance of an En-glish letter-substitution cipher (Bauer, 2007).
Theresults do show, however, that character trigramprobabilities are an effective indicator of the mostlikely solution, even when the language model andtest data are from very different genres (here, theWall Street Journal and Wikipedia, respectively).These results also show that our algorithm is ef-fective as a way of decoding simple ciphers.
80%of our runs finished before the 12 hour cutoff inthe first experiment.1045Cipher Time Enqueued Expanded Zeros Accuracy Word Error Rate1 2:03:06 964 964 44157 100% 0%2 0:13:00 132 132 5197 100% 0%3 0:05:42 91 91 3080 100% 0%4 Unfinished N/A N/A N/A N/A N/A5 Unfinished N/A N/A N/A N/A N/A6 5:33:50 2521 2521 114283 100% 0%7 6:02:41 2626 2626 116392 100% 0%8 3:19:17 1483 1483 66070 100% 0%9 9:22:54 4814 4814 215086 100% 0%10 1:23:21 950 950 42107 100% 0%Table 1: Time consumption and accuracy on a sample of 10 6000-character texts.Size Time Enqueued Expanded Zeros Accuracy Word Error Rate1000 40:06:05 119759 119755 5172631 92.59% 1.89%3500 0:38:02 615 614 26865 96.30% 0.17%6000 0:12:34 147 147 5709 100% 0%8500 8:52:25 1302 1302 60978 100% 0%11000 1:03:58 210 210 8868 100% 0%13500 0:54:30 219 219 9277 100% 0%Table 2: Time consumption and accuracy on prefixes of a single 13500-character ciphertext.As far as the running time of the algorithm goes,we see a substantial variance: from a few minutesto several hours for most of the longer ciphers, andthat there are some that take longer than the thresh-old we gave in the experiment.
Specifically, thereis substantial variability in the the running timesseen.Desiring to reduce the variance of the runningtime, we look at the second set of tests for possiblecauses.
In the second test set, there is a generaldecrease in both the running time and the numberof solutions expanded as the length of the ciphersincreases.
Running time correlates very well withA* queue size.Asymptotically, the time required for eachsweep of the Viterbi algorithm increases, but thisis more than offset by the decrease in the numberof required sweeps.The results, however, do not show that runningtime monotonically decreases with length.
In par-ticular, the length 8500 cipher generates more so-lutions than the length 3500 or 6000 ones.
Recallthat the ciphers in this section are all prefixes ofthe same string.
Because the algorithm fixes char-acters starting from the end of the cipher, theseprefixes have very different character orderings,c1, .
.
.
, cnC , and thus a very different order of par-tial solutions.
The running time of our algorithmdepends very crucially on these initial conditions.Perhaps most interestingly, we note that thenumber of enqueued partial solutions is in ev-ery case identical or nearly identical to the num-ber of partial solutions expanded.
From a the-oretical perspective, we must also remember thezero-probability solutions, which should in a sensecount when judging the effectiveness of our A*heuristic.
Naturally, these are ignored by our im-plementation because they are so badly scoredthat they could never be considered.
Neverthe-less, what these numbers show is that scores basedon character-level trigrams, while theoretically ad-missible, are really not all that clever when itcomes to navigating through the search space ofall possible letter substitution ciphers, apart fromtheir very keen ability at assigning zeros to alarge number of partial solutions.
A more com-plex heuristic that can additionally rank non-zeroprobability solutions with more prescience wouldlikely make a very great difference to the runningtime of this method.10466 ConclusionsIn the above paper, we have presented an algo-rithm for solving letter-substitution ciphers, withan eye towards discovering unknown encodingstandards in electronic documents on the fly.
Ina test of our algorithm over ciphers drawn fromWikipedia, we found its accuracy to be 100% onthe ciphers that it solved within a threshold of 12hours, this being 80% of the total attempted.
Wefound that the running time of our algorithm ishighly variable depending on the order of char-acters attempted, and, due to the linear-time the-oretical complexity of this method, that runningtimes tend to decrease with larger ciphertexts dueto our character-level language model?s facility ateliminating highly improbable solutions.
There is,however, a great deal of room for improvement inthe trigram model?s ability to rank partial solutionsthat are not eliminated outright.Perhaps the most valuable insight gleaned fromthis study has been on the role of the languagemodel.
This algorithm?s asymptotic runtime com-plexity is actually a function of entropic aspects ofthe character-level language model that it uses ?more uniform models provide less prominent sep-arations between candidate partial solutions, andthis leads to badly ordered queues, in which ex-tended partial solutions can never compete withpartial solutions that have smaller domains, lead-ing to a blind search.
We believe that there is agreat deal of promise in characterizing natural lan-guage processing algorithms in this way, due to theprevalence of Bayesian methods that use languagemodels as priors.Our approach makes no explicit attempt to ac-count for noisy ciphers, in which characters areerroneously mapped, nor any attempt to accountfor more general substitution ciphers in which asingle plaintext (resp.
ciphertext) letter can map tomultiple ciphertext (resp.
plaintext) letters, nor forciphers in which ciphertext units corresponds tolarger units of plaintext such syllables or words.Extensions in these directions are all very worth-while to explore.ReferencesFriedrich L. Bauer.
2007.
Decrypted Secrets.Springer-Verlag, Berlin Heidelberg.George W. Hart.
1994.
To Decode Short Cryptograms.Communications of the ACM, 37(9): 102?108.Kevin Knight.
1999.
Decoding Complexity in Word-Replacement Translation Models.
ComputationalLinguistics, 25(4):607?615.Kevin Knight, Anish Nair, Nishit Rathod, Kenji Ya-mada.
Unsupervised Analysis for DeciphermentProblems.
Proceedings of the COLING/ACL 2006,2006, 499?506.George Nagy, Sharad Seth, Kent Einspahr.
1987.Decoding Substitution Ciphers by Means of WordMatching with Application to OCR.
IEEE Transac-tions on Pattern Analysis and Machine Intelligence,9(5):710?715.Shmuel Peleg and Azriel Rosenfeld.
1979.
BreakingSubstitution Ciphers Using a Relaxation Algorithm.Communications of the ACM, 22(11):589?605.Sujith Ravi, Kevin Knight.
2008.
Attacking Decipher-ment Problems Optimally with Low-Order N-gramModels Proceedings of the ACL 2008, 812?819.Antal van den Bosch, Sander Canisius.
2006.
Im-proved Morpho-phonological Sequence Processingwith Constraint Satisfaction Inference Proceedingsof the Eighth Meeting of the ACL Special InterestGroup on Computational Phonology at HLT-NAACL2006, 41?49.1047
