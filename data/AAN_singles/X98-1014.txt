ALGORITHMS THAT LEARN TO EXTRACT INFORMATION mBBN:  T IPSTER PHASE IIIScott Miller, Michael Crystal, Heidi Fox, Lance Ramshaw, Richard Schwartz,Rebecca Stone, and Ralph WeischedelBBN Technologies70 Fawcett StreetCambridge, MA 02138weischedel @bbn.comABSTRACTAll of BBN's research under the TIPSTER IIIprogram has focused on doing extraction byapplying statistical models trained on annotateddata, rather than by using programs that executehand-written rules.
Within the context of MUC-7, the SIFT system for extraction of templateentities (TE) and template relations (TR) used anovel, integrated syntactic/semantic languagemodel to extract sentence l vel information, andthen synthesized information across sentencesusing in part a trained model for cross-sentencerelations.
At the named entity (NE) level as well,in both MET-1 and MUC-7, BBN employed atrained, HMM-based model.The results in these TIPSTER evaluations areevidence that such trained systems, even at theircurrent level of development, can performroughly on a par with those based on rules hand-tailored by experts.
In addition, such trainedsystems have some significant advantages:?
They can be easily ported to new domainsby simply annotating fresh data.?
The complex interactions that make rule-based systems difficult to develop andmaintain can here be learned automaticallyfrom the training data.We believe that improved and extended versionsof such trained models have the potential forsignificant further progress toward practicalsystems for information extraction.INTRODUCTIONWe believe that trained statistical models offersignificant advantages for information extractiontasks.
In this report on BBN's research under theTIPSTER III program, we describe a number ofresearch efforts that developed fully-trainedsystems whose extraction performance was closeto the highest levels achieved by carefullyoptimized systems based on hand-written rules.SIFT, the first system described, extracts entitiesand relations from text.
On the sentence l vel, itcombines yntactic and semantic knowledge in anovel way, thus taking advantage of thesignificant recent progress in statistical parsingand leveraging those techniques for informationextraction.
Knowledge of English syntaxextracted from the Penn Treebank isautomatically combined with semanticallyannotated training material in the target domainthat identifies how the entities and relations ofinterest in the domain are signaled in text.
At themessage level, the local entities and relationsidentified within each sentence are then merged,and cross-sentence relations are identified usingan additional trained model.
The resultingsystem achieved the second-best core of thoseparticipating in the MUC-7 evaluation.The second system described here is theIdentiFinder TM system for locating namedentities.
This system is a fully-trained, HMM-based model that learns from examples thecontextual clues that help to identify names inthe text.STAT IST ICAL  EXTRACTION OFENT IT IES  AND RELAT IONSThe SIFT system ("Statistically-derivedInformation From Text") combines a sentence-level model with message-level processing tomerge elements and identify cross-sentencerelations.At the sentence level, SIFT employs a unifiedstatistical process to map from words to semanticstructures.
That is, part-of-speech determination,name-finding, parsing, and relationship-finding75all happen as part of the same process.
Thisallows each element of the model to influencethe others, and avoids the assembly-line trap ofhaving to commit to a particular part-of-speechchoice, say, early on in the process, when onlylocal information is available to inform thechoice.The SIFT sentence-level model was trained fromtwo sources:?
General knowledge of English sentencestructure was learned from the PennTreebank corpus of one million words ofWall Street Journal text.?
Specific knowledge about how the targetentities and relations are expressed inEnglish was learned from about 500 Kwords of on-domain text annotated withnamed entities, descriptors, and semanticrelations.In the on-domain training data, the names anddescriptors of relevant items (persons,organizations, locations, and artifacts) aremarked, as well as the target relationshipsbetween them that are signaled syntactically.
Forexample, in the phrase "GTE Corp. ofStamford", the annotation would record a"location-of" connection between the companyand the city.
The model can thus learn thestructures that are typically used in English toconvey the target relationships.
Doing extractionin a new domain would require freshsemantically annotated training data appropriateto the new domain, but the general syntacticknowledge acquired from the Penn Treebankwould still be applicable.After the sentence-level model has identifiednames, descriptors, and relationships that aresyntactially signaled within each sentence,further message-level processing is required tolink up entities mentioned more than once or indifferent sentences, and to try to identify cross-sentence relationships or those not syntacticallysignaled.
After the names, descriptors, and localrelationships have been extracted from thesentence-level decoder's output, a mergingprocess is applied to link multiple occurrences ofthe same name or of alternative forms of thename from different sentences.
A second, cross-sentence model is then invoked to try to identifyrelationships that were not picked up by thedecoder, such as when the two entities do notoccur in the same sentence.
Finally, someadditional fields required by the MUC answerspecification are filled in using heuristic tests anda gazetteer database, and output filters areapplied to select which of the proposed internalstructures hould be included in the output.
Weare actively exploring ways of integrating thismessage-level processing more closely with thesentence-level model, since an integratedstatistical model is the only way in which tomake every choice in a nuanced way, based onall the available information.The following sections describe the sentence-level and message-level processing of the SIFTsystem in more detail.SIFT's Sentence-Level ModelFigure 1 is a block diagram of the sentence-levelmodel showing the main components and datapaths.
Two types of annotations are used to trainthe model: syntactic annotations for learningabout the general structure of English, andsemantic annotations for learning about thetarget entities and relations.
From theseannotations, the training program estimates theparameters of a unified statistical model thataccounts for both syntax and semantics.
Later,when presented with a new sentence, the searchprogram explores the statistical model to find themost likely combined semantic and syntacticinterpretation.Training DataOur source for syntactically annotated trainingdata was the Penn Treebank (Marcus et al,1993).
Significantly, we do not require thatsyntactic annotations be from the same source, orcover the same domain, as the target ask.
Forexample, while the Penn Treebank consists ofWall Street Journal text, the target source for thisevaluation was New York Times newswire.Similarly, although the Penn Treebank domaincovers general and financial news, the targetdomain for the MUC-7 evaluation was spacetechnology.
The ability to use syntactic trainingfrom a different source and domain than thetarget is an important feature of our model.Since the Penn Treebank serves as oursyntactically annotated training corpus, we needonly create a semantically annotated corpus.Stated generally, semantic annotations erve todenote the entities and relations of interest in the76syntactic annotations ,\[(Penn Treebank) ~ trainingprogramsemantic annotations "1. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~... s.t.a.d s.t.i.c.a.1...~ .
.
.
.
.
.
.
.
.
.
.
.
.
tr.m.
'.n!.n.g .....L model j decodingsentences , search ~ combined semantic-"\[ program syntactic interpretationsFigure 1: Block diagram of sentence-level model.target domain.
More specifically, entities aremarked as either names or descriptors, with co-reference between entities marked as well.Figure 2 shows a semantically annotatedfragment of a typical sentence.From only these simple semantic annotations,the system can be trained to work in a newdomain.
To train SIFT for MUC-7, weannotated approximately 500,000 words of NewYork Times newswire text, covering the domainsof air disasters and space technology.
(We havenot yet run experiments to see how performancevaries with more/less training data.
)Semantic/Syntactic S ructureWhile our semantic annotations are quite simple,the internal model of sentence structure issubstantially more complicated, since thiscombined model must account for syntacticstructure as well as for entities and semanticrelations.
Our underlying training algorithmrequires examples of these internal structures inorder to estimate the parameters of the unifiedsemantic/syntactic model.
However, we do notwish to incur the high cost of annotating parsetrees.
Instead, we use the following multi-steptraining procedure, exploiting the PennTreebank:1) Train the sentence-level model on the purelysyntactic parse trees in the Treebank.
Oncethis step is complete, the model will functionas a state-of-the-art statistical parser.2) For each sentence in the semanticallyannotated corpus:a) Apply the sentence level model tosyntactically parse the sentence,constraining the model to produce onlyparses that are consistent with thesemantic annotation.b) Augment the resulting parse tree toreflect semantic structure as well assyntactic structure.3) Retrain the sentence-level model on theaugmented parse trees produced in step 2.Once this step is complete, we have anintegrated model of semantics and syntax.Details of the statistical model will be discussedJNance , whocoreference ~ employee~ ~ relation ~person-descriptori-organization 7is also a paid consultant to ABC News , saidFigure 2: An example of semantic annotation.77later.
For now, we turn our attention to (a)constraining the decoder and (b) augmenting theparse trees with semantic structure.Constraints are simply bracketing boundariesthat may not be crossed by any parse constituent.There are two types of constraints: hardconstraints that cannot be violated under anyconditions, and soft constraints, that may beviolated only if enforcing them would result inno plausible parse.
All named entities anddescriptors are treated as hard constraints; themodel is prohibited from producing anyconstituents hat overlap either edge of the spanof these elements.
In addition, we attempt okeep possible appositives together through softconstraints.
Whenever there is a co-referentialrelation between two entities that are eitheradjacent or separated by only a comma, we positan appositive and introduce a soft constraint toencourage the parser to keep the elementstogether.Once a constrained parse is found, it must beaugmented to reflect the semantic structure.Augmentation is a five step process.1) Nodes are inserted into the parse tree todistinguish names and descriptors that arenot bracketed in the parse.
For example, theparser produces a single noun phrase withno internal structure for "Lt. Cmdr.
DavidEdwin Lewis".
Additional nodes must beinserted to distinguish the descriptor, "Lt.Cmdr.," and the name, "David EdwinLewis.
"2) Semantic labels are attached to all nodes thatcorrespond to names or descriptors.
Theselabels reflect he entity type, such as person,organization, or location, as well as whetherthe node is a proper name or a descriptor.3) For relations between entities, where oneentity is not a syntactic modifier of theother, the lowermost parse node that spansboth entities is identified.
A semantic tag isthen added to that node denoting therelationship.
For example, in the sentence"Mary Fackler Schiavo is the inspectorgeneral of the U.S. Department ofTransportation," a co-reference semanticlabel is added to the S node spanning thename, "Mary Fackler Schiavo," and thedescriptor, "the inspector general of the U.S.Department of Transportation.
"4) Nodes are inserted into the parse tree todistinguish the arguments to each relation.In cases where there is a relation betweentwo entities, and one of the entities is asyntactic modifier of the other, the insertednode serves to indicate the relation as wellas the argument.
For example, in the phrase"Lt. Cmdr.
David Edwin Lewis," a node isinserted to indicate that "Lt.
Cmdr."
is adescriptor for "David Edwin Lewis.
"5) Whenever a relation involves an entity thatis not a direct descendant of that relation inthe parse tree, semantic pointer labels areattached to all of the intermediate nodes.These labels serve to form a continuouschain between the relation and its argument.Figure 3 shows an augmented parse treecorresponding to the semantic annotation inFigure 2.
Note that nodes with semantic labelsending in "-r" mark MUC reportable names anddescriptors.Statistical ModelIn SIFT's statistical model, augmented parsetrees are generated according to a process imilarto that described in Collins (1996, 1997).
Foreach constituent, the head is generated first,followed by the modifiers, which are generatedfrom the head outward.
Head words, along withtheir part-of-speech tags and features, aregenerated for each modifier as soon as themodifier is created.
Word features areintroduced primarily to help with unknownwords, as in Weischedel et al (1993).We illustrate the generation process by walkingthrough a few of the steps of the parse shown inFigure 3.
At each step in the process, a choice ismade from a statistical distribution, with theprobability of each possible selection dependenton particular features of previously-generatedelements.
We pick up the derivation just after thetopmost S and its head word, said, have beenproduced.
The next steps are to generate inorder:1.
A head constituent for the S, in this case aVP.2.
Pre-modifier constituents for the S. In thiscase, there is only one: a PER/NP.3.
A head part-of-speech tag for the PER/NP,in this case PER/NNP.78pednpper-desc-of/sbar-lnkIper-desc-ptr/sbarper-r/npIper/nnpI INance ,vp/ per~_desc.ptr/v p~- - - - - - - - -~gsc - r /np  ///wp vbz rb det vbn per-desc/nn to org'/nnporg/nnpI I I I I I I I Iwho is also a paid consultant to ABC NewsFigure 3: An augmented parse tree., vbdI I, said4.
A head word for the PER/NP, in this casenance.5.
Word features for the head word of thePER/NP, in this case capitalized.6.
A head constituent for the PER/NP, in thiscase a PER-R/NP.7.
Pre-modifier constituents for the PER/NP.In this case, there are none.8.
Post-modifier constituents for the PER/NP.First a comma, then an SBAR structure, andthen a second comma are each generated inturn.This generation process is continued until theentire tree has been produced.We now briefly summarize the probabilitystructure of the model.
The categories for headconstituents, Ch, are predicted based solely on thecategory of the parent node, Cp:e(c h I Cp ), e.g.
P(vp I s)Modifier constituent categories, Cm, arepredicted based on their parent node, cp, the headconstituent of their parent node, Chp, thepreviously generated modifier, Cm-1, and the headword of their parent, Wp.
Separate probabilitiesare maintained for left (pre) and right (post)modifiers:PL(Cm I Cp,Chp,Cm_l,Wp), e.g.PL (per /np I s, vp, null, said)PR( Cm I Cp,Chp,Cm_l,Wp), e.g.PR(null I s, vp, null, said)Part-of-speech tags, tin, for modifiers arepredicted based on the modifier, Cm, the part-of-speech tag of the head word , th, and the headword itself, wh:P(t m I c m, t h , w h ), e.g.P(per / nnp I per /np, vbd, said)Head words, win, for modifiers are predictedbased on the modifier, Cm, the part-of-speech tag79of the modifier word, tin, the part-of-speech tagof the head word , th, and the head word itself,Wh:e(w m \] Cm, tm,th, Wh ), e.g.P(nance I per I np, per I nnp, vbd, said)Finally, word features, fro, for modifiers arepredicted based on the modifier, Cm, the part-of-speech tag of the modifier word, tm, the part-of-speech tag of the head word, th, the head worditself, wh, and whether or not the modifier headword, Win, is known or unknown.P( \]m ICm, tra,th, Wh, known(w m )), e.g.P(cap I per / np, per / nnp, vbd, said, true)The probability of a complete tree is the productof the probabilities of generating each element inthe tree.
If we generalize the tree components(constituent labels, words, tags, etc.)
and treatthem all as simply elements, e, and treat all theconditioning factors as the history, h, we canwrite:P(tree) = H P(e I h)?
~ treeTraining the ModelMaximum likelihood estimates for all modelprobabilities are obtained by observingfrequencies in the training corpus.
However,because these estimates are too sparse to berelied upon, they must be smoothed by mixing inlower-dimensional estimates.
We determine themixture weights using the Witten-Bellsmoothing method.For modifier constituents, the mixturecomponents are:P ' (c  m ICp,Chp,Cm_l,Wp)=21 P (c  m I Cp,Chp,Cm_l,Wp)-I-~, 2 P (c  m ICp,Chp,Cm-l)For part-of-speech tags, the mixture componentsare:P'(t m I Cm, t h, w h) = 21 P(t m I cm, w h )+2 2 P(t m \]Cm,th)+2 3 P(t m I c m)For head words, the mixture components are:P'(W m I Cm,tm,th,Wh) = 21 P(W m I cm,tm,W h)+2 2 P(W m ICm,tm,t h)+2 3 P(w m I Cm,t m)-1"2 4 e (w m It m)Finally, for word features, the mixturecomponents are:P' (  fm I c m , t m , t h , w h , known (w m )) =21 P( fm ICm,tm,Wh,known(w,))+22 P( fm ICm,tm'th'known(wm))+23 P( fm Icm,tm,kn?wn(Wm))+24 P( fm I tm,kn?wn(Wm))Searching the ModelGiven a sentence to be analyzed, the searchprogram must find the most likely semantic andsyntactic interpretation.
More concretely, it mustfind the most likely augmented parse tree.Although mathematically the model predicts treeelements in a top-down fashion, we search thespace bottom-up using a chart based search.
Thesearch is kept tractable through a combination ofCKY-style dynamic programming and pruningof low probability elements.Dynamic Programming: Whenever two or moreconstituents are equivalent relative to all possiblelater parsing decisions, we apply dynamicprogramming, keeping only the most likelyconstituent in the chart.
Two constituents areconsidered equivalent if:1.
They have identical category labels.2.
Their head constituents have identical labels.3.
They have the same head word.4.
Their leftmost modifiers have identicallabels.5.
Their rightmost modifiers have identicallabels.Pruning: Given multiple constituents hat coveridentical spans in the chart, only thoseconstituents with probabilities within a thresholdof the highest scoring constituent are maintained;all others are pruned.
For purposes of pruning,and only for purposes of pruning, the priorprobability of each constituent category ismultiplied by the generative probability of that80constituent (Goodman, 1997).
We can think ofthis prior probability as an estimate of theprobability of generating a subtree with theconstituent category, starting at the topmostnode.
Thus, the scores used in pruning can beconsidered as the product of:1.
The probability of generating a constituentof the specified category, starting at thetopmost node.2.
The probability of generating the structurebeneath that constituent, having alreadygenerated a constituent of that category.The outcome of the search process is a treestructure that encodes both the syntactic andsemantic structure of the sentence, so that the TEentities and local TR relations can be directlyextracted from these sentential trees.SIFT's Message-Level ProcessingThe sentence-level model in SIFT predictsnames, descriptors, and relationships that arecued by the local sentence structure, but itconsiders each sentence in isolation.
Mergingsuch information between sentences is animportant and difficult problem in informationextraction.
The information that indicates thepresence of a template relation is oftendistributed across multiple sentences, and thismerging problem would naturally become evenmore severe when trying to extract morecomplex structures like full scenario templates.We have explored various approaches to thismerging problem in our TIPSTER research.Our overall goal is to use trained and integratedmodels where possible, particularly for all of thelanguage understanding.
For some portions ofSIFT's message-level processing, we used hand-written rules combined with external sources likegazetteers.
The MUC-7 deadlines caused us touse an existing alias process for merging namesrather than implementing a statistical aliasprocedure.
In the current system, simple heuristiccode handles the filling of the type and countryfields that are required by the MUCspecification, and the distinction betweensubstantial and non-substantial descriptors.
(TheMUC guidelines call for ignoring certaindescriptors like "the company".
)A trained cross-sentence r lation model is usedto identify template relations that link entitiesacross different sentences.
This model wastrained on 200 articles annotated with full MUCanswer keys, so that even non-local relationswere marked.
(That level of semantic annotationwas available for only a small subset of the dataused to train the sentence-level model.)
Themodel applies a set of structural and contextualfeatures that help to indicate when such arelation might be present.
Feature counts fromthe training data are used to estimate theprobability of a relationship between eachpossible pair of entities mentioned in separatesentences in the text.While the cross-sentence model is currentlyapplied as a separate step after the sentence-leveldecoding is complete, we are exploring variousapproaches toward integrating the two modelsmore closely, and also toward doing more of thenamed entity merging and type field predictionby means of trained models.Merging Named EntitiesThe first step in merging the results of thesentence-level model is to group together thedifferent mentions of the same named entity.
InSIFT, a set of heuristic rules were used for this.Different mentions of the same name (say,different mentions of "IBM") would be grouped,as would strings that were related in certainpredictable ways, for example, by initials(linking "IBM" with "International BusinessMachines") or by the addition of a corporatedesignator (linking "International BusinessMachines" with "International BusinessMachines, Inc.").
This merging process alsotested whether one name was a prefix of theother, linking "Legg Mason Wood Walker, Inc."with "Legg Mason".The Cross-Sentence R lation ModelThe cross-sentence model then uses structuraland contextual clues to hypothesize templaterelations between two elements that are notmentioned within the same sentence.
Since 80-90% of the relations found in the answer keysconnect wo elements that are mentioned in thesame sentence, the cross sentence model has anarrow target o shoot for.
Very few of the pairsof entities seen in different sentences turn out tobe actually related.
This model uses featuresextracted from related pairs in training data to tryto identify those cases.81It is a classifier model that considers all pairs ofentities in a message whose types are compatiblewith a given relation; for example, a person andan organization would suggest a possibleemployment relation.
For the three MUC-7relations, it turned out to be somewhatadvantageous to build in a functional constraint,so that the model would not consider, forexample, a possible employment relation for aperson already known from the sentence-levelmodel to be employed elsewhere.Given the measured features for a possiblerelation, the probability of a relation holding ornot holding can be computed as follows:p( rel I feats) = p( feats l rel) p( rel)p(feats)p(feats l ~rel) p(~rel) p(~rel l feats) =p(feats)If the ratio of those two probabilities, computedas follows, is greater than 1, the model predicts arelation:p(rell feats) p(featsl rel)p(rel)p(-rel l  feats) p(featsl ~rel)p(-rel)We approximate this ratio by assuming featureindependence and taking the product of thecontributions for each feature.p(rel I feats) p(rel)FIi P(feati I rel)p(~rel I feats) p(~rel )H p(feat, I ~rel)IThe cross-sentence f ature model applies toentities found by the sentence-level model,which is run over all of the sentence-likeportions of the text.
An initial heuristicprocedure checks for sections of the preamble ortrailer that look like sentential material, thatshould be treated like the body text.
There is alsoa separate handwritten procedure that searchesthe preamble text for any byline, and, if one isfound, instantiates an appropriate employeerelationship.Model FeaturesTwo classes of features were used in this model:structural features that reflect properties of thetext surrounding references to the entitiesinvolved in the suggested relation, and contentfeatures based on the actual entities and relationsencountered in the training data.Structural FeaturesThe structural features exploit simplecharacteristics of the text surrounding referencesto the possibly-related entities.
The mostpowerful structural feature, not surprisingly, wasdistance, reflecting the fact that related elementstend to be mentioned in close proximity, evenwhen they are not mentioned in the samesentence.
Given a pair of entity references in thetext, the distance between them was quantizedinto one of three possible values:Code Distance Value0 Within the same sentence1 Neighboring sentences2 More remote than neighboringsentencesFor each pair of possibly-related elements, thedistance feature value was defined as theminimum distance between some reference inthe text to the first element and some reference tothe second.A second structural feature grew out of theintuition that entities mentioned in the firstsentence of an article often play a special topicalrole throughout the article.
The "Topic Sentence"feature was defined to be true if some referenceto one of the two entities involved in thesuggested relation occurred in the first sentenceof the text-field body of the article.Other structural features that were considered butnot implemented included the count of thenumber of references to each entity.Content FeaturesWhile the structural features learn general factsabout the patterns in which related referencesoccur and the text that surrounds them, thecontent features learn about he actual names anddescriptors of entities seen to be related in thetraining data.
The three content features incurrent use test for a similar relationship intraining by name or by descriptor or for aconflicting relationship in training by name.The simplest content feature tests using nameswhether the entities in the proposed relationshiphave ever been seen before to be related.
To test82this feature, the model maintains adatabase of allthe entities seen to be related in training, and ofthe names used to refer to them.
The "by name"content feature is true if, for example, aperson insome training message who shared at least onename string with the person in the proposedrelationship was employed in that trainingmessage by an organization that shared at leastone name string with the organization in theproposed relationship,A somewhat weaker feature makes the same kindof test for a previously seen relationship usingdescriptor strings.
This feature fires when anentity that shares a descriptor string with the firstargument of the suggested relation was related intraining to an entity that shares a name with thesecond argument.
Since titles like "General"count as descriptor strings, one effect of thisfeature is to increase the likelihood of generalsbeing employed by armies.
Observing suchexamples, but noting that the training didn'tinclude all the reasonable combinations of titlesand organizations, the training for this featurewas seeded by adding a virtual messageconstructed from a list of such titles andorganizations, o that any reasonable such pairwould turn up in training.The third content feature was a kind of inverse ofthe first "by name" feature which was true ifsome entity sharing a name with the firstargument of the proposed relation was related toan entity that did not  share a name with thesecond argument.
Using the employment relationagain as an example, it is less likely (though stillpossible) that a person who was known inanother message to be employed by a differentorganization should be reported here asemployed by the suggested one.TrainingGiven enough fully annotated ata, with bothsentence-level semantic annotation and message-level answer keys recorded along with theconnections between them, training the featureswould be quite straightforward.
For eachpossibly-related pair of entities mentioned in adocument, one would just count up the 2x2 tableshowing how many of them exhibited the givenstructural feature and how many of them wereactually related.
The training issues that did arisestemmed from the limited supply of answer keysand that the keys were not connected to thesentence-level annotations.The government raining and dry run dataprovided 200 messages' worth of TE and TRanswer keys, Those answer keys, however,contained strings without recording where in thetext they were found.
In order to train structuralfeatures from that data, we needed the locationsof references within the text.
A heuristic stringmatching process was used to make thatconnection, with a special check to ensure fornames that the shorter version of a name did notmatch a string in the text that also matched alonger version of the same name.Training the content features, on the other hand,did not require positional information about thereferences.
The plain answer keys could be usedin combination with a database of the name anddescriptor strings for entities related in trainingto count up the feature probabilities for actuallyrelated and non-related pairs.
The string databasewas collected first, and one-out raining was thenused, so that the rest of the training corpusprovided the string database for training thefeature counts on each particular message.
Theadditional training data that was semanticallyannotated for training the sentence-level modelbut for which answer keys were not availablecould still also be used in building up the stringdatabase for the content features.The probabilities based on the final featurecounts were smoothed by mixing them with0.01% of a uniform model.Other Message Level ProcessingAfter the cross sentence model has been applied,some further heuristic message-level processingis done before generating the answers in MUCtemplate form.
In one step, those portions of thepreamble of the message, which includes the titleand by-line, that are not English sentences aresearched for a possible employment relationbetween the article author and the organizationholding the copyright.
A limited form of votingwas also applied across messages, o that if thesame name was identified by the sentence-levelmodel as, say, an organization i  one case and aperson in another, only the plurality type isactually output.
Heuristic models are used to fillin some additional required fields,distinguishing, for instance, between civilian,military, and government organizations; thiscould have been trained, but time did not permitthis.
Identifying the type and country of locations83is a simple process, benefiting greatly fromgazetteer lookup.Finally, a heuristic hoice is made whether or notto output each element.
For example, adescriptorthat was not paired by the sentence-levelprocessing with any named entity could eitheractually be an isolated descriptor or it could beone where the true link with a named entity wasmissed by the sentence-level model.
Lacking atthis point any trained model to distinguish thosetwo cases, SIFT plays it safe by not outputtingsuch entities.S IFT  System ExamplesThe main determinant of SIFT's performance isthe sentence-level model, and the semanticstructures that it produces.
Secondary but stillsignificant effects on performance ome from themessage-level processing steps that derive TEand TR output from the sentence-level decodertree:?
Extracting elements and relations?
Merging TE elements?
Searching for additional relations with thecross-sentence model?
Filtering candidate ntities and relations foroutputThis section will present examples from theoutput for one of the MUC-7 test messages,demonstrating the different effects that applied.Example 1 shows a case where everythingworked as planned.Here the decoder correctly recognized a personname (PER/NPA) bound to a person descriptor(PER-DESC/NP-R).
That descriptor contains anorganization (ORG/NP) which in turn is linkedto a location.
The LINK and PTR nodes connectthe descriptor with the person, the organizationwith the person descriptor (and thus indirectlywith the person), and the location with theorganization.
In the post-processing, the personname is extracted, with the descriptor text islinked to it, the organization ame is extracted,and the employment relationship noted.
Theorganization is also linked to the nested location;(SINV(VBD said))(PER/NP(PER/NPA(PER/NPP(NNP Eric)(NNP Stallmer)))(, ,)(PER-DESC-OF/NP-LINK(PER-DESC/NP-R(PER-DESC/NPA(NN spokesman))(ORG-OF/NP-PP-LINK(ORG-PTR/PP(IN for)(ORG/NP(ORG/NPA(DT the)(ORG/NPP(NNP Space)(NNP Transportation)(NNP Association)))(LOC-OF/NP-PP-LINK(LOC-PTR/PP(IN of)(LOC-PTR/NPA(LOC/NPP(LOC/NPP(NNP Arlington))(, ,)(LOC/NPP(NNP Virginia)))))))))))Example 184of the two location elements in the LOC phrase,the first is taken as the LOCALE field filler,while the second is looked up in the gazetteer toidentify a country in which the locale value isthen looked up.Example 2 shows the effect of a decoder error.
(ORG/NP(ORG/NPA(ORG/NPP(NNP Bloomberg)(NNP Information)(NNP Television)))(ORG-DESC-OF/NP-LINK(ORG-DESC/NP-R(ORG-DESC/NPA(DT a)(NN unit))(PP(IN of)(ORG/NPA(ORG/NPP(NNP Bloomberg)(NNP L.P.))))))(, ,)(ORG-DESC-OF/NP-LINK(ORG-DESC/NP-R(ORG-DESC/NPA(DT the)(NN parent))(PP(IN of)(ORG/NPA(ORG/NPP(NNP Bloomberg)(NNP Business)(NNP News))))))(, ,))Example 2Here the sentence-level decoder linked bothorganization descriptors back to the top-levelnamed organization, while the correct readingwould have attached the second escriptor to thenested "Bloomberg L.P.".
The post-processingalso therefore links both descriptor phrases to"Bloomberg Information Television" internally.Only the longest descriptor, however, is actuallyoutput, which in this case results in output ofonly the mistaken value.Not surprisingly, a number of the decoder errorsthat affected output stemmed from conjunctions.In another paragraph, for example, themanufacturer organization name "LockheedSpace and Strategic Missiles" was incorrectlybroken at the conjunction, causing the locationrelation with Bethesda to be missed.The cross sentence model is the systemcomponent that tries to find further relationsbeyond those identified by the sentence-levelmodel.
In the walk-through article, thatcomponent did not happen to succeed in findingany such relations.
Example 3 shows the sort ofrelation that we would like that model to be ableto get.
There the sentence-level decoder did linkRubenstein to the organization descriptor"company", but since that descriptor was neverlinked to "News Corporation", the employeerelation was missed.
However, since NewsCorporation is mentioned both in that sentenceand the following sentence, an improved crosssentence model would be one way of attackingsuch examples.
( PER-DESC/NP( PER-DESC/NP( PER-DESC/NPA-R(ORG-DESC-OF/NP-LINK( ORG-DESC/NP-R(NN company) ) )(NN spokesman) )( PER-OF/NPA-LINK( PER-PTR/NPA( PER/NPP(NNP Howard)(NNP J.
)(NNP Rubenstein) ) ) ) )Example 3The last step in processing is the output filter,which heuristically determines whether aproposed constituent should be included in theoutput.
Example 4 shows two examples wherethis filter overrode correct decoder structure.
(s(ART-DESC/NP-R(ART-DESC/NPA(DT A)(JJ Chinese)(NN rocket) )(ART-PTR/VP(VBG carrying)(ART-DESC/NPA-R(DT an)(ORG/NPP(NNP Intelsat))(NN satellite) ) ) )(VP(VBD exploded)Example 4Here the decoder correctly identified both theartifact descriptors "A Chinese rocket" and "anIntelsat satellite", but the output filter chose notto include them.
That choice was made becauseof frequent cases where an indefinite artifactdescriptor not linked to any named artifactshould not be output; an example from elsewherein this message is "the last rocket I'd85recommend".
But this example shows that thisdecision ot to output such cases sometimes costthe system points.SIFT System Results and SummaryThe SIFT system worked by first applying thesentence-level model to each sentence in themessage and then extracting entities, descriptors,and relations from the resulting trees,heuristically merging TE elements, applying thecross-sentence model to identify non-localrelations, and finally filtering and formatting TEand TR templates for output.
In the MUC-7evaluation, the system's core on the TE taskwas 83% recall with 84% precision, for an F of83.49%.
Its score on TR was 64% recall with81% precision, for an F of 71.23%.Because most of the relations in the answer keyswere locally signaled, the cross sentence modelin this application adds only a small boost to theperformance of the sentence-level model.
Whenmeasured before the evaluation on 10 randomly-selected messages from the airplane crashdomain training, the cross sentence modelimproved TR scores by 5 points.
It proved a bitless effective on the 100 messages of the MUC-7test set, improving scores there by only 2 points.
(The F score on the formal test set with the crosssentence model component disabled was69.33%.
)A STAT IST ICAL  NAME-F INDEROverview of the IdentiFinder HMMMode lFor identifying named entities in text, BBN hasdeveloped the IdentiFinder TM trained namedentity extraction system (Bikel, et.
al., 1997),which utilizes an HMM to recognize the entitiespresent in the text.The HMM labels each word either with one ofthe desired classes (e.g., person, organization,etc.)
or with the label NOT-A-NAME (torepresent "none of the desired classes").
Thestates of the HMM fall into regions, one regionfor each desired class plus one for NOT-A-NAME.
(See Figure 4.)
The HMM thus has amodel of each desired class and of the other text.Note that the implementation is not confined tothe seven name classes used in the NE task; theparticular classes to be recognized can be easilychanged via a parameter.Within each of the regions, we use a statisticalbigram language model, and emit exactly oneword upon entering each state.
Therefore, thenumber of states in each of the name-classregions is equal to the vocabulary size, Ivl.Additionally, there are two special states, theSTART-OF-SENTENCE and END-OF-SENTENCEstates.
In addition to generating the word, statesmay also generate features of that word.Features used in the MUC-7 version of thesystem include several features pertaining tonumeric expressions, capitalization, andmembership in lists of important words (e.g.START-OF SENTENCE END.OF SENTENCEFigure 4: Pictorial representation of conceptual model86known corporate designators).The generation of words and name-classesproceeds in the following steps:1.
Select a name-class NC, conditioning on theprevious name-class and the previous word..
Generate the first word inside that name-class, conditioning on the current andprevious name-classes..
Generate all subsequent words inside thecurrent name-class, where each subsequentword is conditioned on its immediatepredecessor.4.
If not at the end of a sentence, go to 1.Whenever a person or organization ame isrecognized, the vocabulary of the system isdynamically updated to include possible aliasesfor that name.
Using the Viterbi algorithm, wesearch the entire space of all possible name-classassignments, maximizing Pr(W,F,NC), the jointprobability of words, features, and name classes.This model allows each type of "name" to haveits own language, with separate bigramprobabilities for generating its words.
Thisreflects our intuition that:There is generally predictive internalevidence regarding the class of a desiredentity.
Consider the following evidence:Organization ames tend to be stereotypicalfor airlines, utilities, law firms, insurancecompanies, other corporations, andgovernment organizations.
Organizationstend to select names to suggest he purposeor type of the organization.
For personnames, first person names are stereotypicalin many cultures; in Chinese, family namesare stereotypical.
In Chinese and Japanese,special characters are used to transliterateforeign names.
Monetary amounts typicallyinclude a unit term, e.g., Taiwan dollars,yen, German marks, etc.?
Local evidence often suggests theboundaries and class of one of the desiredexpressions.
Titles signal beginnings ofperson names.
Closed class words, such asdeterminers, pronouns, and prepositionsoften signal a boundary.
Corporatedesignators (Inc., Ltd., Corp., etc.)
oftenend a corporation ame.While the number of word-states within eachname-class i equal to Ivl, this "interior" bigramlanguage model is ergodic, i.e., there is a non-zero probability associated with every one of the\[VI 2 transitions.
As a parameterized, trainedmodel, for transitions that were never observed,the model "backs off'  to a less-powerful modelwhich allows for the possibility of unknownwords.TrainingThe model as used for the MUC-7 NE evaluationwas trained on a total of approximately 790,000words of NYT newswire data, annotated withapproximately 65,500 named entities.
In orderto increase the size of our training set beyond the90,000 words of training of airline crashdocuments provided by the Government, weselected additional training data from the NorthAmerican News Text corpus.
We annotated fullarticles before discovering a more effectiveannotation strategy.
Since the test domain was tobe similar to the dry-run domain of air crashes,we used the University of MassachusettsINQUERY system to select 2000 articles whichwere similar to the 200 dry run training and testdocuments.
About half of our training dataconsisted of full messages; this portion includedthe 200 messages provided by the Governmentas well as 319 messages from the 2000 retrievedby INQUERY.
The second half of the dataconsisted of sample sentences selected from theremainder of the 2000 messages with the hope ofincreasing the variety of training data.
Thissampling strategy proved more effective thanannotating full messages.
Improvement inperformance asmeasured on the (dry run) airlinecrash test set is shown in Figure 5.87NYT Aidlne Crash Domain96\[ 95939210000 100000 1000000NO.
of WordsFigure 5: F-Measure Increases With Size of Training SetIdentiFinder Results under VaryingTest and Training ConditionsOur F-measure for the official MUC-7 test,90.44, is shown as "Text Baseline" in Figure 6.In addition to this baseline condition, weperformed some unofficial experiments tomeasure the accuracy of the system under moredifficult conditions.
Specifically, we evaluatedthe system on the test data modified to removeall case information ("Upper Case" in Figure 6),and also on the test data in SNOR (SpeechNormalized Orthographic Representation) format("SNOR" in Figure 6).
By converting the text toall upper case characters, information useful forrecognizing names in English is removed.Automatically transcribed speech, even with norecognition errors, is harder due to the lack ofpunctuation, spelling numbers out as words, andupper case in SNOR format.The degradation i performance from mixed caseto all upper case is somewhat greater than thatpreviously observed in similar tests run ongeneric newswire data (about 2 points).
Onepossible explanation is that case information ismore useful in instances where the test domain isdifferent than the domain of the training set.
Thedegradation from all upper case to SNOR issimilar to that previously observed.We also measured the effect of the training setsize on the performance of the system in the aircrash domain of the dry run.
As is to beexpected, increasing the amount of training dataresults in improved system performance.Figure 5 shows an almost wo point increase inF-measure as the training set size was doubledfrom 91,000 words to 176,000 words.
However,the next doubling of the number of words in thetraining set only resulted in a one point increasein F-measure.
This is most likely due to the factthat as training set size increases, the likelihoodof seeing a unique name or constructiondecreases.
Though performance might not havepeaked, adding more training data will have aprogressively smaller effect since the system willnot be seeing many constructions which it hasnot already seen in previous training.MUC-7 NYT Testg- -  OJ~Input ConditionsFigure 6: IdentiFinder Named Entity ResultsCONCLUSIONSThroughout its extraction research under theTIPSTER III program, BBN's goal has been toapply statistical models trained from data in asintegrated a fashion as possible.
We believe thatthis approach is fully capable of matching theperformance of systems based on ruleshandwritten by experts, and that it further offerssignificant advantages in applicability to newproblems and new domains, and to degradedinput (e.g., from a speech recognizer, from OCR,or from sources less polished than newspapertext).88The SIPI" system successfully uses an integratedsyntactic/semantic model to extract entities andrelations.
It employs the Penn Treebank as itssource of syntactic information, and thus requiresfor its training data only the semantic annotationof entities, descriptors, and relationships.
Itssentence-level model determines parts of speech,parses, finds names, and identifies semanticrelationships in a single, integrated process, witha separate merging model then used to connectinformation between sentences.
Given thecurrent early stage of development of the SIFTsystem, we believe that significant performanceimprovements are still possible.
We are alsointerested in measuring performance as afunction of training set size, and have begunapplying SIFT to the broadcast news domain.IdentiFinder is BBN's trained system foridentifying named entities.
Its performance in theMUC-7 evaluation demonstrates the robustnessof the learning algorithm used, even when thetesting is in a different though similar domain tothat of the training material.
Further tests alsoshowed its robustness toall upper case input, andinput with no punctuation.
Our future plans forIdentiFinder include:?
evaluation in the broadcast news domain,which requires speech input in a muchbroader domain,?
applying IdentiFinder to unsegmentedlanguages, and?
working on performance improvements andimprovements in the training process.ACKNOWLEDGEMENTSThe work reported here was supported in part bythe Defense Advanced Research ProjectsAgency.
Technical agents for part of this workwere Fort Huachucha nd AFRL under contractnumbers DABT63-94-C-0062, F30602-97-C-0096, and 4132-BBN-001.
The views andconclusions contained in this document are thoseof the authors and should not be interpreted asnecessarily representing the official policies,either expressed or implied, of the DefenseAdvanced Research Projects Agency or theUnited States Government.We appreciate the contributions of theAnnotation Group at BBN: Ann Albrect,Elizabeth Arentzen, Rachel Bers, Ada Brunstein,Georgina Garcia, Maia Mesnil, and Hugh Walsh.We thank Michael Collins of the University ofPennsylvania for his valuable suggestions.REFERENCESBikel, Dan; S. Miller; R. Schwartz; and R.Weischedel.
(1997) "NYMBLE: A High-Performance Learning Name-finder."
InProceedings of the Fifth Conference on AppliedNatural Language Processing, Association forComputational Linguistics, pp.
194-201.Collins, Michael.
(1996) "A New StatisticalParser Based on Bigram Lexical Dependencies.
"In Proceedings of the 34th Annual Meeting of theAssociation for Computational Linguistics, pp.184-191.Collins, Michael.
(1997) "Three Generative,Lexicalised Models for Statistical Parsing."
InProceedings of the 35th Annual Meeting of theAssociation for Computational Linguistics, pp.16-23.Marcus, M.; B. Santorini;Marcinkiewicz.
(1993) "BuildingAnnotated Corpus of English:Treebank."
Computational19(2):313-330.and M.a Largethe PennLinguistics,Goodman, Joshua.
(1997) "Global Thresholdingand Multiple-Pass Parsing."
In Proceedings ofthe Second Conference on Empirical Methods inNatural Language Processing, Association forComputational Linguistics, pp.
11-25.Weischedel, Ralph; Marie Meteer; RichardSchwartz; Lance Ramshaw; and Jeff Palmucci.
(1993) "Coping with Ambiguity and UnknownWords through Probabilistic Models.
"Computational Linguistics, 19(2):359-382.89
