Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1119?1127,Beijing, August 2010Syntax Based Reordering with Automatically Derived Rules forImproved Statistical Machine TranslationKarthik VisweswariahIBM Researchv-karthik@in.ibm.comJiri NavratilIBM Researchjiri@us.ibm.comJeffrey SorensenGoogle, Inc.sorenj@google.comVijil ChenthamarakshanIBM Researchvijil.e.c@in.ibm.comNanda KambhatlaIBM Researchkambhatla@in.ibm.comAbstractSyntax based reordering has been shownto be an effective way of handling wordorder differences between source andtarget languages in Statistical MachineTranslation (SMT) systems.
We presenta simple, automatic method to learn rulesthat reorder source sentences to moreclosely match the target language word or-der using only a source side parse tree andautomatically generated alignments.
Theresulting rules are applied to source lan-guage inputs as a pre-processing step anddemonstrate significant improvements inSMT systems across a variety of lan-guages pairs including English to Hindi,English to Spanish and English to Frenchas measured on a variety of internal testsets as well as a public test set.1 IntroductionDifferent languages arrange words in different or-ders, whether due to grammatical constraints orother conventions.
Dealing with these word orderpermutations is one of the fundamental challengesof machine translation.
Given an exceptionallylarge training corpus, a phrase-based system canlearn these reordering on a case by case basis.But, if our systems are to generalize to phrases notseen in the training data, they must explicitly cap-ture and model these reorderings.
However, per-mutations are difficult to model and impractical tosearch.Presently, approaches that handle reorderingstypically model word and phrase movements viaa distortion model and rely on the target languagemodel to produce words in the right order.
Earlydistortion models simply penalized longer jumpsmore than shorter jumps (Koehn et al, 2003)independent of the source or target phrasesin question.
Other models (Tillman, 2004),(Al-Onaizan and Papineni, 2006) generalize thisto include lexical dependencies on the source.Another approach is to incorporate features,based on the target syntax, during modeling anddecoding, and this is shown to be effective for var-ious language pairs (Yamada and Knight, 2001),(Zollmann and Venugopal, 2006).
Hierarchicalphrase-based decoding (Chiang, 2005) also al-lows for long range reordering without explic-itly modeling syntax.
While these approacheshave been shown to improve machine translationperformance (Zollmann et al, 2008) they usuallycombine chart parsing with the decoding process,and are significantly more computationally inten-sive than phrase-based systems.A third approach, one that has proved to beuseful for phrase-based SMT systems, is to re-order each source-side sentence using a set ofrules applied to a parse tree of the source sen-tence.
The goal of these rules is to make theword order of the source sentence more sim-ilar to the expected target sentence word or-der.
With this approach, the reordering rulesare applied before training and testing with anSMT system.
The efficacy of these methods hasbeen shown on various language pairs including:French to English (Xia and McCord, 2004), Ger-man to English (Collins et al, 2005), English to1119Chinese, (Wang et al, 2007) and Hindi to English(Ramanathan et al, 2008).In this paper, we propose a simple model for re-ordering conditioned on the source side parse tree.The model is learned using a parallel corpus ofsource-target sentence pairs, machine generatedword alignments, and source side parses.
We ap-ply the reordering model to both training and testdata, for four different language pairs: English?
Spanish, English ?
French, English ?
Hindi,and English ?
German.
We show improvementsin machine translation performance for all of thelanguage pairs we consider except for English ?German.
We use this negative result to proposeextensions to our reordering model.
We note thatthe syntax based reordering we propose can becombined with other approaches to handling re-ordering and does not have to be followed by anassumption of monotonicity.
In fact, our phrase-based model, trained upon reordered data, retainsits reordering models and search, but we expectthat these facilities are employed much more spar-ingly with reordered inputs.2 Related workThere is a significant quantity of work in syntaxbased reordering employed to improve machinetranslation systems.
We summarize our contribu-tions to be:?
Learning the reordering rules based on train-ing data (without relying on linguistic knowl-edge of the language pair)?
Requiring only source side parse trees?
Experimental results showing the efficacy formultiple language pairs?
Using a lexicalized distortion model for ourbaseline decoderThere have been several studies that havedemonstrated improvements with syntaxbased reordering based upon hand-writtenrules.
There have also been studies inves-tigating the sources of these improvements(Zwarts and Dras, 2007).
Hand-written rulesdepend upon expert knowledge of the linguis-tic properties of the particular language pair.Initial efforts (Niessen and Ney, 2001) weremade at improving German-English translationby handling two phenomena: question inver-sion and detachable verb prefixes in German.In (Collins et al, 2005), (Wang et al, 2007),(Ramanathan et al, 2008), (Badr et al, 2009)rules are developed for translation from Ger-man to English, Chinese to English, Englishto Hindi, and English to Arabic respectively.
(Xu et al, 2009) develop reordering rules basedupon a linguistic analysis of English and Koreansentences and then apply those rules to trans-lation from English into Korean and four otherlanguages: Japanese, Hindi, Urdu and Turkish.Unlike this body of work, we automatically learnthe rules from the training data and show efficacyon multiple language pairs.There have been some studies that try to learnrules from the data.
(Habash, 2007) learns re-ordering rules based on a dependency parse andthey report a negative result for Arabic to En-glish translation.
(Zhang et al, 2007) learn re-ordering rules on chunks and part of speechtags, but the rules they learn are not hierarchi-cal and would require large amounts of trainingdata to learn rules for long sentences.
Addition-ally, we only keep a single best reordering (in-stead of a lattice with possible reorderings) whichmakes the decoding significantly more efficient.
(Xia and McCord, 2004) uses source and targetside parse trees to automatically learn rules to re-order French sentences to match English order.The requirement to have both source and targetside parse trees makes this method inapplicableto any language that does not have adequate treebank resources.
In addition, this work reports re-sults using monotone decoding, since their exper-iments using non-monotone decoding without adistortion model were actually worse.3 Reordering issues in specific languagesIn this section we discuss the reordering issuestypical of translating between English and Hindi,French, Spanish and German which are the fourlanguage pairs we experiment on in this paper.3.1 Spanish and FrenchTypical word ordering patterns common to thesetwo European languages relate to noun phrases in-cluding groups of nouns and adjectives.
In con-1120trast to English, French and Spanish adjectivesand adjunct nouns follow the main noun, i.e.
wetypically observe a reversal of word order in nounphrases, e.g., ?A beautiful red car?
translatesinto French as ?Une voiture rouge beau?, and as?Un coche rojo bonito?
into Spanish.
Phrase-based MT systems are capable of capturing thesepatterns provided they occur with sufficient fre-quency for each example in the training data.
Forrare noun phrases, however, the MT may pro-duce erroneous word order that can lead to seri-ous distortions in the meaning.
Particularly dif-ficult are nominal phrases from specialized do-mains that involve challenging terminology, forexample: ?group reference attribute?
and ?valida-tion checking code?.
In both instances, the base-line MT system generated translations with an in-correct word order and, consequently, possibly adifferent meaning.
We will return to these two ex-amples in Section 5.1 to compare the output of aMT system with and without reordering.3.2 GermanUnlike French and Spanish, German poses a con-siderably different challenge with respect to wordordering.
The most frequent reordering in Germanrelates to verbs, particularly verb groups consist-ing of auxiliary and main verbs, as well as verbsin relative clauses.
Moreover, reordering patternsbetween German and English tend to span largeportions of the sentence.
We included German inour investigations to determine whether our auto-mated rule extraction procedure can capture suchlong distance patterns.3.3 HindiHindi word order is significantly different thanEnglish word order; the typical order followedis Subject Object Verb (although Object SubjectVerb order can be used if nouns are followed byappropriate case markers).
This is in contrast toEnglish which has a Subject Verb Object order.This can result in words that are close in Englishmoving arbitrarily far apart in Hindi depending onthe length of the noun phrase representing the ob-ject and the length of the verb phrase.
These longrange reorderings are generally hard for a phrasebased system to capture.
Another way Hindi andEnglish differ is that prepositions in English be-come postpositions in Hindi and appear after thenoun phrase.
Again, this reordering can lead tolong distance movements of words.
We includeHindi in our investigation since it has significantlydifferent structure as compared to English.4 Learning reordering rulesIn this section we describe how we learn rules thattransform source parse trees so the leaf word orderis more like the target language.
We restrict our-selves to reorderings that can be obtained by per-muting child nodes at various interior nodes in aparse tree.
With many reordering phenomena dis-cussed in Section 3 this is a fairly strong assump-tion about pairs of languages, and there are exam-ples in English?Hindi where such an assumptionwill not allow us to generate the right reordering.As an example consider the English sentence ?Ido not want to play?.
The sentence has a parse:SNPPRPIVPVBPdoRBnotVPVBwantSVPTOtoVPVBplayThe correct word order of the translation in Hindiis ?I to play not want?
In this case, the word notbreaks up the verb phrase want to play and hencethe right Hindi word order cannot be obtained bythe reordering allowed by our model.
We foundsuch examples to be rare in English?Hindi, andwe impose this restriction for the simplicity of themodel.
Experimental results on several languagesshow benefits of reordering in spite of this simpli-fying assumption.Consider a source sentence s and its corre-sponding constituency parse tree S1.
We set upthe problem in a probabilistic framework, i.e.
wewould like to build a probabilistic model P (T |S)that assigns probabilities to trees such that the1In this paper we work with constituency parse trees.
Ini-tial experiments, applying similar techniques to dependencyparse trees did not yield improvements.1121word order in trees T which are assigned higherprobability match the order of words in the targetlanguage.
A parse tree, S is a set of nodes.
Inte-rior nodes have an ordered list of children.
Leafnodes in the tree are the words in the sentences, and interior nodes are labeled by the linguis-tic constituent that they represent.
Each word hasa parent node (with only one child) labeled by thepart-of-speech tag of the word.Our model assigns non-zero probabilities totrees that can be obtained by permuting the childnodes at various interior nodes of the tree S. Weassume that children of a node are ordered inde-pendently of all other nodes in the tree.
ThusP (T |S) =?n?I(S)P (?
(cn)|S, n, cn),where I(S) is the set of interior nodes in the treeS, cn is the list of children of node n and ?
is apermutation.
We further assume that the reorder-ing at a particular node is dependent only on thelabels of its children:P (T |S) =?n?I(S)P (?
(cn)|cn).We parameterize our model using a log-linearmodel:P (?
(cn)|cn) =1Z(cn)exp(?T f(?, cn)).
(1)We choose the simplest possible set of featurefunctions: for each observed sequence of non-terminals we have one boolean feature per per-mutation of the sequence of non-terminals, withthe feature firing iff that particular sequence is ob-served.
Assuming, we have a training corpus C of(T, S) tree pairs, we could optimize the parame-ters of our model to maximize :?S?C P (T |S).With the simple choice of feature functions de-scribed above, this amounts to:P (?
(cn)|cn) =count(?
(cn))count(cn),where count(cn) is the number of times the se-quences of nodes cn is observed in the trainingdata and count(?
(cn)) is the number of timesthat cn in S is permuted to ?
(cn) in T .
In Sec-tion 6, we show considering more general fea-ture functions and relaxing some of the indepen-dence might yield improvements on certain lan-guage pairs.For each source sentence s with parse S we findthe tree T that makes the given alignment for thatsentence pair most monotone.
For each node n inthe source tree S let Dn be the set of words thatare descendants of n. Let us denote by tpos(n) theaverage position of words in the target sentencethat are aligned to words in Dn.
Thentpos(n) = 1|Dn|?w?Dna(w),where a(w) is the index of the word on the targetside that w is aligned with.
If a word w is notaligned to any target word, we leave it out fromthe mean position calculation above.
If a word wis aligned to many words we let a(w) be the meanposition of the words that w is aligned to.
For eachnode n in the tree we transform the tree by sortingthe list of children of n according to tpos.
Thepairs of parse trees that we obtain (S, T ) in thismanner form our training corpus to estimate ourparameters.In using our model, we once again go for thesimplest choice, we simply reorder the source sidesentences by choosing arg maxT P (T |S) both intraining and in testing; this amounts to reorderingeach interior node based on the most frequent re-ordering of the constituents seen in training.
Toreduce the effect of noise in training alignmentswe apply the reordering, only if we have seen theconstituent sequence often enough in our trainingdata (a count threshold parameter) and if the mostfrequent reordering is sufficiently more frequentthan the next most frequent reordering (a signifi-cance threshold).5 Experiments5.1 Results for French, Spanish, and GermanIn each language, the rule extraction wasperformed using approximately 1.2M sen-tence pairs aligned using a maxent aligner(Ittycheriah and Roukos, 2005) trained using avariety of domains (Europarl, computer manuals)1122and a maximum entropy parser for English(Ratnaparkhi, 1999).
With a significance thresh-old of 1.2, we obtain about 1000 rules in theeventual reordering process.Phrase-based systems were trained for each lan-guage pair using 11M sentence pairs spanning avariety of publicly available (e.g.
Europarl, UNspeeches) and internal corpora (IT technical andnews domains).
The system phrase blocks wereextracted based on a union of HMM and max-ent alignments with corpus-selective count prun-ing.
The lexicalized distortion model was usedas described in (Al-Onaizan and Papineni, 2006)with a window width of up to 5 and a maximumnumber of skipped (not covered) words during de-coding of 2.
The distortion model assigns a prob-ability to a particular word to be observed witha specific jump.
The decoder uses a 5-gram in-terpolated language model spanning the variousdomains mentioned above.
The baseline systemwithout reordering and a system with reorderingwas trained and evaluated in contrastive experi-ments.
The evaluation was performed utilizing thefollowing (single-reference) test sets:?
News: 541 sentences from the news domain.?
TechA: 600 sentences from a computer-related technical domain, this has been usedas a dev set.?
TechB: 1038 sentences from a similar do-main as TechA used as a blind test.?
Dev09: 1026 sentences defined as the news-dev2009b development set of the Workshopon Statistical Machine Translation 2009 2.This set provides a reference measurementusing a public data set.
Previously publishedresults on this set can be found, for example,in (Popovic et al, 2009).In order to assess changes in word ordering pat-terns prior to and after an application of the re-ordering, we created histograms of word jumpsin the alignments obtained in the baseline as wellas in the reordered system.
Given a source wordsi at index i and the target word tj it is alignedto at index j, a jump of 1 would correspond tosi+1 aligning to target word tj+1, while an align-ment to tj?1 corresponds to a jump of -1, etc.
A2http://statmt.org/wmt09/?8 ?6 ?4 ?2 0 2 4 6 8 10?1?0.500.511.52x 105Cnt2?Cnt1Difference of histograms after and before reordering (EN?ES)?8 ?6 ?4 ?2 0 2 4 6 8 10?500005000100001500020000Distance to next positionCnt2?Cnt1Difference of histograms after and before reordering (EN?FR)Figure 1: Difference-histogram of word orderdistortions for English?Spanish (upper), andEnglish?French (lower).histogram over the jump values gives us a sum-mary of word order distortion.
If all of the jumpswere one, then there is no reordering between thetwo languages.
To gain insight into changes in-troduced by our reordering we look at differencesof the two histograms i.e., counts after reorderingminus counts before reordering.
We would hopethat after reordering most of the jumps are smalland concentrated around one.
Figure 1 showssuch difference-histograms for the language pairsEnglish?Spanish and English?French, respec-tively, on a sample of about 15k sentence pairsheld out of the system training data.
Here, a pos-itive difference value indicates an increased num-ber after reordering.
In both cases a consistenttrend toward monotonicity is observed, i.e morejumps of size one and two, and fewer large jumps.This confirms the intended reordering effect andindicates that the reordering rules extracted gen-eralize well.Table 1 shows the resulting uncased BLEUscores for English-Spanish and English-French.In both cases the reordering has a consistentpositive effect on the BLEU scores across test sets.In examining the sources of improvement, we no-ticed that word order in several noun phrases that1123System News TechA TechB Dev09Baseline 0.3849 0.3371 0.3483 0.2244SpanishReordered 0.4031 0.3582 0.3605 0.2320Baseline 0.5140 0.2971 0.3035 0.2014FrenchReordered 0.5242 0.3152 0.3154 0.2092Baseline 0.2580 0.1582 0.1697 0.1281GermanReordered 0.2544 0.1606 0.1682 0.1271Baseline 20.0HindiReordered 21.7Table 1: Uncased BLEU scores for phrase-basedmachine translation.were not common in the training data were fixedby use of the reordering rules.Table 1 shows the BLEU scores for theEnglish?German language pair, for which amixed result is observed.
The difference-histogram for English?German, shown in Figure2, differs from those of the other languages withseveral increases in jumps of large magnitude, in-dicating failure of the extracted rules to general-ize.The failure of our simple method to gain con-sistent improvements comparable to Spanish andFrench, along with our preliminary finding that arelatively few manually crafted reordering rules(we describe these in Section 6.4) tend to outper-form our method, leads us to believe that a morerefined approach is needed in this case and will besubject of further discussion below.5.2 Results for HindiOur Hindi-English experiments were run withan internal parallel corpus of roughly 250k sen-tence pairs (5.5M words) consisting of variousdomains (including news).
To learn reorderingrules we used HMM alignments and a maxentparser (Ratnaparkhi, 1999), with a count thresh-old of 100, and a significance threshold of 1.7(these settings gave us roughly 200 rules).
We alsoexperimented with other values of these thresh-olds and found that the performance of our sys-tems were not very sensitive to these thresholds.We trained Direct Translation Model 2 (DTM)?10 ?5 0 5 10?600?400?200020040060080010001200Distance to next positionCnt2?Cnt1Difference of histograms after and before reordering (EN?DE)Figure 2: Difference-histogram of word order dis-tortions for English?German.systems (Ittycheriah and Roukos, 2007) with andwithout source reordering and evaluated on a testset of 357 sentences from the News domain.We note that the DTM baseline includes features(functions of target words and jump size) that al-low it to model lexicalized reordering phenomena.The reordering window size was set to +/- 8 wordsfor the baseline and system with reordered in-puts.
Table 1 shows the uncased BLEU scores forEnglish-Hindi, showing a gain from using the re-ordering rules.
For the reordered case, the HMMalignments are rederived, but the accuracy of thesewere no better than those of the unreordered in-put and experiments showed that the gains in per-formance were not due to the effect on the align-ments.Figure 3 shows difference-histograms for thelanguage pair English?Hindi, on a sample ofabout 10k sentence pairs held out of the systemtraining data.
The histogram indicates that ourreordering rules generalize and that the reorderedEnglish is far more monotonic with respect to theHindi.6 Analysis of errors and futuredirectionsIn this section, we analyze some of the sources oferrors in reordering rules learned via our model, tobetter understand directions for further improve-ment.1124?8 ?6 ?4 ?2 0 2 4 6 8 10?1.5?1?0.500.511.5x 104Distance to next positionCnt2?Cnt1Difference of histograms after and before reordering (EN?HI)Figure 3: Difference-histogram of word order dis-tortions for English?Hindi.6.1 Model weaknessIn our initial experiments, we noticed that for themost frequent reordering rules in English?Hindi(e.g that IN NP or NP PP flips in Hindi) the prob-ability of a reordering was roughly 65%.
Thiswas concerning since it meant that on 35% of thedata we would be making wrong reordering deci-sions by choosing the most likely reordering.
Toget a better feel for whether we needed a strongermodel (e.g by lexicalization or by looking at largercontext in the tree rather than just the children),we analyzed some of the cases in our training datawhere (IN,NP), (NP, PP) pairs were left unalteredin Hindi.
In doing that analysis, we noticed exam-ples involving negatives that our model does notcurrently handle.
The first issue was mentionedin Section 4, where the assumption that we canachieve the right word order by reordering con-stituent phrases, is incorrect.
The second issueis illustrated by the following sentences: I havesome/no books, which have similar parse struc-tures, the only difference being the determinersome vs the determiner no.
In Hindi, the orderof the fragments some books and the fragmentno books are different (in the first case the wordsstay in order, in the second the flip).
Handlingthis example would need our model to be lexical-ized.
These issue of negatives requiring specialhandling also came up in our analysis of German(Section 6.4).
Other than the negatives (which re-quire a lexicalized model), the major reason forthe lack of sharpness of the reordering rule proba-bility was alignment errors and parser issues.
WeAlignerNumber ofSentences fMeasure BLEU scoreHMM 250k 62.4 21.7MaxEnt 250k 76.6 21.4Manual 5k - 21.3Table 2: Using different alignmentslook at these topics next.6.2 Alignment accuracySince we rely on automatically generated align-ments to learn the rules, low accuracy ofthe alignments could impact the quality ofthe rules learned.
This is especially a con-cern for English?Hindi since the quality ofHMM alignments are fairly low.
To quan-tify this effect, we learn reordering rules us-ing three sets of alignments: HMM alignments,alignments from a supervised MaxEnt aligner(Ittycheriah and Roukos, 2005), and hand align-ments.
Table 2 summarizes our results usingaligners with differing alignment qualities for ourEnglish?Hindi task and shows that quality ofalignments in learning the rules is not the drivingfactor in affecting rule quality.6.3 Parser accuracyAccuracy of the parser in the source language isa key requirement for our reordering method, be-cause we choose the single best reordering basedon the most likely parse of the source sentence.This would especially be an issue in translat-ing from languages other than English, where theparser would not be of quality comparable to theEnglish parser.In examining some of the errors in reorderingwe did observe a fair fraction attributable toissues in parsing, as seen in the example sentence:The rich of this country , corner almost 90% ofthe wealth .The second half of the sentence is parsed by theBerkeley parser (Petrov et al, 2006) as:FRAGNP-SBJNNcornerADVPRBalmostNP-SBJNPCD90%PPINofNPDTtheNNwealth1125and by IBM?s maximum entropyparser parser (Ratnaparkhi, 1999) as:VPVBcornerNPNPQPRBalmostCD90%PPINofNPDTtheNNwealthWith the first parse, we get the right Hindi orderfor the second part of the sentence which is: thewealth of almost 90% corner .
To investigate theeffect of choice of parser we compared using theBerkeley parser and the IBM parser for reorder-ing, and we found the BLEU score essentiallyunchanged: 21.6 for the Berkeley parser and21.7 for the IBM parser.
A potential source ofimprovements might be to use alternative parses(via different parsers or n-best parses) to generaten-best reorderings both in training and at test.6.4 Remarks on German reorderingDespite a common heritage, German word order isdistinct from English, particularly regarding verbplacement.
This difference can be dramatic, if anauxiliary (e.g.
modal) verb is used in conjunctionwith a full verb, or the sentence contains a subor-dinate clause.
In addition to our experiments withautomatically learned rules, a small set of hand-crafted reordering rules was created and evalu-ated.
Our preliminary results indicate that the lat-ter rules tend to outperform the automatically de-rived ones by 0.5-1.0 BLEU points on average.These rules are summarized as follows:1.
In a VP immediately following an NP, movethe negation particle to main verb.2.
Move a verb group away from a modal verb;to the end the of a VP.
Negation also movesalong with verb.3.
Move verb group to end of an embed-ded/relative clause.4.
In a VP following a subject, move negationto the end of VP (handling residual cases)The above hand written rules show several weak-nesses of our automatically learned rules for re-ordering.
Since our model is not lexicalized, nega-tions are not handled properly as they are taggedRB (along with other adverbs).
Another limitationapparent from the first rule above (the movementof verbs in a verb phrase depends on the previousphrase being a noun phrase) is that the automaticreordering rule for a node?s children depends onlyon the children of that node and not a larger con-text.
For instance, a full verb following a modalverb is typically parsed as a VP child node of themodal VP node, hence the automatic rule, as cur-rently considered, will not take the modal verb(being a sibling of the full-verb VP node) into ac-count.
We are currently investigating extensionsof the automatic rule extraction alorithm to ad-dress these shortcomings.6.5 Future directionsBased on our analysis of the errors and on thehand designed German rules we would like to ex-tend our model with more general feature func-tions in Equation 1 by allowing features: thatare dependent on the constituent words (or head-words), that examine a large context than just anodes children (see the first German rule above)and that fire for all permutations when the con-stituent X is moved to the end (or start).
Thiswould allow us to generalize more easily to learnrules of the type ?move X to the end of thephrase?.
Another direction that we feel should beexplored, is the use of multiple parses to obtainmultiple reorderings and combine these at a laterstage.7 ConclusionsIn this paper we presented a simple method toautomatically derive rules for reordering sourcesentences to make it look more like targetlanguage sentences.
Experiments (on inter-nal and public test sets) indicate performancegains for English?French, English?Spanish,and English?Hindi.
For English?German wedid not see improvements with automaticallylearned rules while a few hand designed rules didgive improvements, which motivated a few direc-tions to explore.1126References[Al-Onaizan and Papineni2006] Al-Onaizan, Yaser andKishore Papineni.
2006.
Distortion models for sta-tistical machine translation.
In Proceedings of ACL.
[Badr et al2009] Badr, Ibrahim, Rabih Zbib, andJames Glass.
2009.
Syntactic phrase reordering forenglish-to-arabic statistical machine translation.
InProceedings of EACL.
[Chiang2005] Chiang, David.
2005.
A hierarchicalphrase-based model for statistical machine transla-tion.
In Proceedings of ACL.
[Collins et al2005] Collins, Michael, Philipp Koehn,and Ivona Kucerova.
2005.
Clause restructuringfor statistical machine translation.
In Proceedingsof ACL.
[Habash2007] Habash, Nizar.
2007.
Syntactic prepro-cessing for statistical machine translation.
In MTSummit.
[Ittycheriah and Roukos2005] Ittycheriah, Abrahamand Salim Roukos.
2005.
A maximum entropyword aligner for arabic-english machine translation.In Proceedings of HLT/EMNLP.
[Ittycheriah and Roukos2007] Ittycheriah, Abrahamand Salim Roukos.
2007.
Direct translation model2.
In Proceedings of HLT-NAACL, pages 57?64.
[Koehn et al2003] Koehn, Philipp, Franz Och, andDaniel Marcu.
2003.
Statistical phrase-based trans-lation.
In Proceedings of HLT-NAACL.
[Niessen and Ney2001] Niessen, Sonja and HermannNey.
2001.
Morpho-syntactic analysis for reorder-ing in statistical machine translation.
In Proc.
MTSummit VIII.
[Petrov et al2006] Petrov, Slav, Leon Barrett, RomainThibaux, and Dan Klein.
2006.
Learning accu-rate, compact, and interpretable tree annotation.
InCOLING-ACL.
[Popovic et al2009] Popovic, Maja, David Vilar,Daniel Stein, Evgeny Matusov, and Hermann Ney.2009.
The RWTH machine translation system forWMT 2009.
In Proceedings of WMT 2009.
[Ramanathan et al2008] Ramanathan, A., P. Bhat-tacharyya, J. Hegde, R. M. Shah, and M. Sasikumar.2008.
Simple syntactic and morphological process-ing can help english-hindi statistical machine trans-lation.
In Proceedings of International Joint Con-ference on Natural Language Processing.
[Ratnaparkhi1999] Ratnaparkhi, Adwait.
1999.
Learn-ing to parse natural language with maximum en-tropy models.
Machine Learning, 34(1-3).
[Tillman2004] Tillman, Christoph.
2004.
A unigramorientation model for statistical machine translation.In Proceedings of HLT-NAACL.
[Wang et al2007] Wang, Chao, Michael Collins, andPhilipp Koehn.
2007.
Chinese syntactic reorderingfor statistical machine translation.
In Proceedingsof EMNLP-CoNLL.
[Xia and McCord2004] Xia, Fei and Michael McCord.2004.
Improving a statistical mt system with auto-matically learned rewrite patterns.
In Proceedingsof Coling.
[Xu et al2009] Xu, Peng, Jaeho Kang, Michael Ring-gaard, and Franz Och.
2009.
Using a dependencyparser to improve SMT for Subject-Object-Verb lan-guages.
In Proceedings of NAACL-HLT.
[Yamada and Knight2001] Yamada, Kenji and KevinKnight.
2001.
A syntax-based statistical translationmodel.
In Proceedings of ACL.
[Zhang et al2007] Zhang, Yuqi, Richard Zens, andHermann Ney.
2007.
Chunk-level reorderingof source language sentences with automaticallylearned rules for statistical machine translation.
InNAACL-HLT AMTA Workshop on Syntax and Struc-ture in Statistical Translation.
[Zollmann and Venugopal2006] Zollmann, Andreasand Ashish Venugopal.
2006.
Syntax augmentedmachine translation via chart parsing.
In Pro-ceedings on the Workshop on Statistical MachineTranslation.
[Zollmann et al2008] Zollmann, Andreas, AshishVenugopal, Franz Och, and Jay Ponte.
2008.
Asystematic comparison of phrase-based, hierar-chical and syntax-augmented statistical MT.
InProceedings of COLING.
[Zwarts and Dras2007] Zwarts, Simon and Mark Dras.2007.
Syntax-based word reordering in phrase-based statistical machine translation: why does itwork?
In Proc.
MT Summit.1127
