Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 390?398,Sydney, July 2006. c?2006 Association for Computational LinguisticsCorrective Models for Speech Recognition of Inflected LanguagesIzhak Shafran and Keith HallCenter for Language and Speech ProcessingJohns Hopkins UniversityBaltimore, MD 21218{zakshafran,keith hall}@jhu.eduAbstractThis paper presents a corrective modelfor speech recognition of inflected lan-guages.
The model, based on a discrim-inative framework, incorporates word n-grams features as well as factored mor-phological features, providing error reduc-tion over the model based solely on wordn-gram features.
Experiments on a largevocabulary task, namely the Czech portionof the MALACH corpus, demonstrate per-formance gain of about 1.1?1.5% absolutein word error rate, wherein morphologi-cal features contribute about a third of theimprovement.
A simple feature selectionmechanism based on ?2 statistics is shownto be effective in reducing the number offeatures by about 70% without any loss inperformance, making it feasible to exploreyet larger feature spaces.1 IntroductionN -gram models have long been the stronghold ofstatistical language modeling approaches.
Withinthe n-gram paradigm, straightforward approachesfor increasing accuracy include using larger train-ing sets and augmenting the contextual informa-tion within the n-gram window.
Incorporatingsyntactic features into the context has been at theforefront of recent research (Collins et al, 2005;Rosenfeld et al, 2001; Chelba and Jelinek, 2000;Hall and Johnson, 2004).
However, much of theprevious work has focused on English languagesyntax.
This paper addresses syntax as capturedby the inflectional morphology of highly inflectedlanguage.High inflection in a language is generally cor-related with some level of word-order flexibil-ity.
Morphological features either directly identifyor help disambiguate the syntactic participants ofa sentence.
Inflectional morphology works as aproxy for structured syntax in a language.
Model-ing morphological features in these languages notonly provides an additional source of informationbut can also alleviate data sparsity problems.Czech speech recognition needs to deal withtwo sources of errors which are absent in En-glish, namely, the inflectional morphology and thedifferences in the formal (written) and colloquial(spoken) forms.
Table 1 presents an example out-put of our speech recognizer on an utterance froma Holocaust survivor, who is recounting GeneralRomel?s desert campaign during the Second WorldWar.
In this example, the feminine past-tenseform of the Czech verb for to be is chosen mis-takenly, which is followed by a sequence of in-correct words chosen primarily to maintain agree-ment with the feminine form of the verb.
This isan example of what we refer to as the morpho-logical grouping effect.
When the acoustic modelprefers a word with an incorrect inflection, the lan-guage model effectively propagates the error tolater words.
A language model based on word-forms prefers sequences observed in the trainingdata, which will implicitly force an agreementwith the inflections of preceding words, making itdifficult to stop propagating errors.
Although thisanalysis is anecdotal in nature, the grouping effectappears to be prevalent in the Czech dataset usedin this work.
The proposed corrective model withmorphological features is expected to alleviate thegrouping effect as well as to improve the recogni-tion of inflected languages in general.In the following section, we present a briefreview of related work on morphological lan-guage modeling and discriminative language mod-390REF no Jez???s?
to uz?
byl Romel hnedle pr?ed Alexandri?
?gloss well Jesus by that time already was Romel just in front of Alexandriatranslation oh Jesus, Romel was already just in front of Alexandria by that timeHYP no Jez???s?
to uz?
byla sama hned leps???
Alexandriegloss well Jesus by that time already (she) was herself just better Alexandriatranslation oh Jesus, she was herself just better Alexandria by that timeTable 1: An example of the grouping effect.
The incorrect form of the verb to be begins a group ofincorrect words in the hypothesis, but these words agree in their morphological inflection.els.
We begin the description of our work in sec-tion 3 with the type of morphological featuresmodeled as well as their computation from the out-put word-lattices of a speech recognizer.
Section 4presents the corrective model and the training ap-proach explored in the current work.
A simple andeffective feature selection mechanism is describedin section 5.
In section 6, the proposed frameworkis evaluated on a large vocabulary Czech speechrecognition task.
Results show that the morpho-logical features provide a significant improvementover models lacking these features; subsequently,two different analyses are provided to understandthe contribution of different morphological fea-tures.2 Related WorkIt has long been assumed that incorporating mor-phological features into a language models shouldhelp improve the performance of speech recogni-tion systems.
Early models for German showedlittle improvements over bigram language mod-els and almost no improvement over trigram mod-els (Geutner, 1995).
More recently, morphology-based models have been shown to help reduce er-ror rate for out-of-vocabulary words (Carki et al,2000; Podvesky and Machek, 2005).Much of the early work on morphological lan-guage modeling was focused on utilizing compos-ite morphological tags, largely due to the difficultyin teasing apart the intricate interdependencies ofthe morphological features.
Apart from a few ex-ceptions, there has been little work done in explor-ing the morphological systems of highly inflectedlanguages.Kirchhoff and colleagues (2004) successfullyincorporated morphological features for Arabicusing a factored language model.
In their ap-proach, morphological inflections are modeled ina generative framework, and the space of factoredmorphological tags is explored using a genetic al-gorithm.Adopting a different tactic, Choueiter andcolleagues (2006) exploited morphological con-straints to prune illegal morpheme sequences fromASR output.
They noticed that the gains obtainedfrom the application of such constraints in Arabicdepends on the size of the vocabulary ?
an absolutegain of 2.4% in word error rate (WER) reducedto 0.2% when the size was increased from 64k to800k.Our approach to modeling morphology differsfrom that of Vergyri et al (2004) and Choueiter etal.
(2006).
By choosing a discriminative frame-work and maximum entropy based estimation, weallow arbitrary features or constraints and theircombinations without the need for explicit elab-oration of the factored space and its backoff ar-chitecture.
Thus, morphological features can beincorporated in the absence of knowledge abouttheir interdependencies.Several researchers have investigated tech-niques for improving automatic speech recogni-tion (ASR) results by modeling the errors (Collinset al, 2005; Shafran and Byrne, 2004).
Collinset al (2005) present a corrective language modelbased on a discriminative framework.
Initially, aset of hypotheses is generated by a baseline de-coder with standard acoustic and language models.A corrective model is estimated such that it scoresdesired or oracle hypotheses higher than compet-ing hypotheses.
The parameters are learned viathe perceptron algorithm which shifts weight awayfrom features associated with poor hypotheses andtowards those associated with better hypotheses.By the appropriate choice of desired hypotheses,the model parameters can be estimated to mini-mize WER in speech recognition.
During decod-ing, the model can then be used to rerank a setof hypotheses, and hence, it is also known as areranking framework.
This paradigm allows mod-eling arbitrary input features, even syntactic fea-tures obtained from a parser.
We adopt a vari-ant of this framework where the corrective modelis based on a conditional model estimated by themaximum entropy procedure (Charniak and John-391son, 2005) and we investigate its effectiveness inmodeling morphological features for highly in-flected languages, in particular, Czech.3 Inflectional MorphologyInflectional abundance in a language generallycorresponds to some flexibility in word order.
Ina free word-order language, the order of senten-tial participants is relatively unconstrained.
Thisdoes not mean a speaker of the language can ar-bitrarily choose an order.
Word-order choice maychange the semantic and/or pragmatic interpreta-tion of an utterance.
Czech is known as a freeword-order language allowing for subject, object,and verbal components to come in any order.
Mor-phological inflection in these languages must in-clude a syntactic case marker to allow the determi-nation of which participants are subjects (nomina-tive case), objects (accusative or dative) and othersuch entities.
Additionally, morphological inflec-tion encodes features such as gender and number.The agreement of these features between senten-tial components (adjectives with nouns, subjectswith verbs, etc.)
may further disambiguate the tar-get of a modifier (e.g., identifying the noun that ismodified by a particular adjective).The increased flexibility in word order aggra-vates the data sparsity of standard n-gram lan-guage model for two reasons: first, the number ofvalid configurations of a group of words increaseswith the free order; and second, lexical items aredecorated with the inflectional morphemes, multi-plying the number of word-forms that appear.In addition to modeling sequences of word-forms, we model sequences of morphologicallyreduced lemmas, sequence of morphological tagsand sequences of various factored representationsof the morphological tags.
Factoring a wordinto the semantics-bearing lemma and syntax-bearing morphological tag alleviates the data spar-sity problem to some extent.
However, the numberof possible factorizations of n-grams is large.
Theapproach adopted in this work is to provide a richclass of features and defer the modeling of theirinteraction to the learning procedure.3.1 Extracting Morphological FeaturesThe extraction of reliable morphological featurescritically effects further morphological modeling.Here, we first select the most likely morphologi-cal analysis for each word using a morphologicalLabel Description # Valueslemma Reduced lexeme < |vocab|POS Coarse part-of-speech 12D-POS Detailed part-of-speech 65gen Grammatical Gender 10num Grammatical Number 5case Grammatical Case 8Table 2: Czech morphological features used in thecurrent work.
The # Values field indicates the sizeof the closed set of possible values.
Not all valuesare used in the annotated data.tagger.
In particular, we use the Czech feature-based tagger distributed with the Prague Depen-dency Treebank (Hajic?
et al, 2005).
The tagger isbased on a morphological analyzer which uses alexicon and a rule-based tag guesser for words notfound in the lexicon.
Trained by the maximum en-tropy procedure, the tagger uses left and right con-textual features from the input string.
Currently,this is the best available Czech-language tagger.See Hajic?
and Vidova?-Hladka?
(1998) for furtherdetails on the tagger.A disadvantage of such an approach is thatthe tagger works on strings rather than the word-lattices that we expect from an ASR system.Therefore, we must extract a set of strings from thelattices prior to tagging.
An alternative approach isto hypothesize all morphological analyses for eachword in the lattice, thereby considering the entireset of analyses as features in the model.
In the cur-rent implementation we have chosen to use a tag-ger to reduce the complexity of the model by lim-iting the number of active features while still ob-taining relatively reliable features.
Moreover, sys-tematic errors in tagging can be potentially com-pensated by the corrective model.The initial stage of feature extraction beginswith an analysis of the data on which we train andtest our models.
The process follows:1.
Extract the n-best hypotheses according to abaseline model, where n varies from 50 to1000 in the current work.2.
Tag each of the hypotheses with the morpho-logical tagger.3.
Re-encode the original word strings alongwith their tagged morphological analysis ina weighted finite state transducer to allow392Word-form to obdob??
bylo pome?rne?
kra?tke?gloss that period was relatively shortlemma ten obdob??
by?t pome?rne?
kra?tky?tag PDNS1 NNNS1 VpNS- Dg?
AAFS2Table 3: A morphological analysis of Czech.
This analyses was generated by the Hajic?
tagger.form to obdob??
bylo pome?rne?
kra?tke?to obdob??
obdob??
bylo bylo pome?rne?
pome?rne?
kra?tke?lemma ten obdob??
by?t pome?rne?
kra?tky?ten obdob??
obdob??
by?t by?t pome?rne?
pome?rne?
kra?tky?tag PDNS1 NNNS1 VpNS- Dg?
AAFS2PDNS1 NNNS1 NNNS1 VpNS- VpNS- Dg?
Dg?
AAFS2POS P N V D AP N N V V D D A. .
.
.
.
.case 1 1 - - 21 1 1 - - 0 - 2num/case S1 S1 S- ?
S2S1 S1 S1 S- S- ?
?
S2.
.
.
.
.
.Table 4: Examples of the n-grams extracted from the Czech sentence To obdob??
bylo pome?rne?
kra?tke?.
Asubset of the feature classes is presented here.
The morphological feature values are those assigned bythe Hajic?
tagger.an efficient means of projecting the hypothe-ses from word-form to morphology and viceversa.4.
Extract appropriately factored n-gram fea-tures for each hypothesis as described below.Each word state in the original lattice has anassociated lemma/tag from which a variety of n-gram features can be extracted.From the morphological features assigned bythe tagger, we chose to retain only a subset and dis-card the less reliable features which are semanticin nature.
The basic morphological features usedare detailed in Table 2.
In the tag-based model, astring of 5 characters representing the 5 morpho-logical fields is used as a unique identifier.
Thederived features include n-grams of POS, D-POS,gender (gen), number (num), and case features aswell as their combinations.POS, D-POS Captures the sub-categorization ofthe part-of-speech tags.gen, num Captures complex gender-numberagreement features.num, case Captures number agreement betweenspecific case markers.POS, case Captures associated POS/Case fea-tures (e.g., adjectives associated with nomi-native elements).The paired features allow for complex inflec-tional interactions and are less sparse than thecomposite 5-component morphological tags.
Ad-ditionally, the morphologically reduced lemmaand n-grams of lemmas are used as features in themodels.Table 3 presents a morphological analysis of theCzech sentence To obdob??
bylo pome?rne?
kra?tke?.The encoded tags represent the first 5 fields of thePrague Dependency Treebank morphological en-coding and correspond to the last 5 rows of Ta-ble 2.
Features for this sentence include the word-form, lemma, and composite tag features as wellas the components of each tag and the above men-tioned concatenation of tag fields.
Additionally,n-grams of each of these features are included.
Bi-gram features extracted from an example sentenceare illustrated in Table 4.The following section describes how the fea-393tures extracted above are modeled in a discrimi-native framework to reduce word error rate.4 Corrective Model and EstimationIn this work, we adopt the reranking frameworkof Charniak and Johnson (2005) for incorporatingmorphological features.
The model scores eachtest hypothesis y using a linear function, v?
(y), offeatures extracted from the hypothesis fj(y) andmodel parameters ?j , i.e., v?
(y) =?j ?jfj(y).The hypothesis with the highest score is then cho-sen as the output.The model parameters, ?, are learned from atraining set by maximum entropy estimation of thefollowing conditional model:?s?yi?Ys:g(yi)=maxjg(yj)P?
(yi|Ys)Here, Ys = {yj} is the set of hypotheses for eachtraining utterance s and the function g returns anextrinsic evaluation score, which in our case isthe WER of the hypothesis.
P?
(yi|Ys) is modeledby a maximum entropy distribution of the form,P?
(yi|Ys) = exp v?
(yi)/?j exp v?(yj).
Thischoice simplifies the numerical estimation proce-dure since the gradient of the log-likelihood withrespect to a parameter, say ?j , reduces to differ-ence in expected counts of the associated feature,E?
[fj |Ys]?E?
[fj |yi ?
Ys : g(yi) = maxjg(yj)].To allow good generalization properties, a Gaus-sian regularization term is also included in the costfunction.A set of hypotheses Ys is generated for eachtraining utterance using a baseline ASR system.Care is taken to reduce the bias in decoding thetraining set by following a jack-knife procedure.The training set is divided into 20 subsets and eachsubset is decoded after excluding the transcriptsof that subset from the language model of the de-coder.The model allows the exploration of a large fea-ture space, including n-grams of words, morpho-logical tags, and factored tags.
In a large vocab-ulary system, this could be an enormous space.However, in a discriminative maximum entropyframework, only the observed features are consid-ered.
Among the observed features, those associ-ated with words that are correct in all hypothesesdo not provide any additional discrimination ca-pability.
Mathematically, the gradient of the log-likelihood with respect to the parameters of thesefeatures tends to zero and they may be discarded.Additionally, the parameters associated with fea-tures that are rarely observed in the training set aredifficult to learn reliably and may be discarded.To avoid redundant features, we focus on wordswhich are frequently incorrect; this is the error re-gion we aim to model.
In the training utterance,the error regions of a hypothesis are identified us-ing the alignment corresponding to the minimumedit distance from the reference, akin to comput-ing word error rate.
To mark all the error regions inan ASR lattice, the minimum edit distance align-ment is obtained using equivalent finite state ma-chine operations (Mohri, 2002).
From amongst allthe error regions in the training lattices, the mostfrequent 12k words in error are shortlisted.
Fea-tures are computed in the corrective model only ifthey involve words for the shortlist.
The parame-ters, ?, are estimated by numerical optimization asin (Charniak and Johnson, 2005).5 Feature SelectionThe space of features spanned by the cross-product space of words, lemmas, tags, factored-tags and their n-gram can potentially be over-whelming.
However, not all of these featuresare equally important and many of the featuresmay not have a significant impact on the worderror rate.
The maximum entropy framework af-fords the luxury of discarding such irrelevant fea-tures without much bookkeeping, unlike maxi-mum likelihood models.
In the context of mod-eling morphological features, we investigate theefficacy of simple feature selection based on the?2 statistics, which has been shown to effectivein certain text categorization problems.
e.g.
(Yangand Pedersen, 1997).The ?2 statistics measures the lack of indepen-dence by computing the deviation of the observedcounts Oi from the expected counts Ei.
?2 =?i(Oi ?
Ei)2/EiIn our case, there are two classes ?
oracle hy-potheses c and competing hypotheses c?.
Theexpected count is the count marginalized overclasses.
?2(f, c) =(P (f, c)?
P (f))2P (f)+(P (f, c?)?
P (f))2P (f)+(P (f?
, c)?
P (f?
))2P (f?
)+(P (f?
, c?)?
P (f?
))2P (f?
)394This can be simplified using a two-way contin-gency table of feature and class, where A is thenumber of times f and c co-occur, B is the num-ber of times f occurs without c, C is the numberof times c occurs without f , and D is the numberof times neither f nor c occurs, and N is the totalnumber of examples.
Then, the ?2 is defined tobe:?2(f, c) =N ?
(AD ?
CB)2(A+ C)?
(B +D)?
(A+B)?
(C +D)The ?2 statistics are computed for all the fea-tures and the features with larger value are re-tained.
Alternatives feature selection mechanismssuch as those based on mutual information and in-formation gain are less reliable than ?2 statisticsfor heavy-tailed distributions.
More complex fea-ture selection mechanism would entail computinghigher order interaction between features which iscomputationally expensive and so is not exploredin this work.6 Empirical EvaluationThe corrective model presented in this work isevaluated on a large vocabulary task consistingof spontaneous spoken testimonies in Czech lan-guage, which is a subset of the multilingualMALACH corpus (Psutka et al, 2003).6.1 TaskFor acoustic model training, transcripts are avail-able for about 62 hours of speech from 336 speak-ers, amounting to 507k spoken words from a vo-cabulary of 79k.
A portion of this data containingspeech from 44 speakers, about 21k words in allis treated as development set (dev).
The test set(eval) consists of about 2 hours of speech from 10new speakers and contains about 15k words.6.2 Baseline ASR SystemThe baseline ASR system uses perceptual linearprediction (PLP) features which is computed on44KHz input speech at the rate of 10 frames persecond, and is normalized to have zero mean andunit variance per speaker.
The acoustic models aremade of 3-state HMM triphones, whose observa-tion distributions are clustered into about 4500 al-lophonic (triphone) states.
Each state is modeledby a 16 component Gaussian mixture with diag-onal covariances.
The parameters of the acousticmodels are initially estimated by maximum likeli-hood and then refined by five iterations of maxi-mum mutual information estimation (MMI).Unlike other comparable corpora, this corpuscontains a relatively high percentage of colloquialwords ?
about 9% of the vocabulary and 7% of thetokens.
For the sake of downstream application,the colloquial variants are subsumed in the lexi-con.
As a result, common words contain severalpronunciation variants, and a few have as many as14 variants.For the first pass decoding, a language modelwas created by interpolating the in-domain model(weight=0.75), estimated from 600k words oftranscripts with an out-of-domain model, esti-mated from 15M words of Czech National Cor-pus (Psutka et al, 2003).
Both models are param-eterized by a trigram language model with Katzback-off.
The decoding graph was built by com-posing the language model, the lexical transducerand the context-dependent transducer (phones totriphones) into a single compact finite state ma-chine.The baseline ASR system decodes test utter-ance in two passes.
A first pass decoding is per-formed with MMIE acoustic models, whose out-put transcripts are bootstrapped to estimate twomaximum likelihood linear regression transformsfor each speaker using five iterations.
A secondpass decoding is then performed with the newspeaker adapted acoustic models.
The resultingperformance is given in Table 5.
The performancereflects the difficulty of transcribing spontaneousspeech from the elderly speakers whose speech isalso heavily accented and emotional in this corpus.1-best 1000-bestDev 29.9 21.5Eval 35.9 22.4Table 5: The performance of the baseline ASRsystem is reported, showing the word error rateof 1-best MAP hypothesis and the oracle in 1000-best hypotheses for dev and eval sets.6.3 Experiments With MorphologyWe present a set of contrastive experiments togauge the performance of the corrective modelsand the contribution of morphological features.For training the corrective models, 50 best hy-potheses are generated for each utterance using the39528.628.8 2929.229.429.629.8 3000.20.40.60.81WERFraction offeatures used?baseline?wordn-gram+ morph n-gram34.234.434.634.8 3535.235.435.635.8 3600.20.40.60.81WERFraction offeatures used?baseline?wordn-gram+ morph n-gram(a)Devel (b)EvalFigure 1: Feature selection via ?2 statistics helps reduce the number of parameters by 70% without anyloss in performance, as observed in dev (a) and eval (b) sets.jack-knife procedure mentioned earlier.
For eachhypothesis, bigram and unigram features are com-puted which consist of word-forms, lemmas, mor-phologoical tags, factored morphological tags, andthe likelihood from the baseline ASR system.
Fortesting, the baseline ASR system is used to gener-ate 1000 best hypotheses for each utterance.
Theseare then evaluated using the corrective models andthe best scored hypothesis is chosen as the output.Table 6 summarizes the results on two test sets?
the dev and the eval set.
A corrective model withword bigram features improve the word error rateby about an absolute 1% over the baseline.
Mor-phological features provide a further gain on boththe test sets consistently.Features Dev EvalBaseline 29.9 35.9Word bigram 29.0 34.8+ Morph bigram 28.7 34.4Table 6: The word error rate of the correctivemodel is compared with that of the baseline ASRsystem, illustrating the improvement in perfor-mance with morphological features.The gains on the dev set are significant at thelevel of p < 0.001 for three standard NIST tests,namely, matched pair sentence segment, signedpair comparison, and Wilcoxon signed rank tests.For the smaller eval set the significant levels werelower for morphological features.
The relativegains observed are consistent over a variety of con-ditions that we have tested including the ones re-ported below.Subsequently, we investigated the impact of re-ducing the number of features using ?2 statistics,as described in section 5.
The experiments withbigram features of word-forms and morphologywere repeated using reduced feature sets, and theperformance was measured at 10%, 30% and 60%of their original features.
The results, as illustratedin Figure 1, show that the word error rate does notchange significantly even after the number of fea-tures are reduced by 70%.
We have also observedthat most of the gain can be achieved by evalu-ating 200 best hypotheses from the baseline ASRsystem, which could further reduce the computa-tional cost for time-sensitive applications.6.4 Analysis of Feature ClassesThe impact of feature classes can be analyzed byexcluding all features from a particular class andevaluating the performance of the resulting modelwithout re-estimation.
Figure 2 illustrates the ef-fectiveness of different features class.
The y-axisshows the gain in F-score, which is monotonicwith the word error rate, on the entire develop-ment dataset.
In this analysis, the likelihood scorefrom the baseline ASR system was omitted sinceour interest is in understanding the effectivenessof categorical features such as words, lemmas andtags.The most independently influential feature classis the factored tag features.
This corresponds with396-0.00100.0010.0020.0030.0040.005TNG#1 TNG#2 LNG#2 FNG#2 TFAC#1 LNG#1 FNG#1 TFAC#2Figure 2: Analysis of features classes for a bigramform, lemma, tag, and factored tag model.
Y -axisis the contribution of this feature if added to anotherwise complete model.
Feature classes are la-beled: TNG ?
tag n-gram, LNG ?
lemma n-gram,FNG ?
form n-gram and TFAC ?
factored tag n-grams.
The number following the # represents theorder of the n-gram.our belief that modeling morphological featuresrequires detailed models of the morphology; inthis model the composite morphological tag n-gram features (TNG) offer little contribution in thepresence of the factored features.Analysis of feature reduction by the ?2 statisticsreveals a similar story.
When features are rankedaccording to their ?2 statistics, about 57% of thefactored tag n-grams occur in the top 10% whileonly 7% of the word n-grams make it.
The lemmaand composite tag n-grams give about 6.2% and19.2% respectively.
Once again, the factored tagis the most influential feature class.7 ConclusionWe have proposed a corrective modeling frame-work for incorporating inflectional morphologyinto a discriminative language model.
Empiricalresults on a difficult Czech speech recognition tasksupport our claim that morphology can help im-prove speech recognition results for these types oflanguages.
Additionally, we present a feature se-lection method that effectively reduces the modelsize by about 70% while having little or no im-pact on recognition accuracy.
Model size reduc-tion greatly reduces training time which can oftenbe prohibitively expensive for maximum entropytraining.Analysis of the models learned on our task showthat factored morphological tags along with word-forms provide most of the discriminative power;and, in the presence of these features, compositemorphological tags are of little use.The corrective model outlined here operates onthe word lattices produced by an ASR system.
Themorphological tags are inferred from the word se-quences in the lattice.
Alternatively, by employ-ing an ASR system that models the morphologicalconstraints in the acoustics as in (Chung and Sen-eff, 1999), the corrective model could be applieddirectly to a lattice with morphological tags.When dealing with ASR word lattices, the ef-ficacy of the proposed feature selection mecha-nism can be exploited to eliminate the intermedi-ate tagger, a potential source of errors.
Instead ofconsidering the best morphological analysis, themodel could consider all possible analyses of thewords.
Further, the feature space could be en-riched with syntactic features which are known tobe useful (Collins et al, 2005).
The task of mod-eling is then tackled by feature selection and themaximum entropy training procedure.8 AcknowledgementsThe authors would like to thank William Byrne fordiscussions on modeling aspects, and Jan Hajic?,Petr Ne?mec, and Vaclav Nova?k for discussionsregarding Czech morphology and tagging.
Thiswork was supported by the NSF (U.S.A) under theInformation Technology Research (ITR) program,NSF IIS Award No.
0122466.ReferencesKenan Carki, Petra Geutner, and Tanja Schultz.
2000.Turkish LVCSR: towards better speech recognitionfor agglutinative languages.
In Proceedings of the2000 IEEE International Conference on Acoustics,Speech, and Signal Processing, pages 3688?3691.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminativereranking.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguis-tics.Ciprian Chelba and Frederick Jelinek.
2000.
Struc-tured language modeling.
Computer Speech andLanguage, 14(4):283?332.Ghinwa Choueiter, Daniel Povey, Stanley Chen, andGeoffrey Zweig.
2006.
Morpheme-based languagemodeling for Arabic LVCSR.
In Proceedings of the2006 IEEE International Conference on Acoustics,Speech, and Signal Processing, Toulouse, France.Grace Chung and Stephanie Seneff.
1999.
A hierar-chical duration model for speech recognition based397on the ANGIE framework.
Speech Communication,27:113?134.Michael Collins, Brian Roark, and Murat Saraclar.2005.
Discriminative syntactic language modelingfor speech recognition.
In Proceedings of the 43rdAnnual Meeting of the Association for Computa-tional Linguistics (ACL?05), pages 507?514, AnnArbor, Michigan, June.
Association for Computa-tional Linguistics.Petra Geutner.
1995.
Using morphology towards bet-ter large-vocabulary speech recognition systems.
InProceedings of the 1995 IEEE International Confer-ence on Acoustics, Speech, and Signal Processing,pages 445?448, Detroit, MI.Jan Hajic?
and Barbora Vidova?-Hladka?.
1998.
Tagginginflective languages: Prediction of morphologicalcategories for a rich, structured tagset.
In Proceed-ings of the COLING-ACL Conference, pages 483?490, Montreal, Canada.Jan Hajic?, Eva Hajic?ova?, Petr Pajas, JarmilaPanevova?, Petr Sgall, and Barbora Vidova?
Hladka?.2005.
The prague dependency treebank 2.0.http://ufal.mff.cuni.cz/pdt2.0.Keith Hall and Mark Johnson.
2004.
Attention shiftingfor parsing speech.
In Proceedings of the 42nd An-nual Meeting of the Association for ComputationalLinguistics, pages 41?47, Barcelona.Mehryar Mohri.
2002.
Edit-distance of weightedautomata.
In Proceedings of the 7th Interna-tional Conference on Implementation and Applica-tion of Automata, Jean-Marc Champarnaud and De-nis Maurel, Eds.Petr Podvesky and Pavel Machek.
2005.
Speechrecognition of Czech?inclusion of rare wordshelps.
In Proceedings of the ACL Student ResearchWorkshop, pages 121?126, Ann Arbor, Michigan,June.
Association for Computational Linguistics.Josef Psutka, Pavel Ircing, Josef V. Psutka, VlastaRadovic, William Byrne, Jan Hajic?, Jiri Mirovsky,and Samuel Gustman.
2003.
Large vocabulary ASRfor spontaneous Czech in the MALACH project.In Proceedings of the 8th European Conference onSpeech Communication and Technology, Geneva,Switzerland.Roni Rosenfeld, Stanley F. Chen, and Xiaojin Zhu.2001.
Whole-sentence exponential language mod-els: a vehicle for linguistic-statistical integration.Computers Speech and Language, 15(1).Izhak Shafran and William Byrne.
2004.
Task-specificminimum Bayes-risk decoding using learned editdistance.
In Proceedings of the 7th InternationalConference on Spoken Language Processing, vol-ume 3, pages 1945?48, Jeju Islands, Korea.Dimitra Vergyri, Katrin Kirchhoff, Kevin Duh, and An-dreas Stolcke.
2004.
Morphology-based languagemodeling for arabic speech recognition.
In Proceed-ings of the International Conference on Spoken Lan-guage Processing (ICSLP/Interspeech 2004).Yiming Yang and Jan 0.
Pedersen.
1997.
A compara-tive study on feature selection in text categorization.In Proceedings of the 14th International Conferenceon Machine Learning, pages 412 ?
420, San Fran-cisco, CA, USA.398
