Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 572?581,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsOmni-word Feature and Soft Constraintfor Chinese Relation ExtractionYanping Chen?Qinghua Zheng?
?MOEKLINNS Lab, Department of Computer Science and TechnologyXi?an Jiaotong University, Chinaypench@gmail.com, qhzheng@mail.xjtu.edu.cn?Amazon.com, Inc.wzhan@amazon.comWei Zhang?AbstractChinese is an ancient hieroglyphic.
It is inat-tentive to structure.
Therefore, segmentingand parsing Chinese are more difficult and lessaccurate.
In this paper, we propose an Omni-word feature and a soft constraint method forChinese relation extraction.
The Omni-wordfeature uses every potential word in a sentenceas lexicon feature, reducing errors caused byword segmentation.
In order to utilize thestructure information of a relation instance, wediscuss how soft constraint can be used to cap-ture the local dependency.
Both Omni-wordfeature and soft constraint make a better useof sentence information and minimize the in-fluences caused by Chinese word segmenta-tion and parsing.
We test these methods onthe ACE 2005 RDC Chinese corpus.
The re-sults show a significant improvement in Chi-nese relation extraction, outperforming othermethods in F-score by 10% in 6 relation typesand 15% in 18 relation subtypes.1 IntroductionInformation Extraction (IE) aims at extractingsyntactic or semantic units with concrete conceptsor linguistic functions (Grishman, 2012; McCal-lum, 2005).
Instead of dealing with the whole doc-uments, focusing on designated information, mostof the IE systems extract named entities, relations,quantifiers or events from sentences.The relation recognition task is to find the rela-tionships between two entities.
Successful recog-nition of relation implies correctly detecting boththe relation arguments and relation type.
Althoughthis task has received extensive research.
The per-formance of relation extraction is still unsatisfac-tory with a F-score of 67.5% for English (23 sub-types) (Zhou et al, 2010).
Chinese relation extrac-tion also faces a weak performance having F-scoreabout 66.6% in 18 subtypes (Dandan et al, 2012).The difficulty of Chinese IE is that Chinesewords are written next to each other without de-limiter in between.
Lacking of orthographic wordmakes Chinese word segmentation difficult.
InChinese, a single sentence often has several seg-mentation paths leading to the segmentation ambi-guity problem (Liang, 1984).
The lack of delimiteralso causes the Out-of-Vocabulary problem (OOV,also known as new word detection) (Huang andZhao, 2007).
These problems are worsened by thefact that Chinese has a large number of charactersand words.
Currently, the state-of-the-art ChineseOOV recognition system has performance about75% in recall (Zhong et al, 2012).
The errorscaused by segmentation and OOV will accumulateand propagate to subsequent processing (e.g.
part-of-speech (POS) tagging or parsing).Therefore, the Chinese relation extraction ismore difficult.
According to our survey, com-pared to the same work in English, the Chinese re-lation extraction researches make less significantprogress.Based on the characteristics of Chinese, in thispaper, an Omni-word feature and a soft constraintmethod are proposed for Chinese relation extrac-tion.
We apply these approaches in a maximumentropy based system to extract relations from theACE 2005 corpus.
Experimental results show thatour method has made a significant improvement.The contributions of this paper include1.
Propose a novel Omni-word feature for Chi-nese relation extraction.
Unlike the tradi-tional segmentation based method, which is apartition of the sentence, the Omni-word fea-ture uses every potential word in a sentenceas lexicon feature.2.
Aiming at the Chinese inattentive structure,we utilize the soft constraint to capture thelocal dependency in a relation instance.
Fourconstraint conditions are proposed to gener-572ate combined features to capture the local de-pendency and maximize the classification de-termination.The rest of this paper is organized as follows.Section 2 introduces the related work.
The Omni-word feature and soft constrain are proposed inSection 3.
We give the experimental results in Sec-tion 3.2 and analyze the performance in Section 4.Conclusions are given in Section 5.2 Related WorkThere are two paradigms extracting the relation-ship between two entities: the Open Relation Ex-traction (ORE) and the Traditional Relation Ex-traction (TRE) (Banko et al, 2008).Based on massive and heterogeneous corpora,the ORE systems deal with millions or billionsof documents.
Even strict filtrations or constrainsare employed to filter the redundancy information,they often generate tens of thousands of relationsdynamically (Hoffmann et al, 2010).
The practi-cability of ORE systems depends on the adequate-ness of information in a big corpus (Brin, 1999).Most of the ORE systems utilize weak supervi-sion knowledge to guide the extracting process,such as: Databases (Craven and Kumlien, 1999),Wikipedia (Wu and Weld, 2007; Hoffmann et al,2010), Regular expression (Brin, 1999; Agichteinand Gravano, 2000), Ontology (Carlson et al,2010; Mohamed et al, 2011) or Knowledge Baseextracted automatically from Internet (Mintz et al,2009; Takamatsu et al, 2012).
However, wheniteratively coping with large heterogeneous data,the ORE systems suffer from the ?semantic drift?problem, caused by error accumulation (Curranet al, 2007).
Agichtein, Carlson and Fader etal.
(2010; 2011; 2000) propose syntactic and se-mantic constraints to prevent this deficiency.
Thesoft constraints, proposed in this paper, are com-bined features like these syntactic or semantic con-straints, which will be discussed in Section 3.2.The TRE paradigm takes hand-tagged ex-amples as input, extracting predefined relationtypes (Banko et al, 2008).
The TRE systemsuse techniques such as: Rules (Regulars, Pat-terns and Propositions) (Miller et al, 1998), Ker-nel method (Zhang et al, 2006b; Zelenko et al,2003), Belief network (Roth and Yih, 2002), Lin-ear programming (Roth and Yih, 2007), Maximumentropy (Kambhatla, 2004) or SVM (GuoDong etal., 2005).
Compared to the ORE systems, theTRE systems have a robust performance.
Disad-vantages of the TRE systems are that the manu-ally annotated corpus is required, which is time-consuming and costly in human labor.
And mi-grating between different applications is difficult.However, the TRE systems are evaluable and com-parable.
Different systems running on the samecorpus can be evaluated appropriately.In the field of Chinese relation extraction, Liuet al (2012) proposed a convolution tree ker-nel.
Combining with external semantic resources,a better performance was achieved.
Che etal.
(2005) introduced a feature based method,which utilized lexicon information around entitiesand was evaluated on Winnow and SVM classi-fiers.
Li and Zhang et al (2008; 2008) exploredthe position feature between two entities.
For eachtype of these relations, a SVM was trained andtested independently.
Based on Deep Belief Net-work, Chen et al (2010) proposed a model han-dling the high dimensional feature space.
In addi-tion, there are mixed models.
For example, Lin etal.
(2010) employed a model, combining both thefeature based and the tree kernel based methods.Despite the popularity of kernel based method,Huang et al (2008) experimented with differentkernel methods and inferred that simply migratingfrom English kernel methods can result in a badperformance in Chinese relation extraction.
Chenand Li et al (2008; 2010) also pointed out that,due to the inaccuracy of Chinese word segmenta-tion and parsing, the tree kernel based approachis inappropriate for Chinese relation extraction.The reason of the tree kernel based approach notachieve the same level of accuracy as that from En-glish may be that segmenting and parsing Chineseare more difficult and less accurate than process-ing English.In our research, we proposed an Omni-wordfeature and a soft constraint method.
Both ap-proaches are based on the Chinese characteristics.Therefore, better performance is expected.
In thefollowing, we introduce the feature construction,which discusses the proposed two approaches.3 Feature ConstructionIn this section, the employed candidate featuresare discussed.
And four constraint conditionsare proposed to transform the candidate featuresinto combined features.
The soft constraint is the573method to generate the combine features1.3.1 Candidate Feature SetIn the ACE corpus, an entity is an object or set ofobjects in the world.
An entity mention is a ref-erence to an entity.
The entity mention is anno-tated with its full extent and its head, referred to asthe extend mention and the head mention respec-tively.
The extent mention includes both the headand its modifiers.
Each relation has two entities asarguments: Arg-1 and Arg-2, referred to as E1 andE2.
A relation mention (or instance) is the embod-iment of a relation.
It is referred by the sentence(or clause) in which the relation is located in.
Inour work, we focus on the detection and recogni-tion of relation mention.Relation identification is handled as a classifi-cation problem.
Entity-related information (e.g.head noun, entity type, subtype, CLASS, LDC-TYPE, etc.)
are supposed to be known and pro-vided by the corpus.
In our experiment, the entitytype, subtype and the head noun are used.All the employed features are simply classi-fied into five categories: Entity Type and Subtype,Head Noun, Position Feature, POS Tag and Omni-word Feature.
The first four are widely used.
Thelast one is proposed in this paper and is discussedin detail.Entity Type and Subtype: In ACE 2005 RDCChinese corpus, there are 7 entity types (Person,Organization, GPE, Location, Facility, Weaponand Vehicle) and 44 subtypes (e.g.
Group, Gov-ernment, Continent, etc.
).Head Noun: The head noun (or head mention)of entity mention is manually annotated.
This fea-ture is useful and widely used.Position Feature: The position structure be-tween two entity mentions (extend mentions).
Be-cause the entity mentions can be nested, two en-tity mentions may have four coarse structures: ?E1is before E2?, ?E1 is after E2?, ?E1 nests in E2?and ?E2 nests in E1?, encoded as: ?E1_B_E2?,?E1_A_E2?, ?E1_N_E2?
and ?E2_N_E1?.POS Tag: In our model, we use only the ad-jacent entity POS tags, which lie in two sides ofthe entity mention.
These POS tags are labelledby the ICTCLAS package2.
The POS tags are notused independently.
It is encoded by combining1If without ambiguity, we also use the terminology of?soft constraint?
denoting features generated by the em-ployed constraint conditions.2http://ictclas.org/the POS tag with the adjacent entity mention in-formation.
For example ?E1_Right_n?
meansthat the right side of the first entity is a noun (?n?
).Omni-word Feature: The notion of ?word?in Chinese is vague and has never played a rolein the Chinese philological tradition (Sproat etal., 1996).
Some Chinese segmentation perfor-mance has been reported precision scores above95% (Peng et al, 2004; Xue, 2003; Zhang etal., 2003).
However, for the same sentence, evennative peoples in China often disagree on wordboundaries (Hoosain, 1992; Yan et al, 2010).Sproat et al (1996) has showed that there is a con-sistence of 75% on the segmentation among differ-ent native Chinese speakers.
The word-formationof Chinese also implies that the meanings of acompound word are made up, usually, by themeanings of words that contained in it (Hu andDu, 2012).
So, fragments of phrase are also infor-mative.Because high precision can be received by usingsimple lexical features (Kambhatla, 2004; Li et al,2008).
Making better use of such information isbeneficial.
In consideration of the Chinese char-acteristics, we use every potential word in a rela-tion mention as the lexical features.
For example,relation mention ??????????
(TaipeiDaan Forest Park) has a ?PART-WHOLE?
relationtype.
The traditional segmentation method maygenerate four lexical features {???
?, ???
?, ???
?, ????
}, which is a partition of the relationmention.
On the other hand, the Omni-word fea-ture denoting all the possible words in the relationmention may generate features as:{??
?, ??
?, ??
?, ??
?, ??
?, ??
?, ??
?, ???,???
?, ???
?, ???
?, ???
?, ??????,????????
}3Most of these features are nested or overlappedmutually.
So, the traditional character-based orword-based feature is only a subset of the Omni-word feature.
To extract the Omni-word feature,only a lexicon is required, then scan the sentenceto collect every word.Because the number of lexicon entry determinesthe dimension of the feature space, performanceof Omni-word feature is influenced by the lexiconbeing employed.
In this paper, we generate thelexicon by merging two lexicons.
The first lexicon3The generated Omni-word features dependent on the em-ployed lexicon.574is obtained by segmenting every relation instanceusing the ICTCLAS package, collecting very wordproduced by ICTCLAS.
Because the ICTCLASpackage was trained on annotated corpus contain-ing many meaningful lexicon entries.
We expectthis lexicon to improve the performance.
The sec-ond lexicon is the Lexicon Common Words in Con-temporary Chinese4.Despite the Omni-word can be seen as a sub-set of n-Gram feature.
It is not the same as then-Gram feature.
N-Gram features are more frag-mented.
In most of the instances, the n-Gram fea-tures have no semantic meanings attached to them,thus have varied distributions.
Furthermore, fora single Chinese word, occurrences of 4 charac-ters are frequent.
Even 7 or more characters arenot rare.
Because Chinese has plenty of char-acters5, when the corpus becoming larger, the n-Gram (n?4) method is difficult to be adopted.
Onthe other hand, the Omni-word can avoid theseproblems and take advantages of Chinese charac-teristics (the word-formation and the ambiguity ofword segmentation).3.2 Soft ConstraintThe structure information (or dependent informa-tion) of relation instance is critical for recognition.However, even in English, ?deeper?
analysis (e.g.logical syntactic relations or predicate-argumentstructure) may suffer from a worse performancecaused by inaccurate chunking or parsing.
Hence,the local dependency contexts around the rela-tion arguments are more helpful (Zhao and Gr-ishman, 2005).
Zhang et al (2006a) also showedthat Path-enclosed Tree (PT) achieves the best per-formance in the kernel based relation extraction.In this field, the tree kernel based method com-monly uses the parse tree to capture the struc-ture information (Zelenko et al, 2003; Culotta andSorensen, 2004).
On the other hand, the featurebased method usually uses the combined featureto capture such structure information (GuoDonget al, 2005; Kambhatla, 2004).In the open relation extraction domain, syntac-tic and semantic constraints are widely employedto prevent the ?semantic drift?
problem.
Such con-straints can also be seen as structural constraint.4Published by Ministry of Education of the People?s Re-public of China in 2008, containing 56,008 entries.5Currently, at least 13000 characters are used by na-tive Chinese people.
Modern Chinese Dictionary: http://www.cp.com.cn/Most of these constraints are hard constraints.
Anyrelation instance violating these constraints (or be-low a predefined threshold) will be abandoned.For example, Agichtein and Gravano (2000) gen-erates patterns according to a confidence threshold(?t).
Fader et al (2011) utilizes a confidence func-tion.
And Carlson et al (2010) filters candidateinstances and patterns using the number of timesthey co-occurs.Deleting of relation instances is acceptable foropen relation extraction because it always dealswith a big data set.
But it?s not suitable for tra-ditional relation extraction, and will result in alow recall.
Utilizing the notion of combined fea-ture (GuoDong et al, 2005; Kambhatla, 2004), wereplace the hard constraint by the soft constraint.Each soft constraint (combined feature) has a pa-rameter trained by the classifier indicating the dis-crimination ability it has.
No subjective or priorijudgement is adopted to delete any potential de-terminative constraint (except for the reason of di-mensionality reduction).Most of the researches make use of the com-bined feature, but rarely analyze the influence ofthe approaches we combine them.
In this paper,we use the soft constraint to model the local de-pendency.
It is a subset of the combined feature,generated by four constraint conditions: singleton,position sensitive, bin sensitive and semantic pair.
For every employed candidate feature, an appro-priate constraint condition is selected to combinethem with additional information to maximize theclassification determination.Singleton: A feature is employed as a single-ton feature when it is used without combining withany information.
In our experiments, only the po-sition feature is used as singleton feature.Position Sensitive: A position sensitive featurehas a label indicating which entity mention it de-pends on.
In our experiment, the Head noun andPOS Tag are utilized as position sensitive features,which has been introduced in Section 3.1.
For ex-ample, ???_E1?
means that the head noun ????
depend on the first entity mention.Semantic Pair: Semantic pair is generated bycombining two semantic units.
Two kinds ofsemantic pair are employed.
Those are gener-ated by combining two entity types or two en-tity subtypes into a semantic pair.
For example,?Person_Location?
denotes that the type ofthe first relation argument is a ?Person?
(entity575type) and the second is a ?Location?
(entity type).Semantic pair can capture both the semantic andstructure information in a relation mention.Bin Sensitive: In our study, Omni-word featureis not added as ?bag of words?.
To use the Omni-word feature, we segment each relation mentionby two entity mentions.
Together with the two en-tity mentions, we get five parts: ?FIRST?, ?MID-DLE?, ?END?, ?E1?
and ?E2?
(or less, if the twoentity mentions are nested).
Each part is takenas an independent bin.
A flag is used to distin-guish them.
For example, ??
?_Bin_F?, ???_Bin_E1?
and ???_Bin_E?
mean that thelexicon entry ????
appears in three bins: theFIRST bin, the first entity mention (E1) bin andthe END bin.
They will be used as three indepen-dent features.To sum up, among the five candidate featuresets, the position feature is used as a singleton fea-ture.
Both head noun and POS tag are positionsensitive.
Entity types and subtypes are employedas semantic pair.
Only Omni-word feature is binsensitive.
In the following experiments, focusingon Chinese relation extraction, we will analyze theperformance of candidate feature sets and studythe influence of the constraint conditions.sectionExperimentsIn this section, methodologies of the Omni-word feature and the soft constraint are tested.Then they are compared with the state-of-the-artmethods.3.3 Settings and ResultsWe use the ACE 2005 RDC Chinese corpus, whichwas collected from newswires, broadcasts and we-blogs, containing 633 documents with 6 major re-lation types and 18 subtypes.
There are 8,023 rela-tions and 9,317 relation mentions.
After deleting5 documents containing wrong annotations6, wekeep 9,244 relation mentions as positive instances.To get the negative instances, each document issegmented into sentences7.
Those sentences thatdo not contain any entity mention pair are deleted.For each of the remained sentences, we iterativelyextract every entity mention pair as the argumentsof relation instances for predicting.
For example,suppose a sentence has three entity mentions: A,B6DAVYZW {20041230.1024, 20050110.1403,20050111.1514, 20050127.1720, 20050201.1538}.7The five punctuations are used as sentence boundaries:Period (?
), Question mark (?
), Exclamatory mark (?
),Semicolon (?)
and Comma (?
).and C. Because the relation arguments are ordersensitive, six entity mention pairs can be gener-ated: [A,B], [A,C], [B,C], [B,A], [C,A] and [C,B].After discarding the entity mention pairs that wereused as positive instances, we generated 93,283negative relation instances labelled as ?OTHER?.Then, we have 7 relation types and 19 subtypes.A maximum entropy multi-class classifier istrained and tested on the generated relation in-stances.
We adopt the five-fold cross validationfor training and testing.
Because we are interestedin the 6 annotated major relation types and the 18subtypes, we average the results of five runs on the6 positive relation types (and 18 subtypes) as thefinal performance.
F-score is computed by2?
(Precision?Recall)Precision+RecallTo implement the maximum entropy model, thetoolkit provided by Le (2004) is employed.
Theiteration is set to 30.Five candidate feature sets are employed to gen-erate the combined features.
The entity type andsubtype, head noun, position feature are referredto as Fthp8.
The POS tags are referred to as Fpos.The Omni-word feature set is denoted by Fow.Table 1 gives the performance of our system onthe 6 types and 18 subtypes.
Note that, in this pa-per, bare numbers and numbers in the parenthesesrepresent the results of the 6 types and the 18 sub-types respectively.Table 1: Performance on Type (Subtype)Features P R FFthp61.51 48.85 54.46(52.92) (36.92) (43.49)Fow80.16 75.45 77.74(66.98) (54.85) (60.31)Fthp?
Fpos83.93 77.81 80.76(69.83) (61.63) (65.47)Fthp?
Fow92.40 88.37 90.34(81.94) (70.69) (75.90)Fthp?
Fpos?
Fow92.26 88.51 90.35(80.52) (70.96) (75.44)In Row 1, because Fthpare features directly ob-tained from annotated corpus, we take this per-8?thp?
is an acronym of ?type, head, position?.
Featuresin Fthpare the candidate features combined with the corre-sponding constraint conditions.
The following Fposand Foware the same.576formance as our referential performance.
In Row2, with only the Fowfeature, the F-score alreadyreaches 77.74% in 6 types and 60.31% in 18 sub-types.
The last row shows that adding the Fposal-most has no effect on the performance when boththe Fthpand Foware in use.
The results show thatFowis effective for Chinese relation extraction.The superiorities of Owni-word feature dependon three reasons.
First, the specificity of Chi-nese word-formation indicates that the subphrasesof Chinese word (or phrase) are also informative.Second, most of relation instances have limitedcontext.
The Owni-word feature, utilizing everypossible word in them, is a better way to capturemore information.
Third, the entity mentions aremanually annotated.
They can precisely segmentthe relation instance into corresponding bins.
Seg-mentation of bins bears the sentence structure in-formation.
Therefore, the Owni-word feature withbin information can make a better use of both thesyntactic information and the local dependency.3.4 ComparisonVarious systems were proposed for Chinese re-lation extraction.
We mainly focus on systemstrained and tested on the ACE corpus.
Table 2 liststhree systems.Table 2: Survey of Other SystemsSystem P R FChe et al (2005) 76.13 70.18 73.27Zhang et al (2011)80.71 62.48 70.43(77.75) (60.20) (67.86)Liu et al (2012)81.1 61.0 69.0(79.1) (57.5) (66.6)Che et al (2005) was implemented on the ACE2004 corpus, with 2/3 data for training and 1/3 fortesting.
The performance was reported on 7 re-lation types: 6 major relation types and the nonerelation (or negative instance).
Zhang et al (2011)was based on the ACE 2005 corpus with 75% datafor training and 25% for testing.
Performancesabout the 7 types and 19 subtypes were given.Both of them are feature based methods.
Liu etal.
(2012) is a kernel based method evaluated onthe ACE 2005 corpus.
The five-fold cross valida-tion was used and declared the performances on 6relation types and 18 subtypes.The data preprocessing makes differences fromour experiments to others.
In order to give a bet-ter comparison with the state-of-the-art methods,based on our experiment settings and data, we im-plement the two feature based methods proposedby Che et al (2005) and Zhang et al (2011) in Ta-ble 2.
The results are shown in Table 3.In Table 3, Ei (i ?
1, 2) represents entity men-tion.
?Order?
in Che et al (2005) denotes the posi-tion structure of entity mention pair.
Four types oforder are employed (the same as ours).
WordEi+?kand POSEi+?kare the words and POS of Ei, ?+?k?means that it is the kth word (of POS) after (+)or before (-) the corresponding entity mention.
Inthis paper, k = 1 and k = 2 were set.In Row 2, the ?Uni-Gram?
represents the Uni-gram features of internal and external charactersequences.
Internal character sequences are thefour entity extend and head mentions.
Five kindsof external character sequences are used: one In-Between character sequence between E1 and E2and four character sequences around E1 and E2 ina given window size w s. The w s is set to 4.
The?Bi-Gram?
is the 2-gram feature of internal andexternal character sequences.
Instead of the 4 po-sition structures, the 9 position structures are used.Please refer to Zhang et al (2011) for the detailsof these 9 position structures.In Table 3, it is shown that our system outper-forms other systems, in F-score, by 10% on 6 re-lation types and by 15% on 18 subtypes.For researchers who are interested in our work,the source code of our system and our imple-mentations of Che et al (2005) and Zhang etal.
(2011) are available at https://github.com/YPench/CRDC.4 DiscussionIn this section, we analyze the influences of em-ployed feature sets and constraint conditions onthe performances.Most papers in relation extraction try to aug-ment the number of employed features.
In our ex-periment, we found that this does not always guar-antee the best performance, despite the classifierbeing adopted is claimed to control these featuresindependently.
Because features may interact mu-tually in an indirect way, even with the same fea-ture set, different constraint conditions can havesignificant influences on the final performance.In Section 3, we introduced five candidate fea-ture sets.
Instead of using them as independentfeatures, we combined them with additional in-577Table 3: Comparing With the State-of-the-Art MethodsSystem Feature Set P R F(Che et al, 2005)Ei.Type, Ei.Subtype, Order, WordEi+?1,WordEi+?2, POSEi+?1, POSEi+?284.81 75.69 79.99(64.89) (52.99) (58.34)(Zhang et al, 2011)Ei.Type, Ei.Subtype, 9 Position Feature,Uni-Gram, Bi-Gram79.56 72.99 76.13(66.78) (54.56) (60.06)Ours Fthp?
Fpos?
Fow92.26 88.51 90.35(80.52) (70.96) (75.44)formation.
We proposed four constraint condi-tions to generate the soft constraint features.
InTable 4, the performances of candidate featuresare compared when different constraint conditionswas employed.In Column 3 of Table 4 (Constraint Condi-tion), (1), (2), (3), (4) and (5) stand for the referen-tial feature sets9in Table 1.
Symbol ?/?
means thatthe corresponding candidate features in the refer-ential feature set are substituted by the new con-straint condition.
Par in Column 4 is the num-ber of parameters in the trained maximum entropymodel, which indicate the model complexity.
I inColumn 5 is the influence on performance.
?-?
and?+?
mean that the performance is decreased or in-creased.The first observation is that the combined fea-tures are more powerful than used as singletons.Model parameters are increased by the combinedfeatures.
Increasing of parameters projects therelation extraction problem into a higher dimen-sional space, making the decision boundaries be-come more flexible.The named entities in the ACE corpus are alsoannotated with the CLASS and LDCTYPE labels.Zhou et al (2010) has shown that these labels canresult in a weaker performance.
Row 1, 2 and 3show that, no matter how they are used, the perfor-mances decrease obviously.
The reason of the per-formance degradation may be caused by the prob-lem of over-fitting or data sparseness.At most of the time, increase of model param-eters can result in a better performance.
Exceptin Row 8 and Row 11, when two head nounsof entity pair were combined as semantic pairand when POS tag were combined with the en-tity type, the performances are decreased.
Thereare 7356 head nouns in the training set.
Combin-ing two head nouns may increase the feature space9(1), (2), (3), (4) and (5) denote Fthp, Fow, Fthp?Fpos,Fthp?
Fowand Fthp?
Fpos?
Fowrespectively.by 7356?
(7356?
1).
Such a large feature spacemakes the occurrence of features close to a randomdistribution, leading to a worse data sparseness.In Row 4, 10 and 13, these features are used assingleton, the performance degrades considerably.This means that, the missing of sentence structureinformation on the employed features can lead toa bad performance.Row 9 and 12 show an interesting result.
Com-paring the reference set (5) with the reference set(3), the Head noun and adjacent entity POS tagget a better performance when used as singletons.These results reflect the interactions between dif-ferent features.
Discussion of this issue is be-yond this paper?s scope.
In this paper, for a betterdemonstration of the constraint condition, we stilluse the Position Sensitive as the default setting touse the Head noun and the adjacent entity POStag.Row 13 and 14 compare the Omni-word fea-ture (By-Omni-word) with the traditional seg-mentation based feature (By-Segmentation).
By-Segmentation denotes the traditional segmentationbased feature set generated by a segmentation tool,collecting every output of relation mention.
In thisplace, the ICTCLAS package is adopted too.Conventionally, if a sentence is perfectly seg-mented, By-Segmentation is straightforward andeffective.
But, our experiment shows different ob-servations.
Row 13 and 14 show that the Omni-word method outperforms the traditional method.Especially, when the bin information is used (Row15), the performance of Omni-word feature in-creases considerably.Row 14 shows that, compared with the tradi-tional method, the Omni-word feature improvesthe performance by about 8.79% in 6 relationtypes and 11.83% in 18 subtypes in F-core.
Suchimprovement may reside in the three reasons dis-cussed in Section 3.3.In short, from Table 4 we have seen that the en-578Table 4: Influence of Feature SetNo.
Feature Constraint Condition Par P R F I1entityCLASS andLDCTYPE(1)/as singleton21,112 60.29 42.82 50.07 -4.3921,910 (41.70) (25.18) (31.40) -12.092(1)/combined withpositional Info21,159 63.02 44.47 52.15 -2.3122,013 (41.61) (26.31) (32.24) -11.253 (1)/as semantic pair21,207 63.35 47.67 54.40 -0.0622,068 (42.98) (31.34) (36.25) -7.244Type,Subtypesemanticpair(1)/as singleton19,390 51.37 29.16 37.20 -17.26147,435 (32.8) (18.97) (24.06) -19.435(1)/combined withpositional info19,524 61.77 43.67 51.17 -3.2920,297 (41.13) (26.83) (32.47) -11.026 (5)/as singleton105,865 91.39 87.92 89.62 -0.73121,218 (79.32) (68.73) (73.65) -1.797head noun(3)/as singleton21,450 85.66 75.74 80.40 -0.3622,409 (64.38) (57.14) (60.55) -0.348 (3)/as semantic pair77,333 83.05 73.14 77.78 -2.5477,947 (59.70) (51.70) (55.41) -5.489 (5)/as singleton100,963 92.50 88.90 90.66 +0.31115,499 (82.63) (71.67) (76.76) +1.3210adjacententity POStag(3)/as singleton21,450 72.66 61.16 66.41 -13.9122,409 (62.42) (45.69) (52.76) -8.1311(3)/combined withentity type22,151 80.66 71.67 75.90 -4.4223,357 (63.41) (53.16) (57.83) -3.0612 (5)/as singleton106,931 92.50 88.66 90.54 +0.19121,194 (82.04) (71.36) (76.33) +0.8913Omni-wordfeature(2)/By-Segmentation assingleton36,916 67.19 60.12 63.46 -14.2841,652 (55.85) (44.50) (49.54) -10.7714(2)/By-Segmentationwith bins79,430 71.12 66.90 68.95 -8.7984,715 (54.76) (43.50) (48.48) -11.8315(2)/By-Omni-word assingleton47,428 69.67 63.77 66.59 -11.1557,702 (54.85) (48.84) (51.67) -8.6416 (5)/as singleton57,321 91.43 86.37 88.83 -1.5267,722 (76.43) (69.57) (72.84) -2.60tity type and subtype maximize the performancewhen used as semantic pair.
Head noun andadjacent entity POS tag are employed to com-bine with positional information.
Omni-word fea-ture with bins information can increase the perfor-mance considerably.
Our model (in Section 3.3)uses these settings.
This insures that the perfor-mances of the candidate features are optimized.5 ConclusionIn this paper, We proposed a novel Omni-wordfeature taking advantages of Chinese sub-phrases.We also introduced the soft constraint method forChinese relation recognition.
The soft constraintutilizes four constraint conditions to catch thestructure information in a relation instance.
Boththe Omni-word feature and soft constrain makebetter use of information a sentence has, and min-imize the deficiency caused by Chinese segmenta-tion and parsing.The size of the employed lexicon determines thedimension of the feature space.
The first impres-sion is that more lexicon entries result in morepower.
However, more lexicon entries also in-crease the computational complexity and bring innoises.
In our future work, we will study this issue.The notion of soft constraints can also be extendedto include more patterns, rules, regexes or syntac-579tic constraints that have been used for informationextraction.
The usability of these strategies is alsoleft for future work.AcknowledgmentsThe research was supported in part by NSF ofChina (91118005, 91218301, 61221063); 863Program of China (2012AA011003); CheungKong Scholar?s Program; Pillar Program ofNST (2012BAH16F02); Ministry of Educationof China Humanities and Social Sciences Project(12YJC880117); The Ministry of Education Inno-vation Research Team (IRT13035).ReferencesEugene Agichtein and Luis Gravano.
2000.
Snow-ball: Extracting relations from large plain-text col-lections.
In Proceedings of DL ?00, pages 85?94.ACM.Michele Banko, Oren Etzioni, and Turing Center.2008.
The tradeoffs between open and traditionalrelation extraction.
Proceedings of ACL-HLT ?08,pages 28?36.Sergey Brin.
1999.
Extracting patterns and relationsfrom the world wide web.
The World Wide Web andDatabases, pages 172?183.Andrew Carlson, Justin Betteridge, Richard C. Wang,Estevam R. Hruschka Jr, and Tom M. Mitchell.2010.
Coupled semi-supervised learning for infor-mation extraction.
In Proceedings of WSDM ?10,pages 101?110.Wanxiang Che, Ting Liu, and Sheng Li.
2005.
Auto-matic entity relation extraction.
Journal of ChineseInformation Processing, 19(2):1?6.Yu Chen, Wenjie Li, Yan Liu, Dequan Zheng, andTiejun Zhao.
2010.
Exploring deep belief networkfor chinese relation extraction.
In Proceedings ofCLP ?10, pages 28?29.Mark Craven and Johan Kumlien.
1999.
Constructingbiological knowledge bases by extracting informa-tion from text sources.
In Proceedings of ISMB ?99,pages 77?86.Aron Culotta and Jeffrey Sorensen.
2004.
Dependencytree kernels for relation extraction.
In Proceedingsof ACL ?04, page 423.James R. Curran, Tara Murphy, and Bernhard Scholz.2007.
Minimising semantic drift with mutual ex-clusion bootstrapping.
In Proceedings of PACLING?07, pages 172?180.Liu Dandan, Hu Yanan, and Qian Longhua.
2012.Exploiting lexical semantic resource for tree kernel-based chinese relation extraction.
Natural LanguageProcessing and Chinese Computing, pages 213?224.Anthony Fader, Stephen Soderland, and Oren Etzioni.2011.
Identifying relations for open informationextraction.
In Proceedings of EMNLP ?11, pages1535?1545.Ralph Grishman.
2012.
Information extraction: Capa-bilities and challenges.
Notes prepared for the.Zhou GuoDong, Su Jian, Zhang Jie, and Zhang Min.2005.
Exploring various knowledge in relation ex-traction.
In Proceedings of ACL ?05, pages 427?434.Raphael Hoffmann, Congle Zhang, and Daniel S.Weld.
2010.
Learning 5000 relational extractors.In Proceedings of ACL ?10, volume 10, pages 286?295.Rumjahn Hoosain.
1992.
Psychological reality of theword in chinese.
Advances in psychology, 90:111?130.He Hu and Xiaoyong Du.
2012.
Radical features forchinese text classification.
In Proceedings of FSKD?12, pages 720?724.Changning Huang and Hai Zhao.
2007.
Chinese wordsegmentation : A decade review.
Journal of ChineseInformation Processing, 21(3):8?19.Ruihong Huang, Le Sun, and Yuanyong Feng.
2008.Study of kernel-based methods for chinese relationextraction.
Information Retrieval Technology, pages598?604.Nanda Kambhatla.
2004.
Combining lexical, syntac-tic, and semantic features with maximum entropymodels for extracting relations.
In Proceedings ofACL-demo ?04, page 22.Zhang Le.
2004.
Maximum entropy modeling toolkitfor python and c++.
Natural Language ProcessingLab, Northeastern University, China.Wenjie Li, Peng Zhang, Furu Wei, Yuexian Hou, andQin Lu.
2008.
A novel feature-based approach tochinese entity relation extraction.
In Proceedings ofHLT-Short ?08, pages 89?92.Nanyuan Liang.
1984.
Written chinese word segmen-tation system-cdws.
Journal of Beijing Institute ofAeronautics and Astronautics, 4.Ruqi Lin, Jinxiu Chen, Xiaofang Yang, and HongleiXu.
2010.
Research on mixed model-based chineserelation extraction.
In Proceedings of ICCSIT ?10,volume 1, pages 687?691.Andrew McCallum.
2005.
Information extraction:Distilling structured data from unstructured text.Queue, 3(9):48?57.580Scott Miller, Michael Crystal, Heidi Fox, LanceRamshaw, Richard Schwartz, Rebecca Stone, andRalph Weischedel.
1998.
Algorithms that learn toextract information: Bbn: Tipster phase iii.
In Pro-ceedings of TIPSTER ?98, pages 75?89.Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-sky.
2009.
Distant supervision for relation extrac-tion without labeled data.
In Proceedings of ACL?09, pages 1003?1011.Thahir P Mohamed, Estevam R Hruschka Jr., andTom M Mitchell.
2011.
Discovering relations be-tween noun categories.
In Proceedings of EMNLP?11, pages 1447?1455.Fuchun Peng, Fangfang Feng, and Andrew McCallum.2004.
Chinese segmentation and new word detec-tion using conditional random fields.
In Proceedingsof COLING ?04.Dan Roth and Wen-tau Yih.
2002.
Probabilistic rea-soning for entity & relation recognition.
In Proceed-ings of COLING ?02, pages 1?7.Dan Roth and Wen-tau Yih.
2007.
Global inferencefor entity and relation identification via a linear pro-gramming formulation.
Introduction to StatisticalRelational Learning, pages 553?580.Richard Sproat, William Gale, Chilin Shih, and NancyChang.
1996.
A stochastic finite-state word-segmentation algorithm for chinese.
Computationallinguistics, 22(3):377?404.Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.2012.
Reducing wrong labels in distant supervisionfor relation extraction.
In Proceedings of ACL ?12,pages 721?729.Fei Wu and Daniel S. Weld.
2007.
Autonomously se-mantifying wikipedia.
In Proceedings of CIKM ?07,pages 41?50.Nianwen Xue.
2003.
Chinese word segmentation ascharacter tagging.
Computational Linguistics andChinese Language Processing, 8(1):29?48.Ming Yan, Reinhold Kliegl, Eike Richter, Antje Nuth-mann, and Hua Shu.
2010.
Flexible saccade-targetselection in chinese reading.
The Quarterly Journalof Experimental Psychology, 63(4):705?725.Dmitry Zelenko, Chinatsu Aone, and AnthonyRichardella.
2003.
Kernel methods for relationextraction.
The Journal of Machine Learning Re-search, 3:1083?1106.Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, and QunLiu.
2003.
Hhmm-based chinese lexical analyzerictclas.
In Proceedings of SIGHAN ?03, pages 184?187.Min Zhang, Jie Zhang, and Jian Su.
2006a.
Exploringsyntactic features for relation extraction using a con-volution tree kernel.
In Proceedings of HLT-NAACL?06, pages 288?295.Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou.2006b.
A composite kernel to extract relations be-tween entities with both flat and structured features.In Proceedings of ACL ?06, pages 825?832.Peng Zhang, Wenjie Li, Furu Wei, Qin Lu, and YuexianHou.
2008.
Exploiting the role of position feature inchinese relation extraction.
In Proceedings of LREC?08.Peng Zhang, Wenjie Li, Yuexian Hou, and Dawei Song.2011.
Developing position structure-based frame-work for chinese entity relation extraction.
ACMTransactions on Asian Language Information Pro-cessing (TALIP), 10(3):14.Shubin Zhao and Ralph Grishman.
2005.
Extractingrelations with integrated information using kernelmethods.
In Proceedings of ACL ?05, pages 419?426.Ming Zhong, Sheng Wang, and Ming Wu.
2012.
Re-vising word lattice using support vector machine forchinese word segmentation.
In Proceedings of II-WAS ?12, pages 352?355.Guodong Zhou, Longhua Qian, and Jianxi Fan.
2010.Tree kernel-based semantic relation extraction withrich syntactic and semantic information.
Informa-tion Sciences, 180(8):1313?1325.581
