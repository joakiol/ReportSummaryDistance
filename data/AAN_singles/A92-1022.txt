Evaluating Parsing Strategies Using Standardized Parse FilesRa lph  Gr i shman and Cather ine  Mac leod  andComputer  Sc ience Depar tmentNew York  Un ivers i ty715 Broadway,  7 th  F loorNew York ,  New York  10003John  Ster l ing*Abst rac tThe availability of large files of manually-reviewed parse trees from the University ofPennsylvania "tree bank", along with a pro-gram for comparing system-generated parsesagainst these "standard" parses, provides anew opportunity for evaluating different pars-ing strategies.
We discuss some of the restruc-turing required to the output of our parser sothat it could be meaningfully compared withthese standard parses.
We then describe severalheuristics for improving parsing accuracy andcoverage, such as closest attachment of mod-ifiers, statistical grammars, and fitted parses,and present a quantitative evaluation of the im-provements obtained with each strategy.1 The  Prob lemThe systematic improvement of parsing strategies re-quires some way of evaluating competing strategies.
Weneed to understand their effect on both overall systemperformance and the ability to parse sentences correctly.In order to evaluate parser output, we will need a file ofstandard "correct" parses and a mechanism for compar-ing parser output with this standard.
Preparing such afile is a quite time-consuming operation.
In addition, ifwe would like our comparison of strategies to be mean-ingful to other researchers and extendible to other sys-tems, we would like a standard and a metric which are,as much as possible, system independent.One resource which is newly available to meet thisneed is the "tree bank" at the University of Pennsyl-vania.
This bank includes a large number of parse treeswhich have been prepared manually following a fairly de-tailed standard.
At first glance the comparison of parsetrees among systems based on different theories wouldseem to be very difficult.
However, based on an ideaproposed by Ezra Black \[Black et al, 1991\], a group or-ganized by Phil Harrison has developed a metric anda program for performing just such a comparison.
Apreliminary assessment of this metric, based on a small"This paper is based upon work supported by the DefenseAdvanced Research Projects Agency under Grant N00014-90-J-1851 from the Office of Naval Research, and by the NationalScience Foundation under Grant IRI-89-02304.sample of short sentences (50 13-word sentences), hasbeen quite promising \[Harrison et al, 1991\].Our goal, in the work described here, was to extendthis effort to larger samples and more complex sentencesand to use the metric to compare various parsing heuris-tics which we have employed in our system.
We describebriefly in the next sections the nature of the task andthe text to which is was applied, the form of the treebank, and the comparison metric.
We consider some ofthe systematic mis-alignments between the standard andour output, and describe how we have reduced these.
Fi-nally, we describe some of our parsing strategies and seehow they affect both the parse scores and the overallsystem performance.2 The  App l i ca t ion  TaskThe experiments described below were performed on aversion of the PROTEUS system as prepared for MUC-3,the third Message Understanding Conference \[Sundheim,1991\].
The task set before the participants at these Con-ferences is one of information eztraction: taking free-textinput on a particular subject matter, extracting specifiedtypes of information, and filling a data base with thisinformation.
For MUC-3, the texts consisted of shortnews reports about terrorist incidents; most are in typ-ical newspaper style, with complex sentences, althoughtranscripts of speeches and radio broadcasts are also in-cluded.The PROTEUS system has five basic stages of process-ing: syntactic analysis, semantic analysis, reference res-olution, discourse analysis, and data base creation.
Syn-tactic analysis consists of parsing with a broad-coverageEnglish grammar and dictionary, followed by syntacticregularization.
The regularized parse is then given to thesemantic analyzer, which produces a predicate-argumentstructure.
The semantic analyzer also provides feedbackto the parser as to whether the parse satisfies semantic(selectional) constraints.
Thus correct data base creationis dependent on correct predicate-argument structures,which are in turn dependent on at least locally correctsyntactic structures.Because of the richness of the text, it is not possible toprovide a complete set of semantic patterns.
The seman-tic patterns included in the system are largely limited tothose relevant for filling data base entries.
For the terror-ist domain, this includes patterns for all sorts of terrorist156incidents (attacks, bombings, ...), for the entities whichcan be involved in these incidents (as perpetrators, tar-gets, instruments .
.
.
.
), and for other structures whichmay affect truth value or certainty (claim, deny, allege,...).
We use a system of preference semantics which pe-nalizes but does not exclude constructs not matchingany of these semantic patterns \[Grishman and Sterling,1990\].
Our parser does a best-first search for the analysiswith the lowest penalty.The information extraction task has the benefit of pro-viding relatively clear measures of overall system perfor-mance.
As part of MUC-3, a set of standard (correct)data base entries was prepared and a program was devel-oped for scoring system responses against his standard.Roughly speaking, the scoring program computes threecounts: Std, the number of data base fills in the standard(correct) data base; Sys, the number of fills generated bythe system; and Cor, the number of correct fills gener-ated by the system.
System performance is then statedin terms of recall (= Cor / Std) and precision (= Cor /Sys).3 The  Tree BankThe goal of the "Treebank" at the University of Pennsyl-vania is to construct a large data base of English anno-tated with detailed grammatical structure.
Among thetexts which they have annotated is a portion of the de-velopment corpus used for MUC-3; they have annotated356 of the 1300 articles in that corpus.Sentences are annotated in a semi-automated proce-dure in which sentences are first parsed automaticallyand these parses are then manually revised.
The result isa file of labeled bracketings conforming in general termsto the syntactic structures of transformational genera-tive grammar (X-bar theory).
The bracketing is shownin considerable detail at the clause level; some of the de-tails at the NP level are suppressed.
The guidelines forthe bracketing are given in \[Santorini, 1991\].
A samplesentence from the MUC-3 corpus, as bracketed for theTreebank, is shown in Figure 1.4 The  Eva luat ion  Met r i cThe basic idea for the evaluation metric was developedby Ezra Black.
It was then refined and tested at a work-shop (and through extensive subsequent electronic om-munication) organized by Phil Harrison.
This effort hasinvolved computational linguists from eight sites whohave applied this metric to compare - -  for a set of 50short sentences - -  the Penn standard with the "ideal"parses their systems would generate for these sentences.At first glance the parses produced by different sys-tems may seem so different as to be incomparable; nodelabels in particular may be entirely different.
The met-ric therefore liminates node labels and only comparestree structures.
Certain constituents are treated verydifferently by different systems; for example, auxiliariesare treated as main verbs by some systems but not oth-ers.
Several classes of constituents, uch as auxiliaries,pre-infinitival "to", "not", null elements, punctuation,( (S (PP in (NP car tagena) )(s (NP i t )Has(VP reported(SBAR(SBARthat(S (NP caste l la r )(VP faced(NP at?revolutionary trial(PP by(NP the ELN))) ) ) )and(SBAR that(S (NP he)was(VP (VP found(ADJP gu i l ty )  )and(VP executed) ) ) ) ) ) ) ).
)Figure 1: A sample sentence from the U. of PennsylvaniaTree Banketc., are therefore deleted before trees are compared.
Af-ter these eliminations, brackets with no elements inside,brackets around single words, and multiple brackets ur-rounding the same sequence of words are removed.
Fi-nally, these two bracketed sequences are compared.We count the number of brackets in the standard out-put (Std), the total number in the system output (Sys),and the number of matching brackets (M).
We then de-fine - -  just as we did for system performance - - measuresof recall (= i / Std) and precision (= M / Sys).
We alsocount the number of "crossings": the number of caseswhere a bracketed sequence from the standard overlapsa bracketed sequence from the system output, but nei-ther sequence is properly contained in the other.
Mostautomatic parsing systems generate considerably moredetailed bracketing than that produced by the Treebank;in such cases precision would be reduced, but the recalland crossing count would still accurately reflect the cor-rectness of the system-generated parses.Phil Harrison and Steven Abney have developed anddistributed a program which computes these variousmeasures.5 Improv ing  A l ignmentEven with all the parse tree simplifications described inthe previous section, there can still be substantial sys-tematic differences between the Treebank standard andsystem output.
The NYU system is a particularly good(bad?)
example in this regard, since it is based on Har-ris's Linguistic String Theory and the trees produced aretherefore quite different in several respects from X-bar157trees.
The result is a uniform degradation of recall andprecision.To reduce these differences, we have implemented asimple tree transducer to restructure the output of ourparser.
This is implemented as a set of tree rewritingrules which are applied at all levels of the parse tree.Our primary goal has been to reduce crossings and in-crease recall by producing trees which correspond moreclosely to the standard; secondarily, we have also aimedto increase precision by eliminating levels of bracketingin our trees.One of the principal differences is that our gram-mar divides the sentence (ASSERTION) into SUBJECT,VERB, and OBJECT, while the standard ivides it intoNP (subject) and VP (verb + object).
We therefore in-clude a rule to insert a VP node into our trees:((ASSERTION (SA .
?sal)(SUBJECT .
?sub j )(Sh .
?sa2)(VERB.
?verb)(SA .
?sa3)(OBJECT .
?ob j )(Sh .
?sa4))->(s (SA .
?sa l )(S (SUBJECT .
?sub j )(SA .
?sa2)(VP (VERB .
?verb)(SA .
?sa3)(OBJECT .
?ob j ) )(SA .
?sa4) ) ) )This rule is complicated by the presence of SAs (sentenceadjuncts), which may be empty.
Depending on theirposition, they are placed in the restructured tree beneathVP, beneath S, or as a sister of the main S.Differences in basic constituent structure have furtherramifications for the scoping of coordinate conjunction.For example, "The terrorists attacked the village andmurdered the mayor."
will be analyzed as NP VP andVP in the standard, while the PROTEUS system will ana-lyze it as ASSERTION and ASSERTION, with the SUB-JECT  of the second ASSERTION empty.
We thereforeinclude a rule to restructure this as VP and VP.Rewrite rules are also required to handle differences intile treatment of right modifiers of the noun.
We have auniform structure for the NP, left modifiers ?
head noun+ right modifiers.
The standard, in contrast, uses rulesof the form NP ---* NP VP, so that a verbal right modifieris treated as a right sister of the structure consisting ofthe head and left modifiers.
Finally, since the standardemploys only minimal bracketing of left modifiers of thenoun, we have included rules to eliminate most of thatstructure from our trees.
All told, we currently have 26rewriting rules.Our set of restructuring rules is not complete.
Morebrackets would need to be eliminated to match the levelof bracketing used in the standard.
Also, we treat certainphrases (such as "according to", "because of", and "asa consequence of") as idioms, and so bracket them asa single unit, whereas they are assigned a full phrasestructure in the standard.
To gauge how closely ourrestructured trees match the standard, we have takena set of 13 sentences which appear to parse correctlywith our system and scored them against he standard.Without the restructuring rules we obtained an averagerecall of 85.38%, an average precision of 76.55%, andan average of 0.15 crossings per sentence.
Applying therestructuring rules improved these scores to an averagerecall of 94.62%, an average precision of 92.48?~, and anaverage of 0.08 crossings per sentence.6 Eva luat ionFor an evaluation of our different parsing heuristics, wetook the first 33 of the 356 news reports from the MUC-3 corpus which had been bracketed by the Universityof Pennsylvania.
Three of these were deleted becauseof differences in the way the sentence boundaries wereassigned by our system and UPenn.
This left 30 reportswith 317 sentences for the evaluation runs.Our syntactic analyzer uses an augmented-context-free grammar: a context-free core plus a set of con-straints.
Some of these are stated as absolute con-straints: if the constraint is violated, the analysis is re-jected.
Others are stated as preferences: associated witheach of these constraints is a penalty to be assessed ifthe constraint is violated.
Penalties from different con-straints are combined additively, and the analysis withthe least penalty is selected as the final parse.
The parseruses a best-first search in which, at each point, the hy-pothesis with the lowest penalty is pursued.
Amongthe constraints which are realized as preferences in thesystem are: some grammatical constraints (includingadverb position, count noun, and comma constraints),closest attachment of modifiers, statistical preference forproductions, and all semantic onstraints.6.1 The  base runAs a "base run", we excluded almost all preferential con-straints.
We left in penalties on two constructs which,while grammatical, ead to substantial extra computa-tion - -  headless noun phrases and relative clauses with-out relative pronouns.
We also left in the ability to re-lax (with penalty) a few grammatical constraints: con-straints on adverb position, the constraint on countnouns requiring determiners, and the constraints oncommas.
Most sentences do not involve either of theseconstructs or violate any of these constraints, and so donot incur any penalty.
In fact, most sentences get lotsof parses under these conditions: on the average we got60 parses per sentence (these sentences are an average of22.5 words long).Since the parsing algorithm requires in the worst caseexponential time, we place some bound on the number ofhypotheses (edges in the chart) which may be generatedfor each sentence; for the current evaluation, we used alimit of 33000.
For some sentences, all hypotheses canbe explored within this limit; for the others, parsing isterminated when the edge limit is reached.
Each set ofsentences may be further divided into those which obtainone or more parses and those which do not obtain any.Adding heuristics in the form of additional preferencesmay reduce the search space, so that some sentences158which previously hit the edge limit without getting aparse will now get a parse.
These preferences can thusimprove recall and precision in two distinct ways: byselecting among parses produced by the base run in bet-ter than random fashion, and by shifting some sentencesfrom the "no parse (edge limit)" column to the "parsed"column.
In order to separate these two effects, we alsocompute the averages of crossings, recall, and precisionover the subset of 206 sentences which parsed in the baserun and did not reach the edge limit.
Improvements inthese numbers will show the value of a particular prefer-ence as a filter in selecting among parses, separate fromits value in guiding the parsing process.For the base run on the subset of 206 sentences, wecomputed two sets of statistics.
For the first set, whena sentence has N > 1 parses, we weight each parse by1/N in computing the average number of crossings andthe total recall and precision.
This is in some sense afair evaluation of how well we would do if we were forcedto pick a single parse for each sentence and we did soat random.
The second set of statistics is computed byselecting (from the parses for one sentence) the parsewith the minimal number of crossings and, among thesethe parse with the highest recall.
This represents howwell we could do - -  given the limitations of our basegrammar - -  if we had an ideal filter to select among theparses generated.6.2 Pars ing  Heur ist icsWe describe below briefly the various heuristics used inour experiments.
The results computed over the entirecorpus are summarized in Table 1; results over the 206sentences which parsed in the base run without hittingthe edge limit are summarized in Table 2.
For sentenceswith N > 1 parses, we weight each parse by 1/N in com-puting the average number of crossings and total recalland precision (as we did for the first set of statistics forthe base run, described in the previous paragraph).
Inaddition, we have included in Table 2, line lm, the sec-ond set of statistics for the base run, based on pickingthe best parse for each sentence.6.2.1 Closest a t tachmentThe simplest preference we tested was closest attach-ment of modifiers.
We penalized each modifier by thenumber of words separating the modifier from the wordit modifies, with some additional penalty for successivemodifiers at the same level of the tree.
This producedan improvement in performance on all measures (line 2in both tables).6.2.2 Stat ist ica l  g rammarUsing a sample of 260 sentence from the MUC-3 cor-pus (disjoint from the set used here for evaluation),we computed the probability of each production inour context-free grammar using an iterative unsuper-vised training scheme similar to the inside-outside al-gorithm \[Fujisaki, 1984; Chitrao and Grishman, 1990;Chitrao, 1990\].
We then used the logarithms of the prob-abilities as penalties in applying the productions duringour analysis of the evaluation corpus.
We used these sta-tistical weights by themselves and in combination withclosest attachment.
Note that statistical weighting byitself is not particularly effective in focusing the search,since longer hypotheses almost invariably have higherpenalties than shorter ones; as we have previously re-ported, its effectiveness i  greatly increased when com-bined with a weighting scheme which prefers longer hy-potheses.
Statistical weighting by itself produced an im-provement on all measures (line 3).
Furthermore, thecombination of statistical weighting and closest attach-ment (line 4) did better than either heuristic separately,except for a slight loss in average recall over the entirecorpus.6.2.3 Merg ingIn our base parser, alternative hypotheses are nevermerged.
If X is a non-terminal symbol of our grammar,we may generate several separate dges in the chart rep-resenting X spanning words wl to w2, each representinga different analysis.
When merging is enabled, we re-tain only the highest-scoring analysis, so there will beonly one edge for each non-terminal nd span of words.
1Merging, when added to the statistical grammar andclosest attachment, had little effect on the performanceof sentences which parsed in the base run (Table 2, line5).
However, it did substantially increase the number ofsentences parsed (by reducing the number of hypothesesto be followed, it allowed sentences which had previouslyhit the edge limit to complete), and therefore increasedthe average recall over the entire corpus (and the numberof crossings).6.2.4 JunkAlthough our grammar has moderately broad cover-age, there are certainly still a fair number of grammaticalstructures which have not been included.
In addition,there are frequent passages not conformiaag to "standardgrammar", particularly in the reports containing verba-tim transcripts of speeches.
Recognizing that most ofthese sentences still include long grammatical sequences,we have taken two different measures to attempt o an-alyze them.Our first measure involved a small modification to ourgrammar.
We introduced a new non-terminal, JUNK.JUNK was permitted to occur wherever a sentence mod-ifier, a post-nominal modifier, or a pre-nominal adjec-tive could occur.
With a fixed penalty, JUNK wouldmatch a single word or any sequence of words delimitedby dashes, parentheses, or brackets.
With an additionalpenalty per word, it could match any sequence of twoor more words.
This, roughly speaking, should find thegrammatical analysis skipping the minimum number ofwords.Adding JUNK has a slight effect on the set of sentenceswhich parsed in the base run.
~ However, it substantiallyincreased the number of sentences parsed and thus the1There are a few exceptions to this merging; in particu-lar, we do not merge different complement s ructures becausethere are subcategorization constraints which check for a spe-cific complement.2Only because the mix of penalties caused one sentencewhich had previously parsed to hit the edge limit.159parse strategy # parsed1.
base run 2272. close 2283. stat 2214. stat+close 2215. stat+close+mer~;e6.
stat+close+junk7.
stat+close+junk+mer~e8.
stat+close+junk+merge+fitavg C avg recall2.07 45.731.71 49.901.47 47.771.30 49.51238 1.79 56.08252 1.53292 2.27315 3.18avg precision65.8970.8573.4175.4973.2853.63 74.3065.45 71.5570.60 67.70Table 1: Parsing evaluation over entire corpus, showing number of sentences obtaining one or more parses, averagenumber of crossings per sentence, and average recall and precision.parse strate~;y avg C1.
base run 2.43lm.
base run - best parses 0.992. close 1.933. stat 1.864. stat+close 1.635. stat+close+mer~;e 1.546. stat+close+junk 1.637. stat+close+junk+rner6e 1.628. stat+close+junk+merge+fit 1.62Table 2: Parsing evaluation over 206 sentences, showing averageand precision.ave; recall70.28ave; precision67.0885.31 81.2276.51 72.2775.96 73.5978.89 75.7276.65 76.3478.32 75.7678.89 75.9878.89 75.98number of crossings per sentence, and average recallparse strategy1.
base runsystem recall5.
stat+close+mer~e40system precision592.
close 43 613. stat 43 614. stat+close 42 6145 624446476. stat+close+junk7.
stat+close+junk+mer~;e8.
stat+close+junk+mer~;e+fit616060Table 3: System performance in template filling task.160overall recall (Table 1, line 6 vs. line 4).
CombiningJUNK and merging produced even more dramatic im-provements (line 7).6.2.5 F i t ted  parseThe second measure we introduced to process sen-tences which still did not parse was the fitted parse.
Thebasic idea of a fitted parse is to "cover" the sentenceusing constituents of a few types \[Jensen et al, 1983\].In our implementation, we looked first for the longestinitial S and then repeatedly for the longest S or NP inthe remaining words.
This managed to get some sort of"parse" for all but two sentences in the corpus.
Includ-ing these in the scoring produced a further jump in totalaverage recall, to over 70%, but - -  not surprisingly - -with a loss of precision and an increase in crossings.6.2.6 OverviewWhile all the heuristics do appreciably better than thebase run, we can see that, except for the base run, therecall for the subset of 206 sentences (Table 2) varieslittle (less than 3%).
Most of the gain in recall, measuredover the entire corpus (Table 1), therefore reflects theability to analyze a larger number of sentences (withouta substantial degradation i parsing quality).6.3 Sys tem per fo rmanceTable 3 shows the performance of the entire extractionsystem for the same 8 runs.
For each run, the total recalland precision (in terms of slots filled in the templates) istabulated.
The precision is fairly constant; the systemrecall correlates very roughly with the average parse treerecall measured over the entire corpus.
As we have justnoted, the average parse recall is in turn (except for thebase run) closely correlated to the total number of sen-tences parsed.7 ConclusionThe availability of a substantial file of parses conformingto a relatively well-specified standard affords a numberof opportunities for interesting computational linguisticresearch.
One such opportunity is the evaluation of bothgrammars and parsing strategies, uch as we have shownin this paper.
We believe that such evaluation will becrucial to the further development of efficient, broad-coverage syntactic analyzers.
By restructuring the gen-erated parse trees to conform to those of the standard,and using agreed-upon comparison metrics, it should bepossible to make meaningful comparisons across ystems,even those using very different strategies.We intend to apply our evaluation methods to addi-tional parsing heuristics in the near future.
In particular,we noted earlier that selectional constraints are imple-mented as penalties in our system (preference seman-tics), and these evaluation methods can assist us in ob-taining a proper balance of syntactic and semantic penal-ties.By extending our approach, it should be possible toproduce more detailed diagnostic information about theparser output.
For example, by stripping away some ofthe structure from both the standard and the system-generated parses, it would be possible to focus on thesystem's performance on NP bracketing or on S bracket-ing.
Such an approach should also allow for meaningfulcomparisons with some systems (e.g., some fast deter-ministic parsers) which generate only more local struc-tures and postpone attachment decisions.Re ferences\[Black el al., 1991\] Ezra Black, Steven Abney, DanFlickenger, Claudia Gdaniec, Ralph Grishman, PhilipHarrison, Donald Hindle, Robert Ingria, Fred Jelinek,Judith Klavans, Mark Liherman, Mitch Marcus, SalimRoukos, Beatrice Santorini, and Tomek Strzalkowski.A procedure for quantitatively comparing the syntac-tic coverage of English.
In Proceedings of the Speechand Natural Language Workshop, pages 306-311, Pa-cific Grove, CA, February 1991.
Morgan Kaufmann.\[Chitrao and Grishman, 1990\] MaheshChitrao and Ralph Grishman.
Statistical parsing ofmessages.
In Proceedings of the Speech and NaturalLanguage Workshop, pages 263-266, Hidden Valley,PA, June 1990.
Morgan Kaufmann.\[Chitrao, 1990\] Mahesh Chitrao.
Statistical Techniquesfor Parsing Messages.
PhD thesis, New York Uni-versity, 1990.
Published as Proteus Project Memo-randum No.
38, Computer Science Department, NewYork University.\[Fujisaki, 1984\] Tetsunosuke Fujisaki.
A stochastic ap-proach to sentence parsing.
In Proc.
lOth Int'l Conf.Computational Linguisitics and 22nd Annl.
MeetingAssn.
Computational Linguistics, 1984.\[Grishman and Sterling, 1990\] Ralph Grishman andJohn Sterling.
Information extraction and semanticconstraints.
In Proc.
13th Int'l Conf.
ComputationalLinguistics (COLING 90), 1990.\[Harrison et al, 1991\] Philip Harrison, Steven Abney,Ezra Black, Dan Flickinger, Claudia Gdaniec, RalphGrishman, Donald Hindle, Robert Ingria, Mitch Mar-cus, Beatrice Santorini, and Tomek Strzalkowski.Evaluating syntax performance of parser/grammars.In Proceedings of the Natural Language ProcessingSystems Evaluation Workshop, Berkeley, CA, June1991.
To be published as a Rome Laboratory Techni-cal Report.\[Jensen et al, 1983\] K. Jensen, G. E. Heidorn, L. A.Miller, and Y. Ravin.
Parse fitting and prose fixing:getting a hold on ill-formedness.
Am.
J. Computa-tional Linguistics, 9(3-4):147-160, 1983.\[Santorini, 1991\] Beatrice Santorini.
Bracketing uide-lines for the penn treebank project.
Department ofComputer Science, University of Pennsylvania, May1991.\[Sundheim, 1991\] Beth Sundheim.
Third message un-derstanding evaluation and conference (MUC-3):Phase 1 status report.
In Proceedings of the Speechand Natural Language Workshop, pages 301-305, Pa-cific Grove, CA, February 1991.
Morgan Kaufmann.161
