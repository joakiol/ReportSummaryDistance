Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 462?466,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsLeveraging Domain-Independent Information in Semantic ParsingDan GoldwasserUniversity of MarylandCollege Park, MD 20740goldwas1@umiacs.umd.eduDan RothUniversity of IllinoisUrbana, IL 61801danr@illinois.eduAbstractSemantic parsing is a domain-dependentprocess by nature, as its output is definedover a set of domain symbols.
Motivatedby the observation that interpretation canbe decomposed into domain-dependentand independent components, we suggesta novel interpretation model, which aug-ments a domain dependent model with ab-stract information that can be shared bymultiple domains.
Our experiments showthat this type of information is useful andcan reduce the annotation effort signifi-cantly when moving between domains.1 IntroductionNatural Language (NL) understanding can be intu-itively understood as a general capacity, mappingwords to entities and their relationships.
However,current work on automated NL understanding(typically referenced as semantic parsing (Zettle-moyer and Collins, 2005; Wong and Mooney,2007; Chen and Mooney, 2008; Kwiatkowski etal., 2010; Bo?rschinger et al, 2011)) is restrictedto a given output domain1 (or task) consisting of aclosed set of meaning representation symbols, de-scribing domains such as robotic soccer, databasequeries and flight ordering systems.In this work, we take a first step towards con-structing a semantic interpreter that can leverageinformation from multiple tasks.
This is not astraight forward objective ?
the domain specificnature of semantic interpretation, as described inthe current literature, does not allow for an easymove between domains.
For example, a sys-tem trained for the task of understanding databasequeries will not be of any use when it will be givena sentence describing robotic soccer instructions.In order to understand this difficulty, a closerlook at semantic parsing is required.
Given a sen-tence, the interpretation process breaks it into a1The term domain is overloaded in NLP; in this work weuse it to refer to the set of output symbols.set of interdependent decisions, which rely on anunderlying representation mapping words to sym-bols and syntactic patterns into compositional de-cisions.
This representation takes into account do-main specific information (e.g., a lexicon mappingphrases to a domain predicate) and is therefore oflittle use when moving to a different domain.In this work, we attempt to develop a domain in-dependent approach to semantic parsing.
We do itby developing a layer of representation that is ap-plicable to multiple domains.
Specifically, we addan intermediate layer capturing shallow semanticrelations between the input sentence constituents.Unlike semantic parsing which maps the input toa closed set of symbols, this layer can be used toidentify general predicate-argument structures inthe input sentence.The following example demon-strates the key idea behind our representation ?two sentences from two different domains have asimilar intermediate structure.Example 1.
Domains with similar intermediate structures?
The [Pink goalie]ARG [kicks]PRED to [Pink11]ARGpass(pink1, pink11)?
[She]ARG [walks]PRED to the [kitchen]ARGgo(sister, kitchen)In this case, the constituents of the firstsentence (from the Robocup domain (Chenand Mooney, 2008)), are assigned domain-independent predicate-argument labels (e.g., theword corresponding to a logical function is identi-fied as a PRED).
Note that it does not use any do-main specific information, for example, the PREDlabel assigned to the word ?kicks?
indicates thatthis word is the predicate of the sentence, not aspecific domain predicate (e.g., pass(?)).
The in-termediate layer can be reused across domains.The logical output associated with the second sen-tence is taken from a different domain, using a dif-ferent set of output symbols, however it shares thesame predicate-argument structure.Despite the idealized example, in practice,462leveraging this information is challenging, as thelogical structure is assumed to only weakly corre-spond to the domain-independent structure, a cor-respondence which may change in different do-mains.
The mismatch between the domain in-dependent (linguistic) structure and logical struc-tures typically stems from technical considera-tions, as the domain logical language is designedaccording to an application-specific logic and notaccording to linguistic considerations.
This situa-tion is depicted in the following example, in whichone of the domain-independent labels is omitted.?
The [Pink goalie]ARG [kicks]PRED the [ball]ARG to [Pink11]ARGpass(pink1, pink11)In order to overcome this difficulty, we suggesta flexible model that is able to leverage the super-vision provided in one domain to learn an abstractintermediate layer, and show empirically that itlearns a robust model, improving results signifi-cantly in a second domain.2 Semantic Interpretation ModelOur model consists of both domain-dependent(mapping between text and a closed set of sym-bols) and domain independent (abstract predicate-argument structures) information.
We formulatethe joint interpretation process as a structured pre-diction problem, mapping a NL input sentence (x),to its highest ranking interpretation and abstractstructure (y).
The decision is quantified using alinear objective, which uses a vector w, mappingfeatures to weights and a feature function ?
whichmaps the output decision to a feature vector.
Theoutput interpretation y is described using a sub-set of first order logic, consisting of typed con-stants (e.g., robotic soccer player), functions cap-turing relations between entities, and their prop-erties (e.g., pass(x, y), where pass is a functionsymbol and x, y are typed arguments).
We usedata taken from two grounded domains, describingrobotic soccer events and household situations.We begin by formulating the domain-specificprocess.
We follow (Goldwasser et al, 2011;Clarke et al, 2010) and formalize semantic infer-ence as an Integer Linear Program (ILP).
Due tospace consideration, we provide a brief descrip-tion (see (Clarke et al, 2010) for more details).We then proceed to augment this model withdomain-independent information, and connect thetwo models by constraining the ILP model.2.1 Domain-Dependent ModelInterpretation is composed of several decisions,capturing mapping of input tokens to logical frag-ments (first order) and their composition intolarger fragments (second).
We encode a first-orderdecision as ?cs, a binary variable indicating thatconstituent c is aligned with the logical symbol s.A second-order decision ?cs,dt, is encoded as a bi-nary variable indicating that the symbol t (associ-ated with constituent d) is an argument of a func-tion s (associated with constituent c).
The overallinference problem (Eq.
1) is as follows:Fw(x) = arg max?,?
?c?x?s?D ?cs ?wT?1(x, c, s)+?c,d?x?s,t?D ?cs,dt ?wT?2(x, c, s, d, t) (1)We restrict the possible assignments to the deci-sion variables, forcing the resulting output formulato be syntactically legal, for example by restrict-ing active ?-variables to be type consistent, andforcing the resulting functional composition to beacyclic and fully connected (we refer the reader to(Clarke et al, 2010) for more details).
We take ad-vantage of the flexible ILP framework and encodethese restrictions as global constraints.Features We use two types of feature, first-order?1 and second-order ?2.
?1 depends on lexicalinformation: each mapping of a lexical item c to adomain symbol s generates a feature.
In additioneach combination of a lexical item c and an sym-bol type generates a feature.
?2 captures a pair of symbols and their alignmentto lexical items.
Given a second-order decision?cs,dt, a feature is generated considering the nor-malized distance between the head words in theconstituents c and d. Another feature is gener-ated for every composition of symbols (ignoringthe alignment to the text).2.2 Domain-Independent InformationWe enhance the decision process with informa-tion that abstracts over the attributes of specificdomains by adding an intermediate layer consist-ing of the predicate-argument structure of the sen-tence.
Consider the mappings described in Exam-ple 1.
Instead of relying on the mapping betweenPink goalie and pink1, this model tries to iden-tify an ARG using different means.
For example, thefact that it is preceded by a determiner, or capital-ized provide useful cues.
We do not assume anylanguage specific knowledge and use features thathelp capture these cues.463This information is used to assist the overalllearning process.
We assume that these labels cor-respond to a binding to some logical symbol, andencode it as a constraint forcing the relations be-tween the two models.
Moreover, since learningthis layer is a by-product of the learning process(as it does not use any labeled data) forcing theconnection between the decisions is the mecha-nism that drives learning this model.Our domain-independent layer bears somesimilarity to other semantic tasks, most no-tably Semantic-Role Labeling (SRL) introducedin (Gildea and Jurafsky, 2002), in which identi-fying the predicate-argument structure is consid-ered a preprocessing step, prior to assigning ar-gument labels.
Unlike SRL, which aims to iden-tify linguistic structures alone, in our frameworkthese structures capture both natural-language anddomain-language considerations.Domain-Independent Decision Variables Weadd two new types of decisions abstracting overthe domain-specific decisions.
We encode the newdecisions as ?c and ?cd.
The first (?)
captures localinformation helping to determine if a given con-stituent c is likely to have a label (i.e., ?Pc for pred-icate or ?Ac for argument).
The second (?)
consid-ers higher level structures, quantifying decisionsover both the labels of the constituents c,d as apredicate-argument pair.
Note, a given word c canbe labeled as PRED or ARG if ?c and ?cd are active.Model?s Features We use the following fea-tures: (1) Local Decisions ?3(?
(c)) use a featureindicating if c is capitalized, a set of features cap-turing the context of c (window of size 2), suchas determiner and quantifier occurrences.
Finallywe use a set of features capturing the suffix lettersof c, these features are useful in identifying verbpatterns.
Features indicate if c is mapped to an ARGor PRED.
(2) Global Decision ?4(?
(c, d)): a featureindicating the relative location of c compared to din the input sentence.
Additional features indicateproperties of the relative location, such as if theword appears initially or finally in the sentence.Combined Model In order to consider bothtypes of information we augment our decisionmodel with the new variables, resulting in the fol-lowing objective function (Eq.
2).Fw(x) = arg max?,?
?c?x?s?D ?cs?w1T?1(x, c, s)+?c,d?x?s,t?D?i,j ?csi,dtj ?
w2T?2(x, c, si, d, tj) +?c?x ?c ?w3T?3(x, c)+?c,d?x ?cd ?w4T?4(x, c, d) (2)For notational convenience we decompose theweight vector w into four parts, w1,w2 for fea-tures of (first, second) order domain-dependent de-cisions, and similarly for the independent ones.In addition, we also add new constraints tyingthese new variables to semantic interpretation :?c ?
x (?c ?
?c,s1 ?
?c,s2 ?
... ?
?c,sn)?c ?
x, ?d ?
x (?c,d ?
?c,s1,dt1??c,s2,dt1?...?
?c,sn,dtn)(where n is the length of x).2.3 Learning the Combined ModelThe supervision to the learning process is givenvia data consisting of pairs of sentences and (do-main specific) semantic interpretation.
Given thatwe have introduced additional variables that cap-ture the more abstract predicate-argument struc-ture of the text, we need to induce these as la-tent variables.
Our decision model maps an inputsentence x, into a logical output y and predicate-argument structure h. We are only supplied withtraining data pertaining to the input (x) and out-put (y).
We use a variant of the latent structureperceptron to learn in these settings2.3 Experimental SettingsSituated Language This dataset, introduced in(Bordes et al, 2010), describes situations in a sim-ulated world.
The dataset consists of triplets of theform - (x,u, y), where x is a NL sentence describ-ing a situation (e.g., ?He goes to the kitchen?
), uis a world state consisting of grounded relations(e.g., loc(John, Kitchen)) description, and y isa logical interpretation corresponding to x.The original dataset was used for concept tag-ging, which does not include a compositional as-pect.
We automatically generated the full logicalstructure by mapping the constants to function ar-guments.
We generated additional function sym-bols of the same relation, but of different aritywhen needed 3.
Our new dataset consists of 25 re-lation symbols (originally 15).
In our experimentswe used a set of 5000 of the training triplets.Robocup The Robocup dataset, originally in-troduced in (Chen and Mooney, 2008), describesrobotic soccer events.
The dataset was collectedfor the purpose of constructing semantic parsersfrom ambiguous supervision and consists of both?noisy?
and gold labeled data.
The noisy dataset2Details omitted, see (Chang et al, 2010) for more details.3For example, a unary relation symbol for ?He plays?,and a binary for ?He plays with a ball?.464System Training ProcedureDOM-INIT w1: Noisy probabilistic model, described below.PRED-ARGS Onlyw3,w4 Trained over the Situ.
dataset.COMBINEDRL w1,w2,w3,w4:learned from Robocup goldCOMBINEDRI+S w3,w4: learned from the Situ.
dataset,w1 uses the DOM-INIT Robocup model.COMBINEDRL+S w3,w4: Initially learned over the Situ.
dataset,updated jointly with w1,w2 over Robocup goldTable 1: Evaluated System descriptions.was constructed by temporally aligning a streamof soccer events occurring during a robotic soc-cer match with human commentary describing thegame.
This dataset consists of pairs (x, {y0, yk}),x is a sentence and {y0, yk} is a set of events (log-ical formulas).
One of these events is assumed tocorrespond to the comment, however this is notguaranteed.
The gold labeled labeled data con-sists of pairs (x, y).
The data was collected fromfour Robocup games.
In our experiments we fol-low other works and use 4-fold cross validation,training over 3 games and testing over the remain-ing game.
We evaluate the Accuracy of the parserover the test game data.4 Due to space consider-ations, we refer the reader to (Chen and Mooney,2008) for further details about this dataset.Semantic Interpretation Tasks We considertwo of the tasks described in (Chen and Mooney,2008) (1) Semantic Parsing requires generatingthe correct logical form given an input sentence.
(2) Matching, given a NL sentence and a set ofseveral possible interpretation candidates, the sys-tem is required to identify the correct one.
In allsystems, the source for domain-independent infor-mation is the Situated domain, and the results areevaluated over the Robocup domain.Experimental Systems We tested several vari-ations, all solving Eq.
2, however different re-sources were used to obtain Eq.
2 parameters (seesec.
2.2).
Tab.
1 describes the different varia-tions.
We used the noisy Robocup dataset to ini-tialize DOM-INIT, a noisy probabilistic model, con-structed by taking statistics over the noisy robocupdata and computing p(y|x).
Given the training set{(x, {y1, .., yk})}, every word in x is aligned toevery symbol in every y that is aligned with it.
Theprobability of a matching (x, y)is computed as theproduct: ?ni=1 p(yi|xi), where n is the numberof symbols appearing in y, and xi, yi is the word4In our model accuracy is equivalent to F-measure.System Matching ParsingPRED-ARGS 0.692 ?DOM-INIT 0.823 0.357COMBINEDRI+S 0.905 0.627(BO?RSCHINGER ET AL., 2011) ?
0.86(KIM AND MOONEY, 2010) 0.885 0.742Table 2: Results for the matching and parsing tasks.
Oursystem performs well on the matching task without any do-main information.
Results for both parsing and matchingtasks show that using domain-independent information im-proves results dramatically.level matching to a logical symbol.
Note that thismodel uses lexical information only.4 Knowledge Transfer ExperimentsWe begin by studying the role of domain-independent information when very little domaininformation is available.
Domain-independent in-formation is learned from the situated domainand domain-specific information (Robocup) avail-able is the simple probabilistic model (DOM-INIT).This model can be considered as a noisy proba-bilistic lexicon, without any domain-specific com-positional information, which is only availablethrough domain-independent information.The results, summarized in Table 2, show thatin both tasks domain-independent information isextremely useful and can make up for missing do-main information.
Most notably, performance forthe matching task using only domain independentinformation (PRED-ARGS) was surprisingly good,with an accuracy of 0.69.
Adding domain-specificlexical information (COMBINEDRI+S) pushes thisresult to over 0.9, currently the highest for this task?
achieved without domain specific learning.The second set of experiments study whetherusing domain independent information, when rel-evant (gold) domain-specific training data is avail-able, improves learning.
In this scenario, thedomain-independent model is updated accordingto training data available for the Robocup domain.We compare two system over varying amountsof training data (25, 50, 200 training samplesand the full set of 3 Robocup games), one boot-strapped using the Situ.
domain (COMBINEDRL+S)and one relying on the Robocup training dataalone (COMBINEDRL).
The results, summarized intable 3, consistently show that transferring domainindependent information is helpful, and helps pushthe learned models beyond the supervision offeredby the relevant domain training data.
Our finalsystem, trained over the entire dataset achieves a465System # training ParsingCOMBINEDRL+S (COMBINEDRL) 25 0.16 (0.03)COMBINEDRL+S (COMBINEDRL) 50 0.323 (0.16)COMBINEDRL+S (COMBINEDRL) 200 0.385 (0.36)COMBINEDRL+S (COMBINEDRL) full game 0.86 (0.79)(CHEN ET AL., 2010) full game 0.81Table 3: Evaluating our model in a learning settings.
Thedomain-independent information is used to bootstrap learn-ing from the Robocup domain.
Results show that this infor-mation improves performance significantly, especially whenlittle data is availablescore of 0.86, significantly outperforming (Chenet al, 2010), a competing supervised model.
Itachieves similar results to (Bo?rschinger et al,2011), the current state-of-the-art for the pars-ing task over this dataset.
The system used in(Bo?rschinger et al, 2011) learns from ambigu-ous training data and achieves this score by usingglobal information.
We hypothesize that it can beused by our model and leave it for future work.5 ConclusionsIn this paper, we took a first step towards a newkind of generalization in semantic parsing: con-structing a model that is able to generalize to anew domain defined over a different set of sym-bols.
Our approach adds an additional hiddenlayer to the semantic interpretation process, cap-turing shallow but domain-independent semanticinformation, which can be shared by different do-mains.
Our experiments consistently show thatdomain-independent knowledge can be transferredbetween domains.
We describe two settings; inthe first, where only noisy lexical-level domain-specific information is available, we observe thatthe model learned in the other domain can be usedto make up for the missing compositional infor-mation.
For example, in the matching task, evenwhen no domain information is available, iden-tifying the abstract predicate argument structureprovides sufficient discriminatory power to iden-tify the correct event in over 69% of the times.In the second setting domain-specific examplesare available.
The learning process can still utilizethe transferred knowledge, as it provides scaffold-ing for the latent learning process, resulting in asignificant improvement in performance.6 AcknowledgementThe authors would like to thank Julia Hockenmaier, GeraldDeJong, Raymond Mooney and the anonymous reviewers fortheir efforts and insightful comments.Most of this work was done while the first author wasat the University of Illinois.
The authors gratefully ac-knowledge the support of the Defense Advanced ResearchProjects Agency (DARPA) Machine Reading Program un-der Air Force Research Laboratory (AFRL) prime contractno.
FA8750-09-C-0181.
In addition, this material is basedon research sponsored by DARPA under agreement numberFA8750-13-2-0008.
The U.S. Government is authorized toreproduce and distribute reprints for Governmental purposesnotwithstanding any copyright notation thereon.
The viewsand conclusions contained herein are those of the authors andshould not be interpreted as necessarily representing the offi-cial policies or endorsements, either expressed or implied, ofDARPA,AFRL, or the U.S. Government.ReferencesA.
Bordes, N. Usunier, R. Collobert, and J. Weston.2010.
Towards understanding situated natural lan-guage.
In AISTATS.B.
Bo?rschinger, B. K. Jones, and M. Johnson.
2011.Reducing grounded learning tasks to grammaticalinference.
In EMNLP.M.
Chang, D. Goldwasser, D. Roth, and V. Srikumar.2010.
Discriminative learning over constrained la-tent representations.
In NAACL.D.
Chen and R. Mooney.
2008.
Learning to sportscast:a test of grounded language acquisition.
In ICML.D.
L. Chen, J. Kim, and R. J. Mooney.
2010.
Traininga multilingual sportscaster: Using perceptual con-text to learn language.
Journal of Artificial Intelli-gence Research, 37:397?435.J.
Clarke, D. Goldwasser, M. Chang, and D. Roth.2010.
Driving semantic parsing from the world?sresponse.
In CoNLL.D.
Gildea and D. Jurafsky.
2002.
Automatic labelingof semantic roles.
Computational Linguistics.D.
Goldwasser, R. Reichart, J. Clarke, and D. Roth.2011.
Confidence driven unsupervised semanticparsing.
In ACL.J.
Kim and R. J. Mooney.
2010.
Generative alignmentand semantic parsing for learning from ambiguoussupervision.
In COLING.T.
Kwiatkowski, L. Zettlemoyer, S. Goldwater, , andM.
Steedman.
2010.
Inducing probabilistic ccggrammars from logical form with higher-order uni-fication.
In EMNLP.Y.W.
Wong and R. Mooney.
2007.
Learningsynchronous grammars for semantic parsing withlambda calculus.
In ACL.L.
Zettlemoyer and M. Collins.
2005.
Learning to mapsentences to logical form: Structured classificationwith probabilistic categorial grammars.
In UAI.466
