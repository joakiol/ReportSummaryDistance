Evaluating the Effectiveness of Ensembles of Decision Treesin Disambiguating Senseval Lexical SamplesTed PedersenDepartment of Computer ScienceUniversity of MinnesotaDuluth, MN, 55812 USAtpederse@d.umn.eduAbstractThis paper presents an evaluation of anensemble?based system that participatedin the English and Spanish lexical sampletasks of SENSEVAL-2.
The system com-bines decision trees of unigrams, bigrams,and co?occurrences into a single classi-fier.
The analysis is extended to includethe SENSEVAL-1 data.1 IntroductionThere were eight Duluth systems that participatedin the English and Spanish lexical sample tasks ofSENSEVAL-2.
These systems were all based on thecombination of lexical features with standard ma-chine learning algorithms.
The most accurate ofthese systems proved to be Duluth3 for English andDuluth8 for Spanish.
These only differ with respectto minor language specific issues, so we refer tothem generically as Duluth38, except when the lan-guage distinction is important.Duluth38 is an ensemble approach that assigns asense to an instance of an ambiguous word by takinga vote among three bagged decision trees.
Each treeis learned from a different view of the training ex-amples associated with the target word.
Each viewof the training examples is based on one of the fol-lowing three types of lexical features: single words,two word sequences that occur anywhere within thecontext of the word being disambiguated, and twoword sequences made up of this target word and an-other word within one or two positions.
These fea-tures are referred to as unigrams, bigrams, and co?occurrences.The focus of this paper is on determining if themember classifiers in the Duluth38 ensemble arecomplementary or redundant with each other andwith other participating systems.
Two classifiersare complementary if they disagree on a substantialnumber of disambiguation decisions and yet attaincomparable levels of overall accuracy.
Classifiersare redundant if they arrive at the same disambigua-tion decisions for most instances of the ambiguousword.
There is little advantage in creating an ensem-ble of redundant classifiers, since they will make thesame disambiguation decisions collectively as theywould individually.
An ensemble can only improveupon the accuracy of its member classifiers if theyare complementary to each other, and the errors ofone classifier are offset by the correct judgments ofothers.This paper continues with a description of thelexical features that make up the Duluth38 system,and then profiles the SENSEVAL-1 and SENSEVAL-2 lexical sample data that is used in this evaluation.There are two types of analysis presented.
First, theaccuracy of the member classifiers in the Duluth38ensemble are evaluated individually and in pair-wise combinations.
Second, the agreement betweenDuluth38 and the top two participating systems inSENSEVAL-1 and SENSEVAL-2 is compared.
Thispaper concludes with a review of the origins of ourapproach.
Since the focus here is on analysis, imple-mentation level details are not extensively discussed.Such descriptions can be found in (Pedersen, 2001b)or (Pedersen, 2002).July 2002, pp.
81-87.
Association for Computational Linguistics.Disambiguation: Recent Successes and Future Directions, Philadelphia,Proceedings of the SIGLEX/SENSEVAL Workshop on Word Sense2 Lexical FeaturesUnigram features represent words that occur five ormore times in the training examples associated witha given target word.
A stop?list is used to eliminatehigh frequency function words as features.For example, if the target word is water and thetraining example is I water the flowering flowers,the unigrams water, flowering and flowers are eval-uated as possible unigram features.
No stemmingor other morphological processing is performed, soflowering and flowers are considered as distinct uni-grams.
I and the are not considered as possible fea-tures since they are included in the stop?list.Bigram features represent two word sequencesthat occur two or more times in the training exam-ples associated with a target word, and have a log?likelihood value greater than or equal to 6.635.
Thiscorresponds to a p?value of 0.01, which indicatesthat according to the log?likelihood ratio there is a99% probability that the words that make up this bi-gram are not independent.If we are disambiguating channel and have thetraining example Go to the channel quickly, thenthe three bigrams Go to, the channel, and channelquickly will be considered as possible features.
tothe is not included since both words are in the stop?list.Co?occurrence features are defined to be a pairof words that include the target word and anotherword within one or two positions.
To be selected asa feature, a co?occurrence must occur two or moretimes in the lexical sample training data, and have alog?likelihood value of 2.706, which corresponds toa p?value of 0.10.
A slightly higher p?value is usedfor the co?occurrence features, since the volume ofdata is much smaller than is available for the bigramfeatures.If we are disambiguating art and have the trainingexample He and I like art of a certain period, weevaluate I art, like art, art of, and art a as possibleco?occurrence features.All of these features are binary, and indicate if thedesignated unigram, bigram, or co?occurrence ap-pears in the context with the ambiguous word.
Oncethe features are identified from the training examplesusing the methods described above, the decision treelearner selects from among those features to deter-mine which are most indicative of the sense of theambiguous word.
Decision tree learning is carriedout with the Weka J48 algorithm (Witten and Frank,2000), which is a Java implementation of the classicC4.5 decision tree learner (Quinlan, 1986).3 Experimental DataThe English lexical sample for SENSEVAL-1 ismade up of 35 words, six of which are used in mul-tiple parts of speech.
The training examples havebeen manually annotated based on the HECTORsense inventory.
There are 12,465 training exam-ples, and 7,448 test instances.
This corresponds towhat is known as the trainable lexical sample in theSENSEVAL-1 official results.The English lexical sample for SENSEVAL-2 con-sists of 73 word types, each of which is associ-ated with a single part of speech.
There are 8,611sense?tagged examples provided for training, whereeach instance has been manually assigned a Word-Net sense.
The evaluation data for the English lexi-cal sample consists of 4,328 held out test instances.The Spanish lexical sample for SENSEVAL-2 con-sists of 39 word types.
There are 4,480 training ex-amples that have been manually tagged with sensesfrom Euro-WordNet.
The evaluation data consists of2,225 test instances.4 System ResultsThis section (and Table 1) summarizes the per-formance of the top two participating systems inSENSEVAL-1 and SENSEVAL-2, as well as the Du-luth3 and Duluth8 systems.
Also included are base-line results for a decision stump and a majority clas-sifier.
A decision stump is simply a one node deci-sion tree based on a co?occurrence feature, while themajority classifier assigns the most frequent sense inthe training data to every occurrence of that word inthe test data.Results are expressed using accuracy, which iscomputed by dividing the total number of correctlydisambiguated test instances by the total number oftest instances.
Official results from SENSEVAL arereported using precision and recall, so these are con-verted to accuracy to provide a consistent point ofcomparison.
We utilize fine grained scoring, wherea word is considered correctly disambiguated only ifit is assigned exactly the sense indicated in the man-ually created gold standard.In the English lexical sample task of SENSEVAL-1the two most accurate systems overall were hopkins-revised (77.1%) and ets-pu-revised (75.6%).
TheDuluth systems did not participate in this exercise,but have been evaluated using the same data afterthe fact.
The Duluth3 system reaches accuracy of70.3%.
The simple majority classifier attains accu-racy of 56.4%.In the English lexical sample task of SENSEVAL-2 the two most accurate systems were JHU(R)(64.2%) and SMUls (63.8%).
Duluth3 attains an ac-curacy of 57.3%, while a simple majority classifierattains accuracy of 47.4%.In the Spanish lexical sample task of SENSEVAL-2 the two most accurate systems were JHU(R)(68.1%) and stanford-cs224n (66.9%).
Duluth8 hasaccuracy of 61.2%, while a simple majority classi-fier attains accuracy of 47.4%.The top two systems from the first and sec-ond SENSEVAL exercises represent a wide range ofstrategies that we can only hint at here.
The SMUlsEnglish lexical sample system is perhaps the mostdistinctive in that it incorporates information fromWordNet, the source of the sense distinctions inSENSEVAL-2.
The hopkins-revised, JHU(R), andstanford-cs224n systems use supervised algorithmsthat learn classifiers from a rich combination of syn-tactic and lexical features.
The ets-pu-revised sys-tem may be the closest in spirit to our own, since itcreates an ensemble of two Naive Bayesian classi-fiers, where one is based on topical context and theother on local context.More detailed description of the SENSEVAL-1and SENSEVAL-2 systems and lexical samples canbe found in (Kilgarriff and Palmer, 2000) and (Ed-monds and Cotton, 2001), respectively.5 Decomposition of EnsemblesThe three bagged decision trees that make up Du-luth38 are evaluated both individually and as pair-wise ensembles.
In Table 1 and subsequent discus-sion, we refer to the individual bagged decision treesbased on unigrams, bigrams and co?occurrences asU, B, and C, respectively.
We designate ensemblesthat consist of two or three bagged decision trees byTable 1: Accuracy in Lexical Sample Taskssystem accuracy correctEnglish SENSEVAL-1hopkins-revised 77.1% 5,742.4ets-pu-revised 75.6% 5,630.7UC 71.3% 5,312.8UBC 70.3% 5,233.9BC 70.1% 5,221.7UB 69.5% 5,176.0C 69.0% 5,141.8B 68.1% 5,074.7U 63.6% 4,733.7stump 60.7% 4,521.0majority 56.4% 4,200.0English SENSEVAL-2JHU(R) 64.2% 2,778.6SMUls 63.8% 2,761.3UBC 57.3% 2,480.7UC 57.2% 2,477.5BC 56.7% 2,452.0C 56.0% 2,423.7UB 55.6% 2,406.0B 54.4% 2,352.9U 51.7% 2,238.2stump 50.0% 2,165.8majority 47.4% 2,053.3Spanish SENSEVAL-2JHU(R) 68.1% 1,515.2stanford-cs224n 66.9% 1,488.5UBC 61.2% 1,361.3BC 60.1% 1,337.0UC 59.4% 1,321.9UB 59.0% 1,312.5B 58.6% 1,303.7C 58.6% 1,304.2stump 52.6% 1,171.0U 51.5% 1,146.0majority 47.4% 1,053.7using the relevant combinations of letters.
For exam-ple, UBC refers to a three member ensemble consist-ing of unigram (U), bigram (B), and co?occurrence(C) decision trees, while BC refers to a two memberensemble of bigram (B) and co-occurrence (C) deci-sion trees.
Note of course that UBC is synonymouswith Duluth38.Table 1 shows that Duluth38 (UBC) achieves ac-curacy significantly better than the lower boundsrepresented by the majority classifier and the de-cision stump, and comes within seven percentagepoints of the most accurate systems in each of thethree lexical sample tasks.
However, UBC does notsignificantly improve upon all of its member clas-sifiers, suggesting that the ensemble is made up ofredundant rather than complementary classifiers.In general the accuracies of the bigram (B) andco?occurrence (C) decision trees are never signifi-cantly different than the accuracy attained by the en-sembles of which they are members (UB, BC, UC,and UBC), nor are they significantly different fromeach other.
This is an intriguing result, since theco?occurrences represent a much smaller feature setthan bigrams, which are in turn much smaller thanthe unigram feature set.
Thus, the smallest of ourfeature sets is the most effective.
This may be due tothe fact that small feature sets are least likely to suf-fer from fragmentation during decision tree learning.Of the three individual bagged decision trees U,B, and C, the unigram tree (U) is significantly lessaccurate for all three lexical samples.
It is onlyslightly more accurate than the decision stump forboth English lexical samples, and is less accuratethan the decision stump in the Spanish task.The relatively poor performance of unigrams canbe accounted for by the large number of possiblefeatures.
Unigram features consist of all words notin the stop?list that occur five or more times in thetraining examples for a word.
The decision treelearner must search through a very large featurespace, and under such circumstances may fall vic-tim to fragmentation.Despite these results, we are not prepared to dis-miss the use of ensembles or unigram decision trees.An ensemble of unigram and co?occurrence de-cision trees (UC) results in greater accuracy thanany other lexical decision tree for the EnglishSENSEVAL-1 lexical sample, and is essentially tiedwith the most accurate of these approaches (UBC) inthe English SENSEVAL-2 lexical sample.
In princi-ple unigrams and co?occurrence features are com-plementary, since unigrams represent topical con-text, and co?occurrences represent local context.This follows the line of reasoning developed by(Leacock et al, 1998) in formulating their ensembleof Naive Bayesian classifiers for word sense disam-biguation.Adding the bigram decision tree (B) to the ensem-ble of the unigram and co?occurrence decision trees(UC) to create UBC does not result in significantimprovements in accuracy for the any of the lexicalsamples.
This reflects the fact that the bigram andco?occurrence feature sets can be redundant.
Bi-grams are two word sequences that occur anywherewithin the context of the ambiguous word, whileco?occurrences are bigrams that include the targetword and a word one or two positions away.
Thus,any consecutive two word sequence that includes theword to be disambiguated and has a log?likelihoodratio greater than the specified threshold will be con-sidered both a bigram and a co?occurrence.Despite the partial overlap between bigrams andco?occurrences, we believe that retaining them asseparate feature sets is a reasonable idea.
We haveobserved that an ensemble of multiple decision treeswhere each is learned from a representation of thetraining examples that has a small number of fea-tures is more accurate than a single decision treethat is learned from one large representation of thetraining examples.
For example, we mixed the bi-gram and co?occurrence features into a single fea-ture set, and then learned a single bagged decisiontree from this representation of the training exam-ples.
We observed drops in accuracy in both theSpanish and English SENSEVAL-2 lexical sampletasks.
For Spanish it falls from 59.4% to 58.2%, andfor English it drops from 57.2% to 54.9%.
Interest-ingly enough, this mixed feature set of bigrams andco?occurrences results in a slight increase over anensemble of the two in the SENSEVAL-1 data, risingfrom 71.3% to 71.5%.6 Agreement Among SystemsThe results in Table 1 show that UBC and its mem-ber classifiers perform at levels of accuracy signif-icantly higher than the majority classifier and de-cision stumps, and approach the level of some ofthe more accurate systems.
This poses an intrigu-ing possibility.
If UBC is making complementaryerrors to those other systems, then it might be pos-sible to combine these systems to achieve an evenhigher level of accuracy.
The alternative is that thedecision trees based on lexical features are largelyredundant with these other systems, and that thereis a hard core of test instances that are resistant todisambiguation by any of these systems.We performed a series of pairwise comparisonsto establish the degree to which these systems agree.We included the two most accurate participating sys-tems from each of the three lexical sample tasks,along with UBC, a decision stump, and a majorityclassifier.In Table 2 the column labeled ?both?
shows thepercentage and count of test instances where bothsystems are correct, the column labeled ?one?
showsthe percentage and count where only one of the twosystems is correct, and the column labeled ?none?shows how many test instances were not correctlydisambiguated by either system.
We note that inthe pairwise comparisons there is a high level ofagreement for the instances that both systems wereable to disambiguate, regardless of the systems in-volved.
For example, in the SENSEVAL-1 results thethree pairwise comparisons among UBC, hopkins-revised, and ets-pu-revised all show that approxi-mately 65% of the test instances are correctly dis-ambiguated by both systems.
The same is truefor the English and Spanish lexical sample tasks inSENSEVAL-2, where each pairwise comparison re-sults in agreement in approximately half the test in-stances.Next we extend this study of agreement to a three?way comparison between UBC, hopkins-revised,and ets-pu-revised for the SENSEVAL-1 lexical sam-ple.
There are 4,507 test instances where allthree systems agree (60.5%), and 973 test instances(13.1%) that none of the three is able to get correct.These are remarkably similar values to the pair?wisecomparisons, suggesting that there is a fairly consis-tent number of test instances that all three systemshandle in the same way.
When making a five?waycomparison that includes these three systems and thedecision stump and the majority classifier, the num-Table 2: System Pairwise Agreementsystem pair both one zeroEnglish SENSEVAL-1hopkins ets-pu 67.8% 17.1% 12.1%5,045 1,274 1,126UBC hopkins 64.8% 18.3% 17.0%4,821 1,361 1,263UBC ets-pu 64.4% 17.4% 18.2%4,795 1,295 1,355stump majority 53.4% 13.7% 32.9%3,974 1,022 2,448English SENSEVAL-2JHU(R) SMUls 50.4% 27.3% 22.3%2,180 1,183 965UBC JHU(R) 49.2% 24.1% 26.8%2,127 1,043 1,158UBC SMUls 47.2% 27.5% 25.2%2,044 1,192 1,092stump majority 45.2% 11.8% 43.0%1,955 511 1,862Spanish SENSEVAL-2JHU(R) cs224n 52.9% 29.3% 17.8%1,177 651 397UBC cs224n 52.8% 23.2% 24.0%1,175 517 533UBC JHU(R) 48.3% 33.5% 18.2%1,074 746 405stump majority 45.4% 20.4% 34.2%1,011 453 761ber of test instances that no system can disambiguatecorrectly drops to 888, or 11.93%.
This is interest-ing in that it shows there are nearly 100 test instancesthat are only disambiguated correctly by the decisionstump or the majority classifier, and not by any of theother three systems.
This suggests that very simpleclassifiers are able to resolve some test instances thatmore complex techniques miss.The agreement when making a three way compar-ison between UBC, JHU(R), and SMUls in the En-glish SENSEVAL-2 lexical sample drops somewhatfrom the pair?wise levels.
There are 1,791 test in-stances that all three systems disambiguate correctly(41.4%) and 828 instances that none of these sys-tems get correct (19.1%).
When making a five waycomparison between these three systems, the deci-sion stump and the majority classifier, there are 755test instances (17.4%) that no system can resolve.This shows that these three systems are performingsomewhat differently, and do not agree as much asthe SENSEVAL-1 systems.The agreement when making a three way com-parison between UBC, JHU(R), and cs224n in theSpanish lexical sample task of SENSEVAL-2 re-mains fairly consistent with the pairwise compar-isons.
There are 960 test instances that all threesystems get correct (43.2%), and 308 test instanceswhere all three systems failed (13.8%).
When mak-ing a five way comparison between these three sys-tems and the decision stump and the majority classi-fier, there were 237 test instances (10.7%) where nosystems was able to resolve the sense.
Here againwe see three systems that are handling quite a fewtest instances in the same way.Finally, the number of cases where neither the de-cision stump nor the majority classifier is correctvaries from 33% to 43% across the three lexical sam-ples.
This suggests that the optimal combination ofa majority classifier and decision stump could attainoverall accuracy between 57% and 66%, which iscomparable with some of the better results for theselexical samples.
Of course, how to achieve such anoptimal combination is an open question.
This isstill an interesting point, since it suggests that thereis a relatively large number of test instances thatrequire fairly minimal information to disambiguatesuccessfully.7 Duluth38 BackgroundThe origins of Duluth38 can be found in an ensem-ble approach based on multiple Naive Bayesian clas-sifiers that perform disambiguation via a majorityvote (Pedersen, 2000).
Each member of the ensem-ble is based on unigram features that occur in vary-ing sized windows of context to the left and right ofthe ambiguous word.
The sizes of these windows are0, 1, 2, 3, 4, 5, 10, 25, and 50 words to the left andto the right, essentially forming bags of words to theleft and right.
The accuracy of this ensemble disam-biguating the nouns interest (89%) and line (88%) isas high as any previously published results.
How-ever, each ensemble consists of 81 Naive Bayesianclassifiers, making it difficult to determine whichfeatures and classifiers were contributing most sig-nificantly to disambiguation.The frustration with models that lack an intuitiveinterpretation led to the development of decisiontrees based on bigram features (Pedersen, 2001a).This is quite similar to the bagged decision treesof bigrams (B) presented here, except that the ear-lier work learns a single decision tree where trainingexamples are represented by the top 100 ranked bi-grams, according to the log?likelihood ratio.
Thisearlier approach was evaluated on the SENSEVAL-1 data and achieved an overall accuracy of 64%,whereas the bagged decision tree presented hereachieves an accuracy of 68% on that data.Our interest in co?occurrence features is inspiredby (Choueka and Lusignan, 1985), who showed thathumans determine the meaning of ambiguous wordslargely based on words that occur within one ortwo positions to the left and right.
Co?occurrencefeatures, generically defined as bigrams where oneof the words is the target word and the other oc-curs within a few positions, have been widely usedin computational approaches to word sense disam-biguation.
When the impact of mixed feature setson disambiguation is analyzed, co?occurrences usu-ally prove to contribute significantly to overall ac-curacy.
This is certainly our experience, where theco?occurrence decision tree (C) is the most accurateof the individual lexical decision trees.
Likewise,(Ng and Lee, 1996) report overall accuracy for thenoun interest of 87%, and find that that when theirfeature set only consists of co?occurrence featuresthe accuracy only drops to 80%.Our interest in bigrams was indirectly motivatedby (Leacock et al, 1998), who describe an ensem-ble approach made up of local context and topicalcontext.
They suggest that topical context can berepresented by words that occur anywhere in a win-dow of context, while local contextual features arewords that occur within close proximity to the targetword.
They show that in disambiguating the adjec-tive hard and the verb serve that the local context ismost important, while for the noun line the topicalcontext is most important.
We believe that statisti-cally significant bigrams that occur anywhere in thewindow of context can serve the same role, in thatsuch a two word sequence is likely to carry heavysemantic (topical) or syntactic (local) weight.8 ConclusionThis paper analyzes the performance of the Duluth3and Duluth8 systems that participated in the Englishand Spanish lexical sample tasks in SENSEVAL-2.
We find that an ensemble offers very limitedimprovement over individual decision trees basedon lexical features.
Co?occurrence decision treesare more accurate than bigram or unigram decisiontrees, and are nearly as accurate as the full ensemble.This is an encouraging result, since the number ofco?occurrence features is relatively small and easyto learn from compared to the number of bigram orunigram features.9 AcknowledgmentsThis work has been partially supported by a NationalScience Foundation Faculty Early CAREER Devel-opment award (#0092784).The Duluth38 system (and all other Du-luth systems that participated in SENSEVAL-2)can be downloaded from the author?s web site:http://www.d.umn.edu/?tpederse/code.html.ReferencesY.
Choueka and S. Lusignan.
1985.
Disambiguationby short contexts.
Computers and the Humanities,19:147?157.P.
Edmonds and S. Cotton, editors.
2001.
Proceedings ofthe Senseval?2 Workshop.
Association for Computa-tional Linguistics, Toulouse, France.A.
Kilgarriff and M. Palmer.
2000.
Special issue onSENSEVAL: Evaluating word sense disambiguationprograms.
Computers and the Humanities, 34(1?2).C.
Leacock, M. Chodorow, and G. Miller.
1998.
Usingcorpus statistics and WordNet relations for sense iden-tification.
Computational Linguistics, 24(1):147?165,March.H.T.
Ng and H.B.
Lee.
1996.
Integrating multipleknowledge sources to disambiguate word sense: Anexemplar-based approach.
In Proceedings of the 34thAnnual Meeting of the Association for ComputationalLinguistics, pages 40?47.T.
Pedersen.
2000.
A simple approach to building en-sembles of Naive Bayesian classifiers for word sensedisambiguation.
In Proceedings of the First AnnualMeeting of the North American Chapter of the Asso-ciation for Computational Linguistics, pages 63?69,Seattle, WA, May.T.
Pedersen.
2001a.
A decision tree of bigrams is an ac-curate predictor of word sense.
In Proceedings of theSecond Annual Meeting of the North American Chap-ter of the Association for Computational Linguistics,pages 79?86, Pittsburgh, July.T.
Pedersen.
2001b.
Machine learning with lexical fea-tures: The duluth approach to senseval-2.
In Pro-ceedings of the Senseval-2 Workshop, pages 139?142,Toulouse, July.T.
Pedersen.
2002.
A baseline methodology for wordsense disambiguation.
In Proceedings of the Third In-ternational Conference on Intelligent Text Processingand Computational Linguistics, pages 126?135, Mex-ico City, February.J.
Quinlan.
1986.
Induction of decision trees.
MachineLearning, 1:81?106.I.
Witten and E. Frank.
2000.
Data Mining - Practi-cal Machine Learning Tools and Techniques with JavaImplementations.
Morgan?Kaufmann, San Francisco,CA.
