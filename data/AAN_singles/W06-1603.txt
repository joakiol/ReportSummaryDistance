Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 18?26,Sydney, July 2006. c?2006 Association for Computational LinguisticsParaphrase Recognition via Dissimilarity Signicance ClassicationLong Qiu, Min-Yen Kan and Tat-Seng ChuaDepartment of Computer ScienceNational University of SingaporeSingapore, 117543{qiul,kanmy,chuats}@comp.nus.edu.sgAbstractWe propose a supervised, two-phaseframework to address the problem of para-phrase recognition (PR).
Unlike most PRsystems that focus on sentence similarity,our framework detects dissimilarities be-tween sentences and makes its paraphrasejudgment based on the significance of suchdissimilarities.
The ability to differenti-ate significant dissimilarities not only re-veals what makes two sentences a non-paraphrase, but also helps to recall addi-tional paraphrases that contain extra butinsignificant information.
Experimentalresults show that while being accurateat discerning non-paraphrasing dissimilar-ities, our implemented system is able toachieve higher paraphrase recall (93%), atan overall performance comparable to thealternatives.1 IntroductionThe task of sentence-level paraphrase recognition(PR) is to identify whether a set of sentences (typ-ically, a pair) are semantically equivalent.
In sucha task, ?equivalence?
takes on a relaxed meaning,allowing sentence pairs with minor semantic dif-ferences to still be considered as paraphrases.PR can be thought of as synonym detection ex-tended for sentences, and it can play an equallyimportant role in natural language applications.As with synonym detection, applications such assummarization can benefit from the recognitionand canonicalization of concepts and actions thatare shared across multiple documents.
Automaticconstruction of large paraphrase corpora couldmine alternative ways to express the same con-cept, aiding machine translation and natural lan-guage generation applications.In our work on sentence-level PR, we have iden-tified two main issues through observation of sam-ple sentences.
The first is to identify all discrete in-formation nuggets, or individual semantic contentunits, shared by the sentences.
For a pair of sen-tences to be deemed a paraphrase, they must sharea substantial amount of these nuggets.
A trivialcase is when both sentences are identical, wordfor word.
However, paraphrases often employ dif-ferent words or syntactic structures to express thesame concept.
Figure 1 shows two sentence pairs,in which the first pair is a paraphrase while thesecond is not.
The paraphrasing pair (also denotedParaphrase (+pp):Authorities said a young man injured Richard Miller.Richard Miller was hurt by a young man.Non-Paraphrase (-pp):The technology-laced Nasdaq Composite Index.IXIC added 1.92 points, or 0.12 percent, at 1,647.94.The technology-laced Nasdaq Composite Index.IXIC dipped 0.08 of a point to 1,646.Figure 1: Examples: Paraphrasing & Non-paraphrasingas the +pp class) use different words.
Focusingjust on the matrix verbs, we note differences be-tween ?injured?
and ?hurt?.
A paraphrase recogni-tion system should be able to detect such semanticsimilarities (despite the different syntactic struc-tures).
Otherwise, the two sentences could lookeven less similar than two non-paraphrasing sen-tences, such as the two in the second pair.
Also inthe paraphrasing pair, the first sentence includes anextra phrase ?Authorities said?.
Human annotatorstend to regard the pair as a paraphrase despite thepresence of this extra information nugget.18This leads to the second issue: how to recognizewhen such extra information is extraneous withrespect to the paraphrase judgment.
Such para-phrases are common in daily life.
In news articlesdescribing the same event, paraphrases are widelyused, possibly with extraneous information.We equate PR with solving these two issues,presenting a natural two-phase architecture.
In thefirst phase, the nuggets shared by the sentencesare identified by a pairing process.
In the secondphase, any unpaired nuggets are classified as sig-nificant or not (leading to ?pp and +pp classifica-tions, respectively).
If the sentences do not containunpaired nuggets, or if all unpaired nuggets are in-significant, then the sentences are considered para-phrases.
Experiments on the widely-used MSRcorpus (Dolan et al, 2004) show favorable results.We first review related work in Section 2.
Wethen present the overall methodology and describethe implemented system in Section 3.
Sections 4and 5 detail the algorithms for the two phases re-spectively.
This is followed with our evaluationand discussion of the results.2 Related WorkPossibly the simplest approach to PR is an infor-mation retrieval (IR) based ?bag-of-words?
strat-egy.
This strategy calculates a cosine similar-ity score for the given sentence set, and if thesimilarity exceeds a threshold (either empiricallydetermined or learned from supervised trainingdata), the sentences are paraphrases.
PR systemsthat can be broadly categorized as IR-based in-clude (Corley and Mihalcea, 2005; Brockett andDolan, 2005).
In the former work, the authorsdefined a directional similarity formula reflect-ing the semantic similarity of one text ?with re-spect to?
another.
A word contributes to the di-rectional similarity only when its counterpart hasbeen identified in the opposing sentence.
The as-sociated word similarity scores, weighted by theword?s specificity (represented as inverted docu-ment frequency, idf ), sum to make up the direc-tional similarity.
The mean of both directionsis the overall similarity of the pair.
Brockettand Dolan (2005) represented sentence pairs asa feature vector, including features (among oth-ers) for sentence length, edit distance, number ofshared words, morphologically similar word pairs,synonym pairs (as suggested by WordNet and asemi-automatically constructed thesaurus).
A sup-port vector machine is then trained to learn the{+pp,?pp} classifier.Strategies based on bags of words largely ig-nore the semantic interactions between words.Weeds et al (2005) addressed this problem byutilizing parses for PR.
Their system for phrasalparaphrases equates paraphrasing as distributionalsimilarity of the partial sub-parses of a candidatetext.
Wu (2005)?s approach relies on the genera-tive framework of Inversion Transduction Gram-mar (ITG) to measure how similar two sentencesarrange their words based on edit distance.Barzilay and Lee (2003) proposed to applymultiple-sequence alignment (MSA) for tradi-tional, sentence-level PR.
Given multiple articleson a certain type of event, sentence clusters arefirst generated.
Sentences within the same clus-ter, presumably similar in structure and content,are then used to construct a lattice with ?back-bone?
nodes corresponding to words shared by themajority and ?slots?
corresponding to different re-alization of arguments.
If sentences from differ-ent clusters have shared arguments, the associatedlattices are claimed to be paraphrase.
Likewise,Shinyama et al (2002) extracted paraphrases fromsimilar news articles, but use shared named enti-ties as an indication of paraphrasing.
It should benoted that the latter two approaches are geared to-wards acquiring paraphrases rather than detectingthem, and as such have the disadvantage of requir-ing a certain level of repetition among candidatesfor paraphrases to be recognized.All past approaches invariably aim at a propersimilarity measure that accounts for all of thewords in the sentences in order to make a judg-ment for PR.
This is suitable for PR where in-put sentences are precisely equivalent semanti-cally.
However, for many people the notion ofparaphrases also covers cases in which minor orirrelevant information is added or omitted in can-didate sentences, as observed in the earlier ex-ample.
Such extraneous content should not be abarrier to PR if the main concepts are shared bythe sentences.
Approaches that focus only on thesimilarity of shared contents may fail when the(human) criteria for PR include whether the un-matched content is significant or not.
Correctlyaddressing this problem should increase accuracy.In addition, if extraneous portions of sentencescan be identified, their confounding influence onthe sentence similarity judgment can be removed,19leading to more accurate modeling of semanticsimilarity for both recognition and acquisition.3 MethodologyAs noted earlier, for a pair of sentences to be aparaphrase, they must possess two attributes:1. similarity: they share a substantial amount ofinformation nuggets;2. dissimilarities are extraneous: if extra infor-mation in the sentences exists, the effect ofits removal is not significant.A key decision for our two-phase PR frameworkis to choose the representation of an informationnugget.
A simple approach is to use representativewords as information nuggets, as is done in theSimFinder system (Hatzivassiloglou et al, 2001).Instead of using words, we choose to equate in-formation nuggets with predicate argument tuples.A predicate argument tuple is a structured repre-sentation of a verb predicate together with its argu-ments.
Given a sentence from the example in Fig-ure 1, its predicate argument tuple form in Prop-Bank (Kingsbury et al, 2002) format is:target(predicate): hurtarg0: a young manarg1: Richard MillerWe feel that this is a better choice for the repre-sentation of a nugget as it accounts for the action,concepts and their relationships as a single unit.In comparison, using fine-grained units such aswords, including nouns and verbs may result in in-accuracy (sentences that share vocabulary may notbe paraphrases), while using coarser-grained unitsmay cause key differences to be missed.
In the restof this paper, we use the term tuple for concisenesswhen no ambiguity is introduced.An overview of our paraphrase recognition sys-tem is shown in Figure 2.
A pair of sentences isfirst fed to a syntactic parser (Charniak, 2000) andthen passed to a semantic role labeler (ASSERT;(Pradhan et al, 2004)), to label predicate argu-ment tuples.
We then calculate normalized tuplesimilarity scores over the tuple pairs using a met-ric that accounts for similarities in both syntacticstructure and content of each tuple.
A thesaurusconstructed from corpus statistics (Lin, 1998) isutilized for the content similarity.We utilize this metric to greedily pair togetherthe most similar predicate argument tuples acrossFigure 2: System architecturesentences.
Any remaining unpaired tuples repre-sent extra information and are passed to a dissim-ilarity classifier to decide whether such informa-tion is significant.
The dissimilarity classifier usessupervised machine learning to make such a deci-sion.4 Similarity Detection and PairingWe illustrate this advantage of using predicate ar-gument tuples from our running example.
In Ta-ble 1, one of the model sentences is shown in themiddle column.
Two edited versions are shown onthe left and right columns.
While it is clear thatthe left modification is an example of a paraphraseand the right is not, the version on the left in-volves more changes in its syntactic structure andvocabulary.
Standard word or syntactic similar-ity measures would assign the right modification ahigher similarity score, likely mislabeling one orboth modifications.In contrast, semantic role labeling identifies thedependencies between predicates and their argu-ments, allowing a more precise measurement ofsentence similarity.
Assuming that the argumentsin predicate argument tuples are assigned the samerole when their roles are comparable1 , we definethe similarity score of two tuples Ta and Tb asthe weighted sum of the pairwise similarities ofall their shared constituents C={(ca, cb)} (c beingeither the target or one of the arguments that both1ASSERT, which is trained on the Propbank, only guaran-tees consistency of arg0 and arg1 slots, but we have found inpractice that aligning arg2 and above arguments do not causeproblems.20Modification 1: paraphrase Model Sentence Modification 2: non-paraphraseSentence Richard Miller was hurt by ayoung man.Authorities said a young man in-jured Richard Miller.Authorities said Richard Miller injureda young man.
(Paired)Tuplestarget: saidarg0: Authoritiesarg1: a young man injuredRichard Millertarget: saidarg0: Authoritiesarg1: Richard Miller injured ayoung mantarget: hurtarg0: a young manarg1: Richard Millertarget: injuredarg0: a young manarg1: Richard Millertarget: injuredarg0: Richard Millerarg1: a young manTable 1: Similarity Detection: pairing of predicate argument tuplestuples have):Sim(Ta, Tb) =1?XSim(ca, cb)?c={target,argshared}wc==targettarget (1)where normalization factor ?
is the sum of theweights of constituents in C , i.e.:?
= ?{argshared}?
+ wtarget (2)In our current implementation we reduce tar-gets and their arguments to their syntactic head-words.
These headwords are then directly com-pared using a corpus-based similarity thesaurus.As we hypothesized that targets are more impor-tant for predicate argument tuple similarity, wemultiply the target?s similarity by a weighting fac-tor wtarget , whose value we have empirically de-termined as 1.7, based on a 300-pair developmentset from the MSR training set.We then proceed to pair tuples in the two sen-tences using a greedy iterative algorithm.
The al-gorithm locates the two most similar tuples fromeach sentence, pairs them together and removesthem from futher consideration.
The process stopswhen subsequent best pairings are below the simi-larity threshold or when all possible tuples are ex-hausted.
If unpaired tuples still exist in a givensentence pair, we further examine the copular con-structions and noun phrases in the opposing sen-tence for possible pairings2 .
This results in a one-2Copular constructions are not handled by ASSERT.
Suchconstructions account for a large proportion of the semanticmeaning in sentences.
Consider the pair ?Microsoft rose 50cents?
and ?Microsoft was up 50 cents?, in which the secondis in copular form.
Similarly, NPs can often be equivalentto predicate argument tuples when actions are nominalized.Consider an NP that reads ?
(be blamed for) frequent attackson soldiers?
and a predicate argument tuple: ?
(be blamed for)attacking soldiers?.
Again, identical information is conveyedbut not captured by semantic role labeling.
In such cases,they can be paired if we allow a candidate tuple to pair withthe predicative argument (e.g., 50 cents) of a copula, or (thehead of) an NP in the opposing sentence.
As these heuristicmatches may introduce errors, we resort to these methods ofmatching tuple only in the contingency when there are un-paired tuples.to-one mapping with possibly some tuples left un-paired.
The curved arrows in Table 1 denote thecorrect results of similarity pairing: two tuples arepaired up if their target and shared arguments areidentical or similar respectively, otherwise they re-main unpaired even if the ?bag of words?
they con-tain are the same.5 Dissimilarity SignificanceClassificationIf some tuples remain unpaired, they are dissimilarparts of the sentence that need to be labeled by thedissimilarity classifier.
Such unpaired informa-tion could be extraneous or they could be semanti-cally important, creating a barrier for paraphrase.We frame this as a supervised machine learningproblem in which a set of features are used toinform the classifier.
A support vector machine,SVMLight, was chosen as the learning model as ithas shown to yield good performance over a wideapplication range.
We experimented with a wideset of features of unpaired tuples, including inter-nal counts of numeric expressions, named entities,words, semantic roles, whether they are similarto other tuples in the same sentence, and contex-tual features like source/target sentence length andpaired tuple count.
Currently, only two featuresare correlated in improved classification, whichwe detail now.Syntactic Parse Tree Path: This is a series offeatures that reflect how the unpaired tuple con-nects with the context: the rest of the sentence.It models the syntactic connection between theconstituents on both ends of the path (Gildea andPalmer, 2002; Pradhan et al, 2004).
Here, wemodel the ends of the path as the unpaired tupleand the paired tuple with the closest shared ances-tor, and model the path itself as a sequence of con-stituent category tags and directions to reach thedestination (the paired target) from the source (the21unpaired target) via the the shared ancestor.
Whenno tuples have been paired in the sentence pair,the destination defaults to the root of the syntacticparse tree.
For example, the tuples with target ?in-jured?
are unpaired when the model sentence andthe non-paraphrasing modification in Table 1 arebeing compared.
A path ?
?V BD, ?V P , ?S , ?SBAR, ?V P , ?V BD?
links a target ?injured?
to the pairedtarget ?said?, as shown in Figure 3.VP`````     VBDsaidSBARSXXXXNPaa!
!NNPRichardNNPMillerVPbb""VBDinjuredNPFigure 3: Syntactic parse tree pathThe syntactic path can act as partial evidencein significance classification.
In the above exam-ple, the category tag ?V BD?
assigned to ?injured?indicates that the verb is in its past tense.
Sucha predicate argument tuple bears the main con-tent of the sentence and generally can not be ig-nored if its meaning is missing in the opposingsentence.
Another example is shown in Figure4.
The second sentence has one unpaired target?proposed?
while the rest all find their counter-part.
The path we get from the syntactic parse treereads ?
?V BN , ?NP , ?S , ...?, showing that the un-paired tuple (consisting of a single predicate) is amodifier contained in an NP.
It can be ignored ifthere is no contradiction in the opposing sentence.We represent a syntactic path by a set of n-gram(n ?
4) features of subsequences of category tagsfound in the path, along with the respective direc-tion.
We require these n-gram features to be nomore than four category tags away from the un-paired target, as our primary concern is to modelwhat role the target plays in its sentence.Sheena Young of Child, the national infertility sup-port network, hoped the guidelines would lead to a more?fair and equitable?
service for infertility sufferers.Sheena Young, a spokesman for Child, the nationalinfertility support network, said the proposed guide-lines should lead to a more ?fair and equitable?
servicefor infertility sufferers.Figure 4: Unpaired predicate argument tuple asmodifier in a paraphrasePredicate: This is the lexical token of predi-cate argument tuple?s target, as a text feature.
Asthis feature is liable to run into sparse data prob-lems, the semantic category of the target would bea more suitable feature.
However, verb similar-ity is generally regarded as difficult to measure,both in terms of semantic relatedness as well asin finding a consistent granularity for verb cate-gories.
We investigated using WordNet as well asLevin?s classification (Levin, 1993) as additionalfeatures on our validation data, but currently findthat using the lexical form of the target works best.5.1 Classifier Training Set AcquisitionCurrently, no training corpus for predicate argu-ment tuple significance exists.
Such a corpus is in-dispensable for training the classifier.
Rather thanmanually annotating training instances, we usean automatic method to construct instances fromparaphrase corpora.
This is possible as the para-phrase judgments in the corpora can imply whichportion of the sentence(s) are significant barriersto paraphrasing or not.
Here, we exploit the simi-larity detector implemented for the first phase forthis purpose.
If unpaired tuples exist after greedypairing, we classify them along two dimensions:whether the sentence pair is a (non-)paraphrase,and the source of the unpaired tuples:1.
[PS] paraphrasing pairs and unpaired predicate argu-ment tuples are only from a single sentence;2.
[NS] non-paraphrasing pairs and only one single un-paired predicate argument tuple exists;3.
[PM] paraphrasing pairs and unpaired predicate argu-ment tuples are from multiple (both) sentences;4.
[NM] non-paraphrasing pairs and multiple unpairedpredicate argument tuples (from either one or both sen-tences) exist.Assuming that similarity detector pairs tuplescorrectly, for the first two categories, the para-phrasing judgment is directly linked to the un-paired tuples.
PS tuple instances are thereforeused as insignificant class instances, and NS assignificant ones.
The last two categories can-not be used for training data, as it is unclear whichof the unpaired tuples is responsible for the (non-)paraphrasing as the similarity measure may mis-takenly leave some similar predicate argument tu-ples unpaired.6 EvaluationThe goal of our evaluation is to show that our sys-tem can reliably determine the cause(s) of non-22MSR Corpus Label +pp -ppsystem prediction correct?
T F T F total# sentence pairs (s-ps) 85 23 55 37 200# labelings (H&C agree) 80 19 53 35 187# tuple pairs (t-ps) (S) 80 6 36 35 157# correct t-ps (H&S agree) 74 6 34 30 144# missed t-ps (H) 11 10 5 5 31# sig.
unpaired tuples(H) 6 4 69 51 130# sig.
unpaired tuples(S) 0 32 70 0 102# sig.
unpaired tuples(H&S) 0 4 43 0 47# -pp for other reasons 0 0 5 2 7Table 2: (H)uman annotations vs. (C)orpus anno-tations and (S)ystem outputparaphrase examples, while maintaining the per-formance level of the state-of-the-art PR systems.For evaluation, we conduct both componentevaluations as well as a holistic one, resulting inthree separate experiments.
In evaluating the firsttuple pairing component, we aim for high preci-sion, so that sentences that have all tuples pairedcan be safely assumed to be paraphrases.
In evalu-ating the dissimilarity classifier, we simply aim forhigh accuracy.
In our overall system evaluation,we compare our system versus other PR systemson standard corpora.Experimental Data Set.
For these experi-ments, we utilized two widely-used corpora forparaphrasing evaluation: the MSR and PASCALRTE corpora.
The Microsoft Research Paraphrasecoupus (Dolan et al, 2004) consists of 5801newswire sentence pairs, 3900 of which are an-notated as semantically equivalent by human an-notators.
It reflects ordinary paraphrases that peo-ple often encounter in news articles, and may beviewed as a typical domain-general paraphraserecognition task that downstream NLP systemswill need to deal with.
The corpus comes dividedinto standard training (70%) and testing (30%) di-visions, a partition we follow in our experiments.ASSERT (the semantic role labeler) shows for thiscorpus a sentence contains 2.24 predicate argu-ment tuples on average.
The second corpus isthe paraphrase acquisition subset of the PASCALRecognizing Textual Entailment (RTE) Challengecorpus (Dagan et al, 2005).
This is much smaller,consisting of 50 pairs, which we employ for test-ing only to show portability.To assess the component performance, we needadditional ground truth beyond the {+pp, ?pp}labels provided by the corpora.
For the first eval-uation, we need to ascertain whether a sentencepair?s tuples are correctly paired, misidentified ormispaired.
For the second, which tuple(s) (if any)are responsible for a ?pp instance.
However, cre-ating ground truth by manual annotation is expen-sive, and thus we only sampled the data set to getan indicative assessment of performance.
We sam-pled 200 random instances from the total MSRtesting set, and first processed them through ourframework.
Then, five human annotators (two au-thors and three volunteers) annotated the groundtruth for tuple pairing and the semantic signifi-cance of the unpaired tuples, while checking sys-tem output.
They also independently came up withtheir own {+pp,-pp} judgment so we could assessthe reliability of the provided annotations.The results of this annotation is shown in Ta-ble 2.
Examining this data, we can see that thesimilarity detector performs well, despite its sim-plicity and assumption of a one-to-one mapping.Out of the 157 predicate argument tuple pairsidentified through similarity detection, annotatorsagreed that 144 (92%) are semantically similar orequivalent.
However, 31 similar pairs were missedby the system, resulting in 82% recall.
We deferdiscussion on the other details of this table to Sec-tion 7.To assess the dissimilarity classifier, we focuson the ?pp subset of 55 instances recognized bythe system.
For 43 unpaired tuples from 40 sen-tence pairs (73% of 55), the annotators?
judgmentsagree with the classifier?s claim that they are sig-nificant.
For these cases, the system is able to bothrecognize that the sentence pair is not a paraphraseand further correctly establish a cause of the non-paraphrase.In addition to this ground truth sampled evalu-ation, we also show the effectiveness of the clas-sifier by examining its performance on PS and NStuples in the MSR corpus as described in Section5.
The test set consists of 413 randomly selectedPS and NS instances among which 145 are signif-icant (leading to non-paraphrases).
The classifierpredicts predicate argument tuple significance atan accuracy of 71%, outperforms a majority clas-sifier (65%), a gain which is marginally statisti-cally significant (p < .09).significant insignificant112 263 insignificant by classifier33 5 significant by classifierWe can see the classifier classifies the majorityof insignificant tuples correctly (by outputting a23709 Sentence Pairs Without 1016 Sentence Pairs WithUnpaired Tuples Unpaired Tuples OverallAlgorithm (41.1% of Test set) (58.9% of Test set) (100% of Test set)Acc R P Acc R P Acc R P F1Majority Classifier 79.5% 100% 79.5% 57.4% 100% 57.4% 66.5% 100% 66.5% 79.9%SimFinder 82.2% 92.2% 86.4% 66.3% 84.9% 66.1% 72.9% 88.5% 75.1% 81.3%CM05 - - - - - - 71.5% 92.5% 72.3% 81.2%Our System 79.5% 100% 79.5% 66.7% 87.0% 66.0% 72.0% 93.4% 72.5% 81.6%Table 3: Results on MSR test set17 Sentence Pairs Without 33 Sentence Pairs WithAlgorithm Unpaired Tuples Unpaired Tuples Overall(34% of Test set) (66% of Test set) (100% of Test set)Acc R P Acc R P Acc R P F1Majority Classifier 65% 100% 65% 42% 100% 42% 50% 100% 50% 67%SimFinder 71% 91% 71% 42% 21% 27% 52% 52% 52% 52%Our System 65% 100% 65% 48% 64% 43% 54% 80% 53% 64%Table 4: Results on PASCAL PP test setscore greater than zero), which is effectively a98% recall of insignificant tuples.
However, theprecision is less satisfatory.
We suspect this is par-tially due the tuples that fail to be paired up withtheir counterpart.
Such noise is found among theautomatically collected PS instances used in train-ing.020406080100120140160180>-.5-0.5--0.25-.25-00 -.25.25-.5.5 -.75.75-1<1SVM PredictionFrequencyInsignificantSignificantFigure 5: Dissimilarity classifier performanceFor the final system-wide evaluation, we imple-mented two baseline systems: a majority classifierand SimFinder (Hatzivassiloglou et al, 2001), abag-of-words sentence similarity module incorpo-rating lexical, syntactic and semantic features.
InTable 3, precision and recall are measured with re-spect to the paraphrasing class.
The table showssentence pairs falling under the column ?pairswithout unpaired tuples?
are more likely to beparaphrasing than an arbitrary pair (79.5% ver-sus 66.5%), providing further validation for usingpredicate argument tuples as information nuggets.The results for the experiment benchmarking theoverall system performance are shown under the?Overall?
column: our approach performs compa-rably to the baselines at both accuracy and para-phrase recall.
The system performance reported in(CM05; (Corley and Mihalcea, 2005)), which isamong the best we are aware of, is also includedfor comparison.We also ran our system (trained on the MSRcorpus) on the 50 instances in the PASCAL para-phrase acquisition subset.
Again, the system per-formance (as shown in Table 4) is comparable tothe baseline systems.7 DiscussionWe have just shown that when two sentences haveperfectly matched predicate argument tuples, theyare more likely to be a paraphrase than a randomsentence pair drawn from the corpus.
Further-more, in the sampled human evaluation in Table2, among the 88 non-paraphrasing instances withwhose MSR corpus labels our annotators agreed(53 correctly and 35 incorrectly judged by our sys-tem), the cause of the ?pp is correctly attributedin 81 cases to one or more predicate argument tu-ples.
The remaining 7 cases (as shown in the lastrow) are caused by phenomenon that are not cap-tured by our tuple representation.
We feel this jus-tifies using predicate argument tuples as informa-tion nuggets, but we are currently considering ex-panding our representation to account for some ofthese cases.The evaluation also confirms the difficulty ofmaking paraphrase judgements.
Although the24MSR corpus used strict means of resolving inter-rater disagreements during its construction, the an-notators agreed with the MSR corpus labels only93.5% (187/200) of the time.One weakness of our system is that we rely on athesaurus (Lin, 1998) for word similarity informa-tion for predicate argument tuple pairing.
How-ever, it is designed to provide similarity scoresbetween pairs of individual words (rather thanphrases).
If a predicate argument tuple?s target orone argument is realized as a phrase (borrow ?check out, for instance), the thesaurus is unable toprovide an accurate similarity score.
For similaritybetween predicate argument tuples, negation andmodality have yet to be addressed, although theyaccount for only a tiny fraction of the corpus.We further estimated how the similarity detec-tor can be affected when the semantic role labelermakes mistakes (by failing to identify argumentsor assigning incorrect role names).
Checking 94pairs ground-truth similar tuples, we found that thesystem mislabels 43 of them.
The similarity detec-tor failed to pair around 30% of them.
In compar-sion, all the tuple pairs free of labeling errors arecorrectly paired.
A saving grace is that labelingerrors rarely lead to incorrect pairing (one pairingin all the examined sentences).
The labeling er-rors impact the whole system in two ways: 1) theycaused similar tuples that should have been pairedup to be added as noise in that dissimilarity clas-sifier?s training set and 2) paired tuples with label-ing errors erroneously increase the target weightin Equation (1).Some example paraphrasing cases that are prob-lematic for our current system are:1.
Non-literal language issues such as implica-ture, idiom, metaphor, etc.
are not addressed inour current system.
When predicate argument tu-ples imply each other, they are less similar thanwhat our system currently is trained for, causingthe pairing to fail:+pp, Later in the day, a standoff developed between Frenchsoldiers and a Hema battlewagon that attempted to pass theUN compound.French soldiers later threatened to open fire on a Hema bat-tlewagon that tried to pass near the UN compound.2.
A paraphrasing pair may exceed the systems?threshold for syntactic difference:+pp, With the exception of dancing, physical activity did notdecrease the risk.Dancing was the only physical activity associated with alower risk of dementia.3.
One or more unpaired tuples exist, but theirsignificance is not inferred correctly:+pp, Inhibited children tend to be timid with new people,objects, and situations, while uninhibited children sponta-neously approach them.Simply put, shy individuals tend to be more timid with newpeople and situations.In the MSR corpus, the first error case is morefrequent than the other two.
We identify these aschallenging cases where idiomatic processing isneeded.Below we show some unpaired predicate ar-gument tuples (underlined) that are significantenough to be paraphrase barriers.
These examplesgive an indicative categorization of what signifi-cant tuples are and their corpus frequency (whenpredicate argument tuples are the reasons; we ex-amined 40 such cases for this estimation).
Thereis one thing in common: every case involves sub-stantial information that is difficult to infer fromcontext.
Such tuples appear as:1.
(40%) The nucleus of the sentence (often thematrix tuple):Michael Hill, a Sun reporter who is a member of theWashington-Baltimore Newspaper Guild?s bargaining com-mittee, estimated meetings to last late Sunday.2.
(30%) A part of a coordination:Security lights have also been installed and police haveswept the grounds for booby traps.3.
(13%) A predicate of a modifying clause:Westermayer was 26 then, and a friend and former managerwho knew she was unhappy in her job tipped her to anotherposition.4.
(7%) An adjunct:While waiting for a bomb squad to arrive, the bomb exploded,killing Wells.5.
(7%) An embedded sentence:Dean told reporters traveling on his 10-city ?SleeplessSummer?
tour that he considered campaigning in Texas achallenge.6.
(3%) Or factual content that conflicts withthe opposing sentence:Total sales for the period declined 8.0 percent to USD1.99billion from a year earlier.Wal-Mart said sales at stores open at least a year rose 4.6percent from a year earlier.8 ConclusionsWe have proposed a new approach to the para-phrase recognition (PR) problem: a supervised,25two-phase framework emphasizing dissimilarityclassification.
To emulate human PR judgmentin which insignificant, extraneous informationnuggets are generally allowed for a paraphrase,we estimate whether such additional informationnuggets affect the final paraphrasing status of asentence pair.
This approach, unlike previous PRapproaches, has the key benefit of explaining thecause of a non-paraphrase sentence pair.In the first, similarity detection module, usingpredicate argument tuples as the unit for compar-ison, we pair them up in a greedy manner.
Un-paired tuples thus represent additional informationunrepresented in the opposing sentence.
A second,dissimilarity classification module uses the lexicalhead of the predicates and the tuples?
path of at-tachment as features to decide whether such tuplesare barriers to paraphrase.Our evaluations show that the system obtains 1)high accuracy for the similarity detector in pairingpredicate argument tuples, 2) robust dissimilar-ity classification despite noisy training instancesand 3) comparable overall performance to currentstate-of-the-art PR systems.
To our knowledge thisis the first work that tackles the problem of identi-fying what factors stop a sentence pair from beinga paraphrase.We also presented corpus examples that illus-trate the categories of errors that our frameworkmakes, suggesting future work in PR.
While wecontinue to explore more suitable representationof unpaired predicate argument tuples, we plan toaugment the similarity measure for phrasal unitsto reduce the error rate in the first component.
An-other direction is to detect semantic redundancy ina sentence.
Unpaired tuples that are semanticallyredundant should also be regarded as insignificant.ReferencesRegina Barzilay and Lillian Lee.
2003.
Learning toparaphrase: An unsupervised approach using multiple-sequence alignment.
In Proceedings of HLT-NAACL 2003.Chris Brockett and Bill Dolan.
2005.
Support vector ma-chines for paraphrase identification and corpus construc-tion.
In Proceedings of the 3rd International Workshop onParaphrasing.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of the First Annual Meeting of theNorth American Chapter of the Association for Computa-tional Linguistics (NAACL?2000).Courtney Corley and Rada Mihalcea.
2005.
Measuring thesemantic similarity of texts.
In Proceedings of the ACLWorkshop on Empirical Modeling of Semantic Equiva-lence and Entailment, pages 13?18, Ann Arbor, USA.Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005.The pascal recognising textual entailment challenge.
InPASCAL Proceedings of the First Challenge Workshop?Recognizing Textual Entailment, Southampton,UK.Bill Dolan, Chris Quirk, and Chris Brockett.
2004.
Unsuper-vised construction of large paraphrase corpora: Exploitingmassively parallel news sources.
In Proceedings of the20th International Conference on Computational Linguis-tics, Geneva, Switzerland.Daniel Gildea and Martha Palmer.
2002.
The necessity ofparsing for predicate argument recognition.
In Proceed-ings of the 40th Annual Meeting of the Association forComputational Linguistics, Philadelphia, USA.Vassileios Hatzivassiloglou, Judith Klavans, Melissa Hol-combe, Regina Barzilay, Min-Yen Kan, and KathleenMcKeown.
2001.
Simfinder: A flexible clustering toolfor summarization.
In Proceedings of the NAACL Work-shop on Automatic Summarization, pages 41?49.Paul Kingsbury, Martha Palmer, and Mitch Marcus.
2002.Adding semantic annotation to the penn treebank.
In Pro-ceedings of the Human Language Technology Conference,San Diego, USA.Beth Levin.
1993.
English verb classes and alternations: Apreliminary investigation.
University of Chicago Press.Dekang Lin.
1998.
Automatic retrieval and clustering ofsimilar words.
In Proceedings of COLING-ACL ?98, pages768?774, Montreal, Canada.Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Mar-tin, and Dan Jurafsky.
2004.
Shallow semantic pars-ing using support vector machines.
In Proceedings ofHLT/NAACL, Boston, USA.Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo, and RalphGrishman.
2002.
Automatic paraphrase acquisition fromnews articles.
In Proceedings of the Human LanguageTechnology Conference, pages 40?46, San Diego, USA.Julie Weeds, David Weir, and Bill Keller.
2005.
The dis-tributional similarity of sub-parses.
In Proceedings of theACL Workshop on Empirical Modeling of Semantic Equiv-alence and Entailment, pages 7?12, Ann Arbor, USA.Dekai Wu.
2005.
Recognizing paraphrases and textual en-tailment using inversion transduction grammars.
In Pro-ceedings of the ACL Workshop on Empirical Modeling ofSemantic Equivalence and Entailment, Ann Arbor, USA.26
