Document Clustering with Grouping andChaining AlgorithmsYllias Chali and Soufiane NoureddineDepartment of Computer Science,University of LethbridgeAbstract.
Document clustering has many uses in natural language toolsand applications.
For instance, summarizing sets of documents that alldescribe the same event requires first identifying and grouping thosedocuments talking about the same event.
Document clustering involvesdividing a set of documents into non-overlapping clusters.
In this paper,we present two document clustering algorithms: grouping algorithm, andchaining algorithm.
We compared them with k-means and the EM algo-rithms.
The evaluation results showed that our two algorithms performbetter than the k-means and EM algorithms in different experiments.1 IntroductionDocument clustering has many uses in natural language tools and applications.For instance, summarizing sets of documents that all describe the same eventrequires first identifying and grouping those documents talking about the sameevent.
Document clustering involves dividing a set of texts into non-overlappingclusters, where documents in a cluster are more similar to one another than todocuments in other clusters.
The term more similar, when applied to clustereddocuments, usually means closer by some measure of proximity or similarity.According to Manning and Schutze [1], there are two types of structures pro-duced by clustering algorithms, hierarchical clustering and flat or non-hierarchical clustering.
Flat clustering are simply groupings of similar objects.Hierarchical clustering is a tree of subclasses which represent the cluster thatcontains all the objects of its descendants.
The leaves of the tree are the individ-ual objects of the clustered set.
In our experiments, we used the non-hierarchicalclustering k-means and EM [2] and our own clustering algorithms.There are several similarity measures to help find out groups of related doc-uments in a set of documents [3].
We use identical word method and semanticrelation method to assign a similarity score to each pair of compared texts.
Forthe identical word method, we use k-means algorithm, the EM algorithm, andour own grouping algorithm to cluster the documents.
For the semantic relationmethod, we use our own grouping algorithm and chaining algorithm to do theclustering job.
We choose WordNet 1.6 as our background knowledge.
WordNetconsists of synsets gathered in a hypernym/hyponym hierarchy [4].
We use itto get word senses and to evaluate the semantic relations between word senses.R.
Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
280?291, 2005.c?
Springer-Verlag Berlin Heidelberg 2005Document Clustering with Grouping and Chaining Algorithms 2812 Identical Word SimilarityTo prepare the texts for the clustering process using identical word similarity,we perform the following steps on each of the selected raw texts:1.
Preprocessing which consists in extracting file contents from the raw texts,stripping special characters and numbers, converting all words to lower casesand removing stopwords, and converting all plural forms to singular forms.2.
Create document word vectors: each document was processed to record theunique words and their frequencies.
We built the local word vector for eachdocument, each vector entry will record a single word and its frequency.
Wealso keep track of the unique words in the whole texts to be tested.
Afterprocessing all the documents, we convert each local vector to a global vectorusing the overall unique words.3.
Compute the identical word similarity score among documents: given anytwo documents, if we have their global vectors x, y, we can use the cosinemeasure [5] to calculate the identical word similarity score between thesetwo texts.cos(x, y) =?ni=1 xiyi?
?ni=1 x2i?
?ni=1 y2i(1)where x and y are n-dimensional vectors in a real-valued space.Now, we determined a global vector for each text.
We also have the identicalword similarity scores among all texts.
We can directly use these global vectorsto run the k-means or the EM algorithms to cluster the texts.
We can also usethe identical word similarity scores to run grouping algorithm (defined later) todo the clustering via a different approach.3 Semantic Relation SimilarityTo prepare the texts for clustering process using semantic relation similarity, thefollowing steps are performed on each raw texts:1.
Preprocessing which consists in extracting file contents, and removing specialcharacters and numbers.2.
Extract all the nouns from the text using part-of-speech tagger (i.e.
UPenn-sylvania tagger).
The tagger parses each sentence of the input text intoseveral forms with specific tags.
We get four kinds of nouns as the results ofrunning the tagger: NN, NNS, NNP and NNPS.
We then run a process togroup all the nouns into meaningful nouns and non-meaningful nouns.
Thebasic idea is to construct the largest compound words using the possible ad-jective and following nouns, then check whether or not the compound wordshave a meaning in WordNet.
If not, we break the compound words into pos-sible smaller ones, then check again until we find the ones with meanings in282 Y. Chali and S. NoureddineWordnet.
When we get a noun (or a compound noun) existing in WordNet,we insert it into the meaningful word set, which we call set of regular nouns,otherwise we insert it into the non-meaningful word set, which we call set ofproper nouns.During the processing of each document, we save the over-all uniquemeaningful nouns in an over-all regular nouns set.
Because of the big over-head related to accessing WordNet, we try to reduce the overall access timesto a minimal level.
Our approach is to use these over-all unique nouns toretrieve the relevant information from WordNet and save them in a globalfile.
For each sense of each unique noun, we save its synonyms, two levelhypernyms, and one level hyponyms.
If any process frequently needs theWordNet information, it can use the global file to store the information in ahash and thus provides fast access to its members.3.
Word sense disambiguation.Similarly to Galley and McKeown [6], we use lexical chain approachto disambiguate the nouns in the regular nouns for each document [7,8].A lexical chain is a sequence of related words in the text, spanning short(adjacent words or sentences) or long distances (entire text).
WordNet isone lexical resource that may be used for the identification of lexical chains.Lexical chains can be constructed in a source text by grouping (chaining)sets of word-senses that are semantically related.
We designate the followingnine semantic relations between two senses:(a) Two noun instances are identical, and used in the same sense;(b) Two noun instances are used in the same sense (i.e., are synonyms );(c) The senses of two noun instances have a hypernym/hyponym relationbetween them;(d) The senses of two noun instances are siblings in the hypernym/hyponymtree;(e) The senses of two noun instances have a grandparent/grandchild relationin the hypernym/hyponym tree;(f) The senses of two noun instances have a uncle/nephew relation in thehypernym/hyponym tree;(g) The senses of two noun instances are cousins in the hypernym/hyponymtree (i.e., two senses share the same grandparent in the hypernym treeof WordNet);(h) The senses of two noun instances have a great-grandparent/great-grand-child relation in the hypernym/hyponym tree (i.e., one sense?s grand-parent is another sense?s hyponym?s great-grandparent in the hypernymtree of WordNet).
(i) The senses of two noun instances do not have any semantic relation.To disambiguate all the nouns in the regular nouns of a text, we proceedwith the following major steps:(a) Evaluate the semantic relation between any two possible senses accordingto the hypernym/hyponym tree in WordNet.
For our experiments, we useDocument Clustering with Grouping and Chaining Algorithms 283the following scoring scheme for the relations defined above as shown inTable 1.
The score between Ai (sense i of word A) and Bj (sense j of wordB) is denoted as score(Ai, Bj).
These scores are established empiricallyand give more weight to closer words according to WordNet hierarchy.Table 1.
Scoring Scheme for RelationsRelation Score(Ai, Bj)Identical log(16)Synonyms log(15)Hypernyms/hyponyms log(14)Siblings log(13)Grandparent/grandchild log(12)Uncle/nephew log(11)Cousins log(10)Great-grandparent/great-grandchild log(9)No relation 0(b) Build the lexical chains using all possible senses of all nouns.
To buildthe lexical chains, we assume each noun possesses all the possible sensesfrom WordNet.
For each sense of each noun in a text, if it is related to allthe senses of any existing chain, then we put this sense into this chain,else we create a new chain and push this sense into the new empty chain.After this, we will have several lexical chains with their own scores.
(c) Using the lexical chain, try to assign a specific sense to each nouns.We sort the chains by their scores in a non-increasing order.
We selectthe chain with the highest score and assign the senses in that chainto the corresponding words.
These words are disambiguated now.
Next,we process the next chain with the next highest score.
If it contains adifferent sense of any disambiguated words, we skip it to process thenext chain until we reach the chains with a single entry.
We mark thechains which we used to assign senses to words as selected.
For the singleentry chains, if the sense is the only sense of the word, we mark it asdisambiguated.
For each undisambiguated word, we check each of itssenses against all the selected chains.
If it has a relation with all thesenses in a selected chain, we will then remember which sense-chainpair has the highest relation score, then we assign that sense to thecorresponding word.After these steps, the leftover nouns will be the undisambiguated words.
Wesave the disambiguated words and the undisambiguated words with theirfrequencies for calculating the semantic relation scores between texts.4.
Compute the similarity score for each pair of texts.Now, we should have three parts of nouns for each text: disambiguatednouns, undisambiguated nouns and the non-meaningful nouns (propernouns).
We will use all of them to calculate the semantic similarity scores284 Y. Chali and S. Noureddinebetween each pair of texts.
For the purpose of calculating the semantic sim-ilarity scores among texts, we use only the first three relations (a), (b), and(c) and the last relation (i) and their corresponding scores defined in Table 1.For a given text pair, we proceed as in the following steps to calculate thesimilarity scores:?
Using the disambiguated nouns, the score score1 of the similarity be-tween two texts T1 and T2 is computed as follows:score1 =?ni=1?mj=1 score(Ai, Bj) ?
freq(Ai) ?
freq(Bj)?
?ni=1 freq2(Ai)?
?mj=1 freq2(Bj)(2)where Ai is a word sense from T1 and Bj is a word sense from T2;score(Ai, Bj) is a semantic relation score defined in Table 1; n and mare the numbers of disambiguated nouns in T1 and T2; freq(x) is thefrequency of a word sense x.?
For the undisambiguated nouns, if two nouns are identical in their wordformats, then the probability that they take the same sense in bothtexts is 1/s, where s is the number of their total possible senses.
Thesimilarity score score2 between two texts T1 and T2 according to theundisambiguated nouns is computed as follows:score2 =?ni=1log(16)?freq1(Ai)?freq2(Ai)si?
?ni=1 freq21(Ai)?
?nj=1 freq22(Aj)(3)where Ai is a word common to T1 and T2; n is the number of commonwords to T1 and T2; freq1(Ai) is the frequency of Ai in T1; freq2(Ai) isthe frequency of Ai in T2; si is the number of senses of Ai.?
The proper nouns are playing an important role in relating texts to eachother.
So, we use a higher score (i.e., log(30)) for the identical propernouns.
The similarity score score3 between two texts T1 and T2 amongthe proper nouns between is computed as follows:score3 =?ni=1 log(30) ?
freq1(Ai) ?
freq2(Ai)?
?ni=1 freq21(Ai)?
?nj=1 freq22(Aj)(4)where Ai is a proper noun common to T1 and T2; n is the number ofcommon proper nouns to T1 and T2; freq1(Ai) is the frequency of Ai inT1; freq2(Ai) is the frequency of Ai in T2.?
Adding all the scores together as the total similarity score of the textpair:score = score1 + score2 + score3 (5)Now we make it ready to use the grouping algorithm or chaining algorithmdefined shortly to cluster the texts.Document Clustering with Grouping and Chaining Algorithms 2854 Clustering AlgorithmsGenerally, every text should have a higher semantic similarity score with thetexts from its group than the texts from a different groups [9].
There are afew rare cases where this assumption could fail.
One case is that the semanticsimilarity score does not reflect the relationships among the texts.
Another caseis that the groups are not well grouped by common used criteria or the topic istoo broad in that group.By all means, the texts of any well formed clusters shouldhave stronger relations among its members than the texts in other clusters.
Basedon this idea, we developed two text clustering algorithms: grouping algorithmand chaining algorithm .
They share some common features but with differentapproaches.One major issue in partitioning texts into different clusters is choosing thecutoff on the relation scores.
Virtually, all texts are related with each other tosome extent.
The problem here is how similar (or close) they should be so thatwe can put them into one cluster and how dissimilar (or far away) they shouldbe so that we can group them into different clusters.
Unless the similarity scoresamong all the texts can be represented as binary values, we will always face thisproblem with any kind of texts.
In order to address this problem, we introducetwo reference values in our text clustering algorithms: high-threshold and low-threshold.
The high-threshold means the high standard for bringing two textsinto the same cluster.
The low-threshold means the minimal standard for possiblybringing two texts into the same cluster.
If the score between any two textsreaches or surpasses the high-threshold, then they will go to the same cluster.If the score reaches the low-threshold but is lower than the high-threshold, thenwe will carry out further checking to decide if we should bring two texts into thesame cluster or not, else, the two texts will not go to the same cluster.We get our high-threshold and low-threshold for our different algorithms byrunning some experiments using the grouped text data.
The high-threshold weused for our two algorithms is 1.0 and the low-threshold we used is 0.6.
For ourexperiment, we always take a number of grouped texts and mix them up to makea testing text set.
So, each text must belong to one cluster with certain numberof texts.4.1 Grouping AlgorithmThe basic idea is that each text could gather its most related texts to forman initial group, then we decide which groups have more strength over othergroups, make the stronger groups as final clusters, and use them to bring anypossible texts to their clusters.
First, we use each text as a leading text (Tl) toform a cluster.
To do this, we put all the texts which have a score greater thanthe high-threshold with Tl into one group and add each score to the group?stotal score.
By doing this for all texts, we will have N possible different groupswith different entries and group scores, where N is the number of the total textsin the set.
Next, we select the final clusters from those N groups.
We arrangeall the groups by their scores in a non-increasing order.
We choose the group286 Y. Chali and S. Noureddinewith the highest score and check if any text in this group has been clusteredto the existing final clusters or not.
If not more than 2 texts are overlappingwith the final clusters, then we take this group as a final cluster, and remove theoverlapping texts from other final clusters.
We process the group with the nexthighest score in the same way until the groups?
entries are less than 4.
For thosegroups, we would first try to insert their texts into the existing final clusters ifthey can fit in one of them.
Otherwise, we will let them go to the leftover clusterwhich holds all the texts that do not belong to any final clusters.
The followingis the pseudocode for the grouping algorithm:Grouping Algorithm// Get the initial clustersfor each text ticonstruct a text cluster including all the texts(tj)which score(ti, tj) >= high-threshold;compute the total score of the text cluster;find out its neighbor with maximum relation score;end for// Build the final clusterssort the clusters by their total score in non-increasing order;for each cluster gi in the sorted clustersif member(gi) > 3 and overlap-mem(gi) <= 2take gi as a final cluster ci;mark all the texts in ci as clustered;elseskip to process next cluster;end ifend for// Process the leftover texts and insert them into one of the final clustersfor each text tjif tj has not been clusteredfind cluster ci with the highest score(ci, tj);if the average-score(ci, tj) >= low-thresholdput tj into the cluster ci;else if the max score neighbor tm of tj is in ckput tj into cluster ck;elseput tj into the final leftover cluster;end ifend ifend foroutput the final clusters and the final leftover cluster;Document Clustering with Grouping and Chaining Algorithms 287where: member(gi) is the number of members in group gi; overlap-mem(gi) isthe number of members that are overlapped with any final clusters; score(ci,tj) is the sum of scores between tj and each text in ci; average-score(ci, tj) isscore(ci, tj) divide by the number of texts in ci.4.2 Chaining AlgorithmThis algorithm is based on the observation of the similarities among the texts ingroups.Within a text group, not all texts are always strongly relatedwith any othertexts.
Sometimes there are several subgroups existing in a single group, i.e., cer-tain texts have stronger relations with their subgroup members and have a weakerrelation with other subgroup members.
Usually one or more texts have strongerrelation crossing different subgroups to connect them together, otherwise all thetexts in the group could not be grouped together.
So, there is a chaining effect ineach group connecting subgroups together to form one entire group.We use this chaining idea in the chaining algorithm.
First, for each text Tj ,we find all the texts which have similarity scores that are greater or equal thanthe high-threshold with Tj and use them to form a closer-text-set.
All the textsin that set are called closer-text of Tj.Next, for each text which has not been assigned to a final chain, we use itsinitial closer-text-set members to form a new chain.
For each of the texts inthe chain, if any of its closer-texts are relatively related (i.e., the score >= low-threshold) to all the texts in the chain, then we add it into the current chain.One thing needs to be noticed here is that we do not want to simply bringall the closer-texts of each current chain?s members into the chain.
The reasonis to eliminate the unwanted over-chaining effect that could bring many othertexts which are only related to one text in the existing chain.
So, we check eachcandidate text against all the texts in the chain to prevent the over-chainingeffect.
We repeat this until the chain?s size are not increasing.
If the chain hasless than 4 members, we will not use this chain for a final cluster and try tore-assign the chain members to other chains.After the above process, if any text has not been assigned to a chain we checkit against all existing chains and find the chain which has highest similarity scorebetween the chain and this text.
If the average similarity score with each chainmembers is over low-threshold,we insert this text into that chain, else we put it intothe final leftover chain.
The following is the pseudocode for the chaining algorithm:5 ApplicationWe chose as our input data the documents sets used in the Document Under-standing Conferences [10,11], organized by NIST.
We collected 60 test documentdirectories for our experiments.
Each directory is about a specific topic and hasabout 10 texts and each text has about 1000 words.
Our experiment is to mixup the 60 directories and try to reconstitute them using one of our clustering288 Y. Chali and S. NoureddineChaining Algorithm// construct a closer-text-set for each textfor each text ti 0 < i <= Nfor each text tj 0 < j <= Nif score(ti, tj) >= high-thresholdput tj into closer-text-set si;end ifend forend for// Build the chainsc = 0;for each text ti of all the textsif it has not been chained input text ti into chain c and mark it as been chained;bring all the text in closer text-set si into the new chain c;mark si as processed;while (the size of chain c is changing)for each text tk in chain cfor each text tm in sk of tkif the score between tm and any text in chain c >= low-thresholdput tm into chain c;mark tm as been chained to chain c;end ifend forend forend whileif the size of chain c < 4discard chain c;remark the texts in chain c as unchained;end ifc++;end ifend for// Process the leftover texts and insert them into one of the existing chainsfor each unchained text tjfind chain ci with the highest score(ci, tj);if the average-score(ci, tj) >= low-thresholdput tj into the chain ci;elseput tj into the final leftover chain;end ifend foroutput the valid chains and the final leftover chain.Document Clustering with Grouping and Chaining Algorithms 289algorithm.
Then, we measure how successful are these algorithms in reconstitut-ing the original directories.
We implemented the k-means algorithm and the EMalgorithm to compare them with our algorithms.In our test, we found out that the chaining algorithm did not work wellfor identical method.
We tested grouping algorithm, chaining algorithm, andEM algorithm with semantic method, and k-means algorithm, EM algorithm,and grouping algorithm with identical methods.
We run the k-means and theEM algorithms 4 times with each experiment texts set and take the averageperformance.
As we described before, semantic method represents text relationswith scores, so k-means algorithm which needs input data in vector format willnot be applied to semantic method.6 EvaluationFor our testing, we need to compare the system clusters with the testing clusters(original text directories) to evaluate the performance of each system.
We firstcompare each system cluster with all of the testing clusters to find the bestmatched cluster pair with the maximum number of identical texts.
We then userecall, precision, and F-value to evaluate each matching pair.
Finally, we use theaverage F-value to evaluate the whole system performance.
For a best matchedpair TCj (testing cluster) and SCi (system cluster), the recall (R), precision (P),and F-value (F) are defined as follows:R =mt(6)P =mm + n(7)F (TCj, SCi) =2PRP + R(8)where m is the number of the overlapping texts between TCj and SCi; n is thenumber of the non-overlapping texts in SCi; t is the total number of texts inTCj.For the whole system evaluation, we use the Average F which is calculatedusing the F-values of each matched pair of clusters.Average F =?i,j max(F (SCi, TCj))max(m, n)(9)Where i <= min(m, n), j <= m, m is the number of testing clusters, and nis the number of system clusters.290 Y. Chali and S. Noureddine7 ResultsThe performance of grouping algorithm and chaining algorithm are very closeusing the semantic relation approach and most of their Average F are over90%.
For the identical word approach, the grouping algorithm performanceis much better than the performances of the k-means algorithm and the EMalgorithm.
The poor performance of the k-means algorithm results from ran-domly selected k initial values.
Those initial N-dimensional values usually donot represent the whole data very well.
For the semantic relation approach,both grouping and chaining algorithms performed better than theEM algorithm.Table 2 and 3 are the system Average F values for the different algorithms.The identical word similarity method used grouping algorithm, k-means algo-rithm, and EM algorithm.
The semantic similarity method used grouping algo-rithm, chaining algorithm and EM algorithm.Table 2.
Comparisons of F-value using Identical Word SimilarityIdentical Word SimilarityGrouping EM k-means0.98 0.81 0.66Table 3.
Comparisons of F-value using Semantic Relation SimilaritySemantic Relation SimilarityGrouping Chaining EM0.92 0.91 0.768 ConclusionDocument clustering is an important tool for natural language applications.
Wepresented two novel algorithms grouping algorithm and chaining algorithm forclustering sets of documents, and which can handle a large set of documentsand clusters.
The two algorithms use semantic similarity and identical wordmeasure, and their performance is much better than the performance of the K-means algorithm and the performance of the EM algorithm, used as a baselinefor our evaluation.Evaluating the system quality has been always a difficult issue.
We presentedan evaluation methodology to assess how the system clusters are related to themanually generated clusters using precision and recall measures.The grouping and the chaining algorithm may be used in several naturallanguage processing applications requiring clustering tasks such as summarizingset of documents relating the same event.Document Clustering with Grouping and Chaining Algorithms 291AcknowledgmentsThis work was supported by the Natural Sciences and Engineering ResearchCouncil (NSERC) research grant.References1.
Manning, C.D., Schutze, H.: Foundations of Statistical Natural Language Process-ing.
MIT Press (2000)2.
Berkhin, P.: Survey of clustering data mining techniques.
Technical report, AccrueSoftware, San Jose, CA (2002)3.
Duda, R., Hart, P.: Pattern Classification and Scene Analysis.
John Wiley & Sons,New York, NY (1973)4.
Miller, G.A., Beckwith, R., Fellbaum, C., Gross, D., Miller, K.: Five papers onwordnet.
CSL Report 43, Cognitive Science Laboratory, Princeton University(1993)5.
Salton, G.: Automatic Text Processing: The Transformation, Analysis, and Re-trieval of Information by Computer.
Addison-Wesley Series in Computer Sciences(1989)6.
Galley, M., McKeown, K.: Improving word sense disambiguation in lexical chaining.In: Proceedings of the 18th International Joint Conference on Artificial Intelligence,Acapulco, Mexico.
(2003)7.
Barzilay, R., Elhadad, M.: Using lexical chains for text summarization.
In: Pro-ceedings of the 35th Annual Meeting of the Association for Computational Linguis-tics and the 8th European Chapter Meeting of the Association for ComputationalLinguistics, Workshop on Intelligent Scalable Text Summarization, Madrid (1997)10-178.
Silber, H.G., McCoy, K.F.
: Efficiently computed lexical chains as an intermediaterepresentation for automatic text summarization.
Computational Linguistics 28(2002) 487?4969.
Pantel, P., Lin, D.: Document clustering with committees.
In: Proceedings of theACM SIGIR?02, Finland (2002)10.
Over, P., ed.
: Proceedings of the Document Understanding Conference, NIST(2003)11.
Over, P., ed.
: Proceedings of the Document Understanding Conference, NIST(2004)
