Efficient Integrated Tagging of Word ConstructsAndrew BredenkampFrederik FouvryDept.
Language and LinguisticsUniversity of EssexWivenhoe ParkColchesterEssex CO4 3SQUnited KingdomThierry DeclerckIMSUniversity of StuttgartD-70174 StuttgartGermanythierry@ims, uni-stuttgart, de{andrewb, fouvry}~essex .ac .ukBradley MusicCenter for SprogteknologiNjalsgade 80DK-2300 Copenhagen SDenmarkmusic~cst, ku.
dkAbstractWe describe a robust ext-handling com-ponent, which can deal with free text ina wide range of formats and can suc-cessfully identify a wide range of phe-nomena, including chemical formulae,dates, numbers and proper nouns.
Theset of regular expressions used to cap-ture numbers in written form ("sech-sundzwanzig") in German is given asan example.
Proper noun "candidates"are identified by means of regular ex-pressions, these being then rejected oraccepted on the basis of run-time in-teraction with the user.
This taggingcomponent isintegrated in a large-scalegrammar development environment, andprovides direct input to the grammat-ical analysis component of the systemby means of "lift" rules which converttagged text into partial inguistic struc-tures.1 Motivation1.1 The prob lem : messy  deta i lsMessy details are text constructs which do notlend themselves well to treatment by traditionaltechniques for linguistic analysis, whence their'messiness'.
Typical examples are numbers, codesor other (sequences of) word-forms which can oc-cur in many variations (often infinite), making im-possible a comprehensive tr atment by traditionalmeans.There are various types of phenomena classi-fied as messy details which can be subclassifiedaccording to at what level of generality as regardstext structure they occur, viz.
general formatlevel, sentence level and word level phenomena.General .format level phenomena occur over sen-tence boundaries, example being headers, meta-comments and tables.
Phenomena classified assentence level occur within a single sentence, butcannot be considered word constructs of a fixednature.
These are more 'linguistic' than the usualmessy details, but are considered messy detailssince they lend themselves topartial analysis via asimilar type of pre-processing.
Examples of theseare the use of parentheses and commas which canbe used within practical implementations as a ba-sis for segmentation during pre-processing.Word level phenomena are usually the most fre-quently occurring messy details, including suchthings as dates, document references of varioussorts, codes, numbers and proper nouns.
For anyrealistic application these types of construct mustbe processed efficiently, the alternative being cod-ing them individually in some lexicon and/or im-plementing sets of grammar rules for parsing themsyntactically.This problem area was given priority in the EU-sponsored LSGRAM project (LRE 61-029) whichaimed to integrate an approach to messy detailsinto a large-scale grammar implementation.
Thecoverage of the grammars developed was based oncorpus analyses within each language group of theproject, these revealing a large number of messydetails of the types mentioned.
What was calledfor then was an efficient means of identifying word-level messy details (or word constructs) such thatthey could be processed in a general way, avoid-ing additional grammar rules and the need for aninfinite number of lexical entries.2 The basic approach2.1 Identif ication using regularexpress ionsThe types of word construct of interest here lendthemselves well to identification by matching reg-ular expressions over each input sentence (con-sidered as a record), tagging them as specific in-stances of general phenomena (e.g.
dates, num-bers, etc.
).awk is a programming language specifically de-signed for in this type of string manipulation1028(matching, replacement, splitting).
It has specialprovisions for treating text in the form of records.auk reads input record by record, matching user-defined regular expressions and executing corre-sponding actions according to whether a matchhas been retold.The regular expressions can be stored in vari-ables and reused to build more complex expres-sions.
This is important, as some of the phenom-ena we were attempting to match were complex(see below) and occurred in a vm'iety of formats.The auk-implemented tagger developed for thisproject, tag \ [ t ,  can be used as a stand-alone tag-ger for SGML texts.
It has been integrated withinthe text handling procedures of the ALEP systemafter sentence recognition and before word recog-nition.
When a pattern matches against the in-put, the matched string is replaced with a generaltag of the relevant ype (e.g.
DATE, NUMBER).
Sub-sequent agging and morphological parsing thenskip these tags, and further processing (i.e.
syn-tactic analysis) is based on the tag value, not theoriginal input string.2.2 Sample  case : cur rency  pat terns  inGermantag i t  has been integrated into the German LS-GRAM grmnmar for the identification of wordconstructs occurring in the mini-corpus taken asthe departure point for the work of the Germangroup, consisting of an article on economics (takenfrom the weekly newspaper "Die Zeit").
As usualwhen using 'real-world' texts, mm~y messy detailswere found, including dates and numbers usedwithin t)ercentages and, as would be expectedfrom the text type, within amounts of currency.These occur both with and without numerals, e.g.
"16,7 Millionen Dollar", "Sechsundzwanzig Mil-lim'den D-Mark".
The text examples are espe-cially problematic given the German method ofexpressing the ones-digit before the tens-digit, e.g.
"Sechsundzwmmig" is literally "six-and-twenty".In order to deal with this phenomenon, reg-ular expression patterns describing the currencymnounts were defned in awk.
First, patterns forcardinals were specified, e.g}~Umlauted characters and "if' are matched by thesystem, though they are not shown here.Note that regular expressions are specified as stringsand must be quoted using pairs of double quotes.
Vari-ables are not evaluated when they occur in quotes, soquoting is ended, and then restarted after the vari-able name, whence the proliferation of double quoteswithin the complex patterns.Some auk syntax : "=" is the assignment operator,parentheses are used for grouping, "1" is the disjunc-tion operator, "?"
indicates optionality of the preced-ing expression, "+" means one or more instances of thetwo_to_nlne : "( \[Zz\] .e i  I \[Dd\] re i  I \[Vv\] ie r  I " \\[Ff\] unf I \[Ss\] echs I \[Ss\] ieben I \[Aa\] cht i \[Nn\] eun)"one_to_nine = "( lee\] in \["two_to_nine")"card = "("one to_nine")"number = "\[0-9\]+(, \[0-9\]+)?
"range = "("number" i "card")"The actual pattern used in the implementationis more complex and goes up to 999, but the ex-ample shows the principle.
Given this set of vari-ables, the pattern assigned to card can matd,the text version of all cardinal numbers from \] to999, e.g.
"Drei", "Neunzehn", "Zweiundzwanzig","Achthundert Ffinflmdvierzig", etc.
The valueassigned to range can match number, optionallywith a comma as decimal point, e.g.
"99,09".
Thefollowing patterns are also needed :amount = "(Mill ionenlMill iarden)"currency = "(Mark ID-Mark I Dollar)"carmeasure = "( ("amount"  ( "cur rency" )  ?)
I " \"("currency") )"measure  = I, ("range" "curmeasure")"The last two patterns described efine measurebeing the succession of a cardinal number (as adigit or a string) followed by curmeasure, be-ing the concatenation of amount and currency.But both of them, amount and currency,  are de-fined as being optional.
So that inputs like "30,6Mill\[arden Dollar", "Zweiundzwanzig Dollar" or"Dreiundvierzig Mill\[arden Dollar" are automati-cally recognized.
But the definition of 'measure'disallows the tagging of "Zweiundzwanzig" as a'measure' expression.
The tag provided for thisstring will be the same as for any other cardinals.tag i t  applies these patterns to each recordwithin the input, assigning the appropriate taginformation in case a match is found.
Further pro-cessing is described below.3 Extens ion  fo r  p roper  nouns  :in teract ive  tagg ingProper nouns present another problem that fallsunder messy details.
A small extract from the cor-pus used for tim English grammar showed a widerange of possible proper noun configurations :"James Sledz", "Racketeer Influenced and Cor-rupt Organizations", "Sam A.
Call", "Mr. Ya-suda", "Mr. Genji Yasuda", .
.
.Regular expressions can catch several of thosecases, but it is difficult to get certainty, e.g.
"ThenYasuda .
.
. "
vs "Genii Yasuda" : one can neverbe sure that an English word is not a name inanother language.
Since this is a pre-processingtreatment, here is no disambiguating informationpresent, and fully automatic tagging cannot bepreceding expression, square brackets urround alter-native characters (possible specified as a range, e.g.
"\[0-9\] ").1029done, unless the program can have access to eithersome lookup facility and/or can iater~ct with ahuman user.3.1 Pat terns  for p roper  nounsFor financial texts, the domain of our referencecorpus, the proper nouns are company or institu-tion names and person names.
Product and com-pany names can be very unconventional.
There-fore the regular expressions need to be rather gen-erous.
The interaction with the user and the dic-tionaries will provide a way to tune the effect ofthese expressions.We defined the proper noun regular expressionto be nearly anything, preceded by a capital.
Per-son names can contain initials, and they might bemodified by titles ("Mr", .
.
. )
or functions, busi-ness names can be modified by some standard ter-minology (like "Ltd.").
Lower case words are al-lowed if they are not longer than three chm'acters(for nmnes containing "and" etc.
).3.2 In teract ing  w i th  the  userTagging proper nouns presents a special prob-lem, since, unlike the case of numbers and dates,there is a great deal of uncertainty involvedas to whether something is a proper noun ornot.
Therefore a natural extension to tag i t  wasthe implementation of an interactive capabilityfor confirming certain tag types such as propernouns.
2If a proper noun is found, then the tagger firstdoes some lookup to limit the number of interac-tions during the tagging.
We used the two follow-ing heuristics :1.
Has it already been tagged as a proper noun ?If so, do it again.2.
Has it already been offered as a proper noun,but was it rejected ?
If so, and if it occurs atthe beginning of a sentence, reject it again.Those two checks are kept exclusively disjunc-tive.
If a word occurs both as a proper noun andas a "non-proper noun", the user will be asked ifhe or she wants it to be tagged.
This allows one touse different name dictionaries for different exts.If the program itself is certain that a propernoun is found, then it tags it and goes on to anext match.
Otherwise it asks the user what todo with the match that was found.
There are twopossible answers to this question :1.
The user accepts  the match as a propernoun.
The program tags it, stores it for fu-ture use, and proceeds.2The graphical interface to the interactive tool hasbeen implemented in Tel/Tk.When the match is not entirely a propernoun, the matching string can be edited.
Thisconsists of removing the words before and/orafter the first proper noun in the match.
3 Theremaining substring of the match is tagged asa proper noun and stored.
The words beforethe first word are skipped (and also stored);everything that comes after the tagged propernoun is resubmitted.2.
The user re jects  the match that is offered.The program stores it (as a "non-propernoun") and proceeds.4 Integration with linguisticanalysisThe ALEP platform (Alshawi et al, 1991) pro-vides the user with a Text Handling (TH) com-ponent which allows a "pre-processing" of input.An ASCII text will first go through a processingchain consisting in a SGML-based tagging of theelements of the input.
The default setup of thesystem defines the following processing chain : thetext is first converted to an EDIF (Eurotra Doc-ument Interchange Format) format.
Then threerecognition processes are provided : paragraphrecognition, sentence recognition and word recog-nition.
The output from those processes consist ofthe input decorated with tags for the recognizedelements : 'p' for paragraphs, 'S' for sentences, 'W'for words (in case of morphological nalysis, thetag '/4' is provided for morphemes) and 'PT' forpunctuation signs.
Some specialized features arealso provided for the tagged words, allowing tocharacterize them more precisely, so for exmnple'ACR0' for acronyms and so on.So the single input "John sees Mary."
after be-ing processed by the TH component will take thefo rm :<P> <S> <W>John</W><W>sees</W><W>Mary</W><PT>.
</PT><IS><IP><P> and </P> mark the beginning and the re-spective ending of the recognized paragraph struc-ture.
The other tags must be interpreted analo-gously.In the default case, it this this kind of infor-mation which is the input to the TH-LS compo-nent (Text-Handling to Linguistic Structure) ofthe system.
Within this component, one specifiesso-called 'tsAs' (text structure to linguistic struc-ture) rules, which transfornl the TH output into3To extend the matches, the user would need tochange the regular expressions.1030partial inguistic structure (in ALEP terminology,this conversion is called lifting).
The syntax ofthese lift rules is the following :ts_is_rule( <id>, <tag_name>,\[<features>f, <tag content> )where : <ld> is a Linguistic Description (LD);<tag_name> is the name of an SGML tag (e.g.
'S','W'); <features> is a list of feature-value d scrip--tions of the tag's features; <tag content> is tileatomic content of tile string within the tag (op-tional in the lift rule).This kind of mapping rule allows a flow of in-formation between text structures and linguisticstructures.
So if the input is one already havingPoS information (as the result of a corpus tag-ging), tim TH-LS is the appropriate place to as-sure the flow of information.
This allows a consid-erable improvement of parse time, since some in-formation is already instantiated before the parsestarts.The TIt component of the ALEP platform alsoforesees the integration of user-defined tags.
Thetag <USR> is used if the text is tagged by a user-defined tagger, as is done when processing messydetails.When tag i t  matches a pattern against im in-put, the matched string is replaced with an appro-priate USR tag.
Thus "l)reiundvierzig Milliardenl)ollm'" is matched by the pattern measure  (seeabove), and is replaced by the SGML markup <USRVAL="Dreiundvierzig Milliarden Dollar" LEVEL=MTYPE=MEhSURE>Dreiundvierzig Mil liarden_Dol lar</USR>Note that tile matched sequence is copied intothe attribute VAL and that in the data con-tent spaces are replaced by underscores.
Forsome pattern types, a generalized representationof the matched sequence is computed and storedin an attribute CONY.
For instance, when thepattern for dates matches the input "March 15,1995", CONV is assigned a standardized version,i.e.
CONV="95/03/15".This version with USR tags inserted is then pro-cessed by the set of lift rules.
The \[bllowing en-eral lift rule does the conversion for all USR tags :ts_is rule(Id:{ sign => sign:{string => STRING,synsem => synsem:{syn => syn:{constype => morphol:{lemma => VALvaluo,lu => TYPEvalue } } } } },'USR ' , \[ 'TYPE' =>TYPEvalue, ' VAL' =>VALvalue\] ,STRING ).Here we (:an see tile mapping of inforlnationbetween the user-defined tlS/t tag (the attributesof which are listed in tim last line of this rule)and the linguistic description ('ld'--'linguistic de-scription', a structured type within tile TypedFeature System), using the rule-internal variableTYPEvalue: the value of the attribute TYPE is as-signed to the lexical unit ('lu') value of the lin-guistic description.
After applying this rule to theresult of matching "Dreiundvierzig Dollar", the ldis the following :id:{ sign => sign:{string => 'Dreiundvierzig_Dollar',synsem => synsem:{syn => syn:{constype => morphol :{lemma => 'Dreiundvierzig Dollar'lu => 'M~ASURE' } } } } }Although the original input sequence is avail-able as the value of the feature lemma, further pro-cessing is based solely on the lu value ' MEASURE',thus making it possible to have a single lexicalentry for handling all sequences matched by thepattern measure  shown above.
The definition ofsuch generic entries in the lexicon keeps the lexi-con smaller by dealing with what otherwise couhlonly be coded with an infinite number of entries.In addition, treating such word constrncts as a siregle unit gives a significant improvement in parsingruntirne, since only the string 'MEASURE' is usedas a basis for further processing, instead of theoriginal sequence of three words.
Finally, runtinmis also improved and development eased by thefact that no grammar ules need be defined forparsing such sequences.5 ConclusionThe implementation described here handles a va-riety of word-level messy details efficiently, speed-ing up overall processing time and simplifying thegrammars and lexica.
General format level andsentence l vel phenomena c n be handled in a sim-ilar way.
Within our project, reimplementationusing a more powerful tool per1 is taking place,allowing filrther extensions to the flmctionality.We maintain that user-interaction combinedwith some table lookup is the only viable approachto the robust agging of free texts.
The fact thatan interactive tagging tool can be so easily inte~grated in to the linguistic processing system is ofobvious and considerable benefit.ReferencesAlshawi H., Arnold \]).
J., Backofen R.., Carter1).
M., Lindop J., Netter K., Pulrnar~ S., Tsuji,I., Uszkoreit 11.
1991. l'\]urotra ET6/h Rule~brmalism and Virtual Machine Design Study(final report).
CEC1031,
