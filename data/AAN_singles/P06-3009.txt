Proceedings of the COLING/ACL 2006 Student Research Workshop, pages 49?54,Sydney, July 2006. c?2006 Association for Computational LinguisticsIntegrated Morphological and Syntactic Disambiguationfor Modern HebrewReut TsarfatyInstitute for Logic, Language and Computation, University of AmsterdamPlantage Muidergratch 24, 1018 TV Amsterdam, The Netherlandsrtsarfat@science.uva.nlAbstractCurrent parsing models are not immedi-ately applicable for languages that exhibitstrong interaction between morphologyand syntax, e.g., Modern Hebrew (MH),Arabic and other Semitic languages.
Thiswork represents a first attempt at model-ing morphological-syntactic interaction ina generative probabilistic framework to al-low for MH parsing.
We show that mor-phological information selected in tandemwith syntactic categories is instrumentalfor parsing Semitic languages.
We furthershow that redundant morphological infor-mation helps syntactic disambiguation.1 IntroductionNatural Language Processing is typically viewedas consisting of different layers,1 each of which ishandled separately.
The structure of Semitic lan-guages poses clear challenges to this traditionaldivision of labor.
Specifically, Semitic languagesdemonstrate strong interaction between morpho-logical and syntactic processing, which limits theapplicability of standard tools for, e.g., parsing.This work focuses on MH and explores theways morphological and syntactic processing in-teract.
Using a morphological analyzer, a part-of-speech tagger, and a PCFG-based general-purposeparser, we segment and parse MH sentences basedon a small, annotated corpus.
Our integratedmodel shows that percolating morphological am-biguity to the lowest level of non-terminals in thesyntactic parse tree improves parsing accuracy.1E.g., phonological, morphological, syntactic, semanticand pragmatic.Moreover, we show that morphological cues facil-itate syntactic disambiguation.
A particular contri-bution of this work is to demonstrate that MH sta-tistical parsing is feasible.
Yet, the results obtainedare not comparable to those of, e.g., state-of-the-art models for English, due to remaining syntacticambiguity and limited morphological treatment.We conjecture that adequate morphological andsyntactic processing of MH should be done in aunified framework, in which both levels can inter-act and share information in both directions.Section 2 presents linguistic data that demon-strate the strong interaction between morphologyand syntax in MH, thus motivating our choice totreat both in the same framework.
Section 3 sur-veys previous work and demonstrates again theunavoidable interaction between the two.
Sec-tion 4.1 puts forward the formal setting of an inte-grated probabilistic language model, followed bythe evaluation metrics defined for the integratedtask in section 4.2.
Sections 4.3 and 4.4 thendescribe the experimental setup and preliminaryresults for our baseline implementation, and sec-tion 5 discusses more sophisticated models we in-tend to investigate.2 Linguistic DataPhrases and sentences in MH, as well as Arabicand other Semitic languages, have a relatively freeword order.2 In figure 1, for example, two distinctsyntactic structures express the same grammaticalrelations.
It is typically morphological informa-tion rather than word order that provides cues forstructural dependencies (e.g., agreement on gen-der and number in figure 1 reveals the subject-predicate dependency).2MH allows for both SV and VS, and in some circum-stances also VSO, SOV and others.49SNP-SBJDhtheNildchild.MSVPVicago.out.MSPPPmfromNPDhtheNbithouseSPPPmfromNPDhtheNbithouseVPVicago.out.MSNP-SBJDhtheNildchild.MSFigure 1: Word Order in MH Phrases (marking theagreement features M(asculine), S(ingular))S-CNJCC?wandSSBARRELkfwhenSPPPmfromNPDhtheNbit?houseVPV?ica?go.outNPD?htheNild?boyS...Figure 2: Syntactic Structures of MH Phrases(marking word boundaries with ?
?
)Furthermore, boundaries of constituents in thesyntactic structure of MH sentences need not co-incide with word boundaries, as illustrated in fig-ure 2.
A MH word may coincide with a singleconstituent, as in ?ica?3 (go out), it may overlapwith an entire phrase, as in ?h ild ?
(the boy), or itmay span across phrases as in ?w kf m h bit ?
(andwhen from the house).
Therefore, we concludethat in order to perform syntactic analysis (pars-ing) of MH sentences, we must first identify themorphological constituents that form MH words.There are (at least) three distinct morphologi-cal processes in Semitic languages that play a rolein word formation.
Derivational morphology is anon-concatenative process in which verbs, nouns,and adjectives are derived from (tri-)consonantalroots plugged into templates of consonant/vowelskeletons.
The word-forms in table 1, for example,are all derived from the same root, [i][l][d] (child,birth), plugged into different templates.
In addi-tion, MH has a rich array of agreement features,such as gender, number and person, expressed inthe word?s inflectional morphology.
Verbs, adjec-tives, determiners and numerals must agree on theinflectional features with the noun they comple-3We adopt the transliteration of (Sima?an et al, 2001).a.
?ild?
b.
?iild?
c.
?mwld?
[i]e[l]e[d] [i]i[l](l)e[d] mw[][l](l)a[d]child deliver a child innateTable 1: Derivational Morphology in MH ([..]mark templates?
slots for consonantal roots, (..)mark obligatory doubling of roots?
consonants.)a.
ild gdwl b. ildh gdwlhchild.MS big.MS child.FS big.FSa big boy a big girlTable 2: Inflectional Morphology in MH (markingM(asculine)/F(eminine), S(ingular)/P(lural))ment or modify.
It can be seen in table 2 that thesuffix h alters the noun ?ild ?
(child) as well as itsmodifier ?gdwl ?
(big) to feminine gender.
Finally,particles that are prefixed to the word may servedifferent syntactic functions, yet a multiplicity ofthem may be concatenated together with the stemto form a single word.
The word ?wkfmhbit ?
infigure 2, for instance, is formed from a conjunc-tion w (and), a relativizer kf (when), a prepositionm (from), a definite article h (the) and a noun bit(house).
Identifying such particles is crucial foranalyzing syntactic structures as they reveal struc-tural dependencies such as subordinate clauses,adjuncts, and prepositional phrase attachments.At the same time, MH exhibits a large-scale am-biguity already at the word level, which means thatthere are multiple ways in which a word can bebroken down to its constituent morphemes.
Thisis further complicated by the fact that most vo-calization marks (diacritics) are omitted in MHtexts.
To illustrate, table 3 lists two segmenta-tion possibilities, four readings, and five mean-ings of different morphological analyses for theword-form ?fmnh?.4 Yet, the morphological anal-ysis of a word-form, and in particular its mor-phological segmentation, cannot be disambiguatedwithout reference to context, and various morpho-logical features of syntactically related forms pro-vide useful hints for morphological disambigua-tion.
Figure 3 shows the correct analyses of theform ?fmnh ?
in different syntactic contexts.
Notethat the correct analyses maintain agreement ongender and number between the noun and its mod-ifier.
In particular, the analysis ?that counted?
(b)4A statistical study on a MH corpus has shown that theaverage number of possible analyses per word-form was 2.1,while 55% of the word-forms were morphologically ambigu-ous (Sima?an et al, 2001).50?fmnh?
?fmnh?
?fmnh?
?fmnh?
?f + mnh?shmena shamna shimna shimna she + manafat.FS got-fat.FS put-oil.FS oil-of.FS that + countedfat (adj) got fat (v) put-oil (v) her oil (n) that (rel) counted (v)Table 3: Morphological Analyses of the Word-form ?fmnh?a.
NPNildh.FSchild.FSAfmnh.FSfat.FSb.
NPNild.MSchild.MSCPRelfthatVmnh.MScounted.MSFigure 3: Ambiguity Resolution in Different Syn-tactic Contextsis easily disambiguated, as it is the only one main-taining agreement with the modified noun.In light of the above, we would want to con-clude that syntactic processing must precede mor-phological analysis; however, this would contra-dict our previous conclusion.
For this reason,independent morphological and syntactic analyz-ers for MH will not suffice.
We suggest per-forming morphological and syntactic processingof MH utterances in a single, integrated, frame-work, thereby allowing shared information to sup-port disambiguation in multiple tasks.3 Related WorkAs of yet there is no statistical parser for MH.Parsing models have been developed for differentlanguages and state-of-the-art results have beenreported for, e.g., English (Collins, 1997; Char-niak, 2000).
However, these models show impov-erished morphological treatment, and they havenot yet been successfully applied for MH parsing.
(Sima?an et al, 2001) present an attempt to parseMH sentences based on a small, annotated corpusby applying a general-purpose Tree-gram model.However, their work presupposes correct morpho-logical disambiguation prior to parsing.5In order to treat morphological phenomenaa few stand-alone morphological analyzers havebeen developed for MH.6 Most analyzers considerwords in isolation, and thus propose multiple anal-yses for each word.
Analyzers which also at-tempt disambiguation require contextual informa-tion from surrounding word-forms or a shallowparser (e.g., (Adler and Gabai, 2005)).5The same holds for current work on parsing Arabic.6Available at mila.cs.technion.ac.il.A related research agenda is the development ofpart-of-speech taggers for MH and other Semiticlanguages.
Such taggers need to address the seg-mentation of words into morphemes to which dis-tinct morphosyntactic categories can be assigned(cf.
figure 2).
It was illustrated for both MH (Bar-Haim, 2005) and Arabic (Habash and Rambow,2005) that an integrated approach towards mak-ing morphological (segmentation) and syntactic(POS tagging) decisions within the same architec-ture yields excellent results.
The present work fol-lows up on insights gathered from such studies,suggesting that an integrated framework is an ade-quate solution for the apparent circularity in mor-phological and syntactic processing of MH.4 The Integrated ModelAs a first attempt to model the interaction betweenthe morphological and the syntactic tasks, we in-corporate an intermediate level of part-of-speech(POS) tagging into our model.
The key idea is thatPOS tags that are assigned to morphological seg-ments at the word level coincide with the lowestlevel of non-terminals in the syntactic parse trees(cf.
(Charniak et al, 1996)).
Thus, POS tags canbe used to pass information between the differenttasks yet ensuring agreement between the two.4.1 Formal SettingLet wm1 be a sequence of words from a fixed vo-cabulary, sn1 be a sequence of segments of wordsfrom a (different) vocabulary, tn1 a sequence ofmorphosyntactic categories from a finite tag-set,and let pi be a syntactic parse tree.We define segmentation as the task of identi-fying the sequence of morphological constituentsthat were concatenated to form a sequence ofwords.
Formally, we define the task as (1), whereseg(wm1 ) is the set of segmentations resultingfrom all possible morphological analyses of wn1 .sn1 ?
= argmaxsn1 ?seg(wm1 )P (sn1 |wm1 ) (1)Syntactic analysis, parsing, identifies the structureof phrases and sentences.
In MH, such tree struc-tures combine segments of words that serve differ-ent syntactic functions.
We define it formally as(2), where yield(pi?)
is the ordered set of leaves ofa syntactic parse tree pi?.pi?
= argmaxpi?{pi?:yield(pi?
)=sn1 }P (pi|sn1 ) (2)51Similarly, we define POS tagging as (3), whereanalysis(sn1 ) is the set of all possible POS tag as-signments for sn1 .tn1 ?
= argmaxtn1 ?analyses(sn1 )P (tn1 |sn1 ) (3)The task of the integrated model is to find themost probable segmentation and syntactic parsetree given a sentence in MH, as in (4).
?pi, sn1 ??
= argmax?pi,sn1 ?P (pi, sn1 |wm1 ) (4)We reinterpret (4) to distinguish the morphologicaland syntactic tasks, conditioning the latter on theformer, yet maximizing for both.
?pi, sn1 ??
= argmax?pi,sn1 ?P (pi|sn1 , wm1 )?
??
?parsingP (sn1 |wm1 )?
??
?segmentation(5)Agreement between the tasks is implemented byincorporating morphosyntactic categories (POStags) that are assigned to morphological segmentsand constrain the possible trees, resulting in (7).
?pi, tn1 , sn1 ??
= argmax?pi,tn1 ,sn1 ?P (pi, tn1 , sn1 |wm1 ) (6)= argmax?pi,tn1 ,sn1 ?P (pi|tn1 , sn1 , wm1 )?
??
?parsingP (tn1 |sn1 , wm1 )?
??
?taggingP (sn1 |wm1 )?
??
?segmentation(7)Finally, we employ the assumption thatP (wm1 |sn1 ) ?
1, since segments can only beconjoined in a certain order.7 So, instead of (5)and (7) we end up with (8) and (9), respectively.?
argmax?pi,sn1 ?P (pi|sn1 )?
??
?parsingP (sn1 |wm1 )?
??
?segmentation(8)?
argmax?pi,tn1 ,sn1 ?P (pi|tn1 , sn1 )?
??
?parsingP (tn1 |sn1 )?
??
?taggingP (sn1 |wm1 )?
??
?segmentation(9)4.2 Evaluation MetricsThe intertwined nature of morphology and syn-tax in MH poses additional challenges to standardparsing evaluation metrics.
First, note that we can-not use morphemes as the basic units for com-parison, as the proposed segmentation need notcoincide with the gold segmentation for a givensentence.
Since words are complex entities that7Since concatenated particles (conjunctions et al) appearin front of the stem, pronominal and inflectional affixes at theend of the stem, and derivational morphology inside the stem,there is typically a unique way to restore word boundaries.can span across phrases (see figure 2), we can-not use them for comparison either.
We proposeto redefine precision and recall by considering thespans of syntactic categories based on the (space-free) sequences of characters to which they corre-spond.
Formally, we define syntactic constituentsas ?i, A, j?
where i, j mark the location of char-acters.
T = {?i, A, j?|A spans from i to j} andG = {?i, A, j?|A spans from i to j} represent thetest/gold parses, respectively, and we calculate:8Labeled Precision = #(G ?
T )/#T (10)Labeled Recall = #(G ?
T )/#G (11)4.3 Experimental SetupOur departure point for the syntactic analysis ofMH is that the basic units for processing are notwords, but morphological segments that are con-catenated together to form words.
Therefore, weobtain a segment-based probabilistic grammar bytraining a Probabilistic Context Free Grammar(PCFG) on a segmented and annotated MH cor-pus (Sima?an et al, 2001).
Then, we use exist-ing tools ?
i.e., a morphological analyzer (Segal,2000), a part-of-speech tagger (Bar-Haim, 2005),and a general-purpose parser (Schmid, 2000) ?
tofind compatible morphological segmentations andsyntactic analyses for unseen sentences.The Data The data set we use is taken from theMH treebank which consists of 5001 sentencesfrom the daily newspaper ?ha?aretz?
(Sima?an etal., 2001).
We employ the syntactic categories andPOS tag sets developed therein.
Our data set in-cludes 3257 sentences of length greater than 1 andless than 21.
The number of segments per sen-tence is 60% higher than the number of words persentence.9 We conducted 8 experiments in whichthe data is split to training and test sets and applycross-fold validation to obtain robust averages.The Models Model I uses the morphological an-alyzer and the POS tagger to find the most prob-able segmentation for a given sentence.
This isdone by providing the POS tagger with multiplemorphological analyses per word and maximizingthe sum ?tn1 P (tn1 , sn1 |wm1 ) (Bar-Haim, 2005, sec-tion 8.2).
Then, the parser is used to find the most8Covert definite article errors are counted only at the POStags level and discounted at the phrase-level.9The average number of words per sentence in the com-plete corpus is 17 while the average number of morphologicalsegments per sentence is 26.52probable parse tree for the selected sequence ofmorphological segments.
Formally, this model isa first approximation of equation (8) using a step-wise maximization instead of a joint one.10In Model II we percolate the morphological am-biguity further, to the lowest level of non-terminalsin the syntactic trees.
Here we use the morpholog-ical analyzer and the POS tagger to find the mostprobable segmentation and POS tag assignmentby maximizing the joint probability P (tn1 , sn1 |wm1 )(Bar-Haim, 2005, section 5.2).
Then, the parseris used to parse the tagged segments.
Formally,this model attempts to approximate equation (9).
(Note that here we couple a morphological anda syntactic decision, as we are looking to max-imize P (tn1 , sn1 |wm1 ) ?
P (tn1 |sn1 )P (sn1 |wm1 ) andconstrain the space of trees to those that agree withthe resulting analysis.
)11In both models, smoothing the estimated prob-abilities is delegated to the relevant subcompo-nents.
Out of vocabulary (OOV) words are treatedby the morphological analyzer, which proposesall possible segmentations assuming that the stemis a proper noun.
The Tri-gram model used forPOS tagging is smoothed using Good-Turing dis-counting (see (Bar-Haim, 2005, section 6.1)), andthe parser uses absolute discounting with variousbackoff strategies (Schmid, 2000, section 4.4).The Tag-Sets To examine the usefulness of var-ious morphological features shared with the pars-ing task, we alter the set of morphosyntactic cate-gories to include more fine-grained morphologicaldistinctions.
We use three sets: Set A contains barePOS categories, Set B identifies also definite nounsmarked for possession, and Set C adds the distinc-tion between finite and non-finite verb forms.Evaluation We use seven measures to evaluateour models?
performance on the integrated task.10At the cost of incurring indepence assumptions, a step-wise architecture is computationally cheaper than a joint oneand this is perhaps the simplest end-to-end architecture forMH parsing imaginable.
In the absence of previous MH pars-ing results, this model is suitable to serve as a baseline againstwhich we compare more sophisticated models.11We further developed a third model, Model III, whichis a more faithful approximation, yet computationally afford-able, of equation (9).
There we percolate the ambiguity all theway through the integrated architecture by means of provid-ing the parser with the n-best sequences of tagged morpho-logical segments and selecting the analysis ?pi, tn1 , sn1 ?
whichmaximizes the production P (pi|tn1 , sn1 )P (sn1 , tn1 |wm1 ).
How-ever, we have not yet obtained robust results for this modelprior to the submission of this paper, and therefore we leaveit for future discussion.String Labeled POS tags Segment.Cover.
Prec.
/ Rec.
Prec.
/ Rec.
Prec.
/ Rec.Model I-A 99.2% 60.3% / 58.4% 82.4% / 82.6% 94.4% / 94.7 %Model II-A 95.9% 60.7% / 60.5% 84.5% / 84.8% 91.3% / 91.6%Model I-B 99.2 % 60.3% / 58.4% 81.6% / 82.3% 94.2% / 95.0%Model II-B 95.7% 60.7% / 60.5% 82.8% / 83.5% 90.9% / 91.7%Model I-C 99.2% 60.9% / 59.2% 80.4% / 81.1% 94.2% / 95.1%Model II-C 95.9% 61.7% / 61.9% 81.6% / 82.3% 91.0% / 91.9%Table 4: Evaluation Metrics, Models I and IIFirst, we present the percentage of sentences forwhich the model could propose a pair of corre-sponding morphological and syntactic analyses.This measure is referred to as string coverage.
Toindicate morphological disambiguation capabili-ties we report segmentation precision and recall.To capture tagging and parsing accuracy, we referto our redefined Parseval measures and separatethe evaluation of morphosyntactic categories, i.e.,POS tags precision and recall, and phrase-levelsyntactic categories, i.e., labeled precision and re-call (where root nodes are discarded and emptytrees are counted as zero).12 The labeled cate-gories are evaluated against the original tag set.4.4 ResultsTable 4 shows the evaluation scores for models I-Ato II-C. To the best of our knowledge, these are thefirst parsing results for MH assuming no manualinterference for morphological disambiguation.For all sets, parsing of tagged-segments (ModelII) shows improvement of up to 2% over pars-ing bare segments?
sequences (Model I).
This indi-cates that morphosyntactic information selected intandem with morphological segmentation is moreinformative for syntactic analysis than segmenta-tion alone.
We also observe decreasing string cov-erage for Model II, possibly since disambiguationbased on short context may result in a probable,yet incorrect, POS tag assignment for which theparser cannot recover a syntactic analysis.
Cor-rect disambiguation may depend on long-distancecues, e.g., agreement, so we advocate percolatingthe ambiguity further up to the parser.Comparing the performance for the different tagsets, parsing accuracy increases for models I-B/Cand II-B/C while POS tagging results decrease.These results seem to contradict the common wis-dom that performance on a ?complex?
task de-12Since we evaluate the models?
performance on an inte-grated task, sentences in which one of the subcomponentsfailed to propose an analysis counts as zero for all subtasks.53pends on a ?simpler?, preceding one; yet, they sup-port our thesis that morphological information or-thogonal to syntactic categories facilitates syntac-tic analysis and improves disambiguation capacity.5 DiscussionDevising a baseline model for morphological andsyntactic processing is of great importance for thedevelopment of a broad-coverage statistical parserfor MH.
Here we provide a set of standardizedbaseline results for later comparison while con-solidating the formal and architectural underpin-ning of an integrated model.
However, our resultswere obtained using a relatively small set of train-ing data and a weak (unlexicalized) parser, due tothe size of the corpus and its annotated scheme.13Training a PCFG on our treebank resulted in aseverely ambiguous grammar, mainly due to highphrase structure variability.To compensate for the flat, ambiguous phrase-structures, in the future we intend to employ prob-abilistic grammars in which all levels of non-terminals are augmented with morphological in-formation percolated up the tree.
Furthermore,the MH treebank annotation scheme features a setof so-called functional features14 which expressgrammatical relations.
We propose to learn thecorrelation between various morphological mark-ings and functional features, thereby constrainingthe space of syntactic structures to those which ex-press meaningful predicate-argument structures.Since our data set is relatively small,15 introduc-ing orthogonal morphological information to syn-tactic categories may result in severe data sparse-ness.
In the current architecture, smoothing ishandled separately by each of the subcomponents.Enriched grammars would allow us to exploit mul-tiple levels of information in smoothing the esti-mated probabilities and to redistribute probabilitymass to unattested events based on their similarityto attested events in their integrated representation.6 ConclusionTraditional approaches for devising parsing mod-els, smoothing techniques and evaluation metricsare not well suited for MH, as they presuppose13The lack of head marking, for instance, precludes the useof lexicalized models a` la (Collins, 1997).14SBJ for subject, OBJ for object, COM for complement,etc.
(Sima?an et al, 2001).15The size of our treebank is less than 30% of the ArabicTreebank, and less than 10% of the WSJ Penn Treebank.separate levels of processing.
Different languagesmark regularities in their surface structures in dif-ferent ways ?
English encodes regularities in wordorder, while MH provides useful hints about gram-matical relations in its derivational and inflectionalmorphology.
In the future we intend to developmore sophisticated models implementing closerinteraction between morphology and syntax, bymeans of which we hope to boost parsing accu-racy and improve morphological disambiguation.Acknowledgments I would like to thank KhalilSima?an for supervising this work, Remko Scha,Rens Bod and Jelle Zuidema for helpful com-ments, and Alon Itai, Yoad Winter and ShulyWintner for discussion.
The Knowledge Cen-ter for Hebrew Processing provided corpora andtools, and Roy Bar-Haim provided knowledge andtechnical support for which I am grateful.
Thiswork is funded by the Netherlands Organizationfor Scientific Research (NWO) grant 017.001.271.ReferencesMeni Adler and Dudi Gabai.
2005.
MorphologicalAnalyzer and Disambiguator for Modern Hebrew.Knowledge Center for Processing Hebrew.Roy Bar-Haim.
2005.
Part-of-Speech Tagging for He-brew and Other Semitic Languages.
Master?s thesis,Technion, Haifa, Israel.Eugene Charniak, Glenn Carroll, John Adcock, An-thony R. Cassandra, Yoshihiko Gotoh, Jeremy Katz,Michael L. Littman, and John McCann.
1996.
Tag-gers for Parsers.
AI, 85(1-2):45?57.Eugene Charniak.
2000.
A Maximum-Entropy-Inspired Parser.
In Proceedings of NAACL 2000.Michael Collins.
1997.
Three Generative, LexicalisedModels for Statistical Parsing.
In Proceedings ofACL-EACL 1997.Nizar Habash and Owen Rambow.
2005.
Arabic Tok-enization, Part-of-Speech Tagging and Morphologi-cal Disambiguation in One Fell Swoop.
In Proceed-ings of ACL 2005.Helmut Schmid, 2000.
LoPar: Design and Implemen-tation.
Institute for Computational Linguistics, Uni-versity of Stuttgart.Erel Segal.
2000.
A Probabilistic Morphological An-alyzer for Hebrew Undotted Texts.
Master?s thesis,Computer Science Department, Technion, Isreal.Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altman,and Noa Nativ.
2001.
Building a Tree-Bank forModern Hebrew Text.
In Traitement Automatiquedes Langues, volume 42, pages 347?380.54
