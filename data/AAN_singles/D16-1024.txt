Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 247?256,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsAttention-based LSTM Network for Cross-Lingual Sentiment ClassificationXinjie Zhou, Xiaojun Wan and Jianguo XiaoInstitute of Computer Science and Technology, Peking UniversityThe MOE Key Laboratory of Computational Linguistics, Peking University{zhouxinjie,wanxiaojun,xiaojianguo}@pku.edu.cnAbstractMost of the state-of-the-art sentiment classifi-cation methods are based on supervised learn-ing algorithms which require large amountsof manually labeled data.
However, thelabeled resources are usually imbalanced indifferent languages.
Cross-lingual sentimentclassification tackles the problem by adaptingthe sentiment resources in a resource-richlanguage to resource-poor languages.
In thisstudy, we propose an attention-based bilingualrepresentation learning model which learnsthe distributed semantics of the documents inboth the source and the target languages.
Ineach language, we use Long Short Term Mem-ory (LSTM) network to model the documents,which has been proved to be very effectivefor word sequences.
Meanwhile, we proposea hierarchical attention mechanism for thebilingual LSTM network.
The sentence-levelattention model learns which sentences of adocument are more important for determiningthe overall sentiment while the word-levelattention model learns which words in eachsentence are decisive.
The proposed modelachieves good results on a benchmark datasetusing English as the source language andChinese as the target language.1 IntroductionMost of the sentiment analysis research focuses onsentiment classification which aims to determinewhether the users attitude is positive, neutral ornegative.
There are two classes of mainstreamingsentiment classification algorithms: unsupervisedmethods which usually require a sentiment lexicon(Taboada et al, 2011) and supervised methods(Pang et al, 2002) which require manually labeleddata.
However, both of these sentiment resources areunbalanced in different languages.
The sentimentlexicon or labeled data are rich in several languagessuch as English and are poor in others.
Manuallybuilding these resources for all the languages willbe expensive and time-consuming.
Cross-lingualsentiment classification tackles the problem by try-ing to adapt the resources in one language to otherlanguages.
It can also be regarded as a special kindof cross-lingual text classification task.Recently, there have been several bilingual rep-resentation learning methods such as (Hermannand Blunsom, 2014; Gouws et al, 2014) forcross-lingual sentiment or text classification whichachieve promising results.
They try to learn ajoint embedding space for different languages suchthat the training data in the source language canbe directly applied to the test data in the targetlanguage.
However, most of the studies onlyuse simple functions, e.g.
arithmetic average, tosynthesize representations for larger text sequences.Some of them use more complicated compositionalmodels such as the bi-gram non-linearity model in(Hermann and Blunsom, 2014) which also fail tocapture the long distance dependencies in texts.In this study, we propose an attention-basedbilingual LSTM network for cross-lingual sentimentclassification.
LSTMs have been proved to bevery effective to model word sequences and arepowerful to learn on data with long range temporaldependencies.
After translating the training datainto the target language using machine translation247tools, we use the bidirectional LSTM network tomodel the documents in both of the source andthe target languages.
The LSTMs show strongability to capture the compositional semantics forthe bilingual texts in our experiments.For the traditional LSTM network, each word inthe input document is treated with equal importance,which is reasonable for traditional text classificationtasks.
In this paper, we propose a hierarchicalattention mechanism which enables our model tofocus on certain part of the input document.
Themotivation mainly comes from the following threeobservations: 1) the machine translation tool that weuse to translate the documents will always introducemuch noise for sentiment classification.
We hopethat the attention mechanism can help to filter outthese noises.
2) In each individual language, thesentiment of a document is usually decided by arelative small part of it.
In a long review document,the user might discuss both the advantages anddisadvantages of a product.
The sentiment willbe confusing if we consider each sentence of thesame contribution.
For example, in the first reviewof Table 1, the first sentence reveals a negativesentiment towards the movie but the second onereveals a positive sentiment.
As human readers,we can understand that the review is expressinga positive overall sentiment but it is hard for thesequence modeling algorithms including LSTM tocapture.
3) At the sentence level, it is important tofocus on the sentiment signals such as the sentimentwords.
They are usually very decisive to determinethe polarity even for a very long sentence, e.g.?easy?
and ?nice?
in the second example of Table1.
?I felt it could have been a lot better with a littleless comedy and a little more drama to get thepoint across.
However, its still a must see forany Jim Carrey fan.
?
?It is easy to read, it is easy to look things up inand provides a nice section on the treatments.
?Table 1: Examples of the sentiment attentionIn sum, the main contributions of this study aresummarized as follows:1) We propose a bilingual LSTM network forcross-lingual sentiment classification.
Compared tothe previous methods which only use weighted orarithmetic average of word embeddings to representthe document, LSTMs have obvious advantage tomodel the compositional semantics and to capturethe long distance dependencies between words forbilingual texts.2) We propose a hierarchical bilingual attentionmechanism for our model.
To the best of ourknowledge, this is the first attention-based modeldesigned for cross-lingual sentiment analysis.3) The proposed framework achieves good resultson a benchmark dataset from a cross-languagesentiment classification evaluation.
It outperformsthe best team in the evaluation as well as severalstrong baseline methods.2 Related WorkSentiment analysis is the field of studying andanalyzing peoples opinions, sentiments, evaluations,appraisals, attitudes, and emotions (Liu, 2012).
Themost common task of sentiment analysis is polarityclassification which arises with the emergence ofcustomer reviews on the Internet.
Pang et al (2002)used supervised learning methods and achievedpromising results with simple unigram and bi-gramfeatures.
In subsequent research, more featuresand learning algorithms were tried for sentimentclassification by a large number of researchers.
Re-cently, the emerging of deep learning has also shedlight on this area.
Lots of representation learningmethods has been proposed to address the sentimentclassification task and many of them achieve thestate-of-the-art performance on several benchmarkdatasets, such as the recursive neural tensor network(Socher et al, 2013), paragraph vector (Le andMikolov, 2014), multi-channel convolutional neuralnetworks (Kim, 2012), dynamic convolutionalneural network (Blunsom et al, 2014) and treestructure LSTM (Tai et al, 2015).
Very recently,Yang et al (2016) proposed a similar hierarchicalattention network based on GRU in the monolingualsetting.
Note that our work is independent withtheirs and their study was released online after wesubmitted this study.Cross-lingual sentiment classification is also apopular research topic in the sentiment analysis248community which aims to solve the sentimentclassification task from a cross-language view.
It isof great importance since it can exploit the existinglabeled information in a source language to build asentiment classification system in any other targetlanguage.
Cross-lingual sentiment classification hasbeen extensively studied in the very recent years.Mihalcea et al (2007) translated English subjec-tivity words and phrases into the target languageto build a lexicon-based classifier.
Wan (2009)translated both the training data (English to Chinese)and the test data (Chinese to English) to train differ-ent models in both the source and target languages.Chen et al (2015) proposed a knowledge validationmethod and incorporated it into a boosting modelto transfer credible information between the twolanguages during training.There have also been several studies addressingthe task via multi-lingual text representation learn-ing.
Xiao and Guo (2013) learned different repre-sentations for words in different languages.
Part ofthe word vector is shared among different languagesand the rest is language-dependent.
Klementiev etal.
(2012) treated the task as a multi-task learningproblem where each task corresponds to a singleword, and the task relatedness is derived from co-occurrence statistics in bilingual parallel corpora.Chandar A P et al (2014) and Zhou et al (2015)used the autoencoders to model the connectionsbetween bilingual sentences.
It aims to minimizethe reconstruction error between the bag-of-wordsrepresentations of two parallel sentences.
Phamet al (2015) extended the paragraph model intobilingual setting.
Each pair of parallel sentencesshares the same paragraph vector.Compared to the existing studies, we propose touse the bilingual LSTM network to learn the docu-ment representations of reviews in each individuallanguage.
It has obvious advantage to model thecompositional semantics and to capture the longdistance dependencies between words.
Besides, wepropose a hierarchical neural attention mechanismto capture the sentiment attention in each document.The attention model helps to filter out the noisewhich is irrelevant to the overall sentiment.3 Preliminaries3.1 Problem DefinitionCross-language sentiment classification aims to usethe training data in the source language to build amodel which is adaptable for the test data in thetarget language.
In our setting, we have labeledtraining data in English LEN = {xi, yi}Ni=1 , wherexi is the review text and yi is the sentiment labelvector.
yi = (1, 0) represents the positive sentimentand yi = (0, 1) represents the negative sentiment.In the target language Chinese, we have the testdata TCN = {xi}Ti=1 and unlabeled data UCN ={xi}Mi=1.
The task is to use LEN and UCN to learna model and classify the sentiment polarity for thereview texts in TCN .In our method, the labeled, unlabeled and test dataare all translated into the other language using anonline machine translation tool.
In the subsequentpart of the paper, we refer to a document and itscorresponding translation in the other language asa pair of parallel documents.3.2 RNN and LSTMRecurrent neural network (RNN) (Rumelhart etal., 1988) is a special kind of feed-forward neuralnetwork which is useful for modeling time-sensitivesequences.
At each time t, the model receivesinput from the current example and also from thehidden layer of the network?s previous state.
Theoutput is calculated given the hidden state at thattime stamp.
The recurrent connection makes theoutput at each time associated with all the previousinputs.
The vanilla RNN model has been consideredto be difficult to train due to the well-known problemof vanishing and exploding gradients.
The LSTM(Hochreiter and Schmidhuber, 1997) addresses theproblem by re-parameterizing the RNN model.
Thecore idea of LSTM is introducing the ?gates?
tocontrol the data flow in the recurrent neural unit.The LSTM structure ensures that the gradient of thelong-term dependencies cannot vanish.
The detailedarchitecture that we use in shown in Figure 1.4 FrameworkIn this study, we try to model the bilingual textsthrough the attention based LSTM network.
We first249Figure 1: The LSTM architecture.
The image is adopted from(Jozefowicz et al, 2015).describe the general architecture of the model andthen describe the attention mechanism used in it.Figure 2: The architecture of the proposed framework.
Theinputs xcn and xen are parallel documents.
Due to spacelimit, we only illustrate the attention based LSTM network inChinese language.
For the English document xen, the networkarchitecture is the same as the Chinese side but has differentmodel parameters.4.1 ArchitectureThe general architecture of our approach is shown inFigure 2.
For a pair of parallel documents xcn andxen, each of them is sent into the attention basedLSTM network.
The English-side and Chinese-side architectures are the same but have differentparameters.
We only show the Chinese-side networkin the figure due to space limit.
The whole modelis divided into four layers.
In the input layer,the documents are represented as a word sequencewhere each position corresponds to a word vectorfrom pre-trained word embeddings.
In the LSTMlayer, we get the high-level representation from abidirectional LSTM network.
We use the hiddenunits from both the forward and backward LSTMs.In the document representation layer, we incorporatethe attention model into the network and derivethe final document representation.
At the outputlayer, we concatenate the representations of theEnglish and Chinese documents and use the softmaxfunction to predict the sentiment label.Input Layer: The input layer of the network isthe word sequences in a document x which can beeither Chinese or English.
The document x containsseveral sentences {si}|x|i=1 and each sentence iscomposed of several words si = {wi,j}|si|j=1 .
Werepresent each word in the document as a fixed-sizevector from pre-trained word embeddings.LSTM Layer: In each individual language,we use bi-directional LSTMs to model the inputsequences.
In the bidirectional architecture, thereare two layers of hidden nodes from two separateLSTMs.
The two LSTMs capture the dependenciesin different directions.
The first hidden layers haverecurrent connections from the past words whilesecond one?s direction of recurrent of connectionsis flipped, passing activation backwards in the texts.Therefore, in the LSTM layer, we can get theforward hidden state ~hi,j from the forward LSTMnetwork and the backward hidden state ~hi,j from thebackward LSTM network.
We represent the finalstate at position (i, j), i.e.
the j-th word in the i-thsentence of the document, with the concatenation of~hi,j and ~hi,j .hi,j = ~hi,j ?
~hi,jIt captures the compositional semantics in bothdirections of the word sequences.Document Representation Layer:As describedabove, different parts of the document usually havedifferent importance for the overall sentiment.
Some250sentences or words can be decisive while the othersare irrelevant.
In this study, we use a hierarchicalattention mechanism which assigns a real valuescore for each word and a real value score for eachsentence.
The detailed strategy of our attentionmodel will be described in the next subsection.Suppose we have the sentence attention score Aifor each sentence si ?
x, and the word attentionscore ai,j for each word wi,j ?
si, both of thescores are normalized which satisfy the followingequations,?iAi = 1 and?jai,j = 1The sentence attention measures which sentenceis more important for the overall sentiment whilethe word attention captures sentiment signals suchas sentiment words in each sentence.
Therefore,the document representation r for document x iscalculated as follows,r =?i[Ai ?
?j(ai,j ?
hi,j)]Note that many LSTM based models represent theword sequences only using the hidden layer at thefinal node.
In this study, the hidden states at allthe positions are considered with different attentionweights.
We believe that, for document sentimentclassification, focusing on some certain parts of thedocument will be effective to filter out the sentiment-irrelevant noise.Output Layer: At the output layer, we need topredict the overall sentiment of the document.
Foreach English document xen and its correspondingtranslation xcn, suppose the document representa-tions of them are obtained in previous steps as renand rcn, we simply concatenate them as the featurevector and use the softmax function to predict thefinal sentiment.y?
= softmax(rcn ?
ren)4.2 Hierarchical Attention MechanismFor document-level sentiment classification task, wehave shown that capturing both the sentence andword level attention is important.
The general ideais inspired by previous works such as Bahdanau etal.
(2014) and Hermann et al (2015) which havesuccessfully applied the attention model to machinetranslation and question answering.
Bahdanau etal.
(2014) incorporated the attention model into thesequence to sequence learning framework.
Duringthe decoding phase of the machine translation task,the attention model helps to find which input wordshould be ?aligned?
to the current output.
In ourcase, the output of the model is not a sequencebut only one sentiment vector.
We hope to findthe important units in the input sequence which areinfluential for the output.We propose to learn a hierarchical attention modeljointly with the bilingual LSTM network.
Thefirst level is the sentence attention model whichmeasures which sentences are more important forthe overall sentiment of a document.
For eachsentence si = {wi,j}|si|j=1 in the document, werepresent the sentence via the final hidden state ofthe forward LSTM and the backward LSTM, i.e.si = ~hi,|si| ?
~hi,1We use a two-layer feed-forward neural networkto predict the attention score of siA?i = f(si; ?s)Ai =exp(A?i)?j exp(A?j)where f denotes the two-layer feed-forward neuralnetwork and ?s denotes the parameters in it.At the word level, we represent each word wi,jusing its word embedding and the hidden state ofthe bidirectional LSTM layer, i.e.
hi,j .
Similarly,we use a two-layer feed forward neural network topredict the attention score of wi,j ,ei,j = wi,j ?
~hi,j ?
~hi,ja?i,j = f(ei,j ; ?w)ai,j =exp(a?i,j)?j exp(a?i,j)where ?w denotes the parameters for predictingword attention.2514.3 Training of the Proposed ModelThe proposed model is trained in a semi-supervisedmanner.
In the supervised part, we use the crossentropy loss to minimize the sentiment prediction er-ror between the output results and the gold standardlabels,L1 =?
(xen,xcn)?i?yi log(y?i)where xen and xcn are a pair of parallel documentsin the training data, y is the gold-standard sentimentvector and y?
is the predicted vector from our model.The unsupervised part tries to minimize thedocument representations between the parallel data.Following previous research, we simply measure thedistance of two parallel documents via the EuclideanDistance,L2 =?
(xen,xcn)?ren ?
rcn?2where xen and xcn are a pair of parallel documentsfrom both the labeled and unlabeled data.The final objective function is a weighted sum ofL1 and L2,L = L1 + ?
?
L2where ?
is the hyper-parameter controlling theweight.
We use Adadelta (Zeiler, 2012) to updatethe parameters during training.
It can dynamicallyadapt over time using only first order informationand has minimal computational overhead beyondvanilla stochastic gradient descent.In the test phase, the test document in TCN issent into our model along with the correspondingmachine translated text in TEN .
The final senti-ment is predicted via a softmax function over theconcatenated representation of the bilingual texts asdescribed above.5 Experiment5.1 DatasetWe use the dataset from the cross-language senti-ment classification evaluation of NLP&CC 2013.11The dataset can be found athttp://tcci.ccf.org.cn/conference/2013/index.html.
NLP&CCis an annual conference specialized in the fields of NaturalThe dataset contains reviews in three domainsincluding book, DVD and music.
In each domain,it has 2000 positive reviews and 2000 negativereviews in English for training and 4000 Chinesereviews for test.
It also contains 44113, 17815 and29678 unlabeled reviews for book, DVD and musicrespectively.5.2 Implementation DetailWe use Google Translate2 to translate the labeleddata to Chinese and translate the unlabeled data andtest data to English.
All the texts are tokenized andconverted into lower case.In the proposed framework, the dimensions ofthe word vectors and the hidden layers of LSTMsare set as 50.
The initial word embeddings aretrained on both the unlabeled and labeled reviewsusing word2vec in each individual language.
Theword vectors are fine-tuned during the trainingprocedure.
The hyper-parameter a is set to 0.2.
Thedropout rate is set to 0.5 to prevent overfitting.
Tenpercent of the training data are randomly selectedas validation set.
The training procedure is stoppedwhen the prediction accuracy does not improve for10 iterations.
We implement the framework basedon theano (Bastien et al, 2012) and use a GTX980TI graphic card for training.5.3 Baselines and ResultsTo evaluate the performance of our model, wecompared it with the following baseline methods:LR and SVM: We use logistic regression andSVM to learn different classifiers based on thetranslated Chinese training data.
We simply useunigram features.MT-PV: Paragraph vector (Le and Mikolov,2014) is considered as one of the state-of-the-artmonolingual document modeling methods.
Wetranslate all the training data into Chinese and useparagraph vector to learn a vector representationfor the training and test data.
A logistic regressionclassifier is used to predict the sentiment polarity.Bi-PV: Pham et al (2015) is one the state-of-the-art bilingual document modeling methods.
Itextends the paragraph vector into bilingual setting.Language Processing (NLP) and Chinese Computing (CC)organized by Chinese Computer Federation (CCF).2http://translate.google.com/252Each pair of parallel sentences in the training datashares the same vector representation.BSWE: Zhou et al (2015) proposed the bilin-gual sentiment word embedding algorithm basedon denoising autoencoders.
It learns the vectorrepresentations for 2000 sentiment words.
Eachdocument is then represented by the sentimentwords and the corresponding negation words in it.H-Eval: Gui et al (2013) got the highestperformance in the NLP&CC 2013 cross-lingualsentiment classification evaluation.
It uses a mixedCLSC model by combining co-training and transferlearning strategies.A-Eval: This is the average performance of all theteams in the NLP&CC 2013 cross-lingual sentimentclassification evaluation.The attention-based models EN-Attention, CN-Attention and BI-Attention: Bi-Attention is themodel described in the above sections which con-catenate the document representations of the Englishside and the Chinese side texts.
EN-Attention onlytranslates the Chinese test data into English and usesEnglish-side attention model while CN-Attentiononly uses the Chinese side attention model.Method Domains Averagebook DVD musicLR 0.765 0.796 0.741 0.767SVM 0.779 0.814 0.707 0.767MT-PV 0.753 0.799 0.748 0.766Bi-PV 0.785 0.820 0.753 0.796BSWE 0.811 0.816 0.794 0.807A-Eval 0.662 0.660 0.675 0.666H-Eval 0.785 0.777 0.751 0.771EN-Attention 0.798 0.827 0.808 0.811CN-Attention 0.820 0.840 0.809 0.823BI-Attention 0.821 0.837 0.813 0.824Table 2: Cross-lingual sentiment prediction accuracy of ourmethods and the comparison approaches.Table 2 shows the cross-lingual sentiment clas-sification accuracy of all the approaches.
The firstkind baseline algorithms are based on traditionalbag-of-word features.
SVM performs better thanLR on book and DVD but gets much worse resulton music.
The second kind baseline algorithmsare based on deep learning methods which learnthe vector representations for words or documents.MT-PV achieves similar results with LR.
Bi-PVimproves the accuracy by about 0.03 using boththe bilingual documents.
While MT-PV and Bi-PV directly learn document representations, BSWElearns the embedding for the words in a bilingualsentiment lexicon.
It gets higher accuracy than bothBi-PV and MT-PV which shows that the sentimentwords are very important for this task.Our attention based models achieve the highestprediction accuracy among all the approaches.
Theresults show that CN-Attention always outperformsEN-Attention.
The combination of the English-sideand Chinese-side model brings improvement to boththe book and music domains and yields the highestaverage prediction accuracy.
The attention-basedmodels outperform the algorithms using traditionalfeatures as well as the existing deep learning basedmethods.
Compared to the highest performance inthe NLP&CC evaluation, we improve the averageaccuracy by about 0.05.5.4 Influence of the Attention MechanismIn this study, we propose a hierarchical attentionmechanism to capture the sentiment-related infor-mation of each document.
In table 3, we showthe results of models with different attention mech-anisms.
All the models are based on the bilingualbi-directional LSTM network as shown in Figure 2.LSTM is the basic bilingual bi-directional LSTMnetwork.
LSTM+SA considers only sentence-levelattention while LSTM+WA considers only word-level attention.
LSTM+HA combines both word-level and sentence-level attentions.
From the results,we can observe that LSTM+HA outperforms theother three methods, which proves the effectivenessof the hierarchical attention mechanism.
Besides,the word-level attention shows better performancethan the sentence-level attention.Method Average AccuracyLSTM 0.811LSTM+SA 0.814LSTM+WA 0.821LSTM+HA 0.824Table 3: Comparison of different attention mechanismsWe also conduct a case study using the examplesin Table 1.
We show the visualized word attention253using a heat map in Figure 3 by drawing theattention of each word in it.
The darker colorreveals higher attention scores while the lighter parthas little importance.
We can observe that ourmodel successfully identifies the important unitsof the sentence.
The sentiment word ?easy?
getsmuch higher attention score than the other words.The word ?nice?
gets the third highest score inthe sentence right after the two ?easy?.
Note thatour attention mechanism considers both the wordembedding vector and the hidden state vectors.Therefore, the same word ?easy?
gets differentscores in different positions.Figure 3: Attention visualization for a review sentence5.5 Influence of the Word EmbeddingsFor the deep learning based methods, the initialword embeddings used as the inputs for the networkusually play an important role.
We study fourdifferent settings called rand, static, fine-tuned andmulti-channel, respectively.
In rand setting, theword embeddings are randomly initialized.
Thestatic setting keeps initial embedding fixed whilethe fine-tuned setting learns a refined embeddingduring the training procedure.
Multi-channel is thecombination of static and fine-tuned.
Two sameword vectors are concatenated to represent eachword.
During the training procedure, half of it isfine-tuned while the rest is fixed.
Note that fine-tuned is the embedding setting that we use in ourmodel.Embedding Domains AverageSettings book DVD musicrand 0.789 0.786 0.746 0.774static 0.804 0.810 0.784 0.799fine-tuned 0.821 0.837 0.813 0.824multi-channel 0.822 0.835 0.806 0.821Table 4: Performance of our model with four different wordembedding settingsTable 4 shows the performance of our model inthese settings.
Rand gets the lowest accuracy amongthem.
The fine-tuned word embeddings performbetter than static which fits the results in previousstudy (Kim, 2012).
Multi-channel gets similarresults with fine-tuned on DVD and music but is a bitlower on book.
We also find that using pre-trainedword embeddings helps the model to converge muchfaster than random initialization.5.6 Influence of Vector SizesIn our experiment, we set the size of the hiddenlayers in both the forward and backward LSTMs thesame as the size of the input word vectors.
There-fore, the dimension of the document representationis twice of the word vector size.
In Figure 4, weshow the performance of our model with differentinput vector sizes.
We use the vector size in thefollowing set {10, 25, 50, 100, 150, 200}.
Note thatthe dimensions of all the units in the model alsochange with that.We can observe from Figure 4 that the predictionaccuracy for the book domain keeps steady whenthe vector size changes.
For DVD and music, theperformance increases at the beginning and becomesstable after the vector size grows larger than 50.
Itshows that our model is robust to a wide range ofvector sizes.Figure 4: Performance with different vector sizes6 ConclusionIn this paper, we propose an attention based LSTMnetwork for cross-language sentiment classification.We use the bilingual bi-directional LSTMs to modelthe word sequences in the source and target lan-guages.
Based on the special characteristics of thesentiment classification task, we propose a hierar-chical attention model which is jointly trained withthe LSTM network.
The sentence level attention254enables us to find the key sentences in a documentand the word level attention helps to capture thesentiment signals.
The proposed model achievespromising results on a benchmark dataset usingChinese as the source language and English as thetarget language.
It outperforms the best results in theNLPC&CC cross-language sentiment classificationevaluation as well as several strong baselines.
Infuture work, we will evaluate the performance of ourmodel on more datasets and more language pairs.The sentiment lexicon is also another kind of usefulresource for classification.
We will explore how tomake full usages of these resources in the proposedframework.AcknowledgmentsThe work was supported by National Natural Sci-ence Foundation of China (61331011), National Hi-Tech Research and Development Program (863 Pro-gram) of China (2015AA015403, 2014AA015102)and IBM Global Faculty Award Program.
Wethank the anonymous reviewers for their helpfulcomments.
Xiaojun Wan is the correspondingauthor.ReferencesDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2014.
Neural machine translation by jointlylearning to align and translate.
arXiv preprint arX-iv:1409.0473.Fre?de?ric Bastien, Pascal Lamblin, Razvan Pascanu,James Bergstra, Ian Goodfellow, Arnaud Bergeron,Nicolas Bouchard, David Warde-Farley, and YoshuaBengio.
2012.
Theano: new features and speedimprovements.
arXiv preprint arXiv:1211.5590.Phil Blunsom, Edward Grefenstette, and Nal Kalch-brenner.
2014.
A convolutional neural network formodelling sentences.
In Proceedings of the 52ndAnnual Meeting of the Association for ComputationalLinguistics.Sarath Chandar A P, Stanislas Lauly, Hugo Larochelle,Mitesh Khapra, Balaraman Ravindran, Vikas CRaykar, and Amrita Saha.
2014.
An autoencoder ap-proach to learning bilingual word representations.
InAdvances in Neural Information Processing Systems,pages 1853?1861.Qiang Chen, Wenjie Li, Yu Lei, Xule Liu, and YanxiangHe.
2015.
Learning to adapt credible knowledge incross-lingual sentiment analysis.
In Proceedings of52rd Annual Meeting of the Association for Compu-tational Linguistic.Stephan Gouws, Yoshua Bengio, and Greg Corrado.2014.
Bilbowa: Fast bilingual distributed repre-sentations without word alignments.
arXiv preprintarXiv:1410.2455.Lin Gui, Ruifeng Xu, Jun Xu, Li Yuan, Yuanlin Yao,Jiyun Zhou, Qiaoyun Qiu, Shuwei Wang, Kam-FaiWong, and Ricky Cheung.
2013.
A mixed model forcross lingual opinion analysis.
In Natural LanguageProcessing and Chinese Computing, pages 93?104.Karl Moritz Hermann and Phil Blunsom.
2014.
Multilin-gual models for compositional distributed semantics.In Proceedings of 52rd Annual Meeting of the Associ-ation for Computational Linguistic, pages 58?68.Karl Moritz Hermann, Tomas Kocisky, Edward Grefen-stette, Lasse Espeholt, Will Kay, Mustafa Suleyman,and Phil Blunsom.
2015.
Teaching machines to readand comprehend.
In Advances in Neural InformationProcessing Systems, pages 1684?1692.Sepp Hochreiter and Ju?rgen Schmidhuber.
1997.
Longshort-term memory.
Neural computation, 9(8):1735?1780.Yoon Kim.
2012.
Convolutional neural networks forsentence classification.
In Proceedings of EMNLP2014, pages 1746?1751.Alexandre Klementiev, Ivan Titov, and Binod Bhattarai.2012.
Inducing crosslingual distributed representa-tions of words.
In Proceedings of COLING 2012,pages 1759?1774.Quoc Le and Tomas Mikolov.
2014.
Distributedrepresentations of sentences and documents.
InProceedings of the 31st International Conference onMachine Learning (ICML-14), pages 1188?1196.B Liu.
2012.
Sentiment analysis and opinion mining:Synthesis lectures on human language technologies,vol.
16.
Morgan & Claypool Publishers, San Rafael.Rada Mihalcea, Carmen Banea, and Janyce Wiebe.2007.
Learning multilingual subjective language viacross-lingual projections.
In Proceedings of the 45thAnnual Meeting of the Association of ComputationalLinguistics.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: sentiment classification usingmachine learning techniques.
In Proceedings of theACL-02 conference on Empirical methods in naturallanguage processing-Volume 10, pages 79?86.
Asso-ciation for Computational Linguistics.Hieu Pham, Minh-Thang Luong, and Christopher DManning.
2015.
Learning distributed representationsfor multilingual text sequences.
In Proceedings ofNAACL-HLT, pages 88?94.255David E Rumelhart, Geoffrey E Hinton, and Ronald JWilliams.
1988.
Learning representations by back-propagating errors.
Cognitive modeling, 5(3):1.Richard Socher, Danqi Chen, Christopher D Manning,and Andrew Ng.
2013.
Reasoning with neuraltensor networks for knowledge base completion.
InAdvances in Neural Information Processing Systems,pages 926?934.Maite Taboada, Julian Brooke, Milan Tofiloski, KimberlyVoll, and Manfred Stede.
2011.
Lexicon-based meth-ods for sentiment analysis.
Computational linguistics,37(2):267?307.Kai Sheng Tai, Richard Socher, and Christopher DManning.
2015.
Improved semantic representationsfrom tree-structured long short-term memory network-s. arXiv preprint arXiv:1503.00075.Xiaojun Wan.
2009.
Co-training for cross-lingualsentiment classification.
In Proceedings of the JointConference of the 47th Annual Meeting of the ACLand the 4th International Joint Conference on NaturalLanguage Processing of the AFNLP: Volume 1-Volume1, pages 235?243.
Association for ComputationalLinguistics.Min Xiao and Yuhong Guo.
2013.
Semi-supervisedrepresentation learning for cross-lingual text classifi-cation.
In Proceedings of EMNLP 2013, pages 1465?1475.Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,Alex Smola, and Eduard Hovy.
2016.
Hierarchicalattention networks for document classification.
InProceedings of the 2016 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 1480?1489.
Association for Computational Lin-guistics.Matthew D Zeiler.
2012.
Adadelta: an adaptive learningrate method.
arXiv preprint arXiv:1212.5701.Huiwei Zhou, Long Chen, Fulin Shi, and Degen Huang.2015.
Learning bilingual sentiment word embeddingsfor cross-language sentiment classification.
In Pro-ceedings of 52rd Annual Meeting of the Associationfor Computational Linguistic, pages 430?440.256
