Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 56?63,Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational LinguisticsCombining the Best of Two Worlds:A Hybrid Approach to Multilingual Coreference ResolutionChen Chen and Vincent NgHuman Language Technology Research InstituteUniversity of Texas at DallasRichardson, TX 75083-0688{yzcchen,vince}@hlt.utdallas.eduAbstractWe describe our system for the CoNLL-2012shared task, which seeks to model corefer-ence in OntoNotes for English, Chinese, andArabic.
We adopt a hybrid approach tocoreference resolution, which combines thestrengths of rule-based methods and learning-based methods.
Our official combined scoreover all three languages is 56.35.
In particu-lar, our score on the Chinese test set is the bestamong the participating teams.1 IntroductionTheCoNLL-2012 shared task extends last year's taskon coreference resolution from a monolingual to amultilingual setting (Pradhan et al, 2012).
Unlikethe SemEval-2010 shared task on Coreference Reso-lution inMultiple Languages (Recasens et al, 2010),which focuses on coreference resolution in Europeanlanguages, the CoNLL shared task is arguably morechallenging: it focuses on three languages that comefrom very different language families, namely En-glish, Chinese, and Arabic.We designed a system for resolving references inall three languages.
Specifically, we participatedin four tracks: the closed track for all three lan-guages, and the open track for Chinese.
In compari-son to last year's participating systems, our resolverhas two distinguishing characteristics.
First, unlikelast year's resolvers, which adopted either a rule-based method or a learning-based method, we adopta hybrid approach to coreference resolution, attempt-ing to combine the strengths of both methods.
Sec-ond, while last year's resolvers did not exploit genre-specific information, we optimize our system's pa-rameters with respect to each genre.Our decision to adopt a hybrid approach is mo-tivated by the observation that rule-based meth-ods and learning-based methods each have theirunique strengths.
As shown by the Stanford coref-erence resolver (Lee et al, 2011), the winner oflast year's shared task, many coreference relations inOntoNotes can be identified using a fairly small setof simple hand-crafted rules.
On the other hand, ourprior work on machine learning for coreference res-olution suggests that coreference-annotated data canbe profitably exploited to (1) induce lexical features(Rahman and Ng, 2011a, 2011b) and (2) optimizesystem parameters with respect to the desired coref-erence evaluation measure (Ng, 2004, 2009).Our system employs a fairly standard architecture,performing mention detection prior to coreferenceresolution.
As we will see, however, the parametersof these two components are optimized jointly withrespect to the desired evaluation measure.In the rest of this paper, we describe the men-tion detection component (Section 2) and the coref-erence resolution component (Section 3), show howtheir parameters are jointly optimized (Section 4),and present evaluation results on the development setand the official test set (Section 5).2 Mention DetectionTo build a mention detector that strikes a relativelygood balance between precision and recall, we em-ploy a two-step approach.
First, in the extrac-tion step, we identify named entities (NEs) and em-ploy language-specific heuristics to extract mentions56from syntactic parse trees, aiming to increase our up-per bound on recall as much as possible.
Then, inthe pruning step, we aim to improve precision byemploying both language-specific heuristic pruningand language-independent learning-based pruning.Section 2.1 describes the language-specific heuris-tics for extraction and pruning, and Section 2.2 de-scribes our learning-based pruning method.2.1 Heuristic Extraction and PruningEnglish.
During extraction, we create a candidatemention from a contiguous text span s if (1) s is aPRP or an NP in a syntactic parse tree; or (2) s cor-responds to a NE that is not a PERCENT, MONEY,QUANTITY or CARDINAL.
During pruning, weremove a candidate mentionmk if (1)mk is embed-ded within a larger mentionmj such thatmj andmkhave the same head, where the head of a mention isdetected using Collins's (1999) rules; (2) mk has aquantifier or a partitive modifier; or (3) mk is a sin-gular common NP, with the exception that we retainmentions related to time (e.g., "today").Chinese.
Similar to English mention extraction,we create Chinese mentions from all NP and QPnodes in syntactic parse trees.
During pruning, weremove a candidate mentionmk if (1)mk is embed-ded within a larger mentionmj such thatmj andmkhave the same head, except if mj and mk appearin a newswire document since, unlike other docu-ment annotations, Chinese newswire document an-notations do consider such pairs coreferent; (2) mkis a NE that is a PERCENT, MONEY, QUANTITYand CARDINAL; or (3) mk is an interrogative pro-noun such as "??
[what]", "??
[where]".Arabic.
We employ as candidate mentions all theNPs extracted from syntactic parse trees, removingthose that are PERCENT, MONEY, QUANTITY orCARDINAL.2.2 Learning-Based PruningWhile the heuristic pruning method identifies can-didate mentions, it cannot determine which candi-date mentions are likely to be coreferent.
To improvepruning (and hence the precision of mention detec-tion), we employ learning-based pruning, where weemploy the training data to identify and subsequentlydiscard those candidate mentions that are not likelyto be coreferent with other mentions.Language Recall Precision F-ScoreEnglish 88.59 40.56 55.64Chinese 85.74 42.52 56.85Arabic 81.49 21.29 33.76Table 1: Mention detection results on the development setobtained prior to coreference resolution.Specifically, for each mention mk in the test setthat survives heuristic pruning, we compute its men-tion coreference probability, which indicates thelikelihood that the head noun of mk is coreferentwith another mention.
If this probability does notexceed a certain threshold tC , we will remove mkfrom the list of candidate mentions.
Section 4 dis-cusses how tC is jointly learned with the parametersof the coreference resolution component to optimizethe coreference evaluation measure.We estimate the mention coreference probabilityofmk from the training data.
Specifically, since onlynon-singleton mentions are annotated in OntoNotes,we can compute this probability as the number oftimes mk 's head noun is annotated (as a gold men-tion) divided by the total number of timesmk 's headnoun appears.
If mk 's head noun does not appear inthe training set, we set its coreference probability to1, meaning that we let it pass through the filter.
Inother words, we try to be conservative and do notfilter any mention for which we cannot compute thecoreference probability.Table 1 shows the mention detection results of thethree languages on the development set after heuris-tic extraction and pruning but prior to learning-basedpruning and coreference resolution.3 Coreference ResolutionLike the mention detection component, our corefer-ence resolution component employs heuristics andmachine learning.
More specifically, we employStanford's multi-pass sieve approach (Lee et al,2011) for heuristic coreference resolution, but sincemost of these sieves are unlexicalized, we seek to im-prove the multi-pass sieve approach by incorporat-ing lexical information using machine learning tech-niques.
As we will see below, while different sievesare employed for different languages, the way we in-corporate lexical information into the sieve approachis the same for all languages.573.1 The Multi-Pass Sieve ApproachA sieve is composed of one or more heuristic rules.Each rule extracts a coreference relation betweentwo mentions based on one or more conditions.
Forexample, one rule in Stanford's discourse processingsieve posits two mentions as coreferent if two con-ditions are satisfied: (1) they are both pronouns; and(2) they are produced by the same speaker.Sieves are ordered by their precision, with themost precise sieve appearing first.
To resolve a setof mentions in a document, the resolver makes mul-tiple passes over them: in the i-th pass, it attemptsto use only the rules in the i-th sieve to find an an-tecedent for each mention mk.
Specifically, whensearching for an antecedent formk, its candidate an-tecedents are visited in an order determined by theirpositions in the associated parse tree (Haghighi andKlein, 2009).
The partial clustering of the mentionscreated in the i-th pass is then passed to the i+1-thpass.
Hence, later passes can exploit the informa-tion computed by previous passes, but a coreferencelink established earlier cannot be overridden later.3.2 The Sieves3.2.1 Sieves for EnglishOur sieves for English are modeled after those em-ployed by the Stanford resolver (Lee et al, 2011),which is composed of 12 sieves.1 Since we partic-ipated in the closed track, we re-implemented the10 sieves that do not exploit external knowledgesources.
These 10 sieves are listed under the "En-glish" column in Table 2.
Specifically, we leave outthe Alias sieve and the Lexical Chain sieve, whichcompute semantic similarity using information ex-tracted from WordNet, Wikipedia, and Freebase.3.2.2 Sieves for ChineseRecall that for Chinese we participated in both theclosed track and the open track.
The sieves we em-ploy for both tracks are the same, except that we useNE information to improve some of the sieves in thesystem for the open track.2 To obtain automatic NEannotations, we employ a NE model that we trainedon the gold NE annotations in the training data.1Table 1 of Lee et al's (2011) paper listed 13 sieves, but oneof them was used for mention detection.2Note that the use of NEs puts a Chinese resolver in the opentrack.English ChineseDiscourse Processing Chinese Head MatchExact String Match Discourse ProcessingRelaxed String Match Exacth String MatchPrecise Constructs Precise ConstructsStrict Head Match A?C Strict Head Match A?CProper Head Match Proper Head MatchRelaxed Head Match PronounsPronouns --Table 2: Sieves for English and Chinese (listed in the or-der in which they are applied).The Chinese resolver is composed of 9 sieves,as shown under the "Chinese" column of Table 2.These sieves are implemented in essentially the sameway as their English counterparts except for a fewof them, which are modified in order to account forsome characteristics specific to Chinese or the Chi-nese coreference annotations.
As described in de-tail below, we introduce a new sieve, the ChineseHead Match sieve, and modify two existing sieves,the Precise Constructs sieve, and the Pronoun sieve.1.
Chinese Head Match sieve: Recall from Sec-tion 2 that the Chinese newswire articles werecoreference-annotated in such away that amen-tion and its embedding mention can be coref-erent if they have the same head.
To iden-tify these coreference relations, we employ theSame Head sieve, which posits two mentionsmj and mk as coreferent if they have the samehead and mk is embedded within mj .
There isan exception to this rule, however: if mj is acoordinated NP composed of two or more baseNPs, and mk is just one of these base NPs, thetwo mentions will not be considered coreferent(e.g., ???????
[Charles and Diana]and???
[Diana]).2.
Precise Constructs sieve: Recall from Leeet al (2011) that the Precise Constructs sieveposits two mentions as coreferent based on in-formation such as whether one is an acronym ofthe other and whether they form an appositiveor copular construction.
We incorporate addi-tional rules to this sieve to handle specific casesof abbreviations in Chinese: (a) Abbreviationof foreign person names, e.g., ???????
[Saddam Hussein] and ???
[Saddam].
(b) Abbreviation of Chinese person names, e.g.,58???
[Chen President] and ?????
[Chen Shui-bian President].
(c) Abbreviationof country names, e.g, ??
[Do country] and????
[Dominica].3.
Pronouns sieve: The Pronouns sieve resolvespronouns by exploiting grammatical informa-tion such as the gender and number of a men-tion.
While such grammatical information isprovided to the participants for English, thesame is not true for Chinese.To obtain such grammatical information forChinese, we employ a simple method, whichconsists of three steps.First, we employ simple heuristics to extractgrammatical information from those ChineseNPs for which such information can be easilyinferred.
For example, we can heuristically de-termine that the gender, number and animacyfor ?
[she] is {Female, Single and Animate};and for??
[they] is {Unknown, Plural, Inani-mate}.
In addition, we can determine the gram-matical attributes of a mention by its namedentity information.
For example, a PERSONcan be assigned the grammatical attributes {Un-known, Single, Animate}.Next, we bootstrap from these mentions withheuristically determined grammatical attributevalues.
This is done based on the observationthat all mentions in the same coreference chainshould agree in gender, number, and animacy.Specifically, given a training text, if one of thementions in a coreference chain is heuristicallylabeled with grammatical information, we au-tomatically annotate all the remaining mentionswith the same grammatical attribute values.Finally, we automatically create six word lists,containing (1) animate words, (2) inanimatewords, (3) male words, (4) female words, (5)singular words, and (6) plural words.
Specif-ically, we populate these word lists with thegrammatically annotated mentions from theprevious step, where each element of a wordlist is composed of the head of a mention and acount indicating the number of times the men-tion is annotated with the corresponding gram-matical attribute value.We can then apply these word lists to determinethe grammatical attribute values of mentions ina test text.
Due to the small size of these wordlists, and with the goal of improving precision,we consider two mentions to be grammaticallyincompatible if for one of these three attributes,onemention has anUnknown value whereas theother has a known value.As seen in Table 2, our Chinese resolver doesnot have the Relaxed String Match sieve, unlike itsEnglish counterpart.
Recall that this sieve markstwo mentions as coreferent if the strings after drop-ping the text following their head words are identical(e.g.,MichaelWolf, andMichaelWolf, a contributingeditor for "New York").
Since person names in Chi-nese are almost always composed of a single wordand that heads are seldom followed by other wordsin Chinese, we believe that Relaxed HeadMatch willnot help identify Chinese coreference relations.
Asnoted before, cases of Chinese person name abbrevi-ation will be handled by the Precise Constructs sieve.3.2.3 Sieves for ArabicWe only employ one sieve for Arabic, the exactmatch sieve.
While we experimented with additionalsieves such as the Head Match sieve and the Pro-nouns sieve, we ended up not employing them be-cause they do not yield better results.3.3 Incorporating Lexical InformationAsmentioned before, we improve the sieve approachby incorporating lexical information.To exploit lexical information, we first computelexical probabilities.
Specifically, for each pair ofmentions mj and mk in a test text, we first com-pute two probabilities: (1) the string-pair probability(SP-Prob), which is the probability that the stringsof the two mentions, sj and sk, are coreferent; and(2) the head-pair probability (HP-Prob), which is theprobability that the head nouns of the two mentions,hj and hk, are coreferent.
For better probability esti-mation, we preprocess the training data and the twomentions by (1) downcasing (but not stemming) eachEnglish word, and (2) replacing each Arabic word wby a string formed by concatenating w with its lem-matized form, its Buckwalter form, and its vocalizedBuckwalter form.
Note that SP-Prob(mj ,mk) (HP-59Prob(mj ,mk)) is undefined if one or both of sj (hj)and sk (hk) do not appear in the training set.Next, we exploit these lexical probabilities to im-prove the resolution of mj and mk by presentingtwo extensions to the sieve approach.
The first ex-tension aims to improve the precision of the sieveapproach.
Specifically, before applying any sieve,we check whether SP-Prob(mj ,mk) ?
tSPL or HP-Prob(mj ,mk)?
tHPL for some thresholds tSPL andtHPL.
If so, our resolver will bypass all of thesieves and simply posit mj and mk as not corefer-ent.
In essence, we use the lexical probabilities toimprove precision, specifically by positing twomen-tions as not coreferent if there is "sufficient" infor-mation in the training data for us to make this de-cision.
Note that if one of the lexical probabilities(say SP-Prob(mj ,mk)) is undefined, we only checkwhether the condition on the other probability (in thiscase HP(mj ,mk) ?
tHPL) is satisfied.
If both ofthem are undefined, this pair of mentions will sur-vive this filter and be processed by the sieve pipeline.The second extension, on the other hand, aims toimprove recall.
Specifically, we create a new sieve,the Lexical Pair sieve, which we add to the end ofthe sieve pipeline and which posits two mentionsmjand mk as coreferent if SP-Prob(mj ,mk) ?
tSPUor HP-Prob(mj ,mk) ?
tHPU .
In essence, we usethe lexical probabilities to improve recall, specifi-cally by positing two mentions as coreferent if thereis "sufficient" information in the training data forus to make this decision.
Similar to the first ex-tension, if one of the lexical probabilities (say SP-Prob(mj ,mk)) is undefined, we only check whetherthe condition on the other probability (in this caseHP(mj ,mk) ?
tHPU ) is satisfied.
If both of themare undefined, the Lexical Pair sieve will not processthis pair of mentions.The four thresholds, tSPL, tHPL, tSPU , andtHPU , will be tuned to optimize coreference perfor-mance on the development set.4 Parameter EstimationAs discussed before, we learn the system parametersto optimize coreference performance (which, for theshared task, is Uavg, the unweighted average of thethree commonly-used evaluation measures, MUC,B3, and CEAFe) on the development set.
Our sys-tem has two sets of tunable parameters.
So far, wehave seen one set of parameters, namely the five lex-ical probability thresholds, tC , tSPL, tHPL, tSPU ,and tHPU .
The second set of parameters contains therule relaxation parameters.
Recall that each rule ina sieve may be composed of one or more conditions.We associate with condition i a parameter ?i, whichis a binary value that controls whether condition ishould be removed or not.
In particular, if ?i=0, con-dition iwill be dropped from the corresponding rule.The motivation behind having the rule relaxation pa-rameters should be clear: they allow us to optimizethe hand-crafted rules using machine learning.
Thissection presents two algorithms for tuning these twosets of parameters on the development set.Before discussing the parameter estimation algo-rithms, recall from the introduction that one of thedistinguishing features of our approach is that webuild genre-specific resolvers.
In other words, foreach genre of each language, we (1) learn the lexi-cal probabilities from the corresponding training set;(2) obtain optimal parameter values ?1 and ?2 forthe development set using parameter estimation al-gorithms 1 and 2 respectively; and (3) among?1 and?2, take the one that yields better performance onthe development set to be the final set of parameterestimates for the resolver.Parameter estimation algorithm 1.
This algo-rithm learns the two sets of parameters in a sequentialfashion.
Specifically, it first tunes the lexical proba-bility thresholds, assuming that all the rule relaxationparameters are set to one.
To tune the five probabil-ity thresholds, we try all possible combinations ofthe five probability thresholds and select the combi-nation that yields the best performance on the devel-opment set.
To ensure computational tractability, weallow each threshold to have the following possiblevalues.
For tC , the possible values are?0.1, 0, 0.05,0.1, .
.
., 0.3; for tSPL and tHPL, the possible valuesare ?0.1, 0, 0.05, 0.15, .
.
., 0.45; and for tSPU andtHPU , the possible values are 0.55, 0.65, .
.
., 0.95,1.0 and 1.1.
Note that the two threshold values?0.1and 1.1 render a probability threshold useless.
Forexample, if tC = ?0.1, that means all mentions willsurvive learning-based pruning in the mention detec-tion component.
As another example, if tSPU andtHPU are both 1.1, it means that the String Pair sieve60will be useless because it will not posit any pair ofmentions as coreferent.Given the optimal set of probability thresholds, wetune the rule relaxation parameters.
To do so, we ap-ply the backward elimination feature selection algo-rithm, viewing each condition as a feature that can beremoved from the "feature set".
Specifically, all theparameters are initially set to one, meaning that allthe conditions are initially present.
In each iterationof backward elimination, we identify the conditionwhose removal yields the highest score on the de-velopment set and remove it from the feature set.
Werepeat this process until all conditions are removed,and identify the subset of the conditions that yieldsthe best score on the development set.Parameter estimation algorithm 2.
In this algo-rithm, we estimate the two sets of parameters in aninterleaved, iterative fashion, where in each itera-tion, we optimize exactly one parameter from oneof the two sets.
More specifically, (1) in iteration2n, we optimize the (n mod 5)-th lexical probabil-ity threshold while keeping the remaining parame-ters constant; and (2) in iteration 2n+1, we optimizethe (n mod m)-th rule relaxation parameter whilekeeping the remaining parameters constant, wheren = 1, 2, .
.
., and m is the number of rule relax-ation parameters.
When optimizing a parameter in agiven iteration, the algorithm selects the value that,when used in combination with the current values ofthe remaining parameters, optimizes theUavg valueon the development set.
We begin the algorithm byinitializing all the rule relaxation parameters to one;tC , tSPL and tHPL to ?0.1; and tSPU and tHPUto 1.1.
This parameter initialization is equivalent tothe configuration where we employ all and only thehand-crafted rules as sieves and do not apply learn-ing to perform any sort of optimization at all.5 Results and DiscussionThe results of our Full coreference resolver on thedevelopment set with optimal parameter values areshown in Table 3.
As we can see, both the men-tion detection results and the coreference results (ob-tained via MUC, B3, and CEAFe) are expressed interms of recall (R), precision (P), and F-measure (F).In addition, to better understand the role played bythe two sets of system parameters, we performed ab-lation experiments, showing for each language-trackcombination the results obtained without tuning (1)the rule relaxation parameters (?
?i's); (2) the proba-bility thresholds (?
tj 's); and (3) any of these param-eters (?
?i's & tj).
Note that (1) we do not have anyrule relaxation parameters for the Arabic resolverowing to its simplicity; and (2) for comparison pur-poses, we show the results of the Stanford resolverfor English in the row labeled "Lee et al (2011)".A few points regarding the results in Table 3 de-serve mention.
First, these mention detection re-sults are different from those shown in Table 1: here,the scores are computed over the mentions that ap-pear in the non-singleton clusters in the coreferencepartitions produced by a resolver.
Second, our re-implementation of the Stanford resolver is as goodas the original one.
Third, parameter tuning is com-paratively less effective for Chinese, presumably be-cause we spent more time on engineering the sievesfor Chinese than for the other languages.
Fourth,our score on Arabic is the lowest among the threelanguages, primarily because Arabic is highly inflec-tional and we have little linguistic knowledge of thelanguage to design effective sieves.
Finally, theseresults and our official test set results (Table 4), aswell as our supplementary evaluation results on thetest set obtained using gold mention boundaries (Ta-ble 5) and gold mentions (Table 6), exhibit similarperformance trends.Table 7 shows the optimal parameter values ob-tained for the Full resolver on the development set.Since there are multiple genres for English and Chi-nese, we show in the table the probability thresholdsaveraged over all the genres and the correspondingstandard deviation values.
For the rule relaxationparameters, among the 36 conditions in the Englishsieves and the 61 conditions in the Chinese sieves,we show the number of conditions being removed(when averaged over all the genres) and the corre-sponding standard deviation values.
Overall, differ-ent conditions were removed for different genres.To get a better sense of the usefulness ofthe probability thresholds, we show in Tables 8and 9 some development set examples of cor-rectly and incorrectly identified/pruned mentionsand coreferent/non-coreferent pairs for English andChinese, respectively.
Note that no Chinese exam-ples for tC are shown, since its tuned value cor-61Mention Detect.
MUC B-CUBED CEAFe AvgLanguage Track System R P F R P F R P F R P F FEnglish Closed Full 74.8 75.6 75.2 65.6 67.3 66.4 69.1 74.7 71.8 49.8 47.9 48.8 62.3?
?i 's 75.2 73.4 74.3 64.6 65.8 65.2 68.5 74.1 71.2 48.8 47.6 48.2 61.5?
tj 's 76.4 73.0 74.7 65.1 65.3 65.2 68.6 73.8 71.1 48.6 48.3 48.4 61.6?
?i 's & tj 's 75.2 72.8 74.0 64.2 64.8 64.5 68.0 73.4 70.6 47.8 47.1 47.5 60.8Lee et al (2011) 74.1 72.5 73.3 64.3 64.9 64.6 68.2 73.1 70.6 47.0 46.3 46.7 60.6Chinese Closed Full 72.2 72.7 72.4 62.4 65.8 64.1 70.8 77.7 74.1 52.3 48.9 50.5 62.9?
?i 's 71.3 72.8 71.9 61.8 66.7 64.2 70.2 78.2 74.0 52.2 47.6 49.9 62.6?
tj 's 72.7 71.1 71.9 62.3 64.8 63.5 70.7 77.1 73.8 51.2 48.8 50.0 62.4?
?i 's & tj 's 71.7 71.4 71.5 61.5 65.1 63.3 70.0 77.6 73.6 51.3 47.9 49.5 62.1Chinese Open Full 73.1 72.6 72.9 63.5 67.2 65.3 71.6 78.2 74.8 52.5 48.9 50.7 63.6?
?i 's 72.5 73.1 72.8 63.2 67.0 65.1 71.3 78.1 74.5 52.4 48.7 50.4 63.3?
tj 's 72.8 72.5 72.7 63.5 66.5 65.0 71.4 77.8 74.5 51.9 48.9 50.4 63.3?
?i 's & tj 's 72.4 72.5 72.4 63.0 66.3 64.6 71.0 77.8 74.3 51.7 48.5 50.1 63.0Arabic Closed Full 56.6 64.5 60.3 40.4 42.8 41.6 58.9 62.7 60.7 40.4 37.8 39.1 47.1?
tj 's 52.0 64.3 57.5 33.1 40.2 36.3 53.4 67.9 59.8 41.9 34.2 37.6 44.6Table 3: Results on the development set with optimal parameter values.Mention Detect.
MUC B-CUBED CEAFe AvgLanguage Track System R P F R P F R P F R P F FEnglish Closed Full 75.1 72.6 73.8 63.5 64.0 63.7 66.6 71.5 69.0 46.7 46.2 46.4 59.7Chinese Closed Full 71.1 72.1 71.6 59.9 64.7 62.2 69.7 77.8 73.6 53.4 48.7 51.0 62.2Chinese Closed Full 71.5 73.5 72.4 62.5 67.1 64.7 71.2 78.4 74.6 53.6 49.1 51.3 63.5Arabic Closed Full 56.2 64.0 59.8 38.1 40.0 39.0 60.6 62.5 61.5 41.9 39.8 40.8 47.1Table 4: Official results on the test set.Mention Detect.
MUC B-CUBED CEAFe AvgLanguage Track System R P F R P F R P F R P F FEnglish Closed Full 74.8 75.7 75.2 63.3 66.8 65.0 65.4 73.6 69.2 48.8 44.9 46.8 60.3Chinese Closed Full 82.0 79.0 80.5 70.8 72.1 71.4 74.4 79.9 77.0 58.0 56.4 57.2 68.6Chinese Open Full 82.4 80.1 81.2 73.5 74.3 73.9 76.3 80.5 78.3 58.2 57.3 57.8 70.0Arabic Closed Full 57.2 62.6 59.8 38.7 39.2 39.0 61.5 61.8 61.7 41.6 40.9 41.2 47.3Table 5: Supplementary results on the test set obtained using gold mention boundaries and predicted parse trees.Mention Detect.
MUC B-CUBED CEAFe AvgLanguage Track System R P F R P F R P F R P F FEnglish Closed Full 80.8 100 89.4 72.3 89.4 79.9 64.6 85.9 73.8 76.3 46.4 57.7 70.5Chinese Closed Full 84.7 100 91.7 76.6 92.4 83.8 73.0 91.4 81.2 83.6 57.9 68.4 77.8Chinese Open Full 84.8 100 91.8 78.1 93.2 85.0 75.0 91.6 82.5 84.0 59.2 69.4 79.0Arabic Closed Full 58.3 100 73.7 41.7 63.2 50.3 50.0 75.3 60.1 64.6 36.2 46.4 52.3Table 6: Supplementary results on the test set obtained using gold mentions and predicted parse trees.tC tHPL tSPL tHPU tSPU Rule RelaxationLanguage Track Avg.
St.Dev.
Avg.
St.Dev.
Avg.
St.Dev.
Avg.
St.Dev.
Avg.
St.Dev.
Avg.
St.Dev.English Closed ?0.06 0.11 ?0.04 0.08 ?0.06 0.12 0.90 0.23 0.60 0.05 6.13 1.55Chinese Closed ?0.10 0.00 ?0.08 0.06 0.00 0.95 1.01 0.22 0.88 0.27 4.67 1.63Chinese Open ?0.10 0.00 ?0.08 0.06 ?0.05 0.05 1.01 0.22 0.88 0.27 5.83 1.94Arabic Closed 0.05 0.00 0.00 0.00 ?0.10 0.00 1.10 0.00 0.15 0.00 0.00 0.00Table 7: Optimal parameter values.responds to the case where no mentions should bepruned.6 ConclusionWe presented a multilingual coreference resolver de-signed for the CoNLL-2012 shared task.
We adopted62Parameter Correct IncorrecttC no problem; the same that; that ideatHPL (people,that); (both of you,that) (ours,they); (both of you,us)tSPL (first,first); (the previous year,its) (China,its); (Taiwan,its)tHPU (The movie's,the film); (Firestone,the company's) (himself,he); (My,I)tSPU (Barak,the Israeli Prime Minister); (she,the woman); (Taiwan,the island)(Kostunica,the new Yugoslav President)Table 8: Examples of correctly & incorrectly identified/pruned English mentions and coreferent/non-coreferent pairs.Parameter Correct IncorrecttC --- ---tHPL (????,??
); (????,?)
(?????,??
); (??,?
)tSPL (??,??
); (???,???)
(??,??
); (??,?
)tHPU (??,????
); (??,???)
(???,??
); (??,?
)tSPU (??,????
); (??,??
); (??,?)
; (????,??
)Table 9: Examples of correctly & incorrectly identified/pruned Chinese mentions and coreferent/non-coreferent pairs.a hybrid approach to coreference resolution, whichcombined the advantages of rule-based methods andlearning-based methods.
Specifically, we proposedtwo extensions to Stanford's multi-pass sieve ap-proach, which involved the incorporation of lexicalinformation using machine learning and the acqui-sition of genre-specific resolvers.
Experimental re-sults demonstrated the effectiveness of these exten-sions, whether or not they were applied in isolationor in combination.In future work, we plan to explore other waysto combine rule-based methods and learning-basedmethods for coreference resolution, as well as im-prove the performance of our resolver on Arabic.AcknowledgmentsWe thank the two anonymous reviewers for theircomments on the paper.
This work was supported inpart by NSF Grants IIS-0812261 and IIS-1147644.ReferencesMichael John Collins.
1999.
Head-Driven StatisticalModels for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania, Philadelphia, PA.Aria Haghighi and Dan Klein.
2009.
Simple coreferenceresolution with rich syntactic and semantic features.In Proceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing, pages 1152--1161.Heeyoung Lee, Yves Peirsman, Angel Chang, NathanaelChambers, Mihai Surdeanu, and Dan Jurafsky.
2011.Stanford's multi-pass sieve coreference resolution sys-tem at the CoNLL-2011 shared task.
In Proceedingsof the Fifteenth Conference on Computational NaturalLanguage Learning: Shared Task, pages 28--34.Vincent Ng.
2004.
Learning noun phrase anaphoricityto improve conference resolution: Issues in represen-tation and optimization.
In Proceedings of the 42ndMeeting of the Association for Computational Linguis-tics, pages 151--158.Vincent Ng.
2009.
Graph-cut-based anaphoricity deter-mination for coreference resolution.
In Proceedings ofthe 2009 Conference of the North American Chapterof the Association for Computational Linguistics: Hu-man Language Technologies, pages 575--583.Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,Olga Uryupina, and Yuchen Zhang.
2012.
CoNLL-2012 shared task: Modeling multilingual unrestrictedcoreference in OntoNotes, In Proceedings of theSixteenth Conference on Computational Natural Lan-guage Learning.Altaf Rahman and Vincent Ng.
2011a.
Coreference reso-lution with world knowledge.
In Proceedings of the49th Annual Meeting of the Association for Compu-tational Linguistics: Human Language Technologies,pages 814--824.Altaf Rahman and Vincent Ng.
2011b.
Narrowing themodeling gap: A cluster-ranking approach to corefer-ence resolution.
Journal of Artificial Intelligence Re-search, 40:469--521.Marta Recasens, Llu?s M?rquez, Emili Sapena,M.
Ant?nia Mart?, Mariona Taul?, V?roniqueHoste, Massimo Poesio, and Yannick Versley.
2010.Semeval-2010 task 1: Coreference resolution in multi-ple languages.
In Proceedings of the 5th InternationalWorkshop on Semantic Evaluation, pages 1--8.63
