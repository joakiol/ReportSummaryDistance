Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 199?205,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsEffective Document-Level Features for Chinese Patent WordSegmentationSi LiChinese Language Processing GroupBrandeis UniversityWaltham, MA 02453, USAlisi@brandeis.eduNianwen XueChinese Language Processing GroupBrandeis UniversityWaltham, MA 02453, USAxuen@brandeis.eduAbstractA patent is a property right for an inven-tion granted by the government to the in-ventor.
Patents often have a high con-centration of scientific and technical termsthat are rare in everyday language.
How-ever, some scientific and technical termsusually appear with high frequency onlyin one specific patent.
In this paper, wepropose a pragmatic approach to Chineseword segmentation on patents where wetrain a sequence labeling model based ona group of novel document-level features.Experiments show that the accuracy of ourmodel reached 96.3% (F1score) on the de-velopment set and 95.0% on a held-out testset.1 IntroductionIt is well known that Chinese text does not comewith natural word delimiters, and the first stepfor many Chinese language processing tasks isword segmentation, the automatic determinationof word boundaries in Chinese text.
Tremendousprogress was made in this area in the last decadeor so due to the availability of large-scale humansegmented corpora coupled with better statisticalmodeling techniques.
On the data side, there exista few large-scale human annotated corpora basedon established word segmentation standards, andthese include the Chinese TreeBank (Xue et al,2005), the Sinica Balanced Corpus (Chen et al,1996), the PKU Peoples?
Daily Corpus (Duan etal., 2003), and the LIVAC balanced corpus (T?souet al, 1997).
Another driver for the improvemen-t in Chinese word segmentation accuracy comesfrom the evolution of statistical modeling tech-niques.
Dictionaries used to play a central rolein early heuristics-based word segmentation tech-niques (Chen and Liu, 1996; Sproat et al, 1996).Modern word segmentation systems have movedaway from dictionary-based approaches in favorof character tagging approaches.
This allows theword segmentation problem to be modeled as asequence labeling problem, and lends itself to dis-criminative sequence modeling techniques (Xue,2003; Peng et al, 2004).
With these better model-ing techniques, state-of-the-art systems routinelyreport accuracy in the high 90%, and a few recen-t systems report accuracies of over 98% in F1s-core (Sun, 2011; Zeng et al, 2013b).Chinese word segmentation is not a solvedproblem however and significant challenges re-main.
Advanced word segmentation systems per-form very well in domains such as newswirewhere everyday language is used and there is alarge amount of human annotated training data.There is often a rapid degradation in performancewhen systems trained on one domain (let us call itthe source domain) are used to segment data in adifferent domain (let us call it the target domain).This problem is especially severe when the targetdomain is distant from the source domain.
This isthe problem we are facing when we perform wordsegmentation on Chinese patent data.
The wordsegmentation accuracy on Chinese patents is verypoor if the word segmentation model is trained onthe Chinese TreeBank data, which consists of datasources from a variety of genres but no patents.To address this issue, we annotated a corpus of142 patents which contain about 440K words ac-cording to the Chinese TreeBank standards.
Wetrained a character-tagging based CRF model forword segmentation, and based on the writing styleof patents, we propose a group of document-levelfeatures as well as a novel character part-of-speechfeature (C_POS).
Our results show these new fea-tures are effective and we are able to achieve anaccuracy of 96.3% (F1score) on the developmentset and 95% (F1score) on the test set.1992 MethodWe adopt the character-based sequence labelingapproach, first proposed in (Xue, 2003), as ourmodeling technique for its simplicity and effec-tiveness.
This approach treats each sentence as asequence of characters and assigns to each charac-ter a label that indicates its position in the word.
Inthis paper, we use the BMES tag set to indicate thecharacter positions.
The tag set has four labels thatrepresent for possible positions a character can oc-cupy within a word: B for beginning, M for mid-dle, E for ending, and S for a single character as aword.
After each character in a sentence is taggedwith a BMES label, a sequence of words can bederived from this labeled character sequence.We train a Conditional Random Field (CRF)(Lafferty et al, 2001) model for this sequencelabeling.
When extracting features to train aCRF model from a sequence of n charactersC1C2...Ci?1CiCi+1...Cn, we extract features foreach character Cifrom a fixed window.
We startwith a set of core features extracted from the anno-tated corpus that have been shown to be effectivein previous works and propose some new featuresfor patent word segmentation.
We describe eachgroup of features in detail below.2.1 Character features (CF)When predicting the position of a character with-in a word, features based on its surrounding char-acters and their types have shown to be the mosteffective features for this task (Xue, 2003).
Thereare some variations of these features depending onthe window size in terms of the number of char-acters to examine, and here we adopt the featuretemplates used in (Ng and Low, 2004).Character N-gram features The N-gram fea-tures are various combinations of the surroundingcharacters of the candidate character Ci.
The 10features we used are listed below:?
Character unigrams: Ck(i?
3 < k < i+ 3)?
Character bigrams: CkCk+1(i ?
3 < k <i+ 2) and Ck?1Ck+1(k = i)Character type N-gram features We classifythe characters in Chinese text into 4 types: Chi-nese characters or hanzi, English letters, numbersand others.
Tiis the character type of Ci.
Thecharacter type has been used in the previous work-s in various forms (Ng and Low, 2004; Jiang et al,2009), and the 4 features we use are as follows:?
Character type unigrams: Tk(k = i)?
Character type bigrams: TkTk+1(i?2 < k <i+ 1) and Tk?1Tk+1(k = i)Starting with this baseline, we extract some newfeatures to improve Chinese patent word segmen-tation accuracy.2.2 POS of single-character words (C_POS)Chinese words are composed of Chinese hanzi,and an overwhelming majority of these Chinesecharacters can be single-character words them-selves in some context.
In fact, most of the multi-character words are compounds that are 2-4 char-acters in length.
The formation of these compoundwords is not random and abide by word formationrules that are similar to the formation of phras-es (Xue, 2000; Packard, 2000).
In fact, the Chi-nese TreeBank word segmentation guidelines (X-ia, 2000) specify how words are segmented basedon the part-of-speech (POS) of their componen-t characters.
We hypothesize that the POS tagsof the single-character words would be useful in-formation to help predict how they form the com-pound words, and these POS tags are more fine-grained information than the character type infor-mation described in the previous section, but aremore robust and more generalizable than the char-acters themselves.Since we do not have POS-tagged patent da-ta, we extract this information from the ChineseTreeBank (CTB) 7.0, a 1.2-million-word out-of-domain dataset.
We extract the POS tags for al-l the single-character words in the CTB.
Some ofthe single-character words will have more than onePOS tag.
In this case, we select the POS tag withthe highest frequency as the C_POS tag for thischaracter.
The result of this extraction process isa list of single-character Chinese words, each ofwhich is assigned a single POS tag.When extracting features for the target characterCi, if Ciis in this list, the POS tag of Ciis used asa feature for this target character.2.3 Document-level featuresA patent is a property right for an invention grant-ed by the government to the inventor, and many ofthe patents have a high concentration of scientif-ic and technical terms.
From a machine learningperspective, these terms are hard to detect and seg-ment because they are often "new words" that arenot seen in everyday language.
These technical200Algorithm 1 Longest n-gram sequence extraction.Input:Sentences {si} in patent Pi;Output:Longest n-gram sequence list for Pi;1: For each sentence siin Pido:n-gram sequence extraction(2?n?length(si));2: Count the frequency of each n-gram sequence;3: Delete the sequence if its frequency<2;4: Delete sequence i if it is contained in a longersequence j;5: All the remaining sequences form a longest n-gram sequence list for Pi;6: return Longest n-gram sequences list.terminologies also tend to be very sparse, eitherbecause they are related to the latest invention thathas not made into everyday language, or becauseour limited patent dataset cannot possibly cover allpossible technical topics.
However, these techni-cal terms are also topical and they tend to havehigh relative frequency within a patent documenteven though they are sparse in the entire patent da-ta set.
We attempt to exploit this distribution prop-erty with some document-level features which areextracted based on each patent document.Longest n-gram features (LNG) We propose alongest n-gram (LNG) feature as a document-levelfeature.
Each patent document is treated as an in-dependent unit and the candidate longest n-gramsequence lists for each patent are obtained as de-scribed in Algorithm 1.For a given patent, the LNG feature value for thetarget character Ci?s LNG is set to 'S' if the bigram(Ci,Ci+1) are the first two characters of an n-gramsequence in this patent?s longest n-gram sequencelist.
If (Ci?1, Ci) are the last two characters of ann-gram sequence in this patent?s longest n-gramsequence list, the target character Ci?s LNG is setto 'F'.
It is set to 'O' otherwise.
If Cican be labeledas both 'S' and 'F' at the same time, label 'T' will begiven as the final label.
For example, if '?'
is thetarget character Ciin patent A and the sequence'??Z6?'
is in patent A?s longest n-gram se-quence list.
If the character next to '?'
is '?
', thevalue of the LNG feature is set to 'S'.
If the nextcharacter is not '?
', the value of the LNG featureis set to 'O'.Algorithm 2 Pseudo KL divergence.Input:Sentences {si} in patent Pi;Output:Pseudo KL divergence values between differ-ent characters in Pi;1: For each sentence siin Pido:trigram sequences extraction;2: Count the frequency of each trigram;3: Delete the trigram if its frequency<2;4: For Ciin trigram CiCi+1Ci+2do :PKL(Ci, Ci+1) = p(Ci1)logp(Ci1)p(Ci+12)(1)PKL(Ci, Ci+2) = p(Ci1)logp(Ci1)p(Ci+23)(2)The superscripts {1,2,3} indicate the characterposition in trigram sequences;5: return PKL(Ci, Ci+1) and PKL(Ci, Ci+2)for the first character Ciin each trigram.Pseudo Kullback-Leibler divergence (PKL)The second document-level feature we proposeis the Pseudo Kullback-Leibler divergence fea-ture which is calculated following the form ofthe Kullback-Leibler divergence.
The relativeposition information is very important for Chi-nese word segmentation as a sequence labelingtask.
Characters XY may constitute a meaningfulword, but characters Y X may not be.
Therefore,if we want to determine whether character X andcharacter Y can form a word, the relative positionof these two characters should be considered.
Weadopt a pseudo KL divergence with the relative po-sition information as a measure of the associationstrength between two adjacent characters X andY .
The pseudo KL divergence is an asymmetricmeasure.
The PKL value between character Xand character Y is described in Algorithm 2.The PKL values are real numbers and are s-parse.
A common solution to sparsity reductionis binning.
We rank the PKL values between t-wo adjacent characters in each patent from low tohigh, and then divide all values into five bins.
Eachbin is assigned a unique ID and all PKL values inthe same bin are replaced by this ID.
This ID isthen used as the PKL feature value for the targetcharacter Ci.201Pointwise Mutual information (PMI) Point-wise Mutual information has been widely usedin previous work on Chinese word segmentation(Sun and Xu, 2011; Zhang et al, 2013b) and it is ameasure of the mutual dependence of two stringsand reflects the tendency of two strings appearingin one word.
In previous work, PMI statistics aregathered on the entire data set, and here we gatherPMI statistics for each patent in an attempt to cap-ture character strings with high PMI in a particu-lar patent.
The procedure for calculating PMI isthe same as that for computing pseudo KL diver-gence, but the functions (1) and (2) are replacedwith the following functions:PMI(Ci, Ci+1) = logp(Ci1, Ci+12)p(Ci1)p(Ci+12)(3)PMI(Ci, Ci+2) = logp(Ci1, Ci+23)p(Ci1)p(Ci+23)(4)For the target character Ci, we obtain the valuesfor PMI(Ci, Ci+1) and PMI(Ci, Ci+2).
In eachpatent document, we rank these values from highto low and divided them into five bins.
Then thePMI feature values are represented by the bin IDs.3 Experiments3.1 Data preparationWe annotated 142 Chinese patents following theCTB word segmentation guidelines (Xia, 2000).Since the original guidelines are mainly designedto cover non-technical everyday language, manyscientific and technical terms found in patents arenot covered in the guidelines.
We had to extendthe CTB word segmentation guidelines to han-dle these new words.
Deciding on how to seg-ment these scientific and technical terms is a bigchallenge since these patents cover many differ-ent technical fields and without proper technicalbackground, even a native speaker has difficultyin segmenting them properly.
For difficult scien-tific and technical terms, we consult BaiduBaike("Baidu Encyclopedia")1, which we use as a scien-tific and technical terminology dictionary duringour annotation.
There are still many words thatdo not appear in BaiduBaiKe, and these includechemical names and formulas.
These chemicalnames and formulas (e.g., /??
???
?Z/1-bromo-3-chloropropane0) are usually very1http://baike.baidu.com/Table 1: Training, development and test data onPatent dataData set # of words # of patentTraining 345336 113Devel.
46196 14Test 48351 15long, and unlike everyday words, they often havenumbers and punctuation marks in them.
We de-cided not to try segmenting the internal structuresof such chemical terms and treat them as singlewords, because without a technical background inchemistry, it is very hard to segment their internalstructures consistently.The annotated patent dataset covers many topicsand they include chemistry, mechanics, medicine,etc.
If we consider the words in our annotateddataset but not in CTB 7.0 data as new words (orout-of-vocabulary, OOV), the new words accountfor 18.3% of the patent corpus by token and 68.1%by type.
This shows that there is a large number ofwords in the patent corpus that are not in the ev-eryday language vocabulary.
Table 1 presents thedata split used in our experiments.3.2 Main resultsWe use CRF++ (Kudo, 2013) to train our sequencelabeling model.
Precision, recall, F1score andROOVare used to evaluate our word segmentationmethods, whereROOVfor our purposes means therecall of new words which do not appear in CTB7.0 but in patent data.Table 2 shows the segmentation results on thedevelopment and test sets with different featuretemplates and different training sets.
The CTBtraining set includes the entire CTB 7.0, which has1.2 million words.
The model with the CF fea-ture template is considered to be the baseline sys-tem.
We conducted 4 groups of experiments basedon the different datasets: (1) patent training set +patent development set; (2) patent training set +patent test set; (3) CTB training set + patent de-velopment set; (4) CTB training set + patent testset.The results in Table 2 show that the model-s trained on the patent data outperform the mod-els trained on the CTB data by a big margin onboth the development and test set, even if the CTBtraining set is much bigger.
That proves the im-portance of having a training set in the same do-202Table 2: Segmentation performance with different feature sets on different datasets.Train set Test set Features P R F1ROOVPatent train Patent dev.CF 95.34 95.28 95.32 90.02CF+C_POS 95.58 95.40 95.49 90.40CF+C_POS+LNG 96.32 96.00 96.15 91.22CF+C_POS+PKL 95.62 95.41 95.51 90.40CF+C_POS+PMI 95.65 95.40 95.53 89.94CF+C_POS+PMI+PKL 95.72 95.53 95.62 90.37CF+C_POS+LNG+PMI 96.42 96.09 96.26 91.66CF+C_POS+LNG+PMI+PKL 96.48 96.12 96.30 91.69Patent train Patent testCF 93.98 94.49 94.23 85.19CF+C_POS+LNG+PKL+PMI 94.89 95.10 95.00 87.89CTB train Patent dev.
CF+C_POS+LNG+PKL+PMI 89.04 90.75 89.89 72.80CTB train Patent test CF+C_POS+LNG+PKL+PMI 87.88 89.03 88.45 70.89main.
The results also show that adding the newfeatures we proposed leads to consistent improve-ment across all experimental conditions, and thatthe LNG features are the most effective and bringabout the largest improvement in accuracy.4 Related workMost of the previous work on Chinese word seg-mentation focused on newswire, and one wide-ly adopted technique is character-based represen-tation combined with sequential learning models(Xue, 2003; Low et al, 2005; Zhao et al, 2006;Sun and Xu, 2011; Zeng et al, 2013b; Zhanget al, 2013b; Wang and Kan, 2013).
More re-cently, word-based models using perceptron learn-ing techniques (Zhang and Clark, 2007) also pro-duce very competitive results.
There are also somerecent successful attempts to combine character-based and word-based techniques (Sun, 2010;Zeng et al, 2013a).As Chinese word segmentation has reached avery high accuracy in the newswire domain, theattention of the field has started to shift to otherdomains where there are few annotated resourcesand the problem is more challenging, such as workon the word segmentation of literature data (Li-u and Zhang, 2012) and informal language gen-res (Wang and Kan, 2013; Zhang et al, 2013a).Patents are distinctly different from the above gen-res as they contain scientific and technical termsthat require some special training to understand.There has been very little work in this area, andthe only work that is devoted to Chinese wordsegmentation is (Guo et al, 2012), which reportswork on Chinese patent word segmentation witha fairly small test set without any annotated train-ing data in the target domain.
They reported anaccuracy of 86.42% (F1score), but the results areincomparable with ours as their evaluation data isnot available to us.
We differ from their work inthat we manually segmented a significant amountof data, and trained a model with document-levelfeatures designed to capture the characteristics ofpatent data.5 ConclusionIn this paper, we presented an accurate character-based word segmentation model for Chinesepatents.
Our contributions are two-fold.
Our firstcontribution is that we have annotated a signifi-cant amount of Chinese patent data and we planto release this data once the copyright issues havebeen cleared.
Our second contribution is that wedesigned document-level features to capture thedistributional characteristics of the scientific andtechnical terms in patents.
Experimental resultsshowed that the document-level features we pro-posed are effective for patent word segmentation.AcknowledgmentsThis paper is supported by the Intelligence Ad-vanced Research Projects Activity (IARPA) vi-a contract NO.
D11PC20154.
All views ex-pressed in this paper are those of the authors anddo not necessarily represent the view of IARPA,DoI/NBC, or the U.S. Government.203ReferencesKeh-Jiann Chen and Shing-Huan Liu.
1996.
WordIdentification for Mandarin Chinese Sentences.
InProceedings of COLING?92, pages 101?107.Keh-Jiann Chen, Chu-Ren Huang, Li-Ping Chang, andHui-Li Hsu.
1996.
Sinica Corpus: Design Method-ology for Balanced Corpora.
In Proceedings of the11 th Pacific Asia Conference on Language, Infor-mation and Computation, pages 167?176.Huiming Duan, Xiaojing Bai, Baobao Chang, and Shi-wen Yu.
2003.
Chinese word segmentation atPeking University.
In Proceedings of the secondSIGHAN workshop on Chinese language process-ing, pages 152?155.Zhen Guo, Yujie Zhang, Chen Su, and Jinan Xu.
2012.Exploration of N-gram Features for the DomainAdaptation of Chinese Word Segmentation.
In Pro-ceedings of Natural Language Processing and Chi-nese Computing Natural Language Processing andChinese Computing, pages 121?131.Wenbin Jiang, Liang Huang, and Qun Liu.
2009.
Au-tomatic Adaptation of Annotation Standards: Chi-nese Word Segmentation and POS Tagging - A CaseStudy.
In Proceedings of ACL?09, pages 522?530.Taku Kudo.
2013.
CRF++: Yet Another CRF toolkit.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Proceedings of ICML?01, pages282?289.Yang Liu and Yue Zhang.
2012.
Unsupervised Do-main Adaptation for Joint Segmentation and POS-Tagging.
In Proceedings of COLING?12, pages745?754.Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo.
2005.A Maximum Entropy Approach to Chinese WordSegmentation.
In Proceedings of the 4th SIGHANWorkshop on Chinese Language Processing, pages970?979.Hwee Tou Ng and Jin Kiat Low.
2004.
Chinese Part-of-Speech Tagging: One-at-a-Time or All-at-Once?Word-Based or Character-Based?
In Proceedings ofEMNLP?04, pages 277?284.Jerome Packard.
2000.
The Morphology of Chinese: acognitive and linguistic approach.
Cambridge Uni-versity Press.Fuchun Peng, Fangfang Feng, and Andrew McCallum.2004.
Chinese Segmentation and New Word Detec-tion using Conditional Random Fields.
In Proceed-ings of COLING?04.Richard Sproat, Chilin Shih, William Gale, and Nan-cy Chang.
1996.
A Stochastic Finite-State Word-Segmentation Algorithm for Chinese.
Computation-al Linguistics, 22(3):377?404.Weiwei Sun and Jia Xu.
2011.
Enhancing ChineseWord Segmentation Using Unlabeled Data.
In Pro-ceedings of EMNLP?11, pages 970?979.Weiwei Sun.
2010.
Word-based and character-basedword segmentation models: Comparison and com-bination.
In Proceedings of ACL?10, pages 1211?1219.Weiwei Sun.
2011.
A Stacked Sub-Word Modelfor Joint Chinese Word Segmentation and Part-of-Speech Tagging.
In Proceedings of ACL?11, pages1385?1394.Benjamin K. T?sou, Hing-Lung Lin, Godfrey Liu,Terence Chan, Jerome Hu, Ching hai Chew, andJohn K.P.
Tse.
1997.
A Synchronous Chinese Lan-guage Corpus from Different Speech Communities:Construction and Application.
International Jour-nal of Computational Linguistics and Chinese Lan-guage Processing, 2(1):91?104.Aobo Wang and Min-Yen Kan. 2013.
Mining Infor-mal Language from Chinese Microtext: Joint WordRecognition and Segmentation.
In Proceedings ofACL?13, pages 731?741.Fei Xia.
2000.
The segmentation guidelines for thePenn Chinese Treebank (3.0).Nianwen Xue, Fei Xia, Fu-Dong Chiou, and MarthaPalmer.
2005.
The Penn Chinese TreeBank: PhraseStructure Annotation of a Large Corpus.
NaturalLanguage Engineering, 11(2):207?238.Nianwen Xue.
2000.
Defining and identifying wordsin Chinese.
Ph.D. thesis, University of Delaware.Nianwen Xue.
2003.
Chinese Word Segmentation asCharacter Tagging.
International Journal of Com-putational Linguistics and Chinese Language Pro-cessing, 8(1):29?48.Xiaodong Zeng, Derek F. Wong, Lidia S. Chao, andIsabel Trancoso.
2013a.
Co-regularizing character-based and word-based models for semi-supervisedChinese word segmentation.
In Proceedings of A-CL?13, pages 171?176.Xiaodong Zeng, Derek F. Wong, Lidia S. Chao, andIsabel Trancoso.
2013b.
Graph-based Semi-Supervised Model for Joint Chinese Word Segmen-tation and Part-of-Speech Tagging.
In Proceedingsof ACL?13, pages 770?779.Yue Zhang and Stephen Clark.
2007.
Chinese Seg-mentation Using a Word-based Perceptron Algorith-m.
In Proceedings of ACL?07, pages 840?847.Longkai Zhang, Li Li, Zhengyan He, Houfeng Wang,and Ni Sun.
2013a.
Improving Chinese Word Seg-mentation on Micro-blog Using Rich Punctuations.In Proceedings of ACL?13, pages 177?182.204Longkai Zhang, Houfeng Wang, Xu Sun, and MairgupMansur.
2013b.
Exploring Representations fromUnlabeled Data with Co-training for Chinese WordSegmentation.
In Proceedings of EMNLP?13, pages311?321.Hai Zhao, Chang-Ning Huang, and Mu Li.
2006.
Animproved Chinese word segmentation system withconditional random field.
In Proceedings of the 5thSIGHAN Workshop on Chinese Language Process-ing, pages 162?165.205
