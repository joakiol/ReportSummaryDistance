Coling 2008: Companion volume ?
Posters and Demonstrations, pages 59?62Manchester, August 2008Bayes Risk-based Dialogue Managementfor Document Retrieval System with Speech InterfaceTeruhisa Misu ?
?
?School of Informatics,Kyoto UniversitySakyo-ku, Kyoto, JapanTatsuya Kawahara?
?National Institute of Informationand Communications TechnologyHikari-dai, Seika-cho, Soraku-gun,Kyoto, JapanAbstractWe propose an efficient dialogue manage-ment for an information navigation sys-tem based on a document knowledge basewith a spoken dialogue interface.
In orderto perform robustly for fragmental speechinput and erroneous output of an auto-matic speech recognition (ASR), the sys-tem should selectively use N-best hypothe-ses of ASR and contextual information.The system also has several choices in gen-erating responses or confirmations.
In thiswork, we formulate the optimization ofthe choices based on a unified criterion:Bayes risk, which is defined based on re-ward for correct information presentationand penalty for redundant turns.
We haveevaluated this strategy with a spoken di-alogue system which also has question-answering capability.
Effectiveness of theproposed framework was confirmed in thesuccess rate of retrieval and the averagenumber of turns.1 IntroductionIn the past years, a great number of spoken dia-logue systems have been developed.
Their typi-cal task domains include airline information (ATIS& DARPA Communicator) and bus location tasks.Although the above systems can handle simpledatabase retrieval or transactions with constraineddialogue flows, they are expected to handle morec?
2008.
Teruhisa Misu and Tatsuya Kawahara,Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license(http://creativecommons.org/licenses/by-nc-sa/3.0/).
Somerights reserved.complex tasks.
Meanwhile, more and more elec-tronic text resources are recently being accumu-lated.
Since most documents are indexed (e.g., viaWeb search engines), we are potentially capable ofaccessing these documents.
Reflecting such a situ-ation, in recent years, the target of spoken dialoguesystems has been extended to retrieval of generaldocuments (Chang et al, 2002).There are quite a few choices for handling userutterances and generating responses in the spo-ken dialogue systems that require parameter tun-ing.
Since a subtle change in these choices mayaffect the behavior the entire system, they are usu-ally tuned by hand by an expert.
It is also thecase in speech-baed document retrieval systems.We can make use of N-best hypotheses to realizerobust retrieval against errors in automatic speechrecognition (ASR).
Input queries are often vagueor fragmented in speech interfaces, thus concate-nation of contextual information is important tomake meaningful retrieval.
Such decisions tend tobe optimized module by module, but they shouldbe done in an integrated way.
For example, wecould make more appropriate retrieval by rescoringthe N-best ASR hypotheses by the information re-trieval scores.
Even if the target document is iden-tified, the system has several choices for generat-ing responses.
Confirmation is needed to elimi-nate any misunderstandings caused by ASR errors,but users easily become irritated with so many re-dundant confirmations.
Although there are severalworks dealing with dialogue management in callrouting systems (Levin and Pieraccini, 2006), theycannot handle the complex decision making pro-cesses in information guidance tasks.Therefore, we address the extension of conven-tional optimization methods of dialogue manage-ment to be applicable to general document retrieval59tasks.
In particular, we propose a dialogue man-agement that optimizes the choices in responsegeneration by minimizing Bayes risk.
The Bayesrisk is defined based on reward for correct informa-tion presentation and penalty for redundant turnsas well as the score of document retrieval and an-swer extraction.2 Task and Knowledge Base (KB)As the target domain, we adopt a sightseeingguide for Kyoto city.
The KBs of this system areWikipedia documents concerning Kyoto and theofficial tourist information of Kyoto city (810 doc-uments, 220K words in total).
?Dialogue Navigator for Kyoto City?
is a doc-ument retrieval system with a spoken dialogue in-terface.
The system can retrieve information fromthe above-mentioned document set.
This system isalso capable of handling user?s specific question,such as ?Who built this shrine??
using the QAtechnique.3 Dialogue Management and ResponseGeneration in Document RetrievalSystem3.1 Choices in Generating ResponsesWe analyzed the dialogue sessions collected in thefield trial of the ?Dialogue Navigator for KyotoCity?, and found that we could achieve a highersuccess rate by dealing with following issues.1.
Use of N-best hypotheses of ASRThere have been many studies that have usedthe N-best hypotheses (or word graph) of ASRfor making robust interpretations of user utter-ances in relational database query tasks (Ray-mond et al, 2003).
We also improved retrievalby using all of the nouns in the 3-best hypothe-ses(Misu and Kawahara, 2007).
However, theanalysis also showed that some retrieval fail-ures were caused by some extraneous nouns in-cluded in erroneous hypotheses, and a highersuccess rate could be achieved by selecting anoptimal hypothesis.2.
Incorporation of contextual informationIn interactive query systems, users tend to makequeries that include anaphoric expressions.
Inthese cases, it is impossible to extract the cor-rect answer using only the current query.
Forexample, ?When was it built??
makes no sensewhen used by itself.
We deal with this problemby concatenating the contextual information orkeywords from the user?s previous utterances togenerate a query.
However, this may include in-appropriate context when the user changes thetopic.3.
Choices in generating responses or confirma-tionsAn indispensable part of the process to avoidpresenting inappropriate documents is confir-mation, especially when the score of retrievalis low.
This decision is also affected by points 1and 2 mentioned above.
The presentation of theentire document may also be ?safer?
than pre-senting the specific answer to the user?s ques-tion, when the score of answer extraction is low.3.2 Generation of Response CandidatesThe manners of response for a document d con-sist of the following three actions.
One is the pre-sentation (Pres(d)) of the document d, which ismade by summarizing it.
Second is making a con-firmation (Conf(d)) for presenting the documentd.
The last is answering (Ans(d)) the user?s spe-cific question, which is generated by extracting onespecific sentence from the document d.For these response candidates, we define theBayes risk based on the reward for success, thepenalty for a failure, and the probability of suc-cess.
Then, we select the candidate with the mini-mal Bayes risk.
The system flow of these processesis summarized below.1.
Make search queries Wi(i = 1, .
.
.
, 8) using the1st, 2nd, and 3rd hypothesis of ASR, and all ofthem, with/without contextual information.2.
For each query Wi, retrieve from the KB andobtain a candidate document diand its likeli-hood p(di).3.
For each document di, generate presentationPres(di), confirmation Conf(di), and answer-ing Ans(di) response candidates.4.
Calculate the Bayes risk for 25 response can-didates, which are the combination of 4 (N-besthypotheses)?
2 (use of contextual information)?
3 (choice in response generation) + 1 (rejec-tion).5.
Select the optimal response candidate that hasthe minimal Bayes risk.603.3 Definition of Bayes Risk for CandidateResponseFor these response candidates, we define the Bayesrisk based on the reward for success, the penaltyfor a failure, and the probability of success (ap-proximated by the confidence measure).
Thatis, a reward is given according to the mannerof response (RwdRetor RwdQA) when the sys-tem presents an appropriate response.
On theother hand, a penalty is given based on extrane-ous time, which is approximated by the numberof sentences before obtaining the appropriate in-formation when the system presents an incorrectresponse.
For example, the penalty for a confir-mation is 2 {system?s confirmation + user?s ap-proval}, and that of a rejection is 1 {system?s re-jection}.
When the system presents incorrect in-formation, the penalty for a failure FailureRisk(FR) is calculated, which consists of the improperpresentation, the user?s correction, and the sys-tem?s request for a rephrasal.
Additional sentencesfor the completion of a task (AddSent) are alsogiven as extraneous time before accessing the ap-propriate document when the user rephrases thequery/question.
The value of AddSent is calcu-lated as an expected number of risks assuming theprobability of success by rephrasal was p1.The Bayes risk for the response candidatesis formulated as follows using the likelihoodof retrieval p(d), likelihood of answer extrac-tion pQA(d), and the reward pair (RwdRetandRwdQA; RwdRet< RwdQA) for successful pre-sentations as well as the FR for inappropriate pre-sentations.?
Presentation of document d (without confir-mation)Risk(Pres(d)) = ?RwdRet?
p(d)+(FR + AddSent) ?
(1 ?
p(d))?
Confirmation for presenting document dRisk(Conf(d)) = (?RwdRet+ 2) ?
p(d)+(2 + AddSent) ?
(1 ?
p(d))?
Answering user?s question using document dRisk(Ans(d)) = ?RwdQA?
pQA(d) ?
p(d)+(FR + AddSent) ?
(1 ?
pQA(d) ?
p(d))?
RejectionRisk(Rej) = 1 + AddSent1In the experiment, we use the success rate of the field trialpresented in (Misu and Kawahara, 2007).?
?User utterance: When did the shogun order tobuild the temple?
(Previous query:) Tell me about the SilverPavilion.Response candidates:* With context:?
p(Silver Pavilion history) = 0.4?
pQA(Silver Pavilion history) = 0.2 : In 1485- Risk(Pres(Silver Pavilion history)) = 6.4- Risk(Conf(Silver Pavilion history))= 4.8-Risk(Ans(Silver Pavilion history; In1485)) = 9.7. .
.
* Rejection- Risk(Rej) = 9.0?Response: Conf (Silver Pavilion history)?Do you want to know the history of the SilverPavilion???
?Figure 1: Example of calculating Bayes riskFigure 1 shows an example of calculating aBayes risk (where FR = 6, RwdRet= 5,RwdQA= 40).
In this example, an appropriatedocument is retrieved by incorporating the previ-ous user query.
However, since the answer to theuser?s question does not exist in the knowledgebase, the score of answer extraction is low.
There-fore, the system chooses a confirmation before pre-senting the entire document.4 Experimental Evaluation by CrossValidationWe have evaluated the proposed method using theuser utterances collected in the ?Dialogue Navi-gator for Kyoto City?
field trial.
We transcribedin-domain 1,416 utterances (1,084 queries and332 questions) and labeled their correct docu-ments/NEs by hand.The evaluation measures we used were the suc-cess rate and the average number of sentences forinformation access.
We regard a retrieval as suc-cessful if the system presents (or confirms) the ap-propriate document/NE for the query.
The num-ber of sentences for information access is used asan approximation of extraneous time before ac-cessing the document/NE.
That is, it is 1 {userutterance} if the system presents the requesteddocument without a confirmation.
If the systemmakes a confirmation before presentation, it is 361{user utterance + system?s confirmation + user?sapproval}, and that for presenting an incorrect doc-ument is 15 {user utterance + improper presenta-tion (3 = # presented sentences) + user?s correction+ system?s apology + request for rephrasing + ad-ditional sentences for task completion} (FR = 6 &AddSent = 8), which are determined based on thetypical recovery pattern observed in the field trial.We determined the value of the parameters by a2-fold cross validation by splitting the test set intotwo (set-1 & set-2), that is, set-1 was used as a de-velopment set to estimate FR and Rwd for eval-uating set-2, and vice versa.
The parameters weretuned to minimize the total number of sentencesfor information access in the development set.
Wecompared the proposed method with the followingconventional methods.
Note that method 1 is thebaseline method and method 2 was adopted in theoriginal ?Dialogue Navigator for Kyoto City?
andused in the field trial.Method 1 (baseline)?
Make a search query using the 1st hypothesisof ASR.?
Incorporate the contextual information relatedto the current topic.?
Make a confirmation when the ASR confi-dence of the pre-defined topic word is low.?
Answer the question when the user query isjudged a question.Method 2 (original system)?
Make a search query using all nouns in the 1st-3rd hypotheses of ASR.?
The other conditions are the same as in method1.The comparisons to these conventional methodsare shown in Table 1.
The improvement comparedwith that in baseline method 1 is 6.4% in the re-sponse success rate and 0.78 of a sentence in thenumber of sentences for information access.A breakdown of the selected response candi-dates by the proposed method is shown in Table2.
Many of the responses were generated using asingle hypothesis from the N-best list of ASR.
Theresult confirms that the correct hypothesis may notbe the first one, and the proposed method selectsthe appropriate one by considering the likelihoodof retrieval.
Most of the confirmations were gen-erated using the 1st hypothesis of ASR.
The An-swers to questions were often generated from thesearch queries with contextual information.
ThisTable 1: Comparison with conventional methodsSuccess rate # sentencesfor presentationMethod 1 (baseline) 59.2% 5.49Method 2 63.4% 4.98Proposed method 65.6% 4.71Table 2: Breakdown of selected candidatesw/o context with contextPres Conf Ans Pres Conf Ans1st hyp.
233 134 65 2 151 22nd hyp.
140 43 28 2 2 63rd hyp.
209 50 46 1 6 5merge all 75 11 3 18 0 91rejection 111result suggests that when users used anaphoric ex-pressions, the appropriate contextual informationwas incorporated into the question.5 ConclusionWe have proposed a dialogue framework to gener-ate an optimal response.
Specifically, the choicesin response generation are optimized as a mini-mization of the Bayes risk based on the reward fora correct information presentation and a penaltyfor redundant time.
Experimental evaluations us-ing real user utterances were used to demonstratethat the proposed method achieved a higher suc-cess rate for information access with a reducednumber of sentences.
Although we implementedonly a simple confirmation using the likelihood ofretrieval, the proposed method is expected to han-dle more complex dialogue management such asthe confirmation considering the impact for the re-trieval (Misu and Kawahara, 2006).ReferencesChang, E., F. Seide, H. Meng, Z. Chen, Y. Shi, and Y. Li.2002.
A System for Spoken Query Information Retrievalon Mobile Devices.
IEEE Trans.
on Speech and AudioProcessing, 10(8):531?541.Levin, E. and R. Pieraccini.
2006.
Value-based Optimal De-cision for Dialog Systems.
In Proc.
Spoken Laguage Tech-nology Workshop (SLT), pages 198?201.Misu, T. and T. Kawahara.
2006.
Dialogue strategy to clarifyuser?s queries for document retrieval system with speechinterface.
Speech Communication, 48(9):1137?1150.Misu, T. and T. Kawahara.
2007.
Speech-based interactiveinformation guidance system using question-answeringtechnique.
In Proc.
ICASSP.Raymond, C., Y. Esteve, F. Bechet, R. De Mori, andG.
Damnati.
2003.
Belief Confirmation in Spoken Di-alog Systems using Confidence Measures.
In Proc.
Au-tomatic Speech Recognition and Understanding Workshop(ASRU).62
