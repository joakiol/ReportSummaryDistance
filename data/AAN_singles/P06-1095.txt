Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 753?760,Sydney, July 2006. c?2006 Association for Computational LinguisticsMachine Learning of Temporal RelationsInderjeet Mani?
?, Marc Verhagen?, Ben Wellner?
?Chong Min Lee?
and James Pustejovsky?
?The MITRE Corporation202 Burlington Road, Bedford, MA 01730, USA?Department of Linguistics, Georgetown University37th and O Streets, Washington, DC 20036, USA?Department of Computer Science, Brandeis University415 South St., Waltham, MA 02254, USA{imani, wellner}@mitre.org, {marc, jamesp}@cs.brandeis.edu, cml54@georgetown.eduAbstractThis paper investigates a machine learn-ing approach for temporally ordering andanchoring events in natural languagetexts.
To address data sparseness, weused temporal reasoning as an over-sampling method to dramatically expandthe amount of training data, resulting inpredictive accuracy on link labeling ashigh as 93% using a Maximum Entropyclassifier on human annotated data.
Thismethod compared favorably against a se-ries of increasingly sophisticated base-lines involving expansion of rules de-rived from human intuitions.1 IntroductionThe growing interest in practical NLP applica-tions such as question-answering and text sum-marization places increasing demands on theprocessing of temporal information.
In multi-document summarization of news articles, it canbe useful to know the relative order of events soas to merge and present information from multi-ple news sources correctly.
In question-answering, one would like to be able to ask whenan event occurs, or what events occurred prior toa particular event.A wealth of prior research by (Passoneau1988), (Webber 1988), (Hwang and Schubert1992), (Kamp and Reyle 1993), (Lascarides andAsher 1993), (Hitzeman et al 1995), (Kehler2000) and others, has explored the differentknowledge sources used in inferring the temporalordering of events, including temporal adver-bials, tense, aspect, rhetorical relations, prag-matic conventions, and background knowledge.For example, the narrative convention of eventsbeing described in the order in which they occuris followed in (1), but overridden by means of adiscourse relation, Explanation in (2).
(1) Max stood up.
John greeted him.
(2) Max fell.
John pushed him.In addition to discourse relations, which oftenrequire inferences based on world knowledge,the ordering decisions humans carry out appearto involve a variety of knowledge sources, in-cluding tense and grammatical aspect (3a), lexi-cal aspect (3b), and temporal adverbials (3c):(3a) Max entered the room.
He had drunk a lotof wine.
(3b) Max entered the room.
Mary was seatedbehind the desk.
(3c) The company announced Tuesday thatthird-quarter sales had fallen.Clearly, substantial linguistic processing maybe required for a system to make these infer-ences, and world knowledge is hard to makeavailable to a domain-independent program.
Animportant strategy in this area is of course thedevelopment of annotated corpora than can fa-cilitate the machine learning of such orderinginferences.This paper 1  investigates a machine learningapproach for temporally ordering events in natu-ral language texts.
In Section 2, we describe theannotation scheme and annotated corpora, andthe challenges posed by them.
A basic learningapproach is described in Section 3.
To addressdata sparseness, we used temporal reasoning asan over-sampling method to dramatically expandthe amount of training data.As we will discuss in Section 5, there are nostandard algorithms for making these inferencesthat we can compare against.
We believestrongly that in such situations, it?s worthwhilefor computational linguists to devote consider-1Research at Georgetown and Brandeis on this prob-lem was funded in part by a grant from the ARDAAQUAINT Program, Phase II.753able effort to developing insightful baselines.Our work is, accordingly, evaluated in compari-son against four baselines: (i) the usual majorityclass statistical baseline, shown along with eachresult, (ii) a more sophisticated baseline that useshand-coded rules (Section 4.1), (iii) a hybridbaseline based on hand-coded rules expandedwith Google-induced rules (Section 4.2), and (iv)a machine learning version that learns from im-perfect annotation produced by (ii) (Section 4.3).2 Annotation Scheme and Corpora2.1 TimeMLTimeML (Pustejovsky et al 2005)(www.timeml.org) is an annotation scheme formarkup of events, times, and their temporal rela-tions in news articles.
The TimeML scheme flagstensed verbs, adjectives, and nominals withEVENT tags with various attributes, includingthe class of event, tense, grammatical aspect, po-larity (negative or positive), any modal operatorswhich govern the event being tagged, and cardi-nality of the event if it?s mentioned more thanonce.
Likewise, time expressions are flagged andtheir values normalized, based on TIMEX3, anextension of the ACE (2004) (tern.mitre.org)TIMEX2 annotation scheme.For temporal relations, TimeML defines aTLINK tag that links tagged events to otherevents and/or times.
For example, given (3a), aTLINK tag orders an instance of the event ofentering to an instance of the drinking with therelation type AFTER.
Likewise, given the sen-tence (3c), a TLINK tag will anchor the eventinstance of announcing to the time expressionTuesday (whose normalized value will be in-ferred from context), with the relationIS_INCLUDED.
These inferences are shown (inslightly abbreviated form) in the annotations in(4) and (5).
(4) Max <EVENT eventID=?e1?class=?occurrence?
tense=?past?
as-pect=?none?>entered</EVENT> the room.He <EVENT eventID=?e2?class=?occurrence?
tense=?past?
as-pect=?perfect?>had drunk</EVENT>alot of wine.<TLINK eventID=?e1?
relatedToEven-tID=?e2?
relType=?AFTER?/>(5) The company <EVENT even-tID=?e1?
class=?reporting?tense=?past?
as-pect=?none?>announced</EVENT><TIMEX3 tid=?t2?
type=?DATE?
tempo-ralFunction=?false?
value=?1998-01-08?>Tuesday </TIMEX3> that third-quarter sales <EVENT eventID=?e2?class=?occurrence?
tense=?past?
as-pect=?perfect?> had fallen</EVENT>.<TLINK eventID=?e1?
relatedToEven-tID=?e2?
relType=?AFTER?/><TLINK eventID=?e1?
relatedTo-TimeID=?t2?
relType=?IS_INCLUDED?/>The anchor relation is an Event-Time TLINK,and the order relation is an Event-Event TLINK.TimeML uses 14 temporal relations in theTLINK RelTypes, which reduce to a disjunctiveclassification of 6 temporal relations RelTypes ={SIMULTANEOUS, IBEFORE, BEFORE, BE-GINS, ENDS, INCLUDES}.
An event or time isSIMULTANEOUS with another event or time ifthey occupy the same time interval.
An event ortime INCLUDES another event or time if thelatter occupies a proper subinterval of the former.These 6 relations and their inverses map one-to-one to 12 of Allen?s 13 basic relations (Allen1984)2.
There has been a considerable amount ofactivity related to this scheme; we focus here onsome of the challenges posed by the TLINK an-notation, the part that is directly relevant to thetemporal ordering and anchoring problems.2.2 ChallengesThe annotation of TimeML information is on apar with other challenging semantic annotationschemes, like PropBank, RST annotation, etc.,where high inter-annotator reliability is crucialbut not always achievable without massive pre-processing to reduce the user?s workload.
In Ti-meML, inter-annotator agreement for time ex-pressions and events is 0.83 and 0.78 (average ofPrecision and Recall) respectively, but onTLINKs it is 0.55 (P&R average), due to thelarge number of event pairs that can be selectedfor comparison.
The time complexity of the hu-man TLINK annotation task is quadratic in thenumber of events and times in the document.Two corpora have been released based on Ti-meML: the TimeBank (Pustejovsky et al 2003)(we use version 1.2.a) with 186 documents and2Of the 14 TLINK relations, the 6 inverse relations are re-dundant.
In order to have a disjunctive classification, SI-MULTANEOUS and IDENTITY are collapsed, sinceIDENTITY is a subtype of SIMULTANEOUS.
(Specifi-cally, X and Y are identical if they are simultaneous andcoreferential.)
DURING and IS_INCLUDED are collapsedsince DURING is a subtype of IS_INCLUDED that anchorsevents to times that are durations.
IBEFORE (immediatelybefore) corresponds to Allen?s MEETS.
Allen?s OVER-LAPS relation is not represented in TimeML.
More detailscan be found at timeml.org.75464,077 words of text, and the Opinion Corpus(www.timeml.org), with 73 documents and38,709 words.
The TimeBank was developed inthe early stages of TimeML development, andwas partitioned across five annotators with dif-ferent levels of expertise.
The Opinion Corpuswas developed very recently, and was partitionedacross just two highly trained annotators, andcould therefore be expected to be less noisy.
Inour experiments, we merged the two datasets toproduce a single corpus, called OTC.Table 1 shows the distribution of EVENTs andTIMES, and TLINK RelTypes3 in the OTC.
Themajority class percentages are shown in paren-theses.
It can be seen that BEFORE and SI-MULTANEOUS together form a majority ofevent-ordering (Event-Event) links, whereasmost of the event anchoring (Event-Time) linksare INCLUDES.12750 Events, 2114 TimesRelation Event-Event Event-TimeIBEFORE 131 15BEGINS 160 112ENDS 208 159SIMULTANEOUS 1528 77INCLUDES 950 3001 (65.3%)BEFORE 3170 (51.6%) 1229TOTAL 6147 4593Table 1.
TLINK Class Distributions in OTCCorpusThe lack of TLINK coverage in human anno-tation could be helped by preprocessing, pro-vided it meets some threshold of accuracy.
Giventhe availability of a corpus like OTC, it is naturalto try a machine learning approach to see if it canbe used to provide that preprocessing.
However,the noise in the corpus and the sparseness oflinks present challenges to a learning approach.3 Machine Learning Approach3.1 Initial LearnerThere are several sub-problems related to in-ferring event anchoring and event ordering.
Oncea tagger has tagged the events and times, the firsttask (A) is to link events and/or times, and thesecond task (B) is to label the links.
Task A ishard to evaluate since, in the absence of massivepreprocessing, many links are ignored by thehuman in creating the annotated corpora.
In addi-3The number of TLINKs shown is based on the number ofTLINK vectors extracted from the OTC.tion, a program, as a baseline, can trivially linkall tagged events and times, getting 100% recallon Task A.
We focus here on Task B, the label-ing task.
In the case of humans, in fact, when aTLINK is posited by both annotators between thesame pairs of events or times, the inter-annotatoragreement on the labels is a .77 average of P&R.To ensure replicability of results, we assume per-fect (i.e., OTC-supplied) events, times, and links.Thus, we can consider TLINK inference as thefollowing classification problem: given an or-dered pair of elements X and Y, where X and Yare events or times which the human has relatedtemporally via a TLINK, the classifier has to as-sign a label in RelTypes.
Using RelTypes insteadof RelTypes ?
{NONE} also avoids the prob-lem of heavily skewing the data towards theNONE class.To construct feature vectors for machinelearning, we took each TLINK in the corpus andused the given TimeML features, with theTLINK class being the vector?s class feature.For replicability by other users of these corpora,and to be able to isolate the effect of components,we used ?perfect?
features; no feature engineer-ing was attempted.
The features were, for eachevent in an event-ordering pair, the event-class,aspect, modality, tense and negation (all nominalfeatures); event string, and signal (a preposi-tion/adverb, e.g., reported on Tuesday), whichare string features, and contextual features indi-cating whether the same tense and same aspectare true of both elements in the event pair.
Forevent-time links, we used the above event andsignal features along with TIMEX3 time features.For learning, we used an off-the-shelf Maxi-mum Entropy (ME) classifier (from Carafe,available at sourceforge.net/projects/carafe).
Asshown in the UNCLOSED (ME) column in Ta-ble 24, accuracy of the unclosed ME classifierdoes not go above 77%, though it?s always betterthan the majority class (in parentheses).
We alsotried a variety of other classifiers, including theSMO support-vector machine and the na?veBayes tools in WEKA (www.weka.net.nz).
SMOperformance (but not na?ve Bayes) was compa-rable with ME, with SMO trailing it in a fewcases (to save space, we report just ME perform-ance).
It?s possible that feature engineering couldimprove performance, but since this is ?perfect?data, the result is not encouraging.4All machine learning results, except for ME-C in Table 4,use 10-fold cross-validation.
?Accuracy?
in tables is Predic-tive Accuracy.755UNCLOSED (ME) CLOSED (ME-C)Event-Event Event-Time Event-Event Event-TimeAccuracy: 62.5 (51.6) 76.13 (65.3) 93.1 (75.2) 88.25 (62.3)Relation Prec Rec F Prec Rec F Prec Rec F Prec Rec FIBEFORE 50.00 27.27 35.39 0 0 0 77.78 60.86 68.29 0 0 0BEGINS 50.00 41.18 45.16 60.00 50.00 54.54 85.25 82.54 83.87 76.47 74.28 75.36ENDS 94.74 66.67 78.26 41.67 27.78 33.33 87.83 94.20 90.90 79.31 77.97 78.62SIMULTANEOUS 50.35 50.00 50.17 33.33 20.00 25.00 62.50 38.60 47.72 73.68 56.00 63.63INCLUDES 47.88 34.34 40.00 80.92 62.72 84.29 90.41 88.23 89.30 86.07 80.78 83.34BEFORE 68.85 79.24 73.68 70.47 62.72 66.37 94.95 97.26 96.09 90.16 93.56 91.83Table 2.
Machine learning results using unclosed and closed data3.2 Expanding Training Data using Tem-poral ReasoningTo expand our training set, we use a temporalclosure component SputLink (Verhagen 2004),that takes known temporal relations in a text andderives new implied relations from them, in ef-fect making explicit what was implicit.
SputLinkwas inspired by (Setzer and Gaizauskas 2000)and is based on Allen?s interval algebra, takinginto account the limitations on that algebra thatwere pointed out by (Vilain et al 1990).
It is ba-sically a constraint propagation algorithm thatuses a transitivity table to model the composi-tional behavior of all pairs of relations in adocument.
SputLink?s transitivity table is repre-sented by 745 axioms.
An example axiom:If relation(A, B) = BEFORE &&relation(B, C) = INCLUDESthen infer relation(A, C) = BEFOREOnce the TLINKs in each document in thecorpus are closed using SputLink, the same vec-tor generation procedure and feature representa-tion described in Section 3.1 are used.
The effectof closing the TLINKs on the corpus has a dra-matic impact on learning.
Table 2, in theCLOSED (ME-C) column shows that accura-cies for this method (called ME-C, for MaximumEntropy learning with closure) are now in thehigh 80?s and low 90?s, and still outperform theclosed majority class (shown in parentheses).What is the reason for the improvement?5 Onereason is the dramatic increase in the amount oftraining data.
The more connected the initial un-5Interestingly, performance does not improve for SIMUL-TANEOUS.
The reason for this might be due to the rela-tively modest increase in SIMULTANEOUS relations fromapplying closure (roughly factor of 2).closed graph for a document is in TLINKs, thegreater the impact in terms of closure.
When theOTC is closed, the number of TLINKs goes upby more than 11 times, from 6147 Event-Eventand 4593 Event-Time TLINKs to 91,157 Event-Event and 29,963 Event-Time TLINKs.
Thenumber of BEFORE links goes up from 3170(51.6%) Event-Event and 1229 Event-TimeTLINKs (26.75%) to 68585 (75.2%) Event-Event and 18665 (62.3%) Event-Time TLINKs,making BEFORE the majority class in the closeddata for both Event-Event and Event-TimeTLINKs.
There are only an average of 0.84TLINKs per event before closure, but after clo-sure it shoots up to 9.49 TLINKs per event.
(Note that as a result, the majority class percent-ages for the closed data have changed from theunclosed data.
)Being able to bootstrap more training data isof course very useful.
However, we need to digdeeper to investigate how the increase in dataaffected the machine learning.
The improvementprovided by temporal closure can be explainedby three factors:  (1) closure effectively creates anew classification problem with many more in-stances, providing more data to train on; (2) theclass distribution is further skewed which resultsin a higher majority class baseline (3) closureproduces additional data in such a way as to in-crease the frequencies and statistical power ofexisting features in the unclosed data, as opposedto adding new features.
For example, with un-closed data, given A BEFORE B and B BE-FORE C, closure generates A BEFORE C whichprovides more significance for the features re-lated to A and C appearing as first and secondarguments, respectively, in a BEFORE relation.In order to help determine the effects of theabove factors, we carried out two experiments inwhich we sampled 6145 vectors from the closed756data ?
i.e.
approximately the number of Event-Event vectors in the unclosed data.
This effec-tively removed the contribution of factor (1)above.
The first experiment (Closed Class Dis-tribution) simply sampled 6145 instances uni-formly from the closed instances, while the sec-ond experiment (Unclosed Class Distribution)sampled instances according to the same distri-bution as the unclosed data.
Table 3 shows theseresults.
The greater class distribution skew in theclosed data clearly contributes to improved accu-racy.
However, when using the same class distri-bution as the unclosed data (removing factor (2)from above), the accuracy, 76%, is higher thanusing the full unclosed data.
This indicates thatclosure does indeed help according to factor (3).4 Comparison against Baselines4.1 Hand-Coded RulesHumans have strong intuitions about rules fortemporal ordering, as we indicated in discussingsentences (1) to (3).
Such intuitions led to thedevelopment of pattern matching rules incorpo-rated in a TLINK tagger called GTag.
GTagtakes a document with TimeML tags, along withsyntactic information from part-of-speech tag-ging and chunking from Carafe, and then uses187 syntactic and lexical rules to infer and labelTLINKs between tagged events and other taggedevents or times.
The tagger takes pairs ofTLINKable items (event and/or time) andsearches for the single most-confident rule toapply to it, if any, to produce a labeled TLINKbetween those items.
Each (if-then) rule has aleft-hand side which consists of a conjunction oftests based on TimeML-related feature combina-tions (TimeML features along with part-of-speech and chunk-related features), and a right-hand side which is an assignment to one of theTimeML TLINK classes.The rule patterns are grouped into several dif-ferent classes: (i) the event is anchored with orwithout a signal to a time expression within thesame clause, e.g., (3c), (ii) the event is anchoredwithout a signal to the document date (as is oftenthe case for reporting verbs in news), (iii) anevent is linked to another event in the same sen-tence, e.g., (3c), and (iv) the event in a mainclause of one sentence is anchored with a signalor tense/aspect cue to an event in the main clauseof the previous sentence, e.g., (1-2), (3a-b).The performance of this baseline is shown inTable 4 (line GTag).
The top most accurate rule(87% accuracy) was GTag Rule 6.6, which linksa past-tense event verb joined by a conjunction toanother past-tense event verb as being BEFOREthe latter (e.g., they traveled and slept thenight ..):If sameSentence=YES &&sentenceType=ANY &&conjBetweenEvents=YES &&arg1.class=EVENT &&arg2.class=EVENT &&arg1.tense=PAST &&arg2.tense=PAST &&arg1.aspect=NONE &&arg2.aspect=NONE &&arg1.pos=VB &&arg2.pos=VB &&arg1.firstVbEvent=ANY &&arg2.firstVbEvent=ANYthen infer relation=BEFOREThe vast majority of the intuition-bred ruleshave very low accuracy compared to ME-C, withintuitions failing for various feature combina-tions and relations (for relations, for example,GTag lacks rules for IBEFORE, STARTS, andENDS).
The bottom-line here is that even whenheuristic preferences are intuited, those prefer-ences need to be guided by empirical data,whereas hand-coded rules are relatively ignorantof the distributions that are found in data.4.2 Adding Google-Induced Lexical RulesOne might argue that the above baseline is tooweak, since it doesn?t allow for a rich set of lexi-cal relations.
For example, pushing can result infalling, killing always results in death, and soforth.
These kinds of defeasible rules have beeninvestigated in the semantics literature, includingthe work of Lascarides and Asher cited in Sec-tion 1.However, rather than hand-creating lexicalrules and running into the same limitations aswith GTag?s rules, we used an empirically-derived resource called VerbOcean (Chklovskiand Pantel 2004), available athttp://semantics.isi.edu/ocean.
This resource con-sists of lexical relations mined from Googlesearches.
The mining uses a set of lexical andsyntactic patterns to test for pairs of verbstrongly associated on the Web in an asymmetric?happens-before?
relation.
For example, the sys-tem discovers that marriage happens-before di-vorce, and that tie happens-before untie.We automatically extracted all the ?happens-before?
relations from the VerbOcean resource atthe above web site, and then automatically con-verted those relations to GTag format, producing4,199 rules.
Here is one such converted rule:757If arg1.class=EVENT &&arg2.class=EVENT &&arg1.word=learn &&arg2.word=forget &&then infer relation=BEFOREAdding these lexical rules to GTag (with mor-phological normalization being added for rulematching on word features) amounts to a consid-erable augmentation of the rule-set, by a factor of22.
GTag with this augmented rule-set might bea useful baseline to consider, since one wouldexpect the gigantic size of the Google ?corpus?
toyield fairly robust, broad-coverage rules.What if both a core GTag rule and a VerbO-cean-derived rule could both apply?
We assumethe one with the higher confidence is chosen.However, we don?t have enough data to reliablyestimate rule confidences for the original GTagrules; so, for the purposes of VerbOcean ruleintegration, we assigned either the original Ver-bOcean rules as having greater confidence thanthe original GTag rules in case of a conflict (i.e.,a preference for the more specific rule), or vice-versa.The results are shown in Table 4 (linesGTag+VerbOcean).
The combined rule set, un-der both voting schemes, had no statistically sig-nificant difference in accuracy from the originalGTag rule set.
So, ME-C beat this baseline aswell.The reason VerbOcean didn?t help is againone of data sparseness, due to most verbs occur-ring rarely in the OTC.
There were only 19 occa-sions when a happens-before pair from VerbO-cean correctly matched a human BEFORETLINK, of which 6 involved the same rule beingright twice (including learn happens-before for-get, a rule which students are especially familiarwith!
), with the rest being right just once.
Therewere only 5 occasions when a VerbOcean ruleincorrectly matched a human BEFORE TLINK,involving just three rules.Closed Class Distribution UnClosed Class DistributionRelation Prec Rec F Accuracy Prec Rec F AccuracyIBEFORE 100.0 100.0 100.0 83.33 58.82 68.96BEGINS 0 0 0 72.72 50.0 59.25ENDS 66.66 57.14 61.53 62.50 50.0 55.55SIMULTANEOUS 14.28 6.66 9.09 60.54 66.41 63.34INCLUDES 73.91 77.98 75.89 75.75 77.31 76.53BEFORE 90.68 92.60 91.6387.20(72.03)84.09 84.61 84.3576.0(40.95)Table 3.
Machine Learning from subsamples of the closed dataAccuracy BaselineEvent-Event Event-TimeGTag 63.43 72.46GTag+VerbOcean - GTag overriding VerbOcean 64.80 74.02GTag+VerbOcean - VerbOcean overriding GTag 64.22 73.37GTag+closure+ME-C 53.84 (57.00) 67.37 (67.59)Table 4.
Accuracy of ?Intuition?
Derived Baselines4.3 Learning from Hand-Coded RulesBaselineThe previous baseline was a hybrid confi-dence-based combination of corpus-inducedlexical relations with hand-created rules for tem-poral ordering.
One could consider another obvi-ous hybrid, namely learning from annotationscreated by GTag-annotated corpora.
Since theintuitive baseline fares badly, this may not bethat attractive.
However, the dramatic impact ofclosure could help offset the limited coverageprovided by human intuitions.Table 4 (line GTag+closure+ME-C) shows theresults of closing the TLINKs produced byGTag?s annotation and then training ME fromthe resulting data.
The results here are evaluatedagainst a held-out test set.
We can see that evenafter closure, the baseline of learning from un-closed human annotations is much poorer thanME-C, and is in fact substantially worse than themajority class on event ordering.This means that for preprocessing new datasets to produce noisily annotated data for thisclassification task, it is far better to use machine-learning from closed human annotations rather758than machine-learning from closed annotationsproduced by an intuitive baseline.5 Related WorkOur approach of classifying pairs independ-ently during learning does not take into accountdependencies between pairs.
For example, aclassifier may label <X, Y> as BEFORE.
Giventhe pair <X, Z>,  such a classifier has no idea if<Y, Z> has been classified as BEFORE, inwhich case, through closure, <X, Z> should beclassified as BEFORE.
This can result in theclassifier producing an inconsistently annotatedtext.
The machine learning approach of (Cohenet al 1999) addresses this, but their approach islimited to total orderings involving BEFORE,whereas TLINKs introduce partial orderings in-volving BEFORE and five other relations.
Futureresearch will investigate methods for tighter in-tegration of temporal reasoning and statisticalclassification.The only closely comparable machine-learning approach to the problem of TLINK ex-traction was that of (Boguraev and Ando 2005),who trained a classifier on Timebank 1.1 forevent anchoring for events and times within thesame sentence, obtaining an F-measure (for tasksA and B together) of 53.1.
Other work in ma-chine-learning and hand-coded approaches,while interesting, is harder to compare in termsof accuracy since they do not use common taskdefinitions, annotation standards, and evaluationmeasures.
(Li et al 2004) obtained 78-88% accu-racy on ordering within-sentence temporal rela-tions in Chinese texts.
(Mani et al 2003) ob-tained 80.2 F-measure training a decision tree on2069 clauses in anchoring events to referencetimes that were inferred for each clause.
(Ber-glund et al 2006) use a document-level evalua-tion approach pioneered by (Setzer and Gai-zauskas 2000), which uses a distinct evaluationmetric.
Finally, (Lapata and Lascarides 2004) usefound data to successfully learn which (possiblyambiguous) temporal markers connect a mainand subordinate clause, without inferring under-lying temporal relations.In terms of hand-coded approaches, (Mani andWilson 2000) used a baseline method of blindlypropagating TempEx time values to events basedon proximity, obtaining 59.4% on a small sampleof 8,505 words of text.
(Filatova and Hovy 2001)obtained 82% accuracy on ?timestamping?clauses for a single type of event/topic on a dataset of 172 clauses.
(Schilder and Habel 2001)report 84% accuracy inferring temporal relationsin German data, and (Li et al 2001) report 93%accuracy on extracting temporal relations in Chi-nese.
Because these accuracies are on differentdata sets and metrics, they cannot be compareddirectly with our methods.Recently, researchers have developed othertools for automatically tagging aspects of Ti-meML, including EVENT (Sauri et al 2005) at0.80 F-measure and TIMEX36 tags at 0.82-0.85F-measure.
In addition, the TERN competition(tern.mitre.org) has shown very high (close to .95F-measures) for TIMEX2 tagging, which is fairlysimilar to TIMEX3.
These results suggest thetime is ripe for exploiting ?imperfect?
features inour machine learning approach.6 ConclusionOur research has uncovered one new finding:semantic reasoning (in this case, logical axiomsfor temporal closure), can be extremely valuablein addressing data sparseness.
Without it, per-formance on this task of learning temporal rela-tions is poor; with it, it is excellent.
We showedthat temporal reasoning can be used as an over-sampling method to dramatically expand theamount of training data for TLINK labeling, re-sulting in labeling predictive accuracy as high as93% using an off-the-shelf Maximum Entropyclassifier.
Future research will investigate thiseffect further, as well as examine factors thatenhance or mitigate this effect in different cor-pora.The paper showed that ME-C performed sig-nificantly better than a series of increasingly so-phisticated baselines involving expansion ofrules derived from human intuitions.
Our resultsin these comparisons confirm the lessons learnedfrom the corpus-based revolution, namely thatrules based on intuition alone are prone to in-completeness and are hard to tune without accessto the distributions found in empirical data.Clearly, lexical rules have a role to play in se-mantic and pragmatic reasoning from language,as in the discussion of example (2) in Section 1.Such rules, when mined by robust, large corpus-based methods, as in the Google-derived VerbO-cean, are clearly relevant, but too specific to ap-ply more than a few times in the OTC corpus.It may be possible to acquire confidenceweights for at least some of the intuitive rules inGTag from Google searches, so that we have a6http://complingone.georgetown.edu/~linguist/GU_TIME_DOWNLOAD.HTML759level field for integrating confidence weightsfrom the fairly general GTag rules and the fairlyspecific VerbOcean-like lexical rules.
Further,the GTag and VerbOcean rules could be incorpo-rated as features for machine learning, along withfeatures from automatic preprocessing.We have taken pains to use freely download-able resources like Carafe, VerbOcean, andWEKA to help others easily replicate andquickly ramp up a system.
To further facilitatefurther research, our tools as well as labeled vec-tors (unclosed as well as closed) are available forothers to experiment with.ReferencesJames Allen.
1984.
Towards a General Theory of Ac-tion and Time.
Artificial Intelligence, 23, 2, 123-154.Anders Berglund, Richard Johansson and PierreNugues.
2006.
A Machine Learning Approach toExtract Temporal Information from Texts in Swed-ish and Generate Animated 3D Scenes.
Proceed-ings of EACL-2006.Branimir Boguraev and Rie Kubota Ando.
2005.
Ti-meML-Compliant Text Analysis for TemporalReasoning.
Proceedings of IJCAI-05, 997-1003.Timothy Chklovski and Patrick Pantel.2004.VerbOcean: Mining the Web for Fine-Grained Semantic Verb Relations.
Proceedings ofEMNLP-04.
http://semantics.isi.edu/oceanW.
Cohen, R. Schapire, and Y.
Singer.
1999.
Learn-ing to order things.
Journal of Artificial Intelli-gence Research, 10:243?270, 1999.Janet Hitzeman, Marc Moens and Clare Grover.
1995.Algorithms for Analyzing the Temporal Structureof Discourse.
Proceedings of  EACL?95, Dublin,Ireland, 253-260.C.H.
Hwang and L. K. Schubert.
1992.
Tense Trees asthe fine structure of discourse.
Proceedings ofACL?1992, 232-240.Hans Kamp and Uwe Ryle.
1993.
From Discourse toLogic (Part 2).
Dordrecht: Kluwer.Andrew Kehler.
2000.
Resolving Temporal Relationsusing Tense Meaning and Discourse Interpretation,in M. Faller, S. Kaufmann, and M. Pauly, (eds.
),Formalizing the Dynamics of Information, CSLIPublications, Stanford.Mirella Lapata and Alex Lascarides.
2004.
InferringSentence-internal Temporal Relations.
In Proceed-ings of the North American Chapter of the Assoca-tion of Computational Linguistics, 153-160.Alex Lascarides and Nicholas Asher.
1993.
TemporalRelations, Discourse Structure, and CommonsenseEntailment.
Linguistics and Philosophy 16, 437-494.Wenjie Li, Kam-Fai Wong, Guihong Cao and ChunfaYuan.
2004.
Applying Machine Learning to Chi-nese Temporal Relation Resolution.
Proceedings ofACL?2004, 582-588.Inderjeet Mani, Barry Schiffman, and Jianping Zhang.2003.
Inferring Temporal Ordering of Events inNews.
Short Paper.
Proceedings of HLT-NAACL'03, 55-57.Inderjeet Mani and George Wilson.
2000.
RobustTemporal Processing of News.
Proceedings ofACL?2000.Rebecca J. Passonneau.
A Computational Model ofthe Semantics of Tense and Aspect.
ComputationalLinguistics, 14, 2, 1988, 44-60.James Pustejovsky, Patrick Hanks, Roser Sauri, An-drew See, David Day, Lisa Ferro, Robert Gai-zauskas, Marcia Lazo, Andrea Setzer, and BethSundheim.
2003.
The TimeBank Corpus.
CorpusLinguistics, 647-656.James Pustejovsky, Bob Ingria, Roser Sauri, JoseCastano, Jessica Littman, Rob Gaizauskas, AndreaSetzer, G. Katz,  and I. Mani.
2005.
The Specifica-tion Language TimeML.
In I. Mani, J. Pustejovsky,and R. Gaizauskas, (eds.
), The Language of Time:A Reader.
Oxford University Press.Roser Saur?, Robert Knippen, Marc Verhagen andJames Pustejovsky.
2005.
Evita: A Robust EventRecognizer for QA Systems.
Short Paper.
Proceed-ings of HLT/EMNLP 2005: 700-707.Frank Schilder and Christof Habel.
2005.
From tem-poral expressions to temporal information: seman-tic tagging of news messages.
In I. Mani, J. Puste-jovsky, and R. Gaizauskas, (eds.
), The Language ofTime: A Reader.
Oxford University Press.Andrea Setzer and Robert Gaizauskas.
2000.
Annotat-ing Events and Temporal Information in NewswireTexts.
Proceedings of LREC-2000, 1287-1294.Marc Verhagen.
2004.
Times Between The Lines.Ph.D.
Dissertation, Department of Computer Sci-ence, Brandeis University.Marc Vilain, Henry Kautz, and Peter Van Beek.
1989.Constraint propagation algorithms for temporalreasoning: A revised report.
In D. S. Weld and J.de Kleer (eds.
), Readings in Qualitative Reasoningabout Physical Systems, Morgan-Kaufman, 373-381.Bonnie Webber.
1988.
Tense as Discourse Anaphor.Computational Linguistics, 14, 2, 1988, 61-73.760
