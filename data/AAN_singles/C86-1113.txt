Distributed Memory: A Basis for Chart ParsingJon M. SlackHuman Cognition Research LaboratoryOpen UniversityMilton Keynes, MK7 6AAENGLANDAbstractThe properties of distributed representations andmemory systems are explored as a potential basis fornon-deterministic parsing mechanisms.
The structure ofa distributed chart parsing representation is outlined.Such a representation encodes both immediate-dominance and terminal projection information on asingle composite memory vector.
A parsing architectureis described which uses a permanent store ofcontext-free rule patterns encoded as split compositevectors, and two interacting working memory units.These latter two units encode vectors which correspondto the active and inactive edges of an active chartparsing scheme.
This type of virtual parsingmechanism is compatible with both a macro-levelimplementation based on standard sequentialprocessing and a micro-level implementation using amassively parallel architecture.A lot of recent research has focused on theproblem of building psychologically feasible models ofnatural anguage comprehension.
Much of this workhas been based on the connectionist paradigm ofFeldman and Ballard (1982) and other massivelyparallel architectures (Fahlman, Hinton and Sejnowski,1983).
For example, Waltz and Pollack (1984) havedevised a model of word-sense and syntacticdisambiguation, and Cottrell (1985) has proposed aneural network style model of parsing.
Originally, suchsystems were limited in thier capability to handle tasksinvolving rule-based processing.
For example, the Waltzand Pollack model uses the output of a conventionalchart parser to derive a structure for the syntactic parseof a sentence.
However, more recent connectionistparsing systems (e.g., Selman and Hirst, 1985) aremore suited to handling sequential rule-based parsing.In this type of model grammar rules are represented bymeans of a small set of connectionist primitives.
Thesyntactic ategories of the grammar are represented in aIocalist manner, that is, by a computational unit in anetwork.
The interconnections of the units within thenetwork are determined by the grammar rules.
SuchIocalised representations are obviously useful in theconstruction of connectionist models of rule-basedprocessing, but they suffer from an inherent capacitylimitation and are usually non-adaptive.The research to be discussed here differs fromprevious work in that it explores the properties ofdistributed representations as a basis for constructingparallel parsing architectures.
Rather than beingrepresented by Iocalised networks of processing units,the grammar rules are encoded as patterns which havetheir effect through simple, yet well-specified forms ofinteraction.
The aim of the research is to devise a virtualmachine for parsing context-free languages based onthe mutual interaction of relatively simple memorycomponents.1.
DISTRIBUTED MEMORY SYSTEMSConstraints on space make it impossible todescribe distributed memory systems in detail, and thissection merely outlines their salient properties.
For amore detailed description of their properties andoperation see Slack (1984a, b).In distributed memory systems items of informationare encoded as k-element vectors, where each elementranges over some specified interval, such as \[-1 ,+1\].Such vectors might correspond to the pattern of activityover a set of neurons, or the pattern of activation levelsof a collection of processing units within a connectionistmodel.
Irrespective of the manifest form of vectors, theinformation they encode can be manipulated by meansof three basic operations; association (denoted by *),concatenation (+), and retrieval (#).
The associationoperator creates a composite memory vector whichencodes the association of two items of information(denoting memory vectors by angular brackets, theassociation of items <A> and <B> is denoted <A>*<B>).The concatenation operator creates a compositememory vector encoding individual items or vectors(<A>+<B>).
Thus, a single composite vector canencode large numbers of items, and associationsbetween items.
The individual elements encoded on acomposite vector are accessible through the retrievaloperator.
When a retrievalkey vector is input to acomposite vector the retrieval operator produces a newcomposite vector which encodes those items ofinformation which were associated with the key vectoron the original composite trace.
An important propertyof the retrieval and assocaition operators is that they areboth distributive over concatenation.
However, only theassociation operator is associative.476These basic properties enable distributive memorysystems to encode large amounts of knowledge on asingle composite memory vector.
The capacitylimitations of such systems are determined by the noiselevels implicit in the output of the retrieval operation.The generated noise is a function of the number oftraces encoded on a vector and the size of the vector.Two classes of distributive memory can bedistinguished, permanent and working.
The formerprovide permanent storage of information, while thelatter have no permanent contents but are used to buildtemporary representations of inputs.
An importantfeature of working distributed memories is that the tracesencoded on them decay.
Associated with each vectorencoded on ~t working memory is a decay, or strength,index.
The decay function is event dependent, ratherthan time dependent, in that the indices are adjusted onthe oceurence of a new input.
This property provides auseful form of event indexing which is crucial to the chartparsing scheme.2.
DISTRIBUTED CHART PARSINGREPRESENTATIONThe aim of chart parsing using distributedrepresentations i to derive a composite memory vectorwhich encodes the type of structural information shownin figure 1.
The figure shows the constituent structure(C-structure) built for the sentence The man hit the boy.4-SNP ,111 iN VVPi iNPDET DET NI 2 3 4 5 6The man hit the boyf igur~ IThis structure incorporates features of different notationsused to describe chart parsing schemes (Earley, 1970;Kay, 1980).
The structure represents a passive latticeconsisting of one cell, or an equivalent edge, perconstituent.
It embodies structural information relating tothe immediate-dominance of constituents, as well asinformation about the range of terminals panned byeach constituent.
This structure can be collapsed underthe operations of association and concatenation toderive a composite memory vector representation whichpreserves both types of structural information.
Eachconstituent category is encoded as a random memoryvector; <S>, <NP>, <DET> and so on.
The vertices ofthe chart (1-6 in figure 1 ) are mapped onto the set ofdecay indices registered by the working memorysystems at each input point.
Moving from left to rightthrough the table shown in figure 1 the vectors arecombined by tile concatenation operator.
Thedominance structure exhibited within the table ispreserved by the association operator going frombottom to top.
These procedures produce the followingcomposite vector which encodes the informationrepresented in figure 1 :<S>*<I-6>*{<NP>*<I-3>*{<IDET>*<I-2>+<N><2-3>}+ <VP>*<3-6>*{<V>'=<3-4> +<N P>*<4-6>*{<DET>*<4-5>+<N>*<5-6>}}Each component of the pattern consists of a vectorencoding a constituent label associated with the inputindices defining the left and right edges of theconstituent.
This form of representation is similar to theidea of assertion sets which have been shown to haveuseful properties as parsing representations (Bartonand Berwick, 1985).
One of the major advantages of therepresentation is that the two types of structuralinformation are jointly accessible using the appropriateretrieval vector, that is category label vector.
Forexample, if the composite vector is accessed using theretrieval vector <VP>, then the retrieved pattern encodesboth the categories dominated by VP and the range ofinput terminals covered by VP.3.
PARSING ARCHITECTUREA possible parsing architecture for realising theabove scheme is shown in figure 2.
The parsingmechanism consists of three distributed memory units,one permanent unit which stores context-free rulepatterns, and two working memory units which encodetemporary constituent structures.
In figure 2, the storedrules pattern memory adds retrieved lists of ruleconstituents to the active patterns working memory.The double arrows on the lines connecting to theinactive patterns memory depict the two-wayinteractions between the inactive patterns and both thestored rule patterns and active patterns.The input to the parsing mechanism comprises thesyntactic ategories of each constituent of the inputstring.
This input is received by the inactive patternsworking memory and each new input triggers theindexing of patterns in accordance with the decayfunction.
Thus, while a category label vector is held onthe inactive patterns unit its decay index is continuallyup-dated.477ActivepatternsInactivepatternsINPUTF/gure 2Representing the rules of the CFG by the followingnotation:label: Constituent category ---> List of constituentsorL i:Cat i ---> Cij'sEach element of a rule is encoded as a memory vector,and the rules are stored in the permanent memory unitin terms of the following patterns:<Cil >*(<Li>*<Cati>//<Ci2>*.
...... *<Cik>*<Li> )The vector <Cil> encodes the first constituent on theRHS of the CF rule labeled <Li>.
When this constituentis input to the rule patterns store the split pattern withwhich it is associated is retrieved as output.
Theretrieved pattern is split in that the two halves of thepattern are output over different lines.
The first half ofthe pattern, <Li>*<Cati>, is output to the inactivepatterns unit, and the second half, <Ci2>*....*<Cik>*<Li>,is output to the active patterns memory unit.
Thisretrieval process is equivalent o building an active edgein active chart parsing (Kay, 1980; Thompson, 1983).
Amajor difference, however, is that the vector encodingthe active edge is split over the two working memoryunits.The active patterns unit now encodes the list ofremaining constituents which must occur for the RHS ofrule <Li> to be satisfied.
Meanwhile, the inactivepatterns unit encodes the category of the hypothesised,or active, edge.
If the list of necessary constituents ismatched by later inptus then the label of the satisfiedrule is retrieved on the active patterns unit and output tothe inactive patterns store.
This new input retrieves theconstituent category associated with the rule label onthe inactive patterns unit.
This retrieval process isequivalent o building an inactive edge in standardactive chart parsing.
Each time a new inactive edgepattern is created it is output to both the other units todetermine whether it satisfies any of the constituents of a478stored rule, or any remaining active patterns.
Thoseactive patterns which do not have their constituentelements matched rapidly decay, and fade out of thecomposite traces held on the two working memory units.When a rule is satisfied and an inactive pattern, oredge, is built the index range spanned by the rule isretrieved at the same time as the rule's category vector.Thus, an inactive pattern is encoded as <Cati>*<m-q>,where <m-q> encodes the range of input indicesspanned by the category <Cati>.Within this scheme the alternative CF rules for aparticular constituent category have to be encoded asseparate rule patterns.
For example, the rules forbuilding NPs would include the following:1: NP ---> DETNP 22: NP---> NP 23: NP---> NP PP4: NP 2 - -> N5: NP2---> ADJ NP 2To simulate a top-down parsing scheme the activeelements encoded on the inactive patterns unit canfunction as retrieval vectors to the stored rules unit.
Thisfeedback loop enables a retrieved category to retrievefurther rule patterns; those for which it is the firstconstituent.
In this way, all the rule patterns which areapplicable at a particular point in the parse are retrievedand held as active edges split across the two workingmemory units.
On each cylce of feedback the newlyretrieved rule categories are associated with the activeelements which retrieved them to form patterns such as<La>*<S>*(<Lb>*<NP> ).On successfully parsing a sentence the inactivepatterns unit holds a collection of inactive edge patternswhich form a composite vector of the type described insection 2.
All active patterns, and those inactive edgeswhich are incompatible with the final parse rapidlydecay leaving only the derived constituent structure inworking memory.
If an input sentence is syntacticallyambiguous then all possible C-structures are encodedon the final composite vector.
That is, as it stands theparsing scheme does not embody a theory of syntacticclosure, or preference.4.
A PARSING EXAMPLEThe parsing architecture described above hasbeen implemented on standard sequential processingmachinery.
To achieve this it was necessary to decideon computational functions for the three memoryoperators, and to build an agenda mechanism toschedule the ordering of the memory unit input/outputprocesses.
The association and retrieval operatorswere accurately simulated using convolution andcorrelation functions, respectively.
The concatenationoperator was simulated through a combination of vectoraddition and a normalisation function.
All syntacticcategories, rule labels, and input indices were encodedas randomly assigned 1000-element vectors.The agenda mechanism imposes the followingsequences on input-output operations:\[A\] The input lines to the inactive patterns unit areprocessed in the order - constituent input, input fromstored rule patterns unit, and finally, input from activepatterns unit.\[B\] The retrieval and output operations for theunits are cycled through in the order - activation ofstored rule patterns, including top-down feedback;matching of active patterns, and building of inactiveedge patterns.\[C\] The final operation is to increment he inputindices, before accepting the next constituent input.To illustrate the operation of the parsingmechanism it is useful to consider an example.
As asimple example, assume that the stored rule patternsunit holds the following context free rules:1: NP---> DETNP 2 6: S---> NP VP2: NP---> NP 2 7: VP---> V3: NP ---> NP PP 8: VP---> V NP4: NP 2-*-> N 9: VP---> VP PP5: NP2---> ADJ NP 2 10: PP---> Prep NPIn parsing the sentence The old man the boats thelexical ambiguity associated with the words old and manproduces the input string<Det> <Adj+N> <N+V> <Det> <N>The first input vector, <Det>, retrieves rule 1,setting up <NP2> on the active patterns unit and<I>*<NP>*(<Det>) on the inactive patterns unit (theinput indices have been omitted to simplify the notation).Through feedback, rules 3 and 6 are also retrievedcreating the pattern <NP2>*<I>+<PP>*<3>+<VP>*<6> onthe active patterns unit, and the pattern<6>*<s>*((<1 >*<NP>*(< Det>)+(<6>*<N P>*(<I >*<NP>*(<Det>)))on the inactive patterns unit.On input of the second word which covers thecategories Adj and N, the composite vector <Adj+N>retrieves rules 4 and 5, and through feedback rules 2, 3and 6.
The ,:N> pattern component of the input vectorsatisfies the <I>*<NP2> pattern held on the activepatterns unit which leads to the creation of the firstinactive edge pattern, <6>*<S>*(<NP>*(<Det>+<N>)).The third input is also ambiguous and triggers alarge number of rules, including rules 7 and 8.
Again,the <N> pattern in the input vector triggers theconstruction of an NP inactive pattern -<6>*<S>*(<N P>*(<Det>+<Adj>+<N>)).In addition, the retrieval of rule 7 produces the inactiveedge pattern <S>*(<NP>*(<Det>+<N>)+<VP>*<V>)),corresponding to a premature interpretation of the inputas The old(N) man(V).
However, this pattern rapidlydecays as new input is received.The final two inputs set up another <NP> inactiveedge which when output to the active patterns unitretrieves the label vector associated with theconstituents of rule 8.
This produces a <VP> inactiveedge which through feedback to the active patterns unitretrieves the label vector <6>, as it completes the <S>rule.
In turn, an inactive edge pattern for <S> is created.To complete the parse, the period symbol, orcorresponding vector, can be used to trigger anoperation which refreshes only those inactive edgeswhich contain the final <S> pattern.
All other patternsdecay rapidly leaving only the constituent structurepattern on the inactive patterns unit.This parsing example is not concordant with mostpeople's reading of the sentence The old man the boats.In the first instance, the sentence tends to be parsed astwo consecutive NPs, (The old man)(the boats), Suchphenomena would tend to imply that the human parseris more deterministic then the present model suggests.However, the model is net a complete comprehensionmechanism and other factors influence the finaloutcome of the parse.
In particular, many of thephenomena associated with closure and ambiguity canbe explained in terms of lexical items having preferredinterpretations (Kaplan and Bresnan, 1982).
In thepresent example, the preferred categories for old andman are Adj and N, respectively.
Such an idea is easilyaccommodated within the present scheme in terms ofthe concept of pattern strength (see Slack, 1984b, fordetails).5.
MACRO- AND MICRO-LEVELDESCRIPTIONSThe parsing architecture described in section 3specifies a viwtual machine which is compatible withboth traditional sequential processing based on VonNeumann architecture, and massively parallelprocessing using architectures based on connectionistprinciples.
The parsing scheme outlined in the lastsection represents a macro-level implementation of thedistributed representation parsing mechanism.
Thememory operators and agenda control structures thatdetermine the sequence of states of the mechanism arewell-specified, and the parallelism implicit in the systemis buried deep in the representation.
However, theconcept of a memory vector, and its theoreticaloperators, can also be mapped onto connectionistconcepts to provide a micro-level implementation of thesystem.A connectionist system would employ three layers,or collections, of elementary processing units.
Thesesets of units correspond to the three distributed memoryunits, and the gross connections between them wouldbe the same.
Within each set of units an item of479information is encoded as a distributed representation,that is, as a different pattern of activation over the units.The memory operations are modeled in terms ofexcitatory and inhibitory processes between units of thesame and different layers.
As with all connectionistmodels, it is necessary to delineate the principles whichdetermine the settings of the input weights and unitthresholds.
However, no global sequencing mechanismis required as the activation processes have theirinfluence over a number of interative cycles.
Each newinput pattern stimulates the activity of the system, andthrough their mutual interactions the three sets of unitsgradually converge on an equilibrium state.
Providingthe connection weights and unit thresholds are setcorrectly, the two working memory layers should encodethe types of active and inactive patterns describedpreviously.The major advantage of the distributedrepresentation parsing scheme is that it obviates theneed for special-purpose binding units as used in otherconnectionist parsing systems (Selman and Hirst, 1985;Cottrell, 1985).
The function of such units can beachieved through the appropriate use of the associationoperator.6.
TOWARD A COMPLETE LANGUAGE INPUTANALYSERThe main goal of this research is to establish therelationship between language understanding andmemory.
This involves building a complete languageanalysis system based on a homogenous set of memoryprocessing principles.
These principles would seem toinclude, (a) the interaction of mutual constraints, (b) ahigh degree of parallel processing, and (c) the necessityfor distributed representations to facilitate the simpleconcatenation of multiple constraints.
Such principlesneed to be matched against linguistic models andconstructs to derive the most integrated account oflinguistic phenomena.Within these goals, the present chart parsingscheme is not allied to any particular grammaticalformalism.
However, distributed memory systems havepreviously been used as the basis for a parsingmechanism based on the principles of lexical functionalgrammar (Slack, 1984a).
Originally, this systemincorporated a conventional CFG module, but this cannow be simulated using distributed memory systems.Thus, the processing underlying the language analyseris based on a more homogenous set of principles, andmore importantly, memory principles.As a component of the language analyser the CFGmodule generates a constituent structure comprising anaugmented phrase-structure tree.
This structure isderived using augmented CF rules of the form:6: S ---> NP VP('~Subj=~)480These augmented rules are easily assimilated into thepresent chart parsing scheme as follows: The functionalequations {e.g., ( t  Subj=j,)} are replaced by grammaticalfunction vectors; <SUB J>, <OBJ>, and so on.
Thesevectors are encoded on the stored CF rule patterns as athird form of output associated with sub-patterns uch as(<6>*<S>*<NP>).
The function vectors are output to aworking distributive memory system which encodes thefunctional structure of an input sentence.
As long as itsassociated sub-pattern is active within the CFG modulethe appropriate function vector will be output.
Thismeans that a particular grammatical function vector willonly be active in the system while the rule andconstituent with which it is associated are also active.7.
REFERENCESBarton, G.E.
and Berwick, R.C.
(1985) Parsing withassertion sets and information monotonicity.
InProceedings of MCAI-85, Los Angeles.Cottrell, G.W.
(1985) Connectionist Parsing.
InProceedings of the Seventh Annual Conference of theCognitive Science Society, Irvine, California, 201-212.Earley, J.
(1970) An efficient context-free parsingalgorithm.
Communications of the Association forComputing Machinery, 13, 94-102.Fahlrnan, S.E., Hinton, G.E., and Sejnowski, T.J.(1983) Massively parallel architectures for Al: NETL,Thistle, and Boltzmann machines.
In Proceedings of AAAI,Washington, 109-113.Feldman, J.A.
and Ballard, D. (1982) Connectionistmodels and their properties.
Cognitive Science, 6,205-254.Kaplan, R. and Bresnan, J.
(1982) Lexical FunctionalGrammar: A formal system for grammatical representation.In Bresnan, J.
(ed.
), The Mental Representation ofGrammatical Relations, MIT Press, Cambridge, Mass..Kay, M (1980) Algorithm schemata and data structuresin syntactic processing.
In Proceedings of the Symposiumon Text Processing.
Nobel Academy.Pollack, J. and Waltz, D. (1985) Massively parallelparsing: A strongly interactive model of natural languageinterpretation.
Cognitive Science, 9, 51-74.Selman, B. and Hirst, G. (1985) A rule-basedconnectionist parsing system.
In Proceedings of theSeventh Annual Conference of the Cognitive ScienceSociety, Irvine, California.Slack, J.M.
(1984a) A parsing architecture based ondistributed memory machines.
In Proceedings ofCOLING-84, Stanford.Slack, J.M.
(1984b) The role of distributed memory innatural language parsing.
In Proceedings of ECAI-84,Pisa.Thompson, H.S.
(1983) MCHART: A flexible, modularchart parsing system.
In Proceedings of AAAI, Washington.481
