Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 73?81,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsAutomated Content Scoring of Spoken Responses in an Assessment forTeachers of EnglishKlaus Zechner, Xinhao WangEducational Testing Service660 Rosedale RoadPrinceton, NJ 08541, USAkzechner@ets.org, xwang002@ets.orgAbstractThis paper presents and evaluates approachesto automatically score the content correctnessof spoken responses in a new language test forteachers of English as a foreign language whoare non-native speakers of English.
Most ex-isting tests of English spoken proficiency elic-it responses that are either very constrained(e.g., reading a passage aloud) or are of a pre-dominantly spontaneous nature (e.g., statingan opinion on an issue).
However, the assess-ment discussed in this paper focuses on essen-tial speaking skills that English teachers needin order to be effective communicators in theirclassrooms and elicits mostly responses thatfall in between these extremes and are moder-ately predictable.
In order to automaticallyscore the content accuracy of these spoken re-sponses, we propose three categories of robustfeatures, inspired from flexible text matching,n-grams, as well as string edit distance met-rics.
The experimental results indicate thateven based on speech recognizer output, mostof the feature correlations with human expertrater scores are in the range of r = 0.4 to r =0.5, and further, that a scoring model for pre-dicting human rater proficiency scores that in-cludes our content features can significantlyoutperform a baseline without these features(r = 0.56 vs. r = 0.33).1 IntroductionWith the increased need for instruction of interna-tional learners of English as a foreign language(EFL), there is a concomitant rise in demand toassess the language competence of English teach-ers who are non-native speakers of English.
Thissituation arises because it is neither possible noraffordable for countries where English is not spo-ken as a native language to employ only or evenmostly native speakers of English as EFL teachers.Moreover, as the language of instruction increas-ingly becomes English in most classrooms, teach-ers?
competence in the productive languagemodality of speaking becomes substantially moreimportant than in the past.
In order to meet thisdemand for assessing the English language profi-ciency of teachers of English, a new test, EnglishTeachers Language Assessment (ETLA), was de-veloped recently and piloted in 2012.
The testcomprises items for all four main language modali-ties: reading, listening, writing and speaking.While reading and listening items use a multi-ple-choice paradigm, test items for speaking andwriting elicit open responses.
For cost and effi-ciency reasons, we aim to employ automated scor-ing of written and spoken responses in this test.This paper is concerned in particular with the con-ceptualization, implementation and evaluation offeatures that can assess one aspect of Englishspeaking proficiency: the content correctness of atest taker?s response.
Our automated speech scor-ing system, SpeechRaterSMThe speaking items in ETLA range in complexi-ty from reading a text passage aloud to more chal-lenging tasks requiring multi-sentence responsesrelated to typical teaching situations.
The items,therefore, elicit speech in which predictabilityranges from high (e.g., reading aloud) to medium(e.g., open responses based on teaching material).
(Zechner et al 2009),also has features addressing other aspects of speak-ing proficiency, such as fluency or pronunciation,but the details of these features will not be dis-cussed as part of this paper.73While approaches to capture the content of mostlypredictable speech have been widely used in thepast (see, e.g., Alwan et al 2007; Franco et al2010), this is not the case for responses that exhibitconsiderable variation but are still much shorterand more constrained than spontaneous items fromother language tests, such as TOEFL iBT?Therefore, the goal of the study reported in thispaper is to conceptualize, implement and evaluatefeatures that can address the subset of ETLAspeaking items where responses are not stronglypredictable but are still fairly short and constrainedby the context of the item stimulus and prompt..1To illustrate what an ETLA speaking item maylook like, we provide a relatively simple examplehere.
Suppose the test taker (i.e., an English lan-guage teacher) is asked to request that the classopen their textbooks on page 55.
We could see arange of responses, from ?perfect?
(score level 3,e.g., ?Please open your textbooks on page 55.?
or?Please open your textbooks and turn to page55.?
), to ?good?
(score level 2, e.g., ?Please openthe books on the page 55.?)
and to ?poor?
(scorelevel 1, e.g., ?Open book page 55.?).
Again, notethat for this paper we are not interested in potentialissues with fluency, such as long pauses or speak-ing rate, nor with pronunciation or prosody.
Wejust look at the content of the test takers?
respons-es, either in idealized form by means of a humantranscription of what a test taker actually said, or ina realistic operational scenario, where we look atthe output of an ASR system.
In both cases, weconsider the sequence of words only (i.e., a textualrepresentation of the test takers?
spoken respons-es).One important aspect of any features used for con-tent scoring is that they have to be robust with re-spect to speech recognition errors.
Robustness isnecessary because we are using an automaticspeech recognition (ASR) system as a front end,and the average word error rate of the system isaround 27% for moderately predictable item re-sponses.In order to investigate the effectiveness of can-didate content features in a short-term developmentcycle before a larger amount of pilot data would beavailable, we first conducted a small scale in-house1 A test item is a basic element of a test, consisting of stimulusmaterial, such as text and/or visuals, and a prompt (test ques-tion) that elicits a response from the test taker.data collection effort focusing on the moderatelypredictable spoken items in ETLA.
Based on theanalysis of this mini-corpus, several different cate-gories of promising features were selected for po-tential operational use and then evaluated on thepilot data.The paper is organized as follows: Section 2provides an overview on related work; Section 3describes the in-house data set, the pilot data andthe ASR system; the developed features are pre-sented in Section 4; Section 5 presents our experi-ments; we then discuss our findings in Section 6and we conclude the paper in Section 7.2 Related WorkRelated to the automated assessment of writingfree-text, research to date has concentrated mainlyon two tasks: (1) scoring of short answers (Mitch-ell et al 2002; Leacock and Chodorow, 2003;Mohler and Mihalcea, 2009) and (2) scoring ofessays (Foltz et al 1999; Kanejiya et al 2003;Attali and Burstein, 2006).
For example, Leacockand Chodorow (2003) built an automated scoringsystem, c-rater?, to evaluate the short constructedor free-text responses, where the concepts given intest items were modeled, and the presence of theseexpected concepts in students?
answers would bedetected.As for the evaluation of free-text essays, Attaliand Burstein (2006) used a selected set of mean-ingful features to measure different constructedaspects of writing essays, such as grammar, usage,mechanics, style, organization, development, lexi-cal complexity and prompt-specific vocabularyusage.
In addition, the Intelligent Essay Assessor(Foltz et al 1999) used Latent Semantic Analysis(LSA) to score students?
answers by comparingthem to domain-representative texts.
Since LSA isbased on the bag-of-words model, researchers havealso tried to expand it by introducing additionalinformation, such as part-of-speech (POS) tags(Kanejiya et al 2003).In addition, research efforts have also beenmade to evaluate the content relatedness and cor-rectness for spoken responses.
For example, Xie etal.
(2012) used LSA and Pairwise Mutual Infor-mation approaches to evaluate the content correct-ness of unrestricted spontaneous spoken responses.Moreover, Chen and Zechner (2011) explored fea-74tures related to grammatical complexity in an au-tomated speech scoring system.In order to address the moderately predictablespeaking test items in the new ETLA, this paperpresents several different types of features to scorethe content correctness of the elicited spoken re-sponses.
Following a series of experiments andcomparisons, seven features from three contentfeature categories are selected and evaluated.3 Data Sets and ASR SystemThis study conducts experiments and evaluationsbased on two different data sets: (1) a small scalein-house data collection effort, which was used forthe design and development of content features;and (2) a larger-scale pilot data collection, whichwas used to further evaluate the features selectedaccording to the in-house data and to build scoringmodels for the prediction of human proficiencyscores.3.1 In-house Data CollectionTwenty-two items from ETLA with moderatelypredictable responses were selected for the in-house data collection.22 We decided to focus our efforts only on the moderately pre-dictable items since scoring of highly predictable item typeshas been extensively studied in previous research already.Firstly, 1,053 text responsesin total for all three score levels (3 = high profi-ciency, 2 = medium proficiency, 1 = low profi-ciency) were drafted and collected by humanexperts.
In order to simulate the operational scenar-io with an ASR system in place, a subset of re-sponses was recorded by a small set ofpredominantly non-native speakers of English.
Foreach test item, four responses were randomly se-lected from each score level, which resulted in 22?
3 ?
4 = 264 responses for voice recording.
Theremainder of 789 text responses comprised the setfor feature development and training.
In addition,about two thirds of the 264 text responses wererandomly double-recorded by a second speaker,resulting in a speech corpus with 444 spoken re-sponses in total, used as the evaluation set.
Fur-thermore, all these spoken responses weremanually transcribed to accommodate the errorsintroduced by reading, such as insertions of variousspeech disfluencies.3.2 Pilot Data CollectionThis study uses data from a 2012 pilot administra-tion of the ETLA assessment.
In particular, we fo-cus on 14 moderately predictable items from thepilot, covering 2,308 test takers.
In order to buildthe automatic speech recognizer and the scoringmodels, the pilot data were partitioned into fivedifferent subsets without any speaker and responseoverlaps.
The first three data partitions were usedfor training, development and evaluation of thespeech recognition system (hereafter, ?asrTrain?,?asrDev?
and ?asrEval?
), which included spokenresponses from both the moderately and highlypredictable items.
The asrTrain partition was fur-ther used to develop and train the content featuresdescribed below.
The remaining two partitionswere used for training and evaluation of scoringmodels that predicted item scores based on a set offeatures (hereafter, ?smTrain?
and ?smEval?
),where only the spoken responses from 14 moder-ately predictable items from one pilot form wereincluded.The detailed partition information is listed inTable 1.
All these spoken responses have beenmanually transcribed and scored with holisticscores from 1 to 3 by trained human expert raters.For the smTrain and smEval partitions, there were6,367 responses receiving double annotation, andthe inter-rater correlation was 0.73.
Furthermore,the average length of responses from smTrain andsmEval sets was 10.5 words, and the correspond-ing vocabulary size was 855 (not including partialwords).Partitions # Speakers # ResponsesasrTrain 1,658 27,604asrDev 25  700asrEval 25  700smTrain 300  3,452smEval 300  3,466Table 1.
Number of speakers and number of responsesincluded within each data partition.3.3 System ArchitectureOur automated speech scoring system,SpeechRater (Zechner et al 2009), consists of anASR system described below which generates aword hypothesis for every response by a test taker,including information about timing, energy andpitch, and other information from the input audio75file.
Next, the feature computation modules takethe outputs of the ASR system and compute a setof features, related to fluency, pronunciation, pros-ody, as well as content, the focus of this paper.
Fi-nally, a scoring model (linear regression model) istrained based on the smTrain set to predict scoresand then evaluated on unseen data (smEval set).3.4 ASR SystemIn this study, a state-of-the-art gender-independentHidden Markov Model speech recognition systemtrained on about 800 hours of non-native speech istaken as the baseline recognizer, and its languagemodel (LM) is then further adapted using the tran-scriptions from the asrTrain data partition.
Thelanguage model adaptation weights are tuned onthe asrDev set, and the resulting word error rate(WER) on the asrEval set (with both moderatelyand highly predictable responses) is 11.7%, and itsWER on the subset of 264 moderately predictableresponses is 19.7%.
This speech recognizer is fur-ther evaluated on both smTrain and smEval sets asshown in Table 2, only including moderately pre-dictable responses.Partition WER (%)smTrain 26.7smEval 26.9Table 2.
Word error rates (WER) of the speech recog-nizer on smTrain and smEval34 Content Featuresdata sets.Following a careful inspection and analysis of thecollected in-house data (described in Section 3.1above), several different categories of content fea-tures were designed and developed.
The initial dataanalysis showed that features need to be able tocapture very narrow ranges of expressions withminor variations, but also should be able to capturesomething like the ?overall accuracy?
of expres-sion, where local word sequences or phrasesshould conform to the expectations of the item de-sign without requiring that a response follows aconfined pattern in its entirety.
For the former situ-ation, features like regular expression matches3 The calculation of WER is based on only the recognizedoutputs with more than one word.
Thus, the number of actual-ly recognized responses is less than that in Table 1, i.e., 3,264responses for smTrain and 3,255 responses for smEval.seem appropriate to be a good match, whereas forthe latter, more flexible approaches such as n-grammodels or string edit distance metrics may be moreappropriate.
We list and describe our proposedcontent features in the following section.A.
Flexible String Matching MetricsAI.
Regular ExpressionsSince many responses in ETLA are expected tofollow certain patterns, it is intuitive to constructlimited regular expressions (RegEx) to match goldstandard responses for candidates with high profi-ciency score levels.
Accordingly, one type of regu-lar expression related features, re_match, can beextracted to detect whether the test response can bematched by any of the pre-built regular expres-sions.
This feature can obtain the values of 0 (doesnot match), 1 (partially matches) and 2 (exactlymatches).
Here, a partial match indicates that aRegEx can be matched within a test response thatalso has other spoken material, which is usefulwhen the speaker repeats or corrects the answermultiple times in a single item response, and thecompiled RegEx can still be used to match parts ofthe test response.This content feature has the advantage of highprecision, as it can precisely examine the contentcorrectness of the test responses.
Thus, the RegExshould be compiled to match all the example re-sponses at the highest score level 3 from the train-ing set.
For some test items with relatively shortand fixed answer patterns, this feature is quite use-ful; however, it is very time-consuming and diffi-cult to manually build regular expressions foritems with longer and more flexible expressions.Meanwhile, the mechanism of exact matching canmake this feature fail in very small variations ofexpression.
Especially when applying this featureon ASR output, it is difficult to successfully matchsome content-correct responses that havedisfluencies or recognition errors.Therefore, in order to improve the robustness ofRegEx, another regular expression related featureis proposed.
In general, for each item in ETLA,some pieces of specific expressions are required ina test response to represent its content correctness.Accordingly, we can segment the reference re-sponses into several fragments and identify somepieces as key fragments.
For example, when look-ing at the reference response ?Please open your76text books and turn to page 55.?
two key fragmentscan be extracted with ?Please open your textbooks?
and ?turn to page 55.?
We group versionsof these key fragments from the training corpustogether and construct regular expressions to matcheach group.
Afterwards, a feature can be defined tocount how many key fragments can be matched bya test response, namely num_fragments.AII.
Keyword DetectionFor moderately predictable items on ETLA, key-word lists can be extracted from the stimulus mate-rial and the item prompt, containing the words thatneed to be included in a test response by test tak-ers.
Then a feature, num_keywords, can be used toexamine how many keywords appear in a test re-sponse, which can be further normalized by thenumber of predefined keywords for each item, i.e.,percent_keywords.
In addition, as some keywordsmay be a phrase with multiple words, such as?page 55,?
we can split all the keywords into sin-gle words and get another sub-keywords list.
Thentwo corresponding features can be extracted asnum_sub_keywords and percent_sub_keywords.B.
N-gramsBI.
Word N-gramsThe word n-gram model is introduced here to cap-ture the similarity of word usage between the testand the reference responses.
Based on the collectedtraining samples, trigrams are trained using the textresponses from the highest score level 3.
Then, theLM can be used to score a test response, and theresulting probability can be taken as feature, calledlm_3.BII.
POS SimilarityThis feature measures the syntactic complexity oftest responses based on the distribution of POStags.
First, all the responses from the training dataset are assigned with POS tag sequences via anautomatic POS tagger.
Then, a POS vector accord-ing to each score level can be obtained by gather-ing the POS unigram, bigram or trigram statisticsfrom the same score level.Given a test response, its corresponding POSsequence can be determined by the same POS tag-ger, and the cosine similarities between the testPOS n-gram vector and the POS vectors from threedifferent score levels can be calculated as pos_1,pos_2 and pos_3, where pos_3 is used as a featurein our experiments below.
Furthermore, by com-paring these three cosine similarities, the score cat-egory with the highest similarity can be extractedas another feature, i.e., pos_score.BIII.
Machine Translation Evaluation Metric(BLEU)BLEU (Papineni et al 2002) is one of the mostpopular metrics for automatic evaluation of ma-chine translation, where the score is calculatedbased on the modified n-gram precision.
In thisstudy, the BLEU score is introduced to evaluatethe content quality of a test response, where threedifferent gold standard reference corpora are ex-tracted from the training set according to eachscore level.
Similar to the edit distance and WERfeatures described below, three BLEU scores arecalculated by comparing them with reference re-sponses from each score level (i.e., bleu_1, bleu_2and bleu_3).
We decide to use the following twofeatures for our experiments below: bleu_3 andbleu_score, the score level which receives themaximum BLEU score.C.
String Edit Distance MetricsCI.
String Edit DistanceAs the edit distance is an effective string metric formeasuring the amount of difference between twoword sequences, including insertions, deletions andsubstitutions, we use it to capture the sequence dis-tance between the test and reference responses.Given a test response, we can separately calcu-late the edit distance by comparing it with trainingresponses from each score level.
Afterwards, theminimum edit distance from each score level canbe extracted as ed_1, ed_2 and ed_3, where ed_3 isselected as feature for our experiments.
Further-more, by comparing these three edit distances, thescore category with the minimum value is taken asanother feature, ed_score.CII.
Word Error Rate (WER)By dividing the edit distance by the length of thereference response, we obtain the word error rate(WER) metrics, commonly used in speech recogni-tion, and two additional features, wer_3 andwer_score, similarly as above, can be calculated.Compared to the above category of n-gram re-lated features, which capture the n-gram fragment77matching between the test and reference samples,the category of edit distance features try to find themost similar reference sample to the test sample atthe whole-response level.Finally, all the proposed features are implement-ed and then examined based on both the ideal hu-man transcription and the realistic ASR output.The speech recognizer used with the small in-house data is the same as the ASR system de-scribed in Section 3.4, but its language model isadapted with the much smaller set of 789 trainingtext responses.
The WER of this system is 17.8%,evaluated on 444 spoken responses.In addition, in order to increase the robustness ofthe extracted features, a preprocessing stage is in-troduced to remove all the disfluencies from theASR output, such as filler words, recognized par-tial words and repeated words.
Afterwards, eachfeature is evaluated on both the transcription andthe ASR output of the 444 collected spoken re-sponses, and its corresponding Pearson correlationcoefficient with human scores is presented in Table3.Based on overall correlation, inter-correlationanalyses, as well as on construct45 Experiments and Resultsconsiderations,seven content features from three categories areselected and will be evaluated on a larger scale onETLA pilot data in the next section: re_match(A1), num_fragments (A2), percent_sub_keywords(A3), bleu_3 (B1), ed_score (C1), wer_3 (C2) andwer_score (C3).This section first describes experiments related tothe performance of the seven selected content fea-tures on a larger corpus from an ETLA pilot ad-ministration (described above in Section 3.2).Then, a similar analysis is conducted based on hu-man rater analytic content scores on a subset ofthis data.
Finally, the selected content features arecombined with other features related to pronuncia-tion, prosody and fluency to build a scoring modelfor the prediction of human scores.4 A construct is the set of knowledge, skills and abilitiesmeasured by a test.
The term ?construct considerations?
in thecontext of feature selection refers to the process of ensuringthat the selected feature set obtains a high coverage of all as-pects of the relevant construct.Feature Trans ASRAre_match 0.789 0.537num_fragments 0.629 0.523num_keywords 0.269 0.254percent_keywords 0.419 0.375num_sub_keywords 0.249 0.239percent_sub_keywords 0.482 0.417Blm_3 0.482 0.461pos_3 0.270 0.270pos_score 0.315 0.339bleu_3 0.531 0.458bleu_score 0.144 0.194Ced_3 -0.362 -0.337ed_score 0.642 0.614wer_3 -0.573 -0.513wer_score 0.585 0.557Table 3.
Pearson correlation coefficients (r) of contentfeatures with human holistic scores.5.1 Feature Evaluation on Pilot DataIn the following experiments, we use the asrTrainset to train the content features.
Then these featuresare examined on the smTrain and smEval data sets.In order to extract the edit distance, WER- andBLEU-related features for each item, three textreference corpora according to different score lev-els, are needed.
Duplicate reference responses withthe same content are removed within each scorelevel.Furthermore, we improve two RegEx featuresusing the reference responses from the highestscore level 3 in the asrTrain set.
(1) Since the pre-viously obtained re_match feature based on the in-house data may not be able to match multiple con-tent-correct responses in the pilot data, we need toaugment the set of RegEx for this feature based oncorrect responses from score level 3 in the asrTrainset.
(2) Since the maximum number of candidatefragments varies across different ETLA items, thenum_fragments feature values are not comparableacross items.
Therefore, we redesign this featureby assigning a list of manually selected keywordsfor each fragment.
During feature extraction, wecount the number of distinct keywords associatedwith all the matched fragments and divide thisnumber by the number of predefined keywords foreach item (as in AII.
Keyword Detection), whichresults in another feature: perc_fragment_kw (A2).Based on the ASR output of smTrain andsmEval data sets, seven content features are ex-tracted and their Pearson correlation coefficientswith the holistic human scores are calculated andshown in Table 4.78FeaturesmTrain (r) smEval (r)Trans ASR Trans ASRA1 0.53 0.415 0.534 0.441A2 0.576 0.458 0.583 0.48A3 0.42 0.286 0.419 0.297B1 0.597 0.478 0.564 0.452C1 0.535 0.412 0.52 0.39C2 -0.588 -0.469 -0.564 -0.446C3 0.554 0.433 0.51 0.428Table 4.
Pearson correlation coefficients between con-tent features and human holistic scores, based on boththe transcription and the ASR output of smTrain andsmEval.55.2 Evaluations Using Human Rater Analyt-ic Content ScoresFeatures include A1 (re_match), A2(perc_fragment_kw), A3 (percent_sub_keywords), B1(bleu_3), C1 (ed_score), C2 (wer_3) and C3(wer_score)In addition to the human rating of all spoken re-sponses of the ETLA pilot data set with holisticscores that take into account both the dimensionsof ?delivery?
(fluency, pronunciation, prosody)and ?content,?
a subset of the data was furtherscored by human expert raters in these two dimen-sions separately, resulting in so-called analyticscores for delivery and content.
The inter-correlation for content analytic scores was 0.79.1,410 responses from the smTrain set and 1,402responses from the smEval set received such ana-lytic content scores.
On this subset, table 5 showsthe Pearson correlation coefficients between thecontent features and the analytic content scores, aswell as the holistic scores, for comparison.5.3 Scoring Model ComparisonWe further examine these content features by in-troducing them in a scoring model to predict hu-man rater holistic proficiency scores, usingsmTrain for training of the models and smEval fortheir evaluation.
The baseline system employs 14features related to the construct dimension of de-livery, such as pronunciation, prosody and fluency.5 The evaluation is conducted on recognition output with morethan one word.
In addition, due to technical problems, such ashigh background noise, some responses are non-scorable forhuman raters, and these responses are removed from the eval-uation sets.
Finally, there are 3176 responses included insmTrain, and 3084 responses in smEval.FeaturesmTrain (r)Holistic ContentTrans ASR Trans ASRA1 0.529 0.415 0.563 0.434A2 0.564 0.46 0.646 0.525A3 0.422 0.283 0.452 0.277B1 0.6 0.499 0.654 0.504C1 0.527 0.43 0.555 0.46C2 -0.588 -0.473 -0.627 -0.488C3 0.542 0.434 0.563 0.462FeaturesmEval (r)Holistic ContentTrans ASR Trans ASRA1 0.525 0.424 0.538 0.436A2 0.579 0.472 0.621 0.512A3 0.423 0.308 0.454 0.321B1 0.563 0.442 0.606 0.471C1 0.521 0.4 0.539 0.422C2 -0.543 -0.42 -0.584 -0.457C3 0.514 0.417 0.529 0.439Table 5.
Pearson correlation coefficients between con-tent features and human analytic content scores as wellas human holistic scores.Furthermore, an extended scoring model is built byadding the selected seven content features to themodel.
Table 6 provides the comparison betweenthese two scoring models, reporting both quadraticweighted kappa and Pearson correlation coeffi-cients between automatically predicted scores andhuman holistic scores on the smEval data set.Scoring Model Kappa rBaseline (Delivery only) 0.30 0.33Extended (Delivery+Content) 0.53 0.56Table 6.
Scoring model comparison: quadratic weightedkappa and Pearson correlation coefficients between pre-dicted scores (unrounded) and human holistic scores.6 DiscussionThe goal of this paper was to conceptualize, im-plement and evaluate features that can determinethe content correctness of spoken item responses inan English language test for teachers of Englishwho are not native speakers of English.Based on observations from a small in-house da-ta collection, where human test developers andcontent experts created example responses to 22test items for three different score levels, we de-cided to implement a range of features that cancapture the content correctness of test takers?
re-sponses in varying degree of precision.
Our fea-79tures belong to three classes: features related tofixed expressions, with potential small variations,such as regular expressions or keywords; featuresbased on n-grams of words or POS tags, includingthe BLEU metrics frequently used for evaluationsof machine translation output; and features relatedto measures of string edit distance, including theWER metrics commonly used in speech recogni-tion evaluations.It should be noted that we use the term ?content?in a fairly broad way in this paper, namely, every-thing in a spoken response that is not related tolower-level aspects of speech production such asfluency or pronunciation.
Since the scoring rubricsfor ETLA place a high emphasis both on thegrammatical accuracy, as well as on the correctcontent (in a more narrow sense), this situation isreflected by our choice of features that focus bothon elements traditionally associated with content(such as matching of keywords), as well as on ele-ments more related to correct grammatical expres-sions (e.g., sequences of POS tags).Our initial evaluations on the small in-house da-ta collection showed that most of these featurescorrelate well with human expert scores, bothwhen using transcribed speech as well as whenusing ASR output.
The absolute correlations forhuman transcriptions of speech range from r =0.144 (bleu_score) to r = 0.789 (re_match), and forASR output from r = 0.194 (bleu_score) to r =0.614 (ed_score).
The relative drop in correlationbetween these two conditions varies across fea-tures, but is generally around 5%-15%, withre_match having a much larger performance dropfrom r = 0.789 for transcribed speech to r = 0.537for ASR output (32% relative decrease in perfor-mance).
6From this initial set of 15 features, we selectedseven features based on feature performance, inter-correlation analyses (i.e., avoiding features thathave a high inter-correlation and measure a similaraspect of content), and considerations of construct,i.e., which features are representing content in away that is consistent with what human expertswould consider important in determining the con-tent correctness of a response.
This subset of seven6 The correlation of one feature, pos_3, remained unchangedbetween the two conditions, and two features, pos_score andbleu_score, showed higher correlations for ASR output thanfor human transcriptions.features includes three features each from the clas-ses of flexible string matching and string edit dis-tance, and one feature (bleu_3) from the n-gramclass.When evaluating these seven features on a largerdata set, the smTrain and smEval sets of the 2012ETLA pilot data, we find absolute correlations be-tween features and human holistic scores rangingfrom r = 0.286 to r = 0.480 for ASR output, andfrom r = 0.419 to r = 0.597 for transcriptions.
Therelative decrease in correlation between transcrip-tions and ASR outputs ranges from 16% to 32% inthese data sets (smTrain and smEval).
The magni-tude of content feature correlations observed in thisstudy is similar to that of features related to fluen-cy and pronunciation computed on spontaneousspeech, as reported in Zechner et al(2009).
Infact, due to the brevity of the moderately predicta-ble responses in ETLA, features related to fluencyand pronunciation achieve correlations of less than0.3 on this data set, making content features crucialfor the assessment of speech here.When comparing the six content features thatare identical between the original feature set of 15features (in-house data collection) and the finalfeature set, we observe a relative drop in featurecorrelation between the in-house data set and thesmEval pilot data set between 1% (blue_3) and36% (ed_score), with an average decrease of 20%.This performance decrease can be explained by (1)the more challenging data set of the pilot, as indi-cated, e.g., by a much higher word error rate of theASR system (27% vs. 18%); and (2) the fact thatthe in-house data collection was much more con-strained in terms of test taker response variationcompared to the real-world pilot data.Since a subset of the ETLA responses was alsoscored analytically by human raters, we could fur-ther compare the feature correlations between ho-listic vs. analytic content scores (Section 5.2).
Wefind that on smEval, for all features, absolute cor-relations increase on human analytic content scorescompared to human holistic scores.
Although thesedifferences are rather small (0.01 to 0.04), this isan indicator that our features are measuring whatthey are supposed to measure, since the holisticscores also take other dimensions of speech, suchas fluency and pronunciation, into account.807 Conclusion and Future WorkThis paper presented a study whose aim was toconceptualize, implement and evaluate features tomeasure the content correctness of test takers?
re-sponses in a new assessment for EFL teacherswhose native language is not English.We implemented and evaluated an initial set of15 content features from three feature classes: flex-ible string matching, n-grams and string edit dis-tance metrics.
A subset of these features was thenevaluated on a 2012 ETLA pilot administration,and we found correlations between features andhuman holistic scores in the range of r = 0.29 to r= 0.48 on ASR output.
Correlations increasedwhen comparing features with human analytic con-tent scores.Finally, we compared a baseline regression scor-ing model for prediction of human holistic scoreswithout any content features to an extended modelusing seven content features and found that themodel correlation substantially improved from r =0.33 (baseline) to r = 0.56 (extended model).Future work will include devising strategies onhow to obtain RegEx features more quickly in asemi-automated way in order to reduce human la-bor.
Further, we plan more in-depth analysis of thefeature performance across different test items anditem types which potentially could lead to furtherimprovements and refinements of our content fea-tures.ReferencesAbeer Alwan, Yijian Bai, Matt Black, Larry Casey,Matteo Gerosa, Margaret Heritage, Markus Iseli,Barbara Jones, Abe Kazemzadeh, Sungbok Lee,Shrikanth Narayanan, Patti Price, Joseph Teppermanand Shizhen Wang.
2007.
A system for technologybased assessment of language and literacy in youngchildren: the role of multiple information sources.Proceedings of IEEE International Workshop onMultimedia Signal Processing, 26-30.Yigal Attali and Jill Burstein.
2006.
Automated essayscoring with e-rater?
V.2.0.
Journal of Technology,Learning, and Assessment, 4(3): 159-174.Miao Chen and Klaus Zechner.
2011.
Computing andevaluating syntactic complexity features forautomated scoring of spontaneous non-nativespeech.
Proceedings of ACL, 722-731.Peter W. Foltz, Darrell Laham and Thomas K.Landauer.
1999.
The intelligent essay assessor:Applications to educational technology.
InteractiveMultimedia Electronic Journal of Computer-Enhanced Learning, 1(2).Horacio Franco, Harry Bratt, Romain Rossier, VenkataRao Gadde, Elizabeth Shriberg, Victor Abrash andKristin Precoda.
2010.
EduSpeak?
: A speechrecognition and pronunciation scoring toolkit forcomputer-aided language learning applications.Language Testing, 27(3): 401-418.Dharmendra Kanejiya, Arun Kumary and SurendraPrasad.
2003.
Automatic evaluation of students'answers using syntactically enhanced LSA.Proceedings of Workshop on Building EducationalApplications Using Natural Language Processing,53-60.Claudia Leacock and Martin Chodorow.
2003.
C-rater:Automated scoring of short-answer questions.Computers and Humanities,37: 389?405.Tom Mitchell, Terry Russell, Peter Broomhead andNicola Aldridge.
2002.
Towards robustcomputerised marking of free-text responses.Proceedings of International Computer AssistedAssessment Conference, 233-249.Michael Mohler and Rada Mihalcea.
2009.
Text-to-textsemantic similarity for automatic short answergrading.
Proceedings of EACL, 567-575.Kishore Papineni, Salim Roukos, Todd Ward and Wei-Jing Zhu.
2002.
BLEU: A method for automaticevaluation of machine translation.
Proceedings ofACL, 311-318.Shasha Xie, Keelan Evanini and Klaus Zechner.
2012.Exploring content features for automated speechscoring.
Proceedings of NAACL-HLT, 103-111.Klaus Zechner, Derrick Higgins, Xiaoming Xi andDavid M. Williamson.
2009.
Automatic scoring ofnon-mative spontaneous speech in tests of spokenEnglish.
Speech Communication, 51: 883-895.81
