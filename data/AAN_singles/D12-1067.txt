Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 732?743, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsParse, Price and Cut?Delayed Column and Row Generation for GraphBased ParsersSebastian Riedel David Smith Andrew McCallumDepartment of Computer ScienceUniversity of Massachusetts, Amherst{riedel,dasmith,mccallum}@cs.umass.eduAbstractGraph-based dependency parsers suffer fromthe sheer number of higher order edges theyneed to (a) score and (b) consider during opti-mization.
Here we show that when workingwith LP relaxations, large fractions of theseedges can be pruned before they are fullyscored?without any loss of optimality guar-antees and, hence, accuracy.
This is achievedby iteratively parsing with a subset of higher-order edges, adding higher-order edges thatmay improve the score of the current solu-tion, and adding higher-order edges that areimplied by the current best first order edges.This amounts to delayed column and row gen-eration in the LP relaxation and is guaranteedto provide the optimal LP solution.
For secondorder grandparent models, our method consid-ers, or scores, no more than 6?13% of the sec-ond order edges of the full model.
This yieldsup to an eightfold parsing speedup, while pro-viding the same empirical accuracy and cer-tificates of optimality as working with the fullLP relaxation.
We also provide a tighter LPformulation for grandparent models that leadsto a smaller integrality gap and higher speed.1 IntroductionMany problems in NLP, and structured prediction ingeneral, can be cast as finding high-scoring struc-tures based on a large set of candidate parts.
Forexample, in second order graph-based dependencyparsing (K?bler et al2009) we have to choose aquadratic number of first order and a cubic numberof second order edges such that the graph is bothhigh-scoring and a tree.
In coreference, we haveto select high-scoring clusters of mentions from anexponential number of candidate clusters, such thateach mention is in exactly one cluster (Culotta etal., 2007).
In segmentation of citation strings, weneed to consider a quadratic number of possible seg-ments such that every token is part of exactly onesegment (Poon and Domingos, 2007).What makes such problems challenging is thelarge number of possible parts to consider.
Thisnumber not only affects the cost of search or opti-mization but also slows down the process of scor-ing parts before they enter the optimization prob-lem.
For example, the cubic grandparent edges insecond-order dependency parsing slow down dy-namic programs (McDonald and Pereira, 2006), be-lief propagation (Smith and Eisner, 2008) and LPsolvers (Martins et al2009), since there are morevalue functions to evaluate, more messages to pass,or more variables to consider.
But to even calculatethe score for each part we need a cubic number ofoperations that usually involve expensive feature ex-traction.
This step often becomes a major bottleneckin parsing, and structured prediction in general.Candidate parts can often be heuristically pruned.In the case of dependency parsing, previous workhas used coarse-to-fine strategies where simpler firstorder models are used to prune unlikely first or-der edges, and hence all corresponding higher or-der edges (Koo and Collins, 2010; Martins et al2009; Riedel and Clarke, 2006).
While such meth-ods can be effective, they are more convoluted, oftenrequire training of addition models as well as tuningof thresholding hyper-parameters, and usually pro-vide no guarantees of optimality.We present an approach that can solve problemswith large sets of candidate parts without consider-ing all of these parts in either optimization or scor-732ing.
And in contrast to most pruning heuristics, ouralgorithm can give certificates of optimality beforehaving optimized over, or even scored, all parts.
Itdoes so without the need of auxiliary models or tun-ing of threshold parameters.
This is achieved by adelayed column and row generation algorithm thatiteratively solves an LP relaxation over a small sub-set of current candidate parts, and then finds newcandidates that score highly and can be inserted intothe current optimal solution without removing highscoring existing structure.
The latter step subtractsfrom the cost of a part the price of resources the partrequires, and is often referred as pricing.
Sometimesparts may score highly after pricing, but are neces-sary in order to make the current solution feasible.We add such parts in a step that roughly amounts toviolated cuts to the LP.We illustrate our approach in terms of a second-order grandparent model for dependency parsing.We solve these models by iteratively parsing, pric-ing, and cutting.
To this end we use a variant of theLP relaxation formulated by Martins et al2009).Our variant of this LP is designed to be amenable tocolumn generation.
It also turns out to be a tighterouter bound that leads to fewer fractional solutionsand faster runtimes.
To find high scoring grandpar-ent edges without explicitly enumerating all of them,we prune out a large fraction using factorized upperbounds on grandparent scores.Our parse, price and cut algorithm is evaluatedusing a non-projective grandparent model on threelanguages.
Compared to a brute force approach ofsolving the full LP, we only score about 10% of thegrandparent edges, consider only 8% in optimiza-tion, and so observe an increase in parsing speed ofup to 750%.
This is possible without loss of opti-mality, and hence accuracy.
We also find that ourextended LP formulation leads to a 15% reductionof fractional solutions, up to 12 times higher speed,and generally higher accuracy when compared to thegrandparent formulation of Martins et al2009).2 Graph-Based Dependency ParsingDependency trees are representations of the syntac-tic structure of a sentence (Nivre et al2007).
Theydetermine, for each token of a sentence, the syntac-tic head the token is modifying.
As a lightweight al-ternative to phrase-based constituency trees, depen-dency representations have by now seen widespreaduse in the community in various domains such asquestion answering, machine translation, and infor-mation extraction.To simplify further exposition, we now formalizethe task, and mostly follow the notation of Martins etal.
(2009).
Consider a sentence x = ?t0, t1, .
.
.
, tn?where t1, .
.
.
, tn correspond to the n tokens of thesentence, and t0 is an artificial root token.
LetV , {0, .
.
.
, n} be a set of vertices correspondingto the tokens in x, and C ?
V ?V a set of candidatedirected edges.
Then a directed graph y ?
C is alegal dependency parse if and only if it is a tree overV rooted at vertex 0.
Given a sentence x, we use Yto denote the set of its legal parses.
Note that all ofthe above definitions depend on x, but for simplicitywe omit this dependency in our notation.2.1 Arc-Factored ModelsGraph-based models define parametrized scoringfunctions that are trained to discriminate betweencorrect and incorrect parse trees.
So called arc-factored or first order models are the most basicvariant of such functions: they assess the quality of atree by scoring each edge in isolation (McDonald etal., 2005b; McDonald et al2005a).
Formally, arc-factored models are scoring functions of the forms (y;x,w) =??h,m??ys?h,m?
(x,w) (1)where w is a weight vector and s?h,m?
(x,w) scoresthe edge ?h,m?
with respect to sentence x andweightsw.
From here on we will omit both x andwfrom our notation if they are clear from the context.Given such a scoring function, parsing amounts tosolving:maximizey??h,m?
?ys?h,m?subject to y ?
Y.
(2)2.2 Higher Order ModelsArc-factored models cannot capture higher order de-pendencies between two or more edges.
Higherorder models remedy this by introducing scoresfor larger configurations of edges appearing in the733tree (McDonald and Pereira, 2006).
For example,in grandparent models, the score of a tree also in-cludes a score sgp?g,p,c?
for each grandparent-parent-child triple ?g, p, c?
:s (y) =??h,m??ys?h,m?
+??g,p??y,?p,c??ysgp?g,p,c?
(3)There are other variants of higher order modelsthat include, in addition to grandparent triples, pairsof siblings (adjacent or not) or third order edges.However, to illustrate our approach we will focuson grandparent models and note that most of whatwe present can be generalized to other higher ordermodels.2.3 Feature TemplatesFor our later exposition the factored andparametrized nature of the scoring functionswill be crucial.
In the following we thereforeillustrate this property in more detail.The scoring functions for arcs or higher orderedges usually decompose into a sum of feature tem-plate scores.
For example, the grandparent edgescore sgp?g,p,c?
is defined assgp?g,p,c?
,?t?T gpsgp,t?g,p,c?
(4)where T gp is the set of grandparent templates, andeach template t ?
T gp defines a scoring func-tion sgp,t?g,p,c?
to assess a specific property of thegrandparent-parent-child edge ?g, p, c?.The template scores again decompose.
Consider-ing grandparent scores, we getst?g,p,c?
, w>t ft (htg, htp, htc, dtg,p,c)(5)where hti is an attribute of token ti, say h101i =Part-of-Speech (ti).
The term dtg,p,c corresponds toa representation of the relation between tokens cor-responding to g, p and g. For example, for template101 it could return their relative positions to eachother:d101g,p,c , ?I [g > p] , I [g > c] , I [p > c]?
.
(6)The feature function f t maps the representationsof g, p and c into a vector space.
For the purposes ofour work this mapping is not important, and hencewe omit details.2.4 LearningThe scoring functions we consider are parametrizedby a family of per-template weight vectors w =?wt?t?T .
During learning we need to estimate wsuch that our scoring functions learns to differenti-ate between correct and incorrect parse trees.
Thiscan be achieved in many ways: large margin train-ing, maximizing conditional likelihood, or variantsin between.
In this work we follow Smith and Eis-ner (2008) and train the models with stochastic gra-dient descent on the conditional log-likelihood of thetraining data, using belief propagation in order tocalculate approximate gradients.3 LP and ILP FormulationsRiedel and Clarke (2006) showed that dependencyparsing can be framed as Integer Linear Pro-gram (ILP), and efficiently solved using an off-the-shelf optimizer if a cutting plane approach is used.1Compared to tailor made dynamic programs, suchgeneric solvers give the practitioner more modelingflexibility (Martins et al2009), albeit at the costof efficiency.
Likewise, compared to approximatesolvers, ILP and Linear Program (LP) formulationscan give strong guarantees of optimality.
The studyof Linear LP relaxations of dependency parsing hasalso lead to effective alternative methods for parsing,such as dual decomposition (Koo et al2010; Rushet al2010).
As we see later, the capability of LPsolvers to calculate dual solutions is also crucial forefficient and exact pruning.
Note, however, that dy-namic programs provide dual solutions as well (seesection 4.5 for more details).3.1 Arc-Factored ModelsTo represent a parse y ?
Y we first introduce anvector of variables z , ?za?a where za is 1 if a ?
yand 0 otherwise.
With this representation parsingamounts to finding a vector z that corresponds to alegal parse tree and that maximizes?a zasa.
Oneway to achieve this is to search through the convexhull of all legal incidence vectors, knowing that anylinear objectives would take on its maximum on oneof the hull?s vertices.
We will use Z to denote thisconvex hull of incidence vectors of legal parse trees,1Such as the highly efficient and free-for-academic-useGurobi solver.734and callZ the arborescence polytope (Martins et al2009).
The Minkowski-Weyl theorem tells us thatZcan be represented as an intersection of halfspaces,or constraints, Z = {z|Az ?
b}.
Hence optimaldependency parsing, in theory, can be addressed us-ing LPs.However, it is difficult to describe Z with a com-pact number of constraints and variables that lendthemselves to efficient optimization.
In general wetherefore work with relaxations, or outer bounds, onZ .
Such outer bounds are designed to cut off allillegal integer solutions of the problem, but still al-low for fractional solutions.
In case the optimum isachieved at an integer vertex of the outer bound, itis clear that we have found the optimal solution tothe original problem.
In case we find a fractionalpoint, we need to map it onto Z (e.g., by projectionor rounding).
Alternatively, we can use the outerbound together with 0/1 constraints on z, and thenemploy an ILP solver (say, branch-and-bound) tofind the true optimum.
Given the NP-hardness ofILP, this will generally be slow.In the following we will present the outer boundZ?
?
Z proposed by Martins et al2009).Compared to the representation Riedel and Clarke(2006), this bound has the benefit a small polyno-mial number of constraints.
Note, however, that of-ten exponentially many constraints can be efficientlyhandled if polynomial separation algorithms exists,and that such representations can lead to tighterouter bounds.The constraints we employ are:No Head For Root In a dependency tree the rootnode never has a head.
While this could be capturedthrough linear constraints, it is easier to simply re-strict the candidate set C to never contain edges ofthe form ?
?, 0?.Exactly One Head for Non-Roots Any non-roottoken has to have exactly one head token.
We canenforce this property through the set of constraints:m > 0 :?hz?h,m?
= 1.
(OneHead)No Cycles A parse tree cannot have cycles.
This isequivalent, together with the head constraints above,to enforcing that the tree be fully connected.
Mar-tins et al2009) capture this connectivity constraintusing a single commodity flow formulation.
Thisrequires the introduction of flow variables ?
,?
?a?a?C .
By enforcing that token 0 has n outgoingflow, ?m>0??0,m?
= n, (Source)that any other token consumes one unit of flow,t > 0 :?h??h,t?
??m>0??t,m?
= 1 (Consume)and that flow is zero on disabled arcs??h,m?
?
nz?h,m?, (NoFlow)connectivity can be ensured.Assuming we have such a representation, parsingwith an LP relaxation amounts to solvingmaximizez?0?a?Azasasubject to A[z?]?
b.
(7)3.2 Higher Order ModelsThe 1st-Order LP can be easily extended to capturesecond (or higher) order models.
For for the caseof grandparent models, this amounts to introduc-ing another class of variables, zgpg,p,c, that indicate ifthe parse contains both the edge ?g, p?
and the edge?p, c?.
With the help of the indicators zgp we can rep-resent the second order objective as a linear function.We now need an outer bound on the convex hull ofvectors ?z, zgp?
where z is a legal parse tree and zgpis a consistent set of grandparent indicators.
We willrefer to this convex hull as the grandparent polytopeZgp.We can re-use the constraints A of section 3.1 toensure that z is in Z .
To make sure zgp is consistentwith z, Martins et al2009) linearize the equiva-lence zgpg,p,c ?
zg,p ?
zp,c we know to hold for legalincidence vectors, yieldingg, p, c : z?g,p?
+ z?p,c?
?
zgp?g,p,c?
?
1 (ArcGP)andg, p, c : z?g,p?
?
zgp?g,p,c?, z?p,c?
?
zgp?g,p,c?
(GPArc)There are additional constraints we know to hold inZgp.
First, we know that for any active edge ?p, c?
?735y with p > 0 there is exactly one grandparent edge?g, p, c?.
Likewise, for an inactive edge ?p, c?
/?
ythere must be no grandparent edge ?g, p, c?.
Thiscan be captured through the constraint:p > 0, c :?gzgp?g,p,c?
= z?p,c?.
(OneGP)We also know that if an edge ?g, p?
in inactive,there must not be any grandparent edge ?g, p, c?
thatgoes through ?g, p?
:g, p :?czgp?g,p,c?
?
nz?g,p?.
(NoGP)It can be easily shown that for integer solu-tions the constraints ArcGP and GPArc of Martinset al2009) are sufficient conditions for consis-tency between z and zgp.
It can equally be shownthat the same holds for the constraints OneGP andNoGP.
However, when working with LP relax-ations, the two polytopes have different fractionalvertices.
Hence, by combining both constraint sets,we can get a tighter outer bound on the grandparentpolytope Zgp.
In section 6 we show empirically thatthis combined polytope in fact leads to fewer frac-tional solutions.
Note that when using the union ofall four types of constraints, the NoGP constraint isimplied by the constraint GPArc (left) by summingover c on both sides, and can hence be omitted.4 Parse, Price and CutWe now introduce our parsing algorithm.
To thisend, we first give a general description of columnand row generation for LPs; then, we illustrate howthese techniques can be applied to dependency pars-ing.4.1 Column and Row GenerationLPs often have too many variables and constraintsto be efficiently solved.
In such cases delayedcolumn and row generation can substantially re-duce runtime by lazily adding variables only whenneeded (Gilmore and Gomory, 1961; L?bbecke andDesrosiers, 2004).To illustrate column and row generation let usconsider the following general primal LP and its cor-responding dual problem:Primalmaximizez?0s?zsubject to Az ?
bDualminimize??0?
?bsubject to A??
?
s.Say you are given a primal feasible z?
and a dual fea-sible ??
for which complementary slackness holds:for all variables i we have z?i > 0?
si =?j ?
?jai,jand for all constraints j we have ?
?j > 0 ?
bj =?i z?iai,j .
In this case it is easy to show that z?
isan optimal primal solution, ??
and optimal dual so-lution, and that both objectives meet at these val-ues (Bertsekas, 1999).The idea behind delayed column and row gener-ation is to only consider a small subset of variables(or columns) I and subset of constraints (or rows) J .Optimizing over this restricted problem, either withan off-the-shelf solver or a more specialized method,yields the pair(z?I ,?
?J)of partial primal and dualsolutions.
This pair is feasible and complementarywith respect to variables I and constraints J .
Wecan extend it to a solution (z?,y?)
over all variablesand constraints by heuristically setting the remain-ing primal and dual variables.
If it so happens that(z?,y?)
is feasible and complementary for all vari-ables and constraints, we have found the optimal so-lution.
If not, we add the constraints and variablesfor which feasibility and slackness are violated, andresolve the new partial problem.In practice, the uninstantiated primal and dualvariables are often set to 0.
In this case complemen-tary slackness holds trivially, and we only need tofind violated primal and dual constraints.
For primalconstraints,?i ziai,j ?
bi, searching for violatingconstraints j is the well-known separation step incutting plane algorithms.
For the dual constraints,?j ?jai,j ?
si, the same problem is referred toas pricing.
Pricing is often framed as searching forall, or some, variables i with positive reduced costri , si?
?j ?jai,j .
Note that while these problemsare, naturally, dual to each other, they can have verydifferent flavors.
When we assess dual constraintswe need to calculate a cost si for variable i, andusually this cost would be different for different i.For primal constraints the corresponding right-hand-sides are usually much more homogenous.736Algorithm 1 Parse, Price and Cut.Require: Initial candidate edges and hyperedges P .Ensure: The optimal z.1: repeat2: z,?
?
parse(P )3: N ?
price(?
)4: M ?
cut(z)5: P ?
P ?N ?M6: until N = ?
?M = ?7: return zThe reduced cost ri = si ?
?j ?jai,j has sev-eral interesting interpretations.
First, intuitively itmeasures the score we could gain by setting zi = 1,and subtracts an estimate of what we would loosebecause zi = 1 may compete with other variablesfor shared resources (constraints).
Second, it cor-responds to the coefficient of zi in the LagrangianL (?, z) , s?z + ?
[b?Az].
For any ?, Uzi=k =maxz?0,zi=k L (?, z) is an upper bound on the bestpossible primal objective with zi = k. This meansthat ri = Uzi=1 ?
Uzi=0 is the difference betweenan upper bound that considers zi = 1, and one thatconsiders zi = 0.
The tighter the bound Uzi=0 is,the closer ri is to an upper bound on the maximalincrease we can get for setting zi to 1.
At conver-gence of column generation, complementary slack-ness guarantees that Uzi=0 is tight for all z?i = 0, andhence ri is a true an upper bound.4.2 Application to Dependency ParsingThe grandparent formulation in section 3.2 has a cu-bic number of variables z?g,p,c?
as well as a cubicnumber of constraints.
For longer sentences thisnumber can slow us down in two ways.
First, theoptimizer works with a large search space, and willnaturally become slower.
Second, for every grand-parent edge we need to calculate the score s?g,p,c?,and this calculation can often be a major bottleneck,in particular when using complex feature functions.To overcome this bottleneck, our parse, price and cutalgorithm, as shown in algorithm 1, uses column androw generation.
In particular, it lazily instantiatesthe grandparent edge variables zgp?g,p,c?, and the cor-responding cubic number of constraints.
All unin-stantiated variables are implicitly set to 0.The algorithm requires some initial set of vari-ables to start with.
In our case this set P contains allfirst-order edges ?h,m?
in the candidate set C, andfor each of these one grandparent edge ?0, h,m?.The primary purpose of these grandparent edges isto ensure feasibility of the OneGP constraints.In step 2, the algorithm parses with the currentset of candidates P by solving the corresponding LPrelaxation.
The LP contains all columns and con-straints that involve the edges and grandparent edgesof P .
The solver returns both the best primal solu-tion z (for both edges and grandparents), and a com-plementary dual solution ?.In step 3 the dual variables?
are used to find unin-stantiated grandparent edges ?g, p, c?
with positivereduced cost.
The price routine returns such edgesin N .
In step 4 the primal solution is inspected forviolations of constraint ArcGP.
The cut routine per-forms this operation, and returns M , the set of edges?g, p, c?
that violate ArcGP.In step 5 the algorithm converges if no more con-straint violations, or promising new columns, canbe found.
If there have been violations (M 6= ?
)or promising columns (N 6= ?
), steps 2 to 4 arerepeated, with the newly found parts added to theproblem.
Note that LP solvers can be efficientlywarm-started after columns and rows have beenadded, and hence the cost of calls to the solver instep 2 is substantially reduced after the first itera-tion.4.3 PricingIn the pricing step we need to efficiently find aset of grandparent edge variables zgp?g,p,c?
with posi-tive reduced cost, or the empty set if no such vari-ables exist.
Let ?OneGP?p,c?
be the dual variables forthe OneGP constraints and ?NoGP?g,p?
the duals for con-straints NoGP.
Then for the reduced cost of zgp?g,p,c?we know that:r?g,p,c?
= s?g,p,c?
?
?OneGP?p,c?
?
?NoGP?g,p?
.
(8)Notice that the duals for the remaining two con-straints ArcGP and GPArc do not appear in thisequation.
This is valid because we can safely settheir duals to zero without violating dual feasibilityor complementary slackness of the solution returnedby the solver.7374.3.1 Upper Bounds for Efficient PricingA naive pricing implementation would exhaus-tively iterate over all ?g, p, c?
and evaluate r?g,p,c?for each.
In this case we can still substantially re-duce the number of grandparent variables that en-ter the LP, provided many of these variables havenon-positive reduced cost.
However, we still need tocalculate the score s?g,p,c?
for each ?g, p, c?, an ex-pensive operation we hope to avoid.
In the follow-ing we present an upper bound on the reduced cost,r?gp?g,p,c?
?
rgp?g,p,c?, which decomposes in a way thatallows for more efficient search.
Using this bound,we find all new grandparent edges N?
for which thisupper bound is positive:N?
?
{?g, p, c?
|r?gp?g,p,c?
> 0}.
(9)Next we prune away all but the grandparent edgesfor which the exact reduced cost is positive:N ?
N?
\ {e : rgpe > 0} .
(10)Our bound r?gp?g,p,c?
on the reduced cost of ?g, p, c?is based on an upper bound s?gp?g,p,??
?
maxc sgp?g,p,c?on the grandparent score involving ?g, p?
as grand-parent and parent, and the bound s?gp??,p,c?
?maxg sgp?g,p,c?
on the grandparent score involving?p, c?
as parent and child.
Concretely, we haver?gp?g,p,c?
, min(s?gp?g,p,?
?, s?gp??,p,c?)?
?OneGP?p,c?
?
?NoGP?g,p?
.
(11)To find edges ?g, p, c?
for which this bound ispositive, we can filter out all edges ?p, c?
such thatsgp??,p,c??
?OneGP?p,c?
is non-positive.
This is possible be-cause NoGP is a?
constraint and therefore ?NoGP?g,p?
?0.2 Hence r?gp?g,p,c?
is at most s?gp??,p,c?
?
?OneGP?p,c?
.
Thisfiltering step cuts off a substantial number of edges,and is the main reason why can avoid scoring alledges.Next we filter, for each remaining ?p, c?, all pos-sible grandparents g according to the definition ofr?gp?g,p,c?.
This again allows us to avoid calling the2Notice that in section 4.1 we discussed the LP dual incase were all constraints are inequalities.
When equality con-straints are used, the corresponding dual variables have no signconstraints.
Hence we could not make the same argument for?OneGP?p,c?
.grandparent scoring function on ?g, p, c?, and yieldsthe candidate set N?
.
Only if r?gp?g,p,c?
is positive do wehave to evaluate the exact reduced cost and score.4.3.2 Upper Bounds on ScoresWhat remains to be done is the calculation of up-per bounds s?gp?g,p,??
and s?gp??,p,c?.
Our bounds factorinto per-template bounds according to the definitionsin section 2.3.
In particular, we haves?gp??,p,c?
,?t?T gps?gp,t??,p,c?
(12)where s?t??,p,c?
is a per-template upper bound definedass?gp,t??,p,c?
, maxv?range(ht)e?range`dt?w>t ft (v, htp, htc, e).
(13)That is, we maximize over all possible attribute val-ues v any token g could have, and any possible rela-tion e a token g can have to p and c.Notice that these bounds can be calculated offline,and hence amortize after deployment of the parser.4.3.3 Tightening DualsTo price variables, we use the duals returned bythe solver.
This is a valid default strategy, but maylead to ?
with overcautious reduced costs.
Note,however, that we can arbitrary alter ?
to minimizereduced costs of uninstantiated variables, as long aswe ensure that feasibility and complementary slack-ness are maintained for the instantiated problem.We use this flexibility for increasing ?OneGP?p,c?
, andhence lowering reduced costs zgp?g,p,c?
for all tokens c.Assume that z?p,c?
= 0 and let r?p,c?
= ?OneGP?p,c?
+ Kbe the current reduced cost for z?p,c?
in the instanti-ated problem.
Here K is a value depending on s?p,c?and the remaining constraints z?p,c?
is involved in.We know that r?p,c?
?
0 due to dual feasibilityand hence r?p,c?
may be 0, but note that r?p,c?
< 0 inmany cases.
In such cases we can increase ?OneGP?p,c?to ?K and get r?p,c?
= 0.
With respect to z?p,c?
thismaintains dual feasibility (because r?p,c?
?
0) andcomplementary slackness (because z?p,c?
= 0).
Fur-thermore, with respect to the zgp?g,p,c?
for all tokens cthis also maintains feasibility (because the increased?OneGP?p,c?
appears with negative sign in 8) and com-plementary slackness (because zgp?g,p,c?
= 0 due toz?p,c?
= 0).7384.4 SeparationWhat happens if both z?g,p?
and z?p,c?
are activewhile zgp?g,p,c?
is still implicitly set to 0?
In this casewe violate constraint ArcGP.
We could remedy thisby adding the cut z?g,p?
+ z?p,c?
?
1, resolve theLP, and then use the dual variable corresponding tothis constraint to get an updated reduced cost r?g,p,c?.However, in practice we found this does not happenas often, and when it does, it is cheaper for us to addthe corresponding column r?g,p,c?
right away insteadof waiting to the next iteration to price it.To find all pairs of variables for z?g,p?
+ z?p,c?
?
1is violated, we first filter out all edges ?h,m?
forwhich z?h,m?
= 0 as these automatically satisfyany ArcGP constraint they appear in.
Now for eachz?g,p?
> 0 all z?p,c?
> 0 are found, and if their sumis larger than 1, the corresponding grandparent edge?g, p, c?
is returned in the result set.4.5 Column Generation in Dynamic ProgramsColumn and Row Generation can substantially re-duce the runtime of an off-the-shelf LP solver, aswe will find in section 6.
Perhaps somewhat sur-prisingly, it can also be applied in the context of dy-namic programs.
It is well known that for each dy-namic program there is an equivalent polynomial LPformulation (Martin et al1990).
Roughly speak-ing, in this formulation primal variables correspondto state transitions, and dual variables to value func-tions (e.g., the forward scores in the Viterbi algo-rithm).In pilot studies we have already used DCG tospeed up (exact) Viterbi on linear chains (Belangeret al2012).
We believe it could be equally appliedto dynamic programs for higher order dependencyparsing.5 Related WorkOur work is most similar in spirit to the relaxationmethod presented by Riedel and Smith (2010) thatincrementally adds second order edges to a graphi-cal model based on a gain measure?the analog ofour reduced cost.
However, they always score everyhigher order edge, and also provide no certificates ofoptimality.Several works in parsing, and in MAP inferencein general, perform some variant of row genera-tion (Riedel and Clarke, 2006; Tromble and Eis-ner, 2006; Sontag and Jaakkola, 2007; Sontag et al2008).
However, none of the corresponding methodslazily add columns, too.
The cutting plane methodof Riedel (2008) can omit columns, but only if theircoefficient is negative.
By using the notion of re-duced costs we can also omit columns with positivecoefficient.
Niepert (2010) applies column gener-ation, but his method is limited to the case of k-Bounded MAP Inference.Several ILP and LP formulations of dependencyparsing have been proposed.
Our formulation is in-spired by Martins et al2009), and hence uses fewerconstraints than Riedel and Clarke (2006).
For thecase of grandparent edges, our formulation also im-proves upon the outer bound of Martins et al2009)in terms of speed, tightness, and utility for columngeneration.
Other recent LP relaxations are basedon dual decomposition (Rush et al2010; Koo etal., 2010; Martins et al2011).
These relaxationsallow the practitioner to utilize tailor-made dynamicprograms for tractable substructure, but still everyedge needs to be scored.
Given that column gener-ation can also be applied in dynamic programs (seesection 4.5), our algorithm could in fact acceleratedual decomposition parsing as well.Pruning methods are a major part of many struc-tured prediction algorithms in general, and of pars-ing algorithms in particular (Charniak and Johnson,2005; Martins et al2009; Koo and Collins, 2010;Rush and Petrov, 2012).
Generally these meth-ods follow a coarse-to-fine scheme in which sim-pler models filter out large fractions of edges.
Suchmethods are effective, but require tuning of thresh-old parameters, training of additional models, andgenerally lead to more complex pipelines that areharder to analyze and have fewer theoretical guar-antees.A* search (Ahuja et al1993) has been usedto search for optimal parse trees, for example byKlein and Manning (2003) or, for dependency pars-ing, by Dienes et al2003).
There is a direct rela-tion between both A* and Column Generation basedon an LP formulation of the shortest path problem.Roughly speaking, in this formulation any feasibledual assignments correspond to a consistent (andthus admissible) heuristic, and the corresponding re-duced costs can be used as edge weights.
Run-739ning Dijkstra?s algorithm with these weights thenamounts to A*.
Column generation for the shortestpath problem can then be understood as a method tolazily construct a consistent heuristic.
In every stepthis method finds edges for which consistency is vi-olated, and updates the heuristic such that all theseedges are consistent.6 ExperimentsWe claim that LP relaxations for higher order pars-ing can be solved without considering, and scoring,all candidate higher order edges.
In practice, howmany grandparent edges do we need to score, andhow many do we need to add to the optimizationproblem?
And what kind of reduction in runtimedoes this reduction in edges lead to?We have also pointed out that our outer bound onthe grandparent polytope of legal edge and grand-parent vectors is tighter than the one presented byMartins et al2009).
What effect does this boundhave on the number of fractional solutions and theoverall accuracy?To answer these questions we will focus on a setof non-projective grandparent models, but point outthat our method and formulation can be easily ex-tended to projective parsing as well as other typesof higher order edges.
We use the Danish test dataof Buchholz and Marsi (2006) and the Italian andHungarian test datasets of Nivre et al2007).6.1 Impact of Price and CutTable 1 compares brute force optimization (BF) withthe full model, in spirit of Martins et al2009),to running parse, price and cut (PPC) on the samemodel.
This model contains all constraints presentedin 3.2.
The table shows the average number ofparsed sentences per second, the average objective,number of grandparent edges scored and added, allrelative to the brute force approach.
We also presentthe average unlabeled accuracy, and the percentageof sentences with integer solutions.
This numbershows us how often we not only found the optimalsolution to the LP relaxation, but also the optimalsolution to the full ILP.We first note that both systems achieve the sameobjective, and therefore, also the same accuracy.This is expected, given that column and row gen-eration are known to yield optimal solutions.
Nextwe see that the number of grandparent edges scoredand added to the problem is reduced to 5?13% of thefull model.
This leads to up to 760% improvementin speed.
This improvement comes for free, withoutany sacrifice in optimality or guarantees.
We alsonotice that in all cases at least 97% of the sentenceshave no fractional solutions, and are therefore opti-mal even with respect to the ILP.
Table 1 also showsthat our bounds on reduced costs are relatively tight.For example, in the case of Italian we score onlyone percent more grandparent edges than we actu-ally need to add.Our fastest PCC parser processes about one sen-tence per second.
This speed falls below the reportednumbers of Martins et al2009) of about 0.6 sec-onds per sentence.
Crucially, however, in contrast totheir work, our speed is achieved without any first-order pruning.
In addition, we expect further im-provements in runtime by optimizing the implemen-tation of our pricing algorithm.6.2 Tighter Grandparent PolytopeTo investigate how the additional grandparent con-straints in section 3.2 help, we compare three mod-els, this time without PPC.
The first model followsMartins et al2009) and uses constraints ArcGP andGPArc only.
The second model uses only constraintsOneGP and NoGP.
The final model incorporates allfour constraints.Table 2 shows speed relative to the baseline modelwith constraints ArcGP and GPArc, as well as thepercentage of integer solutions and the average un-labeled accuracy?all for the Italian and Hungariandatasets.
We notice that the full model has less frac-tional solutions than the partial models, and eithersubstantially (Italian) or slightly (Hungarian) fasterruntimes than ArcGP+GPArc.
Interestingly, bothsets of constraints in isolation perform worse, in par-ticular the OneGP and NoGP model.7 ConclusionWe have presented a novel method for parsing insecond order grandparent models, and a generalblueprint for more efficient and optimal structuredprediction.
Our method lazily instantiates candidateparts based on their reduced cost, and on constraint740Italian Hungarian DanishBF PPC BF PPC BF PPCSent./sec.
relative to BF 100% 760% 100% 380% 100% 390%GPs Scored relative to BF 100% 6% 100% 12% 100% 13%GPs Added relative to BF 100% 5% 100% 7% 100% 7%Objective rel.
to BF 100% 100% 100% 100% 100% 100%% of Integer Solutions 98% 98% 97% 97% 97% 97%Unlabeled Acc.
88% 88% 81% 81% 88% 88%Table 1: Parse, Price and Cut (PPC) vs Brute Force (BF).
Speed is the number of sentences per second,relative to the speed of BF.
Objective, GPs scored and added are also relative to BF.GPArc+ OneGP+Constraints ArcGP NoGP AllSent./sec.
100% 1000% 1200%% Integer 77% 9% 98%Unlabeled Acc.
87% 85% 88%(a) ItalianGPArc+ OneGP+Constraints ArcGP NoGP AllSent./sec.
100% 162% 105%% Integer 71% 3% 97%Unlabeled Acc.
80% 77% 81%(b) HungarianTable 2: Different outer bounds on the grandpar-ent polytope, for nonprojective parsing of Italian andDanish.violations.
This allows us to discard a large fractionof parts during both scoring and optimization, lead-ing to nearly 800% speed-ups without loss of accu-racy and certificates.
We also present a tighter boundon the grandparent polytope that is useful in its ownright.Delayed column and row generation is very usefulwhen solving large LPs with off-the-shelf solvers.Given the multitude of work in NLP that uses LPsand ILPs in this way (Roth and Yih, 2004; Clarkeand Lapata, 2007), we hope that our approach willprove itself useful for other applications.
We stressthat this approach can also be used when workingwith dynamic programs, as pointed out in section4.5, and therefore also in the context of dual de-composition.
This suggests even wider applicabil-ity, and usefulness in various structured predictionproblems.The underlying paradigm could also be useful formore approximate methods.
In this paradigm, al-gorithms maintain an estimate of the cost of certainresources (duals), and use these estimates to guidesearch and the propose new structures.
For exam-ple, a local-search based dependency parser couldestimate how contested certain tokens, or edges, are,and then use these estimates to choose better nextproposals.
The notion of reduced cost can give guid-ance on what such estimates should look like.AcknowledgementsThis work was supported in part by the Center forIntelligent Information Retrieval and the Univer-sity of Massachusetts and in part by UPenn NSFmedium IIS-0803847.
We gratefully acknowledgethe support of Defense Advanced Research ProjectsAgency (DARPA) Machine Reading Program underAir Force Research Laboratory (AFRL) prime con-tract no.
FA8750-09-C-0181.
Any opinions, find-ings, and conclusion or recommendations expressedin this material are those of the authors and do notnecessarily reflect the view of DARPA, AFRL, orthe US government.ReferencesRavindra K. Ahuja, Thomas L. Magnanti, and James B.Orlin.
1993.
Network Flows: Theory, Algorithms, andApplications.
Prentice Hall, 1 edition, February.David Belanger, Alexandre Passos, Sebastian Riedel, andAndrew McCallum.
2012.
A column generation ap-proach to connecting regularization and map infer-ence.
In Inferning: Interactions between Inferenceand Learning, ICML 2012 Workshop.741Dimitri P. Bertsekas.
1999.
Nonlinear Programming.Athena Scientific, 2nd edition, September.Sabine Buchholz and Erwin Marsi.
2006.
Conll-x sharedtask on multilingual dependency parsing.
In Proceed-ings of the 10th Conference on Computational Natu-ral Language Learning (CoNLL?
06), CoNLL-X ?06,pages 149?164, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative rerank-ing.
In Proceedings of the 43rd Annual Meeting of theAssociation for Computational Linguistics (ACL ?05),pages 173?180.James Clarke and Mirella Lapata.
2007.
Modellingcompression with discourse constraints.
In Proceed-ings of the 2007 Joint Conference on Empirical Meth-ods in Natural Language Processing and Computa-tional Natural Language Learning (EMNLP-CoNLL?07), pages 1?11.A.
Culotta, M. Wick, R. Hall, and A. McCallum.
2007.First-order probabilistic models for coreference reso-lution.
In Joint Human Language Technology Con-ference/Annual Meeting of the North American Chap-ter of the Association for Computational Linguistics(HLT-NAACL ?07), pages 81?88.Peter Dienes, Alexander Koller, and Marco Kuhlmann.2003.
Statistical a-star dependency parsing.
In Pro-ceedings of the workshop on Prospects and Advancesof the Syntax/Semantics Interface, Nancy, 2003, pp.85-89.P.C.
Gilmore and R.E.
Gomory.
1961.
A linear program-ming approach to the cutting-stock problem.
Opera-tions research, pages 849?859.Dan Klein and Christopher D. Manning.
2003.
A* pars-ing: Fast exact viterbi parse selection.
In Proceedingsof the 41st Annual Meeting of the Association for Com-putational Linguistics (ACL ?03), pages 119?126.Terry Koo and Michael Collins.
2010.
Efficient third-order dependency parsers.
In Proceedings of the 48thAnnual Meeting of the Association for ComputationalLinguistics (ACL ?11).Terry Koo, Alexander M. Rush, Michael Collins, TommiJaakkola, and David Sontag.
2010.
Dual decomposi-tion for parsing with nonprojective head automata.
InProceedings of the Conference on Empirical methodsin natural language processing (EMNLP ?10).Sandra K?bler, Ryan T. McDonald, and Joakim Nivre.2009.
Dependency Parsing.
Synthesis Lectures onHuman Language Technologies.
Morgan & ClaypoolPublishers.Marco L?bbecke and Jacques Desrosiers.
2004.
Selectedtopics in column generation.
Operations Research,53:1007?1023.R.
Kipp Martin, Ronald L. Rardin, and Brian A. Camp-bell.
1990.
Polyhedral characterization of discretedynamic programming.
Oper.
Res., 38(1):127?138,February.Andr?
F. T. Martins, Noah A. Smith, and Eric P. Xing.2009.
Concise integer linear programming formu-lations for dependency parsing.
In Proceedings ofthe Joint Conference of the 47th Annual Meeting ofthe ACL and the 4th International Joint Conferenceon Natural Language Processing of the AFNLP (ACL?09), pages 342?350, Morristown, NJ, USA.
Associa-tion for Computational Linguistics.Andr?
F. T. Martins, Noah A. Smith, Pedro M. Q. Aguiar,and M?rio A. T. Figueiredo.
2011.
Dual decomposi-tion with many overlapping components.
In Proceed-ings of the Conference on Empirical methods in natu-ral language processing (EMNLP ?11), EMNLP ?11,pages 238?249, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.R.
McDonald and F. Pereira.
2006.
Online learningof approximate dependency parsing algorithms.
InProceedings of the 11th Conference of the EuropeanChapter of the ACL (EACL ?06), pages 81?88.R.
McDonald, K. Crammer, and F. Pereira.
2005a.
On-line large-margin training of dependency parsers.
InProceedings of the 43rd Annual Meeting of the Associ-ation for Computational Linguistics (ACL ?05), pages91?98.R.
McDonald, F. Pereira, K. Ribarov, and J. Hajic.
2005b.Non-projective dependency parsing using spanningtree algorithms.
In HLT-EMNLP, 2005.Mathias Niepert.
2010.
A delayed column generationstrategy for exact k-bounded map inference in markovlogic networks.
In Proceedings of the 26th AnnualConference on Uncertainty in AI (UAI ?10), pages384?391, Corvallis, Oregon.
AUAI Press.J.
Nivre, J.
Hall, S. Kubler, R. McDonald, J. Nilsson,S.
Riedel, and D. Yuret.
2007.
The conll 2007 sharedtask on dependency parsing.
In Conference on Em-pirical Methods in Natural Language Processing andNatural Language Learning, pages 915?932.Hoifung Poon and Pedro Domingos.
2007.
Joint infer-ence in information extraction.
In Proceedings of the22nd AAAI Conference on Artificial Intelligence (AAAI?07), pages 913?918.Sebastian Riedel and James Clarke.
2006.
Incremen-tal integer linear programming for non-projective de-pendency parsing.
In Proceedings of the Conferenceon Empirical methods in natural language processing(EMNLP ?06), pages 129?137.Sebastian Riedel and David A. Smith.
2010.
Relaxedmarginal inference and its application to dependency742parsing.
In Joint Human Language Technology Con-ference/Annual Meeting of the North American Chap-ter of the Association for Computational Linguistics(HLT-NAACL ?10), pages 760?768, Los Angeles, Cal-ifornia, June.
Association for Computational Linguis-tics.Sebastian Riedel.
2008.
Improving the accuracy and ef-ficiency of MAP inference for markov logic.
In Pro-ceedings of the 24th Annual Conference on Uncer-tainty in AI (UAI ?08), pages 468?475.D.
Roth andW.
Yih.
2004.
A linear programming formu-lation for global inference in natural language tasks.
InProceedings of the 8th Conference on ComputationalNatural Language Learning (CoNLL?
04), pages 1?8.Alexander Rush and Slav Petrov.
2012.
Vine pruning forefficient multi-pass dependency parsing.
In Joint Hu-man Language Technology Conference/Annual Meet-ing of the North American Chapter of the Associationfor Computational Linguistics (HLT-NAACL ?12).Alexander M. Rush, David Sontag, Michael Collins, andTommi Jaakkola.
2010.
On dual decompositionand linear programming relaxations for natural lan-guage processing.
In Proceedings of the Conferenceon Empirical methods in natural language processing(EMNLP ?10).David A. Smith and Jason Eisner.
2008.
Dependencyparsing by belief propagation.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 145?156, Hon-olulu, October.D.
Sontag and T. Jaakkola.
2007.
New outer bounds onthe marginal polytope.
In Advances in Neural Infor-mation Processing Systems (NIPS ?07), pages 1393?1400.David Sontag, T. Meltzer, A. Globerson, T. Jaakkola, andY.
Weiss.
2008.
Tightening LP relaxations for MAPusing message passing.
In Proceedings of the 24th An-nual Conference on Uncertainty in AI (UAI ?08).Roy W. Tromble and Jason Eisner.
2006.
A fastfinite-state relaxation method for enforcing global con-straints on sequence decoding.
In Joint Human Lan-guage Technology Conference/Annual Meeting of theNorth American Chapter of the Association for Com-putational Linguistics (HLT-NAACL ?06), pages 423?430.743
