Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 44?48,Baltimore, Maryland, USA, June 26, 2014. c?2014 Association for Computational LinguisticsOptimizing Features in Active Machine Learning for ComplexQualitative Content AnalysisJasy Liew Suet YanSchool of Information StudiesSyracuse University, USAjliewsue@syr.eduNancy McCrackenSchool of Information StudiesSyracuse University, USAnjmccrac@syr.eduShichun ZhouCollege of Engineering andComputer ScienceSyracuse University, USAszhou02@syr.eduKevin CrowstonNational ScienceFoundation, USAcrowston@syr.eduAbstractWe propose a semi-automatic approach forcontent analysis that leverages machine learn-ing (ML) being initially trained on a small setof hand-coded data to perform a first pass incoding, and then have human annotators cor-rect machine annotations in order to producemore examples to retrain the existing modelincrementally for better performance.
In this?active learning?
approach, it is equally im-portant to optimize the creation of the initialML model given less training data so that themodel is able to capture most if not all posi-tive examples, and filter out as many negativeexamples as possible for human annotators tocorrect.
This paper reports our attempt to op-timize the initial ML model through featureexploration in a complex content analysisproject that uses a multidimensional codingscheme, and contains codes with sparse posi-tive examples.
While different codes respondoptimally to different combinations of fea-tures, we show that it is possible to create anoptimal initial ML model using only a singlecombination of features for codes with atleast 100 positive examples in the gold stand-ard corpus.1 IntroductionContent analysis, a technique for finding evi-dence of concepts of theoretical interest throughtext, is an increasingly popular technique socialscientists use in their research investigations.
Inthe process commonly known as ?coding?, socialscientists often have to painstakingly combthrough large quantities of natural language cor-pora to annotate text segments (e.g., phrase, sen-tence, and paragraphs) with codes exhibiting theconcepts of interest (Miles & Huberman, 1994).Analyzing textual data is very labor-intensive,time-consuming, and is often limited to the capa-bilities of individual researchers (W. Evans,1996).
The coding process becomes even moredemanding as the complexity of the project in-creases especially in the case of attempting toapply a multidimensional coding scheme with asignificant number of codes (D?nmez, Ros?,Stegmann, Weinberger, & Fischer, 2005).With the proliferation and availability of dig-ital texts, it is challenging, if not impossible, forhuman coders to manually analyze torrents oftext to help advance social scientists?
understand-ing of the practices of different populations ofinterest through textual data.
Therefore, compu-tational methods offer significant benefits to helpaugment human capabilities to explore massiveamounts of text in more complex ways for theorygeneration and theory testing.
Content analysiscan be framed as a text classification problem,where each text segment is labeled based on apredetermined set of categories or codes.Full automation of content analysis is still farfrom being perfect (Grimmer & Stewart, 2013).The accuracy of current automatic approaches onthe best performing codes in social science re-search ranges from 60-90% (Broadwell et al.,2013; Crowston, Allen, & Heckman, 2012; M.Evans, McIntosh, Lin, & Cates, 2007; Ishita,Oard, Fleischmann, Cheng, & Templeton, 2010;Zhu, Kraut, Wang, & Kittur, 2011).
While thepotential of automatic content analysis is promis-ing, computational methods should not beviewed as a replacement for the role of the pri-mary researcher in the careful interpretation oftext.
Rather, the computers?
pattern recognitioncapabilities can be leveraged to seek out the mostlikely examples for each code of interest, thusreducing the amount of texts researchers have toread and process.We propose a semi-automatic method thatpromotes a close human-computer partnershipfor content analysis.
Machine learning (ML) isused to perform the first pass of coding on theunlabeled texts.
Human annotators then have tocorrect only what the ML model identifies aspositive examples of each code.
The initial ML44model needs to learn only from a small set ofhand-coded examples (i.e., gold standard data),and will evolve and improve as machine annota-tions that are verified by human annotators areused to incrementally retrain the model.
In con-trast to conventional machine learning, this ?ac-tive learning?
approach will significantly reducethe amount of training data needed upfront fromthe human annotators.
However, it is still equallyimportant to optimize the creation of the initialML model given less training data so that themodel is able to capture most if not all positiveexamples, and filter out as many negative exam-ples as possible for human annotators to correct.To effectively implement the active learningapproach for coding qualitative data, we have tofirst understand the nature and complexity ofcontent analysis projects in social science re-search.
Our pilot case study, an investigation ofleadership behaviors exhibited in emails from aFLOSS development project (Misiolek, Crow-ston, & Seymour, 2012), reveals that it is com-mon for researchers to use a multidimensionalcoding scheme consisting of a significant numberof codes in their research inquiry.
Previous workhas shown that not all dimensions in a multidi-mensional coding scheme could be applied fullyautomatically with acceptable level of accuracy(D?nmez et al., 2005) but little is known if it ispossible at all to train an optimal model for allcodes using the same combination of features.Also, the distribution of codes is often times un-even with some rarely occurring codes havingonly few positive examples in the gold standardcorpus.This paper presents our attempt in optimiz-ing the initial ML model through feature explora-tion using gold standard data created from a mul-tidimensional coding scheme, including codesthat suffer from sparseness of positive examples.Specifically, our study is guided by two researchquestions:a) How can features for an initial machinelearning model be optimized for all codes ina text classification problem based on multi-dimensional coding schemes?
Is it possibleto train a one-size-fits-all model for all codesusing a single combination of features?b) Are certain features better suited for codeswith sparse positive examples?2 Machine Learning ExperimentsTo optimize the initial machine learning model,we systematically ran multiple experiments usinga gold standard corpus of emails from afree/libre/open-source software (FLOSS) devel-opment project coded for leadership behaviors(Misiolek et al., 2012).
The coding scheme con-tained six dimensions: 1) social/relationship, 2)task process, 3) task substance, 4) dual processand substance, 5) change behaviors, and 6) net-working.
The number of codes for each dimen-sion ranged from 1 to 14.
There were a total of35 codes in the coding scheme.
Each sentencecould be assigned more than one code.
Framingthe problem as a multi-label classification task,we trained a binary classification model for eachcode using support vector machine (SVM) withten-fold cross-validation.
This gold standard cor-pus consisted of 3,728 hand-coded sentencesfrom 408 email messages.For the active learning setup, we tune the ini-tial ML model for high recall since having theannotators pick out positive examples that havebeen incorrectly classified by the model is pref-erable to missing machine-annotated positiveexamples to be presented to human annotatorsfor verification (Liew, McCracken, & Crowston,2014).
Therefore, the initial ML model with lowprecision is acceptable.Category FeaturesContent Unigram, bigram, pruning,tagging, lowercase, stop-words, stemming, part-of-speech (POS) tagsSyntactic Token countOrthographic Capitalization of first letter ofa word, capitalization of entirewordWord list Subjectivity wordsSemantic Role of sender (software de-veloper or not)Table 1.
Features for ML model.As shown in Table 1, we have selected gen-eral candidate features that have proven to workwell across various text classification tasks, aswell as one semantic feature specific to the con-text of FLOSS development projects.
For contentfeatures, techniques that we have incorporated toreduce the feature space include pruning, substi-tuting certain tokens with more generic tags,converting all tokens to lowercase, excludingstopwords, and stemming.
Using the wrapperapproach (Kohavi & John, 1997), the same clas-sifier is used to test the prediction performanceof various feature combinations listed in Table 1.45Model SINGLE MULTIPLEMeasure Mean Recall Mean Precision Mean Recall Mean PrecisionOverallAll (35) 0.690 0.065 0.877 0.068DimensionChange (1) 0.917 0.011 1.000 0.016Dual Process andSubstance (13)0.675 0.069 0.852 0.067Networking (1) 0.546 0.010 0.843 0.020Process (3) 0.445 0.006 0.944 0.024Relationship (14) 0.742 0.083 0.872 0.089Substance (3) 0.735 0.061 0.919 0.051Table 2.
Comparison of mean recall and mean precision between SINGLE and MULTIPLE models.Figure 1.
Recall and precision for each code (grouped by dimension).3 Results and DiscussionWe ran 343 experiments with different combina-tions of the 13 features in Table 1.
We first com-pare the performance of the best one-size-fits-allinitial machine learning model that produces thehighest recall using a single combination of fea-tures for all codes (SINGLE) with an ?ensemble?model that uses different combinations of fea-tures to produce the highest recall for each code(MULTIPLE).
The SINGLE model combinescontent (unigram + bigram + POS tags + lower-case + stopwords) with syntactic, orthographic,and semantic features.
None of the best featurecombination for each code in the MULTIPLEmodel coincides with the feature combination inthe SINGLE model.
For example, the best fea-ture combination for code ?Phatics/Salutations?consists of only 2 out of the 13 features (unigram+ bigram).The best feature combination for each codein the MULTIPLE model varies with only someregularity noted in a few codes within the Dualand Substance dimensions.
However, these pat-terns are not consistent across all codes in a sin-gle dimension indicating that the pertinent lin-guistic features for codes belonging to the samedimension may differ despite their conceptualsimilarities, and even fitting an optimal modelfor all codes within a single dimension mayprove to be difficult especially when the distribu-tion of codes is uneven, and positive examplesfor certain codes are sparse.
There are also noconsistent feature patterns observed from thecodes with sparse positive examples in theMULTIPLE model.00.20.40.60.81External MonitoringInformingIssue DirectiveCorrectionOffer/Provide AssistanceApprovalRequest/InviteCommit/Assume ResponsibilityConfirm/ClarifyObjection/DisagreementQuery/QuestionUpdateSuggest/RecommendExplanationNetworking/Boundary SpanningRemindProcedureScheduleCriticismProactive InformingApologyConsultingHumorAppreciationSelf-disclosureVocativeAgreementEmotional ExpressionPhatics/SalutationsInclusive ReferenceOpinion/PreferenceAcronym/JargonGenerate NewIdeaEvaluation/FeedbackProvide InformationBest Recall (MULTIPLE) Best Recall (SINGLE) Precision (MULTIPLE) Precision (SINGLE)Change Dual Networking Process Relationship Substance46Figure 2.
Recall and precision for each code (sorted by gold frequency)The comparison between the two models inTable 2 further demonstrates that the MULTI-PLE model outperforms the SINGLE model bothin the overall mean recall of all 35 codes, as wellas the mean recall for each dimension.
Figure 1(codes grouped by dimensions) illustrates thatthe feature combination on the SINGLE model isill-suited for the Process codes, and half the DualProcess and Substance codes.
Recall for eachcode for the SINGLE model are mostly below orat par with the recall for each code in the MUL-TIPLE model.
Thus, creating a one-size-fits-allinitial model may not be optimal when trainingdata is limited.
Figure 2 (codes sorted based ongold frequency as shown beside the code namesin the x-axis) exhibits that the SINGLE model isable to achieve similar recall to the MULTIPLEmodel for codes with over 100 positive examplesin the training data.
Precision for these codes arealso higher compared to codes with sparse posi-tive examples.
This finding is promising becauseit implies that creating a one-size-fits-all initialML model may be possible even for a multidi-mensional coding scheme if there are more than100 positive examples for each code.4 Conclusion and Future WorkWe conclude that creating an optimal initial one-size-fits-all ML model for all codes in a multi-dimensional coding scheme using only a singlefeature combination is not possible when codeswith sparse positive examples are present, andtraining data is limited, which may be commonin real world content analysis projects in socialscience research.
However, our findings alsoshow that the potential of using a one-size-fits-allmodel increases when the size of positive exam-ples for each code in the gold standard corpus areabove 100.
For social scientists who may notpossess the technical skills needed for featureselection to optimize the initial ML model, thisdiscovery confirms that we can create a ?canned?model using a single combination of features thatwould work well in text classification for a widerange of codes with the condition that research-ers must be able to provide sufficient positiveexamples above a certain threshold to train theinitial model.
This would make the application ofmachine learning for qualitative content analysismore accessible to social scientists.The initial ML model with low precisionmeans that the model is over-predicting.
As aresult, human annotators will have to correctmore false positives in the machine annotations.For future work, we plan to experiment with dif-ferent sampling strategies to pick the most ?prof-itable?
machine annotations to be corrected byhuman annotators.
We will also work on design-ing an interactive and adaptive user interface topromote greater understanding of machine learn-ing outputs for our target users.00.20.40.60.81Remind (4)Generate NewIdea (4)Criticism(5)Proactive Informing (5)Informing (7)Procedure (7)External Monitoring (8)Issue Directive (8)Schedule (8)Apology (8)Evaluation/Feedback (9)Correction (11)Offer/Provide Assistance (11)Approval (12)Consulting (13)Request/Invite (14)Commit/Assume Responsibility (17)Networking/Boundary Spanning (18)Acronym/Jargon (19)Humor (21)Appreciation (23)Self-disclosure (37)Vocative (41)Confirm/Clarify (45)Objection/Disagreement (51)Agreement (67)Emotional Expression (108)Query/Question (115)Phatics/Salutations (116)Update (119)Suggest/Recommend (138)Inclusive Reference (146)Opinion/Preference (215)Provide Information (312)Explanation (327)Best Recall (MULTIPLE) Best Recall (SINGLE) Precision (MULTIPLE) Precision (SINGLE)47AcknowledgmentsThis material is based upon work supportedby the National Science Foundation under GrantNo.
IIS-1111107.
Kevin Crowston is supportedby the National Science Foundation.
Any opin-ions, findings, and conclusions or recommenda-tions expressed in this material are those of theauthor(s) and do not necessarily reflect the viewsof the National Science Foundation.
The authorswish to thank Janet Marsden for assisting withthe feature testing experiments, and gratefullyacknowledge helpful suggestions by the review-ers.ReferencesBroadwell, G. A., Stromer-Galley, J., Strzalkowski,T., Shaikh, S., Taylor, S., Liu, T., Boz, U., Elia,A., Jiao, L., Webb, N. (2013).
Modeling Soci-ocultural phenomena in discourse.
Natural Lan-guage Engineering, 19(02), 213?257.Crowston, K., Allen, E. E., & Heckman, R. (2012).Using natural language processing technology forqualitative data analysis.
International Journal ofSocial Research Methodology, 15(6), 523?543.D?nmez, P., Ros?, C., Stegmann, K., Weinberger, A.,& Fischer, F. (2005).
Supporting CSCL with au-tomatic corpus analysis technology.
In Proceed-ings of 2005 Conference on Computer Supportfor Collaborative Learning (pp.
125?134).Evans, M., McIntosh, W., Lin, J., & Cates, C. (2007).Recounting the courts?
Applying automated con-tent analysis to enhance empirical legal research.Journal of Empirical Legal Studies, 4(4), 1007?1039.Evans, W. (1996).
Computer-supported content anal-ysis: Trends, tools, and techniques.
Social Sci-ence Computer Review, 14(3), 269?279.Grimmer, J., & Stewart, B. M. (2013).
Text as data:The promise and pitfalls of automatic contentanalysis methods for political texts.
PoliticalAnalysis, 21(3), 267?297.Ishita, E., Oard, D. W., Fleischmann, K. R., Cheng,A.-S., & Templeton, T. C. (2010).
Investigatingmulti-label classification for human values.
Pro-ceedings of the American Society for InformationScience and Technology, 47(1), 1?4.Liew, J. S. Y., McCracken, N., & Crowston, K.(2014).
Semi-automatic content analysis of quali-tative data.
In iConference 2014 Proceedings (pp.1128?1132).Miles, M. B., & Huberman, A. M. (1994).
Qualitativedata analysis: An expanded sourcebook (2nded.).
Sage.Misiolek, N., Crowston, K., & Seymour, J.
(2012).Team dynamics in long-standing technology-supported virtual teams.
Presented at the Acade-my of Management Annual Meeting, Organiza-tional Behavior Division, Boston, MA.Zhu, H., Kraut, R. E., Wang, Y.-C., & Kittur, A.(2011).
Identifying shared leadership in Wikipe-dia.
In Proceedings of the SIGCHI Conference onHuman Factors in Computing Systems (pp.3431?3434).
New York, NY, USA.48
