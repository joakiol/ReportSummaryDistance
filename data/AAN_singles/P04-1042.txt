Deep dependencies from context-free statistical parsers: correcting thesurface dependency approximationRoger LevyDepartment of LinguisticsStanford Universityrog@stanford.eduChristopher D. ManningDepartments of Computer Science and LinguisticsStanford Universitymanning@cs.stanford.eduAbstractWe present a linguistically-motivated algorithm for recon-structing nonlocal dependency in broad-coverage context-freeparse trees derived from treebanks.
We use an algorithm basedon loglinear classifiers to augment and reshape context-freetrees so as to reintroduce underlying nonlocal dependencieslost in the context-free approximation.
We find that our algo-rithm compares favorably with prior work on English using anexisting evaluation metric, and also introduce and argue for anew dependency-based evaluation metric.
By this new eval-uation metric our algorithm achieves 60% error reduction ongold-standard input trees and 5% error reduction on state-of-the-art machine-parsed input trees, when compared with thebest previous work.
We also present the first results on non-local dependency reconstruction for a language other than En-glish, comparing performance on English and German.
Ournew evaluation metric quantitatively corroborates the intuitionthat in a language with freer word order, the surface dependen-cies in context-free parse trees are a poorer approximation tounderlying dependency structure.1 IntroductionWhile parsers are been used for other purposes, theprimary motivation for syntactic parsing is as anaid to semantic interpretation, in pursuit of broadergoals of natural language understanding.
Propo-nents of traditional ?deep?
or ?precise?
approachesto syntax, such as GB, CCG, HPSG, LFG, or TAG,have argued that sophisticated grammatical for-malisms are essential to resolving various hidden re-lationships such as the source phrase of moved wh-phrases in questions and relativizations, or the con-troller of clauses without an overt subject.
Knowl-edge of these hidden relationships is in turn es-sential to semantic interpretation of the kind prac-ticed in the semantic parsing (Gildea and Jurafsky,2002) and QA (Pasca and Harabagiu, 2001) litera-tures.
However, work in statistical parsing has forthe most part put these needs aside, being content torecover surface context-free (CF) phrase structuretrees.
This perhaps reflects the fact that context-freephrase structure grammar (CFG) is in some senseat the the heart of the majority of both formal andcomputational syntactic research.
Although, uponintroducing it, Chomsky (1956) rejected CFG as anadequate framework for natural language descrip-tion, the majority of work in the last half centuryhas used context-free structural descriptions and re-lated methodologies in one form or another as animportant component of syntactic analysis.
CFGsseem adequate to weakly generate almost all com-mon natural language structures, and also facilitatea transparent predicate-argument and/or semanticinterpretation for the more basic ones (Gazdar et al,1985).
Nevertheless, despite their success in pro-viding surface phrase structure analyses, if statisti-cal parsers and the representations they produce donot provide a useful stepping stone to recovering thehidden relationships, they will ultimately come tobe seen as a dead end, and work will necessarily re-turn to using richer formalisms.In this paper we attempt to establish to what de-gree current statistical parsers are a useful step inanalysis by examining the performance of furtherstatistical classifiers on non-local dependency re-covery from CF parse trees.
The natural isomor-phism from CF trees to dependency trees inducesonly local dependencies, derived from the head-sister relation in a CF local tree.
However, if theoutput of a context-free parser can be algorithmi-cally augmented to accurately identify and incor-porate nonlocal dependencies, then we can say thatthe context-free parsing model is a safe approxima-tion to the true task of dependency reconstruction.We investigate the safeness of this approximation,devising an algorithm to reconstruct non-local de-pendencies from context-free parse trees using log-linear classifiers, tested on treebanks of not only En-glish but also German, a language with much freerword order and correspondingly more discontinuitythan English.
This algorithm can be used as an in-termediate step between the surface output trees ofmodern statistical parsers and semantic interpreta-tion systems for a variety of tasks.11Many linguistic and technical intricacies are involved inthe interpretation and use of non-local annotation structurefound in treebanks.
A more complete exposition of the workpresented here can be found in Levy (2004).SNP-3NNPFarmersVPVBDwasADJPJJquickS*ICH*-2NPNNyesterdayS-2NP*-3VPTOtoVPVBpointPRTRPoutNPNPDTtheNNproblemsSBARWHNP-10SNPPRPitVPVBZseesNP*T*-1..Figure 1: Example of empty and nonlocal annota-tions from the Penn Treebank of English, includingnull complementizers (0), relativization (*T*-1), right-extraposition (*ICH*-2), and syntactic control (*-3).1.1 Previous WorkPrevious work on nonlocal dependency has focusedentirely on English, despite the disparity in type andfrequency of various non-local dependency con-structions for varying languages (Kruijff, 2002).Collins (1999)?s Model 3 investigated GPSG-styletrace threading for resolving nonlocal relative pro-noun dependencies.
Johnson (2002) was the firstpost-processing approach to non-local dependencyrecovery, using a simple pattern-matching algorithmon context-free trees.
Dienes and Dubey (2003a,b)and Dienes (2003) approached the problem by pre-identifying empty categories using an HMM on un-parsed strings and threaded the identified emptiesinto the category structure of a context-free parser,finding that this method compared favorably withboth Collins?
and Johnson?s.
Traditional LFG pars-ing, in both non-stochastic (Kaplan and Maxwell,1993) and stochastic (Riezler et al, 2002; Kaplanet al, 2004) incarnations, also divides the labor oflocal and nonlocal dependency identification intotwo phases, starting with context-free parses andcontinuing by augmentation with functional infor-mation.2 DatasetsThe datasets used for this study consist of the WallStreet Journal section of the Penn Treebank of En-glish (WSJ) and the context-free version of theNEGRA (version 2) corpus of German (Skut et al,1997b).
Full-size experiments on WSJ described inSection 4 used the standard sections 2-21 for train-ing, 24 for development, and trees whose yield isunder 100 words from section 23 for testing.
Ex-periments described in Section 4.3 used the samedevelopment and test sets but files 200-959 of WSJas a smaller training set; for NEGRA we followedDubey and Keller (2003) in using the first 18,602sentences for training, the last 1,000 for develop-ment, and the previous 1,000 for testing.
Consistentwith prior work and with common practice in statis-tical parsing, we stripped categories of all functionaltags prior to training and testing (though in severalcases this seems to have been a limiting move; seeSection 5).Nonlocal dependency annotation in Penn Tree-banks can be divided into three major types: unin-dexed empty elements, dislocations, and control.The first type consists primarily of null complemen-tizers, as exemplified in Figure 1 by the null rela-tive pronoun 0 (c.f.
aspects that it sees), and do notparticipate in (though they may mediate) nonlocaldependency.
The second type consists of a dislo-cated element coindexed with an origin site of se-mantic interpretation, as in the association in Fig-ure 1 of WHNP-1 with the direct object positionof sees (a relativization), and the association of S-2 with the ADJP quick (a right dislocation).
Thistype encompasses the classic cases of nonlocal de-pendency: topicalization, relativization, wh- move-ment, and right dislocation, as well as expletives andother instances of non-canonical argument position-ing.
The third type involves control loci in syntac-tic argument positions, sometimes coindexed withovert controllers, as in the association of the NPFarmers with the empty subject position of the S-2 node.
(An example of a control locus with nocontroller would be [S NP-* [VP Eating ice cream ]]is fun.)
Controllers are to be interpreted as syntac-tic (and possibly semantic) arguments both in theirovert position and in the position of loci they con-trol.
This type encompasses raising, control, pas-sivization, and unexpressed subjects of to- infinitiveand gerund verbs, among other constructions.2NEGRA?s original annotation is as dependencytrees with phrasal nodes, crossing branches, andno empty elements.
However, the distribution in-cludes a context-free version produced algorithmi-cally by recursively remapping discontinuous partsof nodes upward into higher phrases and markingtheir sites of origin.3 The resulting ?traces?
cor-respond roughly to a subclass of the second classof Penn Treebank empties discussed above, and in-clude wh- movement, topicalization, right extrapo-sitions from NP, expletives, and scrambling of sub-2Four of the annotation errors in WSJ lead to uninter-pretable dislocation and sharing patterns, including failure toannotate dislocations corresponding to marked origin sites, andmislabelings of control loci as origin sites of dislocation thatlead to cyclic dislocations (which are explicitly prohibited inWSJ annotation guidelines).
We corrected these errors manu-ally before model testing and training.3For a detailed description of the algorithm for creating thecontext-free version of NEGRA, see Skut et al (1997a).SVAFIN VP $, $.AP wird PP VVPP .ADV NP ADJD PROAV begonnen , VPErst ADJA NN spa?ter damit NP VZlange Zeit ART NE PTKZU VVINFden RMV zu schaffenSAP-2ADVErstnot untilNPADJAlangelongNNZeittimeADJDspa?terlaterVAFINwirdwillVP*T2* PPPROAVdamitwith it*T1*VVPPbegonnenbe begun$,,VP-1NPARTdentheNERMVRMVVZPTKZUzutoVVINFschaffenform$..?The RMV will not begin to be formed for a long time.
?Figure 2: Nonlocal dependencies via right-extraposition(*T1*) and topicalization (*T2*) in the NEGRA cor-pus of German, before (top) and after (bottom) transfor-mation to context-free form.
Dashed lines show wherenodes go as a result of remapping into context-free form.jects after other complements.
The positioning ofNEGRA?s ?traces?
inside the mother node is com-pletely algorithmic; a dislocated constituent C hasits trace at the edge of the original mother closestto C?s overt position.
Given a context-free NEGRAtree shorn of its trace/antecedent notation, however,it is far from trivial to determine which nodes aredislocated, and where they come from.
Figure 2shows an annotated sentence from the NEGRA cor-pus with discontinuities due to right extraposition(*T1*) and topicalization (*T2*), before and aftertransformation into context-free form with traces.3 AlgorithmCorresponding to the three types of empty-elementannotation found in the Penn Treebank, our algo-rithm divides the process of CF tree enhancementinto three phases.
Each phase involves the identifi-cation of a certain subset of tree nodes to be oper-ated on, followed by the application of the appro-priate operation to the node.
Operations may in-volve the insertion of a category at some positionamong a node?s daughters; the marking of certainnodes as dislocated; or the relocation of dislocatednodes to other positions within the tree.
The contentand ordering of phases is consistent with the syntac-tic theory upon which treebank annotation is based.For example, WSJ annotates relative clauses lackingovert relative pronouns, such as the SBAR in Fig-ure 1, with a trace in the relativization site whoseantecedent is an empty relative pronoun.
This re-quires that empty relative pronoun insertion precededislocated element identification.
Likewise, dislo-cated elements can serve as controllers of controlloci, based on their originating site, so it is sensibleto return dislocated nodes to their originating sitesbefore identifying control loci and their controllers.For WSJ, the three phases are:1.
(a) Determine nodes at which to insert nullCOMPlementizers4 (IDENTNULL)(b) For each COMP insertion node, determineposition of each insertion and insert COMP(INSERTNULL)2.
(a) Classify each tree node as +/- DISLOCATED(IDENTMOVED)(b) For each DISLOCATED node, choose an ORI-GIN node (RELOCMOVED)(c) For each pair ?DISLOCATED,origin?, choosea position of insertion and insert dislocated(INSERTRELOC)3.
(a) Classify each node as +/- control LOCUS(IDENTLOCUS)(b) For each LOCUS, determine position of inser-tion and insert LOCUS (INSERTLOCUS)(c) For each LOCUS, determine CONTROLLER (ifany) (FINDCONTROLLER)Note in particular that phase 2 involves the classifi-cation of overt tree nodes as dislocated, followedby the identification of an origin site (annotatedin the treebank as an empty node) for each dislo-cated element; whereas phase 3 involves the iden-tification of (empty) control loci first, and of con-trollers later.
This approach contrasts with John-son (2002), who treats empty/antecedent identifi-cation as a joint task, and with Dienes and Dubey(2003a,b), who always identify empties first and de-termine antecedents later.
Our motivation is that itshould generally be easier to determine whether anovert element is dislocated than whether a given po-sition is the origin of some yet unknown dislocatedelement (particularly in the absence of a sophisti-cated model of argument expression); but controlloci are highly predictable from local context, suchas the subjectless non-finite S in Figure 1?s S-2.5 In-deed this difference seems to be implicit in the non-local feature templates used by Dienes and Dubey(2003a,b) in their empty element tagger, in partic-ular lookback for wh- words preceding a candidateverb.As described in Section 2, NEGRA?s nonlocalannotation schema is much simpler, involving no4The WSJ contains a number of SBARs headed by emptycomplementizers with trace S?s.
These SBARs are introducedin our algorithm as projections of identified empty complemen-tizers as daughters of non-SBAR categories.5Additionally, whereas dislocated nodes are always overt,control loci may be controlled by other (null) control loci,meaning that identifying controllers before control loci wouldstill entail looking for nulls.IDENTMOVED SNP?it/there?
VPS/SBARExpletive dislocationIDENTLOCUS SVP?
?VP-internal contextto determine nullsubjecthoodINSERTNULLS S VP Possible null com-plementizer (recordssyntactic path fromevery S in sentence)Figure 3: Different classifiers?
specialized tree-matchingfragments and their purposesuncoindexed empties or control loci.
Correspond-ingly, our NEGRA algorithm includes only phase2 of the WSJ algorithm, step (c) of which is trivialfor NEGRA due to the deterministic positioning oftrace insertion in the treebank.In each case we use a loglinear model for nodeclassification, with a combination of quadratic reg-ularization and thresholding by individual featurecount to prevent overfitting.
In the second and thirdparts of phases 2 and 3, when determining an orig-inating site or controller for a given node N, oran insertion position for a node N?
in N, we use acompetition-based setting, using a binary classifica-tion (yes/no for association with N) on each node inthe tree, and during testing choosing the node withthe highest score for positive association with N.6All other phases of classification involve indepen-dent decisions at each node.
In phase 3, we includea special zero node to indicate a control locus withno antecedent.3.1 Feature templatesEach subphase of our dependency reconstruction al-gorithm involves the training of a separate modeland the development of a separate feature set.
Wefound that it was important to include both a varietyof general feature templates and a number of manu-ally designed, specialized features to resolve spe-cific problems observed for individual classifiers.We developed all feature templates exclusively onthe training and development sets specified in Sec-tion 2.Table 1 shows which general feature templateswe used in each classifier.
The features are6The choice of a unique origin site makes our algorithm un-able to deal with right-node raising or parasitic gaps.
Casesof right-node raising could be automatically transformed intosingle-origin dislocations by making use of a theory of coordi-nation such as Maxwell and Manning (1996), while parasiticgaps could be handled with the introduction of a secondaryclassifier.
Both phenomena are low-frequency, however, andwe ignore them here.Feature type IdentNullInsertNullIdentMovedRelocMovedInsertRelocIdentLocusInsertLocusFindControllerTAG X XHD XCAT?MCAT ?
XCAT?MCAT?GCAT X X XCAT?HD?MCAT?MHD ?CAT?TAG?MCAT?MTAG ?CAT?TAG X XCAT?HD ?
(FIRST/LAST)CAT X X(L/RSIS)CAT X XDPOS?CAT XPATH X XCAT?RCAT XTAG?RCAT XCAT?TAG?RCAT XCAT?RCAT?DPOS XHD?RHD ?CAT?HD?RHD XCAT?DCAT X X X XMHD?HD ?# Special 9 0 11 0 0 12 0 3Table 1: Shared feature templates.
See text for templatedescriptions.
# Special is the number of special templatesused for the classifier.
?
denotes that all subsets of thetemplate conjunction were included.coded as follows.
The prefixes {?,M,G,D,R} in-dicate that the feature value is calculated with re-spect to the node in question, its mother, grand-mother, daughter, or relative node respectively.7{CAT,POS,TAG,WORD} stand for syntactic cate-gory, position (of daughter) in mother, head tag, andhead word respectively.
For example, when deter-mining whether an infinitival VP is extraposed, suchas S-2 in Figure 1, the plausibility of the VP headbeing a deep dependent of the head verb is capturedwith the MHD?HD template.
(FIRST/LAST)CATand (L/RSIS)CAT are templates used for choosingthe position to insert insert relocated nodes, respec-tively recording whether a node of a given categoryis the first/last daughter, and the syntactic categoryof a node?s left/right sisters.
PATH is the syntac-tic path between relative and base node, defined asthe list of the syntactic categories on the (inclusive)node path linking the relative node to the node inquestion, paired with whether the step on the pathwas upward or downward.
For example, in Figure2 the syntactic path from VP-1 to PP is [?-VP,?-S,?-VP,?-PP].
This is a crucial feature for the rel-ativized classifiers RELOCATEMOVED and FIND-CONTROLLER; in an abstract sense it mediates thegap-threading information incorporated into GPSG-7The relative node is DISLOCATED in RELOCMOVED andLOCUS in FINDCONTROLLER.Gold trees Parser outputJn Pres Jn DD PresNP-* 62.4 75.3 55.6 (69.5) 61.1WH-t 85.1 67.6 80.0 (82.0) 63.30 89.3 99.6 77.1 (48.8) 87.0SBAR 74.8 74.7 71.0 73.8 71.0S-t 90 93.3 87 84.5 83.6Table 2: Comparison with previous work using John-son?s PARSEVAL metric.
Jn is Johnson (2002); DD isDienes and Dubey (2003b); Pres is the present work.style (Gazdar et al, 1985) parsers, and in concreteterms it closely matches the information derivedfrom Johnson (2002)?s connected local tree set pat-terns.
Gildea and Jurafsky (2002) is to our knowl-edge the first use of such a feature for classificationtasks on syntactic trees; they found it important forthe related task of semantic role identification.We expressed specialized hand-coded featuretemplates as tree-matching patterns that capture afragment of the content of the pattern in the fea-ture value.
Representative examples appear in Fig-ure 3.
The italicized node is the node for whicha given feature is recorded; underscores indi-cate variables that can match any category; and theangle-bracketed parts of the tree fragment, togetherwith an index for the pattern, determine the featurevalue.84 Evaluation4.1 Comparison with previous workOur algorithm?s performance can be compared withthe work of Johnson (2002) and Dienes and Dubey(2003a) on WSJ.
Valid comparisons exist for theinsertion of uncoindexed empty nodes (COMP andARB-SUBJ), identification of control and raisingloci (CONTROLLOCUS), and pairings of dislo-cated and controller/raised nodes with their origins(DISLOC,CONTROLLER).
In Table 2 we presentcomparative results, using the PARSEVAL-basedevaluation metric introduced by Johnson (2002) ?
acorrect empty category inference requires the stringposition of the empty category, combined with theleft and right boundaries plus syntactic category ofthe antecedent, if any, for purposes of compari-son.9,10 Note that this evaluation metric does not re-quire correct attachment of the empty category into8A complete description of feature templates can be foundat http://nlp.stanford.edu/?rog/acl2004/templates/index.html9For purposes of comparability with Johnson (2002) weused Charniak?s 2000 parser as P .10Our algorithm was evaluated on a more stringent standardfor NP-* than in previous work: control loci-related mappingswere done after dislocated nodes were actually relocated by thealgorithm, so an incorrect dislocation remapping can render in-correct the indices of a correct NP-* labeled bracketing.
Addi-tionally, our algorithm does not distinguish the syntactic cate-PCF P A ?
P J ?
P D G A ?G J ?GOverall 91.2 87.6 90.5 90.0 88.3 95.7 99.4 98.5NP 91.6 89.9 91.4 91.2 89.4 97.9 99.8 99.6S 93.3 83.4 91.2 89.9 89.2 89.0 98.0 96.0VP 91.2 87.3 90.2 89.6 88.0 95.2 99.0 97.7ADJP 73.1 72.8 72.9 72.8 72.5 99.7 99.6 98.8SBAR 94.4 66.7 89.3 84.9 85.0 72.6 99.4 94.1ADVP 70.1 69.7 69.5 69.7 67.7 99.4 99.4 99.7Table 3: Typed dependency F1 performance when com-posed with statistical parser.
PCF is parser output eval-uated by context-free (shallow) dependencies; all oth-ers are evaluated on deep dependencies.
P is parser, Gis string-to-context-free-gold-tree mapping, A is presentremapping algorithm, J is Johnson 2002, D is the COM-BINED model of Dienes 2003.the parse tree.
In Figure 1, for example, WHNP-1 could be erroneously remapped to the right edgeof any S or VP node in the sentence without result-ing in error according to this metric.
We thereforeabandon this metric in further evaluations as it isnot clear whether it adequately approximates perfor-mance in predicate-argument structure recovery.114.2 Composition with a context-free parserIf we think of a statistical parser as a function fromstrings to CF trees, and the nonlocal dependencyrecovery algorithm A presented in this paper as afunction from trees to trees, we can naturally com-pose our algorithm with a parser P to form a func-tion A ?
P from strings to trees whose dependencyinterpretation is, hopefully, an improvement overthe trees from P .To test this idea quantitatively we evaluate perfor-mance with respect to recovery of typed dependencyrelations between words.
A dependency relation,commonly employed for evaluation in the statisticalparsing literature, is defined at a node N of a lexi-calized parse tree as a pair ?wi, wj?
where wi is thelexical head of N and wj is the lexical head of somenon-head daughter of N. Dependency relations mayfurther be typed according to information at or nearthe relevant tree node; Collins (1999), for exam-ple, reports dependency scores typed on the syn-tactic categories of the mother, head daughter, anddependent daughter, plus on whether the dependentprecedes or follows the head.
We present here de-pendency evaluations where the gold-standard de-pendency set is defined by the remapped tree, typedgory of null insertions, whereas previous work has; as a result,the null complementizer class 0 and WH-t dislocation class areaggregates of classes used in previous work.11Collins (1999) reports 93.8%/90.1% precision/recall in hisModel 3 for accurate identification of relativization site in non-infinitival relative clauses.
This figure is difficult to comparedirectly with other figures in this section; a tree search indi-cates that non-infinitival subjects make up at most 85.4% of theWHNP dislocations in WSJ.Performance on gold trees Performance on parsed treesID Rel Combo ID ComboP R F1 Acc P R F1 P R F1 P R F1WSJ(full) 92.0 82.9 87.2 95.0 89.6 80.1 84.6 34.5 47.6 40.0 17.8 24.3 20.5WSJ(sm) 92.3 79.5 85.5 93.3 90.4 77.2 83.2 38.0 47.3 42.1 19.7 24.3 21.7NEGRA 73.9 64.6 69.0 85.1 63.3 55.4 59.1 48.3 39.7 43.6 20.9 17.2 18.9Table 4: Cross-linguistic comparison of dislocated node identification and remapping.
ID is correct identificationof nodes as +/?
dislocated; Rel is relocation of node to correct mother given gold-standard data on which nodes aredislocated (only applicable for gold trees); Combo is both correct identification and remapping.by syntactic category of the mother node.12 In Fig-ure 1, for example, to would be an ADJP dependentof quick rather than a VP dependent of was; andFarmers would be an S dependent both of to in topoint out .
.
.
and of was.
We use the head-findingrules of Collins (1999) to lexicalize trees, and as-sume that null complementizers do not participatein dependency relations.
To further compare the re-sults of our algorithm with previous work, we ob-tained the output trees produced by Johnson (2002)and Dienes (2003) and evaluated them on typed de-pendency performance.
Table 3 shows the results ofthis evaluation.
For comparison, we include shal-low dependency accuracy for Charniak?s parser un-der PCF.4.3 Cross-linguistic comparisonIn order to compare the results of nonlocal depen-dency reconstruction between languages, we mustidentify equivalence classes of nonlocal dependencyannotation between treebanks.
NEGRA?s nonlocaldependency annotation is quite different from WSJ,as described in Section 2, ignoring controlled andarbitrary unexpressed subjects.
The natural basisof comparison is therefore the set of all nonlocalNEGRA annotations against all WSJ dislocations,excluding relativizations (defined simply as dislo-cated wh- constituents under SBAR).13Table 4 shows the performance comparison be-tween WSJ and NEGRA of IDENTDISLOC and RE-LOCMOVED, on sentences of 40 tokens or less.For this evaluation metric we use syntactic cate-gory and left & right edges of (1) dislocated nodes(ID); and (2) originating mother node to which dis-located node is mapped (Rel).
Combo requires both(1) and (2) to be correct.
NEGRA is smaller thanWSJ (?350,000 words vs. 1 million), so for fair12Unfortunately, 46 WSJ dislocation annotations in this test-set involve dislocated nodes dominating their origin sites.
Itis not entirely clear how to interpret the intended semantics ofthese examples, so we ignore them in evaluation.13The interpretation of comparative results must be modu-lated by the fact that more total time was spent on feature en-gineering for WSJ than for NEGRA, and the first author, whoengineered the NEGRA feature set, is not a native speaker ofGerman.comparison we tested WSJ using the smaller train-ing set described in Section 2, comparable in sizeto NEGRA?s.
Since the positioning of traces withinNEGRA nodes is trivial, we evaluate remapping andcombination performances requiring only proper se-lection of the originating mother node; thus wecarry the algorithm out on both treebanks throughstep (2b).
This is adequate for purposes of ourtyped dependency evaluation in Section 4.2, sincetyped dependencies do not depend on positional in-formation.
State-of-the-art statistical parsing is farbetter on WSJ (Charniak, 2000) than on NEGRA(Dubey and Keller, 2003), so for comparison ofparser-composed dependency performance we usedvanilla PCFG models for both WSJ and NEGRAtrained on comparably-sized datasets; in addition tomaking similar types of independence assumptions,these models performed relatively comparably onlabeled bracketing measures for our developmentsets (73.2% performance for WSJ versus 70.9% forNEGRA).Table 5 compares the testset performance of al-gorithms on the two treebanks on the typed depen-dency measure introduced in Section 4.2.145 DiscussionThe WSJ results shown in Tables 2 and 3 suggestthat discriminative models incorporating both non-local and local lexical and syntactic information canachieve good results on the task of non-local depen-dency identification.
On the PARSEVAL metric,our algorithm performed particularly well on nullcomplementizer and control locus insertion, and onS node relocation.
In particular, Johnson noted thatthe proper insertion of control loci was a difficultissue involving lexical as well as structural sensitiv-ity.
We found the loglinear paradigm a good onein which to model this feature combination; whenrun in isolation on gold-standard development trees,our model reached 96.4% F1 on control locus inser-tion, reducing error over the Johnson model?s 89.3%14Many head-dependent relations in NEGRA are explicitlymarked, but for those that are not we used a Collins (1999)-style head-finding algorithm independently developed for Ger-man PCFG parsing.PCF P A ?
P G A ?GWSJ(full) 76.3 75.4 75.7 98.7 99.7WSJ(sm) 76.3 75.4 75.7 98.7 99.6NEGRA 62.0 59.3 61.0 90.9 93.6Table 5: Typed dependency F1 performance when com-posed with statistical parser.
Remapped dependenciesinvolve only non-relativization dislocations and excludecontrol loci.by nearly two-thirds.
The performance of our algo-rithm is also evident in the substantial contributionto typed dependency accuracy seen in Table 3.
Forgold-standard input trees, our algorithm reduces er-ror by over 80% from the surface-dependency base-line, and over 60% compared with Johnson?s re-sults.
For parsed input trees, our algorithm reducesdependency error by 23% over the baseline, and by5% compared with Johnson?s results.
Note that thedependency figures of Dienes lag behind even theparsed results for Johnson?s model; this may wellbe due to the fact that Dienes built his model asan extension of Collins (1999), which lags behindCharniak (2000) by about 1.3-1.5%.Manual investigation of errors on English gold-standard data revealed two major issues that suggestfurther potential for improvement in performancewithout further increase in algorithmic complexityor training set size.
First, we noted that annotationinconsistency accounted for a large number of er-rors, particularly false positives.
VPs from which anS has been extracted ([SShut up,] he [VP said t]) areinconsistently given an empty SBAR daughter, sug-gesting the cross-model low-70?s performance onnull SBAR insertion models (see Table 2) may bea ceiling.
Control loci were often under-annotated;the first five development-set false positive controlloci we checked were all due to annotation error.And why-WHADVPs under SBAR, which are al-ways dislocations, were not so annotated 20% of thetime.
Second, both control locus insertion and dis-located NP remapping must be sensitive to the pres-ence of argument NPs under classified nodes.
Buttemporal NPs, indistinguishable by gross category,also appear under such nodes, creating a major con-found.
We used customized features to compensateto some extent, but temporal annotation already ex-ists in WSJ and could be used.
We note that Kleinand Manning (2003) independently found retentionof temporal NP marking useful for PCFG parsing.As can be seen in Table 3, the absolute improve-ment in dependency recovery is smaller for bothour and Johnson?s postprocessing algorithms whenapplied to parsed input trees than when applied togold-standard input trees.
It seems that this degra-dation is not primarily due to noise in parse tree out-puts reducing recall of nonlocal dependency iden-tification: precision/recall splits were largely thesame between gold and parsed data, and manualinspection revealed that incorrect nonlocal depen-dency choices often arose from syntactically rea-sonable yet incorrect input from the parser.
Forexample, the gold-standard parse right-wing whites.
.
.
will [VP step up [NP their threats [S [VP * to takematters into their own hands ]]]] has an unindexedcontrol locus because Treebank annotation specifiesthat infinitival VPs inside NPs are not assigned con-trollers.
Charniak?s parser, however, attaches the in-finitival VP into the higher step up .
.
.
VP.
InfinitivalVPs inside VPs generally do receive controllers fortheir null subjects, and our algorithm accordinglyyet mistakenly assigns right-wing-whites as the an-tecedent.The English/German comparison shown in Ta-bles 4 and 5 is suggestive, but caution is necessaryin its interpretation due to the fact that differencesin both language structure and treebank annotationmay be involved.
Results in the G column of Ta-ble 5, showing the accuracy of the context-free de-pendency approximation from gold-standard parsetrees, quantitatively corroborates the intuition thatnonlocal dependency is more prominent in Germanthan in English.Manual investigation of errors made on Germangold-standard data revealed two major sources of er-ror beyond sparsity.
The first was a widespread am-biguity of S and VP nodes within S and VP nodes;many true dislocations of all sorts are expressed atthe S and VP levels in CFG parse trees, such as VP-1 of Figure 2, but many adverbial and subordinatephrases of S or VP category are genuine dependentsof the main clausal verb.
We were able to find anumber of features to distinguish some cases, suchas the presence of certain unambiguous relative-clause introducing complementizers beginning an Snode, but much ambiguity remained.
The secondwas the ambiguity that some matrix S-initial NPsare actually dependents of the VP head (in thesecases, NEGRA annotates the finite verb as the headof S and the non-finite verb as the head of VP).
Thisis not necessarily a genuine discontinuity per se,but rather corresponds to identification of the sub-ject NP in a clause.
Obviously, having access toreliable case marking would improve performancein this area; such information is in fact included inNEGRA?s morphological annotation, another argu-ment for the utility of involving enhanced annota-tion in CF parsing.As can be seen in the right half of Table 4, per-formance falls off considerably on vanilla PCFG-parsed data.
This fall-off seems more dramatic thanthat seen in Sections 4.1 and 4.2, no doubt partlydue to the poorer performance of the vanilla PCFG,but likely also because only non-relativization dis-locations are considered in Section 4.3.
These dis-locations often require non-local information (suchas identity of surface lexical governor) for identifi-cation and are thus especially susceptible to degra-dation in parsed data.
Nevertheless, seemingly dis-mal performance here still provided a strong boostto typed dependency evaluation of parsed data, asseen in A ?
P of Table 5.
We suspect this indicatesthat dislocated terminals are being usefully iden-tified and mapped back to their proper governors,even if the syntactic projections of these terminalsand governors are not being correctly identified bythe parser.6 Further WorkAgainst the background of CFG as the standardapproximation of dependency structure for broad-coverage parsing, there are essentially three op-tions for the recovery of nonlocal dependency.
Thefirst option is to postprocess CF parse trees, whichwe have closely investigated in this paper.
Thesecond is to incorporate nonlocal dependency in-formation into the category structure of CF trees.This was the approach taken by Dienes and Dubey(2003a,b) and Dienes (2003); it is also practicedin recent work on broad-coverage CCG parsing(Hockenmaier, 2003).
The third would be to in-corporate nonlocal dependency information into theedge structure parse trees, allowing discontinuousconstituency to be explicitly represented in the parsechart.
This approach was tentatively investigatedby Plaehn (2000).
As the syntactic diversity oflanguages for which treebanks are available grows,it will become increasingly important to comparethese three approaches.7 AcknowledgementsThis work has benefited from feedback from DanJurafsky and three anonymous reviewers, and frompresentation at the Institute of Cognitive Science,University of Colorado at Boulder.
The au-thors are also grateful to Dan Klein and JennyFinkel for use of maximum-entropy software theywrote.
This work was supported in part bythe Advanced Research and Development Activity(ARDA)?s Advanced Question Answering for Intel-ligence (AQUAINT) Program.ReferencesCharniak, E. (2000).
A Maximum-Entropy-inspired parser.
InProceedings of NAACL.Chomsky, N. (1956).
Three models for the description of lan-guage.
IRE Transactions on Information Theory, 2(3):113?124.Collins, M. (1999).
Head-Driven Statistical Models for NaturalLanguage Parsing.
PhD thesis, University of Pennsylvania.Dienes, P. (2003).
Statistical Parsing with Non-local Depen-dencies.
PhD thesis, Saarland University.Dienes, P. and Dubey, A.
(2003a).
Antecedent recovery: Ex-periments with a trace tagger.
In Proceedings of EMNLP.Dienes, P. and Dubey, A.
(2003b).
Deep processing by com-bining shallow methods.
In Proceedings of ACL.Dubey, A. and Keller, F. (2003).
Parsing German with sister-head dependencies.
In Proceedings of ACL.Gazdar, G., Klein, E., Pullum, G., and Sag, I.
(1985).
General-ized Phrase Structure Grammar.
Harvard.Gildea, D. and Jurafsky, D. (2002).
Automatic labeling of se-mantic roles.
Computational Linguistics, 28(3):245?288.Hockenmaier, J.
(2003).
Data and models for Statistical Pars-ing with Combinatory Categorial Grammar.
PhD thesis,University of Edinburgh.Johnson, M. (2002).
A simple pattern-matching algorithm forrecovering empty nodes and their antecedents.
In Proceed-ings of ACL, volume 40.Kaplan, R., Riezler, S., King, T. H., Maxwell, J. T., Vasserman,A., and Crouch, R. (2004).
Speed and accuracy in shallowand deep stochastic parsing.
In Proceedings of NAACL.Kaplan, R. M. and Maxwell, J. T. (1993).
The interface be-tween phrasal and functional constraints.
ComputationalLinguistics, 19(4):571?590.Klein, D. and Manning, C. D. (2003).
Accurate unlexicalizedparsing.
In Proceedings of ACL.Kruijff, G.-J.
(2002).
Learning linearization rulesfrom treebanks.
Invited talk at the FormalGrammar?02/COLOGNET-ELSNET Symposium.Levy, R. (2004).
Probabilistic Models of Syntactic Discontinu-ity.
PhD thesis, Stanford University.
In progress.Maxwell, J. T. and Manning, C. D. (1996).
A theory of non-constituent coordination based on finite-state rules.
In Butt,M.
and King, T. H., editors, Proceedings of LFG.Pasca, M. and Harabagiu, S. M. (2001).
High performancequestion/answering.
In Proceedings of SIGIR.Plaehn, O.
(2000).
Computing the most probable parse for adiscontinuous phrase structure grammar.
In Proceedings ofIWPT, Trento, Italy.Riezler, S., King, T. H., Kaplan, R. M., Crouch, R. S., Maxwell,J.
T., and Johnson, M. (2002).
Parsing the Wall Street Jour-nal using a Lexical-Functional Grammar and discriminativeestimation techniques.
In Proceedings of ACL, pages 271?278.Skut, W., Brants, T., Krenn, B., and Uszkoreit, H. (1997a).Annotating unrestricted German text.
In Fachtagung derSektion Computerlinguistik der Deutschen Gesellschaft frSprachwissenschaft, Heidelberg, Germany.Skut, W., Krenn, B., Brants, T., and Uszkoreit, H. (1997b).
Anannotation scheme for free word order languages.
In Pro-ceedings of ANLP.
