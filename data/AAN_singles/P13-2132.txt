Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 753?758,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsVariable Bit Quantisation for LSHSean MoranSchool of InformaticsThe University of EdinburghEH8 9AB, Edinburgh, UKsean.moran@ed.ac.ukVictor LavrenkoSchool of InformaticsThe University of EdinburghEH8 9AB, Edinburgh, UKvlavrenk@inf.ed.ac.ukMiles OsborneSchool of InformaticsThe University of EdinburghEH8 9AB, Edinburgh, UKmiles@inf.ed.ac.ukAbstractWe introduce a scheme for optimally al-locating a variable number of bits perLSH hyperplane.
Previous approaches as-sign a constant number of bits per hyper-plane.
This neglects the fact that a subsetof hyperplanes may be more informativethan others.
Our method, dubbed VariableBit Quantisation (VBQ), provides a data-driven non-uniform bit allocation acrosshyperplanes.
Despite only using a fractionof the available hyperplanes, VBQ outper-forms uniform quantisation by up to 168%for retrieval across standard text and imagedatasets.1 IntroductionThe task of retrieving the nearest neighbours to agiven query document permeates the field of Nat-ural Language Processing (NLP).
Nearest neigh-bour search has been used for applications as di-verse as automatically detecting document transla-tion pairs for the purposes of training a statisticalmachine translation system (SMT) (Krstovski andSmith, 2011), the large-scale generation of nounsimilarity lists (Ravichandran et al 2005) to anunsupervised method for extracting domain spe-cific lexical variants (Stephan Gouws and Metzle,2011).There are two broad approaches to nearestneighbour based search: exact and approximatetechniques, which are differentiated by their abil-ity to return completely correct nearest neighbours(the exact approach) or have some possibility ofreturning points that are not true nearest neigh-bours (the approximate approach).
Approximatenearest neighbour (ANN) search using hashingtechniques has recently gained prominence withinNLP.
The hashing-based approach maps the datainto a substantially more compact representationreferred to as a fingerprint, that is more efficientfor performing similarity computations.
The re-sulting compact binary representation radically re-duces memory requirements while also permittingfast sub-linear time retrieval of approximate near-est neighbours.Hashing-based ANN techniques generally com-prise two main steps: a projection stage followedby a quantisation stage.
The projection stageperforms a neighbourhood preserving embedding,mapping the input data into a lower-dimensionalrepresentation.
The quantisation stage subse-quently reduces the cardinality of this represen-tation by converting the real-valued projectionsto binary.
Quantisation is a lossy transformationwhich can have a significant impact on the result-ing quality of the binary encoding.Previous work has quantised each projected di-mension into a uniform number of bits (Indyk andMotwani, 1998) (Kong and Li, 2012) (Kong et al2012) (Moran et al 2013).
We demonstrate thatuniform allocation of bits is sub-optimal and pro-pose a data-driven scheme for variable bit alloca-tion.
Our approach is distinct from previous workin that it provides a general objective function forbit allocation.
VBQ makes no assumptions on thedata and, in addition to LSH, it applies to a broadrange of other projection functions.2 Related WorkLocality sensitive hashing (LSH) (Indyk and Mot-wani, 1998) is an example of an approximatenearest neighbour search technique that has beenwidely used within the field of NLP to preserve theCosine distances between documents (Charikar,2002).
LSH for cosine distance draws a largenumber of random hyperplanes within the inputfeature space, effectively dividing the space intonon-overlapping regions (or buckets).
Each hy-perplane contributes one bit to the encoding, thevalue (0 or 1) of which is determined by comput-753[a] [b] 		     					Figure 1: Left: Data points with identical shapes are 1-NN.
Two hyperplanes h1, h2 are shown alongsidetheir associated normal vectors (n1, n2).
Right top: Projection of points onto the normal vectors n1and n2 of the hyperplanes (arrows denote projections).
Right middle: Positioning of the points alongnormal vector n2.
Three quantisation thresholds (t1, t2, t3, and consequently 2 bits) can maintain theneighbourhood structure.
Right bottom: the high degree of mixing between the 1-NN means that thishyperplane (h1) is likely to have 0 bits assigned (and therefore be discarded entirely).ing the dot product of a data-point (x) with thenormal vector to the hyperplane (ni): that is, ifx.ni < 0, i ?
{1 .
.
.
k}, then the i-th bit is setto 0, and 1 otherwise.
This encoding scheme isknown as single bit quantisation (SBQ).
More re-cent hashing work has sought to inject a degreeof data-dependency into the positioning of the hy-perplanes, for example, by using the principal di-rections of the data (Wang et al 2012) (Weisset al 2008) or by training a stack of restrictedBoltzmann machines (Salakhutdinov and Hinton,2009).Existing quantisation schemes for LSH allocateeither one bit per hyperplane (Indyk and Motwani,1998) or multiple bits per hyperplane (Kong et al2012) (Kong and Li, 2012) (Moran et al 2013).For example, (Kong et al 2012) recently pro-posed the Manhattan Hashing (MQ) quantisationtechnique where each projected dimension is en-coded with multiple bits of natural binary code(NBC).
The Manhattan distance between the NBCencoded data points is then used for nearest neigh-bour search.
The authors demonstrated that MQcould better preserve the neighbourhood structurebetween the data points as compared to SBQ withHamming distance.Other recent quantisation work has focused onthe setting of the quantisation thresholds: for ex-ample (Kong and Li, 2012) suggested encodingeach dimension into two bits and using an adaptivethresholding scheme to set the threshold positions.Their technique dubbed, Double Bit Quantisation(DBQ), attempts to avoid placing thresholds be-tween data points with similar projected values.
Inother work (Moran et al 2013) demonstrated thatretrieval accuracy could be enhanced by using atopological quantisation matrix to guide the quan-tisation threshold placement along the projecteddimensions.
This topological quantisation matrixspecified pairs of ?-nearest neighbours in the orig-inal feature space.
Their approach, Neighbour-hood Preserving Quantisation (NPQ), was shownto achieve significant increases in retrieval accu-racy over SBQ,MQ and DBQ for the task of imageretrieval.
In all of these cases the bit allocation isuniform: each hyperplane is assigned an identicalnumber of bits.3 Variable Bit QuantisationOur proposed quantisation scheme, Variable BitQuantisation (VBQ), assigns a variable number ofbits to each hyperplane subject to a maximum up-per limit on the total number of bits1.
To do so,VBQ computes an F-measure based directly on thepositioning of the quantisation thresholds along aprojected dimension.
The higher the F-measurefor a given hyperplane, the better that hyperplaneis at preserving the neighbourhood structure be-tween the data points, and the more bits the hyper-plane should be afforded from the bit budget B.Figure 1(a) illustrates the original 2-dimensional feature space for a toy example.1Referred to as the bit budget B, typically 32 or 64 bits.754The space is divided into 4 buckets by tworandom LSH hyperplanes.
The circles, diamonds,squares and stars denote 1-nearest neighbours(1-NN).
Quantisation for LSH is performed byprojecting the data points onto the normal vectors(n1, n2) to the hyperplanes (h1, h2).
This leadsto two projected dimensions.
Thresholding theseprojected dimensions at zero, and determiningwhich side of zero a given data-point falls, yieldsthe bit encoding for a given data-point.Figure 1(b) demonstrates our proposed quanti-sation scheme.
Similar to vanilla LSH, the data-points are projected onto the normal vectors, toyield two projected dimensions.
This is illustratedon the topmost diagram in Figure 1(b).
VBQ dif-fers in how these projected dimensions are thresh-olded to yield the bit encoding: rather than onethreshold situated at zero, VBQ employs one ormore thresholds and positions these thresholds inan adaptive manner based upon maximisation ofan F-measure.
Using multiple thresholds enablesmore than one bit to be assigned per hyperplane2.Figure 1(b) (middle, bottom) depicts the F-measure driven threshold optimisation along theprojected dimensions.
We define as a positivepair, those pairs of data points in the original fea-ture space that are ?-nearest neighbours (?-NN),and a negative pair otherwise.
In our toy exam-ple, data points with the same shape symbol forma positive pair, while points with different sym-bols are negative pairs.
Intuitively, the thresholdsshould be positioned in such a way as to maxi-mize the number of positive pairs that fall withinthe same thresholded region, while also ensuringthe negative pairs fall into different regions.This intuition can be captured by an F-measurewhich counts the number of positive pairs that arefound within the same thresholded regions (truepositives, TP), the number of negative pairs foundwithin the same regions (false positives, FP), andthe number of positive pairs found in different re-gions of the threshold partitioned dimension (falsenegatives, FN).
For n2, three thresholds are opti-mal, given they perfectly preserve the neighbour-hood structure.
For n1, no thresholds can provide aneighbourhood preserving quantisation and there-fore it is better to discard the hyperplane h1.
VBQuses random restarts to optimise the F-measure3.The computed F-measure scores per hyper-2b bits, requires 2b ?
1 thresholds.3More details on the computation of the F-measure perhyperplane can be found in (Moran et al 2013).plane (h), per bit count (b) are an effective sig-nal for bit allocation: more informative hyper-planes tend to have higher F-measure, for higherbit counts.
VBQ applies a binary integer linearprogram (BILP) on top of the F-measure scoresto obtain the bit allocation.
To do so, the algo-rithm collates the scores in a matrix F with ele-ments Fb,h, where b ?
{0, .
.
.
, k} 4 indexes therows, with k being the maximum number of bitsallowable for any given hyperplane (set to 4 in thiswork), and h ?
{1 .
.
.
, B} indexes the columns.The BILP uses F to find the bit allocation thatmaximises the cumulative F-measure across the Bhyperplanes (Equation 1).max ?F ?
Z?subject to ?Zh?
= 1 h ?
{1 .
.
.
B}?Z ?
D?
?
BZ is binary(1)?.?
denotes the Frobenius L1 norm, ?
theHadamard product and D is a constraint matrix,with Db,h = b, ensuring that the bit allocationremains within the bit budget B.
The BILP issolved using the standard branch and bound op-timization algorithm (Land and Doig, 1960).
Theoutput from the BILP is an indicator matrix Z ?
{0, 1}(k+1)?B whose columns specify the optimalbit allocation for a given hyperplane i.e.
Zb,h = 1if the BILP decided to allocate b bits for hyper-plane h, and zero otherwise.
Example matrices forthe toy problem in Figure 1 are given hereunder (inthis example, k = 2 and B = 2).?
?F h1 h2b0 0.25 0.25b1 0.35 0.50b2 0.40 1.00???
?D0 01 12 2???
?Z1 00 00 1?
?Notice how the indicator matrix Z specifies anassignment of 0 bits for hyperplane h1 and 2 bitsfor hyperplane h2 as this yields the highest cu-mulative F-measure across hyperplanes while alsomeeting the bit budget.
VBQ is therefore a princi-pled method to select a discriminative subset ofhyperplanes, and simultaneously allocate bits tothe remaining hyperplanes, given a fixed overallbit budget B, while maximizing cumulative F-measure.4For 0 bits, we compute the F-measure without anythresholds along the projected dimension.755Dataset CIFAR-10 TDT-2 Reuters-21578SBQ MQ DBQ NPQ VBQ SBQ MQ DBQ VBQ SBQ MQ DBQ VBQSIKH 0.042 0.063 0.047 0.090 0.161 0.034 0.045 0.031 0.092 0.102 0.112 0.087 0.389LSH 0.119 0.093 0.066 0.153 0.207 0.189 0.097 0.089 0.229 0.276 0.201 0.175 0.538BLSI 0.038 0.135 0.111 0.155 0.231 0.283 0.210 0.087 0.396 0.100 0.030 0.030 0.156SH 0.051 0.135 0.111 0.167 0.202 0.146 0.212 0.167 0.370 0.033 0.028 0.030 0.154PCAH 0.036 0.137 0.107 0.153 0.219 0.281 0.208 0.094 0.374 0.095 0.034 0.027 0.154Table 1: Area under the Precision Recall curve (AUPRC) for all five projection methods.
Results are for32 bits (images) and at 128 bits (text).
The best overall score for each dataset is shown in bold face.4 Experiments4.1 DatasetsOur text datasets are Reuters-21578 and TDT-2.The original Reuters-21578 corpus contains 21578documents in 135 categories.
We use the ModApteversion and discard those documents with multi-ple category labels.
This leaves 8,293 documentsin 65 categories.
The corpus contains 18,933 dis-tinct terms.
The TDT-2 corpus consists of 11,201on-topic documents which are classified into 96semantic categories.
We remove those documentsappearing in two or more categories and keep onlythe largest 30 categories.
This leaves 9,394 docu-ments in total with 36,771 distinct terms.
Both textdatasets are TF-IDF and L2 norm weighted.
Todemonstrate the generality of VBQ we also evalu-ate on the CIFAR-10 image dataset (Krizhevsky,2009), which consists of 60,000 images repre-sented as 512 dimensional Gist descriptors (Olivaand Torralba, 2001).
All of the datasets are identi-cal to those that have been used in previous ANNhashing work (Zhang et al 2010) (Kong and Li,2012) and are publicly available on the Internet.4.2 Projection MethodsVBQ is independent of the projection stage andtherefore can be used the quantise the projectionsfrom a wide range of different projection func-tions, including LSH.
In our evaluation we takea sample of the more popular data-independent(LSH, SIKH) and data-dependent (SH, PCAH,BLSI) projection functions used in recent hashingwork:?
SIKH: Shift-Invariant Kernel Hashing(SIKH) uses random projections that approx-imate shift invariant kernels (Raginsky andLazebnik, 2009).
We follow previous workand use a Gaussian kernel with a bandwidthset to the average distance to the 50th nearestneighbour (Kong et al 2012) (Raginsky andLazebnik, 2009).?
LSH: Locality Sensitive Hashing uses aGaussian random matrix for projection (In-dyk and Motwani, 1998) (Charikar, 2002).?
BLSI: Binarised Latent Semantic Indexing(BLSI) forms projections through SingularValue Decomposition (SVD) (Salakhutdinovand Hinton, 2009).?
SH: Spectral Hashing (SH) uses the eigen-functions computed along the principal com-ponent directions of the data for projec-tion (Weiss et al 2008).?
PCAH: Principal Component AnalysisHashing (PCAH) employs the eigenvectorscorresponding the the largest eigenvalues ofthe covariance matrix for projection (Wanget al 2012).4.3 BaselinesSingle Bit Quantisation (SBQ) (Indyk and Mot-wani, 1998), Manhattan Hashing (MQ) (Kong etal., 2012), Double Bit Quantisation (DBQ) (Kongand Li, 2012) and Neighbourhood PreservingQuantisation (NPQ) (Moran et al 2013).
MQ,DBQ and NPQ all assign 2 bits per hyperplane,while SBQ assigns 1 bit per hyperplane.
All meth-ods, including VBQ, are constrained to be withinthe allocated bit budget B.
If a method assignsmore bits to one hyperplane, then it either dis-cards, or assigns less bits to other hyperplanes.4.4 Evaluation ProtocolWe adopt the standard Hamming ranking evalua-tion paradigm (Kong et al 2012).
We randomlyselect 1000 query data points per run.
Our re-sults are averaged over 10 runs, and the averagereported.
The ?-neighbours of each query point756[1]        								[3]      	[5]        	[2]        	    [4]        	    [6]        	    Figure 2: [1] LSH AUPRC vs bits for CIFAR-10 [2] LSH Precision-Recall curve for CIFAR-10 [3]LSH AUPRC vs bits for TDT-2 [4] LSH Precision-Recall curve for TDT-2 [5] LSH AUPRC vs bits forReuters-21578 [6] LSH Precision-Recall curve for Reuters-21578form the ground truth for evaluation.
The thresh-old ?
is computed by sampling 100 training data-points at random from the training dataset and de-termining the distance at which these points have50 nearest neighbours on average.
Positive pairsand negative pairs for F-measure computation arecomputed by thresholding the training datasetEuclidean distance matrix by ?.
We adopt theManhattan distance and multi-bit binary encodingmethod as suggested in (Kong et al 2012).
TheF-measure we use for threshold optimisation is:F?
= (1+?2)TP/((1+?2)TP +?2FN +FP ).We select the parameter ?
on a held-out valida-tion dataset.
The area under the precision-recallcurve (AUPRC) is used to evaluate the quality ofretrieval.4.5 ResultsTable 1 presents our results.
For LSH on text(Reuters-21578) at 128 bits we find a substantial95% gain in retrieval performance over uniformlyassigning 1 bit per hyperplane (SBQ) and a 168%gain over uniformly assigning 2 bits per hyper-plane (MQ).
VBQ gain over SBQ at 128 bits is sta-tistically significant based upon a paired Wilcoxonsigned rank test across 10 random train/test parti-tions (p-value: ?
0.0054).
This pattern is repeatedon TDT-2 (for 128 bits, SBQ vs VBQ: p-value?
0.0054) and CIFAR-10 (for 32 bits, SBQ vsVBQ: p-value: ?
0.0054).
VBQ also reaps sub-stantial gains for the Eigendecomposition basedprojections (PCAH, SH, BLSI) effectively exploit-ing the imbalanced variance across hyperplanes -that is, those hyperplanes capturing higher propor-tions of the variance in the data are allocated morebits from the fixed bit budget.
Figure 2 (top row)illustrates that VBQ is effective across a range ofbit budgets.
Figure 2 (bottom row) presents theprecision-recall (PR) curves at 32 bits (CIFAR-10)and 128 bits (TDT-2, Reuters-21578).
We confirmour hypothesis that judicious allocation of variablebits is significantly more effective than uniform al-location.5 ConclusionsOur proposed quantisation scheme computes anon-uniform bit assignment across LSH hyper-planes.
The novelty of our approach is centredupon a binary integer linear program driven by anovel F-measure based objective function that de-termines the most appropriate bit allocation: hy-perplanes that better preserve the neighbourhoodstructure of the input data points are awarded morebits from a fixed bit budget.
Our evaluation onstandard datasets demonstrated that VBQ can sub-stantially enhance the retrieval accuracy of a se-lection of popular hashing techniques across twodistinct modalities (text and images).
In this paperwe concentrated on the hamming ranking basedscenario for hashing.
In the future, we would liketo examine the performance of VBQ in the lookupbased hashing scenario where hash tables are usedfor fast retrieval.757ReferencesMoses Charikar.
2002.
Similarity estimation tech-niques from rounding algorithms.
In STOC, pages380?388.Piotr Indyk and Rajeev Motwani.
1998.
Approximatenearest neighbors: towards removing the curse of di-mensionality.
In Proceedings of the thirtieth annualACM symposium on Theory of computing, STOC?98, pages 604?613, New York, NY, USA.
ACM.Weihao Kong and Wu-Jun Li.
2012.
Double-bit quan-tization for hashing.
In AAAI.Weihao Kong, Wu-Jun Li, andMinyi Guo.
2012.
Man-hattan hashing for large-scale image retrieval.
SI-GIR ?12, pages 45?54.Alex Krizhevsky.
2009.
Learning Multiple Layers ofFeatures from Tiny Images.
Master?s thesis.Kriste Krstovski and David A. Smith.
2011.
A Mini-mally Supervised Approach for Detecting and Rank-ing Document Translation Pairs.
In Proceedings ofthe Sixth Workshop on Statistical Machine Transla-tion, Edinburgh, Scotland.
Association for Compu-tational Linguistics.A.
H. Land and A. G. Doig.
1960.
An automaticmethod of solving discrete programming problems.Econometrica, 28:pp.
497?520.Sean Moran, Victor Lavrenko, and Miles Osborne.2013.
Neighbourhood preserving quantisation forlsh.
In 36th Annual International ACM Conferenceon Research and Development in Information Re-trieval (SIGIR), Dublin, Ireland, 07/2013.Aude Oliva and Antonio Torralba.
2001.
Modeling theshape of the scene: A holistic representation of thespatial envelope.
International Journal of ComputerVision, 42(3):145?175.Maxim Raginsky and Svetlana Lazebnik.
2009.Locality-sensitive binary codes from shift-invariantkernels.
In NIPS ?09, pages 1509?1517.Deepak Ravichandran, Patrick Pantel, and EduardHovy.
2005.
Randomized algorithms and nlp: usinglocality sensitive hash function for high speed nounclustering.
ACL ?05, pages 622?629.
Associationfor Computational Linguistics.Ruslan Salakhutdinov and Geoffrey Hinton.
2009.Semantic hashing.
Int.
J. Approx.
Reasoning,50(7):969?978.Dirk Hovy Stephan Gouws and Donald Metzle.
2011.Unsupervised mining of lexical variants from noisytext.
In Proceedings of the First workshop on Unsu-pervised Learning in NLP, EMNLP ?11, page 8290.Association for Computational Linguistics.Jun Wang, S. Kumar, and Shih-Fu Chang.
2012.
Semi-supervised hashing for large-scale search.
IEEETransactions on Pattern Analysis and Machine In-telligence, 34(12):2393?2406.Yair Weiss, Antonio B. Torralba, and Robert Fergus.2008.
Spectral hashing.
In NIPS, pages 1753?1760.Dell Zhang, Jun Wang, Deng Cai, and Jinsong Lu.2010.
Self-taught hashing for fast similarity search.In SIGIR, pages 18?25.758
