Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 15?24,Baltimore, Maryland, 26-27 July 2014.c?2014 Association for Computational LinguisticsGrammatical error correction using hybrid systems and type filteringMariano Felice Zheng Yuan ?istein E. AndersenHelen Yannakoudakis Ekaterina KochmarComputer Laboratory, University of Cambridge, United Kingdom{mf501,zy249,oa223,hy260,ek358}@cl.cam.ac.ukAbstractThis paper describes our submission to theCoNLL 2014 shared task on grammaticalerror correction using a hybrid approach,which includes both a rule-based and anSMT system augmented by a large web-based language model.
Furthermore, wedemonstrate that correction type estima-tion can be used to remove unnecessarycorrections, improving precision withoutharming recall.
Our best hybrid systemachieves state-of-the-art results, rankingfirst on the original test set and second onthe test set with alternative annotations.1 IntroductionGrammatical error correction has attracted con-siderable interest in the last few years, especiallythrough a series of ?shared tasks?.
These effortshave helped to provide a common ground for eval-uating and comparing systems while encouragingresearch in the field.
These shared tasks have pri-marily focused on English as a second or foreignlanguage and addressed different error types.
TheHOO 2011 task (Dale and Kilgarriff, 2011), forexample, included all error types whereas HOO2012 (Dale et al., 2012) and the CoNLL 2013shared task (Ng et al., 2013) were restricted to onlytwo and five types respectively.In this paper, we describe our submission to theCoNLL 2014 shared task (Ng et al., 2014), whichinvolves correcting all the errors in essays writ-ten in English by students at the National Univer-sity of Singapore.
An all-type task poses a greaterchallenge, since correcting open-class types (suchas spelling or collocation errors) requires differentcorrection strategies than those in closed classes(such as determiners or prepositions).In this scenario, hybrid systems or combinationsof correction modules seem more appropriate andtypically produce good results.
In fact, most ofthe participating teams in previous shared taskshave used a combination of modules or systemsfor their submissions, even for correcting closed-class types (Dahlmeier et al., 2011; Bhaskar etal., 2011; Rozovskaya et al., 2011; Ivanova et al.,2011; Rozovskaya et al., 2013; Yoshimoto et al.,2013; Xing et al., 2013; Kunchukuttan et al., 2013;Putra and Szabo, 2013; Xiang et al., 2013).In line with previous research, we present a hy-brid approach that employs a rule-based error cor-rection system and an ad-hoc statistical machinetranslation (SMT) system, as well as a large-scalelanguage model to rank alternative corrections andan error type filtering technique.The remainder of this paper is organised as fol-lows: Section 2 describes our approach and eachcomponent in detail, Section 3 presents our experi-ments using the CoNLL 2014 shared task develop-ment set and Section 4 reports our official resultson the test set.
Finally, we discuss the performanceof our system and present an error analysis in Sec-tion 5 and conclude in Section 6.2 ApproachWe tackle the error correction task using a pipelineof processes that combines results from multiplesystems.
Figure 1 shows the interaction of thecomponents in our final hybrid system, producingthe results submitted to the CoNLL 2014 sharedtask.
The following sections describe each of thesecomponents in detail.2.1 Rule-based error correction system(RBS)The rule-based system is a component of the Self-Assessment and Tutoring (SAT) system, a webservice developed at the University of Cambridgeaimed at helping intermediate learners of English15Figure 1: Overview of components and interac-tions in our final hybrid system.in their writing tasks1(Andersen et al., 2013).
Theoriginal SAT system provides three main function-alities: 1) text assessment, producing an overallscore for a piece of text, 2) sentence evaluation,producing a sentence-level quality score, and 3)word-level feedback, suggesting specific correc-tions for frequent errors.
Since the focus of theshared task is on strict correction (as opposed todetection), we only used the word-level feedbackcomponent of the SAT system.This module uses rules automatically derivedfrom the Cambridge Learner Corpus2(CLC)(Nicholls, 2003) that are aimed at detecting error-ful unigrams, bigrams and trigrams.
In order toensure high precision, rules are based on n-gramsthat have been annotated as incorrect at least fivetimes and at least ninety per cent of the timesthey occur.
In addition to these corpus-derivedrules, many cases of incorrect but plausible deriva-tional and inflectional morphology are detected bymeans of rules derived from a machine-readabledictionary.
For further details on specific compo-nents, we refer the reader to the aforementionedpaper.Given an input text, the rule-based system pro-duces an XML file containing a list of suggestedcorrections.
These corrections can either be ap-plied to the original text or used to generate mul-tiple correction candidates, as described in Sec-tion 2.3.2.2 SMT systemWe follow a similar approach to the one describedby Yuan and Felice (2013) in order to train an SMT1The latest version of the system, called ?Write& Improve?, is available at http://www.cambridgeenglish.org/writeandimprovebeta/.2More information at http://www.cambridge.org/elt/catalogue/subject/custom/item3646603/system that can ?translate?
from incorrect into cor-rect English.
Our training data comprises a set ofdifferent parallel corpora, where the original (in-correct) sentences constitute the source side andcorrected versions based on gold standard anno-tations constitute the target side.
These corporainclude:?
the NUCLE v3.1 corpus (Dahlmeier et al.,2013), containing around 1,400 essays writ-ten in English by students at the NationalUniversity of Singapore (approx.
1,220,257tokens in 57,152 sentences),?
phrase alignments involving corrections ex-tracted automatically from the NUCLE cor-pus (with up to 7 tokens per side), which areused to boost the probability of phrase align-ments that involve corrections so as to im-prove recall,?
the CoNLL 2014 shared task developmentset, containing 50 essays from the previousyear?s test set (approx.
29,207 tokens in 1,382sentences),?
the First Certificate in English (FCE) cor-pus (Yannakoudakis et al., 2011), contain-ing 1,244 exam scripts and 2 essays perscript (approx.
532,033 tokens in 16,068 sen-tences),?
a subset of the International English Lan-guage Testing System (IELTS) examinationdataset extracted from the CLC corpus, con-taining 2,498 exam scripts and 2 essays perscript (approx.
1,361,841 tokens in 64,628sentences), and?
a set of sentences from the English Vo-cabulary Profile3(EVP), which have beenmodified to include artificially generated er-rors (approx.
351,517 tokens in 18,830 sen-tences).
The original correct sentences are asubset of the CLC and come from examina-tions at different proficiency levels.
The ar-tificial error generation method aims at repli-cating frequent error patterns observed in theNUCLE corpus on error-free sentences, asdescribed by Yuan and Felice (2013).3Sentences were automatically scraped from http://www.englishprofile.org/index.php?option=com_content&view=article&id=4&Itemid=516Word alignment was carried out using pialign(Neubig et al., 2011), after we found it outper-formed GIZA++ (Och and Ney, 2000; Och andNey, 2003) and Berkeley Aligner (Liang et al.,2006; DeNero and Klein, 2007) in terms of pre-cision and F0.5on the development set.
Insteadof using heuristics to extract phrases from theword alignments learnt by GIZA++ or Berker-ley Aligner, pialign created a phrase table directlyfrom model probabilities.In addition to the features already defined by pi-align, we added character-level Levenshtein dis-tance to each mapping in the phrase table.
Thiswas done to allow for the fact that, in error correc-tion, most words translate into themselves and er-rors are often similar to their correct forms.
Equalweights were assigned to these features.We then built a lexical reordering model usingthe alignments created by pialign.
The maximumphrase length was set to 7, as recommended in theSMT literature (Koehn et al., 2003; Koehn, 2014).The IRSTLM Toolkit (Federico et al., 2008)was used to build a 4-gram target language modelwith Kneser?Ney smoothing (Kneser and Ney,1995) on the correct sentences from the NUCLE,full CLC and EVP corpora.Decoding was performed with Moses (Koehn etal., 2007), using the default settings and weights.No tuning process was applied.
The resulting sys-tem was used to produce the 10 best correctioncandidates for each sentence in the dataset, whichwere further processed by other modules.Segmentation, tokenisation and part-of-speechtagging were performed using NLTK (Bird etal., 2009) for consistency with the shared taskdatasets.2.3 Candidate generationIn order to integrate corrections from multiple sys-tems, we developed a method to generate all thepossible corrected versions of a sentence (candi-dates).
Candidates are generated by computing allpossible combinations of corrections (irrespectiveof the system from which they originate), includ-ing the original tokens to allow for a ?no correc-tion?
option.
The list of candidates produced foreach sentence always includes the original (un-modified) sentence plus any other versions derivedfrom system corrections.In order for a combination of corrections to gen-erate a valid candidate, all the corrections must beFigure 2: An example showing the candidate gen-eration process.Model CE ME UE P R F0.5SMT IRSTLM 651 2766 1832 0.2621 0.1905 0.2438Microsoft WebN-grams666 2751 1344 0.3313 0.1949 0.2907Table 1: Performance of language models on thedevelopment set after ranking the SMT system?s10-best candidates per sentence.
CE: correct ed-its, ME: missed edits, UE: unnecessary edits, P:precision, R: recall.compatible; otherwise, the candidate is discarded.We consider two or more corrections to be com-patible if they do not overlap, in an attempt toavoid introducing accidental errors.
In addition,if different correction sets produce the same can-didate, we only keep one.
Figure 2 illustrates thecandidate generation process.2.4 Language model rankingGenerated candidates are ranked using a languagemodel (LM), with the most probable candidate be-ing selected as the final corrected version.We tried two different alternatives for ranking:1) using the target LM embedded in our SMT sys-tem (described in Section 2.2) and 2) using a largen-gram LM built from web data.
In the lattercase, we used Microsoft Web N-gram Services,which provide access to large smoothed n-gramlanguage models (with n=2,3,4,5) built from webdocuments (Gao et al., 2010).
All our experimentsare based on the 5-gram ?bing-body:apr10?
model.The ranking performance of these two modelswas evaluated on the 10-best hypotheses generatedby the SMT system for each sentence in the devel-opment set.
Table 1 shows the results from theM2Scorer (Dahlmeier and Ng, 2012), the officialscorer for the shared task that, unlike previous ver-sions, weights precision twice as much as recall.Results show that using Microsoft?s Web LMyields better performance, which is unsurprisinggiven the vast amounts of data used to build that17System CE ME UE P R F0.5RBS 95 3322 107 0.4703 0.0278 0.1124SMT 452 2965 690 0.3958 0.1323 0.2830Table 2: Results of individual systems on the de-velopment set.model.
For this reason, we adopt Microsoft?smodel for all further experiments.We also note that without normalisation, higherprobabilities may be assigned to shorter sentences,which can introduce a bias towards preferringdeletions or skipping insertions.2.5 Type filteringAnalysing performance by error type is very valu-able for system development and tuning.
How-ever, this can only be performed for correctionsin the gold standard (either matched or missed).To estimate types for unnecessary corrections, wedefined a set of heuristics that analyse differencesin word forms and part-of-speech tags betweenthe original phrases and their system corrections,based on common patterns observed in the train-ing data.
We had previously used a similar strat-egy to classify errors in our CoNLL 2013 sharedtask submission (Yuan and Felice, 2013) but havenow included a few improvements and rules fornew types.
Estimation accuracy is 50.92% on thetraining set and 67.57% on the development set,which we consider to be acceptable for our pur-poses given that the final test set is more similar tothe development set.Identifying types for system corrections is notonly useful during system development but canalso be exploited to filter out and reduce the num-ber of proposed corrections.
More specifically, ifa system proposes a much higher number of un-necessary corrections than correct suggestions fora specific error type, we can assume the system isactually degrading the quality of the original text,in which case it is preferable to filter out those er-ror types.
Such decisions will lower the total num-ber of unnecessary edits, thus improving overallprecision.
However, they will also harm recall,unless the number of matched corrections for theerror type is zero (i.e.
unless Ptype= 0).
To avoidthis, only corrections for types having zero preci-sion should be removed.3 Experiments and resultsWe carried out a series of experiments on the de-velopment set using different pipelines and com-binations of systems in order to find an optimalsetting.
The following sections describe them indetail.3.1 Individual system performanceOur first set of experiments were aimed at inves-tigating individual system performance on the de-velopment set, which is reported in Table 2.
Re-sults show that the SMT system has much betterperformance, which is expected given that it hasbeen trained on texts similar to those in the testset.3.2 PipelinesSince corrections from the RBS and SMT systemsare often complementary, we set out to explorecombination schemes that would integrate correc-tions from both systems.
Table 3 shows results fordifferent combinations, where RBS and SMT in-dicate all corrections from the respective systems,subscript ?c?
indicates candidates generated froma system?s individual corrections, subscript ?10-best?
indicates the 10-best list of candidates pro-duced by the SMT system, ?>?
indicates a pipelinewhere the output of one system is the input to theother and ?+?
indicates a combination of candi-dates from different systems.
All these pipelinesuse the RBS system as the first processing step inorder to perform an initial correction, which is ex-tremely beneficial for the SMT system.Results reveal that the differences betweenthese pipelines are small in terms of F0.5, althoughthere are noticeable variations in precision and re-call.
The best results are achieved when the 10best hypotheses from the SMT system are rankedwith Microsoft?s LM, which confirms our resultsin Table 1 showing that the SMT LM is outper-formed by a larger web-based model.A simple pipeline using the RBS system firstand the SMT system second (#3) yields per-formance that is better than (or comparable to)pipelines #1, #2 and #4, suggesting that there is noreal benefit in using more sophisticated pipelineswhen only the best hypothesis from the SMT sys-tem is used.
However, performance is improvedwhen the 10 best SMT hypotheses are considered.The only difference between pipelines #5 and #6lies in the way corrections from the RBS system18# Pipeline CE ME UE P R F0.5?1 RBS > SMTc> LM 372 3045 481 0.4361 0.1088 0.27232 RBSc+ SMTc> LM 400 3017 485 0.4520 0.1171 0.28753 RBS > SMT 476 2941 738 0.3921 0.1393 0.28774 RBSc> LM > SMT 471 2946 718 0.3961 0.1378 0.28815 RBS > SMT10-best> LM 678 2739 1368 0.3314 0.1984 0.29226 RBSc> LM > SMT10-best> LM 681 2736 1366 0.3327 0.1993 0.2934Table 3: Results for different system pipelines on the development set.System CE ME UE P R F0.5RBSc> LM > SMT10-best> LM 681 2736 1366 0.3327 0.1993 0.2934RBSc> LM > SMT10-best> LM > Filter 681 2736 1350 0.3353 0.1993 0.2950Table 4: Results for individual systems on the development set.are handled.
In the first case, all corrections areapplied at once whereas in the second, the sug-gested corrections are used to generate candidatesthat are subsequently ranked by our LM, often dis-carding some of the suggested corrections.3.3 FilteringAs described in Section 2.5, we can evaluate per-formance by error type in order to identify and re-move unnecessary corrections.
In particular, wetried to optimise our best hybrid system (#6) byfiltering out types with zero precision.
Table 5shows type-specific performance for this system,where three zero-precision types can be identi-fied: Reordering (a subset of Others that we treatseparately), Srun (run-ons/comma splices) and Wa(acronyms).
Although reordering was explicitlydisabled in our SMT system, a translation tablecan still include this type of mappings if they areobserved in the training data (e.g.
?you also can??
?you can also?
).In order to remove such undesired corrections,the following procedure was applied: first, in-dividual corrections were extracted by compar-ing the original and corrected sentences; second,the type of each extracted correction was pre-dicted, subsequently deleting those that matchedunwanted types (i.e.
reordering, Srun or Wa); fi-nally, the set of remaining corrections was appliedto the original text.
This method improves pre-cision while preserving recall (see Table 4), al-though the resulting improvement is not statisti-cally significant (paired t-test, p > 0.05).4 Official evaluation resultsOur submission to the CoNLL 2014 shared task isthe result of our best hybrid system, described inthe previous section and summarised in Figure 1.The official test set comprised 50 new essays (ap-prox.
30,144 tokens in 1,312 sentences) written inresponse to two prompts, one of which was alsoincluded in the training data.Systems were evaluated using the M2Scorer,which uses F0.5as its overall measure.
As in previ-ous years, there were two evaluation rounds.
Thefirst one was based on the original gold-standardannotations provided by the shared-task organis-ers whereas the second was based on a revisedversion including alternative annotations submit-ted by the participating teams.
Our submitted sys-tem achieved the first and second place respec-tively.
The official results of our submission inboth evaluation rounds are reported in Table 6.5 Discussion and error analysisIn order to assess how our system performed pererror type on the test set, we ran our type estima-tion script and obtained the results shown in Ta-ble 7.
Although these results are estimated andtherefore not completely accurate,4they can stillprovide valuable insights, at least at a coarse level.The following sections discuss our main findings.5.1 Type performanceAccording to Table 7, our system achieves the bestperformance for types WOadv (adverb/adjectiveposition) and Wtone (tone), but these results are4Estimation accuracy was found to be 57.90% on the testset.19Error type CE ME UE P R F0.5ArtOrDet 222 465 225 0.4966 0.3231 0.4485Cit 0 6 0 ?
0.0000 ?Mec 31 151 15 0.6739 0.1703 0.4235Nn 138 256 136 0.5036 0.3503 0.4631Npos 4 25 45 0.0816 0.1379 0.0889Others 1 34 12 0.0769 0.0286 0.0575Pform 1 25 22 0.0435 0.0385 0.0424Pref 1 38 5 0.1667 0.0256 0.0794Prep 61 249 177 0.2563 0.1968 0.2417Reordering 0 1 12 0.0000 0.0000 ?Rloc- 13 115 80 0.1398 0.1016 0.1300SVA 32 86 25 0.5614 0.2712 0.4624Sfrag 0 4 0 ?
0.0000 ?Smod 0 16 0 ?
0.0000 ?Spar 4 30 0 1.0000 0.1176 0.4000Srun 0 55 28 0.0000 0.0000 ?Ssub 7 64 15 0.3182 0.0986 0.2201Trans 13 128 36 0.2653 0.0922 0.1929Um 0 34 0 ?
0.0000 ?V0 2 16 3 0.4000 0.1111 0.2632Vform 28 90 68 0.2917 0.2373 0.2789Vm 9 86 41 0.1800 0.0947 0.1525Vt 18 137 53 0.2535 0.1161 0.2050WOadv 0 12 0 ?
0.0000 ?WOinc 2 35 71 0.0274 0.0541 0.0304Wa 0 5 2 0.0000 0.0000 ?Wci 28 400 241 0.1041 0.0654 0.0931Wform 65 161 54 0.5462 0.2876 0.4630Wtone 1 12 0 1.0000 0.0769 0.2941TOTAL 681 2736 1366 0.3327 0.1993 0.2934Table 5: Type-specific performance of our best hy-brid system on the development set.
Types withzero precision are marked in bold.Test set CE ME UE P R F0.5Original 772 1793 1172 0.3971 0.3010 0.3733Revised 913 1749 1042 0.4670 0.3430 0.4355Table 6: Official results of our system on the orig-inal and revised test sets.not truly representative as they only account for asmall fraction of the test data (0.64% and 0.36%respectively).The third best performing type is Mec, whichcomprises mechanical errors (such as punctuation,capitalisation and spelling mistakes) and repre-sents 11.58% of the errors in the data.
The remark-ably high precision obtained for this error typesuggests that our system is especially suitable forcorrecting such errors.We also found that our system was particularlygood at enforcing different types of agreement, asdemonstrated by the results for SVA (subject?verbagreement), Pref (pronoun reference), Nn (nounnumber) and Vform (verb form) types, which addup to 22.80% of the errors.
The following exampleshows a successful correction:Error type CE ME UE P R F0.5ArtOrDet 185 192 206 0.4731 0.4907 0.4766Mec 86 219 16 0.8431 0.2820 0.6031Nn 122 106 143 0.4604 0.5351 0.4736Npos 2 13 59 0.0328 0.1333 0.0386Others 0 30 10 0.0000 0.0000 ?Pform 8 26 21 0.2759 0.2353 0.2667Pref 19 77 12 0.6129 0.1979 0.4318Prep 100 159 144 0.4098 0.3861 0.4049Reordering 0 0 7 0.0000 ?
?Rloc- 23 89 116 0.1655 0.2054 0.1722SVA 38 85 31 0.5507 0.3089 0.4762Sfrag 0 4 0 ?
0.0000 ?Smod 0 2 0 ?
0.0000 ?Spar 0 10 0 ?
0.0000 ?Srun 0 14 1 0.0000 0.0000 ?Ssub 8 39 19 0.2963 0.1702 0.2581Trans 17 54 39 0.3036 0.2394 0.2881Um 2 21 0 1.0000 0.0870 0.3226V0 8 20 15 0.3478 0.2857 0.3333Vform 31 93 46 0.4026 0.2500 0.3588Vm 7 27 35 0.1667 0.2059 0.1733Vt 26 108 40 0.3939 0.1940 0.3266WOadv 10 11 0 1.0000 0.4762 0.8197WOinc 1 33 37 0.0263 0.0294 0.0269Wci 33 305 146 0.1844 0.0976 0.1565Wform 42 49 29 0.5915 0.4615 0.5600Wtone 4 7 0 1.0000 0.3636 0.7407TOTAL 772 1793 1172 0.3971 0.3010 0.3733Table 7: Type-specific performance of our submit-ted system on the original test set.ORIGINAL SENTENCE:He or she has the right not to tell anyone .SYSTEM HYPOTHESIS:They have the right not to tell anyone .GOLD STANDARD:They have the right not to tell anyone .In other cases, our system seems to do a goodjob despite gold-standard annotations:ORIGINAL SENTENCE:This is because his or her relatives have theright to know about this .SYSTEM HYPOTHESIS:This is because their relatives have the rightto know about this .GOLD STANDARD:This is because his or her relatives have theright to know about this .
(unchanged)The worst performance is observed for Others(including Reordering) and Srun, which only ac-count for 1.69% of the errors.
We also note thatReordering and Srun errors, which had explicitlybeen filtered out, still appear in our final results,20which is due to differences in the edit extractionalgorithms used by the M2Scorer and our own im-plementation.
According to our estimations, oursystem has poor performance on the Wci type (thesecond most frequent), suggesting it is not verysuccessful at correcting idioms and collocations.Corrections for more complex error types suchas Um (unclear meaning), which are beyond thescope of this shared task, are inevitably missed.5.2 DeletionsWe have also observed that many mismatches be-tween our system?s corrections and the gold stan-dard are caused by unnecessary deletions, as in thefollowing example:ORIGINAL SENTENCE:I could understand the feeling of the carrier .SYSTEM HYPOTHESIS:I understand the feeling of the carrier .GOLD STANDARD:I could understand the feeling of the carrier .
(unchanged)This effect is the result of using 10-best hy-potheses from the SMT system together with LMranking.
Hypotheses from an SMT system can in-clude many malformed sentences which are effec-tively discarded by the embedded target languagemodel and additional heuristics.
However, rank-ing these raw hypotheses with external systemscan favour deletions, as language models will gen-erally assign higher probabilities to shorter sen-tences.
A common remedy for this is normali-sation but we found it made no difference in ourexperiments.In other cases, deletions can be ascribed to dif-ferences in the domain of the training and test sets,as observed in this example:ORIGINAL SENTENCE:Nowadays , social media are able to dissemi-nate information faster than any other media .SYSTEM HYPOTHESIS:Nowadays , the media are able to disseminateinformation faster than any other media .GOLD STANDARD:Nowadays , social media are able to dissemi-nate information faster than any other media .
(unchanged)5.3 Uncredited correctionsOur analysis also reveals a number of cases wherethe system introduces changes that are not in-cluded in the gold standard but we consider im-prove the quality of a sentence.
For example:ORIGINAL SENTENCE:Demon is not easily to be defeated and it isrequired much of energy and psychologicalsupport .SYSTEM HYPOTHESIS:Demon is not easily defeated and it requiresa lot of energy and psychological support .GOLD STANDARD:The demon is not easily defeated and it re-quires much energy and psychological sup-port .Adding alternative corrections to the gold stan-dard alleviates this problem, although the list ofalternatives will inevitably be incomplete.There are also a number of cases where the sen-tences are considered incorrect as part of a longertext but are acceptable when they are evaluated inisolation.
Consider the following examples:ORIGINAL SENTENCE:The opposite is also true .SYSTEM HYPOTHESIS:The opposite is true .GOLD STANDARD:The opposite is also true .
(unchanged)ORIGINAL SENTENCE:It has erased the boundaries of distance andtime .SYSTEM HYPOTHESIS:It has erased the boundaries of distance andtime .
(unchanged)GOLD STANDARD:They have erased the boundaries of distanceand time .In both cases, system hypotheses are perfectlygrammatical but they are considered incorrectwhen analysed in context.
Such mismatch is theresult of discrepancies between the annotation andevaluation criteria: while the gold standard is an-notated taking discourse into account, system cor-21rections are proposed in isolation, completely de-void of discursive context.Finally, the inability of the M2Scorer to com-bine corrections from different annotators (as op-posed to selecting only one annotator?s correctionsfor the whole sentence) can also result in underes-timations of performance.
However, it is clear thatexploring these combinations during evaluation isa challenging task itself.6 ConclusionsWe have presented a hybrid approach to error cor-rection that combines a rule-based and an SMTerror correction system.
We have explored dif-ferent combination strategies, including sequen-tial pipelines, candidate generation and ranking.In addition, we have demonstrated that error typeestimations can be used to filter out unnecessarycorrections and improve precision without harm-ing recall.Results of our best hybrid system on the offi-cial CoNLL 2014 test set yield F0.5=0.3733 forthe original annotations and F0.5=0.4355 for alter-native corrections, placing our system in the firstand second place respectively.Error analysis reveals that our system is partic-ularly good at correcting mechanical errors andagreement but is often penalised for unnecessarydeletions.
However, a thorough inspection showsthat the system tends to produce very fluent sen-tences, even if they do not match gold standardannotations.AcknowledgementsWe would like to thank Marek Rei for his valuablefeedback and suggestions as well as CambridgeEnglish Language Assessment, a division of Cam-bridge Assessment, for supporting this research.References?istein E. Andersen, Helen Yannakoudakis, FionaBarker, and Tim Parish.
2013.
Developing and test-ing a self-assessment and tutoring system.
In Pro-ceedings of the Eighth Workshop on Innovative Useof NLP for Building Educational Applications, BEA2013, pages 32?41, Atlanta, GA, USA, June.
Asso-ciation for Computational Linguistics.Pinaki Bhaskar, Aniruddha Ghosh, Santanu Pal, andSivaji Bandyopadhyay.
2011.
May I check theEnglish of your paper!!!
In Proceedings of theGeneration Challenges Session at the 13th Euro-pean Workshop on Natural Language Generation,pages 250?253, Nancy, France, September.
Associ-ation for Computational Linguistics.Steven Bird, Edward Loper, and Ewan Klein.2009.
Natural Language Processing with Python.O?Reilly Media Inc.Daniel Dahlmeier and Hwee Tou Ng.
2012.
Bet-ter evaluation for grammatical error correction.
InProceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,NAACL 2012, pages 568?572, Montreal, Canada.Daniel Dahlmeier, Hwee Tou Ng, and Thanh Phu Tran.2011.
NUS at the HOO 2011 Pilot Shared Task.
InProceedings of the Generation Challenges Sessionat the 13th European Workshop on Natural Lan-guage Generation, pages 257?259, Nancy, France,September.
Association for Computational Linguis-tics.Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.2013.
Building a Large Annotated Corpus ofLearner English: The NUS Corpus of Learner En-glish.
In Proceedings of the 8th Workshop on Inno-vative Use of NLP for Building Educational Appli-cations, BEA 2013, pages 22?31, Atlanta, Georgia,USA, June.Robert Dale and Adam Kilgarriff.
2011.
HelpingOur Own: The HOO 2011 Pilot Shared Task.
InProceedings of the Generation Challenges Sessionat the 13th European Workshop on Natural Lan-guage Generation, pages 242?249, Nancy, France,September.
Association for Computational Linguis-tics.Robert Dale, Ilya Anisimoff, and George Narroway.2012.
HOO 2012: A Report on the Prepositionand Determiner Error Correction Shared Task.
InProceedings of the Seventh Workshop on BuildingEducational Applications Using NLP, pages 54?62,Montr?eal, Canada, June.
Association for Computa-tional Linguistics.John DeNero and Dan Klein.
2007.
Tailoring wordalignments to syntactic machine translation.
In Pro-ceedings of the 45th Annual Meeting of the Asso-ciation of Computational Linguistics, pages 17?24,Prague, Czech Republic, June.
Association for Com-putational Linguistics.Marcello Federico, Nicola Bertoldi, and Mauro Cet-tolo.
2008.
IRSTLM: an open source toolkit forhandling large scale language models.
In Proceed-ings of the 9th Annual Conference of the Interna-tional Speech Communication Association, INTER-SPEECH 2008, pages 1618?1621, Brisbane, Aus-tralia, September.
ISCA.Jianfeng Gao, Patrick Nguyen, Xiaolong Li, ChrisThrasher, Mu Li, and Kuansan Wang.
2010.
AComparative Study of Bing Web N-gram LanguageModels for Web Search and Natural Language Pro-cessing.
In Web N-gram Workshop, Workshop of the2233rd Annual International ACM SIGIR Conference(SIGIR 2010), pages 16?21, Geneva, Switzerland,July.Elitza Ivanova, Delphine Bernhard, and Cyril Grouin.2011.
Handling Outlandish Occurrences: UsingRules and Lexicons for Correcting NLP Articles.
InProceedings of the Generation Challenges Sessionat the 13th European Workshop on Natural Lan-guage Generation, pages 254?256, Nancy, France,September.
Association for Computational Linguis-tics.Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for m-gram language modeling.
In Pro-ceedings of the IEEE International Conference onAcoustics, Speech and Signal Processing, volume I,pages 181?184, Detroit, Michigan, May.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of the 2003 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics on Human Language Technology, vol-ume 1 of NAACL ?03, pages 48?54, Edmonton,Canada.
Association for Computational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ond?rej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: opensource toolkit for statistical machine translation.
InProceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,ACL ?07, pages 177?180, Prague, Czech Republic.Association for Computational Linguistics.Philipp Koehn, 2014.
Moses: Statistical Ma-chine Translation System ?
User Manual and CodeGuide.
University of Edinburgh, April.
Availableonline at http://www.statmt.org/moses/manual/manual.pdf.Anoop Kunchukuttan, Ritesh Shah, and Pushpak Bhat-tacharyya.
2013.
IITB System for CoNLL 2013Shared Task: A Hybrid Approach to Grammati-cal Error Correction.
In Proceedings of the Seven-teenth Conference on Computational Natural Lan-guage Learning: Shared Task, pages 82?87, Sofia,Bulgaria, August.
Association for ComputationalLinguistics.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In Proceedings of the HumanLanguage Technology Conference of the NAACL,Main Conference, pages 104?111, New York City,USA, June.
Association for Computational Linguis-tics.Graham Neubig, Taro Watanabe, Eiichiro Sumita,Shinsuke Mori, and Tatsuya Kawahara.
2011.
Anunsupervised model for joint phrase alignment andextraction.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguis-tics: Human Language Technologies, pages 632?641, Portland, Oregon, USA, June.
Association forComputational Linguistics.Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, ChristianHadiwinoto, and Joel Tetreault.
2013.
The CoNLL-2013 Shared Task on Grammatical Error Correction.In Proceedings of the Seventeenth Conference onComputational Natural Language Learning: SharedTask, pages 1?12, Sofia, Bulgaria, August.
Associa-tion for Computational Linguistics.Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, ChristianHadiwinoto, Raymond Hendy Susanto, and Christo-pher Bryant.
2014.
The CoNLL-2014 Shared Taskon Grammatical Error Correction.
In Proceedings ofthe Eighteenth Conference on Computational Natu-ral Language Learning: Shared Task (CoNLL-2014Shared Task), Baltimore, Maryland, USA, June.
As-sociation for Computational Linguistics.
To appear.Diane Nicholls.
2003.
The Cambridge Learner Cor-pus: Error coding and analysis for lexicography andELT.
In Dawn Archer, Paul Rayson, Andrew Wil-son, and Tony McEnery, editors, Proceedings ofthe Corpus Linguistics 2003 conference, pages 572?581, Lancaster, UK.
University Centre for ComputerCorpus Research on Language, Lancaster Univer-sity.Franz Josef Och and Hermann Ney.
2000.
Improvedstatistical alignment models.
In Proceedings of the38th Annual Meeting on Association for Computa-tional Linguistics, ACL ?00, pages 440?447, HongKong, October.
Association for Computational Lin-guistics.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Comput.
Linguist., 29(1):19?51, March.Desmond Darma Putra and Lili Szabo.
2013.
UdSat CoNLL 2013 Shared Task.
In Proceedings ofthe Seventeenth Conference on Computational Natu-ral Language Learning: Shared Task, pages 88?95,Sofia, Bulgaria, August.
Association for Computa-tional Linguistics.Alla Rozovskaya, Mark Sammons, Joshua Gioja, andDan Roth.
2011.
University of Illinois System inHOO Text Correction Shared Task.
In Proceedingsof the Generation Challenges Session at the 13th Eu-ropean Workshop on Natural Language Generation,pages 263?266, Nancy, France, September.
Associ-ation for Computational Linguistics.Alla Rozovskaya, Kai-Wei Chang, Mark Sammons,and Dan Roth.
2013.
The University of IllinoisSystem in the CoNLL-2013 Shared Task.
In Pro-ceedings of the Seventeenth Conference on Compu-tational Natural Language Learning: Shared Task,pages 13?19, Sofia, Bulgaria, August.
Associationfor Computational Linguistics.23Yang Xiang, Bo Yuan, Yaoyun Zhang, Xiaolong Wang,Wen Zheng, and Chongqiang Wei.
2013.
A hy-brid model for grammatical error correction.
In Pro-ceedings of the Seventeenth Conference on Compu-tational Natural Language Learning: Shared Task,pages 115?122, Sofia, Bulgaria, August.
Associa-tion for Computational Linguistics.Junwen Xing, Longyue Wang, Derek F. Wong, Lidia S.Chao, and Xiaodong Zeng.
2013.
UM-Checker: AHybrid System for English Grammatical Error Cor-rection.
In Proceedings of the Seventeenth Confer-ence on Computational Natural Language Learn-ing: Shared Task, pages 34?42, Sofia, Bulgaria, Au-gust.
Association for Computational Linguistics.Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.2011.
A new dataset and method for automaticallygrading esol texts.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages180?189, Portland, Oregon, USA, June.
Associationfor Computational Linguistics.Ippei Yoshimoto, Tomoya Kose, Kensuke Mitsuzawa,Keisuke Sakaguchi, Tomoya Mizumoto, YutaHayashibe, Mamoru Komachi, and Yuji Matsumoto.2013.
NAIST at 2013 CoNLL Grammatical Er-ror Correction Shared Task.
In Proceedings of theSeventeenth Conference on Computational NaturalLanguage Learning: Shared Task, pages 26?33,Sofia, Bulgaria, August.
Association for Computa-tional Linguistics.Zheng Yuan and Mariano Felice.
2013.
Constrainedgrammatical error correction using statistical ma-chine translation.
In Proceedings of the SeventeenthConference on Computational Natural LanguageLearning: Shared Task, pages 52?61, Sofia, Bul-garia, August.
Association for Computational Lin-guistics.24
