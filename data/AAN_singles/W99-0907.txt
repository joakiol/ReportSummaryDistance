Detecting Sub-Topic Correspondencethrough Bipartite Term ClusteringZvika MARXThe Center for Neural Computation,The Hebrew University of JerusalemandThe Institute for IR and Comp.
Linguistics,Mathematics and Computer Science Dept.,Bar-Ilan UniversityRamat-Gan 52900, Israel,Marxzv@cs.biu.ac.ilAbstractIdo DAGANThe Institute for Info.
Retrievaland Computational Linguistics,The Department ofMathematicsand Computer Science,Bar-Ilan UniversityRamat-Gan 52900, Israel,dagan@cs.biu.ac.ilrelated to each other?This paper addresses a novel task ofdetecting sub-topic correspondence in apair of text fragments, enhancing commonnotions of text similarity.
This task isaddressed by coupling corresponding termsubsets through bipartite clustering.
Thepaper presents a cost-based clusteringscheme and compares it with a bipartiteversion of the single-link method,providing illustrating results.1.
Introduction: Corresponding Entitiesin Text FragmentsInformation technology is continuouslychallenged by growing demand for accurateperformance in fields such as data retrieval,document classification and knowledgerepresentation.
A typical task in these areasrequires well-developed capabilities of assessingsimilarity between text fragments (see, forexample, Chapter 6 in Kowalski, 1997).Nevertheless, it is apparent that standardmethods for detecting document similarity donot cover the whole range of what peopleperceive as similar.Common treatment of document similaritytypically aims at a unified pairwise measureexpressing the extent to which documents aresimilar to each other.
Consequently, anInternet-surfer hitting the "what's related"button in her browser gets a list of pages that aresupposed to be similar to the currently viewedone.
Here, we originally address questionssituated one-step ahead: Given documents thatare already known to be similar, how they areEli SHAMIRInstitute of ComputerScienceThe Hebrew Universityof JerusalemJerusalem 91904,Israelshamir@cs.huji.ac.ilHow to refer a user torelevant aspects in a large collection of similardocuments?
One possible strategy, rooted incognitive considerations, i  to present for eachpair of similar documents a detailed "map",connecting corresponding concepts, entities orsub-topics.
The present article provides initialdirections towards identification of suchcorrespondences.Consider, for example, the following twofragments taken from a pair of 1986 Reuters'news-articles:i.
LOS ANGELES,  March  13 - ComputerMemor ies  Inc .
.
.
.
agreed  to acquireHemdale  F i lm Corp  .
.
.
.
Thatcompany ' s owner,  John Daly, wouldthen  become chief executive officerof the combined  company.
.
.2.
NEW YORK, March  25 - Mess idor  L tdsa id  i t  s igned  a le t te r  of  in tent  toacquire i00 pct  of the outs tand ingshares  of Tri ton Be legg inengNeder land  B. V .
.
.
.
If  approved,  thepresident of Triton, Hendr ik  Bokma,wi l l  be nominated  as cha i rman of thecombined  company .
.
.
.Both fragments deal with the intention of acertain company to acquire another company.Since the word "acquire' appears in botharticles, keyword-based methods would interpretit as a positive evidence for evaluating the textfragments as similar to each other.
Moresophisticated methods (e.g.
Latent SemanticIndexing; Deerwester et al, 1990) incorporatevector-based statistical term-similarity modelsthat may take into account correspondence ofdifferent erms that resemble in their meaning.For example, the corresponding term pairs'owner" ~ 'president', and 'chief executiveofficer' ~ 'chairman' may contribute to the45unified value of evaluated similarity.
Now,consider another pair of terms: 'become' - -'nominated'.
These terms probably share only amoderate degree of similarity in general, but ahuman reader will find their correspondencemuch more meaningful in this particular context.Identification of this context-dependentequivalence nables a reader to perceive thatJohn Daly and Hendrik Bokma, respectivelymentioned in the above texts, play an analogouspart of being appointed to a managerial position.Existing similarity evaluation methods do notconsider such analogies and do not provide toolsfor pointing them out.Unlike common methods in automated naturallanguage processing, cognitive research hasemphasized the role of analogy in humanthinking.
The ability to detect analogoussimilarities between complex objects ispresented by cognitive theories as a foremostmechanism in reasoning, problem solving and inhuman intelligence in general.
The structuremapping theory (Gentner, 1983) presentsanalogy as mapping between two distinctsystems.
Particular entities that compose achsystem are not similar in general, but rather therelations among them resemble each other.Hence, entities in one system are perceived asplaying a similar role to that played bycorresponding entities in the other system.Another approach (Hofstadter et al, 1995)emphasizes the context-dependent interplaybetween perceiving features of the systemsunder comparison and creating representationsthat are suitable for mutual mapping.Motivated by the above considerations, wepresent an initial step towards identifyingautomatically corresponding entities in naturallanguage texts.
At this stage of our research,correspondences are based on term similarityonly, so terms describing similar topics arecoupled.
Identification and mapping of bothentities and relations, using additionalinformation, such as syntactic constructs (adirection which has been proposed in Hasse,1995), will be handled in subsequent stages.However, presenting context-dependent topiccorrespondences in a pair of texts is by itself anon-trivial elaboration of standard approaches todocument similarity.Unsupervised specification of precise structure,let alne the optimal structure, is known to be anill-posed problem even in classical tasks such asstraightforward clustering.
Nevertheless, weobserve that our task here is to find relevantstructure in the data.
In section 2, we present amodel for the structure we aim at.
Then, insection 3, we recourse to a standard mechanismof capturing the quality of the proposed structureby a suitable cost function, followed by analgorithm seeking to minimize the cost.
As inmore studied learning-tasks, alternative costs oroptimization methods are possible and formlegitimate subject for future research.
At thisstage, we concentrate on demonstrating thefeasibility of getting sub-topic similarity mapsbetween text fragments through a novel bipartiteclustering setting.2.
The Model: Term Subset Coupling byBipartite ClusteringThe present study suggests a framework foridentifying corresponding sub-topics within apair of text fragments.
Our model representssub-topics as groups of related terms, which areassumed to correspond to actual sub-topics.
Forthis, the sets of terms appearing in each one ofthe fragments are divided into coupled subsets.A pair of coupled subsets, one from eachfragment, is supposed to representcorresponding sub-topics from the comparedfragments.For the illustration, consider the following smallterm sets:(i) {attendant,  min is ter ,  government}(2) {employee, manager}(3) {student, un ivers i ty}Term-subset coupling, based on semantic termsimilarity, applied to the first two term sets,might produce the following subset couples:{attendant  } -- {employee}{minister,  government}  -- {manager}For similar considerations applied to sets (1) and(3), the result might look like:{attendant,  min is ter}  -- {student}{government  } -- {univers i ty}These illustrative examples demonstrateexpected topical partitions of the term setsaccording to the diagnosticity principle(Tversky, 1977).
How each set is divideddepends on how terms of both sets resembleeach other: in the first case, the grouped topicsare "workers" and "management"; in the secondcase - "individuals" and "institutions".46For obtaining subset coupling, we applyclustering methods.
Quite a few previous worksinvestigated the idea of identifying semanticsubstances with term clusters.
Term clusteringmethods are typically based on the statistics ofterm co-occurrence within a word window, orwithin syntactic constructs (e.g.
Pereira et al,1993).
The notion pairwise clustering refers toclustering established, as in the present study, onprevious assessment of term similarity values -a process often based by itself on term co-occurrence data (e.g.
Lin, 1999).A standard pairwise clustering problem can berepresented by a weighted graph, where eachnode stands for a data point and each edge isweighted according to the degree of similarity ofthe nodes it connects.
A (hard) clusteringprocedure produces partition of the graph nodesto disjoint connected components forming acluster configuration.Our setting is special in that it considers onlysimilarity values referring to term pairs from twodistinct text fragments, such as 'attendant'-'manager' in the example above, but not' attendant'-' minister'.
The exclusion of within-fragment similarities is conformed to ourcontext-oriented approach, but there is noessential restriction on incorporating them in amore comprehensive model.
Consequently, oursetting is represented by a bipartite graphcontaining only edges connecting nodes fromtwo distinct node sets, each of which associatedwith terms from a different text fragment.
Aterm that appears in both articles is representedindependently in both sets.The use of clustering within a bipartite graph(bipartite clustering) is not common in naturallanguage processing.
Hofmann and Puzicha(1998) introduce taxonomy of likelihood-basedclustering algorithms for co- occurrence data,some of which produce bipartite clustering.
Toillustrate their soft clustering method, theypresent sets of nouns and adjectives that tend toco-occur in a large corpus.
Sheffer-Hazan(1997) developed a bipartite clustering algorithmbased on description length considerations forpurposes of knowledge summarization and textmining.
Both works exploit co-occurrence datafor exposure of global characteristics of acorpus.
The present study refers too, through itsuse of pre-compiled similarity data, toco-occurrence statistics in a corpus.
Here, we gobeyond that to get fine-grained context-dependent groupings in the term sets ofparticular text-fragment pairs.When pairwise clustering algorithms are appliedon a bipartite graph, the assignment of a termfrom one of the sets into a cluster is influencedby the assignments of similar terms from theother set.
Each one of the resulting clusters, ifcontains more than a single element, necessarilycontains terms from both parts, e.g.
<minister,government, manager> in the example above.Therefore, a cluster couples two term subsets,each from a different fragment: the subset{manager} is coupled to the subset {minister,government}.
Clusters containing a singleelement represent terms that could not beassigned to any of the coupled subsets by theclustering method.3.
Algorithms: Balancing Within-Clusterand Between-Cluster SimilaritiesLet X and Y denote the sets of the termsappearing in a given pair of articles.
Wecurrently use the "bag of words" model, whereterm repetitions are not counted.
Non-negativesimilarity values, s(x,y), are given (as input) foreach x~X and y~Y.
Assume that someclustering procedure is applied to the appropriatebipartite graph, so that a partition of the graphnodes is given.
Denote by Cx the partcontaining x~X.
Recall that if Cx containsadditional elements, some of them must beelements of Y.
Hence, Cx represents coupling ofthe subsets XACx and YnCx.A basic clustering strategy is the greedy single-link agglomerative method.
It starts with aconfiguration in which for each x~X and y~ Y,Cx = {x}, Cy = {y}.
Then, the methodrepeatedly merges a pair of clusters Cx and Cysuch that x and y are the most similar elementsfor which Cx ?= Cy.
The result is a hierarchicalarrangement of clusters, also called dendogram.There is no fixed recipe of how to select he bestclustering configuration (partitioning) in thehierarchy.
Furthermore, in our case the numberof target sub-topics is not known in advance.We thus refer to the obtained hierarchy asrepresenting a range of possible clusterconfigurations, corresponding to varyinggranularity levels.47An alternative approach states in advance whatis expected from a good clusteringconfiguration, rather than letting the mergingprocess dictate the clustering as in the case ofsingle-link.
This is customarily done byformulating a cost function, to be minimized byan optimal configuration.
In our case, as inclustering in general, a cost function reflects theinterplay between two dual constraints:(i) Maximizing within-cluster similarity, i.e.
thetendency to include similar objects in the samecluster.
It should be stressed that in thebipartite setting the notion of 'within-cluster'refers to similarity values between pairs ofterms from coupled subsets, while the actualsimilarities within each subset are notconsidered.
The excessive satisfaction of thisconstraint dictates a cluster configurationcontaining many small clusters, eachcharacterized by high similarity values amongits members.
(ii) Minimizing between-cluster similarity, i.e.the tendency to avoid assigning similar objects(in the bipartite setting - from distinctfragments) into different clusters.
Theexcessive satisfaction of this constraint resultsin obtaining large clusters, so that onlyminimal between-cluster similarity is present.We have considered several cost functionschemes, reflecting different types ofinteractions between the above two constraints.One particular scheme, which enables obtainingcontext-dependent subset coupling at variousgranularity levels, is presented here.This scheme captures the between-clustersimilarity minimization constraint by including,for each term xe X (and correspondingly for eachye Y), a cost component proportional to thebetween-cluster similarity values associated withthat term, i.e.
proportional to ~y~ r_cxS(x,y).According to the other constraint of within-cluster similarity maximization, each term x issupposed to be assigned into a cluster such thatits contribution to the total measure of within-cluster similarity is maximal.
To obtain a costmeasure, which is inversely proportional to thecontribution of x to total within-clustersimilarity, we measure the total degree ofwithin-cluster similarity obtained if x wereremoved from its cluster Cx.
That is, we add foreach x~ X (and correspondingly for each y~ Y) acost component proportional to the totalcontribution to within-cluster similarity of theother subset members: ~c~_lx~#cxS(x',y).This component is further multiplied by 1/IX!
fornormalizing it relatively to the entire set size.Finally, the cost function scheme introduces aparameter, 0 < a < 1, which controls the relativeimpact of each of the two constraints.
Theresulting scheme is thus weighted sum of thetwo cost components for all terms in X and Y:~M)=x~_X--Cy li\[ y~yc~Cy.qy} k ~.Xr-Cy }JVarying ~ has the effect of changing cluster sizewithin the optimal configuration, due to thevarying impact of the two constraints (increasinga reduces cluster size, and vice versa).
Anotherinteresting property of this scheme is thatcoupling two singletons, which have a positivesimilarity value, always reduces the total cost.This is because such coupling, forming a two-member cluster, reduces between-clustersimilarity cost and does not increase within-cluster similarity cost.Note that E(M) pretends to reflect balance ofconstrains, as described above, only for aparticular pair of documents at a time.
Itspotential value as a basis for unified documentsimilarity measure, sensitive to context-dependent and analogous imilarities, is yet tobe investigated.There are sophisticated techniques to computean optimal solution minimizing the cost functionfor a given o~ value, e.g.
simulated annealing anddeterministic annealing (Hofmann andBuhmann, 1997).
A simple strategy, assumed tosuffice for preliminary demonstration of costfunction behavior for any a, is a greedy method,similar to the single-link method.
It starts witha configuration in which for each x and y,Cx = {x}, Cy = {y} and then merges repeatedlythe two clusters whose merge minimizes the costmostly.
Unlike single-link, this process stopswhen no further cost reduction is possible.4.
Results: Hierarchy and GranularityOur experiments were performed for termcoupling between pairs of Reuters news articles.4.8Here we qualitatively demonstrate the resultsusing the same pair of articles of the example inSection 1 (devising a quantitative valuationmethod for our task is an issue for futureresearch).
We used pairwise term similarityvalues that were compiled by Dekang Lin, usinga similarity measure based on informationtheoretical considerations, from co-occurrencedata in a large corpus of news articles (Lin,1999; data available for download fromhttp://www.cs.umanitoba.ca/-lindek/sims.tgz).The term sets were taken to be the sets of words,in each article, which had at least one positivesimilarity value with a term in the other article.The vocabulary included verbs, nouns,adjectives and adverbs, excluding a small set ofstop words (e.g.
'about').
The ConexorNP&Name Parser (Voutilainen, 1997) was usedto obtain word lemmas.Figure I displays detailed term subset couplinggenerated by the single-link procedure.
Thehierarchy is indicated by different widths ofcontours bounding term subsets.
Each contourwidth presents the clusters obtained after allmerges that were imposed by similarity valueslarger than a threshold t. Coupling connectionsare displayed for the most detailed granularitylevel.
An apparent drawback of this method isthat many terms are assigned into clusters onlyin a late stage, although they seem to be relatedto one or more of the smaller clusters.
E.g.
'management' seems to be related, and indeedhas non-zero similarity values, to 'chairman''director' and 'president' as well as to 'chief.This is indicated by including such terms in thelargest hin frames in Figure 1, but not in anybold smaller frame.We have also implemented more sophisticatedmethods proposed recently (Blatt et al, 1997;Gdalyahu et al, 1999) that are related to thesingle-link strategy.
These methods aredesigned to overcome cases where few "noisy"data points invoke union of clusters that wouldhave remain separate in the absence of thesepoints.
Both methods repeatedly samplestochastic approximated cluster configurations.Elements, persistently found in the same clusteracross the sample, are assigned to the samecluster also in the final solution.
The resultsobtained with these methods are qualitativelysimilar to those obtained with single-link.
Thissuggests that the fact that certain terms remainuncoupled in high granularity levels can not beattributed to random inaccuracies in the data.Figure 2 displays a detailed term subset couplinggenerated by the cost-guided greedy strategy.The lack of strict hierarchy prevents displaying awide range of granularity levels within thefigure, so a sample of clusters is presented.
Thegray clusters demonstrate the impact of lower o~values on cluster granularity.
Several of thecoupled term-subsets represent actual sub-topics,such as "trade operations" and "managerialpositions".
Comparing with the single linkalgorithm, the cost-based algorithm doessucceed to couple related terms such as'management' and 'chairman' within arelatively tight cluster.
Note also that thealgorithm couples the words 'become' and'nominate', as discussed in Section 1.5.
ConclusionsThis paper describes a preliminary step,suggesting bipartite term coupling as anattractive approach for detecting sub-topiccorrespondence.
Future work is required toinvestigate aspects that have already beenmentioned, such as the use of other similaritymeasures, the incorporation of within-documentsimilarities and additional search strategies.Another dffection we are considering isintegration of data from several sub-topic maps,in order to modify the original term similaritymatrix, starting an iterative algorithm in the EMstyle.
In addition, we wish to study howadditional attitudes to clustering, e.g.
the onedescribed by Pereira et al (1993), are related toour setting.
It is also necessary to develop aquantitative evaluation method, possibly basedon comparing the performance of our methodwith that of human subjects in similar tasks.49Article 1issueasset mergel : transactiotrenaemehaveagreeoff~Article 2I common, ~cquire acquisition~sign'  ...,approve '1-1 I I I Iapproval  control  result  \ [ ~cash  connec~omnOrycerta in end computerdisk f i lm meetequiva lent :d i rectorJ~ r a n tztor p res~ Jconsistlettermanagementpublic nomin~Figure 1: Detailed hierarchical subset coupling in a pair of Reuters news articles, as generated withthe single link method.
Each of the presented merging stages is characterized by the similarityvalue t that imposed the merge at that stage.
The different stages are indicated by differentcontour widths.
Coupling connections are indicated as straight lines and are displayed only forthe most detailed level t = 0.18.From a broader perspective, this researchinitiates an original unsupervised learningframework, capturing similarity of complexobjects.
It is hoped that future results willprovide a significant contribution to both settingand achieving information technology tasks, sothey better reflect human thinking and needs.AcknowledgementsWe thank Dekang Lin for the use of hisAutomatically Generated Thesaurus.
Thisresearch was supported by ISRAEL SCIENCEFOUNDATION founded by The Academy ofSciences and Humanities (grant 574/98-1).ReferencesBlatt M., Weisman S., and Domany E. (1997) DataClustring Using Model Granular Magnet.
NeuralComputation, 9/8, pp.
1805-1842.Deerwester S.,.
Dumais S. D., Furnas G. W.,Landauer T.K., and Harshman R.A. (1990)Indexing by Latent Semantic Analysis.
Journal ofthe American Society for Information Science,41/6, pp.
391---407.Gdalyahu Y., Weinshall D., and Werman M. (1999)A Randomized Algorithm for Pairwise Clustering.In "Advances in Neural Information ProcessingSystems 11 (NIPS*98)", M. S. Kearns, S. A. Solla& D. A. Cohn, ed., MIT Press, Boston (to appear).Gentner, D. (1983) Structure-Mapping: A TheoreticalFramework for Analogy.
Cognitive Science, 7/2,pp.
155-170.Haase K. (1995) Analogy in the Large.
In"IJCAI-95: Proceedings of the FourteenthInternational Joint Conference on ArtificialIntelligence" Vol.
2, Montreal, pp.
1375-1380.50I I IArt ic le Ir l lm memoryapproval connectionmerger purchase,ransaction computerArt ic le 2investmentowner s n a r e n o l a e r ~ f f i c e r  president~orate newspecialconsist letterFigure 2: A sample of subset coupling output as generated with a greedy method applied to thecost-based scheme for cx values between 0.53 to 0.73.
Coupling connections are indicated asstraight lines.
The gray areas indicate coupled subsets of lower granularity level, which wereformed for 0.53 < cx < 0.68 (I), 0.53 ~ ct __.
0.55 (11) and cx = 0.53 (111).
Terms that swapped theirmembership for different o~ values appear, whenever possible, in the intersection of theappropriate subsets; otherwise, they are marked with asterisk.
Terms that for some cx value werenot included in any coupled subset are in Italics.Hofmann T. and Puzicha J.
(1998) Statistical Modelsfor Co-occurrence Data.
AI Memo No.
1625 /CBCL Memo No.
159, Artificial IntelligenceLaboratory and Center for Biological andComputational Learning, Massachusetts In titute ofTechnology, Boston, 21 p,Hofmann T. and Buhmann J.
(1997) Pairwise DataClustering by Deterministic Annealing.
IEEETransactions on Pattern Analysis and MachineIntelligence, 19/1, pp 1-14.Hofstadter D. R. and the Fluid Analogies ResearchGroup (1995) Fluid Concepts and CreativeAnalogies.
Basic Books, New-York, 518 p.Lin D. (1999) Automatic Retrieval and Clustering ofSimilar Words.
In "Proceedings of the SeventeenthInternational Conference on ComputationalLinguistics and the Thirty-Sixth Annual Meeting ofthe Association for Computational Linguistics(COLING-ACL '98)", Montreal (to appear).Kowalski G. (1997) Information Retrieval Systems:Theory and Implementation.
Kluwer AcademicPublishers, Boston, pp 125-148.Pereira, F. C. N., Tishby N. Z., and Lee L. J.
(1993)Distributional Clustering of English Words.
In"Proceedings of the Thirty-First Annual Meeting ofthe Association for Computational Linguistics(ACL)", Columbus, OH, pp 183-190.Sheffer-Hazan S. (1997) Knowledge Discovery andSummerization by Bipartite Graph.
UnpublishedMaster's Thesis (in Hebrew).
Bar-Ilan University,Ramat-Gan, Israel, 68p.Tversky A.
(1977) Features of Similarity.Psychological Review, 84/4, pp 327-352..Voutilainen A.
(1997) The Conexor NP&NameParser (ENG-CG).
Web page:http://conexor.co.helsinki.fi/NPintro.html.51
