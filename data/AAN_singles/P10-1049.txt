Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 475?484,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsTraining Phrase Translation Models with Leaving-One-OutJoern Wuebker and Arne Mauser and Hermann NeyHuman Language Technology and Pattern Recognition GroupRWTH Aachen University, Germany<surname>@cs.rwth-aachen.deAbstractSeveral attempts have been made to learnphrase translation probabilities for phrase-based statistical machine translation thatgo beyond pure counting of phrasesin word-aligned training data.
Mostapproaches report problems with over-fitting.
We describe a novel leaving-one-out approach to prevent over-fittingthat allows us to train phrase models thatshow improved translation performanceon the WMT08 Europarl German-Englishtask.
In contrast to most previous workwhere phrase models were trained sepa-rately from other models used in transla-tion, we include all components such assingle word lexica and reordering mod-els in training.
Using this consistenttraining of phrase models we are able toachieve improvements of up to 1.4 pointsin BLEU.
As a side effect, the phrase tablesize is reduced by more than 80%.1 IntroductionA phrase-based SMT system takes a source sen-tence and produces a translation by segmenting thesentence into phrases and translating those phrasesseparately (Koehn et al, 2003).
The phrase trans-lation table, which contains the bilingual phrasepairs and the corresponding translation probabil-ities, is one of the main components of an SMTsystem.
The most common method for obtain-ing the phrase table is heuristic extraction fromautomatically word-aligned bilingual training data(Och et al, 1999).
In this method, all phrases ofthe sentence pair that match constraints given bythe alignment are extracted.
This includes over-lapping phrases.
At extraction time it does notmatter, whether the phrases are extracted from ahighly probable phrase alignment or from an un-likely one.Phrase model probabilities are typically definedas relative frequencies of phrases extracted fromword-aligned parallel training data.
The jointcounts C(f?
, e?)
of the source phrase f?
and the tar-get phrase e?
in the entire training data are normal-ized by the marginal counts of source and targetphrase to obtain a conditional probabilitypH(f?
|e?)
=C(f?
, e?)C(e?).
(1)The translation process is implemented as aweighted log-linear combination of several mod-els hm(eI1, sK1 , fJ1 ) including the logarithm of thephrase probability in source-to-target as well as intarget-to-source direction.
The phrase model iscombined with a language model, word lexiconmodels, word and phrase penalty, and many oth-ers.
(Och and Ney, 2004) The best translation e?I?1as defined by the models then can be written ase?I?1 = argmaxI,eI1{M?m=1?mhm(eI1, sK1 , fJ1 )}(2)In this work, we propose to directly train ourphrase models by applying a forced alignment pro-cedure where we use the decoder to find a phrasealignment between source and target sentences ofthe training data and then updating phrase transla-tion probabilities based on this alignment.
In con-trast to heuristic extraction, the proposed methodprovides a way of consistently training and usingphrase models in translation.
We use a modifiedversion of a phrase-based decoder to perform theforced alignment.
This way we ensure that allmodels used in training are identical to the onesused at decoding time.
An illustration of the basic475Figure 1: Illustration of phrase training withforced alignment.idea can be seen in Figure 1.
In the literature thismethod by itself has been shown to be problem-atic because it suffers from over-fitting (DeNeroet al, 2006), (Liang et al, 2006).
Since our ini-tial phrases are extracted from the same trainingdata, that we want to align, very long phrases canbe found for segmentation.
As these long phrasestend to occur in only a few training sentences, theEM algorithm generally overestimates their prob-ability and neglects shorter phrases, which bettergeneralize to unseen data and thus are more usefulfor translation.
In order to counteract these effects,our training procedure applies leaving-one-out onthe sentence level.
Our results show, that this leadsto a better translation quality.Ideally, we would produce all possible segmen-tations and alignments during training.
However,this has been shown to be infeasible for real-worlddata (DeNero and Klein, 2008).
As training usesa modified version of the translation decoder, it isstraightforward to apply pruning as in regular de-coding.
Additionally, we consider three ways ofapproximating the full search space:1. the single-best Viterbi alignment,2.
the n-best alignments,3.
all alignments remaining in the search spaceafter pruning.The performance of the different approaches ismeasured and compared on the German-EnglishEuroparl task from the ACL 2008 Workshop onStatistical Machine Translation (WMT08).
Ourresults show that the proposed phrase model train-ing improves translation quality on the test set by0.9 BLEU points over our baseline.
We find thatby interpolation with the heuristically extractedphrases translation performance can reach up to1.4 BLEU improvement over the baseline on thetest set.After reviewing the related work in the fol-lowing section, we give a detailed descriptionof phrasal alignment and leaving-one-out in Sec-tion 3.
Section 4 explains the estimation of phrasemodels.
The empirical evaluation of the differentapproaches is done in Section 5.2 Related WorkIt has been pointed out in literature, that trainingphrase models poses some difficulties.
For a gen-erative model, (DeNero et al, 2006) gave a de-tailed analysis of the challenges and arising prob-lems.
They introduce a model similar to the onewe propose in Section 4.2 and train it with the EMalgorithm.
Their results show that it can not reacha performance competitive to extracting a phrasetable from word alignment by heuristics (Och etal., 1999).Several reasons are revealed in (DeNero et al,2006).
When given a bilingual sentence pair, wecan usually assume there are a number of equallycorrect phrase segmentations and correspondingalignments.
For example, it may be possible totransform one valid segmentation into another bysplitting some of its phrases into sub-phrases or byshifting phrase boundaries.
This is different fromword-based translation models, where a typical as-sumption is that each target word corresponds toonly one source word.
As a result of this am-biguity, different segmentations are recruited fordifferent examples during training.
That in turnleads to over-fitting which shows in overly deter-minized estimates of the phrase translation prob-abilities.
In addition, (DeNero et al, 2006) foundthat the trained phrase table shows a highly peakeddistribution in opposition to the more flat distribu-tion resulting from heuristic extraction, leaving thedecoder only few translation options at decodingtime.Our work differs from (DeNero et al, 2006)in a number of ways, addressing those problems.476To limit the effects of over-fitting, we apply theleaving-one-out and cross-validation methods intraining.
In addition, we do not restrict the train-ing to phrases consistent with the word alignment,as was done in (DeNero et al, 2006).
This allowsus to recover from flawed word alignments.In (Liang et al, 2006) a discriminative transla-tion system is described.
For training of the pa-rameters for the discriminative features they pro-pose a strategy they call bold updating.
It is simi-lar to our forced alignment training procedure de-scribed in Section 3.For the hierarchical phrase-based approach,(Blunsom et al, 2008) present a discriminativerule model and show the difference between usingonly the viterbi alignment in training and using thefull sum over all possible derivations.Forced alignment can also be utilized to train aphrase segmentation model, as is shown in (Shenet al, 2008).
They report small but consistentimprovements by incorporating this segmentationmodel, which works as an additional prior proba-bility on the monolingual target phrase.In (Ferrer and Juan, 2009), phrase models aretrained by a semi-hidden Markov model.
Theytrain a conditional ?inverse?
phrase model of thetarget phrase given the source phrase.
Addition-ally to the phrases, they model the segmentationsequence that is used to produce a phrase align-ment between the source and the target sentence.They used a phrase length limit of 4 words withlonger phrases not resulting in further improve-ments.
To counteract over-fitting, they interpolatethe phrase model with IBM Model 1 probabilitiesthat are computed on the phrase level.
We also in-clude these word lexica, as they are standard com-ponents of the phrase-based system.It is shown in (Ferrer and Juan, 2009), thatViterbi training produces almost the same resultsas full Baum-Welch training.
They report im-provements over a phrase-based model that usesan inverse phrase model and a language model.Experiments are carried out on a custom subset ofthe English-Spanish Europarl corpus.Our approach is similar to the one presented in(Ferrer and Juan, 2009) in that we compare Viterbiand a training method based on the Forward-Backward algorithm.
But instead of focusing onthe statistical model and relaxing the translationtask by using monotone translation only, we use afull and competitive translation system as startingpoint with reordering and all models included.In (Marcu and Wong, 2002), a joint probabilityphrase model is presented.
The learned phrasesare restricted to the most frequent n-grams up tolength 6 and all unigrams.
Monolingual phraseshave to occur at least 5 times to be consideredin training.
Smoothing is applied to the learnedmodels so that probabilities for rare phrases arenon-zero.
In training, they use a greedy algorithmto produce the Viterbi phrase alignment and thenapply a hill-climbing technique that modifies theViterbi alignment by merge, move, split, and swapoperations to find an alignment with a better prob-ability in each iteration.
The model shows im-provements in translation quality over the single-word-based IBM Model 4 (Brown et al, 1993) ona subset of the Canadian Hansards corpus.The joint model by (Marcu and Wong, 2002)is refined by (Birch et al, 2006) who usehigh-confidence word alignments to constrain thesearch space in training.
They observe that due toseveral constraints and pruning steps, the trainedphrase table is much smaller than the heuristicallyextracted one, while preserving translation quality.The work by (DeNero et al, 2008) describesa method to train the joint model described in(Marcu and Wong, 2002) with a Gibbs sampler.They show that by applying a prior distributionover the phrase translation probabilities they canprevent over-fitting.
The prior is composed ofIBM1 lexical probabilities and a geometric distri-bution over phrase lengths which penalizes longphrases.
The two approaches differ in that we ap-ply the leaving-one-out procedure to avoid over-fitting, as opposed to explicitly defining a priordistribution.3 AlignmentThe training process is divided into three parts.First we obtain all models needed for a normaltranslations system.
We perform minimum errorrate training with the downhill simplex algorithm(Nelder and Mead, 1965) on the development datato obtain a set of scaling factors that achieve agood BLEU score.
We then use these models andscaling factors to do a forced alignment, wherewe compute a phrase alignment for the trainingdata.
From this alignment we then estimate newphrase models, while keeping all other models un-477changed.
In this section we describe our forcedalignment procedure that is the basic training pro-cedure for the models proposed here.3.1 Forced AlignmentThe idea of forced alignment is to perform aphrase segmentation and alignment of each sen-tence pair of the training data using the full transla-tion system as in decoding.
What we call segmen-tation and alignment here corresponds to the ?con-cepts?
used by (Marcu and Wong, 2002).
We ap-ply our normal phrase-based decoder on the sourceside of the training data and constrain the transla-tions to the corresponding target sentences fromthe training data.Given a source sentence fJ1 and target sentenceeI1, we search for the best phrase segmentation andalignment that covers both sentences.
A segmen-tation of a sentence into K phrase is defined byk ?
sk := (ik, bk, jk), for k = 1, .
.
.
,Kwhere for each segment ik is last position of kthtarget phrase, and (bk, jk) are the start and endpositions of the source phrase aligned to the kthtarget phrase.
Consequently, we can modify Equa-tion 2 to define the best segmentation of a sentencepair as:s?K?1 = argmaxK,sK1{M?m=1?mhm(eI1, sK1 , fJ1 )}(3)The identical models as in search are used: condi-tional phrase probabilities p(f?k|e?k) and p(e?k|f?k),within-phrase lexical probabilities, distance-basedreordering model as well as word and phrasepenalty.
A language model is not used in this case,as the system is constrained to the given target sen-tence and thus the language model score has noeffect on the alignment.In addition to the phrase matching on the sourcesentence, we also discard all phrase translationcandidates, that do not match any sequence in thegiven target sentence.Sentences for which the decoder can not findan alignment are discarded for the phrase modeltraining.
In our experiments, this is the case forroughly 5% of the training sentences.3.2 Leaving-one-outAs was mentioned in Section 2, previous ap-proaches found over-fitting to be a problem inphrase model training.
In this section, we de-scribe a leaving-one-out method that can improvethe phrase alignment in situations, where the prob-ability of rare phrases and alignments might beoverestimated.
The training data that consists ofNparallel sentence pairs fn and en for n = 1, .
.
.
, Nis used for both the initialization of the transla-tion model p(f?
|e?)
and the phrase model training.While this way we can make full use of the avail-able data and avoid unknown words during train-ing, it has the drawback that it can lead to over-fitting.
All phrases extracted from a specific sen-tence pair fn, en can be used for the alignment ofthis sentence pair.
This includes longer phrases,which only match in very few sentences in thedata.
Therefore those long phrases are trained tofit only a few sentence pairs, strongly overesti-mating their translation probabilities and failing togeneralize.
In the extreme case, whole sentenceswill be learned as phrasal translations.
The aver-age length of the used phrases is an indicator ofthis kind of over-fitting, as the number of match-ing training sentences decreases with increasingphrase length.
We can see an example in Figure2.
Without leaving-one-out the sentence is seg-mented into a few long phrases, which are unlikelyto occur in data to be translated.
Phrase boundariesseem to be unintuitive and based on some hiddenstructures.
With leaving-one-out the phrases areshorter and therefore better suited for generaliza-tion to unseen data.Previous attempts have dealt with the over-fitting problem by limiting the maximum phraselength (DeNero et al, 2006; Marcu and Wong,2002) and by smoothing the phrase probabilitiesby lexical models on the phrase level (Ferrer andJuan, 2009).
However, (DeNero et al, 2006) expe-rienced similar over-fitting with short phrases dueto the fact that the same word sequence can be seg-mented in different ways, leading to specific seg-mentations being learned for specific training sen-tence pairs.
Our results confirm these findings.
Todeal with this problem, instead of simple phraselength restriction, we propose to apply the leaving-one-out method, which is also used for languagemodeling techniques (Kneser and Ney, 1995).When using leaving-one-out, we modify thephrase translation probabilities for each sentencepair.
For a training example fn, en, we have toremove all phrases Cn(f?
, e?)
that were extractedfrom this sentence pair from the phrase counts that478Figure 2: Segmentation example from forced alignment.
Top: without leaving-one-out.
Bottom: withleaving-one-out.we used to construct our phrase translation table.The same holds for the marginal counts Cn(e?)
andCn(f?).
Starting from Equation 1, the leaving-one-out phrase probability for training sentence pair nispl1o,n(f?
|e?)
=C(f?
, e?)?
Cn(f?
, e?)C(e?)?
Cn(e?
)(4)To be able to perform the re-computation in anefficient way, we store the source and target phrasemarginal counts for each phrase in the phrase ta-ble.
A phrase extraction is performed for eachtraining sentence pair separately using the sameword alignment as for the initialization.
It is thenstraightforward to compute the phrase counts afterleaving-one-out using the phrase probabilities andmarginal counts stored in the phrase table.While this works well for more frequent obser-vations, singleton phrases are assigned a probabil-ity of zero.
We refer to singleton phrases as phrasepairs that occur only in one sentence.
For thesesentences, the decoder needs the singleton phrasepairs to produce an alignment.
Therefore we retainthose phrases by assigning them a positive proba-bility close to zero.
We evaluated with two differ-ent strategies for this, which we call standard andlength-based leaving-one-out.
Standard leaving-one-out assigns a fixed probability ?
to singletonphrase pairs.
This way the decoder will prefer us-ing more frequent phrases for the alignment, but isable to resort to singletons if necessary.
However,we found that with this method longer singletonphrases are preferred over shorter ones, becausefewer of them are needed to produce the target sen-tence.
In order to better generalize to unseen data,we would like to give the preference to shorterphrases.
This is done by length-based leaving-one-out, where singleton phrases are assigned theprobability ?(|f?
|+|e?|) with the source and targetTable 1: Avg.
source phrase lengths in forcedalignment without leaving-one-out and with stan-dard and length-based leaving-one-out.avg.
phrase lengthwithout l1o 2.5standard l1o 1.9length-based l1o 1.6phrase lengths |f?
| and |e?| and fixed ?
< 1.
In ourexperiments we set ?
= e?20 and ?
= e?5.
Ta-ble 1 shows the decrease in average source phraselength by application of leaving-one-out.3.3 Cross-validationFor the first iteration of the phrase training,leaving-one-out can be implemented efficiently asdescribed in Section 3.2.
For higher iterations,phrase counts obtained in the previous iterationswould have to be stored on disk separately for eachsentence and accessed during the forced alignmentprocess.
To simplify this procedure, we proposea cross-validation strategy on larger batches ofdata.
Instead of recomputing the phrase counts foreach sentence individually, this is done for a wholebatch of sentences at a time.
In our experiments,we set this batch-size to 10000 sentences.3.4 ParallelizationTo cope with the runtime and memory require-ments of phrase model training that was pointedout by previous work (Marcu and Wong, 2002;Birch et al, 2006), we parallelized the forcedalignment by splitting the training corpus intoblocks of 10k sentence pairs.
From the initialphrase table, each of these blocks only loads thephrases that are required for alignment.
The align-479ment and the counting of phrases are done sep-arately for each block and then accumulated tobuild the updated phrase model.4 Phrase Model TrainingThe produced phrase alignment can be given as asingle best alignment, as the n-best alignments oras an alignment graph representing all alignmentsconsidered by the decoder.
We have developedtwo different models for phrase translation proba-bilities which make use of the force-aligned train-ing data.
Additionally we consider smoothing bydifferent kinds of interpolation of the generativemodel with the state-of-the-art heuristics.4.1 ViterbiThe simplest of our generative phrase models esti-mates phrase translation probabilities by their rel-ative frequencies in the Viterbi alignment of thedata, similar to the heuristic model but with countsfrom the phrase-aligned data produced in trainingrather than computed on the basis of a word align-ment.
The translation probability of a phrase pair(f?
, e?)
is estimated aspFA(f?
|e?)
=CFA(f?
, e?)?f?
?CFA(f?
?, e?
)(5)where CFA(f?
, e?)
is the count of the phrase pair(f?
, e?)
in the phrase-aligned training data.
This canbe applied to either the Viterbi phrase alignmentor an n-best list.
For the simplest model, eachhypothesis in the n-best list is weighted equally.We will refer to this model as the count model aswe simply count the number of occurrences of aphrase pair.
We also experimented with weight-ing the counts with the estimated likelihood of thecorresponding entry in the the n-best list.
The sumof the likelihoods of all entries in an n-best list isnormalized to 1.
We will refer to this model as theweighted count model.4.2 Forward-backwardIdeally, the training procedure would consider allpossible alignment and segmentation hypotheses.When alternatives are weighted by their posteriorprobability.
As discussed earlier, the run-time re-quirements for computing all possible alignmentsis prohibitive for large data tasks.
However, wecan approximate the space of all possible hypothe-ses by the search space that was used for the align-ment.
While this might not cover all phrase trans-lation probabilities, it allows the search space andtranslation times to be feasible and still containsthe most probable alignments.
This search spacecan be represented as a graph of partial hypothe-ses (Ueffing et al, 2002) on which we can com-pute expectations using the Forward-Backward al-gorithm.
We will refer to this alignment as the fullalignment.
In contrast to the method described inSection 4.1, phrases are weighted by their poste-rior probability in the word graph.
As suggested inwork on minimum Bayes-risk decoding for SMT(Tromble et al, 2008; Ehling et al, 2007), we usea global factor to scale the posterior probabilities.4.3 Phrase Table InterpolationAs (DeNero et al, 2006) have reported improve-ments in translation quality by interpolation ofphrase tables produced by the generative and theheuristic model, we adopt this method and also re-port results using log-linear interpolation of the es-timated model with the original model.The log-linear interpolations pint(f?
|e?)
of thephrase translation probabilities are estimated aspint(f?
|e?)
=(pH(f?
|e?))1???(pgen(f?
|e?))(?
)(6)where ?
is the interpolation weight, pH theheuristically estimated phrase model and pgen thecount model.
The interpolation weight ?
is ad-justed on the development corpus.
When inter-polating phrase tables containing different sets ofphrase pairs, we retain the intersection of the two.As a generalization of the fixed interpolation ofthe two phrase tables we also experimented withadding the two trained phrase probabilities as ad-ditional features to the log-linear framework.
Thisway we allow different interpolation weights forthe two translation directions and can optimizethem automatically along with the other featureweights.
We will refer to this method as feature-wise combination.
Again, we retain the intersec-tion of the two phrase tables.
With good log-linear feature weights, feature-wise combinationshould perform at least as well as fixed interpo-lation.
However, the results presented in Table 5480Table 2: Statistics for the Europarl German-English dataGerman EnglishTRAIN Sentences 1 311 815Run.
Words 34 398 651 36 090 085Vocabulary 336 347 118 112Singletons 168 686 47 507DEV Sentences 2 000Run.
Words 55 118 58 761Vocabulary 9 211 6 549OOVs 284 77TEST Sentences 2 000Run.
Words 56 635 60 188Vocabulary 9 254 6 497OOVs 266 89show a slightly lower performance.
This illustratesthat a higher number of features results in a lessreliable optimization of the log-linear parameters.5 Experimental Evaluation5.1 Experimental SetupWe conducted our experiments on the German-English data published for the ACL 2008Workshop on Statistical Machine Translation(WMT08).
Statistics for the Europarl data aregiven in Table 2.We are given the three data sets TRAIN ,DEVand TEST .
For the heuristic phrase model, wefirst use GIZA++ (Och and Ney, 2003) to computethe word alignment on TRAIN .
Next we obtaina phrase table by extraction of phrases from theword alignment.
The scaling factors of the trans-lation models have been optimized for BLEU onthe DEV data.The phrase table obtained by heuristic extractionis also used to initialize the training.
The forcedalignment is run on the training data TRAINfrom which we obtain the phrase alignments.Those are used to build a phrase table accordingto the proposed generative phrase models.
After-ward, the scaling factors are trained on DEV forthe new phrase table.
By feeding back the newphrase table into forced alignment we can reiteratethe training procedure.
When training is finishedthe resulting phrase model is evaluated on DEVTable 3: Comparison of different training setupsfor the count model on DEV .leaving-one-out max phr.len.
BLEU TERbaseline 6 25.7 61.1none 2 25.2 61.33 25.7 61.34 25.5 61.45 25.5 61.46 25.4 61.7standard 6 26.4 60.9length-based 6 26.5 60.6and TEST .
Additionally, we can apply smooth-ing by interpolation of the new phrase table withthe original one estimated heuristically, retrain thescaling factors and evaluate afterwards.The baseline system is a standard phrase-basedSMT system with eight features: phrase transla-tion and word lexicon probabilities in both transla-tion directions, phrase penalty, word penalty, lan-guage model score and a simple distance-based re-ordering model.
The features are combined in alog-linear way.
To investigate the generative mod-els, we replace the two phrase translation prob-abilities and keep the other features identical tothe baseline.
For the feature-wise combinationthe two generative phrase probabilities are addedto the features, resulting in a total of 10 features.We used a 4-gram language model with modifiedKneser-Ney discounting for all experiments.
Themetrics used for evaluation are the case-sensitiveBLEU (Papineni et al, 2002) score and the trans-lation edit rate (TER) (Snover et al, 2006) withone reference translation.5.2 ResultsIn this section, we investigate the different as-pects of the models and methods presented be-fore.
We will focus on the proposed leaving-one-out technique and show that it helps in findinggood phrasal alignments on the training data thatlead to improved translation models.
Our finalresults show an improvement of 1.4 BLEU overthe heuristically extracted phrase model on the testdata set.In Section 3.2 we have discussed several meth-ods which aim to overcome the over-fitting prob-481Figure 3: Performance on DEV in BLEU of thecount model plotted against size n of n-best liston a logarithmic scale.lems described in (DeNero et al, 2006).
Table 3shows translation scores of the count model on thedevelopment data after the first training iterationfor both leaving-one-out strategies we have in-troduced and for training without leaving-one-outwith different restrictions on phrase length.
Wecan see that by restricting the source phrase lengthto a maximum of 3 words, the trained model isclose to the performance of the heuristic phrasemodel.
With the application of leaving-one-out,the trained model is superior to the baseline, thelength-based strategy performing slightly betterthan standard leaving-one-out.
For these experi-ments the count model was estimated with a 100-best list.The count model we describe in Section 4.1 esti-mates phrase translation probabilities using countsfrom the n-best phrase alignments.
For smaller nthe resulting phrase table contains fewer phrasesand is more deterministic.
For higher values ofn more competing alignments are taken into ac-count, resulting in a bigger phrase table and asmoother distribution.
We can see in Figure 3that translation performance improves by movingfrom the Viterbi alignment to n-best alignments.The variations in performance with sizes betweenn = 10 and n = 10000 are less than 0.2 BLEU.The maximum is reached for n = 100, which weused in all subsequent experiments.
An additionalbenefit of the count model is the smaller phrasetable size compared to the heuristic phrase extrac-tion.
This is consistent with the findings of (Birchet al, 2006).
Table 4 shows the phrase table sizesfor different n. With n = 100 we retain only 17%of the original phrases.
Even for the full model, weTable 4: Phrase table size of the count model fordifferent n-best list sizes, the full model and forheuristic phrase extraction.N # phrases % of full table1 4.9M 5.310 8.4M 9.1100 15.9M 17.21000 27.1M 29.210000 40.1M 43.2full 59.6M 64.2heuristic 92.7M 100.0do not retain all phrase table entries.
Due to prun-ing in the forced alignment step, not all translationoptions are considered.
As a result experimentscan be done more rapidly and with less resourcesthan with the heuristically extracted phrase table.Also, our experiments show that the increased per-formance of the count model is partly derived fromthe smaller phrase table size.
In Table 5 we can seethat the performance of the heuristic phrase modelcan be increased by 0.6 BLEU on TEST by fil-tering the phrase table to contain the same phrasesas the count model and reoptimizing the log-linearmodel weights.
The experiments on the number ofdifferent alignments taken into account were donewith standard leaving-one-out.The final results are given in Table 5.
We cansee that the count model outperforms the base-line by 0.8 BLEU on DEV and 0.9 BLEU onTEST after the first training iteration.
The perfor-mance of the filtered baseline phrase table showsthat part of that improvement derives from thesmaller phrase table size.
Application of cross-validation (cv) in the first iteration yields a perfor-mance close to training with leaving-one-out (l1o),which indicates that cross-validation can be safelyapplied to higher training iterations as an alterna-tive to leaving-one-out.
The weighted count modelclearly under-performs the simpler count model.A second iteration of the training algorithm showsnearly no changes in BLEU score, but a small im-provement in TER.
Here, we used the phrase tabletrained with leaving-one-out in the first iterationand applied cross-validation in the second itera-tion.
Log-linear interpolation of the count modelwith the heuristic yields a further increase, show-ing an improvement of 1.3 BLEU onDEV and 1.4BLEU on TEST over the baseline.
The interpo-482Table 5: Final results for the heuristic phrase tablefiltered to contain the same phrases as the countmodel (baseline filt.
), the count model trained withleaving-one-out (l1o) and cross-validation (cv),the weighted count model and the full model.
Fur-ther, scores for fixed log-linear interpolation of thecount model trained with leaving-one-out with theheuristic as well as a feature-wise combination areshown.
The results of the second training iterationare given in the bottom row.DEV TESTBLEU TER BLEU TERbaseline 25.7 61.1 26.3 60.9baseline filt.
26.0 61.6 26.9 61.2count (l1o) 26.5 60.6 27.2 60.5count (cv) 26.4 60.7 27.0 60.7weight.
count 25.9 61.4 26.4 61.3full 26.3 60.0 27.0 60.2fixed interpol.
27.0 59.4 27.7 59.2feat.
comb.
26.8 60.1 27.6 59.9count, iter.
2 26.4 60.3 27.2 60.0lation weight is adjusted on the development setand was set to ?
= 0.6.
Integrating both modelsinto the log-linear framework (feat.
comb.)
yieldsa BLEU score slightly lower than with fixed inter-polation on both DEV and TEST .
This mightbe attributed to deficiencies in the tuning proce-dure.
The full model, where we extract all phrasesfrom the search graph, weighted with their poste-rior probability, performs comparable to the countmodel with a slightly worse BLEU and a slightlybetter TER.6 ConclusionWe have shown that training phrase models canimprove translation performance on a state-of-the-art phrase-based translation model.
This isachieved by training phrase translation probabil-ities in a way that they are consistent with theiruse in translation.
A crucial aspect here is the useof leaving-one-out to avoid over-fitting.
We haveshown that the technique is superior to limitingphrase lengths and smoothing with lexical prob-abilities alone.While models trained from Viterbi alignmentsalready lead to good results, we have demonstratedthat considering the 100-best alignments allows tobetter model the ambiguities in phrase segmenta-tion.The proposed techniques are shown to be supe-rior to previous approaches that only used lexicalprobabilities to smooth phrase tables or imposedlimits on the phrase lengths.
On the WMT08 Eu-roparl task we show improvements of 0.9 BLEUpoints with the trained phrase table and 1.4 BLEUpoints when interpolating the newly trained modelwith the original, heuristically extracted phrase ta-ble.
In TER, improvements are 0.4 and 1.7 points.In addition to the improved performance, thetrained models are smaller leading to faster andsmaller translation systems.AcknowledgmentsThis work was partly realized as part of theQuaero Programme, funded by OSEO, FrenchState agency for innovation, and also partly basedupon work supported by the Defense AdvancedResearch Projects Agency (DARPA) under Con-tract No.
HR001-06-C-0023.
Any opinions,ndings and conclusions or recommendations ex-pressed in this material are those of the authors anddo not necessarily reect the views of the DARPA.ReferencesAlexandra Birch, Chris Callison-Burch, Miles Os-borne, and Philipp Koehn.
2006.
Constraining thephrase-based, joint probability statistical translationmodel.
In smt2006, pages 154?157, Jun.Phil Blunsom, Trevor Cohn, and Miles Osborne.
2008.A discriminative latent variable model for statisti-cal machine translation.
In Proceedings of ACL-08:HLT, pages 200?208, Columbus, Ohio, June.
Asso-ciation for Computational Linguistics.P.
F. Brown, V. J. Della Pietra, S. A. Della Pietra, andR.
L. Mercer.
1993.
The mathematics of statisticalmachine translation: parameter estimation.
Compu-tational Linguistics, 19(2):263?312, June.John DeNero and Dan Klein.
2008.
The complexityof phrase alignment problems.
In Proceedings of the46th Annual Meeting of the Association for Compu-tational Linguistics on Human Language Technolo-gies: Short Papers, pages 25?28, Morristown, NJ,USA.
Association for Computational Linguistics.John DeNero, Dan Gillick, James Zhang, and DanKlein.
2006.
Why Generative Phrase Models Un-derperform Surface Heuristics.
In Proceedings of the483Workshop on Statistical Machine Translation, pages31?38, New York City, June.John DeNero, Alexandre Buchard-Co?te?, and DanKlein.
2008.
Sampling Alignment Structure undera Bayesian Translation Model.
In Proceedings ofthe 2008 Conference on Empirical Methods in Natu-ral Language Processing, pages 314?323, Honolulu,October.Nicola Ehling, Richard Zens, and Hermann Ney.
2007.Minimum bayes risk decoding for bleu.
In ACL ?07:Proceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,pages 101?104, Morristown, NJ, USA.
Associationfor Computational Linguistics.Jesu?s-Andre?s Ferrer and Alfons Juan.
2009.
A phrase-based hidden semi-markov approach to machinetranslation.
In Procedings of European Associationfor Machine Translation (EAMT), Barcelona, Spain,May.
European Association for Machine Translation.Reinhard Kneser and Hermann Ney.
1995.
ImprovedBacking-Off for M-gram Language Modelling.
InIEEE Int.
Conf.
on Acoustics, Speech and SignalProcessing (ICASSP), pages 181?184, Detroit, MI,May.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of the 2003 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics on Human Language Technology - Vol-ume 1, pages 48?54, Morristown, NJ, USA.
Associ-ation for Computational Linguistics.Percy Liang, Alexandre Buchard-Co?te?, Dan Klein, andBen Taskar.
2006.
An End-to-End DiscriminativeApproach to Machine Translation.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and the 44th annual meeting of the As-sociation for Computational Linguistics, pages 761?768, Sydney, Australia.Daniel Marcu and William Wong.
2002.
A phrase-based, joint probability model for statistical machinetranslation.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP-2002), July.J.A.
Nelder and R. Mead.
1965.
A Simplex Methodfor Function Minimization.
The Computer Journal),7:308?313.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51,March.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine trans-lation.
Computational Linguistics, 30(4):417?449,December.F.J.
Och, C. Tillmann, and H. Ney.
1999.
Improvedalignment models for statistical machine translation.In Proc.
of the Joint SIGDAT Conf.
on EmpiricalMethods in Natural Language Processing and VeryLarge Corpora (EMNLP99), pages 20?28, Univer-sity of Maryland, College Park, MD, USA, June.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting on Association for Computa-tional Linguistics, pages 311?318, Morristown, NJ,USA.
Association for Computational Linguistics.Wade Shen, Brian Delaney, Tim Anderson, and RaySlyh.
2008.
The MIT-LL/AFRL IWSLT-2008 MTSystem.
In Proceedings of IWSLT 2008, pages 69?76, Hawaii, U.S.A., October.Matthew Snover, Bonnie Dorr, Rich Schwartz, LinneaMicciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proc.
of AMTA, pages 223?231, Aug.Roy Tromble, Shankar Kumar, Franz Och, and Wolf-gang Macherey.
2008.
Lattice Minimum Bayes-Risk decoding for statistical machine translation.In Proceedings of the 2008 Conference on Empiri-cal Methods in Natural Language Processing, pages620?629, Honolulu, Hawaii, October.
Associationfor Computational Linguistics.N.
Ueffing, F.J. Och, and H. Ney.
2002.
Genera-tion of word graphs in statistical machine translation.In Proc.
of the Conference on Empirical Methodsfor Natural Language Processing, pages 156?163,Philadelphia, PA, USA, July.484
