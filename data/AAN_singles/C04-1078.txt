Cascading Use of Soft and Hard Matching Pattern Rules for WeaklySupervised Information ExtractionJing XiaoSchool of Computing,National University ofSingapore, 117543xiaojing@comp.nus.edu.sgTat-Seng ChuaSchool of Computing,National University ofSingapore, 117543chuats@comp.nus.edu.sgHang CuiSchool of Computing,National University ofSingapore, 117543cuihang@comp.nus.edu.sgAbstractCurrent rule induction techniques based on hardmatching (i.e., strict slot-by-slot matching) tend tofare poorly in extracting information from naturallanguage texts, which often exhibit greatvariations.
The reason is that hard matchingtechniques result in relatively high precision butlow recall.
To tackle this problem, we takeadvantage of the newly proposed soft pattern ruleswhich offer high recall through the use ofprobabilistic matching.
We propose abootstrapping framework in which soft and hardmatching pattern rules are combined in a cascadingmanner to realize a weakly supervised ruleinduction scheme.
The system starts with a smallset of hand-tagged instances.
At each iteration, wefirst generate soft pattern rules and utilize them totag new training instances automatically.
We thenapply hard pattern rule induction on the overalltagged data to generate more precise rules, whichare used to tag the data again.
The process can berepeated until satisfactory results are obtained.
Ourexperimental results show that our bootstrappingscheme with two cascaded learners approaches theperformance of a fully supervised informationextraction system while using much fewer hand-tagged instances.1 IntroductionInformation Extraction (IE) aims to extract specificinformation items of interest from free or semi-structured texts, and pattern rule induction is oneof the most common techniques for IE tasks(Muslea, 1999).
There has been much work inlearning extraction pattern rules from tagged data,e.g., AutoSlog-TS (Riloff, 1996), WHISK(Soderland, 1999) and LP2 (Ciravegna, 2001).
In atypical IE system, generalized pattern rules areusually represented as regular expressions andmatched against test instances through exactmatching for each slot, which we call hardmatching.
Utilizing hard matching pattern rulescould obtain precise results from test instances.However, the approach is problematic in dealingwith natural language text, such as news articles,which often exhibits great variations in both lexicaland syntactic constructions.
For instance, in theterrorism domain, given a common rule ?<victim>be kidnapped by ?
?, hard matching pattern rulescannot pick up the instance ?<victim> , kidnappedby ??
due to the mismatch in only one token.Such hard matching techniques often result in lowrecall.
To achieve flexibility in pattern matchingfor natural language texts, soft matching patternrules have been proposed for question answering(Cui, et al, 2004).
Soft pattern rules match testinstances using a probabilistic model to betteraccommodate variations in expressions.
However,differing from the question answering problem, theIE task needs to precisely locate the boundaries ofthe extracted slots.
As such, soft pattern rules maynot meet the precision requirement of the task.In this paper, we aim to minimize the number ofhand-tagged training instances needed to start thelearning process by adopting a bootstrappingstrategy such as that proposed in Riloff and Jones(1999).
In contrast to the existing work, wepropose a weakly supervised IE framework whichtakes advantages of both soft and hard matchingpattern rules in both the training and test phases.Starting with only a small set of hand-taggedtraining instances, we first generate a set of softpattern rules and utilize them to tag more traininginstances.
Next, we apply a hard matching patternrule induction algorithm, GRID (Xiao, et al,2003), over both manually and automaticallytagged instances to generalize precise hard-matching rules.
These hard pattern rules areutilized to tag training instances for soft patternrule generation in the next iteration.
The processruns iteratively till the termination criteria are met.At the end of the training process, we obtain twosets of pattern rules, namely the hard and softpattern rules.
During the test phase, both sets ofpattern rules are used in a cascaded way, with hardpattern rules followed by soft pattern rules, toextract target slots from new documents.
We haveconducted two experiments on both semi-structured and free texts to demonstrate theeffectiveness of our method.
The experimentalresults show that the bootstrapping scheme withtwo cascaded pattern rule learners could achieve aperformance close to that obtained by fullysupervised learning while using only 5~10% of thehand-tagged data.The main contribution of our work is inincorporating soft matching pattern rules in thebootstrapping framework.
Rooted in instance-based learning, soft pattern rules are moreappropriate in dealing with sparse data (Cui, et al,2004), and thus can be learned from a relativelysmall number of training instances to start thebootstrapping process.
Moreover, in test phase,soft pattern rules are expected to cover moreunseen instances, which are likely to be missed byhard-matching rules, with its flexible matchingmechanism.The rest of the paper is organized as follows.Section 2 presents the design of our system.Section 3 describes the details of data preparation,soft pattern matching, hard pattern rule inductionand the application of the two pattern rules on newtest instances.
Section 4 presents the experimentalevaluation.
We review other work in Section 5 andconclude the paper in Section 6.2 System DesignFigure 1 shows the overall system architecture ofour IE system.
The training phase of the system iscarried out as follows:(a) We take a small set of hand-tagged instances(seed instances) provided by the user.
(b) We generate soft pattern rules using the seedinstances, and denote the soft pattern rules as SPi.
(c) We apply the learned soft pattern rules (SPi) toautomatically tag unannotated data.
We employ asimple cut-off strategy that keeps only the highly-ranked instances by the soft pattern rules.
(d) We generate hard pattern rules using GRIDover the automatically tagged instances and seedinstances.
The resulting hard pattern rules aredenoted as HPi.
(e) If the termination condition is satisfied, theprocess ends with a set of learned soft and hardpattern rules.
Otherwise, the hard pattern rules HPiare used to tag the training data again.
We start anew round of training from Step (b) using thenewly tagged training instances and seed instances.In the test phase, we apply both the hard and softpattern rules to match against test instances.Specifically, soft matching pattern rules wouldassign a probabilistic score to an instance that isnot matched by any of the hard matching patternrules.
Only those fields that are matched by thehard pattern rules or have high scores in softpattern matching will be extracted.Figure 1: Architecture of our IE system3 Soft and Hard Pattern Rule Learning3.1 Data PreparationBefore pattern rule learning commences, we pre-process the training and test instances by using anatural language chunker 1  to perform part-of-speech (PoS) tagging and chunking.
We also use arule-based named entity tagger (Chua and Liu,2002) to capture semantic entities.
Given a taggedinstance, we consider the left and right k chunksaround the tagged slot as the context:<c-k>?<c-2><c-1>tagged_slot<c+1><c+2>?<c+k>Here <ci> {i=-k to +k} represents the contextualchunks (or slots) of the tagged slot, where k is thenumber of contextual slots considered.
<ci> can beof various feature types, namely words,punctuations, chunking tags like verb and nounphrases, or semantic classes.
We perform selectivesubstitution to generalize the specific terms in eachslot so as to make the learned pattern rules generalenough to be applied to other instances.
Table 1shows the substitution heuristics employed in oursystem with examples.
(1)Figure 2 gives five examples of original traininginstances for ?starting time?
in the seminarannouncement domain.
We substitute the more1  We use NLProcessor, a commercial parser fromInfogistics Ltd. http://www.infogistics.com/.general syntactic or semantic classes for the lexicaltokens according to the heuristics in Table 1.Tokens Substitution Examples9 types ofnamedentitiesNP_Person,NP_Location,NP_Organization,NP_Date,NP_Day,NP_Time,NP_Percentage,NP_Money,NP_Number.?Friday??NP_Day?Feb.27?
?NP_DateNounPhrase NP_HeadNoun?the seminar?
?NP_seminarVerb Phrase(passive oractive)VPpas_RootVerb,VPact_RootVerb?will speak?
?VPact_speak,?will be held?
?VPpas_holdPrepositionPhrasePP?in civilian clothes??
PPAdjectivalandadverbialmodifiersTo be deletedAll otherwords andpunctuationsNo substitution ?Time?, ?at?, ?by?, etc.
are unchanged.Table 1: Substitution heuristics3.2 Soft Matching Pattern RulesSoft pattern rules have been successfully applied totext mining (Nahm and Mooney, 2001) andquestion answering (Cui, et al, 2004).
We employa variation of the soft pattern rules generation andmatching method presented in Cui, et al (2004).We expect soft pattern rules to offer highercoverage in matching against a variety of instancesin both the training and test phases.For each type of tagged slot (Slot0) such as stimein Figure 2, we accumulate all the tagged instancesand align them according to the positions of Slot0.As a result, we obtain a virtual vector Parepresenting the contextual soft pattern rule as:<Slot-k, ?
, Slot-2, Slot-1, Slot0, Slot1, Slot2, ?, Slotk:Pa>                                                                      (2)where Sloti is a vector of tokens occurring in thatslot with their probabilities of occurrence:<(tokeni1, weighti1), (tokeni2, weighti2) ?.
(tokenim,weightim): Sloti>                                                   (3)Here, tokenij denotes any word, punctuation,syntactic or semantic tag contained in Sloti, andweightij gives the proportion of occurrences of thejth token to the ith slot.
Figure 3 shows thegenerated soft pattern rules for the examples givenin Figure 2.
(1) Training instances:Time : <stime> NP_Time </stime>VPact_be at <stime> NP_Time </stime>NP_Day , NP_Date <stime> NP_Time </stime> - NP_TimeVPact_be at <stime> NP_Time </stime> , NP_Day , NP_DateTime : <stime> NP_Time </stime> - NP_Time(2) Soft pattern rules based on the instances:??
<Slot-2>          <Slot-1>            <Slot0>           <Slot1> ?...Time 0.4VPact_be 0.4, 0.2: 0.4at 0.4NP_Date  0.2NP_Time  1 - 0.67, 0.33Figure 3: An excerpt of soft pattern rulesWhat results from the generalization process is avirtual vector Pa representing the soft pattern rule.The soft pattern vector Pa is then used to computethe degree of match for the unseen instances.
Theunseen instances are first pre-processed with theidentical procedures as outlined in Section 3.1.Using the same window size k, the token fragmentS surrounding the potential slot is derived:(1) Original instances for slot <stime>:Time : <stime> 2:30 PM </stime>?
will be at <stime> 3 pm </stime> ?
?Friday, February 17 <stime> 12:00pm </stime> - 1:00pm?
will be at <stime> 4pm </stime> , Monday, Feb. 27 ?Time: <stime> 12:00 PM </stime> - 1:30 PM(2) Substituted instances:Time : <stime> NP_Time </stime>VPact_be at <stime> NP_Time </stime>NP_Day , NP_Date <stime> NP_Time </stime> - NP_TimeVPact_be at <stime> NP_Time </stime> , NP_Day , NP_DateTime : <stime> NP_Time </stime> - NP_Time<token-k,?, token-2, token-1, Potential_Slot, token1,token2, ?, tokenk: S>                                           (4)The degree of match for the unseen instanceagainst the soft pattern rules is measured by thesimilarity between the vector S and the virtual softpattern vector Pa.
In particular, the match degree isthe combination of the individual slot contentsimilarities and the fidelity degree of slotsequences measured by a bi-gram model (Cui, etal., 2004).Figure 2: Illustration of generalizing instancesWhen applying the soft pattern rules toautomatically tag training instances, for eachpotential slot, we assign a target tag whose softpattern rule gives the highest score beyond a pre-defined threshold.3.3 Hard Pattern Rule InductionWe employ a pattern rule induction algorithmcalled GRID (Xiao, et al, 2003) to generalize thehard pattern rules over all instances hand-taggedby users and automatically annotated by softpattern rules.
GRID is a supervised coveringalgorithm.
It uses chunks as contextual slots andconsiders a context size of k slots around thetagged item as definition in Equation (1).Given the cluster of training instances for aspecific slot type, GRID aligns all the instancesaccording to the central slot (Slot0) as is done insoft pattern rules.
For each context slot, we storeall possible representations of slot units as listed inTable 1 at the levels of lexical, syntactic andsemantic simultaneously.
Thus, we obtain a globalcontext feature representation for the wholetraining corpus as shown in Figure 4.
GRIDrecords the occurrences of the common slotfeatures at a specific position as eij (i = -k, ?
,  -1,0, 1, ?, k; jth feature for Sloti).inst.1: Slot-k, ?, Slot-2, Slot-1, Slot0, Slot1, Slot2, ?, Slotkinst.2: Slot-k, ?, Slot-2, Slot-1, Slot0, Slot1, Slot2, ?, Slotk.
.
?
.
.
.
.
.
?
..         .
?
.
.
.
.
.
?
..         .
?
.
.
.
.
.
?
.inst.h: Slot-k, ?, Slot-2, Slot-1, Slot0, Slot1, Slot2, ?, SlotkGRID generates a pattern rule rk(f) by adding slotfeatures into the feature set f. The quality of rk(f) isdetermined not only by its coverage in the positivetraining set but also by the number of instances inthe negative set that it covers which would beregarded as errors.
We define the remaininginstances which are not annotated by human andsoft pattern rules as negative instances.We use a modified Laplacian expected error(Soderland, 1999) to define the quality of the ruleas follows:17.01))((21 +?+++=kkkkk ppnnfrLaplacianwhere pk1 denotes the number of instances coveredby rule rk(f) in the manually annotated set, and pk2denotes the number of instances covered by therule rk(f) in the automatically annotated set.
nk isthe number of negative examples or errors coveredby the rule.
We consider all the manuallyannotated instances as correctly tagged and thuswe put more weight on them than on theautomatically annotated data set.Instead of generalizing a rule from a specificinstance as is done in most existing pattern ruleinduction algorithms, GRID examines the globalfeature distribution on the whole set of trainingexamples in order to make better decision on ruleinduction.
Each time, GRID selects top w features(in terms of the eij values) and selects slot feature fijwith the minimum Laplacian value of the rule(rk(f?fij)) according to Equation (5) to inducepattern rules (Xiao, et  al., 2003).We use GRID to generate rules that cover allseed instances and discard some rules generatedfrom the automatically tagged instances whoseLaplacian value is greater than a preset threshold.3.4 Cascading Matching of Hard and SoftPattern RulesAfter we have obtained the set of hard pattern rulesand the set of soft pattern rules through thebootstrapping rule induction process, we applyboth sets of rules in a cascaded way to assignappropriate tag to potential slots in new instances.The tag assigned to the given test instance t isselected by:1) tagg   matched by GRID ruleg;2) If not matched by any GRID rule,tagi  ?>?
)|Pr(maxarg iPaPaPatiWe apply the high-precision hard pattern rulesgenerated by GRID first.
In this case, we assigntagg to the instance if it matches ruleg.
In order toincrease the coverage of the hard pattern rules, weallow up to one shift in the context vectors of newtest instances when matching the instances againstthe hard pattern rules.For the remaining test instances that are notmatched by any of the hard pattern rules, we scorethem using the soft pattern rules.
A test instance isassigned tagi if it has the highest conditionalprobability of having t given the soft pattern rule i(represented by vector Pai) which is greater than apre-defined threshold ?
among all the soft patternrules.4 EvaluationTo verify the generality and effectiveness of ourbootstrapping framework, we have conducted twoexperiments on free and semi-structured texts.
Inour supervised IE system using GRID (Xiao, et al,2003), we had done some trial experiments toexamine the effect of varying the different contextlength k, and found the IE performance becamestable when the context length reached 4.
As such,we set the context length k to 4 for all subsequentexperiments.4.1 Results on free text corpusThe first evaluation was conducted on theterrorism domain using the MUC-4 free textcorpus (MUC-4, 1992).
We employed the sameevaluation measures as that in (Riloff, 1996; Xiao,et al, 2003).
The target extracted slots were?perpetrator?
(Perp.
), ?victim?
(Vic.)
and ?target?(Tar.).
We varied the number of the human-annotated instances from the 772 relevantPos.e-kj ?
e-2j e-1j e0j e1j e2j ?
ekj?
?
?
?Figure 4: Global distribution of positive instances(5)documents set (the standard training documents forMUC-4 plus TST1 and TST2) used in supervisedIE learning.
The manual annotation was guided bythe associated answer keys given in the MUC-4corpus.
During testing, we used the 100 textscomprising 25 relevant and 25 irrelevant texts fromthe TST3 test set, and 25 relevant and 25 irrelevanttexts from the TST4 test set.Following the procedure discussed in Section 2,we repeated the automated annotation processseveral times (i ?1 in Figure 1).
To examine thevariation of performance along with the changingof the number of iterations, we plotted the averageF1 measures of the three target slots against theiteration number (see Figure 5).
We also varied thenumber of manually tagged instances that wereutilized as seed instances for starting thebootstrapping process.
As can be seen in Figure 5,the results improved as the number of iterationsincreased.
The system achieved a steadyperformance when the number of iterationsreached four.
Accordingly in the next experiments,we considered the system?s performance based onfour bootstrapping iterations.40455055601 2 3 4 5 6 7 8 9 10IterationAverageF1measure5% manually annotated instances10% manually annotated instances20% manually annotated instancesFigure 5: Effect of the number of iterationsTable 2 shows the performance of the system onthe test data in terms of F1-measure (withrecall/precision value in the brackets) using variousamounts of manually tagged data after fouriterations.
To demonstrate the effectiveness of thecombination of hard and soft pattern rules, we alsoran four iterations using only soft pattern rules (SP)and another four with only GRID rules.From Table 2, we can draw the followingconclusions:(a) The cascaded learner by combining SP andGRID outperforms the learner SP or GRID alone.The soft pattern learner (SP) alone cannot achievegood precision while the hard pattern learner(GRID) alone cannot achieve high recall with asmall set of hand-annotated instances.Perp.
Vic.
Tar.5%(SP) 36 (42/32)45(49/42)42(47/38)5%(GRID) 34 (35/33)44(40/49)39(36/43)5%(SP+GRID) 47 (49/45)58(59/57)50(50/50)10%(SP) 38 (45/33)46(51/42)45(49/42)10%(GRID) 37 (39/35)46(41/52)44(41/47)10%(SP+GRID) 50 (53/47)61(63/59)53(52/54)20%(SP) 40 (46/35)48(54/43)47(50/44)20%(GRID) 40 (41/39)47(44/50)47(45/49)20%(SP+GRID) 51 (52/50)62(63/61)54(55/53)AutoSlog-TS 38 (53/30)48(62/39)47(58/39)supervised(GRID) 52(48/57)62(58/67)56(51/62)Results presented in terms of F1(recall/precision).Table 2: Results on free text domain(b) Compared with another weakly supervised IEsystem in the same domain, AutoSlog-TS (Riloff,1996), our cascaded learner outperforms it with theuse of only 5% of the manually tagged instances.
(c) As the percentage of the hand-annotatedinstances increases from 5% to 20%, theperformance of the cascaded learner (SP+GRID)increases steadily, indicating that the bootstrappingprocess is stable and consistent.
(d) With 20% of hand-tagged training instances,the performance of the cascaded learnerapproaches that of the fully supervised IE tagger.When more manually tagged instances (>20%) areused, the performance of the cascaded learnerbecomes steady.
(e) Looking at the instances automatically taggedby the soft pattern rules, we found that about 75%instances are correctly annotated in the first andsecond iteration.
The percentage of correctlytagged instances by soft pattern rules increases to90% when the bootstrapping process runs for fourtimes.
The percentage increase verifies that ourautomated annotation can provide relativelyaccurate training instances for later rule induction.Nevertheless, our system missed some caseswhich needed deeper NLP analysis.
For example,given a test sentence ?THEY ARE THE TOPMILITARY AND POLITICAL FIGURES INALFREDO CRISTIANI'S ADMINISTRATION.
?, thesystem could not identify ?ALFREDO CRISTIAN?SADMINISTRATION?
as the ?perpetrator?.
If wecould associate the previously found ?perpetrator?
(maybe located far away) to ?they?, then we mightbe able to infer that the ?ALFREDO CRISTIAN?SADMINISTRATION?
is the ?perpetrator?
too.4.2 Results on semi-structured corpusThe second experiment was conducted on semi-structured text documents.
We used the CMUseminar announcements2 for the evaluation.
The IEtask for this domain is to extract the entities of?speaker?
(SP), ?location?
(LOC), ?starting time?
(ST), and ?ending time?
(ET) from a seminarannouncement.
There were 485 seminarannouncements.
In the supervised IE experiments,we made five runs and in each run we used onehalf for training and the other half for testing.Similarly, to evaluate our weakly supervisedlearning framework, we did five trials as well.
Ineach run, we varied the percentage of manuallyannotated instances for training in the supervisedexperiments.
Table 3 shows the performance (theaverage F1 measure and recall/precision for fiveruns) of the system with different percentage ofmanually tagged instances used to start thetraining.
We also compare the performancesbetween the single learners and the cascadedlearner.
All results are based on four bootstrappingiterations.SP LOC ST ET5%(SP) 70 (74/66)65(70/61)94(95/93)90(93/88)5%(GRID) 68 (65/72)61(59/64)93(91/94)89(86/92)5%(SP+GRID) 82 (83/81)73(74/72)98(98/98)94(96/92)10%(SP) 72 (75/70)68(72/64)96(96/95)93(94/92)10%(GRID) 72 (67/77)67(63/72)95(94/96)93(91/96)10%(SP+GRID) 84 (84/83)75(75/74)99(99/99)95(97/94)20%(SP) 75  (77/74)71(75/67)97(97/97)95(96/95)20%(GRID) 75 (69/82)71(66/77)97(95/99)95(94/96)20%(SP+GRID) 85 (85/85)76(76/75)99(99/99)96(97/95)supervised(GRID)86(84/88)76(73/80)99(99/100)96(95/97)Results presented in terms of F1(recall/precision).Table 3: Results on semi-structured dataFrom Table 3, we make the followingobservations:(a) The cascaded learner with two pattern learnerssignificantly outperforms the learner SP or GRIDalone as in the case of free text corpus.
With 10%of hand-tagged instances, the cascaded learner(SP+GRID) approaches the performance of thefully supervised IE tagger.
Also the performance ofthe cascaded learner increases steadily when thenumber of hand-tagged instances increases from5% to 20%.
(b) With more hand-annotated instances (>20%),the performance of the bootstrapping system withthe cascading use of SP and GRID becomes stableand consistent.2 http://www.isi.edu/info-agents/RISE/repository.html(c) Soft pattern rules tag 90% of the instancescorrectly, as we found out in our random checks.The lower performance of our system on the?location?
slot is mainly due to the use of a generalnamed entity recognizer which is good atidentifying common locations such as cities,mountains etc.
In seminar announcements, manylocations are room numbers such as ?WeH 8220?
;thus, we missed out some seminar venues.5 Related WorkMany hard pattern rule inductive learning systemshave been developed for information extractionfrom free texts or semi-structured texts.Specifically, AutoSlog-TS (Riloff, 1996) generatesextraction patterns using annotated text and a set ofheuristic rules and it eliminates the dependency ontagged text and only requires the pre-classifiedtexts as input.
WHISK (Soderland, 1999) inducesmulti-slot rules from a training corpus top-down.
Itis designed to handle text styles ranging fromhighly structured text to free text.
WHISKperforms rule induction starting from a randomlyselected seed instance.
(LP)2 (Ciravegna, 2001) is acovering algorithm for adaptive IE systems thatinduces symbolic rules.
In (LP)2, training isperformed in two steps: first, a set of tagging rulesis learned to identify the boundaries of slots; next,additional rules are induced to correct mistakes inthe first step of tagging.
In contrast to their work,GRID utilizes global feature distribution to inducepattern rules and uses chunk as the context unit.Nahm and Mooney (2001) proposed the learningof soft matching rules from texts by combiningrule-based and instance-based learning.
Words ineach slot are generalized by traditional ruleinduction techniques and test instances arematched to the rules by their cosine similarities.The learning of soft pattern rules in this paperaugments the soft matching method advocated byNahm and Mooney (2001) by combining lexicaltokens alongside syntactic and semantic featuresand adopting a probabilistic framework thatcombines slot content and sequential fidelity incomputing the degree of pattern match.The bootstrapping scheme using the co-training(Blum and Mitchell, 1998) technique has beenwidely explored for IE tasks in recent years.Collins and Singer (1999) presented severaltechniques using co-training schemes for NamedEntity (NE) extraction seeded by a small set ofmanually crafted NE rules.
Riloff and Jones (1999)presented a multi-level bootstrapping algorithmthat generates both the semantic lexicon andextraction patterns simultaneously.
Yangarber(2003) proposed a counter-training approach toprovide natural stopping criteria for unsupervisedlearning.Our framework of combining two patternlearners is close to Niu, et al (2003) in which twosuccessive learners are used to learn named entitiesclassifiers starting from a small set of concept-based seed words.
The bootstrapping procedure isimplemented as training a decision list and anHMM classifier sequentially.
The HMM classifieruses the training corpus automatically, tagged bythe first learner, i.e., the decision list learner.
Ourwork differs from Niu, et al (2003) in two ways.First, we repeat the automatic annotation processuntil it satisfies the stopping criteria.
Second, weapply different patterns (hard and soft patternrules) in both the training and test phases.6 ConclusionWe have presented a novel bootstrappingapproach for information extraction by thecascading use of soft and hard pattern rules.
Ourframework takes advantages of the high-recall ofsoft pattern rules and the high-precision of hardpattern rules.
We use soft pattern rules toautomatically annotate more training instances soas to provide a more comprehensive basis for hardpattern rule induction.
The integration of softpattern matching in the extraction phase alsoprovides more target entities from test instancesthat would otherwise be missed by hard patternmatching.
With much less manual input, theproposed bootstrapping system approaches theperformance obtained by fully supervised learningon both semi-structured and free texts corpora.7 AcknowledgementThe authors would like to thank Alexia Leongfor proofreading this paper.
The third author issupported by Singapore Millennium FoundationScholarship (ref no.
2003-SMS-0230).ReferencesA.
Blum and T. Mitchell.
1998.
CombiningLabeled and Unlabeled Data with Co-training.Proceedings of the 11th Annual Conference onComputational Learning Theory (COLT-98),pages 92-100.T.-S. Chua and J. Liu.
2002.
Learning PatternRules for Chinese Named Entity Extraction.Proceedings of the 18th National Conference onArtificial Intelligence.
(AAAI-02), pages 411-418.F.
Ciravegna.
2001.
Adaptive InformationExtraction from Text by Rule Induction andGeneralisation.
Proceedings of the 17thInternational Joint Conference on ArtificialIntelligence (IJCAI-2001),  pages 1251-1256.M.
Collins and Y.
Singer.
1999.
UnsupervisedModels for Named Entity Classification.Proceedings of the 1999 Joint SIGDATConference on EMNLP and VLC.H.
Cui, M.-Y.
Kan and T.-S. Chua.
2004.Unsupervised Learning of Soft Patterns forDefinitional Question Answering.
Proceedingsof 13th World Wide Web Conference.
(WWW-04),pages 90-99.MUC-4, 1992.
Proceedings of the Fourth MessageUnderstanding Conference.
San Mateo, CA:Morgan Kaufmann.
1992.I.
Muslea.
1999.
Extraction Patterns forInformation Extraction Tasks: A Survey.
TheAAAI-99 Workshop on Machine Learning forInformation Extraction.U.
Y. Nahm and R. J. Mooney.
2001.
Mining SoftMatching Rules from Textual Data.
Proceedingsof the 17th International Joint Conference onArtificial Intelligence.
(IJCAI-01), pages 979-986.C.
Niu, W. Li, J. Ding and R. K. Srihari.
2003.
ABootstrapping Approach to Named EntityClassification Using Successive Learners.Proceedings of the 41st Annual Meeting of theAssociation for Computational Linguistics.
(ACL-03), pages 335-342.E.
Riloff.
1996.
Automatically GeneratingExtraction  Patterns from Untagged Text.Proceedings of the 13th National Conference onArtificial Intelligence (AAAI-96), pages 1044-1049.E.
Riloff and R. Jones, 1999,  LearningDictionaries for Information Extraction byMulti-Level Bootstrapping, Proceedings of theSixteenth National Conference on ArtificialIntelligence (AAAI-99), pages 474-479.S.
Soderland.
1999.
Learning InformationExtraction Rules for Semi-structured and FreeText.
Machine Learning, vol.34, pages 233-272.J.
Xiao, T.-S. Chua and J. Liu.
2003.
A GlobalRule Induction Approach to InformationExtraction.
Proceedings of the 15th IEEEInternational Conference on Tools with ArtificialIntelligence.
(ICTAI-03), pages 530-536.R.
Yangarber.
2003.
Counter-Training inDiscovery of Semantic Patterns.
Proceedings ofthe 41st Annual Meeting of the Association forComputational Linguistics (ACL-03), pages 343-350.
