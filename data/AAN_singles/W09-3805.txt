Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 33?36,Paris, October 2009. c?2009 Association for Computational LinguisticsEmpirical lower bounds on translation unit error rate for the full class ofinversion transduction grammarsAnders S?gaardCenter for Language TechnologyUniversity of Copenhagensoegaard@hum.ku.dkDekai WuHuman Language Technology CenterHong Kong Univ.
of Science and Technologydekai@cs.ust.hkAbstractEmpirical lower bounds studies in whichthe frequency of alignment configurationsthat cannot be induced by a particular for-malism is estimated, have been importantfor the development of syntax-based ma-chine translation formalisms.
The for-malism that has received most attentionhas been inversion transduction grammars(ITGs) (Wu, 1997).
All previous workon the coverage of ITGs, however, con-cerns parse failure rates (PFRs) or sen-tence level coverage, which is not di-rectly related to any of the evaluation mea-sures used in machine translation.
S?gaardand Kuhn (2009) induce lower bounds ontranslation unit error rates (TUERs) for anumber of formalisms, incl.
normal formITGs, but not for the full class of ITGs.Many of the alignment configurations thatcannot be induced by normal form ITGscan be induced by unrestricted ITGs, how-ever.
This paper estimates the differenceand shows that the average reduction inlower bounds on TUER is 2.48 in absolutedifference (16.01 in average parse failurerate).1 IntroductionThe first stage in training a machine translationsystem is typically that of aligning bilingual text.The quality of alignments is in that case of vi-tal importance to the quality of the induced trans-lation rules used by the system in subsequentstages.
In string-based statistical machine trans-lation, the alignment space is typically restrictedby the n-grams considered in the underlying lan-guage model, but in syntax-based machine trans-lation the alignment space is restricted by verydifferent and less transparent structural contraints.While it is easy to estimate the consequences ofrestrictions to n-grams of limited size, it is lesstrivial to estimate the consequences of the struc-tural constraints imposed by syntax-based ma-chine translation formalisms.
Consequently, muchwork has been devoted to this task (Wu, 1997;Zens and Ney, 2003; Wellington et al, 2006;Macken, 2007; S?gaard and Kuhn, 2009).The task of estimating the consequences ofthe structural constraints imposed by a particularsyntax-based formalism consists in finding what isoften called ?empirical lower bounds?
on the cov-erage of the formalism (Wellington et al, 2006;S?gaard and Kuhn, 2009).
Gold standard align-ments are constructed and queried in some wayas to identify complex alignment configurations,or they are parsed by an all-accepting grammarsuch that a parse failure indicates that no align-ment could be induced by the formalism.The assumption in this and related work that en-ables us to introduce a meaningful notion of align-ment capacity is that simultaneously recognizedwords are aligned (Wu, 1997; Zhang and Gildea,2004; Wellington et al, 2006; S?gaard and Kuhn,2009).
As noted by S?gaard (2009), this defi-nition of alignment has the advantageous conse-quence that candidate alignments can be singledout by mere inspection of the grammar rules.
Italso has the consequence that alignments are tran-sitive (Goutte et al, 2004), since simultaneity istransitive.While previous work (S?gaard and Kuhn, 2009)has estimated empirical lower bounds for normalform ITGs at the level of translation units (TUER),or cepts (Goutte et al, 2004), defined as maxi-mally connected subgraphs in alignments, nobodyhas done this for the full class of ITGs.
Whatis important to understand is that while normalform ITGs can induce the same class of transla-tions as the full class of ITGs, they do not inducethe same class of alignments.
They do not, for ex-33ample, induce discontinuous translation units (seeSect.
3).
Sect.
2 briefly presents some related re-sults in the literature.
Some knowledge about for-malisms used in machine translation is assumed.2 Related workAho and Ullman (1972) showed that 4-ary syn-chronous context-free grammars (SCFGs) couldnot be binarized, and Satta and Peserico (2005)showed that the hiearchy of SCFGs beyond ternaryones does not collapse; they also showed that thecomplexity of the universal recognition problemfor SCFGs is NP-complete.
ITGs on the otherhand has a O(|G|n6) solvable universal recog-nition problem, which coincides with the unre-stricted alignment problem (S?gaard, 2009).
Theresult extends to decoding in conjunction with abigram language model (Huang et al, 2005).Wu (1997) introduced ITGs and normal formITGs.
ITGs are a notational variant of the sub-class of SCFGs such that all indexed nonterminalsin the source side of the RHS occur in the sameorder or exactly in the inverse order in the targetside of the RHS.
It turns out that this subclass ofSCFGs defines the same set of translations that canbe defined by binary SCFGs.
The different formsof production rules are listed below with the morerestricted normal form production rules in the rightcolumn, with ?
?
(N ?
{e/f | e ?
T ?, f ?
T ?})?
(N nonterminals and T terminals, as usual).
TheRHS operator [ ] preserves source language con-stituent order in the target language, while ?
?
re-verses it.1A ?
[?]
A ?
[BC]A ?
???
A ?
?BC?A ?
e/fSeveral studies have adressed the alignment ca-pacity of ITGs and normal form ITGs.
Zens andNey (2003) induce lower bounds on PRFs fornormal form ITGs.
Wellington et al (2006) in-duce lower bounds on PRFs for ITGs.
S?gaardand Kuhn (2009) induce lower bounds on TUERfor normal form ITGs and more expressive for-malisms for syntax-based machine translation.
Noone has, however, to the best our knowledge in-duced lower bounds on TUER for ITGs.1One reviewer argues that our definition of full ITGs isnot equivalent to the definition in Wu (1997), which, in thereviewer?s words, allows ?at most one lexical item from eachlanguage?.
Sect.
6 of Wu (1997), however, explicitly encour-ages lexical elements in rules to have more than one lexicalitem in many cases.3 ExperimentsAs already mentioned empirical lower boundsstudies differ in four important respects, namelywrt.
: (i) whether they use hand-aligned or auto-matically aligned gold standards, (ii) the level atwhich they count failures, e.g.
sentence, align-ment or translation unit level, (iii) whether theyinterpret translation units disjunctively or conjunc-tively, and (iv) whether they induce the lowerbounds (a) by running an all-accepting grammaron the gold standard data, (b) by logical charac-terization of the structures that can be induced bya formalism, or (c) by counting the frequency ofcomplex alignment configurations.
The advantageof (a) and (b) is that they are guaranteed to find thehighest possible lower bound on the gold standarddata, whereas (c) is more modular (formalism-independent) and actually tells us what configu-rations cause trouble.
(i) In this study we use hand-aligned gold stan-dard data.
It should be obvious why this is prefer-able to automatically aligned data.
The only rea-son that some previous studies used automaticallyaligned data is that hand-aligned data are hard tocome by.
This study uses the data also used byS?gaard and Kuhn (2009), which to the best ofour knowledge uses the largest collection of hand-aligned parallel corpora used in any of these stud-ies.
(ii) Failures are counted at the level of trans-lation units as argued for in the above, but sup-plemented by parse failure rates for completeness.
(iii) Since we count failures at the level of transla-tion units, it is natural to interpret them conjunc-tively.
Otherwise we would in reality count fail-ures at the level of alignments.
(iv) We use (c).The conjunctive interpretation of translationunits was also adopted by Fox (2002) and is mo-tivated by the importance of translation units anddiscontinuous ones in particular to machine trans-lation in general (Simard and colleagues, 2005;Ayan and Dorr, 2006; Macken, 2007; Shieber,2007).
In brief,TUER = 1 ?
2|SU ?GU ||SU |+ |GU |where GU are the translation units in the gold stan-dard, and SU the translation units produced bythe system.
This evaluation measure is related toconsistent phrase error rate (CPER) introduced inAyan and Dorr (2006), except that it does not onlyconsider contiguous phrases.343.1 DataThe characteristics of the hand-aligned gold stan-dard parallel corpora used are presented in Fig-ure 1.
The Danish-Spanish text is part ofthe Copenhagen Dependency Treebank (Parole),English-German is from Pado and Lapata (2006)(Europarl), and the six combinations of English,French, Portuguese and Spanish are documentedin Graca et al (2008) (Europarl).3.2 Alignment configurationsThe full class of ITGs induces many alignmentconfigurations that normal form ITGs do not in-duce, incl.
discontinuous translation units (DTUs),i.e.
translation units with at least one gap, double-sided DTUs, i.e.
DTUs with both a gap in thesource side and a gap in the target side, and multi-gap DTUs with arbitrarily many gaps (as long asthe contents in the gap are either respect the linearorder of the source side or the inverted order).ITGs do not induce (i) inside-out alignments,(ii) cross-serial DTUs, (iii) what is called the ?bon-bon?
configuration below, and (iv) multigap DTUswith mixed order in the target side.
The reader isreferred to Wu (1997) for discussion of inside-outalignments.
(ii) and (iii) are explained below.3.2.1 Induced configurationsDTUs are easily induced by unrestricted ITG pro-ductions, while they cannot be induced by pro-ductions in normal form.
The combination of theproduction rules A ?
[?/ne B nothing/pas] andB ?
[change/modifie], for example, induces aDTU with a gap in the French side for the pair ofsubstrings ?change nothing, ne modifie pas?.Multigap DTUs with up to three gaps are fre-quent (S?gaard and Kuhn, 2009) and have shownto be important for translation quality (Simard andcolleagues, 2005).
While normal form ITGs donot induce multigap DTUs, ITGs induce a partic-ular subclass of multigap DTUs, namely those thatare constructed by linear or inverse interpolation.3.2.2 Non-induced configurationsInside-out alignments were first described byWu (1997), and their frequency has been a mat-ter of some debate (Lepage and Denoual, 2005;Wellington et al, 2006; S?gaard and Kuhn, 2009).Cross-serial DTUs are made of two DTUs non-contiguous to the same side such that both havematerial in the gap of each other.
Bonbons aresimilar, except the DTUs are non-contiguous todifferent sides, i.e.
D has a gap in the source sidethat contains at least one token in E, and E hasa gap in the target side that contains at least onetoken in D. Here?s an example of a bonbon con-figuration from Simard et al (2005):Pierre ne mange pasPierre does not eatMultigap DTUs with mixed transfer are, as al-ready mentioned multigap DTUs with crossingalignments from material in two distinct gaps.3.3 ResultsThe lower bounds on TUER for the full class ofITGs are obtained by summing the ratios of inside-out alignments, cross-serial DTUs, bonbons andmixed order multigap DTUs, subtracting any over-lap between these classes of configurations.
Thelower bounds on TUER for normal form ITGssum ratios of inside-out aligments and DTUs sub-tracting any overlap.
Figure 1 presents the ratio(?100), and Figure 2 presents the induced lowerbounds on the full class of ITGs and normal formITGs.
Any two configurations differ on all trans-lation units in order to count as two distinct con-figurations in these statistics.
Otherwise a singletranslation unit could be removed to simplify twoor more configurations.4 DiscussionThe usefulness of alignment error rate (AER) (Ochand Ney, 2000) has been questioned lately (Fraserand Marcu, 2007); most importantly, AER doesnot always seem to correlate with translation qual-ity.
TUER is likely to correlate better with transla-tion quality, since it by definition correlates withCPER (Ayan and Dorr, 2006).
No large-scaleexperiment has been done yet to estimate thestrength of this correlation.Our study also relies on the assumption thatsimulatenously recognized words are aligned inbilingual parsing.
The relationship between pars-ing and alignment can of course be complicated inways that will alter the alignment capacity of ITGand its normal form; on some definitions the twoformalisms may even become equally expressive.5 ConclusionIt was shown that the absolute reduction in averagelower bound on TUER is 2.48 for the full class ofITGs over its canonical normal form.
For PRF, itis 16.01.35Snts TUs IOAs DTUs CDTUs Bonbons MIX-DTUsDa-Sp 926 6441 0.56 9.16 0.81 0.16 0.23En-Fr 100 869 0.23 2.99 0.12 0.23 0.23En-Ge 987 17354 1.75 5.55 0.45 0.05 0.79En-Po 100 783 0.26 2.17 0.00 0.00 0.38En-Sp 100 831 0.48 1.32 0.00 0.00 0.36Po-Fr 100 862 0.23 3.13 0.58 0.00 0.46Po-Sp 100 882 0.11 0.90 0.00 0.00 0.00Sp-Fr 100 914 0.11 2.95 0.55 0.00 0.22Figure 1: Characteristics of the parallel corpora and frequency of configurations ( nTUs ?
100).ITGs NF-ITGsLB-TUER LB-PFR Ovlp(TUs) Ovlp(Snts) LB-TUER PFR Ovlp(TUs) Ovlp(Snts)Da-Sp 1.58 10.37 11 10 8.54 40.50 76 32En-Fr 0.69 6.00 1 1 2.88 22.00 3 2En-Ge 2.75 47.32 49 42 5.24 69.30 357 236En-Po 0.64 5.00 0 0 2.43 19.00 0 0En-Sp 0.84 7.00 0 0 1.80 15.00 0 0Po-Fr 1.04 9.00 2 2 3.36 24.00 0 0Po-Sp 0.11 1.00 1 1 0.90 8.00 1 1Sp-Fr 0.77 7.00 1 1 3.06 23.00 0 0AV 1.05 11.59 3.53 27.60Figure 2: Induced lower bounds for ITGs and normal form ITGs (NF-ITGs).
LB-TUER lists the lower bounds on TUER.LB-PFR lists the lower bounds on parse failure rates.
Finally, the third and fourth columns list configuration overlaps at thelevel of translation units, resp.
sentences.ReferencesAlfred Aho and Jeffrey Ullman.
1972.
The theory of parsing,translation and compiling.
Prentice-Hall.Necip Ayan and Bonnie Dorr.
2006.
Going beyond AER.
InCOLING-ACL?06, Sydney, Australia.Heidi Fox.
2002.
Phrasal cohesion and statistical machinetranslation.
In EMNLP?02, Philadelphia, PA.Alexander Fraser and Daniel Marcu.
2007.
Measuring wordalignment quality for statistical machine translation.
Com-putational Linguistics, 33(3):293?303.Cyril Goutte, Kenji Yamada, and Eric Gaussier.
2004.Aligning words using matrix factorisation.
In ACL?04,Barcelona, Spain.Joao Graca, Joana Pardal, Lu?
?sa Coheur, and DiamantinoCaseiro.
2008.
Building a golden collection of paral-lel multi-language word alignments.
In LREC?08, Mar-rakech, Morocco.Liang Huang, Hao Zhang, and Daniel Gildea.
2005.
Ma-chine translation as lexicalized parsing with hooks.
InIWPT?05, pages 65?73, Vancouver, BC.Yves Lepage and Etienne Denoual.
2005.
Purest everexample-based machine translation.
Machine Translation,19(3?4):251?282.Lieve Macken.
2007.
Analysis of translational correspon-dence in view of sub-sentential alignment.
In METIS-II,pages 9?18, Leuven, Belgium.Franz Och and Hermann Ney.
2000.
A comparison of align-ment models for statistical machine translation.
In COL-ING?00, Saarbru?cken, Germany.Sebastian Pado?
and Mirella Lapata.
2006.
Optimal con-stituent alignment with edge covers for semantic projec-tion.
In ACL-COLING?06, Sydney, Australia.Giorgio Satta and Enoch Peserico.
2005.
Some compu-tational complexity results for synchronous context-freegrammars.
In HLT-EMNLP?05, Vancouver, BC.Stuart Shieber.
2007.
Probabilistic synchronous tree-adjoining grammars for machine translation.
In SSST?07,pages 88?95, Rochester, NY.Michel Simard and colleagues.
2005.
Translating with non-contiguous phrases.
In HLT-EMNLP?05, Vancouver, BC.Anders S?gaard and Jonas Kuhn.
2009.
Empirical lowerbounds on alignment error rates in syntax-based machinetranslation.
In NAACL-HLT?09, SSST-3, Boulder, CO.Anders S?gaard.
2009.
On the complexity of alignmentproblems in two synchronous grammar formalisms.
InNAACL-HLT?09, SSST-3, Boulder, CO.Benjamin Wellington, Sonjia Waxmonsky, and DanMelamed.
2006.
Empirical lower bounds on the com-plexity of translational equivalence.
In ACL?06, pages977?984, Sydney, Australia.Dekai Wu.
1997.
Stochastic inversion transduction gram-mars and bilingual parsing of parallel corpora.
Computa-tional Linguistics, 23(3):377?403.Richard Zens and Hermann Ney.
2003.
A comparative studyon reordering constraints in statistical machine translation.In ACL?03, Sapporo, Japan.Hao Zhang and Daniel Gildea.
2004.
Syntax-based align-ment: supervised or unsupervised?
In COLING?04, pages418?424, Geneva, Switzerland.36
