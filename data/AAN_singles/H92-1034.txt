Subphonetic Modeling for Speech RecognitionMei-Yuh Hwang Xuedong HuangSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, Pennsylvania 15213ABSTRACTHow to capture important acoustic lues and estimate ssen-tial parameters eliably is one of the central issues in speechrecognition, since we will never have sufficient training datato model various acoustic-phonetic phenomena.
Success-ful examples include subword models with many smoothingtechniques.
In comparison with subword models, subphoneticmodeling may provide a finer level of details.
We proposeto model subphonetic events with Markov states and treat hestate in phonetic hidden Markov models as our basic sub-phonetic unit - -  senone.
A word model is a concatenationof state-dependent senones and senones can be shared acrossdifferent word models.
Senones not only allow parametersharing, but also enable pronunciation optimization and newword learning, where the phonetic baseform is replaced bythe senonic baseform.
In this paper, we report preliminarysubphonetic modeling results, which not only significantly re-duced the word error ate for speaker-independent co inuousspeech recognition but also demonstrated a novel applicationfor new word learning.1 INTRODUCTIONFor large-vocabulary speech recognition, we will never havesufficient raining data to model all the various acoustic-phonetic phenomena.
How to capture important acousticclues and estimate ssential parameters reliably is one ofthe central issues in speech recognition.
To share parame-ters among different word modes, context-dependent subwordmodels have been used successfully in many state-of-the-artspeech recognition systems \[1, 2, 3, 4\].
The principle of pa-rameter sharing can also be extended to subphonetic models.For subphonetic modeling, fenones \[5, 6\] have been used asthe front end output of the IBM acoustic processor.
To gener-ate a fenonic pronunciation, multiple xamples of each wordare obtained.
The fenonic baseform is built by searching fora sequence of fenones which has the maximum probability ofgenerating all the given multiple utterances.
The codeword-dependent fenonic models are then trained just like phoneticmodels.
We believe that he 200 codeword-dependent f onesmay be insufficient for large-vocabulary continuous peechrecognition.In this paper, we propose to model subphonetic events withMarkov states.
We will treat the state in hidden Markovmodels (HMMs) as a basic subphonetic unit - -  senone.
Thetotal number of HMM states in a system is often too largeto be well trained.
To reduce the number of free parame-ters, we can cluster the state-dependent ou put distributions.Each clustered output distribution is denoted as a senone.
Inthis way, senones can be shared across different models asillustrated in Figure 1.senone codebookFigure 1: Creation of the senone codebook.The advantages ofsenones include better parameter sharingand improved pronunciation optimization.
After clustering,different states in different models may share the same senoneif they exhibit acoustic similarity.
Clustefingat the granularityof the state rather than the entire model (like generalizedtripbones) can keep the dissimilar states of two similar modelsapart while the other corresponding states are merged, and thuslead to better parameter sharing.
For instance, the first, or thesecond states of the/ey/phones in PLACE and RELAT IONmay be tied together.
However, to magnify the acoustic effectsof the fight contexts, their last states may be kept separately.In addition to finer parameter sharing, senones also give usthe freedom to use a larger number of states for each phoneticmodel.
Although an increase in the number of states willincrease the total number of free parameters, with senonesharing we can essentially eliminate those redundant statesand have the luxury of maintaining the necessary ones.Since senones depend on Markov states, the senonic base-form of a word can be constructed naturally with the forward-backward algorithm \[7\].
Regarding pronunciation optimiza-tion as well as new word learning, we can use the forward-backward algorithm to iteratively optimize asenone sequenceappropriate for modeling multiple utterances of a word.
Thatis, given the multiple examples, we can train a word HMMwith the forward-backward algorithm.
When the reestimation174reaches its optimality, the estimated states can be quantizedwith the codebook of senones.
The closest one can be used tolabel the state of the word HMM.
This sequence of senonesbecomes the senonic baseform of the word.
Here arbitrary se-quences of senones are allowed to provide the freedom for theautomatically learned pronunciation.
After the senonic base-form of every word is determined, the senonic word modelsmay be trained, resulting in a new set of senones.
Althougheach senonic word model generally has more states than thetraditional phoneme-concatenated word model, the numberof parameters emains the same since the size of the senonecodebook is intact.In dictation applications, new words will often appear dur-ing user's usage.
A natural extension for pronunciation opti-mization is to generate senonic baseforms for new words.
Au-tomatic determination f phonetic baseforms has been consid-ered by \[8\], where four utterartces and spelling-to-soundrulesare need.
For the senonic baseform, we can derive senonicbaseform only using acoustic data without any spelling infor-mation.
This is useful for acronym words like IEEE (pro-nounced as 1-triple-E), CAT-2 (pronounced as cat-two) andforeign person ames, where speUing-to-soundrules arehardto generalize.
The acoustic-driven senonic baseform can alsocapture pronunciation ofeach individual speaker, since in dic-tation applications, multiple new-word samples are often fromthe same speaker.By constructing senone codebook and using senones in thetriphone system, we were able to reduce the word error rateof the speaker-independent Resource Management task by20% in comparison with the generalized triphone \[2\].
Whensenones were used for pronunciation optimization, our pre-liminary results gave us another 15% error reduction in aspeaker-independent continuous pelling task.
The word er-ror rate was reduced from 11.3% to 9.6%.
For new wordlearning, we used 4 utterances for each new word.
Our pre-liminary results indicate that the error rate of automaticallygenerated senonic baseform is comparable to that of hand-written phonetic baseform.2 SHARED DISTR IBUT ION MODELSIn phone-based HMM systems, each phonetic model is formedby a sequence of states.
Phonetic models are shared acrossdifferent word models.
In fact, the state can also be sharedacross different phonetic models.
This section will describethe usage of senones for parameter sharing.2.1 Senone Construction by State ClusteringThe number of triphones in a large vocabulary system is gen-erally very large.
With limited training data, there is no hopeto obtain well-trained models.
Therefore, different echnolo-gies have been studied to reduce the number of parameters\[1, 9, 2, 10, 11\].
In generalized triphones, every state of atriphone is merged with the corresponding state of anothertriphone in the same cluster.
It may be true that some statesare merged not because they are similar, but because the otherstates of the involved models resemble ach other.
To fulfillmore accurate modeling, states with differently-shaped out-put distributions should be kept apart, even though the otherstates of the models are tied.
Therefore, clustering shouldbe carried out at the output-distribution level rather than themodel evel.
The distribution clustering thus creates a senonecodebook as Figure 2 shows \[12\].
The clustered istributionsor senones are fed back to instantiatephonetic models.
Thus,states of different phonetic models may share the same senone.This is the same as theshared-distributionmodel (SDM) \[13\].Moreover, different states within the same model may also betied together if too many states are used to model this phone'sacoustic variations or i fa certain acoustic event appears repet-itively within the phone..2....All HMMs are first estimated.Initially, every output distribution of all HMMs iscreated as a cluster.Find the most similar pair of clusters and mergethem together.For each element in each cluster of the currentconfiguration, move it to another cluster if thatresults in improvement.
Repeat his shifting untilno improvement can be made.Go to step 3 unless some convergence riterion ismet.Figure 2: The construction of senones (clustered output dis-tributions).Sentries also give us the freedom to use a larger numberof states for each phonetic model.
Although an increase inthe number of states will increase the total number of freeparameters, yet by clustering similar states we can essentiallyeliminate those redundant s ates and have the luxury to main-tain the necessary ones \[13\].2.2 Performance EvaluationWe incorporated the above distribution clustering technique inthe SPHINX-II system \[14\] and experimented onthe speaker-independent DARPA Resource Management (RM) task witha word-pair grammar of perplexity 60.
The test set consistedof the February 89 and October 89 test sets, totaling 600sentences.
Table 1 shows the word error rates of severalsystems.System Word Error % Error ReductionGeneralized Triphone 4.7 %3500-SDM 4.2% 11%4500-SDM 3.8% 20%5500-SDM 4.1% 13%Table 1: Results of the generalized triphone vs. the SDM.In the SPHINX system, there were 1100 generalized tri-phones, each with 3 distinct output distributions.
In the175SPHINX-II system, we used 5-state Bakis triphone modelsand clustered all the output distributions in the 7500 or so tri-phones down to 3500-5500 senones.
The system with 4500senones had the best performance with the given 3990 trainingsentences.
The similarity between two distributions was mea-sured by their entropies.
After two distributions are merged,the entropy-increase, w ighted by counts, is computed:(ca + cb)no+b -- Coaa - CbHbwhere Ca is the summation of the entries of distribution a interms of counts, and Ha is the entropy.
The less the entropy-increase is, the closer the two distributions are.
Weighting en-tropies by counts enables those distributions with less occur-ring frequency be merged before frequent ones.
This makeseach senone (shared istribution) more trainable.2.3 Behavior of State ClusteringTo understand the quality of the senone codebook, we exam-ined several examples in comparison with 1100 generalizedtriphone models.
As shown in Figure 3, the two/ey/triphonesin -PLACE and --LaTION were mapped to the same general-ized triphone.
Similarly, phone/d/ in START and ASTORIAwere mapped to another generalized triphone.
Both has thesame left context, but different right contexts.
States with thesame color were tied to the same senone in the 4500-SDMsystem, x, y, z, and w represent different sentries.
Figure(a) demonstrates that distribution clustering can keep dissim-ilar states apart while merging similar states of two models.Figure (b) shows that redundant s ates inside a model can besqueezed.
It also reveals that distribution clustering is able tolearn the same effect of similar contexts (/aa/ and /at/) on thecurrent phone (/dO.It is also interesting tonote that when 3 states, 5 states, and7 states per triphone model are used with a senone codebooksize of 4500, the average number of distinct senones a triphoneused is 2.929, 4.655, and 5.574 respectively.
This mightimply that 5 states per phonetic model are optimal to modelthe acoustic variations within a triphone unit for the givenDARPA RM training database.
In fact, 5-state models indeedgave us the best performance.3 PRONUNCIAT ION OPT IMIZAT IONAs shown in Figure 1, senones can be shared not only bydifferent phonetic models, but also by different word models.This section will describe one of the most important applica-tions of senones: word pronunciation ptimization.3.1 Senonic Baseform by State QuantizationPhonetic pronunciation optimization has been considered by\[15, 8\].
Subphonetic modeling also has a potential pplicationto pronunciation learning.
Most speech recognition systemsuse a fixed phonetic transcription for each word in the vocabu-lary.
If a word is transcribed improperly, it will be difficult forthe system to recognize it.
There may be quite a few impropertranscriptions in a large vocabulary system for the given task.Most importantly, some words may be pronounced in severaldifferent ways such as THE (/dh ax /or /dh  ih/), TOMATO( / tax  m ey dx owl or / t  ax m aa dx ow/), and so(a)-LaTION-PLACECo)STARTASTORIAFigure 3: Examples of triphone-clustering and distribution-clustering.
Figure (a) shows two/ey/triphones which werein the same generalized triphone cluster; (b) shows two/d/triphones in another cluster.
In each sub-figure, states withthe same color were tied to the same senone, z, y, z, and wrepresent different senones.on.
We can use multiple phonetic transcriptions for everyword, or to learn the pronunciation automatically from thedata.Figure 4 shows the algorithm which looks for the mostappropriate s nonic baseform for a given word when trainingexamples are available.1.
Compute the average duration (number of time-frames), given multiple tokens of the word.2.
Build a Bakis word HMM with the number ofstates equal to a portion of the average duration(usually 0.8).3.
Run several iterations (usually 2 - 3) of theforward-backward algorithm on the word modelstarting from uniform output distributions, usingthe given utterance tokens.4.
Quantize ach state of the estimated word modelwith the senone codebook.Figure 4: The determination of a senonic baseforrn, givenmultiple training tokens.Here arbitrary sequences of senones are allowed to providethe freedom for the automatically learned pronunciation.
Thissenonic baseform tightly combines the model and acousticdata.
After the senonic baseform of every word is determined,176the senonic word models may be trained, resulting in a newset of senones.Similar to fenones, sentries take full advantage of the mul-tiple utterances in baseform construction.
In addition, bothphonetic baseform and senonic baseform can be used to-gether, without doubling the number of parameters in con-trast to fenones.
So we can keep using phonetic baseforrnwhen training examples are unavailable.
The senone code-book also has a better acoustic resolution in comparison withthe 200 VQ-dependent fenones.
Although each senonic wordmodel generally has more states than the traditional phoneme-concatenated word model, the number of parameters are notincreased since the size of the senone codebook is fixed.3.2 Performance EvaluationAs a pivotal experiment for pronunciation learning, we usedthe speaker-independent co tinuous spelling task (26 Englishalphabet).
No grammar is used.
There are 1132 training sen-tences from 100 speakers and 162 testing sentences from 12new speakers.
The training data were segmented into wordsby a set of existing HMMs and the Viterbi alignment \[16, 1\].For each word, we split its training data into several groupsby a DTW clustering procedure according to their acousticresemblance.
Different groups represent different acousticrealizations of the same word.
For each word group, we es-timated the word model and computed a senonic baseform asFigure 4 describes.
The number of states of a word model wasequal to 75% of the average duration.
The Euclidean distancewas used as the distortion measure during state quantization.We calculated the predicting ability of the senonic wordmodel M,o,a obtained from the g-th group of word w as:log P(X,olM~,,g) / ~ IX,,IX,.
egroup g X,.
~group awhere X~o is an utterance of word w.For each word, we picked two models that had the bestpredicting abilities.
The pronunciation f each word utterancein the training set was labeled by:modei(Xto) = argmax { p(X~lM~o,a)}M~o,a E top2After the training data were labeled in this way, we re-trained the system parameters by using the senonic baseform.Table 2 shows the word error rate.
Both systems used thesex-dependent semi-continuous HMMs.
The baseline usedword-dependent phonetic models.
Therefore, it was essen-tially a word-based system.
Fifty-six word-dependent pho-netic models were used.
Note both systems used exactly thesame number of parameters.This preliminary results indicated that he senonic baseformcan capture detailed pronunciation variations for speaker-independent speech recognition.4 NEW WORD LEARNINGIn dictation applications, we can start from speaker-independent system.
However, new words will often appearwhen users are dictating.
In real applications, these newSystem Word Error % Error Reductionphonetic baseform 11.3%senonic baseform 9.6% 15%Table 2: Results of the phonetic baseform vs. the senonicbaseform on the spelling task.word samples are often speaker-dependent albeit speaker-independent systems may be used initially.
A natural ex-tension for pronunciation ptimization is to generate speaker-dependent senonic baseforms for these new words.
In thisstudy, we assume possible new words are already detected,and we want to derive the senonic baseforms of new wordsautomatically.
We are interested in using acoustic data only.This is useful for acronym words like IEEE (pronounced asl-triple-E), CAT-2 (pronounced as cat-two) and foreign personnames, where spelling-to-sound rules are hard to generalize.The senonic baseform can also capture pronunciation charac-teristics of each individual speaker that cannot be representedin the phonetic baseform.4.1 Experimental Database and System ConfigurationWith word-based senonic models, it is hard to incorporatebetween-word co-articulation modeling.
Therefore, our base-line system used within-word triphone models only.
Againwe chose RM as the experimental task.
Speaker-independent(SI) sex-dependent SDMs were used as our baseline systemfor this study.
New word training and testing data are speaker-dependent (SD).
We used the four speakers (2 females, 2males) from the June-1990 test set; each supplied 2520 SDsentences.
The SD sentences were segmented into wordsusing the Viterbi alignment.Then we chose randomly 42 words that occurred frequentlyin the SD database (so that we have enough testing data) asshown in Table 3, where their frequencies in the speaker-independent training database are also included.
For eachspeaker and each of these words, 4 utterances were used assamples to learn the senonic baseform, and at most 10 otherutterances as testing.
Therefore, the senonic baseform of aword is speaker-dependent.
There were together 1460 testingword utterances for the four speakers.
During recognition,the segmented data were tested in an isolated-speech modewithout any grammar.4.2 State Quantization of the Senonic BaseformFor each of the 42 words, we used 4 utterances to constructthe senonic baseform.
The number of states was set to be 0.8of the average duration.
To quantize states at step 4 of Figure4, we aligned the sample utterances against he estimatedword model by the Viterbi algorithm.
Thus, each state had5 to 7 frames on average.
Each state of the word model isquantized to the senone that has the maximum probability ofgenerating all the aligned frames.
Given a certain senone,senone, the probability of generating the aligned frames ofstate s is computed inthe same manner as the semi-continuousoutput probability:177word wordAAWARCTICASUWASWAVERAGESI(F/M)training15/287/2413/2711/3033/7/0JAPANLATITUDELATITUDESLINK- 11LONGITUDESI(F/M)training10/2822/4912/276/2024/46C1 12/33 LONGITUDES 13/223C2 17133 MAX 15/23C3 16/41 MAXIMUM 24/55C4 14/33 MIW 16/3 1C5 11/44 MOB 10/29CAPABLE 34/99 MOZAMBIQUE 13/28CAPACITY 3/'7/5 NTDS 10/26CASREP 3/9 NUCLEAR 6/15CASREPED 2/4 PACFLT 3/8CASREPS 11/8 PEARL-HARBOR 2/6CASUALTY 42/88 PROPULSION 15/21CHINA 12/27 READINESS 59/136FLEET 39/96 SOLOMON 12/24FLEETS 2/9 STRAIT 26/7/7FORMOSA 9/29 THAILAND 13/26INDIAN 9/29 TOKIN 13/27Table 3: New words and their frequencies in the speaker-independent training set (Female/Male).Pr(Xlsen?ne) = ~I Pr(xilsenone)VXi a l igned to sL= 1~ ~b(klsenone)A(x,)VXi a l igned to s k=lwhere b(.
I senone) denote the discrete output distribution thatrepresents senone, L denotes the size of the front-end VQcodebook, and fk (.)
denote the probability density functionof codeword k.4.3 Experimental PerformanceFor the hand-written phonetic baseform, the word error ratewas 2.67% for the 1460 word utterances.
As a pilot study,a separate senonic baseform was constructed for CASREPand its derivatives (CASREPED, and CASREPS).
Similarly,for the singular and plural forms of the selected nouns.
Theselected 42 words were modeled by automatically constructedsenonic baseforrns.
They are used together with the rest 955words (phonetic baseforms) in the RM task.
The word errorrate was 6.23%.
Most of the errors came from the derivativeconfusion.To reduce the derivative confusion, we concatenated theoriginal senonic baseform with the possible suffix phonemesas the baseform for the derived words.
For example, thebaseform of FLEETS became/f leet  <ts  s ix -z>~,  where thecontext-independent phone model/ts/,/s/, and the concate-nated/ix z/were appended parallelly after the senonic base-form of FLEE T. In this way, no training data were used to learnthe pronunciations of the derivatives.
This suffix senonic ap-proach significantly reduced the word error to 3.63%.
Stillthere were a lot of misrecognitions of CASREPED tO beCASREP and MAX tO be NEXT.
These were due to the highconfusion between/td/and/pd/,/m/and/n/.
The above resultsare summarized in Table 4.system error ratehand-written phonetic baseform 2.67 %pilot senonic baseform 6.23 %suffix senonic baseform 3.63%Table 4: Results of the senonic baseforms on the 1460 wordutterances for the selected 42 words.The study reported here is preliminary.
Refinement onthe algorithm of senonic-baseform construction (especiallyincorporation of the spelling information) is still under inves-tigation.
Our goal is to approach the phonetic system.5 CONCLUSIONIn this paper, we developed the framework of senones - -state-dependent subphonetic unit.
Senones are created byclustering states of triphone models.
Thus, we reduced thethe number of system parameters with the senone codebook,which renders finer acoustic modeling and provides a wayto learn the model topology.
In the mean time, we canconstruct senonic baseforms to improve phonetic baseformsand learn new words without enlarging system parameters.Senonic baseforms are constructed by quantizing the statesof estimated word models with the senone codebook.
Wedemonstrated that senones can not only significantly improvespeaker-independent continuous speech recognition but alsohave a novel application for new word learning.AcknowledgementsThis research was sponsored by the Defense Advanced Re-search Projects Agency (DOD), Arpa Order No.
5167, undercontract number N00039-85-C-0163.
The authors would liketo express their gratitude to Professor Raj Reddy for his en-couragement and support, and other members of CMU speechgroup for their help.References\[1\] Schwartz, R., Chow, Y., Kimball, O., Roucos, S., Kras-ner, M., and Makhoul, J. Context-Dependent Model-ing for Acoustic-Phonetic Recognition of ContinuousSpeech.
in: IEEE International Conference on Acous-tics, Speech, and Signal Processing.
1985, pp.
1205-1208.\[2\] Lee, K. Context-Dependent Phonetic Hidden MarkovModels for Continuous Speech Recognition.
IEEETransactions on Acoustics, Speech, and Signal Pro-cessing, April 1990, pp.
599--609.178\[3\] Lee, C., Giachin, E., Rabiner, R., L. E, and Rosen-berg, A.
Improved Acoustic Modeling for ContinuousSpeech Recognition.
in: DARPA Speech and Lan.gnage Workshop.
Morgan Kaufmann Publishers, SanMateo, CA, 1990.\[4\] Bahl, L., de Souza, P., Gopalakrishnan, P. Naharnoo, D.,and Picheny, M. Decision Trees for Phonological Rulesin Continuous Speech.
in: IEEE International Con-ference on Acoustics, Speech, and Signal Processing.1991, pp.
185-188.\[15\] Bernstein, J., Cohen, M., Murveit, H., and Weintraub,M.
Linguistic Constraints in Hidden Markov ModelBased Speech Recognition.
in: IEEE InternationalConference on Acoustics, Speech, and Signal Pro-cessing.
1989.\[16\] Viterbi, A. J.
Error Bounds for Convolutional Codesand an Asymptotically Optimum Decoding Algorithm.IEEE Transactions on Information Theory, vol.
IT-13 (1967), pp.
260-269.\[5\] Bahl, L., Brown, P., de Souza, P., and Mercer, R. a. AMethod for the Construction of Acoustic Markov Modelsfor Words.
no.
RC 13099 (#58580), IBM Thomas J.Watson R~earch Center, September 1987.\[6\] Bahl, L., Brown, E, De Souza, P., and Mercer, R. Acous-tic Markov Models Used in the Tangora Speech Recog-nition System.
in: IEEE International Conference onAcoustics, Speech, and Signal Processing.
1988.\[7\] Bahl, L. R., Jelinek, E, and Mercer, R. A MaximumLikelihood Approach to Continuous Speech Recogni-tion.
IEEE Transactions on Pattern Analysis andMachine Intelligence, vol.
PAMI-5 (1983), pp.
179-190.\[8\] Bahl, L. and et.
al.
Automatic Phonetic BaseformDetermination.
in: IEEE International Conferenceon Acoustics, Speech, and Signal Processing.
1991,pp.
173-176.\[9\] Schwartz, R., Kimball, O., Kubala, E, Feng, M., Chow,Y., C., B., and J., M. Robust Smoothing Methods forDiscrete Hidden Markov Models.
in: IEEE Interna-tional Conference on Acoustics, Speech, and SignalProcessing.
1989, pp.
548-551.\[10\] Paul, D. The Lincoln Tied-Mixture HMM ContinuousSpeech Recognizer.
in: DARPA Speech and LanguageWorkshop.
Morgan Kaufmann Publishers, San Mateo,CA, 1990, pp.
332-336.\[11\] Huang, X., Ariki, ?., and Jack, M. Hidden MarkovModels for Speech Recognition.
Edinburgh UniversityPress, Edinburgh, U.K., 1990.\[12\] Lee, K., Hon, H., and Reddy, R. An Overview of theSPHINX Speech Recognition System.
IEEE Transac-tions on Acoustics, Speech, and Signal Processing,January 1990, pp.
35--45.\[13\] Hwang, M. and Huang, X. Acoustic Classification ofPhonetic Hidden Markov Models.
in: Proceedings ofEurospeech.
1991.\[14\] Huang, X., Alleva, F., Hon, H., Hwang, M., and Rosen-feld, R. The SPHINX-H Speech Recognition System:An Overview.
Technical Report, no.
CMU-CS-92-112,School of Computer Science, Carnegie Mellon Univer-sity, Pittsburgh, PA, February 1992.179
