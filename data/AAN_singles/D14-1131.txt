Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1237?1249,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsExact Decoding for Phrase-Based Statistical Machine TranslationWilker Aziz?Marc Dymetman?Lucia Specia?
?Department of Computer Science, University of Sheffield, UKW.Aziz@sheffield.ac.ukL.Specia@sheffield.ac.uk?Xerox Research Centre Europe, Grenoble, FranceMarc.Dymetman@xrce.xerox.comAbstractThe combinatorial space of translationderivations in phrase-based statistical ma-chine translation is given by the intersec-tion between a translation lattice and a tar-get language model.
We replace this in-tractable intersection by a tractable relax-ation which incorporates a low-order up-perbound on the language model.
Exactoptimisation is achieved through a coarse-to-fine strategy with connections to adap-tive rejection sampling.
We perform ex-act optimisation with unpruned languagemodels of order 3 to 5 and show search-error curves for beam search and cubepruning on standard test sets.
This is thefirst work to tractably tackle exact opti-misation with language models of ordershigher than 3.1 IntroductionIn Statistical Machine Translation (SMT), the taskof producing a translation for an input string x =?x1, x2, .
.
.
, xI?
is typically associated with find-ing the best derivation d?compatible with the in-put under a linear model.
In this view, a derivationis a structured output that represents a sequence ofsteps that covers the input producing a translation.Equation 1 illustrates this decoding process.d?= argmaxd?D(x)f(d) (1)The set D(x) is the space of all derivations com-patible with x and supported by a model of trans-lational equivalences (Lopez, 2008).
The func-tion f(d) = ?
?
H(d) is a linear parameteri-sation of the model (Och, 2003).
It assigns areal-valued score (or weight) to every derivationd ?
D(x), where ?
?
Rmassigns a relativeimportance to different aspects of the derivationindependently captured by m feature functionsH(d) = ?H1(d), .
.
.
,Hm(d)?
?
Rm.The fully parameterised model can be seen asa discrete weighted set such that feature func-tions factorise over the steps in a derivation.
Thatis, Hk(d) =?e?dhk(e), where hkis a (local)feature function that assesses steps independentlyand d = ?e1, e2, .
.
.
, el?
is a sequence of l steps.Under this assumption, each step is assigned theweightw(e) = ?
?
?h1(e), h2(e), .
.
.
, hm(e)?.
ThesetD is typically finite, however, it contains a verylarge number of structures ?
exponential (or evenfactorial, see ?2) with the size of x ?
makingexhaustive enumeration prohibitively slow.
Onlyin very restricted cases combinatorial optimisationtechniques are directly applicable (Tillmann et al.,1997; Och et al., 2001), thus it is common to resortto heuristic techniques in order to find an approxi-mation to d?
(Koehn et al., 2003; Chiang, 2007).Evaluation exercises indicate that approximatesearch algorithms work well in practice (Bojaret al., 2013).
The most popular algorithms pro-vide solutions with unbounded error, thus pre-cisely quantifying their performance requires thedevelopment of a tractable exact decoder.
Todate, most attempts were limited to short sentencesand/or somewhat toy models trained with artifi-cially small datasets (Germann et al., 2001; Igle-sias et al., 2009; Aziz et al., 2013).
Other workhas employed less common approximations to themodel reducing its search space complexity (Ku-mar et al., 2006; Chang and Collins, 2011; Rushand Collins, 2011).
These do not answer whetheror not current decoding algorithms perform well atreal translation tasks with state-of-the-art models.We propose an exact decoder for phrase-basedSMT based on a coarse-to-fine search strategy(Dymetman et al., 2012).
In a nutshell, we re-lax the decoding problem with respect to the Lan-guage Model (LM) component.
This coarse viewis incrementally refined based on evidence col-1237lected via maximisation.
A refinement increasesthe complexity of the model only slightly, hencedynamic programming remains feasible through-out the search until convergence.
We test our de-coding strategy with realistic models using stan-dard data sets.
We also contribute with optimumderivations which can be used to assess future im-provements to approximate decoders.
In the re-maining sections we present the general model(?2), survey contributions to exact optimisation(?3), formalise our novel approach (?4), presentexperiments (?5) and conclude (?6).2 Phrase-based SMTIn phrase-based SMT (Koehn et al., 2003), thebuilding blocks of translation are pairs of phrases(or biphrases).
A translation derivation d is anordered sequence of non-overlapping biphraseswhich covers the input text in arbitrary order gen-erating the output from left to right.1f(d) = ?
(y) +l?i=1?
(ei) +l?1?i=1?
(ei, ei?1) (2)Equation 2 illustrates a standard phrase-basedmodel (Koehn et al., 2003): ?
is a weighted tar-get n-gram LM component, where y is the yieldof d; ?
is a linear combination of features thatdecompose over phrase pairs directly (e.g.
back-ward and forward translation probabilities, lexi-cal smoothing, and word and phrase penalties);and ?
is an unlexicalised penalty on the num-ber of skipped input words between two adjacentbiphrases.
The weighted logic program in Figure1 specifies the fully parameterised weighted set ofsolutions, which we denote ?D(x), f(d)?.2A weighted logic program starts from its ax-ioms and follows exhaustively deducing new itemsby combination of existing ones and no deductionhappens twice.
In Figure 1, a nonteminal itemsummarises partial derivation (or hypotheses).
It isdenoted by [C, r, ?]
(also known as carry), where:C is a coverage vector, necessary to impose thenon-overlapping constraint; r is the rightmost po-sition most recently covered, necessary for thecomputation of ?
; and ?
is the last n ?
1 words1Preventing phrases from overlapping requires an expo-nential number of constraints (the powerset of x) renderingthe problem NP-complete (Knight, 1999).2Weighted logics have been extensively used to describeweighted sets (Lopez, 2009), operations over weighted sets(Chiang, 2007; Dyer and Resnik, 2010), and a variety of dy-namic programming algorithms (Cohen et al., 2008).ITEM[{0, 1}I, [0, I + 1],?n?1]GOAL[1I, I + 1, EOS]AXIOM?BOS?
BOS?
[0I, 0, BOS] : ?
(BOS)EXPAND[C, r, yj?1j?n+1] ?xi?i?r???
yj?j?
[C?, i?, yj?j?
?n+2]: w?i?k=ick=?0where c?k= ckif k < i or k > i?else?1w = ?r?
?
(r, i)?
?
(yj?j|yj?1j?n+1)ACCEPT [1I, r, ?
][1I, I + 1, EOS] : ?
(r, I + 1)?
?(EOS|?
)r ?
IFigure 1: Specification for the weighted set oftranslation derivations in phrase-based SMT withunconstrained reordering.in the yield, necessary for the LM component.
Theprogram expands partial derivations by concatena-tion with a translation rule?xi?i?r???
yj?j?, that is, aninstantiated biphrase which covers the span xi?iandyields yj?jwith weight ?r.
The side condition im-poses the non-overlapping constraint (ckis the kthbit in C).
The antecedents are used to compute theweight of the deduction, and the carry is updatedin the consequent (item below the horizontal line).Finally, the rule ACCEPT incorporates the end-of-sentence boundary to complete items.3It is perhaps illustrative to understand the set ofweighted translation derivations as the intersectionbetween two components.
One that is only locallyparameterised and contains all translation deriva-tions (a translation lattice or forest), and one thatre-ranks the first as a function of the interactionsbetween translation steps.
The model of transla-tional equivalences parameterised only with ?
isan instance of the former.
An n-gram LM compo-nent is an instance of the latter.2.1 HypergraphsA backward-hypergraph, or simply hypergraph,is a generalisation of a graph where edges havemultiple origins and one destination (Gallo et al.,1993).
They can represent both finite-state andcontext-free weighted sets and they have beenwidely used in SMT (Huang and Chiang, 2007).A hypergraph is defined by a set of nodes (or ver-3Figure 1 can be seen as a specification for a weightedacyclic finite-state automaton whose states are indexed by[l, C, r] and transitions are labelled with biphrases.
However,for generality of representation, we opt for using acyclic hy-pergraphs instead of automata (see ?2.1).1238tices) V and a weighted set of edges ?E,w?.
Anedge e connects a sequence of nodes in its tailt[e] ?
V?under a head node h[e] ?
V and hasweight w(e).
A node v is a terminal node if ithas no incoming edges, otherwise it is a nontermi-nal node.
The node that has no outgoing edges,is called root, with no loss of generality we canassume hypergraphs to have a single root node.Hypergraphs can be seen as instantiated logicprograms.
In this view, an item is a templatefor the creation of nodes, and a weighted deduc-tion rule is a template for edges.
The tail ofan edge is the sequence of nodes associated withthe antecedents, and the head is the node associ-ated with the consequent.
Even though the spaceof weighted derivations in phrase-based SMT isfinite-state, using a hypergraph as opposed to afinite-state automaton makes it natural to encodemulti-word phrases using tails.
We opt for rep-resenting the target side of the biphrase as a se-quence of terminals nodes, each of which repre-sents a target word.3 Related Work3.1 Beam filling algorithmsBeam search (Koehn et al., 2003) and cube prun-ing (Chiang, 2007) are examples of state-of-the-artapproximate search algorithms.
They approximatethe intersection between the translation forest andthe language model by expanding a limited beamof hypotheses from each nonterminal node.
Hy-potheses are organised in priority queues accord-ing to common traits and a fast-to-compute heuris-tic view of outside weights (cheapest way to com-plete a hypothesis) puts them to compete at a fairerlevel.
Beam search exhausts a node?s possible ex-pansions, scores them, and discards all but the khighest-scoring ones.
This process is wasteful inthat k is typically much smaller than the number ofpossible expansions.
Cube pruning employs a pri-ority queue at beam filling and computes k high-scoring expansions directly in near best-first order.The parameter k is known as beam size and it con-trols the time-accuracy trade-off of the algorithm.Heafield et al.
(2013a) move away from us-ing the language model as a black-box and builda more involved beam filling algorithm.
Eventhough they target approximate search, some oftheir ideas have interesting connections to ours(see ?4).
They group hypotheses that share partiallanguage model state (Li and Khudanpur, 2008)reasoning over multiple hypotheses at once.
Theyfill a beam in best-first order by iteratively vis-iting groups using a priority queue: if the topgroup contains a single hypothesis, the hypothesisis added to the beam, otherwise the group is parti-tioned and the parts are pushed back to the queue.More recently, Heafield et al.
(2014) applied theirbeam filling algorithm to phrase-based decoding.3.2 Exact optimisationExact optimisation for monotone translation hasbeen done using A?search (Tillmann et al., 1997)and finite-state operations (Kumar et al., 2006).Och et al.
(2001) design near-admissible heuris-tics for A?and decode very short sentences (6-14 words) for a word-based model (Brown et al.,1993) with a maximum distortion strategy (d = 3).Zaslavskiy et al.
(2009) frame phrase-based de-coding as an instance of a generalised Travel-ling Salesman Problem (TSP) and rely on ro-bust solvers to perform decoding.
In this view,a salesman graph encodes the translation options,with each node representing a biphrase.
Non-overlapping constraints are imposed by the TSPsolver, rather than encoded directly in the sales-man graph.
They decode only short sentences(17 words on average) using a 2-gram LM due tosalesman graphs growing too large.4Chang and Collins (2011) relax phrase-basedmodels w.r.t.
the non-overlapping constraints,which are replaced by soft penalties through La-grangian multipliers, and intersect the LM com-ponent exhaustively.
They do employ a maximumdistortion limit (d = 4), thus the problem theytackle is no longer NP-complete.
Rush and Collins(2011) relax a hierarchical phrase-based model(Chiang, 2005)5w.r.t.
the LM component.
Thetranslation forest and the language model tradetheir weights (through Lagrangian multipliers) soas to ensure agreement on what each componentbelieves to be the maximum.
In both approaches,when the dual converges to a compliant solution,the solution is guaranteed to be optimal.
Other-4Exact decoding had been similarly addressed with Inte-ger Linear Programming (ILP) in the context of word-basedmodels for very short sentences using a 2-gram LM (Ger-mann et al., 2001).
Riedel and Clarke (2009) revisit that for-mulation and employ a cutting-plane algorithm (Dantzig etal., 1954) reaching 30 words.5In hierarchical translation, reordering is governed by asynchronous context-free grammar and the underlying prob-lem is no longer NP-complete.
Exact decoding remains in-feasible because the intersection between the translation for-est and the target LM is prohibitively slow.1239wise, a subset of the constraints is explicitly addedand the dual optimisation is repeated.
They handlesentences above average length, however, resort-ing to compact rulesets (10 translation options perinput segment) and using only 3-gram LMs.In the context of hierarchical models, Aziz etal.
(2013) work with unpruned forests using up-perbounds.
Their approach is the closest to ours.They also employ a coarse-to-fine strategy withthe OS?framework (Dymetman et al., 2012), andinvestigate unbiased sampling in addition to op-timisation.
However, they start from a coarserupperbound with unigram probabilities, and theirrefinement strategies are based on exhaustive in-tersections with small n-gram matching automata.These refinements make forests grow unmanage-able too quickly.
Because of that, they only dealwith very short sentences (up to 10 words) andeven then decoding is very slow.
We design bet-ter upperbounds and a more efficient refinementstrategy.
Moreover, we decode long sentences us-ing language models of order 3 to 5.64 Approach4.1 Exact optimisation with OS?Dymetman et al.
(2012) introduced OS?, a unifiedview of optimisation and sampling which can beseen as a cross between adaptive rejection sam-pling (Robert and Casella, 2004) and A?optimisa-tion (Hart et al., 1968).
In this framework, a com-plex goal distribution is upperbounded by a sim-pler proposal distribution for which optimisation(and sampling) is feasible.
This proposal is incre-mentally refined to be closer to the goal until themaximum is found (or until the sampling perfor-mance exceeds a certain level).Figure 2 illustrates exact optimisation with OS?.Suppose f is a complex target goal distribution,such that we cannot optimise f , but we can as-sess f(d) for a given d. Let g(0)be an upper-bound to f , i.e., g(0)(d) ?
f(d) for all d ?
D(x).Moreover, suppose that g(0)is simple enough tobe optimised efficiently.
The algorithm proceedsby solving d0= argmaxdg(0)(d) and comput-6The intuition that a full intersection is wasteful is alsopresent in (Petrov et al., 2008) in the context of approximatesearch.
They start from a coarse distribution based on au-tomatic word clustering which is refined in multiple passes.At each pass, hypotheses are pruned a posteriori on the basisof their marginal probabilities, and word clusters are furthersplit.
We work with upperbounds, rather than word clusters,with unpruned distributions, and perform exact optimisation.fg(0)d0D(x)g(1)d1d*f1f0f*Figure 2: Sequence of incrementally refined up-perbound proposals.ing the quantity r0=f(d0)/g(0)(d0).
If r0weresufficiently close to 1, then g(0)(d0) would besufficiently close to f(d0) and we would havefound the optimum.
However, in the illustrationg(0)(d0)  f(d0), thus r01.
At this pointthe algorithm has concrete evidence to motivatea refinement of g(0)that can lower its maximum,bringing it closer to f?= maxdf(d) at the costof some small increase in complexity.
The re-fined proposal must remain an upperbound to f .To continue with the illustration, suppose g(1)isobtained.
The process is repeated until eventuallyg(t)(dt) = f(dt), where dt= argmaxdg(t)(d),for some finite t. At which point dtis the opti-mum derivation d?from f and the sequence ofupperbounds provides a proof of optimality.74.2 ModelWe work with phrase-based models in a standardparameterisation (Equation 2).
However, to avoidhaving to deal with NP-completeness, we con-strain reordering to happen only within a limitedwindow given by a notion of distortion limit.
Werequire that the last source word covered by anybiphrase must be within d words from the leftmostuncovered source position (Lopez, 2009).
This isa widely used strategy and it is in use in the Mosestoolkit (Koehn et al., 2007).8Nevertheless, the problem of finding the best7If d is a maximum from g and g(d) = f(d), then it iseasy to show by contradiction that d is the actual maximumfrom f : if there existed d?such that f(d?)
> f(d), then itfollows that g(d?)
?
f(d?)
> f(d) = g(d), and hence dwould not be a maximum for g.8A distortion limit characterises a form of pruning thatacts directly in the generative capacity of the model leadingto induction errors (Auli et al., 2009).
Limiting reorderinglike that lowers complexity to a polynomial function of I andan exponential function of the distortion limit.1240derivation under the model remains impractica-ble due to nonlocal parameterisation (namely,the n-gram LM component).
The weighted set?D(x), f(d)?, which represents the objective, isa complex hypergraph which we cannot affordto construct.
We propose to construct instead asimpler hypergraph for which optimisation by dy-namic programming is feasible.
This proxy rep-resents the weighted set?D(x), g(0)(d)?, whereg(0)(d) ?
f(d) for every d ?
D(x).
Note thatthis proposal contains exactly the same translationoptions as in the original decoding problem.
Thesimplification happens only with respect to the pa-rameterisation.
Instead of intersecting the com-plete n-gram LM distribution explicitly, we im-plicitly intersect a simpler upperbound view of it,where by simpler we mean lower-order.g(0)(d) =l?i=1?
(y[ei]) +l?i=1?
(ei) +l?1?i=1?
(ei, ei?1) (3)Equation 3 shows the model we use as a proxyto perform exact optimisation over f .
In compar-ison to Equation 2, the term?li=1?
(y[ei]) replaces?
(y) = ??pLM(y).
While ?
weights the yield ytaking into account all n-grams (including thosecrossing the boundaries of phrases), ?
weightsedges in isolation.
Particularly, ?
(y[ei]) =?
?qLM(y[ei]), where y[ei] returns the sequence oftarget words (a target phrase) associated with theedge, and qLM(?)
is an upperbound on the true LMprobability pLM(?)
(see ?4.3).
It is obvious fromEquation 3 that our proxy model is much simplerthan the original ?
the only form of nonlocal pa-rameterisation left is the distortion penalty, whichis simple enough to represent exactly.The program in Figure 3 illustrates the con-struction of?D(x), g(0)(d)?.
A nonterminal item[l, C, r] stores: the leftmost uncovered position land a truncated coverage vector C (together theytrack d input positions); and the rightmost positionr most recently translated (necessary for the com-putation of the distortion penalty).
Observe hownonterminal items do not store the LM state.9Therule ADJACENT expands derivations by concate-nation with a biphrase?xi?i?
yj?j?starting at theleftmost uncovered position i = l. That causesthe coverage window to move ahead to the nextleftmost uncovered position: l?= l + ?1(C) + 1,9Drawing a parallel to (Heafield et al., 2013a), a nontermi-nal node in our hypergraph groups derivations while exposingonly an empty LM state.ITEM[[1, I + 1], {0, 1}d?1, [0, I + 1]]GOAL [I, ?, I + 1]AXIOMS?BOS?
BOS?
[1, 0d?1, 0] : ?
(BOS)ADJACENT[l, C, r]?xi?i?r???
yj?j?
[l?, C?, i?]
: ?r?
?
(r, i?)?
?
(yj?j)i = l?i?
?lk=i?lck=?0where l?= l + ?1(C) + 1C?
?1(C) + 1NON-ADJACENT[l, C, r]?xi?i?r???
yj?j?
[l, C?, i?]
: ?r?
?
(r, i?)?
?
(yj?j)i > l?i?
?lk=i?lck=?0|r ?
i+ 1| ?
d|i??
l + 1| ?
dwhere c?k= ckif k < i?
l or k > i??
l else?1ACCEPT[I + 1, C, r][I + 1, ?, I + 1] : ?
(r, I + 1)?
?
(EOS)r ?
IFigure 3: Specification of the initial proposal hy-pergraph.
This program allows the same reorder-ings as (Lopez, 2009) (see logic WLd), however,it does not store LM state information and it usesthe upperbound LM distribution ?(?
).where ?1(C) returns the number of leading 1s inC, and C?
?1(C) + 1 represents a left-shift.The rule NON-ADJACENT handles the remainingcases i > l provided that the expansion skips atmost d input words |r ?
i+ 1| ?
d. In the conse-quent, the window C is simply updated to recordthe translation of the input span i..i?.
In the non-adjacent case, a gap constraint imposes that theresulting item will require skipping no more thand positions before the leftmost uncovered word istranslated |i??
l + 1| ?
d.10Finally, note thatdeductions incorporate the weighted upperbound?(?
), rather than the true LM component ?(?
).114.3 LM upperbound and Max-ARPAFollowing Carter et al.
(2012) we compute anupperbound on n-gram conditional probabilitiesby precomputing max-backoff weights stored ina ?Max-ARPA?
table, an extension of the ARPAformat (Jurafsky and Martin, 2000).A standard ARPA table T stores entries10This constraint prevents items from becoming dead-endswhere incomplete derivations require a reordering step largerthan d. This is known to prevent many search errors in beamsearch (Chang and Collins, 2011).11Unlike Aziz et al.
(2013), rather than unigrams only, wescore all n-grams within a translation rule (including incom-plete ones).1241?Z,Z.p,Z.b?, where Z is an n-gram equal to theconcatenation Pz of a prefix P with a word z, Z.pis the conditional probability p(z|P), and Z.b isa so-called ?backoff?
weight associated with Z.The conditional probability of an arbitrary n-gramp(z|P), whether listed or not, can then be recov-ered from T by the simple recursive procedureshown in Equation 4, where tail deletes the firstword of the string P.p(z|P) =??
?p(z| tail(P)) Pz 6?
T and P 6?
Tp(z| tail(P))?
P.b Pz 6?
T and P ?
TPz.p Pz ?
T(4)The optimistic version (or ?max-backoff?)
q ofp is defined as q(z|P) ?
maxHp(z|HP), whereH varies over all possible contexts extending theprefix P to the left.
The Max-ARPA table allows tocompute q(z|P) for arbitrary values of z and P. Itis constructed on the basis of the ARPA table T byadding two columns to T : a column Z.q that storesthe value q(z|P) and a column Z.m that stores anoptimistic version of the backoff weight.These columns are computed offline in twopasses by first sorting T in descending order ofn-gram length.12In the first pass (Algorithm 1),we compute for every entry in the table an opti-mistic backoff weight m. In the second pass (Algo-rithm 2), we compute for every entry an optimisticconditional probability q by maximising over 1-word history extensions (whose .q fields are al-ready known due to the sorting of T ).The following Theorem holds (see proof be-low): For an arbitrary n-gram Z = Pz, the prob-ability q(z|P) can be recovered through the proce-dure shown in Equation 5.q(z|P) =??
?p(z|P) Pz 6?
T and P 6?
Tp(z|P)?
P.m Pz 6?
T and P ?
TPz.q Pz ?
T(5)Note that, if Z is listed in the table, we return itsupperbound probability q directly.
When the n-gram is unknown, but its prefix is known, we takeinto account the optimistic backoff weight m of theprefix.
On the other hand, if both the n-gram andits prefix are unknown, then no additional contextcould change the score of the n-gram, in whichcase q(z|P) = p(z|P).In the sequel, we will need the following defini-tions.
Suppose ?
= yJIis a substring of y = yM1.12If an n-gram is listed in T , then all its substrings mustalso be listed.
Certain pruning strategies may corrupt thisproperty, in which case we make missing substrings explicit.Then pLM(?)
?
?Jk=Ip(yk|yk?11) is the contribu-tion of ?
to the true LM score of y.
We then ob-tain an upperbound qLM(?)
to this contribution bydefining qLM(?)
?
q(yI|)?Jk=I+1q(yk|yk?1I).Proof of Theorem.
Let us first suppose that the lengthof P is strictly larger than the order n of the languagemodel.
Then for any H, p(z|HP) = p(z|P); this is be-cause HP /?
T and P /?
T , along with all intermedi-ary strings, hence, by (4), p(z|HP) = p(z| tail(HP)) =p(z| tail(tail(HP))) = .
.
.
= p(z|P).
Hence q(z|P) =p(z|P), and, because Pz /?
T and P /?
T , the theoremis satisfied in this case.Having established the theorem for |P| > n, wenow assume that it is true for |P| > m and prove byinduction that it is true for |P| = m. We use thefact that, by the definition of q, we have q(z|P) =maxx??q(z|xP).
We have three cases to consider.First, suppose that Pz /?
T and P /?
T .
ThenxPz /?
T and xP /?
T , hence by induction q(z|xP) =p(z|xP) = p(z|P) for any x, therefore q(z|P) =p(z|P).
We have thus proven the first case.Second, suppose that Pz /?
T and P ?
T .
Then, forany x, we have xPz /?
T , and:q(z|P) = maxx?
?q(z|xP)= max( maxx?
?, xP/?Tq(z|xP), maxx?
?, xP?Tq(z|xP)).For xP /?
T , by induction, q(z|xP) = p(z|xP) =p(z|P), and therefore maxx?
?, xP/?Tq(z|xP) =p(z|P).
For xP ?
T , we have q(z|xP) = p(z|xP) ?xP.m = p(z|P)?
xP.b?
xP.m.
Thus, we have:maxx?
?, xP?Tq(z|xP) = p(z|P)?
maxx?
?, xP?TxP.b?xP.m.But now, because of lines 3 and 4 of Algorithm1, P.m = maxx?
?, xP?TxP.b ?
xP.m, hencemaxx?
?, xP?Tq(z|xP) = p(z|P) ?
P.m. Therefore,q(z|P) = max(p(z|P), p(z|P)?P.m) = p(z|P)?P.m,where we have used the fact that P.m ?
1 due to line 1of Algorithm 1.
We have thus proven the second case.Finally, suppose that Pz ?
T .
Then, again,q(z|P) = maxx?
?q(z|xP)= max(maxx?
?, xPz/?T, xP/?Tq(z|xP),maxx?
?, xPz/?T, xP?Tq(z|xP),maxx?
?, xPz?Tq(z|xP) ).For xPz /?
T, xP /?
T , we have q(z|xP) =p(z|xP) = p(z|P) = Pz.p, where the last equality isdue to the fact that Pz ?
T .
For xPz /?
T, xP ?
T , wehave q(z|xP) = p(z|xP)?
xP.m = p(z|P)?
xP.b?xP.m = Pz.p?
xP.b?
xP.m.
For xPz ?
T , we haveq(z|xP) = xPz.q.
Overall, we thus have:q(z|P) = max( Pz.p,maxx?
?, xPz/?T, xP?TPz.p?
xP.b?
xP.m,maxx?
?, xPz?TxPz.q ).Note that xPz ?
T ?
xP ?
T , and then one cancheck that Algorithm 2 exactly computes Pz.q as thismaximum over three maxima, hence Pz.q = q(z|P).1242Algorithm 1 Max-ARPA: first pass1: for Z ?
T do2: Z.m?
13: for x ?
?
s.t xZ ?
T do4: Z.m?
max(Z.m,xZ.b?
xZ.m)5: end for6: end forAlgorithm 2 Max-ARPA: second pass1: for Z = Pz ?
T do2: Pz.q?
Pz.p3: for x ?
?
s.t xP ?
T do4: if xPz ?
T then5: Pz.q?
max(Pz.q,xPz.q)6: else7: Pz.q?
max(Pz.q,Pz.p?
xP.b?
xP.m)8: end if9: end for10: end for4.4 SearchThe search for the true optimum derivation is il-lustrated in Algorithm 3.
The algorithm takes asinput the initial proposal distribution g(0)(d) (see?4.2, Figure 3) and a maximum error  (which weset to a small constant 0.001 rather than zero, toavoid problems with floating point precision).
Inline 3 we find the optimum derivation d in g(0)(see ?4.5).
The variable g?stores the maximumscore w.r.t.
the current proposal, while the vari-able f?stores the maximum score observed thusfar w.r.t.
the true model (note that in line 5 we as-sess the true score of d).
In line 6 we start a loopthat runs until the error falls below .
This error isthe difference (in log-domain) between the proxymaximum g?and the best true score observed thusfar f?.13In line 7, we refine the current proposalusing evidence from d (see ?4.6).
In line 9, weupdate the maximum derivation searching throughthe refined proposal.
In line 11, we keep track ofthe best score so far according to the true model,in order to compute the updated gap in line 6.4.5 Dynamic ProgrammingFinding the best derivation in a proposal hyper-graph is straightforward with standard dynamicprogramming.
We can compute inside weightsin the max-times semiring in time proportional13Because g(t)upperbounds f everywhere, in optimisationwe have a guarantee that the maximum of f must lie in theinterval [f?, g?)
(see Figure 2) and the quantity g??
f?isan upperbound on the error that we incur if we early-stop thesearch at any given time t. This bound provides a principledcriterion in trading accuracy for performance (a direction thatwe leave for future work).
Note that most algorithms for ap-proximate search produce solutions with unbounded error.Algorithm 3 Exact decoding1: function OPTIMISE(g(0), )2: t?
0 .
step3: d?
argmaxdg(t)(d)4: g??
g(t)(d)5: f??
f(d)6: while (q??
f??
) do .
 is the maximum error7: g(t+1)?
refine(g(t),d) .
update proposal8: t?
t+ 19: d?
argmaxdg(t)(d) .
update argmax10: g??
g(t)(d)11: f??
max(f?, f(d)) .
update ?best so far?12: end while13: return g(t), d14: end functionto O(|V | + |E|) (Goodman, 1999).
Once insideweights have been computed, finding the Viterbi-derivation starting from the root is straightforward.A simple, though important, optimisation con-cerns the computation of inside weights.
The in-side algorithm (Baker, 1979) requires a bottom-uptraverse of the nodes in V .
To do that, we topolog-ically sort the nodes in V at time t = 0 and main-tain a sorted list of nodes as we refine g throughoutthe search ?
thus avoiding having to recompute thepartial ordering of the nodes at every iteration.4.6 RefinementIf a derivation d = argmaxdg(t)(d) is such thatg(t)(d) f(d), there must be in d at least one n-gram whose upperbound LM weight is far aboveits true LM weight.
We then lower g(t)locally byrefining only nonterminal nodes that participate ind.
Nonterminal nodes are refined by having theirLM states extended one word at a time.14For an illustration, assume we are perform-ing optimisation with a bigram LM.
Supposethat in the first iteration a derivation d0=argmaxdg(0)(d) is obtained.
Now consider anedge in d0[l, C, r, ] ?y1w??
[l0, C0, r0, ]where an empty LM state is made explicit (with anempty string ) and ?y1represents a target phrase.We refine the edge?s head [l0, C0, r0, ] by creatinga node based on it, however, with an extended LMstate, i.e., [l0, C0, r0, y1].
This motivates a splitof the set of incoming edges to the original node,such that, if the target projection of an incoming14The refinement operation is a special case of a generalfinite-state intersection.
However, keeping its effect local toderivations going through a specific node is non-trivial usingthe general mechanism and justifies a tailored operation.1243edge ends in y1, that edge is reconnected to thenew node as below.
[l, C, r, ] ?y1w??
[l0, C0, r0, y1]The outgoing edges from the new node arereweighted copies of those leaving the originalnode.
That is, outgoing edges such as[l0, C0, r0, ] y2?w??
[l?, C?, r?, ??
]motivate edges such as[l0, C0, r0, y1] y2?w?w?????
[l?, C?, r?, ??
]where w?= ?
?qLM(y1y2)/qLM(y2) is a change in LMprobability due to an extended context.Figure 4 is the logic program that constructs therefined hypergraph in the general case.
In com-parison to Figure 3, items are now extended tostore an LM state.
The input is the original hy-pergraph G = ?V,E?
and a node v0?
V to berefined by left-extending its LM state ?0with theword y.
In the program,?u?w??
v?with u,v ?
Vand ?
?
?
?represents an edge in E. An item[l, C, r, ?
]v(annotated with a state v ?
V ) rep-resents a node (in the refined hypergraph) whosesignature is equivalent to v (in the input hyper-graph).
We start with AXIOMS by copying thenodes in G. In COPY, edges from G are copiedunless they are headed by v0and their target pro-jections end in y?0(the extended context).
Suchedges are processed by REFINE, which instead ofcopying them, creates new ones headed by a re-fined version of v0.
Finally, REWEIGHT contin-ues from the refined node with reweighted copiesof the edges leaving v0.
The weight update repre-sents a change in LM probability (w.r.t.
the upper-bound distribution) due to an extended context.5 ExperimentsWe used the dataset made available by the Work-shop on Statistical Machine Translation (WMT)(Bojar et al., 2013) to train a German-Englishphrase-based system using the Moses toolkit(Koehn et al., 2007) in a standard setup.
Forphrase extraction, we used both Europarl (Koehn,2005) and News Commentaries (NC) totallingabout 2.2M sentences.15For language modelling,in addition to the monolingual parts of Europarl15Pre-processing: tokenisation, truecasing and automaticcompound-splitting (German only).
Following Durrani et al.
(2013), we set the maximum phrase length to 5.INPUTG = ?V,E?v0= [l0, C0, r0, ?0] ?
V where ?0?
?
?y ?
?ITEM [l, C, r, ?
?
??
]AXIOMS[l, C, r, ?
]vv ?
VCOPY[l, C, r, ?]u?u?w??
v?
[l?, C?, r?, ??
]v: wv 6= v0?
??
6= ?y?0?, ?
?, ?, ?
?
?
?REFINE[l, C,R, ?]u?u?w??
v0?
[l0, C0, r0, y?0] : w??
= ?y?0?, ?, ?
?
?
?REWEIGHT[l0, C0, r0, y?0]?v0?w??
v?
[l, C, r, ?
]v: w ?
w?
?, ?
?
?
?where w?= ?
?qLM(y?0)qLM(?0)Figure 4: Local intersection via LM right state re-finement.
The input is a hypergraph G = ?V,E?,a node v0?
V singly identified by its carry[l0, C0, r0, ?0] and a left-extension y for its LMcontext ?0.
The program copies most of the edges?u?w??
v??
E. If a derivation goes through v0and the string under v0ends in y?0, the programrefines and reweights it.and NC, we added News-2013 totalling about 25Msentences.
We performed language model interpo-lation and batch-mira tuning (Cherry and Foster,2012) using newstest2010 (2,849 sentence pairs).For tuning we used cube pruning with a large beamsize (k = 5000) and a distortion limit d = 4.
Un-pruned language models were trained using lmplz(Heafield et al., 2013b) which employs modifiedKneser-Ney smoothing (Kneser and Ney, 1995).We report results on newstest2012.Our exact decoder produces optimal translationderivations for all the 3,003 sentences in the testset.
Table 1 summarises the performance of ournovel decoder for language models of order n = 3to n = 5.
For 3-gram LMs we also varied the dis-tortion limit d (from 4 to 6).
We report the averagetime (in seconds) to build the initial proposal, thetotal run time of the algorithm, the number of it-erations N before convergence, and the size of thehypergraph in the end of the search (in thousandsof nodes and thousands of edges).1616The size of the initial proposal does not depend on LMorder, but rather on distortion limit (see Figure 3): on aver-age (in thousands) |V0| = 0.6 and |E0| = 27 with d = 4,|V0| = 1.3 and |E0| = 70 with d = 5, and |V0| = 2.5 and1244n d build (s) total (s) N |V | |E|3 4 1.5 21 190 2.5 1593 5 3.5 55 303 4.4 3433 6 10 162 484 8 7254 4 1.5 50 350 4 2885 4 1.5 106 555 6.1 450Table 1: Performance of the exact decoder interms of: time to build g(0), total decoding time in-cluding build, number of iterations (N), and num-ber of nodes and edges (in thousands) at the end ofthe search.It is insightful to understand how different as-pects of the initial proposal impact on perfor-mance.
Increasing the translation option limit (tol)leads to g(0)having more edges (this dependencyis linear with tol).
In this case, the number ofnodes is only minimally affected ?
due to the pos-sibility of a few new segmentations.
The maxi-mum phrase length (mpl) introduces in g(0)moreconfigurations of reordering constraints ([l, C] inFigure 3).
However, not many more, due to Cbeing limited by the distortion limit d. In prac-tice, we observe little impact on time performance.Increasing d introduces many more permutationsof the input leading to exponentially many morenodes and edges.
Increasing the order n of the LMhas no impact on g(0)and its impact on the overallsearch is expressed in terms of a higher number ofnodes being locally intersected.An increased hypergraph, be it due to addi-tional nodes or additional edges, necessarily leadsto slower iterations because at each iteration wemust compute inside weights in timeO(|V |+|E|).The number of nodes has the larger impact on thenumber of iterations.
OS?is very efficient in ig-noring hypotheses (edges) that cannot compete foran optimum.
For instance, we observe that run-ning time depends linearly on tol only through thecomputation of inside weights, while the numberof iterations is only minimally affected.17An in-|E0| = 178 with d = 6.
Observe the exponential depen-dency on distortion limit, which also leads to exponentiallylonger running times.17It is possible to reduce the size of the hypergraphthroughout the search using the upperbound on the searcherror g??
f?to prune hypotheses that surely do not standa chance of competing for the optimum (Graehl, 2005).
An-other direction is to group edges connecting the same nonter-minal nodes into one partial edge (Heafield et al., 2013a) ?this is particularly convenient due to our method only visitingthe 1-best derivation from g(d) at each iteration.nNodes at level m LM states at level m0 1 2 3 4 1 2 3 43 0.4 1.2 0.5 - - 113 263 - -4 0.4 1.6 1.4 0.3 - 132 544 212 -5 0.4 2.1 2.4 0.7 0.1 142 790 479 103Table 2: Average number of nodes (in thousands)whose LM state encode an m-gram, and averagenumber of unique LM states of order m in the fi-nal hypergraph for different n-gram LMs (d = 4everywhere).creased LM order, for a fixed distortion limit, im-pacts much more on the number of iterations thanon the average running time of a single iteration.Fixing d = 4, the average time per iteration is 0.1(n = 3), 0.13 (n = 4) and 0.18 (n = 5).
Fixing a3-gram LM, we observe 0.1 (d = 4), 0.17 (d = 5)and 0.31 (d = 6).
Note the exponential growthof the latter, due to a proposal encoding exponen-tially many more permutations.Table 2 shows the average degree of refine-ment of the nodes in the final proposal.
Nodesare shown by level of refinement, where m indi-cates that they store m words in their carry.
Thetable also shows the number of unique m-gramsever incorporated to the proposal.
This table il-lustrates well how our decoding algorithm movesfrom a coarse upperbound where every node storesan empty string to a variable-order representationwhich is sufficient to prove an optimum derivation.In our approach a complete derivation is opti-mised from the proxy model at each iteration.
Weobserve that over 99% of these derivations projectonto distinct strings.
In addition, while the opti-mum solution may be found early in the search, acertificate of optimality requires refining the proxyuntil convergence (see ?4.1).
It turns out that mostof the solutions are first encountered as late as inthe last 6-10% of the iterations.We use the optimum derivations obtained withour exact decoder to measure the number of searcherrors made by beam search and cube pruning withincreasing beam sizes (see Table 3).
Beam searchreaches optimum derivations with beam sizes k ?500 for all language models tested.
Cube prun-ing, on the other hand, still makes mistakes atk = 1000.
Table 4 shows translation qualityachieved with different beam sizes for cube prun-ing and compares it to exact decoding.
Note thatfor k ?
104cube pruning converges to optimum1245kBeam search Cube pruning3 4 5 3 4 510 938 1294 1475 2168 2347 237710219 60 112 613 999 11261030 0 0 29 102 1671040 0 0 0 4 7Table 3: Beam search and cube pruning search er-rors (out of 3,003 test samples) by beam size usingLMs of order 3 to 5 (d = 4).order 3 4 5k d = 4 d = 5 d = 6 d = 4 d = 410 20.47 20.13 19.97 20.71 20.6910221.14 21.18 21.08 21.73 21.7610321.27 21.34 21.32 21.89 21.9110421.29 21.37 21.37 21.92 21.93OS?21.29 21.37 21.37 21.92 21.93Table 4: Translation quality in terms of BLEU asa function of beam size in cube pruning with lan-guage models of order 3 to 5.
The bottom rowshows BLEU for our exact decoder.derivations in the vast majority of the cases (100%with a 3-gram LM) and translation quality in termsof BLEU is no different from OS?.
However, withk < 104both model scores and translation qualitycan be improved.
Figure 5 shows a finer view onsearch errors as a function of beam size for LMsof order 3 to 5 (fixed d = 4).
In Figure 6, we fixa 3-gram LM and vary the distortion limit (from 4to 6).
Dotted lines correspond to beam search anddashed lines correspond to cube pruning.6 Conclusions and Future WorkWe have presented an approach to decoding withunpruned hypergraphs using upperbounds on thelanguage model distribution.
The algorithm is aninstance of a coarse-to-fine strategy with connec-tions to A?and adaptive rejection sampling knownas OS?.
We have tested our search algorithm us-ing state-of-the-art phrase-based models employ-ing robust language models.
Our algorithm is ableto decode all sentences of a standard test set inmanageable time consuming very little memory.We have performed an analysis of search errorsmade by beam search and cube pruning and foundthat both algorithms perform remarkably well forphrase-based decoding.
In the case of cube prun-ing, we show that model score and translation102 103 104[log] Beam size100101102103104[log] SearcherrorsSearch errors in newstest2012CP 3-gramCP 4-gramCP 5-gramBS 3-gramBS 4-gramBS 5-gramFigure 5: Search errors made by beam search andcube pruning as a function of beam-size.102 103 104[log] Beam size100101102103104[log] SearcherrorsSearch errors in newstest2012 (3-gram LM)CP d=4CP d=5CP d=6BS d=4BS d=5BS d=6Figure 6: Search errors made by beam search andcube pruning as a function of the distortion limit(decoding with a 3-gram LM).quality can be improved for beams k < 10, 000.There are a number of directions that we intendto investigate to speed up our decoder, such as: (1)error-safe pruning based on search error bounds;(2) use of reinforcement learning to guide the de-coder in choosing which n-gram contexts to ex-tend; and (3) grouping edges into partial edges,effectively reducing the size of the hypergraph andultimately computing inside weights in less time.AcknowledgmentsThe work of Wilker Aziz and Lucia Specia wassupported by EPSRC (grant EP/K024272/1).1246ReferencesMichael Auli, Adam Lopez, Hieu Hoang, and PhilippKoehn.
2009.
A systematic analysis of transla-tion model search spaces.
In Proceedings of theFourth Workshop on Statistical Machine Transla-tion, StatMT ?09, pages 224?232, Stroudsburg, PA,USA.
Association for Computational Linguistics.Wilker Aziz, Marc Dymetman, and Sriram Venkatapa-thy.
2013.
Investigations in exact inference for hi-erarchical translation.
In Proceedings of the EighthWorkshop on Statistical Machine Translation, pages472?483, Sofia, Bulgaria, August.
Association forComputational Linguistics.James K. Baker.
1979.
Trainable grammars for speechrecognition.
In Proceedings of the Spring Confer-ence of the Acoustical Society of America, pages547?550, Boston, MA, June.Ond?rej Bojar, Christian Buck, Chris Callison-Burch,Christian Federmann, Barry Haddow, PhilippKoehn, Christof Monz, Matt Post, Radu Soricut, andLucia Specia.
2013.
Findings of the 2013 Work-shop on Statistical Machine Translation.
In Pro-ceedings of the Eighth Workshop on Statistical Ma-chine Translation, pages 1?44, Sofia, Bulgaria, Au-gust.
Association for Computational Linguistics.Peter F. Brown, Vincent J. Della Pietra, StephenA.
Della Pietra, and Robert L. Mercer.
1993.The mathematics of statistical machine translation:parameter estimation.
Computational Linguistics,19(2):263?311, June.Simon Carter, Marc Dymetman, and GuillaumeBouchard.
2012.
Exact sampling and decoding inhigh-order hidden Markov models.
In Proceedingsof the 2012 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, pages 1125?1134, JejuIsland, Korea, July.
Association for ComputationalLinguistics.Yin-Wen Chang and Michael Collins.
2011.
Exact de-coding of phrase-based translation models throughLagrangian relaxation.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, EMNLP ?11, pages 26?37, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Colin Cherry and George Foster.
2012.
Batch tun-ing strategies for statistical machine translation.
InProceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,NAACL HLT ?12, pages 427?436, Stroudsburg, PA,USA.
Association for Computational Linguistics.David Chiang.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In Pro-ceedings of the 43rd Annual Meeting on Associationfor Computational Linguistics, ACL ?05, pages 263?270, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33:201?228.Shay B. Cohen, Robert J. Simmons, and Noah A.Smith.
2008.
Dynamic programming algorithms asproducts of weighted logic programs.
In Maria Gar-cia de la Banda and Enrico Pontelli, editors, LogicProgramming, volume 5366 of Lecture Notes inComputer Science, pages 114?129.
Springer BerlinHeidelberg.G Dantzig, R Fulkerson, and S Johnson.
1954.
So-lution of a large-scale traveling-salesman problem.Operations Research, 2:393?410.Nadir Durrani, Barry Haddow, Kenneth Heafield, andPhilipp Koehn.
2013.
Edinburgh?s machine trans-lation systems for European language pairs.
In Pro-ceedings of the Eighth Workshop on Statistical Ma-chine Translation, pages 114?121, Sofia, Bulgaria,August.
Association for Computational Linguistics.Chris Dyer and Philip Resnik.
2010.
Context-freereordering, finite-state translation.
In Human Lan-guage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, HLT ?10, pages 858?866, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Marc Dymetman, Guillaume Bouchard, and SimonCarter.
2012.
Optimization and sampling for NLPfrom a unified viewpoint.
In Proceedings of theFirst International Workshop on Optimization Tech-niques for Human Language Technology, pages 79?94, Mumbai, India, December.
The COLING 2012Organizing Committee.Giorgio Gallo, Giustino Longo, Stefano Pallottino, andSang Nguyen.
1993.
Directed hypergraphs andapplications.
Discrete Applied Mathematics, 42(2-3):177?201, April.Ulrich Germann, Michael Jahr, Kevin Knight, DanielMarcu, and Kenji Yamada.
2001.
Fast decoding andoptimal decoding for machine translation.
In Pro-ceedings of the 39th Annual Meeting on Associationfor Computational Linguistics, ACL ?01, pages 228?235, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Joshua Goodman.
1999.
Semiring parsing.
Comput.Linguist., 25(4):573?605, December.Jonathan Graehl.
2005.
Relatively useless pruning.Technical report, USC Information Sciences Insti-tute.Peter E. Hart, Nils J. Nilsson, and Bertram Raphael.1968.
A formal basis for the heuristic determina-tion of minimum cost paths.
IEEE Transactions OnSystems Science And Cybernetics, 4(2):100?107.Kenneth Heafield, Philipp Koehn, and Alon Lavie.2013a.
Grouping language model boundary words1247to speed k-best extraction from hypergraphs.
In Pro-ceedings of the 2013 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages958?968, Atlanta, Georgia, USA, June.Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.Clark, and Philipp Koehn.
2013b.
Scalable modi-fied Kneser-Ney language model estimation.
In Pro-ceedings of the 51st Annual Meeting of the Associa-tion for Computational Linguistics (Volume 2: ShortPapers), pages 690?696, Sofia, Bulgaria, August.Association for Computational Linguistics.Kenneth Heafield, Michael Kayser, and Christopher D.Manning.
2014.
Faster Phrase-Based decoding byrefining feature state.
In Proceedings of the Associa-tion for Computational Linguistics, Baltimore, MD,USA, June.Liang Huang and David Chiang.
2007.
Forest rescor-ing: Faster decoding with integrated language mod-els.
In Proceedings of the 45th Annual Meeting ofthe Association of Computational Linguistics, pages144?151, Prague, Czech Republic, June.
Associa-tion for Computational Linguistics.Gonzalo Iglesias, Adri`a de Gispert, Eduardo R. Banga,and William Byrne.
2009.
Rule filtering by patternfor efficient hierarchical translation.
In Proceed-ings of the 12th Conference of the European Chapterof the ACL (EACL 2009), pages 380?388, Athens,Greece, March.
Association for Computational Lin-guistics.Daniel Jurafsky and James H. Martin.
2000.
Speechand Language Processing: An Introduction to Nat-ural Language Processing, Computational Linguis-tics and Speech Recognition.
Series in Artificial In-telligence.
Prentice Hall, 1 edition.Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for m-gram language modeling.
Ac-coustics, Speech, and Signal Processing, 1:181?184.Kevin Knight.
1999.
Decoding complexity in word-replacement translation models.
Comput.
Linguist.,25(4):607?615, December.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of the 2003 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics on Human Language Technology - Vol-ume 1, NAACL ?03, pages 48?54, Stroudsburg, PA,USA.
Association for Computational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ond?rej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: opensource toolkit for statistical machine translation.
InProceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,ACL ?07, pages 177?180, Stroudsburg, PA, USA.Association for Computational Linguistics.Philipp Koehn.
2005.
Europarl: A parallel corpusfor statistical machine translation.
In Proceedingsof Machine Translation Summit, pages 79?86.Shankar Kumar, Yonggang Deng, and William Byrne.2006.
A weighted finite state transducer transla-tion template model for statistical machine transla-tion.
Natural Language Engineering, 12(1):35?75,March.Zhifei Li and Sanjeev Khudanpur.
2008.
A scal-able decoder for parsing-based machine translationwith equivalent language model state maintenance.In Proceedings of the Second Workshop on Syntaxand Structure in Statistical Translation, SSST ?08,pages 10?18, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Adam Lopez.
2008.
Statistical machine translation.ACM Computing Surveys, 40(3):8:1?8:49, August.Adam Lopez.
2009.
Translation as weighted de-duction.
In Proceedings of the 12th Conference ofthe European Chapter of the Association for Com-putational Linguistics, EACL ?09, pages 532?540,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Franz Josef Och, Nicola Ueffing, and Hermann Ney.2001.
An efficient A* search algorithm for statisti-cal machine translation.
In Proceedings of the work-shop on Data-driven methods in machine translation- Volume 14, DMMT ?01, pages 1?8, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics, volume 1 of ACL ?03, pages 160?167, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Slav Petrov, Aria Haghighi, and Dan Klein.
2008.Coarse-to-fine syntactic machine translation usinglanguage projections.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, EMNLP ?08, pages 108?116, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Sebastian Riedel and James Clarke.
2009.
Revisit-ing optimal decoding for machine translation IBMmodel 4.
In Proceedings of Human Language Tech-nologies: The 2009 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, Companion Volume: Short Pa-pers, NAACL-Short ?09, pages 5?8, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.1248Christian P. Robert and George Casella.
2004.
MonteCarlo Statistical Methods (Springer Texts in Statis-tics).
Springer-Verlag New York, Inc., Secaucus,NJ, USA.Alexander M. Rush and Michael Collins.
2011.
Ex-act decoding of syntactic translation models throughLagrangian relaxation.
In Proceedings of the 49thAnnual Meeting of the Association for Computa-tional Linguistics: Human Language Technologies- Volume 1, HLT ?11, pages 72?82, Stroudsburg, PA,USA.
Association for Computational Linguistics.Christoph Tillmann, Stephan Vogel, Hermann Ney,and A. Zubiaga.
1997.
A DP based search usingmonotone alignments in statistical translation.
InProceedings of the eighth conference on Europeanchapter of the Association for Computational Lin-guistics, EACL ?97, pages 289?296, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Mikhail Zaslavskiy, Marc Dymetman, and Nicola Can-cedda.
2009.
Phrase-based statistical machinetranslation as a traveling salesman problem.
In Pro-ceedings of the Joint Conference of the 47th AnnualMeeting of the ACL and the 4th International JointConference on Natural Language Processing of theAFNLP: Volume 1 - Volume 1, ACL ?09, pages 333?341, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.1249
