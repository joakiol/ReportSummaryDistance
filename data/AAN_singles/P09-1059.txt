Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 522?530,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPAutomatic Adaptation of Annotation Standards:Chinese Word Segmentation and POS Tagging ?
A Case StudyWenbin Jiang ?
Liang Huang ?
Qun Liu ?
?Key Lab.
of Intelligent Information Processing ?Google ResearchInstitute of Computing Technology 1350 Charleston Rd.Chinese Academy of Sciences Mountain View, CA 94043, USAP.O.
Box 2704, Beijing 100190, China lianghuang@google.com{jiangwenbin, liuqun}@ict.ac.cn liang.huang.sh@gmail.comAbstractManually annotated corpora are valuablebut scarce resources, yet for many anno-tation tasks such as treebanking and se-quence labeling there exist multiple cor-pora with different and incompatible anno-tation guidelines or standards.
This seemsto be a great waste of human efforts, andit would be nice to automatically adaptone annotation standard to another.
Wepresent a simple yet effective strategy thattransfers knowledge from a differently an-notated corpus to the corpus with desiredannotation.
We test the efficacy of thismethod in the context of Chinese wordsegmentation and part-of-speech tagging,where no segmentation and POS taggingstandards are widely accepted due to thelack of morphology in Chinese.
Experi-ments show that adaptation from the muchlarger People?s Daily corpus to the smallerbut more popular Penn Chinese Treebankresults in significant improvements in bothsegmentation and tagging accuracies (witherror reductions of 30.2% and 14%, re-spectively), which in turn helps improveChinese parsing accuracy.1 IntroductionMuch of statistical NLP research relies on somesort of manually annotated corpora to train theirmodels, but these resources are extremely expen-sive to build, especially at a large scale, for ex-ample in treebanking (Marcus et al, 1993).
How-ever the linguistic theories underlying these anno-tation efforts are often heavily debated, and as a re-sult there often exist multiple corpora for the sametask with vastly different and incompatible anno-tation philosophies.
For example just for Englishtreebanking there have been the Chomskian-style{1 B2 o3 ?4 ?5 u6NR NN VV NRU.S.
Vice-President visited China{1 B2 o3 ?4 ?5 u6ns b n vU.S.
Vice President visited-ChinaFigure 1: Incompatible word segmentation andPOS tagging standards between CTB (upper) andPeople?s Daily (below).Penn Treebank (Marcus et al, 1993) the HPSGLinGo Redwoods Treebank (Oepen et al, 2002),and a smaller dependency treebank (Buchholz andMarsi, 2006).
A second, related problem is thatthe raw texts are also drawn from different do-mains, which for the above example range fromfinancial news (PTB/WSJ) to transcribed dialog(LinGo).
These two problems seem be a greatwaste in human efforts, and it would be nice ifone could automatically adapt from one annota-tion standard and/or domain to another in orderto exploit much larger datasets for better train-ing.
The second problem, domain adaptation, isvery well-studied, e.g.
by Blitzer et al (2006)and Daume?
III (2007) (and see below for discus-sions), so in this paper we focus on the less stud-ied, but equally important problem of annotation-style adaptation.We present a very simple yet effective strategythat enables us to utilize knowledge from a differ-ently annotated corpora for the training of a modelon a corpus with desired annotation.
The basicidea is very simple: we first train on a source cor-pus, resulting in a source classifier, which is usedto label the target corpus and results in a ?source-style?
annotation of the target corpus.
We then522train a second model on the target corpus with thefirst classifier?s prediction as additional featuresfor guided learning.This method is very similar to some ideas indomain adaptation (Daume?
III and Marcu, 2006;Daume?
III, 2007), but we argue that the underly-ing problems are quite different.
Domain adapta-tion assumes the labeling guidelines are preservedbetween the two domains, e.g., an adjective is al-ways labeled as JJ regardless of from Wall StreetJournal (WSJ) or Biomedical texts, and only thedistributions are different, e.g., the word ?control?is most likely a verb in WSJ but often a nounin Biomedical texts (as in ?control experiment?
).Annotation-style adaptation, however, tackles theproblem where the guideline itself is changed, forexample, one treebank might distinguish betweentransitive and intransitive verbs, while merging thedifferent noun types (NN, NNS, etc.
), and for ex-ample one treebank (PTB) might be much flatterthan the other (LinGo), not to mention the fun-damental disparities between their underlying lin-guistic representations (CFG vs. HPSG).
In thissense, the problem we study in this paper seemsmuch harder and more motivated from a linguistic(rather than statistical) point of view.
More inter-estingly, our method, without any assumption onthe distributions, can be simultaneously applied toboth domain and annotation standards adaptationproblems, which is very appealing in practice be-cause the latter problem often implies the former,as in our case study.To test the efficacy of our method we chooseChinese word segmentation and part-of-speechtagging, where the problem of incompatible an-notation standards is one of the most evident: sofar no segmentation standard is widely accepteddue to the lack of a clear definition of Chinesewords, and the (almost complete) lack of mor-phology results in much bigger ambiguities andheavy debates in tagging philosophies for Chi-nese parts-of-speech.
The two corpora used inthis study are the much larger People?s Daily (PD)(5.86M words) corpus (Yu et al, 2001) and thesmaller but more popular Penn Chinese Treebank(CTB) (0.47M words) (Xue et al, 2005).
Theyused very different segmentation standards as wellas different POS tagsets and tagging guidelines.For example, in Figure 1, People?s Daily breaks?Vice-President?
into two words while combinesthe phrase ?visited-China?
as a compound.
AlsoCTB has four verbal categories (VV for normalverbs, and VC for copulas, etc.)
while PD has onlyone verbal tag (v) (Xia, 2000).
It is preferable totransfer knowledge from PD to CTB because thelatter also annotates tree structures which is veryuseful for downstream applications like parsing,summarization, and machine translation, yet it ismuch smaller in size.
Indeed, many recent effortson Chinese-English translation and Chinese pars-ing use the CTB as the de facto segmentation andtagging standards, but suffers from the limited sizeof training data (Chiang, 2007; Bikel and Chiang,2000).
We believe this is also a reason why state-of-the-art accuracy for Chinese parsing is muchlower than that of English (CTB is only half thesize of PTB).Our experiments show that adaptation from PDto CTB results in a significant improvement in seg-mentation and POS tagging, with error reductionsof 30.2% and 14%, respectively.
In addition, theimproved accuracies from segmentation and tag-ging also lead to an improved parsing accuracy onCTB, reducing 38% of the error propagation fromword segmentation to parsing.
We envision thistechnique to be general and widely applicable tomany other sequence labeling tasks.In the rest of the paper we first briefly reviewthe popular classification-based method for wordsegmentation and tagging (Section 2), and thendescribe our idea of annotation adaptation (Sec-tion 3).
We then discuss other relevant previouswork including co-training and classifier combina-tion (Section 4) before presenting our experimen-tal results (Section 5).2 Segmentation and Tagging asCharacter ClassificationBefore describing the adaptation algorithm, wegive a brief introduction of the baseline characterclassification strategy for segmentation, as well asjoint segmenation and tagging (henceforth ?JointS&T?).
following our previous work (Jiang et al,2008).
Given a Chinese sentence as sequence of ncharacters:C1 C2 .. Cnwhere Ci is a character, word segmentation aimsto split the sequence into m(?
n) words:C1:e1 Ce1+1:e2 .. Cem?1+1:emwhere each subsequence Ci:j indicates a Chineseword spanning from characters Ci to Cj (both in-523Algorithm 1 Perceptron training algorithm.1: Input: Training examples (xi, yi)2: ~??
03: for t?
1 .. T do4: for i?
1 .. N do5: zi ?
argmaxz?GEN(xi) ?
(xi, z) ?
~?6: if zi 6= yi then7: ~??
~?
+ ?
(xi, yi)??
(xi, zi)8: Output: Parameters ~?clusive).
While in Joint S&T, each word is furtherannotated with a POS tag:C1:e1/t1 Ce1+1:e2/t2 .. Cem?1+1:em/tmwhere tk(k = 1..m) denotes the POS tag for theword Cek?1+1:ek .2.1 Character Classification MethodXue and Shen (2003) describe for the first timethe character classification approach for Chineseword segmentation, where each character is givena boundary tag denoting its relative position in aword.
In Ng and Low (2004), Joint S&T can alsobe treated as a character classification problem,where a boundary tag is combined with a POS tagin order to give the POS information of the wordcontaining these characters.
In addition, Ng andLow (2004) find that, compared with POS taggingafter word segmentation, Joint S&T can achievehigher accuracy on both segmentation and POStagging.
This paper adopts the tag representationof Ng and Low (2004).
For word segmentationonly, there are four boundary tags:?
b: the begin of the word?
m: the middle of the word?
e: the end of the word?
s: a single-character wordwhile for Joint S&T, a POS tag is attached to thetail of a boundary tag, to incorporate the wordboundary information and POS information to-gether.
For example, b-NN indicates that the char-acter is the begin of a noun.
After all charac-ters of a sentence are assigned boundary tags (orwith POS postfix) by a classifier, the correspond-ing word sequence (or with POS) can be directlyderived.
Take segmentation for example, a char-acter assigned a tag s or a subsequence of wordsassigned a tag sequence bm?e indicates a word.2.2 Training Algorithm and FeaturesNow we will show the training algorithm of theclassifier and the features used.
Several classi-fication models can be adopted here, however,we choose the averaged perceptron algorithm(Collins, 2002) because of its simplicity and highaccuracy.
It is an online training algorithm andhas been successfully used in many NLP tasks,such as POS tagging (Collins, 2002), parsing(Collins and Roark, 2004), Chinese word segmen-tation (Zhang and Clark, 2007; Jiang et al, 2008),and so on.Similar to the situation in other sequence label-ing problems, the training procedure is to learn adiscriminative model mapping from inputs x ?
Xto outputs y ?
Y , where X is the set of sentencesin the training corpus and Y is the set of corre-sponding labelled results.
Following Collins, weuse a function GEN(x) enumerating the candi-date results of an input x , a representation?map-ping each training example (x, y) ?
X ?
Y to afeature vector?
(x, y) ?
Rd, and a parameter vec-tor ~?
?
Rd corresponding to the feature vector.For an input character sequence x, we aim to findan output F (x) that satisfies:F (x) = argmaxy?GEN(x)?
(x, y) ?
~?
(1)where?
(x, y) ?~?
denotes the inner product of fea-ture vector ?
(x, y) and the parameter vector ~?.Algorithm 1 depicts the pseudo code to tune theparameter vector ~?.
In addition, the ?averaged pa-rameters?
technology (Collins, 2002) is used to al-leviate overfitting and achieve stable performance.Table 1 lists the feature template and correspond-ing instances.
Following Ng and Low (2004),the current considering character is denoted as C0,while the ith character to the left of C0 as C?i,and to the right as Ci.
There are additional twofunctions of which each returns some property of acharacter.
Pu(?)
is a boolean function that checkswhether a character is a punctuation symbol (re-turns 1 for a punctuation, 0 for not).
T (?)
is amulti-valued function, it classifies a character intofour classifications: number, date, English letterand others (returns 1, 2, 3 and 4, respectively).3 Automatic Annotation AdaptationFrom this section, several shortened forms areadopted for representation inconvenience.
We usesource corpus to denote the corpus with the anno-tation standard that we don?t require, which is of524Feature Template InstancesCi (i = ?2..2) C?2 =?, C?1 =, C0 =c, C1 =?, C2 = RCiCi+1 (i = ?2..1) C?2C?1 =?, C?1C0 =c, C0C1 =c?, C1C2 =?RC?1C1 C?1C1 =?Pu(C0) Pu(C0) = 0T (C?2)T (C?1)T (C0)T (C1)T (C2) T (C?2)T (C?1)T (C0)T (C1)T (C2) = 11243Table 1: Feature templates and instances from Ng and Low (Ng and Low, 2004).
Suppose we areconsidering the third character ?c?
in ?
? c ?R?.course the source of the adaptation, while targetcorpus denoting the corpus with the desired stan-dard.
And correspondingly, the two annotationstandards are naturally denoted as source standardand target standard, while the classifiers follow-ing the two annotation standards are respectivelynamed as source classifier and target classifier, ifneeded.Considering that word segmentation and JointS&T can be conducted in the same character clas-sification manner, we can design an unified stan-dard adaptation framework for the two tasks, bytaking the source classifier?s classification resultas the guide information for the target classifier?sclassification decision.
The following section de-picts this adaptation strategy in detail.3.1 General Adaptation StrategyIn detail, in order to adapt knowledge from thesource corpus, first, a source classifier is trainedon it and therefore captures the knowledge it con-tains; then, the source classifier is used to clas-sify the characters in the target corpus, althoughthe classification result follows a standard that wedon?t desire; finally, a target classifier is trainedon the target corpus, with the source classifier?sclassification result as additional guide informa-tion.
The training procedure of the target clas-sifier automatically learns the regularity to trans-fer the source classifier?s predication result fromsource standard to target standard.
This regular-ity is incorporated together with the knowledgelearnt from the target corpus itself, so as to ob-tain enhanced predication accuracy.
For a givenun-classified character sequence, the decoding isanalogous to the training.
First, the character se-quence is input into the source classifier to ob-tain an source standard annotated classificationresult, then it is input into the target classifierwith this classification result as additional infor-mation to get the final result.
This coincides withthe stacking method for combining dependencyparsers (Martins et al, 2008; Nivre and McDon-source corpustrain withnormal featuressource classifiertrain withadditional featurestarget classifiertarget corpus source annotationclassification resultFigure 2: The pipeline for training.raw sentence source classifier source annotationclassification resulttarget classifiertarget annotationclassification resultFigure 3: The pipeline for decoding.ald, 2008), and is also similar to the Pred baselinefor domain adaptation in (Daume?
III and Marcu,2006; Daume?
III, 2007).
Figures 2 and 3 showthe flow charts for training and decoding.The utilization of the source classifier?s classi-fication result as additional guide information re-sorts to the introduction of new features.
For thecurrent considering character waiting for classi-fication, the most intuitive guide features is thesource classifier?s classification result itself.
How-ever, our effort isn?t limited to this, and more spe-cial features are introduced: the source classifier?sclassification result is attached to every featurelisted in Table 1 to get combined guide features.This is similar to feature design in discriminativedependency parsing (McDonald et al, 2005; Mc-525Donald and Pereira, 2006), where the basic fea-tures, composed of words and POSs in the context,are also conjoined with link direction and distancein order to obtain more special features.
Table 2shows an example of guide features and basic fea-tures, where ??
= b ?
represents that the sourceclassifier classifies the current character as b, thebeginning of a word.Such combination method derives a series ofspecific features, which helps the target classifierto make more precise classifications.
The parame-ter tuning procedure of the target classifier will au-tomatically learn the regularity of using the sourceclassifier?s classification result to guide its deci-sion making.
For example, if a current consid-ering character shares some basic features in Ta-ble 2 and it is classified as b, then the target clas-sifier will probably classify it as m. In addition,the training procedure of the target classifier alsolearns the relative weights between the guide fea-tures and the basic features, so that the knowledgefrom both the source corpus and the target corpusare automatically integrated together.In fact, more complicated features can beadopted as guide information.
For error tolerance,guide features can be extracted from n-best re-sults or compacted lattices of the source classifier;while for the best use of the source classifier?s out-put, guide features can also be the classificationresults of several successive characters.
We leavethem as future research.4 Related WorksCo-training (Sarkar, 2001) and classifier com-bination (Nivre and McDonald, 2008) are twotechnologies for training improved dependencyparsers.
The co-training technology lets two dif-ferent parsing models learn from each other dur-ing parsing an unlabelled corpus: one modelselects some unlabelled sentences it can confi-dently parse, and provide them to the other modelas additional training corpus in order to trainmore powerful parsers.
The classifier combina-tion lets graph-based and transition-based depen-dency parsers to utilize the features extracted fromeach other?s parsing results, to obtain combined,enhanced parsers.
The two technologies aim tolet two models learn from each other on the samecorpora with the same distribution and annota-tion standard, while our strategy aims to integratethe knowledge in multiple corpora with differentBaseline FeaturesC?2 ={C?1 =BC0 =oC1 =?C2 =?C?2C?1 ={BC?1C0 =BoC0C1 =o?C1C2 =?
?C?1C1 =B?Pu(C0) = 0T (C?2)T (C?1)T (C0)T (C1)T (C2) = 44444Guide Features?
= bC?2 ={ ?
?
= bC?1 =B ?
?
= bC0 =o ?
?
= bC1 =?
?
?
= bC2 =?
?
?
= bC?2C?1 ={B ?
?
= bC?1C0 =Bo ?
?
= bC0C1 =o?
?
?
= bC1C2 =??
?
?
= bC?1C1 =B?
?
?
= bPu(C0) = 0 ?
?
= bT (C?2)T (C?1)T (C0)T (C1)T (C2) = 44444 ?
?
= bTable 2: An example of basic features and guidefeatures of standard-adaptation for word segmen-tation.
Suppose we are considering the third char-acter ?o?
in ?
{B o ?
?u?.annotation-styles.Gao et al (2004) described a transformation-based converter to transfer a certain annotation-style word segmentation result to another style.They design some class-type transformation tem-plates and use the transformation-based error-driven learning method of Brill (1995) to learnwhat word delimiters should be modified.
How-ever, this converter need human designed transfor-mation templates, and is hard to be generalized toPOS tagging, not to mention other structure label-ing tasks.
Moreover, the processing procedure isdivided into two isolated steps, conversion aftersegmentation, which suffers from error propaga-tion and wastes the knowledge in the corpora.
Onthe contrary, our strategy is automatic, generaliz-able and effective.In addition, many efforts have been devotedto manual treebank adaptation, where they adaptPTB to other grammar formalisms, such as suchas CCG and LFG (Hockenmaier and Steedman,2008; Cahill and Mccarthy, 2007).
However, theyare heuristics-based and involve heavy human en-gineering.5265 ExperimentsOur adaptation experiments are conducted fromPeople?s Daily (PD) to Penn Chinese Treebank 5.0(CTB).
These two corpora are segmented follow-ing different segmentation standards and labeledwith different POS sets (see for example Figure 1).PD is much bigger in size, with about 100K sen-tences, while CTB is much smaller, with onlyabout 18K sentences.
Thus a classifier trained onCTB usually falls behind that trained on PD, butCTB is preferable because it also annotates treestructures, which is very useful for downstreamapplications like parsing and translation.
For ex-ample, currently, most Chinese constituency anddependency parsers are trained on some versionof CTB, using its segmentation and POS taggingas the de facto standards.
Therefore, we expect theknowledge adapted from PD will lead to more pre-cise CTB-style segmenter and POS tagger, whichwould in turn reduce the error propagation to pars-ing (and translation).Experiments adapting from PD to CTB are con-ducted for two tasks: word segmentation alone,and joint segmentation and POS tagging (JointS&T).
The performance measurement indicatorsfor word segmentation and Joint S&T are bal-anced F-measure, F = 2PR/(P +R), a functionof Precision P and Recall R. For word segmen-tation, P indicates the percentage of words in seg-mentation result that are segmented correctly, andR indicates the percentage of correctly segmentedwords in gold standard words.
For Joint S&T, Pand R mean nearly the same except that a wordis correctly segmented only if its POS is also cor-rectly labelled.5.1 Baseline Perceptron ClassifierWe first report experimental results of the singleperceptron classifier on CTB 5.0.
The originalcorpus is split according to former works: chap-ters 271 ?
300 for testing, chapters 301 ?
325 fordevelopment, and others for training.
Figure 4shows the learning curves for segmentation onlyand Joint S&T, we find all curves tend to moder-ate after 7 iterations.
The data splitting conven-tion of other two corpora, People?s Daily doesn?treserve the development sets, so in the followingexperiments, we simply choose the model after 7iterations when training on this corpus.The first 3 rows in each sub-table of Table 3show the performance of the single perceptron0.8800.8900.9000.9100.9200.9300.9400.9500.9600.9700.9801  2  3  4  5  6  7  8  9  10Fmeasurenumber of iterationssegmentation onlysegmentation in Joint S&TJoint S&TFigure 4: Averaged perceptron learning curves forsegmentation and Joint S&T.Train on Test on Seg F1% JST F1%Word SegmentationPD PD 97.45 ?PD CTB 91.71 ?CTB CTB 97.35 ?PD ?
CTB CTB 98.15 ?Joint S&TPD PD 97.57 94.54PD CTB 91.68 ?CTB CTB 97.58 93.06PD ?
CTB CTB 98.23 94.03Table 3: Experimental results for both baselinemodels and final systems with annotation adap-tation.
PD ?
CTB means annotation adaptationfrom PD to CTB.
For the upper sub-table, items ofJST F1 are undefined since only segmentation isperforms.
While in the sub-table below, JST F1is also undefined since the model trained on PDgives a POS set different from that of CTB.models.
Comparing row 1 and 3 in the sub-tablebelow with the corresponding rows in the uppersub-table, we validate that when word segmenta-tion and POS tagging are conducted jointly, theperformance for segmentation improves since thePOS tags provide additional information to wordsegmentation (Ng and Low, 2004).
We also seethat for both segmentation and Joint S&T, the per-formance sharply declines when a model trainedon PD is tested on CTB (row 2 in each sub-table).In each task, only about 92% F1 is achieved.
Thisobviously fall behind those of the models trainedon CTB itself (row 3 in each sub-table), about 97%F1, which are used as the baselines of the follow-ing annotation adaptation experiments.527POS #Word #BaseErr #AdaErr ErrDec%AD 305 30 19 36.67 ?AS 76 0 0BA 4 1 1CC 135 8 8CD 356 21 14 33.33 ?CS 6 0 0DEC 137 31 23 25.81 ?DEG 197 32 37 ?DEV 10 0 0DT 94 3 1 66.67 ?ETC 12 0 0FW 1 1 1JJ 127 41 44 ?LB 2 1 1LC 106 3 2 33.33 ?M 349 18 4 77.78 ?MSP 8 2 1 50.00 ?NN 1715 151 126 16.56 ?NR 713 59 50 15.25 ?NT 178 1 2 ?OD 84 0 0P 251 10 6 40.00 ?PN 81 1 1PU 997 0 1 ?SB 2 0 0SP 2 2 2VA 98 23 21 08.70 ?VC 61 0 0VE 25 1 0 100.00 ?VV 689 64 40 37.50 ?SUM 6821 213 169 20.66 ?Table 4: Error analysis for Joint S&T on the devel-oping set of CTB.
#BaseErr and #AdaErr denotethe count of words that can?t be recalled by thebaseline model and adapted model, respectively.ErrDec denotes the error reduction of Recall.5.2 Adaptation for Segmentation andTaggingTable 3 also lists the results of annotation adap-tation experiments.
For word segmentation, themodel after annotation adaptation (row 4 in uppersub-table) achieves an F-measure increment of 0.8points over the baseline model, corresponding toan error reduction of 30.2%; while for Joint S&T,the F-measure increment of the adapted model(row 4 in sub-table below) is 1 point, which cor-responds to an error reduction of 14%.
In addi-tion, the performance of the adapted model forJoint S&T obviously surpass that of (Jiang et al,2008), which achieves an F1 of 93.41% for JointS&T, although with more complicated models andfeatures.Due to the obvious improvement brought by an-notation adaptation to both word segmentation andJoint S&T, we can safely conclude that the knowl-edge can be effectively transferred from on an-Input Type Parsing F1%gold-standard segmentation 82.35baseline segmentation 80.28adapted segmentation 81.07Table 5: Chinese parsing results with differentword segmentation results as input.notation standard to another, although using sucha simple strategy.
To obtain further informationabout what kind of errors be alleviated by annota-tion adaptation, we conduct an initial error analy-sis for Joint S&T on the developing set of CTB.
Itis reasonable to investigate the error reduction ofRecall for each word cluster grouped together ac-cording to their POS tags.
From Table 4 we findthat out of 30 word clusters appeared in the devel-oping set of CTB, 13 clusters benefit from the an-notation adaptation strategy, while 4 clusters suf-fer from it.
However, the compositive error rate ofRecall for all word clusters is reduced by 20.66%,such a fact invalidates the effectivity of annotationadaptation.5.3 Contribution to Chinese ParsingWe adopt the Chinese parser of Xiong et al(2005), and train it on the training set of CTB 5.0as described before.
To sketch the error propaga-tion to parsing from word segmentation, we rede-fine the constituent span as a constituent subtreefrom a start character to a end character, ratherthan from a start word to a end word.
Note that ifwe input the gold-standard segmented test set intothe parser, the F-measure under the two definitionsare the same.Table 5 shows the parsing accuracies with dif-ferent word segmentation results as the parser?sinput.
The parsing F-measure corresponding tothe gold-standard segmentation, 82.35, representsthe ?oracle?
accuracy (i.e., upperbound) of pars-ing on top of automatic word segmention.
Afterintegrating the knowledge from PD, the enhancedword segmenter gains an F-measure increment of0.8 points, which indicates that 38% of the errorpropagation from word segmentation to parsing isreduced by our annotation adaptation strategy.6 Conclusion and Future WorksThis paper presents an automatic annotation adap-tation strategy, and conducts experiments on aclassic problem: word segmentation and Joint528S&T.
To adapt knowledge from a corpus with anannotation standard that we don?t require, a clas-sifier trained on this corpus is used to pre-processthe corpus with the desired annotated standard, onwhich a second classifier is trained with the firstclassifier?s predication results as additional guideinformation.
Experiments of annotation adapta-tion from PD to CTB 5.0 for word segmentationand POS tagging show that, this strategy can makeeffective use of the knowledge from the corpuswith different annotations.
It obtains considerableF-measure increment, about 0.8 point for wordsegmentation and 1 point for Joint S&T, with cor-responding error reductions of 30.2% and 14%.The final result outperforms the latest work on thesame corpus which uses more complicated tech-nologies, and achieves the state-of-the-art.
More-over, such improvement further brings striking F-measure increment for Chinese parsing, about 0.8points, corresponding to an error propagation re-duction of 38%.In the future, we will continue to research onannotation adaptation for other NLP tasks whichhave different annotation-style corpora.
Espe-cially, we will pay efforts to the annotation stan-dard adaptation between different treebanks, forexample, from HPSG LinGo Redwoods Treebankto PTB, or even from a dependency treebankto PTB, in order to obtain more powerful PTBannotation-style parsers.AcknowledgementThis project was supported by National NaturalScience Foundation of China, Contracts 60603095and 60736014, and 863 State Key Project No.2006AA010108.
We are especially grateful toFernando Pereira and the anonymous reviewersfor pointing us to relevant domain adaption refer-ences.
We also thank Yang Liu and Haitao Mi forhelpful discussions.ReferencesDaniel M. Bikel and David Chiang.
2000.
Two statis-tical parsing models applied to the chinese treebank.In Proceedings of the second workshop on Chineselanguage processing.John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In Proceedings of EMNLP.Eric Brill.
1995.
Transformation-based error-drivenlearning and natural language processing: a casestudy in part-of-speech tagging.
In ComputationalLinguistics.Sabine Buchholz and Erwin Marsi.
2006.
Conll-xshared task on multilingual dependency parsing.
InProceedings of CoNLL.Aoife Cahill and Mairead Mccarthy.
2007.
Auto-matic annotation of the penn treebank with lfg f-structure information.
In in Proceedings of theLREC Workshop on Linguistic Knowledge Acquisi-tion and Representation: Bootstrapping AnnotatedLanguage Data.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, pages 201?228.Michael Collins and Brian Roark.
2004.
Incrementalparsing with the perceptron algorithm.
In Proceed-ings of the 42th Annual Meeting of the Associationfor Computational Linguistics.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof the Empirical Methods in Natural Language Pro-cessing Conference, pages 1?8, Philadelphia, USA.Hal Daume?
III and Daniel Marcu.
2006.
Domain adap-tation for statistical classifiers.
In Journal of Artifi-cial Intelligence Research.Hal Daume?
III.
2007.
Frustratingly easy domain adap-tation.
In Proceedings of ACL.Jianfeng Gao, Andi Wu, Mu Li, Chang-Ning Huang,Hongqiao Li, Xinsong Xia, and Haowei Qin.
2004.Adaptive chinese word segmentation.
In Proceed-ings of ACL.Julia Hockenmaier and Mark Steedman.
2008.
Ccg-bank: a corpus of ccg derivations and dependencystructures extracted from the penn treebank.
InComputational Linguistics, volume 33(3), pages355?396.Wenbin Jiang, Liang Huang, Yajuan Lu?, and Qun Liu.2008.
A cascaded linear model for joint chineseword segmentation and part-of-speech tagging.
InProceedings of the 46th Annual Meeting of the As-sociation for Computational Linguistics.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of english: The penn treebank.
In Computa-tional Linguistics.Andre?
F. T. Martins, Dipanjan Das, Noah A. Smith, andEric P. Xing.
2008.
Stacking dependency parsers.In Proceedings of EMNLP.Ryan McDonald and Fernando Pereira.
2006.
Onlinelearning of approximate dependency parsing algo-rithms.
In Proceedings of EACL, pages 81?88.529Ryan McDonald, Koby Crammer, and FernandoPereira.
2005.
Online large-margin training of de-pendency parsers.
In Proceedings of ACL, pages 91?98.Hwee Tou Ng and Jin Kiat Low.
2004.
Chinese part-of-speech tagging: One-at-a-time or all-at-once?word-based or character-based?
In Proceedings ofthe Empirical Methods in Natural Language Pro-cessing Conference.Joakim Nivre and Ryan McDonald.
2008.
Integrat-ing graph-based and transition-based dependencyparsers.
In Proceedings of the 46th Annual Meetingof the Association for Computational Linguistics.Stephan Oepen, Kristina Toutanova, Stuart Shieber,Christopher Manning Dan Flickinger, and ThorstenBrants.
2002.
The lingo redwoods treebank: Moti-vation and preliminary applications.
In In Proceed-ings of the 19th International Conference on Com-putational Linguistics (COLING 2002).Anoop Sarkar.
2001.
Applying co-training methods tostatistical parsing.
In Proceedings of NAACL.Fei Xia.
2000.
The part-of-speech tagging guidelinesfor the penn chinese treebank (3.0).
In TechnicalReports.Deyi Xiong, Shuanglong Li, Qun Liu, and ShouxunLin.
2005.
Parsing the penn chinese treebank withsemantic knowledge.
In Proceedings of IJCNLP2005, pages 70?81.Nianwen Xue and Libin Shen.
2003.
Chinese wordsegmentation as lmr tagging.
In Proceedings ofSIGHAN Workshop.Nianwen Xue, Fei Xia, Fu-Dong Chiou, and MarthaPalmer.
2005.
The penn chinese treebank: Phrasestructure annotation of a large corpus.
In NaturalLanguage Engineering.Shiwen Yu, Jianming Lu, Xuefeng Zhu, HuimingDuan, Shiyong Kang, Honglin Sun, Hui Wang,Qiang Zhao, and Weidong Zhan.
2001.
Processingnorms of modern chinese corpus.
Technical report.Yue Zhang and Stephen Clark.
2007.
Chinese seg-mentation with a word-based perceptron algorithm.In Proceedings of the 45th Annual Meeting of theAssociation for Computational Linguistics.530
