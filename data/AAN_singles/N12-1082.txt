2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 641?645,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsOn The Feasibility of Open Domain Referring ExpressionGeneration Using Large Scale FolksonomiesFabia?n Pacheco Pablo Ariel Duboue?Facultad de Matema?tica, Astronom?
?a y F?
?sicaUniversidad Nacional de Co?rdobaCo?rdoba, ArgentinaMart?
?n Ariel Dom?
?nguezAbstractGenerating referring expressions has receivedconsiderable attention in Natural LanguageGeneration.
In recent years we start seeingdeployments of referring expression genera-tors moving away from limited domains withcustom-made ontologies.
In this work, we ex-plore the feasibility of using large scale noisyontologies (folksonomies) for open domainreferring expression generation, an importanttask for summarization by re-generation.
Ourexperiments on a fully annotated anaphoraresolution training set and a larger, volunteer-submitted news corpus show that existing al-gorithms are efficient enough to deal withlarge scale ontologies but need to be extendedto deal with undefined values and some mea-sure for information salience.1 IntroductionGiven an entity1 (the referent) and a set of com-peting entities (the set of distractors), the task ofreferring expression generation (REG) involves cre-ating a mention to the referent so that, in the eyesof the reader, it is clearly distinguishable from anyother entity in the set of distractors.
In a traditionalgeneration pipeline, referring expression generationhappens at the sentence planning level.
As a result,its output is not a textual nugget but a descriptionemployed later on by the surface realizer.
In this pa-per, we consider the output of the REG system to?To whom correspondence should be addressed.
Email:pablo.duboue@gmail.com.1Or set of entities, but not in this work.be Definite Descriptions (DD) consisting of a set ofpositive triples and a set of negative triples, enumer-ating referent-related properties.Since the seminal work by Dale and Re-iter (1995), REG has received a lot of attention in theNatural Language Generation (NLG) community.However, most of the early work on REG has beenon traditional NLG systems, using custom-tailoredontologies.
In recent years (Belz et al, 2010) therehas been a shift towards what we term ?Open Do-main Referring Expression Generation,?
(OD REG),that is, a REG task where the properties come froma folksonomy, a large-scale volunteer-built ontology.In particular, we are interested in changinganaphoric references for entities appearing in sen-tences drafted from different documents, as donein multi-document summarization (Advaith et al,2011).
For example, consider the following sum-mary excerpt2 as produced by Newsblaster (McKe-own et al, 2002):Thousands of cheering, flag-waving Palestinians gavePalestinian Authority President Mahmoud Abbas an en-thusiastic welcome in Ramallah on Sunday, as he toldthem triumphantly that a ?Palestinian spring?
had beenborn following his speech to the United Nations lastweek.3 The president pressed Israel, in unusually frankterms, to reach a final peace agreement with the Pales-tinians, citing the boundaries in place on the eve of theJune 1967 Arab-Israeli War as the starting point for ne-2From http://newsblaster.cs.columbia.edu/archives/2011-10-07-04-51-35/web/summaries/ 2011-10-07-04-51-35-011.html.3After his stint at UN, Abbas is politically stronger than ever(haaretz.com, 10/07/2011, 763 words).641gotiation about borders.4Here the second sentence refers to U.S. presi-dent Barack Obama and a referring expression of theform ?U.S.
president?
should have been used.
Suchexpressions depend on the set of distractors presentin the text, a requirement that highlights the dynamicnature of the problem.
Our experiments extractedthousands of complex cases (such as distinguishingone musician from a set of five) which we used totest existing algorithms against a folksonomy, dbPe-dia5 (Bizer et al, 2009).
This folksonomy contains1.7M triples (for its English version) and has beencurated from Wikipedia.6We performed two experiments: first we em-ployed sets of distractors derived from a set of docu-ments annotated with anaphora resolution informa-tion (Hasler et al, 2006).
We found that roughlyhalf of the entities annotated in the documents werepresent in the folksonomy, which speaks of the feasi-bility of using a folksonomy for OD REG, given thefact that Wikipedia has strict notability requirementsfor adding information.
In the second experiment,we obtained sets of distractors from Wikinews,7 aservice where volunteers submit news articles inter-spersed with Wikipedia links.
We leveraged saidlinks to assemble 40k referring expression tasks.For algorithms, we employed Dale and Re-iter (1995), Gardent (2002) and Full Brevity (FB)(Bohnet, 2007).
Our results show that the first twoalgorithms produce results in a majority of the re-ferring expression tasks, with the Dale and Reiteralgorithm being the most efficient and resilient ofthe three.
The results, however, are of mixed qualityand more research is needed to overcome two prob-lems we have identified in our experiments: dealingwith undefined information in the folksonomy andthe need to incorporate a rough user model in theform of information salience.In the next section we briefly summarize the threealgorithms we employed in our experiments.
In Sec-tion 3, we describe the data employed.
Section 4contains the results of our experiments and subse-quent analysis.
We conclude discussing future work.4Obama prods Mideast allies to embrace reform, makepeace (Washington Post, 10/07/2011, 371 words).5http://dbpedia.org6http://wikipedia.org7http://wikinews.org2 Referring Expression Generation (REG)REG literature is vast and spans decades of work.We picked three algorithms with the followingdesiderata: all the algorithms can deal with singleentity referents (a significant amount of recent workwent into multi-entity referents) and we wanted toshowcase a classic algorithm (Dale and Reiter?s), analgorithm generating negations (Gardent?s) and analgorithm with a more exhaustive search of the solu-tions space (Full Brevity).
We very briefly describeeach of the algorithms in turn, where R is the refer-ent, C is the set of distractors and P is a list of prop-erties, triples in the form (entity, property, value),describing R:Dale and Reiter (1995).
They assume the prop-erties in P are ordered according to an establishedcriteria.
Then the algorithm iterates over P , addingeach triple one at a time and removing from C allentities ruled out by the new triple.
Triples that donot eliminate any new entities from C are ignored.The algorithm terminates when C is empty.Gardent (2002).
The algorithm uses ConstraintSatisfaction Programming to solve two basic con-straints: find a set of positive properties P+ and neg-ative properties P?, such that all properties in P+are true for the referent and all in P?
are false, andit is the smaller P+ ?
P?
such that for every c ?
Cthere exist a property in P+ that does not hold for cor a property in P?
that holds for c.8Full Brevity (Bohnet, 2007).
Starting from astate E of the form (L,C, P ) with L = ?
(selectedproperties), it keeps these states into a queue, whereit loops until C = ?.
In each loop it generates newstates (added to the end of the queue), as follows:given a state E = (L,C, P ) for each p ?
P , if p re-moves elements rem from C, it adds (L?
{p}, C ?rem, P ?
{p}), otherwise (L,C, P ?
{p}).3 DatadbPedia.
dbPedia (Bizer et al, 2009) isan ontology curated from Wikipedia infoboxes,small tables containing structured information atthe top of most Wikipedia pages.
The ver-sion employed in this paper (?Ontology InfoboxProperties?)
contains 1,7520,158 triples.
Each8We employed the Choco CSP solver Java library:http://www.emn.fr/z-info/choco-solver/.642Former [[New Mexico]] {{w|Governor of NewMexico|governor}} {{w|Gary Johnson}} endedhis campaign for the {{w|Republican Party(United States)|Republican Party}} (GOP)presidential nomination to seek the backingof the {{w|Libertarian Party (UnitedStates)|Libertarian Party}} (LP).Figure 1: Wikinews example, from http://en.wikinews.org/wiki/U.S.
presidential candidate Gary Johnson leaves GOP to vie forthe LP nomentity is represented by a URI starting withhttp://dbpedia.org/resource/ followed bythe name of its associated Wikipedia title.
See thenext section for some example triples.Pilot.
While creating unambiguous descriptionsis the NLG task known as referring expression gen-eration, its NLU counterpart is anaphora resolu-tion.
We took a hand-annotated corpus for traininganaphora resolution algorithms (Hasler et al, 2006)consisting of 74 documents containing 239 corefer-ence chains.
Each of the chains is an entity that canbe used for our experiments, if the entity is in db-Pedia and there are other suitable distractors in thesame document.
We hand annotated each of those239 coreference chains by type (person, organiza-tion and location) and associated them to dbPediaURIs for the ones we found on Wikipedia.
We foundroughly half of the chains in dbPedia (106 out of239, 44%).
This percentage speaks of the coverageof dbPedia for OD REG.
However, only 16 docu-ments contain multiple entities of the same type andpresent in dbPedia, our pilot study criteria.
These 16documents result in the 16 tasks for our pilot.
For alarge scale evaluation we turned to Wikinews.Wikinews.
Wikinews is a news service operatedas a wiki.
As the news articles are interspersedwith interwiki links, multiple entities can be disam-biguated as Wikipedia pages (which in turn are db-Pedia URIs).
For example, in Figure 1, both the Lib-ertarian Party and Republican Party can be consid-ered potential distractors, as both are organizations.The Wikimedia Foundation makes a databasedump available for all Wikinews interwiki links (thelinks in braces in the above example).
If a page con-tains more than one organization or person, we ex-tracted the whole set of people (or organizations) asa referring expression task.
To see whether a URIis a person or an organization we check for a birthdate or creation date, respectively.
In this manner,we obtained 4,230 tasks for people and 12,998 fororganizations.
This is dataset is freely available.94 ResultsPilot.
The 16 tasks were split into 40 runs (a taskspans n runs each, where n is the number of entitiesin the task, by rotating through the different alterna-tive pairs of referent / set of distractors).
From thesetasks, Dale and Reiter produced no output 12 timesand FB Brevity was unable to produce a result in 23times.
Gardent produced output for every run.
Weconsider this an example of the increased expressivepower of negative descriptions (it included a nega-tion in 25% of the runs).
For the other two algo-rithms, the lack of an unique triple differentiatingone entity from the set of distractors seemed to bethe main issue but there were multiple cases were FBran out of memory for its queue of candidate nodes.With respect to execution timings, Dale and Re-iter ran into some corner cases and took time com-parable to Gardent?s algorithm.
FB was 16 timesslower (we found this counter-intuitive, as Gardent?salgorithm is more demanding).
Therefore, two ofthese algorithms were able to produce results usinglarge scale ontological information.
As FB ran intoproblems both in terms of execution time and failurerates, we omitted it from the large scale experiments.We adjusted the parameters for the algorithms onthis set to obtain the best possible quality outputgiven the data and the problem.
As such, we do notreport quality assessments on the pilot data.Wikinews.
The tasks obtained from wikinewscontained a large number of entities per task (an av-erage of 12 people per task) and therefore span alarge number of runs: 17,814 runs for people (from4,230 tasks) and 44,080 for organizations (from12,998 tasks).On these large runs, execution time differencesare in line with our a priori expectations: the greedyapproach of Dale and Reiter is very fast10 with Gar-dent?s more comprehensive search taking about 40times more time.
Dale and Reiter failure rate was9http://www.cs.famaf.unc.edu.ar/?pduboue/data/ also mirroredat http://duboue.ca/data.10Dale and Reiter takes less than 3?
for the 44,080 runs fororganizations in a 2.3 GHz machine.643Referent Dale and Reiter Output Gardent OutputEB { (EB occupation Software Freedom Law Center) } { (EB occupation Software Freedom Law Center) }LL { (LL birthPlace United States), (LL, occupation Harvard Law School) } { (LL birthPlace Rapid City, South Dakota) }LT { (LT occupation Software engineer) } { (LT nationality Finnish American) }Figure 2: Example output for the task: {?Eben Moglen?
(EB), ?Lawrence Lessig?
(LL), ?Linus Torvalds?
(LT) }.comparable or better than in the pilot (for organiza-tions that are more mixed, it was slightly lower butfor people it was as low 2.8%).
Gardent missed 2%of the people (and only 54 organizations), employ-ing negatives 14% of the time for people and 12% ofthe time for organizations.Evaluating referring expressions is hard.
Effortsto automate this task in NLG (Gatt et al, 2007)have taken an approach similar to machine transla-tion BLEU scores (Papinini et al, 2001), for exam-ple, by asking multiple judges to produce referringexpressions for a given scenario.
These settings usu-ally involve images of physical objects and relate tosmall ontologies.
While such an approach could beadapted to the Open Domain case, a major problemis the need for the judges to be acquainted with someof the less popular entities in the training set.
Atthis point in our research, we decided to analyze thequality of a sample of the output ourselves.
Thisprocess involved consulting information about eachentity to determine the soundness of the result.We looked at a random sample of 20 runs and an-notated it by two authors, measuring a Cohen?s ?
of60% for annotating DD results and 79% for deter-mining whether the folksonomy had enough infor-mation to build a satisfactory DD.
We then extendedthe evaluation to 60 runs and annotated them by oneauthor.
We found that Dale and Reiter produced asatisfactory DD in 41.6% of the cases and Gardentin 43.4% of the cases and that the folksonomy con-tained enough information 81.6% of the time.
Fig-ure 2 shows some example output.From the evaluation we learned that the defaultordering strategy employed by Dale and Reiter isnot stable across different types of people (compare:politicians vs. musicians) or organizations.
We alsosaw that Gardent?s algorithm in many cases selecteda single triple with very little practical value (an ob-scure fact about the entity) or a negative piece of in-formation which is actually true for the referent butit is a missing piece of information.The first two problems can be solved by either fur-ther subdividing the taxonomies of entities or (moreinterestingly) by incorporating some measure aboutthe salience of each piece of information, a possibil-ity which we will discuss next.
The last issue can beaddressed by having some form of meaningful de-fault value.The negations produced by Gardent?s algorithmhighlighted errors on the folksonomy.
For example,when referring to China with distractors Peru andTaiwan, it will produce ?the place where they do notspeak Chinese,?
as China has the different Chinesedialects spelled out on the folksonomy (and somePeruvians do speak Chinese).
Given these limita-tions, we find the current results very encouragingand we believe folksonomies can help focus on ro-bust NLG for noisy (ontological) inputs.5 DiscussionWe have shown that by using a folksonomy it shouldbe possible to deploy traditional NLG referring ex-pression generation algorithms in Open Domaintasks.
To fulfill this vision, three tasks remain:Dealing with missing information.
Some form ofsmart default values are needed, we are consideringusing a nearest-neighbor approach to find ontologi-cal siblings which can provide such defaults.Estimating salience of each piece of ontologicalinformation.
The importance for each triple has tobe obtained in a way consistent with the Open Do-main nature of the task.
For this problem, we believesearch engine salience can be of great help.Transform the extracted triples into actual text.This problem has received attention in the past.
Wewould like to explore traditional surface realizerwith a custom-made grammar.AcknowledgmentsWe would like to thank the anonymous reviewers aswell as Annie Ying and Victoria Reggiardo.644ReferencesSiddharthan Advaith, Nenkova Ani, and McKeown Kath-leen.
2011.
Information status distinctions and refer-ring expressions: An empirical study of references topeople in news summaries.
Computational Linguis-tics, 37(4):811?842.Anja Belz, Eric Kow, Jette Viethen, and Albert Gatt.2010.
Generating referring expressions in context:The grec task evaluation challenges.
In Emiel Krah-mer and Marit Theune, editors, Empirical Methodsin Natural Language Generation, volume 5790 ofLecture Notes in Computer Science, pages 294?327.Springer.C.
Bizer, J. Lehmann, G. Kobilarov, S. Auer, C. Becker,R.
Cyganiak, and S. Hellmann.
2009.
DBpedia-acrystallization point for the web of data.
Web Seman-tics: Science, Services and Agents on the World WideWeb, 7(3):154?165.B.
Bohnet.
2007. is-fbn, is-fbs, is-iac: The adaptationof two classic algorithms for the generation of refer-ring expressions in order to produce expressions likehumans do.
MT Summit XI, UCNLG+ MT, pages 84?86.R.
Dale and E. Reiter.
1995.
Computational interpreta-tions of the gricean maxims in the generation of refer-ring expressions.
Cognitive Science, 19(2):233?263.C.
Gardent.
2002.
Generating minimal definite descrip-tions.
In Proceedings of the 40th Annual Meeting onAssociation for Computational Linguistics, pages 96?103.
Association for Computational Linguistics.A.
Gatt, I.
Van Der Sluis, and K. Van Deemter.
2007.Evaluating algorithms for the generation of referringexpressions using a balanced corpus.
In Proceedingsof the Eleventh European Workshop on Natural Lan-guage Generation, pages 49?56.
Association for Com-putational Linguistics.L.
Hasler, C. Orasan, and K. Naumann.
2006.
NPsfor events: Experiments in coreference annotation.
InProceedings of the 5th edition of the InternationalConference on Language Resources and Evaluation(LREC2006), pages 1167?1172.Kathleen R. McKeown, R. Barzilay, D. Evans, V. Hatzi-vassiloglou, J. L. Klavans, A. Nenkova, C. Sable,B.
Schiffman, and S. Sigelman.
2002.
Tracking andsummarizing news on a daily basis with columbia?snewsblaster.
In Proc.
of HLT.Kishore Papinini, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
Bleu: a method for automatic evalua-tion of machine translation.
Technical report, IBM.645
