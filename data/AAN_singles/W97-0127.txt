Probabilistic Word Classification Based on Context-Sensitive Binary Tree MethodJun Gao, XiXian ChenInformation Technology LaboratoryBeijing University of Posts and TelecommuncationsP.O.
Box 103, Beijing University of Posts and TelecommuncationsNo.
10, Xi Tu Cheng street, Hal Dian districtBeijing, 100088, Chinae-maih b9507311@buptedu.cnI :IIIIIAbstractCorpus-based statistical-oriented Chinese word classif ication can beregarded as a fundamental step for automatic or non-automatic, mono-lingual natural processing system.
Word classif ication can solve theproblems of data sparseness and have far fewer parameters.
So far,much r:,la~v~ work about word classification has been done.
All thework is based on some similarity metrics.
We use average mutualinformation as global similarity metric to do classification.
Theclustering process is top-down splitting and the binary tree isgrowin~ with splitting.
In natural lan~lage, the effect of leftneighbors and right neighbors of a word are asymmetric.
To utilizethis directional information, we induce the left-right binary andright-left binary tree to represent this property.
The probabil ity isalso introduced in our algorithm to merge the resulting classes fromleft-right and right-left binary tree.
Also, we use the resultingclasses to do experiments on word class-based language model.
Someclasses results and perplexity of word class-based language model arepresented.1.
IntroductionWord c'lassification play an important role in computational_i:~gu~s~.~.
Many casks in computational linguistics, whether theyuse statistical or symbolic methods, reduce the complexity of theprobl~m by dealing with classes of words rather than individual words.we know that some words share similar sorts of l inguistic properties,thus they should belong to the same class.
Some words have severalfunctions, thus they could belong to more than one class.
Thequestions are: What attributes distinguish one word from another?
Howshould we group similar words together so that the partit ion of word297spaces is most l ikely,  to reflect the linguistic properties oflanguaqe?
What meaningful label or name should be given to each wordgroup?
These questions constitute the problem of finding a wordclassification.
At present, no method can find the optimal wordclassification.
However, researchers have been trying hard to findsub-optimal strategies which lead to useful classification.From practical point of view, word classif ication addresses questionsof data sparseness and generalization in statistical language models.Specially, it can be used as an alternative to grammatical part-of-speech tagging (Brili,1993; Cutting, Kupiec, Pederson and Sibun, 1992;Chang and  Chen 1993a; Chang and Chen 1993b; Lee and Chang Chien,1992; Kupiec,1992; Lee, 1993; Merialdo,1994; Pop,1996; Peng, 1993;Zhou, 1995; Schutze, 1995;) on statistical language modeling(Huang,Alleva, Hwang, Lee and Rosenfeld 1993; Rosefield, 1994;), becauseChinese language models u@ing part-of-speech information have had onlya very l imited success(e.g.
Chang, 1992; Lee, Dung, Lai, and ChangChien, 1993;).
The reason why there are so many of the diff iculties inChineuc part of-speech tagging are described by Chang and Chen (1995)and Zhao (1995).Much relative work on word classif ication has been done.
The work isbased on some similarity metrics.
( Bahl, Brown, DeSouza and Mercer,1989; Brown, Pietra, deSouza and Mercer,1992; Chang, 1995;DeRose,1988; Garside, 1987; Hughes, 1994; Jardino,1993; Jelinek,Mercer, and Roukos, 1990b; Wu, Wang, Yu and Wang, 1995; Magerman,1994; McMahon, 1994; McMahon, 1995; Pereira, 1992; Resnik, 1992; Zhao,1995;)Brill (1993) and Pop (1996) present a transformation-based tagging.Before a part-of-speech tagger can be built, the word classificationsare performed to help us choose a set of part-of-speech.
They use thesum of two relative entropies obtained from neighboring words as thesimilarity metric to compare two words.Schutze (1995) shows a long-distance left and right context of a wordas left vector and right vector and the dimensions of each vector are50.
MR ~ses Cosine as metric to measure the similarity between twowords.
To solve the sparseness of the data, he applies a singularvalue decomposition.
Comparing with Bril l ,E.'s method, Schutze,H.takes 50 neighbors into account for each word.Chang and Chen (1995) proposed a simulated annealing method, thesame as Jardino and Adda 's (1993).
The pel-plexity, which is theinverse of the probabi l i ty over the whole text, is measured.
The newvalue of the perplexity and a control parameter Cp(Metropolisalgorithm) will decide whether 'a  new classif ication ( obtained bymoving only one word from its class to another, both word and classbeing randomly chosen) will replace a previous one.
Compared to thetwo methods described above, this method attempts to optimize theclustering using perplexity as a global measure.Pereira, Tishby and Lee (1993) investigate how to factor wordassociation tendencies into associations of words to certain hiddensenses classes and associations between the classes themselves.
More298specifically, they model senses as probabil ist ic concepts or clustersC with correspondin~ cluster membership probabil it ies P(Clw ) for eachword w. That is, while most other class-based modeling techniques fornatural language rely on "hard" Boolean classes, Pereira, F. etal.
(1993) propose a method for "soft" class clustering.
He suggests adeterministic annealing procedure for clustering.
But as stated intheir paper, they only considers the special case of classifying nounsaccording to the distribution as direct objects of verbs.To addless the problems and util ize the advantages of the methodspresented above, we put forward a new algorithm to automaticallyclassify the words.2.
Chinese Word Classification Method2.1 Basic IdeaWe adopt the top-down binary splitting technique to the all wordsusing average mutual information as similarity metric like McMahon(1995).
This method has its merits: Top-down technique can representthe hierarchy information explicitly; the posit ion of the word inclass-space can be obtained without reference to the positions ofother words, while the bottom-up technique treats every word in thevocabulary as one class and merges two classes among this vocabularyaccording to certain similarity metric, then repeats the mergingprocess until the demanded number of classes is obtained.2.1.1 Theoretical BasisBrown r.~.
a\] .
(1992) have shown that any classif ication system whoseaverage class mutual information is maximized will lead to class-basedlanguage models of lower perplexities.The concept of mutual information, taken from information theory, wasproposed as a measure of word association (Church 1990; "Jelinek et al1990,1992; Dagan, 1995;).
It reflects the strength of relationshipbetween words by comparing their actual co-occurrence probabil ity withthe probabi I ity that would be expected by chance.
The mutualinformation of two events x and y is defined as follows:P(x,,x,_ ) (1)I(x, ,x. , )  = log., P(x, )P(x,.
)where P(x,)and P(x,)are the probabilit ies of the events, and P(x,,x2)isthe probabil ity of the joint event.
If there is a strong associationbetween x,and x, then P(x,,x:)>>P(x,)P(x:) as a result I(x,,x2)>>O.
Ifthere is a weak association between x, and x~ then P(x,,x,)=P(x,)P(x2)and/(x,,x2)=0.
If P(x,,x,.
)<<P(x,)P(x:) then I(x,,x2)<<O.
Owing to theunrel iabi l ity of measuring negative mutual information values betweencontent words in corpora that are not extremely large, we haveconsidered that any negative value to be 0.
We also set \](xl,x2) to 0 if= 0 .The average mutual information I~ between events x,,x2, .... x N is definedsimilarly.299", "!'
' P (x , ,x ,  )Z = Z )?
log., (2),o, , .
,  P(x,)P(x,)Rather than estimate the relationship between words, we measure themutual information between classes.
Let C,, C\] be the classes,i.j= 0,1.2 ..... N ; N denotes the number of classes.Then average mutual information between classes 01, C 2,..., C N isp(c,,c,)I~ = ~ ~ PC C,, C, ) x log:: (3),=, ,=, P (c , )P (c , )2.1.2 The Basic Aigo~thmThe complening process is described as follows:We split the vocabulary into a binary tree.
We only consider onedimension neighbor.#1:Take the whole words' in vocabulary as one class and take thislevel " in the binary tree as 0.
That is Level=0, Branch=0,Class(Level,Branch)=Vocabulary Set.
Then, Level=Level+l.#2:Class(Level,Branch)=Class(Level- l ,Branch/2).
Old ~=0.Class(Level,Branch+l)=empty.
Select a word w, E Class(Level,Branch)#3:Move this word to Class(Level,Branch+l).Calculate the ~(w,)#4:Move this word back to Class(Level,Branch).If (all words in Class(Level,Branch) have been selected), thengoto #5else select another unselected word w,E Class(Level,Branch) toClass(Level,Branch+l), goto #3#5:Move the word having Maximum(~) from Class(Level,Branch) toClass(Level,Branch+l).-#6:If (Maximum(~)> Old ~) thenOld ~= Maximum(~) ,Se lect  a word WE Class (Level,Branch), goto #3#7:Branch=Branch+2.If (Branch<2 ~?'?'
) goto #2#8:Level=Level+l, Branch=0;If (Level< pre-def ined classes number) goto #2;else goto end.From the algorithm descrbed above, we can conclude that thecomputation time is to be order O(hV 3) for tree height h andvocabulary size V to move a word from one class to another.If the height of the binary tree is h, the number of all possibleclasses will be 2 h. During the splitting process, especial ly at thebottom of the binary tree, some classes may be empty because theclasses higher than them can not be splitted further more.2.2 Improvement to the Basic Algorithm2.2.1 l,ength of Neighbor Dimensions300As mentioned in Introduction, Brill (1993), and McMahon ~1995) onlyconsider one dimension neighbor, while Schutze (1995) consider 50dimensions "neighbors.
How long the dimensions neighbors should beindeed?
For long-distance bigrams mentioned in Huang, et al (1993)and Rosefield (1994), training-set perplexity is low for theconventional bigram(d=l), and it increases signif icantly as they movethough W = 2,3,4 and 5.
For" d = 6,...\]0, training-set perplexity remained atabout the same level.
Thus, Huang,X.-D.et al (!993) conclude that someinformation indeed exists in the more distant past but it is spreadthinly across the entire history.
We do the test on Chinese in thesame way.
And similar results are obtained.
So, 50 is too long fordimensions and the search in searching space is computationallyprohibitive, and I is so small for dimensions that much informationwill be lost.In this paper, we let d = 2.so, P(~), P(C,)and P(~,C/) can be calculated as follows:P(C)  - - -  (4)s,'~C:P(c ,  ) - - -  (5") mtc,I(6) P(C,, C, ) = N,o,ol dwhere N,o,, I is the total times of words which are in the vocabularyoccurring in the corpus.d is the calculating distance considered in the corpus.N,,.
is the total times of word w occurring in the corpus\]V,~.,w: is the total times of words couple wtw 2 occurring in the corpuswithin the distance d.2.2.2 Context-Sensitive ProcessingIn the works of Brill (1993), Brill,E.
et al use the sum of tworelative entropies as the similarity metric to  compare two words.
Theytreat the word's neighbors equally without considering the possibledifferent influences of l.eft neighbor and right neighbor to the word.But in natural language, the effect from left neighbor and rightneighbor is asymmetric, that is, the effect is directional.
Forexample, In "~,~.~"(  "I ate an apple"), the Chinese word ~"(~I" )and "%~9~"(~apple ") has different functions in this sentence.
We cannot say that "~.
.
.~ .~"("An  apple ate I").
So, it is necessary toinduce a similarity metric which reflects this directional property.Applying this idea in our algorithm, we create two binary trees torepresent different directions.
One binary tree is produced torepresent the word relation direction from the left to the right, and301the other is to represent the word relation direction from the rightto the left.
The former is from the left to the right is the defaultcircumstance mentioned in 2.1.2.The similar idea about directional property is presented by Dagan, etai.
(1995) also.
Dagan, et al (1995) defines a similarity metric oftwo words that can reflect the directional property according tomutual information to determine the degree of simil~rity between twowords.
But the metric does not have transitivity.
The intransitivityof the metric detex-mines this metric can not be used in clusteringwords to equivalence classes.To reflect the different influence of left neighbor and rightneighbor of the word, we introduce the probability for each word w toevery class.
That is, for the classes produced by the binary treewhich represent the word relation direction from the left to theright, we distribute probability ~r(~Iw)for each word w correspondingevery class ~,  the probability ~(~lw)  reflect the degree the word wbelongs to class ~.
For the classes the binary tree which representthe word relation direction from the right to the left produce,Pw(~lw) is calculated likewise.Mutual information can be explained as: the ability of dispelling theuncertainty of information source.
And entropy of information isdefined as the uncertainty of information source.
So, the probabilityword w which belongs to class ~ can be presented as follows:where I(~,~) is the mutual information between the class ~ and the iother class ~ which is in the same binary branch with ~.
S (~)  is the |entropy of class ~.So, 1- ~ denotes the probability that word w didn't belong to class IThat is, in the binary tree, \]-Pc, denotes the probability of theother branch class corresponding to ~.
Because the average mutualinformation is little, it is possible that Pc, is less than l-Pc.
To Iavoid distributing the less probability to the word assigned to thisclass than the probability to the word not assigned to this class, we ?distribute the probability 1 to the word assigned to the class.
|Thus, for each class in the certain level of the binary tree, wemultiple the probabilities either 1 or |- Pc, to its original lprobabilities, in which ~ is the other branch class opposite to the Jclass the word not belonging to.The description on above is only word w belong to a certain class in Icertain level without consider the affection from its upper levels.
Toobtain the real probabiliSy of word w belonging to certain class, allbelonging probabilities of its ancestors should be multiplied Itogether.The distribution of the probability is not optimal, but it reflectsthe degree a word belonging to a class.
It should be noted that ?302~-'~P(~iW) must be normalized both for the left-right and the right-leftresults.
And the normalized results of the left-right and the right-left binary tree also must be normalized together.2.2.3 Probabilisfic BoSom-up C~ssesMergingSince there is directional property between words, the transitivitywill not be satisf ied between different directions.
That is, if wedidn't introduce the probabil i ty ~r(~lw) and P~(~lw), we would notmerge the classes because there is no transit ivity between the classin which word relation is from the left to the right and the classin which word relation is from the right to the left.
For example,"~\ ] " ( "we")  and ~ i~."
( "you  ") are contained in one class derived bythe left-right binary tree, and other two words ~\ ] " (~you")  and"~_~"("apple")  belong to another class derived from right-left binarytree.
This do not mean that the words "~\ ] " ( "we")  and ~"("app le" )belong to one class.But when we put forward the probability, unlike the intransit ivity ofsimilarity metric presented by Dagan, et al (!995), the classesgenerated by two binary trees can be merged because the probabilit iescan make the "hard" intransit ivity "soft".Although this top-down splitting method has the advantage wementioned above, it has its obvious shortcomings.
Magerman, (1994)describes these shortcomings in detail.
Since the spl itt ing procedureis restricted to be trees, as opposed to arbitrary directed graphs,there is no mechanism for merging two or more nodes in the treegrowing process.
That is to say, if we distribute the words to thewrong classes from global sense, we will not be able to any longermove it back.
So, it is difficult to merge the classes obtained byleft-right binary tree and right-left binary tree during the processof growing tree.
To solve this problem, we adopt the bottom-up mergingmethod to the resulting classes.A number of different similarity measures can be used.
We choose touse relative entropy , also known as the Kullback-Leiblerdistance(Pereira, et ai.1993; Brili,1993;).
Rather than merge twowords, we merge the two classes which belong to the resulting classesgenerated by left-right" binary tree and right-left binary treerespectively, and select the merged class which can lead to maximumvalue of similarity metric.
This procedure can be done recursivelyuntil the demanded number of classes is reached.Let P and Q be the probabil i ty distribution.
The Kullback-Leiblerdistance from P to Q is defined as:D(PII Q) = ~ Pf w) log Pfw) (8), , ,  Q(w)The divergence of P and Q is then defined as:miv( P, Q) = D~(Q, P) = D( PIIQ) + D(QII P) (9)For two words w and w I , let Pa(w, wl) be the probabi l i ty of word woccurring to the left of w I within the distance d. The probability,303Pa(w1,w), Pd(w, w2) and Pa(W2,w), are defined likewise.
And let Ptr(Cilwl),Plr(Cilw2), Prt(O/\[w,)amLd Prl(Ojlw2) be the probabil it ies of words w I and w 2contained in classes C, and C~ in the left-right and right-left treesrespectively.
Then, the Kullback-Leibler distance between words w Iand w_, in the left-right tree is:t',,( w. .,1)e,.
(C, l w. )D,,(w, II w.,) = ,,v~F-'Pa(w'w')t"~(C'lw))l?g P,,(w,w,)P~,.
(C~lw,_)The divergence of words w I and w 2 in  the left-right tree is:Div,,.
(w,, w 2 ) = D,,.
(w, II w2 ) + m,,(w.,ll w, )Similarly, the Kullback-Leibler distance between words w I and w 2 inthe right-left tree is:D,~(w, llw,_)= ~"~P,,.
(w,w,)P,~(C, lw,)logwGl"where V is the vocabulary.P (w. w.)e. (C, lw.)P,,(w.w._)e,,(C,l..,..
)We can then define the similarity of wl and w 2 as:\]S(  ~1", .
~,': ) = \] - ~ { D~'~ ( w , .
~'~ ) + D iv  ~ ( . '
t  .
~'~ ) } ( 1 O) -S(w,,w:) ranges  f rom 0 to  1, w i th  S(w,w)=l.The computation cost of this simiarity is not high, for thecomponents of equation ( I0 )  have been obtained during the earlycomputation.The number of all possible classes is 2 h. During the spl itt ingprocess, especial ly at the bottom of the binary tree, it may be emptyfor some classes because the classes at higher level than it can notbe splitted further more according to the rule of maximum averagemutual information.
The .number of the result ing classes can not becontrolled accurately.
So, we can define the number of the demandingclasses in advance.
As long as the number of the result ing classes isless than the pre-def ined number, the spl itt ing process will becontinued.
When the number of the resulting classes is larger than thepre-defined number, we use the merging technique presented above toreduce the number until it is equal to the pre-def ined number.
Theprocedure can be described as follows: After we have merged twoclasses taken from the left-right and the right-left treesrespectively, we use this merged class to replace two original classesrespectively.
Then we repeat this process until certain step isreached.
In this paper, we define the number of steps as equal to thelarger number of the classes between two trees' resulting classes.Finally, we merge all resulting classes until the pre-def ined numberis reached.This merging process guaranteed the probabi l i l ty to be nonzerowhenever the word distributions are.
This is a useful advantagecompared with agglomerative clustering techniques that need to compareindividual objects being considered for grouping.3043.
Experimental Resul~ and Discussion3.1Word Classification ResultsWe use Pentium 586/133MHz, 32M memory to calculate.
The OS is WindowsNT 4.0.
And Visual C++ 4.0 is our programming language.We use the electric news corpus named "Chinese One hundred kinds ofnewspapers---1994".
The total size of it is 780 mil l ion bytes.
It isnot feasible to do classif ication experiments on this original corpus.So, we extract a part of it which contain the news published in Aprilfrom the original news texts.To be convenient, the sentence boundary markers, {l, ?
.... ; : ,} are replaced by only two sentence boundarymarkers: "! "
and " " which denote the beginning and end of thesentence or word phrase respectively.The texts are segmented by Variable-distance algorithm\[ Gao, J. andChen, X.X.
(1996)\]We select four subcorpora which contains 10323, 17451, 25130 and44326 Chinese words.
The vocabulary contains 2103, 3577, 4606 and6472 words correspondingly.
The results of the classif ication withoutintroducing probabil it ies can be summarized in Table I.The computation of merging process is only equal to the splittingcalculation in one level in the tree.
From table I, we can findsurprisely that the computation time for right-left is much shorterthan the time for left-right.
But this is reasonable.
In the processof left-right, the left branch contains more words than the rightbranch.
To move each word from the left branch to the right branch, weneed to match this word throughout the corpus.
But when we do theprocess of right-left, the left branch has less words than the right.We only need to match the small number of words in the corpus.
Fromthis, we can know that the preprocessed procedure costs much time.The number of empty classes is increasing with the tree grows.
TableII shows the number of empty classes in different levels in the left-right tree when we process the subcorpora containing 10323 words.Although our method is to calculate distributional classification, itstill demonstrates that it has powerful part-of-speech functions.Table I. Summarization of classifying four subcorporaWords in~he CorpusLeft-rigktresultingclassesRight-leftResultingclassesPre-definednumber ofclassesTime forleft-rightTime forright-left10,323 161 178 150 22 hours 19 hours17,451 347 323 300 2.4 days I.I days25,330 642 583 500 5.1 days 2.7 days1,225 600 44,026 9 days 1,118 4 days305' " ILevel1!emptyclass 0Table II.
The number of empty classesLevel Level Level Level Level Level2 3 4 5 60 1 3 7 21 63Level8123Level9351Some typical word classes which is the part of results of subcorpuscontaining 17451 words are listed below.
(Resulting classes of left-right binary tree) .class !3: ~ ~ ~ ~ ~,~- ~ ~ ~ ~ ~ ~class ~:  ,~ ~ ~ ~ ~T f@-~ ~ ~ ~ ~ ~-~Class 96: _-'-~.. Jq ~T" ~ H.~ ~ ?~ ~,~ \[~'~ :~.~ ~ ~\[~jBut some of classes present no obvious part-of-speech category.
Mostof them conZain only' very small number of words.
This may caused bythe predefined classif ication number.
Thus, excessive or insufficientclassif ication may be encountered.
And another shortcoming is that asmall number of words in almost every resulting class doesn't belongto the part-of-speech categories which most of words in that classbelong to.3.2 Use Word Classification Resul~ in Statistical Language ModefingWord class-based language model is more competitive than word-basedlanguage model.
It has far fewer parameters, thus making better use oftraining data to solve the problem of data sparseness.
We compare wordclass-based N-gram language model with typical N-gram language modelusing perplexity.Perplexity (Jelinek, 1990a; McCandless,1994;) is an information-theoretic measure for evaluating how well a statistical language modelpredicts a part icular test set.
It is an excellent metric forcomparing two language models because it is entirely independent ofhow each language model functions internally, and also because it isvery simple to compute.
For a given vocabulary size, a language modelwith lower perplexity is modeling language more accurately, which willgeneral ly correlate with lower error rates during speech recognition.Perplexity is derived from the average log probabi l i ty that thelanguage model assigns to each word in the test set:~=_1 x ~'~Jog2 P(wilw,,...,w,_,) (11)IV I--iwhere wl,...,w~, are all words of the test set constructed by listingthe sentences of the test set end to end, separated by a sentence306boundary marker.
The perplexity is then 2 ~, S may be interpreted asthe average number of bits of information needed to compress each wordin the test set given that the language model is providing us withinformation.We compare the perplexity result of the N-gram language model withclass-based N-gram language model.
The perplexit ies PP  of N-gram forword and class are:Unigram for word:I,%"?xp(- ~ ,__~ In(P(~ )11 (12/Bigram for word:|xexp(- ~ ~ln(P(~ I~-, )11 (13)1=1Bigram for  c lass2  exp(--77~ln(P(~lC(w,))P(C(~)lC(~_t))) ) (14)where w, denotes the ith word in the corpus and C(~)  denotes theclass that w, is assigned to.
N is the number of words in the corpus.P(C(wi)IC(w,_,))can be estimated by:P(C(w, ), C(w,_, ))P(C(w, ) lC (w,_ , ) )  = (151Nwhere P(C(~ ),C(w,_, )) = X e(w, )P(C(~ )~, )P(C(~_,)I~ 1i=INP(C(w,_,)) = ~P(w,_t)P(C(w,_,)\[~_,),=2The perplexit ies PP  based on different N-gram for word and class arepresented in table III.Note that we present "hard" classif ication and ~soft" classif icationresults in word class- based language model respectively.
Forprobabil ist ic classification, we define the word as belonging tocertain class in which this word has the largest probability.The training corpus contains more than 12,000 Chinese words.
And thevocabulary has I034 Chinese words which are most frequent.
We use foursubcorpora mentioned above as test sets.An arbitrary nonzero probabi l i ty is given to all Chinese words and1symbols that do not exist in the vocabulary.
We set P(w)- 2N to theword w which are not in the vocabulary.
N is the number of words inthe training corpus.From table III, we can know that perplexity of "hard" class-basedbigram is 28.7% lower than the word-based bigram, while perplexity ofthe "soft" class-based bigram is much lower than the "hard" class-based bigram, perplexity reduction is about 43% compared with ~hard"class-based bigram.307Table I I I .
Perplexity comparision between N-gram for word and N-gramfor classSubcorpus size 443'26 10323wordsPerplexity ofClass Bigram(soft)17451words25130words wordsPerplexity 0fUnigram 293.4 734.1 1106.3 1757.7Perplexity ofBigram 198.9 220.6 427.5 704.2Perplexity clfClass Bigram 147.5 153.2 314.3 525.4(hard)119.6 243.8 454.7 140.76.
ConclusionsIn this paper we show a new method for Chinese words classification.But it can be applied in multiple language too.
It integrates top-downand bottom-up idea in word classification.
Thus top-down splitt ingtechniques can learn from bottom-up idea's strong points to offset itsobvious weakness and keep the advantage of itself.
Especially, unlikeother classif ication methods, this method takes the context-sensit iveinformation which most classif ication methods do not consider intoaccount and make it reflect the properties of natural language moreclearly.
Moreover, the probabil it ies are assigned to the words todemonstrate how well a word belongs to classes.
This property is veryuseful in word class-based language model ing used in speechrecognition, for it allows the system to have several powerfulcandidates to be matched during recognition.It, however, is important to consider the l imitations of the method.The computational cost fs very high.
The algorithm's complexity iscubic when we move one word from one class to another.
Also, theprobabil i t ies the word assign to each class is not global optimal.
Itreflects the degree of a word belonging to classes approximately.
Andexcessive or insufficient classif ication may occur because the classnumber is fixed artificially.Re~renceBahl, L.R., Brown, P.F.
DeSouza, P. V. and Mercer, R. L. (1989).
Atree-based statistical language model for natural language speechrecognition.
IEEE Transactions on ASSP, 37(7): 1001-1008.Brill, E. (1993).
A Corpus-Based Approach To Language Learning.Ph.D.
thesis.Brown.
P., Della Pietra, S., & Mercer, R. (1991).
Word sensedisambiguation using statistical methods.
Proceedings of the AnnualMeet ing of the ACL, pp.264-170.308IIIIIIIiIIIIBrown.
P., Della Pietra, V., deSouza, P., Lai, J. and Mercer, R.(1992).
Class-based n-gram models of natural language.
ComputationalLinguistics 18, pp.
467-479.Chang C.-H. and Chen, C.-D. (1993a).
A Study on Integrating ChineseWord Segmentation and Part-of-Speech Tagging.
Communications of COLIPSVol 3, No i, pp 69-77.Chang C.-H. and Chen, C.-D. (1993b).
HMM-based part-of-speech taggingfor Chinese corpora.
Proceedings of the workshop on Very LargeCorpora: Academic and Industrial Perspectives, pp.40-47, Columbus,Ohio, USA.Chang C.-H. and Chen, C.-D. (1995).
A Study on .Corpus-BasedClassification of Chinese Words.
Communications of COLIPS, Vol 5, Nol&2, pp.l-7.Church, K. W. and Mercer, R.L.(1993).
Introduction to the specialissue in computational linguistics using large corpora.
ComputationalLinguistics 19, pp.l-24.Cutting,D., Kupiec, J., Pederson, J., Sibun, P. (1992).
A PracticalPart-of-Speech Tagger.
Applied Natural Language Processing, Trento,Italy, pp.133-140.Dagan, I.,Marcus, S. and Markovitch,S.
(1995).
Contextual wordsimilarity and estimation from sparse data Computer Speech andLanguage, pp.
123-152.DeRose, S. (1988).
Grammatical category disambiguation bystatistical optimization..Computational Linguistics 14, pp.
31-39.Finch, S.P.
(1993).
Finding Structure in Language, Ph.D. thesis,Centre for Cognitive Science, University of Edinburgh.Gao, J. and Chen, X.X.
(1996).
Automatic Word Segmentation ofChinese Texts Based on Variable Distance Method.
Communications ofCOLIPS, Vol 6, No.2.Garside, R., Leech, G. and Sampson, G. (1987).
The computationalanalysis of English: A corpus-based approach.
Longman.Huang, X.-D., Alleva, F., Hon, H.-W., Hwang, M.-Y., Lee, K.-F. andRosenfeld, R. (1993).
The SPHINX-II Speech Recognition System: AnOverview.
Computer Speech and Language, Vol 2, pp.137-148.Hughes, J.
(1994).
Automatically Acquiring a Classification ofWords.
Ph.D. thesis, School of Computer Studies, University of Leeds.Jardino, M. and Adda, G. (1993).
Automatic word classification usingsimulated annealing.
Proceedings of ICASSP-93, pp.
II:41-44.Minneapolis, Minnesota, USA.Jelinek, F.(1990a).
Self-Organized language modeling for speechrecognition.
Readings in speech recognition, Alex Waibel and Kai-FuLee, eds, pp.450-506.Jelinek, F., Mercer, R. and Roukos, S. (1990b).
Classifying Wordsfor improved Statistical Language Models.
IEEE Proceedings of ICASSP'90, pp.621-624.Jelinek, F., Mercer, R. and Roukos, S. (1992).
Principles oflexical language modeling for speech recognition.
Advances in SpeechSignal Processing, pp.
651-699, Mercer Pekker, Inc.Kupiec, J.
(1992).
Robust part-of-speech tagging using a hidden309markov model.
Computer Seech and Language, 6: 225-242.Lee, H.-J.
and Chang, .C.-H. (1992).
A Markov language model inhandwritten Chinese text recognition.
Proceedings of Wordshop onCorpus-based Researches.
and Techniques for Natural LanguageProcessing, Taipei, Taiwan.Lee, H.-J., Dung, C.-H., Lai, F.-M. and Chang Chien.
C.-H.(1993).Applications of Markov language models.
Proceedings of Wordshop onAdvanced Information Systems, Hsinchu, Taiwan.Lin, Y.-C., Chiang, T.-H. and Su, K.-Y.
(1993).
A preliminary studyon unknown word problem in Chinese word segmentation.
Proceedings ofROCLING VI, pp.l19-141, sitou, Nantou, Taiwan.Magerman, D. M. (1994).
Natural Language Parsing as StatisticalPattern Recognition.
Ph.D. thesis.McCandless, M. K. (1994).
Automatically acquisition of languagemodels for speech recognition.
M.S thesis, Massachusetts Institute ofTechnology.McMahon, J. and Smith, F.J. (1995).
Improving Statistical LanguageModel Performance with Automatical ly Generated Word Hierarchies.Computational Linguistics.McMahon,J.
(1994).
Statistical language processing based on self-organising word classification.
Ph.D. thesis, The Queen's Universityof Belfast.Merialdo, B.
(1994).
Tagging English Text with a ProbabilisticModel.
Computational Linguistics 20 (2), pp.
155-172.Peng, T.-Y.
and Chang, J.-S. (1993).
A study on chinese lexicalambiguity - word segmentation and part-of -speech tagging.
Proceedingsof ROCLING VI, pp.173-193.Pereira, F. and Tishby, N. (1992).
Distributional similarity, phasetransitions and hierarchical" clustering.
Working Notes: AAAI Fallsymposium on Probabilistic Approaches to Natural Language, pp.
108-112.Pereira, F., Tishby, N. and Lee, L. (1993).
DistributionalClustering of English Words.
Proceedings of the Annual Meeting of theACL, pp.
183-190.Pop, M. (1996).
Unsupervised Part-of-Speech Tagging.
the JohnHopkins University.Resnik, P. (1992).
Wdrdnet and destributional analysis: A class-based approach to lexical discorvery.
AAAI Wordshop on Statistically-based Natural Language Processing Techniques, July, pp.
56-64.Rosenfeld, R. (1994).
Adaptive Statistical Language Modeling: AMaximum Entropy Aproach.
Ph.D. thesis.Schutze, H. (1995).
Distributional Part-of-Speech tagging.
EACL95.Wu, J., Wang, Z.-Y., Yu, F. and Wang, X.
(1995).
AutomaticClassification of Chinese texts.
Jouznal of Chinese InformationProcessing.
Vol.
9 No.4, pp.23-31.IIIIII310
