In: Proceedings of CoNLL-2000 and LLL-2000, pages 209-218, Lisbon, Portugal, 2000.The Acquisition of Word Order bya Computational Learning SystemAline VillavicencioComputer  Laboratory, University of CambridgeNew Museums Site, Cambridge, CB2 3QG, England, UKAline.
Vil lavicencio@cl.
cam.
ac.
ukAbst rac tThe purpose of this work is to investigate theprocess of grammatical acquisition from data.We are using a computational learning sys-tern that is composed of a Universal Grammarwith associated parameters, and a learning al-gorithm, following the Principles and Parame-ters Theory.
The Universal Grammar is imple-mented as a Unification-Based Generalised Cat-egorial Grammar, embedded in a default inher-itance network of lexical types.
The learning al-gorithm receives input from a corpus annotatedwith logical forms and sets the parameters basedon this input.
This framework is used as basisto investigate several aspects of language acqui-sition.
In this paper we are concentrating on theacquisition of word order for different learners.The results obtained show the different learnershaving a similar performance and converging to-wards the target grammar given the input dataavailable, regardless of their starting points.
Italso shows how the amount of noise present inthe input data affects the speed of convergenceof the learners towards the target.1 In t roduct ionIn trying to solve the question of how to get amachine to automatically earn linguistic infor-mation from data, we can look at the way peopledo it.
Gold (1967) when investigating languageidentification i  the limit, obtained results thatimplied that natural languages could not belearned only on the basis of positive evidence.These results were used as a confirmation for theproposal that children must have some innateknowledge about language, the Universal Gram-mar (UG), to help them overcome the prob-lem of the poverty of the stimulus and acquirea grammar on the basis of positive evidenceonly.
According to Chomsky's Principles andParameters Theory (Chomsky 1981), the UGis composed of principles and parameters, andthe process of learning a language is regardedas the setting of values of a number of parame-ters, given exposure to this particular language.We employ this idea in the learning frameworkimplemented.In this work we are interested in investigatingthe acquisition of grammatical knowledge fromdata, focusing on the acquisition of word or-der, that reflects the underlying order in whichconstituents occur in different languages (e.g.SVO and SOV languages).
The learning sys-tem is equipped with a UG and associated pa-rameters, encoded as a Unification-Based Gen-eralised Categorial Grammar, and a learning al-gorithm that fixes the values of the parametersto a particular language.
The learning algo-rithm follows the Bayesian Incremental Param-eter Setting (BIPS) algorithm (Briscoe 1999),and when setting the parameters it uses a Mini-mum Description Length (MDL) style bias tochoose the most probable grammar that de-scribes the data well, given the goal of converg-ing to the target grammar.
In section 2 we de-scribe the components of the learning system.In section 3, we investigate the acquisition ofword order within this framework and discussthe results obtained by different learners.
Fi-nally we present some conclusions and futurework.2 The Learning SystemThe learning system is composed of a languagelearner equipped with a UG and a learning al-gorithm that updates the initial parameter set-tings, based on exposure to a corpus of utter-ances.
Each of these components i  discussed in209more detail in the following sections.2.1 The  Universa l  GrammarThe UG consists of pr inc ip les  and  parame-te rs ,  and the latter are set according to thelinguistic environment (Chomsky 1981).
Thisproposal suggests that human languages followa common set of principles and differ amongone another only in finitely many respects, rep-resented by a finite number of parameters thatcan vary according to a finite number of val-ues (which makes them learnable in Gold'sparadigm).
In this section, we discuss theUG and associated parameters, which are for-malised in terms of a Unification-Based Gen-eralised Categorial Grammar (UB-GCG), em-bedded in a default inheritance network of lex-ical types.
We concentrate on the descriptionof word order parameters, which reiiect the ba-sic order in which constituents occur in differentlanguages.UB-GCGs extend the basic Categorial Gram-mars ((Bar Hillel, 1964)) by including the use ofattribute-value pairs associated with each cate-gory and by using a larger set of rules and op-erators.
Words, categories and rules are repre-sented in terms of typed default feature struc-tures (TDFSS), that encode orthographic, syn-tactic and semantic information.
There aretwo types of categories: atomic categories (s- sentence- ,  np  - noun  phrase- ,  and  n -noun) ,  that are saturated, and complex cat-egories, that are unsaturated.
Complex cate-gories have a functor category (defined in RE-SUET), and a list of subcategorised lements (de-fined in ACTIVE), with each element in the listdefined in terms of two features: SIGN, encodingthe category, and DIRECTION, encoding the di-rection in which the category is to be combined(where VALUE can be either forward or back-ward).
As an example, in English an intransi-tire verb (s\np) is encoded as shown in figure1, where only the relevant attributes are shown.In this work, we employ the rules of (forwardand backward) application, (forward and back-ward) composition and generalised weak permu-ration.
A more detailed description of the UB-GCG used can be found in (Villavicencio 2000).The UG is implemented as a UB-GCG, era-bedded in a default inheritance network of lex-ical types (Villavicencio 1999), implemented inthe YADU framework (Lascarides and Copes-take 1999).
The categories and rules in thegrammar are defined as types in the hierarchy,represented in terms of TDFSS and the feature-structures associated with any given categoryor rule are defined by the inheritance chain.With different sub-networks used to encode dif-ferent kinds of linguistic knowledge, linguisticregularities are encoded near the top of a net-work, while types further down the network areused to represent sub-regularities or exceptions.Thus, types are concisely defined, with onlyspecific information being described, since moregeneral information is inherited from the super-types.
The resulting UB-GCG is compact, sinceit avoids redundant specifications and the infor-mation is structured in a clear and concise waythrough the specification of linguistic regulari-ties and sub-regularities and exceptions.Regarding the categories of  the UB-GCG,word order parameters are those that specifythe direction of each element in the subcate-gorisation list of a complex category.
In figure1, sub jd i r  is a parameter specifying that thenp subject is to be combined backwards.
As thecategories are defined in terms of an inheritancehierarchy, the parameters (and their values) inthese categories are propagated throughout thehierarchy, from supertypes to subtypes, whichinherit his information by default.
There are 28parameters defined, and they are also in a hier-archical relationship, with the supertype beinggendir ,  which specifies, by default, the generaldirection for a language, and from which all theother parameters inherit.
Among the subtypes,we have subjd i r ,  which specifies the directionof the subject, vargdir ,  which specifies the di-rection of the other verbal arguments and ndir,which specifies the direction of nominal care-gories.
A fragment of the parameters hierarchycan be seen in figure 2.
With these 28 binary-valued parameters the UG defines a space ofalmost 800 grammars.The parameters are set based on exposure toa particular language, and while they are un-set, they inherit their value by default, fromtheir supertypes.
Then, when they are set, theycan either continue to inherit by default, in casethey have the same value as the supertype, orthey can override this default and specify theirown value, breaking the inheritance chain.
Forinstance, in the case of English, the value of210intransitiveRESULT : SIGN : s /JACTIVE : SIGN :np\[ subjdir DIRECTION : \[VALUE backw d\]\]Figure 1: Intransitive Verb typegendirsubjdir vargdir ndir/ \nmdir detdirFigure 2: Fragment of The Parameters Hierar-chya default inheritance schema reduces the piecesof information to be acquired by the learner,since the information is structured and what itlearns is not a single isolated category, but astructure that represents his information in ageneral manner.
This is a clear and concise wayof defining the UG with the parameters beingstraightforwardly defined in the categories, ina way  that takes advantage of the default in-heritance mechanism, to propagate informationabout parameters, throughout the lexical inher-itance network.gendir  is defined, by default, as forward, cap-turing the fact that it is a predominantly right-branching language, and all its subtypes, likesubjd ir  and vargdir  inherit this default in-formation.
Then an intransitive verb, whichhas the direction of the subject specified bysubjdir,  will be defined as S/NP, with sub-jd ir  having default value forward.
However,as in English, the subject NP occurs to the leftof the verb, utterances with the subject o theleft will trigger a change in subjd i r  to back-ward, which overrides the default value, break-ing the inheritance chain, figure 3.
As a re-sult, intransitive verbs are defined as S\NP, fig-ure 1, for the grammar to account for thesesentences.
In the syntactic dimension of thisnetwork, intransitive verbs can be consideredthe general case of verbs, and the informationdefined in this node is propagated through thehierarchy to its subtypes, such as the transitiveverbs, figure 3.
For the learner, the informationabout subjects (subjdir  = backward) has al-ready been acquired while learning intransitiveverbs, and the learner does not need to learnit again for transitive verbs, which not only in-herit this information, but also have the direc-tion for the object defined by vargdir  (vargdir= forward), as shown in figure 3.
The use of2.2 The CorpusThe UG has to be general enough to capturethe grammar for any language, and the param-eters have to be set to account for a particularlanguage, based on exposure to that language.This can be obtained by means of a corpus ofutterances, annotated with logical forms, whichis described in this section.
Among these sen-tences, some will be triggers for certain param-eters, in the sense that, to parse that sentence,some of the parameters will have to be set toa given value.
We are using the Sachs cor-pus (Sachs 1983) from the CHILDES project(MacWhinney 1995), that contains interactionsbetween only one child and her parents, fromthe age of 1 year and 1 month to 5 years and 1month.
From the resulting corpus, we extractedmaterial for generating two different corpora:one containing only the child's sentences andthe other containing the caretakers' entences.The caretakers' corpus is given as input to thelearner to mirror the input to which a childlearning a language is exposed.
And the child'scorpus is used for comparative purposes.In order to annotate the caretakers' corpuswith the associated logical forms, a UB-GCGfor English was built, that covers all the con-structions in the corpus: several verbal con-211top.._._~_~' = / complexI ndir var.gdiir subjdir = \ intransitive,2 ............................. nrndir detdir .
.
.
.
.
,~::::: .......
""""f"'"'~~.'"'"''.............
tra'n's~tiv.e..~ oblique intransitive-control.............. ~p) /~p (s~np)/pp (s~np)/(s\np)d i t ra~s i t~=~'~-~e_cont ro l((s\np)/np)/np ((s~np)/np)/pp ((s~np)/np)/(s~np)Figure 3: A Fragment of the Network of Typesstructions (intransitives, transitives, ditransi-tives, obliques, control verbs, verbs with senten-tim complements, etc), declarative, imperativeand interrogative sentences, and unbounded de-pendencies (wh-questions and relative clauses),among others.
Thus the caretakers' corpus con-tains sentences annotated with logical forms,and an example can be seen in figure 4, for thesentence I wil l  take him, where a simplified ver-sion of the relevant attributes is shown, for rea-sons of clarity.
Each predicate in the semanticslist is associated with a word in the sentence,and, among other things, it contains informa-tion about the identifier of the predicate (SIT),the required arguments (e.g.
ACTOR and UN-DERGOER for the verb take), as well as aboutthe interaction with other predicates, specifiedby the boxed indices (e.g.
take:ACTOR = \[\] =/:SIT).
This grammar is not only used for anno-tating the corpus, but is also the target o whichthe learner has to converge.
At the momentaround 1,300 utterances were annotated withcorresponding logical forms, with data rangingfrom when the child is 14 months old to 20months old.2.3 The  Learn ing  A lgor i thmThe learning algorithm implernents theBayesian Incremental Parameter Setting(BIPS) algorithm defined by Briscoe (1999).The parameters are binary-valued, where eachpossible value in a parameter is associated witha prior and a posterior probability.
The valuewith highest posterior probability is used asthe current value.
Initially, in the learningprocess, the posterior probability associatedwith each parameter is initialised to the priorprobability, and these values are going to definethe parameter settings used.
Then, as triggersentences are successfully parsed, the posteriorprobabilities of the parameter settings that al-lowed the sentence to be parsed are reinforced.Otherwise, when a sentence cannot be parsed(with the correct logical form) the learningalgorithm checks if a successful parse can beachieved by changing the values of some of theparameters, in constrained ways.
If that is thecase, the posterior probability of the valuesused are reinforced in each of the parameters,and if they achieve a certain threshold, theyare retained as the current values, otherwisethe previous values are kept.
This constrainton the setting of the parameters ensures that atrigger does not cause an immediate change toa different grammar.
The learner, instead, hasto wait for enough evidence in the data beforeit can change the value of any parameter.
Asa consequence, the learner behaves in a moreconservative way, being robust to noise presentin the input data.Following Briscoe (1999) the probabilities as-sociated with the parameter values correspondto weights represented in terms of fractions,with the denominator storing the total evidencefor a parameter and the numerator storing theevidence for a particular value of that param-eter.
For instance, if the value backward  ofthe sub jd i r  parameter has a weight of 9/10,it means that from 10 times that evidence wasprovided for subjd i r ,  9 times it was for thevalue backward,  and only once for the othervalue, forward.
Table 1 shows a possible ini-tialisation for the sub jd i r  parameter, where theprior has a weight of 1/10 for forward,  corre-sponding to a probability of 0.1, and a weight of212s ignORTH : <i, will, take, him>CAT : sSEM \[ will \]: S IT :  \[5\]ARGU~IENT : \[\]?
take \]SIT: \[\]ACTOR: \[\]UNDERGOER : \[\]L S IT  : \[\] \]Figure 4: Sentence: I will take him9/10 for backward,  corresponding to a proba-bility of 0.9.
The posterior is initialised with thesame values as the prior, and as backward hasa higher posterior probability it is used as thecurrent value for the parameter.
These initialparameter values determine the initial gram-mar for the learner.
As triggers are processed,they provide evidence for certain parametersand these are represented as additions to thedenominator and/or numerator of each of theposterior weights of the parameter values.
Ta-ble 2 shows the status of the parameter after5 triggers that provided evidence for the valuebackward.
Initially, the learner uses the evi-dence provided by the triggers to choose certainparameter values, in order to be able to parsethese triggers uccessfully while generating theappropriate logical form.
After that, the trig-gers are used to reinforce these values, or tonegate them.Table 1: Initialisation of a ParameterValue Prior PosteriorProb.
\[Weight Prob.
\[Weight1 0.1 1 Forward O.
1 1-o 1oBackward 0.9 ~ 0.9 10 10and specifies its own value, breaking the inheri-tance chain.
For instance, in figure 3, subjd iroverrides the default value specified by gendir,breaking the inheritance chain.
Unset subtypeparameters inherit, by default, the current valueof their supertypes, and while they are unsetthey do not influence the values of their super-types.As the parameters are defined in a defaultinheritance hierarchy, each time the posteriorprobability of a given parameter is updated, itis necessary to update the posterior probabili-ties of its supertypes and examine the currentparameter settings to determine what the mostappropriate hierarchy for these settings is, giventhe goal of converging to the target.
The learnerhas a preference for grammars (and thus hi-erarchies) that not only model the data (rep-resented by the current settings) well, but arealso compact, following the Minimum Descrip-tion Length (MDL) Principle.
In this case, themost probable grammar in the grammar space,among the ones consistent with the parametersettings, is the one where the default inheritancehierarchy is the more concise, having the min-imum number of non-default parameter valuesspecified, as described in (Villavicencio 2000).Table 2: Status of the ParameterThe 28 word order parameters are defined ina hierarchical relation, with the supertype pa-rameters being set in accordance with the sub-types, to reflect he value of the majority of thesubtypes.
In this way, as the values of the sub-types are being set, they influence the value ofthe supertypes.
If the value of a given sub-type differs from the value of the supertype,the subtype overrides the inherited efault valueValueForwardI Prior PosteriorProb.
I Weight Prob.
I Weight1 0.07 1 O.
1 1-o 1-5Backward 0.9 0.93 1j I0 152133 The Acquisition of  Word  OrderWe are investigating the acquisition of wordorder, which reflects the underlying order inwhich constituents occur in different languages.In this section we describe one experiment,where we compare the performance, of differ-ent learners under four conditions.
Each learneris given as input the annotated corpus of sen-tences paired with logical forms, and they haveto change the values of the parameters corre-sponding to the relevant constituents oaccountfor the order in which these constituents ap-pear in the input sentences.
We defined fivedifferent learners corresponding to five differ-ent initialisations of the parameter settings ofthe UG, to investigate how the init~alisations,or starting points, of the learners influence con-vergence to the target grammar.
The first one,the unset learner, is initialised with all param-eters unset, and the others, the default learn-ers, are each initialised with default parametervalues corresponding to one of four basic wordorders, defined in terms of the canonical orderof the verb (V), subject (S) and objects (O):SVO,  SOV,  VSO and OVS.
We initialised theparameters subjdir, vargdir and gendir  of thedefault learners according to each of the basicorders, with gendir  having the same directionas vargdir, and all the other parameters hav-ing unset values.
These parameters have theprior and posterior probabilities initialised with0.1 for one value and 0.9 for the other.
In thisway, an SVO learner, for example, is initialisedwith subjd i r  having as current value backward(0.9), vargdir  forward (0.9) and gendir  for-ward (0.9).The sentences in the input corpus are pre-sented to a learner only once, sequentially, inthe original order.
The input to a learner ispre-processed by a system \[Waldron, 2000\] thatassigns categories to each word in a sentence.The sentences with their putative category as-signments are given as input to the learner.
Thelearner then evaluates the category assignmentsfor each sentence and only uses those that arevalid according to the UG to set the parame-ters; the others are discarded.
The corpus con-tains 1,041 English sentences (which follow theSVO order), but from these only a small propor-tion are triggers for the parameters, in the sensethat, for the learner to process them, it has toselect certain parameter values.
As each trigger-ing sentence is processed, the learner changes orreinforces its parameter values to reflect he or-der of constituents in these sentences.We wanted to check how the different learnersperformed in a normal noisy environment, witha limited corpus as input, and also to check ifthere is an interaction between the different ini-tialisations and the noise in the input data.
Todo that we tested how the learners performedunder four conditions.
Each condition was run10 times for each learner, and we report herethe average results obtained.3.1 Condi t ion 1 :Learners -10  in aNoisy EnvironmentIn the first condition, we initialised the param-eters subjdir ,  vargdir  and gendir  of the de-fault learners with the prior and posterior prob-abilities of 0.1 corresponding to a weight of1/10, and probabilities of 0.9 to a weight of9/10.
Results from the first experiment can beseen in table 3, where the learners are specifiedin the first column, the number of input triggersin the second, the number of correct parametersin relation to the target is in the third, and thenumber of parameters that are set with thesetriggers is in the fourth column.Table 3: Convergence of the different learners -Learners-10Learners Triggers Parameters ParametersCorrect Set-Unset 179 22.3 10.5SVO-10 211.4 22.5 11-SOV-10 205.4 22.2 10.2OVS-10 271.5 22.5 11VSO-10 198.7 22.1 10.2The results show no significant variation inthe performance of the different Learners.
Thisis the case with the number of parameters thatare correct in relation to the target, with an av-erage of 22.3 parameters out of 28, and also withthe number of parameters that are set given thetriggers available, with an average of 10.5 pa-rameters out of 28.The only difference between the learners was214Sub jd i r  - No isy  Env i ronment1= 0.9 \[- - - Unsetsvo- o~" I sov - lo  a'~ 0.6 \[ VSO-IO0.5TriggersFigure 5: Convergence of Subjdir- Learners-10 - Noisy Environmentthe time needed for each learner to converge:the closer the starting point of the learner wasto the target, the faster it converged, as canbe seen in figure 5, for the subjd i r  parame-ter.
This figure shows all the learners converg-ing to the target value, with high probability,and with a convergence pattern very similar tothe one presented by the unset learner.
Eventhose default learners that were initialised withvalues incompatible with the target soon over-came this initial bias and converged to the tar-get.
The same thing happens for vargdir  andgendir.
This figure also shows some sharp fallsin the convergence to the target value, for theselearners.
For example, the unset learner had asharp drop in probability, which fell from 0.94to 0.85, around trigger 16.
These declines werecaused by noise in the category assignments ofthe input triggers, which provided incorrect ev-idence for the parameter values.3.2 Condition 2:Learners-10 in aNoise-free EnvironmentIn order to test if and how much of the learn-ers' performance was affected by the presenceof noisy triggers, using the same initialisationsas the ones in condition 1, we tested how thelearners performed in a noise-free nvironment.To obtain such an environment, aseach triggerwas processed, a module was used for correctingthe category assignment, if noise was detected.The results are shown in table 4.These learners have performances similar toTable 4: Convergence of the different learners -Learners-10 - Noise-freeLearners Triggers Parameters ParametersCorrect SetUnset 235.1 22.3 10.6SVO-10 227.9 22.3 10.6SOV-10 213.9 22.6 11.2OVS-10 212.2 22.3 10.6VSO-10 172.4 22 10those in condition 1 (section 3.1), with an av-erage of 22.3 of the 28 parameters correct inrelation to the target, and an average of 10.6 pa-rameters that can be set with the triggers avail-able.
But, in this condition the convergence wasslightly faster for all learners, as can be seen infigure 6.
These results show that, indeed, thepresence of noise slows down the convergence ofthe learners, because they need more triggers tocompensate for the effect produced by the noisytriggers.3.3 Condition 3:Learners-50 in aNoisy EnvironmentWe then tested if the use of stronger weightsto initialise the learners would affect the learn-ers performance.
The parameters ubjdir,vargdir and gendir were initialised with aweight of 5/50 for the probability of 0.1 and a2151= 0.9'~ 0.80.7~ 0.60.0.5Subjdir- Noise-free Environmenti i i i i  I I I - -  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.//!aIr!
!
i !
i ; i iTriggers- - Unset-- -- SVO-IOSOV- 10. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
OVS-IO.
.
.
.
.
.
VSO-IOFigure 6: Convergence of Subjdir- Learners-10 - Noise-free Environmentweight of 45/50 for the probability ot70.9.
Theseweights provide an extreme bias for each of thelearners.
In this condition, the learners weretested again in a normal noisy environment.Figure 7 shows the convergence patterns pre-sented by these learners for the subjd i r  param-eter.
The effect produced by the noise was in-creased with these stronger weights, such thatall the learners had a slower convergence to thetarget.
Even those default leaxners initialisedwith values compatible with the target had aslightly slower convergence when compared tothose in condition 1, with weaker weights, be-cause they had to overcome the stronger initialbias before converging to the target values.
But,in spite of that, the performance of the learnersis only slightly affected by the stronger weights,as shown in table 5.
They had a performancesimilar to the ones obtained by the learners inthe previous conditions, as shown in figure 8,comparing these learners with those in condi-tion 1.3.4 Condi t ion 4 :Learners -50  in aNoise-free Envi ronmentWhen the noise-free nvironment was used withthese stronger weights, the convergence patternwas slightly faster for all learners, when com-pared to condition 3 (which used a noisy envi-ronment), but still slower than conditions 1and2, as shown in figure 9.
These learners had asimilar performance to those obtained in all theprevious conditions, as can be seen in table 6,Noisy Environment28i 20 \[ \ ]  Set 1612 I \ [ \ ]  Correct840 J i ~-"~ y j  ~ y J  : J  ; JLearnersFigure 8: Learners in Noisy Environmentand in figure 10, which also shows the resultsobtained by the learners in condition 2, whichTable 5: Convergence of the different learners -Learners-50 - NoiseLearners Triggers Parameters ParametersCorrect SetSVO-50 230.3 22.9 11.8SOV-50 168.1 22.4 10.4OVS-50 221.4 22.1 10.1VSO-50 154.6 21.9 9.72161 .,,,~ 0.9"~ 0.8.~ 0.7"~ 0.6OI1.0.5Subjdir - Noisy EnvironmentTriggersl SVO-50 soy-50- ................ OVS-50- - VSO-50Figure 7: Convergence of Subjdir - Learners-50 - Noisy EnvironmentSubjdir-  Noise-free Environment~o."~0.30.
"E1, r -TriggersI ~SVO-50  SOV-50 ~.
........ OVS-50 - - VSO-50Figure 9: Convergence of Subjdir - Learners-50 - Noise-free Environmentused weaker weights.Table 6: Convergence of the different learners -Learners-50 - Noise FreeLearners Triggers Parameters ParametersCorrect SetSVO-50 221.7 23.2 11.5SOV-50 195.4 23.2 11.8OVS-50 223.2 22.1 9.9VSO-50 223.4 21.8 9.83.5 D iscuss ionAs confirmed by these results, there is a stronginteraction between the different starting pointsand the presence of noise.
The noise has astrong influence on the convergence of the learn-ers, slowing down the learning process, since thelearners need more triggers to compensate forthe effect caused by the noisy ones.
The dif-ferent initialisations caused little impact in thelearners' performance, in spite of noticeably de-laying the convergence to the target of thoselearners that have values incompatible with thetarget.
Thus, when combining the presence ofnoise with the use of stronger weights , there was217a significant delay in convergence, w\]~ere the fi-nal posterior probability was up to 10% lowerthan in the noise-free case (e.g.
for the OVSlearner), as can be seen in figures 7 and 9.Nonetheless, these learners were robust to thepresence of noise in the input data, only select-ing or changing a value for a given parameterwhen there was enough evidence for that.
Asa consequence, all the learners were convergingtowards the target, even with the small amountof available triggers, regardless of the initialisa-tions and the presence of noise.
This is the caseeven with an extreme bias in the initial values.Moreover, the learners make effective use of theinheritance mechanism topropagate default val-ues, with an average of around 4.2 non-defaultspecifications for these learners.4 Conc lus ion  and Future  WorkThe purpose of this work is to investigate theprocess of grammatical cquisition from a com-putational perspective, focusing on the acqui-sition of word order from data.
Five differentlearners were implemented in this frameworkand we investigated how the starting point forthe learners affects their performance in con-verging to the target and its interaction withnoise.
The learners were all converging towardsthe target grammar, where the different start-ing points and the presence of noise affectedonly convergence times, with learners more faraway from the target having a slower conver-gence pattern.
Future works include annotat-ing more data to have a bigger corpus, and run-ning more experiments with this corpus, testinghow much data is required for all the triggersNoise-Free Environmento4 ,,__., .
.
.
.
.
.
.Learnersto converge, with high probability to the tar-get grammar.
After that, we will concentrateon investigating the acquisition of subcategori-sation frames and argument structure, using thesame framework for learning.
Although this isprimarily a cognitive computational model, it ispotentially relevant to the development of moreadaptive NLP technology.5 AcknowledgementsI would like to thank Ted Briscoe for his com-ments and advice on this paper, and FabioNemetz for his support.
Thanks also to theanonymous reviewers for their comments.
Theresearch reported on this paper is supported bydoctoral studentship from CAPES/Brazil.ReferencesBar Hillel, Y.
Language and Information.
Wesley,Reading, Mass.
1964.Briscoe, T. The Acquisition of Grammar in anEvolving Population of Language Agents.
Linkop-ing Electronic Articles in Computer and Informa-tion Science, http://www.ep.liu.se/ea/cis/1999.Chomsky, N. Lectures on Government and Binding.Foris Publications, 1981.Gold, E.M.
Language Identification in the Limit.
In-formation and Control, v.10, p.447-474, 1967.Lascarides, A. and Copestake, A.
Default Represen-tation in Constraint-based Frameworks.
Compu-tational Linguistics, v.25 n.1, p.55-105, 1999.MacWhinney, B.
The CHILDES Project: Tools forAnalyzing Talk.
Second Edition, 1995.Sachs, J.
Talking about the there and then: the emer-gence of displaced reference in parent-child is-course.
In K. E. Nelson editor, Children's lan-guage, v.4, 1983.Villavicencio, A.
Representing a System of LexicalTypes Using Default Unification.
Proceedings ofEACL, 1999.Villavicencio, A.
The Acquisition of a Unification-Based Generalised Categorial Grammar.
Proceed-ings of the Third CLUK Colloquium, 2000.Waldron, B.
Learning Natural Language within theframework of categorial grammar.
Proceedings ofthe Third CLUK Colloquium, 2000.Figure 10: Learners in Noise Free Environment218
