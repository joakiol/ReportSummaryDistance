Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 42?49,Baltimore, Maryland, USA.
June 27, 2014.c?2014 Association for Computational LinguisticsAn Impact Analysis of Features in a Classification Approach toIrony Detection in Product ReviewsKonstantin Buschmeier, Philipp Cimiano and Roman KlingerSemantic Computing GroupCognitive Interaction Technology ?
Center of Excellence (CIT-EC)Bielefeld University33615 Bielefeld, Germanykbuschme@techfak.uni-bielefeld.de{rklinger,cimiano}@cit-ec.uni-bielefeld.deAbstractIrony is an important device in human com-munication, both in everyday spoken con-versations as well as in written texts includ-ing books, websites, chats, reviews, andTwitter messages among others.
Specificcases of irony and sarcasm have been stud-ied in different contexts but, to the best ofour knowledge, only recently the first pub-licly available corpus including annotationsabout whether a text is ironic or not hasbeen published by Filatova (2012).
How-ever, no baseline for classification of ironicor sarcastic reviews has been provided.With this paper, we aim at closing this gap.We formulate the problem as a supervisedclassification task and evaluate differentclassifiers, reaching an F1-measure of up to74 % using logistic regression.
We analyzethe impact of a number of features whichhave been proposed in previous research aswell as combinations of them.1 IntroductionIrony is often understood as ?the use of words thatmean the opposite of what you really think espe-cially in order to be funny?
or ?a situation thatis strange or funny because things happen in away that seems to be the opposite?
of what is ex-pected.1Many dictionaries make this differencebetween verbal irony and situational irony (BritishDictionary, 2014; New Oxford American Dictio-nary, 2014; Merriam Webster Dictionary, 2014).1as defined in the Merriam Webster Dictionary(2014), http://www.merriam-webster.com/dictionary/ironyThe German Duden (2014) mentions sarcasm assynonym to irony, while the comprehension of sar-casm as a special case of irony might be morecommon.
For instance, the Merriam Webster Dic-tionary (2014) defines sarcasm as ?a sharp andoften satirical or ironic utterance designed to cut orgive pain?.2Irony is a frequent phenomenon within humancommunication, occurring both in spoken and writ-ten discourse including books, websites, fora, chats,Twitter messages, Facebook posts, news articlesand product reviews.
Even for humans it is some-times difficult to recognize irony.
Irony markersare thus often used in human communication, sup-porting the correct interpretation (Attardo, 2000).The automatic identification of ironic formulationsin written text is a very challenging as well as im-portant task as shown by the comment3?Read the book!
?which in the context of a movie review could beregarded as ironic and as conveying the fact that thefilm was far worse compared to the book.
Anotherexample is taken from a review for the book ?GreatExpectations?
by Charles Dickens:4?i would recomend this book to friendswho have insomnia or those who i abso-lutely despise.
?The standard approach of recommending X impliesthat X is worthwhile is clearly not valid in the givencontext as the author is stating that she disliked thebook.2http://www.merriam-webster.com/dictionary/sarcasm, accessed April 28, 20143Example from Lee (2009).4http://www.amazon.com/review/R86RAMEBZSB11, access date March 10, 201442In real world applications of sentiment analysis,large data sets are automatically classified into pos-itive statements or negative statements and suchoutput is used to generate summaries of the sen-timent about a product.
In order to increase theaccurateness of such systems, ironic or sarcasticstatements need to be identified in order to inferthe actual communicative intention of the author.In this paper, we are concerned with approachesfor the automatic detection of irony in texts, whichis an important task in a variety of applications,including the automatic interpretation of text-basedchats, computer interaction or sentiment analysisand opinion mining.
In the latter case, the detec-tion is of outmost importance in order to correctlyassign a polarity score to an aspect of a reviewedproduct or a person mentioned in a Twitter mes-sage.
In addition, the automatic detection of ironyor sarcasm in text requires an operational definitionand has therefore the potential to contribute to adeeper understanding of the linguistic propertiesof irony and sarcasm as linguistic phenomena andtheir corpus based evaluation and verification.The rest of this paper is structured as follows:We introduce the background and theories on ironyin Section 1.1 and discuss previous work in the areaof automatically recognizing irony in Section 1.2.In the methods part in Section 2, we present ourset of features (Section 2.1) and the classifiers wetake into account (Section 2.2).
In Section 3, wediscuss the data set used in this work in more detail(Section 3.1), present our experimental setting (Sec-tion 3.2) and show the evaluation of our approach(Section 3.3).
We conclude with a discussion andsummary (Section 4) and with an outlook on possi-ble future work (Section 5).1.1 BackgroundIrony is an important and frequent device in humancommunication that is used to convey an attitudeor evaluation towards the propositional content of amessage, typically in a humorous fashion (Abrams,1957, p. 165?168).
Between the age of six (Nakas-sis and Snedeker, 2002) and eight years (Creusere,2007), children are able to recognize ironic utter-ances or at least notice that something in the sit-uation is not common (Glenwright and Pexman,2007).
The principle of inferability (Kreuz, 1996)states that figurative language is used if the speakeris confident that the addressee will interpret theutterance and infer the communicative intentionof the speaker/author correctly.
It has been shownthat irony is ubiquitous, with 8 % of the utterancesexchanged between interlocutors that are familiarwith each other being ironic (Gibbs, 2007).Utsumi (1996) claim that an ironic utterance canonly occur in an ironic environment, whose pres-ence the utterance implicitly communicates.
Giventhe formal definition it is possible to computation-ally resolve if an utterance is ironic using first-orderpredicate logic and situation calculus.
Different the-ories such as the echoic account (Wilson and Sper-ber, 1992), the Pretense Theory (Clark and Gerrig,1984) or the Allusional Pretense Theory (Kumon-Nakamura et al., 1995) have challenged the un-derstanding that an ironic utterance typically con-veys the opposite of its literal propositional content.However, in spite of the fact that the attributivenature of irony is widely accepted (see Wilson andSperber (2012)), no formal or operational definitionof irony is available as of today.1.2 Previous WorkCorpora providing annotations as to whether ex-pressions are ironic or not are scarce.
Kreuz andCaucci (2007) have automatically generated sucha corpus exploiting Google Book search5.
Theycollected excerpts containing the phrase ?said sar-castically?, removed that phrase and performed aregression analysis on the remaining text, exploit-ing the number of words as well as the occurrenceof adjectives, adverbs, interjections, exclamationand question marks as features.Tsur et al.
(2010) present a system to identifysarcasm in Amazon product reviews exploiting fea-tures such as sentence length, punctuation marks,the total number of completely capitalized wordsand automatically generated patterns which arebased on the occurrence frequency of differentterms (following the approach by Davidov andRappoport (2006)).
Unfortunately, their corpusis not publicly available.
Carvalho et al.
(2009) useeight patterns to identify ironic utterances in com-ments on articles from a Portuguese online newspa-per.
These patterns contain positive predicates andutilize punctuation, interjections, positive words,emoticons, or onomatopoeia and acronyms forlaughing as well as some Portuguese-specific pat-terns considering the verb-morphology.
Gonz?alez-Ib?a?nez et al.
(2011) differentiate between sarcasticand positive or negative Twitter messages.
They5http://books.google.de/43exploit lexical features like unigrams, punctuation,interjections and dictionary-based as well as prag-matic features including references to other usersin addition to emoticons.
Reyes et al.
(2012) distin-guish ironic and non-ironic Twitter messages basedon features at different levels of linguistic analysisincluding quantifiers of sentence complexity, struc-tural, morphosyntactic and semantic ambiguity, po-larity, unexpectedness, and emotional activation,imagery, and pleasantness of words.
Teppermanet al.
(2006) performed experiments to recognizesarcasm in spoken language, specifically in the ex-pression ?yeah right?, using spectral, contextualand prosodic cues.
On the one hand, their resultsshow that it is possible to identify sarcasm based onspectral and contextual features and, on the otherhand, they confirm that prosody is insufficient toreliably detect sarcasm (Rockwell, 2005, p. 118).Very recently, Filatova (2012) published a prod-uct review corpus from Amazon, being annotatedwith Amazon Mechanical Turk.
It contains 437ironic and 817 non-ironic reviews.
A more de-tailed description of this resource can be found inSection 3.1.
To our knowledge, no automatic classi-fication approach has been evaluated on this corpus.We therefore contribute a text classification systemincluding the previously mentioned features.
Ourresults serve as a strong baseline on this corpus aswell as an ?executable review?
of previous work.62 MethodsWe model the task of irony detection as a super-vised classification problem in which a review iscategorized as being ironic or non-ironic.
We inves-tigate different classifiers and focus on the impactanalysis of different features by investigating whateffect their elimination has on the performance ofthe approach.
In the following, we describe thefeatures used and the set of classifiers compared.2.1 FeaturesTo estimate if a review is ironic or not, we measurea set of features.
Following the idea that irony isexpressing the opposite of its literal content, wetake into account the imbalance between the over-all (prior) polarity of words in the review and thestar-rating (as proposed by Davidov et al.
(2010)).We assume the imbalance to hold if the star-rating6The system as implemented to perform the describedexperiments is made available at https://github.com/kbuschme/irony-detection/is positive (i. e., 4 or 5 stars) but the majority ofwords is negative, and, vice versa, if the star-ratingis negative (i. e., 1 or 2 stars) but occurs with amajority of positive words.
We refer to this featureas Imbalance.
The polarity of words is determinedbased on a dictionary consisting of about 6,800words with their polarity (Hu and Liu, 2004).7The feature Hyperbole (Gibbs, 2007) indicatesthe occurrence of a sequence of three positive ornegative words in a row.
Similarly, the featureQuotes indicates that up to two consecutive adjec-tives or nouns in quotation marks have a positiveor negative polarity.The feature Pos/Neg&Punctuation indicates thata span of up to four words contains at least onepositive (negative) but no negative (positive) wordand ends with at least two exclamation marks or asequence of a question mark and an exclamationmark (Carvalho et al., 2009).
Analogously, the fea-ture Pos/Neg&Ellipsis indicates that such a positiveor negative span ends with an ellipsis (?.
.
.
?).
El-lipsis and Punctuation indicates that an ellipsis isfollowed by multiple exclamation marks or a com-bination of an exclamation and a question mark.The Punctuation feature conveys the presence ofan ellipses as well as multiple question or excla-mation marks or a combination of the latter two.The Interjection feature indicates the occurrence ofterms like ?wow?
and ?huh?, and Laughter mea-sures onomatopoeia (?haha?)
as well as acronymsfor grin or laughter (?
*g*?, ?lol?).
In addition, thefeature Emoticon indicates the occurrence of anemoticon.
In order to capture a range of emotions,it combines a variety of emoticons such as happy,laughing, winking, surprised, dissatisfied, sad, cry-ing, and sticking tongue out.
In addition, we useeach occurring word as a feature (bag-of-words).All together, we have 21,773 features.
The num-ber of specific features (i. e., without bag-of-words)alone is 29.2.2 ClassifiersIn order to perform the classification based on thefeatures mentioned above, we explore a set of stan-dard classifiers typically used in text classificationresearch.
We employ the open source machinelearning library scikit-learn (Pedregosa et al., 2011)for Python.7Note that examples can show that this is not always thecase.
Funny or odd products ironically receive a positive star-rating.
However, this feature may be a strong indicator forirony.44We use a support vector machine (SVM, Cortesand Vapnik (1995)) with a linear kernel in the im-plementation provided by libSVM (Fan et al., 2005;Chang and Lin, 2011).
The na?
?ve Bayes classifier isemployed with a multinomial prior (Zhang, 2004;Manning et al., 2008).
This classifier might sufferfrom the issue of over-counting correlated features,such that we compare it to the logistic regressionclassifier as well (Yu et al., 2011).Finally, we use a decision tree (Breiman et al.,1984; Hastie et al., 2009) and a random forest clas-sifier (Breiman, 2001).3 Experiments and Results3.1 Data SetThe data set by Filatova (2012) consists of 1,254Amazon reviews, of which 437 are ironic, i. e.,contain situational irony or verbal irony, and817 are non-ironic.
It has been acquired usingthe crowd sourcing platform Amazon MechanicalTurk8.
Note that Filatova (2012) interprets sarcasmas being verbal irony.In a first step, the workers were asked to findpairs of reviews on the same product so that oneof the reviews is ironic while the other one is not.They were then asked to submit the ID of bothreviews, and, in the case of an ironic review, toprovide the fragment conveying the irony.In a second step, each collected review was an-notated by five additional workers and remainedin the corpus if three of the five new annotatorsconcurred with the initial category, i. e., ironic ornon-ironic.
The corpus contains 21,744 distincttokens9, of which 5,336 occur exclusively in ironicreviews, 9,468 exclusively in non-ironic reviews,and the remaining 6,940 tokens occur in both ironicand non-ironic reviews.
Thus, all ironic reviewscomprise a total of 12,276 distinct tokens, whereasa total of 16,408 distinct tokens constitute all non-ironic reviews.
On average, a single review consistsof 271.9 tokens, a single ironic review of an aver-age of 261.4 and a single non-ironic review of anaverage of 277.5 tokens.
The distribution of ironicand non-ironic reviews for the different star-ratingsis shown in Table 2.
Note that this might be a resultof the specific annotation procedure applied by the8https://www.mturk.com/mturk/, accessed onMarch 10, 20149Using the TreeBankWordTokenizer as implemented in theNatural Language Toolkit (NLTK) (http://www.nltk.org/)annotators to search for ironic reviews.
Neverthe-less, this motivates a simple baseline system whichjust takes one feature into account: the numbers ofstars assigned to the respective review (?Star-ratingonly?
).3.2 Experimental SettingsWe run experiments for three baselines: The star-rating baseline relies only on the number of starsassigned in the review as a feature.
The bag-of-words baseline exploits only the unigrams in thetext as features.
The sentiment word count onlyuses the information whether the number of posi-tive words in the text is larger than the number ofnegative words.We emphasize that the first baseline is only oflimited applicability as it requires the explicit avail-ability of a star-rating.
The second baseline relieson standard text classification features that are notspecific for the task.
The third baseline relies on aclassical feature used in sentiment analysis, but isnot specific for irony detection.We refer to the feature set ?All?
encompassingall features described in Section 2.1, including bag-of-words and the set ?Specific Features?.In order to understand the impact of a specificfeature A, we run three sets of experiments:?
Using all features with the exception of A.?
Using all specific features with the exceptionof A.?
Using A as the only feature.In addition to evaluating each single feature asdescribed above, we evaluate the set of positive andnegative instantiations of features when using thesentiment dictionary.
The ?Positive set?
and ?Neg-ative set?
take into account the respective subsetsof all specific features.Each experiment is performed in a 10-fold cross-validation setting on document level.
We reportrecall, precision and F1-measure for each of theclassifiers.3.3 EvaluationTable 1 shows the results for the three baselines anddifferent feature set combinations, all for the differ-ent classifiers.
The star-rating as a feature alone is avery strong indicator for irony.
However, this resultis of limited usefulness as it only regards reviewsof a specific rating as ironic, namely results with45Linear SVM Logistic Regression Decision Tree Random Forest Naive BayesFeature set R. P. F1R.
P. F1R.
P. F1R.
P. F1R.
P. F1Star-rating only 66.7 78.4 71.7 66.7 78.4 71.7 66.7 78.4 71.7 66.7 78.4 71.7 66.7 78.4 71.7BOW only 61.8 67.2 64.1 63.3 76.0 68.8 53.8 53.4 53.4 21.7 70.4 32.9 48.1 77.4 59.1Sentiment Word Count 57.3 59.4 58.1 57.3 59.4 58.1 57.3 59.4 58.1 57.3 59.4 58.1 0.0 100.0 0.0All + Star-rating 69.0 74.4 71.3 68.9 81.7 74.4 71.7 73.2 72.2 34.0 85.0 48.2 55.3 79.7 65.0All (= Sp.
Features + BOW) 61.3 68.0 64.3 62.2 75.2 67.8 55.0 59.8 56.9 24.1 73.2 35.3 50.9 77.3 61.2All ?
Imbalance 62.4 67.1 64.4 62.5 75.0 67.9 53.0 54.3 53.3 22.3 75.9 33.8 47.8 75.8 58.4All ?
Hyperbole 61.3 68.0 64.3 62.2 75.2 67.8 57.1 61.5 58.9 22.3 79.6 34.4 50.9 77.3 61.2All ?
Quotes 61.3 68.0 64.3 62.8 75.1 68.2 57.2 61.7 59.1 25.9 76.8 38.5 50.6 77.0 60.9All ?
Pos/Neg&Punctuation 61.5 67.9 64.4 62.4 75.2 68.0 56.7 60.1 58.0 21.8 77.8 33.5 50.9 77.3 61.2All ?
Pos/Neg&Ellipsis 61.0 67.4 63.8 63.0 75.1 68.3 57.6 60.5 58.8 29.0 79.2 42.2 50.4 76.6 60.7All ?
Ellipsis and Punctuation 61.3 68.0 64.3 62.4 75.2 68.0 55.1 59.7 56.9 24.6 73.6 36.2 50.9 77.3 61.2All ?
Punctuation 61.8 67.9 64.5 62.5 74.9 67.8 56.1 61.2 58.3 28.6 78.1 41.5 50.2 76.7 60.6All ?
Injections 61.3 68.0 64.3 62.2 75.0 67.8 56.1 61.8 58.5 24.1 75.2 35.6 50.9 77.3 61.2All ?
Laughter 61.3 68.2 64.4 62.4 75.3 68.0 56.6 60.9 58.2 24.0 79.3 36.5 50.9 77.3 61.2All ?
Emoticons 61.3 68.2 64.4 62.6 75.3 68.1 57.7 60.2 58.6 24.3 76.5 36.7 50.9 77.3 61.2All ?
Negative set 61.0 68.0 64.1 62.3 74.7 67.7 59.0 61.1 59.7 25.4 76.8 37.6 50.2 76.6 60.5All ?
Positive set 62.6 67.3 64.6 62.5 75.7 68.2 53.7 55.1 54.2 20.5 67.7 31.1 47.8 75.8 58.4Sp.
Features 37.5 77.2 50.2 38.2 77.5 50.8 38.3 76.0 50.6 38.3 74.8 50.2 34.3 80.5 47.7Sp.
Features ?
Imbalance 9.3 50.4 15.4 11.0 54.1 18.1 11.3 48.5 18.1 12.9 47.4 20.0 5.9 55.8 10.3Sp.
Features ?
Hyperbole 37.5 77.4 50.3 38.2 77.5 50.8 38.3 76.7 50.7 38.8 76.4 51.2 34.3 80.9 47.8Sp.
Features ?
Quotes 37.7 76.9 50.3 38.0 78.1 50.7 37.8 75.6 50.1 38.3 73.6 50.0 34.3 80.5 47.7Sp.
Features ?
Pos/Neg&Punctuation 37.7 77.9 50.5 37.8 77.6 50.5 37.1 74.5 49.2 38.2 73.8 49.9 33.3 80.2 46.7Sp.
Features ?
Pos/Neg&Ellipsis 37.7 77.3 50.4 38.1 78.2 50.9 37.9 76.2 50.4 39.1 72.3 50.3 34.5 79.7 47.8Sp.
Features ?
Ellipsis and Punctuation 37.8 76.9 50.3 37.8 76.9 50.3 38.3 75.8 50.6 39.0 72.5 50.5 34.5 80.2 47.9Sp.
Features ?
Punctuation 37.1 79.7 50.3 37.6 78.7 50.6 37.0 76.7 49.6 38.4 75.4 50.5 32.6 78.9 45.6Sp.
Features ?
Interjections 37.7 76.9 50.3 37.9 77.5 50.6 38.1 76.1 50.4 38.7 75.2 50.7 34.3 80.5 47.7Sp.
Features ?
Laughter 37.8 77.3 50.5 38.0 77.7 50.7 37.3 75.5 49.6 37.5 73.4 49.4 34.5 81.2 48.0Sp.
Features ?
Emoticons 37.3 78.2 50.2 38.2 77.5 50.8 38.0 75.4 50.2 38.7 75.0 50.7 33.4 80.7 46.8Sp.
Features ?
Positive set 10.5 48.7 17.1 11.0 56.3 18.1 9.9 49.3 16.3 12.3 50.8 19.5 6.3 64.8 11.0Sp.
Features ?
Negative set 37.7 78.2 50.6 38.0 78.7 50.9 38.2 75.1 50.3 37.6 72.0 48.9 34.9 79.8 48.3Imbalance only 36.9 81.4 50.4 36.9 81.4 50.4 36.9 81.4 50.4 36.9 81.4 50.4 0.0 100.0 0.0Hyperbole only 0.0 80.0 0.0 0.0 90.0 0.0 0.0 80.0 0.0 0.2 55.0 0.4 0.0 100.0 0.0Quotes only 3.9 45.5 7.0 0.9 67.0 1.7 4.0 43.8 7.0 2.5 52.2 4.5 0.0 100.0 0.0Pos/Neg&Punctuation only 0.9 90.0 1.8 0.5 90.0 0.9 0.0 90.0 0.0 0.4 90.0 0.8 0.9 90.0 1.8Pos/Neg&Ellipsis only 6.8 59.0 12.1 6.8 59.0 12.1 6.8 59.0 12.1 6.8 59.0 12.1 0.0 100.0 0.0Ellipsis and Punctuation only 0.9 90.0 1.7 0.4 90.0 0.8 0.9 90.0 1.7 0.9 90.0 1.7 0.0 100.0 0.0Punctuation only 5.4 64.6 9.8 5.4 64.6 9.8 3.3 60.8 6.2 4.0 60.8 7.5 4.7 64.6 8.6Interjections only 0.5 75.8 0.9 0.3 82.5 0.5 0.5 75.8 0.9 1.4 74.2 2.7 0.0 100.0 0.0Laughter only 0.0 100.0 0.0 0.0 100.0 0.0 0.0 100.0 0.0 0.0 80.0 0.0 0.0 100.0 0.0Emoticons only 0.0 100.0 0.0 0.0 100.0 0.0 0.0 100.0 0.0 0.0 100.0 0.0 0.0 80.0 0.0Positive set only 36.9 81.4 50.4 36.9 81.1 50.4 37.1 80.5 50.5 37.3 79.3 50.5 32.4 80.7 45.6Negative set only 8.2 54.5 14.1 7.3 48.8 12.5 8.8 49.4 14.8 9.0 49.9 15.2 0.0 80.0 0.0Table 1: Comparison of different classification methods using different feature sets.
?All?
refers to thefeatures described in Section 2 including bag-of-words (?BOW?).
?Sp.
Features?
are ?All?
without?BOW?.a positive rating by the author, as explained by Ta-ble 2, which shows the more real-world compatibleresult of a rich feature set in addition.
Obviously,the depicted distribution is very similar to the dis-tribution of the manually annotated data set, whichcan obviously not be achieved by the star-ratingfeature alone.The best result is achieved by using the star-rating together with bag-of-words and specific fea-tures with a logistic regression approach (leadingto an F1-measure of 74 %).
The SVM and decisiontree have a comparable performance on the task,which is albeit lower compared to the performanceof the logistic regression approach.Using the task-agnostic pure bag-of-words ap-proach leads to a performance of 68.8 % for logisticregression; this classifier has the property of deal-ing well with correlated features and the additionalspecific features cannot contribute positively to theresult.
Similarly, the F1-measure of 64.1 % pro-duced by the SVM cannot be increased by includ-ing additional features.
In contrast, a positive im-pact of additional features can be observed for thedecision tree in the case that specific features arecombined with bag-of-word-based features, reach-ing close to 59 % F1in comparison to 53.4 % F1for bag-of-words alone.It would be desirable to have a model only ormainly based on the problem-specific features, asthis leads to a much more compact and therefore ef-46ficient representation than taking all words into ac-count.
In addition, the model would be easier to un-derstand.
By exploiting task-specific features alone,the performance reaches at most an F1-measure of50.9 %, which shows that task-agnostic featuressuch as unigram features are needed.
A significantdrop in performance when leaving out a featureor feature set can be observed for the Imbalancefeature and the Positive set.
Both these feature setstake into account the star-rating.The task-specific features alone yield high preci-sion results at the expense of a very low recall.
Thisclearly shows that task-specific features shouldbe used with standard, task-independent features(the bag-of-words).
The most helpful task-specificfeatures are: Imbalance, Positive set, Quotes andPos/Neg&Ellipses.4 Discussion and SummaryThe best performance is achieved with very corpus-specific features taking into account meta-datafrom Amazon, namely the product rating of thereviewer.
This leads to an F1-measure of 74 %.However, we could not show a competitive perfor-mance with more problem-specific features (lead-ing to 51 % F1) or in combination with bag-of-word-based features (leading to 68 % F1).The baseline only predicting based on the star-rating itself is highly competitive, however, notapplicable to texts without meta-data and of lim-ited use due to its naturally highly biased outcometowards positive reviews being non-ironic and neg-ative reviews being ironic.
Our results show thatthe best results are achieved via meta-data and it re-mains an open research task to develop comparablygood approaches only based on text features.It should be noted that the corpus used in thisDistributionCorpus PredictedRating ironic non-ironic ironic non-ironic5 114 605 126 5934 14 96 17 933 20 35 14 412 27 17 17 271 262 64 192 1341?5 437 817 366 888Table 2: Frequencies for the different star-ratingsof a review, as annotated, and according to thelogistic regression classifier with the feature set?All ?
Imbalance?.work is not a random sample from all reviews avail-able in a specific group of products.
We actuallyassume ironic reviews to be much more sparsewhen sampling equally distributed.
The evaluationshould be seen from the angle of the applicationscenario: For instance, in a discovery setting inwhich the task is to retrieve examples for ironicreviews, a highly precise system would be desir-able.
In a setting in which only a small numberof reviews should be used for opinion mining, thepolarity of a text would be discovered taking theclassifier?s result into account ?
therefore a sys-tem with high precision and high recall would beneeded.5 Future WorkAs discussed at the end of the last section, a studyon the distribution of irony in the entirety of avail-able reviews is needed to better shape the structureand characteristics of an irony or sarcasm detectionsystem.
This could be approached by perform-ing a random sample from reviews and annotation,though this would lead to a substantial amount ofannotation work in comparison to the directed se-lection procedure used in the corpus by Filatova(2012).Future research should focus on the developmentof approaches analyzing the vocabulary used in thereview in a deeper fashion.
Our impression is thatmany sarcastic and ironic reviews use words andphrases which are non-typical for the specific do-main or product class.
Such out-of-domain vocabu-lary can be detected with text similarity approaches.Preliminary experiments taking into account the av-erage cosine similarity of a review to be classifiedto a large set of reviews from the same product classhave been of limited success.
We propose that fu-ture research should focus on analyzing the specificvocabulary and develop semantic similarity mea-sures which we assume to be more promising thanapproaches taking into account lexical approachesonly.Most work has been performed on text sets fromone source like Twitter, books, reviews, etc.
Someof the proposed features mentioned in this paperor previous publications are probably transferablebetween text sources.
However, this still needsto be proven and further development might benecessary to actually provide automated domainadaption for the area of irony and sarcasm detection.We assume that not only the vocabulary changes47(as known in other domain adaptation tasks) butactually the linguistic structure might change.Finally, it should be noted that the corpus is actu-ally a mixture of ironic and sarcastic reviews.
Ironyand sarcasm are not fully exchangeable and can beassumed to have different properties.
Further inves-tigations and analyses regarding the characteristicsthat can be transferred are necessary.AcknowledgementsRoman Klinger has been funded by the ?It?sOWL?
project (?Intelligent Technical SystemsOstwestfalen-Lippe?, http://www.its-owl.de/), a leading-edge cluster of the German Min-istry of Education and Research.
We thank thereviewers for their valuable comments.
We thankChristina Unger for proof-reading the manuscriptand helpful comments.ReferencesMeyer Howard Abrams.
1957.
A Glossary of LiteraryTerms.
Cengage Learning Emea, 9th edition.Salvatore Attardo.
2000.
Irony markers and functions:Towards a goal-oriented theory of irony and its pro-cessing.
Rask: Internationalt Tidsskrift for Sprog ogKommunikation, 12:3?20.Leo Breiman, Jerome H. Friedman, Richard A. Olshen,and Charles J.
Stone.
1984.
Classification and Re-gression Trees.
Wadsworth, Belmont, California.Leo Breiman.
2001.
Random forests.
Machine Learn-ing, 45(1):5?32.British Dictionary.
2014.
MacMillan Publishers.
On-line: http://www.macmillandictionary.com/dictionary/british/irony.
ac-cessed April 28, 2014.Paula Carvalho, Lu?
?s Sarmento, M?ario J. Silva, andEug?enio de Oliveira.
2009.
Clues for detectingirony in user-generated contents: oh.
.
.
!!
it?s ?soeasy?
;-).
In Proceedings of the 1st internationalCIKM workshop on Topic-sentiment analysis formass opinion, TSA ?09, pages 53?56, New York,NY, USA.
ACM.Chih-Chung Chang and Chih-Jen Lin.
2011.
Libsvm:A library for support vector machines.
ACM Trans-actions on Intelligent Systems and Technology, 2(3).Herbert H. Clark and Richard J. Gerrig.
1984.
On thepretense theory of irony.
Journal of ExperimentalPsychology: General, 113(1):121?126.Corinna Cortes and Vladimir Vapnik.
1995.
Support-vector networks.
Machine Learning, 20(3):273?297.Marlena A. Creusere.
2007.
A developmental testof theoretical perspective on the understanding ofverbal irony: Children?s recognition of allusion andpragmatic insincerity.
In Raymond W. Jr. Gibbs andHerbert L. Colston, editors, Irony in Language andThought: A Cognitive Science Reader, chapter 18,pages 409?424.
Lawrence Erlbaum Associates, 1stedition.Dmitry Davidov and Ari Rappoport.
2006.
Efficientunsupervised discovery of word categories usingsymmetric patterns and high frequency words.
InProceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguistics,pages 297?304, Sydney, Australia, July.
Associationfor Computational Linguistics.Dmitry Davidov, Oren Tsur, and Ari Rappoport.
2010.Semi-supervised recognition of sarcastic sentencesin twitter and amazon.
In Proceedings of theFourteenth Conference on Computational NaturalLanguage Learning, CoNLL ?10, pages 107?116,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Duden.
2014.
Duden Verlag.
Online: http://www.duden.de/rechtschreibung/Ironie.
ac-cessed April 28, 2014.Rong-En Fan, Pai-Hsuen Chen, and Chih-Jen Lin.2005.
Working set selection using second orderinformation for training support vector machines.Jounral of Machine Learning Reasearch, 6:1889?1918.Elena Filatova.
2012.
Irony and sarcasm: Corpus gen-eration and analysis using crowdsourcing.
In Nico-letta Calzolari, Khalid Choukri, Thierry Declerck,Mehmet U?gur Do?gan, Bente Maegaard, Joseph Mar-iani, Jan Odijk, and Stelios Piperidis, editors, Pro-ceedings of the Eighth International Conference onLanguage Resources and Evaluation (LREC-2012),pages 392?398, Istanbul, Turkey, May.
EuropeanLanguage Resources Association (ELRA).Raymond W. Jr. Gibbs.
2007.
Irony in talk amongfriends.
In Raymond W. Jr. Gibbs and Herbert L.Colston, editors, Irony in Language and Thought:A Cognitive Science Reader, chapter 15, pages339?360.
Lawrence Erlbaum Associates, 1st edition,May.Melanie Harris Glenwright and Penny M. Pexman.2007.
Children?s perceptions of the social func-tions of verbal irony.
In Raymond W. Jr. Gibbs andHerbert L. Colston, editors, Irony in Language andThought: A Cognitive Science Reader, chapter 20,pages 447?464.
Lawrence Erlbaum Associates, 1stedition.Roberto Gonz?alez-Ib?a?nez, Smaranda Muresan, andNina Wacholder.
2011.
Identifying sarcasm in twit-ter: a closer look.
In Proceedings of the 49th Annual48Meeting of the Association for Computational Lin-guistics: Human Language Technologies: short pa-pers - Volume 2, HLT ?11, pages 581?586, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Trevor Hastie, Robert Tibshirani, and Jerome H. Fried-man.
2009.
Elements of Statistical Learning.Springer, 2nd edition.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the tenthACM SIGKDD international conference on Knowl-edge discovery and data mining, KDD ?04, pages168?177, New York, NY, USA.
ACM.Roger J. Kreuz and Gina M. Caucci.
2007.
Lexicalinfluences on the perception of sarcasm.
In Proceed-ings of the Workshop on Computational Approachesto Figurative Language, pages 1?4, Rochester, NewYork, April.
Association for Computational Linguis-tics.Roger J. Kreuz.
1996.
The use of verbal irony:Cues and constraints.
In Jeffery S. Mio and Al-bert N. Katz, editors, Metaphor: Implications andApplications, pages 23?38, Mahwah, NJ, October.Lawrence Erlbaum Associates.Sachi Kumon-Nakamura, Sam Glucksberg, and MaryBrown.
1995.
How about another piece of pie: Theallusional pretense theory of discourse irony.
Jour-nal of Experimental Psychology: General, 124(1):3?21, Mar 01.
Last updated - 2013-02-23.Lillian Lee.
2009.
A tempest or, on the flood of inter-est in: sentiment analysis, opinion mining, and thecomputational treatment of subjective language.
Tu-torial at ICWSM, May.Christopher D. Manning, Prabhakar Raghavan, andHinrich Sch?utze.
2008.
Introduction to InformationRetrieval.
Cambridge University Press.Merriam Webster Dictionary.
2014.
Merriam WebsterInc.
Online: www.merriam-webster.com/dictionary/irony.
accessed April 28, 2014.Constantine Nakassis and Jesse Snedeker.
2002.
Be-yond sarcasm: Intonation and context as relationalcues in children?s recognition of irony.
In A. Green-hill, M. Hughs, H. Littlefield, and H. Walsh, editors,Proceedings of the Twenty-sixth Boston UniversityConference on Language Development, Somerville,MA, July.
Cascadilla Press.New Oxford American Dictionary.
2014.
Ox-ford University Press.
Online: http://www.oxforddictionaries.com/us/definition/american_english/ironic.accessed April 28, 2014.Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-fort, Vincent Michel, Bertrand Thirion, OlivierGrisel, Mathieu Blondel, Peter Prettenhofer, RonWeiss, Vincent Dubourg, Jake Vanderplas, Alexan-dre Passos, David Cournapeau, Matthieu Brucher,Matthieu Perrot, and?Edouard Duchesnay.
2011.Scikit-learn: Machine learning in Python.
Journalof Machine Learning Research, 12:2825?2830.Antonio Reyes, Paolo Rosso, and Davide Buscaldi.2012.
From humor recognition to irony detection:The figurative language of social media.
Data &Knowledge Engineering, 74:1?12.Patricia Rockwell.
2005.
Sarcasm on television talkshows: Determining speaker intent through verbaland nonverbal cues.
In Anita V. Clark, editor, Psy-chology of Moods, chapter 6, pages 109?122.
NovaScience Pubishers Inc.Joseph Tepperman, David Traum, and Shrikanth S.Narayanan.
2006.
?yeah right?
: Sarcasm recogni-tion for spoken dialogue systems.
In Proceedingsof InterSpeech, pages 1838?1841, Pittsburgh, PA,September.Oren Tsur, Dmitry Davidov, and Ari Rappoport.
2010.ICWSM ?
A Great Catchy Name: Semi-SupervisedRecognition of Sarcastic Sentences in Online Prod-uct Reviews.
In Proceedings of the Fourth Interna-tional AAAI Conference on Weblogs and Social Me-dia, pages 162?169.
The AAAI Press.Akira Utsumi.
1996.
A unified theory of irony and itscomputational formalization.
In Proceedings of the16th conference on Computational linguistics - Vol-ume 2, COLING ?96, pages 962?967, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Deirdre Wilson and Dan Sperber.
1992.
On verbalirony.
Lingua, 87:53?76.Deirdre Wilson and Dan Sperber, 2012.
ExplainingIrony, chapter 6, pages 123?145.
Cambridge Uni-versity Press, 1st edition, April.Hsiang-Fu.
Yu, Fang-Lan Huang, and Chih-Jen Lin.2011.
Dual coordinate descent methods for logisticregression and maximum entropy.
Machine Learn-ing, 85(1?2):41?75, October.Harry Zhang.
2004.
The optimality of naive bayes.
InValerie Barr and Zdravko Markov, editors, Proceed-ings of the Seventeenth International Florida Artifi-cial Intelligence Research Society (FLAIRS) Confer-ence, pages 3?9.
AAAI Press.49
