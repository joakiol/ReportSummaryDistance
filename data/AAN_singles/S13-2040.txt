Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 222?231, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsSemEval-2013 Task 12: Multilingual Word Sense DisambiguationRoberto Navigli, David Jurgens and Daniele VannellaDipartimento di InformaticaSapienza Universita` di RomaViale Regina Elena, 295 ?
00161 Roma Italy{navigli,jurgens,vannella}@di.uniroma1.itAbstractThis paper presents the SemEval-2013 task onmultilingual Word Sense Disambiguation.
Wedescribe our experience in producing a mul-tilingual sense-annotated corpus for the task.The corpus is tagged with BabelNet 1.1.1,a freely-available multilingual encyclopedicdictionary and, as a byproduct, WordNet 3.0and the Wikipedia sense inventory.
We presentand analyze the results of participating sys-tems, and discuss future directions.1 IntroductionWord Sense Disambiguation (WSD), the task of au-tomatically assigning predefined meanings to wordsoccurring in context, is a fundamental task in com-putational lexical semantics (Navigli, 2009; Navigli,2012).
Several Senseval and SemEval tasks havebeen organized in the past to study the performanceand limits of disambiguation systems and, evenmore importantly, disambiguation settings.
Whilean ad-hoc sense inventory was originally chosen forthe first Senseval edition (Kilgarriff, 1998; Kilgarriffand Palmer, 2000), later tasks (Edmonds and Cot-ton, 2001; Snyder and Palmer, 2004; Mihalcea etal., 2004) focused on WordNet (Miller et al 1990;Fellbaum, 1998) as a sense inventory.
In 2007 theissue of the fine sense granularity of WordNet wasaddressed in two different SemEval disambiguationtasks, leading to the beneficial creation of coarser-grained sense inventories from WordNet itself (Nav-igli et al 2007) and from OntoNotes (Pradhan et al2007).In recent years, with the exponential growth ofthe Web and, consequently, the increase of non-English speaking surfers, we have witnessed an up-surge of interest in multilinguality.
SemEval-2010tasks on cross-lingual Word Sense Disambiguation(Lefever and Hoste, 2010) and cross-lingual lexi-cal substitution (Mihalcea et al 2010) were orga-nized.
While these tasks addressed the multilin-gual aspect of sense-level text understanding, theydeparted from the traditional WSD paradigm, i.e.,the automatic assignment of senses from an existinginventory, and instead focused on lexical substitu-tion (McCarthy and Navigli, 2009).
The main factorhampering traditional WSD from going multilingualwas the lack of a freely-available large-scale multi-lingual dictionary.The recent availability of huge collaboratively-built repositories of knowledge such as Wikipediahas enabled the automated creation of large-scalelexical knowledge resources (Hovy et al 2013).Over the past few years, a wide-coverage multi-lingual ?encyclopedic?
dictionary, called BabelNet,has been developed (Navigli and Ponzetto, 2012a).BabelNet1 brings together WordNet and Wikipediaand provides a multilingual sense inventory that cur-rently covers 6 languages.
We therefore decided toput the BabelNet 1.1.1 sense inventory to the testand organize a traditional Word Sense Disambigua-tion task on a given English test set translated into 4other languages (namely, French, German, Spanishand Italian).
Not only does BabelNet enable mul-tilinguality, but it also provides coverage for bothlexicographic (e.g., apple as fruit) and encyclopedic1http://babelnet.org222meanings (e.g., Apple Inc. as company).
In this pa-per we describe our task and disambiguation datasetand report on the system results.2 Task SetupThe task required participating systems to annotatenouns in a test corpus with the most appropriatesense from the BabelNet sense inventory or, alter-natively, from two main subsets of it, namely theWordNet or Wikipedia sense inventories.
In contrastto previous all-words WSD tasks we did not focuson the other three open classes (i.e., verbs, adjec-tives and adverbs) since BabelNet does not currentlyprovide non-English coverage for them.2.1 Test CorpusThe test set consisted of 13 articles obtained fromthe datasets available from the 2010, 2011 and 2012editions of the workshop on Statistical MachineTranslation (WSMT).2 The articles cover differentdomains, ranging from sports to financial news.The same article was available in 4 different lan-guages (English, French, German and Spanish).
Inorder to cover Italian, an Italian native speaker man-ually translated each article from English into Ital-ian, with the support of an English mother tongueadvisor.
In Table 1 we show for each language thenumber of words of running text, together with thenumber of multiword expressions and named enti-ties annotated, from the 13 articles.2.2 Sense Inventories2.2.1 BabelNet inventoryTo semantically annotate all the single- and multi-word expressions, as well as the named entities, oc-curring in our test corpus we used BabelNet 1.1.1(Navigli and Ponzetto, 2012a).
BabelNet is a mul-tilingual ?encyclopedic dictionary?
and a semanticnetwork currently covering 6 languages, namely:English, Catalan, French, German, Italian and Span-ish.
BabelNet is obtained as a result of a novel inte-gration and enrichment methodology.
This resourceis created by linking the largest multilingual Web en-cyclopedia ?
i.e., Wikipedia ?
to the most popularcomputational lexicon ?
i.e., WordNet 3.0.
The inte-gration is performed via an automatic mapping and2http://www.statmt.org/wmt12/by filling in lexical gaps in resource-poor languageswith the aid of Machine Translation (MT).Its lexicon includes lemmas which denote bothlexicographic meanings (e.g., balloon) and ency-clopedic ones (e.g., Montgolfier brothers).
Thebasic meaning unit in BabelNet is the Babelsynset, modeled after the WordNet synset (Milleret al 1990; Fellbaum, 1998).
A Babel synsetis a set of synonyms which express a conceptin different languages.
For instance, { Globusaerosta`ticCA, BalloonEN, Ae?rostationFR, BallonDE,Pallone aerostaticoIT, .
.
.
, Globo aerosta?ticoES } isthe Babel synset for the balloon aerostat, where thelanguage of each synonym is provided as a subscriptlabel.
Thanks to their multilingual nature, we wereable to use Babel synsets as interlingual concept tagsfor nouns occurring within text written in any of thecovered languages.2.2.2 WordNet and Wikipedia inventoriesSince BabelNet 1.1.1 is a superset of the Word-Net 3.0 and Wikipedia sense inventories,3 once textis annotated with Babel synsets, it turns out tobe annotated also according to either WordNet orWikipedia, or both.
In fact, in order to induce theWordNet annotations, one can restrict to those lex-ical items annotated with Babel synsets which con-tain WordNet senses for the target lemma; similarly,for Wikipedia, we restrict to those items tagged withBabel synsets which contain Wikipedia pages for thetarget lemma.2.3 BabelNet sense inventory validationBecause BabelNet is an automatic integration ofWordNet and Wikipedia, the resulting Babel synsetsmay contain WordNet and Wikipedia entries aboutdifferent meanings of the same lemma.
The under-lying cause is a wrong mapping between the twooriginal resources.
For instance, in BabelNet 1.1the WordNet synset { arsenic, As, atomic number33 } was mapped to the Wikipedia page AS (RO-MAN COIN), and therefore the same Babel synsetmixed the two meanings.In order to avoid an inconsistent semantic tag-ging of text, we decided to manually check all themappings in BabelNet 1.1 between Wikipedia pages3For version 1.1.1 we used the English Wikipedia databasedump from October 1, 2012.223Language Instances Single- Multiword Named Mean senses Mean senseswords expressions Entities per instance per lemmaBabelNetEnglish 1931 1604 127 200 1.02 1.09French 1656 1389 89 176 1.05 1.15German 1467 1267 21 176 1.00 1.05Italian 1706 1454 211 41 1.22 1.27Spanish 1481 1103 129 249 1.15 1.19WikipediaEnglish 1242 945 102 195 1.15 1.16French 1039 790 72 175 1.18 1.14German 1156 957 21 176 1.07 1.08Italian 1977 869 85 41 1.20 1.18Spanish 1103 758 107 248 1.11 1.10WordNetEnglish 1644 1502 85 57 1.01 1.10Table 1: Statistics for the sense annotations of the test set.and WordNet senses involving lemmas in our En-glish test set for the task.
Overall, we identified 8306synsets for 978 lemmas to be manually checked.
Werecruited 8 annotators in our research group and as-signed each lemma to two annotators.
Each anno-tator was instructed to check each Babel synset anddetermine whether any of the following three opera-tions was needed:?
Delete a mapping and separate the WordNetsense from the Wikipedia page (like in the ar-senic vs. AS (ROMAN COIN) example above);?
Add a mapping between a WordNet sense and aWikipedia page (formerly available as two sep-arate Babel synsets);?
Merge two Babel synsets which express thesame concept.After disagreement adjudication carried out bythe first author, the number of delete, add and mergeoperations was 493, 203 and 43, respectively, for atotal of 739 operations (i.e., 8.8% of synsets cor-rected).
As a result of our validation of BabelNet1.1, we obtained version 1.1.1, which is currentlyavailable online.2.4 Sense AnnotationTo ensure high quality annotations, the annotationprocess was completed in three phases.
BecauseBabelNet is a superset of both the WordNet andWikipedia sense inventories, all annotators used theBabelNet 1.1.1 sense inventory for their respectivelanguage.
These BabelNet annotations were thenprojected into WordNet and Wikipedia senses.
An-notation was performed by one native speaker eachfor English, French, German and Spanish and, forItalian, by two native speakers who annotated dif-ferent subsets of the corpus.In the first phase, each annotator was instructedto inspect each instance to check that (1) the lemmawas tagged with the correct part of speech, (2) lem-mas were correctly annotated as named entity ormultiword expressions, and (3) the meaning of theinstance?s lemma had an associated sense in Ba-belNet.
Based on these criteria, annotators removeddozens of instances from the original data.In the second phase, each instance in the En-glish dataset was annotated using BabelNet senses.To reduce the time required for annotation in theother languages, the sense annotations for the En-glish dataset were then projected onto the other four224Language Projected Valid Invalidinstances projections projectionsFrench 1016 791 225German 592 373 219Italian 1029 774 255Spanish 911 669 242Table 2: Statistics when using the English sense an-notations to project the correct sense of a lemma inanother language of the sentence-aligned test data.languages using the sense translation API of Babel-Net (Navigli and Ponzetto, 2012d).
The projectionoperated as follows, using the aligned sentences inthe English and non-English texts.
For an instancein the non-English text, all of the senses for that in-stance?s lemma were compared with the sense an-notations in the English sentence.
If any of thatlemma?s senses was used in the English sentence,then that sense was selected for the non-Englishinstance.
The matching procedure operates at thesentence-aligned level because the instances them-selves are not aligned; i.e., different languages havedifferent numbers of instances per sentence, whichare potentially ordered differently due to language-specific construction.
Ultimately, this projection la-beled approximately 50-70% of the instances in theother four languages.
Given the projected senses,the annotators for the other four languages were thenasked to (1) correct the projected sense labels and(2) annotate those still without senses.4 These anno-tations were recorded in text in a stand-off file; nofurther annotation tools were used.The resulting sense projection proved highly use-ful for selecting the correct sense.
Table 2 showsthe number of corrections made by the annotatorsto the projected senses, who changed only 22-37%of the labels.
While simple, the projection methodoffers significant potential for generating good qual-ity sense-annotated data from sentence-aligned mul-tilingual text.In the third phase, an independent annotator re-viewed the labels for the high-frequency lemmas for4During the second phase, annotators were also allowedto add and remove instances that were missed during the firstphase, which resulted in small number of changes.all languages to check for systematic errors and dis-cuss possible changes to the labeling.
This reviewresulted in only a small number of changes to lessthan 5% of the total instances, except for Germanwhich had a slightly higher percentage of changes.Table 1 summarizes the sense annotation statis-tics for the test set.
Annotators were allowed to usemultiple senses in the case of ambiguity, but en-couraged to use a single sense whenever possible.In rare cases, a lemma was annotated with sensesfrom a different lemma.
For example, WordNet doesnot contain a sense for ?card?
that corresponds tothe penalty card meaning (as used in sports suchas football).
In contrast, BabelNet has a sense for?penalty card?
from Wikipedia which, however, isnot mapped to the lemma ?card?.
In such cases,we add both the closest meaning from the originallemma (e.g., the rectangual piece of paper sense inWordNet) and the most suitable sense that may havea different lemma form (e.g., PENALTY CARD).Previous annotation studies have shown that,when a fine-grained sense inventory is used, annota-tors will often label ambiguous instances with multi-ple senses if allowed (Erk and McCarthy, 2009; Jur-gens and Klapaftis, 2013).
Since BabelNet is a com-bination of a fine-grained inventory (WordNet) andcontains additional senses from Wikipedia, we ana-lyzed the average number of BabelNet sense anno-tations per instance, shown in column six of Table 1.Surprisingly, Table 1 suggests that the rate of mul-tiple sense annotation varies significantly betweenlanguages.BabelNet may combine multiple Wikipedia pagesinto a single BabelNet synset.
As a result, whenWikipedia is used as a sense inventory, instances areannotated with all of the Wikipedia pages associatedwith each BabelNet synset.
Indeed, Table 1 shows amarkedly increased multi-sense annotation rate forthree languages when using Wikipedia.As a second analysis, we considered the observedlevel of polysemy for each of the unique lemmas.The last column of Table 1 shows the average num-ber of different senses seen for each lemma acrossthe test sets.
In all languages, often only a singlesense of a lemma was used.
Because the test set isconstructed based on topical documents, infrequentlemmas mostly occurred within a single documentwhere they were used with a consistent interpreta-225tion.
However, we note that in the case of lem-mas that were only seen with a single sense, thissense does not always correspond to the most fre-quent sense as seen in SemCor.3 EvaluationTask 12 uses the standard definitions of precisionand recall for WSD evaluation (see, e.g., (Navigli,2009)).
Precision measures the percentage of thesense assignments provided by the system that areidentical to the gold standard; Recall measures thepercentage of instances that are correctly labeled bythe system.
When a system provides sense labelsfor all instances, precision and recall are equivalent.Systems using BabelNet and WordNet senses arecompared against the Most Frequent Sense (MFS)baseline obtained by using the WordNet most fre-quent sense.
For the Wikipedia sense inventory, weconstructed a pseudo-MFS baseline by selecting (1)the Wikipedia page associated with the highest rank-ing WordNet sense, as ranked by SemCor frequency,or (2) when no synset for a lemma was associ-ated with a WordNet sense, the first Wikipedia pagesorted using BabelNet?s ordering criteria, i.e., lexi-cographic sorting.
We note that, in the second case,this procedure frequently selected the page with thesame name as the lemma itself.
For instance, thefirst sense of Dragon Ball is the cartoon with titleDRAGON BALL, followed by two films (DRAGONBALL (1990 FILM) and DRAGON BALL EVOLU-TION).Systems were scored separately for each sense in-ventory.
We note that because the instances in eachtest set are filtered to include only those that canbe labeled with the respective inventory, both theWikipedia and WordNet test sets are subsets of theinstances in the BabelNet test set.4 Participating SystemsThree teams submitted a total of seven systems forthe task, with at least one participant attemptingall of the sense inventory and language combina-tions.
Six systems participated in the WSD taskwith BabelNet senses; two teams submitted four sys-tems using WordNet senses; and one team submittedthree systems for Wikipedia-based senses.
Notably,all systems used graph-based approaches for sensedisambiguation, either using WordNet or BabelNet?ssynset graphs.
We summarize the teams?
systems asfollows.DAEBAK!
DAEBAK!
submitted one systemcalled PD (Peripheral Diversity) based on BabelNetpath indices from the BabelNet synset graph.
Us-ing a ?5 sentence window around the target word,a graph is constructed for all senses of co-occurringlemmas following the procedure proposed by Nav-igli and Lapata (2010).
The final sense is selectedbased on measuring connectivity to the synsets ofneighboring lemmas.
The MFS is used as a backoffstrategy when no appropriate sense can be pickedout.GETALP GETALP submitted three systems, twofor BabelNet and one for WordNet, all based onthe ant-colony algorithm of (Schwab et al 2012),which uses the sense inventory network structureto identify paths connecting synsets of the targetlemma to the synsets of other lemmas in context.The algorithm requires setting several parametersfor the weighting of the structure of the context-based graph, which vary across the three systems.The BN1 system optimizes its parameters from thetrial data, while the BN2 and WN1 systems arecompletely unsupervised and optimize their param-eters directly from the structure of the BabelNet andWordNet graphs.UMCC-DLSI UMCC-DLSI submitted threesystems based on the ISR-WN resource (Gutie?rrezet al 2011), which enriches the WordNet se-mantic network using edges from multiple lexicalresources, such as WordNet Domains and theeXtended WordNet.
WSD was then performedusing the ISR-WN network in combination withthe algorithm of Gutie?rrez (2012), which is anextension of the Personalized PageRank algorithmfor WSD (Agirre and Soroa, 2009) which includessenses frequency.
The algorithm requires initial-izing the PageRank algorithm with a set of seedsynsets (vertices) in the network; this initializationrepresents the key variation among UMCC?s threeapproaches.
The RUN-1 system performs WSDusing all noun instances from the sentence context.In contrast, the RUN-2 works at the discourse leveland initializes the PageRank using the synsets of all226Team System English French German Italian SpanishDAEBAK!
PD 0.604 0.538 0.591 0.613 0.600GETALP BN-1 0.263 0.261 0.404 0.324 -GETALP BN-2 0.266 0.257 0.400 0.324 0.371UMCC-DLSI RUN-1 0.677 0.605 0.618 0.657 0.705UMCC-DLSI RUN-2 0.685 0.605 0.621 0.658 0.710UMCC-DLSI RUN-3 0.680 - - - -MFS 0.665 0.453 0.674 0.575 0.645Table 3: System performance, reported as F1, for all five languages in the test set when using BabelNetsenses.
Top performing systems are marked in bold.nouns in the document.
Finally, the RUN-3 systeminitializes using all words in the sentence.5 Results and DiscussionAll teams submitted at least one system using theBabelNet inventory, shown in Table 3.
The UMCC-DLSI systems were consistently able to outperformthe MFS baseline (a notoriously hard-to-beat heuris-tic) in all languages except German.
Additionally,the DAEBAK!
system outperformed the MFS base-line on French and Italian.
The UMCC-DLSI RUN-2 system performed the best for all languages.
No-tably, this system leverages the single-sense per dis-course heuristic (Yarowsky, 1995), which uses thesame sense label for all occurrences of a lemma in adocument.UMCC-DLSI submitted the only three sys-tems to use Wikipedia-based senses.
Table 4 showstheir performance.
Of the three sense inventories,Wikipedia had the most competitive MFS baseline,scoring at least 0.694 on all languages.
Notably,the Wikipedia-based system has the lowest recall ofall systems.
Despite having superior precision to theMFS baseline, the low recall brought the resultingF1 measure below the MFS.Two teams submitted four total systems for Word-Net, shown in Table 5.
The UMCC-DLSI RUN-2system was again the top-performing system, under-scoring the benefit of using discourse information inselecting senses.
The other two UMCC-DLSI sys-tems also surpassed the MFS baseline.
Though stillperforming worse than the MFS baseline, when us-ing the WordNet sense graph, the GETALP systemsees a noticeable improvement of 0.14 over its per-00.10.20.30.40.50.60.70.80.90  5  10  15  20  25  30  35  40  45WSD F1Number of senses for the instanceDAEBAK!
PDGETALP BN-2 UMCC-DLSI Run-2Figure 1: F1 measure according to the degree ofinstance polysemy, reported when at least ten in-stances have the specified polysemy.formance on English data when using the WordNetsense graph.The disambiguation task encompasses multipletypes of entities.
Therefore, we partitioned the Ba-belNet test data according to the type of instance be-ing disambiguated; Table 6 highlights the results perinstance type, averaged across all languages.5 Bothmultiword expressions and named entities are lesspolysemous, resulting in a substantially higher MFSbaseline that no system was able to outperform onthe two classes.
However, for instances made of asingle term, both of the UMCC-DLSI systems wereable to outperform the MFS baseline.BabelNet adds many Wikipedia senses to the ex-isting WordNet senses, which increases the poly-5We omit the UMCC-DLSI Run-3 system from analysis, asit participated in only a single language.227English French German Italian SpanishTeam System Prec.
Rec.
F1 Prec.
Rec.
F1 Prec.
Rec.
F1 Prec.
Rec.
F1 Prec.
Rec.
F1UMCC-DLSI RUN-1 0.619 0.484 0.543 0.817 0.480 0.605 0.758 0.460 0.572 0.785 0.458 0.578 0.773 0.493 0.602UMCC-DLSI RUN-2 0.620 0.487 0.546 0.815 0.478 0.603 0.769 0.467 0.581 0.787 0.463 0.583 0.778 0.502 0.610UMCC-DLSI RUN-3 0.622 0.489 0.548 - - - - - - - - - - - -MFS 0.860 0.753 0.803 0.698 0.691 0.694 0.836 0.827 0.831 0.833 0.813 0.823 0.830 0.819 0.824Table 4: The F1 measure for each system across all five languages in the test set when using Wikipedia-basedsenses.Team System Precision Recall F1GETALP WN-1 0.406 0.406 0.406UMCC-DLSI RUN-1 0.639 0.635 0.637UMCC-DLSI RUN-2 0.649 0.645 0.647UMCC-DLSI RUN-3 0.642 0.639 0.640MFS 0.630 0.630 0.630Table 5: System performance when using WordNet senses.
Top performing systems are marked in bold.Team System Single term Multiword expression Named EntityDAEBAK!
PD 0.502 0.801 0.910GETALP BN-1 0.232 0.724 0.677GETALP BN-2 0.235 0.740 0.656UMCC-DLSI RUN-1 0.582 0.806 0.865UMCC-DLSI RUN-2 0.584 0.809 0.864MFS 0.511 0.853 0.920Table 6: System F1 per instance type, averaged across all submitted languages, with the highest systemscores in bold.English French German Italian SpanishTeam System Prec.
Rec.
F1 Prec.
Rec.
F1 Prec.
Rec.
F1 Prec.
Rec.
F1 Prec.
Rec.
F1DAEBAK PD 0.769 0.364 0.494 0.747 0.387 0.510 0.762 0.307 0.438 0.778 0.425 0.550 0.778 0.450 0.570GETALP BN-2 0.793 0.111 0.195 0.623 0.130 0.215 0.679 0.124 0.210 0.647 0.141 0.231 0.688 0.177 0.282UMCC-DLSI RUN-1 0.787 0.421 0.549 0.754 0.441 0.557 0.741 0.330 0.457 0.796 0.461 0.584 0.830 0.525 0.643UMCC-DLSI RUN-2 0.791 0.419 0.548 0.760 0.436 0.554 0.746 0.332 0.460 0.799 0.453 0.578 0.837 0.530 0.649Table 7: System performance when the system?s annotations are restricted to only those senses that it alsouses in the aligned sentences of at least two other languages.semy of most instances.
As a further analysis, weconsider the relationship between the polysemy ofan instance?s target and system performance.
In-stances were grouped according to the number ofBabelNet senses that their lemma had; following,systems were scored on each grouping.
Figure 1shows the performance of the best system from eachteam on each polysemy-based instance grouping,with a general trend of performance decay as thenumber of senses increases.
Indeed, all systems?performances are negatively correlated with the de-gree of polysemy, ranging from -0.401 (UMCC-DLSI RUN-1) to -0.654 (GETALP BN-1) whenmeasured using Pearson?s correlation.
All systems?228correlations are significant at p < 0.05.Last, we note that all systems operated by sense-annotating each language individually without tak-ing advantage of either the multilingual structure ofBabelNet or the sentence alignment of the test data.For example, the sense projection method used tocreate the initial set of multilingual annotations onour test data (cf.
Table 2) suggests that the sensetranslation API could be used as a reliable source forestimating the correctness of an annotation; specifi-cally, given the sense annotations for each language,the translation API could be used to test whether thesense is also present in the aligned sentence in theother languages.Therefore, we performed a post-hoc analysis ofthe benefit of multilingual sense alignment using theresults of the four systems that submitted for all lan-guages in BabelNet.
For each language, we filterthe sense annotations such that an annotation for aninstance is retained only if the system assigned thesame sense to some word in the aligned sentencefrom at least two other languages.Table 7 shows the resulting performance for thefour systems.
As expected, the systems exhibit sig-nificantly lower recall due to omitting all language-specific instances.
However, the resulting precisionis significantly higher than the original performance,shown in Table 3.
Additionally, we analyzed the setof instances reported for each system and confirmedthat the improvement is not due to selecting onlymonosemous lemmas.
Despite the GETALP systemhaving the lower performance of the four systemswhen all instances are considered, the system ob-tains the highest precision for the English dataset.Furthermore, the UMCC-DLSI systems still obtainmoderate recall, while enjoying 0.106-0.155 abso-lute improvements in precision across all languages.While the resulting F1 is lower due to a loss of recall,we view this result as a solid starting point for othermethods to sense-tag the remaining instances.
Over-all, these results corroborate previous studies sug-gesting that highly precise sense annotations can beobtained by leveraging multiple languages (Navigliand Ponzetto, 2012b; Navigli and Ponzetto, 2012c).6 Conclusion and Future DirectionsFollowing recent SemEval efforts with word sensesin multilingual settings, we have introduced a newtask on multilingual WSD that uses the recentlyreleased BabelNet 1.1.1 sense inventory.
Using adata set of 13 articles in five languages, all nomi-nal instances were annotated with BabelNet senses.Because BabelNet is a superset of WordNet andWikipedia, the task also facilitates analysis in thosesense inventories.Three teams submitted seven systems, with allsystems leveraging the graph-based structure ofWordNet and BabelNet.
Several systems were ableto outperform the competitive MFS baseline, exceptin the case of Wikipedia, but current performanceleaves significant room for future improvement.
Inaddition, we believe that future research could lever-age sense parallelism available in sentence-alignedmultilingual corpora, together with enriched infor-mation available in future versions of BabelNet.
Allof the resources for this task, including the newest1.1.1 version of BabelNet, were released on the taskwebsite.6AcknowledgmentsThe authors gratefully acknowledgethe support of the ERC StartingGrant MultiJEDI No.
259234.A large group of people assisted with SemEval-2013 Task 12, and without whose help thistask would not have been possible.
In particular,we would like to thank Philipp Cimiano, MaudErhmann, Sascha Hinte, Jesu?s Roque Campan?aGo?mez, and Andreas Soos for their assistancein sense annotation; our fellow LCL team mem-bers: Moreno De Vincenzi, Stefano Faralli, TizianoFlati, Marc Franco Salvador, Andrea Moro, SilviaNecs?ulescu, and Taher Pilehvar for their invaluableassistance in creating BabelNet 1.1.1, preparing andvalidating sense annotations, and sense-tagging theItalian corpus; last, we thank Jim McManus for hishelp in producing the Italian test data.6http://www.cs.york.ac.uk/semeval-2013/task12/229ReferencesEneko Agirre and Aitor Soroa.
2009.
PersonalizingPageRank for Word Sense Disambiguation.
In Pro-ceedings of EACL, Athens, Greece, pages 33?41.Philip Edmonds and Scott Cotton.
2001.
Senseval-2:Overview.
In Proceedings of The Second InternationalWorkshop on Evaluating Word Sense DisambiguationSystems, pages 1?6, Toulouse, France.Katrin Erk and Diana McCarthy.
2009.
Graded wordsense assignment.
In Proceedings of Empirical Meth-ods in Natural Language Processing, pages 440?449,Singapore.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Database.
MIT Press, Cambridge, MA.Yoan Gutie?rrez, Antonio Ferna?ndez Orqu?
?n, SoniaVa?zquez, and Andre?s Montoyo.
2011.
Enriching theintegration of semantic resources based on wordnet.Procesamiento del Lenguaje Natural, 47:249?257.Yoan Gutie?rrez.
2012.
Ana?lisis sema?ntico multidimen-sional aplicado a la desambiguacio?n del lenguaje nat-ural.
Ph.D. thesis, Universidad de Alicante.Eduard H. Hovy, Roberto Navigli, and Simone PaoloPonzetto.
2013.
Collaboratively built semi-structuredcontent and artificial intelligence: The story so far.
Ar-tificial Intelligence, 194:2?27.David Jurgens and Ioannis Klapaftis.
2013.
Semeval-2013 task 13: Word sense induction for graded andnon-graded senses.
In Proceedings of the 7th Interna-tional Workshop on Semantic Evaluation.Adam Kilgarriff and Martha Palmer.
2000.
Introductionto the special issue on senseval.
Computers and theHumanities, 34(1-2):1?13.Adam Kilgarriff.
1998.
Senseval: An exercise in eval-uating word sense disambiguation programs.
In Pro-ceedings of the First International Conference on Lan-guage Resources and Evaluation, pages 1255?1258,Granada, Spain.Els Lefever and Veronique Hoste.
2010.
Semeval-2010task 3: Cross-lingual word sense disambiguation.
InProceedings of the 5th International Workshop on Se-mantic Evaluation, pages 15?20, Uppsala, Sweden.Association for Computational Linguistics.Diana McCarthy and Roberto Navigli.
2009.
The En-glish lexical substitution task.
Language Resourcesand Evaluation, 43(2):139?159.Rada Mihalcea, Timothy Chklovski, and Adam Kilgar-riff.
2004.
The Senseval-3 English lexical sampletask.
In Proceedings of the 3rd International Work-shop on the Evaluation of Systems for the SemanticAnalysis of Text (SENSEVAL-3) at ACL-04, Barcelona,Spain, 25?26 July 2004, pages 25?28.Rada Mihalcea, Ravi Sinha, and Diana McCarthy.
2010.Semeval-2010 task 2: Cross-lingual lexical substitu-tion.
In Proceedings of the 5th international workshopon semantic evaluation, pages 9?14, Uppsala, Sweden.Association for Computational Linguistics.George A. Miller, R.T. Beckwith, Christiane D. Fell-baum, D. Gross, and K. Miller.
1990.
WordNet: anonline lexical database.
International Journal of Lexi-cography, 3(4):235?244.Roberto Navigli and Mirella Lapata.
2010.
An exper-imental study on graph connectivity for unsupervisedWord Sense Disambiguation.
IEEE Transactions onPattern Analysis and Machine Intelligence, 32(4):678?692.Roberto Navigli and Simone Paolo Ponzetto.
2012a.
Ba-belNet: The automatic construction, evaluation andapplication of a wide-coverage multilingual semanticnetwork.
Artificial Intelligence, 193:217?250.Roberto Navigli and Simone Paolo Ponzetto.
2012b.BabelRelate!
a joint multilingual approach to com-puting semantic relatedness.
In Proceedings of theTwenty-Sixth AAAI Conference on Artificial Intelli-gence (AAAI), Toronto, Ontario, Canada.Roberto Navigli and Simone Paolo Ponzetto.
2012c.Joining forces pays off: Multilingual Joint Word SenseDisambiguation.
In Proceedings of EMNLP-CoNLL,pages 1399?1410, Jeju Island, Korea.Roberto Navigli and Simone Paolo Ponzetto.
2012d.Multilingual WSD with just a few lines of code: theBabelNet API.
In Proceedings of the 50th AnnualMeeting of the Association for Computational Linguis-tics (ACL 2012), Jeju, Korea.Roberto Navigli, Kenneth C. Litkowski, and Orin Har-graves.
2007.
SemEval-2007 Task 07: Coarse-grained English all-words task.
In Proceedings of the4th International Workshop on Semantic Evaluations(SemEval-2007), Prague, Czech Republic, pages 30?35.Roberto Navigli.
2009.
Word Sense Disambiguation: Asurvey.
ACM Computing Surveys, 41(2):1?69.Roberto Navigli.
2012.
A quick tour of Word SenseDisambiguation, Induction and related approaches.
InProceedings of the 38th Conference on Current Trendsin Theory and Practice of Computer Science (SOF-SEM), pages 115?129.Sameer Pradhan, Edward Loper, Dmitriy Dligach, andMartha Palmer.
2007.
SemEval-2007 Task-17: En-glish lexical sample, SRL and all words.
In Proceed-ings of the 4th International Workshop on SemanticEvaluations (SemEval-2007), Prague, Czech Repub-lic, pages 87?92.Didier Schwab, Je?ro?me Goulian, Andon Tchechmedjiev,and Herve?
Blanchon.
2012.
Ant colony algorithm for230the unsupervised word sense disambiguation of texts:Comparison and evaluation.
In Proceedings of the24th International Conference on Computational Lin-guistics (COLING), pages 8?15, Mumbai, India.Benjamin Snyder and Martha Palmer.
2004.
The en-glish all-words task.
In Proceedings of ACL 2004SENSEVAL-3 Workshop, pages 41?43, Barcelona,Spain.David Yarowsky.
1995.
Unsupervised word sense dis-ambiguation rivaling supervised methods.
In Proceed-ings of the 33rd Annual Meeting of the Associationfor Computational Linguistics, pages 189?196, Cam-bridge, MA, USA.231
