WORD-SENSE D ISAMBIGUATION US ING STATISTICALMETHODSPeter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra,and Robert L. MercerIBM Thomas J. Watson Research CenterP.O.
Box 704Yorktown Heights, NY 10598ABSTRACTWe describe a statistical technique for assign-ing senses to words.
An instance of a word is as-signed a sense by asking a question about the con-text in which the word appears.
The question isconstructed to have high mutual information withthe translation of that instance in another lan-guage.
When we incorporated this method of as-signing senses into our statistical machine transla-tion system, the error rate of the system decreasedby thirteen percent.INTRODUCTIONAn alluring aspect of the statistical ~p-proach to machine translation rejuvenated byBrown et al \[Brown et al, 1988, Brown et al,1990\] is the systematic framework it providesfor attacking the problem of lexical disam-biguation.
For example, the system they de-scribe translates the French sentence Je vaisprendre la ddcision as I will make the decision,correctly interpreting prendre as make.
Thestatistical translation model, which suppliesEnglish translations of French words, prefersthe more common translation take, bnt thetrigram language model recognizes that thethree-word sequence make the decision, is muchmore probable than take the decision..The system is not always so successfifl.
Itincorrectly renders Je vais prendre ma propreddcision as 1 will take my own decision.
Thelanguage model does not realize that take myown decision is improbable because take anddecision no longer fall within a single trigram.Errors such as this are common becausethe statistical models only capture local phe-nomena; if the context necessary to determinea translation falls outside the scope of themodels, the word is likely to be translated in-correctly, t\[owever, if the relevant context isencoded locally, the word should be translatedcorrectly.
We can achieve this within the tra-ditional paradigm of analysis, transfer, andsynthesis by incorporating into the analysisphase a sense-disambiguation component thatassigns sense labels to French words.
If pren-dre is labeled with one sense in the contextof ddcision but with a different sense in othercontexts, then the translation model will learnfront trMning data that the first sense usuallytranslates to make, whereas the other senseusuMly translates to take.Previous efforts a.t algorithmic disambigua-tion of word senses \[Lesk, 1986, White, 1988,Ide and V6ronis, 1990\] have concentrated oninformation that can be extracted from elec-tronic dictionaries, and focus, therefore, onsenses as determined by those dictionaries.llere, in contrast, we present a procedure forconstructing a sense-disambiguation compo-nent that labels words so as to elucidate theirtranslations in another language.
We are con-264The proposalLes proposit ionswill not/ne seront pasnow be implementedmises en appl icat ion maintenantFigure 1: Alignment Examplecerned about senses as they occur in a dic-tionary only to the extent that those sensesare translated differently.
The French nounintdr~t, for example, is translated into Ger-man as either Zins or \[nteresse according toits sense, but both of these senses are trans-lated into English as interest, and so we makeno attempt o distinguish them.STAT IST ICAL  TRANSLAT IONFollowing Brown et al \[Brown et al, 1990\],we choose as the translation of a French sen-tence F that sentence E for which Pr (E\[F)is greatest.
By Bayes' rule,Pr (ELF) = Pr (E) PrPr(F) (1)Since the denominator does not depend onE, the sentence for which Pr (El/7') is great-est is also the sentence for which the productPr (E) Pr (F IE)  is greatest.
The first factorin this product is a statistical characteriza-tion of the English language and the secondfactor is a statistical characterization of theprocess by which English sentences are trans-lated into French.
We can compute neitherfactors precisely.
Rather, in statistical trans-lation, we employ models from which we canobtain estimates of these values.
We cM1 themodel from which we compute Pr (E) the lan-guage model and that from which we computePr (F IE  ) the translation model.The translation model used by Brown et al\[Brown et al, 1990\] incorporates the conceptof an alignment in which each word in E actsindependently to produce some of the wordsin F. If we denote a typical alignment by A,then we can write the probability of F givenE as a sum over all possible alignments:Pr (FIE) = ~ Pr (F, AlE ) .
(2)AAlthough the number of possible alignments isa very rapidly growing function of the lengthsof the French and English sentences, only atiny fraction of the alignments contributes sub-stantiMly to the sum, and of these few, onemakes the grea.test contribution.
We ca.ll thismost probable alignment the Viterbi align-ment between E a.nd F.Tile identity of tile Viterbi alignment fora pair of sentences depends on the details ofthe translation model, but once the model isknown, probable alignments can be discoveredalgoritlunically \[Brown et al, 1991\].
Brownet al \[Brown et al, 1990\], show an exampleof such an automatically derived alignment intheir Figure 3.
(For the reader's convenience,we ha.re reproduced that figure here as Figure1.
)265In a Viterbi alignment, a French word thatis connected by a line to an English word issaid to be aligned with that English word.Thus, in Figure 1, Les is aligned with The,propositions with proposal, and so on.
We calla p~ir of aligned words obtained in this way aconnection.From the Viterbi alignments for 1,002,165pairs of short French and English sentencesfrom the Canadian Hansard data \[Brown et al,1990\], we have extracted a set of 12,028,485connections.
Let p(e, f )  be the probabilitythat a connection chosen at random fi:om thisset will connect the English word e to theFrench word f .
Because each French wordgives rise to exactly one connection, the rightmarginM of this distribution is identical tothe distribution of French words in these sen-tences.
The left marginal, however, is notthe same as the distribution of English words:English words that tend to produce severalFrench words at a time are overrepresentedwhile those that tend to produce no Frenchwords are underrepresented.SENSES BASED ON BINARYQUESTIONSUsing p(e, f )  we can compute the mutuMinformation between a French word and itsEnglish mate in a connection.
In this section,we discuss a method for labelling a word witha sense that depends on the context in whichit appears in such a way as to increase themutual information between the members ofa connection.In the sentence Je vats prendre .ma pro-pre ddeision, the French verb prendre shouldbe translated as make because the obiect ofprendre is ddcision.
If we replace ddcision byvoiture, then prendre should be translated astake to yield \[ will take my own ear.
In theseexamples, one can imagine assigning a senseto prendre by asking whether the first noun tothe right of prendre is ddeision or voiture.
Wesay that the noun to the right is the informantfor prendre.In I1 doute que les ndtres gagnent, whichmeans He doubts that we will win, the Frenchword il should be translated as he.
On theother hand, in II faut que les n6tres gagnent,which means It is necessary that we win, ilshould be translated as it.
Here, we can de-termine which sense to assign to il by askingabout the identity of the first verb to its right.Even though we cannot hope to determine thetranslation of il from this informant unam-biguously, we can hope to obtain a significantamount of information about the translation.As a final example, consider the Englishword is.
In the sentence I think it is a prob-lem, it is best to translate is as est as in Jepense que c'est un probl~me.
However, this iscertainly not true in the sentence \[think thereis a problem, which translates as Je pense qu'ily aun  probl~me.
Here we can reduce the en-tropy of the distribution of the translation ofis by asking if the word to the left is there.
Ifso, then is is less likely to be translated as estthan if not.Motivated by examples like these, we in-vestigated a simple method of assigning twosenses to a word w by asking a single binaryquestion about one word of the context inwhich w appears.
One does not know before-hand whether the informant will be the firstnoun to the right, the first verb to the right,or some other word in the context of w. How-ever, one can construct a question for each ofa number of candidate informant sites, andthen choose the most informative question.Given a potential informant such as thefirst noun to the right, we can construct aquestion that has high mutual information withthe translation of w by using the flip-flop algo-rithm devised by Nadas, Nahamoo, Picheny,and Poweli \[Nadas et aL, 1991\].
To under-stand their algorithm, first imagine that w is aFrench word and that English words which arepossible translations of w have been dividedinto two classes.
Consider the prol>lem of con-structing 4.
1)inary question about the poten-tial inform ant th a.t provides maximal inform a-tion about these two English word classes.
Ifthe French vocabulary is of size V, then there266are 2 v possible questions, tlowever, using thesplitting theorem of Breiman, Friedman, O1-shen, and Stone \[Breiman et al, 1984\], it ispossible to find the most informative of these2 v questions in time which is linear in V.The flip-flop Mgorithm begins by makingan initiM assignment of the English transla-tions into two classes, and then uses the split-ting theorem to find the best question aboutthe potential informant.
This question dividesthe French vocabulary into two sets.
One canthen use the splitting theorem to find a di-vision of the English translations of w intotwo sets which has maximal mutual informa-tion with the French sets.
In the flip-flop al-gorithm, one alternates between splitting theFrench vocabulary into two sets and the En-glish translations of w into two sets.
Aftereach such split, the mutual information be-tween the French and English sets is at leastas great as before the split.
Since the mutualinformation is bounded by one bit, the processconverges to a partition of the French vocab-ulary that has high mutual information withthe translation of w.A P ILOT EXPERIMENTWe used the flip-flop algorithm in a pilotexperiment in which we assigned two senses toeach of the 500 most common English wordsand two senses to each of the 200 most com-mon French words.For a French word, we considered ques-tions about seven informants: the word to theleft, the word to the right, the first noun tothe left, the first noun to the right, the firstverb to the left, the first verb to the right,and the tense of either the current word, if itis a verb, or of the first verb to the left of thecurrent word.
For an English word, we onlyconsidered questions about the the word tothe left and the word two to tim left.
We re-stricted the English questions to the l)revioustwo words so that we could easily use themin our translation system which produces anEnglish sentence from left to right.
Whena potential informant did not exist, because,say there was no noun to the left of someWord:Informant:Information:prendreRight noun.381 bitsSense 1TERM_WORDmesurenoteexempletempsinitiativepartSense 2d~cisionparoleconnaissanceengagementfinretr~iteCommon informant values for each sensePr(English \[ Sense 1) Pr(English \[ Sense 2)to_take .433to_make .061to_do .051to_be .045to_make .186to-speak .105to_rise .066to_take .066to_be .058decision .036to-get .025to_have .021Probabilities of English translationsFigure 2: Senses for the French word prendreword in a particular sentence, we used the spe-cial word, TERM_WORD.
To find the nounsand verbs in our French sentences, we usedthe tagging Mgorithm described by MeriMdo\[Merialdo, 1990\].Figure 2 shows the question that was con-str,cted for tile verb prendre.
The noun tothe right yielded the most information, .381bits, about the English translation of prendre.The box in the top of the figure shows thewords which most frequently occupy that site,that is, tile nouns which appear to the rightof prendre with a probability greater than onepart in fifty.
All instance of prendre is assignedthe first or second sense depending on whetherthe first noun to the right appears in the left-ha.nd or the right-hand column.
So, for ex-267Word:Informant:Information:vouloirVerb tense.349 bitsWord:Informant:Information:del)uisWord to the right.738 bitsSense 1 Sense 23rd p sing present1st p sing present3rd p plur present1st p pint present2nd p pint present3rd p sing imperfect1st p sing imperfect3rd p sing future1st p sing conditional3rd p sing conditional3rd p plur conditional3rd  p plur subjunctive1st p plur conditionalCommon informant values for each senseSense 1longtempsdeURquelquesdenx1plustroisSense 2lelal'celes1968Comnmn informant values for each sensePr(Engl ish\[Sense 1) Pr(English \[ Sense 2)to_want .484to_mean .056to_be .056to_wish .033to_rear .022to_like .020toJike .391to_want .169to_have .083to_wish .066me .029Probabilities of English translationsFigure 3: Senses for the French word vouloirample, if the noun to the right of prendre isddeision, parole, or eonnaissance, then pren-dre is assigned the second sense.
The box atthe bottom of the figure shows the most prob-able translations of each of the two senses.Notice that the English verb to_make is threetimes as likely when prendre has the secondsense as when it has the first sense.
Peoplemake decisions, speeches, and acquaintances,they do not take them.Figure 3 shows our results for the verbvouloir.
Here, the best informant is the tenseof vouloir.
The first sense is three times morelikely than the second sense to translate asto_want, but twelve times less likely to trans-late as to_like.
In polite English, one says Iwould like so and so more commonly than \[would want so and so.Pr (English I Sense 1) Pr (English I Sense 2)for .432last .123long .102past .078over .027in .022overdue .021since .772from .040Probabilities of English translationsFigure 4: Senses for the French word depuisTile question in Figure 4 reduces the en-tropy of the translation of the French prepo-sition depuis by .738 bits.
When depuis is fol-lowed by an article, it translates with proba-bility .772 to .since, and otherwise only withprobability .016.Finally, consider the English word cent.
Inour text, it is either a denomination of cur-rency, in which case it is usually preceded bya number and translated as c., or it is thesecond half of per cent, in which case it is pre-ceded by per and transla,ted along with per as~0.
The results in Figure 5 show that the al-gorithm has discovered this, and in so doinghas reduced the entropy of the translation ofcent by .378 bits.268Word: centInformant: Word to the leftInformation: .378 bitsSense 1 Sense 2per 0852aone47Common informant values for each sensePr(French I Sense 1) Pr(French \[Sense 2)% .891 c. .592cent .239sou .046% .022Probabilities of French translationsFigure 5: Senses for the English word centPleased with these results, we incorporatedsense-assignment questions for the 500 mostcommon English words and 200 most com-mon French words into our translation sys-tem.
This system is an enhanced version ofthe one described by Brown et al \[Brownet al, 1990\] in that it uses a trigram lan-guage model, and has a French vocabulary of57,802 words, and an English vocabulary of40,809 words.
We translated 100 randomlyselected Hansard sentences each of which is10 words or less in length.
We judged 45of the resultant ranslations as acceptable ascompared with 37 acceptable translations pro-duced by the same system running withoutsense-disambiguation questions.FUTURE WORKAlthough our results are promising, thisparticular method of assigning senses to wordsis quite limited.
It assigns at most two sensesto a word, and thus can extract no more thanone bit of information about the translation ofthat word.
Since the entropy of the transla-tion of a common word can be as high as fivebits, there is reason to hope that using moresenses will fitrther improve the performance ofour system.
Our method asks a single ques-tion about a single word of context.
We canthink of tlfis as the first question in a deci-sion tree which can be extended to additionallevels \[Lucassen, 1983, Lucassen and Mercer,1984, Breiman et al, 1984, Bahl et al, 1989\].We are working on these and other improve-ments and hope to report better results in thefuture.REFERENCES\[Bahl et aL, 1989\] BMd, L., Brown,P., de Souza, P., and Mercer, R. (1989).A tree-based statistical language model fornatural anguage speech recognition.
IEEETransactions on Acoustics, Speech and Sig-nal Processing, 37:1001-1008.\[Breiman et ai., 1984\] Breiman, L., Fried-man, J.
tI., Olshen, R. A., and Stone,C.
J.
(1984).
Classification and Regres-sion Trees.
Wadsworth & Brooks/Cole Ad-vanced Books & Software, Monterey, Cali-fornia.\[Brown et aL, 1990\] Brown, P. F., Cocke, J.,DellaPietra, S. A., DellaPietra, V. J., Je-linek, F., Lafferty, J. D., Mercer, R. L.,and Roossin, P. S. (1990).
A statistical ap-l)roach to machine translation.
Computa-tional Linguistics, 16(2):79--85.\[Brown et al, 1988\] Brown, P. F., Cocke, J.,DellaPietra, S. A., DellaPietra, V. J., Je-linek, F., Mercer, R. L., and Roossin, P. S.(1988).
A statistical approach to languagetranslation.
I!1 Proceedings of the 12th In-ternational Conference on ComputationalLinguistics, Budapest, Hungary.\[Brown et aL, 1991\] Brown, P. F., DellaPi-etra, S. A., DellaPietta, V. J., and Mercer,R.
L. (1991).
Parameter estimation for ma-chine translation.
In preparation.\[hie and V@onis, 1990\] Ide, N. and V6ronis,.I.
(1990).
Mapping dictionaires: A spread-269ing activation approach.
I:!
Proccedil~!ls ofthe Sixth Annual Conferen~:e of the UII'Centre for the New Oxford English Dictio-nary and Text Research, pages 52-6,t, Wa-terloo, Canada.\[Lesk, 1986\] Lesk, M. E. (1986).
Auto-mated sense disambiguation using machine-readable dictionaries: How to tell a pinecone from an ice cream cone.
In Proceed-ings of the SIGDOC Conference.\[Lncassen, 1983\] Lucassen, J. M. (1983).
Dis-covering phonemic baseforms automati-cally: an information theoretic approach.Technical Report RC 9833, IBM ResearchDivision.\[Lucassen and Mercer, 1984\] Lucassen, J. M.and Mercer, R. L. (1984).
An informationtheoretic approach to automatic determi-nation of phonemic baseforms.
In Proceed-ings of the IEEE International Conferenceon Acoustics, Speech and Signal Processing,pages 42.5.1-42.5.4, San Diego, California.\[Meria\]do, 1990\] Merialdo, B.
(1990).
Tag-ging text with a probabilistic model.
InProceedii~gs of the IBM Natural LanguageITL, pages 161-172, Paris, France.\[Nadas et at., 1991\] Nadas, A., Nahamoo,D., Picheny, M. A., and Powell, J.
(1991).An iterative "flip-flop" approximation ofthe most informative split in the construc-tion of decision trees.
In Proceedings of theIEEE International Conference on Acous-tics, Speech and Signal Processing, Toronto,Canada.\[White, 1988\] White, J. S. (1988).
Deter-mination of lexical-semantic relations formulti-lingual terminology structures.
InRelational Models of the Lexicon,.
Cam-bridge University Press, Cambridge, OK.270
