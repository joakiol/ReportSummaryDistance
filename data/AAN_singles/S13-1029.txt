Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conferenceand the Shared Task, pages 202?206, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational LinguisticsCLaC-CORE: Exhaustive Feature Combination for Measuring TextualSimilarityEhsan ShareghiCLaC LaboratoryConcordia UniversityMontreal, QC H3G 1M8, CANADAeh share@cse.concordia.caSabine BerglerCLaC LaboratoryConcordia UniversityMontreal, QC H3G 1M8, CANADAbergler@cse.concordia.caAbstractCLaC-CORE, an exhaustive feature combina-tion system ranked 4th among 34 teams in theSemantic Textual Similarity shared task STS2013.
Using a core set of 11 lexical featuresof the most basic kind, it uses a support vectorregressor which uses a combination of theselexical features to train a model for predictingsimilarity between sentences in a two phasemethod, which in turn uses all combinationsof the features in the feature space and trainsseparate models based on each combination.Then it creates a meta-feature space and trainsa final model based on that.
This two step pro-cess improves the results achieved by single-layer standard learning methodology over thesame simple features.
We analyze the correla-tion of feature combinations with the data setsover which they are effective.1 IntroductionThe Semantic Textual Similarity (STS) shared taskaims to find a unified way of measuring similaritybetween sentences.
In fact, sentence similarity isa core element of tasks trying to establish how twopieces of text are related, such as Textual Entailment(RTE) (Dagan et al 2006), and Paraphrase Recog-nition (Dolan et al 2004).
The STS shared task wasintroduced for SemEval-2012 and was selected as itsfirst shared task.
Similar in spirit, STS differs fromthe well-known RTE shared tasks in two importantpoints: it defines a graded similarity scale to mea-sure similarity of two texts, instead of RTE?s binaryyes/no decision and the similarity relation is consid-ered to be symmetrical, whereas the entailment rela-tion of RTE is inherently unidirectional.The leading systems in the 2012 competition useda variety of very simple lexical features.
Each sys-tem combines a different set of related features.CLaC Labs investigated the different combinationpossibilities of these simple lexical features andmeasured their performance on the different datasets.
Originally conceived to explore the space ofall possible feature combinations for ?feature com-bination selection?, a two-step method emerged thatdeliberately compiles and trains all feature combina-tions exhaustively and then trains an SVM regressorusing all combination models as its input features.It turns out that this technique is not nearly as pro-hibitive as imagined and achieves statistically sig-nificant improvements over the alternative of featureselection or of using any one single combination in-dividually.We propose the method as a viable approach whenthe characteristics of the data are not well under-stood and no satisfactory training set is available.2 Related WorkRecently, systems started to approach measuringsimilarity by combining different resources andmethods.
For example, the STS-2012 shared task?sleading UKP (Ba?r et al 2012) system uses n-grams,string similarity, WordNet, and ESA, and a regres-sor.
In addition, they use MOSES, a statistical ma-chine translation system (Koehn et al 2007), totranslate each English sentence into Dutch, German,and Spanish and back into English in an effort to in-crease their training set of similar text pairs.202TakeLab (S?aric et al 2012), in place two of the2012 STS shared task, uses n-gram models, twoWordNet-based measures, LSA, and dependenciesto align subject-verb-object predicate structures.
In-cluding named-entities and number matching in thefeature space improved performance of their supportvector regressor.
(Shareghi and Bergler, 2013) illustrates two ex-periments with STS-2012 training and test sets us-ing the basic core features of these systems, outper-forming the STS-2012 task?s highest ranking sys-tems.
The STS-2013 submission CLaC-CORE usesthe same two-step approach.3 CLaC MethodologyPreprocessing consists of tokenizing, lemmatizing,sentence splitting, and part of speech (POS) tagging.We extract two main categories of lexical features:explicit and implicit.3.1 Explicit Lexical FeaturesSentence similarity at the explicit level is basedsolely on the input text and measures the similar-ity between two sentences either by using an n-grammodel (ROUGE-1, ROUGE-2, ROUGE-SU4) or byreverting to string similarity (longest common sub-sequence, jaro, ROUGE-W):Longest Common Subsequence (Allison andTrevor, 1986) compare the length of thelongest sequence of characters, not necessarilyconsecutive ones, in order to detect similaritiesJaro (Jaro, 1989) identifies spelling variation be-tween two inputs based on the occurrence ofcommon characters between two text segmentsat a certain distanceROUGE-W (Lin et al 2004a), a weighted versionof longest common subsequence, takes into ac-count the number of the consecutive charactersin each match, giving higher score for thosematches that have larger number of consecu-tive characters in common.
This metric was de-veloped to measure the similarity between ma-chine generated text summaries and a manuallygenerated gold standardROUGE-1 unigrams (Lin et al 2004a)ROUGE-2 bigrams (Lin et al 2004a)ROUGE-SU4 4-Skip bigrams (including Uni-grams) (Lin et al 2004a)3.2 Implicit Lexical FeaturesSentence similarity at the implicit level uses exter-nal resources to make up for the lexical gaps thatgo otherwise undetected at the explicit level.
Thesynonymy of bag and suitcase is an example of animplicit similarity.
This type of implicit similaritycan be detected using knowledge sources such asWordNet or Roget?s Thesaurus based on the Word-Net::Similarity package (Pedersen et al 2004) andcombination techniques (Mihalcea et al 2006).
Forthe more semantically challenging non-ontologigalrelations, for example sanction and Iran, which lex-ica do not provide, co-occurrence-based measureslike ESA are more robust.
We use:Lin (Lin, 1998) uses the Brown Corpus of Ameri-can English to calculate information content oftwo concepts?
least common subsumer.
Thenhe scales it using the sum of the informationcontent of the compared conceptsJiang-Conrath (Jiang and Conrath, 1997) uses theconditional probability of encountering a con-cept given an instance of its parent to calculatethe information content.
Then they define thedistance between two concepts to be the sumof the difference between the information con-tent of each of the two given concepts and theirleast common subsumerRoget?s Thesaurus is another lexical resource andis based on well-crafted concept classifica-tion and was created by professional lexicogra-phers.
It has a nine-level ontology and doesn?thave one of the major drawbacks of WordNet,which is lack of links between part of speeches.According to the schema proposed by (Jarmaszand Szpakowicz, 2003) the distance of twoterms decreases within the interval of [0,16],as the the common head that subsumes themmoves from top to the bottom and becomesmore specific.
The electronic version of Ro-get?s Thesaurus which was developed by (Jar-masz and Szpakowicz, 2003) was used for ex-tracting this score203Explicit Semantic Analyzer (Gabrilovich andMarkovitch, 2007) In order to have broadercoverage on word types not represented inlexical resources, specifically for named enti-ties, we add explicit semantic analyzer (ESA)generated features to our feature space3.3 CLaC-CORECLaC-CORE first generates all combinations of the11 basic features (jaro, Lemma, lcsq, ROUGE-W,ROUGE-1, ROUGE-2, ROUGE-SU4, roget, lin, jcn,esa), that is 211 ?
1 = 2047 non-empty combina-tions.
The Two Phase Model Training step trainsa separate Support Vector Regressor (SVR) foreach combination creating 2047 Phase One Models.These 2N ?
1 predicted scores per text data itemform a new feature vector called Phase Two Fea-tures, which feed into a SVR to train our Phase TwoModel.On a standard 2 core computer with ?100 GBof RAM using multi-threading (thread pool of size200, a training process per thread) it took roughly 15hours to train the 2047 Phase One Models on 5342text pairs and another 17 hours to build the PhaseTwo Feature Space for the training data.
Buildingthe Phase Two Feature Space for the test sets tookroughly 7.5 hours for 2250 test pairs.For the current submissions we combine all train-ing sets into one single training set used in all of oursubmissions for the STS 2013 task.4 Analysis of ResultsOur three submission for STS-2013 compare a base-line of Standard Learning (RUN-1)with two ver-sions of our Two Phase Learning (RUN-2, RUN-3).
For the Standard Learning baseline, one regres-sor was trained on the training set on all 11 BasicFeatures and tested on the test sets.
For the remain-ing runs the Two Phase Learning method was used.All our submissions use the same 11 Basic Features.RUN-2 is our main contribution.
RUN-3 is identicalto RUN-2 except for reducing the number of supportvectors and allowing larger training errors in an ef-fort to assess the potential for speedup.
This wasdone by decreasing the value of ?
(in the RBF ker-nel) from 0.01 to 0.0001, and decreasing the value ofC (error weight) from 1 to 0.01.
These parametersresulted in a smoother and simpler decision surfacebut negatively affected the performance for RUN-3as shown in Table 1.The STS shared task-2013 used the Pearson Cor-relation Coefficient as the evaluation metric.
The re-sults of our experiments are presented in Table 1.The results indicate that the proposed method, RUN-rank headlines OnWN FNWN SMTRUN-1 10 0.6774 0.7667 0.3793 0.3068RUN-2 7 0.6921 0.7367 0.3793 0.3375RUN-3 46 0.5276 0.6495 0.4158 0.3082STS-bl 73 0.5399 0.2828 0.2146 0.2861Table 1: CLaC-CORE runs and STS baseline perfor-mance2, was successful in improving the results achievedby our baseline RUN-1 ever so slightly (the confi-dence invervals at 5% differ to .016 at the upper end)and far exceeds the reduced computation version ofRUN-3.4.1 Successful Feature CombinationsHaving trained separate models based on each sub-set of features we can use the predicted scores gen-erated by each of these models to calculate their cor-relations to assess which of the feature combinationswere more effective in making predictions and howthis most successful combination varies bewteen thedifferent datasets.best worstheadlines [ ROUGE-1 ROUGE-SU4 esa lem][jcn lem lcsq]0.7329 0.3375OnWN [ROUGE-1 ROUGE-SU4 esa lin jcn rogetlem lcsq ROUGE-W ][jaro]0.7768 0.1425FNWN [roget ROUGE-1ROUGE-SU4][ROUGE-2 lem lcsq]0.4464 -0.0386SMT [lin jcn rogetROUGE-1][esa lcsq]0.3648 0.2305Table 2: Best and worst feature combination performanceon test setTable 2 lists the best and worst feature combina-tions on each test set.
ROUGE-1 (denoted by RO-1), unigram overlap, is part of all four best perform-ing subsets.
The features ROUGE-SU4 and Roget?s204appear in three of the best four feature combina-tions, making Roget?s the best performing lexicon-based feature outperforming WordNet features onthis task.
esa, lin, jcn are part of two of the bestsubsets, where lin and jcn occur together both times,suggesting synergy.
Looking at the worst perform-ing feature combinations is also instructive and sug-gests that lcsq was not an effective feature (despitebeing at the heart of the more successful ROUGE-Wmeasure).We also analyze performance of individual fea-tures over different datasets.
Table 3 lists all the fea-tures and, instead of looking at only the best com-bination, takes the top three best combinations foreach test and compares how many times each fea-ture has occurred in the resulting 12 combinations(first column).
Three clear classes of effectivenessemerge, high (10-7), medium (6-4), and low (3-0).Next, we observe that the test sets differ in the aver-age length of the data: headlines and OnWN glossesare very short, in contrast to the other two.
Table 3shows in fact contrastive feature behavior for thesetwo categories (denoted by short and long).
The lastcolumn reports the number of time a feature has oc-curred in the best combinations (out of 4).
Again,ROUGE-1, ROUGE-SU4, and roget prove effectiveacross different test sets.
esa and lem seem most re-liable when we deal with short text fragments, whileroget and ROUGE-SU4 are most valuable on longertexts.
The individual most valuable features overallare ROUGE-1, ROUGE-SU4, and roget.Features total (/12) short (/6) long (/6) best (/4)esa 6 6 0 2lin 6 3 3 2jcn 4 1 3 2roget 9 3 6 3lem 6 6 0 2jaro 0 0 0 0lcsq 3 3 0 1ROUGE-W 7 4 3 1ROUGE-1 10 6 4 4ROUGE-2 3 1 2 0ROUGE-SU4 10 5 5 3Table 3: Feature contribution to the three best results overfour datasets5 ConclusionCLaC-CORE investigated the performance possibil-ities of different feature combinations for 11 basiclexical features that are frequently used in seman-tic distance measures.
By exhaustively training allcombinations in a two-phase regressor, we were ableto establish a few interesting observations.First, our own baseline of simply training a SVMregressor on all 11 basic features achieves rank 10and outperforms the baseline used for the sharedtask.
It should probably become the new standardbaseline.Second, our two-phase exhaustive model, whileresource intensive, is not at all prohibitive.
If theknowledge to pick appropriate features is not avail-able and if not enough training data exists to per-form feature selection, the exhaustive method canproduce results that outperform our baseline and onethat is competitive in the current field (rank 7 of 88submissions).
But more importantly, this method al-lows us to forensically analyze feature combinationbehavior contrastively.
We were able to establishthat unigrams and 4-skip bigrams are most versatile,but surprisingly that Roget?s Thesaurus outperformsthe two leading WordNet-based distance measures.In addition, ROUGE-W, a weighted longest com-mon subsequence algorithm that to our knowledgehas not previously been used for similarity mea-surements shows to be a fairly reliable measure forall data sets, in contrast to longest common subse-quence, which is among the lowest performers.We feel that the insight we gained well justifiedthe expense of our approach.AcknowledgmentsWe are grateful to Michelle Khalife and Jona Schu-man for their comments and feedback on this work.This work was financially supported by the Natu-ral Sciences and Engineering Research Council ofCanada (NSERC).ReferencesAdam Kilgarriff, Pavel Rychly, Pavel Smrz, and DavidTugwell.
2004.
ITRI-04-08 The Sketch Engine.
In-formation Technology.205Alex J. Smola and Bernhard Scho?lkopf.
2004.
A Tutorialon Support Vector Regression.
Statistics and Comput-ing, 14(3).Alexander Budanitsky and Graeme Hirst.
2006.
Eval-uating WordNet-based Measures of Lexical SemanticRelatedness.
Computational Linguistics, 32(1).Chin-Yew Lin.
2004a.
ROUGE: A Package for Auto-matic Evaluation of Summaries.
In Text Summariza-tion Branches Out: Proceedings of the ACL-04 Work-shop.Chin-Yew Lin and Franz Josef Och.
2004b.
Auto-matic Evaluation of Machine Translation Quality Us-ing Longest Common Subsequence and Skip-BigramStatistics.
In Proceedings of the 42nd Annual Meetingon Association for Computational Linguistics.
Associ-ation for Computational Linguistics.Christiane Fellbaum 2010.
WordNet.
Theory andApplications of Ontology: Computer Applications.Springer.Daniel Ba?r, Chris Biemann, Iryna Gurevych, and TorstenZesch.
2012.
UKP: Computing Semantic TextualSimilarity by Combining Multiple Content SimilarityMeasures.
In Proceedings of the 6th InternationalWorkshop on Semantic Evaluation (SemEval 2012), inconjunction with the First Joint Conference on Lexicaland Computational Semantics.Dekang Lin.
1998.
An Information-Theoretic Definitionof Similarity.
In Proceedings of the 15th InternationalConference on Machine Learning, volume 1.Ehsan Shareghi, Sabine Bergler.
2013.
Feature Combi-nation for Sentence Similarity.
To appear in Proceed-ings of the 26st Conference of the Canadian Societyfor Computational Studies of Intelligence (CanadianAI?13).
Advances in Artificial Intelligence, Regina,SK, Canada.
Springer-Verlag Berlin Heidelberg.Eneko Agirre, Daniel Cer, Mona Diab, and AitorGonzalez-Agirre.
2012.
Semeval-2012 Task 6: APilot on Semantic Textual Similarity.
In Proceedingsof the 6th International Workshop on Semantic Eval-uation (SemEval 2012), in conjunction with the FirstJoint Conference on Lexical and Computational Se-mantics.Evgeniy Gabrilovich and Shaul Markovitch.
2007.
Com-puting Semantic Relatedness Using Wikipedia-basedExplicit Semantic Analysis.
In Proceedings of the20th International Joint Conference on Artificial In-telligence.Frane S?aric, Goran Glavas?, Mladen Karan, Jan S?najder,and Bojana Dalbelo Bas?ic.
2012.
TakeLab: Systemsfor Measuring Semantic Text Similarity.
In Proceed-ings of the 6th International Workshop on SemanticEvaluation (SemEval 2012), in conjunction with theFirst Joint Conference on Lexical and ComputationalSemantics.Ido Dagan, Oren Glickman, and Bernardo Magnini.2006.
The Pascal Recognising Textual EntailmentChallenge.
Machine Learning Challenges.
Evaluat-ing Predictive Uncertainty, Visual Object Classifica-tion, and Recognising Tectual Entailment.Jay J. Jiang and David W. Conrath.
1997.
Semantic Sim-ilarity Based on Corpus Statistics and Lexical Taxon-omy.
Proceedings of the 10th International Confer-ence on Research on Computational Linguistics.Lloyd Allison and Trevor I. Dix.
1986.
A Bit-StringLongest-Common-Subsequence Algorithm.
Informa-tion Processing Letters, 23(5).Mario Jarmasz and Stan Szpakowicz.
2003.
Rogets The-saurus and Semantic Similarity.
In Proceedings of theConference on Recent Advances in Natural LanguageProcessing.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA Data Mining Software: an Update.ACM SIGKDD Explorations Newsletter, 11(1).Matthew A. Jaro.
1989.
Advances in Record-LinkageMethodology as Applied to Matching the 1985 Censusof Tampa, Florida.
Journal of the American StatisticalAssociation.Philip Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcelo Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondr?ej Bojar.
2007.
Moses: OpenSource Toolkit for Statistical Machine Translation.
InProceedings of the 45th Annual Meeting of the ACL onInteractive Poster and Demonstration Sessions.
Asso-ciation for Computational Linguistics.Rada Mihalcea, Courtney Corley, and Carlo Strapparava.2006.
Corpus-based and Knowledge-based Measuresof Text Semantic Similarity.
In Proceedings of the Na-tional Conference on Artificial Intelligence.Ted Pedersen, Siddharth Patwardhan and Jason Miche-lizzi.
2004.
WordNet:: Similarity: Measuring theRelatedness of Concepts.
In Demonstration Papers atNorth American Chapter of the Association for Com-putational Linguistics: Human Language Technolo-gies.
Association for Computational Linguistics.William B. Dolan, Chris Quirk, and Chris Brockett.2004.
Unsupervised Construction of Large ParaphraseCorpora: Exploiting Massively Parallel News Sources.In Proceedings of the 20th International Conferenceon Computational Linguistics.
Association for Com-putational Linguistics.206
