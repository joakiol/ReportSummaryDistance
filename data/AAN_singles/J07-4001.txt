ACL Lifetime Achievement AwardWord PlayLauri Karttunen?Palo Alto Research CenterStanford UniversityThis article is a perspective on some important developments in semantics and in computationallinguistics over the past forty years.
It reviews two lines of research that lie at opposite ends ofthe field: semantics and morphology.
The semantic part deals with issues from the 1970s suchas discourse referents, implicative verbs, presuppositions, and questions.
The secondpart presents a brief history of the application of finite-state transducers to linguistic analysisstarting with the advent of two-level morphology in the early 1980s and culminating insuccessful commercial applications in the 1990s.
It offers some commentary on the relationship,or the lack thereof, between computational and paper-and-pencil linguistics.
The final sectionreturns to the semantic issues and their application to currently popular tasks such as textualinference and question answering.1.
PrologueThirty-eight years ago, in the summer of 1969 at the second meeting of COLING inSa?nga-Sa?by in Sweden, I stood for the first time in front of a computational audienceand started my talk on Discourse Referents by reading the following passage (Karttunen1976):Consider a device designed to read a text in some natural language, interpret it,and store the content in some manner, say, for the purpose of being able to answerquestions about it.
To accomplish this task, the machine will have to fulfill at least thefollowing basic requirement.
It has to be able to build a file that consists of records of allindividuals, that is, events, objects, etc., mentioned in the text and, for each individual,record whatever is said about it.
Of course, for the time being at least, it seems that sucha text interpreter is not a practical idea, but this should not discourage us from studyingin the abstract what kind of capabilities the machine would have to possess, providedthat our study provides us with some insight into natural language in general.The paper went on to discuss the circumstances that allow a pronoun or a definitedescription to refer to an object introduced by an indefinite noun phrase.
For example,in (1a), the pronoun It can refer to Bill?s car, but in (1b) it cannot.?
Palo Alto Research Center, 3333 Coyote Hill Road, Palo Alto, CA 94062, USA.
E-mail:karttunen@parc.com.
This article is the text of the talk given on receipt of the ACL?s LifetimeAchievement Award in 2007.?
2007 Association for Computational LinguisticsComputational Linguistics Volume 33, Number 4(1) a.
Bill has a cari.
Iti/The cari is black.b.
Bill doesn?t have a cari.
*Iti/*The cari is black.
1A year later in 1970, I gave my first ACL presentation at the 8th annual meeting inColumbus, Ohio.
The title of the invited talk was The Logic of English Predicate Comple-ment Constructions.
It started off with the following declaration (Karttunen 1971b):It is evident that logical relations between main sentences and their complements are ofgreat significance in any system of automatic data processing that depends on naturallanguage.
For this reason a systematic study of such relations, of which this paper is anexample, will certainly have a great practical value, in addition to what it maycontribute to the theory of the semantics of natural languages.The paper presented a classification of verbs and constructions that take sententialcomplements, that-clauses and infinitival complements, based on whether the sentencecommits the author to the truth or falsity of the complement clause.
For example, all thesentences in (2) imply, for different reasons, that the complement is true while all thesentences in (3) imply that the complement is false.2(2) a. John forgot that Mary was sick.
 Mary was sick.b.
Bill managed to solve the problem.
 Bill solved the problem.c.
Harry forced Ed to leave.
 Ed left.
(3) a. John pretended that Mary was sick.
 Mary was not sick.b.
Bill failed to solve the problem.
 Bill did not solve the problem.c.
Harry prevented Ed from leaving.
 Ed did not leave.Neither one of these two papers would have been accepted at this 2007 ACL conference.There was no implementation, no evaluation, and very little discussion of related work.In the happy childhood of computational linguistics even the most junior person in thefield, like myself, was allowed?even invited?to give a talk at the main COLING/ACLsession about uncharted linguistic phenomena.
It was a small field then.A future historian of the field might be puzzled by the 1969 and 1970 papers.They were written by a postdoctoral research associate at the University of Texas atAustin, who had arrived from Finland in 1964 by way of the University of Indiana atBloomington where he had just received a Ph.D. in Linguistics.
Where did the youngman acquire, and why was he spouting, that kind of computational rhetoric, when therecord shows that for the next ten years he never laid his hands on a computer?The fact is that I did have a brush with computational linguistics before settlingdown to do pure semantics in the 1970s.
I wanted to do linguistics because of SyntacticStructures (Chomsky 1957) and when the Uralic and Altaic Studies Department inBloomington offered me a job in 1964 as a ?native informant?
in Finnish I acceptedand managed to get into the Linguistics department as a graduate student.
My job titleturned out not to be accurate.
During my first two years in Bloomington I was teachingFinnish on my own for nine hours per week.
Luckily, I signed up for a course oncomputational linguistics taught by an excellent teacher and mentor, Robert E. Wall.
BobWall had participated in an early MT project at Harvard and in a project on automatic1 The subscripts i and j are referential indices.
Two noun phrases with the same referential index aresupposed to be coreferential, that is, they should refer to the same object.2 I am using the word imply as a generic cover term for entail, presuppose, and conventionally implicate.
Moreabout this in Sections 2.2 and 2.3.444Karttunen Word Playsummarization at IBM.
In his course, we learned formal language theory from notesthat eventually became a book (Wall 1972), a bit of Fortran and COMIT, a languagedeveloped by Victor Yngve at MIT.
I wrote a program on punched cards to randomlygenerate sentences from a small grammar of Finnish.
Thanks to Bob, I was rescued frommy indentured servitude in the Uralic and Altaic Studies.
In my third and final year inBloomington, I worked as a research assistant in the Computer Center with no specificduties other than to be a liaison to the Linguistics Department.
My only accomplishmentin that role was to save piles of anthropological data from obsolescence by writing aprogram to transform rolls of 5-channel paper tape to 6-channel magnetic tapes.
Bydoing that, I became one of the few linguists who could explain the joke, There are 10kinds of linguists: those who know binary and those who don?t.
I suspect that the data on mytapes for the Control Data 3600 computer have now been lost.
We still have the data onmanuscripts hundreds of years old but much of the content created in the first decadesof the computer age is gone forever.Just as I was starting to work on my dissertation in 1967, I had the good fortune ofgetting a one-year fellowship at the RAND corporation in Santa Monica, California, inthe group headed by David G. Hays, the author of the first textbook in our field (Hays1967), and the founder of the Association for Machine Translation and ComputationalLinguistics (AMTCL, the predecessor of our ACL).
The main focus of Hays?s team wasRussian-to-English machine translation.
Remarkably, Hays was also one of the authorsof the infamous 1966 ALPAC report that inexorably caused the shutdown of all gov-ernmentally funded MT projects, including the one at RAND.
Because the term machinetranslation had acquired a bad odor, the 1968 meeting of AMTCL dropped MT from itsname and became ACL.
At the 1970 meeting, the first one that I attended, people werestill bitterly arguing about the matter.In Hays?s group at RAND I again met Martin Kay, who had been my teacher in acourse on parsing at the 1966 Linguistic Institute at UCLA.
My term paper for Martin?scourse had been on the computational analysis of Finnish morphology, a topic towhich I would eventually return some fifteen years later.
Happy to become Martin?sstudent again, I learned Algol, an elegant new programming language, and got anunderstanding of the beauty of recursive algorithms.
Martin was running an excitingweekly colloquium series.
I teamed up with an intern by the name of Ronald Kaplanfor a small study project and we gave a joint presentation about our findings.
Ron andI agree that we did this together but neither one remembers what we said.
It probablywas about the similarities and differences between pronouns and logical variables, thetopic of my first published paper (Karttunen 1969b).At the time the prevailing assumption was that symbolic logic provides an appro-priate system for semantic representation within transformational grammar (McCawley1970).
But as I showed in the 1969 CSL paper, even cases as simple as (4a) and (4b) couldnot be treated adequately within the proposed framework.
(4) a.
The mani who loved hisi wifej kissed herj.b.
I gave each student a cookiei.
Some of them ate iti right away.c.
The piloti who shot at itj hit the Migj that chased himi.The problem with (4a) is that the phrase hisi wife has to be treated in situ as it cannotbe replaced by another coreferential noun phrase, say Mary, without changing themeaning.
(4a) implies that only one man in some group of men loved his wife, which isnot the same as there being just one man who loved Mary even if the two noun phrasespick out the same individual.
For this reason there was no way in a system such asMcCawley?s to link hisi wifej to herj.
In the case of (4b), the problem is that there is no445Computational Linguistics Volume 33, Number 4unique cookie to serve as the referent of it.
Being an anaphoric pronoun linked to anantecedent does not necessarily mean that the two are coreferential, at least not in anynaive sense of coreference.Even worse problem cases were known.
(4c) is a typical ?Bach-and-Peters sen-tence,?
named after its inventors, Emmon Bach and Stanley Peters.
As I was goingto devote a chapter of my dissertation to this topic, I was lucky to run into them at aconference in San Diego.
I found the two very intimidating in their suits and crew cuts.They looked like Haldeman and Ehrlichman, a pair of Nixon aides.
But at least I foundout that the problem had not been solved.As the year at RAND when on, I spent less and less time at the computer and a lotof time walking up and down the Santa Monica pier just down the cliff from my officethinking about pronouns, variables, reference, and definiteness.
I went up to UCLA afew times to discuss these issues with Barbara Partee and gave a talk in her seminar.The topic of my dissertation was Problems of Reference in Syntax, in principle due beforeI left RAND but finished half-a-year later (Karttunen 1969a).By the summer of 1968 I had two job offers.
David Hays was leaving RAND for SUNYin Buffalo and he offered me a job there.
But I chose to become a Faculty Associatein the Linguistics Department at the University of Texas at Austin.
Climate was oneconsideration, but, more importantly, Austin was where Emmon Bach and StanleyPeters were.
My Indiana mentor, Bob Wall, had just moved into the same department.For the next ten years I had very little contact with Martin Kay and Ronald Kaplan butthey became very important people in Act II of my life.2.
Act I: Framing ProblemsI started my career in Austin in the fall of 1968 and became a regular faculty memberin 1970.
My work on discourse referents was largely done when I arrived in Austin.I went on to study so-called implicative verbs such as manage and fail, a subtopic inthe discourse referents paper, and branched to other types of verbs that take sententialcomplements.
One important semantic class of verbs with sentential complements,called factives, had already been identified and discussed by Zeno Vendler (1967) andPaul and Carol Kiparsky (Kiparsky and Kiparsky 1971) at MIT.
Factive verbs were saidto presuppose that the complement clause is true.As it happened, I started to think about factives and presuppositions at MIT in theFall of 1972.
In the spring before I had a surprise phone call from Paul Kiparsky whosaid that the MIT Department was still looking for a one-year replacement for DavidPerlmutter who was going on a sabbatical.
Would I be interested?
Of course I was.
I hadcome to the U.S. seven years earlier to study linguistics because of Noam Chomskyand now I had an office just across from hall from his.
But during the year I wasthere, I lost interest in transformational syntax.
I found Chomsky?s Thursday lecturesof that year, on themes later published as Conditions on Transformations (Chomsky 1973),uncompelling.My sense of what was interesting had changed.
The ?Linguistic Wars?
(Harris1995) between generative (George Lakoff, John Ross, James D. McCawley, Paul Postal,and others) and interpretive semantics (Ray Jackendoff and others) had been won byChomsky for the interpretivists, although Chomsky himself was, and still is, skepticalof any kind of formal theory of meaning.
My sympathies were with the losing side.But I sensed that both camps were essentially doing syntax, albeit in different ways.Barbara Partee had convinced me in our discussions about pronouns and variables atUCLA that model theory and intensional logic was the right approach to semantics.
But446Karttunen Word Playit was going to take a while before I could do anything original within that emergingparadigm.
At MIT I gave a ?formal methods?
course for a few linguistic students startingwith Bob Wall?s textbook (Wall 1972) and finishing with Montague Grammar that I wasjust learning about myself (Montague 1970a, 1970b, 1973; Partee 1995) and a seminar onmy own topics: discourse referents, implicative verbs, and presuppositions.
I had onestar student in the seminar by the name of Mark Liberman, who wrote a Master?s Thesispoking holes in my emerging ideas about presuppositions.In the 1970s, the Linguistics Department in Austin was an excellent place for ayoung semanticist.
I learned tremendously from my colleagues there: Emmon Bach, LeeBaker, Stanley Peters, Carlota Smith, and Robert Wall.
We had some excellent semanticsand syntax students.
David Dowty, Per-Kristian Halvorsen, Roland Hausser, OrvokkiHeina?ma?ki, Jim McCloskey, and Hans Uszkoreit got their degrees from UT Austin whileI was there.
Orvokki was my first Ph.D. student.
She wrote an insightful thesis on themeaning of before and other temporal connectives (Heina?ma?ki 1974).3In the 1970s I worked on four general topics: discourse referents, implicative verbs,presuppositions, and questions.
This is not an occasion to deep-end into any of thesetopics but I will discuss each of them briefly in the following sections to give a generalidea of what I think my contributions were.2.1 Discourse ReferentsThe obvious difference between definite and indefinite noun phrases is that garden-variety definite NPs such as the car in simple main clauses imply the existence of anindividual or an object but indefinite noun phrases such as a car often do not.
In thatrespect, definite NPs are similar to definite pronouns such as it in contexts where thepronoun does not play the role of a bound variable.
The reason for the incoherence of(1b) is that it tells us explicitly that there is no such car.Given an indefinite noun phrase, when is there supposed to be a correspondingindividual that it describes?
This was the question I tried to answer in the 1969 paperon discourse referents, excerpted from the first chapter of my Indiana dissertation.
Theconclusion I came to was that a pronoun or a definite description could refer back to(or be an anaphor for) an indefinite NP (the antecedent) just in case the existence ofthe individual was semantically implied by the text.
Put in this simple way, the answerseems obvious but it gave rise to many problems some of which remain unsolved tothis day.
The novelty of the approach was that it rephrased the problem of pronom-inalization in semantic terms.
Up to that point in transformational grammar, all thediscussion about anaphors and antecedents had been about the constraints on theirsyntactic configurations.Simple affirmative sentences such as Bill has a car in (1a) obviously imply existenceand simple negative sentences such as Bill doesn?t have a car in (1b) imply the opposite.The type of the verb matters, as seen in (5).
(5) a.
The director is looking at an innocent blondei.
Shei is from Bean Blossom.b.
The director is looking for an innocent blondei.
#Shei is from Bean Blossom.So-called intensional verbs such as look for, need, and want introduce an ambiguity.In (5b), the phrase an innocent blonde may be understood in two ways.
In the specificsense it describes a particular individual that we can refer to as she.
But (5b) can also be3 As I was preparing this talk, I heard the sad news from Finland that Orvokki Heina?ma?ki had died.447Computational Linguistics Volume 33, Number 4interpreted nonspecifically, describing the type of girl the director is looking for.
In thatsense, the continuation is incoherent because there is not yet any individual to refer to.
(5a) has no such ambiguity; it entails the existence of an innocent blonde in the actualworld and we can talk about her.But matters are more complicated.
Although (5b) under a nonspecific reading ofan innocent blonde does not establish a discourse referent in the actual world, we cannevertheless have one in a modal or hypothetical context, as in (6).
(6) The director is looking for an innocent blondei.
Shei must be 17 years old.There is another problem here.
If we interpret an innocent blonde nonspecifically in (6),then must has a deontic reading.
It is a requirement that she be 17 years old.
However,on the specific reading must gets an epistemic interpretation.
That is, we have made aninference about the age of the girl in question from her looks or other evidence.My work on discourse referents was a harbinger of the vast literature yet to come onthis topic including Bonnie Webber?s 1978 dissertation (Webber 1978), Irene Heim?s filechange semantics (Heim 1982), and the theory of discourse representation structures(DR(S) Theory) proposed by Hans Kamp (1981) and Uwe Reyle (Kamp and Reyle 1993).Looking back at my old paper, I am amused by the youthful innocence with which itapproached the topic but I am also impressed by the fact that some of the problems ituncovered, such as the deontic/epistemic contrast in (6), apparently remain unsolved.2.2 Semantics of ComplementationAn indefinite noun phrase creates a stable discourse referent just in case the clause itis bound to is implied to be true by the context in which it appears.4 That was themain idea in the 1969 paper.
In the course of seeking evidence for this thesis, I cameacross an interesting class of verbs and constructions that give rise to such implications(Karttunen 1971a, 1971b).
For example, the contrast between (7a) and (7b) is explainedby the semantic properties of the two verbs, manage and fail.
(7) a. John managed to get a sabbaticali.
Iti starts in September.b.
John failed to get a sabbaticali.
*Iti starts in September.
(7a) entails that John got a sabbatical, (7b) entails that he didn?t.
The interesting factabout these verbs is that when we change the polarity from positive to negative we stillget an entailment, but of the opposite polarity as seen in (8).
(8) a. John didn?t manage to get a sabbaticali.
*Iti starts in September.b.
John didn?t fail to get a sabbaticali.
Iti starts in September.There exists quite a number of such two-way implicatives that yield an entailment inboth positive and negative contexts.
Verbs like manage yield a positive entailment inpositive contexts (++) and a negative entailment in negative contexts (??).
Let us callthem ++/??
implicatives.
Two-way implicatives like fail flip the polarity, so we callthem +?/?+ implicatives.
Table 1 gives a few examples of both types of verbs andconstructions.4 The term ?stable?
is in contrast with ?short-term?
for referents that have a limited life span.
For example,we can talk about a nonexistent car as in in I wish Mary had a cari.
She could take me to work in iti.
I coulddrive the cari too.
as long as we are elaborating a hypothetical situation.448Karttunen Word PlayTable 1Some two-way implicative verbs and constructions.++ /??
implicatives +?
/?+ implicativesmanage (to) fail(to)succeed (in) neglect (to)remember (to) forget (to)happen (to) avoid .
.
.
(ing)see fit (to) refrain (from)take the time .
.
.
(to) shy away (from)have the foresight (to) stop NP (from)Table 2Some one-way implicative verbs and constructions.++ implicatives +?
implicatives ??
implicatives ?+ implicativescause NP (to) prevent NP (from) can hesitate (to)force NP (to) preclude NP (from) be able (to)Table 2 contains examples of verbs and constructions that certainly yield an entail-ment in one direction but not necessarily the other way.For example, it is tempting to conclude from (9a) that the president attended themeeting?and if there is no reason to think otherwise, the reader is entitled to thatconclusion.
Nevertheless, the author may take away that ?invited inference?
(Geis andZwicky 1971) without contradicting himself as in (9b).
(9) a.
The president was able to attend the meeting.b.
The president was able to attend the meeting but decided not to.The entailments of constructions involving more than one implicative verb have tobe computed from ?top?down.?
The two examples in (10) establish a stable discoursereferent because they both entail that a picture was taken.
(10) a. John managed not to forget to take a picture.b.
Bill failed to prevent John from taking a picture.The early version of Kamp?s Discourse Representation Theory did not include anymechanism for computing lexical entailments about existence.
I found the DRS boxesdisappointingly static at the time.The semantics of complementation that I proposed was picked up by some compu-tational linguists.
Among the early adopters were Joshi and Weischedel (1973).
RalphWeischedel?s Ph.D. dissertation (Weischedel 1975) showed that useful inferences canbe computed directly by the parser, in contrast to the then prevailing view of the AIcommunity that all inferences have to come from some giant inference engine.
Thiswas the starting point of Jerrold Kaplan?s work on ?cooperative responses?
in databasesystems (Kaplan 1977).2.3 Presuppositions?Conventional ImplicaturesThe semantics of two-way implicatives puzzled me greatly when I first discovered them(Karttunen 1971a).
If the entailments in (11) both hold, in standard logic it would follow449Computational Linguistics Volume 33, Number 4that the construction manage to is empty of meaning.
In general, if p entails q and ?pentails ?q, then it logically follows that p and q are equivalent: p ?
q.
(11) a. John managed to speak.
 John spoke.b.
John did not manage to speak.
 John did not speak.But this is of course wrong as far as (11) is concerned.
Choosing the construction manageto commits the speaker to the view that there was some difficulty involved.
All theverbs in Table 1 bring in some additional commitment over and beyond what is entailedalthough it is difficult in some cases to pin down exactly what it is.
Furthermore, thecommitment remains the same regardless of whether the sentence is affirmative ornegative.
It is also present in questions and conditionals as shown in (12).
(12) Did John manage to speak?If John managed to speak, it is a good sign.The extra bits of meaning attached to the two-way implicatives were yet another in-stance of a phenomenon that had already been discussed for some time under the termpresupposition.
The term came from philosophers who had been debating heatedlyand for a long time whether The present king of France is false, meaningless, or lackinga truth value (Frege 1892; Russell 1905; Strawson 1950; Russell 1957; Strawson 1964).When linguists got into the act in the late 1960s, being more systematic observers oflanguage, within a span of just a few years they collected a large zoo of other typesof constructions besides definite descriptions that seem to involve presuppositions(Fillmore 1971; Keenan 1971; Kiparsky and Kiparsky 1971).
Unfortunately, they did notsort them into different habitats.
Here are some examples: Factive verbs: Mary forgot/didn?t forget that John had left. Factive adjectives: It is/isn?t odd that the room is closed. Change-of-state verbs: John stopped/hasn?t stopped smoking. Verbs of judging: John criticized/didn?t criticize Harry for writing the letter. Wh-questions: Who is coming for dinner? Headless relatives: Chicago is/isn?t where Fred met Sally. Cleft sentences: It was/wasn?t John who caught the thief. Pseudo-clefts: What she wants/doesn?t want to talk about is herself. Temporal subordinate clauses: John left/didn?t leave after Mary called. Iteratives: Fred called/didn?t call again.
Fred ate/didn?t eat another turnip.In addition to vastly enlarging the presupposition population, the linguistic communityalso came up with a problem that had been ignored in the philosophical literature up tothat point:Projection problem: How are the presuppositions of a complex sentence derivedfrom the presuppositions of the component clauses?450Karttunen Word PlayThis question was first posed by Langendoen and Savin (1971).
Their answer was(page 57):The projection principle for presuppositions, therefore, is as follows: presuppositions ofa subordinate clause do not amalgamate either with presuppositions or assertions ofhigher clauses; rather they stand as presuppositions of the complex sentence in whichthey occur.They were badly mistaken.
Although the consequent clause of (13) by itself presupposesthe existence of a unique king of France, (13) as a whole obviously does not.
(13) If France has a king, I bet the king of France speaks only French.In a conditional sentence, a presupposition of the consequent clause can be ?filtered?
or?cancelled?
away if it is entailed by the antecedent and general background knowledge.If a presupposition is not filtered locally and is not part of the context of the discourse,the reader or hearer must in some way adjust his or her state of knowledge to incorpo-rate the new information.
The idea is in Karttunen (1974, page 191):If the current conversational context does not suffice, the listener is entitled andexpected to extend it as required.
He must determine for himself what context he issupposed to be in on the basis of what is said and, if he is willing to go along with it,make the same tacit extension that his interlocutor appears to have made.Lewis (1979) called this process accommodation.
There is a huge literature on theprojection problem and accommodation.
Among the papers often cited are Karttunen(1973), Stalnaker (1973), Karttunen (1974), Karttunen and Peters (1979), Gazdar (1979),Lewis (1979), Soames (1982), Heim (1983), van der Sandt and Geurts (1991), van derSandt (1992), Zeevat (1992), Beaver (1995), Geurts (1999), and Kamp (2001).
Geurts(1999, page 5) sums up the early developments as follows:An especially stark illustration of the disparity of the field, at least in its early days, isthe work of a Karttunen, who within the span of six years published three theories thatwere mutually inconsistent, technically as well as conceptually.I don?t disagree with that assessment.5 It seems to me that by now the notions of pre-supposition projection and accommodation have outlived their usefulness.
It is evidentthat no uniform theory can account for all the phenomena that historically have beenlumped together under the label presupposition.But at least one good insight has emerged from this line of research.
The accommo-dation strategy for definite descriptions is closely linked to anaphora resolution (van derSandt 1992).
One motivation for Kamp?s DRS theory was to be able to handle ?donkeyanaphora?
in sentences such as (14a).
(14) a.
If John has a donkeyi, he beats iti.b.
If John has children, all of John?s children are bald.What van der Sandt observed was that the treatment of the anaphor in (14a) could beused in (14b) to eliminate the presupposition that John has children.5 Except for the dismissive Gricean implications triggered by the indefinite article in a Karttunen.451Computational Linguistics Volume 33, Number 4Assimilating presupposition projection into anaphora resolution is probably theright approach for definite descriptions and for iterative presuppositions triggered byprefixes such as re- in verbs like recalculate and particles such as too and again.
However,it does not seem applicable to the kinds of presuppositions triggered by implicativeverbs or factives.But the whole idea of accommodation is inappropriate for implicative verbs.
Exam-ples such as (11) and (12) commit the speaker to the view that it was difficult for Johnto speak.
The audience may take note of that piece of information but it does not needto be accepted or accommodated for the discourse to proceed.
Another phenomenonthat does not call for any accommodation is it-clefts.
As Ellen Prince (1978) showed, asentence such as (15) does not covertly slip into the discourse a piece of new informationdisguised as being old.
On the contrary, the rhetorical force of the it-cleft is to tell yousomething that presumably you did not know before in a manner that makes the newpiece of information incontestable.
(15) It was/wasn?t Barbara Partee who in a private conversation around1980 suggested to me that anaphora resolution and the satisfactionof the presuppositions of definite descriptions was the same problem.In my joint last paper on presuppositions (Karttunen and Peters 1979), StanleyPeters and I proposed to do the sensible thing, namely to divide up the heterogeneouscollection of phenomena that had been lumped together under this misbegotten label.We suggested that many cases that had been called presupposition are best seen asinstances of what Grice (1979) had called conventional implicature.
Conventional im-plicatures are propositions that the speaker or the author of the sentence is committedto by virtue of choosing particular words or constructions to express himself or herself.However, whether those implicatures are true or not does not have any bearing onwhether the sentence is true or false.
For example, because of the word even, (16)commits the author to the view that Bill is an unlikely person to agree with Mary.
(16) Even Bill agrees with Mary.But the meaning contributed by even plays no role in determining the truth conditionsof the sentence.
(16) is true if Bill agrees with Mary and false otherwise.Our good advice went unheeded for a long time but in recent work by ChristopherPotts (2004) we see an attempt to build the sort of two-dimensional semantics Stanleyand I sketched out that separates conventional implicatures from truth-conditionalaspects of meaning.2.4 Syntax and Semantics of QuestionsMy paper on questions (Karttunen 1977) was an ambitious effort to give a unifiedaccount in the framework of Montague Grammar of the meaning of all types of inter-rogative phrases including direct questions such as the examples in (17) and embeddedinterrogatives illustrated in (18).
(17) a.
Is it raining?b.
Do you want to go or do you want to stay?c.
Which book did Mary read?d.
Which girls date which boys?452Karttunen Word Play(18) a. John knows whether Bill smokes.b.
Mary is thinking about whether to stay home or go to the movies.c.
Bill remembers to whom John gave the book?d.
She doesn?t care about who did what to whom.Examples (17a) and (18a) are yes/no questions.
(17b) and (18b) are alternative ques-tions that pose two or more choices.
As (17c,d) and (18c,d) illustrate, wh-questions maycontain any number of interrogative quantifiers.By and large, embedded yes/no and alternative questions have the same syntacticdistribution as embedded wh-questions.6 For that reason a syntactician would preferto have just a single category of embedded questions.
In the framework of MontagueGrammar this is possible only if all types of embedded interrogatives have the sametype of meaning.
From a semantic point of view, it would be desirable to assign asingle type of meaning to both direct and embedded questions.
For example, whichgirls date which boys should have the same meaning as a direct question that it haswhen embedded under a verb such as find out.
Finally, whatever meaning we assignto interrogatives, it should help us to elucidate the meaning of question-embeddingverbs including the examples in (18) and the one in (19) that sets up a relation betweentwo questions.
(19) Whether Mary comes to the party depends on who invites her.With these desiderata in mind, I came to the conclusion that the best solution wouldbe to adopt an approach proposed by Hamblin (1973) for direct questions and carry itfurther.
Hamblin?s idea was to let every direct question denote a set of propositions,namely, the set of propositions expressed by all the possible answers to the question.For example, under Hamblin?s analysis Is it raining?
denotes the set containing twopropositions {It is raining, It is not raining}.
My improvement of that idea was to makethe meaning of a question be a function that in each possible world picks up the set oftrue answers to the question.Under this new analysis it is possible to relate, for example, the meaning of knowwith a that-complement to the meaning of know with an embedded question as in (18a).If Bill in our actual world is a smoker, then in our world whether Bill smokes picks outthe set consisting of the proposition that Bill smokes.
In that case, what John knows isthat Bill smokes.
In examples such as (19) the meaning of depend on can be explicatedas a function that in each world maps the true answers to who invites Mary onto thetrue answer(s) to whether Mary comes to the party.
I worked out these ideas with all therigor of a Montague grammarian.
After years of apprenticeship I had finally become acompetent formal semanticist.7A less restless soul would have stopped right there.
But I didn?t.
As others sawit, I fell from the pinnacle of semantics into the low life of finite-state automata.
Mysemanticist friends kept asking, ?What happened to you Lauri?
You were such a goodsemanticist.?
The politely unstated premise was that I had fallen onto skid row.6 There are two yet unexplained exceptions to this generalization.
So-called ?emotive factives?
such asbe surprised take embedded wh-questions but not whether-questions: You?d be surprised where you find us,*You?d be surprised whether you find us.
?Dubitative?
verbs such as doubt have the opposite characteristic.7 Compared to the lively activity on the presupposition playground, the field of questions attracts fewvisitors.
For later developments, see Hausser and Zaefferer (1979), Hausser (1983), Groenendijk andStokhof (1984), and Ginsburg (1992, 1996).453Computational Linguistics Volume 33, Number 43.
InterludeTowards the end of the 1970s I began to think that I had stumbled on, and helped tocreate and frame, more problems in semantics than I could ever solve.
It was time tomove on and leave the mess for others to clean up.
In a bold move I signed up to teacha course on computational linguistics.
As every professor knows, teaching a courseon something you know next to nothing about is a great learning opportunity.
To getsome idea of how the field had developed in the previous ten years I went to the 1979ACL Annual Meeting in La Jolla, California, and immediately ran into two of my oldcolleagues from RAND, Martin Kay and Ron Kaplan, very surprised to see me.
?Whatare you doing here??
they asked.
I said I had picked up a new hobby and was planningto do some computational work on Finnish morphology, the topic of my term paper forMartin?s course in 1966.On my sabbatical year at the Center for Advanced Study in Behavioral Sciences(CASBS) at Stanford, 1981?1982, I often went to visit Martin at the Palo Alto ResearchCenter (PARC) just a short drive from CASBS.
I learned about unification and InterLisp.I got to compute on the Alto personal computer and even had my own personal 1 MBfloppy for it, about the size of a large briefcase.
On the floppy was the project Martinand I were collaborating on, a unification based parser/generator for Finnish.
Finnishwas a good test case for Martin?s functional unification grammar (FUG) formalism.
InFUG, constituents could be labeled by a syntactic category such as NP and could beassigned functional roles such as CONTRAST and TOPIC.
We showed how the constraintson Finnish word order could be described and implemented in those terms (Karttunenand Kay 1985a).I joined the Artificial Intelligence Center at SRI in 1984.
It was a good time tomake the move from academia to industrial research.
SRI had an excellent mix ofcomputational linguists and AI people: Barbara Grosz, Jerry Hobbs, David Israel, RobertMoore, Fernando Pereira, Ray Perrault, Stuart Shieber, and Hans Uszkoreit were there,among others.
SRI?s AI Center and Xerox PARC were cofounders of the new Center forthe Study of Language and Information (CSLI) at Stanford, funded by a generous grantfrom the System Development Foundation, an offshoot of the RAND Corporation.
AtSRI, Stuart Shieber had designed and implemented his influential PATR II formalismfor unification-based grammars (Shieber et al 1983).
I implemented it (Karttunen 1984;Karttunen and Kay 1985b; Karttunen 1986) at CSLI in Interlisp on a Xerox Dandelion, awonderful machine with Interlisp as the language of the operating system.In 1987 I joined my friends at Xerox PARC to concentrate on my other computationalinterest: finite-state morphology.
In making the crosstown transit from Menlo Park toPalo Alto, I graduated from my lovely Dandelion to the top-of-the-line Xerox Dorado,still the best computing experience in my life.
After all the years spent on theorizingand playing with formalisms, I wanted to do something practical that would have animpact on the real world.4.
Act II: Providing SolutionsIn the early 1980s, morphological analysis of natural language was a challenge tocomputational linguists.
Simple cut-and-paste programs could be written to analyzestrings in particular languages, but there was no general language-independent methodavailable.
Furthermore, cut-and-paste programs for analysis were not reversible, theycould not be used to generate words.454Karttunen Word PlayGenerative phonologists of that time described morphological alternations bymeans of ordered rewrite rules introduced by Chomsky and Halle (1968).
These rulesare of the form ?
?
?
/ ?
?, where ?, ?, ?, and ?
can be arbitrarily complex strings orfeature matrices.
It was not understood how such rules could be used for analysis.In 1981 I had organized a conference on parsing in Austin.
Present at the conferencewas a visitor from Finland, Kimmo Koskenniemi, who was looking for a dissertationtopic.
Martin Kay and Ronald Kaplan were also there and it turned out that all four ofus were interested in morphology.
I demoed a small system I had built with my studentsfor Finnish (Karttunen, Uszkoreit, and Root 1981).
Martin and Ron reported that theyhad recently made a breakthrough discovery in computing with rewrite rules.
Kimmowent on to California to visit them at PARC to learn more.
That was the beginning of ourlong collaboration.4.1 OriginsThe discovery Kaplan and Kay had made was actually a rediscovery of a result that hadbeen published a decade before in a book that none of us knew about at that time, aUC Berkeley dissertation by C. Douglas Johnson (1972).
Johnson observed that althoughthe same context-sensitive rule could be applied several times recursively to its ownoutput, phonologists have always assumed implicitly that the site of application movesto the right or to the left in the string after each application.
For example, if the rule?
?
?
/ ?
?
is used to rewrite the string ???
as ??
?, any subsequent application of thesame rule must leave the ?
part unchanged, affecting only ?
or ?.
Johnson demonstratedthat the effect of this constraint is that the pairs of inputs and outputs produced by aphonological rewrite rule can be modeled by a finite-state transducer.8Johnson was already aware of an important mathematical property of finite-statetransducers established by Schu?tzenberger (1961): for any pair of transducers appliedsequentially there exists an equivalent single transducer.
Any cascade of rule trans-ducers can in principle be composed into a single transducer that maps lexical formsdirectly into the corresponding surface forms, and vice versa, without any intermediaterepresentations.Koskenniemi was impressed by the theoretical result he learned from Kaplan andKay, but not convinced about the practicality of the approach for morphological analysis.Traditional phonological rewrite rules describe the correspondence between lexicalforms and surface forms as a one-directional, sequential mapping from lexical forms tosurface forms.
Even if it were possible to model the generation of surface forms efficientlyby means of finite-state transducers, it was not evident that it would lead to an efficientanalysis procedure going in the reverse direction, from surface forms to lexical forms.Let us consider a simple illustration of the problem with two sequentially appliedrewrite rules, N -> m / p and p -> m / m .
The corresponding transducers mapthe lexical form kaNpat unambiguously to kammat, with kampat as the intermediaterepresentation.
However if we apply the same transducers in the opposite directionto the input kammat, we get the three results shown in Figure 1.
This asymmetry is an8 Johnson did a careful analysis of what at the time was one of the most comprehensive descriptions ofphonological alternations described in the Chomsky?Halle paradigm.
This was the unpublished MITQualifying Paper by James D. McCawley on Finnish.
The data came from McCawley?s classmate,Paul Kiparsky, a native speaker of the language.
One can claim that every advance in computationalmorphology in the last 30 years involves Finnish and people whose last name begins with K. SeeSection 4.3.455Computational Linguistics Volume 33, Number 4Figure 1Deterministic generation, nondeterministic analysis.inherent property of the generative approach to phonological description.
If all the rulesare deterministic and obligatory and if the order of the rules is fixed, each lexical formgenerates only one surface form.
But a surface form can typically be generated in morethan one way, and the number of possible analyses grows with the number of rules thatare involved.4.2 Two-Level MorphologyBack in Finland, Koskenniemi invented a new way to describe phonological alterna-tions in finite-state terms.
Instead of cascaded rules with intermediate stages and thecomputational problems they seemed to lead to, rules could be thought of as statementsthat directly constrain the surface realization of lexical strings.
The rules would not beapplied sequentially but in parallel.
Each rule would constrain a certain lexical/surfacecorrespondence and the environment in which the correspondence was allowed, re-quired, or prohibited.
For his 1983 dissertation, Koskenniemi (1983) constructed aningenious implementation of his constraint-based model that did not depend on a rulecompiler, composition, or any other finite-state algorithm, and he called it two-levelmorphology.
Two-level morphology is based on three ideas: Rules are symbol-to-symbol constraints that are applied in parallel, notsequentially like rewrite rules. The constraints can refer to the lexical context, to the surface context, or toboth contexts at the same time. Lexical lookup and morphological analysis are performed in tandem.Applying the rules in parallel does not in itself solve the overanalysis problem illus-trated in Figure 1.
The two constraints just sketched allow kammat to be analyzed askaNpat, kampat, or kammat.
However, the problem becomes manageable when thereare no intermediate levels of analysis.
In Koskenniemi?s 1983 system, the lexicon wasrepresented as a forest of tries (= letter trees), tied together by continuation-class linksfrom leaves of one tree to the root of another tree or trees.9 Lexical lookup and the9 The TEXFIN analyzer I had demoed to Koskenniemi on his 1981 visit to Austin had the same lexiconarchitecture (Karttunen, Uszkoreit, and Root 1981) .456Karttunen Word PlayFigure 2Following a path in the lexicon.analysis of the surface form are performed in tandem.
In order to arrive at the pointshown in Figure 2, the analyzer has traversed a branch in the lexicon that containsthe lexical string kaN. At this point, it only considers symbol pairs whose lexical sidematches one of the outgoing arcs of the current state.
It does not pursue analyses thathave no matching lexical path.
All the rule networks must accept every lexical:surfacepair.
In the case at hand, the p:m pair is accepted by the N:m Rule that requires a p as theright context on the lexical side and by the p:m Rule that requires an m as the left contexton the surface side.
In two-level rules, zero (epsilon) is treated as an ordinary symbol.Because of this, a two-level rule represents an equal-length relation.
Conceptually, thesystem in Figure 2 simulates the intersection of the rules and the composition of therules with the lexicon.Koskenniemi?s two-level morphology was the first practical general model in thehistory of computational linguistics for the analysis of morphologically complex lan-guages.
The language-specific components, the rules and the lexicon, were combinedwith a universal runtime engine applicable to all languages.4.2.1 The Texas KIMMO System.
I met Koskenniemi again in Finland around Christmastime in 1982.
He had just finished the first implementation of a two-level system andgave me a printout of the program to take along, a thick stack of Pascal code.
Backhome I unfolded the long printout on the floor of a corridor and spent quite a bit of timecrawling up and down the code trying to understand what it did, and learning Pascalalong the way.
I was going to teach computational linguistics again in the spring.
Hav-ing figured out Kimmo?s program, it occurred to me that doing a Lisp implementationof the two-level model would be a good class project.We completed the project and published a collection of papers on the topic, alongwith our Lisp code (Gajek et al 1983; Karttunen 1983).
To make sure that Koskenniemigot the credit for the invention, we called it the KIMMO system.
The name stuck andinspired many other KIMMO implementations.
The most popular of these is PC-KIMMO,a free C implementation from the Summer Institute of Linguistics (Antworth 1990).In Europe, two-level morphological analyzers became a standard componentin several large systems for natural language processing such as the British Alveyproject (Black et al 1987; Ritchie et al 1987, 1992), SRI?s CLE Core Language Engine(Carter 1995), the ALEP Natural Language Engineering Platform (Pulman 1991), and theMULTEXT project (Armstrong 1996).4.2.2 A Compiler for Two-Level Rules.
In his dissertation Koskenniemi (1983) introduceda formalism for two-level rules.
The semantics of two-level rules was well-defined457Computational Linguistics Volume 33, Number 4but there was no rule compiler available at the time.
Koskenniemi and other earlypractitioners of two-level morphology constructed their rule automata by hand.
This istedious in the extreme and very difficult for all but very simple rules.To address this problem Kaplan, Kay, and I pooled our CSLI funds and invitedKoskenniemi to Stanford in the Summer of 1985.
Although two-level rules are concep-tually quite different from the rewrite rules studied by Kaplan and Kay, the methodsthat had been developed for compiling rewrite rules were applicable to two-level rulesas well.
In both formalisms, the most difficult case is a rule where the symbol thatis replaced or constrained also appears in the context part of the rule.
This problemKaplan and Kay had already solved by an ingenious technique for introducing andthen eliminating auxiliary symbols to mark context boundaries.
Another fundamentalinsight they had was the encoding of context restrictions in terms of double negation.For example, a constraint such as ?p must be followed by q?
can be expressed as ?it isnot the case that something ending in p is not followed by something starting with q.?In Koskenniemi?s formalism, p => q.In the course of the summer, Kaplan and Koskenniemi worked out the basic com-pilation algorithm for two-level rules.
The first two-level rule compiler was writtenin InterLisp by Koskenniemi and me in 1985?1987 using Kaplan?s implementationof the finite-state calculus (Koskenniemi 1986; Karttunen, Koskenniemi, and Kaplan1987).
The current C-version two-level compiler, called TWOLC, was created at PARC(Karttunen and Beesley 1992).
It has extensive systems for helping the linguist to avoidand resolve rule conflicts, the bane of all large-scale two-level descriptions.4.2.3 Two-Level Descriptions.
Many languages have been described morphologically inthe two-level framework.
But in many cases the work has been done for companiessuch as Lingsoft and Inxight that are in the morphology business, and the descrip-tions have not been made public for obvious reasons.
Here are some of the languagesfor which there is a large-scale two-level grammar and a publication describing it:Finnish (Koskenniemi 1983), Estonian (Uibo 2006), German (Schiller 1996), NothernSa?mi (Moshagen, Sammallahti, and Trosterud 2006), and Turkish (Oflazer 1994).4.3 Lexical TransducersSoon after arriving at PARC I made a serendipitous discovery.
At the time PARC wascollaborating with Microlytics, a company that marketed spell-checkers, the first suc-cess story of finite-state morphology.10 Microlytics had licensed from Koskenniemi?scompany, Lingsoft, the rights to the Finnish analyzer.
I was asked to extract from theLingsoft two-level analyzer a network of surface forms that could be fed to Kaplan?scompression routine to make a Finnish spell-checker in the Microlytics format.
Forthat task I designed an algorithm that simultaneously carried out the intersection ofKoskenniemi?s 23-rule automata and the composition with the lexicon.
I was surprisedto see that not only did it work but the result was not significantly larger than theoriginal source lexicon.
Figure 3 is a sketch of that process.
Just intersecting the ruleautomata by themselves was barely possible for us then because of the exponentialworst-case complexity of the intersection algorithm.
We assumed that the compositionwith a large lexicon might make the computation even harder to carry out.
In fact the10 Ron Kaplan will tell you more about that some day.458Karttunen Word PlayFigure 3Intersecting and composing two-level rules with a lexicon.Figure 4A path in a lexical transducer.opposite happened.
The reason should have been obvious from the beginning.
Theintersection of a set of two-level rules explodes because it has to compute a result for anylexical string.
But if the set of lexical inputs is restricted to the forms that actually existin the language, there is no blowup.
The same applies to the composition of transducersderived from rewrite rules.
If the rule cascade is computed starting with the lexicon, the?overanalysis?
problem illustrated in Figure 1 never arises.The surface forms that Microlytics wanted for the Finnish spell-checker were easilyextracted from the transducer, but we realized that keeping the lexical forms and theirsurface realizations in a single network would be even more valuable.
I created atransducer for English with a small number of two-level rules.
It consisted of mappingssuch as in Figure 4.
Annie Zaenen and Carol Neidle created, with a large numberof rules, a much more ambitious proof-of-concept, a lexical transducer for French,mapping lemmas such as vouloir+Verb+IndP+Sg+P3 to the corresponding surface formveut.
Such a transducer is the ultimate ?two-level model?
for a language as it compactlyencodes all the lemmas (lexical forms with morphological tags), all the inflected surface forms, all the mappings between lexical forms and surface forms.A comprehensive analyzer such as we built for English and French consists of tens ofthousands of states and hundreds of thousands of arcs; but physically they can be quitesmall, a couple of megabytes in size.
The same network can be applied in two ways: toprovide an analysis for a surface form or to generate a surface from a lexical form in atiny fraction of a second.
Karttunen, Kaplan, and Zaenen (1992) and Karttunen (1994)are the first published reports on lexical transducers.In 1993 Xerox established a new European research center (XRCE) near Grenoble,France.
Annie Zaenen and I went there to launch the Center?s research on natural459Computational Linguistics Volume 33, Number 4language.
We started with a couple of employees in an unfinished building with threeempty floors, an elevator, and a pile of Sun workstations stacked at the entrance.
Notknowing a word of French made it a hardship assignment for me, but in every otherrespect it was a lucky break.
Because XRCE was a start-up as a research center in needof visibility and recognition on the level of the Xerox Corporation, I got more resourcesand help for my work than I could possibly have had at PARC.
A Xerox business unitmade a contract with XRCE to produce morphological analyzers and disambiguators(= ?taggers?)
for six European languages.
Kenneth R. Beesley, who had worked forMicrolytics, came to Grenoble to manage the development effort.
I headed a smallfinite-state team of researchers and programmers charged with the mission of creatingbetter development and run-time tools such as XFST (Xerox Finite State Tool) and LEXC(Lexicon Compiler).It became evident that large systems of two-level rules were difficult to debug.We concluded that lexical transducers are easier to construct with sequentially appliedrules than with the parallel two-level rules.
Andre?
Kempe and I therefore developeda compiler for replace rules (Karttunen 1995, 1996; Kempe and Karttunen 1996).11 TheXFST regular expression language now includes a large set of different types of replaceexpressions: parallel replacement, replacement with multiple contexts, replacementwith left-to-right or right-to-left shortest or longest match constraints, in addition tothe usual finite-state operations union, intersection, composition, and negation.Ken Beesley and I managed to get permission from the XRCE management to releaseto researchers most of the tools that were developed in Grenoble for creating andapplying finite-state networks, not just for morphological analysis but also for otheruseful NLP tasks such as tokenization and named-entity recognition.
Ken and I wrotea book, Finite State Morphology (Beesley and Karttunen 2003), a pedagogical text thatexplains and documents the tools that come with the book.
There have been manyimprovements in the software since then.
A new edition of the book is in the making.By the time I left Grenoble to come back to PARC in 2001, Inxight, a Xerox spinoffcompany in California, was marketing finite-state morphological analyzers and stem-mers for about three dozen languages.
From a computational point of view morphologywas a solved problem.4.4 Computational vs. Paper-and-Pencil MorphologyHistorically, computational linguists and their ?paper-and-pencil?
counterparts in lin-guistics departments have been curiously out of sync in their approach to phonologyand morphology.
When computational linguists implemented parallel two-level modelsin the 1980s, paper-and-pencil linguists were stuck in the sequential Chomsky?Halleparadigm.
Many arguments had been advanced in the phonological literature in the1970s to show that phonological alternations could not be described or explained ad-equately without sequential rewrite rules.
The idea of rules as constraints between alexical symbol and its surface realization was seen as misguided.
It went unnoticedthat two-level rules could have the same effect as ordered rewrite rules because therealization of a lexical symbol could be constrained by the lexical side and/or by the11 Our compilation algorithm was inspired by the landmark article of Kaplan and Kay (1994).
We found away to express the constraints on replacement using fewer auxiliary symbols than Kaplan and Kay.
Thecompilation time and the size of the intermediate networks is very sensitive to the size of the auxiliaryalphabet.460Karttunen Word Playsurface side.
The standard arguments for rule ordering were based on the a prioriassumption that a rule could refer only to the input context (Karttunen 1993).But in the mid 1990s when most computational linguists working with the Xeroxtools embraced the sequential model as the more practical approach, a two-level theorytook over paper-and-pencil linguistics by storm in the guise of Optimality Theory (OT)(Prince and Smolensky 1993; Kager 1999; McCarthy 2002).
In just a few years virtuallyall working phonologists switched into the OT paradigm.
From my perspective OT is atwo-level model where the ranking of the constraints plays the role that rule-orderinghas in the sequential model.If one believes, as I do, that the mapping from lexical forms to inflected surfaceforms is basically a regular relation, then the choice between the two ways of decompos-ing it, either as a composed cascade of replace operations or as an intersection of parallelrules, has important practical consequences but it is not a deep theoretical divide.
In fact,the two-level analyzer for French discussed in Karttunen, Kaplan, and Zaenen (1992)combined parallel rules with composition.
It is unclear to me why my paper-and-pencilcolleagues seem to think that it has to be absolutely one or the other.I have written several papers in the hope of getting my paper-and-pencil colleaguesinterested in, or at least aware of, what is happening in computational morphology(Karttunen 1993, 1998, 2003, 2006).
I have not succeeded.
Paper-and-pencil morphol-ogists in general are not interested in creating complete descriptions for particularlanguages.
They design formalisms for expressing generalizations about morphologicalphenomena commonly found in all natural languages.
Practical issues that arise in thecontext of real-life applications such as completeness of coverage, physical size, andspeed of applications are irrelevant from an academic morphologist?s point of view.The main purpose of a morphologist writing for an audience of fellow linguists is to beconvincing that his theory of word formation provides a more insightful and elegantaccount of this aspect of the human linguistic endowment than the competing theoriesand formalisms.My frustration is best summed up in a fable that I attached to my paper on afinite-state implementation of Gregory Stump?s realizational morphology (Stump 2001;Karttunen 2003).Time after time, from Johnson (1972) to Ellison (1994) to Eisner (2002), computa-tional knights have presented themselves at the Royal Court of Linguistics, rushed upto the Princess of Phonology and Morphology in great excitement to deliver the samemessage:Dear Princess.
I have wonderful news for you: You are not like some of yourNP-complete sisters.
You are regular.
You are rational.
You are finite-state.Please marry me.
Together we can do great things.And time after time, the put-down response from the Princess has been the same:Not interested.
You do not understand Theory.
Go away, you geek.Because the most suitable suitor has always been rejected, I suspect that the Princess hasa vested interest in making simple things appear more complicated than they really are.The good news that the computational knights are trying to deliver is unwelcome.
ThePrincess prefers the pretense that phonology/morphology is a profoundly complicatedsubject, believing herself to be shrouded by veils of theories.461Computational Linguistics Volume 33, Number 4If that is the correct analysis of the situation, computational linguists should adopta different strategy.
Instead of being the eternal rejected suitor at the Royal Court, theyshould adopt the role of the innocent boy in the street shoutingThe Princess has no clothes!
The Princess has no clothes!
.
.
.That was my conclusion in the 2003 paper.5.
EpilogueI am very happy to see that the topics I worked on at the very beginning of my careerhave finally become relevant in NLP.
To quote again the opening paragraph of my 1970ACL presentation (Karttunen 1971b):It is evident that logical relations between main sentences and their complements are ofgreat significance in any system of automatic data processing that depends on naturallanguage.
For this reason a systematic study of such relations, of which this paperis an example, will certainly have a great practical value, in addition to what it maycontribute to the theory of the semantics of natural languages.This 37-year-old prediction of semantics having practical value is becoming a realityin the context of automated question answering and reasoning initiatives such as thePASCAL Textual Entailment Challenge (Dagan, Glickman, and Magnini 2005) and theARDA-sponsored AQUAINT project (Karttunen and Zaenen 2005; Zaenen, Karttunen,and Crouch 2005).
The first computational implementation of textual inferences arisingfrom the six types of implicative constructions in Tables 1 and 2, and their interactionwith factive verbs, is presented in Nairn, Condoravdi, and Karttunen (2006).
We maysoon see search engines that actually make use of semantic processing in addition tosimple string matching.
The ability to draw textual inferences will significantly improvethe quality of question answering and Web searches.From a linguistic perspective, this is an auspicious time to take a fresh look at issuessuch as the classification of complement constructions.
The availability of search enginessuch as Google makes it possible to check the linguist?s semantic intuitions againstactual usage.
One question I have always had about the classification of implicative con-structions is whether the commitment to the truth or falsity of the complement clause isalways based on a semantic entailment or whether some of these cases should be lookedupon as a usage convention.
For example, if you google the pattern didn?t hesitate to, itis immediately evident that hesitate to really is one of the rare ?+implicatives.
(20) a.
Head Coach Jon Gruden didn?t hesitate to share interesting Buccaneerinformation with the two Chamber of Commerce crowds on Friday.b.
John didn?t hesitate to refer the file to CIB and from there it went to theVictoria Police.c.
George Patton with all his swagger and confidence didn?t hesitate to throwhimself and his men into the teeth of the German offensive and won the day.When you see examples such as in (20) in their full context, it is obvious that the authorpresents the complement clause of hesitate to as a fact.
But it is difficult to explain whythings should be this way starting from the concept of hesitation or the semantics of theverb hesitate.462Karttunen Word PlayAs a sample of things I am planning to do next, I will leave you two little puzzles tosolve.
The construction didn?t wait to is ambiguous.
Here are a couple of examples fromGoogle to to illustrate the ambiguity.
(21) a. Deena did not wait to talk to anyone.
Instead, she ran home.b.
It hurt like hell, but I?m glad she didn?t wait to tell me.
(21a) implies Deena did not talk to anyone.
But (21b) implies She told me something rightaway.Question 1: How does it come about that X didn?t wait to do Y means either that Xdid Y right away or that X didn?t do Y at all?When you look at examples with didn?t wait to in their full context, it is nearly alwayspossible to tell which of the two meanings the author has in mind.
In (21a), for instance,the negative polarity item anyone and the word instead are telltale indicators.
In (21b),the cataphoric pronoun it indicates that a telling event took place.
I am sure that it ispossible to learn to pick the intended meaning by statistical techniques.
But statisticsalone will not give you an answer to Question 1, nor will it solve the related problem inQuestion 2.Question 2: Why is it not possible to translate expressions such as Neil didn?t waitto take off his coat to other languages in a way that preserves the ambiguity thesentence has in English?In languages such as Dutch, Finnish, French, German, Hungarian, and Japanese, amongothers, it is of course possible to express the two meanings of X did not wait to Y, but notin one and the same sentence.
My answer to these two questions will have to wait untilmy next semantics paper.AcknowledgmentsMany thanks to Kenneth R. Beesley, DanielG.
Bobrow, Robert Dale, Aravind K. Joshi,John T. Maxwell, III, Bonnie Webber, andAnnie Zaenen for their help on the style andcontent of this article.ReferencesAntworth, Evan L. 1990.
PC-KIMMO:A Two-Level Processor for MorphologicalAnalysis.
Number 16 in Occasionalpublications in academic computing.Summer Institute of Linguistics, Dallas.Armstrong, Susan.
1996.
Multext:Multilingual text tools and corpora.
InH.
Feldweg and E. W. Hinrichs, editors,Lexikon und Text.
Max Niemeyer Verlag,Tuebingen, Germany, pages 107?112.Beaver, David I.
1995.
Presupposition andAssertion in Dynamic Semantics.
Ph.D.thesis, Center for Cognitive Science,University of Edinburgh, Edinburgh,Scotland.Beesley, Kenneth R. and Lauri Karttunen.2003.
Finite State Morphology.
CSLIPublications, Stanford, CA.Bennett, Winfield S. and Jonathan Slocum.1985.
The LRC machine translation system.Computational Linguistics, 11(2?3):111?121.Black, A., G. Ritchie, S. Pulman, andG.
Russell.
1987.
Formalisms formorphographemic description.
InProceedings of the Third Conference of theEuropean Chapter of the Association forComputational Linguistics, pages 11?18,Copenhagen, Denmark.Carter, D. 1995.
Rapid development ofmorphological descriptions for fulllanguage processing systems.
InProceedings of the Seventh Conference of theEuropean Chapter of the Association forComputational Linguistics, pages 202?209,Dublin, Ireland.Chomsky, Noam.
1957.
Syntactic Structures.Mouton, Gravenhage, The Netherlands.Chomsky, Noam.
1973.
Conditions ontransformations.
In Steven Anderson and463Computational Linguistics Volume 33, Number 4Paul Kiparsky, editors, A Festschrift forMorris Halle.
Holt, Reinhard, and Winston,New York, NY, pages 232?286.Chomsky, Noam and Morris Halle.
1968.
TheSound Pattern of English.
Harper and Row,New York, NY.Dagan, Ido, Oren Glickman, and BernardoMagnini.
2005.
The PASCAL recognisingtextual entailment challenge.
In Proceedingsof the PASCAL Challenges Workshopon Recognising Textual Entailment,pages 1?8, Southampton, UK.Eisner, Jason.
2002.
Phonologicalcomprehension and the compilation ofoptimality theory.
In Proceedings of the40th Annual Meeting of the Association forComputational Linguistics, pages 56?63,Washington, DC.Ellison, T. Mark.
1994.
Phonologicalderivation in Optimality Theory.
InProceedings of the 15th InternationalConference on Computational Linguistics,pages 1007?1013, Kyoto, Japan.Fillmore, Charles.
1971.
Verbs of judging:An exercise in semantic description.In Charles J. Fillmore and TerenceLangendoen, editors, Studies inLinguistic Semantics.
Holt, Rinehartand Winston, Inc., New York, NY,pages 273?289.Frege, Gottlob.
1892.
U?ber Sinn undBedeutung.
Zeitschrift fu?r Philosophie undPhilosophische Kritik, pages 25?50.
Englishtranslation: ?On Sense and Meaning,?in Brian McGuiness, editor, Frege:Collected Works.
Basil Blackwell, Oxford,pages 157?177.Gajek, Oliver, Hanno T. Beck, Diane Elder,and Greg Whittemore.
1983.
Lispimplementation.
In Mary Dalrymple, EditDoron, John Goggin, Beverly Goodman,and John McCarthy, editors, TexasLinguistic Forum, Vol.
22.
Department ofLinguistics, The University of Texas atAustin, Austin, TX, pages 187?202.Gazdar, Gerald.
1979.
Conventionalimplicature.
In Choon-Kyu Oh and DavidA.
Dinneen, editors, Syntax and Semantics,Volume 11: Presupposition.
AcademicPress, New York, NY, pages 57?89.Geis, Michael and Arnold Zwicky.
1971.On invited inferences.
Linguistic Inquiry,2:561?565.Geurts, Bart.
1999.
Presuppositions andPronouns.
Elsevier, Cambridge, MA.Ginzburg, Jonathan.
1992.
Questions, Queriesand Facts: A Semantics and Pragmatics forInterrogatives.
Ph.D. thesis, StanfordUniversity, Stanford, CA.Ginzburg, Jonathan.
1996.
Interrogatives:Questions, facts, and dialogue.
In ShalomLappin, editor, Handbook of ContemporarySemantic Theory.
Blackwell Publishers,Oxford, UK, pages 385?422.Grice, H. Paul.
1979.
Logic and conversation.In P. Cole and J. L. Morgan, editors,Speech Acts.
Academic Press, New York,NY, pages 41?58.Groenendijk, Jeroen and Martin Stokhof.1984.
On the semantics of questions andthe pragmantics of answers.
In FredLandman and Frank Veltman, editors,Varieties of Formal Semantics.
ForisPublications, Dordrecht, The Netherlands,pages 143?170.Hamblin, Charles L. 1973.
Questions inMontague English.
Foundations ofLanguage, 10:41?53.Harris, Randy Allen.
1995.
The LinguisticsWars.
Oxford University Press,Oxford, UK.Hausser, Roland.
1983.
The Syntax andSemantics of English Mood.
In FerencKiefer, editor, Questions and Answers.Reidel, Dordrecht, The Netherlands,pages 97?158.Hausser, Roland and Dietmar Zaefferer.1979.
Questions and answers in a contextdependent Montague grammar.
InSiegfried Josef Schmidt and FranzGuenthner, editors, Formal Semanticsand Pragmatics for Natural Languages.Reidel, Dordrecht, The Netherlands,pages 339?358.Hays, David G. 1967.
Introduction toComputational Linguistics.
Elsevier, NewYork, NY.Heim, Irene.
1982.
The Semantics of Definiteand Indefinite Noun Phrases.
Ph.D. thesis,University of Massachusetts, Amherst, MA.Heim, Irene.
1983.
On the projection problemfor presuppositions.
In West-CoastConference on Formal Linguistics, volume 2,pages 114?126, Stanford, CA.Heina?ma?ki, Orvokki.
1974.
Semantics ofEnglish Temporal Connectives.
Ph.D. thesis,University of Texas, Austin, TX.Johnson, C. Douglas.
1972.
Formal Aspectsof Phonological Description.
Mouton,The Hague, The Netherlands.Joshi, Aravind K. and Ralph M. Weischedel.1973.
Some frills for the modal tic-tac-toeof Davies and Isard: Semantics of predicatecomplement constructions.
In IJCAI,pages 352?355, Stanford, CA.Kager, Rene?.
1999.
Optimality Theory.Cambridge University Press,Cambridge, UK.464Karttunen Word PlayKamp, Hans.
1981.
Eve?nements,repre?sentation discursive et re?fe?rencetemporelle.
Langages, 64:39?64.Kamp, Hans.
2001.
The importance ofpresupposition.
In Christian Rohrer, AntjeRo?deutscher, and Hans Kamp, editors,Linguistic Form and its Computation.
CSLI,Stanford, CA, pages 207?254.Kamp, Hans and Uwe Reyle.
1993.
FromDiscourse to Logic.
Kluwer, Dordrecht,The Netherlands.Kaplan, Ronald M. and Martin Kay.
1994.Regular models of phonological rulesystems.
Computational Linguistics,20(3):331?378.Kaplan, S. Jerrold.
1977.
Cooperative Responsesfrom a Natural Language Query System.Ph.D.
thesis, University of Pennsylvania,Philadelphia, PA.Karttunen, Lauri.
1969a.
Problems of Referencein Syntax.
Ph.D. thesis, Indiana University,Bloomington, Indiana.Karttunen, Lauri.
1969b.
Pronouns andvariables.
In CLS 5: Proceedings of theFifth Regional Meeting, pages 108?116,Chicago, IL.Karttunen, Lauri.
1971a.
Implicative verbs.Language, 47:340?358.Karttunen, Lauri.
1971b.
The logic of Englishpredicate complement constructions.
TheIndiana University Linguistics Club.Bloomington, Indiana.Karttunen, Lauri.
1973.
Presuppositions ofcompound sentences.
Linguistic Inquiry,4:167?193.Karttunen, Lauri.
1974.
Presupposition andlinguistic context.
Theoretical Linguistics,1(1):181?194.Karttunen, Lauri.
1976.
Discourse referents.In James D. McCawley, editor, Syntax andSemantics Volume 7, Notes from the LinguisticUnderground.
Academic Press, New York,NY, pages 363?385.Karttunen, Lauri.
1977.
Syntax and semanticsof questions.
Linguistics and Philosophy,1:1?44.Karttunen, Lauri.
1983.
KIMMO: A generalmorphological processor.
In MaryDalrymple, Edit Doron, John Goggin,Beverley Goodman, and John McCarthy,editors, Texas Linguistic Forum, volume 22.Department of Linguistics, The Universityof Texas at Austin, Austin, TX,pages 165?186.Karttunen, Lauri.
1984.
Features and values.In COLING?84, pages 28?33, July 2?6,Stanford, CA.Karttunen, Lauri.
1986.
D-PATR:A development environment forunification-based grammars.
InCOLING?86, pages 74?80, Bonn, Germany.Karttunen, Lauri.
1993.
Finite-stateconstraints.
In John Goldsmith, editor,The Last Phonological Rule.
University ofChicago Press, Chicago, IL.Karttunen, Lauri.
1994.
Constructing lexicaltransducers.
In COLING?94, pages 406?411,Kyoto, Japan.Karttunen, Lauri.
1995.
The replaceoperator.
In ACL?95, cmp-lg/9504032.Cambridge, MA.Karttunen, Lauri.
1996.
Directedreplacement.
In ACL?96, cmp-lg/9606029.Santa Cruz, CA.Karttunen, Lauri.
1998.
The proper treatmentof optimality in computational phonology.In FSMNLP?98.
International Workshop onFinite-State Methods in Natural LanguageProcessing, cmp-lg/9804002.
BilkentUniversity, Ankara, Turkey.Karttunen, Lauri.
2003.
Computing withrealizational morphology.
In AlexanderGelbukh, editor, Computational Linguisticsand Intelligent Text Processing, volume 2588of Lecture Notes in Computer Science.Springer Verlag, Heidelberg, Germany,pages 205?216.Karttunen, Lauri.
2006.
The insufficiency ofpaper-and-pencil linguistics: The case ofFinnish prosody.
In Miriam Butt, MaryDalrymple, and Tracy Holloway King,editors, Intelligent Linguistic Architectures,pages 287?300.
CSLI Publications,Stanford, CA.Karttunen, Lauri and Kenneth R. Beesley.1992.
Two-level rule compiler.
TechnicalReport ISTL-92-2, Xerox Palo AltoResearch Center, Palo Alto, CA.Karttunen, Lauri, Ronald M. Kaplan,and Annie Zaenen.
1992.
Two-levelmorphology with composition.In COLING?92, pages 141?148,Nantes, France.Karttunen, Lauri and Martin Kay.
1985a.Parsing in a free word order language.
InDavid R. Dowty, Lauri Karttunen, andArnold Zwicky, editors, Natural LanguageParsing.
Cambridge University Press,Cambridge, UK, pages 279?306.Karttunen, Lauri and Martin Kay.
1985b.Structure sharing with binary trees.
InProceedings of the 23rd Meeting of theAssociation for Computational Linguistics,pages 133?136, Chicago, IL.Karttunen, Lauri, Kimmo Koskenniemi, andRonald M. Kaplan.
1987.
A compiler fortwo-level phonological rules.
In MaryDalrymple, Ronald Kaplan, Lauri465Computational Linguistics Volume 33, Number 4Karttunen, Kimmo Koskenniemi, SamiShaio, and Michael Wescoat, editors, Toolsfor Morphological Analysis.
Center for theStudy of Language and Information,Stanford University, Palo Alto, CA,pages 1?61.Karttunen, Lauri and Stanley Peters.
1979.Conventional implicature.
In Choon-KyuOh and David A. Dinneen, editors, Syntaxand Semantics, Volume 11: Presupposition.Academic Press, New York, NY,pages 1?56.Karttunen, Lauri, Hans Uszkoreit, andRebecca Root.
1981.
Morphologicalanalysis of Finnish by computer.
InProceedings of the 71st Annual Meeting ofSASS, Albuquerque, NM.Karttunen, Lauri and Annie Zaenen.
2005.Veridicity.
In Graham Katz, JamesPustejovsky, and Frank Schilder, editors,Annotating, Extracting and Reasoning aboutTime and Events, number 05151 in DagstuhlSeminar Proceedings.
InternationalesBegegnungs-und Forschungszentrum(IBFI), Schloss Dagstuhl, Germany,http://drops.dagstuhl.de/opus/volltexte/2005/314.Keenan, Edward L. 1971.
Two kinds ofpresupposition in natural language.
InCharles J. Fillmore and TerenceLangendoen, editors, Studies in LinguisticSemantics.
Holt, Rinehart and Winston,Inc., New York, NY, pages 45?54.Kempe, Andre?
and Lauri Karttunen.
1996.Parallel replacement in finite-statecalculus.
In COLING?96, cmp-lg/9607007.Copenhagen, Denmark.Kiparsky, Paul and Carol Kiparsky.
1971.Fact.
In D. Steinberg and L. Jakobovits,editors, Semantics.
An InderdisciplinaryReader, pages 345?369.
CambridgeUniversity Press, Cambridge, UK.Koskenniemi, Kimmo.
1983.
Two-levelmorphology: A general computationalmodel for word-form recognition andproduction.
Publication 11, Universityof Helsinki, Department of GeneralLinguistics, Helsinki.Koskenniemi, Kimmo.
1986.
Compilation ofautomata from morphological two-levelrules.
In Fred Karlsson, editor, Papersfrom the Fifth Scandinavian Conference onComputational Linguistics, pages 143?149,Helsinki, Finland.Langendoen, Terence and Harris B. Savin.1971.
The projection problem forpresuppositions.
In Charles J. Fillmoreand Terence Langendoen, editors,Studies in Linguistic Semantics.
Holt,Rinehart and Winston, Inc., New York,pages 55?62.Lewis, David.
1979.
Scorekeeping in alanguage game.
Journal of PhilosophicalLogic, 8:339?359.McCarthy, John J.
2002.
The Foundations ofOptimality Theory.
Cambridge UniversityPress, Cambridge, UK.McCawley, James D. 1970.
Where do nounphrases come from?
In Roderick A. Jacobsand Peter S. Rosenbaum, editors, Readingsin English Transformational Grammar.Ginn and Company, Waltham, MA,pages 166?183.Montague, Richard.
1970a.
English as aformal language.
In B. Visentini et al,editors, Linguaggi nella Societa` e nellaTecnica.
Edizioni di Comunita`, Milan,Italy, pages 189?224.Montague, Richard.
1970b.
Universalgrammar.
Theoria, 36:373?398.Montague, Richard.
1973.
The propertreatment of quantification in ordinaryEnglish.
In P. Suppes K. J. J. Hintikka,J.
M. E. Moravcsik, editors, Approaches toNatural Language.
Reidel, Dordrecht,The Netherlands, pages 221?242.Moshagen, Sjur, Pekka Sammallahti, andTrond Trosterud.
2006.
Twol at work.In Antti Arppe, Lauri Carlson, KristerLinde?n, Jussi Piitulainen, MickaelSuominen, Martti Vainio, HannaWesterlund, and Anssi Yli-Jyra?,editors, Inquiries into Words, Constraintsand Contexts.
CSLI, Stanford, CA,pages 94?105.Nairn, Rowan, Cleo Condoravdi, and LauriKarttunen.
2006.
Computing relativepolarity for textual inference.
In Inferencein Computational Semantics (ICoS-5),Buxton, UK.Oflazer, Kemal.
1994.
Two-level descriptionof Turkish morphology.
Literary andLinguistic Computing, 9(2):137?148.Partee, Barbara.
1995.
Montague grammarand transformational grammar.
LinguisticInquiry, 6:203?300.Potts, Christopher.
2004.
The Logic ofConventional Implicatures.
OxfordUniversity Press, Oxford, UK.Prince, Allan and Paul Smolensky.1993.
Optimality Theory: Constraintinteraction in generative grammar.RuCCS Technical Report 2.
RutgersCenter for Cognitive Science.
RutgersUniversity, Piscataway, NJ.Prince, Ellen.
1978.
A comparison ofWh-clefts and it-clefts in discourse.Language, 54:883?906.466Karttunen Word PlayPulman, Stephen.
1991.
Two levelmorphology.
In H. Alshawi, D. Arnold,R.
Backofen, D. Carter, J. Lindop, K. Netter,S.
Pulman, J. Tsujii, and H. Uskoreit,editors, ET6/1 Rule Formalism and VirtualMachine Design Study.
CEC, Luxembourg,chapter 5.Ritchie, G., A.
Black, S. Pulman, andG.
Russell.
1987.
The Edinburgh/Cambridge morphological analyserand dictionary system (version 3.0)user manual.
Technical Report SoftwarePaper No.
10, Department of ArtificialIntelligence, University of Edinburgh,Edinburgh, UK.Ritchie, G., G. Russell, A.
Black, andS.
Pulman.
1992.
Computational Morphology:Practical Mechanisms for the English Lexicon.The MIT Press, Cambridge, MA.Russell, Bertrand.
1905.
On denoting.
Mind,14:479?493.Russell, Bertrand.
1957.
Mr. Strawson onreferring.
Mind, 66:385?389.Schiller, Anne.
1996.
Deutsche Flexions-und Kompositionsmorphologie mitPC-KIMMO.
In Roland Hausser,editor, Linguistische Verifikation:Dokumentation zur Ersten Morpholympics1994, number 34 in Sprache undInformation, pages 37?52.
Max NiemeyerVerlag, Tu?bingen.Schu?tzenberger, Marcel-Paul.
1961.
A remarkon finite transducers.
Information andControl, 4:185?196.Shieber, Stuart, Hans Uszkoreit, FernandoPereira, Jane Robinson, and Mabry Tyson.1983.
The formalism and implementationof PATR-II.
In Barbara J. Grosz and MarkStickel, editors, Research on InteractiveAcquisition and Use of Knowledge.
SRIInternational, Menlo Park, CA,techreport 4, pages 39?79.Soames, S. 1982.
How presuppositions areinherited: A solution to the projectionproblem.
Linguistic Inquiry, 13:483?545.Stalnaker, Robert.
1973.
Presuppositions.The Journal of Philosophical Logic,2:447?457.Strawson, Peter.
1950.
On referring.
Mind,59:320?344.Strawson, Peter.
1964.
Identifying referenceand truth values.
Theoria, 30:320?344.Stump, Gregory T. 2001.
InflectionalMorphology.
A Theory of Paradigm Structure.Cambridge University Press, Cambridge,UK.Uibo, Heli.
2006.
Eesti keele morfoloogiamodelleerimisest lo?plike muundurite abil.
[on modelling the estonian morphology bythe means of finite-state transducers].
InM.
Koit, R. Pajusalu, and H. O?im, editors,Keel ja arvuti, number 6 in Tartu U?likooliu?ldkeeleteaduse o?ppetooli toimetised.
MaxNiemeyer Verlag, Tu?bingen, Germany.van der Sandt, Rob.
1992.
Presuppositionprojection as anaphora resolution.
Journalof Semantics, 9:333?377.van der Sandt, Rob A. and Bart Geurts.
1991.Presupposition, anaphora, and lexicalcontent.
In Text Understanding in LiLOG,Integrating Computational Linguistics andArtificial Intelligence, Final Report on the IBMGermany LILOG-Project.
Springer Verlag,London, UK, pages 259?296.Vendler, Zeno.
1967.
Linguistics andPhilosophy.
Cornell University Press,Ithaca, NY.Wall, Robert E. 1972.
Introduction toMathematical Linguistics.
Prentice Hall,Englewood Cliffs, NJ.Webber, Bonnie L. 1978.
A Formal Approach toDiscourse Anaphora.
Ph.D. thesis, HarvardUniversity, Cambridge, MA.Weischedel, Ralph.
1975.
Computation of aUnique Class of Inferences: Presuppositionand Entailments.
Ph.D. thesis, Universityof Pennsylvania, Philadelphia, PA.Zaenen, Annie, Lauri Karttunen, andRichard Crouch.
2005.
Local textualinference: Can it be defined orcircumscribed?
In Workshop on the EmpiricalModeling of Semantic Equivalence andEntailment, pages 31?36, Ann Arbor, MI.Zeevat, Henk.
1992.
Presupposition andaccommodation in update semantics.Journal of Semantics, 9:379?412.467
