Towards Unsupervised Extraction of Verb Paradigms from LargeCorporaCorne l ia  H.  Parkes ,  A lexander  M.  Ma lek  and Mi tche l l  P.  MarcusDepartment  of Computer  & Information ScienceUniversity of Pennsylvaniacparkes@)linc.cis.upenn.edu*Abst rac tA verb paradigm is a set of inflectional cate-gories for a single verb lemma.
To obtain verbparadigms we extracted left and right bigramsfor the 400 most frequent verbs from over 100million words of text, calculated the KullbackLeibler distance for each pair of verbs for leftand right contexts eparately, and ran a hier-archical clustering algorithm for each context.Our new method for finding unsupervised cutpoints in the cluster trees produced results thatcompared favorably with results obtained usingsupervised methods, such as gain ratio, a re-vised gain ratio and number of correctly classi-fied items.
Left context clusters correspond toinflectional categories, and right context clus-ters correspond to verb lemmas.
For our testdata, 91.5% of the verbs are correctly classi-fied for inflectional category, 74.7% are correctlyclassified for lemma, and the correct joint classi-fication for lemma and inflectional category wasobtained for 67.5% of the verbs.
These resultsare derived only from distributional informationwithout use of morphological information.1 In t roduct ionThis paper presents a new, largely unsupervisedmethod which, given a list of verbs from a cor-pus, will simultaneously classify the verbs bylemma and inflectional category.
Our long termresearch goal is to take a corpus in an unana-lyzed language and to extract a grammar forthe language in a matter of hours using sta-tistical methods with minimum input from anative speaker.
Unsupervised methods avoid" This work was supported by grants from PalladiumSystems and the Glidden Company to the first author.The comments and suggestions of Martha Palmer, HoaTrang Dang, Adwait Ratnaparkhi, Bill Woods, LyleUngar, and anonymous reviewers are also gratefullyacknowledged.110labor intensive annotation required to producethe training materials for supervised methods.The cost of annotated ata becomes particu-larly onerous for large projects across many lan-guages, such as machine translation.
If ourmethod ports well to other languages, it couldbe used as a way of automatically creating amorphological analysis tool for verbs in lan-guages where verb inflections have not alreadybeen thoroughly studied.
Precursors to thiswork include (Pereira et al 1993), (Brown etal.
1992), (Brill & Kapur, 1993), (Jelinek,1990), and (Brill et al 1990) and, as appliedto child language acquisition, (Finch & Chater,1992).Clustering algorithms have been previouslyshown to work fairly well for the classificationof words into syntactic and semantic classes(Brown et al 1992), but determining the opti-mum number of classes for a hierarchical clustertree is an ongoing difficult problem, particularlywithout prior knowledge of the item classifica-tion.
For semantic lassifications, the correct as-signment of items to classes is usually not knownin advance.
In these cases only an unsuper-vised method which has no prior knowledge ofthe item classification can be applied.
Our ap-proach is to evaluate our new, largely unsuper-vised method in a domain for which the correctclassification of the items is well known, namelythe inflectional category and lemma of a verb.This allows us to compare the classification pro-duced by the unsupervised method to the classi-fications produced by supervised methods.
Thesupervised methods we examine are based oninformation content and number of items cor-rectly classified.
Our unsupervised method usesa single parameter, the expected size of the clus-ter.
The classifications by inflectional categoryand lemma are additionally interesting becausethey produce trees with very different shapes.The classification tree for inflectional categoryhas a few large clusters, while the tree for verblemmas has many small clusters.
Our unsuper-vised method not only performs as well as thesupervised methods, but is also more robust fordifferent shapes of the classification tree.Our results are based solely on distributionalcriteria nd are independent of morphology.
Wecompletely ignore relations between words thatare derived from spelling.
We assume that anydifference in form indicates a different item andhave not "cleaned up" the data by removingcapitalization, etc.
Morphology is importantfor the classification of verbs, and it may wellsolve the problem for regular verbs.
However,morphological nalysis will certainly not handlehighly irregular, high frequency verbs.
What issurprising is that strictly local context can makea significant contribution to the classification ofboth regular and irregular verbs.
Distributionalinformation ismost easily extracted for high fre-quency verbs, which are the verbs that tend tohave irregular morphology.This work is important because it develops amethodology for analyzing distributional infor-mation in a domain that is well known.
Thismethodology can then be applied with someconfidence to other domains for which the cor-rect classification of the items is not known inadvance, for example to the problem of semanticclassification.2 DataThis report is on our investigation of En-glish text using verbs tagged for inflectionalcategory, l The tags identify six inflectionalcategories: past tense (VBD), tenseless (VB),third-person singular present tense (VBZ),other present tense (VBP), -ing (VBG) and par-ticiple (VBN).
The use of tagged verbs enablesus to postpone the problem of resolving ambigu-ous inflectional forms, such as the homonyms,"work" as tenseless and "work" as present tense,a conflation that is pervasive in English for thesecategories.
We also do not address how to sep-arate the past participle and passive participleuses of VBN.The methods reported in this paper were de-veloped on two different corpora.
The first cor-pus consisted of the 300 most frequent verbsIThe verbs were automatically tagged by the Brilltagger.
Tag errors, such as "\[ \VBG" tended to formisolated clusters.111Table 1: Distribution of verbs by inflectionalcategory in 400 most frecI n f lec t iona l  CategoryVBVBNVBDVBGVBZVBPLuent verbsVerbs10980766746 l22 Ifrom the 52 million word corpus of the New YorkTimes, 1995.
2 For this corpus, both the verbsand the contexts consisted of tagged words.
Asa somewhat independent test, we applied ourmethods to the 400 most frequent verbs from asecond corpus containing over 100 million wordsfrom the WM1 Street Journal (1990-94).
For thesecond corpus, the tags for context words wereremoved.
The results for the two corpora arevery similar.
For reasons of space, only the re-sults from the second corpus are reported here.The distribution of verbs is very different forinflectional category and lemma.
The distribu-tion of verbs with respect o lemmas is typicalof the distribution of tokens in a corpus.
Ofthe 176 lemmas represented in the 400 most fre-quent verbs, 79 (45%) have only one verb.
Onelemma, BE, has 14 verbs.
3 Even in 100 millionwords, the 400 th most frequent verb occurs only356 times.
We have not yet looked at the re-l~ition between corpus frequency and clusteringbehavior of an item.
The distribution of verbsin inflectional categories has a different profile(See Table 1).
This may be related to the factthat, unlike lemmas, inflectional categories forma small, closed class.3 C lus ter  Ana lys i sFor each verb we extracted frequency counts forleft and right bigrams called the left and rightcontexts, respectively.
A similarity matrix forleft and right contexts was created by calcu-lating the relative entropy or Kullback Leibler(KL) distance 4 between the vectors of contextfrequency counts for each pair of verbs.
TheKL distance has been used previously (Mager-man & Marcus (1990), Pereira et al (1993))7Tagged corpora were provided by the LinguisticData Consortium (http://www.ldc.upenn.edu).SUpper and lower case forms are counted as distinct.4For an introduction torelative ntropy see (CoverThomas, 1991)to measure the similarity between two di:stri-butions of word bigrams.
For the moment; weadded a small constant to smooth over zero fre-quencies.
Because the distance between verbiand verbj is not in general equal to the distancebetween verbj and verbi, the KL distances be-tween each pair of verbs are added to produce asymmetric matrix.
We tested other measures ofdistance between words.
The total divergenceto the average, based on the KL distance (Da-gan et al 1994), produced comparable results,but the the cosine measure (Schuetze, 1993)produced significantly poorer results.
We con-clude that entropy methods produce more re-liable estimates of the probability distributionsfor sparse data (Ratnaparkhi, 1997).The similarity matrices for left and right con-texts are analyzed by a hierarchical clusteringalgorithm for compact clusters.
The use of a"hard" instead of a "soft" clustering algorithmis justified by the observation (Pinker, 1984)that the verbs do not belong to more thanone inflectional category or lemma.
5 A hierar-chical clustering algorithm (Seber, 1984) con-structs from the bottom up using an agglomer-ative method that proceeds by a series of suc-cessive fusions of the N objects into clusters.The compactness of the resulting cluster is usedas the primary criterion for membership.
Thismethod of complete linkage, also known as far-thest neighbor, defines the distance between twoclusters in terms of the largest dissimilarity be-tween a member of cluster L1 and a memberof cluster L2.
We determined experimentallyon the development corpus that this algorithmproduced the best clusters for our data.Figures 1 and 2 show portions of the clus-ter trees for left and right contexts.
The scalesat the top of the Figures indicate the height atwhich the cluster is attached to the tree in thearbitrary units of the distance metric.
The leftcontext ree in Figure 1 shows large, nearly pureclusters for the VBD and VBZ inflectional cat-egories.
The right context ree in Figure 2 hassmaller clusters for regular and irregular verblemmas.
Note that some, but not all, of theforms of BE form a single cluster.To turn the cluster tree into a classification,we need to determine the height at which to ter-minate the clustering process.
A cut point is a5The only exception in our data is " 's" which belongsto the lemmas BE and HAVE.5 10 15 20closeckVBDk~t/VBDpm.xWBO r I-d, ,~ ,eo  --.-----i i===>---7_____qw.vvBz ~ ~ I-j I Ih,H,A'BZ ~ nFigure 1: Verb clusters for left contextsline drawn across the tree at height T that de-fines the cluster membership.
A high cut pointwill produce larger clusters that may includemore than one category.
A low cut point willproduce more single category clusters, but moreitems will remain unclustered.
Selecting the op-timum height at which to make the cut is knownas the cutting problem.A supervised method for cutting the clustertree is one that takes into account he knownclassification of the items.
We look at super-vised methods for cutting the tree in order toevaluate the success of our proposed unsuper-vised method.4 Superv ised  methodsFor supervised methods the distribution of cat-egories C in clusters L at a given height T ofthe tree is represented by the notation in Table2.
For a given cluster tree, the total number ofcategories C, the distribution of items in cate-gories nc, and the total number of items N areconstant across heights.
Only the values for L,rnt, and fd will vary for each height T.There are several methods for choosing a1120 $ 10t L ,h in t /v8  ,iFigure 2: Verb clusters for right contextsclus-1cat-1 !
J l icat -C  fC1Total ml... c lus-L-.- f l L?
. "
fCL?
.
.
m LTotaln loo .ncNC is the number of categoriesL is the number of clustersmz is the number of instances in cluster l.fd is the instances of category c in cluster lN is the total number of instances for cut Tnc is the number of instances in category cTable 2: Notationcut point in a hierarchical cluster analysis.
6We investigated three supervised methods, twobased on information content, and a third basedon counting the number of correctly classifieditems.4.1 Gain Rat ioInformation gain (Russell & Norvig, 1995) andgain ratio (Quinlan, 1990) were developed asmetrics for automatic learning of decision trees.Their application to the cutting problem forcluster analysis is straightforward.
Informa-6For a review of these methods, see (Seber, 1984).113tion gain is a measure of mutual information,the reduction in uncertainty of a random vari-able given another andom variable (Cover &Thomas, 1991).
Let C be a random variable de-scribing categories and L another andom vari-able describing clusters with probability massfunctions p(c) and q(1), respectively.
The en-tropy for categories H(C) is defined byH(C) = - ~p(c)log2p(c)where p(c) = nc/N in the notation of Table2.
The average ntropy of the categories withinclusters, which is the conditional entropy of cat-egories given clusters, is defined byH(CIL) = - ~ q(1) ~ p(cll)log2p(cll )l cwhere q(l) = mdN and p(c\]l) = ffct/mt in ournotation.Information gain and mutual informationI(C; L) are defined bygain = I(C; L) = H(C) - H(CIL)Information gain increases as the mixture of cat-egories in a cluster decreases.
The purer thecluster, the greater the gain.
If we measure in-fiJrmation gain for each height T, T = 1, ..., 40of the cluster tree, the optimum cut is at theheight with the maximum information gain.Information gain, however, cannot be used di-rectly for our application, because, as is wellknown, the gain function favors many smallclusters, such as found at the bottom of a hier-archical cluster tree.
Quinlan (1990) proposedthe gain ratio to correct for this.
Let H(L) bethe entropy for clusters defined, as above, byH(L) = - ~ q(l)log2q(l)Then the gain ratio is defined bygain ratio =I(C; L)H(L)The gain ratio corrects the mutual informationbetween categories and clusters by the entropyof the clusters.
..H(L) H(C)Figure 3: Relationship between entropy andmutual information adapted from (Cover &Thomas, 1991).4.2 Revised Gain Rat ioWe found that the gain ratio still sometimesmaximizes at the lowest height in the tree, thusfailing to indicate an optimum cut point.
We ex-perimented with the revised version of the gainratio, shown below, that sometimes overcomesthis difficulty.revised ratio = I(C; L)H(L) - H(C)The number and composition of the clusters,and hence H(L), changes for each cut of thecluster tree, but the entropy of the categories,H(C), is constant.
This function maximizeswhen these two entropies are most equal.
Fig-ure 3 shows the relationship between entropyand mutual information and the quantities de-fined for the gain ratio and revised ratio.4.3 Percent  CorrectAnother method for determining the cut point isto count the number of items correctly classifiedfor each cut of the tree.
The number of correctitems for a given cluster is equal to the num-ber of items in the category with the maximumvalue for that cluster.
For singleton clusters, anitem is counted as correctly categorized if thethe category is also a singleton.
The percentcorrect is the total number of correct items di-vided by the total number of items.
This valueis useful for comparing results between clustertrees as well as for finding the cut point.5 Unsuperv ised  MethodAn unsupervised method that worked well was?
to select a cut point that maximizes the number114of clusters formed within a specified size range.Let s be an estimate of the size of the clusterand r < 1 the range.
The algorithm counts thenumber of clusters at height T that fall withinthe interval:?
(1 - ?
(1 + r)\]The optimum cut point is at the height hat hasthe most clusters in this interval.The value of r = .8 was constant for bothright and left cluster trees.
For right contexts,where we expected many small clusters, s = 8,giving a size range of 2 to 14 items in a cluster.For left contexts, where we expected a few largeclusters, s = 100, giving a size range of 20 to180.
The expected cluster size is the only as-sumption we make to adjust the cut point giventhe disparity in the expected number of cate-gories for left and right contexts.
A fully unsu-pervised method would not make an assumptionabout expected size.6 Resu l tsWhile our goal was to use a supervised methodto evaluate the performance of the unsupervisedmethod, the supervised functions we tested dif-fered widely in their ability to predict the op-timum cut point for the left and right contexttrees.
The performance of the gain ratio, re-vised ratio, and percent correct are compared tothe unsupervised method on left and right con-text cluster trees in Figures 4 and 5.
The x axisgives the height at which the cut is evaluated bythe function, and the y axis is the scaled valueof the function for that height.
The optimumcut point indicated by each function is at theheight for which the function has the maximumvalue.
These heights are given in Table 3.
Forthe right context ree, for which the optimumcut produces many small clusters, there is gen-eral agreement between the unsupervised andsupervised methods.
For the left context ree,for which the optimum cut produces a few largeclusters, there is a lack of agreement among thesupervised methods with the gain ratio failingto indicate a meaningful cut.
The maximumfor the unsupervised method falls between themaxima for the revised ratio and percent cor-rect.
Based on these results, we used the un-supervised maximum to select he cut point forthe left and right context cluster trees.There are four steps in the analysis of thedata.
First, the cutpoint for the left contextTable 3: Heights for maximum valuesSupervised Left RightMethods \ ]Contexts  ContextsGain ratio l 1 12Revised ratio 33-34 11Percent correct 15 \] 10Unsupervised 18-22 ' 12tree is determined.
Second, the right similaritymatrix is enhanced with data from left contextclusters.
Third, the cut point for the enhancedright context tree is determined.
Finally, theverbs are cross-classified by left and right con-text cluster membership.Step 1: We select the cut point for the leftcontext ree at height T = 18, the unsupervisedmaximum.
This cut creates clusters that axe90.7% correct as compared to 91.5% at heightT = 15.
At T = 18 there are 20 clusters for6 inflectional categories, of which 6 are in thesize range 20-180 specified by the unsupervisedmethod.Step 2: Reasoning that two verbs in the samecluster for inflectional category should be in dif-ferent clusters for lemmas, 7 we created a newsimilarity matrix for the right context by in-creasing the distance between each pair of verbsthat occurred in the same left context cluster.The distance was increased by substituting aconstant equal to the value of the maximumdistance between verbs.
The number of verbscorrectly classified increased from 63.5% for theoriginal right context tree to 74.7% for the en-hanced right context ree.Step3: We select the cut point for the en-hanced right context tree at height T = 12,the unsupervised maximum.
This cut createsclusters that are 72.2% correct as compared to74.7% at height T = 10.
There are 155 clustersat height T = 12, which is 21 less than the totalnumber of lemmas in the data set.
29% of theclusters are singletons which is lower than theproportion of singleton lemmas (45%).
60% ofthe singleton clusters contain singleton lemmasand are counted as correct.Step ~: Each left context cluster was givena unique left ID and labeled for the dominantinflectional category.
Each right context clus-ter was given a right ID and labeled for thedominant lemma.
By identifying the left and7This is true except for a few verbs that have freevariation between certain regular and irregular forms.115Table 4:Cluster IDRight 68Left 12 VBZLeft 19 VBGLeft 11 VBNLeft 8 VBDLeft 7 VBRight  69Left 19 VBGLeft 6 VBZ~Left 7 VBR ight  70Left 13 VBGLeft 10 VBZLeft 1 VBPLeft 5 VBNLeft 7 VBLeft 8 VBDRight 71Left 19 VBGLeft 10 VBZLeft 1 VBPLeft 7 VBLeft 8 VBDVerb paradigmsLabel VerbCALLADDCOMEMAKEcalls/VBZcalling/VBGcalled/VBNcalled/VBDcaI1/VBadding/VBGadded/VBDadd/VBcoming/VBGcomes/VBZcome/VBPcome/VBNcome/VBcame/VBDmaking/VBGmakes/VBZmake/VBPmake/VBmade/VBDright cluster membership for each verb, we wereable to predict the correct inflectional categoryand lemma for 67.5% of the 400 verbs.
Table4 shows a set of consecutive right clusters inwhich the lemma and inflectional category areco.rrectly predicted for all but one verb.6.1 Conclus ionWe have clearly demonstrated that a surprisingamount of information about the verb paradigmis strictly local to the verb.
We believe thatdistributional information combined with mor-phological information will produce extremelyaccurate classifications for both regular and ir-regular verbs.
The fact that information con-sisting of nothing more than bigrams can cap-ture syntactic information about English hasalready been noted by (Brown et al 1992).Our contribution has been to develop a largelyunsupervised method for cutting a cluster treethat produces reliable classifications.
We alsodeveloped an unsupervised method to enhancethe classification of one cluster tree by usingthe classification of another cluster tree.
Theverb paradigm is extracted by cross classifyinga verb by lemma and inflectional category.
Thismethod depends on the successful classificationof verbs by lemma and inflectional category sep-o/ .
.
.
.
.~ :/" \/.
.
.
.
Revised Ral~o"/' - - -  % C0fmct?
.
.
.
.
.
.
.
i;o go go ;oue i~tFigure 4: Value by height for left contextsf4.
*,/ , i!
.
.
.
.
.
? "
,I;o 2o ~ ioHamFigure 5: Value by height for right contexts116arately.We are encouraged by these results to con-tinue working towards fully unsupervised meth-ods for extracting verb paradigms using distri-butional information.
We hope to extend thisexploration to other languages.
We would alsolike to explore how the dual mechanisms of en-coding verb lemmas and inflectional categoriesboth by distributional criteria and by morphol-ogy can be exploited both by the child languagelearner and by automatic grammar extractionprocesses.ReferencesE.
Brill, D. Magerman, M. Marcus, and B.Santorini.
1990.
Deducing linguistic structurefrom the statistics of large corpora.
Proceed-ings of the DARPA Speech and Natural Lan-9ua9 e Workshop.
pp.
275-282.E.
Brill and S. Kapur.
1993.
An information-theoretic solution to parameter setting.
Pro-ceedings of the Georgetown University RoundTable on Languages and Linguistics: Sessionon Corpus Linguistics.P.F.
Brown, P.V.
deSouza, R.L.
Mercer, V.J.Della Pietra, J.C. Lai.
1992.
Class-based n-gram models of natural anguage.
Computa-tional Linguistics 18:467-480.T.
Cover and J. Thomas.
1991.
Elements off In-formation Theory.
Wiley: New York.I.
Dagan, F. Pereira, and L. Lee.
1994.Similarity-based stimation of word cooccur-rence probabilities.
Proceedings of the 32ndAnnual Meeting of the A CL.
Lass Cruces, NM.pp.
272-278.S.
Finch and N. Chater.
1992.
BootstrappingSyntactic Categories.
Proceedings of the 14thAnnual Conference of the Cognitive ScienceSociety of America.
Bloomington, Indiana.pp.
820-825.F.
Jelinek.
1990.
Self-organized language mod-eling for speech recognition.
In Waibel & Lee(Eds.)
Readings in Speech Recognition Mor-gan Kaufmann Pub., SanMateo, CA.D.M.
Magerman and M.P.
Marcus.
1990.
Pars-ing a natural anguage using mutual infor-mation statistics.
Proceedings of AAAI-90.Boston, MA.F.
Pereira, N. Tishby, and L. Lee.
1993.
Distri-butional clustering of English words, In Pro-ceedings of the 30th Annual Meeting of theACL.
Columbus, OH.
pp.
183-190.117S.
Pinker.
1984.
Language Learnability andLanguage Development.
Harvard UniversityPress, Cambridge, MA.J.
R. Quinlan.
1990.
Induction of DecisionTrees.
In J. W. Shavlik and T.G.
Dietterich(Eds.)
Readings in Machine Learning.
Mor-gan Kaufinann, Pub., SanMateo, CA.A.
Ratnaparkhi.
1997.
A simple introductionto maximum entropy models for natural an-guage processing.
Technical Report 97-08, In-stitute for Research in Cognitive Science,University of Pennsylvania.S.
Russell and P. Norvig.
1995.
Introduction toArtificial Intelligence.
Prentice Hall, Engle-wood Cliffs, NJ.
pp.
540-541G.A.F.
Seber.
1984.
Multivariate Observations.John Wiley & Sons: New York.
pp.
347-391.H.
Schuetze.
1993.
Part-of-speech inductionfrom scratch.
Proceedings of the 31st AnnualMeeting of the A CL.
Columbus, OH.
