Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 930?940,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsImproving Bilingual Projections via Sparse Covariance MatricesJagadeesh JagarlamudiUniversity of MarylandCollege Park, USAjags@umiacs.umd.eduRaghavendra UdupaMicrosoft ResearchBangalore, Indiaraghavu@microsoft.comHal Daume?
IIIUniversity of MarylandCollege Park, USAhal@umiacs.umd.eduAbhijit BholeMicrosoft ResearchBangalore, Indiav-abbhol@microsoft.comAbstractMapping documents into an interlingual rep-resentation can help bridge the language bar-rier of cross-lingual corpora.
Many existingapproaches are based on word co-occurrencesextracted from aligned training data, repre-sented as a covariance matrix.
In theory, sucha covariance matrix should represent seman-tic equivalence, and should be highly sparse.Unfortunately, the presence of noise leads todense covariance matrices which in turn leadsto suboptimal document representations.
Inthis paper, we explore techniques to recoverthe desired sparsity in covariance matrices intwo ways.
First, we explore word associationmeasures and bilingual dictionaries to weighthe word pairs.
Later, we explore differentselection strategies to remove the noisy pairsbased on the association scores.
Our experi-mental results on the task of aligning compa-rable documents shows the efficacy of sparsecovariance matrices on two data sets from twodifferent language pairs.1 IntroductionAligning documents from different languages arisesin a range of tasks such as parallel phrase extrac-tion (Gale and Church, 1991; Rapp, 1999), miningtranslations for out-of-vocabulary words for statis-tical machine translation (Daume III and Jagarla-mudi, 2011) and document retrieval (Ballesteros andCroft, 1996; Munteanu and Marcu, 2005).
In thistask, we are given a comparable corpora and somedocuments in one language are assumed to have acomparable document in the other language and thegoal is to recover this hidden alignment.
In this pa-per, we address this problem by mapping the docu-ments into a common subspace (interlingual repre-sentation).
This common subspace generalizes thenotion of vector space model for cross-lingual ap-plications (Turney and Pantel, 2010).Most of the existing approaches use manuallyaligned document pairs to find a common subspacein which the aligned document pairs are maximallycorrelated.
The sub-space can be found using ei-ther generative approaches based on topic modeling(Mimno et al, 2009; Jagarlamudi and Daume?
III,2010; Zhang et al, 2010; Vu et al, 2009) or dis-criminative approaches based on variants of Princi-pal Component Analysis (PCA) and Canonical Cor-relation Analysis (CCA) (Susan T. Dumais, 1996;Vinokourov et al, 2003; Platt et al, 2010; Haghighiet al, 2008).
Both styles rely on document levelterm co-occurrences to find the latent representation.The discriminative approaches capture essentialword co-occurrences in terms of two monolingualcovariance matrices and a cross-covariance matrix.Subsequently, they use these covariance matrices tofind projection directions in each language such thataligned documents lie close to each other (Sec.
2).The strong reliance of these approaches on the co-variance matrices leads to problems, especially withthe noisy data caused either by the noisy wordsin a document or the noisy document alignments.Noisy data is not uncommon and is usually the casewith data collected from community based resourcessuch as Wikipedia.
This degrades performance of a930variety of tasks, such as transliteration Mining (Kle-mentiev and Roth, 2006; Hermjakob et al, 2008;Ravi and Knight, 2009) and multilingual web search(Gao et al, 2009).In this paper, we address the problem of identi-fying and removing noisy entries in the covariancematrices.
We address this problem in two stages.In the first stage, we explore the use of word asso-ciation measures such as Mutual Information (MI)and Yule?s ?
(Reis and Judd, 2000) in computingthe strength of a word pair (Sec.
3.1).
We alsoexplore the use of bilingual dictionaries developedfrom cleaner resources such as parallel data.
In thesecond stage, we use the association strengths in fil-tering out the noisy word pairs from the covariancematrices.
We pose this as a word pair selection prob-lem and explore multiple strategies (Sec.
3.2).We evaluate the utility of sparse covariance ma-trices in improving the bilingual projections incre-mentally (Sec.
4).
We first report results on syn-thetic multi-view data where the true correspon-dences between features of different views are avail-able.
Moreover, this also lets us systematically ex-plore the effect of noise level on the accuracy.
Ourexperimental results show a significant improvementwhen the true correspondences are available.
Later,we report our experimental results on the documentalignment task on Europarl and Wikipedia data setsand on two language pairs.
We found that sparsify-ing the covariance matrices helps in general, but us-ing cleaner resource such bilingual dictionaries per-formed best.2 Canonical Correlation Analysis (CCA)In this section, we describe how Canonical Correla-tion Analysis is used to solve the problem of align-ing bilingual documents.
We mainly focus on repre-senting the solution of CCA in terms of covariancematrices.
Since most of the existing discriminativeapproaches are variants of CCA, showing the advan-tage of recovering sparseness in CCA makes it appli-cable to the other variants as well.Given a training data of n aligned document pairs,CCA finds projection directions for each language,so that the documents when projected along these di-rections are maximally correlated (Hotelling, 1936).Let X (d1?n) and Y (d2?n) be the representationof data in both the languages and further assume thatthe data is centered (subtract the mean vector fromeach document i.e.
xi?xi?
?x and yi ?
yi?
?y).Then CCA finds projection directions a and b whichmaximize:aTXY Tb?aTXXTa?bTY Y Tbs.t.
aTXXTa = 1 & bTY Y Tb = 1The projection directions are obtained by solving thegeneralized eigen system:[0 CxyCyx 0] [ab]=[(1-?
)Cxx+?I 00 (1-?
)Cyy+?I] [ab](1)where Cxx = XXT , Cyy = Y Y T are the monolin-gual covariance matrices, Cxy = XY T is the cross-covariance matrix and ?
is the regularization param-eter.
Using these eigenvectors as columns, we formthe projection matrices A and B.
These projectionmatrices are used to map documents in both the lan-guages into interlingual representation.Given any new pair of documents, their similarityis computed by first mapping them into the lower di-mensions space and computing the cosine similaritybetween their projections.
In general, using all theeigenvectors is sub optimal and thus retaining topeigenvectors leads to better generalizability.3 Covariance SelectionAs shown above, the underlying objective functionin most of the discriminative approaches is of theform aTXY Tb.
This can be rewritten as :aTXY Tb =n?k=1?xk,a?
?yk,b?=n?k=1( d1?i=1Xi,kai ?d2?j=1Yj,kbj)=d1?i=1d2?j=1aibj( n?k=1Xi,kYj,k)=d1,d2?i,j=1aibjCxyij (2)Similarly, the constraints can also be rewritten as?d1i,j=1 aiajCxxij = 1 and?d2i,j=1 bibjCyyij = 1.931Maximizing this objective function, under theconstraints, involves a careful selection of the vec-tors a and b such that aibj is high whenever Cxyijis high.
So, every non-zero entry of the cross-covariance matrix restricts the choice of the pro-jection directions.
While this may not be a severeproblem when the training data is clean, but this isvery uncommon especially in the case of high di-mensional data like text documents.
Moreover, theinherent ambiguity of natural languages increasesthe chances of seeing a noisy word in any docu-ment.
Every occurrence of a noisy word will have anon-zero contribution towards the covariance matrixmaking it dense, which in turn prevents the selectionof appropriate projection directions.In this section, we describe some techniques torecover the sparsity by removing the noisy entriesfrom the covariance matrices.
We break this taskinto two sub problems: computing an associationscore for every word pair and then using an appro-priate strategy to identify the noisy pairs based ontheir weights.
We explore multiple ways to addressboth the steps in the following two sections.
Forthe sake of convenience and clarity, we describe ourtechniques in the context of cross-covariance ma-trix between English and Spanish language pair.
Butthese techniques extend directly to monolingual co-variance matrices, and to different language pairs aswell.3.1 Computing Word Pair AssociationThe first step in filtering out the noisy word co-occurrences is to use an appropriate measure to com-pute the strength of word pairs (English and Span-ish words).
This is a well studied problem and sev-eral association measures have been proposed in theNLP literature (Dunning, 1993; Inkpen and Hirst,2002; Moore, 2004).
These association measurescan be divided into groups based on the statisticsthey use (Hoang et al, 2009).
Here we explore a fewof them for sparsifying the cross-covariance matrix.3.1.1 CovarianceThe first option is to use the cross-covariancematrix itself.
As noted above, when the data ma-trix is centered, the cross-covariance of an Englishword (ei) with a Spanish word (fj) is given by?nk=1 XikYjk.
It measures the strength with whichtwo words co-occur together.
This measure uses in-formation about the occurrence of a word pair inaligned documents and doesn?t use other statisticssuch as ?how often this pair doesn?t co-occur to-gether?
and so on.3.1.2 Mutual InformationAssociation measures like covariance and Point-wise Mutual Information, which only use the fre-quency with which a word pair co-occurs, oftenoverestimate the strength of low frequent words(Moore, 2004).
On the other hand, measureslike Log-likelihood ratio (Dunning, 1993) and Mu-tual Information (MI) use other statistics like themarginal probabilities of each of the words.For any two words, ei and fj , let n11, n10, n01and n00 denote the number of documents in whichboth the words co-occur, only English word occurs,only Spanish word occurs and none of the words oc-cur.
Then the Mutual Information of this word pairis given by:MI(ei, fj) =1n?i,j?
{0,1}nij lognij ?
nninj(3)where ni and nj denote the number of documentsin which the English and the Spanish word occursand n is the total number of documents.
We treatthe occurrence of a word in a document slightly dif-ferent from others, we treat a word as occurring ina document if it has occurred more than its averagefrequency in the corpus.
Log-likelihood ratio andthe MI differ only in terms of the constant they use,so we use only MI in our experiments.3.1.3 Yule?s ?Yule?s ?
is another popular association measureused in psychology (Reis and Judd, 2000).
It usessame statistics used by Mutual Information but dif-fers in the way in which they are combined.
MI con-verts the frequencies into probabilities before com-puting the association measure where as Yule?s ?uses the observed frequencies directly, and doesn?tmake any assumptions about the underlying proba-bility distributions.
Given the same interpretation ofthe variables as introduced in the previous section,the Yule?s ?
is estimated as:?
=?n00n11 ?
?n01n10?n00n11 +?n01n10(4)932This way of combining the frequencies bears simi-larity with the log-odds ratio.3.1.4 Bilingual DictionaryThe above three association measures use thesame training data that is available to compute thecovariance matrices in CCA.
Thus, their utility inbringing additional information, which is not cap-tured by the covariance matrices, is arguable (ourexperiments show that they are indeed helpful).Moreover, they use document level co-occurrenceinformation which is coarse compared to the co-occurrence at sentence level or the translational in-formation provided by a bilingual dictionary.
So,we use bilingual dictionaries as our final resource toweigh the word co-occurrences.
Notice that, usingbilingual information brings in information gleanedfrom an external corpus.We use translation tables learnt using Giza++(Och and Ney, 2003) on Europarl data set.
Since thetranslation tables are asymmetric, we combine trans-lation tables from both the directions.
We first use athreshold on the conditional probability to filter outthe low probability ones and then convert them intojoint probabilities before combining.
For each wordpair (ei, fj), we compute the score as:12(P (ei|fj)P (fj) + P (fj|ei)P (ei))While the first three association measures can alsobe applied to monolingual data, bilingual dictionarycan?t be used for weighting monolingual word pairs.So in this case, we use either of the above mentionedtechniques for weighting monolingual word pairs.3.2 Selection StrategiesThe next step after computing association measurefor all word pairs is to use them in selecting the pairsthat need to be retained.
In this section, we describesome approaches such as thresholding and matchingfor the word pair selection.3.2.1 ThresholdingA straight forward way to remove the noisy wordco-occurrences is to zero out the entries of thecross-covariance matrix that are lower than a thresh-old.
To understand the motivation, consider therewritten objective function of CCA, aTXY Tb =?ij Cxyij aibj .
This is linear in terms of the individ-ual components of the cross-covariance matrix.
So,if we want to remove some of the entries of the co-variance matrix with minimal change in the value ofthe objective function, then the optimal choice is tosort the entries of the covariance matrix and filter outthe less confident word pairs.3.2.2 Relative ThresholdingWhile the thresholding strategy described in theabove section is very simple, it is often biased bythe frequent words.
Since a frequent word co-occurswith other words often, it naturally tends to havehigh association with most of the other words.
Asa result, absolute thresholding tends to remove allthe less frequent word pairs while leaving the co-occurrences of the frequent words untouched.
Even-tually, this may lead to zeroing out some of the rowsor the columns of the cross-covariance matrix.To circumvent this, we try thresholding at wordlevel.
For every English word, we choose a fewSpanish words that have high association and viceversa.
Since the nearest neighbour property is asym-metric, we take the union of all the selected wordpairs.
That is, we retain a word pair, if either theSpanish word is in the top ranked list of the Englishword or vice versa.3.2.3 Maximal MatchingThough relative thresholding overcomes the prob-lem of zeroing out entire rows or columns posed bydirect thresholding, it is still biased by the frequentwords.
The high association measure of a frequentEnglish word with many Spanish words, makes it anearest neighbour for lot of Spanish words.
One wayto prevent this is to discourage an already selectedEnglish word from associating with a new Spanishword.
This requires a global knowledge of all theselected pairs and can not be done by looking at theindividual words, as is the case with the greedy strat-egy employed by the relative thresholding.We use matching to solve this problem.
We for-mulate the selection of the word pairs as a networkflow problem (Jagarlamudi et al, 2011).
The objec-tive is to select word pairs that have high associationmeasure while constraining each word to be asso-ciated with only a few words from other language.Let Iij denote an indicator variable taking a value of9330 or 1 depending on if the word pair (ei, fj) is se-lected or not.
We want each word to be associatedwith k words from other language, i.e.
?j Iij = kand?i Iij = k. Moreover, we want word pairswith high association score to be selected.
We canencode this objective and the constraints as the fol-lowing optimization problem:argmaxId1,d2?i,j=1Cxyij Iij (5)?i?jIij = k; ?j?iIij = k; ?i, j Iij ?
{0, 1}If k = 1, then this problem reduces to a linear as-signment problem and can be solved optimally us-ing the Hungarian algorithm (Jonker and Volgenant,1987).
For other values of k, this can be solved byrelaxing the constraint Iij ?
{0, 1} to 0 ?
Iij ?
1.The optimal solution of the relaxed problem can befound efficiently using linear programming (Ravin-dra et al, 1993).
The uni-modular nature of theconstraints guarantees an integral solution (Schri-jver, 2003), so relaxing the original integer problemdoesn?t introduce any error in the optimal solution.3.2.4 Monolingual AugmentationThe above three selection strategies operate on thecovariance matrices independently.
In this sectionwe propose to combine them.
Specifically, we pro-pose to augment the set of selected bilingual wordpairs using the monolingual word pairs.
We first useany of the above mentioned strategies to select bilin-gual and monolingual word pairs.
Let Ixy, Ixx andIyy be the binary matrices that indicate the selectedword pairs based on the bilingual and monolingualassociation scores.
Then the monolingual augmen-tation strategy updates Ixy in the following way:Ixy ?
Binarize(IxxIxyIyy)i.e., we multiply Ixy with the monolingual selectionmatrices and then binarize the resulting matrix.
Ourmonolingual augmentation is motivated by the fol-lowing probabilistic interpretation:P (x, y) =?x?,y?P (x|x?
)P (y|y?
)P (x?, y?
)which can be rewritten as P ?
T xP (T y)T whereT x and T y are monolingual state transition matrices.3.3 Our ApproachIn this section we summarize our approach for thetask of finding aligned documents from a cross-lingual comparable corpora.
The training phase in-volves finding projection directions for documentsof both the languages.
We compute the covariancematrices using the training data.
Then we use anyof the word association measures (Sec.
3.1) alongwith a selection criteria (Sec.
3.2) to recover thesparseness in either only the cross-covariance or allof the covariance matrices.
Let Ixy, Ixx and Iyybe the binary matrices which represent the wordpairs that are selected based on the chosen sparsi-fication technique.
Now, we replace the covariancematrices in Eq.
1 as follows: Cxx ?
Cxx ?
Ixx,Cyy ?
Cyy ?
Iyy and Cxy ?
Cxy ?
Ixy where?
denotes the element-wise matrix product.
Subse-quently, we solve the generalized eigenvalue prob-lem shown in Eq.
1 to obtain the projection direc-tions.
Let A and B be the matrices formed with topeigenvectors of Eq.
1 as the columns.
These pro-jection matrices are used to map documents into theinterlingual representation.
Such an interlingual rep-resentation is useful in many tasks like cross-lingualtext categorization (Bel et al, 2003) multilingualweb search (Gao et al, 2009) and so on.During the testing, given an English document x,finding an aligned Spanish document involves solv-ing:argmaxyxT((ABT )?
Ixy)y?xT((AAT )?
Ixx)x?yT((BBT )?
Iyy)yIf the documents are normalized before hand, thenthe above equation reduces to computing only thenumerator.4 Experiments4.1 Experimental SetupWe experiment with the task of finding aligned doc-uments from a cross-lingual comparable corpora.
Inthis task, we are given comparable corpora consist-ing of two document collections, each in a differ-ent language.
As the corpora are comparable, somedocuments in one collection have a comparable doc-ument in the other collection.
The task is to recover934this hidden alignment.
The recovered alignment iscompared against the ground truth.We evaluate our idea of sparsifying the covari-ance matrices incrementally.
We first evaluate theeffectiveness of our approach on synthetic data, asit enables us to systematically study the effect ofnoise.
Subsequently, we evaluate each of the abovediscussed sparsification strategies on real world datasets.
We have discussed four possible ways forcomputing word association measure and three ap-proaches for word pair selection.
That leaves us 12different ways for sparsifying the covariance matri-ces, with each method having parameters to controlthe amount of sparseness.
We use a small amount ofdevelopment data for model selection and parametertuning and choose a few promising models.
Finally,we compare these selected models with state-of-the-art baselines on two language pairs and on two dif-ferent data sets.In each case, we use the training data to learnthe projection directions.
And then, for each of thetest documents, we find the aligned document fromother language.
We report average accuracy of thetop ranked document and also the Mean ReciprocalRank (MRR) of the true aligned document.4.2 Synthetic DataWe follow the generative story introduced in Bachand Jordan (2005) to generate synthetic multi-viewdata.
Their method does not assume any correspon-dence between the feature dimensions of both theviews.
We modify their approach slightly so thatwe know the actual correspondence between the fea-tures.
We use these true feature correspondences forsparsification of the cross-covariance matrix.We first generate a d dimensional vector in thecommon latent space and then use the projectionmatrices to map it into the individual feature spacesas follows:z ?
N (0, Id)x|z ?
(W1z + ?1) + ?
N (0, Id1)y|z ?
(W1z + ?2) + ?
N (0, Id2)Notice that we use the same projection matrix W1for both the views, this ensures a one-to-one corre-spondence between the features of both the spaces.Moreover, we also introduce a parameter ?
whichcontrols the amount of noise in the data.00.10.20.30.40.50.60.70.80.911  1.5  2  2.5  3  3.5  4Sparse MRRSparse AccuracyCCA MRRCCA AccuracyFigure 1: Accuracy of CCA and our sparsified versionwith the noise parameter.We generate a total of 3000 pairs of points and use2000 of them for training the models and the restfor evaluation.
We use the true feature correspon-dences to form the cross-covariance selection ma-trix Ixy (Sec.
3.3).
For this experiment, we use thefull monolingual covariance matrices.
We train bothCCA and our sparse version on the training data andevaluate them on the test data.
We repeat this mul-tiple times and report the average accuracies.
Fig.
1shows the performance of CCA and our sparse CCA,as we vary the noise parameter ?
from 1 to 4.
Itis very clear that the sparse version performs sig-nificantly better than CCA.
As the noise increases,the performance of CCA drops quickly.
This exper-iment demonstrates a significant performance gainwhen the true correspondences are available.
Butthis information is not available in the case of realworld data sets, so we try to approximate it.4.3 Model SelectionAs we have discussed, there are several choices forcomputing the association measure and for selectingthe word pairs to be retained.
And each of them havesparsity parameters, giving raise to many possiblemodels.
For model selection, we use approximately5000 document pairs collected from the Wikipediabetween English and Spanish.
We use the cross-language links provided as the ground truth.
We to-kenize the documents, retain only the most frequent2000 words in each language and convert the docu-9350.40.450.50.550.60.652000  4000  6000  8000  10000  12000  14000  16000  18000  20000MI+MatchYule+MatchCov+MatchMI+RelThresholdYule+RelThresholdCov+RelThresholdMI+ThresholdYule+ThresholdCov+ThresholdCCAFigure 2: Comparison of the word association measuresalong with different selection criteria.
The x-axis plotsthe number of non-zero entries in the covariance matricesand the y-axis plots the accuracy of top-ranked document.ments into TFIDF vectors.
We use 60% of the datafor training different models and the rest for evaluat-ing the models.
We choose a few promising modelsbased on this development set results and evaluatethem on bigger data sets.4.3.1 Selection StrategiesIn the first experiment, we combine the threeassociation measures, Covariance (Cov), MI andYule?s ?, with the three selection criteria, Thresh-old, Relative Threshold (RelThreshold) and Match-ing (Match).
Fig.
2 shows the performance of thesedifferent combinations with varying levels of spar-sity in the covariance matrices.
The horizontal linerepresents the performance of CCA on this data set.We start with 2000 non-zero entries in the covari-ance matrices and experiment up to 20,000 non-zeroentries.
Since our data set has 2000 words in eachlanguage, 2000 non-zero entries in a covariance ma-trix implies that, on an average, every word is as-sociated with only one word.
This results in highlysparse covariance matrices.Overall, we observe that reducing the level ofsparsity , i.e.
selecting more number of elements inthe covariance matrices, increases the performanceslightly and then decreases again.
From the figure, itseems that sparsifying the covariance matrices mighthelp in improving the performance of the task.
Butit is interesting to note that not all the models per-form better than CCA.
In fact, both the models thatachieve better scores use Matching as the selectioncriteria.
This suggests that, apart from the weightingof the word pairs, appropriate selection of the wordpairs is also equally important.
In the rest of the ex-periments we mainly report results with Matching asthe selection criterion.
From this figure, we observethat Mutual Information and Yule?s ?
perform com-petitively but they consistently outperform modelsthat use covariance as the association measure.
Soin the rest of the experiments we report results withMI or Yule?s ?.4.3.2 Amount of SparsityIn the previous experiment, we used same levelof sparsity for all the covariance matrices, i.e.
samenumber of associations were selected for each wordin all the three covariance matrices.
In the followingexperiment, we use different levels of sparsity forthe individual covariance matrices.
Fig.
3 shows theperformance of Yule+Match and Dictionary+Matchcombinations with different levels of sparsity.
Inthe Yule+Match combination, we use Yule?s ?
as-sociation measure for weighting the word pairs anduse matching for selection.
In the Dictionary+Matchcombination, we use bilingual dictionary for sparsi-fying cross-covariance matrix, i.e.
we keep all theword pairs whose conditional translation probabil-ity is above a threshold.
And for monolingual wordpairs, we use MI for weighting and matching forword pair selection.For each level of sparsity of the cross-covariancematrix, we experiment with different levels of spar-sity on the monolingual covariance matrices.
?OnlyXY?
indicates we use the full monolingual covari-ance matrices.
In ?Match(k)?
runs, we allow eachword to be associated with a total of k words (Eq.
5).?Aug?
indicates that we use monolingual augmen-tation to refine the sparsity of the cross-covariancematrix (Sec.
3.2.4).From both the figures 3(a) and 3(b), we observethat ?Only XY?
run (dark blue) performs poorlycompared to the other runs, indicating that sparsify-ing all the covariance matrices is better than spar-sifying only the cross-covariance matrix.
In the936(a) Performance of Yule+Match combination.
The x-axis plotsthe number of Spanish words selected per each English wordand vice versa.
This determines the sparsity of Cxy.
Matchingis used as selection criteria for all the covariance matrices.
(b) Performance of Dictionary+Match combination.
The x-axisplots the threshold on bilingual translation probability and it deter-mines the sparsity of Cxy.
Matching is used to select only the mono-lingual sparsity.Figure 3: Comparison of Yule+Match and Dictionary+Match combination with different levels of sparsity for thecovariance matrices.
In both the figures, the x-axis plots the sparsity of the cross-covariance matrix and for eachvalue we try different levels of sparsity on the monolingual covariance matrices (which are grouped together).
Thedescription of these individual runs is provided in the relevant parts of the text.
The y-axis plots the accuracy of thetop-ranked document.
CCA achieves 61% accuracy on this data set.Yule+Match combination, Fig.
3(a), all the runsseem to be performing better when each Englishword is allowed to associate with 2 or 3 Spanishwords and vice versa.
Among different ways of se-lecting the monolingual word pairs, Match(2)+Augperforms better than the remaining runs.
So we useMatch(2)+Aug combination for the Yule?s ?
mea-sure.Unlike the Yule+Match combinations, there is noclear winner for Dictionary+Match combinations.First of all, the performance increase as we increasethe translation probability threshold and then de-creases again (indicated by the ?Average?
perfor-mance in Fig.
3(b)).
On an average, all the sys-tems perform better with a threshold of 0.01, whichwe use in our final experiments.
In this case, bothMatch(1) and Match(2)+Aug runs (orange and greenbars respectively) perform competitively so we useboth of these models in our final experiments.In both the above experiments, the performancebars are very similar when we use MI instead ofYule and vice versa for weighting monolingual wordpairs.
Thus, to illustrate the main ideas we choseYule?s ?
for the former combination and MI for thelatter combination.4.3.3 Promising ModelsBased on the above experiments, we choose thefollowing combinations for our final experiments.Yule(l)+Match(k), where l ?
{2, 3} is the numberof Spanish words allowed for each English wordand vice versa and k=2 is the number of monolin-gual word associations for each word.
We also runboth these combinations with monolingual augmen-tation, indicated by Yule(l)+Match(k)+Aug.
Fordictionary based weighting, Dictionary+Match(k),we choose a translation probability threshold of 0.01and try k ?
{1, 2}.
Again, we run these combina-tions with monolingual augmentation identified byDictionary+Match(k)+Aug.4.4 ResultsFor our final results, we choose data in two languagepairs (English-Spanish and English-German) fromtwo different resources, Europarl (Koehn, 2005) andWikipedia.
For Europarl data sets, we artificiallymake them comparable by considering the first half937Wikipedia EuroparlEnglish-Spanish English-German English-Spanish English-GermanAcc.
MRR Acc.
MRR Acc.
MRR Acc.
MRRCCA 0.776 0.852 0.570 0.699 0.872 0.920 0.748 0.831OPCA 0.781 0.856 0.570 0.700 0.870 0.920 0.748 0.831Yule(2)+Match(2) 0.798?
0.866?
0.576 0.703 0.901?
0.939?
0.780?
0.853?Yule(2)+Match(2)+Aug 0.811?
0.876?
0.602?
0.723?
0.883 0.927 0.771?
0.847?Yule(3)+Match(2) 0.803?
0.870?
0.572 0.700 0.856 0.907 0.747 0.830Yule(3)+Match(2)+Aug 0.793?
0.861?
0.610?
0.726?
0.878+ 0.925+ 0.763+ 0.843?Dictionary+Match(1) 0.811?
0.875?
0.656?
0.762?
0.928?
0.957?
0.874?
0.922?Dictionary+Match(2) 0.811?
0.876?
0.623?
0.736?
0.923?
0.955?
0.853?
0.907?Dictionary+Match(2)+Aug 0.825?
0.885?
0.630?
0.735?
0.897?
0.935?
0.866?
0.917?Table 1: Performance of our models in comparison with CCA and OPCA on English-Spanish and English-Germanlanguage pairs.
?
and + indicate statistical significance measured by paired t-test at p=0.01 and 0.05 levels respectively.When an improvement is significant at p=0.01 it is automatically significant at p=0.05 and hence is not shown.of English document and the second half of itsaligned foreign language document (Mimno et al,2009).
For Wikipedia data set, we use the cross-language link as the ground truth.
For each of thesedata sets, we choose approximately 5000 aligneddocument pairs.
We remove the stop words and keepall the words that occur in at least five documents.After the preprocessing, on an average, we are leftwith 4700 words in each language.
Subsequently weconvert the documents into their TFIDF representa-tion.In Platt et al (2010), the authors compare differ-ent systems on the comparable document retrievaltask and show that discriminative approaches workbetter compared to their generative counter parts.So, here we compare only with the state-of-the-art discriminative systems such as CCA and OPCA(Platt et al, 2010).
For each of the systems, we re-port the average results of five-fold cross validation.We divide the data into 3:1:1 ratio for training, vali-dation and test sets.
The validation data set is used toselect the best number of dimensions of the commonsub space.
For both CCA and our models, we set theregularization parameter ?
to 0.3 which we foundworks well in a relevant but different experiments.For OPCA, we manually tried different regulariza-tion parameters ranging from 0.0001 to 1 and foundthat a value of 0.001 worked best.The results are shown in Table 1.
On these datasets, both CCA and OPCA performed competitively.OPCA takes advantage of the common vocabularyin both the languages.
But in our data sets, vocab-ulary of both the languages is treated differently, soit is not surprising that they give almost the sameresults.
From the results, it is clear that sparsify-ing the covariance matrices helps improving the ac-curacies significantly.
In all the four data sets, thebest performing method always used dictionary forcross-lingual sparsity selection.
This indicates thatusing fine granular information such as a bilingualdictionary gleaned from an external source is veryhelpful in improving the accuracies.
Among themodels that rely solely on the training data, modelsthat use monolingual augmentation performed bet-ter on Wikipedia data set, while models that do notuse augmentation performed better on Europarl datasets.
This suggests that, when the aligned documentsare clean (closer to being parallel) the statistics com-puted from cross-lingual corpora are trustworthy.
Asthe documents become comparable, we need to usemonolingual statistics to refine the bilingual statis-tics.
Moreover, these models achieve higher gains inthe case of Wikipedia data set compared to the gainsin Europarl.
This conforms with our initial hunchthat, when the training data is clean the covariancematrices tend to be less noisy.5 DiscussionIn this paper, we have proposed the idea of sparsi-fyng covariance matrices to improve bilingual pro-938jection directions.
We are not aware of any NLPresearch that attempts to recover the sparseness ofthe covariance matrices to improve the projectiondirections.
Our work is different from the sparseCCA (Hardoon and Shawe-Taylor, 2011; Rai andDaume?
III, 2009) proposed in the Machine Learningliterature.
Their objective is to find projection di-rections such that the original documents are repre-sented as a sparse vectors in the common sub-space.Another seemingly relevant but different directionis the sparse covariance matrix selection research(Banerjee et al, 2005).
The objective in this workis to find matrices such that the inverse of the co-variance matrix is sparse which has applications inGaussian processes.In this paper, we tried sparsification in the con-text of CCA only but our technique is general andcan be applied to its variants like OPCA.
Our ex-perimental results show that using external informa-tion such as bilingual dictionaries which is gleanedfrom cleaner resources brings significant improve-ments.
Moreover, we also observe that computingword pair association measures from the same train-ing data along with an appropriate selection criteriacan also yield significant improvements.
This is cer-tainly encouraging and in future we would like toexplore more sophisticated techniques to recover thesparsity based on the training data itself.6 AcknowledgmentsWe thank the anonymous reviewers for their help-ful comments.
This material is partially supportedby the National Science Foundation under Grant No.1139909.ReferencesFrancis R. Bach and Michael I. Jordan.
2005.
A proba-bilistic interpretation of canonical correlation analysis.Technical report, Dept Statist Univ California Berke-ley CA Tech.Lisa Ballesteros and W. Bruce Croft.
1996.
Dictio-nary methods for cross-lingual information retrieval.In Proceedings of the 7th International Conferenceon Database and Expert Systems Applications, DEXA?96, pages 791?801, London, UK.
Springer-Verlag.Onureena Banerjee, Alexandre d?Aspremont, and Lau-rent El Ghaoui.
2005.
Sparse covariance selectionvia robust maximum likelihood estimation.
CoRR,abs/cs/0506023.Nuria Bel, Cornelis H. A. Koster, and Marta Villegas.2003.
Cross-lingual text categorization.Hal Daume III and Jagadeesh Jagarlamudi.
2011.
Do-main adaptation for machine translation by mining un-seen words.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics:Human Language Technologies, pages 407?412, Port-land, Oregon, USA, June.
Association for Computa-tional Linguistics.Ted Dunning.
1993.
Accurate methods for the statis-tics of surprise and coincidence.
Comput.
Linguist.,19(1):61?74, March.William A. Gale and Kenneth W. Church.
1991.
A pro-gram for aligning sentences in bilingual corpora.
InProceedings of the 29th annual meeting on Associ-ation for Computational Linguistics, pages 177?184,Morristown, NJ, USA.
Association for ComputationalLinguistics.Wei Gao, John Blitzer, Ming Zhou, and Kam-Fai Wong.2009.
Exploiting bilingual information to improveweb search.
In Proceedings of Human Language Tech-nologies: The 2009 Conference of the Association forComputational Linguistics, ACL-IJCNLP ?09, pages1075?1083, Morristown, NJ, USA.
ACL.Aria Haghighi, Percy Liang, Taylor B. Kirkpatrick, andDan Klein.
2008.
Learning bilingual lexicons frommonolingual corpora.
In Proceedings of ACL-08:HLT, pages 771?779, Columbus, Ohio, June.
Associa-tion for Computational Linguistics.David R. Hardoon and John Shawe-Taylor.
2011.
Sparsecanonical correlation analysis.
Journal of MachineLearning, 83(3):331?353.Ulf Hermjakob, Kevin Knight, and Hal Daume?
III.
2008.Name translation in statistical machine translation -learning when to transliterate.
In Proceedings of ACL-08: HLT, pages 389?397, Columbus, Ohio, June.
As-sociation for Computational Linguistics.Hung Huu Hoang, Su Nam Kim, and Min-Yen Kan.2009.
A Re-examination of Lexical Association Mea-sures.
In Proceedings of ACL-IJCNLP 2009 Workshopon Multiword Expressions: Identification, Interpre-tation, Disambiguation and Applications, Singapore,August.
Association for Computational Linguistics.H.
Hotelling.
1936.
Relation between two sets of vari-ables.
Biometrica, 28:322?377.Diana Zaiu Inkpen and Graeme Hirst.
2002.
Ac-quiring collocations for lexical choice between near-synonyms.
In Proceedings of the ACL-02 workshopon Unsupervised lexical acquisition - Volume 9, ULA?02, pages 67?76, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.939Jagadeesh Jagarlamudi and Hal Daume?
III.
2010.
Ex-tracting multilingual topics from unaligned compara-ble corpora.
In Advances in Information Retrieval,32nd European Conference on IR Research, ECIR,volume 5993, pages 444?456, Milton Keynes, UK.Springer.Jagadeesh Jagarlamudi, Hal Daume III, and RaghavendraUdupa.
2011.
From bilingual dictionaries to interlin-gual document representations.
In Proceedings of the49th Annual Meeting of the Association for Compu-tational Linguistics: Human Language Technologies,pages 147?152, Portland, Oregon, USA, June.
Associ-ation for Computational Linguistics.R.
Jonker and A. Volgenant.
1987.
A shortest augment-ing path algorithm for dense and sparse linear assign-ment problems.
Computing, 38(4):325?340.Alexandre Klementiev and Dan Roth.
2006.
Weaklysupervised named entity transliteration and discoveryfrom multilingual comparable corpora.
In Proceed-ings of the 21st International Conference on Compu-tational Linguistics and the 44th annual meeting of theAssociation for Computational Linguistics, ACL-44,pages 817?824, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Philipp Koehn.
2005.
Europarl: A Parallel Corpusfor Statistical Machine Translation.
In ConferenceProceedings: the tenth Machine Translation Summit,pages 79?86, Phuket, Thailand.
AAMT, AAMT.David Mimno, Hanna M. Wallach, Jason Naradowsky,David A. Smith, and Andrew McCallum.
2009.Polylingual topic models.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing: Volume 2 - Volume 2, EMNLP ?09,pages 880?889, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Robert C. Moore.
2004.
On Log-Likelihood-Ratiosand the Significance of Rare Events.
In Dekang Linand Dekai Wu, editors, Proceedings of EMNLP 2004,pages 333?340, Barcelona, Spain, July.
Associationfor Computational Linguistics.Dragos Stefan Munteanu and Daniel Marcu.
2005.
Im-proving machine translation performance by exploit-ing non-parallel corpora.
Comput.
Linguist., 31:477?504, December.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.John C. Platt, Kristina Toutanova, and Wen-tau Yih.2010.
Translingual document representations fromdiscriminative projections.
In Proceedings of the2010 Conference on Empirical Methods in NaturalLanguage Processing, EMNLP ?10, pages 251?261,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Piyush Rai and Hal Daume?
III.
2009.
Multi-label pre-diction via sparse infinite cca.
In Advances in NeuralInformation Processing Systems, Vancouver, Canada.Reinhard Rapp.
1999.
Automatic identification of wordtranslations from unrelated english and german cor-pora.
In Proceedings of the 37th annual meetingof the Association for Computational Linguistics onComputational Linguistics, ACL ?99, pages 519?526,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Sujith Ravi and Kevin Knight.
2009.
Learning phonememappings for transliteration without parallel data.
InProceedings of Human Language Technologies: The2009 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 37?45, Boulder, Colorado, June.
Association forComputational Linguistics.K.
Ahuja Ravindra, L. Magnanti Thomas, and B. OrlinJames.
1993.
Network Flows: Theory, Algorithms,and Applications.
Prentice-Hall, Inc.Harry T Reis and Charles M Judd.
2000.
Handbook ofResearch Methods in Social and Personality Psychol-ogy.
Cambridge University Press.Alexander Schrijver.
2003.
Combinatorial Optimization.Springer.Michael L. Littman Susan T. Dumais, Thomas K. Lan-dauer.
1996.
Automatic cross-linguistic informationretrieval using latent semantic indexing.
In WorkingNotes of the Workshop on Cross-Linguistic Informa-tion Retrieval, SIGIR, pages 16?23, Zurich, Switzer-land.
ACM.Peter D. Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of semantics.J.
Artif.
Intell.
Res.
(JAIR), 37:141?188.Alexei Vinokourov, John Shawe-taylor, and Nello Cris-tianini.
2003.
Inferring a semantic representationof text via cross-language correlation analysis.
InAdvances in Neural Information Processing Systems,pages 1473?1480, Cambridge, MA.
MIT Press.Thuy Vu, AiTi Aw, and Min Zhang.
2009.
Feature-basedmethod for document alignment in comparable newscorpora.
In EACL, pages 843?851.Duo Zhang, Qiaozhu Mei, and ChengXiang Zhai.
2010.Cross-lingual latent topic extraction.
In Proceed-ings of the 48th Annual Meeting of the Associationfor Computational Linguistics, pages 1128?1137, Up-psala, Sweden, July.
Association for ComputationalLinguistics.940
