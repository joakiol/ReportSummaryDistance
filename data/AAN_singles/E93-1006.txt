Using an Annotated Corpus as a Stochastic GrammarRens BodDepartment of Computational LinguisticsUniversity of AmsterdamSpuistraat 134NL-1012 VB Amsterdamrens@alf.leLuva.nlAbstractIn Data Oriented Parsing (DOP), an annotatedcorpus is used as a stochastic grammar.
Aninput string is parsed by combining subtreesfrom the corpus.
As a consequence, one parsetree can usually be generated by severalderivations that involve different subtrces.
Thisleads to a statistics where the probability of aparse is equal to the sum of the probabilities ofall its derivations.
In (Scha, 1990) an informalintroduction to DOP is given, while (Bed,1992a) provides a formalization of the theory.In this paper we compare DOP with otherstochastic grammars in the context of FormalLanguage Theory.
It it proved that it is notpossible to create for every DOP-model astrongly equivalent s ochastic CFG which alsoassigns the same probabilities to the parses.We show that the maximum probability parsecan be estimated in polynomial time byapplying Monte Carlo techniques.
The modelwas tested on a set of hand-parsed strings fromthe Air Travel Information System (ATIS)spoken language corpus.
Preliminaryexperiments yield 96% test set parsingaccuracy.1 MotivationAs soon as a formal grammar characterizes a non-trivial part of a natural language, .almost every inputstring of reasonable ngth gets an unmanageably largenumber of different analyses.
Since most of theseanalyses are not perceived as plausible by a humanlanguage user, there is a need for distinguishing theplausible parse(s) of an input string from theimplausible ones.
In stochastic language processing, itis assumed that the most plausible parse of an inputstring is its most probable parse.
Most instantiationsof this idea estimate the probability of a parse byassigning application probabilities to context freerewrite roles (Jelinek, 1990), or by assigningcombination probabilities to elementary structures(Resnik, 1992; Schabes, 1992).There is some agreement ow that context free rewriterules are not adequate for estimating the probability ofa parse, since they cannot capture syntactie/lexicalcontext, and hence cannot describe how the probabilityof syntactic structures or lexical items depends on thatcontext.
In stochastic tree-adjoining grammar(Schabes, 1992), this lack of context-sensitivity sovercome by assigning probabilities to largerstructural units.
However, it is not always evidentwhich structures should be considered as elementarystructures.
In (Schabes, 1992) it is proposed to infer astochastic TAG from a large training corpus using aninside-outside-like t rative algorithm.Data Oriented Parsing fDOP) (Scha, 1990; Bod,1992a), distinguishes itself from other statisticalapproaches in that it omits the step of inferring agrammar from a corpus.
Instead, an annotated corpusis directly used as a stochastic grammar.
An inputstring is parsed by combining subtrees from thecorpus.
In this view, every subtree can be consideredas an elementary structure.
As a consequence, oneparse tree can usually be generated by severalderivations that involve different subtrees.
This leadsto a statistics where the probability of a parse is equalto the sum of the probabilities of all its derivations.
Itis hoped that this approach can accommodate allstatistical properties of a language corpus.37Let  us illustrate DOP with an extremely simpleexample.
Suppose that a cotpns consists of only twotrees:ANP VP NP VPSuppose that our combination operation (indicatedwith o) consists of substituting a subtree on theleftmost identically labeled leaf node of anothersubtree.
Then the sentence Mary likes Susan can beparsed as an S by combining the following subtre~from the corpus.S o NP o NP/kv NPlNP VPI AMary V NPi s !But the same parse tree can also be derived bycombining other subirees, for instance:S o NP  o V/kv ~ISmS o NP o VP o NP/"-,.w sI, L,,LThus, a parse can have several derivations involvingdifferent subtrees.
These derivations have differentprobabilities.
Using the corpus as our stochasticgrammar, we estimate the probabifity of substituting acertain subtree on a specific node as the probability ofselecting this subtree among all subtrees in the corpusthat could be substituted on that node.
The probabilityof a derivation can be computed as the product of theprobabilities of the subtre~ that are combined.
For theexample derivations above, this yields:P(Ist example) = 1/20 ?
1/4 ?
1/4P(2nd example) = 1/20 ?
1/4 ?
1/2P(3rd example) = 2/20 ?
1/4 ?
1/8 ?
1/4= 1/320= 1/160= 1/1280This example illustrates that a stntigtical languagemodel which defines probabilities over parses bytaking into ac~unt only one ,derivation, does notaccommodate all statistical properties of a languagecorpus.
Instead, we will defme the probability of aparse as the sum of the probabilities of all itsderivations.
Finally, the probability of a string isequal to the sum of the probabilities of all its parses.We will show ,hat conventional parsing techniquescan be applied to DOP, but that this becomes veryinefficient, since the number of derivations of a parsegrows exponentially with the length of the inputsuing.
However, we will show that DOP can beparsed in polynomial time by using Monte Carlotechniques.An important advantage of using a corpus forprobability calculation, is that no tr0jning ofparameters is needed, as is the case for other stochasticgrammars (Jelinek et al, 1990; Pereira and Schabes,1992; Schabes, 1992).
Secondly, since we take intoaccount all derivations of a parse, no relationship thatmight possibly be of statistical interest is ignored.382 The ModelAs might be clear by now, a IX)P-model ischaracterized by a corpus of tree structures, togetherwith a set of operations that combine subtrees fromthe corpus into new trees.
In this section we explainmore precisely what we mean by subtree, operationsetc., in order to arrive at definitions of a parse and theprobability of a parse with respect o a corpus.
For atreatment of DOP in more formal terms we refer to(Bod, 1992a).2.1 SubtreeA subtree of a tree T is a connected subgraph S of Tsuch that for every node in S holds that if it hasdaughter nodes, then these are equal to the daughternodes of the corresponding node in T. It is trivial tosee that a subuee is also a tree.
In the followingexample T 1 and T2 are subtrees of T, whereas T 3isn't.Y SI / xJohn V NP~s Tavp T3sNP VP V NPA I I IV NP likes John NPThe general definition above also includes subUeesconsisting of one node.
Since such subtrees do notcontribute to the parsing process, we exclude thesepathological cases and consider as the set of sublreesthe non-trivial ones consisting of more than one node.We shall use the following notation to indicate that atree t is a non-trivial subtree of a tree in a corpus C:t e C =oer 3 T 6 C: t is a non-trivial subtree o f  T2.2 OperationsIn this article we will limit ourselves to the basicoperation of substitution.
Other possible operationsare left to future research.
If t and u are trees, such thatthe leftmost non-terminal leaf  of  t is equal to the rootof u, then tou is the tree that results from substitutingthis non-terminal leaf in t by tree u.
The partialfunction o is called substitution.
We will write(tou)ov as touov, and in general (..((tlot2)ot3)o..)otn astlot2ot3o...otn.
The restriction le?tmost in the defin-ition is motivated by the fact that it eliminatesdifferent derivations consisting of the same subtrees.2.3 ParseTree Tis a parse of input string s with respect o acorpus C, iffthe y ie ldof  Tis equal to s and there aresubtrees tI,...,tn e C, such that T-- tlO.., otn.
The setof parses of s with respect to C, is thus given by:parses(s,C) ={T I yield(T) = s A 3 tl ..... tne C: T = tlo...otn}The definition correctly includes the trivial case of asubtree from the corpus whose yield is equal to thecomplete input string.2.4 DerivationA derivation of a parse T with respect to a corpus C,is a tuple of subtrees (t l  ..... ta) such  that  tl  ..... tne  Cand tlo...otn = T. The set of derivations of T withrespect to C, is thus given by:Derivations(T,C) ={(tl ..... t~) I t l  ..... tne  C A tlO...otn= T}2.5 Probability2.5.1 SubtreeGiven a subtree tl e C, a function root that yields theroot of a tree, and a node labeled X, the conditionalprobability P(t=tl / root(t)=X) denotes the probabilitythat t/ is substituted on X.
If root (Q)?
X, tinsprobability is 0.
If root(t1) = X, this probability canbe estimated as the ratio between the number ofoccurrences of tl in C and the total number ofoccurrences of subtrees t' in C for which holds thatroot(f) = X. Evidently, Zi  P(t=-ti I root(O=X) = 1holds.2.5.2 DerivationThe probability of a derivation (tl ..... tn) is equal tothe probability that the subtrees tl ..... tn are combined.This probability can be computed as the product of the39conditional probabilities of the subtrees tl ..... t o. LetlnI(x) be the leflmost non-terminal leaf of tree x, then:P(t=tllrOOt(t)--S) ?
I-li-_.2ton P(t=ti I root(t) = lnl(ti.l))2.5.3 ParseThe probability of a parse is equal to the probabilitythat any of its derivations occurs.
Since thederivations are mutually exclusive, the probability of aparse T is the sum of the probabilities of all itsderivations.
Let Detivations(T,C) = \[ d I ..... dn}, then:P(T) = ~,i P(di).
The conditional probability of aparse T given input siring s, can be computed as theratio between the probability of T and the sum of theprobabilities of all parses of s.2.5.4 StringThe probability of a string is equal to the probabilitythat any of its parses occurs.
Since the parses aremutually exclusive, the probability of a string s can becomputed as the sum of the probabilities of all itsparses.
Let Parse.s(s,C) = {T I ..... Tn}, then: P(s) =2~ i P(T/).
It can be shown that ~'i P(si) = 1 holds.3 Superstrong EquivalenceThere is an important question as to whether it ispossible to create for every DOP-model a stronglyequivalent stochastic CFG which also assigns thesame probabifities to the parses.
In order to discussthis question, we introduce the notion of superstrongequivalence.
Two stochastic grammars are calledsuperstrongly equivalent, if they are stronglyequivalent (i.e.
they generate the same strings with thesame trees) and they generate the same probabilitydistribution over the trees.The question as to whether for every DOP-model thereexists a strongly equivalent s ochastic CFG, is rathertrivial, since every subtree can be decomposed intorewrite rules describing exactly every level ofconstituent s ructure of that subtree.
The question asto whether for every DOP-model there exists asupets?ongly equivalent s ochastic CFG, can also beanswered without oo much difficulty.
We shall give acounter-example, showing that there exists a DOP-model for which there is no superstrongly equivalentstochastic CFG.Proposition It is not the case that/'or every DOP-model there exists a superstrongly equivalentstochastic CFG.ProofConsider the following DOP-model, consisting of acorpus with just one tree.S bIaThis corpus contains three subtrees, namelyS SS bIatlSIS b at2 t3The conditional probabilities of the subtrees are:P(t=-t I I root(t)=S) = 1/3, P(t=t 2 1 root(t)=S) = 1/3,P(~t3 1 root(t)=S) = 1/3.
Thus, Z, i P(t=ti fi'oot(t)=S) =1 holds.
The language generated by this model is{ab*}.
Let us consider the probabilities of the parsesof the strings a and ab.
The parse of siring a can begenerated by exactly one derivation: by applyingsubtree t3.
The probability of this parse is hence qualto 1/3.
The parse of ab can be generated by twoderivations: by applying subtree tl, or by combiningsubUees t2 and t3.
The probability of this parse isequal to the sum of the probabilities of its twoderivations, which is equal to P(t=--tl~OOt(t)=S) +P(~t2~oot(t)=S) * P(t=t31root(t)=S)= 1/3 + 1/3,1/3=4/9.If we now want o construct asuperstrongly equivalentstochastic CFG, it should assign the sameprobabilities to these parses.
We will show that this isimpossible.
A CFG which is strongly equivalent withthe DOP-model above, should contain the followingrewrite rules.S ~ Sb (1)S --, a (2)There may be other ules as well, but they should notmodify the language or slructures generated by theCFG above.
Thus, the rewrite rule S --~ A may be40added to the rules, as well as A --~ B, whereas therewrite rule S -o ab may not be added.Our problem is now whether we can assignprobabilities to these rules such that the probability ofthe parse of a equals 1/3, and the probability of theparse of ab equals 4/9.
The parse of a can exhaustivelybe generated by applying rule (2), while the parse ofab can exhaustively be generated by applying rules (1)and (2).
Thus the following should hold:P(2) = 1/3P(1)*P(2) = 4/9This implies that t)(I),1/3 = 4/9, thus P(1) = 4/9 ?
3= 4/3.
This means that the probability of rule (1)should be larger than I, which is not allowed.
Thus,we have proved that not for every DOP-model thereexists a superstrongly equivalent stochastic CFG.
In(Bod, 1992b) superstrong equivalence relationsbetween other stochastic grammars are studied.4 Monte Carlo ParsingIt is easy to show that an input string can be parsedwith conventional parsing techniques, by applyingsubtrees instead of rules to the input string (Bod,1992a).
Every subtree t can be seen as a productionrule toot(O --, ~ where the non-terminals of the yieldof the right hand side constitute the symbols to whichnew rules/subtrees are applied.
Given a polynomialtime parsing algoritiun, a derivation of the inputstring, and hence a parse, can be calculated inpolynomial time.
But if we calculate the probabilityof a parse by exhaustively calculating all itsderivations, the time complexity becomes exponential,since the number of derivations of a parse of an inputstring grows exponentially with the length of theinput string.Nevertheless, by applying Monte Carlo techniquesCrlammersley and Handscomb, 1964), we can estimatethe probability of a parse and make its error arbitrarilysmall in polynomial time.
The essence of MonteCarlo is very simple: it estimates a probabilitydistribution of events by taking random samples.
Thelarger the samples we take, the higher the reliability.For DOP this means that, instead of exhaustivelycalculating all parses with all their derivations, werandomly calculate N parses of an input string (bytaking random samples from the subtrees that can besubstituted on a specific node in the parsing process).The estimated probability of a certain parse given theinput string, is then equal to the number of times thatparse occurred normalized with respect to N. We canestimate a probability as accurately as we want bychoosing Nas large as we want, since according to theStrong Law of Large Numbers the estimatedprobability converges to the actual probability.
From aclassical result of probability theory (Chebyshev'sinequality) it follows that the time complexity ofachieving a maximum error e is given by O(e'2).
Thusthe error of probability estimation can be madearbitrarily small in polynomial time - provided thatthe parsing algorithm is not worse than polynomial.Obviously, probable parses of an input string are morelikely to be generated than improbable ones.
Thus, inorder to estimate the maximum probability parse, itsuffices to sample until stability in the top of theparse distribution occurs.
The parse which is generatedmost often is then the maximum probability parse.We now show that the probability that a certain parseis generated by Monte Carlo, is exactly the probabilityof that parse according to the DOP-model.
First, theprobability that a subtree t e C is sampled at a certainpoint in the parsing process (where a non-terminal Xis to be substituted) is equal to P(  t I root(t) = X ).Secondly, the probability that a certain sequencetl ..... tn of subtrees that constitutes a derivation of aparse T, is sampled, is equal to the product of theconditional probabilities of these subtrees.
Finally, theprobability that any sequence of subtrees thatconstitutes a derivation of a certain parse T, issampled, is equal to the sum of the probabilities thatthese derivations are sampled.
This is the probabilitythat a certain parse T is sampled, which is equivalentto the probability of T according to the DOP-model.We shall call a parser which applies this Monte Carlotechnique, a Monte Carlo parser.
With respect to thetheory of computation, a Monte Carlo parser is aprobabilistic algorithm which belongs to the class ofBounded error Probabilistic Polynomial time (BPP)algorithms.
BPP-problems are characterized by thefollowing: it may take exponential time to solve themexactly, but there exists an estimation algorithm witha probability of error that becomes arbitrarily small inpolynomial time.Experiments on the ATIS corpusFor our experiments we used part-of-speech sequencesof spoken-language transcriptions from the Air TravelInformation System (ATIS) corpus (Hemphill et al,1990), with the labeled-bracketings of those sequencesin the Penn Treebank (Marcus, 1991).
The 75041labeled-bracketings were divided at random into aDOP-corpus of 675 trees and a test set of 75 part-of-speech sequences.
The following tree is an examplefrom the DOP-corpns, where for reasons of readabilitythe lexical items are added to the part-of-speech tags.
( (S (NP *)fVP (VB Show)(NP (PP me))(NP (NP (PDT all))(DT the) (JJ nonstop) (NNS flights)(Pp (PP ON from)(NP (NP Dallas)))(PP (TO to)(NP (NP Denver))))(ADJP (JJ early)(PP (IN in)(NP (DT the)(NN morning)))))) .
)As a measure for pars/n# accuracy we took thepercentage of the test sentences for which themaximum probability parse derived by the MonteCarlo parser (for a sample size N) is identical to theTreebankparse.It is one of the most essential features of the DOPapproach, that arbitrarily large subtrees are taken intoconsideration.
In order to test the usefulness of thisfeature, we performed different experimentsconstraining the depth of the subtrees.
The depth of atree is defmed as the length of its longest path.
Thefollowing table shows the results of sevenexperiments.
The accuracy refers to the parsingaccuracy at sample size N= I00, and is rounded off tothe nearest integer.depth accuracyii~2 87%~3 92%.~4 93%.~ 93%~6 95%~7 95%unbounded 96%Parsing accuracy for the ATIS corpus, ample size N= I00.The table shows that there is a relatively rapid inc~'~asein parsing accuracy when enlarging the maximumdepth of the subUees to 3.
The accuracy keepsincreasing, at a slower ate, when the depth is enlargedfurther.
The highest accuracy is obtained by using allsubtrees from the corpus: 72 out of the 75 sentencesfrom the test set are parsed correctly.In the following figure, parsing accuracy is plottedagainst the sample size Nfor three of our experiments:the experiments where the depth of the subtrees isconstrained to 2 and 3, and the experiment where thedepth is unconswained.
(The maximum depth in theATIS corpus is 13.
)75I I Isample size N100Parsing accuracy for the ATIS corpus, with depth < 2, withdepth < 3 and with unbounded depth.In (Pereira and Schabes, 1992), 90.36% bracketingaccuracy was reported using a stochastic CFG trainedon bracketings from the ATIS corpus.
Though wecannot make a direct c?~parison, our pilot experimentsuggests that our model may have better performancethan a stochastic CFG.
However, there is still an errorrate of 4%.
Although there is no reason to expect100% accuracy in the absence of any semantic orpragmatic analysis, it seems that the accuracy mightbe further improved.
Three limitations of the currentexperiments are worth mentioning,Fn~t, the Treebank annotations are not rich enough.Although the Treebank uses a relatively rich part-of-speech system (48 terminal symbols), there are only15 non-terwinal symbols.
Especially the internalsu~cmre of noun phrases is very poor.
Semanticannotations are completely absent.42Secondly, it could be that subtrees which occur onlyonce in the corpus, give bad estimations of their actualprobabilities.
The question as to whether reestimationtechniques would further improve the accuracy, mustbe considered in future research.Thirdly, it could be that our corpus is not largeenough.
This brings us to the question as to howmuch parsing accuracy depends on the size of thecorpus.
For studying this question, we performedadditional experiments with different corpus sizes.Starting with a corpus of only 50 parse trees(randomly chosen from the initial DOP-corpus of 675trees), we increased its size with intervals of 50.
Asour test set, we took the same 75 p-o-s sequences asused in the previous experiments.
In the next figurethe parsing accuracy, for sample size N = 100, isplotted against the corpus size, using all corpussubtrees.10075.25.000 000 0OO0 OOi~o ~ 3~o & 5~o &corpus izeParsing accuracy for the ATIS corpus, with unboundeddepth.675The figure shows the increase in parsing accuracy.
Fora corpus ize of 450 trees, the accuracy reaches already88%.
After this, the growth decreases, but the accuracyis still growing at corpus size 675.
Thus, we wouldexpect a higher accuracy if the corpus is furtherenlarged.6 Conclusions and Future ResearchWe have presented a language model that uses anannotated corpus as a stochastic grammar.
Werestricted ourselves to substitution as the onlycombination operation between corpus subtrees.
Astatistical parsing theory was developed, where oneparse can be generated by different derivations, andwhere the probability of a parse is computed as thesum of the probabilities of all its derivations.
It wasshown that our model cannot always be described by astochastic CFG.
It turned out that the maximumprobability parse can be estimated as accurately asdesired in polynomial time by using Monte Carlotechniques.
The method has been succesfully tested ona set of part-of-speech sequences derived from theATIS corpus.
It turned out that parsing accuracyimproved if larger subtrees were used.We would like to extend our experiments o largercorpora, like the Wall Street Journal corpus.
Thismight raise computational problems, since the numberof subtrees becomes extremely large.
Furthermore, inorder to tackle the problem of data sparseness, thepossibility of abstracting from corpus data should beincluded, but statistical models of abstractions offeatures and categories are not yet available.AcknowledgementsThe author is very much indebted to Remko Scha formany valuable comments on earlier versions of thispaper.
The author is also grateful to Mitch Marcus forsupplying the ATIS corpus.ReferencesR.
Bod, 1992a.
"A Computational Model ofLanguage Performance: Data Oriented Parsing",Proceedings COLING~92, Nantes.R.
Bod, 1992b.
"Mathematical Properties of the DataOriented Parsing Model", paper presented at the Th/rdMeeting on Mathematics of Language OVIOL3),Austin, Texas.J.M.
Hammersley and D.C. Handscomb, 1964.
MonteCarlo Methods, Chapman and Hall, London.C.T.
Hemphill, J.J. Godfrey and G.R.
Doddington,1990.
"The ATIS spoken language systems pilotcorpus".
DARPA Speech and Natural LanguageWorkshop, Hidden Valley, Morgan Kaufmann.F.
Jelinek, J.D.
Lafferty and R.L.
Mercer, 1990.
BasicMethods of Probabilistic Context Free Grammars,Technical Report IBM RC 16374 (#72684), YorktownHeights.43M.
Marcus, 1991.
"Very Large Annotated Database ofAmerica~ English".
DARPA Speech and NaawalLanguage Workshop, ~ Grove, MorganKaufmarm.F.
Pereira and Y. Schabes, 1992.
"Inside-OutsideReestimation from Partially Bracketed Corlmra',Proceedings ACY., 92, Newark.P.
Resnik, 1992.
"Probabilistic Tree-AdjoiningGrammar as a Framework for Statistical NaturalLanguage Processing", Proceedings COLING92,Nantes.R.
Scha, 1990.
"Language Theory and LanguageTechnology; Competence and Performance" (inDutch), in Q.A.M.
de Kort & G.L.J.
Leordam (eds.
),Computeltoepassingen in de Needanclistiek, Almere:Landelijkc Vereniging van Neerlandici (LVVN-jaatbock).Y.
Schabes, 1992.
"Stochastic Lexicalized Tree-Adjoining Grammars", Proceedings COLING'92,Nantes.44
