Div id ing  and Conquer ing  Long  Sentences  in a T rans la t ionSystemPeter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, Robert L. Mercer,and Surya Mohanty *IBM Thomas J. Watson Research CenterYorktown Heights, NY 10598ABSTRACTThe time required for our translation system to handlea sentence of length I is a rapidly growing function of i.We describe here a method for analyzing a sentence intoa series of pieces that can be translated sequentially.
Weshow that for sentences with ten or fewer words, it is pos-sible to decrease the translation time by 40% with almostno effect on translation accuracy.
We argue that for longersentences, the effect should be more dramatic.IntroductionIn a recent series of papers, Brown et aL intro-duce a new, statistical approach to machine transla-tion based on the mathematical theory of communi-cation through a noisy channel, and apply it to theproblem of translating naturMly occurring French sen-tences into English \[1, 2, 3, 4\].
They develop a proba-bilistic model for the noisy channel and show how toestimate the parameters of their model from a largecollection of pairs of aligned sentences.
By treating asentence in the source language (French) as a garbledversion of the corresponding sentence in the targetlanguage (English), they recast the problem of trans-lating a French sentence into English as one of find-ing that English sentence which is most likely to bepresent at the input to the noisy channel when thegiven French sentence is known to be present at itsoutput.
For a French sentence of any realistic length,the most probable English translation isone of a set of"This work was supported, in part, by DARPA con-tract N00014-91-C-0135, administered by the Office of NavalResearch.English sentences that, although finite, is nonethelessso large as to preclude an exhaustive search.
Brownet aL employ a suboptimal search based on the stackalgorithm used in speech recognition.
Even so, as wesee in Figure 1, the time required for their system totranslate a sentence grows very rapidly with sentencelength.
As a result, they have focussed their attentionon short sentences.10080= 60E40 ~PE 200I I I I I IT?o_ i  -L_oo  @ @~ 0 I t I I I I4 6 8 10 12 14Sentence LengthFigure 1: Search time as a function of sentence length.The designatum of some French words is so spe-cific that they can be reliably translated almost any-where they occur without regard for the context inwhich they appear.
For example, only the most con-trived circumstances could require one to translatethe French techndtium into English as anything buttechnetium.
Alas, this charming class of words is woe-fully small: for the great majority of words, phrases,and even sentences, the more we know of the context267in which they appear, the more confidently and elo-quently we are able to translate them.
But the exam-ple provided by simultaneous translators hows thatat the expense of eloquence it is possible to producesatisfactory translation segment by segment seriatim.In this paper, we describe a method for analyzinglong sentences into smaller units that can be trans-lated sequentially.
Obviously any such analysis risksrupturing some organic whole within the sentence,thereby precipitating an erroneous translation.
Thus,phrases like (potatoes frites I French fries), (pommesde discorde I bones of contention), (potatoes de terre Ipotatoes), and (pommes auvages I crab apples), offerscant hope for subdivision.
Even when the analysisavoids splitting a noun from an associated adjectiveor the opening word of an idiom from its conclusion,we cannot expect that breaking a sentence into pieceswill improve translation.
The gain that we can ex-pect is in the speed of translation.
In general we mustweigh this gain in translation speed against the loss intranslation accuracy when deciding whether to dividea sentence at a particular point.RiftsBrown et al \[1\] define an alignment between anEnglish sentence and its French translation to be a di-agram showing for each word in the English sentencethose words in the French sentence to which it givesrise (see their Figure 3).
The line joining an Englishword to one of its French dependents in such a dia-gram is called a connection.
Given an alignment, wesay that the position between two words in a Frenchsentence is a rift provided none of the connections towords to the left of that position crosses any of theconnections to words to the right and if, further, noneof the words in the English sentence has connectionsto words on both the left and the right of the position.A set of rifts divides the sentence in which it occursinto a series of segments.
These segments may, butneed not, resemble grammatical phrases.If a French sentence contains a rift, it is clear thatwe can construct a translation of the complete sen-tence by concatenating a translation for the words tothe right of the rift with a translation for the wordsto the left of the rift.
Similarly, if a French sentencecontains a number of rifts, then we can piece togethera translation of the cbmptete sentence from transla-tions of the individual segments.
Because of this, weassume that breaking a French sentence at a rift isless likely to cause a translation error than breakingit elsewhere.Let Pr(e, alf  ) be the conditional probability of theEnglish sentence e and the alignment a given theFrench sentence f = f l f2 .
.
.
fM .
For 1 < i < M, letI(i; e, a, f )  be \] if there is a rift between fi and fi+lwhen f is translated as e with alignment a, and zerootherwise.
The probability that f has a rift betweenfi and fi+l is given byp(rli;f) _= ~ I ( i ;e ,a , f )  Pr(e, alf).e ja(1)Notice that p(r\[i,f) depends on f, but not on anytranslation of it, and can therefore be determinedsolely from an analysis of f itself.The DataWe have at our disposal a large collection ofFrench sentences aligned with their English transla-tions \[2, 4\].
From this collection, we have extractedsentences comprising 27,2\]7,234 potential rift loca-tions as data from which to construct a model for es-timating p(r\[i; f).
Of these locations, we determined13,268,639 to be rifts and the remaining 13,948,592not to be rifts.
Thus, if we are asked whether a par-ticular position is or is not a rift, but are given noinformation about the position, then our uncertaintyas to the answer will be 0.9995 bits.
We were sur-prised that this entropy should be so great.In the examples below, which we have chosen fromour aligned data,, the rifts are indicated by carets ap-pearing between some of the words.I.
LaAr6ponseA~t^laAquestion #2^estAouiA.2.
Ce^chiffreAcomprisAla rEmunErationdu temps supplSmentaire^.3.
La^Soci6t5 du cr6dit agricole^fair savoirAce qui suit:The exact positions of the rifts in these sentences de-pends on the English translation with which they arealigned.
For the first sentence above, the HansardEnglish is The answer to part two is yes.
If, instead,it lind been For part two, yes is the answer, then theonly rift in the sentence would have appeared imme-diately before the final punctuation.268The Dec is ion  TreeBrown et al \[3\] describe a method for assigningsense labels to words in French sentences.
Their ideais this.
Given a French word f ,  find a series of yes-no questions about the context in which it occurs sothat knowing the answers to these questions reducesthe entropy of the translation of f .
They assume thatthe sense of f can be determined from an examinationof the French words in the vicinity of f .
They referto these words as informants and limit their search toquestions of the form Is some particular informant ina particular subset of the French vocabulary.
The setof possible answers to these questions can be displayedas a tree, the leaves of which they take to correspondto the senses of .f.We have adapted this technique to construct a de-cision tree for estimating p(r\[i,f).
Changing any ofthe words in f may affect p(r\]i ,f),  but we consideronly its dependence on fi-1 through fi+2, the fourwords closest to the location of the potential rift, andon the parts of speech of these words.
We treat eachof these eight items as a candidate informant.
Foreach of the 27,217,234 training locations, we createda record of the form vl v~ v3 v4 v5 v6 v7 vs b, where vsis the value of the informant at site s and b is 1 or0 according as the location is or is not a rift.
Us-ing 20,000,000 of these records as data, we have con-structed a binary decision tree with a total of 245leaves.Each of the 244 internal nodes of this tree hasassociated with it one of the eight informant sites, asubset of the informant vocabulary for that site, a leftson, and a right son.
For node n, we represent hisinformation by the quadruple (s(n),S(n), l(n), r(n)>.Given any location in a French sentence, we constructvl v2 v3 v4 v5 v6 v7 vs and assign the location to a leafas follows.1.
Set a to the root node.2.
If a is a leaf, then assign the location to a andstop.3.
If v~(~) E 8(a), then set a to l(a), otherwise seta to r(a).4.
Go to step 2.We call this process pouring the data down the tree.We call the series of values that a takes the path ofthe data down the tree.
Each path begins at the rootnode and ends at a leaf node.We used this algorithm to pour our 27,217,234training locations down the tree.
We estimate p(r\[i, f)at a leaf to be the fraction of these training locationsat the leaf for which b -- 1.
In a similar manner, wecan estimate p(r\[i, f) at each of the internal nodes ofthe tree.
We write p?
(n) for the estimate of p(r\[i, f)obtained in this way at node n. The average ntropyof b at the leaves is 0.7669 bits.
Thus, by using the de-cision tree, we can reduce the entropy of b for trainingdata by 0.2326 bits.To warrant our tree against idiosyncrasies in thetraining data, we used an additional 528,509 locationsas data for smoothing the distributions at the leaves.We obtain a smooth estimate, p(n), ofp(r\]i,f) at eachnode as follows.
At the root, we take p(n) to equalpc(n).
At all other nodes, we definep(n) = A(bn)p~(n) + (l - A(b~))p(the parent of n),(2)where bn is one of fifty buckets associated with a nodeaccording to the count of training locations at thenode.
Bucket I is for counts of 0 and l, bucket 50is for counts equal to or greater than 1,000,000, andfor 1 < i < 50, bucket i is for counts greater thanor equal to zl - ox/~7 and less than x~ + ax /~,  withx2 - ax/~72 = 2, x49 + a,~/~ff = 1,000,000, and xl +a,v/~ = x i+ l -a~/~qfor  1 < i < 49. ltere, x2 = 438,and a = 21.Segmenting\[Jet t(l) be the expected time requited by our sys-tem to translate a sequence of I French words.
We canestimate t(1) for small values of 1 by using our systemto translate a number of sentences of length I.
If webreak f into m+l  pieces by splitting it between fh andfh+l ,  between fie and f,:2+1, and so on, finishing witha split between fire and fi , ,+l, 1 _< il < i2 < ""  <im< M, then the expected time to translate all of thepieces is t( il )+t( i2-i l  )+.
.
"+ t( im- i , , - l )+t(  M- im ).Translation accuracy will be largely unaffected ex-actly when each split falls on a rift.
Assuming thatrifts occur independently of one another, the proba-bility of this event is I-I~=lp(r\[ik,f).
We define theutility, S~(i,f),  of a split i = (il, i2,... ,ira) for f by~nS~(i,f) = o~logp(r \ [ ik , f ) - -k=l269(1 - a)(t( i l )  + t(i2 - i l) +" "+t ( im- im-1)+t (M- im) ) .Here, cr is a parameter weighing accuracy againsttranslation time: when c~ is near 1, we favor accuracy(and, hence,, few segments) at the expense of transla-tion time; when oz is near zero, we favor translationtime (and, hence, many segments) at the expense ofaccuracy.Given a French sentence f and the decision treementioned above for approximating p(r l i , f ) ,  it isstraightforward using dynamic programming to findthe split that maximizes Sa.If we approximate t(l) to be zero for l less thansome threshold and infinite for l equal to or greaterthan that threshold, then we can discard o~.
Our util-ity becomes implymSO,f)  = ~ logp( r l i k , f )k=lprovided all of the segments are less than the thresh-old.
If the length of any segment is equal to or greaterthan the threshold, then the utility is -exp.DecodingIn the absence of segmentation, we employ ananMysis-transfer-synthesis paradigm in our decoder asdescribed in detail by Brown et al \[5\].
We have in-sinuated the segmenter into the system between theanalysis and the transfer phases o four  processing.The analysis operation, therefore, is unaffected by thepresence of the segmenter.
We have also modified thetransfer portion of the decoder so as to investigateonly those translations that are consistent with thesegmented input, but have otherwise left it alone.
Asa result, we get the benefit of the English languagemodel across segment boundaries, but save time bynot considering the great number of translations thatare not consistent with the segmented input.ResultsTo test the usefulness of segmenting, we decoded400 short sentences four different ways.
We compiledthe results in Table l, where: Tree is a shorthand forsegmentation using the tree described above with athreshold of 7; Every 5 is a shorthand for segmentsmade regularly after every five words; Every 4 is ashorthand for segments made regularly after everyfour words; and None is a shorthand for using no seg-mentation at all.
We see from the first line of the ta-ble that the decoder performed somewhat better withsegmentation as determined by the decision tree.
Ifwe carried out an exhaustive search, this could nothappen, but because our search is suboptimal it ispossible for the various shortcuts that we have takento interact in such a way as to make the result betterwith segmentation than without.
The result with thedecision tree is clearly superior to the results obtainedwith either of the rigid segmentation schemes.In Table 2, we show the decoding time in min-utes for the four decoders.
Using the segmentationtree, the decoder is about 41% faster than withoutit.
We use a trigram language model to provide the apriori probability for English sentences.
This meansthat the translation of one segment may depend onthe result of the immediately preceding segment, butshould not be much affected by the translation of anyearlier segment provided that segments average morethan two Words in length.
Because of this, we expecttranslation time with the segmenter to grow approxi-mately linearly with sentence length, while translationtime without the segmenter grows much more rapidly.Therefore, we anticipate that the benefit of segment-ing to decoding speed will be greater for longer sen-tences.Tree vs. NoneEvery 5 vs. NoneEvery 4 vs. NoneTree vs. Every 5Tree vs. Every 4Better Worse Equal8 12 38016 55 32911 61 32854 18 32859 11 330Table 1.
Comparison of segmentation schemesMethodNoneEvery 5TreeEvery 4Translation time(in minutes)8716462851244414Average SegmentLength9.354.123.673.44Table 2.
Translation times and segment lengthsREFERENCES\[1\] P. F. Brown, J. Cocke, S. A. DellaPietra, V. J.DellaPietra, F. Jelinek, J. D. Lafferty, R. L. Met-270cer, and P. S. Roossin, "A statistical approach tomachine translation," Computational Linguistics,vol.
16, pp.
79-85, June 1990.
[2] P. F. Brown, J. C. Lai, and R. L. Mercer, "Align-ing sentences in parallel corpora.," in Proceedings29th Annual Meeting o/the Association/or Com-putational Linguistics, (Berkeley, CA), pp.
169-176, June 1991.
[3] P. F. Brown, S. A. DellaPietra, V. J. DellaPietra,and R. L. Mercer, "Word sense disambiguation us-ing statisticM methods," in Proceedings 29th An-nual Meeting of the Association/or ComputationalLinguistics, (Berkeley, CA), pp.
265-270, June1991.
[4] P. F. Brown, S. A. DellaPietra, V. J. DellaPietra,and R. L. Mercer, "The mathematics of machinetramsla~tion: Parameter estimation."
Submitted toComputational Linguistics, 1991.
[5] P. F. Brown, S. A. DellaPietra, V. J. DellaPietra,J.
Lafferty, and R. L. Mercer, "Analysis, statis-tical transfer, and synthesis in machine transla-tion."
Submitted to TMI-92, Fourth InternationalConference on Theoretical and Methodological Is-sues in Machine Translation, 1992.271
