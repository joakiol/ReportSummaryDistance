Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 846?856,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsAnalyzing Methods for Improving Precision of Pivot Based BilingualDictionariesXabier Saralegi, Iker Manterola, In?aki San VicenteR&D Elhuyar FoundationZelai haundi 3, Osinalde Industrialdea20170 Usurbil, Basque Country{x.saralegi, i.manterola, i.sanvicente}@elhuyar.comAbstractAn A-C bilingual dictionary can be inferredby merging A-B and B-C dictionaries using Bas pivot.
However, polysemous pivot wordsoften produce wrong translation candidates.This paper analyzes two methods for pruningwrong candidates: one based on exploitingthe structure of the source dictionaries, andthe other based on distributional similaritycomputed from comparable corpora.
Asboth methods depend exclusively on easilyavailable resources, they are well suitedto less resourced languages.
We studiedwhether these two techniques complementeach other given that they are based ondifferent paradigms.
We also researchedcombining them by looking for the bestadequacy depending on various applicationscenarios.1 IntroductionNobody doubts the usefulness and multipleapplications of bilingual dictionaries: as the finalproduct in lexicography, translation, languagelearning, etc.
or as a basic resource in several fieldssuch as Natural Language Processing (NLP) orInformation Retrieval (IR), too.
Unfortunately, onlymajor languages have many bilingual dictionaries.Furthermore, construction by hand is a very tediousjob.
Therefore, less resourced languages (as well asless-common language pairs) could benefit from amethod to reduce the costs of constructing bilingualdictionaries.
With the growth of the web, resourceslike Wikipedia seem to be a good option to extractnew bilingual lexicon (Erdmann et al, 2008), butthe reality is that a dictionary is quite different froman encyclopedia.
Wiktionary1 is a promising assetmore oriented towards lexicography.
However, thepresence of less resourced languages in these kindsof resources is still relative -in Wikipedia, too-.Another way to create bilingual dictionaries isby using the most widespread languages (e.g.,English, Spanish, French...) as a bridge betweenless resourced languages, since most languageshave some bilingual dictionary to/from a majorlanguage.
These pivot techniques allow newbilingual dictionaries to be built automatically.However, as the next section will show, it is nosmall task because translation between words isnot a transitive relation at all.
The presence ofpolysemous or ambiguous words in any of thedictionaries involved may produce wrong translationpairs.
Several techniques have been proposedto deal with these ambiguity cases (Tanaka andUmemura, 1994; Shirai and Yamamoto, 2001; Bondet al, 2001; Paik et al, 2004; Kaji et al, 2008;Shezaf and Rappoport, 2010).
However, eachtechnique has different performance and propertiesproducing dictionaries of certain characteristics,such as different levels of coverage of entries and/ortranslations.
The importance of these characteristicsdepends on the context of use of the dictionary.For example, a small dictionary containing themost basic vocabulary and the corresponding mostfrequent translations can be adequate for someIR and NLP tasks, tourism, or initial stages oflanguage learning.
Alternatively, a dictionary whichmaximizes the vocabulary coverage is more orientedtowards advanced users or translation services.This paper addresses the problem of pruning1http://www.wiktionary.org/846wrong translations when building bilingualdictionaries by means of pivot techniques.
Weaimed to come up with a method suitable forless resourced languages.
We analyzed two ofthe approaches proposed in the literature whichare not very demanding on resources: InverseConsultation (IC) (Tanaka and Umemura, 1994) andDistributional Similarity (DS) (Kaji et al, 2008),their strong points and weaknesses, and proposedthat these two paradigms be combined.
For thispurpose, we studied the effect the attributes ofthe source dictionaries have on the performanceof IC and DS-based methods, as well as thecharacteristics of the dictionaries produced.
Thiscould allow us to predict the performance of eachmethod just by looking at the characteristics of thesource dictionaries.
Finally, we tried to providethe best combination adapted to various applicationscenarios which can be extrapolated to otherlanguages.The basis of the pivot technique is dealt with inthe next section, and the state of the art in pivottechniques is reviewed in the third section.
Afterthat, the analysis of the aforementioned approachesand experiments carried out for that purpose arepresented, and a proposal for combining bothparadigms is included.
The paper ends by drawingsome conclusions from the results.2 Pivot TechniqueThe basic pivot-oriented construction method isbased on assuming the transitive relation of thetranslation of a word between two languages.
Thus:if p (pivot word) is a translation of s (sourceword) in the A-B dictionary and t (target word)is a translation of p in the B-C dictionary, wecan say that t is therefore a translation of s, ortranslationA,B(s) = p and translationB,C(p) =t?
translationA,C(s) = tThis simplification is incorrect because it doesnot take into account word senses.
Translationscorrespond to certain senses of the source words.
Ifwe look at figure 1, t (case of t1 and t2) can be thetranslation of p (p2) for a sense c (c3) different fromthe sense for which p (p2) is the equivalent of s (c1).This can happen when p pivot word is polysemous.It could be thought that these causalities areFigure 1: Ambiguity problem of the pivot technique.not frequent, and that the performance of thisbasic approach could be acceptable.
Let usanalyze a real case.
We merged a Basque-Englishdictionary composed of 17,672 entries and 43,021pairs with an English-Spanish one composed of16,326 entries and 38,128 pairs, and obtaineda noised Basque-Spanish dictionary comprising14,000 entries and 104,165 pairs.
10,000 (99,844pairs) among all the entries have more than onetranslation.
An automatic evaluation shows that80.32% of these ambiguous entries contain incorrecttranslation equivalents (80,200 pairs out of 99,844).These results show that a basic pivot-orientedmethod is very sensitive to the ambiguity level ofthe source dictionaries.
The conclusion is that thetransitive relation between words across languagescan not be assumed, because of the large number ofambiguous entries that dictionaries actually have.
Amore precise statement for the transitive property inthe translation process would be:if p (pivot word) is a translation of s with respectto a sense c and t is a translation of p withrespect to the same sense c we can say that t isa translation of s, or translationA,B(sc1) = pand translationB,C(pc2) = t and c1 = c2 ?translationA,C(s) = tUnfortunately, most dictionaries lack comparableinformation about senses in their entries.
So it is notpossible to map entries and translation equivalentsaccording to their corresponding senses.
As analternative, most papers try to guide this mappingaccording to semantic distances extracted from thedictionaries themselves or from external resources847such as corpora.Another problem inherent in pivot-basedtechniques consists of missing translations.
Thisconsists of pairs of equivalents not identified in thepivot process because there is no pivot word, or elseone of the equivalents is not present.
We will not bedealing with this issue in this work so that we canfocus on the translation ambiguity problem.3 State of the ArtIn order to reject wrong translation pairs, Tanakaet al (1994) worked with the structure of thesource dictionaries and introduced the IC methodwhich measures the semantic distance between twowords according to the number of pivot-wordsthey share.
This method was extended by usingadditional information from dictionaries, such assemantic classes and POS information in (Bond etal., 2001; Bond and Ogura, 2007).
Sjo?bergh (2005)compared full definitions in order to detect wordscorresponding to the same sense.
However, not allthe dictionaries provide this kind of information.Therefore, external knowledge needs to be usedin order to guide mapping according to sense.Istva?n et al (2009) proposed using WordNet, onlyfor the pivot language (for English in their case),to take advantage of all the semantic informationthat WordNet can provide.
Mausam et.
al.
(2009) researched the use of multiple languages aspivots, on the hypothesis that the more languagesused, the more evidences will be found to findtranslation equivalents.
They used Wiktionary forbuilding a multilingual lexicon.
Tsunakawa et al(2008) used parallel corpora to estimate translationprobabilities between possible translation pairs.Those reaching a minimum threshold are acceptedas correct translations to be included in the targetdictionary.
However, even if this strategy achievesthe best results in the terminology extraction field,it is not adequate when less resourced languages areinvolved because parallel corpora are very scarce.As an alternative, (Kaji et al, 2008; Gamalloand Pichel, 2010) proposed methods to eliminatespurious translations using cross-lingual context ordistributional similarity calculated from comparablecorpora.
In this line of work, (Shezaf andRappoport, 2010) propose a variant of DS, and showhow it outperforms the IC method.
In comparison,our work focuses on analyzing the strong and weakpoints of each technique and aims to combine thebenefits of each of them.Other characteristics of the merged dictionarieslike directionality (Paik et al, 2004) also influencethe results.4 Experimental SetupThis work focuses on adequate approaches for lessresourced languages.
Thus, the assumption for theexperimentation is that few resources are availablefor both source and target languages.
The resourcesfor building the new dictionary are two basic (nodefinitions, no senses) bilingual dictionaries (A-B,B-C) including source (A), target (C) and a pivotlanguage (B), as well as a comparable corpus forthe source-target (A-C) language pair.
We exploredthe IC (Tanaka and Umemura, 1994) and DS (Kajiet al, 2008; Gamallo and Pichel, 2010) approaches.In our experiments, the source and target languagesare Basque and Spanish, respectively, and English isused for pivot purposes.
In any case, the experimentscould be conducted with any other language set, solong the required resources are available.It must be noted that the proposed task is nota real problem because there is a Basque-Spanishdictionary already available.
Resources likeparallel corpora for that language pair are alsoavailable.
These dictionaries and pivot languagewere selected in order to be able to evaluate theresults automatically.
During the evaluation wealso used frequency information extracted from aparallel corpus, but then again, this corpus was notused during the dictionary building process, andtherefore, it would not be used in a real applicationenvironment.4.1 ResourcesIn order to carry out the experiments we used threedictionaries.
The two dictionaries mentioned inthe previous section (Basque-English Deu?en andEnglish-Spanish Den?es) were used to produce anew Basque-Spanish Deu?en?es dictionary.
Inaddition, we used a Basque-Spanish Deu?esdictionary for evaluation purposes.
Its broadcoverage is indicative of its suitability as a reference848dictionary.
Table 1 shows the main characteristics ofthe dictionaries.
We can observe that the ambiguitylevel of the entries (average number of translationsper source word) is significant.
This produces morenoise in the pivot process, but it also benefits IC dueto the increase in pivot words.
As for the directionsof source dictionaries, English is taken as target.Like Paik et al (2004) we obtained the best coverageof pairs in that way.Dictionary #entries #pairs ambiguitylevelDeu?en 17,672 43,021 2.43Den?es 16,326 38,128 2.33Deu?es(reference) 57,334 138,579 2.42Deu?en?es(noisy) 14,601 104,172 7.13Table 1: Characteristics of the dictionaries.Since we were aiming to merge two generaldictionaries, the most adequate strategy was to useopen domain corpora to compute DS.
The domainof journalism is considered to be close to the opendomain, and so we constructed a Basque-Spanishcomparable corpus composed of news articles (seeTable 2).
The articles were gathered from thenewspaper Diario Vasco (Hereinafter DV) for theSpanish part and from the Berria newspaper for theBasque part.
Both publications focus on the BasqueCountry.
In order to achieve a higher comparabilitydegree, some constraints were applied:?
News in both languages corresponded to thesame time span, 2006-2010.?
News corresponding to unrelated categoriesbetween newspapers were discarded.Corpus #words #docsBerria(eu) 40Mw 149,892DV(es) 77Mw 306,924Table 2: Characteristics of the comparable corpora.In addition, as mentioned above, we extractedthe frequencies of translation pairs from aBasque-Spanish parallel corpus.
The corpushad 295,026 bilingual segments (4 Mw in Basqueand 4.7 Mw in Spanish) from the domain ofjournalism.5 Pruning MethodsIC and DS a priori suffer different weak points.
ICdepends on the structure of the source dictionaries.On the other hand, DS depends on a goodcomparable corpus and translation process.
DS ismeasured more precisely between frequent wordsbecause context representation is richer.The conditions for good performance of both ICand DS are analyzed below.
These conditions willthen be linked to the required characteristics for theinitial dictionaries.
In addition, we will measurehow divergent the entries solved for each methodare.5.1 Inverse consultationIC uses the structure of the Da?b and Db?csource dictionaries to measure the similarity ofthe meanings between source word and translationcandidate.
The description provided by Tanaka etal.
(1994) is summarized as follows.
To findsuitable equivalents for a given entry, all targetlanguage translations of each pivot translation arelooked up (e.g., Db?c(Da?b(s))).
This way, allthe ?equivalence candidates?
(ECs) are obtained.Then, each one is looked up in the inverse direction(following the previous example, Dc?b(t)) to createa set of words called ?selection area?
(SA).
Thenumber of common elements of the same languagebetween SA and the translations or equivalences (E)obtained in the original direction (Da?b(s)) is usedto measure the semantic distance between entriesand corresponding translations.
The more matchesthere are, the better the candidate is.
If only oneinverse dictionary is consulted, the method is called?one time inverse consultation?
or IC1.
If n inversedictionaries are consulted, the method is called ?ntime inverse consultation?.
As there is no significantdifference in performance, we simply implementedIC1.
Assuming that each element (x) of these twosets (SA,E) has a weight that is determined by thenumber of times it appears in the set that belongs(X), this weight is denoted as ?(X,x).
In the sameway, the number of common elements between SAand E is denoted as follows:?
(E,SA) =?x?SA?
(E, x) (1)849IC asks for more than one pivot word betweensource word s and translation candidate t. In ourexample:?
(Da?b(s), Dc?b(t)) > 1 (2)In general, this condition guarantees that pivotwords belong to the same sense of the source word(e.g.
iturri?tap?grifo, iturri?faucet?grifo).Consequently, source word and target word alsobelong to the same sense.Conceptually, the IC method is based on theconfluence of two evidences.
Let us takeour dictionaries as examples.
If two or morepivot words share a translation t in the Des?endictionary (|tr(tc, Des?en| > 1) (e.g.
grifo?tap,grifo?faucet) we could hypothesize that theyare lexical variants belonging to a unique sensec.
If an entry s includes those translations(|tr(sc, Deu?en)| > 1) (e.g.
iturri?tap,iturri?faucet)) in the Deu?en dictionary, we couldalso hypothesize the same.
We can conclude thatentry s and candidate t are mutual translationsbecause the hypothesis that ?faucet?
and ?tap?
arelexical variants of the same sense c is contrastedagainst two evidences.
This makes IC highlydependant on the number of lexical variants.Specifically, IC needs several lexical variants inthe pivot language per each entry sense in bothdictionaries.
Assuming that wrong pairs cannotfulfill this requirement (see Formula 2) we canestimate the probabilities of the conditions forsolving an ambiguous pair (s, t) where s and t ?
c,as follows:(a) p(|tr(sc, Da?b)| > 1): Estimated bycomputing the average coverage of lexicalvariants in the pivot language for each entry inDa?b.
(b) p(|tr(tc, Dc?b)| > 1): Estimated bycomputing the average coverage of lexicalvariants in the pivot language for each entry inDc?b.
(c) p(|tr(sc, Da?b)?
tr(tc, Dc?b)| > 1):Convergence degree between translations of sand t in Da?b and Dc?b corresponding to c.So, in order to obtain a good performance with IC,the dictionaries used need to provide a high coverageof lexical variants per sense in the pivot language.If we assume that variants of a sense do not varyconsiderably between dictionaries, performance ofIC in terms of recall would be estimated as follows:R = p(|tr(sc, Da?b)| > 1) ?
p(|tr(tc, Dc?b)| > 1)(3)We estimated the adequacy of the differentdictionaries in the experimental setup accordingto estimations (a) and (b).
Average coverage oflexical variants in the pivot language was calculatedfor both dictionaries.
It was possible becauselexical variants in the target language were groupedaccording to senses in both dictionaries.
Onlyambiguous entries were analyzed because they arethe set of entries which IC must solve.
In theDeu?en dictionary more than 75% of senses havemore than one lexical variant in the pivot language.So, p(|tr(sc, Deu?en)| > 1) = 0.75.
InDes?en this percentage (23%) is much lower.
So,p(|tr(tc, Des?en)| > 1) = 0.23.
Therefore,Deu?en dictionary is more suited to the IC methodthan Des?en.
As the conditions must be met inthe maximum of both dictionaries, performanceaccording to Formula 3 would be: 0.75 ?
0.23 =0.17.
This means that IC alone could solve about17% of ambiguous entries.5.2 Distributional SimilarityDS has been used successfully for extractingbilingual terminology from comparable corpora.The underlying idea is to identify as translationequivalents those words which show similardistributions or contexts across two corpora ofdifferent languages, assuming that this similarityis proportional to the semantic distance.
Inother words, establishing an equivalence betweencross lingual semantic distance and translationprobability.
This technique can be used for pruningwrong translations produced in a pivot-baseddictionary building process (Kaji et al, 2008;Gamallo and Pichel, 2010).We used the traditional approach to computeDS (Fung, 1995; Rapp, 1999).
Following the?bag-of-words?
paradigm, the contexts of a word w850are represented by weighted collections of words.Those words are delimited by a window (?5words around w) and punctuation marks.
Thecontext words are weighted with regard to waccording to the Log-likelihood ratio measure, andthe context vector ofw is formed.
After representingword contexts in both languages, the algorithmcomputes for each source word the similaritybetween its context vector and all the context vectorscorresponding to words in the target language bymeans of the cosine measure.
To be able tocompute the cross-lingual similarity, the contextvectors are put in the same space by translatingthe vectors of the source words into the targetlanguage.
This is done by using a seed bilingualdictionary.
The problem is that we do not have thatbilingual dictionary, since that is precisely the onewe are trying to build.
We propose that dictionariesextracted from our noisy dictionary (Deu?en?es) beused:?
Including the unambiguous entries only?
Including unambiguous entries and selectingthe most frequent candidates according to thetarget language corpus for ambiguous entries?
The dictionary produced by the IC1 methodThe second method performed better in the testswe carried out.
So, that is the method implementedfor the experiments in the next section.DS calls for several conditions in order to performwell.
For solving an ambiguous translation tof a source word s, both context representationsmust be accurate.
The higher their frequency inthe comparable corpus, the richer their contextrepresentation will be.
In addition to contextrepresentation, the translation quality of contexts isalso a critical factor for the performance of DS.Factors can be formulated as follows if we assumebig and highly comparable corpora:(a) Precision of context representation: this can beestimated by computing the frequency of thewords(b) Precision of translation process: this can beestimated by computing the quality of the seeddictionary6 ResultsIn order to evaluate the performance of each pruningmethod, the quality of the translations was measuredaccording to the average precision and recall oftranslations per entry with respect to the referencedictionary.
As we were not interested in dealing withmissing translations, the reference for calculatingrecall was drawn up with respect to the intersectionbetween the merged dictionary (Deu?en?es) andthe reference dictionary (Deu?es).
F-score is themetric that combines both precision and recall.We also introduced the frequency of use of bothentry and pair as an aspect to take into account in theanalysis of the results.
It is better to deal effectivelywith frequent words and frequent translations thanrare ones.
Frequency of use of Basque wordsand frequency of source-target translation equivalentpairs were extracted respectively from the opendomain monolingual corpus and the parallel corpusdescribed in the previous section.
Corpora werelemmatized and POS tagged in both cases in orderto extract the frequency information of the lemmas.Figure 2: Precision results according to the minimumfrequency of entries.6.1 Inverse ConsultationResults show that IC precision is about 0.6 (SeeFigure 2).
This means that many wrong pairsfulfill IC conditions.
After analyzing the wrongpairs by hand, we observed that some of themcorresponded to correct pairs not included in thereference dictionary.
They are not included in851Figure 3: Recall results according to the minimumfrequency of entries.Figure 4: F-score results according to the minimumfrequency of entries.the reference because not all synonyms -or lexicalvariants- are included in it, only the most commonones.
This is an inherent problem in automaticevaluation, and affects all the experiments presentedthroughout section 6 equally.
Other wrong pairscomprise translation equivalents which have thesame stem but different gramatical categories (e.g.,?aldakuntza?
(noun) (change, shift) ?
?cambiar?
(verb) (to change, to shift)).
These wrong casescould be filtered if POS information would beavailable in the source dictionaries.Precision is slightly better when dealing withfrequent words, a maximum of 0.62 is reached whenminimum frequency is between 150 and 2,000.Precision starts to decline significantly when dealingFigure 5: Recall results according to the minimumfrequency of translation pairs.with those entries over a minimum frequency of10,000.
However, only very few entries (234) reachthat minimum frequency.Recall is about 0.2 (See Figure 3), close to theestimation computed in section 5.1.
It presents amore marked variability according to the frequencyof entries, improving the performance as thefrecuency increases.
This could be due to the factthat frequent entries tend to have more translationvariants (See Table 3).
The fact that there aretoo many candidates to solve would explain whythe recall starts to decline when dealing with veryfrequent entries.Global performance according to F-score reflectsthe variability depending on frequency (See Figure4).Recall according to frequency of pairs providesinformation about whether IC selects raretranslations or the most probable ones (SeeFigure 5).
It must be noted that this recall iscalculated with respect to the translation pairs ofthe merged dictionary Deu?en?es which appearin the parallel corpus (see section 4.1).
Results(See Figure 5) show that IC deals much betterwith frequent translation pairs.
However, recallfor pairs whose frequency is higher than 100 onlyreaches 0.5.
Even if the maximum recall is achievedfor pairs whose frequency is above 40,000, it isnot significant because they suppose a minimumnumber (3 pairs).
In short, we can conclude that ICoften does not find the most probable translation852(e.g.
?usain???olor?
(smell), ?zulo???agujero?(hole),...
).6.2 Distributional SimilarityDS provides an idea of semantic distance.
However,in order to determine whether a candidate is acorrect translation, a minimum threshold mustbe established.
It is very difficult to establisha threshold manually because its performancedepends on the characteristics of the corpora and theseed dictionaries.
The threshold can be applied at aglobal level, by establishing a numeric threshold forall candidates, or at local level by selecting certaintop ranked candidates for each entry.
The dictionarycreated by IC or unambiguous pairs can be usedas a reference for tuning the threshold in a robustway with respect to the evaluation score such asF-score.
In our experiments, thresholds estimatedagainst the dictionary created by IC are very close tothose calculated with respect to the whole referencedictionary (see Figure 6).Figure 6: Threshold parameter tuning comparison fordifferent Fn scores.
Tuning against dictionary created byIC vs. Reference dictionary.There is not much variation in performancebetween local and global thresholds.
Precisionincreases from 0.4 to 0.5 depending on the strictnesslevel of the threshold (See Figure 2), the stricterthe better.
In all cases, precision is slightly betterwhen dealing with frequent words (frequency >20).
This improvement is more marked withthe strictest thresholds (TOP1, 0.1).
However,if global thresholds are used, performance startsto decline significantly when dealing with wordswhose frequency is above 1,000.
So, it seems thatlocal thresholds (TOP3) perform more consistendlywith respect to the high frequencies of entries.Recall (See Figure 3) goes from 0.5 to 0.7depending on the strictness level of the threshold.It starts declining when frequency is above 50depending on the type of threshold.
In this case,global thresholds seem to perform better becausethe most frequent entries are handled better.
Theseentries tend to have many translations.
Thereforethresholds based on top ranks are too rigid.There is no significant difference between globaland local thresholds in terms of F-Score (See Figure4).
Each threshold type is more stable in precision orrecall.
So the F-Score is similar for both.
Variabilityof F-Score according to frequency is lower thanin precision and recall.
As performance peaks onboth measures at different points of frequency, thevariability is mitigated when measures are combinedby F-Score.We have plotted the recall according to thefrequency of pairs calculated from a parallel corpusin order to analyze the performance of DS whendealing with frequent translation pairs (See Figure5).
The performance decreases when dealing withpairs whose frequency is higher than 100.
Thismeans that DSs performance is worse when dealingwith the most common translation pairs.
So it isclear that it is very difficult to represent the contextsof very frequent words correctly.The results show that DS rankings are worsewhen dealing with some words above a certainfrequency threshold (e.g.
?on?
?good?, ?berriz?
?again?, ?buru?
?head?, ?orain?
?now?...).
Althoughcontext representation of frequent words is basedon many evidences, high polysemy level relatedto high frequency leads to a poorer representation.Alternatively we found that some of those frequentwords are not very polysemous.
Those wordsdo not have strong collocates, that is, they tendto appear freely in contexts, which also leads topoor representation.
This low quality representationhampers an accurate computation of semanticdistance.6.3 Comparison between IC and DSAs for average precision, IC provides better resultsthan DS if all entries are taken into account.However, DS tips the scales in its favor if onlyentries with frequencies above 50 are considered andstrict thresholds are used (TOP1, 0.1).DS clearly outperforms IC in terms of averagerecall of translations.
Even if strict thresholdsare used, DS outperforms IC for all entries whose853frequency is lower than 640.If average precision and recall are evaluatedtogether by means of F-score, DS outperforms IC(Figure 4).
Only when dealing with very frequententries (frequency > 8, 000) is ICs performanceclose to DSs, but these entries make up a very smallgroup (234 entries).In order to compare the recall with respect tothe frequency of translation pairs under the sameconditions, we have to select a threshold thatprovides a similar precision to IC.
TOP1 is themost similar one (see figure 2).
As Figure 5shows, again DS is better than IC.
Even if IC?srecall clearly surpasses DS?s when dealing withfrequent translation pairs (frequency > 2, 560), itonly represents a minimal number of pairs (39).6.4 Combining IC and DS according todifferent scenariosIn order to see how the methods can complementeach other, we calculated the performancefor solving ambiguous entries obtained bycombining the results of both methods usingvarious alternatives:?
Union: IC ?
DS: Pairs obtained by bothmethods are merged.
Duplicated pairs arecleaned.?
Lineal combination (Lcomb): IC ?
k +DS ?
(1 ?
k).
Each method provides avalue representing the translation score.
ForIC that value is the number of pivot words(see Formula 1), and the context similarityscore in the case of DS.
Those values arelinearly combined and applied over the noiseddictionary.As mentioned in the first section, one of the goalsof the paper was to analyze which method andwhich combination was best depending on the usecase.
We have selected some measures which are agood indicator of good performance for different usecases:?
AvgF : Average F-score per entry.?
wAvgF : Average F-score per entry weightedby the frequency of the entry.
Higher frequencyincreases the weight.?
AvgF2: Average F-score per entry where recallis weighted higher.?
AvgF0.5: Average F-score per entry whereprecision is weighted higher.For the use cases presented in section 1, somemeasures will provide richer information thanothers.
On the one hand, if we aim to build small,accurate dictionaries, AvgF0.5 would be a betterindicator since it attaches more importance to highprecision.
In addition, if we want the dictionariesto cover the most common entries (e.g., in a basicdictionary for language learners) it is also interestingto look at wAvgF values because greater value isgiven to finding translations for the most frequentwords.
On the other hand, if our objective is tobuild big dictionaries with a high recall, it wouldbe better to look at AvgF2 measure which attachesimportance to recall.Method AvgF wAvgF AvgF2 AvgF0.5IC 0.34 0.27 0.27 0.46DS 0.47 0.44 0.64 0.46Union 0.52 0.49 0.65 0.49Lcomb 0.52 0.49 0.67 0.52Table 3: Performance results of methods for ambiguousentries according to different measures.Table 3 shows the results for the differentcombinations.
The parameters of all methodsare optimized for each metric (as explained insection 6.2, see figure 6).
In all cases, thecombinations surpass the results of both methodsseparately.
There is a reasonable improvement overDS (10.6% for AvgF ), and an even more startlingone over IC (52.9% for AvgF ).
IC only getsanywhere near the other methods when precisionis given priority (AvgF0.5).
There is no significantdifference in terms of performance between the twocombinations, although Lcomb is slightly better.wAvgF measure is stricter than the others sinceit takes frequency of entries into account.
This isemphasised more in the case of IC where resultsdecrease notably compared with AvgF .8547 ConclusionsThis paper has analyzed IC and DS, for thetask of pruning wrong translations from bilingualdictionaries built by means of pivot techniques.After analyzing their strong and weak points wehave showed that IC requires high ambiguity leveldictionaries with several lexical variants per entrysense.
With an average ambiguity close to 2translation candidates DS obtains better results.
ICis a high precision method, but contrary to ourexpectations, it seems that it is not much moreprecise than DS.
In addition, DS offers much betterrecall of translations and entries.
As a result, DSperforms the best if both precision and recall aretaken into account by F-score.Both methods prune most probable translationsfor a significant number of frequent entries.
DSencounters a problem when dealing with veryfrequent words due to the difficulty in representingtheir context.
The main reason behind this is thehigh polysemy level of those words.Our initial beliefs were that the translationsfound by each method would diverge to a certainextent.
The results obtained when combining thetwo methods show that although the performancedoes not increase as much as expected (10.6%improvement over DS), there is in fact somedivergence.
As for the different use cases proposed,combinations offer the best performance in all cases.IC is indeed the poorer method, although it presentscompetitive results when precision is given priority.Future experiments include contrasting theseresults with other dictionaries and language pairs.8 AknowledgmentsThis work has been partially founded by the IndustryDepartment of the Basque Government under grantsIE09-262 (Berbatek project) and SA-2010/00245(Pibolex+ project).ReferencesFrancis Bond and Kentaro Ogura.
2007.
Combininglinguistic resources to create a machine-tractableJapanese-Malay dictionary.
Language Resources andEvaluation, 42(2):127?136.Francis Bond, Ruhaida Binti Sulong, TakefumiYamazaki, and Kentaro Ogura.
2001.
Design andconstruction of a machine-tractable Japanese-Malaydictionary.
Proceedings of ASIALEX, SEOUL,2001(2001):200?205.Maike Erdmann, Kotaro Nakayama, Takahiro Hara, andShojiro Nishio.
2008.
An approach for extractingbilingual terminology from wikipedia.
In Proceedingsof the 13th international conference on Databasesystems for advanced applications, DASFAA?08,pages 380?392, Berlin, Heidelberg.
Springer-Verlag.ACM ID: 1802552.Pascale Fung.
1995.
Compiling bilingual lexicon entriesfrom a non-parallel English-Chinese corpus.
In DavidYarovsky and Kenneth Church, editors, Proceedingsof the Third Workshop on Very Large Corpora,pages 173?183, Somerset, New Jersey.
Association forComputational Linguistics.Pablo Gamallo and Jose?
Pichel.
2010.
Automaticgeneration of bilingual dictionaries using intermediarylanguages and comparable corpora.
In AlexanderGelbukh, editor, Computational Linguistics andIntelligent Text Processing, 11th InternationalConference, CICLing 2010.
Proceedings, volume6008 of Lecture Notes in Computer Science, pages473?483.
Springer.Varga Istva?n and Yokoyama Shoichi.
2009.
Bilingualdictionary generation for low-resourced languagepairs.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Processing:Volume 2 - Volume 2, EMNLP ?09, pages 862?870,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.
ACM ID: 1699625.Hiroyuki Kaji, Shin?ichi Tamamura, andDashtseren Erdenebat.
2008.
Automaticconstruction of a Japanese-Chinese dictionaryvia English.
In Proceedings of the SixthInternational Language Resources and Evaluation(LREC?08), Marrakech, Morocco.
EuropeanLanguage Resources Association (ELRA).http://www.lrec-conf.org/proceedings/lrec2008/.Mausam, Stephen Soderland, Oren Etzioni, Daniel SWeld, Michael Skinner, and Jeff Bilmes.
2009.Compiling a massive, multilingual dictionary viaprobabilistic inference.
In Proceedings of the JointConference of the 47th Annual Meeting of the ACLand the 4th International Joint Conference on NaturalLanguage Processing of the AFNLP: Volume 1 -Volume 1, ACL ?09, page 262270, Stroudsburg,PA, USA.
Association for Computational Linguistics.ACM ID: 1687917.Kyonghee Paik, Satoshi Shirai, and Hiromi Nakaiwa.2004.
Automatic construction of a transfer dictionaryconsidering directionality.
In Proceedings of theWorkshop on Multilingual Linguistic Ressources,MLR ?04, pages 31?38, Stroudsburg, PA, USA.855Association for Computational Linguistics.
ACM ID:1706243.R.
Rapp.
1999.
Automatic identification of wordtranslations from unrelated English and Germancorpora.
In Proceedings of the 37th annual meeting ofthe Association for Computational Linguistics, pages519?526, College Park, USA.
ACL.Daphna Shezaf and Ari Rappoport.
2010.
Bilinguallexicon generation using non-aligned signatures.
InProceedings of the 48th Annual Meeting of theAssociation for Computational Linguistics, ACL ?10,page 98107, Stroudsburg, PA, USA.
Association forComputational Linguistics.
ACM ID: 1858692.S.
Shirai and K. Yamamoto.
2001.
Linking englishwords in two bilingual dictionaries to generate anotherlanguage pair dictionary.
In Proceedings of ICCPOL,pages 174?179.J.
Sjo?bergh.
2005.
Creating a free digitalJapanese-Swedish lexicon.
In Proceedings ofPACLING 2005.Kumiko Tanaka and Kyoji Umemura.
1994.Construction of a bilingual dictionary intermediatedby a third language.
In Proceedings of the16th International Conference on ComputationalLinguistics (COLING?94), pages 297?303.Takashi Tsunakawa, Naoaki Okazaki, and Jun?ichiTsujii.
2008.
Building bilingual lexicons usinglexical translation probabilities via pivot languages.Proceedings of the Sixth International LanguageResources and Evaluation (LREC?08).856
