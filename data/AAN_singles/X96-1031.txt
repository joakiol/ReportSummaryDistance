RECENT ADVANCES IN HNC'S CONTEXT VECTORINFORMATION RETRIEVAL TECHNOLOGYMarc R. Ilgen, David A. RushallHNC Software, Inc., 5930 Cornerstone Court West, San Diego, CA  92121 USAemaih mr i@hne.eom, dar@hnc.eomABSTRACTOver the past few years, HNC has developeda neural network based, vector space approach to textretrieval.
This approach, embodied in a system calledMatchPlus, allows the user to retrieve information onthe basis of meaning and context of a fi'ee text query.The MatchPlus system uses a neural network based,constrained sel~organization technique to learn wordstem interrelationships directly 3~om a trainingcorpus, thereby eliminating the need for hand craftedlinguistic knowledge bases and their often substantialmaintenance requirements.
This paper presentsresults fi'om recent enhancements o the basicMatchPlus concept.
These nhancements include thedevelopment ofa one step learning law that greatlyreduces the amount of time and~or computationalresources required to train the system, and thedevelopment ofa prototype multilingual (English andSpanish) text retrieval system.1.
INTRODUCTIONWhile the current MatchPlus learning law hasproven to be effective in encoding relationshipsbetween words, it is computationally intensive andrequires multiple passes through the training corpus.The purpose of the one step learning law is toapproximate he behavior of the original earning lawwhile performing only a single pass through thetraining corpus.
The one step learning law uses asingle pass through the training corpus to obtaindesired dot product values for the set of trainedcontext vectors.
The desired dot product values aredetermined on the basis of information theoreticstatistical relationships between co-occurring wordstems found in the training corpus.
Desired dotproducts are found such that words that tend to co-occur will have context vectors that point in similardirections while words that do not co-occur will havecontext vectors that tend to be orthogonal.
Thesedesired ot products are used to perform a quasi-lineartransformation  an initial set of quasi-orthogonal,high dimensional vectors.
This vector transformationand subsequent renormalization results in a set ofcontext vectors that represents the relationshipsbetween word stems in a near-optimal fashion.
Thetime requirements for training this set of vectors caleas O(Nn) where N is the number of word stems in thevocabulary and n is the average number of word stemsfound to co-occur (and/or be related to) any givenword stem (usually on the order of several hundred).This new learning law reduces the training time by afactor on the order of 100 over the original contextvector learning law with little or no degradation iperformance.
Results can be improved even further byadjusting the dimension of the context vectors.HNC has also developed an approach tolearning stem-level relationships across multiplelanguages and has used this approach to develop aprototype multilingual retrieval system.
Thistechnique, called "symmetric learning", is based uponthe use of tie words, which provide connectivitybetween each language's portion of the context vectorspace.
In the symmetric approach, learning isconducted using both languages simultaneously, thusremoving any donor language biases.
Tie words areused to connect the context vector space for multiplelanguages through a "unified hash table".
The unifiedhash table provides the mechanism to translate a steminto an associated context vector.
In the English-onlyMatchPlus ystem, this is a straight forward process.The stem in question is fed to the hashing function andthe index is produced.
The resulting index is the offsetin the stem hash table.
The content of that location inthe hash table is a pointer to the context vector datastructure.
Using this approach (hash fimctioncollisions not withstanding), each unique stem resultsin a unique entry and thus a unique context vector.
Inthe multilingual system, a tie word list is used toprovide multiple references, one word stem ffi'om eachlanguage, for common context vectors.
Context vectorlearning is performed in multiple languages149simultaneously using multilingual training corpora.HNC has performed preliminary evaluation of anEnglish-Spanish version of this system by examiningstem trees for tie words and non-tie words.
Resultsindicate that the English-Spanish MatchPlus prototypeis able to learn reasonable word stem interrelationshipsfor tie words and non-tie words, thereby demonstratingthe suitability of this concept for further development.2.
TECHNICAL BACKGROUNDThe HNC MatchPlus system was developedas part of the ARPA-sponsored TIPSTER textprogram.
MatchPlus uses an informationrepresentation scheme called context vectors to encodesimilarity of usage.
Other vector space approaches totext retrieval exist, but none embody the ability tolearn word-level relationships \[1-5\].
Key attributes ofthe context vector approach are as follows: Duringthis effort, the initially proposed context vectorapproach using human defined coordinates and initialconditions was extended and refmed to allow fullyautomatic generation of context vectors for textsymbols (stems) based upon their demonstratedcontext of usage in training text.
The MatchPlussystem learns relationships at the stem level and thenuses those relationships to construct a context vectorrepresentation for sets of symbols.
For the text case,these sets of symbols are paragraphs, documents andqueries.To start the learning process, each stem isassociated with a random vector in the context vectorspace.
Random unit vectors in high dimensionalfloating point spaces have a property that is referred toa "quasi-orthogonality"\[6\].
That is, the expected valueof the dot product between any pair of random contextvectors elected fi'om the set is approximately equal tozero (i.e.
all vectors are approximately perpendicularto one another).
This property of quasi-orthogonalityis important because it serves as the initial conditionfor the context vector learning algorithm.
The usageof the context vector technique is predicated upon therule that symbols (stems) that are used in a similarcontext (exhibit proximate co-occurrence behavior)will have trained vectors that point in similardirections.
Conversely, stems that never appear in asimilar context will have context vectors that areapproximately orthogonal.To achieve the desired representation, thecontext vector learning algorithm must take thecontext vectors for symbols that co-occur and movethem toward each other.
Symbols that do not co-occurare left in their quasi-orthogonal original condition.
Itis a basic tenet of the MatchPlus approach that "wordsthat are used in a similar context convey similarmeaning".
Since the learning is driven by proximateco-occurrence of words, the learning results in a vectorset where closeness in the space is equivalent ocloseness in subject content.
To perform learning, alearning window is used to identify local context.
Thewindow is "slid" through each document in the corpus.The window has 1 target stem and multiple neighborstems.
Once the context window has been determined,the learning rule of "Move context vector for target inthe direction of the context vector of the neighbors" isapplied.
Once the correction is made, we move thelearning window to next location and the learningoperation is repeated.
The equation for this learning isshown in Figure 1.Jwhere:TJ ?~ = Context vector of target i after update~old = Context vector of target i before update7 = Adjustment s ep sizeN# = Context vector for neighborj of target ia~j = Desired context vector dot product fortarget i and neighborjFigure 1.
MatchPlus Learning EquationsSeveral points should be noted:?
All stem vectors are of length 1 (unit vectors).
Inthis paradigm, only the direction of the vectorcarries information.?
Fully trained vectors have the property that wordsthat are used in a similar context will have vectorsthat point in similar directions as measured by thedot product.?
Words that are never used in a similar context willretain their initial condition of quasi-orthogonality.
That is, approximately orthogonalwith a dot product of approximately zero.?
Trained context vectors result in a concept spacewhere similarity of direction corresponds tosimilarity of meaning.?
No human knowledge is required for training tooccur.
Only flee text examples are needed.150* The algorithm determines the coordinate space ofthe context vectors.When the training is complete, "words thatare used in a similar context will have their associatedvectors point in similar directions".
Conversely,words that are never used in a similar context willhave vectors that are approximately orthogonal.At the summary level, the MatchPlus ystemtranslates flee text into a mathematical representationin a meaningful way.
Note that the MatchPlusapproach does not use any external dictionaries,thesauri or knowledge bases to determine word vectorrelationships.
These relationships are learnedautomatically using only the text examples providedfor learning.
The result of the learning procedure is avocabulary of stem context vectors that can be usedfor a variety of applications including documentretrieval \[7\], routing \[8\], document clustering andother text processing tasks.Once the stem learning is complete, it ispossible to "query" the vector set to determine thenature of the learned relationships.
To perform thisoperation, the user selects a "root" word and thetrained context vector for that word is determined by atable lookup in the context vector vocabulary.MatchPlus computes the dot product of every otherword vector in the vocabulary to the selected word.The resulting dot products are sorted by magnitudewhere larger means closer in usage.Sets of words (text passages and queries) anddocuments can also be represented by context vectorsin the same information space.
Document contextvectors are derived as the inverse documentfrequency-weighted sum of the context vectorsassociated with words in the document.
Documentcontext vectors are normalized to prevent longdocuments from being favored over short documents.The resulting document context vectors have theproperty that documents that discuss similar themeswill have context vectors that point in similardirections.
It is this property that translates theproblem of assessment of similarity of content for textinto a geometry problem.
Documents that are similarare close in the space and dissimilar documents are faraway.
Additionally, it should be noted that alldocument vectors are unit length.
This preventssystem biases in retrieval due to document length.3.
ONE STEP CONTEXT VECTORLEARNINGThe sections below describe an approach tocontext vector learning that greatly reduces the amountof computer time and resources required to obtain atrained set of stem context vectors.
This approachuses a single pass through the training corpus (orcorpora) to obtain desired dot product values for theset of trained context vectors.
These desired dotproducts are used in a single pass through thevocabulary of word stems to expand a starting set ofquasi-orthogonal, high dimensional vectors.
Thisvector expansion and subsequent renormalizationresults in a set of context vectors that represents herelationships between words stems in a near-optimalfashion.
The time requirements for training this set ofvectors cale as O(Nn) where N is the number of wordstems in the vocabulary and n is the average number ofword stems found to co-occur (and/or be related to)any given word stem (usually on the order of severalhundred).
Using a near-worst case estimate of n = 1000word stems, a vocabulary size of 50,000 words, andassuming that at least ten iterations of the originallearning law are required for convergence (more oftenat least one hundred iterations are required), this newlearning law reduces the training time by a factor ofbetween 10 and 500 (depending on whether or not thenon co-occurring terms are explicitly considered in thecurrent learning law).3.1 Current MatchPlus Context VectorLearning LawThe current MatchPlus context vectorlearning law is presented in Figure 1 and discussed inSection 2.
This learning law can be derived as astochastic gradient descent procedure for minimizingthe cost function1 a 2t,Jwhere:T~ = Context vector for word stem iTj = Context vector for word stemjate = Desired ot product forword stem i andj context vectorsFigure 2.
Learning Law Cost Functionsubject to the constraints151nail = I1 11 ='where:H = (r.Figure 3.
Learning Law Vector MagnitudeConstraintsThe factors at ,  b are the desired ot productsfor the trained set of context vectors.
These desireddot products are found as a function of co-occurrencestatistics for word stems i and j.
In most cases thenumber of words for which a j .
j  is non-zero (i.e.
theco-occurring words) is several orders of magnitudesmaller than the size of the vocabulary.
In theory, thesummation on the right hand side extends over allword stems in the vocabulary.
In practice, however,this summation is performed only over words that co-occur with the target word stem i.
Since n=number ofco-occurring words is usually much less thanN=number of vocabulary word stems, summing onlyover co-occurring words represents a considerabletime savings.
Non co-occurring word stem contextvectors are adjusted by subtracting the mean contextvector at the end of each update iteration.
This has theeffect of spreading out the context vectors, hopefullydriving the context vectors of non co-occurring wordscloser to orthogonality.
With this approximation, thetime requirements for the current learning law scale asO(kNn) where k is the number of iterations requiredfor convergence.3.2 Approach to One Step LearningThe objective of any learning law used totrain context vectors is to minimize the cost functionspecified in Figure 2 subject to the constraints inFigure 3.
In order to avoid the requirement formultiple iterations, HNC proposes to evaluate theperformance ofthe following one step learning law:r : ' -  = r,JTiN~w Ti N?
"= I I r : " l lwhere:i~ M" = Context vector of target i after one step updatei~ = Context vector of target i before updatei~ = Context vector for word stem j before update.
Wordstem j co- occurs with word stem itl = Design parameter chosen to optimize performancea u = Desired context vector dot product fortarget i and co- occurring stemjFigure 4.
One Step Learning Law Equations.Note that the summation in Figure 4 is overco-occurring word stems.
This learning law ismotivated by the following observation.
Supposethere exists a cost function of two variables Xl and x2,where J (x , ,x2)=~X(a#-x ,  xb) 2 .
Supposei , j= l ,2further that we wish to choose 8x I and 8x  2 such thatreplacing x~ and xz with the quantities x I + 8x  I andx 2 +Sx  2 minimizes the cost function.
For thesituation in which IIx,ll=llx211 =1 and 8x I and 8x 2are assumed to be small, it is easily demonstrated thatthe solutions for 8x I and #x 2 are 8x  I = a~2 x2 and2_ O~12 #x 2 - -~- -x  I .
Adding these solutions to x~ and x2yields an expression similar to that of Figure 4.
Ofcourse, the fact that the resulting vectors must benormalized makes the analogy only approximate.However, Figure 4 can be viewed as a one stepapproximation to the optimal solution.
The value ofthis approximate solution is that it provides adequateperformance with only a fraction of the computationalrequirements.
This one step learning law scales asO(Nn), so that it is faster than the current learning lawby a factor of k (number of original learning lawiterations) and is faster than the theoretically derivedlearning law by a factor of kN/n.
For reasonablevalues of k, N, and n, this translated into a timesavings of a factor of 10 to 1000.3.3 Summary of One Step LearningThe successful development and testing ofthe one step learning law offers the possibility of muchfaster context vector training.
The performance of the152system using this law can be optimized throughparameter sweeps on context vector dimension andfree parameter 1"1 (see Figure 4).4.
APPROACH TO MULTILINGUALINFORMATION RETRIEVAL (MIR)The objective of solving the MIR problem isto provide the analyst/user with a flexible highperformance tool to allow retrieval of relevantinformation from multilingual corpora without theneed for prior translation of large volumes of text.The key issue is prior translation of theforeign language material.
Clearly, if all material wastranslated to a uniform representation, say English, theproblem is solved.
However, translation is timeconsuming, costly and subjective.
Additionally, thecurrent volumes of information would overwhelm anyorganization who attempted to perform bulktranslation.
Machine translation efforts have beenpartially successful, but these techniques frequentlyignore subtleties in the translation process.Additionally, the cost of development, tuning andvalidation of this approach is a hindrance towidespread use.HNC has developed an approach to the MIRproblem that leverages the context vector technology.It is called symmetric learning and its attributes, aswell as the implications of its attributes, are discussedin the section below.
Explanations of the approachwill be given from the frame of reference of twosimultaneous languages.
However, it should be notedthat hese approaches are extensible to many languagesbeing processed simultaneously.
These discussionsassume that language 1 is English and language 2 isSpanish.
It should also be noted that HNC hasimplemented a minimal subset of the symmetricapproach as a proof of concept.
The preliminaryresults are extremely encouraging.
A description ofthis system, the training corpus and the preliminaryresults are provided in Section 5.4.1 Symmetric LearningHNC has developed an approach to learningstem-level relationships across multiple languages.This technique, called "symmetric learning", is basedupon the use of tie words.
These tie words provideconnectivity between each language's portion of thecontext vector space.
However, learning is conductedusing both languages imultaneously, thus removingany donor language biases.The symmetric approach is based upon theuse of a "unified hash table".
The unified hash tableprovides the mechanism to translate a stem into anassociated context vector.
In the English-onlyMatchPlus ystem, this is a straight forward process.The stem in question is fed to the hashing function andan index is produced.
The resulting index is the offsetin the stem hash table.
The contents of that location inthe hash table is a pointer to the context vector datastructure.
Using this approach (hash functioncollisions not withstanding), each unique stem resultsin a unique entry and thus a unique context vector.What is proposed is to use the tie word list to providereferences for common context vectors.
An exampleis shown in Figure 5.
Assume that "attack" and"ataque" have been chosen as a tie word pair.
Sincethese words should have the same context vector,some form of connection must be made between thewords.
Figure 5 shows 4 words in the unified hashtable: "rebel", "attack", "ataque", and "contra".Without hash table unification based upon the tie wordlist, all four words would have unique and independentcontext vectors.
However, as can be seen in thefigure, the hash table entries for the tie words havebeen forced to point to a common context vector entry.This very simple approach allows multiple referencesto the same context vector entity.RebelAttackAtaqueContraStem IntoDataStructureStem IntoDatav\[ StructureStem Into \[DataStructure \[rContext Vectorfor "Rebel"Context VectorFor "Attack"and "Ataque"Context Vectorfor "Conlra"Figure 5.
Unified Hash Table Example.Once the mechanism for multiple referenceshas been established, the next step is to consider theactual training algorithm.
Example training text forEnglish and Spanish is shown in Figure 6.
For thisexample, it is assumed that the pair "attack" and"ataque" are a tie word pair.
Note that in thisexample, the text chosen is a near-literal translation.There is no requirement for parallel text for the153symmetric learning algorithm.
The English text inFigure 6 comes from the passage, "Four people werekilled in the attack by the rebel group Shining Path".The corresponding Spanish text is "Quatro personasfiJeron matadas en el ataque por el group contrasSendero Luminoso".
Figure 6 shows the contextwindow for the stemmed text centered on the tie wordattack.Like the standard MatchPlus context vectorlearning algorithm, the symmetric learning approachwill utilize a convolutional "context window" with acenter and neighbors.
The stem at the center of thewindow is called the "target".
The context vector forthe target stem is adjusted in the direction of itsneighbors' context vectors.peopl kill attack rebel groupneighbor target neighborperson mat ataq group contrneighbor target neighborFigure 6.
Symmetric Learning Example.The steps that will occur during learninggiven the text example shown in Figure 6 are asfollows:?
The convolutional window location is chosen andthe target and neighbor stems are identified.
Inthe English portion of this example, the window iscentered on the word "attack".
The neighborwords are "people", "killed", "rebel", and"group".?
The context vector for "attack" is moved in thedirection of its neighbors.
When the update iscompleted, the window is moved and the processis repeated.?
Spanish text is processed using the sameapproach.
In the Spanish portion of this example,the window has as its center the word "ataque".Neighbors for "ataque" are "personas", matado","groupo", and "contra".
The context vector for"ataque" is moved in the direction of itsneighbors.
When the update is completed, thewindow is moved and the process is repeated.?
Note that "attack" and "ataque" are a tie wordpair.
As a consequence, they share a commoncontext vector.
As a consequence, the contextvector for this pair has been influenced by thewords that have occurred in a similar context inboth languages.
Specifically, the attack-ataquetie word pair has been influenced by "people","kill", "rebel", "group", "personas", "matado","groupo", and "contra".?
Since all context vectors are in the sameinformation space, the symmetric learningtechnique will result in a unified informationspace for both languages.
Because of the "secondorder" learning effects of the context vectorapproach, not only will "attack" be related to"people" and "personas", but "people" will berelated to "personas", matado", groupo", etc.The block diagram for generation of a systemusing the symmetric approach is shown in Figure 7.
Ascan be seen in this figure, the symmetric system builduses the unified hash table as the basis for combiningthe stem sets from both languages.
Once this processhas taken place, all stem context vectors are stored in asingle dataset.
This unified set of context vectors isthe basis for formation of document context vectors.When the system generation is complete, MIR is readyfor query processing.
The block diagram for thisprocess is shown in Figure 8.Figure 7.
Symmetric Approach System Generation.154LinguaIQuery ~ Context Veokx Retd~al DocumentsMultilingualFigure 8.
Symmetric Approach Query Processing.Attributes of the symmetric learning approachare as follows:1.
Once tie word pairs (or n-toples) have beenselected, all subsequent processing is fullyautomated.
No other external knowledge sourcesare required.2.
Training text can be presented in any order.
Allof language 1 can be presented, followed bylanguage 2.
Alternately, documents from the twolanguages can be presented in intermixed order.3.
Context vector approach will learn "second order"relationships between the languages used fortraining.
The resulting unified context vector setcan be used to identify relationships betweenwords in the two languages.4.
The user can enter multi lingual queries based ontie words as well as non tie words.
Because allthe text is used during training second orderrelationships will be formed between non tiewords in different languages.
As an extremeexample if "white" is only used as "white house"and "blanca" is only used as in "casablanca" theuser will be able to query using only "white" andSpanish documents about "casa blanca" will beretrieved.
This is not so in the previous approachwhere the user is limited to using only tie wordsas query terms.5.
The basic approach described here is extensibleand capable of processing more than twolanguages at once.
Additionally, this approachcan be utilized for ideographic languages such asJapanese, Chinese and Korean.The key benefit of the MatchPlus contextvector approach is its ability to learn the relationshipsbetween words.
To simply disregard the relationshipscontained in the foreign data simply does not makesense.
The Symmetric Learning approach exploits thelearned relationships without he need to translate theforeign text.
The Symmetric Learning approachrequires only the translation of a limited number ofwords (tie words).
Furthermore, this operation eedonly be done once.The benefits of a multilingual approach totext processing extend well beyond text retrieval.Obviously, text routing and index term assignmentcould benefit from multilingual technology.
Languagelearning tools could exploit he technology to analyzethe relationships between word usage's acrosslanguages.
Finally, as innovative text visualizationtechniques are found, multilingual text processing willsurely enhance the value of such technology.5.
PRELIMINARY RESULTS OFSYMMETRIC LEARNING TESTSAs stated above, HNC has implemented alimited scope preliminary test of the symmetriclearning approach.
A preliminary set of 465 tie-wordswas prepared.
This list consisted of words of nearlyequivalent meaning in both English and Spanish.Approximately 100 tie-words were selected from theSpanish TREC topics.
The balance were selectedfrom high frequency words in the Spanish text.
Thecorpus used for testing consisted of data from threesources as shown in Table 1.Source Language Year DocumentsEl Norte Spanish 1993 395TREC English 1990 69APNewsData English 1993 416TimesTOTAL 880Size1.55 MB226 KB1.37 MB3.145MBTable 1.
Bilingual Training Corpus Statistics.The total number of stems in this test was32739.
The stemmer was disabled for both Spanishand English.
The existing MatchPlus learningalgorithm was run on the resulting bilingual corpus.When training was complete, a series of stem treeswere prepared to assess the nature of the learnedrelationships.
Ideally, one would hope to see bothEnglish and Spanish words in the stem trees.
Thepresence of bilingual information in the tree wouldindicate that he basic approach is viable.Stem trees were performed for both tie-wordsand non-tie-words.
Based upon the earlier assertion155that the presence of bilingual information indicatedcorrect behavior, the true proof of the concept is todemonstrate hat bilingual information occurs in stemtrees for non-tie-words.Stem Doc Stem DotFreq Freq Productaids 86 578 1.000aids~patients 12 22 0.699infected 15 40 0.579cases 20 35 0.540hiv 18 49 0.529tests 26 44 0.528related 20 26 0.519humana 17 21 0.517infectados 14 20 0.500portadores 16 31 0.5 O0epidemic 27 56 0.498virus 67 163 0.493smndrome 27 33 0.483inmunodeficiencia 21 26 0.481infectadas 18 21 0.479discrimination 22 38 0.463activists 22 38 0.442adquirida 26 28 0.442panel 12 22 0.415disease 21 53 0.403sick 31 58 0.368education 36 49 0.362Table 2.
Stem Tree for Tie Word "Aids".Stem Doc Word DotFreq Freq Productimmunodeficiency 6 6 1.000hiv 18 49 0.821virus 67 163 0.730infected 15 40 0.661causes 25 35 0.599humana 17 21 0.557aids 86 578 0.550inmunodeficiencia 21 26 0.514touted 5 5 0.488smndrome 27 33 0.452portadores 16 31 0.444human 107 198 0.433infectadas 18 21 0.418positive 20 27 0.417cases 20 35 0.371inmuno 6 8 0.366infectados 14 20 0.360portadoras 5 5 0.358deficiencia 8 10 0.351vih 10 18 0.343adquirida 26 28 0.340infection 12 22 0.337aids^patients 12 22 0.328test 16 21 0.321abusers^drug 5 6 0.319Table 3.
Stem Tree for Non-Tie-Word"Immunodeficiency"156Stem Dec Word DotFreq Freq Productinmunodeficiencia 21 26 1.000srundrome 27 33 0.939adquirida 26 28 0.886humana 17 21 0.855inmuno 6 8 0.739virus 67 163 0.711deficiencia 8 10 0.607portadores 16 31 0.528immunodeficiency 6 6 0.514infectadas 18 21 0.504hiv 18 49 0.496vih 10 18 0.489aids 86 578 0.481infectados 14 20 0.390portadoras 5 5 0.362causes 25 35 0.332aidsApatients 12 22 0.329infected 15 40 0.327causante 5 7 0.314muerto 15 18 0.309estima 19 19 0.301epidemic 27 56 0.298portador 8 10 0.287sick 31 58 0.287provoca 11 12 0.279linguistic boundaries.
Specifically, the list contains"infected" and "infectadas" and "infectados".
Also,the list contains related terms like "portadores'(carriers).
These relationships, though notunexpected, bodes well for the potential of theapproach.
Clearly, for tie-words, the technique willwork.
The true test is the stem trees for non-tie-words.Table 3 and Table 4 show the stem trees forthe non-tie-words "immuniodeficiency" and"inmunodeficiencia".
Clearly, their context of usagein the two languages should be similar andconsequently, their" stem trees should be similar.Inspection of Table 3 and Table 4 show exactly thetype of behavior desired.
Indeed, all the correctSpanish terms are present in the stem tree for theEnglish root "immuniodeficiency" and likewise for theSpanish root "inmunodeficiencia".
This data suggeststhat the proposed approach as a very high probabilityof correctly representing the terms in both Spanish andEnglish in a unified meaning space.To add one more example of the ability of thecontext vector approach to identify second orderrelationships, consider the stem tree for the term"monopoly" shown in Table 5.
Notice that MatchPluscorrectly detects the relationship between "monopoly"and "pemex" the Mexican national oil company.Table 4.
Stem Tree for Non-Tie-Word"Imnunodeficiencia"Table 2 shows the stem tree for the tie-word"AIDS".
In Spanish, AIDS has the acronym "SIDA"which stands for sindrome inmuno deficienciaadquirida.
Also note that HIV in Spanish is VIH forvirus inmunodeficiencia humana.
Inspection of Table2 shows that all stems present make sense and that thestem tree captures the contextual similarity across157Stem Doc Word DotFreq.
Freq.
Productmonopoly i 20 43 1.000pemex 44 314 0.639oil 36 48 0.565stolen 16 16 0.498refinacisn 12 23 0.475reestructuracisn 15 24 0.445basica 7 9 0.423petrolera 12  44 0.372Icompetencia 28 53 0.351shell 9 18 0.347reforma 24 46 0.323privatization 12 25 0.306exploracisn 9 18 0.305petrsleos 22 26 0.304oil%hell 3 5 0.304petroqummica 12 33 0.302Table 5.
Stem Tree for Tie-Word "Monopoly"REFERENCES\[ 1 \] Salton, G.
(ed.
), "The SMART Retrieval System -Experiments in Automatic Document Processing",Prentice-Hall, 1971.\[2\] Salton, G., "Another Look at Automatic TextRetrieval Systems", Communications of the ACM,Vol.
20, 1986, pp.
648 - 656.\[3\] Salton, G., "Automatic Text Processing",Addison-Wesley, 1989.\[4\] Sutcliffe, R., "Distributed Representations in aText Based Information Retrieval System: A NewWay of Using the Vector Space Model",Communications of the ACM, Jan. 1991, pp.
123 -132.\[5\] KoU, M.B., "WEIRD: An Approach to Concept-Based Information Retrieval", SIGIR Forum, Vol.
13,No.
4, Spring 1979, pp.
32 - 50.\[6\] Watson, G.S., "Statistics on Spheres", John Wileyand Sons, 1983.\[7\] Gallant, S.l., W. R. Caid, et al "Feedback andMixing Experiments with MatchPlus", ProceedingsTREC-2 Conference, D. Harman, Ed, Gaithersburg,MD.
Aug. 1993.\[8\] Sasseen, R. V., J. L. Carleton, W. R. Caid,"CONVECTIS: A Context Vector-Based On-LineIndexing System", in Proceedings IEEE Dual-UseConference, 1995.5.1.
Multilingual Approach SummaryThe success demonstrated on bilingual textcharacterization strongly suggests that the contextvector approach will provide an effective means ofproviding multilingual information retrieval.
Becausethe preliminary results are so positive, HNC proposesto extend the preliminary demonstration system toSpanish in two steps.
The first step for Spanish will beto develop all the required support files and softwarefor the Spanish language including stop lists,stemmers, etc.
This will result in a full capabilityMatchPlus ystem for Spanish.
The second step willbe to continue development of an English-SpanishMatchPlus ystem by augmenting the current ie-wordlist and to perform a series of engineeringexperiments.
These experiments will identify thesensitivity of system performance to thecharacteristics of the tie-words chosen.158
