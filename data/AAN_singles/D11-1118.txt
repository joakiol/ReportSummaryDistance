Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1281?1290,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsUnsupervised Dependency Parsing without Gold Part-of-Speech TagsValentin I.
Spitkovsky?
?valentin@cs.stanford.eduHiyan Alshawi?hiyan@google.comAngel X.
Chang?
?angelx@cs.stanford.eduDaniel Jurafsky?
?jurafsky@stanford.edu?Computer Science DepartmentStanford UniversityStanford, CA, 94305?Google ResearchGoogle Inc.Mountain View, CA, 94043?Department of LinguisticsStanford UniversityStanford, CA, 94305AbstractWe show that categories induced by unsuper-vised word clustering can surpass the perfor-mance of gold part-of-speech tags in depen-dency grammar induction.
Unlike classic clus-tering algorithms, our method allows a wordto have different tags in different contexts.In an ablative analysis, we first demonstratethat this context-dependence is crucial to thesuperior performance of gold tags ?
requir-ing a word to always have the same part-of-speech significantly degrades the performanceof manual tags in grammar induction, elim-inating the advantage that human annotationhas over unsupervised tags.
We then introducea sequence modeling technique that combinesthe output of a word clustering algorithm withcontext-colored noise, to allow words to betagged differently in different contexts.
Withthese new induced tags as input, our state-of-the-art dependency grammar inducer achieves59.1% directed accuracy on Section 23 (allsentences) of the Wall Street Journal (WSJ)corpus ?
0.7% higher than using gold tags.1 IntroductionUnsupervised learning ?
machine learning withoutmanually-labeled training examples ?
is an activearea of scientific research.
In natural language pro-cessing, unsupervised techniques have been success-fully applied to tasks such as word alignment for ma-chine translation.
And since the advent of the web,algorithms that induce structure from unlabeled datahave continued to steadily gain importance.
In thispaper we focus on unsupervised part-of-speech tag-ging and dependency parsing ?
two related prob-lems of syntax discovery.
Our methods are applica-ble to vast quantities of unlabeled monolingual text.Not all research on these problems has been fullyunsupervised.
For example, to the best of our knowl-edge, every new state-of-the-art dependency gram-mar inducer since Klein and Manning (2004) reliedon gold part-of-speech tags.
For some time, multi-point performance degradations caused by switchingto automatically induced word categories have beeninterpreted as indications that ?good enough?
parts-of-speech induction methods exist, justifying the fo-cus on grammar induction with supervised part-of-speech tags (Bod, 2006), pace (Cramer, 2007).
Oneof several drawbacks of this practice is that it weak-ens any conclusions that could be drawn about howcomputers (and possibly humans) learn in the ab-sence of explicit feedback (McDonald et al, 2011).In turn, not all unsupervised taggers actually in-duce word categories: Many systems ?
known aspart-of-speech disambiguators (Merialdo, 1994) ?rely on external dictionaries of possible tags.
Ourwork builds on two older part-of-speech inducers?
word clustering algorithms of Clark (2000) andBrown et al (1992) ?
that were recently shown tobe more robust than other well-known fully unsuper-vised techniques (Christodoulopoulos et al, 2010).We investigate which properties of gold part-of-speech tags are useful in grammar induction andparsing, and how these properties could be intro-duced into induced tags.
We also explore the numberof word classes that is good for grammar induction:in particular, whether categorization is needed at all.By removing the ?unrealistic simplification?
of us-ing gold tags (Petrov et al, 2011, ?3.2, Footnote 4),we will go on to demonstrate why grammar induc-tion from plain text is no longer ?still too difficult.
?1281NNS VBD IN NN ?Payrolls fell in September .P = (1 ?0?
??
?PSTOP(, L, T)) ?
PATTACH(, L, VBD)?
(1 ?
PSTOP(VBD, L, T)) ?
PATTACH(VBD, L, NNS)?
(1 ?
PSTOP(VBD, R, T)) ?
PATTACH(VBD, R, IN)?
(1 ?
PSTOP(IN, R, T)) ?
PATTACH(IN, R, NN)?
PSTOP(VBD, L, F) ?
PSTOP(VBD, R, F)?
PSTOP(NNS, L, T) ?
PSTOP(NNS, R, T)?
PSTOP(IN, L, T) ?
PSTOP(IN, R, F)?
PSTOP(NN, L, T) ?
PSTOP(NN, R, T)?
PSTOP(, L, F)?
??
?1?
PSTOP(, R, T)?
??
?1.Figure 1: A dependency structure for a short WSJ sen-tence and its probability, factored by the DMV, using goldtags, after summing out PORDER (Spitkovsky et al, 2009).2 MethodologyIn all experiments, we model the English grammarvia Klein and Manning?s (2004) Dependency Modelwith Valence (DMV), induced from subsets of not-too-long sentences of the Wall Street Journal (WSJ).2.1 The ModelThe original DMV is a single-state head automatamodel (Alshawi, 1996) over lexical word classes{cw} ?
gold part-of-speech tags.
Its generative storyfor a sub-tree rooted at a head (of class ch) rests onthree types of independent decisions: (i) initial di-rection dir ?
{L, R} in which to attach children, viaprobability PORDER(ch); (ii) whether to seal dir, stop-ping with probability PSTOP(ch, dir, adj), conditionedon adj ?
{T, F} (true iff considering dir?s first, i.e., ad-jacent, child); and (iii) attachments (of class ca), ac-cording to PATTACH(ch, dir, ca).
This recursive processproduces only projective trees.
A root token ?
gen-erates the head of the sentence as its left (and only)child (see Figure 1 for a simple, concrete example).2.2 Learning AlgorithmsThe DMV lends itself to unsupervised learning viainside-outside re-estimation (Baker, 1979).
Kleinand Manning (2004) initialized their system using an?ad-hoc harmonic?
completion, followed by trainingusing 40 steps of EM (Klein, 2005).
We reproducethis set-up, iterating without actually verifying con-vergence, in most of our experiments (#1?4, ?3?4).Experiments #5?6 (?5) employ our new state-of-the-art grammar inducer (Spitkovsky et al, 2011),which uses constrained Viterbi EM (details in ?5).2.3 Training DataThe DMV is usually trained on a customized sub-set of Penn English Treebank?s Wall Street Jour-nal portion (Marcus et al, 1993).
Following Kleinand Manning (2004), we begin with reference con-stituent parses, prune out all empty sub-trees andremove punctuation and terminals (tagged # and $)that are not pronounced where they appear.
We thentrain only on the remaining sentence yields consist-ing of no more than fifteen tokens (WSJ15), in mostof our experiments (#1?4, ?3?4); by contrast, Kleinand Manning?s (2004) original system was trainedusing less data: sentences up to length ten (WSJ10).1Our final experiments (#5?6, ?5) employ a simplescaffolding strategy (Spitkovsky et al, 2010a) thatfollows up initial training at WSJ15 (?less is more?
)with an additional training run (?leapfrog?)
that in-corporates most sentences of the data set, at WSJ45.2.4 Evaluation MethodsEvaluation is against the training set, as is standardpractice in unsupervised learning, in part becauseKlein and Manning (2004, ?3) did not smooth theDMV (Klein, 2005, ?6.2).
For most of our experi-ments (#1?4, ?3?4), this entails starting with the ref-erence trees from WSJ15 (as modified in ?2.3), au-tomatically converting their labeled constituents intounlabeled dependencies using deterministic ?head-percolation?
rules (Collins, 1999), and then com-puting (directed) dependency accuracy scores of thecorresponding induced trees.
We report overall per-centages of correctly guessed arcs, including thearcs from sentence root symbols, as is standard prac-tice (Paskin, 2001; Klein and Manning, 2004).For a meaningful comparison with previous work,we also test some of the models from our earlier ex-periments (#1,3) ?
and both models from final ex-periments (#5,6) ?
against Section 23 of WSJ?, af-ter applying Laplace (a.k.a.
?add one?)
smoothing.1WSJ15 contains 15,922 sentences up to length fifteen (a to-tal of 163,715 tokens, not counting punctuation) ?
versus 7,422sentences of at most ten words (only 52,248 tokens) comprisingWSJ10 ?
and is a better trade-off between the quantity andcomplexity of training data in WSJ (Spitkovsky et al, 2009).1282Accuracy Viable1.
manual tags Unsupervised Sky Groupsgold 50.7 78.0 36mfc 47.2 74.5 34mfp 40.4 76.4 160ua 44.3 78.4 3282. tagless lexicalized modelsfull 25.8 97.3 49,180partial 29.3 60.5 176none 30.7 24.5 13. tags from a flat (Clark, 2000) clustering47.8 83.8 1974. prefixes of a hierarchical (Brown et al, 1992) clusteringfirst 7 bits 46.4 73.9 968 bits 48.0 77.8 1659 bits 46.8 82.3 262Table 1: Directed accuracies for the ?less is more?
DMV,trained on WSJ15 (after 40 steps of EM) and evaluatedalso against WSJ15, using various lexical categories inplace of gold part-of-speech tags.
For each tag-set, weinclude its effective number of (non-empty) categories inWSJ15 and the oracle skylines (supervised performance).3 Motivation and Ablative AnalysesThe concepts of polysemy and synonymy are of fun-damental importance in linguistics.
For words thatcan take on multiple parts of speech, knowing thegold tag can reduce ambiguity, improving parsing bylimiting the search space.
Furthermore, pooling thestatistics of words that play similar syntactic roles,as signaled by shared gold part-of-speech tags, cansimplify the learning task, improving generalizationby reducing sparsity.
We begin with two sets of ex-periments that explore the impact that each of thesefactors has on grammar induction with the DMV.3.1 Experiment #1: Human-Annotated TagsOur first set of experiments attempts to isolate theeffect that replacing gold part-of-speech tags withdeterministic one class per word mappings has onperformance, quantifying the cost of switching to amonosemous clustering (see Table 1: manual; andTable 4).
Grammar induction with gold tags scores50.7%, while the oracle skyline (an ideal, supervisedinstance of the DMV) could attain 78.0% accuracy.It may be worth noting that only 6,620 (13.5%) of49,180 unique tokens in WSJ appear with multiplepart-of-speech tags.
Most words, like it, are alwaystagged the same way (5,768 times PRP).
Some words,token mfc mfp uait {PRP} {PRP} {PRP}gains {NNS} {VBZ, NNS} {VBZ, NNS}the {DT} {JJ, DT} {VBP, NNP, NN, JJ, DT, CD}Table 2: Example most frequent class, most frequent pairand union all reassignments for tokens it, the and gains.like gains, usually serve as one part of speech (227times NNS, as in the gains) but are occasionally useddifferently (5 times VBZ, as in he gains).
Only 1,322tokens (2.7%) appear with three or more differentgold tags.
However, this minority includes the mostfrequent word ?
the (50,959 times DT, 7 times JJ,6 times NNP and once as each of CD, NN and VBP).2We experimented with three natural reassign-ments of part-of-speech categories (see Table 2).The first, most frequent class (mfc), simply mapseach token to its most common gold tag in the entireWSJ (with ties resolved lexicographically).
This ap-proach discards two gold tags (types PDT and RBR arenot most common for any of the tokens in WSJ15)and costs about three-and-a-half points of accuracy,in both supervised and unsupervised regimes.Another reassignment, union all (ua), maps eachtoken to the set of all of its observed gold tags, againin the entire WSJ.
This inflates the number of group-ings by nearly a factor of ten (effectively lexicaliz-ing the most ambiguous words),3 yet improves theoracle skyline by half-a-point over actual gold tags;however, learning is harder with this tag-set, losingmore than six points in unsupervised training.Our last reassignment, most frequent pair (mfp),allows up to two of the most common tags intoa token?s label set (with ties, once again, resolvedlexicographically).
This intermediate approach per-forms strictly worse than union all, in both regimes.3.2 Experiment #2: Lexicalization BaselinesOur next set of experiments assesses the benefits ofcategorization, turning to lexicalized baselines thatavoid grouping words altogether.
All three modelsdiscussed below estimated the DMV without usingthe gold tags in any way (see Table 1: lexicalized).2Some of these are annotation errors in the treebank (Bankoand Moore, 2004, Figure 2): such (mis)taggings can severelydegrade the accuracy of part-of-speech disambiguators, withoutadditional supervision (Banko and Moore, 2004, ?5, Table 1).3Kupiec (1992) found that the 50,000-word vocabulary ofthe Brown corpus similarly reduces to ?400 ambiguity classes.1283First, not surprisingly, a fully-lexicalized modelover nearly 50,000 unique words is able to essen-tially memorize the training set, supervised.
(With-out smoothing, it is possible to deterministically at-tach most rare words in a dependency tree correctly,etc.)
Of course, local search is unlikely to find goodinstantiations for so many parameters, causing unsu-pervised accuracy for this model to drop in half.For our next experiment, we tried an intermediate,partially-lexicalized approach.
We mapped frequentwords ?
those seen at least 100 times in the trainingcorpus (Headden et al, 2009) ?
to their own indi-vidual categories, lumping the rest into a single ?un-known?
cluster, for a total of under 200 groups.
Thismodel is significantly worse for supervised learn-ing, compared even with the monosemous clustersderived from gold tags; yet it is only slightly morelearnable than the broken fully-lexicalized variant.Finally, for completeness, we trained a model thatmaps every token to the same one ?unknown?
cat-egory.
As expected, such a trivial ?clustering?
isineffective in supervised training; however, it out-performs both lexicalized variants unsupervised,4strongly suggesting that lexicalization alone may beinsufficient for the DMV and hinting that some de-gree of categorization is essential to its learnability.Cluster #173 Cluster #1881. open 1. get2.
free 2. make3.
further 3. take4.
higher 4. find5.
lower 5. give6.
similar 6. keep7.
leading 7. pay8.
present 8. buy9.
growing 9. win10.
increased 10. sell......37. cool 42. improve......1,688. up-wind 2,105. zero-outTable 3: Representative members for two of the flat wordgroupings: cluster #173 (left) contains adjectives, espe-cially ones that take comparative (or other) complements;cluster #188 comprises bare-stem verbs (infinitive stems).
(Of course, many of the words have other syntactic uses.
)4Note that it also beats supervised training.
That isn?t a bug:Spitkovsky et al (2010b, ?7.2) explain this paradox in the DMV.4 Grammars over Induced Word ClustersWe have demonstrated the need for grouping simi-lar words, estimated a bound on performance lossesdue to monosemous clusterings and are now readyto experiment with induced part-of-speech tags.
Weuse two sets of established, publicly-available hardclustering assignments, each computed from a muchlarger data set than WSJ (approximately a millionwords).
The first is a flat mapping (200 clusters)constructed by training Clark?s (2000) distributionalsimilarity model over several hundred million wordsfrom the British National and the English Gigawordcorpora.5 The second is a hierarchical clustering ?binary strings up to eighteen bits long ?
constructedby running Brown et al?s (1992) algorithm over 43million words from the BLLIP corpus, minus WSJ.64.1 Experiment #3: A Flat Word ClusteringOur main purely unsupervised results are with a flatclustering (Clark, 2000) that groups words havingsimilar context distributions, according to Kullback-Leibler divergence.
(A word?s context is an orderedpair: its left- and right-adjacent neighboring words.
)To avoid overfitting, we employed an implemen-tation from previous literature (Finkel and Manning,2009).
The number of clusters (200) and the suf-ficient amount of training data (several hundred-million words) were tuned to a task (NER) that isnot directly related to dependency parsing.
(Table 3shows representative entries for two of the clusters.
)We added one more category (#0) for unknownwords.
Now every token in WSJ could again be re-placed by a coarse identifier (one of at most 201,instead of just 36), in both supervised and unsuper-vised training.
(Our training code did not change.
)The resulting supervised model, though not asgood as the fully-lexicalized DMV, was more thanfive points more accurate than with gold part-of-speech tags (see Table 1: flat).
Unsupervised accu-racy was lower than with gold tags (see also Table 4)but higher than with all three derived hard assign-ments.
This suggests that polysemy (i.e., ability to5http://nlp.stanford.edu/software/stanford-postagger-2008-09-28.tar.gz:models/egw.bnc.2006http://people.csail.mit.edu/maestro/papers/bllip-clusters.gz12841 4 16 64 256 1,024 (# of clusters) 49,18020406080%goldmfc mfp uafullpartialnoneflatgoldmfcmfpuafullpartialnoneflatk = 1 2 3 4 5 6 7 8 9 10 11 12 ?
18 bitsFigure 2: Parsing performance (accuracy on WSJ15) as a ?function?
of the number of syntactic categories, for all prefixlengths ?
k ?
{1, .
.
.
, 18} ?
of a hierarchical (Brown et al, 1992) clustering, connected by solid lines (dependencygrammar induction in blue; supervised oracle skylines in red, above).
Tagless lexicalized models (full, partial andnone) connected by dashed lines.
Models based on gold part-of-speech tags, and derived monosemous clusters (mfc,mfp and ua), shown as vertices of gold polygons.
Models based on a flat (Clark, 2000) clustering indicated by squares.tag a word differently in context) may be the primaryadvantage of manually constructed categorizations.4.2 Experiment #4: A Hierarchical ClusteringThe purpose of this batch of experiments is to showthat Clark?s (2000) algorithm isn?t unique in its suit-ability for grammar induction.
We found that Brownet al?s (1992) older information-theoretic approach,which does not explicitly address the problems ofrare and ambiguous words (Clark, 2000) and was de-signed to induce large numbers of plausible syntac-tic and semantic clusters, can perform just as well.Once again, the sufficient amount of data (43 mil-lion words) was tuned in earlier work (Koo, 2010).His task of interest was, in fact, dependency parsing.But since this algorithm is hierarchical (i.e., thereisn?t a parameter for the number of categories), wedoubt that there was a strong enough risk of overfit-ting to question the clustering?s unsupervised nature.As there isn?t a set number of categories, we usedbinary prefixes of length k from each word?s addressin the computed hierarchy as cluster labels.
Resultsfor 7 ?
k ?
9 bits (approximately 100?250 non-empty clusters, close to the 200 we used before) aresimilar to those of flat clusters (see Table 1: hierar-chical).
Outside of this range, however, performancecan be substantially worse (see Figure 2), consistentwith earlier findings: Headden et al (2008) demon-strated that (constituent) grammar induction, usingthe singular-value decomposition (SVD-based) tag-ger of Schu?tze (1995), also works best with 100?200clusters.
Important future research directions mayinclude learning to automatically select a good num-ber of word categories (in the case of flat clusterings)and ways of using multiple clustering assignments,perhaps of different granularities/resolutions, in tan-dem (e.g., in the case of a hierarchical clustering).4.3 Further EvaluationIt is important to enable easy comparison with pre-vious and future work.
Since WSJ15 is not a stan-dard test set, we evaluated two key experiments ?
?less is more?
with gold part-of-speech tags (#1, Ta-ble 1: gold) and with Clark?s (2000) clusters (#3, Ta-ble 1: flat) ?
on all sentences (not just length fifteenand shorter), in Section 23 of WSJ (see Table 4).This required smoothing both final models (?2.4).We showed that two classic unsupervised word1285System Description Accuracy#1 (?3.1) ?less is more?
(Spitkovsky et al, 2009) 44.0#3 (?4.1) ?less is more?
with monosemous induced tags 41.4 (-2.6)Table 4: Directed accuracies on Section 23 of WSJ (all sentences) for two experiments with the base system.clusterings ?
one flat and one hierarchical ?
canbe better for dependency grammar induction thanmonosemous syntactic categories derived from goldpart-of-speech tags.
And we confirmed that the un-supervised tags are worse than the actual gold tags,in a simple dependency grammar induction system.5 State-of-the-Art without Gold TagsUntil now, we have deliberately kept our experimen-tal methods simple and nearly identical to Klein andManning?s (2004), for clarity.
Next, we will explorehow our main findings generalize beyond this toysetting.
A preliminary test will simply quantify theeffect of replacing gold part-of-speech tags with themonosemous flat clustering (as in experiment #3,?4.1) on a modern grammar inducer.
And our lastexperiment will gauge the impact of using a polyse-mous (but still unsupervised) clustering instead, ob-tained by executing standard sequence labeling tech-niques to introduce context-sensitivity into the origi-nal (independent) assignment or words to categories.These final experiments are with our latest state-of-the-art system (Spitkovsky et al, 2011) ?
a par-tially lexicalized extension of the DMV that usesconstrained Viterbi EM to train on nearly all of thedata available in WSJ, at WSJ45 (48,418 sentences;986,830 non-punctuation tokens).
The key contribu-tion that differentiates this model from its predeces-sors is that it incorporates punctuation into grammarinduction (by turning it into parsing constraints, in-stead of ignoring punctuation marks altogether).
Intraining, the model makes a simplifying assumption?
that sentences can be split at punctuation and thatthe resulting fragments of text could be parsed inde-pendently of one another (these parsed fragments arethen reassembled into full sentence trees, by pars-ing the sequence of their own head words).
Fur-thermore, the model continues to take punctuationmarks into account in inference (using weaker, moreaccurate constraints, than in training).
This systemscores 58.4% on Section 23 of WSJ?
(see Table 5).5.1 Experiment #5: A Monosemous ClusteringAs in experiment #3 (?4.1), we modified the basesystem in exactly one way: we swapped out goldpart-of-speech tags and replaced them with a flat dis-tributional similarity clustering.
In contrast to sim-pler models, which suffer multi-point drops in ac-curacy from switching to unsupervised tags (e.g.,2.6%), our new system?s performance degrades onlyslightly, by 0.2% (see Tables 4 and 5).
This resultimproves over substantial performance degradationspreviously observed for unsupervised dependencyparsing with induced word categories (Klein andManning, 2004; Headden et al, 2008, inter alia).7One risk that arises from using gold tags is thatnewer systems could be finding cleverer ways to ex-ploit manual labels (i.e., developing an over-relianceon gold tags) instead of actually learning to acquirelanguage.
Part-of-speech tags are known to containsignificant amounts of information for unlabeled de-pendency parsing (McDonald et al, 2011, ?3.1), sowe find it reassuring that our latest grammar induceris less dependent on gold tags than its predecessors.5.2 Experiment #6: A Polysemous ClusteringResults of experiments #1 and 3 (?3.1, 4.1) suggestthat grammar induction stands to gain from relaxingthe one class per word assumption.
We next test thisconjecture by inducing a polysemous unsupervisedword clustering, then using it to induce a grammar.Previous work (Headden et al, 2008, ?4) foundthat simple bitag hidden Markov models, classicallytrained using the Baum-Welch (Baum, 1972) variantof EM (HMM-EM), perform quite well,8 on aver-age, across different grammar induction tasks.
Suchsequence models incorporate a sensitivity to contextvia state transition probabilities PTRAN(ti | ti?1), cap-turing the likelihood that a tag ti immediately fol-lows the tag ti?1; emission probabilities PEMIT(wi | ti)capture the likelihood that a word of type ti is wi.7We also briefly comment on this result in the ?punctuation?paper (Spitkovsky et al, 2011, ?7), published concurrently.8They are also competitive with Bayesian estimators, onlarger data sets, with cross-validation (Gao and Johnson, 2008).1286System Description Accuracy(?5) ?punctuation?
(Spitkovsky et al, 2011) 58.4#5 (?5.1) ?punctuation?
with monosemous induced tags 58.2 (-0.2)#6 (?5.2) ?punctuation?
with context-sensitive induced tags 59.1 (+0.7)Table 5: Directed accuracies on Section 23 of WSJ (all sentences) for experiments with the state-of-the-art system.We need a context-sensitive tagger, and HMMmodels are good ?
relative to other tag-inducers.However, they are not better than gold tags, at leastwhen trained using a modest amount of data.9 Forthis reason, we decided to relax the monosemousflat clustering, plugging it in as an initializer for theHMM.
The main problem with this approach is that,at least without smoothing, every monosemous la-beling is trivially at a local optimum, since P(ti | wi)is deterministic.
To escape the initial assignment,we used a ?noise injection?
technique (Selman etal., 1994), inspired by the contexts of Clark (2000).First, we collected the MLE statistics for PR(ti+1 | ti)and PL(ti | ti+1) in WSJ, using the flat monosemoustags.
Next, we replicated the text of WSJ 100-fold.Finally, we retagged this larger data set, as follows:with probability 80%, a word kept its monosemoustag; with probability 10%, we sampled a new tagfrom the left context (PL) associated with the origi-nal (monosemous) tag of its rightmost neighbor; andwith probability 10%, we drew a tag from the rightcontext (PR) of its leftmost neighbor.10 Given thatour initializer ?
and later the input to the grammarinducer ?
are hard assignments of tags to words, weopted for (the faster and simpler) Viterbi training.In the spirit of reproducibility, we again used anoff-the-shelf component for tagging-related work.11Viterbi training converged after just 17 steps, re-placing the original monosemous tags for 22,280 (of1,028,348 non-punctuation) tokens in WSJ.
For ex-9All of Headden et al?s (2008) grammar induction experi-ments with induced parts-of-speech were worse than their bestresults using gold part-of-speech tags, most likely because theyused a very small corpus (half of WSJ10) to cluster words.10We chose the sampling split (80:10:10) and replication pa-rameter (100) somewhat arbitrarily, so better results could likelybe obtained with tuning.
However, we suspect that the real gainswould come from using soft clustering techniques (Hinton andRoweis, 2003; Pereira et al, 1993, inter alia) and propagating(joint) estimates of tag distributions into a parser.
Our ad-hocapproach is intended to serve solely as a proof of concept.11David Elworthy?s C+ tagger, with options -i t -G -l,available from http://friendly-moose.appspot.com/code/NewCpTag.zip.ample, the first changed sentence is #3 (of 49,208):Some ?circuit breakers?
installed afterthe October 1987 crash failed their firsttest, traders say, unable to cool the sellingpanic in both stocks and futures.Above, the word cool gets relabeled as #188 (from#173 ?
see Table 3), since its context is moresuggestive of an infinitive verb than of its usualgrouping with adjectives.
(A proper analysis of allchanges, however, is beyond the scope of this work.
)Using this new context-sensitive hard assignmentof tokens to unsupervised categories our gram-mar inducer attained a directed accuracy of 59.1%,nearly a full point better than with the monosemoushard assignment (see Table 5).
To the best of ourknowledge it is also the first state-of-the-art unsuper-vised dependency parser to perform better with in-duced categories than with gold part-of-speech tags.6 Related WorkEarly work in dependency grammar induction al-ready relied on gold part-of-speech tags (Carroll andCharniak, 1992).
Some later models (Yuret, 1998;Paskin, 2001, inter alia) attempted full lexicaliza-tion.
However, Klein and Manning (2004) demon-strated that effort to be worse at recovering depen-dency arcs than choosing parse structures at random,leading them to incorporate gold tags into the DMV.Klein and Manning (2004, ?5, Figure 6) had alsotested their own models with induced word classes,constructed using a distributional similarity cluster-ing method (Schu?tze, 1995).
Without gold part-of-speech tags, their combined DMV+CCM model wasabout five points worse, both in (directed) unlabeleddependency accuracy (42.3% vs. 47.5%)12 and unla-beled bracketing F1 (72.9% vs. 77.6%), on WSJ10.In constituent parsing, earlier Seginer (2007a, ?6,Table 1) built a fully-lexicalized grammar inducer12On the same evaluation set (WSJ10), our context-sensitivesystem without gold tags (Experiment #6, ?5.2) scores 66.8%.1287that was competitive with DMV+CCM despite notusing gold tags.
His CCL parser has since beenimproved via a ?zoomed learning?
technique (Re-ichart and Rappoport, 2010).
Moreover, Abend etal.
(2010) reused CCL?s internal distributional rep-resentation of words in a cognitively-motivated part-of-speech inducer.
Unfortunately their tagger didnot make it into Christodoulopoulos et al?s (2010)excellent and otherwise comprehensive evaluation.Outside monolingual grammar induction, fully-lexicalized statistical dependency transduction mod-els have been trained from unannotated parallel bi-texts for machine translation (Alshawi et al, 2000).More recently, McDonald et al (2011) demonstratedan impressive alternative to grammar induction byprojecting reference parse trees from languages thathave annotations to ones that are resource-poor.13 Ituses graph-based label propagation over a bilingualsimilarity graph for a sentence-aligned parallel cor-pus (Das and Petrov, 2011), inducing part-of-speechtags from a universal tag-set (Petrov et al, 2011).Even in supervised parsing we are starting to seea shift away from using gold tags.
For example,Alshawi et al (2011) demonstrated good results formapping text to underspecified semantics via depen-dencies without resorting to gold tags.
And Petrov etal.
(2010, ?4.4, Table 4) observed only a small per-formance loss ?going POS-less?
in question parsing.We are not aware of any systems that induce bothsyntactic trees and their part-of-speech categories.However, aside from the many systems that inducetrees from gold tags, there are also unsupervisedmethods for inducing syntactic categories from goldtrees (Finkel et al, 2007; Pereira et al, 1993), aswell as for inducing dependencies from gold con-stituent annotations (Sangati and Zuidema, 2009;Chiang and Bikel, 2002).
Considering that Headdenet al?s (2008) study of part-of-speech taggers foundno correlation between standard tagging metrics andthe quality of induced grammars, it may be time fora unified treatment of these very related syntax tasks.13When the target language is English, however, their best ac-curacy (projected from Greek) is low: 45.7% (McDonald et al,2011, ?4, Table 2); tested on the same CoNLL 2007 evaluationset (Nivre et al, 2007), our ?punctuation?
system with context-sensitive induced tags (trained on WSJ45, without gold tags)performs substantially better, scoring 51.6%.
Note that this isalso an improvement over our system trained on the CoNLL setusing gold tags: 50.3% (Spitkovsky et al, 2011, ?8, Table 6).7 Discussion and ConclusionsUnsupervised word clustering techniques of Brownet al (1992) and Clark (2000) are well-suited to de-pendency parsing with the DMV.
Both methods out-perform gold parts-of-speech in supervised modes.And both can do better than monosemous clustersderived from gold tags in unsupervised training.
Weshowed how Clark?s (2000) flat tags can be relaxed,using context, with the resulting polysemous cluster-ing outperforming gold part-of-speech tags for theEnglish dependency grammar induction task.Monolingual evaluation is a significant flaw in ourmethodology, however.
One (of many) take-homepoints made in Christodoulopoulos et al?s (2010)study is that results on one language do not neces-sarily correlate with other languages.14 Assumingthat our results do generalize, it will still remain toremove the present reliance on gold tokenization andsentence boundary labels.
Nevertheless, we feel thateliminating gold tags is an important step towardsthe goal of fully-unsupervised dependency parsing.We have cast the utility of a categorization schemeas a combination of two effects on parsing accuracy:a synonymy effect and a polysemy effect.
Resultsof our experiments with both full and partial lexi-calization suggest that grouping similar words (i.e.,synonymy) is vital to grammar induction with theDMV.
This is consistent with an established view-point, that simple tabulation of frequencies of wordsparticipating in certain configurations cannot be reli-ably used for comparing their likelihoods (Pereira etal., 1993, ?4.2): ?The statistics of natural languagesis inherently ill defined.
Because of Zipf?s law, thereis never enough data for a reasonable estimation ofjoint object distributions.?
Seginer?s (2007b, ?1.4.4)argument, however, is that the Zipfian distribution?
a property of words, not parts-of-speech ?should allow frequent words to successfully guide14Furthermore, it would be interesting to know how sensitivedifferent head-percolation schemes (Yamada and Matsumoto,2003; Johansson and Nugues, 2007) would be to gold versusunsupervised tags, since the Magerman-Collins rules (Mager-man, 1995; Collins, 1999) agree with gold dependency annota-tions only 85% of the time, even for WSJ (Sangati and Zuidema,2009).
Proper intrinsic evaluation of dependency grammar in-ducers is not yet a solved problem (Schwartz et al, 2011).1288parsing and learning: ?A relatively small number offrequent words appears almost everywhere and mostwords are never too far from such a frequent word(this is also the principle behind successful part-of-speech induction).?
We believe that it is important tothoroughly understand how to reconcile these onlyseemingly conflicting insights, balancing them bothin theory and in practice.
A useful starting point maybe to incorporate frequency information in the pars-ing models directly ?
in particular, capturing therelationships between words of various frequencies.The polysemy effect appears smaller but is lesscontroversial: Our experiments suggest that the pri-mary drawback of the classic clustering schemesstems from their one class per word nature ?
andnot a lack of supervision, as may be widely believed.Monosemous groupings, even if they are themselvesderived from human-annotated syntactic categories,simply cannot disambiguate words the way gold tagscan.
By relaxing Clark?s (2000) flat clustering, us-ing contextual cues, we improved dependency gram-mar induction: directed accuracy on Section 23 (allsentences) of the WSJ benchmark increased from58.2% to 59.1% ?
from slightly worse to better thanwith gold tags (58.4%, previous state-of-the-art).Since Clark?s (2000) word clustering algorithm isalready context-sensitive in training, we suspect thatone could do better simply by preserving the polyse-mous nature of its internal representation.
Importingthe relevant distributions into a sequence tagger di-rectly would make more sense than going through anintermediate monosemous summary.
And exploringother uses of soft clustering algorithms ?
perhaps asinputs to part-of-speech disambiguators ?
may beanother fruitful research direction.
We believe thata joint treatment of grammar and parts-of-speech in-duction could fuel major advances in both tasks.AcknowledgmentsPartially funded by the Air Force Research Laboratory (AFRL),under prime contract no.
FA8750-09-C-0181, and by NSF, viaaward #IIS-0811974.
We thank Omri Abend, Spence Green,David McClosky and the anonymous reviewers for many help-ful comments on draft versions of this paper.ReferencesO.
Abend, R. Reichart, and A. Rappoport.
2010.
Im-proved unsupervised POS induction through prototypediscovery.
In ACL.H.
Alshawi, S. Bangalore, and S. Douglas.
2000.
Learn-ing dependency translation models as collections offinite-state head transducers.
Computational Linguis-tics, 26.H.
Alshawi, P.-C. Chang, and M. Ringgaard.
2011.
De-terministic statistical mapping of sentences to under-specied semantics.
In IWCS.H.
Alshawi.
1996.
Head automata for speech translation.In ICSLP.J.
K. Baker.
1979.
Trainable grammars for speech recog-nition.
In Speech Communication Papers for the 97thMeeting of the Acoustical Society of America.M.
Banko and R. C. Moore.
2004.
Part of speech taggingin context.
In COLING.L.
E. Baum.
1972.
An inequality and associated maxi-mization technique in statistical estimation for proba-bilistic functions of Markov processes.
In Inequalities.R.
Bod.
2006.
An all-subtrees approach to unsupervisedparsing.
In COLING-ACL.P.
F. Brown, V. J. Della Pietra, P. V. deSouza, J. C. Lai,and R. L. Mercer.
1992.
Class-based n-gram modelsof natural language.
Computational Linguistics, 18.G.
Carroll and E. Charniak.
1992.
Two experiments onlearning probabilistic dependency grammars from cor-pora.
Technical report, Brown University.D.
Chiang and D. M. Bikel.
2002.
Recovering latentinformation in treebanks.
In COLING.C.
Christodoulopoulos, S. Goldwater, and M. Steedman.2010.
Two decades of unsupervised POS induction:How far have we come?
In EMNLP.A.
Clark.
2000.
Inducing syntactic categories by contextdistribution clustering.
In CoNLL-LLL.M.
Collins.
1999.
Head-Driven Statistical Models forNatural Language Parsing.
Ph.D. thesis, Universityof Pennsylvania.B.
Cramer.
2007.
Limitations of current grammar induc-tion algorithms.
In ACL: Student Research.D.
Das and S. Petrov.
2011.
Unsupervised part-of-speech tagging with bilingual graph-based projections.In ACL.J.
R. Finkel and C. D. Manning.
2009.
Joint parsing andnamed entity recognition.
In NAACL-HLT.J.
R. Finkel, T. Grenager, and C. D. Manning.
2007.
Theinfinite tree.
In ACL.J.
Gao and M. Johnson.
2008.
A comparison of Bayesianestimators for unsupervised Hidden Markov ModelPOS taggers.
In EMNLP.1289W.
P. Headden, III, D. McClosky, and E. Charniak.2008.
Evaluating unsupervised part-of-speech taggingfor grammar induction.
In COLING.W.
P. Headden, III, M. Johnson, and D. McClosky.2009.
Improving unsupervised dependency parsingwith richer contexts and smoothing.
In NAACL-HLT.G.
Hinton and S. Roweis.
2003.
Stochastic neighborembedding.
In NIPS.R.
Johansson and P. Nugues.
2007.
Extendedconstituent-to-dependency conversion for English.
InNODALIDA.D.
Klein and C. D. Manning.
2004.
Corpus-based induc-tion of syntactic structure: Models of dependency andconstituency.
In ACL.D.
Klein.
2005.
The Unsupervised Learning of NaturalLanguage Structure.
Ph.D. thesis, Stanford Univer-sity.T.
Koo.
2010.
Advances in Discriminative DependencyParsing.
Ph.D. thesis, MIT.J.
Kupiec.
1992.
Robust part-of-speech tagging usinga hidden Markov model.
Computer Speech and Lan-guage, 6.D.
M. Magerman.
1995.
Statistical decision-tree modelsfor parsing.
In ACL.M.
P. Marcus, B. Santorini, and M. A. Marcinkiewicz.1993.
Building a large annotated corpus of English:The Penn Treebank.
Computational Linguistics, 19.R.
McDonald, S. Petrov, and K. Hall.
2011.
Multi-source transfer of delexicalized dependency parsers.In EMNLP.B.
Merialdo.
1994.
Tagging English text with a proba-bilistic model.
Computational Linguistics, 20.J.
Nivre, J.
Hall, S. Ku?bler, R. McDonald, J. Nils-son, S. Riedel, and D. Yuret.
2007.
The CoNLL2007 shared task on dependency parsing.
In EMNLP-CoNLL.M.
A. Paskin.
2001.
Grammatical bigrams.
In NIPS.F.
Pereira, N. Tishby, and L. Lee.
1993.
Distributionalclustering of English words.
In ACL.S.
Petrov, P.-C. Chang, M. Ringgaard, and H. Alshawi.2010.
Uptraining for accurate deterministic questionparsing.
In EMNLP.S.
Petrov, D. Das, and R. McDonald.
2011.
A universalpart-of-speech tagset.
In ArXiv.R.
Reichart and A. Rappoport.
2010.
Improved fully un-supervised parsing with zoomed learning.
In EMNLP.F.
Sangati and W. Zuidema.
2009.
Unsupervised meth-ods for head assignments.
In EACL.H.
Schu?tze.
1995.
Distributional part-of-speech tagging.In EACL.R.
Schwartz, O. Abend, R. Reichart, and A. Rappoport.2011.
Neutralizing linguistically problematic annota-tions in unsupervised dependency parsing evaluation.In ACL.Y.
Seginer.
2007a.
Fast unsupervised incremental pars-ing.
In ACL.Y.
Seginer.
2007b.
Learning Syntactic Structure.
Ph.D.thesis, University of Amsterdam.B.
Selman, H. A. Kautz, and B. Cohen.
1994.
Noisestrategies for improving local search.
In AAAI.V.
I. Spitkovsky, H. Alshawi, and D. Jurafsky.
2009.Baby Steps: How ?Less is More?
in unsupervised de-pendency parsing.
In NIPS: Grammar Induction, Rep-resentation of Language and Language Learning.V.
I. Spitkovsky, H. Alshawi, and D. Jurafsky.
2010a.From Baby Steps to Leapfrog: How ?Less is More?
inunsupervised dependency parsing.
In NAACL-HLT.V.
I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D. Man-ning.
2010b.
Viterbi training improves unsuperviseddependency parsing.
In CoNLL.V.
I. Spitkovsky, H. Alshawi, and D. Jurafsky.
2011.Punctuation: Making a point in unsupervised depen-dency parsing.
In CoNLL.H.
Yamada and Y. Matsumoto.
2003.
Statistical de-pendency analysis with support vector machines.
InIWPT.D.
Yuret.
1998.
Discovery of Linguistic Relations UsingLexical Attraction.
Ph.D. thesis, MIT.1290
