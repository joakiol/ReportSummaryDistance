Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 265?272,Sydney, July 2006. c?2006 Association for Computational LinguisticsLearning to Generate Naturalistic Utterances Using Reviews in SpokenDialogue SystemsRyuichiro HigashinakaNTT Corporationrh@cslab.kecl.ntt.co.jpRashmi PrasadUniversity of Pennsylvaniarjprasad@linc.cis.upenn.eduMarilyn A. WalkerUniversity of Sheffieldwalker@dcs.shef.ac.ukAbstractSpoken language generation for dialoguesystems requires a dictionary of mappingsbetween semantic representations of con-cepts the system wants to express and re-alizations of those concepts.
Dictionarycreation is a costly process; it is currentlydone by hand for each dialogue domain.We propose a novel unsupervised methodfor learning such mappings from user re-views in the target domain, and test it onrestaurant reviews.
We test the hypothesisthat user reviews that provide individualratings for distinguished attributes of thedomain entity make it possible to map re-view sentences to their semantic represen-tation with high precision.
Experimentalanalyses show that the mappings learnedcover most of the domain ontology, andprovide good linguistic variation.
A sub-jective user evaluation shows that the con-sistency between the semantic representa-tions and the learned realizations is highand that the naturalness of the realizationsis higher than a hand-crafted baseline.1 IntroductionOne obstacle to the widespread deployment ofspoken dialogue systems is the cost involvedwith hand-crafting the spoken language generationmodule.
Spoken language generation requires adictionary of mappings between semantic repre-sentations of concepts the system wants to expressand realizations of those concepts.
Dictionary cre-ation is a costly process: an automatic methodfor creating them would make dialogue technol-ogy more scalable.
A secondary benefit is that alearned dictionary may produce more natural andcolloquial utterances.We propose a novel method for mining user re-views to automatically acquire a domain specificgeneration dictionary for information presentationin a dialogue system.
Our hypothesis is that re-views that provide individual ratings for variousdistinguished attributes of review entities can beused to map review sentences to a semantic rep-An example user review (we8there.com)Ratings Food=5, Service=5, Atmosphere=5,Value=5, Overall=5ReviewcommentThe best Spanish food in New York.
I amfrom Spain and I had my 28th birthdaythere and we all had a great time.
Salud!
?Review comment after named entity recognitionThe best {NE=foodtype, string=Spanish} {NE=food,string=food, rating=5} in {NE=location, string=NewYork}.
.
.
.
?Mapping between a semantic representation (a set ofrelations) and a syntactic structure (DSyntS)?
Relations:RESTAURANT has FOODTYPERESTAURANT has foodquality=5RESTAURANT has LOCATION([foodtype, food=5, location] for shorthand.)?
DSyntS:???????????????????
?lexeme : foodclass : common nounnumber : sgarticle : defATTR[lexeme : bestclass : adjective]ATTR?
?lexeme : FOODTYPEclass : common nounnumber : sgarticle : no-art??ATTR????
?lexeme : inclass : prepositionII?
?lexeme : LOCATIONclass : proper nounnumber : sgarticle : no-art??????????????????????????
?Figure 1: Example of procedure for acquiring ageneration dictionary mapping.resentation.
Figure 1 shows a user review in therestaurant domain, where we hypothesize that theuser rating food=5 indicates that the semantic rep-resentation for the sentence ?The best Spanishfood in New York?
includes the relation ?RESTAU-RANT has foodquality=5.
?We apply the method to extract 451 mappingsfrom restaurant reviews.
Experimental analysesshow that the mappings learned cover most of thedomain ontology, and provide good linguistic vari-ation.
A subjective user evaluation indicates thatthe consistency between the semantic representa-tions and the learned realizations is high and thatthe naturalness of the realizations is significantlyhigher than a hand-crafted baseline.265Section 2 provides a step-by-step description ofthe method.
Sections 3 and 4 present the evalua-tion results.
Section 5 covers related work.
Sec-tion 6 summarizes and discusses future work.2 Learning a Generation DictionaryOur automatically created generation dictionaryconsists of triples (U ,R,S) representing a map-ping between the original utterance U in the userreview, its semantic representation R(U), and itssyntactic structure S(U).
Although templates arewidely used in many practical systems (Seneff andPolifroni, 2000; Theune, 2003), we derive syn-tactic structures to represent the potential realiza-tions, in order to allow aggregation, and othersyntactic transformations of utterances, as well ascontext specific prosody assignment (Walker et al,2003; Moore et al, 2004).The method is outlined briefly in Fig.
1 and de-scribed below.
It comprises the following steps:1.
Collect user reviews on the web to create apopulation of utterances U .2.
To derive semantic representations R(U):?
Identify distinguished attributes andconstruct a domain ontology;?
Specify lexicalizations of attributes;?
Scrape webpages?
structured data fornamed-entities;?
Tag named-entities.3.
Derive syntactic representations S(U).4.
Filter inappropriate mappings.5.
Add mappings (U ,R,S) to dictionary.2.1 Creating the corpusWe created a corpus of restaurant reviews byscraping 3,004 user reviews of 1,810 restau-rants posted at we8there.com (http://www.we8-there.com/), where each individual review in-cludes a 1-to-5 Likert-scale rating of differentrestaurant attributes.
The corpus consists of18,466 sentences.2.2 Deriving semantic representationsThe distinguished attributes are extracted from thewebpages for each restaurant entity.
They in-clude attributes that the users are asked to rate,i.e.
food, service, atmosphere, value, and over-all, which have scalar values.
In addition, otherattributes are extracted from the webpage, suchas the name, foodtype and location of the restau-rant, which have categorical values.
The nameattribute is assumed to correspond to the restau-rant entity.
Given the distinguished attributes, aDist.
Attr.
Lexicalizationfood food, mealservice service, staff, waitstaff, wait staff, server,waiter, waitressatmosphere atmosphere, decor, ambience, decorationvalue value, price, overprice, pricey, expensive,inexpensive, cheap, affordable, affordoverall recommend, place, experience, establish-mentTable 1: Lexicalizations for distinguished at-tributes.simple domain ontology can be automatically de-rived by assuming that a meronymy relation, rep-resented by the predicate ?has?, holds between theentity type (RESTAURANT) and the distinguishedattributes.
Thus, the domain ontology consists ofthe relations:??????????????
?RESTAURANT has foodqualityRESTAURANT has servicequalityRESTAURANT has valuequalityRESTAURANT has atmospherequalityRESTAURANT has overallqualityRESTAURANT has foodtypeRESTAURANT has locationWe assume that, although users may discussother attributes of the entity, at least some of theutterances in the reviews realize the relations spec-ified in the ontology.
Our problem then is to iden-tify these utterances.
We test the hypothesis that,if an utterance U contains named-entities corre-sponding to the distinguished attributes, thatR forthat utterance includes the relation concerning thatattribute in the domain ontology.We define named-entities for lexicalizations ofthe distinguished attributes, starting with the seedword for that attribute on the webpage (Table 1).1For named-entity recognition, we use GATE (Cun-ningham et al, 2002), augmented with named-entity lists for locations, food types, restaurantnames, and food subtypes (e.g.
pizza), scrapedfrom the we8there webpages.We also hypothesize that the rating given for thedistinguished attribute specifies the scalar valueof the relation.
For example, a sentence contain-ing food or meal is assumed to realize the re-lation ?RESTAURANT has foodquality.
?, and thevalue of the foodquality attribute is assumed to bethe value specified in the user rating for that at-tribute, e.g.
?RESTAURANT has foodquality = 5?
inFig.
1.
Similarly, the other relations in Fig.
1 areassumed to be realized by the utterance ?The bestSpanish food in New York?
because it contains1In future, we will investigate other techniques for boot-strapping these lexicalizations from the seed word on thewebpage.266filter filtered retainedNo Relations Filter 7,947 10,519Other Relations Filter 5,351 5,168Contextual Filter 2,973 2,195Unknown Words Filter 1,467 728Parsing Filter 216 512Table 2: Filtering statistics: the number of sen-tences filtered and retained by each filter.one FOODTYPE named-entity and one LOCATIONnamed-entity.
Values of categorical attributes arereplaced by variables representing their type be-fore the learned mappings are added to the dictio-nary, as shown in Fig.
1.2.3 Parsing and DSyntS conversionWe adopt Deep Syntactic Structures (DSyntSs) asa format for syntactic structures because they canbe realized by the fast portable realizer RealPro(Lavoie and Rambow, 1997).
Since DSyntSs are atype of dependency structure, we first process thesentences with Minipar (Lin, 1998), and then con-vert Minipar?s representation into DSyntS.
Sinceuser reviews are different from the newspaper ar-ticles on which Minipar was trained, the outputof Minipar can be inaccurate, leading to failure inconversion.
We check whether conversion is suc-cessful in the filtering stage.2.4 FilteringThe goal of filtering is to identify U that realizethe distinguished attributes and to guarantee highprecision for the learned mappings.
Recall is lessimportant since systems need to convey requestedinformation as accurately as possible.
Our proce-dure for deriving semantic representations is basedon the hypothesis that if U contains named-entitiesthat realize the distinguished attributes, thatRwillinclude the relevant relation in the domain ontol-ogy.
We also assume that if U contains named-entities that are not covered by the domain ontol-ogy, or words indicating that the meaning of U de-pends on the surrounding context, that R will notcompletely characterizes the meaning of U , and soU should be eliminated.
We also require an accu-rate S for U .
Therefore, the filters described be-low eliminate U that (1) realize semantic relationsnot in the ontology; (2) contain words indicatingthat its meaning depends on the context; (3) con-tain unknown words; or (4) cannot be parsed ac-curately.No Relations Filter: The sentence does not con-tain any named-entities for the distinguishedattributes.Other Relations Filter: The sentence containsnamed-entities for food subtypes, personRatingDist.Attr.1 2 3 4 5 Totalfood 5 8 6 18 57 94service 15 3 6 17 56 97atmosphere 0 3 3 8 31 45value 0 0 1 8 12 21overall 3 2 5 15 45 70Total 23 15 21 64 201 327Table 3: Domain coverage of single scalar-valuedrelation mappings.names, country names, dates (e.g., today, to-morrow, Aug. 26th) or prices (e.g., 12 dol-lars), or POS tag CD for numerals.
These in-dicate relations not in the ontology.Contextual Filter: The sentence contains index-icals such as I, you, that or cohesive markersof rhetorical relations that connect it to somepart of the preceding text, which means thatthe sentence cannot be interpreted out of con-text.
These include discourse markers, suchas list item markers with LS as the POS tag,that signal the organization structure of thetext (Hirschberg and Litman, 1987), as wellas discourse connectives that signal semanticand pragmatic relations of the sentence withother parts of the text (Knott, 1996), such ascoordinating conjunctions at the beginning ofthe utterance like and and but etc., and con-junct adverbs such as however, also, then.Unknown Words Filter: The sentence containswords not in WordNet (Fellbaum, 1998)(which includes typographical errors), orPOS tags contain NN (Noun), which may in-dicate an unknown named-entity, or the sen-tence has more than a fixed length of words,2indicating that its meaning may not be esti-mated solely by named entities.Parsing Filter: The sentence fails the parsing toDSyntS conversion.
Failures are automati-cally detected by comparing the original sen-tence with the one realized by RealPro takingthe converted DSyntS as an input.We apply the filters, in a cascading manner, to the18,466 sentences with semantic representations.As a result, we obtain 512 (2.8%) mappings of(U ,R,S).
After removing 61 duplicates, 451 dis-tinct (2.4%) mappings remain.
Table 2 shows thenumber of sentences eliminated by each filter.3 Objective EvaluationWe evaluate the learned expressions with respectto domain coverage, linguistic variation and gen-erativity.2We used 20 as a threshold.267# Combination of Dist.
Attrs Count1 food-service 392 food-value 213 atmosphere-food 144 atmosphere-service 105 atmosphere-food-service 76 food-foodtype 47 atmosphere-food-value 48 location-overall 39 food-foodtype-value 310 food-service-value 211 food-foodtype-location 212 food-overall 213 atmosphere-foodtype 214 atmosphere-overall 215 service-value 116 overall-service 117 overall-value 118 foodtype-overall 119 food-foodtype-location-overall 120 atmosphere-food-service-value 121 atmosphere-food-overall-service-value1Total 122Table 4: Counts for multi-relation mappings.3.1 Domain CoverageTo be usable for a dialogue system, the mappingsmust have good domain coverage.
Table 3 showsthe distribution of the 327 mappings realizing asingle scalar-valued relation, categorized by theassociated rating score.3 For example, there are 57mappings with R of ?RESTAURANT has foodqual-ity=5,?
and a large number of mappings for boththe foodquality and servicequality relations.
Al-though we could not obtain mappings for some re-lations such as price={1,2}, coverage for express-ing a single relation is fairly complete.There are also mappings that express several re-lations.
Table 4 shows the counts of mappingsfor multi-relation mappings, with those contain-ing a food or service relation occurring more fre-quently as in the single scalar-valued relation map-pings.
We found only 21 combinations of rela-tions, which is surprising given the large poten-tial number of combinations (There are 50 com-binations if we treat relations with different scalarvalues differently).
We also find that most of themappings have two or three relations, perhaps sug-gesting that system utterances should not expresstoo many relations in a single sentence.3.2 Linguistic VariationWe also wish to assess whether the linguisticvariation of the learned mappings was greaterthan what we could easily have generated with ahand-crafted dictionary, or a hand-crafted dictio-nary augmented with aggregation operators, as in3There are two other single-relation but not scalar-valuedmappings that concern LOCATION in our mappings.
(Walker et al, 2003).
Thus, we first categorizedthe mappings by the patterns of the DSyntSs.
Ta-ble 5 shows the most common syntactic patterns(more than 10 occurrences), indicating that 30%of the learned patterns consist of the simple form?X is ADJ?
where ADJ is an adjective, or ?X is RBADJ,?
where RB is a degree modifier.
Furthermore,up to 55% of the learned mappings could be gen-erated from these basic patterns by the applicationof a combination operator that coordinates mul-tiple adjectives, or coordinates predications overdistinct attributes.
However, there are 137 syntac-tic patterns in all, 97 with unique syntactic struc-tures and 21 with two occurrences, accounting for45% of the learned mappings.
Table 6 shows ex-amples of learned mappings with distinct syntacticstructures.
It would be surprising to see this typeof variety in a hand-crafted generation dictionary.In addition, the learned mappings contain 275 dis-tinct lexemes, with a minimum of 2, maximum of15, and mean of 4.63 lexemes per DSyntS, indi-cating that the method extracts a wide variety ofexpressions of varying lengths.Another interesting aspect of the learned map-pings is the wide variety of adjectival phrases(APs) in the common patterns.
Tables 7 and 8show the APs in single scalar-valued relation map-pings for food and service categorized by the as-sociated ratings.
Tables for atmosphere, value andoverall can be found in the Appendix.
Moreover,the meanings for some of the learned APs are veryspecific to the particular attribute, e.g.
cold andburnt associated with foodquality of 1, attentiveand prompt for servicequality of 5, silly and inat-tentive for servicequality of 1. and mellow for at-mosphere of 5.
In addition, our method places theadjectival phrases (APs) in the common patternson a more fine-grained scale of 1 to 5, similar tothe strength classifications in (Wilson et al, 2004),in contrast to other automatic methods that clas-sify expressions into a binary positive or negativepolarity (e.g.
(Turney, 2002)).3.3 GenerativityOur motivation for deriving syntactic representa-tions for the learned expressions was the possibil-ity of using an off-the-shelf sentence planner toderive new combinations of relations, and applyaggregation and other syntactic transformations.We examined how many of the learned DSyntSscan be combined with each other, by taking ev-ery pair of DSyntSs in the mappings and apply-ing the built-in merge operation in the SPaRKygenerator (Walker et al, 2003).
We found thatonly 306 combinations out of a potential 81,318268# syntactic pattern example utterance count ratio accum.1 NN VB JJ The atmosphere is wonderful.
92 20.4% 20.4%2 NN VB RB JJ The atmosphere was very nice.
52 11.5% 31.9%3 JJ NN Bad service.
36 8.0% 39.9%4 NN VB JJ CC JJ The food was flavorful but cold.
25 5.5% 45.5%5 RB JJ NN Very trendy ambience.
22 4.9% 50.3%6 NN VB JJ CC NN VB JJ The food is excellent and the atmosphere is great.
13 2.9% 53.2%7 NN CC NN VB JJ The food and service were fantastic.
10 2.2% 55.4%Table 5: Common syntactic patterns of DSyntSs, flattened to a POS sequence for readability.
NN, VB,JJ, RB, CC stand for noun, verb, adjective, adverb, and conjunction, respectively.
[overall=1, value=2] Very disappointing experience forthe money charged.
[food=5, value=5] The food is excellent and plentiful at areasonable price.
[food=5, service=5] The food is exquisite as well as theservice and setting.
[food=5, service=5] The food was spectacular and so wasthe service.
[food=5, foodtype, value=5] Best FOODTYPE food witha great value for money.
[food=5, foodtype, value=5] An absolutely outstandingvalue with fantastic FOODTYPE food.
[food=5, foodtype, location, overall=5] This is the bestplace to eat FOODTYPE food in LOCATION.
[food=5, foodtype] Simply amazing FOODTYPE food.
[food=5, foodtype] RESTAURANTNAME is the best of thebest for FOODTYPE food.
[food=5] The food is to die for.
[food=5] What incredible food.
[food=4] Very pleasantly surprised by the food.
[food=1] The food has gone downhill.
[atmosphere=5, overall=5] This is a quiet little placewith great atmosphere.
[atmosphere=5, food=5, overall=5, service=5, value=5]The food, service and ambience of the place are all fabu-lous and the prices are downright cheap.Table 6: Acquired generation patterns (with short-hand for relations in square brackets) whose syn-tactic patterns occurred only once.combinations (0.37%) were successful.
This isbecause the merge operation in SPaRKy requiresthat the subjects and the verbs of the two DSyntSsare identical, e.g.
the subject is RESTAURANT andverb is has, whereas the learned DSyntSs oftenplace the attribute in subject position as a definitenoun phrase.
However, the learned DSyntS canbe incorporated into SPaRKy using the semanticrepresentations to substitute learned DSyntSs intonodes in the sentence plan tree.
Figure 2 showssome example utterances generated by SPaRKywith its original dictionary and example utteranceswhen the learned mappings are incorporated.
Theresulting utterances seem more natural and collo-quial; we examine whether this is true in the nextsection.4 Subjective EvaluationWe evaluate the obtained mappings in two re-spects: the consistency between the automaticallyderived semantic representation and the realiza-food=1 awful, bad, burnt, cold, very ordinaryfood=2 acceptable, bad, flavored, not enough, verybland, very goodfood=3 adequate, bland and mediocre, flavorful butcold, pretty good, rather bland, very goodfood=4 absolutely wonderful, awesome, decent, ex-cellent, good, good and generous, great, out-standing, rather good, really good, tradi-tional, very fresh and tasty, very good, veryvery goodfood=5 absolutely delicious, absolutely fantastic, ab-solutely great, absolutely terrific, ample, wellseasoned and hot, awesome, best, delectableand plentiful, delicious, delicious but simple,excellent, exquisite, fabulous, fancy but tasty,fantastic, fresh, good, great, hot, incredible,just fantastic, large and satisfying, outstand-ing, plentiful and outstanding, plentiful andtasty, quick and hot, simply great, so deli-cious, so very tasty, superb, terrific, tremen-dous, very good, wonderfulTable 7: Adjectival phrases (APs) in single scalar-valued relation mappings for foodquality.tion, and the naturalness of the realization.For comparison, we used a baseline of hand-crafted mappings from (Walker et al, 2003) ex-cept that we changed the word decor to at-mosphere and added five mappings for overall.For scalar relations, this consists of the realiza-tion ?RESTAURANT has ADJ LEX?
where ADJ ismediocre, decent, good, very good, or excellent forrating values 1-5, and LEX is food quality, service,atmosphere, value, or overall depending on the re-lation.
RESTAURANT is filled with the name ofa restaurant at runtime.
For example, ?RESTAU-RANT has foodquality=1?
is realized as ?RESTAU-RANT has mediocre food quality.?
The locationand food type relations are mapped to ?RESTAU-RANT is located in LOCATION?
and ?RESTAU-RANT is a FOODTYPE restaurant.
?The learned mappings include 23 distinct se-mantic representations for a single-relation (22 forscalar-valued relations and one for location) and50 for multi-relations.
Therefore, using the hand-crafted mappings, we first created 23 utterancesfor the single-relations.
We then created three ut-terances for each of 50multi-relations using differ-ent clause-combining operations from (Walker etal., 2003).
This gave a total of 173 baseline utter-ances, which together with 451 learned mappings,269service=1 awful, bad, great, horrendous, horrible,inattentive, forgetful and slow, marginal,really slow, silly and inattentive, stillmarginal, terrible, youngservice=2 overly slow, very slow and inattentiveservice=3 bad, bland and mediocre, friendly andknowledgeable, good, pleasant, prompt,very friendlyservice=4 all very warm and welcoming, attentive,extremely friendly and good, extremelypleasant, fantastic, friendly, friendly andhelpful, good, great, great and courteous,prompt and friendly, really friendly, sonice, swift and friendly, very friendly, veryfriendly and accommodatingservice=5 all courteous, excellent, excellent andfriendly, extremely friendly, fabulous,fantastic, friendly, friendly and helpful,friendly and very attentive, good, great,great, prompt and courteous, happy andfriendly, impeccable, intrusive, legendary,outstanding, pleasant, polite, attentive andprompt, prompt and courteous, promptand pleasant, quick and cheerful, stupen-dous, superb, the most attentive, unbeliev-able, very attentive, very congenial, verycourteous, very friendly, very friendly andhelpful, very friendly and pleasant, veryfriendly and totally personal, very friendlyand welcoming, very good, very helpful,very timely, warm and friendly, wonderfulTable 8: Adjectival phrases (APs) in single scalar-valued relation mappings for servicequality.yielded 624 utterances for evaluation.Ten subjects, all native English speakers, eval-uated the mappings by reading them from a web-page.
For each system utterance, the subjects wereasked to express their degree of agreement, on ascale of 1 (lowest) to 5 (highest), with the state-ment (a) The meaning of the utterance is consis-tent with the ratings expressing their semantics,and with the statement (b) The style of the utter-ance is very natural and colloquial.
They wereasked not to correct their decisions and also to rateeach utterance on its own merit.4.1 ResultsTable 9 shows the means and standard deviationsof the scores for baseline vs. learned utterances forconsistency and naturalness.
A t-test shows thatthe consistency of the learned expression is signifi-cantly lower than the baseline (df=4712, p < .001)but that their naturalness is significantly higherthan the baseline (df=3107, p < .001).
However,consistency is still high.
Only 14 of the learnedutterances (shown in Tab.
10) have a mean consis-tency score lower than 3, which indicates that, byand large, the human judges felt that the inferredsemantic representations were consistent with themeaning of the learned expressions.
The correla-tion coefficient between consistency and natural-ness scores is 0.42, which indicates that consis-Original SPaRKy utterances?
Babbo has the best overall quality among the selectedrestaurants with excellent decor, excellent service andsuperb food quality.?
Babbo has excellent decor and superb food qualitywith excellent service.
It has the best overall qualityamong the selected restaurants.
?Combination of SPaRKy and learned DSyntS?
Because the food is excellent, the wait staff is pro-fessional and the decor is beautiful and very com-fortable, Babbo has the best overall quality among theselected restaurants.?
Babbo has the best overall quality among the selectedrestaurants because atmosphere is exceptionally nice,food is excellent and the service is superb.?
Babbo has superb food quality, the service is excep-tional and the atmosphere is very creative.
It has thebest overall quality among the selected restaurants.Figure 2: Utterances incorporating learnedDSyntSs (Bold font) in SPaRKy.baseline learned stat.mean sd.
mean sd.
sig.Consistency 4.714 0.588 4.459 0.890 +Naturalness 4.227 0.852 4.613 0.844 +Table 9: Consistency and naturalness scores aver-aged over 10 subjects.tency does not greatly relate to naturalness.We also performed an ANOVA (ANalysis OfVAriance) of the effect of each relation in R onnaturalness and consistency.
There were no sig-nificant effects except that mappings combiningfood, service, and atmosphere were significantlyworse (df=1, F=7.79, p=0.005).
However, thereis a trend for mappings to be rated higher forthe food attribute (df=1, F=3.14, p=0.08) and thevalue attribute (df=1, F=3.55, p=0.06) for consis-tency, suggesting that perhaps it is easier to learnsome mappings than others.5 Related WorkAutomatically finding sentences with the samemeaning has been extensively studied in the fieldof automatic paraphrasing using parallel corporaand corpora with multiple descriptions of the sameevents (Barzilay and McKeown, 2001; Barzilayand Lee, 2003).
Other work finds predicates ofsimilar meanings by using the similarity of con-texts around the predicates (Lin and Pantel, 2001).However, these studies find a set of sentences withthe same meaning, but do not associate a specificmeaning with the sentences.
One exception is(Barzilay and Lee, 2002), which derives mappingsbetween semantic representations and realizationsusing a parallel (but unaligned) corpus consistingof both complex semantic input and correspond-ing natural language verbalizations for mathemat-270shorthand for relations and utterance score[food=4] The food is delicious and beautifullyprepared.2.9[overall=4] A wonderful experience.
2.9[service=3] The service is bland and mediocre.
2.8[atmosphere=2] The atmosphere here is eclec-tic.2.6[overall=3] Really fancy place.
2.6[food=3, service=4] Wonderful service andgreat food.2.5[service=4] The service is fantastic.
2.5[overall=2] The RESTAURANTNAME is once agreat place to go and socialize.2.2[atmosphere=2] The atmosphere is unique andpleasant.2.0[food=5, foodtype] FOODTYPE and FOODTYPEfood.1.8[service=3] Waitstaff is friendly and knowl-edgeable.1.7[atmosphere=5, food=5, service=5] The atmo-sphere, food and service.1.6[overall=3] Overall, a great experience.
1.4[service=1] The waiter is great.
1.4Table 10: The 14 utterances with consistencyscores below 3.ical proofs.
However, our technique does not re-quire parallel corpora or previously existing se-mantic transcripts or labeling, and user reviews arewidely available in many different domains (Seehttp://www.epinions.com/).There is also significant previous work on min-ing user reviews.
For example, Hu and Liu (2005)use reviews to find adjectives to describe products,and Popescu and Etzioni (2005) automatically findfeatures of a product together with the polarity ofadjectives used to describe them.
They both aim atsummarizing reviews so that users can make deci-sions easily.
Our method is also capable of findingpolarities of modifying expressions including ad-jectives, but on a more fine-grained scale of 1 to5.
However, it might be possible to use their ap-proach to create rating information for raw reviewtexts as in (Pang and Lee, 2005), so that we cancreate mappings from reviews without ratings.6 Summary and Future WorkWe proposed automatically obtaining mappingsbetween semantic representations and realizationsfrom reviews with individual ratings.
The resultsshow that: (1) the learned mappings provide goodcoverage of the domain ontology and exhibit goodlinguistic variation; (2) the consistency betweenthe semantic representations and realizations ishigh; and (3) the naturalness of the realizations aresignificantly higher than the baseline.There are also limitations in our method.
Eventhough consistency is rated highly by human sub-jects, this may actually be a judgement of whetherthe polarity of the learned mapping is correctlyplaced on the 1 to 5 rating scale.
Thus, alter-nate ways of expressing, for example foodqual-ity=5, shown in Table 7, cannot be guaranteed tobe synonymous, which may be required for use inspoken language generation.
Rather, an examina-tion of the adjectival phrases in Table 7 shows thatdifferent aspects of the food are discussed.
Forexample ample and plentiful refer to the portionsize, fancy may refer to the presentation, and deli-cious describes the flavors.
This suggests that per-haps the ontology would benefit from represent-ing these sub-attributes of the food attribute, andsub-attributes in general.
Another problem withconsistency is that the same AP, e.g.
very goodin Table 7 may appear with multiple ratings.
Forexample, very good is used for every foodqualityrating from 2 to 5.
Thus some further automaticor by-hand analysis is required to refine what islearned before actual use in spoken language gen-eration.
Still, our method could reduce the amountof time a system designer spends developing thespoken language generator, and increase the natu-ralness of spoken language generation.Another issue is that the recall appears to bequite low given that all of the sentences concernthe same domain: only 2.4% of the sentencescould be used to create the mappings.
One wayto increase recall might be to automatically aug-ment the list of distinguished attribute lexicaliza-tions, using WordNet or work on automatic iden-tification of synonyms, such as (Lin and Pantel,2001).
However, the method here has high pre-cision, and automatic techniques may introducenoise.
A related issue is that the filters are in somecases too strict.
For example the contextual fil-ter is based on POS-tags, so that sentences that donot require the prior context for their interpreta-tion are eliminated, such as sentences containingsubordinating conjunctions like because, when, if,whose arguments are both given in the same sen-tence (Prasad et al, 2005).
In addition, recall isaffected by the domain ontology, and the automat-ically constructed domain ontology from the re-view webpages may not cover all of the domain.In some review domains, the attributes that getindividual ratings are a limited subset of the do-main ontology.
Techniques for automatic featureidentification (Hu and Liu, 2005; Popescu and Et-zioni, 2005) could possibly help here, althoughthese techniques currently have the limitation thatthey do not automatically identify different lexi-calizations of the same feature.A different type of limitation is that dialoguesystems need to generate utterances for informa-tion gathering whereas the mappings we obtained271can only be used for information presentation.Thus these would have to be constructed by hand,as in current practice, or perhaps other types ofcorpora or resources could be utilized.
In addi-tion, the utility of syntactic structures in the map-pings should be further examined, especially giventhe failures in DSyntS conversion.
An alternativewould be to leave some sentences unparsed anduse them as templates with hybrid generation tech-niques (White and Caldwell, 1998).
Finally, whilewe believe that this technique will apply across do-mains, it would be useful to test it on domains suchas movie reviews or product reviews, which havemore complex domain ontologies.AcknowledgmentsWe thank the anonymous reviewers for their help-ful comments.
This work was supported by aRoyal Society Wolfson award to Marilyn Walkerand a research collaboration grant from NTT tothe Cognitive Systems Group at the University ofSheffield.ReferencesRegina Barzilay and Lillian Lee.
2002.
Bootstrapping lex-ical choice via multiple-sequence alignment.
In Proc.EMNLP, pages 164?171.Regina Barzilay and Lillian Lee.
2003.
Learning toparaphrase: An unsupervised approach using multiple-sequence alignment.
In Proc.
HLT/NAACL, pages 16?23.Regina Barzilay and Kathleen McKeown.
2001.
Extractingparaphrases from a parallel corpus.
In Proc.
39th ACL,pages 50?57.Hamish Cunningham, Diana Maynard, Kalina Bontcheva,and Valentin Tablan.
2002.
GATE: A framework andgraphical development environment for robust NLP toolsand applications.
In Proc.
40th ACL.Christiane Fellbaum.
1998.
WordNet: An Electronic LexicalDatabase (Language, Speech, and Communication).
TheMIT Press.Julia Hirschberg and Diane.
J. Litman.
1987.
Now let?s talkabout NOW: Identifying cue phrases intonationally.
InProc.
25th ACL, pages 163?171.Minqing Hu and Bing Liu.
2005.
Mining and summarizingcustomer reviews.
In Proc.
KDD, pages 168?177.Alistair Knott.
1996.
A Data-Driven Methodology for Moti-vating a Set of Coherence Relations.
Ph.D. thesis, Univer-sity of Edinburgh, Edinburgh.Benoit Lavoie and Owen Rambow.
1997.
A fast and portablerealizer for text generation systems.
In Proc.
5th AppliedNLP, pages 265?268.Dekang Lin and Patrick Pantel.
2001.
Discovery of infer-ence rules for question answering.
Natural Language En-gineering, 7(4):343?360.Dekang Lin.
1998.
Dependency-based evaluation of MINI-PAR.
In Workshop on the Evaluation of Parsing Systems.Johanna D. Moore, Mary Ellen Foster, Oliver Lemon, andMichael White.
2004.
Generating tailored, comparativedescriptions in spoken dialogue.
In Proc.
7th FLAIR.Bo Pang and Lillian Lee.
2005.
Seeing stars: Exploitingclass relationships for sentiment categorization with re-spect to rating scales.
In Proc.
43st ACL, pages 115?124.Ana-Maria Popescu and Oren Etzioni.
2005.
Extractingproduct features and opinions from reviews.
In Proc.HLT/EMNLP, pages 339?346.Rashmi Prasad, Aravind Joshi, Nikhil Dinesh, Alan Lee,Eleni Miltsakaki, and Bonnie Webber.
2005.
The PennDiscourse TreeBank as a resource for natural languagegeneration.
In Proc.
Corpus Linguistics Workshop on Us-ing Corpora for NLG.Stephanie Seneff and Joseph Polifroni.
2000.
Formal andnatural language generation in the mercury conversationalsystem.
In Proc.
ICSLP, volume 2, pages 767?770.Marie?t Theune.
2003.
From monologue to dialogue: naturallanguage generation in OVIS.
In AAAI 2003 Spring Sym-posium on Natural Language Generation in Written andSpoken Dialogue, pages 141?150.Peter D. Turney.
2002.
Thumbs up or thumbs down?
se-mantic orientation applied to unsupervised classificationof reviews.
In Proc.
40th ACL, pages 417?424.Marilyn Walker, Rashmi Prasad, and Amanda Stent.
2003.A trainable generator for recommendations in multimodaldialog.
In Proc.
Eurospeech, pages 1697?1700.Michael White and Ted Caldwell.
1998.
EXEMPLARS: Apractical, extensible framework for dynamic text genera-tion.
In Proc.
INLG, pages 266?275.Theresa Wilson, Janyce Wiebe, and Rebecca Hwa.
2004.Just how mad are you?
finding strong and weak opinionclauses.
In Proc.
AAAI, pages 761?769.AppendixAdjectival phrases (APs) in single scalar-valuedrelation mappings for atmosphere, value, andoverall.atmosphere=2 eclectic, unique and pleasantatmosphere=3 busy, pleasant but extremely hotatmosphere=4 fantastic, great, quite nice and simple,typical, very casual, very trendy, wonder-fulatmosphere=5 beautiful, comfortable, excellent, great,interior, lovely, mellow, nice, nice andcomfortable, phenomenal, pleasant, quitepleasant, unbelievably beautiful, verycomfortable, very cozy, very friendly,very intimate, very nice, very nice andrelaxing, very pleasant, very relaxing,warm and contemporary, warm and verycomfortable, wonderfulvalue=3 very reasonablevalue=4 great, pretty good, reasonable, very goodvalue=5 best, extremely reasonable, good, great,reasonable, totally reasonable, very good,very reasonableoverall=1 just bad, nice, thoroughly humiliatingoverall=2 great, really badoverall=3 bad, decent, great, interesting, reallyfancyoverall=4 excellent, good, great, just great, neverbusy, not very busy, outstanding, recom-mended, wonderfuloverall=5 amazing, awesome, capacious, delight-ful, extremely pleasant, fantastic, good,great, local, marvelous, neat, new, over-all, overwhelmingly pleasant, pampering,peaceful but idyllic, really cool, reallygreat, really neat, really nice, special,tasty, truly great, ultimate, unique and en-joyable, very enjoyable, very excellent,very good, very nice, very wonderful,warm and friendly, wonderful272
