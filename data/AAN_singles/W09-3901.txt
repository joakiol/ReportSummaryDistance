Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 1?10,Queen Mary University of London, September 2009. c?2009 Association for Computational LinguisticsEvaluating the Effectiveness of Information Presentationin a Full End-To-End Dialogue SystemTaghi PaksimaEnterprise Search GroupMicrosoftD-81669 Munich, Germanytaghi.paksima@microsoft.comKallirroi GeorgilaInstitute for Creative TechnologiesUniversity of Southern CaliforniaMarina del Rey, CA 90292, USAkgeorgila@ict.usc.eduJohanna D. MooreSchool of InformaticsUniversity of EdinburghEdinburgh, EH8 9AB, UKj.moore@ed.ac.ukAbstractRecent work on information presenta-tion in dialogue systems combines usermodelling (UM) and stepwise refine-ment through clustering and summarisa-tion (SR) in the UMSR approach.
An eval-uation in which participants rated dialoguetranscripts showed that UMSR presentscomplex trade-offs understandably, pro-vides users with a good overview of theiroptions, and increases users?
confidencethat all relevant options have been pre-sented (Demberg and Moore, 2006).
Inthis paper, we evaluate the effectivenessof the UMSR approach in a more realis-tic setting, by incorporating this informa-tion presentation technique into a full end-to-end dialogue system in the city infor-mation domain, and comparing it with thetraditional approach of presenting infor-mation sequentially.
Our results suggestthat despite complications associated witha real dialogue system setting, the UMSRmodel retains its advantages.1 IntroductionSpoken dialogue systems (SDS) that help usersfind a desired option (e.g., flight, restaurant,movie) from the set of options satisfying their con-straints typically present options sequentially, or-dered along a default dimension (e.g., by price ordeparture time).
An example is shown in Fig.
1.The user can then navigate through the optionsand refine them by offering new constraints untila suitable option has been found.
However, whenthe number of available options is large, this pro-cess can be painstaking, leading to long dialoguesThere are six restaurant options matching your query.Number 1: Voujon offers a bright, airy and contempo-rary dining area, with simple floral displays and leatherseating.
It serves Indian cuisine.
It is located in the citycentre.
The average price is ?24 per person.Number 2: Saffrani?s decor is modern, the dining roomwee, though the menu is enormous, and the atmospherecharming.
It offers new Indian dishes never before seenin Edinburgh.
It serves Indian, seafood cuisine.
It islocated in the city centre.
The average price is ?28 perperson.Number 3: Britannia Spice .
.
.Figure 1: Example of sequential information pre-sentation in the city information domain (modi-fied version of the TownInfo system (Lemon et al,2006)).and reduced user satisfaction.
Thus a major chal-lenge in the development of SDS is to improveinformation presentation algorithms.
This is im-portant for several reasons: (1) to avoid overbur-dening the user?s memory by presenting too manyoptions; (2) to ensure that the user is given anoverview of the available option space so that theoptimal option can be found; and (3) to minimisethe number of dialogue turns (hence dialogue du-ration) required for the user to find an acceptableoption.
As Walker et al (2001) showed, failing tomeet this third goal may reduce overall user satis-faction.Recently several approaches have been pro-posed to overcome the shortcomings of the se-quential enumeration strategy (Polifroni et al,2003; Chung, 2004; Demberg and Moore, 2006;Polifroni and Walker, 2008).
Because of the com-plexity of building a complete end-to-end SDS,these approaches have been evaluated using an?overhearer?
methodology in which dialogues areeither hand-crafted or simulated and then pre-sented to subjects, either as textual transcripts1(Demberg and Moore, 2006; Polifroni and Walker,2008) or audio recordings (Walker et al, 2004),for rating.
The general consensus from these stud-ies is that users significantly prefer approachesthat take their preferences into account.
How-ever, because users were not interacting with theseSDS, the evaluation criteria were limited to users?perceptions (e.g., informativeness, good overviewof options, confidence in choice, etc.
), and met-rics such as effectiveness (i.e., actual or perceivedtask completion) and efficiency (i.e., length of di-alogue) could not be assessed.
To address thisissue, Winterboer and Moore (2007) carried outa Wizard-of-Oz (WOz) study in which users par-ticipated in dialogues controlled by two differentinformation presentation algorithms.
They foundthat not only did users prefer presentations basedon a user model, dialogues employing the ?user-model based summarise and refine?
(UMSR) ap-proach led to greater task success and dialogue ef-ficiency.In this paper, we take this one step further, andevaluate the effectiveness of the UMSR approachin a more realistic setting, incorporating this con-tent selection and presentation strategy into a fullend-to-end dialogue system, and comparing it tothe traditional sequential enumeration approach.Our results suggest that despite complications as-sociated with a real dialogue system setting, theUMSR model retains its advantages.
Our resultsalso verify the hypothesis that the UMSR modelpresents complex trade-offs in a concise, yet un-derstandable way.
Furthermore, as in the WOzstudy, the UMSR approach leads to a significantreduction in the number of dialogue turns.The structure of the paper is as follows: InSec.
2, we discuss related work.
In Sec.
3 wepresent the full end-to-end SDS used for com-parison between the standard sequential enumer-ation approach and the UMSR approach.
In Sec.
4we describe how we implemented the UMSR ap-proach.
Then in Sec.
5 we provide an example.
InSec.
6 we describe our experimental design and inSec.
7 our results.
Finally in Sec.
8, we present ourconclusions.2 Previous ApproachesAs noted above, a number of approaches to in-formation presentation in SDS have recently beenproposed.
The user-model based (UM) approachemploys a model of the users preferences and de-cision theory techniques to identify and present asmall number of options that best match the user?spreferences (Carenini and Moore, 2001; Walker etal., 2004; Moore et al, 2004).
Fig.
2 shows a sam-ple presentation generated using the UM approachfor a student user who cares most about price andflying direct.There?s a direct flight on BMI with a good price.
Itarrives at four ten p.m. and costs a hundred and twelvepounds.
The cheapest flight is on Ryanair.
It arrives attwelve forty-five p.m. and costs just fifty pounds, but itrequires a connection in Dublin.Figure 2: Example of information presentationbased on the UM approach, from (Moore et al,2004).Although the UM approach enables a concisepresentation of a small number of options, high-lighting the ways in which these options satisfyuser preferences, it does not scale up to presentinga large number of options.
When there are hun-dreds of potentially relevant options to consider(e.g., all Italian restaurants in New York City),there may be a large number of options that fitthe user model, making it difficult to narrow downthe number of options.
In addition, users maynot be able to provide constraints until they arepresented with more details concerning the optionspace.
Moreover, the system does not provide anoverview of the available options, which may leadto the user?s actual or perceived missing out onpossibly better alternatives.An alternative approach, which we call the sum-marise and refine approach (SR) (Polifroni et al,2003; Chung, 2004) structures large datasets forsummarisation and successive refinement.
It sup-ports the user in narrowing in on a suitable optionby grouping the options in the database into clus-ters of options with similar features.
The systemcan then summarise the common attributes of theoptions and thus suggest further refinement con-straints to the user.
In this content structuring ap-proach, the prompts presented to the user and theorder in which they appear, are determined at run-time based on an algorithm that computes the mostuseful set of attributes, as dictated by the set of op-tions that satisfy the user query.
For large datasets,attributes that partition the data into the minimalnumber of clusters are chosen, so that a concisesummary can be presented to the user to refine(Fig.
3).In addition to its many advantages, the SR ap-proach also has several limitations.
First, depend-ing on the size and complexity of the domain,many turns may be required during the refinement2M: What restaurants do you have?S: I have found 983 restaurants.
Most of them arelocated in Boston and Cambridge.
There are 32choices for cuisine.
I also have information aboutprice range.M: Okay tell me about the ones in Boston.S: I have found 401 restaurants in Boston.
There are29 choices for cuisine.M: Do you have any that serve seafood?S: I have found 19 seafood restaurants.
They arepredominantly in Back Bay, the North End, SouthBoston and the South End.Figure 3: Sample dialogue between simulator (M)and SR system (S), from (Polifroni et al, 2003).process.
Because the user?s preferences are nottaken into account, the clusters may contain manyirrelevant entities, which must be filtered out suc-cessively with each refinement step.
Second, ifthere is no optimal solution, exploration of trade-offs between options can be difficult.
Finally, thechosen clusters may be based on attributes that areirrelevant for the specific user.In an attempt to combine the benefits of the UMand SR approaches, Demberg & Moore (2006)devised the user-model based summarise and re-fine (UMSR) approach to information presenta-tion.
This approach first clusters the values of eachattribute in order to group them so that the op-tions can be summarised more easily later, and la-bels like ?cheap?, ?moderate?, ?expensive?
can beassigned to values of continuous categories suchas ?price?.
The system then structures optionsinto an option tree based on the ranking of at-tributes in the user model, the options returnedfrom the database, and the attribute-value clus-tering.
The resulting option tree determines howdifferent options relate to one another, and whichones are most attractive for the user.
After the treestructure is built, it is pruned to decide which op-tions are compelling to the user according to theuser model.
This allows the system to save timeby omitting options that are not of any potentialinterest to the user.
Once pruning is complete,each branch of the tree describes a possible refine-ment path, and thus can be used to direct dialogueflow.
Trade-offs between alternative options arepresented explicitly in order to provide the userwith a better overview of the option space.
In ad-dition, to give users confidence that they are beingpresented with all of the relevant options, a briefaccount of all the remaining (irrelevant) options isalso provided.
For a more detailed discussion ofthe UMSR approach, see (Demberg and Moore,2006).
In Sec.
4 we describe how we employedthe UMSR approach in our system.3 The TownInfo SystemThe TownInfo SDS was developed as part of theEC project TALK (Lemon et al, 2006).
Userscan search for hotels, bars and restaurants inan artificial town.
The system supports two di-alogue strategies, one hand-crafted and anotherlearnt using Reinforcement Learning (Hendersonet al, 2008).
For the current experiment we usedthe hand-crafted strategy.
Natural language un-derstanding is performed using a keyword-basedparser and natural language generation is basedon templates.
The information presentation is se-quential.
An example is given in Fig.
1, takenfrom the modified version of TownInfo for the cur-rent experiment.
Although the original TownInfosystem supported speech input and speech output,here we use text input/output to make sure that ourresults are not influenced by poor recognition ac-curacy or intelligibility due to poor speech syn-thesis.
Of course, as we mention in Sec.
8, thenext step would be to perform an experiment withspeech input/output.For our current experiment we focussed onrestaurant recommendations and the TownInfodatabase had to be extended to include a muchwider range of options to provide more realisticinformation presentation scenarios.
The databaseused in our experiments contains a total of 80restaurants in Edinburgh, UK.4 The UMSR AlgorithmThis section briefly describes our implementationof the UMSR algorithm; for more details see(Demberg and Moore, 2006).
Sec.
5 provides anexample for clarity.4.1 The User ModelThe user model contains the user?s ranking andpreferred values for the relevant attributes in therestaurant domain: price, distance, starrating, service rating, and cuisinetype.
Table 1 shows a sample user model.
TheRank field indicates the relative importance of theattributes for the user, with 1 being most impor-tant.
The Value field indicates the user?s preferredvalue for each attribute.11If two attributes in a user model have identical ranks, theorder of the preferences is used to decide which has a higherpriority.3UserID Attribute Value Rank1 Price Cheap 1.001 Distance Near 2.001 Star High 3.001 Cuisine Indian 4.001 Service Don?t Care 5.00Table 1: Sample user model for a student.According to Elzer et al (1994), some prefer-ences are enough to reject options outright (andtherefore are more like goals) whereas others aremore purely like preferences (to be weighed andranked).
Here we do not make such a distinction.4.2 Adapting to Changes to the User ModelIn the original design, the user model was cre-ated at the outset and not modified during the dia-logue.
However, during initial piloting of the sys-tem, we found that this design did not support ?sit-uational preferences?.
For example, consider theuser model for the student in Table 1.
This usernormally prefers to have Indian food if she has theoption to (a ?dispositional preference?).
If, how-ever, in the current situation she is entertaining afriend from out of town who wishes to try Scottishfood, the user may decide to explore options forScottish cuisine (a ?situational preference?).
Here,the user changes her original query for the situa-tion, thus redefining her preferences.
When thisoccurs, we must perform a new database queryand rebuild the option tree.
To take these dynamicchanges into account during the course of the dia-logue, at each dialogue turn the user query is com-pared against the user model, and if any differenceis noted, the user model is updated to reflect thecurrent preferences, the tree is rebuilt using thenew user model, and the dialogue continues witha summary of the available options based on thisnew tree.Note that for individual models, i.e.
user modelsthat are designed for individual people and not forclasses of users (student or business person), somequeries could justify situational changes and somecould indicate permanent (or at least less tempo-rary) changes to the user model (e.g., ?Are thereany nicer restaurants?
I got a new job?).
In ourexperiment we use only class models and we donot allow permanent changes to the user model.4.3 The Clustering AlgorithmFollowing (Polifroni et al, 2003) and (Dembergand Moore, 2006), we used agglomerative group-average clustering to automatically group valuesfor each attribute.
The algorithm begins by assign-ing each unique attribute value to its own bin, andsuccessively merging bins whose means are mostsimilar until a stopping criterion (a target of nomore than three clusters, in our implementation)is met.
The bins are then assigned predefined la-bels, e.g., ?cheap?, ?moderately priced?
and ?ex-pensive?
for price.
Clustering attribute valueswith this algorithm allows for database-dependentlabelling.
Therefore, a restaurant with a price of?35 might be considered as expensive for Edin-burgh, but inexpensive for London.4.4 Building the Option TreeThe tree building algorithm is recursive.
It beginsat the root node, which contains all entities in theretrieved dataset, and builds up the tree level bylevel based on the ranking of attributes in the usermodel.
At each node of the tree, it retrieves thenext attribute preference from the user model andthen invokes the clustering algorithm for this at-tribute?s values.
Once the current dataset has beenclustered, the algorithm then adds the resultantclusters as the children of the current node.
Af-ter each cluster is added, the algorithm is invokedrecursively on the newly created children of thecurrent node.As the tree is being constructed, the algorithmarranges the nodes in the tree such that the childrenof each node are ordered from left to right in de-creasing order of desirability.
For example, if theparticular user prefers restaurants that are far fromthe city centre, the clusters based on distancewould be ordered such that ?far?
is the leftmostchild and ?near?
is the rightmost child.
Fig.
5 de-picts an option tree structure for the user model ofTable 1, in the context of the example of Sec.
5.The numbers in the nodes indicate how many op-tions are represented by the node.Given an option tree ordered in this way, to findthe best available options, the system traverses thetree in a depth-first fashion starting from the rootand selecting the leftmost branch at each node.4.5 Pruning the Option TreeThe goal of the UMSR algorithm is to present anoverview of the available options, that are mostrelevant to the user?s preferences, concisely andunderstandably.
To determine the relevance of op-tions, we use the notion of ?dominance?
definedin Demberg & Moore (2006).
Dominant optionsare those for which there is no other option in thedataset that is better on all attributes.
A domi-4nated option is in all respects equal to or worsethan some other option in the relevant subset ofthe database; it should not be of interest for anyrational user.The pruning algorithm follows Demberg &Moore (2006), and thus we summarise it onlybriefly here.
The algorithm operates directly onthe ordered option tree, using the tree structure sothat it can efficiently determine dominance rela-tions without having to compare each pair of op-tions.
The algorithm traverses the tree in depth-first order, generating constraints during this pro-cess.
These constraints encode the properties thatother options would need to satisfy in order not tobe dominated by the options which have alreadybeen deemed to be dominant.
A node must ful-fil the constraints that apply to it, otherwise it ispruned from the tree.
If an option (or a cluster ofoptions) satisfies a constraint, the property that sat-isfied the constraint is marked as the options?
jus-tification.
If some, but not all, of the constraintscan be satisfied by an option, the constraints arepropagated to the other nodes (see Fig.
5).4.6 Natural Language GenerationOnce a pruned option tree has been constructed,the system can generate a presentation to the user.The natural language generation (NLG) algorithmincludes three steps described below.4.6.1 Identifying Trade-offsTo identify the trade-offs, the algorithm tra-verses the tree looking for constraints that weregenerated during the pruning process.
For eachnode that generated a constraint, the algorithmfinds the best sibling, which satisfies the con-straint.
It does this by first checking the siblingsof the current node, and if none satisfy the con-straint, it moves up the tree and recursively tra-verses siblings of the ancestor node.
Once a trade-off node is found, it is recorded in the option treeat that point, and the algorithm then searches up-ward in the tree to find the lowest common parentof the trade-off nodes.
This is the ?DifferentiatingParent?
for the trade-off; the attribute that deter-mines the children of this node indicates the trade-off the user has to make.
In the example shownin Fig.
5, the attribute that the user must trade-off is distance.
The system then generates anatural language description of the trade-off usingtemplates, e.g., ?For a restaurant that serves Indiancuisine you would have to go farther from the citycentre?.
If a constraint cannot be satisfied, the sys-tem generates a sentence describing this situation,e.g., ?However, there are no inexpensive restau-rants that match your preferences?.4.6.2 SummarisationThe summarisation process aims to present theuser with an understandable and concise overviewof the available options, so that she can makeinformed decisions on how to proceed with herquery.
In a SDS, it is important not to present toomuch information in a single turn in order to keepthe memory load on the user manageable.
Thus,in our implementation, the system provides a sum-mary of the first three attributes (corresponding tothe first three levels of the tree), e.g., ?I found 12restaurants which are inexpensive.
Only 7 of themare near the city centre, out of which only 4 havea high star rating?.
If there are any trade-offs overthe first three attributes, they are also mentioned tothe user, using the algorithm described above.4.6.3 Turn GenerationThe turn generation algorithm first determineswhether the current user?s query is consistent withthe preferences defined in the user model.
Sincethe option tree, and consequently the generation sofar, have primarily been based on the user model,if the algorithm notices an inconsistency it willgive priority to the most recent user query as a sit-uational adjustment (see Sec.
4.2), and then updatethe user model to reflect the adjustment.
If the usermodel is modified, the algorithm rebuilds the op-tion tree, and generates the next turn using the newuser model and the most recent user query.To ensure the user that the system has under-stood her correctly, and to increase her confidence,the algorithm includes a summary of all attributevalues that are common to the cluster in the optiontree that is being used to generate the summary.For example, for the second turn in the dialogue ofFig.
4, although the user?s most recent query wasabout cuisine, the system includes a summaryof all the attributes already mentioned to the user:?All of them are cheap, are near the city centre,and have a high star rating?.5 ExampleTo clarify things further, here we present an exam-ple.
A dialogue between the UMSR system anda student (user model of Table 1) is depicted inFig.
4.Fig.
5 depicts the tree structure for the stu-dent user of our example.
The user begins by5U: Tell me about the restaurants you have.S: I found 12 restaurants which are inexpensive.
Only7 of them are near the city centre, out of which only4 have a high star rating.
I also have informationabout cuisine and service quality.U: Tell me about the cuisine.S: There is only 1 restaurant that serves Scottish food,and 3 restaurants that serve French food.
They allprovide good service quality.
All of them are cheap,are near the city centre, and have a high star rating.For a restaurant that serves Indian cuisine youwould have to go farther from the city centre.U: Okay, tell me about the Scottish one.S: The Jam House?s menu includes Scottish dishessparked with international flavour, with a decentchoice of vegetarian dishes.
It is located in thecentre.
The average price is ?22 per person.U: What about Indian restaurants?S: I found 2 restaurants that serve Indian food.Both of them are cheap, within average distancefrom the city centre, and have a high star rating.Only one of them provides good service quality.U: Okay, tell me about this one.S: The Raj on the Shore?s theme extends from thesoldier figurines behind the exotic blue bar to thepredominantly Bangladeshi, Indian menu.
It servesIndian and Bangladeshi cuisine.
It is locatedin the north.
The average price is ?20 per person.Figure 4: Example dialogue with our UMSR ap-proach (student user of Table 1).asking generally for a restaurant recommendation,and the system provides a summary based on thethree top-ranking attributes for this user: price,distance and star rating, which corre-spond to the first three levels of the option tree.Next the user asks about cuisine type, whichgenerates a trade-off since there are no Indianrestaurants (user?s preference) that are cheap, nearthe city centre, and of high star rating.
The userthen asks about the Scottish option, before switch-ing back to her preferred cuisine type (Indian).
Be-cause Indian cuisine was in the user?s initial pref-erence model, a constraint of cuisine=Indianwas generated when traversing the leftmost branchof the tree, and this justified not pruning the un-shaded nodes in the right subtree of Fig.
5, in or-der to generate the trade-off.
However, if the userhad asked about expensive restaurants, then a newdatabase query would have been made and a newoption tree would have been built.
A more com-plex example is given in the Appendix.6 Experimental DesignIn total 18 subjects interacted with our two sys-tems.
Each participant interacted three times withthe modified TownInfo system, and another threetimes with the system that supported our imple-mentation of the UMSR model (108 dialogues inFigure 5: A sample option tree structure for thestudent user of Table 1.
Pruned nodes are shownas shaded.total).
The order of the dialogues was randomisedamong the subjects.
Each experiment took be-tween 40 and 50 minutes on average.For each task, subjects were provided with theuser profile and the actual scenario for the spe-cific task in hand.
The tasks were carefully con-structed so that half of them could be solved with-out making any trade-offs and the other half re-quired a trade-off to be made.
At the end of eachtask the subjects had to fill out a questionnaire with10 questions on a 7-point Likert scale.
They werealso asked if they had been able to accomplish thegiven task (perceived task completion), i.e., to finda suitable restaurant for the scenario and user pro-file in hand.
Finally, after each task they had toprovide the name(s) of the restaurants they chosefor the task.
The name(s) stated for this task werethen used to compare perceived task completionwith actual task completion.
At the end of eachtask with the UMSR system, the profiles were re-set to the default attribute values and ranks.Both systems had identical software configura-tions, i.e., they only differed in the informationpresentation component.
Yet another importantfeature was that the UMSR based model did notaccept multiple attributes in a single query.
Sofor instance the user could not ask ?I am look-ing for a moderately priced restaurant near the citycentre that serves Italian food?.
This seemed tobe a major shortcoming of the UMSR based sys-tem compared to the TownInfo system with se-quential information presentation.
However, as wewill see in the following, even with this shortcom-6System U CC CF A EUMSR-all 5.04 4.65 3.22 3.66 4.69TownInfo-all 4.87 4.04 2.93 3.20 3.59UMSR-with TO 4.74 4.59 2.67 3.26 4.15TownInfo-with TO 4.59 3.41 2.74 2.33 2.70UMSR-no TO 5.33 4.70 3.78 4.08 5.22TownInfo-no TO 5.15 4.67 3.11 4.07 4.48Table 2: Average scores of the question-naires for all dialogues, dialogues with trade-offs (with TO) and dialogues without trade-offs(no TO) (U=understandability, CC=conciseness,CF=confidence, A=accessibility, E=efficiency).ing the UMSR approach retained its advantagesand proved more successful than the traditional se-quential enumeration approach.7 ResultsThe perceived task completion (PTC) for theUMSR system and the TownInfo system was90.74% and 85.19% respectively, and the actualtask completion (ATC) 74.07% and 62.96%.
Thusthe UMSR approach led to a relatively better userconfidence in having achieved the task.The average number of turns was 9.24 forUMSR compared to 17.78 for TownInfo, whichdenotes a significant reduction in the number ofdialogue turns required to accomplish a giventask.
This reduction becomes even more promi-nent when there is a trade-off involved.
With suchdialogues, the average number of turns for UMSRremained almost constant at 9.41, whereas Town-Info showed an increase reaching up to 24.19.This huge difference is obviously a significantimprovement in system efficiency and user sat-isfaction.
It also supports our hypothesis thatthe UMSR approach can present trade-offs under-standably.
For dialogues without a trade-off thenumber of turns was 9.07 for UMSR and 11.37for TownInfo.Dialogue duration also showed a great improve-ment in UMSR over TownInfo (4:49 (m:s) vs.6:11).
The duration however was almost the samefor the two systems when a trade-off existed (4:40vs.
4:49).
This could mean that although the num-ber of turns in this case is smaller for UMSR, thelength of the generated output is longer, and re-quires more attention to understand.
Yet again indialogues without a trade-off, UMSR had a con-siderably shorter duration than TownInfo (4:57 vs.7:34).Average scores of the questionnaires are givenin Table 2.In response to the question ?I thought the waythe system provided information to me was easyto understand?
the average score over all 108 di-alogues was 5.04 for UMSR and 4.87 for Town-Info.
The preference for UMSR exists for dia-logues both with and without a trade-off.
How-ever, for all three cases the differences were notsignificant (p > 0.05).Conciseness is the quality of providing a con-cise overview of all the available options to theuser.
The UMSR system was preferred at 4.65over 4.04 for TownInfo (p = 0.034).
The differ-ence between the two systems is very significantfor dialogues with a trade-off (p < 0.003).
How-ever, for dialogues without a trade-off p = 0.92.This was predictable as the main innovation inUMSR is the ability to present trade-offs in a con-cise and understandable way, hence the significantdifference for the dialogues with trade-offs.To evaluate their confidence in having heard allthe relevant options, the subjects were asked torate the statement ?I thought there were better op-tions for my request than what the system gaveme?.
Because of the negative nature of the ques-tion, the Likert scale was inverted before analysis.The average score was 3.22 and 2.93 for UMSRand TownInfo respectively.
This indicates thatthe users have slightly more confidence in hav-ing heard all the relevant options with the UMSRsystem, although this difference is not significant(p > 0.05).
For dialogues with a trade-off, theaverage confidence score was slightly better forTownInfo (2.74 vs. 2.67), but not significant (p =0.8).
However, there is a significant difference fordialogues without a trade-off (p < 0.03).
An-other notable issue is the overall low scores for thecases with a trade-off.
This signifies that perhapsmore information needs to be given to the userfor dialogue turns describing a trade-off.
A care-ful balance needs to be drawn between concise-ness and comprehensiveness in these cases.
Thishowever, will obviously increase dialogue dura-tion, and might affect understandability.By accessibility, we mean ease of use andcommunication with the system.
The scores forUMSR and TownInfo were 3.66 and 3.20 respec-tively (p = 0.18).
A more significant differencein accessibility was noted for dialogues with atrade-off (p = 0.008).
Again it seemed that userspreferred UMSR when it came down to dealingwith trade-offs.
However, the accessibility scoresfor dialogues without a trade-off were almost thesame (p = 0.92).7Efficiency is the quality of enabling users tofind the optimal option quickly.
The statement?In this task, the system allowed me to find theoptimal restaurant quickly?, resulted in an aver-age score of 4.69 for UMSR vs. 3.59 for Town-Info (p = 0.002).
Once again, a significant dif-ference was noted for dialogues with a trade-off,with 4.15 and 2.70 for UMSR and TownInfo re-spectively (p = 0.004).
However, the differencefor dialogues without a trade-off was not signifi-cant (p = 0.12).8 Conclusions and Future WorkIn this paper, we evaluated the effectiveness of theUMSR approach in information presentation in afull end-to-end dialogue system.
The UMSR ap-proach was compared with the traditional sequen-tial enumeration of options.
Our results verifiedour hypothesis that the UMSR approach presents abetter overview of the trade-offs within the optionspace, and improves user experience and confi-dence in the system.
Furthermore, with the UMSRapproach there is a significant reduction in thenumber of dialogue turns required to complete thetask.
The results also showed that UMSR specifi-cally outperforms TownInfo when there is a trade-off involved.
The UMSR results presented statisti-cally significant improvement for conciseness, ac-cessibility, and efficiency.
Overall, subjects weremore satisfied with the UMSR system.
When theywere asked if they would use the system again asa deployed product the score was 4.74 for UMSRand 3.70 for TownInfo (p = 0.002), further veri-fying that the users preferred the UMSR approachover the sequential enumeration of TownInfo.In future work we intend to make a number ofimprovements.
For example in the turn genera-tion algorithm, we will optimise the generated out-put in an effort to strike a balance between un-derstandability and complexity.
Another impor-tant issue is to modify the UMSR algorithm so thatit can accept multiple attributes in a single query.Moreover, we will perform experiments with bothspeech input and output.
Finally, we will com-pare the UMSR approach with the UM and SR ap-proaches in the same setting, i.e., a full end-to-endSDS.AcknowledgementsThis paper is based on a research experiment con-ducted at the University of Edinburgh.
Paksimawas funded by the European Commission Eras-mus Mundus scholarship program.
Georgila waspartially funded by the Wellcome Trust VIP Awardand is currently funded by the U.S. Army Re-search, Development, and Engineering Command(RDECOM).
The content does not necessarily re-flect the position or the policy of the U.S. Gov-ernment, and no official endorsement should beinferred.
The authors thank the three anonymousreviewers.ReferencesG.
Carenini and J.D.
Moore.
2001.
An empirical studyof the influence of user tailoring on evaluative argu-ment effectiveness.
In Proc.
of IJCAI.G.
Chung.
2004.
Developing a flexible spoken dialogsystem using simulation.
In Proc.
of ACL.V.
Demberg and J.D.
Moore.
2006.
Information pre-sentation in spoken dialogue systems.
In Proc.
ofEACL.S.
Elzer, J. Chu-Carroll, and S. Carberry.
1994.
Recog-nizing and utilizing user preferences in collaborativeconsultation dialogues.
In Proc.
of UM ?94.J.
Henderson, O.
Lemon, and K. Georgila.
2008.
Hy-brid reinforcement/supervised learning of dialoguepolicies from fixed datasets.
Computational Lin-guistics, 34(4):487?511.O.
Lemon, K. Georgila, J. Henderson, and M. Stut-tle.
2006.
An isu dialogue system exhibiting re-inforcement learning of dialogue policies: genericslot-filling in the talk in-car system.
In Proc.
ofEACL.J.D.
Moore, M.E.
Foster, O.
Lemon, and M. White.2004.
Generating tailored, comparative descriptionsin spoken dialogue.
In Proc.
of FLAIRS.J.
Polifroni and M. Walker.
2008.
Intensional sum-maries as cooperative responses in dialogue: Au-tomation and evaluation.
In Proc.
of ACL?HLT.J.
Polifroni, G. Chung, and M. Walker.
2003.
To-wards automatic generation of mixed-initiative di-alogue systems from web content.
In Proc.
of Eu-rospeech.M.A.
Walker, R.J. Passonneau, and J.E.
Boland.
2001.Quantitative and qualitative evaluation of darpacommunicator spoken dialogue systems.
In Proc.of ACL.M.
Walker, S. Whittaker, A. Stent, P. Maloor, J.D.Moore, M. Johnston, and G. Vasireddy.
2004.
Gen-eration and evaluation of user tailored responses indialogue.
Cognitive Science, 28:811?840.A.
Winterboer and J.D.
Moore.
2007.
Evaluating in-formation presentation strategies for spoken recom-mendations.
In RecSys ?07: Proc.
of the 2007 ACMconference on Recommender systems.8AppendixOption Tree A for User Model A is built.U: I?d like to get some food tonight.S: I found 17 restaurants with a high star rating.
Only 8 of them provide good service quality, out of whichonly 3 serve Scottish food.
I also have information about distance and price.U: Are they close to the city centre?S: I found 3 restaurants that are located far from the city centre.
1 of them is cheap and 2 are expensive.They all have a high star rating, provide good service quality, and serve Scottish food.
For a restaurant nearthe city centre you would have to go for average service quality.U: How about French cuisine?The System cannot answer the query because the ?French?cuisine branches of Option Tree A have beenpruned and their nodes are shown as shaded.User Model A changes to User Model B (situational query).Option Tree B for User Model B is built.S: I found 10 restaurants that serve French food.
Only 5 of them have a high star rating, out of which only 3provide good service quality.
I also have information about distance and price.U: Tell me about the distance from the city centre.S: Only 1 of them is located near the city centre and is inexpensive.
It serves French food, has a high star rating,and provides good service quality.U: Tell me about it.S: La Bagatelle?s dishes include a starter of roast pigeon with canelloni of shiitake mushroom, and a main courseof panfried veal with prunes.U: That?s great.
I?ll take it.. .
.Figure 6: Example dialogue with our UMSR approach for a business person, User Model A (Table 3)and User Model B (Table 4), Option Tree A (Fig.
7) and Option Tree B (Fig.
8).UserID Attribute Value Rank2 Star High 1.002 Service Good 2.002 Cuisine Scottish 3.002 Distance Near 4.002 Price Don?t Care 5.00Table 3: Sample user model for a business person (User Model A).UserID Attribute Value Rank2 Cuisine French 0.952 Star High 1.002 Service Good 2.002 Distance Near 4.002 Price Don?t Care 5.00Table 4: Modified user model for a business person after the situational query ?I?d like a French restau-rant?
(User Model B).9Figure 7: Option tree structure (Option Tree A) corresponding to the User Model A of Table 3.
Prunednodes are shown as shaded.Figure 8: Option tree structure (Option Tree B) corresponding to the User Model B of Table 4.
Prunednodes are shown as shaded.10
