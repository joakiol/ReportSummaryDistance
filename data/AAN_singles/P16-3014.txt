Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ?
Student Research Workshop, pages 93?99,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsGraph- and surface-level sentence chunkingEwa Muszy?nskaComputer LaboratoryUniversity of Cambridgeemm68@cam.ac.ukAbstractThe computing cost of many NLP tasks in-creases faster than linearly with the lengthof the representation of a sentence.
Forparsing the representation is tokens, whilefor operations on syntax and semantics itwill be more complex.
In this paper wepropose a new task of sentence chunking:splitting sentence representations into co-herent substructures.
Its aim is to makefurther processing of long sentences moretractable.
We investigate this idea exper-imentally using the Dependency MinimalRecursion Semantics (DMRS) representa-tion.1 IntroductionLong sentences pose a challenge in many Nat-ural Language Processing (NLP) tasks, such asparsing or translation.
We propose chunking asa way of making such sentences more tractablebefore further processing.
Chunking a sentencemeans cutting a complex sentence into grammat-ical constituents that can be processed indepen-dently and then recombined without loss of infor-mation.
Such an operation can be defined both onthe surface string of a sentence and on its semanticrepresentation, and is applicable to a wide range oftasks.Some approaches to parsing have space andtime requirements which are much worse than lin-ear in sentence length.
This can lead to practicaldifficulties in processing.
For example, the ACEprocessor1running the English Resource Gram-mar (ERG) (Copestake and Flickinger, 2000) re-quires roughly 530 MB of RAM to parse Sen-tence 1.
In fact, longer and more complicated sen-1Woodley Packard?s Answer Constraint Engine, http://sweaglesw.org/linguistics/ace/tences can cause the parser to time out or run outof memory before a solution is found.
(1) Marcellina has hired Bartolo as her coun-sel, since Figaro had once promised tomarry her if he should default on a loanshe had made to him, and she intends toenforce that promise.Chunking would make processing of long sen-tences more tractable.
For example, we aim tosplit sentences like Sentence 1 into chunks 2a?d.
(2) a. Marcellina has hired Bartolo as hercounsel.b.
Figaro had once promised to marryher.c.
He should default on a loan she madeto him.d.
She intends to enforce that promise.Each of these shorter sentences can be parsed withless than 20 MB, requiring in total less than a fifthof RAM needed to parse the full sentence.What exactly constitutes a valid chunk has tobe considered in the context of the task which wewant to simplify by chunking.
In this sense a po-tentially useful analogy could be made to the useof factoids in summarisation (Teufel and Van Hal-teren, 2004; Nenkova et al, 2007).
However, wecan make some general assumptions about the na-ture of ?good?
chunks.
They have to be semanti-cally and grammatically self-contained parts of thelarger sentence.Sentence chunking resembles clause splitting asdefined by the CoNLL-2001 shared task (Tjonget al, 2001).
Each of the chunks a?d is a fi-nite clause, although each consists of multiplesmaller clauses.
This points to a crucial differ-ence between sentence chunking and clause split-ting which justifies treating them as separate tasks.93We define chunking in terms of its purpose as apre-processing step and because of that it is morerestrictive.
Not every clause boundary is a chunkboundary.
A key aspect of sentence chunking isdeciding where to place a chunk border so that theresulting chunks can be processed and recombinedwithout loss of information.Another difference between sentence chunkingand clause splitting is the domain of the task.Clause splitting is performed on the surface stringof a sentence, while we can define chunking notonly on the surface representation but also on morecomplex ones, such a graph-based semantic repre-sentation.There are two reasons why chunking a semanticrepresentation is a good idea:1.
Many operations on graphs have worse thanlinear complexity, some types of graphmatching are NP-complete.
Chunking se-mantic representations can make their manip-ulation more tractable (Section 1.1).2.
Such a form of chunking, apart from beinguseful in its own right, can also help chunkingsurface sentences (Section 1.2).1.1 Chunking semantic representationsIn this paper we describe an approach to sen-tence chunking based on Dependency MinimalRecursion Semantics (DMRS) graphs (Copestake,2009).
We chunk a sentence by dividing its seman-tic representation into subgraphs corresponding tological chunks.
The link structure of a DMRSgraph reveals appropriate chunk boundaries.
Sincewe envision chunking to be one of the steps in aprocessing pipeline, we prioritize precision overcoverage to minimize error propagation.
The goalis to chunk fewer sentences but correctly ratherthan more but with low precision.Sentence chunking understood as graph chunk-ing of a semantic representation can be directlyuseful for applications that already use the rep-resentation.
Although we use the DMRS, chunk-ing could be just as well adapted for other seman-tic representations, for example AMR (AbstractMeaning Representation) (Banarescu et al, 2013).Part of our reason to choose the DMRS frame-work was the fact that the DMRS format is readilyinterchangeable with Minimal Recursion Seman-tics (MRS).
Thanks to this relationship our sys-tem is compatible with any applications stemmingfrom the DELPH-IN initiative2.Horvat et al (2015) introduce a statistical ap-proach to realization, in which they treat realiza-tion like a translation problem.
As part of theirapproach, they extract grammatical rules based onDMRS subgraphs.
Since operations on subgraphsare computationally expensive, chunking the sen-tence before the algorithm is applied could reducethe complexity of the task.Another task which could benefit from chunk-ing is treebanking.
LinGO Redwoods 2 (Oepen etal., 2004) is an initiative aimed at designing anddeveloping a treebank which supports the HPSGgrammar.
The treebank relies on discriminants todifferentiate and choose between possible parses.Chunking could be used to preferentially selectparses which contain subtrees corresponding towell-formed chunk subgraphs.1.2 Towards string chunkingThe DMRS-based rule approach cannot be itselfused to improve parsing because it requires a fullparse to find the chunks in the first place.
How-ever, development of the surface chunking ma-chine learning algorithm can extend applicabilityof chunking to parsing and other tasks for which adeep parse is unavailable.The alignment between the semantic and sur-face representations of a sentence allows us to cutthe sentence string into surface chunks.
We intendto use the rule-based approach to create trainingdata for a minimally supervised machine learningalgorithm.Following Rei (2013, pp.
11-12) we use theterm ?minimally supervised?
to mean a systemtrained using ?domain-specific resources, otherthan annotated training data, which could be pro-duced by a domain-expert in a relatively shorttime?.
In our case the resource is a small set ofmanually coded rules developed through examina-tion of data.The ultimate goal of our work is the creation ofa reliable tool which performs chunking of sen-tence strings without relying on semantic repre-sentation and deep parsing.
The applicability ofchunking would then extend to tasks which cannotrely on deep parsing, such as statistical machinetranslation or parsing itself.The next sections give more details on the2Deep Linguistic Processing with HPSG,www.delph-in.net94Since I bought a cat, we have had no problems with mice.ARG1/HARG2/HARG1/NEQARG2/NEQRSTR/HARG1/NEQ ARG1/EQARG1/EQRSTR/H ARG2/NEQFigure 1: A DMRS graph of a sentence Since I bought a cat, we have had no problems with mice.
Thetwo chunks are marked, while since is separated as a functional chunk and chunking trigger.
The linkswith circular labels are crucial for chunking.DELPH-IN framework, DMRS and our approachto rule-based chunking.
We present our prelimi-nary results in Section 4 and outline our currentinvestigation focus and future research directionsin Sections 5.
Chunking is a new task, however itis related to several existing ones as discussed inSection 6.2 DELPH-IN framework and DMRSThe rule-based chunking system we devel-oped is based on the English Resource Gram-mar (ERG) (Flickinger, 2000), a broad-coverage,symbolic grammar of English.
It was developed aspart of DELPH-IN initiative and LinGO3project.The ERG uses Minimal Recursion Semantics(MRS) (Copestake et al, 2005) as its semanticrepresentation.
The MRS format can be trans-formed into a more readable Dependency MinimalRecursion Semantics (DMRS) graph (Copestake,2009), which represents its dependency structure.The nodes correspond to predicates; edges, re-ferred to as links, represent relations betweenthem.
An example of a DMRS graph is shown inFigure 1.DMRS graphs can be manipulated using twoexisting Python libraries.
The pyDelphin4isa more general MRS-dedicated library.
It al-lows conversions between MRS and DMRS rep-resentations but internally performs operations onMRS objects.
The pydmrs library5(Copestakeet al, 2016) is dedicated solely to DMRS manip-ulations.
The work described in Section 4 usedpyDelphin.3Linguistic Grammars Online, lingo.stanford.edu4https://github.com/delph-in/pydelphin5https://github.com/delph-in/pydmrsThe ERG is a bidirectional grammar which sup-ports both parsing and generation.
There exist sev-eral processors, which parse sentences into MRSsand generate surface forms from MRS represen-tations using chart generation.
In our experimentswe use ACE6to obtain MRSs and to generate fromthem, so that parsing and generation themselvesare performed using already existing DELPH-INtools.
The chunking algorithm operates on graphs?
we use the pyDelphin and pydmrs librariesfor MRS-DMRS conversion and for manipulatingDMRS objects.3 DMRS-based chunkingIn our research so far we have restricted validchunks to finite clauses.
A sentence is chunkedcorrectly if all the chunks are either full finiteclauses with a subject-verb structure or functionaltrigger chunks, such as since or and.
A chunk canconsist of multiple clauses if it is needed to ensurethat all chunks are satisfactory.The finite clause restriction was introduced be-cause well-formedness of finite clauses can be eas-ily checked and they can be more readily pro-cessed independently and recombined than othertypes of clauses.We developed the chunking rules through ex-amination of data and finding structural patternsin DMRS graphs.
Currently chunking is based onthree grammatical constructions: clausal coordi-nation (3), suboordinating conjunctions (4ab) andclausal complements (5).
(3) The cat chased a toy and the dog slept un-der the table.6Woodley Packard?s Answer Constraint Engine, http://sweaglesw.org/linguistics/ace/95(4) a.
The cat chased a toy because it wasbored.b.
Since the dog slept, Kim didn?t offer ita snack.
(5) Kim thought that they should talk.Extending the coverage of the technique to otherstructures is one of future directions of investiga-tion.We discover potential chunking points by spot-ting trigger nodes.
Those are the nodes whichcorrespond to coordinating and subordinating con-junctions, and to verbs with clausal complements.In the example from Figure 1 since is a trigger.After a trigger is found, we check whether theclauses associated with it are finite.
We can dothat by following links outgoing from the trig-ger node which lead to heads of the clauses.
Wemarked these links in the figure with circular la-bels.
In symmetric constructions, such as coordi-nation, chunks are separated unambiguously by aconjunction.
In other cases, such as the one in theexample, we can find the chunk border by detect-ing a gap in the graph?s link structure.
No linksoutgoing from either of the main chunks span thegap between cat and we in Figure 1.4 Preliminary resultsSo far we evaluated the system using a parsing andregeneration procedure, leveraging bidirectional-ity of the ERG.
The surface of each sentence waschunked into substrings based on its semantic rep-resentation.
Each of the resulting surface chunkswas then parsed using the ACE.
Next we fed thetop parse for each chunk as input to the ACE gen-erator, which produced the surface matching thesemantic representation of the chunk.
Finally, werecombined the surfaces generated in this fashionand compared the results with the original sen-tence.The parsing and regeneration is a way of check-ing whether any information loss was caused bychunking.
We do not attempt to improve pars-ing, only to evaluate how well the chunks meetthe criteria of well-formedness and applicabilitywe posit.
At the same time this form of evalua-tion assesses the semantic representation chunk-ing only indirectly, focusing on the quality of pro-duced surface chunks.
This is desirable in for cre-ating a good quality dataset for the minimally su-pervised machine learning algorithm discussed inSection 1.2.As our dataset, we used the 1212 release of theWikiWoods corpus (Flickinger et al, 2010) whichis a snapshot of Wikipedia from July 2008.
Theentire corpus contains 44,031,336 entries, fromwhich we selected only long sentences, viz.
sen-tences with more than 40 nodes in their DMRSgraph.
Additionally we filtered out some non-sentential entries.We compared the results obtained using theDMRS-based system with a simple string-basedheuristic baseline, similar to one of the techniquesused currently in statistical machine translationcommunity7.
The baseline attempts to chunk 67%of long sentences it encounters, compared with25% attempted by the DMRS-based approach.
Asa result, the absolute number of sentences thebaseline chunks correctly is greater but low pre-cision makes the heuristic approach highly unreli-able.
Any application which used it would requirea lot of human supervision.
The DMRS-basedprocedure correctly chunks 42.0% of sentences inwhich it finds chunking opportunities, while base-line correctly chunks only 19.6% of sentences.The evaluation method with which we obtainedthese results was harsh.
It required all non-functional chunks to be finite clauses.
If even oneof many chunks was not a finite clause, we countedthe entire sentence as chunked incorrectly.
Someerrors occurred in the final step of the evaluation:generation from chunk?s surface string.
We re-quired a high similarity between the reconstructedsentence and the original.
For example, accordingto the ERG lexicon, St and Street have the samesemantic representation and the generator can?tchoose between them.
If a generated string con-tained Baker Street when the original used BakerSt, the difference would be penalised even thoughthe two are equivalent.
More than one mistake ofthis kind in a sentence would be enough to rejectthe result as incorrect.A significant percentage of errors stems fromthe dataset itself.
Sentences and parses in theWikiWoods dataset were not checked by humans.In fact, not all Wikiwoods entries are grammaticalsentences and many of them could not be easilyfiltered out.
Bearing that in mind we briefly re-peated the experiment with a smaller WeSciencecorpus8(Ytrest?l et al, 2009).
Like WikiWoods,7Cambridge SMT system: Source sentence chop-ping, http://ucam-smt.github.io/tutorial/basictrans.html#chopping8http://moin.delph-in.net/WeScience96Algorithm (Dataset) Precision Correct Incorrect AttemptedDMRS-based (WikiWoods) 42.0% 3036 4195 24.9%Baseline (WikiWoods) 19.6% 3783 15526 66.6%DMRS-based (WeScience) 62.7% 106 63 22.7%Baseline (WeScience) 14.2% 60 362 56.7%Table 1: Performance of the DMRS-based chunking algorithm and the baseline on the WikiWoods andWeScience datasets.
Precision is the percentage of attempted sentences which were chunked correctly,while Correct and Incorrect columns give absolute numbers of correctly and incorrectly chunked sen-tences.
Attempted column is the percentage of sentences for which a chunking opportunity was foundand attempted.it originates from Wikipedia but has been checkedby human annotators.Indeed, the chunking procedure performs muchbetter on the human-checked dataset: 62.7% cor-rect chunkings as compared with 42% for Wiki-Woods (Table 1), indicating the algorithm?s sensi-tivity to parsing errors.The error analysis of the WeScience experimentreveals that over 25% of the errors made by therules-based system can be explained by the pres-ence of grammatical structures which the rulesdid not account for.
Increasing the coverage ofstructures used for chunking should decrease thenumber of errors of this origin.
Another commonsource of errors were adverbs and prepositionalphrases left behind after chunking sentences be-ginning with However, when.
.
.
or For example,if.
.
.
.
We address this issue in the newer version ofthe system.For comparison, the string heuristics baselinemakes chunking decisions based solely on thepresence of trigger words, such as and, withoutthe knowledge of what clauses are involved.
Theposition of good chunking boundaries is often de-termined by dependencies between distant partsof the surface, which are difficult to capture withstring-based rules, but are clearly reflected in theDMRS link structure.
This results in the baselineyielding unsatisfactory chunks like those under-lined in Sentence 6.
(6) The dog barked and chased the cat.5 Current work and future researchCurrently we are preparing a different evalua-tion technique which will directly compare DMRSrepresentations of chunks and the original sen-tence, eliminating the generation step responsi-ble for many errors.
In the new evaluation chunkgraphs are matched against the full graph usingthe pyDmrs matching module (Copestake et al,2016) which scores the degree of the match on acontinuous scale.We are also cooperating with the authors of thestatistical approach to realisation (Horvat et al,2015) on incorporating chunking into their graphmanipulations.
We hope to use their system forextrinsic evaluation.Sentences which would most benefit fromchunking are also, not accidentally, sentences withwhich parsers struggle most.
Chunking often failsbecause the parse on which we base it is incor-rect.
In the future we would like to experimentwith considering a number of parses instead of justthe top one.
This would enable us to mix chunkinginto the correct parse selection procedure.One of the investigation directions is extend-ing the catalogue of grammatical structures onwhich we base the chunks.
Some syntacticalstructures we consider as extensions are relativeclauses, verb phrase coordinations, gerund-basedadjuncts, parentheticals and appositions.
Their in-clusion would increase the coverage and quality ofchunks, crucial for our purposes.The treatment of clausal complements needsimprovement as well.
Some clauses are obligatorysyntactic elements and their removal changes howthe main clause is parsed.
We do not address thisissue in the current early version of the system butthe lexicalist nature of the ERG offers a solution.The information about whether a clausal comple-ment is obligatory for a given verb is contained inthe grammar?s lexicon and can be leveraged to im-prove chunking decisions.
We aim to include thismechanism in a later version of the algorithm.DMRS graphs store information about thealignment between nodes and surface fragments.This information allows us to chunk surfaces ofsentences based on the results of graph chunking.97As discussed in Section 1.2, we intend to create atraining dataset for a machine learning algorithmwhich would perform surface chunking.
Since, asthe WeScience experiment showed, our rule-basedapproach is sensitive to errors in original parses offull sentences, we might base our training corpuson the RedWoods treebank, which is larger thanWeScience but still human-checked.6 Related workWe define sentence chunking as a new task.
Asdiscussed in Introduction, it bears similarity toclause splitting but because of its definition interms of functionality, it has to be considered sep-arately.The most important similarity between chunk-ing and clause splitting is how the two problemscan be defined for the purpose of machine learn-ing.
Clause splitting was the CoNLL-2001 sharedtask (Tjong et al, 2001) and the results of thatresearch can guide the development of a machinelearning system for chunking.
Another task whichcan provide insights into how to design a suitablemachine learning system is Sentence BoundaryDisambiguation (SBD) task (Walker et al, 2001).Other research related to chunking was con-ducted in the context of text simplification.
Sen-tence chunking is a natural step in a simplificationprocess, among other rewrite operations such asparaphrase extraction, but the two tasks have dif-ferent goals.
While sentence simplification mod-ifies sentences, replacing lexical items and rear-ranging order of information, sentence chunkingaims to preserve as much of the original sentenceas possible.Chandrasekar et al (1996) suggested usingdependency structures for simplifying sentences.The authors gave an example of simplifying rela-tive clauses that is similar to chunking but outsideof the current scope of our experiments.
This re-search represented early work on automatic syn-tactic simplification and was succeeded by Sid-dharthan (2010) who performs simplification bydefining transformation rules over type depen-dency structures.
Siddharthan?s approach mixeslexical and syntactical transformations and cannotbe directly compared with chunking.Another example of work on simplification isa paper by Woodsend and Lapata (2011).
Theauthors call sentence chunking sentence splittingand approach it from the perspective of tree-basedQuasi-synchronous Grammar (QG).
Their algo-rithm learns possible chunking points by aligningthe original sentence with two shorter target sen-tences.
Unlike the method we propose, the QGapproach requires a manually created dataset con-sisting of original and target sentences from whichthe rules can be inferred.
Unfortunately, it is im-possible to compare the performance of our sen-tence chunking and the authors?
sentence splitting.The QG splitting algorithm is an integral part ofthe text simplification system and the paper de-scribing it does not give any numbers regardingthe performance of individual parts of the system.7 ConclusionsWe defined sentence chunking in terms of its use-fulness for other tasks.
Its aim is to producechunks which can be processed and recombinedwithout loss of information.
The procedure can bedefined for both the surface of a sentence and forits semantic representation.In our experiments we perform chunking us-ing rules based on the DMRS graphs of sentences.Our work is an early attempt at the task so we fo-cus on easier cases, aiming to gradually increasecoverage.
Since chunking is intended as a pre-processing step for other tasks, the reliability andprecision are more important than chunking asmany sentences as possible.
Bearing this in mind,we are satisfied to report that according to pre-liminary experiments, our chunking procedure at-tempted 25% of all sentences in the dataset andit chunked 42% of these correctly.
For compari-son, a baseline using heuristics attempted to chunk67% of sentences, but only 19.6% of these sen-tences were chunked correctly.The DMRS-based graph chunking can be usedto improve existing systems such as the statisti-cal realization algorithm (Horvat et al, 2015) orto guide the selection of parses for LinGO Red-Woods 2 treebank (Oepen et al, 2004).The surface chunking machine learning toolwill extend the applicability of chunking evenfurther.
Eliminating the immediate reliance on theparse could allow chunking to replace the stringheuristics for machine translation and to influenceparsing itself, reducing the difficulty of the task.98ReferencesLaura Banarescu, Claire Bonial, Shu Cai, MadalinaGeorgescu, Kira Griffitt, Ulf Hermjakob, KevinKnight, Philipp Koehn, Martha Palmer, and NathanSchneider.
2013.
Abstract Meaning Representationfor sembanking.
In Proceedings of the 7th Linguis-tic Annotation Workshop and Interoperability withDiscourse, pages 178?186.R.
Chandrasekar, Christine Doran, and B. Srinivas.1996.
Motivations and methods for text simplifica-tion.
In Proceedings of the Sixteenth InternationalConference on Computational Linguistics (COLING?96, pages 1041?1044.Ann Copestake and Dan Flickinger.
2000.
Anopen source grammar development environment andbroad-coverage English grammar using HPSG.
InProceedings of LREC 2000, pages 591?600.Ann Copestake, Dan Flickinger, Carl Pollard, andIvan A.
Sag.
2005.
Minimal recursion semantics:An introduction.
Research on Language and Com-putation, 3(2):281?332.Ann Copestake, Guy Emerson, Michael Wayne Good-man, Matic Horvat, Alexander Kuhnle, and EwaMuszy?nska.
2016.
Resources for building applica-tions with Dependency Minimal Recursion Seman-tics.
In Proceedings of the Tenth Language Re-sources and Evaluation Conference (LREC ?16).
Inpress.Ann Copestake.
2009.
Slacker semantics: Why su-perficiality, dependency and avoidance of commit-ment can be the right way to go.
In Proceedingsof the 12th Conference of the European Chapter ofthe ACL (EACL 2009), pages 1?9, Athens, Greece,March.
Association for Computational Linguistics.Dan Flickinger, Stephan Oepen, and Gisle Ytrest?l.2010.
WikiWoods: syntacto-semantic annotationfor English Wikipedia.
In Nicoletta Calzolari (Con-ference Chair), Khalid Choukri, Bente Maegaard,Joseph Mariani, Jan Odijk, Stelios Piperidis, MikeRosner, and Daniel Tapias, editors, Proceedingsof the Seventh International Conference on Lan-guage Resources and Evaluation (LREC?10), Val-letta, Malta, may.
European Language ResourcesAssociation (ELRA).Dan Flickinger.
2000.
On building a more efficientgrammar by exploiting types.
Natural LanguageEngineering, 6(1):15?28.Matic Horvat, Ann Copestake, and William Byrne.2015.
Hierarchical Statistical Semantic Realizationfor Minimal Recursion Semantics.
In Proceedingsof the International Conference on ComputationalSemantics (IWCS 2015).Ani Nenkova, Rebecca Passonneau, and KathleenMcKeown.
2007.
The pyramid method: Incorporat-ing human content selection variation in summariza-tion evaluation.
ACM Trans.
Speech Lang.
Process.,4(2), May.Stephan Oepen, Dan Flickinger, Kristina Toutanova,and ChristopherD.
Manning.
2004.
LinGO Red-woods.
Research on Language and Computation,2(4):575?596.Marek Rei.
2013.
Minimally supervised dependency-based methods for natural language processing.Technical Report 840, Computer Laboratory, Uni-versity of Cambridge.
PhD thesis.Advaith Siddharthan.
2010.
Complex lexico-syntacticreformulation of sentences using typed dependencyrepresentations.
In Proceedings of the 6th Inter-national Natural Language Generation Conference,INLG ?10, pages 125?133, Stroudsburg, PA, USA.Association for Computational Linguistics.Cambridge SMT system: source sentence chop-ping.
http://ucam-smt.github.io/tutorial/basictrans.html#chopping.Accessed: 2016-04-26.Simone Teufel and Hans Van Halteren.
2004.
Evaluat-ing information content by factoid analysis: Humanannotation and stability.
In Proceedings of Confer-ence on Empirical Methods on Natural LanguageProcessing (EMNLP), pages 419?426.Erik F. Tjong, Kim Sang, and Herv?e D?ejean.
2001.
In-troduction to the CoNLL-2001 shared task: Clauseidentification.
In Proceedings of the 2001 Work-shop on Computational Natural Language Learning- Volume 7, ConLL ?01, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Daniel J. Walker, David E. Clements, Maki Darwin,and Jan W. Amtrup.
2001.
Sentence boundary de-tection: A comparison of paradigms for improvingMT quality.
In roceedings of MT Summit VIII: San-tiago de Compostela, pages 18?22.Kristian Woodsend and Mirella Lapata.
2011.
Learn-ing to simplify sentences with quasi-synchronousgrammar and integer programming.
In Proceedingsof the Conference on Empirical Methods in Natu-ral Language Processing, EMNLP ?11, pages 409?420, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Gisle Ytrest?l, Dan Flickinger, and Stephan Oepen.2009.
Extracting and annotating Wikipedia sub-domains - towards a new eScience community re-source.
In Proceedings of the Seventh InternationalWorkshop on Treebanks and Linguistic Theories(TLT 7), pages 185?197, Groningen, The Nether-lands.99
