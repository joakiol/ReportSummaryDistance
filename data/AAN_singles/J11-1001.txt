SquibsNouveau-ROUGE: A Novelty Metric forUpdate SummarizationJohn M. Conroy?IDA/Center for Computing SciencesJudith D. Schlesinger?IDA/Center for Computing SciencesDianne P.
O?Leary?
?University of MarylandAn update summary should provide a fluent summarization of new information on a time-evolving topic, assuming that the reader has already reviewed older documents or summaries.In 2007 and 2008, an annual summarization evaluation included an update summarizationtask.
Several participating systems produced update summaries indistinguishable from human-generated summaries when measured using ROUGE.
However, no machine system performednear human-level performance in manual evaluations such as pyramid and overall responsive-ness scoring.We present a metric called Nouveau-ROUGE that improves correlation with manualevaluation metrics and can be used to predict both the pyramid score and overall responsivenessfor update summaries.
Nouveau-ROUGE can serve as a less expensive surrogate for manualevaluations when comparing existing systems and when developing new ones.1.
IntroductionUpdate summaries focus on what is new relative to a previous body of information.They pose new challenges both to algorithm developers and to evaluation of sum-maries.
In 2007, DUC (Document Understanding Conference) introduced an updatesummarization task, repeated in 2008 for TAC (Text Analysis Conference).1 This taskconsisted of producing a multi-document summary for a set of articles on a single topic,followed by one (2008) or two (2007) multi-document summaries for sets of articles on?
Institute for Defense Analyses, Center for Computing Sciences, 17100 Science Drive, Bowie, MD 20715USA.
E-mail: {judith,conroy}@super.org.??
Computer Science Department, Institute for Advanced Computer Studies, University of Maryland,College Park, MD 20742 USA.
E-mail: oleary@cs.umd.edu.1 DUC (http://duc.nist.gov), the summarization evaluation event, was replaced in 2008 by TAC(http://www.nist.gov/tac).
Both were sponsored by NIST, the U.S. National Institute of Standardsand Technology.Submission received: 14 January 2010; revised submission received: 15 May 2010; accepted for publication:27 September 2010.?
2011 Association for Computational LinguisticsComputational Linguistics Volume 37, Number 1the same topic published at later dates.
The goal was to generate a good first summary,along with update(s) that contained new content and minimized redundancy.The modifier manual is used to identify evaluations, and the corresponding scores,produced by humans.
The modifier automatic is used to identify evaluations, andthe corresponding scores, produced by machines.
Similarly, human-generated andmachine-generated will be used to distinguish between summaries created by humansand those generated by machine systems, respectively.2Because we are working with update summarization, there is a minimum of twosummaries for a set of documents.
The first summary for the document set is called theoriginal (Task A) summary and a later summary is called an update (Task B) summary.Several machine summarizing systems produced update summaries that werestatistically indistinguishable from human-generated summaries, as measured by theROUGE metrics, the standard metrics for automatic evaluation of summaries.
However,none of these machine systems performed near human levels in overall responsivenessor pyramid evaluation, the currently used manual evaluation metrics.We define the metric gap (or gap) as the distance between a prediction of a manualscore, based on automatic scores, and the observed manual score.The purpose of our work is to investigate and mitigate this metric gap by in-troducing an automatic evaluation that is a better predictor of manual evaluation.Reducing the metric gap is important for two reasons.
First, the gap severely limits theusefulness of automatic evaluation and forces the use of much more expensive manualevaluation for comparing existing systems.
More importantly, however, this gap is asevere handicap to research on new update summarization methods because it makesit difficult to evaluate new ideas and compare them with existing methods.In Section 2, we analyze the results of the TAC 2008 summarization task, demon-strating the large gap between ROUGE automatic metrics and manual evaluation ofupdate summaries.
In Section 3, we modify ROUGE to produce scores that correlatesignificantly better with manual evaluation.
We evaluate our new metric on TAC 2008data in Section 4, demonstrating its superiority as a predictor of manual evaluations.2.
State-of-the-Art Evaluation of Update SummariesTAC 2008 presented 48 20-document sets, with 10 documents in each of two subsets, Aand B. Subset B documents were more recent.
Original summaries were generated forthe A subsets and update summaries were then produced for the B subsets.In TAC 2008, ROUGE was used for automatic evaluation.
ROUGE (Lin and Hovy2000) compares any summary to any other (typically human-generated) summary usinga recall-oriented approach.
ROUGE-1 and -2 are based on unigrams and bigrams,respectively; ROUGE-SU4 uses bigrams with a maximum skip distance of 4 betweenbigrams; ROUGE-BE (Hovy, Lin, and Zhou 2005) is an n-gram approach based on basicelements, computed via parsing or automatic entity recognition.
ROUGE-2, ROUGE-SU4, and ROUGE-BE were the official automatic metrics for TAC 2008, used to com-pare machine-generated summaries to human-generated summaries, and to comparehuman-generated summaries to each other using a jackknife approach.
In addition tothe three official metrics, we include ROUGE-1 in our study as it is often competitivewith the official metrics.2 NIST uses ?model?
for human-generated summaries and ?peer?
for machine-generated summaries.2Conroy, Schlesinger, and O?Leary Nouveau-ROUGEThree manual evaluation metrics were used in TAC 2008: pyramid, overall respon-siveness, and linguistic quality (not considered in our work).
The pyramid method(Nenkova and Passonneau 2004) is a content-based metric for which human annotatorsmark content units in the human-generated summaries.
The content units are collectedacross a set of human-generated summaries for a topic, and a weight is computedbased on how many human-generated summaries include this content unit.
TAC 2008also used a manual overall responsiveness score.
After evaluating data from 2005?2007 (Dang 2007; Conroy and Dang 2008), NIST decided that this score, which eval-uates summary usefulness including linguistic quality, is a reliable and stable manualevaluation.We analyzed the three official TAC 2008 automatic evaluation scores to see howwell they predict the manual evaluation metrics of overall responsiveness and pyramidscore.
Figure 1 shows scatter plots of responsiveness and pyramid scores vs. the threeofficial ROUGE measures for the TAC 2008 update task.
Each solid data point representsthe average score for a human summarizer over 24 document sets, and a dashed linemarks the minimum; each open data point represents the average score for a machinesystem.
We also show robust linear least squares fits to the data as well as the Pearsoncorrelation coefficients between ROUGE-BE, -2, and -SU4 and the manual-evaluationscores.
Surprisingly, these correlations are higher for the update task than for theoriginal summarization task (data not shown); nevertheless, the gap between the lines?predictions and the scores for the human-generated summaries is larger.Figure 1TAC 2008: The update task (Task B) responsiveness and pyramid scores vs. ROUGE scores;human-generated summaries (solid points) and machine-generated summaries (open points).3Computational Linguistics Volume 37, Number 1We report correlation coefficients only for the machine-generated summaries.
Scoresfor the human-generated summaries are distributed differently, and correlation for theset of human-generated summaries is often not significant due to the small number ofhuman summarizers.The correlation coefficients in Figure 1 show that the automatic metrics do wellin predicting responsiveness and pyramid scoring for machine-generated summaries.
Incontrast, the scores for human-generated summaries far exceed the predictions, witha large gap between predicted and actual scores.
As may be expected, ROUGE is morehighly correlated with the pyramid evaluation, which is a pure content evaluation score,whereas the responsiveness score also reflects linguistic quality.3.
Improving Automatic Evaluation?Nouveau-ROUGEWe more formally define the metric gap to be the absolute value of the difference betweena manual evaluation score and our prediction of it based only on automatic evaluationscores.
A number of TAC 2008 machine systems performed within statistical confidenceof human performance in the automatic evaluation metrics, but no system performednear human performance in the manual evaluations.
This has also been observed inprevious summarization evaluations (Conroy and Dang 2008).
Progress has been madein closing this metric gap but it persists, especially for update summaries.A good update summary must contain essential information but focus on newinformation.
When a machine-generated update summary is good, it is similar to thehuman-generated update summaries.
This is assessed quite well by a ROUGE score.
But themachine-generated update summary should also be different from the human-generatedoriginal summaries, and we need an automatic metric to assess this difference, or lack ofredundancy.
We suggest using a ROUGE score to measure similarity, and thus redun-dancy, between a given original summary and an update summary: A high ROUGEscore indicates high redundancy.To illustrate this, we used the CLASSY algorithm (Conroy, Schlesinger, and O?Leary2006; Schlesinger, O?Leary, and Conroy 2008) to produce original summaries and updatesummaries for the TAC 2008 data.
We also produced update summaries using a variant,projected-CLASSY, that reduces overlap by using a linear algebra projection of theterm-sentence matrix (Conroy, Schlesinger, and O?Leary 2007) of candidate sentencesagainst the matrix for the original (Task A) summary in order to favor new infor-mation.
Table 1 gives average ROUGE-2 scores and 95% confidence intervals, com-puted via bootstrapping (Efron and Tibshirani 1993), over the 48 document sets.Two scores are given: R(BB)2 compares each CLASSY update (Task B) summary to thehuman-generated summaries, and R(AB)2 compares each to the original (Task A) modelsummaries.
Whereas the two variants score comparably using R(BB)2 , there is a significantdifference in the R(AB)2 metric, as desired.Table 1TAC 2008: Average ROUGE-2 scores and 95% confidence intervals for update summariesproduced by two variants of CLASSY.Variation R(BB)2 R(AB)2projected-CLASSY 0.087 (0.080, 0.094) 0.075 (0.070, 0.079)CLASSY 0.089 (0.082, 0.096) 0.083 (0.078, 0.088)4Conroy, Schlesinger, and O?Leary Nouveau-ROUGETable 2TAC 2008: Nouveau-ROUGE ?-parameters.Predicting Responsiveness Predicting Pyramid Scores?i,0 ?i,1 ?i,2 ?i,0 ?i,1 ?i,2R1 ?0.0271 ?7.3550 13.4227 ?0.2143 ?1.9011 3.1118R2 0.9126 ?5.4536 21.1556 ?0.0143 ?1.3499 4.3778RSU4 1.1381 ?2.6931 35.8555 0.0346 ?1.1680 7.2589RBE 1.0602 ?5.0811 24.8365 0.0145 ?1.3156 5.0446Given this evidence, we propose predicting manual scores for update summaries byusing two ROUGE scores, R(AB)i and R(BB)i (i = 1, 2, SU4, ...), in a three-parameter modelcalled Nouveau-ROUGE:Ni = ?i,0 + ?i,1R(AB)i + ?i,2R(BB)iWe determine the ?
parameters (Table 2) using robust linear regression on the TAC 2008evaluation data so that the Nouveau-ROUGE score Ni best predicts the manual scoresof responsiveness and pyramid performance.Nouveau-ROUGE could be used by researchers to predict how a new system wouldcompare with the TAC 2008 systems in overall responsiveness and pyramid scoring, acomparison that up to now has been impossible.4.
Evaluating Nouveau-ROUGEWe evaluate Nouveau-ROUGE using cross validation studies to demonstrate that ifmanual scores are available for a subset of summaries (in this case, those from 29machine systems, half of those that participated in TAC 2008), then Nouveau-ROUGEcan predict manual scores for the remaining (held-back) summaries.4.1 Improved Correlation with Manual EvaluationCorrelation scores between automatic and manual scores have traditionally been usedas a measure of the effectiveness of automatic evaluation as a surrogate for manualevaluation.
Pearson correlation coefficients, shown in Table 3, were computed for thescores for the held-back subset of summaries.
Correlation is indeed higher for theNouveau-ROUGE scores than for any of the ROUGE scores.Table 3Correlation scores for TAC 2008 human evaluations.Average Responsiveness Score Average Pyramid ScoreMetric i = 1 i = 2 i = SU4 i = BE i = 1 i = 2 i = SU4 i = BER(AB)i 0.676 0.576 0.619 0.490 0.698 0.592 0.634 0.483R(BB)i 0.870 0.921 0.902 0.933 0.910 0.952 0.933 0.964Ni 0.888 0.929 0.912 0.935 0.946 0.961 0.951 0.9695Computational Linguistics Volume 37, Number 1Figure 2TAC 2008: ROUGE and Nouveau-ROUGE responsiveness and pyramid predictions forsubtask B.6Conroy, Schlesinger, and O?Leary Nouveau-ROUGETable 4TAC 2008: Median Pearson correlation coefficients for automatic vs. manual evaluations.Average Responsiveness Score Jackknife Pyramid ScoreMetric R(AB)i R(BB)i Ni p-value R(AB)i R(BB)i Ni p-valueR1 0.378 0.804 0.920 5.4e-284 0.406 0.837 0.943 3.5e-307R2 0.149 0.889 0.925 1.6e-104 0.177 0.909 0.941 1.8e-99RSU4 0.267 0.846 0.913 1.3e-176 0.291 0.875 0.933 8.7e-214RBE 0.222 0.913 0.919 6.2e-09 0.243 0.924 0.933 1.7e-17Figure 2 shows that ROUGE-BE and ROUGE-1 predictions of both responsivenessand pyramid scores are inferior to the Nouveau-ROUGE-BE predictions.
Plots for N2and NSU4 are omitted due to space restrictions, but performance improvement relativeto ROUGE is greater than that for NBE and less than that for N1.4.2 Validation Using Bootstrapping ExperimentsTo show that our results are not due to a lucky partitioning of the data, we usedbootstrapping (Efron and Tibshirani 1993), a resampling method, which allows us tocompute our statistical confidence in the results.
This model assumes that observed data(scores for the 58 systems) characterize all data.
Given this model, the proper samplingmethod is to choose subsets with replacement.
We chose 58 systems (with replacement)and used half to determine the Nouveau-ROUGE parameters and half to test the model.We repeated this process 1,000 times.
Table 4 gives the correlation coefficients (for thetested-half of the data) for all four ROUGE metrics with each of the manual evalua-tions when comparing the machine-generated summaries with the human-generatedsummaries.
Data in the columns labeled ?p-value?
result from a Mann-Whitney U-test for equal medians of R(BB)i and Ni of the distributions of correlations returned bythe bootstrapping procedure.
Because all p-values are small, we can conclude that thedifferences between the R(BB)i and Ni correlation scores are statistically significant for allvariants of ROUGE.4.3 Narrowing the Gap for Update SummariesTable 5 gives the median gap on the TAC 2008 data for predicting responsiveness andpyramid scores.
Recall that the gap is the absolute value of the difference between theTable 5Narrowing the TAC 2008 metric gap.Responsiveness Metric Gap Pyramid Metric GapMetric R Gap N Gap p-value R Gap N Gap p-valueR1 2.025 1.277 7.8e-03 0.285 0.187 7.8e-03R2 1.655 1.518 7.8e-03 0.241 0.197 7.8e-03RSU4 1.887 1.344 7.8e-03 0.273 0.206 7.8e-03RBE 1.591 1.547 7.8e-03 0.229 0.206 7.8e-037Computational Linguistics Volume 37, Number 1manual score and the prediction of it.
The median gap is always smaller for Nouveau-ROUGE than for ROUGE; in fact, the gap is smaller on every trial.We used the Wilcox sign test to test the significance of this observation.
The nullhypothesis is that the differences in the gaps between ROUGE and Nouveau-ROUGEhas median 0.
The p-values from the Wilcox test indicate that the null hypothesis is truewith probability 1128 ?
7.812 ?
10?3, so it can be safely rejected.5.
ConclusionOur new metric, Nouveau-ROUGE, includes a measure of novelty for an update sum-mary.
We demonstrated that it has higher correlation to manual evaluation of overallresponsiveness and to pyramid scores than does ROUGE.
The most obvious deficiencyin ROUGE is the lack of a linguistic quality measurement, which we take to encompass alllanguage-related issues: lexical, syntactic, and semantic.
Therefore, we conjecture thatmost remaining prediction error in Nouveau-ROUGE is a result of omitting linguisticquality and caution that better prediction would be achieved only for systems ofcomparable linguistic quality.We believe that responsiveness is an imperfect surrogate for task-based summaryevaluation such as that done in SUMMAC (Mani et al 1999).
We would welcomea return to task-based evaluation, as well as research increasing the reliability andconsistency of manual evaluation metrics.
Investigation could also quantify the impactof low responsiveness and pyramid scores on the ability to perform a specific task.ReferencesConroy, John M. and Hoa Trang Dang.
2008.Mind the gap: Dangers of divorcingevaluations of summary content fromlinguistic quality.
In Proceedings of the 22ndInternational Conference on ComputationalLinguistics (Coling 2008), pages 145?152,Manchester.Conroy, John M., Judith D. Schlesinger, andDianne P. O?Leary.
2006.
Topic-focusedmulti-document summarization using anapproximate oracle score.
In Proceedings ofthe COLING/ACL 2006 Main ConferencePoster Sessions, pages 152?159, Sydney.Conroy, John M., Judith D. Schlesinger, andDianne P. O?Leary.
2007.
CLASSY 2007 atDUC 2007.
In Proceedings of the SeventhDocument Understanding Conference (DUC),Rochester, NY.
Available at http://duc.nist.gov/pubs.html#2007.Dang, Hoa Trang.
2007.
Overview of DUC2007.
In Proceedings of the Seventh DocumentUnderstanding Conference (DUC), Rochester,NY.
Available at http://duc.nist.gov/pubs.html#2005.Efron, B. and R. J. Tibshirani.
1993.
AnIntroduction to the Bootstrap.
Chapman& Hall, New York.Hovy, Eduard, Chin-Yew Lin, and LiangZhou.
2005.
Evaluating DUC 2005 usingbasic elements.
In Proceedings of the FifthDocument Understanding Conference (DUC),Vancouver.
Available at www-nlpir/nist.gov/projects/duc/pubs.html.Lin, Chin-Yew and Eduard Hovy.
2000.
Theautomated acquisition of topic signaturesfor text summarization.
In Proceedingsof the 18th Conference on ComputationalLinguistics, pages 495?501, Morristown, NJ.Mani, Inderjeet, Therese Firmin, DavidHouse, Gary Klein, Beth Sundheim, andLynette Hirschman.
1999.
The TIPSTERSUMMAC text summarization evaluation.In Proceedings of EACL?99: Ninth Conferenceof the European Chapter of the Association forComputational Linguistics, pages 77?85,Bergen.Nenkova, Ani and Rebecca Passonneau.2004.
Evaluating content selection insummarization: The pyramid method.In Proceedings of the Human LanguageTechnology Conference of the NorthAmerican Chapter of the Association forComputational Linguistics, pages 145?152,Boston, MA.Schlesinger, Judith D., Dianne P. O?Leary,and John M. Conroy.
2008.
Arabic/English multi-document summarizationwith CLASSY?The past and the future.In Alexander F. Gelbukh, editor, CICLing,volume 4919 of Lecture Notes in ComputerScience.
Springer, Haifa, pages 568?581.8
