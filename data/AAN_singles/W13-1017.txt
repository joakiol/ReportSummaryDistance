Proceedings of the 9th Workshop on Multiword Expressions (MWE 2013), pages 116?125,Atlanta, Georgia, 13-14 June 2013. c?2013 Association for Computational LinguisticsCombining Different Features of Idiomaticity for the AutomaticClassification of Noun+Verb Expressions in BasqueAntton GurrutxagaElhuyar FoundationZelai Haudi 3, Osinalde industrialdeaUsurbil 20170.
Basque Countrya.gurrutxaga@elhuyar.comIn?aki AlegriaIXA group, Univ.
of the Basque CountryManuel Lardizabal 1Donostia 20018.
Basque Countryi.alegria@ehu.esAbstractWe present an experimental study of how dif-ferent features help measuring the idiomatic-ity of noun+verb (NV) expressions in Basque.After testing several techniques for quantify-ing the four basic properties of multiword ex-pressions or MWEs (institutionalization, se-mantic non-compositionality, morphosyntac-tic fixedness and lexical fixedness), we testdifferent combinations of them for classifica-tion into idioms and collocations, using Ma-chine Learning (ML) and feature selection.The results show the major role of distribu-tional similarity, which measures composi-tionality, in the extraction and classificationof MWEs, especially, as expected, in the caseof idioms.
Even though cooccurrence andsome aspects of morphosyntactic flexibilitycontribute to this task in a more limited mea-sure, ML experiments make benefit of thesesources of knowledge, allowing to improvethe results obtained using exclusively distribu-tional similarity features.1 IntroductionIdiomaticity is considered the defining feature of theconcept of multiword expressions (MWE).
It is de-scribed as a non-discrete magnitude, whose ?value?depends on a combination of features like in-stitutionalization, non-compositionality and lexico-syntactic fixedness (Granger and Paquot, 2008).Idiomaticity appears as a continuum rather than asa series of discrete values.
Thus, the classification ofMWEs into discrete categories is a difficult task.
Avery schematic classification that has achieved a fairdegree of general acceptance among experts distin-guishes two main types of MWEs at phrase-level:idioms and collocations.This complexity of the concept of idiomaticity hasposed a challenge to the development of methodsaddressing the measurement of the aforementionedfour properties.
Recent research has resulted inthis issue nowadays being usually addressed throughmeasuring the following phenomena: (i) cooccur-rence, for institutionalization; (ii) distributional sim-ilarity, for non-compositionality; (iii) deviation fromthe behavior of free combinations, for morphosyn-tactic fixedness; and (iv) substitutability, for lexicalfixedness.
This is the broad context of our experi-mental work on the automatic classification of NVexpressions in Basque.2 Related Work2.1 Statistical Idiosyncrasy orInstitutionalizationUsing the cooccurrence of the components of a com-bination as a heuristic of its institutionalization goesback to early research on this field (Church andHanks, 1990), and is computed using associationmeasures (AM), usually in combination with lin-guistic techniques, which allows the use of lemma-tized and POS-tagged corpora, or the use of syntac-tic dependencies (Seretan, 2011).
In recent years,the comparative analysis of AMs (Evert, 2005) andthe combination of them (Lin et al 2008; Pecina,2010) have aroused considerable interest.This approach has been recently explored inBasque (Gurrutxaga and Alegria, 2011).1162.2 CompositionalityThe central concept in characterizing compositional-ity is the hypothesis of distributional similarity (DS)As proposed by Baldwin and Kim (2010), ?the un-derlying hypothesis is that semantically idiomaticMWEs will occur in markedly different lexical con-texts to their component words.
?Berry-Rogghe (1974) proposed R-value to mea-sure the compositionality of verb-particle construc-tions (VPCs), by dividing the overlap between thesets of collocates associated with the particle bythe total number of collocates of the VPC.
Wulff(2010) proposes two extensions to the R-valuein her research on verb-preposition-noun construc-tions, combining and weighting in different ways in-dividual R-values of each component.The Vector Space Model (VSM) is applied,among others, by Fazly and Stevenson (2007), whouse the cosine as a similarity measure.
The sharedtask Distributional Semantics and Compositionality(DiSCo) at ACL-HLT 2011 shows a variety of tech-niques for this task, mainly association measuresand VSM (Biemann and Giesbrecht, 2011).
LSA(Latent Semantic Analysis) is used in several stud-ies (Baldwin et al 2003; Katz and Giesbrecht, 2006;Schone and Jurafsky, 2001).Those approaches have been applied recently toBasque (Gurrutxaga and Alegria, 2012)2.3 Morphosyntactic Flexibility (MSFlex)Morphosyntactic fixedness is usually computed interms of relative flexibility, as the statistical dis-tance between the behavior of the combination and(i) the average behavior of the combinations withequal POS composition (Fazly and Stevenson, 2007;Wulff, 2010), or (ii) the average behavior of thecombinations containing each one of the compo-nents of the combination (Bannard, 2007).Fazly and Stevenson (2007) use Kullback-Leibler divergence (KL-div) to compute this dis-tance.
They analyze a set of patterns: determination(a/the), demonstratives, possessives, singular/pluraland passive.
They compute two additional measure-ments (dominant pattern and presence of absence ofadjectival modifiers preceding the noun).Wulff (2010) considers (i) tree-syntactic, (ii)lexico-syntactic and (iii) morphological flexibilities,and implements two metrics for these features: (i) anextension of Barkema proposal (NSSD, normalizedsum of squared deviations), (ii) a special conceptionof ?relative entropy?
(Hrel).Bannard (2007), using CPMI (conditional point-wise mutual information), analyses these variants:(i) variation, addition or dropping of a determiner;(ii) internal modification of the noun phrase; and (iii)verb passivation.2.4 Lexical Flexibility (LFlex)The usual procedure for measuring lexical flexibilityis to compute the substitutability of each componentof the combination using as substitutes its synony-mous, quasi-synonyms, related words, etc.The pioneering work in this field is Lin (1999),who uses a thesaurus automatically built from text.This resource is used in recent research (Fazly andStevenson, 2007).
They assume that the target pairis lexically fixed to the extent that its PMI deviatesfrom the average PMI of its variants generated bylexical substitution.
They compute flexibility usingthe z-score.In Van de Cruys and Moiro?n (2007), a techniquebased on KL-div is used for Duch.
They define Rnvas the ratio of noun preference for a particular verb(its KL-div), compared to the other nouns that arepresent in the cluster of substitutes.
Similarly forRvn.
The substitute candidates are obtained fromthe corpus using standard distributional similaritytechniques.2.5 Other MethodsFazly and Stevenson (2007) consider two other fea-tures: (i) the verb itself; and (ii) the semantic cate-gory of the noun according to WordNet.2.6 Combined SystemsIn order to combine several sources of knowledge,several studies have experimented with using Ma-chine Learning methods (ML).For Czech, Pecina (2010) combines only AMs us-ing neural networks, logistic regression and SVM(Support Vector Machine).
Lin et al(2008) employlogistic linear regression model (LLRM) to combinescores of AMs.Venkatapathy and Joshi (2005) propose a mini-mally supervised classification scheme that incorpo-117rates a variety of features to group verb-noun combi-nations.
Their features drawn from AM and DS, butsome of each type are tested and combined.
Theycompute ranking correlation using SVM, achievingresults of about 0.45.Fazly and Stevenson (2007) use all the types ofknowledge, and decision trees (C5.0) as a learningmethod, and achieve average results (F-score) nearto 0.60 for 4 classes (literal, abstract, light verbs andidioms).
The authors claim that the syntactic andcombined fixedness measures substantially outper-form measures of collocation extraction.3 Experimental Setup3.1 Corpus and PreprocessingWe use a journalistic corpus of 75 million words(MW) from two sources: (1) Issues publishedin 2001-2002 by the newspaper EuskaldunonEgunkaria (28 MW); and (2) Issues published in2006-2010 by the newspaper Berria (47 MW).The corpus is annotated with lemma, POS, finegrained POS (subPOS), case and number informa-tion using Eustagger developed by the IXA group ofthe University of the Basque Country.
A precision of95.42% is reported for POS + subPOS + case analy-sis (Oronoz et al 2010).3.2 Extraction of Bigram CandidatesThe key data for defining a Basque NV bigram arelemma and case for the noun, and lemma for theverb.
Case data is needed to differentiate, for exam-ple, kontu hartu (?to ask for an explanation?)
fromkontuan hartu (?to take into account?
), where kontuis a noun lemma in the inessive case.In order to propose canonical forms, we need, fornouns, token, case and number annotations in bi-gram data.
Those canonical forms can be formulatedusing number normalization, as described in Gur-rutxaga and Alegria (2011).
Bigrams belonging tothe same key noun lemma/noun case+verb lemmaare normalized; a single bigram with the most fre-quent form is created, and the frequencies of bi-grams and those of the noun unigrams summed.We use the Ngram Statistics Package-NSP (Banerjee and Pedersen, 2010) to generate NVbigrams from a corpus generated from the output ofEustagger.
Taking into account our previous results(Gurrutxaga and Alegria, 2011), we use a windowspan of ?1 and a frequency threshold of f > 30.Before generation, some surface-grammar rules areapplied to correct annotations that produce noise.For example, in most Basque AdjN combinations,the adjetive is a verb in a participe form (eg.
indararmatuak, ?armed forces?).
Similarly, those kindof participles can function as nouns (gobernuarenaliatuak, ?the allies of the government?).
Nottagging those participles properly would introducenoise in the extraction of NV combinations.3.3 Experiments Using Single KnowledgeSources3.3.1 CooccurrenceThe cooccurrence data provided by NSP in the bi-gram extraction step is processed to calculate AMs.To accomplish this, we use Stefan Evert?s UCStoolkit (Evert, 2005).
The most common AMs arecalculated: f , t-score, log-likelihood ratio, MI, MI3,and chi-square (?2).3.3.2 Distributional SimilarityThe idea is to compare the contexts of each NVbigram with the contexts of its corresponding com-ponents, by means of different techniques.
Themore similar the contexts, the more compositionalthe combination.Context Generation We extract the context wordsof each bigram from the sentences with contiguouscooccurrences of the components.
The noun has tooccur in the grammatical case in which it has beendefined after bigram normalization.The contexts of the corresponding noun and verbare extracted separately from sentences where theydid not occur together.
Only content-bearing lem-mas are included in the contexts (nouns, verbs andadjectives).Context Comparison We process the contexts intwo different ways:First, we construct a VSM model, representingthe contexts as vectors.
As similarity measures, weuse Berry-Roghe?s R-value (RBR) and the two ex-tensions to it proposed by Wulff (RW1 and RW2),Jaccard index and cosine.
For the cosine, differentAMs have been tested for vector weights (f , t-score,118LLR and PMI).
We experiment with different per-centages of the vector and different numbers of col-locates, using the aforementioned measures to rankthe collocates.
The 100 most frequent words in thecorpus are stopped.Second, we represent the same contexts as doc-uments, and compare them by means of differ-ent indexes using the Lemur Toolkit (Allan et al2003).
The contexts of the bigrams are used asqueries against a document collection containing thecontext-documents of all the members of the bi-grams.
This can be implemented in different ways;the best results were obtained using the following:?
Lemur 1 (L1): As with vectors, the contexts ofa bigram are included in a single query docu-ment, and the same is done for the contexts ofits members?
Lemur 2 (L2): The context sentences of bi-grams are treated as individual documents, butthe contexts of each one of its members are rep-resented in two separate documentsDue to processing reasons, the number of contextsentences used in Lemur to generate documents islimited to 2,000 (randomly selected from the wholeset of contexts).We further tested LSA (using Infomap1), but theabove methods yielded better results.3.3.3 Morphosyntactic FlexibilityWe focus on the variation of the N slot, dis-tinguishing the main type of extensions and num-ber inflections.
Among left-extensions, we takeinto account relative clauses.
In addition, we con-sider the order of components as a parameter.
Wepresent some examples of the free combination libu-rua irakurri (?to read a book?)?
Determiner: liburu bat irakurri dut (?I haveread one book?
), zenbat liburu irakurri dituzu?
(?how many books have you read??)?
Postnominal adjective: liburu interesgarriairakurri nuen (?I read an interesting book?)?
Prenominal adjective: italierazko liburuairakurri (?to read a book in Italian?)1http://infomap-nlp.sourceforge.net/?
Relative clause: irakurri dudan liburua (?thebook I have read?
), anaiak irakurritako liburubatzuk (?some books read by my brother?)?
Number inflection: liburua/liburuak/liburu/liburuok irakurri (?to reada/some/?/these book(s)?)?
Order of components (NV / VN): liburuairakurri dut / irakurri dut liburua (?I have reada book?
)We count the number of variations for each bi-gram, for all NV bigrams, and for each combinationof the type bigram component+POS of the othercomponent (e.g, for liburua irakurri, the variationsof all the combinations liburua+V and N+irakurri).To calculate flexibility, we experiment with all themeasures described in section 2.3: Fazly?s KL-div,Wulff?s NSSD and Hrel (relative entropy), and Ban-nard?s CPMI.3.3.4 Lexical FlexibilityIn order to test the substitutability of the compo-nents of bigrams, we use two resources: (i) ELH:Sinonimoen Kutxa, a Basque dictionary of syn-onyms, published by the Elhuyar Foundation (fornouns and verbs, 40,146 word-synomyn pairs); (ii)WN: the Basque version of WordNet2(68,217 word-synomyn pairs).
First, we experimented with bothresources on their own, but the results show thatin many cases there either was no substitute candi-date, or the corpus lacked combinations containinga substitute.
In order to ensure a broader coverage,we combined both resources (ELHWN), and we ex-panded the set of substitutes including the siblingsretrieved from Basque WordNet (ELHWNexpand).To calculate flexibility, we experiment with thetwo measures described in section 2.4: z-score andKL-div based R.3.4 Combining Knowledge Sources UsingMachine LearningWe use some ML methods included in the Wekatoolkit (Hall et al 2009) in order to combine re-sults obtained in experiments using single knowl-edge sources (described in section 3.3).
The values2http://ixa2.si.ehu.es/cgi-bin/mcr/public/wei.consult.perl119of the different measures obtained in those experi-ments were set as features.We have selected five methods corresponding todifferent kind of techniques which have been usedsuccessfully in this field: Naive Bayes, C4.5 deci-sion tree (j48), Random Forest, SVM (SMO algo-rithm) and Logistic Regression.
Test were carriedout using either all features, the features from eachtype of knowledge, and some subsets, obtained af-ter manual and automatic selection.
Following Fa-zly and Stevenson (2007), verbs are also included asfeatures.Since, as we will see in section 3.5, the amount ofinstances in the evaluation dataset is not very high(1,145), cross-validation is used in the experimentsfor model validation (5 folds).
In the case of auto-matic attribute selection, we use AttributeSelected-Classifier, which encapsulates the attribute selectionprocess with the classifier itself, so the attribute se-lection method and the classifier only see the data inthe training set of each fold.3.5 Evaluation3.5.1 Reference Dataset and HumanJudgmentsAs an evaluation reference, we use a subset of1,200 combinations selected randomly from a ex-tracted set of 4,334 bigrams, that is the result ofmerging the 2,000-best candidates of each AM rank-ing from the w = ?1 and f > 30 extraction set.The subset has been manually classified by threelexicographers into idioms, collocations and freecombinations.
Annotators were provided with anevaluation manual, containing the guidelines forclassification and illustrative examples.The agreement among evaluators was calculatedusing Fleiss?
?.
We obtained a value of 0.58, whichcan be considered moderate, close to fair, agree-ment.
Although this level of agreement is relativelylow when compared to Krenn et al(2004), it iscomparable to the one reported by Pecina (2010),who attributed his ?relatively low?
value to the factthat ?the notion of collocation is very subjective,domain-specific, and also somewhat vague.?
Streetet al(2010) obtain quite low inter-annotator agree-ment for annotation of idioms in the ANC (Ameri-can National Corpus).
Hence, we consider that thelevel of agreement we have achieved is acceptable.For the final classification of the evaluation set,cases where agreement was two or higher were au-tomatically adopted, and the remaining cases wereclassified after discussion.
We removed 55 combina-tions that did not belong to the NV category, or thatwere part of larger MWEs.
The final set included1,145 items, out of which 80 were idioms 268 collo-cations, and 797 free combinations.3.5.2 ProcedureIn order to compare the results of the individualtechniques, we based our evaluation on the rank-ings provided by each measure.
If we were to havean ideal measure, the set of bigram categories (?id?,?col?
and ?free?)
would be an ordered set, with ?id?values on top of the ranking, ?col?
in the middle, and?free?
at the bottom.
Thus, the idea is to computethe distance between a rank derived from the ideallyordered set, which contains a high number of ties,and the rank yielded by each measure.
To this end,we use Kendall?s ?B as a rank-correlation measure.Statistical significance of the Kendall?s ?B correla-tion coefficient is tested with the Z-test.
The realistictopline, yielded by a measure that ranks candidatesideally, but without ties, would be 0.68.In addition, average precision values (AP) werecalculated for each ranking.In the case of association measures, similaritymeasures applied to VSM, and measures of flexibil-ity, the bigrams were ranked by means of the val-ues of the corresponding measure.
In the case of ex-periments with Lemur, the information used to rankthe bigrams consisted of the positions of the docu-ments corresponding to each member of the bigramin the document list retrieved (?rank?
in Table 1).
Forthe experiments in which the context sentences havebeen distributed in different documents, average po-sitions were calculated and weighted, in relation tothe amount of documents for each bigram analysis(?rank weight?).
The total number of documents inthe list (or ?hits?)
is weighted in the same manner(?hit rel?
).When using ML techniques, several measuresprovided by Weka were analyzed: percentage ofCorrectly Classified Instances (CCI), F-measuresfor each class (id, col, free), Weighted Average F-measure and Average F-measure.120measure ?B AP MWE AP id AP colrandom rank (-0.02542) 0.30879 0.0787 0.23358AMf 0.18853 0.43573 0.07391 0.37851t-score 0.19673 0.45461 0.08442 0.38312log-likelihood 0.15604 0.42666 0.10019 0.33480PMI (-0.12090) 0.25732 0.08648 0.18234chi-squared (-0.03699) 0.30227 0.11853 0.20645DSRBR NV (MI -50%) 0.27034 0.47343 0.21738 0.30519RW1(2000 MI f3 50%) 0.26206 0.47152 0.19664 0.30967L1 Indri rankNV 0.31438 0.53536 0.22785 0.35299L1 KL rankNV 0.29559 0.51694 0.23558 0.33607L2 Indri hit rel NV 0.32156 0.56612 0.29416 0.35389L2 KL hit rel NV 0.30848 0.55146 0.31977 0.33241L2 Indri rankN weight 0.21387 0.45567 0.26148 0.28025L2 Indri rankV weight 0.31398 0.55208 0.12837 0.43143MSFlexHrel Det 0.07295 0.38995 0.12749 0.27704Hrel PostAdj (-0.05617) 0.31673 0.04401 0.29597Hrel PreAdj 0.11459 0.38561 0.09897 0.29223Hrel Rel 0.09115 0.40502 0.12913 0.29012Hrel Num 0.11861 0.43381 0.13387 0.31318Hrel ord (0.02319) 0.31661 0.08124 0.24052CPMI (components) 0.05785 0.41917 0.12630 0.30831LFlexRnv ELHWN (0.08998) 0.36717 0.07521 0.29896Rvn ELHWN (0.03306) 0.31752 0.08689 0.24369z-score V ELHWNexpand 0.10079 0.35687 0.12232 0.25019z-score N ELHWNexpand 0.08412 0.35534 0.07245 0.29005Table 1: Kendall?s ?B rank-correlations relative to an ideal idiomaticity ranking, obtained by different idiomaticitymeasures.
Non-significant values of ?B in parentheses (p > 0.05).
Average precisions for MWEs in general, andspecific values for idioms and collocations.4 Experimental Results4.1 Single Knowledge ExperimentsThe results for Kendall?s ?B and AP for MWEs andseparate AP values for idioms and collocations aresummarized in Table 1 (only the experiments withthe most noteworthy results are included).The best results are obtained in the Lemur exper-iments, most notably in the Lemur 2 type, using ei-ther Indri or KL-div indexes.
In the MWE rankings,measures of the R-value type only slightly outper-form AMs.In the case of idioms, DS measures obtain signif-icantly better ranks than the other measures.
Idiomsbeing the least compositional expressions, his resultis expected, and supports the hypothesis that seman-tic compositionality can better be characterized us-ing measures of DS than using AMs.Regarding collocations, no such claim can bemade, as the AP values for t-score and f outper-form DS values, with a remarkable exception: thebest AP is obtained by an Indri index that com-pares the semantic similarity between the verb incombination with the noun and the verb in contextswithout the noun (L2 Indri rankV weight), accord-ingly with the claim that the semantics of the verbcontribute to the semicompositionality of colloca-tions.
By contrast, the corresponding measure forthe noun (L2 Indri rankN weight) works quite a bitbetter with idioms than the previous verb measure.Figure 1 shows the precision curves for the extrac-tion of MWEs by the best measure of each compo-nent of idiomaticity.In Figure 2 and 3, we present separately the preci-121Figure 1: Precision results for the compositionality rank-ings of MWEs.sion curves for idioms and collocations.
We plot themeasures with the best precision values.Figure 2: Precision results for the compositionality rank-ings of idioms.Regarding the precision for collocations in Fig-ure 3, the differences are not obviously significant.Even though the DS measure has the better perfor-mance, precision values for the t-score are not toomuch lower, and the t-score has a similar perfor-mance at the beginning of the ranking (n < 150).4.2 Machine Learning ExperimentsWe report only the results of the three methods withthe best overall performance: Logistic Regression(LR), SMO and RandomForest (RF).In Table 2, we present the results obtained withdatasets containing only DS attributes (the sourceof knowledge with the best results in single ex-Figure 3: Precision results for the compositionality rank-ings of collocations.periments); datasets containing all features corre-sponding to the four properties of idiomaticity; anddatasets obtained adding the verb of the bigram as astring-type attribute.As the figures show, it is difficult to improve theresults obtained using only DS.
The results of SMOare better when the features of the four componentsof idiomaticity are used, and even better when theverb is added, especially for idioms.
The verb causesthe performance of RF be slightly worse; in the caseof LR, it generates considerable noise.It can be observed that the figures for LR aremore unstable.
Using SMO and RF, convergencedoes not depend on how many noisy variables arepresent (Biau, 2012).
Thus, feature selection couldimprove the results when LR is used.In a complementary experiment, we observed theimpact of removing the attributes of each source ofknowledge (without including verbs).
The most ev-ident result was that the exclusion of LFlex featurescontributes the most to improving F. This was an ex-pected effect, considering the poor results for LFlexmeasures described in section 4.1.
More interest-ing is the fact that removing MSFlex features had ahigher negative impact on F than not taking AMs asfeatures.Table 3 shows the results for two datasets gener-ated through two manual selection of attributes: (1)manual 1: the 20 attributes with best AP average re-sults; and (2) manual 2: a manual selection of theattributes from each knowledge source with the bestAP MWE, best AP id and best AP col.
The third122Features Method CCI F id F col F free F W.Av.
F Av.DSLR 72.489 0.261 0.453 0.838 0.707 0.517SMO 74.061 0.130 0.387 0.824 0.575 0.447RF 71.441 0.295 0.440 0.821 0.695 0.519all idiom.
propertiesLR 71.703 0.339 0.514 0.821 0.716 0.558SMO 76.507 0.367 0.505 0.857 0.740 0.576RF 74.498 0.323 0.486 0.844 0.724 0.551all + verbLR 60.000 0.240 0.449 0.726 0.627 0.472SMO 75.808 0.400 0.540 0.848 0.744 0.596RF 74.061 0.243 0.459 0.846 0.713 0.516Table 2: Results of Machine Learning experiments combining knowledge sources in three ways: (i) DS: distributionalsimilarity features; (ii) knowledge related to the four components of idiomaticity (AM+DS+MSFlex+LFlex); (iii)previous features+verb components of bigrams.section presents the results obtained with AttributeS-electedClassifier using CfsSubsetEval (CS) as evalu-ator3 and BestFirst (BS) as search method.
Lookingat the results of the selection process in each fold, wesaw that the attributes selected in more than 2 foldsare 36: 1 AM, 20 from DS, 7 from MSFlex, 1 fromLFlex and 7 verbs.Features Method F W.Av.
F Av.manual 1LR 0.709 0.525SMO 0.585 0.304RF 0.680 0.485manual 2LR 0.696 0.518SMO 0.581 0.286RF 0.688 0.519CS-BFLR 0.727 0.559SMO 0.693 0.485RF 0.704 0.531Table 3: F Weighted average and F average results for ex-periments using: (1) the 20 attributes with best AP aver-age results; (2) a manual selection of the 3 best attributesfrom each knowledge source; and (3) AttributeSelected-Classifier with automatic attribute selection using Cfs-SubsetEval as evaluator and BestFirst as search methodThe results show that, for each method, auto-matic selection outperforms the two manual selec-tions.
Most of the attributes automatically selectedare DS measures, but it is interesting to observe thatMSFlex and the verb slot contribute to improvingthe results.
Using automatic attribute selection and3http://wiki.pentaho.com/display/DATAMINING/CfsSubsetEvalLR, the results are close to the best figure of F W.Av.using SMO and all the features (0.727 vs 0.744).5 DiscussionThe most important conclusions from our experi-ments are the following:?
In the task of ranking the candidates, the bestresults are obtained using DS measures, and,in particular, Indri and KL-div in L2 experi-ments.
This is true for both type of MWEs, andis ratified in ML experiments when automaticattribute filtering is carried out.
It is, however,particularly notable with regard to idioms; inthe case of collocations, the difference betweenthe performance of DS and that of and MS andAM were not that significant.?
MSFlex contributes to the classification taskwhen used in combination with DS, but getpoor results by themselves.
The most relevantparameter MSFlex is number inflection.?
SMO is the most precise method when a highamount of features is used.
It gets the best over-all F-score.
The other methods need feature se-lection to obtain similar results.?
Automatic attribute selection using CS-BF fil-ter yields better results than manual selections.The method that takes the most advantage isLR, whose scores are little bit worse than thoseof SMO using the whole set of attributes.123Some of these conclusions differ from those reachedby earlier works.
In particular, the claims in Fazlyand Stevenson (2007) and Van de Cruys and Moiro?n(2007) that syntactic as well as lexical flexibility out-perform other techniques of MWE characterizationare not confirmed in this work for Basque.
Somehypothesis could be formulated to explain thosedifferences: (1) Basque idioms could be syntacti-cally more flexible, whereas some free combinationscould present a non-negligible level of fixedness; (2)Basque, especially in the journalistic register, couldbe sociolinguistically less fixed than, say, Englishor Spanish; thus, the lexical choice of the collocatecould be not so clearly established; (3) the Basquelexical resources to test substitutability could haveinsufficient coverage; and (4) Fazly and Stevenson(2007) use the cosine for DS, a measure which in ourexperiments is clearly below other measures.
Thosehypotheses require experimental testing and deeperlinguistic analysis.6 Conclusions and Future WorkWe have presented an in-depth analysis of the per-formance of different features of idiomaticity in thecharacterization of NV expressions, and the resultsobtained combining them using ML methods.
Theresults confirm the major role of DS, especially, asexpected, in the case of idioms.
It is remarkable thatthe best results have been obtained using Lemur, anIR tool.
ML experiments show that other featurescontribute to improve the results, especially someaspects of MSFlex, the verb of the bigram and, toa more limited extent, AMs.
The performance ofDS being the best one for idioms confirm previousresearch on other languages, but MSFlex and LFlexbehave below the expected.
The explanations pro-posed for this issue require further verification.We are planning experiments using these tech-niques for discriminating between literal and id-iomatic occurrences of MWEs in context.
Work onparallel corpora is planned for the future.AcknowledgmentsThis research was supported in part by the Span-ish Ministry of Education and Science (TACARDI-TIN2012-38523-C02-011) and by the BasqueGovernment (Berbatek project, Etortek-IE09-262;KONBITZ project, Saiotek 2012).
Ainara Estar-rona and Larraitz Uria (IXA group) and Ainara On-darra and Nerea Areta (Elhuyar) are acknowledgedfor their work as linguists in the manual evaluation.Maddalen Lopez de la Calle and In?aki San Vicente(Elhuyar) and Oier Lopez de la Calle (IXA group)have contributed with their expertise to the design ofthe experiments with Lemur and Infomap.
Finally,special thanks goes to Olatz Arregi (IXA group)for having guided us in the experiments with Weka,and to Yosu Yurramendi from the University of theBasque Country, for his advice on the statistics inthe evaluation step.ReferencesAllan, J., J. Callan, K. Collins-Thompson, B. Croft,F.
Feng, D. Fisher, J. Lafferty, L. Larkey,T.
Truong, P. Ogilvie, et al(2003).
The LemurToolkit for language modeling and informationretrieval.Baldwin, T., C. Bannard, T. Tanaka, and D. Wid-dows (2003).
An empirical model of multi-word expression decomposability.
In Proceed-ings of the ACL 2003 workshop on Multiwordexpressions: analysis, acquisition and treatment-Volume 18, pp.
96.Baldwin, T. and S. Kim (2010).
Multiword expres-sions.
Handbook of Natural Language Process-ing, second edition.
Morgan and Claypool.Banerjee, S. and T. Pedersen (2010).
The design,implementation, and use of the Ngram StatisticsPackage.
Computational Linguistics and Intelli-gent Text Processing, 370?381.Bannard, C. (2007).
A measure of syntactic flexi-bility for automatically identifying multiword ex-pressions in corpora.
In Proceedings of the Work-shop on a Broader Perspective on Multiword Ex-pressions, pp.
1?8.Berry-Rogghe, G. (1974).
Automatic identificationof phrasal verbs.
Computers in the Humanities,16?26.Biau, G. (2012).
Analysis of a random forestsmodel.
The Journal of Machine Learning Re-search 98888, 1063?1095.Biemann, C. and E. Giesbrecht (2011).
Distri-butional semantics and compositionality 2011:124Shared task description and results.
Workshopon Distributional semantics and compositionality2011.
ACL HLT 2011, 21.Church, K. and P. Hanks (1990).
Word associa-tion norms, mutual information, and lexicogra-phy.
Computational linguistics 16(1), 22?29.Evert, S. (2005).
The statistics of word cooccur-rences: Word pairs and collocations.
Ph.
D. the-sis, University of Stuttgart.Fazly, A. and S. Stevenson (2007).
Distinguish-ing subtypes of multiword expressions usinglinguistically-motivated statistical measures.
InProceedings of the Workshop on A Broader Per-spective on Multiword Expressions, pp.
9?16.
As-sociation for Computational Linguistics.Granger, S. and M. Paquot (2008).
Disentanglingthe phraseological web.
Phraseology.
An inter-disciplinary perspective, 27?50.Gurrutxaga, A. and I. Alegria (2011).
Automaticextraction of NV expressions in Basque: basicissues on cooccurrence techniques.
Proc.
of theWorkshop on Multiword Expressions.
ACL HLT2011, 2?7.Gurrutxaga, A. and I. Alegria (2012).
Measuringthe compositionality of nv expressions in basqueby means of distributional similarity techniques.LREC2012.Hall, M., E. Frank, G. Holmes, B. Pfahringer,P.
Reutemann, and I. H. Witten (2009).
The wekadata mining software: an update.
Volume 11, pp.10?18.
ACM.Katz, G. and E. Giesbrecht (2006).
Automaticidentification of non-compositional multi-wordexpressions using latent semantic analysis.
InProceedings of the Workshop on Multiword Ex-pressions: Identifying and Exploiting UnderlyingProperties, pp.
12?19.
Association for Computa-tional Linguistics.Krenn, B., S. Evert, and H. Zinsmeister (2004).
De-termining intercoder agreement for a collocationidentification task.
In Proceedings of KONVENS,pp.
89?96.Lin, D. (1999).
Automatic identification of non-compositional phrases.
In Proceedings of the 37thannual meeting of the ACL, pp.
317?324.
Associ-ation for Computational Linguistics.Lin, J., S. Li, and Y. Cai (2008).
A new colloca-tion extraction method combining multiple asso-ciation measures.
In Machine Learning and Cy-bernetics, 2008 International Conference on, Vol-ume 1, pp.
12?17.
IEEE.Oronoz, M., A. D. de Ilarraza, and K. Gojenola(2010).
Design and evaluation of an agreement er-ror detection system: testing the effect of ambigu-ity, parser and corpus type.
In Advances in Natu-ral Language Processing, pp.
281?292.
Springer.Pecina, P. (2010).
Lexical association measures andcollocation extraction.
Language resources andevaluation 44(1), 137?158.Schone, P. and D. Jurafsky (2001).
Is knowledge-free induction of multiword unit dictionary head-words a solved problem.
In Proc.
of the 6thEMNLP, pp.
100?108.
Citeseer.Seretan, V. (2011).
Syntax-Based Collocation Ex-traction.
Text, Speech and Language Technology.Dordrecht: Springer.Street, L., N. Michalov, R. Silverstein, M. Reynolds,L.
Ruela, F. Flowers, A. Talucci, P. Pereira,G.
Morgon, S. Siegel, et al(2010).
Like finding aneedle in a haystack: Annotating the american na-tional corpus for idiomatic expressions.
In Proc.of LREC?2010.Van de Cruys, T. and B. Moiro?n (2007).
Semantics-based multiword expression extraction.
In Pro-ceedings of the Workshop on A Broader Perspec-tive on Multiword Expressions, pp.
25?32.
Asso-ciation for Computational Linguistics.Venkatapathy, S. and A. Joshi (2005).
Measuring therelative compositionality of verb-noun (vn) collo-cations by integrating features.
In Proceedings ofHLT/EMNLP, pp.
899?906.
Association for Com-putational Linguistics.Wulff, S. (2010).
Rethinking Idiomaticity.
Corpusand Discourse.
New York: Continuum Interna-tional Publishing Group Ltd.125
