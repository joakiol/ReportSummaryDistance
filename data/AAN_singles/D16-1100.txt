Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 981?986,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsMulti-Granularity Chinese Word EmbeddingRongchao Yin?
?, Quan Wang??
, Rui Li?
?, Peng Li??
?, Bin Wang??
?Institute of Information Engineering, Chinese Academy of Sciences, Beijing 100093, China?University of Chinese Academy of Sciences, Beijing 100049, China{yinrongchao,wangquan,lirui,lipeng,wangbin}@iie.ac.cnAbstractThis paper considers the problem of learningChinese word embeddings.
In contrast to En-glish, a Chinese word is usually composed ofcharacters, and most of the characters them-selves can be further divided into componentssuch as radicals.
While characters and radical-s contain rich information and are capable ofindicating semantic meanings of words, theyhave not been fully exploited by existing wordembedding methods.
In this work, we proposemulti-granularity embedding (MGE) for Chi-nese words.
The key idea is to make full use ofsuch word-character-radical composition, andenrich word embeddings by further incorpo-rating finer-grained semantics from charactersand radicals.
Quantitative evaluation demon-strates the superiority of MGE in word sim-ilarity computation and analogical reasoning.Qualitative analysis further shows its capabili-ty to identify finer-grained semantic meaningsof words.1 IntroductionWord embedding, also known as distributed wordrepresentation, is to represent each word as a real-valued low-dimensional vector, through which thesemantic meaning of the word can be encoded.
Re-cent years have witnessed tremendous success ofword embedding in various NLP tasks (Bengio et al,2006; Mnih and Hinton, 2009; Collobert et al, 2011;Zou et al, 2013; Kim, 2014; Liu et al, 2015; Iyyeret al, 2015).
The basic idea behind is to learn thedistributed representation of a word using its con-text.
Among existing approaches, the continuousbag-of-words model (CBOW) and Skip-Gram mod-el are simple and effective, capable of learning wordembeddings efficiently from large-scale text corpo-ra (Mikolov et al, 2013a; Mikolov et al, 2013b).
?Corresponding author: Peng Li.Besides the success in English, word embeddinghas also been demonstrated to be extremely usefulfor Chinese language processing (Xu et al, 2015;Yu et al, 2015; Zhou et al, 2015; Zou et al, 2013).The work on Chinese generally follows the sameidea as on English, i.e., to learn the embedding ofa word on the basis of its context.
However, incontrast to English where words are usually takenas basic semantic units, Chinese words may have acomplicated composition structure of their seman-tic meanings.
More specifically, a Chinese word isoften composed of several characters, and most ofthe characters themselves can be further divided in-to components such as radicals (??
).1 Both char-acters and radicals may suggest the semantic mean-ing of a word, regardless of its context.
For exam-ple, the Chinese word ???
(have a meal)?
con-sists of two characters ?
?
(eat)?
and ??
(meal)?,where ?
?
(eat)?
has the radical of ??
(mouth)?,and ??
(meal)?
the radical of ??
(food)?.
The se-mantic meaning of ????
can be revealed by theconstituent characters as well as their radicals.Despite being the linguistic nature of Chinese andcontaining rich semantic information, such word-character-radical composition has not been fully ex-ploited by existing approaches.
Chen et al (2015)introduced a character-enhanced word embeddingmodel (CWE), which learns embeddings jointly forwords and characters but ignores radicals.
Sun et al(2014) and Li et al (2015) utilized radical informa-tion to learn better character embeddings.
Similarly,Shi et al (2015) split characters into small compo-nents based on the Wubi method,2 and took into ac-count those components during the learning process.In their work, however, embeddings are learned on-ly for characters.
For a word, the embedding is gen-erated by simply combining the embeddings of theconstituent characters.
Since not all Chinese word-1https://en.wikipedia.org/wiki/Radical (Chinese characters)2 https://en.wikipedia.org/wiki/Wubi method981??
(go back home) ??
(meet friends) ?
(eat) ?(food)?
(go back)?(home)?(meet)?(friends)??
(have a meal)hidden layerWord embeddingsCharacter embeddingsRadical embeddingsHidden layerFigure 1: A simple illustration of MGE, where embeddingsare learned jointly for words, characters, and radicals.
Givena sequence of words {???
(go back home)?, ???
(have ameal)?, ???
(meet friends)?
}, MGE predicts the central word????
by using 1) the embedding composed by each contextword and its constituent characters, and 2) the embedding asso-ciated with each radical detected in the target word.s are semantically compositional (e.g., transliteratedwords such as ???
(soda)?
), embeddings obtainedin this way may be of low quality for these words.In this paper, aiming at making full use of the se-mantic composition in Chinese, we propose multi-granularity embedding (MGE) which learns embed-dings jointly for words, characters, and radicals.
Theframework of MGE is sketched in Figure 1.
Given aword, we learn its embedding on the basis of 1) thecontext words (blue bars in the figure), 2) their con-stituent characters (green bars), and 3) the radicalsfound in the target word (orange bars).
Comparedto utilizing context words alone, MGE enriches theembeddings by further incorporating finer-grainedsemantics from characters and radicals.
Similarideas of adaptively using multiple levels of embed-dings have also been investigated in English recent-ly (Kazuma and Yoshimasa, 2016; Miyamoto andCho, 2016).We evaluate MGE with the benchmark tasks ofword similarity computation and analogical reason-ing, and demonstrate its superiority over state-of-the-art metods.
A qualitative analysis further showsthe capability of MGE to identify finer-grained se-mantic meanings of words.2 Multi-Granularity Word EmbeddingThis section introduces MGE based on the contin-uous bag-of-words model (CBOW) (Mikolov et al,2013b) and the character-enhanced word embeddingmodel (CWE) (Chen et al, 2015).MGE aims at improving word embedding byleveraging both characters and radicals.
We denotethe Chinese word vocabulary asW , the character vo-cabulary as C, and the radical vocabulary asR.
Eachword wi ?
W is associated with a vector embeddingwi, each character ci ?
C a vector embedding ci,and each radical ri ?
R a vector embedding ri.
Giv-en a sequence of words D = {w1, ?
?
?
, wN}, MGEpredicts each word wi ?
D conditioned on 1) con-text words in a sliding window with size ?, denotedas Wi = {wi?
?, ...wi?1, wi+1, ..., wi+?
}, 2) charac-ters in each context word wj ?
Wi, denoted as Cj ,and 3) radicals in the target word wi, denoted as Ri.See Figure 1 for a simple illustration.More specifically, given the corpusD, MGE max-imizes the overall log likelihood as follows:L(D) =?wi?Dlog p(wi|hi).
(1)Here hi is a hidden vector composed by the embed-dings of context words, constituent characters, andradicals, defined as:hi=12[ 1|Wi|?wj?Wi(wj?1|Cj |?ck?Cjck)+ 1|Ri|?rk?Rirk].
(2)For each context word wj ?
Wi, a word-charactercomposition (wj ?
1|Cj |?c?Cj c) is first generatedby the embeddings of wj and its constituent charac-ters Cj .
These word-character compositions are thencombined with the radical embeddings in Ri to pre-dict the target word.
|Wi|/|Ri|/|Cj | is the cardinalityof Wi/Ri/Cj , and ?
is the composition operation.3Given hi, the conditional probability p(wi|hi) is de-fined by a softmax function:p(wi|hi) =exp(h?i wi)?wi?
?W exp(h?i wi?).
(3)We use negative sampling and stochastic gradientdescent to solve the optimization problem.Note that 1) Not all Chinese words are semantical-ly compositional, e.g., transliterated words and enti-ty names.
For such words we use neither charactersnor radicals.
2) A Chinese character usually plays3There are a variety of options for ?, e.g., addition and con-catenation.
This paper follows (Chen et al, 2015) and uses theaddition operation.982different roles when it appears at different positionswithin a word.
We follow (Chen et al, 2015) anddesign a position-based MGE model (MGE+P).
Thekey idea of MGE+P is to keep three embeddings foreach character, corresponding to its appearance atthe positions of ?begin?, ?middle?, and ?end?.
Fordetails, please refer to (Chen et al, 2015).3 ExperimentsWe evaluate MGE with the tasks of word similaritycomputation and analogical reasoning.3.1 Experimental SetupsWe select the Chinese Wikipedia Dump4 for embed-ding learning.
In preprocessing, we use the THU-LAC tool5 to segment the corpus.
Pure digit word-s, non-Chinese words, and words whose frequenciesare less than 5 in the corpus are removed.
We furthercrawl from an online Chinese dictionary6 and builda character-radical index with 20,847 characters and269 radicals.
We use this index to detect the radicalof each character in the corpus.
As such, we get atraining set with 72,602,549 words, 277,200 uniquewords, 8,410 unique characters, and 256 unique rad-icals.
Finally, we use THULAC to perform ChinesePOS tagging on the training set and identify all enti-ty names.
For these entity names, neither charactersnor radicals are considered during learning.
Actual-ly, Chen et al (2015) categorized non-compositionalChinese words into three groups, i.e., transliterat-ed words, single-morpheme multi-character words,and entity names.
In their work, they used a human-annotated corpus, manually determining each wordto be split or not.
Since human annotation could betime-consuming and labor intensive, we just consid-er automatically identified entity names.We compare MGE with CBOW (Mikolov et al,2013b)7 and CWE (Chen et al, 2015)8.
Both CWEand MGE are extensions of CBOW, with the for-mer taking into account characters and the latter fur-ther incorporating radical information.
We furtherconsider position-based CWE and MGE, denoted asCWE+P and MGE+P, respectively.We follow (Chen4http://download.wikipedia.com/zhwiki5http://thulac.thunlp.org/6http://zd.diyifanwen.com/zidian/bs/7https://code.google.com/p/word2vec/8https://github.com/Leonard-Xu/CWEMethod WordSim-239 WordSim-293k=100 k=200 k=100 k=200CBOW 0.4917 0.4971 0.5667 0.5723CWE 0.5121 0.5197 0.5511 0.5655CWE+P 0.4989 0.5026 0.5427 0.5545MGE 0.5670 0.5769 0.5555 0.5659MGE+P 0.5511 0.5572 0.5530 0.5692Table 1: Results on word similarity computation.et al, 2015) and use the same hyperparameter set-ting.
For all the methods, we set the context windowsize to 3, and select the embedding dimension k in{100, 200}.
During optimization, we use 10-wordnegative sampling and fix the initial learning rate to0.025.3.2 Word Similarity ComputationThis task is to evaluate the effectiveness of embed-dings in preserving semantic relatedness between t-wo words.
We use the WordSim-240 and WordSim-296 datasets9 provided by Chen et al (2015) for e-valuation, both containing Chinese word pairs withhuman-labeled similarity scores.
On WordSim-240there is a pair containing new words (i.e., wordsthat have not appeared in the training set), and onWordSim-296 there are 3 such pairs.
We removethese pairs from both datasets, and accordingly getWordSim-239 and WordSim-293.We compute the Spearman correlation coefficient(Myers et al, 2010) between the similarity scoresgiven by the embedding models and those given byhuman annotators.
For the embedding models, thesimilarity score between two words is calculated asthe cosine similarity between their embeddings.
TheSpearman correlation coefficient is a nonparametricmeasure of rank correlation, assessing how well therelationship between two variables can be described.The results are shown in Table 1.From the results, we can see that 1) On WordSim-239, MGE(+P) performs significantly better thanCWE(+P), which in turn outperforms CBOW.
Thisobservation demonstrates the superiority of incor-porating finer-grained semantics, particularly fromradicals.
For example, MGE performs much betteron word pairs such as ???
(bank)?
and ??
(mon-ey)?, in which the two words share the same radi-cal of ??(gold)?.
2) On WordSim-293, MGE(+P)9https://github.com/Leonard-Xu/CWE/tree/master/data9830.460.480.50.520.540.560.580.63 4 5 6 7Spearmancorrelationcoefficientcontext window sizeCBOW CWE MGEFigure 2: Word similarity computation results with differentcontext window sizes on WordSim-239 (k = 200).performs equally well as CWE(+P), but both are s-lightly worse than CBOW.
The reason may be thatWordSim-293 contains a great many of word pairs inwhich the two words belonging to different domain-s, e.g., ???
(rooster)?
and ???
(flying range)?.These pairs usually get low human-labeled similari-ty scores.
However, splitting the words in such pairsinto characters, and further the characters into radi-cals will not help to effectively identify the dissimi-larity between them.10We further investigate the influence of the contextwindow size in word similarity computation.
Fig-ure 2 gives the results of CBOW, CWE, and MGEon WordSim-239, with the context window size setin {3, 4, 5, 6, 7}.
The results indicate that MGE per-forms consistently better than CBOW and CWE onthis dataset, unaffected by varying the context win-dow size.3.3 Word Analogical ReasoningThis task evaluates the effectiveness of embeddingsin capturing linguistic regularities between pairs ofwords, in the form of ???
(London) :??
(Eng-land) ?
??
(Paris) : ??
(France)?.
We use thedataset provided by Chen et al (2015) for evalua-tion.
It contains 1,124 analogies categorized into 3types: 1) capitals of countries (677 groups); 2) s-tates/provinces of cities (175 groups); and 3) familyrelations (272 groups).
All the words in this dataset10This observation is inconsistent with that reported in (Chenet al, 2015), which shows that CWE outperforms CBOW onWordSim-296.
The reason may be that Chen et al (2015) used ahuman-annotated corpus for embedding learning, and manuallydetermined each word to be split or not.
In contrast, we use thepublicly available Chinese Wikipedia data, and automaticallysegment the corpus and identify entity names (words that arenot to be split), without human annotation.Method Total Capital State FamilyCBOW 0.7498 0.8109 0.8400 0.5294CWE 0.7248 0.8375 0.8541 0.3566CWE+P 0.7391 0.8065 0.8114 0.5147MGE 0.7524 0.8804 0.8686 0.3529MGE+P 0.7720 0.8685 0.8857 0.4485Table 2: Results on word analogical reasoning (k = 200).are covered by the training set.For each analogy ?a : b ?
c : d ?, we create aquestion ?a : b ?
c : ?
?, and predict the answer as:d?
= argmaxw?W cos (b?a+c,w).
Here a, b, c,w are the word embeddings, and cos(?, ?)
the cosinesimilarity.
The question is considered to be correctlyanswered if d?
= d. We use accuracy as the evalua-tion metric, and report the results in Table 2.The results indicate that 1) MGE(+P) substantial-ly outperforms the baseline methods on almost alltypes of analogies (except for the Family type).
Thisagain demonstrates the superiority of incorporatingradical information.
2) For the Capital and Statetypes, all the words are entity names for which nei-ther characters nor radicals are used.
MGE(+P) stilloutperforms the baselines on these two types, show-ing its capability to learn better embeddings even fornon-compositional words.
3) On the Family type,both MGE(+P) and CWE(+P) perform worse thanCBOW.
This may be caused by the inappropriate de-composition of family words into characters.
Con-sider, for example, the question ???
(uncle) : ??
(aunt) ???
(prince) : ?
?.
If we split ???
?into ??
(king)?
and ??
(son)?, we will more likelyto predict ?
??
(queen)?
rather than the correc-t answer ???
(princess)?, since ????
containsthe character ?
?
(daughter)?
which is usually theantonym of ??
(son)?.3.4 Case StudyBesides quantitative evaluation, this section furtherprovides qualitative analysis to show in what man-ner the semantic meaning of a radical, character andword can be captured by their embeddings.Take the word ???
(swimming)?
as an example.Table 3 presents the words that are most similar to it(with the highest cosine similarity between their em-beddings), discovered by MGE, CWE, and CBOW.The results show that 1) By incorporating the char-acter information, MGE and CWE are capable of984MGE??
(underwater swimming),??
(swimming happily)??
(front crawl swimming),??(swimmer)??
(swimming skill),??
(winter swimming)??
(swimming skill),??
(track and field)CWE??
(underwater swimming),??
(swimming happily)??
(front crawl swimming),??
(track and field)??(swimmer),??
(learn to swim)??
(winter swimming),??
(swimming skill)CBOW??
(track and field),??
(high jump)??(diving),??
(rope skipping)??(boating),???
(pole vaulting)???(canoeing),??
(gymnastics)Table 3: The most similar words to ???
(swimming)?.Radical ?(illness)?(rickets)?
(chronic disease)Closest ?
(bending one?s back)?
(epidemic disease)characters ?(tuberculosis)?(quenching)?(scabies)?(hemorrhoids)??(rickets)??
(ringworm scabies)Closest ??(pock)??
(communicable subtropical disease)words ??(traumata)??(scar)??(measles)??
(pemphigus)Table 4: The most similar characters/words to ??
(illness)?.capturing finer-grained semantics that are more spe-cific to the word.
The top words discovered by themare semantically related to ???
(swimming)?
it-self, e.g., ???
(underwater swimming)?
and ???
(front crawl swimming)?.
But the top words dis-covered by CBOW are just other types of sports inparallel with ???
(swimming)?, e.g., ???
(highjump)?
and ???
(diving)?.
2) MGE performs evenbetter than CWE by further incorporating the radicalinformation.
The less relevant word ?
??
(trackand field)?
is ranked 4th by CWE.
But after introduc-ing the radical ??
(water)?, MGE can successfullyrank ???
(swimmer)?, ???
(swimming skill)?,and ???
(winter swimming)?
before it.
All thesewords contain the radical ??(water)?
and are morerelevant to ???
(swimming)?.We further take the radical ??
(illness)?
as an ex-ample, and list the most similar characters and wordsdiscovered by MGE in Table 4.
The similarity be-tween a radical and a character/word is also definedas the cosine similarity between their embeddings.From the results, we can see that almost all the char-acters and words are disease-related, e.g., ??
(rick-ets)?, ??
(tuberculosis)?, and ???
(ringworm s-cabies)?, and most of them share the same radical??
(illness)?.
This observation demonstrates the ra-tionality of embedding Chinese words, characters,and radicals into the same vector space, and measur-ing their similarities directly in that space.
Note thatthis operation might be problematic for English.
Forexample, it could be hard to figure out what kind ofsimilarity there is between the character ?i?
and theword ?ill?.
But for Chinese, this problem might bealleviated since characters and radicals themselvescontain rich semantic information.4 Conclusion and Future WorkIn this paper we propose a new approach to Chineseword embedding, referred to as multi-granularityembedding (MGE).
MGE improves word embed-ding by further leveraging both characters and radi-cals, and hence makes full use of the word-character-radical semantic composition.
Experimental resultson word similarity computation and analogical rea-soning demonstrate the superiority of MGE overstate-of-the-art methods.
A qualitative analysis fur-ther shows that by incorporating radical informationMGE can identify finer-grained semantic meaningsof words.As future work, we would like to 1) Investigatemore complicate composition manners among radi-cals, characters, and words, e.g., a hierarchical struc-ture of them.
2) Explore the semantic compositionof higher level language units such as phrases, sen-tences, and even documents.5 AcknowledgementWe would like to thank the anonymous reviewersfor their insightful comments and suggestions.
Thisresearch is supported by the National Natural Sci-ence Foundation of China (grant No.
61402465 andNo.
61402466) and the Strategic Priority ResearchProgram of the Chinese Academy of Sciences (grantNo.
XDA06030200).ReferencesYoshua Bengio, Holger Schwenk, Jean-Se?bastienSene?cal, Fre?deric Morin, and Jean-Luc Gauvain.2006.
Neural probabilistic language models.
InInnovations in Machine Learning, pages 137?186.Xinxiong Chen, Lei Xu, Zhiyuan Liu, Maosong Sun, andHuanbo Luan.
2015.
Joint learning of character and985word embeddings.
In Proceedings of the 24th Inter-national Conference on Artificial Intelligence, pages1236?1242.Ronan Collobert, Jason Weston, Le?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.The Journal of Machine Learning Research, 12:2493?2537.Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber,and Hal Daume?
III.
2015.
Deep unordered compo-sition rivals syntactic methods for text classification.In Proceedings of the 53rd Annual Meeting of the As-sociation for Computational Linguistics and the 7thInternational Joint Conference on Natural LanguageProcessing, pages 1681?1691.Hashimoto Kazuma and Tsuruoka Yoshimasa.
2016.Adaptive joint learning of compositional and non-compositional phrase embeddings.
In Proceedings ofthe 54th Annual Meeting of the Association for Com-putational Linguistics.Yoon Kim.
2014.
Convolutional neural networks for sen-tence classification.
In Proceedings of the 2014 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1746?1751.Yanran Li, Wenjie Li, Fei Sun, and Sujian Li.
2015.Component-enhanced chinese character embeddings.In Proceedings of the 2015 Conference on EmpiricalMethods in Natural Language Processing, pages 829?834.Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng,Kevin Duh, and Ye-Yi Wang.
2015.
Representationlearning using multi-task deep neural networks for se-mantic classification and information retrieval.
In Pro-ceedings of the 2015 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics: Human Language Technologies, pages 912?921.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corra-do, and Jeff Dean.
2013a.
Distributed representationsof words and phrases and their compositionality.
InAdvances in Neural Information Processing Systems26, pages 3111?3119.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013b.
Linguistic regularities in continuous spaceword representations.
In Proceedings of the 2013 Con-ference of the North American Chapter of the Associa-tion for Computational Linguistics: Human LanguageTechnologies, pages 746?751.Yasumasa Miyamoto and Kyunghyun Cho.
2016.
Gat-ed word-character recurrent language model.
arXivpreprint arXiv:1606.01700.Andriy Mnih and Geoffrey E. Hinton.
2009.
A scalablehierarchical distributed language model.
In Advancesin Neural Information Processing Systems 21, pages1081?1088.Jerome L Myers, Arnold Well, and Robert FrederickLorch.
2010.
Research design and statistical analy-sis.
Routledge.Xinlei Shi, Junjie Zhai, Xudong Yang, Zehua Xie, andChao Liu.
2015.
Radical embedding: Delving deeperto chinese radicals.
In Proceedings of the 53rd Annu-al Meeting of the Association for Computational Lin-guistics and the 7th International Joint Conference onNatural Language Processing, pages 594?598.Yaming Sun, Lei Lin, Nan Yang, Zhenzhou Ji, and Xiao-long Wang, 2014.
Radical-Enhanced Chinese Char-acter Embedding, chapter Proceedings of the 21st In-ternational Conference on Neural Information Pro-cessing, pages 279?286.Ruifeng Xu, Tao Chen, Yunqing Xia, Qin Lu, Bin Liu,and Xuan Wang.
2015.
Word embedding compositionfor data imbalances in sentiment and emotion classifi-cation.
Cognitive Computation, 7(2):226?240.Mo Yu, Matthew R. Gormley, and Mark Dredze.
2015.Combining word embeddings and feature embeddingsfor fine-grained relation extraction.
In Proceedings ofthe 2015 Conference of the North American Chapterof the Association for Computational Linguistics: Hu-man Language Technologies, pages 1374?1379.Guangyou Zhou, Tingting He, Jun Zhao, and Po Hu.2015.
Learning continuous word embedding withmetadata for question retrieval in community questionanswering.
In Proceedings of the 53rd Annual Meet-ing of the Association for Computational Linguisticsand the 7th International Joint Conference on NaturalLanguage Processing, pages 250?259.Will Y. Zou, Richard Socher, Daniel Cer, and Christo-pher D. Manning.
2013.
Bilingual word embeddingsfor phrase-based machine translation.
In Proceedingsof the 2013 Conference on Empirical Methods in Nat-ural Language Processing, pages 1393?1398.986
