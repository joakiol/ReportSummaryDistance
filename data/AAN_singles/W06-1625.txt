Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 208?215,Sydney, July 2006. c?2006 Association for Computational LinguisticsHumor: Prosody Analysis and Automatic Recognitionfor F * R * I * E * N * D * S *Amruta Purandare and Diane LitmanIntelligent Systems ProgramUniversity of Pittsburghfamruta,litmang@cs.pitt.eduAbstractWe analyze humorous spoken conversa-tions from a classic comedy televisionshow, FRIENDS, by examining acoustic-prosodic and linguistic features and theirutility in automatic humor recognition.Using a simple annotation scheme, we au-tomatically label speaker turns in our cor-pus that are followed by laughs as hu-morous and the rest as non-humorous.Our humor-prosody analysis reveals sig-nificant differences in prosodic character-istics (such as pitch, tempo, energy etc.
)of humorous and non-humorous speech,even when accounted for the gender andspeaker differences.
Humor recognitionwas carried out using standard supervisedlearning classifiers, and shows promisingresults significantly above the baseline.1 IntroductionAs conversational systems are becoming preva-lent in our lives, we notice an increasing need foradding social intelligence in computers.
There hasbeen a considerable amount of research on incor-porating affect (Litman and Forbes-Riley, 2004)(Alm et al, 2005) (D?Mello et al, 2005) (Shroderand Cowie, 2005) (Klein et al, 2002) and person-ality (Gebhard et al, 2004) in computer interfaces,so that, for instance, user frustrations can be rec-ognized and addressed in a graceful manner.
As(Binsted, 1995) correctly pointed out, one way toalleviate user frustrations, and to make human-computer interaction more natural, personal andinteresting for the users, is to model HUMOR.Research in computational humor is still invery early stages, partially because humorous lan-guage often uses complex, ambiguous and incon-gruous syntactic and semantic expressions (At-tardo, 1994) (Mulder and Nijholt, 2002) which re-quire deep semantic interpretation.
Nonetheless,recent studies have shown a feasibility of auto-matically recognizing (Mihalcea and Strapparava,2005) (Taylor and Mazlack, 2004) and generating(Binsted and Ritchie, 1997) (Stock and Strappar-ava, 2005) humor in computer systems.
The stateof the art research in computational humor (Bin-sted et al, 2006) is, however, limited to text (suchas humorous one-liners, acronyms or wordplays),and to our knowledge, there has been no work todate on automatic humor recognition in spokenconversations.Before we can model humor in real applicationsystems, we must first analyze features that char-acterize humor.
Computational approaches to hu-mor recognition so far primarily rely on lexicaland stylistic cues such as alliteration, antonyms,adult slang (Mihalcea and Strapparava, 2005).
Thefocus of our study is, on the other hand, on ana-lyzing acoustic-prosodic cues (such as pitch, in-tensity, tempo etc.)
in humorous conversationsand testing if these cues can help us to auto-matically distinguish between humorous and non-humorous (normal) utterances in speech.
We hy-pothesize that not only the lexical content butalso the prosody (or how the content is expressed)makes humorous expressions humorous.The following sections describe our data collec-tion and pre-processing, followed by the discus-sion of various acoustic-prosodic as well as othertypes of features used in our humorous-speechanalysis and classification experiments.
We thenpresent our experiments, results, and finally endwith conclusions and future work.2082 FRIENDS Corpus(Scherer, 2003) discuss a number of pros and consof using real versus acted data, in the context ofemotional speech analysis.
His main argument isthat while real data offers natural expressions ofemotions, it is not only hard to collect (due to eth-ical issues) but also very challenging to annotateand analyze, as there are very few instances ofstrong expressions and the rest are often very sub-tle.
Acted data (also referred to as portrayed orsimulated), on the other hand, offers ample of pro-totypical examples, although these are criticizedfor not being natural at times.
To achieve somebalance between naturalness and strength/numberof humorous expressions, we decided to use di-alogs from a comedy television show FRIENDS,which provides classical examples of casual, hu-morous conversations between friends who oftendiscuss very real-life issues, such as job, career,relationships etc.We collected a total of 75 dialogs (scenes) fromsix episodes of FRIENDS, four from Season I(Monica Gets a New Roommate, The One withTwo Parts: Part 1 and 2, All the Poker) and twofrom Season II (Ross Finds Out, The Prom Video),all available on The Best of Friends Volume IDVD.
This gave us approximately 2 hrs of audio.Text transcripts of these episodes were obtainedfrom: http://www.friendscafe.org/scripts.shtml,and were used to extract lexical features (used laterin classification).Figure 1 shows an excerpt from one of the di-alogs in our corpus.3 Audio Segmentation and AnnotationWe segmented each audio file (manually) by mark-ing speaker turn boundaries, using Wavesurfer(http://www.speech.kth.se/wavesurfer).
We applya fairly straightforward annotation scheme to au-tomatically identify humorous and non-humorousturns in our corpus.
Speaker turns that are fol-lowed by artificial laughs are labeled as Humor-ous, and all the rest as Non-Humorous.
For ex-ample, in the dialog excerpt shown in figure 1,turns 3, 7, 9, 11 and 16 are marked as humor-ous, whereas turns 1, 2, 5, 6, 13, 14, 15 aremarked as non-humorous.
Artificial laughs, si-lences longer than 1 second and segments of au-dio that contain purely non-verbal sounds (suchas phone rings, door bells, music etc.)
were ex-cluded from the analysis.
By considering only[1] Rachel: Guess what?
[2] Ross: You got a job?
[3] Rachel: Are you kidding?
I am trained fornothing!
[4] <Laughter>[5] Rachel: I was laughed out of twelve inter-views today.
[6] Chandler: And yet you?re surprisingly up-beat.
[7] Rachel: You would be too if you found Johnand David boots on sale, fifty percent off!
[8] <Laughter>[9] Chandler: Oh, how well you know me...[10] <Laughter>[11] Rachel: They are my new, I don?t need a job,I don?t need my parents, I got great boots, boots!
[12] <Laughter>[13] Monica: How?d you pay for them?
[14] Rachel: Uh, credit card.
[15] Monica: And who pays for that?
[16] Rachel: Um... my...
father.
[17] <Laughter>Figure 1: Dialog Excerptspeaker turns that are followed by laughs as hu-morous, we also automatically eliminate cases ofpure visual comedy where humor is expressed us-ing only gestures or facial expressions.
In short,non-verbal sounds or silences followed by laughsare not treated as humorous.
Henceforth, byturn, we mean proper speaker turns (and not non-verbal turns).
We currently do not apply any spe-cial filters to remove non-verbal sounds or back-ground noise (other than laughs) that overlap withspeaker turns.
However, if artificial laughs overlapwith a speaker turn (there were only few such in-stances), the speaker turn is chopped by marking aturn boundary exactly before/after the laughs be-gin/end.
This is to ensure that our prosody anal-ysis is fair and does not catch any cues from thelaughs.
In other words, we make sure that ourspeaker turns are clean and not garbled by laughs.After segmentation, we got a total of 1629speaker turns, of which 714 (43.8%) are humor-ous, and 915 (56.2%) are non-humorous.
We alsomade sure that there is a 1-to-1 correspondence be-tween speaker turns in text transcripts that wereobtained online and our audio segments, and cor-rected few cases where there was a mis-match (dueto turn-chopping or errors in online transcripts).209Figure 2: Audio Segmentation, Transcription and Feature Extraction using Wavesurfer4 Speaker DistributionsThere are 6 main actors/speakers (3 male and 3 fe-male) in this show, along with a number of (in ourdata 26) guest actors who appear briefly and rarelyin some of our dialogs.
As the number of guestactors is quite large, and their individual contribu-tion is less than 5% of the turns in our data, wedecided to group all the guest actors together inone GUEST class.As these are acted (not real) conversations,there were only few instances of speaker turn-overlaps, where multiple speakers speak together.These turns were given a speaker label MULTI.
Ta-ble 1 shows the total number of turns and humor-ous turns for each speaker, along with their per-centages in braces.
Percentages for the Humor col-umn show, out of the total (714) humorous turns,how many are by each speaker.
As one can notice,the distribution of turns is fairly balanced amongthe six main speakers.
We also notice that eventhough each guest actors?
individual contributionis less than 5% in our data, their combined contri-bution is fairly large, almost 16% of the total turns.Table 2 shows that the six main actors togetherform a total of 83% of our data.
Also, of the to-tal 714 humorous turns, 615 (86%) turns are bythe main actors.
To study if prosody of humor dif-fers across males and females, we also groupedthe main actors into two gender classes.
Table2 shows that the gender distribution is fairly bal-Speaker #Turns(%) #Humor (%)Chandler (M) 244 (15) 163 (22.8)Joey (M) 153 (9.4) 57 (8)Monica (F) 219 (13.4) 74 (10.4)Phoebe (F) 180 (11.1) 104 (14.6)Rachel (F) 273 (16.8) 90 (12.6)Ross (M) 288 (17.7) 127 (17.8)GUEST (26) 263 (16.1) 95 (13.3)MULTI 9 (0.6) 4 (0.6)Table 1: Speaker Distributionanced among the main actors, with 50.5% maleand 49.5% female turns.
We also see that of the685 male turns, 347 turns (almost 50%) are hu-morous, and of the 672 female turns, 268 (ap-proximately 40%) are humorous.
Guest actors andmulti-speaker turns are not considered in the gen-der analysis.Speaker #Turns #HumorMale 685 347(50.5% of Main) (50.6% of Male)Female 672 268(49.5% Of Main) (39.9% of Female)Total 1357 615Main (83.3% of Total) (86.1% of Humor)Table 2: Gender Distribution for Main Actors2105 FeaturesLiterature in emotional speech analysis (Liscombeet al, 2003)(Litman and Forbes-Riley, 2004)(Scherer, 2003)(Ang et al, 2002) has shown thatprosodic features such as pitch, energy, speak-ing rate (tempo) are useful indicators of emotionalstates, such as joy, anger, fear, boredom etc.
Whilehumor is not necessarily considered as an emo-tional state, we noticed that most humorous ut-terances in our corpus (and also in general) oftenmake use of hyper-articulations, similar to thosefound in emotional speech.For this study, we use a number of acoustic-prosodic as well as some non acoustic-prosodicfeatures as listed below:Acoustic-Prosodic Features: Pitch (F0): Mean, Max, Min, Range, Stan-dard Deviation Energy (RMS): Mean, Max, Min, Range,Standard Deviation Temporal: Duration, Internal Silence, TempoNon Acoustic-Prosodic Features: Lexical Turn Length (#Words) SpeakerOur acoustic-prosodic features make use ofthe pitch, energy and temporal information inthe speech signal, and are computed usingWavesurfer.
Figure 2 shows Wavesurfer?s energy(dB), pitch (Hz), and transcription (.lab) panes.The transcription interface shows text correspond-ing to the dialog turns, along with the turn bound-aries.
All features are computed at the turn level,and essentially measure the mean, maximum, min-imum, range (maximum-minimum) and standarddeviation of the feature value (F0 or RMS) overthe entire turn (ignoring zeroes).
Duration is mea-sured in terms of time in seconds, from the be-ginning to the end of the turn including pauses(if any) in between.
Internal silence is measuredas the percentage of zero F0 frames, and essen-tially account for the amount of silence in the turn.Tempo is computed as the total number of sylla-bles divided by the duration of the turn.
For com-puting the number of syllables per word, we usedthe General Inquirer database (Stone et al, 1966).Our lexical features are simply all words (alpha-numeric strings including apostrophes and stop-words) in the turn.
The value of these features isintegral and essentially counts the number of timesa word is repeated in the turn.
Although this indi-rectly accounts for alliterations, in the future stud-ies, we plan to use more stylistic lexical featureslike (Mihalcea and Strapparava, 2005).Turn length is measured as the number of wordsin the turn.
For our classification study, we con-sider eight speaker classes (6 Main actors, 1 forGuest and Multi) as shown in table 1, whereas forthe gender study, we consider only two speakercategories (male and female) as shown in table 2.6 Humor-Prosody AnalysisFeature Humor Non-HumorMean-F0 206.9 208.9Max-F0* 299.8 293.5Min-F0* 121.1 128.6Range-F0* 178.7 164.9StdDev-F0 41.5 41.1Mean-RMS* 58.3 57.2Max-RMS* 76.4 75Min-RMS* 44.2 44.6Range-RMS* 32.16 30.4StdDev-RMS* 7.8 7.5Duration* 3.18 2.66Int-Sil* 0.452 0.503Tempo* 3.21 3.03Length* 10.28 7.97Table 3: Humor Prosody: Mean feature values forHumor and Non-Humor groupsTable 3 shows mean values of various acoustic-prosodic features over all speaker turns in our data,across humor and non-humor groups.
Featuresthat have statistically (p<=0.05 as per indepen-dent samples t-test) different values across the twogroups are marked with asterisks.
As one cansee, all features except Mean-F0 and StdDev-F0show significant differences across humorous andnon-humorous speech.
Table 3 shows that humor-ous turns in our data are longer, both in terms ofthe time duration and the number of words, thannon-humorous turns.
We also notice that humor-ous turns have smaller internal silence, and hencerapid tempo.
Pitch (F0) and energy (RMS) fea-tures have higher maximum, but lower minimum211values, for humorous turns.
This in turn giveshigher values for range and standard deviation forhumor compared to the non-humor group.
This re-sult is somewhat consistent with previous findingsof (Liscombe et al, 2003) who found that most ofthese features are largely associated with positiveand active emotional states such as happy, encour-aging, confident etc.
which are likely to appear inour humorous turns.7 Gender Effect on Humor-ProsodyTo analyze prosody of humor across two genders,we conducted a 2-way ANOVA test, using speakergender (male/female) and humor (yes/no) as ourfixed factors, and each of the above acoustic-prosodic features as a dependent variable.
Thetest tells us the effect of humor on prosody ad-justed for gender, the effect of gender on prosodyadjusted for humor and also the effect of interac-tion between gender and humor on prosody (i.e.if the effect of humor on prosody differs accord-ing to gender).
Table 4 shows results of 2-wayANOVA, where Y shows significant effects, andN shows non-significant effects.
For example, theresult for tempo shows that tempo differs signifi-cantly only across humor and non-humor groups,but not across the two gender groups, and thatthere is no effect of interaction between humorand gender on tempo.
As before, all features ex-cept Mean-F0 and StdDev-F0 show significant dif-ferences across humor and no-humor conditions,even when adjusted for gender differences.
Thetable also shows that all features except inter-nal silence and tempo show significant differencesacross two genders, although only pitch features(Max-F0, Min-F0, and StdDev-F0) show the ef-fect of interaction between gender and humor.
Inother words, the effect of humor on these pitch fea-tures is dependent on gender.
For instance, if malespeakers raise their pitch while expressing humor,female speakers might lower.
To confirm this,we computed means values of various features formales and females separately (See Tables 5 and6).
These tables indeed suggest that male speak-ers show higher values for pitch features (Mean-F0, Min-F0, StdDev-F0), while expressing humor,whereas females show lower.
Also for male speak-ers, differences in Min-F0 and Min-RMS valuesare not statistically significant across humor andnon-humor groups, whereas for female speakers,features Mean-F0, StdDev-F0 and tempo do notshow significant differences across the two groups.One can also notice that the differences in themean pitch feature values (specifically Mean-F0,Max-F0 and Range-F0) between humor and non-humor groups are much higher for males than forfemales.In summary, our gender analysis shows that al-though most acoustic-prosodic features are differ-ent for males and females, the prosodic style of ex-pressing humor by male and female speakers dif-fers only along some pitch-features (both in mag-nitude and direction).Feature Humor Gender Humorx GenderMean-F0 N Y NMax-F0 Y Y YMin-F0 Y Y YRange-F0 Y Y NStdDev-F0 N Y YMean-RMS Y Y NMax-RMS Y Y NMin-RMS Y Y NRange-RMS Y Y NStdDev-RMS Y Y NDuration Y Y NInt-Sil Y N NTempo Y N NLength Y Y NTable 4: Gender Effect on Humor Prosody: 2-WayANOVA Results8 Speaker Effect on Humor-ProsodyWe then conducted similar ANOVA test to accountfor the speaker differences, i.e.
by considering hu-mor (yes/no) and speaker (8 groups as shown in ta-ble 1) as our fixed factors and each of the acoustic-prosodic features as a dependent variable for a 2-Way ANOVA.
Table 7 shows results of this analy-sis.
As before, the table shows the effect of humoradjusted for speaker, the effect of speaker adjustedfor humor and also the effect of interaction be-tween humor and speaker, on each of the acoustic-prosodic features.
According to table 7, we nolonger see the effect of humor on features Min-F0, Mean-RMS and Tempo (in addition to Mean-F0 and StdDev-F0), in presence of the speakervariable.
Speaker, on the other hand, shows sig-nificant effect on prosody for all features.
But212Feature Humor Non-HumorMean-F0* 188.14 176.43Max-F0* 276.94 251.7Min-F0 114.54 113.56Range-F0* 162.4 138.14StdDev-F0* 37.83 34.27Mean-RMS* 57.86 56.4Max-RMS* 75.5 74.21Min-RMS 44.04 44.12Range-RMS* 31.46 30.09StdDev-RMS* 7.64 7.31Duration* 3.1 2.57Int-Sil* 0.44 0.5Tempo* 3.33 3.1Length* 10.27 8.1Table 5: Humor Prosody for Male Speakerssurprisingly, again only pitch features Mean-F0,Max-F0 and Min-F0 show the interaction effect,suggesting that the effect of humor on these pitchfeatures differs from speaker to speaker.
In otherwords, different speakers use different pitch varia-tions while expressing humor.9 Humor Recognition by SupervisedLearningWe formulate our humor-recognition experimentas a classical supervised learning problem, byautomatically classifying spoken turns into hu-mor and non-humor groups, using standard ma-chine learning classifiers.
We used the decisiontree algorithm ADTree from Weka, and ran a10-fold cross validation experiment on all 1629turns in our data1.
The baseline for these ex-periments is 56.2% for the majority class (non-humorous).
Table 8 reports classification resultsfor six feature categories: lexical alone, lexical +speaker, prosody alone, prosody + speaker, lexical+ prosody and lexical + prosody + speaker (all).Numbers in braces show the number of featuresin each category.
There are total 2025 featureswhich include 2011 lexical (all word types plusturn length), 13 acoustic-prosodic and 1 for thespeaker information.
Feature Length was includedin the lexical feature group, as it counts the num-ber of lexical items (words) in the turn.1We also tried other classifiers like Naive Bayes and Ad-aBoost, although since the results were equivalent to ADTree,we do not report those here.Feature Humor Non-HumorMean-F0 235.79 238.75Max-F0* 336.15 331.14Min-F0* 133.63 143.14Range-F0* 202.5 188StdDev-F0 46.33 46.6Mean-RMS* 58.44 57.64Max-RMS* 77.33 75.57Min-RMS* 44.08 44.74Range-RMS* 33.24 30.83StdDev-RMS* 8.18 7.59Duration* 3.35 2.8Int-Sil* 0.47 0.51Tempo 3.1 3.1Length* 10.66 8.25Table 6: Humor Prosody for Female SpeakersAll results are significantly above the baseline(as measured by a pair-wise t-test) with the bestaccuracy of 64% (8% over the baseline) obtainedusing all features.
We notice that the classifica-tion accuracy improves on adding speaker infor-mation to both lexical and prosodic features.
Al-though these results do not show a strong evidencethat prosodic features are better than lexical, it isinteresting to note that the performance of just afew (13) prosodic features is comparable to thatof 2011 lexical features.
Figure 3 shows the deci-sion tree produced by the classifier in 10 iterations.Numbers indicate the order in which the nodes arecreated, and indentations mark parent-child rela-tions.
We notice that the classifier primarily se-lected speaker and prosodic features in the first10 iterations, whereas lexical features were se-lected only in the later iterations (not shown here).This seems consistent with our original hypothe-sis that speech features are better at discriminatingbetween humorous and non-humorous utterancesin speech than lexical content.Although (Mihalcea and Strapparava, 2005) ob-tained much higher accuracies using lexical fea-tures alone, it might be due to the fact that our datais homogeneous in the sense that both humorousand non-humorous turns are extracted from thesame source, and involve same speakers, whichmakes the two groups highly alike and hence chal-lenging to distinguish.
To make sure that thelower accuracy we get is not simply due to usingsmaller data compared to (Mihalcea and Strappar-213Feature Humor Speaker Humorx SpeakerMean-F0 N Y YMax-F0 Y Y YMin-F0 N Y YRange-F0 Y Y NStdDev-F0 N Y NMean-RMS N Y NMax-RMS Y Y NMin-RMS Y Y NRange-RMS Y Y NStdDev-RMS Y Y NDuration Y Y NInt-Sil Y Y NTempo N Y NLength Y Y NTable 7: Speaker Effect on Humor Prosody: 2-Way ANOVA ResultsFeature -Speaker +SpeakerLex 61.14 (2011) 63.5 (2012)Prosody 60 (13) 63.8 (14)Lex + Prosody 62.6 (2024) 64 (2025)Table 8: Humor Recognition Results (% Correct)ava, 2005), we looked at the learning curve for theclassifier (see figure 4) and found that the classi-fier performance is not sensitive to the amount ofdata.Table 9 shows classification results by gender,using all features.
For the male group, the base-line is 50.6%, as the majority class humor is 50.6%(See Table 2).
For females, the baseline is 60%(for non-humorous) as only 40% of the femaleturns are humorous.Gender Baseline ClassifierMale 50.6 64.63Female 60.1 64.8Table 9: Humor Recognition Results by GenderAs Table 9 shows, the performance of the classi-fier is somewhat consistent cross-gender, althoughfor male speakers, the relative improvement ismuch higher (14% above the baseline), than forfemales (only 5% above the baseline).
Our earlierobservation (from tables 5 and 6) that differencesin pitch features between humor and non-humorj (1)SPEAKER = chandler: 0.469j (1)SPEAKER != chandler: -0.083j j (4)SPEAKER = phoebe: 0.373j j (4)SPEAKER != phoebe: -0.064j (2)DURATION < 1.515: -0.262j j (5)SILENCE < 0.659: 0.115j j (5)SILENCE >= 0.659: -0.465j j (8)SD F0 < 9.919: -1.11j j (8)SD F0 >= 9.919: 0.039j (2)DURATION >= 1.515: 0.1j j (3)MEAN RMS < 56.117: -0.274j j (3)MEAN RMS >= 56.117: 0.147j j j (7)come < 0.5: -0.056j j j (7)come >= 0.5: 0.417j j (6)SD F0 < 57.333: 0.076j j (6)SD F0 >= 57.333: -0.285j j (9)MAX RMS < 86.186: 0.011j j j (10)MIN F0 < 166.293: 0.047j j j (10)MIN F0 >= 166.293: -0.351j j (9)MAX RMS >= 86.186: -0.972Legend: +ve = humor, -ve = non-humorFigure 3: Decision Tree (only the first 10 iterationsare shown)groups are quite higher for males than for females,may explain why we see higher improvement formale speakers.10 ConclusionsIn this paper, we presented our experiments onhumor-prosody analysis and humor recognitionin spoken conversations, collected from a clas-sic television comedy, FRIENDS.
Using a sim-ple automated annotation scheme, we labeledspeaker turns in our corpus that are followedby artificial laughs as humorous, and the rest asnon-humorous.
We then examined a number ofacoustic-prosodic features based on pitch, energyand temporal information in the speech signal,that have been found useful by previous studies inemotion recognition.Our prosody analysis revealed that humorousand non-humorous turns indeed show significantdifferences in most of these features, even whenaccounted for the speaker and gender differences.Specifically, we found that humorous turns tendto have higher tempo, smaller internal silence, andhigher peak, range and standard deviation for pitchand energy, compared to non-humorous turns.On the humor recognition task, our classifier214Figure 4: Learning Curve: %Accuracy versus%Fraction of Dataachieved the best performance when acoustic-prosodic features were used in conjunction withlexical and other types of features, and in all ex-periments attained the accuracy statistically signif-icant over the baseline.
While prosody of humorshows some differences due to gender, the perfor-mance on the humor recognition task is equiva-lent for males and females, although the relativeimprovement over the baseline is much higher formales than for females.Our current study focuses only on lexical andspeech features, primarily because these featurescan be computed automatically.
In the future, weplan to explore more sophisticated semantic andpragmatic features such as incongruity, ambiguity,expectation-violation etc.
We also like to inves-tigate if our findings generalize to other types ofcorpora besides TV-show dialogs.ReferencesC.
Alm, D. Roth, and R. Sproat.
2005.
Emotions fromtext: Machine learning for text-based emotion pre-diction.
In Proceedings of HLT/EMNLP, Vancou-ver, CA.J.
Ang, R. Dhillon, A. Krupski, E. Shriberg, andA.
Stolcke.
2002.
Prosody-based automatic de-tection of annoyance and frustration in human-computer dialog.
In Proceedings of ICSLP.S.
Attardo.
1994.
Linguistic Theory of Humor.
Moun-ton de Gruyter, Berlin.K.
Binsted and G. Ritchie.
1997.
Computational rulesfor punning riddles.
Humor, 10(1).K.
Binsted, B. Bergen, S. Coulson, A. Nijholt,O.
Stock, C. Strapparava, G. Ritchie, R. Manurung,H.
Pain, A. Waller, and D. O?Mara.
2006.
Com-putational humor.
IEEE Intelligent Systems, March-April.K.
Binsted.
1995.
Using humour to make natural lan-guage interfaces more friendly.
In Proceedings ofthe AI, ALife and Entertainment Workshop, Mon-treal, CA.S.
D?Mello, S. Craig, G. Gholson, S. Franklin, R. Pi-card, and A. Graesser.
2005.
Integrating affect sen-sors in an intelligent tutoring system.
In Proceed-ings of Affective Interactions: The Computer in theAffective Loop Workshop.P.
Gebhard, M. Klesen, and T. Rist.
2004.
Color-ing multi-character conversations through the ex-pression of emotions.
In Proceedings of AffectiveDialog Systems.J.
Klein, Y.
Moon, and R. Picard.
2002.
This computerresponds to user frustration: Theory, design, and re-sults.
Interacting with Computers, 14.J.
Liscombe, J. Venditti, and J. Hirschberg.
2003.Classifying subject ratings of emotional speech us-ing acoustic features.
In Proceedings of Eurospeech,Geneva, Switzerland.D.
Litman and K. Forbes-Riley.
2004.
Predictingstudent emotions in computer-human tutoring dia-logues.
In Proceedings of ACL, Barcelona, Spain.R.
Mihalcea and C. Strapparava.
2005.
Makingcomputers laugh: Investigations in automatic humorrecognition.
In Proceedings of HLT/EMNLP, Van-couver, CA.M.
Mulder and A. Nijholt.
2002.
Humor research:State of the art.
Technical Report 34, CTIT Techni-cal Report Series.Scherer.
2003.
Vocal communication of emotion: Areview of research paradigms.
Speech Communica-tion, 40(1-2):227?256.M.
Shroder and R. Cowie.
2005.
Toward emotion-sensitive multimodal interfaces: the challenge of theeuropean network of excellence humaine.
In Pro-ceedings of User Modeling Workshop on Adaptingthe Interaction Style to Affective Factors.O.
Stock and C. Strapparava.
2005.
Hahaacronym:A computational humor system.
In Proceedings ofACL Interactive Poster and Demonstration Session,pages 113?116, Ann Arbor, MI.P.
Stone, D. Dunphy, M. Smith, and D. Ogilvie.
1966.The General Inquirer: A Computer Approach toContent Analysis.
MIT Press, Cambridge, MA.J.
Taylor and L. Mazlack.
2004.
Computationally rec-ognizing wordplay in jokes.
In Proceedings of theCogSci 2004, Chicago, IL.215
