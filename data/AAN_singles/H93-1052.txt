ONE SENSE PER COLLOCATIONDavid Yarowsky*Department  of  Computer  and In format ion ScienceUnivers i ty of  Pennsy lvaniaPhiladelphia, PA 19104yarowsky@unagi .c is .upenn.eduABSTRACTPrevious work \[Gale, Church and Yarowsky, 1992\] showed that withhigh probability a polysemous word has one sense per discourse.In this paper we show that for certain definitions of collocation, apolysemous word exhibits essentially only one sense per collocation.We test his empirical hypothesis for several definitions of sense andcollocation, and discover that it holds with 90-99% accuracy forbinary ambiguities.
We utilize this property in a disambiguationalgorithm that achieves precision of 92% using combined models ofvery local context.1.
INTRODUCTIONThe use of collocations to resolve lexical ambiguities i cer-tainly not a new idea.
The first approaches to sense dis-ambiguation, such as \[Kelly and Stone 1975\], were basedon simple hand-built decision tables consisting almost ex-clusively of questions about observed word associations inspecific positions.
Later work from the AI community reliedheavily upon selectional restrictions for verbs, although pri-marily in terms of features exhibited by their arguments ( uchas +DRINKABLE) rather than in terms of individual words orword classes.
More recent work \[Brown et al 1991\]\[Hearst1991\] has utilized a set of discrete local questions (such asword-to-the-right) in the development of statistical decisionprocedures.
However, astrong trend in recent years is to treata reasonably wide context window as an unordered bag of in-dependent evidence points.
This technique from informationretrieval has been used in neural networks, Bayesian discrim-inators, and dictionary definition matching.
In a comparativepaper in this volume \[Leacock et al 1993\], all three methodsunder investigation used words in wide context as a pool ofevidence independent of relative position.
It is perhaps nota coincidence that this work has focused almost exclusivelyon nouns, as will be shown in Section 6.2.
In this studywe will return again to extremely local sources of evidence,and show that models of discrete syntactic relationships haveconsiderable advantages.
*This research was supported by an NDSEG Fellowship and by DARPAgrant N00014-90-J-1863.
The author is also affiliated with the LinguisticsResearch Department ofAT&T Bell Laboratories, and greatly appreciates theuse of its resources in support of this work.
He would also like to thank EricBfill, Bill Gale, Libby Levison, Mitch Marcus and Philip Resnik for theirvaluable feedback.2.
DEF IN IT IONS OF  SENSEThe traditional definition of word sense is "One of severalmeanings assigned to the same orthographic string".
Asmeanings can always be partitioned into multiple refinements,senses are typically organized in a tree such as one finds in adictionary.
In the extreme case, one could continue makingrefinements until a word has a slightly different sense everytime it is used.
If so, the title of this paper is a tautology.However, the studies in this paper are focused on the sensedistinctions atthe top of the tree.
A good working definition ofthe distinctions considered are those meanings which are nottypically translated to the same word in a foreign language.Therefore, one natural type of sense distinction to considerare those words in English which indeed have multiple trans-lations in a language such as French.
As is now standard inthe field, we use the Canadian Hansards, a parallel bilingualcorpus, to provide sense tags in the form of French transla-tions.
Unfortunately, the Hansards are highly skewed in theirsense distributions, and it is difficult to find words for whichthere are adequate numbers of a second sense.
More diverselarge bilingual corpora re not yet readily available.We also use data sets which have been hand-tagged bynativeEnglish speakers.
To make the selection of sense distinc-tions more objective, we use words such as bass where thesense distinctions (fish and musical instrument) correspondto pronunciation differences (\[b~es\] and \[beIs\]).
Such data isoften problematic, as the tagging is potentially subjective anderror-filled, and sufficient quantities are difficult o obtain.As a solution to the data shortages for the above methods,\[Gale, Church and Yarowsky 1992b\] proposed the use of"pseudo-words," artificial sense ambiguities created by tak-ing two English words with the same part of speech (such asguerilla and reptile), and replacing each instance of both in acorpus with a new polysemous word guerrilla~reptile.
As itis entirely possible that the concepts guerrilla nd reptile arerepresented by the same orthographic string in some foreignlanguage, choosing between these two meanings based oncontext is a problem a word sense disambiguation algorithmcould easily face.
"Pseudo-words" are very useful for devel-oping and testing disambiguation methods because of theirnearly unlimited availability and the known, fully reliable266ground truth they provide when grading performance.Finally, we consider sense disambiguation for mediums otherthan clean English text.
For example, we look at word pairssuch as terse/tense and cookie/rookie which may be plausi-bly confused in optical character recognition (OCR).
Homo-phones, such as aid~aide, and censor/sensor, are ideal can-didates for such a study because large data sets with knownground truth are available in written text, yet they are trueambiguities which must be resolved routinely in oral commu-nication.We discover that the central claims of this paper hold for allof these potential definitions of sense.
This corroboratingevidence makes us much more confident in our results than ifthey were derived solely from a relatively small hand-taggeddata set.3.
DEF IN IT IONS OF  COLLOCATIONCollocation means the co-occurrence of two words in somedefined relationship.
We look at several such relationships, in-cluding direct adjacency and first word to the left or right hav-ing a certain part-of-speech.
We also consider certain directsyntactic relationships, uch as verb/object, subject/verb, andadjective/noun pairs.
It appears that content words (nouns,verbs, adjectives, and adverbs) behave quite differently fromfunction words (other parts of speech); we make use of thisdistinction in several definitions of collocation.We will attempt to quantify the validity of the one-sense-per-collocation hypothesis for these different collocation types.4.
EXPERIMENTSIn the experiments, we ask two central, related questions:For each definition of sense and collocation,?
What is the mean entropy of the distributionPr(Sense\[Collocation)??
What is the performance of a disambiguation algorithmwhich uses only that collocation type as evidence?We examine several permutations for each, and are interestedin how the results of these questions differ when applied topolysemous nouns, verbs, and adjectives.To limit the already very large number of parameters consid-ered, we study only binary sense distinctions.
In all cases thesenses being compared have the same part of speech.
Theselection between different possible parts of speech as beenheavily studied and is not replicated here.4.1.
Sample CollectionAll samples were extracted from a 380 million word cor-pus collection consisting of newswire text (AP Newswire and?
Hand Tagged (homographs): bass, axes, chi, bow,colon, lead, IV, sake, tear, ...?
French Translation Distinctions: sentence, duty, drug,language, position, paper, single ....?
Homophones: aid/aide, cellar/seller, censor/sensor,cue/queue, pedal/petal ....?
OCR Ambiguities: terse/tense, gum/gym, deaf/dear,cookie/rookie, beverage/leverage ....?
Pseudo-Words: covered/waved, kissed/slapped,abused/escorted, cute/compatible ....Table 1: A sample of the words used in the experimentsWall Street Journal), scientific abstracts (from NSF and theDepartment ofEnergy), the Canadian Hansards parliamentarydebate records, Grolier's Encyclopedia, a medical encyclo-pedia, over 100 Harper & Row books, and several smallercorpora including the Brown Corpus, and ATIS and TIMITsentences.1The homophone pairs used were randomly selected from alist of words having the same pronunciation orwhich differedin only one phoneme.
The OCR and pseudo-word pairs wererandomly selected from corpus wordlists, with the formerrestricted to pairs which could plausibly be confused in anoisy FAX, typically words differing in only one character.Due to the difficulty of obtaining new data, the hand-taggedand French translation examples were borrowed from thoseused in our previous tudies in sense disambiguation.4.2.
Measuring EntropiesWhen computing the entropy of Pr(Sense\[Collocation),we enumerate all collocations of a given type observed for theword or word pair being disambiguated.
Table 2 shows theexample of the homophone ambiguity aid~aide for the collo-cation type content-word-to-the-left.
We list all words 2 ap-pearing in such a collocation with either of these two "senses"of the homograph, and calculate the raw distributional countfor each.Note that the vast majority of the entries in Table 2 have zeroas one of the frequency counts.
It is not acceptable, however,t Training and test samples were not only extracted from different articlesor discourses but also from entirely different blocks of the corpus.
This wasdone to minimize long range discourse ffects such as one finds in the AP orHansards.2Note: the entries in this table are lemmas (uninflected root forms), ratherthan raw words.
By treating the verbal inflections squander, squanders,squandering, and squandered asthe same word, one can improve statisticsand coverage at a slight cost of lost subtlety.
Although we will refer to "wordsin collocation" throughout this paper for simplicity, this should always beinterpreted as "lemmas in collocation.
"267Frequency as Frequency asCollocation Aid Aideforeignfederalwesternprovidezovertapposefuture~imilarpresidential:hieflongtimeaids-infecteddeepydisaffectedLndispensable~ractical;quander7182971468826139600000022110000000634026211100Table 2: A typical collocational distribution for the homo-phone ambiguity aid/aide.to treat these as having zero probability and hence a zeroentropy for the distribution.
It is quite possible, especiallyfor the lower frequency distributions, that we would see acontrary example in a larger sample.
By cross-validation, wediscover for the aid~aide xample that for collocations with anobserved 1/0 distribution, we would actually expect he minorsense to occur 6% of the time in an independent sample, onaverage.
Thus a fairer distribution would be .94/.06, givinga cross-validated ntropy of .33 bits rather than 0 bits.
Fora more unbalanced observed istribution, such as 10/0, theprobability of seeing the minor sense decreases to 2%, givinga cross-validated ntropy of H(.98,.02) = .14 bits.
Repeatingthis process and taking the weighted mean yields the entropyof the full distribution, in this case .09 bits for the aid/aideambiguity.For each type of collocation, we also compute how well anobserved probability distribution predicts the correct classifi-cation for novel examples.
In general, this is a more usefulmeasure for most of the comparison purposes we will address.Not only does it reflect he underlying entropy of the distribu-tion, but it also has the practical advantage of showing how aworking system would perform given this data.5.
ALGORITHMThe sense disambiguation algorithm used is quite straightfor-ward.
When based on a single collocation type, such as theobject of the verb or word immediately to the left, the pro-cedure is very simple.
One identifies if this collocation typeexists for the novel context and if the specific words foundare listed in the table of probability distributions (as computedabove).
If so, we return the sense which was most frequentfor that collocation in the training data.
If not, we return thesense which is most frequent overall.When we consider more than one collocation type and com-bine evidence, the process is more complicated.
The algo-rithm used is based on decision lists \[Rivest, 1987\], and wasdiscussed in \[Sproat, Hirschberg, and Yarowsky 1992\].
Thegoal is to base the decision on the single best piece of evi-dence available.
Cross-validated probabilities are computedas in Section 4.2, and the different ypes of evidence aresorted by the absolute value of the log of these probabil-?
?
.
P r  Sense l  Co l loca~ion i )  ratios.
Abs(Log(prls,n,~  Conocauo,~,)))" When a novel ltycontext is encountered, one steps through the decision listuntil the evidence at that point in the list (such as word-to-/eft="presidential") matches the current context under con-sideration.
The sense with the greatest listed probability isreturned, and this cross-validated probability represents heconfidence in the answer.This approach is well-suited for the combination of multi-ple evidence types which are clearly not independent (suchas those found in this study) as probabilities are never com-bined.
Therefore this method offers advantages over Bayesianclassifier techniques which assume independence of the fea-tures used.
It also offers advantages over decision tree basedtechniques because the training pools are not split at eachquestion.
The interesting problems are how one should re-estimate probabilities conditional on questions asked earlierin the list, or how one should prune lower evidence whichis categorically subsumed by higher evidence or is entirelyconditional on higher evidence.
\[Bahl et al 1989\] have dis-cussed some of these issues at length, and there is not spaceto consider them here.
For simplicity, in this experiment nosecondary smoothing or pruning is done.
This does not ap-pear to be problematic when small numbers of independentevidence types are used, but performance should increase ifthis extra step is taken.6.
RESULTS AND DISCUSSION6.1.
One  Sense Per  Co l locat ionFor the collocations tudied, it appears that the hypothesisof one sense per collocation holds with high probability forbinary ambiguities.
The experimental results in the precisioncolumn of Table 3 quantify the validity of this claim.
Accu-racy varies from 90% to 99% for different types of collocationand part of speech, with a mean of 95%.
The significance ofthese differences will be discussed in Section 6.2.These precision values have several interpretations.
First,they reflect the underlying probability distributions of sense268Collocation Part Ent Prec Rec No NoType of Sp.
Coil DataContent ALL .18 .97 .29 .57 .14word to Noun .98 .25 .66 .09immediate Verb .95 .14 .71 .15right \[A\] Adj .97 .51 .27 .22Content ALL .24 .96 .26 .58 .16word to Noun .99 .33 .56 .11immediate Verb .91 .23 .47 .30left \[B\] Adj .96 .15 .75 .10First ALL .33 .94 .51 .09 .40Content Noun .94 .49 .13 .38Word to Verb .91 .44 .05 .51Right Adj .96 .58 .04 .38First ALL .40 .92 .50 .06 .44Content Noun .96 .58 .06 .36Word to Verb .87 .37 .05 .58Left Adj .90 .45 .06 .49Subject ~ Noun .33 .94 .13 .87 .06Verb Pairs Verb .43 .91 .28 .33 .38Verb ~ Noun .46 .90 .07 .81 .07Object Pairs Verb .29 .95 .36 .32 .32Adj ~-+ Noun Adj .14 .98 .54 .20 .26A&BAbove ALL - .97 .47 I .31 I .21 IAll Above ALL - .92 .98 .00 .02Table 3: IncludestheentropyofthePr(SennelGollocation ) distributionfor several types of collocation, and the performance achieved when basingsense disambiguation solely on that evidence.
Results are itemized by thepart of speech of the ambiguous word (not of the collocate).
Precision (Prec.
)indicates percent correct and Recall (Rec.)
refers to the percentage of samplesfor which an answer is returned.
Precision is measured on this subset.
Nocollocation (No Coil) indicates the failure to provide an answer becauseno collocation of that type was present in the test context, and "No Data"indicates the failure to return an answer because no data for-the observedcollocation was present in the model.
See Section 7.3 for a discussion of the"All Above" result.
The results tated above are based on the average of thedifferent types of sense considered, and have a mean prior probability of .69and a mean sample size of 3944.conditional on collocation.
For example, for the collocationtype content-word-to-the-right, t evalue of .97 indicates thaton average, given a Specific collocation we will expect o seethe same sense 97% of the time.
This mean distribution isalso reflected in the entropy column.However, these numbers have much more practical interpre-tations.
If we actually build a disambiguation procedure usingexclusively the content word to the right as information, sucha system performs with 97% precision on new data where acontent word appears to the right and for which there is in-formation in the model .3 This is considerably higher than the3The correlation between these numbers is not a coincidence.
Becausethe probability distributions are based oncross-validated t sts on indepen-dent data and weighted by collocation frequency, if on average we find thatPer formance  Us ing  Ev idence  a t  D i f fe rent  D is tances8" - ~ t  VerbsAdjectives2'o ,'o 8'o go ,noDistanceFigure 1: Comparison of the performance ofnouns, verbs andadjectives based strictly on a 5 word window centered at thedistance shown on the horizontal axis.performance of 69% one would expect simply by chance dueto the unbalanced prior probability of the two senses.It should be noted that such precision is achieved at onlypartial recall.
The three rightmost columns of Table 3 givethe breakdown of the recall.
On average, the model content-word-to-right could only be applied in 29% of the test samples.In 57% of the cases, no content word appeared to the right,so this collocational model did not hold.
In 14% of the cases,a content word did appear to the right, but no instances ofthat word appeared in the training data, so the model had noinformation on which to base a decision.
There are severalsolutions to both these deficiencies, and they are discussed inSection 7.6.2.
Part of  Speech DifferencesIt is interesting to note the difference in behavior betweendifferent parts of speech.
Verbs, for example, derive moredisambiguating information from their objects (.95) than fromtheir subjects (.90).
Adjectives derive almost all of theirdisambiguatinginformation fr m the nouns they modify (.98).Nouns are best disambiguated bydirectly adjacent adjectivesor nouns, with the content word to the left indicating a singlesense with 99% precision.
Verbs appear to be less usefulfor noun sense disambiguation, although they are relativelybetter indicators when the noun is their object rather than theirsubject.97% of samples of a given collocation exhibit he same sense, this is theexpected precision of a disambiguafion algorithm which assumes one senseper collocation, when applied to new samples of these collocations.269Figure \] shows that nouns, verbs and adjectives also differ intheir ability to be disambiguated by wider context.
\[Gale tal.
1993\] previously showed that nouns can be disambiguatedbased strictly on distant context, and that useful informationwas present up to 10,000 words away.
We replicated an exper-iment in which performance was calculated for disambigua-tions based strictly on 5 word windows centered at variousdistances (shown on the horizontal axis).
Gale's observationwas tested only on nouns; our experiment also shows thatreasonably accurate decisions may be made for nouns usingexclusively remote context.
Our results in this case are basedon test sets with equal numbers of the two senses.
Hencechance performance is at 50%.
However, when tested onverbs and adjectives, precision drops off with a much steeperslope as the distance from the ambiguous word increases.
Thiswould indicate that approaches giving equal weight o all po-sitions in a broad window of context may be less well-suitedfor handling verbs and adjectives.
Models which give greaterweight o immediate context would seem more appropriate inthese circumstances.A similar experiment was applied to function words, and thedropoff beyond strictly immediate context was precipitous,converging at near chance performance for distances greaterthan 5.
However, function words did appear to have pre-dictive power of roughly 5% greater than chance in directlyadjacent positions.
The effect was greatest for verbs, wherethe function word to the right (typically a preposition or par-ticle) served to disambiguate at a precision of 13% abovechance.
This would indicate that methods which excludefunction words from models to minimize noise should con-sider their inclusion, but only for restricted local positions.6.3.
Comparison of Sense DefinitionsResults for the 5 different definitions of sense ambiguity stud-ied here are similar.
However they tend to fluctuate relativeto each other across experiments, and there appears to beno consistent ordering of the mean entropy of the differenttypes of sense distributions.
Because of the very large num-ber of permutations considered, it is not possible to give afull breakdown of the differences, and such a breakdown doesnot appear to be terribly informative.
The important observa-tion, however, is that the basic conclusions drawn from thispaper hold for each of the sense definitions considered, andhence corroborate and strengthen the conclusions which canbe drawn from any one.6.4.
Performance Given Little EvidenceOne of the most striking conclusions to emerge from this studyis that for the local collocations considered, decisions basedon a single data point are highly reliable.
Normally one wouldconsider a 1/0 sense distribution i  a 3944 sample training setto be noise, with performance based on this information otLow Counts  a re  Re l iab le; ~ ,'o ~o ,;o o;o ,go.,Training Frequency (f)Figure 2: Percentage correct for disambiguations based solelyon a single content-word-to-the-rightcollocation seen ft imesin the training data without counter-examples.likely to much exceed the 69% prior probability expected bychance.
But this is not what we observe.
For example, whentested on the word-to-the-right collocation, disambiguationsbased solely on a single data point exceed 92% accuracy, andperformance on 2/0 and 3/0 distributions climb rapidly fromthere, and reach nearly perfect accuracy for training samples assmall as 15/0, as shown in Figure 2.
In contrast, acollocation30 words away which also exhibits a 1/0 sense distributionhas a predictive value of only 3% greater than chance.
Thisdifference in the reliability of low frequency data from localand wide context will have implications for algorithm design.7.
APPL ICAT IONS7.1.
Training Set Creation and VerificationThis last observation has relevance for new data set creationand correction.
Collocations with an ambiguous content wordwhich have frequency greater than 10-15 and which do notbelong exclusively to one sense should be flagged for humanreinspection, as they are most likely in error.
One can speedthe sense tagging process by computing the most frequent col-locates, and for each one assigning all examples to the samesense.
For the data in Table 2, this will apparently fail for theforeignAid/Aide example in 1 out of 719 instances ( till 99.9%correct).
However, in this example the model's classificationwas actually correct; the given usage was a misspelling inthe 1992 AP Newswire: "Bush accelerated foreign aide andweapons ales to Iraq.".
It is quite likely that if were in-deed a foreign assistant being discussed, this example wouldalso have another collocation (with the verb, for example),270which would indicate the correct sense.
Such inconsisten-cies should also be flagged for human supervision.
Workingfrom the most to least frequent collocates in this manner, onecan use previously tagged collocates to automatically suggestthe classification of other words appearing in different collo-cation types for those tagged examples.
The one sense perdiscourse constraint can be used to refine this process further.We are working on a similar use of these two constraints forunsupervised sense clustering.7.2.
Algor i thm DesignOur results also have implications for algorithm design.
Forthe large number of current approaches which treat wide con-text as an unordered bag of words, it may be beneficial tomodel certain local collocations separately.
We have shownthat reliability of collocational evidence differs considerablybetween local and distant context, especially for verbs andadjectives.
If one one is interested inproviding aprobabilitywith an answer, modeling local collocations separately willimprove the probability estimates and reduce cross entropy.Another eason for modeling local collocations separately isthat his will allow the reliable inclusion of evidence with verylow frequency counts.
Evidence with observed frequency dis-tributions of 1/0 typically constitute on the order of 50% ofall available vidence types, yet in a wide context windowthis low frequency evidence is effectively noise, with predic-tive power little better than chance.
However, in very localcollocations, ingle data points carry considerable informa-tion, and when used alone can achieve precision in excess of92%.
Their inclusion should improve system recall, with amuch-reduced danger of overmodeling the data.7.3.
Bui lding a Full  Disambiguation SystemFinally, one may ask to what extent can local collocationalevidence alone support apractical sense disambiguation algo-rithm.
As shown in Table 3, our models of single collocationtypes achieve high precision, but individually their applica-bility is limited.
However, if we combine these models asdescribed in Section 5, and use an additional function wordcollocation model when no other evidence is available, weachieve full coverage at a precision of 92%.
This result iscomparable to those previously reported in the literature us-ing wider context of up to 50 words away \[5,6,7,12\].
Dueto the large number of variables involved, we shall not at-tempt o compare these directly.
Our results are encouraging,however, and and we plan to conduct amore formal compari-son of the "bag of words" approaches relative to our separatemodeling of local collocation types.
We will also consider ad-ditional collocation types covering awider range of syntacticrelationships.
In addition, we hope to incorporate class-basedtechniques, uch as the modeling of verb-argument selectionalpreferences \[Resnik, 1992\], as a mechanism for achieving im-proved performance on unfamiliar collocations.8.
CONCLUSIONThis paper has examined some of the basic distributional prop-erties of lexical ambiguity in the English language.
Our ex-periments have shown that for several definitions of senseand collocation, an ambiguous word has only one sense in agiven collocation with a probability of 90-99%.
We showedhow this claim is influenced by part-of-speech, distance, andsample frequency.
We discussed the implications of theseresults for data set creation and algorithm design, identifyingpotential weaknesses in the common "bag of words" approachto disambiguation.
Finally, we showed that models of localcollocation can be combined in a disambiguation algorithmthat achieves overall precision of 92%.References1.
Bahl, L., P. Brown, P. de Souza, R. Mercer, "A Tree-Based Sta-tistical Language Model for Natural Language Speech Recog-nition," in IEEE Transactions on Acoustics, Speech, and SignalProcessing, 37, 1989.2.
Brown, Peter, Stephen Della Pietra, Vincent Della Pietra, andRobert Mercer, "Word Sense Disambiguation using Statisti-cal Methods," Proceedings of the 29th Annual Meeting of theAssociation for Computational Linguistics, 1991, pp 264-270.3.
Gale, W., K. Church, and D. Yarowsky, "One Sense Per Dis-course," Proceedings of the 4th DARPA Speech and NaturalLanguage Workshop, 1992.4.
Gale, W., K. Church, and D. Yarowsky, "On Evaluation ofWord-Sense Disambiguation Systems," in Proceedings, 30thAnnual Meeting of the Association for Computational Linguis-tics, 1992b.5.
Gale, W., K. Church, and D. Yarowsky, "A Method for Disam-biguating Word Senses in a Large Corpus," in Computers andthe Humanities, 1993.6.
Hearst, Marti, "Noun Homograph Disambiguation Using LocalContext in Large Text Corpora," in Using Corpora, Universityof Waterloo, Waterloo, Ontario, 1991.7.
Leacock, Claudia, Geoffrey Towell and Ellen Voorhees"Corpus-Based Statistical Sense Resolution," inProceedings,ARPA Human Language Technology Workshop, 1993.8.
Kelly, Edward, and Phillip Stone, Computer Recognition ofEnglish Word Senses, North-Holland, Amsterdam, 1975.9.
Resnik, Philip, "A Class-based Approach to Lexical Discov-ery," in Proceedings of 3Oth Annual Meeting of the Associationfor Computational Linguistics, 1992.10.
Rivest, R. L., "Learning Decision Lists," in Machine Learning,2, 1987, pp 229-246.11.
Sproat, R., J. Hirschberg and D. Yarowsky "A Corpus-basedSynthesizer," in Proceedings, International Conference onSpoken Language Processing, Banff, Alberta.
October 1992.12.
Yarowsky, David "Word-Sense Disambiguation Using Statisti-cal Models of Roget's Categories Trained on Large Corpora,"in Proceedings, COLING-92, Nantes, France, 1992.271
