Proceedings of NAACL-HLT 2015 Student Research Workshop (SRW), pages 147?153,Denver, Colorado, June 1, 2015. c?2015 Association for Computational LinguisticsRecognizing Textual Entailment using Dependency Analysis andMachine LearningNidhi Sharma Richa Sharma Kanad K. Biswascs5080219@cse.iitd.ac.in anz087535@cse.iitd.ac.in kkb@cse.iitd.ac.inIndian Institute of Technology DelhiHauz Khas, New Delhi, India - 110016AbstractThis paper presents a machine learning systemthat uses dependency-based features and lexi-cal features for recognizing textual entailment.The proposed system evaluates the featurevalues automatically.
The performance of theproposed system is evaluated by conductingexperiments on RTE1, RTE2 and RTE3 da-tasets.
Further, a comparative study of the cur-rent system with other ML-based systems forRTE to check the performance of the pro-posed system is also presented.
The depend-ency-based heuristics and lexical featuresfrom the current system have resulted in sig-nificant improvement in accuracy over exist-ing state-of-art ML-based solutions for RTE.1 IntroductionRecognizing textual entailment (RTE) has arousedlot of interest in natural language researchcommunity with recent Pascal RTE challenges.RTE provides a generic evaluation framework andis useful across various applications like question-answering, information-extraction, machine trans-lation etc.Textual Entailment is a directional relationbetween text fragments (Dagan et al, 2005) whichholds true when the truth of one text fragment, re-ferred to as ?hypothesis?, follows from another,referred to as ?text?.
The task of recognizing textu-al entailment can be thought of as a classificationproblem to classify a given pair of sentences, text(T) and hypothesis (H), as true or false entailmentas suggested by Bos and Markert (2005).
MachineLearning approaches to RTE challenges have usedcombination of features like syntactic, semantic orlexical features.
However, in most of the cases, thefeatures used for the purpose are either large innumber which makes the evaluation time consum-ing or are not very intuitive which makes them dif-ficult to comprehend.
In our work, we haveattempted to address these two concerns.Our approach uses a combination of dependen-cy and lexical features to train Machine Learning(ML) classifiers.
We use only 8 features that aresimple and intuitive.
The process of evaluatingfeature values is automated, thereby reducing anymanual effort and intervention.
The systemperformance has been tested over RTE1, RTE2 andRTE3 datasets.
Our system shows significantimprovement in accuracy over the state-of-the-artML solutions to RTE challenges.The paper is organized as follows.
Section 2gives a brief of the earlier work of ML basedapproaches for RTE.
Section 3 describes oursolution approach for RTE, including details on thefeatures used and the experimental setup.
Wepresent the results and observations in Section 4,followed by conclusion in Section 5.2 Related WorkThere have been various solution approaches pro-posed to RTE challenges like rule-based, logical-inference based, graph-based and ML-based.
Ofthese, applying ML algorithms to automaticallylearn models from training examples is an effectiveway to approach RTE challenges like other NLPproblems.ML-based systems often use lexical matchingfeatures (Inkpen et al 2006, Kozareva and Motoyo2006 and Pakray et al 2011) such as word overlapcount, word similarity, n-gram, etc, and semantic147features such as WordNet similarity measures(Kozareva and Motoyo 2006).
Inkpen et al (2006)have achieved an accuracy of 52.85% on RTE2dataset using lexical match and mismatch features.Bos and Markert (2005) use a combination ofshallow and deep semantic features using logicalinference to build a hybrid model that achieves anaccuracy of 57.7 % on RTE1 dataset.
They alsoshow that using task label as feature in their modelincreases the overall accuracy to 61.2%.
Pazienzaet al (2009) have defined a measure for textualentailment based on graph matching theory appliedto syntactic dependency graphs.
They performcomparison of rule-based and SVM-basedapproach with rule-based approach giving anaccuracy of 52.45% and SVM-based approachgiving an accuracy of 51.82%.
Pakray et al (2011)describe a two-way textual entailment recognitionsystem that uses lexical features such as n-grammatch, stemming etc.
and syntactic features likesubject comparison, subject-verb comparison, etc.Our approach builds mainly on the works ofInkpen et al (2006) and Pakray et al (2011) and,improves accuracy over their work as presented inthe following section.3 Our ApproachWe have developed an RTE system that takes asinput sentence pairs, text (T) and hypothesis (H),and outputs an entailment decision (True/False) foreach pair.
The system evaluates a set of 8 differentdependency and lexical features for the inputsentence pairs.
One of these features is a mismatchfeature while the other seven are match features.For evaluating the dependency features, we haveused Stanford Dependency parser (Marneffe et al,2006) to obtain the dependency relations present inthe sentences.
We generate a structured representa-tion for both text and hypothesis using theirrespective dependency relations.
This structuredrepresentation is used to evaluate six of the eightlexical and the syntactic features.
Structured repre-sentation proves to be an effective representationof the sentence for calculating feature values.We first present a brief overview of thestructured representation of the sentences beforediscussing the features used in the feature vector todevelop classifiers using ML algorithms.3.1 Structured RepresentationThe Stanford dependencies describe the grammati-cal relationships in a sentence.
Each dependency isa tuple consisting of a pair of words in a sentenceand the relation that links them.
A dependency isrepresented as follows:reln(govVal,depVal)where,reln is the dependency relationdepVal is the dependent valuegovVal is the governing valueThe structured representation is generated byusing the dependency tags and converting them toa slot-filler frame-based structure.
The entitiesextracted from the dependency relations are:a) Subject: The dependencies tagged as nsubj(Nominal Subject), nsubjpass (passive nominalsubject), csubj (clausal subject), csubjpass (passiveclausal subject) and xsubj (controlling subject) areused to extract the words acting as subject in thesentence.b) Subject Modifier: The dependency tagsadvmod (adverbial modifier), amod (adjectivalmodifier), appos (appositional modifier), nn (nouncompound modifier) and npadvmod (noun phraseas adverbial modifier) are used to identify modifi-ers.
Each dependency relation is returned as a pairof (governing value, dependent value).
If for agiven modifier relation, the governing value is asubject in the sentence, then the dependent valueacts as the subject modifier.c) Object: The Stanford parser returns dobj(direct object), iobj (indirect object) and pobj(prepositional object) as tags for different objects.We include all these in the frame entity ?object?.d) Object Modifier: The process to extractobject modifier is similar to the one used for Sub-ject Modifier except that if the governing value inthe modifier relation is an object in the sentence,the dependent value acts as the object modifier.e) Verb: The dependency tagged as root isgenerally the main verb of the sentence.
The tagscop (copula), ccomp (clausal complement) andxcomp (open clausal complement) also list theverbs in the sentence.f) Verb Complements: In some cases, thedependencies tagged as root, xcomp or ccomp(clausal complement) contain noun instead of verb.The dependent value is then listed as a verb com-plement.
The tags acomp (adjectival complement),148pcomp (prepositional complement), advcl(adverberial clause modifier) and vmod (verbalmodifier) also contains dependency values thatcomplement the verb.g) Negation: The parser uses the tag neg(negation modifier) to identify negation words(such as, not, don?t, etc.)
in the sentence.
Thegoverning value of this dependency contains theword (usually verb) it negates.
We store this valuewith the negation word for negation frame entity.h) Number: The dependency tagged as num(numeric modifier) contains a numeric value andthe noun-phrase that it modifies.
We store both thenumber and the entity it modifies under this label.The generation of the frame-based structuredrepresentation is illustrated using statement S1 andthis structured representation is shown in Table 1.S1: A two-day auction of property belonging toactress Katharine Hepburn brought in 3.2 millionpounds.Label ValueSubject AuctionSubject Modifier two-dayObject property, Hepburn,poundsObject Modifier ActressVerb BroughtVerb Complements BelongingNegation -Number 3.2 million (pounds)Table 1: Structured Representation for S13.2 FeaturesAfter obtaining a structured representation, weevaluate the following features (i) to (viii).
Whilefeatures (i) and (ii) have been borrowed from pre-vious work (Inkpen et al 2006, Kozareva and Mo-toyo 2006, Pakray et al 2011), the features (iii),(iv) and (iv), present significant modifications tofeatures used by researchers (Inkpen et al 2006,Molla 2003 and Pakray et al 2011) in the past.
Thefeatures (vi), (vii) and (viii) are new features con-tributing to our feature set.
The dependency over-lap and word overlap features do not requirestructured representation for evaluation.
(i) Word OverlapThis feature is a ratio of the count of directly over-lapping words between text and hypothesis to thecount of words in hypothesis, after removal of stopwords.
A direct word count also takes care of theoverlapping named entities.
This feature is a signif-icant contributor to entailment.
The overlap isevaluated as follows:countHcountTnHpwordOverla ?where,countTnH = number of common words in text andhypothesis after stop word removalcountH = total number of words in hypothesis afterstop word removal(ii) Negation CheckThis feature checks if a verb in hypothesis hasbeen negated in text or vice-versa.
Negation can beexplicit in the form of keywords, such as ?not?,?can't?, ?don't?, etc.
or it can be implicit in the formof antonym or negative sense of the verb.
We cap-ture explicit as well as implicit negation checkthrough the structured representation of the sen-tence.
In order to identify if the antonym of a verb(non-negated) in hypothesis is present in text orvice-versa, we first identify the root form of theverbs present in text as well as hypothesis usingWordnet1.
The root form of the verbs is thenchecked for antonym (or negative sense) relation-ship by using VerbOcean2.This is a binary feature assuming a value of 1for the presence of negation, either explicit orimplicit and, it remains 0 otherwise.
For example,consider the following text-hypothesis pair:T: The Philippine Stock Exchange CompositeIndex rose 0.1 percent to 1573.65H: The Philippine Stock Exchange CompositeIndex dropped.In this example, the verbs ?rose?
and ?dropped?are converted to their root forms ?rise?
and ?drop?respectively and found to have an antonym relation(rise [opposite-of] drop) in VerbOcean.
(iii) Number AgreementThis is a binary feature to check if the numericmodifiers of the same governing entities are inagreement in text-hypothesis pair.
We use struc-tured representation to evaluate this feature.
Thefeature takes a value of 1 for number agreementand 0 otherwise.
We illustrate number agreement1http://projects.csail.mit.edu/jwi/2 http://demo.patrickpantel.com/demos/verbocean/149using the pair T1-H1 and number disagreementwith the help of pair T2-H2 as follows:T1: The twin buildings are 88 stories each,compared with the Sears Tower's 110 stories.H1: The Sears Tower has 110 stories.T2: A small bronze bust of Spencer Tracy soldfor ?174,000.H2: A small bronze bust of Spencer Tracymade ?180,447.
(iv) Dependency OverlapDependency overlap has been considered as a goodapproximation to sentence meaning in context ofquestion-answering problem by Molla (2003).
Wehave borrowed the same idea to approximate theentailment relationship between text and hypothe-sis.
The dependency relations returned by theStanford parser consist of a pair of words from thesentence that are related.
We count such similarpairs irrespective of the relation binding them.
Thevalue of the feature is computed as:countHcountTnHdepOverlap ?where,countTnH = number of overlapping dependencypairs in text and hypothesis and,countH = total number of dependencies in hypoth-esisConsidering an example:T: His family has steadfastly denied the chargesH: The charges were denied by his familyDependency list for T is:[poss(family-2, His-1), nsubj(denied-5, family-2),aux(denied-5, has-3), advmod(denied-5, steadfast-ly-4), root(ROOT-0, denied-5), det(charges-7, the-6), dobj(denied-5, charges-7)]Dependency list for H is:[det(charges-2, The-1), nsubjpass(denied-4,charges-2), auxpass(denied-4, were-3),root(ROOT-0, denied-4), poss(family-7, his-6),agent(denied-4, family-7)]This example has five overlapping dependencypairs, namely: the-charges, denied-charges,ROOT-denied, his-family and denied-family.
Weevaluate dependency overlap for this example asfollows:833.065 ???
countHcountTnHdepOverlap(v) Syntactic Role MatchThis feature is set to 1 if the (subject, object, verb)tuple in text matches the (subject, object, verb)tuple in hypothesis.
The subject and object arematched directly whereas the verbs are matchedafter extracting their root forms from Wordnet andusing the ?similar?
relation from VerbOcean.Similar feature has been used in Pakray et al?s(2011) approach, wherein they have consideredmatching pairs of subject-verb, verb-object, sub-ject-subject and object-object.
However, the se-mantics of any sentence are governed by subject,verb and the object, if present.
Our feature differsin the sense that a value of 1 is assigned for match-ing of the subject, object and the verb altogether;else its value remains 0.
For example:T: Israeli Prime Minister Ariel Sharon threat-ened to dismiss Cabinet ministers who don't sup-port his plan to withdraw from the Gaza Strip.H: Israeli Prime Minister Ariel Sharon warnedto fire cabinet opponents of his Gaza withdrawalplan.In this example, the subject in both T and H isAriel Sharon, the direct object in T is plan whereasthe direct object in H, is opponents but H has planas the prepositional object and so we consider it asan object agreement.
The verbs ?threaten?
in T and?warn?
in H are similar as inferred from Verb-Ocean.
Therefore, the value of syntactic-role matchfeature for the above-mentioned text-hypothesispair is 1.
In contrast, following Pakray et al?s(2011) approach, the value of Wordnet-based sub-ject-verb feature is 0.5 instead of 1 and the value ofWordnet-based verb-object feature is 0 due tomismatch in direct object.
(vi) Complement Verb MatchThe sentences are not always simple and apartfrom main action-verbs, there can be entailmentrelationship due to complementing verb or clausalcomponents.
This feature performs a semanticmatch of root form (derived from Wordnet) of suchverbs of text and hypothesis using VerbOcean.
Inaddition, it also checks if the acting verb ofhypothesis matches the acting verb or verb com-plement of the text and vice-versa.
Let us consideran example to understand such pairs:T: Officials said Michael Hamilton was killedwhen gunmen opened fire and exchanged shotswith Saudi security forces yesterday.150H: Michael Hamilton died yesterday.The main verb in T is ?said?
while the mainverb in H is ?died?
and these verbs do not match.However, ?killed?
is a clausal complement in Twhich is similar to the verb ?died?
in H. Thus, amatch results in this case assigning a value of 1 tothe feature else the value of the feature would be 0.
(vii) Modifier RelationIn this feature, we check if the subject-object pairof hypothesis appears as subject-subject modifieror object-object modifier pair in the text.
It is alsoa binary feature assuming a value of 1 for matchand 0 for mismatch.
For example:T: Antonio Fazio, the Bank of Italy governor,engulfed in controversy.H: Antonio Fazio works for the Bank of Italy.In T, ?Antonio Fazio?
is the subject and ?Bankof Italy governor?
is the appositional modifier ofthe subject.
In H, ?Antonio Fazio?
is the subjectand ?Bank of Italy?
is the object.
Therefore, amatch occurs and the value of feature assigned is 1.
(viii) NominalizationThis features checks for nominal forms of theverbs as there can be correspondence between textand hypothesis owing to nominal verbs.
We checkif the nominal form of a verb in hypothesis acts asobject in the text or the nominal form of verb intext acts as object in hypothesis.
If a match isfound, then we assign 1 to this feature else we as-sign 0.
Following pair presents one such example:T: Satomi Mitarai died of blood loss.H: Satomi Mitarai bled to death.In this example, the verb ?bled?
in H has itsnoun-form ?blood?
in T and the verb ?died?
in Thas its noun-form ?death?
in H.3.3 Experimental SetupThe system performance is evaluated by conduct-ing experiments on RTE1, RTE2 and RTE3 da-tasets.
The RTE1 dataset consists of 567 sentencespairs (T and H) in the development set and 800sentence pairs in the test set.
These sets are furtherdivided into seven subsets, namely: InformationRetrieval (IR), Comparable Documents (CD),Question Answering (QA), Information Extraction(IE), Machine Translation (MT) and ParaphraseAcquisition (PP).
The RTE2 and RTE3 datasetscontain 800 sentence pairs each in their develop-ment as well as test sets.
Both the development andtest sets of RTE2 and RTE3 are subdivided intofour tasks, namely: IE, IR, QA and SUM (summa-rization).We have conducted experiments with differentML algorithms including Support Vector Machines(SVM), Na?ve Bayes and Decision Trees (DT) us-ing Weka3 tool.
For each of the RTE datasets, re-spective training set has been used whileexperimenting with corresponding test-set.
Wehave also performed task based analysis for RTE1dataset.
Following section summarizes the obser-vations of our experiments.4 ResultsTable 2 presents the results achieved with 67%split evaluation of the classifiers on each of thedevelopment (training) datasets:Classifier Accuracy Precision RecallRTE - 1NB 59.28 57.8 68.6SVM 67.02 63.3 80.9DT 66.07 64 73.6RTE - 2NB 60.62 62.2 54.3SVM 65.75 67 62DT 63.0 60.7 73.8RTE - 3NB 64.75 68.2 59.2SVM 66.62 66.4 71.4DT 67.87 67 74Table 2: Validation of system on development setsAs evident from table 2, highest accuracy isachieved with DT algorithm and SVM with RBFkernel.
DT learns very fast and identifies strongrelationship between input and target values (Ville,2006).
In our case, DT turned out to be efficientand fast learners to identify relationship betweenthe feature vectors and the expected entailmentresults.
For SVM, though it is not guaranteedwhich kernel performs better in a situation, RBFkernel is generally more flexible than the linear orpolynomial kernels as it can model a high dimen-sional feature space with minimum error.
The ob-servations with these algorithms are strengthenedby the test-set results as presented in table 3.We have also experimented by using task labelas a feature in our system as Bos and Markert3 http://www.cs.waikato.ac.nz/ml/weka/151(2005) experimented with their system.
Like Bosand Markert?s (2005) observation, we also foundthat the system performance increases with DTalgorithms in contrast to other ML classifiers.Table 4 shows our system?s performance on RTE1,RTE2 and RTE3 datasets using DT algorithm.Classifier Accuracy Precision RecallRTE ?
1NB 57.62 56.4 67.5SVM 57.25 57.6 55.3DT 60.12 60.3 68.7RTE ?
2NB 59.12 60.9 60SVM 59.62 60 61DT 59.87 57.5 73.2RTE ?
3NB 60.62 62.1 63.9SVM 62.12 61.5 69.75DT 62.75 62 71Table3: Performance of system on test setsDataSet Accuracy Precision RecallRTE1 61.25 61.7 57.7RTE2 60.41 62.8 60.3RTE3 64.38 62 78Table 4: System Performance - Task label as FeatureFor task-based analysis, we experimented withthe tasks of RTE1 dataset separately.
We presentthe comparative study of the accuracy achieved byour system with the SVM-based solution of Pazi-enza et al (2005) and DT-based solution of Bosand Markert (2005) in table 5.
The improvement inaccuracy by our system is reflected in table 5.Table 5: Task-based performance comparison for RTE1test setWe carried out a comparative study of our sys-tem with other ML-based systems for RTE tocheck the performance of our system.
The observa-tions from this comparative analysis of our systemwith relevant related systems for RTE along withthe feature counts (FC) used by the respectivesystems in presented in table 6.
The comparativestudy indicates significant improvement in accura-cy of our system over most of the existing state-of-art ML-based solutions for RTE except for fewsolutions only.Accuracy FC RTE1RTE2RTE3(Bos&Markert,2005)4> 8 57.7 - -(Inkpen et al,2006)26 - 58.25  -(Kozareva &Montoyo, 2006)17 - 55.8 -(Pakray et al,2011)16 53.7 59.2 61(MacCartney etal., 2006)28 59.1 - -(Hickl et al,2006)12 - 65.25 -(Adams et al,2006)13 - - 67Ours 8 60.12 59.87 62.75Table 6: Comparison of accuracy of our system withother systems5 ConclusionAs the results indicate, our dependency-based heu-ristics and lexical features have resulted in signifi-cant improvement in accuracy of RTE1, RTE2 andRTE3 datasets.
DT outperforms other classifierswith only 8 features that are syntactic and lexicalin nature.
SVM classifier shows comparable per-formance with the RBF kernel.
The features aresimple and intuitive; easy to comprehend and eval-uate.
The task-based performance for RTE1 datasetshows improved performance as compared to thesimilar study by Pazienza et al (2005) and by Bosand Markert (2005).
We intend to identify moresyntactic and semantic features in future and im-prove upon and, experiment with them to refine theresults further.4Authors have used 8 deep semantic feature and some shallowlexical features, count of which is not clear from the paper.Therefore, we are considering their feature-count to be morethan 8Task Pazi-enzaet al(2005)OurSystem(SVM)Bos &Markert(2005)OurSystem(DT)IE 49.17 59.16 54.2 55.83IR 48.89 71.33 62.2 67.51QA 45.74 63.84 56.9 60.76MT 47.9 62.5 52.5 58.33RC 52.14 62.0 50.7 61.3CD 64.43 83.46 70.0 81.04PP 50.0 78.03 56 75.75152ReferencesAndrew Hickl, Jeremy Bensley, John Williams, KirkRoberts, Bryan Rink and Ying Shi.
2006.
Recogniz-ing Textual Entailment with LCC?s Groundhog Sys-tem.
In Proceedings of the Second PASCALChallenges Workshop on Recognizing Textual En-tailment.Barry de Ville.
2006.
Decision Trees for Business Intel-ligence and Data Mining.
SAS Enterprise Miner.Bill MacCartney, Trond Grenager, Marie-Catherine deMarneffe, Daniel Cer and Christopher D. Manning.2006.
Learning to recognize features of valid textualentailments.
In Proceedings of North AmericanChapter of ACL (NAACL-2006.Diana Inkpen, Darren Kipp, and Vivi Nastase.
2006.Machine Learning Experiments for Textual Entail-ment.
In Proceedings of the Second Challenge Work-shop Recognizing Textual Entailment: 10-15, Italy.Diego Molla.
2003.
Towards semantic-based overlapmeasures for question answering.
In Proceedings ofthe Australasian Language Technology Workshop2003, Australia.Fabio M. Zanzotto, Marco Pennacchiotti and Ales-sandro Moschitti.
2009.
A machine learning ap-proach to textual entailment recognition.
NaturalLanguage Engineering, 15(4): 551-582.Ido Dagan, Oren Glickman and Bernardo Magnini.2005.
The PASCAL Recognizing Textual EntailmentChallenge, In Proceedings of the First PASCAL Rec-ognizing Textual Entailment Workshop.Johan Bos and Katja Markert.
2005.
Recognising textu-al entailment with logical inference, In Proceedingsof the conference on Human Language Technologyand Empirical Methods in Natural Language Pro-cessing: 628-635.Maria T. Pazienza, Marco Pennacchiotti and Fabio M.Zanzotto.
2005.
Textual Entailment as SyntacticGraph Distance: a Rule Based and a SVM Based Ap-proach.
In Proceedings of first PASCAL RTE chal-lenge:528?535.Marie-Catherine de Marneffe, Bill MacCartney andChristopher D. Manning.
2006.
Generating TypedDependency Parses from Phrase Structure Parses.In LREC 2006.Partha Pakray, Alexander Gelbukh and Sivaji Bandyo-padhyay.
2011.
Textual Entailment using Lexical andSyntactic Similarity.
International Journal of Artifi-cial Intelligence & Applications (IJAIA), 2(1): 43-58.Rod Adams, Gabriel Nicolae, Cristina Nicolae and San-da Harabagiu.
2007.
Textual Entailment through Ex-tended Lexical Overlap and Lexico-SemanticMatching.
In Proceedings of the Third PASCALChallenges Workshop on Recognizing Textual En-tailment.Zornitsa Kozareva and Andr?s Montoyo.
2006.
MLEnt:The Machine Learning Entailment System of theUniversity of Alicante.
In Proceedings of the SecondChallenge Workshop Recognizing Textual Entail-ment: 17-20.153
