Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 754?765, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsWeakly Supervised Training of Semantic ParsersJayant KrishnamurthyCarnegie Mellon University5000 Forbes AvenuePittsburgh, PA 15213jayantk@cs.cmu.eduTom M. MitchellCarnegie Mellon University5000 Forbes AvenuePittsburgh, PA 15213tom.mitchell@cmu.eduAbstractWe present a method for training a semanticparser using only a knowledge base and an un-labeled text corpus, without any individuallyannotated sentences.
Our key observation isthat multiple forms of weak supervision can becombined to train an accurate semantic parser:semantic supervision from a knowledge base,and syntactic supervision from dependency-parsed sentences.
We apply our approachto train a semantic parser that uses 77 rela-tions from Freebase in its knowledge repre-sentation.
This semantic parser extracts in-stances of binary relations with state-of-the-art accuracy, while simultaneously recoveringmuch richer semantic structures, such as con-junctions of multiple relations with partiallyshared arguments.
We demonstrate recoveryof this richer structure by extracting logicalforms from natural language queries againstFreebase.
On this task, the trained semanticparser achieves 80% precision and 56% recall,despite never having seen an annotated logicalform.1 IntroductionSemantic parsing converts natural language state-ments into logical forms in a meaning repre-sentation language.
For example, the phrase?town in California?
might be represented as?x.CITY(x) ?
LOCATEDIN(x,CALIFORNIA), whereCITY, LOCATEDIN and CALIFORNIA are predicatesand entities from a knowledge base.
The expressiv-ity and utility of semantic parsing is derived fromthis meaning representation, which is essentially aprogram that is directly executable by a computer.In this sense, broad coverage semantic parsing is thegoal of natural language understanding.Unfortunately, due to data annotation constraints,modern semantic parsers only operate in narrow do-mains.
The best performing semantic parsers aretrained using extensive manual annotation: typi-cally, a number of sentences must be annotated withtheir desired logical form.
Although other forms ofsupervision exist (Clarke et al2010; Liang et al2011), these methods similarly require annotationsfor individual sentences.
More automated trainingmethods are required to produce semantic parserswith richer meaning representations.This paper presents an algorithm for training a se-mantic parser without per-sentence annotations.
In-stead, our approach exploits two easily-obtainablesources of supervision: a large knowledge base and(automatically) dependency-parsed sentences.
Thesemantic parser is trained to identify relation in-stances from the knowledge base while simulta-neously producing parses that syntactically agreewith the dependency parses.
Combining these twosources of supervision allows us to train an accuratesemantic parser for any knowledge base without an-notated training data.We demonstrate our approach by training a Com-binatory Categorial Grammar (CCG) (Steedman,1996) that parses sentences into logical forms con-taining any of 77 relations from Freebase.
Ourtraining data consists of relation instances fromFreebase and automatically dependency-parsed sen-tences from a web corpus.
The trained semanticparser extracts binary relations with state-of-the-artperformance, while recovering considerably richersemantic structure.
We demonstrate recovery of thissemantic structure using natural language queries754townN : ?x.CITY(x)Lexin(N\N)/N : ?f.?g.?x.
?y.f(y) ?
g(x) ?
LOCATEDIN(x, y)Lex CaliforniaN : ?x.x = CALIFORNIALexN\N : ?g.?x.
?y.y = CALIFORNIA ?
g(x) ?
LOCATEDIN(x, y)>N : ?x.
?y.y = CALIFORNIA ?
CITY(x) ?
LOCATEDIN(x, y)<Figure 1: An example parse of ?town in California?
using the example CCG lexicon.
The first stage in parsingretrieves a category from each word from the lexicon, represented by the ?Lex?
entries.
The second stage applies CCGcombination rules, in this case both forms of function application, to combine these categories into a semantic parse.against Freebase.
Our weakly-supervised semanticparser predicts the correct logical form for 56% ofqueries, despite never seeing a labeled logical form.This paper is structured as follows.
We first pro-vide some background information on CCG and thestructure of a knowledge base in Section 2.
Section3 formulates the weakly supervised training prob-lem for semantic parsers and presents our algorithm.Section 4 describes how we applied our algorithm toconstruct a semantic parser for Freebase, and Sec-tion 5 presents our results.
We conclude with relatedwork and discussion.2 Background2.1 Combinatory Categorial GrammarCombinatory Categorial grammar (CCG) is a lin-guistic formalism that represents both the syntax andsemantics of language (Steedman, 1996).
CCG is alexicalized formalism that encodes all grammaticalinformation in a lexicon ?.
This lexicon containssyntactic and semantic categories for each word.
Alexicon may include entries such as:town := N : ?x.CITY(x)California := N : ?x.x = CALIFORNIAin := (N\N)/N : ?f.?g.?x.
?y.f(y) ?
g(x) ?
LOCATEDIN(x, y)Each entry of the lexicon w := s : l maps a word orshort phrase w to a syntactic category s and a logicalform l. Syntactic categories s may be atomic (N ) orcomplex (N\N ).
Logical forms l are lambda calcu-lus expressions constructed using predicates from aknowledge base.
These logical forms combine dur-ing parsing to form a complete logical form for theparsed text.Parses are constructed by combining adjacent cat-egories using several combination rules, such as for-ward (>) and backward (<) application:X/Y : f Y : g =?
X : f(g) (>)Y : g X\Y : f =?
X : f(g) (<)These rules mean that the complex categoryX/Y(X\Y ) behaves like a function which accepts an ar-gument of type Y on its right (left) and returns avalue of type X .
Parsing amounts to sequentiallyapplying these two rules, as shown in Figure 1.
Theresult of parsing is an ordered pair, containing botha syntactic parse tree and an associated logical form.We refer to such an ordered pair as a semantic parse,or by using the letter `.Given a lexicon, there may be multiple seman-tic parses ` for a given phrase w. Like context-freegrammars (CFGs), CCGs can be extended to repre-sent a probability distribution over parses P (`|w; ?
)where ?
is a parameter vector.2.2 Knowledge BaseThe main input to our system is a propositionalknowledge base K = (E,R,C,?
), containingentities E, categories C, relations R and relationinstances ?.
Categories and relations are pred-icates which operate on entities and return truthvalues; categories c ?
C are one-place predi-cates (CITY(e)) and relations r ?
R are two-place predicates (LOCATEDIN(e1, e2)).
Entities e ?E represent real-world entities and have a set ofknown text names.
For example, CALIFORNIAis an entity whose text names include ?Califor-nia?
and ?CA.?
Relation instances r(e1, e2) ?
?are facts asserted by the knowledge base, suchas LOCATEDIN(SACRAMENTO,CALIFORNIA).
Ex-amples of such knowledge bases include Freebase(Bollacker et al2008), NELL (Carlson et al2010), and YAGO (Suchanek et al2007).The knowledge base influences the semanticparser in two ways.
First, CCG logical forms areconstructed by combining categories, relations andentities from the knowledge base with logical con-nectives; hence, the predicates in the knowledgebase determine the expressivity of the parser?s se-mantic representation.
Second, the known relation755instances r(e1, e2) ?
?
are used as weak supervi-sion to train the semantic parser.3 Weakly Supervised Semantic ParsingWe define weakly supervised semantic parsing asthe following learning problem.Input:1.
A knowledge base K = (E,R,C,?
), as de-fined above.2.
A corpus of dependency-parsed sentences S.3.
A CCG lexicon ?
that produces logical formscontaining predicates from K. Section 4.1 de-scribes an approach to generate this lexicon.4.
A procedure for identifying mentions of enti-ties from K in sentences from S.
(e.g., simplestring matching).Output:1.
Parameters ?
for the CCG that produce correctsemantic parses ` for sentences s ?
S.This problem is ill-posed without additional as-sumptions: since the correct logical form for a sen-tence is never observed, there is no a priori reasonto prefer one semantic parse to another.
Our train-ing algorithm makes two assumptions about correctsemantic parses, which are encoded as weak super-vision constraints.
These constraints make learningpossible by adding an inductive bias:1.
Every relation instance r(e1, e2) ?
?
is ex-pressed by at least one sentence in S (Riedelet al2010; Hoffmann et al2011).2.
The correct semantic parse of a sentence s con-tains a subset of the syntactic dependenciescontained in a dependency parse of s.Our weakly supervised training uses these con-straints as a proxy for labeled semantic parses.
Thetraining algorithm has two steps.
First, the algo-rithm constructs a graphical model that containsboth the semantic parser and constant factors en-coding the above two constraints.
This graphicalmodel is then used to estimate parameters ?
for thesemantic parser, essentially optimizing ?
to produceparses that satisfy the weak supervision constraints.If our assumptions are correct and sufficiently con-strain the parameter space, then this procedure willidentify parameters for an accurate semantic parser.3.1 Encoding the Weak SupervisionConstraintsThe first step of training constructs a graphicalmodel containing the semantic parser and two weaksupervision constraints.
However, the first weak su-pervision constraint couples the semantic parses forevery sentence s ?
S. Such coupling would result inan undesirably large graphical model.
We thereforemodify this constraint to enforce that every relationr(e1, e2) is expressed at least once in S(e1,e2) ?
S,the subset of sentences which mention both e1 ande2.
These mentions are detected using the providedmention-identification procedure.Figure 2 depicts the graphical model constructedfor training.
The semantic constraint couples the ex-tractions for all sentences S(e1,e2), so the graphicalmodel is instantiated once per (e1, e2) tuple.
Themodel has 4 types of random variables and values:Si = si represents a sentence, Li = `i representsa semantic parse, Zi = zi represents the satisfac-tion of the syntactic constraint and Yr = yr repre-sents the truth value of relation r. Si, Li and Zi arereplicated once for each sentence s ?
S(e1,e2), whileYr is replicated once for each relation type r in theknowledge base (all r ?
R).For each entity pair (e1, e2), this graphical modeldefines a conditional distribution over L,Y,Z givenS.
This distribution factorizes as:p(Y = y,Z = z,L = `|S = s; ?)
=1Zs?r?
(yr, `)?i?
(zi, `i, si)?
(si, `i; ?
)The factorization contains three replicated fac-tors.
?
represents the semantic parser, which isparametrized by ?
and produces a semantic parse`i for each sentence si.
?
and ?
are deterministicfactors representing the two weak supervision con-straints.
We now describe each factor in more detail.Semantic ParserThe factor ?
represents the semantic parser, whichis a log-linear probabilistic CCG using the input lex-icon ?.
Given a sentence s and parameters ?, theparser defines an unnormalized probability distribu-tion over semantic parses `, each of which includesboth a syntactic CCG parse tree and logical form.756YlocatedIn?Yacquired?YcapitalOf?L1S1?Z1?L2S2?Z2?Figure 2: Factor graph containing the semantic parser?
and weak supervision constraints ?
and ?, instanti-ated for an (e1, e2) tuple occurring in 2 sentences S1 andS2, with corresponding semantic parses L1 and L2.
Theknowledge base contains 3 relations, represented by theY variables.Let f(`, s) represent a feature function mapping se-mantic parses to vectors of feature values1.
The fac-tor ?
is then defined as:?
(s, `; ?)
= exp{?T f(`, s)}If the features f(`, s) factorize according to thestructure of the CCG parse tree, it is possible toperform exact inference using a CKY-style dynamicprogramming algorithm.
However, other aspects ofthe graphical model preclude exact inference, so weperform approximate inference using beam search.Inference is explained in more detail in Section 3.2.Semantic ConstraintThe semantic constraint states that, given an entitytuple (e1, e2), every relation instance r(e1, e2) ?
?must be expressed somewhere in S(e1,e2).
Further-more, no semantic parse can express a relation in-stance which is not in the knowledge base.
This con-straint is identical to the multiple deterministic-ORconstraint used by Hoffmann et al2011) to train asentential relation extractor.The graphical model contains a semantic con-straint factor ?
and one binary variable Yr for eachrelation r in the knowledge base.
Yr representswhether r(e1, e2) is expressed by any sentence inS(e1,e2).
The ?
factor determines whether each se-mantic parse in ` extracts a relation between e1 ande2.
It then aggregates these sentence-level extrac-tions using a deterministic OR: if any sentence ex-tracts r(e1, e2) then Yr = 1.
Otherwise, Yr = 0.1Section 4.3 describes the features used by our semanticparser for Freebase.?
(Yr, `) =1 if Yr = 1 ?
?i.EXTRACTS(`i, r, e1, e2)1 if Yr = 0 ?
6 ?i.EXTRACTS(`i, r, e1, e2)0 otherwiseThe EXTRACTS function determines the relationinstances that are asserted by a semantic parse `.EXTRACTS(`, r, e1, e2) is true if ` asserts the rela-tion r(e1, e2) and false otherwise.
This function es-sentially converts the semantic parser into a senten-tial relation extractor, and its implementation maydepend on the types of logical connectives includedin the lexicon ?.
Logical forms in our Freebase se-mantic parser consist of conjunctions of predicatesfrom the knowledge base; we therefore define EX-TRACTS(`, r, e1, e2) as true if `?s logical form con-tains the clauses r(x, y), x = e1 and y = e2.Syntactic ConstraintA problem with the semantic constraint is that itadmits a large number of ungrammatical parses.
Thesyntactic constraint penalizes ungrammatical parsesby encouraging the semantic parser to produce parsetrees that agree with a dependency parse of the samesentence.
Specifically, the syntactic constraint re-quires the predicate-argument structure of the CCGparse to agree with the predicate-argument structureof the dependency parse.Agreement is defined as a function of each CCGrule application in `.
In the parse tree `, each ruleapplication combines two subtrees, `h and `c, into asingle tree spanning a larger portion of the sentence.A rule application is consistent with a dependencyparse t if the head words of `h and `c have a depen-dency edge between them in t. AGREE(`, t) is trueif and only if every rule application in ` is consistentwith t. This syntactic constraint is encoded in thegraphical model by the ?
factors and Z variables:?
(z, `, s) = 1 if z = AGREE(`,DEPPARSE(s))0 otherwise3.2 Parameter EstimationTo train the model, a single training example is con-structed for every tuple of entities (e1, e2).
The in-put to the model is s = S(e1,e2), the set of sentences757containing e1 and e2.
The weak supervision vari-ables, y, z, are the output of the model.
y is con-structed by setting yr = 1 if r(e1, e2) ?
?, and 0otherwise.
This setting trains the semantic parser toextract every true relation instance between (e1, e2)from some sentence in S(e1,e2), while simultane-ously avoiding incorrect instances.
Finally, z = 1,to encourage agreement between the semantic anddependency parses.
The training data for the modelis therefore a collection, {(sj , yj , zj)}nj=1, where jindexes entity tuples (e1, e2).Training optimizes the semantic parser parame-ters ?
to predict Y = yj ,Z = zj given S = sj .
Theparameters ?
are estimated by running the structuredperceptron algorithm (Collins, 2002) on the trainingdata defined above.
The structured perceptron al-gorithm iteratively applies a simple update rule foreach example (sj , yj , zj) in the training data:`predicted ?
arg max`maxy,zp(`, y, z|sj ; ?t)`actual ?
arg max`p(`|yj , zj , sj ; ?t)?t+1 ?
?t +?if(`actuali , si)?
?if(`predictedi , si)Each iteration of training requires solving twomaximization problems.
The first maximization,max`,y,z p(`, y, z|s; ?t), is straightforward because yand z are deterministic functions of `.
Therefore,it is solved by finding the maximum probability as-signment `, then choosing values for y and z thatsatisfy the weak supervision constraints.The second maximization, max` p(`|y, z, s; ?t), ismore challenging.
When y and z are given, the infer-ence procedure must restrict its search to the parses` which satisfy these weak supervision constraints.The original formulation of the ?
factors permittedtractable inference (Hoffmann et al2011), but theEXTRACTS function and the ?
factors preclude ef-ficient inference.
We approximate this maximiza-tion using beam search over CCG parses `.
For eachsentence s, we perform a beam search to producek = 300 possible semantic parses.
We then checkthe value of ?
for each generated parse and elimi-nate parses which do not satisfy this syntactic con-straint.
Finally, we apply EXTRACTS to each parse,then use the greedy approximate inference proce-dure from Hoffmann et al2011) for the ?
factors.4 Building a Grammar for FreebaseWe apply the training algorithm from the previoussection to produce a semantic parser for a subset ofFreebase.
This section describes details of the gram-mar we construct for this task, including the con-struction of the lexicon ?, some extensions to theCCG parser, and the features used during training.In this section, we assume access to a knowledgebase K = (E,C,R,?
), a corpus of dependency-parsed sentences S and a procedure for identifyingmentions of entities in sentences.4.1 Constructing the Lexicon ?The first step in constructing the semantic parseris defining a lexicon ?.
We construct ?
by ap-plying simple dependency-parse-based heuristics tosentences in the training corpus.
The resulting lex-icon ?
captures a variety of linguistic phenomena,including verbs, common nouns (?city?
), noun com-pounds (?California city?)
and prepositional modi-fiers (?city in California?
).The first step in lexicon construction is to use themention identification procedure to identify all men-tions of entities in the sentences S. This processresults in (e1, e2, s) triples, consisting of sentenceswith two entity mentions.
The dependency path be-tween e1 and e2 in s is then matched against the de-pendency parse patterns in Table 1.
Each matchedpattern adds one or more lexical entries to ?Each pattern in Table 1 has a corresponding lexi-cal category template, which is a CCG lexical cate-gory containing parameters e, c and r that are chosenat initialization time.
Given the triple (e1, e2, s), re-lations r are chosen such that r(e1, e2) ?
?, andcategories c are chosen such that c(e1) ?
?
orc(e2) ?
?.
The template is then instantiated withevery combination of these e, c and r values.After instantiating lexical categories for each sen-tence in S, we prune infrequent lexical categories toimprove parser efficiency.
This pruning step is re-quired because the common noun pattern generatesa large number of lexical categories, the majorityof which are incorrect.
Therefore, we eliminate allcommon noun categories instantiated by fewer than758Part ofDependency Parse Pattern Lexical Category TemplateSpeechProper (name of entity e) w :=N : ?x.x = eNoun Sacramento Sacramento :=N : ?x.x = SACRAMENTOCommon e1SBJ===?
[is, are, was, ...]OBJ?=== w w :=N : ?x.c(x)Noun Sacramento is the capital capital :=N : ?x.CITY(x)Noun e1NMOD?===== e2 Type change N : ?x.c(x) to N |N : ?f.?x.
?y.c(x) ?
f(y) ?
r(x, y)Modifier Sacramento, California N : ?x.CITY(x) to N |N : ?f.?x.
?y.CITY(x) ?
f(y) ?
LOCATEDIN(x, y)Prepositione1NMOD?===== wPMOD?===== e2 w := (N\N)/N : ?f.?g.?x.
?y.f(y) ?
g(x) ?
r(x, y)Sacramento in California in := (N\N)/N : ?f.?g.?x.
?y.f(y) ?
g(x) ?
LOCATEDIN(x, y)e1SBJ===?
VB*ADV?=== wPMOD?===== e2 w := PP/N : ?f.
?x.f(x)Sacramento is located in California in := PP/N : ?f.?x.f(x)Verbe1SBJ===?
w*OBJ?=== e2 w* := (S\N)/N : ?f.?g.
?x, y.f(y) ?
g(x) ?
r(x, y)Sacramento governs California governs := (S\N)/N : ?f.?g.
?x, y.f(y) ?
g(x) ?
LOCATEDIN(x, y)e1SBJ===?
w*ADV?=== [IN,TO]PMOD?===== e2 w* := (S\N)/PP : ?f.?g.
?x, y.f(y) ?
g(x) ?
r(x, y)Sacramento is located in California is located := (S\N)/PP : ?f.?g.
?x, y.f(y) ?
g(x) ?
LOCATEDIN(x, y)e1NMOD?===== w*ADV?=== [IN,TO]PMOD?===== e2 w* := (N\N)/PP : ?f.?g.
?y.f(y) ?
g(x) ?
r(x, y)Sacramento located in California located := (N\N)/PP : ?f.?g.
?y.f(y) ?
g(x) ?
LOCATEDIN(x, y)Forms of(none) w* := (S\N)/N : ?f.?g.
?x.g(x) ?
f(x)?to be?Table 1: Dependency parse patterns used to instantiate lexical categories for the semantic parser lexicon ?.
Eachpattern is followed by an example phrase that instantiates it.
An * indicates a position that may be filled by multipleconsecutive words in the sentence.
e1 and e2 are the entities identified in the sentence, r represents a relation wherer(e1, e2), and c represents a category where c(e1).
Each template may be instantiated with multiple values for thevariables e, c, r.5 sentences in S. The other rules are less fertile, sowe do not need to prune their output.In addition to these categories, the grammar in-cludes type-changing rules from N to N |N .
Theserules capture noun compounds by allowing nouns tobecome functions from nouns to nouns.
There areseveral such type-changing rules since the resultingcategory includes a hidden relation r between thenoun and its modifier (see Table 1).
As with lexicalcategories, the set of type changing rules includedin the grammar is determined by matching depen-dency parse patterns to the training data.
Similarrules for noun compounds are used in other CCGparsers (Clark and Curran, 2007).The instantiated lexicon represents the semanticsof words and phrases as conjunctions of predicatesfrom the knowledge base, possibly including exis-tentially quantified variables and ?
expressions.
Thesyntactic types N and PP are semantically rep-resented as functions from entities to truth values(e.g., ?x.CITY(x)), while sentences S are statementswith no ?
terms, such as ?x, y.x = CALIFORNIA ?CITY(y) ?
LOCATEDIN(x, y).
Variables in the seman-tic representation (x, y) range over entities from theknowledge base.
Intuitively, the N and PP cate-gories represent sets of entities, while sentences rep-resent assertions about the world.4.2 Extensions to CCGThe semantic parser is trained using sentences froma web corpus, which contains many out-of-domainwords.
As a consequence, many of the words en-countered during training cannot be represented us-ing the vocabulary of predicates from the knowl-edge base.
To handle these extraneous words, weallow the CCG parser to skip words while parsinga sentence.
During parsing, the parser first decideswhether to retrieve a lexical category for each wordin the sentence.
The sentence is then parsed as ifonly the retrieved lexical categories existed.4.3 FeaturesThe features f(`, s) for our probabilistic CCG con-tain two sets of features.
The first set contains lexi-cal features, which count the number of times eachlexical entry is used in `.
The second set containsrule application features, which count the numberof times each combination rule is applied to eachpossible set of arguments.
An argument is definedby its syntactic and semantic category, and in somecases by the lexical entry which created it.
We lex-759icalize arguments for prepositional phrases PP andcommon nouns (initialized by the second rule in Ta-ble 1).
This lexicalization allows the parser to dis-tinguish between prepositional phrases headed bydifferent prepositions, as well as between differentcommon nouns.
All other types are distinguishedsolely by syntactic and semantic category.5 EvaluationIn this section, we evaluate the performance ofa semantic parser for Freebase, trained using ourweakly-supervised algorithm.
Empirical compari-son is somewhat difficult because the most compara-ble previous work ?
weakly-supervised relation ex-traction ?
uses a shallower semantic representation.Our evaluation therefore has two components: (1) abinary relation extraction task, to demonstrate thatthe trained semantic parser extracts instances of bi-nary relations with performance comparable to otherstate-of-the-art systems, and (2) a natural languagedatabase query task, to demonstrate the parser?s abil-ity to extract more complex logical forms than bi-nary relation instances, such as logical expressionsinvolving conjunctions of multiple categories and re-lations with partially shared arguments.5.1 Corpus ConstructionOur experiments use a subset of 77 relations2 fromFreebase3 as the knowledge base and a corpus ofweb sentences.
We constructed the sentence corpusby first sampling sentences from a web crawl andparsing them with MaltParser (Nivre et al2006).Long sentences tended to have noisy parses whilealso rarely expressing relations, so we discardedsentences longer than 10 words.
Entities were iden-tified by performing a simple string match betweencanonical entity names in Freebase and proper nounphrases identified by the parser.
In cases where asingle noun phrase matched multiple entities, we se-lected the entity participating in the most relations.The resulting corpus contains 2.5 million (e1, e2, s)triples, from which we reserved 10% for validationand 10% for testing.
The validation set was usedto estimate performance during algorithm develop-2These relations are defined by a set of MQL queries andpotentially traverse multiple relation links.3http://www.freebase.comRelation NameRelationSentencesInstancesCITYLOCATEDINSTATE 2951 13422CITYLOCATEDINCOUNTRY 1696 7904CITYOFPERSONBIRTH 397 440COMPANIESHEADQUARTEREDHERE 326 432MUSICARTISTMUSICIAN 251 291CITYUNIVERSITIES 239 338CITYCAPITALOFCOUNTRY 123 2529HASHUSBAND 103 367PARENTOFPERSON 85 356HASSPOUSE 81 461Table 2: Occurrence statistics for the 10 most frequentrelations in the training data.
?Relation Instances?
showsthe number of entity tuples (e1, e2) that appear as positiveexamples for each relation, and ?Sentences?
shows thetotal number of sentences in which these tuples appear.ment, while the test set was used to generate the fi-nal experimental results.
All triples for each (e1, e2)tuple were placed in the same set.Approximately 1% of the resulting (e1, e2, s)triples are positive examples, meaning there existssome relation r where r(e1, e2) ?
?4.
To improvetraining efficiency and prediction performance, wesubsample 5% of the negative examples for training,producing a training set of 125k sentences with 27kpositive examples.
The validation and test sets retainthe original positive/negative ratio.
Table 2 showssome statistics of the most frequent relations in thetest set.5.2 Relation ExtractionThe first experiment measures the semantic parser?sability to extract relations from sentences in our webcorpus.
We compare our semantic parser to MUL-TIR (Hoffmann et al2011), which is a state-of-the-art weakly supervised relation extractor.
Thismethod uses the same weak supervision constraintand parameter estimation procedure, but replaces thesemantic parser by a linear classifier.
The featuresfor this classifier include the dependency path be-tween the entity mentions, the type of each mention,and the intervening context (Mintz et al2009).Both the semantic parser and MULTIR weretrained by running 5 iterations of the structured per-4Note that the positive/negative ratio was much lower with-out the length filter or entity disambiguation, which is partlywhy filtering was performed.760MULTIRPARSE+DEPPARSEPARSE-DEP0 0.2 0.4 0.6 0.8 1.000.20.40.60.81.0Figure 3: Aggregate precision as a function of recall, forMULTIR (Hoffman et al2011) and our three semanticparser variants.MULTIRPARSE+DEPPARSEPARSE-DEP0 600 1200 1800 2400 300000.20.40.60.81.0Figure 4: Sentential precision as a function of the ex-pected number of correct extractions for MULTIR (Hoff-man et al2011) and our three semantic parser variants.ceptron algorithm5.
At test time, both models pre-dicted a relation r ?
R or NONE for each (e1, e2, s)triple in the test set.
The parser parses the sen-tence without considering the entities marked in thesentence, then applies the EXTRACTS function de-fined in Section 3.1 to identify a relation between e1and e2.
We compare three versions of the semanticparser: PARSE, which is the basic semantic parser,PARSE+DEP which additionally observes the cor-rect dependency parse at test time, and PARSE-DEPwhich is trained without the syntactic constraint.Note that MULTIR uses the sentence?s dependencyparse to construct its feature vector.Our evaluation considers two performance mea-sures: aggregate and sentential precision/recall.
Ag-gregate precision takes the union of all extracted re-lation instances r(e1, e2) from the test corpus andcompares these instances to Freebase.
To pro-5The structured perceptron algorithm does not converge to aparameter estimate, and we empirically found that performancedid not improve beyond 5 iterations.MULTIRPARSE+DEPPARSE0 0.2 0.4 0.6 0.8 1.000.20.40.60.81.0Figure 5: Aggregate precision as a function of recall,ignoring the two most frequent relations, CITYLOCATE-DINSTATE and CITYLOCATEDINCOUNTRY.duce a precision/recall curve, each extracted in-stance r(e1, e2) is assigned the maximum score overall sentences which extracted it.
This metric is easyto compute, but may be inaccurate due to inaccura-cies and missing relations in Freebase.Sentential precision computes the precision of ex-tractions on individual (e1, e2, s) tuples.
This met-ric is evaluated by manually sampling and evaluat-ing 100 test sentences from which a relation was ex-tracted per model.
Unfortunately, it is difficult tocompute recall for this metric, since the true numberof sentences expressing relations is unknown.
Weinstead report precision as a function of the expectednumber of correct extractions, which is directly pro-portional to recall.Figure 3 displays aggregate precision/recall andFigure 4 displays sentential precision/recall for all4 models.
Generally, PARSE behaves like MUL-TIR with somewhat lower recall.
In the sententialevaluation, PARSE+DEP outperforms both PARSEand MULTIR.
The difference between PARSE+DEP?saggregate and sentential precision stems from thefact that PARSE+DEP extracts each relation instancefrom more sentences than either MULTIR or PARSE.PARSE-DEP has the worst performance in both eval-uations, suggesting the importance of syntactic su-pervision.
Precision in the aggregate experiment islow partially due to examples with incorrect entitydisambiguation.We found that the skewed distribution of relationtypes hides interesting differences between the mod-els.
Therefore, we include Figure 5 comparing oursyntactically-supervised parsers to MULTIR, ignor-ing the two most frequent relations (which together761make up over half of all relation instances).
BothPARSE and PARSE+DEP are considerably more pre-cise than MULTIR on these less frequent relationsbecause their compositional meaning representationshares parameter strength between relations.
Forexample, the semantic parsers learn that ?in?
oftencombines with a city to form a prepositional phrase;the parsers can apply this knowledge to identify cityarguments of any relation.
However, MULTIR is ca-pable of higher recall, since its dependency parsefeatures can represent syntactic dependencies thatcannot be represented by our semantic parsers.
Thislimitation is a consequence of our heuristic lexiconinitialization procedure, and could be rectified by amore flexible initialization procedure.5.3 Natural Language Database QueriesThe second experiment measures our trainedparser?s ability to correctly translate natural lan-guage queries into logical queries against Freebase.To avoid biasing the evaluation, we constructeda test corpus of natural language queries in a data-driven fashion.
We searched the test data for sen-tences with two related entities separated by an ?isa?
expression.
The portion of the sentence before the?is a?
expression was discarded and the remainderretained as a candidate query.
For example ?Jesse isan author from Austin, Texas,?
was converted intothe candidate query ?author from Austin, Texas.
?Each candidate query was then annotated with a log-ical form using categories and relations from theknowledge base; candidate queries without satisfac-tory logical forms were discarded.
We annotated 50validation and 50 test queries in this fashion.
Thevalidation set was used to estimate performance dur-ing algorithm development and the test set was usedto generate the final results.
Example queries withtheir annotated logical forms are shown in Table 3.Table 4 displays the results of the query evalua-tion.
For this evaluation, we forced the parser to in-clude every word of the query in the parse.
Precisionis the percentage of successfully parsed queries forwhich the correct logical form was predicted.
Re-call is the percentage of all queries for which thecorrect logical form was predicted.
This evalua-tion demonstrates that the semantic parser success-fully interprets common nouns and identifies mul-tiple relations with shared arguments.
The perfor-Example Query Logical Formcapital of Russia ?x.CITYCAPITALOFCOUNTRY(x, RUSSIA)wife of Abraham ?x.HASHUSBAND(x,ABRAHAM)vocalist from ?x.MUSICIAN(x)?London, England PERSONBORNIN(x, LONDON)?CITYINCOUNTRY(LONDON, ENGLAND)home of ?x.HEADQUARTERS(CONOCOPHILLIPS, x)ConocoPhillips ?CITYINCOUNTRY(x, CANADA)in CanadaTable 3: Example natural language queries and their cor-rect annotated logical form.Precision RecallPARSE 0.80 0.56PARSE-DEP 0.45 0.32Table 4: Precision and recall for predicting logical formsof natural language queries against Freebase.
The tablecompares PARSE, trained with syntactic supervision toPARSE-DEP, trained without syntactic supervision.mance difference between PARSE and PARSE-DEPalso demonstrates the benefit of including syntacticsupervision.Examining the system output, we find two ma-jor sources of error.
The first is missing lexical cat-egories for uncommon words (e.g., ?ex-guitarist?
),which negatively impact recall by making somequeries unparsable.
The second is difficulty distin-guishing between relations with similar type signa-tures, such as CITYLOCATEDINCOUNTRY and CITY-CAPITALOFCOUNTRY.6 Related WorkThere are many approaches to supervised seman-tic parsing, including inductive logic programming(Zelle and Mooney, 1996), probabilistic and syn-chronous grammars (Ge and Mooney, 2005; Wongand Mooney, 2006; Wong and Mooney, 2007; Lu etal., 2008), and automatically learned transformationrules (Kate et al2005).
This work most closelyfollows the work on semantic parsing using CCG(Zettlemoyer and Collins, 2005; Zettlemoyer andCollins, 2007; Kwiatkowski et al2010).
These su-pervised systems are all trained with annotated sen-tence/logical form pairs; hence these approaches arelabor intensive and do not scale to broad domainswith large numbers of predicates.Several recent papers have attempted to reducethe amount of human supervision required to train762a semantic parser.
One line of work eliminates theneed for an annotated logical form, instead usingonly the correct answer for a database query (Lianget al2011) or even a binary correct/incorrect sig-nal (Clarke et al2010).
This type of feedback maybe easier to obtain than full logical forms, but stillrequires individually annotated sentences.
Other ap-proaches are completely unsupervised, but do not tiethe language to an existing meaning representation(Poon and Domingos, 2009).
It is also possible toself-train a semantic parser without any labeled data(Goldwasser et al2011).
However, this approachdoes not perform as well as more supervised ap-proaches, since the parser?s self-training predictionsare not constrained by the correct logical form.Recent research has produced several weakly su-pervised relation extractors (Craven and Kumlien,1999; Mintz et al2009; Wu and Weld, 2010; Riedelet al2010; Hoffmann et al2011).
These sys-tems scale up to hundreds of predicates, but havemuch shallower semantic representations than se-mantic parsers.
For example, these systems can-not be directly used to respond to natural languagequeries.
This work extends weakly supervised rela-tion extraction to produce richer semantic structure,using only slightly more supervision in the form ofdependency parses.7 DiscussionThis paper presents a method for training a seman-tic parser using only a knowledge base and a cor-pus of unlabeled sentences.
Our key observation isthat multiple forms of weak supervision can be com-bined to train an accurate semantic parser: semanticsupervision from a knowledge base of facts, and syn-tactic supervision in the form of a standard depen-dency parser.
We presented an algorithm for train-ing a semantic parser in the form of a probabilisticCombinatory Categorial Grammar, using these twotypes of weak supervision.
We used this algorithmto train a semantic parser for an ontology of 77 Free-base predicates, using Freebase itself as the weak se-mantic supervision.Experimental results show that our trained se-mantic parser extracts binary relations as well asa state-of-the-art weakly supervised relation extrac-tor (Hoffmann et al2011).
Further experimentstested our trained parser?s ability to extract morecomplex meanings from sentences, including logi-cal forms involving conjunctions of multiple relationand category predicates with shared arguments (e.g.,?x.MUSICIAN(x) ?
PERSONBORNIN(x, LONDON) ?CITYINCOUNTRY(LONDON, ENGLAND)).
To test thiscapability, we applied the trained parser to naturallanguage queries against Freebase.
The semanticparser correctly interpreted 56% of these queries,despite the broad domain and never having seen anannotated logical form.
Together, these two experi-mental analyses suggest that the combination of syn-tactic and semantic weak supervision is indeed a suf-ficient basis for training semantic parsers for a di-verse range of corpora and predicate ontologies.One limitation of our method is the reliance onhand-built dependency parse patterns for lexicon ini-tialization.
Although these patterns capture a va-riety of linguistic phenomena, they require manualengineering and may miss important relations.
Anarea for future work is developing an automatedway to produce this lexicon, perhaps by extend-ing the recent work on automatic lexicon generation(Kwiatkowski et al2010) to the weakly supervisedsetting.
Such an algorithm seems especially impor-tant if one wishes to model phenomena such as ad-jectives, which are difficult to initialize heuristicallywithout generating large numbers of lexical entries.An elegant aspect of semantic parsing is that it iseasily extensible to include more complex linguis-tic phenomena, such as quantification and events(multi-argument relations).
In the future, we planto increase the expressivity of our parser?s mean-ing representation to capture more linguistic and se-mantic phenomena.
In this fashion, we can makeprogress toward broad coverage semantic parsing,and thus natural language understanding.AcknowledgmentsThis research has been supported in part by DARPAunder contract number FA8750-09-C-0179, and by agrant from Google.
Additionally, we thank Yahoo!for use of their M45 cluster.
We also gratefully ac-knowledge the contributions of our colleagues on theNELL project, Justin Betteridge for collecting theFreebase relations, Jamie Callan and colleagues forthe web crawl, and Thomas Kollar and Matt Gardnerfor helpful comments on earlier drafts of this paper.763ReferencesKurt Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: a col-laboratively created graph database for structuring hu-man knowledge.
In Proceedings of the 2008 ACMSIGMOD International Conference on Management ofData, pages 1247?1250.Andrew Carlson, Justin Betteridge, Bryan Kisiel, BurrSettles, Estevam R. Hruschka Jr., and Tom M.Mitchell.
2010.
Toward an architecture for never-ending language learning.
In Proceedings of theTwenty-Fourth AAAI Conference on Artificial Intelli-gence.Stephen Clark and James R. Curran.
2007.
Wide-coverage efficient statistical parsing with CCG andlog-linear models.
Computational Linguistics,33(4):493?552.James Clarke, Dan Goldwasser, Ming-Wei Chang, andDan Roth.
2010.
Driving semantic parsing fromthe world?s response.
In Proceedings of the Four-teenth Conference on Computational Natural Lan-guage Learning.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: theory and experi-ments with perceptron algorithms.
In Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing.Mark Craven and Johan Kumlien.
1999.
Constructingbiological knowledge bases by extracting informationfrom text sources.
In Proceedings of the Seventh Inter-national Conference on Intelligent Systems for Molec-ular Biology.Ruifang Ge and Raymond J. Mooney.
2005.
A statisticalsemantic parser that integrates syntax and semantics.In Proceedings of the Ninth Conference on Computa-tional Natural Language Learning.Dan Goldwasser, Roi Reichart, James Clarke, and DanRoth.
2011.
Confidence driven unsupervised semanticparsing.
Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics.Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke S.Zettlemoyer, and Daniel S. Weld.
2011.
Knowledge-based weak supervision for information extraction ofoverlapping relations.
In The 49th Annual Meetingof the Association for Computational Linguistics: Hu-man Language Technologies.Rohit J. Kate, Yuk Wah Wong, and Raymond J. Mooney.2005.
Learning to transform natural to formal lan-guages.
In Proceedings, The Twentieth National Con-ference on Artificial Intelligence and the SeventeenthInnovative Applications of Artificial Intelligence Con-ference.Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-ter, and Mark Steedman.
2010.
Inducing probabilisticCCG grammars from logical form with higher-orderunification.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Processing.Percy Liang, Michael I. Jordan, and Dan Klein.
2011.Learning dependency-based compositional semantics.In Proceedings of the Association for ComputationalLinguistics, Portland, Oregon.
Association for Com-putational Linguistics.Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S. Zettle-moyer.
2008.
A generative model for parsing naturallanguage to meaning representations.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing.Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.2009.
Distant supervision for relation extraction with-out labeled data.
In Proceedings of the Joint Confer-ence of the 47th Annual Meeting of the ACL and the 4thInternational Joint Conference on Natural LanguageProcessing of the AFNLP.Joakim Nivre, Johan Hall, and Jens Nilsson.
2006.
Malt-parser: A data-driven parser-generator for dependencyparsing.
In Proceedings of the 21st International Con-ference on Computational Linguistics and 44th AnnualMeeting of the Association for Computational Linguis-tics.Hoifung Poon and Pedro Domingos.
2009.
Unsuper-vised semantic parsing.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing.Sebastian Riedel, Limin Yao, and Andrew McCallum.2010.
Modeling relations and their mentions with-out labeled text.
In Proceedings of the 2010 Europeanconference on Machine learning and Knowledge Dis-covery in Databases.Mark Steedman.
1996.
Surface Structure and Interpre-tation.
The MIT Press.Fabian M. Suchanek, Gjergji Kasneci, and GerhardWeikum.
2007.
Yago: a core of semantic knowl-edge.
In Proceedings of the 16th international confer-ence on World Wide Web, WWW ?07, pages 697?706,New York, NY, USA.
ACM.Yuk Wah Wong and Raymond J. Mooney.
2006.
Learn-ing for semantic parsing with statistical machine trans-lation.
In Proceedings of the Human Language Tech-nology Conference of the NAACL.Yuk Wah Wong and Raymond J. Mooney.
2007.
Learn-ing synchronous grammars for semantic parsing withlambda calculus.
In Proceedings of the 45th AnnualMeeting of the Association for Computational Linguis-tics.Fei Wu and Daniel S. Weld.
2010.
Open informationextraction using Wikipedia.
In Proceedings of the 48th764Annual Meeting of the Association for ComputationalLinguistics.John M. Zelle and Raymond J. Mooney.
1996.
Learningto parse database queries using inductive logic pro-gramming.
In Proceedings of the thirteenth nationalconference on Artificial Intelligence.Luke S. Zettlemoyer and Michael Collins.
2005.
Learn-ing to map sentences to logical form: structured clas-sification with probabilistic categorial grammars.
InUAI ?05, Proceedings of the 21st Conference in Un-certainty in Artificial Intelligence.Luke S. Zettlemoyer and Michael Collins.
2007.
Onlinelearning of relaxed ccg grammars for parsing to logicalform.
In Proceedings of the 2007 Joint Conference onEmpirical Methods in Natural Language Processingand Computational Natural Language Learning.765
