Machine Translation with a Stochastic Grammatical  ChannelDekai Wu and Hongsing WONGHKUSTHuman Language Technology CenterDepartment of  Computer ScienceUniversity of  Science and TechnologyClear Water Bay, Hong Kong{dekai,wong}@cs.ust.hkAbstractWe introduce a stochastic grammatical channelmodel for machine translation, that synthesizes sev-eral desirable characteristics of both statistical andgrammatical machine translation.
As with thepure statistical translation model described by Wu(1996) (in which a bracketing transduction gram-mar models the channel), alternative hypothesescompete probabilistically, exhaustive search of thetranslation hypothesis pace can be performed inpolynomial time, and robustness heuristics arisenaturally from a language-independent inversion-transduction model.
However, unlike pure statisti-cal translation models, the generated output stringis guaranteed to conform to a given target gram-mar.
The model employs only (1) a translationlexicon, (2) a context-free grammar for the targetlanguage, and (3) a bigram language model.
Thefact that no explicit bilingual translation rules areused makes the model easily portable to a variety ofsource languages.
Initial experiments show that italso achieves ignificant speed gains over our ear-lier model.1 MotivationSpeed of statistical machine translation methodshas long been an issue.
A step was taken byWu (Wu, 1996) who introduced a polynomial-timealgorithm for the runtime search for an optimaltranslation.
To achieve this, Wu's method substi-tuted a language-independent stochastic bracketingtransduction grammar (SBTG) in place of the sim-pler word-alignment channel models reviewed inSection 2.
The SBTG channel made exhaustivesearch possible through dynamic programming, in-stead of previous "stack search" heuristics.
Trans-lation accuracy was not compromised, because theSBTG is apparently flexible enough to model word-order variation (between English and Chinese) eventhough it eliminates large portions of the space of1408word alignments.
The SBTG can be regarded asa model of the language-universal hypothesis thatclosely related arguments end to stay together (Wu,1995a; Wu, 1995b).In this paper we introduce a generalization ofWu's method with the objectives of1.
increasing translation speed further,2.
improving meaning-preservation accuracy,3.
improving rammaticality of the output, and4.
seeding a natural transition toward transduc-tion rule models,under the constraint of?
employing no additional knowledge resourcesexcept a grammar for the target language.To achieve these objectives, we:?
replace Wu's SBTG channel with a fullstochastic inversion transduction grammar orSITG channel, discussed in Section 3, and?
(mis-)use the target language grammar as aSITG, discussed in Section 4.In Wu's SBTG method, the burden of generatinggrammatical output rests mostly on the bigram lan-guage model; explicit grammatical knowledge can-not be used.
As a result, output grammaticality can-not be guaranteed.
The advantage is that language-dependent syntactic knowledge resources are notneeded.We relax those constraints here by assuming agood (monolingual) context-free grammar for thetarget language.
Compared to other knowledgeresources (such as transfer ules or semantic on-tologies), monolingual syntactic grammars are rel-atively easy to acquire or construct.
We use thegrammar in the SITG channel, while retaining thebigram language model.
The new model facilitatesexplicit coding of grammatical knowledge and finercontrol over channel probabilities.
Like Wu's SBTGmodel, the translation hypothesis space can be ex-haustively searched in polynomial time, as shown inSection 5.
The experiments discussed in Section 6show promising results for these directions.2 Review: Noisy Channel ModelThe statistical translation model introduced by IBM(Brown et al, 1990) views translation as a noisychannel process.
The underlying enerative modelcontains a stochastic Chinese (input) sentence gen-erator whose output is "corrupted" by the transla-tion channel to produce English (output) sentences.Assume, as we do throughout this paper, that theinput language is English and the task is to trans-late into Chinese.
In the IBM system, the languagemodel employs imple n-grams, while the transla-tion model employs several sets of parameters asdiscussed below.
Estimation of the parameters hasbeen described elsewhere (Brown et al, 1993).Translation is performed in the reverse directionfrom generation, as usual for recognition under gen-erative models.
For each English sentence  to betranslated, the system attempts to find the Chinesesentence c, such that:c* = argmaxPr(cle ) = argmaxPr(ele ) Pr(c) (1)g gIn the IBM model, the search for the optimal c,  isperformed using a best-first heuristic "stack search"similar to A* methods.One of the primary obstacles to making the statis-tical translation approach practical is slow speed oftranslation, as performed in A* fashion.
This priceis paid for the robustness that is obtained by usingvery flexible language and translation models.
Thelanguage model allows sentences of arbitrary or-der and the translation model allows arbitrary word-order permutation.
No structural constraints andexplicit linguistic grammars are imposed by thismodel.The translation channel is characterized by twosets of parameters: translation and alignment prob-abilities, l The translation probabilities describe lex-ical substitution, while alignment probabilities de-scribe word-order permutation.
The key problemis that the formulation of alignment probabilitiesa(ilj , V, T) permits the English word in position jof a length-T sentence to map to any position i of alength-V Chinese sentence.
So V T alignments arepossible, yielding an exponential space with corre-spondingly slow search times.I Various models have been constructed by the IBM team(Brown et al, 1993).
This description corresponds toone of thesimplest ones, "Model 2"; search costs for the more complexmodels are correspondingly higher.3 A SITG Channel ModelThe translation channel we propose is based onthe recently introduced bilingual anguage model-ing approach.
The model employs a stochastic ver-sion of an inversion transduction grammar or ITG(Wu, 1995c; Wu, 1995d; Wu, 1997).
This formal-ism was originally developed for the purpose of par-allel corpus annotation, with applications for brack-eting, alignment, and segmentation.
Subsequently,a method was developed to use a special case of theITGRthe aforementioned BTGRfor the translationtask itself (Wu, 1996).
The next few paragraphsbriefly review the main properties of ITGs, beforewe describe the SITG channel.An ITG consists of context-free productionswhere terminal symbols come in couples, for ex-ample x/y, where x is a English word and y is anChinese translation of x, with singletons of the formx/e or e/y representing function words that are usedin only one of the languages.
Any parse tree thusgenerates both English and Chinese strings imulta-neously.
Thus, the tree:(1) \[I/~-~ \[\[took/$-~ \[a/-- e/:~s: book/:~\]N P \]vP\[for/.~ you/~J~\]pp \]VP Isproduces, for example, the mutual translations:(2) a.
\ [~  \ [ \ [~~ \ [ - - :~\ ]NP  \]VP \[,,~'{~\]PP \]VP \]Sb.
\[I \[\[took \[a book\]Nv \]va \[for you\]pp \]vp \]sAn additional mechanism accommodates a con-servative degree of word-order variation betweenthe two languages.
With each production of thegrammar is associated either a straight orientationor an inverted orientation, respectively denoted asfollows: VP ~ \[VPPP\]VP ~ (VPPP)In the case of a production with straight orien-tation, the right-hand-side symbols are visited left-to-right for both the English and Chinese streams.But for a production with inverted orientation, theright-hand-side symbols are visited left-to-right forEnglish and right-to-left or Chinese.
Thus, the tree:(3) \[I/~ (\[took/~T \[a/-- e/:~ book\]--~\]N P \]VP\[for/,,~ you/~J~\]pp)vp \]Sproduces translations with different word order:(4) a.
\[I \[\[took \[a book\]Np \]vP \[for you\]pp \]vp \]sb.
\ [~ \[\[.~/~\]pp \ [~7 \ [ - -2~\ ]NP  \]VP \]VP \]SThe surprising ability of ITGs to accommodatenearly all word-order variation between fixed-word-order languages 2 (English and Chinese in particu-lar), has been analyzed mathematically, linguisti-2With the exception of higher-order phenomena such asneg-raising and wh-movement.1409cally, and experimentally (Wu, 1995b; Wu, 1997).Any ITG can be transformed to an equivalentbinary-branching ormal form.A stochastic ITG associates a probability witheach production.
It follows that a SITG assignsa probability Pr(e,c,q) to all generable trees qand sentence-pairs.
In principle it can be used asthe translation channel model by normalizing withPr(c) and integrating out Pr(q) to give Pr(clc) inEquation (1).
In practice, a strong language modelmakes this unnecessary, sowe can instead optimizethe simpler Viterbi approximationc, = argmaxPr(e ,c ,q)  Pr(c) (2)cTo complete the picture we add a bigram modelgc~_~c~ = g(cj \] cj-1) for the Chinese languagemodel Pr(c).This approach was used for the SBTG chan-nel (Wu, 1996), using the language-independentbracketing degenerate case of the SITG: 3allA -4 \[AA\]aOA --+ (AA)A b(54Y) x/y VX, y lexical translationsA b(.~?)
.z'/~?
VX language 1vocabularyA b(_~y) e/y Vy language 2 vocabularyIn the proposed model, a structured language-dependent ITG is used instead.4 A Grammatical Channel ModelStated radically, our novel modeling thesis is thata mirrored version of the target language grammarcan parse sentences of the source language.Ideally, an ITG would be tailored for the desiredsource and target languages, enumerating the trans-duction patterns pecific to that language pair.
Con-structing such an ITG, however, requires massivemanual abor effort for each language pair.
Instead,our approach is to take a more readily acquiredmonolingual context-free grammar for the targetlanguage, and use (or perhaps misuse) it in the SITGchannel, by employing the three tactics describedbelow: production mirroring, part-of-speech map-ping, and word skipping.In the following, keep in mind our conventionthat language 1 is the source (English), while lan-guage 2 is the target (Chinese).3Wu (Wu, 1996) experimented with Chinese-English trans-lation, while this paper experiments with English-Chinesetranslation.1410S -4 NPVPPuncVP -4 V NPNP -4 NModNIPmS ~ \[NP VP Punc\] / (Punc VP NP)VP -4 \ [VNP\] I (NPV)NP -4 \[N Mod N\] I (N Mod N) I \[Prn\]Figure 1: An input CFG and its mirrored ITG.4.1 Production MirroringThe first step is to convert the monolingual Chi-nese CFG to a bilingual ITG.
The production mir-roring tactic simply doubles the number of pro-ductions, transforming every monolingual produc-tion into two bilingual productions, 4 one straightand one inverted, as for example in Figure 1 wherethe upper Chinese CFG becomes the lower ITG.The intent of the mirroring is to add enough flex-ibility to allow parsing of English sentences usingthe language 1 side of the ITG.
The extra produc-tions accommodate r versed subconstituent order inthe source language's constituents, at the same timerestricting the language 2 output sentence to con-form the given target grammar whether straight orinverted productions are used.The following example illustrates how produc-tion mirroring works.
Consider the input sentenceHe is the son of Stephen, which can be parsed bythe ITG of Figure 1 to yield the corresponding out-put sentence ~ ~ 1 ~ : ~ ,  with the followingparse tree:(5) \[\[\[He/{~ \]Pro\]No \[\[is/~ \]v \[the/e\]NOlSE( \ [ son /~\ ]N  \[of/~\]Moa \ [S tephen/~f f  \]N)NP\]VP \[.\]o \]Punc \]SProduction mirroring produced the inverted NPconstituent which was necessary to parse son ofStephen, i.e., ( son / .~  of/flcJ S tephen/~)Np.If the target CFG is purely binary branching,then the previous theoretical and linguistic analy-ses (Wu, 1997) suggest hat much of the requisiteconstituent and word order transposition may be ac-commodated without change to the mirrored ITG.On the other hand, if the target CFG contains pro-ductions with long right-hand-sides, then merely in-verting the subconstituent order will probably be in-sufficient.
In such cases, a more complex transfor-mation heuristic would be needed.Objective 3 (improving grammaticality of theoutput) can be directly tackled by using a tight tar-4Except for unary productions, which yield only one bilin-gual production.get grammar.
To see this, consider using a mir-rored Chinese CFG to parse English sentences withthe language 1side of the ITG.
Any resulting parsetree must be consistent with the original Chinesegrammar.
This follows from the fact that both thestraight and inverted versions of a production havelanguage 2 (Chinese) sides identical to the originalmonolingual production: inverting production ori-entation cancels out the mirroring of the right-hand-side symbols.
Thus, the output grammaticality de-pends directly on the tightness of the original Chi-nese grammar.In principle, with this approach a single tar-get grammar could be used for translation fromany number of other (fixed word-order) source lan-guages, so long as a translation lexicon is availablefor each source language.Probabilities on the mirrored ITG cannot be re-liably estimated from bilingual data without a verylarge parallel corpus.
A straightforward approxima-tion is to employ EM or Viterbi training on just amonolingual target language (Chinese) corpus.4.2 Part-of-Speech MappingThe second problem is that the part-of-speech (PoS)categories used by the target (Chinese) grammar donot correspond to the source (English) words whenthe source sentence is parsed.
It is unlikely that anyEnglish lexicon will list Chinese parts-of-speech.We employ a simple part-of-speech mappingtechnique that allows the PoS tag of any corre-sponding word in the target language (as found inthe translation lexicon) to serve as a proxy for thesource word's PoS.
The word view, for example,may be tagged with the Chinese tags nc and vn,since the translation lexicon holds both viewyy/~~nc and v iewvB/~vn.Unknown English words must be handled iffer-ently since they cannot be looked up in the transla-tion lexicon.
The English PoS tag is first found bytagging the English sentence.
A set of possible cor-responding Chinese PoS tags is then found by tablelookup (using a small hand-constructed mapping ta-ble).
For example, NN may map to nc, loc and pref,while VB may map to vi, vn, vp, vv, vs, etc.
Thismethod generates many hypotheses and should onlybe used as a last resort.4.3 Word SkippingRegardless of how constituent-order transposition ishandled, some function words simply do not oc-cur in both languages, for example Chinese aspect1411markers.
This is the rationale for the singletonsmentioned in Section 3.If we create an explicit singleton hypothesis forevery possible input word, the resulting searchspace will be too large.
To recognize singletons, weinstead borrow the word-skipping technique fromspeech recognition and robust parsing.
As formal-ized in the next section, we can do this by modifyingthe item extension step in our chart-parser-like algo-rithm.
When the dot of an item is on the rightmostposition, we can use such constituent, a subtree, toextend other items.
In chart parsing, the valid sub-trees that can be used to extend an item are thosethat are located on the adjacent right of the dot po-sition of the item and the anticipated category of theitem should also be equal to that of the subtrees.If word-skipping is to be used, the valid subtreescan be located a few positions right (or, left for theitem corresponding to inverted production) to thedot position of the item.
In other words, words be-tween the dot position and the start of the subtee areskipped, and considered to be singletons.Consider Sentence 5 again.
Word-skipping han-dled the the which has no Chinese counterpart.
At acertain point during translation, we have the follow-ing item: VP--+\[is/x~\]veNP.
With word-skipping,it can be extended to VP --+\[is/x~\]vNPe by the sub-tree (son /~ of/~ Stephen/~)Np,  even thesubtree is not adjacent (but within a certain distance,see Section 5) to the dot position of the item.
Thethe located on the adjacent to the dot position of theitem is skipped.Word-skipping provides us the flexibility to parsethe source input by skipping possible singleton(s),if when we doing so, the source input can be parsedwith the highest likelihood, and grammatical outputcan be produced.5 Translation AlgorithmThe translation search algorithm differs from that ofWu's SBTG model in that it handles arbitrary gram-mars rather than binary bracketing rammars.
Assuch it is more similar to active chart parsing (Ear-ley, 1970) rather than CYK parsing (Kasami, 1965;Younger, 1967).
We take the standard notion ofitems (Aho and Ullman, 1972), and use the term an-ticipation to mean an item which still has symbolsright of its dot.
Items that don't have any symbolsright of the dot are called subtree.As with Wu's SBTG model, the algorithm max-imizes a probabilistic objective function, Equa-tion (2), using dynamic programming similar to thatfor HMM recognition (Viterbi, 1967).
The presenceof the bigram model in the objective function ne-cessitates indexes in the recurrence not only on sub-trees over the source English string, but also on thedelimiting words of the target Chinese substrings.The dynamic programming exploits a recursiveformulation of the objective function as follows.Some notation remarks: es..t denotes the subse-quence of English tokens e,+l ,  e~+2, ?
?
.
,  et.
Weuse C(s .
.
t )  to denote the set of Chinese words thatare translations of the English word created by tak-ing all tokens in es..t together.
C(s ,  t) denotes theset of Chinese words that are translations of any ofthe English words anywhere within es..t. K is themaximium number of consecutive English wordsthat can be skipped.
5 Finally, the argmax operator isgeneralized to vector notation to accommodate mul-tiple indices.1.
Initialization60rstYy = bi (es.
.
?/Y) ,O<s<t<TY e c(s .
.
t )r is  Y ' s  PoS2.
RecursionFor all r, s, t, u, v such thatr is the category of a constituent spanning s to t0_<s<t<Tu, v are the l--eftmost/rightmost words of the constituent(~,'stuv"\[rstuv= maxr6\[\] ,6 0 x?
1 ?
t rstuv rstuv, t'rstuvJ-0 ~orstuv--  ma, r6\[\] 0 if6~{t~,o > , " t  rst~,~,0 otherwisewhere 6:r\[\] r$ tu~'n l  ax8, <t  t ~S,aelO<s)+l--tt<K= argmaxS, <t ,  <-%+1O<s,+l-t,<Kai(r) f l  dr,s,t,u,v, gv,u,+,i=0r lai(r)  H ~rls|tlttlvlffvlttt'kli=0Sln our experiments, It"was set to 4%0 = s, sn = t, u?
= u, vn ~ v, gv,u,+a = gv,+lun :1, qi = (r iait iuivi)1412~0r.~tuv ~0 7"rstu vmaxr-+(ro...rn)s ,<t ,  ~.%+XO<s,+I-G<_K= argmaxr-+(~o ..... )s,<tt<_s,-t-1O<s,+x-t,<_Ka i ( r )  f l  ~r,s,t,u,v, 9v,+lu,i=Onai(r)  H ~ .
.
.
.
t,u,v,ffv,+,u,i=03.
ReconstructionLet qo = (S, 0, T, u, v) be the optimal root.
where(u, v) = maxu, vEC(O.T) ~S st U v For any child ofq = (r, s, t, u, v) is given by:{ r~ \] "\[\] , i fTq=\ [ \ ]  A.risitiuiviCHILD(q, r) : 7-~) 0 ifTq 0 ~risit iuivi ;  "-NIL  otherwiseAssuming the number of translation per word isbounded by some constant, then the maximum sizeof C(s ,  t) is proportional to t - s. The asymptotictime complexity for our algorithm is thus boundedby O(Tr ) .
However, note that in theory the com-plexity upper bound rises exponentially rather thanpolynomially with the size of the grammar, justas for context-free parsing (Barton et al, 1987),whereas this is not a problem for Wu's SBTG algo-rithm.
In practice, natural language grammars areusually sufficiently constrained so that speed is ac-tually improved over the SBTG algorithm, as dis-cussed later.The dynamic programming is efficiently im-plemented by an active-chart-parser-style ag nda-based algorithm, sketched as follows:1.
Initialization For each word in the input sentence, put asubtree with category equal to the PoS of its translationinto the agenda.2.
Recursion Loop while agenda is not empty:(a) If the current item is a subtree of category X, ex-tend existing anticipations by calling ANTIEIPA-TIONEXTENSION, For each rule in the grammarof Z ~ XW.
.
.
Y, add an initial anticipation ofthe form Z ~ X ?
W. .
.
Y and put it into theagenda.
Add subtree X to the chart.
(b) If the current item is an anticipation of the formZ ~ W. .
.
*X .
.
.
Y from s to to, find all subtreesin the chart with category X that start at position t~and use each subtree to extend this anticipation bycalling ANTICIPATIONEXTENSION.ANTICIPATIONEXTENS1ON : Assuming the subtree wefound is of category X from position sl to t, for anyanticipation of the form Z --+ W.. .
?
X ... Y from soto \ [s l - I f ,  sl\], extend it to Z --+ IV... X ?
... Y withspan from so to t and add it to the agenda.3.
Reconstruction The output string is recursively recon-structed from the highest likelihood subtree, with cate-gory S, that span the whole input sentence.6 ResultsThe grammatical channel was tested in the SILCtranslation system.
The translation lexicon waspartly constructed by training on government tran-scripts from the HKUST English-Chinese Paral-lel Bilingual Corpus, and partly entered by hand.The corpus was sentence-aligned statistically (Wu,1994); Chinese words and collocations were ex-tracted (Fung and Wu, 1994; Wu and Fung, 1994);then translation pairs were learned via an EM pro-cedure (Wu and Xia, 1995).
Together with hand-constructed entries, the resulting English vocabu-lary is approximately 9,500 words and the Chinesevocabulary is approximately 14,500 words, with amany-to-many translation mapping averaging 2.56Chinese translations per English word.
Since thelexicon's content is mixed, we approximate ransla-tion probabilities by using the unigram distributionof the target vocabulary from a small monolingualcorpus.
Noise still exists in the lexicon.The Chinese grammar we used is not tight--it was written for robust parsing purposes, and assuch it over-generates.
Because of this we have notyet been able to conduct a fair quantitative assess-ment of objective 3.
Our productions were con-structed with reference to a standard grammar (Bei-jing Language and Culture Univ., 1996) and totalled316 productions.
Not all the original productionsare mirrored, since some (128) are unary produc-tions, and others are Chinese-specific lexical con-structions like S ~ ~-~ S NP ~ S, which areobviously unnecessary to handle English.
About27.7% of the non-unary Chinese productions weremirrored and the total number of productions in thefinal ITG is 368.For the experiment, 222 English sentences witha maximum length of 20 words from the parallelcorpus were randomly selected.
Some examples ofthe output are shown in Figure 2.
No morphologicalprocessing has been used to correct he output, andup to now we have only been testing with a bigrammodel trained on extremely small corpus.With respect to objective 1(increasing translationspeed), the new model is very encouraging.
Ta-ble 1 shows that over 90% of the samples can beprocessed within one minute by the grammaticalchannel model, whereas that for the SBTG channelmodel is about 50%.
This demonstrates the stronger1413T ime(x)x < 30 secs.30 secs.
< x < 1 min.x > 1 min.SBTG GrammaticalChannel Channel83.3% 15.6%34.9%49.5%7.6%9.1%Table 1: Translation speed.Sentence meaning SBTG Grammaticalpreservation Channel ChannelCorrect 25.9% 32.3%Incorrect 74.1% 67.7 %Table 2: Translation accuracy.constraints on the search space given by the SITG.The natural trade-off is that constraining thestructure of the input decreases robustness some-what.
Approximately 13% of the test corpus couldnot be parsed in the grammatical channel model.As mentioned earlier, this figure is likely to varywidely depending on the characteristics of the tar-get grammar.
Of course, one can simply back offto the SBTG model when the grammatical channelrejects an input sentence.With respect to objective 2 (improving meaning-preservation accuracy), the new model is alsopromising.
Table 2 shows that the percentage ofmeaningfully translated sentences rises from 26% to32% (ignoring the rejected cases).
7 We have judgedonly whether the correct meaning is conveyed by thetranslation, paying particular attention to word orderand grammaticality, but otherwise ignoring morpho-logical and function word choices.7 ConclusionCurrently we are designing a tight generation-oriented Chinese grammar to replace our robustparsing-oriented grammar.
We will use the newgrammar to quantitatively evaluate objective 3.
Weare also studying complementary approaches tothe English word deletion performed by word-skipping--i.e., extensions that insert Chinese wordssuggested by the target grammar into the output.The framework seeds a natural transition towardpattern-based translation models (objective 4).
One7These accuracy rates are relatively low because these ex-periments are being conducted with new lexicons and grammaron a new translation direction (English-Chinese).can post-edit the productions of a mirrored SITGmore carefully and extensively than we have donein our cursory pruning, gradually transforming theoriginal monolingual productions into a set of truetransduction rule patterns.
This provides a smoothevolution from a purely statistical model toward ahybrid model, as more linguistic resources becomeavailable.We have described a new stochastic grammati-cal channel model for statistical machine translationthat exhibits several nice properties in comparisonwith Wu's SBTG model and IBM's word alignmentmodel.
The SITG-based channel increases trans-lation speed, improves meaning-preservation accu-racy, permits tight target CFGs to be incorporatedfor improving output grammaticality, and suggestsa natural evolution toward transduction rule mod-els.
The input CFG is adapted for use via produc-tion mirroring, part-of-speech mapping, and word-skipping.
We gave a polynomial-time translationalgorithm that requires only a translation lexicon,plus a CFG and bigram language model for the tar-get language.
More linguistic knowledge about thetarget language is employed than in pure statisti-cal translation models, but Wu's SBTG polynomial-time bound on search cost is retained and in fact thesearch space can be significantly reduced by usinga good grammar.
Output always conforms to thegiven target grammar.AcknowledgmentsThanks to the SILC group members: Xuanyin Xia, DanielChan, Aboy Wong, Vincent Chow & James Pang.ReferencesAlfred V. Aho and Jeffrey D. Ullman.
1972.
The Theorb, of Parsing.Translation.
and Compiling.
Prentice Hall, Englewood Cliffs, NJ.G.
Edward Barton, Robert C. Berwick, and Eric.
S Ristad.
1987.
Com-putational Complexity and Natural Language.
MIT Press, Cam-bridge, MA.Beijing Language and Culture Univ.. 1996.
Sucheng Hanyu ChujiJiaocheng (A Short h~tensive Elementary Chb~ese Course), volume1-4.
Beijing Language And Culture Univ.
Press.Peter E Brown, John Cocke, Stephen A. DellaPietm, Vincent J. Del-laPietra, Frederick Jelinek, John D. Lafferty, Robert L. Mercer, andPaul S. Roossin.
1990.
A statistical approach to machine transla-tion.
ComputationalLinguistics, 16(2):29-85.Peter E Brown, Stephen A. DellaPietra, Vincent J. DellaPietra, andRobert L. Mercer.
1993.
The mathematics of statistical ma-chine translation: Parameter stimation.
Computational Lfl~guis-tics, 19(2):263-311.Jay Earley.
1970.
An efficient context-free parsing algorithm.
Com-munications ofthe Assoc.
for Computing Machinerb', 13(2):94-102.Pascale Fung and Dekai Wu.
1994.
Statistical augmentation f a Chi-nese machine-readabledictionary.
In Proc.
of the 2nd Annual Work-shop on Verb' Large Corpora, pg 69-85, Kyoto, Aug.Input : I entirely agree with this point of view.Output: ~J~'~" ~,, ~ ,1~ ~1~ - ll~ ~i oCorpus: ~, ,~~_~'~oInput : Th is  would create a tremendous financialburden to taxpayers in Hong Kong.Output: i~::~: ~ ~ ~J ~)i~ )~ ~lJ ~ .~ }k. \[~J ":'-'-'-~ ~\[~ fl"-J ~.
~ oCorpus: ~ l ~ i ~ J ~  ) , .~ i~gD\ ]~ ,~ ~I~ oInput : The Government wants, and will work for, thebest education for all the children of Hong Kong.Output: :~  ~ ~\]I~ J( ~ P--J ~ ,:~,~, .,~  I \]f~ ,,~ ~J~ ~ ~j~ i~J )~.~ ~ ~1~:  oCorpus: ~ , ~  ~ ~ " ~ 2 ~  ~lg l /9~g,  ~ l~ l~ '~c~\ ]~_~oInput : Let me repeat one simple point yet again.Output: ~ ~\[\] .~ ~ ~'~  ~'\[~ :~ oCorpus : -~~-~-g~oInput : We are very disappointed.Output: ~ J~\ ]  J~ +~: ~ ~ \[ItJ oCorpus: ~'~,~:~oFigure 2: Example translation outputs from thegrammatical channel model.T.
Kasami.
1965.
An efficient recognition and syntax analysis al-gorithm for context-free languages.
Technical Report AFCRL-65-758, Air Force Cambridge Research Lab., Bedford, MA.Andrew J. Viterbi.
1967.
Error bounds for convolutional codes and anasymptotically optimal decoding algorithm.
IEEE Transactions onh!formation Theory, 13:260-269.Dekai Wu and Pascale Fang.
1994.
Improving Chinese tokenizationwith linguistic filters on statistical lexical acquisition.
In Proc.
of4th Conf.
on ANLP, pg 180-181, Stuttgart, Oct.Dekai Wu and Xuanyin Xia.
1995.
Large-scale automatic extractionof an English-Chinese lexicon.
Machh~e Translation, 9(3--4):285-313.Dekai Wu.
1994.
Aligning a parallel English-Chinese corpus tatisti-cally with lexical criteria.
In Proc.
of 32nd Annual Conf.
of Assoc.fi~r ComputationalLinguistics, pg 80-87, Las Cruces, Jun.Dekai Wu.
1995a.
An algorithm for simultaneously bracketing paralleltexts by aligning words.
In Proc.
of 33rd Annual Conf.
of Assoc.
forComputational Linguistics, pg 244-251, Cambridge, MA, Jun.Dekai Wu.
1995b.
Grammarless extraction of phrasal translation ex-amples from parallel texts.
In TMI-95, Proc.
of the 6th hmi Conf.on Theoretical and Methodological Issues in Machine Translation,volume 2, pg 354-372, Leuven, Belgium, Jul.Dekai Wu.
1995c.
Stochastic inversion transduction grammars, withapplication to segmentation, bracketing, and alignment of parallelcorpora.
In Proc.
of IJCAI-95, 14th InM Joint Conf.
on ArtificialIntelligence, pg 1328-1334, Montreal, Aug.Dekai Wu.
1995d.
Trainable coarse bilingual grammars for paralleltext bracketing.
In Proc.
of the 3rdAnnual Workshop on Verb' LargeCorpora, pg 69-81, Cambridge, MA, Jun.Dekai Wu.
1996.
A polynomial-time algorithm for statistical machinetranslation.
InProc.
of the 34th Annual Conf.
of the Assoc.
for Com,putational Linguistics, pg 152-158, Santa Cruz, CA, Jun.Dekai Wu.
1997.
Stochastic inversion transduction grammars andbilingual parsing of parallel corpora.
Computational Linguistics,23(3):377--404, Sept.David H. Younger.
1967.
Recognition and parsing of context-free lan-guages in time n 3. hzformation and Control, 10(2): 189-208.1414Machine Translation with a Stochastic Grammatical  Channel(~Y~~~ I~ I~~I~~~)Dekai WU (~,~)  and Hongsing WONG (~-~)( deka i ,  wong) +cs.
usL.
hk'~,  ~_.~:i~:~-~?_~ o 1"~ Wu (1996) ~\]\[~1~l~,~,,j~L~f/l)&~J~-~:~_ (~'~--~121~9~::~:~~ ' I  =' ~- ) ,  ~'fl"+ ~:~_~'t~+' J :1415
