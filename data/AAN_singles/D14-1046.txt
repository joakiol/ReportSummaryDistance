Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 414?418,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsAutomatic Domain Assignment for Word Sense AlignmentTommaso CaselliTrentoRISE / Via Sommarive, 1838123 Povo, Italyt.caselli@gmail.comCarlo StrapparavaFBK / Via Sommarive, 1838123 Povo, Italystrappa@fbk.euAbstractThis paper reports on the development of a hy-brid and simple method based on a machinelearning classifier (Naive Bayes), Word SenseDisambiguation and rules, for the automaticassignment of WordNet Domains to nominalentries of a lexicographic dictionary, the SensoComune De Mauro Lexicon.
The system ob-tained an F1 score of 0.58, with a Precisionof 0.70.
We further used the automatically as-signed domains to filter out word sense align-ments between MultiWordNet and Senso Co-mune.
This has led to an improvement in thequality of the sense alignments showing thevalidity of the approach for domain assign-ment and the importance of domain informa-tion for achieving good sense alignments.1 Introduction and Problem StatementLexical knowledge, i.e.
how words are used and ex-press meaning, plays a key role in Natural LanguageProcessing.
Lexical knowledge is available in manydifferent forms, ranging from unstructured terminolo-gies (i.e.
word list), to full fledged computational lexicaand ontologies (e.g.
WordNet (Fellbaum, 1998)).
Theprocess of creation of lexical resources is costly bothin terms of money and time.
To overcome these lim-its, semi-automatic approaches have been developed(e.g.
MultiWordNet (Pianta et al., 2002)) with differ-ent levels of success.
Furthermore, important informa-tion is scattered in different resources and difficult touse.
Semantic interoperability between resources couldrepresent a viable solution to allow reusability and de-velop more robust and powerful resources.
Word sensealignment (WSA) qualifies as the preliminary require-ment for achieving this goal (Matuschek and Gurevych,2013).WSA aims at creating lists of pairs of senses fromtwo, or more, (lexical-semantic) resources which de-note the same meaning.
Different approaches to WSAhave been proposed and they all share some commonelements, namely: i.)
the extensive use of sense de-scriptions of the words (e.g.
WordNet glosses); and ii.
)the extension of the basic sense descriptions with addi-tional information such as hypernyms, synonyms anddomain or category labels.The purpose of this work is two folded: first, we exper-iment on the automatic assignment of domain labels tosense descriptions, and then, evaluate the impact of thisinformation for improving an existing sense aligneddataset for nouns.
Previous works has demonstratedthat domain labels are a good feature for obtaining highquality alignments of entries (Navigli, 2006; Toral etal., 2009; Navigli and Ponzetto, 2012).
The Word-Net (WN) Domains (Magnini and Cavaglia, 2000; Ben-tivogli et al., 2004) have been selected as reference do-main labels.
We will use as candidate lexico-semanticresources to be aligned two Italian lexica, namely, Mul-tiWordNet (MWN) and the Senso Comune De MauroLexicon (SCDM) (Vetere et al., 2011).The two resources differ in terms of modelization: theformer, MWN, is an Italian version of WN obtainedthrough the ?expand model?
(Vossen, 1996) and per-fectly aligned to Princeton WN 1.6, while the latter,SCDM, is a machine readable dictionary obtained froma paper-based reference lexicographic dictionary, DeMauro GRADIT.
Major issues for WSA of the lexicaconcern the following aspects:?
SCMD has no structure of word senses (i.e.
notaxonomy, no synonymy relations, no distinctionbetween core senses and subsenses for polyse-mous entries) unlike MWN;?
SCDM has no domain or category labels associ-ated to senses (with the exception of specific ter-minological entries) unlike MWN;?
the Italian section of MWN has only 2,481 glossesin Italian over 28,517 synsets for nouns (i.e.8.7%).The remainder of this paper is organized as follows:Section 2 will report on the methodology and exper-iments implemented for the automatic assignment ofthe WN Domains to the SCDM entries.
Section 3 willdescribe the dataset used for the evaluation of the WSAexperiments and the use of the WN Domains for filter-ing the sense alignments.
Finally, Section 4 illustratesconclusion and future work.2 Methodology and ExperimentsThe WN Domains consist of a set of 166 hierarchicallyorganized labels which have been associated to each414Classifiers P R F1 10-Fold F1NaiveBayeslemma0.77 0.58 0.66 0.66MaxEntlemma0.70 0.49 0.58 0.63NaiveBayeswsd0.77 0.58 0.66 0.69MaxEntwsd0.74 0.54 0.62 0.67Table 2: Results for the Naive Bayes and Maximum Entropy binary classifiers.synset1and express a subject field label (e.g.
SPORT,MEDICINE).
A special label, FACTOTUM, has beenused for those synsets which can appear in almost allsubject fields.The identification of a domain label to the nominal en-tries in the SCDM Lexicon is based the ?One Domainper Discourse?
(ODD) hypothesis applied to the sensedescriptions.
We have used a reduced set of domainslabels (45 normalized domains) following (Magnini etal., 2001).To assign the WN domain label to the SCDM entries,we have developed a hybrid method: first a binary clas-sifier is applied to the SCDM sense descriptions to dis-criminate between two domain values, FACTOTUMand OTHER, where the OTHER value includes all re-maining 44 normalized domains.
After this, all entriesclassified with the OTHER value are analyzed by a rulebased system and associated with a specific domain la-bel (i.e.
SPORT, MEDICINE, FOOD .
.
.
).2.1 Classifier and feature selectionWe have developed a training set by manually align-ing noun senses between the two lexica.
The sensealignment allows us to associate all the information of asynset to a corresponding entry in the SCDM lexicon,including the WN Domain label.
Concerning the testset, we have used an existing dataset of aligned nounpairs as in (Caselli et al., 2014).
We report in Table 1the figures for the training and test sets.
Multiple align-ments with the same domain label have been excludedfrom the training set.Characteristics Training Set Test Set# lemmas 131 46# of aligned pairs 369 166# of SCDM senses 747 216# of MWN synsets 675 229# SCDM withWN Domain label 350 118Table 1: Training and test sets for the classifier.In order for the classifier to predict the binary do-main labels (FACTOTUM and OTHER), each sensedescription of the SCDM Lexicon has been repre-sented by means of a two-dimensional feature vector(e.g.
for training data: BINARY DOMAIN LABEL1The full set of labels and hierarchy is available athttp://wndomains.fbk.eu/hierarchy.htmlGENERIC:val SPECIFIC:val).
Feature values havebeen obtained through two strategies:?
lemma label: we extract all normalized domainlabels associated to each sense of each lemma inthe sense description from MWN.
The value ofthe feature GENERIC corresponds to the sum ofthe FACTOTUM labels.
The value of the fea-ture SPECIFIC corresponds to the sum of all otherspecific domain labels (e.g.
MEDICINE, SPORTetc.)
after they have been collapsed into a singlevalue (i.e.
NOT-FACTOTUM).?
word sense label: for each sense description, wehave first performed Word Sense Disambiguationby means of an adapted version to Italian of theUKB package2(Agirre et al., 2010; Agirre et al.,2014)3.
Only the highest ranked synset, and as-sociated WN Domain(s), was retained as good.Similarly to the lemma label strategy, the sum ofthe domain label FACTOTUM is assigned to thefeature GENERIC, while the sum of all other do-main labels collapsed into the single value NOT-FACTOTUM is assigned to the feature SPECIFIC.We experimented with two classifiers: Naive Bayesand Maximum Entropy as implemented in the MAL-LET package (McCallum, 2002).
We illustrate the re-sults in Table 2.
The classifiers have been evaluatedwith respect to standard Precision (P), Recall (R) andF1 against the test set.
Ten-fold cross validation hasbeen performed on the training set as well.
Classifierstrained with the first strategy will be associated with thelabel lemma, while those trained with the second strat-egy with the label wsd.Both classifiers obtains good results with respect tothe test data in terms of Precision and Recall.
TheNaive Bayes classifier outperforms the Maximum En-tropy one in both training approaches, suggesting bettergeneralization capabilities even in presence of a smalltraining set and basic features.
The role of WSD hasa positive impact, namely for the Maximum Entropyclassifier (Precision +4 points, Recall +5 points withrespect to the lemma label).
Although such a positiveeffect of the WSD does not emerge for the Naive Bayesclassifier with respect to the test set, we can still ob-serve an improvement over the ten-fold cross valida-tion (F1= 0.69 vs. F1=0.66).
We finally selected the2Available at http://ixa2.si.ehu.es/ukb/3We used the WN Multilingual Central Repository asknowledge base and the MWN entries as dictionary415predictions of Naive Bayeswsdclassifier as input to therule-based system as it provides the highest scores.2.2 Rules for WN Domain assignmentThe rule based classifier for final WN Domain assign-ment works as follows:?
lemmatized and word sense disambiguated lem-mas in the sense descriptions are associated withthe corresponding WN Domains from MWN;?
frequency counts on the WN Domain labels is ap-plied; the most frequent WN Domain is assignedas the correct WN Domain of the nominal entry;?
in case two or more WN Domains have same fre-quency, the following assignment strategy is ap-plied: if the frequency scores of the WN Do-mains is equal to 1, the value FACTOTUM is se-lected; on the contrary, if the frequency score ishigher than 1, all WN Domain labels are retainedas good.We report the results on final domain assignmentin Table 3.
The final system, NaiveBayes+Rules, hasbeen compared to two baselines.
Both baselines ap-ply frequency counts over the WN Domains labelsof the lemmas of the sense descriptions for the en-tire set of the 45 normalized domain values, includingthe FACTOTUM label, as explained in Section2.
TheBaselinelemmaassigns the domain by taking into ac-count every WN Domain associated to each lemma.
Onthe other hand, the Baselinewsdselects only the WNDomain of sense disambiguated lemmas.
WSD for thesecond baseline has been performed by applying thesame method described in Section 2.1.
The results ofboth baselines have high values for Precision (0.58 forBaselinelemma, 0.70 for Baselinewsd).
We considerthis as a further support to the validity of the ODD hy-pothesis which seems to hold even for text descriptionslike dictionary glosses which normally use generic lex-ical items to illustrate word senses.
It is also interestingto notice that WSD on its own has a positive impact inBaselinewsdsystem for the assignment of specific do-main labels (F1=0.53).The hybrid system performs better than both base-lines in terms of F1 scores (F1=0.58 vs. F1=0.45 forBaselinelemmavs.
F1=0.53 for Baselinewsd).
How-ever, both the hybrid system and the Baselinewsdob-tain the same Precision.
To better evaluate the per-formance of our hybrid approach, we computed thepaired t-test.
The results of the hybrid system are sta-tistically significant with respect to the Baselinelemma(p < 0.05) and for Recall only when compared to theBaselinewsd.To further analyze the difference between the hybridsystem and the Baselinewsd, we performed an erroranalysis on their outputs.
We have identified that thehybrid system is more accurate in the prediction of theSystem P R F1NaiveBayeswsd+Rules 0.70?
0.50??
0.58?Baselinelemma0.58 0.36 0.45Baselinewsd0.70 0.43 0.53Table 3: Results of WN Domain Assignment over theSDCM entries.
Statistical significance of the Naive-Bayes+Rules system has been marked with a ?
for theBaselinelemmaand with a ?
for the BaselinewsdFACTOTUM class with respect to the baseline.
In par-ticular, the accuracy of the hybrid system on this classis 79% while that of the baseline is only 65%.
In addi-tion to this, the hybrid system provides better results interms of Recall (R=0.50 vs. R=0.43).
Although compa-rable, the hybrid system provides more accurate resultswith respect to the baseline.3 Domain Filtering for WSAThis section reports on the experiments for improvingexisting WSA for nouns between SDCM and MWN.
Inthis work we have used the same dataset and alignmentmethods as in (Caselli et al., 2014), shortly describedhere:?
Lexical Match: for each word w and for eachsense s in the given resources R ?
{MWN,SCDM}, we constructed a sense descriptionsdR(s) as a bag of words in Italian.
The alignmentis based on counting the number of overlappingtokens between the two strings, normalized by thelength of the strings;?
Cosine Similarity: we used the Personalized PageRank (PPR) algorithm (Agirre et al., 2010) withWN 3.0 as knowledge base extended with the?Princeton Annotated Gloss Corpus?.
Once thePPR vector pairs are obtained, the alignment isobtained on the basis of the cosine score for eachpair4.The dataset consists of 166 pairs of aligned sensesfrom MWN and SCDM for 46 nominal lemmas(see also column ?Test set?
in Table 1).
Overall,SCDM covers 53.71The main difference with respectto (Caselli et al., 2014) is that the proposed alignmentshave been additionally filtered on the basis of the outputof the WN domain system (NaiveBayeswsd+Rules).
Inparticular, for each aligned pair which was consideredas good in (Caselli et al., 2014), we have applied a fur-ther filtering based on the WN domain system resultsas follows: if two senses are aligned but do not havethe same domain, they are excluded from the WSA re-sults, otherwise they are retained.
Table 4 illustrates4The vectors for the SCDM entries were obtained by, first,applying Google Translate API to get the English translationsand, then, PPR over WN 3.0.416System P R F1LexicalMatch 0.76 (0.69) 0.27 (0.44) 0.40 (0.55)Cosine noThreshold 0.27 (0.12) 0.47 (0.94) 0.35 (0.21)Cosine > 0.1 0.77 (0.52) 0.21 (0.32) 0.33 (0.40)Cosine > 0.2 0.87 (0.77) 0.14 (0.21) 0.24 (0.33)LexicalMatch+Cosine > 0.1 0.73 (na) 0.40 (na) 0.51 (na)LexicalMatch+Cosine > 0.2 0.77 (0.67) 0.37 (0.61) 0.50 (0.64)Table 4: Results for WSA of nouns with domain filtering.the results of the WSA approaches with domain fil-ters.
We report in brackets the results from (Caselli etal., 2014).
The filtering based on WN Domains has abig impact on Precision and contributes to increase thequality of the aligned senses.
Although, in general, wehave a downgrading of the performance with respect toRecall, the increase in Precision will reduce the man-ual post-processing effort to fully aligned the two re-sources5.
Furthermore, it is interesting to notice that,when merging together the results of the pre-filteredalignments from the two alignment approaches (Lex-icalMatch+Cosine > 0.1 and LexicalMatch+Cosine >0.2), we still have a very high Precision (> 0.70) and anincrease in Recall (> 0.40) with respect to the results ofeach approach.
Finally, we want to point out that whatwas reported as the best alignment results in (Caselliet al., 2014), namely LexicalMatch+Cosine > 0.2, canbe obtained, at least for Precision, with a lower filteringcut-off threshold on the Cosine Similarity approach (i.ecut-off threshold at or higher than 0.1)4 Conclusions and Future WorkThis work describes a hybrid approach based on aNaive Bayes classifier, Word Sense Disambiguationand rules for assigning WN Domains to nominal sensedescriptions of a lexicographic dictionary, the SensoComune De Mauro Lexicon.
The assignment of do-main labels has been used to improve WSA results onnouns between the Senso Comune Lexicon and Mul-tiWordNet.
The results support some observations,namely: i.)
domain filtering plays an important rolein WSA, namely as a strategy to exclude wrong align-ments (false positives) and improve the quality of thealigned pairs; ii.)
the method we have proposed is a vi-able approach for automatically enriching existing lex-ical resources in a reliable way; and iii.)
the ODD hy-pothesis also apply to sense descriptions.An advantage of our approach is its simplicity.
We haveused features based on frequency counts and obtainedgood results, with a Precision of 0.70 for automatic WNDomain assignment.
Nevertheless, an important roleis played by Word Sense Disambiguation.
The use ofdomain labels obtained from sense disambiguated lem-mas improves both the results of the classifier and those5The F1 of 0.64 in (Caselli et al., 2014) is obtained with aPrecision of 0.67, suggesting that some alignments are falsepositivesof the rules.
The absence of statistical significance withrespect to the Baselinewsdis not to be considered as anegative result.
As the error analysis has showed, theclassifier mostly contributes to the identification of theFACTOTUM value, which tends to be overestimatedeven with sense disambiguated lemmas, and to Recall.We are planning to extend this work to include do-main clusters to improve the domain assignment re-sults, namely in terms of Recall.AcknowledgmentsOne of the author wants to thank Vrije UniverisiteitAmsterdam for sponsoring the attendance to theEMNLP conference.ReferencesEneko Agirre, Montse Cuadros, German Rigau, andAitor Soroa.
2010.
Exploring knowledge basesfor similarity.
In Nicoletta Calzolari (ConferenceChair), Khalid Choukri, Bente Maegaard, JosephMariani, Jan Odijk, Stelios Piperidis, Mike Ros-ner, and Daniel Tapias, editors, Proceedings of theSeventh International Conference on Language Re-sources and Evaluation (LREC?10), Valletta, Malta,may.
European Language Resources Association(ELRA).Eneko Agirre, Oier L?opez de Lacalle, and Aitor Soroa.2014.
Random walks for knowledge-based wordsense disambiguation.
Computational Linguistics,40(1):57?84.Luisa Bentivogli, Pamela Forner, Bernardo Magnini,and Emanuele Pianta.
2004.
Revising the wordnetdomains hierarchy: semantics, coverage and balanc-ing.
In Proceedings of the Workshop on Multilin-gual Linguistic Resources, pages 101?108.
Associa-tion for Computational Linguistics.Tommaso Caselli, Carlo Strapparava, Laure Vieu, andGuido Vetere.
2014.
Aligning an italianwordnetwith a lexicographic dictionary: Coping with limiteddata.
In Proceedings of the Seventh Global WordNetConference, pages 290?298.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database (Language, Speech, and Commu-nication).
MIT Press.417Bernardo Magnini and Gabriela Cavaglia.
2000.
Inte-grating subject field codes into wordnet.
In Proceed-ings of the conference on International LanguageResources and Evaluation (LREC 2000).Bernardo Magnini, Carlo Strapparava, Giovanni Pez-zulo, and Alfio Gliozzo.
2001.
Using domain in-formation for word sense disambiguation.
In TheProceedings of the Second International Workshopon Evaluating Word Sense Disambiguation Systems,pages 111?114.
Association for Computational Lin-guistics.Michael Matuschek and Iryna Gurevych.
2013.Dijkstra-wsa: A graph-based approach to word sensealignment.
Transactions of the Association for Com-putational Linguistics (TACL), 2:to appear.Andrew Kachites McCallum.
2002.
Mallet: A ma-chine learning for language toolkit.Rada Mihalcea.
2007.
Using Wikipedia for automaticword sense disambiguation.
In Human LanguageTechnologies 2007: The Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics; Proceedings of the Main Confer-ence, Rochester, New York.Roberto Navigli and Simone Paolo Ponzetto.
2012.BabelNet: The automatic construction, evaluationand application of a wide-coverage multilingual se-mantic network.
Artificial Intelligence, 193:217?250.Roberto Navigli.
2006.
Meaningful clustering ofsenses helps boost word sense disambiguation per-formance.
In Proceedings of the 44thAnnual Meet-ing of the Association for Computational Linguis-tics joint with the 21stInternational Conference onComputational Linguistics (COLING-ACL), Sydney,Australia.Elisabeth Niemann and Iryna Gurevych.
2011.
Thepeoples web meets linguistic knowledge: Automaticsense alignment of Wikipedia and WordNet.
InProceedings of the 9th International Conference onComputational Semantics, pages 205?214, Singa-pore, January.Emanuele Pianta, Luisa Bentivogli, and Cristian Gi-rardi.
2002.
MultiWordNet: developing an alignedmultilingual database.
In First International Confer-ence on Global WordNet, Mysore, India.German Rigau and Agirre Eneko.
1995.
Disambiguat-ing bilingual nominal entries against WordNet.
InProceedings of workshop The Computational Lexi-con, 7th European Summer School in Logic, Lan-guage and Information, Barcelona, Spain.Adriana Roventini, Nilda Ruimy, Rita Marinelli,Marisa Ulivieri, and Michele Mammini.
2007.Mapping concrete entities from PAROLE-SIMPLE-CLIPS to ItalWordNet: Methodology and results.
InProceedings of the 45th Annual Meeting of the As-sociation for Computational Linguistics CompanionVolume Proceedings of the Demo and Poster Ses-sions, Prague, Czech Republic, June.Maria Ruiz-Casado, Enrique Alfonseca, and PabloCastells.
2005.
Automatic assignment of Wikipediaencyclopedic entries to WordNet synsets.
In Pro-ceedings of the Third international conference onAdvances in Web Intelligence, AWIC?05, Berlin,Heidelberg.
Springer-Verlag.Antonio Toral, Oscar Ferr?andez, Eneko Aguirre, andRafael Munoz.
2009.
A study on linking and disam-biguating wikipedia categories to wordnet using textsimilarity.
Proceedings of the International Confer-ence on Recent Advances in Natural Language Pro-cessing (RANLP 2009).Guido Vetere, Alessandro Oltramari, Isabella Chiari,Elisabetta Jezek, Laure Vieu, and Fabio MassimoZanzotto.
2011.
Senso Comune, an open knowl-edge base for italian.
JTraitement Automatique desLangues, 53(3):217?243.Piek Vossen.
1996.
Right or wrong: Combining lexi-cal resources in the eurowordnet project.
In Euralex,volume 96, pages 715?728.
Citeseer.418
