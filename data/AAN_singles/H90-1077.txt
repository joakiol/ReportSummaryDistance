Recent Results from the ARMContinuous Speech Recognition ProjectMartin Russell  and Keith PontingSpeech Research UnitRSKE, Malvern, Worcs WR14 3PS, UKIntroduct ionThis paper describes ome of the most recent workon continuous peech recognition using phoneme-level hidden Markov models (HMMs) which hasbeen conducted at the UK Speech Research Unitas part of the ARM (Airborne Reconnaissance Mis-sion) project \[11\].
The goal of the project is au-tomatic recognition of spoken airborne reconnais-sance reports.
The project draws on many years ofresearch undertaken i the UK by the Joint SpeechResearch Unit and the current RSRE Speech Re-search Unit, and also on the work on continuousspeech recognition using sub-word HMMs whichhas been conducted under the current DAttPA pro-gramme, particularly at MIT Lincoln Laboratory\[5\] and Carnegie Mellon University \[3\], and by thespeech groups at IBM and BBN.The project began with the definition, implemen-tation and evaluation of a simple speaker-dependent"baseline" system.
This was then systematicallyimproved and assessed in order to measure the per-formance gain resulting from each enhancement.The most recent version of the speaker-dependentARM system scores an average 86.8% word accu-racy with no syntax on the 497 word vocabularyARM task.
An overview of the development ofthe speaker- dependent AlUM system is presentedin \[11\] and more detailed information about partic-ular stages in the evolution of the system can befound in a set of separate reports \[7, 6, 10, 8, 9\].For completeness, the ARM task and the most re-cent version of the speaker- dependent AttM sys-tem are both described in the present paper.
Thepaper goes on to report work in progress in twoareas: initial work towards the development of aspeaker-independent version of the ARM system,and a study of the performance of versions of thespeaker-dependent ARM system from the viewpointof the number of system parameters.The development of a speaker-independent ver-sion of the ARM system is a current goal of theproject.
This has necessitated the collection ofa new 340 speaker speech corpus which includesrecordings of ARM reports for each subject.
Or-thographic annotation of this corpus is proceed-ing in parallel with the development of a baselinespeaker-independent version of the recognition sys-tem through a process of "annotation by forcedrecognition".
At the time of writing over 2000 sen-tences from 120 speakers have been labelled in thisway and systems trained on 10, 20, 30, 40, 50 and60 male speakers have been evaluated.
This workis described in more detail below.For a fixed size of training set, the number of sys-tem parameters i clearly an important considera-tion in the design of any statistically based speechrecognition system.
The final section presents astudy of some of the systems which have been evalu-ated as part of the ARM project from the perspec-tive of number of parameters.
The results showa range of sizes of parameter set which are largeenough to fully exploit the training data in termsof accurate modelling of speech patterns and at thesame time small enough to be supported by thetraining set.
Experiments using clustered triphoneswith state-specific covariance matrices, which werestimulated by these results, are also reported.The Airborne ReconnaissanceMission TaskTexts of simulated airborne reconnaissance r portswere created using an automatic sentence generatorbased on a finite state syntax (perplexity 6) and 497word vocabulary, defined by the Royal AerospaceEstablishment (RAE), Farnborough UK.
A typicalARM report is as follows:397"Inflight report 1-alpha/268.
Target map ref fox-trot kilo 9012, correction 2435.
Sighting at zero oneoh eight zulu.
New target defended strip.
Less than13 helicopters, type possibly hip.
Runways headingnorthwest wholly damaged, SAM defences to westintact.
TARWI 7/Sths at 2000, end of message"The start (first four sentences) and end (final sen-tence) of the report specify a mission reference num-ber, target location, time of sighting, target cate-gory and weather conditions respectively and aretightly structured.
The remaining central part ofthe report, which describes what can be seen fromthe aircraft, is relatively free format.The Speaker-Dependent ARMSystemThe development of the speaker-dependent ARMsystem is described in detail in \[11\].
This sectionis concerned with a description of its most recentversion (ARM version 7 of \[11\]).The acoustic front-end for the ARM system isbased on a conventional fi terbank analyser with 27critical band spaced filters covering frequencies upto 10kHz and producing 10O frames per second.
Themean channel amplitude of each filterbank frameis subtracted from all components of that frame,and a cosine transform is then applied.
The 17 di-mensional representation consisting of cosine coef-ficients 1 to 16 and mean filterbank channel ampli-tude forms the acoustic front-end parameterisationfor the ARM system (see \[8\]).
The frame-rate isreduced by approximately 50% using the variableframe rate technique described in \[7, 6\].Acoustic-phonetic processing in the currentspeaker-dependent version of the system uses a setof approximately 1500 HMMs (the precise numberdepends on the speaker) consisting of:?
Four single state "non-speech" HMMs to modelnon-speech sounds in regions of the test databetween spoken sentences.?
Six word-level HMMs for the commonly occur-ing short words "air", "at", "in", "of", "oh"and "or".
The number of states in these word-level HMMs is equal to three times the numberof phonemes in the baseform transcription ofthe corresponding word.?
Approximately 1490 three-state HMMs, one foreach word-internal triphone \[12\] which occursin the ARM vocabulary.
Since the baseformpronunciations of ARM vocabulary words varybetween speakers in the speaker dependent sys-tem, the precise number of triphone HMMs isdifferent for each speaker.All HMM states are identified with single multivari-ate Gaussian state output probability density func-tions with diagonal (co)variance matrices.
A single"grand" covariance matrix is shared by all states\[4,9\].Words in the ARM vocabulary are related tophonemes through a dictionary of "baseform"phonemic transcriptions (one transcription perword).
In the current, speaker-dependent, systemthis dictionary is modified for each speaker.
Themodifications are concerned with broad differences,for example between "northern British English" and"southern British English", rather than with finedetails of the speakers pronunciation.Parameter estimation is based on standard sub-word HMM training procedures in which sentencelevel HMMs are constructed from phoneme-levelHMMs (using the dictionary of baseform pronoun-ciations).
These are then mapped onto the sentencelevel acoustic data using the forward- backward al-gorithm to obtain contributions to the new modelparameter estimates.
Training is done in 3 stages:estimation of the parameters of context-insensitivemonophone-HMMs, estimation of the parameters ofcontext-sensitive triphone-HMMs (using the mono-phone HMM parameters as initial statistics), andestimation of the grand (co)variance matrix.Per fo rmance  o f  the  speaker -dependentARM sys temThe system was trained and evaluated separately onthree speakers.
For each speaker, 37 spoken ARMreports (224 sentences, approximately 15 minutesof speech), labelled orthographically at the sen-tence level, were used to estimate the parametersof the phoneme-hvel HMMs, and 10 reports (540words) were used as a test set.
Recognition is per-formed using a one-pass dynamic programming al-gorithm with beam search and partial-traceback \[1\].In experiments conducted in autumn 1989 the sys-tem scored an average 86.8% word-accuracy with-out syntax (93.8% words correct) \[11\].398The "baseline" speaker indepen-dent ARM systemA current goal of the ARM project is to develop aspeaker-independent version of the system.
Thiswill involve two stages: the creation of a set ofspeaker-independent triphone HMMs, and the de-velopment of adaptation techniques, uch as thosedescribed in \[2\], which will enable the parametersof these models to be adapted for new speakers.This section reports on the first of these two stages.As in the speaker-dependent work, this phase ofthe project has started with the implementationof a simple "baseline" speaker-independent system.This is obtained by training the system describedin the previous section using a corpus of AtLM re-ports spoken by a number of speakers.
This hasnecessitated the recording of a new speech corpuswhich includes recordings of ARM reports spokenby a large number of speakers.The  "Speaker  Independent"  S IA  SpeechCorpusThe SIA corpus contains recordings of 340 speakers,each speaking the following material:?
3 ARM reports6 extracts from ARM reports.
These extractsconsist of the centre sections of the reportswhich describe what the "observer" can seefrom the aircraft.
These sections of the reportsare less constrained than the initial and finalparts, and consequently contain a richer vari-ety of phonemic ontexts.?
10 sentences generated from an air-traffic con-trol application?
10 "TIMIT like" English sentences.Only the first two sets of recordings are used inthe current phase of the project, the remainderare intended for future work.
As with the ear-lier speaker-dependent database, all recordings weremade digitally on video cassette (44kHz samplerate) in a sound proof room using a Shure SM10head-mounted microphone.Annotat ion  o f  the  S IA  corpusAlthough it is possible to estimate triphone HMMparameters using speech labelled at the report level,in practice it desirable that the training materialshould be labelled at a finer level.
In the present ex-periments annotation is nominally at the sentencelevel, however segments of speech which are sepa-rated by long portions of non-speech are labelledas distinct items.
Thus if a subject speaks reportsas a sequence of fluent sentences, the data will belabelled at the sentence l vel, but if a long pause oc-curs in the middle of a sentence, that sentence willbe labelled as two separate segments.
Labelling ofthe speech corpus is proceeding in parallel with thedevelopment of the baseline speaker- independentARM system through a process of "forced recog-nition".
New reports are labelled by the ARMrecognition software using the current best speaker-independent models in conjunction with a report-specific syntax which allows non-speech models tooccur between words but ensures recognition of thecorrect word sequence.
The results of this auto-matic labelling process are checked manually andcorrected if necessary.
Thus, reports spoken bythe first ten training speakers were labelled us-ing speaker-dependent triphone HMMs, and reportsspoken by subsequent groups of training speakerswere labelled using triphone HMMs trained on allprevious peakers.At the time of writing 360 reports from 120 speak-ers have been labelled in this way and recognitionsystems trained on 10, 20, 30, 40, 50 and 60 malespeakers have been evaluated.
For each speaker inthe training set, all three ARM reports were usedas training material.Per fo rmance  o f  the  base l ine  speaker  inde-pendent  ARM sys temFigure 1 shows percentage word accuracy with nosyntax as a function of number of training speakersfor a set of ten test subjects, none of whom were inthe training set.
The training and test speakers areall male.
It is clear from the figure that there aretwo modes of performance.For the eight best speakers, recognition accuracyincreases with number of training speakers for train-ing sets with up to 40 speakers, after which it isapproximately constant.
The average word accu-racy for these 8 subjects with models trained on 60speakers is 59.2%, with individual scores rangingfrom 38.5% to 76.5%.For the remaining 2 speakers the performanceof the system is badly degraded, with an averageword accuracy of -38.6%.
No obvious reason forthis poor performance is apparent from listening tothe recordings, for example the speaking styles of399100 -~o WordAccuracy90-80-70-60-50-40-30-20-10--10  --20  --30  --40  --50  --60  --70  -I I | I I I10 20 30 40 50 60Number  o f  SpeakersFigure 1: Speaker-independent word accuracy with-out syntax as a function of number of  trainingspeakers for 10 male test speakers using HMMstrained on male speakers.these two speakers are, subjectively, no more atyp-ical than  those of the other 8 speakers.
Currentinvestigations are concentrat ing on the possibi l itythat  some components  of the ARM system are moresensitive to speaker diffences than was anticipated.Performance as a Function of Sizeof Parameter  SetDuring the development of the speaker dependentARM system several factors were varied whichchange the number of system parameters.
These in-clude the acoustic front-end parameterisation, thenumber of HMMs and their topologies, and the useof shared or state-specific covariance matrices.
Al-though the effect on performance which results froma particular change is normally attributed to itsappropriateness in terms of speech pattern mod-elling, there will also be effects due to the abilityof the training set to support dhTerent numbers ofparameters.
For example, early results showed thatfor monophone HMMs there was sufficient trainingmaterial to support state-specific covariance matri-ces and that the introduction of a shared "grand"covariance matrix resulted in poorer performance\[8\].
By contrast the introduction of triphone HMMswith state-specific covariance matrices resulted ineither a sma\]_l increase or a significant decrease inperformance, because of the large number of systemparameters, and large improvements in recognitionaccuracy were not observed until a shared covari-ance matrix was used \[9\].
In terms of number ofparameters these two cases represent extremes inthe development of the system, but the results sug-gest that it would be fruitful to look at the perfor-mance of a range of versions of the system from theperspective of number of parameters.Figure 2 shows %word accuracy with no syntax asa function of number of system parameters for 22speaker- dependent systems which were evaluatedas part of the ARM project.
To a first approxima-tion stoat1, med ium and large numbers of parame-ters correspond to monophone HMM systems withalternative acoustic front-end parameterisations \[8\],clustered triphone systems \[I0\], and triphone sys-teins with state-specific covariance matrices \[9\] re-spectively.
No distinction has been made betweenmeans, variances and transition probabilities in thecalculation of parameter set size.
The figure clearlysuggests an underlying effect of parameter set size,with poor performance resulting both from smallnumbers of parameters, which do not permit suffi-ciently accurate modelling of the speech patterns,and large numbers of parameters which cannot besupported by the training set.
The figure indicatesthat an acceptable balance between detailed mod-elling and trainability is achieved with sets of be-tween 20,000 and I00,000 parameters.400I00 100%WordAccuracy90?O0  80 ?70 -%60-50-40-30-20-I0 -0 i0 40?
O0I I I I I80 120 160 200 240lO00s o f  ParametersFigure 2: Performance of various versions of  theARM system for speaker SJ  as a function of  size ofparameter set.Fur ther  Exper iments  w i th  C lus tered  Tr i -phonesThe results of the previous ection suggest a mod-ification to the triphone clustering experiments de-scribed in \[10\], which demonstrate hat the numberof triphone HMMs in the ARM system can be re-duced from 1500 to 300 by clustering with no signif-icant drop in performance.
The HMM sets in theseexperiments have a single shared covariance matrix.If state-specific covariance matrices had been used,the number of parameters for sets of 280 and 480 tri-phones would have been 47,317 and 79,917 respec-tively.
According to the previous ection, these sizesof parameter set can be supported by the trainingdata.
Hence one would predict that improved per-formance would result from the use of state-specificcovariance matrices for sets of 280 and 480 triphoneHMMs.The dotted line in figure 3 is taken from \[10\]and shows %word accuracy as a function of num-ber of triphones for sets of triphones with sharedcovariance matrices.
The solid line shows new re-sults and is the corresponding graph for sets of tri-WordAccuracy50 o?
'0 ?'
' ' I ' ' ' ' I ' ' ' ' I0 500 1000 1500Number  of  Tr iphonesFigure 3: Word accuracy with no syntax as a func-tion of number of triphones averaged over 3 speak-ers.
Shared (dotted //ne) and state-specific (solidline) covariance matrices.phones with state-specific covariance matrices.
Aspredicted by figure 2, the overall best performanceis obtained from sets of 280 and 480 triphones withstate-specific covariance matrices.
The poorer per-formance obtained with sets of 80, 680 and 880triphones with state specific covariance matrices isalso consistent with the results shown in figure 2.The fact that the superior performance of the setsof 280 and 480 triphone HMMs with state specificcovariance matrices was predicted from figure 2 con-firms that an understanding of the size of parameterset which can be supported by a given training setis important in the design of this type of system.AcknowledgementThe author wishes to acknowledge the contributionsto the ARM project which have been made by allmembers of staff at the Speech Research Unit.References\[1\] J S Bridle, M D Brown and R M Chamber-lain, "A one-pass algorithm for connected word401recognition", IEEE-ICASSP, 899-902, 1982.
[2] S J Cox and J S Bridle, "Unsupervisedspeaker adaptation by probabilistic spectrumfittlng',ICASSP 89, Glasgow, Scotland, 1989.
[3] K-F Lee, "Large vocab-ulary speaker-independent continuous peechrecognition: the SPHINX system", PhD The-sis, Carnegie Mellon University, 1988.
[4] D B Paul, "A speaker-stress resistant iso-lated word recognizer", ICASSP'87, Dallas,TX, 1987.
[5] D B Paul, "The Lincoln robust continu-ous speech recognizer", ICASSP 89, Glasgow,Scotland, 1989.
[6] S M Peeling and K M Ponting, "Further experi-ments in variable frame rate analysis for speechrecognition", RSRE memorandum 4336, 1989.
[7] K M Ponting and S M Peeling, "Experimentsin variable frame rate analysis for speech recog-nition', RSRE memorandum 4330, 1989.
[8] M J Russell, D Lowe, M D Bedworth and KM Ponting, "Improved Front-End Analysis inthe ARM System: Linear Transformations ofSKUbank", RSRE memorandum 4358, Febru-ary 1990.
[9] M J Russell and K M Ponting, "Experimentswith Grand Variance in the ARM ContinuousSpeech Recognition System", RSttE memoran-dum 4359, February 1990.
[10] M J Russell, K M Ponting, S tt Browning, SDowney and P Howell, "Triphone Clustering inthe ARM System", ItSitE memorandum 4357,February 1990.
[11] M J Russell, K M Ponting, S M Peeling,S It Browning, J S Bridle, It K Moore, IGaliano and P Howell, "The ARM Continu-ous Speech Recognition System", ICASSP'90,Albuquerque, New Mexico, April 1990.
[12] It M Schwartz, Y L Chow, O A Kim-ball, S Roucos, M Krasner and J Makhoul,"Context-Dependent Modelling for acoustic-phonetic recognition of continuous peech",ICASSP 85, Tampa, April 1985.402
