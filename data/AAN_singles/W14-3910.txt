Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 87?93,October 25, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsLanguage Identification in Code-Switching ScenarioNaman JainLTRC, IIIT-H, Hyderabad, Indianaman.jain@research.iiit.ac.inRiyaz Ahmad BhatLTRC, IIIT-H, Hyderabad, Indiariyaz.bhat@research.iiit.ac.inAbstractThis paper describes a CRF based tokenlevel language identification system en-try to Language Identification in Code-Switched (CS) Data task of CodeSwitch2014.
Our system hinges on using con-ditional posterior probabilities for the in-dividual codes (words) in code-switcheddata to solve the language identificationtask.
We also experiment with other lin-guistically motivated language specific aswell as generic features to train the CRFbased sequence labeling algorithm achiev-ing reasonable results.1 IntroductionThis paper describes our participation in the Lan-guage Identification in Code-Switched Data taskat CodeSwitch 2014 (Solorio et al., 2014).
Theworkshop focuses on NLP approaches for theanalysis and processing of mixed-language datawith a focus on intra sentential code-switching,while the shared task focuses on the identifica-tion of the language of each word in a code-switched data, which is a prerequisite for ana-lyzing/processing such data.
Code-switching is asociolinguistics phenomenon, where multilingualspeakers switch back and forth between two ormore common languages or language-varieties,in the context of a single written or spokenconversation.
Natural language analysis of code-switched (henceforth CS) data for various NLPtasks like Parsing, Machine Translation (MT), Au-tomatic Speech Recognition (ASR), InformationRetrieval (IR) and Extraction (IE) and SemanticProcessing, is more complex than monolingualdata.
Traditional NLP techniques perform miser-ably when processing mixed language data.
Theperformance degrades at a rate proportional to theamount and level of code-switching present in thedata.
Therefore, in order to process such data,a separate language identification component isneeded, to first identify the language of individualwords.Language identification in code-switched datacan be thought of as a sub-task of a documentlevel language identification task.
The latter aimsto identify the language a given document is writ-ten in (Baldwin and Lui, 2010), while the formeraddresses the same problem, however at the tokenlevel.
Although, both the problems have separategoals, they can fundamentally be modeled with asimilar set of features and techniques.
However,language identification at the word level is morechallenging than a typical document level lan-guage identification problem.
The number of fea-tures available at document level is much higherthan at word level.
The available features for wordlevel identification are word morphology, syllablestructure and phonemic (letter) inventory of thelanguage(s).
Since these features are related to thestructure of a word, letter based n-gram modelshave been reported to give reasonably accurate andcomparable results (Dunning, 1994; Elfardy andDiab, 2012; King and Abney, 2013; Nguyen andDogruoz, 2014; Lui et al., 2014).
In this work, wepresent a token level language identification sys-tem which mainly hinges on the posterior prob-abilities computed using n-gram based languagemodels.The rest of the paper is organized as follows: InSection 2, we discuss about the data of the sharedtask.
In Section 3, we discuss the methodologywe adapted to address the problem of languageidentification, in detail.
Experiments based on ourmethodology are discussed in Section 4.
In Sec-tion 5, we present the results obtained, with a briefdiscussion.
Finally we conclude in Section 6 withsome future directions.872 DataThe Language Identification in the Code-Switched(CS) data shared task is meant for languageidentification in 4 language pairs (henceforthLP) namely, Nepali-English (N-E), Spanish-English (S-E), Mandarin-English (M-E) and Mod-ern Standard Arabic-Arabic dialects (MSA-A).
Soas to get familiar with the training and testing data,trial data sets consisting of 20 tweets each, corre-sponding to all the language-pairs, were first re-leased.
Additional test data as ?surprise genre?
forS-E, N-E and MSA-A were also released, whichcomprised of data from Facebook, blogs and Ara-bic commentaries.2.1 Tag DescriptionEach word in the training data is classified intoone of the 6 different classes which are, Lang1,Lang2, Mixed, Other, Ambiguous and NE.?Lang1?
and ?Lang2?
tags correspond to wordsspecific to the languages in an LP.
?Mixed?
wordsare those words that are partially in both the lan-guages.
?Ambiguous?
words are the ones thatcould belong to either of the language.
All gib-berish and unintelligible words and words thatdo not belong to any of the languages fall under?Other?
category.
?Named Entities?
(NE) com-prise of proper names that refer to people, places,organizations, locations, movie titles and song ti-tles etc.2.2 Data Format and Data CrawlingDue to Twitter policies, distributing the data di-rectly is not possible in the shared task and thus thetrial, training and testing data are provided as charoffsets with label information along with tweetID1and userID2.
We use twitter3python script to crawlthe tweets and our own python script to further to-kenize and synchronize the tags in the data.Since the data for ?surprise genre?
comes fromdifferent social media sources, the ID formatvaries from file to file but all the other details arekept as is.
In addition to the details, the tokens ref-erenced by the offsets are provided unlike Twitterdata.
(1) and (2) below, show the format of tweetsin train and test data respectively, while (3) showsa typical tweet in the surprise genre data.1Each tweet on Twitter has a unique tweetID2Each user on Twitter carries a userID3http://emnlp2014.org/workshops/CodeSwitch/scripts/twitter.zip(1) TweetID UserID startIndex endIndex Tag(2) TweetID UserID startIndex endIndex(3) SocialMediaID UserID startIndex endIn-dex Word2.3 Data StatisticsThe CS data is divided into two types oftweets (henceforth posts)4namely, Code-switchedposts and Monolingual posts.
Table 1 shows theoriginal number of posts that are released for theshared task for all LPs, along with their tag counts.Due to the dynamic nature of social media, theposts can be either deleted or updated and thusdifferent participants would have crawled differentnumber of posts.
Thus, to come up with a compa-rable platform for all the teams, the intersection ofdata from all the users is used as final testing datato report the results.
Table 1 shows the number oftweets or posts in testing data that are finally usedfor the evaluation.3 MethodologyWe divided the language identification task intoa pipeline of 3 sub-tasks namely Pre-Processing,Language Modeling, and Sequence labeling usingCRF5.
The pipeline is followed for all the LPs withsome LP specific variations in selecting the mostrelevant features to boost the results.3.1 Pre-ProcessingIn the pre-processing stage, we crawl the tweetsfrom Twitter given their offsets in the training dataand then tokenize and synchronize the words withtheir tags as mentioned in Section 2.2.
For eachLP we separate out the tokens into six classes touse the data for Language Modeling and also tomanually analyze the language specific propertiesto be used as features further in sequence labeling.
While synchronizing the words in a tweet withtheir tags, we observed that some offsets do notmatch with the words and this would lead to mis-match of labels with tokens and thus degrade thequality of training data.To filter out the incorrect instances from thetraining data, we frame pattern matching ruleswhich are specific to the languages present.
Butthis filtering is done only for the words present in4In case of twitter data, we have tweets but in case of sur-prise genre data we have posts5Conditional Random Field88Language Pairs # Tweets # TokensCodeSwitched Monolingual Ambiguous Lang1 Lang2 Mixed NE OtherTrainMSA-A dialects 774 5,065 1,066 79,134 16,291 15 14,112 8,699Mandarin-English 521 478 0 12,114 2,431 12 1,847 1,025Nepali-English 7,203 2,790 126 45,483 60,697 117 3982 35,651Spanish-English 3,063 8,337 344 77,107 33,099 51 2,918 27,227TestMSA-A dialects I 32 2,300 11 44,314 141 0 5,939 3,902Mandarin-English 247 66 0 4,703 881 1 254 442Nepali-English 2,665 209 0 12,286 17,216 60 1,071 9,635Spanish-English 471 1,155 43 7,040 5,549 12 464 4,311MSA-A dialects II 293 1,484 119 10,459 14,800 2 4,321 2,940SurpriseMSA-A dialects - - 110 2,687 6,930 3 1,097 1,190Nepali-English 20 82 0 173 699 0 127 88Spanish-English 22 27 1 636 306 1 38 120Table 1: Data Statistics?Lang1?
and ?Lang2?
classes.
There are two rea-sons to consider these labels.
First, ?Lang1?
and?Lang2?
classes hold maximum share of words inany LP as shown in Table 1, and thus have a higherimpact on the overall accuracy of the languageidentification system.
In addition to the above,these categories correspond to the focus point ofthe shared task.
Second, for ?Ambiguous?, ?NE?and ?Other?
categories, it is difficult to find thepatterns according to their definitions.
Althoughrules can be framed for ?Mixed?
category, sincetheir count is too less as compared to the othercategories (Table 1), it is of no use to train a sepa-rate language model with very less number of in-stances.For Mandarin and Arabic data sets, any wordpresent in Roman script is excluded from the data.Similarly for English and Nepali, if any word con-tains characters other than Roman or numeral theyare excluded from the data.
In addition to therule for English and Nepali, the additional alpha-bets in Spanish are also included in the set of Ro-man and numeral entries.
Table 2 shows the num-ber of words that remained in each of the lan-guages/dialects, after the preprocessing.One of the bonus points in the shared task isthat 3 out of 4 LPs share ?English?
as their sec-ond language.
In order to increase the training sizefor English, we merged all the English words intoa single file and thus reduced the number of lan-guage models to be trained from 8 to 6, one foreach language (or dialect).Language Data Size Average Token LengthArabic 10,380 8.14English 105,014 3.83Mandarin 12,874 4.99MSA 53,953 8.93Nepali 35,620 4.26Spanish 32,737 3.96Table 2: Data Statistics after Filtering3.2 Language ModelingIn this stage, we train separate smoothed n-grambased language models for each language in an LP.We compute the conditional probability for eachword using these language models, which is thenused as a feature, among others for sequence la-beling to finally predict the tags.3.2.1 N-gram Language ModelsGiven a word w, we compute the conditional prob-ability corresponding to k6classes c1, c2, ... , ckas:p(ci|w) = p(w|ci) ?
p(ci) (1)The prior distribution p(c) of a class is es-timated from the respective training sets shownin Table 2.
Each training set is used to train aseparate letter-based language model to estimatethe probability of word w. The language modelp(w) is implemented as an n-gram model usingthe IRSTLM-Toolkit (Federico et al., 2008) withKneser-Ney smoothing.
The language model is6In our case value of k is 2 as there are 2 languages in anLP89defined as:p(w) =n?i=1p(li|li?1i?k) (2)where l is a letter and k is a parameter indicatingthe amount of context used (e.g., k=4 means 5-gram model).3.3 CRF based Sequence LabelingAfter Language Modeling, we use CRF-based(Conditional Random Fields (Lafferty et al.,2001)) sequence labeling to predict the labels ofwords in their surrounding context.
The CRF algo-rithm predicts the class of a word in its surround-ing context taking into account other features notexplicitly represented in its structure.3.3.1 Feature SetIn order to train CRF models, we define a featureset which is a hybrid combination of three sub-types of features namely, Language Model Fea-tures (LMF), Language Specific Features (LSF)and Morphological Features (MF).LMF: This sub-feature set consists of poste-rior probability scores calculated using languagemodels for each language in an LP.
Althoughwe trained language models only for ?Lang1?and ?Lang2?
classes, we computed the probabil-ity scores for all the words belonging to any of thecategories.LSF: Each language carries some specific traitsthat could assist in language identification.
Inthis sub-feature set we exploited some of the lan-guage specific features exclusively based on thedescription of the tags provided.
The common fea-tures for all the LPs are HAS NUM (Numeral ispresent in the word), HAS PUNC (Punctuation ispresent in the word), IS NUM (Word is a numeral),IS PUNC (word is a punctuation or a collection ofpunctuations), STARTS NUM (word starts with anumeral) and STARTS PUNC (word starts with apunctuation).
All these features are used to gener-ate variations to distinguish ?Other?
class from restof the classes during prediction.Two features exclusively used for the Englishsharing LPs are HAS CAPITAL (capital letters arepresent in the word) and IS ENGLISH (word be-longs to English or not).
HAS CAPITAL is usedto capture the capitalization property of the En-glish writing system.
This feature is expected tohelp in the identification of ?NEs?.
IS ENGLISH isused to indicate whether a word is an valid Englishword or not, based on its presence in English dic-tionaries.
We used dictionaries available in PyEn-chant7.For the M-E LP, we are using ?TYPE?8as afeature with possible values as ENGLISH, MAN-DARIN, NUM, PUNC and OTHER.
If all thecharacters in the word are English alphabets EN-GLISH is taken as the value and Mandarin oth-erwise.
Similar checks are used for NUM andPUNC types.
But if no case is satisfied, OTHERis taken as the value.We observed that the above features did not con-tribute much to distinguish between any of the tagsin case of the MSA-A LP.
Since this pair consistsof two different dialects of a language rather thantwo different languages, the posterior probabilitieswould be close to each other as compared to otherLPs.
Thus we use the difference of these probabil-ities as a feature in order to discriminate ambigu-ous words or NEs that are spelled similarly.MF: This sub-feature set comprises of the mor-phological features corresponding to a word.
Weautomatically extracted these features using apython script.
The first feature of this set is a bi-nary length variable (MORE/LESS) depending onthe length of the word with threshold value 4.
Theother 8 features capture the prefix and suffix prop-erties of a word, 4 for each type.
In prefix type,4, 3, 2 and 1 characters, if present, are taken fromthe beginning of a word as 4 features.
Similarlyfor the suffix type, 1, 2, 3 and 4 characters, againif present, are taken from the end of a word as 4features.
In both the cases if any value is miss-ing, it is kept as NULL (LL).
(4) below, showsa typical example from English data with the MFsub-feature set for the word ?one?, where F1 rep-resents the value of binary length variable, F2-F5and F6-F9 represent the prefix and suffix featuresrespectively.
(4) oneWordLessF1LLF2oneF3onF4oF5LLF6oneF7neF8eF93.3.2 Context WindowAlong with the above mentioned features, wechose an optimal context template to train the CRF7PyEnchant is a spell checking library in Python(http://pythonhosted.org/pyenchant/)8Since it captures the properties of IS NUM andIS PUNC, these features are not used again90models.
We selected the window size to be 5, with2 words before and after the target word.
Furnish-ing the training, testing and surprise genre datawith the features discussed in 3.3.1, we trained 4CRF models on training data using feature tem-plates based on the context decided.
These mod-els are used to finally predict the tags on the testingand surprise genre data.4 ExperimentsThe pipeline mentioned in Section 3 was used forthe language identification task for all the LPs.We carried out a series of experiments with pre-processing to clean the training data and also tosynchronize the testing data.
We also did somepost-processing to handle language and tag spe-cific cases.In order to generate language model scores,we trained 6 language models (one for each lan-guage/dialect) on the filtered-out training data asmentioned in Table 2.
We experimented with dif-ferent values of n-gram to select the optimal valuebased on the F1-measure.
Table 3 shows the opti-mal order of n-gram, selected corresponding to thehighest value of F1-score.
Using the optimal valueof n-gram, language models have been trained andthen posterior probabilities have been calculatedusing equation (1).Finally, we trained separate CRF models foreach LP, using the CRF++9tool kit based on thefeatures described in Section 3.3.1 and the featuretemplate in Section 3.3.2.
To empirically find therelevance of features we also performed leave-oneout experiments so as to decide the optimal fea-tures for the language identification task (more de-tails in Section 4.1).
Then, using these CRF mod-els, tags were predicted on the testing and surprisegenre datasets.Language-Pair N-gramMSA-A 5M-E 5N-E 6S-E 5Table 3: Optimal Value of N-gram4.1 Feature RankingWe expect that some features would be more im-portant than others and would impact the task9http://crfpp.googlecode.com/svn/trunk/doc/index.html?source=navbarof language identification irrespective of the lan-guage pair.
In order to identify such optimal fea-tures for the language identification task, we rankthem based on their information gain scores.4.1.1 Information GainWe used information gain to score features ac-cording to their expected usefulness for the task athand.
Information gain is an information theoreticconcept that measures the amount of knowledgethat is gained about a given class by having accessto a particular feature.
If f is the occurrence anindividual feature and?f the non-occurrence of afeature, information gain can be measured by thefollowing formula:G(x) = P (f)?P (y|f)logP (y|f)+ P (?f)?logP (y|?f)logP (y|?f)(3)For each language pair, the importance of fea-ture types are represented by the following order:?
MSA-A dialects: token > word morphology> posterior probabilities > others?
Mandarin-English: token > posterior prob-abilities > word morphology > languagetype > others?
Nepali-English: token > posterior probabil-ities > word morphology > dictionary > oth-ers?
Spanish-English: token > posterior proba-bilities > word morphology > others > dic-tionaryApart from MSA-A dialects, top 3 features sug-gested by information gain are token and its sur-rounding context, posterior probabilities and wordmorphology.
For Arabic dialects word morphol-ogy is more important than posterior probabilities.It could be due to the fact that Arabic dialects sharea similar phonetic inventory and thus have similarposterior probabilities.
However, they differ sig-nificantly in their morphological structure (Zaidanand Callison-Burch, 2013).We also carried out leave-one-out experimentsover all the features to ascertain their impact on theclassification performance.
The results of theseexperiments are shown in Table (5).
Accuraciesare averaged over 5-fold cross-validation.91Token LevelLanguage Pairs Ambiguous Lang1 Lang2 Mixed NE OtherR P F1 R P F1 R P F1 R P F1 R P F1 R P F1 Overall AccuracyTestMSA-A I 0.00 0.00 0.00 0.92 0.95 0.94 0.40 0.03 0.06 - - - 0.70 0.77 0.73 0.90 0.85 0.87 0.90M-E - - - 0.98 0.98 0.98 0.67 0.66 0.67 0.00 1.00 0.00 0.84 0.38 0.53 0.22 0.71 0.33 0.88N-E - - - 0.95 0.93 0.94 0.98 0.96 0.97 0.00 1.00 0.00 0.39 0.79 0.52 0.94 0.96 0.95 0.95S-E 0.00 1.00 0.00 0.88 0.81 0.84 0.83 0.90 0.86 0.00 1.00 0.00 0.16 0.40 0.23 0.83 0.80 0.82 0.83MSA-A II 0.00 0.00 0.00 0.91 0.47 0.62 0.36 0.84 0.51 0.00 1.00 0.00 0.59 0.80 0.68 0.80 0.71 0.75 0.60SurpriseMSA-A 0.00 0.00 0.00 0.94 0.38 0.54 0.46 0.93 0.61 0.00 1.00 0.00 0.52 0.78 0.62 0.96 0.96 0.96 0.62N-E - - - 0.92 0.76 0.84 0.95 0.89 0.91 - - - 0.35 0.92 0.50 0.85 0.89 0.87 0.86S-E 0.00 1.00 0.00 0.86 0.81 0.83 0.82 0.87 0.85 0.00 1.00 0.00 0.15 0.40 0.22 0.82 0.78 0.80 0.94Table 4: Token Level ResultsLeft Out Feature MSA-A M-E N-E S-EContext 76.32 94.07 93.97 92.30Morphology 79.29 93.67 93.98 93.51Probability 79.24 89.16 93.86 93.28Dictionary - 87.75 93.73 92.99Language Type - 87.97 - -Others 78.80 83.84 92.10 92.20All Features 79.37 95.11 94.52 93.54Table 5: Leave-one-out Experiments5 Results and DiscussionEach language identification system is evaluatedagainst two data tracks namely, ?Testing?
and ?Sur-prise Genre?
data as mentioned in Section 2.
Sur-prise genre data of Mandarin-English LP was notprovided, so no results are available.
All the resultsare provided on two levels, comment/post/tweetand token level.
Tables 4 and 6 show results of ourlanguage identification system on both the levelsrespectively.In case of Tweets, systems are evaluated usingthe following measures: Accuracy, Recall, Preci-sion and F-Score.
However at token level, sys-tems are evaluated separately for each tag in anLP using Recall, Precision and F1-Score as themeasures.
Table 4 shows that the results for ?Am-biguous?
and ?Mixed?
categories are either miss-ing (due to absence of tokens in that category), orhave 0.00 F1-Score.
One obvious reason could bethe sparsity of data for these categories.6 Conclusion and Future WorkIn this paper, we have described a CRF based to-ken level language identification system that uses aset of naive easily computable features guarantee-ing reasonable accuracies over multiple languagepairs.
Our analysis showed that the most importantLanguage Pairs Tweet LevelAccuracy Recall Precision F-scoreTestMSA-A I 0.605 0.719 0.025 0.048M-E 0.751 0.814 0.863 0.838N-E 0.948 0.979 0.966 0.972S-E 0.835 0.773 0.692 0.730MSA-A II 0.469 0.823 0.213 0.338SurpriseMSA-A 0.457 0.833 0.128 0.222N-E 0.735 0.900 0.419 0.571S-E 0.830 0.765 0.689 0.725Table 6: Comment/Post/Tweet Level Resultsfeature is the word structure which in our systemis captured by n-gram posterior probabilities andword morphology.
Our analysis of Arabic dialectsshows that word morphology plays an importantrole in the identification of mixed codes of closelyrelated languages.7 AcknowledgementWe would like to thank Himani Chaudhry for hervaluable comments and suggestions that helped usto improve the quality of the paper.ReferencesTimothy Baldwin and Marco Lui.
2010.
Languageidentification: The long and the short of the mat-ter.
In Human Language Technologies: The 2010Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 229?237.
Association for Computational Lin-guistics.Ted Dunning.
1994.
Statistical identification of lan-guage.
Computing Research Laboratory, New Mex-ico State University.Heba Elfardy and Mona T Diab.
2012.
Token levelidentification of linguistic code switching.
In COL-ING (Posters), pages 287?296.Marcello Federico, Nicola Bertoldi, and Mauro Cet-tolo.
2008.
Irstlm: an open source toolkit for han-92dling large scale language models.
In Interspeech,pages 1618?1621.Ben King and Steven P Abney.
2013.
Labeling thelanguages of words in mixed-language documentsusing weakly supervised methods.
In HLT-NAACL,pages 1110?1119.John Lafferty, Andrew McCallum, and Fernando CNPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Proceedings of the InternationalConference on Machine Learning, pages 282?289.Marco Lui, Jey Han Lau, and Timothy Baldwin.
2014.Automatic detection and language identification ofmultilingual documents.
volume 2, pages 27?40.Dong Nguyen and A Seza Dogruoz.
2014.
Wordlevel language identification in online multilingualcommunication.
In Proceedings of the 2013 Con-ference on Empirical Methods in Natural LanguageProcessing.Thamar Solorio, Elizabeth Blair, Suraj Maharjan, SteveBethard, Mona Diab, Mahmoud Gonheim, AbdelatiHawwari, Fahad AlGhamdi, Julia Hirshberg, AlisonChang, and Pascale Fung.
2014.
Overview for thefirst shared task on language identification in code-switched data.
In Proceedings of the First Workshopon Computational Approaches to Code-Switching.EMNLP 2014, Conference on Empirical Methods inNatural Language Processing, Octobe, 2014, Doha,Qatar.Omar F Zaidan and Chris Callison-Burch.
2013.
Ara-bic dialect identification.
Computational Linguis-tics, 40(1):171?202.93
