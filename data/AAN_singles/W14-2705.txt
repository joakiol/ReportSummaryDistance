Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 33?41,Baltimore, Maryland USA, 27 June 2014.c?2014 Association for Computational LinguisticsA Unified Topic-Style Model for Online DiscussionsYing Ding, Jing Jiang, Qiming DiaoSchool of Information SystemsSingapore Management University{ying.ding.2011, jingjiang, qiming.diao.2010}@smu.edu.sgAbstractForums have become major places foronline communications for many years,where people often share and expressopinions.
We observe that, when editingposts, while some people seriously statetheir opinions, there are also many peo-ple playing jokes and writing meaninglessposts on the discussed topics.
We designa unified probabilistic graphical model tocapture both topic-driven words and style-driven words.
The model can help us sepa-rate serious and unserious posts/users andidentify slang words.
An extensive setof experiments demonstrates the effective-ness of our model.1 IntroductionWith the fast growth of the popularity of onlinesocial media, people nowadays are very used tosharing their thoughts and interacting with theirfriends on the Internet.
Large online social net-work sites such as Facebook, Twitter and Flickrhave attracted hundreds of millions of users.
A-mong these online social media platforms, forumshave always played an important role with its spe-cial characteristics.
Unlike personal blogs, forum-s allow many users to engage in online conversa-tions with a topic focus.
Unlike Facebook, forumsare usually open to public and users who post inforums do not need to reveal too much personalinformation.
Unlike Wikipedia or Freebase, fo-rums encourage users to exchange not only factualinformation but more importantly subjective opin-ions.
All these characteristics make online forumsa valuable source from which we can retrieve andsummarize the general public?s opinions about agiven topic.
This is especially important for busi-nesses who want to find out how their productsand services have been received and policy mak-ers who are concerned about people?s opinions onsocial issues.While the freedom with which users can post inonline forums has promoted the popularity of on-line forums, it has also led to the diversity in postquality.
There are posts which contribute positive-ly to a discussion by offering relevant, serious andmeaningful opinions, but there are also many postswhich appear irrelevant, disrespectful or meaning-less.
These posts are uninformative, hard to con-sume and sometimes even destructive.
Let us lookat some examples.
Table 1 shows two forum postsin response to a piece of news about GDP bonusesfor senior civil servants in Singapore.
We can seethat User A?s post is clearly written.
User B?s post,on the other hand, is hard to comprehend.
We seebroken sentences, many punctuation marks suchas ???
and colloquial expressions such as ?ha.
?User B is not seriously contributing to the onlinediscussion but rather trying to make a joke of theissue.
Generally speaking, User B?s post is lessuseful than User A?s post in helping us understandthe public?s response to the news.Senior civil servants to get bumperGDP bonusesUser A let us ensure this will be the LAST timethey accord themselves ceiling salary s-cales and bonuses.
i suspect MANY cit-izens are eagerly looking forward to theGE.User B Fever night, fever night, fe..ver..Fever like to do itGot it??????
Ha..ha..ha...Table 1: Two example online posts.In this work, we opt for a fully unsupervisedapproach to modeling this phenomenon in onlinediscussions.
Our solution is based on the observa-tion that the writing styles of serious posts and un-serious posts are different, and the writing stylesare often characterized by the words used in theposts.
Moreover, the same user usually exhibits33User PostUser ARe: Creativity, Art in the eyes of beholder.
yourtake?The difference is, the human can get tired orsick, and then it will affect his work, but therobot can work 24 hours a day 365 days a yearand yet produce the same every time.Re: Diesel oil spill turns Manila Bay red, posesrisk to health - STThe question is, will this environmental haz-ard turn up on the shores of it neighbors?
Andmaybe even affect Singapore waters?User BRe: Will PAP know who i vote in GE?Hey!
Who are you??
?You make.
ha..ha..ha.. he..he..he..very angry lahRe: Gender discrimination must end for Singa-pore to flourish, says AWAREHao nan bu gen nu dou Let you win lahha..ha..ha..Table 2: Sample posts of two example users.the same writing style in most of his posts.
Forexample, Table 2 shows two example users, eachwith two sample posts.
We can see that their writ-ing styles are consistent in the two posts.
If wetreat each writing style as a latent factor associat-ed with a word distribution, we can associate ob-served words with the underlying writing styles.However, not all words in a post are style-driven.Many words in forum posts are chosen based onthe topic of the corresponding thread.
Our modeltherefore jointly considers both topics and writingstyles.We apply our topic-style model to a real on-line forum dataset from Singapore.
By setting thenumber of styles to two, we clearly find that onewriting style corresponds to the more serious postswhile the other corresponds to posts that are not soserious.
This topic-style model also automaticallylearns a meaningful slang lexicon.
Moreover, wefind that topics discovered by our topic-style mod-el are more distinctive from each other than topicsproduced by standard LDA.Our contributions in this paper can be summa-rized as follows: 1) We propose a principled topic-style model to jointly model topics and writingstyles at the same time in online forums.
2) Anextensive set of experiments shows that our mod-el is effective in separating the more serious postsand unserious posts and identifying slang words.2 Related WorkLatent Dirichlet Allocation (LDA) (Blei et al.,2003) has been shown to be useful for many ap-plications.
Many extensions of LDA have beendesigned for different tasks, which are not detailedhere.
Our model is also an extension of LDA.
Weintroduce two types of word distributions, one rep-resenting topics and the other representing writingstyles.
We use switch variables to alternate be-tween these two types of word distributions.
Wealso assume an author-level distribution over writ-ing styles.
It is worth pointing out that althoughour model bears similarity to a number of oth-er LDA extensions, our objectives are differentfrom existing work.
E.g., the author topic mod-el (Rosen-Zvi et al., 2004) also assumes an author-level distribution over topics, but the author-leveldistribution is meant to capture an author?s topicalinterests.
In contrast, our user-level distribution isover writing styles and is meant to identify seriousversus unserious users.
Similar to the models byMei et al.
(2007) and Paul et al.
(2010) , we alsouse switch variables to alternate between differenttypes of word distributions, but our goal is to iden-tify words associated with writing styles instead ofsentiment words or perspective words.Another body of related research is around s-tudying text quality, formality and sarcasm.
Pitlerand Nenkova (2008) investigated different fea-tures for text readability judgement and empirical-ly demonstrated that discourse relations are high-ly correlated with perceived readability.
Brookeet al.
(2010) applied Latent Semantic Analysisto determine the formality level of lexical items.Agichtein et al.
(2008) presented a general classifi-cation framework incorporating community feed-back to identify high quality content in social me-dia.
Davidov et al.
(2010) proposed the first robustalgorithm for recognition of sarcasm.
Gonz?alez-Ib?a?nez et al.
(2011) took a closer look at sarcasmin Twitter messages and found that automatic clas-sification can be as good as human classification.All these studies mainly rely on supervised tech-niques and human annotation needs to be done,which is very time consuming.
Our method is ful-ly unsupervised, which can automatically uncoverdifferent styles and separate serious posts from un-serious posts.Our work is also related to spam/spammer de-tection in social media, which has been studiedover different platforms for a few years.
Jindaland Liu (2008) first studied opinion spam in on-line reviews and proposed a classification methodfor opinion spam detection.
Bhattarai et al.
(2009)34investigated different content attributes of com-ment spam in the Blogsphere and built a detectionsystem with good performance based on these at-tributes.
Ding et al.
(2013) proposed to utilize bothcontent and social features to detect spams in on-line question answer website.
Existing work onspam detection need annotated data to learn the s-pam features but our model does not as it is fullyunsupervised.3 A Topic-Style ModelWriting styles can be reflected in many differen-t ways.
Besides choices of words or expression-s, many other linguistic features such as sentencelength, sentence complexity and use of punctua-tion marks may all be associated with one?s writ-ing style.
In this work, however, we try to takean approach that does not rely on heavy linguisticanalysis or feature engineering.
Part of the reasonis that we want our approach to be independent oflanguage, culture or social norms so that it is ro-bust and can be easily applied to any online forum.To this end, we represent a writing style simplyas a distribution over words, much like a topic inLDA.
We assume that there are S latent writingstyles shared by all users contributing to a forum.Meanwhile, we also assume a different set of Tlatent topics.
We mix writing styles and topics toexplain the generation of words in forum posts.A key assumption we have is that the same us-er tends to maintain a consistent writing style, andtherefore we associate each user with a multinomi-al distribution over our latent writing styles.
Thisis similar to associating a document with a distri-bution over topics in LDA, where the assumptionis that a single document tends to have focusedtopics.
Another assumption of our model is thateach word in a post is generated from either thebackground or a topic or a writing style, as deter-mined by a binary switch variable.3.1 Model DescriptionWe now formally describe the topic-style modelwe propose.
The model is depicted in Figure 1.We assume that there are T latent topics, where?tis the word distribution for topic t. There areS latent writing styles, where ?sis the word dis-tribution for writing style s. There are E threads,where each thread e has a topic distribution ?e, andthere are U users, where each user u has a writingstyle distribution piu.Figure 1: Topic-Style ModelNotation Description?, ?E, ?U,?B, ?T, ?SHyper-parameters of Dirichlet distributions?
A global multinomial distribution overswitching variables x?e, piuThread-specific topic distributions and user-specific style distributions?B, ?t, ?sWord distributions of background, topicsand stylesxe,p,n,ye,p,n,ze,p,nHidden variables: xe,p,nfor switching,ye,p,nfor style of style words, ze,p,nfortopic of topic wordse, p, n Indices: e for threads, p for posts, n forwordsE,Pe, U,Ne,pNumber of threads, numbers of posts inthreads, number of users and numbers ofwords in postsS,K, V Numbers of styles, topics and word typesTable 3: Notation used in our model.For each word in a post, first a binary switchvariable x is sampled from a global Bernoulli dis-tribution parameterized by ?.
If x = 0, we draw aword from the background word distribution.
Oth-erwise, if x = 1, we draw a topic from the corre-sponding thread?s topic distribution; if x = 2, wedraw a writing style from the corresponding user?swriting style distribution.
We then draw the wordfrom the corresponding word distribution.The generative process of our model is de-scribed as follows.
The notation we use in themodel is also summarized in Table 3.?
Draw a global multinomial switching variable distribu-tion ?
?
Dirichlet(?).?
Draw a multinomial background word distribution?B?
Dirichlet(?B).?
For each topic t = 1, 2, .
.
.
, T , draw a multinomialtopic-word distribution ?t?
Dirichlet(?T).?
For each writing style s = 1, 2, .
.
.
, S, draw a multi-nomial style-word distribution ?s?
Dirichlet(?S).?
For each user u = 1, 2, .
.
.
, U , draw a multinomialstyle distribution piu?
Dirichlet(?u).?
For each thread e = 1, 2, .
.
.
, E35?
draw a multinomial topic distribution ?e?Dir(?E).?
for each post p = 1, 2, .
.
.
, Pein the thread,where ue,p?
{1, 2, .
.
.
, U} is the user who haswritten the post?
for each word n = 1, 2, .
.
.
, Ne,pin thethread, where we,p,n?
{1, 2, .
.
.
, V } is theword type?
draw xe,p,n?
Multinomial(?).?
If x = 0, draw we,p,n?Multinomial(?B)?
If x = 1, draw ye,p,n?Multinomial(piue,p), and then drawwe,p,n?
Multinomial(?ye,p,n).?
If x = 2, draw ze,p,n?Multinomial(?e), and then drawwe,p,n?
Multinomial(?ze,p,n).3.2 Parameters EstimationWe use Gibbs sampling to estimate the parameters.The sampling probability that assign the nth wordin post p of thread e to the background topic is asfollows:P (xe,p,n= 0|W ,U ,X?i,Y ?i,Z?i)?(?
+ n0)?
?B+ nwe,p,nBV ?B+ n0where n0is the number of words assigned as back-ground words and nwe,p,nBis the number of timesword type of we,p,nassigned to background.
Theprobability to assign this word to style s is as fol-lows:P (xe,p,n= 1, ye,p,n= s|W ,U ,X?i,Y ?i,Z?i)?(?
+ n1)?
?U+ nsue,pS?U+ n?ue,p?
?S+ nwe,p,nsV ?S+ n?swhere n1is the number of words assigned as stylewords, n?ue,pand nsue,pare the number of wordswritten by user ue,pand assigned as style words,and the number of these words assigned to styles, respectively.
n?sand nwe,p,nsare the number ofwords assigned to style s and the number of timesword type of term we,p,nassigned to style s. Theprobability to assign this word topic t is as follows:P (xe,p,n= 2, ze,p,n= t|W ,U ,X?i,Y ?i,Z?i)?(?
+ n2)?
?E+ nteK?E+ n?e?
?T+ nwe,p,ntV ?T+ n?twhere n2is the number of words assigned as topicwords, n?eis the number of words in thread e as-signed as topic words, nteis the number of wordsin thread e assigned to topic t, n?tis the number ofwords assigned to topic t, and nwe,p,ntis the num-ber of times word type of we,p,nis assigned to top-ic t.After running Gibbs sampling for a number ofiterations, we can estimate the parameters basedon the sampled topic assignments.
They can becalculated by the equations below:?wt=?T+ nwtV ?T+ n?t?ws=?S+ nwsV ?S+ n?s?te=?E+ nteK?E+ n?e?su=?U+ nsuS?U+ n?u4 Experiment4.1 Data Set and Experiment SetupTo evaluate our model, we use forum threads fromAsiaOne1, a popular online forum site in Singa-pore.
We crawled all the threads between January2011 and June 2013 under a category called ?Sin-gapore,?
which is the largest category on AsiaOne.In the preprocessing stage, we removed the URL-s, HTML tags and tokenized the text.
Emoticonsare kept in our data set as they frequently occurand indicate users?
emotions.
All stop words andwords occurring less than 4 times are deleted.
Wealso removed users who have fewer than 8 post-s and threads attracting fewer than 21 posts.
Thedetailed statistics of the processed dataset are giv-en in Table 4.#Users #Words #Tokens #Posts/User #Posts/Thread580 29,619 2,940,886 205.3 69.5Table 4: Detailed statistics of the dataset.We fix the hyper-parameters ?, ?E, ?U, ?Tand?Sto be 10, 1, 1, 0.01 and 0.01 respectively.
weset ?B,vto be H ?
pB(v), where H is set to be 20and pB(v) is the probability of word v as estimatedfrom the entire corpus.
The number of topics K isset to be 40 empirically.4.2 Model DevelopmentBefore we evaluate the effectiveness of our model,we first show how we choose the number of stylesto use.
Note that although we are interested in sep-arating serious and unserious posts, our model cangenerally handle any arbitrary number of writingstyles.
We therefore vary the number of writingstyles to see which number empirically gives themost meaningful results.Assuming that different styles are characterizedby words, we expect to see that the discovered1http://www.asiaone.com362 3 4 5 6 7 8 91213141516171819Number of StylesAverageDivergenceFigure 2: Average Divergence over different num-bers of styles.Style No.
Top Words2 Style 1 singapore, people, years, governmentstyles Style 2 BIGGRIN, TONGUE, lah, ha3 Style 1 people, make, WINK, goodstyles Style 2 singapore, years, government, mrStyle 3 BIGGRIN, TONGUE, lah, ha4 Style 1 ha, lah, WINK, dontstyles Style 2 singapore, year, mr, yearsStyle 3 people, good, make, singaporeStyle 4 BIGGRIN, TONGUE, EEK, MADTable 5: Sample style wordsword distributions for different styles are very d-ifferent from each other.
To measure the distinc-tion among a set of styles, we define a metriccalled Average Divergence (AD) based on KL-divergence.
Average Divergence can be calculatedas follows.AD(S) =2N(N ?
1)?i6=jSKL(si||sj),where S is a set of style-word distributions, N isthe size of S and siis the i-th distribution in S.SKL(si||sj) is the symmetric KL divergence be-tween siand sj(i.e., DKL(si||sj) +DKL(sj||si)).The higher Average Divergence is, the more dis-tinctive distributions in S are.Figure 2 shows the Average Divergence over d-ifferent numbers of styles.
We can clearly see thatthe Average Divergence reaches the highest valuewhen there are only two styles and decreases withthe increase of style number.
This means the stylesare mostly distinct from each other when the num-ber is 2 and their difference decreases when thereare more styles.To get a better understanding of the differencesof using different numbers of styles, we comparethe top words in each style when the number ofstyles is set to be 2, 3 and 4.
The results are shownin Table 5 where all uppercase words represent e-moticons.
From the top words of the first row, weSerious UnseriousStyle Stylesingapore lahpeople hayears dontgovernment stupidtime lehmade ahyear lorpublic liaoTable 6: Top words of different stylescan see that Style 1 is dominated by formal wordswhile Style 2 is dominated by emoticons like BIG-GRIN and slang words like ?lah?
and ?ha.?
Thesetwo styles are well distinguished from each otherand humans can easily tell the difference betweenthem.
Also, Style 2 is an unserious style character-ized by emoticons, slang and urban words.
Table 6shows the top words of these 2 styles excluding e-moticons.
From this table, we can observe thatStyle 2 has many slang words with high probabil-ity while top words in Style 1 are all very formal.However, styles in the second and third rows ofTable 5 are not easily distinguishable from eachother.
In these results, there often exist two stylesvery similar to the styles in row 1 while the otherstyles look like the combination of these two stylesand humans cannot tell their meanings very clear-ly.
Based on these observations, we fix the numberof styles to 2 in the following experiments.0 2 4 6 8 10 12 14 16 1800.050.10.150.20.25Word LengthProbabilityStyle 1Style 2Figure 3: Word length distributionOne previous work uses word length as an in-dicator of formality (Karlgren and Cutting, 1994).Here, we borrow this idea and compare the wordlength of Style 1 and Style 2.
We calculate thedistributions of word length and show the resultsin Figure 3.
It shows that the majority of word-s in Style 1 are longer compared with those inStyle 2.
To have a quantitative view of the differ-ence between the word lengths of these two styles,we heuristically extract words labeled with Style 137and Style 2 in our dataset in the final iteration ofGibbs sampling and apply Mann-Whitney U teston these two word length populations.
The nullhypothesis that the two input populations are thesame is rejected at the 1% significance level.
Thisverifies the intuition that serious posts tend to uselonger words than unserious posts.4.3 Post IdentificationOur model can also be used to separate seriousposts and unserious posts.
We treat this as a re-trieval problem and use precision/recall for evalu-ation.We use a simple scoring function, which is theproportion of words assigned to the unserious stylewhen we terminate the Gibbs sampling at the 800-th iteration, to score each post.
When applying thismethod to our data, emoticons are all removed.For comparison, we rank post according to thenumber of emoticons inside a post as the baseline.After getting the result of each method, we ask t-wo annotators to label the first and last 50 postsin the ranking list.
The first 50 posts are used forevaluation of unserious post retrieval and the last50 post are used for evaluation of serious post re-trieval.
This evaluation is based on the assumptionthat if a method can separate serious and unseriousposts very well, posts ranked at the top positionshould be unserious ones and those ranked nearto the bottom should be serious ones.
The resultsare shown in Table 7 where our method is denot-ed as TSM and the baseline method is denoted asEMO.
In serious post retrieval, the baseline havea perfect performance and our method is compet-itive.
We can see that EMO has a perfect perfor-mance in identifying serious posts.
When postsare ranked in reverse order according to the num-ber of emoticons they contain, the last 50 ones donot contain any emoticons.
They can be regardedas a random sample of posts without emoticons.Compared with identifying serious posts, identi-fying unserious posts looks much more difficult.EMO?s poor performance on this task tells us thatemoticon is not a promising sign to detect unse-rious posts.
However, the word style a post usesmatters more, which also proves the value of ourproposed model.4.4 User IdentificationIn this section, we evaluate the performance ofTSM on identifying serious and unserious users.This identification task is very important as manyP@5 P@15 P@25 P@35SeriousEMO 1.0 1.0 1.0 1.0TSM 1.0 1.0 1.0 0.97UnseriousEMO 0.4 0.67 0.64 0.6TSM 1.0 0.93 0.96 0.97Table 7: Precision for Serious and Unserious PostRetrieval.
P@N stands for the precision of the firstN results in ranking list.P@5 P@15 P@25 P@35SeriousBaseline 0.6 0.8 0.8 0.83TSM 1.0 1.0 1.0 0.94UnseriousBaseline 1.0 0.87 0.92 0.91TSM 1.0 1.0 1.0 1.0Table 8: Precision for serious and unserious userretrieval.research tasks such as opinion mining and expertfinding are more interested in the serious users.We treat this task as a retrieval problem as well,which means we will rank users by a scoring func-tion and do evaluation on this ranking result.We rank user according to their style distribu-tion piuand pick the first 50 and last 50 users forevaluation.
For each user, 10 posts are sampledto be shown to the annotators.
We mix these 100users and ask two graduate students to do the an-notations.
The evaluation strategy is the same asthat in Section 4.3.
We choose a simple base-line which ranks users by the number of emoticonsthey use per post.
The evaluation result is shownin Table 8 for serious and unserious user retrievalrespectively.In both serious and unserious user retrievaltasks, our method gets almost perfect perfor-mance, which is better than the baseline.
Thismeans the user style distributions learned by ourmodel can help separate serious and unserioususers.4.5 PerplexityPerplexity is a widely used criterion in statisticalnatural language processing.
It measures the pre-dictive power of a model on unseen data, whichis algebraically equivalent to the inverse of the ge-ometric mean per-word likelihood.
A lower per-plexity means the test data, which is unseen in thetraining phase, can be generated by the model witha higher probability.
So it also indicates that themodel has a better generalization performance.In this experiment, we leave 10% data for test-ing and use the remaining 90% data for training.We choose LDA as a baseline for comparison and38treat each thread as a document.
The perplexity forboth models is calculated over different numbersof topics, which ranges from 10 to 100.
The resultis show in Figure 4.
We can clearly see that ourproposed model has a substantially lower perplexi-ty than LDA over different numbers of topics.
Thisproves that our model fits the forum discussion da-ta better and has a stronger generalization power.It also indicates that separating topic-driven wordsand style-driven words can better fit the generationof user generated content in forum discussions.4.6 Topic DistinctionIn traditional topic modeling, like LDA, all wordsare regarded as topic-driven words, which are gen-erated by mixture of topics.
However, this may notbe true to user-generated content in online forum-s as not all words are driven by discussed topics.Take the following post for example:?
Okay lah.
Let them be.
I mean its their KKBright?
Let it rot lor.In this post, the words ?lah?
and ?lor?
are not relat-ed to the topics under discussion.
They appear inthe post because the authors are used to using thesewords, which means these words are style driven.Style-driven words are related to a user?s charac-teristics and should not be clustered into any top-ic.
Without separating these two types of words,style-driven words may appear in different topicsand make topics less distinct to each other.Figure 5 compares the Average Divergenceamong discovered topics between TSM (TopicStyle model) and LDA over different numbers oftopics.
We can clearly see that the Average Diver-gence of TSM is substantially larger than that of L-DA over different numbers of topics.
This provesthat in TSM, the learned topics are more distinctfrom each other.
This is because LDA mixes thesetwo kinds of words, which introduces noise intothe learned topics and decreases their distinctionbetween each other.
But topic driven words andstyle driven words are well separated in TSM.
Fig-ure 5 also plots the Average Divergence betweenthe learned two styles, which is the curve denot-ed by DIFF.
We can see the AD between differ-ent styles is even larger than that among topics inTSM.
Different topics may still have some over-lap in frequently used words but styles may sharefew words with each other.
So AD of styles canget higher value.
This also proves the effective-P@5 P@10 P@20 P@30 P@40 P@50E 0 0.2 0.25 0.23 0.225 0.2T 0.8 0.9 0.8 0.8 0.675 0.62Table 9: Slang identification precision.
E: Emoti-con; T:TSM.#Word/Post #PostFormal User 34.9 158.3Informal User 14.5 381Table 10: Mean Value of average post length andnumber of post for different type of usersness of our model in identifying writing styles anduncovering more distinct topics.4.7 Discovering SlangBy looking at Table 5, we notice that the unse-rious style contains many slang words with highprobability.
This indicates that the unserious stylein the dataset we use is also characterized by slangwords.
In this section, we will show the useful-ness of our model in slang discovery.
The base-line method is denoted as Emoticon as it rankswords according to their probability of occurringin a post containing emoticons.
We ask two Sin-gaporean annotators to help us identify Singapore-an slang in the top 50 words.
The result is shownin Table 9.
It tells us the unserious style learnedin our model has very good performance in iden-tifying local slang words.
For people preferringunserious writing style, they would write posts ina very flexible way and use many informal words,abbreviations and slang expressions.
So our un-serious style will be characterized by these slangwords and performs very well in identifying theseslang words.4.8 Analysis of UsersIn this subsection, we analyze users in our datasetbased on the result learned by TSM.
Figure 8shows the distribution of the histogram of seriousstyle probability.
The majority of users have ahigh serious style probability, which means mostusers in our dataset are more eager to give seriouscomments and express their opinions.
This satis-fies our observation that most people use forumsmainly to discuss and seek knowledge on differ-ent topics and they are very eager to express theirthoughts in a serious way.We heuristically split all users into two sets ac-cording to user-style probability by setting 0.5 asthreshold.
Users with probability of serious style3910 20 30 40 50 60 70 80 90 100400040504100415042004250430043504400445045004550Number of topicsPerplexityTSMLDAFigure 4: Perplexity10 20 30 40 50 60 70 80 90 10011.21.41.61.822.22.42.62.83Number of topicsLog(AD)TSMLDADIFFFigure 5: Average DivergenceSerious User Unserious User0200400600800100012001400160018002000Number of PostsFigure 6: Box plot of post number for seriousand unserious usersSerious User Unserious User050100150200Number of Words Per PostFigure 7: Box plot of average post length for se-rious and unserious users0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1020406080100120140160180200Seriouness ScoreNumber of UserFigure 8: Seriousness Score of Userslarger than 0.5 are regarded as serious users andthe remaining are unserious users.
Next, we ex-tract the number of posts each user edit and theaverage number of words per post for each userand compare the difference between these two us-er sets.
Figure 6 and Figure 7 show the box plotsof post number and average post length respective-ly.
We can see that serious users edit fewer postsbut use more words in each post.
To see the dif-ference between serious and unserious users moreclearly, we apply Mann-Whitney U test on the postnumber populations and average post length pop-ulations.
The Mann-Whitney U test on both dataset reject the null hypothesis that two input popu-lations are the same at the 1% significance level.The mean value for post number and average postlength are also computed and shown in Table 10.We can find that serious users tend to publish few-er but longer posts than unserious users.
This re-sult is intuitive as serious users often spend moreeffort editing their posts to express their opinionsmore clearly.
However, for unserious users, theymay just use a few words to play a joke or showsome emotions and they can post many posts with-out spending too much time.5 ConclusionsIn this paper, we propose a unified probabilisticgraphical model, called Topic-Style Model, whichmodels topics and styles at the same time.
Tra-ditional topic modeling methods treat a corpus asa mixture of topics.
But user-generated contentin forum discussions contains not only words re-lated to topics but also words related to differentwriting styles.
The proposed Topic-Style Modelcan perform well in separating topic-driven word-s and style-driven words.
In this model, we as-sume that writing style is a consistent writing pat-tern a user will express in her posts across differ-ent threads and use a latent variable at user lev-el to capture the user specific preference of writ-ing styles.
Our model can successfully discoverwriting styles which are different from each otherboth in word distribution and formality.
Words be-longing to different writing styles and user specificstyle distribution are captured by our model at thesame time.
An extensive set of experiments showsthat our method has good performances in sepa-rating serious and unserious posts and users.
Atthe same time, the model can identify slang wordswith promising accuracy, which is proven by ourexperiments.
An analysis based on the learnedparameters in our model reveal the difference be-tween serious and unserious users in average post40length and post number.ReferencesEugene Agichtein, Carlos Castillo, Debora Donato,Aristides Gionis, and Gilad Mishne.
2008.
Findinghigh-quality content in social media.
In Proceedingsof the 2008 International Conference on Web Searchand Data Mining, pages 183?194, New York, NY,USA.
ACM.Archana Bhattarai, Vasile Rus, and Dipankar Dasgup-ta.
2009.
Characterizing comment spam in the blo-gosphere through content analysis.
In Computation-al Intelligence in Cyber Security, 2009.
CICS?09.IEEE Symposium on, pages 37?44.
IEEE.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet allocation.
J. Mach.
Learn.Res., 3:993?1022, March.Julian Brooke, Tong Wang, and Graeme Hirst.
2010.Automatic acquisition of lexical formality.
In Pro-ceedings of the 23rd International Conference onComputational Linguistics: Posters, pages 90?98,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Dmitry Davidov, Oren Tsur, and Ari Rappoport.
2010.Semi-supervised recognition of sarcastic sentencesin twitter and amazon.
In Proceedings of the Four-teenth Conference on Computational Natural Lan-guage Learning, pages 107?116, Stroudsburg, PA,USA.
Association for Computational Linguistics.Zhuoye Ding, Yeyun Gong, Yaqian Zhou, Qi Zhang,and Xuanjing Huang.
2013.
Detecting spammer-s in community question answering.
In Proceedingof International Joint Conference on Natural Lan-guage Processing, pages 118?126, Nagoya, Japan.Association for Computational Linguistics.Roberto Gonz?alez-Ib?a?nez, Smaranda Muresan, and Ni-na Wacholder.
2011.
Identifying sarcasm in twitter:A closer look.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies: Short Pa-pers - Volume 2, pages 581?586, Stroudsburg, PA,USA.
Association for Computational Linguistics.Nitin Jindal and Bing Liu.
2008.
Opinion spam andanalysis.
In Proceedings of the 2008 InternationalConference on Web Search and Data Mining, WSD-M ?08, pages 219?230, New York, NY, USA.
ACM.Jussi Karlgren and Douglass Cutting.
1994.
Recogniz-ing text genres with simple metrics using discrim-inant analysis.
In Proceedings of the 15th Con-ference on Computational Linguistics - Volume 2,pages 1071?1075, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,and ChengXiang Zhai.
2007.
Topic sentiment mix-ture: Modeling facets and opinions in weblogs.
InProceedings of the 16th International Conference onWorld Wide Web, WWW ?07, pages 171?180, NewYork, NY, USA.
ACM.Michael J. Paul, ChengXiang Zhai, and Roxana Gir-ju.
2010.
Summarizing contrastive viewpoints inopinionated text.
In Proceedings of the 2010 Con-ference on Empirical Methods in Natural LanguageProcessing, EMNLP ?10, pages 66?76, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Emily Pitler and Ani Nenkova.
2008.
Revisitingreadability: A unified framework for predicting textquality.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,pages 186?195, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers,and Padhraic Smyth.
2004.
The author-topic modelfor authors and documents.
In Proceedings of the20th Conference on Uncertainty in Artificial Intelli-gence, pages 487?494, Arlington, Virginia, UnitedStates.
AUAI Press.41
