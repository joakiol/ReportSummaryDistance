Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 802?811,Honolulu, October 2008. c?2008 Association for Computational LinguisticsA Phrase-Based Alignment Model for Natural Language InferenceBill MacCartney, Michel Galley, Christopher D. ManningNatural Language Processing Group, Stanford University{wcmac,mgalley,manning}@stanford.eduAbstractThe alignment problem?establishing linksbetween corresponding phrases in two relatedsentences?is as important in natural languageinference (NLI) as it is in machine transla-tion (MT).
But the tools and techniques ofMT alignment do not readily transfer to NLI,where one cannot assume semantic equiva-lence, and for which large volumes of bitextare lacking.
We present a new NLI aligner,the MANLI system, designed to address thesechallenges.
It uses a phrase-based alignmentrepresentation, exploits external lexical re-sources, and capitalizes on a new set of su-pervised training data.
We compare the per-formance of MANLI to existing NLI and MTaligners on an NLI alignment task over thewell-known Recognizing Textual Entailmentdata.
We show that MANLI significantly out-performs existing aligners, achieving gains of6.2% in F1 over a representative NLI alignerand 10.5% over GIZA++.1 IntroductionThe problem of natural language inference (NLI) isto determine whether a natural-language hypothesisH can reasonably be inferred from a given premisetext P .
In order to recognize that Kennedy was killedcan be inferred from JFK was assassinated, onemust first recognize the correspondence betweenKennedy and JFK, and between killed and assas-sinated.
Consequently, most current approaches toNLI rely, implicitly or explicitly, on a facility foralignment?that is, establishing links between cor-responding entities and predicates in P and H .
Re-cent entries in the annual Recognizing Textual En-tailment (RTE) competition (Dagan et al, 2005)have addressed the alignment problem in a varietyof ways, though often without distinguishing it asa separate subproblem.
Glickman et al (2005) andJijkoun and de Rijke (2005), among others, have ex-plored approaches based on measuring the degree oflexical overlap between bags of words.
While ig-noring structure, such methods depend on matchingeach word in H to the word in P with which it ismost similar?in effect, an alignment.
At the otherextreme, Tatu and Moldovan (2007) and Bar-Haimet al (2007) have formulated the inference problemas analogous to proof search, using inferential ruleswhich encode (among other things) knowledge oflexical relatedness.
In such approaches, the corre-spondence between the words of P andH is implicitin the steps of the proof.Increasingly, however, the most successful RTEsystems have made the alignment problem explicit.Marsi and Krahmer (2005) and MacCartney et al(2006) first advocated pipelined system architec-tures containing a distinct alignment component, astrategy crucial to the top-performing systems ofHickl et al (2006) and Hickl and Bensley (2007).However, each of these systems has pursued align-ment in idiosyncratic and poorly-documented ways,often using proprietary data, making comparisonsand further development difficult.In this paper we undertake the first systematicstudy of alignment for NLI.
We propose a new NLIalignment system which uses a phrase-based repre-sentation of alignment, exploits external resourcesfor knowledge of semantic relatedness, and capi-talizes on the recent appearance of new supervisedtraining data for NLI alignment.
In addition, weexamine the relation between NLI alignment andMT alignment, and investigate whether existing MTaligners can usefully be applied in the NLI setting.2 NLI alignment vs. MT alignmentThe alignment problem is familiar in machine trans-lation (MT), where recognizing that she came is agood translation for elle est venue requires establish-802ing a correspondence between she and elle, and be-tween came and est venue.
The MT community hasdeveloped not only an extensive literature on align-ment (Brown et al, 1993; Vogel et al, 1996; Marcuand Wong, 2002; DeNero et al, 2006), but alsostandard, proven alignment tools such as GIZA++(Och and Ney, 2003).
Can off-the-shelf MT alignersbe applied to NLI?
There is reason to be doubtful.Alignment for NLI differs from alignment for MTin several important respects, including:1.
Most obviously, it is monolingual rather thancross-lingual, opening the door to utilizingabundant (monolingual) sources of informationon semantic relatedness, such as WordNet.2.
It is intrinsically asymmetric: P is often muchlonger thanH , and commonly contains phrasesor clauses which have no counterpart in H .3.
Indeed, one cannot assume even approximatesemantic equivalence?usually a given in MT.Because NLI problems include both valid andinvalid inferences, the semantic content of Hmay diverge substantially from P .
An NLIaligner must be designed to accommodate fre-quent unaligned words and phrases.4.
Little training data is available.
MT align-ment models are typically trained in unsu-pervised fashion, inducing lexical correspon-dences from massive quantities of sentence-aligned bitexts.
While NLI aligners could inprinciple do the same, large volumes of suit-able data are lacking.
NLI aligners must there-fore depend on smaller quantities of supervisedtraining data, supplemented by external lexi-cal resources.
Conversely, while existing MTaligners can make use of dictionaries, they arenot designed to harness other sources of infor-mation on degrees of semantic relatedness.Consequently, the tools and techniques of MT align-ment may not transfer readily to NLI alignment.
Weinvestigate the matter empirically in section 5.2.3 DataUntil recently, research on alignment for NLI hasbeen hampered by a paucity of high-quality, publiclyavailable data from which to learn.
Happily, that hasbegun to change, with the release by Microsoft Re-search (MSR) of human-generated alignment anno-InmostPacificcountriesthereareveryfewwomeninparliament.Womenare poorlyrepresentedin parliament.Figure 1: The MSR gold-standard alignment for problem116 from the RTE2 development set.tations (Brockett, 2007) for inference problems fromthe second Recognizing Textual Entailment (RTE2)challenge (Bar-Haim et al, 2006).
To our knowl-edge, this work is the first to exploit this data fortraining and evaluation of NLI alignment models.The RTE2 data consists of a development set anda test set, each containing 800 inference problems.Each problem consists of a premise and a hypoth-esis.
The premises contain 29 words on average;the hypotheses, 11 words.
Each problem is markedas a valid or invalid inference (50% each); how-ever, these annotations are ignored during align-ment, since they would not be available during test-ing of a complete NLI system.The MSR annotations use an alignment repre-sentation which is token-based, but many-to-many,and thus allows implicit alignment of multi-wordphrases.
Figure 1 shows an example in which veryfew has been aligned with poorly represented.In the MSR data, every alignment link is markedas SURE or POSSIBLE.
In making this distinction,the annotators have followed a convention commonin MT, which permits alignment precision to bemeasured against both SURE and POSSIBLE links,while recall is measured against only SURE links.In this work, however, we have chosen to ignorePOSSIBLE links, embracing the argument made by(Fraser and Marcu, 2007) that their use has impededprogress in MT alignment models, and that SURE-803only annotation is to be preferred.Each RTE2 problem was independently annotatedby three people, following carefully designed an-notation guidelines.
Inter-annotator agreement washigh: Brockett (2007) reports Fleiss?
kappa1 scoresof about 0.73 (?substantial agreement?)
for map-pings from H tokens to P tokens; and all three an-notators agreed on ?70% of proposed links, whileat least two of three agreed on more than 99.7%of proposed links,2 attesting to the high quality ofthe annotation data.
For this work, we merged thethree independent annotations, using majority rule,3to obtain a gold-standard annotation containing anaverage of 7.3 links per RTE problem.4 The MANLI alignerIn this section, we describe the MANLI aligner, anew alignment system designed expressly for NLIalignment.
The MANLI system consists of four el-ements: (1) a phrase-based representation of align-ment, (2) a feature-based linear scoring function foralignments, (3) a decoder which uses simulated an-nealing to find high-scoring alignments, and (4) per-ceptron learning to optimize feature weights.4.1 A phrase-based alignment representationMANLI uses an alignment representation which isintrinsically phrase-based.
(Following the usagecommon in MT, we use ?phrase?
to mean any con-tiguous span of tokens, not necessarily correspond-ing to a syntactic phrase.)
We represent an alignmentE between a premise P and a hypothesis H as a setof phrase edits {e1, e2, .
.
.
}, each belonging to oneof four types:?
an EQ edit connects a phrase in P with an equal(by word lemmas) phrase in H?
a SUB edit connects a phrase in P with an un-equal phrase in H?
a DEL edit covers an unaligned phrase in P?
an INS edit covers an unaligned phrase in HFor example, the alignment shown in fig-ure 1 can be represented by the set {DEL(In1),1Fleiss?
kappa generalizes Cohen?s kappa to the case wherethere are more than two annotators.2The SURE/POSSIBLE distinction is taken as significant incomputing all these figures.3The handful of three-way disagreements were treated asPOSSIBLE links, and thus were not used here.DEL(most2), DEL(Pacific3), DEL(countries4),DEL(there5), EQ(are6, are2), SUB(very7 few8,poorly3 represented4), EQ(women9, Women1),EQ(in10, in5), EQ(parliament11, parliament6),EQ(.12, .7)}.4Alignments are constrained to be one-to-one atthe phrase level: every token in P and H belongsto exactly one phrase, which participates in exactlyone edit (possibly DEL or INS).
However, the phraserepresentation permits alignments which are many-to-many at the token level.
In fact, this is the chiefmotivation for the phrase-based representation: wecan align very few and poorly represented as units,without being forced to make an arbitrary choice asto which word goes with which word.
Moreover, ourscoring function can make use of lexical resourceswhich have information about semantic relatednessof multi-word phrases, not merely individual words.About 23% of the MSR gold-standard align-ments are not one-to-one (at the token level), andare therefore technically unreachable for MANLI,which is constrained to generate one-to-one align-ments.
However, by merging contiguous token linksinto phrase edits of size > 1, most MSR align-ments (about 92%) can be straightforwardly con-verted into MANLI-reachable alignments.
For thepurpose of model training (but not for the evalua-tion described in section 5.4), we generated a ver-sion of the MSR data in which all alignments wereconverted to MANLI-reachable form.54.2 A feature-based scoring functionTo score alignments, we use a simple feature-basedlinear scoring function, in which the score of analignment is the sum of the scores of the edits it con-tains (including not only SUB and EQ edits, but alsoDEL and INS edits), and the score of an edit is thedot product of a vector encoding its features and avector of weights.
If E is a set of edits constituting4DEL and INS edits of size > 1 are possible in principle, butare not used in our training data.5About 8% of the MSR alignments contain non-contiguouslinks, most commonly because P contains two references toan entity (e.g., Christian Democrats and CDU) which are bothlinked to a reference to the same entity in H (e.g., ChristianDemocratic Union).
In such cases, one or more links must beeliminated to achieve a MANLI-reachable alignment.
We useda string-similarity heuristic to break such conflicts, but wereobliged to make an arbitrary choice in about 2% of cases.804an alignment, and ?
is a vector of feature functions,the score s is given by:s(E) =?e?Es(e) =?e?Ew ??
(e)We?ll explain how the feature weights w are set insection 4.4.
The features used to characterize eachedit are as follows:Edit type features.
We begin with boolean fea-tures encoding the type of each edit.
We expect EQsto score higher than SUBs, and (sinceP is commonlylonger than H) DELs to score higher than INSs.Phrase features.
Next, we have features whichencode the sizes of the phrases involved in the edit,and whether these phrases are non-constituents (insyntactic parses of the sentences involved).Lexical similarity feature.
For SUB edits, a veryimportant feature represents the lexical similarity ofthe substituends, as a real value in [0, 1].
This simi-larity score is computed as a max over a number ofcomponent scoring functions, some based on exter-nal lexical resources, including:?
various string similarity functions, of whichmost are applied to word lemmas?
measures of synonymy, hypernymy, antonymy,and semantic relatedness, including a widely-used measure due to Jiang and Conrath (1997),based on manually constructed lexical re-sources such as WordNet and NomBank?
a function based on the well-known distribu-tional similarity metric of Lin (1998), whichautomatically infers similarity of words andphrases from their distributions in a very largecorpus of English textThe ability to leverage external lexical resources?both manually and automatically constructed?iscritical to the success of MANLI.Contextual features.
Even when the lexical sim-ilarity for a SUB edit is high, it may not be agood match.
If P or H contains multiple occur-rences of the same word?which happens frequentlywith function words, and occasionally with contentwords?lexical similarity may not suffice to deter-mine the right match.
To remedy this, we introducecontextual features for SUB and EQ edits.
A real-valued distortion feature measures the differenceInputs?
an alignment problem ?P,H??
a number of iterations N (e.g.
100)?
initial temperature T0 (e.g.
40) and multiplier r (e.g.
0.9)?
a bound on edit size max (e.g.
6)?
an alignment scoring function, SCORE(E)Initialize?
Let E be an ?empty?
alignment for ?P,H?
(containingonly DEL and INS edits, no EQ or SUB edits)?
Set E?
= ERepeat for i = 1 to N?
Let {F1, F2, ...} be the set of possible successors of E.To generate this set:?
Consider every possible edit f up to size max?
Let C(E, f) be the set of edits in E which ?con-flict?
with f (i.e., involve at least some of the sametokens as f )?
Let F = E ?
{f} \ C(E, f)?
Let s(F ) be a map from successors of E to scores gener-ated by SCORE?
Set p(F ) = exp s(F ), and then normalize p(F ), trans-forming the score map to a probability distribution?
Set Ti = r ?
Ti?1?
Set p(F ) = p(F )1/Ti , smoothing or sharpening p(F )?
Renormalize p(F )?
Choose a new value for E by sampling from p(F )?
If SCORE(E) > SCORE(E?
), set E?
= EReturn E?Figure 2: The MANLI-ALIGN algorithmbetween the relative positions of the substituendswithin their respective sentences, while booleanmatching neighbors features indicate whether the to-kens before and after the substituends are equal orsimilar.4.3 Decoding using simulated annealingThe problem of decoding?that is, finding ahigh-scoring alignment for a particular inferenceproblem?is made more complex by our choice of aphrase-based alignment representation.
For a modelwhich uses a token-based representation (say, onewhich simply maps H tokens to P tokens), decod-ing is trivial, since each token can be aligned inde-pendently of its neighbors.
(This is the case for thebag-of-words aligner described in section 5.1.)
Butwith a phrase-based representation, things are morecomplicated.
The segmentation into phrases is notgiven in advance, and every phrase pair consideredfor alignment must be consistent with its neighborswith respect to segmentation.
Consequently, the de-coding problem cannot be factored into a number of805independent decisions.To address this difficulty, we have devised astochastic alignment algorithm, MANLI-ALIGN (fig-ure 2), which uses a simulated annealing strategy.Beginning from an arbitrary alignment, we make aseries of local steps, at each iteration sampling froma set of possible successors according to scores as-signed by our scoring function.
The sampling is con-trolled by a ?temperature?
which falls over time.
Atthe beginning of the process, successors are sampledwith nearly uniform probability, which helps to en-sure that the space of possibilities is explored andlocal maxima are avoided.
As the temperature falls,there is a ever-stronger bias toward high-scoring suc-cessors, so that the algorithm converges on a near-optimal alignment.
Clever use of memoization helpsto ensure that computational costs remain manage-able.
Using the parameter values suggested in fig-ure 2, aligning an average RTE problem takes abouttwo seconds.While MANLI-ALIGN is not guaranteed to pro-duce optimal alignments, there is reason to believethat it usually comes very close.
After training, thealignment found by MANLI scored at least as highas the gold alignment for 99.6% of RTE problems.64.4 Perceptron learningTo tune the parameters w of the model, we usean adaptation of the averaged perceptron algorithm(Collins, 2002), which has proven successful on arange of NLP tasks.
The algorithm is shown in fig-ure 3.
After initializing w to 0, we perform N train-ing epochs.
(Our experiments used N = 50.)
Ineach epoch, we iterate through the training data, up-dating the weight vector at each training example ac-cording to the difference between the features of thetarget algnment and the features of the alignmentproduced by the decoder using the current weightvector.
The size of the update is controlled by alearning rate which decreases over time.
At the endof each epoch, the weight vector is normalized andstored.
The final result is the average of the stored6This figure is based on the MANLI-reachable version ofthe gold-standard data described in section 4.1.
For the rawgold-standard data, the figure is 88.1%.
The difference is almostentirely attributable to unreachable gold alignments, which tendto score higher simply because they contain more edits (andbecause the learned weights are mostly positive).Inputs?
training problems ?Pj , Hj?, j = 1..n?
corresponding gold-standard alignments Ej?
a number of learning epochs N (e.g.
50)?
a ?burn-in?
period N0 < N (e.g.
10)?
initial learning rate R0 (e.g.
1) and multiplier r (e.g.
0.8)?
a vector of feature functions ?(E)?
an alignment algorithm ALIGN(P,H;w) which finds agood alignment for ?P,H?
using weight vector wInitialize?
Set w = 0Repeat for i = 1 to N?
Set Ri = r ?Ri?1, reducing the learning rate?
Randomly shuffle the training problems?
For j = 1 to n:?
Set E?j = ALIGN(Pj , Hj ; w)?
Set w = w + Ri ?
(?(Ej)??(E?j))?
Set w = w/?w?2 (L2 normalization)?
Set w[i] = w, storing the weight vector for this epochReturn an averaged weight vector:?
wavg = 1/(N ?N0)PNi=N0+1w[i]Figure 3: The MANLI-LEARN algorithmweight vectors, omitting vectors from a fixed num-ber of epochs at the beginning of the run (which tendto be of poor quality).
Using the parameter valuessuggested in figure 3, training runs on the RTE2 de-velopment set required about 20 hours.5 Evaluating aligners on MSR dataIn this section, we describe experiments designed toevaluate the performance of various alignment sys-tems on the MSR gold-standard data described insection 3.
For each system, we report precision,recall, and F-measure (F1).7 Note that these aremacro-averaged statistics, computed per problem bycounting aligned token pairs,8 and then averagedover all problems in a problem set.9 We also re-7MT researchers conventionally report results in terms ofalignment error rate (AER).
Since we use only SURE links in thegold-standard data (see section 3), AER is equivalent to 1?F1.8For phrase-based alignments like those generated byMANLI, two tokens are considered to be aligned iff they arecontained within phrases which are aligned.9MT evaluations conventionally use micro-averaging, whichgives greater weight to problems containing more aligned pairs.This makes sense in MT, where the purpose of alignment is toinduce phrase tables.
But in NLI, where the ultimate goal isto maximize the number of inference problems answered cor-rectly, it is more fitting to give all problems equal weight, andso we macro-average.
We have also generated all results usingmicro-averaging, and found that the relative comparisons are806port the exact match rate, that is, the proportion ofproblems in which the guessed alignment exactlymatches the gold alignment.
The results are sum-marized in table 1.5.1 A robust baseline: the bag-of-words alignerAs a baseline, we use a simple alignment algorithminspired by the lexical entailment model of Glick-man et al (2005), and similar to the simple heuristicmodel described in (Och and Ney, 2003).
Each hy-pothesis word h is aligned to the premise word p towhich it is most similar, according to a lexical sim-ilarity function sim(p, h) which returns scores in[0, 1].
While Glickman et al used a function basedon web co-occurrence statistics, we use a much sim-pler function based on string edit distance:sim(w1, w2) = 1?dist(lem(w1), lem(w2))max(|lem(w1)|, |lem(w2)|)(Here lem(w) denotes the lemma of word w; dist()denotes Levenshtein string edit distance; and | ?
| de-notes string length.
)This model can be easily extended to generate analignment score, which will be of interest in sec-tion 6.
We define the score for a specific hypoth-esis token h to be the log of its similarity withthe premise token p to which it is aligned, and thescore for the complete alignment of hypothesis Hto premise P to be the sum of the scores of the to-kens in H , weighted by inverse document frequencyin a large corpus10 (so that common words get lessweight), and normalized by the length of H:score(h|P ) = logmaxp?Psim(p, h)score(H|P ) =1|H|?h?Hidf(h) ?
score(h|P )Despite the simplicity of this alignment model, itsperformance is fairly robust, with good recall.
Itsprecision, however, its mediocre?chiefly because,by design, it aligns every h with some p. The modelcould surely be improved by allowing it to leavesome H tokens unaligned, but this was not pursued.not greatly affected.10We use idf(w) = log(N/Nw), where N is the number ofdocuments in the corpus, and Nw is the number of documentscontaining word w.System Data P % R % F1 % E %Bag-of-words dev 57.8 81.2 67.5 3.5(baseline) test 62.1 82.6 70.9 5.3GIZA++ dev 83.0 66.4 72.1 9.4(using lex, ?)
test 85.1 69.1 74.8 11.3Cross-EM dev 67.6 80.1 72.1 1.3(using lex, ?)
test 70.3 81.0 74.1 0.8Stanford RTE dev 81.1 61.2 69.7 0.5test 82.7 61.2 70.3 0.3Stanford RTE dev 81.1 75.8 78.4 ?(punct.
corr.)
test 82.7 75.8 79.1 ?MANLI dev 83.4 85.5 84.4 21.7(this work) test 85.4 85.3 85.3 21.3Table 1: Performance of various aligners on the MSRRTE2 alignment data.
The columns show the data setused (800 problems each); average precision, recall, andF-measure; and the exact match rate (see text).5.2 MT aligners: GIZA++ and Cross-EMGiven the importance of alignment for NLI, and theavailability of standard, proven tools for MT align-ment, an obvious question presents itself: why notuse an off-the-shelf MT aligner for NLI?
Althoughwe have argued (section 2) that this is unlikely tosucceed, to our knowledge, we are the first to inves-tigate the matter empirically.11The best-known MT aligner is undoubtedlyGIZA++ (Och and Ney, 2003), which contains im-plementations of various IBM models (Brown et al,1993), as well as the HMM model of Vogel et al(1996).
Most practitioners use GIZA++ as a blackbox, via the Moses MT toolkit (Koehn et al, 2007).We followed this practice, running with Moses?
de-fault parameters on the RTE2 data to obtain asym-metric word alignments in both directions (P -to-Hand H-to-P ).
We then performed symmetrizationusing the well-known INTERSECTION heuristic.Unsurprisingly, the out-of-the-box performancewas quite poor, with most words aligned apparentlyat random.
Precision was fair (72%) but recall wasvery poor (46%).
Even equal words were usually notaligned?because GIZA++ is designed for cross-linguistic use, it does not consider word equality be-tween source and target sentences.
To remedy this,we supplied GIZA++ with a lexicon, using a trick11However, Dolan et al (2004) explore a closely-relatedtopic: using an MT aligner to identify paraphrases.807common in MT: we supplemented the training datawith synthetic data consisting of matched pairs ofequal words.
This gives GIZA++ a better chanceof learning that, e.g., man should align with man.The result was a big boost in recall (+23%), and asmaller gain in precision.
The results for GIZA++shown in table 1 are based on using the lexicon andINTERSECTION.
With these settings, GIZA++ prop-erly aligned most pairs of equal words, but contin-ued to align other words apparently at random.Next, we compared the performance of INTER-SECTION with other symmetrization heuristics de-fined in Moses?including UNION, GROW, GROW-DIAG, GROW-DIAG-FINAL (the default), and GROW-DIAG-FINAL-AND?and with asymmetric align-ments in both directions.
While all these alterna-tives achieved better recall than INTERSECTION, allshowed substantially worse precision and F1.
Onthe RTE2 test set, the asymmetric alignment fromH to P scored 68% in F1; GROW scored 58%; andall other alternatives scored below 52%.As an additional experiment, we tested the Cross-EM aligner (Liang et al, 2006) from the Berke-leyAligner package on the MSR data.
While thisaligner is in many ways simpler than GIZA++ (itlacks any model of fertility, for example), its methodof jointly training two simple asymmetric HMMmodels has outperformed GIZA++ on standard eval-uations of MT alignment.
As with GIZA++, we ex-perimented with a variety of symmetrization heuris-tics, and ran trials with and without a supplementallexicon.
The results were broadly similar: INTER-SECTION greatly outperformed alternative heuris-tics, and using a lexicon provided a big boost (upto 12% in F1).
Under optimal settings, the Cross-EM aligner showed better recall and worse preci-sion than GIZA++, with F1 just slightly lower.
LikeGIZA++, it did well at aligning equal words, butaligned most other words at random.The mediocre performance of MT aligners onNLI alignment comes as no surprise, for reasons dis-cussed in section 2.
Above all, the quantity of train-ing data is simply too small for unsupervised learn-ing to succeed.
A successful NLI aligner will needto exploit supervised training data, and will need ac-cess to additional sources of knowledge about lexi-cal relatedness.5.3 The Stanford RTE alignerA better comparison is thus to an alignment sys-tem expressly designed for NLI.
For this purpose,we used the alignment component of the StanfordRTE system (Chambers et al, 2007).
The Stanfordaligner performs decoding and learning in a simi-lar fashion to MANLI, but uses a simpler, token-based alignment representation, along with a richerset of features for alignment scoring.
It representsalignments as an injective map from H tokens toP tokens.
Phrase alignments are not directly repre-sentable, although the effect can be approximated bya pre-processing step which collapses multi-tokennamed entities and certain collocations into singletokens.
The features used for alignment scoring in-clude not only measures of lexical similarity, butalso syntactic features intended to promote the align-ment of similar predicate-argument structures.Despite this sophistication, the out-of-the-boxperformance of the Stanford aligner is mediocre, asshown in table 1.
The low recall figures are partic-ularly noteworthy.
However, a partial explanationis readily available: by design, the Stanford systemignores punctuation.12 Because punctuation tokensconstitute about 15% of the aligned pairs in the MSRdata, this sharply reduces measured recall.
However,since punctuation matters little in inference, such re-call errors probably should be forgiven.
Thus, ta-ble 1 also shows adjusted statistics for the Stanfordsystem in which all recall errors involving punctua-tion are (generously) ignored.Even after this adjustment, the recall figures areunimpressive.
Error analysis reveals that the Stan-ford aligner does a poor job of aligning functionwords.
About 13% of the aligned pairs in the MSRdata are matching prepositions or articles; the Stan-ford aligner misses about 67% of such pairs.
(Bycontrast, MANLI misses only 10% of such pairs.
)While function words matter less in inference thannouns and verbs, they are not irrelevant, and becausesentences often contain multiple instances of a par-ticular function word, matching them properly is byno means trivial.
If matching prepositions and ar-ticles were ignored (in addition to punctuation), thegap in F1 between the MANLI and Stanford systems12In fact, it operates on a dependency-graph representationfrom which punctuation is omitted.808would narrow to about 2.8%.Finally, the Stanford aligner is handicapped by itstoken-based alignment representation, often failing(partly or completely) to align multi-word phrasessuch as peace activists with protesters, or hackerswith non-authorized personnel.5.4 The MANLI alignerAs table 1 indicates, the MANLI aligner was foundto outperform all other aligners evaluated on ev-ery measure of performance, achieving an F1 score10.5% higher than GIZA++ and 6.2% higher thanthe Stanford aligner (even with the punctuation cor-rection).13 MANLI achieved a good balance be-tween precision and recall, and matched more than20% of the gold-standard alignments exactly.Three factors seem to have contributed most toMANLI?s success.
First, MANLI is able to outper-form the MT aligners principally because it is ableto leverage lexical resources to identify the similar-ity between pairs of words such as jail and prison,prevent and stop, or injured and wounded.
Second,MANLI?s contextual features enable it to do bet-ter than the Stanford aligner at matching functionwords, a weakness of the Stanford aligner discussedin section 5.3.
Third, MANLI gains a marginal ad-vantage because its phrase-based representation ofalignment permits it to properly align phrase pairssuch as death penalty and capital punishment, or ab-dicate and give up.However, the phrase-based representation con-tributed far less than we had hoped.
SettingMANLI?s maximum phrase size to 1 (effectively,restricting it to token-based alignments) caused F1to fall by just 0.2%.
We do not interpret this tomean that phrase alignments are not useful?indeed,about 2.6% of the links in the gold-standard data in-volve phrases of size > 1.
Rather, we think it showsthat we have failed to fully exploit the advantagesof the phrase-based representation, chiefly becausewe lack lexical resources providing good informa-tion on similarity of multi-word phrases.Error analysis suggests that there is ample roomfor improvement.
A large proportion of recall errors(perhaps 40%) occur because the lexical similarityfunction assigns too low a value to pairs of words13Reported results for MANLI are averages over 10 runs.or phrases which are clearly similar, such as con-servation and protecting, server and computer net-works, organization and agencies, or bone fragilityand osteoporosis.
Better exploitation of lexical re-sources could help to reduce such errors.
Anotherimportant category of recall errors (about 12%) re-sult from the failure to identify one- and multi-wordversions of the name of some entity, such as Lennonand John Lennon, or Nike Inc. and Nike.
A special-purpose similarity function could help here.
Note,however, that about 10% of recall errors are un-avoidable, given our choice of alignment represen-tation, since they involve cases where the gold stan-dard aligns one or more tokens on one side to a non-contiguous set of tokens on the other side.Precision errors may be harder to reduce.
Theseerrors are dominated by cases where we mistakenlyalign two equal function words (49% of precision er-rors), two forms of the verb to be (21%), two equalpunctuation marks (7%), or two words or phrasesof other types having equal lemmas (18%).
Be-cause such errors often occur because the aligneris forced to choose between nearly equivalent alter-natives, they may be difficult to eliminate.
The re-maining 5% of precision errors result mostly fromaligning words or phrases rightly judged to be highlysimilar, such as expanding and increasing, labor andbirth, figures and number, or 223,000 and 220,000.6 Using alignment to predict RTE answersIn section 5, we evaluated the ability of aligners torecover gold-standard alignments.
But since align-ment is just one component of the NLI problem, wemight also examine the impact of different align-ers on the ability to recognize valid inferences.
If ahigh-scoring alignment indicates a close correspon-dence between H and P , does this also indicate avalid inference?
We have previously emphasized(MacCartney et al, 2006) that there is more to infer-ential validity than close lexical or structural corre-spondence: negations, modals, non-factive and im-plicative verbs, and other linguistic constructs canaffect validity in ways hard to capture in alignment.Nevertheless, alignment score can be a strong pre-dictor of inferential validity, and some NLI systems(e.g., (Glickman et al, 2005)) rely entirely on somemeasure of alignment quality to predict validity.809System data acc % avgP %Bag-of-words aligner dev 61.3 61.5test 57.9 58.9Stanford RTE aligner dev 63.1 64.9test 60.9 59.2MANLI aligner dev 59.3 69.0(this work) test 60.3 61.0RTE2 entries (average) test 58.5 59.1LCC (Hickl et al, 2006) test 75.4 80.8Table 2: Performance of various aligners and completeRTE systems in predicting RTE2 answers.
The columnsshow the data set used, accuracy, and average precision(the recommended metric for RTE2).If an aligner generates real-valued alignmentscores, we can use the RTE data to test its ability topredict inferential validity with the following simplemethod.
For a given RTE problem, we predict YES(valid) if its alignment score14 exceeds a threshold?
, and NO otherwise.
We tune ?
to maximize accu-racy on the RTE2 development set, and then measureperformance on the RTE2 test set using the same ?
.Table 2 shows results for several NLI aligners,along with some results for complete RTE systems,including the LCC system (the top performer atRTE2) and an average of all systems participating inRTE2.
While none of the aligners rivals the perfor-mance of the LCC system, all achieve respectableresults, and the Stanford and MANLI aligners out-perform the average RTE2 entry.
Thus, even if align-ment quality does not determine inferential validity,many NLI systems could be improved by harnessinga well-designed NLI aligner.7 Related workGiven the extensive literature on phrase-based MT,it may be helpful further to situate our phrase-basedalignment model in relation to past work.
The stan-dard approach to training a phrase-based MT systemis to apply phrase extraction heuristics using word-aligned training sets (Och and Ney, 2003; Koehnet al, 2007).
Unfortunately, word alignment mod-els assume that source words are individually trans-14For good results, it may be necessary to normalize thealignment score.
Scores from MANLI were normalized by thenumber of tokens in the problem.
The Stanford aligner performsa similar normalization internally.lated into target words, which stands at odds withthe key assumption in phrase-based systems thatmany translations are non-compositional.
More re-cently, several works (Marcu and Wong, 2002; De-Nero et al, 2006; Birch et al, 2006; DeNero andKlein, 2008) have presented more unified phrase-based systems that jointly align and weight phrases,though these systems have not come close to thestate of the art when evaluated in terms of MT per-formance.We would argue that previous work in MT phrasealignment is orthogonal to our work.
In MANLI,the need for phrases arises when word-based rep-resentations are not appropriate for alignment (e.g.,between close down and terminate), though longerphrases are not needed to achieve good alignmentquality.
In MT phrase alignment, it is beneficial toaccount for arbitrarily large phrases, since the largercontexts offered by these phrases can help realizemore dependencies among translated words (e.g.,word order, agreement, subcategorization).
Per-haps because MT phrase alignment is dealing withmuch larger contexts, no existing work in MT phrasealignment (to our knowledge) directly models wordinsertions and deletions, as in MANLI.
For exam-ple, in figure 1, MANLI can just skip In most Pacificcountries there, while an MT phrase-based modelwould presumably align In most Pacific countriesthere are to Women are.
Hence, previous work isof limited applicability to our problem.8 ConclusionWhile MT aligners succeed by unsupervised learn-ing of word correspondences from massive amountsof bitext, NLI aligners are forced to rely on smallerquantities of supervised training data.
With theMANLI system, we have demonstrated how to over-come this lack of data by utilizing external lexicalresources, and how to gain additional power from aphrase-based representation of alignment.Acknowledgements The authors wish to thank theanonymous reviewers for their helpful comments onan earlier draft of this paper.
This paper is basedon work funded in part by the Defense AdvancedResearch Projects Agency through IBM and in partby the CIA ATP as part of the OCCAM project.810ReferencesR.
Bar-Haim, I. Dagan, B. Dolan, L. Ferro, D. Giampic-colo, B. Magnini, and I. Szpektor.
2006.
The Sec-ond PASCAL Recognising Textual Entailment Chal-lenge.
In Proceedings of the Second PASCAL Chal-lenges Workshop on Recognising Textual Entailment.R.
Bar-Haim, I. Dagan, I. Greental, and E. Shnarch.
2007.Semantic Inference at the Lexical-Syntactic Level.
InProceedings of AAAI-07.A.
Birch, C. Callison-Burch, M. Osborne, and P. Koehn.2006.
Constraining the Phrase-Based, Joint Probabil-ity Statistical Translation Model.
In Proceedings of theACL-06 Workshop on Statistical Machine Translation.C.
Brockett.
2007.
Aligning the RTE 2006 Corpus.
Tech-nical Report MSR-TR-2007-77, Microsoft Research.P.
F. Brown, S. D. Pietra, V. J. D. Pietra, and R. L. Mercer.1993.
The Mathematics of Statistical Machine Trans-lation: Parameter Estimation.
Computational Linguis-tics, 19(2):263?311.N.
Chambers, D. Cer, T. Grenager, D. Hall, C. Kid-don, B. MacCartney, M. C. de Marneffe, D. Ramage,E.
Yeh, and C. D. Manning.
2007.
Learning Align-ments and Leveraging Natural Logic.
In Proceedingsof the ACL-07 Workshop on Textual Entailment andParaphrasing.M.
Collins.
2002.
Discriminative training methods forhidden Markov models.
In Proceedings of EMNLP-02.I.
Dagan, O. Glickman, and B. Magnini.
2005.
The PAS-CAL Recognising Textual Entailment Challenge.
InProceedings of the PASCAL Challenges Workshop onRecognising Textual Entailment.J.
DeNero and D. Klein.
2008.
The Complexity of PhraseAlignment Problems.
In Proceedings of ACL/HLT-08:Short Papers, pages 25?28.J.
DeNero, D. Gillick, J. Zhang, and D. Klein.
2006.Why Generative Phrase Models Underperform SurfaceHeuristics.
In Proceedings of the ACL-06 Workshop onStatistical Machine Translation, pages 31?38.B.
Dolan, C. Quirk, and C. Brockett.
2004.
Unsupervisedconstruction of large paraphrase corpora.
In Proceed-ings of COLING-04.A.
Fraser and D. Marcu.
2007.
Measuring WordAlignment Quality for Statistical Machine Translation.Computational Linguistics, 33(3):293?303.O.
Glickman, I. Dagan, and M. Koppel.
2005.
Webbased probabilistic textual entailment.
In Proceedingsof the PASCAL Challenges Workshop on RecognizingTextual Entailment.A.
Hickl and J. Bensley.
2007.
A DiscourseCommitment-Based Framework for Recognizing Tex-tual Entailment.
In ACL-07 Workshop on Textual En-tailment and Paraphrasing, Prague.A.
Hickl, J. Williams, J. Bensley, K. Roberts, B. Rink,and Y. Shi.
2006.
Recognizing Textual Entailmentwith LCC?s GROUNDHOG System.
In Proceedingsof the Second PASCAL Challenges Workshop on Rec-ognizing Textual Entailment.J.
J. Jiang and D. W. Conrath.
1997.
Semantic simi-larity based on corpus statistics and lexical taxonomy.In Proceedings of the International Conference on Re-search in Computational Linguistics.V.
Jijkoun and M. de Rijke.
2005.
Recognizing tex-tual entailment using lexical similarity.
In Proceedingsof the PASCAL Challenges Workshop on RecognizingTextual Entailment, pages 73?76.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, et al 2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proceed-ings of ACL-07, demonstration session.P.
Liang, B. Taskar, and D. Klein.
2006.
Alignment byAgreement.
In Proceedings of NAACL-06, New York.D.
Lin.
1998.
Automatic retrieval and clustering of simi-lar words.
In Proceedings of COLING/ACL-98, pages768?774, Montreal, Canada.B.
MacCartney, T. Grenager, M. C. de Marneffe, D. Cer,and C. D. Manning.
2006.
Learning to RecognizeFeatures of Valid Textual Entailments.
In Proceedingsof NAACL-06, New York.D.
Marcu and W. Wong.
2002.
A phrase-based, jointprobability model for statistical machine translation.In Proceedings of EMNLP-02, pages 133?139.E.
Marsi and E. Krahmer.
2005.
Classification of se-mantic relations by humans and machines.
In ACL-05Workshop on Empirical Modeling of Semantic Equiv-alence and Entailment, Ann Arbor.F.
J. Och and H. Ney.
2003.
A Systematic Comparison ofVarious Statistical Alignment Models.
ComputationalLinguistics, 29(1):19?51.M.
Tatu and D. Moldovan.
2007.
COGEX at RTE3.
InProceedings of ACL-07.S.
Vogel, H. Ney, and C. Tillmann.
1996.
HMM-based word alignment in statistical translation.
InProceedings of COLING-96, pages 836?841, Copen-hagen, Denmark.811
