2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 553?557,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsHow Text Segmentation Algorithms Gain from Topic ModelsMartin Riedl and Chris BiemannUbiquitous Knowledge Processing LabComputer Science Department, Technische Universita?t DarmstadtHochschulstrasse 10, D-64289 Darmstadt, Germanyriedl@ukp.informatik.tu-darmstadt.de, biem@cs.tu-darmstadt.deAbstractThis paper introduces a general method to in-corporate the LDA Topic Model into text seg-mentation algorithms.
We show that seman-tic information added by Topic Models signifi-cantly improves the performance of two word-based algorithms, namely TextTiling and C99.Additionally, we introduce the new TopicTil-ing algorithm that is designed to take betteradvantage of topic information.
We show con-sistent improvements over word-based meth-ods and achieve state-of-the art performanceon a standard dataset.1 IntroductionTexts are often structured into segments to ease un-derstanding and readability of texts.
Knowing aboutsentence boundaries is advantageous for natural lan-guage processing (NLP) tasks such as summariza-tion or indexing.
While many genres such as en-cyclopedia entries or scientific articles follow ratherformal conventions of breaking up a text into mean-ingful units, there are plenty of electronically avail-able texts without defined segments, e.g.
web doc-uments.
Text segmentation is the task of automati-cally segmenting texts into parts.
Viewing a well-written text as sequence of subtopics and assumingthat subtopics correspond to segments, a segmenta-tion algorithm needs to find changes of subtopics toidentify the natural division of an unstructured text.In this work, we utilize semantic informationfrom Topic Models (TMs) to inform text segmen-tation algorithms.
For this, we compare two earlyword-based algorithms with their topic-based vari-ants, and construct our own algorithm called Topic-Tiling.
We show that using topics estimated by La-tent Dirichlet Allocation (LDA) in lieu of words sub-stantially improves earlier segmentation algorithms.In comparison to TextTiling (TT), neither smoothingnor a blocksize or window size is needed.
TT usingTMs and our own algorithm improve on the state-of-the-art for a standard dataset, while being conceptu-ally simpler and computationally more efficient thanother topic-based segmentation algorithms.2 Related WorkBased on the observation of Halliday and Hasan(1976) that the density of coherence relations ishigher within segments than between segments,most algorithms compute a coherence score to mea-sure the difference of textual units for informinga segmentation decision.
TextTiling (TT) (Hearst,1994) relies on the simplest coherence relation ?word repetition ?
and computes similarities betweentextual units based on the similarities of word spacevectors.
With C99 (Choi, 2000) an algorithm wasintroduced that uses a matrix-based ranking and aclustering approach in order to relate the most sim-ilar textual units and to cluster groups of consecu-tive units into segments.
Both TT and C99 charac-terize textual units by the words they contain.
Gal-ley et al (2003) showed that using TF-IDF termweights in the term vector improves the performanceof TT.
Proposals using Dynamic Programming (DP)are given in (Utiyama and Isahara, 2001; Fragkou etal., 2004).
Related to our work are the approachesdescribed in (Misra et al, 2009; Sun et al, 2008):here, TMs are also used to alleviate the sparsity ofword vectors.
Misra et al (2009) extended the DPalgorithm U00 from Utiyama and Isahara (2001) us-553ing TMs.
At this, the topic assignments have to beinferred for each possible segment, resulting in highcomputational cost.
In addition to these linear topicsegmentation algorithms, there are hierarchical seg-mentation algorithms, see (Yaari, 1997; Hsueh et al,2006; Eisenstein, 2009).For topic modeling, we use the widely appliedLDA (Blei et al, 2003).
This generative probabilis-tic model uses a training corpus of documents to cre-ate document-topic and topic-word distributions andis parameterized by the number of topics N as wellas by two hyperparameters.
To generate a documentd the topic proportions are drawn using a Dirichletdistribution with hyperparameter ?.
Adjacent foreach word i a topic zdi is chosen according to amultinomial distribution using hyperparameter ?zdi .Unseen documents can be annotated with an existingTM using Bayesian inference methods (here: Gibbssampling).3 Method: From Words to TopicsThe underlying mechanism described here is verysimple: Instead of using words directly as featuresto characterize textual units, we use the topic IDsassigned by Bayesian inference.
LDA inference as-signs a topic ID to each word in the test documentin each inference iteration step, based on a TM es-timated on a training corpus.
We use the topic ID,lastly assigned to each word.
This might lead to in-stabilities as a word with high probabilities for sev-eral topics could be assigned to different topics indifferent inference iterations.
To avoid these insta-bilities, we save all topic IDs assigned to a word foreach inference iteration.
Finally, the most frequenttopic ID is assigned to each word.
This mechanismwe call the mode method.
Both word replacementscan be applied to most segmentation algorithms.In this work, we use this general setup to imple-ment topic-based versions of TT and C99 and de-velop a new TextTiling-based method called Topic-Tiling.4 Topic-based Segmentation Algorithms4.1 TextTiling using Topic ModelsIn TextTiling (TT) (Hearst, 1994) using topic IDs(TTLDA), a document D, which is subject to seg-mentation, is represented as a sequence of n topicIDs1.
TT splits the document into topic-sequences,instead of sentences, where each sequence consistsof w topic IDs.
To calculate the similarity betweentwo topic-sequences, called sequence-gap, TT usesk topic-sequences, named block, to the left and tothe right of the sequence gap.
This parameter k de-fines the so-called blocksize.
The cosine similarityis applied to computed a similarity score based onthe topic frequency of the adjacent blocks at eachsequence-gap.
A value close to 1 indicates a highsimilarity among two blocks, a value close to zerodenotes a low similarity.
Then for each sequence-gap a depth score di is calculated for describing thesharpness of a gap, by di = 1/2(hl(i)?si+hr(i)?si).
The function hl(i) returns the highest similarityscore on the left side of the sequence-gap index i thatdoes not increase and hr(i) returns the highest scoreon the right side.
Then all local maxima positionsare searched based on the depth scores.In the next step, these obtained maxima scores aresorted.
If the number of segments n is given as inputparameter, the n highest depth scores are used, oth-erwise a cut-off function is used that applies a seg-ment only if the depth score is larger than ?
?
?/2,where mean ?
and the standard deviation ?
are cal-culated based on the entirety of depth scores.
As TTcalculates the depth on every topic-sequence usingthe highest gap, this could lead to a segmentationin the middle of a sentence.
To avoid this, a finalstep ensures that the segmentation is positioned atthe nearest sentence boundary.4.2 C99 using Topic ModelsFor the C99 algorithm (Choi, 2000), named(C99LDA) when using topic IDs, the text is dividedinto minimal units on sentence boundaries.
A sim-ilarity matrix Sm?m is computed, where m denotesthe number of units (sentences).
Every element sijis calculated using the cosine similarity between uniti and j.
Next, a rank matrix R is computed to im-prove the contrast of S: Each element rij containsthe number of neighbors of sij that have lower simi-larity scores then sij itself.
In a final step a top-downclustering algorithm is performed to split the docu-ment into m segments B = b1, .
.
.
, bm.
This algo-1words instead of topic IDs are utilized in the original ap-proach.554rithm starts with the whole document considered asone segment and splits off segments until the stopcriteria are met, e.g.
the number of segments or asimilarity threshold.4.3 TopicTilingTopicTiling is a new TextTiling-based algorithm andis adjusted to use TMs.
As we have found in dataanalysis, it is frequently the case that a topic dom-inates within a sampling unit (sentence), and thatunits from the same segment frequently are domi-nated by the same topic.
In contrast to word-basedrepresentations, we expect no need to face sparsityissues that require smoothing methods (see TT) andranking methods (see C99), which allows us to sim-plify the algorithm.
Initially, the document is splitinto minimal units on sentence boundaries.
To mea-sure the coherence between units, the cosine similar-ity (vector dot product) between two adjacent sen-tences is computed.
Each sentences s is representedas a N -dimensional vector, where N is the numberof topics defined in the TMs.
The i-th element of thevector contains the number of times the i-th topicis observed in the sentence.
In comparison to TTwe search all local minima based on these similar-ity scores and calculate for these positions the depthscore as described in TT.
If the number of segmentsis known in advance, the segments of the n-highestdepth-scores are used, otherwise the cut-off scorecriteria used in TT is adapted.5 EvaluationAs laid out in Section 3, a LDA Model is estimatedon a training dataset and used for inference on thetest set.
To ensure that we do no use informa-tion from the test set, we perform a 10-fold CrossValidation (CV) for all reported results.
To reducethe variance of the shown results, derived by the ran-dom nature of sampling and inference, the resultsfor each fold are calculated 30 times using differentLDA models.The LDA model is trained with N=100 top-ics, 500 sampling iterations and symmetric hy-perparameters as recommended by Griffiths andSteyvers (2004)(?=50/N and ?=0.01), using JGibb-sLda (Phan and Nguyen, 2007).
For the annota-tion of unseen data with topic information, we useLDA inference, sampling 100 iterations.
Inferenceis executed sentence-wise, since sentences form theminimal unit of our segmentation algorithms and wecannot use document information in the test setting.The performance of the algorithms is measured us-ing Pk and WindowDiff (WD) metrics (Beefermanet al, 1999; Pevzner and Hearst, 2002).
The C99 al-gorithm is initialized with a 11?11 ranking mask, asrecommended in Choi (2000).
TT is configured ac-cording to Choi (2000) with sequence length w=20and block size k=6.5.1 Data SetFor evaluation, we rely on the Choi data set (Choi,2000), which has been used in several other text seg-mentation approaches to ensure comparability.
Thisdata set is generated artificially using the Brown cor-pus and consists of 700 documents.
Each docu-ment consists of 10 segments.
For its generation,3?11 sentences are sequentially extracted from arandomly selected document and merged together.While our CV evaluation setting is designed to avoidusing the same documents for training and testing,this cannot be guaranteed as the segments within thedocuments generated by Choi are included in sev-eral documents.
This problem also occurs in otherapproaches, but has not be described in (Fragkou etal., 2004; Misra et al, 2009; Galley et al, 2003),where parts or the whole dataset are used for train-ing either TF-IDF values or topic models.5.2 ResultsFor the experiments the C99 and TT implementa-tions2 are executed in two settings: using words andusing topics.
When using words, TT and C99 usestemmed words and filter out words using a stop-word list.
C99 additional removes words using pre-defined regular expressions.
In the case of topic IDs,no stopword filtering was deemed necessary.
Table1 shows the result of the different algorithms with allcombination of provided segment number and usingthe mode method.We note that WD values are always higher thanthe Pk values, and these measures are highly corre-lated.
First we discuss results for the setting withnumber of segments provided (see column 2-5 of2We use the implementations by Choi available at http://code.google.com/p/uima-text-segmenter/.555Method Segments provided Segments unprovidedmode=false mode=true mode=false mode=truePk WD Pk WD Pk WD Pk WDC99 11.20 12.07 12.73 14.57C99LDA 4.16 4.89 2.67 3.08 8.69 10.52 3.24 4.08TT 44.48 47.11 49.51 66.16TTLDA 1.85 2.10 1.04 1.18 16.41 21.40 2.89 3.67TopicTiling 2.65 3.02 2.12 2.42 4.12 5.75 2.30 3.08TopicTiling 1.50 1.72 1.06 1.21 3.24 4.58 1.39 1.84(filtered)Table 1: Results by segment length for TT withwords and topics (TTLDA), C99 with words and topics(C99LDA) and TopicTiling using all sentences and usingonly sentences with more then 5 word tokens (filtered).Table 1).
A significant improvement for C99 andTT can be achieved when using topic IDs.
In caseof C99LDA, the error rate is at least halved and forTTLDA the error rate is reduced by a factor of 20.Using the most frequent topic ID assigned duringthe Bayesian inference (mode method) reduces theerror rates further for the TM-based approaches, asthe probability for randomly assigned topic IDs isdecreased.
The newly introduced algorithm Top-icTiling as described above does not improve overTTLDA.
Analysis revealed that the Choi corpus in-cludes also captions and other ?non-sentences?
thatare marked as sentences, which causes TopicTil-ing to introduce false positive segments since thetopic vectors are too sparse for these short ?non-sentences?.
We therefore filter out ?sentences?
withless than 5 words (see bottom line in Table 1).This leads to errors values that are close to the re-sults achieved with TTLDA when the mode is used.When the number of segments is not given in ad-vance (see columns 6-9 in Table 1), we again ob-serve significantly better results comparing topic-based methods to word-based methods.
But the er-ror rates of TTLDA are unexpectedly high when themode method is not used.
We discovered in dataanalysis that TT estimates too many segments, as thetopic ID distributions between adjacent sentenceswithin a segment are often too diverse, especiallyin face of random fluctuations from the topic assign-ments.
Estimating the number of segments is betterachieved using TopicTiling instead of TTLDA.In Table 2, we compare TTLDA, C99LDA andour TopicTiling algorithm to other published resultson the same dataset.
We can see that all introducedtopic-based methods outperform the yet best pub-Method Segmentsprovided unprovidedTT 44.48 49.51C99 11.20 12.73U00 (Utiyama and Isahara, 2001) 9 10F04 (Fragkou et al, 2004) 5.39M09 (Misra et al, 2009) 2.73C99LDA (mode = true) 2.67 3.24TTLDA (mode=true) 1.04 2.89TopicTiling (mode=true, filtered) 1.06 1.39Table 2: List of lowest Pk values for the Choi data set fordifferent algorithms in the literature.lished M09 algorithm (Misra et al, 2009).
Theimprovements of C99, TTLDA and TopicTiling incomparison to M09 are significant3.TopicTiling and TTLDA are computationallymore efficient than M09.
Whereas our linear methodhas a complexity of O(T ) (T is the number ofsentences), dynamic algorithms like M09 have acomplexity of O(T 2) (cf.
Fragkou et al (2004)),which also applies to the number of topic inferenceruns.
When the number of segments is not givenin advance, TopicTiling outperforms TTLDA sig-nificantly.
As an additional benefit, TopicTiling iseven simpler than TT, as no smoothing parameter isneeded and the depth scores are only calculated forthe minima of the similarity scores.6 ConclusionThe method introduced in this paper shows that us-ing semantic information, provided by TMs, can im-prove existing algorithm significantly.
This is at-tested modifying the algorithm TT and C99.
WithTopicTiling a new simplistic topic based algorithmis developed that can produce state-of-the-art resultsbased on the Choi corpus and outperform TTLDAwhen the number of segments is unknown.
Addi-tionally this method is computationally more effi-cient in comparison to other topic based segmenta-tion algorithms.
Another contribution is the modemethod for stabilizing topic ID assignments.7 AcknowledgmentsThis work has been supported by LOEWE as part ofthe research center ?Digital Humanities?.
We wouldlike to thank the anonymous reviewers for their com-ments, which truly helped to improve the paper.3using a one sampled t-test with ?
= 0.05556ReferencesDoug Beeferman, Adam Berger, and John Lafferty.1999.
Statistical models for text segmentation.
Ma-chine learning, 34(1):177?210.David M. Blei, Andrew Y Ng, and Michael I. Jordan.2003.
Latent Dirichlet Allocation.
Journal of Ma-chine Learning Research, 3:993?1022.Freddy Y. Y. Choi.
2000.
Advances in domain inde-pendent linear text segmentation.
In Proceedings ofthe 1st North American chapter of the Association forComputational Linguistics conference, pages 26?33,Seattle, WA, USA.Jacob Eisenstein.
2009.
Hierarchical text segmenta-tion from multi-scale lexical cohesion.
In Proceedingsof Human Language Technologies: The 2009 AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics, pages 353?361, Boulder, CO, USA.Pavlina Fragkou, Vassilios Petridis, and Athanasios Ke-hagias.
2004.
A Dynamic Programming Algorithmfor Linear Text Segmentation.
Journal of IntelligentInformation Systems, 23(2):179?197.Michel Galley, Kathleen McKeown, Eric Fosler-Lussier,and Hongyan Jing.
2003.
Discourse segmentationof multi-party conversation.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics, volume 1, pages 562?569, Sapporo,Japan.Thomas L. Griffiths and Mark Steyvers.
2004.
Find-ing scientific topics.
Proceedings of the NationalAcademy of Sciences, 101:5228?5235.M A K Halliday and Ruqaiya Hasan.
1976.
Cohesion inEnglish, volume 1 of English Language Series.
Long-man.Marti A. Hearst.
1994.
Multi-paragraph segmentationof expository text.
In Proceedings of the 32nd annualmeeting on Association for Computational Linguistics,pages 9?16, Las Cruces, NM, USA.P.-Y.
Hsueh, J. D. Moore, and S. Renals.
2006.
Auto-matic segmentation of multiparty dialogue.
AMI-156.Hemant Misra, Joemon M Jose, and Olivier Cappe?.
2009.Text Segmentation via Topic Modeling : An Analyti-cal Study.
In Proceeding of the 18th ACM Conferenceon Information and Knowledge Management, pages1553?1556, Hong Kong.Lev Pevzner and Marti A. Hearst.
2002.
A critique andimprovement of an evaluation metric for text segmen-tation.
Computational Linguistic, 28(1):19?36.Xuan-Hieu Phan and Cam-Tu Nguyen.
2007.
Gibb-sLDA++: A C/C++ implementation of latent Dirichletallocation (LDA).
http://jgibblda.sourceforge.net/.Qi Sun, Runxin Li, Dingsheng Luo, and Xihong Wu.2008.
Text segmentation with LDA-based Fisher ker-nel.
Proceedings of the 46th Annual Meeting of theAssociation for Computational Linguistics on HumanLanguage Technologies, pages 269?272.Masao Utiyama and Hitoshi Isahara.
2001.
A statisti-cal model for domain-independent text segmentation.In Proceedings of the 39th Annual Meeting on Associ-ation for Computational Linguistics, pages 499?506,Toulouse, France.Yaakov Yaari.
1997.
Segmentation of expository textsby hierarchical agglomerative clustering.
In Proceed-ings of the Conference on Recent Advances in NaturalLanguage Processing, Tzigov Chark, Bulgaria.557
