The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 22?32,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsAutomatic Grading of Scientific InquiryAvirup SilComputer and Information SciencesTemple UniversityPhiladelphia, PAavirup.sil@temple.eduAngela SheltonCollege of EducationTemple UniversityPhiladelphia, PAangi@temple.eduDiane Jass KetelhutTeaching and Learning, Policy and LeadershipUniversity of MarylandCollege Park, MDdjk@umd.eduAlexander YatesComputer and Information SciencesTemple UniversityPhiladelphia, PAyates@temple.eduAbstractThe SAVE Science project is an attempt to ad-dress the shortcomings of current assessmentsof science.
The project has developed two vir-tual worlds that each have a mystery or natu-ral phenomenon requiring scientific explana-tion; by recording students?
behavior as theyinvestigate the mystery, these worlds can beused to assess their understanding of the scien-tific method.
Currently, however, the scoringof the assessment depends either on manualgrading of students?
written responses, or, onmultiple choice questions.
This paper presentsan automated grader that can combine withSAVE Science?s virtual worlds to provide acheap mechanism for assessments of the abil-ity to apply scientific methodology.
In experi-ments on over 300 middle school students, ourbest automated grader improves by over 50%relative to the closest system from previouswork in predicting grades supplied by humanjudges.1 IntroductionEducation researchers criticize current standardizedtests of science on many grounds.
First, they lackcontext (Behrens et al, 2007), which complicates astudent?s task of applying classroom-based learning,as the theory of situated cognition suggests (Brownet al, 1989).
Second, many have criticized suchtests for failing to engage students long enough toapply their understanding to the question.
Further-more and perhaps worst of all, standardized tests failto assess scientific inquiry?the ability of studentsto apply the scientific method?authentically ratherthan as scientific content (National Research Coun-cil, 2005; Singley and Taft, 1995).We consider an assessment conducted by theSituated Assessment using Virtual Environmentsfor Science Content and Inquiry (SAVE Science)project (Ketelhut et al, 2010; Ketelhut et al, 2009),whose long-term goal is to address the shortcomingsof current standardized tests of science.
The assess-ments from SAVE Science have produced an abun-dance of data on how students interact with a vir-tual world, when trying to conduct scientific inquiry.Observing student behavior in virtual environmentsoffers the potential for new insights into both howstudents learn and what they know.
However, thisbenefit can only be realized if we can make sense ofthe stream of data and text produced by the students.In this paper, we attempt to automate the processof grading students in SAVE Science assessments, tomake the evaluations as cost-effective as standard-ized tests.
Unlike most previous systems for au-tomated grading (Sukkarieh and Stoyanchev, 2009;Sukkarieh et al, 2004; Higgins et al, 2004; Wanget al, 2008), the data for this task includes a shortparagraph (usually 50-60 words) natural languageresponse stating a hypothesis and evidence in sup-port of it.
In addition, there is a wealth of relationaldata about student behavior in a virtual environment.We develop novel predictors for automatically grad-ing the written responses using a wide variety of nat-ural language features, as well as features from thedata on student behavior in the virtual world.
Onstudent data from two virtual worlds, our best auto-mated grader has correlations of r = 0.58 and 0.44with human judgments, improving over the closest22technique from previous work by 56% for the firstworld, and by 120% for the second.The rest of the paper is organized as follows.The next section contrasts this project with previ-ous work.
Section 3 describes the SAVE Scienceproject and the student data it has produced.
Section4 details our automated grading models.
Section 5reports on experiments, and Section 6 concludes.2 Previous WorkWang et al (2008) have previously conducted astudy on assessing creative problem-solving in sci-ence education by automatically grading student es-says.
Our techniques improve substantially overtheirs, as we demonstrate empirically.
In part, weimprove by including more sophisticated language-processing features in our model than the unigramand bigram features they use; as others have noted,bag-of-words representations and latent semanticindexing become less useful as word order andcausal relationships become important for judgingan essay?s quality (Malatesta et al, 2002; Wiemer-Hastings et al, 2005).
A secondary reason for ourimprovement is that we also have access to non-linguistic data about the students that we can minefor additional patterns.Most previous research on automated grading ofwritten text focuses on short, factual text (Wiemer-Hastings et al, 1999; Mohler and Mihalcea, 2009;Leacock and Chodorow, 2003; Sukkarieh and Stoy-anchev, 2009; Sukkarieh et al, 2004; Mitchell et al,2002; Pulman and Sukkarieh, 2005), whereas SAVEScience?s texts are only partly factual.
Responsesare meant to convey a scientific explanation of amystery, and therefore, correct responses contain in-ferences, observations of the world, and causal linksbetween observations and inferences.Automatic systems for grading longer responsestypically grade essays for coherence and discoursestructure (Burstein et al, 2001; Higgins et al, 2004),but these global discourse criteria are only partiallyindicative of the quality of a student?s response to theSAVE Science assessments.
To be considered fullycorrect in these tests, student responses must containfactually correct information, as well as causal rela-tionships that justify the student?s inferences, suchas ?The balls don?t bounce outside because it?s cold,and lower temperatures decrease pressure.
?3 Assessing Scientific Inquiry UsingVirtual WorldsWe now give a brief overview of SAVE Science,which aims to complement (or even replace) cur-rent standardized tests for evaluating students?
un-derstanding of science.
We first present the project?sgoals and methodology, and then describe the chal-lenges involved in creating an automated evalua-tion of student performance for this new assessmentparadigm.3.1 The SAVE Science ProjectSAVE Science (Ketelhut et al, 2010; Ketelhut et al,2009; Ketelhut et al, 2012) is a novel project forevaluating students?
understanding of the scientificmethod ?
problem identification, gathering data,analyzing data, developing a hypothesis, and com-municating results ?
by asking students to solvea mystery in a virtual world through the applica-tion of the scientific method to a content-based prob-lem.
Using immersive virtual environments for as-sessments is a current area of focus among educa-tion researchers (Clarke-Midura, 2010); SAVE Sci-ence is unique in its attempt to assess understand-ing of both inquiry as well as content.
That is, thetest is designed to assess students?
ability to applytheir knowledge of the scientific inquiry processesto a problem they have never seen before, but withina content area they have just studied.
To be suc-cessful, students must explore a virtual environment,collect appropriate data about it, and find evidencethat supports their inference about the cause of themystery.
Part of the reasoning for a particular con-clusion draws on scientific knowledge learned in theclassroom, but for these mysteries such knowledgeof scientific content is insufficient.
Students mustalso be able to explore the virtual world and create ahypothesis about the cause of the problem, based ontheir observations and analysis of collected data.For this study, we concentrate on two virtualworlds produced by the SAVE Science project team,Basketball and Weather Trouble.
Screenshots ofthe two virtual worlds are shown in Figure 1.
Stu-dents are represented by an avatar, or virtual char-acter, whom they can control in the virtual world23Figure 1: Screenshots from SAVE Science?s virtual environments.
Left: the Basketball module.
Right: the WeatherTrouble module.
The bar of icons along the bottom of the screen shows various tools that students may choose to usein the world, including a map, compass, graphing tool, note pad, and instruments like a barometer and thermometer,among others.
Glowing green arrows indicate ?objects?
(sometimes including people) with which the student?s avatarmay interact, by making observations, by taking measurements, or through conversation.with a mouse or key presses.
When the test be-gins, one character in the world informs the studentof a mystery that the student needs to explain.
Inthe Weather Trouble world, citizens of Scientopolisare concerned with the lack of rain recently, and askthe avatar to determine whether it will rain soon.
Inthe Basketball world, a basketball tournament stafferis concerned that students cannot play basketball onthe outdoor playground, because the balls will notbounce high enough outdoors, even though the sameballs bounce just fine indoors.Once informed of the mission, the student(through her or his avatar) explores the world, andinteracts with objects or other characters in the vir-tual world by ?colliding?
with them.
Interactionswith characters mostly involve the character tellingthe avatar some part of the story of the world throughtheir eyes (e.g., ?It hasn?t rained here in weeks; Ihope it rains soon!?).
The conversation may yielduseful clues, or it may be ?folk science?
(e.g., ?Thesheep are lying down, so it is probably going to rainsoon?).
When the avatar interacts with an object, thestudent can choose from a set of tools to determinemeasurements of the object.
Measurements that astudent deems interesting can be recorded in the stu-dent?s clipboard, and a graphing tool allows studentsto construct charts from the data in the clipboard.Once students have finished exploring, collect-ing data, and analyzing the data, they are asked tocommunicate the results by writing a brief expla-nation for the cause of the mystery for the world.In addition, students are asked to provide what theyconsider to be the top three pieces of evidence fortheir explanation.
Both the explanation and theranked evidence are written in freeform text, con-sisting of 48.5 words on average for Basketball, and62.4 for Weather Trouble.
We refer to the expla-nation and ranked evidence collectively as the stu-dent?s freeform response.
These texts are criticalcomponents of the overall data about the student, asthey can be used to assess the student?s ability tocommunicate findings.3.2 Assessing the ability to make scientificinquiriesThe virtual worlds from SAVE Science provide anabundance of data about each student?s ability toapply the scientific method, as well as their un-derstanding of content, but the current assessmentscheme involves either manual grading of freeformresponses, or multiple choice questions.
The firstis problematic because of the effort and expense in-volved; the second is problematic because of the dif-ficulty in designing multiple choice questions thataccurately assess everything a student has learned(Wang et al, 2008; Chang and Chiu, 2005; Singleyand Taft, 1995).
The focus of this paper is to pro-vide an automated way of assessing students?
abilityto perform scientific inquiry based on their behav-ior in the virtual world and their freeform responses.We first describe the current assessment mechanismsavailable in SAVE Science?s data, which we then use24Score Criteria4 Provides a correct hypothesis with supportingdata gathered from within the world3 Provides a correct hypothesis with only folkor incorrect evidence2 Provides a somewhat correct answer1 Provides a hypothesis0 No hypothesis, or nonsenseTable 1: Rubric for manual scoring of freeform re-sponses.Score Example3 it?s because the air outside is more colderthan the air inside here the cold air causesthe air molecules to gather up toghter tighttoghter causeing the ball to deflate and haveless bounce .
.
.1 the wieght isnt up to regulations but the bouceis ok everyball i bouce it bouced accordingto regulartion but almost every ball has theweight of 1.25 .
.
.Figure 2: Example portions of two freeform responsesfrom Basketball, presented as written by the students.below as gold standards for automated predictors forassessment.Manual grading of the freeform responses uses arubric of integer scores from 0 to 4.
Guidelines forthe rubric scores are shown in Table 1, and two ex-ample responses are shown in Figure 2.
Two anno-tators, the first holding a PhD in education and thesecond a PhD student in computer science, indepen-dently judged each response, achieving a high inter-annotator agreement ?
for Basketball, Cohen?s ?
=0.95, Pearson?s ?
= 0.98; and for Weather Trouble?
= 0.8, ?
= 0.93.
For our experiments, we usethe judgments of the first annotator, who helped de-sign the virtual worlds and has experience in gradingstudent essays, but the choice of which annotator?sjudgments to use makes little difference to the re-sults.The multiple choice questions, which we call quizquestions, consist of two types, as shown in Table2.
The first type, which we call contextualized ques-tions, directly test students?
understanding of the sci-entific issues that arise in the virtual environmentof the module.
Non-contextualized questions are re-lated to the topic of the module, but they can be an-swered correctly using general scientific knowledgerather than specific knowledge gleaned from explo-ration of the virtual world.
The non-contextualizedquestions are taken from the benchmark exams of amajor urban school district.4 Predictors for Scientific Inquiry GradesWe now focus on the task of building automated pre-dictors for assessing students?
ability to make scien-tific inquiries.
To do this, we turn the grading taskinto a classical machine learning problem, in whichthe system must learn from a set of training data(students and their grades) how to predict a gradefor new students included in separate test data.
Wefocus on two main types of models: ones that cangrade by predicting how many multiple-choice ques-tions (contextualized, non-contextualized, or both)a student will answer correctly, and ones that canpredict the manual grade assigned to a freeform re-sponse.Unlike typical automated-grading systems forgrading written or spoken natural language, our taskincludes a large additional source of evidence for thepredictions: data about the students?
behavior in thevirtual world.
Our prediction models therefore makeextensive use of both the freeform response and datafrom the students?
behavior in the world, which werefer to as world data.4.1 ModelsWe use Support Vector Machines with Radial Ba-sis Function kernels (RBF-SVM) (Pang-Ning et al,2006; Smola and Scho?lkopf, 1998) for learningnon-linear regression models of grading.
Let S bethe set of students evaluated through SAVE Sci-ence?s virtual environment, and let f : S ?
Rn bea vector-valued feature function providing n real-valued features for each student, based on the stu-dent?s freeform response and behavior in the virtualworld.
Let g : S ?
R be the target grading func-tion, which provides a real-valued grade for eachstudent.
The hypothesis spaceH for RBF-SVMs in-cludes functions h : S ?
R of the formh(s) =m?i=1?iK(xi, f(s)) + b (1)25Contextualized Questions Non-Contextualized QuestionsWhat variable would you change to cor-rect this basketball problem?1.
TemperatureA.
Make it 75?FB.
Make it 55?FC.
Make it 35?F2.
Court TypeA.
Concrete onlyB.
Wood onlyC.
Court Type makes little to no differ-ence3.
Basketball usedA.
Replace one Wade Park ball with oneJordan Gym ballB.
Purchase a new set of balls for WadeParkC.
New basketballs will not help thisproblem1.
A child riding a bicycle notices that the tires are more in-flated on hot days than on cold days, even though no air isbeing added or removed.
How can this be explained?A.
A higher temperature of the air in the tires causes the par-ticles in the air to stick together and take up more space.B.
A higher temperature of the air in the tires causes the num-ber of particles in the air to increase.C.
A higher temperature of the air in the tires causes the pres-sure of the air to drop and the volume of the air to increase.D.
A higher temperature of the air in the tires causes boththe pressure and volume of the air to increase.2.
A sample of oxygen is being stored in a closed containerat a constant temperature.
What will happen to the gas ifit is transferred to a container with a smaller volume?A.
Its weight will increaseB.
Its weight will decreaseC.
Its pressure will increaseD.
The size of its particles will decreaseTable 2: Complete list of Basketball contextualized and non-contextualized quiz questions.
Bold indicates the correctanswer.where the xi are the support vectors, and K is theRBF kernel function, given by:K(x,x?)
= exp(???x?
x?
?2) (2)Here, ?i, b, ?
?
R are parameters to be learned fromthe training data.
We use the Weka (Hall et al, 2009)toolkit for running standard training and predictionalgorithms with the SVM.We train models for four distinct prediction tasks,each defined by a different grading function g(s):1) g(s) is the manually-assessed grade on stu-dent s?s freeform responses; 2) g(s) is the num-ber of correctly-answered contextualized questions;3) g(s) is the number of correctly-answered non-contextualized questions; and 4) g(s) is the totalnumber of correctly-answered quiz questions (thesum of g(s) from 2 and 3).
We use the same featurefunction f for all models, which we describe next.4.2 World FeaturesFrom the database that records a student?s activity inthe immersive virtual environment, we extract fea-tures describing the frequency and types of activi-ties in which students engaged.
For both modules,we include features for the number of object interac-tions, the number of distinct objects interacted with,the total number of measurements made, the numberof measurements saved in the student?s clipboard,and the number of graphs made.
We also includemodule-specific features: for example, in the Bas-ketball assessment module, we counted how manydistinct basketballs were interacted with, how manymeasurements were made using each type of toolavailable in the Basketball world, whether a givenstudent created graphs of temperature inside vs. out-side, or graphs of temperature vs. pressure, etc.
Intotal, the model contains 69 world features in theWeather module, and 65 in the Basketball module.All features conform to the pattern of counts overparticular types of actions the avatar might take.
Wecall the features from the virtual environment worldfeatures.We note that the relational data in this world islarge and complex, containing temporal and sequen-tial information which these features currently ig-nore.
This feature set serves as an initial explorationof the world data, but we fully expect that future in-vestigation will improve on this representation.
For26this paper we are primarily interested in features ofthe freeform responses, which we now turn to.4.3 Natural Language FeaturesWe investigate standard text mining features frombag-of-words representations and Latent SemanticAnalysis, as well as a variety of features tailored tothe grading task.
Spelling is a major problem forthis type of prediction task, but spelling-correctorsare investigated elsewhere (Kernighan et al, 1990)and are not a focus of this research.
We thereforemanually corrected spelling errors throughout thetexts before extracting features and conducting ex-periments.
No correction of grammar or punctuationwas performed.4.3.1 Latent Semantic Analysis FeaturesAfter removing 34 common stopwords, weextract a bag-of-words representation from thefreeform responses (Manning and Schu?tze, 1999).We apply Latent Semantic Analysis (LSA) (Lan-dauer and Dumais, 1997; Steyvers and Griffiths,2006) to this set of features to produce a smallerset of 72 latent features for Basketball, and 94 forWeather Trouble, based on a threshold of retaining90% of the variance in the data.4.3.2 Features from Hidden Markov ModelsLSA and other topic models identify latent struc-ture based on document-level cooccurrence statis-tics, but the ?documents?
in our data are short fortopic-modeling purposes, and we have less than200 of them for each world.
As a result, stan-dard topic modeling techniques may have difficultyidentifying the appropriate structure.
We thereforealso consider Hidden Markov Models (HMMs) (Ra-biner, 1989), generative models which rely both oncooccurrence within a sentence and on sequence in-formation for determining model parameters.
Fol-lowing recent work by Huang et al (2011) onusing HMMs to build representations, we esti-mate parameters for a fully-connected HMM with100 latent states over the freeform responses us-ing Expectation-Maximization.
We then decode theHMM over the corpus to produce a Viterbi-optimallatent state for each word.
Finally, we use counts ofthese 100 latent states to produce 100 new featuresfor each freeform response.4.3.3 Detecting disengagementA small number of students show little enthusi-asm for the test, and their responses and general per-formance are quite poor.
Often their freeform re-sponses are short, or they repeat the same text mul-tiple times.
We include three features that help iden-tify such cases: the overall length of the response,the number of times a full sentence is repeated ex-actly, and the number of tokens that are repeatedacross multiple sentences.4.3.4 Ngram and Pattern FeaturesWhile HMM and LSA features help combat spar-sity in the predictive model, they may ignore thestrong signal from a few expressions that are par-ticularly important for a domain.
By soliciting ad-vice from domain experts, we selected importantunigrams, bigrams, and trigrams for each module,and created features that count each of these.
Like-wise, we selected important two-word and three-word sets, which we call loose patterns, that weaklyindicate that a student understood the problem, ifthey all occur in the same response but not neces-sarily near one another.
Again, these words were se-lected as a result of combination of empirical obser-vations and expert domain knowledge from design-ers.
For instance, if a response contains the threewords ?temperature,?
?pressure,?
and ?because,?
itwould match one of these loose patterns.
For eachpattern, we create a feature to count the number ofmatches in a response.The selected patterns and ngrams both consist ofthree kinds of words: ones that indicate types ofmeasurable phenomena or properties (e.g., ?temper-ature?
), locations (e.g., ?outside?
), or causal or com-parative words (e.g., ?causes,?
?higher,?
?than,?
or?decrease?).
Because the responses discuss numer-ical observations like temperature and pressure val-ues, we also allow a wildcard for matching any num-ber as part of the loose patterns.4.3.5 Semantic FeaturesWe use the Senna1 semantic role labeling (SRL)system (Collobert et al, 2011) to automatically iden-tify predicate-argument relationships in the freeformresponses.
In general, the SRL system is only able1http://ml.nec-labs.com/senna/27to identify predicate-argument structures in well-crafted sentences, which on its own is a good indi-cator that the student will do well in the evaluation.In addition, we extract semantic features (SFs) thatcount how often certain predicate-argument struc-tures appear which are indicative of a good answer:SF1 Count how often the freeform response con-tains any predicate.SF2 Count how often the response contains predi-cates that involve causality, such as ?causes?
orchange-of-value predicates like ?increase.
?SF3 Count how often measurement words (e.g.,temperature, pressure) appear as arguments toany predicate.SF4 Count how often measurement words appear asarguments to the predicates related to causality.4.4 Feature SelectionWe perform feature selection using a correlation-based technique that tries to identify maximally-relevant and minimally-redundant features (Hall,1998; Deng and Moore, 1998).
The algorithm eval-uates the value of a subset of features by consideringthe individual correlation between each feature andthe gold standard, as well as the correlation betweenfeatures.
We use the default parameter settings forfeature selection, as specified in Weka.5 Experiments5.1 Experimental SetupWe use a dataset collected by the SAVE Scienceproject, consisting of the world data, freeform re-sponses, and quiz answers from public middle-school students in a major urban area of the UnitedStates.
120 students completed the Weather Trou-ble module, and 184 students completed Basket-ball.
After manually correcting spelling errors inthe freeform responses, we extracted features as de-scribed above.Following Wang et al (2008), we evaluate our re-gression models using Pearson correlation betweenthe predicted outcome and the gold standard out-come.
Four different gold standards are consid-ered for each module: manually-assigned grades forthe freeform text, and three versions of the num-ber of correctly-answered quiz questions (contextu-alized only, non-contextualized only, and all).
Weuse a ?2 test with a threshold of p < 0.05 to deter-mine statistical significance.
We train and test mod-els using 10-fold cross-validation to reduce variabil-ity, and the results are averaged over the folds.We evaluate several variants of our system, in-cluding a World variant that only includes featuresfrom the world data; an NLP variant that only in-cludes features from the freeform responses; and acombined World+NLP variant that includes all fea-tures before feature selection is performed.Our evaluation compares against the essay grad-ing technique by Wang et al Like ours, their sys-tem uses RBF-SVM regression with default param-eter settings as implemented in Weka, and like oursthe system is trained on student texts proposing so-lutions to a science problem (in their case, a highschool chemistry problem).
The system is trainedon human judgments of the quality of the studentanswers.
The major difference between our tech-nique and theirs lies in the representation of the data;Wang et al use two types of features: unigrams, andbigrams that occur at least five times during train-ing.
In our implementation of their technique, weuse a lower threshold for bigrams ?
they must oc-cur at least twice.
This is because we have less textto work with, and the higher threshold yields toofew bigrams.
Using the lower threshold improvedperformance slightly, so we report only those resultsbelow.5.2 Results and DiscussionThe full system for automatic grading is accurate,across both worlds and all gold standards.
Figure3 shows the results of predicting human judgmentsof the freeform responses, where the World+NLPsystem achieves a correlation of 0.58 for Basket-ball and 0.44 for Weather Trouble.
The same sys-tem achieves 0.55 and 0.54 on the World ques-tions of Basketball and Weather Trouble, respec-tively (Figures 4 and 5).
Our best models are sta-tistically significantly different from the Wang et almodel (for predicting contextualized questions forbasketball: p = .009, ?2 = 6.87162; for gradingfreeform responses: p ?
0, ?2 = 14.21725).
Cor-relations from World+NLP for other quiz types ?280.260.370.58 0.580.15 0.200.43 0.440.00.10.20.30.40.50.60.7World Wang et al NLP World+NLPCorrelationCoefficientCorrelation: Predicted vs Human Rubric ScoresBasketballWeatherFigure 3: Our NLP features dramatically improve predic-tion over the Wang et al model for grading freeform sci-ence essays, by a margin of 0.21 on Basketball and 0.23on Weather Trouble.0.33 0.340.450.550.09  0.120.34 0.400.200.330.40 0.4600.10.20.30.40.50.6Wang et al World NLP World+NLPCorrelationCoefficientAutomatic Grading  of Basketball Quiz AnswersContextualized Non-contextualized AllFigure 4: The World+NLP model outperforms bothWorld and NLP, and substantially outperforms the Wanget al system.non-contextualized and all questions ?
were some-what lower, but still statistically significant (p =.002, ?2 = 10.05986).The language features are currently the major fac-tor in the predictive models for automated grad-ing.
The NLP model substantially outperforms boththe simpler Wang et al model and the World-onlymodel in predicting quiz answers for both worlds.It achieves correlations that are statistically signifi-cantly different from the baseline, for all gold stan-dards and both worlds.The story in the case of grading freeform essaysis similar.
Our NLP model beats the Wang et almodel and the World-only model.
Our full modelWorld+NLP, however, outperforms the NLP modelby only a small fraction.
Also, the Wang et al modelperforms slightly better than the World-only modelon freeform responses.
For Basketball, the correla-tion coefficient of their model is greater by 0.11 andfor Weather by 0.05.
We believe that the NLP-based0.130.060.53 0.540.00 0.060.460.300.10 0.120.38 0.41-0.100.10.20.30.40.50.6Wang et al World NLP World+NLPCorrelationCoefficientAutomatic Grading  of Weather Trouble Quiz AnswersContextualized Non-contextualized AllFigure 5: The NLP model substantially outperformsWorld and Wang et al on predicting quiz questions forWeather Trouble, and the combined World+NLP modelachieves a 0.54 correlation for contextualized questions.models, including Wang et al?s, are outperformingthe World model because the current representationof the World data fails to capture all of the pertinentinformation from students?
behavior in the virtualenvironments.
Our plans for future work include thedevelopment of features that can capture temporalpatterns in student activity.Each type of language feature appears to pro-vide a beneficial and complementary source of ev-idence.
We tested the model using only individualsubsets of the NLP features, such as HMM featuresonly, LSA features only, ngrams and loose patternsonly, and features from semantic role labeling only.On their own, each set of features provides only asmall improvement over the mean predictor.
Whencombined with the world features, each subset ofthe NLP features again provides only a small im-provement over the World-only model.
For exam-ple, for predicting Basketball world quiz questions,World features achieve r = 0.34, World+HMM andWorld+LSA achieve 0.35, and World+(ngrams andloose patterns) achieves 0.39.
The relative rankingof these subsets of features is not consistent acrossdifferent tasks; for Weather contextualized ques-tions, World+HMM is best, and for Weather non-contextualized questions, World+LSA is best.
Fea-tures selected by the feature selection algorithm alsoindicate that the different types of language featurescomplement one another.
The feature selection al-gorithm for the World+NLP model selects some fea-tures for every different type we presented, althoughthe HMM, LSA, loose pattern, and unigram fea-tures dominate.
We believe that the best procedure29for developing grading systems for science essaysis therefore to construct a large number of possiblefeatures using a variety of techniques, and then traina model for a particular task and gold standard.
In-cluding significantly more varieties of features, per-haps from additional kinds of language models orNLP pipeline tools, is an important future directionfor further improving the grading accuracy.While the accuracies of the models for contextu-alized and non-contextualized questions are broadlysimilar, the models themselves are not.
For the con-textualized questions, 4 important world behaviorfeatures were deemed important and non-redundantby the feature selection algorithm: the number ofdistinct collisions, the number of people collidedwith, the number of distinct objects (basketballs orballoons) whose pressure was measured, and thenumber of distinct temperature measurements thatwere recorded into clipboards.
The essential taskin this virtual world is to discover that a decreasein the temperature of several gas systems (basket-balls and balloons filled with air) is causing theirpressure to decrease.
The model for the contex-tualized questions thus includes variables that arehighly relevant to a student?s understanding of thecore problem in the world, which in turn indicatesthat automated data mining techniques are capableof identifying when students are learning to prac-tice the scientific method, by observing student be-havior.
On the other hand, the model for the non-contextualized questions includes only 2 world fea-tures: The number of collisions made and numberof different objects whose circumference was mea-sured.
The first one is an indicator of the activitylevel of a student and the second variable is an indi-cator for whether the student has identified the prob-lem (the basketballs are not bouncing because theyare deflated), but not for the underlying cause ofthe problem (the outside temperature causes a dropin pressure, which causes the basketball circumfer-ence to decrease).
Thus the model that predicts non-contextualized questions very accurately has littleinformation about whether the student understoodthe core problem of the world or not; instead, it hasinformation about whether the student is active inthe world.
These observations lend some support tothe criticism that the standardized tests are not prop-erly assessing inquiry.Performance on the Weather Trouble module isconsistently lower than on Basketball.
In part, thisreflects the increased difficulty of this world; humaninter-annotator agreement is a bit lower (?
= 0.8vs.
0.95 on Basketball).
However, another largepart of the difference is that the world features pro-vide far less information in Weather Trouble ?
theWorld-only model has less than half the correlationon Weather than on Basketball, for all quiz ques-tion types.
We suspect that the cause is the natureof the task on the Weather Trouble world, wheretemporal information plays a bigger role as measure-ments of air pressure and wind direction may changeover time.
Investigating world features that can dis-tinguish different patterns of student behavior overtime is an important area for further investigation.6 ConclusionOur automated grader uses a wide variety of NLPpipeline tools to produce features for students?
es-says on the answers to scientific mysteries.
Thegrader achieves significant correlation with humanjudges and multiple choice quiz evaluations, sub-stantially outperforming a simpler grader from priorwork.
The findings of this research suggest that au-thentic assessments of scientific inquiry through vir-tual environments can be graded purely automati-cally, like high stakes multiple choice tests.
Ongoingwork on SAVE Science is investigating the differ-ences in how students respond to standard multiple-choice tests and tests based on virtual environments.But the contextualized assessments from SAVE Sci-ence provide evaluation of scientific inquiry thatmultiple choice tests currently do not, and they cannow be graded just as cheaply.AcknowledgmentsThis material is based upon work supportedunder National Science Foundation Grant No.0822308/1157534.
Any opinions, findings, and con-clusions or recommendations expressed in this ma-terial are those of the authors and do not necessarilyreflect the views of the NSF.
We would like to thankCatherine Schifter and the rest of the SAVE Sci-ence team for their help and support.
We also wishto thank the anonymous reviewers for their helpfulcomments.30ReferencesJ.
T. Behrens, D. Frezzo, R. Mislevy, M. Kroopnick, andD.
Wise.
2007.
Structural, Functional, and SemioticSymmetries in Simulation-based Games and Assess-ments.
In E. Baker, J. Dickieson, W. Wulfeck, andH.
O?Neil, editors, Assessment of Problem Solving Us-ing Simulations.
Lawrence Erlbaum Associates.J.
S. Brown, A. Collins, , and P. Duguid.
1989.
Situatedcognition and the culture of learning.
Educational Re-searcher, 18(1):32?41.J.
Burstein, C. Leacock, and R. Swartz.
2001.
Auto-mated evaluation of essays and short answers.
In 5thInternational Computer Assisted Assessment Confer-ence.
Loughborough University.S.-N. Chang and M.-H. Chiu.
2005.
The development ofauthentic assessment to investigate ninth graders sci-entific literacy: In the case of scientific cognition con-cerning the concepts of chemistry and physics.
Inter-national Journal of Science and Mathematics Educa-tion, 3:117?140.J.
Clarke-Midura.
2010.
The Role of Technology inScience Assessments.
Better: Evidence-based Edu-cation, 3(1).R.
Collobert, J. Weston, L. Bottou, M. Karlen,K.
Kavukcuoglu, and P. Kuksa.
2011.
Natural lan-guage processing (almost) from scratch.
Journal ofMachine Learning Research, 12:2493?2537.Kan Deng and Andrew Moore.
1998.
On the greedinessof feature selection algorithms.
In Proc.
InternationalConference on Machine Learning (ICML), June 1998.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA Data Mining Software: An Update.SIGKDD Explorations, 11(1).M.
A.
Hall.
1998.
Correlation-based feature subsetselection for machine learning.
In Hamilton, NewZealand.D.
Higgins, J. Burstein, D. Marcu, and C. Gentile.
2004.Evaluating multiple aspects of coherence in studentessays.
In Proceedings of the annual meeting of theNorth American Chapter of the Association for Com-putational Linguistics, Boston, MA.Fei Huang, Alexander Yates, Arun Ahuja, and DougDowney.
2011.
Language Models as Representationsfor Weakly Supervised NLP Tasks.
In Conference onComputational Natural Language Learning (CoNLL).Mark D. Kernighan, Kenneth W. Church, and William A.Gale.
1990.
A spelling correction program based ona noisy channel model.
In Proceedings of the 13thConference on Computational Linguistics, pages 205?210.D.J.
Ketelhut, B. Nelson, and C. Schifter.
2009.
VirtualEnvironments for Situated Science Assessment.
InProceedings of the International Conference on Cog-nition and Exploratory Learning in the Digital Age,pages 507?508.D.J.
Ketelhut, B. Nelson, C. Schifter, and Y. Kim.
2010.Using Immersive Virtual Environments to Assess Sci-ence Content Understanding: The Impact of Context.In D. G. Kinshuk, J. M. Sampson, P. Spector, D. Isaas,Ifenthaler, and R. Vasiu, editors, Proceedings of theIADIS International Conference on Cognition and Ex-ploratory Learning in the Digital Age (CELDA), pages227?230.Diane Jass Ketelhut, Alexander Yates, Avirup Sil, andMichael Timms.
2012.
Applying Educational Datamining in E-learning environments.
In Section withinthe New Measurement Paradigm Report, p 47-52.T.K.
Landauer and S.T.
Dumais.
1997.
A solution toPlatos problem: The latent semantic analysis theoryof acquisition, induction, and representation of knowl-edge.
In Psychological Review, 104.C.
Leacock and M. Chodorow.
2003.
C-rater: Auto-mated scoring of short-answer questions.
In Comput-ers and the Humanities, 37(4):389405.K.I.
Malatesta, P. Wiemer-Hastings, and J. Robertson.2002.
Beyond the short answer question with researchmethods tutor.
In Proceedings of the Intelligent Tutor-ing Systems Conference.Chris Manning and Hinrich Schu?tze.
1999.
Foundationsof Statistical Natural Language Processings.
MITPress.T.
Mitchell, T. Russell, P. Broomhead, and N. Aldridge.2002.
Towards robust computerised marking of free-text responses.
In Proceedings of the 6th InternationalComputer Assisted Assessment (CAA) Conference.Michael Mohler and Rada Mihalcea.
2009.
Text-to-textsemantic similarity for automatic short answer grad-ing.
In Proceedings of the 12th Conference of the Eu-ropean Chapter of the Association for ComputationalLinguistics, EACL.National Research Council.
2005.
America?s Lab Re-port: Investigations in High School Science.
NationalAcademies Press.T.
Pang-Ning, M. Steinbach, and V. Kumar.
2006.
Intro-duction to Data Mining.
Pearson Addison-Wesley.S.G.
Pulman and J.Z.
Sukkarieh.
2005.
Automaticshort answer marking.
In Proceedings of the Secondworkshop on Building Educational Applications UsingNLP.Lawrence R. Rabiner.
1989.
A tutorial on hiddenMarkov models and selected applications in speechrecognition.
Proceedings of the IEEE, 77(2):257?285.M.K.
Singley and H.L.
Taft.
1995.
Open-endedapproaches to science assessment using computers.Journal of Science Education and Technology, 4(1):7?20.31A.
Smola and B. Scho?lkopf.
1998.
A tutorial on supportvector regression.
Technical report, Royal HollowayCollege, University of London, UK.Mark Steyvers and Tom Griffiths.
2006.
Probabilistictopic models.
In T. Landauer, D. McNamara, S. Den-nis, and W. Kintsch, editors, Latent Semantic Analy-sis: A Road to Meaning, pages 427?448.
LawrenceErlbaum Associates.J.
Sukkarieh and S. Stoyanchev.
2009.
Automatingmodel building in C-rater.
In Proceedings of the 2009Workshop on Applied Textual Inference, pages 6169,Suntec, Singapore, August.J.Z.
Sukkarieh, S.G. Pulman, and N. Raikes.
2004.
Auto-marking 2: An update on the ucles-oxford universityresearch into using computational linguistics to scoreshort, free text responses.
In International Associationof Educational Assessment, Philadephia.H-C. Wang, C-Y.
Chang, and T-Y Li.
2008.
Assessingcreative problem solving with automated text grading.In Computers and Education.P.
Wiemer-Hastings, K. Wiemer-Hastings, andA.
Graesser.
1999.
Improving an intelligent tu-tors comprehension of students with latent semanticanalysis.
In Artificial Intelligence in Education, pages535542.P.
Wiemer-Hastings, E. Arnott, and D. Allbritton.
2005.Initial results and mixed directions for research meth-ods tutor.
In AIED2005 - Supplementary Proceedingsof the 12th International Conference on Artificial In-telligence in Education, Amsterdam.32
