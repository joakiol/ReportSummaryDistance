DIAGNOSIS AS A NOTION OF GRAMMARMitchel l  MarcusArt i f ic ia l  Intel l igence LaboratoryM.I.T.This paper will sketch an approach tonatural language parsing based on a newconception of what makes up a recognit iongrammar for syntactic analysis and how sucha grammar should be structured.
This theoryof syntactic analysis formalizes a notionvery much like the psychologist 's  notion of"perceptual strategies" \[Bever "70\] andmakes this formal ized notion - which will becalled the notion of wait -and-seediagnostics - a central and integral part ofa theory of what one knows about thestructure of language.
By recognit iongrammar, we mean here what a speaker of alanguage knows about that language thatallows him to assign grammatical  structureto the word strings that make up utterancesin that language.This theory of grammar is based on thehypothesis that every language user knows aspart of his recognit ion grammar a set ofhighly specif ic diagnostics that he uses todecide determinist ica l ly  what structure tobuild next at each point in the process ofparsing an utterance.
By determinist ica l i?I mean that once grammatical  structure isbuilt, it cannot be discarded in the normalcourse of the parsing process, i.e.
that no"backtracking" can take place unless thesentence is consciously perceived as being a"garden path".
This notion of grammar putsknowledge about control l ing the parsingprocess on an equal footing with knowledgeabout its possible outputs.To test this theory of grammar, aparser has been implemented that provides alanguage for writ ing grammars of this sort,and a grammar for Engl ish is current ly beingwritten that attempts to capture thewait -and-see diagnost ics needed to parseEngl ish within the constraints of thetheory.
The control  structure of the parserstrongly ref lects the assumptions the theorymakes about the structure of language, andthe discussion below wil l  use the structureof the parser as an example of theimpl icat ions of this theory for the parsingprocess.
The current grammar of Engl ish isdeep but not yet broad; this has al lowedinvest igat ion of the sorts of wait -and-seediagnost ics needed to handle complex Engl ishconstruct ions without a need to wait unti l  agrammar for the entire range of Engl ishconstruct ions could be written.
To givesome idea of the scope of the grammar, theparser is capable of handling sentenceslike:Do all the boys the l ibrarian gavebooks to want to read them?The men John wanted to be bel ieved by shothim yesterday.It should be ment ioned that certaingrammatical  phenomena are not handled at allby the present grammar, chief among themconjunct ion and certain important sorts oflexical ambiguity.
There is everyintention, however, of expanding the grammarto deal with them.Two ParadigmsTo explain exactly what the details ofthis wait -and-see (W&S) paradigm are, it isuseful to compare this notion with thecurrent prevai l ing parsing paradigm, which Iwil l  call the guess-and-then-backup (G&B)paradigm.
This paradigm is central to theparsers of both Terry' Winograd's SHRDLU\[Winograd "72\] and Bill Woods" LUNAR \[Woods"72\] systems.In a parser based on the G&B paradigm,various options are enumerated in theparser's grammar for the next possibleconst i tuent at any given point in the parseand these options are tested one at a timeagainst the input.
The parser assumestentat ively that one of these options iscorrect and then proceeds with this optionuntil  either the parse is completed or theoption fails, at which point the parsersimply backs up and tries the next optionenumerated in the parser's grammar.
This isthe paradigm of G&B: enumerate all options,pick one, and then (if it fails) backup andpick another.
While attempts have been madeto make this backup process clever,especial ly  in Winograd's  SHRDLU, it seemsthat it is very diff icult,  if not impossiblein general, to tell from the nature of thecul de sac exactly where the parser has goneastray.
In order to parse a sentence ofeven moderate complexity, there are not onebut many points at which a G&B parser mustmake guesses about what sort of structure toexpect next and at all of these points thecorrect hypothesis must be found before theparse can be successful ly  completed.Furthermore, the parser may proceedarb i t rar i ly  far ahead on any of thesehypotheses before discover ing that thehypothesis  was incorret, perhapsinval idat ing several other hypothesescontingent upon the first.
In essence, theG&B paradigm considers the grammar of anatural  language to be a t ree-structuredspace through which the parser must blindly,though perhaps cleverly, search to find acorrect parse.The W&S paradigm rejects the notion ofbackup as a standard control mechanism forparsing.
At each point in the parsingprocess, a W&S parser wil l  only buildgrammatical  structure it is sure it can use.The parser does this by determining, by atwo part process, which of the hypothesespossible at any given point of the parse iscorrect before attempting any of them.
Theparser first recognizes the specif ics i tuat ion it is in, determined both on thebasis of global expectat ions result ing fromwhatever structure it has parsed andabsorbed, and from features of lower levelsubstructures from a l ittle ahead in theinput to which internal structure can beassigned with certainty but whose functionis as yet undetermined.
Each such s i tuat ioncan be so defined that it restrains the setof possible hypotheses to at most two orthree.
If only one hypothesis is possible,a W&S parser wil l  take it as given,otherwise it wil l  proceed to the second step!!i!
!IIIIIIIIIIIiII!Iof the determinat ion process ,  to do a~if ferential  diagnosis to decide between thecompeting hypotheses.
For each dif ferentsituation, a W&S grammar includes a seriesof easi ly computed tests that decidesbetween the competing hypotheses.
The keyassumption of  the W&S paradigm, then?
i_gsthat the structure of  natural languageprovides enough and the right information t__~o~etermine exactly what too d__oo next at  eachpoint of  ~ Parse.
There is not suff ic ientroom here to discuss this assumption; thereader is invited to read \[Marcus ~74\],which discusses this assumption at length.Th___~e Parser Itself?
To  firm up this talk of "expectations',,"situations", and the like, it it useful tosee how these notions are real ized in theexist ing W&S parsing system.
Before we cando this, it wil l  be necessary to get anoverview of the structure and operat ion ofthe parser itself.A grammar in this system is made up ofpackets of pattern- invoked demons, whichwil l  be cal led modules.
(The notion ofpacket here derives from work by ScottFahlman \[Fahlman "73\].)
The parser itselfconsists of two levels, a group level and aclause level, and any packet of modules isintended to function at one level or theo~her.
Modules at group level are intendedto work on a buffer of words and word levelstructures and to eventual ly bu i ld  grouplevel structures, such as Noun Grouos (i.e.Noun Phrases up to the head noun) and VerbGrouPs (i.e.
the verb cluster up to themain verb), which are then put onto the endof a buffer of group level structures notyet absorbed by higher level processes.Modules at clause level are intended to workon these substructures and to assemble theminto clauses.
The group buffer and the wordbuffer can both grow up to somepredetermined length, on the order of 3, 4,or 5 structures.
Thus the modules at thelevel above needn't immediately use eachstructure as it comes into the buffer; butrather can let a small number of structures"pile up" and then examine these structuresbefore deciding how to use the first ofthem.
In this sense the modules at eachlevel have a l imited, sharply constra inedlook-ahead abil ity; they can wait and seewhat sort of environment surrounds asubstructure in the buffer below beforedeciding what the higher level funct ion ofthat substructure is.
(It should be notedthat the amount of look-ahead is constra inednot only by maximum buffer length but alsoby the restr ict ion that a module may accessonly the two substructures immediatelyfol lowing the one it is current ly trying touti l ize.
This constraint  is necessarybecause the substructure about to beut i l ized at any moment may not be the firstin the buffer, for various reasons.
)Every module consists of a pattern, apretest procedure, and a body to be executedif the pattern matches and the pretestsucceeds.
Each pattern consists of anordered list of sets of features.
Asstructures are built up by the parser, theyare label led with features, where a featureis any property of a structure that thegrammar wants to be visible at a glance toany module looking even casual ly at thatstructure.
(Structures can also haveregisters attached to them, carrying morespecial ized sorts of information; thecontents of a register are pr iv i leged inthat a module can access the contents of aregister only if it knows the name of thatregister.)
A module's  pattern matches if thefeature sets of the pattern are subsumed bythe feature sets of consecut ive structuresin the appropr iate buffer, with the matchstart ing at the effect ive beginning of thebuffer.Very few modules in any W&S grammar aealways active, wait ing to be tr iggered whentheir patterns match; a module is activeonly when a packet it is in has beenactivated, i.e.
added to the set ofpresent ly active packets.
Packets areact ivated or deact ivated by the parser atthe specif ic order of individual  modules;any module can add or remove packets fromthe set of active packets if it has reasonto do so.A pr ior i ty ordering of modules providessti l l  further control.
Every module isassigned a numerical  priority, creating apart ial  order ing on the active modules.
Atany time, only the h ighest -pr ior i t ied moduleof those whose patterns match wil l  beal lowed to run.
Thus, a special  purposemodule can edge out a general purpose moduleboth of whose patterns match in a givenenvironment,  or a module to handle somelast-resort  case can lurk low in a pool ofact ive modules, to serve as default only ifno h lgher -pr ior i t ied  module responds to asituation.F irmin~ Up The Notion Of  Si tuat ionThis, in brief, is the structure of theW&S parser; now we can turn to a discussionof how this structure ref lects thetheoret ica l  f ramework discussed above.
Letus begin by recast ing a statement madeabove: In decid ing what unique course ofact ion to take at any point in the parse,the parser first recognizes a speci f icwel l -def lned s i tuat ion on the basis of acombinat ion of global expectat ions and thespecif ic  features of lower levelsubstructures which are as yet unabsorbed.It should now become clear that what itmeans to have a global expectat ion is thatthe appropr iate packet is active in theparser, and that each module is i tself  thespecial ist  for the s i tuat ion that itspacket, pattern and pretest define.
Thegrammar act ivates and deact ivates packets toref lect its global expectat ions aboutsyntact ic structures that may be encounteredas a result of what it has seen so far.
(The parser might also act ivate packets onthe basis of what some higher level processin a natural  language understanding systemtells it to expect by way of d iscoursephenomena.)
These packets often ref lectrather large scale grammatical  expectat ions;for example, the fo l lowing are some packetswithin the exist ing grammar: S imple-sentencestart, a packet of modules that parse thesubject, main verb, and init ial modif iers ofmajor clauses, determining clause type andthe like; S imple-sentence after VG, whosemodules parse objects and clause levelpreposit ional  phrases in top level clauses;WH-quest ion after VG, similar toS imple-sentence after VG, except that itsmodules are responsible for act ivelydeciding where to "replace" the groupfronted to form a WH-quest ion and also whereto replace the NP "pulled out" of a relativeclause.
A dif ferent sort of packet isNG-object o__cr To-complement,  a packet ofmodules that includes as subpacket thepacket To-complement,  whose modules assigndeep subjects to to-complements with"deleted" subjects, e.g.
"John wanted tobuy a bicycle.".
Beyond this, NG-object o__rrTo-complement includes modules which delaymodules in other active "after VG" packetsfrom assigning any NP appear ing after the VGas the object of that VG until it can bedetermined ?whether or not it is the subjectof a fol lowing to-complement - Compare "Hewanted a free pass to the movie.
",  "Hewanted a friend to see the movie.
", "Hewanted a fr iend to see the movie with.".
Itshould be noted that packets of modules,i.e.
expectat ions in our theoret icalframework, roughly correspond to states inWoods" Augmented Transit ion Network modelwith some major di f ferences: Not only dopackets correspond to much largergrammatical  chunks than the typical  usage ofATN states, but more importantly,  manypackets may be active at the same time andthe modules in these packets may stronglyinteract with each other if they so choose.The other determinant of s i tuat ion inthe theoret ical  framework is the generalfeatures of unabsorbed lower levelsubstructures.
It should be obvious thatthis corresponds to the pattern-matchbetween a module 's  pattern and thesubstructures in the buffer below it.
Thatthese lower structures are avai lable atclause level is the result of theinteract ion between the modules at grouplevel and those at clause level.
Again, fora d iscuss ion of why such substructures canbe built independently,  the reader isreferred to \[Marcus "74\].
It is animportant claim of the W&S theory thatthough these patterns are very simple and -viewed from the level above - very local,they suffice, in conjunct ion withexpectat ions ref lected by packetact ivat ions,  to so restr ict the range ofpossible options open to the parser at anypoint that a determinist ic  diagnosis ofthose options can be made.
While a module'spretest might ut i l ize very complex teststhat strongly aid in the restr ict ion ofsituation, in fact pretests seem to beneeded mainly to compute simple booleanfunctions of features tha are more complexthan the implic it  logical  conjunct iondemanded by the patterns themselves.D i f ferent iat ing Between HypothesesNow that it is clear how a s ituat ion isdefined, i.e.
how a given module becomesactive, we must consider how the correctcourse of act ion can be determined given theset of possible a l ternat ives defined by asituation, i.e.
what a module knows thatmakes it a diagnost ic special ist.
We needto invest igate in part icular what sorts oftests a module can use to dependably yield adi f ferent ial  diagnosis for the s ituat ion itis a special ist  for.A module has avai lable severaldi f ferent sorts of information fordiagnosis; it can use the syntact icinformat ion it can ferret out direct ly fromthe structures it has built, and it can askquest ions of both a crude but fast semanticmodel and a full world model.
To gathersyntact ic informat ion the parser has afaci l i ty for invest igat ing any node of anytree structure it knows about; thus it caninvest igate the features or registers of anynode it pleases.
While the parser i tselfattempts to build an annotated surfacestructure s imi lar to that built byWinograd's  SHRDLU, it converses constant lywith a case frame interpreter  that isintended to serve as interface between itand deep world model l ing.
The parser isobl iged to inform the case frame interpreteras soon as it can of information such aswhat the main verb of a given clause is,what its subject is, what its objects andpreposi t ional  phrases are.
(The case frameinterpreter  speaks the parser's language; itknows how to map the grammatical  re lat ionsthe parser deals with into its own caseroles.)
In return, the parser can ask thecase frame interpreter  for many dif ferentsorts of information: whether or not a givenNP or prepos i t iona l  phrase can fill a givengrammatical  role, given the informat ion thecase frame has so far; which of twopotent ia l  candidates the case frame wouldprefer in a given grammatical  role, and byhow much; how many slots of various sortsare sti l l  open in the case frame; etc.Furthermore,  when necessary, the parser canask precompi led f i l l - in - the-b lank quest ionsof the world model i tself  (the world modelin the current system being the author); toresolve the ambigui ty  of a phrase like "athird cup of sugar", for example, the parsermight ask the world model a precompi ledquest ion equivalent  to asking whether itmakes sense in the present discourse contextto speak of a 3rd (as opposed to I/3rd) cupof sugar.Regardless of which source or sourcesof in format ion a module uses, it wil l  oftenneed to know not merely whether  onehypothesis  or another is acceptable,  butwhich of two hypotheses is better.
A modulewil l  often need to ask the case frameinterpreter,  for example, not merely  whethersome NP is semant ica l ly  acceptable in agiven role for a given verb, but ratherwhich of two NPs is better in that role andby how much.
A good example of this is thediagnost ic  used by the module which resolvesthe quest ion of which NP serves as directand indirect object in sentences such as(i) Who did John give the book?
(ii) What did John give the elephant?
(iii) Who did John give the e lephant?
(where (iii) may be at least quest ionable inIIII!I1IIIILII11IIIIthe idiolects of some readers).
Thisdiagnostic is also a good general example ofthe sorts of diagnostic rules that a W&Sgrammar contains.
The module containingthis diagnostic is in packet WH-quest ionafter VG, active when the under ly ing role ofthe quest ion group has not yet beendetermined.
It is tr iggered when a singleNP follows the main verb of the clause,fol lowed by neither another NP nor aPreposit ional  Phrase.
The module first asksthe case frame interpreter whether more thanone slot is feasibly open for NP objects.The answer here is yes as there are twoslots open, so the module knows that it canuse both the fol lowing NP and the quest iongroup to fill these slots and it tells thecase frame interpreter  to commit itself tousing both slots.
The remaining diagnost icis essent ia l ly  this: whi le the module has amild syntact ic preference to use thefol lowing NP as indirect object, it wil laccept the question group as indirect objectif the case frame strongly prefers it to thefo l low ing  NG as IO, with a feature added tothe top level clause indicat ing that theclause is a wee bit ungrammatical ;  this isthe path taken for  (i) above.
If the caseframe does prefer the next NP to thequest ion group, the module is very happy;this is the path for (ii).
If the caseframe mildly prefers the question group tothe fol lowing NP, balancing the syntact icpreference, as in (iii), the sentence isperceived as very wierd (although a fewpeople have no trouble here), and the modulewil l  do in desperat ion what many speakersseem to do in this case - take the quest iongroup as IO on semantic grounds whi lecomplaining loudly.W&S As A Psvchological  ModelThough the diagnostic above is far morespecif ic in both appl icabi l i ty  and in detai lthan what is normal ly considered aperceptual strategy, it fi l ls much the samerole that perceptual  strategies are assumedto play.
There are many crucialdifferences, however.
Wait -and-seediagnost ics are treated as rules of grammarin the W&S theory, and the parser appl iesthem in a consistent, rule- l ike manner.Indeed, in the grammar itself  (unlike thetheoret ical  framework), no d i f ferent iat ionis made expl ic i t ly  between grammar code thatdecides what to do and grammar code thatdoes it; both are integral parts of thegrammar.
Wait -and-see diagnostics, unl ikeperceptual  strategies, in general  usesyntact ic and semantic dist inct ions togetherto diagnose the correct course of action,although the information a diagnostic seeksis usual ly very specif ic and very focused.It is also important to note that adiagnostic chooses between a set of veryspecif ic possible options, and is not at alla general  rule of thumb.But what if one of these W&Sdiagnost ics fai ls?
If the parser takes awrong turn because one of the W&Sdiagnost ics in its grammar was misled, thenthe parse cannot be successfu l ly  completedand the sentence is a "garden path" withrespect to that grammar.
In thesesituations, however, a W&S parser wil l  not"fail", in the sense of throwing up itshands and y ie ld ing no parse at all, butrather it wil l  yield up whatever structureit has constructed at the point of blockageand wil l  attempt to build whatevergrammatical  substructures it can with theremainder of the input, much as people seemto do when confronted with sentences thatare garden paths with respect to theirgrammars.
A higher level problem solvermight then use higher level grammaticalknowledge to try to diagnose the source ofthe garden path, e.g.
it might know aboutdangl ing part ic ip les and how to fix them.IThis property of W&S grammars suggestsa global test of the plausibi l i ty  of the W&Stheory as a psychological  model as well as alocal test for the adequacy of anydiagnostic within a W&S grammar: Not onlyshould an ideal W&S grammar be able to parsecorrect ly all sentences that people parsecorrectly, but it should also perceive asgarden paths exact ly those sentences thatpeople perceive as garden paths.
At the~resent stage of grammar development, itshould be added, it does not seem to bediff icult to build individual W&Sdiagnost ics that behave local ly as people doin terms of percept ions of garden paths.
Tothis extent it seems reasonable to suggestthat people may use diagnost ics that ares imi lar to the diagnost ics the W&S modelposits.One other property of the implementedW&S system is also interest ing in terms ofthe p laus ib i l i ty  of the W&S model as apsychological  model.
It turns out that thelength of time required for the parser toparse input is d irect ly proport ional  to thelength of the input; the time per wordrequired by the parser to parse any sentenceseems to vary by no more than 40% over thetime taken to parse "simple" sentences.Construct ions that do take proport ional lylonger than simple sentences do so becauseof factors such as the addit ionalcomputat ion needed by diagnost ics thatdetermine where to insert "deleted"structures and the like, but, again, thetotal increase in time per word, averagedover the entire sentence is rarely more than40% greater than simple sentences.
Thesetime relat ions are consistent with the rangeof results from psychological  exper imentsthat attempt to measure latencies forsentence comprehens ion (such as \[Wanner"74\]), a l though no detai led comparison ofthe time behavior of the parser and thatmeasured by psychological  test ing has yetbeen undertaken.
It is possible that thistime behavior wil l  change as the W&S grammargrows, but this sort of behavior would seemto be intr insic to any parser and grammarbased on the W&S model.REFERENCESBever, T.G., The cognit ive bases forl inguist ic  structures.
In Cognit ion andthe ~ of Language, J. Hayes,editor, John Wiley & Sons, 1970Fahlman, S., A Hypothes is -Frame Syste~ FQrProblems, Working Paper 57,MIT-AI Lab, 1973Marcus, M., Wait and See Strategies forParsing Natural Language, Working Paper75, MIT-AI Lab, 1974Wanner, E., , Kaplan, R. and Shiner, S.,Garden Paths i_~n Relative Clauses,unpublished, Harvard University, 1974Winograd, T., Understanding NaturalLanguage, Academic Press, 1972Woods, W.A., Kaplan, R.M.
and Nash-Webber,B., The Lunar Sciences Natural LanguageInformation System, BBN Report No.2378, 1972lOI1!IIII!
!IiJIiIIIIICOMPUTATIONAL UNDERSTANDINGChristopher K. RiesbeckI.
METHODOLOGICAL POSITIONThe problem of computat ionalunderstanding has often been broken into twosub-problems: how to syntact ical ly  analyze anatural  language sentence and how tosemant ica l ly  interpret the results of thesyntact ic analysis.
There are many reasonsfor this subdivis ion of the task, involvinghistor ical  inf luences from Americanstructural  l inguist ics and the early"knowledge-free" approaches to Art i f ic ia lIntel l igence.
The sub-divis ion has remainedbasic to much work in the area becausesyntact ic analysis seems to be much moreamenable to computat ional  methods thansemantic interpretat ion does, and thus moreworkers have been attracted developingsyntact ic analyzers first.It is my bel ief that this subdivis ionhas hindered rather than helped workers inthis area.
It has led to much wasted efforton syntact ic parsers as ends in themselves.It raises false issues, such as how muchsemantics should be done by the syntact icanalyzer and how much syntactics should bedone by the semantic interpreter.
It leadsresearchers into a l l -or -none choices onlanguage processing when they are trying todevelop complete systems.
E i ther  theresearcher tries to build a syntact icanalyzer first, and usually gets no farther,or he ignores language processingaltogether.The point to real ize is that theseproblems arise from an overemphasis on thesyntax/semant ics  dist inction.
Certa in lyboth syntact ic knowledge and semanticknowledge are used in the process ofcomprehension.
The false problems arisewhen the comprehension process i tself  issect ioned off into weakly communicat ingsub-processes, one of which does syntact icanalysis  and the other of which doessemantic.
Why should considerat ion of themeaning of a sentence have to depend uponthe successful  syntactic analysis of thatsentence?
This is certainly not arestr ict ion that appl ies to people.
Whyshould computer programs be more l imited?A better model of comprehensiontherefore is one that uses a coherent set ofprocesses operat ing upon information ofdi f ferent varieties.
When this is done itbecomes clearer that the real problems ofcomputat ional  understanding involvesquest ions like: what information isnecessary for understanding a part iculartext, how does the text cue in thisinformation, how is general informat ion"tuned" to the current context, how isinformat ion removed from play, and so on.These quest ions must be asked for all thedif ferent kinds of information that areused.Notice that these quest ions are thesame ones that must be asked about ANY modeliiof memory processes.
The reason for this isobvious: COMPREHENSION IS A MEMORY PROCESS.This simple statement has several impor tantimpl icat ions about what a comprehensionmodel should look like.
Comprehension as amemory process implies a set of concernsvery different from those that arose whennatural  language processing was looked at byl inguistics.
It implies that the answersinvolve the generat ion of simple mechanismsand large data bases.
It implies that thesemechanisms should either be or at least looklike the mechanisms used for common-sensereasoning.
It implies that the informationin the data bases should be organized forusefulness -- i.e., so that textual cueslead to the RAPID retr ieval  of ALL theRELEVANT information -- rather than foruni formity -- e.g., syntax in one place,semantics in another.The next section of this paper isconcerned with a system of analysismechanisms that I have been developing.While the discussion is l imited pr imari ly tothe problem of computat ional  understanding,I hope it wil l  be clear that both themechanisms and the organizat ion of the database given are part of a more general modelof human memory.
