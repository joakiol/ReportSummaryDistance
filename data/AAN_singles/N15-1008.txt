Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 75?83,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsA Hybrid Generative/Discriminative Approach To Citation PredictionChris TannerBrown UniversityProvidence, RIchristanner@cs.brown.eduEugene CharniakBrown UniversityProvidence, RIec@cs.brown.eduAbstractText documents of varying nature (e.g., sum-mary documents written by analysts or pub-lished, scientific papers) often cite others asa means of providing evidence to support aclaim, attributing credit, or referring the readerto related work.
We address the problemof predicting a document?s cited sources byintroducing a novel, discriminative approachwhich combines a content-based generativemodel (LDA) with author-based features.
Fur-ther, our classifier is able to learn the im-portance and quality of each topic within ourcorpus ?
which can be useful beyond thistask ?
and preliminary results suggest its met-ric is competitive with other standard met-rics (Topic Coherence).
Our flagship system,Logit-Expanded, provides state-of-the-art per-formance on the largest corpus ever used forthis task.1 IntroductionThe amount of digital documents (both online andoffline) continues to grow greatly for several rea-sons, including the eagerness of users to gener-ate content (e.g., social media, Web 2.0) and thedecrease in digital storage costs.
Many differenttypes of documents link to or cite other documents(e.g., websites, analyst summary reports, academicresearch papers), and they do so for various rea-sons: to provide evidence, attribute credit, refer thereader to related work, etc.
Given the plethora ofdocuments, it can be highly useful to have a sys-tem which can automatically predict relevant cita-tions, for this could (1) aid authors in citing rele-vant, useful sources which they may otherwise notknow about; and (2) aid readers in finding usefuldocuments which otherwise might not have beendiscovered, due to the documents?
being unpopu-lar or poorly cited by many authors.
Specifically,we are interested in citation prediction ?
that is, weaim to predict which sources each report documentcites.
We define a report as any document that citesanother document in our corpus, and a source as adocument that is cited by at least one report.
Natu-rally, many documents within a corpus can be botha report and a source.
Note, we occasionally referto linking a report and source, which is synonymouswith saying the report cites the source.Citation prediction can be viewed as a specialcase of the more general, heavily-researched areaof link prediction.
In fact, past research mentionedin Section 2 refers to this exact task as both cita-tion prediction and link prediction.
However, linkprediction is a commonly used phrase which maybe used to describe other problems not concerningdocuments and citation prediction.
In these generalcases, a link may be relatively abstract and repre-sent any particular relationship between other ob-jects (such as users?
interests or interactions).
Tra-ditionally, popular techniques for link predictionand recommendation systems have included feature-based classification, matrix factorization, and othercollaborative filtering approaches ?
all of which typ-ically use meta-data features (e.g., names and in-terests) as opposed to modelling complete contentsuch as full text documents (Sarwar et al, 2001;Al Hasan and Zaki, 2011).
However, starting withHofmann and Cohn?s (2001) seminal work on ci-75tation prediction (PHITS), along with Erosheva et.al.
?s (2004) work (LinkLDA), content-based mod-elling approaches have extensively used generativemodels ?
while largely ignoring meta-data featureswhich collaborative filtering approaches often use ?thus creating somewhat of a dichotomy between twoapproaches towards the same problem.
We demon-strate that combining (1) a simple, yet effective,generative approach to modelling content with (2)author-based features into a discriminative classifiercan improve performance.
We show state-of-the-art performance on the largest corpus for this task.Finally, our classifier learns the importance of eachtopic within our corpus, which can be useful beyondthis task.In the next section, we describe related research.In Section 3 we describe our models and motivationsfor them.
In Section 4 we detail our experiments, in-cluding data and results, and compare our work tothe current state-of-the-art system.
We finally con-clude in Section 5.2 Related WorkHofmann and Cohn?s (2001) PHITS seminal workon citation prediction included a system that wasbased on probabilistic latent semantic analysis(PLSA) (Hofmann, 1999).
Specifically, they ex-tended PLSA by representing each distinct link toa document as a separate word token ?
as shown inEquation 1 and represented by sl.
(Note: Table 1displays common notation that is used consistentlythroughout this paper.)
PHITS assumes both thelinks and words are generated from the same globaltopic distributions, and like PLSA, a topic distribu-tion is inferred for each document in the corpus.P (wi|dj) =K?k=1P (wi|zk)P (zk|dj),P (sl|dj) =K?k=1P (sl|zk)P (zk|dj)(1)Later, Erosheva et.
al.
?s (2004) system replacedPLSA with LDA as the fundamental generative pro-cess; thus, the topic distributions were assumed tobe sampled from a Dirichlet prior, as depicted inthe plate notation of Figure 1.
We will refer to thismodel as it is commonly referred, LinkLDA, and itM total # documents in the corpus (both reports and sources)N # of words in the particular documentr a report documents a source documentd a document (report and/or source)w a word in a documentK total # of topicsz a particular topicV corpus?
vocabulary size?, ?
concentration parameters to corpus-wide Dirichlet priors4(p) a simplex of dimension (p-1)L number of citations in a particular document?kd?probability of a link to document d?w.r.t.
topic ksla token representing a link to source sTable 1: Notation GuideFigure 1: Plate notation of LinkLDAis the closest model to our baseline approach (laterintroduced as LDA-Bayes).Others have researched several variants of thisLDA-inspired approach, paving the field withpromising, generative models.
For example, Link-PLSA-LDA is the same as LinkLDA but it treatsthe generation of the source documents as a separateprocess inferred by PLSA (Nallapati et al, 2008).Related, Cite-LDA and Cite-PLSA-LDA (Kataria etal., 2010) extend LinkLDA and Link-PLSA-LDA,respectively, by asserting that the existence of a linkbetween a report and source is influenced by thecontext of where the citation link occurs within thereport document.
Note, the authors supplementedcorpora to include context that surrounds each cita-tion; however, there is currently no freely-available,widely-used corpus which allows one to discernwhere citations appear within each report.
There-fore, few systems rely on citation context.TopicBlock (Ho et al, 2012) models citation pre-diction with a hierarchical topic model but only usesthe first 200 words of each document?s abstract.
To76our knowledge, Topic-Link-LDA (Liu et al, 2009)is the only research which includes both author in-formation and document content into a generativemodel in order to predict citations.
Topic-Link-LDA estimates the probability of linking a report-source pair according to the similarity between thedocuments?
(1) author communities and (2) topicdistributions ?
these two latent groups are linearlycombined and weighted, and like the aforemen-tioned systems, are inferred by a generative process.PMTLM (Zhu et al, 2013) is reported as the cur-rent state-of-the-art system.
In short, it is equivalentto PLSA but extended by having a variable associ-ated with each document, which represents that doc-ument?s propensity to form a link.As mentioned, although Collaborative Filteringhas been used towards citation prediction (McNeeet al, 2002), there is little research which includesfeatures based on the entire content (i.e., docu-ments).
Very recently, (Wilson et al, 2014) usedtopic modelling to help predict movie recommenda-tions.
Specifically, one feature into their system wasthe KL-divergence between candidate items?
topicdistributions, but applying this towards citation pre-diction has yet to be done.
Most similar to ourwork, (Bethard and Jurafsky, 2010) used a classifierto predict citations, based on meta-data features andcompressed topic information (e.g., one feature isthe cosine similarity between a report-source pair?stopic distribution).
As explained in Section 4, weexpand the topic information into a vector of lengthK, which not only improves performance but yieldsan estimate of the most important, ?quality?
topics.Further, our system also uses our LDA-Bayes base-line as a feature, which by itself yields excellent re-sults compared to other systems on our large cor-pus.
Notably, Bethard and Jurafsky?s system (2010)also differs from ours in that (1) their system hasan iterative process that alternates between retriev-ing candidate source documents and learning modelweights by training a supervised classifier; and (2)they only assume access to the content of the ab-stract, not the entire documents.
Nonetheless, weuse their system?s most useful features to construct acomparable system (which we name WSIC ?
?WhoShould I Cite?
), which we describe in more detail inSection 3.3 and show results for in Section 4.3.3 New Models3.1 LDA-BayesFor a baseline system, we first implementedLDA (Blei et al, 2003) topic modelling and ran iton our entire corpus.
However, unlike past systems,after our model was trained, we performed citationprediction (i.e., P (s|r)) according to Equation 2.Notice, although LDA does not explicitly estimateP (s|z), we can approximate it via Bayes Rule, andwe consequently call our baseline LDA-Bayes.
Do-ing so allows us to include the prior probability ofthe given source being cited (i.e., P (s)), accord-ing to the maximum-likelihood estimate seen duringtraining.P (s|r) =K?kP (s|zk)P (zk|r),where P (s|zj) =P (zj|s)P (s)?s?P (zj|s?
)P (s?
)(2)Of the past research which uses generative mod-els for citation prediction, we believe LinkLDA isthe only other system in which a source?s prior cita-tion probability plays any role in training the model.Specifically, in LinkLDA, the prediction metric isidentical to ours in that the topics are marginalizedover topics (Equation 3).
It differs, however, in thattheir model directly infers P (s|zk), for it treats eachcitation link as a word token.
Although this doesnot explicitly factor in each source?s prior probabil-ity of being cited, it is implicitly influenced by such,for the sources which are more heavily cited duringtraining will tend to have a higher probability of be-ing generated from topics.P (s|r) =K?kP (s|zk)P (zk|r),(3)Note: the other generative models mentioned inSection 2, after inference, predict citations by sam-pling from a random variable (typically a Bernoullior Poisson distribution) which has been conditionedon the topic distributions.3.2 Logit-ExpandedIn attempt to combine the effectiveness of LDAin generating useful topics with the ability of dis-77Table 2: A randomly chosen report and its predictedsources, per LDA-Bayes, illustrating that a report andpredicted source may be contextually similar but thattheir titles may have few words in common.Report: Japanese Dependency Structure Analysis Based On Support Vector Machines (2000)PositionCited Year Source NameSource?11996 A Maximum Entropy Approach To Natural Language ProcessingNatural Language Processing21993 Building A Large Annotated CorpusOf English: The Penn Treebank3 1996 A Maximum Entropy Model For Part-Of-Speech Tagging41994 A Syntactic Analysis Method Of Long JapaneseSentences Based On The Detection Of Conjunctive Structures5 1992 Class-Based N-Gram Models Of Natural Language... ... ...111996 Three New Probabilistic Models ForDependency Parsing: An Exploration122000 Introduction To The CoNLL-2000Shared Task: Chunking13 1995 A Model-Theoretic Coreference Scoring Scheme141988 A Stochastic Parts Program And NounPhrase Parser For Unrestricted Text15X 1999 Japanese Dependency Structure AnalysisBased On Maximum Entropy Modelscriminative classifiers to learn important featuresfor classification, we use logistic regression witha linear kernel.
Specifically, we train using L2-regularization, which during test time allows us toget a probability estimate for each queried vector(i.e., a report-source pair).The details of the training and testing data are pro-vided in Section 4.2.
However, it is important to un-derstand that each training and testing instance cor-responds to a distinct report-source document pairand is represented as a single fixed-length vector.The vector is comprised of the following features,which our experiments illustrate are useful for deter-mining if there exists a link between the associatedreport and source:3.2.1 Topic/Content-Based Features?
LDA-Bayes: Our baseline system showedstrong results by itself, so we include its pre-dictions as a feature (that is, P (s|r)).?
Topics: LDA-Bayes ranks report-source pairsby marginalizing over all topics (see Equation2); however, we assert that not all topics areequally important.
Allowing each topic to berepresented as its own feature, while keepingthe value based on the report-source?s relation-ship for that topic (i.e., the absolute value ofthe difference), can potentially allow the lo-gistic regression to learn both (1) the impor-tance for report-source pairs to be generallysimilar across most topics and (2) the rela-tive importance of each topic.
For all of ourexperiments (including LDA-Bayes) we used125 topics to model the corpus; thus, this fea-ture becomes expanded to 125 individual in-dices within our vector, which is why we namethis system Logit-Expanded.
Namely, ?i ?K, let feature fi= |?ri?
?si|.3.2.2 Meta-data Features?
Report Author Previously Cited Source?
:We believe authors have a tendency to cite doc-uments they have cited in the past?
Report Author Previously Cited a SourceAuthor?
: Authors also have a tendency to?subscribe?
to certain authors and are more fa-miliar with particular people?s works, and thuscite those papers more often.?
Prior Citation Probability: A distinguishingfeature of our LDA-Bayes model is that it fac-tors in the prior probability of a source beingcited, based on the maximum likelihood esti-mate from the training data.
So, we explicitlyinclude this as a feature.?
Number of Overlapping Authors: Authorshave a tendency to cite their co-authors, in partbecause their co-authors?
past work has an in-creased chance of being relevant.?
Number of Years between Report andSource: Authors tend to cite more recent pa-pers.?
Title Similarity between Report and Source:As shown in Table 2, some sources erroneouslyreturned by our baseline system could havebeen discarded had we judged them by howdissimilar their titles are from the report?s title.In Table 2?s example, the one correct source tofind (within ?12,000) was returned at position15 and has many words in common with the re-port (namely, ?Japanese Dependency StructureAnalysis Based On?
appears in the titles of boththe report and correctly predicted source).783.3 WSIC (Who Should I Cite?
)In attempt to compare our systems against Bethardand Jurafsky?s system (2010), we implemented thefeatures they concluded to be most useful for re-trieval, and like our Logit-Expanded system, usedlogistic regression as the mechanism for learningcitation prediction.
Instead of using only the textfrom the abstracts, like in their research, to makethe comparison more fair we used text from the en-tire documents ?
just like we did for the rest of oursystems.
Specifically, adhering to their naming con-vention, the features from their system that we usedare: citation-count, venue-citation-count, author-citation-count, author-h-index, age (# years betweenreport and source), terms-citing, topics, authors,authors-cited-article, and authors-cited-author.4 Experiments4.1 CorporaThe past research mentioned in Section 2 primarilymakes use of three corpora: Cora, CiteSeer, and We-bKB.
As shown in Table 3, these corpora are rela-tively small with ~3,000 documents, an average ofless than three links per document, and a modestnumber of unique word types.We wanted to use a corpus which was larger, pro-vided the complete text of the original documents,and included meta-data such as author information.Thus, we used the ACL Anthology (Radev et al,2013) (the December 2013 release), which providesauthor and year information for each paper, and thecorpus details are listed in Table 3.
For the task ofcitation prediction, we are the first to use full contentinformation from a corpus this large.4.2 Training/Testing DataThe research listed in Section 2 commonly uses 90%of all positive links (i.e., a distinct report-to-sourceinstance) for training purposes and 10% for testing.LDA-based topic modelling approaches, which arestandard for this task, require that at testing timeeach report and candidate source has already beenobserved during training.
This is because at test timethe topic distribution for each document must havealready been inferred.
Additionally, it is common tomake the assumption that the corpus is split into abipartite graph: a priori we know which documentsare reports and which are sources, with most beingboth.
At testing time, one then predicts sources fromthe large set of candidate sources, all of which wereseen at some point during training (as either a reportor a source document).We follow suit with the past research and ran-domly split the ACL Anthology?s report-to-sourcelinks (citations) into 90% for training and 10% fortesting, with the requirement that every candidatesource document during testing was seen duringtraining as either a report or a source ?
ensuringwe have a topic distribution for each document.
Onaverage, each report has 6.8 sources, meaning typ-ically at test time each report has just a few (e.g.,1-5) sources which we hope to predict from our12,265 candidate sources.
For all of our exper-iments, the systems (e.g., LDA-Bayes, LinkLDA,Logit-Expanded, etc) were evaluated on the exactsame randomly chosen split of training/testing data.As for training Logit-Expanded, naturally thereare vastly more negative examples (i.e., no link be-tween the given report-source pair) than positive ex-amples; most sources are not cited for a given re-port.
This represents a large class-imbalance prob-lem, which could make it difficult for the classifier tolearn our task.
Consequently, we downsampled thenegative examples.
Specifically, for each report, weincluded all positive examples (the cited sources),and for each positive example, we included 5 ran-domly selected negative examples (sources).
Note,for testing our system, we still need to evaluate ev-ery possible candidate report-source pair ?
that is?12,265 candidate sources per tested report.Table 3: Report-to-Source Citation Prediction CorporaCora CiteSeer WebKB ACL# docs 2,708 3,312 3,453 17,298# links 5,429 4,608 1,733 106,992vocab size 1,433 3,703 24,182 137,885# authors - - - 14,4074.3 Results4.3.1 Report-To-Source Citation PredictionFirst, we tested our LDA-Bayes baseline systemand compared it to LinkLDA and PMTLM (Zhu et79Figure 2: Average Recall Performance across all Reportsfrom a 1,000 document subset of the ACL Anthologyal., 2013) ?
the current state-of-the-art system.
Dueto the slow running time of PMTLM, we restrictedour preliminary experiment to just 1,000 documentsof the ACL Anthology, and Figure 2 shows the av-erage recall performance across all reports.
Surpris-ingly, PMTLM performed worst.
Note: the authorsof PMTLM compared their system to LinkLDA fora different task (predicting research area) but did notcompare to LinkLDA during their analysis of cita-tion prediction performance.
Thus, it was not previ-ously asserted that PMTLM would outperform Lin-kLDA.As we can see, LDA-Bayes, despite being simple,performs well.
As mentioned, LDA-Bayes explicitlycaptures the prior probability of each source beingcited (via maximum-likelihood estimate), whereasLinkLDA and PMTLM approximates this during in-ference.
We believe this contributes towards the per-formance differences.It was expected that when run on the entire ACLcorpus, WSIC and our Logic-Expanded systemswould have sufficient data to learn authors?
citingpreferences and would outperform the other genera-tive models.
As shown in Figure 3 and 4, our flag-ship Logit-Expanded system greatly outperformedall other systems, while our baseline LDA-Bayescontinued to offer strong results.
Note, the full re-call performance results include returning 12,265sources, but we only show the performance for re-turning the first 200 returned sources.
Further, Ta-ble 4 shows the same experimental results but forthe performance when returning just the first 50 pre-Figure 3: Average Recall Performance across all reportsfrom the full ACL Anthologydicted sources per report.Table 4: Performance of each system, averaged across allreports while returning the top 50 predicted sources foreach.
125 topics were used for every system.recall precision fscoreLogit-Expanded .647 .016 .031LDA-Bayes .496 .012 .024WSIC .442 .011 .021LinkLDA .431 .011 .021LDA-Bayes (uniform prior) .309 .007 .014Again, we further see how effective it is to havea model influenced by a source?s prior probabil-ity, for when we change LDA-Bayes such thatP (SourceCited) is uniform for all sources, perfor-mance falls greatly ?
represented as LDA (uniformprior).We analyzed the benefits of each feature of Logit-Expanded in 2 ways: (1) starting with the full-feature set experiment (whose results we showed),we evaluate each feature by running an experimentwhereby the said feature is removed; and (2) start-ing with our LDA-Bayes baseline as the only fea-ture for our Logit-Expanded system, we evaluateeach feature by running an experiment whereby thesaid feature is paired with LDA-Bayes as the onlytwo features used.
For both of these approaches,we measure performance by looking at recall, pre-cision, and f-score when returning the first 50 pre-dicted sources.
The results are shown in Table 5;technique (1) is shown in column removal, and (2)80Figure 4: Recall vs Precision Performance across all Re-ports from the full ACL Anthology.
Logit-Expanded?sslight blips at recall = 0.25, 0.33, and 0.5 is due to thetruth set having many reports with only 4, 3, or 2 goldensources, respectively.is in column addage.Table 5 reveals insightful results: it is clear thatLDA-Bayes is a strong baseline and useful feature toinclude in our system, for removing it from our fea-ture list causes performance to decrease more thanremoving any other feature.
PrevCitedSource andTopics Expanded are the second and third strongestfeatures, respectively.
We suspect that PrevCit-edSource was a good feature because our corpuswas sufficiently large; had our corpus been muchsmaller, there might not have been enough data forthis feature to provide any benefit.
Next, Title Simi-larity and # Shared Authors were comparably goodfeatures.
PrevCitedAuthor and # Years Betweenwere the worst features, as we see negligible perfor-mance difference when we (1) pair either with LDA-Bayes, or (2) remove either from our full feature list.An explanation for the former feature?s poor per-formance could be that authors vary in (1) how of-ten they repeatedly cite authors, and most likely (2)many authors have small publication histories withintraining, so it might be unwise to base predictionon this limited information.
Last, it is worth not-ing that when we pair Topics Expanded with LDA-Bayes, that alone is not enough to give the bestperformance from a pair.
An explanation is that itdominates the system with too much content-based(i.e., topic) information, overshadowing the prior-citation-probability that plays a role in LDA-Bayes.Supporting this idea, we see the biggest performanceincrease when we pair LDA-Bayes with the PrevCit-edSource feature ?
a non-topic-based feature, whichprovides the system with a different type of data toleverage.Table 5: Analysis of each feature used in Logit-Expanded.
Results based on the first 50 sources returned,averaged over all reports.
Our Starting Point* systemlisted within the ?Addage?
columns used LDA-Bayes asthe only feature.
Our Starting Point* system within the?Removal?
columns used every feature.Addage Removalrecall precision fscore recall precision fscoreStarting Point* .496 .012 .024 .647 .016 .031LDA-Bayes - - - .583 .014 .028Topics Expanded .564 .014 .027 .606 .015 .028PrevCitedSource .581 .014 .028 .599 .014 .028PrevCitedAuthor .484 .012 .023 .641 .016 .030# Shared Authors .543 .013 .026 .636 .015 .029Prior Prob.
Cited .501 .012 .023 .639 .015 .030Title Similarity .513 .012 .023 .623 .015 .029# Years Between .498 .012 .023 .645 .016 .030Additionally, when using only the metadata fea-tures (i.e., not LDA-Bayes or Topics-Expanded),performance for returning 50 sources averaged0.403, 0.010, and 0.019 for recall, precision, andfscore, respectively ?
demonstrating that the meta-data features alone do not yield strong results butthat they complement the LDA-Bayes and Topics-Expanded features.4.3.2 Topic ImportanceAlthough Report-to-Source citation predictionwas our primary objective, our feature representa-tion of topics allows logistic regression to appropri-ately learn which topics are most useful for predict-ing citations.
In turn, these topics are arguably themost cohesive; thus, our system, as a byproduct, pro-vides a metric for measuring the ?quality?
of eachtopic.
Namely, the weight associated with each topicfeature indicates the topic?s importance ?
the lowerthe weight the better.Table 6 shows our system?s ranking of the mostimportant topics, signified by ?Logit-weight.?
Wedid not prompt humans to evaluate the quality of thetopics, so in attempt to offer a comparison, we alsorank each topic according to two popular metrics:Pointwise Mutual Information (PMI) and Topic Co-herence (TC) (Mimno et al, 2011).
For a topic k,81let V(k)represent the top M words for K; whereV(k)= (v(k)i, ..., v(k)M) and D(v) represents the doc-ument frequency of word type v. Then, PMI(k)is defined by Equation 4 and TC(k) is defined byEquation 5.In Table 6, we see that our most useful topic(Topic 49) concerns vision research, and since ourcorpus is heavily filled with research concerning(non-vision-related) natural language processing, itmakes sense for this topic to be highly important forpredicting citations.
Similarly, we see the other top-ranking topics all represent a well-defined, subfieldof natural language processing research, includingparsing, text generation, and Japanese-English ma-chine translation.PMI(k;V(k)) =M?m=2m?1?l=1logp(V(k)m, V(k)l)p(V(k)m)p(V(k)l)(4)TC(k;V(k)) =M?m=2m?1?l=1logD(V(k)m, V(k)l)D(V(k)m)(5)Table 7 shows the worst 5 topics according toLogit-Expanded.
Topic 96 concerns Wikipedia asa corpus, which naturally encompasses many areasof research, and as we would expect, the mention ofsuch is probably a poor indicator for predicting ci-tations.
Topic 77 concerns artifacts from the OCR-rendering of our corpus, which offers no meaning-ful information.
In general, the worst-ranking topicsconcern words that span many documents and do notrepresent cohesive, well-defined areas of research.Additionally, in both Table 6 and 7 we see thatPointwise Mutual Information (PMI) disagrees quitea bit with our Logit-Expanded?s ranking, and fromthis initial result, it appears Logit-Expanded?s rank-ing might be a better metric than PMI ?
at least interms of quantifying relevance towards documentsbeing related and linked via a citation.This cursory, qualitative critique of the met-rics warrants more research, ideally with human-evaluation.
However, one can see how these met-rics differ: TC and PMI are both entirely concernedwith just the co-occurrence of terms, normalized bythe general popularity of the said terms.
There-fore, words could highly co-occur together but oth-erwise represent nothing special about the corpus atlarge.
On the other hand, Logit-Expanded?s rank-ing is mainly concerned with quantifying how welleach topic represents discriminatively useful contentwithin a document.Table 6: The highest quality topics (out of 125), sortedaccording to Logit-Expanded?s estimate.
Topics are alsoranked according to Pointwise Mutual Information (PMI)and Topic Coherence (TC).Logit?sRankPMIRankTCRankLogitWeightTopic # Top Words1 116 103 -5.50 49image, visual, multimodal, images, spatial, gesture,objects, object, video, scene, instructions, pointing2 33 44 -4.76 25grammar, parsing, grammars, left, derivation,terminal, nonterminal, items, free, string,item, derivations, cfg3 68 37 -4.71 65generation, generator, generated, realization,content, planning, choice, nlg, surface, generate4 49 27 -4.28 32noun, nouns, phrases, adjectives, adjective,compound, verb, head, compounds, preposition5 107 61 -4.24 0japanese, ga, expressions, wo, accuracy, bunsetsu,ni, dictionary, wa, kanji, noun, expressionTable 7: The lowest quality topics (out of 125), sortedby Logit-Expanded?s estimate.
Topics are also rankedaccording to Pointwise Mutual Information (PMI) andTopic Coherence (TC).Logit?sRankPMIRankTCRankLogitWeightTopic # Top Words121 13 110 -1.45 96wikipedia, links, link, articles, article, title,page, anchor, pages, wiki, category, attributes122 83 122 -1.20 77 x1, x2, c1, c2, p2, a1, p1, a2, r1, l1, xf, fi123 42 36 -1.09 91annotation, agreement, annotated, annotators,annotator, scheme, inter, annotate, gold, kappa124 10 34 -0.75 43selection, learning, active, selected, random,confidence, sample, sampling, cost, size, select125 65 115 -0.33 30region, location, texts, city, regions, weather,locations, map, place, geographic, country5 ConclusionsWe have provided a strong baseline, LDA-Bayes,which when run on the largest corpus for this task,offers compelling performance.
We have demon-strated that modelling the prior probability of eachcandidate source being cited is simple yet impor-tant, for it allows all of our systems to outperformthe previous state-of-the-art ?
our large corpus helpstowards making this a useful feature, too.Our biggest contribution is our new system,Logit-Expanded, which combines both the effective-ness of the generative model LDA with the powerof logistic regression to discriminately learn impor-tant features for classification.
By representing eachtopic as its own feature, while still modelling the re-82lationship between the candidate report-source pair,we allow our system to learn (1) that having simi-lar topic distributions between reports and sourcesis indicative of a link, and (2) which topics are mostimportant for predicting a link.
Because we used alinear kernel, we are able to discern exactly how im-portant it ranks each topic.
A cursory, qualitativeassessment of its metric shows promising and com-petitive performance with that of Pointwise MutualInformation and Topic Coherence.ReferencesMohammad Al Hasan and Mohammed J Zaki.
2011.
Asurvey of link prediction in social networks.
In Socialnetwork data analytics, pages 243?275.
Springer.Steven Bethard and Dan Jurafsky.
2010. Who shouldi cite: learning literature search models from citationbehavior.
In Proceedings of the 19th ACM interna-tional conference on Information and knowledge man-agement, pages 609?618.
ACM.David M Blei, Andrew Y Ng, and Michael I Jordan.2003.
Latent dirichlet alocation.
the Journal of ma-chine Learning research, 3:993?1022.Elena Erosheva, Stephen Fienberg, and John Lafferty.2004.
Mixed-membership models of scientific pub-lications.
Proceedings of the National Academy ofSciences of the United States of America, 101(Suppl1):5220?5227.Qirong Ho, Jacob Eisenstein, and Eric P Xing.
2012.Document hierarchies from text and links.
In Pro-ceedings of the 21st international conference on WorldWide Web, pages 739?748.
ACM.David Hofmann and Thomas Cohn.
2001.
The missinglink-a probabilistic model of document content and hy-pertext connectivity.
In Proceedings of the 2000 Con-ference on Advances in Neural Information ProcessingSystems.
The MIT Press, pages 430?436.Thomas Hofmann.
1999.
Probabilistic latent semanticindexing.
In Proceedings of the 22nd annual interna-tional ACM SIGIR conference on Research and devel-opment in information retrieval, pages 50?57.
ACM.Saurabh Kataria, Prasenjit Mitra, and Sumit Bhatia.2010.
Utilizing context in generative bayesian mod-els for linked corpus.
In AAAI, volume 10, page 1.Yan Liu, Alexandru Niculescu-Mizil, and WojciechGryc.
2009.
Topic-link lda: joint models of topic andauthor community.
In proceedings of the 26th annualinternational conference on machine learning, pages665?672.
ACM.Sean M McNee, Istvan Albert, Dan Cosley, PrateepGopalkrishnan, Shyong K Lam, Al Mamunur Rashid,Joseph A Konstan, and John Riedl.
2002.
On the rec-ommending of citations for research papers.
In Pro-ceedings of the 2002 ACM conference on Computersupported cooperative work, pages 116?125.
ACM.David Mimno, Hanna M Wallach, Edmund Talley,Miriam Leenders, and Andrew McCallum.
2011.
Op-timizing semantic coherence in topic models.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing, pages 262?272.
Asso-ciation for Computational Linguistics.Ramesh M Nallapati, Amr Ahmed, Eric P Xing, andWilliam W Cohen.
2008.
Joint latent topic modelsfor text and citations.
In Proceedings of the 14th ACMSIGKDD international conference on Knowledge dis-covery and data mining, pages 542?550.
ACM.DragomirR.
Radev, Pradeep Muthukrishnan, VahedQazvinian, and Amjad Abu-Jbara.
2013.
The aclanthology network corpus.
Language Resources andEvaluation, pages 1?26.Badrul Sarwar, George Karypis, Joseph Konstan, andJohn Riedl.
2001.
Item-based collaborative filter-ing recommendation algorithms.
In Proceedings ofthe 10th international conference on World Wide Web,pages 285?295.
ACM.Jobin Wilson, Santanu Chaudhury, Brejesh Lall, and Pra-teek Kapadia.
2014.
Improving collaborative filter-ing based recommenders using topic modelling.
arXivpreprint arXiv:1402.6238.Yaojia Zhu, Xiaoran Yan, Lise Getoor, and Cristo-pher Moore.
2013.
Scalable text and link analy-sis with mixed-topic link models.
In Proceedings ofthe 19th ACM SIGKDD international conference onKnowledge discovery and data mining, pages 473?481.
ACM.83
