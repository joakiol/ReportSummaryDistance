Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 257?260,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsMulti-domain Sentiment ClassificationShoushan Li and Chengqing ZongNational Laboratory of Pattern RecognitionInstitute of Automation, Chinese Academy of Sciences, Beijing 100190, China{sshanli,cqzong}@nlpr.ia.ac.cnAbstractThis paper addresses a new task in sentimentclassification, called multi-domain sentimentclassification, that aims to improve perform-ance through fusing training data from multi-ple domains.
To achieve this, we propose twoapproaches of fusion, feature-level and classi-fier-level, to use training data from multipledomains simultaneously.
Experimental stud-ies show that multi-domain sentiment classi-fication using the classifier-level approachperforms much better than single domainclassification (using the training data indi-vidually).1 IntroductionSentiment classification is a special task of textcategorization that aims to classify documentsaccording to their opinion of, or sentiment towarda given subject (e.g., if an opinion is supported ornot) (Pang et al, 2002).
This task has created aconsiderable interest due to its wide applications.Sentiment classification is a very domain-specific problem; training a classifier using thedata from one domain may fail when testingagainst data from another.
As a result, realapplication systems usually require some labeleddata from multiple domains, guaranteeing anacceptable performance for different domains.However, each domain has a very limited amountof training data due to the fact that creating large-scale high-quality labeled corpora is difficult andtime-consuming.
Given the limited multi-domaintraining data, an interesting task arises, how tobest make full use of all training data to improvesentiment classification performance.
We namethis new task, ?multi-domain sentimentclassification?.In this paper, we propose two approaches tomulti-domain sentiment classification.
In the first,called feature-level fusion, we combine the featuresets from all the domains into one feature set.Using the unified feature set, we train a classifierusing all the training data regardless of domain.
Inthe second approach, classifier-level fusion, wetrain a base classifier using the training data fromeach domain and then apply combination methodsto combine the base classifiers.2 Related WorkSentiment classification has become a hot topicsince the publication work that discusses classifi-cation of movie reviews by Pang et al (2002).This was followed by a great many studies intosentiment classification focusing on many do-mains besides that of movie.Research into sentiment classification overmultiple domains remains sparse.
It is worth not-ing that Blitzer et al (2007) deal with the domainadaptation problem for sentiment classificationwhere labeled data from one domain is used totrain a classifier for classifying data from a differ-ent domain.
Our work focuses on the problem ofhow to make multiple domains ?help each other?when all contain some labeled samples.
These twoproblems are both important for real applicationsof sentiment classification.3 Our Approaches3.1 Problem StatementIn a standard supervised classification problem,we seek a predictor f (also called a classifier) that257maps an input vector x to the corresponding classlabel y.
The predictor is trained on a finite set oflabeled examples { ( , )i iX Y } (i=1,?,n) and itsobjective is to minimize expected error, i.e.,l arg min ( ( ), )ni if if L f X Y?= ?
?Where L is a prescribed loss function and H is aset of functions called the hypothesis space, whichconsists of functions from x to y.
In sentimentclassification, the input vector of one document isconstructed from weights of terms.
The terms1( ,..., )Nt t  are possibly words, word n-grams, oreven phrases extracted from the training data, withN being the number of terms.
The output label yhas a value of 1 or -1 representing a positive ornegative sentiment classification.In multi-domain classification, m differentdomains are indexed by k={1,?,m}, each withkn training samples ( , )k ki iX Y {1,..., }k ki n= .
Astraightforward approach is to train a predictor kffor the k-th domain only using the trainingdata {( , )}k ki iX Y .
We call this approach singledomain classification and show its architecture inFigure 1.Figure 1: The architecture of single domain classifica-tion.3.2 Feature-level Fusion ApproachAlthough terms are extracted from multiple do-mains, some occur in all domains and convey thesame sentiment (this can be called global senti-ment information).
For example, some terms like?excellent?
and ?perfect?
express positive senti-ment information independent of domain.
To learnthe global sentiment information more correctly,we can pool the training data from all domains fortraining.
Our first approach is using a common setof terms 1( ' ,..., ' )allNt t  to construct a uniform fea-ture vector 'x  and then train a predictor using alltraining data:m1 1arg min ( ( ' ), )kk kall all knmall i if k if L f X Y?
= == ??
?We call this approach feature-level fusion andshow its architecture in Figure 2.
The common setof terms is the union of the term sets frommultiple domains.Figure 2: The architecture of the feature-level fusionapproachFeature-level fusion approach is simple toimplement and needs no extra labeled data.
Notethat training data from different domainscontribute differently to the learning process for aspecific domain.
For example, given data fromthree domains, books, DVDs and kitchen, wedecide to train a classifier for classifying reviewsfrom books.
As the training data from DVDs ismuch more similar to books than that fromkitchen (Blitzer et al, 2007), we should give thedata from DVDs a higher weight.
Unfortunately,the feature-level fusion approach lacks thecapacity to do this.
A more qualified approach isrequired to deal with the differences among theclassification abilities of training data fromdifferent domains.3.3 Classifier-level Fusion ApproachAs mentioned in sub-Section 2.1, single domainclassification is used to train a single classifier foreach domain using the training data in the corre-sponding domain.
As all these single classifiersaim to determine the sentiment orientation of adocument, a single classifier can certainly be usedto classify documents from other domains.
Givenmultiple single classifiers, our second approach isto combine them to be a multiple classifier systemfor sentiment classification.
We call this approachclassifier-level fusion and show its architecture inFigure 3.
This approach consists of two main steps:Training Datafrom Domain 1Training Datafrom Domain 2Training Datafrom Domain mClassifier1Classifier2ClassifiermTesting Datafrom Domain 1Testing Datafrom Domain 2Testing Datafrom Domain m. .
.. .
.. .
.Training Datafrom Domain 1Training Datafrom Domain 2Training Datafrom Domain mClassifierTesting Datafrom Domain 1Testing Datafrom Domain 2Testing Datafrom Domain m. .
.. .
.Training Data from all Domainsusing a Uniform Feature Vector258(1) train multiple base classifiers (2) combine thebase classifiers.
In the first step, the base classifi-ers are multiple single classifiers kf  (k=1,?,m)from all domains.
In the second step, many com-bination methods can be applied to combine thebase classifiers.
A well-known method calledmeta-learning (ML) has been shown to be veryeffective (Vilalta and Drissi, 2002).
The key ideabehind this method is to train a meta-classifierwith input attributes that are the output of the baseclassifiers.Figure 3: The architecture of the classifier-level fusionapproachFormally, let 'kX denote a feature vector of asample from the development data of the'-thk domain ( ' 1,..., )k m= .
The output of the-thk base classifier kf on this sample is theprobability distribution over the set of classes1 2{ , ,..., }nc c c , i.e.,' 1 ' '( )  ( | ),..., ( | )k k k k k n kp X p c X p c X= < >For the '-thk domain, we train a meta-classifier'  ( ' 1,..., )kf k m= using the development data fromthe '-thk domain with the meta-level featurevector 'meta m nkX R??'
1 ' ' ' ( ),..., ( ),..., ( )metak k k k m kX p X p X p X= < >Each meta-classifier is then used to test the testingdata from the same domain.Different from the feature-level approach, theclassifier-level approach treats the training datafrom different domains individually and thus hasthe ability to take the differences in classificationabilities into account.4 ExperimentsData Set:  We carry out our experiments on thelabeled product reviews from four domains: books,DVDs, electronics, and kitchen appliances1.
Eachdomain contains 1,000 positive and 1,000negative reviews.Experiment Implementation: We apply SVMalgorithm to construct our classifiers which hasbeen shown to perform better than many otherclassification algorithms (Pang et al, 2002).
Here,we use LIBSVM2 with a linear kernel function fortraining and testing.
In our experiments, the datain each domain are partitioned randomly intotraining data, development data and testing datawith the proportion of 70%, 20% and 10%respectively.
The development data are used totrain the meta-classifier.Baseline: The baseline uses the single domainclassification approach mentioned in sub-Section2.1.
We test four different feature sets to constructour feature vector.
First, we use unigrams (e.g.,?happy?)
as features and perform the standard fea-ture selection process to find the optimal featureset of unigrams (1Gram).
The selection method isBi-Normal Separation (BNS) that is reported to beexcellent in many text categorization tasks (For-man, 2003).
The criterion of the optimization is tofind the set of unigrams with the best performanceon the development data through selecting thefeatures with high BNS scores.
Then, we get theoptimal word bi-gram (e.g., ?very happy?)
(2Gram)and mixed feature set (1+2Gram) in the same way.The fourth feature set (1Gram+2Gram) also con-sists of unigrams and bi-grams just like the thirdone.
The difference between them lies in their se-lection strategy.
The third feature set is obtainedthrough selecting the unigrams and bi-grams withhigh BNS scores while the fourth one is obtainedthrough simply uniting the two optimal sets of1Gram and 2Gram.From Table 1, we see that 1Gram+2Gram fea-tures perform much better than other types of fea-tures, which implies that we need to select goodunigram and bi-gram features separately beforecombine them.
Although the size of our trainingdata are smaller than that reported in Blitzer et al1 This data set is collected by Blitzer et al (2007):http://www.seas.upenn.edu/~mdredze/datasets/sentiment/2 LIBSVM is an integrated software for SVM:http://www.csie.ntu.edu.tw/~cjlin/libsvm/Training Datafrom Domain 1Training Datafrom Domain 2Training Datafrom Domain mMultiple ClassifierSystem 1Testing Datafrom Domain 1Testing Datafrom Domain 2Testing Datafrom Domain m. .
.. .
.Base Classifier1Base Classifier2Base Classifierm.
.
.Multiple ClassifierSystem 2Multiple ClassifierSystem mDevelopment Datafrom Domain 1Development Datafrom Domain 2Development Datafrom Domain m. .
.. .
.259(2007) (70% vs. 80%), the classification perform-ance is comparative to theirs.We implement the fusion using 1+2Gram and1Gram+2Gram respectively.
From Figure 4, wesee that both the two fusion approaches generallyoutperform single domain classification when us-ing 1+2Gram features.
They increase the averageaccuracy from 0.8 to 0.82375 and 0.83875, a sig-nificant relative error reduction of 11.87% and19.38% over baseline.1+2Gram Features76.581 808382.5 82.5 82.58183 848683727476788082848688Books DVDs Electronics KitchenAccuracy(%)1Gram+2Gram Features7984.5848284.5 85 838283.5868889747678808284868890Books DVDs Electronics KitchenAccuracy(%)Single domain classificationFeature-level fusionClassifier-level fusion with MLFigure 4: Accuracy results on the testing data usingmulti-domain classification with different approaches.However, when the performance of baseline in-creases, the feature level approach fails to help theperformance improvement in three domains.
Thisis mainly because the base classifiers perform ex-tremely unbalanced on the testing data of thesedomains.
For example, the four base classifiersfrom Books, DVDs, Electronics, and Kitchenachieve the accuracies of 0.675, 0.62, 0.85, and0.79 on the testing data from Electronics respec-tively.
Dealing with such an unbalanced perform-ance, we definitely need to put enough highweight on the training data from Electronics.However, the feature-level fusion approach sim-ply pools all training data from different domainsand treats them equally.
Thus it can not capturethe unbalanced information.
In contrast, meta-learning is able to learn the unbalance automati-cally through training the meta-classifier using thedevelopment data.
Therefore, it can still increasethe average accuracy from 0.8325 to 0.8625, animpressive relative error reduction of 17.91% overbaseline.5 ConclusionIn this paper, we propose two approaches to multi-domain classification task on sentiment classifica-tion.
Empirical studies show that the classifier-level approach generally outperforms the featureapproach.
Compared to single domain classifica-tion, multi-domain classification with the classi-fier-level approach can consistently achieve muchbetter results.AcknowledgmentsThe research work described in this paper hasbeen partially supported by the Natural ScienceFoundation of China under Grant No.
60575043,and 60121302, National High-Tech Research andDevelopment Program of China under Grant No.2006AA01Z194, National Key TechnologiesR&D Program of China under Grant No.2006BAH03B02, and Nokia (China) Co. Ltd aswell.ReferencesJ.
Blitzer, M. Dredze, and F. Pereira.
2007.
Biographies,Bollywood, Boom-boxes and Blenders: Domain ad-aptation for sentiment classification.
In Proceedingsof ACL.G.
Forman.
2003.
An extensive empirical study of fea-ture selection metrics for text classification.
Journalof Machine Learning Research, 3: 1533-7928.B.
Pang, L. Lee, and S. Vaithyanathan.
2002.
Thumbsup?
Sentiment classification using machine learningtechniques.
In Proceedings of EMNLP.R.
Vilalta and Y. Drissi.
2002.
A perspective view andsurvey of meta-learning.
Artificial Intelligence Re-view, 18(2): 77?95.Features Books DVDs Elec-tronicKitchen1Gram 0.75 0.84 0.8 0.8252Gram 0.75 0.73 0.815 0.7851+2Gram 0.765 0.81 0.825 0.801Gram+2Gram 0.79 0.845 0.85 0.845Table 1: Accuracy results on the testing data of singledomain classification using different feature sets.260
