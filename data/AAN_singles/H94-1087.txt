Language Identification via Large VocabularySpeaker Independent Continuous Speech RecognitionSteve Lowe, Anne Demedts, Larry Gillick, Mark Mandel, Barbara PeskinDragon Systems, Inc.320 Nevada StreetNewton,  Massachusetts  02160ABSTRACTThe goal of this study is to evaluate the potential for usinglarge vocabulary continuous peech recognition as an enginefor automatically classifying utterances according to the lan-guage being spoken.
The problem of language identificationis often thought of as being separate from the problem ofspeech recognition.
But in this paper, as in Dragon's earlierwork on topic and speaker identification, we explore a uni-fying approach to all three message classification problemsbased on the underlying stochastic process which gives riseto speech.
We discuss the theoretical framework upon whichour message classification systems are built and report ona series of experiments in which this theory is tested, usinglarge vocabulary continuous speech recognition to distinguishEnglish from Spanish.1.
INTRODUCTIONIn this paper we describe preliminary work being con-ducted at Dragon Systems exploring the use of large vo-cabulary continuous speech recognition as an engine forautomatically classifying spoken utterances by language.Several approaches to the problem of language identifi-cation have already appeared in the literature, but theygenerally address the problem as quite separate from theproblem of speech recognition.
For example, LIMSI \[1\]has reported results in French-English discrimination viaphone recognition and a number of sites, such as OGI \[2\],have performed language classification by using broadphonetic labels and analyzing sets of phonological "fea-tures".Our approach to the problem of language identificationgrows naturally out of our model for the underlyingstochastic process giving rise to speech.
In earlier pa-pers (\[3\], \[4\]) we have described our unified approach tothe problems of topic and speaker identification via largevocabulary continuous peech recognition and demon-strated the success of this strategy even in classifyingspeech data in domains where the recognition task is fartoo difficult to obtain accurate transcriptions.
We be-lieve that the contextual information - both acoustic andlanguage model - available in full-scale large vocabularycontinuous speech recognition is invaluable in extractingreliable data from difficult speech channels.
We now ex-amine how this same framework supports work on theproblem of language identification.In the next section we describe the theoretical founda-tions upon which our message classification systems arebased and discuss ome simplifying approximations in-troduced in their implementation.
We then describe ourinitial testing of English-Spanish discrimination, primar-ily work with microphone data using our Wall StreetJournal speech recognition system, but also work we arenow beginning in language identification on telephonespeech.
Finally, we discuss some lessons learned fromthese early explorations and suggest plans for futurework.2.
THEORET ICAL  FRAMEWORKWe briefly review the theoretical background escribedin our earlier papers \[3\] and \[4\].
Our approach to themessage classification problem - for topic, speaker, orlanguage identification - is based on modelling speech asa stochastic process.
We assume that a given stream ofspeech is generated by one of several possible stochasticsources, one corresponding to each of the languages (ortopics or speakers) in question.
We are faced with theproblem of deciding, based on the acoustic data alone,which is the true source of the speech.Standard statistical theory provides us with the optimalsolution to such a classification problem.
We denote thestring of acoustic observations by A and introduce therandom variable T to designate which stochastic modelhas produced the speech, where T may take on the valuesfrom 1 to n for the n possible speech sources.
If we letPi denote the prior probability of stochastic source i andassume that all classification errors have the same cost,then we should choose the source T = ~ for which= argmax Pi P(A \] T = i).iWe assume, for the purposes of this work, that all priorprobabilities are equal, so that the classification problemreduces imply to choosing the source i for which theconditional probability of the acoustics given the sourceis maximized.437In principle, to compute each of the probabilitiesP(A \[ T = i) we would have to sum over all possibletranscriptions W of the speech:P(A\[ T=i )=~P(A ,W\ [  T=i ) .wIn practice, such a collection of computations is unwieldyso to limit the computational burden we introduce a sim-plifying approximation.
Instead of computing the fullprobability P(A \[ T = i), we approximate the sumby its largest erm: the joint probability of A and thesingle most probable word sequence W = W/max.
Ofcourse, generating such an optimal word sequence is ex-actly what speech recognition is designed to do.
Thus,we could imagine running n different speech recogniz-ers, one trained in each of the n languages, and thencompare the resulting probabilities P(A, W/m~ \[ T = i)corresponding to each of the n optimal transcriptionsW/max.
The speech would then be assigned to the lan-guage whose recognizer produced the best score.This approach still requires us to make multiple recog-nition passes across the test speech, one pass for eachstochastic source.
In the cases of topic and speakeridentification studied earlier, we were able to furtherlimit the demand on the recognizer by producing a sin-gle "best" transcription W = Wm~,, using a speaker-independent topic-independent recognizer, to approxi-mate the optimal transcriptions produced by each of thestochastic sources T = i.
The corresponding probabili-ties P(A, Wmax \[ T = i) were then computed by rescor-ing this "best" transcription using either topic-specificlanguage models in the case of topic identification, orspeaker-specific a oustic models for speaker identifica-tion.
(See the above-cited articles for further details.
)For the problem of language identification, we donot have the option of obtaining a single "language-independent" ranscription: the transcription dependsinextricably on the language we are recognizing.
Thusit would appear that in this case we are forced to runseveral recognition passes on each test utterance, one foreach language in question, or at the very least performthe recognition using a recognizer capable of running sev-eral sets of models/languages simultaneously and allowthe best performing language to threshold hypothesesfrom poorer performing ones.We are currently working to develop parallel recogniz-ers trained on telephone-quality speech in a number oflanguages which should allow us to perform exactly thisexperiment.
This effort is described in more detail be-low.
While this development effort is under way, we areexploring the possibility of performing two-language dis-crimination using a single recognizer trained in one ofthe languages.
There are several ways of using the the-ory above to construct a one-recognizer test.
Using tworecognizers, one trained in each language, we would es-timate P(A I T = i) for each of the two languages andthen, as described above, assign the speech sample tothe recognizer producing the best score.
Alternatively,we could look at the log likelihood ratioP(A\[ T = 1)S=logp(A \ ]  T=2) 'and make the assignment based on a threshold S = So,assigning the sample to language #1 if S > So, andto language #2 otherwise.
With only one recognizer,trained, say, in language #1, we could simply impose athreshold on log P(A I T = 1) alone, assigning the speechsample to language #1 if the score was good enough andto language #2 otherwise.
This naive solution suffersfrom a number of problems, most significantly that therecognition score depends on many variables unrelatedto the language - such as speaker, channel, or phoneticcontent - that are not properly controlled for without henormalizing effect of the denominator in the likelihoodratio.In the experiments described below, we have exploredthe possibility of controlling for these confounding fac-tors in the acoustics by introducing a normalizationbased on the acoustics of individual speech frames.
InDragon's speech recognition system the acoustics foreach frame are represented in terms of a feature vec-tor and the recognizer's acoustic models consist in partof probability distributions on the occurrence of thesefeature vectors.
We refer to these models, the outputdistributions for nodes of our hidden Markov models,as PELs (for "phonetic elements").
In normal speech,the PEL sequences we expect o see are constrained bythe phonemic sequences within the words in the recog-nizer's vocabulary, but as a group the PELs should pro-vide good coverage of that region of acoustic parameterspace where speech data lie.
To normalize the recog-nition scores for the one-recognizer tests we compute asecond score using, for each speech frame, the probabilitycorresponding to whichever PEL model - unconstrainedby word-level hypotheses - best matches the acoustics inthat frame.
The product of these frame-by-frame prob-abilities provides a second score (referred to below asthe "maximal acoustic score") that can be used as thedenominator in the log likelihood ratio above.
Presum-ably, when the speech being recognized is in the languageof the recognizer, this optimal frame-by-frame PEL se-quence should be reasonably close to the true PEL se-quence, but when the language is different he maximalacoustic score should be far better than the score pro-duced by the recognizer.438This normalization using best-matching PELs capturessome sense of how well the acoustic signal fits the recog-nizer's models independent of constraints imposed by thewords in the language we are recognizing.
Thus, we ex-pect it to help minimize sources of variability unrelatedto differences between languages.
However, it may bethat different languages cover somewhat different partsof acoustic parameter space and, as we shall see below,this normalization may also have the undesirable side ef-fect of normalizing away this language-rich informationas well.The scores produced by the language identification sys-tem are negative log probabilities, normalized by thenumber of frames in the utterance.
Thus, in practice,the log likelihood ratio translates to a simple difference ofrecognizer (or recognizer and maximal acoustic) scores.3.
IN IT IAL  EXPERIMENTSOur approach to language identification depends cru-cially on the existence of a large vocabulary continuousspeech recognition system, so in order to test the feasi-bility of our language identification strategy, we turnedto our primary LVCSR system, the Wall Street Jour-nal recognizer developed under the ARPA SLS program.This recognition system has been described extensivelyelsewhere (see, for example, \[5\] and \[6\]).
We review itsbasic properties here.The recognizer is a time-synchronous hidden Markovmodel based system.
It makes use of a basic set of 32signal-processing parameters: 1 overall amplitude term,7 spectral parameters, 12 mel-cepstral parameters, and12 mel-cepstral differences.
Our standard practice is toemploy an IMELDA transform \[7\], a transformation con-structed via linear discriminant analysis to select direc-tions in parameter space that are most useful in distin-guishing between designated classes while reducing vari-ation within classes.
For speaker-independent r cogni-tion we choose directions which maximize the averagevariation between phonemes while being relatively in-sensitive to differences within the phoneme class, such asmight arise from different speakers, channels, etc.
Sincethe IMELDA transform generates a new set of parame-ters ordered with respect to their value in discriminatingclasses, directions with little discriminating power be-tween phonemes can be dropped.
We used only the top16 IMELDA parameters for speaker-independent r cogni-tion, divided into four 4-parameter streams.
For speaker-independent recognition, we also normalize the averagespeech spectra cross utterances via blind deconvolutionprior to performing the IMELDA transform, in order tofurther educe channel differences.Each word pronunciation is represented asa sequence ofphoneme models called PICs (phonemes-in-context) de-signed to capture coarticulatory effects due to the pre-ceding and succeeding phonemes.
Because it is imprac-.tical to model all the triphones that could in principlearise, we model only the most common ones and backoff to more generic forms when a recognition hypothe-sis calls for a PIC which has not been built.
The PICsthemselves are modelled as linear HMMs with one ormore nodes, each node being specified by an output dis-tribution - the PELs referred to above - and a doubleexponential duration distribution.
The output distribu-tions of the states were modelled as tied mixtures ofGaussian distributions.
The recognizer used for our lan-guage identification work was trained from the standardWSJ0 SI-12 training speakers (using 7200 sentences inall, totalling about 16 hours of speech data).
BecauseDragon's in-house recordings are made at 12 kHz, ratherthan the WSJ standard of 16 kHz, the training data wasfirst down-sampled to 12 kHz before training the mod-els.
For these experiments, the standard WSJ 20K vo-cabulary and digram language model (based on about40 million words of newspaper text) were used.For the language identification test material, three bilin-gual Dragon employees each recorded 20 English sen-tences taken from a current issue of the Wall StreetJournal.
For Spanish data, they read 20 Spanish sen-tences taken from the financial section of a current issueof America Economia, a Spanish language news mag-azine.
The resulting test corpus thus consisted of 60English and 60 Spanish utterances, averaging about 8seconds in length and recorded on a Shure SM-10 micro-phone at a 12 kHz sample rate.Using the simple (unnormalized) one-recognizer strategydescribed above, we obtained an 83% probability of de-tection at the equal error point (i.e.
the point wherethe probability of detection equals the probability offalse alarm).
After rescoring using the maximal acous-tic score normalization, this figure improved to 95%.
Itis also worth noting that using the maximal acousticscore alone we obtained a result of 68%.
Such a non-speech-based strategy is similar in spirit to approachesto language identification using suhword acoustic fea-tures rather than full speech recognition.
The resultsare summarized in the first line of Table 1.Inspired by the success of this initial trial on read speechdata, we next turned to an assessment of performanceon spontaneous telephone speech, using as test mate-rial speech drawn from the OGI corpus \[8\] of recordedtelephone messages.
This multi-lingual corpus contains"evoked monologues" from 90 speakers in each of ten lan-guages.
For our in-house testing, we selected 10 Spanish439I I R IR -M I M\ ]no IMELDA 82% 90% 73%Table 1: English-Spanish discrimination using the WallStreet Journal recognizer.
The figures give the probabil-ity of detection at the equal error point for the recognizerscore R, the maximal acoustic score M, and the normal-ized recognition score R -  M.and 10 English calls from among the designated OGItraining material.
We used the "story-bt" segments ofthese calls, which run up to 50 seconds in length.
Priorto testing, these were broken at pauses into shorter seg-ments using an automatic "acoustic hopper".
This re-sulted in 102 English segments and 104 Spanish seg-ments, each less than about 10 seconds in length.For our first foray into language discrimination on tele-phone speech, we used the same SWITCHBOARD speechrecognition system used in our topic and speaker identifi-cation work.
This recognizer was trained - and for topicand speaker identification, tested - on telephone conver-sations from the SWITCHBOARD corpus \[9\], collected byTI and now available through the Linguistic Data Con-sortium.
Details of the recognizer are given in \[3\] and\[4\]; it is similar in structure to our Wall Street Journalrecognizer, but was trained on only about 9 hours ofconversational telephone speech.
The recognition per-formance ven on SWITCHBOARD data is very weak, al-though it is still capable of extracting sufficient infor-mation to achieve good topic and speaker identificationperformance.
When used for language identification onOGI utterances the results were disappointing: it wasunable to perform at anything better than chance levels,even aided by the acoustic normalization scoring.Dragon Systems is currently engaged in an effort tocollect telephone data in a number of languages usingan "evoked monologue" format similar to that used forthe OGI corpus.
Our first collection efforts focussed onSpanish data collection and, using about 3 hours of ourown Spanish data and an additional 15 minutes of OGISpanish training material, we built a rudimentary ecog-nition system for Spanish telephone speech.
It has a 5Kvocabulary and a digram language model trained from30 million words of Spanish newswire data.This new Spanish recognizer achieved a 72% probabil-ity of detection at the equal error point on the OGI testdata when using the simple (unnormalized) recognitionscores.
In this case, unlike for the Wall Street Journaland SWITCHBOARD experiments, there was no advantageto using acoustic normalization techniques.
Instead, us-ing the maximal acoustic score for normalization actuallydegraded performance: probability of detection droppedto only 66% at the equal error point.
Interestingly, forthis system, the maximal acoustic score alone did as wellas the regular ecognition score: 74% probability of de-tection at the equal error point.We conjectured that this behavior might be due at leastin part to the fact that the Spanish recognizer, unlikethe Wall Street Journal and SWITCHBOARD recognizers,did not employ our usual speaker-independent IMELDAtransformation.
Recall that this transform is designedto emphasize differences between phoneme classes whileminimizing differences within each class and so may wellbe suppressing language-informative phonetic distinc-tions.
Acoustic normalization may help to overcome thisdeficiency, but may be unnecessary - or even counterpro-ductive - with non-IMELDA models.
To test this hypoth-esis, we re-ran the WSJ language identification test, butthis time with models trained without he IMELDA trans-form.
The results are reported in the second line of Table1.
Without IMELDA, the recognition itself was some-what less accurate, but the language identification per-formance using the recognizer scores was essentially un-changed.
However, as expected, the performance of themaximal acoustic score alone improved without IMELDA,even though that performance r mained well below thatof the full recognizer, and there was a corresponding dropin the normalized score performance.4.
DISCUSSION AND FUTUREWORKAs the initial Wall Street Journal trials indicate, largevocabulary continuous peech recognition is clearly asuccessful strategy for language discrimination on high-quality microphone speech.
Unlike some other trials oflanguage identification on read speech, the WSJ testwas designed to control for such confounding factors asspeaker and channel differences.
The chief drawbacks ofthe test were its small size and the possible bias intro-duced by a recognizer so tuned to the Wall Street Journalgrammar (despite our best efforts to choose Spanish datain a matched omain), but despite these objections theevidence for an LVCSR approach to language identifica-tion is very strong.The performance in the much harder domain of spon-taneous telephone speech is more difficult to interpret.The preliminary testing described above differed in somany respects from the read speech experiments hat itis hard to tease apart the effects without further exper-imentation.
We look forward to exploring the roles of440several components, for example:the use of IMELDA transformations for the speechmodels - -  As suggested by the experiments above,the use of a speaker-independent IMELDA transfor-mation, while unquestionably improving the recog-nition performance, may be removing importantclues about language differences.
To take advan-tage of the recognition boost without sacrificinglanguage~rich information, the best strategy may beto perform an initial recognition pass using IMELDAmodels to generate a transcription, but to scorethe transcript using non-IMELDA models - a two-pass strategy similar to that used in our speakeridentification work.
Indeed, we may want to use a"language sensitive" IMELDA transformation - i.e.one which chooses directions in acoustic parameterspace most useful in distinguishing languages ratherthan phonemes - for the scoring pass, in much thesame way that we employed a "speaker sensitive"IMELDA in our work on speaker classification.?
acoustic normalization of recognition scores - -  Inthe case of one-recognizer tests, it seems importantto have some way of controlling for such sourcesof variation as speaker differences, which shouldplay no role in language discrimination.
The acous-tic normalization described above is a simple at-tempt to achieve this.
However, like the speaker-independent IMELDA, it may also be removing im-portant information about which regions of acousticspace different languages inhabit.?
recognition quality - -  It is our experience from ourtopic and speaker work that small improvements inrecognition performance can yield enormous gainsin classification tasks.
However, in order to takeadvantage of the contextual information availablein large vocabulary CSR, the recognition must ex-ceed a certain minimal evel of performance.
We be-lieve that none of our telephone speech recognitionsystems yet achieve the recognition levels neededto demonstrate he advantage of large vocabularyCSR as a language classification engine, but thatthis minimal evel is well within reach.We are now focussing attention on the task of improv-ing our telephone speech recognizers.
The Spanish rec-oguizer was constructed in under a week's time whenthe Spanish telephone data became available and couldstill profit from such simple measures as further train-ing iterations.
We also hope to introduce into our tele-phone recognition systems uch improved features asphonetically-tied mixture modelling, now used routinelyin our microphone speech recognizers.
The task of recog-nizing natural speech appears to be much more difficultthan recognizing read speech and may require new tech-niques to address the problems of speaking rate, wordcontraction or fragmentation, and non-speech events.Dragon's telephone data collection effort is also contin-uing.
We hope to have at least five hours of recordedtelephone speech in each of seven languages by the endof 1994 with further collections cheduled for next year.This will allow us to create parallel recognition systemsin a number of languages and finally run a two- (or n-)recognizer test of language identification.
In particular,we will be collecting English telephone data and lookforward to building a new English telephone speech rec-ognizer more directly analogous to our current Spanishsystem.
It should be interesting to see how this new En-glish recognizer and the SWITCHBOARD recognizer per-form on each other's data.We believe that these improvements should allow us toachieve the strong language identification performancewe anticipate, based on our earlier work on topic andspeaker identification.Re ferences1.
J.-L. Gauvain and L.F. Lamel, "Identification of Non-Linguistic Speech Features," Proc.
ARPA HLT Work-shop, Princeton, March 1993.2.
Y.K.
Muthusamy and R.A. Cole, "Automatic Segmen-tation and Identification of Ten Languages Using Tele-phone Speech," Proc.
Intl.
Conf.
on Spoken LanguageProcessing 9P, Banff, October 1992.3.
B. Peskin et al, "Topic and Speaker Identificationvia Large Vocabulary Continuous Speech Recognition,"Proe.
ARPA HLT Workshop, Princeton, March 1993.4.
L. Gillick et al, "Application of Large Vocabulary Con-tinuous Speech Recognition to Topic and Speaker Iden-tification Using Telephone Speech," Proc.
ICASSP-93,Minneapolis, Minnesota, April 1993.5.
J.K. Baker et al, "Large Vocabulary Recognition of WallStreet Journal Sentences at Dragon Systems," Pro?.DARPA Speech and Natural Language Workshop, Har-riman, New York, February 1992.6.
R. Roth et al, "Large Vocabulary Continuous SpeechRecognition of Wall Street Journal Data," Proc.ICASSP-93, Minneapolis, Minnesota, April 1993.7.
M.J. Hunt, D.C. Bateman, S.M.
Richardson, and A.Piau, "An Investigation of PLP and IMELDA Acous-tic Representations and of their Potential for Combina-tion," Proc.
ICASSP-91, Toronto, May 1991.8.
Y.K.
Muthusamy, R.A. Cole, and B.T.
Oshika "The OGIMulti-Language Telephone Speech Corpus," Proc.
Intl.Conf.
on Spoken Language Processing g~, Banff, Octo-ber 1992.9.
J.J. Godfrey, E.C.
Holliman, and J. MeDaniel,"SWITCHBOARD: Telephone Speech Corpus for Re-search and Development," Proc.
ICASSP-9~, San Fran-cisco, March 1992.441
