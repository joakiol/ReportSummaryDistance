Repurposing Theoretical Linguistic Datafor Tool Development and SearchFei XiaUniversity of WashingtonSeattle, WA 98195fxia@u.washington.eduWilliam D. Lewis?Microsoft ResearchRedmond, WA 98052-6399wilewis@microsoft.comAbstractFor the majority of the world?s languages,the number of linguistic resources (e.g., an-notated corpora and parallel data) is verylimited.
Consequently, supervised methods,as well as many unsupervised methods, can-not be applied directly, leaving these lan-guages largely untouched and unnoticed.
Inthis paper, we describe the construction of aresource that taps the large body of linguisti-cally analyzed language data that has madeits way to the Web, and propose using thisresource to bootstrap NLP tool development.1 IntroductionUntil fairly recently, most NLP research has focusedon the ten or so majority languages of the world, thecanonical high density languages.
Low density, orresource poor languages (RPLs), have more recentlycaptured the interest of NLP research, mostly be-cause of recent advances in computational technolo-gies and computing power.
As indicated by theirname, RPLs suffer from a lack of resources, namelydata.
Supervised learning techniques generally re-quire large amounts of annotated data, somethingthat is nonexistent or scare for most RPLs.
A greaternumber of RPLs, however, have raw data that isavailable, and the amount and availability of this rawdata is increasing every day as more of it makes itsway to the Web.
Likewise, advances in un- and semi-supervised learning techniques have made raw datamore readily viable for tool development.
Still, how-ever, such techniques often require ?seeds?, or ?pro-totypes?
(c.f., (Haghighi and Klein, 2006)) which areused to prune search spaces or direct learners.An important question is how to create such seedsfor the hundreds to thousands of RPLs.
We describethe construction of a resource that taps the largebody of linguistically analyzed language data thathas made its way to the Web, and propose using this?The work described in this document was done whileLewis was faculty at the University of Washington.resource as a means to bootstrap NLP tool devel-opment.
Interlinear Glossed Text, or IGT, a semi-structured data type quite common to the field oflinguistics, is used to present data and analysis for alanguage and is generally embedded in scholarly lin-guistic documents as part of a larger analysis.
IGT?sunique structure ?
effectively each instance consistsof a bitext between English and some target language?
can be easily enriched through alignment and pro-jection (e.g., (Yarowsky and Ngai, 2001), (Hwa et al,2002)).
The reader will note that the IGT instancein Example (1) consists of a bitext between some tar-get language on the first line, or the target line (inthis case in Welsh), and a third line in English, thetranslation line.
The canonical IGT form, which thisexample is representative of, has intervening linguis-tic annotations and glosses on a second line, the glossline.
Because the gloss line aligns with words andmorphemes on the target line, and contains glossesthat are similar to words on the translation line, itcan serve as a bridge between the target and transla-tion lines; high word alignment accuracy between thethree lines can be achieved without requiring paralleldata or bilingual dictionaries (Xia and Lewis, 2007).Furthermore, the gloss line provides additional in-formation about the target language data, such asa variety of grammatical annotations, including ver-bal and tense markers (e.g., 3sg), case markers, etc.,all of which can provide useful knowledge about thelanguage.
(1) Rhoddodd yr athro lyfr i?r bachgen ddoegave-3sg the teacher book to-the boy yesterday?The teacher gave a book to the boy yesterday?
(Bailyn, 2001)ODIN, the Online Database of INterlinear text(Lewis, 2006), is a resource built over the past fewyears from data harvested from scholarly documents.Currently, ODIN has over 41,581 instances of IGT for944 languages, and the number of IGT instances isexpected to double or triple in the near-term as newmethods for collecting data are brought online.
Al-though the number of instances per language varies,e.g., the maximum currently is 2,891 instances (for529Table 1: The numbers of languages in ODINRange of # of # of % ofIGT instances languages instances instances1000-2891 10 15019 36.11500-999 11 8111 19.50250-499 18 6274 15.08100-249 22 3303 7.9450-99 38 2812 6.7625-49 60 2089 5.0210-24 127 1934 4.651-9 658 2039 4.91Japanese), and the overall number per language mayappear small, it is still possible to harvest significantvalue from IGT for targeted RPLs.
In this paper,we present the ODIN database and methods used tocreate it.
We also present methods we have employedto enrich IGT in order to make it more readily usefulfor bootstrapping NLP tools.
Because the canon ofknowledge embodied in the hundred or so years oflinguistic analysis remains virtually untapped by theNLP community, we provide a bridge between thecommunities by providing linguistic data in a waythat NLP researchers will find useful.
Likewise, be-cause IGT is a common linguistic data type, we pro-vide a search facility over these data, which has al-ready been found to be quite useful to the theoreticallinguistics community.2 Building ODINODIN currently has 41,581 IGT instances for 944languages.
Table 1 shows the number of languagesthat fall into buckets defined by the number of IGTinstances for each language.
For instance, the fourthrow (?bucket?)
says that 22 languages each have 100to 249 IGT instances, and the 3,303 instances in thisbucket account for 7.94% of all instances.
ODIN isbuilt in three steps, as described below.12.1 Crawling for IGT documentsBecause a large number of instances of IGT exist onthe Web,2 we have focused on searching for these1The work of creating ODIN, in some ways, speaksto the need of standardizing IGT (perhaps along withother linguistic data types) such that both humans andmachines can more readily consume the data.
Some re-cent efforts to develop standards for encoding IGT (e.g.,(Hughes et al, 2003), (Bickel et al, 2004)) have met withlimited success, however, since they have not been widelyrecognized and even less frequently adopted.
Over timeit is our hope that these or other standards will see wideruse thus eliminating the need for much of the work pro-posed here.2Although we have no direct data about the totalnumber of IGT instances that exist on the Web, we hy-instances.
The major difficulty with locating docu-ments that contain IGT, however, is reducing the sizeof the search space.
We decided very early in the de-velopment of ODIN that unconstrained Web crawl-ing was too time and resource intensive a process tobe feasible, mostly due to the Web?s massive size.We discovered that highly focused metacrawls werefar more fruitful.
Metacrawling essentially involvesthrowing queries against an existing search engine,such as Google, Yahoo or MSN Live, and crawlingonly the pages returned by those queries.
We foundthat the most successful queries were those that usedstrings contained within IGT itself, e.g.
grammaticalannotations, or grams, such as 3sg, NOM, ACC, etc.In addition, we found precision increased when weincluded two or more search terms per query, withthe most successful queries being those which com-bined grams and language names.
Thus, for exam-ple, although NOM alone returned a large number oflinguistic documents, NOM combined with ACC (orany other high frequency term), or a language name,returned a far less noisy and far more relevant set ofdocuments.Other queries we have developed include: queriesby language names and language codes (drawn fromthe Ethnologue database (Gordon, 2005), which con-tains about 40,000 language names and their vari-ants), by linguists?
names and the languages theywork on (drawn from the Linguist List?s linguistdatabase), by linguistically relevant terms (drawnfrom the SIL linguistic glossary), and by particularwords or morphemes found in IGT and their gram-matical markup.
Table 2 shows the statistics for themost successful crawls and their related search term?types?.
Calculated from the top 100 queries for eachtype, the table presents the most successful querytypes, the average number of documents returned foreach, the average number of documents in which IGTwas actually found, and the average number of IGTinstances netted by each query.
The most relevantmeasure of success is the number of IGT instancesreturned (the obvious focus of our crawling); in turn,the most successful query types are those which con-tain a combination of grams and language names.3pothesize that the total supply is at least several hundredthousand instances.
Given that ODIN contains 41,581instances which have been extracted from approximately3,000 documents, and given that we have located at least60,000 more documents that might contain IGT, we feelour estimate to be reasonable.3Note that target documents are often returned bymultiple queries.
For instance, the documents returnedby ?NOM+ACC+Icelandic?
will also be returned by theindividual query terms ?NOM?, ?ACC?, and ?Icelandic?.530Table 2: The Most Successful Query TypesQuery Type Avg # Avg # docs Avg #docs w/ IGT IGTsGram(s) 1184 239 50Language name(s) 1314 259 33Both grams 1536 289 77and namesLanguage words 1159 193 02.2 IGT detectionAfter crawling, the next step is to identify IGT in-stances in the retrieved documents.
This is a dif-ficult task for which machine learning methods arewell suited.2.2.1 Difficulty in IGT detectionThe canonical form of IGT, as presented in Sec-tion 1, consists of three parts and each part is on asingle line.
However, many IGT instances do not fol-low the canonical format for several reasons.
First,when IGT examples appear in a group, very often thetranslation or glosses are dropped for some examplesin the group because the missing parts can be recov-ered from the context, resulting in two-part IGT.
Inother cases, some IGT examples include multiple tar-get transcriptions (e.g., one part in the native script,and another in a latin transliteration) or even, in rarecases, multiple translations.Second, dictated by formatting constraints, longIGT examples may need to be wrapped one or moretimes, and there are no conventions on how wrappingshould be done, nor how many times it can be done.For short IGT examples, sometimes linguists put thetranslation to the right of the target line rather thanbelow it.
As a result, each part of IGT examplesmay appear on multiple lines and multiple parts canappear on a single line.Third, most IGT-bearing documents on the Webare in PDF, and the PDF-to-text conversion toolswill sometimes corrupt IGT instances (most oftenon the target line).
In some instances, some wordsor morphemes on the target line are inadvertentlydropped in the conversion, or are displaced up ordown a line.
Finally, an IGT instance could fall intomultiple categories.
For instance, a two-part IGTinstance could have a corrupted target line.
All ofthis makes the detection task difficult.2.2.2 Applying machine learning methodsThe first system that we designed for IGT detec-tion used regular expression ?templates?, effectivelylooking for text that resembled IGT.
An example isshown in (2), which matches any three-line instance(e.g., the IGT instance in (1)) such that the first linestarts with an example number (e.g., (1)) and thethird line starts with a quotation mark.
(2) \s*\(\d+\).*\n\s*.*\n\s*\[??"].
*\nUnfortunately, this approach tends to over-selectwhen applied to the documents crawled from theWeb.
Further, many true IGT instances do notmatch any of hand-written templates due to the is-sues mentioned in the previous section.
As a result,both precision and recall are quite low (see Table 4).Given the irregular structure of IGT instances, astatistical system is likely to outperform a rule-basedsystem.
In our second system, we treat the IGT de-tection task as a sequence labeling problem, and ap-ply machine learning methods to the task: first, wetrain a learner and use it to tag each line in a doc-ument with a tag in a pre-defined tag set; then weconvert the best tag sequence into a span sequence.A span is a (start, end) pair, which indicates the be-ginning and ending line numbers of an IGT instance.Among all the tagging schemes we experimentedwith (including the standard BIO tagging scheme),the following 5-tag scheme works the best on the de-velopment set: The five tags are BL (any blank line),O (outside IGT that is not a BL), B (the first linein an IGT), E (the last line in an IGT), I (inside anIGT that is not a B, E, or BL).For machine learning, we use four types of features:F1: The words that appear on the current line.These are the features typically used in a textclassification task.F2: Sixteen features that look at various cues for thepresence of an IGT.
For example, whether theline starts with a quotation, whether the linestarts with an example number (e.g., (1)), andwhether the line contains a large portion of hy-phenated or non-English tokens.F3: In order to find good tag sequences, we includefeatures for the tags of the previous two lines.F4: The same features as in F2, but they are checkedagainst the neighboring lines.
For instance, if afeature f5 in F2 checks whether the current linecontains a citation, f+15 checks whether the nextline contains a citation.After the lines in a document are tagged by thelearner, we identify IGT instances by finding all thespans in the document that match the ?B [I | BL]?E?
pattern; that is, the span starts with a B, endswith an E, and has zero or more I or BL in between.44Other heuristics for converting tag sequences to spansequences produce similar results.531Table 3: Data sets for the IGT detection experiments# files # lines # IGTsTraining data 41 39127 1573Dev data 10 8932 447Test data 10 14592 8432.2.3 Experimental resultsTo evaluate the two detectors, we randomly se-lected 61 ODIN documents and manually markedthe occurrence of IGT instances.
The files were thensplit into training, development, and test sets, andthe size of each set is shown in Table 3.
The annota-tion speed was about four thousand lines per hour.Each file in the development and test sets was an-notated independently by two annotators, and theinter-annotator agreement (f-score) on IGT bound-ary was 93.74% when using exact match (i.e., twospans match iff they are identical).
When partialmatch (i.e., two spans match iff they overlap) wasused, the f-score increased to 98.66%.We used four machine learning algorithms imple-mented in Mallet (McCallum, 2002): decision tree,Naive Bayes, maximum entropy (MaxEnt), and con-ditional random field (CRF).5 Table 4 shows theMaxEnt model?s performance on the development setwith different combinations of features: the highestf-score for exact match in each group is marked inboldface.6 In addition to exact and partial matchresults, we also list the number of spans producedby the system (cf.
the span number in the gold stan-dard is 447) and the classification accuracy (i.e., thepercent of lines receiving correct labels).
The resultsfor CRF are very similar to those for MaxEnt, andboth outperform decision tree and Naive Bayes.Several observations are in order.
First, as ex-pected, the machine learning approach outperformsthe regular expression approach.
Second, althoughF2 contains only sixteen features, it works much bet-ter than F1, which uses all the words occurring in thetraining data.
Third, F4 works much better than F3in capturing contextual information, mainly becauseF4 allows the learner to take into account the infor-mation that appears on both the preceding lines andthe succeeding lines.7 Last, adding F1 and F3 to5For the first three methods, we implemented beamsearch to find the best tag sequences; and for CRF, weused features in F1, F2, and F4, as the model itself in-corporates the information about previous tags already.6F4is an extension of F2, so every combination withF4should include F2as well.
Also, F3should not be usedalone.
Therefore, Table 4 in fact lists all the possiblefeature combinations.7The window for F4is set empirically to [-2,3]; thatis, F4uses the information from the preceding two linesthe F2 + F4 system offers a modest but statisticallysignificant gain.Table 5 shows the results on the test data.
Theperformance of MaxEnt on this data set is slightlyworse than on the development set mainly becausethe test set contains much more corrupted data (dueto pdf-to-text conversion) than both the training anddevelopment sets.8 Nevertheless, the machine learn-ing approach outperforms the regex approach signifi-cantly, reducing the error rate by 52.3%.
In addition,the partial match results are much better than ex-act match results, indicating that many span errorscould be potentially fixed by postprocessing.2.3 Manual review and language IDAbout 45% of IGT instances in the current ODINdatabase were manually checked to verify IGTboundaries and to identify the language names ofthe target lines.
Subsequently, we trained severallanguage ID algorithms with the labeled data, andused them to label the remaining 55% of the IGTinstances in ODIN automatically.The language ID task in this context is differentfrom a typical language ID task in several ways.First, the number of languages in IGT is close toa thousand or even more.
In contrast, the amountof training data for many of the languages is verylimited; for instance, hundreds of languages have lessthan 10 sentences, as shown in Table 1.
Second, somelanguages in the test data might never occur in thetraining data, a problem that we shall call the un-known language problem.
Third, the target sentencesin IGT are very short (e.g., a few words), making thetask more challenging.
Fourth, for languages that donot use a latin-based writing system, the target sen-tences are often transliterated, making the characterencoding scheme less informative.
Last, the context,such as the language names occurring in the docu-ment, provides important cues for the language IDof IGT instances.Given these properties, applying common lan-guage ID algorithms directly will not produce sat-isfactory results.
For instance, Cavnar and Tren-kle?s N-gram-based algorithm yields an accuracy ofas high as 99.8% when tested on newsgroup arti-cles in eight languages (Cavnar and Trenkle, 1994).9and the succeeding three lines.8The corruption not only affects the target lines, butalso the layout of IGT (e.g., the indentation of the threelines).
As a result, features in F2and F4are not as effec-tive as for the development set.
Since the regex templateapproach uses fewer layout features, its performance isnot affected as much.9The accuracy ranges from 92.9% to 99.8% dependingon the article length and a model parameter called profile532Table 4: Performance on the development set (the span number in the gold standard is 447)Features System Classification Exact match Partial matchspan num accuracy prec recall fscore prec recall fscoreRegex templates 269 N/A 68.40 41.16 51.40 99.26 59.73 74.58F1130 81.50 68.46 19.91 30.85 97.69 28.41 44.02F2405 93.28 58.27 52.80 55.40 95.56 86.58 90.85F1+ F3180 80.26 61.67 24.83 35.40 81.11 32.66 46.57F1+ F2420 94.42 63.09 59.28 61.13 93.81 88.14 90.88F2+ F3339 92.68 75.81 57.49 65.39 93.21 70.69 80.40F2+ F4456 96.91 80.92 82.55 81.73 93.64 95.53 94.57F1+ F2+ F3370 93.39 75.14 62.20 68.05 93.51 77.40 84.70F1+ F2+ F4444 97.00 84.68 84.11 84.40 95.95 95.30 95.62F2+ F3+ F4431 97.79 86.77 83.67 85.19 97.68 94.18 95.90F1+ F2+ F3+ F4431 98.00 90.02 86.80 88.38 97.22 93.74 95.44Table 5: Performance on the test set (the span number in the gold standard is 843)Features System Classification Exact match Partial matchspan num accuracy prec recall fscore prec recall fscoreRegex templates 587 N/A 74.95 52.19 61.54 98.64 68.68 80.98F2719 92.45 57.02 48.64 52.50 94.02 80.19 86.56F2+ F4849 95.66 75.50 76.04 75.77 93.76 94.42 94.09F2+ F3+ F4831 95.95 77.14 76.04 76.58 95.19 93.83 94.50F1+ F2+ F3+ F4830 96.83 82.29 81.02 81.65 96.51 95.02 95.76However, when we ran the same algorithm on theIGT data, the accuracy was only 50.2%.10 In con-trast, a heuristic approach that predicts the languageID according to the language names occurring in thedocument yields an accuracy of 65.6%.Because the language name associated with anIGT instance almost always appears somewhere inthe document, we propose to treat the language IDtask as a reference resolution problem, where IGTinstances are the mentions and the language namesappearing in the document are the entities.
A lan-guage identifier simply needs to link the mentionsto the entities, allowing us to apply any good res-olution algorithms such as (Soon et al, 2001; Ng,2005; Luo, 2007) and to provide an elegant solutionto the unknown language problem.
More detail onthis approach will be reported elsewhere.3 Using ODINWe see ODIN being used in a number of differentways.
In another study (Lewis and Xia, 2008), wedemonstrated a method for using ODIN to discoverinteresting and computationally relevant typologicalfeatures for hundreds of the world?s languages auto-matically.
In this section we present two more useslength.10The setting for our preliminary experiments is as fol-lows: there are 10,415 IGT instances over 549 languagesin the training data, and 3064 instances in the test data.The language names of about 12.2% of IGT instances inthe test data never appear in the training data.for ODIN?s data: bootstrapping NLP tools (specif-ically taggers), and providing search over ODIN?sdata (as a kind of large-scale multi-lingual search).3.1 IGT for bootstrapping NLP toolsSince the target line in IGT data does not come withannotations (e.g., POS tags), it is first necessary toenrich it.
Once enriched, the data can be used as abootstrap for tools such as taggers.3.1.1 Enriching IGTIn a previous study (Xia and Lewis, 2007), we pro-posed a three-step process to enrich IGT data: (1)parse the English translation with an English parserand convert English phrase structures (PS) into de-pendency structures (DS) with a head percolationtable (Magerman, 1995), (2) align the target line andthe English translation using the gloss line, and (3)project the syntactic structures (both PS and DS)from English onto the target line.
For instance, giventhe IGT example in Ex (1), the enrichment algorithmwill produce the word alignment in Figure 1 and thesyntactic structures in Figure 2.The   t eache r   gave   a   book   t o     t he     boy    yes te rdayRhoddodd   y r    a th ro      l y f r      i ?
r      bachgen   ddoeG loss  l i ne :T r a n s l a t i o n :T a r g e t  l i n e :g a v e - 3 s g   t h e   t e a c h e r  b o o k   t o - t h e   b o y    y e s t e r d a yFigure 1: Aligning the target line and the Englishtranslation with the help of the gloss line533gave(a) Projecting DSathrobachgenlyfryrddoei?rRhoddodd SNP1 VPNNteacherVBDgaveNP2DTaNP4PPNNtheIN NP3yesterdayNNDTbookNNboyDTtoSNPNNVBDNP NPPPNNIN+DTNNNNDTrhoddodd(gave) yr(the)athro(teacher)lyfr(book)i?r(to-the)bachogen(boy) ddoe(yesterday)teachera boythebooktheyesterdaytoThe(b) Projecting PSFigure 2: Projecting syntactic structure from English to the target languageWe evaluated the algorithm on a small set of 538IGT instances for several languages.
On average,the accuracy of the English DS (i.e., the percentageof correct dependency links in the DS) is 93.48%;the f-score of the word alignment links between thetranslation and target lines is 94.03%, and the ac-curacy of the target DS produced by the projectionalgorithm is 81.45%.
When we replace the automati-cally generated English DS and word alignment withthe ones in the gold standard, the accuracy of targetDS increases significantly, from 81.45% to 90.64%.The details on the algorithms and the experimentscan be found in (Xia and Lewis, 2007).3.1.2 Bootstrapping NLP toolsThe enriched data produced by the projection al-gorithms contains (1) the English DS and PS pro-duced by an English parser, (2) the word alignmentamong the three parts of IGT data, and (3) the tar-get DS and PS produced by the projection algorithm.From the enriched data, various kinds of informationcan be extracted.
For instance, the target syntacticstructures form small monolingual treebanks, fromwhich grammars in various formalisms can be ex-tracted (e.g., (Charniak, 1996)).
The English andtarget syntactic structures form parallel treebanks,from which transfer rules and translation lexicon canbe extracted and used for machine translation (e.g.,(Meyers et al, 2000; Menezes, 2002; Xia and Mc-Cord, 2004)).There are many ways of using the enriched datato bootstrap NLP tools.
Suppose we want to build aPOS tagger.
Previous studies on unsupervised POStagging can be divided into several categories accord-ing to the kind of information available to the learner.The first category (e.g., (Kupiec, 1992; Merialdo,1994; Banko and Moore, 2004; Wang and Schuur-mans, 2005)) assumes there is a lexicon that liststhe allowable tags for each word in the text.
Thecommon approach is to use the lexicon to initializethe emission probability in a Hidden Markov Model(HMM), and run the Baum-Welch algorithm (Baumet al, 1970) on a large amount of unlabeled datato re-estimate transition and emission probability.The second category uses unlabeled data only (e.g.,(Schu?tze, 1995; Clark, 2003; Biemann, 2006; Das-gupta and Ng, 2007)).
The idea is to cluster wordsbased on morphological and/or distributional cues.Haghighi and Klein (2006) showed that adding asmall set of prototypes to the unlabeled data canimprove tagging accuracy significantly.The tagged target lines in the enriched IGT datacan be incorporated in each category of work men-tioned above.
For instance, the frequency collectedfrom the data can be used to bias initial transi-tion and emission probabilities in an HMM model;the tagged words in IGT can be used to label theresulting clusters produced by the word clusteringapproach; the frequent and unambiguous words inthe target lines can serve as prototype examples inthe prototype-driven approach (Haghighi and Klein,2006).
Finally, we can apply semi-supervised learn-ing algorithms (e.g., self-training (Yarowsky, 1995),co-training (Blum and Mitchell, 1998), and transduc-tive support vector machines (Vapnik, 1998)), usingthe tagged sentences as seeds.3.2 SearchOne focus of ODIN is and has always been search:how can linguists find the data that they are inter-ested in and how can the data be encoded in such away as to accommodate the variety of queries that alinguist might ask.
We currently allow four types ofsearch queries: search by language name and code,search by language family, search by concept/gram,and search by linguistic constructions.
The first al-lows the user to specify a language name or ISO codeto search for, and allows the user to view documentsthat contain instances of IGT in that language, aswell as the instances themselves.
The second al-lows the user to specify a language family (familiesas specified in the Ethnologue), and returns similarresults, except grouped by language.
The third al-lows the user to select from a list of known grams,all of which have been mapped to a conceptual space534used by linguists (the GOLD ontology, (Farrar andLangendoen, 2003)).11The final query type, the Construction Search isthe most powerful and most innovative of the queryfacilities currently provided by ODIN.
Rather thanlimiting search to just the content and markup na-tively contained within IGT, Construction Searchsearches over enriched content.
For instance, a searchfor relative clauses can look for either the POS tagsequences that contain a noun followed by an ap-propriate relativizer, or the parse trees that containan NP node with an NP child and a clause child.Currently, 15 construction queries have been imple-mented, with some 40 additional queries being eval-uated and built.
Note that currently constructionqueries are performed on the English translation, noton the target language data.
As syntactic projectionbecomes more reliable, we will allow constructionqueries on the target language data and even querieson both the English and the target (e.g., for com-parative linguistic analyses).
For example, a querycould be something like Find examples where the tar-get line uses imperfective aspect and is in active voiceand the English translation uses passive voice.4 Conclusion and Future DirectionsIn this paper, we introduce Interlinear Glossed Text(IGT), a data type that has been rarely tapped bythe NLP community, and describe the process of cre-ating ODIN, a database of IGT data.
We show thatusing machine learning methods can significantly im-prove the performance of IGT detection.
We thendemonstrate how IGT instances can be enriched anddiscuss several ways of using enriched data to boot-strap NLP tools such as POS taggers.
Finally, wereview the four types of linguistic search that are cur-rently implemented in ODIN.
All of the above showthe value of ODIN as a resource for both NLP re-searchers and linguists.
In the future, we plan to im-prove the IGT detection and language ID algorithmsand will apply them to all the crawled documents.We expect the size of ODIN to grow dramatically.We also plan to use the enriched data to bootstraptaggers and parsers, starting with the ideas outlinedin Section 3.1.2.Acknowledgements This work has been sup-ported, in part, by the Royalty Research Fund atthe University of Washington.
We would also liketo thank Dan Jinguji for providing the preliminary11Most gram-to-concept mapping has been done byhand.
We are currently exploring methods to use ma-chine learning to enhance our ability to identify and mapadditional unknown grams (to be discussed elsewhere).results on language ID expriments, and three anony-mous reviewers for their valuable comments.ReferencesJohn Frederick Bailyn.
2001.
Inversion, dislocationand optionality in Russian.
In Gerhild Zybatow,editor, Current Issues in Formal Slavic Linguis-tics.Michele Banko and Robert C. Moore.
2004.
Partof Speech Tagging in Context.
In Proc.
of the20th International Conference on ComputationalLinguistics (Coling 2004), pages 556?561, Geneva,Switzerland.L.
E. Baum, T. Petrie, G. Soules, and N. Weiss.1970.
A maximization technique occurring inthe statistical analysis of probabilistic functions ofMarkov chains.
Ann.
Math.
Statistics, 41(1):164?171.Balthasar Bickel, Bernard Comrie, and MartinHaspelmath.
2004.
The Leipzig GlossingRules: Conventions for interlinear morpheme-by-morpheme glosses (revised version).
Technical re-port, Max Planck Institute for Evolutionary An-thropology and the Department of Linguistics ofthe University of Leipzig.Chris Biemann.
2006.
Unsupervised part-of-speechtagging employing efficient graph clustering.
InProceedings of the COLING/ACL 2006 StudentResearch Workshop, pages 7?12, Sydney, Aus-tralia, July.Avrim Blum and Tom Mitchell.
1998.
CombiningLabeled and Unlabeled Data with Co-training.
InProc.
of the Workshop on Computational LearningTheory (COLT-1998).William B. Cavnar and John M. Trenkle.
1994.
N-gram-based text categorization.
In Proceedings ofSDAIR-94, 3rd Annual Symposium on DocumentAnalysis and Information Retrieval, pages 161?175, Las Vegas, US.Eugene Charniak.
1996.
Treebank Grammars.
InProc.
of the 13th National Conference on ArtificialIntelligence (AAAI-1996).Alexander Clark.
2003.
Combining distributionaland morphological information for part of speechinduction.
In Proc.
of the 10th Conference of theEuropean Chapter of the Association for Compu-tational Linguistics (EACL-2003).Sajib Dasgupta and Vincent Ng.
2007.
Unsuper-vised part-of-speech acquisition for resource-scarcelanguages.
In Proceedings of the 2007 Joint Con-ference on Empirical Methods in Natural Language535Processing and Computational Natural LanguageLearning (EMNLP-CoNLL), pages 218?227.Scott Farrar and D. Terence Langendoen.
2003.
Alinguistic ontology for the Semantic Web.
GLOTInternational, 7(3):97?100.Raymond G. Gordon, editor.
2005.
Ethnologue:Languages of the World.
SIL International, Dallas,TX, fifteenth edition.Aria Haghighi and Dan Klein.
2006.
Prototype-driven learning for sequence models.
In Proceed-ings of the Human Language Technology Confer-ence of the NAACL (HLT/NAACL 2006), pages320?327, New York City, USA.Baden Hughes, Steven Bird, and Cathy Bow.
2003.Interlinear text facilities.
In E-MELD 2003, Michi-gan State University.Rebecca Hwa, Philip Resnik, Amy Weinberg, andOkan Kolak.
2002.
Evaluating translational cor-respondence using annotation projection.
In Pro-ceedings of the 40th Annual Meeting of the ACL,Philadelphia, Pennsylvania.J.
Kupiec.
1992.
Robust part-of-speech tagging us-ing a hidden Markov model.
Computer Speech andLanguage, 6.William Lewis and Fei Xia.
2008.
AutomaticallyIdentifying Computationally Relevant Typologi-cal Features.
In Proc.
of the Third InternationalJoint Conference on Natural Language Processing(IJCNLP-2008), Hyderabad, India.William Lewis.
2006.
ODIN: A Model for Adapt-ing and Enriching Legacy Infrastructure.
In Proc.of the e-Humanities Workshop, held in cooperationwith e-Science 2006: 2nd IEEE International Con-ference on e-Science and Grid Computing, Ams-terdam.Xiaoqiang Luo.
2007.
Coreference or not: A twinmodel for coreference resolution.
In Proc.
ofthe Conference on Human Language Technologies(HLT/NAACL 2007), pages 73?80, Rochester,New York.David M. Magerman.
1995.
Statistical Decision-Tree Models for Parsing.
In Proc.
of the 33rdAnnual Meeting of the Association for Computa-tional Linguistics (ACL-1995), Cambridge, Mas-sachusetts, USA.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://mallet.cs.umass.edu.Arul Menezes.
2002.
Better contextual translationusing machine learning.
In Proc.
of the 5th con-ference of the Association for Machine Translationin the Americas (AMTA 2002).Bernard Merialdo.
1994.
Tagging English text witha probabilistic model.
Computational Linguistics,20(2).Adam Meyers, Michiko Kosaka, and Ralph Grish-man.
2000.
Chart-based transfer rule applicationin machine translation.
In Proc.
of the 18th Inter-national Conference on Computational Linguistics(COLING 2000).Vincent Ng.
2005.
Machine learning for coreferenceresolution: From local classification to global rank-ing.
In Proc.
of the 43rd Annual Meeting of theAssociation for Computational Linguistics (ACL2005), pages 157?164, Ann Arbor, Michigan.Hinrich Schu?tze.
1995.
Distributional part-of-speechtagging.
In Proc.
of the EACL, pages 141?148.Wee Meng Soon, Hwee Tou Ng, and DanielChung Yong Lim.
2001.
A machine learning ap-proach to coreference resolution of noun phrases.Computational Linguistics, 27(4).V.
Vapnik.
1998.
Statistical learning theory.
Wiley-Interscience.Qin Iris Wang and Dale Schuurmans.
2005.Improved Estimation for Unsupervised Part-of-Speech Tagging.
In Proc.
of IEEE InternationalConference on Natural Language Processing andKnowledge Engineering (IEEE NLP-KE 2005).Fei Xia and William Lewis.
2007.
Multilingual struc-tural projection across interlinear text.
In Proc.
ofthe Conference on Human Language Technologies(HLT/NAACL 2007), pages 452?459, Rochester,New York.Fei Xia and Michael McCord.
2004.
Improv-ing a Statistical MT System with AutomaticallyLearned Rewrite Patterns.
In Proc.
of the 20thInternational Conference on Computational Lin-guistics (COLING 2004), Geneva, Switzerland.David Yarowsky and Grace Ngai.
2001.
Induc-ing Multilingual POS Taggers and NP Bracketersvia Robust Projection across Aligned Corpora.
InProc.
of the 2001 Meeting of the North Americanchapter of the Association for Computational Lin-guistics (NAACL-2001), pages 200?207.David Yarowsky.
1995.
Unsupervised word sensedisambiguation rivaling supervised methods.
InProceedings of the 33rd Annual Meeting of theAssociation for Computational Linguistics (ACL-1995), pages 189?196, Cambridge, Massachussets.536
