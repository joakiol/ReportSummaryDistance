Linguistic Structure Prediction?
2012 Association for Computational LinguisticsNoah A. SmithCarnegie Mellon UniversityMorgan & Claypool (Synthesis Lectures on Human Language Technologies, editedby Graeme Hirst, volume 13), 2011, xx+248 pp; paperbound, ISBN 978-1-60845-405-1,$60.00; ebook, ISBN 978-1-60845-406-8, $30.00 or by subscriptionReviewed byChris QuirkMicrosoft ResearchNoah Smith?s ambitious newmonograph, Linguistic Structure Prediction, ?aims to bridgethe gap between natural language processing and machine learning.?
Given that cur-rent natural language processing (NLP) research makes heavy demands on machine-learning techniques, and a sizeable fraction of modern machine learning (ML) researchfocuses on structure prediction, this is clearly a timely and important topic.
To addressthe gaps and overlaps between these two large and well-developed fields in five briefchapters is a difficult feat.
The text, though not without its flaws, does an admirable jobof building this bridge.An introductory first chapter surveys current research areas in statistical NLP,cataloging and defining many common linguistic structure prediction tasks.
Machinelearning students new to the area are likely to find this helpful albeit a bit terse; NLPstudents will likely consider this section primarily a review.
The subsequent chapterschange character abruptly, delving into mathematical details and heavy formalism.Chapter 2 introduces the concept of decoding, presenting five distinct viewpointson the search for the highest scoring structure.
The reader is quickly ushered throughgraphical models, polytopes, grammars, hypergraphs, and weighted deduction sys-tems, with descriptions based on an example in sequence tagging.
The broad coverage,multi-viewpoint discussion encourages the reader to make connections between manydistinct approaches, and provides solid formalism for reasoning about decoding prob-lems.
It is a comprehensive introduction to the most common and effective decodingapproaches, with one significant exception: the recent advances in dual decompositionand Lagrangian relaxation methods.
Timing is likely the culprit.
This book was devel-oped mainly from 2006 to 2009, whereas dual decomposition did not attain notoriety inour community until a few years later (Rush et al 2010).
Relaxation approaches, thoughpotentially a passing phase, have successfully broadened the reach of simpler decodingtechniques into more complicated domains such as structured event extraction.
Theywould have made a nice addition.
Regardless, this second chapter equips the readerwith sufficient machinery to solve a number of structured prediction problems.Chapter 3 applies the machinery described in the prior chapter to the problemof supervised structure induction.
Probabilistic generative and conditional models areintroduced in some detail, followed by a discussion of margin-based methods.
HiddenMarkov models (HMMs) and probabilistic context-free grammars are introduced indetail, followed by solid descriptions of maximum likelihood estimation and smooth-ing.
The section on conditional models is well written and crucial, because so manycommonly used tasks can be treated as sequence modeling using techniques suchas conditional random fields.
Much of the subject matter introduced abstractly inChapter 2 is presented in this chapter using specific algorithms.
For instance, sequenceComputational Linguistics Volume 38, Number 2modeling is discussed broadly in Chapter 2; the specific algorithms for HMMs are fullydefined in Chapter 3.
This coarse-to-fine introduction of material may challenge readerswho are accustomed to more practical descriptions of material.
Were I to teach a coursebased on this book, I would be tempted to present the third chapter before the second.Chapter 4 focuses on semisupervised, unsupervised, and hidden variable learning.With a good mix of theory and practical examples, Expectation-Maximization (EM) isintroduced and grounded in several problems, then generalized with log-linear modelsand approximated with contrastive estimation.
Hard EM is mentioned in the contextof several examples, though a more detailed description of this potentially importanttechnique (cf.
Spitkovsky et al, 2010) would bridge thematerial of Chapters 2 and 4.
Thechapter then describes Bayesian approaches to NLP, working from theory into specifictechniques and landing in models.
Finally, a brief section is devoted to the related areaof hidden variable learning.Chapter 5 begins by describing the partition function, as well as inference tech-niques for the partition function and decoding methods.
I found it strange that thisimportant section was postponed so late in the book; much of the material was forward-referenced throughout Chapter 4.
Regardless, the techniques are described in a unifying,generic manner.
The book concludes with a discussion of minimum Bayes risk decod-ing, and a few other variants.Four appendices are devoted to optimization, experimental techniques, maximumentropy, and locally normalized conditional models.
All of these sections provide someuseful background.
The section on hypothesis testing in Appendix B would be espe-cially useful to students new to the area.
It can be difficult to pick the correct hypothesistesting method in general, and this problem is exacerbated in structure prediction.
Thismaterial serves as a good guide for a researcher hoping to evaluate how effective thesemethods are.I have some concerns about the intended audience.
Descriptions quickly descendinto heavy notation and require knowledge of a broad range of mathematical concepts,from marginal polytopes to semirings.
I suspect the average NLP graduate studentwould find it difficult to approach much of the material without a series of coursesin probability, statistics, and machine learning.
The book is also very theoretical: Fewconcrete algorithms are provided.
Instead, the concepts are introduced using onlymathematics and formalism.
For readers already conversant in the mapping frommathematical descriptions into concrete algorithms and implementations, this will notbe a significant barrier.
From the other direction, the structures used in NLP (e.g.,dependency trees) are relatively well motivated, though a machine-learning researchernew to the areamight benefit from a fuller introduction to NLP.
However, the text servesas an effective guide for introducing the machine-learning community into the NLPcommunity, but I feel it would be challenging to use in the other direction.This may be a personal bias, but I was surprised by the avoidance of machine-translation?related techniques, despite obvious influences.
Why resort to the term ?de-coding?
if not because of decipherment and translation?
One of the most effective usesof hidden variable learning is in word alignment; it seems like a personal example.Of course, building an effective machine-translation system requires a huge amountof engineering in addition to the underlying theory, but I felt some discussion of theproblem and effective techniques would be pertinent.Despite my struggles with the book?s organization and a few important omissions,I must admit that I want a copy for my bookshelf.
The author covers a huge amount ofmaterial in just under 200 pages, touching on some of the most important algorithmictechniques and viewpoints in modern statistical NLP.
At times the text reads like a456Book Reviewssurvey, touching very briefly on a huge range of topics.
Yet the survey is comprehensiveand enlightening, tying together a broad range of topics and viewpoints.
Younger grad-uate students may require serious effort to comprehend the full text, but the modernNLP researcher looking to advance the state of the art in structured prediction musttruly understand the concepts presented here.ReferencesRush, Alexander M., David Sontag,Michael Collins, and Tommi Jaakkola.2010.
On dual decomposition and linearprogramming relaxations for naturallanguage processing.
In Proceedings of the2010 Conference on Empirical Methods inNatural Language Processing, pages 1?11.Cambridge, MA.Spitkovsky, Valentin I., Hiyan Alshawi,Daniel Jurafsky, and ChristopherD.
Manning.
2010.
Viterbi trainingimproves unsupervised dependencyparsing.
In Proceedings of the FourteenthConference on Computational NaturalLanguage Learning, pages 9?17.Uppsala.This book review was edited by Pierre Isabelle.Chris Quirk is a Researcher in the Natural Language Processing group at Microsoft Research.
Hisresearch interests include machine translation and paraphrase, particularly at the confluence oflarge data and linguistic analysis tools.
Quirk?s e-mail address is chrisq@microsoft.com.457
