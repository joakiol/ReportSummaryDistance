Proceedings of ACL-IJCNLP 2015 System Demonstrations, pages 139?144,Beijing, China, July 26-31, 2015.c?2015 ACL and AFNLPWriteAhead: Mining Grammar Patterns in Corpora for Assisted WritingTzu-Hsi Yen, Jian-Cheng Wu+Department of Computer ScienceNational Tsing Hua UniversityHsinchu, Taiwan, R.O.C.
30013{joe, jiancheng}@nlplab.ccJoanne Boisson, Jim Chang, Jason Chang+National Academy for Educational ResearchMinstry of EducationTaipei, Taiwan, R.O.C.
30013{joanne,jim,jason}@nlplab.ccAbstractThis paper describes WriteAhead, aresource-rich, Interactive Writing Envi-ronment that provides L2 learners withwriting prompts, as well as ?get it right?advice, to helps them write fluently andaccurately.
The method involves automat-ically analyzing reference and learner cor-pora, extracting grammar patterns with ex-ample phrases, and computing dubious,overused patterns.
At run-time, as the usertypes (or mouses over) a word, the systemautomatically retrieves and displays gram-mar patterns and examples, most relevantto the word.
The user can opt for patternsfrom a general corpus, academic corpus,learner corpus, or commonly overused du-bious patterns found in a learner corpus.WriteAhead proactively engages the userwith steady, timely, and spot-on informa-tion for effective assisted writing.
Pre-liminary experiments show that WriteA-head fulfills the design goal of foster-ing learner independence and encouragingself-editing, and is likely to induce betterwriting, and improve writing skills in thelong run.1 IntroductionThe British Council has estimated that roughlya billion people are learning and using Englisharound the world (British Council 1997), mostlyas a second language, and the numbers are grow-ing.
Clearly, many of L2 speakers of English feelthemselves to be at a disadvantage in work thatrequires communication in English.
For exam-ple, Flowerdew (1999) reports that a third of HongKong academics feel disadvantged in publishing apaper internationally, as compared to native speak-ers.These L2 speakers and learners provide moti-vation for research and development of computerassisted language learning, in particular tools thathelp identify and correct learners?
writing errors.Much work has been done on developing tech-nologies for automated gramatical error correc-tion (GEC) to assist language learners (Leacock,Chodorow, Gamon, and Tetreault 2010).
How-ever, such efforts have not led to the developmentof a production system (Wampler, 2002).However, Milton (2010) pointed out that fo-cusing on fully-automatic, high quality GEC so-lutions has overlooked the long-term pedagogi-cal needs of L2 learner writers.
Learners couldbe more effectively assisted in an interactivewritring environment (IWE) that constantly pro-vides context-sensitive writing suggestions, rightin the process of writing or self-editing.Consider an online writer who starts a sentencewith ?This paper discusses ....?
The best way thesystem can help is probably displaying the patternsrelated to the last word discuss such as discusssomething and discusses with someone, that helpthe user to write accurately and fluently.
If the usersomehow writes or pastes in some incorrect sen-tence, ?This paper discusses about the influence ofinterference and reflection of light.?
The best waythe system can help is probably displaying the er-roneous or overused pattern, discuss about some-thing, that prompts the user to change the sentenceto ?This paper discusses the influence of interfer-ence and reflection of light.
?Intuitively, by extracting and displaying suchpatterns and examples, distilled from a very largecorpus, we can guide the user towards writing flu-139Figure 1: Example WriteAhead session where an user typed ?This paper present method?.ently, and free of grammatical errors.We present a new system, WriteAhead, thatproactively provides just-in-time writing sugges-tions to assist student writers, while they typeaway.
Example WriteAhead suggestions for ?Wediscussed ...?
are shown in Figure 1.
WriteAheadhas determined the best patterns and examples ex-tracted from the underlying corpus.
WriteAheadlearns these patterns and examples automaticallyduring training by analyzing annotated dictionaryexamples and automatically tagged sentences in acorpus.
As will be described in Section 4, we usedthe information on collocation and syntax (ICS)for example sentences from online Macmillan En-glish Dictionary, as well as in the Citeseer x cor-pus, to develop WriteAhead.At run-time, WriteAhead activates itself as theuser types in yet another word (e.g., ?discussed?in the prefix ?We discussed ...?).
WriteAhead thenretrieves patterns related to the last word.
WriteA-head goes one step further and re-ranks the sug-gestions, in an attempt to move most relevant sug-gestions to the top.
WriteAhead can be accessed athttp://writehead.nlpweb.org/.In our prototype, WriteAhead returns the sug-gestions to the user directly (see Figure 1); alterna-tively, the suggestions returned by WriteAhead canbe used as input to an automatic grammar checkeror an essay rater.The rest of this paper is organized as follows.We review the related work in the next section.Then we present our method for automaticallylearning normal and overused grammar patternsand examples for use in an interactive writing en-vironment (Section 3).
Section 5 gives a demon-stration script of the interactive writing environ-ment.2 Related WorkMuch work described in a recent survey (Lea-cock, Chodorow, Gamon, and Tetreault 2010)show that the elusive goal of fully-automatic andhigh-quality grammatical error correction is farfrom a reality.
Moreover, Milton (2010) pointedout that we should shift the focus and responsi-bility to the learner, since no conclusive evidenceshows explicit correction by a teacher or machineis leads to improved writing skills (Truscott, 1996;Ferris and Hedgcock, 2005).
In this paper, wedevelop an interactive writing environment (IWE)that constantly provides context-sensitive writingsuggestions, right in the process of writing or self-editing.Autocompletion has been widely used in manylanguage production tasks (e.g., search query andtranslation).
Examples include Google Suggestand TransType, which pioneered the interactiveuser interface for statistical machine translation(Langlais et al., 2002; Casacuberta et al.
2009).However, previous work focuses exclusively on140???????????????????????
?-Procedure ExtractPatterns(Sent, Keywords, Corpus)(1) Learning phrase templates for grammar patterns ofcontent words (Section 3.1.1)(2) Extracting grammar patterns for all keywords in thegiven corpus based on phrase templates (Section 3.1.2)(3) Extracting exemplary instances for all patterns of allkeywords (Section 3.1.3)???????????????????????
?-Figure 2: Outline of the pattern extraction processproviding surface suggestions lacking in general-ity to be truely effective for all users in differentwriting situation.
In contrast, we provide sugges-tions in the form of theoretical and pedagogicallysound language representation, in the form of Pat-tern Grammar (Hunston and Francis 2000).
Wealso provide concise examples much like concor-dance advocated by Sinclair (1991).Much work has been done in deriving context-free grammar from a corpus, while very little workhas been done in deriving pattern grammar.
Ma-son and Hunston (2004) reports on a pilot studyto automatically recognize grammar patterns forverbs, using only limited linguistic knowledge.
Itis unclear whether their method can scale up andextend to other parts of speech.
In contrast, weshow it is feasible to extract grammar patterns fornouns, verbs, and adjectives on a large scale usinga corpus with hundreds of million words.Researchers have been extracting error patternsin the form of word or part of speech (POS) se-quencesto detect real-word spelling errors (e.g.,Golding and Schabes, 1996; Verberne, 2002).
Forexample, the sequence of det.
det.
n. definitelyindicate an error, while v. prep.
adv.
might ormight not indicate an error.
For this reason, func-tion words (e.g., prepositions) are not necessarilyreduced to POS tags (e.g., v. to adv.).
Sometimes,even lexicalized patterns are necessary (e.g., go toadv.)
Sun et al.
(2007) extend n-grams to non-continuous sequential patterns allowing arbitrarygaps between words.
In a recent study closer toour work, Gamon (2011) use high-order part-of-speech ngram to model and detect learner errorson the sentence level.In contrast to the previous research in devel-oping computer assisted writing environment, wepresent a system that automatically learns gram-mar patterns and examples from an academic writ-ten corpus as well as learner corpus, with the goalof providing relevant, in-context suggestions.3 MethodNon-native speakers often make grammatically er-ror, particularly in using some common words inwriting (e.g., discuss vs. discuss *about).
In addi-tion, using dictionaries or mere lexical suggestionsto assist learner in writing is often not sufficient,and the information could be irrelevant at times.In this section, we address such a problem.
Givenvarious corpora (e.g., BNC or CiteseerX) in a spe-cific genre/domain and a unfinished or completedsentence, we intend to assist the user by retrievingand displaying a set of suggestions extracted fromeach corpus.
For this, by a simple and intuitionalmethod, we extract grammatical error patterns andcorrection such that the top ranked suggestions arelikely to contain a pattern that fits well with thecontext of the unfinished sentence.
We describethe stage of our solution to this problem in the sub-sections that followed.3.1 Extracting Grammar PatternsWe attempt to extract grammatical error patternsand correction for keywords in a given corpus toprovide writing suggestions, in order to assist ESLlearners in an online writing session.
Our extrac-tion process is shown in Figure 2.3.1.1 Learning Extraction Templates In thefirst stage of the extraction process (Step (1) inFigure 2), we generate a set of phrase templatesfor identifying grammar patterns based on infor-mation on Collocation and Syntax (ICS) in an on-line dictionary.For example, the dictionary entry of difficultymay provide examples with ICS pattern, such ashave difficulty/problem (in) doing something:Six months after the accident, he still has difficultywalking.
This complicated pattern with parenthet-ical and alternative parts can be expanded to yieldpatterns such as have difficulty in doing some-thing.
By generalizing such a pattern into tem-plates with PoS and phrase tags (e.g., v. np prep.v np, we can identify instances of such a patternin tagged and chunked sentences.
For this, we ex-pand the parentheticals (e.g., (in)) and alternatives(e.g., difficulty/problem) in ICS.Then, we replace (non-entry) words in ICS withthe most frequent part of speech tags or phrasetags, resulting in sequences of POS and phrase la-bels (e.g., v. difficulty prep.
v. np).
Then, wetake only the complementation part (e.g., prep.v.
np).
Finally, we convert each complementa-141tion into a regular expression for a RegExp chunkparser.Subsequently, we convert each template into aregular expression of chunk labels, intended tomatch instances of potential patterns in taggedsentences.
The chunk labels typically are repre-sented using B-I-O symbols followed by phrasetype, with each symbol denoting Beginning,Inside, and Outside of the phrase.
Note that in or-der to identify the head of a phrase, we change theB-I-O representation to I-H-O, with H signifyingthe Head.3.1.2 Extracting Patterns In the second stageof the extraction process (Step (2) in Figure 2),we identify instances of potential patterns for allkeywords.
These instances are generated for eachtagged and chunked sentence in the given corpusand for each chunk templates obtained in the pre-vious stage.We adopt the MapReduce framework to extractsalient patterns.
At the start of the Map Proce-dure, we perform part of speech, lemmatization,and base phrase tagging on the sentences.
Wethen find all pattern instances anchoring at a key-word and matching templates obtained in the firststage.
Then, from each matched instance, we ex-tract the tuple, (grammar pattern, collocation, andngrams).
Finally, we emit all tuples extracted fromthe tagged sentence.
The map procedure is appliedto every tagged sentence in the given corpus.In the reduce part, the ReducePattern Proce-dure receives a batch of tuples, locally sorted andgrouped by keyword, as is usually done in theMapReduce paradigm.
At the start of the Redu-cePattern Procedure, we further group the tupleby pattern.
Then we count the number of tuplesof each pattern as well as within-group averageand standard deviation of the counts.
Finally, withthese statistics, we filter and identify patterns morefrequent than average by 1 standard deviation.
TheReducePattern Procedure is applied to all tuplesgenerated in the Map Procedure.
Sample outputof this stage is shown in Table 1.3.1.3 Extracting Exemplary Phrases In thethird and final stage of extraction, we generate ex-emplary phrases for all patterns of all keywordsof interest using the ReduceCollExm Procedure,which is done after the Map procedure, and essen-tially the same as the ReducePattern Procedure inthe second stage (Section 3.1.2).In the spirit of the GDEX method (KilgarriffTable 1: Example difficulty patterns extracted.Pattern Count Exampledifficulty of something 2169 of the problemdifficulty in doing something 1790 in solving the problemsdifficulty of doing something 1264 of solving this problemdifficulty in something 1219 in the previous analysesdifficulty with something 755 with this approachdifficulty doing something 718 using itNote: There are 11200 instances of potential difficulty pat-terns with average count of 215 and a standard deviation of318et al.
2008) of selecting good dictionary exam-ples for a headword via collocations, we proposea method for selection good example for a pattern.For this, we count and select salient collocations(e.g., the heads of phrases, difficulty in process inpattern instance difficulty in the optimization pro-cess).
For each selected collocation, we choosethe most frequent instance (augmented with con-text) to show the user the typical situation of usingthe collocation.These examples also facilitate the system inranking patterns (as will be described in Section3.2).
For that, we add one chunk before, and onechunk after the collocational instance.
For exam-ple, the collocation, method for solution of equa-tion is exemplified using the authentic corpus ex-ample, ?method for exact solution of homogeneouslinear differential equation?
in the context of ?re-port a new analytical ...
with.?
We use a similarprocedure as describe in Section 3.1.2 to extractexamples.After the grammar patterns are extracted from areference corpus and a learner corpus, we normal-ize and compared the counts of the same pattern inthe two corpora and compuate an overuse ratio forall patterns and retain patterns with a high overuseratio.3.2 Retrieving and Ranking SuggestionsOnce the patterns and examples are automaticallyextracted for each keyword in the given corpus,they are stored and indexed by keyword.
At run-time in a writing session, WriteAway constantlyprobes and gets the last keyword of the unfinishedsentence Sent in the text box (or the word underthe mouse when in editing mode).
With the key-word as a query, WriteAway retrieves and ranks allrelevant patterns and examples (Pat and Exm) aim-ing to move the most relevant information towardthe top.
We compute the longest common subse-quence (LCS) of Sent and an example, Exm.
Theexamples and patterns are ranked by142Score(Exm) = | LCS(Exm, Sent) | ?
Count(Exm).Score(Pat) =?Score(E), where E is an example of PatTo improve ranking, we also try to find thelongest similar subsequence (LSS) between theuser input, Sent and retrieved example, Exmbased on distributional word similarity using theword2vec (Mikolov et al., 2013) cosine distance.The new score function is:Score(Exm) = LSS(Exm, Sent) ?
Count(Exm),LSS(Exm, Sent) = max sim(Exmsub, Sentsub),sim(A, B) = 0, if |A| 6= |B|.sim(A, B) =?word-sim(Ai, Bi), otherwise.4 Experiments and ResultsFor training, we used a collection of approxi-mately 3,000 examples for 700 headwords ob-tained from online Macmillan English Dictionary(Rundel 2007), to develop the templates of pat-terns.
The headwords include nouns, verbs, ad-jectives, and adverbs.
We then proceeded to ex-tract writing grammar patterns and examples fromthe British National Corpus (BNC, with 100 mil-lion words), CiteseerX corpus (with 460 millionwords) and Taiwan Degree Thesis Corpus (with10 million words).
First, we used Tsujii POS Tag-ger (Tsuruoka and Tsujii 2005) to generate taggedsentences.
We applied the proposed method togenerate suggestions for each of the 700 contentkeywords in Academic Keyword List.4.1 Technical ArchitectureWriteAhead was implemented in Python and FlaskWeb framework.
We stored the suggestions inJSON format using PostgreSQL for faster access.WriteAhead server obtains client input from a pop-ular browser (Safari, Chrome, or Firefox) dynam-ically with AJAX techniques.
For uninterruptedservice and ease of scaling up, we chose to hostWriteAhead on Heroku, a cloud-platform-as-a-service (PaaS) site.4.2 Evaluating WriteAheadTo evaluate the performance of WriteAhead, werandomly sampled 100 sentences from a learnercorpus with complementation errors.
For eachsentence, we identify the keyword related to the er-ror and checked whether we have identify an over-used pattern relevant to the error, and if positivethe rank of this pattern.
We then use the Mean Re-ciprocate Rank (MRR) to measure performance.Evaluation of WriteAhead showed a MMR rate of.30 and a recall rate of 24%.
The Top 1, 2, 3 recallrates are 31%, 35%, and 38% respectively5 Demo scriptIn this demo, we will present a new writing assis-tance system, WriteAhead, which makes it easy toobtain writing tips as you type away.
WriteAheaddoes two things really well.First, it examines the unfinished sentence youjust typed in and then automatically gives you tipsin the form of grammar patterns (accompaniedwith examples similar to those found in a gooddictionary ) for continuing your sentence.Second, WriteAhead automatically ranks sug-gestions relevant to your writing, so you spend lesstime looking at tips, and focus more on writingyour text.You might type in The paper present methodand you are not sure about how to continue.
Youwill instantly receive tips on grammar as well ascontent as shown in Figure 1.
At a quick glance,you might find a relevant pattern, method for do-ing something with examples such as This paperpresents/describes a method for generating solu-tions.
That could tip you off as to change the sen-tence into This paper presents a method, thus get-ting rid of tense and article errors, and help youcontinue to write something like method for ex-tracting information.Using WriteAhead this way, you could at oncespeed up writing and avoid making common writ-ing errors.
This writing and tip-taking process re-peats until you finish writing a sentence.
And asyou start writing a new, the process starts all overagain.Most autocompletion systems such as GoogleSuggest and TransType offer word-level sugges-tions, while WriteAhead organizes, summarizes,and ranks suggestions, so you can, at a glance,grasp complex linguistic information and makequick decision.
Our philosophy is that it is impor-tant to show information from general to specificto reduce the cognitive load, so while minding theform, you can still focus on the content of writing.WriteAhead makes writing easy and fun, and italso turns writing into a continuous learning pro-cess by combining problem solving and informa-tion seeking together to create a satisfying userexperience.
WriteAhead can even help you beatWriters Block.
WriteAhead can be accessed athttp://writeahead.nlpweb.org/.1436 ConclusionMany avenues exist for future research and im-provement of WriteAhead.
For example, corporafor different language levels, genres (e.g., emails,news) could be used to make the suggestions morerelevant to users with diverse proficiency levelsand interests.
NLP, IR, and machine learningtechniques could be used to provide more rele-vant ranking, to pin-point grammatical errors, orto generate finer-grained semantic patterns (e.g.,assist someone in something or attend activ-ity/institution) Additionally, an interesting direc-tion is identifying grammar patterns using a CRFsequence labeller.In summary, in an attempt to assist learner writ-ers, we have proposed a method for providingwriting suggestion as a user is typewriting.
Themethod involves extracting, retrieving, and rank-ing grammar patterns and examples.
We have im-plemented and evaluated the proposed method asapplied to a scholarly corpus with promising re-sults.ReferencesCasacuberta, Francisco, et al.
?Human interaction forhigh-quality machine translation.?
Communicationsof the ACM 52.10 (2009): 135-138.Dagneaux, Estelle, Sharon Denness, and SylvianeGranger.
?Computer-aided error analysis.?
System26.2 (1998): 163-174.Flowerdew, John.
?Problems in writing for scholarlypublication in English: The case of Hong Kong.
?Journal of Second Language Writing 8.3 (1999):243-264.Ferris, Dana, and J. S. Hedgcock.
?Teacher responseto student writing: Issues in oral and written feed-back.?
Teaching ESL composition: Purpose, pro-cess and practice (2005): 184-222.Graddol, David.
?The future of English?
: A guide toforecasting the popularity of the English language inthe 21st century.?
(1997).Granger, Sylviane, and Paul Rayson.
Automatic profil-ing of learner texts.?
Learner English on computer(1998): 119-131.Granger, Sylviane, and Stephanie Tyson.
?Connectorusage in the English essay writing of native and non-native EFL speakers of English.?
World Englishes15.1 (1996): 17-27.Golding, Andrew R., and Yves Schabes.
?Combin-ing trigram-based and feature-based methods forcontext-sensitive spelling correction.?
Proceedingsof the 34th annual meeting on Association for Com-putational Linguistics.
Association for Computa-tional Linguistics, 1996.Gamon, Michael.
?High-order sequence modeling forlanguage learner error detection.?
Proceedings ofthe 6th Workshop on Innovative Use of NLP forBuilding Educational Applications.
Association forComputational Linguistics, 2011.Hunston, Susan, and Gill Francis.
Pattern grammar:A corpus-driven approach to the lexical grammar ofEnglish.
Amsterdam: John Benjamins, 2000.Leacock, Claudia, et al.
?Automated grammatical errordetection for language learners.?
Synthesis lectureson human language technologies 3.1 (2010): 1-134.Milton, John, and Vivying SY Cheng.
?A toolkitto assist L2 learners become independent writers.
?Proceedings of the NAACL HLT 2010 Workshopon Computational Linguistics and Writing: Writ-ing Processes and Authoring Aids.
Association forComputational Linguistics, 2010.Mason, Oliver, and Susan Hunston.
?The auto-matic recognition of verb patterns: A feasibilitystudy.?
International journal of corpus linguistics9.2 (2004): 253-270.Mikolov, Tomas, et al.
?Distributed representations ofwords and phrases and their compositionality.?
Ad-vances in Neural Information Processing Systems.2013.Sun, Guihua, et al.
?Detecting erroneous sentences us-ing automatically mined sequential patterns.?
An-nual Meeting-Association for Computational Lin-guistics.
Vol.
45.
No.
1.
2007.Truscott, John.
?The case against grammar correc-tion in L2 writing classes.?
Language learning 46.2(1996): 327-369.Tsuruoka, Yoshimasa, and Jun?ichi Tsujii.
?Chunkparsing revisited.?
Proceedings of the Ninth Inter-national Workshop on Parsing Technology.
Associ-ation for Computational Linguistics, 2005.Verberne, Suzan.
?Context-sensitive spell checkingbased on word trigram probabilities.?
Unpublishedmasters thesis, University of Nijmegen (2002).Sinclair J.
(1991) Corpus, Concordance, Collocation.Oxford University Press, Hong Kong.P.
Langlais, G. Foster, and G. Lapalme.
2000.TransType: a computer-aided translation typing sys-tem.
In Workshop on Embedded Machine Transla-tion Systems.Caragea, Cornelia, et al.
?CiteSeer x: A ScholarlyBig Dataset.?
Advances in Information Retrieval.Springer International Publishing, 2014.
311-322.144
