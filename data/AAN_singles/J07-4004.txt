Wide-Coverage Efficient Statistical Parsingwith CCG and Log-Linear ModelsStephen Clark?University of OxfordJames R.
Curran?
?University of SydneyThis article describes a number of log-linear parsing models for an automatically extractedlexicalized grammar.
The models are ?full?
parsing models in the sense that probabilities aredefined for complete parses, rather than for independent events derived by decomposing the parsetree.
Discriminative training is used to estimate the models, which requires incorrect parses foreach sentence in the training data as well as the correct parse.
The lexicalized grammar formalismused is Combinatory Categorial Grammar (CCG), and the grammar is automatically extractedfrom CCGbank, a CCG version of the Penn Treebank.
The combination of discriminative trainingand an automatically extracted grammar leads to a significant memory requirement (up to25 GB), which is satisfied using a parallel implementation of the BFGS optimization algorithmrunning on a Beowulf cluster.
Dynamic programming over a packed chart, in combination withthe parallel implementation, allows us to solve one of the largest-scale estimation problems in thestatistical parsing literature in under three hours.A key component of the parsing system, for both training and testing, is a Maximum En-tropy supertagger which assigns CCG lexical categories to words in a sentence.
The super-tagger makes the discriminative training feasible, and also leads to a highly efficient parser.Surprisingly, given CCG?s ?spurious ambiguity,?
the parsing speeds are significantly higherthan those reported for comparable parsers in the literature.
We also extend the existing parsingtechniques for CCG by developing a new model and efficient parsing algorithm which exploitsall derivations, including CCG?s nonstandard derivations.
This model and parsing algorithm,when combined with normal-form constraints, give state-of-the-art accuracy for the recovery ofpredicate?argument dependencies from CCGbank.
The parser is also evaluated on DepBank andcompared against the RASP parser, outperforming RASP overall and on the majority of relationtypes.
The evaluation on DepBank raises a number of issues regarding parser evaluation.This article provides a comprehensive blueprint for building a wide-coverage CCG parser.We demonstrate that both accurate and highly efficient parsing is possible with CCG.?
University of Oxford Computing Laboratory, Wolfson Building, Parks Road, Oxford, OX1 3QD, UK.E-mail: stephen.clark@comlab.ox.ac.uk.??
School of Information Technologies, University of Sydney, NSW 2006, Australia.E-mail: james@it.usyd.edu.au.Submission received: 27 April 2006; revised submission received: 30 November 2006; accepted for publication:16 March 2007.?
2007 Association for Computational LinguisticsComputational Linguistics Volume 33, Number 41.
IntroductionLog-linear models have been applied to a number of problems in NLP, for example,POS tagging (Ratnaparkhi 1996; Lafferty, McCallum, and Pereira 2001), named entityrecognition (Borthwick 1999), chunking (Koeling 2000), and parsing (Johnson et al1999).
Log-linear models are also referred to as maximum entropy models and randomfields in the NLP literature.
They are popular because of the ease with which complexdiscriminating features can be included in the model, and have been shown to givegood performance across a range of NLP tasks.Log-linear models have previously been applied to statistical parsing (Johnsonet al 1999; Toutanova et al 2002; Riezler et al 2002; Malouf and van Noord 2004),but typically under the assumption that all possible parses for a sentence can beenumerated.
For manually constructed grammars, this assumption is usually sufficientfor efficient estimation and decoding.
However, for wide-coverage grammars extractedfrom a treebank, enumerating all parses is infeasible.
In this article we apply the dy-namic programming method of Miyao and Tsujii (2002) to a packed chart; however,because the grammar is automatically extracted, the packed charts require a consid-erable amount of memory: up to 25 GB.
We solve this massive estimation problem bydeveloping a parallelized version of the estimation algorithm which runs on a Beowulfcluster.The lexicalized grammar formalism we use is Combinatory Categorial Grammar(CCG; Steedman 2000).
A number of statistical parsing models have recently been devel-oped for CCG and used in parsers applied to newspaper text (Clark, Hockenmaier, andSteedman 2002; Hockenmaier and Steedman 2002b; Hockenmaier 2003b).
In this articlewe extend existing parsing techniques by developing log-linear models for CCG, as wellas a new model and efficient parsing algorithm which exploits all CCG?s derivations,including the nonstandard ones.Estimating a log-linear model involves computing expectations of feature values.For the conditional log-linear models used in this article, computing expectations re-quires a sum over all derivations for each sentence in the training data.
Because therecan be a massive number of derivations for some sentences, enumerating all derivationsis infeasible.
To solve this problem, we have adapted the dynamic programming methodof Miyao and Tsujii (2002) to packed CCG charts.
A packed chart efficiently represents allderivations for a sentence.
The dynamic programming method uses inside and outsidescores to calculate expectations, similar to the inside?outside algorithm for estimatingthe parameters of a PCFG from unlabeled data (Lari and Young 1990).Generalized Iterative Scaling (Darroch and Ratcliff 1972) is a common choice inthe NLP literature for estimating a log-linear model (e.g., Ratnaparkhi 1998; Curranand Clark 2003).
Initially we used generalized iterative scaling (GIS) for the parsingmodels described here, but found that convergence was extremely slow; Sha and Pereira(2003) present a similar finding for globally optimized log-linear models for sequences.As an alternative to GIS, we use the limited-memory BFGS algorithm (Nocedal andWright 1999).
As Malouf (2002) demonstrates, general purpose numerical optimizationalgorithms such as BFGS can converge much faster than iterative scaling algorithms(including Improved Iterative Scaling; Della Pietra, Della Pietra, and Lafferty 1997).Despite the use of a packed representation, the complete set of derivations for thesentences in the training data requires up to 25 GB of RAM for some of the models in thisarticle.
There are a number of ways to solve this problem.
Possibilities include using asubset of the training data; repeatedly parsing the training data for each iteration ofthe estimation algorithm; or reading the packed charts from disk for each iteration.494Clark and Curran Wide-Coverage Efficient Statistical ParsingThese methods are either too slow or sacrifice parsing performance, and so we usea parallelized version of BFGS running on an 18-node Beowulf cluster to perform theestimation.
Even given the large number of derivations and the large feature sets in ourmodels, the estimation time for the best-performing model is less than three hours.
Thisgives us a practical framework for developing a statistical parser.A corollary of CCG?s base-generative treatment of long-range dependencies in rel-ative clauses and coordinate constructions is that the standard predicate?argument re-lations can be derived via nonstandard surface derivations.
The addition of ?spurious?derivations in CCG complicates the modeling and parsing problems.
In this article weconsider two solutions.
The first, following Hockenmaier (2003a), is to define a model interms of normal-form derivations (Eisner 1996).
In this approach we recover only onederivation leading to a given set of predicate?argument dependencies and ignore therest.The second approach is to define a model over the predicate?argument dependen-cies themselves, by summing the probabilities of all derivations leading to a given setof dependencies.
We also define a new efficient parsing algorithm for such a model,based on Goodman (1996), which maximizes the expected recall of dependencies.
Thedevelopment of this model allows us to test, for the purpose of selecting the correctpredicate?argument dependencies, whether there is useful information in the additionalderivations.
We also compare the performance of our best log-linear model againstexisting CCG parsers, obtaining the highest results to date for the recovery of predicate?argument dependencies from CCGbank.A key component of the parsing system is a Maximum Entropy CCG supertagger(Ratnaparkhi 1996; Curran and Clark 2003) which assigns lexical categories to wordsin a sentence.
The role of the supertagger is twofold.
First, it makes discriminativeestimation feasible by limiting the number of incorrect derivations for each trainingsentence; the supertagger can be thought of as supplying a number of incorrect butplausible lexical categories for each word in the sentence.
Second, it greatly increases theefficiency of the parser, which was the original motivation for supertagging (Bangaloreand Joshi 1999).
One possible criticism of CCG has been that highly efficient parsing isnot possible because of the additional ?spurious?
derivations.
In fact, we show that anovel method which tightly integrates the supertagger and parser leads to parse timessignificantly faster than those reported for comparable parsers in the literature.The parser is evaluated on CCGbank (available through the Linguistic Data Con-sortium).
In order to facilitate comparisons with parsers using different formalisms, wealso evaluate on the publicly available DepBank (King et al 2003), using the Briscoeand Carroll annotation consistent with the RASP parser (Briscoe, Carroll, and Watson2006).
The dependency annotation is designed to be as theory-neutral as possible toallow easy comparison.
However, there are still considerable difficulties associated witha cross-formalism comparison, which we describe.
Even though the CCG dependenciesare being mapped into another representation, the accuracy of the CCG parser is over81% F-score on labeled dependencies, against an upper bound of 84.8%.
The CCG parseralso outperforms RASP overall and on the majority of dependency types.The contributions of this article are as follows.
First, we explain how to estimate afull log-linear parsing model for an automatically extracted grammar, on a scale as largeas that reported anywhere in the NLP literature.
Second, the article provides a compre-hensive blueprint for building a wide-coverage CCG parser, including theoretical andpractical aspects of the grammar, the estimation process, and decoding.
Third, we inves-tigate the difficulties associated with cross-formalism parser comparison, evaluating theparser on DepBank.
And finally, we develop new models and decoding algorithms for495Computational Linguistics Volume 33, Number 4CCG, and give a convincing demonstration that, through use of a supertagger, highlyefficient parsing is possible with CCG.2.
Related WorkThe first application of log-linear models to parsing is the work of Ratnaparkhi andcolleagues (Ratnaparkhi, Roukos, and Ward 1994; Ratnaparkhi 1996, 1999).
Similar toDella Pietra, Della Pietra, and Lafferty (1997), Ratnaparkhi motivates log-linear modelsfrom the perspective of maximizing entropy, subject to certain constraints.
Ratnaparkhimodels the various decisions made by a shift-reduce parser, using log-linear distri-butions defined over features of the local context in which a decision is made.
Theprobabilities of each decision are multiplied together to give a score for the completesequence of decisions, and beam search is used to find the most probable sequence,which corresponds to the most probable derivation.A different approach is proposed by Abney (1997), who develops log-linear modelsfor attribute-value grammars, such as Head-driven Phrase Structure Grammar (HPSG).Rather than define a model in terms of parser moves, Abney defines a model directlyover the syntactic structures licensed by the grammar.
Another difference is that Abneyuses a global model, in which a single log-linear model is defined over the completespace of attribute?value structures.
Abney?s motivation for using log-linear models isto overcome various problems in applying models based on PCFGs directly to attribute-value grammars.
A further motivation for using global models is that these do not sufferfrom the label bias problem (Lafferty, McCallum, and Pereira 2001), which is a potentialproblem for Ratnaparkhi?s approach.Abney defines the following model for a syntactic analysis ?:P(?)
=?i ?fi(?
)iZ (1)where fi(?)
is a feature, or feature function, and ?i is its corresponding weight; Z is anormalizing constant, also known as the partition function.
In much work using log-linear models in NLP, including Ratnaparkhi?s, the features of a model are indicatorfunctions which take the value 0 or 1.
However, in Abney?s models, and in the modelsused in this article, the feature functions are integer valued and count the number oftimes some feature appears in a syntactic analysis.1 Abney calls the feature functionsfrequency functions and, like Abney, we will not always distinguish between a featureand its corresponding frequency function.There are practical difficulties with Abney?s proposal, in that finding the maximum-likelihood solution during estimation involves calculating expectations of feature val-ues, which are sums over the complete space of possible analyses.
Abney suggests aMetropolis-Hastings sampling procedure for calculating the expectations, but does notexperiment with an implementation.Johnson et al (1999) propose an alternative solution, which is to maximize theconditional likelihood function.
In this case the likelihood function is the product ofthe conditional probabilities of the syntactic analyses in the data, each probability condi-tioned on the respective sentence.
The advantage of this method is that calculating theconditional feature expectations only requires a sum over the syntactic analyses for the1 In principle the features could be real-valued, but we only use integer-valued features in this article.496Clark and Curran Wide-Coverage Efficient Statistical Parsingsentences in the training data.
The conditional-likelihood estimator is also consistentfor the conditional distributions (Johnson et al 1999).
The same solution is arrived atby Della Pietra, Della Pietra, and Lafferty (1997) via a maximum entropy argument.Another feature of Johnson et al?s approach is the use of a Gaussian prior term to avoidoverfitting, which involves adding a regularization term to the likelihood function; theregularization term penalizes models whose weights get too large in absolute value.This smoothing method for log-linear models is also proposed by Chen and Rosenfeld(1999).Calculating the conditional feature expectations can still be problematic if the gram-mar licenses a large number of analyses for some sentences.
This is not a problem forJohnson et al (1999) because their grammars are hand-written and constraining enoughto allow the analyses for each sentence to be enumerated.
However, for grammars withwider coverage it is often not possible to enumerate the analyses for each sentence inthe training data.
Osborne (2000) investigates training on a sample of the analyses foreach sentence, for example the top-n most probable according to some other probabilitymodel, or simply a random sample.The CCG grammar used in this article is automatically extracted, has wide cover-age, and can produce an extremely large number of derivations for some sentences,far too many to enumerate.
We adapt the feature-forest method of Miyao and Tsujii(2002), which involves using dynamic programming to efficiently calculate the featureexpectations.
Geman and Johnson (2002) propose a similar method in the context of LFGparsing; an implementation is described in Kaplan et al (2004).Miyao and Tsujii have carried out a number of investigations similar to the workin this article.
In Miyao and Tsujii (2003b, 2003a) log-linear models are developed forautomatically extracted grammars for Lexicalized Tree Adjoining Grammar (LTAG) andHead Driven Phrase Structure Grammar (HPSG).
One of Miyao and Tsujii?s motivationsis to model predicate?argument dependencies, including long-range dependencies,which was one of the original motivations of the wide-coverage CCG parsing project.Miyao and Tsujii (2003a) present another log-linear model for an automatically extractedLTAG which uses a simple unigram model of the elementary trees together with a log-linear model of the attachments.
Miyao and Tsujii (2005) address the issue of practicalestimation using an automatically extracted HPSG grammar.
A simple unigram modelof lexical categories is used to limit the size of the charts for training, in a similar way tohow we use a CCG supertagger to restrict the size of the charts.The main differences between Miyao and Tsujii?s work and ours, aside from thedifferent grammar formalisms, are as follows.
The CCG supertagger is a key componentof our parsing system.
It allows practical estimation of the log-linear models as wellas highly efficient parsing.
The Maximum Entropy supertagger we use could also beapplied to Miyao and Tsujii?s grammars, although whether similar performance wouldbe obtained depends on the characteristics of the grammar; see subsequent sections formore discussion of this issue in relation to LTAG.
The second major difference is in ouruse of a cluster and parallelized estimation algorithm.
We have found that significantlyincreasing the size of the parse space available for discriminative estimation, which ispossible on the cluster, improves the accuracy of the resulting parser.
Another advan-tage of parallelization, as discussed in Section 5.5, is the reduction in estimation time.Again, our parallelization techniques could be applied to Miyao and Tsujii?s framework.Malouf and van Noord (2004) present similar work to ours, in the context of anHPSG grammar for Dutch.
One similarity is that their parsing system uses an HMMtagger before parsing, similar to our supertagger.
One difference is that we use aMaximum Entropy tagger which allows more flexibility in terms of the features that can497Computational Linguistics Volume 33, Number 4be encoded; for example, we have found that using Penn Treebank POS tags as featuressignificantly improves supertagging accuracy.
Another difference is that Malouf andvan Noord use the random sampling method of Osborne (2000) to allow practicalestimation, whereas we construct the complete parse forest but use the supertagger tolimit the size of the charts.
Their work is also on a somewhat smaller scale, with theDutch Alpino treebank containing 7,100 sentences, compared with the 36,000 sentenceswe use for training.Kaplan et al (2004) present similar work to ours in the context of an LFG grammarfor English.
The main difference is that the LFG grammar is hand-built, resulting in lessambiguity than an automatically extracted grammar and thus requiring fewer resourcesfor model estimation.
One downside of hand-built grammars is that they are typicallyless robust, which Kaplan et al address by developing a ?fragment?
grammar, togetherwith a ?skimming mode,?
which increases coverage on Section 23 of the Penn Treebankfrom 80% to 100%.
Kaplan et al also present speed figures for their parser, comparingwith the Collins parser.
Comparing parser speeds is difficult because of implementationand accuracy differences, but their highest reported speed is around 2 sentences persecond on sentences from Section 23.
The parse speeds that we present in Section 10.3are an order of magnitude higher.More generally, the literature on statistical parsing using linguistically motivatedgrammar formalisms is large and growing.
Statistical parsers have been developedfor TAG (Chiang 2000; Sarkar and Joshi 2003), LFG (Riezler et al 2002; Kaplan et al2004; Cahill et al 2004), and HPSG (Toutanova et al 2002; Toutanova, Markova, andManning 2004; Miyao and Tsujii 2004; Malouf and van Noord 2004), among others.The motivation for using these formalisms is that many NLP tasks, such as MachineTranslation, Information Extraction, and Question Answering, could benefit from themore sophisticated linguistic analyses they provide.The formalism most closely related to CCG from this list is TAG.
TAG grammars havebeen automatically extracted from the Penn Treebank, using techniques similar to thoseused by Hockenmaier (Chen and Vijay-Shanker 2000; Xia, Palmer, and Joshi 2000).
Also,the supertagging idea which is central to the efficiency of the CCG parser originated withTAG (Bangalore and Joshi 1999).
Chen et al (2002) describe the results of reranking theoutput of an HMM supertagger using an automatically extracted LTAG.
The accuracyfor a single supertag per word is slightly over 80%.
This figure is increased to over91% when the tagger is run in n-best mode, but at a considerable cost in ambiguity,with 8 supertags per word.
Nasr and Rambow (2004) investigate the potential impactof LTAG supertagging on parsing speed and accuracy by performing a number oforacle experiments.
They find that, with the perfect supertagger, extremely high parsingaccuracies and speeds can be obtained.
Interestingly, the accuracy of LTAG supertaggersusing automatically extracted grammars is significantly below the accuracy of the CCGsupertagger.
One possible way to increase the accuracy of LTAG supertagging is to use aMaximum Entropy, rather than HMM, tagger (as discussed previously), but this is likelyto result in an improvement of only a few percentage points.
Thus whether the differ-ence in supertagging accuracy is due to the nature of the formalisms, the supertaggingmethods used, or properties of the extracted grammars, is an open question.Related work on statistical parsing with CCG will be described in Section 3.3.
Combinatory Categorial GrammarCombinatory Categorial Grammar (CCG) (Steedman 1996, 2000) is a type-driven lex-icalized theory of grammar based on Categorial Grammar (Wood 1993).
CCG lexical498Clark and Curran Wide-Coverage Efficient Statistical Parsingentries consist of a syntactic category, which defines valency and directionality, anda semantic interpretation.
In this article we are concerned with the syntactic compo-nent; see Steedman (2000) for how a semantic interpretation can be composed dur-ing a syntactic derivation, and also Bos et al (2004) for how semantic interpretationscan be built for newspaper text using the wide-coverage parser described in thisarticle.Categories can be either basic or complex.
Examples of basic categories are S (sen-tence), N (noun), NP (noun phrase), and PP (prepositional phrase).
Complex categoriesare built recursively from basic categories, and indicate the type and directionalityof arguments (using slashes), and the type of the result.
For example, the followingcategory for the transitive verb bought specifies its first argument as a noun phrase to itsright, its second argument as a noun phrase to its left, and its result as a sentence:bought := (S\NP)/NP (2)In the theory of CCG, basic categories are regarded as complex objects that includesyntactic features such as number, gender, and case.
For the grammars in this article,categories are augmented with some additional information, such as head information,and also features on S categories which distinguish different types of sentence, such asdeclarative, infinitival, and wh-question.
This additional information will be describedin later sections.Categories are combined in a derivation using combinatory rules.
In the originalCategorial Grammar (Bar-Hillel 1953), which is context-free, there are two rules offunctional application:X/Y Y ?
X (>) (3)Y X\Y ?
X (<) (4)where X and Y denote categories (either basic or complex).
The first rule is forwardapplication (>) and the second rule is backward application (<).
Figure 1 gives anexample derivation using these rules.CCG extends the original Categorial Grammar by introducing a number of addi-tional combinatory rules.
The first is forward composition, which Steedman denotesFigure 1Example derivation using forward and backward application.499Computational Linguistics Volume 33, Number 4Figure 2Example derivation using type-raising and forward composition.by > B (because B is the symbol used by Curry to denote function composition incombinatory logic; Curry and Feys 1958):X/Y Y/Z ?B X/Z (> B) (5)Forward composition is often used in conjunction with type-raising (T), as in Figure 2.In this case type-raising takes a subject noun phrase and turns it into a functor lookingto the right for a verb phrase; the fund is then able to combine with reached using forwardcomposition, giving the fund reached the category S[dcl]/NP (a declarative sentencemissing an object).
It is exactly this type of constituent which the object relative pronouncategory is looking for to its right: (NP\NP)/(S[dcl]/NP).Note that the fund reached is a perfectly reasonable constituent in CCG, having thetype S[dcl]/NP.
This allows analyses for sentences such as the fund reached but investorsdisagreed with the agreement, even though this construction is often described as ?non-constituent coordination.?
In this example, the fund reached and investors disagreed withhave the same type, allowing them to be coordinated, resulting in the fund reached butinvestors disagreed with having the type S[dcl]/NP.
Note also that it is this flexible notionof constituency which leads to so-called spurious ambiguity, because even the simplesentence the fund reached an agreement will have more than one derivation, with eachderivation leading to the same set of predicate?argument dependencies.Forward composition is generalized to allow additional arguments to the rightof the Z category in (5).
For example, the following combination allows analysis ofsentences such as I offered, and may give, a flower to a policeman (Steedman 2000):may give(S\NP)/(S\NP) ((S\NP)/PP)/NP>B((S\NP)/PP)/NPThis example shows how the categories for may and give combine, resulting in a cate-gory of the same type as offered, which can then be coordinated.
Steedman (2000) givesa more precise definition of generalized forward composition.Further combinatory rules in the theory of CCG include backward composition(< B) and backward crossed composition (< BX):Y\Z X\Y ?B X\Z (< B) (6)Y/Z X\Y ?B X/Z (< BX) (7)500Clark and Curran Wide-Coverage Efficient Statistical ParsingBackward composition provides an analysis for sentences involving ?argument clustercoordination,?
such as I gave a teacher an apple and a policeman a flower (Steedman 2000).Backward crossed composition is required for heavy NP shift and coordinations suchas I shall buy today and cook tomorrow the mushrooms.
In this coordination example fromSteedman (2000), backward crossed composition is used to combine the categoriesfor buy, (S\NP)/NP, and today, (S\NP)\(S\NP), and similarly for cook and tomorrow,producing categories of the same type which can be coordinated.
This rule is alsogeneralized in an analogous way to forward composition.Finally, there is a coordination rule which conjoins categories of the same type,producing a further category of that type.
This rule can be implemented by assumingthe following category schema for a coordination term: (X\X)/X, where X can be anycategory.All of the combinatory rules described above are implemented in our parser.
Othercombinatory rules, such as substitution, have been suggested in the literature to dealwith certain linguistic phenomena, but we chose not to implement them.
The reason isthat adding new combinatory rules reduces the efficiency of the parser, and we felt that,in the case of substitution, for example, the small gain in grammatical coverage was notworth the reduction in speed.
Section 9.3 discusses some of the choices we made whenimplementing the grammar.One way of dealing with the additional ambiguity in CCG is to only considernormal-form derivations.
Informally, a normal-form derivation is one which uses type-raising and composition only when necessary.
Eisner (1996) describes a technique foreliminating spurious ambiguity entirely, by defining exactly one normal-form deriva-tion for each semantic equivalence class of derivations.
The idea is to restrict thecombination of categories produced by composition; more specifically, any constituentwhich is the result of a forward composition cannot serve as the primary (left) functor inanother forward composition or forward application.
Similarly, any constituent whichis the result of a backward composition cannot serve as the primary (right) functorin another backward composition or backward application.
Eisner only deals with agrammar without type-raising, and so the constraints cannot guarantee a normal-formderivation when applied to the grammars used in this article.
However, the constraintscan still be used to significantly reduce the parsing space.
Section 9.3 describes thevarious normal-form constraints used in our experiments.A recent development in the theory of CCG is the multi-modal treatment given byBaldridge (2002) and Baldridge and Kruijff (2003), following the type-logical approachesto Categorial Grammar (Moortgat 1997).
One possible extension to the parser andgrammar described in this article is to incorporate the multi-modal approach; Baldridgesuggests that, as well as having theoretical motivation, a multi-modal approach canimprove the efficiency of CCG parsing.3.1 Why Use CCG for Statistical Parsing?CCG was designed to deal with the long-range dependencies inherent in certainconstructions, such as coordination and extraction, and arguably provides the mostlinguistically satisfactory account of these phenomena.
Long-range dependencies arerelatively common in text such as newspaper text, but are typically not recovered bytreebank parsers such as Collins (2003) and Charniak (2000).
This has led to a numberof proposals for post-processing the output of the Collins and Charniak parsers, inwhich trace sites are located and the antecedent of the trace determined (Johnson 2002;Dienes and Dubey 2003; Levy and Manning 2004).
An advantage of using CCG is that501Computational Linguistics Volume 33, Number 4the recovery of long-range dependencies can be integrated into the parsing process ina straightforward manner, rather than be relegated to such a post-processing phase(Clark, Hockenmaier, and Steedman 2002; Hockenmaier 2003a; Clark, Steedman, andCurran 2004).Another advantage of CCG is that providing a compositional semantics for thegrammar is relatively straightforward.
It has a completely transparent interface betweensyntax and semantics and, because CCG is a lexicalized grammar formalism, providing acompositional semantics simply involves adding semantic representations to the lexicalentries and interpreting the small number of combinatory rules.
Bos et al (2004) showhow this can be done for the grammar and parser described in this article.Of course some of these advantages could be obtained with other grammar for-malisms, such as TAG, LFG, and HPSG, although CCG is especially well-suited toanalysing coordination and long-range dependencies.
For example, the analysis of?non-constituent coordination?
described in the previous section is, as far as we know,unique to CCG.Finally, the lexicalized nature of CCG has implications for the engineering of a wide-coverage parser.
Later we show that use of a supertagger (Bangalore and Joshi 1999) priorto parsing can produce an extremely efficient parser.
The supertagger uses statisticalsequence tagging techniques to assign a small number of lexical categories to each wordin the sentence.
Because there is so much syntactic information in lexical categories,the parser is required to do less work once the lexical categories have been assigned;hence Srinivas and Joshi, in the context of TAG, refer to supertagging as almost parsing.The parser is able to parse 20 Wall Street Journal (WSJ) sentences per second on standardhardware, using our best-performing model, which compares very favorably with otherparsers using linguistically motivated grammars.A further advantage of the supertagger is that it can be used to reduce the parsespace for estimation of the log-linear parsing models.
By focusing on those parseswhich result from the most probable lexical category sequences, we are able to performeffective discriminative training without considering the complete parse space, whichfor most sentences is prohibitively large.The idea of supertagging originated with LTAG; however, in contrast to the CCGgrammars used in this article, the automatically extracted LTAG grammars have, as yet,been too large to enable effective supertagging (as discussed in the previous section).
Weare not aware of any other work which has demonstrated the parsing efficiency benefitsof supertagging using an automatically extracted grammar.3.2 Previous Work on CCG Statistical ParsingThe work in this article began as part of the Edinburgh wide-coverage CCG parsingproject (2000?2004).
There has been some other work on defining stochastic categorialgrammars, but mainly in the context of grammar learning (Osborne and Briscoe 1997;Watkinson and Manandhar 2001; Zettlemoyer and Collins 2005).An early attempt from the Edinburgh project at wide-coverage CCG parsing is pre-sented in Clark, Hockenmaier, and Steedman (2002).
In order to deal with the problemof the additional, nonstandard CCG derivations, a conditional model of dependencystructures is presented, based on Collins (1996), in which the dependencies are modeleddirectly and derivations are not modeled at all.
The conditional probability of a depen-dency structure ?, given a sentence S, is factored into two parts.
The first part is theprobability of the lexical category sequence, C, and the second part is the dependencystructure, D, giving P(?|S) = P(C|S)P(D|C, S).
Intuitively, the category sequence is gen-502Clark and Curran Wide-Coverage Efficient Statistical Parsingerated first, conditioned on the sentence, and then attachment decisions are made toform the dependency links.
The probability of the category sequence is estimated usinga maximum entropy model, following the supertagger described in Clark (2002).
Theprobabilities of the dependencies are estimated using relative frequencies, followingCollins (1996).The model was designed to include some long-range predicate?argument depen-dencies, as well as local dependencies.
However, there are a number of problems withthe model, as the authors acknowledge.
First, the model is deficient, losing probabilitymass to dependency structures not generated by the grammar.
Second, the relativefrequency estimation of the dependency probabilities is ad hoc, and cannot be seenas maximum likelihood estimation, or some other principled method.
Despite theseflaws, the parser based on this model was able to recover CCG predicate?argumentdependencies at around 82% overall F-score on unseen WSJ text.Hockenmaier (2003a) and Hockenmaier and Steedman (2002b) present a generativemodel of normal-form derivations, based on various techniques from the statisticalparsing literature (Charniak 1997; Goodman 1997; Collins 2003).
A CCG binary deriva-tion tree is generated top-down, with the probability of generating particular childnodes being conditioned on some limited context from the previously generated struc-ture.
Hockenmaier?s parser uses rule instantiations read off CCGbank (see Section 3.3)and some of these will be instances of type-raising and composition; hence the parsercan produce non-normal-form derivations.
However, because the parsing model isestimated over normal-form derivations, any non-normal-form derivations will receivelow probabilities and are unlikely to be returned as the most probable parse.Hockenmaier (2003a) compares a number of generative models, starting with abaseline model based on a PCFG.
Various extensions to the baseline are considered:increasing the amount of lexicalization; generating a lexical category at its maximalprojection; conditioning the probability of a rule instantiation on the grandparent node(Johnson 1998); adding features designed to deal with coordination; and adding dis-tance to the dependency features.
Some of these extensions, such as increased lexicaliza-tion and generating a lexical category at its maximal projection, improved performance,whereas others, such as the coordination and distance features, reduced performance.Hockenmaier (2003a) conjectures that the reduced performance is due to the problem ofdata sparseness, which becomes particularly severe for the generative model when thenumber of features is increased.
The best performing model outperforms that of Clark,Hockenmaier, and Steedman (2002), recovering CCG predicate?argument dependencieswith an overall F-score of around 84% using a similar evaluation.Hockenmaier (2003b) presents another generative model of normal-form deriva-tions, which is based on the dependencies in the predicate?argument structure, in-cluding long-range dependencies, rather than the dependencies defined by the localtrees in the derivation.
Hockenmaier also argues that, compared to Hockenmaier andSteedman (2002b), the predicate?argument model is better suited to languages withfreer word order than English.
The model was also designed to test whether the inclu-sion of predicate?argument dependencies improves parsing accuracy.
In fact, the resultsgiven in Hockenmaier (2003b) are lower than previous results.
However, Hockenmaier(2003b) reports that the increased complexity of the model reduces the effectiveness ofthe dynamic programming used in the parser, and hence a more aggressive beam searchis required to produce reasonable parse times.
Thus the reduced accuracy could be dueto implementation difficulties rather than the model itself.The use of conditional log-linear models in this article is designed to overcome someof the weaknesses identified in the approach of Clark, Hockenmaier, and Steedman503Computational Linguistics Volume 33, Number 4(2002), and to offer a more flexible framework for including features than the generativemodels of Hockenmaier (2003a).
For example, adding long-range dependency featuresto the log-linear model is straightforward.
We also showed in Clark and Curran (2004b)that, in contrast with Hockenmaier (2003a), adding distance to the dependency featuresin the log-linear model does improve parsing accuracy.
Another feature of conditionallog-linear models is that they are trained discriminatively, by maximizing the condi-tional probability of each gold-standard parse relative to the incorrect parses for thesentence.
Generative models, in contrast, are typically trained by maximizing the jointprobability of the ?training sentence, parse?
pairs, even though the sentence does notneed to be inferred.3.3 CCGbankThe treebank used in this article performs two roles: It provides the lexical category setused by the supertagger, plus some unary type-changing rules and punctuation rulesused by the parser, and it is used as training data for the statistical models.
The treebankis CCGbank (Hockenmaier and Steedman 2002a; Hockenmaier 2003a), a CCG versionof the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993).
Penn Treebankconversions have also been carried out for other linguistic formalisms, including TAG(Chen and Vijay-Shanker 2000; Xia, Palmer, and Joshi 2000), LFG (Burke et al 2004), andHPSG (Miyao, Ninomiya, and Tsujii 2004).CCGbank was created by converting the phrase-structure trees in the Penn Tree-bank into CCG normal-form derivations.
Some preprocessing of the phrase-structuretrees was required, in order to allow the correct CCG analyses for some constructions,such as coordination.
Hockenmaier (2003a) gives a detailed description of the procedureused to create CCGbank.
Figure 3 shows an example normal-form derivation for an (ab-breviated) CCGbank sentence.
The derivation has been inverted, so that it is representedas a binary tree.Sentence categories (S) in CCGbank carry features, such as [dcl] for declarative, [wq]for wh-questions, and [for] for small clauses headed by for; see Hockenmaier (2003a) forthe complete list.
S categories also carry features in verb phrases; for example, S[b]\NPFigure 3Example CCG derivation as a binary tree for the sentence Under new features, participants cantransfer money from the new funds.504Clark and Curran Wide-Coverage Efficient Statistical Parsingis a bare-infinitive; S[to]\NP is a to-infinitive; S[pss]\NP is a past participle in passivemode.
Note that, whenever an S or S\NP category is modified, any feature on the S iscarried through to the result category; this is true in our parser also.
Finally, determinersspecify that the resulting noun phrase is non-bare: NP[nb]/N, although this feature islargely ignored by the parser described in this article.As well as instances of the standard CCG combinatory rules?forward and back-ward application, forward and backward composition, backward-crossed composi-tion, type-raising, coordination of like types?CCGbank contains a number of unarytype-changing rules and rules for dealing with punctuation.
The type-changing rulestypically change a verb phrase into a modifier.
The following examples, taken fromHockenmaier (2003a), demonstrate the most common rules.
The bracketed expressionhas the type-changing rule applied to it: S[pss]\NP ?
NP\NPworkers [exposed to it] S[adj]\NP ?
NP\NPa forum [likely to bring attention to the problem] S[ng]\NP ?
NP\NPsignboards [advertising imported cigarettes] S[ng]\NP ?
(S\NP)\(S\NP)became chairman [succeeding Ian Butler] S[dcl]/NP ?
NP\NPthe millions of dollars [it generates]Another common type-changing rule in CCGbank, which appears in Figure 3, changesa noun category N into a noun phrase NP.
Appendix A lists the unary type-changingrules used by our parser.There are also a number of rules in CCGbank for absorbing punctuation.
For exam-ple, Figure 3 contains a rule which takes a comma followed by a declarative sentenceand returns a declarative sentence:, S[dcl] ?
S[dcl]There are a number of similar comma rules for other categories.
There are also similarpunctuation rules for semicolons, colons, and brackets.
There is also a rule schemawhich treats a comma as a coordination:, X ?
X\XAppendix A contains the complete list of punctuation rules used in the parser.A small number of local trees in CCGbank?consisting of a parent and one or twochildren?do not correspond to any of the CCG combinatory rules, or the type-changingrules or punctuation rules.
This is because some of the phrase structure subtrees in the505Computational Linguistics Volume 33, Number 4Penn Treebank are difficult to convert to CCG combinatory rules, and because of noiseintroduced by the Treebank conversion process.3.4 CCG Dependency StructuresDependency structures perform two roles in this article.
First, they are used for parserevaluation: The accuracy of a parsing model is measured using precision and recallover CCG predicate?argument dependencies.
Second, dependency structures form thecore of the dependency model: Probabilities are defined over dependency structures,and the parsing algorithm for this model returns the highest scoring dependencystructure.We define a CCG dependency structure as a set of CCG predicate?argument depen-dencies.
They are defined as sets, rather than multisets, because the lexical items in adependency are considered to be indexed by sentence position; this is important forevaluation purposes and, for the dependency model, determining which derivationslead to a given set of dependencies.
However, there are situations where the lexicalitems need to be considered independently of sentence position, for example whendefining feature functions in terms of dependencies.
Such cases should be clear from thecontext.We define CCG predicate?argument relations in terms of the argument slots in CCGlexical categories.
Thus the transitive verb category, (S\NP)/NP, has two predicate?argument relations associated with it, one corresponding to the object NP argumentand one corresponding to the subject NP argument.
In order to distinguish differentargument slots, the arguments are numbered from left to right.
Thus, the subject relationfor a transitive verb is represented as ?
(S\NP1)/NP2, 1?.The predicate?argument dependencies are represented as 5-tuples: ?hf , f, s, ha, l?,where hf is the lexical item of the lexical category expressing the dependency relation,f is the lexical category, s is the argument slot, ha is the head word of the argument, andl encodes whether the dependency is non-local.
For example, the dependency encodingcompany as the object of bought (as in IBM bought the company) is represented as follows:?bought2, (S\NP1)/NP2, 2, company4, ??
(8)The subscripts on the lexical items indicate sentence position, and the final field (?
)indicates that the dependency is a local dependency.Head and dependency information is represented on the lexical categories, anddependencies are created during a derivation as argument slots are filled.
Long-rangedependencies are created by passing head information from one category to anotherusing unification.
For example, the expanded category for the control verb persuade is:persuade := ((S[dcl]persuade\NP1)/(S[to]2\NPX))/NPX,3 (9)The head of the infinitival complement?s subject is identified with the head of theobject, using the variable X. Unification then passes the head of the object to the subjectof the infinitival, as in standard unification-based accounts of control.
In the currentimplementation, the head and dependency markup depends on the category only andnot the lexical item.
This gives semantically incorrect dependencies in some cases; for506Clark and Curran Wide-Coverage Efficient Statistical Parsingexample, the control verbs persuade and promise have the same lexical category, whichmeans that promise Brooks to go is assigned a structure meaning promise Brooks that Brookswill go.The kinds of lexical items that use the head passing mechanism are raising, aux-iliary and control verbs, modifiers, and relative pronouns.
Among the constructionsthat project unbounded dependencies are relativization and right node raising.
Thefollowing relative pronoun category (for words such as who, which, and that) shows howheads are co-indexed for object-extraction:who := (NPX\NPX,1)/(S[dcl]2/NPX) (10)In a sentence such as The company which IBM bought, the co-indexing will allow com-pany to be returned as the object of bought, which is represented using the followingdependency:?bought2, (S\NP1)/NP2, 2, company4, (NP\NP)/(S[dcl]/NP)?
(11)The final field indicates the category which mediated the long-range dependency, in thiscase the object relative pronoun category.The dependency annotation also permits complex categories as arguments.
Forexample, the marked up category for about (as in about 5,000 pounds) is:(NX/NX)Y/(N/N)Y,1 (12)If 5,000 has the category (NX/NX)5,000, the dependency relation marked on the (N/N)Y,1argument in (12) allows the dependency between about and 5,000 to be captured.In the current implementation every argument slot in a lexical category correspondsto a dependency relation.
This means, for example, that the parser produces subjects ofto-infinitival clauses and auxiliary verbs.
In the sentence IBM may like to buy Lotus, IBMwill be returned as the subject of may, like, to, and buy.
The only exception is duringevaluation, when some of these dependencies are ignored in order to be consistent withthe predicate?argument dependencies in CCGbank, and also DepBank.
In future workwe may investigate removing some of these dependencies from the parsing model andthe parser output.4.
Log-Linear Parsing Models for CCGThis section describes two parsing models for CCG.
The first defines the probabil-ity of a dependency structure, and the second?the normal-form model?defines theprobability of a single derivation.
In many respects, modeling single derivations issimpler than modeling dependency structures, as the rest of the article will demonstrate.However, there are a number of reasons for modeling dependency structures.
First,for many applications predicate?argument dependencies provide a more useful outputthan derivations, and the parser evaluation is over dependencies; hence it would seemreasonable to optimize over the dependencies rather than the derivation.
Second, wewant to investigate, for the purposes of parse selection, whether there is useful infor-mation in the nonstandard derivations.
We can test this by defining the probability ofa dependency structure in terms of all the derivations leading to that structure, rather507Computational Linguistics Volume 33, Number 4than emphasising a single derivation.
Thus, the probability of a dependency structure,?, given a sentence, S, is defined as follows:P(?|S) =?d??(?
)P(d,?|S) (13)where ?(?)
is the set of derivations which lead to ?.This approach is different from that of Clark, Hockenmaier, and Steedman (2002),who define the probability of a dependency structure simply in terms of the depen-dencies.
One reason for modeling derivations (either one distinguished derivation or aset of derivations), in addition to predicate?argument dependencies, is that derivationsmay contain useful information for inferring the correct dependency structure.For both the dependency model and the normal-form model, the probability of aparse is defined using a log-linear form.
However, the meaning of parse differs in thetwo cases.
For the dependency model, a parse is taken to be a ?d,??
pair, as in Equa-tion (13).
For the normal-form model, a parse is simply a (head-lexicalized) derivation.2We define a conditional log-linear model of a parse ?
?
?, given a sentence S, asfollows:P(?|S) = 1ZSe?
?
f (?)
(14)where ?
?
f (?)
=?i ?ifi(?).
The function fi is the integer-valued frequency function ofthe ith feature; ?i is the weight of the ith feature; and ZS is a normalizing constant whichensures that P(?|S) is a probability distribution:ZS =?????(S)e?
?
f (?? )
(15)where ?
(S) is the set of possible parses for S. For the normal-form model, featuresare defined over single derivations, including local word?word dependencies arisingfrom lexicalized rule instantiations.
The feature set is derived from the gold-standardnormal-form derivations in CCGbank.
For the dependency model, features are definedover dependency structures as well as derivations, and the feature set is derived fromall derivations leading to gold-standard dependency structures, including nonstandardderivations.
Section 7 describes the feature types in more detail.4.1 Estimating the Dependency ModelFor the dependency model, the training data consists of gold-standard dependencystructures, namely, sets of CCG predicate?argument dependencies, as described earlier.We follow Riezler et al (2002) in using a discriminative estimation method by maximiz-ing the conditional log-likelihood of the model given the data, minus a Gaussian prior2 We could model predicate?argument dependencies together with the derivation, but we wanted to usefeatures from the derivation only, following Hockenmaier and Steedman (2002b).508Clark and Curran Wide-Coverage Efficient Statistical Parsingterm to prevent overfitting (Chen and Rosenfeld 1999; Johnson et al 1999).
Thus, giventraining sentences S1, .
.
.
, Sm, gold-standard dependency structures, ?1, .
.
.
,?m, and thedefinition of the probability of a dependency structure from Equation (13), the objectivefunction is:L?(?)
= L(?)
?
G(?)
(16)= logm?j=1P?
(?j|Sj) ?n?i=1?2i2?2i=m?j=1log?d??
(?j ) e?
?
f (d,?j )????
(Sj ) e?
?
f (?)
?n?i=1?2i2?2i=m?j=1log?d??
(?j )e?
?
f (d,?j ) ?m?j=1log????
(Sj )e?
?
f (?)
?n?i=1?2i2?2iwhere L(?)
is the log-likelihood of model ?, G(?)
is the Gaussian prior term, and n isthe number of features.
We use a single smoothing parameter ?, so that ?i = ?
for alli; however, grouping the features into classes and using a different ?
for each class isworth investigating and may improve the results.Optimization of the objective function, whether using iterative scaling or moregeneral numerical optimization methods, requires calculation of the gradient of the ob-jective function at each iteration.
The components of the gradient vector are as follows:?L?(?)??i=m?j=1?d??
(?j )e?
?
f (d,?j )fi(d,?j)?d??
(?j ) e?
?
f (d,?j )(17)?m?j=1????
(Sj )e?
?
f (?)fi(?)????
(Sj ) e?
?
f (?)
?
?i?2iThe first two terms are expectations of feature fi: the second expectation is over allderivations for each sentence in the training data, and the first is over only the deriva-tions leading to the gold-standard dependency structure for each sentence.The estimation process attempts to make the expectations in Equation (17) equal(ignoring the Gaussian prior term).
Another way to think of the estimation process isthat it attempts to put as much mass as possible on the derivations leading to the gold-standard structures (Riezler et al 2002).
The Gaussian prior term prevents overfittingby penalizing any model whose weights get too large in absolute value.The estimation process can also be thought of in terms of the framework ofDella Pietra, Della Pietra, and Lafferty (1997), because setting the gradient in Equation(17) to zero yields the usual maximum entropy constraints, namely that the expectedvalue of each feature is equal to its empirical value (again ignoring the Gaussian priorterm).
However, in this case the empirical values are themselves expectations, over allderivations leading to each gold-standard dependency structure.509Computational Linguistics Volume 33, Number 44.2 Estimating the Normal-Form ModelFor the normal-form model, the training data consists of gold-standard normal-formderivations.
The objective function and gradient vector for the normal-form model are:L?(?)
= L(?)
?
G(?)
(18)= logm?j=1P?
(dj|Sj) ?n?i=1?2i2?2i?L?(?)?
?i=m?j=1fi(dj) (19)?m?j=1?d??
(Sj )e?
?
f (d)fi(d)?d??
(Sj ) e?
?
f (d) ?
?i?2iwhere dj is the the gold-standard normal-form derivation for sentence Sj and ?
(Sj) is theset of possible derivations for Sj.
Note that ?
(Sj) could contain some non-normal-formderivations; however, because any non-normal-form derivations will be consideredincorrect, the resulting model will typically assign low probabilities to non-normal-formderivations.The empirical value in Equation (19) is simply a count of the number of timesthe feature appears in the gold-standard normal-form derivations.
The second term inEquation (19) is an expectation over all derivations for each sentence.4.3 The Limited-Memory BFGS AlgorithmThe limited memory BFGS (L-BFGS) algorithm is a general purpose numerical optimiza-tion algorithm (Nocedal and Wright 1999).
In contrast to iterative scaling algorithmssuch as GIS, which update the parameters one at a time on each iteration, L-BFGSupdates the parameters all at once on each iteration.
It does this by considering thetopology of the feature space and moving in a direction which is guaranteed to increasethe value of the objective function.The simplest way in which to consider the shape of the feature space is to movein the direction in which the value of the objective function increases most rapidly;this leads to the method of steepest-ascent.
Hence steepest-ascent uses the first partialderivative (the gradient) of the objective function to determine parameter updates.L-BFGS improves on steepest-ascent by also considering the second partial derivative(the Hessian).
In fact, calculation of the Hessian can be prohibitively expensive, and soL-BFGS estimates this derivative by observing the change in a fixed number of previousgradients (hence the limited memory).Malouf (2002) gives a more thorough description of numerical optimization meth-ods applied to log-linear models.
He also presents a convincing demonstration that gen-eral purpose numerical optimization methods can greatly outperform iterative scalingmethods for many NLP tasks.3 Malouf uses standard numerical computation libraries3 One NLP task for which we have found GIS to be especially suitable is sequence tagging, and we still useGIS to estimate tagging models (Curran and Clark 2003).510Clark and Curran Wide-Coverage Efficient Statistical Parsingas the basis of his implementation.
One of our aims was to provide a self containedestimation code base, and so we implemented our own version of the L-BFGS algorithmas described in Nocedal and Wright (1999).5.
Efficient EstimationThe L-BFGS algorithm requires the following values at each iteration: the expectedvalue and the empirical expected value of each feature, for calculating the gradient;and the value of the likelihood function.
For the normal-form model, the empiricalexpected values and the likelihood can be easily obtained, because these only involvethe single gold-standard derivation for each sentence.
For the dependency model, thecomputations of the empirical expected values and the likelihood function are morecomplex, because these involve sums over just those derivations leading to the gold-standard dependency structures.
We explain how these derivations can be found inSection 5.4.
The next section explains how CCG charts can be represented in a way whichallows efficient estimation.5.1 Packed CCG Charts as Feature ForestsThe packed charts perform a number of roles.
First, they compactly represent every?derivation, dependency-structure?
pair, by grouping together equivalent chart entries.Entries are equivalent when they interact in the same manner with both the genera-tion of subsequent parse structure and the statistical parse selection.
In practice, thismeans that equivalent entries have the same span; form the same structures, that is, theremaining derivation plus dependencies, in any subsequent parsing; and generate thesame features in any subsequent parsing.
Back pointers to the daughters indicate howan individual entry was created, so that any derivation plus dependency structure canbe recovered from the chart.The second role of the packed charts is to allow recovery of the highest scoringderivation or dependency structure without enumerating all derivations.
And finally,packed charts are an instance of a feature forest, which Miyao and Tsujii (2002) showcan be used to efficiently estimate expected values of features, even though the expec-tation may involve a sum over an exponential number of trees in the forest.
One of thecontributions of this section is showing how Miyao and Tsujii?s feature forest approachcan be applied to a particular grammar formalism, namely CCG.
As Chiang (2003) pointsout, Miyao and Tsujii do not provide a way of constructing a feature forest given asentence, but provide the mathematical tools for estimation once the feature forest hasbeen constructed.In our packed charts, entries are equivalent when they have the same categorytype, identical head, and identical unfilled dependencies.
The equivalence test mustaccount for heads and unfilled dependencies because equivalent entries form the samedependencies in any subsequent parsing.
Individual entries in the chart are obtainedby combining canonical representatives of equivalence classes, using the rules of thegrammar.
Equivalence classes in the chart are sets of equivalent individual entries.A feature forest ?
is defined as a tuple ?C, D, R,?, ??
where: C is a set of conjunctive nodes; D is a set of disjunctive nodes;511Computational Linguistics Volume 33, Number 4 R ?
D is a set of root disjunctive nodes; ?
: D ?
2C is a conjunctive daughter function; ?
: C ?
2D is a disjunctive daughter function.The interpretation of a packed chart as a feature forest is straightforward.
First, onlyentries which are part of a derivation spanning the whole sentence are relevant.
Theseentries can be found by traversing the chart top-down, starting with the entries whichspan the sentence.
Individual entries in a cell are the conjunctive nodes, which are either?lexical category, word?
pairs at the leaves, or have been obtained by combining twoequivalence classes (or applying a unary rule to an equivalence class).
The equivalenceclasses of individual entries are the disjunctive nodes.
And finally, the equivalenceclasses at the roots of the CCG derivations are the root disjunctive nodes.For each feature function defined over parses (see Section 4) there is a correspondingfeature function defined over conjunctive nodes, that is, for each fi : ?
?
N there isa corresponding fi : C ?
N which counts the number of times feature fi appears on aparticular conjunctive node.
The value of fi for a parse is then the sum of the values of fifor each conjunctive node in the parse.The features used in the parsing model determine the definition of the equivalencerelation used for grouping individual entries.
In our models, features are defined interms of individual dependencies and local rule instantiations, where a rule instan-tiation is the local tree arising from the application of a rule in the grammar.
Notethat features can be defined in terms of long-range dependencies, even though suchdependencies may involve words which are a long way apart in the sentence.
Ourearlier definition of equivalence is consistent with these feature types.As an example, consider the following composition of will with buy using theforward composition rule:(S[dcl]will\NP)/NP(S[dcl]will\NP)/(S[b]\NP) (S[b]buy\NP)/NPThe equivalence class of the resulting individual entry is determined by the CCG cate-gory plus heads, in this case (S[dcl]will\NP)/NP, plus the dependencies yet to be filled.The dependencies are not shown, but there are two subject dependencies on the firstNP, one encoding the subject of will and one encoding the subject of buy, and there isan object dependency on the second NP encoding the object of buy.
Entries in the sameequivalence class are identical for the purposes of creating new dependencies for theremainder of the parsing.5.2 Feature LocalityIt is possible to extend the locality of the features beyond single rule instantiations andlocal dependencies.
For example, the definition of equivalence given earlier allows theincorporation of long-range dependencies as features.
The equivalence test considersunfilled dependencies which are both local and long-range; thus any individual entrieswhich have different long-range dependencies waiting to be filled will be in differentequivalence classes.
One of the advantages of log-linear models is that it is easy toinclude such features; Hockenmaier (2003b) describes the difficulties in including such512Clark and Curran Wide-Coverage Efficient Statistical Parsingfeatures in a generative model.
One of the early motivations of the Edinburgh CCGparsing project was to see if the long-range dependencies recovered by a CCG parsercould improve the accuracy of a parsing model.
In fact, we have found that addinglong-range dependencies to any of the models described in this article has no impact onaccuracy.
One possible explanation is that the long-range dependencies are so rare that amuch larger amount of training data would be required for these dependencies to havean impact.
Of course the fact that CCG enables recovery of long-range dependencies isstill a useful property, even if these dependencies are not currently useful as features,because it improves the utility of the parser output.There is considerable flexibility in defining the features for a parsing model in ourlog-linear framework, as the long-range dependency example demonstrates, but theneed for dynamic programming for both estimation and decoding reduces the range offeatures which can be used.
Any extension to the ?locality?
of the features would reducethe effectiveness of the chart packing and any dynamic programming performed overthe chart.
Two possible extensions, which we have not investigated, include defining de-pendency features which account for all three elements of the triple in a PP-attachment(Collins and Brooks 1995), and defining a rule feature which includes the grandparentnode (Johnson 1998).
Another alternative for future work is to compare the dynamicprogramming approach taken here with the beam-search approach of Collins and Roark(2004), which allows more ?global?
features.5.3 Calculating Feature ExpectationsFor estimating both the normal-form model and the dependency model, the followingexpectation of each feature fi, with respect to some model ?, is required:E?
fi =?S1ZS????(S)e?
?
f (?)fi(?)
(20)where ?
(S) is the set of all parses for sentence S, and ?
is the vector of weights for ?.This is essentially the same calculation for both models, even though for the de-pendency model, features can be defined in terms of dependencies as well as thederivations.
Dependencies can be stored as part of the individual entries (conjunctivenodes) at which they are created; hence all features can be defined in terms of theindividual entries which make up the derivations.Calculating E?
fi requires summing over all derivations ?
which include fi for eachsentence S in the training data.
The key to performing this sum efficiently is to writethe sum in terms of inside and outside scores for each conjunctive node.
The inside andoutside scores can be defined recursively.
If the inside score for a conjunctive node cis denoted ?c, and the outside score denoted ?c, then the expected value of fi can bewritten as follows:E?
fi =?S1ZS?c?CSfi(c)?c?c (21)where CS is the set of conjunctive nodes in the packed chart for sentence S.513Computational Linguistics Volume 33, Number 4Figure 4Example feature forest.Figure 4 gives an example feature forest, and shows the nodes used to calculatethe inside and outside scores for conjunctive node c5.
The inside score for a disjunctivenode, ?d, is the sum of the inside scores for its conjunctive node daughters:?d =?c??
(d)?c (22)The inside score for a conjunctive node, ?c, is defined in terms of the inside scoresof c?s disjunctive node daughters:?c =?d??
(c)?d e?
?
f (c) (23)where ?
?
f (c) =?i ?ifi(c).
If the conjunctive node is a leaf node, the inside score is justthe exponentiation of the sum of the feature weights on that node.The outside score for a conjunctive node, ?c, is the outside score for its disjunctivenode mother:?c = ?d where c ?
?
(d) (24)The calculation of the outside score for a disjunctive node, ?d, is a little moreinvolved; it is defined as a sum over the conjunctive mother nodes, of the productof the outside score of the mother, the inside score of the disjunctive node sister, andthe feature weights on the mother.
For example, the outside score of d4 in Figure 4 is thesum of two product terms.
The first term is the product of the outside score of c5, theinside score of d5, and the feature weights at c5; and the second term is the product ofthe outside score of c2, the inside score of d3, and the feature weights at c2.
The definitionis as follows; the outside score for a root disjunctive node is 1, otherwise:?d =?{c|d??(c)}???c?{d?|d???(c),d?
=d}?d?
e?
?
f (c)??
(25)514Clark and Curran Wide-Coverage Efficient Statistical ParsingThe normalization constant ZS is the sum of the inside scores for the root disjunctivenodes:ZS =?dr?R?dr (26)In order to calculate inside scores, the scores for daughter nodes need to be calcu-lated before the scores for mother nodes (and vice versa for the outside scores).
This caneasily be achieved by ordering the nodes in the bottom-up CKY parsing order.5.4 Estimation for the Dependency ModelFor the dependency model, the computations of the empirical expected values (17)and the log-likelihood function (16) require sums over just those derivations leadingto the gold-standard dependency structure.
We will refer to such derivations as correctderivations.
As far as we know, this problem of identifying derivations in a packed chartwhich lead to a particular dependency structure has not been addressed before in theNLP literature.Figure 5 gives an algorithm for finding nodes in a packed chart which appear incorrect derivations.
cdeps(c) returns the number of correct dependencies on conjunctivenode c, and returns the incorrect marker ?
if there are any incorrect dependencies onc; dmax(c) returns the maximum number of correct dependencies produced by anysub-derivation headed by c, and returns ?
if there are no sub-derivations producingFigure 5Algorithm for finding nodes in correct derivations.515Computational Linguistics Volume 33, Number 4only correct dependencies; dmax(d) returns the same value but for disjunctive node d.Recursive definitions of these functions are given in Figure 5; the base case occurs whenconjunctive nodes have no disjunctive daughters.The algorithm identifies all those root nodes heading derivations which producejust the correct dependencies, and traverses the chart top-down marking the nodesin those derivations.
The insight behind the algorithm is that, for two conjunctivenodes in the same equivalence class, if one node heads a sub-derivation producingmore correct dependencies than the other node (and each sub-derivation only producescorrect dependencies), then the node with less correct dependencies cannot be part of acorrect derivation.The conjunctive and disjunctive nodes appearing in correct derivations form anew feature forest, which we call a correct forest.
The correct forest forms a subsetof the complete forest (containing all derivations for the sentence).
The correct andcomplete forests can be used to estimate the required log-likelihood value and featureexpectations.
Let E??
fi be the expected value of fi over the forest ?
for model ?
; then thevalues in Equation (17) can be obtained by calculating E?j?
fi for the complete forest ?jfor each sentence Sj in the training data, and also E?j?
fi for each correct forest ?j:?L(?)??i=m?j=1(E?j?
fi ?
E?j?
fi) (27)The log-likelihood in Equation (16) can be calculated as follows:L(?)
=m?j=1(log Z?j ?
log Z?j ) (28)where log Z?
and log Z?
are the normalization constants for ?
and ?.5.5 Estimation in PracticeEstimating the parsing models consists of generating packed charts for each sentencein the training data, and then repeatedly calculating the values needed by the L-BFGSestimation algorithm until convergence.
Even though the packed charts are an efficientrepresentation of the derivation space, the charts for the complete training data (Sec-tions 02?21 of CCGbank) take up a considerable amount of memory.
One solution is toonly keep a small number of charts in memory at any one time, and to keep reading inthe charts on each iteration.
However, given that the L-BFGS algorithm takes hundredsof iterations to converge, this approach would be infeasibly slow.Our solution is to keep all charts in memory by developing a parallel version ofthe L-BFGS training algorithm and running it on an 18-node Beowulf cluster.
As wellas solving the memory problem, another significant advantge of parallelization is thereduction in estimation time: using 18 nodes allows our best-performing model to beestimated in less than three hours.We use the the Message Passing Interface (MPI) standard for the implementation(Gropp et al 1996).
The parallel implementation is a straightforward extension of theBFGS algorithm.
Each machine in the cluster deals with a subset of the training data,516Clark and Curran Wide-Coverage Efficient Statistical Parsingholding the packed charts for that subset in memory.
The key stages of the algorithmare the calculations of the model expectations and the likelihood function.
For a single-process version these are calculated by summing over all the training instances in oneplace.
For a multi-process version, these are summed in parallel, and at the end of eachiteration the parallel sums are combined to give a master sum.
Producing a masteroperation across a cluster using MPI is a reduce operation.
In our case, every node needsto be holding a copy of the master sum, so we use an all reduce operation.The MPI library handles all aspects of the parallelization, including finding theoptimal way of summing across the nodes of the Beowulf cluster (typically it is doneusing a tree algorithm).
In fact, the parallelization only adds around twenty lines ofcode to the single-process implementation.
Because of the simplicity of the parellelcommunication between the nodes, parallelizing the estimation code is an example ofan embarrassingly parallel problem.
One difficult aspect of the parallel implementationis that debugging can be much harder, in which case it is often easier to test a non-MPIversion of the program first.6.
The DecoderFor the normal-form model, the Viterbi algorithm is used to find the most probablederivation from a packed chart.
For each equivalence class, we record the individualentry at the root of the subderivation which has the highest score for the class.
Theequivalence classes were defined so that any other individual entry cannot be part of thehighest scoring derivation for the sentence.
The score for a subderivation d is?i ?ifi(d)where fi(d) is the number of times the ith feature occurs in the subderivation.
Thehighest-scoring subderivations can be calculated recursively using the highest-scoringequivalence classes that were combined to create the individual entry.For the dependency model, the highest scoring dependency structure is required.Clark and Curran (2003) outline an algorithm for finding the most probable dependencystructure, which keeps track of the highest scoring set of dependencies for each node inthe chart.
For a set of equivalent entries in the chart (a disjunctive node), this involvessumming over all conjunctive node daughters which head sub-derivations leading tothe same set of high scoring dependencies.
In practice large numbers of such conjunctivenodes lead to very long parse times.As an alternative to finding the most probable dependency structure, we havedeveloped an algorithm which maximizes the expected labeled recall over dependen-cies.
Our algorithm is based on Goodman?s (1996) labeled recall algorithm for thephrase-structure PARSEVAL measures.
As far as we know, this is the first applicationof Goodman?s approach to finding highest scoring dependency structures.
Watson,Carroll, and Briscoe (2005) have also applied our algorithm to the grammatical relationsoutput by the RASP parser.The dependency structure, ?max, which maximizes the expected recall is:?max = argmax???iP(?i|S)|?
?
?i| (29)where ?i ranges over the dependency structures for S. The expectation for a single de-pendency structure ?
is realized as a weighted intersection over all possible dependencystructures ?i for S. The intuition is that, if ?i is the gold standard, then the number ofdependencies recalled in ?
is |?
?
?i|.
Because we do not know which ?i is the gold517Computational Linguistics Volume 33, Number 4standard, then we calculate the expected recall by summing the recall of ?
relative toeach ?i, weighted by the probability of ?i.The expression can be expanded further:?max = argmax???iP(?i|S)???
?1 if ?
?
?i= argmax????????|????P(?
?|S)= argmax??????d??(??
)|???
?P(d|S) (30)The reason for this manipulation is that the expected recall score for ?
is nowwritten in terms of a sum over the individual dependencies in ?, rather than a sum overeach dependency structure for S. The inner sum is over all derivations which containa particular individual dependency ?.
Thus the final score for a dependency structure?
is a sum of the scores for each dependency ?
in ?
; and the score for a dependency ?is the sum of the probabilities of those derivations producing ?.
This latter sum can becalculated efficiently using inside and outside scores:?max = argmax????
?1ZS?c?C?c?c if ?
?
deps(c) (31)where ?c is the inside score and ?c is the outside score for node c; C is the set of conjunc-tive nodes in the packed chart for sentence S and deps(c) is the set of dependencies onconjunctive node c. The intuition behind the expected recall score is that a dependencystructure scores highly if it has dependencies produced by high probability derivations.4The reason for rewriting the score in terms of individual dependencies is to makeuse of the packed chart: The score for an individual dependency can be calculated usingdynamic programming (as explained previously), and the highest scoring dependencystructure can be found using dynamic programming also.
The algorithm which finds?max is essentially the same as the Viterbi algorithm described earlier, efficiently findinga derivation which produces the highest scoring set of dependencies.7.
Model FeaturesThe log-linear modeling framework allows considerable flexibility for representing theparse space in terms of features.
In this article we limit the features to those definedover local rule instantiations and single predicate?argument dependencies.
The fea-ture sets described below differ for the dependency and normal-form models.
The4 Coordinate constructions can create multiple dependencies for a single argument slot; in this case thescore for these multiple dependencies is the average of the individual scores.518Clark and Curran Wide-Coverage Efficient Statistical ParsingTable 1Features common to the dependency and normal-form models.Feature type ExampleLexCat + Word (S/S)/NP + BeforeLexCat + POS (S/S)/NP + INRootCat S[dcl]RootCat + Word S[dcl] + wasRootCat + POS S[dcl] + VBDRule S[dcl]?NP S[dcl]\NPRule + Word S[dcl]?NP S[dcl]\NP + boughtRule + POS S[dcl]?NP S[dcl]\NP + VBDdependency model has features defined over the CCG predicate?argument dependen-cies, whereas the dependencies for the normal-form model are defined in terms oflocal rule instantiations in the derivation.
Another difference is that the rule featuresfor the normal-form model are taken from the gold-standard normal-form deriva-tions, whereas the dependency model contains rule features from non-normal-formderivations.There are a number of features defined over derivations which are common tothe dependency model and the normal-form model.5 First, there are features whichrepresent each ?word, lexical-category?
pair in a derivation, and generalizations of thesewhich represent ?POS, lexical-category?
pairs.
Second, there are features representingthe root category of a derivation, which we also extend with the head word of theroot category; this latter feature is then generalized using the POS tag of the head (aspreviously described).
Third, there are features which encode rule instantiations?localtrees consisting of a parent and one or two children?in the derivation.
The first set ofrule features encode the combining categories and the result category; the second set offeatures extend the first by also encoding the head of the result category; and the thirdset generalizes the second using POS tags.
Table 1 gives an example for each of thesefeature types.The dependency model also has CCG predicate?argument dependencies as features,defined as 5-tuples as in Section 3.4.
In addition these features are generalized in threeways using POS tags, with the word?word pair replaced with word?POS, POS?word,and POS?POS.
Table 2 gives some examples.We extend the dependency features further by adding distance information.
Thedistance features encode the dependency relation and the word associated with thelexical category (but not the argument word), plus some measure of distance betweenthe two dependent words.
We use three distance measures which count the following:the number of intervening words, with four possible values 0, 1, 2, or more; the numberof intervening punctuation marks, with four possible values 0, 1, 2, or more; and thenumber of intervening verbs (determined by POS tag), with three possible values 0, 1,or more.
Each of these features is again generalized by replacing the word associatedwith the lexical category with its POS tag.5 Each feature has a corresponding frequency function, defined in Equation (14), which counts the numberof times the feature appears in a derivation.519Computational Linguistics Volume 33, Number 4Table 2Predicate?argument dependency features for the dependency model.Feature type ExampleWord?Word ?bought, (S\NP1)/NP2, 2, stake, (NP\NP)/(S[dcl]/NP)?Word?POS ?bought, (S\NP1)/NP2, 2, NN, (NP\NP)/(S[dcl]/NP)?POS?Word ?VBD, (S\NP1)/NP2, 2, stake, (NP\NP)/(S[dcl]/NP)?POS?POS ?VBD, (S\NP1)/NP2, 2, NN, (NP\NP)/(S[dcl]/NP)?Word + Distance(words) ?bought, (S\NP1)/NP2, 2, (NP\NP)/(S[dcl]/NP)?
+ 2Word + Distance(punct) ?bought, (S\NP1)/NP2, 2, (NP\NP)/(S[dcl]/NP)?
+ 0Word + Distance(verbs) ?bought, (S\NP1)/NP2, 2, (NP\NP)/(S[dcl]/NP)?
+ 0POS + Distance(words) ?VBD, (S\NP1)/NP2, 2, (NP\NP)/(S[dcl]/NP)?
+ 2POS + Distance(punct) ?VBD, (S\NP1)/NP2, 2, (NP\NP)/(S[dcl]/NP)?
+ 0POS + Distance(verbs) ?VBD, (S\NP1)/NP2, 2, (NP\NP)/(S[dcl]/NP)?
+ 0Table 3Rule dependency features for the normal-form model.Feature type ExampleWord?Word ?company, S[dcl]?NP S[dcl]\NP, bought?Word?POS ?company, S[dcl]?NP S[dcl]\NP, VBD?POS?Word ?NN, S[dcl]?NP S[dcl]\NP, bought?POS?POS ?NN, S[dcl]?NP S[dcl]\NP, VBD?Word + Distance(words) ?bought, S[dcl]?NP S[dcl]\NP?
+ > 2Word + Distance(punct) ?bought, S[dcl]?NP S[dcl]\NP?
+ 2Word + Distance(verbs) ?bought, S[dcl]?NP S[dcl]\NP?
+ 0POS + Distance(words) ?VBD, S[dcl]?NP S[dcl]\NP?
+ > 2POS + Distance(punct) ?VBD, S[dcl]?NP S[dcl]\NP?
+ 2POS + Distance(verbs) ?VBD, S[dcl]?NP S[dcl]\NP?
+ 0For the normal-form model we follow Hockenmaier and Steedman (2002b) by de-fining dependency features in terms of the local rule instantiations, by adding the headsof the combining categories to the rule instantiation features.6 These are generalizedin three ways using POS tags, as shown in Table 3.
There are also the three distancemeasures which encode the distance between the two head words of the combining cat-egories, as for the dependency model.
Here the distance feature encodes the combiningcategories, the result category, the head of the result category (either as a word or POStag), and the distance between the two head words.For the features in the normal-form model, a frequency cutoff of two was applied;that is, a feature had to occur at least twice in the gold-standard normal-form deriva-tions to be included in the model.
The same cutoff was applied to the features in thedependency model, except for the rule instantiation feature types.
For these features thecounting was done across all derivations licensed by the gold-standard lexical categorysequences and a frequency cutoff of 10 was applied.
The larger cutoff was used becausethe productivity of the grammar can lead to very large numbers of these features.
We6 We have also considered a model containing predicate?argument dependencies as well as local ruledependencies, but adding the extra dependency feature types had no impact on the accuracy of thenormal-form model.520Clark and Curran Wide-Coverage Efficient Statistical Parsingalso only included those features which had a nonzero empirical count, that is, thosefeatures which occured on at least one correct derivation.
These feature types andfrequency cutoffs led to 475,537 features for the normal-form model and 632,591 featuresfor the dependency model.8.
The SupertaggerParsing with lexicalized grammar formalisms such as CCG is a two-step process: first,elementary syntactic structures?in CCG?s case lexical categories?are assigned to eachword in the sentence, and then the parser combines the structures together.
The firststep can be performed by simply assigning to each word all lexical categories the wordis seen with in the training data, together with some strategy for dealing with rareand unknown words (such as assigning the complete lexical category set; Hockenmaier2003a).
Because the number of lexical categories assigned to a word can be high, somestrategy is needed to make parsing practical; Hockenmaier, for example, uses a beamsearch to discard chart entries with low scores.In this article we take a different approach, by using a supertagger (Bangalore andJoshi 1999) to perform step one.
Clark and Curran (2004a) describe the supertagger,which uses log-linear models to define a distribution over the lexical category set foreach local five-word context containing the target word (Ratnaparkhi 1996).
The featuresused in the models are the words and POS tags in the five-word window, plus thetwo previously assigned lexical categories to the left.
The conditional probability of asequence of lexical categories, given a sentence, is then defined as the product of theindividual probabilities for each category.The most probable lexical category sequence can be found efficiently using a variantof the Viterbi algorithm for HMM taggers.
We restrict the categories which can beassigned to a word by using a tag dictionary: for words seen at least k times in thetraining data, the tagger can only assign categories which have been seen with theword in the data.
For words seen less than k times, an alternative based on the word?sPOS tag is used: The tagger can only assign categories which have been seen with thePOS tag in the data.
We have found the tag dictionary to be beneficial in terms of bothefficiency and accuracy.
A value of k = 20 was used in the experiments described in thisarticle.The lexical category set used by the supertagger is described in Clark and Curran(2004a) and Curran, Clark, and Vadas (2006).
It includes all lexical catgeories which ap-pear at least 10 times in Sections 02?21 of CCGbank, resulting in a set of 425 categories.The Clark and Curran paper shows this set to have very high coverage on unseen data.The accuracy of the supertagger on Section 00 of CCGbank is 92.6%, with a sentenceaccuracy of 36.8%.
Sentence accuracy is the percentage of sentences whose words areall tagged correctly.
These figures include punctuation marks, for which the lexicalcategory is simply the punctuation mark itself, and are obtained using gold standardPOS tags.
With automatically assigned POS tags, using the POS tagger of Curran andClark (2003), the accuracies drop to 91.5% and 32.5%.
An accuracy of 91?92% may ap-pear reasonable given the large lexical category set; however, the low sentence accuracysuggests that the supertagger may not be accurate enough to serve as a front-end to aparser.
Clark (2002) reports that a significant loss in coverage results if the supertaggeris used as a front-end to the parser of Hockenmaier and Steedman (2002b).
In orderto increase the number of words assigned the correct category, we develop a CCGmultitagger, which is able to assign more than one category to each word.521Computational Linguistics Volume 33, Number 4Table 4Supertagger ambiguity and accuracy on Section 00.?
k CATS/WORD ACC SENT ACC ACC (POS) SENT ACC0.075 20 1.27 97.34 67.43 96.34 60.270.030 20 1.43 97.92 72.87 97.05 65.500.010 20 1.72 98.37 77.73 97.63 70.520.005 20 1.98 98.52 79.25 97.86 72.240.001 150 3.57 99.17 87.19 98.66 80.24The multitagger uses the following conditional probabilities:P(yi|w1, .
.
.
, wn) =?y1,...,yi?1,yi+1,...,ynP(yi, y1, .
.
.
, yi?1, yi+1, .
.
.
, yn|w1, .
.
.
, wn) (32)Here yi is to be thought of as a constant category, whereas yj ( j = i) varies over thepossible categories for word j.
In words, the probability of category yi, given thesentence, is the sum of the probabilities of all sequences containing yi.
This sum canbe calculated efficiently using a variant of the forward?backward algorithm.
For eachword in the sentence, the multitagger then assigns all those categories whose probabilityaccording to Equation (32) is within some factor, ?, of the highest probability categoryfor that word.
In the implementation used here the forward?backward sum is limited tothose sequences allowed by the tag dictionary.
For efficiency purposes, an extra pruningstrategy is also used to discard low probability sub-sequences before the forward?backward algorithm is run.
This uses a second variable-width beam of 0.1?.Table 4 gives the per-word accuracy of the supertagger on Section 00 for variouslevels of category ambiguity, together with the average number of categories per word.7The SENT column gives the percentage of sentences whose words are all supertaggedcorrectly.
The set of categories assigned to a word is considered correct if it containsthe correct category.
The table gives results when using gold standard POS tags and, inthe final two columns, when using POS tags automatically assigned by the POS taggerdescribed in Curran and Clark (2003).
The drop in accuracy is expected, given theimportance of POS tags as features.The table demonstrates the significant reduction in the average number of cate-gories that can be achieved through the use of a supertagger.
To give one example,the number of categories in the tag dictionary?s entry for the word is is 45.
However,in the sentence Mr. Vinken is chairman of Elsevier N.V., the Dutch publishing group., thesupertagger correctly assigns one category to is for all values of ?.In our earlier work (Clark and Curran 2004a) the forward?backward algorithm wasnot used to estimate the probability in Equation (32).
Curran, Clark, and Vadas (2006)investigate the improvement obtained from using the forward?backward algorithm,and also address the drop in supertagger accuracy when using automatically assignedPOS tags.
We show how to maintain some POS ambiguity through to the supertaggingphase, using a multi-POS tagger, and also how POS tag probabilities can be encodedas real-valued features in the supertagger.
The drop in supertagging accuracy when7 The ?
values used here are slightly different to the values used in earlier publications because thepruning strategy used in the supertagger has changed slightly.522Clark and Curran Wide-Coverage Efficient Statistical Parsingmoving from gold to automatically assigned POS tags is reduced by roughly 50% acrossthe various values of ?.9.
Parsing in Practice9.1 Combining the Supertagger and the ParserThe philosophy in earlier work which combined the supertagger and parser (Clark,Hockenmaier, and Steedman 2002; Clark and Curran 2003) was to use an unrestrictivesetting of the supertagger, but still allow a reasonable compromise between speed andaccuracy.
The idea was to give the parser the greatest possibility of finding the correctparse, by initializing it with as many lexical categories as possible, but still retainreasonable efficiency.
However, for some sentences, the number of categories in thechart gets extremely large with this approach, and parsing is unacceptably slow.
Hence alimit was applied to the number of categories in the chart, and a more restrictive settingof the supertagger was reverted to if the limit was exceeded.In this article we consider the opposite approach: Start with a very restrictive settingof the supertagger, and only assign more categories if the parser cannot find an analysisspanning the sentence.
In this way the parser interacts much more closely with thesupertagger.
In effect, the parser is using the grammar to decide if the categories pro-vided by the supertagger are acceptable, and if not the parser requests more categories.The advantage of this adaptive supertagging approach is that parsing speeds are muchhigher, without any corresponding loss in accuracy.
Section 10.3 gives results for thespeed of the parser.9.2 Chart Parsing AlgorithmThe algorithm used to build the packed charts is the CKY chart parsing algorithm(Kasami 1965; Younger 1967) described in Steedman (2000).
The CKY algorithm appliesnaturally to CCG because the grammar is binary.
It builds the chart bottom-up, startingwith constituents spanning a single word, incrementally increasing the span until thewhole sentence is covered.
Because the constituents are built in order of span size, at anypoint in the process all the sub-constituents which could be used to create a particularnew constituent must be present in the chart.
Hence dynamic programming can be usedto prevent the need for backtracking during the parsing process.9.3 Grammar ImplementationThere is a trade-off between the size and coverage of the grammar and the efficiencyof the parser.
One of our main goals in this work has been to develop a parser whichcan provide analyses for the vast majority of linguistic constructions in CCGbank, but isalso efficient enough for large-scale NLP applications.
In this section we describe someof the decisions we made when implementing the grammar, with this trade-off in mind.First, the lexical category set we use does not contain all the categories in Sec-tions 02?21 of CCGbank.
Applying a frequency cutoff of 10 results in a set of 425 lexicalcategories.
This set has excellent coverage on unseen data (Clark and Curran 2004a)and is a manageable size for adding the head and dependency information, and alsomapping to grammatical relations for evaluation purposes (Section 11).523Computational Linguistics Volume 33, Number 4Second, for the normal-form model, and also the hybrid dependency model de-scribed in Section 10.2.1, two types of contraints on the grammar rules are used.
Sec-tion 3 described the Eisner constraints, in which any constituent which is the result ofa forward composition cannot serve as the primary (left) functor in another forwardcomposition or forward application; an analogous constraint applies for backwardcomposition.
The second type of constraint only allows two categories to combine ifthey have been seen to combine in the training data.
Although this constraint onlypermits category combinations seen in Sections 02?21 of CCGbank, we have found thatit is detrimental to neither parser accuracy nor coverage.Neither of these constraints guarantee a normal-form derivation, but they are botheffective at reducing the size of the charts, which can greatly increase parser speed(Clark and Curran 2004a).
The constraints are also useful for training.
Section 10 showsthat having a less restrictive setting on the supertagger, when creating charts for dis-criminative training, can lead to more accurate models.
However, the optimal settingon the supertagger for training purposes can only be used when the constraints areapplied, because otherwise the memory requirements are prohibitive.Following Steedman (2000), we place the following constraint on backward crossedcomposition (for all models): The Y category in (7) cannot be an N or NP category.
Wealso place a similar constraint on backward composition.
Both constraints reduce thesize of the charts considerably with no impact on coverage or accuracy.Type-raising is performed by the parser for the categories NP, PP, and S[adj]\NP.It is implemented by adding one of three fixed sets of categories to the chart wheneveran NP, PP, or S[adj]\NP is present.
Appendix A gives the category sets.
Each categorytransformation is an instance of the following two rule schemata:X ?T T/(T\X) (> T) (33)X ?T T\(T/X) (< T) (34)Appendix A lists the punctuation and type-changing rules implemented in theparser.
This is a larger grammar than we have used in previous articles (Clark andCurran 2004b, 2004a, 2006), mainly because the improvement in the supertagger sincethe earlier work means that we can now use a larger grammar but still maintain highlyefficient parsing.10.
ExperimentsThe statistics relating to model estimation were obtained using Sections 02?21 of CCG-bank as training data.
The results for parsing accuracy were obtained using Section00 as development data and Section 23 as the final test data.
The results for parsingspeed were obtained using Section 23.
There are various hyperparameters in the parsingsystem, for example the frequency cutoff for features, the ?
parameter in the Gaussianprior term, the ?
values used in the supertagger, and so on.
All of these were setexperimentally using Section 00 as development data.10.1 Model EstimationThe gold standard for the normal-form model consists of the normal-form derivationsin CCGbank.
For the dependency model, the gold-standard dependency structures are524Clark and Curran Wide-Coverage Efficient Statistical Parsingproduced by running our CCG parser over the normal-form derivations.
It is essentialthat the packed charts for each sentence contain the gold standard; for the normal-form model this means that our parser must be able to produce the gold-standardderivation from the gold-standard lexical category sequence; and for the dependencymodel this means that at least one derivation in the chart must produce the gold-standard dependency structure.
Not all rule instantiations in CCGbank can be producedby our parser, because some are not instances of combinatory rules, and others are veryrare punctuation and type-changing rules which we have not implemented.
Hence itis not possible for the parser to produce the gold standard for every sentence in Sec-tions 02?21, for either the normal-form or the dependency model.
These sentences arenot used in the training process.For parsing the training data, we ensure that the correct category is a member of theset assigned to each word.
(We do not do this when parsing the test data.)
The averagenumber of categories assigned to each word is determined by the ?
parameter in thesupertagger.
A category is assigned to a word if the category?s probability is within ?
ofthe highest probability category for that word.
Hence the value of ?
has a direct effecton the size of the packed charts: Smaller ?
values lead to larger charts.For training purposes, the ?
parameter determines how many incorrect derivationswill be used for each sentence for the discriminative training algorithm.
We have foundthat the ?
parameter can have a large impact on the accuracy of the resulting models:If the ?
value is too large, then the training algorithm does not have enough incorrectderivations to ?discriminate against?
; if the ?
value is too small, then this introducestoo many incorrect derivations into the training process, and can lead to impracticalmemory requirements.For some sentences, the packed charts can become very large.
The supertaggingapproach we adopt for training differs from that used for testing and follows the originalapproach of Clark, Hockenmaier, and Steedman (2002): If the size of the chart exceedssome threshold, the value of ?
is increased, reducing ambiguity, and the sentence issupertagged and parsed again.
The threshold which limits the size of the charts was setat 300,000 individual entries.
(This is the threshold used for training; a higher value wasused for testing.)
For a small number of long sentences the threshold is exceeded evenat the largest ?
value; these sentences are not used for training.For the normal-form model we were able to use 35,732 sentences for training (90.2%of Sections 02?21) and for the dependency model 35,889 sentences (90.6%).
Table 5gives training statistics for the normal-form and dependency models (and a hybridmodel described in Section 10.2.1), for various sequences of ?
values, when the trainingalgorithm is run to convergence on an 18-node cluster.
The training algorithm is definedto have converged when the percentage change in the objective function is less than0.0001%.
The ?
value in Equation (16), which was determined experimentally using thedevelopment data, was set at 1.3 for all the experiments in this article.Table 5Training statistics.Model ?
values CPU time (min.)
Iterations RAM (GB)Dependency 0.1 176.0 750 24.4Normal-form 0.1 17.2 420 5.3Normal-form 0.0045, 0.0055, 0.01, 0.05, 0.1 72.1 466 16.1Hybrid 0.0045, 0.0055, 0.01, 0.05, 0.1 128.4 610 22.5525Computational Linguistics Volume 33, Number 4The main reason that the normal-form model requires less memory and convergesfaster than the dependency model is that, for the normal-form model, we applied thetwo types of normal-form restriction described in Section 9.3: First, categories can onlycombine if they appear together in a rule instantiation in Sections 2?21 of CCGbank;and second, we applied the Eisner constraints described in Section 3.We conclude this section by noting that it is only through the use of the supertaggerthat we are able to perform the discriminative estimation at all; without it the memoryrequirements would be prohibitive, even when using the cluster.10.2 Parsing AccuracyThis section gives accuracy figures on the predicate?argument dependencies in CCG-bank.
Overall results are given, as well as results broken down by relation type, asin Clark, Hockenmaier, and Steedman (2002).
Because the purpose of this article isto demonstrate the feasibility of wide-coverage parsing with CCG, we do not give anevaluation targeted specifically at long-range dependencies; such an evaluation waspresented in Clark, Steedman, and Curran (2004).For evaluation purposes, the threshold parameter which limits the size of the chartswas set at 1,000,000 individual entries.
This value was chosen to maximize the coverageof the parser, so that the evaluation is performed on as much of the unseen data aspossible.
This was also the threshold parameter used for the speed experiments inSection 10.3.All of the intermediate results were obtained using Section 00 of CCGbank asdevelopment data.
The final test result, showing the performance of the best performingmodel, was obtained using Section 23.
Evaluation was performed by comparing thedependency output of the parser against the predicate?argument dependencies in CCG-bank.
We report precision, recall, and F-scores for labeled and unlabeled dependencies,and also category accuracy.
The category accuracy is the percentage of words assignedthe correct lexical category by the parser (including punctuation).
The labeled depen-dency scores take into account the lexical category containing the dependency relation,the argument slot, the word associated with the lexical category, and the argumenthead word: All four must be correct to score a point.
For the unlabeled scores, onlythe two dependent words are considered.
The F-score is the balanced harmonic mean ofprecision (P) and recall (R): 2PR/(P + R).
The scores are given only for those sentenceswhich were parsed successfully.
We also give coverage values showing the percentageof sentences which were parsed successfully.Using the CCGbank dependencies for evaluation is a departure from our earlierwork, in which we generated our own gold standard by running the parser over thederivations in CCGbank and outputting the dependencies.
In this article we wanted touse a gold standard which is easily accessible to other researchers.
However, there aresome differences between the dependency scheme used by our parser and CCGbank.For example, our parser outputs some coordination dependencies which are not inCCGbank; also, because the parser currently encodes every argument slot in each lexicalcategory as a dependency relation, there are some relations, such as the subject of toin a to-infinitival construction, which are not in CCGbank either.
In order to providea fair evaluation, we ignore those dependency relations.
This still leaves some minordifferences.
We can measure the remaining differences as follows: Comparing the CCG-bank dependencies in Section 00 against those generated by running our parser overthe derivations in 00 gives labeled precision and recall values of 99.80% and 99.18%,526Clark and Curran Wide-Coverage Efficient Statistical Parsingrespectively.
Thus there are a small number of dependencies in CCGbank which thecurrent version of the parser can never get right.10.2.1 Dependency Model vs. Normal-Form Model.
Table 6 shows the results for the normal-form and dependency models evaluated against the predicate?argument dependen-cies in CCGbank.
Gold standard POS tags were used; the LF(POS) column gives thelabeled F-score with automatically assigned POS tags for comparison.
Decoding withthe dependency model involves finding the maximum-recall dependency structure, anddecoding with the normal-form model involves finding the most probable derivation,as described in Section 6.
The ?
value refers to the setting of the supertagger used fortraining and is the first in the sequence of ?s from Table 5.
The ?
values used duringthe testing are those in Table 4 and the new, efficient supertagging strategy of taking thehighest ?
value first was used.With the same ?
values used for training (?
= 0.1), the results for the dependencymodel are slightly higher than for the normal-form model.
However, the coverage of thenormal-form model is higher (because the use of the normal-form constraints mean thatthere are less sentences which exceed the chart-size threshold).
One clear result from thetable is that increasing the chart size used for training, by using smaller ?
values, cansignificantly improve the results, in this case around 1.5% F-score for the normal-formmodel.The training of the dependency model already uses most of the RAM available onthe cluster.
However, it is possible to use smaller ?
values for training the dependencymodel if we also apply the two types of normal-form restriction used by the normal-form model.
This hybrid model still uses the features from the dependency model;it is still trained using dependency structures as the gold standard; and decoding isstill performed using the maximum-recall algorithm; the only difference is that thederivations in the charts are restricted by the normal-form constraints (both for trainingand testing).
Table 5 gives the training statistics for this model, compared to the de-pendency and normal-form models.
The number of sentences we were able to use fortraining this model was 36,345 (91.8% of Sections 02?21).
The accuracy of this hybriddependency model is given in Table 7.
These are the highest results we have obtainedto date on Section 00.
We also give the results for the normal-form model from Table 6for comparison.Table 8 gives the results for the hybrid dependency model, broken down by relationtype, using the same relations given in Clark, Hockenmaier, and Steedman (2002).Automatically assigned POS tags were used.10.2.2 Final Test Results.
Table 9 gives the final test results on Section 23 for the hybriddependency model.
The coverage for these results is 99.63% (for gold-standard POSTable 6Results for dependency and normal-form models on Section 00.LF SENT CATModel ?
LP LR LF (POS) ACC UP UR UF ACC covDependency 0.1 86.52 84.97 85.73 84.24 32.06 92.91 91.24 92.07 93.37 98.17Normal-form 0.1 85.50 84.68 85.08 83.34 31.93 92.38 91.49 91.93 93.04 99.06Normal-form 0.0045 87.17 86.30 86.73 84.74 34.99 93.21 92.28 92.74 94.05 99.06527Computational Linguistics Volume 33, Number 4Table 7Results on Section 00 with both models using normal-form constraints.LF SENT CATModel ?
LP LR LF (POS) ACC UP UR UF ACC covNormal-form 0.0045 87.17 86.30 86.73 84.74 34.99 93.21 92.28 92.74 94.05 99.06Hybrid dependency 0.0045 88.06 86.43 87.24 85.25 35.67 93.88 92.13 93.00 94.16 99.06Table 8Results for the hybrid dependency model on Section 00 by dependency relation.Lexical category Arg Slot LP % # deps LR % # deps F-scoreNX/NX,1 1 nominal modifier 95.28 7,314 95.62 7,288 95.45NPX/NX,1 1 determiner 96.57 4,078 96.03 4,101 96.30(NPX\NPX,1)/NP2 2 np modifying prep 82.17 2,574 88.90 2,379 85.40(NPX\NPX,1)/NP2 1 np modifying prep 81.58 2,285 85.74 2,174 83.61((SX\NPY)\(SX,1\NPY))/NP2 2 vp modifying prep 71.94 1,169 73.32 1,147 72.63((SX\NPY)\(SX,1\NPY))/NP2 1 vp modifying prep 70.92 1,073 71.93 1,058 71.42(S[dcl]\NP1)/NP2 1 transitive verb 81.62 914 85.55 872 83.54(S[dcl]\NP1)/NP2 2 transitive verb 81.57 971 86.37 917 83.90(SX\NPY)\(SX,1\NPY) 1 adverbial modifier 86.85 745 86.73 746 86.79PP/NP1 1 prep complement 75.06 818 70.09 876 72.49(S[b]\NP1)/NP2 2 inf transitive verb 84.01 663 87.03 640 85.50(S[dcl]\NPX,1)/(S[b]2\NPX) 2 auxiliary 97.70 478 97.90 477 97.80(S[dcl]\NPX,1)/(S[b]2\NPX) 1 auxiliary 94.15 479 92.99 485 93.57(S[b]\NP1)/NP2 1 inf transitive verb 77.82 496 73.95 522 75.83(NPX/NX,1)\NP2 1 s genitive 96.57 379 95.56 383 96.06(NPX/NX,1)\NP2 2 s genitive 97.35 377 98.66 372 98.00(S[dcl]\NP1)/S[dcl]2 1 sentential comp verb 94.88 371 90.96 387 92.88(NPX\NPX,1)/(S[dcl]2\NPX) 1 subject rel pronoun 85.77 260 81.39 274 83.52(NPX\NPX,1)/(S[dcl]2\NPX) 2 subject rel pronoun 97.45 275 97.10 276 97.28(NPX\NPX,1)/(S[dcl]2/NPX) 1 object rel pronoun 81.82 22 69.23 26 75.00(NPX\NPX,1)/(S[dcl]2/NPX) 2 object rel pronoun 86.36 22 82.61 23 84.44NP/(S[dcl]1/NP) 1 headless obj rel pron 100.00 17 100.00 17 100.00Table 9Results for the hybrid dependency model on Section 23.LP LR LF SENT UP UR UF CAT ACC covHybrid dependency 88.34 86.96 87.64 36.53 93.74 92.28 93.00 94.32 99.63Hybrid dependency (POS) 86.17 84.74 85.45 32.92 92.43 90.89 91.65 92.98 99.58Hockenmaier (2003) 84.3 84.6 84.4 ?
91.8 92.2 92.0 92.2 99.83Hockenmaier (POS) 83.1 83.5 83.3 ?
91.1 91.5 91.3 91.5 99.83tags), which corresponds to 2,398 of the 2,407 sentences in Section 23 receiving ananalysis.
When using automatically assigned POS tags, the coverage is slightly lower:99.58%.
We used version 1.2 of CCGbank to obtain these results.
Results are also givenfor Hockenmaier?s parser (Hockenmaier 2003a) which used an earlier, slightly differentversion of the treebank.
We wanted to use the latest version to enable other researchersto compare with our results.528Clark and Curran Wide-Coverage Efficient Statistical Parsing10.3 Parse TimesThe results in this section were obtained using a 3.2 GHz Intel Xeon P4.
Table 10 givesparse times for the 2,407 sentences in Section 23 of CCGbank.
In order not to optimizespeed by compromising accuracy, we used the hybrid dependency model, togetherwith both kinds of normal-form constraints, and the maximum-recall decoder.
Timesare given for both automatically assigned POS tags and gold-standard POS tags (POS).The sents and words columns give the number of sentences, and the number of words,parsed per second.
For all of the figures reported on Section 23, unless stated otherwise,we chose settings for the various parameters which resulted in a coverage of 99.6%.
It ispossible to obtain an analysis for the remaining 0.4%, but at a significant loss in speed.The parse times and speeds include the failed sentences, and include the time taken bythe supertagger, but not the POS tagger; however, the POS tagger is extremely efficient,taking less than 4 seconds to supertag Section 23, most of which consists of load timefor the Maximum Entropy model.The first row corresponds to the strategy of earlier work by starting with an un-restrictive setting of the supertagger.
The first value of ?
is 0.005; if the parser cannotfind a spanning analysis, this is changed to ?
= 0.001k=150, which increases the averagenumber of categories assigned to a word by decreasing ?
and increasing the tag-dictionary parameter.
If the node limit is exceeded at ?
= 0.005 (for these experimentsthe node limit is set at 1,000,000), ?
is changed to 0.01.
If the node limit is still exceeded,?
is changed to 0.03, and finally to 0.075.The second row corresponds to the new strategy of starting with the most restrictivesetting of the supertagger (?
= 0.075), and moving through the settings if the parsercannot find a spanning analysis.
The table shows that the new strategy has a significantimpact on parsing speed, increasing it by a factor of 3 over the earlier approach (giventhe parameter settings used in these experiments).The penultimate row corresponds to using only one supertagging level with ?
=0.075; the parser ignores the sentence if it cannot get an analysis at this level.
The per-centage of sentences without an analysis is now over 6% (with automatically assignedPOS tags), but the parser is extremely fast, processing over 30 sentences per second.
Thisconfiguration of the system would be useful for obtaining data for lexical knowledgeacquisition, for example, for which large amounts of data are required.
The oracle rowgives the parser speed when it is provided with only the correct lexical categories,showing the speeds which could be achieved given the perfect supertagger.Table 11 gives the percentage of sentences which are parsed at each supertaggerlevel, for both the new and old parsing strategies.
The results show that, for the oldapproach, most of the sentences are parsed using the least restrictive setting of thesupertagger (?
= 0.005); conversely, for the new approach, most of the sentences areTable 10Parse times for Section 23.Supertagging/parsing Time Sents/ Words/ Time (POS) Sents (POS)/ Words (POS)/constraints (sec) sec sec (sec) sec sec?
= 0.005 ?
.
.
.
?
0.075 379.4 6.3 145.9 369.0 6.5 150.0?
= 0.075 ?
.
.
.
0.001k=150 99.9 24.1 554.3 116.6 20.6 474.7?
= 0.075 (95.3/93.4% cov) 79.7 30.2 694.6 79.8 30.1 693.5Oracle (94.7% cov) 23.8 101.0 2,322.6529Computational Linguistics Volume 33, Number 4Table 11Supertagger ?
levels used on Section 00.0.075 FIRST 0.005 FIRST?
CATS/WORD PARSES % PARSES %0.075 1.27 1,786 93.4 3 0.20.03 1.43 45 2.4 5 0.30.01 1.72 24 1.3 2 0.10.005 1.98 18 0.9 1,863 97.40.001k=150 3.57 22 1.2 22 1.2FAIL 18 0.9 18 0.9Table 12Comparing parser speeds on Section 23 of the WSJ Penn Treebank.Parser Time (min.
)Collins 45Charniak 28Sagae 11CCG 1.9parsed using the most restrictive setting (?
= 0.075).
This suggests that, in order toincrease the accuracy of the parser without losing efficiency, the accuracy of the su-pertagger at the ?
= 0.075 level needs to be improved, without increasing the numberof categories assigned on average.A possible response to our policy of adaptive supertagging is that any statisticalparser can be made to run faster, for example by changing the beam parameter inthe Collins (2003) parser, but that any increase in speed is typically associated with areduction in accuracy.
For the CCG parser, the accuracy did not degrade when using thenew adaptive parsing strategy.
Thus the accuracy and efficiency of the parser were nottuned separately: The configuration used to obtain the speed results was also used toobtain the accuracy results in Sections 10.2 and 11.To give some idea of how these parsing speeds compare with existing parsers,Table 12 gives the parse times on Section 23 for a number of well-known parsers.
Sagaeand Lavie (2005) is a classifier-based linear time parser.
The times for the Sagae, Collins,and Charniak parsers were taken from the Sagae and Lavie paper, and were obtainedusing a 1.8 GHz P4, compared to a 3.2 GHz P4 for the CCG numbers.
Comparing parserspeeds is especially problematic because of implementation differences and the factthat the accuracy of the parsers is not being controlled.
Thus we are not making anystrong claims about the efficiency of parsing with CCG compared to other formalisms.However, the results in Table 12 add considerable weight to one of our main claims inthis article, namely, that highly efficient parsing is possible with CCG, and that large-scale processing is possible with linguistically motivated grammars.11.
Cross-Formalism ComparisonAn obvious question is how well the CCG parser compares with parsers using differentgrammar formalisms.
One question we are often asked is whether the CCG derivations530Clark and Curran Wide-Coverage Efficient Statistical Parsingoutput by the parser could be converted to Penn Treebank?style trees to enable a com-parison with, for example, the Collins and Charniak parsers.
The difficulty is that CCGderivations often have a different shape to the Penn Treebank analyses (coordinationbeing a prime example) and reversing the mapping used by Hockenmaier to createCCGbank is a far from trivial task.There is some existing work comparing parser performance across formalisms.Briscoe and Carroll (2006) evaluate the RASP parser on the Parc Dependency Bank(DepBank; King et al 2003).
Cahill et al (2004) evaluate an LFG parser, which uses anautomatically extracted grammar, against DepBank.
Miyao and Tsujii (2004) evaluatetheir HPSG parser against PropBank (Palmer, Gildea, and Kingsbury 2005).
Kaplan et al(2004) compare the Collins parser with the Parc LFG parser by mapping Penn Treebankparses into the dependencies of DepBank, claiming that the LFG parser is more accuratewith only a slight reduction in speed.
Preiss (2003) compares the parsers of Collinsand Charniak, the grammatical relations finder of Buchholz, Veenstra, and Daelemans(1999), and the Briscoe and Carroll (2002) parser, using the gold-standard grammaticalrelations (GRs) from Carroll, Briscoe, and Sanfilippo (1998).
The Penn Treebank trees ofthe Collins and Charniak parsers, and the GRs of the Buchholz parser, are mapped intothe required grammatical relations, with the result that the GR finder of Buchholz is themost accurate.There are a number of problems with such evaluations.
The first is that, whenconverting the output of the Collins parser, for example, into the output of anotherparser, the Collins parser is at an immediate disadvantage.
This is especially true if thealternative output is significantly different from the Penn Treebank trees and if the in-formation required to produce the alternative output is hard to extract.
One could arguethat the relative lack of grammatical information in the output of the Collins parser is aweakness and any evaluation should measure that.
However, we feel that the onus ofmapping into another formalism should ideally lie with the researchers making claimsabout their own particular parser.
The second difficulty is that some constructions maybe analyzed differently across formalisms, and even apparently trivial differences suchas tokenization can complicate the comparison (Crouch et al 2002).Despite these difficulties we have attempted a cross-formalism comparison of theCCG parser.
For the gold standard we chose the version of DepBank reannotated byBriscoe and Carroll (2006) (hereafter B&C), consisting of 700 sentences from Section 23of the Penn Treebank.
The B&C scheme is similar to the original DepBank scheme inmany respects, but overall contains less grammatical detail.
Briscoe and Carroll (2006)describe the differences between the two schemes.We chose this resource for the following reasons: It is publicly available, allowingother researchers to compare against our results; the GRs making up the annotationshare some similarities with the predicate?argument dependencies output by the CCGparser; and we can directly compare our parser against a non-CCG parser, namely theRASP parser?and because we are converting the CCG output into the format used byRASP the CCG parser is not at an unfair advantage.
There is also the SUSANNE GR goldstandard (Carroll, Briscoe, and Sanfilippo 1998), on which the B&C annotation is based,but we chose not to use this for evaluation.
This earlier GR scheme is less like the depen-dencies output by the CCG parser, and the comparison would be complicated further byfact that, unlike CCGbank, the SUSANNE corpus is not based on the Penn Treebank.The GRs are described in Briscoe (2006), Briscoe and Carroll (2006), and Briscoe,Carroll, and Watson (2006).
Table 13 contains the complete list of GRs used in theevaluation, with examples taken from Briscoe.
The CCG dependencies were trans-formed into GRs in two stages.
The first stage was to create a mapping between the531Computational Linguistics Volume 33, Number 4Table 13The grammatical relations scheme used in the cross-formalism comparison.GR Description Example Relevant GRs in exampleconj coordinator Kim likes oranges, apples, (dobj likes and)and satsumas or clementines (conj and oranges)(conj and apples)(conj and or)(conj or satsumas)(conj or clementines)aux auxiliary Kim has been sleeping (aux sleeping has)(aux sleeping been)det determiner the man (det man the)ncmod non-clausal modifier the old man in the barn slept (ncmod man old)(ncmod man in)(dobj in barn)the butcher?s shop (ncmod poss shop butcher)xmod unsaturated predicative who to talk to (xmod to who talk)modifier (iobj talk to)(dobj to who)cmod saturated clausal modifier although he came, Kim left (cmod left although)(ccomp although came)ncsubj non-clausal subject Kim left (ncsubj left Kim )the upset man (ncsubj upset man obj)(passive upset)Kim wants to go (ncsubj go Kim )He?s going said Kim (ncsubj said Kim inv)passive passive verb issues were filed (passive filed)(ncsubj filed issues obj)xsubj unsaturated predicative subject leaving matters (xsubj matters leaving )csubj saturated clausal subject that he came matters (csubj matters came that)dobj direct object she gave it to Kim (dobj gave it)(iobj gave to)(dobj to Kim)obj2 second object she gave Kim toys (obj2 gave toys)(dobj gave Kim)iobj indirect object Kim flew to Paris from Geneva (iobj flew to)(iobj flew from)(dobj to Paris)(dobj from Geneva)pcomp PP which is a PP complement Kim climbed through into the attic (pcomp climbed through)(pcomp through into)(dobj into attic)xcomp unsaturated VP complement Kim thought of leaving (xcomp thought of)(xcomp of leaving)ccomp saturated clausal complement Kim asked about him playing rugby (ccomp asked about)(ccomp about him)(ncsubj playing him )(dobj playing rugby)ta textual adjunct delimited He made the discovery: (ta colon discovery was)by punctuation Kim was the abbotCCG dependencies and the GRs.
This involved mapping each argument slot in the 425lexical categories in the CCG lexicon onto a GR.
In the second stage, the GRs createdfor a particular sentence?by applying the mapping to the parser output?were passedthrough a Python script designed to correct some of the obvious remaining differencesbetween the CCG and GR representations.In the process of performing the transformation we encountered a methodologicalproblem: Without looking at examples it was difficult to create the mapping and im-possible to know whether the two representations were converging.
Briscoe, Carroll,532Clark and Curran Wide-Coverage Efficient Statistical Parsingand Watson (2006) split the 700 sentences in DepBank into a test and development set,but the latter only consists of 140 sentences which we found was not enough to reliablycreate the transformation.
There are some development files in the RASP release whichprovide examples of the GRs, which we used when possible, but these only cover asubset of the CCG lexical categories.Our solution to this problem was to convert the gold-standard dependencies fromCCGbank into GRs and use these to develop the transformation.
So we did inspect theannotation in DepBank, and compared it to the transformed CCG dependencies, butonly the gold-standard CCG dependencies.
Thus the parser output was never used dur-ing this process.
We also ensured that the dependency mapping and the post-processingare general to the GRs scheme and not specific to the test set.
Table 14 gives someexamples of the dependency mapping.
Because the number of sentences annotated withGRs is so small, the only other option would have been to guess at various DepBankanalyses, which would have made the the evaluation even more biased against the CCGparser.One advantage of this approach is that, by comparing the transformed gold-standard CCG dependencies with the gold-standard GRs, we can measure how closethe CCG representation is to the GRs.
This provides some indication of how difficult itis to perform the transformation, and also provides an upper bound on the accuracy ofthe parser on DepBank.
This method would be useful when converting the output ofthe Collins parser into an alternative representation (Kaplan et al 2004): Applying thetransformation to the gold-standard Penn Treebank trees and comparing with DepBankwould provide an upper bound on the performance of the Collins parser and give someindication of the effectiveness of the transformation.11.1 Mapping the CCG Dependencies to GRsTable 14 gives some examples of the mapping.
In our notation, %l indicates the wordassociated with the lexical category and %f is the head of the constituent filling theTable 14Examples of the CCG dependency to GRs mapping; $l denotes the word associated with thelexical category and $f is the filler.CCG lexical category arg slot GR(S[dcl]\NP1)/NP2 1 (ncsubj %l %f )(S[dcl]\NP1)/NP2 2 (dobj %l %f)(S\NP)/(S\NP)1 1 (ncmod %f %l)(NP\NP1)/NP2 1 (ncmod %f %l)(NP\NP1)/NP2 2 (dobj %l %f)NP[nb]/N1 1 (det %f %l)(NP\NP1)/(S[pss]\NP)2 1 (xmod %f %l)(NP\NP1)/(S[pss]\NP)2 2 (xcomp %l %f)((S\NP)\(S\NP)1)/S[dcl]2 1 (cmod %f %l)((S\NP)\(S\NP)1)/S[dcl]2 2 (ccomp %l %f)(S[dcl]\NP1)/(S[adj]\NP)2 1 (ncsubj %l %f )(S[dcl]\NP1)/(S[adj]\NP)2 2 (xcomp %l %f)((S[dcl]\NP1)/NP2)/NP3 1 (ncsubj %l %f )((S[dcl]\NP1)/NP2)/NP3 2 (obj2 %l %f)((S[dcl]\NP1)/NP2)/NP3 3 (dobj %l %f)(S[dcl]\NP1)/(S[b]\NP)2 2 (aux %f %l)533Computational Linguistics Volume 33, Number 4argument slot.
For many of the CCG dependencies, the mapping into GRs is straightfor-ward.
For example, the first two rows of Table 14 show the mapping for the transitiveverb category (S[dcl]\NP1)/NP2: Argument slot 1 is a non-clausal subject and argumentslot 2 is a direct object.
In the example Kim likes juicy oranges, likes is associated with thetransitive verb category, Kim is the subject, and oranges is the head of the constituentfilling the object slot, leading to the following GRs: (ncsubj likes Kim ) and (dobjlikes oranges).
The third row shows an example of a modifier: (S\NP)/(S\NP) modi-fies a verb phrase to the right.
Note that, in this example, the order of the lexical category(%l) and filler (%f) is switched compared to the previous example to match the DepBankannotation.There are a number of reasons why creating the dependency transformation ismore difficult than these examples suggest.
The first problem is that the mapping fromCCG dependencies to GRs is many-to-many.
For example, the transitive verb category(S[dcl]\NP)/NP applies to the copular in sentences like Imperial Corp. is the parent ofImperial Savings & Loan.
With the default annotation the relation between is and parentwould be dobj, whereas in DepBank the argument of the copular is analyzed as anxcomp.
Table 15 gives some examples of how we attempt to deal with this problem.The constraint in the first example means that, whenever the word associated with thetransitive verb category is a form of be, the second argument is xcomp, otherwise thedefault case applies (in this case dobj).
There are a number of categories with similarconstraints, checking whether the word associated with the category is a form of be.The second type of constraint, shown in the third line of the table, checks the lexicalcategory of the word filling the argument slot.
In this example, if the lexical category ofthe preposition is PP/NP, then the second argument of (S[dcl]\NP)/PP maps to iobj;thus in The loss stems from several factors the relation between the verb and prepositionis (iobj stems from).
If the lexical category of the preposition is PP/(S[ng]\NP),then the GR is xcomp; thus in The future depends on building cooperation the relationbetween the verb and preposition is (xcomp depends on).
There are a number ofCCG dependencies with similar constraints, many of them covering the iobj/xcompdistinction.The second difficulty in creating the transformation is that not all the GRs are binaryrelations, whereas the CCG dependencies are all binary.
The primary example of this isto-infinitival constructions.
For example, in the sentence The company wants to wean itselfaway from expensive gimmicks, the CCG parser produces two dependencies relating wants,to and wean, whereas there is only one GR: (xcomp to wants wean).
The final row ofTable 15Examples of the many-to-many nature of the CCG dependency to GRs mapping, and aterniary GR.CCG lexical category Slot GR Constraint Example(S[dcl]\NP1)/NP2 2 (xcomp %l %f) word=be The parent is Imperial(dobj %l %f) The parent sold Imperial(S[dcl]\NP1)/PP2 2 (iobj %l %f) cat=PP/NP The loss stems fromseveral factors(xcomp %l %f) cat=PP/(S[ng]\NP) The future depends onbuilding cooperation(S[dcl]\NP1)/ 2 (xcomp %f %l %k) cat=(S[to]\NP)/ wants to wean itself(S[to]\NP)2 (S[b]\NP) away from534Clark and Curran Wide-Coverage Efficient Statistical ParsingTable 15 gives an example.
We implement this constraint by introducing a %k variableinto the GR template which denotes the argument of the category in the constraintcolumn (which, as before, is the lexical category of the word filling the argument slot).
Inthe example, the current category is (S[dcl]\NP1)/(S[to]\NP)2, which is associated withwants; this combines with (S[to]\NP)/(S[b]\NP), associated with to; and the argument of(S[to]\NP)/(S[b]\NP) is wean.
The %k variable allows us to look beyond the argumentsof the current category when creating the GRs.A further difficulty in creating the transformation is that the head passing con-ventions differ between DepBank and CCGbank.
By ?head passing?
we mean themechanism which determines the heads of constituents and the mechanism by whichwords become arguments of long-range dependencies.
For example, in the sentenceThe group said it would consider withholding royalty payments, the DepBank and CCGbankannotations create a dependency between said and the following clause.
However, inDepBank the relation is between said and consider, whereas in CCGbank the relation isbetween said and would.
We fixed this problem by changing the head of would consider tobe consider rather than would.
In practice this means changing the annotation of all therelevant lexical categories in the markedup file.8 The majority of the categories to whichthis applies are those creating aux relations.A related difference between the two resources is that there are more subject re-lations in CCGbank than DepBank.
In the previous example, CCGbank has a subjectrelation between it and consider, and also it and would, whereas DepBank only hasthe relation between it and consider.
In practice this means ignoring a number of thesubject dependencies output by the CCG parser, which is implemented by annotatingthe relevant lexical categories plus argument slot in the markedup file with an ?ignore?marker.Another example where the dependencies differ in the two resources is the treat-ment of relative pronouns.
For example, in Sen. Mitchell, who had proposed the stream-lining, the subject of proposed is Mitchell in CCGbank but who in DepBank.
Again, weimplemented this change by fixing the head annotation in the lexical categories whichapply to relative pronouns.In summary, considerable changes were required to the markedup file in order tobring the dependency annotations of CCGbank and DepBank closer together.
The majortypes of changes have been described here, but not all the details.11.2 Post-Processing of the GR OutputDespite the considerable changes made to the parser output described in the previoussection, there were still significant differences between the GRs created from the CCGdependencies and the DepBank GRs.
To obtain some idea of whether the schemeswere converging, we performed the following oracle experiment.
We took the CCGderivations from CCGbank corresponding to the sentences in DepBank, and ran theparser over the gold-standard derivations, outputting the newly created GRs.9 Treatingthe DepBank GRs as a gold standard, and comparing these with the CCGbank GRs,8 The markedup file is the file containing the lexical categories, together with annotation which determinesdependency and head information, plus the CCG dependency to GR mapping.
Appendix B shows part ofthe markedup file.9 All GRs involving a punctuation mark were removed because the RASP evaluation script can only handletokens which appear in the gold-standard GRs for the sentence.535Computational Linguistics Volume 33, Number 4gave precision and recall scores of only 76.23% and 79.56%, respectively.
Thus given thecurrent mapping, the perfect CCGbank parser would achieve an F-score of only 77.86%when evaluated against DepBank.On inspecting the output, it was clear that a number of general rules could beapplied to bring the schemes closer together, which we implemented as a Python post-processing script.
We now provide a description of some of the major changes, to givean indication of the kinds of rules we implemented.
We tried to keep the changes asgeneral as possible and not specific to the test set, although some rules, such as thehandling of monetary amounts, are genre-specific.
We decided to include these rulesbecause they are trivial to implement and significantly affect the score, and we felt that,without these changes, the CCG parser would be unfairly penalized.The first set of changes deals with coordination.
One significant difference betweenDepBank and CCGbank is the treatment of coordinations as arguments.
Consider theexample The president and chief executive officer said the loss stems from several factors.
Inboth CCGbank and DepBank there are two conj GRs arising from the coordination:(conj and president) and (conj and officer).10 The difference arises in the subjectof said: in DepBank the subject is and: (ncsubj said and ), whereas in CCGbank thereare two subjects: (ncsubj said president ) and (ncsubj said officer ).
We dealwith this problem by replacing any pairs of GRs which differ only in their arguments,and where the arguments are coordinated items, with a single GR containing the coor-dination term as the argument.
Two arguments are coordinated if they appear in conjrelations with the same coordinating term, where ?same term?
is determined by boththe word and sentence position.Another source of conj errors is coordination terms acting as sentential modifiers,with category S/S, often at the beginning of a sentence.
These are labeled conj inDepBank, but the GR for S/S is ncmod.
So any ncmod whose modifier?s lexical categoryis S/S, and whose POS tag is CC, is changed to conj.Ampersands are also a significant problem, and occur frequently in WSJ text.
Forexample, the CCGbank analysis of Standard & Poor?s index assigns the lexical categoryN/N to both Standard and &, treating them as modifiers of Poor, whereas DepBank treats& as a coordinating term.
We fixed this by creating conj GRs between any & and the twowords on either side; removing the modifier GR between the two words; and replacingany GRs in which the words on either side of the & are arguments with a single GR inwhich & is the argument.The ta relation, which identifies text adjuncts delimited by punctuation (Briscoe2006), is difficult to assign correctly to the parser output.
The simple punctuation rulesused by the parser, and derived from CCGbank, do not contain enough information todistinguish between the various cases of ta.
Thus the only rule we have implemented,which is somewhat specific to the newspaper genre, is to replace GRs of the form(cmod say arg) with (ta quote arg say), where say can be any of say, said, or says.This rule applies to only a small subset of the ta cases but has high enough precision tobe worthy of inclusion.A common source of error is the distinction between iobj and ncmod, which is notsurprising given the difficulty that human annotators have in distinguishing argumentsand adjuncts.
There are many cases where an argument in DepBank is an adjunct inCCGbank, and vice versa.
The only change we have made is to turn all ncmod GRs with10 CCGbank does not contain GRs in this form, although we will continue to talk as though it does; these arethe GRs after the CCGbank dependencies have been put through the dependency to GRs mapping.536Clark and Curran Wide-Coverage Efficient Statistical Parsingof as the modifier into iobj GRs (unless the ncmod is a partitive predeterminer).
This wasfound to have high precision and applies to a significant number of cases.There are some dependencies in CCGbank which do not appear in DepBank.
Exam-ples include any dependencies in which a punctuation mark is one of the arguments,and so we removed these from the output of the parser.We have made some attempt to fill the subtype slot for some GRs.
The subtypeslot specifies additional information about the GR; examples include the value obj ina passive ncsubj, indicating that the subject is an underlying object; the value num inncmod, indicating a numerical quantity; and prt in ncmod to indicate a verb particle.The passive case is identified as follows: Any lexical category which starts S[pss]\NPindicates a passive verb, and we also mark any verbs POS tagged VBN and assignedthe lexical category N/N as passive.
Both these rules have high precision, but still leavemany of the cases in DepBank unidentified.
Many of those remaining are POS taggedJJ and assigned the lexical category N/N, but this is also true of many non-passivemodifiers, so we did not attempt to extend these rules further.
The numerical case isidentified using two rules: the num subtype is added if any argument in a GR is assignedthe lexical category N/N[num], and if any of the arguments in an ncmod is POS taggedCD.
prt is added to an ncmod if the modifiee has a POS tag beginning V and if themodifier has POS tag RP.We are not advocating that any of these post-processing rules should form partof a parser.
It would be preferable to have the required information in the treebankfrom which the grammar is extracted, so that it could be integrated into the parser in aprincipled way.
However, in order that the parser evaluation be as fair and informativeas possible, it is important that the parser output conform as closely to the gold standardas possible.
Thus it is appropriate to use any general transformation rules, as long asthey are simple and not specific to the test set, to achieve this.The final columns of Table 16 show the accuracy of the transformed gold-standardCCGbank dependencies when compared with DepBank; the simple post-processingrules have increased the F-score from 77.86% to 84.76%.
However, note that thisF-score provides an upper bound on the performance of the CCG parser, and that thisscore is still below the F-scores reported earlier when evaluating the parser outputagainst CCGbank.
Section 11.4 contains more discussion of this issue.11.3 ResultsThe results in Table 16 were obtained by parsing the sentences from CCGbank corre-sponding to those in the 560-sentence test set used by Briscoe, Carroll, and Watson(2006).
We used the CCGbank sentences because these differ in some ways from theoriginal Penn Treebank sentences (there are no quotation marks in CCGbank, for ex-ample) and the parser has been trained on CCGbank.
Even here we experienced someunexpected difficulties, because some of the tokenization is different between DepBankand CCGbank (even though both resources are based on the Penn Treebank), andthere are some sentences in DepBank which have been significantly shortened (for noapparent reason) compared to the original Penn Treebank sentences.
We modified theCCGbank sentences?and the CCGbank analyses because these were used for the oracleexperiments?to be as close to the DepBank sentences as possible.
All the results wereobtained using the RASP evaluation scripts, with the results for the RASP parser takenfrom Briscoe, Carroll, and Watson (2006).
The results for CCGbank were obtained usingthe oracle method described previously.537Computational Linguistics Volume 33, Number 4Table 16Accuracy on DepBank.RASP CCG parser CCGbankRelation Prec Rec F Prec Rec F Prec Rec F # GRsdependent 79.76 77.49 78.61 84.07 82.19 83.12 88.83 84.19 86.44 10,696aux 93.33 91.00 92.15 95.03 90.75 92.84 96.47 90.33 93.30 400conj 72.39 72.27 72.33 79.02 75.97 77.46 83.07 80.27 81.65 595ta 42.61 51.37 46.58 51.52 11.64 18.99 62.07 12.59 20.93 292det 87.73 90.48 89.09 95.23 94.97 95.10 97.27 94.09 95.66 1,114arg mod 79.18 75.47 77.28 81.46 81.76 81.61 86.75 84.19 85.45 8,295mod 74.43 67.78 70.95 71.30 77.23 74.14 77.83 79.65 78.73 3,908ncmod 75.72 69.94 72.72 73.36 78.96 76.05 78.88 80.64 79.75 3,550xmod 53.21 46.63 49.70 42.67 53.93 47.64 56.54 60.67 58.54 178cmod 45.95 30.36 36.56 51.34 57.14 54.08 64.77 69.09 66.86 168pmod 30.77 33.33 32.00 0.00 0.00 0.00 0.00 0.00 0.00 12arg 77.42 76.45 76.94 85.76 80.01 82.78 89.79 82.91 86.21 4,387subj or dobj 82.36 74.51 78.24 86.08 83.08 84.56 91.01 85.29 88.06 3,127subj 78.55 66.91 72.27 84.08 75.57 79.60 89.07 78.43 83.41 1,363ncsubj 79.16 67.06 72.61 83.89 75.78 79.63 88.86 78.51 83.37 1,354xsubj 33.33 28.57 30.77 0.00 0.00 0.00 50.00 28.57 36.36 7csubj 12.50 50.00 20.00 0.00 0.00 0.00 0.00 0.00 0.00 2comp 75.89 79.53 77.67 86.16 81.71 83.88 89.92 84.74 87.25 3,024obj 79.49 79.42 79.46 86.30 83.08 84.66 90.42 85.52 87.90 2,328dobj 83.63 79.08 81.29 87.01 88.44 87.71 92.11 90.32 91.21 1,764obj2 23.08 30.00 26.09 68.42 65.00 66.67 66.67 60.00 63.16 20iobj 70.77 76.10 73.34 83.22 65.63 73.38 83.59 69.81 76.08 544clausal 60.98 74.40 67.02 77.67 72.47 74.98 80.35 77.54 78.92 672xcomp 76.88 77.69 77.28 77.69 74.02 75.81 80.00 78.49 79.24 381ccomp 46.44 69.42 55.55 77.27 70.10 73.51 80.81 76.31 78.49 291pcomp 72.73 66.67 69.57 0.00 0.00 0.00 0.00 0.00 0.00 24macroaverage 62.12 63.77 62.94 65.71 62.29 63.95 71.73 65.85 68.67microaverage 77.66 74.98 76.29 81.95 80.35 81.14 86.86 82.75 84.76The CCG parser results are based on automatically assigned POS tags, using theCurran and Clark (2003) tagger.
For the parser we used the hybrid dependencymodel and the maximum recall decoder, because this obtained the highest accuracy onCCGbank, with the same parser and supertagger parameter settings as described inSection 10.2.11 The coverage of the parser on DepBank is 100%.
The coverage of theRASP parser is also 100%: 84% of the analyses are complete parses rooted in S and therest are obtained using a robustness technique based on fragmentary analyses (Briscoeand Carroll 2006).
The coverage for the oracle experiments is less than 100% (around95%) since there are some gold-standard derivations in CCGbank which the parser isunable to follow exactly, because the grammar rules used by the parser are a subsetof those in CCGbank.
The oracle figures are based only on those sentences for whichthere is a gold-standard analysis, because we wanted to measure how close the tworesources are and provide an approximate upper bound for the parser.
(But, to repeat,the accuracy figures for the parser are based on the complete test set.
)11 The results reported in Clark and Curran (2007) differ from those here because Clark and Curran usedthe normal-form model and Viterbi decoder.538Clark and Curran Wide-Coverage Efficient Statistical ParsingF-score is the balanced harmonic mean of precision (P) and recall (R): 2PR/(P + R).# GRs is the number of GRs in DepBank.
For a GR in the parser output to be correct,it has to match the gold-standard GR exactly, including any subtype slots; however, itis possible for a GR to be incorrect at one level but correct at a subsuming level.
Forexample, if an ncmod GR is incorrectly labeled with xmod, but is otherwise correct, it willbe correct for all levels which subsume both ncmod and xmod, for example mod.
Thus thescores at the most general level in the GR hierarchy (dependent) correspond to unlabeledaccuracy scores.
The micro-averaged scores are calculated by aggregating the counts forall the relations in the hierarchy, whereas the macro-averaged scores are the mean of theindividual scores for each relation (Briscoe, Carroll, and Watson 2006).The results show that the performance of the CCG parser is higher than RASPoverall, and also higher on the majority of GR types.
Relations on which the CCG parserperforms particularly well, relative to RASP, are conj, det, ncmod, cmod, ncsubj, dobj,obj2, and ccomp.
The relations for which the CCG parser performs poorly are some ofthe less frequent relations: ta, pmod, xsubj, csubj, and pcomp; in fact pmod and pcomp arenot in the current CCG dependencies to GRs mapping.
The overall F-score for the CCGparser, 81.14%, is only 3.6 points below that for CCGbank, which provides an upperbound for the CCG parser.Briscoe and Carroll (2006) give a rough comparison of RASP with the Parc LFG parser(Kaplan et al 2004) on DepBank, obtaining similar results overall, but acknowledgingthat the results are not strictly comparable because of the different annotation schemesused.11.4 DiscussionWe might expect the CCG parser to perform better than RASP on this data because RASPis not tuned to newspaper text and uses an unlexicalized parsing model.
On the otherhand the relatively low upper bound for the CCG parser on DepBank demonstrates theconsiderable disadvantage of evaluating on a resource which uses a different annotationscheme to the parser.
Our feeling is that the overall F-score on DepBank understates theaccuracy of the CCG parser, because of the information lost in the translation.One aspect of the CCGbank evaluation which is more demanding than the DepBankevaluation is the set of labeled dependencies used.
In CCGbank there are many morelabeled dependencies than GRs in DepBank, because a dependency is defined as a lexicalcategory-argument slot pair.
In CCGbank there is a distinction between the direct objectof a transitive verb and ditransitive verb, for example, whereas in DepBank these wouldboth be dobj.
In other words, to get a dependency correct in the CCGbank evaluation,the lexical category?typically a subcategorization frame?has to be correct.
In a finalexperiment we used the GRs generated by transforming CCGbank as a gold standard,against which we compared the GRs from the transformed parser output.
The resultingF-score of 89.60% shows the increase obtained from using gold-standard GRs generatedfrom CCGbank rather than the CCGbank dependencies themselves (for which theF-score was 85.20%).Another difference between DepBank and CCGbank is that DepBank has been man-ually corrected, whereas CCGbank, including the test sections, has been produced semi-automatically from the Penn Treebank.
There are some constructions in CCGbank?noun compounds being a prominent example?which are often incorrectly analyzed,simply because the required information is not in the Penn Treebank.
Thus the evalua-tion on CCGbank overstates the accuracy of the parser, because it is tuned to produce539Computational Linguistics Volume 33, Number 4the output in CCGbank, including constructions where the analysis is incorrect.
Asimilar comment would apply to other parsers evaluated on, and using grammarsextracted from, the Penn Treebank.A contribution of this section has been to highlight the difficulties associated withcross-formalism parser comparisons.
Note that the difficulties are not unique to CCG,and many would apply to any cross-formalism comparison, especially with parsersusing automatically extracted grammars.
Parser evaluation has improved on the origi-nal PARSEVAL measures (Carroll, Briscoe, and Sanfilippo 1998), but the challenge stillremains to develop a representation and evaluation suite which can be easily applied toa wide variety of parsers and formalisms.12.
Future WorkOne of the key questions currently facing researchers in statistical parsing is how toadapt existing parsers to new domains.
There is some experimental evidence showingthat, perhaps not surprisingly, the performance of parsers trained on the WSJ PennTreebank drops significantly when the parser is applied to domains outside of news-paper text (Gildea 2001; Lease and Charniak 2005).
The difficulty is that developingnew treebanks for each of these domains is infeasible.
Developing the techniques toextract a CCG grammar from the Penn Treebank, together with the preprocessing of thePenn Treebank which was required, took a number of years; and developing the PennTreebank itself also took a number of years.Clark, Steedman, and Curran (2004) applied the parser described in this article toquestions from the TREC Question Answering (QA) track.
Because of the small numberof questions in the Penn Treebank, the performance of the parser was extremely poor?well below that required for a working QA system.
The novel idea in Clark, Steedman,and Curran was to create new training data from questions, but to annotate at the lexicalcategory level only, rather than annotate with full derivations.
The idea is that, becauselexical categories contain so much syntactic information, adapting just the supertaggerto the new domain, by training on the new question data, may be enough to obtain goodparsing performance.
This technique assumes that annotation at the lexical categorylevel can be done relatively quickly, allowing rapid porting of the supertagger.
We wereable to annotate approximately 1, 000 questions in around a week, which led to anaccurate supertagger and, combined with the Penn Treebank parsing model, an accurateparser of questions.There are ways in which this porting technique can be extended.
For example, wehave developed a method for training the dependency model which requires lexicalcategory data only (Clark and Curran 2006).
Partial dependency structures are extractedfrom the lexical category sequences, and the training algorithm for the dependencymodel is extended to deal with partial data.
Remarkably, the accuracy of the depen-dency model trained on data derived from lexical category sequences alone is only 1.3%labeled F-score less than the full data model.
This result demonstrates the significantamount of syntactic information encoded in the lexical categories.
Future work will lookat applying this method to biomedical text.We have shown how using automatically assigned POS tags reduces the accuracyof the supertagger and parser.
In Curran, Clark, and Vadas (2006) we investigate usingthe multi-tagging techniques developed for the supertagger at the POS tag level.
Theidea is to maintain some POS tag ambiguity for later parts of the parsing process, usingthe tag probabilities to decide which tags to maintain.
We were able to reduce the drop540Clark and Curran Wide-Coverage Efficient Statistical Parsingin supertagger accuracy by roughly one half.
Future work will also look at maintaingthe POS tag ambiguity through to the parsing stage.Currently we do not use the probabilities assigned to the lexical categories by thesupertagger as part of the parse selection process.
These scores could be incorporatedas real-valued features, or as auxiliary functions, as in Johnson and Riezler (2000).We would also like to investigate using the generative model of Hockenmaier andSteedman (2002b) in a similar way.
Using a generative model?s score as a feature ina discriminative framework has been beneficial for reranking approaches (Collins andKoo 2005).
Because the generative model uses local features similar to those in ourlog-linear models, it could be incorporated into the estimation and decoding processeswithout the need for reranking.One way of improving the accuracy of a supertagger is to use the parser to providelarge amounts of additional training data, by taking the lexical categories chosen by theparser as gold-standard training data.
If enough unlabeled data is parsed, then the largevolume can overcome the noise in the data (Steedman et al 2002; Prins and van Noord2003).
We plan to investigate this idea in the context of our own parsing system.13.
ConclusionThis article has shown how to estimate a log-linear parsing model for an automat-ically extracted CCG grammar, on a very large scale.
The techniques that we havedeveloped, including the use of a supertagger to limit the size of the charts and theuse of parallel estimation, could be applied to log-linear parsing models using othergrammar formalisms.
Despite memory requirements of up to 25 GB we have shownhow a parallelized version of the estimation process can limit the estimation time tounder three hours, resulting in a practical framework for parser development.
One ofthe problems with modeling approaches which require very long estimation times isthat it is difficult to test different configurations of the system, for example differentfeature sets.
It may also not be possible to train or run the system on anything otherthan short sentences (Taskar et al 2004).The supertagger is a key component in our parsing system.
It reduces the size ofthe charts considerably compared with naive methods for assigning lexical categories,which is crucial for practical discriminative training.
The tight integration of the su-pertagger and parser enables highly efficient as well as accurate parsing.
The parser issignificantly faster than comparable parsers in the NLP literature.
The supertagger wehave developed can be applied to other lexicalized grammar formalisms.Another contribution of the article is the development of log-linear parsing modelsfor CCG.
In particular, we have shown how to define a CCG parsing model whichexploits all derivations, including nonstandard derivations.
These nonstandard deriva-tions are an integral part of the formalism, and we have answered the question ofwhether efficent estimation and parsing algorithms can be defined for models whichuse these derivations.
We have also defined a new parsing algorithm for CCG whichmaximizes expected recall of predicate?argument dependencies.
This algorithm, whencombined with normal-form constraints, gives the highest parsing accuracy to date onCCGbank.
We have also given competitive results on DepBank, outperforming a non-CCG parser (RASP), despite the considerable difficulties involved in evaluating on a goldstandard which uses a different annotation scheme to the parser.There has perhaps been a perception in the NLP community that parsing withCCG is necessarily ineffficient because of CCG?s ?spurious?
ambiguity.
We have541Computational Linguistics Volume 33, Number 4demonstrated, using state-of-the-art statistical models, that both accurate and highlyefficient parsing is practical with CCG.
Linguistically motivated grammars can now beused for large-scale NLP applications.12Appendix AThe following rules were selected primarily on the basis of frequency of occurrence inSections 02?21 of CCGbank.Type-Raising Categories for NP, PP and S[adj]\NPS/(S\NP)(S\NP)\((S\NP)/NP)((S\NP)/NP)\(((S\NP)/NP)/NP)((S\NP)/(S[to]\NP))\(((S\NP)/(S[to]\NP))/NP)((S\NP)/PP)\(((S\NP)/PP)/NP)((S\NP)/(S[adj]\NP))\(((S\NP)/(S[adj]\NP))/NP)(S\NP)\((S\NP)/PP)(S\NP)\((S\NP)/(S[adj]\NP))Unary Type-Changing RulesThe category on the left of the rule is rewritten bottom-up as the category on the right.N ?
NPNP ?
S/(S/NP)NP ?
NP/(NP\NP)S[dcl]\NP ?
NP\NPS[pss]\NP ?
NP\NPS[ng]\NP ?
NP\NPS[adj]\NP ?
NP\NPS[to]\NP ?
NP\NP(S[to]\NP)/NP ?
NP\NPS[dcl]/NP ?
NP\NPS[dcl] ?
NP\NPS[pss]\NP ?
(S\NP)\(S\NP)12 The parser and supertagger, including source code, are freely available via the authors?
Web pages.542Clark and Curran Wide-Coverage Efficient Statistical ParsingS[ng]\NP ?
(S\NP)\(S\NP)S[adj]\NP ?
(S\NP)\(S\NP)S[to]\NP ?
(S\NP)\(S\NP)S[ng]\NP ?
(S\NP)/(S\NP)S[pss]\NP ?
S/SS[ng]\NP ?
S/SS[adj]\NP ?
S/SS[to]\NP ?
S/SS[ng]\NP ?
S\SS[dcl] ?
S\SS[ng]\NP ?
NPS[to]\NP ?
N\NPunctuation RulesA number of categories absorb a comma to the left, implementing the following schema:, X ?
XThe categories are as follows, where S[?]
matches an S category with any or no feature:N, NP, S[?
], N/N, NP\NP, PP\PP, S/S, S\S, S[?
]\NP, (S\NP)\(S\NP),(S\NP)/(S\NP), ((S\NP)\(S\NP))\((S\NP)\(S\NP))Similarly, a number of categories absorb a comma to the right, implementing the fol-lowing schema:X , ?
XThe categories are as follows:N, NP, PP, S[dcl], N/N, NP\NP, S/S, S\S, S[?
]\NP, (S[dcl]\NP)/S,(S[dcl]\S[dcl])\NP, (S[dcl]\NP)/NP, (S[dcl]\NP)/PP, (NP\NP)/(S[dcl]\NP),(S\NP)\(S\NP), (S\NP)/(S\NP)These are the categories which absorb a colon or semicolon to the left, in the same wayas for the comma:N, NP, S[dcl], NP\NP, S[?
]\NP, (S\NP)\(S\NP)These are the categories which absorb a colon or semicolon to the right:N, NP, PP, S[dcl], NP\NP, S/S, S[?
]\NP, (S[dcl]\NP)/S[dcl], (S\NP)\(S\NP),(S\NP)/(S\NP)543Computational Linguistics Volume 33, Number 4These are the categories which absorb a period to the right:N, NP, S[?
], PP, NP\NP, S\S, S[?
]\NP, S[?
]\PP, (S[dcl]\S[?
])\NP, (S\NP)\(S\NP)These are the categories which absorb a round bracket to the left:N, NP, S[dcl], NP\NP, (S\NP)\(S\NP)These are the categories which absorb a round bracket to the right:N, NP, S[dcl], N\N, N/N, NP\NP, S[dcl]\NP, S/S, S\S, (N/N)\(N/N),(S\NP)\(S\NP), (S\NP)/(S\NP)There are some binary type-changing rules involving commas, where the two categorieson the left are rewritten bottom-up as the category on the right:, NP ?
(S\NP)\(S\NP)NP , ?
S/SS[dcl]/S[dcl] , ?
S/SS[dcl]/S[dcl] , ?
(S\NP)\(S\NP)S[dcl]/S[dcl] , ?
(S\NP)/(S\NP)S[dcl]/S[dcl] , ?
S\SS[dcl]\S[dcl] , ?
S/SFinally, there is a comma coordination rule, and a semicolon coordination rule, repre-sented by the following two schema:, X ?
X\X; X ?
X\XThe categories which instantiate the comma schema are as follows:N, NP, S[?
], N/N, NP\NP, S[?
]\NP, (S\NP)\(S\NP)The categories which instantiate the semicolon schema are as follows:NP, S[?
], S[?
]\NP544Clark and Curran Wide-Coverage Efficient Statistical ParsingOther RulesThere are two rules for combining sequences of noun phrases and sequences of declar-ative sentences:NP NP ?
NPS[dcl] S[dcl] ?
S[dcl]Finally, there are some coordination constructions in the original Penn Treebank whichwere difficult to convert into CCGbank analyses, for which the following rule is used:conj N ?
NAppendix BThe annotation in the markedup file for some of the most frequent categories in CCG-bank is shown in Section 11.1.
The annotation provides information about heads anddependencies, and also the mapping from CCG dependencies to the GRs in DepBank.The first line after the unmarked lexical category gives the number of dependencyrelations plus the category annotated with head and dependency information.
Variablesin curly brackets indicate heads, with ?
?
used to denote the word associated with thelexical category.
For example, if the word buys is assigned the transitive verb category((S[dcl]{ }\NP{Y}?1?
){ }/NP{Z}?2?
){ }, then the head on the resulting S[dcl] is buys.Co-indexing of variables allows head passing; for example, in the relative pronoun cat-egory ((NP{Y}\NP{Y}?1?
){ }/(S[dcl]{Z}?2?\NP{Y?
}){Z}){ }, the head of the resultingNP is taken from the NP which is modified to the left, and this head also becomes thesubject of the verb phrase to the right.
So in the man who owns the company, the subject ofowns is man.Numbers in angled brackets indicate dependency relations.
For example, in thenominal modifier category (N{Y}/N{Y}?1?
){ }, there is one dependency between themodifier and the modifiee.
Long-range dependencies are indicated by marking headvariables with ?.
The ?
in the relative pronoun category indicates that when the Y vari-able unifies with a lexical item, this creates a long-range subject dependency.Some categories have a second head and dependency annotation, indicated with a!.
This is used to produce the DepBank GRs.
For example, the relative pronoun categoryhas a second annotation which results in who being the subject of owns in the man whoowns the company, rather than man, because this is consistent with DepBank.
The firstannotation is consistent with CCGbank.The remaining lines in a category entry give the CCG dependencies to GRs mapping,described in Section 11.1.N/N1 (N{Y}/N{Y}<1>){_}1 ncmod _ %f %lNP[nb]/N1 (NP[nb]{Y}/N{Y}<1>){_}1 det %f %l545Computational Linguistics Volume 33, Number 4(NP\NP)/NP2 ((NP{Y}\NP{Y}<1>){_}/NP{Z}<2>){_}1 ncmod _ %f %l2 dobj %l %f((S\NP)\(S\NP))/NP2 (((S[X]{Y}\NP{Z}){Y}\(S[X]{Y}<1>\NP{Z}){Y}){_}/NP{W}<2>){_}1 ncmod _ %f %l2 dobj %l %fPP/NP1 (PP{_}/NP{Y}<1>){_}1 dobj %l %f(S[dcl]\NP)/NP2 ((S[dcl]{_}\NP{Y}<1>){_}/NP{Z}<2>){_}1 ncsubj %l %f _2 xcomp _ %l %f =be2 dobj %l %f(S\NP)\(S\NP)1 ((S[X]{Y}\NP{Z}){Y}\(S[X]{Y}<1>\NP{Z}){Y}){_}1 ncmod _ %f %l(S[b]\NP)/NP2 ((S[b]{_}\NP{Y}<1>){_}/NP{Z}<2>){_}1 ncsubj %l %f _2 xcomp _ %l %f =be2 dobj %l %f(S[to]\NP)/(S[b]\NP)2 ((S[to]{_}\NP{Z}<1>){_}/(S[b]{Y}<2>\NP{Z*}){Y}){_}1 ignore2 ignore(S[dcl]\NP)/(S[b]\NP)2 ((S[dcl]{_}\NP{Y}<1>){_}/(S[b]{Z}<2>\NP{Y*}){Z}){_}!
((S[dcl]{Z}\NP{Y}<1>){Z}/(S[b]{Z}<2>\NP{Y*}){Z}){_}1 ignore =aux1 ncsubj %l %f _2 aux %f %l =aux2 xcomp _ %l %f(NP[nb]/N)\NP2 ((NP[nb]{Y}/N{Y}<1>){_}\NP{Z}<2>){_}1 ncmod poss %f %22 ignoreS[adj]\NP1 (S[adj]{_}\NP{Y}<1>){_}1 ignoreS[pss]\NP1 (S[pss]{_}\NP{Y}<1>){_}1 ncsubj %l %f obj(N/N)/(N/N)1 ((N{Y}/N{Y}){Z}/(N{Y}/N{Y}){Z}<1>){_}546Clark and Curran Wide-Coverage Efficient Statistical Parsing1 ncmod _ %f %l(S[ng]\NP)/NP2 ((S[ng]{_}\NP{Y}<1>){_}/NP{Z}<2>){_}1 ncsubj %l %f _2 dobj %l %f(S\NP)/(S\NP)1 ((S[X]{Y}\NP{Z}){Y}/(S[X]{Y}<1>\NP{Z}){Y}){_}1 ncmod _ %f %l(S[dcl]\NP)/S[dcl]2 ((S[dcl]{_}\NP{Y}<1>){_}/S[dcl]{Z}<2>){_}1 ncsubj %l %f _2 ccomp _ %l %fS[dcl]\NP1 (S[dcl]{_}\NP{Y}<1>){_}1 ncsubj %l %f _(S[dcl]\NP)/(S[pt]\NP)2 ((S[dcl]{_}\NP{Y}<1>){_}/(S[pt]{Z}<2>\NP{Y*}){Z}){_}!
((S[dcl]{Z}\NP{Y}<1>){Z}/(S[pt]{Z}<2>\NP{Y*}){Z}){_}1 ignore =aux1 ncsubj %l %f _2 aux %f %l =aux2 xcomp _ %l %fS/S1 (S[X]{Y}/S[X]{Y}<1>){_}1 ncmod _ %f %l(NP\NP)/(S[dcl]\NP)2 ((NP{Y}\NP{Y}<1>){_}/(S[dcl]{Z}<2>\NP{Y*}){Z}){_}!
((NP{Y}\NP{Y}<1>){_}/(S[dcl]{Z}<2>\NP{_}){Z}){_}1 cmod %l %f %22 ignoreAcknowledgmentsMuch of this work was carried out atthe University of Edinburgh?s School ofInformatics as part of Mark Steedman?sEPSRC grant GR/M96889.
We are immenselygrateful to Mark for his guidance and adviceduring that time, and for allowing us thefreedom to pursue the approach taken inthis article.
We are also grateful to JuliaHockenmaier for the use of CCGbank, theresource which made this work possible.Many of the ideas in this article emerged outof discussions with Mark and Julia.
JasonBaldridge, Johan Bos, David Chiang, ClaireGrover, Frank Keller, Yuval Krymolowski,Alex Lascarides, Mirella Lapata, YusukeMiyao, Miles Osborne, Stefan Riezler, andBonnie Webber have all given usefulfeedback.
Thanks to Ted Briscoe, RebeccaWatson, and Stephen Pulman for helpwith the GRs, and thanks to Rebecca forcarrying out the GR evaluation.
We havealso benefited from reviewers?
commentson this article and various conferencepapers related to this work, and from thefeedback of audiences at the universities ofCambridge, Edinburgh, Johns Hopkins,Oxford, Sheffield, and Sussex.
While atEdinburgh, James Curran was fundedby a Commonwealth scholarship and aUniversity of Sydney traveling scholarship.He is currently supported by the AustralianResearch Council under Discovery ProjectDP0453131.ReferencesAbney, Steven.
1997.
Stochastic attribute-value grammars.
Computational Linguistics,23(4):597?618.Baldridge, Jason.
2002.
Lexically SpecifiedDerivational Control in CombinatoryCategorial Grammar.
Ph.D. thesis,Edinburgh University, Edinburgh, UK.547Computational Linguistics Volume 33, Number 4Baldridge, Jason and Geert-Jan Kruijff.
2003.Multi-modal Combinatory CategorialGrammar.
In Proceedings of the 10th Meetingof the EACL, pages 211?218, Budapest,Hungary.Bangalore, Srinivas and Aravind Joshi.
1999.Supertagging: An approach to almost parsing.Computational Linguistics, 25(2):237?265.Bar-Hillel, Yehoshua.
1953.
Aquasi-arithmetical notation for syntacticdescription.
Language, 29:47?58.Borthwick, Andrew.
1999.
A MaximumEntropy Approach to Named EntityRecognition.
Ph.D. thesis, New YorkUniversity, New York.Bos, Johan, Stephen Clark, Mark Steedman,James R. Curran, and Julia Hockenmaier.2004.
Wide-coverage semanticrepresentations from a CCG parser.In Proceedings of COLING-04,pages 1240?1246, Geneva, Switzerland.Briscoe, Ted.
2006.
An introduction to tagsequence grammars and the RASPsystem parser.
Technical ReportUCAM-CL-TR-662, University ofCambridge Computer Laboratory.Briscoe, Ted and John Carroll.
2002.
Robustaccurate statistical annotation of generaltext.
In Proceedings of the 3rd LRECConference, pages 1499?1504, Las Palmas,Gran Canaria.Briscoe, Ted and John Carroll.
2006.Evaluating the accuracy of anunlexicalized statistical parser on thePARC DepBank.
In Proceedings of thePoster Session of the Joint Conferenceof the International Committee onComputational Linguistics and theAssociation for Computational Linguistics(COLING/ACL-06), Sydney, Australia.Briscoe, Ted, John Carroll, and RebeccaWatson.
2006.
The second release ofthe RASP system.
In Proceedings ofthe Interactive Demo Session of theJoint Conference of the InternationalCommittee on Computational Linguisticsand the Association for ComputationalLinguistics (COLING/ACL-06), pages 77?80,Sydney, Australia.Buchholz, Sabine, Jorn Veenstra, and WalterDaelemans.
1999.
Cascaded grammaticalrelation assignment.
In Proceedings ofEMNLP/VLC-99, pages 239?246,University of Maryland, College Park, MD.Burke, Michael, Aoife Cahill, RuthO?Donovan, Josef van Genabith, andAndy Way.
2004.
Large-scale inductionand evaluation of lexical resources fromthe Penn-II Treebank.
In Proceedings of the42nd Meeting of the ACL, pages 367?374,Barcelona, Spain.Cahill, Aoife, Michael Burke, RuthO?Donovan, Josef van Genabith,and Andy Way.
2004.
Long-distancedependency resolution in automaticallyacquired wide-coverage PCFG-based LFGapproximations.
In Proceedings of the 42ndMeeting of the ACL, pages 320?327,Barcelona, Spain.Carroll, John, Ted Briscoe, and AntonioSanfilippo.
1998.
Parser evaluation: Asurvey and a new proposal.
In Proceedingsof the 1st LREC Conference, pages 447?454,Granada, Spain.Charniak, Eugene.
1997.
Statistical parsingwith a context-free grammar and wordstatistics.
In Proceedings of the 14th NationalConference on Artificial Intelligence,pages 598?603, Menlo Park, CA.Charniak, Eugene.
2000.
Amaximum-entropy-inspired parser.
InProceedings of the 1st Meeting of the NAACL,pages 132?139, Seattle, WA.Chen, John, Srinivas Bangalore, MichaelCollins, and Owen Rambow.
2002.Reranking an N-gram supertagger.In Proceedings of the TAG+ Workshop,pages 259?268, Venice, Italy.Chen, John and K. Vijay-Shanker.
2000.Automated extraction of TAGS from thePenn Treebank.
In Proceedings of IWPT2000, Trento, Italy.Chen, Stanley and Ronald Rosenfeld.
1999.A Gaussian prior for smoothing maximumentropy models.
Technical ReportCMU-CS-99-108, Carnegie MellonUniversity, Pittsburgh, PA.Chiang, David.
2000.
Statistical parsingwith an automatically-extracted TreeAdjoining Grammar.
In Proceedings of the38th Meeting of the ACL, pages 456?463,Hong Kong.Chiang, David.
2003.
Mildly contextsensitive grammars for estimatingmaximum entropy models.
In Proceedingsof the 8th Conference on Formal Grammar,Vienna, Austria.Clark, Stephen.
2002.
A supertagger forCombinatory Categorial Grammar.In Proceedings of the TAG+ Workshop,pages 19?24, Venice, Italy.Clark, Stephen and James R. Curran.
2003.Log-linear models for wide-coverage CCGparsing.
In Proceedings of the EMNLPConference, pages 97?104, Sapporo, Japan.Clark, Stephen and James R. Curran.2004a.
The importance of supertaggingfor wide-coverage CCG parsing.
In548Clark and Curran Wide-Coverage Efficient Statistical ParsingProceedings of COLING-04, pages 282?288,Geneva, Switzerland.Clark, Stephen and James R. Curran.
2004b.Parsing the WSJ using CCG and log-linearmodels.
In Proceedings of the 42nd Meeting ofthe ACL, pages 104?111, Barcelona, Spain.Clark, Stephen and James R. Curran.
2006.Partial training for a lexicalized-grammarparser.
In Proceedings of the HumanLanguage Technology Conference andthe Annual Meeting of the NorthAmerican Chapter of the Association forComputational Linguistics (HLT-NAACL?06),pages 144?151, New York.Clark, Stephen and James R. Curran.
2007.Formalism-independent parser evaluationwith CCG and DepBank.
In Proceedingsof the 45th Meeting of the ACL, Prague,Czech Republic.Clark, Stephen, Julia Hockenmaier,and Mark Steedman.
2002.
Buildingdeep dependency structures with awide-coverage CCG parser.
In Proceedingsof the 40th Meeting of the ACL,pages 327?334, Philadelphia, PA.Clark, Stephen, Mark Steedman, andJames R. Curran.
2004.
Object-extractionand question-parsing using CCG.
InProceedings of the EMNLP Conference,pages 111?118, Barcelona, Spain.Collins, Michael.
1996.
A new statisticalparser based on bigram lexicaldependencies.
In Proceedings of the34th Meeting of the ACL, pages 184?191,Santa Cruz, CA.Collins, Michael.
2003.
Head-drivenstatistical models for natural languageparsing.
Computational Linguistics,29(4):589?637.Collins, Michael and James Brooks.
1995.Prepositional phrase attachment througha backed-off model.
In Proceedings of the3rd Workshop on Very Large Corpora,pages 27?38, Cambridge, MA.Collins, Michael and Terry Koo.
2005.Discriminative reranking for naturallanguage parsing.
ComputationalLinguistics, 31(1):25?69.Collins, Michael and Brian Roark.
2004.Incremental parsing with the perceptronalgorithm.
In Proceedings of the 42ndMeeting of the ACL, pages 111?118,Barcelona, Spain.Crouch, Richard, Ronald M. Kaplan,Tracy H. King, and Stefan Riezler.2002.
A comparison of evaluationmetrics for a broad-coverage stochasticparser.
In Proceedings of the LREC BeyondPARSEVAL workshop, pages 67?74,Las Palmas, Spain.Curran, James R. and Stephen Clark.2003.
Investigating GIS and smoothingfor maximum entropy taggers.
InProceedings of the 10th Meeting of theEACL, pages 91?98, Budapest, Hungary.Curran, James R., Stephen Clark, andDavid Vadas.
2006.
Multi-tagging forlexicalized-grammar parsing.
InProceedings of the Joint Conferenceof the International Committee onComputational Linguistics and theAssociation for Computational Linguistics(COLING/ACL-06), pages 697?704,Sydney, Australia.Curry, Haskell B. and Robert Feys.
1958.Combinatory Logic: Vol.
I. Amsterdam,The Netherlands.Darroch, J. N. and D. Ratcliff.
1972.Generalized iterative scaling for log-linearmodels.
The Annals of MathematicalStatistics, 43(5):1470?1480.Della Pietra, Stephen, Vincent Della Pietra,and John Lafferty.
1997.
Inducing featuresof random fields.
IEEE Transactions PatternAnalysis and Machine Intelligence,19(4):380?393.Dienes, Peter and Amit Dubey.
2003.
Deepsyntactic processing by combining shallowmethods.
In Proceedings of the 41st Meetingof the ACL, pages 431?438, Sapporo, Japan.Eisner, Jason.
1996.
Efficient normal-formparsing for Combinatory CategorialGrammar.
In Proceedings of the 34th Meetingof the ACL, pages 79?86, Santa Cruz, CA.Geman, Stuart and Mark Johnson.
2002.Dynamic programming for parsing andestimation of stochastic unification-basedgrammars.
In Proceedings of the 40thMeeting of the ACL, pages 279?286,Philadelphia, PA.Gildea, Daniel.
2001.
Corpus variation andparser performance.
In 2001 Conference onEmpirical Methods in Natural LanguageProcessing (EMNLP), pages 167?172,Pittsburgh, PA.Goodman, Joshua.
1996.
Parsing algorithmsand metrics.
In Proceedings of the 34thMeeting of the ACL, pages 177?183,Santa Cruz, CA.Goodman, Joshua.
1997.
Probabilisticfeature grammars.
In Proceedings of theInternational Workshop on ParsingTechnologies, Cambridge, MA.Gropp, W., E. Lusk, N. Doss, andA.
Skjellum.
1996.
A high-performance,portable implementation of the MPImessage passing interface standard.Parallel Computing, 22(6):789?828.549Computational Linguistics Volume 33, Number 4Hockenmaier, Julia.
2003a.
Data and Modelsfor Statistical Parsing with CombinatoryCategorial Grammar.
Ph.D. thesis,University of Edinburgh, Edinburgh, UK.Hockenmaier, Julia.
2003b.
Parsing withgenerative models of predicate-argumentstructure.
In Proceedings of the 41stMeeting of the ACL, pages 359?366,Sapporo, Japan.Hockenmaier, Julia and Mark Steedman.2002a.
Acquiring compact lexicalizedgrammars from a cleaner treebank.
InProceedings of the Third LREC Conference,pages 1974?1981, Las Palmas, Spain.Hockenmaier, Julia and Mark Steedman.2002b.
Generative models for statisticalparsing with Combinatory CategorialGrammar.
In Proceedings of the40th Meeting of the ACL, pages 335?342,Philadelphia, PA.Johnson, Mark.
1998.
PCFG models oflinguistic tree representations.Computational Linguistics, 24(4):613?632.Johnson, Mark.
2002.
A simplepattern-matching algorithm for recoveringempty nodes and their antecedents.
InProceedings of the 40th Meeting of the ACL,pages 136?143, Philadelphia, PA.Johnson, Mark, Stuart Geman, StephenCanon, Zhiyi Chi, and Stefan Riezler.1999.
Estimators for stochastic?unification-based?
grammars.
InProceedings of the 37th Meeting of theACL, pages 535?541, University ofMaryland, College Park, MD.Johnson, Mark and Stefan Riezler.
2000.Exploiting auxiliary distributions instochastic unification-based grammars.In Proceedings of the 1st Meeting of theNAACL, Seattle, WA.Kaplan, Ron, Stefan Riezler, Tracy H. King,John T. Maxwell, III, AlexanderVasserman, and Richard Crouch.
2004.Speed and accuracy in shallow and deepstochastic parsing.
In Proceedings of theHuman Language Technology Conference andthe 4th Meeting of the North AmericanChapter of the Association for ComputationalLinguistics (HLT-NAACL?04), pages 97?104,Boston, MA.Kasami, J.
1965.
An efficient recognitionand syntax analysis algorithm forcontext-free languages.
TechnicalReport AFCRL-65-758, Air ForceCambridge Research Laboratory,Bedford, MA.King, Tracy H., Richard Crouch, StefanRiezler, Mary Dalrymple, and Ronald M.Kaplan.
2003.
The PARC 700 DependencyBank.
In Proceedings of the 4th InternationalWorkshop on Linguistically InterpretedCorpora, Budapest, Hungary.Koeling, Rob.
2000.
Chunking withmaximum entropy models.
In Proceedingsof the CoNLL Workshop 2000, pages 139?141,Lisbon, Portugal.Lafferty, John, Andrew McCallum, andFernando Pereira.
2001.
Conditionalrandom fields: Probabilistic models forsegmenting and labeling sequence data.In Proceedings of the 18th InternationalConference on Machine Learning,pages 282?289, Williams College,Williamstown, MA.Lari, K. and S. J.
Young.
1990.
The estimationof stochastic context-free grammars usingthe inside-outside algorithm.
ComputerSpeech and Language, 4(1):35?56.Lease, Matthew and Eugene Charniak.2005.
Parsing biomedical literature.
InProceedings of the Second International JointConference on Natural Language Processing(IJCNLP-05), Jeju Island, Korea.Levy, Roger and Christopher Manning.2004.
Deep dependencies fromcontext-free statistical parsers: Correctingthe surface dependency approximation.In Proceedings of the 41st Meeting of theACL, pages 328?335, Barcelona, Spain.Malouf, Robert.
2002.
A comparison ofalgorithms for maximum entropyparameter estimation.
In Proceedingsof the Sixth Workshop on NaturalLanguage Learning, pages 49?55,Taipei, Taiwan.Malouf, Robert and Gertjan van Noord.2004.
Wide coverage parsing withstochastic attribute value grammars.In Proceedings of the IJCNLP-04 Workshop:Beyond Shallow Analyses?Formalisms andStatistical Modeling for Deep Analyses,Hainan Island, China.Marcus, Mitchell, Beatrice Santorini, andMary Marcinkiewicz.
1993.
Building alarge annotated corpus of English: ThePenn Treebank.
Computational Linguistics,19(2):313?330.Miyao, Yusuke, Takashi Ninomiya, andJun?ichi Tsujii.
2004.
Corpus-orientedgrammar development for acquiring ahead-driven phrase structure grammarfrom the Penn Treebank.
In Proceedings ofthe First International Joint Conference onNatural Language Processing (IJCNLP-04),pages 684?693, Hainan Island, China.Miyao, Yusuke and Jun?ichi Tsujii.
2002.Maximum entropy estimation forfeature forests.
In Proceedings of the550Clark and Curran Wide-Coverage Efficient Statistical ParsingHuman Language Technology Conference,San Diego, CA.Miyao, Yusuke and Jun?ichi Tsujii.
2003a.
Amodel of syntactic disambiguation basedon lexicalized grammars.
In Proceedings ofthe Seventh Conference on Natural LanguageLearning (CoNLL), pages 1?8, Edmonton,Canada.Miyao, Yusuke and Jun?ichi Tsujii.
2003b.Probabilistic modeling of argumentstructures including non-localdependencies.
In Proceedings of theConference on Recent Advances inNatural Language Processing (RANLP),pages 285?291, Borovets, Bulgaria.Miyao, Yusuke and Jun?ichi Tsujii.
2004.Deep linguistic analysis for the accurateidentification of predicate-argumentrelations.
In Proceedings of COLING-2004,pages 1392?1397, Geneva, Switzerland.Miyao, Yusuke and Jun?ichi Tsujii.
2005.Probabilistic disambiguation modelsfor wide-coverage HPSG parsing.
InProceedings of the 43rd meeting of the ACL,pages 83?90, University of Michigan,Ann Arbor.Moortgat, Michael.
1997.
Categorial typelogics.
In Johan van Benthem andAlice ter Meulen, editors, Handbook ofLogic and Language.
Elsevier, Amsterdam,and The MIT Press, Cambridge, MA,pages 93?177.Nasr, Alexis and Owen Rambow.
2004.Supertagging and full parsing.
InProceedings of the TAG+7 Workshop,Vancouver, Canada.Nocedal, Jorge and Stephen J. Wright.1999.
Numerical Optimization.
Springer,New York, NY.Osborne, Miles.
2000.
Estimation ofstochastic attribute-value grammarsusing an informative sample.
InProceedings of the 18th InternationalConference on Computational Linguistics,pages 586?592, Saarbru?cken, Germany.Osborne, Miles and Ted Briscoe.
1997.Learning stochastic categorial grammars.In Proceedings of the ACL CoNLL Workshop,pages 80?87, Madrid, Spain.Palmer, Martha, Dan Gildea, and PaulKingsbury.
2005.
The Proposition Bank:A corpus annotated with semantic roles.Computational Linguistics, 31(1):71?105.Preiss, Judita.
2003.
Using grammaticalrelations to compare parsers.
In Proceedingsof the 10th Meeting of the EACL,pages 291?298, Budapest, Hungary.Prins, Robbert and Gertjan van Noord.
2003.Reinforcing parser preferences throughtagging.
Traitement Automatique desLangues, 44(3):121?139.Ratnaparkhi, Adwait.
1996.
A maximumentropy part-of-speech tagger.
InProceedings of the EMNLP Conference,pages 133?142, Philadelphia, PA.Ratnaparkhi, Adwait.
1998.
MaximumEntropy Models for Natural LanguageAmbiguity Resolution.
Ph.D. thesis,University of Pennsylvania.Ratnaparkhi, Adwait.
1999.
Learning toparse natural language with maximumentropy models.
Machine Learning,34(1?3):151?175.Ratnaparkhi, Adwait, Salim Roukos, andTodd Ward.
1994.
A maximum entropymodel for parsing.
In Proceedings ofthe International Conference on SpokenLanguage Processing, pages 803?806,Yokohama, Japan.Riezler, Stefan, Tracy H. King, Ronald M.Kaplan, Richard Crouch, John T. Maxwell,III, and Mark Johnson.
2002.
Parsing theWall Street Journal using a Lexical-Functional Grammar and discriminativeestimation techniques.
In Proceedings of the40th Meeting of the ACL, pages 271?278,Philadelphia, PA.Sagae, Kenji and Alon Lavie.
2005.A classifier-based parser with linearrun-time complexity.
In Proceedings of the9th International Workshop on ParsingTechnologies, pages 125?132, Vancouver,Canada.Sarkar, Anoop and Aravind Joshi.
2003.Tree-adjoining grammars and itsapplication to statistical parsing.
In RensBod, Remko Scha, and Khalil Sima?an,editors, Data-oriented parsing.
CSLIPublications, Stanford, CA.Sha, Fei and Fernando Pereira.
2003.Shallow parsing with conditional randomfields.
In Proceedings of the HLT/NAACLconference, pages 213?220, Edmonton,Canada.Steedman, Mark.
1996.
Surface Structureand Interpretation.
The MIT Press,Cambridge, MA.Steedman, Mark.
2000.
The Syntactic Process.The MIT Press, Cambridge, MA.Steedman, Mark, Steven Baker, StephenClark, Jeremiah Crim, Julia Hockenmaier,Rebecca Hwa, Miles Osborne, PaulRuhlen, and Anoop Sarkar.
2002.Semi-supervised training for statisticalparsing: Final report.
Technical ReportCLSP WS-02, Center for Language andSpeech Processing, Johns HopkinsUniversity, Baltimore, MD.551Computational Linguistics Volume 33, Number 4Taskar, B., D. Klein, M. Collins, D. Koller,and C. Manning.
2004.
Max-marginparsing.
In Proceedings of the EMNLPConference, pages 1?8, Barcelona, Spain.Toutanova, Kristina, ChristopherManning, Stuart Shieber, Dan Flickinger,and Stephan Oepen.
2002.
Parsedisambiguation for a rich HPSGgrammar.
In Proceedings of the FirstWorkshop on Treebanks and LinguisticTheories, pages 253?263, Sozopol, Bulgaria.Toutanova, Kristina, Penka Markova, andChristopher Manning.
2004.
The leafprojection path view of parse trees:Exploring string kernels for HPSGparse selection.
In Proceedings of theEMNLP conference, pages 166?173,Barcelona, Spain.Watkinson, Stephen and Suresh Manandhar.2001.
Acquisition of large categorialgrammar lexicons.
In Proceedings of theConference of the Pacific Association forComputational Linguistics (PACLING-01),Kitakyushu, Japan.Watson, Rebecca, John Carroll, and E. J.Briscoe.
2005.
Efficient extraction ofgrammatical relations.
In Proceedingsof the 9th International Workshop onParsing Technologies, pages 160?170,Vancouver, Canada.Wood, Mary McGee.
1993.
CategorialGrammars.
Routledge, London.Xia, Fei, Martha Palmer, and Aravind Joshi.2000.
A uniform method of grammarextraction and its applications.
InProceedings of the EMNLP Conference,pages 53?62, Hong Kong.Younger, D. 1967.
Recognition andparsing of context-free languagesin time n3.
Information and Control,10(2):189?208.Zettlemoyer, Luke S. and Michael Collins.2005.
Learning to map sentences tological form: Structured classificationwith probabilistic categorial grammars.In Proceedings of the 21st Conferenceon Uncertainty in Artificial Intelligence,pages 658?666, Edinburgh, UK.552
