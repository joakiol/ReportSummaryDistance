Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1311?1321, Dublin, Ireland, August 23-29 2014.Group based Self Training for E-Commerce Product Record LinkageWayne Xin Zhao1,2, Yuexin Wu2, Hongfei Yan2and Xiaoming Li21School of Information, Renmin University of China, China2School of Electronic Engineering and Computer Science, Peking University, Chinabatmanfly@gmail.com, wuyuexin@gmail.com,yhf1029@gmail.com, lxm@pku.edu.cnAbstractIn this paper, we study the task of product record linkage across multiple e-commerce web-sites.
We solve this task via a semi-supervised approach and adopt the self-training algorithm forlearning with little labeled data.
In previous self-training algorithms, the learner tries to convertthe most confidently predicted unlabeled examples of each class into labeled training examples.However, they evaluate the confidence of an instance only based on the individual evidence fromthe instance.
The correlation among data instances is rarely considered.To address it, we develop a novel variant of the self-training algorithm by leveraging the datacharacteristics for the task of product record linkage.
We joint consider a candidate linked pairand its corresponding correlated pairs as a group at the selection of pseudo labeled data.
Wepropose a novel confidence evaluation method for a group of instances, and incorporate it as are-ranking step in the self-training algorithm.
We evaluate the novel self-training algorithm ontwo large datasets constructed based on real e-commerce Websites.
We adopt several competitivemethods as comparisons and perform extensive experiments.
The results show that our methodoutperforms these baselines that do not consider data correlation.1 IntroductionRecent years have witnessed the rapid development of online e-commerce business, e.g.
Amazon andeBay, which raises the need for better storing, organizing and analyzing the large amount of productrecords.
An important task is how to effectively link product records across multiple databases or web-sites.
This task serves as a fundamental step for many applications.
For example, it will be useful toprovide entity-oriented search and product comparison analysis in eBay, where record linkage can helpto unify the corresponding records (i.e.
records from different sellers) given a product.
Record linkage hasbeen shown to be important in many fields, including biology (Needleman and Wunsch, 1970), database(Neiling, 2006) and text mining (Goiser and Christen, 2006; Bilenko and Mooney, 2003).
In this paper,we mainly focus on the task of product record linkage for online e-commerce websites, but our methodis easy to be extended to other data sources and tasks.Early studies on record linkage were mainly based on the classical probabilistic approach develope-d by Fellegi and Sunter (1969), furthermore it was improved by the application of the expectation-maximization (EM) algorithm (Winkler, 1988) and the use of approximate string comparison algorithms(Christen, 2006; Winkler, 2006).
The early work was not flexible to incorporate rich information.
Thedevelopment of machine learning techniques in the late 1990s provides a new approach for record link-age, and it has become the mainstream methodology for this task.
The task of record linkage is usuallyre-casted as the record pair classification problem, i.e.
whether a record pair refers to the same entity ornot (Elfeky et al., 2002; Neiling, 2006; Tejada et al., 2002; Nahm et al., 2002).
Supervised methods canalso be used to learn distance measures for approximate string comparisons (Bilenko and Mooney, 2003;This work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/1311Cohen et al., 2003).
Although supervised techniques often achieve good linkage quality, they are largelylimited by the availability of the training data.To address this problem, semi-supervised learning approaches aim to make good use of a small portionof labeled and a large amount of unlabeled data to build a better classifier (Yarowsky, 1995).
Self-trainingis a commonly used algorithm for semi-supervised learning, where in each iteration the learner convertsthe most confidently predicted unlabeled examples of each class into labeled training examples.
It hasbeen successfully applied to many tasks, such as sentiment analysis (He and Zhou, 2011; Riloff et al.,2003) and object detection from images (Rosenberg et al., 2005).In this paper, we solve the task of product record linkage via a semi-supervised approach and adoptthe flexible self-training framework for learning with little labeled data.
We propose a novel variant ofthe self-training algorithm by incorporating the correlation existing in the data instances, which is rarelystudied in previous studies.
To introduce our idea, we first present an illustrative example in Figure 1.There are two databases D and D?, and we have three records r1, r2, r3?
D and another three recordsr?1, r?2, r?3?
D?.
Furthermore, we assume r1and r?1refer to the same product.
We can see that r1isinvolved in three candidate pairs, i.e.
(r1, r?1), (r1, r?2) and (r1, r?3).
Similarly, r?1is involved in threecandidate pairs, i.e.
(r?1, r1), (r?1, r2) and (r?1, r3).
Usually, each individual database does not containduplicate records, once we know r1is linked to r?1, we can infer the rest candidate pairs should not belinked.
In other words, only if we are confident that no pair in the set {(r1, r?2), (r1, r?3), (r2, r?1), (r3, r?1)}is not linked, r1is likely to be linked with r?1.?????
?Figure 1: An illustrative example for correlation among record pairs.
The real line denotes the real linkagerelation and the dash line denotes the candidate linkage relation.For the task of record linkage, the number of positive instances (i.e.
linked record pairs) are usuallymuch less than that of negative instances.
We mainly consider the confidence evaluation of the candidatepositive instance.
By following the above idea, given a candidate linked pair, we treat all the correlatedrecord pairs together as a group and evaluate the linkage confidence based on the evidence of all recordpairs in this group, i.e.
group confidence evaluation.
We incorporate the group confidence evaluationinto the self-training algorithm as a re-ranking step.
Interestingly, once we have identified a linked pair,the rest correlated record pairs can be naturally judged as negative instances.
We evaluate the novelself-training algorithm on two large datasets constructed based on real e-commerce Websites.
We adoptseveral competitive methods as comparisons and perform extensive experiments.
The results show thatour method outperforms these baselines that do not consider data correlation.2 Related WorkWe have briefly described the supervised approaches for record linkage in the introduction.
Now wediscuss other related studies, including unsupervised clustering techniques, genetic programming basedapproaches and linking based on more complex constraints.Unsupervised clustering techniques have been investigated both for improved blocking (Cohen andRichman, 2002; McCallum et al., 2000) and for automatic record pair classification (Elfeky et al., 2002).Usually, such techniques do not perform not as well as supervised approaches.Most recently, genetic programming (GP) (Koza et al., 1999) has also been utilized to the task ofrecord linkage.
GenLink (Isele and Bizer, 2012) is a GP-based supervised learning algorithm in orderto learn linkage rules from a set of existing reference links, which also suffers from the problem oflack of labeled data.
Ngomo and Lyko (2013) evaluated linear and boolean classifiers against classifiers1312computed by using genetic programming for the record linkage problem.
Their experiments showed thatboth approaches did not perform well on real data.Some other studies exploit more complex constraints that include relationships between different entitytypes to link all types of entities in coordination (Bhattacharya and Getoor, 2007; Dong et al., 2005; Onet al., 2007).
The usage of such constraints can indeed help to get better linkage results, but is in manycases domain-dependent.
We try to develop an approach which can be applicable across domains.In order to address the problem of limited labeled data, we mainly consider the semi-supervised ap-proaches.
There are rarely semi-supervised approaches specially for the record linkage problem.
Somestudies on improving self-training algorithms are related to our work.
Self-training with editing (Li andZhou, 2005) can help to reduce mislabeled pseudo training examples, and reserved self-training (Guanand Yang, 2013) is designed for handling imbalanced data.
We have very different focus with theirs, i.e.incorporating the instance correlations into learning algorithms, which can applied to other self-trainingvariants.3 Problem DefinitionIn this section, we first introduce the preliminary related to our task.
Then we formally define our studiedtask.Product record.
A product record r is characterized by a referred product entity e and a set of attributevalues V = {(vi)}i, where videnotes the value of the ith attribute in r. We use r.e and r.V to indexthe product entity and attribute value set of the record r respectively.
A product record corresponds to aunique product entity but a product entity can map to multiple product records across multiple databases.Attribute values are represented as strings, i.e.
a sequence of characters.
An attribute of a product mightcorrespond to different descriptive text across websites.Product record linkage.
The task of product record linkage is to judge whether two product records referto the same product entity.
Given two product records r and r?, we aim to judge whether r.e is the sameto r?.e.
Usually, r and r?come from different product databases.
Although different product databasescan have different attributes for the same product and different attribute names for the same attribute, wemake an assumption about the task: candidate record pairs share the same set of attributes.
It is relativelyeasy to automatically identify common attributes and align attributes (H?arder et al., 1999; Rundensteiner,1999; Hassanzadeh et al., 2013), which is not our focus in this paper.
We mainly study product recordlinkage under the same set of attributes, and this assumption makes our study more focused.
If r and r?refer to the same product entity, denoted by r ?
r?
; otherwise, we denote it by r 6?
r?.4 A General Machine Learning based ApproachGiven a product type, as we mentioned above, we assume that it corresponds to a specific set of attributes,and all the product records share the same set of attributes but possibly with different descriptive text forattribute values.
In this section, we further present a general supervised approach with similarity features.4.1 Defining the similarity functionGiven two product records r and r?, we can obtain the similarity between their descriptive text of an at-tribute by using a similarity function.
The major intuition is that if two records refer to the same product,they should have similar text for the same attribute, i.e.
the similarity function should return a large sim-ilarity value.
Let f(?, ?)
denote a similarity function, which takes two text strings and returns a similarityvalue within the interval [0, 1] for these two strings.
As revealed in (Bilenko and Mooney, 2003), differ-ent attributes or fields may need different similarity functions to achieve best similarity evaluation.
Thus,instead of fixing a single similarity function, we consider using the following widely used similarityfunctions: 1) Exact match; 2) Cosine similarity; 3) Jaccard coefficient; 4) K-Gram similarity (Kondrak,2005); 5) Levenshtein similarity (Levenshtein, 1966); 6) Affine Gap similarity (Needleman and Wunsch,1970).13134.2 The learning frameworkBased on these similarity functions, we propose a general learning framework for product record linkageby using similarity values of different fields as features.Given a product type, we assume that there are A attributes and K similarity functions.
For two recordsr and r?, we can obtain a similarity feature vector x = [xa,k]Ai=1,Kk=1, which is indexed by an attribute anda similarity function: xa,kdenotes the similarity of the ath attribute between r and r?by using the kthsimilarity function.
Furthermore, each feature vector x will correspond to a unique binary label y whichindicates that r and r?refer to the same product entity.
Given a set of record pairs and their linkage labels{(x, y)}, we can learn a classifier which is able to predict the linkage label given the similarity featurevector of two records.
To this end, we have reformulated the task of product record linkage as a binaryclassification problem.
Any classifiers can be used for this task.
In what follows, we will use instancesand candidate pairs alternatively.5 Group based Self-TrainingIn the above, we have presented a supervised learning approach for product record linkage.
The approachis easy to apply in practice, however, the performance is largely limited by the availability of trainingdata.
For our current task, i.e.
product record linkage, the generation of labeled data becomes even muchharder: there are usually many product types and it is infeasible to create a large amount of labeled datafor each type.
Although it is difficult to obtain labeled data, we can easily obtain sufficient unlabeled data.Thus, in this paper, we study the task of product record linkage in a semi-supervised setting by leveragingboth the learning ability of the classifiers and the usefulness of the large amount of unlabeled data.
Wepropose a novel group based self-training algorithm for product record linkage.
Before introducing ourmethod, we first introduce the general self-training algorithm.5.1 The general self-training algorithmSelf-training is a semi-supervised learning algorithm.
It starts training on labeled data only, after eachiteration, the most confidently predicted unlabeled samples would be incorporated as new labeled data,i.e.
pseudo labeled data, decided by confidence scores from the classifier.
After several iterations, it isexpected to get a better classifier trained with both labeled data and pseudo labeled data.
The generalprocedure of self-training algorithm is summarized in Algorithm 1.Algorithm 1: The general procedure of the self-training algorithm.1 Input: labeled dataset L, unlabeled dataset U , the classifier C.2 U??
S randomly selected examples from U , S is usually set to 0.5 ?
|U|;3 repeat4 Training the classifier: Use L to train C, and label the examples in U?
;5 Selecting pseudo labeled data: Select T most confidently classified examples from U?and add them to L;6 Filling unlabeled data: Refill U?with examples from U , to keep U?at a constant size of S examples.7 until I iterations or U = ?
;8 return The extended labeled dataset L and the trained classifier C.We can see that self-training is a wrapper algorithm by taking a classifier as the learning component,and it has three major steps in an iteration: 1) training classifier; 2) selecting pseudo labeled data; and3) filling unlabeled data.
Among the three steps, the most important step is the pseudo labeled dataselection.
Previously, the most commonly used method is to select the top confident instances of theclassifier, and it is easy to see that the performance of self-training relies on the learning ability of theembedded classifier.5.2 Group confidence evaluationRecall that each instance is a pair of product records (r, r?)
and their label indicates whether they shouldbe linked or not.
Let PL(r, r?)
denote the confidence that r and r?refer to the same product entity (linked1314confidence), and PN(r, r?)
denote the confidence that r and r?refer to different product entities (non-linked confidence).
PL(r, r?)
and PN(r, r?)
can be estimated by the confidence scores from the classifier.In the task of product record linkage, there are usually more negative instances, i.e.
the number of non-linked pairs is much more than that of linked pairs.
Thus, we mainly study the confidence of a candidatepositive instance.
The standard self-training algorithm selects top ranked positive instances accordingto the confidence scores estimated by the classifier, i.e.
we select the instances with large linked con-fidence PL(?, ?).
However, when applied to product record linkage, it ignores important characteristicsunderlying the data, which will be potentially helpful to the task.Let us examine the illustrative example in Figure 1.
Recall that r1and r?1refer to the same product,i.e.
r1?
r?1.
We can see that r1is involved in three candidate pairs, i.e.
(r1, r?1), (r1, r?2) and (r1, r?3).Similarly, r?1is involved in three candidate pairs, i.e.
(r?1, r1), (r?1, r2) and (r?1, r3).
We totally have a setof five candidate pairs, i.e.
{(r1, r?1), (r1, r?2), (r1, r?3), (r2, r?1), (r3, r?1)}.
Here we follow the assumptionof the one-to-one mapping, i.e.
given two databases, a product record can link to at most one record inthe other database.
By leveraging the correlation among candidate pairs, with r1?
r?1, we can infer therest four candidate pairs must not be linked, i.e.
r16?
r?2, r16?
r?3, r26?
r?1, r36?
r?1.
Next, we formallycharacterize the above idea and present the algorithm.
Given two databases D and D?, let C ?
D ?
D?denote the candidate pair set where two product records in a pair come from D and D?respectively.Consider a candidate pair (r, r?)
?
C, where r ?
D, r??
D?.
We consider the following two sets:Sr= {(r, b)|(r, b) ?
C, b ?
D?and b 6= r?}
and Sr?= {(a, r?
)|(a, r?)
?
C, a ?
D and a 6= r}.Intuitively, if we know r ?
r?, then all the pairs in both Srand Sr?must not be linked.
Thus, we definethe conflicting set of pair (r, r?)
as Sr,r?cfl= Sr?
Sr?.With the definition of the conflicting set, let us reconsider the pseudo labeled data selection.
Thestraightforward way is to evaluate each instance with their linked confidence PL() from the classifier.However, it oversimplifies the data dependence and does not make use of the correlated characteristics.Consider an instance, which is a record pair (r, r?
), we can have the following two properties:?
If r ?
r?, then ?
(a, b) ?
Sr,r?cfl, we have a 6?
b;?
If ?
(a, b) ?
Sr,r?cfland a ?
b, then we have r 6?
r?.The above properties suggest that it should be helpful to consider the correlation among instanceswhen evaluating the confidence of a positive instance, i.e.
a candidate linked record pair.
Intuitively, iftwo records refer to the same product entity, they should have large linked confidence and their conflictingpairs should have large non-linked confidence.
We propose to use the following method to evaluate thelinkage confidence between r and r?Conf(r, r?)
= PL(r, r?)(?
(a,b)?Sr,r?cflPN(a, b))1/M, (1)where M = |Sr,r?cfl|, PL(?, ?)
and PN(?, ?)
are positive and negative confidence scores estimated bythe classifier respectively.
Note that we take the geometric mean of the non-linked confidence of theseconflicting pairs, which is to reduce the affect of large outlier values and the varying size of the conflictsets.
We treat a candidate linked pair and all the candidate pairs in its conflicting set as a group.
The groupconfidence evaluation consists of two intuitions: 1) the confidence that two records should be linked; 2)the confidence that any pair of records in the conflicting set must not be linked.
We have taken these twoaspects into a unified evaluation score.5.3 The proposed self-training algorithmIn this part, we present the novel self-training algorithm based on the group confidence evaluation.
Wehave the similar steps with the general self-training algorithm in Algorithm 1.
The major focus is to mod-ify the step of pseudo labeled data selection.
As mentioned above, we mainly consider the confidenceevaluation of positive instances.
Our method for pseudo labeled data selection is three-step process:1315?
Select top T?most confidently classified positive examples by the classifier;?
Rerank these T?examples by the group confidence scores defined in Equation 1;?
Select top T examples from the reranked T?examples (T ?
T?)
as pseudo positive instances andtheir corresponding conflicting instances in the conflicting sets as pseudo negative instances.We select positive instances not only based on the instance itself but also their corresponding conflict-ing instances: if we have high confidence about a positive instance, then the confidence of their conflict-ing instances being negative should be high, too.
Next, we present the detailed group based self-trainingalgorithm in Algorithm 2.Algorithm 2: The procedure of the group based self-training algorithm.1 Input: labeled dataset L, unlabeled dataset U , the classifier C.2 U??
S randomly selected examples from U ;3 repeat4 Training the classifier: Use L to train C, and label the examples in U?
;5 Selecting pseudo labeled data selection:?
Select T?most confident positive examples from U?and add them to L;?
Calculate the group confidence scores for the T?examples according to Equation 1.?
Rerank these T?examples by their group confidence scores and add top T examples to L as the pseudo positiveinstances.?
For each of the T examples, add their conflicting instances to L into as the pseudo negative instances.Filling unlabeled data: Refill U?with examples from U , to keep U?at a constant size of S examples.6 until I iterations or U = ?
;7 return The extended labeled dataset L and the trained classifier C.On one hand, our group based self-training algorithm naturally exploits the correlation among datainstances and evaluate the confidence scores in a broader view, which avoids the decision conflicts causedby the data dependence.
On the other hand, we focus on evaluating the confidence of being a positiveinstance, which further reduces the bias from imbalanced data distribution.
Thus, it is expected to achievebetter performance in the task of product record linkage.Most classifiers can provide the estimated confidence scores PL() (i.e.
for a positive instance) andPN() (i.e.
for a negative instance): Maximum-Entropy models output the conditional probabilities of aninstance for each class (Berger et al., 1996); the Decision Tree C4.5 algorithm is also able to computethe probability distribution over different classes for each instance (Quinlan, 1993).6 Experiments6.1 Construction of the test collectionWe test our method on two real e-commerce datasets respectively from Jingdong1and eTao2.
Jingdong isthe largest B2C e-commerce company and eTao is one of the largest product search portals in China.
Dueto the extremely large product databases, it is infeasible to generate training data on each product typefor these two product databases.
We consider two popular kinds of products: laptop and camera.
Thesetwo kinds of products cover a considerable amount of brands and models, especially suitable for the testof record linkage.
Both Jindong and eTao have set up specific categories for these two kinds of productsrespectively, thus we can easily crawl the product records under the corresponding category label.
Togenerate linked record datasets, we first manually align attributes (i.e.
fields) for these two kinds betweenJindong and eTao.
We summarize the numbers of aligned fields and some example fields in Table 1.
Notall the records contain the information for all the fields, we set the value of the empty field to a ?NULL?string.1http://www.jd.com2http://www.etao.com1316We adopt a blocking approach (Baxter et al., 2003) to automatically generate a set of candidate pairs,i.e.
a record in Jindong is to be linked with a record in eTao.
This approach consider all pairwise linksbetween Jindong records and eTao records for the same kind of product.
If there exists at least one com-mon word in the field of brand or model between a record pair, we consider it to be a candidate pair.
Theautomatic method generates 20,094 candidate pairs and 12,157 candidate pairs respectively for LAPTOPand CAMERA.
Then we invite professional workers from an e-commerce company to link records acrossthese two product databases.
Instead of examining all the candidate pairs, the labeling process adopts aproduct-oriented way to generate the gold standard.
Given a product record of a database, the annotatorfirst identifies the product entity that the record refers to, then she looks for the corresponding record inanother database.
In the annotation process, Web access is available all the time.
Annotators can makeuse of the search engines of Jindong and eTao to accelerate the product lookup.
A linked record pair istreated as a positive instance.
Finally, we identify 501 linkable products (i.e.
501 positive instances) inLAPTOP dataset, and 478 linkable products (i.e.
478 positive instances) in CAMERA dataset.
All theother candidate pairs are automatically labeled as negative.
We present the the data statistics in Table 1.Dataset# positive # negative# fields Example fieldsinstances instancesLAPTOP 501 19593 10 OS, screen size, CPU type, ram sizeCAMERA 478 11679 11 lens type, sensor type, focal length, aperture sizeTable 1: Basic statistics of datasets.6.2 Experimental setupFor each kind of product, we divide the dataset into two parts, i.e.
a training set and a test set.
In orderto examine different methods in a semi-supervised setting, we keep a small amount of instances in thetraining set, and we assume all the methods can use of the data (without labels) in the test set.
There aremore negative instances, we mainly consider the amount of positive instances, and the number of positiveinstances is called as the number of seeds.
We randomly generate the training set with the given numberof seeds.
Once we add one positive instance into the training set, we add all the its conflicting instancesinto the training set.
This is to reduce the correlation between training instances and testing instances fora fair comparison.
In later experiments, given the seed number, we will generate ten random training setsand take the average of ten runs as the final performance.
In later experiments, we do not explicitly reportthe number of negative instances unless needed.We adopt three widely used evaluation metrics for the classification task: Precision, Recall and theF-measure3.We compare the following methods for the task of product record linkage:?
Supervised Classifier (SC): the standard supervised classifier, which does not consider the unlabeleddata at all.?
Traditional Self-Training (t-ST): the traditional self-training method in Algorithm 1 which adds anequal amount of samples of each class in pseudo labeled selection at each iteration.?
Proportional Self-Training (p-ST): the traditional self-training method in Algorithm 1 but add sam-ples according to the class distribution at each iteration.?
Simple Group Based Self-Training (s-ST): a simplified version of our approach without the groupconfidence valuation, which directly selects samples of high confidence scores estimated from theclassifier together with their conflicting pairs as negative samples at each iteration.?
Group Based Self-Training (g-ST): the proposed group based self-training algorithm in Algorithm 2,which uses the group confidence evaluation method to select pseudo positive instances.3http:/en.wikipedia.org/wiki/Precision and recall1317Recall all the methods rely on the wrapped classifier.
We select two classic but very different classi-fiers: the Maximum Entropy model (MaxEnt) and the Decision Tree C4.5 (Tree).
We implement thesetwo classifiers using the machine learning toolkit Weka4.
We use the six similarity functions to obtainsimilarity values between two records on each field as features.
All the self-training based methods runten iterations and at each iteration they add the same number of positive instances, i.e.
30.
Differen-t methods select pseudo negative instances differently.
t-ST does not consider the correlation betweendata instances, and it adds top 30 confident negative instances.
p-ST adds top 30 ?#negative instances#positive instancescon-fident negative instances.
Both p-ST and g-ST take all the conflicting instances of the selected pseudopositive instances as the negative instances.
We present the average numbers of pseudo negative instancesat an iteration in Table 2.
As will be revealed later, although p-ST adds more negative instances, g-STperforms much better than p-ST, which indicates simply adding more negative instances might not leadto better performance.
We do not perform specific preprocessing steps to make the data balanced (e.g.under-sampling or over-sampling), and we find the data distribution does not significantly affect theperformance of the classifiers on our dataset.Dataset t-ST p-ST s-ST g-STLAPTOP 30 950 845 854CAMERA 30 655 569 584Table 2: Average numbers of pseudo negative instances selected at each iteration.6.3 Results and analysisOverall performance comparison.
To test the performance under weak supervision, we first set theseed number to 30, which nearly takes up a proportion of 5% of the labeled data.
We present the resultsof different methods in Table 3 and Table 4.
We first examine the performance of the baselines.
We cansee that semi-supervised learning is very effective to improve over the the supervised classifier when theamount of training data is small.
It is interesting to see that s-ST performs best among all the baselines.Recall that the major difference between s-ST and other baselines is that it select the conflicting pairsof the pseudo positive instances as the negative instances.
It indicates that it is important to consider thecorrelation among the data instances.
In addition, Decision Tree seems to be more competitive than Max-imum Entropy Model for product record linkage.
Then we take our group based self-training algorithminto comparison.
In terms of F1 measure, we can see that it is consistently better than all the baselineson two datasets respectively by using two different classifiers.
It is worth looking into the performancecomparison on precision and recall.
We can see that (1) s-ST and g-ST yield better results in terms ofprecision while the other baselines yield better results in terms of recall; (2) our method g-ST largelyimproves over the best baseline s-ST.
It is not surprising to have these observations since that our groupevaluation method is more careful at the selection of pseudo positive instance: it considers the evidencefrom the conflicting instances.Methods MaxEnt Decision TreeP R F1 P R F1SC 0.246 0.910 0.382 0.301 0.931 0.454t-ST 0.264 0.925 0.411 0.328 0.921 0.484p-ST 0.350 0.831 0.487 0.412 0.887 0.539s-ST 0.979 0.632 0.767 0.909 0.754 0.823g-ST 0.936 0.742 0.826 0.912 0.843 0.876Table 3: Results on LAPTOP dataset.Parameter tuning.
In the above, we have shown the results of different methods with 30 positive in-stances.
The number of seeds is particularly important for self-training algorithms, and we want to ex-4http://www.cs.waikato.ac.nz/ml/weka1318Methods MaxEnt Decision TreeP R F1 P R F1SC 0.387 0.891 0.540 0.493 0.965 0.652t-ST 0.352 0.892 0.504 0.537 0.963 0.677p-ST 0.501 0.871 0.626 0.573 0.942 0.700s-ST 0.931 0.479 0.632 0.962 0.570 0.716g-ST 0.917 0.574 0.706 0.965 0.588 0.731Table 4: Results on CAMERA dataset.amine how it affects the performance of these methods.
By varying the number of seeds from 10 to 50with a step of 10, we present the F1 results in Figure 2 on two datasets by using two classifiers.
We cansee that our method is consistently better than baselines with the varying of the seed number.
Especially,our method still works well when there is little labeled data, i.e.
#seeds = 10.
With a weaker classifier,i.e.
MaxEnt, our method yields more improvement than that with Tree.
Besides the seed number, thereare another two factors which potentially affect the performance: (1) the iteration number and (2) thenumber of pseudo positive instances selected at each iteration.
We also examine the tuning results ofthese two parameters and find our method is consistently better than s-ST with the varying of these twofactors.
These results show that our method is very effective and it is of high stability and practicability.0.20.30.40.50.60.70.80.910  15  20  25  30  35  40  45  50F1# of Labeled SeedsSCt-STp-STs-STg-ST(a) LAPTOP, MaxEnt0.20.30.40.50.60.70.80.910  15  20  25  30  35  40  45  50F1# of Labeled SeedsSCt-STp-STs-STg-ST(b) LAPTOP, Tree0.40.450.50.550.60.650.70.750.810  15  20  25  30  35  40  45  50F1# of Labeled SeedsSCt-STp-STs-STg-ST(c) CAMERA, MaxEnt0.350.40.450.50.550.60.650.70.750.80.8510  15  20  25  30  35  40  45  50F1# of Labeled SeedsSCt-STp-STs-STg-ST(d) CAMERA, TreeFigure 2: Performance comparison with varying seed numbers (i.e.
# of positive instances).13197 ConclusionIn this paper, we develop a novel variant of the self-training algorithm by leveraging the data characteris-tic for the task of product record linkage.
We joint consider a candidate linked pair and its correspondingcorrelated pairs as a group, at the selection of pseudo labeled data.
We propose a confidence evaluationmethod for a group of instances, and incorporate it as a re-ranking step in the self-training algorithm.
Weevaluate the novel self-training algorithm on two large datasets constructed based on real e-commerceWebsites.
We adopt several competitive methods as comparisons and perform extensive experiments.The results show that our method outperforms these baselines that do not consider data correlation.
Wealso carefully examine the affects of various parameters, and the tuning results indicate the stability androbustness of our method.The major contribution and novelty of this paper is the novel group confidence evaluation to modelthe correlation existing in data.
Although we develop the idea in the setting of self-training algorithms,it will be promising to be applied in other learning algorithms, i.e.
active learning.AcknowledgementsWe thank the anonymous reviewers for his/her thorough review and highly appreciate the comments.This work was partially supported by the National Key Basic Research Program (973 Program) of Chinaunder grant No.
2014CB340403, 2014CB340405 and NSFC Grant 61272340.
Xin Zhao was supportedby MSRA PhD fellowship.
Xin Zhao and Yuexin Wu contributed equally to this work and should beconsidered as joint first authors.
Xin Zhao is the corresponding author.ReferencesRohan Baxter, Peter Christen, and Tim Churches.
2003.
A comparison of fast blocking methods for record linkage.In ACM SIGKDD, volume 3, pages 25?27.
Citeseer.Adam L Berger, Vincent J Della Pietra, and Stephen A Della Pietra.
1996.
A maximum entropy approach tonatural language processing.
Computational linguistics, 22(1):39?71.Indrajit Bhattacharya and Lise Getoor.
2007.
Collective entity resolution in relational data.
ACM Transactions onKnowledge Discovery from Data (TKDD), 1(1):5.Mikhail Bilenko and Raymond J Mooney.
2003.
Adaptive duplicate detection using learnable string similaritymeasures.
In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery anddata mining, pages 39?48.
ACM.Peter Christen.
2006.
A comparison of personal name matching: Techniques and practical issues.
In Data MiningWorkshops, 2006.
ICDM Workshops 2006.
Sixth IEEE International Conference on, pages 290?294.
IEEE.William W Cohen and Jacob Richman.
2002.
Learning to match and cluster large high-dimensional data sets fordata integration.
In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discoveryand data mining, pages 475?480.
ACM.William W Cohen, Pradeep D Ravikumar, Stephen E Fienberg, et al.
2003.
A comparison of string distancemetrics for name-matching tasks.
In IIWeb, volume 2003, pages 73?78.Xin Dong, Alon Halevy, and Jayant Madhavan.
2005.
Reference reconciliation in complex information spaces.
InProceedings of the 2005 ACM SIGMOD international conference on Management of data, pages 85?96.
ACM.Mohamed G Elfeky, Vassilios S Verykios, and Ahmed K Elmagarmid.
2002.
Tailor: A record linkage toolbox.
InData Engineering, 2002.
Proceedings.
18th International Conference on, pages 17?28.
IEEE.Ivan P Fellegi and Alan B Sunter.
1969.
A theory for record linkage.
Journal of the American Statistical Associa-tion, 64(328):1183?1210.Karl Goiser and Peter Christen.
2006.
Towards automated record linkage.
In Proceedings of the fifth Australasianconference on Data mining and analystics-Volume 61, pages 23?31.
Australian Computer Society, Inc.Zhiguang Liu Xishuang Dong Yi Guan and Jinfeng Yang.
2013.
Reserved self-training: A semi-supervised senti-ment classification method for chinese microblogs.1320Theo H?arder, G?unter Sauter, and Joachim Thomas.
1999.
The intrinsic problems of structural heterogeneity andan approach to their solution.
The VLDB Journal, 8(1):25?43.Oktie Hassanzadeh, Ken Q Pu, Soheil Hassas Yeganeh, Ren?ee J Miller, Lucian Popa, Mauricio A Hern?andez,and Howard Ho.
2013.
Discovering linkage points over web data.
Proceedings of the VLDB Endowment,6(6):445?456.Yulan He and Deyu Zhou.
2011.
Self-training from labeled features for sentiment analysis.
Information Process-ing & Management, 47(4):606?616.Robert Isele and Christian Bizer.
2012.
Learning expressive linkage rules using genetic programming.
Proceed-ings of the VLDB Endowment, 5(11):1638?1649.Grzegorz Kondrak.
2005.
N-gram similarity and distance.
In String Processing and Information Retrieval, pages115?126.
Springer.John R Koza, Forrest H Bennett III, and Oscar Stiffelman.
1999.
Genetic programming as a Darwinian inventionmachine.
Springer.Vladimir I Levenshtein.
1966.
Binary codes capable of correcting deletions, insertions and reversals.
In Sovietphysics doklady, volume 10, page 707.Ming Li and Zhi-Hua Zhou.
2005.
Setred: Self-training with editing.
In Advances in Knowledge Discovery andData Mining, pages 611?621.
Springer.Andrew McCallum, Kamal Nigam, and Lyle H Ungar.
2000.
Efficient clustering of high-dimensional data setswith application to reference matching.
In Proceedings of the sixth ACM SIGKDD international conference onKnowledge discovery and data mining, pages 169?178.
ACM.Un Yong Nahm, Mikhail Bilenko, and Raymond J Mooney.
2002.
Two approaches to handling noisy variation intext mining.
In Proceedings of the ICML-2002 workshop on text learning (TextML2002), pages 18?27.
Citeseer.Saul B Needleman and Christian D Wunsch.
1970.
A general method applicable to the search for similarities inthe amino acid sequence of two proteins.
Journal of molecular biology, 48(3):443?453.Mattis Neiling.
2006.
Identification of real-world objects in multiple databases.
In From Data and InformationAnalysis to Knowledge Engineering, pages 63?74.
Springer.Axel-Cyrille Ngonga Ngomo and Klaus Lyko.
2013.
Unsupervised learning of link specifications: Deterministicvs.
non-deterministic.
Ontology Matching, page 25.Byung-Won On, Nick Koudas, Dongwon Lee, and Divesh Srivastava.
2007.
Group linkage.
In Data Engineering,2007.
ICDE 2007.
IEEE 23rd International Conference on, pages 496?505.
IEEE.John Ross Quinlan.
1993.
C4.
5: programs for machine learning, volume 1.
Morgan kaufmann.Ellen Riloff, Janyce Wiebe, and Theresa Wilson.
2003.
Learning subjective nouns using extraction pattern boot-strapping.
In Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume4, pages 25?32.
Association for Computational Linguistics.Chuck Rosenberg, Martial Hebert, and Henry Schneiderman.
2005.
Semi-supervised self-training of object detec-tion models.Elke Rundensteiner.
1999.
Special issue on data transformation.
IEEE Techn.
Bull.
Data Engineering, 22(1).Sheila Tejada, Craig A Knoblock, and Steven Minton.
2002.
Learning domain-independent string transformationweights for high accuracy object identification.
In Proceedings of the eighth ACM SIGKDD internationalconference on Knowledge discovery and data mining, pages 350?359.
ACM.William E Winkler.
1988.
Using the em algorithm for weight computation in the fellegi-sunter model of recordlinkage.
In Proceedings of the Section on Survey Research Methods, American Statistical Association, volume667, page 671.William E Winkler.
2006.
Overview of record linkage and current research directions.
In Bureau of the Census.Citeseer.David Yarowsky.
1995.
Unsupervised word sense disambiguation rivaling supervised methods.
In Proceedings ofthe 33rd annual meeting on Association for Computational Linguistics, pages 189?196.
Association for Com-putational Linguistics.1321
