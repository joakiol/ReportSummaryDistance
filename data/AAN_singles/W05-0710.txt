Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages, pages 71?78,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsClassifying Amharic News Text Using Self-Organizing MapsSamuel EyassuDepartment of Information ScienceAddis Ababa University, Ethiopiasamueleya@yahoo.comBjo?rn Gamba?ck?Swedish Institute of Computer ScienceBox 1263, SE?164 29 Kista, Swedengamback@sics.seAbstractThe paper addresses using artificial neu-ral networks for classification of Amharicnews items.
Amharic is the language forcountrywide communication in Ethiopiaand has its own writing system contain-ing extensive systematic redundancy.
It isquite dialectally diversified and probablyrepresentative of the languages of a conti-nent that so far has received little attentionwithin the language processing field.The experiments investigated documentclustering around user queries using Self-Organizing Maps, an unsupervised learn-ing neural network strategy.
The bestANN model showed a precision of 60.0%when trying to cluster unseen data, and a69.5% precision when trying to classify it.1 IntroductionEven though the last years have seen an increasingtrend in investigating applying language processingmethods to other languages than English, most ofthe work is still done on very few and mainly Euro-pean and East-Asian languages; for the vast numberof languages of the African continent there still re-mains plenty of work to be done.
The main obsta-cles to progress in language processing for these aretwo-fold.
Firstly, the peculiarities of the languagesthemselves might force new strategies to be devel-oped.
Secondly, the lack of already available re-sources and tools makes the creation and testing ofnew ones more difficult and time-consuming.
?Author for correspondence.Many of the languages of Africa have few speak-ers, and some lack a standardised written form, bothcreating problems for building language process-ing systems and reducing the need for such sys-tems.
However, this is not true for the major Africanlanguages and as example of one of those this pa-per takes Amharic, the Semitic language used forcountrywide communication in Ethiopia.
With morethan 20 million speakers, Amharic is today probablyone of the five largest on the continent (albeit diffi-cult to determine, given the dramatic population sizechanges in many African countries in recent years).The Ethiopian culture is ancient, and so are thewritten languages of the area, with Amharic usingits own script.
Several computer fonts for the scripthave been developed, but for many years it had nostandardised computer representation1 which was adeterrent to electronic publication.
An exponentiallyincreasing amount of digital information is now be-ing produced in Ethiopia, but no deep-rooted cul-ture of information exchange and dissemination hasbeen established.
Different factors are attributed tothis, including lack of digital library facilities andcentral resource sites, inadequate resources for elec-tronic publication of journals and books, and poordocumentation and archive collections.
The diffi-culties to access information have led to low expec-tations and under-utilization of existing informationresources, even though the need for accurate and fastinformation access is acknowledged as a major fac-tor affecting the success and quality of research anddevelopment, trade and industry (Furzey, 1996).1An international standard for Amharic was agreed on onlyin year 1998, following Amendment 10 to ISO?10646?1.
Thestandard was finally incorporated into Unicode in year 2000:www.unicode.org/charts/PDF/U1200.pdf71In recent years this has lead to an increasing aware-ness that Amharic language processing resourcesand digital information access and storage facili-ties must be created.
To this end, some work hasnow been carried out, mainly by Ethiopian Telecom,the Ethiopian Science and Technology Commission,Addis Ababa University, the Ge?ez Frontier Foun-dation, and Ethiopian students abroad.
So have, forexample, Sisay and Haller (2003) looked at Amharicword formation and lexicon building; Nega and Wil-lett (2002) at stemming; Atelach et al (2003a) attreebank building; Daniel (Yacob, 2005) at the col-lection of an (untagged) corpus, tentatively to behosted by Oxford University?s Open Archives Ini-tiative; and Cowell and Hussain (2003) at charac-ter recognition.2 See Atelach et al (2003b) for anoverview of the efforts that have been made so far todevelop language processing tools for Amharic.The need for investigating Amharic informationaccess has been acknowledged by the EuropeanCross-Language Evaluation Forum, which added anAmharic?English track in 2004.
However, the taskaddressed was for accessing an English databasein English, with only the original questions beingposed in Amharic (and then translated into English).Three groups participated in this track, with Atelachet al (2004) reporting the best results.In the present paper we look at the problem ofmapping questions posed in Amharic onto a col-lection of Amharic news items.
We use the Self-Organizing Map (SOM) model of artificial neuralnetworks for the task of retrieving the documentsmatching a specific query.
The SOMs were imple-mented using the Matlab Neural Network Toolbox.The rest of the paper is laid out as follows.
Sec-tion 2 discusses artificial neural networks and in par-ticular the SOM model and its application to infor-mation access.
In Section 3 we describe the Amhariclanguage and its writing system in more detail to-gether with the news items corpora used for trainingand testing of the networks, while Sections 4 and 5detail the actual experiments, on text retrieval andtext classification, respectively.
Finally, Section 6sums up the main contents of the paper.2In the text we follow the Ethiopian practice of referring toEthiopians by their given names.
However, the reference listfollows Western standard and is ordered according to surnames(i.e., the father?s name for an Ethiopian).2 Artificial Neural NetworksArtificial Neural Networks (ANN) is a computa-tional paradigm inspired by the neurological struc-ture of the human brain, and ANN terminology bor-rows from neurology: the brain consists of millionsof neurons connected to each other through long andthin strands called axons; the connecting points be-tween neurons are called synapses.ANNs have proved themselves useful in derivingmeaning from complicated or imprecise data; theycan be used to extract patterns and detect trends thatare too complex to be noticed by either humans orother computational and statistical techniques.
Tra-ditionally, the most common ANN setup has beenthe backpropagation architecture (Rumelhart et al,1986), a supervised learning strategy where inputdata is fed forward in the network to the outputnodes (normally with an intermediate hidden layerof nodes) while errors in matches are propagatedbackwards in the net during training.2.1 Self-Organizing MapsSelf-Organizing Maps (SOM) is an unsupervisedlearning scheme neural network, which was in-vented by Kohonen (1999).
It was originally devel-oped to project multi-dimensional vectors on a re-duced dimensional space.
Self-organizing systemscan have many kinds of structures, a common oneconsists of an input layer and an output layer, withfeed-forward connections from input to output lay-ers and full connectivity (connections between allneurons) in the output layer.A SOM is provided with a set of rules of a lo-cal nature (a signal affects neurons in the immedi-ate vicinity of the current neuron), enabling it tolearn to compute an input-output pairing with spe-cific desirable properties.
The learning process con-sists of repeatedly modifying the synaptic weightsof the connections in the system in response to input(activation) patterns and in accordance to prescribedrules, until a final configuration develops.
Com-monly both the weights of the neuron closest match-ing the inputs and the weights of its neighbourhoodnodes are increased.
At the beginning of the trainingthe neighbourhood (where input patterns cluster de-pending on their similarity) can be fairly large andthen be allowed to decrease over time.722.2 Neural network-based text classificationNeural networks have been widely used in text clas-sification, where they can be given terms and hav-ing the output nodes represent categories.
Ruizand Srinivasan (1999) utilize an hierarchical arrayof backpropagation neural networks for (nonlinear)classification of MEDLINE records, while Ng et al(1997) use the simplest (and linear) type of ANNclassifier, the perceptron.
Nonlinear methods havenot been shown to add any performance to linearones for text categorization (Sebastiani, 2002).SOMs have been used for information accesssince the beginning of the 90s (Lin et al, 1991).
ASOM may show how documents with similar fea-tures cluster together by projecting the N-dimen-sional vector space onto a two-dimensional grid.The radius of neighbouring nodes may be varied toinclude documents that are weaker related.
The mostelaborate experiments of using SOMs for documentclassification have been undertaken using the WEB-SOM architecture developed at Helsinki Universityof Technology (Honkela et al, 1997; Kohonen et al,2000).
WEBSOM is based on a hierarchical two-level SOM structure, with the first level forming his-togram clusters of words.
The second level is usedto reduce the sensitivity of the histogram to smallvariations in document content and performs furtherclustering to display the document pattern space.A Self-Organizing Map is capable of simulatingnew data sets without the need of retraining itselfwhen the database is updated; something which isnot true for Latent Semantic Indexing, LSI (Deer-wester et al, 1990).
Moreover, LSI consumes am-ple time in calculating similarities of new queriesagainst all documents, but a SOM only needs to cal-culate similarities versus some representative subsetof old input data and can then map new input straightonto the most similar models without having to re-compute the whole mapping.The SOM model preparation passes through theprocesses undertaken by the LSI model and the clas-sical vector space model (Salton and McGill, 1983).Hence those models can be taken as particular casesof the SOM, when the neighbourhood diameter ismaximized.
For instance, one can calculate theLSI model?s similarity measure of documents versusqueries by varying the SOM?s neighbourhood diam-eter, if the training set is a singular value decom-position reduced vector space.
Tambouratzis et al(2003) use SOMs for categorizing texts according toregister and author style and show that the results areequivalent to those generated by statistical methods.3 Processing AmharicEthiopia with some 70 million inhabitants is thethird most populous African country and harboursmore than 80 different languages.3 Three of theseare dominant: Oromo, a Cushitic language spokenin the South and Central parts of the country andwritten using the Latin alphabet; Tigrinya, spoken inthe North and in neighbouring Eritrea; and Amharic,spoken in most parts of the country, but predomi-nantly in the Eastern, Western, and Central regions.Both Amharic and Tigrinya are Semitic and about asclose as are Spanish and Portuguese (Bloor, 1995),3.1 The Amharic language and scriptAlready a census from 19944 estimated Amharic tobe mother tongue of more than 17 million people,with at least an additional 5 million second languagespeakers.
It is today probably the second largest lan-guage in Ethiopia (after Oromo).
The Constitutionof 1994 divided Ethiopia into nine fairly indepen-dent regions, each with its own nationality language.However, Amharic is the language for countrywidecommunication and was also for a long period theprincipal literal language and medium of instructionin primary and secondary schools in the country,while higher education is carried out in English.Amharic and Tigrinya speakers are mainly Ortho-dox Christians, with the languages drawing com-mon roots to the ecclesiastic Ge?ez still used by theCoptic Church.
Both languages are written usingthe Ge?ez script, horizontally and left-to-right (incontrast to many other Semitic languages).
Writ-ten Ge?ez can be traced back to at least the 4thcentury A.D.
The first versions of the script in-cluded consonants only, while the characters in laterversions represent consonant-vowel (CV) phonemepairs.
In modern written Amharic, each syllable pat-3How many languages there are in a country is as much a po-litical as a linguistic issue.
The number of languages of Ethiopiaand Eritrea together thus differs from 70 up to 420, dependingon the source; however, 82 (plus 4 extinct) is a common number.4Published by Ethiopia?s Central Statistal Authority 1998.73Order 1 2 3 4 5 6 7VHHHHC /9/ /u/ /i/ /5/ /e/ /1/ /o//s/ ?
?
?
?
?
s ?/m/ ?
?
?
?
?
m ?Table 1: The orders for s (/s/) andm (/m/)tern comes in seven different forms (called orders),reflecting the seven vowel sounds.
The first order isthe basic form; the other orders are derived from itby more or less regular modifications indicating thedifferent vowels.
There are 33 basic forms, giving7*33 syllable patterns, or fidEls.Two of the base forms represent vowels in isola-tion (a and ?
), but the rest are for consonants (orsemivowels classed as consonants) and thus corre-spond to CV pairs, with the first order being the basesymbol with no explicit vowel indicator (though avowel is pronounced: C+/9/).
The sixth order is am-biguous between being just the consonant or C+/1/.The writing system also includes 20 symbols forlabialised velars (four five-character orders) and 24for other labialisation.
In total, there are 275 fidEls.The sequences in Table 1 (for s and m) exemplifythe (partial) symmetry of vowel indicators.Amharic also has its own numbers (twenty sym-bols, though not widely used nowadays) and its ownpunctuation system with eight symbols, where thespace between words looks like a colon :, while thefull stop, comma and semicolon are ~, , and ;.
Thequestion and exclamation marks have recently beenincluded in the writing system.
For more thoroughdiscussions of the Ethiopian writing system, see, forexample, Bender et al (1976) and Bloor (1995).Amharic words have consonantal roots withvowel variation expressing difference in interpreta-tion, making stemming a not-so-useful technique ininformation retrieval (no full morphological anal-yser for the language is available yet).
There is noagreed upon spelling standard for compounds andthe writing system uses multitudes of ways to denotecompound words.
In addition, not all the letters ofthe Amharic script are strictly necessary for the pro-nunciation patterns of the language; some were sim-ply inherited from Ge?ez without having any seman-tic or phonetic distinction in modern Amharic: thereare many cases where numerous symbols are used todenote a single phoneme, as well as words that haveextremely different orthographic form and slightlydistinct phonetics, but the same meaning.
As a re-sult of this, lexical variation and homophony is verycommon, and obviously deteriorates the effective-ness of Information Access systems based on strictterm matching; hence the basic idea of this research:to use the approximative matching enabled by self-organizing map-based artificial neural networks.3.2 Test data and preprocessingIn our SOM-based experiments, a corpus of newsitems was used for text classification.
A main ob-stacle to developing applications for a language likeAmharic is the scarcity of resources.
No large cor-pora for Amharic exist, but we could use a smallcorpus of 206 news articles taken from the electronicnews archive of the website of the Walta InformationCenter (an Ethiopian news agency).
The trainingcorpus consisted of 101 articles collected by Saba(Amsalu, 2001), while the test corpus consisted ofthe remaining 105 documents collected by Theodros(GebreMeskel, 2003).
The documents were writtenusing the Amharic software VG2 Main font.The corpus was matched against 25 queries.
Theselection of documents relevant to a given query,was made by two domain experts (two journal-ists), one from the Monitor newspaper and the otherfrom the Walta Information Center.
A linguist fromGonder College participated in making consensus ofthe selection of documents made by the two jour-nalists.
Only 16 of the 25 queries were judged tohave a document relevant to them in the 101 docu-ment training corpus.
These 16 queries were foundto be different enough from each other, in the con-tent they try to address, to help map from documentcollection to query contents (which were taken asclass labels).
These mappings (assignment) of doc-uments to 16 distinct classes helped to see retrievaland classification effectiveness of the ANN model.The corpus was preprocessed to normalizespelling and to filter out stopwords.
One prepro-cessing step tried to solve the problems with non-standardised spelling of compounds, and that thesame sound may be represented with two or moredistinct but redundant written forms.
Due to the sys-tematic redundancy inherited from the Ge?ez, onlyabout 233 of the 275 fidEls are actually necessary to74Sound pattern Matching Amharic characters/s9/ ?, P/R9/ ?, ?/h9/ ?, ?, H, K, p, s/i9/ ?, ?, a, ATable 2: Examples of character redundancyrepresent Amharic.
Some examples of character re-dundancy are shown in Table 2.
The different formswere reduced to common representations.A negative dictionary of 745 words was created,containing both stopwords that are news specific andthe Amharic text stopwords collected by Nega (Ale-mayehu and Willett, 2002).
The news specific com-mon terms were manually identified by looking attheir frequency.
In a second preprocessing step, thestopwords were removed from the word collectionbefore indexing.
After the preprocessing, the num-ber of remaining terms in the corpus was 10,363.4 Text retrievalIn a set of experiments we investigated the devel-opment of a retrieval system using Self-OrganizingMaps.
The term-by-document matrix producedfrom the entire collection of 206 documents wasused to measure the retrieval performance of the sys-tem, of which 101 documents were used for train-ing and the remaining for testing.
After the prepro-cessing described in the previous section, a weightedmatrix was generated from the original matrix usingthe log-entropy weighting formula (Dumais, 1991).This helps to enhance the occurrence of a term inrepresenting a particular document and to degradethe occurrence of the term in the document col-lection.
The weighted matrix can then be dimen-sionally reduced by Singular Value Decomposition,SVD (Berry et al, 1995).
SVD makes it possible tomap individual terms to the concept space.A query of variable size is useful for compar-ison (when similarity measures are used) only ifits size is matrix-multiplication-compatible with thedocuments.
The pseudo-query must result from theglobal weight obtained in weighing the original ma-trix to be of any use in ranking relevant documents.The experiment was carried out in two versions, withthe original vector space and with a reduced one.4.1 Clustering in unreduced vector spaceIn the first experiment, the selected documents wereindexed using 10,363 dimensional vectors (i.e., onedimension per term in the corpus) weighted usinglog-entropy weighting techniques.
These vectorswere fed into an Artificial Neural Network that wascreated using a SOM lattice structure for mappingon a two-dimensional grid.
Thereafter a query and101 documents were fed into the ANN to see howdocuments cluster around the query.For the original, unnormalised (unreduced,10,363 dimension) vector space we did not try totrain an ANN model for more than 5,000 epochs(which takes weeks), given that the network perfor-mance in any case was very bad, and that the net-work for the reduced vector space had its apex atthat point (as discussed below).Those documents on the node on which the sin-gle query lies and those documents in the imme-diate vicinity of it were taken as being relevant tothe query (the neighbourhood was defined to be sixnodes).
Ranking of documents was performed usingthe cosine similarity measure, on the single queryversus automatically retrieved relevant documents.The eleven-point average precision was calculatedover all queries.
For this system the average preci-sion on the test set turned out to be 10.5%, as can beseen in the second column of Table 3.The table compares the results on training on theoriginal vector space to the very much improvedones obtained by the ANN model trained on the re-duced vector space, described in the next section.Recall Original vector Reduced vector0.00 0.2080 0.83110.10 0.1986 0.76210.20 0.1896 0.74200.30 0.1728 0.70100.40 0.0991 0.68880.50 0.0790 0.65460.60 0.0678 0.59390.70 0.0543 0.53000.80 0.0403 0.47890.90 0.0340 0.34401.00 0.0141 0.2710Average 0.1052 0.5998Table 3: Eleven-point precision for 16 queries754.2 Clustering in SVD-reduced vector spaceIn a second experiment, vectors of numerically in-dexed documents were converted to weighted matri-ces and further reduced using SVD, to infer the needfor representing co-occurrence of words in identify-ing a document.
The reduced vector space of 101pseudo-documents was fed into the neural net fortraining.
Then, a query together with 105 documentswas given to the trained neural net for simulation andinference purpose.For the reduced vectors a wider range of valuescould be tried.
Thus 100, 200, .
.
.
, 1000 epochswere tried at the beginning of the experiment.
Thenetwork performance kept improving and the train-ing was then allowed to go on for 2000, 3000,. .
.
, 10,000, 20,000 epochs thereafter.
The averageclassification accuracy was at an apex after 5,000epochs, as can been seen in Figure 1.The neural net with the highest accuracy was se-lected for further analysis.
As in the previous model,documents in the vicinity of the query were rankedusing the cosine similarity measure and the precisionon the test set is illustrated in the third column of Ta-ble 3.
As can be seen in the table, this system waseffective with 60.0% eleven-point average precisionon the test set (each of the 16 queries was tested).Thus, the performance of the reduced vectorspace system was very much better than that ob-tained using the test set of the normal term docu-ment matrix that resulted in only 10.5% average pre-cision.
In both cases, the precision of the training setwas assessed using the classification accuracy whichshows how documents with similar features clustertogether (occur on the same or neighbouring nodes).50556065700 5 10 15 20%Epochs (*103)Figure 1: Average network classification accuracy5 Document ClassificationIn a third experiment, the SVD-reduced vector spaceof pseudo-documents was assigned a class label(query content) to which the documents of the train-ing set were identified to be more similar (by ex-perts) and the neural net was trained using thepseudo-documents and their target classes.
This wasperformed for 100 to 20,000 epochs and the neuralnet with best accuracy was considered for testing.The average precision on the training set wasfound to be 72.8%, while the performance of theneural net on the test set was 69.5%.
A matrix ofsimple queries merged with the 101 documents (thathad been used for training) was taken as input toa SOM-model neural net and eventually, the 101-dimensional document and single query pairs weremapped and plotted onto a two-dimensional space.Figure 2 gives a flavour of the document clustering.The results of this experiment are compatible withthose of Theodros (GebreMeskel, 2003) who usedthe standard vector space model and latent semanticindexing for text categorization.
He reports that thevector space model gave a precision of 69.1% on thetraining set.
LSI improved the precision to 71.6%,which still is somewhat lower than the 72.8% ob-tained by the SOM model in our experiments.
Go-ing outside Amharic, the results can be compared tothe ones reported by Cai and Hofmann (2003) on theReuters-21578 corpus5 which contains 21,578 clas-sified documents (100 times the documents availablefor Amharic).
Used an LSI approach they obtaineddocument average precision figures of 88?90%.In order to locate the error sources in our exper-iments, the documents missed by the SOM-basedclassifier (documents that were supposed to be clus-tered on a given class label, but were not found un-der that label), were examined.
The documents thatwere rejected as irrelevant by the ANN using re-duced dimension vector space were found to containonly a line or two of interest to the query (for thetraining set as well as for the test set).
Also withinthe test set as well as in the training set some relevantdocuments had been missed for unclear reasons.Those documents that had been retrieved as rel-evant to a query without actually having any rele-vance to that query had some words that co-occur5Available at www.daviddlewis.com/resources76Figure 2: Document clustering at different neuron positionswith the words of the relevant documents.
Very im-portant in this observation was that documents thatcould be of some interest to two classes were foundat nodes that are the intersection of the nodes con-taining the document sets of the two classes.6 Summary and ConclusionsA set of experiments investigated text retrieval of se-lected Amharic news items using Self-OrganizingMaps, an unsupervised learning neural networkmethod.
101 training set items, 25 queries, and 105test set items were selected.
The content of eachnews item was taken as the basis for document in-dexing, and the content of the specific query wastaken for query indexing.
A term?document ma-trix was generated and the occurrence of terms perdocument was registered.
This original matrix waschanged to a weighted matrix using the log-entropyscheme.
The weighted matrix was further reducedusing SVD.
The length of the query vector was alsoreduced using the global weight vector obtained inweighing the original matrix.The ANN model using unnormalised vector spacehad a precision of 10.5%, whereas the best ANNmodel using reduced dimensional vector space per-formed at a 60.0% level for the test set.
For this con-figuration we also tried to classify the data around aquery content, taken that query as class label.
Theresults obtained then were 72.8% for the training setand 69.5% for the test set, which is encouraging.7 AcknowledgmentsThanks to Dr. Gashaw Kebede, Kibur Lisanu, LarsAsker, Lemma Nigussie, and Mesfin Getachew; andto Atelach Alemu for spotting some nasty bugs.The work was partially funded by the Faculty ofInformatics at Addis Ababa University and the ICTsupport programme of SAREC, the Department forResearch Cooperation at Sida, the Swedish Inter-national Development Cooperation Agency.77ReferencesNega Alemayehu and Peter Willett.
2002.
Stemming ofAmharic words for information retrieval.
Literary andLinguistic Computing, 17(1):1?17.Atelach Alemu, Lars Asker, and Gunnar Eriksson.2003a.
An empirical approach to building an Amharictreebank.
In Proc.
2nd Workshop on Treebanks andLinguistic Theories, Va?xjo?
University, Sweden.Atelach Alemu, Lars Asker, and Mesfin Getachew.2003b.
Natural language processing for Amharic:Overview and suggestions for a way forward.
In Proc.10th Conf.
Traitement Automatique des Langues Na-turelles, Batz-sur-Mer, France, pp.
173?182.Atelach Alemu, Lars Asker, Rickard Co?ster, and JussiKarlgren.
2004.
Dictionary-based Amharic?Englishinformation retrieval.
In 5th Workshop of the CrossLanguage Evaluation Forum, Bath, England.Saba Amsalu.
2001.
The application of information re-trieval techniques to Amharic.
MSc Thesis, School ofInformation Studies for Africa, Addis Ababa Univer-sity, Ethiopia.Marvin Bender, Sydney Head, and Roger Cowley.
1976.The Ethiopian writing system.
In Bender et al, eds,Language in Ethiopia.
Oxford University Press.Michael Berry, Susan Dumais, and Gawin O?Brien.1995.
Using linear algebra for intelligent informationretrieval.
SIAM Review, 37(4):573?595.Thomas Bloor.
1995.
The Ethiopic writing system: aprofile.
Journal of the Simplified Spelling Society,19:30?36.Lijuan Cai and Thomas Hofmann.
2003.
Text catego-rization by boosting automatically extracted concepts.In Proc.
26th Int.
Conf.
Research and Development inInformation Retrieval, pp.
182?189, Toronto, Canada.John Cowell and Fiaz Hussain.
2003.
Amharic characterrecognition using a fast signature based algorithm.
InProc.
7th Int.
Conf.
Image Visualization, pp.
384?389,London, England.Scott Deerwester, Susan Dumais, George Furnas,Thomas Landauer, and Richard Harshman.
1990.Indexing by latent semantic analysis.
Journalof the American Society for Information Science,41(6):391?407.Susan Dumais.
1991.
Improving the retrieval of informa-tion from external sources.
Behavior Research Meth-ods, Instruments and Computers, 23(2):229?236.Sisay Fissaha and Johann Haller.
2003.
Application ofcorpus-based techniques to Amharic texts.
In Proc.MT Summit IX Workshop on Machine Translation forSemitic Languages, New Orleans, Louisana.Jane Furzey.
1996.
Enpowering socio-economic devel-opment in Africa utilizing information technology.
Acountry study for the United Nations Economic Com-mission for Africa, University of Pennsylvania.Theodros GebreMeskel.
2003.
Amharic text retrieval:An experiment using latent semantic indexing (LSI)with singular value decomposition (SVD).
MSc The-sis, School of Information Studies for Africa, AddisAbaba University, Ethiopia.Timo Honkela, Samuel Kaski, Krista Lagus, and TeuvoKohonen.
1997.
WEBSOM ?
Self-Organizing Mapsof document collections.
In Proc.
Workshop on Self-Organizing Maps, pp.
310?315, Espoo, Finland.Teuvo Kohonen, Samuel Kaski, Krista Lagus, JarkkoSaloja?rvi, Jukka Honkela, Vesa Paatero, and AnttiSaarela.
2000.
Self organization of a massive doc-ument collection.
IEEE Transactions on Neural Net-works, 11(3):574?585.Teuvo Kohonen.
1999.
Self-Organization and Associa-tive Memory.
Springer, 3 edition.Xia Lin, Dagobert Soergel, and Gary Marchionini.
1991.A self-organizing semantic map for information re-trieval.
In Proc.
14th Int.
Conf.
Research and Develop-ment in Information Retrieval, pp.
262?269, Chicago,Illinois.Hwee Tou Ng, Wei Boon Goh, and Kok Leong Low.1997.
Feature selection, perceptron learning, and a us-ability case study for text categorization.
In Proc.
20thInt.
Conf.
Research and Development in InformationRetrieval, pp.
67?73, Philadelphia, Pennsylvania.Miguel Ruiz and Padmini Srinivasan.
1999.
Hierarchicalneural networks for text categorization.
In Proc.
22ndInt.
Conf.
Research and Development in InformationRetrieval, pp.
281?282, Berkeley, California.David Rumelhart, Geoffrey Hinton, and RonaldWilliams.
1986.
Learning internal representations byerror propagation.
In Rumelhart and McClelland, eds,Parallel Distributed Processing, vol 1.
MIT Press.Gerard Salton and Michael McGill.
1983.
Introductionto Modern Information Retrieval.
McGraw-Hill.Fabrizio Sebastiani.
2002.
Machine learning in auto-mated text categorization.
ACM Computing Surveys,34(1):1?47.George Tambouratzis, N. Hairetakis, S. Markantonatou,and G. Carayannis.
2003.
Applying the SOM modelto text classification according to register and stylisticcontent.
Int.
Journal of Neural Systems, 13(1):1?11.Daniel Yacob.
2005.
Developments towards an elec-tronic Amharic corpus.
In Proc.
TALN 12 Workshopon NLP for Under-Resourced Languages, Dourdan,France, June (to appear).78
