Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 368?375,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsAutomated Vocabulary Acquisition and Interpretationin Multimodal Conversational SystemsYi Liu Joyce Y. Chai Rong JinDepartment of Computer Science and EngineeringMichigan State UniversityEast Lansing, MI 48824, USA{liuyi3, jchai, rongjin}@cse.msu.eduAbstractMotivated by psycholinguistic findings thateye gaze is tightly linked to human lan-guage production, we developed an unsuper-vised approach based on translation modelsto automatically learn the mappings betweenwords and objects on a graphic display dur-ing human machine conversation.
The ex-perimental results indicate that user eye gazecan provide useful information to establishsuch mappings, which have important impli-cations in automatically acquiring and inter-preting user vocabularies for conversationalsystems.1 IntroductionTo facilitate effective human machine conversation,it is important for a conversational system to haveknowledge about user vocabularies and understandhow these vocabularies are mapped to the internalentities for which the system has representations.For example, in a multimodal conversational systemthat allows users to converse with a graphic inter-face, the system needs to know what vocabulariesusers tend to use to describe objects on the graphicdisplay and what (type of) object(s) a user is attend-ing to when a particular word is expressed.
Here,we use acquisition to refer to the process of acquir-ing relevant vocabularies describing internal entities,and interpretation to refer to the process of automat-ically identifying internal entities given a particularword.
Both acquisition and interpretation have beentraditionally approached by either knowledge engi-neering (e.g., manually created lexicons) or super-vised learning from annotated data.
In this paper,we describe an unsupervised approach that relieson naturally co-occurred eye gaze and spoken utter-ances during human machine conversation to auto-matically acquire and interpret vocabularies.Motivated by psycholinguistic studies (Just andCarpenter, 1976; Griffin and Bock, 2000; Tenenhauset al, 1995) and recent investigations on computa-tional models for language acquisition and ground-ing (Siskind, 1995; Roy and Pentland, 2002; Yuand Ballard, 2004), we are particularly interested intwo unique questions related to multimodal conver-sational systems: (1) In a multimodal conversationthat involves more complex tasks (e.g., both userinitiated tasks and system initiated tasks), is therea reliable temporal alignment between eye gaze andspoken references so that the coupled inputs can beused for automated vocabulary acquisition and inter-pretation?
(2) If such an alignment exists, how canwe model this alignment and automatically acquireand interpret the vocabularies?To address the first question, we conducted anempirical study to examine the temporal relation-ships between eye fixations and their correspond-ing spoken references.
As shown later in section 4,although a larger variance (compared to the find-ings from psycholinguistic studies) exists in terms ofhow eye gaze is linked to speech production duringhuman machine conversation, eye fixations and thecorresponding spoken references still occur in a veryclose vicinity to each other.
This natural couplingbetween eye gaze and speech provides an opportu-nity to automatically learn the mappings between368words and objects without any human supervision.Because of the larger variance, it is difficult toapply rule-based approaches to quantify this align-ment.
Therefore, to address the second question,we developed an approach based on statistical trans-lation models to explore the co-occurrence patternsbetween eye fixated objects and spoken references.Our preliminary experiment results indicate that thetranslation model can reliably capture the mappingsbetween the eye fixated objects and the correspond-ing spoken references.
Given an object, this modelcan provide possible words describing this object,which represents the acquisition process; given aword, this model can also provide possible objectsthat are likely to be described, which represents theinterpretation process.In the following sections, we first review some re-lated work and introduce the procedures used to col-lect eye gaze and speech data during human machineconversation.
We then describe our empirical studyand the unsupervised approach based on translationmodels.
Finally, we present experiment results anddiscuss their implications in natural language pro-cessing applications.2 Related WorkOur work is motivated by previous work in the fol-lowing three areas: psycholinguistics studies, multi-modal interactive systems, and computational mod-eling of language acquisition and grounding.Previous psycholinguistics studies have shownthat the direction of gaze carries information aboutthe focus of the user?s attention (Just and Carpenter,1976).
Specifically, in human language processingtasks, eye gaze is tightly linked to language produc-tion.
The perceived visual context influences spo-ken word recognition and mediates syntactic pro-cessing (Tenenhaus et al, 1995).
Additionally, be-fore speaking a word, the eyes usually move to theobjects to be mentioned (Griffin and Bock, 2000).These psycholinguistics findings have provided afoundation for our investigation.In research on multimodal interactive systems, re-cent work indicates that the speech and gaze inte-gration patterns can be modeled reliably for indi-vidual users and therefore be used to improve mul-timodal system performances (Kaur et al, 2003).Studies have also shown that eye gaze has a poten-tial to improve resolution of underspecified referringexpressions in spoken dialog systems (Campana etal., 2001) and to disambiguate speech input (Tanaka,1999).
In contrast to these earlier studies, our workfocuses on a different goal of using eye gaze for au-tomated vocabulary acquisition and interpretation.The third area of research that influenced ourwork is computational modeling of language acqui-sition and grounding.
Recent studies have shownthat multisensory information (e.g., through visionand language processing) can be combined to effec-tively acquire words to their perceptually groundedobjects in the environment (Siskind, 1995; Roy andPentland, 2002; Yu and Ballard, 2004).
Especially in(Yu and Ballard, 2004), an unsupervised approachbased on a generative correspondence model wasdeveloped to capture the mapping between spokenwords and the occurring perceptual features of ob-jects.
This approach is most similar to the transla-tion model used in our work.
However, comparedto this work where multisensory information comesfrom vision and language processing, our work fo-cuses on a different aspect.
Here, instead of applyingvision processing on objects, we are interested in eyegaze behavior when users interact with a graphic dis-play.
Eye gaze is an implicit and subconscious inputmodality during human machine interaction.
Eyegaze data inevitably contain a significant amount ofnoise.
Therefore, it is the goal of this paper to exam-ine whether this modality can be utilized for vocab-ulary acquisition for conversational systems.3 Data CollectionWe used a simplified multimodal conversational sys-tem to collect synchronized speech and eye gazedata.
A room interior scene was displayed on a com-puter screen, as shown in Figure 1.
While watchingthe graphical display, users were asked to communi-cate with the system on topics about the room dec-orations.
A total of 28 objects (e.g., multiple lampsand picture frames, a bed, two chairs, a candle, adresser, etc., as marked in Figure 1) are explicitlymodeled in this scene.
The system is simplified inthe sense that it only supports 14 tasks during humanmachine interaction.
These tasks are designed tocover both open-ended utterances (e.g., the system369Figure 1: The room interior scene for user studies.For easy reference, we give each object an ID.
TheseIDs are hidden from the system users.asks users to describe the room) and more restrictedutterances (e.g., the system asks the user whetherhe/she likes the bed) that are commonly supported inconversational systems.
Seven human subjects par-ticipated in our study.User speech inputs were recorded using the Au-dacity software1, with each utterance time-stamped.Eye movements were recorded using an EyeLink IIeye tracker sampled at 250Hz.
The eye tracker au-tomatically saved two-dimensional coordinates of auser?s eye fixations as well as the time-stamps whenthe fixations occurred.The collected raw gaze data is extremely noisy.To refine the gaze data, we further eliminated in-valid and saccadic gaze points (known as ?saccadicsuppression?
in vision studies).
Since eyes do notstay still but rather make small, frequent jerky move-ments, we also smoothed the data by averagingnearby gaze locations to identify fixations.4 Empirical Study on Speech-GazeAlignmentBased on the data collected, we investigated the tem-poral alignment between co-occurred eye gaze andspoken utterances.
In particular, we examined thetemporal alignment between eye gaze fixations andthe corresponding spoken references (i.e., the spo-ken words that are used to refer to the objects on thegraphic display).According to the time-stamp information, we can1http://audacity.sourceforge.net/measure the length of time gap between a user?s eyefixation falling on an object and the correspondingspoken reference being uttered (which we refer toas ?length of time gap?
for brevity).
Also, we cancount the number of times that user fixations hap-pen to change their target objects during this timegap (which we refer to as ?number of fixated objectchanges?
for brevity).
The nine most frequently oc-curred spoken references in utterances from all users(as shown in Table 1) are chosen for this empiricalstudy.
For each of those spoken references, we usehuman judgment to decide which object is referredto.
Then, from both before and after the onset ofthe spoken reference, we find the closest occurrenceof the fixation falling on that particular object.
Al-together we have 96 such speech-gaze pairs.
In 54pairs, the eye gaze fixation occurred before the cor-responding speech reference was uttered; and in theother 42 pairs, the eye fixation occurred after thecorresponding speech reference was uttered.
Thisobservation suggests that in human machine conver-sation, eye fixation on an object does not necessarilyalways proceed the utterance of the correspondingspeech reference.Further, we computed the average absolute lengthof the time gap and the average number of fixatedobject changes, as well as their variances for each of5 selected users2 as shown in Table 1.
From Table 1,it is easy to observe that: (I) A spoken reference al-ways appears within a short period of time (usually1-2 seconds) before or after the corresponding eyegaze fixation.
But, the exact length of the period isfar from constant.
(II) It is not necessary for a userto utter the corresponding spoken reference imme-diately before or after the eye gaze fixation falls onthat particular object.
Eye gaze fixations may moveback and forth.
Between the time an object is fixatedand the corresponding spoken reference is uttered, auser?s eye gaze may fixate on a few other objects(reflected by the average number of eye fixated ob-ject changes shown in the table).
(III) There is alarge variance in both the length of time gap and thenumber of fixated object changes in terms of 1) thesame user and the same spoken reference at differ-ent time-stamps, 2) the same user but different spo-2The other two users are not selected because the nine se-lected words do not appear frequently in their utterances.370Spoken Average Absolute Length of Time Gap (in seconds) Average Number of Eye Fixated Object ChangesReference User 1 User 2 User 3 User 4 User 5 User 1 User 2 User 3 User 4 User 5bed 1.27?
1.40 1.02?
0.65 0.32?
0.21 0.59?
0.77 2.57?
3.25 2.1?
3.2 2.1?
2.2 0.4?
0.5 1.4?
2.2 5.3?
7.9tree - 0.24?
0.24 - - - - 0.0?
0.0 - - -window - 0.67?
0.74 - - 1.95?
3.20 - 0.0?
0.0 - - 3.3?
5.9mirror - 1.04?
1.36 - - - - 1.0?
1.4 - - -candle - - 3.64?
0.59 - - - - 8.5?
2.1 - -waterfall 1.80?
1.12 - - - - 5.5?
4.9 - - - -painting 0.10?
0.10 - - - - 0.2?
0.4 - - - -lamp 0.74?
0.54 1.70?
0.99 0.26?
0.35 1.98?
1.72 2.84?
2.42 1.3?
1.3 1.8?
1.5 0.3?
0.6 4.8?
4.3 2.7?
2.2door 2.47?
0.84 - - 2.49?
1.90 6.36?
2.29 5.0?
2.6 - - 6.7?
5.5 13.3?
6.7Table 1: The average absolute length of time and the number of eye fixated object changes within the timegap of eye gaze and corresponding spoken references.
Variances are also listed.
Some of the entries are notavailable because the spoken references were never or rarely used by the corresponding users.ken references, and 3) the same spoken reference butdifferent users.
We believe this is due to the differentdialog scenarios and user language habits.To summarize our empirical study, we find thatin human machine conversation, there still exists anatural temporal coupling between user speech andeye gaze, i.e.
the spoken reference and the corre-sponding eye fixation happen within a close vicinityof each other.
However, a large variance is also ob-served in terms of these temporal vicinities, whichindicates an intrinsically more complex gaze-speechpattern.
Therefore, it is hard to directly quantifythe temporal or ordering relationship between spo-ken references and corresponding eye fixated objects(for example, through rules).To better handle the complexity in the gaze-speech pattern, we propose to use statistical transla-tion models.
Given a time window of enough length,a speech input that contains a list of spoken refer-ences (e.g., definite noun phrases) is always accom-panied by a list of naturally occurred eye fixationsand therefore a list of objects receiving those fixa-tions.
All those pairs of speech references and cor-responding fixated objects could be viewed as paral-lel, i.e.
they co-occur within the time window.
Thissituation is very similar to the training process oftranslation models in statistical machine translation(Brown et al, 1993), where parallel corpus is used tofind the mappings between words from different lan-guages by exploiting their co-occurrence patterns.The same idea can be borrowed here: by exploringthe co-occurrence statistics, we hope to uncover theexact mapping between those eye fixated objects andspoken references.
The intuition is that, the more of-ten a fixation is found to exclusively co-occur with aspoken reference, the more likely a mapping shouldbe established between them.5 Translation Models for VocabularyAcquisition and InterpretationFormally, we denote the set of observations byD = {wi,oi}Ni=1 where wi and oi refers tothe i-th speech utterance (i.e., a list of wordsof spoken references) and the i-th correspondingeye gaze pattern (i.e., a list of eye fixated ob-jects) respectively.
When we study the prob-lem of mapping given objects to words (for vo-cabulary acquisition), the parameter space ?
={Pr(wj |ok), 1 ?
j ?
mw, 1 ?
k ?
mo} consists ofthe mapping probabilities of an arbitrary word wjto an arbitrary object ok, where mw and mo repre-sent the total number of unique words and objectsrespectively.
Those mapping probabilities are sub-ject to constraints ?mwj=1 Pr(wj |ok) = 1.
Note thatPr(wj |ok) = 0 if the corresponding word wj and oknever co-occur in any observed list pair (wi,oi).Let lwi and loi denote the length of lists wi andoi respectively.
To distinguish with the notationswj and ok whose subscripts are indices for uniquewords and objects respectively, we use w?i,j to de-note the word in the j-th position of the list wi ando?i,k to denote the object in the k-th position of thelist oi.
In translation models, we assume that anyword in the list wi is mapped to an object in the cor-responding list oi or a null object (we reserve theposition 0 for it in every object list).
To denote allthe word-object mappings in the i-th list pair, we in-troduce an alignment vector ai, whose element ai,jtakes the value k if the word w?i,j is mapped to o?i,k.Then, the likelihood of the observations given the371parameters can be computed as followsPr(D;?)
=N?i=1Pr(wi|oi) =N?i=1?aiPr(wi,ai|oi)=N?i=1?aiPr(lwi |oi)(loi + 1)lwilwi?j=1Pr(w?i,j |o?ai,j )=N?i=1Pr(lwi |oi)(loi + 1)lwi?ailwi?j=1Pr(w?i,j |o?ai,j )Note that the following equation holds:lwi?j=1loi?k=0Pr(w?i,j |o?i,k) =loi?ai,1=1?
?
?loi?ai,lwi =1lwi?j=1Pr(w?i,j |o?ai,j )where the right-hand side is actually the expansionof?ai?lwij Pr(w?i,j |o?ai,j ).
Therefore, the likelihoodcan be simplified asPr(D;?)
=N?i=1Pr(lwi |oi)(loi + 1)lwilwi?j=1loi?k=0Pr(w?i,j |o?i,k)Switching to the notations wj and ok, we havePr(D;?
)=N?i=1Pr(lwi |oi)(loi + 1)lwimw?j=1[ mo?k=0Pr(wj |ok)?oi,k]?wi,jwhere ?wi,j = 1 if w?i,j ?
wi and ?wi,j = 0 otherwise,and ?oi,k = 1 if o?i,k ?
oi and ?oi,k = 0 otherwise.Finally, the translation model can be formalizedas the following optimization problemarg max?
log Pr(D;?
)s.t.mw?j=1Pr(wj |ok) = 1,?kThis optimization problem can be solved by the EMalgorithm (Brown et al, 1993).The above model is developed in the con-text of mapping given objects to words, i.e., itssolution yields a set of conditional probabilities{Pr(wj |ok),?j} for each object ok, indicating howlikely every word is mapped to it.
Similarly, wecan develop the model in the context of mappinggiven words to objects (for vocabulary interpreta-tion), whose solution leads to another set of prob-abilities {Pr(ok|wj),?k} for each word wj indicat-ing how likely every object is mapped to it.
In ourexperiments, both models are implemented and wewill present the results later.6 ExperimentsWe experimented our proposed statistical translationmodel on the collected data mentioned in Section 3.6.1 PreprocessingThe main purpose of preprocessing is to create a?parallel corpus?
for training a translation model.Here, the ?parallel corpus?
refers to a series ofspeech-gaze pairs, each of them consisting of a listof words from the spoken references in the user ut-terances and a list of objects that are fixated uponwithin the same time window.Specifically, we first transcribed the user speechinto scripts by automatic speech recognition soft-ware and then refined them manually.
A time-stampwas associated with each word in the speech script.Further, we detected long pauses in the speech scriptas splitting points to create time windows, since along pause usually marks the start of a sentencethat indicates a user?s attention shift.
In our exper-iment, we set the threshold of judging a long pauseto be 1 second.
From all the data gathered from 7users, we get 357 such time windows (which typi-cally contain 10-20 spoken words and 5-10 fixatedobject changes).Given a time window, we then found the objectsbeing fixated upon by eye gaze (represented by theirIDs as shown in Figure 1).
Considering that eye gazefixation could occur during the pauses in speech, weexpanded each time window by a fixed length at bothits start and end to find the fixations.
In our experi-ments, the expansion length is set to 0.5 seconds.Finally, we applied a part-of-speech tagger toeach sentence in the user script and only singled outnouns as potential spoken references in the word list.The Porter stemming algorithm was also used to getthe normalized forms of those nouns.The translation model was trained based on thispreprocessed parallel data.6.2 Evaluation MetricsAs described in Section 5, by using a statisticaltranslation model we can get a set of translationprobabilities, either from any given spoken word toall the objects, or from any given object to all thespoken words.
To evaluate the two sets of trans-lation probabilities, we use precision and recall as372#Rank Precision Recall #Rank Precision Recall1 0.6667 0.2593 6 0.2302 0.53702 0.4524 0.3519 7 0.2041 0.55563 0.3810 0.4444 8 0.1905 0.59264 0.3095 0.4815 9 0.1799 0.62965 0.2667 0.5185 10 0.1619 0.6296Table 2: Average precision/recall of mapping givenobjects to words (i.e., acquisition)#Rank Precision Recall #Rank Precision Recall1 0.7826 0.3214 6 0.3043 0.75002 0.5870 0.4821 7 0.2671 0.76793 0.4638 0.5714 8 0.2446 0.80364 0.3804 0.6250 9 0.2293 0.83935 0.3478 0.7143 10 0.2124 0.8571Table 3: Average precision/recall of mapping givenwords to objects.
(i.e., interpretation)evaluation metrics.Specifically, for a given object ok the trans-lation model will yield a set of probabilities{Pr(wj |ok),?j}.
We can sort the probabilities andget a ranked list.
Let us assume that we have theground truth about all the spoken words to whichthe given object should be mapped.
Then, at a givennumber n of top ranked words, the precision of map-ping the given object ok to words is defined as# words that ok is correctly mapped to# words that ok is mapped toand the recall is defined as# words that ok is correctly mapped to# words that ok should be mapped toAll the counting above is done within the top n rank.Therefore, we can get different precision/recall atdifferent ranks.
At each rank, the overall perfor-mance can be evaluated by averaging the preci-sion/recall for all the given objects.
Human judg-ment is used to decide whether an object-word map-ping is correct or not, as ground truth for evaluation.Similarly, based on the set of probabilities of map-ping a given object with spoken words, we canfind a ranked list of objects for a given word, i.e.{Pr(ok|wj),?k}.
Thus, at a given rank the preci-sion and recall of mapping a given word wj to ob-jects can be measured.6.3 Experiment ResultsVocabulary acquisition is the process of findingthe appropriate word(s) for any given object.
Forthe sake of statistical significance, our evaluation isdone on 21 objects that were mentioned at least 3times by the users.Table 2 gives the average precision/recall evalu-ated at the top 10 ranks.
As we can see, if we usethe most probable word acquired for each object,about 66.67% of them are appropriate.
With therank increasing, more and more appropriate wordscan be acquired.
About 62.96% of all the appropri-ate words are included within the top 10 probablewords found.
The results indicate that by using atranslation model, we can obtain the words that areused by the users to describe the objects with rea-sonable accuracy.Table 4 presents the top 3 most probable wordsfound for each object.
It shows that although theremay be more than one word appropriate to describea given object, those words with highest probabil-ities always suggest the most popular way of de-scribing the corresponding object among the users.For example, for the object with ID 26, the wordcandle gets a higher probability than the wordcandlestick, which is in accordance with ourobservation that in our user study, on most occasionsusers tend to use the word candle rather than theword candlestick.Vocabulary interpretation is the process of find-ing the appropriate object(s) for any given spokenword.
Out of 176 nouns in the user vocabulary,we only evaluate those used at least three times forstatistical significance concerns.
Further, abstractwords (such as reason, position) and generalwords (such as room, furniture) are not eval-uated since they do not refer to any particular objectsin the scene.
Finally, 23 nouns remain for evalua-tion.We manually enumerated all the object(s) thatthose 23 nouns refer to as the ground truth in ourevaluation.
Note that a given noun can possiblybe used to refer to multiple objects, such as lamp,since we have several lamps (with object ID 3, 8, 17,and 23) in the experiment setting, and bed, sincebed frame, bed spread, and pillows (with object ID19, 21, and 20 respectively) are all part of a bed.Also, an object can be referred to by multiple nouns.For example, the words painting, picture,or waterfall can all be used to refer to the ob-ject with ID 15.373Object Rank 1 Rank 2 Rank 31 paint (0.254) * wall (0.191) left (0.150)2 pictur (0.305) * girl (0.122) niagara (0.095) *3 wall (0.109) lamp (0.093) * floor (0.084)4 upsid (0.174) * left (0.151) * paint (0.149) *5 pictur (0.172) window (0.157) * wall (0.116)6 window (0.287) * curtain (0.115) pictur (0.076)7 chair (0.287) * tabl (0.088) bird (0.083)9 mirror (0.161) * dresser (0.137) bird (0.098) *12 room (0.131) lamp (0.127) left (0.069)14 hang (0.104) favourit (0.085) natur (0.064)15 thing (0.066) size (0.059) queen (0.057)16 paint (0.211) * pictur (0.116) * forest (0.076) *17 lamp (0.354) * end (0.154) tabl (0.097)18 bedroom (0.158) side (0.128) bed (0.104)19 bed (0.576) * room (0.059) candl (0.049)20 bed (0.396) * queen (0.211) * size (0.176)21 bed (0.180) * chair (0.097) orang (0.078)22 bed (0.282) door (0.235) * chair (0.128)25 chair (0.215) * bed (0.162) candlestick (0.124)26 candl (0.145) * chair (0.114) candlestick (0.092) *27 tree (0.246) * chair (0.107) floor (0.096)Table 4: Words found for given objects.
Each rowlists the top 3 most probable spoken words (beingstemmed) for the corresponding given object, withthe mapping probabilities in parentheses.
Asterisksindicate correctly identified spoken words.
Notethat some objects are heavily overlapped, so the cor-responding words are considered correct for all theoverlapping objects, such as bed being consideredcorrect for objects with ID 19, 20, and 21.Word Rank 1 Rank 2 Rank 3 Rank 4curtain 6 (0.305) * 5 (0.305) * 7 (0.133) 1 (0.121)candlestick 25 (0.147) * 28 (0.135) 24 (0.131) 22 (0.117)lamp 22 (0.126) 12 (0.094) 17 (0.093) * 25 (0.093)dresser 12 (0.298) * 9 (0.294) * 13 (0.173) * 7 (0.104)queen 20 (0.187) * 21 (0.182) * 22 (0.136) 19 (0.136) *door 22 (0.200) * 27 (0.124) 25 (0.108) 24 (0.106)tabl 9 (0.152) * 12 (0.125) * 13 (0.112) * 22 (0.107)mirror 9 (0.251) * 12 (0.238) 8 (0.109) 13 (0.081)girl 2 (0.173) 22 (0.128) 16 (0.099) 10 (0.074)chair 22 (0.132) 25 (0.099) * 28 (0.085) 24 (0.082)waterfal 6 (0.226) 5 (0.215) 1 (0.118) 9 (0.083)candl 19 (0.156) 22 (0.139) 28 (0.134) 24 (0.131)niagara 4 (0.359) * 2 (0.262) * 1 (0.226) 7 (0.045)plant 27 (0.230) * 22 (0.181) 23 (0.131) 28 (0.117)tree 27 (0.352) * 22 (0.218) 26 (0.100) 13 (0.062)upsid 4 (0.204) * 12 (0.188) 9 (0.153) 1 (0.104) *bird 9 (0.142) * 10 (0.138) 12 (0.131) 7 (0.121)desk 12 (0.170) * 9 (0.141) * 19 (0.118) 8 (0.118)bed 19 (0.207) * 22 (0.141) 20 (0.111) * 28 (0.090)upsidedown 4 (0.243) * 3 (0.219) 6 (0.203) 5 (0.188)paint 4 (0.188) * 16 (0.148) * 1 (0.137) * 15 (0.118) *window 6 (0.305) * 5 (0.290) * 3 (0.085) 22 (0.065)lampshad 3 (0.223) * 7 (0.137) 11 (0.137) 10 (0.137)Table 5: Objects found for given words.
Each rowlists the 4 most probable object IDs for the corre-sponding given words (being stemmed), with themapping probabilities in parentheses.
Asterisks in-dicate correctly identified objects.
Note that someobjects are heavily overlapped, such as the candle(with object ID 26) and the chair (with object ID25), and both were considered correct for the re-spective spoken words.Table 3 gives the average precision/recall evalu-ated at the top 10 ranks.
As we can see, if we use themost probable object found for each speech word,about 78.26% of them are appropriate.
With the rankincreasing, more and more appropriate objects canbe found.
About 85.71% of all the appropriate ob-jects are included within the top 10 probable objectsfound.
The results indicate that by using a trans-lation model, we can predict the objects from userspoken words with reasonable accuracy.Table 5 lists the top 4 probable objects found foreach spoken word being evaluated.
A close look re-veals that in general, the top ranked objects tend togather around the correct object for a given spokenword.
This is consistent with the fact that eye gazetends to move back and forth.
It also indicates thatthe mappings established by the translation modelcan effectively find the approximate area of the cor-responding fixated object, even if it cannot find theobject due to the noisy and jerky nature of eye gaze.The precision/recall in vocabulary acquisition isnot as high as that in vocabulary interpretation, par-tially due to the relatively small scale of our exper-iment data.
For example, with only 7 users?
speechdata on 14 conversational tasks, some words wereonly spoken a few times to refer to an object, whichprevented them from getting a significant portion ofprobability mass among all the words in the vocab-ulary.
This degrades both precision and recall.
Webelieve that in large scale experiments or real-worldapplications, the performance will be improved.7 Discussion and ConclusionPrevious psycholinguistic findings have shown thateye gaze is tightly linked with human language pro-duction.
During human machine conversation, ourstudy shows that although a larger variance is ob-served on how eye fixations are exactly linked withcorresponding spoken references (compared to thepsycholinguistic findings), eye gaze in general isclosely coupled with corresponding referring ex-pressions in the utterances.
This close coupling na-ture between eye gaze and speech utterances pro-vides an opportunity for the system to automatically374acquire different words related to different objectswithout any human supervision.
To further explorethis idea, we developed a novel unsupervised ap-proach using statistical translation models.Our experimental results have shown that this ap-proach can reasonably uncover the mappings be-tween words and objects on the graphical display.The main advantages of this approach include: 1) Itis an unsupervised approach with minimum humaninference; 2) It does not need any prior knowledge totrain a statistical translation model; 3) It yields prob-abilities that indicate the reliability of the mappings.Certainly, our current approach is built upon sim-plified assumptions.
It is quite challenging to in-corporate eye gaze information since it is extremelynoisy with large variances.
Recent work has shownthat the effect of eye gaze in facilitating spoken lan-guage processing varies among different users (Quand Chai, 2007).
In addition, visual properties ofthe interface also affect user gaze behavior and thusinfluence the predication of attention (Prasov et al,2007) based on eye gaze.
Our future work will de-velop models to address these variations.Nevertheless, the results from our current workhave several important implications in building ro-bust conversational interfaces.
First of all, mostconversational systems are built with static knowl-edge space (e.g., vocabularies) and can only be up-dated by the system developers.
Our approach canpotentially allow the system to automatically ac-quire knowledge and vocabularies based on the nat-ural interactions with the users without human in-tervention.
Furthermore, the automatically acquiredmappings between words and objects can also helplanguage interpretation tasks such as reference res-olution.
Given the recent advances in eye track-ing technology (Duchowski, 2002), integrating non-intrusive and high performance eye trackers withconversational interfaces becomes feasible.
Thework reported here can potentially be integrated inpractical systems to improve the overall robustnessof human machine conversation.AcknowledgmentThis work was supported by funding from NationalScience Foundation (IIS-0347548, IIS-0535112,and IIS-0643494) and Disruptive Technology Of-fice.
The authors would like to thank Zahar Prasovfor his contribution to data collection.ReferencesP.
F. Brown, S. A. Della Pietra, V. J. Della Pietra, andR.
L. Mercer.
1993.
The mathematics of statisticalmachine translation: Parameter estimation.
Computa-tional Linguistics, 19(2):263?311.E.
Campana, J. Baldridge, J. Dowding, B.
A. Hockey,R.
Remington, and L. S. Stone.
2001.
Using eyemovements to determine referents in a spoken dialogsystem.
In Proceedings of PUI?01.A.
T. Duchowski.
2002.
A breath-first survey of eyetracking applications.
Behavior Research methods, In-struments, and Computers, 33(4).Z.
M. Griffin and K. Bock.
2000.
What the eyes sayabout speaking.
Psychological Science, 11:274?279.M.
A.
Just and P. A. Carpenter.
1976.
Eye fixations andcognitive processes.
Cognitive Psychology, 8:441?480.M.
Kaur, M. Tremaine, N. Huang, J. Wilder, Z. Gacovski,F.
Flippo, and C. S. Mantravadi.
2003.
Where is ?it?
?Event synchronization in gaze-speech input systems.In Proceedings of ICMI?03, pages 151?157.Z.
Prasov, J. Y. Chai, and H. Jeong.
2007.
Eye gazefor attention prediction in multimodal human-machineconversation.
In 2007 Spring Symposium on Inter-action Challenges for Artificial Assistants, Palo Alto,California, March.S.
Qu and J. Y. Chai.
2007.
An exploration of eye gazein spoken language processing for multimodal con-versational interfaces.
In NAACL?07, pages 284?291,Rochester, New York, April.D.
Roy and A. Pentland.
2002.
Learning words fromsights and sounds, a computational model.
CognitiveScience, 26(1):113?1146.J.
M. Siskind.
1995.
Grounding language in perception.Artificial Intelligence Review, 8:371?391.K.
Tanaka.
1999.
A robust selection system using real-time multi-modal user-agent interactions.
In Proceed-ings of IUI?99, pages 105?108.M.
K. Tenenhaus, M. Sivey-Knowlton, E. Eberhard, andJ.
Sedivy.
1995.
Integration of visual and linguisticinformation during spoken language comprehension.Science, 268:1632?1634.C.
Yu and D. H. Ballard.
2004.
On the integration ofgrounding language and learning objects.
Proceedingsof AAAI?04.375
