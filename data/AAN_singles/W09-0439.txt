Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 242?249,Athens, Greece, 30 March ?
31 March 2009. c?2009 Association for Computational LinguisticsStabilizing Minimum Error Rate TrainingGeorge Foster and Roland KuhnNational Research Council Canadafirst.last@nrc.gc.caAbstractThe most commonly used method fortraining feature weights in statistical ma-chine translation (SMT) systems is Och?sminimum error rate training (MERT) pro-cedure.
A well-known problemwith Och?sprocedure is that it tends to be sensitiveto small changes in the system, particu-larly when the number of features is large.In this paper, we quantify the stabilityof Och?s procedure by supplying differentrandom seeds to a core component of theprocedure (Powell?s algorithm).
We showthat for systems with many features, thereis extensive variation in outcomes, both onthe development data and on the test data.We analyze the causes of this variation andpropose modifications to the MERT proce-dure that improve stability while helpingperformance on test data.1 IntroductionMost recent approaches in SMT, eg (Koehn et al,2003; Chiang, 2005), use a log-linear model tocombine probabilistic features.
Minimum Error-Rate Training (MERT) aims to find the set of log-linear weights that yields the best translation per-formance on a development corpus according tosome metric such as BLEU.
This is an essen-tial step in SMT training that can significantlyimprove performance on a test corpus comparedto setting weights by hand.
MERT is a difficultproblem, however, because calculating BLEU as afunction of log-linear weights requires decoding,which is an expensive operation.
Moreover, be-cause this function is not differentiable, efficientgradient-based optimization algorithms cannot beused.Och?s procedure is the most widely-used ver-sion of MERT for SMT (Och, 2003).
To reducecomputational cost, it relies on the key techniqueof optimizing weights over n-best lists of transla-tion hypotheses rather than over all possible hy-potheses.
This allows the most probable hypoth-esis under a given set of weights?and the corre-sponding BLEU score?to be found by enumer-ating n-best entries rather than decoding.
Somevariant on Powell?s algorithm (Press et al, 2002)is typically used to maximize BLEU in this set-ting.
The n-best lists are constructed by alternat-ing decoding and BLEU maximization operations:decoding adds new hypotheses to the current lists,then BLEU is maximized over the lists to find newbest weights for the subsequent decoding step, etc.This process continues until no new hypothesesare found.Och?s procedure works well in practice, usuallyconverging after 10?20 calls to the decoder, farfewer than would be required to maximize BLEUdirectly with a general-purpose optimization algo-rithm.
However, it tends to be sensitive to smallchanges in the system, particularly for large fea-ture sets.
This is a well-known problem withOch?s procedure (Och et al, 2004).
It makes itdifficult to assess the contribution of features, be-cause the measured gain in performance due to anew feature can depend heavily on the setting ofsome apparently unrelated parameter such as thesize of n-best list used.
Features with the poten-tial for statistically significant gains may be re-jected because Och?s procedure failed to find goodweights for them.In this paper we attempt to quantify the stabil-ity of Och?s procedure under different conditionsby measuring the variation in test-set scores acrossdifferent random seeds used with Powell?s algo-rithm.
We show that there is extensive variationfor large feature sets, and that it is due to two mainfactors: the occasional failure of Och?s procedureto find a good maximum on the development set,and the failure of some maxima to generalize to242the test set.
We analyze the causes of each of theseproblems, and propose solutions for improving thestability of the overall procedure.2 Previous WorkOne possible approach to estimating log-linearweights on features is to dispense with the n-bestlists employed by Och?s procedure and, instead,to optimize weights by directly accessing the de-coder.
The disadvantage of this approach is thatfar more iterations of decoding of the full devel-opment set are required.
In (Zens and Ney, 2004)the downhill simplex method is used to estimatethe weights; around 200 iterations are required forconvergence to occur.
However, each iteration isunusually fast, because only monotone decodingis permitted (i.e., the order of phrases in the tar-get language mirrors that in the source language).Similarly, Cettolo and Federico (2004) apply thesimplex method to optimize weights directly usingthe decoder.
In their experiments on NIST 2003Chinese-English data, they found about 100 iter-ations of decoding were required.
Although theyobtained consistent and stable performance gainsfor MT, these were inferior to the gains yieldedby Och?s procedure in (Och, 2003).
Taking Och?sMERT procedure as a baseline, (Zens et al, 2007)experiment with different training criteria for SMTand obtain the best results for a criterion they call?expected BLEU score?.Moore and Quirk (2008) share the goal under-lying our own research: improving, rather thanreplacing, Och?s MERT procedure.
They focuson the step in the procedure where the set of fea-ture weights optimizing BLEU (or some other MTmetric) for an n-best list is estimated.
Typically,several different starting points are tried for thisset of weights; often, one of the starting points isthe best set of weights found for the previous setof n-best hypotheses.
The other starting points areoften chosen randomly.
In this paper, Moore andQuirk look at the best way of generating the ran-dom starting points; they find that starting pointsgenerated by a random walk from previous max-ima are superior to those generated from a uni-form distribution.
The criterion used throughoutthe paper to judge the performance of MERT is theBLEU score on the development test set (ratherthan, for instance, the variance of that score, orthe BLEU score on held-out test data).
Anothercontribution of the paper is ingenious methods forpruning the set of n-best hypotheses at each itera-tion.Cer et al(2008) also aim at improving Och?sMERT.
They focus on the search for the best setof weights for an n-best list that follows choiceof a starting point.
They propose a modified ver-sion of Powell?s in which ?diagonal?
directionsare chosen at random.
They also modify the ob-jective function used by Powell?s to reflect thewidth of the optima found.
They are able to showthat their modified version of MERT outperformsboth a version using Powell?s, and a more heuris-tic search algorithm devised by Philipp Koehnthat they call Koehn Coordinate Descent, as mea-sured on the development set and two test datasets.
(Duh and Kirchhoff, 2008) ingeniously usesMERT as a weak learner in a boosting algorithmthat is applied to the n-best reranking task, withgood results (a gain of about 0.8 BLEU on the testset).Recently, some interesting work has been doneon what might be considered a generalization ofOch?s procedure (Macherey et al, 2008).
In thisgeneralization, candidate hypotheses in each iter-ation of the procedure are represented as lattices,rather than as n-best lists.
This makes it possi-ble for a far greater proportion of the search spaceto be represented: a graph density of 40 arcs perphrase was used, which corresponds to an n-bestsize of more than two octillion (2 ?
1027) entries.Experimental results for three NIST 2008 taskswere very encouraging: though BLEU scores forthe lattice variant of Och?s procedure did not typ-ically exceed those for the n-best variant on de-velopment data, on test data the lattice variant out-performed the n-best approach by between 0.6 and2.5 BLEU points.
The convergence behaviour ofthe lattice variant was also much smoother thanthat of the n-best variant.
It would be interestingto apply some of the insights of the current paperto the lattice variant of Och?s procedure.3 Och?s MERT ProcedureOch?s procedure works as follows.
First the de-coder is run using an initial set of weights to gen-erate n best translations (usually around 100) foreach source sentence.
These are added to exist-ing n-best lists (initially empty).
Next, Powell?salgorithm is used to find the weights that maxi-mize BLEU score when used to choose the besthypotheses from the n-best lists.
These weights243are plugged back into the decoder, and the pro-cess repeats, nominally until the n-best lists stopgrowing, but often in practice until some criterionof convergence such as minimum weight changeis attained.
The weights that give the best BLEUscore when used with the decoder are output.The point of this procedure is to bypass di-rect search for the weights that result in maxi-mum BLEU score, which would involve decodingusing many different sets of weights in order tofind which ones gave the best translations.
Och?sprocedure typically runs the decoder only 10?20times, which is probably at least one order of mag-nitude fewer than a direct approach.
The maintrick is to build up n-best lists that are represen-tative of the search space, in the sense that a givenset of weights will give approximately the sameBLEU score when used to choose the best hy-potheses from the n-best lists as it would when de-coding.
By iterating, the algorithm avoids weightsthat give good scores on the n-best lists but badones with the decoder, since the bad hypothesesthat are scored highly by such weights will getadded to the n-best lists, thereby preventing thechoice of these weights in future iterations.
Unfor-tunately, there is no corresponding guarantee thatweights which give good scores with the decoderbut bad ones on the nbest lists will get chosen.Finding the set of weights that maximizesBLEU score over n-best lists is a relatively easyproblem because candidate weight sets can beevaluated in time proportional to n (simply cal-culate the score of each hypothesis according tothe current weight set, then measure BLEU on thehighest scoring hypothesis for each source sen-tence).
Powell?s algorithm basically loops overeach feature in turn, setting its weight to an op-timum value before moving on.1 Och?s linemaxalgorithm is used to perform this optimization effi-ciently and exactly.
However this does not guaran-tee that Powell?s algorithm will find a global max-imum, and so Powell?s is typically run with manydifferent randomly-chosen initial weights in orderto try to find a good maximum.4 Experimental SetupThe experiments described here were carried outwith a standard phrase-based SMT system (Koehn1It can also choose to optimize linear combinations ofweights in order to avoid ridges that are not aligned with theoriginal coordinates, which can be done just as easily.corpus num sents num Chinese toksdev1 1506 38,312dev2 2080 55,159nist04 1788 53,446nist06 1664 41,798Table 1: Development and test corpora.et al, 2003) employing a log-linear combinationof feature functions.
HMM and IBM2 modelswere used to perform separate word alignments,which were symmetrized by the usual ?diag-and?algorithm prior to phrase extraction.
Decodingused beam search with the cube pruning algorithm(Huang and Chiang, 2007).We used two separate log-linear models forMERT:?
large: 16 phrase-table features, 2 4-gram lan-guage model features, 1 distortion feature,and 1 word-count feature (20 features in to-tal).?
small: 2 phrase-table features, 1 4-gram lan-guage model feature, 1 distortion feature, and1 word-count feature (5 features in total).The phrase-table features for the large model werederived as follows.
Globally-trained HMM andIBM2 models were each used to extract phrasesfrom UN and non-UN portions of the training cor-pora (see below).
This produced four separatephrase tables, each of which was used to generateboth relative-frequency and ?lexical?
conditionalphrase-pair probabilities in both directions (targetgiven source and vice versa).
The two languagemodel features in the large log-linear model weretrained on the UN and non-UN corpora.
Phrase-table features for the small model were derived bytaking the union of the four individual tables, sum-ming joint counts, then calculating relative fre-quencies.All experiments were run using the Chi-nese/English data made available for NIST?s 2008MT evaluation.
This included approximately 5Msentence pairs of data from the UN corpus, andapproximatel 4M sentence pairs of other mate-rial.
The English Gigaword corpus was not usedfor language model training.
Two separate devel-opment corpora were derived from a mix of theNIST 2005 evaluation set and some webtext drawnfrom the training material (disjoint from the train-ing set used).
The evaluation sets for NIST 2004244cfg nist04 nist06avg ?
S avg ?
SS1 31.17 1.09 0.28 26.95 0.90 0.27S2 31.44 0.22 0.07 27.38 0.71 0.19L1 33.03 1.09 0.37 29.22 0.97 0.34L2 33.37 1.49 0.49 29.61 2.14 0.66Table 2: Test-set BLEU score variation with 10different random seeds, for small (S) and large (L)models on dev sets 1 and 2.
The avg column givesthe average BLEU score over the 10 runs; ?
givesthe difference between the maximum and mini-mum scores, and S is the standard deviation.and NIST 2005 corpora were used for testing.
Ta-ble 1 summarizes the sizes of the devtest corpora,all of which have four reference translations.5 Measuring the Stability of Och?sAlgorithmTo gauge the response of Och?s algorithm to smallchanges in system configuration, we varied theseed value for initializing the random number gen-erator used to produce random starting points forPowell?s algorithm.
For each of 10 different seedvalues, Och?s algorithm was run for a maximum of30 iterations2 using 100-best lists.
Table 2 showsthe results for the two different log-linear modelsdescribed in the previous section.The two development sets exhibit a similar pat-tern: the small models appear to be somewhatmore stable, but all models show considerablevariation in test-set BLEU scores.
For the largemodels, the average difference between best andworst BLEU scores is almost 1.5% absolute, withan average standard deviation of almost 0.5%.Differences of as little as 0.35% are significant ata 95% confidence level according to paired boot-strap resampling tests on this data, so these varia-tions are much too large to be ignored.The variation in table 2 might result from Och?salgorithm failing to maximize development-setBLEU properly on certain runs.
Alternatively, itcould be finding different maxima that vary in theextent to which they generalize to the test sets.Both of these factors appear to play a role.
Theranges of BLEU scores on the two developmentcorpora with the large models are 0.86 and 1.3 re-spectively; the corresponding standard deviations2Sufficient for effective convergence in all cases wetested.dev nist04 nist06 inter?
r ?
r ?dev1 0.18 0.42 -0.27 0.07 0.73dev2 0.55 0.60 0.73 0.85 0.94Table 3: Pearson (?)
and Spearman rank (r) cor-relation between dev-set and test-set BLEU scoresfor the large log-linear model.
The final columnshows nist04/nist06 correlation.are 0.27 and 0.38.
Different runs clearly have sig-nificantly different degrees of success in maximiz-ing BLEU.To test whether the variation in development-set BLEU scores accounts completely for the vari-ation in test-set scores, we measured the correla-tion between them.
The results in table 3 showthat this varies considerably across the two de-velopment and test corpora.
Although the rankcorrelation is always positive and is in somecases quite high, there are many examples wherehigher development-set scores lead to lower test-set scores.
Interestingly, the correlation betweenthe two test-set scores (shown in the last column ofthe table) is much higher than that between the de-velopment and test sets.
Since the test sets are notparticularly similar to each other, this suggests thatsome sets of log-linear weights are in fact overfit-ting the development corpus.5.1 Bootstrapping with Random SeedsThe results above indicate that the stability prob-lems with Och?s MERT can be quite severe, es-pecially when tuning weights for a fairly largenumber of features.
However, they also consti-tute a baseline solution to these problems: runMERT some number of times with different ran-dom seeds, then choose the run that achieves thehighest BLEU score on a test set.
Since test-set scores are highly correlated, these weights arelikely to generalize well to new data.
Applyingthis procedure using the nist04 corpus to chooseweights yields a BLEU increase of 0.69 on nist06compared to the average value over the 10 runs intable 2; operating in the reverse direction gives anincrease of 0.37 on nist04.33These increases are averages over the increases on eachdevelopment set.
This comparison is not strictly fair to thebaseline single-MERT procedure, since it relies on a test setfor model selection (using the development set would haveyielded gains of 0.25 for nist06 and 0.27 for nist04).
How-ever, it is fairly typical to select models (involving differentfeature sets, etc) using a test set, for later evaluation on a2452929.229.429.629.83030.230.430.61  2  3  4  5  6  7  8  9  10number of runsNIST06 BLEU scores versus number of random runsdev2dev1Figure 1: Results on the nist06 test corpus, usingnist04 to choose best weights from varying num-bers of MERT runs, averaged over 1000 randomdraws.
The error bars indicate the magnitude ofthe standard deviation.An obvious drawback to this technique is thatit requires the expensive MERT procedure to berun many times.
To measure the potential gainfrom using fewer runs, and to estimate the stabilityof the procedure, we used a bootstrap simulation.For each development set and each n from 1 to 10,we randomly drew 1000 sets of n runs from thedata used for table 2, then recorded the behaviourof the nist06 scores that corresponded to the bestnist04 score.
The results are plotted in figure 1.There is no obvious optimal point on the curves,although 7 runs would be required to reduce thestandard deviation on dev2 (the set with the highervariance) below 0.35.
In the following sectionswe evaluate some alternatives that are less com-putationally expensive.
The large model setting isassumed throughout.6 Improving MaximizationIn this section we address the problem of improv-ing the maximization procedure over the devel-opment corpus.
In general, we expect that beingable to consistently find higher maxima will leadto lower variance in test-set scores.
Previous work,eg (Moore and Quirk, 2008; Cer et al, 2008), hasfocused on improving the performance of Powell?salgorithm.
The degree to which this is effective de-pends on how good an approximation the currentn-best lists are to the true search space.
As illus-second, blind, test set.
A multi-MERT strategy could be nat-urally incorporated into such a regime, and seems unlikely togive rise to substantial bias.A B CFigure 2: True objective function (bold curve)compared to n-best approximation (light curve).Och?s algorithm can correct for false maxima likeB by adding hypotheses to n-best lists, but maynot find the true global maximum (C), convergingto local peaks like A instead.24681012141618201  2  3  4  5  6  7  8  9  10  11  12  13  14  15BLEUiterBLEU scores versus Och iterationbest runworst runFigure 3: Development-set BLEU scores aftereach Och iteration for two different training runson the dev2 corpus.trated in figure 2, it is possible for the true space tocontain maxima that are absent from the approxi-mate (n-best) space.
Figure 3 gives some evidencethat this happens in practice.
It shows the evolu-tion of decoder BLEU scores with iteration for thebest and worst runs for dev2.
Although the worstrun explores a somewhat promising area at itera-tion 7, it converges soon afterwards in a region thatgives lower true BLEU scores.
This is not due toa failure of Powell?s algorithm, since the scores onthe n-best lists rise monotonically in this range.We explored various simple strategies for avoid-ing the kind of local-maximum behaviour exhib-ited in figure 3.
These are orthogonal to improve-ments to Powell?s algorithm, which was used inits standard form.
Our baseline implementation ofOch?s algorithm calls Powell?s three times start-ing with each of the three best weight sets fromthe previous iteration, then a certain number oftimes with randomly-generated weights.
The to-tal number of Powell?s calls is determined by analgorithm that tries to minimize the probability of246a new starting point producing a better maximum.4The first strategy was simply to re-seed the ran-dom number generator (based on a given globalseed value) for each iteration of Och?s algorithm.Our implementation had previously re-used thesame ?random?
starting points for Powell?s acrossdifferent Och iterations.
This is arguably justifi-able on the grounds that the function to be opti-mized is different each time.The second strategy was motivated by the ob-servation that after the first several iterations ofOch?s algorithm, the starting point that leads tothe best Powell?s result is nearly always one ofthe three previous best weight sets rather than arandomly-generated set.
To encourage the algo-rithm to consider other alternatives, we used thethree best results from all previous Och?s itera-tions.
That is, on iteration n, Powell?s is startedwith the three best results from iteration n?1, thenthe three best from n?2, and so forth.
If more than3(n ?
1) points are required by the stopping al-gorithm described above, then they are generatedrandomly.The final strategy is more explicitly aimed atforcing the algorithm to cover a broader por-tion of the search space.
Rather than choosingthe maximum-BLEU results from Powell?s algo-rithm for the subsequent decoding step, we chooseweight vectors that yield high BLEU scores andare dissimilar from previous decoding weights.Formally:??
= argmax?
?Pw rbleu(?)
+ (1?
w) rdist(?
),where P is the set of all weight vectors returnedby Powell?s on the current iteration, rbleu(?)
is?
?s BLEU score divided by the highest score forany vector in P , and rdist(?)
is ?
?s distance toprevious weights divided by the largest distancefor any vector in P .
Distance to previous weightsis measured by taking the minimum L2 distancefrom ?
to any of the decoding weight vectors usedduring the previous m Och iterations.Intuitively, the weight w that controls the im-portance of BLEU score relative to novelty shouldincrease gradually as Och?s algorithm progressesin order to focus the search on the best maxi-4Whenever a new maximum is encountered, at least thecurrent number of new starting points must be tried beforestopping, with a minimum of 10 points in total.
Experimentswhere the total number of starts was fixed at 30 did not pro-duce significantly different results.mum found (roughly similar to simulated anneal-ing search).
To accomplish this, w is defined as:w = 1?
a/(iter + b),where b ?
0 and a ?
b + 1 are parameters thatcontrol w?s decay, and iter is the current Och iter-ation.Each of the three strategies outlined above wasrun using 10 random seeds with both developmentcorpora.
The weight selection strategy was runwith two different sets of values for the a and bparameters: a = 1, b = 1 and a = 5, b = 9.
Eachassigns equal weight to BLEU score and noveltyon the first iteration, but under the first parameter-ization the weight on novelty decays more swiftly,to 0.03 by the final iteration compared to to 0.13.The results are shown in table 4.
The best strat-egy overall appears to be a combination of all threetechniques outlined above.
Under the a = 5,b = 9, m = 3 parametrization for the final (weightselection) strategy, this improves the developmentset scores by an average of approximately 0.4%BLEU compared to the baseline, while signifi-cantly reducing the variation across different runs.Performance of weight selection appears to bequite insensitive to its parameters: there is no sig-nificant difference between the a = 1, b = 1 anda = 5, b = 9 settings.
It is possible that furthertuning of these parameters would yield better re-sults, but this is an expensive procedure; we werealso wary of overfitting.
A good fallback is thefirst two strategies, which together achieve resultsthat are almost equivalent to the final gains due toweight selection.7 GeneralizationAs demonstrated in section 5, better performanceon the development set does not necessarily leadto better performance on the test set: two weightvectors that give approximately the same dev-setBLEU score can give very different test-set scores.We investigated several vectors with this charac-teristic from the experiments described above, butwere unable to find any intrinsic property that wasa good predictor of test-set performance, perhapsdue to the fact that the weights are scale invari-ant.
We also tried averaging BLEU over boot-strapped samples of the development corpora, butthis was also not convincingly correlated with test-set BLEU.247strategy dev avg ?
Sbaseline 1 22.64 0.87 0.272 19.11 1.31 0.38re-seed 1 22.87 0.65 0.212 19.37 0.60 0.17+history 1 22.99 0.43 0.152 19.44 0.35 0.11+sel 1,1,3 1 23.12 0.59 0.192 19.53 0.38 0.13+sel 5,9,3 1 23.11 0.42 0.132 19.46 0.44 0.14Table 4: Performance of various strategies for im-proving maximization on the dev corpora: base-line is the baseline used in section 5; re-seed israndom generator re-seeding; history is accumu-lation of previous best weights as starting point;and sel a,b,m is the final, weight selection, strat-egy described in section 6, parameterized by a, b,and m. Strategies are applied cumulatively, as in-dicated by the + signs.An alternate approach was inspired by the reg-ularization method described in (Cer et al, 2008).In essence, this uses the average BLEU score fromthe points close to a given maximum as a surro-gate for the BLEU at the maximum, in order topenalize maxima that are ?narrow?
and thereforemore likely to be spurious.
While Cer et aluse thistechnique while maximizing along a single dimen-sion within Powell?s algorithm, we apply it overall dimensions with the vectors output from Pow-ell?s.
Each individual weight is perturbed accord-ing to a normal distribution (with variance 1e-03),then the resulting vector is used to calculate BLEUover the n-best lists.
The average score over 10such perturbed vectors is used to calculate rbleuin the weight-selection method from the previoussection.The results from regularized weight selectionare compared to standard weight selection and tothe baseline MERT algorithm in table 5.
Regu-larization appears to have very little effect on theweight selection approach.
This does not neces-sarily contradict the results of Cer et al since it isapplied in a very different setting.
The standardweight selection technique (in combination withthe re-seeding and history accumulation strate-gies) gives a systematic improvement in averagetest-set BLEU score over the baseline, although itdoes not substantially reduce variance.strategy dev test avg ?
Sbaseline 1 04 33.03 1.09 0.3706 29.22 0.97 0.342 04 33.37 1.49 0.4906 29.61 2.14 0.66(+) sel 5,9,3 1 04 33.43 1.23 0.4106 29.62 0.98 0.312 04 33.95 1.03 0.3706 30.32 0.88 0.30+ reg 10 1 04 33.36 1.45 0.4906 29.56 1.25 0.392 04 33.81 0.94 0.2806 30.17 1.21 0.35Table 5: Performance of various MERT tech-niques on the test corpora.
(+) sel 5,9,3 is the sameconfiguration as +sel 5,9,3 in table 4; + reg 10uses regularized BLEU within this procedure.8 ConclusionIn this paper, we have investigated the stabilityof Och?s MERT algorithm using different randomseeds within Powell?s algorithm to simulate theeffect of small changes to a system.
We foundthat test-set BLEU scores can vary by 1 percentor more across 10 runs of Och?s algorithm withdifferent random seeds.
Using a bootstrap analy-sis, we demonstrate that an effective, though ex-pensive, way to stabilize MERT would be to run itmany times (at least 7), then choose the weightsthat give best results on a held-out corpus.
Wepropose less expensive simple strategies for avoid-ing local maxima that systematically improve test-set BLEU scores averaged over 10 MERT runs, aswell as reducing their variance in some cases.
Anattempt to improve on these strategies by regular-izing BLEU was not effective.In future work, we plan to integrate improvedvariants on Powell?s algorithm, which are orthog-onal to the investigations reported here.9 AcknowlegementThis material is partly based upon work supportedby the Defense Advanced Research ProjectsAgency (DARPA) under Contract No.
HR0011-06-C-0023.
Any opinions, findings and conclu-sions or recommendations expressed in this ma-terial are those of the authors and do not neces-sarily reflect the views of the Defense AdvancedResearch Projects Agency (DARPA).248ReferencesDaniel Cer, Daniel Jurafsky, and Christopher D. Man-ning.
2008.
Regularization and search for minimumerror rate training.
In Proceedings of the ACL Work-shop on Statistical Machine Translation, Columbus,June.
WMT.Mauro Cettolo and Marcello Federico.
2004.
Min-imum error training of log-linear translation mod-els.
In International Workshop on Spoken LanguageTranslation, Kyoto, September.David Chiang.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In Pro-ceedings of the 43th Annual Meeting of the Associ-ation for Computational Linguistics (ACL), Ann Ar-bor, Michigan, July.Kevin Duh and Katrin Kirchhoff.
2008.
Beyond log-linear models: Boosted minimum error rate trainingfor n-best re-ranking.
In Proceedings of the 46th An-nual Meeting of the Association for ComputationalLinguistics (ACL), Columbus, Ohio, June.Liang Huang and David Chiang.
2007.
Forest rescor-ing: Faster decoding with integrated language mod-els.
In Proceedings of the 45th Annual Meetingof the Association for Computational Linguistics(ACL), Prague, Czech Republic, June.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Ed-uard Hovy, editor, Proceedings of the Human Lan-guage Technology Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics, pages 127?133, Edmonton, Alberta,Canada, May.
NAACL.Wolfgang Macherey, Franz Josef Och, Ignacio Thayer,and Jakob Uszkoreit.
2008.
Lattice-based minimumerror rate training for statistical machine transla-tion.
In Proceedings of the 2008 Conference on Em-pirical Methods in Natural Language Processing(EMNLP), Honolulu.Robert C. Moore and Chris Quirk.
2008.
Randomrestarts in minimum error rate training for statisti-cal machine translation.
In Proceedings of the Inter-national Conference on Computational Linguistics(COLING) 2008, Manchester, August.Franz Josef Och, Daniel Gildea, and Sanjeev Khudan-pur et al 2004.
Final report of johns hopkins2003 summer workshop on syntax for statistical ma-chine translation (revised version).
Technical report,February 25.Franz Josef Och.
2003.
Minimum error rate trainingfor statistical machine translation.
In Proceedingsof the 41th Annual Meeting of the Association forComputational Linguistics (ACL), Sapporo, July.William H. Press, Saul A. Teukolsky, William T. Vet-terling, and Brian P. Flannery.
2002.
NumericalRecipes in C++.
Cambridge University Press, Cam-bridge, UK.Richard Zens and Hermann Ney.
2004.
Improve-ments in phrase-based statistical machine transla-tion.
In Proceedings of Human Language Technol-ogy Conference / North American Chapter of theACL, Boston, May.Richard Zens, Sasa Hasan, and Hermann Ney.
2007.A systematic comparison of training criteria for sta-tistical machine translation.
In Proceedings of the2007 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), Prague, Czech Re-public.249
