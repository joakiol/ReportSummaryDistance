Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 894?904,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsThe Role of Syntax in Vector Space Models of Compositional SemanticsKarl Moritz Hermann and Phil BlunsomDepartment of Computer ScienceUniversity of OxfordOxford, OX1 3QD, UK{karl.moritz.hermann,phil.blunsom}@cs.ox.ac.ukAbstractModelling the compositional process bywhich the meaning of an utterance arisesfrom the meaning of its parts is a funda-mental task of Natural Language Process-ing.
In this paper we draw upon recentadvances in the learning of vector spacerepresentations of sentential semantics andthe transparent interface between syntaxand semantics provided by CombinatoryCategorial Grammar to introduce Com-binatory Categorial Autoencoders.
Thismodel leverages the CCG combinatory op-erators to guide a non-linear transforma-tion of meaning within a sentence.
We usethis model to learn high dimensional em-beddings for sentences and evaluate themin a range of tasks, demonstrating thatthe incorporation of syntax allows a con-cise model to learn representations that areboth effective and general.1 IntroductionSince Frege stated his ?Principle of SemanticCompositionality?
in 1892 researchers have pon-dered both how the meaning of a complex expres-sion is determined by the meanings of its parts,and how those parts are combined.
(Frege, 1892;Pelletier, 1994).
Over a hundred years on thechoice of representational unit for this processof compositional semantics, and how these unitscombine, remain open questions.Frege?s principle may be debatable from a lin-guistic and philosophical standpoint, but it hasprovided a basis for a range of formal approachesto semantics which attempt to capture meaning inlogical models.
The Montague grammar (Mon-tague, 1970) is a prime example for this, build-ing a model of composition based on lambda-calculus and formal logic.
More recent workin this field includes the Combinatory CategorialGrammar (CCG), which also places increased em-phasis on syntactic coverage (Szabolcsi, 1989).Recently those searching for the right represen-tation for compositional semantics have drawn in-spiration from the success of distributional mod-els of lexical semantics.
This approach representssingle words as distributional vectors, implyingthat a word?s meaning is a function of the envi-ronment it appears in, be that its syntactic role orco-occurrences with other words (Pereira et al,1993; Schu?tze, 1998).
While distributional se-mantics is easily applied to single words, spar-sity implies that attempts to directly extract distri-butional representations for larger expressions aredoomed to fail.
Only in the past few years hasit been attempted to extend these representationsto semantic composition.
Most approaches hereuse the idea of vector-matrix composition to learnlarger representations from single-word encodings(Baroni and Zamparelli, 2010; Grefenstette andSadrzadeh, 2011; Socher et al, 2012b).
Whilethese models have proved very promising for com-positional semantics, they make minimal use oflinguistic information beyond the word level.In this paper we bridge the gap between recentadvances in machine learning and more traditionalapproaches within computational linguistics.
Weachieve this goal by employing the CCG formal-ism to consider compositional structures at anypoint in a parse tree.
CCG is attractive both for itstransparent interface between syntax and seman-tics, and a small but powerful set of combinatoryoperators with which we can parametrise our non-linear transformations of compositional meaning.We present a novel class of recursive mod-els, the Combinatory Categorial Autoencoders(CCAE), which marry a semantic process pro-vided by a recursive autoencoder with the syn-tactic representations of the CCG formalism.Through this model we seek to answer two ques-894tions: Can recursive vector space models be recon-ciled with a more formal notion of compositional-ity; and is there a role for syntax in guiding seman-tics in these types of models?
CCAEs make use ofCCG combinators and types by conditioning eachcomposition function on its equivalent step in aCCG proof.
In terms of learning complexity andspace requirements, our models strike a balancebetween simpler greedy approaches (Socher etal., 2011b) and the larger recursive vector-matrixmodels (Socher et al, 2012b).We show that this combination of state of the artmachine learning and an advanced linguistic for-malism translates into concise models with com-petitive performance on a variety of tasks.
In bothsentiment and compound similarity experimentswe show that our CCAE models match or bettercomparable recursive autoencoder models.12 BackgroundThere exist a number of formal approaches to lan-guage that provide mechanisms for composition-ality.
Generative Grammars (Jackendoff, 1972)treat semantics, and thus compositionality, essen-tially as an extension of syntax, with the generative(syntactic) process yielding a structure that can beinterpreted semantically.
By contrast Montaguegrammar achieves greater separation between thesemantic and the syntactic by using lambda calcu-lus to express meaning.
However, this greater sep-aration between surface form and meaning comesat a price in the form of reduced computability.While this is beyond the scope of this paper, seee.g.
Kracht (2008) for a detailed analysis of com-positionality in these formalisms.2.1 Combinatory Categorial GrammarIn this paper we focus on CCG, a linguisticallyexpressive yet computationally efficient grammarformalism.
It uses a constituency-based structurewith complex syntactic types (categories) fromwhich sentences can be deduced using a smallnumber of combinators.
CCG relies on combi-natory logic (as opposed to lambda calculus) tobuild its expressions.
For a detailed introductionand analysis vis-a`-vis other grammar formalismssee e.g.
Steedman and Baldridge (2011).CCG has been described as having a transpar-ent surface between the syntactic and the seman-1A C++ implementation of our models is available athttp://www.karlmoritz.com/Tina likes tigersN (S[dcl]\NP)/NP NNP NP>S[dcl]\NP<S[dcl]Figure 1: CCG derivation for Tina likes tigers withforward (>) and backward application (<).tic.
It is this property which makes it attractivefor our purposes of providing a conditioning struc-ture for semantic operators.
A second benefit ofthe formalism is that it is designed with computa-tional efficiency in mind.
While one could debatethe relative merits of various linguistic formalismsthe existence of mature tools and resources, suchas the CCGBank (Hockenmaier and Steedman,2007), the Groningen Meaning Bank (Basile et al,2012) and the C&C Tools (Curran et al, 2007) isanother big advantage for CCG.CCG?s transparent surface stems from its cate-gorial property: Each point in a derivation corre-sponds directly to an interpretable category.
Thesecategories (or types) associated with each term in aCCG govern how this term can be combined withother terms in a larger structure, implicitly makingthem semantically expressive.For instance in Figure 1, the word likes has type(S[dcl]\NP)/NP, which means that it first looksfor a type NP to its right hand side.
Subsequentlythe expression likes tigers (as type S[dcl]\NP) re-quires a second NP on its left.
The final type ofthe phrase S[dcl] indicates a sentence and hence acomplete CCG proof.
Thus at each point in a CCGparse we can deduce the possible next steps in thederivation by considering the available types andcombinatory rules.2.2 Vector Space Models of SemanticsVector-based approaches for semantic tasks havebecome increasingly popular in recent years.Distributional representations encode an ex-pression by its environment, assuming the context-dependent nature of meaning according to whichone ?shall know a word by the company it keeps?
(Firth, 1957).
Effectively this is usually achievedby considering the co-occurrence with other wordsin large corpora or the syntactic roles a word per-forms.Distributional representations are frequentlyused to encode single words as vectors.
Such rep-895resentations have then successfully been appliedto a number of tasks including word sense disam-biguation (Schu?tze, 1998) and selectional prefer-ence (Pereira et al, 1993; Lin, 1999).While it is theoretically possible to apply thesame mechanism to larger expressions, sparsityprevents learning meaningful distributional repre-sentations for expressions much larger than singlewords.2Vector space models of compositional seman-tics aim to fill this gap by providing a methodol-ogy for deriving the representation of an expres-sion from those of its parts.
While distributionalrepresentations frequently serve to encode singlewords in such approaches this is not a strict re-quirement.There are a number of ideas on how to de-fine composition in such vector spaces.
A gen-eral framework for semantic vector compositionwas proposed in Mitchell and Lapata (2008), withMitchell and Lapata (2010) and more recently Bla-coe and Lapata (2012) providing good overviewsof this topic.
Notable approaches to this issue in-clude Baroni and Zamparelli (2010), who com-pose nouns and adjectives by representing them asvectors and matrices, respectively, with the com-positional representation achieved by multiplica-tion.
Grefenstette and Sadrzadeh (2011) use a sim-ilar approach with matrices for relational wordsand vectors for arguments.
These two approachesare combined in Grefenstette et al (2013), produc-ing a tensor-based semantic framework with ten-sor contraction as composition operation.Another set of models that have very success-fully been applied in this area are recursive autoen-coders (Socher et al, 2011a; Socher et al, 2011b),which are discussed in the next section.2.3 Recursive AutoencodersAutoencoders are a useful tool to compress in-formation.
One can think of an autoencoderas a funnel through which information has topass (see Figure 2).
By forcing the autoencoderto reconstruct an input given only the reducedamount of information available inside the funnelit serves as a compression tool, representing high-dimensional objects in a lower-dimensional space.Typically a given autoencoder, that is the func-tions for encoding and reconstructing data, are2The experimental setup in (Baroni and Zamparelli, 2010)is one of the few examples where distributional representa-tions are used for word pairs.Figure 2: A simple three-layer autoencoder.
Theinput represented by the vector at the bottom isbeing encoded in a smaller vector (middle), fromwhich it is then reconstructed (top) into the samedimensionality as the original input vector.used on multiple inputs.
By optimizing the twofunctions to minimize the difference between allinputs and their respective reconstructions, this au-toencoder will effectively discover some hiddenstructures within the data that can be exploited torepresent it more efficiently.As a simple example, assume input vectorsxi ?
Rn, i ?
(0..N), weight matrices W enc ?R(m?n),W rec ?
R(n?m) and biases benc ?
Rm,brec ?
Rn.
The encoding matrix and bias are usedto create an encoding ei from xi:ei = fenc(xi) =W encxi + benc (1)Subsequently e ?
Rm is used to reconstruct x asx?
using the reconstruction matrix and bias:x?i = f rec(ei) =W recei + brec (2)?
= (W enc,W rec, benc, brec) can then be learnedby minimizing the error function describing thedifference between x?
and x:E = 12N?i?
?x?i ?
xi?
?2 (3)Now, if m < n, this will intuitively lead to eiencoding a latent structure contained in xi andshared across all xj , j ?
(0..N), with ?
encodingand decoding to and from that hidden structure.It is possible to apply multiple autoencoders ontop of each other, creating a deep autoencoder(Bengio et al, 2007; Hinton and Salakhutdinov,2006).
For such a multi-layered model to learnanything beyond what a single layer could learn, anon-linear transformation g needs to be applied ateach layer.
Usually, a variant of the sigmoid (?
)896Figure 3: RAE with three inputs.
Vectors withfilled (blue) circles represent input and hiddenunits; blanks (white) denote reconstruction layers.or hyperbolic tangent (tanh) function is used forg (LeCun et al, 1998).fenc(xi) = g (W encxi + benc) (4)f rec(ei) = g (W recei + brec)Furthermore, autoencoders can easily be used asa composition function by concatenating two inputvectors, such that:e = f(x1, x2) = g (W (x1?x2) + b) (5)(x?1?x?2) = g(W ?e+ b?
)Extending this idea, recursive autoencoders (RAE)allow the modelling of data of variable size.
Bysetting the n = 2m, it is possible to recursivelycombine a structure into an autoencoder tree.
SeeFigure 3 for an example, where x1, x2, x3 are re-cursively encoded into y2.The recursive application of autoencoders wasfirst introduced in Pollack (1990), whose recursiveauto-associative memories learn vector represen-tations over pre-specified recursive data structures.More recently this idea was extended and appliedto dynamic structures (Socher et al, 2011b).These types of models have become increas-ingly prominent since developments within thefield of Deep Learning have made the trainingof such hierarchical structures more effective andtractable (LeCun et al, 1998; Hinton et al, 2006).Intuitively the top layer of an RAE will encodeaspects of the information stored in all of the inputvectors.
Previously, RAE have successfully beenapplied to a number of tasks including sentimentanalysis, paraphrase detection, relation extractionModel CCG ElementsCCAE-A parseCCAE-B parse + rulesCCAE-C parse + rules + typesCCAE-D parse + rules + child typesTable 1: Aspects of the CCG formalism used bythe different models explored in this paper.and 3D object identification (Blacoe and Lapata,2012; Socher et al, 2011b; Socher et al, 2012a).3 ModelThe models in this paper combine the power ofrecursive, vector-based models with the linguisticintuition of the CCG formalism.
Their purpose isto learn semantically meaningful vector represen-tations for sentences and phrases of variable size,while the purpose of this paper is to investigatethe use of syntax and linguistic formalisms in suchvector-based compositional models.We assume a CCG parse to be given.
Let C de-note the set of combinatory rules, and T the setof categories used, respectively.
We use the parsetree to structure an RAE, so that each combina-tory step is represented by an autoencoder func-tion.
We refer to these models Categorial Com-binatory Autoencoders (CCAE).
In total this pa-per describes four models making increasing useof the CCG formalism (see table 1).As an internal baseline we use model CCAE-A, which is an RAE structured along a CCG parsetree.
CCAE-A uses a single weight matrix each forthe encoding and reconstruction step (see Table 2.This model is similar to Socher et al (2011b), ex-cept that we use a fixed structure in place of thegreedy tree building approach.
As CCAE-A usesonly minimal syntactic guidance, this should al-low us to better ascertain to what degree the use ofsyntax helps our semantic models.Our second model (CCAE-B) uses the compo-sition function in equation (6), with c ?
C.fenc(x, y, c) = g (W cenc(x?y) + bcenc) (6)f rec(e, c) = g (W crece+ bcrec)This means that for every combinatory rule we de-fine an equivalent autoencoder composition func-tion by parametrizing both the weight matrix andbias on the combinatory rule (e.g.
Figure 4).In this model, as in the following ones, we as-sume a reconstruction step symmetric with the897Model Encoding FunctionCCAE-A f(x, y)= g (W (x?y) + b)CCAE-B f(x, y, c)= g (W c(x?y) + bc)CCAE-C f(x, y, c, t)= g(?p?
{c,t} (W p(x?y) + bp))CCAE-D f(x, y, c, tx, ty)= g (W c (W txx+W tyy)+ bc)Table 2: Encoding functions of the four CCAE models discussed in this paper.?
: X/Y ?
: Y>??
: Xg (W>enc(???)
+ b>enc)Figure 4: Forward application as CCG combinatorand autoencoder rule respectively.Figure 5: CCAE-B applied to Tina likes tigers.Next to each vector are the CCG category (top)and the word or function representing it (bottom).lex describes the unary type-changing operation.> and < are forward and backward application.composition step.
For the remainder of this paperwe will focus on the composition step and drop theuse of enc and rec in variable names where it isn?texplicitly required.
Figure 5 shows model CCAE-B applied to our previous example sentence.While CCAE-B uses only the combinatoryrules, we want to make fuller use of the linguis-tic information available in CCG.
For this pur-pose, we build another model CCAE-C, whichparametrizes on both the combinatory rule c ?
Cand the CCG category t ?
T at every step (seeFigure 2).
This model provides an additional de-gree of insight, as the categories T are semanti-cally and syntactically more expressive than theCCG combinatory rules by themselves.
Summingover weights parametrised on c and t respectively,adds an additional degree of freedom and also al-lows for some model smoothing.An alternative approach is encoded in modelCCAE-D.
Here we consider the categories not ofthe element represented, but of the elements it isgenerated from together with the combinatory ruleapplied to them.
The intuition is that in the firststep we transform two expressions based on theirsyntax.
Subsequently we combine these two con-ditioned on their joint combinatory rule.4 LearningIn this section we briefly discuss unsupervisedlearning for our models.
Subsequently we de-scribe how these models can be extended to allowfor semi-supervised training and evaluation.Let ?
= (W,B, L) be our model parametersand ?
a vector with regularization parameters forall model parameters.
W represents the set of allweight matrices, B the set of all biases and L theset of all word vectors.
LetN be the set of trainingdata consisting of tree-nodes n with inputs xn, ynand reconstruction rn.
The error given n is:E(n|?)
= 12??
?rn ?
(xn?yn)??
?2 (7)The gradient of the regularised objective func-tion then becomes:?J??
=1NN?n?E(n|?)??
+ ??
(8)We learn the gradient using backpropagationthrough structure (Goller and Ku?chler, 1996), andminimize the objective function using L-BFGS.For more details about the partial derivativesused for backpropagation, see the documentationaccompanying our model implementation.33http://www.karlmoritz.com/8984.1 Supervised LearningThe unsupervised method described so far learnsa vector representation for each sentence.
Such arepresentation can be useful for some tasks such asparaphrase detection, but is not sufficient for othertasks such as sentiment classification, which weare considering in this paper.In order to extract sentiment from our models,we extend them by adding a supervised classifieron top, using the learned representations v as inputfor a binary classification model:pred(l=1|v, ?)
= sigmoid(Wlabel v + blabel) (9)Given our corpus of CCG parses with label pairs(N, l), the new objective function becomes:J = 1N?
(N,l)E(N, l, ?)
+ ?2 ||?||2 (10)Assuming each node n ?
N contains childrenxn, yn, encoding en and reconstruction rn, so thatn = {x, y, e, r} this breaks down into:E(N, l, ?)
= (11)?n?N?Erec (n, ?)
+ (1??
)Elbl(en, l, ?
)Erec(n, ?)
=12???[xn?yn]?
rn??
?2 (12)Elbl(e, l, ?)
=12 ?l ?
e?2 (13)This method of introducing a supervised aspectto the autoencoder largely follows the model de-scribed in Socher et al (2011b).5 ExperimentsWe describe a number of standard evaluationsto determine the comparative performance of ourmodel.
The first task of sentiment analysis allowsus to compare our CCG-conditioned RAE withsimilar, existing models.
In a second experiment,we apply our model to a compound similarity eval-uation, which allows us to evaluate our modelsagainst a larger class of vector-based models (Bla-coe and Lapata, 2012).
We conclude with somequalitative analysis to get a better idea of whetherthe combination of CCG and RAE can learn se-mantically expressive embeddings.In our experiments we use the hyperbolic tan-gent as nonlinearity g. Unless stated otherwise weuse word-vectors of size 50, initialized using theembeddings provided by Turian et al (2010) basedon the model of Collobert and Weston (2008).4We use the C&C parser (Clark and Curran,2007) to generate CCG parse trees for the dataused in our experiments.
For models CCAE-C andCCAE-D we use the 25 most frequent CCG cate-gories (as extracted from the British National Cor-pus) with an additional general weight matrix inorder to catch all remaining types.5.1 Sentiment AnalysisWe evaluate our model on the MPQA opinioncorpus (Wiebe et al, 2005), which annotates ex-pressions for sentiment.5 The corpus consists of10,624 instances with approximately 70 percentdescribing a negative sentiment.
We apply thesame pre-processing as (Nakagawa et al, 2010)and (Socher et al, 2011b) by using an additionalsentiment lexicon (Wilson et al, 2005) during themodel training for this experiment.As a second corpus we make use of the sentencepolarity (SP) dataset v1.0 (Pang and Lee, 2005).6This dataset consists of 10662 sentences extractedfrom movie reviews which are manually labelledwith positive or negative sentiment and equallydistributed across sentiment.Experiment 1: Semi-Supervised Training Inthe first experiment, we use the semi-supervisedtraining strategy described previously and initial-ize our models with the embeddings provided byTurian et al (2010).
The results of this evalua-tion are in Table 3.
While we achieve the best per-formance on the MPQA corpus, the results on theSP corpus are less convincing.
Perhaps surpris-ingly, the simplest model CCAE-A outperformsthe other models on this dataset.When considering the two datasets, sparsityseems a likely explanation for this difference inresults: In the MPQA experiment most instancesare very short with an average length of 3 words,while the average sentence length in the SP corpusis 21 words.
The MPQA task is further simplifiedthrough the use or an additional sentiment lexicon.Considering dictionary size, the SP corpus has adictionary of 22k words, more than three times thesize of the MPQA dictionary.4http://www.metaoptimize.com/projects/wordreprs/5http://mpqa.cs.pitt.edu/6http://www.cs.cornell.edu/people/pabo/movie-review-data/899Method MPQA SPVoting with two lexica 81.7 63.1MV-RNN (Socher et al, 2012b) - 79.0RAE (rand) (Socher et al, 2011b) 85.7 76.8TCRF (Nakagawa et al, 2010) 86.1 77.3RAE (init) (Socher et al, 2011b) 86.4 77.7NB (Wang and Manning, 2012) 86.7 79.4CCAE-A 86.3 77.8CCAE-B 87.1 77.1CCAE-C 87.1 77.3CCAE-D 87.2 76.7Table 3: Accuracy of sentiment classification onthe sentiment polarity (SP) and MPQA datasets.For NB we only display the best result among alarger group of models analysed in that paper.This issue of sparsity is exacerbated in the morecomplex CCAE models, where the training pointsare spread across different CCG types and rules.While the initialization of the word vectors withpreviously learned embeddings (as was previouslyshown by Socher et al (2011b)) helps the mod-els, all other model variables such as compositionweights and biases are still initialised randomlyand thus highly dependent on the amount of train-ing data available.Experiment 2: Pretraining Due to our analy-sis of the results of the initial experiment, we ran asecond series of experiments on the SP corpus.
Wefollow (Scheible and Schu?tze, 2013) for this sec-ond series of experiments, which are carried out ona random 90/10 training-testing split, with somedata reserved for development.Instead of initialising the model with externalword embeddings, we first train it on a largeamount of data with the aim of overcoming thesparsity issues encountered in the previous exper-iment.
Learning is thus divided into two steps:The first, unsupervised training phase, uses theBritish National Corpus together with the SP cor-pus.
In this phase only the reconstruction signalis used to learn word embeddings and transforma-tion matrices.
Subsequently, in the second phase,only the SP corpus is used, this time with both thereconstruction and the label error.By learning word embeddings and compositionmatrices on more data, the model is likely to gen-eralise better.
Particularly for the more complexmodels, where the composition functions are con-ditioned on various CCG parameters, this shouldTrainingModel Regular PretrainingCCAE-A 77.8 79.5CCAE-B 76.9 79.8CCAE-C 77.1 81.0CCAE-D 76.9 79.7Table 4: Effect of pretraining on model perfor-mance on the SP dataset.
Results are reported on arandom subsection of the SP corpus; thus numbersfor the regular training method differ slightly fromthose in Table 3.help to overcome issues of sparsity.If we consider the results of the pre-trained ex-periments in Table 4, this seems to be the case.In fact, the trend of the previous results has beenreversed, with the more complex models now per-forming best, whereas in the previous experimentsthe simpler models performed better.
Using theTurian embeddings instead of random initialisa-tion did not improve results in this setup.5.2 Compound SimilarityIn a second experiment we use the dataset fromMitchell and Lapata (2010) which contains sim-ilarity judgements for adjective-noun, noun-nounand verb-object pairs.7 All compound pairs havebeen ranked for semantic similarity by a number ofhuman annotators.
The task is thus to rank thesepairs of word pairs by their semantic similarity.For instance, the two compounds vast amountand large quantity are given a high similarity scoreby the human judges, while northern region andearly age are assigned no similarity at all.We train our models as fully unsupervised au-toencoders on the British National Corpus for thistask.
We assume fixed parse trees for all of thecompounds (Figure 6), and use these to computecompound level vectors for all word pairs.
Wesubsequently use the cosine distance between eachcompound pair as our similarity measure.
Weuse Spearman?s rank correlation coefficient (?)
forevaluation; hence there is no need to rescale ourscores (-1.0 ?
1.0) to the original scale (1.0 ?
7.0).Blacoe and Lapata (2012) have an extensivecomparison of the performance of various vector-based models on this data set to which we compareour model in Table 5.
The CCAE models outper-7http://homepages.inf.ed.ac.uk/mlap/resources/index.html900Verb ObjectVB NN(S\NP)/NP NNP>S\NPNoun NounNN NNN/N N>NAdjective NounJJ NNN/N N>NFigure 6: Assumed CCG parse structure for the compound similarity evaluation.Method Adj-N N-N V-ObjHuman 0.52 0.49 0.55(Blacoe and Lapata, 2012)/+ 0.21 - 0.48 0.22 - 0.50 0.18 - 0.35RAE 0.19 - 0.31 0.24 - 0.30 0.09 - 0.28CCAE-B 0.38 0.44 0.34CCAE-C 0.38 0.41 0.23CCAE-D 0.41 0.44 0.29Table 5: Correlation coefficients of model predic-tions for the compound similarity task.
Numbersshow Spearman?s rank correlation coefficient (?
).Higher numbers indicate better correlation.form the RAE models provided by Blacoe and La-pata (2012), and score towards the upper end of therange of other models considered in that paper.5.3 Qualitative AnalysisTo get better insight into our models we also per-form a small qualitative analysis.
Using one of themodels trained on the MPQA corpus, we gener-ate word-level representations of all phrases in thiscorpus and subsequently identify the most relatedexpressions by using the cosine distance measure.We perform this experiment on all expressions oflength 5, considering all expressions with a wordlength between 3 and 7 as potential matches.As can be seen in Table 6, this works with vary-ing success.
Linking expressions such as convey-ing the message of peace and safeguard(ing) peaceand security suggests that the model does learnsome form of semantics.On the other hand, the connection between ex-pressed their satisfaction and support and ex-pressed their admiration and surprise suggeststhat the pure word level content still has an impacton the model analysis.
Likewise, the expressionsis a story of success and is a staunch supporterhave some lexical but little semantic overlap.
Fur-ther reducing this link between the lexical and thesemantic representation is an issue that should beaddressed in future work in this area.6 DiscussionOverall, our models compare favourably with thestate of the art.
On the MPQA corpus modelCCAE-D achieves the best published results weare aware of, whereas on the SP corpus we achievecompetitive results.
With an additional, unsuper-vised training step we achieved results beyond thecurrent state of the art on this task, too.Semantics The qualitative analysis and the ex-periment on compounds demonstrate that theCCAE models are capable of learning semantics.An advantage of our approach?and of autoen-coders generally?is their ability to learn in anunsupervised setting.
The pre-training step forthe sentiment task was essentially the same train-ing step as used in the compound similarity task.While other models such as the MV-RNN (Socheret al, 2012b) achieve good results on a particu-lar task, they do not allow unsupervised training.This prevents the possiblity of pretraining, whichwe showed to have a big impact on results, and fur-ther prevents the training of general models: TheCCAE models can be used for multiple tasks with-out the need to re-train the main model.Complexity Previously in this paper we arguedthat our models combined the strengths of otherapproaches.
By using a grammar formalism weincrease the expressive power of the model whilethe complexity remains low.
For the complex-ity analysis see Table 7.
We strike a balance be-tween the greedy approaches (e.g.
Socher et al(2011b)), where learning is quadratic in the lengthof each sentence and existing syntax-driven ap-proaches such as the MV-RNN of Socher et al(2012b), where the size of the model, that is thenumber of variables that needs to be learned, isquadratic in the size of the word-embeddings.Sparsity Parametrizing on CCG types and rulesincreases the size of the model compared to agreedy RAE (Socher et al, 2011b).
The effectof this was highlighted by the sentiment analysistask, with the more complex models performing901Expression Most Similarconvey the message of peace safeguard peace and securitykeep alight the flame of keep up the hopehas a reason to repent has no righta significant and successful strike a much better positionit is reassuring to believe it is a positive developmentexpressed their satisfaction and support expressed their admiration and surpriseis a story of success is a staunch supporterare lining up to condemn are going to voice their concernsmore sanctions should be imposed charges being leveledcould fray the bilateral goodwill could cause serious damageTable 6: Phrases from the MPQA corpus and their semantically closest match according to CCAE-D.ComplexityModel Size LearningMV-RNN O(nw2) O(l)RAE O(nw) O(l2)CCAE-* O(nw) O(l)Table 7: Comparison of models.
n is dictionarysize, w embedding width, l is sentence length.
Wecan assume l  n  w. Additional factors suchas CCG rules and types are treated as small con-stants for the purposes of this analysis.worse in comparison with the simpler ones.
Wewere able to overcome this issue by using addi-tional training data.
Beyond this, it would also beinteresting to investigate the relationships betweendifferent types and to derive functions to incorpo-rate this into the learning procedure.
For instancemodel learning could be adjusted to enforce somemirroring effects between the weight matrices offorward and backward application, or to supportsimilarities between those of forward applicationand composition.CCG-Vector Interface Exactly how the infor-mation contained in a CCG derivation is best ap-plied to a vector space model of compositionalityis another issue for future research.
Our investi-gation of this matter by exploring different modelsetups has proved somewhat inconclusive.
WhileCCAE-D incorporated the deepest conditioning onthe CCG structure, it did not decisively outperformthe simpler CCAE-B which just conditioned onthe combinatory operators.
Issues of sparsity, asshown in our experiments on pretraining, have asignificant influence, which requires further study.7 ConclusionIn this paper we have brought a more formal no-tion of semantic compositionality to vector spacemodels based on recursive autoencoders.
This wasachieved through the use of the CCG formalismto provide a conditioning structure for the matrixvector products that define the RAE.We have explored a number of models, each ofwhich conditions the compositional operations ondifferent aspects of the CCG derivation.
Our ex-perimental findings indicate a clear advantage fora deeper integration of syntax over models that useonly the bracketing structure of the parse tree.The most effective way to condition the compo-sitional operators on the syntax remains unclear.Once the issue of sparsity had been addressed, thecomplex models outperformed the simpler ones.Among the complex models, however, we couldnot establish significant or consistent differencesto convincingly argue for a particular approach.While the connections between formal linguis-tics and vector space approaches to NLP may notbe immediately obvious, we believe that there is acase for the continued investigation of ways to bestcombine these two schools of thought.
This paperrepresents one step towards the reconciliation oftraditional formal approaches to compositional se-mantics with modern machine learning.AcknowledgementsWe thank the anonymous reviewers for their feed-back and Richard Socher for providing additionalinsight into his models.
Karl Moritz would furtherlike to thank Sebastian Riedel for hosting him atUCL while this paper was written.
This work hasbeen supported by the EPSRC.902ReferencesMarco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InProceedings of EMNLP, pages 1183?1193.Valerio Basile, Johan Bos, Kilian Evang, and NoortjeVenhuizen.
2012.
Developing a large semanticallyannotated corpus.
In Proceedings of LREC, pages3196?3200, Istanbul, Turkey.Yoshua Bengio, Pascal Lamblin, Dan Popovici, andHugo Larochelle.
2007.
Greedy layer-wise trainingof deep networks.
In Advances in Neural Informa-tion Processing Systems 19, pages 153?160.William Blacoe and Mirella Lapata.
2012.
A com-parison of vector-based representations for seman-tic composition.
In Proceedings of EMNLP-CoNLL,pages 546?556.Stephen Clark and James R. Curran.
2007.
Wide-coverage efficient statistical parsing with ccg andlog-linear models.
CL, 33(4):493?552, December.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: Deepneural networks with multitask learning.
In Pro-ceedings of ICML.James Curran, Stephen Clark, and Johan Bos.
2007.Linguistically motivated large-scale nlp with c&cand boxer.
In Proceedings of ACL Demo and PosterSessions, pages 33?36.J.
R. Firth.
1957.
A synopsis of linguistic theory 1930-55.
1952-59:1?32.Gottfried Frege.
1892.
U?ber Sinn und Bedeutung.
InMark Textor, editor, Funktion - Begriff - Bedeutung,volume 4 of Sammlung Philosophie.
Vandenhoeck& Ruprecht, Go?ttingen.Christoph Goller and Andreas Ku?chler.
1996.
Learn-ing task-dependent distributed representations bybackpropagation through structure.
In Proceedingsof the ICNN-96, pages 347?352.
IEEE.Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011.Experimental support for a categorical composi-tional distributional model of meaning.
In Proceed-ings of EMNLP, pages 1394?1404.Edward Grefenstette, Georgiana Dinu, Yao-ZhongZhang, Mehrnoosh Sadrzadeh, and Marco Baroni.2013.
Multi-step regression learning for composi-tional distributional semantics.G.
E. Hinton and R. R. Salakhutdinov.
2006.
Reduc-ing the dimensionality of data with neural networks.Science, 313(5786):504?507.Geoffrey E. Hinton, Simon Osindero, Max Welling,and Yee Whye Teh.
2006.
Unsupervised discoveryof nonlinear structure using contrastive backpropa-gation.
Cognitive Science, 30(4):725?731.Julia Hockenmaier and Mark Steedman.
2007.
CCG-bank: A Corpus of CCG Derivations and Depen-dency Structures Extracted from the Penn Treebank.CL, 33(3):355?396, September.Ray Jackendoff.
1972.
Semantic Interpretation inGenerative Grammar.
MIT Press, Cambridge, MA.Marcus Kracht.
2008.
Compositionality in MontagueGrammar.
In Edouard Machery und Markus Wern-ing Wolfram Hinzen, editor, Handbook of Composi-tionality, pages 47 ?
63.
Oxford University Press.Yann LeCun, Leon Bottou, Genevieve Orr, and Klaus-Robert Muller.
1998.
Efficient backprop.
In G. Orrand Muller K., editors, Neural Networks: Tricks ofthe trade.
Springer.Dekang Lin.
1999.
Automatic identification of non-compositional phrases.
In Proceedings of ACL,pages 317?324.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In In Proceedingsof ACL, pages 236?244.Jeff Mitchell and Mirella Lapata.
2010.
Compositionin distributional models of semantics.
Cognitive Sci-ence, 34(8):1388?1429.R.
Montague.
1970.
Universal grammar.
Theoria,36(3):373?398.Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.2010.
Dependency tree-based sentiment classifica-tion using crfs with hidden variables.
In NAACL-HLT, pages 786?794.Bo Pang and Lillian Lee.
2005.
Seeing stars: exploit-ing class relationships for sentiment categorizationwith respect to rating scales.
In Proceedings of ACL,pages 115?124.Francis Jeffry Pelletier.
1994.
The principle of seman-tic compositionality.
Topoi, 13:11?24.Fernando Pereira, Naftali Tishby, and Lillian Lee.1993.
Distributional clustering of english words.
InProceedings of ACL, ACL ?93, pages 183?190.Jordan B. Pollack.
1990.
Recursive distributed repre-sentations.
Artificial Intelligence, 46:77?105.Christian Scheible and Hinrich Schu?tze.
2013.
Cuttingrecursive autoencoder trees.
In Proceedings of theInternational Conference on Learning Representa-tions.Hinrich Schu?tze.
1998.
Automatic word sense dis-crimination.
CL, 24(1):97?123, March.Richard Socher, Eric H. Huang, Jeffrey Pennington,Andrew Y. Ng, and Christopher D. Manning.
2011a.Dynamic Pooling and Unfolding Recursive Autoen-coders for Paraphrase Detection.
In Advances inNeural Information Processing Systems 24.903Richard Socher, Jeffrey Pennington, Eric H. Huang,Andrew Y. Ng, and Christopher D. Manning.
2011b.Semi-supervised recursive autoencoders for predict-ing sentiment distributions.
In Proceedings ofEMNLP, pages 151?161.Richard Socher, Brody Huval, Bharath Bhat, Christo-pher D. Manning, and Andrew Y. Ng.
2012a.Convolutional-Recursive Deep Learning for 3D Ob-ject Classification.
In Advances in Neural Informa-tion Processing Systems 25.Richard Socher, Brody Huval, Christopher D. Man-ning, and Andrew Y. Ng.
2012b.
Semantic com-positionality through recursive matrix-vector spaces.In Proceedings of EMNLP-CoNLL, pages 1201?1211.Mark Steedman and Jason Baldridge, 2011.
Combina-tory Categorial Grammar, pages 181?224.
Wiley-Blackwell.Anna Szabolcsi.
1989.
Bound Variables in Syntax:Are There Any?
In Renate Bartsch, Johan van Ben-them, and Peter van Emde Boas, editors, Semanticsand Contextual Expression, pages 295?318.
Foris,Dordrecht.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general methodfor semi-supervised learning.
In Proceedings ofACL, pages 384?394.Sida Wang and Christopher D. Manning.
2012.
Base-lines and bigrams: simple, good sentiment and topicclassification.
In Proceedings of ACL, pages 90?94.Janyce Wiebe, Theresa Wilson, and Claire Cardie.2005.
Annotating expressions of opinions and emo-tions in language.
Language Resources and Evalu-ation, 39(2-3):165?210.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of EMNLP-HLT, HLT ?05, pages 347?354.904
