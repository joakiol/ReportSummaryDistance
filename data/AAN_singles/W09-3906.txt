Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 38?45,Queen Mary University of London, September 2009. c?2009 Association for Computational LinguisticsDealing with Interpretation Errors in Tutorial DialogueMyroslava O. Dzikovska, Charles B. Callaway, Elaine Farrow, Johanna D. MooreSchool of InformaticsUniversity of Edinburgh, Edinburgh, United Kingdommdzikovs,ccallawa,efarrow,jmoore@inf.ed.ac.ukNatalie Steinhauser, Gwendolyn CampbellNaval Air Warfare Training Systems DivisionOrlando, Florida, USAAbstractWe describe an approach to dealing withinterpretation errors in a tutorial dialoguesystem.
Allowing students to provide ex-planations and generate contentful talk canbe helpful for learning, but the languagethat can be understood by a computer sys-tem is limited by the current technology.Techniques for dealing with understandingproblems have been developed primarily forspoken dialogue systems in information-seeking domains, and are not always appro-priate for tutorial dialogue.
We present aclassification of interpretation errors and ourapproach for dealing with them within animplemented tutorial dialogue system.1 IntroductionError detection and recovery is a known problem inthe spoken dialogue community, with much researchdevoted to determining the best strategies, and learn-ing how to choose an appropriate strategy from data.Most existing research is focused on dealing withproblems in an interaction resulting from speechrecognition errors.
This focus is justified, since themajority of understanding problems observed in cur-rent spoken dialogue systems (SDS) are indeed dueto speech recognition errors.Recovery strategies, therefore, are sometimes de-vised specifically to target speech recognition prob-lems - for example, asking the user to repeat the ut-terance, or to speak more softly, which only makessense if speech recognition is the source of trouble.However, errors can occur at all levels of process-ing, including parsing, semantic interpretation, in-tention recognition, etc.
As speech recognition im-proves and more sophisticated systems are devel-oped, strategies for dealing with errors coming fromhigher (and potentially more complex) levels of pro-cessing will have to be developed.This paper presents a classification of non-understandings, defined as the errors where the sys-tem fails to arrive at an interpretation of the user?sutterance (Bohus and Rudnicky, 2005), and a set ofstrategies for dealing with them in an implementedtutorial dialogue system.
Our system differs frommany existing systems in two ways.
First, all di-alogue is typed.
This was done in part to avoidspeech recognition issues and allow for more com-plex language input than would otherwise be pos-sible.
But it is also a valid modality for tutoring -there are now many GUI-based tutoring systems inexistence, and as distance and online learning havebecome more popular, students are increasingly fa-miliar with typed dialogue in chat rooms and discus-sion boards.
Second, different genres impose dif-ferent constraints on the set of applicable recoverystrategies - as we discuss in Section 2, certain helpstrategies developed for task-oriented dialogue sys-tems are not suitable for tutorial dialogue, becausetutoring systems should not give away the answer.We propose a targeted help approach for dealingwith interpretation problems in tutorial dialogue byproviding help messages that target errors at differ-ent points in the pipeline.
In our system they arecombined with hints as a way to lead the studentto an answer that can be understood.
While some38parts of the system response are specific to tutorialdialogue, the targeted help messages themselves canserve as a starting point for developing appropriaterecovery strategies in other systems where errors athigher levels of interpretation are a problem.The rest of this paper is organized as follows.
InSection 2, we motivate the need for error handlingstrategies in tutorial dialogue.
In Section 3 we de-scribe the design of our system.
Section 4 discussesa classification of interpretation problems and ourtargeted help strategy.
Section 5 provides a prelim-inary evaluation based on a set of system tests con-ducted to date.
Finally, we discuss how the approachtaken by our system compares to other systems.2 Background and MotivationTutorial dialogue systems aim to improve learningby engaging students in contentful dialogue.
Thereis a mounting body of evidence that dialogue whichencourages students to explain their actions (Alevenand Koedinger, 2000), or to generate contentful talk(Purandare and Litman, 2008), results in improvedlearning.
However, the systems?
ability to under-stand student language, and therefore to encouragecontentful talk, is limited by the state of current lan-guage technology.
Moreover, student language maybe particularly difficult to interpret since studentsare often unaware of proper terminology, and mayphrase their answers in unexpected ways.
For exam-ple, a recent error analysis for a domain-independentdiagnoser trained on a large corpus showed that ahigh proportion of errors were due to unexpectedparaphrases (Nielsen et al, 2008).In small domains, domain-specific grammars andlexicons can cover most common phrasings usedby students to ensure robust interpretation (Aleven,2003; Glass, 2000).
However, as the size of thedomain and the range of possible questions and an-swers grows, achieving complete coverage becomesmore difficult.
For essays in large domains, sta-tistical methods can be used to identify problemswith the answer (Jordan et al, 2006; Graesser etal., 1999), but these approaches do not perform wellon relatively short single-sentence explanations, andsuch systems often revert to short-answer questionsduring remediation to ensure robustness.To the best of our knowledge, none of these tu-torial systems use sophisticated error handling tech-niques.
They rely on the small size of the domainor simplicity of expected answers to limit the rangeof student input.
They reject utterances they cannotinterpret, asking the user to repeat or rephrase, ortolerate the possibility that interpretation problemswill lead to repetitive or confusing feedback.We are developing a tutorial dialogue system thatbehaves more like human tutors by supporting open-ended questions, as well as remediations that allowfor open-ended answers, and gives students detailedfeedback on their answers, similar to what we ob-served with human tutors.
This paper takes the firststep towards addressing the problem of handling er-rors in tutorial dialogue by developing a set of non-understanding recovery strategies - i.e.
strategiesused where the system cannot find an interpretationfor an utterance.In early pilot experiments we observed that if thesystem simply rejects a problematic student utter-ance, saying that it was not understood, then stu-dents are unable to determine the reason for thisrejection.
They either resubmit their answer mak-ing only minimal changes, or else they rephrase thesentence in a progressively more complicated fash-ion, causing even more interpretation errors.
Evenafter interacting with the system for over an hour,our students did not have an accurate picture as towhich phrasings are well understood by the systemand which should be avoided.
Previous research alsoshows that users are rarely able to perceive the truecauses of ASR errors, and tend to form incorrect the-ories about the types of input a system is able to ac-cept (Karsenty, 2001).A common approach for dealing with these is-sues in spoken dialogue systems is to either changeto system initiative with short-answer questions (?Isyour destination London??
), or provide targeted help(?You can say plane, car or hotel?).
Neither of theseis suitable for our system.
The expected utterancesin our system are often more complex (e.g., ?Thebulb must be in a closed path with the battery?
), andtherefore suggesting an utterance may be equivalentto giving away the entire answer.
Giving studentsshort-answer questions such as ?Are the terminalsconnected or not connected??
is a valid tutoringstrategy sometimes used by the tutors.
However,it changes the nature of the question from a recall39task to a recognition task, which may affect the stu-dent?s ability to remember the correct solution in-dependently.
Therefore, we decided to implementstrategies that give the student information about thenature of the mistake without directly giving infor-mation about the expected answer, and encouragethem to rephrase their answers in ways that can beunderstood by the system.We currently focus on strategies for dealingwith non-understanding rather than misunderstand-ing strategies (i.e.
cases where the system finds aninterpretation, but an incorrect one).
It is less clearin tutorial dialogue what it means for a misunder-standing to be corrected.
In task-oriented dialogue,if the system gets a slot value different from whatthe user intended, it should make immediate correc-tions at the user?s request.
In tutoring, however, itis the system which knows the expected correct an-swer.
So if the student gives an answer that does notmatch the expected answer, when they try to correctit later, it may not always be obvious whether thecorrection is due to a true misunderstanding, or dueto the student arriving at a better understanding ofthe question.
Obviously, true misunderstandings canand will still occur - for example, when the systemresolves a pronoun incorrectly.
Dealing with suchsituations is planned as part of future work.3 System ArchitectureOur target application is a system for tutoring ba-sic electricity and electronics.
The students readsome introductory material, and interact with a sim-ulator where they can build circuits using batteries,bulbs and switches, and measure voltage and cur-rent.
They are then asked two types of questions:factual questions, like ?If the switch is open, willbulb A be on or off?
?, and explanation questions.The explanation questions ask the student to explainwhat they observed in a circuit simulation, for exam-ple, ?Explain why you got the voltage of 1.5 here?,or define generic concepts, such as ?What is volt-age??.
The expected answers are fairly short, one ortwo sentences, but they involve complex linguisticphenomena, including conjunction, negation, rela-tive clauses, anaphora and ellipsis.The system is connected to a knowledge basewhich serves as a model for the domain and a rea-soning engine.
It represents the objects and rela-tionships the system can reason about, and is usedto compute answers to factual questions.1 The stu-dent answers are processed using a standard NLPpipeline.
All utterances are parsed to obtain syntac-tic analyses.2 The lexical-semantic interpreter takesanalyses from the parser and maps them to seman-tic representations using concepts from the domainmodel.
A reference resolution algorithm similar to(Byron, 2002) is used to find referents for named ob-jects such as ?bulb A?
and for pronouns.Once an interpretation of a student utterance hasbeen obtained, it is checked in two ways.
First, itsinternal consistency is verified.
For example, if thestudent says ?Bulb A will be on because it is in aclosed path?, we first must ensure that their answeris consistent with what is on the screen - that bulb Ais indeed in a closed path.
Otherwise the studentprobably has a problem either with understandingthe diagrams or with understanding concepts such as?closed path?.
These problems indicate lack of basicbackground knowledge, and need to be remediatedusing a separate tutorial strategy.Assuming that the utterance is consistent with thestate of the world, the explanation is then checkedfor correctness.
Even though the student utterancemay be factually correct (Bulb A is indeed in aclosed path), it may still be incomplete or irrelevant.In the example above, the full answer is ?Bulb Ais in a closed path with the battery?, hence the stu-dent explanation is factually correct but incomplete,missing the mention of the battery.In the current version of our system, we are partic-ularly concerned about avoiding misunderstandings,since they can result in misleading tutorial feedback.Consider an example of what can happen if there isa misunderstanding due to a lexical coverage gap.The student sentence ?the path is broken?
should beinterpreted as ?the path is no longer closed?, corre-sponding to the is-open relation.
However, the1Answers to explanation questions are hand-coded by tutorsbecause they are not always required to be logically complete(Dzikovska et al, 2008).
However, they are checked for consis-tency as described later, so they have to be expressed in termsthat the knowledge base can reason about.2We are using a deep parser that produces semantic analysesof student?s input (Allen et al, 2007).
However, these have toundergo further lexical interpretation, so we are treating themas syntactic analyses for purposes of this paper.40most frequent sense of ?broken?
is is-damaged,as in ?the bulb is broken?.
Ideally, the system lex-icon would define ?broken?
as ambiguous betweenthose two senses.
If only the ?damaged?
sense isdefined, the system will arrive at an incorrect inter-pretation (misunderstanding), which is false by defi-nition, as the is-damaged relation applies only tobulbs in our domain.
Thus the system will say ?yousaid that the path is damaged, but that?s not true?.Since the students who used this phrasing were un-aware of the proper terminology in the first instance,they dismissed such feedback as a system error.
Amore helpful feedback message is to say that the sys-tem does not know about damaged paths, and thesentence needs to be rephrased.3Obviously, frequent non-understanding messagescan also lead to communication breakdowns and im-pair tutoring.
Thus we aim to balance the need toavoid misunderstandings with the need to avoid stu-dent frustration due to a large number of sentenceswhich are not understood.
We approach this by us-ing robust parsing and interpretation tools, but bal-ancing them with a set of checks that indicate poten-tial problems.
These include checking that the stu-dent answer fits with the sortal constraints encodedin the domain model, that it can be interpreted un-ambiguously, and that pronouns can be resolved.4 Error Handling PoliciesAll interpretation problems in our system are han-dled with a unified tutorial policy.
Each message tothe user consists of three parts: a social response,the explanation of the problem, and the tutorial re-sponse.
The social response is currently a simpleapology, as in ?I?m sorry, I?m having trouble under-standing.?
Research on spoken dialogue shows thatusers are less frustrated if systems apologize for er-rors (Bulyko et al, 2005).The explanation of the problem depends on theproblem itself, and is discussed in more detail below.The tutorial response depends on the general tu-torial situation.
If this is the first misunderstanding,the student will be asked to rephrase/try again.
If3This was a real coverage problem we encountered early on.While we extended the coverage of the lexical interpreter basedon corpus data, other gaps in coverage may remain.
We discussthe issues related to the treatment of vague or incorrect termi-nology in Section 4.they continue to phrase things in a way that is mis-understood, they will be given up to two differenthints (a less specific hint followed by a more spe-cific hint); and finally the system will bottom outwith a correct answer.
Correct answers produced bythe generator are guaranteed to be parsed and under-stood by the interpretation module, so they can serveas templates for future student answers.The tutorial policy is also adjusted dependingon the interaction history.
For example, if a non-understanding comes after a few incorrect answers,the system may decide to bottom out immediately inorder to avoid student frustration due to multiple er-rors.
At present we are using a heuristic policy basedon the total number of incorrect or uninterpretableanswers.
In the future, such policy could be learnedfrom data, using, for example, reinforcement learn-ing (Williams and Young, 2007).In the rest of this section we discuss the explana-tions used for different problems.
For brevity, weomit the tutorial response from our examples.4.1 Parse FailuresAn utterance that cannot be parsed represents theworst possible outcome for the system, since detect-ing the reason for a syntactic parse failure isn?t pos-sible for complex parsers and grammars.
Thus, inthis instance the system does not give any descrip-tion of the problem at all, saying simply ?I?m sorry,I didn?t understand.
?Since we are unable to explain the source of theproblem, we try hard to avoid such failures.
We usea spelling corrector and a robust parser that outputsa set of fragments covering the student?s input whena full parse cannot be found.
The downstream com-ponents are designed to merge interpretations of thefragments into a single representation that is sent tothe reasoning components.Our policy is to allow the system to use such frag-mentary parses when handling explanation ques-tions, where students tend to use complex language.However, we require full parses for factual ques-tions, such as ?Which bulbs will be off??
We foundthat for those simpler questions students are able toeasily phrase an acceptable answer, and the lack ofa full parse signals some unusually complex lan-guage that downstream components are likely tohave problems with as well.41One risk associated with using fragmentary parsesis that relationships between objects from differentfragments would be missed by the parser.
Our cur-rent policy is to confirm the correct part of the stu-dent?s answer, and prompt for the missing parts, e.g.,?
Right.
The battery is contained in a closed path.And then??
We can do this because we use a diag-noser that explicitly identifies the correct objects andrelationships in the answer (Dzikovska et al, 2008),and we are using a deep generation system that cantake those relationships and automatically generatea rephrasing of the correct portion of the content.4.2 Lexical Interpretation ErrorsErrors in lexical interpretation typically come fromthree main sources: unknown words which the lex-ical interpreter cannot map into domain concepts,unexpected word combinations, and incorrect usesof terminology that violate the sortal constraints en-coded in the domain model.Unknown words are the simplest to deal with inthe context of our lexical interpretation policy.
Wedo not require that every single word of an utter-ance should be interpreted, because we want thesystem to be able to skip over irrelevant asides.However, we require that if a predicate is inter-preted, all its arguments should be interpreted aswell.
To illustrate, in our system the interpretation of?the bulb is still lit?
is (LightBulb Bulb-1-1)(is-lit Bulb-1-1 true).
The adverbial?still?
is not interpreted because the system is un-able to reason about time.4 But since all argumentsof the is-lit predicate are defined, we considerthe interpretation complete.In contrast, in the sentence ?voltage is the mea-surement of the power available in a battery?, ?mea-surement?
is known to the system.
Thus, its argu-ment ?power?
should also be interpreted.
However,the reading material in the lessons never talks aboutpower (the expected answer is ?Voltage is a mea-surement of the difference in electrical states be-tween two terminals?).
Therefore the unknown worddetector marks ?power?
as an unknown word, andtells the student ?I?m sorry, I?m having a problemunderstanding.
I don?t know the word power.
?4The lexical interpretation algorithm makes sure that fre-quency and negation adverbs are accounted for.The system can still have trouble interpreting sen-tences with words which are known to the lexicalinterpreter, but which appear in unexpected combi-nations.
This involves two possible scenarios.
First,unambiguous words could be used in a way thatcontradicts the system?s domain model.
For exam-ple, the students often mention ?closed circuit?
in-stead of the correct term ?closed path?.
The formeris valid in colloquial usage, but is not well definedfor parallel circuits which can contain many differ-ent paths, and therefore cannot be represented in aconsistent knowledge base.
Thus, the system con-sults its knowledge base to tell the student about theappropriate arguments for a relation with which thefailure occurred.
In this instance, the feedback willbe ?I?m sorry, I?m having a problem understanding.I don?t understand it when you say that circuits canbe closed.
Only paths and switches can be closed.
?5The second case arises when a highly ambiguousword is used in an unexpected combination.
Theknowledge base uses a number of fine-grained rela-tions, and therefore some words can map to a largenumber of relations.
For example, the word ?has?means circuit-component in ?The circuit has2 bulbs?, terminals-of in ?The bulb has ter-minals?
and voltage-property in ?The bat-tery has voltage?.
The last relation only applies tobatteries, but not to other components.
These dis-tinctions are common for knowledge representationand reasoning systems, since they improve reason-ing efficiency, but this adds to the difficulty of lex-ical interpretation.
If a student says ?Bulb A has avoltage of 0.5?, we cannot determine the concept towhich the word ?has?
corresponds.
It could be eitherterminals-of or voltage-property, sinceeach of those relations uses one possible argumentfrom the student?s utterance.
Thus, we cannot sug-gest appropriate argument types and instead we in-dicate the problematic word combination, for exam-ple, ?I?m sorry, I?m having trouble understanding.
Ididn?t understand bulb has voltage.
?Finally, certain syntactic constructions involvingcomparatives or ellipsis are known to be difficult5Note that these error messages are based strictly on the factthat sortal constraints from the knowledge base for the relationthat the student used were violated.
In the future, we may alsowant to adjust the recovery strategy depending on whether theproblematic relation is relevant to the expected answer.42open problems for interpretation.
While we areworking on interpretation algorithms to be includedin future system versions, the system currently de-tects these special relations, and produces a mes-sage telling the student to rephrase without the prob-lematic construction, e.g., ?I?m sorry.
I?m having aproblem understanding.
I do not understand sameas.
Please try rephrasing without the word as.
?4.3 Reference ErrorsReference errors arise when a student uses an am-biguous pronoun, and the system cannot find a suit-able object in the knowledge base to match, or oncertain occasions when an attachment error in aparse causes an incorrect interpretation.
We use ageneric message that indicates the type of the ob-ject the system perceived, and the actual word used,for example, ?I?m sorry.
I don?t know which switchyou?re referring to with it.
?To some extent, reference errors are instances ofmisunderstandings rather than non-understandings.There are actually 2 underlying cases for referencefailure: either the system cannot find any referent atall, or it is finding too many referents.
In the futurea better policy would be to ask the student which ofthe ambiguous referents was intended.
We expect topilot this policy in one of our future system tests.5 EvaluationSo far, we have run 13 pilot sessions with our sys-tem.
Each pilot consisted of a student going through1 or 2 lessons with the system.
Each lesson lastsabout 2 hours and has 100-150 student utterances(additional time is taken with building circuits andreading material).
Both the coverage of the interpre-tation component and the specificity of error mes-sages were improved between each set of pilots, thusit does not make sense to aggregate the data fromthem.
However, over time we observed the trendthat students are more likely to change their behav-ior when the system issues more specific messages.Examples of successful and unsuccessful interac-tions are shown in Figure 1.
In (a), the student usedincorrect terminology, and a reminder about how theword ?complete?
is interpreted was enough to getthe conversation back on track.The dialogue fragment in (b) shows how mes-sages which are not specific enough can cause abreakdown in conversation.
The system used an in-sufficiently specific message at the beginning (omit-ting the part that says that only switches and pathscan be closed).
This led the student away from ananswer which was nearly correct with slightly im-perfect terminology to an answer which was insuffi-cient (it?s not enough for the components to be con-nected, they have to be in a closed path), and thento rephrase it in a more complicated way that wasimpossible for the system to understand (consistentwith findings of Bulyko et al (2005)).The next step would be to conduct a formal evalu-ation of our policy.
We are planning to do this usingrecovery rate after different strategies as our evalu-ation measure (Bohus and Rudnicky, 2005), whichis the percentage of times when the utterance imme-diately following a non-understanding was success-fully interpreted by the system.6 DiscussionOur error handling policy attempts to balance ro-bustness in interpretation with the need for pre-cise tutorial feedback.
Human tutors accept a widerange of variation in terminology and phrasing.However, there is a known trade-off between non-understandings and misunderstandings.
We decidedthat misunderstandings are likely to cause more seri-ous problems with respect to tutoring.
Therefore weset strict understanding requirements and focused ondeveloping a policy that would encourage studentsto use constructions that the system can interpret re-liably.
This is also supported by our observations inthe human-human data ?
tutors believe that teachingcorrect terminology is important, and we observedthem saying things like ?It?s good to put it into yourown words like that.
Just for the sake of the lesson,could you also use the term closed path?
?.Our approach is similar to the TargetedHelp ap-proach of (Hockey et al, 2003), where in the case ofan interpretation failure the task assistant producesa report of what the recognizer heard, a descriptionof the problem, and a possible in-domain utteranceas an alternative.
In a user evaluation such targetedhelp messages resulted in significantly lower taskcompletion times compared to a system that pro-duced a generic ?I didn?t understand?
message, and43(a)S: the open switch makes the path incompleteT: I?m sorry, I?m having a problem understanding.
Paths can?t be complete.
Only circuits can becomplete.
Please try again.S: the open switch makes the path openT: Right.
An open switch creates a gap.
(b)T: What are the conditions that are required to make a bulb light up?S: The lightbulb has to be in a closed circuit with a batteryT: I?m sorry, I?m having a problem understanding.
I don?t understand when you say that circuitscan be closed.
Please try again.S: The lightbulb has to be connected to a batteryT: Yes.
You correctly mentioned a battery and a bulb.
Keep trying.S: Both the positive and negative connectors have to be connected toT: I?m sorry, I?m having a problem understanding.
Try rephrasing your answer.S: The lightbulb has to be connected to the positive and negative connectors of the batteryT: I?m sorry, I didn?t understand.
Here?s the answer.
The battery and the bulb must be containedin the same closed path.Figure 1: Sample student-system interactions.
S: is student, T: is tutor (system).
(a) A successful interactionwhere the student changes their language; (b) a failed interaction where system feedback was unhelpful.subjects gradually learned how to talk to the sys-tem, reducing the number of misunderstandings overtime.
This gives us reason to believe that our sys-tem can achieve similar effects in tutorial dialogue.While we don?t suggest alternative domain utter-ances due to the tutoring reasons described earlier,the progressively more specific hints serve a simi-lar function.
To what extent this impacts learningand interaction with the system will have to be de-termined in future evaluations.The error handling in our system is significantlydifferent from systems that analyze user essays be-cause it needs to focus on a single sentence at a time.In a system that does essay analysis, such as AUTO-TUTOR (Graesser et al, 1999) or Why2-Atlas (Jor-dan et al, 2006) a single essay can have many flaws.So it doesn?t matter if some sentences are not fullyunderstood as long as the essay is understood wellenough to identify at least one flaw.
Then that par-ticular flaw can be remediated, and the student canresubmit the essay.
However, this can also cause stu-dent frustration and potentially affect learning if thestudent is asked to re-write an essay many times dueto interpretation errors.Previous systems in the circuit domain focused ontroubleshooting rather than conceptual knowledge.The SHERLOCK tutor (Katz et al, 1998) used onlymenu-based input, limiting possible dialogue.
Cir-cuit Fix-It Shop (Smith and Gordon, 1997) was atask-oriented system which allowed for speech in-put, but with very limited vocabulary.
Our system?slarger vocabulary and complex input result in differ-ent types of non-understandings that cannot be re-solved with simple confirmation messages.A number of researchers have developed er-ror taxonomies for spoken dialogue systems (Paek,2003; Mo?ller et al, 2007).
Our classification doesnot have speech recognition errors (since we are us-ing typed dialogue), and we have a more complexinterpretation stack than the domain-specific pars-ing utilized by many SDSs.
However, some typesof errors are shared, in particular, our ?no parse?,?unknown word?
and ?unknown attachment?
errorscorrespond to command-level errors, and our sor-tal constraint and reference errors correspond toconcept-level errors in the taxonomy of Mo?ller et al(2007).
This correspondence is not perfect becauseof the nature of the task - there are no commands ina tutoring system.
However, the underlying causesare very similar, and so research on the best way44to communicate about system failures would benefitboth tutoring and task-oriented dialogue systems.
Inthe long run, we would like to reconcile these differ-ent taxonomies, leading to a unified classification ofsystem errors and recovery strategies.7 ConclusionIn this paper we described our approach to handlingnon-understanding errors in a tutorial dialogue sys-tem.
Explaining the source of errors, without givingaway the full answer, is crucial to establishing ef-fective communication between the system and thestudent.
We described a classification of commonproblems and our approach to dealing with differentclasses of errors.
Our experience with pilot studies,as well as evidence from spoken dialogue systems,indicates that our approach can help improve dia-logue efficiency.
We will be evaluating its impact onboth student learning and on dialogue efficiency inthe future.8 AcknowledgmentsThis work has been supported in part by Office ofNaval Research grant N000140810043.ReferencesV.
A. Aleven and K. R. Koedinger.
2000.
The need fortutorial dialog to support self-explanation.
In Proc.
ofAAAI Fall Symposion on Building Dialogue Systemsfor Tutorial Applications.O.
P. V. Aleven.
2003.
A knowledge-based approachto understanding students?
explanations.
In School ofInformation Technologies, University of Sydney.J.
Allen, M. Dzikovska, M. Manshadi, and M. Swift.2007.
Deep linguistic processing for spoken dialoguesystems.
In Proceedings of the ACL-07 Workshop onDeep Linguistic Processing.D.
Bohus and A. Rudnicky.
2005.
Sorry, i didn?t catchthat!
- an investigation of non-understanding errorsand recovery strategies.
In Proceedings of SIGdial-2005, Lisbon, Portugal.I.
Bulyko, K. Kirchhoff, M. Ostendorf, and J. Goldberg.2005.
Error-correction detection and response gener-ation in a spoken dialogue system.
Speech Communi-cation, 45(3):271?288.D.
K. Byron.
2002.
Resolving Pronominal Refer-ence to Abstract Entities.
Ph.D. thesis, University ofRochester.M.
O. Dzikovska, G. E. Campbell, C. B. Callaway, N. B.Steinhauser, E. Farrow, J. D. Moore, L. A. Butler, andC.
Matheson.
2008.
Diagnosing natural language an-swers to support adaptive tutoring.
In Proceedings21st International FLAIRS Conference.M.
Glass.
2000.
Processing language input in theCIRCSIM-Tutor intelligent tutoring system.
In Proc.of the AAAI Fall Symposium on Building Dialogue Sys-tems for Tutorial Applications.A.
C. Graesser, P. Wiemer-Hastings, P. Wiemer-Hastings,and R. Kreuz.
1999.
Autotutor: A simulation of ahuman tutor.
Cognitive Systems Research, 1:35?51.B.
A. Hockey, O.
Lemon, E. Campana, L. Hiatt, G. Aist,J.
Hieronymus, A. Gruenstein, and J. Dowding.
2003.Targeted help for spoken dialogue systems: intelligentfeedback improves naive users?
performance.
In Pro-ceedings of EACL.P.
Jordan, M. Makatchev, U. Pappuswamy, K. VanLehn,and P. Albacete.
2006.
A natural language tuto-rial dialogue system for physics.
In Proceedings ofFLAIRS?06.L.
Karsenty.
2001.
Adapting verbal protocol methods toinvestigate speech systems use.
Applied Ergonomics,32:15?22.S.
Katz, A. Lesgold, E. Hughes, D. Peters, G. Eggan,M.
Gordin, and L. Greenberg.
1998.
Sherlock 2: Anintelligent tutoring system built on the lrdc framework.In C. Bloom and R. Loftin, editors, Facilitating thedevelopment and use of interactive learning environ-ments.
ERLBAUM.S.
Mo?ller, K.-P. Engelbrecht, and A. Oulasvirta.
2007.Analysis of communication failures for spoken dia-logue systems.
In Proceedings of Interspeech.R.
D. Nielsen, W. Ward, and J. H. Martin.
2008.
Clas-sification errors in a domain-independent assessmentsystem.
In Proc.
of the Third Workshop on InnovativeUse of NLP for Building Educational Applications.T.
Paek.
2003.
Toward a taxonomy of communicationerrors.
In Proceedings of ISCA Workshop on ErrorHandling in Spoken Dialogue Systems.A.
Purandare and D. Litman.
2008.
Content-learningcorrelations in spoken tutoring dialogs at word, turnand discourse levels.
In Proc.of FLAIRS.R.
W. Smith and S. A. Gordon.
1997.
Effects of variableinitiative on linguistic behavior in human-computerspoken natural language dialogue.
ComputationalLinguistics.J.
D. Williams and S. Young.
2007.
Scaling POMDPs forspoken dialog management.
IEEE Trans.
on Audio,Speech, and Language Processing, 15(7):2116?2129.45
