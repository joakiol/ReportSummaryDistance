The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 208?215,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsInforming Determiner and Preposition Error Correction with Word ClustersAdriane Boyd Marion Zepf Detmar MeurersSeminar fu?r SprachwissenschaftUniversita?t Tu?bingen{adriane,mzepf,dm}@sfs.uni-tuebingen.deAbstractWe extend our n-gram-based data-driven pre-diction approach from the Helping Our Own(HOO) 2011 Shared Task (Boyd and Meur-ers, 2011) to identify determiner and preposi-tion errors in non-native English essays fromthe Cambridge Learner Corpus FCE Dataset(Yannakoudakis et al, 2011) as part of theHOO 2012 Shared Task.
Our system focuseson three error categories: missing determiner,incorrect determiner, and incorrect preposi-tion.
Approximately two-thirds of the errorsannotated in HOO 2012 training and test datafall into these three categories.
To improveour approach, we developed a missing deter-miner detector and incorporated word cluster-ing (Brown et al, 1992) into the n-gram pre-diction approach.1 IntroductionWe extend our n-gram-based prediction approach(Boyd and Meurers, 2011) from the HOO 2011Shared Task (Dale and Kilgarriff, 2011) for the HOO2012 Shared Task.
This approach is an extensionof the preposition prediction approach presentedin Elghafari, Meurers and Wunsch (2010), whichuses a surface-based approach to predict preposi-tions in English using frequency information fromweb searches to choose the most likely prepositionin a given context.
For each preposition in the text,the prediction algorithm considers up to three wordsof context on each side of the preposition, buildinga 7-gram with a preposition slot in the middle:rather a question the scales fallingFor each prediction task, a cohort of queries is con-structed with each of the candidate prepositions inthe slot to be predicted:1. rather a question of the scales falling2.
rather a question to the scales falling3.
rather a question in the scales falling.
.
.9. rather a question on the scales fallingIn Elghafari, Meurers and Wunsch (2010), thequeries are submitted to the Yahoo search engineand in Boyd and Meurers (2011), the search engineis replaced with the ACL Anthology Reference Cor-pus (ARC, Bird et al, 2008), which contains texts ofthe same genre as the HOO 2011 data.
If no hits arefound for any of the 7-gram queries, shorter over-lapping n-grams are used to approximate the 7-gramquery.
For instance, a 7-gram may be approximatedby two overlapping 6-grams:[rather a question of the scales falling]?
[rather a question of the scales][a question of the scales falling]If there are still no hits, the overlap backoff willcontinue reducing the n-gram length until it reaches3-grams with one word of context on each side ofthe candidate correction.
If no hits are found atthe 3-gram level, the Boyd and Meurers (2011) ap-proach predicts the original token, effectively mak-ing no modifications to the original text.
The ap-proach from Elghafari, Meurers and Wunsch (2010),addressing a prediction task rather than a correctiontask (i.e., the original token is masked), predicted themost frequent preposition of if no hits were found.208Elghafari, Meurers and Wunsch (2010) showedthis surface-based approach to be competitive withpublished state-of-the-art machine learning ap-proaches using complex feature sets (Gamon et al,2008; De Felice, 2008; Tetreault and Chodorow,2008; Bergsma et al, 2009).
For a set of nine fre-quent prepositions (of, to, in, for, on, with, at, by,from), they accurately predicted 76.5% on nativedata from section J of the British National Corpus.For these nine prepositions, De Felice (2008) iden-tified a baseline of 27% for the task of choosinga preposition in a slot (choose of ) and her systemachieved 70.1% accuracy.
Humans performing thesame task agree 89% of the time (De Felice, 2008).For the academic texts in the HOO 2011 SharedTask, Boyd and Meurers (2011) detected 67% of de-terminer and preposition substitution errors (equiva-lent to detection recall in the current task) and pro-vided the appropriate correction for approximatelyhalf of the detected cases.
We achieved a detectionF-score of approximately 80% and a correction F-score of 44% for the four function word predictiontasks we considered (determiners, prepositions, con-junctions, and quantifiers).2 Our ApproachFor the 2012 shared task corpus, we do not havethe advantage of access to a genre-specific referencecorpus such as the ARC used for the first challenge,so we instead use the Google Web 1T 5-gram Cor-pus (Web1T5, Brants and Franz, 2006), which con-tains 1-gram to 5-gram counts for a web corpus withapproximately 1 trillion tokens and 95 billion sen-tences.
Compared to our earlier approach, using theWeb1T5 corpus reduces the size of available contextby going from 7-grams to 5-grams, but we are inten-tionally keeping the corpus resources and algorithmsimple.
We are particularly interested in exploringthe space between surface forms and abstractionsby incorporating information from word clustering,an issue which is independent from the choice of amore sophisticated learning algorithm.Rozovskaya and Roth (2011) compared a range oflearning algorithms for the task of correcting errorsmade by non-native writers, including an averagedperceptron algorithm (Rizzolo and Roth, 2007) andan n-gram count-based approach (Bergsma et al,2009), which is similar to our approach.
They foundthat the count-based approach performs nearly aswell as the averaged perceptron approach whentrained with ten times as much data.
Without accessto a large multi-genre corpus even a tenth the sizeof the Web1T5 corpus, we chose to use Web1T5.Our longest queries thus are 5-grams with at leastone word of context on each side of the candidatefunction word and the shortest are 3-grams withone word of context on each side.
A large multi-genre corpus would improve the results by support-ing access to longer n-grams, and it would also makedeeper linguistic analysis such as part-of-speech tag-ging feasible.Table 1 shows the sets of determiners and prepo-sitions for each of the three categories addressed byour system: missing determiner (MD), incorrect de-terminer (RD), and incorrect preposition (RT).
Thefunction word lists are compiled from all single-word corrections of these types in the training data.The counts show the frequency of the error types inthe test data, along with the total frequency of func-tion word candidates.The following sections describe the main exten-sions to our system for the 2012 shared task: a sim-ple correction probability model, a missing deter-miner detector, and the addition of hierarchical wordclustering to the prediction approach.2.1 Correction Probability ModelTo adapt the system for the CLC FCE learner data,we added a simple correction probability model tothe n-gram predictor that multiplies the counts foreach n-gram by the probability of a particular re-placement in the training data.
The model includesboth correct and incorrect occurrences of each can-didate, ignoring any corrections that make up lessthan 0.5% of the corrections for a particular token.For instance, the word among has the following cor-rection probabilities: among 0.7895, from 0.1053,between 0.0526.
Even such a simplistic probabilitymodel has a noticeable effect on the system perfor-mance, improving the overall correction F-score byapproximately 3%.
The preposition substitution er-ror detection F-score alone improves by 9%.Prior to creating the probability model, we exper-imented with the addition of a bias toward the origi-nal token, which we hoped would reduce the number209Category # Errors Candidate Corrections # OccurrencesOriginal RevisedMD 125 131 a, an, another, any, her, his, its, my, our, that,the, their, these, this, those, which, your-RD 39 37 a, an, another, any, her, his, its, my, our, that,the, their, these, this, those, which, your1924RT 136 148 about, after, against, along, among, around, as,at, before, behind, below, between, by,concerning, considering, during, for, from, in,into, like, near, of, off, on, onto, out, outside,over, regarding, since, through, throughout, till,to, toward, towards, under, until, via, with,within, without2202Table 1: Single-Word Prepositions and Determiners with Error and Overall Frequency in Test Dataof overcorrections generated by our system.
With-out the probability model, a bias toward the originaltoken improves the results, however, with the prob-ability model, the bias is no longer useful.2.2 Word ClusteringIn the 2011 shared task, we observed that data spar-sity issues are magnified in non-native texts becausethe n-gram context may contain additional errorsor other infrequent or unusual n-gram sequences.We found that abstracting to part-of-speech tagsand lemmas in certain contexts leads to small im-provements in system performance.
For the 2012shared task, we explore the effects of abstracting toword clusters derived from co-occurrence informa-tion (Brown et al, 1992), another type of abstractionrelevant to our n-gram prediction approach.
We hy-pothesize that replacing tokens in the n-gram contextin our prediction tasks with clusters will reduce thedata sparsity for non-native text.Clusters derived from co-occurrence frequenciesoffer an attractive type of abstraction that occupya middle ground between relatively coarse-grainedmorphosyntactic abstractions such as part-of-speechtags and fine-grained abstractions such as lemmas.For determiner and preposition prediction, part-of-speech tags clearly retain too few distinctions.
Forexample, the choice of a/an before a noun phrase de-pends on the onset of the first word in the phrase, in-formation which is not preserved by part-of-speechtagging.
Likewise, preposition selection may be de-pendent on lexical specifications (e.g., phrasal verbssuch as depend on) or on semantic or world knowl-edge (cf.
Wechsler, 1994).Brown et al (1992) present a hierarchical wordclustering algorithm that can handle a large num-ber of classes and a large vocabulary.
The algorithmclusters a vocabulary into C clusters given a corpusto estimate the parameters of an n-gram languagemodel.
Summarized briefly, the algorithm first cre-ates C clusters for the C most frequent words inthe corpus.
Then, a cluster is added containing thenext most frequent word.
After the new cluster isadded, the pair of clusters is merged for which theloss in average mutual information is smallest, re-turning the number of clusters to C. The remainingwords in the vocabulary are added one by one andpairs of clusters are merged in the same fashion un-til all words have been divided into C clusters.Using the implementation from Liang (2005),1we generate word clusters for the most frequent100,000 tokens in the ukWaC corpus (Baroni et al,2009).
We convert all tokens to lower case, replaceall lower frequency words with a single unique to-ken, and omit from the clustering the candidate cor-rections from Table 1 along with the low frequencytokens.
Our corpus is the first 18 million sentencesfrom ukWaC.2 After converting all tokens to lower-case and omitting the candidate function words, atotal of 75,333 tokens are clustered.We create three sets of clusters with sizes 500,1000, and 2000.
Due to time constraints, we did notyet explore larger sizes.
Brown et al (1992) reportthat the words in a cluster appear to share syntac-tic or semantic features.
The clusters we obtainedappear to be overwhelmingly semantic in nature.1Available at http://cs.stanford.edu/?pliang/software2Those sentences in the file ukwac dep parsed 01.210Cluster ID Selected Cluster Members(1) 00100 was..., woz, wasn?t, was, wasnt(2) 0111110111101 definetly, definatly, assuredly, definately, undoubtedly, certainly, definitely(3) 1001110100 extremely, very, incredibly, inordinately, exceedingly, awfully(4) 1110010001 john, richard, peter, michael, andrew, david, stephen(5) 11101001001 12.30pm, 7am, 2.00pm, 4.00pm, weekday, tuesdaysTable 2: Sample Clusters from ukWaC with 2000 ClustersTable 2 shows examples from the set of 2000 clus-ters.
Examples (1) and (2) show how tokens witherrors in tokenization or misspellings are clusteredwith tokens with standard spelling and standard tok-enization.
Such clusters may be useful for the sharedtask by allowing the system to abstract away fromspelling errors in the learner essays.
Examples (3)?
(5) show semantically similar clusters.An excerpt of the hierarchical cluster tree for thecluster ID from example (3) is shown in Figure 1.The tree shows a subset of the clusters for clusterIDs beginning with the sequence 1001110.
Each bi-nary branch appends a 0 or 1 to the cluster ID asshown in the edge labels.
The cluster 1001110100(extremely, very) is found in the left-most leaf ofthe right branch.
A few of the most frequent clus-ter members are shown for each leaf of the tree.In our submissions to the shared task, we includedfive different cluster settings: 1) using the originalword-based approach with no clusters, 2) using only2000 clusters, 3) using the word-based approach ini-tially and backing off to 2000 clusters if no hits arefound, 4) backing off to 1000 clusters, and 5) back-ing off to 500 clusters.
The detailed results will bepresented in section 3.2.3 Missing Determiner DetectorWe newly developed a missing determiner detectorto identify those places in the learner text wherea determiner is missing.
Since determiners mostlyoccur in noun phrases, we extract all noun phrasesfrom the text and put them through a two-stage clas-sifier.
For a single-stage classifier, always predict-ing ?no error?
leads to a very high baseline accu-racy of 98%.
Therefore, we first filter out thosenoun phrases which already contain a determiner, apossessive pronoun, another possessive token (e.g.,?s), or an existential there, or whose head is a pro-noun.
This prefiltering reduces the baseline accu-racy to 93.6%, but also filters out 10% of learner er-rors (false negatives), which thus cannot be detectedin stage two.In the second stage, a decision tree classifier de-cides for every remaining noun phrase whether a de-terminer is missing.
From the 203 features we orig-inally extracted to inform the classification, the chisquared algorithm selected 30.
Almost all of the se-lected features capture properties of either the headof the noun phrase, its first word, or the token im-mediately preceding the noun phrase.
We followMinnen et al (2000) in defining the head of a nounphrase as the rightmost noun, or if there is no noun,the rightmost token.
As suggested by Han et al(2004), the classifier considers the parts of speechof these three words, while the features that recordthe respective literal word were discarded.We also experimented with using the entire nounphrase and its part-of-speech tag sequence as fea-tures (Han et al, 2004), which proved not to behelpful due to the limited size of the training data.We replaced the part-of-speech tag sequence with anumber of boolean features that each indicate equiv-alence with a particular sequence.
Of these featuresonly the one that checks whether the whole nounphrase consists of a single common noun in the sin-gular was included in the final feature set.
Addi-tionally, the selected features include countabilityinformation from noun countability lists generatedby Baldwin and Bond (2003), which assign nounsto one or more countability classes: countable, un-countable/mass noun, bipartite, or plural only.The majority of the 30 selected features refer tothe position of one of the three tokens (head, firstword, and preceding token) in the cluster hierarchydescribed in section 2.2.
The set of 500 clustersproved not to be fine-grained enough, so we used2111001110100111011001110111001110111.
.
.1001110110slightlysignificantly0 1100111010100111010110011101011. .
.10011101010terriblyquite0 11001110100extremelyvery0 10 110011100100111001more100111000fewerless0 10 1Figure 1: Hierarchical Clustering Subtree for Cluster Prefix 1001110the set of 1000 clusters.
To take full advantage of thehierarchical nature of the cluster IDs, we extract pre-fixes of all possible lengths (1?18 characters) fromthe cluster ID of the respective token.
For the headand the first word, prefixes of length 3?14 were se-lected by the attribute selector, in addition to a prefixof length 6 for the preceding token?s cluster ID.Among the discarded features are many extractedfrom the context surrounding the noun phrase, in-cluding the parts of speech and cluster membershipof three words to the left and right of the nounphrase, excluding the immediately preceding token.Features referring to possible sister conjuncts of thenoun phrase, the next 3rd person pronoun in a fol-lowing sentence, or previous occurrences of the headin the text also turned out not to be useful.
The per-formance of the classifier was only marginally af-fected by the reduction in the number of features.We conclude from this that missing determiner de-tection is sufficiently informed by local features.In order to increase the robustness of the classifier,we generated additional data from the written por-tion of the BNC by removing a determiner in 20% ofall sentences.
The resulting rate of errors is roughlyequal to the rate of errors in the learner texts and theaddition of the BNC data increases the amount oftraining data by a factor of 13.
We trained a classifieron both datasets (referred to as HOO-BNC below).It achieves an F-score of 46.7% when evaluated on30% of the shared task training data, which was heldout from the classifier training data.
On the revisedtest data, it reaches an F-score of 44.5%.3 ResultsThe following two sections discuss our overall re-sults for the shared task and our performance on thethree error types targeted by our system.3.1 OverallFigure 2 shows the overall recognition and correc-tion F-score for the cluster settings described insection 2.2.
With the missing determiner detec-tor HOO-BNC described in section 2.3, these cor-respond to runs #5?9 submitted to the shared task.For the unrevised data, Run #6 (2000 clusters only)gives our best result for overall detection F-score(30.26%) and Run #7 (2000 cluster backoff) for cor-rection F-score (18.44%).
For the revised data, Run212051015202530NoClusters2,000Clusters2,000Backoff1,000Backoff500BackoffF?ScoreCluster SettingsRecognitionCorrectionFigure 2: Recognition and Correction F-Score with Clustering#7 (2000 cluster backoff) has our best overall detec-tion F-score (32.21%) and Run #5 (no clusters) hasour best overall correction F-score (22.46%).Runs using clusters give the best results in twoother metrics reported in the shared task results forthe revised data.
Run #6 (2000 clusters only) givesthe best results for determiner correction F-score andRun #2 (2000 cluster backoff), which differs onlyfrom Run #7 in the choice of missing determiner de-tector, gives the best results for preposition detectionand recognition F-scores.The detailed results for Runs #5?9 with the re-vised data are shown in Figure 2.
This graph showsthat the differences between the systems with andwithout clusters are very small.
The recognition F-score is best with 2000 cluster backoff and the cor-rection F-score is best with no clusters.
In bothcases, the difference between the top two results isless than 0.01.
There is, however, a noticeable in-crease in performance as the number of clusters in-creases, which indicates that a larger number of clus-ters may improve results further.
The set of 2000clusters may still retain too few distinctions for thistask.3.2 Targeted Error TypesOur system handles three of the six error types in theshared task: missing determiner (MD), incorrect de-terminer (RD), and incorrect preposition (RT).
Therecognition and correction F-scores for our best-forming run for each type are shown in Figure 3.051015202530354045MDRD RTF?ScoreError TypeRecognitionCorrectionFigure 3: Recognition and Correction F-Score for theTargeted Error TypesIn a comparison of performance on individual er-ror types in the shared task, our system does beston the task for which it was originally developed,213preposition prediction.
We place 4th in recognitionand 3rd in correction F-score for this error type.
Formissing determiner (MD) and incorrect determiner(RD) errors, our system is ranked similarly as in ouroverall performance (4th?6th).For the sake of replicability, as the HOO 2012 testdata is not publicly available, we include our resultson the HOO training data for the preposition and de-terminer substitution errors in Table 3.Error No ClustersType Recognition CorrectionPrec Rec Prec RecRT 32.69 29.94 24.85 22.77RD 10.63 18.56 8.37 14.61Error 2000 BackoffType Recognition CorrectionPrec Rec Prec RecRT 25.87 35.60 18.26 25.13RD 9.71 23.65 7.48 18.23Table 3: Results for HOO 2012 Training DataResults are reported for the no cluster and 2000cluster backoff settings, which show that incorpo-rating the cluster backoff improves recall at the ex-pense of precision.
Missing determiner errors arenot reported directly as the missing determiner de-tector was trained on the training data, but see theevaluation at the end of section 2.3.4 Discussion and ConclusionThe n-gram prediction approach with the new miss-ing determiner detector performed well in the HOO2012 Shared Task, placing 6th in terms of detectionand 5th in terms of correction out of fourteen teamsparticipating in the shared task.
In our best sub-missions evaluated using the revised test data, weachieved a detection F-score of 32.71%, a recogni-tion F-score of 29.21% and a correction F-score of22.73%.
For the three error types addressed by ourapproach, our correction F-scores are 39.17% formissing determiners, 9.23% for incorrect determin-ers, and 30.12% for incorrect prepositions.
Informa-tion from hierarchical word clustering (Brown et al,1992) extended the types of abstractions availableto our n-gram prediction approach and improved theperformance of the missing determiner detector.For the n-gram prediction approach, word clustersIDs from the hierarchical word clustering replace to-kens in the surrounding context in order to improverecall for learner texts which may contain errorsor infrequent token sequences.
The use of cluster-based contexts with 2000 clusters as a backoff fromthe word-based approach leads to a very small im-provement in the overall recognition F-score for theHOO 2012 Shared Task, but our best overall correc-tion F-score was obtained using our original word-based approach.
The differences between the word-based and cluster-based approaches are quite small,so we did not see as much improvement from theword cluster abstractions as we had hoped.
Weexperimented with sets of clusters of several sizes(500, 1000, 2000) and found that as the numberof clusters becomes smaller, the performance de-creases, suggesting that a larger number of clustersmay lead to more improvement for this task.Information from the word cluster hierarchy wasalso integrated into our new missing determiner de-tector, which uses a decision tree classifier to decidewhether a determiner should be inserted in front ofa determiner-less NP.
Lexical information from theextracted noun phrases and surrounding context arenot as useful for the classifier as information aboutthe position of the tokens in the word cluster hier-archy.
In particular, cluster information appears tohelp compensate for lexical sparsity given a rela-tively small amount of training data.In future work, we plan to explore additional clus-tering approaches and to determine when the use ofword cluster abstractions is helpful for the task ofpredicting determiners, prepositions, and other func-tion words.
An approach that refers to word clus-ters in certain contexts or in a customized fashionfor each candidate correction may lead to improvedperformance for the task of detecting and correctingsuch errors in texts by non-native writers.ReferencesTimothy Baldwin and Francis Bond, 2003.
Learn-ing the countability of English nouns from corpusdata.
In Proceedings of the 41st Annual Meet-ing on Association for Computational Linguistics(ACL).
pp.
463?470.214M.
Baroni, S. Bernardini, A. Ferraresi andE.
Zanchetta, 2009.
The WaCky Wide Web: ACollection of Very Large Linguistically ProcessedWeb-Crawled Corpora.
Language Resources andEvaluation, 43(3):209?226.Shane Bergsma, Dekang Lin and Randy Goebel,2009.
Web-scale N-gram models for lexical dis-ambiguation.
In Proceedings of the 21st interna-tional jont conference on Artifical intelligence (IJ-CAI?09).
Morgan Kaufmann Publishers Inc., SanFrancisco, CA, USA.Steven Bird, Robert Dale et al, 2008.
The ACL An-thology Reference Corpus.
In Proceedings of the6th International Conference on Language Re-sources and Evaluation (LREC).
Marrakesh, Mo-rocco.Adriane Boyd and Detmar Meurers, 2011.
Data-Driven Correction of Function Words in Non-Native English.
In Proceedings of the 13th Eu-ropean Workshop on Natural Language Genera-tion ?
Helping Our Own (HOO) Challenge.
As-sociation for Computational Linguistics, Nancy,France.Thorsten Brants and Alex Franz, 2006.
Web 1T5-gram Version 1.
Linguistic Data Consortium.Philadelphia.Peter F. Brown, Peter V. deSouza, Robert L. Mer-cer, T. J. Watson, Vincent J. Della Pietra andJenifer C. Lai, 1992.
Class-Based n-gram Modelsof Natural Language.
Computational Linguistics,18(4):467?479.Robert Dale and Adam Kilgarriff, 2011.
HelpingOur Own: The HOO 2011 Pilot Shared Task.
InProceedings of the 13th European Workshop onNatural Language Generation.
Nancy, France.Rachele De Felice, 2008.
Automatic Error Detectionin Non-native English.
Ph.D. thesis, Oxford.Anas Elghafari, Detmar Meurers and Holger Wun-sch, 2010.
Exploring the Data-Driven Predictionof Prepositions in English.
In Proceedings of the23rd International Conference on ComputationalLinguistics (COLING).
Beijing.Michael Gamon, Jianfeng Gao et al, 2008.
Us-ing Contextual Speller Techniques and LanguageModeling for ESL Error Correction.
In Proceed-ings of the Third International Joint Conferenceon Natural Language Processing.
Hyderabad.Na-Rae Han, Martin Chodorow and Claudia Lea-cock, 2004.
Detecting Errors in English Arti-cle Usage with a Maximum Entropy ClassifierTrained on a Large, Diverse Corpus.
In Proceed-ings of the 4th International Conference on Lan-guage Resources and Evaluation (LREC).
Lisbon.Percy Liang, 2005.
Semi-Supervised Learning forNatural Language.
Master?s thesis, MassachusettsInstitute of Technology.Guido Minnen, Francis Bond and Ann Copestake,2000.
Memory-based learning for article gener-ation.
In Proceedings of the 2nd Workshop onLearning Language in Logic and the 4th Confer-ence on Computational Natural Language Learn-ing.
volume 7, pp.
43?48.Nick Rizzolo and Dan Roth, 2007.
Modeling Dis-criminative Global Inference.
In Proceedings ofthe First International Conference on SemanticComputing (ICSC).
IEEE, Irvine, California, pp.597?604.Alla Rozovskaya and Dan Roth, 2011.
AlgorithmSelection and Model Adaptation for ESL Cor-rection Tasks.
In Proceedings of the 49th An-nual Meeting of the Association for Computa-tional Linguistics: Human Language Technolo-gies (ACL-HLT).
Portland, Oregon.Joel Tetreault and Martin Chodorow, 2008.
NativeJudgments of Non-Native Usage: Experiments inPreposition Error Detection.
In Proceedings of the22nd International Conference on ComputationalLinguistics (COLING).
Manchester.Stephen Wechsler, 1994.
Preposition Selection Out-side the Lexicon.
In Raul Aranovich, WilliamByrne, Susanne Preuss and Martha Senturia(eds.
), Proceedings of the Thirteenth West CoastConference on Formal Linguistics.
CSLI Publica-tions, Stanford, California, pp.
416?431.H.
Yannakoudakis, T. Briscoe and B. Medlock,2011.
A new dataset and method for automati-cally grading ESOL texts.
In Proceedings of the49th Annual Meeting of the Association for Com-putational Linguistics: Human Language Tech-nologies (ACL-HLT).215
