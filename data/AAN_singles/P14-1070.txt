Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 743?753,Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational LinguisticsStrategies for Contiguous Multiword Expression Analysis andDependency ParsingMarie CanditoAlpageParis Diderot UnivINRIAmarie.candito@linguist.univ-paris-diderot.frMatthieu ConstantUniversite?
Paris-EstLIGMCNRSMatthieu.Constant@u-pem.frAbstractIn this paper, we investigate various strate-gies to predict both syntactic dependencyparsing and contiguous multiword expres-sion (MWE) recognition, testing them onthe dependency version of French Tree-bank (Abeille?
and Barrier, 2004), as in-stantiated in the SPMRL Shared Task(Seddah et al, 2013).
Our work focuseson using an alternative representation ofsyntactically regular MWEs, which cap-tures their syntactic internal structure.
Weobtain a system with comparable perfor-mance to that of previous works on thisdataset, but which predicts both syntacticdependencies and the internal structure ofMWEs.
This can be useful for capturingthe various degrees of semantic composi-tionality of MWEs.1 IntroductionA real-life parsing system should comprise therecognition of multi-word expressions (MWEs1),first because downstream semantic-oriented ap-plications need some marking in order to dis-tinguish between regular semantic compositionand the typical semantic non-compositionality ofMWEs.
Second, MWE information, is intuitivelysupposed to help parsing.That intuition is confirmed in a classical butnon-realistic setting in which gold MWEs are pre-grouped (Arun and Keller, 2005; Nivre and Nils-son, 2004; Eryig?it et al, 2011).
But the situationis much less clear when switching to automaticMWE prediction.
While Cafferkey et al (2007)report a small improvement on the pure parsing1Multiword expressions can be roughly defined as con-tinuous or discontinuous sets of tokens, which either do notexhibit full freedom in lexical selection or whose meaning isnot fully compositional.
We focus in this paper on contiguousmultiword expressions, also known as ?words with spaces?.task when using external MWE lexicons to helpEnglish parsing, Constant et al (2012) report re-sults on the joint MWE recognition and parsingtask, in which errors in MWE recognition allevi-ate their positive effect on parsing performance.While the realistic scenario of syntactic pars-ing with automatic MWE recognition (either donejointly or in a pipeline) has already been investi-gated in constituency parsing (Green et al, 2011;Constant et al, 2012; Green et al, 2013), theFrench dataset of the SPMRL 2013 Shared Task(Seddah et al, 2013) only recently provided theopportunity to evaluate this scenario within theframework of dependency syntax.2 In such a sce-nario, a system predicts dependency trees withmarked groupings of tokens into MWEs.
Thetrees show syntactic dependencies between se-mantically sound units (made of one or severaltokens), and are thus particularly appealing fordownstream semantic-oriented applications, as de-pendency trees are considered to be closer topredicate-argument structures.In this paper, we investigate various strate-gies for predicting from a tokenized sentenceboth MWEs and syntactic dependencies, using theFrench dataset of the SPMRL 13 Shared Task.
Wefocus on the use of an alternative representationfor those MWEs that exhibit regular internal syn-tax.
The idea is to represent these using regularsyntactic internal structure, while keeping the se-mantic information that they are MWEs.We devote section 2 to related work.
In sec-tion 3, we describe the French dataset, how MWEsare originally represented in it, and we presentand motivate an alternative representation.
Sec-tion 4 describes the different architectures we test2The main focus of the Shared Task was on pre-dicting both morphological and syntactic analysis formorphologically-rich languages.
The French dataset is theonly one containing MWEs: the French treebank has theparticularity to contain a high ratio of tokens belonging toa MWE (12.7% of non numerical tokens).743for predicting both syntax and MWEs.
Section 5presents the external resources targeted to improveMWE recognition.
We describe experiments anddiscuss their results in section 6 and conclude insection 7.2 Related workWe gave in introduction references to previouswork on predicting MWEs and constituency pars-ing.
To our knowledge, the first works3 on predict-ing both MWEs and dependency trees are thosepresented to the SPMRL 2013 Shared Task thatprovided scores for French (which is the onlydataset containing MWEs).
Constant et al (2013)proposed to combine pipeline and joint systems ina reparser (Sagae and Lavie, 2006), and rankedfirst at the Shared Task.
Our contribution withrespect to that work is the representation of theinternal syntactic structure of MWEs, and use ofMWE-specific features for the joint system.
Thesystem of Bjo?rkelund et al (2013) ranked secondon French, though with close UAS/LAS scores.
Itis a less language-specific system that reranks n-best dependency parses from 3 parsers, informedwith features from predicted constituency trees.
Ituses no feature nor treatment specific to MWEs asit focuses on the general aim of the Shared Task,namely coping with prediction of morphologicaland syntactic analysis.Concerning related work on the representa-tion of MWE internal structure, we can cite thePrague Dependency Bank, which captures bothregular syntax of non-compositional MWEs andtheir MWE status, in two distinct annotation lay-ers (Bejc?ek and Stranak, 2010).
Our represen-tation also resembles that of light-verb construc-tions (LVC) in the hungarian dependency treebank(Vincze et al, 2010): the construction has regularsyntax, and a suffix is used on labels to express itis a LVC (Vincze et al, 2013).3 Data: MWEs in Dependency TreesThe data we use is the SPMRL 13 dataset forFrench, in dependency format.
It contains pro-jective dependency trees that were automaticallyderived from the latest status of the French Tree-bank (Abeille?
and Barrier, 2004), which con-sists of constituency trees for sentences from the3Concerning non contiguous MWEs, we can cite the workof Vincze et al (2013), who experimented joint dependencyparsing and light verb construction identification.newspaper Le Monde, manually annotated withphrase structures, morphological information, andgrammatical functional tags for dependents ofverbs.
The Shared Task used an enhanced versionof the constituency-to-dependency conversion ofCandito et al (2010), with different handling ofMWEs.
The dataset consists of 18535 sentences,split into 14759, 1235 and 2541 sentences fortraining, development, and final evaluation respec-tively.We describe below the flat representation ofMWEs in this dataset, and the modified represen-tation for regular MWEs that we propose.a.
Flat representation:L?
abus de biens sociaux fut de?nonce?
en vainsujdetdepcpddep cpddep cpdauxtpsmoddepcpdb.
Structured representation:L?
abus de biens sociaux fut de?nonce?
en vainsujdetdepobj.pmodauxtpsmoddepcpdFigure 1: French dependency tree for L?abus debiens sociaux fut de?nonce?
en vain (literally themisuse of assets social was denounced in vain,meaning The misuse of corporate assets was de-nounced in vain), containing two MWEs (in red).Top: original flat representation.
Bottom: Tree af-ter regular MWEs structuring.3.1 MWEs in Gold Data: Flat representationIn gold data, the MWEs appear in an expandedflat format: each MWE bears a part-of-speechand consists of a sequence of tokens (hereafterthe ?components?
of the MWE), each having theirproper POS, lemma and morphological features.In the dependency trees, there is no ?node?
for aMWE as a whole, but one node per MWE com-ponent (more generally one node per token).
Thefirst component of a MWE is taken as the headof the MWE.
All subsequent components of theMWE depend on the first one, with the speciallabel dep_cpd (hence the name flat represen-744tation).
Furthermore, the first MWE componentbears a feature mwehead equal to the POS of theMWE.
An example is shown in Figure 1.
TheMWE en vain (pointlessly) is an adverb, contain-ing a preposition and an adjective.
The latter de-pends on former, which bears mwehead=ADV+.The algorithm to recover MWEs is: any nodehaving dependents with the dep_cpd label formsa MWE with such dependents.3.2 Alternative representation for regularMWEsIn the alternative representation we propose, ir-regular MWEs are unchanged and appear as flatMWEs (e.g.
en vain in Figure 1 has pattern prepo-sition+adjective, which is not considered regularfor an adverb, and is thus unchanged).
RegularMWEs appear with ?structured?
syntax: we mod-ify the tree structure to recover the regular syn-tactic dependencies.
For instance, in the bottomtree of the figure, biens is attached to the prepo-sition, and the adjective sociaux is attached to bi-ens, with regular labels.
Structured MWEs can-not be spotted using the tree topology and la-bels only.
Features are added for that purpose:the syntactic head of the structured MWE bearsa regmwehead for the POS of the MWE (abusin Figure 1), and the other components of theMWE bear a regcomponent feature (the orangetokens in Figure 1).4 With this representation,the algorithm to recover regular MWEs is: anynode bearing regmwehead forms a MWE withthe set of direct or indirect dependents bearing aregcomponent feature.3.2.1 MotivationsOur first motivation is to increase the quantity ofinformation conveyed by the dependency trees,by distinguishing syntactic regularity and seman-tic regularity.
Syntactically regular MWEs (here-after regular MWEs) show various degrees of se-mantic non-compositionality.
For instance, in theFrench Treebank, population active (lit.
activepopulation, meaning ?working population?)
is apartially compositional MWE.
Furthermore, somesequences are both syntactically and semanticallyregular, but encoded as MWE due to frozen lexi-cal selection.
This is the case for de?ficit budge?taire(lit.
budgetary deficit, meaning ?budget deficit?
),4The syntactic head of a structured MWE may not be thefirst token, whereas the head token of a flat MWE is alwaysthe first one.because it is not possible to use de?ficit du bud-get (budget deficit).
Our alternative representa-tion distinguishes between syntactic internal reg-ularity and semantic regularity.
This renders thesyntactic description more uniform and it providesan internal structure for regular MWEs, which ismeaningful if the MWE is fully or partially com-positional.
For instance, it is meaningful to havethe adjective sociaux attach to biens instead of onthe first component abus.
Moreover, such a dis-tinction opens the way to a non-binary classifica-tion of MWE status: the various criteria leading toclassify a sequence as MWE could be annotatedseparately and using nominal or scaled categoriesfor each criteria.
For instance, de?ficit budge?tairecould be marked as fully compositional, but withfrozen lexical selection.
Further, annotation is of-ten incoherent for the MWEs with both regularsyntax and a certain amount of semantic compo-sitionality, the same token sequence (with samemeaning) being sometimes annotated as MWEand sometimes not.More generally, keeping a regular representa-tion would allow to better deal with the interac-tion between idiomatic status and regular syntax,such as the insertion of modifiers on MWE sub-parts (e.g.
make a quick decision).Finally, using regular syntax for MWEs pro-vides a more uniform training set.
For instance fora sequence N1 preposition N2, though some exter-nal attachments might vary depending on whetherthe sequence forms a MWE or not, some maynot, and the internal dependency structure (N1 ?
(preposition ?
N2)) is quite regular.
One objec-tive of the current work is to investigate whetherthis increased uniformity eases parsing or whetherit is mitigated by the additional difficulty of find-ing the internal structure of a MWE.Total Nb of regular MWEsnb of (% of nouns, adverbs,MWEs prepositions, verbs)train 23658 12569 (64.7, 19.2, 14.6, 1.5)dev 2120 1194 (66.7, 17.7, 14.7, 0.8)test 4049 2051 (64.5, 19.9, 13.6, 2.0)Table 1: Total number of MWEs and number ofregular MWEs in training, development and testset (and broken down by POS of MWE).7453.2.2 ImplementationWe developed an ad hoc program for structur-ing the regular MWEs in gold data.
MWEs arefirst classified as regular or irregular, using reg-ular expressions over the sequence of parts-of-speech within the MWE.
To define the regularexpressions, we grouped gold MWEs accordingto the pair [global POS of the MWE + sequenceof POS of the MWE components], and designedregular expressions to match the most frequentpatterns that looked regular according to our lin-guistic knowledge.
The internal structure for thematching MWEs was built deterministically, us-ing heuristics favoring local attachments.5 Table 1shows the proportions of MWEs classified as regu-lar, and thus further structured.
About half MWEsare structured, and about two thirds of structuredMWEs are nouns.For predicted parses with structured MWEs, weuse an inverse transformation of structured MWEsinto flat MWEs, for evaluation against the golddata.
When a predicted structured MWE is flat-tened, all the dependents of any token of the MWEthat are not themselves belonging to the MWE areattached to the head component of the MWE.3.3 Integration of MWE features into labelsIn some experiments, we make use of alterna-tive representations, which we refer later as ?la-beled representation?, in which the MWE featuresare incorporated in the dependency labels, so thatMWE composition and/or the POS of the MWE betotally contained in the tree topology and labels,and thus predictable via dependency parsing.
Fig-ure shows the labeled representation for the sen-tence of Figure 1.For flat MWEs, the only missing information isthe MWE part-of-speech: we concatenate it to thedep_cpd labels.
For instance, the arc from ento vain is relabeled dep_cpd_ADV.
For struc-tured MWEs, in order to get full MWE accountwithin the tree structure and labels, we need to in-corporate both the MWE POS, and to mark it as5The six regular expressions that we obtained cover nomi-nal, prepositional, adverbial and verbal compounds.
We man-ually evaluated both the regular versus irregular classificationand the structuring of regular MWEs on the first 200 MWEsof the development set.
113 of these were classified as regu-lar, and we judged that all of them were actually regular, andwere correctly structured.
Among the 87 classified as irregu-lar, 7 should have been tagged as regular and structured.
For4 of them, the classification error is due to errors on the (gold)POS of the MWE components.c.
Labeled representation:L?
abus de biens sociaux fut de?nonce?
en vainsujdetdepr Nobj.pr Nmodr NauxtpsmoddepcpdADVFigure 2: Integration of all MWE information intolabels for the example of Figure 1.belonging to a MWE.
The suffixed label has theform FCT_r_POS.
For instance, in bottom treeof Figure 1, arcs pointing to the non-head compo-nents (de, biens, sociaux) are suffixed with _r tomark them as belonging to a structured MWE, andwith _N since the MWE is a noun.In both cases, this label suffixing is translatedback into features for evaluation against gold data.4 Architectures for MWE Analysis andParsingThe architectures we investigated vary dependingon whether the MWE status of sequences of to-kens is predicted via dependency parsing or via anexternal tool (described in section 5), and this di-chotomy applies both to structured MWEs and flatMWEs.
More precisely, we consider the followingalternative for irregular MWEs:?
IRREG-MERGED: gold irregular MWEs aremerged for training; for parsing, irregularMWEs are predicted externally, merged intoone token at parsing time, and re-expandedinto several tokens for evaluation;?
IRREG-BY-PARSER: the MWE status, flattopology and POS are all predicted via de-pendency parsing, using representations fortraining and parsing, with all information forirregular MWEs encoded in topology and la-bels (as for in vain in Figure 2).For regular MWEs, their internal structure is al-ways predicted by the parser.
For instance the un-labeled dependencies for abus de biens sociaux arethe same, independently of predicting whether it746forms a MWE or not.
But we use two kinds ofpredictions for their MWE status and POS:?
REG-POST-ANNOTATION: the regularMWEs are encoded/predicted as shown forabus de biens sociaux in bottom tree ofFigure 1, and their MWE status and POS ispredicted after parsing, by an external tool.?
REG-BY-PARSER: all regular MWE infor-mation (topology, status, POS) is predictedvia dependency parsing, using representa-tions with all information for regular MWEsencoded in topology and labels (Figure 2).Name prediction of prediction ofreg MWEs irreg MWEsJOINT irreg-by-parser reg-by-parserJOINT-REG irreg-merged reg-by-parserJOINT-IRREG irreg-by-parser reg-post-annotPIPELINE irreg-merged reg-post-annotTable 2: The four architectures, depending on howregular and irregular MWEs are predicted.We obtain four architectures, schematized in ta-ble 2.
We describe more precisely two of them,the other two being easily inferable:JOINT-REG architecture:?
training set: irregular MWEs merged intoone token, regular MWEs are structured, andintegration of regular MWE information intothe labels (FCT_r_POS).?
parsing: (i) MWE analysis with classifica-tion of MWEs into regular or irregular, (ii)merge of predicted irregular MWEs, (iii) tag-ging and morphological prediction, (iv) pars-ingJOINT-IRREG architecture:?
training set: flat representation of irregu-lar MWEs, with label suffixing (dep_cpd_POS), structured representation of regularMWEs without label suffixing.?
parsing: (i) MWE analysis and classifica-tion into regular or irregular, used for MWE-specific features, (ii) tagging and morpholog-ical prediction, (iii) parsing,We compare these four architectures betweenthem and also with two simpler architectures usedby (Constant et al, 2013) within the SPMRL 13Shared Task, in which regular and irregular MWEsare not distinguished:Uniform joint architecture: The joint systemsperform syntactic parsing and MWE analysis viaa single dependency parser, using representationsas in 3.3.Uniform pipeline architecture:?
training set: MWEs merged into one token?
parsing: (i) MWE analysis, (ii) merge of pre-dicted MWEs, (iii) tagging and morphologi-cal prediction, (iv) parsingFor each architecture, we apply the appropriatenormalization procedures on the predicted parses,in order to evaluate against (i) the pseudo-golddata in structured representation, and (ii) the golddata in flat representation.5 Use of external MWE resourcesIn order to better deal with MWE prediction, weuse external MWE resources, namely MWE lexi-cons and an MWE analyzer.
Both resources helpto predict MWE-specific features (section 5.3) toguide the MWE-aware dependency parser.
More-over, in some of the architectures, the externalMWE analyzer is used either to pre-group irreg-ular MWEs (for the architectures using IRREG-MERGED), or to post-annotate regular MWEs.5.1 MWE lexiconsMWE lexicons are exploited as sources of fea-tures for both the dependency parser and the ex-ternal MWE analyzer.
In particular, two large-coverage general-language lexicons are used: theLefff6 lexicon (Sagot, 2010), which contains ap-proximately half a million inflected word forms,among which approx.
25, 000 are MWEs; andthe DELA7 (Courtois, 2009; Courtois et al, 1997)lexicon, which contains approx.
one million in-flected forms, among which about 110, 000 areMWEs.
These resources are completed with spe-cific lexicons freely available in the platform Uni-tex8: the toponym dictionary Prolex (Piton et al,1999) and a dictionary of first names.
Note that thelexicons do not include any information on the ir-regular or the regular status of the MWEs.
In orderto compare the MWEs present in the lexicons andthose encoded in the French treebank, we appliedthe following procedure (hereafter called lexicon6We use the version available in the POS tagger MElt (De-nis and Sagot, 2009).7We use the version in the platform Unitex(http://igm.univ-mlv.fr/?unitex).
We had to convert theDELA POS tagset to that of the French Treebank.8http://igm.univ-mlv.fr/?unitex747lookup): in a given sentence, the maximum num-ber of non overlapping MWEs according to thelexicons are systematically marked as such.
Weobtain about 70% recall and 50% precision withrespect to MWE spanning.5.2 MWE AnalyzerThe MWE analyzer is a CRF-based sequential la-beler, which, given a tokenized text, jointly per-forms MWE segmentation and POS tagging (ofsimple tokens and of MWEs), both tasks mutu-ally helping each other9.
The MWE analyzer inte-grates, among others, features computed from theexternal lexicons described in section 5.1, whichgreatly improve POS tagging (Denis and Sagot,2009) and MWE segmentation (Constant and Tel-lier, 2012).
The MWE analyzer also jointly classi-fies its predicted MWEs as regular or irregular (thedistinction being learnt on gold training set, withstructured MWEs cf.
section 3.2).5.3 MWE-specific featuresWe introduce information from the external MWEresources in different ways:Flat MWE features: MWE information canbe integrated as features to be used by the de-pendency parser.
We tested to incorporate theMWE-specific features as defined in the gold flatrepresentation (section 3.1): the mwehead=POSfeature for the MWE head token, POS being thepart-of-speech of the MWE; the component=yfeature for the non-first MWE component.Switch: instead or on top of using the mweheadfeature, we use the POS of the MWE instead of thePOS of the first component of a flat MWE.
For in-stance in Figure 1, the token en gets pos=ADV in-stead of pos=P.
The intuition behind this featureis that for an irregular MWE, the POS of the lin-early first component, which serves as head, is notalways representative of the external distributionof the MWE.
For regular MWEs, the usefulness ofsuch a trick is less obvious.
The first componentof a regular MWE is not necessarily its head (forinstance for a nominal MWE with internal patternadjective+noun), so the switch trick could be detri-mental in such cases.109Note that in our experiments, we use this analyzer forMWE analysis only, and discard the POS tagging predic-tion.
Tagging is performed along with lemmatization withthe Morfette tool (section 6.1).10We also experimented to use POS of MWE plus suffixesto force disjoint tagsets for single words, irregular MWEs and6 Experiments6.1 Settings and evaluation metricsMWE Analysis and Tagging: For the MWEanalyzer, we used the tool lgtagger11 (version1.1) with its default set of feature templates, and a10-fold jackknifing on the training corpus.Parser: We used the second-order graph-basedparser available in Mate-tools12 (Bohnet, 2010).We used the Anna3.3 version, in projectivemode, with default feature sets and parametersproposed in the documentation, augmented or notwith MWE-specific features, depending on theexperiments.Morphological prediction: Predicted lemmas,POS and morphology features are computedwith Morfette version 0.3.5 (Chrupa?a et al,2008; Seddah et al, 2010)13, using 10 iterationsfor the tagging perceptron, 3 iterations for thelemmatization perceptron, default beam size forthe decoding of the joint prediction, and theLefff (Sagot, 2010) as external lexicon used forout-of-vocabulary words.
We performed a 10-foldjackknifing on the training corpus.Evaluation metrics: we evaluate our parsing sys-tems by using the standard metrics for depen-dency parsing: Labeled Attachment Score (LAS)and Unlabeled Attachment Score (UAS), com-puted using all tokens including punctuation.
Toevaluate statistical significance of parsing perfor-mance differences, we use eval07.pl14 with -b op-tion, and then Dan Bikel?s comparator.15 ForMWEs, we use the Fmeasure for recognition ofuntagged MWEs (hereafter FUM) and for recog-nition of tagged MWEs (hereafter FTM).6.2 MWE-specific feature predictionIn all our experiments, for the switch trick (section5.3), the POS of MWE is always predicted usingthe MWE analyzer.
For the flat MWE features, weexperimented both with features predicted by theMWE analyzer, and with features predicted usingthe external lexicons mentioned in section 5.1 (us-ing the lexicon lookup procedure).
Both kinds ofregular MWEs, but this showed comparable results.11http://igm.univ-mlv.fr/?mconstan12http://code.google.com/p/mate-tools/13https://sites.google.com/site/morfetteweb/14http://nextens.uvt.nl/depparse-wiki/SoftwarePage15The compare.pl script, formerly available atwww.cis.upenn.edu/ dbikel/748LABELED STRUCTURED FLATREPRES.
REPRESENTATION REPRESENTATIONMWE swi.
swi.
LAS UAS LAS FUM FTM LAS UAS FUM FTMARCHI feats irreg reg irreg irregbsline - - - 84.5 89.3 87.0 83.6 80.6 84.2 88.1 73.5 70.7JOINT best + + + 85.3 89.7 87.5 85.4 82.6 85.2 88.8 77.6 74.5JOINT- bsline - - - 84.7 89.4 87.0 83.5 80.3 84.5 88.0 78.3 75.9IRREG best + + + 85.1 89.8 87.4 85.0 81.6 84.9 88.3 79.0 76.5JOINT- bsline - NA - 84.2 89.1 86.7 84.2 80.8 84.0 88.0 73.3 70.3REG best + NA + 84.7 89.3 86.9 84.1 80.7 84.6 88.3 76.3 73.2PIPE bsline - NA - 84.6 89.2 86.9 84.1 80.7 84.5 87.9 78.8 76.3LINE best - NA + 84.7 89.4 87.0 84.2 80.8 84.6 88.1 78.8 76.3Table 3: Baseline and best results for the four MWE+parsing architectures on the dev set (see text forstatistical significance evaluation).
The UAS for the structured representation is the same as the one forthe labeled representation, and is not repeated.prediction lead to fairly comparable results, so inall the following, the MWE features, when used,are predicted using the external lexicons.6.3 Tuning features for each architectureWe ran experiments for all value combinationsof the following parameters: (i) the architecture,(ii) whether MWE features are used, whether theswitch trick is applied or not (iii) for irregularMWEs and (iv) for regular MWEs.We performed evaluation of the predicted parsesusing the three representations described in sec-tion 3, namely flat, structured and labeled repre-sentations.
In the last two cases, the evaluationis performed against an instance of the gold dataautomatically transformed to match the represen-tation type.
Moreover, for the ?labeled representa-tion?
evaluation, though the MWE information inthe predicted parses is obtained in various ways,depending on the architecture, we always map allthis information in the dependency labels, to ob-tain predicted parses matching the ?labeled repre-sentation?.
While the evaluation in flat represen-tation is the only one comparable to other workson this dataset, the other two evaluations provideuseful information.
In the ?labeled representation?evaluation, the UAS provides a measure of syn-tactic attachments for sequences of words, inde-pendently of the (regular) MWE status of subse-quences.
For the sequence abus de biens sociaux,suppose that the correct internal structure is pre-dicted, but not the MWE status.
The UAS forlabeled representation will be maximal, whereasfor the flat representation, the last two tokens willcount as incorrect for UAS.
For LAS, in both casesthe three last tokens will count as incorrect if thewrong MWE status is predicted.
So to sum up onthe ?labeled evaluation?, we obtain a LAS eval-uation for the whole task of parsing plus MWErecognition, but an UAS evaluation that penalizesless errors on MWE status, while keeping a rep-resentation that is richer: predicted parses containnot only the syntactic dependencies and MWE in-formation, but also a classification of MWEs intoregular and irregular, and the internal syntacticstructure of regular MWEs.The evaluation on ?structured representation?can be interpreted as an evaluation of the parsingtask plus the recognition of irregular MWEs only:both LAS and UAS are measured independentlyof errors on regular MWE status (note the UAS isexactly the same than in the ?labeled?
case).For each architecture, Table 3 shows the resultsfor two systems: first the baseline system withoutany MWE features nor switches and immediatelybelow the best settings for the architecture.
TheJOINT baseline corresponds to a ?pure?
joint sys-tem without external MWE resources (hence theminus sign for the first three columns).
For eacharchitecture except the PIPELINE one, differencesbetween the baseline and the best setting are sta-tistically significant (p < 0.01).
Differences be-tween best PIPELINE and best JOINT-REG arenot.
Best JOINT has statistically significant dif-ference (p < 0.01) over both best JOINT-REGand best PIPELINE.
The situation for best JOINT-IRREG with respect to the other three is borderline(with various p-values depending on the metrics).Concerning the tuning of parameters, it appearsthat the best setting is to use MWE-features, andswitch for both regular and irregular MWEs, ex-cept for the pipeline architecture for which resultswithout MWE features are slightly better.
So over-all, informing the parser with independently pre-749LABELED STRUCTURED FLATREPRESENTATION REPRESENTATION REPRESENTATIONLAS UAS LAS UAS FUM FTM LAS UAS FUM FTMSYSTEM irreg irregbaseline JOINT 84.13 88.93 86.62 88.93 83.6 79.2 83.97 87.80 73.9 70.5best JOINT 84.59 89.21 86.92 89.21 85.7 81.4 84.48 88.13 77.0 73.5best JOINT-IRREG 84.50 89.21 86.97 89.24 86.3 82.1 84.36 87.75 78.6 75.4best JOINT-REG 84.31 89.0 86.63 89.00 84.5 80.4 84.18 87.95 76.4 73.3best PIPELINE 84.02 88.83 86.49 88.83 84.4 80.4 83.88 87.33 77.6 74.4Table 4: Final results on test set for baseline and the best system for each architecture.dicted POS of MWE has positive impact.
Thebest architectures are JOINT and JOINT-IRREG,with the former slightly better than the latter forparsing metrics, though only some of the differ-ences are significant between the two.
It can benoted though, that JOINT-IRREG performs over-all better on MWEs (last two columns of table3), whereas JOINT performs better on irregularMWEs: the latter seems to be beneficial for pars-ing, but is less efficient to correctly spot the regularMWEs.Concerning the three distinct representations,evaluating on structured representation (hencewithout looking at regular MWE status) leads toa rough 2 point performance increase for the LASand a one point increase for the UAS, with respectto the evaluation against flat representation.
Thisquantifies the additional difficulty of deciding fora regular sequence of tokens whether it forms aMWE or not.
The evaluation on the labeled rep-resentation provides an evaluation of the full task(parsing, regular/irregular MWE recognition andregular MWEs structuring), with a UAS that is lessimpacted by errors on regular MWE status, whileLAS reflects the full difficulty of the task.166.4 Results on test set and comparisonWe provide the final results on the test set intable 4.
We compare the baseline JOINT sys-tem with the best system for all four reg/irregarchitectures (cf.
section 6.3).
We observe thesame general trend as in the development corpus,but with tinier differences.
JOINT and JOINT-IRREG significantly outperform the baseline andthe PIPELINE, on labeled representation and flatrepresentation.
We can see that there is no sig-nificant difference between JOINT and JOINT-16The slight differences in LAS between the labeled andthe flat representations are due to side effects of errors onMWE status: some wrong reattachments performed to obtainflat representation decrease the UAS, but also in some casesthe LAS.DEV TESTSystem UAS LAS UAS LASreg/irreg joint 88.79 85.15 88.13 84.48Bjork13 88.30 84.84 87.87 84.37Const13 pipeline 88.73 85.28 88.35 84.91Const13 joint 88.21 84.60 87.76 84.14uniform joint 88.81 85.42 87.96 84.59Table 5: Comparison on dev set of our best archi-tecture with reg/irregular MWE distinction (firstrow), with the single-parser architectures of (Con-stant et al, 2013) (Const13) and (Bjo?rkelund etal., 2013) (Bjork13).
Uniform joint is our reimple-mentation of Const13 joint, enhanced with mwe-features and switch.IRREG and between JOINT-REG and JOINT-IRREG.
JOINT slightly outperforms JOINT-REG(p < 0.05).
On the structured representation, thetwo best systems (JOINT and JOINT-IRREG) sig-nificantly outperform the other systems (p < 0.01for all; p < 0.05 for JOINT-REG).Moreover, we provide in table 5 a comparisonof our best architecture with reg/irregular MWEdistinction with other architectures that do notmake this distinction, namely the two best com-parable systems designed for the SPMRL SharedTask (Seddah et al, 2013): the pipeline sim-ple parser based on Mate-tools of Constant etal.
(2013) (Const13) and the Mate-tools system(without reranker) of Bjo?rkelund et al (2013)(Bjork13).
We also reimplemented and improvedthe uniform joint architecture of Constant et al(2013), by adding MWE features and switch.
Re-sults can only be compared on the flat representa-tion, because the other systems output poorer lin-guistic information.
We computed statistical sig-nificance of differences between our systems andConst13.
On dev, the best system is the enhanceduniform joint, but differences are not significantbetween that and the best reg/irreg joint (1st row)and the Const13 pipeline.
But on the test corpus(which is twice bigger), the best system is Const13750Tasks LAS UAS ALL MWE REG MWE IRREG MWESystem Parsing MWE FUM FTM FUM FTM FUM FTMOur best system (best JOINT) + all 85.15 88.78 77.6 74.5 70.8 67.8 85.4 82.6Uniform pipeline/gold MWEs + - 88.73 90.60 - - - - - -CRF-based MWE analyzer - all - - 78.8 76.3 73.5 71.9 84.2 80.8JOINT-REG + all 84.58 88.34 76.3 73.2 69.3 66.5 84.1 80.7JOINT-REG/gold irreg.
MWE + reg.
85.86 89.19 82.9 78.8 70.0 67.2 - -Table 6: Comparison with simpler tasks on the flat representation of the development set.pipeline, with statistically significant differencesover our joint systems.
So the first observationis that our architectures that distinguish betweenreg/irreg MWEs do not outperform uniform ar-chitectures.
But we note that the differences areslight, and the output we obtain is enhanced withregular MWE internal structure.
It can thus benoted that the increased syntactic uniformity ob-tained by our MWE representation is mitigated sofar by the additional complexity of the task.
Thesecond observation is that currently the best sys-tem on this dataset is a pipeline system, as resultson test set show (and somehow contrary to resultson dev set).
The joint systems that integrate MWEinformation in the labels seem to suffer from in-creased data sparseness.6.5 Evaluating the double task with respectto simpler tasksIn this section, we propose to better evaluate thedifficulty of combining the tasks of MWE analy-sis and dependency parsing by comparing our sys-tems with systems performing simpler tasks: i.e.MWE recognition without parsing, and parsingwith no or limited MWE recognition, simulated byusing gold MWEs.
We also provide a finer eval-uation of the MWE recognition task, in particularwith respect to their regular/irregular status.We first compare our best system with a parserwhere all MWEs have been perfectly pre-grouped,in order to quantify the difficulty that MWEs addto the parsing task.
We also compare the per-formance on MWEs of our best system with thatachieved by the CRF-based analyzer described insection 5.2.
Next, we compare the best JOINT-REG system with the one based on the same ar-chitecture but where the irregular MWEs are per-fectly pre-identified, in order to quantify the dif-ficulty added by the irregular MWEs.
Results aregiven in table 6.
Without any surprise, the taskis much easier without considering MWE recog-nition.
We can see that without considering MWEanalysis the parsing accuracy is about 2.5 pointsbetter in terms of LAS.
In the JOINT-REG ar-chitecture, assuming gold irregular MWE identi-fication, increases LAS by 1.3 point.
In termsof MWE recognition, as compared with the CRF-based analyzer, our best system is around 2 pointsbelow.
But the situation is quite different whenbreaking the evaluation by MWE type.
Our sys-tem is 1 point better than the CRF-based analyzerfor irregular MWEs.
This shows that consideringa larger syntactic context helps recognition of ir-regular MWEs.
The ?weak point?
of our system istherefore the identification of regular MWEs.7 ConclusionWe experimented strategies to predict both MWEanalysis and dependency structure, and testedthem on the dependency version of French Tree-bank (Abeille?
and Barrier, 2004), as instantiatedin the SPMRL Shared Task (Seddah et al, 2013).Our work focused on using an alternative repre-sentation of syntactically regular MWEs, whichcaptures their syntactic internal structure.
We ob-tain a system with comparable performance to thatof previous works on this dataset, but which pre-dicts both syntactic dependencies and the internalstructure of MWEs.
This can be useful for captur-ing the various degrees of semantic composition-ality of MWEs.
The main weakness of our systemcomes from the identification of regular MWEs, aproperty which is highly lexical.
Our current useof external lexicons does not seem to suffice, andthe use of data-driven external information to bet-ter cope with this identification can be envisaged.ReferencesAnne Abeille?
and Nicolas Barrier.
2004.
Enrichinga french treebank.
In Proceedings of LREC 2004,Lisbon, Portugal.Abhishek Arun and Frank Keller.
2005.
Lexicalizationin crosslinguistic probabilistic parsing: The case of751french.
In Proceedings of ACL 2005, Ann Arbor,USA.Eduard Bejc?ek and Pavel Stranak.
2010.
Annota-tion of multiword expressions in the prague depen-dency treebank.
Language Resources and Evalua-tion, 44:7?21.Anders Bjo?rkelund, ?Ozlem C?etinog?lu, Thomas Farkas,Richa?rdand Mu?ller, and Wolfgang Seeker.
2013.
(re)ranking meets morphosyntax: State-of-the-artresults from the spmrl 2013 shared task.
In Pro-ceedings of the 4th Workshop on Statistical Parsingof Morphologically Rich Languages: Shared Task,Seattle, WA.Bernd Bohnet.
2010.
Top accuracy and fast depen-dency parsing is not a contradiction.
In Proceedingsof COLING 2010, Beijing, China.Conor Cafferkey, Deirdre Hogan, and Josef van Gen-abith.
2007.
Multi-word units in treebank-basedprobabilistic parsing and generation.
In Proceed-ings of the 10th International Conference on Re-cent Advances in Natural Language Processing(RANLP?07), Borovets, Bulgaria.Marie Candito, Benoit Crabbe?, and Pascal Denis.2010.
Statistical french dependency parsing : Tree-bank conversion and first results.
In Proceedings ofLREC 2010, Valletta, Malta.Grzegorz Chrupa?a, Georgiana Dinu, and Josef vanGenabith.
2008.
Learning morphology with mor-fette.
In Proceedings of LREC 2008, Marrakech,Morocco.
ELDA/ELRA.Matthieu Constant and Isabelle Tellier.
2012.
Eval-uating the impact of external lexical resources intoa crf-based multiword segmenter and part-of-speechtagger.
In Proceedings of LREC 2012, Istanbul,Turkey.Matthieu Constant, Anthony Sigogne, and Patrick Wa-trin.
2012.
Discriminative strategies to integratemultiword expression recognition and parsing.
InProceedings of ACL 2012, Stroudsburg, PA, USA.Matthieu Constant, Marie Candito, and Djame?
Sed-dah.
2013.
The ligm-alpage architecture for thespmrl 2013 shared task: Multiword expression anal-ysis and dependency parsing.
In Proceedings of the4th Workshop on Statistical Parsing of Morphologi-cally Rich Languages: Shared Task, Seattle, WA.Blandine Courtois, Myle`ne Garrigues, Gaston Gross,Maurice Gross, Rene?
Jung, Mathieu-Colas Michel,Anne Monceaux, Anne Poncet-Montange, Max Sil-berztein, and Robert Vive?s.
1997.
Dictionnairee?lectronique DELAC : les mots compose?s binaires.Technical Report 56, University Paris 7, LADL.Blandine Courtois.
2009.
Un syste`me de dictionnairese?lectroniques pour les mots simples du franc?ais.Langue Franc?aise, 87:11?22.Pascal Denis and Beno?
?t Sagot.
2009.
Coupling anannotated corpus and a morphosyntactic lexicon forstate-of-the-art POS tagging with less human ef-fort.
In Proceedings of the 23rd Pacific Asia Con-ference on Language, Information and Computation(PACLIC?09), Hong Kong.Gu?ls?en Eryig?it, Tugay Ilbay, and Ozan Arkan Can.2011.
Multiword expressions in statistical depen-dency parsing.
In Proceedings of the IWPT Work-shop on Statistical Parsing of Morphologically-RichLanguages (SPMRL?11), Dublin, Ireland.Spence Green, Marie-Catherine de Marneffe, JohnBauer, and Christofer D. Manning.
2011.
Multi-word expression identification with tree substitutiongrammars: A parsing tour de force with french.
InProceedings of EMNLP 2011, Edinburgh, Scotland.Spence Green, Marie-Catherine de Marneffe, andChristopher D Manning.
2013.
Parsing models foridentifying multiword expressions.
ComputationalLinguistics, 39(1):195?227.Joakim Nivre and Jens Nilsson.
2004.
Multiword unitsin syntactic parsing.
In Proceedings of the LRECWorkshop : Methodologies and Evaluation of Multi-word Units in Real-World Applications (MEMURA),Lisbon, Portugal.Odile Piton, Denis Maurel, and Claude Belleil.
1999.The prolex data base : Toponyms and gentiles fornlp.
In Proceedings of the Third International Work-shop on Applications of Natural Language to DataBases (NLDB?99), Klagenfurt, Austria.Kenji Sagae and Alon Lavie.
2006.
Parser combina-tion by reparsing.
In Proceedings of NAACL/HLT2006, Companion Volume: Short Papers, Strouds-burg, PA, USA.Beno?
?t Sagot.
2010.
The lefff, a freely available, accu-rate and large-coverage lexicon for french.
In Pro-ceedings of LREC 2010, Valletta, Malta.Djame?
Seddah, Grzegorz Chrupa?a, Ozlem Cetinoglu,Josef van Genabith, and Marie Candito.
2010.Lemmatization and statistical lexicalized parsing ofmorphologically-rich languages.
In Proceedings ofthe NAACL/HLT Workshop on Statistical Parsing ofMorphologically Rich Languages (SPMRL 2010),Los Angeles, CA.Djame?
Seddah, Reut Tsarfaty, Sandra K?
?ubler, MarieCandito, Jinho Choi, Richa?rd Farkas, Jennifer Fos-ter, Iakes Goenaga, Koldo Gojenola, Yoav Goldberg,Spence Green, Nizar Habash, Marco Kuhlmann,Wolfgang Maier, Joakim Nivre, Adam Przepi-orkowski, Ryan Roth, Wolfgang Seeker, YannickVersley, Veronika Vincze, Marcin Wolin?ski, AlinaWro?blewska, and Eric Villemonte de la Cle?rgerie.2013.
Overview of the spmrl 2013 shared task: Across-framework evaluation of parsing morpholog-ically rich languages.
In Proceedings of the 4thWorkshop on Statistical Parsing of MorphologicallyRich Languages: Shared Task, Seattle, WA.752Veronika Vincze, Do?ra Szauter, Attila Alma?si, Gyo?rgyMo?ra, Zolta?n Alexin, and Ja?nos Csirik.
2010.
Hun-garian dependency treebank.
In Proceedings ofLREC 2010, Valletta, Malta.Veronika Vincze, Ja?nos Zsibrita, and Istva`n Nagy T.2013.
Dependency parsing for identifying hungar-ian light verb constructions.
In Proceedings of In-ternational Joint Conference on Natural LanguageProcessing (IJCNLP 2013), Nagoya, Japan.753
