Is That Your Final Answer?Florence ReederGeorge Mason Univ./MITRE Corp.1820 Dolley Madison Blvd.McLean VA 22102703-883-7156freeder@mitre.orgABSTRACTThe purpose of this research is to test the efficacy of applyingautomated evaluation techniques, originally devised for theevaluation of human language learners, to  the output of machinetranslation  (MT) systems.
We believe that these evaluationtechniques will provide information about both the humanlanguage learning process, the translation process and thedevelopment of machine translation systems.
This, the firstexperiment in a series of experiments, looks at the intelligibility ofMT output.
A language learning experiment showed thatassessors can differentiate native from non-native language essaysin less than 100 words.
Even more illuminating was the factorson which the assessors made their decisions.
We tested this to seeif similar criteria could be elicited from duplicating theexperiment using machine translation output.
Subjects were givena set of up to six extracts of translated newswire text.
Some of theextracts were expert human translations, others were machinetranslation outputs.
The subjects were given three minutes perextract to determine whether they believed the sample output to bean expert human translation or a machine translation.Additionally, they were asked to mark the word at which theymade this decision.
The results of this experiment, along with apreliminary analysis of the factors involved in the decisionmaking process will be presented here.KeywordsMachine translation, language learning, evaluation.1.
INTRODUCTIONMachine translation evaluation and language learner evaluationhave been associated for many years, for example [5, 7].
Oneattractive aspect of language learner evaluation whichrecommends it to machine translation evaluation is theexpectation that the produced language is not perfect, well-formedlanguage.
Language learner evaluation systems are gearedtowards determining the specific kinds of errors that languagelearners make.
Additionally, language learner evaluation, morethan many MT evaluations, seeks to build models of languageacquisition which could parallel (but not correspond directly to)the development of MT systems.
These models frequently arefeature-based and may provide informative metrics for diagnosticevaluation for system designers and users.In a recent experiment along these lines, Jones and Rusk [2]present a reasonable idea for measuring intelligibility, that oftrying to score the English output of translation systems using awide variety of metrics.
In essence, they are looking at the degreeto which a given output is English and comparing this to human-produced English.
Their goal was to find a scoring function forthe quality of English that can enable the learning of a goodtranslation grammar.
Their method for accomplishing this isthrough using existing natural language processing applicationson the translated data and using these to come up with a numericvalue indicating degree of ?Englishness?.
The measures theyutilized included syntactic indicators such as word n-grams,number of edges in the parse (both Collins and Apple Pie parserwere used), log probability of the parse, execution of the parse,overall score of the parse, etc.
Semantic criteria were basedprimarily on WordNet and incorporated the average minimumhyponym path length, path found ratio, percent of words withsense in WordNet.
Other semantic criteria utilized mutualinformation measures.Two problems can be found with their approach.
The first is thatthe data was drawn from dictionaries.
Usage examples indictionaries, while they provide great information, are notnecessarily representative of typical language use.
In fact, theytend to highlight unusual usage patterns or cases.
Second, andmore relevant to our purposes,  is that they were looking at theglass as half-full instead of half-empty.
We believe that ourresults will show that measuring intelligibility is not nearly asuseful as finding a lack of intelligibility.
This is not new in MTevaluation ?
as numerous approaches have been suggested toidentify translation errors, such as [1, 6].
In this instance,however, we are not counting errors to come up with aintelligibility score as much as finding out how quickly theintelligibility can be measured.
Additionally, we are looking to afield where the essence of scoring is looking at error cases, that oflanguage learning.2.
SIMPLE LANGUAGE LEARNINGEXPERIMENTThe basic part of scoring learner language (particularly secondlanguage acquisition and English as a second language) consistsof identifying likely errors and understanding the cause of them.From these, diagnostic models of language learning can be builtand used to effectively remediate learner errors, [3] provide anexcellent example of this.
Furthermore, language learner testingseeks to measure the student's ability to produce language whichis fluent (intelligible) and correct (adequate or informative).These are the same criteria typically used to measure MT systemcapability.1In looking at different second language acquisition (SLA) testingparadigms, one experiment stands out as a useful starting point forour purposes.
One experiment in particular serves as the modelfor this investigation.
In their test of language teachers, Mearaand Babi [3] looked at assessors making a native speaker (L1) /language learner (L2) distinction in written essays.2  They showedthe assessors essays one word at a time and counted the number ofwords it took to make the distinction.They found that assessors could accurately attribute L1 texts83.9% of the time and L2 texts 87.2% of the time for 180 textsand 18 assessors.
Additionally, they found that assessors couldmake the L1/L2 distinction in less than 100 words.
They alsolearned that it took longer to confirm that an essay was a nativespeaker?s than a language learner?s.
It took, on average, 53.9words to recognize an L1 text and only 36.7 words to accuratelydistinguish an L2 text.
While their purpose was to rate thelanguage assessment process, the results are intriguing from anMT perspective.They attribute the fact that L2 took less words to identify to thefact that L1 writing ?can only be identified negatively by theabsence of errors, or the absence of awkward writing.?
Whilethey could not readily select features, lexical or syntactic, onwhich evaluators consistently made their evaluation, theyhypothesize that there is a ?tolerance threshold?
for low qualitywriting.
In essence, once the pain threshold had been reachedthrough errors, missteps or inconsistencies, then the assessorcould confidently make the assessment.
It is this finding that weuse to disagree with Jones and Rusk [2] basic premise.
Instead oflooking for what the MT system got right, it is more fruitful toanalyze what the MT system failed to capture, from anintelligibility standpoint.
This kind of diagnostic is more difficult,as we will discuss later.We take this as the starting point for looking at assessing theintelligibility of MT output.
The question to be answered is doesthis apply to distinguishing between expert translation and MToutput?
This paper reports on an experiment to answer thisquestion.
We believe that human assessors key off of specificerror types and that an analysis of the results of the experimentwill enable us to do a program which automatically gets these.1The discussion of whether or not MT output should be comparedto human translation output is grist for other papers and otherforums.2In their experiment, they were examining students learningSpanish as a second language.3.
SHORT READING TESTWe started with publicly available data which was developedduring the 1994 DARPA Machine Translation Evaluations [8],focusing on the Spanish language evaluation first.
They may beobtained at:  http://ursula.georgetown.edu.3  We selected the first50 translations from each system and from the referencetranslation.
We extracted the first portion of each translation(from 98 to 140 words as determined by sentence boundaries).
Inaddition, we removed headlines, as we felt these served asdistracters.
Participants were recruited through the author?sworkplace, through the author?s neighborhood and a nearbydaycare center.
Most were computer professionals and some werefamiliar with MT development or use.
Each subject was given aset of six extracts ?
a mix of different machine and humantranslations.
The participants were told to read line by line untilthey were able to make a distinction between the possible authorsof the text ?
a human translator or a machine translator.
The firsttwenty-five test subjects were given no information about theexpertise of the human translator.
The second twenty-five testsubjects were told that the human translator was an expert.
Theywere given up to three minutes per text, although they frequentlyrequired much less time.
Finally, they were asked to circle theword at which they made their distinction.
Figure 1 shows asample text.3001GPThe general secretary of the UN, ButrosButros-Ghali, was pronounced on Wednesday infavor of a solution "more properly Haitian"resulting of a "commitment" negotiatedbetween the parts, if the internationalsanctions against Haiti continue beingineffectual to restore the democracy in thatcountry.While United States multiplied the last daysthe threats of an intervention to fight tocompel to the golpistas to abandon thepower, Butros Ghali estimated in a directedreport on Wednesday to the general Assemblyof the UN that a solution of the Haitiancrisis only it will be able be obtained"with a commitment, based on constructiveand consented grants" by the parts.HUMANMACHINEFigure 1:  Sample Test Sheet4.
RESULTSOur first question is does this kind of test apply to distinguishingbetween expert translation and MT output?
The answer is yes.Subjects were able to distinguish MT output from humantranslations 88.4% of the time, overall.
This determination is3Data has since been moved to a new location.more straightforward for readers than the native/non-nativespeaker distinction.
There was a degree of variation on a per-system basis, as captured in Table 1.
Additionally, as presented inTable 2, the number of words to determine that a text was humanwas nearly twice the closest system.4Table 1:  Percentage correct for each systemSYSTEM SCOREGLOBALINK 93.9%LINGSTAT 95.9%PANGLOSS 95.9%PAHO 69.4%SYSTRAN 87.8%HUMAN 89.8%Table 2:  Average Number of Words to DetermineSYSTEM AVG.
# WORDSPANGLOSS 17.6GLOBALINK 25.9SYSTRAN 31.7LINGSTAT 33.8PAHO 37.6HUMAN 62.2The second question is does this ability correlate with theintelligibility scores applied by human raters?
One way to look atthe answer to this is to view the fact that the more intelligible asystem output, the harder it is to distinguish from human output.So, systems which have lower scores for human judgment shouldhave higher intelligibility scores.
Table 3 presents the scores withthe fluency scores as judged by human assessors.Table 3:  Percentage Correct and Fluency ScoresSYSTEM SCORE FLUENCYPANGLOSS 95.9 21.0LINGSTAT 95.9 30.4GLOBALINK 93.9 42.0SYSTRAN 87.8 45.4PAHO 69.4 56.7Indeed, the systems with the lowest fluency scores were mosteasily attributed.
The system with the best fluency score was alsothe one most confused.
Individual articles in the test sample willneed to be evaluated statistically before a definite correlation canbe determined, but the results are encouraging.4For those texts where the participants failed to mark a specificspot, the length of the text was included in the average.The final question is are there characteristics of the MT outputwhich enable the decision to be made quickly?
The initial resultslead us to believe that it is so.
Not translated words (non propernouns) were generally immediate clues as to the fact that a systemproduced the results.
Other factors included:  incorrect pronountranslation; incorrect preposition translation; incorrectpunctuation.
A more detailed breakdown of the selection criteriaand the errors occurring before the selected word is currently inprocess.5.
ANALYSISAn area for further analysis is that of the looking at  the details ofthe post-test interviews.
These have consistently shown that thedeciders utilized error spotting, although the types andsensitivities of the errors differed from subject to subject.
Someerrors were serious enough to make the choice obvious whereothers had to occur more than once to push the decision above athreshold.
Extending this to a new language pair is also desirableas a language more divergent than Spanish from English mightgive different (and possibly even stronger) results.
Finally, we areworking on constructing a program, using principles fromComputer Assisted Language Learning (CALL) program design,which is aimed to duplicate the ability to assess human versussystem texts.6.
ACKNOWLEDGMENTSMy thanks goes to all test subjects and Ken Samuel for review.7.
REFERENCES[1] Flanagan, M.  1994.
Error Classification for MTEvaluation.
In Technology Partnerships for Crossingthe Language Barrier:  Proceedings of the FirstConference of the Association  for MachineTranslation in the Americas, Columbia, MD.
[2] Jones, D. & Rusk, G.  2000.
Toward a ScoringFunction for Quality-Driven Machine Translation.
InProceedings of COLING-2000.
[3] Meara, P. & Babi, A.
1999.
Just a few words:  howassessors evaluate minimal texts.
VocabularyAcquisition Research Group Virtual Library.www.swan.ac.uk/cals/vlibrary/ab99a.html[4] Michaud, L. & K. McCoy.
1999.
Modeling UserLanguage Proficiency in a Writing Tutor for DeafLearners of English.
In M. Olsen, ed., Computer-Mediated Language Assessment and Evaluation inNatural Language Processing,  Proceedings of aSymposium by ACL/IALL.
University of Maryland, p.47-54[5] Somers, H. & Prieto-Alvarez, N.  2000.
MultipleChoice Reading Comprehension Tests for ComparativeEvaluation of MT Systems.
In Proceedings of theWorkshop on MT Evaluation at AMTA-2000.
[6] Taylor, K. & J.
White.
1998.
Predicting What MT isGood for:  User Judgments and Task Performance.Proceedings of AMTA-98, p.
364-373.
[7] Tomita, M., Shirai, M., Tsutsumi, J., Matsumura, M. &Yoshikawa, Y.
1993.
Evaluation of MT Systems byTOEFL.
In Proceedings of the Theoretical andMethodological Implications of Machine Translation(TMI-93).
[8] White, John, et al 1992-1994.
ARPA Workshops onMachine Translation.
Series of 4 workshops oncomparative evaluation.
PRC Inc. McLean, VA.[9] Wilks, Y.
(1994)  Keynote: Traditions in theEvaluation of MT.
In Vasconcellos, M.
(ed.)
MTEvaluation: Basis for Future Directions.
Proceedings ofa workshop sponsored by the National ScienceFoundation, San Diego, California.
