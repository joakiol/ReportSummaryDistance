CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 198?202Manchester, August 2008Hybrid Learning of Dependency Structures from HeterogeneousLinguistic ResourcesYi ZhangLanguage Technology LabDFKI GmbHyzhang@coli.uni-sb.deRui WangComputational LinguisticsSaarland University, Germanyrwang@coli.uni-sb.deHans UszkoreitLanguage Technology LabDFKI GmbHuszkoreit@dfki.deAbstractIn this paper we present our syntactic andsemantic dependency parsing system par-ticipated in both closed and open compe-titions of the CoNLL 2008 Shared Task.By combining the outcome of two state-of-the-art syntactic dependency parsers, weachieved high accuracy in syntactic de-pendencies (87.32%).
With MRSes fromgrammar-based HPSG parsers, we achievedsignificant performance improvement onsemantic role labeling (from 71.31% to71.89%), especially in the out-domainevaluation (from 60.16% to 62.11%).1 IntroductionThe CoNLL 2008 shared task (Surdeanu et al,2008) provides a unique chance of comparing dif-ferent syntactic and semantic parsing techniquesin one unified open competition.
Our contributionin this joint exercise focuses on the combinationof different algorithms and resources, aiming notonly for state-of-the-art performance in the com-petition, but also for the dissemination of the learntlessons to related sub-fields in computational lin-guistics.The so-called hybrid approach we take has twofolds of meaning.
For syntactic dependency pars-ing, we build our system based on state-of-the artalgorithms.
Past CoNLL share task results haveshown that transition-based and graph-based algo-rithms started from radically different ideas, yetachieved largely comparable results.
One of thequestion we would like investigate is whether thec?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.combination of the two approach on the outputlevel leads to even better results.For the semantic role labeling (SRL) task, wewould like to build a system that allows us to testthe contribution of different linguistic resources.To our special interest is to examine the deeplinguistic parsing systems based on hand-craftedgrammars.
During the past decades, various largescale linguistic grammars have been built, someof which achieved both broad coverage and highprecision.
In combination with other advancesin deep linguistic processing, e.g.
efficient pars-ing algorithms, statistical disambiguation modelsand robust processing techniques, several systemshave reached mature stage to be deployed in ap-plications.
Unfortunately, due to the difficultiesin cross-framework evaluation, fair comparison ofthese systems with state-of-the-art data-driven sta-tistical parsers is still hard to achieve.
More impor-tantly, it is not even clear whether deep linguisticanalysis is necessary at all for tasks such as shallowsemantic parsing (also known as SRL).
Drawinga conclusion on this latter point with experimentsusing latest deep parsing techniques is one of ourobjective.The remainder of the paper is structure as fol-lows.
Section 2 introduces the overall system ar-chitecture.
Section 3 explains the voting mecha-nism used in the syntactic parser.
Section 4 de-scribes in detail the semantic role labeling com-ponent.
Section 5 presents evaluation results anderror analysis.
Section 6 concludes the paper.2 System ArchitectureAs shown in Figure 1, our system is a two-stagepipeline.
For the syntactic dependencies, we applytwo state-of-the-art dependency parsers and com-bined their results based on a voting model.
For198Parse Selector(MaltParser)Transition?based DepParser (MST Parser)Graph?based DepParserDeep Linguistic Parser(ERG/PET)Predicate IdentificationArgument IdentificationArgument ClassificationPredicated ClassificationSemanticRoleLabelingSyn.Dep.MRSSyntacticDependencyParsingFigure 1: System Architecturethe semantic roles, we extracted features from theprevious stage, combined with deep parsing results(in MRS), and use statistical classification modelsto make predictions.
In particular, the second partcan be further divided into four stages: predicateidentification (PI), argument identification (AI), ar-gument classification (AC), and predicate classi-fication (PC).
Maximum entropy-based machinelearning techniques are used in both componentswhich we will see in detail in the following sec-tions.3 Syntactic Dependency ParsingFor obtaining syntactic dependencies, we havecombined the results of two state-of-the-art depen-dency parsers: the MST parser (McDonald et al,2005) and the MaltParser (Nivre et al, 2007).The MST parser formalizes dependency parsingas searching for maximum spanning trees (MSTs)in directed graphs.
A major advantage of theirframework is the ability to naturally and efficientlymodel both projective and non-projective parses.To learn these structures they used online large-margin learning that empirically provides state-of-the-art performance.The MaltParser is a transition-based incrementaldependency parser, which is language-independentand data-driven.
It contains a deterministic algo-rithm, which can be viewed as a variant of the ba-sic shift-reduce algorithm.
The learning methodthey applied is support vector machine and experi-mental evaluation confirms that the MaltParser canachieve robust, efficient and accurate parsing for awide range of languages.Since both their parsing algorithms and machinelearning methods are quite different, we decide totake advantages of them.
After a comparison be-tween the results of the two parsers1, we find that,1.
The MST parser is better at the whole struc-ture.
In several sentences, the MaltParser waswrong at the root node, but the MST parser iscorrect.2.
The MaltParser is better at some dependencylabels (e.g.
TMP, LOC, etc.
).These findings motivate us to do a voting basedon both outputs.
The features considered in thevoting model are as follows:?
Dependency path: two categories of depen-dency paths are considered as features: 1)the POS-Dep-POS style and 2) the Dep-Depstyle.
The former consists of part-of-speech(POS) tags and dependency relations appear-ing in turns; and the latter only contains de-pendency relations.
The maximum length ofthe dependency path is three dependency re-lations.?
Root attachments: the number of tokens at-tached to the ROOT node by the parser in onesentence?
Sentence length: the number of tokens ineach input sentence?
Projectivity: whether the parse is projectiveor notWith these features, we apply a statistical modelto predict, for each sentence, we choose the pars-ing result from which parser.
The voted result willbe our syntactic dependency output and be passedto the later stages.4 Semantic Role Labeling4.1 OverviewThe semantic role labeling component of our sys-tem is comprised of a pipeline model with four1In this experiment, we use second order features and theprojective decoder for the MST parser trained with 10 iter-ations, and Arc-eager algorithm with a quadric polynomialkernel for the MaltParser.199sub-components that performs predicate identi-fication (PI), argument identification (AI), argu-ment classification (AC) and predicate classifica-tion (PC) respectively.
The output in previoussteps are taken as input information to the follow-ing stages.
All these components are essentiallybased on a maximum entropy statistical classifier,although with different task-specific optimizationsand feature configurations in each step.
Dependingon the available information from the input datastructure, the same architecture is used for bothclosed and open challenge runs, with different fea-ture types.
Note that our system does not make useof or predict SU chains.Predicate Identification The component makesbinary prediction on each input token whether itforms a predicate in the input sentence.
This pre-dictor precedes other components because it is arelatively easy task (comparing to the followingcomponents).
Also, making this prediction earlyhelps to cut down the search space in the follow-ing steps.
Based on the observation on the trainingdata, we limit the PI predictor to only predict fortokens with certain POS types (POSes marked aspredicates for at least 50 times in the training set).This helps to significantly improve the system effi-ciency in both training and prediction time withoutsacrificing prediction accuracy.It should be noted that the prediction of nominalpredicates are generally much more difficult (basedon CoNLL 2008 shared task annotation).
The PImodel achieved 96.32 F-score on WSJ with verbalpredicates, but only 84.74 on nominal ones.Argument Identification After PI, the argu-ments to the predicted predicates are identifiedwith the AI component.
Similar to the approachtaken in Hacioglu (2004), we use a statistical clas-sifier to select from a set of candidate nodes in adependency tree.
However, instead of selectingfrom a set of neighboring nodes from the predicateword2, we define the concept of argument path asa chain of dependency relations from the predicateto the argument in the dependency tree.
For in-stance, an argument path [???
| ???]
indicates thatif the predicate is syntactically depending as ???
ona node which has a ???
child, then the ???
node2Hacioglu (2004) defines a tree-structured family of apredicate as a measure of locality.
It is a set of dependencyrelation nodes that consists of the predicate?s parent, chil-dren, grandchildren, siblings, siblings?
children and siblings?grandchildren with respect to its dependency tree(sibling to the predicate) is an argument candidate.While Hacioglu (2004)?s approach focus mainlyon local arguments (with respect to the syntacticdependencies), our approach is more suitable ofcapturing long distance arguments from the pred-icate.
Another minor difference is that we allowpredicate word to be its own argument (which isfrequently the case for nominal predicates) with anempty argument path [ | ].The set of effective argument paths are obtainedfrom the training set, sorted and filtered accordingto their frequencies, and used in testing to obtainthe candidate arguments.
By setting a frequencythreshold, we are able to select the most usefulargument paths.
The lower the threshold is, thehigher coverage one might get in finding candi-date arguments, accompanied with a higher aver-age candidate number per predicate and potentiallya more difficult task for the statistical classifier.By experimenting with different frequency thresh-olds on the training set, we established a frequencythreshold of 40, which guarantees candidate argu-ment coverage of 95%, and on average 5.76 candi-dates per predicate.
Given that for the training seteach predicate takes on average 2.13 arguments,the binary classifier will have relatively balancedprediction classes.Argument Classification For each identified ar-gument, an argument label will be assigned duringthe argument classification step.
Unlike the binaryclassifiers in previous two steps, AC uses a multi-class classifier that predicts from the inventory ofargument labels.
For efficiency reasons, we onlyconcern the most frequent 25 argument labels.Predicate Classification The final step in theSRL component labels the predicted predicate witha predicate name.
Due to the lack of lexicalresources in the closed competition, this step isscheduled for the last, in order to benefit from thepredictions made in the previous steps.
Unlike theprevious steps, the statistical model used in thisstep is a ranking model.
We obtained a list of can-didate frames and corresponding rolesets from theprovided PropBank and NomBank data.
Each pre-dicted predicate is mapped onto the potential role-sets it may take.
When the frame for the predicateword is missing from the list, or there is only onecandidate roleset for it, the predicate name is as-signed deterministically (word stem concatenatedwith ?.01?
for frame missing predicates, the unam-200biguous roleset name when there is only one can-didate).
When there are more than one candidaterolesets, a ranking model is trained to select themost probable roleset for a given predicate giventhe syntactic and semantic context.4.2 FeaturesThe feature types used in our SRL component aresummarized in Table 1, with the configurations ofour submitted ?closed?
and ?open?
runs marked.Numerous different configurations with these fea-ture types have been experimented on training anddevelopment data.
The results show that featuretypes 1?14 are the best performing ones.
Fea-tures related to the siblings of the predicate onlyintroduce minor performance variation.
We alsofind the named entity labels does not lead to im-mediate improvement of SRL performance.
TheWordNet sense feature does achieve minor perfor-mance increase on PI and PC, although the signif-icant remains to be further examined.
Based onthe pipeline model, we find it difficult to achievefurther improvement by incorporate more featurestypes from provided annotation.
And the vari-ance of SRL performance with different open fea-tures is usually less than 1%.
To clearly show thecontribution of extra external resources, these lesscontributing features (siblings, named entity labelsand WordNet sense) are not used in our submitted?open?
runs.MRSes as features to SRL As a novel point ofour SRL system, we incorporate parsing resultsfrom a linguistic grammar-based parsing systemin our ?open?
competition run.
In this experi-ment, we used English Resource Grammar (ERG,Flickinger (2000)), a precision HPSG grammar forEnglish.
For parsing, we used PET (Callmeier,2001), an efficient HPSG parser, together with ex-tended partial parsing module (Zhang et al, 2007)for maximum robustness.
The grammar is hand-crafted and the disambiguation models are trainedon Redwoods treebanks.
They present general lin-guistic knowledge, and are not tuned for the spe-cific domains in this competition.While the syntactic analysis of the HPSG gram-mar is largely different from the dependency anno-tation used in this shared task, the semantic rep-resentations do share a similar view on predicate-argument structures.
ERG uses as its semanticrepresentation the Minimal Recursion Semantics(MRS, Copestake et al (2005)), a non-recursive flatstructure that is suitable for underspecifying scopeambiguities.
A predicate-argument backbone ofMRS can be extracted by identifying shared vari-ables between elementary predications (?
?s).In order to align the HPSG parser?s I/O withCoNLL?s annotation, extensive mapping scriptsare developed to preprocess the texts, and extractbackbone from output MRSes.
The unknown wordhandling techniques (Zhang and Kordoni, 2005)are used to overcome lexical gaps.
Only the bestanalysis is used for MRS extraction.
Without par-tial parsing, the ERG achieves around 70% of rawcoverage on the training data.
When partial pars-ing is used, almost all the sentences received ei-ther full or partial analysis (except for several caseswhere computational resources are exhausted), andthe SRL performance improves by ?0.5%.5 ResultsAmong 20+ participant groups, our system rankedseventh in the ?closed?
competition, and first inthe ?open?
challenge.
The performance of the syn-tactic and semantic components of our system aresummarized in Table 2.In-Domain Out-DomainLab.
Unlab.
Lab.
Unlab.Syntactic Dep.
88.14% 90.78% 80.80% 86.12%SRLClosed 72.67% 82.68% 60.16% 76.98%Open 73.08% 83.04% 62.11% 78.48%Table 2: Labeled and unlabeled attachment scoresin syntactic dependency parsing and F1 score forsemantic role labeling.The syntactic voting and semantic labeling partsof our system are implemented in Java togetherwith a few Perl scripts.
Using the open sourceTADM for parameter estimation, our the votingcomponent take no more than 1 minute to train and10 seconds to run (on WSJ testset).
The SRL com-ponent takes about 1 hour for training, and no morethan 30 seconds for labeling (WSJ testset).Result analysis shows that the combination ofthe two state-of-the-art parsers delivers good syn-tactic dependencies (ranked 2nd).
Error analysisshows most of the errors are related to preposi-tions.
One category is the syntactic ambiguity ofpp-attachment, e.g.
in ?when trading was haltedin Philip Morris?, ?in?
can be attached to either?trading?
or ?halted?.
The other category is theLOC and TMP tags in phrases like ?at the end ofthe day?, ?at the point of departure?, etc.2011 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24PlemmaPPOSPrelP-parentPOSAlemmaAPOSArelP-childrenPOSesP-childrenrelP-ApathA-childrenPOSesA-childrenrelsPprecedesA?A?spositionP-siblingsPOSesP-siblingsrelsPNElabelPWNsensePMRS??-namePMRS-argslabelsPMRS-argsPOSesAMRS?
?-nameAMRS-predslabelsAMRS-predsPOSesPI ?
?
?
?
?
?
?
?
  AI ?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
     AC ?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
     PC ?
?
?
?
?
?
?
?
  Table 1: Feature types used in semantic role labeling sub-components.
Feature types marked with ?
areused in the ?closed?
run; feature types marked with  are used in the ?open?
run; feature types markedwith ?
are used in both runs.
P denotes predicate; A denotes semantic argument.The results on semantic role labeling show,sometimes, even with syntactic errors ofLOC/TMP tags, the semantic role labeler canstill predict AM-LOC/AM-TMP correctly, whichindicates the robustness of our hybrid approach.By comparing our ?closed?
and ?open?
runs, theMRS features do introduce a clear performanceimprovement.
The performance gain is evenmore significant in out-domain test, showing thatthe MRS features from ERG are indeed much lessdomain dependent.
Another example worth men-tioning is that, in the sentence ?Scotty regarded theear and the grizzled hair around it with a momentof interest?, it is extremely difficult to know that?Scotty?
is a semantic role of ?interest?.Also, we are the only group that submitted runsfor both tracks, and achieved better performancein open competition.
Although the best ways ofintegrating deep linguistic processing techniquesremain as an open question, the achieved resultsat least show that hand-crafted grammars like ERGdo provide heterogeneous linguistic insights thatcan potentially find their usage in data-driven NLPtasks as such.6 ConclusionIn this paper, we described our hybrid systemon both syntactic and semantic dependencies la-beling.
We built a voting model to combinethe results of two state-of-the-art syntactic depen-dency parsers, and a pipeline model to combinedeep parsing results for SRL.
The experimental re-sults showed the advantages of our hybrid strat-egy, especially on the cross-domain data set.
Al-though the optimal ways of combining deep pro-cessing techniques remains to be explored, theperformance gain achieved by incorporating hand-crafted grammar outputs shows a promising direc-tion of study for both fields.ReferencesCallmeier, Ulrich.
2001.
Efficient parsing with large-scaleunification grammars.
Master?s thesis, Universit?at desSaarlandes, Saarbr?ucken, Germany.Copestake, Ann, Dan Flickinger, Carl J. Pollard, and Ivan A.Sag.
2005.
Minimal recursion semantics: an introduction.Research on Language and Computation, 3(4):281?332.Flickinger, Dan.
2000.
On building a more efficient gram-mar by exploiting types.
Natural Language Engineering,6(1):15?28.Hacioglu, Kadri.
2004.
Semantic role labeling using de-pendency trees.
In Proceedings of COLING 2004, pages1273?1276, Geneva, Switzerland, Aug 23?Aug 27.McDonald, Ryan, Fernando Pereira, Kiril Ribarov, and JanHajic.
2005.
Non-Projective Dependency Parsing us-ing Spanning Tree Algorithms.
In Proceedings of HLT-EMNLP 2005, pages 523?530, Vancouver, Canada.Nivre, Joakim, Jens Nilsson, Johan Hall, Atanas Chanev,G?ulsen Eryigit, Sandra K?ubler, Svetoslav Marinov, and Er-win Marsi.
2007.
Maltparser: A language-independentsystem for data-driven dependency parsing.
Natural Lan-guage Engineering, 13(1):1?41.Surdeanu, Mihai, Richard Johansson, Adam Meyers, Llu?
?sM`arquez, and Joakim Nivre.
2008.
The CoNLL-2008shared task on joint parsing of syntactic and semanticdependencies.
In Proceedings of the 12th Conferenceon Computational Natural Language Learning (CoNLL-2008), Manchester, UK.Zhang, Yi and Valia Kordoni.
2005.
A statistical approachtowards unknown word type prediction for deep grammars.In Proceedings of the Australasian Language TechnologyWorkshop 2005, pages 24?31, Sydney, Australia.Zhang, Yi, Valia Kordoni, and Erin Fitzgerald.
2007.
Partialparse selection for robust deep processing.
In Proceed-ings of ACL 2007 Workshop on Deep Linguistic Process-ing, pages 128?135, Prague, Czech.202
