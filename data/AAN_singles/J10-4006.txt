Distributional Memory: A GeneralFramework for Corpus-Based SemanticsMarco Baroni?University of TrentoAlessandro Lenci?
?University of PisaResearch into corpus-based semantics has focused on the development of ad hoc models that treatsingle tasks, or sets of closely related tasks, as unrelated challenges to be tackled by extractingdifferent kinds of distributional information from the corpus.
As an alternative to this ?one task,one model?
approach, the Distributional Memory framework extracts distributional informationonce and for all from the corpus, in the form of a set of weighted word-link-word tuples arrangedinto a third-order tensor.
Different matrices are then generated from the tensor, and their rowsand columns constitute natural spaces to deal with different semantic problems.
In this way,the same distributional information can be shared across tasks such as modeling word similarityjudgments, discovering synonyms, concept categorization, predicting selectional preferences ofverbs, solving analogy problems, classifying relations between word pairs, harvesting qualiastructures with patterns or example pairs, predicting the typical properties of concepts, andclassifying verbs into alternation classes.
Extensive empirical testing in all these domains showsthat a Distributional Memory implementation performs competitively against task-specific al-gorithms recently reported in the literature for the same tasks, and against our implementationsof several state-of-the-art methods.
The Distributional Memory approach is thus shown to betenable despite the constraints imposed by its multi-purpose nature.1.
IntroductionThe last two decades have seen a rising wave of interest among computational linguistsand cognitive scientists in corpus-basedmodels of semantic representation (Grefenstette1994; Lund and Burgess 1996; Landauer and Dumais 1997; Schu?tze 1997; Sahlgren 2006;Bullinaria and Levy 2007; Griffiths, Steyvers, and Tenenbaum 2007; Pado?
and Lapata2007; Lenci 2008; Turney and Pantel 2010).
These models, variously known as vectorspaces, semantic spaces, word spaces, corpus-based semantic models, or, using the termwe will adopt, distributional semantic models (DSMs), all rely on some version of thedistributional hypothesis (Harris 1954; Miller and Charles 1991), stating that the degreeof semantic similarity between two words (or other linguistic units) can be modeled?
Center for Mind/Brain Sciences (CIMeC), University of Trento, C.so Bettini 31, 38068 Rovereto (TN),Italy.
E-mail: marco.baroni@unitn.it.??
Department of Linguistics T. Bolelli, University of Pisa, Via Santa Maria 36, 56126 Pisa (PI), Italy.E-mail: alessandro.lenci@ling.unipi.it.Submission received: 11 January 2010; revised submission received: 15 April 2010; accepted for publication:1 June 2010.?
2010 Association for Computational LinguisticsComputational Linguistics Volume 36, Number 4as a function of the degree of overlap among their linguistic contexts.
Conversely, theformat of distributional representations greatly varies depending on the specific aspectsof meaning they are designed to model.The most straightforward phenomenon tackled by DSMs is what Turney (2006b)calls attributional similarity, which encompasses standard taxonomic semantic rela-tions such as synonymy, co-hyponymy, and hypernymy.
Words like dog and puppy,for example, are attributionally similar in the sense that their meanings share a largenumber of attributes: They are animals, they bark, and so on.
Attributional similarityis typically addressed by DSMs based on word collocates (Grefenstette 1994; Lund andBurgess 1996; Schu?tze 1997; Bullinaria and Levy 2007; Pado?
and Lapata 2007).
Thesecollocates are seen as proxies for various attributes of the concepts that the wordsdenote.
Words that share many collocates denote concepts that share many attributes.Both dog and puppy may occur near owner, leash, and bark, because these words denoteproperties that are shared by dogs and puppies.
The attributional similarity betweendog and puppy, as approximated by their contextual similarity, will be very high.DSMs succeed in tasks like synonym detection (Landauer and Dumais 1997) orconcept categorization (Almuhareb and Poesio 2004) because such tasks require a mea-sure of attributional similarity that favors concepts that share many properties, suchas synonyms and co-hyponyms.
However, many other tasks require detecting differentkinds of semantic similarity.
Turney (2006b) defines relational similarity as the propertyshared by pairs of words (e.g, dog?animal and car?vehicle) linked by similar semanticrelations (e.g., hypernymy), despite the fact that the words in one pair might not beattributionally similar to those in the other pair (e.g., dog is not attributionally similar tocar, nor is animal to vehicle).
Turney generalizes DSMs to tackle relational similarity andrepresents pairs of words in the space of the patterns that connect them in the corpus.Pairs of words that are connected by similar patterns probably hold similar relations,that is, they are relationally similar.
For example, we can hypothesize that dog?tail ismore similar to car?wheel than to dog?animal, because the patterns connecting dog andtail (of, have, etc.)
are more like those of car?wheel than like those of dog?animal (is a, suchas, etc.).
Turney uses the relational space to implement tasks such as solving analogiesand harvesting instances of relations.
Although they are not explicitly expressed inthese terms, relation extraction algorithms (Hearst 1992, 1998; Girju, Badulescu, andMoldovan 2006; Pantel and Pennacchiotti 2006) also rely on relational similarity, andfocus on learning one relation type at a time (e.g., finding parts).Although semantic similarity, either attributional or relational, has the lion?s sharein DSMs, similarity is not the only aspect of meaning that is addressed by distributionalapproaches.
For instance, the notion of property plays a key role in cognitive science andlinguistics, which both typically represent concepts as clusters of properties (Jackendoff1990; Murphy 2002).
In this case, the task is not to find out that dog is similar to puppyor cat, but that it has a tail, it is used for hunting, and so on.
Almuhareb (2006), Baroniand Lenci (2008), and Baroni et al (2010) use the words co-occurring with a noun toapproximate its most prototypical properties and correlate distributionally derived datawith the properties produced by human subjects.
Cimiano and Wenderoth (2007) in-stead focus on that subset of noun properties known in lexical semantics as qualia roles(Pustejovsky 1995), and use lexical patterns to identify, for example, the constitutiveparts of a concept or its function (this is in turn analogous to the problem of relationextraction).
The distributional semantics methodology also extends to more complexaspects of word meaning, addressing issues such as verb selectional preferences (Erk2007), argument alternations (Merlo and Stevenson 2001; Joanis, Stevenson, and James2008), event types (Zarcone and Lenci 2008), and so forth.
Finally, some DSMs capture674Baroni and Lenci Distributional Memorya sort of ?topical?
relatedness between words: They might find, for example, a relationbetween dog and fidelity.
Topical relatedness, addressed by DSMs based on documentdistributions such as LSA (Landauer and Dumais 1997) and Topic Models (Griffiths,Steyvers, and Tenenbaum 2007), is not further discussed in this article.DSMs have found wide applications in computational lexicography, especially forautomatic thesaurus construction (Grefenstette 1994; Lin 1998a; Curran and Moens2002; Kilgarriff et al 2004; Rapp 2004).
Corpus-based semantic models have also at-tracted the attention of lexical semanticists as a way to provide the notion of synonymywith a more robust empirical foundation (Geeraerts 2010; Heylen et al 2008).
Moreover,DSMs for attributional and relational similarity are widely used for the semi-automaticbootstrapping or extension of terminological repositories, computational lexicons (e.g.,WordNet), and ontologies (Buitelaar, Cimiano, and Magnini 2005; Lenci 2010).
Inno-vative applications of corpus-based semantics are also being explored in linguistics,for instance in the study of semantic change (Sagi, Kaufmann, and Clark 2009), lexicalvariation (Peirsman and Speelman 2009), and for the analysis of multiword expressions(Alishahi and Stevenson 2008).The wealth and variety of semantic issues that DSMs are able to tackle confirmsthe importance of looking at distributional data to explore meaning, as well as thematurity of this research field.
However, if we looked from a distance at the whole fieldof DSMs we would see that, besides the general assumption shared by all models thatinformation about the context of a word is an important key in grasping its meaning, theelements of difference overcome the commonalities.
For instance, DSMs geared towardsattributional similarity represent words in the contexts of other (content) words, therebylooking very different from models that represent word pairs in terms of patternslinking them.
In turn, both these models differ from those used to explore conceptproperties or argument alternations.
The typical approach in the field has been a localone, in which each semantic task (or set of closely related tasks) is treated as a separateproblem, that requires its own corpus-derived model and algorithm, both optimized toachieve the best performance in a given task, but lacking generality, since they resortto task-specific distributional representations, often complemented by additional task-specific resources.
As a consequence, the landscape of DSMs looks more like a jigsawpuzzle in which different parts have been completed and the whole figure starts toemerge from the fragments, but it is not clear yet how to put everything together andcompose a coherent picture.We argue that the ?one semantic task, one distributional model?
approach repre-sents a great limit of the current state of the art.
From a theoretical perspective, corpus-based models hold promise as large-scale simulations of how humans acquire and useconceptual and linguistic information from their environment (Landauer and Dumais1997).
However, existing DSMs lack exactly the multi-purpose nature that is a hallmarkof human semantic competence.
The common view in cognitive (neuro)science is thathumans resort to a single semantic memory, a relatively stable long-term knowledgedatabase, adapting the information stored there to the various tasks at hand (Murphy2002; Rogers and McClelland 2004).
The fact that DSMs need to go back to theirenvironment (the corpus) to collect ad hoc statistics for each semantic task, and the factthat different aspects of meaning require highly different distributional representations,cast many shadows on the plausibility of DSMs as general models of semantic mem-ory.
From a practical perspective, going back to the corpus to train a different model foreach application is inefficient, and it runs the risk of overfitting the model to a specifictask, while losing sight of its adaptivity?a highly desirable feature for any intelligentsystem.
Think, by contrast, of WordNet (Fellbaum 1998), a single, general purpose675Computational Linguistics Volume 36, Number 4network of semantic information that has been adapted to all sorts of tasks, many ofthem certainly not envisaged by the resource creators.
We think that it is not by chancethat no comparable resource has emerged from DSM development.In this article, we want to show that a unified approach is not only a desirablegoal, but it is also a feasible one.
With this aim in mind, we introduce DistributionalMemory (DM), a generalized framework for distributional semantics.
Differently fromother current proposals that share similar aims, we believe that the lack of generalizationin corpus-based semantics stems from the choice of representing co-occurrence statisticsdirectly as matrices?geometrical objects that model distributional data in terms ofbinary relations between target items (the matrix rows) and their contexts (the matrixcolumns).
This results in the development of ad hocmodels that lose sight of the fact thatdifferent semantic spaces actually rely on the same kind of underlying distributionalinformation.
DM instead represents corpus-extracted co-occurrences as a third-ordertensor, a ternary geometrical object that models distributional data in terms of word?link?word tuples.
Matrices are then generated from the tensor in order to perform se-mantic tasks in the spaces they define.
Crucially, these on-demand matrices are derivedfrom the same underlying resource (the tensor) and correspond to different ?views?of the same data, extracted once and for all from a corpus.
DM is tested here on whatwe believe to be the most varied array of semantic tasks ever addressed by a singledistributional model.
In all cases, we compare the performance of several DM imple-mentations to state-of-the-art results.
While some of the ad hoc models that were devel-oped to tackle specific tasks do outperform our most successful DM implementation,the latter is never too far from the top, without any task-specific tuning.
We think thatthe advantage of having a general model that does not need to be retrained for each newtask outweighs the (often minor) performance advantage of the task-specific models.The article is structured as follows.
After framing our proposal within the generaldebate on co-occurrence modeling in distributional semantics (Section 2), we introducethe DM framework in Section 3 and compare it to other unified approaches in Section 4.Section 5 pertains to the specific implementations of the DM framework we will testexperimentally.
The experiments are reported in Section 6.
Section 7 concludes bysummarizing what we have achieved, and discussing the implications of these resultsfor corpus-based distributional semantics.2.
Modeling Co-occurrence in Distributional SemanticsCorpus-based semantics aims at characterizing the meaning of linguistic expressions interms of their distributional properties.
The standard view models such properties interms of two-way structures, that is, matrices coupling target elements (either singlewords or whatever other linguistic constructions we try to capture distributionally)and contexts.
In fact, the formal definition of semantic space provided by Pado?
andLapata (2007) is built around the notion of a matrix M|B|?|T|, with B the set of basiselements representing the contexts used to compare the distributional similarity of thetarget elements T.This binary structure is inherently suitable for approaches that represent distribu-tional data in terms of unstructured co-occurrence relations between an element anda context.
The latter can be either documents (Landauer and Dumais 1997; Griffiths,Steyvers, and Tenenbaum 2007) or lexical collocates within a certain distance from thetarget (Lund and Burgess 1996; Schu?tze 1997; Rapp 2003; Bullinaria and Levy 2007).
Wewill refer to such models as unstructured DSMs, because they do not use the linguisticstructure of texts to compute co-occurrences, and only record whether the target occurs676Baroni and Lenci Distributional Memoryin or close to the context element, without considering the type of this relation.
Forinstance, an unstructured DSM might derive from a sentence like The teacher eats a redapple that eat is a feature shared by apple and red, just because they appear in the samecontext window, without considering the fact that there is no real linguistic relationlinking eat and red, besides that of linear proximity.In structured DSMs, co-occurrence statistics are collected instead in the form ofcorpus-derived triples: typically, word pairs and the parser-extracted syntactic relationor lexico-syntactic pattern that links them, under the assumption that the surface con-nection between two words should cue their semantic relation (Grefenstette 1994; Lin1998a; Curran and Moens 2002; Almuhareb and Poesio 2004; Turney 2006b; Pado?
andLapata 2007; Erk and Pado?
2008; Rothenha?usler and Schu?tze 2009).
Distributional triplesare also used in computational lexicography to identify the grammatical and colloca-tional behavior of a word and to define its semantic similarity spaces.
For instance,the Sketch Engine1 builds ?word sketches?
consisting of triples extracted from parsedcorpora and formed by two words linked by a grammatical relation (Kilgarriff et al2004).
The number of shared triples is then used to measure the attributional similaritybetween word pairs.Structured models take into account the crucial role played by syntactic structuresin shaping the distributional properties of words.
To qualify as context of a target item,a word must be linked to it by some (interesting) lexico-syntactic relation, which isalso typically used to distinguish the type of this co-occurrence.
Given the sentenceThe teacher eats a red apple, structured DSMs would not consider eat as a legitimate con-text for red and would distinguish the object relation connecting eat and apple as adifferent type of co-occurrence from the modifier relation linking red and apple.
On theother hand, structured models require more preliminary corpus processing (parsing orextraction of lexico-syntactic patterns), and tend to be more sparse (because there aremore triples than pairs).
What little systematic comparison of the two approaches hasbeen carried out (Pado?
and Lapata 2007; Rothenha?usler and Schu?tze 2009) suggeststhat structured models have a slight edge.
In our experiments in Section 6.1 herein, theperformance of unstructured and structured models trained on the same corpus is ingeneral comparable.
It seems safe to conclude that structured models are at least notworse than unstructured models?an important conclusion for us, as DM is built uponthe structured DSM idea.Structured DSMs extract a much richer array of distributional information fromlinguistic input, but they still represent it in the same way as unstructured models.The corpus-derived ternary data are mapped directly onto a two-way matrix, eitherby dropping one element from the tuple (Pado?
and Lapata 2007) or, more commonly, byconcatenating two elements.
The two words can be concatenated, treating the links asbasis elements, in order to model relational similarity (Pantel and Pennacchiotti 2006;Turney 2006b).
Alternatively, pairs formed by the link and one word are concatenatedas basis elements to measure attributional similarity among the other words, treatedas target elements (Grefenstette 1994; Lin 1998a; Curran and Moens 2002; Almuhareband Poesio 2004; Rothenha?usler and Schu?tze 2009).
In this way, typed DSMs obtainfiner-grained features to compute distributional similarity, but, by couching distribu-tional information as two-way matrices, they lose the high expressive power of corpus-derived triples.
We believe that falling short of fully exploiting the potential of ternary1 http://www.sketchengine.co.uk.677Computational Linguistics Volume 36, Number 4distributional structures is the major reason for the lack of unification in corpus-basedsemantics.The debate in DSMs has so far mostly focused on the context choice?for example,lexical collocates vs. documents (Sahlgren 2006; Turney and Pantel 2010)?or on thecosts and benefits of having structured contexts (Pado?
and Lapata 2007; Rothenha?uslerand Schu?tze 2009).
Although we see the importance of these issues, we believe that areal breakthrough in DSMs can only be achieved by overcoming the limits of currenttwo-way models of distributional data.
We propose here the alternative DM approach,in which the core geometrical structure of a distributional model is a three-way object,namely a third-order tensor.
As in structured DSMs, we adopt word?link?word tuplesas the most suitable way to capture distributional facts.
However, we extend andgeneralize this assumption, by proposing that, once they are formalized as a three-way tensor, tuples can become the backbone of a unified model for distributionalsemantics.
Different semantic spaces are then generated on demand through the inde-pendently motivated operation of tensor matricization, mapping the third-order tensoronto two-way matrices.
The matricization of the tuple tensor produces both familiarspaces, similar to those commonly used for attributional or relational similarity, andother less known distributional spaces, which will yet prove useful for capturing someinteresting semantic phenomena.
The crucial fact is that all these different semanticspaces are now alternative views of the same underlying distributional object.
Appar-ently unrelated semantic tasks can be addressed in terms of the same distributionalmemory, harvested only once from the source corpus.
Thus, thanks to the tensor-basedrepresentation, distributional data can be turned into a general purpose resource forsemantic modeling.
As a further advantage, the third-order tensor formalization ofcorpus-based tuples allows distributional information to be represented in a similarway to other types of knowledge.
In linguistics, cognitive science, and AI, semantic andconceptual knowledge is represented in terms of symbolic structures built around typedrelations between elements, such as synsets, concepts, properties, and so forth.
This iscustomary in lexical networks like WordNet (Fellbaum 1998), commonsense resourceslike ConceptNet (Liu and Singh 2004), and cognitive models of semantic memory(Rogers andMcClelland 2004).
The tensor representation of corpus-based distributionaldata promises to build new bridges between existing approaches to semantic represen-tation that still appear distant in many respects.
This may indeed contribute to the on-going efforts to combine distributional and symbolic approaches to meaning (Clark andPulman 2007).3.
The Distributional Memory FrameworkWe first introduce the notion of a weighted tuple structure, the format in which DMexpects the distributional data extracted from the corpus to be arranged (and that itshares with traditional structured DSMs).
We then show how aweighted tuple structurecan be represented, in linear algebraic terms, as a labeled third-order tensor.
Finally,we derive different semantic vector spaces from the tensor by the operation of labeledtensor matricization.3.1 Weighted Tuple StructuresRelations among entities can be represented by ternary tuples, or triples.
Let O1 andO2 be two sets of objects, and R ?
O1 ?O2 a set of relations between these objects.A triple ?o1, r, o2?
expresses the fact that o1 is linked to o2 through the relation r. DM678Baroni and Lenci Distributional Memory(like previous structured DSMs) includes tuples of a particular type, namely, weighteddistributional tuples that encode distributional facts in terms of typed co-occurrencerelations among words.
Let W1 and W2 be sets of strings representing content words,and L a set of strings representing syntagmatic co-occurrence links between words ina text.
T ?W1 ?
L?W2 is a set of corpus-derived tuples t = ?w1, l,w2?, such that w1co-occurs with w2 and l represents the type of this co-occurrence relation.
For instance,the tuple ?marine, use, bomb?
in the toy example reported in Table 1 encodes the pieceof distributional information that marine co-occurs with bomb in the corpus, and usespecifies the type of the syntagmatic link between these words.
Each tuple t has aweight, a real-valued score vt, assigned by a scoring function ?
:W1 ?
L?W2 ?
R.A weighted tuple structure consists of the set TW of weighted distributional tuplestw = ?t, vt?
for all t ?
T and ?
(t) = vt.
The ?
function encapsulates all the operationsperformed to score the tuples, for example, by processing an input corpus with adependency parser, counting the occurrences of tuples, and weighting the raw countsby mutual information.
Because our focus is on how tuples, once they are harvested,should be represented geometrically, we gloss over the important challenges of choos-ing the appropriateW1, L andW2 string sets, as well as specifying ?.In this article, we make the further assumption that W1 =W2.
This is a naturalassumption when the tuples represent (link-mediated) co-occurrences of word pairs.Moreover, we enforce an inverse link constraint such that for any link l in L, there is ak in L such that for each tuple tw = ?
?wi, l,wj?, vt?
in the weighted tuple structure TW ,the tuple t?1w = ?
?wj, k,wi?, vt?
is also in TW (we call k the inverse link of l).
Again, thisseems reasonable in our context: If we extract a tuple ?marine, use, gun?
and assign it acertain score, we might as well add the tuple ?gun, use?1, marine?
with the same score.The two assumptions, combined, lead the matricization process described in Section 3.3to generate exactly four distinct vector spaces that, as we discuss there, are needed forthe semantic analyses we conduct.
See Section 6.6 of Turney (2006b) for a discussion ofsimilar assumptions.
Still, it is worth emphasizing that the general formalism we areproposing, where corpus-extracted weighted tuple structures are represented as labeledtensors, does not strictly require these assumptions.
For example,W2 could be a largerset of ?relata?
including not only words, but also documents, morphological features,or even visual features (with appropriate links, such as, for word-document relations,occurs-at-the-beginning-of ).
The inverse link constraint might not be appropriate, forexample, if we use an asymmetric association measure, or if we are only interested inone direction of certain grammatical relations.
We leave the investigation of all thesepossibilities to further studies.Table 1A toy weighted tuple structure.word link word weight word link word weightmarine own bomb 40.0 sergeant use gun 51.9marine use bomb 82.1 sergeant own book 8.0marine own gun 85.3 sergeant use book 10.1marine use gun 44.8 teacher own bomb 5.2marine own book 3.2 teacher use bomb 7.0marine use book 3.3 teacher own gun 9.3sergeant own bomb 16.7 teacher use gun 4.7sergeant use bomb 69.5 teacher own book 48.4sergeant own gun 73.4 teacher use book 53.6679Computational Linguistics Volume 36, Number 43.2 Labeled TensorsDSMs adopting a binary model of distributional information (either unstructured mod-els or structured models reduced to binary structures) are represented by matricescontaining corpus-derived co-occurrence statistics, with rows and columns labeled bythe target elements and their contexts.
In DM,we formalize the weighted tuple structureas a labeled third-order tensor, from which semantic spaces are then derived throughthe operation of labeled matricization.
Tensors are multi-way arrays, conventionallydenoted by boldface Euler script letters: X (Turney 2007; Kolda and Bader 2009).
Theorder (or n-way) of a tensor is the number of indices needed to identify its elements.Tensors are a generalization of vectors and matrices.
The entries in a vector can bedenoted by a single index.
Vectors are thus first-order tensors, often indicated by a boldlowercase letter: v. The i-th element of a vector v is indicated by vi.
Matrices are second-order tensors, and are indicatedwith bold capital letters:A.
The entry (i, j) in the i-th rowand j-th column of a matrix A is denoted by aij.
An array with three indices is a third-order (or three-way) tensor.
The element (i, j, k) of a third-order tensor X is denotedby xijk.
A convenient way to display third-order tensors is via nested tables such asTable 2, where the first index is in the header column, the second index in the firstheader row, and the third index in the second header row.
The entry x321 of the tensor inthe table is 7.0 and the entry x112 is 85.3.
An index has dimensionality I if it ranges overthe integers from 1 to I.
The dimensionality of a third-order tensor is the product of thedimensionalities of its indices I ?
J ?
K. For example, the third-order tensor in Table 2has dimensionality 3?
2?
3.If we fix the integer i as the value of the first index of a matrix A and take theentries corresponding to the full range of values of the other index j, we obtain a rowvector (that we denote ai?).
Similarly, by fixing the second index to j, we obtain thecolumn vector a?j.
Generalizing, a fiber is equivalent to rows and columns in higherorder tensors, and it is obtained by fixing the values of all indices but one.
A mode-nfiber is a fiber where only the n-th index has not been fixed.
For example, in the tensorX of Table 2, x?11 = (40.0, 16.7, 5.2) is a mode-1 fiber, x2?3 = (8.0, 10.1) is a mode-2 fiber,and x32?
= (7.0, 4.7, 53.6) is a mode-3 fiber.A weighted tuple structure can be represented as a third-order tensor whose entriescontain the tuple scores.
As for the two-way matrices of classic DSMs, in order to maketensors linguistically meaningful we need to assign linguistic labels to the elements ofthe tensor indices.
We define a labeled tensor X?
as a tensor such that for each of itsindices there is a one-to-one mapping of the integers from 1 to I (the dimensionality ofthe index) to I distinct strings, that we call the labels of the index.
We will refer herein tothe string ?
uniquely associated to index element i as the label of i, their correspondenceTable 2A labeled third-order tensor of dimensionality 3?
2?
3 representing the weighted tuplestructure of Table 1.j=1:own j=2:use j=1:own j=2:use j=1:own j=2:usek=1:bomb k=2:gun k=3:booki=1:marine 40.0 82.1 85.3 44.8 3.2 3.3i=2:sergeant 16.7 69.5 73.4 51.9 8.0 10.1i=3:teacher 5.2 7.0 9.3 4.7 48.4 53.6680Baroni and Lenci Distributional Memorybeing indicated by i : ?.
A simple way to perform the mapping?the one we apply in therunning example of this section?is by sorting the I items in the string set alhabetically,and mapping increasing integers from 1 to I to the sorted strings.A weighted tuple structure TW built from W1, L, and W2 can be represented by alabeled third-order tensor X?
with its three indices labeled by W1, L, and W2, respec-tively, and such that for each weighted tuple t ?
TW = ?
?w1, l,w2?, vt?
there is a tensorentry (i : w1, j : l, k : w2)= vt.
In other terms, a weighted tuple structure corresponds toa tensor whose indices are labeled with the string sets forming the triples, and whoseentries are the tuple weights.
Given the toy weighted tuple structure in Table 1, theobject in Table 2 is the corresponding labeled third-order tensor.3.3 Labeled MatricizationMatricization rearranges a higher order tensor into a matrix (Kolda 2006; Kolda andBader 2009).
The simplest case is mode-n matricization, which arranges the mode-nfibers to be the columns of the resultingDn ?Dj matrix (whereDn is the dimensionalityof the n-th index, Dj is the product of the dimensionalities of the other indices).
Mode-nmatricization of a third-order tensor can be intuitively understood as the process ofmaking vertical, horizontal, or depth-wise slices of a three-way object like the tensor inTable 2, and arranging these slices sequentially to obtain a matrix (a two-way object).Matricization unfolds the tensor into a matrix with the n-th index indexing the rows ofthe matrix and a column for each pair of elements from the other two tensor indices.
Forexample, the mode-1 matricization of the tensor in Table 2 results in a matrix with theentries vertically arranged as they are in the table, but replacing the second and thirdindices with a single index ranging from 1 to 6 (cf.
matrix A of Table 3).
More explicitly,in mode-n matricization we map each tensor entry (i1, i2, ..., iN ) to matrix entry (in, j),where j is computed as in Equation (1), adapted from Kolda and Bader (2009).j = 1+N?k=1k =n((ik ?
1)k?1?m=1m =nDm) (1)For example, if we apply mode-1 matricization to the tensor of dimensionality 3?
2?
3in Table 2, we obtain the matrix A3?6 in Table 3 (ignore the labels for now).
The tensorentry x3,1,1 is mapped to the matrix cell a3,1; x3,2,3 is mapped to a3,6; and x1,2,2 is mappedto a1,4.
Observe that each column of the matrix is a mode-1 fiber of the tensor: The firstcolumn is the x?11 fiber; the second column is the x?21 fiber, and so on.Matricization has various mathematically interesting properties and practical appli-cations in computations involving tensors (Kolda 2006).
In DM, matricization is appliedto labeled tensors and it is the fundamental operation for turning the third-order tensorrepresenting the weighted tuple structure into matrices whose row and column vectorspaces correspond to the linguistic objects we want to study; that is, the outcome ofmatricization must be labeled matrices.
Therefore, we must define an operation oflabeled mode-n matricization.
Recall from earlier discussion that when mode-n matri-cization is applied, the n-th index becomes the row index of the resultingmatrix, and thecorresponding labels do not need to be updated.
The problem is to determine the labelsof the column index of the resulting matrix.
We saw that the columns of the matrix pro-duced by mode-n matricization are the mode-n fibers of the original tensor.
We must681Computational Linguistics Volume 36, Number 4Table 3Labeled mode-1, mode-2, and mode-3 matricizations of the tensor in Table 2.Amode-1 1:?own, 2:?use, 3:?own, 4:?use, 5:?own, 6:?use,bomb?
bomb?
gun?
gun?
book?
book?1:marine 40.0 82.1 85.3 44.8 3.2 3.32:sergeant 16.7 69.5 73.4 51.9 8.0 10.13:teacher 5.2 7.0 9.3 4.7 48.4 53.6Bmode-2 1:?marine, 2:?serg., 3:?teacher, 4:?marine, 5:?serg., 6:?teacher, 7:?marine, 8:?serg., 9:?teach.,bomb?
bomb?
bomb?
gun?
gun?
gun?
book?
book?
book?1:own 40.0 16.7 5.2 85.3 73.4 9.3 3.2 8.0 48.42:use 82.1 69.5 7.0 44.8 51.9 4.7 3.3 10.1 53.6Cmode-3 1:?marine, 2:?marine, 3:?sergeant, 4:?sergeant, 5:?teacher, 6:?teacher,own?
use?
own?
use?
own?
use?1:bomb 40.0 82.1 16.7 69.5 5.2 7.02:gun 85.3 44.8 73.4 51.9 9.3 4.73:book 3.2 3.3 8.0 10.1 48.4 53.6therefore assign a proper label to mode-n tensor fibers.
A mode-n fiber is obtained byfixing the values of two indices, and by taking the tensor entries corresponding to thefull range of values of the third index.
Thus, the natural choice for labeling a mode-nfiber is to use the pair formed by the labels of the two index elements that are fixed.Specifically, each mode-n fiber of a tensor X?
is labeled with the binary tuple whoseelements are the labels of the corresponding fixed index elements.
For instance, giventhe labeled tensor in Table 2, the mode-1 fiber x?11 = (40, 16.7, 5.2) is labeled with thepair ?own, bomb?, the mode-2 fiber x2?1 = (16.7, 69.5) is labeled with the pair ?sergeant,bomb?, and the mode-3 fiber x32?
= (7.0, 4.7, 53.6) is labeled with the pair ?teacher, use?.Because mode-n fibers are the columns of the matrices obtained through mode-nmatricization, we define the operation of labeled mode-n matricization that, givena labeled third-order tensor X?, maps each entry (i1 : ?1, i2 : ?2, i3 : ?3) to the labeledentry (in : ?n, j : ?j) such that j is obtained according to Equation (1), and ?j is the bi-nary tuple obtained from the triple ?
?1, ?2, ?3?
by removing ?n.
For instance, in mode-1matricization, the entry (1:marine, 1:own, 2:gun) in the tensor in Table 2 is mapped ontothe entry (1:marine, 3:?own, gun?).
Table 3 reports the matrices A, B, and C, respectively,obtained by applying labeled mode-1, mode-2, and mode-3 matricization to the labeledtensor in Table 2.
The columns of each matrix are labeled with pairs, according to thedefinition of labeled matricization we just gave.
From now on, when we refer to mode-nmatricization we always assume we are performing labeledmode-nmatricization.The rows and columns of the three matrices resulting from n-mode matricizationof a third-order tensor are vectors in spaces whose dimensions are the correspondingcolumn and row elements.
Such vectors can be used to perform all standard linearalgebra operations applied in vector-based semantics: Measuring the cosine of theangle between vectors, applying singular value decomposition (SVD) to the wholematrix, and so on.
Under the assumption thatW1 =W2 and the inverse link constraint(see Section 3.1), it follows that for each column of the matrix resulting from mode-1matricization and labeled by ?l,w2?, there will be a column in the matrix resulting682Baroni and Lenci Distributional Memoryfrom mode-3 matricization that is labeled by ?w1, k?
(with k being the inverse link ofl and w1 = w2) and that is identical to the former, except possibly for the order of thedimensions (which is irrelevant to all operations we perform on matrices and vectors,however).
Similarly, for any row w2 in the matrix resulting from mode-3 matricization,there will be an identical row w1 in the mode-1 matricization.
Therefore, given aweighted tuple structure TW extracted from a corpus and subject to the constraints wejust mentioned, by matricizing the corresponding labeled third-order tensor X ?
weobtain the following four distinct semantic vector spaces:word by link?word (W1?LW2): vectors are labeled with words w1, and vectordimensions are labeled with tuples of type ?l,w2?
;word?word by link (W1W2?L): vectors are labeled with tuples of type ?w1,w2?,and vector dimensions are labeled with links l;word?link by word (W1L?W2): vectors are labeled with tuples of type ?w1, l?,and vector dimensions are labeled with words w2;link by word?word (L?W1W2): vectors are labeled with links l and vectordimensions are labeled with tuples of type ?w1,w2?.Words like marine and teacher are represented in the W1?LW2 space by vectors whosedimensions correspond to features such as ?use, gun?
or ?own, book?.
In this space,we can measure the similarity of words to each other, in order to tackle attributionalsimilarity tasks such as synonym detection or concept categorization.
The W1W2?Lvectors represent instead word pairs in a space whose dimensions are links, and itcan be used to measure relational similarity among different pairs.
For example, onecould notice that the link vector of ?sergeant, gun?
is highly similar to that of ?teacher,book?.
Crucially, as can be seen in Table 3, the corpus-derived scores that populate thevectors in these two spaces are exactly the same, just arranged in different ways.
In DM,attributional and relational similarity spaces are different views of the same underlyingtuple structure.The other two distinct spaces generated by tensor matricization look less familiar,and yet we argue that they allow us to subsume under the same general DM frameworkother interesting semantic phenomena.
We will show in Section 6.3 how the W1L?W2space can be used to capture different verb classes based on the argument alternationsthey display.
For instance, this space can be used to find out that the object slot of killis more similar to the subject slot of die than to the subject slot of kill (and, generalizingfrom similar observations, that the subject slot of die is a theme rather than an agent).
TheL?W1W2 space displays similarities among links.
The usefulness of this will of coursedepend on what the links are.
We will illustrate in Section 6.4 one function of this space,namely, to perform feature selection, picking links that can then be used to determine ameaningful subspace of theW1W2?L space.Direct matricization is just one of the possible uses we can make of the labeledtensor.
In Section 6.5 we illustrate another use of the tensor formalism by performingsmoothing through tensor decomposition.
Other possibilities, such as graph-based algo-rithms operating directly on the graph defined by the tensor (Baroni and Lenci 2009), orderiving unstructured semantic spaces from the tensor by removing one of the indices,are left to future work.Before we move on, it is worth emphasizing that, from a computational point ofview, there is virtually no additional cost in the tensor approach, with respect to tra-ditional structured DSMs.
The labeled tensor is nothing other than a formalization of683Computational Linguistics Volume 36, Number 4distributional data extracted in the word?link?word?score format, which is customaryin many structured DSMs.
Labeled matricization can then simply be obtained by con-catenating two elements in the original triple to build the corresponding matrix?again,a common step in building a structured DSM.
In spite of being cost-free in terms ofimplementation, the mathematical formalism of labeled tensors highlights the commoncore shared by different views of the semantic space, thereby making distributionalsemantics more general.4.
Related WorkAs will be clear in the next sections, the ways in which we tackle specific tasks are, bythemselves, mostly not original.
The main element of novelty is the fact that methodsoriginally developed to resort to ad hoc distributional spaces are now adapted to fit intothe unified DM framework.
We will point out connections to related research specific tothe various tasks in the sections devoted to describing their reinterpretation in DM.We omit discussion of our own work that the DM framework is an extension andgeneralization of Baroni et al (2010) and Baroni and Lenci (2009).
Instead, we brieflydiscuss two other studies that explicitly advocate a uniform approach to corpus-basedsemantic tasks, and one article that, like us, proposes a tensor-based formalization ofcorpus-extracted triples.
See Turney and Pantel (2010) for a very recent general surveyof DSMs.Pado?
and Lapata (2007), partly inspired by Lowe (2001), have proposed an interest-ing general formalization of DSMs.
In their approach, a corpus-based semantic model ischaracterized by (1) a set of functions to extract statistics from the corpus, (2) construc-tion of the basis-by-target-elements co-occurrence matrix, and (3) a similarity functionoperating on the matrix.
Our focus is entirely on the second aspect.
A DM, accordingto the characterization in Section 3, is a labeled tensor based on a source weightedtuple structure and coupled with matricization operations.
How the tuple structurewas built (corpus extraction methods, association measures, etc.)
is not part of the DMformalization.
At the other end, DM provides sets of vectors in different vector spaces,but it is agnostic about how they are used (measuring similarity via cosines or othermeasures, reducing the matrices with SVD, etc.).
Of course, much of the interestingprogress in distributional semantics will occur at the two ends of our tensor, with bettertuple extraction and weighting techniques on one side, and better matrix manipulationand similarity measurement on the other.
As long as the former operations result indata that can be arranged into a weighted tuple structure, and the latter procedures acton vectors, such innovations fit into the DM framework and can be used to improveperformance on tasks defined on any space derivable from the DM tensor.Whereas the model proposed by Pado?
and Lapata (2007) is designed only to ad-dress tasks involving the measurement of the attributional similarity between words,Turney (2008) shares with DM the goal of unifying attributional and relational similarityunder the same distributional model.
He observes that tasks that are traditionally solvedwith an attributional similarity approach can be recast as relational similarity tasks.Instead of determining whether two words are, for example, synonymous by lookingat the features they share, we can learn what the typical patterns are that connect syn-onym pairs when they co-occur (also known as, sometimes called, etc.
), and make a de-cision about a potential synonym pair based on their occurrence in similar contexts.Given a list of pairs instantiating an arbitrary relation, Turney?s PairClass algorithmextracts patterns that are correlated with the relation, and can be used to discover new684Baroni and Lenci Distributional Memorypairs instantiating it.
Turney tests his system in a variety of tasks (TOEFL synonyms;SAT analogies; distinguishing synonyms and antonyms; distinguishing pairs that aresemantically similar, associated, or both), obtaining good results across the board.In the DM approach, we collect one set of statistics from the corpus, and then exploitdifferent views of the extracted data and different algorithms to tackle different tasks.Turney, on the contrary, uses a single generic algorithm, but must go back to the corpusto obtain new training data for each new task.
We compare DM with some of Turney?sresults in Section 6 but, independently of performance, we find the DM approach moreappealing.
As corpora grow in size and are enriched with further levels of annotation,extracting ad hoc data from them becomes a very time-consuming operation.
Althoughwe did not carry out any systematic experiments, we observe that the extraction of tuplecounts from (already POS-tagged and parsed) corpora in order to train our sample DMmodels took days, whereas even the most time-consuming operations to adapt DM toa task took on the order of 1 to 2 hours on the same machines (task-specific training isalso needed in PairClass, anyway).
Similar considerations apply to space: Compressed,our source corpora take about 21 GB, our best DM tensor (TypeDM) 1.1 GB (and opti-mized sparse tensor representations could bring this quantity down drastically, if theneed arises).
Perhaps more importantly, extracting features from the corpus requiresa considerable amount of NLP know-how (to pre-process the corpus appropriately, tonavigate a dependency tree, etc.
), whereas the DM representation of distributional dataas weighted triples is more akin to other standard knowledge representation formatsbased on typed relations, which are familiar to most computer and cognitive scientists.Thus, a trained DM can become a general-purpose resource and be used by researchersbeyond the realms of the NLP community, whereas applying PairClass requires a goodunderstanding of various aspects of computational linguistics.
This severely limits itsinterdisciplinary appeal.At a more abstract level, DM and PairClass differ in the basic strategy with whichunification in distributional semantics is pursued.
Turney?s approach amounts to pick-ing a task (identifying pairs expressing the same relation) and reinterpreting other tasksas its particular instances.
Thus, attributional and relational similarity are unified byconsidering the former as a subtype of the latter.
Conversely, DM assumes that eachsemantic task may keep its specificity, and unification is achieved by designing a suffi-ciently general distributional structure, populating a specific instance of the structure,and generating semantic spaces on demand from the latter.
This way, DM is able toaddress a wider range of semantic tasks than Turney?s model.
For instance, languageis full of productive semantic phenomena, such as the selectional preferences of verbswith respect to unseen arguments (eating topinambur vs. eating sympathy).
Predictingthe plausibility of unseen pairs cannot, by definition, be tackled by the current versionof PairClass, which will have to be expanded to deal with such cases, perhaps adoptingideas similar to those we present (that are, in turn, inspired by Turney?s own work onattributional and relational similarity).
A first step in this direction, within a frameworksimilar to Turney?s, was taken by Herdag?delen and Baroni (2009).Turney (2007) explicitly formalizes the set of corpus-extracted word?link?wordtriples as a tensor, and was our primary source of inspiration in formalizing DM in theseterms.
The focus of Turney?s article, however, is on dimensionality reduction techniquesapplied to tensors, and the application to corpora is only briefly discussed.
Moreover,Turney only derives the W1?LW2 space from the tensor, and does not discuss the pos-sibility of using the tensor-based formalization to unify different views of semantic data,which is instead our main point.
The higher-order tensor dimensionality reductiontechniques tested on language data by Turney (2007) and Van de Cruys (2009) can be685Computational Linguistics Volume 36, Number 4applied to the DM tensors before matricization.
We present a pilot study in this direc-tion in Section 6.5.5.
Implementing DM5.1 Extraction of Weighted Tuple Structures from CorporaIn order to make our proposal concrete, we experiment with three different DMmodels,corresponding to different ways to construct the underlying weighted tuple structure(Section 3.1).
All models are based on the natural idea of extracting word?link?wordtuples from a dependency parse of a corpus, but this is not a requirement for DM: Thelinks could for example be based on frequent n-grams as in Turney (2006b) and Baroniet al (2010), or even on very different kinds of relation, such as co-occurring within thesame document.The current models are trained on the concatenation of (1) the Web-derived ukWaCcorpus,2 about 1.915 billion tokens (here and subsequently, counting only stringsthat are entirely made of alphabetic characters); (2) a mid-2009 dump of the EnglishWikipedia,3 about 820 million tokens; and (3) the British National Corpus,4 about95 million tokens.
The resulting concatenated corpus was tokenized, POS-tagged, andlemmatized with the TreeTagger5 and dependency-parsed with the MaltParser.6 Itcontains about 2.83 billion tokens.
The ukWaC and Wikipedia sections can be freelydownloaded, with full annotation, from the ukWaC corpus site.For all our models, the label sets W1 =W2 contain 30,693 lemmas (20,410 nouns,5,026 verbs, and 5,257 adjectives).
These terms were selected based on their frequencyin the corpus (they are approximately the top 20,000 most frequent nouns and top 5,000most frequent verbs and adjectives), augmenting the list with lemmas that we found invarious standard test sets, such as the TOEFL and SAT lists.
In all models, the words arestored in POS-suffixed lemma form.
The weighted tuple structures differ for the choiceof links in L and/or for the scoring function ?.DepDM.
Our first DM model relies on the classic intuition that dependency paths area good approximation to semantic relations between words (Grefenstette 1994; Curranand Moens 2002; Pado?
and Lapata 2007; Rothenha?usler and Schu?tze 2009).
DepDM isalso the model with the least degree of link lexicalization among the three DM instanceswe have built (its only lexicalized links are prepositions).
LDepDM includes the follow-ing noun?verb, noun?noun, and adjective?noun links (in order to select more reliabledependencies and filter out possible parsing errors, dependencies between words withmore than five intervening items were discarded):sbj intr: subject of a verb that has no direct object: The teacher is singing?
?teacher, sbj intr, sing?
; The soldier talked with his sergeant?
?soldier, sbj intr, talk?
;sbj tr: subject of a verb that occurs with a direct object: The soldier is reading abook?
?soldier, sbj tr, read?
;2 http://wacky.sslmit.unibo.it/.3 http://en.wikipedia.org/wiki/Wikipedia:Database download.4 http://www.natcorp.ox.ac.uk.5 http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/.6 http://w3.msi.vxu.se/?nivre/research/MaltParser.html.686Baroni and Lenci Distributional Memoryobj: direct object: The soldier is reading a book?
?book, obj, read?
;iobj: indirect object in a double object construction: The soldier gave the womana book?
?woman, iobj, give?
;nmod: noun modifier: good teacher?
?good, nmod, teacher?
; school teacher?
?school,nmod, teacher?
;coord: noun coordination: teachers and soldiers?
?teacher, coord, soldier?
;prd: predicate noun: The soldier became sergeant?
?sergeant, prd, become?
;verb: an underspecified link between a subject noun and a complement nounof the same verb: The soldier talked with his sergeant?
?soldier, verb, sergeant?
;The soldier is reading a book?
?soldier, verb, book?
;preposition: every preposition linking the noun head of a prepositional phraseto its noun or verb head (a different link for each preposition): I saw a soldierwith the gun?
?gun, with, soldier?
; The soldier talked with his sergeant?
?sergeant, with, talk?.For each link, we also extract its inverse (this holds for all our DMmodels).
For example,there is a sbj intr?1 link between an intransitive verb and its subject: ?talk, sbj intr?1,soldier?.
The cardinality of LDepDM is 796, including direct and inverse links.The weights assigned to the tuples by the scoring function ?
are given by LocalMutual Information (LMI) computed on the raw corpus-derived word?link?word co-occurrence counts.
Given the co-occurrence count Oijk of three elements of interest (inour case, the first word, the link, and the second word), and the corresponding expectedcount under independence Eijk, LMI = Oijk logOijkEijk.
LMI is an approximation to thelog-likelihood ratio measure that has been shown to be a very effective weightingscheme for sparse frequency counts (Dunning 1993; Pado?
and Lapata 2007).
The mea-sure can also be interpreted as the dominant term of average MI or as a heuristicvariant of pointwise MI to avoid its bias towards overestimating the significance oflow frequency events, and it is nearly identical to the Poisson?Stirling measure (Evert2005).
LMI has considerable computational advantages in cases like ours, in which wemeasure the association of three elements, because it does not require keeping trackof the full 2?
2?
2 contingency table, which is the case for the log-likelihood ratio.Following standard practice (Bullinaria and Levy 2007), negative weights (cases wherethe observed value is lower than the expected value) are raised to 0.
The number ofnon-zero tuples in the DepDM tensor is about 110M, including tuples with direct linksand their inverses.
DepDM is a 30, 693?
796?
30, 693 tensor with density 0.0149% (theproportion of non-zero entries in the tensor).LexDM.
The second model is inspired by the idea that the lexical material connect-ing two words is very informative about their relation (Hearst 1992; Pantel andPennacchiotti 2006; Turney 2006b).
LLexDM contains complex links, each with the struc-ture pattern+suffix.
The suffix is in turn formed by two substrings separated by a+, eachrespectively encoding the following features ofw1 andw2: their POS andmorphologicalfeatures (number for nouns, number and tense for verbs); the presence of an article(further specified with its definiteness value) and of adjectives for nouns; the presenceof adverbs for adjectives; and the presence of adverbs, modals, and auxiliaries for verbs,together with their diatheses (for passive only).
If the adjective (adverb) modifyingw1 or w2 belongs to a list of 10 (250) high frequency adjectives (adverbs), the suffix687Computational Linguistics Volume 36, Number 4string contains the adjective (adverb) itself, otherwise only its POS.
For instance, fromthe sentence The tall soldier has already shot we extract the tuple ?soldier, sbj intr+n-the-j+vn-aux-already, shoot?.
Its complex link contains the pattern sbj intr and the suffixn-the-j+vn-aux-already.
The suffix substring n-the-j encodes the information that w1 isa singular noun (n), is definite (the), and has an adjective ( j) that does not belong tothe list of high frequency adjectives.
The substring vn-aux-already specifies that w2 is apast-participle (vn), has an auxiliary (aux), and is modified by already, belonging to thepre-selected list of high frequency adverbs.
The patterns in the LexDM links include:LDepDM : every DepDM link is a potential pattern of a LexDM link: The soldier hasshot?
?soldier, sbj intr+n-the+vn-aux, shoot?
;verb: if the verb link between a subject noun and a complement noun belongs toa list of 52 high frequency verbs, the underspecified verb link of DepDM isreplaced by the verb itself: The soldier used a gun?
?soldier, use+n-the+n-a,gun?
; The soldier read the yellow book?
?soldier, verb+n-the+n-the-j, book?
;is: copulative structures with an adjectival predicate: The soldier is tall?
?tall,is+j+n-the, soldier?
;preposition?link noun?preposition: this schema captures connecting expressionssuch as of a number of, in a kind of ; link noun is one of 48 semi-manuallyselected nouns such as number, variety, or kind; the arrival of a number ofsoldiers?
?soldier, of-number-of+ns+n-the, arrival?
;attribute noun: one of 127 nouns extracted fromWordNet and expressingattributes of concepts, such as size, color, or height.
This pattern connectsadjectives and nouns that occur in the templates (the) attribute noun of(a|the) NOUN is ADJ (Almuhareb and Poesio 2004) and (a|the) ADJattribute noun of NOUN (Veale and Hao 2008): the color of strawberries is red?
?red, color+j+ns, strawberry?
; the autumnal color of the forest?
?autumnal,color+j+n-the, forest?
;as adj as: this pattern links an adjective and a noun that match the template asADJ as (a|the) NOUN (Veale and Hao 2008): as sharp as a knife?
?sharp,as adj as+j+n-a, knife?
;such as: links two nouns occurring in the templates NOUN such as NOUN andsuch NOUN as NOUN (Hearst 1992, 1998): animals such as cats?
?animal, such as+ns+ns, cat?
; such vehicles as cars?
?vehicle, such as+ns+ns, car?.LexDM links have a double degree of lexicalization.
First, the suffixes encode a widearray of surface features of the tuple elements.
Secondly, the link patterns themselves,besides including standard syntactic relations (such as direct object or coordination),extend to lexicalized dependency relations (specific verbs) and lexico-syntactic shallowtemplates.
The latter include patterns adopted in the literature to extract specific piecesof semantic knowledge.
For instance, NOUN such as NOUN and such NOUN as NOUNwere first proposed by Hearst (1992) as highly reliable patterns for hypernym identifi-cation, whereas (the) attribute noun of (a|the) NOUN is ADJ and (a|the) ADJ attribute nounof NOUN were successfully used to identify typical values of concept attributes(Almuhareb and Poesio 2004; Veale and Hao 2008).
Therefore, the LexDM distributionalmemory is a repository of partially heterogeneous types of corpus-derived information,differing in their level of abstractness, which ranges from fairly abstract syntactic rela-tions to shallow lexicalized patterns.
LLexDM contains 3,352,148 links, including inverses.688Baroni and Lenci Distributional MemoryThe scoring function ?
is the same as that in DepDM, and the number of non-zero tuples is about 355M, including direct and inverse links.
LexDM is a 30,693?3,352,148?
30,693 tensor with density 0.00001%.TypeDM.
This model is based on the idea, motivated and tested by Baroni et al (2010)?but see also Davidov and Rappoport (2008a, 2008b) for a related method?that whatmatters is not so much the frequency of a link, but the variety of surface forms thatexpress it.
For example, if we just look at frequency of co-occurrence (or strength ofassociation), the triple ?
fat, of?1, land?
(a figurative expression) is much more commonthan the semantically more informative ?
fat, of?1, animal?.
However, if we count thedifferent surface realizations of the former pattern in our corpus, we find that there areonly three of them (the fat of the land, the fat of the ADJ land, and the ADJ fat of the land),whereas ?
fat, of?1, animal?
has nine distinct realizations (a fat of the animal, the fat of theanimal, fats of animal, fats of the animal, fats of the animals, ADJ fats of the animal, and the fatsof the animal).
TypeDM formalizes this intuition by adopting as links the patterns insidethe LexDM links, while the suffixes of these patterns are used to count their numberof distinct surface realizations.
We call the model TypeDM because it counts types ofrealizations, not tokens.
For instance, the two LexDM links of?1+n-a+n-the and of?1+ns-j+n-the are counted as two occurrences of the same TypeDM link of?1, corresponding tothe pattern in the two original links.The scoring function ?
computes LMI not on the raw word?link?word co-occurrence counts, but on the number of distinct suffix types displayed by a link when itco-occurs with the relevant words.
For instance, a TypeDM link derived from a LexDMpattern that occurs with nine different suffix types in the corpus is assigned a frequencyof 9 for the purpose of the computation of LMI.
The distinct TypeDM links are 25,336.The number of non-zero tuples in the TypeDM tensor is about 130M, including directand inverse links.
TypeDM is a 30, 693?
25, 336?
30, 693 tensor with density 0.0005%.To sum up, the three DM instance models herein differ in the degree of lexicali-zation of the link set, and/or in the scoring function.
LexDM is a heavily lexicalizedmodel, contrasting with DepDM, which has a minimum degree of lexicalization, andconsequently the smallest set of links.
TypeDM represents a sort of middle level bothfor the kind and the number of links.
These consist of syntactic and lexicalized patterns,as in LexDM.
The lexical information encoded in the LexDM suffixes, however, is notused to generate different links, but to implement a different counting scheme as partof a different scoring function.A weighted tuple structure (equivalently: a labeled DM tensor) is intended asa long-term semantic resource that can be used in different projects for differenttasks, analogously to traditional hand-coded resources such as WordNet.
Coherentwith this approach, we make our best DM model (TypeDM) publicly available fromhttp://clic.cimec.unitn.it/dm.
The site also contains a set of Perl scripts that per-form the basic operations on the tensor and its derived vectors we are about to describe.5.2 Semantic Vector ManipulationTheDM framework provides, viamatricization, a set of matrices with associated labeledrow and column vectors.
These labeled matrices can simply be derived from the tupletensor by concatenating two elements in the original triples.
Any operation that canbe performed on the resulting matrices and that might help in tackling a semantictask is fair game.
However, in the experiments reported in this article we will workwith a limited number of simple operations that are well-motivated in terms of the689Computational Linguistics Volume 36, Number 4geometric framework we adopt, and suffice to face all the tasks we will deal with (thedecomposition techniques explored in Section 6.5 are briefly introduced there).Vector length and normalization.
The length of a vector v with dimensions v1, v2, .
.
.
, vn is:||v|| =?
?i=ni=1v2iA vector is normalized to have length 1 by dividing each dimension by the originalvector length.Cosine.We measure the similarity of two vectors x and y by the cosine of the angle theyform:cos(x,y) =?i=ni=1 xiyi||x||||y||The cosine ranges from?1 for vectors pointing in the same direction to 0 for orthogonalvectors.
Other similarity measures, such as Lin?s measure (Lin 1998b), work better thanthe cosine in some tasks (Curran and Moens 2002; Pado?
and Lapata 2007).
However,the cosine is the most natural similarity measure in the geometric formalism we areadopting, and we stick to it as the default approach to measuring similarity.Vector sum.
Two or more vectors are summed in the obvious way, by adding their valueson each dimension.
We always normalize the vectors before summing.
The resultingvector points in the same direction as the average of the summed normalized vectors.We refer to it as the centroid of the vectors.Projection onto a subspace.
It is sometimes useful to measure length or compare vectors bytaking only some of their dimensions into account.
For example, one way to find nounsthat are typical objects of the verb to sing is to measure the length of nouns in aW1?LW2subspace in which only dimensions such as ?obj, sing?
have non-0 values.
We projecta vector onto a subspace of this kind through multiplication of the vector by a squarediagonal matrix with 1s in the diagonal cells corresponding to the dimensions we wantto preserve and 0s elsewhere.
A matrix of this sort performs an orthogonal projection ofthe vector it multiplies (Meyer 2000, chapter 5).6.
Semantic Experiments with the DM SpacesAs we saw in Section 3, labeled matricization generates four distinct semantic spacesfrom the third-order tensor.
For each space, we have selected a set of semantic ex-periments that we model by applying some combination of the vector manipulationoperations of Section 5.2.
The experiments correspond to key semantic tasks in compu-tational linguistics and/or cognitive science, typically addressed by distinct DSMs sofar.
We have also aimed at maximizing the variety of aspects of meaning covered bythe experiments, ranging from synonymy detection to argument structure and conceptproperties, and encompassing all the major lexical classes.
Both these facts support theview of DM as a generalized model that is able to overtake state-of-the-art DSMs inthe number and types of semantic issues addressed, while being competitive in eachspecific task.690Baroni and Lenci Distributional MemoryThe choice of the DM semantic space to tackle a particular task is essentially basedon the ?naturalness?
with which the task can be modeled in that space.
However,alternatives are conceivable, both with respect to space selection, and to the operationsperformed on the space.
For instance, Turney (2008) models synonymy detection witha DSM that closely resembles our W1W2?L space, whereas we tackle this task underthe more standard W1?LW2 view.
It is an open question whether there are principledways to select the optimal space configuration for a given semantic task.
In this article,we limit ourselves to proving that each space derived through tensor matricization issemantically interesting in the sense that it provides the proper ground to address somesemantic task.Feature selection/reweighting and dimensionality reduction have been shown toimprove DSM performance.
For instance, the feature bootstrapping method proposedby Zhitomirsky-Geffet and Dagan (2009) boosts the precision of a DSM in lexical en-tailment recognition.
Even if these methods can be applied to DM as well, we did notuse them in our experiments.
The results presented subsequently should be regarded asa ?baseline?
performance that could be enhanced in future work by exploring varioustask-specific parameters (we will come back in the conclusion to the role of parametertuning in DM).
This is consistent with our current aim of focusing on the generality andadaptivity of DM, rather than on task-specific optimization.
As a first, important stepin this latter direction, however, we conclude the empirical evaluation in Section 6.5by replicating one experiment using tensor-decomposition-based smoothing, a form ofoptimization that can only be performed within the tensor-based approach to DSMs.In order to maximize coverage of the experimental test sets, they are pre-processedwith a mixture of manual and heuristic procedures to assign a POS to the words theycontain, lemmatize, convert some multiword forms to single words, and turn some ad-verbs into adjectives (our models do not contain multiwords or adverbs).
Nevertheless,some words (or word pairs) are unrecoverable, and in such cases we make a randomguess (in cases where we do not have full coverage of a data set, the reported results areaverages across repeated experiments, to account for the variability in random guesses).In many of the experiments herein, DM is not only compared to the results avail-able in the literature, but also to our implementation of state-of-the-art DSMs.
Thesealternative models have been trained on the same corpus (with the same linguistic pre-processing) used to build the DM tuple tensors.
This way, we aim at achieving a fairercomparison with alternative approaches in distributional semantics, abstracting awayfrom the effects induced by differences in the training data.Most experiments report global (micro-averaged) test set accuracy (alone, or com-bined with other measures) to assess the performance of the algorithms.
The number ofcorrectly classified items among all test elements can be seen as a binomially distributedrandom variable, and we follow the ACL Wiki state-of-the-art site7 in reporting alsoClopper?Pearson binomial 95% confidence intervals around the accuracies (binomialintervals and other statistical quantities were computed using the R package;8 where nofurther references are given, we used the standard R functions for the relevant analysis).The binomial confidence intervals give a sense of the spread of plausible populationvalues around the test-set-based point estimates of accuracy.
Where appropriate andinteresting, we compare the accuracy of two specific models statistically with an exactFisher test on the contingency table of correct and wrong responses given by the two7 http://aclweb.org/aclwiki/index.php?title=State Of The Art.8 http://www.r-project.org/.691Computational Linguistics Volume 36, Number 4models.
This approach to significance testing is problematic in many respects, the mostimportant being that we ignore dependencies in correct and wrong counts due to thefact that the algorithms are evaluated on the same test set (Dietterich 1998).
Moreappropriate tests, however, would require access to the fully itemized results from thecompared algorithms, whereas in most cases we only know the point estimate reportedin the earlier literature.
For similar reasons, we do not make significance claims regard-ing other performance measures, such as macro-averaged F. Other forms of statisticalanalysis of the results are introduced herein when they are used; they are mostly limitedto the models for which we have full access to the results.
Note that we are interested inwhether DM performance is overall within state-of-the-art range, and not on makingprecise claims about the models it outperforms.
In this respect, we think that ourgeneral results are clear even where they are not supported by statistical inference, orinterpretation of the latter is problematic.6.1 The W1?LW2 SpaceThe vectors of this space are labeled with words w1 (rows of matrix Amode-1 in Table 3),and their dimensions are labeled with binary tuples of type ?l,w2?
(columns of the samematrix).
The dimensions represent the attributes of words in terms of lexico-syntacticrelations with lexical collocates, such as ?sbj intr, read?, or ?use, gun?.
Consistently, allthe semantic tasks that we address with this space involve the measurement of theattributional similarity between words.TheW1?LW2 matrix is a structured semantic space similar to those used by Curranand Moens (2002), Grefenstette (1994), and Lin (1998a), among others.
To test if theuse of links detracts from performance on attributional similarity tasks, we trained onour concatenated corpus two alternative models?Win and DV?whose features onlyinclude lexical collocates of the target.
Win is an unstructured DSM that does not rely onsyntactic structure to select the collocates, but just on their linear proximity to the targets(Lund and Burgess 1996; Schu?tze 1997; Bullinaria and Levy 2007, and many others).
Itsmatrix is based on co-occurrences of the same 30K words we used for the other modelswithin a window of maximally five content words before or after the target.
DV is animplementation of the Dependency Vectors approach of Pado?
and Lapata (2007).
It is astructured DSM, but dependency paths are used to pick collocates, without being part ofthe attributes.
The DV model is obtained from the same co-occurrence data as DepDM(thus, relying on the dependency paths we picked, not the ones originally selectedby Pado?
and Lapata for their tests).
Frequencies are summed across dependency pathlinks for word?link?word triples with the same first and second words.
Suppose thatsoldier and gun occur in the tuples ?soldier, have, gun?
(frequency 3) and ?soldier, use, gun?
(frequency 37).
In DepDM, this results in two features for soldier: ?have, gun?
and ?use,gun?.
In DV, we would derive a single gun feature with frequency 40.
As for the DMmodels, theWin and DV counts are converted to LMI weights, and negative LMI valuesare raised to 0.
Win is a 30,693?
30,693 matrix with about 110 million non-zero entries(density: 11.5%).
DV is a 30,693?
30,693 matrix with about 38 million non-zero values(density: 4%).6.1.1 Similarity Judgments.
Our first challenge comes from the classic data set ofRubenstein and Goodenough (1965), consisting of 65 noun pairs rated by 51 subjectson a 0?4 similarity scale.
The average rating for each pair is taken as an estimate of theperceived similarity between the two words (e.g., car?automobile: 3.9, cord?smile: 0.0).Following the earlier literature, we use Pearson?s r to evaluate how well the cosines692Baroni and Lenci Distributional Memoryin the W1?LW2 space between the nouns in each pair correlate with the ratings.
Theresults (expressed in terms of percentage correlations) are presented in Table 4, whichalso reports state-of-the-art performance levels of corpus-based systems from the litera-ture (the correlation of all systems with the ratings is very significantly above chance,according to a two-tailed t-test for Pearson correlation coefficients; df = 63, p < 0.0001for all systems).One of the DMmodels, namely TypeDM, does very well on this task, outperformedonly by DoubleCheck, an unstructured system that relies onWeb queries (and thus on amuch larger corpus) and for which we report the best result across parameter settings.We also report the best results from a range of experiments with different models andparameter settings from Herdag?delen, Erk, and Baroni (2009) (whose corpus is abouthalf the size of ours) and Pado?
and Lapata (2007) (who use a much smaller corpus).
Forthe latter, we also report the best result they obtain when using cosine as the similaritymeasure (cosDV-07).
Overall, the TypeDM result is in line with the state of the art, giventhe size of the input corpus, and the fact that we did not perform any tuning.
FollowingPado?, Pado?, and Erk (2007) we used the approximate test proposed by Raghunathan(2003) to compare the correlations with the human ratings of sets of models (this is onlypossible for themodels we developed, as the test requires computation of correlation co-efficients across models).
The test suggests that the difference in correlation with humanratings between TypeDM and our second best model, Win, is significant (Q = 4.55, df =0.23, p< 0.01).
On the other hand, there is no significant difference across Win, DepDM,DV and LexDM (Q = 1.02, df = 1.80, p = 0.55).6.1.2 Synonym Detection.
The previous experiment assessed how the models can simu-late quantitative similarity ratings.
The classic TOEFL synonym detection task focuseson the high end of the similarity scale, asking the models to make a discrete decisionabout which word is the synonym from a set of candidates.
The data set, introducedto computational linguistics by Landauer and Dumais (1997), consists of 80 multiple-choice questions, each made of a target word (a noun, verb, adjective, or adverb) andfour candidates.
For example, given the target levied, the candidates are imposed, believed,requested, correlated, the first one being the correct choice.
Our algorithms pick thecandidate with the highest cosine to the target item as their guess of the right synonym.Table 5 reports results (percentage accuracies) on the TOEFL set for our models aswell as the best model of Herdag?delen and Baroni (2009) and the corpus-based modelsfrom the ACL Wiki TOEFL state-of-the-art table (we do not include those models fromthe Wiki that resort to other knowledge sources, such as WordNet or a thesaurus).
Theclaims to follow about the relative performance of the models must be interpretedcautiously, in light of the spread of the confidence intervals: It suffices to note that,Table 4Percentage Pearson correlation with the Rubenstein and Goodenough (1965) similarity ratings.model r model r model rDoubleCheck1 85 Win 65 DV 57TypeDM 82 DV-073 62 LexDM 53SVD-092 80 DepDM 57 cosDV-073 47Model sources: 1Chen, Lin, and Wei (2006); 2Herdag?delen, Erk, and Baroni (2009); 3Pado?
andLapata (2007).693Computational Linguistics Volume 36, Number 4Table 5Percentage accuracy in TOEFL synonym detection with 95% binomial confidence intervals (CI).model accuracy 95% CI model accuracy 95% CILSA-031 92.50 84.39?97.20 DepDM 75.01 64.06?84.01GLSA2 86.25 76.73?92.93 LexDM 74.37 63.39?83.49PPMIC3 85.00 75.26?92.00 PMI-IR-018 73.75 62.72?82.96CWO4 82.55 72.38?90.09 DV-079 73.00 62.72?82.96PMI-IR-035 81.25 70.97?89.11 Win 69.37 58.07?79.20BagPack6 80.00 69.56?88.11 Human10 64.50 53.01?74.88DV 76.87 66.10?85.57 LSA-9710 64.38 52.90?74.80TypeDM 76.87 66.10?85.57 Random 25.00 15.99?35.94PairClass7 76.25 65.42?85.05Model sources: 1Rapp (2003); 2Matveeva et al (2005); 3Bullinaria and Levy (2007); 4Ruiz-Casado,Alfonseca, and Castells (2005); 5Terra and Clarke (2003); 6Herdag?delen and Baroni (2009); 7Turney(2008); 8Turney (2001); 9Pado?
and Lapata (2007); 10Landauer and Dumais (1997).according to a Fisher test, the difference between the second-best model, GLSA, and thetwelfthmodel, PMI-IR-01, is not significant at the?
= .05 level (p= 0.07).
The differencebetween the bottom model, LSA-97, and random guessing is, on the other hand, highlysignificant (p < .00001).The best DM model is again TypeDM, which also outperforms Turney?s unifiedPairClass approach (supervised, and relying on a much larger corpus), as well as hisWeb-statistics based PMI-IR-01 model.
TypeDM does better than the best Pado?
andLapata model (DV-07), and comparably to our DV implementation.
Its accuracy is morethan 10% higher than the average human test taker and the classic LSAmodel (LSA-97).Among the approaches that outperform TypeDM, BagPack is supervised, and CWOand PMI-IR-03 rely on much larger corpora.
This leaves us with three unsupervised(and unstructured) models from the literature that outperform TypeDM while beingtrained on comparable or smaller corpora: LSA-03, GLSA, and PPMIC.
In all threecases, the authors show that parameter tuning is beneficial in attaining the reportedbest performance.
Further work should investigate how we could improve TypeDM byexploring various parameter settings (many of which do not require going back to thecorpus: feature selection and reweighting, SVD, etc.
).6.1.3 Noun Categorization.
Humans are able to group words into classes or categoriesdepending on their meaning similarities.
Categorization tasks play a prominent rolein cognitive research on concepts and meaning, as a probe into the semantic organiza-tion of the lexicon and the ability to arrange concepts hierarchically into taxonomies(Murphy 2002).
Research in corpus-based semantics has always been interested ininvestigating whether distributional (attributional) similarity could be used to groupwords into semantically coherent categories.
From the computational point of view, thisis a particularly crucial issue because it concerns the possibility of using distributionalinformation to assign a semantic class or type to words.
Categorization requires (at leastin current settings) a discrete decision, as in the TOEFL task, but it is based on detectingnot only synonyms but also less strictly related words that stand in a coordinate/co-hyponym relation.
We focus here on noun categorization, which we operationalize asa clustering task.
Distributional categorization has been investigated for other POS aswell, most notably verbs (Merlo and Stevenson 2001; Schulte imWalde 2006).
However,694Baroni and Lenci Distributional Memoryverb classifications are notoriously more controversial than nominal ones, and deeplyinteract with argument structure properties.
Some experiments on verb classificationwill be carried out in theW1L?W2 space in Section 6.3.Because the task of clustering concepts/words into superordinates has recentlyattracted much attention, we have three relevant data sets from the literature availablefor our tests.
The Almuhareb?Poesio (AP) set includes 402 concepts from WordNet,balanced in terms of frequency and ambiguity.
The concepts must be clustered into21 classes, each selected from one of the 21 uniqueWordNet beginners, and representedby between 13 and 21 nouns.
Examples include the vehicle class (helicopter,motorcycle.
.
.
),the motivation class (ethics, incitement, .
.
.
), and the social unit class (platoon, branch).
SeeAlmuhareb (2006) for the full set.The Battig test set introduced by Baroni et al (2010) is based on the expandedBattig and Montague norms of Van Overschelde, Rawson, and Dunlosky (2004).
Theset comprises 83 concepts from 10 common concrete categories (up to 10 concepts perclass), with the concepts selected so that they are rated as highly prototypical of theclass.
Class examples include land mammals (dog, elephant.
.
.
), tools (screwdriver, hammer)and fruit (orange, plum).
See Baroni et al (2010) for the full list.Finally, the ESSLLI 2008 set was used for one of the Distributional Semantic Work-shop shared tasks (Baroni, Evert, and Lenci 2008).
It is also based on concrete nouns,but it includes fewer prototypical members of categories (rocket as vehicle or snail asland animal).
The 44 target concepts are organized into a hierarchy of classes of in-creasing abstraction.
There are 6 lower level classes, with maximally 13 concepts perclass (birds, land animals, fruit, greens, tools, vehicles).
At a middle level, concepts aregrouped into three classes (animals, vegetables, and artifacts).
At the most abstract level,there is a two-way distinction between living beings and objects.
See http://wordspace.collocations.de for the full set.We cluster the nouns in each set by computing their similarity matrix based onpairwise cosines, and feeding it to the widely used CLUTO toolkit (Karypis 2003).
Weuse CLUTO?s built-in repeated bisections with global optimization method, acceptingall of CLUTO?s default values for this method.Cluster quality is evaluated by percentage purity, one of the standard clusteringquality measures returned by CLUTO (Zhao and Karypis 2003).
If nir is the number ofitems from the i-th true (gold standard) class that were assigned to the r-th cluster, n thetotal number of items, and k the number of clusters, thenPurity = 1nk?r=1maxi(nir)Expressed in words, for each cluster we count the number of items that belong to thetrue class that is most represented in the cluster, and then we sum these counts acrossclusters.
The resulting sum is divided by the total number of items so that, in the bestcase (perfect clusters), purity will be 1 (in percentage terms, 100%).
As cluster quality de-teriorates, purity approaches 0.
For the models where we have full access to the results,we use a heuristic bootstrap procedure to obtain confidence intervals around the puri-ties (Efron and Tibshirani 1994).
We resample with replacement 10K data sets (cluster-assignment+true-label pairs) of the original size.
Empirical 95% confidence intervals arethen computed from the distribution of the purities in the bootstrapped data sets (forthe ESSLLI results, we only perform the procedure for 6-way clustering).
The confidenceintervals give a rough idea of how stable purity estimates are across small variations of695Computational Linguistics Volume 36, Number 4the items in the data sets.
The Random models for this task are baselines assigningthe concepts randomly to the target clusters, with the constraint that each cluster mustcontain at least one concept.
Random assignment is repeated 10K times, and we obtainmeans and confidence intervals from the distribution of these simulations.Table 6 reports purity results for the three data sets, comparing our models tothose in the literature.
Again, the TypeDM model has an excellent performance.
Onthe ESSLLI 2008 set, it outperforms the best configuration of the best shared task systemamong those that did three-level categorization (Katrenko?s), despite the fact that thelatter uses the full Web as a corpus and manually crafted patterns to improve featureextraction.
TypeDM?s performance is equally impressive on the AP set, where it outper-forms AttrValue-05, the best unsupervised model by the data set proponents, trainedon the full Web.
Interestingly, the DepPath model of Rothenha?usler and Schu?tze (2009),which is the only one outperforming TypeDM on the AP set, is another structuredmodel with dependency-based link-mediated features, which would fit well into theTable 6Purity in noun clustering with bootstrapped 95% confidence intervals (CI).Almuhareb & Poesio (AP)model purity 95% CI model purity 95% CIDepPath1 79 NA DV 65 61?69TypeDM 76 72?81 DepDM 62 59?67AttrValue-052 71 NA LexDM 59 56?65Win 71 67?76 Random 16 14?17VSM3 70 67?75Battigmodel purity 95% CI model purity 95% CIWin 96 91?100 DV-104 79 73?89TypeDM 94 89?99 LexDM 78 72?88Strudel4 91 85?98 SVD-104 71 67?83DepDM 90 84?96 AttrValue4 45 44?61DV 84 79?93 Random 29 24?34ESSLLI 2008model 6-way purity 95% CI 3-way purity 2-way purity avg purityTypeDM 84 77?95 98 100 94.0Katrenko5 91 NA 100 80 90.3DV 75 70?89 93 100 89.3DepDM 75 68?89 93 100 89.3LexDM 75 70?89 87 100 87.3Peirsman5 82 NA 84 86 84.0Win 75 70?89 86 59 73.3Shaoul5 41 NA 52 55 49.3Random 38 32?45 49 57 48.0Model sources: 1Rothenha?usler and Schu?tze (2009); 2Almuhareb and Poesio (2005); 3Herdag?delen,Erk, and Baroni (2009); 4Baroni et al (2010); 5ESSLLI 2008 shared task.696Baroni and Lenci Distributional MemoryDM framework.
TypeDM?s purity is extremely high with the Battig set as well, althoughhere it is outperformed by the unstructured Win model.
Our top two performances arehigher than Strudel, the best model by the proponents of the task.
The latter was trainedon about half of the data we used, however (moreover, the confidence intervals of thesemodels largely overlap, suggesting that their difference is not significant).6.1.4 Selectional Preferences.
Our last pair of data sets for the W1?LW2 space illustratehow the space can be used not only to measure similarity among words, but also towork with more abstract notions, such as that of a typical filler of an argument slot of averb (such as the typical killer and the typical killee).
We think that these are especiallyimportant experiments, because they show how the same matrix that has been used fortasks that were entirely bound to lexical items can also be used to generalize to struc-tures that go beyond what is directly observed in the corpus.
In particular, we modelhere selectional preferences (how plausible a noun is as subject/object of a verb), butour method is generalizable to many other semantic tasks that pertain to compositionconstraints; that is, they require measuring the goodness of fit of a word/concept asargument filler of another word/concept, including assigning semantic roles, logicalmetonymy, coercion (Pustejovsky 1995), and many other challenges.The selectional preference test sets are based on averages of human judgmentson a seven-point scale about the plausibility of nouns as arguments (either subjectsor objects) of verbs.
The McRae data set (McRae, Spivey-Knowlton, and Tanenhaus1998) consists of 100 noun?verb pairs rated by 36 subjects.
The Pado?
set (Pado?
2007)has 211 pairs rated by 20 subjects.For each verb, we first use the W1?LW2 space to select a set of nouns that arehighly associated with the verb via a subject or an object link.
In this space, nouns arerepresented as vectors with dimensions that are labeled with ?link, word?
tuples, wherethe word might be a verb, and the link might stand for, among other things, syntacticrelations such as obj (or, in the LexDMmodel, an expansion thereof, such as obj+the-j).
Tofind nouns that are highly associated with a verb v when linked by the subject relation,we project theW1?LW2 vectors onto a subspace where all dimensions are mapped to 0except the dimensions that are labeled with ?lsbj, v?, where lsbj is a link containing eitherthe string sbj intr or the string sbj tr, and v is the verb.
We then measure the length of thenoun vectors in this subspace, and pick the top n longest ones as prototypical subjectsof the verb.
The same operation is performed for the object relation.
In our experiments,we set n to 20, but this is of course a parameter that should be explored.We normalize and sum the vectors (in the fullW1?LW2 space) of the picked nouns,to obtain a centroid that represents an abstract ?subject prototype?
for the verb (andanalogously for objects).
The plausibility of an arbitrary noun as the subject (object) of averb is then measured by the cosine of the noun vector to the subject (object) centroid inW1?LW2 space.
Crucially, the algorithm can provide plausibility scores for nouns thatdo not co-occur with the target verb in the corpus, by looking at how close they areto the centroid of nouns that do often co-occur with the verb.
The corpus may containneither eat topinambur nor eat sympathy, but the topinambur vector will likely be closer tothe prototypical eat object vector than the one of sympathy would be.It is worth stressing that the whole process relies on a single W1?LW2 matrix: Thisspace is first used to identify typical subjects (or objects) of a verb via subspacing, then toconstruct centroid vectors for the verb subject (object) prototypes, and finally tomeasurethe distance of nouns to these centroids.
Our method is essentially the same, save forimplementation and parameter choice details, as the one proposed by Pado?, Pado?,and Erk (2007), in turn inspired by Erk (2007).
However, they treat the identification697Computational Linguistics Volume 36, Number 4of typical argument fillers of a verb as an operation to be carried out using differentresources, whereas we reinterpret it as a different way to use the sameW1?LW2 space inwhich we measure plausibility.Following Pado?
and colleagues, we measure performance by the Spearman ?
corre-lation coefficient between the average human ratings and the model predictions, con-sidering only verb?noun pairs that are present in the model.
Table 7 reports percentagecoverage and correlations for the DM models (the task requires the links to extracttypical subjects and objects, so we cannot use DV nor Win), results from Pado?, Pado?,and Erk (2007) (ParCos is the best among their purely corpus-based systems), and theperformance on the Pado?
data set of the supervised system of Herdag?delen and Baroni(2009).
Testing for significance of the correlation coefficients with two-tailed tests basedon a Spearman-coefficient derived t statistic, we find that the Resnik?s model correlationfor the McRae data is not significantly different from 0 (t = 0.29, df = 92, p = 0.39),ParCos on McRae is significant at ?
= .05 (t = 2.134, df = 89, p = 0.018), and all othermodels on either data set are significant at ?
= .01 and below.TypeDM emerges as an excellent model to tackle selectional preferences, and as theoverall winner on this task.
On the Pado?
data set, it is as good as Pado?
?s (2007) FrameNetbased model, and it is outperformed only by the supervised BagPack approach.
On theMcRae data set, all three DM models do very well, and TypeDM is slightly worse thanthe other two models.
On this data set, the DM models are outperformed by Pado?
?sFrameNet model in terms of correlation, but the latter has a much lower coverage,suggesting that for practical purposes the DM models are a better choice.
According toRaghunathan?s test (see Section 6.1.1), the difference in correlation with human ratingsamong the three DM models is not significant on the McRae data, where TypeDM isbelow the other models (Q = 0.19, df = 0.67, p = 0.50).
On the Pado?
data set, on theother hand, where TypeDM outperforms the other DM models, the same difference ishighly significant (Q = 12.70, df = 1.00, p < 0.001).As a final remark on the W1?LW2 space, we can notice that DM models performvery well in tasks involving attributional similarity.
The performance of unstructuredDSMs (including Win, our own implementation of this type of model) is also high,sometimes even better than that of structured DSMs.
However, our best DMmodel alsoachieves brilliant results in capturing selectional preferences, a task that is not directlyaddressable by unstructured DSMs.
This fact suggests that the real advantage providedby structured DSMs?particularly when linguistic structure is suitably exploited, asTable 7Correlation with verb?argument plausibility judgments.McRae Pado?model coverage ?
model coverage ?Pado?1 56 41 BagPack2 100 60DepDM 97 32 TypeDM 100 51LexDM 97 29 Pado?1 97 51TypeDM 97 28 ParCos1 98 48ParCos1 91 22 DepDM 100 35Resnik1 94 3 LexDM 100 34Resnik1 98 24Model sources: 1Pado?, Pado?, and Erk (2007); 2Herdag?delen and Baroni (2009).698Baroni and Lenci Distributional Memorywith the DM third-order tensor?actually resides in their versatility in addressing amuch larger and various range of semantic tasks.
This preliminary conclusion will alsobe confirmed by the experiments modeled with the other DM spaces.6.2 The W1W2?L SpaceThe vectors of this space are labeled with word pair tuples ?w1,w2?
(columns of matrixBmode-2 in Table 3) and their dimensions are labeled with links l (rows of the samematrix).
This arrangement of our tensor reproduces the ?relational similarity?
space ofTurney (2006b), also implicitly assumed in much relation extraction work, where wordpairs are compared based on the patterns that link them in the corpus, in order to mea-sure the similarity of their relations (Pantel and Pennacchiotti 2006).
The links that inW1?LW2 space provide a form of shallow typing of lexical features (?use, gun?)
associ-ated with single words (soldier) constitute under theW1W2?L view full features (use) as-sociated with word pairs (?soldier, gun?).
Besides exploiting this view of the tensor tosolve classic relational tasks, we will also show how problems that have not been tradi-tionally defined in terms of a word-pair-by-link matrix, such as qualia harvesting withpatterns or generating lists of characteristic properties, can be elegantly recast in theW1W2?L space by measuring the length of ?w1,w2?
vectors in a link (sub)space, thusbringing a wider range of semantic operations under the umbrella of the natural DMspaces.TheW1W2?L space represents pairs of words that co-occur in the corpus within themaximum span determined by the scope of the links connecting them (for our models,this maximum span is never larger than a single sentence).
When words do not co-occuror only co-occur very rarely (and even in large corpora this will often be the case), attri-butional similarity can come to the rescue.
Given a target pair, we can construct other,probably similar pairs by replacing one of the words with an attributional neighbor.
Forexample, given the pair ?automobile, wheel?, we might discover inW1?LW2 space that caris a close neighbor of automobile.
We can then look for the pair ?car, wheel?, and use rela-tional evidence about this pair as if it pertained to ?automobile, wheel?.
This is essentiallythe way to deal withW1W2?L data sparseness proposed by Turney (2006b), except thathe relies on independently harvested attributional and relational spaces, whereas wederive both from the same tensor.
More precisely, in theW1W2?L tasks where we knowthe set of target pairs in advance (Sections 6.2.1 and 6.2.2), we smooth the DMmodels bycombining in turn one of the words of each target pair with the top 20 nearestW1?LW2neighbors of the other word, obtaining a total of 41 pairs (including the original).
Thecentroid of the W1W2?L vectors of these pairs is then taken to represent a target pair(the smoothed ?automobile, wheel?
vector is an average of the ?automobile, wheel?, ?car,wheel?, ?automobile, circle?, etc., vectors).
The nearest neighbors are efficiently searchedin the W1?LW2 matrix by compressing it to 5,000 dimensions via random indexing,using the parameters suggested by Sahlgren (2005).
Smoothing consistently improvedperformance, and we only report the relevant results for the smoothed versions of themodels (including our implementation of LRA, to be discussed next).We reimplemented Turney?s Latent Relational Analysis (LRA) model, training it onour source corpus (LRA is trained separately for each test set, because it relies on a givenlist of word pairs to find the patterns that link them).
We chose the parameter values ofTurney?s main model (his ?baseline LRA system?).
In short (see Turney?s article for de-tails), for a given set of target pairs we count all the patterns that connect them, in eitherorder, in the corpus.
Patterns are sequences of one to three words occurring between thetargets, with all, none, or any subset of the elements replaced bywildcards (with the,with699Computational Linguistics Volume 36, Number 4Table 8Percentage accuracy in solving SAT analogies with 95% binomial confidence intervals (CI).model accuracy 95% CI model accuracy 95% CIHuman1 57.0 52.0?62.3 TypeDM 42.4 37.4?47.7LRA-062 56.1 51.0?61.2 LSA7 42.0 37.2?47.4PERT3 53.3 48.5?58.9 LRA 37.8 32.8?42.8PairClass4 52.1 46.9?57.3 PMI-IR-062 35.0 30.2?40.1VSM1 47.1 42.2?52.5 DepDM 31.4 26.6?36.2BagPack5 44.1 39.0?49.3 LexDM 29.3 24.8?34.3k-means6 44.0 39.0?49.3 Random 20.0 16.1?24.5Model sources: 1Turney and Littman (2005); 2Turney (2006b); 3Turney (2006a); 4Turney (2008);5Herdag?delen and Baroni (2009); 6Bicic?i and Yuret (2006); 7Quesada, Mangalath, and Kintsch(2004).
*, * the, * *).
Only the top 4,000 most frequent patterns are preserved, and a target-pair-by-pattern matrix is constructed (with 8,000 dimensions, to account for directionality).Values in the matrix are log- and entropy-transformed using Turney?s formula.
Finally,SVD is applied, reducing the columns to the top 300 latent dimensions (here and sub-sequently, we use SVDLIBC9 to perform SVD).
For simplicity and to make LRA moredirectly comparable to the DM models, we applied our attributional-neighbor-basedsmoothing technique (the neighbors for target pair expansion are taken from the bestattributional DM model, namely, TypeDM) instead of the more sophisticated one usedby Turney.
Thus, our LRA implementation differs from Turney?s original in two aspects:the smoothing method and the source corpus (Turney uses a corpus of more than50 billion words).
Neither variation pertains to inherent differences between LRA andDM.Given the appropriate resources, a DMmodel could be trained on Turney?s giganticcorpus, and smoothed with his technique.6.2.1 Solving Analogy Problems.
The SAT test set introduced by Turney and collaboratorscontains 374 multiple-choice questions from the SAT college entrance exam.
Each ques-tion includes one target (ostrich?bird) and five candidate analogies (lion?cat, goose?flock,ewe?sheep, cub?bear, primate?monkey).
The data set is dominated by noun?noun pairs,but all other combinations are also attested (noun?verb, verb?adjective, verb?verb, etc.
)The task is to choose the candidate pair most analogous to the target (lion?cat in theprevious example).
This is essentially the same task as the TOEFL, but applied to wordpairs instead of words.
As in the TOEFL, we pick the candidate with the highest cosinewith the target as the right analogy.Table 8 reports our SAT results together with those of other corpus-based methodsfrom the ACL Wiki and other systems.
TypeDM is again emerging as the best amongour models.
To put its performance in context statistically, according to a Fisher test itsaccuracy is not significantly different from that of VSM (p = 0.239), whereas it is betterthan that of PMI-IR-06 (p= 0.043; even the bottommodel, LexDM, is significantly betterthan the random guesser, p = 0.004).TypeDM is at least as good as LRA when the latter is trained on the same dataand smoothed with our method, suggesting that the excellent performance of Turney?sversion of LRA (LRA-06) is due to the fact that he used a much larger corpus, and/or to9 http://tedlab.mit.edu/?dr/SVDLIBC/.700Baroni and Lenci Distributional Memoryhis more sophisticated smoothing technique, and not to the specific way in which LRAcollects corpus-based statistics.
All the algorithms with higher accuracy than TypeDMare based onmuch larger input corpora, except BagPack, which is, however, supervised.The LSA system of Quesada, Mangalath, and Kintsch (2004), which performs similarlyto TypeDM, is based on a smaller corpus, but it relies on hand-coded ?analogy domains?that are represented by lists of manually selected characteristic words.6.2.2 Relation Classification.
Just as the SAT is the relational equivalent of the TOEFL task,the test sets we tackle next are a relational analog to attributional concept clustering,in that they require grouping pairs of words into classes that instantiate the samerelations.
Whereas we cast attributional categorization as an unsupervised clusteringproblem (following much of the earlier literature), the common approach to classify-ing word pairs by relation is supervised, and relies on labeled examples for training.
Inthis article, we exploit training data in a very simple way, via a nearest centroid method.In the SEMEVAL task we are about to introduce, where both positive and negativeexamples are available for each class, we use the positive examples to construct acentroid that represents a target class, and negative examples to construct a centroid rep-resenting items outside the class.
We then decide if a test pair belongs to the target classby measuring its distance from the positive and negative centroids, picking the nearestone.
For example, the Cause?Effect relation has positive training examples such ascycling?happiness and massage?relief and negative examples such as customer?satisfactionand exposure?protection.
We create a positive centroid by summing theW1W2?L vectorsof the first set of pairs, and a negative centroid by summing the latter.
We then mea-sure the cosine of a test item such as smile?wrinkle with the centroids, and decideif it instantiates the Cause?Effect relation based on whether it is closer to the positiveor negative centroid.
For the other tasks (as well as the transitive alternation task ofSection 6.3), we do not have negative examples, but positive examples for differentclasses.
We create a centroid for each class, and classify test items based on the centroidthey are nearest to.Our first test pertains to the seven relations between nominals in Task 4 ofSEMEVAL 2007 (Girju et al 2007): Cause?Effect, Instrument?Agency, Product?Producer, Origin?Entity, Theme?Tool, Part?Whole, Content?Container.
For each rela-tion, the data set includes 140 training and about 80 test items.
Each item consists of aWeb snippet, containing word pairs connected by a certain pattern (e.g., ?
* causes *?
).The retrieved snippets are manually classified by the SEMEVAL organizers as positiveor negative instances of a certain relation (see the earlier Cause?Effect examples).
About50% training and test cases are positive instances.
In our experiments we do not makeuse of the contexts of the target word pairs that are provided with the test set.The second data set (NS) comes from Nastase and Szpakowicz (2003).
It pertains tothe classification of 600 modifier?noun pairs and it is of interest because it proposes avery fine-grained categorization into 30 semantic classes, such as Cause (cloud?storm),Purpose (album?picture), Location-At (pain?chest), Location-From (visitor?country), Fre-quency (superstition?occasional), Time-At (snack?midnight), and so on.
The modifiers canbe nouns, adjectives, or adverbs.
Because the data set is not split into training and testdata we follow Turney (2006b) and perform leave-one-out cross-validation.
The data setalso comes with a coarser five-way classification.
Our unreported results on it are com-parable, in terms of relative performance, to the ones for the 30-way classification.The last data set (OC) contains 1,443 noun?noun compounds classified by O?Se?aghdha and Copestake (2009) into 6 relations: Be (celebrity?winner), Have (door?latch),In (air?disaster), Actor (university?scholarship), Instrument ( freight?train), and About701Computational Linguistics Volume 36, Number 4(bank?panic); see O?
Se?aghdha and Copestake (2009) and references there.
We use thesame five-way cross-validation splits as the data set proponents.Table 9 reports performance of models from our experiments and from the literatureon the three supervised relation classification tasks.
Following the relevant earlier stud-ies, for SEMEVAL we report macro-averaged accuracy, whereas for the other two datasets we report global accuracy (with binomial confidence intervals).
All other measuresare macro-averaged.
Majority is the performance of a classifier that always guesses theTable 9Relation classification performance; all measures macro-averaged, except accuracy in the NS andOC data sets, where we also report the accuracy 95% confidence intervals (CI).SEMEVAL 2007model prec recall F acc model prec recall F accTypeDM 71.7 62.5 66.4 70.2 DepDM 61.0 57.3 58.9 61.8UCD-FC1 66.1 66.7 64.8 66.0 UTH1 56.1 57.1 55.9 58.8UCB1 62.7 63.0 62.7 65.4 Majority 81.3 42.9 30.8 57.0LexDM 64.7 61.3 62.5 65.4 ProbMatch 48.5 48.5 48.5 51.7ILK1 60.5 69.5 63.8 63.5 UC3M1 48.2 40.3 43.1 49.9LRA 63.7 60.0 61.0 63.1 AllTrue 48.5 100.0 64.8 48.5UMELB-B1 61.5 55.7 57.8 62.7Nastase & Szpakowicz (NS)model prec recall F acc acc 95% CILRA-062 41.0 35.9 36.6 39.8 35.9?43.9VSM-AV3 27.9 26.8 26.5 27.8 24.3?31.6LRA 23.0 23.1 21.1 25.5 22.1?29.2VSM-WMTS2 24.0 20.9 20.3 24.7 21.3?28.3TypeDM 19.5 20.2 13.7 15.4 12.5?18-5LexDM 7.5 14.1 8.1 12.1 9.7?15.0DepDM 11.6 14.5 8.1 8.7 6.5?11.2Majority 0.3 3.3 0.5 8.2 6.1?10.6ProbMatch 3.3 3.3 3.3 4.7 3.1?6.7AllTrue 3.3 100 6.4 NA NAO?
Se?aghdha & Copestake (OC)model prec recall F acc acc 95% CIOC-Comb4 NA NA 61.6 63.1 60.6?65.6OC-Rel4 NA NA 49.9 52.1 49.5?54.7TypeDM 33.8 33.5 31.4 32.1 29.7?34.6LRA 31.5 30.8 30.7 31.3 28.9?33.8LexDM 29.9 28.9 28.7 29.7 27.4?32.2DepDM 28.2 28.2 27.0 27.6 25.3?30.0Majority 3.6 16.7 5.9 21.3 19.2?23.5ProbMatch 16.7 16.7 16.7 17.1 15.2?19.2AllTrue 16.7 100 28.5 NA NAModel sources: 1SEMEVAL Task 4; 2Turney (2006b); 3Turney and Littman (2005); 4O?
Se?aghdhaand Copestake (2009).702Baroni and Lenci Distributional Memorymajority class in the test set (in SEMEVAL, for each class, it guesses that all or no itemsbelong to it depending on whether there are more positive or negative examples in thetest data; in the other tasks, it labels all items with the majority class).
AllTrue alwaysassigns an item to the target class (being inherently binary, it does not provide a well-defined multi-class global accuracy).
ProbMatch randomly guesses classes matchingtheir distribution in the test data (in SEMEVAL, it matches the proportion of positiveand negative examples within each class).For SEMEVAL, the table reports the results of those models that took part in theshared task and, like ours, did not use the organizer-provided WordNet sense labelsnor information about the query used to retrieve the examples.
All these models areoutperformed by TypeDM, despite the fact that they exploit the training contextsand/or specific additional resources: an annotated compound database (UCD-FC),more sophisticated machine learning algorithms to train the relation classifiers (ILK,UCD-FC), Web counts (UCB), and so on.For the NS data set, none of the DM models do well, although TypeDM is oncemore the best among them.
The DM models are outperformed by other models fromthe literature, all trained on much larger corpora, and also by our implementation ofLRA.
The difference in global accuracy between LRA and TypeDM is significant (Fishertest, p = 0.00002).
TypeDM?s accuracy is nevertheless well above the best (Majority)baseline accuracy (p = 0.0001).The OC results confirm that TypeDM is the best of our models, again (slightly)outperforming our LRA implementation.
Still, our best performance is well belowthat of OC-Comb, the absolute best, and OC-Rel, the best purely relational modelof O?
Se?aghdha and Copestake (2009) (the difference in global accuracy between thelatter and TypeDM is highly significant, p < 0.000001).
O?
Se?aghdha and Copestake usesophisticated kernel-based methods and extensive parameter tuning to achieve theseresults.
We hope that the TypeDM performance would also improve by improving themachine learning aspects of the procedure.As an ad interim summary, we observe that TypeDM achieves competitive results insemantic tasks involving relational similarity.
In particular, in both analogy solving andtwo out of three relation classification experiments, TypeDM is at least as good as ourLRA implementation.
We now move on to show how this same view of the DM tensorcan be successfully applied to aspects of meaning that are not normally addressed byrelational DSMs.6.2.3 Qualia Extraction.
A popular alternative to the supervised approach to relationextraction is to pick a set of lexico-syntactic patterns that should capture the relationof interest and to harvest pairs they connect in text, as famously illustrated by Hearst(1992) for the hyponymy relation.
In the DM approach, instead of going back to thecorpus to harvest the patterns, we exploit the information already available in theW1W2?L space.
We select promising links as our equivalent of patterns and we measurethe length of word pair vectors in the W1W2?L subspace defined by these links.
Weillustrate this with the data set of Cimiano andWenderoth (2007), which contains qualiastructures (Pustejovsky 1995) for 30 nominal concepts, both concrete (door) and abstract(imagination).
Cimiano and Wenderoth asked 30 subjects to produce qualia for thesewords (each word was rated by at least three subjects), obtaining a total of 1,487 word?quale pairs, instantiating the four roles postulated by Pustejovsky: Formal (the categoryof the object: door?barrier), Constitutive (constitutive parts, materials the object is madeof: food?fat), Agentive (what brings the object about: letter?write), and Telic (the func-tion of the object: novel?entertain).703Computational Linguistics Volume 36, Number 4We approximate the patterns proposed by Cimiano and Wenderoth by manuallyselecting links that are already in our DM models, as reported in Table 10 (here andsubsequently when discussing qualia-harvesting links, we use n and q to indicate thelinear position of the noun and the potential quale with respect to the link).
All qualiaroles have links pertaining to noun?noun pairs.
The Agentive and Telic patterns alsoharvest noun?verb pairs.
For LexDM, we pick all links that begin with one of the stringsin Table 10.
For the DepDM model, the only attested links are n with q (Constitutive),n sbj intr q, n sbj tr q (Telic), and q obj n (Agentive).
Consequently, DepDM does notharvest Formal qualia, and is penalized accordingly in the evaluation.We project all W1W2?L vectors that contain a target noun onto each of the foursubspaces determined by the quale-specific link sets, and we compute their subspacelengths.
Given a target noun n and a potential quale q, the length of the ?n, q?
vectorin the subspace characterized by the links that represent role r is our measure of howgood q is as a quale of type r for n (for example, the length of ?book, read?
in the subspacedefined by the Telic links is our measure of fitness of read as Telic role of book).
We uselength in the subspace associated to the qualia role r to rank all ?n, q?
pairs relevant to r.Following Cimiano and Wenderoth?s evaluation method, for each noun we firstcompute, separately for each role, the ranked list precision (with respect to themanuallyconstructed qualia structure) at 11 equally spaced recall levels from 0% to 100%.
Weselect the precision, recall, and F values at the recall level that results in the highestF score (i.e., in the best precision?recall trade-off).
We then average across the roles, andthen across target nouns.
The task, as framed here, cannot be run with the LRA model,and, because of its open-ended nature (we do not start from a predefined list of pairs),we do not smooth the models.Table 11 reports the performance of our models, as well as the F scores reported byCimiano and Wenderoth.
For our models, where we have access to the itemized data,we also report the standard deviation of F across the target nouns.All the DM models perform well (including DepDM, which is disfavored by thelack of Formal links), and once more TypeDM emerges as the best among them, withan F value that is also (slightly) above the best Cimiano and Wenderoth models (thatare based on co-occurrence counts from the whole Web).
Despite the large standarddeviations, the difference in F across concepts between TypeDM and the second-bestDM model (DepDM) is highly significant (paired t-test, t = 4.02, df = 29, p < 0.001),suggesting that the large variance is due to different degrees of difficulty of the concepts,affecting the models in similar ways.Table 10Links approximating the patterns proposed in Cimiano and Wenderoth (2007).FORMAL CONSTITUTIVEn as-form-of q, q as-form-of n q as-member-of n, q as-part-of n, nwith qn as-kind-of q, n as-sort-of q, n be q nwith-lot-of q, nwith-majority-of qq such as n nwith-number-of q, nwith-sort-of qnwith-variety-of qAGENTIVE TELICn as-result-of q, q obj n n for-use-as q, n for-use-in q, n sbj tr qn sbj intr q704Baroni and Lenci Distributional MemoryTable 11Average qualia extraction performance.model precision recall F F s.d.TypeDM 26.2 22.7 18.4 8.7P1 NA NA 17.1 NAWebP1 NA NA 16.7 NALexDM 19.9 23.6 16.2 7.1WebJac1 NA NA 15.2 NADepDM 17.8 16.9 12.8 6.4Verb-PMI1 NA NA 10.7 NABase1 NA NA 7.6 NAModel source: 1Cimiano and Wenderoth (2007).6.2.4 Predicting Characteristic Properties.
Recently, there has been some interest in theautomated generation of commonsense concept descriptions in terms of intuitivelysalient properties: a dog is a mammal, it barks, it has a tail, and so forth (Almuhareb2006; Baroni and Lenci 2008; Baroni, Evert, and Lenci 2008; Baroni et al 2010).
Similarproperty lists, collected from subjects in elicitation tasks, are widely used in cognitivescience as surrogates of mental features (Garrard et al 2001; McRae et al 2005; Vinsonand Vigliocco 2008).
Large-scale collections of property-based concept descriptions arealso carried out in AI, where they are important for commonsense reasoning (Liu andSingh 2004).In the qualia task, given a concept we had to extract properties of certain kinds (cor-responding to the qualia roles).
The property-based description task is less constrained,because the most salient relations of a nominal concept might be in all sorts of relationswith it (parts, typical behaviors, location, etc.).
Still, we couch the task of unconstrainedproperty extraction as a challenge in theW1W2?L space.
The approach is similar to themethod adopted for qualia roles, but now the wholeW1W2?L space is used, instead ofselected subspaces.
Given all the ?n,w2?
pairs that have the target nominal concept asfirst element, we rank them by length in theW1W2?L space.
The longest ?n,w2?
vectorsin this space should correspond to salient properties of the target concept, as we expecta concept to often co-occur in texts with its important properties (because in the currentDM implementations links are disjoint across POS, we map properties with differentPOS onto the same scale by dividing the length of the vector representing a pair by thelength of the longest vector in the harvested concept?property set that has the same POSpair).
For example, among the longestW1W2?L vectors with car as first itemwe find ?car,drive?, ?car, park?, and ?car, engine?.
The first two pairs are normalized by dividing by thelongest ?noun, verb?
vector in the harvested set, the third by dividing by the longest?noun, noun?
vector.We test this approach in the ESSLLI 2008 Distributional Semantic Workshop un-constrained property generation challenge (Baroni, Evert, and Lenci 2008).
The data setcontains, for each of 44 concrete concepts, 10 properties that are those that were mostfrequently produced by subjects in the elicitation experiment of McRae et al (2005) (the?gold standard lists?).
Algorithms must generate lists of 10 properties per concept, andperformance is measured by overlap with the subject-produced properties, that is, bythe cross-concept average proportions of properties in the generated lists that are alsoin the corresponding gold standard lists.
Smoothing would be very costly (we wouldneed to smooth all pairs that contain a target concept) and probably counterproductive705Computational Linguistics Volume 36, Number 4(as the most typical properties of a concept should be highly specific to it, rather thanshared with neighbors).
Because LRA (at least in a reasonably efficient implementation)requires a priori specification of the target pairs, it is not well suited to this task.Table 12 reports the percentage overlap with the gold standard properties (averagedacross the 44 concepts) for our models as well as the only ESSLLI 2008 participant thattried this task, and for the models of Baroni et al (2010).
TypeDM is the best DMmodel,and it also does quite well compared to the state of the art.
The difference betweenStrudel, the best model from the earlier literature, and TypeDM is not statistically signif-icant, according to a paired t-test across the target concepts (t = 1.1, df = 43, p = 0.27).The difference between TypeDM and DV-10, the second best model from the literature,is highly significant (t = 2.9, df = 43, p < 0.01).
If we consider how difficult this sortof open-ended task is (see the very low performance of the respectable models at thebottom of the list), matching on average two out of ten speaker-generated properties, asTypeDM does, is an impressive feat.6.3 The W1L?W2 SpaceThe vectors of this space are labeled with binary tuples of type ?w1, l?
(columns ofmatrix Cmode-3 in Table 3), and their dimensions are labeled with words w2 (rows of thesamematrix).
We illustrate this space in the task of discriminating verbs participating indifferent argument alternations.
However, other uses of the space can also be foreseen.For example, the rows of W1L?W2 correspond to the columns of the W1?LW2 space(given the constraints on the tuple structure we adopted in Section 3.1).
We could usethe former space for feature smoothing or selection in the latter space, for example, bymerging the features ofW1?LW2 whose corresponding vectors inW1L?W2 have a cosinesimilarity over a given threshold.
We leave this possibility to further work.Among the linguistic objects represented by the W1L?W2 vectors, we find thesyntactic slots of verb frames.
For instance, the vector labeled with the tuple ?read,sbj?1?
represents the subject slot of the verb read in terms of the distribution of itsnoun fillers, which label the dimensions of the space.
We can use theW1L?W2 space toexplore the semantic properties of syntactic frames, and to extract generalizations aboutthe inner structure of lexico-semantic representations of the sort formal semanticistshave traditionally been interested in.
For instance, the high similarity between theobject slot of kill and the subject slot of die might provide a distributional correlate tothe classic cause(subj,die(obj)) analysis of killing by Dowty (1977) and many others.Measuring the cosine between the vectors of different syntactic slots of the sameverb corresponds to estimating the amount of fillers they share.
Measures of ?slotoverlap?
have been used by Joanis, Stevenson, and James (2008) as features to classifyverbs on the basis of their argument alternations.
Levin and Rappaport-Hovav (2005)Table 12Average percentage overlap with subject-generated properties and standard deviation.model overlap s.d.
model overlap s.d.
model overlap s.d.Strudel1 23.9 11.3 LexDM 14.5 12.1 SVD-101 4.1 6.1TypeDM 19.5 12.4 DV-101 14.1 10.3 Shaoul2 1.8 3.9DepDM 16.1 12.6 AttrValue1 8.8 9.9Model sources: 1Baroni et al (2010); 2ESSLLI 2008 shared task.706Baroni and Lenci Distributional Memorydefine argument alternations as the possibility for verbs to have multiple syntacticrealizations of their semantic argument structure.
Alternations involve the expressionof the same semantic argument in two different syntactic slots.
We expect that, if averb undergoes a particular alternation, then the set of nouns that appear in the twoalternating slots should overlap to a certain degree.Argument alternations represent a key aspect of the complex constraints that shapethe syntax?semantics interface.
Verbs differ with respect to the possible alternationsthey can undergo, and this variation is strongly dependent on their semantic proper-ties (semantic roles, event type, etc.).
Levin (1993) has in fact proposed a well-knownclassification of verbs based on their range of syntactic alternations.
Recognizing thealternations licensed by a verb is extremely important in capturing its argument struc-ture properties, and consequently in describing its semantic behavior.
We focus here ona particular class of alternations, namely transitivity alternations, whose verbs allowboth for a transitive NP V NP variant and for an intransitive NP V (PP) variant (Levin1993).
We use the W1L?W2 space to carry out the automatic classification of verbs thatparticipate in different types of transitivity alternations.In the causative/inchoative alternation, the object argument (e.g., John broke the vase)can also be realized as an intransitive subject (e.g., The vase broke).
In a first experiment,we use the W1L?W2 space to discriminate between transitive verbs undergoing thecausative/inchoative alternation (C/I) (e.g., break) and non-alternating ones (e.g.,mince;cf.
John minced the meat vs. *The meat minced).
The C/I data set was introduced byBaroni and Lenci (2009), but not tested in a classification task there.
It consists of 232causative/inchoative verbs and 170 non-alternating transitive verbs from Levin (1993).In a second experiment, we apply the W1L?W2 space to discriminate verbs thatbelong to three different classes, each corresponding to a different type of transitivealternation.
We use the MS data set (Merlo and Stevenson 2001), which includes 19 un-ergative verbs undergoing the induced action alternation (e.g., race), 19 unaccusativeverbs that undergo the causative/inchoative alternation (e.g., break), and 20 object-dropverbs participating in the unexpressed object alternation (e.g., play).
See Levin (1993)for details about each of these transitive alternations.
The complexity of this task isdue to the fact that the verbs in the three classes have both transitive and intransitivevariants, but with very different semantic roles.
For instance, the transitive subject ofunaccusative (The man broke the vase) and unergative verbs (The jockey raced the horse pastthe barn) is an agent of causation, whereas the subject of the intransitive variant of un-accusative verbs has a theme role (i.e., undergoes a change of state: The vase broke), andthe intransitive subject of unergative verbs has instead an agent role (The horse raced pastthe barn).
Thus, their surface identity notwithstanding, the semantic properties of thesyntactic slots of the verbs in each class are very different.
By testing theW1L?W2 spaceon such a task we can therefore evaluate its ability to capture non-trivial properties ofthe verb?s thematic structure.We address these tasks by measuring the similarities between theW1L?W2 vectorsof the transitive subject, intransitive subject, and direct object slots of a verb, and usingthese inter-slot similarities to classify the verb.
For instance, given the definition ofthe C/I alternation, we can predict that with alternating verbs the intransitive subjectslot should be similar to the direct object slot (the things that are broken also break),while this should not hold for non-alternating verbs (mincees are very different frommincers).
For each verb v in a data set, we extract the corresponding W1L?W2 slotvectors ?v, l?
whose links are sbj intr, sbj tr, and obj (for LexDM, we sum the vectorswith links beginning with one of these three patterns).
Then, for each v we build athree-dimensional vector with the cosines between the three slot vectors.
These second707Computational Linguistics Volume 36, Number 4order vectors encode the profile of similarity across the slots of a verb, and can be used tospot verbs that have comparable profiles (e.g., verbs that have a high similarity betweentheir subj intr and obj slots).We model both experiments as classification tasks using the nearest centroidmethod on the three-dimensional vectors, with leave-one-out cross-validation.
We per-form binary classification of the C/I data set (treating non-alternating verbs as negativeexamples), and three-way classification of the MS data.
Table 13 reports the results, withthe baselines computed similarly to the ones in Section 6.2.2 (for C/I, Majority is equiv-alent to AllTrue).
The DM performance is also compared with the results of Merlo andStevenson (2001) for their classifiers tested with the leave-one-out methodology (macro-averaged F has been computed on the class-by-class scores reported in that article).All the DMmodels discriminate the verb classes much more reliably than the base-lines.
The accuracy of DepDM, the worst DM model, is significantly higher than that ofthe best baselines, AllTrue in C/I (Fisher test, p= 0.024) andMajority onMS (p= 0.039).TypeDM is again our best model.
Its performance is comparable to the lower rangeof the Merlo and Stevenson classifiers (considering the large confidence intervals due tothe small sample size, the accuracy of TypeDM is not significantly below even that of thetop model NoPass; p = 0.43).
The TypeDM results were obtained simply by measuringthe verb inter-slot similarities in theW1L?W2 space.
Conversely, the classifiers in Merloand Stevenson (2001) rely on a much larger range of knowledge-intensive featuresselected in an ad hoc fashion for this task (on the other hand, their training corpusTable 13Verb classification performance (precision, recall, and F for MS are macro-averaged).
Globalaccuracy supplemented by 95% binomial confidence intervals (CI).Causative/Inchoative (C/I)model prec recall F acc acc 95% CILexDM 76.0 69.9 72.8 69.9 65.2?74.3TypeDM 75.7 68.5 71.9 69.1 64.4?73.6DepDM 72.8 64.6 68.4 65.7 60.8?70.3AllTrue 57.7 100 73.2 57.7 52.7?62.6ProbMatch 57.7 57.7 57.7 51.2 46.2?56.2Merlo & Stevenson (MS)model prec recall F acc acc 95% CINoPass1 NA NA 71.2 71.2 57.3?81.9AllFeatures1 NA NA 69.1 69.5 55.5?80.5NoTrans1 NA NA 63.8 64.4 50.1?76.0NoCaus1 NA NA 62.6 62.7 48.4?74.5TypeDM 60.7 61.7 60.8 61.5 47.5?73.7NoVBN1 NA NA 61.0 61.0 46.6?73.0NoAnim1 NA NA 59.9 61.0 46.6?73.0LexDM 55.3 56.7 55.8 56.4 43.2?69.8DepDM 52.9 55.0 53.2 54.7 41.5?68.3Majority 11.3 33.3 16.9 33.9 22.5?48.1ProbMatch 33.3 33.3 33.3 33.3 21.0?46.3AllTrue 33.3 100 50.0 NA NAModel source: 1Merlo and Stevenson (2001).708Baroni and Lenci Distributional Memoryis not parsed and it is much smaller than ours).
Finally, we can notice that in bothexperiments the mildly (TypeDM) and heavily (LexDM) lexicalized DM models scorebetter than their non-lexicalized counterpart (DepDM), although the difference betweenthe best DM model and DepDM is not significant on either data set (p = 0.23 for theLexDM/DepDMdifference in C/I; p= 0.57 for the TypeDM/DepDMdifference inMS).Verb alternations do not typically appear among the standard tasks on which DSMsare tested.
Moreover, they involve non-trivial properties of argument structure.
Thegood performance of DM in these experiments is therefore particularly significant insupporting its vocation as a general model for distributional semantics.6.4 The L?W1W2 SpaceThe vectors of this space are labeled with links l (rows of matrix Bmode-2 in Table 3)and their dimensions are labeled with word pair tuples ?w1,w2?
(columns of the samematrix).
Links are represented in terms of the word pairs they connect.
The L?W1W2space supports tasks where we are directly interested in the links as an object ofstudy?for example, characterizing prepositions (Baldwin, Kordoni, and Villavicencio2009) or measuring the relative similarity of different kinds of verb?noun relations.
Wefocus here instead on a potentially more common use of L?W1W2 vectors as a ?featureselection and labeling?
space forW1W2?L tasks.Specifically, we go back to the qualia extraction task of Section 6.2.3.
There, westarted with manually identified links.
Here, we start with examples of noun?qualepairs ?n, qr?
that instantiate a role r. We project all L?W1W2 vectors in a subspace whereonly dimensions corresponding to one of the example pairs are non-zero.
We then pickthe most characteristic links in this subspace to represent the target role r, and look fornew pairs ?n, qr?
in theW1W2?L subspace defined by these automatically picked links,instead of the manual ones.
Although we stop at this point, the procedure can be seenas a DM version of popular iterative bootstrapping algorithms such as Espresso (Panteland Pennacchiotti 2006): Start with some examples of the target relation, find links thatare typical of these examples, use the links to find new examples, and so on.
In DM,the process does not go back to a corpus to harvest new links and example pairs, but ititerates between the column and row spaces of a pre-compiled matrix (i.e, the mode-2matricization in Table 3).For each of the 30 noun concepts in the Cimiano and Wenderoth gold standard, weuse the noun?quale pairs pertaining to the remaining 29 concepts as training examplesto select a set of 20 links that we then use in the same way as the manually selected linksof Section 6.2.3.
Simply picking the longest links in the L?W1W2 subspace defined by theexample ?n, qr?
dimensions does not work, because we harvest links that are frequentin general, rather than characteristic of the qualia roles (noun modification, of, etc.).
Foreach role r, we construct instead two L?W1W2 subspaces, one positive subspace with theexample pairs ?n, qr?
as unique non-zero dimensions, and a negative subspace with non-zero dimensions corresponding to all ?w1,w2?
pairs such that w1 is one of the trainingnominal concepts, and w2 is not a quale qr in the example pairs.
We then measure thelength of each link in both subspaces.
For example, we measure the length of the objlink in a subspace characterized by ?n, qtelic?
example pairs, and the length of obj in asubspace characterized by ?n, w2?
pairs that are probably not Telic examples.
We com-pute the pointwise mutual information (PMI) statistic (Church and Hanks 1990) onthese lengths to find the links that are most typical of the positive subspace corre-sponding to each qualia role.
PMI, with respect to other association measures, findsmore specific links, which is good for our purposes.
However, it is also notoriously709Computational Linguistics Volume 36, Number 4prone to over-estimating the importance of rare items (Manning and Schu?tze 1999,Chapter 5).
Thus, before selecting the top 20 links ranked by PMI, we filter out thoselinks that do not have at least 10 non-zero dimensions in the positive subspace.
Manyparameters here should be tuned more systematically (top n links, association measure,minimum non-zero dimensions), but the current results will nevertheless illustrate ourmethodology.Table 14 reports, for each quale, the TypeDM links that were selected in each of the30 leave-one-concept-out folds.
The links n is q, n in q, and q such as n are a good sketch ofthe Formal relation, which essentially subsumes various taxonomic relations.
The otherFormal links are less conspicuous.
However, note the presence of noun coordination(n coord q and q coord n), consistently with the common claim that coordinated termstend to be related taxonomically (Widdows and Dorow 2002).
Constitutive is mostly awhole?part relation, and the harvested links do a good job at illustrating such a relation.For the Telic, q by n, q through n, and q via n capture cases in which the quale stands in anaction?instrument relation to the target noun (murder by knife).
These links thus encodethe subtype of Telic role that Pustejovsky (1995) calls ?indirect.?
The two verb?nounlinks (q obj n and n sbj intr q) instead capture ?direct?
Telic roles, which are typicallyexpressed by the theme of a verb (read a book, the book reads well).
The least convincingresults are those for the Agentive role, where only q obj n and perhaps q out n areintuitively plausible canonical links.
Interestingly, the manual selections we carried outin Section 6.2.3 also gave very poor results for the Agentive role, as shown by the factthat Table 10 reports just one link for such a role.
This suggests that the problems withthis qualia role might be due to the number and type of lexicalized links used to buildthe DM tensors, rather than to the selection algorithm presented here.Coming now to the quantitative evaluation of the harvested patterns, the results inTable 15 (to be compared to Table 11 in Section 6.2.3) are based on W1W2?L subspaceswhere the non-zero dimensions correspond to the links that we picked automaticallywith the method we just described (different links for each concept, because of theleave-one-concept-out procedure).
TypeDM is the best model in this setting as well.Its performance is even better than the one (reported in Table 11) obtained with themanually picked patterns (although the difference is not statistically significant; pairedt-test, t = 0.75, df = 29, p = 0.46), and the automated approach has more room forimprovement via parameter optimization.We did not get as deeply into L?W1W2 space as we did with the other views, butour preliminary results on qualia harvesting suggest at least that looking at links asTable 14Links selected in all folds of the leave-one-out procedure to extract links typical of each qualiarole.FORMAL CONSTITUTIVEn is q, q is n, q become n, n coord q, n have q, n use q, nwith q, nwithout qq coord n, q have n, n in q, n provide q,q such as nAGENTIVE TELICq after n, q alongside n, q as n, q before n, q behind n, q by n, q like n, q obj n,q besides n, q during n, q in n, q obj n, n sbj intr q, q through n, q via nq out n, q over n, q since n, q unlike n710Baroni and Lenci Distributional MemoryTable 15Average qualia extraction performance with automatically harvested links (compare to Table 11).model precision recall F F s.d.TypeDM 24.2 26.7 19.1 7.7DepDM 18.4 27.0 15.1 4.9LexDM 22.6 18.1 14.8 7.7L?W1W2 vectors might be useful for feature selection in W1W2?L or for tasks in whichwe are given a set of pairs, and we have to find links that can function as verbal labelsfor the relation between the word pairs (Turney 2006a).6.5 Smoothing by Tensor DecompositionDimensionality reduction techniques such as the (truncated) SVD approximate a sparseco-occurrence matrix with a denser lower-rank matrix of the same size, and they havebeen shown to be effective in many semantic tasks, probably because they providea beneficial form of smoothing of the dimensions.
See Turney and Pantel (2010) forreferences and discussion.
We can apply SVD (or similar methods) to any of the tensor-derivedmatrices we used for the tasks herein.
An interesting alternative is to smooth thesource tensor directly by a tensor decomposition technique.
In this section, we present(very preliminary) evidence that tensor decomposition can improve performance, andit is at least as good in this respect as matrix-based SVD.
This is the only experimentin which we operate on the tensor directly, rather than on the matrices derived from it,paving the way to a more active role for the underlying tensor in the DM approach tosemantics.The (truncated) Tucker decomposition of a tensor can be seen as a higher-ordergeneralization of SVD.
Given a tensor X of dimensionality I1 ?
I2 ?
I3, its n-rank Rnis the rank of the vector space spanned by its mode-n fibers (obviously, for eachmode n of the tensor, Rn ?
In).
Tucker decomposition approximates the tensorX havingn-ranks R1, .
.
.
,Rn with X?
, a tensor with n-ranks Qn ?
Rn for all modes n. Unlike thecase of SVD, there is no analytical procedure to find the best lower-rank approximationto a tensor, and Tucker decomposition algorithms search for the reduced rank tensorwith the best fit (as measured by least square error) iteratively.
Specifically, we use thememory-efficient MET(1) algorithm of Kolda and Sun (2008) as implemented in theMatlab Tensor Toolbox.10 Kolda and Bader (2009) provide details on Tucker decompo-sition, its general properties, as well as applications and alternatives.SVD is believed to exploit patterns of higher order co-occurrence between the rowsand columns of a matrix (Manning and Schu?tze 1999; Turney and Pantel 2010), makingrow elements that co-occur with two synonymic columns more similar than in theoriginal space.
Tucker decomposition applied to the mode-3 tuple tensor could capturepatterns of higher order co-occurrence for each of the modes.
For example, it mightcapture at the same time similarities between links such as use and hold and w2 elementssuch as gun and knife.
SVD applied after construction of the W1?LW2 matrix, on theother hand, would miss the composite nature of columns such as ?use, gun?, ?use, knife?and ?hold, gun?.
Another attractive feature of Tucker decomposition is that it could be10 http://csmr.ca.sandia.gov/?tgkolda/TensorToolbox/.711Computational Linguistics Volume 36, Number 4Table 16Purity in Almuhareb?Poesio concept clustering with rank reduction of the APTypeDM tensor;95% confidence intervals (CI) obtained by bootstrapping.reduction rank purity 95% CITucker 250?50?500 75 72?80Tucker 300?50?500 75 71?79Tucker 300?50?450 74 71?79SVD 200 74 71?79SVD 350 74 70?79Tucker 300?40?500 74 70?78Tucker 300?60?500 74 70?78Tucker 350?50?500 73 69?77Tucker 300?50?550 72 69?77SVD 250 72 69?77SVD 150 72 68?77none ?
402 71 69?77SVD 300 71 68?76SVD 100 68 65?73SVD 50 64 61?70applied once to smooth the source tensor, whereas with SVD each matricization mustbe smoothed separately.
However, Tucker decomposition and SVD are computationallyintensive procedures, and, at least with our current computational resources, we are notable to decompose even the smallest DM tensor (similarly, we cannot apply SVD to afull matricization).
Given the continuous growth in computational power and the factthat efficient tensor decomposition is a very active area of research (Turney 2007; Koldaand Sun 2008) full tensor decomposition is nevertheless a realistic near future task.For the current pilot study, we replicated the AP concept clustering experimentdescribed in Section 6.1.3.
Because for efficiency reasons we must work with just aportion of the original tensor, we thought that the AP data set, consisting of a relativelylarge and balanced collection of nominal concepts, would offer a sensible starting pointto extract the subset.
Specifically, we extract from our best tensor TypeDM the valueslabeled by tuples ?wAP, l,w2?where wAP is in the AP set, l is one of the 100 most commonlinks occurring in tuples with a wAP, and w2 is one of the 1,000 most common wordsoccurring in tuples with a wAP and a l. The resulting (sub-)tensor, APTypeDM, hasdimensionality 402?
100?
1, 000 with 1,318,214 non-zero entries (density: 3%).
TheW1?LW2 matricization of APTypeDM results in a 402?
1, 000, 000 matrix with 66,026non-zero columns and the same number of non-zero entries and density as the tensor.The possible combinations of target lower n-ranks constitute a large tridimensionalparameter space, and we leave its systematic exploration to further work.
Instead, wepick 300, 50, and 500 as (intuitively reasonable) initial target n-ranks for the threemodes,and we explore their neighborhood in parameter space by changing one target n-rank ata time, by a relatively small value (300?
50, 50?
10, and 500?
50, respectively).
For theparameters concerning the reduced tensor fitting process, we accept the default valuesof the Tensor Toolbox.
For comparison purposes, we also apply SVD to the W1?LW2matrix derived from APTypeDM.
We systematically explore the SVD target lower rankparameter from 50 to 350 in increments of 50 units.The results are reported in Table 16.
The rank column reports the n-ranks whenreduction is performed on the tensor, and matrix ranks in the other cases.
Bootstrappedconfidence intervals are obtained as described in Section 6.1.3.
In general, the results712Baroni and Lenci Distributional Memoryconfirm that smoothing by rank reduction is beneficial to semantic performance, al-though not spectacularly so, with an improvement of about 4% for the best reducedmodel with respect to the raw APTypeDM tensor (consider however also the relativelywide confidence intervals).
As a general trend, tensor-based smoothing (Tucker) doesbetter than matrix-based smoothing (SVD).
As we said, for Tucker we only report re-sults from a small region of the tridimensional parameter space, whereas the SVD rankparameter range is explored coarsely but exhaustively.
Thus, although other parametercombinations might lead to dramatic changes in Tucker performance, the best SVDperformance in the table is probably close to the SVD performance upper bound.The present pilot study suggests an attitude of cautious optimism towards tensordecomposition as a smoothing technique.
At least in the AP task, it helps as comparedto no smoothing at all.
The same conclusion is reached by Turney (2007), who usesessentially the same method (with some differences in implementation) to tackle theTOEFL task, and obtains more than 10% improvement in accuracy with respect to thecorresponding raw tensor.
At least as a trend, tensor decomposition appears to be betterthan matrix decomposition, but only marginally so (Turney does not perform this com-parison).
Still, even if the tensor- and matrix-based decompositions turned out to havecomparable effects, tensor-based smoothing is more attractive in the DM frameworkbecause we could perform the decomposition once, and use the smoothed tensor as ourstable underlying DM (modulo, of course, memory problems with computing such alarge tensor decomposition).Beyond smoothing, tensor decomposition might provide some novel avenues fordistributional semantics, while keeping to the DM program of a single model for manytasks.
Van de Cruys (2009) used tensor decomposition to find commonalities in latentdimensions across the fiber labels (in the DM formalism, this would amount to findingcommonalities across w1, l, and w2 elements).
Another possible use for smoothingwould be to propagate ?link mass?
across parts of speech.
Our tensors, being based onPOS tagging and dependency parsing, have 0 values for noun-link-noun tuples such as?city, obj, destruction?
and ?city, subj tr, destruction?.
In a smoothed tensor, by the influenceof tuples such as ?city, obj, destroy?
and ?city, sbj tr, destroy?, these tuples will get somenon-0 weight that, hopefully, will make the object relation between city and destructionemerge.
This is at the moment just a conjecture, but it constitutes an exciting directionfor further work focusing on tensor decomposition within the DM framework.7.
ConclusionA general framework for distributional semantics should satisfy the following tworequirements: (1) representing corpus-derived data in such a way as to capture aspectsof meaning that have so far been modeled with different, prima facie incompatible datastructures; (2) using this common representation to address a large battery of semanticexperiments, achieving a performance at least comparable to that of state-of-art, task-specific DSMs.
We can now safely claim that DM satisfies both these desiderata, andthereby represents a genuine step forward in the quest for a general purpose approachto distributional semantics.DM addresses point (1) by modeling distributional data as a structure of weightedtuples that is formalized as a labeled third-order tensor.
This is a generalization withrespect to the common approach of many corpus-based semantic models (the structuredDSMs) that rely on distributional information encoded into word?link?word tuples,associated with weights that are functions of their frequency of co-occurrence in the cor-pus.
Existing structured DSMs still couch this information directly in binary structures,713Computational Linguistics Volume 36, Number 4namely, co-occurrence matrices, thereby giving rise to different semantic spaces and los-ing sight of the fact that such spaces share the same kind of distributional information.The third-order tensor formalization of distributional data allows DM to fully exploitthe potential of corpus-derived tuples.
The four semantic spaces we analyzed and testedin Section 6 are generated from the same underlying third-order tensor, by the standardoperation of tensor matricization.
This way, we derive a set of semantic spaces that canbe used for measuring attributional similarity (finding synonyms, categorizing conceptsinto superordinates, etc.)
and relational similarity (finding analogies, grouping conceptpairs into relation classes, etc.).
Moreover, the distributional information encoded in thetensor and unfolded via matricization leads to further arrangements of the data usefulin addressing semantic problems that do not fall straightforwardly into the attributionalor the relational paradigm (grouping verbs by alternations, harvesting patterns thatrepresent a relation).
In some cases, it is obvious how to reformulate a semantic problemin the new framework.
Other tasks can be reframed in terms of our four semanticspaces using geometric operations such as centroid computations and projection ontoa subspace.
This was the case for selectional preferences, pattern- and example-basedrelation extraction (illustrated by qualia harvesting), and the task of generating typicalproperties of concepts.
We consider a further strength of the DM approach that it natu-rally encourages us to think, as we did in these cases, of ways to tackle apparentlyunrelated tasks with the existing resources, rather than devising unrelated approachesto deal with them.Regarding point (2), that is, addressing a large battery of semantic experiments withgood performance, in nearly all test sets our best implementation of DM (TypeDM) isat least as good as other algorithms reported in recently published papers (typicallydeveloped or tuned for the task at hand), often towards (or at) the top of the state-of-the-art ranking.
Where other models outperform TypeDM by a large margin, there aretypically obvious reasons for this: The rivals have been trained on much larger corpora,or they rely on special knowledge resources, or on sophisticated machine learningalgorithms.
Importantly, TypeDM is consistently at least as good (or better than) thosemodels we reimplemented to be fully comparable to our DMs (i.e., Win, DV, LRA).Moreover, the best DM implementation does not depend on the semantic space:TypeDM outperforms (at least in terms of average performance across tasks) the othertwo models in all four spaces.
This is not surprising (better distributional tuples shouldstill be better when seen from different views), but it is good to have an empirical con-firmation of the a priori intuition.
The current results suggest that one could, for exam-ple, compare alternative DMs on a few attributional tasks, and expect the best DM inthese tasks to also be the best in relational tasks and other semantic challenges.The final experiment of Section 6 briefly explored an interesting aspect of thetensor-based formalism, namely, the possibility of improving performance on sometasks by working directly on the tensor (in this case, applying tensor rank reductionfor smoothing purposes) rather than on the matrices derived from it.
Besides this pilotstudy, we did not carry out any task-specific optimization of TypeDM, which achievesits very good performance using exactly the same underlying parameter configuration(e.g., dependency paths, weighting function) across the different spaces and tasks.Parameter tuning is an important aspect in DSM development, with an often dramaticimpact of parameter variation (Bullinaria and Levy 2007; Erk and Pado?
2009).
Weleave the exploration of parameter space in DM for future research.
Its importance not-withstanding, however, we regard this as a rather secondary aspect, if compared withthe good performance of a DM model (even in its current implementation) in the largeand multifarious set of tasks we presented.714Baroni and Lenci Distributional MemoryOf course, many issues are still open.
It is one thing to claim that the models thatoutperform TypeDMdo so because they rely on larger corpora; it is another to show thatTypeDM trained on more data does reach the top of the current heap.
The differencesbetween TypeDM and the other, generally worse-performing DM models remind usthat the idea of a shared distributional memory per se is not enough to obtain goodresults, and the extraction of an ideal DM from the corpus certainly demands furtherattention.
We need to reach a better understanding of which pieces of distributionalinformation to extract, and whether different semantic tasks require focusing on specificsubsets of distributional data.
Another issue we completely ignored but which will be offundamental importance in applications is how a DM-based system can deal with out-of-vocabulary items.
Ideally, we would like a seamless way to integrate new terms inthe model incrementally, based on just a few extra data points, but we leave it to furtherresearch to study how this could be accomplished, together with the undoubtedly manyfurther practical and theoretical problems that will emerge.
We will conclude, instead,by discussing some general advantages that follow from the DM approach of separatingcorpus-based model building, the multi-purpose long term distributional memory, anddifferent views of the memory data to accomplish different semantic tasks, withoutresorting to the source corpus again.First of all, we would like to make a more general point regarding parametertuning and task-specific optimization, by going back to the analogy with WordNet as asemantic multi-purpose resource.
If you want to improve performance of a WordNet-based system, you will probably not wait for its next release, but rather improve thealgorithms that work on the existing WordNet graph.
Similarly, in the DM approach wepropose that corpus-based resources for distributional semantics should be relativelystable, multi-purpose, large-scale databases (in the form of weighted tuple structures),only occasionally updated (because a better or larger corpus becomes available, a betterparser, etc.).
Still, given the same underlying DM and a certain task, much work can bedone to exploit the DM optimally in the task, with no need to go back to corpus-basedresource construction.
For example, performance on attributional tasks could be raisedby dimension reweighting techniques such as recently proposed by Zhitomirsky-Geffetand Dagan (2009).
For the problem of data sparseness in the W1W2?L space, we couldtreat the tensor as a graph and explore random walks and other graphical approachesthat have been shown to ?scale down?
gracefully to capture relations in sparser datasets (Minkov and Cohen 2007, 2008).
As in our simple example of smoothing relationalpairs with attributional neighbors, more complex tasks may be tackled by combiningdifferent views of DM, and/or resorting to different (sub)spaces within the same view,as in our approach to selectional preferences.
One might even foresee an algorithmicway to mix and match the spaces as most appropriate to a certain task.
We propose asimilar split for the role of supervision in DSMs.
Construction of the DM tensor from thecorpus is most naturally framed as an unsupervised task, because the model will servemany different purposes.
On the other hand, supervision can be of great help in tuningthe DM data to specific tasks (as we did, in a rather naive way, with the nearest centroidapproach to most non-attributional tasks).
A crucial challenge for DSMs is whetherand how corpus-derived vectors can also be used in the construction of meaning forconstituents larger than words.
These are the traditional domains of formal semantics,which is most interested in how the logical representation of a sentence or a discourseis built compositionally by combining the meanings of its constituents.
DSMs have sofar focused on representing lexical meaning, and compositional and logical issues haveeither remained out of the picture, or have received still unsatisfactory accounts.
A gen-eral consensus exists on the need to overcome this limitation, and to build new bridges715Computational Linguistics Volume 36, Number 4between corpus-based semantics and symbolic models of meanings (Clark and Pulman2007; Widdows 2008).
Most problems encountered by DSMs in tackling this challengeare specific instances of more general issues concerning the possibility of representingsymbolic operations with distributed, vector-based data structures (Markman 1999).Many avenues are currently being explored in corpus-based semantics, and interestingsynergies are emerging with research areas such as neural systems (Smolensky 1990;Smolensky and Legendre 2006), quantum information (Widdows and Peters 2003; Aertsand Czachor 2004; Widdows 2004; Van Rijsbergen 2004; Bruza and Cole 2005; Houand Song 2009), holographic models of memory (Jones and Mewhort 2007), and soon.
A core problem in dealing with compositionality with DSMs is to account for therole of syntactic information in determining the way semantic representations are builtfrom lexical items.
For instance, the semantic representation assigned to The dog bitesthe man must be different from the one assigned to The man bites the dog, even if theycontain exactly the same lexical items.
Although it is still unclear which is the bestway to compose the representation of content words in vector spaces, it is nowadayswidely assumed that structured representations like those adopted by DM are in theright direction towards a solution to this issue, exactly because they allow distributionalrepresentations to become sensitive to syntactic structures (Erk and Pado?
2008).
Compo-sitionality and similar issues in DSMs lie beyond the scope of this paper.
However, thereis nothing in DM that prevents it from interacting with any of the research directions wehave mentioned here.
Indeed, we believe that the generalized nature of DM representsa precondition for distributional semantics to be able to satisfactorily address thesemore advanced challenges.
A multi-purpose, distributional semantic resource like DMcan allow researchers to focus on the next steps of semantic modeling.
These includecompositionality, but also modulating word meaning in context (Erk and Pado?
2008;Mitchell and Lapata 2008) and finding ways to embed the distributional memory incomplex NLP systems (e.g., for question answering or textual entailment) or evenembodied agents and robots.DM-style triples predicating a relation between two entities are common currencyin many semantic representation models (e.g., semantic networks) and knowledge-exchange formalisms such as RDF.
This might also pave the way to the integration ofcorpus-based information with other knowledge sources.
It is hard to see how suchintegration could be pursued within generalized systems, such as PairClass (Turney2008), that require keeping a full corpus around and corpus-processing know-how onbehalf of interested researchers from outside the NLP community (see discussion inSection 4 above).
Similarly, the DM triples might help in fostering the dialogue betweencomputational linguists and the computational neuro-cognitive community, where it iscommon to adopt triple-based representations of knowledge, and to use the same set oftuples to simulate various aspects of cognition.
For a recent extended example of thisapproach, see Rogers and McClelland (2004).
It would be relatively easy to use a DMmodel in lieu of their neural network, and use it to simulate the conceptual processesthey reproduce.DM, unlike classic DSM models that go directly from the corpus data to solvingspecific semantic tasks, introduces a clear distinction between an acquisition phase(corpus-based tuple extraction and weighting), the declarative structure at the core ofsemantic modeling (the distributional memory), and the procedural problem-solvingcomponents (possibly supervised procedures to perform different semantic tasks).
Thisseparation is in line with what is commonly assumed in cognitive science and formallinguistics, and we hope it will contribute to make corpus-based modeling a core partof the ongoing study of semantic knowledge in humans and machines.716Baroni and Lenci Distributional MemoryAcknowledgmentsWe thank Abdulrahman Almuhareb,Philipp Cimiano, George Karypis,Tamara Kolda, Thomas Landauer,Mirella Lapata, Ken McRae, Brian Murphy,Vivi Nastase, Diarmuid O?
Se?aghdha,Sebastian and Ulrike Pado?, SuzanneStevenson, Peter Turney, their colleagues,and the SEMEVAL Task 4 organizers fordata and tools.
We thank Gemma Boleda,Phillipp Cimiano, Katrin Erk, Stefan Evert,Brian Murphy, Massimo Poesio, MagnusSahlgren, Tim Van de Cruys, Peter Turney,and three anonymous reviewers for amixture of advice, clarification, and ideas.ReferencesAerts, Diederik and Marek Czachor.
2004.Quantum aspects of semantic analysis andsymbolic artificial intelligence.
Journal ofPhysics A: Mathematical and General,37:123?132.Alishahi, Afra and Suzanne Stevenson.
2008.A distributional account of the semanticsof multiword expressions.
Italian Journal ofLinguistics, 20(1):157?179.Almuhareb, Abdulrahman.
2006.
Attributesin Lexical Acquisition.
Ph.D. thesis,University of Essex.Almuhareb, Abdulrahman and MassimoPoesio.
2004.
Attribute-based andvalue-based clustering: An evaluation.In Proceedings of EMNLP, pages 158?165,Barcelona.Almuhareb, Abdulrahman and MassimoPoesio.
2005.
Concept learning andcategorization from the web.
In Proceedingsof CogSci, pages 103?108, Stresa.Baldwin, Timothy, Valia Kordoni, andAline Villavicencio.
2009.
Prepositions inapplications: A survey and introduction tothe special issue.
Computational Linguistics,35(2):119?149.Baroni, Marco, Eduard Barbu, Brian Murphy,and Massimo Poesio.
2010.
Strudel: Adistributional semantic model based onproperties and types.
Cognitive Science,34(2):222?254.Baroni, Marco, Stefan Evert, andAlessandro Lenci, editors.
2008.
Bridgingthe Gap between Semantic Theory andComputational Simulations: Proceedingsof the ESSLLI Workshop on DistributionalLexical Semantic.
FOLLI, Hamburg.Baroni, Marco and Alessandro Lenci.
2008.Concepts and properties in word spaces.Italian Journal of Linguistics, 20(1):55?88.Baroni, Marco and Alessandro Lenci.
2009.One distributional memory, manysemantic tasks.
In Proceedings of the EACLGEMS Workshop, pages 1?8, Athens.Bicic?i, Ergun and Deniz Yuret.
2006.Clustering word pairs to answer analogyquestions.
In Proceedings of the FifteenthTurkish Symposium on Artificial Intelligenceand Neural Networks, pages 277?284,Mug?la.Bruza, Peter and Richard Cole.
2005.Quantum logic of semantic space:An exploratory investigation of contexteffects in practical reasoning.
InSergei Artemov, Howard Barringer,Arthur d?Avila Garcez, Luis C. Lamb,and John Woods, editors,We Will ShowThem: Essays in Honour of Dov Gabbay,volume one.
College Publications, London,pages 339?361.Buitelaar, Paul, Philipp Cimiano, andBernardo Magnini.
2005.
OntologyLearning from Text.
IOS Press, Amsterdam.Bullinaria, John and Joseph Levy.
2007.Extracting semantic representationsfrom word co-occurrence statistics: Acomputational study.
Behavior ResearchMethods, 39:510?526.Chen, Hsin-Hsi, Ming-Shun Lin, andYu-Chuan Wei.
2006.
Novel associationmeasures using Web search with doublechecking.
In Proceedings of COLING-ACL,pages 1009?1016, Sydney.Church, Kenneth and Peter Hanks.
1990.Word association norms, mutualinformation, and lexicography.Computational Linguistics, 16(1):22?29.Cimiano, Philipp and Johanna Wenderoth.2007.
Automatic acquisition of rankedqualia structures from the Web.
InProceedings of ACL, pages 888?895,Prague.Clark, Stephen and Stephen Pulman.
2007.Combining symbolic and distributionalmodels of meaning.
In Proceedings of theAAAI Spring Symposium on QuantumInteraction, pages 52?55, Stanford, CA.Curran, James and Marc Moens.
2002.Improvements in automatic thesaurusextraction.
In Proceedings of the ACLWorkshop on Unsupervised LexicalAcquisition, pages 59?66, Philadelphia, PA.Davidov, Dmitry and Ari Rappoport.
2008a.Classification of semantic relationshipsbetween nominals using pattern clusters.In Proceedings of ACL, pages 227?235,Columbus, OH.Davidov, Dmitry and Ari Rappoport.2008b.
Unsupervised discovery of717Computational Linguistics Volume 36, Number 4generic relationships using patternclusters and its evaluation byautomatically generated SAT analogyquestions.
In Proceedings of ACL,pages 692?700, Columbus, OH.Dietterich, Thomas.
1998.
Approximatestatistical tests for comparing supervisedclassification learning algorithms.
NeuralComputation, 10(7):1895?1924.Dowty, David.
1977.Word Meaning andMontague Grammar.
Kluwer, Dordrecht.Dunning, Ted.
1993.
Accurate methods forthe statistics of surprise and coincidence.Computational Linguistics, 19(1):61?74.Efron, Bradley and Robert Tibshirani.
1994.An Introduction to the Bootstrap.
Chapmanand Hall, Boca Raton, FL.Erk, Katrin.
2007.
A simple, similarity-basedmodel for selectional preferences.In Proceedings of ACL, pages 216?223,Prague.Erk, Katrin and Sebastian Pado?.
2008.
Astructured vector space model for wordmeaning in context.
In Proceedings ofEMNLP, pages 897?906, Honolulu, HI.Erk, Katrin and Sebastian Pado?.
2009.Paraphrase assessment in structuredvector space: Exploring parameters anddatasets.
In Proceedings of the EACLGEMS Workshop, pages 57?65, Athens.Evert, Stefan.
2005.
The Statistics of WordCooccurrences.
Ph.D. dissertation,Stuttgart University.Fellbaum, Christiane, editor.
1998.WordNet:An Electronic Lexical Database.
MIT Press,Cambridge, MA.Garrard, Peter, Matthew Lambon Ralph,John Hodges, and Karalyn Patterson.2001.
Prototypicality, distinctiveness,and intercorrelation: Analyses of thesemantic attributes of living and nonlivingconcepts.
Cognitive Neuropsychology,18(2):25?174.Geeraerts, Dirk.
2010.
Theories of LexicalSemantics.
Oxford University Press,Oxford.Girju, Roxana, Adriana Badulescu, andDan Moldovan.
2006.
Automatic discoveryof part-whole relations.
ComputationalLinguistics, 32(1):83?135.Girju, Roxana, Preslav Nakov, Vivi Nastase,Stan Szpakowicz, Peter Turney, andDeniz Yuret.
2007.
SemEval-2007 task 04:Classification of semantic relationsbetween nominals.
In Proceedings ofSemEval 2007, pages 13?18, Prague.Grefenstette, Gregory.
1994.
Explorations inAutomatic Thesaurus Discovery.
Kluwer,Boston, MA.Griffiths, Tom, Mark Steyvers, and JoshTenenbaum.
2007.
Topics in semanticrepresentation.
Psychological Review,114:211?244.Harris, Zellig.
1954.
Distributional structure.Word, 10(2-3):1456?1162.Hearst, Marti.
1992.
Automatic acquisitionof hyponyms from large text corpora.
InProceedings of COLING, pages 539?545,Nantes.Hearst, Marti.
1998.
Automated discoveryof WordNet relations.
In ChristianeFellbaum, editor,WordNet: An ElectronicLexical Database.
MIT Press, Cambridge,MA, pages 131?151.Herdag?delen, Amac?
and Marco Baroni.2009.
BagPack: A general framework torepresent semantic relations.
In Proceedingsof the EACL GEMS Workshop, pages 33?40,Athens.Herdag?delen, Amac?, Katrin Erk, andMarco Baroni.
2009.
Measuring semanticrelatedness with vector space modelsand random walks.
In Proceedings ofTextGraphs-4, pages 50?53, Singapore.Heylen, Kris, Yves Peirsman, Dirk Geeraerts,and Dirk Speelman.
2008.
Modelling wordsimilarity: An evaluation of automaticsynonymy extraction algorithms.
InProceedings of LREC, pages 3243?3249,Marrakech.Hou, Yuexian and Dawei Song.
2009.Characterizing pure high-orderentanglements in lexical semanticspaces via information geometry.In Peter Bruza, Donald Sofge, WilliamLawless, and C. J. van Rijsbergen, editors,Quantum Interaction: Third InternationalSymposium, QI 2009.
Springer, Berlin,pages 237?250.Jackendoff, Ray.
1990.
Semantic Structures.MIT Press, Cambridge, MA.Joanis, Eric, Suzanne Stevenson, and DavidJames.
2008.
A general feature space forautomatic verb classification.
NaturalLanguage Engineering, 14(3):337?367.Jones, Michael and Douglas Mewhort.2007.
Representing word meaningand order information in a compositeholographic lexicon.
PsychologicalReview, 114:1?37.Karypis, George.
2003.
CLUTO: A clusteringtoolkit.
Technical Report 02-017,University of Minnesota Departmentof Computer Science, Minneapolis.Kilgarriff, Adam, Pavel Rychly, Pavel Smrz,and David Tugwell.
2004.
The SketchEngine.
In Proceedings of Euralex,pages 105?116, Lorient.718Baroni and Lenci Distributional MemoryKolda, Tamara.
2006.
Multilinear operatorsfor higher-order decompositions.
TechnicalReport 2081, SANDIA, Albuquerque, NM.Kolda, Tamara and Brett Bader.
2009.
Tensordecompositions and applications.
SIAMReview, 51(3):455?500.Kolda, Tamara and Jimeng Sun.
2008.Scalable tensor decompositions formulti-aspect data mining.
In Proceedingsof ICDM, pages 94?101, Pisa.Landauer, Thomas and Susan Dumais.
1997.A solution to Plato?s problem: The latentsemantic analysis theory of acquisition,induction, and representation ofknowledge.
Psychological Review,104(2):211?240.Lenci, Alessandro.
2008.
Distributionalapproaches in linguistic and cognitiveresearch.
Italian Journal of Linguistics,20(1):1?31.Lenci, Alessandro.
2010.
The life cycleof knowledge.
In Chu-Ren Huang,Nicoletta Calzolari, Aldo Gangemi,Alessandro Lenci, Alessandro Oltramari,and Laurent Pre?vot, editors, Ontologyand the Lexicon.
A Natural LanguageProcessing Perspective.
CambridgeUniversity Press, Cambridge, UK,pages 241?257.Levin, Beth.
1993.
English Verb Classes andAlternations: A Preliminary Investigation.University of Chicago Press, Chicago, IL.Levin, Beth and Malka Rappaport-Hovav.2005.
Argument Realization.
CambridgeUniversity Press, Cambridge, UK.Lin, Dekang.
1998a.
Automatic retrievaland clustering of similar words.
InProceedings of COLING-ACL,pages 768?774, Montreal.Lin, Dekang.
1998b.
An information-theoreticdefinition of similarity.
In Proceedings ofICML, pages 296?304, Madison, WI.Liu, Hugo and Push Singh.
2004.ConceptNet: A practical commonsensereasoning toolkit.
BT Technology Journal,pages 211?226.Lowe, Will.
2001.
Towards a theory ofsemantic space.
In Proceedings of CogSci,pages 576?581, Edinburgh, UK.Lund, Kevin and Curt Burgess.
1996.Producing high-dimensional semanticspaces from lexical co-occurrence.Behavior Research Methods, 28:203?208.Manning, Chris and Hinrich Schu?tze.
1999.Foundations of Statistical Natural LanguageProcessing.
MIT Press, Cambridge, MA.Markman, Arthur B.
1999.
KnowledgeRepresentation.
Psychology Press,New York, NY.Matveeva, Irina, Gina-Anne Levow,Ayman Farahat, and Christian Royer.
2005.Generalized latent semantic analysis forterm representation.
In Proceedings ofRANLP, pages 60?68, Borovets.McRae, Ken, George Cree, Mark Seidenberg,and Chris McNorgan.
2005.
Semanticfeature production norms for a large setof living and nonliving things.
BehaviorResearch Methods, 37(4):547?559.McRae, Ken, Michael Spivey-Knowlton,and Michael Tanenhaus.
1998.
Modelingthe influence of thematic fit (andother constraints) in on-line sentencecomprehension.
Journal of Memory andLanguage, 38:283?312.Merlo, Paola and Suzanne Stevenson.
2001.Automatic verb classification based onstatistical distributions of argumentstructure.
Computational Linguistics,27(3):373?408.Meyer, Carl.
2000.Matrix Analysis and AppliedLinear Algebra.
SIAM, Philadelphia, PA.Miller, George and Walter Charles.
1991.Contextual correlates of semanticsimilarity.
Language and CognitiveProcesses, 6:1?28.Minkov, Einat and William Cohen.
2007.Learning to rank typed graph walks:Local and global approaches.
InProceedings of WebKDD/SNA-KDD,pages 1?8, San Jose?, CA.Minkov, Einat and William Cohen.
2008.Learning graph walk based similaritymeasures for parsed text.
In Proceedingsof EMNLP, pages 907?916, Honolulu, HI.Mitchell, Jeff and Mirella Lapata.
2008.Vector-based models of semanticcomposition.
In Proceedings of ACL,pages 236?244, Columbus, OH.Murphy, Gregory.
2002.
The Big Book ofConcepts.
MIT Press, Cambridge, MA.Nastase, Vivi and Stan Szpakowicz.
2003.Exploring noun-modifier semanticrelations.
In Proceedings of the FifthInternational Workshop on ComputationalSemantics, pages 285?301, Tilburg,The Netherlands.O?
Se?aghdha, Diarmuid and Ann Copestake.2009.
Using lexical and relationalsimilarity to classify semantic relations.In Proceedings of EACL, pages 621?629,Athens.Pado?, Sebastian and Mirella Lapata.
2007.Dependency-based construction ofsemantic space models.
ComputationalLinguistics, 33(2):161?199.Pado?, Ulrike.
2007.
The Integration ofSyntax and Semantic Plausibility in a719Computational Linguistics Volume 36, Number 4Wide-Coverage Model of Sentence Processing.Ph.D.
dissertation, Saarland University,Saarbru?cken.Pado?, Ulrike, Sebastian Pado?, and Katrin Erk.2007.
Flexible, corpus-based modellingof human plausibility judgements.
InProceedings of EMNLP, pages 400?409,Prague.Pantel, Patrick and Marco Pennacchiotti.2006.
Espresso: Leveraging genericpatterns for automatically harvestingsemantic relations.
In Proceedings ofCOLING-ACL, pages 113?120, Sydney.Peirsman, Yves and Dirk Speelman.
2009.Word space models of lexical variation.In Proceedings of the EACL GEMSWorkshop, pages 9?16, Athens.Pustejovsky, James.
1995.
The GenerativeLexicon.
MIT Press, Cambridge, MA.Quesada, Jose, Praful Mangalath, andWalter Kintsch.
2004.
Analogy-makingas predication using relational informationand LSA vectors.
In Proceedings of CogSci,page 1623, Chicago, IL.Raghunathan, Trivellore.
2003.
Anapproximate test for homogeneity ofcorrelated correlation coefficients.Quality & Quantity, 37:99?110.Rapp, Reinhard.
2003.
Word sense discoverybased on sense descriptor dissimilarity.In Proceedings of the 9th MT Summit,pages 315?322, New Orleans, LA.Rapp, Reinhard.
2004.
A freely availableautomatically generated thesaurus ofrelated words.
In Proceedings of LREC,pages 395?398, Lisbon.Rogers, Timothy and James McClelland.2004.
Semantic Cognition: A ParallelDistributed Processing Approach.
MIT Press,Cambridge, MA.Rothenha?usler, Klaus and Hinrich Schu?tze.2009.
Unsupervised classification withdependency based word spaces.
InProceedings of the EACL GEMS Workshop,pages 17?24, Athens, Greece.Rubenstein, Herbert and John Goodenough.1965.
Contextual correlates of synonymy.Communications of the ACM, 8(10):627?633.Ruiz-Casado, Maria, Enrique Alfonseca,and Pablo Castells.
2005.
Usingcontext-window overlapping in synonymdiscovery and ontology extension.
InProceedings of RANLP, pages 1?7, Borovets.Sagi, Eyal, Stefan Kaufmann, and BradyClark.
2009.
Semantic density analysis:Comparing word meaning across timeand phonetic space.
In Proceedings of theEACL GEMS Workshop, pages 104?111,Athens.Sahlgren, Magnus.
2005.
An introduction torandom indexing.
http://www.sics.se/?mange/papers/RI intro.pdf.Sahlgren, Magnus.
2006.
The Word-SpaceModel.
Ph.D. dissertation, StockholmUniversity.Schulte im Walde, Sabine.
2006.
Experimentson the automatic induction of Germansemantic verb classes.
ComputationalLinguistics, 32:159?194.Schu?tze, Hinrich.
1997.
Ambiguity Resolutionin Natural Language Learning.
CSLIPublications, Stanford, CA.Smolensky, Paul.
1990.
Tensor productvariable binding and the representationof symbolic structures in connectionistsystems.
Artificial Intelligence, 46:159?216.Smolensky, Paul and Geraldine Legendre.2006.
The Harmonic Mind.
From NeuralComputation to Optimality-theoreticGrammar.
MIT Press, Cambridge, MA.Terra, Egidio and Charles Clarke.
2003.Frequency estimates for statistical wordsimilarity measures.
In Proceedings ofHLT-NAACL, pages 244?251, Edmonton.Turney, Peter.
2001.
Mining the Web forsynonyms: PMI-IR versus LSA on TOEFL.In Proceedings of ECML, pages 491?502,Freiburg.Turney, Peter.
2006a.
Expressing implicitsemantic relations without supervision.In Proceedings of COLING-ACL,pages 313?320, Sydney.Turney, Peter.
2006b.
Similarity of semanticrelations.
Computational Linguistics,32(3):379?416.Turney, Peter.
2007.
Empirical evaluationof four tensor decomposition algorithms.Technical Report ERB-1152, NRC,Ottawa.Turney, Peter.
2008.
A uniform approach toanalogies, synonyms, antonyms andassociations.
In Proceedings of COLING,pages 905?912, Manchester.Turney, Peter and Michael Littman.
2005.Corpus-based learning of analogies andsemantic relations.Machine Learning,60(1-3):251?278.Turney, Peter and Patrick Pantel.
2010.
Fromfrequency to meaning: Vector spacemodels of semantics.
Journal of ArtificialIntelligence Research, 37:141?188.Van de Cruys, Tim.
2009.
A non-negativetensor factorization model for selectionalpreference induction.
In Proceedings of theEACL GEMS Workshop, pages 83?90,Athens.Van Overschelde, James, Katherine Rawson,and John Dunlosky.
2004.
Category720Baroni and Lenci Distributional Memorynorms: An updated and expandedversion of the Battig and Montague (1969)norms.
Journal of Memory and Language,50:289?335.Van Rijsbergen, C. J.
2004.
The Geometry ofInformation Retrieval.
CambridgeUniversity Press, Cambridge, UK.Veale, Tony and Yanfen Hao.
2008.Acquiring naturalistic conceptdescriptions from the Web.
InProceedings of LREC, pages 1121?1124,Marrakech.Vinson, David and Gabriella Vigliocco.
2008.Semantic feature production norms for alarge set of objects and events.
BehaviorResearch Methods, 40(1):183?190.Widdows, Dominic.
2004.
Geometry andMeaning.
CSLI Publications, Stanford, CA.Widdows, Dominic.
2008.
Semantic vectorproducts: Some initial investigations.In Proceedings of the Second AAAISymposium on Quantum Interaction,pages 1?8, Oxford.Widdows, Dominic and Beate Dorow.2002.
A graph model for unsupervisedlexical acquisition.
In Proceedings ofICCL, pages 1?7, Taipei.Widdows, Dominic and Stanley Peters.2003.
Word vectors and quantum logic.In Proceedings of the Eighth Mathematicsof Language Conference, pages 1?14,Bloomington, IN.Zarcone, Alessandra and Alessandro Lenci.2008.
Computational models of event typeclassification in context.
In Proceedings ofLREC, pages 1232?1238, Marrakech.Zhao, Ying and George Karypis.
2003.Criterion functions for documentclustering: Experiments and analysis.Technical Report 01-40, University ofMinnesota Department of ComputerScience, Minneapolis.Zhitomirsky-Geffet, Maayan and Ido Dagan.2009.
Bootstrapping distributional featurevector quality.
Computational Linguistics,35(3):435?461.721
