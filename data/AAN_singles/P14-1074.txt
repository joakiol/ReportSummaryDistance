Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 786?796,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsLinguistic Structured Sparsity in Text CategorizationDani YogatamaLanguage Technologies InstituteSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USAdyogatama@cs.cmu.eduNoah A. SmithLanguage Technologies InstituteSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USAnasmith@cs.cmu.eduAbstractWe introduce three linguistically moti-vated structured regularizers based onparse trees, topics, and hierarchical wordclusters for text categorization.
Theseregularizers impose linguistic bias in fea-ture weights, enabling us to incorporateprior knowledge into conventional bag-of-words models.
We show that ourstructured regularizers consistently im-prove classification accuracies comparedto standard regularizers that penalize fea-tures in isolation (such as lasso, ridge,and elastic net regularizers) on a range ofdatasets for various text prediction prob-lems: topic classification, sentiment anal-ysis, and forecasting.1 IntroductionWhat is the best way to exploit linguistic infor-mation in statistical text processing models?
Fortasks like text classification, sentiment analysis,and text-driven forecasting, this is an open ques-tion, as cheap ?bag-of-words?
models often per-form well.
Much recent work in NLP has fo-cused on linguistic feature engineering (Joshi etal., 2010) or representation learning (Glorot et al,2011; Socher et al, 2013).In this paper, we propose a radical alternative.We embrace the conventional bag-of-words repre-sentation of text, instead bringing linguistic biasto bear on regularization.
Since the seminal workof Chen and Rosenfeld (2000), the importance ofregularization in discriminative models of text?including language modeling, structured predic-tion, and classification?has been widely recog-nized.
The emphasis, however, has largely beenon one specific kind of inductive bias: avoidinglarge weights (i.e., coefficients in a linear model).Recently, structured (or composite) regulariza-tion has been introduced; simply put, it reasonsabout different weights jointly.
The most widelyexplored variant, group lasso (Yuan and Lin, 2006)seeks to avoid large `2norms for groups ofweights.
Group lasso has been shown useful ina range of applications, including computationalbiology (Kim and Xing, 2008), signal processing(Lv et al, 2011), and NLP (Eisenstein et al, 2011;Martins et al, 2011; Nelakanti et al, 2013).
Fortext categorization problems, Yogatama and Smith(2014) proposed groups based on sentences, anidea generalized here to take advantage of richerlinguistic information.In this paper, we show how linguistic informa-tion of various kinds?parse trees, thematic topics,and hierarchical word clusterings?can be used toconstruct group lasso variants that impose linguis-tic bias without introducing any new features.
Ourexperiments demonstrate that structured regulariz-ers can squeeze higher performance out of conven-tional bag-of-words models on seven out of eightof text categorization tasks tested, in six cases withmore compact models than the best-performingunstructured-regularized model.2 NotationWe represent each document as a feature vectorx ?
RV, where V is the vocabulary size.
xvis thefrequency of the vth word (i.e., this is a ?bag ofwords?
model).Consider a linear model that predicts a binaryresponse y ?
{?1,+1} given x and weight vectorw ?
RV.
We denote our training data of D doc-uments in the corpus by {xd, yd}Dd=1.
The goal ofthe learning procedure is to estimate w by mini-mizing the regularized training data loss:w?
= arg minw?
(w) +?Dd=1L(xd,w, yd),where L(x,w, y) is the loss function for docu-ment d and ?
(w) is the regularizer.In this work, we use the log loss:L(xd,w, yd) = ?
log(1 + exp(?ydw>xd)),786Other loss functions (e.g., hinge loss, squared loss)can also be used with any of the regularizers dis-cussed in this paper.Our focus is on the regularizer, ?(w).
For highdimensional data such as text, regularization iscrucial to avoid overfitting.1The usual starting points for regularization arethe ?lasso?
(Tibshirani, 1996) and the ?ridge?
(Ho-erl and Kennard, 1970), based respectively on the`1and squared `2norms:?las(w) = ?las?w?1= ?
?j|wj|?rid(w) = ?rid?w?22= ?
?jw2jBoth methods disprefer weights of large magni-tude; smaller (relative) magnitude means a feature(here, a word) has a smaller effect on the predic-tion, and zero means a feature has no effect.2Thehyperparameter ?
in each case is typically tunedon a development dataset.
A linear combinationof ridge and lasso is known as the elastic net (Zouand Hastie, 2005).
The lasso, ridge, and elastic netare three strong baselines in our experiments.3 Group LassoStructured regularizers penalize estimates of w inwhich collections of weights are penalized jointly.For example, in the group lasso (Yuan and Lin,2006), predefined groups of weights (subvectorsof w) are encouraged to either go to zero (asa group) or not (as a group)?this is known as?group sparsity.
?3The variant of group lasso we explore here usesan `1,2norm.
Let g index the G predefined groupsof weights and wgdenote the subvector of w con-taining weights for group g:?glas(w) =?glas?Gg=1?g?wg?2,1A Bayesian interpretation of regularization is as a prioron the weight vector w; in many cases ?
can be under-stood as a log-prior representing beliefs about the model heldbefore exposure to data.
For lasso regression, the prior isa zero-mean Laplace distribution, whereas for ridge regres-sion the prior is a zero-mean Gaussian distribution.
For non-overlapping group lasso, the prior is a two-level hierarchicalBayes model (Figueiredo, 2002).
The Bayesian interpretationof overlapping group lasso is not yet well understood.2The lasso leads to strongly sparse solutions, in whichmany elements of the estimated w are actually zero.
Thisis an attractive property for efficiency and (perhaps) inter-pretability.
The ridge encourages weights to go toward zero,but usually not all the way to zero; for this reason its solutionsare known as ?weakly?
sparse.3Other structured regularizers include the fused lasso(Tibshirani et al, 2005) and the elitist lasso (Kowalski andTorresani, 2009).where ?glasis a hyperparameter tuned on a devel-opment data, and ?gis a group specific weight.Typically the groups are non-overlapping, whichoffers computational advantages, but this need notbe the case (Jacob et al, 2009; Jenatton et al,2011).4 Structured Regularizers for TextPast work applying the group lasso to NLP prob-lems has considered four ways of defining thegroups.
Eisenstein et al (2011) defined groupsof coefficients corresponding to the same inde-pendent variable applied to different (continuous)output variables in multi-output regression.
Mar-tins et al (2011) defined groups based on fea-ture templates used in chunking and parsing tasks.Nelakanti et al (2013) defined groups based on n-gram histories for language modeling.
In each ofthese cases, the groups were defined based on in-formation from feature types alone; given the fea-tures to be used, the groups were known.Here we build on a fourth approach that exploitsstructure in the data.4Yogatama and Smith (2014)introduced the sentence regularizer, which usespatterns of word cooccurrence in the training datato define groups.
We review this method, then ap-ply the idea to three more linguistically informedstructure in text data.4.1 Sentence RegularizerThe sentence regularizer exploits sentence bound-aries in each training document.
The idea is todefine a group gd,sfor every sentence s in everytraining document d. The group contains coeffi-cients for words that occur in its sentence.
Thismeans that a word is a member of one group forevery distinct (training) sentence it occurs in, andthat the regularizer is based on word tokens, nottypes as in the approach of Martins et al (2011)and Nelakanti et al (2013).
The regularizer is:?sen(w) =?Dd=1?Sds=1?d,s?wd,s?2,where Sdis the number of sentences in documentd.
This regularizer results in tens of thousandsto millions of heavily overlapping groups, sincea standard corpus typically contains thousands tomillions of sentences and many words that appearin more than one sentence.4This provides a compelling reason not to view suchmethods in a Bayesian framework: if the regularizer is in-formed by the data, then it does not truly correspond to aprior.787c0,++c1 c4,+c2 c3The actorsc5,++ c8c6 c7,+are fantastic.Figure 1: An example of a parse tree from the Stanford sen-timent treebank, which annotates sentiment at the level ofevery constituent (indicated here by + and ++; no mark-ing indicates neutral sentiment).
The sentence is The ac-tors are fantastic.
Our regularizer constructs nine groups forthis sentence, corresponding to c0, c1, .
.
.
, c8.
gc0consists of5 weights?
?wthe, wactors, ware, wfantastic, w.?, exactly thesame as the group in the sentence regularizer?gc1consistsof 2 words, gc4of 3 words, etc.
Notice that c2, c3, c6, c7,and c8each consist of only 1 word.
The Stanford sentimenttreebank has an annotation of sentiments at the constituentlevel.
As in this example, most constituents are annotated asneutral.If the norm of wgd,sis driven to zero, then thelearner has deemed the corresponding sentence ir-relevant to the prediction.
It is important to pointout that, while the regularizer prefers to zero outthe weights for all words in irrelevant sentences, italso prefers not to zero out weights for words inrelevant sentences.
Since the groups overlap andmay work against each other, the regularizer maynot be able to drive many weights to zero on itsown.
Yogatama and Smith (2014) used a linearcombination of the sentence regularizer and thelasso (a kind of sparse group lasso; Friedman etal., 2010) to also encourage weights of irrelevantword types to go to zero.54.2 Parse Tree RegularizerSentence boundaries are a rather superficial kindof linguistic structure; syntactic parse trees pro-vide more fine-grained information.
We introducea new regularizer, the parse tree regularizer, inwhich groups are defined for every constituent inevery parse of a training data sentence.Figure 1 illustrates the group structures derivedfrom an example sentence from the Stanford sen-timent treebank (Socher et al, 2013).
This regu-larizer captures the idea that phrases might be se-lected as relevant or (in most cases) irrelevant toa task, and is expected to be especially useful insentence-level prediction tasks.The parse-tree regularizer (omitting the group5Formally, this is equivalent to including one additionalgroup for each word type.coefficients and ?)
for one sentence with the parsetree shown in Figure 1 is:?tree(w) =p|wthe|2+ |wactors|2+ |ware|2+ |wfantastic|2+ |w.|2+p|ware|2+ |wfantastic|2+ |w2.|+p|wthe|2+ |wactors|2+p|ware|2+ |wfantastic|2+ |wthe|+ |wactors|+ |ware|+ |wfantastic|+ |w.|The groups have a tree structure, in that assign-ing zero values to the weights in a group corre-sponding to a higher-level constituent implies thesame for those constituents that are dominated byit.
This resembles the tree-guided group lasso inKim and Xing (2008), although the leaf nodes intheir tree represent tasks in multi-task regression.Of course, in a corpus there are many parse trees(one per sentence, so the number of parse trees isthe number of sentences).
The parse-tree regular-izer is:?tree(w) =?Dd=1?Sds=1?Cd,sc=1?d,s,c?wd,s,c?2,where ?d,s,c= ?glas?
?size(gd,s,c), d rangesover (training) documents and c ranges over con-stituents in the parse of sentence s in docu-ment d. Similar to the sentence regularizer,the parse-tree regularizer operates on word to-kens.
Note that, since each word token is it-self a constituent, the parse tree regularizer in-cludes terms just like the lasso naturally, penal-izing the absolute value of each word?s weightin isolation.
For the lasso-like penalty on eachword, instead of defining the group weights to be1 ?
the number of tokens for each word type, wetune one group weight for all word types on a de-velopment data.
As a result, besides ?glas, we havean additional hyperparameter, denoted by ?las.To gain an intuition for this regularizer, considerthe case where we apply the penalty only for a sin-gle tree (sentence), which for ease of exposition isassumed not to use the same word more than once(i.e., ?x?
?= 1).
Because it instantiates the tree-structured group lasso, the regularizer will requirebigger constituents to be ?included?
(i.e., theirwords given nonzero weight) before smaller con-stituents can be included.
The result is that somewords may not be included.
Of course, in somesentences, some words will occur more than once,and the parse tree regularizer instantiates groupsfor constituents in every sentence in the trainingcorpus, and these groups may work against eachother.
The parse tree regularizer should therefore788be understood as encouraging group behavior ofsyntactically grouped words, or sharing of infor-mation by syntactic neighbors.In sentence level prediction tasks, such assentence-level sentiment analysis, it is known thatmost constituents (especially those that corre-spond to shorter phrases) in a parse tree are un-informative (neutral sentiment).
This was verifiedby Socher et al (2013) when annotating phrasesin a sentence for building the Stanford sentimenttreebank.
Our regularizer incorporates our priorexpectation that most constituents should have noeffect on prediction.4.3 LDA RegularizerAnother type of structure to consider is topics.For example, if we want to predict whether a pa-per will be cited or not (Yogatama et al, 2011),the model can perform better if it knows before-hand the collections of words that represent certainthemes (e.g., in ACL papers, these might includemachine translation, parsing, etc.).
As a result,the model can focus on which topics will increasethe probability of getting citations, and penalizeweights for words in the same topic together, in-stead of treating each word separately.We do this by inferring topics in the trainingcorpus by estimating the latent Dirichlet aloca-tion (LDA) model (Blei et al, 2003)).
Note thatLDA is an unsupervised method, so we can in-fer topical structures from any collection of docu-ments that are considered related to the target cor-pus (e.g., training documents, text from the web,etc.).
This contrasts with typical semi-supervisedlearning methods for text categorization that com-bine unlabeled and labeled data within a genera-tive model, such as multinomial na?
?ve Bayes, viaexpectation-maximization (Nigam et al, 2000) orsemi-supervised frequency estimation (Su et al,2011).
Our method does not use unlabeled datato obtain more training documents or estimate thejoint distributions of words better, but it allows theuse of unlabeled data to induce topics.
We leavecomparison with other semi-supervised methodsfor future work.There are many ways to associate inferred top-ics with group structure.
In our experiments, wechoose the R most probable words given a topicand create a group for them.6The LDA regular-6Another possibility is to group the smallest set of wordswhose total probability given a topic amounts to P (e.g.,0.99).
mass of a topic.
Preliminary experiments found thisizer can be written as:?lda(w) =?Kk=1?k?wk?2,where k ranges over the K topics.
Similar to ourearlier notations, wkcorresponds to the subvec-tor of w such that the corresponding features arepresent in topic k. Note that in this case we canalso have overlapping groups, since words can ap-pear in the top R of many topics.k = 1 k = 2 k = 3 k = 4soccer injury physics mondaystriker knee gravity tuesdaymidfielder ligament moon aprilgoal shoulder sun junedefender cruciate relativity sundayTable 1: A toy example of K = 4 topics.
The top R = 5words in each topics are displayed.
The LDA regularizerwill construct four groups from these topics.
The first groupis ?wsoccer, wstriker, wmidfielder, wgoal, wdefender?, the sec-ond group is ?winjury, wknee, wligament, wshoulder, wcruciate?,etc.
In this example, there are no words occurring in the topR of more than one topic, but that need not be the case ingeneral.To gain an intuition for this regularizer, considerthe toy example in Table 1. the case where wehave K = 4 topics and we select R = 5 top wordsfrom each topic.
Supposed that we want to clas-sify whether an article is a sports article or a sci-ence article.
The regularizer might encourage theweights for the fourth topics?
words toward zero,since they are less useful for the task.
Addition-ally, the regularizer will penalize words in each ofthe other three groups collectively.
Therefore, if(for example) ligament is deemed a useful featurefor classifying an article to be about sports, thenthe other words in that topic will have a smaller ef-fective penalty for getting nonzero weights?evenweights of the opposite sign as wligament.
It is im-portant to distinguish this from unstructured reg-ularizers such as the lasso, which penalize eachword?s weight on its own without regard for re-lated word types.Unlike the parse tree regularizer, the LDA regu-larizer is not tree structured.
Since the lasso-likepenalty does not occur naturally in a non tree-structured regularizer, we add an additional lassopenalty for each word type (with hyperparameter?las) to also encourage weights of irrelevant wordsto go to zero.
Our LDA regularizer is an instanceof sparse group lasso (Friedman et al, 2010).not to work well.7894.4 Brown Cluster RegularizerBrown clustering is a commonly used unsuper-vised method for grouping words into a hierarchyof clusters (Brown et al, 1992).
Because it useslocal information, it tends to discover words withsimilar syntactic behavior, though semantic group-ings are often evident, especially at the more fine-grained end of the hierarchy.We incorporate Brown clusters into a regular-izer in a similar way to the topical word groupsinferred using LDA in ?4.3, but here we make useof the hierarchy.
Specifically, we construct tree-structured groups, one per cluster (i.e., one pernode in the hierarchy).
The Brown cluster regu-larizer is:?brown(w) =?Nv=1?v?wv?2,where v ranges over the N nodes in the Browncluster tree.
As a tree structured regularizer, thisregularizer enforces constraints that a node v?sgroup is given nonzero weights only if those nodesthat dominate v (i.e., are on a path from v to theroot) have their groups selected.Consider a similar toy example to the LDA reg-ularizer (sports vs. science) and the hierarchicalclustering of words in Figure 2.
In this case, theBrown cluster regularizer will create 17 groups,one for every node in the clustering tree.
The regu-larizer for this tree (omitting the group coefficientsand ?)
is:?brown(w) =?7i=0?wvi?2+ |wgoal|+ |wstriker|+ |wmidfielder|+ |wknee|+ |winjury|+ |wgravity|+ |wmoon|+ |wsun|The regularizer penalizes words in a cluster to-gether, exploiting discovered syntactic related-ness.
Additionally, the regularizer can zero outweights of words corresponding to any of the in-ternal nodes, such as v7if the words monday andsunday are deemed irrelevant to prediction.Note that the regularizer already includes termslike the lasso naturally.
Similar to the parsetree regularizer, for the lasso-like penalty on eachword, we tune one group weight for all word typeson a development data with a hyperparameter ?las.A key difference between the Brown clusterregularizer and the parse tree regularizer is thatthere is only one tree for Brown cluster regularizer,whereas the parse tree regularizer can have mil-lions (one per sentence in the training data).
Thev0v1 v5v2 v4v3 v10v8 v9goal strikermidfielderv11 v12knee injuryv6 v7v13 v14moon sunv15 v16monday sundayFigure 2: An illustrative example of Brown clusters for N =9.
The Brown cluster regularizer constructs 17 groups, oneper node in for this tree, v0, v1, .
.
.
, v16.
v0contains 8 words,v1contains 5, etc.
Note that the leaves, v8, v9, .
.
.
, v16, eachcontain one word.LDA and Brown cluster regularizers offer ways toincorporate unlabeled data, if we believe that theunlabeled data can help us infer better topics orclusters.
Note that the processes of learning topicsor clusters, or parsing training data sentences, area separate stage that precedes learning our predic-tive model.5 LearningThere are many optimization methods for learn-ing models with structured regularizers, particu-lary group lasso (Jacob et al, 2009; Jenatton et al,2011; Chen et al, 2011; Qin and Goldfarb, 2012;Yuan et al, 2013).
We choose the optimizationmethod of Yogatama and Smith (2014) since ithandles millions of overlapping groups effectively.The method is based on the alternating directionsmethod of multipliers (ADMM; Hestenes, 1969;Powell, 1969).
We review it here in brief, for com-pleteness, and show how it can be applied to tree-structured regularizers (such as the parse tree andBrown cluster regularizers in ?4) in particular.Our learning problem is, generically:minw?
(w) +?Dd=1L(xd,w, yd).Separating the lasso-like penalty for each wordtype from our group regularizers, we can rewritethis problem as:minw,v?las(w) + ?glas(v) +?Dd=1L(xd,w, yd)s.t.
v = Mwwhere v consists of copies of the elements ofw.
Notice that we work directly on w insteadof the copies for the lasso-like penalty, since itdoes not have overlaps and has its own hyper-parameters ?las.
For the remaining groups withsize greater than one, we create copies v of size790L =?Gg=1size(g).
M ?
{0, 1}L?Vis a ma-trix whose 1s link elements of w to their copies.7We now have a constrained optimization prob-lem, from which we can create an augmented La-grangian problem; let u be the Lagrange variables:?las(w) + ?glas(v) + L(w)+ u>(v ?Mw) +?2?v ?Mw?22ADMM proceeds by iteratively updating eachof w, v, and u, amounting to the following sub-problems:minw?las(w) + L(w)?
u>Mw +?2?v ?Mw?22(1)minv?glas(v) + u>v +?2?v ?Mw?22(2)u = u + ?
(v ?Mw) (3)Yogatama and Smith (2014) show that Eq.
1can be rewritten in a form quite similar to `2-regularized loss minimization.8Eq.
2 is the proximal operator of1?
?glasap-plied to Mw ?u?.
As such, it depends on theform of M. Note that when applied to the col-lection of ?copies?
of the parameters, v, ?glasnolonger has overlapping groups.
Defined Mgasthe rows of M corresponding to weight copies as-signed to group g. Let zg, Mgw ?ug?.
De-note ?g= ?glas?size(g).
The problem can besolved by applying the proximal operator used innon-overlapping group lasso to each subvector:vg= prox?glas,?g?(zg)=??
?0 if ?zg?2??g??zg?2??g?
?zg?2zgotherwise.For a tree structured regularizer, we can getspeedups by working from the root node towardsthe leaf nodes when applying the proximal oper-ator in the second step.
If g is a node in a treewhich is driven to zero, all of its children h thathas ?h?
?gwill also be driven to zero.Eq.
3 is a simple update of the dual variable u.Algorithm 1 summarizes our learning procedure.97For the parse tree regularizer, L is the sum, over alltraining-data word tokens t, of the number of constituents tbelongs to.
For the LDA regularizer, L = R ?
K. For theBrown cluster regularizer, L = V ?
1.8The difference lies in that the squared `2norm in thepenalty penalizes the difference between w and a vector thatdepends on the current values of u and v. This does not affectthe algorithm or its convergence in any substantive way.9We use relative changes in the `2norm of the parametervector w as our convergence criterion (threshold of 10?3),and set the maximum number of iterations to 100.
Other cri-teria can also be used.Algorithm 1 ADMM for overlapping group lassoInput: augmented Lagrangian variable ?, regularizationstrengths ?glasand ?laswhile stopping criterion not met dow = arg minw?las(w)+L(w)+?2PVi=1Ni(wi?
?i)2for g = 1 to G dovg= prox?glas,?g?
(zg)end foru = u + ?
(v ?Mw)end while6 Experiments6.1 DatasetsWe use publicly available datasets to evaluate ourmodel described in more detail below.Topic classification.
We consider four binarycategorization tasks from the 20 Newsgroupsdataset.10Each task involves categorizing adocument according to two related categories:comp.sys: ibm.pc.hardware vs. mac.hardware;rec.sport: baseball vs. hockey; sci: med vs. space;and alt.atheism vs. soc.religion.christian.Sentiment analysis.
One task in sentiment anal-ysis is predicting the polarity of a piece of text, i.e.,whether the author is favorably inclined toward a(usually known) subject of discussion or proposi-tion (Pang and Lee, 2008).
Sentiment analysis,even at the coarse level of polarity we considerhere, can be confused by negation, stylistic use ofirony, and other linguistic phenomena.
Our sen-timent analysis datasets consist of movie reviewsfrom the Stanford sentiment treebank (Socher etal., 2013),11and floor speeches by U.S. Congress-men alongside ?yea?/?nay?
votes on the bill underdiscussion (Thomas et al, 2006).12For the Stan-ford sentiment treebank, we only predict binaryclassifications (positive or negative) and excludeneutral reviews.Text-driven forecasting.
Forecasting from textrequires identifying textual correlates of a re-sponse variable revealed in the future, most ofwhich will be weak and many of which will bespurious (Kogan et al, 2009).
We consider twosuch problems.
The first one is predicting whethera scientific paper will be cited or not within threeyears of its publication (Yogatama et al, 2011);10http://qwone.com/?jason/20Newsgroups11http://nlp.stanford.edu/sentiment/12http://www.cs.cornell.edu/?ainur/data.html791Dataset D # Dev.
# Test V20Nscience 952 235 790 30,154sports 958 239 796 20,832relig.
870 209 717 24,528comp.
929 239 777 20,868Sent.movie 6,920 872 1,821 17,576vote 1,175 257 860 24,508Fore.science 3,207 280 539 42,702bill 37,850 7,341 6,571 10,001Table 2: Descriptive statistics about the datasets.the dataset comes from the ACL Anthology andconsists of research papers from the Associationfor Computational Linguistics and citation data(Radev et al, 2009).
The second task is predictingwhether a legislative bill will be recommended bya Congressional committee (Yano et al, 2012).13Table 2 summarizes statistics about the datasetsused in our experiments.
In total, we evaluate ourmethod on eight binary classification tasks.6.2 SetupIn all our experiments, we use unigram featuresplus an additional bias term which is not regu-larized.
We compare our new regularizers withstate-of-the-art methods for document classifica-tion: lasso, ridge, and elastic net regularization, aswell as the sentence regularizer discussed in ?4.1(Yogatama and Smith, 2014).14We parsed all corpora using the Berkeley parser(Petrov and Klein, 2007).15For the LDA regular-izers, we ran LDA16on training documents withK = 1, 000 and R = 10.
For the Brown clusterregularizers, we ran Brown clustering17on train-ing documents with 5, 000 clusters for the topicclassification and sentiment analysis datasets, and1, 000 for the larger text forecasting datasets (sincethey are bigger datasets that took more time).13http://www.ark.cs.cmu.edu/bills14Hyperparameters are tuned on a separate develop-ment dataset, using accuracy as the evaluation crite-rion.
For lasso and ridge models, we choose ?
from{10?2, 10?1, 1, 10, 102, 103}.
For elastic net, we performgrid search on the same set of values as ridge and lassoexperiments for ?ridand ?las.
For the sentence, Browncluster, and LDA regularizers, we perform grid search onthe same set of values as ridge and lasso experiments for?, ?glas, ?las.
For the parse tree regularizer, because thereare many more groups than other regularizers, we choose?glasfrom {10?4, 10?3, 10?2, 10?1, 10}, ?
and ?lasfromthe same set of values as ridge and lasso experiments.
If thereis a tie on development data we choose the model with thesmallest number of nonzero weights.15https://code.google.com/p/berkeleyparser/16http://www.cs.princeton.edu/?blei/lda-c/17https://github.com/percyliang/brown-cluster6.3 ResultsTable 3 shows the results of our experiments onthe eight datasets.
The results demonstrate the su-periority of structured regularizers.
One of themachieved the best result on all but one dataset.18Itis also worth noting that in most cases all variantsof the structured regularizers outperformed lasso,ridge, and elastic net.
In four cases, the new regu-larizers in this paper outperform the sentence reg-ularizer.We can see that the parse tree regularizer per-formed the best for the movie review dataset.
Thetask is to predict sentence-level sentiment, so eachtraining example is a sentence.
Since constituent-level annotations are available for this dataset, weonly constructed groups for neutral constituents(i.e., we drive neutral constituents to zero duringtraining).
It has been shown that syntactic in-formation is helpful for sentence-level predictions(Socher et al, 2013), so the parse tree regularizeris naturally suitable for this task.The Brown cluster and LDA regularizers per-formed best for the forecasting scientific articlesdataset.
The task is to predict whether an articlewill be cited or not within three years after publi-cation.
Regularizers that exploit the knowledge ofsemantic relations (e.g., topical categories), suchas the Brown cluster and LDA regularizers, aretherefore suitable for this type of prediction.Table 4 shows model sizes obtained by eachof the regularizers for each dataset.
While lassoprunes more aggressively, it almost always per-forms worse.
Our structured regularizers wereable to obtain a significantly smaller model (27%,34%, 19% as large on average for parse tree,Brown, and LDA regularizers respectively) com-pared to the ridge model.Topic and cluster features.
Another way to in-corporate LDA topics and Brown clusters into alinear model is by adding them as additional fea-tures.
For the 20N datasets, we also ran lasso,ridge, and elastic net with additional LDA topicand Brown cluster features.19Note that these newbaselines use more features than our model.
Wecan also add these additional features to our model18This ?bill?
dataset, where they offered no improvement,is the largest by far (37,850 documents), and therefore theone where regularizers should matter the least.
Note that thedifferences are small across regularizers for this dataset.19For LDA, we took the top 10 words in a topic as a feature.For Brown clusters, we add a cluster as an additional featureif its size is less than 50.792Task DatasetAccuracy (%)m.f.c.
lasso ridge elastic sentence parse Brown LDA20Nscience 50.13 90.63 91.90 91.65 96.20 92.66 93.04 93.67sports 50.13 91.08 93.34 93.71 95.10 93.09 93.71 94.97religion 55.51 90.52 92.47 92.47 92.75 94.98 92.89 93.03computer 50.45 85.84 86.74 87.13 90.86 89.45 86.36 88.42Sentimentmovie 50.08 78.03 80.45 80.40 80.72 81.55 80.34 78.36vote 58.37 73.14 72.79 72.79 73.95 73.72 66.86 73.14Forecastingscience 50.28 64.00 66.79 66.23 67.71 66.42 69.02 69.39bill 87.40 88.36 87.70 88.48 88.11 87.98 88.20 88.27Table 3:Classificationaccuracies onvarious datasets.?m.f.c.?
is themost frequentclass baseline.Boldface showsbest results.Task DatasetModel size (%)m.f.c.
lasso ridge elastic sentence parse Brown LDA20Nscience - 1 100 34 12 2 42 9sports - 2 100 15 3 3 16 9religion - 0.3 100 48 94 72 41 15computer - 2 100 24 10 5 24 8Sentimentmovie - 10 100 54 83 87 59 12vote - 2 100 44 6 2 30 4Forecastingscience - 31 100 43 99 9 50 90bill - 7 100 7 8 37 7 7Table 4: Modelsizes (percentagesof nonzerofeatures in theresulting models)on variousdatasets.Dataset+ LDA features LDAlasso ridge elastic reg.science 90.63 91.90 91.90 93.67sports 91.33 93.47 93.84 94.97religion 91.35 92.47 91.35 93.03computer 85.20 86.87 86.35 88.42Dataset+ Brown features Brownlasso ridge elastic reg.science 86.96 90.51 91.14 93.04sports 82.66 88.94 85.43 93.71religion 94.98 96.93 96.93 92.89computer 55.72 96.65 67.57 86.36Table 5: Classification accuracies on the 20N datasets forlasso, ridge, and elastic net models with additional LDA fea-tures (top) and Brown cluster features (bottom).
The last col-umn shows structured regularized models from Table 3.and treat them as regular features (i.e., they donot belong to any groups and are regularized withstandard regularizer such as the lasso penalty).The results in Table 5 show that for these datasets,models that incorporate this information throughstructured regularizers outperformed models thatencode this information as additional features in4 out 4 of cases (LDA) and 2 out of 4 cases(Brown).
Sparse models with Brown clusters ap-pear to overfit badly; recall that the clusters werelearned on only the training data?clusters froma larger dataset would likely give stronger re-sults.
Of course, better performance might alsobe achieved by incorporating new features as wellas using structured regularizers.6.4 ExamplesTo gain an insight into the models, we inspectgroup sparsity patterns in the learned models bylooking at the parameter copies v. This lets us seewhich groups are considered important (i.e., ?se-lected?
vs.
?removed?).
For each of the proposedregularizers, we inspect the model a task in whichit performed well.For the parse tree regularizer, we inspect themodel for the 20N:religion task.
We observed thatthe model included most of the sentences (rootnode groups), but in some cases removed phrasesfrom the parse trees, such as ozzy osbourne in thesentence ozzy osbourne , ex-singer and main char-acter of the black sabbath of good ole days past ,is and always was a devout catholic .For the LDA regularizer, we inspect zero andnonzero groups (topics) in the forecasting scien-tific articles task.
In this task, we observed that642 out of 1,000 topics are driven to zero byour model.
Table 6 shows examples of zero andnonzero topics for the dev.-tuned hyperparametervalues.
We can see that in this particular case, themodel kept meaningful topics such as parsing andspeech processing, and discarded general topicsthat are not correlated with the content of the pa-pers (e.g., acknowledgment, document metadata,equation, etc.).
Note that most weights for non-selected groups, even in w, are near zero.For the Brown cluster regularizer, we inspectthe model from the 20N:science task.
771 outof 5,775 groups were driven to zero for the bestmodel tuned on the development set.
Examplesof zero and nonzero groups are shown in Ta-ble 7.
Similar to the LDA example, the groupsthat were driven to zero tend to contain genericwords that are not relevant to the predictions.
Wecan also see the tree structure effect in the regu-larizer.
The group {underwater, industrial} was793= 0?acknowledgment?
: workshop arpa program session darpa research papers spoken technology systems?document metadata?
: university references proceedings abstract work introduction new been research both?equation?
: pr w h probability wi gram context z probabilities complete?translation?
: translation target source german english length alignment hypothesis translations position6= 0?translation?
: korean translation english rules sentences parsing input evaluation machine verb?speech processing?
: speaker identification topic recognition recognizer models acoustic test vocabulary independent?parsing?
: parser parsing probabilistic prediction parse pearl edges chart phase theory?classification?
: documents learning accuracy bayes classification wt document naive method selectionTable 6: Examples of LDA regularizer-removed and -selected groups (in v) in the forecasting scientific articles dataset.
Wordswith weights (in w) of magnitude greater than 10?3are highlighted in red (not cited) and blue (cited).= 0underwater industrialspotted hit reaped rejuvenated destroyed stretched undertake shake runseeing developing tingles diminishing launching finding investigating receivingmaintainingadds engage explains builds6= 0failure reproductive ignition reproductioncyanamid planetary nikola fertility astronomical geophysical # lunar cometarysupplying astronauticalmagnetic atmosphericstd underwater hpr wordscan exclusively aneutronic industrial peoples obsessivecongenital rare simple bowel hereditary breastTable 7: Examples of Brownregularizer-removed and-selected groups (in v) in the20N:science task.
# denotesany numeral.
Words withweights (in w) of magnitudegreater than 10?3arehighlighted in red (space) andblue (medical).driven to zero, but not once it combined with otherwords such as hpr, std, obsessive.
Note that weran Brown clustering only on the training docu-ments; running it on a larger collection of (unla-beled) documents relevant to the prediction task(i.e., semi-supervised learning) is worth exploringin future work.7 Related and Future WorkOverall, our results demonstrate that linguisticstructure in the data can be used to improve bag-of-words models, through structured regulariza-tion.
State-of-the-art approaches to some of theseproblems have used additional features and repre-sentations (Yessenalina et al, 2010; Socher et al,2013).
For example, for the vote sentiment analy-sis datasets, latent variable models of Yessenalinaet al (2010) achieved a superior result of 77.67%.To do so, they sacrificed convexity and had to relyon side information for initialization.
Our exper-imental focus has been on a controlled compari-son between regularizers for a fixed model family(the simplest available, linear with bag-of-wordsfeatures).
However, the improvements offered byour regularization methods can be applied in fu-ture work to other model families with more care-fully engineered features, metadata features (espe-cially important in forecasting), latent variables,etc.
In particular, note that other kinds of weights(e.g., metadata) can be penalized conventionally,or incorporated into the structured regularizationwhere it makes sense to do so (e.g., n-grams, as inNelakanti et al, 2013).8 ConclusionWe introduced three data-driven, linguisticallyinformed structured regularizers based on parsetrees, topics, and hierarchical word clusters.
Weempirically showed that models regularized us-ing our methods consistently outperformed stan-dard regularizers that penalize features in isolationsuch as lasso, ridge, and elastic net on a rangeof datasets for various text prediction problems:topic classification, sentiment analysis, and fore-casting.AcknowledgmentsThe authors thank Brendan O?Connor for helpwith visualization and three anonymous review-ers for helpful feedback on an earlier draft of thispaper.
This research was supported in part bycomputing resources provided by a grant from thePittsburgh Supercomputing Center, a Google re-search award, and the Intelligence Advanced Re-search Projects Activity via Department of In-terior National Business Center contract numberD12PC00347.
The U.S. Government is authorizedto reproduce and distribute reprints for Govern-mental purposes notwithstanding any copyrightannotation thereon.
The views and conclusionscontained herein are those of the authors andshould not be interpreted as necessarily represent-ing the official policies or endorsements, either ex-pressed or implied, of IARPA, DoI/NBC, or theU.S.
Government.794ReferencesDavid M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet alocation.
Journal of Ma-chine Learning Research, 3:993?1022.Peter F. Brown, Peter V. deSouza, Robert L. Mer-cer, Vincent J. Della Pietra, and Jenifer C. Lai.1992.
Class-based n-gram models of natural lan-guage.
Computational Linguistics, 18:467?479.Stanley F. Chen and Ronald Rosenfeld.
2000.
Asurvey of smoothing techniques for me models.IEEE Transactions on Speech and Audio Process-ing, 8(1):37?50.Xi Chen, Qihang Lin, Seyoung Kim, Jaime G. Car-bonell, and Eric P. Xing.
2011.
Smoothing prox-imal gradient method for general structured sparselearning.
In Proc.
of UAI.Jacob Eisenstein, Noah A. Smith, and Eric P. Xing.2011.
Discovering sociolinguistic associations withstructured sparsity.
In Proc.
of ACL.Mario A. T. Figueiredo.
2002.
Adaptive sparsenessusing Jeffreys?
prior.
In Proc.
of NIPS.Jerome Friedman, Trevor Hastie, and Robert Tibshiran.2010.
A note on the group lasso and a sparse grouplasso.
Technical report, Stanford University.Xavier Glorot, Antoine Bordes, and Yoshua Bengio.2011.
Domain adaptation for large-scale sentimentclassification: A deep learning approach.
In Proc.
ofICML.Magnus R. Hestenes.
1969.
Multiplier and gradientmethods.
Journal of Optimization Theory and Ap-plications, 4:303?320.Arthur E. Hoerl and Robert W. Kennard.
1970.
Ridgeregression: Biased estimation for nonorthogonalproblems.
Technometrics, 12(1):55?67.Laurent Jacob, Guillaume Obozinski, and Jean-Philippe Vert.
2009.
Group lasso with overlap andgraph lasso.
In Proc.
of ICML.Rodolphe Jenatton, Jean-Yves Audibert, and Fran-cis Bach.
2011.
Structured variable selectionwith sparsity-inducing norms.
Journal of MachineLearning Research, 12:2777?2824.Mahesh Joshi, Dipanjan Das, Kevin Gimpel, andNoah A. Smith.
2010.
Movie reviews and rev-enues: An experiment in text regression.
In Proc.of NAACL.Seyoung Kim and Eric P. Xing.
2008.
Feature selec-tion via block-regularized regression.
In Proc.
ofUAI.Shimon Kogan, Dimitry Levin, Bryan R. Routledge,Jacob S. Sagi, and Noah A. Smith.
2009.
Predictingrisk from financial reports with regression.
In Proc.of HLT-NAACL.Matthieu Kowalski and Bruno Torresani.
2009.
Spar-sity and persistence: mixed norms provide simplesignal models with dependent coefficients.
Signal,Image and Video Processing, 3(3):251?0264.Xiaolei Lv, Guoan Bi, and Chunru Wan.
2011.
Thegroup lasso for stable recovery of block-sparse sig-nal representations.
IEEE Transactions on SignalProcessing, 59(4):1371?1382.Andre F. T. Martins, Noah A. Smith, Pedro M. Q.Aguiar, and Mario A. T. Figueiredo.
2011.
Struc-tured sparsity in structured prediction.
In Proc.
ofEMNLP.Anil Nelakanti, Cedric Archambeau, Julien Mairal,Francis Bach, and Guillaume Bouchard.
2013.Structured penalties for log-linear language models.In Proc.
of EMNLP.Kamal Nigam, Andrew McCallum, Sebastian Thrun,and Tom Mitchell.
2000.
Text classification from la-beled and unlabeled documents using em.
MachineLearning, 39(2-3):103?134.Bo Pang and Lilian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and Trends in In-formation Retrieval, 2(1?2):1?135.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Proc.
of HLT-NAACL.M.
J. D. Powell.
1969.
A method for nonlinear con-straints in minimization problems.
In R. Fletcher,editor, Optimization, pages 283?298.
AcademicPress.Zhiwei (Tony) Qin and Donald Goldfarb.
2012.
Struc-tured sparsity via alternating direction methods.Journal of Machine Learning Research, 13:1435?1468.Dragomir R. Radev, Pradeep Muthukrishnan, and Va-hed Qazvinian.
2009.
The ACL anthology net-work corpus.
In Proc.
of ACL Workshop on NaturalLanguage Processing and Information Retrieval forDigital Libraries.Richard Socher, Alex Perelygin, Jean Wu, JasonChuang, Chris Manning, Andrew Ng, and ChrisPotts.
2013.
Recursive deep models for semanticcompositionality over a sentiment treebank.
In Proc.of EMNLP.Jiang Su, Jelber Sayyad-Shirabad, and Stan Matwin.2011.
Large scale text classication using semi-supervised multinomial naive Bayes.
In Proc.
ofICML.Matt Thomas, Bo Pang, and Lilian Lee.
2006.
Get outthe vote: Determining support or opposition fromcongressional floor-debate transcripts.
In Proc.
ofEMNLP.795Robert Tibshirani, Michael Saunders, Saharon Ros-set, Ji Zhu, and Keith Knight.
2005.
Sparsity andsmoothness via the fused lasso.
Journal of RoyalStatistical Society B, 67(1):91?108.Robert Tibshirani.
1996.
Regression shrinkage andselection via the lasso.
Journal of Royal StatisticalSociety B, 58(1):267?288.Tae Yano, Noah A. Smith, and John D. Wilkerson.2012.
Textual predictors of bill survival in congres-sional committees.
In Proc.
of NAACL.Ainur Yessenalina, Yisong Yue, and Claire Cardie.2010.
Multi-level structured models for documentsentiment classification.
In Proc.
of EMNLP.Dani Yogatama and Noah A. Smith.
2014.
Making themost of bag of words: Sentence regularization withalternating direction method of multipliers.
In Proc.of ICML.Dani Yogatama, Michael Heilman, Brendan O?Connor,Chris Dyer, Bryan R. Routledge, and Noah A.Smith.
2011.
Predicting a scientific community?sresponse to an article.
In Proc.
of EMNLP.Ming Yuan and Yi Lin.
2006.
Model selectionand estimation in regression with grouped variables.Journal of the Royal Statistical Society, Series B,68(1):49?67.Lei Yuan, Jun Liu, and Jieping Ye.
2013.
Efficientmethods for overlapping group lasso.
IEEE Trans-actions on Pattern Analysis and Machine Intelli-gence, 35(9):2104?2116.Hui Zou and Trevor Hastie.
2005.
Regularization andvariable selection via the elastic net.
Journal of theRoyal Statistical Society, Series B, 67:301?320.796
