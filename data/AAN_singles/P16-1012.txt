Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 118?129,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsLanguage Transfer Learning for Supervised Lexical SubstitutionGerold Hintz and Chris BiemannResearch Training Group AIPHES / FG Language TechnologyComputer Science Department, Technische Universit?t Darmstadt{hintz,biem}@lt.informatik.tu-darmstadt.deAbstractWe propose a framework for lexical sub-stitution that is able to perform transferlearning across languages.
Datasets forthis task are available in at least threelanguages (English, Italian, and German).Previous work has addressed each of thesetasks in isolation.
In contrast, we regardthe union of three shared tasks as a com-bined multilingual dataset.
We show thata supervised system can be trained effec-tively, even if training and evaluation dataare from different languages.
Successfultransfer learning between languages sug-gests that the learned model is in fact in-dependent of the underlying language.
Wecombine state-of-the-art unsupervised fea-tures obtained from syntactic word em-beddings and distributional thesauri in asupervised delexicalized ranking system.Our system improves over state of the artin the full lexical substitution task in allthree languages.1 IntroductionThe lexical substitution task is defined as replac-ing a target word in a sentence context with asynonym, which does not alter the meaning ofthe utterance.
Although this appears easy to hu-mans, automatically performing such a substitu-tion is challenging, as it implicitly addresses theproblem of both determining semantically simi-lar substitutes, as well as resolving the ambiguityof polysemous words.
In fact, lexical substitutionwas originally conceived as an extrinsic evaluationof Word Sense Disambiguation (WSD) when firstproposed by McCarthy & Navigli (2007).
How-ever, a system capable of replacing words by ap-propriate meaning-preserving substitutes can beutilized in downstream tasks that require para-phrasing of input text.
Examples of such use casesinclude text simplification, text shortening, andsummarization.
Furthermore, lexical substitutioncan be regarded as an alternative to WSD in down-stream tasks requiring word disambiguation.
Forexample, it was successfully applied in SemanticTextual Similarity (B?r et al, 2012).
A given listof substitution words can be regarded as a vectorrepresentation modeling the meaning of a word incontext.
As opposed to WSD systems, this is notreliant on a predefined sense inventory, and there-fore does not have to deal with issues of cover-age, or sense granularity.
On the other hand, per-forming lexical substitution is more complex thanWSD, as a system has to both generate and rank alist of substitution candidates per instance.Over the last decade, a number of shared tasksin lexical substitution has been organized and awide range of methods have been proposed.
Al-though many approaches are in fact language-independent, most existing work is tailored to asingle language and dataset.
In this work, weinvestigate lexical substitution as a multilingualtask, and report experimental results for English,German and Italian datasets.
We consider a su-pervised approach to lexical substitution, whichcasts the task as a ranking problem (Szarvas et al,2013b).
We adapt state-of-the-art unsupervisedfeatures (Biemann and Riedl, 2013; Melamudet al, 2015a) in a delexicalized ranking frame-work and perform transfer learning experimentsby training a ranker model from a different lan-guage.
Finally, we demonstrate the utility of ag-gregating data from different languages and trainour model on this single multilingual dataset.
Weare able to improve the state of the art for the fulltask on all datasets.The remainder of this paper is structured as fol-lows.
In Section 2 we elaborate on the lexical sub-118stitution task and datasets.
Section 3 shows relatedwork of systems addressing each of these tasks.In Section 4 we describe our method for buildinga supervised system capable of transfer learning.Section 5 shows our experimental results and dis-cussion.
Finally in Section 6 we give a conclusionand outlook to future work.2 Lexical substitution datasets andevaluationThe lexical substitution task was first definedat SemEval 2007 (McCarthy and Navigli, 2007,"SE07").
A lexical sample of target word is se-lected from different word classes (nouns, verbs,and adjectives).
Through annotation, a set of validsubstitutes was collected for 10-20 contexts pertarget.
Whereas in the original SE07 task, anno-tators were free to provide ?up to three, but allequally good?
substitutes, later tasks dropped thisrestriction.
Substitutes were subsequently aggre-gated by annotator frequency, creating a rankingof substitutes.
The use of SE07 has become ade-facto standard for system comparison, howeverequivalent datasets have been produced for otherlanguages.
Evalita 2009 posed a lexical substitu-tion task for Italian (Toral, 2009, "EL09").
Par-ticipants were free to obtain a list of substitutioncandidates in any way, most commonly ItalianWordNet1was used.
A WeightedSense baselineprovided by the organizers proved very strong, asall systems scored below it.
This baseline is ob-tained by aggregating differently weighted seman-tic relations from multiple human-created lexicalresources (Ruimy et al, 2002).
A German ver-sion of the lexical substitution task was organizedat GermEval 2015 (Cholakov et al, 2014; Milleret al, 2015, "GE15").
Likewise, WeightedSensewas able to beat both of two participating systemsin oot evaluations (Miller et al, 2015).A variation for cross-lingual lexical substitutionwas proposed by Mihalcea et al (2010), in whichsubstitute words are required in a different lan-guage than the source sentence.
The sentence con-text as well as the target word were given in En-glish, whereas the substitute words should be pro-vided in Spanish (annotators were fluent in bothlanguages).
This variant is motivated by direct ap-plication in Machine Translation systems, or as anaid for human-based translation.
There also ex-1Italian WordNet has later been migrated into MultiWord-Net (MWN), which is used in this work.ists a larger crowd-sourced dataset of 1012 nouns(Biemann, 2013, "TWSI"), as well as an all-wordsdataset in which all words in each sentence areannotated with lexical expansions (Kremer et al,2014).
Evaluation of lexical substitution adheresto metrics defined by SE07 (McCarthy and Nav-igli, 2007), who provide two evaluation settings2;best evaluating only a system?s ?best guess?
of asingle target substitute and oot, an unordered eval-uation of up to ten substitutes.
Thater et.
al (2009)proposed to use Generalized Average Precision(GAP), to compare an output ranking rather thanunordered sets of substitutes.Dataset comparison The proposed lexicalsubstitution datasets (SE07, EL09, GE15) differin their degree of ambiguity of target items.
If adataset contains mostly target words that are un-ambiguous, substitution lists of different instancesof the same target are similar, despite occurring indifferent context.
We can quantify this degree ofvariation by measuring the overlap of gold substi-tutes of each target across all contexts.
For this,we adapt the pairwise agreement (PA) metric de-fined by McCarthy & Navigli (2009).
Instead ofinter-annotator agreement we measure agreementacross different context instances.
Let T be a setof lexical target words, and D dataset of instances(ti,Si)?D, in which target ti?
T is annotated witha set of substitutes Si.
Then we regard for each tar-get word t the substitute sets St?
D for t. We de-fine a substitute agreement as SA(t) as the meanpairwise dice coefficient between all s1,s2?
Stwhere s16= s2.
For each dataset D we list the sub-stitute variance SV = 1?1|T |?t?TSA(t).
Table 1shows this metric for the three datasets, as well asfor subsets of the dataset according to target partof speech.
It can be seen that the variance in goldsubstitutes differs substantially between datasets,but not much between target word type within adataset.
EL09 has the highest degree of variance,suggesting that targets tend to be more ambiguous,whereas GE15 has the lowest degree of variance,suggesting less ambiguity.3 Related WorkLexical substitution has been addressed exten-sively in recent years.
Early systems, havingonly very few training instances available, use un-2The original SE07 task had a third evaluation settingMWE, in which systems had to correctly identify which tar-get words were part of a multiword expression.119datasetsubstitute variance (SV )noun verb adj adv allSemEval-2007 0.78 0.79 0.72 0.66 0.75Evalita-2009 0.84 0.82 0.83 0.82 0.83GermEval-2015 0.59 0.67 0.60 - 0.66all 0.75 0.72 0.73 0.69 0.73Table 1: Degree of variation in gold answerssupervised approaches for determining appropri-ate substitutes.
For the English SE07 task, sys-tems mostly consider substitution candidates fromWordNet (Fellbaum, 1998) and cast lexical sub-stitution into a ranking task.
Experiments mayalso be performed by pooling the set of candi-dates from the gold data, evaluating a pure rank-ing variant.
Early approaches use a contextual-ized word instance representation and rank can-didates according to their similarity to this repre-sentation.
Effective representations are syntacticvector space models (Erk and Pad?, 2008; Thateret al, 2011), which use distributional sparse vec-tor representations based on the syntactic contextof words.
Performance improvement could beshown for different models, including the use ofgraph centrality algorithms on directional wordsimilarity graphs (Sinha and Mihalcea, 2011), andclustering approaches on word instance represen-tations (Erk and Pad?, 2010).
Multiple systemshave built upon the distributional approach.
Ex-tensions include the use of LDA topic models(?
S?aghdha and Korhonen, 2014), and proba-bilistic graphical models (Moon and Erk, 2013).The current state of the art combines a distri-butional model with the use of n-gram languagemodels (Melamud et al, 2015a).
They define thecontext vector of each word in a background cor-pus as a substitute vector, which is a vector of suit-able filler words for the current n-gram context.They then obtain a contextualized paraphrase vec-tor by computing a weighted average of substitutevectors in the background corpus, based on theirsimilarity to the current target instance.
In con-trast to traditional sparse vector representationsobtained through distributional methods, a recenttrend is the use of low-dimensional dense vectorrepresentations.
The use of such vector repre-sentations or word embeddings has been popular-ized by the continuous bag-of-words (CBOW) andSkip-gram model (Mikolov et al, 2013a).
Mela-mud et al (2015b) show a simple and knowledge-lean model for lexical substitution based solely onsyntactic word embeddings.
As we leverage thismodel as a feature in our approach, we will elab-orate on this in Section 4.
Another approach forapplying word embeddings to lexical substitutionis their direct extension with multiple word senses,which can be weighted according to target context(Neelakantan et al, 2014).Biemann (2013) first showed that the lexicalsubstitution task can be solved very well when suf-ficient amount of training data is collected per tar-get.
An approach based on crowdsourcing humanjudgments achieved the best performance on theS07 dataset to day.
However, judgments had tobe collected for each lexical item, and as a conse-quence the approach can not scale to an open vo-cabulary.
As an alternative to per-word supervisedsystems trained on target instances per lexeme, all-words systems aim to generalize over all lexicalitems.
Szarvas et al (2013a) proposed such a sys-tem by using delexicalization: features are gener-alized in such a way that they are independent oflexical items, and thus generalize beyond the train-ing set and across targets.
Originally, a maximumentropy classifier was trained on target-substituteinstances and used for pointwise ranking of sub-stitution candidates.
In a follow-up work it wasshown that learning-to-rank methods could dras-tically improve this approach, achieving state-of-the-art performance with a LambdaMART ranker(Szarvas et al, 2013b).
In this work we will buildupon this model and further generalize not onlyacross lexical items but across different languages.For both EL09 and GE15, existing approacheshave been adapted.
For the Italian dataset, adistributional method was combined with LSA(De Cao and Basili, 2009).
The best perform-ing system applied a WSD system and languagemodels (Basile and Semeraro, 2009).
For the Ger-man dataset, Hintz and Biemann (2015) adaptedthe supervised approach by (Szarvas et al, 2013a),achieving best performance for nouns and adjec-tives.
Jackov (2015) used a deep semantic analy-sis framework employing an internal dependencyrelation knowledge base, which achieved the bestperformance for verbs.4 Method descriptionWe subdivide lexical substitution into two sub-tasks; candidate selection and ranking.
For agiven target t, we consider a list of possible substi-120tutes s ?
Ct, where Ctis a static per-target candi-date list.
Our method is agnostic to the creation ofthis static resource, which can be obtained eitherby an unsupervised similarity-based approach, orfrom a lexical resource.
In particular, candidatesobtained at this stage do not disambiguate possiblemultiple senses of t, and are filtered and ordered inthe ranking stage by a supervised model.In modeling a supervised system, we have ex-perimented with two learning setups.
The firstis applying a standard classification / regressionlearner.
Here, lexical substitution is cast intoa pointwise ranking task by training on target-substitute pairs generated from the gold standard.For each sentence context c, target word t and sub-stitute s, we regard the tuple (c, t,s) as a traininginstance.
We obtain these training instances foreach lexsub instance (c, t) by considering all sub-stitutes s ?
Gt?Ctwhere Gtare all candidates fortarget t pooled from the gold data and Ctare ob-tained from lexical resources.
We then experimentwith two labeling alternatives for a binary classi-fication and a regression setup, respectively.
Forbinary classification we label each instance (c, t,s)as positive if s has been suggested as a substitutefor t by at least one annotator, and as negative oth-erwise.
For regression, we normalize the annota-tion counts for each substitute to obtain a score in(0,1] if a substitute s occurs in the gold data, 0otherwise.
The ranking of substitutes per target isobtained by considering the posterior likelihood ofthe positive label as yielded by a classifier model.We have tried multiple classifiers but have foundno significant improvement over a maximum en-tropy baseline3.
Our second setup is a learning-to-rank framework, adapted from (Szarvas et al,2013b).
Here, we are not restricted to a pointwiseranking model, but consider pairwise and listwisemodels4.We base our feature model on existing research.In addition to basic syntactic and frequency-basedfeatures, we obtained sophisticated features fromtrigram and syntactic thesauri, motivated by thefindings of Biemann and Riedl (2013), as well assyntactic embedding features motivated by Mela-mud et al (2015b).3For classification setup we use Mallet: http://mallet.cs.umass.edu/4For learning-to-rank we use RankLib: http://mallet.cs.umass.edu/datasetmaximum recallw/ MWE w/o MWESemEval-2007 0.459 0.404Evalita-2009 0.369 0.337GermEval-2015 0.192 0.178all 0.242 0.223Table 2: Upper bound for substitute recall basedon lexical resources WordNet, MultiWordNet, Ger-maNet4.1 Candidate selectionWe confirm earlier research (Sinha and Mihalcea,2009) on the high quality of selecting candidatesfrom lexical resources.
We thus base our candidateselection on prevalently used resources: WordNet(Fellbaum, 1998) for English, GermaNet (Hampand Feldweg, 1997) for German and MultiWord-Net (Pianta et al, 2002) for Italian.
For all re-sources, we consider all possible senses for a giventarget word and obtain all synonyms, hypernymsand hyponyms and their transitive hull.
Thus, forthe hypernymy and hyponymy relation, we followthe respective edges in the graph collecting allnodes (synsets) along the path.
For each synset,we extract all lemmas as substitution candidates.Although restricting candidates incurs a relativelylow upper bound on system recall, we still obtainbest results using this rather conservative filter.
Ta-ble 2 shows the upper bound for system recall foreach of the datasets, evaluated with and without re-moving all multiword expressions from both can-didate lists and gold data.
A higher coverage ofWordNet is a plausible explanation for the muchhigher recall on the English data.4.2 Supervised rankingLearning-to-rank methods train a supervisedmodel for ranking a list of items by relevance.
Abasic pointwise approach applies regression tech-niques to obtain a relevance scores for each itemin isolation.
More advanced models are based onpairwise preference information for instance pairs,and listwise approaches, which are optimized on aglobal metric of a given ranking output.
An ex-tensive overview of learning-to-rank models canbe found in (Burges, 2010).
For lexical substi-tution, LambdaMART (Wu et al, 2010) has beenfound to be particularly effective.
LambdaMARTis a listwise method based on gradient boosting ofregression trees.
Its two main hyperparameters are121the number of leaves in each regression tree andthe number of iterations and trees.
We have notperformed extensive tuning of these hyperparame-ters and used default settings, an ensemble of 1000trees with 10 leaves.4.3 Delexicalized featuresThe idea of delexicalization has been proposed,for instance, by Bergsma et al (2007).
Theypropose to use statistical measures based solelyon the frequency of different expansions of thesame target term.
Their feature set has motivateda large subset of the feature model, which weadapt in this work.
The idea of generalizing fea-tures for lexical substitution in such a way thatthey work across lexical items has been shownby Moon and Erk (2013), and made explicit bySzarvas et al (2013a).
Instances are characterizedusing non-lexical features from heterogeneous ev-idence.
The intuition of this feature model is toexploit redundant signals of substitutability fromdifferent sources and methods.In cases where background corpora are re-quired, the following data is used throughout allfeatures: For English, a newspaper corpus com-piled from 105 million sentences from the LeipzigCorpora Collection (Richter et al, 2006) and theGigaword corpus (Parker et al, 2011) was used.For German a 70M sentence newswire corpus(Biemann et al, 2007) was used.
For Italian, asubset of 40M sentences of itWac, a large web-crawl, was used (Baroni et al, 2009).Shallow syntactic features We apply a part-of-speech tagger trained on universal POS tags(Petrov et al, 2012), which we simplify into theclasses noun, verb, adjective and adverb.
Usingthese simplified tags we construct an n-gram slid-ing window, with n ?
[1..5], of POS around thetarget.
We could also reduce window sizes drasti-cally to n = 1,2 without sacrificing performance.Frequency features We use language modelsfor each of the languages to obtain frequency ratiofeatures.
An n-gram sliding window around a tar-get t is used to generate a set of featuresfreq(cl,s,cr)freq(cl,t,cr),where cland crare the left and right context wordsaround t. Here, we normalize the frequency of thesubstitute with the frequency of the n-gram withoriginal target t. As a variant, we further normal-ize frequencies by the set of all substitutes, to ob-tain frequencies featuresfreq(cl,s,cr)?s?
?Ctfreq(cl,s?,cr)where Ctis the set of candidate substitutes for t. In our ex-periments we used sliding windows of size [1..5].We obtain 5-gram counts from web1t (Brants andFranz, 2009).Conjunction ratio features Based on the n-gram resources above, we further define a con-junctive phrase ratio feature, which measures howoften the construct (cl, t,conjunction,s,cr) occursin a background corpus; i.e.
how often t and s co-occur with a conjunction word (?and?, ?or?, ?,?
),within the context of the sentence.
As there is adifferent set of conjunction words for each lan-guage, we first aggregate the mean over all con-junction words:conjl,r(t,s)=1|CONJ|?con?CONJfreq(cl, t,con,s,cr)where l and r is the size of the left and right con-text window, and CONJ is a set of conjunctionwords per-language5.
For left and right contextsize l = r = 0 this feature also captures a context-independent conjunction co-occurrence betweenonly t and s. Again, we normalize this feature overthe set of all candidates:conjl,r(t,s)?s?
?Ctconjl,r(t,s)Distributional features We construct a distri-butional thesaurus (DT) for each of the lan-guages by following Biemann and Riedl (2013)and obtain first-order word-to-context measures,as well as second-order word-to-word similaritymeasures.
As context features we have experi-mented with both syntactic dependencies as wellas left and right neighboring words, and havefound them to perform equivalently.
As a saliencemeasure we use Lexicographer?s Mutual Informa-tion (Bordag, 2008) and prune the data, keep-ing only the 1000 most salient features per word.Word similarity is obtained from an overlap countin the pruned context features.
We model featuresfor the contextualized distributional similarity be-tween t and s as?
percentage of shared context features for thetop-k context features of t and s, globally andrestricted to sentence context (k =5, 20, 50,100, 200)5Conjunction words used are and, or, (comma), for En-glish; und, oder, (comma) for German and e, ed, o, od,(comma) for Italian.122?
percentage of shared words for the top-k sim-ilar words of t and s (k =200)?
sum of salience score of context features of soverlapping with the sentence context?
binary occurrence of s in top-k similar wordsof t (k =100, 200)With the exception of the last feature, these mea-sures are scaled to [0,1] over the set of all substi-tute candidates.Syntactic word embeddings We adapt the un-supervised approach by (Melamud et al, 2015a)as a set of features.
We follow (Levy and Gold-berg, 2014) to construct dependency-based wordembeddings; we obtain syntactic contexts by run-ning a syntactic dependency parser6, and comput-ing word embeddings using dependency edges ascontext features7.
The resulting dense vector rep-resentations for words and context live within thesame vector space.
We compute the semantic sim-ilarity between a target and a substitute word fromthe cosine similarity in the word embedding space,as well as the first-order target-to-context similar-ity.
For a given target word t and substitute s, letCtbe the syntactic context of t and c ?Cta singlecontext ?
i.e.
a dependency edge attached to t; letvt, vsbe the vector representations of t and s in theword embedding space, and vcthe vector represen-tation of c in the context embedding space.
ThenSim1= cos(vs,vc) and Sim2= cos(vs,vt) are thefirst-order and second-order substitutability mea-sures considered by Melamud et al (2015a).
Incontrast to their approach, we do not just consideran unsupervised combination of these two mea-sures, but instead use both Sim1and Sim2as sin-gle features.
We also use their combinations of abalanced / unbalanced, arithmetic / geometricalmean, to obtain six numeric features in total.
Im-portantly, these features are independent of the un-derlying embedding vectors and can therefore gen-eralize across arbitrary embeddings between lan-guages.Semantic resource features To generalizeacross multiple languages we minimize the6We trained models for Mate (https://code.google.com/p/mate-tools/) based on universaldependencies (http://universaldependencies.org/)7We used word2vecf (https://bitbucket.org/yoavgo/word2vecf) for computing syntactic word em-beddingscomplexity of features obtained from semanticresources ?
which may differ notably in sizeand structure.
From the resources listed inSection 4.1 we extract binary features for thesemantic relations synonymy, hypernymy and hy-ponymy, occurring between t and s. We have alsoexperimented with graded variants for transitiverelations, such as encoding n-th level hypernymy,but have not observed any gain from this featurevariation.4.4 Transfer learningTransfer learning is made feasible by a fullylexeme-independent and language-independentfeature space.
Language-specific knowledge re-sides only within the respective resources for eachlanguage, and gets abstracted in feature extraction.Figure 1 illustrates this process at the example oftwo entirely unrelated sentences in different lan-guages (English and German).
A further mediatorfor transfer learning is a model based on boosteddecision trees.
As opposed to linear models, whichcould not be reasonably learned across languages,a LambdaMART ranker is able to learn feature in-teraction across languages.
To give an exampleof what the resulting model can pick up on, wecan regard conditionally strong features.
Considerthe n-gram pair frequency ratio feature of win-dow size (l,r) = (1,0), which compares the fre-quency ratio of the target and substitute includinga single left context word.
Depending on the POSwindow, this feature can be highly informative insome cases, where it is less informative in others.For adjective-noun pairs, in which the noun is thesubstitution target, the model can learn that thisfrequency ratio is strongly positively correlated;in this case, the substitute frequently occurs withthe same adjective than the original target.
Forother POS windows, for example determiner-nounpairs, the same frequency ratio may be less indica-tive, as most nouns frequently occur with a deter-miner.
This property works across languages, aslong as as attributive adjectives are prepositioned.In our subset of languages, this is the case for En-glish and German, but not for Italian, which usespostpositive adjectives.
Nevertheless, we are ableto learn such universal feature interactions.5 Results and discussionEvaluation of lexical substitution requires spe-cial care, as different evaluation settings are used123Figure 1: Visualization of feature extraction and delexicalization.
Two unrelated sentences in Englishand German (translation: ?the strain has to be limited?)
are shown.
Language-specific knowledge isobtained from resources for each language respectively.
The resulting feature space is delexicalized andlanguage independent.throughout previous work and comparability is notalways guaranteed.
We follow the convention ofreporting the full lexical substitution task (bothgenerating and ranking candidates) with the met-rics P-best and P-oot and report the ranking-onlytask (candidates pooled from the gold standard)with the GAP score.
We further observe that pre-vious work commonly discards multiword expres-sions from both the candidate lists as well as thegold data8.
We follow this convention, but notethat our system is in fact capable of successfullyranking multiword expansions out of the box.
Sys-tem performance slightly decreases when includ-ing MWE, as there is virtually no overlap betweenthose provided by the system and those in the goldstandard.For ranking we experiment with different point-wise classifiers as provided by Mallet (MaxEntclassification and regression) as well learning-to-rank models provided by RankLib (RankBoost,RankNet, LambdaMART).
In line with findings in(Szarvas et al, 2013b), we observe that learning-to-rank approaches work better than a pointwiseclassification / regression setup throughout all lan-guages and feature subsets.
Among differentrankers, we confirm LambdaMART to yield thebest performance, and will only report numbersusing this model.
As optimization metric we haveexplored both NDCG@10 and MAP.
The NDCGmetric can incorporate different scoring weights8The omission of MWE by multiple authors has been con-firmed by the authors of (Melamud et al, 2015a).Open evaluation (best-P / oot-P)Training English German ItalianEnglish 16.63 48.16 7.43 26.79 8.57 31.94German 13.20 44.61 11.97 38.45 7.05 28.75Italian 13.91 39.72 4.25 22.66 15.19 40.37others 17.19 46.79 8.15 27.33 10.04 30.82all 17.23 48.83 12.94 41.32 16.15 41.29SOA915.94 36.37 11.20 20.14 10.86 41.46Table 3: Transfer learning results for the open can-didate task (candidates from lexical resources)Ranking evaluation (GAP)Training English German ItalianEnglish 51.0 26.9 44.5German 44.3 56.2 42.9Italian 36.7 22.2 48.0others 43.7 26.7 43.9all 51.9 51.3 50.0Table 4: Transfer learning results on the ranking-only task (candidates pooled from gold)based on annotator overlap, however MAP di-rectly correlates with evaluation score.
We havefound optimizing on MAP to yield slightly bet-ter results, even if this disregards the relative scoreweights between gold substitutes.
For the ranking-only task, we also extended the pooled train-ing data with additional negative examples (i.e.adding all candidates as for the full task) but ob-served a minor decrease in system performance.124We report transfer learning results across allthree datasets.
Table 3 shows a transfer-learningmatrix for the full lexical substitution task,whereas Table 4 shows results for the ranking-onlytask.
For evaluation, we consistently use the com-plete datasets, which are roughly equal in size forall languages (~ 2000 instances).
For the identityentries in this matrix, as well as training on thecomplete dataset (?all?)
we follow previous super-vised work and perform 10-fold cross-validation.Splits are based on the target lexeme, so that notwo instances for the same target word are in dif-ferent sets.
Tables 3 and 4 suggest the feasibil-ity of transfer learning.
Although models trainedon the original language (identity entries of thematrix) perform best, training on a different lan-guage still yields reasonable results.
Training onlyon a single other language, not surprisingly, yieldsworse results for each dataset, however combiningthe data from the two remaining languages (?oth-ers?)
can mitigate this issue to some degree.
Im-portantly, adding the data from two additional lan-guages consistently improves system performancethroughout all datasets for the open candidate task(Table 3).
It is interesting to note that in case ofSE07, training on only other languages performssurprisingly well for the best-P score, beating evena model trained on English.
A possible explana-tion for this is that the SE07 dataset appears to besomewhere in the middle between EL09 and GE15in terms of substitute variance.
For the ranking-only task, transfer learning seems to work a lit-tle less effectively.
In case of German, addingforeign language data in fact hurts GAP perfor-mance.
This potentially originates from a muchsmaller set of training instances and inconsistencyof the amount and overlap of pooled candidatesacross different tasks (as described in Table 1).
Wealso observe that a learning-to-rank model is es-sential for performing transfer learning.
In caseof LambdaMART, an ensemble of decision treesis constructed, which is well suited to exploit re-dundant signals across multiple features.
Linearmodels resulted in worse performance for trans-fer learning, as the resulting weights seem to belanguage-specific.Feature ablation experiments are performed forvarious feature groups in the full and ranking-onlytask (Table 5).
The ablation groups correspond to9State of the art baseline, according to previous reportedresults, c.f.
Table 6the feature categories defined in Section 4.3.
Thefrequency group includes plain frequency featuresas well as conjunction ratio features.
We consideronly our universal model trained on all languagedata (with 10-fold CV for each dataset).
In caseof English, the full system performs best and allfeature groups improve overall performance.
Forother languages these results are mixed.
In caseof the German data, embedding features and se-mantic relation features seem to work well on theirown, so that results for other ablation groups areslightly better.
For ranking-only, embedding fea-tures seem to be largely subsumed by the combi-nation of the other groups.
Ablation of embed-dings differs vastly between the full and ranking-only task; they seem to more more crucial for thefull task.
For all languages, semantic relations arethe best feature in the full task, acting as a strongfilter for candidates; in ranking-only they are moredispensable.In summary, we observe that delexicalizedtransfer learning for lexical substitution is possi-ble.
Existing supervised approaches can be ex-tended to generalize across multiple languageswithout much effort.
Training a supervised sys-tem on different language data emphasizes thatthe learned model is sufficiently generic to belanguage independent.
Our feature space con-structed from heterogeneous evidence consists ofmany features that perform relatively weakly ontheir own.
The resulting ranking model capturesredundancy between these signals.
Finally, Ta-ble 6 shows our results in comparison to previ-ous work.
Note that we omit some participatingsystems from the original SE07 task.
The rea-son we did not list IRST2 (Giuliano et al, 2007)is that for out-of-ten results, the system outputsthe same substitute multiple times and the eval-uation scheme gives credit for each copy of thesubstitute.
Our (and other) systems do not tam-per with the metric in this way, and only yielda set of substitutes.
UNT (Hassan et al, 2007)uses a much richer set of knowledge bases, not allof them easily available, to achieve slightly betteroot scores.
From our experiments, we list both amodel trained per language, as well as a universalmodel trained on all data.
The latter beats nearlyall other approaches on the full lexical substitu-tion task, despite not being optimized for a singlelanguage.
Although omission of MWEs is com-mon practice for SE07, it is unclear if this was125English German Italianbest-P GAP best-P GAP best-P GAPw/o syntax 15.35 49.5 12.33 42.1 15.70 50.3w/o frequency 17.04 48.6 13.30 54.6 15.78 51.5w/o DT 16.88 48.8 12.18 54.6 17.65 51.8w/o sem.
relation 11.51 49.9 6.82 33.9 8.06 49.7w/o embedding 10.05 51.5 11.51 47.1 7.17 54.4full system 17.23 51.9 12.94 51.3 16.15 50.0Table 5: Feature ablation results for the full andranking-only task (universal model trained on alldata)done for EL09 and GE15.
However, re-inclusionof MWE does not drastically alter results10.
Inthe ranking-only variant, we are not able to beatthe learning-to-rank approach by Szarvas et.
al(2013b), we note however that they have per-formed extensive hyperparameter optimization oftheir ranker, which we have omitted.
We are alsonot able to achieve GAP scores reported by Mela-mud at al.
(2015b).
Although we used their ex-act embeddings, we could not reproduce their re-sults11.6 ConclusionWe are the first to model lexical substitution asa language-independent task by considering notjust a single-language dataset, but by merging datafrom distinct tasks in English, German and Ital-ian.
We have shown that a supervised, delex-icalized approach can successfully learn a sin-gle model across languages ?
and thus performtransfer learning for lexical substitution.
We ob-serve that a listwise ranker model such as Lamb-daMART facilitates this transfer learning.
Wehave further shown that incorporating more datahelps training a more robust model and can consis-tently improve system performance by adding for-eign language training data.
We extended an exist-ing supervised learning-to-rank approach for lexi-cal substitution (Szarvas et al, 2013b) with state-of-the-art embedding features (Melamud et al,2015b).
In our experiments, a single model trainedon all data performed best on each language.
In all10For comparison, our scores including MWE for the ?alldata?
model are as follows (best-P, oot-P, GAP).
EL09:15.12, 33.92, 45.8; GE15: 12.20, 41.15, 50.011Our evaluation of (Melamud et al, 2015b), balAddyields a GAP score of 48.8, which is likely related to differentevaluation settings.12baseline by task organizerSemEval ?07method best-P oot-P GAP(Erk and Pad?, 2010) - - 38.6(Thater et al, 2011) - - 51.7(Szarvas et al, 2013a) 15.94 - 52.4(Szarvas et al, 2013b) - - 55.0(Melamud et al, 2015b) 08.09 27.65 52.9(Melamud et al, 2015a) 12.72 36.37 55.2our method (English only) 16.63 48.16 51.0our method (all data) 17.23 48.83 51.9Evalita ?09method best-P oot-P GAP(Basile and Semeraro, 2009) 08.16 41.46 -(Toral, 2009)1210.86 27.52 -our method (Italian only) 15.19 40.37 48.0our method (all data) 16.15 31.18 50.0GermEval ?15method best-P oot-P GAP(Hintz and Biemann, 2015) 11.20 19.49 -(Jackov, 2015) 06.73 20.14 -our method (German only) 11.97 38.45 56.2our method (all data) 12.94 41.32 51.3Table 6: Experimental results of our method com-pared to related work for all three lexical substitu-tion tasksthree datasets we were able to improve the currentstate of the art for the full lexical substitution task.The resulting model can be regarded as language-independent; given an unannotated backgroundcorpus for computing language-specific resourcesand a source of substitution candidates, the sys-tem can be used almost out of the box.
For obtain-ing substitution candidates, we still rely on lexi-cal resources such as WordNet, which have to beavailable for each language.
As future work weaim to make our approach completely knowledge-free by eliminating this dependency.
We can con-sider substitution candidates based on their dis-tributional similarity.
First experiments confirmthat this already yields a much better coverage,i.e.
upper bound on recall, while introducing morenoise.
The remaining key challenge is to bet-ter characterize possible substitutes from bad sub-stitutes in ranked lists of distributionally similarwords, which frequently contain antonyms and co-hyponyms.
We will explore unsupervised acquisi-tion of relational similarity (Mikolov et al, 2013b)for this task.126AcknowledgmentsThis work has been supported by the German Re-search Foundation as part of the Research TrainingGroup ?Adaptive Preparation of Information fromHeterogeneous Sources?
(AIPHES) under grantNo.
GRK 1994/1 and the SEMSCH project, grantNo.
BI 1544.ReferencesDaniel B?r, Chris Biemann, Iryna Gurevych, andTorsten Zesch.
2012.
UKP: Computing seman-tic textual similarity by combining multiple con-tent similarity measures.
In Proceedings of theFirst Joint Conference on Lexical and Computa-tional Semantics - Volume 1: Proceedings of theMain Conference and the Shared Task, and Volume2: Proceedings of the Sixth International Workshopon Semantic Evaluation, pages 435?440, Montreal,Canada.Marco Baroni, Silvia Bernardini, Adriano Ferraresi,and Eros Zanchetta.
2009.
The WaCky wideweb: a collection of very large linguistically pro-cessed web-crawled corpora.
Language resourcesand evaluation, 43(3):209?226.Pierpaolo Basile and Giovanni Semeraro.
2009.UNIBA @ EVALITA 2009 lexical substitutiontask.
In Proceedings of EVALITA workshop, 11thCongress of Italian Association for Artificial Intelli-gence, Reggio Emilia, Italy.Shane Bergsma and Qin Iris Wang.
2007.
Learningnoun phrase query segmentation.
In Proceedingsof the 2007 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning (EMNLP-CoNLL), vol-ume 7, pages 819?826, Prague, Czech Republic.Chris Biemann and Martin Riedl.
2013.
Text: Nowin 2D!
A framework for lexical expansion with con-textual similarity.
Journal of Language Modelling,1(1):55?95.Chris Biemann, Gerhard Heyer, Uwe Quasthoff, andMatthias Richter.
2007.
The Leipzig corpora collec-tion: monolingual corpora of standard size.
In Pro-ceedings of Corpus Linguistics, Birmingham, UK.Chris Biemann.
2013.
Creating a system for lexi-cal substitutions from scratch using crowdsourcing.Language Resources and Evaluation, 47(1):97?122.Stefan Bordag.
2008.
A Comparison of Co-occurrenceand Similarity Measures As Simulations of Context.In Proceedings of the 9th International Conferenceon Computational Linguistics and Intelligent TextProcessing, CICLing 2008, pages 52?63, Haifa, Is-rael.Thorsten Brants and Alex Franz.
2009.
Web 1T 5-gram, 10 European languages version 1.
LinguisticData Consortium.Christopher J.C. Burges.
2010.
From RankNetto LambdaRank to LambdaMART: An overview.Technical Report MSR-TR-2010-82, Microsoft Re-search.Kostadin Cholakov, Chris Biemann, Judith Eckle-Kohler, and Iryna Gurevych.
2014.
Lexical substi-tution dataset for German.
In Proceedings of the 9thInternational Conference on Language Resourcesand Evaluation, pages 1406?1411, Reykjavik, Ice-land.Diego De Cao and Roberto Basili.
2009.
Combiningdistributional and paradigmatic information in a lex-ical substitution task.
In Proceedings of EVALITAworkshop, 11th Congress of Italian Association forArtificial Intelligence, Reggio Emilia, Italy.Katrin Erk and Sebastian Pad?.
2008.
A structuredvector space model for word meaning in context.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing, pages 897?906, Waikiki, HI, USA.Katrin Erk and Sebastian Pad?.
2010.
Exemplar-basedmodels for word meaning in context.
In Proceedingsof the ACL 2010 conference short papers, pages 92?97, Uppsala, Sweden.Christiane Fellbaum.
1998.
WordNet.
Wiley OnlineLibrary.Claudio Giuliano, Alfio Gliozzo, and Carlo Strappa-rava.
2007.
FBK-irst: Lexical substitution taskexploiting domain and syntagmatic coherence.
InProceedings of the 4th International Workshop onSemantic Evaluations (SemEval-2007), pages 145?148, Prague, Czech Republic.Birgit Hamp and Helmut Feldweg.
1997.
GermaNet -a lexical-semantic net for German.
In In Proceed-ings of ACL workshop Automatic Information Ex-traction and Building of Lexical Semantic Resourcesfor NLP Applications, pages 9?15, Madrid, Spain.Samer Hassan, Andras Csomai, Carmen Banea, RaviSinha, and Rada Mihalcea.
2007.
UNT: Sub-finder: Combining knowledge sources for auto-matic lexical substitution.
In Proceedings of the 4thInternational Workshop on Semantic Evaluations(SemEval-2007), pages 410?413, Prague, Czech Re-public.Gerold Hintz and Chris Biemann.
2015.
Delexical-ized supervised German lexical substitution.
In Pro-ceedings of GermEval 2015: LexSub, pages 11?16,Essen, Germany.Luchezar Jackov.
2015.
Lexical substitution usingdeep syntactic and semantic analysis.
In Proceed-ings of GermEval 2015: LexSub, pages 17?20, Es-sen, Germany.127Gerhard Kremer, Katrin Erk, Sebastian Pad?, and Ste-fan Thater.
2014.
What substitutes tell us - analy-sis of an "all-words" lexical substitution corpus.
InProceedings of the 14th Conference of the EuropeanChapter of the Association for Computational Lin-guistics, pages 540?549, Gothenburg, Sweden.Omer Levy and Yoav Goldberg.
2014.
Dependency-based word embeddings.
In Proceedings of the 52ndAnnual Meeting of the Association for Computa-tional Linguistics, volume 2, pages 302?308, Bal-timore, MD, USA.Diana McCarthy and Roberto Navigli.
2007.
Semeval-2007 task 10: English lexical substitution task.
InProceedings of the 4th International Workshop onSemantic Evaluations, pages 48?53, Prague, CzechRepublic.Diana McCarthy and Roberto Navigli.
2009.
The En-glish lexical substitution task.
Language resourcesand evaluation, 43(2):139?159.Oren Melamud, Ido Dagan, and Jacob Goldberger.2015a.
Modeling word meaning in context with sub-stitute vectors.
In Proceedings of the 2015 Confer-ence of the North American Chapter of the Associ-ation for Computational Linguistics: Human Lan-guage Technologies, pages 472?482, Denver, CO,USA.Oren Melamud, Omer Levy, and Ido Dagan.
2015b.
Asimple word embedding model for lexical substitu-tion.
VSM Workshop.
Denver, CO, USA.Rada Mihalcea, Ravi Sinha, and Diana McCarthy.2010.
Semeval-2010 task 2: Cross-lingual lexicalsubstitution.
In Proceedings of the 5th InternationalWorkshop on Semantic Evaluation, pages 9?14, Up-psala, Sweden.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013a.
Distributed represen-tations of words and phrases and their composition-ality.
In Advances in neural information processingsystems, pages 3111?3119, Lake Tahoe, NV, USA.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013b.
Linguistic regularities in continuous spaceword representations.
In Proceedings of the 2013Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 746?751, Atlanta,GA, USA.Tristan Miller, Darina Benikova, and Sallam Abual-haija.
2015.
GermEval 2015: LexSub ?
A sharedtask for German-language lexical substitution.
InProceedings of GermEval 2015: LexSub, pages 1?9, Essen, Germany.Taesun Moon and Katrin Erk.
2013.
An inference-based model of word meaning in context as a para-phrase distribution.
ACM Transactions on Intelli-gent Systems and Technology (TIST), 4(3):42.Arvind Neelakantan, Jeevan Shankar, Alexandre Pas-sos, and Andrew McCallum.
2014.
Efficient non-parametric estimation of multiple embeddings perword in vector space.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1059?1069, Doha, Qatar.Diarmuid ?
S?aghdha and Anna Korhonen.
2014.Probabilistic distributional semantics with latentvariable models.
Computational Linguistics,40(3):587?631.Robert Parker, David Graff, Junbo Kong, Ke Chen, andKazuaki Maeda.
2011.
English Gigaword Fifth Edi-tion.
Technical report, Linguistic Data Consortium,Philadelphia, PA, USA.Slav Petrov, Dipanjan Das, and Ryan McDonald.
2012.A universal part-of-speech tagset.
In Proceedings ofthe Eight International Conference on Language Re-sources and Evaluation, pages 2089?2096, Istanbul,Turkey.Emanuele Pianta, Luisa Bentivogli, and Christian Gi-rardi.
2002.
MultiWordNet: developing an alignedmultilingual database.
In Proceedings of the 1stInternational WordNet Conference, pages 293?302,Mysore, India.Matthias Richter, Uwe Quasthoff, Erla Hallsteinsd?ttir,and Chris Biemann.
2006.
Exploiting the LeipzigCorpora Collection.
In Proceedings of the Infor-mation Society Language Technologies Conference2006, pages 68?73.
Ljubljana, Slovenia.Nilda Ruimy, Monica Monachini, Raffaella Dis-tante, Elisabetta Guazzini, Stefano Molino, MarisaUlivieri, Nicoletta Calzolari, and Antonio Zampolli.2002.
CLIPS, a multi-level Italian computationallexicon: a glimpse to data.
In Proceedings of the 3rdInternational Conference on Language Resourcesand Evaluation, pages 792?799, Las Palmas, Spain.Ravi Sinha and Rada Mihalcea.
2009.
Combining lex-ical resources for contextual synonym expansion.
InProceedings of the Conference in Recent Advancesin Natural Language Processing, pages 404?410,Borovets, Bulgaria.Ravi Som Sinha and Rada Flavia Mihalcea.
2011.Using centrality algorithms on directed graphs forsynonym expansion.
In FLAIRS Conference, pages311?316, Palm Beach, FL, USA.Gy?rgy Szarvas, Chris Biemann, Iryna Gurevych, et al2013a.
Supervised all-words lexical substitution us-ing delexicalized features.
In Proceedings of the2013 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 1131?1141, At-lanta, GA, USA.Gy?rgy Szarvas, R?bert Busa-Fekete, and Eyke H?ller-meier.
2013b.
Learning to rank lexical substitutions.In Proceedings of the 2013 Conference on Empiri-cal Methods in Natural Language Processing, pages1926?1932, Seattle, WA, USA.128Stefan Thater, Georgiana Dinu, and Manfred Pinkal.2009.
Ranking paraphrases in context.
In Proceed-ings of the 2009 Workshop on Applied Textual Infer-ence, pages 44?47, Singapore, Republic of Singa-pore.Stefan Thater, Hagen F?rstenau, and Manfred Pinkal.2011.
Word meaning in context: A simple and ef-fective vector model.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1134?1143, Edinburgh, UK.Antonio Toral.
2009.
The lexical substitution task atEVALITA 2009.
In Proceedings of EVALITA Work-shop, 11th Congress of Italian Association for Arti-ficial Intelligence, Reggio Emilia, Italy.Qiang Wu, Christopher JC Burges, Krysta M Svore,and Jianfeng Gao.
2010.
Adapting boosting for in-formation retrieval measures.
Information Retrieval,13(3):254?270.129
