Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 842?850,Beijing, August 2010Co-STAR: A Co-training Style Algorithm for Hyponymy RelationAcquisition from Structured and Unstructured TextJong-Hoon Oh, Ichiro Yamada, Kentaro Torisawa, and Stijn De SaegerLanguage Infrastructure Group, MASTAR Project,National Institute of Information and Communications Technology (NICT){rovellia,iyamada,torisawa,stijn}@nict.go.jpAbstractThis paper proposes a co-training stylealgorithm called Co-STAR that acquireshyponymy relations simultaneously fromstructured and unstructured text.
In Co-STAR, two independent processes for hy-ponymy relation acquisition ?
one han-dling structured text and the other han-dling unstructured text ?
collaborate by re-peatedly exchanging the knowledge theyacquired about hyponymy relations.
Un-like conventional co-training, the two pro-cesses in Co-STAR are applied to dif-ferent source texts and training data.We show the effectiveness of this al-gorithm through experiments on large-scale hyponymy-relation acquisition fromJapanese Wikipedia and Web texts.
Wealso show that Co-STAR is robust againstnoisy training data.1 IntroductionAcquiring semantic knowledge, especially se-mantic relations between lexical terms, is re-garded as a crucial step in developing high-levelnatural language applications.
This paper pro-poses Co-STAR (a Co-training STyle Algorithmfor hyponymy Relation acquisition from struc-tured and unstructured text).
Similar to co-training (Blum and Mitchell, 1998), two hy-ponymy relation extractors in Co-STAR, one forstructured and the other for unstructured text, it-eratively collaborate to boost each other?s perfor-mance.Many algorithms have been developed to auto-matically acquire semantic relations from struc-tured and unstructured text.
Because term pairsare encoded in structured and unstructured text indifferent styles, different kinds of evidence havebeen used for semantic relation acquisition:Evidence from unstructured text: lexico-syntactic patterns and distributional similar-ity (Ando et al, 2004; Hearst, 1992; Pantelet al, 2009; Snow et al, 2006; De Saeger etal., 2009; Van Durme and Pasca, 2008);Evidence from structured text: topic hierarchy,layout structure of documents, and HTMLtags (Oh et al, 2009; Ravi and Pasca, 2008;Sumida and Torisawa, 2008; Shinzato andTorisawa, 2004).Recently, researchers have used both structuredand unstructured text for semantic-relation acqui-sition, with the aim of exploiting such differentkinds of evidence at the same time.
They ei-ther tried to improve semantic relation acquisitionby putting the different evidence together into asingle classifier (Pennacchiotti and Pantel, 2009)or to improve the coverage of semantic relationsby combining and ranking the semantic relationsobtained from two source texts (Talukdar et al,2008).In this paper we propose an algorithm calledCo-STAR.
The main contributions of this workcan be summarized as follows.?
Co-STAR is a semi-supervised learningmethod composed of two parallel and iter-ative processes over structured and unstruc-tured text.
It was inspired by bilingual co-training, which is a framework for hyponymyrelation acquisition from source texts in twolanguages (Oh et al, 2009).
Like bilingualco-training, two processes in Co-STAR op-erate independently on structured text andunstructured text.
These two processes aretrained in a supervised manner with theirinitial training data and then each of themtries to enlarge the existing training data ofthe other by iteratively exchanging what they842have learned (more precisely, by transfer-ring reliable classification results on com-mon instances to one another) (see Section4 for comparison Co-STAR and bilingualco-training).
Unlike the ensemble semanticframework (Pennacchiotti and Pantel, 2009),Co-STAR does not have a single ?master?classifier or ranker to integrate the differ-ent evidence found in structured and unstruc-tured text.
We experimentally show that, atleast in our setting, Co-STAR works betterthan a single ?master?
classifier.?
Common relation instances found in bothstructured and unstructured text act as acommunication channel between the two ac-quisition processes.
Each process in Co-STAR classifies common relation instancesand then transfers its high-confidence classi-fication results to training data of the otherprocess (as shown in Fig.
1), in order to im-prove classification results of the other pro-cess.
Moreover, the efficiency of this ex-change can be boosted by increasing the?bandwidth?
of this channel.
For this pur-pose each separate acquisition process auto-matically generates a set of relation instancesthat are likely to be negative.
In our experi-ments, we show that the above idea provedhighly effective.?
Finally, the acquisition algorithm we proposeis robust against noisy training data.
Weshow this by training one classifier in Co-STAR with manually labeled data and train-ing the other with automatically generatedbut noisy training data.
We found that Co-STAR performs well in this setting.
This is-sue is discussed in Section 6.This paper is organized as follows.
Sections 2and 3 precisely describe our algorithm.
Section 4describes related work.
Sections 5 and 6 describeour experiments and present their results.
Conclu-sions are drawn in Section 7.2 Co-STARCo-STAR consists of two processes that simul-taneously but independently extract and classifyStructured?TextsUnstructured?TextsItera?onTraining?Data?for?Structured?TextsClassifierClassifierTrainingTrainingEnlarged??Training?Data?for?Structured?TextEnlarged??Training?Data?for?Unstructured?TextsTraining?Data?For?Unstructured?TextsClassifierClassifierFurther?Enlarged?Training?Data?for?Structured?TextsFurther?Enlarged?Training?Data?for?Unstructured?TextsTrainingTrainingTrainingTraining?..
?..Common?instancesTransferring?reliable?classifica?on?results?of?classifiersTransferring?reliable?classifica?on?results?of?classifiersFigure 1: Concept of Co-STAR.hyponymy relation instances from structured andunstructured text.
The core of Co-STAR is thecollaboration between the two processes, whichcontinually exchange and compare their acquiredknowledge on hyponymy relations.
This collabo-ration is made possible through common instancesshared by both processes.
These common in-stances are classified separately by each process,but high-confidence classification results by oneprocess can be transferred as new training data tothe other.2.1 Common InstancesLet S and U represent a source (i.e.
corpus)of structured and unstructured text, respectively.In this paper, we use the hierarchical layout ofWikipedia articles and the Wikipedia categorysystem as structured text S (see Section 3.1), anda corpus of ordinary Web pages as unstructuredtext U .
Let XS and XU denote a set of hyponymyrelation candidates extracted from S and U , re-spectively.
XS is extracted from the hierarchi-cal layout of Wikipedia articles (Oh et al, 2009)and XU is extracted by lexico-syntactic patternsfor hyponymy relations (i.e., hyponym such as hy-ponymy) (Ando et al, 2004) (see Section 3 for adetailed explanation)We define two types of common instances,called ?genuine?
common instances (G) and ?vir-tual?
common instances (V ).
The set of commoninstances is denoted by Y = G ?
V .
Genuinecommon instances are hyponymy relation candi-dates found in both S and U (G = XS ?XU ).
On843the other hand, term pairs are obtained as virtualcommon instances when:?
1) they are extracted as hyponymy relationcandidates in either S or U and;?
2) they do not seem to be a hyponymy rela-tion in the other textThe first condition corresponds to XS ?
XU .Term pairs satisfying the second condition are de-fined as RS and RU , where RS ?
XS = ?
andRU ?XU = ?.RS contains term pairs that are found in theWikipedia category system but neither term ap-pears as ancestor of the other1.
For example, (nu-trition,protein) and (viruses,viral disease), respec-tively, hold a category-article relation, where nu-trition is not ancestor of viruses and vice versa inthe Wikipedia category system.
Here, term pairs,such as (nutrition, viruses) and (viral disease, nu-trition), can be ones in RS .RU is a set of term pairs extracted from Uwhen:?
they are not hyponymy relation candidates inXU and;?
they regularly co-occur in the same sentenceas arguments of the same verb (e.g., A causeB or A is made by B);As a result, term pairs in RU are thought as hold-ing some other semantic relations (e.g., A and Bin ?A cause B?
may hold a cause/effect relation)than hyponymy relation.
Finally, virtual commoninstances are defined as:?
V = (XS ?XU ) ?
(RS ?RU )The virtual common instances, from the view-point of either S or U , are unlikely to hold a hy-ponymy relation even if they are extracted as hy-ponymy relation candidates in the other text.
Thusmany virtual common instances would be a nega-tive example for hyponymy relation acquisition.On the other hand, genuine common instances(hyponymy relation candidates found in both S1A term pair often holds a hyponymy relation if one termin the term pair is a parent of the other in the Wikipedia cat-egory system (Suchanek et al, 2007).and U ) are more likely to hold a hyponymy re-lation than virtual common instances.In summary, genuine and virtual common in-stances can be used as different ground for collab-oration as well as broader collaboration channelbetween the two processes than genuine commoninstances used alone.2.2 AlgorithmWe assume that classifier c assigns class labelcl ?
{yes, no} (?yes?
(hyponymy relation) or?no?
(not a hyponymy relation)) to instances inx ?
X with confidence value r ?
R+, a non-negative real number.
We denote the classifica-tion result by classifier c as c(x) = (x, cl, r).
Weused support vector machines (SVMs) in our ex-periments and the absolute value of the distancebetween a sample and the hyperplane determinedby the SVMs as confidence value r.1: Input: Common instances (Y = G ?
V ) andthe initial training data (L0S and L0U )2: Output: Two classifiers (cnS and cnU )3: i = 04: repeat5: ciS := LEARN(LiS)6: ciU := LEARN(LiU )7: CRiS := {ciS(y)|y ?
Y , y /?
LiS ?
LiU}8: CRiU := {ciU (y)|y ?
Y , y /?
LiS ?
LiU}9: for each (y, clS , rS) ?
TopN(CRiS) and(y, clU , rU ) ?
CRiU do10: if (rS > ?
and rU < ?
)or (rS > ?
and clS = clU ) then11: L(i+1)U := L(i+1)U ?
{(y, clS)}12: end if13: end for14: for each (y, clU , rU ) ?
TopN(CRiU ) and(y, clS , rS) ?
CRiS do15: if (rU > ?
and rS < ?
)or (rU > ?
and clS = clU ) then16: L(i+1)S := L(i+1)S ?
{(y, clU )}17: end if18: end for19: i = i+ 120: until stop condition is metFigure 2: Co-STAR algorithm844The Co-STAR algorithm is given in Fig.
2.
Thealgorithm is interpreted as an iterative procedure1) to train classifiers (ciU , ciS) with the existingtraining data (LiS and LiU ) and 2) to select newtraining instances from the common instances tobe added to existing training data.
These are re-peated until stop condition is met.In the initial stage, two classifiers c0S and c0Uare trained with manually prepared labeled in-stances (or training data) L0S and L0U , respec-tively.
The learning procedure is denoted byc = LEARN(L) in lines 5?6, where c is a re-sulting classifier.
Then ciS and ciU are appliedto classify common instances in Y (lines 7?8).We denote CRiS as a set of the classification re-sults of ciS for common instances, which are notincluded in the current training data LiS ?
LiU .Lines 9?13 describe a way of selecting instancesin CRiS to be added to the existing training datain U .
During the selection, ciS acts as a teacherand ciU as a student.
TopN(CRiS) is a set ofciS(y) = (y, clS , rS), whose rS is the top-N high-est in CRiS .
(In our experiments, N = 900.)
Theteacher instructs his student the class label of y ifthe teacher can decide the class label of y with acertain level of confidence (rS > ?)
and the stu-dent satisfies one of the following two conditions:?
the student agrees with the teacher on classlabel of y (clS = clU ) or?
the student?s confidence in classifying y islow (rU < ?
)rU < ?
enables the teacher to instruct his studentin spite of their disagreement over a class label.If one of the two conditions is satisfied, (y, clS)is added to existing labeled instances L(i+1)U .
Theroles are reversed in lines 14?18, so that ciU be-comes the teacher and ciS the student.The iteration stops if the change in the differ-ence between the two classifiers is stable enough.The stability is estimated by d(ciS , ciU ) in Eq.
(1),where ?i represents the change in the averagedifference between the confidence values of thetwo classifiers in classifying common instances.We terminate the iteration if d(ciS , ciU ) is smallerthan 0.001 in three consecutive rounds (Wang andZhou, 2007).d(ciS , ciU ) = |?i ?
?(i?1)|/|?
(i?1)| (1)3 Hyponymy Relation AcquisitionIn this section we explain how each process ex-tracts hyponymy relations from its respective textsource either Wikipedia or Web pages.
Each pro-cess extracts hyponymy relation candidates (de-noted by (hyper,hypo) in this section).
Becausethere are many non-hyponymy relations in thesecandidates2, we classify hyponymy relation can-didates into correct hyponymy relation or not.
Weused SVMs (Vapnik, 1995) for the classificationin this paper.3.1 Acquisition from Wikipedia(a) Layout structureRangeSiberian tigerBengal tigerSubspeciesTaxonomyTigerMalayan tiger(b) Tree structureFigure 3: Example borrowed from Oh et al(2009): Layout and tree structures of Wikipediaarticle TIGERWe follow the method in Oh et al (2009) foracquiring hyponymy relations from the JapaneseWikipedia.
Every article is transformed into a treestructure as shown in Fig.
3, based on the items inits hierarchical layout including title, (sub)sectionheadings, and list items.
Candidate relations areextracted from this tree structure by regarding anode as a hypernym candidate and all of its subor-dinate nodes as potential hyponyms of the hyper-nym candidate (e.g., (TIGER, TAXONOMY) and(TIGER, SIBERIAN TIGER) from Fig.
3).
We ob-tained 1.9?107 Japanese hyponymy relation can-didates from Wikipedia.2Only 25?30% of candidates was true hyponymy relationin our experiments.845Type DescriptionFeature from Wikipedia Lexical Morphemes and POS of hyper and hypo; hyper and hypo themselves(?WikiFeature?)
Structure Distance between hyper and hypo in a tree structure;Lexical patterns for article or section names, where listed items often appear;Frequently used section headings in Wikipedia (e.g., ?Reference?
);Layout item type (e.g., section or list); Tree node type (e.g., root or leaf);Parent and children nodes of hyper and hypoInfobox Attribute type and its value obtained from Wikipedia infoboxesFeature from Web texts Lexical Morphemes and POS of hyper and hypo; hyper and hypo themselves(?WebFeature?)
Pattern Lexico-syntactic patterns applied to hyper and hypo;PMI score between pattern and hyponymy relation candidate (hyper,hypo)Collocation PMI score between hyper and hypoNoun Class Noun classes relevant to hyper and hypoTable 1: Feature sets (WikiFeature and WebFeature): hyper and hypo represent hypernym and hyponymparts of hyponymy relation candidates, respectively.As features for classification we used lex-ical, structure, and infobox information fromWikipedia (WikiFeature), as shown in Table 1.Because they are the same feature sets as thoseused in Oh et al (2009), here we just give a briefoverview of the feature sets.
Lexical features3are used to recognize the lexical evidence forhyponymy relations encoded in hyper and hypo.For example, the common head morpheme tigerin (TIGER, BENGAL TIGER) can be used as thelexical evidence.
Such information is providedalong with the words/morphemes and the parts ofspeech of hyper and hypo, which can be multi-word/morpheme nouns.Structure features provide evidence found inlayout or tree structures for hyponymy relations.For example, hyponymy relations (TIGER, BEN-GAL TIGER) and (TIGER,MALAYAN TIGER) canbe obtained from tree structure ?
(root node, chil-dren nodes of Subspecies)?
in Fig 3.3.2 Acquisition from Web TextsAs the target for hyponymy relation acquisitionfrom the Web, we used 5 ?
107 pages fromthe TSUBAKI corpus (Shinzato et al, 2008),a 108 page Japanese Web corpus that was de-pendency parsed with KNP (Kurohashi-NagaoParser) (Kurohashi and Kawahara, 2005).
Hy-ponymy relation candidates are extracted from thecorpus based on the lexico-syntactic patterns suchas ?hypo nado hyper (hyper such as hypo)?
and?hypo to iu hyper (hyper called hypo)?
(Ando3MeCab (http://mecab.sourceforge.net/)was used to provide the lexical features.et al, 2004).
We extracted 6 ?
106 Japanesehyponymy relation candidates from the JapaneseWeb texts.
Features (WebFeature) used for classi-fication are summarized in Table 1.
Similar to thehyponymy relation acquisition from Wikipedia,lexical features are used to recognize the lexicalevidence for hyponymy relations.Lexico-syntactic patterns for hyponymy rela-tion show different coverage and accuracy in hy-ponymy relation acquisition (Ando et al, 2004).Further if multiple lexico-syntactic patterns sup-port acquisition of hyponymy relation candidates,these candidates are more likely to be actual hy-ponymy relations.
The pattern feature of hy-ponymy relation candidates is used for these ev-idence.We use PMI (point-wise mutual information)of hyponymy relation candidate (hyper, hypo) asa collocation feature (Pantel and Ravichandran,2004), where we assume that hyper and hypo incandidates would frequently co-occur in the samesentence if they hold a hyponymy relation.Semantic noun classes have been regarded asuseful information in semantic relation acquisi-tion (De Saeger et al, 2009).
EM-based clus-tering (Kazama and Torisawa, 2008) is used forobtaining 500 semantic noun classes4 from 5 ?105 nouns (including single-word and multi-wordones) and their 4?
108 dependency relations with5 ?
105 verbs and other nouns in our target Web4Because EM clustering provides a probability distri-bution over noun class nc, we obtain discrete classes ofeach noun n with a probability threshold p(nc|n) ?0.2 (De Saeger et al, 2009).846Co-training Bilingual co-training Co-STAR(Blum and Mitchell, 1998) (Oh et al, 2009) (Proposed method)Instance space Same Different Almost differentFeature space Split by human decision Split by languages Split by source textsCommon instances Genuine-common Genuine-common Genuine-common and(or All unlabeled) instances instances (Translatable) virtual-common instancesTable 2: Differences among co-training, bilingual co-training, and Co-STARcorpus.
For example, noun class C311 includesbiological or chemical substances such as tatou(polysaccharide) and yuukikagoubutsu (organiccompounds).
Noun classes (i.e., C311) relevant tohyper and hypo, respectively, are used as a nounclass feature.4 Related WorkThere are two frameworks, which are most rele-vant to our work ?
bilingual co-training and en-semble semantics.The main difference between bilingual co-training and Co-STAR lies in an instance space.In bilingual co-training, instances are in differentspaces divided by languages while, in Co-STAR,many instances are in different spaces divided bytheir source texts.
Table 2 shows differences be-tween co-training, bilingual co-training and Co-STAR.Ensemble semantics is a relation acquisitionframework, where semantic relation candidatesare extracted from multiple sources and a singleranker ranks or classifies the candidates in the fi-nal step (Pennacchiotti and Pantel, 2009).
In en-semble semantics, one ranker is in charge of rank-ing all candidates extracted from multiple sources;while one classifier classifies candidates extractedfrom one source in Co-STAR.5 ExperimentsWe used the July version of Japanese Wikipedia(jawiki-20090701) as structured text.
We ran-domly selected 24,000 hyponymy relation candi-dates from those identified in Wikipedia and man-ually checked them.
20,000 of these samples wereused as training data for our initial classifier, therest was equally divided into development and testdata for Wikipedia.
They are called ?WikiSet.
?As unstructured text, we used 5 ?
107 JapaneseWeb pages in the TSUBAKI corpus (Shinzato etal., 2008).
Here, we manually checked 9,500hyponymy relation candidates selected randomlyfrom Web texts.
7,500 of these were used as train-ing data.
The rest was split into development andtest data.
We named this data ?WebSet?.In both classifiers, the development data wasused to select the optimal parameters, and the testdata was used to evaluate our system.
We usedTinySVM (TinySVM, 2002) with a polynomialkernel of degree 2 as a classifier.
?
(the thresholdvalue indicating high confidence), ?
(the thresh-old value indicating low confidence), and TopN(the maximum number of training instances to beadded to the existing training data in each iter-ation) were selected through experiments on thedevelopment set.
The combination of ?
= 1,?
= 0.3, and TopN=900 showed the best perfor-mance and was used in the following experiments.Evaluation was done by precision (P ), recall (R),and F-measure (F ).5.1 ResultsWe compare six systems.
Three of these, B1?B3,show the effect of different feature sets (?Wik-iFeature?
and ?WebFeature?
in Table 1) and dif-ferent training data.
We trained two separate clas-sifiers in B1 and B2, while we integrated featuresets and training data for training a single classi-fier in B3.
The classifiers in these three systemsare trained with manually prepared training data(?WikiSet?
and ?WebSet?).
For the purpose of ourexperiment, we consider B3 as the closest possibleapproximation of the ensemble semantics frame-work (Pennacchiotti and Pantel, 2009).?
B1 consists of two completely independentclassifiers.
Both S and U classifiers aretrained and tested on their own feature anddata sets (respectively ?WikiSet + WikiFea-ture?
and ?WebSet + WebFeature?).847?
B2 is the same as B1, except that both clas-sifiers are trained with all available trainingdata ?
WikiSet and WebSet are combined(27,500 training instances in total).
However,each classifier only uses its own feature set(WikiFeature or WebFeature)5.?
B3 adds a master classifier to B1.
This thirdclassifier is trained on the complete 27,500training instances (same as B2) using allavailable features from Table 1, includingeach instance?s SVM scores obtained fromthe two B1 classifiers6.
The verdict of themaster classifier is considered to be the finalclassification result.The other three systems, BICO, Co-B, and Co-STAR (our proposed method), are for compari-son between bilingual co-training (Oh et al, 2009)(BICO) and variants of Co-STAR (Co-B and Co-STAR).
Especially, we prepared Co-B and Co-STAR to show the effect of different configura-tions of common instances on the Co-STAR al-gorithm.
We use both B1 and B2 as the initialclassifiers of Co-B and Co-STAR.
We notate Co-B and Co-STAR without ???
when B1 is used astheir initial classifier and those with ???
when B2is used.?
BICO implements the bilingual co-trainingalgorithm of (Oh et al, 2009), in whichtwo processes collaboratively acquire hy-ponymy relations in two different languages.For BICO, we prepared 20,000 English and20,000 Japanese training samples (Japaneseones are the same as training data in theWikiSet) by hand.?
Co-B is a variant of Co-STAR that uses onlythe genuine-common instances as commoninstances (67,000 instances)7, to demonstrate5Note that training instances from WebSet (or WikiSet)can have WikiFeature (or WebFeature) if they also appearin Wikipedia (or Web corpus).
But they can always havelexical feature, the common feature set between WikiFeatureand WebFeature.6SVM scores are assigned to the instances in training datain a 10-fold cross validation manner.7Co-B can be considered as conventional co-training (Blum and Mitchell, 1998) in the sense thattwo classifiers collaborate through actual common instances.the effectiveness of the virtual common in-stances.?
Co-STAR is our proposed method, whichuses both genuine-common and virtual-common instances (643,000 instances in to-tal).WebSet WikiSetP R F P R FB1 84.3 65.2 73.5 87.8 74.7 80.7B2 83.4 69.6 75.9 87.4 79.5 83.2B3 82.2 72.0 76.8 86.1 77.7 81.7BICO N/A N/A N/A 84.5 81.8 83.1Co-B 86.2 63.5 73.2 89.7 74.1 81.2Co-B?
85.5 69.9 77.0 89.6 76.5 82.5Co-STAR 85.9 76.0 80.6 88.0 81.8 84.8Co-STAR?
83.3 80.7 82.0 87.6 81.8 84.6Table 3: Comparison of different systemsTable 3 summarizes the result.
Features forcommon instances in Co-B and Co-STAR are pre-pared in the same way as training data in B2, sothat both classifiers can classify the common in-stances with their trained feature sets.Comparison between B1?B3 shows that B2 andB3 outperform B1 in F-measure.
More train-ing data used in B2?B3 (27,500 instances forboth WebSet and WikiSet) results in higher per-formance than that of B1 (7,500 and 20,000 in-stances used separately).
We think that the lexicalfeatures, assigned regardless of source text to in-stances in B2?B3, are mainly responsible for theperformance gain over B1, as they are the leastdomain-dependent type of features.
B2?B3 arecomposed of different number of classifiers, eachof which is trained with different feature sets andtraining instances.
Despite this difference, B2 andB3 showed similar performance in F-measure.Co-STAR outperformed the algorithm similarto the ensemble semantics framework (B3), al-though we admit that a more extensive com-parison is desirable.
Further Co-STAR outper-formed BICO.
While the manual cost for build-ing the initial training data used in Co-STARand BICO is hard to quantify, Co-STAR achievesbetter performance with fewer training data intotal (27,500 instances) than BICO (40,000 in-stances).
The difference in performance betweenCo-B and Co-STAR shows the effectiveness of848the automatically generated virtual-common in-stances.
From these comparison, we can see thatvirtual-common instances coupled with genuine-common instances can be leveraged to enablemore effective collaboration between the two clas-sifiers in Co-STAR.As a result, our proposed method outperformsthe others in F-measure by 1.4?8.5%.
We ob-tained 4.3 ?
105 hyponymy relations from Webtexts and 4.6?
106 ones from Wikipedia8.6 Co-STAR with AutomaticallyGenerated Training DataFor Co-STAR, we need two sets of manually pre-pared training data, one for structured text and theother for unstructured text.
As in any other su-pervised system, the cost of preparing the trainingdata is an important issue.
We therefore investi-gated whether Co-STAR can be trained for a lowercost by generating more of its training data auto-matically.We automatically built training data for Webtexts by using definition sentences9 and categorynames in the Wikipedia articles, while we stuck tomanually prepared training data for Wikipedia.
Toobtain hypernyms from Wikipedia article names,we used definition-specific lexico-syntactic pat-terns such as ?hyponym is hypernym?
and ?hy-ponym is a type of hypernym?
(Kazama and Tori-sawa, 2007; Sumida and Torisawa, 2008).
Then,we extracted hyponymy relations consisting ofpairs of Wikipedia category names and their mem-ber articles when the Wikipedia category nameand the hypernym obtained from the definitionof the Wikipedia article shared the same headword.
Next, we selected a subset of the extractedhyponymy relations that are also hyponymy re-lation candidates in Web texts, as positive in-stances for hyponymy relation acquisition fromWeb text.
We obtained around 15,000 positive in-stances in this way.
Negative instances were cho-sen from virtual-common instances, which alsooriginated from the Wikipedia category systemand hyponymy relation candidates in Web texts8We obtained them with 90% precision by setting theSVM score threshold to 0.23 for Web texts and 0.1 forWikipedia.9The first sentences of Wikipedia articles.
(around 293,000 instances).The automatically built training data was noisyand its size was much bigger than manually pre-pared training data in WebSet.
Thus 7,500 in-stances as training data (the same number of man-ually built training data in WebSet) were ran-domly chosen from the positive and negative in-stances with a positive:negative ratio of 1:410.WebSet WikiSetP R F P R FB1 81.0 47.6 60.0 87.8 74.7 80.7B2 80.0 55.4 65.5 87.1 79.5 83.1B3 82.0 33.7 47.8 87.1 75.6 81.0Co-STAR 82.2 60.8 69.9 87.3 80.7 83.8Co-STAR?
79.2 69.6 74.1 87.0 81.8 84.4Table 4: Results with automatically generatedtraining dataWith the automatically built training data forWeb texts and manually prepared training data forWikipedia, we evaluated B1?B3 and Co-STAR,which are the same systems in Table 3.
The resultsin Table 4 are encouraging.
Co-STAR was robusteven when faced with noisy training data.
FurtherCo-STAR showed better performance than B1?B3, although its performance in Table 4 dropped abit compared to Table 3.
This result shows that wecan reduce the cost of manually preparing trainingdata for Co-STAR with only small loss of the per-formance.7 ConclusionThis paper proposed Co-STAR, an algorithm forhyponymy relation acquisition from structuredand unstructured text.
In Co-STAR, two indepen-dent processes of hyponymy relation acquisitionfrom structured texts and unstructured texts, col-laborate in an iterative manner through commoninstances.
To improve this collaboration, we in-troduced virtual-common instances.Through a series of experiments, we showedthat Co-STAR outperforms baseline systems andvirtual-common instances can be leveraged toachieve better performance.
We also showed thatCo-STAR is robust against noisy training data,which requires less human effort to prepare it.10We select the ratio by testing different ratio from 1:2 to1:5 with our development data in WebSet and B1.849ReferencesAndo, Maya, Satoshi Sekine, and Shun Ishiza.
2004.Automatic extraction of hyponyms from Japanesenewspaper using lexico-syntactic patterns.
In Proc.of LREC ?04.Blum, Avrim and Tom Mitchell.
1998.
Combin-ing labeled and unlabeled data with co-training.
InCOLT?
98: Proceedings of the eleventh annual con-ference on Computational learning theory, pages92?100.De Saeger, Stijn, Kentaro Torisawa, Jun?ichi Kazama,Kow Kuroda, and Masaki Murata.
2009.
Largescale relation acquisition using class dependent pat-terns.
In Proc.
of ICDM 2009, pages 764?769.Hearst, Marti A.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proceedings ofthe 14th conference on Computational linguistics,pages 539?545.Kazama, Jun?ichi and Kentaro Torisawa.
2007.
Ex-ploiting Wikipedia as external knowledge for namedentity recognition.
In Proc.
of Joint Conference onEmpirical Methods in Natural Language Process-ing and Computational Natural Language Learn-ing, pages 698?707.Kazama, Jun?ichi and Kentaro Torisawa.
2008.
In-ducing gazetteers for named entity recognition bylarge-scale clustering of dependency relations.
InProceedings of ACL-08: HLT, pages 407?415.Kurohashi, Sadao and Daisuke Kawahara.
2005.
KNP(Kurohashi-Nagao Parser) 2.0 users manual.Oh, Jong-Hoon, Kiyotaka Uchimoto, and KentaroTorisawa.
2009.
Bilingual co-training for mono-lingual hyponymy-relation acquisition.
In Proc.
ofACL-09: IJCNLP, pages 432?440.Pantel, Patrick and Deepak Ravichandran.
2004.
Au-tomatically labeling semantic classes.
In Proc.
ofHLT-NAACL ?04, pages 321?328.Pantel, Patrick, Eric Crestan, Arkady Borkovsky, Ana-Maria Popescu, and Vishnu Vyas.
2009.
Web-scaledistributional similarity and entity set expansion.
InProceedings of EMNLP ?09, pages 938?947.Pennacchiotti, Marco and Patrick Pantel.
2009.
En-tity extraction via ensemble semantics.
In Proceed-ings of the 2009 Conference on Empirical Methodsin Natural Language Processing, pages 238?247.Ravi, Sujith and Marius Pasca.
2008.
Using structuredtext for large-scale attribute extraction.
In CIKM-08, pages 1183?1192.Shinzato, Keiji and Kentaro Torisawa.
2004.
Ex-tracting hyponyms of prespecified hypernyms fromitemizations and headings in web documents.
InProceedings of COLING ?04, pages 938?944.Shinzato, Keiji, Tomohide Shibata, Daisuke Kawa-hara, Chikara Hashimoto, and Sadao Kurohashi.2008.
Tsubaki: An open search engine infrastruc-ture for developing new information access.
In Pro-ceedings of IJCNLP ?08, pages 189?196.Snow, Rion, Daniel Jurafsky, and Andrew Y. Ng.2006.
Semantic taxonomy induction from heteroge-nous evidence.
In Proceedings of the 21st Inter-national Conference on Computational Linguisticsand the 44th annual meeting of the Association forComputational Linguistics, pages 801?808.Suchanek, Fabian M., Gjergji Kasneci, and GerhardWeikum.
2007.
Yago: A Core of Semantic Knowl-edge.
In Proc.
of WWW ?07, pages 697?706.Sumida, Asuka and Kentaro Torisawa.
2008.
Hack-ing Wikipedia for hyponymy relation acquisition.In Proc.
of the Third International Joint Conferenceon Natural Language Processing (IJCNLP), pages883?888, January.Talukdar, Partha Pratim, Joseph Reisinger, MariusPasca, Deepak Ravichandran, Rahul Bhagat, andFernando Pereira.
2008.
Weakly-supervised acqui-sition of labeled class instances using graph randomwalks.
In Proc.
of EMNLP08, pages 582?590.TinySVM.
2002. http://chasen.org/?taku/software/TinySVM.Van Durme, Benjamin and Marius Pasca.
2008.
Find-ing cars, goddesses and enzymes: Parametrizableacquisition of labeled instances for open-domain in-formation extraction.
In Proc.
of AAAI08, pages1243?1248.Vapnik, Vladimir N. 1995.
The nature of statisticallearning theory.
Springer-Verlag New York, Inc.,New York, NY, USA.Wang, Wei and Zhi-Hua Zhou.
2007.
Analyzing co-training style algorithms.
In ECML ?07: Proceed-ings of the 18th European conference on MachineLearning, pages 454?465.850
