Session 10: THE LEXICONRalph GrishmanDepar tment  o f  Computer  ScienceNew York Univers i tyNew York, NY  10003Work in natural language processing has been moving rapidlytowards the creation of large-scale systems addressed to realtasks.
One aspect of this has been a rapid increase in thevocabulary size of these systems.
"Toy" lexicons of 500 or1000 words are no longer adequate; several tens of thousandslexical entries will be required, at a minimum.
Developersof machine translation systems - -  who have confronted theproblems of "real," largely unrestricted, text much longerthan most other natural anguage researchers - - have longrecognized the central role of large, high quality lexicons.Such broad-coverage lexical resources are of course costlyand time-consuming to develop.
Fortunately, however, thereseems a reasonable prospect hat they can be developed asshared resources.
Current lexicons record for the most partrelatively shallow (simply structured) information about thepronunciation, syntax, and semantics of words.
There appearsto be a general agreement between different system developerson at least some of the features to be captured in the lexicon,even though these features may be represented very differentlyin the various systems.
The agreement seems to be clearestregarding syntactic information, but there is reason to believethat at least a partial consensus can be reached regardingpronunciation and possibly for semantic information as well.All of the presentations in this session addressed the need forbroad-coverage lexical resources.
In addition to the papers in-cluded in this volume, there were presentations byProf.
MarkLiberman of the Univ.
of Pennsylvania and Prof. Makoto Na-gao of Kyoto Univ.Prof.
Liberman discussed some of the plans of the LinguisticData Consortium.
The Linguistic Data Consortium was cre-ated in 1992 with a combination of government and privatefunds in order to create a rich repository of resources for re-search and development of natural language systems.
As partof its mandate, the Consortium intends to assemble a rangeof lexical resources including pronunciation, syntactic, andsemantic information, under the general heading of COM-LEX (a COMmon LEXicon).
Among these efforts, the workon a syntactic lexicon - -  COMLEX Syntax - -  is furthestadvanced; the paper by the group at New York Universitydescribes the status of this project.These works are small in scale when compared to the dic-tionary el~forts in Japan, which were summarized in Prof.Nagao's presentation.
The largest of these efforts is the EDRProject of the Japan Electronic Dictionary Research Institute.This project is producing a collection of interrelated dictionar-ies, including a Japanese dictionary and an English dictionary(each of about 300,000 entries) whose entries are both linkedto a "concept dictionary".Prof.
George Miller and his associates at Princeton Univer-sity have for the past several years been constructing a lex-ical knowledge base called WordNet.
In WordNet, Englishnouns, verbs, and adjectives are organized into synonym sets("synsets"), each representing one underlying lexical concept;these synsets are connected by various semantic relations,such as antonymy, hyponymy, and meronymy.
A word mayhave several meanings and so be assigned to several synsets;a word with its synset can thus be used to identify a partic-ular sense of a word.
The paper "A Semantic Concordance"describes an ongoing effort to "tag" a corpus by identifying,for each content word (noun, verb, adjective, and adverb), thesynset o which it belongs in that context.Corpus tagging can be even more valuable if the same cor-pus is tagged for several different lexical characteristics.
Forexample, the COMLEX Syntax group is considering the pos-sibility of tagging the verbs in a corpus according to thesubcategorization frame used in each context.
Although theCOMLEX Syntax Lexicon will initially not be sense distin-guished, correlating the subcategorization tags with WordNetsense tags would give some indication of the correspondencebetween subcategorizations a d word senses.Identifying the general vocabulary - -  nouns, verbs, adjec-tives .... - -  is only part of the battle in lexical analysis.
Manytexts are replete with proper nouns (names).
Although we caninclude the most frequent of these in our lexicon, the list cannever be complete.
A good lexicon must herefore be comple-mented by effective strategies for identifying and classifyingproper nouns, which typically involve some combination ofpattern matching with information from the lexicon.
The finalpaper in this session, from Syracuse University, describes anapproach to proper noun identification and and evaluation ofthis approach on a sample from the Tipster corpus.299
