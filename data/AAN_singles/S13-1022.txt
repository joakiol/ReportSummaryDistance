Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conferenceand the Shared Task, pages 155?161, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational LinguisticsSRIUBC-Core: Multiword Soft Similarity Models for Textual SimilarityEric YehSRI InternationalMenlo Park, CA USAyeh@ai.sri.comEneko AgirreUniversity of Basque CountryDonostia, Basque Countrye.agirre@ehu.esAbstractIn this year?s Semantic Textual Similarityevaluation, we explore the contribution ofmodels that provide soft similarity scoresacross spans of multiple words, over the pre-vious year?s system.
To this end, we ex-plored the use of neural probabilistic languagemodels and a TF-IDF weighted variant of Ex-plicit Semantic Analysis.
The neural languagemodel systems used vector representations ofindividual words, where these vectors werederived by training them against the contextof words encountered, and thus reflect the dis-tributional characteristics of their usage.
Togenerate a similarity score between spans, weexperimented with using tiled vectors and Re-stricted Boltzmann Machines to identify simi-lar encodings.
We find that these soft similar-ity methods generally outperformed our previ-ous year?s systems, albeit they did not performas well in the overall rankings.
A simple anal-ysis of the soft similarity resources over twoword phrases is provided, and future areas ofimprovement are described.1 IntroductionFor this year?s Semantic Textual Similarity (STS)evaluation, we built upon the best performing sys-tem we deployed last year with several methods forexploring the soft similarity between windows ofwords, instead of relying just on single token-to-token similarities.
From the previous year?s eval-uation, we were impressed by the performance offeatures derived from bigrams and skip bigrams.
Bi-grams capture the relationship between two concur-rent words, while skip bigrams can capture longerdistance relationships.
We found that characterizingthe overlap in skip bigrams between the sentences ina STS problem pair proved to be a major contributorto last year?s system?s performance.Skip bigrams were matched on two criteria, lexi-cal matches, and via part of speech (POS).
Lexicalmatching is brittle, and even if the match were madeon lemmas, we lose the ability to match against syn-onyms.
We could rely on the token-to-token simi-larity methods to account for these non-lexical sim-ilarities, but these do not account for sequence nordependencies in the sentencees.
Using POS basedmatching allows for a level of generalization, but ata much broader level.
What we would like to haveis a model that can capture these long distance re-lationships at a level that is less broad than POSmatching, but allows for a soft similarity scoring be-tween words.
In addition, the ability to encompassa larger window without having to manually insertskips would be desirable as well.To this end we decided to explore the use of neu-ral probabilistic language models (NLPM) for cap-turing this kind of behavior (Bengio et al 2003).NLPMs represent individual words as real valuedvectors, often at a much lower dimensionality thanthe original vocabulary.
By training these rep-resentations to maximize a criterion such as log-likelihood of target word given the other words in itsneighborhood, the word vectors themselves can cap-ture commonalities between words that have beenused in similar contexts.
In previous studies, thesevectors themselves can capture distributionally de-rived similarities, by directly comparing the wordvectors themselves using simple measures such as155Euclidean distance (Collobert and Weston, 2008).In addition, we fielded a variant of ExplicitSemantic Analysis (Gabrilovich and Markovitch,2009) that used TF-IDF weightings, instead of usingthe raw concept vectors themselves.
From previousexperiments, we found that using TF-IDF weight-ings on the words in a pair gave a boost in perfor-mance over sentence length comparisons and above,so this simple modification was incorporated intoour system.In order to identify the contribution of these softsimilarity methods against last year?s system, wefielded three systems:1.
System 1, the system from the previous year,incorporating semantic similarity resources,precision focused and Bilingual Evaluation Un-derstudy (BLEU) overlaps (Papineni et al2002), and several types of skip-bigrams.2.
System 2, features just the new NLPM scoresand TFIDF-ESA.3.
System 3, combines System 1 and System 2.For the rest of this system description, we brieflydescribe the previous year?s system (System 1), theTFIDF weighted Explicit Semantic Analysis, andthe NLPM systems.
We then describe the experi-ment setup, and follow up with results and analysis.2 System 1The system we used in SemEval 2012 consisted ofthe following components:1.
Resource based word-to-word similarities,combined using a Semantic Matrix (Fernandoand Stevenson, 2008).2.
Cosine-based lexical overlap measure.3.
Bilingual Evaluation Understudy (BLEU) (Pa-pineni et al 2002) lexical overlap.4.
Precision focused part-of-speech (POS) fea-tures.5.
Lexical match skip-bigram overlap.6.
Precision focused skip-bigram POS features.The Semantic Matrix assesses similarity betweena pair s1 and s2 by summing over all of the wordto word similarities between the pair, subject to nor-malization, as given by Formula 1.sim(s1, s2) =vT1 Wv2?v1?
?v2?
(1)The matrix W is a symmetric matrix that en-codes the word to word similarities, derived fromthe underlying resources this is drawn from.
Fromthe previous year?s assessment, we used similaritiesderived from Personalized PageRank (Agirre et al2010) over WordNet (Fellbaum, 1998), the ExplicitSemantic Analysis (Gabrilovich and Markovitch,2009) concept vector signatures for each lemma, andthe Dekang Lin Proximity-based Thesaurus 1.The cosine-based lexical overlap measure simplymeasures the cosine similarity, using strict lexicaloverlap, between the sentence pairs.
The BLEU,precision focused POS, and skip-bigrams are direc-tional measures, which measure how well a targetsentence matches a source sentence.
To score pair ofsentences, we simply averaged the score where onesentence is the source, the other the target, and thenvice versa.
These directional measures were origi-nally used as a precision focused means to assess thequality of machine translations output against ref-erence translations.
Following (Finch et al 2005),these measures have also been shown to be good forassessing semantic similarity between pairs of sen-tences.For BLEU, we measured how well ngrams of or-der one through four were matched by the target sen-tence, matching solely on lexical matches, or POSmatches.
Skip bigrams performed similarly, exceptthe bigrams were not contiguous.
The precision fo-cused POS features assess how well each POS tagfound in the source sentence has been matched inthe target sentence, where the matches are first donevia a lemma match.To combine the scores from these features, weused the LIBSVM Support Vector Regression (SVR)package (Chang and Lin, 2011), trained on the train-ing pair gold scores.
Per the previous year, we useda radial basis kernel with a degree of three.1http://webdocs.cs.ualberta.ca/ lindek/downloads.htm156For a more in-depth description of System 1,please refer to (Yeh and Agirre, 2012).3 TFIDF-ESAThis year instead of using Explicit Semantic Anal-ysis (ESA) to populate a word-by-word similaritymatrix, we used ESA to derive a similarity score be-tween the sentences in a STS pair.
For a given sen-tence, we basically treated it as an IR query againstthe ESA concept-base: we tokenized the words, ex-tracted the ESA concept vectors, and performed aTFIDF weighted average to arrive at the sentencevector.
A cutoff of the top 1000 scoring conceptswas further applied, per previous experience, to im-prove performance.
The similarity score for twosentence vectors was computed using cosine simi-larity.4 Neural Probabilistic Language ModelsNeural probabilistic language models representwords as real valued vectors, where these vectors aretrained to jointly capture the distributional statisticsof their context words and the positions these wordsoccur at.
These representations are usually at a muchlower dimensionality than that of the original vocab-ulary, forcing some form of compression to occur inthe vocabulary.
The intent is to train a model thatcan account for words that have not been observedin a given context before, but that word vector hasenough similarity to another word that has been en-countered in that context before.Earlier models simply learnt how to model thenext word in a sequence, where each word in the vo-cabulary is initially represented by a randomly ini-tialized vector.
For each instance, a larger vector isassembled from the concatenation of the vectors ofthe words observed, and act as inputs into a model.This model itself is optimized to maximize the like-lihood of the next word in the observed sequence,with the errors backpropagated through the vectors,with the parameters for the vectors being tied (Ben-gio et al 2003).In later studies, these representations are theproduct of training a neural network to maxi-mize the margin between the scores it assigns toobserved ?correct?
examples, which should havehigher scores, and ?corrupted examples,?
where the"heart"dim=50"attack"dim=50"heart attack"dim=100Figure 1: Vector Window encoding for the phrase ?heartattack.
?token of interest is swapped out to produce an in-correct example and preferably a lower score.
Asshown in (Collobert and Weston, 2008) and then(Huang et al 2012), simple distance measures us-ing the representations derived from this process areboth useful for assessing word similarity and relat-edness.
For this study, we used the contextuallytrained language vectors provided by (Huang et al2012), which were trained to maximize the marginbetween training pairs and to account for documentcontext as well.
The dimensionality of these vectorswas 50.As we are interested in capturing information ata level greater than individual words, we used twomethods to combine these NLPM word vectors torepresent an order n ngram: a Vector Windowwhere we simply concatenated the word vectors, andone that relied on encodings learnt by RestrictedBoltzmann Machines.For this work, we experimented with generatingencodings for ngrams sized 2,3,5,10, and 21.
Thesmaller sizes correspond to commonly those com-monly used to match ngrams, while the larger oneswere used to take advantage of the reduced sparsity.Similarities between a pair of ngram encodings isgiven similarity of their vector encodings.4.1 Vector WindowThe most direct way to encode an order n ngram asa vector is to concatenate the n NLPM word vectorstogether, in order.
For example, to encode ?heartattack?, the vectors for ?heart?
and ?attack?, bothwith dimensionality 50, are linked together to forma larger vector with dimensionality 100 (Figure 1).For size n vector windows where the total numberof tokens is less than n, we pad the left and rightsides of the window with a ?negative?
token, whichwas selected to be a vector that, on the average, isanticorrelated with all the vectors in the vocabulary.157"heart attack"compressed encodingRBM trained encoderoriginal vectorFigure 2: Using a RBM trained compressor to generate acompressed encoding of ?heart attack.
?4.2 Restricted Boltzmann MachinesAlthough the word vectors we used were trainedagainst a ten word context, the vector windows maynot be able to describe similarities at multiwordlevel, as the method is still performing comparisonsat a word-to-word level.
For example the vector win-dow score for the related phrases heart attack andcardiac arrest is 0.35.
In order to account for sim-ilarities at a multiword level, we trained RestrictedBoltzmann Machines (RBM) to further encode thesevector windows (Hinton, 2002).
A RBM is a bi-partite undirected graphical model, where the onlyedges are between a layer of input variables and alayer of latent variables.
The latent layer consists ofsigmoid units, allowing for non-linear combinationsof the inputs.
The training objective is to learn a setof weights that maximize the likelihood of trainingobservations, and given the independences inherent,in the model it can be trained quickly and effectivelyvia Contrastive Divergence.
The end effect is thesystem attempts to force the latent layer to learn anencoding of the input variables, usually at a lower di-mensionality.
In our case, by compressing their dis-tributional representations we hope to amplify sig-nificant similarities between multiword expressions,albeit for those of the same size.To derive a RBM based encoding, we first gen-erate a vector window for the ngram, and then usedthe trained RBM to arrive at the compressed vector(Figure 2).
As before, we derive a similarity scorebetween two RBM based encodings by comparingtheir cosine distance.Following the above example, the vectors from anRBM trained system for heart attack and cardiac ar-rest score the pair at a higher similarity, 0.54.
Forphrases that are unrelated, comparing door key withcardiac arrest gives a score of -0.14 with the vectorwindow, and RBM this is -0.17.To train a RBM encoder for order n ngrams,we generated n sized vector windows over ngramsdrawn from the English language articles inWikipedia.
The language dump was filtered to largersized articles, in order to avoid pages likely to becontent-free, such as redirects.
The training setsize consisted of 35,581,434 words, which was splitapart into 1,519,256 sentences using the OpenNLPsentence splitter tool 2.
The dimensionality of theencoding layer was set to 50 for window sizes 2,3,5,and 200 for the larger windows.4.3 Combining word and ngram similarityscoresIn order to produce an overall similarity score, weused a variant of the weighted variant of the simi-larity combination method given in (Mihalcea et al2006).
Here, we generated a directional similarityscore from a source to target by the following,sim(S, T ) =?s?S maxSim(s, T )|S|(2)where maxSim(s, T ) represents the maximumsimilarity between the token s and the set of tokensin the target sentence, T .
In the case of ngrams withorder 2 or greater, we treat each ngram as a token forthe combination.avgsim(T1, T2) =12(sim(T1, T2) + sim(T2, T1))(3)Unlike the original method, we treated each termequally, in order to account for ngrams with order2 and above.
We also did not filter based off of thepart of speech, relying on the scores themselves tohelp perform the filtering.In addition to the given word window sizes,we also directly assess the word-to-word similarityscores by comparing the word vectors directly, usinga window size of one.5 Evaluation SetupSystem 2, the TFIDF-ESA score for a pair is a fea-ture.
For each of the given ngram sizes, we treated2http://opennlp.apache.org/158Training (2012) Test (2013)Surprise1 (ONWN) FNWNMSRPar HeadlinesSurprise1 (ONWN) ONWNSurprise2 (SMT) SMTTable 1: Train (2012) and Test (2013) sets used to trainthe regressors.the ngram similarity scores from the Vector Windowand RBM methods as individual features.
System3 combines the features from System 2 with thosefrom System 1.
For Systems 2 and 3, the SVR setupused by System 1 was used to develop scorers.
As notraining immediate training sets were provided forthe evaluation sets, we used the train and test parti-tions given in Table 1, training on both the 2012 trainand test data, where gold scores were available.6 Results and DiscussionThe results of our three runs are given in the top halfof Table 2.
To get a better sense of the contributionof the new components, we also ran the NLPM vec-tor window and RBM window models and TFIDF-ESA components individually against the test sets.The NLPM system was trained using the same SVRsetup as the main experiment.In order to provide a lexical match comparison forthe NLPM system, we experimented with a ngrammatching system, where ngrams of size 1,2,3,5,10,and 21 were used to generate similarity scores viathe same combination method as the NLPM models.Here, hard matching was performed, where match-ing ngrams were given a score of 1, else 0.
Again,we used the main experiment SVR setup to combinethe scores from the various ngram sizes.We found that overall the previous year?s sys-tem did not perform adequately on the evaluationdatasets, short of the headlines dataset.
Oddlyenough, TFIDF-ESA by itself would have arrived ata good correlation with OnWN: one possible expla-nation for this would be the fact that TFIDF-ESAby itself is essentially an order-free ?bag of words?model that assesses soft token to token similarity.
Asthe other systems incorporate either some notion ofsequence and/or require strict lexical matching, it ispossible that characterization does not help with theOnWN sense definitions.Combining the new features with the previousyear?s system gave poorer performance; a prelimi-nary assessment over the training sets showed somedegree of overfitting, likely due to high correlationbetween the NLPM features and last year?s direc-tional measures.When using the same combination method, ngrammatching via lexical content over ngrams gavepoorer results than those from NLPM models, asgiven in Table 2.
This would also argue for identi-fying better combination methods than the averagedmaximum similarity method used here.What is interesting to note is that the NLPM andTFIDF-ESA systems do not rely on any part ofspeech information, nor hand-crafted semantic sim-ilarity resources.
Instead, these methods are de-rived from large scale corpora, and generally out-performed the previous year?s system which reliedon that extra information.To get a better understanding of the NLPM andTFIDF-ESA models, we compared how the com-ponents would score the similarity between pairs oftwo word phrases, given in Table 3.
At least over thissmall sampling we genearted, we found that in gen-eral the RBM method tended to have a much widerrange of scores than the Vector Window, althoughboth methods were very correlated.
Both systemshad very low correlation with TFIDF-ESA.7 Future WorkOne area of improvement would be to develop a bet-ter method for combining the various ngram simi-larity scores provided by the NLPMs.
When usinglexical matching of ngrams, we found that the com-bination method used here proved inferior to the di-rectional measures from the previous year?s systems.This would argue for a better way to use the NLPMs.As training STS pairs are available with gold scores,this would argue for some form of supervised train-ing.
For training similarities between multiword ex-pressions, proxy measures for similarity, such as theNormalized Google Distance (Cilibrasi and Vita?nyi,2004), may be feasible.Another avenue would be to allow the NLPMmethods to encode arbitrary sized text spans, as thecurrent restriction on spans being the same size is159System headlines OnWN FNWN SMT mean rankSRIUBC-system1 (Baseline) 0.6083 0.2915 0.2790 0.3065 0.4011 66SRIUBC-system2 (NLPM, TFIDF-ESA) 0.6359 0.3664 0.2713 0.3476 0.4420 57SRIUBC-system3 (Combined) 0.5443 0.2843 0.2705 0.3275 0.3842 70NLPM 0.5791 0.3157 0.3211 0.2698 0.3714TFIDF-ESA 0.5739 0.7222 0.1781 0.2980 0.4431Lex-only 0.5455 0.3237 0.2095 0.3146 0.3483Table 2: Pearson correlation of systems against the test datasets (top).
The test set performance for the new NeuralProbabilistic Language Model (NLPM) and TFIDF-ESA components are given, along with a lexical-only variant forcomparison (bottom).String 1 String 2 Vec.
Window RBM Window TFIDF-ESAheart attack cardiac arrest 0.354 0.544 0.182door key cardiac arrest -0.14 -0.177 0baby food cat food 0.762 0.907 0.079dog food cat food 0.886 0.914 0.158rotten food baby food 0.482 0.473 0.071frozen solid thawed out 0.046 -0.331 0.102severely burnt frozen stiff -0.023 -0.155 0uphill slog raced downhill 0.03 -0.322 0.043small cat large dog 0.817 0.905 0.007ran along sprinted by 0.31 0.238 0.004ran quickly jogged rapidly 0.349 0.327 0.001deathly ill very sick 0.002 0.177 0.004ran to raced to 0.815 0.829 0.013free drinks drinks free 0.001 0.042 1door key combination lock 0.098 0.093 0.104frog blast vent core 0.003 0.268 0.004Table 3: Cosine similarity of two input strings, as given by the vectors generated from the Vector Window size 2, RBMWindow size 2, and TFIDF-ESA.unrealistic.
One possibility is to use recurrent neuralnetwork techniques to generate this type of encod-ing.Finally, the size of the Wikipedia dump used totrain the Restricted Boltzmann Machines could beat issue, as 35 million words could be consideredsmall compared to the full range of expressions wewould wish to capture, especially for the larger win-dow spans.
A larger training corpus may be neededto fully see the benefit from RBMs.AcknowledgmentsSupported by the Artificial Intelligence Center at SRI In-ternational.
The views and conclusions contained hereinare those of the authors and should not be interpreted asnecessarily representing the official policies or endorse-ments, either expressed or implied, of the Artificial Intel-ligence Center, or SRI International.ReferencesEneko Agirre, Montse Cuadros, German Rigau, and AitorSoroa.
2010.
Exploring knowledge bases for similar-ity.
In Proceedings of the International Conference onLanguage Resources and Evaluation 2010.Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Research,3:1137?1155.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIBSVM:A library for support vector machines.
ACM Transac-160tions on Intelligent Systems and Technology, 2:27:1?27:27.Rudi Cilibrasi and Paul M. B. Vita?nyi.
2004.
The googlesimilarity distance.
CoRR, abs/cs/0412098.R.
Collobert and J. Weston.
2008.
A unified architecturefor natural language processing: Deep neural networkswith multitask learning.
In International Conferenceon Machine Learning, ICML.Christine Fellbaum.
1998.
WordNet - An Electronic Lex-ical Database.
MIT Press.Samuel Fernando and Mark Stevenson.
2008.
A se-mantic similarity approach to paraphrase detection.
InComputational Linguistics UK (CLUK 2008) 11th An-nual Research Colloqium.Andrew Finch, Young-Sook Hwang, and Eiichio Sumita.2005.
Using machine translation evaluation tech-niques to determine sentence-level semantic equiva-lence.
In Proceedings of the Third International Work-shop on Paraphrasing (IWP 2005), pages 17?24, JejuIsland, South Korea.Evgeniy Gabrilovich and Shaul Markovitch.
2009.Wikipedia-based semantic interpretation.
Journal ofArtificial Intelligence Research, 34:443?498.Geoffrey E. Hinton.
2002.
Training products of expertsby minimizing contrastive divergence.
Neural Com-putation, 14(8):1771?1800.Eric H. Huang, Richard Socher, Christopher D. Manning,and Andrew Y. Ng.
2012.
Improving Word Represen-tations via Global Context and Multiple Word Proto-types.
In Annual Meeting of the Association for Com-putational Linguistics (ACL).Rada Mihalcea, Courtney Corley, and Carlo Strappar-ava.
2006.
Corpus-based and knowledge-based mea-sures of text semantic similarity.
In Proceedings of theAmerican Association for Artificial Intelligence (AAAI2006), Boston, Massachusetts, July.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting on Association for Computa-tional Linguistics, ACL ?02, pages 311?318, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Eric Yeh and Eneko Agirre.
2012.
Sri and ubc: Simplesimilarity features for semantic textual similarity.
InProceedings of SemEval 2012.161
