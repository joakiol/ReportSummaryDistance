Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 142?146,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsImprove Statistical Machine Translation with Context-SensitiveBilingual Semantic Embedding ModelHaiyang Wu1Daxiang Dong1Wei He1Xiaoguang Hu1Dianhai Yu1Hua Wu1Haifeng Wang1Ting Liu21Baidu Inc., No.
10, Shangdi 10th Street, Beijing, 100085, China2Harbin Institute of Technology, Harbin, Chinawuhaiyang,dongdaxiang,hewei,huxiaoguang,yudianhai,wu hua,wanghaifeng@baidu.comtliu@ir.hit.edu.cnAbstractWe investigate how to improve bilingualembedding which has been successfullyused as a feature in phrase-based sta-tistical machine translation (SMT).
De-spite bilingual embedding?s success, thecontextual information, which is of criti-cal importance to translation quality, wasignored in previous work.
To employthe contextual information, we proposea simple and memory-efficient model forlearning bilingual embedding, taking boththe source phrase and context around thephrase into account.
Bilingual translationscores generated from our proposed bilin-gual embedding model are used as featuresin our SMT system.
Experimental resultsshow that the proposed method achievessignificant improvements on large-scaleChinese-English translation task.1 IntroductionIn Statistical Machine Translation (SMT) sys-tem, it is difficult to determine the translation ofsome phrases that have ambiguous meanings.Forexample, the phrase???
jieguo?
can be trans-lated to either ?results?, ?eventually?
or ?fruit?,depending on the context around it.
There are tworeasons for the problem: First, the length of phrasepairs is restricted due to the limitation of modelsize and training data.
Another reason is that SMTsystems often fail to use contextual informationin source sentence, therefore, phrase sense disam-biguation highly depends on the language modelwhich is trained only on target corpus.To solve this problem, we present to learncontext-sensitive bilingual semantic embedding.Our methodology is to train a supervised modelwhere labels are automatically generated fromphrase-pairs.
For each source phrase, the alignedtarget phrase is marked as the positive labelwhereas other phrases in our phrase table aretreated as negative labels.
Different from previ-ous work in bilingual embedding learning(Zou etal., 2013; Gao et al., 2014), our framework is asupervised model that utilizes contextual informa-tion in source sentence as features and make useof phrase pairs as weak labels.
Bilingual seman-tic embeddings are trained automatically from oursupervised learning task.Our learned bilingual semantic embeddingmodel is used to measure the similarity of phrasepairs which is treated as a feature in decoding.
Weintegrate our learned model into a phrase-basedtranslation system and experimental results indi-cate that our system significantly outperform thebaseline system.
On the NIST08 Chinese-Englishtranslation task, we obtained 0.68 BLEU improve-ment.
We also test our proposed method on muchlarger web dataset and obtain 0.49 BLEU im-provement against the baseline.2 Related WorkUsing vectors to represent word meanings isthe essence of vector space models (VSM).
Therepresentations capture words?
semantic and syn-tactic information which can be used to measuresemantic similarities by computing distance be-tween the vectors.
Although most VSMs representone word with only one vector, they fail to cap-ture homonymy and polysemy of word.
Huanget al.
(2012) introduced global document contextand multiple word prototypes which distinguishesand uses both local and global context via a jointtraining objective.
Much of the research focuson the task of inducing representations for sin-gle languages.
Recently, a lot of progress has142been made at representation learning for bilin-gual words.
Bilingual word representations havebeen presented by Peirsman and Pad?o (2010) andSumita (2000).
Also unsupervised algorithmssuch as LDA and LSA were used by Boyd-Graberand Resnik (2010), Tam et al.
(2007) and Zhao andXing (2006).
Zou et al.
(2013) learn bilingual em-beddings utilizes word alignments and monolin-gual embeddings result, Le et al.
(2012) and Gao etal.
(2014) used continuous vector to represent thesource language or target language of each phrase,and then computed translation probability usingvector distance.
Vuli?c and Moens (2013) learnedbilingual vector spaces from non-parallel data in-duced by using a seed lexicon.
However, noneof these work considered the word sense disam-biguation problem which Carpuat and Wu (2007)proved it is useful for SMT.
In this paper, we learnbilingual semantic embeddings for source contentand target phrase, and incorporate it into a phrase-based SMT system to improve translation quality.3 Context-Sensitive Bilingual SemanticEmbedding ModelWe propose a simple and memory-efficientmodel which embeds both contextual informationof source phrases and aligned phrases in target cor-pus into low dimension.
Our assumption is thathigh frequent words are likely to have multipleword senses; therefore, top frequent words are se-lected in source corpus.
We denote our selectedwords as focused phrase.
Our goal is to learn abilingual embedding model that can capture dis-criminative contextual information for each fo-cused phrase.
To learn an effective context sensi-tive bilingual embedding, we extract context fea-tures nearby a focused phrase that will discrimi-nate focused phrase?s target translation from otherpossible candidates.
Our task can be viewed asa classification problem that each target phrase istreated as a class.
Since target phrases are usu-ally in very high dimensional space, traditionallinear classification model is not suitable for ourproblem.
Therefore, we treat our problem as aranking problem that can handle large number ofclasses and optimize the objectives with scalableoptimizer stochastic gradient descent.3.1 Bilingual Word EmbeddingWe apply a linear embedding model for bilin-gual embedding learning.
Cosine similarity be-tween bilingual embedding representation is con-sidered as score function.
The score functionshould be discriminative between target phrasesand other candidate phrases.
Our score functionis in the form:f(x,y; W,U) = cos(WTx,UTy) (1)where x is contextual feature vector in source sen-tence, and y is the representation of target phrase,W ?
R|X|?k,U ?
R|Y|?kare low rank ma-trix.
In our model, we allow y to be bag-of-wordsrepresentation.
Our embedding model is memory-efficient in that dimensionality of x and y can bevery large in practical setting.
We use |X| and |Y|means dimensionality of random variable x and y,then traditional linear model such as max-entropymodel requires memory space of O(|X||Y|).
Ourembedding model only requires O(k(|X|+ |Y|))memory space that can handle large scale vocabu-lary setting.
To score a focused phrase and targetphrase pair with f(x,y), context features are ex-tracted from nearby window of the focused phrase.Target words are selected from phrase pairs.
Givena source sentence, embedding of a focused phraseis estimated from WTx and target phrase embed-ding can be obtained through UTy.3.2 Context Sensitive FeaturesContext of a focused phrase is extracted fromnearby window, and in our experiment we choosewindow size of 6 as a focused phrase?s con-text.
Features are then extracted from the focusedphrase?s context.
We demonstrate our featureextraction and label generation process from theChinese-to-English example in figure 1.
Windowsize in this example is three.
Position featuresand Part-Of-Speech Tagging features are extractedfrom the focused phrase?s context.
The word fruitFigure 1: Feature extraction and label generation143is the aligned phrase of our focused phrase and istreated as positive label.
The phrase results is arandomly selected phrase from phrase table resultsof ??.
Note that feature window is not well de-fined near the beginning or the end of a sentence.To conquer this problem, we add special paddingword to the beginning and the end of a sentence toaugment sentence.3.3 Parameter LearningTo learn model parameter W and U, we ap-ply a ranking scheme on candidates selected fromphrase table results of each focused phrase.
In par-ticular, given a focus phrase w, aligned phrase istreated as positive label whereas phrases extractedfrom other candidates in phrase table are treatedas negative label.
A max-margin loss is applied inthis ranking setting.I(?)
=1mm?i=1(?
?
f(xi, yi; ?)?
f(xi, y?i; ?
))+(2)Where f(xi,yi) is previously defined, ?
={W,U} and + means max-margin hinge loss.
Inour implementation, a margin of ?
= 0.15 is usedduring training.
Objectives are minimized throughstochastic gradient descent algorithm.
For eachrandomly selected training example, parametersare updated through the following form:?
:= ??
??l(?)??
(3)where ?
= {W,U}.
Given an instance with pos-itive and negative label pair {x,y,y?
}, gradientsof parameter W and U are as follows:?l(W,U)?W= qsx(WTx)T?
pqs3x(UTy) (4)?l(W,U)?U= qsy(UTy)T?
pqs3y(WTx) (5)Where we set p = (WTx)T(UTy), q =1||WTx||2and s =1||UTy||2.
To initialize our model param-eters with strong semantic and syntactic informa-tion, word vectors are pre-trained independentlyon source and target corpus through word2vec(Mikolov et al., 2013).
And the pre-trained wordvectors are treated as initial parameters of ourmodel.
The learned scoring function f(x,y) willbe used during decoding phase as a feature in log-linear model which we will describe in detail later.4 Integrating Bilingual SemanticEmbedding into Phrase-Based SMTArchitecturesTo incorporate the context-sensitive bilingualembedding model into the state-of-the-art Phrase-Based Translation model, we modify the decodingso that context information is available on everysource phrase.
For every phrase in a source sen-tence, the following tasks are done at every nodein our decoder:?
Get the focused phrase as well as its context in thesource sentence.?
Extract features from the focused phrase?s context.?
Get translation candidate extracted from phrase pairs ofthe focused phrase.?
Compute scores for any pair of the focused phrase anda candidate phrase.We get the target sub-phrase using word align-ment of phrase, and we treat NULL as a commontarget word if there is no alignment for the focusedphrase.
Finally we compute the matching score forsource content and target word using bilingual se-mantic embedding model.
If there are more thanone word in the focus phrase, then we add all scoretogether.
A penalty value will be given if target isnot in translation candidate list.
For each phrase ina given SMT input sentence, the Bilingual Seman-tic score can be used as an additional feature inlog-linear translation model, in combination withother typical context-independent SMT bilexiconprobabilities.5 ExperimentOur experiments are performed using an in-house phrase-based system with a log-linearframework.
Our system includes a phrase trans-lation model, an n-gram language model, a lexi-calized reordering model, a word penalty modeland a phrase penalty model, which is similar toMoses (Koehn et al., 2007).
The evaluation metricis BLEU (Papineni et al., 2002).5.1 Data setWe test our approach on LDC corpus first.
Wejust use a subset of the data available for NISTOpenMT08 task1.
The parallel training corpus1LDC2002E18, LDC2002L27, LDC2002T01,LDC2003E07, LDC2003E14, LDC2004T07, LDC2005E83,LDC2005T06, LDC2005T10, LDC2005T34, LDC2006E24,LDC2006E26, LDC2006E34, LDC2006E86, LDC2006E92,LDC2006E93, LDC2004T08(HK News, HK Hansards )144MethodOpenMT08 WebDataBLEU BLEUOur Baseline 26.24 29.32LOC 26.78** 29.62*LOC+POS 26.82** 29.81*Table 1: Results of lowercase BLEU on NIST08task.
LOC is the location feature and POS isthe Part-of-Speech feature * or ** equals to sig-nificantly better than our baseline(?
< 0.05 or?
< 0.01, respectively)contains 1.5M sentence pairs after we filter withsome simple heuristic rules, such as sentence be-ing too long or containing messy codes.
As mono-lingual corpus, we use the XinHua portion of theEnglish GigaWord.
In monolingual corpus we fil-ter sentence if it contain more than 100 wordsor contain messy codes, Finally, we get mono-lingual corpus containing 369M words.
In orderto test our approach on a more realistic scenario,we train our models with web data.
Sentencepairs obtained from bilingual website and com-parable webpage.
Monolingual corpus is gainedfrom some large website such as WiKi.
There are50M sentence pairs and 10B words monolingualcorpus.5.2 Results and AnalysisFor word alignment, we align all of the train-ing data with GIZA++ (Och and Ney, 2003), us-ing the grow-diag-final heuristic to improve recall.For language model, we train a 5-gram modifiedKneser-Ney language model and use MinimumError Rate Training (Och, 2003) to tune the SMT.For both OpenMT08 task and WebData task, weuse NIST06 as the tuning set, and use NIST08 asthe testing set.
Our baseline system is a standardphrase-based SMT system, and a language modelis trained with the target side of bilingual corpus.Results on Chinese-English translation task are re-ported in Table 1.
Word position features and part-of-speech tagging features are both useful for ourbilingual semantic embedding learning.
Based onour trained bilingual embedding model, we caneasily compute a translation score between anybilingual phrase pair.
We list some cases in table2 to show that our bilingual embedding is contextsensitive.Contextual features extracted from source sen-tence are strong enough to discriminate differentSource Sentence4 Nearest Neighbor frombilingual embedding????????????????????????
(Investorscan only get down tobusiness in a stable so-cial environment)will be, can only, will, can????????????????????????
(In compe-titions, the Chinese Dis-abled have shown ex-traordinary athletic abil-ities)skills, ability, abilities, tal-ent???????????????????????
(In the natu-ral environment of CostaRica, grapes do not nor-mally yield fruit.
)fruit, outcome of, the out-come, result?
?
?????????????
(Asa result, Eastern DistrictCouncil passed a pro-posal)in the end, eventually, as aresult, resultsTable 2: Top ranked focused phrases based onbilingual semantic embeddingword senses.
And we also observe from the word???
jieguo?
that Part-Of-Speech Tagging fea-tures are effective in discriminating target phrases.6 ConlusionIn this paper, we proposed a context-sensitivebilingual semantic embedding model to improvestatistical machine translation.
Contextual infor-mation is used in our model for bilingual wordsense disambiguation.
We integrated the bilingualsemantic model into the phrase-based SMT sys-tem.
Experimental results show that our methodachieves significant improvements over the base-line on large scale Chinese-English translationtask.
Our model is memory-efficient and practicalfor industrial usage that training can be done onlarge scale data set with large number of classes.Prediction time is also negligible with regard toSMT decoding phase.
In the future, we will ex-plore more features to refine the model and try toutilize contextual information in target sentences.AcknowledgmentsWe thank the three anonymous reviewers fortheir valuable comments, and Niu Gang and WuXianchao for discussions.
This paper is supportedby 973 program No.
2014CB340505.145ReferencesJordan Boyd-Graber and Philip Resnik.
2010.
Holis-tic sentiment analysis across languages: Multilin-gual supervised latent dirichlet allocation.
In Pro-ceedings of the 2010 Conference on Empirical Meth-ods in Natural Language Processing, pages 45?55,Cambridge, MA, October.
Association for Compu-tational Linguistics.Marine Carpuat and Dekai Wu.
2007.
Improving sta-tistical machine translation using word sense disam-biguation.
In Proceedings of the 2007 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning (EMNLP-CoNLL), pages 61?72, Prague,Czech Republic, June.
Association for Computa-tional Linguistics.Jianfeng Gao, Xiaodong He, Wen-tau Yih, andLi Deng.
2014.
Learning continuous phrase rep-resentations for translation modeling.
In Proc.
ACL.Eric Huang, Richard Socher, Christopher Manning,and Andrew Ng.
2012.
Improving word represen-tations via global context and multiple word proto-types.
In Proceedings of the 50th Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 873?882, Jeju Island,Korea, July.
Association for Computational Linguis-tics.Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.2012.
Continuous space translation models withneural networks.
In Proceedings of the 2012 Con-ference of the North American Chapter of the As-sociation for Computational Linguistics: HumanLanguage Technologies, pages 39?48, Montr?eal,Canada, June.
Association for Computational Lin-guistics.Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.Corrado, and Jeffrey Dean.
2013.
Distributed rep-resentations of words and phrases and their compo-sitionality.
In NIPS, pages 3111?3119.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
In Computational Linguistics, Volume 29,Number 1, March 2003.
Computational Linguistics,March.Franz Josef Och.
2003.
Minimum error rate train-ing in statistical machine translation.
In Proceed-ings of the 41st Annual Meeting of the Associationfor Computational Linguistics, pages 160?167, Sap-poro, Japan, July.
Association for ComputationalLinguistics.Yves Peirsman and Sebastian Pad?o.
2010.
Cross-lingual induction of selectional preferences withbilingual vector spaces.
In Human Language Tech-nologies: The 2010 Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics, pages 921?929, Los Ange-les, California, June.
Association for ComputationalLinguistics.Eiichiro Sumita.
2000.
Lexical transfer using a vector-space model.
In Proceedings of the 38th AnnualMeeting of the Association for Computational Lin-guistics.
Association for Computational Linguistics,August.Yik-Cheung Tam, Ian Lane, and Tanja Schultz.
2007.Bilingual-lsa based lm adaptation for spoken lan-guage translation.
In Proceedings of the 45th An-nual Meeting of the Association of ComputationalLinguistics, pages 520?527, Prague, Czech Repub-lic, June.
Association for Computational Linguis-tics.Ivan Vuli?c and Marie-Francine Moens.
2013.
Cross-lingual semantic similarity of words as the similarityof their semantic word responses.
In Proceedings ofthe 2013 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, pages 106?116, At-lanta, Georgia, June.
Association for ComputationalLinguistics.Bing Zhao and Eric P. Xing.
2006.
Bitam: Bilingualtopic admixture models for word alignment.
In Pro-ceedings of the COLING/ACL 2006 Main Confer-ence Poster Sessions, pages 969?976, Sydney, Aus-tralia, July.
Association for Computational Linguis-tics.Will Y. Zou, Richard Socher, Daniel Cer, and Christo-pher D. Manning.
2013.
Bilingual word embed-dings for phrase-based machine translation.
In Pro-ceedings of the 2013 Conference on Empirical Meth-ods in Natural Language Processing, pages 1393?1398, Seattle, Washington, USA, October.
Associa-tion for Computational Linguistics.146
